<!DOCTYPE html>
<html>
<head><meta charset="utf-8"><title>Weekly Research Intelligence — OR for Generative AI</title>
</head>
<body style="margin:0; padding:0; background:#F5F7FA; font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif; color:#1F2937; font-size:14px; line-height:1.5;">
<div style="display:none; max-height:0; overflow:hidden; mso-hide:all; opacity:0; color:transparent;">Research Intelligence curated digest — Latest papers, research fronts, and framework evolution.</div>
<table width="100%" cellpadding="0" cellspacing="0" style="background:#F5F7FA;">
<tr><td align="center" style="padding:0;">
<table width="85%" cellpadding="0" cellspacing="0" style="width:85%;">

<tr><td style="background:linear-gradient(135deg, #1E3A8A 0%, #2563EB 100%); color:white; padding:20px 24px; border-radius:16px 16px 0 0;">
<table width="100%" cellpadding="0" cellspacing="0">
<tr>
<td style="width:48px; vertical-align:top;">
<div style="width:44px; height:44px; background:rgba(255,255,255,0.2); border-radius:12px; display:flex; align-items:center; justify-content:center; border:2px solid rgba(255,255,255,0.3);">
<span style="font-size:18px; font-weight:800; color:white; line-height:44px; text-align:center; display:block;">RI</span>
</div>
</td>
<td style="padding-left:14px; vertical-align:top;">
<h1 style="margin:0; font-size:20px; font-weight:700; letter-spacing:-0.01em; line-height:1.2;">Weekly Research Intelligence — OR for Generative AI</h1>
<p style="margin:4px 0 0; font-size:11px; opacity:0.85; font-weight:500;">Issue #8 of 2026 · 2026-02-18</p>
</td>
</tr>
</table>
</td></tr>
<tr><td style="background:#FFFFFF; padding:0;">
<table width="100%" cellpadding="0" cellspacing="0"><tr><td style="background:#FFFFFF; padding:20px 24px 12px;">
  <table width="100%" cellpadding="0" cellspacing="0" style="border:1px solid #E1E4E8; border-radius:14px; background:#FAFBFC;">
    <tr>
      <td style="padding:14px 18px; font-size:12px; color:#1F2937; font-weight:700; text-transform:uppercase; letter-spacing:0.03em; border-bottom:1px solid #E1E4E8;">
        This week at a glance
      </td>
    </tr>
    <tr>
      <td style="padding:0 18px 14px;">
        <table width="100%" cellpadding="0" cellspacing="0">
          <tr>
            <td style="width:33%; padding:12px 0; text-align:center;">
              <div style="font-size:24px; font-weight:800; color:#2563EB;">45</div>
              <div style="font-size:11px; color:#6B7280; text-transform:uppercase; letter-spacing:0.02em;">Must-reads</div>
            </td>
            <td style="width:33%; padding:12px 0; text-align:center; border-left:1px solid #E1E4E8; border-right:1px solid #E1E4E8;">
              <div style="font-size:24px; font-weight:800; color:#2563EB;">69</div>
              <div style="font-size:11px; color:#6B7280; text-transform:uppercase; letter-spacing:0.02em;">New papers</div>
            </td>
            <td style="width:33%; padding:12px 0; text-align:center;">
              <div style="font-size:24px; font-weight:800; color:#2563EB;">6</div>
              <div style="font-size:11px; color:#6B7280; text-transform:uppercase; letter-spacing:0.02em;">Active fronts</div>
            </td>
          </tr>
        </table>
        <div style="margin-top:10px; padding-top:10px; border-top:1px solid #E1E4E8; font-size:12px; color:#1F2937;">
          <b>This week's theme:</b> Concept-structured search is outperforming brute code mutation across multiple optimization domains.
        </div>
      </td>
    </tr>
  </table>
</td></tr><tr><td style="padding:20px 24px 12px; border-top:2px solid #F8F9FB; background:#FAFBFC;">
  <h2 style="margin:0; font-size:15px; font-weight:700; color:#1F2937; text-transform:uppercase; letter-spacing:0.03em;">Top Priority Papers</h2>
  <p style="margin:4px 0 0; font-size:11px; color:#6B7280; font-weight:500;">10 must-read papers this week (ranked by significance, recency, and impact)</p>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div style="margin-bottom:8px;">
    <span style="display:inline-block; background:#2563EB; color:white; padding:2px 10px; border-radius:12px; font-size:11px; font-weight:700; margin-right:8px;">#1</span>
    <a href="https://arxiv.org/abs/2502.07115" style="color:#1F2937; text-decoration:none; font-weight:700; font-size:16px; letter-spacing:-0.01em;">Online Scheduling for LLM Inference with KV Cache Constraints</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700;">PRIORITY 8.6/10</span> <span style="background:#DC2626; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600;">MUST-READ</span>
  </div>
  <div style="margin-bottom:6px; font-size:11px; color:#6B7280;">
    2026-01-15 | Massachusetts Institute of Technology, Microsoft Research, HKUST | <a href="https://arxiv.org/abs/2502.07115" style="color:#6B7280; text-decoration:none;">2502.07115</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">M=8</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">P=10</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px;">I=8</span>
  </div>
  <div style="font-size:13px; color:#1F2937; line-height:1.6; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #2563EB;">
    This paper formulates LLM inference scheduling as an Integer Program (IP) that explicitly models the linear memory growth of KV caches, and proposes a 'Memory Constrained Shortest First' (MC-SF) algorithm. The results are rigorous, showing MC-SF achieves near-optimal performance (within 5% of hindsight optimal) on synthetic data and significantly outperforms standard FCFS/threshold heuristics on real traces. The critical takeaway is the 'future feasibility check' (Eq. 5), which validates that a batch will <em>remain</em> within memory limits throughout the generation process based on predicted output lengths—a necessary deviation from standard static-size scheduling. This is foundational reading for our GPUSched project, providing both the exact IP baseline we need and a strong heuristic to benchmark against.
  </div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div style="margin-bottom:8px;">
    <span style="display:inline-block; background:#2563EB; color:white; padding:2px 10px; border-radius:12px; font-size:11px; font-weight:700; margin-right:8px;">#2</span>
    <a href="https://arxiv.org/abs/2504.11320" style="color:#1F2937; text-decoration:none; font-weight:700; font-size:16px; letter-spacing:-0.01em;">Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700;">PRIORITY 8.6/10</span> <span style="background:#DC2626; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600;">MUST-READ</span>
  </div>
  <div style="margin-bottom:6px; font-size:11px; color:#6B7280;">
    2026-01-05 | Massachusetts Institute of Technology, Peking University, Alibaba Group | <a href="https://arxiv.org/abs/2504.11320" style="color:#6B7280; text-decoration:none;">2504.11320</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">M=8</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">P=10</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px;">I=8</span>
  </div>
  <div style="font-size:13px; color:#1F2937; line-height:1.6; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #2563EB;">
    This paper formulates LLM inference as a multi-stage stochastic scheduling problem, introducing 'Nested WAIT'—a threshold-based algorithm that handles unknown output lengths by letting prompts classify themselves as they survive into deeper decode segments. Unlike heuristic baselines (vLLM, Sarathi), they provide rigorous asymptotic optimality proofs and high-probability bounds against memory overflow, validated on A100 simulations. The key takeaway is the 'nested segment' mechanism: instead of predicting job size, structure the queue so short jobs exit early and long jobs naturally migrate to lower-priority/protected tiers, effectively decoupling the memory risk. We should immediately evaluate this threshold logic for our GPUSched formulations, as it likely outperforms our current predictive or FCFS approaches for handling KV cache growth.
  </div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div style="margin-bottom:8px;">
    <span style="display:inline-block; background:#2563EB; color:white; padding:2px 10px; border-radius:12px; font-size:11px; font-weight:700; margin-right:8px;">#3</span>
    <a href="https://arxiv.org/abs/2511.09092" style="color:#1F2937; text-decoration:none; font-weight:700; font-size:16px; letter-spacing:-0.01em;">OR-R1: Automating Modeling and Solving of Operations Research Optimization Problem via Test-Time Reinforcement Learning</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700;">PRIORITY 8.4/10</span> <span style="background:#DC2626; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600;">MUST-READ</span>
  </div>
  <div style="margin-bottom:6px; font-size:11px; color:#6B7280;">
    2025-11-12 | The Hong Kong University of Science and Technology, Arizona State University, University of North Carolina at Chapel Hill | <a href="https://arxiv.org/abs/2511.09092" style="color:#6B7280; text-decoration:none;">2511.09092</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">M=8</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">P=9</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px;">I=8</span>
  </div>
  <div style="font-size:13px; color:#1F2937; line-height:1.6; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #2563EB;">
    OR-R1 introduces a data-efficient framework that fine-tunes Qwen3-8B using Supervised Fine-Tuning (SFT) followed by Test-Time Group Relative Policy Optimization (TGRPO) on unlabeled data. The results are empirically strong: it outperforms ORLM and LLMOPT while using only 1/10th of the synthetic training data, specifically narrowing the consistency gap between Pass@1 and Pass@8. The key takeaway for us is the effectiveness of GRPO (normalizing rewards within a sampled group to estimate baselines) combined with majority-voting rewards; this eliminates the need for a separate critic model while significantly improving code generation consistency. We should immediately evaluate GRPO as a lightweight alternative to PPO for the 'RL-infused' components of our evolutionary search methods.
  </div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div style="margin-bottom:8px;">
    <span style="display:inline-block; background:#2563EB; color:white; padding:2px 10px; border-radius:12px; font-size:11px; font-weight:700; margin-right:8px;">#4</span>
    <a href="https://arxiv.org/abs/2504.07347" style="color:#1F2937; text-decoration:none; font-weight:700; font-size:16px; letter-spacing:-0.01em;">Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700;">PRIORITY 8.4/10</span> <span style="background:#DC2626; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600;">MUST-READ</span>
  </div>
  <div style="margin-bottom:6px; font-size:11px; color:#6B7280;">
    2025-04-24 | Cornell University, Columbia University | <a href="https://arxiv.org/abs/2504.07347" style="color:#6B7280; text-decoration:none;">2504.07347</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">M=8</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">P=9</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px;">I=8</span>
  </div>
  <div style="font-size:13px; color:#1F2937; line-height:1.6; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #2563EB;">
    Li et al. formulate a batch queueing model for LLM inference, proving that 'work-conserving' algorithms (like Sarathi-Serve) which mix prefill and decode tokens are throughput-optimal, whereas separated strategies (vanilla vLLM, FasterTransformer) are theoretically unstable. The results are rigorous, combining fluid limit proofs with empirical validation on A100s showing queue blow-ups in non-optimal schedulers. The key takeaway is the precise definition of stability for token-level batching and the counter-intuitive finding that these locally optimal policies can fail in multi-agent networks due to cyclic resource dependencies. This is foundational reading for our GPUSched project and directly informs how we should model resource allocation for our multi-agent optimization systems.
  </div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div style="margin-bottom:8px;">
    <span style="display:inline-block; background:#2563EB; color:white; padding:2px 10px; border-radius:12px; font-size:11px; font-weight:700; margin-right:8px;">#5</span>
    <a href="https://arxiv.org/abs/2601.21008" style="color:#1F2937; text-decoration:none; font-weight:700; font-size:16px; letter-spacing:-0.01em;">Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and Behavioral Rationality in Operations Research</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700;">PRIORITY 8.3/10</span> <span style="background:#DC2626; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600;">MUST-READ</span>
  </div>
  <div style="margin-bottom:6px; font-size:11px; color:#6B7280;">
    2026-02-08 | Massachusetts Institute of Technology, Alibaba Group | <a href="https://arxiv.org/abs/2601.21008" style="color:#6B7280; text-decoration:none;">2601.21008</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">M=9</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">P=8</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px;">I=9</span>
  </div>
  <div style="font-size:13px; color:#1F2937; line-height:1.6; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #2563EB;">
    Ao et al. introduce a framework for iterative OR model debugging that trains an 8B model using Group Relative Policy Optimization (GRPO) and a Process Reward Model (PRM) to outperform GPT-4o-mini. They utilize Gurobi's Irreducible Infeasible Subsystem (IIS) not just as text feedback, but as a dense reward signal (IIS size reduction) for the PRM, achieving a 95.3% recovery rate versus 86.2% for frontier APIs. <strong>Key Takeaway:</strong> We should steal their PRM construction method—specifically using solver diagnostics (like IIS reduction or compiler error counts) as dense step-level rewards—and their 'faithfulness penalty' to prevent overfitting in our evolutionary search. This is a direct validation of RLVR (Reinforcement Learning with Verifiable Rewards) for OR, proving it superior to large-scale prompting.
  </div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div style="margin-bottom:8px;">
    <span style="display:inline-block; background:#2563EB; color:white; padding:2px 10px; border-radius:12px; font-size:11px; font-weight:700; margin-right:8px;">#6</span>
    <a href="https://arxiv.org/abs/2406.01566" style="color:#1F2937; text-decoration:none; font-weight:700; font-size:16px; letter-spacing:-0.01em;">Helix: Serving Large Language Models over Heterogeneous GPUs and Network via Max-Flow</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700;">PRIORITY 8.3/10</span> <span style="background:#DC2626; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600;">MUST-READ</span>
  </div>
  <div style="margin-bottom:6px; font-size:11px; color:#6B7280;">
    2025-03-05 | Carnegie Mellon University | <a href="https://arxiv.org/abs/2406.01566" style="color:#6B7280; text-decoration:none;">2406.01566</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">M=8</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">P=9</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px;">I=8</span>
  </div>
  <div style="font-size:13px; color:#1F2937; line-height:1.6; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #2563EB;">
    Helix formulates distributed LLM serving on heterogeneous clusters as a max-flow problem, using MILP to optimize model placement and deriving a per-request weighted round-robin scheduler from the flow solution. Unlike standard static pipeline parallelism, it routes every request dynamically based on edge capacities, achieving up to 3.3x throughput gains over Swarm on mixed GPU clusters (L4/T4/A100). The results are rigorous, backed by both physical cluster experiments and high-fidelity simulations. The critical takeaway is the 'per-request pipeline' abstraction: decoupling request routing from static device assignment allows exact OR methods to maximize utilization of weaker hardware—a technique we should immediately evaluate for our GPUSched project.
  </div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div style="margin-bottom:8px;">
    <span style="display:inline-block; background:#2563EB; color:white; padding:2px 10px; border-radius:12px; font-size:11px; font-weight:700; margin-right:8px;">#7</span>
    <a href="https://arxiv.org/abs/2602.02987" style="color:#1F2937; text-decoration:none; font-weight:700; font-size:16px; letter-spacing:-0.01em;">Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700;">PRIORITY 8.2/10</span> <span style="background:#DC2626; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600;">MUST-READ</span>
  </div>
  <div style="margin-bottom:6px; font-size:11px; color:#6B7280;">
    2026-02-03 | The Hong Kong University of Science and Technology | <a href="https://arxiv.org/abs/2602.02987" style="color:#6B7280; text-decoration:none;">2602.02987</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">M=8</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">P=9</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px;">I=7</span>
  </div>
  <div style="font-size:13px; color:#1F2937; line-height:1.6; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #2563EB;">
    Lin et al. formulate LLM inference scheduling as a multiclass many-server queueing network, deriving a 'Gate-and-Route' policy from a steady-state fluid LP that explicitly manages prefill-decode contention. Calibrated on A100s, their approach proves that separating prefill admission (via occupancy tracking) from decode routing (work-conserving) eliminates decode backlogs and maximizes revenue. <strong>Key Takeaway:</strong> The decomposition of scheduling into 'static planning' (solving an LP for target occupancies) and 'dynamic control' (a simple gate tracking those targets) is a scalable alternative to online combinatorial optimization for your GPUSched work. It mathematically formalizes the intuition that prefill is the bottleneck and decode should be kept strictly critical but not backlogged.
  </div>
</td></tr><tr><td style="padding:20px 24px 12px; border-top:2px solid #F8F9FB; background:#FAFBFC;">
  <h2 style="margin:0; font-size:15px; font-weight:700; color:#1F2937; text-transform:uppercase; letter-spacing:0.03em;">Research Front Landscape</h2>
  <p style="margin:4px 0 0; font-size:11px; color:#6B7280; font-weight:500;">6 active fronts | 69 new papers</p>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div>
    <h3 style="margin:0; font-size:16px; font-weight:700; color:#1F2937; letter-spacing:-0.01em;">Optimizing LLM Inference and Automated OR Problem Generation</h3>
  </div>
  <div style="margin-top:8px; line-height:1.8;">
    <span style="background:#0891B2; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700; text-transform:uppercase; display:inline-block;">EMERGING</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">Density: 0.57</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">7 papers</span>
  </div>
  <div style="margin-top:10px; line-height:1.8;"><span style="font-size:11px; color:#6B7280; font-weight:600; text-transform:uppercase; letter-spacing:0.02em; margin-right:6px;">Methods</span><span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">queueing_theory</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">reinforcement_learning</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">llm_code_generation</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">llm_fine_tuned</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">program_synthesis</span></div>
  <div style="margin-top:6px; font-size:10px; color:#6B7280;"><span style="font-weight:700; text-transform:uppercase; letter-spacing:0.02em; margin-right:4px;">Inst:</span>Massachusetts Institute of. 29% &nbsp;·&nbsp; Columbia University 29% &nbsp;·&nbsp; Shanghai Jiao Tong. 29% &nbsp;·&nbsp; Peking University 14%</div>
  <div style="margin-top:10px; font-size:13px; line-height:1.6; color:#1F2937; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #4F46E5;"><p style="margin:0;">This research front explores the application of advanced Operations Research (OR) principles and AI techniques to two critical challenges within the LLM ecosystem: optimizing LLM inference serving and automating the generation of OR models. For LLM inference, papers focus on sophisticated scheduling algorithms to manage GPU resources, KV cache memory, and request batching under various constraints. Concurrently, other works leverage large language models themselves to automatically formulate and solve complex OR problems, bridging the gap between natural language problem descriptions and executable optimization models.</p><p style="margin:8px 0 0;">Key contributions in LLM inference scheduling include Nested WAIT (Paper 1) for multi-stage online scheduling, achieving superior throughput and reduced latency on vLLM/Sarathi. Staggered Batch Scheduling (SBS) (Paper 2) for DP+EP architectures reduced Time-to-First-Token by 30-40% and increased throughput by 15-20% on DeepSeek-V3. Memory Constrained Shortest First (MC-SF) (Paper 4) achieved near-optimal latency (within 5% of hindsight optimal) for KV cache-aware online scheduling. Justitia (Paper 6) introduced a virtual-time based fair scheduler, reducing average job completion time by ~60%. In automated OR modeling, OR-R1 (Paper 3) integrated SFT and Test-Time Group Relative Policy Optimization (TGRPO) to improve modeling accuracy by +4.2% over ORLM. ORLM (Paper 7) and the OR-Instruct framework fine-tuned LLMs with synthetic data, outperforming GPT-4 by up to 38.4% on benchmarks like NL4OPT and IndustryOR.</p><p style="margin:8px 0 0;">This front is emerging, with significant activity in both LLM inference optimization and automated OR modeling. The trajectory suggests continued innovation in developing more robust and adaptive scheduling policies for increasingly complex LLM architectures and deployment scenarios. For automated OR, the next papers will likely focus on enhancing LLM capabilities for solution ranking, handling more diverse and complex problem types, and integrating multi-agent collaboration for sophisticated problem-solving.</p></div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div>
    <h3 style="margin:0; font-size:16px; font-weight:700; color:#1F2937; letter-spacing:-0.01em;">Optimal LLM Inference Scheduling with Queueing Theory and MIP</h3>
  </div>
  <div style="margin-top:8px; line-height:1.8;">
    <span style="background:#0891B2; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700; text-transform:uppercase; display:inline-block;">EMERGING</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">Density: 0.53</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">6 papers</span>
  </div>
  <div style="margin-top:10px; line-height:1.8;"><span style="font-size:11px; color:#6B7280; font-weight:600; text-transform:uppercase; letter-spacing:0.02em; margin-right:6px;">Methods</span><span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">mixed_integer_programming</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">heuristic_search</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">continuous_batching</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">stochastic_control</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">queueing_network</span></div>
  <div style="margin-top:6px; font-size:10px; color:#6B7280;"><span style="font-weight:700; text-transform:uppercase; letter-spacing:0.02em; margin-right:4px;">Inst:</span>The Hong Kong. 33% &nbsp;·&nbsp; Huawei 33% &nbsp;·&nbsp; Harbin Institute of. 17% &nbsp;·&nbsp; Hong Kong Baptist. 17%</div>
  <div style="margin-top:10px; font-size:13px; line-height:1.6; color:#1F2937; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #4F46E5;"><p style="margin:0;">This research front unifies advanced Operations Research techniques, specifically many-server queueing network models, Mixed-Integer Programming (MIP), and stochastic control, to achieve optimal or near-optimal LLM inference scheduling and resource allocation. Key themes include managing prefill-decode contention, optimizing disaggregated expert parallelism in Mixture-of-Experts (MoE) models, and automatic operator-level parallelism planning for distributed deep learning.</p><p style="margin:8px 0 0;">Key contributions include Lin et al.'s (2026) 'Gate-and-Route' policy derived from a fluid LP, demonstrating ~30% lower revenue loss than OPT on Dolly-15k. She et al. (2025) used MIP for operator-level parallelism, reducing pipeline bubbles by 50% for DeepSeek V3. Bari et al. (2025) introduced RAD and SLAI schedulers, achieving a 53% reduction in median TTFT and 26% capacity increase over Sarathi-Serve on Mistral-7B. Pang et al. (2025) combined offline bin-packing with an online Lagrangian heuristic, improving utilization by 8.86% over vLLM FCFS on LLaMA-65B. FinDEP (2025) optimized MoE inference with fine-grained scheduling, yielding up to 1.61x throughput improvement on Qwen3-235B, while DREX (2025) showed 2-12% throughput gains on Llama-EE-70B using dynamic rebatching for early-exit LLMs.</p><p style="margin:8px 0 0;">This front is emerging, characterized by a strong trend towards rigorous mathematical modeling to solve complex LLM serving challenges, moving beyond heuristic-driven approaches. The next papers will likely focus on relaxing simplifying assumptions (e.g., exponential service times), integrating stochastic programming for uncertainty, and extending these optimal control strategies to more heterogeneous and dynamic LLM architectures, such as those with speculative decoding or Mixture-of-Depths.</p></div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div>
    <h3 style="margin:0; font-size:16px; font-weight:700; color:#1F2937; letter-spacing:-0.01em;">Linear Programming for MoE LLM Mixed-Precision Quantization and Pruning</h3>
  </div>
  <div style="margin-top:8px; line-height:1.8;">
    <span style="background:#0891B2; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700; text-transform:uppercase; display:inline-block;">EMERGING</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">Density: 1.00</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">2 papers</span>
  </div>
  <div style="margin-top:10px; line-height:1.8;"><span style="font-size:11px; color:#6B7280; font-weight:600; text-transform:uppercase; letter-spacing:0.02em; margin-right:6px;">Methods</span><span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">post_training_quantization</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">mixed_precision_quantization</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">linear_programming</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">gptq</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">expert_quantization</span></div>
  <div style="margin-top:6px; font-size:10px; color:#6B7280;"><span style="font-weight:700; text-transform:uppercase; letter-spacing:0.02em; margin-right:4px;">Inst:</span>The University of. 100% &nbsp;·&nbsp; Beihang University 100% &nbsp;·&nbsp; The Chinese University. 50% &nbsp;·&nbsp; Centre for Perceptual. 50%</div>
  <div style="margin-top:10px; font-size:13px; line-height:1.6; color:#1F2937; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #4F46E5;"><p style="margin:0;">This research front centers on the Mixture Compressor (MC and MC#) frameworks, which leverage Operations Research techniques, specifically Linear Programming, for the extreme compression of Mixture-of-Experts (MoE) Large Language Models. The core theme involves optimizing mixed-precision quantization and dynamic expert pruning to significantly reduce model size while preserving performance, enabling deployment on resource-constrained hardware.</p><p style="margin:8px 0 0;">The initial paper, "Mixture Compressor for Mixture-of-Experts LLMs Gains More" (Huang et al., 2025-02), introduces a hybrid post-training quantization and dynamic pruning approach. It uses Linear Programming to optimally allocate mixed bit-widths (1-3 bits) to experts based on activation frequency, achieving strong empirical results such as compressing Mixtral 8x7b to ~16GB with only a ~4% drop in zero-shot accuracy, outperforming uniform GPTQ. Building on this, "MC#: Mixture Compressor for Mixture-of-Experts Large Models" (Huang et al., 2025-10) refines the framework by combining Pre-Loading Mixed-Precision Quantization (PMQ) via Linear Programming with Online Top-any Pruning (OTP) using Gumbel-Softmax sampling. This unified approach achieves a 6.2x weight reduction on DeepSeek-VL2 with less than 2% accuracy loss, further demonstrating the efficacy of OR-driven compression.</p><p style="margin:8px 0 0;">This front is currently emerging, with two closely related papers published in 2025, the second building directly on the first. The trajectory suggests a continued focus on refining these OR-driven compression techniques. The likely next paper would explore the adaptation of these methods to new model architectures or more challenging, complex reasoning tasks, while also addressing hardware-specific optimizations.</p></div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div>
    <h3 style="margin:0; font-size:16px; font-weight:700; color:#1F2937; letter-spacing:-0.01em;">Integer Linear Programming for Heterogeneous LLM Serving Resource Allocation</h3>
  </div>
  <div style="margin-top:8px; line-height:1.8;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700; text-transform:uppercase; display:inline-block;">GROWING</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">Density: 0.71</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">10 papers</span>
  </div>
  <div style="margin-top:10px; line-height:1.8;"><span style="font-size:11px; color:#6B7280; font-weight:600; text-transform:uppercase; letter-spacing:0.02em; margin-right:6px;">Methods</span><span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">integer_linear_programming</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">pipeline_parallelism</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">data_parallelism</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">tensor_parallelism</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">performance_modeling</span></div>
  <div style="margin-top:6px; font-size:10px; color:#6B7280;"><span style="font-weight:700; text-transform:uppercase; letter-spacing:0.02em; margin-right:4px;">Inst:</span>University of Cambridge 30% &nbsp;·&nbsp; Peking University 20% &nbsp;·&nbsp; Purdue University 20% &nbsp;·&nbsp; Indian Institute of. 20%</div>
  <div style="margin-top:10px; font-size:13px; line-height:1.6; color:#1F2937; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #4F46E5;"><p style="margin:0;">This research front is unified by the application of Integer Linear Programming (ILP) and Mixed-Integer Linear Programming (MILP) to optimize various aspects of Large Language Model (LLM) serving. The core theme revolves around efficient resource allocation, scheduling, and deployment strategies for LLM inference, particularly addressing challenges posed by heterogeneous GPU clusters, disaggregated serving architectures, and geographically distributed systems. Specific problem domains include multi-round inference, Mixture-of-Expert (MoE) model placement, cascade serving, and carbon-aware caching.</p><p style="margin:8px 0 0;">Key contributions include Dynamo's ILP-based offline deployment for multi-round inference, achieving up to 340% SLO attainment improvement (Paper 1). Jiang et al. (Paper 3) demonstrated ~25% throughput gains over SOTA systems like Helix by co-optimizing GPU composition and parallelism using MILP for heterogeneous clouds. CASCADIA (Paper 5) introduced a bi-level optimization (MILP for deployment, Chebyshev for routing) for cascade serving, yielding 2.3x average throughput gains. SageServe (Paper 6) achieved 25% GPU-hours savings and $2.5M/month savings by coupling ILP with ARIMA forecasting for auto-scaling. Helix (Paper 10) formulated distributed LLM serving as a max-flow MILP, achieving up to 3.3x throughput gains on mixed GPU clusters by dynamic per-request routing. Other notable works include ILP for MoE expert placement (Paper 4), carbon-aware KV cache management (Paper 9), and hybrid RL-Optimal Transport for temporal-aware GPU allocation (Paper 7).</p><p style="margin:8px 0 0;">This front is rapidly maturing, driven by the increasing complexity and scale of LLM deployments. The consistent success of ILP/MILP in achieving significant performance, cost, and energy efficiency gains across diverse LLM serving scenarios indicates a strong and active research trajectory. Future work will likely focus on developing more scalable and dynamic OR solutions, integrating these with advanced LLM-specific optimizations, and expanding to multi-objective and real-time adaptive systems.</p></div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div>
    <h3 style="margin:0; font-size:16px; font-weight:700; color:#1F2937; letter-spacing:-0.01em;">OR-Driven Adaptive Resource Allocation for LLM Inference and Test-Time Search</h3>
  </div>
  <div style="margin-top:8px; line-height:1.8;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700; text-transform:uppercase; display:inline-block;">GROWING</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">Density: 0.47</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">6 papers</span>
  </div>
  <div style="margin-top:10px; line-height:1.8;"><span style="font-size:11px; color:#6B7280; font-weight:600; text-transform:uppercase; letter-spacing:0.02em; margin-right:6px;">Methods</span><span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">llm_in_the_loop</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">resource_allocation</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">llm_as_heuristic</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">rebase</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">process_reward_model</span></div>
  <div style="margin-top:6px; font-size:10px; color:#6B7280;"><span style="font-weight:700; text-transform:uppercase; letter-spacing:0.02em; margin-right:4px;">Inst:</span>Mohamed bin Zayed. 17% &nbsp;·&nbsp; New York University 17% &nbsp;·&nbsp; RIKEN AIP 17% &nbsp;·&nbsp; Institute of Science. 17%</div>
  <div style="margin-top:10px; font-size:13px; line-height:1.6; color:#1F2937; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #4F46E5;"><p style="margin:0;">This research front focuses on applying advanced Operations Research techniques to optimize various aspects of Large Language Model (LLM) inference and serving. Key approaches include Bayesian adaptive stopping for efficient ensemble evaluation, gradient-based scheduling in GoodSpeed for distributed speculative decoding, PROBE's predictive Lookahead Pipelining for MoE inference, and ETS's Integer Linear Programming for KV cache optimization in tree search. Additionally, DORA employs embedding-based resource allocation for test-time search, and VectorLiteRAG uses analytical modeling for RAG serving.</p><p style="margin:8px 0 0;">Contributions include Bayesian adaptive sampling (2-5x compute reduction on AIME/GPQA) and MILP for LLM ensembles. GoodSpeed demonstrates gradient-based scheduling for distributed speculative decoding, achieving fair goodput on H100/L4 clusters. PROBE's Lookahead Pipelining yields a 1.3x speedup for Qwen3-MoE-235B inference, while ETS leverages ILP to achieve 1.8x KV cache reduction and 1.4x throughput increase on MATH500 compared to REBASE. VectorLiteRAG provides 1.5x throughput gains for RAG serving on H100/L40S, and DORA achieves state-of-the-art accuracy on MATH500 with 3.5x fewer FLOPs by optimizing test-time search.</p><p style="margin:8px 0 0;">This front is rapidly maturing, characterized by the increasing sophistication of OR methods integrated directly into LLM serving and reasoning pipelines. The trajectory points towards more tightly coupled, real-time optimization, where OR solvers dynamically adapt resource allocation and search strategies. Future work will likely focus on developing unified frameworks that combine predictive modeling, adaptive sampling, and advanced combinatorial optimization to handle the stochastic and dynamic nature of LLM workloads across diverse hardware architectures and reasoning tasks.</p></div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div>
    <h3 style="margin:0; font-size:16px; font-weight:700; color:#1F2937; letter-spacing:-0.01em;">Convex Optimization and Game Theory for Robust Multi-Objective LLM Alignment</h3>
  </div>
  <div style="margin-top:8px; line-height:1.8;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700; text-transform:uppercase; display:inline-block;">GROWING</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">Density: 1.00</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">3 papers</span>
  </div>
  <div style="margin-top:10px; line-height:1.8;"><span style="font-size:11px; color:#6B7280; font-weight:600; text-transform:uppercase; letter-spacing:0.02em; margin-right:6px;">Methods</span><span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">multi_objective_optimization</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">convex_optimization</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">gradient_descent</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">game_theory</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">llm_as_evaluator</span></div>
  <div style="margin-top:6px; font-size:10px; color:#6B7280;"><span style="font-weight:700; text-transform:uppercase; letter-spacing:0.02em; margin-right:4px;">Inst:</span>Ruhr University Bochum 33% &nbsp;·&nbsp; University College London 33% &nbsp;·&nbsp; University of Basel 33% &nbsp;·&nbsp; Ulsan National Institute. 33%</div>
  <div style="margin-top:10px; font-size:13px; line-height:1.6; color:#1F2937; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #4F46E5;"><p style="margin:0;">This research front unifies recent advancements in applying Operations Research techniques, specifically convex optimization and game theory, to the challenging problem of multi-objective alignment for Large Language Models (LLMs). Papers introduce frameworks like PAMA, Safety Game, and Robust Multi-Objective Decoding (RMOD) to manage conflicting objectives such as harmlessness, helpfulness, sentiment, and length control, often at inference time.</p><p style="margin:8px 0 0;">Key contributions include the PAMA algorithm, which transforms multi-objective RLHF into an O(n) convex optimization problem with a closed-form solution, outperforming MORLHF and MGDA-UB on LLaMA-2 7B for harmlessness. The Safety Game formulates black-box LLM agent alignment as a zero-sum game solvable by an LP solver at inference, achieving up to two-fold accuracy improvement on SafetyBench. RMOD introduces a maximin two-player game for robust multi-objective decoding, solving a convex optimization problem at each step to maximize worst-case value, outperforming MO-DPO and scalarized baselines by +1.2% WCWR on Anthropic HH.</p><p style="margin:8px 0 0;">This front is rapidly growing, demonstrating the power of OR principles to bring robustness and efficiency to LLM alignment. The trajectory indicates a strong focus on mathematically grounded, inference-time control mechanisms. Future work will likely focus on extending these frameworks to more complex, dynamic, and multi-agent scenarios, improving their scalability to a greater number of objectives, and integrating these control mechanisms into broader agentic architectures.</p></div>
</td></tr><tr><td style="padding:20px 24px 12px; border-top:2px solid #F8F9FB; background:#FAFBFC;">
  <h2 style="margin:0; font-size:15px; font-weight:700; color:#1F2937; text-transform:uppercase; letter-spacing:0.03em;">Framework Genealogy</h2>
  <p style="margin:4px 0 0; font-size:11px; color:#6B7280; font-weight:500;">Tracking research lineages and framework evolution</p>
</td></tr><tr><td style="padding:0;">
<div style="padding:12px 24px; background:#F8F9FB; border-radius:12px 12px 0 0; border:1px solid #E1E4E8; border-bottom:none;">
  <div style="font-size:11px; color:#1F2937;">
    <b>38</b> frameworks tracked · <b>36</b> root frameworks · <b>9</b> active (last 30 days)
  </div>
</div>
<div style="padding:16px 24px; background:#FFFFFF; text-align:center; border:1px solid #E1E4E8; border-top:none; border-radius:0 0 12px 12px;">
  <div style="font-size:10px; margin-bottom:10px; color:#6B7280; text-transform:uppercase; letter-spacing:0.03em;">Framework landscape (size = paper count, color = must-read ratio)</div>
  <div style="font-size:12px;">sarathi_serve (2 papers, 2 must-read) • rebase (2 papers, 2 must-read) • dynamo (1 papers, 1 must-read) • grpo (1 papers, 1 must-read) • mlora (1 papers, 0 must-read) • aflow (1 papers, 1 must-read) • many_server_queueing_theory (1 papers, 1 must-read) • sglang (1 papers, 1 must-read) • adaptdl (1 papers, 0 must-read) • lmcache (1 papers, 0 must-read)</div>
  <div style="margin-top:10px; font-size:9px; color:#6B7280;">
    <span style="color:#1A5F7A;">■</span> Active + Must-read &nbsp;
    <span style="color:#64B5CD;">■</span> Active &nbsp;
    <span style="color:#2E7D32;">■</span> Inactive + Must-read &nbsp;
    <span style="color:#A5D6A7;">■</span> Inactive
  </div>
</div>
</td></tr></table>
</td></tr>
<tr><td style="background:#FAFBFC; padding:20px 24px; border-top:1px solid #E1E4E8; border-radius:0 0 16px 16px; text-align:center;">
<p style="margin:0 0 12px; font-size:11px; color:#6B7280; font-weight:500;">Curated by Research Intelligence System</p>
<table cellpadding="0" cellspacing="0" role="presentation" style="margin:0 auto;">
<tr>
<td style="border-radius:12px; background:#2563EB;">
<a href="#" style="display:inline-block; padding:10px 20px; font-size:12px; font-weight:700; color:#FFFFFF; text-decoration:none; border-radius:12px;">View Full Archive →</a>
</td>
</tr>
</table>
</td></tr>
</table>
</td></tr>
</table>
</body>
</html>