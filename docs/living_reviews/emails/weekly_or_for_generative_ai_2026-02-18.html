<!DOCTYPE html>
<html>
<head><meta charset="utf-8"><title>Weekly Research Intelligence — OR for Generative AI</title>
<style>
/* Gmail-compatible toggle: checkbox + sibling selector */
.rd-cb { display:none; }
.rd-content { max-height:0; overflow:hidden; transition:max-height .2s ease; }
.rd-cb:checked ~ .rd-content { max-height:9999px; }
</style>
</head>
<body style="margin:0; padding:0; background:#F5F7FA; font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif; color:#1F2937; font-size:14px; line-height:1.5;">
<div style="display:none; max-height:0; overflow:hidden; mso-hide:all; opacity:0; color:transparent;">Research Intelligence curated digest — Latest papers, research fronts, and framework evolution.</div>
<table width="100%" cellpadding="0" cellspacing="0" style="background:#F5F7FA;">
<tr><td align="center" style="padding:0;">
<table width="85%" cellpadding="0" cellspacing="0" style="width:85%;">

<tr><td style="background:linear-gradient(135deg, #1E3A8A 0%, #2563EB 100%); color:white; padding:20px 24px; border-radius:16px 16px 0 0;">
<table width="100%" cellpadding="0" cellspacing="0">
<tr>
<td style="width:48px; vertical-align:top;">
<div style="width:44px; height:44px; background:rgba(255,255,255,0.2); border-radius:12px; display:flex; align-items:center; justify-content:center; border:2px solid rgba(255,255,255,0.3);">
<span style="font-size:18px; font-weight:800; color:white; line-height:44px; text-align:center; display:block;">RI</span>
</div>
</td>
<td style="padding-left:14px; vertical-align:top;">
<h1 style="margin:0; font-size:20px; font-weight:700; letter-spacing:-0.01em; line-height:1.2;">Weekly Research Intelligence — OR for Generative AI</h1>
<p style="margin:4px 0 0; font-size:11px; opacity:0.85; font-weight:500;">Issue #8 of 2026 · 2026-02-18</p>
</td>
</tr>
</table>
</td></tr>
<tr><td style="background:#FFFFFF; padding:0;">
<table width="100%" cellpadding="0" cellspacing="0"><tr><td style="background:#FFFFFF; padding:20px 24px 12px;">
  <table width="100%" cellpadding="0" cellspacing="0" style="border:1px solid #E1E4E8; border-radius:14px; background:#FAFBFC;">
    <tr>
      <td style="padding:14px 18px; font-size:12px; color:#1F2937; font-weight:700; text-transform:uppercase; letter-spacing:0.03em; border-bottom:1px solid #E1E4E8;">
        This week at a glance
      </td>
    </tr>
    <tr>
      <td style="padding:0 18px 14px;">
        <table width="100%" cellpadding="0" cellspacing="0">
          <tr>
            <td style="width:33%; padding:12px 0; text-align:center;">
              <div style="font-size:24px; font-weight:800; color:#2563EB;">31</div>
              <div style="font-size:11px; color:#6B7280; text-transform:uppercase; letter-spacing:0.02em;">Must-reads</div>
            </td>
            <td style="width:33%; padding:12px 0; text-align:center; border-left:1px solid #E1E4E8; border-right:1px solid #E1E4E8;">
              <div style="font-size:24px; font-weight:800; color:#2563EB;">49</div>
              <div style="font-size:11px; color:#6B7280; text-transform:uppercase; letter-spacing:0.02em;">New papers</div>
            </td>
            <td style="width:33%; padding:12px 0; text-align:center;">
              <div style="font-size:24px; font-weight:800; color:#2563EB;">5</div>
              <div style="font-size:11px; color:#6B7280; text-transform:uppercase; letter-spacing:0.02em;">Active fronts</div>
            </td>
          </tr>
        </table>
        <div style="margin-top:10px; padding-top:10px; border-top:1px solid #E1E4E8; font-size:12px; color:#1F2937;">
          <b>This week's theme:</b> Concept-structured search is outperforming brute code mutation across multiple optimization domains.
        </div>
      </td>
    </tr>
  </table>
</td></tr><tr><td style="padding:20px 24px 12px; border-top:2px solid #F8F9FB; background:#FAFBFC;">
  <h2 style="margin:0; font-size:15px; font-weight:700; color:#1F2937; text-transform:uppercase; letter-spacing:0.03em;">Top Priority Papers</h2>
  <p style="margin:4px 0 0; font-size:11px; color:#6B7280; font-weight:500;">10 must-read papers this week (ranked by significance, recency, and impact)</p>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div style="margin-bottom:8px;">
    <span style="display:inline-block; background:#2563EB; color:white; padding:2px 10px; border-radius:12px; font-size:11px; font-weight:700; margin-right:8px;">#1</span>
    <a href="https://arxiv.org/abs/2502.07115" style="color:#1F2937; text-decoration:none; font-weight:700; font-size:16px; letter-spacing:-0.01em;">Online Scheduling for LLM Inference with KV Cache Constraints</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700;">PRIORITY 8.4/10</span> <span style="background:#DC2626; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600;">MUST-READ</span>
  </div>
  <div style="margin-bottom:6px; font-size:11px; color:#6B7280;">
    2026-01-15 | Massachusetts Institute of Technology, Microsoft Research, HKUST | <a href="https://arxiv.org/abs/2502.07115" style="color:#6B7280; text-decoration:none;">2502.07115</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">M=8</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">P=10</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px;">I=8</span>
  </div>
  <div style="font-size:13px; color:#1F2937; line-height:1.6; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #2563EB;">
    This paper formulates LLM inference scheduling as an Integer Program (IP) that explicitly models the linear memory growth of KV caches, and proposes a 'Memory Constrained Shortest First' (MC-SF) algorithm. The results are rigorous, showing MC-SF achi...<input type="checkbox" id="rd-2502-07115" class="rd-cb"><label for="rd-2502-07115" style="color:#2563EB; cursor:pointer; font-weight:600; font-size:12px; display:block; margin-top:6px;">▶ Read more</label><div class="rd-content" style="padding-top:8px; border-top:1px solid #E1E4E8; margin-top:6px;">This paper formulates LLM inference scheduling as an Integer Program (IP) that explicitly models the linear memory growth of KV caches, and proposes a 'Memory Constrained Shortest First' (MC-SF) algorithm. The results are rigorous, showing MC-SF achieves near-optimal performance (within 5% of hindsight optimal) on synthetic data and significantly outperforms standard FCFS/threshold heuristics on real traces. The critical takeaway is the 'future feasibility check' (Eq. 5), which validates that a batch will <em>remain</em> within memory limits throughout the generation process based on predicted output lengths—a necessary deviation from standard static-size scheduling. This is foundational reading for our GPUSched project, providing both the exact IP baseline we need and a strong heuristic to benchmark against.</div>
  </div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div style="margin-bottom:8px;">
    <span style="display:inline-block; background:#2563EB; color:white; padding:2px 10px; border-radius:12px; font-size:11px; font-weight:700; margin-right:8px;">#2</span>
    <a href="https://arxiv.org/abs/2504.11320" style="color:#1F2937; text-decoration:none; font-weight:700; font-size:16px; letter-spacing:-0.01em;">Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700;">PRIORITY 8.4/10</span> <span style="background:#DC2626; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600;">MUST-READ</span>
  </div>
  <div style="margin-bottom:6px; font-size:11px; color:#6B7280;">
    2026-01-05 | Massachusetts Institute of Technology, Peking University, Alibaba Group | <a href="https://arxiv.org/abs/2504.11320" style="color:#6B7280; text-decoration:none;">2504.11320</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">M=8</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">P=10</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px;">I=8</span>
  </div>
  <div style="font-size:13px; color:#1F2937; line-height:1.6; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #2563EB;">
    This paper formulates LLM inference as a multi-stage stochastic scheduling problem, introducing 'Nested WAIT'—a threshold-based algorithm that handles unknown output lengths by letting prompts classify themselves as they survive into deeper decode se...<input type="checkbox" id="rd-2504-11320" class="rd-cb"><label for="rd-2504-11320" style="color:#2563EB; cursor:pointer; font-weight:600; font-size:12px; display:block; margin-top:6px;">▶ Read more</label><div class="rd-content" style="padding-top:8px; border-top:1px solid #E1E4E8; margin-top:6px;">This paper formulates LLM inference as a multi-stage stochastic scheduling problem, introducing 'Nested WAIT'—a threshold-based algorithm that handles unknown output lengths by letting prompts classify themselves as they survive into deeper decode segments. Unlike heuristic baselines (vLLM, Sarathi), they provide rigorous asymptotic optimality proofs and high-probability bounds against memory overflow, validated on A100 simulations. The key takeaway is the 'nested segment' mechanism: instead of predicting job size, structure the queue so short jobs exit early and long jobs naturally migrate to lower-priority/protected tiers, effectively decoupling the memory risk. We should immediately evaluate this threshold logic for our GPUSched formulations, as it likely outperforms our current predictive or FCFS approaches for handling KV cache growth.</div>
  </div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div style="margin-bottom:8px;">
    <span style="display:inline-block; background:#2563EB; color:white; padding:2px 10px; border-radius:12px; font-size:11px; font-weight:700; margin-right:8px;">#3</span>
    <a href="https://arxiv.org/abs/2601.21008" style="color:#1F2937; text-decoration:none; font-weight:700; font-size:16px; letter-spacing:-0.01em;">Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and Behavioral Rationality in Operations Research</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700;">PRIORITY 8.3/10</span> <span style="background:#DC2626; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600;">MUST-READ</span>
  </div>
  <div style="margin-bottom:6px; font-size:11px; color:#6B7280;">
    2026-02-08 | Massachusetts Institute of Technology, Alibaba Group | <a href="https://arxiv.org/abs/2601.21008" style="color:#6B7280; text-decoration:none;">2601.21008</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">M=9</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">P=8</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px;">I=9</span>
  </div>
  <div style="font-size:13px; color:#1F2937; line-height:1.6; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #2563EB;">
    Ao et al. introduce a framework for iterative OR model debugging that trains an 8B model using Group Relative Policy Optimization (GRPO) and a Process Reward Model (PRM) to outperform GPT-4o-mini. They utilize Gurobi's Irreducible Infeasible Subsyste...<input type="checkbox" id="rd-2601-21008" class="rd-cb"><label for="rd-2601-21008" style="color:#2563EB; cursor:pointer; font-weight:600; font-size:12px; display:block; margin-top:6px;">▶ Read more</label><div class="rd-content" style="padding-top:8px; border-top:1px solid #E1E4E8; margin-top:6px;">Ao et al. introduce a framework for iterative OR model debugging that trains an 8B model using Group Relative Policy Optimization (GRPO) and a Process Reward Model (PRM) to outperform GPT-4o-mini. They utilize Gurobi's Irreducible Infeasible Subsystem (IIS) not just as text feedback, but as a dense reward signal (IIS size reduction) for the PRM, achieving a 95.3% recovery rate versus 86.2% for frontier APIs. <strong>Key Takeaway:</strong> We should steal their PRM construction method—specifically using solver diagnostics (like IIS reduction or compiler error counts) as dense step-level rewards—and their 'faithfulness penalty' to prevent overfitting in our evolutionary search. This is a direct validation of RLVR (Reinforcement Learning with Verifiable Rewards) for OR, proving it superior to large-scale prompting.</div>
  </div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div style="margin-bottom:8px;">
    <span style="display:inline-block; background:#2563EB; color:white; padding:2px 10px; border-radius:12px; font-size:11px; font-weight:700; margin-right:8px;">#4</span>
    <a href="https://arxiv.org/abs/2602.02987" style="color:#1F2937; text-decoration:none; font-weight:700; font-size:16px; letter-spacing:-0.01em;">Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700;">PRIORITY 8.2/10</span> <span style="background:#DC2626; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600;">MUST-READ</span>
  </div>
  <div style="margin-bottom:6px; font-size:11px; color:#6B7280;">
    2026-02-03 | The Hong Kong University of Science and Technology | <a href="https://arxiv.org/abs/2602.02987" style="color:#6B7280; text-decoration:none;">2602.02987</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">M=8</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">P=9</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px;">I=7</span>
  </div>
  <div style="font-size:13px; color:#1F2937; line-height:1.6; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #2563EB;">
    Lin et al. formulate LLM inference scheduling as a multiclass many-server queueing network, deriving a 'Gate-and-Route' policy from a steady-state fluid LP that explicitly manages prefill-decode contention. Calibrated on A100s, their approach proves ...<input type="checkbox" id="rd-2602-02987" class="rd-cb"><label for="rd-2602-02987" style="color:#2563EB; cursor:pointer; font-weight:600; font-size:12px; display:block; margin-top:6px;">▶ Read more</label><div class="rd-content" style="padding-top:8px; border-top:1px solid #E1E4E8; margin-top:6px;">Lin et al. formulate LLM inference scheduling as a multiclass many-server queueing network, deriving a 'Gate-and-Route' policy from a steady-state fluid LP that explicitly manages prefill-decode contention. Calibrated on A100s, their approach proves that separating prefill admission (via occupancy tracking) from decode routing (work-conserving) eliminates decode backlogs and maximizes revenue. <strong>Key Takeaway:</strong> The decomposition of scheduling into 'static planning' (solving an LP for target occupancies) and 'dynamic control' (a simple gate tracking those targets) is a scalable alternative to online combinatorial optimization for your GPUSched work. It mathematically formalizes the intuition that prefill is the bottleneck and decode should be kept strictly critical but not backlogged.</div>
  </div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div style="margin-bottom:8px;">
    <span style="display:inline-block; background:#2563EB; color:white; padding:2px 10px; border-radius:12px; font-size:11px; font-weight:700; margin-right:8px;">#5</span>
    <a href="https://arxiv.org/abs/2602.07663" style="color:#1F2937; text-decoration:none; font-weight:700; font-size:16px; letter-spacing:-0.01em;">A Two-Layer Framework for Joint Online Configuration Selection and Admission Control</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700;">PRIORITY 8.1/10</span> <span style="background:#DC2626; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600;">MUST-READ</span>
  </div>
  <div style="margin-bottom:6px; font-size:11px; color:#6B7280;">
    2026-02-07 | Massachusetts Institute of Technology, Stanford University | <a href="https://arxiv.org/abs/2602.07663" style="color:#6B7280; text-decoration:none;">2602.07663</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">M=8</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">P=9</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px;">I=8</span>
  </div>
  <div style="font-size:13px; color:#1F2937; line-height:1.6; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #2563EB;">
    The authors introduce a 'switching-aware' primal-dual framework for joint configuration selection (e.g., quantization, parallelism) and admission control, demonstrating that dynamically mixing configurations allows for higher resource utilization tha...<input type="checkbox" id="rd-2602-07663" class="rd-cb"><label for="rd-2602-07663" style="color:#2563EB; cursor:pointer; font-weight:600; font-size:12px; display:block; margin-top:6px;">▶ Read more</label><div class="rd-content" style="padding-top:8px; border-top:1px solid #E1E4E8; margin-top:6px;">The authors introduce a 'switching-aware' primal-dual framework for joint configuration selection (e.g., quantization, parallelism) and admission control, demonstrating that dynamically mixing configurations allows for higher resource utilization than any single fixed configuration. Results are rigorous, backed by $\tilde{O}(\sqrt{T})$ regret bounds and experiments on Alibaba cluster traces where the method achieves ~97% competitive ratio (vs. ~85% for greedy). The key takeaway is the 'switching-aware fluid oracle' concept: our resource allocation models for LLM serving must optimize over the convex hull of configurations (mixing CPU-heavy and Mem-heavy setups) rather than searching for a single static optimum. We should adapt their saddle-point formulation for the GPUSched project to handle heterogeneous resource constraints more effectively.</div>
  </div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div style="margin-bottom:8px;">
    <span style="display:inline-block; background:#2563EB; color:white; padding:2px 10px; border-radius:12px; font-size:11px; font-weight:700; margin-right:8px;">#6</span>
    <a href="https://arxiv.org/abs/2512.18134" style="color:#1F2937; text-decoration:none; font-weight:700; font-size:16px; letter-spacing:-0.01em;">Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700;">PRIORITY 8.1/10</span> <span style="background:#DC2626; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600;">MUST-READ</span>
  </div>
  <div style="margin-bottom:6px; font-size:11px; color:#6B7280;">
    2025-12-19 | Stanford University, NVIDIA | <a href="https://arxiv.org/abs/2512.18134" style="color:#6B7280; text-decoration:none;">2512.18134</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">M=8</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">P=9</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px;">I=8</span>
  </div>
  <div style="font-size:13px; color:#1F2937; line-height:1.6; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #2563EB;">
    Twill formulates the complex interplay of software pipelining and warp specialization on modern GPUs (Hopper/Blackwell) as a joint SMT/ILP optimization problem, automatically rediscovering expert-tuned Flash Attention schedules without heuristics. Th...<input type="checkbox" id="rd-2512-18134" class="rd-cb"><label for="rd-2512-18134" style="color:#2563EB; cursor:pointer; font-weight:600; font-size:12px; display:block; margin-top:6px;">▶ Read more</label><div class="rd-content" style="padding-top:8px; border-top:1px solid #E1E4E8; margin-top:6px;">Twill formulates the complex interplay of software pipelining and warp specialization on modern GPUs (Hopper/Blackwell) as a joint SMT/ILP optimization problem, automatically rediscovering expert-tuned Flash Attention schedules without heuristics. The results are rigorous, matching hand-tuned performance within 1-2% and handling new hardware constraints (Blackwell TMEM) automatically. The key takeaway is the 'cost normalization' technique via ILP to make the scheduling search space tractable, and the demonstration that exact constraint solvers can replace human intuition for complex kernel generation. This is essential reading for your work on OR formulations for GPU scheduling and LLM serving optimization, offering a deterministic baseline to compare against evolutionary approaches.</div>
  </div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div style="margin-bottom:8px;">
    <span style="display:inline-block; background:#2563EB; color:white; padding:2px 10px; border-radius:12px; font-size:11px; font-weight:700; margin-right:8px;">#7</span>
    <a href="https://arxiv.org/abs/2507.15615" style="color:#1F2937; text-decoration:none; font-weight:700; font-size:16px; letter-spacing:-0.01em;">DHEvo: Data-Algorithm Based Heuristic Evolution for Generalizable MILP Solving</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700;">PRIORITY 8.1/10</span> <span style="background:#DC2626; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600;">MUST-READ</span>
  </div>
  <div style="margin-bottom:6px; font-size:11px; color:#6B7280;">
    2025-07-21 | Harbin Institute of Technology, Huawei Noah’s Ark Lab, Nanyang Technological University | <a href="https://arxiv.org/abs/2507.15615" style="color:#6B7280; text-decoration:none;">2507.15615</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">M=8</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">P=9</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px;">I=8</span>
  </div>
  <div style="font-size:13px; color:#1F2937; line-height:1.6; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #2563EB;">
    DHEvo introduces a 'data-algorithm co-evolution' framework that iteratively evolves heuristic code while simultaneously filtering the training instance set to retain only 'representative' instances (those where current heuristics perform well/stably)...<input type="checkbox" id="rd-2507-15615" class="rd-cb"><label for="rd-2507-15615" style="color:#2563EB; cursor:pointer; font-weight:600; font-size:12px; display:block; margin-top:6px;">▶ Read more</label><div class="rd-content" style="padding-top:8px; border-top:1px solid #E1E4E8; margin-top:6px;">DHEvo introduces a 'data-algorithm co-evolution' framework that iteratively evolves heuristic code while simultaneously filtering the training instance set to retain only 'representative' instances (those where current heuristics perform well/stably). Empirical results on SCIP diving heuristics show it outperforms FunSearch and EoH by ~60% on Setcover while significantly reducing performance variance, validating the claim that dynamic data curation prevents overfitting. The key takeaway is the counter-intuitive curriculum strategy: rather than training on the hardest instances, filtering for instances with 'regular' feasible regions (high fitness) stabilizes the evolutionary search for code. We should immediately test this dynamic instance filtering in AlgoEvo to improve sample efficiency and generalization.</div>
  </div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div style="margin-bottom:8px;">
    <span style="display:inline-block; background:#2563EB; color:white; padding:2px 10px; border-radius:12px; font-size:11px; font-weight:700; margin-right:8px;">#8</span>
    <a href="https://arxiv.org/abs/2406.01566" style="color:#1F2937; text-decoration:none; font-weight:700; font-size:16px; letter-spacing:-0.01em;">Helix: Serving Large Language Models over Heterogeneous GPUs and Network via Max-Flow</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700;">PRIORITY 8.1/10</span> <span style="background:#DC2626; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600;">MUST-READ</span>
  </div>
  <div style="margin-bottom:6px; font-size:11px; color:#6B7280;">
    2025-03-05 | Carnegie Mellon University | <a href="https://arxiv.org/abs/2406.01566" style="color:#6B7280; text-decoration:none;">2406.01566</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">M=8</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">P=9</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px;">I=8</span>
  </div>
  <div style="font-size:13px; color:#1F2937; line-height:1.6; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #2563EB;">
    Helix formulates distributed LLM serving on heterogeneous clusters as a max-flow problem, using MILP to optimize model placement and deriving a per-request weighted round-robin scheduler from the flow solution. Unlike standard static pipeline paralle...<input type="checkbox" id="rd-2406-01566" class="rd-cb"><label for="rd-2406-01566" style="color:#2563EB; cursor:pointer; font-weight:600; font-size:12px; display:block; margin-top:6px;">▶ Read more</label><div class="rd-content" style="padding-top:8px; border-top:1px solid #E1E4E8; margin-top:6px;">Helix formulates distributed LLM serving on heterogeneous clusters as a max-flow problem, using MILP to optimize model placement and deriving a per-request weighted round-robin scheduler from the flow solution. Unlike standard static pipeline parallelism, it routes every request dynamically based on edge capacities, achieving up to 3.3x throughput gains over Swarm on mixed GPU clusters (L4/T4/A100). The results are rigorous, backed by both physical cluster experiments and high-fidelity simulations. The critical takeaway is the 'per-request pipeline' abstraction: decoupling request routing from static device assignment allows exact OR methods to maximize utilization of weaker hardware—a technique we should immediately evaluate for our GPUSched project.</div>
  </div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div style="margin-bottom:8px;">
    <span style="display:inline-block; background:#2563EB; color:white; padding:2px 10px; border-radius:12px; font-size:11px; font-weight:700; margin-right:8px;">#9</span>
    <a href="https://arxiv.org/abs/2511.15898" style="color:#1F2937; text-decoration:none; font-weight:700; font-size:16px; letter-spacing:-0.01em;">Global Resolution: Optimal Multi-Draft Speculative Sampling via Convex Minimization</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700;">PRIORITY 8.0/10</span> <span style="background:#DC2626; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600;">MUST-READ</span>
  </div>
  <div style="margin-bottom:6px; font-size:11px; color:#6B7280;">
    2025-11-19 | Stanford University, Ritual | <a href="https://arxiv.org/abs/2511.15898" style="color:#6B7280; text-decoration:none;">2511.15898</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">M=9</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">P=8</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px;">I=7</span>
  </div>
  <div style="font-size:13px; color:#1F2937; line-height:1.6; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #2563EB;">
    The authors solve the Optimal Transport Linear Program (OTLP) for multi-draft speculative sampling by reducing it to a convex minimization problem using polymatroid theory and max-flow, rather than using slow general LP solvers. They prove this 'Glob...<input type="checkbox" id="rd-2511-15898" class="rd-cb"><label for="rd-2511-15898" style="color:#2563EB; cursor:pointer; font-weight:600; font-size:12px; display:block; margin-top:6px;">▶ Read more</label><div class="rd-content" style="padding-top:8px; border-top:1px solid #E1E4E8; margin-top:6px;">The authors solve the Optimal Transport Linear Program (OTLP) for multi-draft speculative sampling by reducing it to a convex minimization problem using polymatroid theory and max-flow, rather than using slow general LP solvers. They prove this 'Global Resolution' algorithm is exact for i.i.d. drafts and achieves >90% acceptance with negligible overhead (<100ms), running 10,000x faster than baselines. <strong>Key Takeaway:</strong> The reduction of a discrete token selection problem to a convex optimization problem via polymatroids is a brilliant theoretical trick we could potentially adapt for selecting diverse solution subsets in AlgoEvo. This is a definitive 'OR for LLM infra' paper that obsoletes heuristic verification strategies.</div>
  </div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div style="margin-bottom:8px;">
    <span style="display:inline-block; background:#2563EB; color:white; padding:2px 10px; border-radius:12px; font-size:11px; font-weight:700; margin-right:8px;">#10</span>
    <a href="https://arxiv.org/abs/2602.04431" style="color:#1F2937; text-decoration:none; font-weight:700; font-size:16px; letter-spacing:-0.01em;">MaMa: A Game-Theoretic Approach for Designing Safe Agentic Systems</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700;">PRIORITY 7.9/10</span> <span style="background:#DC2626; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600;">MUST-READ</span>
  </div>
  <div style="margin-bottom:6px; font-size:11px; color:#6B7280;">
    2026-02-04 | Max Planck Institute for Software Systems | <a href="https://arxiv.org/abs/2602.04431" style="color:#6B7280; text-decoration:none;">2602.04431</a>
  </div>
  <div style="margin-bottom:8px;">
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">M=8</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px; margin-right:4px;">P=8</span>
    <span style="background:#F8F9FB; color:#6B7280; padding:2px 8px; border-radius:999px; font-weight:600; font-size:10px;">I=8</span>
  </div>
  <div style="font-size:13px; color:#1F2937; line-height:1.6; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #2563EB;">
    MaMa automates the design of multi-agent systems by formulating the problem as a Stackelberg Security Game: a Meta-Agent evolves system architectures (tools, communication graphs) while a Meta-Adversary iteratively optimizes worst-case agent compromi...<input type="checkbox" id="rd-2602-04431" class="rd-cb"><label for="rd-2602-04431" style="color:#2563EB; cursor:pointer; font-weight:600; font-size:12px; display:block; margin-top:6px;">▶ Read more</label><div class="rd-content" style="padding-top:8px; border-top:1px solid #E1E4E8; margin-top:6px;">MaMa automates the design of multi-agent systems by formulating the problem as a Stackelberg Security Game: a Meta-Agent evolves system architectures (tools, communication graphs) while a Meta-Adversary iteratively optimizes worst-case agent compromises to break them. Empirical results on the BAD-ACTS benchmark show this adversarial co-evolution reduces attack success rates from ~50% (static baselines) to ~15-25% without degrading task quality. The critical takeaway is the implementation of an <strong>adversarial co-evolution loop</strong> within the architecture search—optimizing the 'threat' alongside the 'solution'—which directly addresses the robustness objectives in our RobustMAS project. We should implement this 'Meta-Adversary' concept to stress-test our evolved algorithms during the search phase rather than post-hoc.</div>
  </div>
</td></tr><tr><td style="padding:20px 24px 12px; border-top:2px solid #F8F9FB; background:#FAFBFC;">
  <h2 style="margin:0; font-size:15px; font-weight:700; color:#1F2937; text-transform:uppercase; letter-spacing:0.03em;">Research Front Landscape</h2>
  <p style="margin:4px 0 0; font-size:11px; color:#6B7280; font-weight:500;">5 active fronts | 49 new papers</p>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div>
    <h3 style="margin:0; font-size:16px; font-weight:700; color:#1F2937; letter-spacing:-0.01em;">Fine-Grained MoE Scheduling and Stochastic Control for LLM Inference</h3>
  </div>
  <div style="margin-top:8px; line-height:1.8;">
    <span style="background:#0891B2; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700; text-transform:uppercase; display:inline-block;">EMERGING</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">Density: 1.00</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">2 papers</span>
  </div>
  <div style="margin-top:10px; line-height:1.8;"><span style="font-size:11px; color:#6B7280; font-weight:600; text-transform:uppercase; letter-spacing:0.02em; margin-right:6px;">Methods</span><span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">task_scheduling</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">fine_grained_scheduling</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">disaggregated_expert_parallelism</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">ping_pong_pipeline</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">performance_modeling</span></div>
  <div style="margin-top:6px; font-size:10px; color:#6B7280;"><span style="font-weight:700; text-transform:uppercase; letter-spacing:0.02em; margin-right:4px;">Inst:</span>The Hong Kong. 100% &nbsp;·&nbsp; Harbin Institute of. 50% &nbsp;·&nbsp; Hong Kong Baptist. 50%</div>
  <div style="margin-top:10px; font-size:13px; line-height:1.6; color:#1F2937; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #4F46E5;"><p style="margin:0;">This research front unifies approaches to optimize large-scale LLM inference, specifically addressing Mixture-of-Experts (MoE) scheduling and prefill-decode contention. One key direction involves fine-grained task scheduling, exemplified by the FinDEP algorithm, which maximizes task overlap for disaggregated expert parallelism. The other direction focuses on stochastic control, utilizing many-server queueing network models and fluid approximations to manage heterogeneous workloads and prefill-decode contention.</p><input type="checkbox" id="fs-28" class="rd-cb"><label for="fs-28" style="color:#2563EB; cursor:pointer; font-weight:600; font-size:11px; display:block; margin-top:6px;">&#9660; Read full analysis</label><div class="rd-content" style="padding-top:8px; border-top:1px solid #E1E4E8; margin-top:6px;"><p style="margin:8px 0 0;">Key contributions include FinDEP's fine-grained task scheduling algorithm, which leverages linear performance models and analytical properties to achieve up to 1.61x throughput improvement over PPPipe on H20/A6000 clusters. Concurrently, Lin et al. propose a rigorous multiclass many-server queueing network model, deriving a 'Gate-and-Route' policy from a steady-state fluid LP. This policy effectively manages prefill-decode contention, demonstrating that separating prefill admission from decode routing maximizes revenue, with FI-WSP achieving approximately 30% lower revenue than OPT.</p><p style="margin:8px 0 0;">This front is clearly emerging, with both papers introducing novel, specific methodologies for critical LLM serving challenges. The trajectory suggests a move towards more robust and integrated solutions. Future work will likely focus on relaxing current model assumptions, extending applicability to more complex hardware and workload scenarios, and potentially combining the strengths of fine-grained scheduling with higher-level stochastic control policies.</p></div></div>
  <input type="checkbox" id="fp-28" class="rd-cb"><label for="fp-28" style="margin-top:10px; font-size:11px; color:#2563EB; font-weight:600; cursor:pointer; display:block;">&#9660; 2 papers in this front</label><div class="rd-content"><ul style="margin:4px 0 0; padding:0; border-top:1px solid #E1E4E8;"><li style="padding:5px 0; border-bottom:1px solid #E1E4E8; list-style:none;"><a href="https://arxiv.org/abs/2512.21487" style="color:#2563EB; text-decoration:none; font-size:12px; font-weight:500;">Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism</a> <span style="color:#6B7280; font-size:10px;">M:6</span> <span style="color:#6B7280; font-size:10px;">· 2025-12</span></li><li style="padding:5px 0; border-bottom:1px solid #E1E4E8; list-style:none;"><a href="https://arxiv.org/abs/2602.02987" style="color:#2563EB; text-decoration:none; font-size:12px; font-weight:500;">Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control</a> <span style="background:#059669; color:white; padding:1px 6px; border-radius:999px; font-size:9px; font-weight:700;">★</span> <span style="color:#6B7280; font-size:10px;">M:8</span> <span style="color:#6B7280; font-size:10px;">· 2026-02</span></li></ul></div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div>
    <h3 style="margin:0; font-size:16px; font-weight:700; color:#1F2937; letter-spacing:-0.01em;">Convex Optimization and Game Theory for Robust Multi-Objective LLM Alignment</h3>
  </div>
  <div style="margin-top:8px; line-height:1.8;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700; text-transform:uppercase; display:inline-block;">GROWING</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">Density: 0.67</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">3 papers</span>
  </div>
  <div style="margin-top:10px; line-height:1.8;"><span style="font-size:11px; color:#6B7280; font-weight:600; text-transform:uppercase; letter-spacing:0.02em; margin-right:6px;">Methods</span><span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">multi_objective_optimization</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">convex_optimization</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">gradient_descent</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">game_theory</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">llm_as_evaluator</span></div>
  <div style="margin-top:6px; font-size:10px; color:#6B7280;"><span style="font-weight:700; text-transform:uppercase; letter-spacing:0.02em; margin-right:4px;">Inst:</span>Ruhr University Bochum 33% &nbsp;·&nbsp; University of Warwick 33% &nbsp;·&nbsp; University College London 33% &nbsp;·&nbsp; University of Basel 33%</div>
  <div style="margin-top:10px; font-size:13px; line-height:1.6; color:#1F2937; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #4F46E5;"><p style="margin:0;">This research front unifies recent advancements in applying Operations Research techniques, specifically convex optimization and game theory, to the challenging problem of multi-objective alignment for Large Language Models (LLMs). Papers introduce frameworks like PAMA, Safety Game, and Robust Multi-Objective Decoding (RMOD) to manage conflicting objectives such as harmlessness, helpfulness, sentiment, and length control, often at inference time.</p><input type="checkbox" id="fs-23" class="rd-cb"><label for="fs-23" style="color:#2563EB; cursor:pointer; font-weight:600; font-size:11px; display:block; margin-top:6px;">&#9660; Read full analysis</label><div class="rd-content" style="padding-top:8px; border-top:1px solid #E1E4E8; margin-top:6px;"><p style="margin:8px 0 0;">Key contributions include the PAMA algorithm, which transforms multi-objective RLHF into an O(n) convex optimization problem with a closed-form solution, outperforming MORLHF and MGDA-UB on LLaMA-2 7B for harmlessness. The Safety Game formulates black-box LLM agent alignment as a zero-sum game solvable by an LP solver at inference, achieving up to two-fold accuracy improvement on SafetyBench. RMOD introduces a maximin two-player game for robust multi-objective decoding, solving a convex optimization problem at each step to maximize worst-case value, outperforming MO-DPO and scalarized baselines by +1.2% WCWR on Anthropic HH.</p><p style="margin:8px 0 0;">This front is rapidly growing, demonstrating the power of OR principles to bring robustness and efficiency to LLM alignment. The trajectory indicates a strong focus on mathematically grounded, inference-time control mechanisms. Future work will likely focus on extending these frameworks to more complex, dynamic, and multi-agent scenarios, improving their scalability to a greater number of objectives, and integrating these control mechanisms into broader agentic architectures.</p></div></div>
  <input type="checkbox" id="fp-23" class="rd-cb"><label for="fp-23" style="margin-top:10px; font-size:11px; color:#2563EB; font-weight:600; cursor:pointer; display:block;">&#9660; 3 papers in this front</label><div class="rd-content"><ul style="margin:4px 0 0; padding:0; border-top:1px solid #E1E4E8;"><li style="padding:5px 0; border-bottom:1px solid #E1E4E8; list-style:none;"><a href="https://arxiv.org/abs/2508.07768" style="color:#2563EB; text-decoration:none; font-size:12px; font-weight:500;">Pareto Multi-Objective Alignment for Language Models</a> <span style="color:#6B7280; font-size:10px;">M:7</span> <span style="color:#6B7280; font-size:10px;">· 2025-08</span></li><li style="padding:5px 0; border-bottom:1px solid #E1E4E8; list-style:none;"><a href="https://arxiv.org/abs/2510.09330" style="color:#2563EB; text-decoration:none; font-size:12px; font-weight:500;">Safety Game: Balancing Safe and Informative Conversations with Blackbox Agentic AI using LP Solvers</a> <span style="color:#6B7280; font-size:10px;">M:7</span> <span style="color:#6B7280; font-size:10px;">· 2025-12</span></li><li style="padding:5px 0; border-bottom:1px solid #E1E4E8; list-style:none;"><a href="https://arxiv.org/abs/2503.08796" style="color:#2563EB; text-decoration:none; font-size:12px; font-weight:500;">Robust Multi-Objective Controlled Decoding of Large Language Models</a> <span style="background:#059669; color:white; padding:1px 6px; border-radius:999px; font-size:9px; font-weight:700;">★</span> <span style="color:#6B7280; font-size:10px;">M:8</span> <span style="color:#6B7280; font-size:10px;">· 2025-03</span></li></ul></div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div>
    <h3 style="margin:0; font-size:16px; font-weight:700; color:#1F2937; letter-spacing:-0.01em;">MILP-Driven Resource Allocation for Efficient and Sustainable LLM Serving</h3>
  </div>
  <div style="margin-top:8px; line-height:1.8;">
    <span style="background:#4F46E5; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700; text-transform:uppercase; display:inline-block;">STABLE</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">Density: 0.81</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">7 papers</span>
  </div>
  <div style="margin-top:10px; line-height:1.8;"><span style="font-size:11px; color:#6B7280; font-weight:600; text-transform:uppercase; letter-spacing:0.02em; margin-right:6px;">Methods</span><span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">integer_linear_programming</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">performance_modeling</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">data_parallelism</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">tensor_parallelism</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">pipeline_parallelism</span></div>
  <div style="margin-top:6px; font-size:10px; color:#6B7280;"><span style="font-weight:700; text-transform:uppercase; letter-spacing:0.02em; margin-right:4px;">Inst:</span>University of Cambridge 29% &nbsp;·&nbsp; Indian Institute of. 29% &nbsp;·&nbsp; University of Waterloo 14% &nbsp;·&nbsp; Purdue University 14%</div>
  <div style="margin-top:10px; font-size:13px; line-height:1.6; color:#1F2937; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #4F46E5;"><p style="margin:0;">This research front focuses on leveraging Integer Linear Programming (ILP) and Mixed Integer Linear Programming (MILP) to optimize various aspects of Large Language Model (LLM) serving infrastructure. This includes carbon-aware cache management, efficient multi-round inference over disaggregated systems, forecast-aware auto-scaling in cloud data centers, resource allocation for geographically-distributed inference, topology-driven placement of Mixture-of-Expert (MoE) layers, bi-level optimization for cascade serving, and max-flow based serving over heterogeneous GPUs. The core theme is applying rigorous Operations Research methods to enhance the efficiency, sustainability, and performance of LLM deployment.</p><input type="checkbox" id="fs-14" class="rd-cb"><label for="fs-14" style="color:#2563EB; cursor:pointer; font-weight:600; font-size:11px; display:block; margin-top:6px;">&#9660; Read full analysis</label><div class="rd-content" style="padding-top:8px; border-top:1px solid #E1E4E8; margin-top:6px;"><p style="margin:8px 0 0;">Key contributions include GreenCache's ILP for dynamic KV cache sizing, achieving up to 12.6% carbon reduction for Llama-3 70B. Dynamo's ILP-based offline planner and adaptive routing improved SLO attainment by 67-340% for multi-round inference. SageServe demonstrated 25% GPU-hours savings and $2.5M/month in cloud costs for Llama-2 using ILP and ARIMA forecasting. Other works optimized distributed inference, showing 60-80% latency reduction (Petals-derived), up to 39.1% network hop reduction for MoE placement (ILPLoad), 2.3x throughput gains for cascade serving (Cascadia), and 3.3x decode throughput on heterogeneous GPUs (Helix) via max-flow MILP.</p><p style="margin:8px 0 0;">This research front is stable, demonstrating a robust and expanding application of OR techniques to LLM serving. The trajectory indicates a move towards more complex, dynamic, and integrated optimization problems. Future work will likely focus on scaling these MILP solutions to larger clusters, incorporating more real-time and predictive elements, and adapting to evolving LLM architectures and serving paradigms. The next papers will likely explore dynamic, adaptive OR solutions that can respond to real-time changes in workload and infrastructure, potentially integrating machine learning for more accurate predictions within the optimization loop.</p></div></div>
  <input type="checkbox" id="fp-14" class="rd-cb"><label for="fp-14" style="margin-top:10px; font-size:11px; color:#2563EB; font-weight:600; cursor:pointer; display:block;">&#9660; 7 papers in this front</label><div class="rd-content"><ul style="margin:4px 0 0; padding:0; border-top:1px solid #E1E4E8;"><li style="padding:5px 0; border-bottom:1px solid #E1E4E8; list-style:none;"><a href="https://arxiv.org/abs/2505.23970" style="color:#2563EB; text-decoration:none; font-size:12px; font-weight:500;">Cache Your Prompt When It's Green: Carbon-Aware Caching for Large Language Model Serving</a> <span style="color:#6B7280; font-size:10px;">M:5</span> <span style="color:#6B7280; font-size:10px;">· 2026-01</span></li><li style="padding:5px 0; border-bottom:1px solid #E1E4E8; list-style:none;"><a href="https://arxiv.org/abs/2602.14516" style="color:#2563EB; text-decoration:none; font-size:12px; font-weight:500;">Efficient Multi-round LLM Inference over Disaggregated Serving</a> <span style="background:#059669; color:white; padding:1px 6px; border-radius:999px; font-size:9px; font-weight:700;">★</span> <span style="color:#6B7280; font-size:10px;">M:7</span> <span style="color:#6B7280; font-size:10px;">· 2026-02</span></li><li style="padding:5px 0; border-bottom:1px solid #E1E4E8; list-style:none;"><a href="https://arxiv.org/abs/2502.14617" style="color:#2563EB; text-decoration:none; font-size:12px; font-weight:500;">SageServe: Optimizing LLM Serving on Cloud Data Centers with Forecast Aware Auto-Scaling</a> <span style="background:#059669; color:white; padding:1px 6px; border-radius:999px; font-size:9px; font-weight:700;">★</span> <span style="color:#6B7280; font-size:10px;">M:7</span> <span style="color:#6B7280; font-size:10px;">· 2025-11</span></li><li style="padding:5px 0; border-bottom:1px solid #E1E4E8; list-style:none;"><a href="https://arxiv.org/abs/2512.21884" style="color:#2563EB; text-decoration:none; font-size:12px; font-weight:500;">Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models</a> <span style="background:#059669; color:white; padding:1px 6px; border-radius:999px; font-size:9px; font-weight:700;">★</span> <span style="color:#6B7280; font-size:10px;">M:5</span> <span style="color:#6B7280; font-size:10px;">· 2025-12</span></li><li style="padding:5px 0; border-bottom:1px solid #E1E4E8; list-style:none;"><a href="https://arxiv.org/abs/2508.09229" style="color:#2563EB; text-decoration:none; font-size:12px; font-weight:500;">Cluster Topology-Driven Placement of Experts Reduces Network Traffic in MoE Inference</a> <span style="background:#059669; color:white; padding:1px 6px; border-radius:999px; font-size:9px; font-weight:700;">★</span> <span style="color:#6B7280; font-size:10px;">M:5</span> <span style="color:#6B7280; font-size:10px;">· 2025-08</span></li><li style="padding:5px 0; border-bottom:1px solid #E1E4E8; list-style:none;"><a href="https://arxiv.org/abs/2506.04203" style="color:#2563EB; text-decoration:none; font-size:12px; font-weight:500;">Cascadia: An Efficient Cascade Serving System for Large Language Models</a> <span style="background:#059669; color:white; padding:1px 6px; border-radius:999px; font-size:9px; font-weight:700;">★</span> <span style="color:#6B7280; font-size:10px;">M:7</span> <span style="color:#6B7280; font-size:10px;">· 2025-09</span></li><li style="padding:5px 0; border-bottom:1px solid #E1E4E8; list-style:none;"><a href="https://arxiv.org/abs/2406.01566" style="color:#2563EB; text-decoration:none; font-size:12px; font-weight:500;">Helix: Serving Large Language Models over Heterogeneous GPUs and Network via Max-Flow</a> <span style="background:#059669; color:white; padding:1px 6px; border-radius:999px; font-size:9px; font-weight:700;">★</span> <span style="color:#6B7280; font-size:10px;">M:8</span> <span style="color:#6B7280; font-size:10px;">· 2025-03</span></li></ul></div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div>
    <h3 style="margin:0; font-size:16px; font-weight:700; color:#1F2937; letter-spacing:-0.01em;">Online LLM Inference Scheduling: KV Cache, Staggered Batching, and Fluid Dynamics</h3>
  </div>
  <div style="margin-top:8px; line-height:1.8;">
    <span style="background:#4F46E5; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700; text-transform:uppercase; display:inline-block;">STABLE</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">Density: 1.00</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">3 papers</span>
  </div>
  <div style="margin-top:10px; line-height:1.8;"><span style="font-size:11px; color:#6B7280; font-weight:600; text-transform:uppercase; letter-spacing:0.02em; margin-right:6px;">Methods</span><span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">online_optimization</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">scheduling</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">batching</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">integer_programming</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">competitive_ratio_analysis</span></div>
  <div style="margin-top:6px; font-size:10px; color:#6B7280;"><span style="font-weight:700; text-transform:uppercase; letter-spacing:0.02em; margin-right:4px;">Inst:</span>Massachusetts Institute of. 67% &nbsp;·&nbsp; Microsoft Research 33% &nbsp;·&nbsp; HKUST 33% &nbsp;·&nbsp; Baidu Inc. 33%</div>
  <div style="margin-top:10px; font-size:13px; line-height:1.6; color:#1F2937; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #4F46E5;"><p style="margin:0;">This research front unifies recent advancements in online scheduling for Large Language Model (LLM) inference, specifically addressing the critical challenges posed by KV cache memory constraints. It explores diverse operations research methodologies, including the Memory Constrained Shortest First (MC-SF) algorithm, Staggered Batch Scheduling (SBS), and fluid dynamics approximations with threshold-based online scheduling (Nested WAIT). The core theme is optimizing throughput and latency in dynamic LLM serving environments.</p><input type="checkbox" id="fs-4" class="rd-cb"><label for="fs-4" style="color:#2563EB; cursor:pointer; font-weight:600; font-size:11px; display:block; margin-top:6px;">&#9660; Read full analysis</label><div class="rd-content" style="padding-top:8px; border-top:1px solid #E1E4E8; margin-top:6px;"><p style="margin:8px 0 0;">Key contributions include the MC-SF algorithm, which achieves near-optimal performance (within 5% of hindsight optimal) by incorporating a "future feasibility check" for KV cache management. Staggered Batch Scheduling (SBS) significantly reduces Time-to-First-Token (TTFT) by 30-40% and improves overall throughput by 15-20% on production workloads, leveraging an "IQR-aware lexicographical" decode scheduling. Furthermore, fluid-guided approaches like Nested WAIT provide rigorous asymptotic optimality proofs and high-probability bounds against memory overflow, outperforming heuristic baselines like vLLM and Sarathi by employing a novel "nested segment" mechanism for handling unknown output lengths.</p><p style="margin:8px 0 0;">This front is rapidly emerging, establishing foundational mathematical and algorithmic frameworks for high-efficiency LLM inference. The trajectory suggests a move towards integrating these distinct approaches, refining parameter determination, and extending their applicability to more complex, heterogeneous, and distributed LLM serving architectures. The next papers will likely focus on combining predictive mechanisms with adaptive scheduling policies and scaling solutions for multi-GPU systems.</p></div></div>
  <input type="checkbox" id="fp-4" class="rd-cb"><label for="fp-4" style="margin-top:10px; font-size:11px; color:#2563EB; font-weight:600; cursor:pointer; display:block;">&#9660; 3 papers in this front</label><div class="rd-content"><ul style="margin:4px 0 0; padding:0; border-top:1px solid #E1E4E8;"><li style="padding:5px 0; border-bottom:1px solid #E1E4E8; list-style:none;"><a href="https://arxiv.org/abs/2502.07115" style="color:#2563EB; text-decoration:none; font-size:12px; font-weight:500;">Online Scheduling for LLM Inference with KV Cache Constraints</a> <span style="background:#059669; color:white; padding:1px 6px; border-radius:999px; font-size:9px; font-weight:700;">★</span> <span style="color:#6B7280; font-size:10px;">M:8</span> <span style="color:#6B7280; font-size:10px;">· 2026-01</span></li><li style="padding:5px 0; border-bottom:1px solid #E1E4E8; list-style:none;"><a href="https://arxiv.org/abs/2512.16134" style="color:#2563EB; text-decoration:none; font-size:12px; font-weight:500;">Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference</a> <span style="background:#059669; color:white; padding:1px 6px; border-radius:999px; font-size:9px; font-weight:700;">★</span> <span style="color:#6B7280; font-size:10px;">M:6</span> <span style="color:#6B7280; font-size:10px;">· 2025-12</span></li><li style="padding:5px 0; border-bottom:1px solid #E1E4E8; list-style:none;"><a href="https://arxiv.org/abs/2504.11320" style="color:#2563EB; text-decoration:none; font-size:12px; font-weight:500;">Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints</a> <span style="background:#059669; color:white; padding:1px 6px; border-radius:999px; font-size:9px; font-weight:700;">★</span> <span style="color:#6B7280; font-size:10px;">M:8</span> <span style="color:#6B7280; font-size:10px;">· 2026-01</span></li></ul></div>
</td></tr><tr><td style="padding:16px 24px; border-bottom:1px solid #E1E4E8; background:#FFFFFF;">
  <div>
    <h3 style="margin:0; font-size:16px; font-weight:700; color:#1F2937; letter-spacing:-0.01em;">Adaptive Bayesian Sampling and Predictive Pipelining for LLM Inference Optimization</h3>
  </div>
  <div style="margin-top:8px; line-height:1.8;">
    <span style="background:#4F46E5; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:700; text-transform:uppercase; display:inline-block;">STABLE</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">Density: 1.00</span> <span style="background:#F8F9FB; color:#1F2937; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; display:inline-block;">2 papers</span>
  </div>
  <div style="margin-top:10px; line-height:1.8;"><span style="font-size:11px; color:#6B7280; font-weight:600; text-transform:uppercase; letter-spacing:0.02em; margin-right:6px;">Methods</span><span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">best_of_n</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">majority_voting</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">adaptive_sampling</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">bayesian_modeling</span> <span style="display:inline-block; background:#F8F9FB; color:#2563EB; padding:3px 10px; border-radius:999px; margin:2px; font-size:10px; font-weight:600;">dirichlet_process_prior</span></div>
  <div style="margin-top:6px; font-size:10px; color:#6B7280;"><span style="font-weight:700; text-transform:uppercase; letter-spacing:0.02em; margin-right:4px;">Inst:</span>Mohamed bin Zayed. 50% &nbsp;·&nbsp; New York University 50% &nbsp;·&nbsp; RIKEN AIP 50% &nbsp;·&nbsp; Institute of Science. 50%</div>
  <div style="margin-top:10px; font-size:13px; line-height:1.6; color:#1F2937; padding:12px; background:#F8F9FB; border-radius:12px; border-left:3px solid #4F46E5;"><p style="margin:0;">This research front unifies advanced optimization techniques to enhance the efficiency and performance of Large Language Model (LLM) inference and serving. Paper [1] focuses on adaptive sampling for majority voting using Bayesian modeling (Dirichlet process prior, Bayes factor) and optimally weighted LLM ensembles formulated as a Mixed-Integer Linear Program (MILP) to optimize LLM inference for reasoning tasks. Paper [2] introduces PROBE, a framework for Mixture-of-Experts (MoE) inference optimization, leveraging Continuous Lookahead Pipelining and predictive prefetching for dynamic load balancing.</p><input type="checkbox" id="fs-29" class="rd-cb"><label for="fs-29" style="color:#2563EB; cursor:pointer; font-weight:600; font-size:11px; display:block; margin-top:6px;">&#9660; Read full analysis</label><div class="rd-content" style="padding-top:8px; border-top:1px solid #E1E4E8; margin-top:6px;"><p style="margin:8px 0 0;">Key contributions include Paper [1]'s adaptive generation scheme, which achieves the same accuracy with 2x-5x fewer samples and tokens compared to fixed-budget Best-of-N, and demonstrates LLM ensembles outperforming single LLMs (e.g., 93.3% vs 90.0% for GPT-OSS-20B on AIME2025). Paper [2]'s PROBE framework, with its hardware-aware balance planning and phase-locked co-scheduling, delivers significant speedups, such as 1.32x in prefill latency for SGLang and 1.26x higher decoding throughput over DeepSeek-EPLB on models like Qwen3-MoE-235B.</p><p style="margin:8px 0 0;">This front is rapidly emerging, showcasing novel OR/AI hybrid approaches to critical LLM efficiency challenges. The integration of Bayesian adaptive methods with predictive pipelining points towards a trajectory of more intelligent, dynamic, and resource-aware LLM serving and inference systems. The likely next steps involve integrating these adaptive and predictive control mechanisms with more sophisticated optimization solvers (e.g., fast ALNS or IP formulations) to address real-time, stochastic resource allocation in complex LLM serving environments.</p></div></div>
  <input type="checkbox" id="fp-29" class="rd-cb"><label for="fp-29" style="margin-top:10px; font-size:11px; color:#2563EB; font-weight:600; cursor:pointer; display:block;">&#9660; 2 papers in this front</label><div class="rd-content"><ul style="margin:4px 0 0; padding:0; border-top:1px solid #E1E4E8;"><li style="padding:5px 0; border-bottom:1px solid #E1E4E8; list-style:none;"><a href="https://arxiv.org/abs/2509.21091" style="color:#2563EB; text-decoration:none; font-size:12px; font-weight:500;">Best-of-$\infty$ -- Asymptotic Performance of Test-Time Compute</a> <span style="background:#059669; color:white; padding:1px 6px; border-radius:999px; font-size:9px; font-weight:700;">★</span> <span style="color:#6B7280; font-size:10px;">M:8</span> <span style="color:#6B7280; font-size:10px;">· 2025-09</span></li><li style="padding:5px 0; border-bottom:1px solid #E1E4E8; list-style:none;"><a href="https://arxiv.org/abs/2602.00509" style="color:#2563EB; text-decoration:none; font-size:12px; font-weight:500;">PROBE: Co-Balancing Computation and Communication in MoE Inference via Real-Time Predictive Prefetching</a> <span style="background:#059669; color:white; padding:1px 6px; border-radius:999px; font-size:9px; font-weight:700;">★</span> <span style="color:#6B7280; font-size:10px;">M:5</span> <span style="color:#6B7280; font-size:10px;">· 2026-02</span></li></ul></div>
</td></tr><tr><td style="padding:20px 24px 12px; border-top:2px solid #F8F9FB; background:#FAFBFC;">
  <h2 style="margin:0; font-size:15px; font-weight:700; color:#1F2937; text-transform:uppercase; letter-spacing:0.03em;">Cross-Front Bridge Papers</h2>
  <p style="margin:4px 0 0; font-size:11px; color:#6B7280; font-weight:500;">1 papers connecting multiple research fronts</p>
</td></tr><tr><td style="padding:14px 24px; border-bottom:1px solid #E1E4E8;">
  <div style="margin-bottom:6px;">
    <a href="https://arxiv.org/abs/2512.21487" style="color:#1F2937; text-decoration:none; font-weight:600; font-size:14px; letter-spacing:-0.01em;">Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism</a>
  </div>
  <div style="margin-bottom:6px;">
    <span style="background:#059669; color:white; padding:4px 12px; border-radius:999px; font-size:10px; font-weight:600; text-transform:uppercase;">TRUE SYNTHESIS</span> <span style="background:#F8F9FB; color:#6B7280; padding:3px 10px; border-radius:999px; font-size:10px;">Front 28 → Front 14</span>
  </div>
  <div style="margin-bottom:6px; font-size:11px; color:#6B7280;">
    2025-12-25 · 2512.21487
  </div>
  <div style="font-size:13px; color:#1F2937; line-height:1.5;">
    FinDEP optimizes distributed Mixture-of-Experts (MoE) inference by partitioning tasks (attention, experts, communication) into fine-grained micro-batches and solving a scheduling problem to maximize o...
  </div>
</td></tr><tr><td style="padding:20px 24px 12px; border-top:2px solid #F8F9FB; background:#FAFBFC;">
  <h2 style="margin:0; font-size:15px; font-weight:700; color:#1F2937; text-transform:uppercase; letter-spacing:0.03em;">Framework Genealogy</h2>
  <p style="margin:4px 0 0; font-size:11px; color:#6B7280; font-weight:500;">Tracking research lineages and framework evolution</p>
</td></tr><tr><td style="padding:0;">
<div style="padding:12px 24px; background:#F8F9FB; border-radius:12px 12px 0 0; border:1px solid #E1E4E8; border-bottom:none;">
  <div style="font-size:11px; color:#1F2937;">
    <b>29</b> frameworks tracked · <b>29</b> root frameworks · <b>9</b> active (last 30 days)
  </div>
</div>
<div style="padding:16px 24px; background:#FFFFFF; text-align:center; border:1px solid #E1E4E8; border-top:none; border-radius:0 0 12px 12px;">
  <div style="font-size:10px; margin-bottom:10px; color:#6B7280; text-transform:uppercase; letter-spacing:0.03em;">Framework landscape (size = paper count, color = must-read ratio)</div>
  <div style="font-size:12px;">dynamo (1 papers, 1 must-read) • grpo (1 papers, 1 must-read) • mlora (1 papers, 0 must-read) • aflow (1 papers, 1 must-read) • many_server_queueing_theory (1 papers, 1 must-read) • sglang (1 papers, 1 must-read) • adaptdl (1 papers, 0 must-read) • lmcache (1 papers, 0 must-read) • hedrarag (1 papers, 0 must-read) • finemoe (1 papers, 1 must-read)</div>
  <div style="margin-top:10px; font-size:9px; color:#6B7280;">
    <span style="color:#1A5F7A;">■</span> Active + Must-read &nbsp;
    <span style="color:#64B5CD;">■</span> Active &nbsp;
    <span style="color:#2E7D32;">■</span> Inactive + Must-read &nbsp;
    <span style="color:#A5D6A7;">■</span> Inactive
  </div>
</div>
</td></tr></table>
</td></tr>
<tr><td style="background:#FAFBFC; padding:20px 24px; border-top:1px solid #E1E4E8; border-radius:0 0 16px 16px; text-align:center;">
<p style="margin:0 0 12px; font-size:11px; color:#6B7280; font-weight:500;">Curated by Research Intelligence System</p>
<table cellpadding="0" cellspacing="0" role="presentation" style="margin:0 auto;">
<tr>
<td style="border-radius:12px; background:#2563EB;">
<a href="#" style="display:inline-block; padding:10px 20px; font-size:12px; font-weight:700; color:#FFFFFF; text-decoration:none; border-radius:12px;">View Full Archive →</a>
</td>
</tr>
</table>
</td></tr>
</table>
</td></tr>
</table>
</body>
</html>