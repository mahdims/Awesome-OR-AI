{"meta": {"generated_at": "2026-02-19T12:44:21", "categories": ["LLMs for Algorithm Design", "Generative AI for OR", "OR for Generative AI"], "last_update_by_category": {"LLMs for Algorithm Design": "2026-02-19", "Generative AI for OR": "2026-02-19", "OR for Generative AI": "2026-02-19"}}, "snapshot": {"total_papers": 209, "total_fronts": 17, "total_bridges": 23, "must_read_count": 151, "changes_thinking_count": 90, "emerging_fronts_count": 3, "last_7d_papers": 4, "by_category": {"LLMs for Algorithm Design": {"papers": 79, "fronts": 5, "bridges": 7}, "Generative AI for OR": {"papers": 53, "fronts": 6, "bridges": 16}, "OR for Generative AI": {"papers": 77, "fronts": 6, "bridges": 0}}, "last_update_by_category": {"LLMs for Algorithm Design": "2026-02-19", "Generative AI for OR": "2026-02-19", "OR for Generative AI": "2026-02-19"}}, "papers": [{"arxiv_id": "2602.15983", "arxiv_url": "https://arxiv.org/abs/2602.15983", "title": "ReLoop: Structured Modeling and Behavioral Verification for Reliable LLM-Based Optimization", "authors": ["Junbo"], "abstract": "", "published_date": "2026-02-17", "affiliations": "National University of Singapore, Northwestern University, City University of Hong Kong, Wenzhou University, Wenzhou Buyi Pharmacy Chain Co., Ltd.", "category": "Generative AI for OR", "relevance": {"methodological": 8, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "The paper introduces 'behavioral verification'—using solver sensitivity to parameter perturbations as a proxy for semantic correctness without ground truth. This directly addresses the 'silent failure' problem in our OR-Bench and AlgoEvo pipelines, offering a concrete method to validate generated models/heuristics when ground truth is unavailable."}, "brief": "ReLoop proposes a verification pipeline for LLM-generated optimization models that detects 'silent failures' (code that runs but solves the wrong problem) by perturbing input parameters and checking for expected solver objective shifts. They demonstrate that standard execution feasibility is a poor proxy for correctness (90% gap) on their new RetailOpt-190 benchmark, and that this perturbation testing significantly improves reliability. The critical takeaway is the use of sensitivity analysis as a ground-truth-free process reward signal: we can validate evolved algorithms in AlgoEvo by asserting that specific input perturbations *must* trigger output changes, filtering out semantically invalid candidates before expensive evaluation.", "methodology": {"core_method": "Structured generation (understand, formalize, synthesize, verify) with two-layer behavioral verification (L1 execution recovery, L2 solver-based perturbation testing) and diagnosis-guided repair.", "llm_role": "code_writer", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "ReLoop introduces structured generation mirroring expert modeling practice and formalizes ground-truth-free behavioral verification using solver-based parameter perturbation to detect silent failures.", "components": ["Structured Generation", "L1 Execution Verification", "L2 Behavioral Testing (CPT)", "L2 Behavioral Testing (OPT)", "Diagnosis-Guided Repair", "Safety Guards", "Regression Guards"], "training_required": false}, "tags": {"methods": ["chain_of_thought", "behavioral_verification", "solver_based_perturbation", "iis_diagnostics", "diagnosis_guided_repair", "llm_code_generation", "llm_in_the_loop", "sensitivity_analysis"], "problems": ["optimization_modeling", "silent_failures", "multi_period_retail_inventory_optimization", "milp_general", "program_synthesis"], "contribution_type": ["new_method", "framework", "new_benchmark", "empirical_study"], "framework_lineage": "reloop", "specific_domain": "multi_period_retail_inventory_optimization", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Semantic Correctness of LLM-Generated Optimization Models", "short": "LLM-Opt Code Correctness", "class_": "optimization_modeling_program_synthesis", "properties": ["silent_failures", "compositional", "multi_constraint", "ground_truth_free_verification", "behavioral_invariants"], "scale": "RetailOpt-190: up to 5 products, 52 periods, 5 locations, 20+ parameters, 10-30 constraints; MAMO-ComplexLP: 203 instances; IndustryOR: 100 instances"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2405.13144", "relationship": "improves upon verification limitations of"}, {"paper": "arXiv:2303.11366", "relationship": "improves upon verification limitations of"}, {"paper": "arXiv:2310.01371", "relationship": "improves upon verification limitations of"}, {"paper": "arXiv:2305.15030", "relationship": "provides external semantic signal identified as necessary by"}], "closest_prior_work": "arXiv:2305.15030", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["develop format-adaptive prompting to mitigate CoT disruption in SFT models", "implement adaptive selection for L2 parameters to reduce overhead", "explore cross-model verification strategies", "address coefficient magnitude errors, formulation equivalence errors, and unrepresented problem structures"], "transferable_to": ["general LLM-based optimization modeling tasks", "other domains beyond retail optimization", "program synthesis and code generation requiring semantic correctness verification"], "open_weaknesses": ["CoT structured generation can disrupt SFT models' learned patterns", "L2 behavioral testing incurs linear overhead in tested parameters", "constraint extraction shares the generating LLM, leading to potential failure correlation", "does not address coefficient magnitude errors, formulation equivalence errors, or unrepresented problem structures", "32B-scale models lack sufficient reasoning capacity for complex compositional problems"]}, "artifacts": {"code_url": "https://github.com/junbolian/ReLoop", "models_released": false, "new_benchmark": true}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.3, "experiments": {"benchmarks": ["RetailOpt-190", "MAMO-ComplexLP", "IndustryOR"], "baselines": ["Claude Opus 4.6 Base", "DeepSeek-V3.2 Base", "Qwen3-32B Base", "OptMATH-Qwen2.5-32B Base", "SIRL-Qwen2.5-32B Base", "CoT (Chain-of-Thought)"], "hardware": "Anthropic API, DeepSeek API, Local vLLM (BF16) on unspecified GPUs/CPUs", "instance_sizes": [190, 203, 100]}, "results": {"vs_baselines": {"Claude Opus 4.6 Base (RetailOpt-190)": "+27.9pp Exec%, +8.5pp Acc(10^-4)", "DeepSeek-V3.2 Base (RetailOpt-190)": "+6.3pp Exec%, +5.3pp Acc(10^-4)", "Claude Opus 4.6 Base (MAMO-ComplexLP)": "+9.4pp Acc(10^-6)", "DeepSeek-V3.2 Base (IndustryOR)": "+12.0pp Acc(10^-6)"}, "scalability": "Performance gains vary with error structure; 32B models lack reasoning capacity for complex compositional problems, and L2 overhead is linear in tested parameters.", "statistical_rigor": "Greedy decoding (temperature 0) and fixed solver seed (0) ensure reproducibility; results are single-run (pass@1) with reproducibility verified on a random subset.", "limitations_acknowledged": ["CoT disrupts SFT models' learned patterns", "L2 incurs linear overhead in tested parameters", "Constraint extraction shares generating LLM, potential failure correlation", "Does not address coefficient magnitude errors", "Does not address formulation equivalence errors", "Does not address unrepresented problem structures"]}, "analysis_date": "2026-02-19"}, {"arxiv_id": "2602.16038", "arxiv_url": "https://arxiv.org/abs/2602.16038", "title": "Heuristic Search as Language-Guided Program Optimization", "authors": ["Mingxin"], "abstract": "", "published_date": "2026-02-17", "affiliations": "Massachusetts Institute of Technology", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 10, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper introduces a structural advance in LLM-based evolutionary search by replacing textual reflection with an 'Executable Analyst' that writes code to diagnose solver states. This directly addresses our focus on 'better signal for search' and 'observability' in AlgoEvo, offering a concrete way to improve upon ReEvo-style reflection."}, "brief": "LaGO decomposes automated heuristic design into three explicit modules: evaluation, a code-writing 'Analyst' (backward pass), and a diversity-aware 'Generator' (update), while co-evolving constructive and refinement heuristics. The authors demonstrate significant gains (+0.17 QYI) on PDPTW and Crew Pairing against ReEvo and EoH, showing that joint optimization of initialization and improvement prevents local optima. The critical takeaway is the 'Analyst' module: instead of asking the LLM for text critiques, they ask it to write Python feature extraction functions to statistically characterize solution quality—a technique we should immediately adopt to upgrade our fitness signals in AlgoEvo.", "methodology": {"core_method": "Language-Guided Optimization (LaGO) framework decomposing heuristic discovery into forward, backward, and update stages, utilizing LLMs for reasoned evolution, code-writing analysis, co-evolution of constructive and refinement heuristics, and diversity-aware population management.", "llm_role": "evolutionary_search", "llm_model_used": "Gemini-3-Flash", "search_type": "improvement", "novelty_claim": "We introduce LaGO, a unified framework that formalizes LLM-based AHD as a language-guided optimization problem by decomposing the pipeline into forward, backward, and update modules, enabling systematic analysis and principled modification of its components.", "components": ["Forward Pass (Evaluation)", "Backward Pass (Analyst)", "Update Step (Generator)", "Co-evolution of Constructive and Refinement Heuristics", "Code-writing Analyst", "Diversity-aware Population Management"], "training_required": true}, "tags": {"methods": ["llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_in_the_loop", "llm_as_evaluator", "evolution_of_heuristics", "program_synthesis", "lago", "large_neighborhood_search", "genetic_algorithm", "automated_heuristic_design"], "problems": ["combinatorial_optimization", "pickup_and_delivery_problem_with_time_windows", "airline_crew_pairing_problem", "technology_mapping_problem", "intra_operator_parallelism_scheduling", "traveling_salesman_problem", "heuristic_evolution", "algorithm_discovery"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "lago", "specific_domain": null, "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Automated Heuristic Design", "short": "AHD", "class_": "algorithm_design", "properties": ["constrained", "non_convex_objective", "high_dimensional_search_space", "complex_logic"], "scale": "up to 200 nodes (TSP), large-scale (Crew Pairing, Technology Mapping, Intra-op), 30 instances per problem type"}, "lineage": {"direct_ancestors": [{"paper": "EoH (Liu et al., 2024)", "relationship": "generalizes and improves upon"}, {"paper": "LLM-LNS (Ye et al., 2025)", "relationship": "generalizes and improves upon"}, {"paper": "ReEvo (Ye et al., 2024)", "relationship": "generalizes and improves upon"}], "closest_prior_work": "EoH (Liu et al., 2024)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["dynamic component selection within heuristic pipelines", "alternative feedback modalities beyond code execution-based profiling", "advanced update strategies beyond genetic programming"], "transferable_to": ["other complex combinatorial optimization problems with rich semantic structures", "automated algorithm discovery for other domains (e.g., scientific discovery, game AI)", "problems requiring co-evolution of multiple algorithmic components"], "open_weaknesses": ["modest gains on highly mature and well-studied problems (e.g., TSP) due to limited room for improvement", "less effective in domains with less semantic feedback", "potential for search efficiency impedance due to expanded search space (though mitigated by diversity-aware strategy)"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 9.0, "experiments": {"benchmarks": ["HeuriGym", "Pickup and Delivery Problem with Time Windows (PDPTW)", "Airline Crew Pairing Problem (CPP)", "Technology Mapping Problem", "Intra-Operator Parallelism Scheduling (IOPDDL)", "Traveling Salesman Problem (TSP)"], "baselines": ["HeuriGen (Chen et al., 2025)", "EoH (Liu et al., 2024)", "ReEvo (Ye et al., 2024)", "LLM-LNS (Ye et al., 2025)", "EoH-I", "ReEvo-I", "LLM-LNS-I"], "hardware": "Gemini-3-Flash", "instance_sizes": [100, 124, 127, 150, 159, 200]}, "results": {"vs_baselines": {"PDPTW": "+0.172 QYI on test set vs ReEvo-I", "Crew Pairing": "+0.002 QYI on test set vs LLM-LNS-I", "Techno Mapping": "+0.014 QYI on test set vs LLM-LNS", "Intra-op": "+0.094 QYI on test set vs ReEvo-I", "TSP": "Achieved best results on 3 of 6 instances (rd100, pr124, u159) and second best on 2 others (kroA150, kroB200) compared to EoH, ReEvo, LLM-LNS"}, "scalability": "The framework maintains a tighter generalization gap (0.07) between training and test sets compared to baselines (0.24 on PDPTW), and converges stably to higher fitness despite an expanded search space.", "statistical_rigor": "Results are averaged over 3 runs with standard deviations reported, using Quality-Yield Index (QYI) as a unified metric for robustness and optimality.", "limitations_acknowledged": ["Modest gains on highly mature problems like TSP due to limited room for improvement and less semantic feedback.", "Expanded search space from joint optimization may impede search efficiency (though mitigated by diversity-aware strategy)."]}, "analysis_date": "2026-02-19"}, {"arxiv_id": "2602.15951", "arxiv_url": "https://arxiv.org/abs/2602.15951", "title": "MadEvolve: Evolutionary Optimization of Cosmological Algorithms with Large Language Models", "authors": ["Tianyi"], "abstract": "", "published_date": "2026-02-17", "affiliations": "University of Wisconsin-Madison", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 6, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "The paper introduces a rigorous 'inner-loop' optimization strategy using JAX/AutoDiff that decouples structural code evolution from parameter tuning. This directly solves the credit assignment bottleneck in our AlgoEvo work, where good structures often fail due to poor zero-shot parameter choices."}, "brief": "MadEvolve extends AlphaEvolve by embedding a gradient-based optimization loop (via JAX) inside the fitness evaluation, allowing the LLM to focus purely on code structure while an optimizer (Adam) handles continuous parameters. They demonstrate 20-30% performance gains on complex cosmological reconstruction tasks, validated on held-out simulations. The critical takeaway is the architectural pattern: prompt the LLM to write differentiable code rather than tuning constants, and use a UCB1 bandit to dynamically select between cheap and expensive models. We should immediately adopt the differentiable inner-loop strategy for our continuous heuristic search projects.", "methodology": {"core_method": "LLM-driven evolutionary optimization with nested parameter tuning", "llm_role": "code_writer, mutation_operator, report_generator", "llm_model_used": "Gemini 3 Pro Preview, GPT-5.2, GPT-5, Gemini 2.5 Pro", "search_type": "evolutionary_search", "novelty_claim": "MadEvolve integrates LLM-driven evolution with physical fitness metrics and HPC-compatible evaluation, featuring a nested optimization architecture for continuous parameters and automated scientific report generation.", "components": ["LLM Ensemble", "MAP-Elites grid", "Island model", "Global elite archive", "Autodiff parameter optimization", "Greedy one-shot parameter optimization", "Automated report generation", "UCB1 bandit model selection"], "training_required": false}, "tags": {"methods": ["llm_as_heuristic", "llm_code_generation", "llm_as_evaluator", "llm_evolutionary_search", "evolution_of_heuristics", "program_synthesis", "alphaevolve", "funsearch", "map_elites", "island_model", "gradient_based_optimization", "derivative_free_optimization", "autodiff", "ucb1_bandit"], "problems": ["cosmological_initial_condition_reconstruction", "bao_reconstruction", "21cm_foreground_reconstruction", "baryonic_physics_approximation", "tsz_prediction", "automated_algorithm_design", "scientific_discovery"], "contribution_type": ["new_method", "framework", "sota_result"], "framework_lineage": "madevolve", "specific_domain": null, "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Automated Scientific Algorithm Discovery", "short": "Algorithm Discovery", "class_": "algorithm_design", "properties": ["LLM-guided", "evolutionary", "physics-constrained", "parameter_optimization", "code_synthesis", "cosmological_applications"], "scale": "Algorithms for 64^3 to 512^3 grid simulations"}, "lineage": {"direct_ancestors": [{"paper": "AlphaEvolve", "relationship": "extends and simplifies"}, {"paper": "FunSearch", "relationship": "builds on LLM-guided evolutionary search"}], "closest_prior_work": "AlphaEvolve", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["automated_program_repair", "explicit_multi_objective_pareto_optimization", "cross_task_transfer_learning", "real_time_monitoring_and_steering", "structured_exploration_of_ideas"], "transferable_to": ["n_body_simulations", "hydrodynamical_simulations", "quadratic_estimators", "general_scientific_computing_tasks"], "open_weaknesses": ["llm_generated_algorithms_lack_elegance_and_optimality_proofs", "llm_reasoning_limitations_in_report_generation", "difficulty_of_prompt_engineering", "reward_hacking_in_evaluator_design", "program_compilation_failures_increase_with_complexity"]}, "artifacts": {"code_url": "https://github.com/tianyi-stack/MadEvolve-Cosmo", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.0, "experiments": {"benchmarks": ["Quijote N-body simulation suite", "CAMELS simulation suite"], "baselines": ["Standard BAO reconstruction (Zel'dovich approximation)", "Iterative reconstruction [37]", "Tidal reconstruction formalism [62]", "Lagrangian Deep Learning (LDL) [78]"], "hardware": "AMD Ryzen Threadripper 3960X 24-core processor, 256 GB DDR4 RAM, 2x NVIDIA GeForce RTX 3090 GPUs (24 GB VRAM each)", "instance_sizes": [256, 512, 64]}, "results": {"vs_baselines": {"Standard BAO reconstruction": "+22.8% on ¯rBAO (0.752 -> 0.924)", "Iterative reconstruction": "+2.8% on ¯rBAO (0.933 -> 0.959)", "Tidal reconstruction formalism": "+30.7% on ¯r2D (0.743 -> 0.971)", "Lagrangian Deep Learning (LDL)": "+2.8% on ¯r(k) (0.943 -> 0.969), 63% reduction in test loss"}, "scalability": "Evolved algorithms generalize well to different simulation resolutions and maintain performance on higher resolution meshes without re-optimization.", "statistical_rigor": "Evaluated on 9 held-out test simulations (BAO, 21cm) or 6 independent held-out test simulations (tSZ); results are averaged over these simulations.", "limitations_acknowledged": ["LLM-generated algorithms resemble a patchwork of ideas, not systematic human exploration with optimality proofs.", "LLM-generated reports have shortcomings inherited from limited reasoning ability of current LLMs.", "Performance gains on simulations do not directly imply gains on real galaxy surveys.", "LLMs appear not to be able to simply recall entire state-of-the-art algorithms from memory.", "Current implementation has limitations in automated program repair, explicit multi-objective Pareto optimization, cross-task transfer learning, real-time monitoring, and fast operators library."]}, "analysis_date": "2026-02-19"}, {"arxiv_id": "2602.14516", "arxiv_url": "https://arxiv.org/abs/2602.14516", "title": "Efficient Multi-round LLM Inference over Disaggregated Serving", "authors": ["Wenhao"], "abstract": "", "published_date": "2026-02-16", "affiliations": "University of Cambridge, Peking University, Shanghai Jiao Tong University, Ant Group, Southeast University", "category": "OR for Generative AI", "relevance": {"methodological": 7, "problem": 9, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper directly addresses your 'GPUSched' and 'OR formulations for AI systems' active projects. It provides a concrete Integer Linear Programming (ILP) formulation for optimizing disaggregated LLM serving resources, specifically targeting the interleaved workloads of multi-agent systems."}, "brief": "AMPD introduces a disaggregated serving framework tailored for multi-round LLM agents, utilizing an offline ILP solver to optimize resource allocation (TP/DP configurations) and an online adaptive routing mechanism to handle incremental prefill tasks. The results are strong, showing 67-340% improvements in SLO attainment over vLLM and NVIDIA Dynamo by dynamically routing incremental prefill to decode workers when slack exists. For our 'GPUSched' project, the key takeaway is the specific ILP formulation (Eq. 5) for partitioning prefill/decode resources under global GPU constraints, and the insight that multi-agent workflows create a unique 'incremental prefill' bottleneck that standard disaggregation handles poorly.", "methodology": {"core_method": "Adaptive routing and prefill reordering for online scheduling, combined with an Integer Linear Programming (ILP) based offline deployment planner", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "A novel disaggregated serving framework for multi-round LLM inference that optimizes runtime scheduling and deployment strategy to maximize SLO attainment.", "components": ["Adaptive routing mechanism", "Prefill reordering policy", "Offline deployment planner (ILP)", "Profiler", "Distributed shared memory (Redis)", "NVIDIA Inference Xfer Library (NIXL)"], "training_required": false}, "tags": {"methods": ["adaptive_scheduling", "resource_management", "integer_linear_programming", "performance_modeling", "distributed_systems", "network_communication_optimization", "model_parallelism", "data_parallelism", "continuous_batching", "queueing_theory"], "problems": ["llm_serving_optimization", "resource_allocation", "llm_scheduling", "service_level_objective_optimization", "prefill_decode_disaggregation", "multi_round_inference"], "contribution_type": ["framework", "new_method", "sota_result", "empirical_study"], "framework_lineage": "dynamo", "specific_domain": "llm_serving_optimization", "llm_coupling": null}, "problem": {"formal_name": "Efficient Multi-round LLM Inference over Disaggregated Serving", "short": "Multi-round LLM Serving", "class_": "resource_allocation", "properties": ["multi-round", "disaggregated", "prefill-decode interference", "real-time scheduling", "dynamic resource allocation"], "scale": "Qwen3-32B, Llama3.1-70B, Mixtral-8x7B models; up to 32 GPUs; 3-11.32 rounds; 50-6161 token lengths"}, "lineage": {"direct_ancestors": [{"paper": "NVIDIA, 2026b", "relationship": "extends disaggregated serving from Dynamo"}, {"paper": "Zhong et al., 2024", "relationship": "builds on prefill-decode disaggregation paradigm"}], "closest_prior_work": "NVIDIA, 2026b", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Develop methods to directly optimize SLO attainment with efficient solvers, addressing the P95 latency discrepancy.", "Integrate with other PD disaggregation optimizations like elastic scaling and mixed-GPU support.", "Explore more advanced dynamic scheduling policies beyond fixed window reordering.", "Investigate adaptive tuning of hyperparameters (alpha, beta, window size) for diverse workloads."], "transferable_to": ["Serving other large generative AI models (e.g., multi-modal models, diffusion models).", "General distributed computing systems with interdependent, distinct computational phases.", "Edge and cloud inference scenarios requiring efficient resource disaggregation.", "Real-time stream processing systems with varying workload characteristics."], "open_weaknesses": ["The discrepancy between the ILP's P95 latency objective and the system's SLO attainment goal.", "Potential for more sophisticated performance models beyond the piecewise alpha-beta model.", "Overhead and latency associated with KV cache transmission between disaggregated workers.", "Sensitivity of performance to fixed hyperparameters (alpha, beta, window size) across different workloads."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_status": "growing", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.32, "experiments": {"benchmarks": ["ToolBench", "GAIA", "HotpotQA", "DuReader"], "baselines": ["Dynamo", "vLLM", "vLLM-Continuum"], "hardware": "4 servers, each with 8 NVIDIA H20 (96GB) GPUs", "instance_sizes": [8, 16, 32]}, "results": {"vs_baselines": {"Dynamo": "67.29% average SLO attainment improvement", "vLLM": "339.74% average SLO attainment improvement", "vLLM-Continuum": "Significantly outperforms, as vLLM-Continuum is comparable to vLLM"}, "scalability": "AMPD achieves the highest SLO attainment across all experiments, demonstrating robust performance under varying request arrival rates and model/workload scales.", "statistical_rigor": "Request arrival times generated using Poisson process; performance evaluated using windowed TTFT/ITL statistics and P95 latency.", "limitations_acknowledged": ["Discrepancy between P95 latency (ILP objective) and SLO attainment (system goal) due to infeasibility of optimizing SLO attainment via linear programming."]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2602.10450", "arxiv_url": "https://arxiv.org/abs/2602.10450", "title": "Constructing Industrial-Scale Optimization Modeling Benchmark", "authors": ["Zhong"], "abstract": "", "published_date": "2026-02-11", "affiliations": "Peking University, Huawei Technologies Co., Ltd., Great Bay University", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper invalidates the predictive power of the toy benchmarks (NL4Opt, MAMO) we likely use for baselines, showing that SOTA performance drops from 90% to 18% on real industrial instances. It provides a concrete schema (Loop-Based Structural Scaffolds) for representing large-scale OR problems in LLM contexts, which is directly applicable to our OR-Bench and AlgoEvo problem formulations."}, "brief": "Li et al. introduce MIPLIB-NL, a benchmark of 223 industrial-scale MILP instances (up to 10^7 variables) reverse-engineered from MIPLIB 2017, enforcing strict model-data separation. Results are sobering: SOTA models like GPT-4 and fine-tuned OR-LLMs drop from ~90% accuracy on existing toy benchmarks to ~18% here, failing primarily on structural consistency and index handling at scale. For us, the key takeaway is their \"Loop-Based Structural Scaffold\" taxonomy—a method to compress massive industrial formulations into compact LLM prompts via model-data separation. This is a mandatory read for our OR-Bench project, as it demonstrates that current evaluations are effectively measuring overfitting to toy problems rather than genuine modeling capability.", "methodology": {"core_method": "Structure-aware reverse construction methodology from MIPLIB 2017", "llm_role": "linguistic_polisher, interactive_assistant", "llm_model_used": null, "search_type": "constructive", "novelty_claim": "We introduce MIPLIB-NL, built via a structure-aware reverse construction methodology from real mixed-integer linear programs in MIPLIB 2017.", "components": ["structural_abstraction", "Opt-to-NL_reverse_generation", "semantic_validation", "LLM_assisted_linguistic_polishing", "expert_review", "human_LLM_interaction"], "training_required": false}, "tags": {"methods": ["benchmark_design", "dataset_generation", "reverse_engineering", "structure_aware_modeling", "mixed_integer_linear_programming", "natural_language_generation", "semantic_validation", "expert_in_the_loop", "human_llm_interaction", "llm_in_the_loop", "model_data_separation", "loop_based_structure_recovery"], "problems": ["natural_language_to_optimization", "industrial_optimization", "benchmark_creation", "mixed_integer_linear_programming", "combinatorial_optimization", "resource_allocation", "scheduling", "network_design"], "contribution_type": ["new_benchmark", "new_method", "empirical_study"], "framework_lineage": null, "specific_domain": "natural_language_to_optimization_benchmark_creation", "llm_coupling": "llm_in_the_loop"}, "problem": {"formal_name": "Evaluation of Natural-Language-to-Optimization Modeling Systems on Industrial-Scale Problems", "short": "NL-to-Opt Evaluation", "class_": "benchmark_design", "properties": ["industrial_scale", "mixed_integer_linear_programming", "natural_language_to_optimization", "structure_aware", "model_data_separation", "loop_based_structure"], "scale": "10^3-10^6 variables and constraints"}, "lineage": {"direct_ancestors": [{"paper": "MIPLIB 2017", "relationship": "source of instances for reverse construction"}, {"paper": "OptMath (Lu et al., 2025)", "relationship": "pioneered reverse generation from MIPLIB for training data"}, {"paper": "Bench4Opt (Wang et al., 2025)", "relationship": "pioneered model-data separation and partly MIPLIB-derived instances"}], "closest_prior_work": "OptMath (Lu et al., 2025)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["automate verification and reverse-generation pipeline", "extend coverage to more MIPLIB instances", "explore other optimization problem types (e.g., NLP, SOCP)"], "transferable_to": ["other optimization problem types (e.g., NLP, SOCP)", "other solver formats (e.g., CPLEX, SCIP)", "other domains beyond MILP"], "open_weaknesses": ["high manual effort for construction and validation", "context window limitations for LLMs in complex instances", "absence of semantic variable names in source MPS files", "extreme scale and structural complexity of some instances", "verification challenges with isolated infeasible and open instances"]}, "artifacts": {"code_url": "https://github.com/optsuite/MIPLIB-NL", "models_released": false, "new_benchmark": true}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.96, "experiments": {"benchmarks": ["NL4Opt", "MAMO-E", "MAMO-C", "ComplexOR", "IndustryOR", "NLP4LP", "ReSocratic", "OptMath", "Bench4Opt", "LogiOR", "MIPLIB-NL"], "baselines": ["OptMATH-Qwen2.5-7B", "OptMATH-Qwen2.5-32B", "SIRL-Qwen2.5-7B", "SIRL-Qwen2.5-32B", "ORLM-LlaMa3-8B", "DeepSeek-V3.2-685B", "DeepSeek-V3.2-Think-685B", "Qwen3-Max", "Qwen3-Max-Preview", "Claude-Sonnet-4.5", "Claude-Sonnet-4.5-Think", "Gemini-3-Pro-Preview", "GPT-5.1", "GPT-5.1-CodeX"], "hardware": "NVIDIA H100 GPUs (80GB HBM3) for open-source models, API through AiHubMix for proprietary models", "instance_sizes": [10, 100, 499, 999, 9999, 10000, 1000000, 10000000]}, "results": {"vs_baselines": {"OptMATH-Qwen2.5-7B": "0.00% Pass@1 on MIPLIB-NL, a drop of ~80 percentage points compared to other benchmarks.", "SIRL-Qwen2.5-32B": "1.43% Pass@1 on MIPLIB-NL, a drop of ~80 percentage points compared to other benchmarks.", "GPT-5.1": "39.05% Pass@1 on MIPLIB-NL, a drop of ~40 percentage points compared to other benchmarks."}, "scalability": "Overall performance declines as problem scale increases, with sharp drops from small to medium instances, partial recovery for large problems, and further decrease for very large models.", "statistical_rigor": "Pass@1 and Pass@8 accuracy reported with a relative error tolerance of 1e-6, using LLM temperature 0.6 and fixed solver time budgets.", "limitations_acknowledged": ["Substantial manual effort for structure recovery and semantic validation", "Limitations of structure recovery and validation at extreme scale or under heavy solver-oriented obfuscation", "Absence of semantic variable names in MPS files", "Extreme scale and structural complexity of some instances", "Lack of real-world interpretability for some instances", "Verification challenges with infeasible and open instances"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.10233", "arxiv_url": "https://arxiv.org/abs/2602.10233", "title": "ImprovEvolve: Ask AlphaEvolve to Improve the Input Solution and Then Improvise", "authors": ["Alexey"], "abstract": "", "published_date": "2026-02-10", "affiliations": "MIRIAI, FusionBrain Lab, Institute of Numerical Mathematics", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 5, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper provides a successful template for decomposing LLM evolutionary search: evolving local operators (improve/perturb) within a fixed global loop (basin-hopping) beats evolving monolithic solvers. This directly validates an architectural shift for our AlgoEvo project, suggesting we should evolve ALNS operators rather than full algorithms."}, "brief": "Kravatskiy et al. introduce ImprovEvolve, a framework that restricts the LLM to evolving `improve()` (local search) and `perturb()` (mutation) operators, which are then executed by a fixed basin-hopping algorithm. They achieve new state-of-the-art results on Hexagon Packing and the Second Autocorrelation Inequality, demonstrating that this modular approach generalizes to unseen problem sizes where monolithic AlphaEvolve solutions fail. The critical insight is that LLMs are poor at designing global search logic and tuning hyperparameters (LLM edits actively harmed performance), so we should isolate the LLM to generating local moves while keeping the meta-heuristic framework deterministic. We should immediately apply this 'operator-only' evolution strategy to our ALNS research for VRP.", "methodology": {"core_method": "LLM-guided evolutionary search (MAP-Elites) for local search operators (improve, perturb, generate_config) within a basin-hopping framework", "llm_role": "evolutionary_search, initial_program_generator", "llm_model_used": "Gemini 3 Pro, Gemini 3 Flash Preview", "search_type": "hybrid", "novelty_claim": "ImprovEvolve enhances LLM-based evolutionary approaches by evolving modular local search operators (generate_config, improve, perturb) within a basin-hopping framework, reducing LLM cognitive load.", "components": ["MAP-Elites", "GigaEvo framework", "basin-hopping", "generate_config method", "improve method", "perturb method", "L-BFGS-B optimizer", "SLSQP optimizer", "geometric decay schedule for perturbation intensity"], "training_required": true}, "tags": {"methods": ["llm_evolutionary_search", "alphaevolve", "map_elites", "basin_hopping", "l_bfgs_b", "slsqp", "genetic_programming", "program_synthesis", "llm_code_generation", "llm_as_heuristic", "evolution_of_heuristics", "gigaevo"], "problems": ["hexagon_packing", "second_autocorrelation_inequality", "packing", "functional_optimization"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "alphaevolve", "specific_domain": null, "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Hexagon Packing Problem and Second Autocorrelation Inequality", "short": "HEX n, ACI 2", "class_": "packing, functional_optimization", "properties": ["constrained", "non-convex", "non-smooth", "high-dimensional", "highly-parameterized"], "scale": "HEX n: 11-30 hexagons; ACI 2: up to 1.6 million steps"}, "lineage": {"direct_ancestors": [{"paper": "AlphaEvolve [11]", "relationship": "enhances the LLM-guided evolutionary approach of"}, {"paper": "GigaEvo [8]", "relationship": "uses as the open-source backbone framework"}], "closest_prior_work": "AlphaEvolve [11]", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["generalize to other problem classes like combinatorial optimization, scientific simulation, or ML pipeline design", "integrate with agentic workflows for autonomous module refinement and external solver invocation", "apply to complex mathematical constructions like the kissing number problem", "develop LLM-based hyperparameter tuning strategies"], "transferable_to": ["combinatorial_optimization", "scientific_simulation", "machine_learning_pipeline_design", "kissing_number_problem"], "open_weaknesses": ["generalizability to other problem classes not yet demonstrated", "results are single-run bests without confidence intervals", "outcomes may vary due to stochastic nature of evolution and basin-hopping", "high computational cost of LLM-guided evolution", "LLMs lack reliable sense for choosing optimization hyperparameters"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.25, "experiments": {"benchmarks": ["Hexagon Packing (HEX n)", "Second Autocorrelation Inequality (ACI 2)"], "baselines": ["AlphaEvolve [11]", "ThetaEvolve [15]", "CodeEvolve [1]", "GigaEvo [8]", "Human [3, 7]"], "hardware": "CPU for evolution, GPU for final validation of Hexagon Packing", "instance_sizes": [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 1024, 4096, 16384, 32768, 50000, 100000, 200000, 400000, 800000, 1600000]}, "results": {"vs_baselines": {"AlphaEvolve": "ImprovEvolve achieves new state-of-the-art for HEX 11, 12, 15, 16. ImprovEvolve+E achieves new state-of-the-art for HEX 14, 17, 23, and ACI 2.", "GigaEvo": "ImprovEvolve consistently outperforms GigaEvo baseline on HEX 11 and ACI 2, with GigaEvo failing to generalize for HEX n > 11."}, "scalability": "The evolved program for HEX 11 generalizes to unseen problem sizes (up to n=30) without retraining, finding structured packings. ACI 2 scales up to 1.6M steps.", "statistical_rigor": "Experiments report single-run best results without confidence intervals; outcomes may vary across runs due to stochastic nature.", "limitations_acknowledged": ["generalizability to other problem classes", "single-run best results without confidence intervals", "stochastic nature of outcomes", "computational cost of LLM-guided evolution"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.08585", "arxiv_url": "https://arxiv.org/abs/2602.08585", "title": "Predicting Future Utility: Global Combinatorial Optimization for Task-Agnostic KV Cache Eviction", "authors": ["Ziyao"], "abstract": "", "published_date": "2026-02-09", "affiliations": "Baidu, Fudan University", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 9, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This is a textbook example of applying rigorous Operations Research (combinatorial optimization, convex relaxation) to LLM infrastructure (KV cache), which aligns perfectly with our 'OR formulations for AI systems' focus. The mathematical formulation using PAVA to enable greedy solving of a discrete allocation problem is a technique we can steal for our GPU scheduling or multi-agent resource allocation work."}, "brief": "Tang et al. formulate KV cache eviction not as a heuristic filtering task, but as a global combinatorial optimization problem maximizing 'Oracle Importance' (future utility) across all attention heads. They solve this NP-hard problem efficiently by applying Isotonic Regression (via PAVA) to create a convex surrogate of the eviction loss, enabling an optimal greedy allocation strategy that is deployed via an offline-computed lookup table. Results are strong: they achieve 80% cache reduction on LongBench and RULER with minimal degradation, significantly outperforming dynamic heuristics like AdaKV. **Key Takeaway:** The decomposition of error into 'ranking error' vs. 'allocation error'—and solving the latter via convex-hull relaxation—is a powerful OR pattern we should apply to our own resource allocation and scheduling problems.", "methodology": {"core_method": "Convex-hull relaxation and marginal-utility-based greedy solver for global combinatorial optimization", "llm_role": "none", "llm_model_used": null, "search_type": "greedy", "novelty_claim": "We formulate budget allocation as a global utility maximization problem and introduce an efficient solver using convex-hull relaxation and marginal-utility-based greedy allocation.", "components": ["Oracle Importance definition", "Optimality Gap decomposition", "Convex-hull relaxation", "Marginal-utility-based greedy allocation", "Offline profiling protocol", "Lookup table for online execution"], "training_required": true}, "tags": {"methods": ["combinatorial_optimization", "convex_hull_relaxation", "greedy_algorithm", "isotonic_regression", "pava", "offline_profiling", "lookup_table"], "problems": ["kv_cache_eviction", "llm_inference_optimization", "resource_allocation"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": null, "specific_domain": "kv_cache_eviction", "llm_coupling": null}, "problem": {"formal_name": "Global Combinatorial Optimization for Head-Level KV Cache Budget Allocation", "short": "KV Cache Eviction", "class_": "resource_allocation", "properties": ["long_horizon_utility", "head_level", "task_agnostic", "memory_constrained"], "scale": "4k-128k tokens"}, "lineage": {"direct_ancestors": [{"paper": "AdaKV (Feng et al., 2024)", "relationship": "builds on problem formulation and addresses limitations of dynamic allocation"}, {"paper": "CriticalKV (Feng et al., 2025)", "relationship": "inspired by criticality definition"}, {"paper": "PyramidKV (Cai et al., 2024)", "relationship": "addresses limitations of static allocation"}], "closest_prior_work": "AdaKV (Feng et al., 2024)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Investigate dynamic online adaptation of budget allocation strategies.", "Explore integration with other advanced heuristic metrics beyond SnapKV/KeyDiff/EA.", "Further theoretical analysis of the optimality gap under different LLM architectures."], "transferable_to": ["Memory management in other large neural networks (e.g., large vision models).", "General resource allocation problems in distributed AI systems.", "Optimizing other components of LLM inference beyond KV cache."], "open_weaknesses": ["The offline profiling approach might not perfectly generalize to all unseen tasks or highly dynamic inference patterns.", "The 'optimality gap' is reduced but not eliminated, suggesting room for improving heuristic metrics themselves.", "The current approach is task-agnostic; task-specific fine-tuning could yield further gains."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.42, "experiments": {"benchmarks": ["LongBench", "RULER"], "baselines": ["Uniform", "PyramidKV (Cai et al., 2024)", "AdaKV (Feng et al., 2024)"], "hardware": "GPU memory footprint reduction is a key result, but specific hardware models and training times are not detailed.", "instance_sizes": [4000, 16000, 32000, 64000, 120000]}, "results": {"vs_baselines": {"AdaKV (Mistral-7B-v0.3, KeyDiff)": "+5.67% Avg Acc on LongBench (46.21 vs 40.54)", "Uniform (Mistral-7B-v0.3, SnapKV, RULER-16K)": "+40.45% Avg Acc (69.98 vs 29.53)", "AdaKV (Qwen2.5-32B, KeyDiff)": "+5.94% Avg Acc on LongBench (48.26 vs 42.32)", "AdaKV (Qwen2.5-32B, SnapKV, RULER-16K)": "+36.43% Avg Acc (80.47 vs 44.04)", "Ablation (w/o Optimality Gap)": "-3.53 points Avg Acc"}, "scalability": "The method achieves an 80% reduction in KV cache size with minimal performance degradation, reducing inference latency and GPU memory footprint, and maintains comparable latency to baselines in long-context scenarios.", "statistical_rigor": "Detailed performance tables are provided across various benchmarks and models, but no explicit statistical significance tests (e.g., p-values, confidence intervals) or number of runs per experiment are mentioned.", "limitations_acknowledged": []}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2601.22896", "arxiv_url": "https://arxiv.org/abs/2601.22896", "title": "Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery", "authors": ["Xinyi"], "abstract": "", "published_date": "2026-02-09", "affiliations": "Tsinghua University, Chinese Academy of Sciences, University of Chinese Academy of Sciences, AiRiA", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 9, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper effectively renders static-dataset evolutionary search (like standard EoH/FunSearch) obsolete for general-purpose solver discovery. It provides a concrete recipe for implementing adversarial co-evolution in our AlgoEvo framework, directly merging our interests in LLM evolution and multi-agent optimization."}, "brief": "ASRO adapts Policy Space Response Oracles (PSRO) to code generation, treating heuristic discovery as a zero-sum game where a 'Solver' evolves to minimize gaps and a 'Generator' evolves to create adversarial instances. The results are compelling: it consistently beats the static EoH baseline on TSPLIB and CVRPLIB, proving that adversarial training yields better generalization than training on fixed distributions. The critical takeaway is the architecture: explicitly co-evolving an 'Instance Generator' program alongside the solver prevents overfitting and exposes edge cases (like specific geometric traps in TSP) that static benchmarks miss. This is a direct upgrade to our AlgoEvo/AlphaEvolve pipelines, though it incurs higher computational costs due to the evaluation matrix required for the meta-game.", "methodology": {"core_method": "Algorithm Space Response Oracles (ASRO), a game-theoretic framework for program-level co-evolution between solver and instance generator, extending PSRO to discrete program space with LLM-based best-response oracles", "llm_role": "program_synthesis, evolutionary_search", "llm_model_used": "DeepSeek-V3.2", "search_type": "improvement", "novelty_claim": "We propose Algorithm Space Response Oracles (ASRO), a game-theoretic framework that reframes heuristic discovery as a program level co-evolution between solver and instance generator.", "components": ["LLM-based best-response oracles", "payoff matrix computation", "meta-solver (for mixed meta-strategies)", "strategy pool expansion", "evolutionary program search (EoH-style)", "program-space variation operators"], "training_required": true}, "tags": {"methods": ["game_theory", "zero_sum_game", "policy_space_response_oracles", "algorithm_space_response_oracles", "llm_code_generation", "llm_evolutionary_search", "evolution_of_heuristics", "program_synthesis", "co_evolution", "meta_game", "best_response_oracle"], "problems": ["automated_heuristic_discovery", "combinatorial_optimization", "online_bin_packing", "traveling_salesman_problem", "capacitated_vehicle_routing_problem"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "eoh", "specific_domain": null, "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Combinatorial Optimization Problems (Online Bin Packing, Traveling Salesman Problem, Capacitated Vehicle Routing Problem)", "short": "CO (OBP, TSP, CVRP)", "class_": "packing, routing, graph_optimization", "properties": ["online", "capacitated", "Euclidean"], "scale": "up to 3795 nodes/items/customers"}, "lineage": {"direct_ancestors": [{"paper": "Lanctot et al., 2017", "relationship": "extends Policy Space Response Oracles (PSRO) from parametric policy spaces to discrete program space"}, {"paper": "Liu et al., 2024a", "relationship": "instantiates best-response oracle using Evolution of Heuristics (EoH)-style evolutionary program search"}], "closest_prior_work": "Liu et al., 2024a", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Extend to multi-objective or regularized meta-games", "Support teacher–student interactions for curriculum design", "Generalize program-space game perspective to algorithmic reasoning", "Generalize program-space game perspective to planning and symbolic decision-making"], "transferable_to": ["Other combinatorial optimization problems", "Algorithmic reasoning", "Planning", "Symbolic decision_making"], "open_weaknesses": ["Reliance on approximate reference values for optimality gaps introduces estimation noise", "Additional computational cost compared to static evaluation", "Potential for instability with very small base-generator mixing ratios"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.69, "experiments": {"benchmarks": ["Falkenauer T", "Falkenauer U", "Hard28", "Weibull distributions", "TSPLIB", "CVRPLIB"], "baselines": ["First Fit", "Best Fit", "Nearest Insertion", "Farthest Insertion", "NN+2-opt", "Parallel Insertion", "EoH"], "hardware": "Parallel and batched evaluation, 2200s wall-clock time (for equal-time comparison)", "instance_sizes": [100, 200, 500, 1000, 3795]}, "results": {"vs_baselines": {"EoH": "Outperforms EoH by 0.5-5% on OBP, 0.5-3% on TSP, and 2-30% on CVRP benchmarks, with larger improvements on harder and larger instances.", "First Fit": "Outperforms First Fit by 1-27% on OBP benchmarks.", "Best Fit": "Outperforms Best Fit by 0.5-27% on OBP benchmarks.", "Nearest Insertion": "Outperforms Nearest Insertion by 19-21% on TSP benchmarks.", "Farthest Insertion": "Outperforms Farthest Insertion by 7-12% on TSP benchmarks.", "NN+2-opt": "Outperforms NN+2-opt by 18-25% on CVRP benchmarks.", "Parallel Insertion": "Outperforms Parallel Insertion by 22-72% on CVRP benchmarks."}, "scalability": "Performance gap against baselines widens with increasing problem size, demonstrating stronger scalability and generalization to larger instances.", "statistical_rigor": "Results reported as mean ± std over five independent runs; confidence intervals via bootstrap resampling (20 instances per generator); exploitability metrics tracked for convergence.", "limitations_acknowledged": ["Reliance on approximate reference values for optimality gaps introduces estimation noise into the meta-game."]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.08253", "arxiv_url": "https://arxiv.org/abs/2602.08253", "title": "G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design", "authors": ["Baoyun"], "abstract": "", "published_date": "2026-02-09", "affiliations": "Tsinghua University, University of Chinese Academy of Sciences, Northeastern University", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper sits exactly at the intersection of your primary focus (LLM evolutionary search) and specific domain expertise (ALNS for VRP). It successfully demonstrates that evolving structural LNS operators (Destroy/Repair) is superior to the constructive heuristics typically evolved by FunSearch/EoH, and introduces a transferable 'synergy matrix' mechanism for co-evolving coupled components."}, "brief": "G-LNS extends LLM-based evolutionary search to ALNS by co-evolving Python code for Destroy and Repair operators rather than constructive priority rules. The authors introduce a 'Synergy Matrix' that tracks the performance of specific operator pairs during evaluation, using this data to guide a 'Synergistic Joint Crossover' where the LLM optimizes the coupling between destroy and repair logic. Results are strong: it significantly outperforms FunSearch and EoH on TSP/CVRP and beats OR-Tools on large-scale instances (N=200) under time constraints. The key takeaway for AlgoEvo is the synergy-aware co-evolution mechanism—explicitly tracking and prompting for component interaction is a concrete technique we can apply to multi-agent optimization systems.", "methodology": {"core_method": "Generative Large Neighborhood Search (G-LNS) with co-evolution of destroy and repair operators", "llm_role": "code_writer", "llm_model_used": "DeepSeek-V3.2", "search_type": "improvement", "novelty_claim": "G-LNS is a generative evolutionary framework that extends LLM-based Automated Heuristic Design to co-evolve tightly coupled destroy and repair operators for Large Neighborhood Search.", "components": ["Dual-population architecture", "Cooperative evaluation mechanism", "Adaptive LNS process", "LLM-driven evolutionary strategies (Mutation, Homogeneous Crossover, Synergistic Joint Crossover)", "Pre-evaluation Filter"], "training_required": true}, "tags": {"methods": ["large_neighborhood_search", "adaptive_large_neighborhood_search", "evolutionary_algorithm", "llm_code_generation", "llm_as_variation_operator", "simulated_annealing", "co_evolution", "program_synthesis", "evolution_of_heuristics"], "problems": ["traveling_salesman_problem", "capacitated_vehicle_routing_problem", "open_vehicle_routing_problem", "combinatorial_routing", "operator_discovery"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "funsearch", "specific_domain": "combinatorial_routing", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Traveling Salesman Problem, Capacitated Vehicle Routing Problem, Open Vehicle Routing Problem", "short": "TSP, CVRP, OVRP", "class_": "routing", "properties": ["capacitated", "multiple_vehicles", "open_routes"], "scale": "10-200 nodes"}, "lineage": {"direct_ancestors": [{"paper": "Romera-Paredes et al., 2024", "relationship": "extends LLM-based Automated Heuristic Design (AHD) paradigm from FunSearch"}, {"paper": "Liu et al., 2024b", "relationship": "extends LLM-based Automated Heuristic Design (AHD) paradigm from EoH"}, {"paper": "Ropke & Pisinger, 2006", "relationship": "draws inspiration from Adaptive Large Neighborhood Search (ALNS)"}], "closest_prior_work": "Romera-Paredes et al., 2024", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["extend to multi-objective optimization settings", "investigate applicability to a broader range of combinatorial problems beyond routing tasks"], "transferable_to": ["job_shop_scheduling", "bin_packing", "network_design"], "open_weaknesses": ["limited to single-objective optimization", "currently applied only to routing problems", "mitigating LLM hallucinations"]}, "artifacts": {"code_url": "https://github.com/zboyn/G-LNS", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.24, "experiments": {"benchmarks": ["TSPLib", "CVRPLib"], "baselines": ["LKH-3", "OR-Tools", "POMO", "FunSearch", "EoH", "ReEvo", "EVO-MCTS", "MCTS-AHD", "MCTS-AHD(ACO)", "Standard ALNS"], "hardware": "null", "instance_sizes": [10, 20, 50, 100, 200]}, "results": {"vs_baselines": {"LKH-3": "near-optimal on TSP, matching LKH-3 on TSP10", "OR-Tools": "outperforms OR-Tools on large CVRP/OVRP instances (e.g., 0.00% gap on CVRP100/200 vs 2.09%/1.27% for OR-Tools)", "POMO": "significantly outperforms POMO on all problem sizes (e.g., 1.31% vs 20.35% on TSP200)", "ALNS": "outperforms Standard ALNS on all problem sizes (e.g., 1.31% vs 5.96% on TSP200)", "FunSearch": "significantly outperforms FunSearch on all problem sizes (e.g., 1.31% vs 17.62% on TSP200)", "EoH": "significantly outperforms EoH on all problem sizes (e.g., 1.31% vs 17.84% on TSP200, 2.8% vs 18.1% on TSPLib)", "ReEvo": "significantly outperforms ReEvo on all problem sizes (e.g., 1.31% vs 14.77% on TSP200)", "MCTS-AHD": "significantly outperforms MCTS-AHD on all problem sizes (e.g., 1.31% vs 13.16% on TSP200)", "Evo-MCTS": "significantly outperforms Evo-MCTS on all problem sizes (e.g., 1.31% vs 10.20% on TSP200)", "MCTS-AHD(ACO)": "outperforms MCTS-AHD(ACO) on larger instances (e.g., 1.31% vs 6.22% on TSP200)", "EoH-S": "significantly outperforms EoH-S on standard benchmarks (e.g., 2.8% vs 9.1% on TSPLib, 15.9% vs 40.1% on CVRPLib Set F)"}, "scalability": "G-LNS demonstrates superior scalability and robustness, outperforming baselines on larger instances (e.g., N=200) and generalizing well to unseen distributions.", "statistical_rigor": "Results are averaged over three independent evolutionary runs to mitigate randomness.", "limitations_acknowledged": ["Limited to single-objective optimization", "Currently applied only to routing problems"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.08603", "arxiv_url": "https://arxiv.org/abs/2602.08603", "title": "OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval", "authors": ["Teng"], "abstract": "", "published_date": "2026-02-09", "affiliations": "Shanghai Jiao Tong University, OPPO", "category": "OR for Generative AI", "relevance": {"methodological": 9, "problem": 3, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper perfectly bridges your two core interests: Operations Research (MIP) and LLM Agents. It demonstrates a rigorous way to use offline integer programming to generate 'perfect' agent trajectories for in-context learning, offering a deterministic alternative to the RL/evolutionary steering methods we currently use."}, "brief": "Wang et al. formulate agentic tool-use planning not as a heuristic search (ReAct), but as a two-stage Mixed-Integer Programming (MIP) problem that solves for the mathematically optimal trajectory (tool selection + set operations) on training data. These 'golden trajectories' are then used as retrieved in-context demonstrations to steer the VLM at inference time, achieving SOTA on CIR benchmarks with only 10% of training data. **Key Takeaway:** We can steal this 'Offline MIP $\\to$ Online ICL' paradigm. Instead of relying on noisy online RL or expensive evolutionary loops to guide our AlgoEvo agents, we can solve MIPs on training instances to generate optimal reasoning traces, effectively 'solving' the prompt engineering problem via OR.", "methodology": {"core_method": "Two-stage Mixed-Integer Programming (MIP) for optimal trajectory derivation, followed by VLM-steered in-context learning for inference", "llm_role": "planner_steered_by_optimization", "llm_model_used": "Qwen3-VL-32B", "search_type": "hybrid", "novelty_claim": "We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem.", "components": ["Two-stage Mixed-Integer Programming", "Atomic Retrieval Construction", "Recall-Oriented Selection MIP", "Precision-Oriented Composition MIP", "Golden Library", "VLM Planner", "VLM Verifier", "Set-theoretic operations (union, intersection, difference)"], "training_required": false}, "tags": {"methods": ["mixed_integer_programming", "set_operations", "llm_in_the_loop", "llm_as_planner", "llm_as_evaluator", "in_context_learning", "optimization_steered_planning", "agentic_ai"], "problems": ["composed_image_retrieval", "text_to_image_retrieval"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": null, "specific_domain": "composed_image_retrieval", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Composed Image Retrieval", "short": "CIR", "class_": "image_retrieval", "properties": ["multimodal", "compositional_reasoning", "heterogeneous_constraints", "agentic_planning", "optimization_steered"], "scale": "up to 123,403 images and 28,225 queries"}, "lineage": {"direct_ancestors": [{"paper": "MRA-CIR", "relationship": "improves upon heuristic agentic planning in"}, {"paper": "XR", "relationship": "improves upon heuristic agentic planning in"}], "closest_prior_work": "XR", "novelty_type": "paradigm_shift"}, "extensions": {"next_steps": ["extend optimization-steered paradigm to multi-hop question answering", "extend optimization-steered paradigm to open-ended tool use", "apply to other complex reasoning tasks"], "transferable_to": ["multi_hop_question_answering", "open_ended_tool_use", "complex_reasoning_tasks"], "open_weaknesses": ["polarity mismatch in negative evidence", "offline optimization is computationally intensive", "offline optimization requires ground truth", "performance relies on VLM's tool-calling abilities"]}, "artifacts": {"code_url": "https://anonymous.4open.science/r/OSCAR-3A55/README.md", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.19, "experiments": {"benchmarks": ["CIRCO", "CIRR", "FashionIQ (dress)", "FashionIQ (shirt)", "FashionIQ (toptee)", "Private Industrial Photo Galleries"], "baselines": ["Ops-MM-embedding-v1-7B", "RzenEmbed-v2-7B", "VLM2Vec", "B3-Qwen2-7B", "QQMM-embed-v2", "bge-m3", "Qwen3-Embedding-0.6B", "Qwen3-Embedding-4B", "Qwen3-Embedding-8B", "Pic2Word", "SEARLE", "SEARLE-XL-OTI", "CIReVL", "LinCIR", "LDRE", "FiRE", "MRA-CIR", "AutoCIR", "𝑋𝑅"], "hardware": "A100 GPUs", "instance_sizes": [123403, 28225]}, "results": {"vs_baselines": {"QQMM-embed-v2": "+23.13% mAP@5 on CIRCO, +76.60% Recall@1 on CIRR", "FiRE": "+82.2% mAP@5 on CIRCO, +18.67% Recall@1 on CIRR", "RzenEmbed-v2-7B": "+1.25% Recall@10 on FashionIQ (Avg)", "Ops-MM-v1": "+14.26% NDCG@10, +21.74% Recall@10 on Private Galleries (Avg)", "MRA-CIR": "+15.54% Recall@10 on FashionIQ (Avg)"}, "scalability": "Demonstrates strong generalization of planning logic with only 10% of training data and stable performance across varying numbers of demonstrations.", "statistical_rigor": "Performance reported using Recall@K, mAP@K, and NDCG@K across multiple public and private datasets; robustness to number of demonstrations analyzed.", "limitations_acknowledged": ["Polarity mismatch in negative evidence", "Offline optimization is computationally intensive and requires ground truth", "Single-stage MIP is computationally intractable for large datasets", "Performance relies on VLM's tool-calling abilities"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2601.21008", "arxiv_url": "https://arxiv.org/abs/2601.21008", "title": "Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and Behavioral Rationality in Operations Research", "authors": ["Ruicheng"], "abstract": "", "published_date": "2026-02-08", "affiliations": "Massachusetts Institute of Technology, Alibaba Group", "category": "OR for Generative AI", "relevance": {"methodological": 9, "problem": 8, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper provides a concrete blueprint for 'RL-infused evolution' in OR by successfully applying GRPO and Process Reward Models (PRM) to optimization tasks. It demonstrates that small, specialized models trained with solver feedback (IIS) outperform frontier models, validating a shift from prompting to RL-finetuning for our AlgoEvo work."}, "brief": "Ao et al. introduce a framework for iterative OR model debugging that trains an 8B model using Group Relative Policy Optimization (GRPO) and a Process Reward Model (PRM) to outperform GPT-4o-mini. They utilize Gurobi's Irreducible Infeasible Subsystem (IIS) not just as text feedback, but as a dense reward signal (IIS size reduction) for the PRM, achieving a 95.3% recovery rate versus 86.2% for frontier APIs. **Key Takeaway:** We should steal their PRM construction method—specifically using solver diagnostics (like IIS reduction or compiler error counts) as dense step-level rewards—and their 'faithfulness penalty' to prevent overfitting in our evolutionary search. This is a direct validation of RLVR (Reinforcement Learning with Verifiable Rewards) for OR, proving it superior to large-scale prompting.", "methodology": {"core_method": "Domain-specific Group Relative Policy Optimization (GRPO) with composite reward and three-stage curriculum learning", "llm_role": "agent_for_debugging_and_decision_making", "llm_model_used": "Qwen3-8B-Instruct", "search_type": "improvement", "novelty_claim": "We introduce two benchmarks that place the solver in the evaluation loop, enabling targeted training that allows an 8B model to surpass frontier APIs in OR self-correction and bias mitigation.", "components": ["Supervised Fine-Tuning (SFT)", "Group Relative Policy Optimization (GRPO)", "Composite Reward Function", "Process Reward Model (PRM)", "Curriculum Learning", "Saboteur-based problem generation", "Gurobi 11.0 IIS computation"], "training_required": true}, "tags": {"methods": ["llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "supervised_learning", "rl_ppo", "grpo", "curriculum_learning", "process_reward_model", "reinforcement_learning_with_verifiable_rewards"], "problems": ["llm_evaluation_for_or", "infeasibility_diagnosis", "newsvendor_problem", "llm_self_correction", "behavioral_bias_mitigation", "linear_programming_debugging"], "contribution_type": ["new_benchmark", "new_method", "sota_result", "empirical_study", "framework"], "framework_lineage": "grpo", "specific_domain": null, "llm_coupling": "rl_trained"}, "problem": {"formal_name": "LLM Evaluation for Iterative Debugging of Infeasible Optimization Models and Behavioral Rationality in Operations Management Decisions (Newsvendor Problem)", "short": "OR Debugging & Newsvendor", "class_": "llm_evaluation_for_or", "properties": ["iterative_debugging", "self_correction", "deterministic_feedback", "verifiable_oracle", "behavioral_bias_mitigation", "infeasibility_diagnosis", "inventory_decision"], "scale": "OR-Debug-Bench: 5,000+ problems, IIS 1-11 constraints; OR-Bias-Bench: 2,000 newsvendor instances, demand mean 50-200"}, "lineage": {"direct_ancestors": [{"paper": "CorrectBench", "relationship": "extends self-correction evaluation to OR domain"}, {"paper": "AIM-Bench", "relationship": "extends behavioral bias evaluation to newsvendor problem"}, {"paper": "DeepSeek-R1", "relationship": "applies GRPO algorithm for LLM training"}], "closest_prior_work": "CorrectBench", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["mip_minlp_debugging", "multi_period_operations", "rag_integration_with_or_knowledge_bases", "multi_agent_collaboration"], "transferable_to": ["other_optimization_problem_debugging", "iterative_scientific_simulation_debugging", "complex_engineering_design_validation", "other_operations_management_decision_problems"], "open_weaknesses": ["current_focus_on_linear_programming", "single_period_operations_in_bias_bench", "lack_of_rag_integration", "single_agent_evaluation_framework"]}, "artifacts": {"code_url": null, "models_released": true, "new_benchmark": true}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.44, "experiments": {"benchmarks": ["OR-Debug-Bench", "OR-Bias-Bench"], "baselines": ["o4-mini", "claude-sonnet-4", "gpt-5.2-chat", "Llama-3.3-70B", "DeepSeek-R1", "SFT-only models"], "hardware": "2x NVIDIA A100 80GB, training ~8 GPU-hours", "instance_sizes": [450, 600]}, "results": {"vs_baselines": {"Llama-3.1-8B-GRPO": "+11.1% RR@5 (97.3% vs 86.2% for best API)", "Qwen3-8B-GRPO": "+14.6% DA (62.4% vs 47.8% for o4-mini) and 1.4-2.8x faster repair (2.25 steps vs 3.15-5.08 steps for API models)", "Qwen3-8B-Curriculum": "-9.6% ID→OOD bias drift (only negative drift among models evaluated), reducing systematic bias by 48% (from 20.0% to 10.4%)"}, "scalability": "Domain-specific models achieve superior sample efficiency, requiring 1.87x fewer tokens per successful solution; hard problems show steeper scaling (+26.8% RR@k from k=1 to k=5) than easy problems (+9.8%).", "statistical_rigor": "Evaluated 26 models on 12,000+ samples, using 450 stratified OR-Debug-Bench samples per model and 600 OR-Bias-Bench samples (400 ID + 200 OOD) with stratified sampling by CR buckets.", "limitations_acknowledged": ["MIP/MINLP debugging", "multi-period operations", "RAG integration with OR knowledge bases", "multi-agent collaboration"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2602.07663", "arxiv_url": "https://arxiv.org/abs/2602.07663", "title": "A Two-Layer Framework for Joint Online Configuration Selection and Admission Control", "authors": ["Owen"], "abstract": "", "published_date": "2026-02-07", "affiliations": "Massachusetts Institute of Technology, Stanford University", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper provides a rigorous mathematical proof that static configuration selection (e.g., picking one 'best' quantization level) is suboptimal for multi-resource LLM serving. It offers a concrete primal-dual algorithm to learn optimal configuration mixtures, which is directly applicable to our 'GPUSched' and 'OR formulations for LLM serving' projects."}, "brief": "The authors introduce a 'switching-aware' primal-dual framework for joint configuration selection (e.g., quantization, parallelism) and admission control, demonstrating that dynamically mixing configurations allows for higher resource utilization than any single fixed configuration. Results are rigorous, backed by $\\tilde{O}(\\sqrt{T})$ regret bounds and experiments on Alibaba cluster traces where the method achieves ~97% competitive ratio (vs. ~85% for greedy). The key takeaway is the 'switching-aware fluid oracle' concept: our resource allocation models for LLM serving must optimize over the convex hull of configurations (mixing CPU-heavy and Mem-heavy setups) rather than searching for a single static optimum. We should adapt their saddle-point formulation for the GPUSched project to handle heterogeneous resource constraints more effectively.", "methodology": {"core_method": "SP-UCB-OLP algorithm, which solves an optimistic saddle point problem using UCB for exploration and a threshold-based admission rule derived from a switching-aware fluid oracle", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "We unify configuration exploration (from BwK) with observe-then-decide threshold admission (from OLP) through a switching-aware oracle.", "components": ["switching-aware fluid oracle", "optimistic saddle point problem solver", "UCB exploration", "threshold-based admission rule", "linear programming for saddle point", "conservative budget slack"], "training_required": true}, "tags": {"methods": ["online_optimization", "bandits_with_knapsacks", "online_linear_programming", "saddle_point_optimization", "ucb", "threshold_rules", "linear_programming", "primal_dual_methods", "regret_analysis"], "problems": ["resource_allocation", "admission_control", "configuration_selection", "llm_serving_optimization", "gpu_scheduling", "revenue_management", "online_decision_making"], "contribution_type": ["new_method", "framework", "theoretical_result", "empirical_study"], "framework_lineage": null, "specific_domain": null, "llm_coupling": null}, "problem": {"formal_name": "Online Configuration Selection with Admission Control Problem", "short": "OCSAC", "class_": "resource_allocation", "properties": ["online", "two_layer_decision", "stochastic_rewards_resources", "cumulative_budget_constraints", "irrevocable_decisions", "configuration_selection", "admission_control"], "scale": "K=2-5 configurations, T=100-5000 periods, d=2-3 resource types"}, "lineage": {"direct_ancestors": [{"paper": "Bandits with knapsacks (Badanidiyuru et al., 2018)", "relationship": "unifies configuration exploration from"}, {"paper": "Online linear programming (Agrawal et al., 2014)", "relationship": "unifies observe-then-decide admission from"}], "closest_prior_work": "Online linear programming (Agrawal et al., 2014)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["extensions to non_stationary_arrivals", "contextual_settings", "delayed_feedback"], "transferable_to": ["llm_serving_optimization", "gpu_scheduling", "revenue_management"], "open_weaknesses": ["i.i.d_and_bounded_reward_resource_pair_assumption"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.23, "experiments": {"benchmarks": ["Gaussian scenario (S0)", "Alibaba Cluster Trace v2018", "Complementarity scenario (S4)"], "baselines": ["Greedy (α=0)", "Random", "Oracle", "OneHot"], "hardware": "null", "instance_sizes": [100, 200, 500, 1000, 2000, 5000]}, "results": {"vs_baselines": {"Greedy": "SP-UCB-OLP (α=0.01) achieves 97.38% CR on Alibaba (vs. Greedy 84.81% CR). On S4, SP-UCB-OLP achieves 0.91 CRmix (vs. Greedy 0.96 CRmix).", "Random": "SP-UCB-OLP (α=0.01) achieves 97.38% CR on Alibaba (vs. Random 61.05% CR). On S4, SP-UCB-OLP achieves 0.91 CRmix (vs. Random 0.99 CRmix).", "Oracle": "SP-UCB-OLP (α=0.01) achieves 97.38% CR on Alibaba, approaching Oracle performance (99.95% CR).", "OneHot": "On S4, OneHot achieves 0.51 CRmix, significantly lower than SP-UCB-OLP (0.91 CRmix), confirming its inability to exploit complementarity."}, "scalability": "Regret/sqrt(T) remains approximately constant (≈1.6) across all horizon lengths, confirming the ˜O(√T) scaling predicted by Theorem 9.", "statistical_rigor": "Experiments use 10 to 50 independent seeds. Results report mean and standard deviation (e.g., ±1.96 SE or ±std).", "limitations_acknowledged": ["i.i.d. and bounded reward-resource pair assumption", "extensions to non-stationary arrivals", "contextual settings", "delayed feedback"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.07263", "arxiv_url": "https://arxiv.org/abs/2602.07263", "title": "tLoRA: Efficient Multi-LoRA Training with Elastic Shared Super-Models", "authors": ["Kevin"], "abstract": "", "published_date": "2026-02-06", "affiliations": "University of Illinois Urbana-Champaign", "category": "OR for Generative AI", "relevance": {"methodological": 5, "problem": 8, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper establishes a strong systems/heuristic baseline for our 'GPUSched' project. It demonstrates how to handle heterogeneous LoRA workloads (varying ranks/batch sizes) efficiently, which we should aim to model and improve upon using formal OR formulations."}, "brief": "tLoRA optimizes multi-tenant LoRA training by fusing heterogeneous adapters into a 'Shared Super-Model' and employing an online scheduler that groups jobs based on residual GPU capacity and urgency. They report 1.2–1.8x throughput improvements and ~5x faster job completion times on A100 clusters compared to mLoRA, backed by realistic trace-driven experiments. For our GPUSched and resource allocation work, their hierarchical incremental grouping strategy serves as the state-of-the-art heuristic baseline we must outperform; additionally, their adaptive nano-batching (AIMD controller) is a transferable engineering trick for overlapping communication in distributed LLM workloads.", "methodology": {"core_method": "Shared Super-Model (SSM) abstraction, fused LoRA kernel with adaptive nano-batching, and online residual-capacity-aware scheduling algorithm", "llm_role": "none", "llm_model_used": null, "search_type": "improvement", "novelty_claim": "tLoRA introduces a Shared Super-Model abstraction, a fused heterogeneous LoRA kernel with adaptive nano-batching, and an online, residual-capacity-aware scheduling algorithm to efficiently batch and train multiple LoRA jobs.", "components": ["Shared Super-Model (SSM) abstraction", "Model Fuser", "Kernel Fuser (fused batched LoRA kernel, adaptive nano-batching)", "Adapter Scheduler (hierarchical incremental grouping, urgency score)"], "training_required": false}, "tags": {"methods": ["low_rank_adaptation", "distributed_training", "model_parallelism", "pipeline_parallelism", "gpu_kernel_optimization", "adaptive_nano_batching", "online_scheduling", "resource_allocation", "job_scheduling", "aimd_controller", "shared_super_model_abstraction", "hierarchical_incremental_grouping"], "problems": ["multi_lora_training_optimization", "gpu_scheduling", "llm_resource_management"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "mlora", "specific_domain": "multi_lora_training_optimization", "llm_coupling": null}, "problem": {"formal_name": "Efficient Batch Training of Multiple Heterogeneous LoRA Jobs", "short": "Multi-LoRA Training Optimization", "class_": "resource_allocation", "properties": ["heterogeneous_jobs", "online_arrival", "shared_backbone", "low_rank_adaptation", "distributed_training"], "scale": "LoRA ranks 2-16, batch sizes 1-8, on clusters up to 256 GPUs"}, "lineage": {"direct_ancestors": [{"paper": "mlora", "relationship": "extends batching policy from"}, {"paper": "Megatron-LM", "relationship": "leverages distributed training capabilities from"}], "closest_prior_work": "mlora", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Extend to other PEFT methods beyond LoRA.", "Investigate dynamic adaptation of LoRA parameters (rank, batch size) during training.", "Explore integration with data parallelism and other distributed training strategies.", "Address fault tolerance and preemption in batched multi-LoRA training."], "transferable_to": ["Other parameter-efficient fine-tuning (PEFT) techniques.", "Multi-task learning with shared backbones.", "General heterogeneous distributed machine learning workloads.", "LLM inference serving with dynamic batching of heterogeneous requests."], "open_weaknesses": ["Potential overhead of online profiling and scheduling decisions in highly dynamic environments.", "Scalability to extremely large numbers of concurrent jobs or GPUs beyond current evaluation.", "Generalizability to diverse LLM architectures and hardware configurations not explicitly tested.", "Lack of explicit mechanisms for fault tolerance or job preemption handling."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.1, "experiments": {"benchmarks": ["ACMETrace", "GSM8K dataset"], "baselines": ["mLoRA (Ye et al., 2025)", "Megatron (Narayanan et al., 2021)", "tLoRA w/o Scheduler", "tLoRA w/o Kernel Fuser"], "hardware": "NVIDIA A100 GPUs (12-GPU testbed, 128-GPU default cluster, up to 256-GPU clusters)", "instance_sizes": [1, 2, 4, 8, 16]}, "results": {"vs_baselines": {"mLoRA": "1.2-1.8x higher training throughput, 2.3-5.4x reduction in job completion time", "Megatron": "Higher throughput than Megatron (which trains independently)", "tLoRA w/o Scheduler": "5.4x slower training completion", "tLoRA w/o Kernel Fuser": "Weakens benefits of job co-location"}, "scalability": "tLoRA consistently sustains near-peak cluster throughput under increasing job concurrency and maintains proportional job completion time across varying cluster sizes (32 to 256 GPUs).", "statistical_rigor": "All reported results are averaged over five independent runs; simulation error is within 3%.", "limitations_acknowledged": []}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.05688", "arxiv_url": "https://arxiv.org/abs/2602.05688", "title": "Mining Generalizable Activation Functions", "authors": ["Alex"], "abstract": "", "published_date": "2026-02-05", "affiliations": "Google DeepMind", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 5, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper provides a successful blueprint for using AlphaEvolve (our primary interest) with 'small-scale lab' proxy tasks to solve the evaluation bottleneck. It demonstrates that evolving on cheap, synthetic OOD data yields components that transfer to large-scale benchmarks, a strategy we should replicate for heuristic discovery."}, "brief": "Vitvitskyi et al. (DeepMind) utilize AlphaEvolve to discover novel activation functions by evolving Python code on small, synthetic datasets explicitly designed to test OOD generalization (e.g., polynomials, Feynman equations). The results are credible and backed by downstream transfer: discovered functions like `GELU * (1 + 0.5 sinc(x))` outperform baselines on algorithmic reasoning tasks (CLRS-30) while matching standard vision benchmarks. **Key Takeaway:** The 'Small-Scale Lab' methodology—optimizing on cheap, synthetic proxy tasks to find generalizable logic—is a validated strategy to bypass the computational bottleneck of evaluating evolved candidates on large-scale instances. We should steal this 'proxy evolution' setup for AlgoEvo to drastically reduce evaluation costs while targeting generalization in VRP heuristics.", "methodology": {"core_method": "Evolutionary search powered by AlphaEvolve framework", "llm_role": "code_writer", "llm_model_used": "Gemini", "search_type": "sampling", "novelty_claim": "The paper introduces an evolutionary search for activation functions using frontier LLMs to explore an unbounded space of Python functions, optimizing for out-of-distribution generalization as the fitness function.", "components": ["AlphaEvolve framework", "LLMs ensemble", "Prompt sampler", "Best activation functions DB", "Evaluators (MLP training loop, OOD test)", "Synthetic datasets"], "training_required": true}, "tags": {"methods": ["evolutionary_search", "alphaevolve", "llm_evolutionary_search", "llm_code_generation", "multilayer_perceptron"], "problems": ["activation_function_search", "operator_discovery", "out_of_distribution_generalization", "image_classification", "algorithmic_reasoning", "graph_classification", "symbolic_regression"], "contribution_type": ["new_method", "framework", "empirical_study", "sota_result"], "framework_lineage": "alphaevolve", "specific_domain": "operator_discovery", "llm_coupling": "llm_evolutionary_search"}, "problem": {"formal_name": "Mining Generalizable Activation Functions", "short": "Activation Function Search", "class_": "program_synthesis", "properties": ["generalizable", "out_of_distribution", "LLM-guided", "small_scale_discovery", "function_synthesis"], "scale": "small-scale synthetic data (1D to 20D polynomials), and downstream tasks including CIFAR-10, ImageNet-1K, CLRS-30 (up to length 64), and ogbg-molhiv (41127 molecules)"}, "lineage": {"direct_ancestors": [{"paper": "Novikov et al., 2025", "relationship": "relies on and applies AlphaEvolve framework"}, {"paper": "Ramachandran et al., 2018", "relationship": "departs from previous activation function search methods by exploring an unbounded function space"}], "closest_prior_work": "AlphaEvolve (Novikov et al., 2025)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["theoretical analysis of how discovered functions capture regularities", "explore other neural network components beyond activation functions", "improve generalization of batch-statistic-dependent activation functions"], "transferable_to": ["other neural network architectural components (e.g., optimizers, layers)", "other domains requiring strong out-of-distribution generalization", "program synthesis for other scientific or algorithmic discovery tasks"], "open_weaknesses": ["batch-statistic-dependent functions can lead to OOM issues and poor generalization", "some discovered functions (e.g., Turbulent) overfit to synthetic data", "LLM-generated rationales for functions can be questionable"]}, "artifacts": {"code_url": "https://github.com/Aastha2104/Parkinson-Disease-Prediction", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.55, "experiments": {"benchmarks": ["CIFAR-10", "ImageNet-1K", "CLRS-30", "ogbg-molhiv", "Feynman Symbolic Regression Dataset", "Random Polynomials (1d)", "Random Polynomials (20d)", "Spherical Harmonics", "Random sin Products"], "baselines": ["ReLU", "GELU"], "hardware": "null", "instance_sizes": [64, 41127, 50000, 60000]}, "results": {"vs_baselines": {"GELU-Sinc-Perturbation": "outperforms ReLU by +2.9% on CLRS-30 test score and +2.6% on ogbg-molhiv AUC, and GELU by +1.5% on CLRS-30 and +2.4% on ogbg-molhiv, while maintaining competitive performance on image tasks. Turbulent achieved -68.0% lower test loss than ReLU on synthetic data but underperformed on downstream tasks."}, "scalability": "Discovered activation functions, particularly GELU-Sinc-Perturbation, demonstrate strong generalization from small-scale synthetic data to larger, more complex downstream tasks like ImageNet and CLRS-30.", "statistical_rigor": "The results are reported as mean test scores across algorithms for CLRS-30; no explicit statistical significance tests or variance measures are provided for the main downstream task results.", "limitations_acknowledged": ["Some batch-statistic-dependent activation functions lead to Out-Of-Memory (OOM) issues and poor generalization on downstream tasks", "the Turbulent activation function overfit to synthetic data and failed to transfer", "LLM-generated rationales for functions can be questionable"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.04431", "arxiv_url": "https://arxiv.org/abs/2602.04431", "title": "MaMa: A Game-Theoretic Approach for Designing Safe Agentic Systems", "authors": ["Jonathan"], "abstract": "", "published_date": "2026-02-04", "affiliations": "Max Planck Institute for Software Systems", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 8, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper sits at the exact intersection of our 'AlgoEvo' (automated design) and 'RobustMAS' (robust multi-agent optimization) projects. It successfully demonstrates that integrating an active adversarial search loop (co-evolution) into the system design process yields superior robustness compared to static guardrails, a methodology we should adopt."}, "brief": "MaMa automates the design of multi-agent systems by formulating the problem as a Stackelberg Security Game: a Meta-Agent evolves system architectures (tools, communication graphs) while a Meta-Adversary iteratively optimizes worst-case agent compromises to break them. Empirical results on the BAD-ACTS benchmark show this adversarial co-evolution reduces attack success rates from ~50% (static baselines) to ~15-25% without degrading task quality. The critical takeaway is the implementation of an **adversarial co-evolution loop** within the architecture search—optimizing the 'threat' alongside the 'solution'—which directly addresses the robustness objectives in our RobustMAS project. We should implement this 'Meta-Adversary' concept to stress-test our evolved algorithms during the search phase rather than post-hoc.", "methodology": {"core_method": "Iterative Stackelberg Security Game solving via LLM-based adversarial search, building on AFlow for system design updates", "llm_role": "decomposition_guide, prompt_optimizer, heuristic_generator, evaluator", "llm_model_used": "GPT-5.1, Qwen3:32b, Llama-3.3:70b", "search_type": "hybrid", "novelty_claim": "We introduce the first framework for the automated design of agentic systems which ensures that the designed system is safe in cases of agent failures.", "components": ["Meta-Agent", "Meta-Adversary", "Stackelberg Security Game formulation", "LLM-based adversarial search", "System design archive", "Quality judge LLM", "Safety judge LLM", "Structured agentic system definition", "AFlow-inspired sampling strategy"], "training_required": true}, "tags": {"methods": ["game_theory", "stackelberg_security_game", "llm_adversarial_search", "automated_system_design", "multi_agent_systems", "iterative_optimization", "archive_based_search", "llm_as_heuristic", "llm_as_evaluator", "aflow"], "problems": ["automated_agentic_system_design", "llm_safety", "adversarial_robustness", "agent_safety", "multi_agent_coordination", "travel_planning", "personal_assistant_tasks", "financial_article_writing", "code_generation"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "aflow", "specific_domain": null, "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Automated Design of Safe Agentic Systems", "short": "Agentic System Design", "class_": "Adversarial LLM System Design", "properties": ["multi-agent", "LLM-based", "adversarial robustness", "safety-critical", "game-theoretic"], "scale": "Systems with multiple LLM agents, up to 5 compromised agents"}, "lineage": {"direct_ancestors": [{"paper": "Zhang et al., 2025", "relationship": "extends AFlow by introducing a Meta-Adversary for safety optimization"}], "closest_prior_work": "Aflow (Zhang et al., 2025)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Replace LLM-based evaluation with regression models for performance and safety prediction", "Investigate zero-sum settings where Meta-Agent and Meta-Adversary both optimize quality", "Explore adversaries capable of manipulating tools or communication graphs", "Reduce computational effort required for system evaluation"], "transferable_to": ["Other multi-agent systems requiring adversarial robustness", "Safety-critical AI systems beyond LLM agents", "Systems with stronger adversaries (more compromised agents)", "Systems with different underlying LLMs or attack objectives"], "open_weaknesses": ["High computational cost due to dual evaluation (clean and adversarial)", "Adversary currently only minimizes safety, not optimizing quality", "Limited adversary scope (only agent instructions, not tools/graphs)", "Scalability to very large numbers of adversarial agents"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.96, "experiments": {"benchmarks": ["BAD-ACTS benchmark", "Travel Planning environment", "Personal Assistant environment", "Financial Article Writing environment", "Code Generation environment"], "baselines": ["Guardian-Agents (N¨other et al., 2025)", "AFlow (Zhang et al., 2025) (quality-only optimized MaMa variant)", "Least-Privilege Tool Gating (LPTG)", "Tool Filters (TF)", "Delimiters (Hines et al., 2024)"], "hardware": "CPU: 2x AM EPYC 9555, Memory: 24x 96GB, GPUs: 2x Nvidia H200 141GB NVL", "instance_sizes": [1, 2, 3, 4, 5]}, "results": {"vs_baselines": {"LPTG + TF (initial systems)": "Meta-Adversary significantly decreased safety (e.g., Travel Planning: 4.89 to 1.66)", "MaMa (quality-only optimized / AFlow)": "MaMa systems significantly safer, often with similar or increased quality", "Guardian Agents": "MaMa significantly safer on targeted attacks (e.g., Travel Planning ASR: 0.1700 vs 0.5237)", "LPTG + TF + Delimiters": "MaMa significantly outperforms or matches best defense on indirect tool injection (e.g., Travel Planner ASR: 0.0068 vs 0.1020/0.1904)"}, "scalability": "Most systems remain safe when the adversary controls up to three agents; safety drops steeply for more than four adversarial agents.", "statistical_rigor": "Results reported as mean and standard deviation over three distinct runs/evaluations. Judge models evaluated with Quadratic Weighted Cohen’s Kappa (κ=0.88 for safety, κ=0.8001 for quality).", "limitations_acknowledged": ["Significant computational effort required", "Adversaries focused only on minimizing safety, not optimizing quality", "Future work could explore zero-sum settings for quality optimization", "Future work could explore adversaries manipulating tools or communication graphs"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2602.04296", "arxiv_url": "https://arxiv.org/abs/2602.04296", "title": "ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas", "authors": ["Wenjun"], "abstract": "", "published_date": "2026-02-04", "affiliations": "", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 5, "problem": 7, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "Provides strong empirical evidence that static metrics (Pass@k) fail to predict agent performance, directly impacting how we should design fitness functions for AlgoEvo. Also demonstrates that reasoning-tuned models (DeepSeek-R1) outperform code-tuned models for logic tasks, guiding our base model selection."}, "brief": "ProxyWar introduces a tournament-based evaluation framework for LLM-generated code, using TrueSkill ratings from game simulations (Sudoku, Poker, etc.) instead of static unit tests. The results are robust (10k+ matches) and reveal a low correlation between Pass@1 and actual win rates; notably, 'reasoning' models like DeepSeek-R1 crush 'coding' models like Qwen-Coder in strategic tasks despite lower static scores. For our evolutionary search work, this confirms that we must move beyond static benchmarks to dynamic, competitive evaluation signals to avoid optimizing for syntax over strategy. We should also prioritize reasoning models over code-specialized ones for our agentic logic generation.", "methodology": {"core_method": "ProxyWar framework, a competitive, execution-based evaluation system orchestrating automated code generation, hierarchical testing, iterative repair loops, and multi-agent tournaments with TrueSkill-based ranking.", "llm_role": "code_writer", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "ProxyWar is the first competitive, execution-based evaluation framework for LLM code generation, orchestrating automated execution, testing, and iterative repair mechanisms to evaluate code quality through competitive gameplay.", "components": ["Prompt Manager", "Code Generation Layer", "Testing Layer (hierarchical unit tests)", "Agent Layer", "Tournament Management Layer", "Iterative Code Repair Loop", "TrueSkill Rating System"], "training_required": false}, "tags": {"methods": ["llm_code_generation", "llm_as_agent", "llm_in_the_loop", "iterative_repair", "multi_agent_systems", "competitive_evaluation", "trueskill_ranking", "automated_testing", "program_synthesis"], "problems": ["llm_code_generation_evaluation", "game_ai", "sudoku", "2048", "tower_of_hanoi", "maze", "tic_tac_toe", "connect_four", "reversi", "snake", "texas_holdem"], "contribution_type": ["new_method", "framework", "empirical_study"], "framework_lineage": "proxywar", "specific_domain": "llm_code_generation_evaluation", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Dynamic Assessment of LLM Code Generation in Game Arenas", "short": "LLM Code Gen Eval", "class_": "Program Evaluation Frameworks", "properties": ["dynamic", "competitive", "iterative_repair", "multi_agent", "operational_characteristics"], "scale": "Game environments ranging from simple (Tic-Tac-Toe, 1.97x10^4 states) to complex (Reversi, ~10^28 states; Sudoku, 6.67x10^21 states), with action spaces up to 10 actions."}, "lineage": {"direct_ancestors": [], "closest_prior_work": "llm_code_generation_benchmarking", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["scaling_to_real_world_software_development", "addressing_llm_stochasticity", "expanding_game_environment_diversity", "evaluating_long_term_code_maintainability"], "transferable_to": ["automated_bug_fixing", "llm_agent_development", "competitive_programming_platforms", "general_ai_evaluation_frameworks"], "open_weaknesses": ["limited_game_environment_complexity", "scalability_challenges_for_large_problems", "llm_stochasticity_impact_on_results", "incomplete_code_quality_metrics"]}, "artifacts": {"code_url": "https://github.com/xinke-wang/ProxyWar", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.98, "experiments": {"benchmarks": ["Sudoku", "2048 (4x4)", "Tower of Hanoi", "Maze (grid)", "Tic-Tac-Toe", "Connect Four", "Reversi", "Snake (2-player)", "Texas Hold’em (Limit)"], "baselines": ["Qwen2.5-72B", "Claude3.5-Sonnet", "Phi4", "Gemini2.0-Flash", "DeepSeekV3-0324", "Llama4-Maverick", "GPT4.1-Mini-20250414", "GPT4.1-20250414", "O3-Mini-20250131", "Qwen3-235B-0428", "DeepSeek-R1-0528", "Claude4-Sonnet-20250522", "Gemini-2.5-Flash", "Magistral-Small-2506", "Qwen2.5-Coder", "Codestral-2501", "Mercury-Coder", "Codex-Mini"], "hardware": "Intel i9-12900KF (3.19 GHz), 64 GB RAM, NVIDIA RTX A5000 (24 GB)", "instance_sizes": []}, "results": {"vs_baselines": {"DeepSeek-R1-0528": "Best overall win rate (39.6%) across all games.", "O3-Mini-20250131": "Second best overall win rate (38.8%) across all games.", "Qwen2.5-Coder": "Lowest overall win rate (13.4%), nearly threefold gap compared to DeepSeek-R1.", "DeepSeekV3-0324": "Outperforms Claude3.5-Sonnet by nearly 15 percentage points in win rate despite both having perfect participation.", "Code-specialized models": "Average win rate of 23.6%, which is lower than general-purpose models (28.6%) and reasoning-enhanced models (31.6%)."}, "scalability": "Performance differences among models widen significantly as game complexity increases (e.g., from Tic-Tac-Toe with 10^4 states to Reversi with 10^28 states), indicating varying scalability of generated algorithms.", "statistical_rigor": "Experiments ran for five rounds, with each model generating a fresh agent per round, participating in over 10,000 matches in total. Random seeds were fixed for prompt sampling and environment initialization. TrueSkill rating system used Bayesian inference, reporting conservative skill estimates (mu - 3*sigma) for 99.7% confidence.", "limitations_acknowledged": ["Game environments may not capture the full complexity of real-world software development.", "Scaling evaluation to broader or industry-scale problems poses engineering and cost challenges.", "LLM-based coders exhibit inherent stochasticity, leading to potential result variations across repeated runs.", "The selection of game environments may not fully capture the variety of real-world programming tasks.", "Evaluation relies on specific implementations and external dependencies, which may affect replicability over time.", "No evaluation can exhaustively capture all aspects of code quality, including long-term maintainability or human factors.", "Inadequate or unrepresentative test suites may overestimate capabilities."]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2602.04529", "arxiv_url": "https://arxiv.org/abs/2602.04529", "title": "Landscape-aware Automated Algorithm Design: An Efficient Framework for Real-world Optimization", "authors": ["Haoran"], "abstract": "", "published_date": "2026-02-04", "affiliations": "", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 7, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper provides a concrete architectural blueprint for decoupling LLM evolutionary search from expensive evaluations, a primary bottleneck in our AlgoEvo work. The concept of evolving symbolic proxies to match landscape features is a transferable technique we can adapt."}, "brief": "Yin et al. introduce a framework that decouples algorithm discovery from expensive evaluations by using Genetic Programming to evolve symbolic proxy functions that statistically match the target problem's landscape (via ELA features). Empirical results on photonics problems confirm that algorithms evolved on these cheap proxies transfer successfully to the real tasks, outperforming standard baselines like LSHADE with only 50×D real evaluations. **Key Takeaway:** We can synthesize 'symbolic gyms' that statistically mimic our target problems to run thousands of LLM iterations at near-zero cost. This directly addresses the sample efficiency bottleneck in AlgoEvo and suggests we should move beyond standard neural surrogates to evolved symbolic proxies.", "methodology": {"core_method": "Hybrid framework combining Genetic Programming (GP) for proxy function generation and an LLM-driven Evolutionary Algorithm (LLaMEA) for algorithm discovery, guided by Exploratory Landscape Analysis (ELA) features and Wasserstein distance.", "llm_role": "algorithm_designer", "llm_model_used": "gpt-4o-2024-05-13", "search_type": "hybrid", "novelty_claim": "The framework decouples algorithm discovery from high-cost real-world evaluations by combining a GP function generator with an LLM-driven evolutionary algorithm designer, guided by landscape similarity between generated proxy functions and real-world problems.", "components": ["Genetic Programming (GP) function generator", "LLaMEA (Large Language Model Evolutionary Algorithm)", "Exploratory Landscape Analysis (ELA) features", "Wasserstein distance", "Evolution Strategy (ES)"], "training_required": true}, "tags": {"methods": ["genetic_programming", "evolutionary_algorithm", "llm_evolutionary_search", "llamea", "exploratory_landscape_analysis", "wasserstein_distance", "evolution_strategy", "llm_code_generation"], "problems": ["automated_algorithm_design", "expensive_continuous_optimization", "black_box_optimization"], "contribution_type": ["new_method", "framework", "sota_result"], "framework_lineage": "llamea", "specific_domain": "expensive_continuous_optimization", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Automated Algorithm Design for Single-Objective Continuous Optimization Problems with Expensive Evaluations", "short": "AAD-SOCO-EE", "class_": "optimization", "properties": ["high_dimensionality", "multimodality", "noise", "expensive_evaluation", "single_objective", "continuous"], "scale": "2 to 40 decision variables"}, "lineage": {"direct_ancestors": [{"paper": "LLaMEA: A Large Language Model Evolutionary Algorithm for Program Synthesis", "relationship": "extends the LLaMEA framework by integrating landscape-aware proxy functions"}, {"paper": "Exploratory Landscape Analysis", "relationship": "incorporates features from"}], "closest_prior_work": "LLaMEA: A Large Language Model Evolutionary Algorithm for Program Synthesis", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Improve robustness against stochasticity of AAD and LLMs", "Develop adaptive proxy function generation for varying problem dimensions", "Explore performance with extended evaluation budgets", "Investigate multi-objective or constrained optimization problems"], "transferable_to": ["Other expensive black-box optimization problems", "Multi-objective continuous optimization", "Optimization problems with noisy evaluations", "Algorithm design for discrete optimization problems"], "open_weaknesses": ["Occasional suboptimal performance due to inherent stochasticity", "Proxy functions may prioritize less relevant features for low-dimensional problems", "Current evaluation budget (50*D) is restrictive", "Dependency on LLM capabilities and potential for hallucination"]}, "artifacts": {"code_url": "10.5281/zenodo.18385405", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_5", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.86, "experiments": {"benchmarks": ["Meta-surface design", "mini-Bragg", "Bragg", "Ellipsometry inverse problem", "Photovoltaic design", "BBOB test suite"], "baselines": ["Random Search (RS)", "Differential Evolution (DE)", "LSHADE", "LLaMEA (Real-World Direct)", "LLaMEA (Benchmark-Driven)"], "hardware": "null", "instance_sizes": [2, 20, 40]}, "results": {"vs_baselines": {"Random Search (RS)": "Outperformed by proxy-driven algorithms on most real-world problems (meta-surface, mini-Bragg, Bragg, photovoltaic).", "Differential Evolution (DE)": "Outperformed by proxy-driven algorithms on most real-world problems (meta-surface, mini-Bragg, Bragg, photovoltaic).", "LSHADE": "Outperformed by proxy-driven algorithms on most real-world problems (meta-surface, mini-Bragg, Bragg, photovoltaic).", "LLaMEA (Benchmark-Driven)": "Consistently inferior to proxy-driven algorithms across all problems.", "LLaMEA (Real-World Direct)": "Generally yielded higher efficacy than proxy-driven, but proxy-driven outperformed it on the meta-surface problem."}, "scalability": "The framework significantly curtails search overhead, requiring at least a tenfold reduction in high-cost real-world evaluations, and proxy-driven algorithms show a 'rapid-start' benefit in convergence.", "statistical_rigor": "Results are reported over 10 independent trials, with performance metrics (AOCC, fitness curves) averaged over these runs and presented with violin plots for distribution analysis.", "limitations_acknowledged": ["Occasional suboptimal performance due to inherent stochasticity of AAD and LLMs.", "Proxy functions might prioritize complex landscape features less relevant for low-dimensional problems.", "The evaluation budget of 50 * D is restrictive, suggesting potential for further improvement with extended budgets."]}, "analysis_date": "2026-02-12"}, {"arxiv_id": "2602.00509", "arxiv_url": "https://arxiv.org/abs/2602.00509", "title": "PROBE: Co-Balancing Computation and Communication in MoE Inference via Real-Time Predictive Prefetching", "authors": ["Qianchao"], "abstract": "", "published_date": "2026-02-03", "affiliations": "Kling Infra, Kuaishou Technology", "category": "OR for Generative AI", "relevance": {"methodological": 5, "problem": 9, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper defines the state-of-the-art for 'LLM serving optimization' (one of our active tracks). It creates a specific architectural slot (the lookahead window) where our more advanced OR/optimization methods could replace their simple greedy heuristic."}, "brief": "PROBE optimizes MoE inference by using a distilled MLP to predict next-layer expert activation, enabling proactive load balancing and weight prefetching hidden behind the current layer's computation. The results are strong (1.3x speedup on 235B models) and demonstrate that control plane overheads can be fully masked. The critical takeaway for our `GPUSched` project is the **Lookahead Pipelining** architecture: it carves out a deterministic execution window where we could inject our own specialized solvers (e.g., fast ALNS or IP formulations) to outperform their basic greedy resource allocator. This transforms the stochastic serving problem into a short-horizon deterministic routing problem we are well-equipped to solve.", "methodology": {"core_method": "Continuous Lookahead Pipelining with Gate-Initialized Lookahead Predictor, Hardware-Aware Balance Planning, and Phase-Locked Co-Scheduling", "llm_role": "none", "llm_model_used": null, "search_type": "improvement", "novelty_claim": "PROBE introduces Continuous Lookahead Pipelining, which proactively predicts, plans, and prefetches for upcoming layers while keeping all control overheads off the critical path.", "components": ["Continuous Lookahead Pipelining", "Gate-Initialized Lookahead Predictor", "Hardware-Aware Balance Planning solver", "Phase-Locked Co-Scheduling policy", "Split-phase transmission", "Online distillation", "Greedy rebalancing strategy", "Dual-track execution model"], "training_required": true}, "tags": {"methods": ["llm_inference_optimization", "mixture_of_experts", "expert_parallelism", "load_balancing", "predictive_scheduling", "pipelining", "resource_allocation", "greedy_algorithm", "online_distillation", "real_time_optimization", "system_design", "cuda_graph", "nvshmem", "triton", "all_to_all_collectives", "grouped_gemm", "straggler_mitigation"], "problems": ["moe_inference_optimization", "llm_serving_optimization", "latency_critical_inference", "throughput_optimization", "distributed_system_performance", "computation_communication_co_balancing"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "sglang", "specific_domain": "moe_inference_optimization", "llm_coupling": null}, "problem": {"formal_name": "Mixture-of-Experts Inference Optimization with Dynamic Load Balancing", "short": "MoE Inference Optimization", "class_": "llm_inference_optimization", "properties": ["latency_critical", "real_time", "dynamic_workload", "sparse_experts", "distributed_inference", "computation_communication_co_balancing"], "scale": "120B-235B parameter MoE LLMs on 8 GPUs, up to 640K tokens"}, "lineage": {"direct_ancestors": [{"paper": "Zheng et al., 2024", "relationship": "implemented atop SGLang framework"}, {"paper": "Zhao et al., 2025", "relationship": "integrates DeepEP as communication backend"}, {"paper": "Zhao et al., 2025", "relationship": "addresses limitations of DeepSeek-EPLB's reactive approach"}], "closest_prior_work": "DeepSeek-EPLB (Zhao et al., 2025)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Leverage prediction signal for pre-dispatching hidden states to high-confidence experts.", "Overlap entire All-to-All communication latency with routing computation.", "Explore more advanced planning solvers beyond greedy heuristic.", "Extend predictor to handle more complex cross-layer dependencies."], "transferable_to": ["Disaggregated expert parallelism frameworks.", "Other distributed ML models with dynamic workload patterns.", "General latency-critical distributed systems requiring proactive resource management.", "Cloud scheduling for dynamic resource allocation."], "open_weaknesses": ["Greedy rebalancing strategy may not achieve global optimum.", "Predictor accuracy, while high, is not 100%, leading to potential sub-optimal transfers.", "Memory overhead from expert replication, though managed, is still a trade-off.", "System's reliance on a 'hiding window' might be challenged by extremely fast compute kernels or very slow interconnects."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": true}, "front_id": "or_for_generative_ai_2026-02-18_front_3", "front_status": "growing", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.63, "experiments": {"benchmarks": ["Qwen3-MoE-235B", "GPT-OSS-120B", "Chinese dataset", "Code dataset", "Repeat dataset"], "baselines": ["SGLang", "DeepSeek-EPLB"], "hardware": "8xNVIDIA Hopper-141GB node interconnected via 900 GB/s NVSwitch", "instance_sizes": [265000, 310000, 328000, 640000, 512, 1536]}, "results": {"vs_baselines": {"SGLang": "1.32x speedup in prefill latency", "DeepSeek-EPLB": "1.26x higher decoding throughput"}, "scalability": "PROBE maintains stable, high-throughput trajectory across abrupt semantic shifts and workload volatility, adapting instantly to new datasets without lag, with gains more pronounced on sparser models.", "statistical_rigor": "Results averaged over 500 decoding steps and 35 layers. No explicit variance or significance tests reported.", "limitations_acknowledged": []}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.02943", "arxiv_url": "https://arxiv.org/abs/2602.02943", "title": "3D-Learning: Diffusion-Augmented Distributionally Robust Decision-Focused Learning", "authors": ["Jiaqi"], "abstract": "", "published_date": "2026-02-03", "affiliations": "University of Houston", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "Directly targets our active 'GPUSched' and 'RobustMAS' projects with a superior method for handling distribution shifts in LLM serving. The diffusion-based ambiguity set is a concrete, transferable technique for generating hard/adversarial instances that we should adopt over standard Wasserstein/KL approaches."}, "brief": "Wen et al. introduce '3D-Learning,' a framework that replaces analytic ambiguity sets (Wasserstein/KL) in Distributionally Robust Optimization (DRO) with a diffusion model trained via PPO to generate worst-case scenarios. Applied to LLM resource provisioning, they claim ~40-50% regret reduction on OOD Azure traces compared to standard DRO, though training computational cost is high (6.8GB memory vs 35MB). The critical takeaway is the methodology of parameterizing the ambiguity set with a generative model to find 'realistic' adversarial edge cases that respect the data manifold, solving the support shift issue of KL-DRO. We should steal this 'generative ambiguity set' concept for benchmarking our heuristics in RobustMAS and AlgoEvo.", "methodology": {"core_method": "Diffusion-Augmented Distributionally Robust Decision-Focused Learning (3D-Learning) using DDPM with U-Net backbone, integrating dual learning and diffusion policy optimization for min-max optimization", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "We introduce a new ambiguity modeling based on the score matching loss of diffusion models, enabling the search for worst-case distributions within the parameterized space of a diffusion model.", "components": ["Diffusion model (DDPM with U-Net)", "LSTM predictor", "Dual learning", "Proximal Policy Optimization (PPO)", "Gradient Descent with Max-Oracle (GDMO)", "Differentiable optimization layers"], "training_required": true}, "tags": {"methods": ["diffusion_model", "distributionally_robust_optimization", "decision_focused_learning", "dual_learning", "proximal_policy_optimization", "gradient_descent_with_max_oracle", "lstm", "differentiable_optimization"], "problems": ["resource_provisioning", "llm_serving_optimization", "demand_response", "edge_data_center_selection", "out_of_distribution_generalization"], "contribution_type": ["new_method", "sota_result", "empirical_study", "framework"], "framework_lineage": null, "specific_domain": "llm_resource_provisioning", "llm_coupling": null}, "problem": {"formal_name": "Distributionally Robust Decision-Focused Learning", "short": "DR-DFL", "class_": "resource_allocation", "properties": ["out_of_distribution_generalization", "distributionally_robust", "decision_focused", "stochastic"], "scale": "sequences of 28 time slots"}, "lineage": {"direct_ancestors": [{"paper": "Decision-Focused Learning", "relationship": "extends to be distributionally robust"}, {"paper": "Distributionally Robust Optimization", "relationship": "replaces traditional ambiguity modeling with diffusion models in"}, {"paper": "Denoising Diffusion Probabilistic Models", "relationship": "integrates for ambiguity set modeling"}], "closest_prior_work": "Gen-DFL: Decision-focused generative learning for robust decision making", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Apply diffusion-based ambiguity sets to other distributionally robust optimization problems", "Explore other decision objectives or more complex network scenarios", "Address the high computational overhead during training"], "transferable_to": ["Other predict-then-optimize problems in computing and communication networks", "General distributionally robust optimization problems"], "open_weaknesses": ["High computational overhead during training", "Sensitivity to the adversarial budget parameter epsilon", "Decision-focused training does not always outperform prediction-focused training on all datasets"]}, "artifacts": {"code_url": "https://github.com/CIGLAB-Houston/3DLearning.git", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.53, "experiments": {"benchmarks": ["Azure LLM Inference Traces (2023-Conversations)", "Azure LLM Inference Traces (2023-Code)", "Azure LLM Inference Traces (2024-Code)", "Azure LLM Inference Traces (2024-Conversations)", "Azure LLM Inference Traces (2023-Code&2024-Code)", "Azure LLM Inference Traces (2024-Code&2024-Conversations)", "Azure LLM Inference Traces (2023-Code&2024-Conversations)"], "baselines": ["Decision-Focused Learning (DFL)", "Wasserstein-based DRO (W-DRO) [39]", "KL-divergence-based DRO (KL-DRO) [13]", "Data Augmentation (DA) with Cutout noise", "Data Augmentation (DA) with Gaussian noise"], "hardware": "single NVIDIA RTX 6000 Ada GPU, 3D-Learning training runtime 4900 s", "instance_sizes": [751, 798, 4320]}, "results": {"vs_baselines": {"KL-DRO": "3D-Learning is 37.4% better on average regret", "W-DRO": "3D-Learning is 51.1% better on average regret", "Data Augmentation (DA) with Cutout noise": "3D-Learning is 40.9% better on average regret", "Data Augmentation (DA) with Gaussian noise": "3D-Learning is 44.0% better on average regret", "Decision-Focused Learning (DFL)": "3D-Learning is 54.9% better on average regret"}, "scalability": "At test time, all methods exhibit similar inference overhead (84 s, 418 MB), indicating no additional inference overhead for 3D-Learning. Training overhead is significantly higher for 3D-Learning (4900s runtime, 6.8 GB GPU memory).", "statistical_rigor": "Performance is evaluated by normalized Regret, which is the mean performance on a testing dataset compared to optimal mean performance. No explicit mention of multiple runs, variance, or significance tests.", "limitations_acknowledged": ["Decision-focused training does not always outperform MSE training, performing slightly worse on some datasets because it optimizes for worst-case rather than every distinct distribution.", "High computational overhead during training (4900s runtime, 6.8 GB GPU memory) compared to DFL and DA methods."]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.02987", "arxiv_url": "https://arxiv.org/abs/2602.02987", "title": "Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control", "authors": ["Ruihan"], "abstract": "", "published_date": "2026-02-03", "affiliations": "The Hong Kong University of Science and Technology", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 9, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper provides a rigorous, asymptotically optimal solution for the exact problem targeted by your 'GPUSched' project. It demonstrates that fluid-based stochastic control outperforms standard heuristics and likely scales better than pure integer programming approaches for cluster-level inference."}, "brief": "Lin et al. formulate LLM inference scheduling as a multiclass many-server queueing network, deriving a 'Gate-and-Route' policy from a steady-state fluid LP that explicitly manages prefill-decode contention. Calibrated on A100s, their approach proves that separating prefill admission (via occupancy tracking) from decode routing (work-conserving) eliminates decode backlogs and maximizes revenue. **Key Takeaway:** The decomposition of scheduling into 'static planning' (solving an LP for target occupancies) and 'dynamic control' (a simple gate tracking those targets) is a scalable alternative to online combinatorial optimization for your GPUSched work. It mathematically formalizes the intuition that prefill is the bottleneck and decode should be kept strictly critical but not backlogged.", "methodology": {"core_method": "Stochastic control with fluid approximation and LP-based gate-and-route policies", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "A multiclass many-server queueing network model with state-dependent service rates for LLM prefill-decode contention is developed, and asymptotically optimal gate-and-route control policies are derived from fluid approximation and linear programming.", "components": ["multiclass many-server queueing network model", "fluid approximation", "linear programming (LP)", "gate-and-route policy", "prefill admission gate", "decode router"], "training_required": false}, "tags": {"methods": ["stochastic_control", "queueing_network", "many_server_queueing", "fluid_approximation", "linear_programming", "gate_and_route_policy", "prefill_admission_gate", "decode_router", "lyapunov_function", "kkt_conditions", "ordinary_least_squares", "maximum_likelihood_estimation"], "problems": ["llm_inference_scheduling", "resource_allocation", "queueing_theory", "revenue_management", "scheduling", "llm_serving_optimization", "prefill_decode_contention"], "contribution_type": ["new_method", "framework", "empirical_study", "sota_result", "theoretical_result"], "framework_lineage": "many_server_queueing_theory", "specific_domain": "llm_serving_optimization", "llm_coupling": null}, "problem": {"formal_name": "Large-Scale LLM Inference Scheduling with Prefill-Decode Contention and Heterogeneous Workloads", "short": "LLM-IS", "class_": "scheduling", "properties": ["two-phase (prefill-decode)", "state-dependent service rates", "heterogeneous workloads", "multiclass", "many-server", "stochastic", "revenue-driven", "SLI-constrained", "abandonment"], "scale": "up to 500 GPUs"}, "lineage": {"direct_ancestors": [{"paper": "Whitt 2006", "relationship": "builds on fluid models for multiserver queues"}, {"paper": "Zhang 2013", "relationship": "builds on fluid models for multiserver queues with abandonment"}, {"paper": "Atar et al. 2010", "relationship": "builds on asymptotically optimal scheduling for multiclass queues"}, {"paper": "Long et al. 2020", "relationship": "builds on asymptotically optimal scheduling for multiclass queues"}, {"paper": "Long et al. 2024", "relationship": "builds on asymptotically optimal scheduling for multiclass queues"}, {"paper": "Ao et al. 2025", "relationship": "extends single-GPU LLM inference scheduling to cluster-scale"}, {"paper": "Li et al. 2025", "relationship": "extends single-GPU LLM inference scheduling to cluster-scale"}], "closest_prior_work": "Ao et al. 2025 Optimizing LLM inference: Fluid-guided online scheduling with memory constraints", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Relax exponential service time assumption to general distributions using measure-valued processes", "Develop diffusion approximations for rigorous tail-latency SLI guarantees", "Generalize the model to heterogeneous GPU infrastructures and agent architectures"], "transferable_to": ["Other two-phase resource contention problems in distributed systems", "Cloud resource allocation for diverse AI/ML workloads", "General many-server systems with state-dependent service rates and heterogeneous demands"], "open_weaknesses": ["Reliance on exponential service time assumption for analytical tractability", "Lack of diffusion approximations for characterizing stochastic variability and tail-latency SLIs", "Model currently limited to homogeneous GPU infrastructures", "Separate charging scheme can incentivize decode backlogs and memory pressure"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_0", "front_status": "emerging", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.3, "experiments": {"benchmarks": ["Databricks Dolly-15k (calibrated synthetic workloads)"], "baselines": ["FCFS-and-Immediate without Static Planning (FI-WSP)", "Gate-and-Immediate without Static Planning (GI-WSP)", "Gate-and-FCFS without Static Planning (GF-WSP)", "FCFS-and-Route (Greedy Router) with Static Planning (FG-SP)", "OPT (fluid optimum)"], "hardware": "4x NVIDIA A100-SXM4-40GB GPUs, AMD EPYC 7H12 64-core processor", "instance_sizes": [5, 20, 50, 200, 500]}, "results": {"vs_baselines": {"FI-WSP(Sarathi)": "~30% lower revenue than OPT", "GI-WSP": "~20% lower revenue than OPT", "GF-WSP": "~35% lower revenue than OPT", "FG-SP": "~15% lower revenue than OPT", "GG-SP(Ours)": "asymptotically optimal revenue"}, "scalability": "Per-GPU revenue converges to the fluid optimum as the cluster scales, and normalized queue lengths stabilize near their fluid targets with decreasing standard deviation.", "statistical_rigor": "Experiments run with 5 random seeds per configuration to capture stochastic variability, with standard deviations shown as error bands.", "limitations_acknowledged": ["assumption of exponential service times", "lack of diffusion approximations for tail-latency SLIs", "model limited to homogeneous infrastructures", "separate charging scheme can lead to decode congestion"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.03545", "arxiv_url": "https://arxiv.org/abs/2602.03545", "title": "Persona Generators: Generating Diverse Synthetic Personas at Scale", "authors": ["Davide"], "abstract": "", "published_date": "2026-02-03", "affiliations": "Google DeepMind", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 3, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This is a state-of-the-art application of AlphaEvolve (our core methodology) by DeepMind. While the domain (personas) is irrelevant, the technique of evolving 'generator code' to maximize output diversity is directly transferable to evolving heuristics that produce diverse optimization solutions."}, "brief": "Paglieri et al. (DeepMind) apply AlphaEvolve to optimize Python code that generates synthetic personas, explicitly maximizing diversity metrics (convex hull, coverage) in embedding space rather than just fidelity. They achieve >80% coverage of the behavioral space compared to <50% for baselines, proving that evolving the *generator function* is more effective than prompting for diversity. The key takeaway is their two-stage architecture (autoregressive high-level trait generation $\\to$ parallel detail expansion), which we should steal to evolve 'Solution Generators' for VRP/OR that inherently resist mode collapse. This validates our direction with AlgoEvo but offers a concrete architectural pattern for maintaining population diversity.", "methodology": {"core_method": "AlphaEvolve-driven evolutionary search for Persona Generator code optimization with a two-stage LLM-based generation architecture", "llm_role": "evolutionary_search, code_writer, evaluator, research_agent", "llm_model_used": "Gemini 2.5 Pro, gemma-3-27b-it", "search_type": "improvement", "novelty_claim": "The paper introduces Persona Generators, functions optimized via an AlphaEvolve-driven LLM evolutionary search to produce diverse synthetic populations tailored to arbitrary contexts, featuring a novel two-stage generation architecture.", "components": ["Persona Generator (two-stage: Autoregressive Stage 1, Parallel Stage 2)", "Questionnaire Generator", "Concordia simulations", "Diversity metrics (coverage, convex hull volume, min pairwise distance, avg pairwise distance, dispersion, KL divergence)", "AlphaEvolve"], "training_required": true}, "tags": {"methods": ["llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_in_the_loop", "alphaevolve", "evolution_of_heuristics", "program_synthesis", "generative_agent_based_modeling", "multi_agent_simulation", "evolutionary_algorithm", "diversity_metrics", "monte_carlo_sampling"], "problems": ["synthetic_persona_generation", "synthetic_data_generation", "diversity_maximization", "support_coverage", "stress_testing_ai_systems", "automated_algorithm_design"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study", "new_benchmark"], "framework_lineage": "alphaevolve", "specific_domain": "synthetic_persona_generation", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Diverse Synthetic Persona Generation", "short": "Persona Generation", "class_": "generative_modeling", "properties": ["diverse", "synthetic", "context_tailored", "support_coverage_maximization"], "scale": "25 personas per context"}, "lineage": {"direct_ancestors": [{"paper": "AlphaEvolve", "relationship": "uses as evolutionary search loop"}, {"paper": "Concordia", "relationship": "adapts generator from and uses for simulations"}, {"paper": "Generative Agents", "relationship": "builds on LLM-based persona simulation"}], "closest_prior_work": "AlphaEvolve", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["develop principled metrics for meaningful behavioral diversity in open-ended text", "directly optimize persona generators for behavioral diversity in downstream applications", "explore meta-learning approaches to adapt mutation strategies for broader design space exploration", "scale evaluation to much larger populations per questionnaire"], "transferable_to": ["evaluating novel educational chatbots and new products", "forecasting impacts of artificial general intelligence", "simulating complex social interactions and economic games", "ai safety and red-teaming"], "open_weaknesses": ["measuring behavioral diversity in open-ended text is fundamentally challenging", "optimizing for stated preferences may not strongly correlate with diversity in manifested behavior", "current evolved solutions may reflect structural patterns from mutation prompts", "scalability of diversity gains to arbitrarily large populations needs further study"]}, "artifacts": {"code_url": "https://github.com/akhatua2/synthpersona", "models_released": false, "new_benchmark": true}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.03, "experiments": {"benchmarks": ["50 automatically generated questionnaires (30 training, 10 validation, 10 test)"], "baselines": ["Nemotron Personas (Meyer & Corneil, 2025)", "Concordia formative memory generator (Vezhnevets et al., 2023)", "name-only baseline"], "hardware": "null", "instance_sizes": [25]}, "results": {"vs_baselines": {"Nemotron Personas": "substantially outperformed across all six diversity metrics", "Concordia formative memory generator": "substantially outperformed across all six diversity metrics", "name-only baseline": "substantially outperformed across all six diversity metrics"}, "scalability": "The Persona Generator is lightweight and efficient, enabling rapid, one-shot population synthesis for downstream applications, but the scalability of diversity gains for arbitrarily large populations is left for future work.", "statistical_rigor": "Performance is averaged over 40 training/validation questionnaires and 10 test questionnaires, each generating 25 personas. Diversity metrics are computed with Monte Carlo sampling (10,000 points) and averaged over 1,000 Sobol quasi-random distributions for KL divergence. Variance is acknowledged for some metrics.", "limitations_acknowledged": ["Measuring behavioral diversity in open-ended text remains fundamentally challenging.", "Optimizing solely for diversity in stated preferences may not strongly correlate with diversity in manifested behavior.", "Scalability to much larger populations per questionnaire needs further exploration.", "Evolved solutions may reflect structural patterns from mutation prompts.", "Synthetic personas should complement, not replace, human participants to reduce bias.", "A larger number of more diverse questionnaires may be required for robust evaluations."]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.03070", "arxiv_url": "https://arxiv.org/abs/2602.03070", "title": "ProOPF: Benchmarking and Improving LLMs for Professional-Grade Power Systems Optimization Modeling", "authors": ["Chao"], "abstract": "", "published_date": "2026-02-03", "affiliations": "", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 6, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While focused on Power Systems, the 'modification-based' data synthesis pipeline (Base Model + Structural/Parametric Delta) is a superior strategy for generating training data for our 'automatic extension of VRP variants' and 'OR-Bench' projects compared to generating from scratch."}, "brief": "Shen et al. propose a benchmark (ProOPF) for translating natural language into Optimal Power Flow (OPF) models, treating instances as parametric or structural modifications to a canonical base model rather than generating code from scratch. They introduce a rigorous data synthesis pipeline using 'scenario trees' to map qualitative descriptions (e.g., 'heatwave') to quantitative parameter deltas, and define structural extensions (e.g., adding security constraints) as modular patches. Results are sobering: SOTA models (GPT-4, Claude 3.5) score 0% on the hardest level (semantic inference + structural change), though SFT recovers ~11-35%. **Key Takeaway:** We should steal their 'Base + Delta' synthesis approach for our VRP variant generation and OR-Bench work; it allows for scalable, physically valid data generation without requiring an LLM to hallucinate full solvers, and effectively benchmarks 'ambiguity' handling.", "methodology": {"core_method": "LLM-based code synthesis for optimization modeling from natural language", "llm_role": "code_writer", "llm_model_used": "GPT-5.2, GPT-5.1, Claude 4.5 Sonnet, Claude 3.5 Sonnet, DeepSeek V3.2, DeepSeek-r1, Gemini 3.0 Pro, Qwen3-Coder, Qwen3-30B-A3B", "search_type": "constructive", "novelty_claim": "We introduce ProOPF-D/B, the first dataset & benchmark designed to systematically evaluate LLM competence in specialized power system optimization modeling, shifting LLM-based optimization to large-scale, complex power domain specific modeling.", "components": ["ProOPF-D dataset", "ProOPF-B benchmark", "multi-level dataset construction pipeline", "modification-based OPF representation", "expert-curated scenario trees", "MATPOWER-based toolchain", "data cleaning and text refinement"], "training_required": true}, "tags": {"methods": ["llm_code_generation", "llm_as_evaluator", "llm_fine_tuned", "data_synthesis", "benchmark_design", "in_context_learning", "nonlinear_programming", "mixed_integer_linear_programming", "exact_optimization"], "problems": ["optimal_power_flow", "power_system_optimization", "unit_commitment", "economic_dispatch", "optimal_transmission_switching", "load_shedding", "security_constrained_opf", "multi_period_opf"], "contribution_type": ["new_benchmark", "new_method", "empirical_study", "framework"], "framework_lineage": "proopf", "specific_domain": "optimal_power_flow", "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Optimal Power Flow", "short": "OPF", "class_": "power_system_optimization", "properties": ["parametric_modifications", "structural_extensions", "security_constraints", "uncertainty", "nonlinear_program", "tightly_coupled_physical_constraints"], "scale": "4-bus to 9241-bus systems"}, "lineage": {"direct_ancestors": [{"paper": "Yang et al., 2025b", "relationship": "builds on model-centric data synthesis from ReSocratic"}, {"paper": "Li et al., 2024", "relationship": "builds on model-centric data synthesis from MILP-Evolve"}], "closest_prior_work": "Optibench (Yang et al., 2025b)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Develop domain-aware training and supervision strategies for LLMs in power systems.", "Improve LLM capabilities for semantic parameter inference from qualitative descriptions.", "Enhance LLM proficiency in implementing complex structural modifications to optimization models.", "Address LLM deficiencies in specific resolution specification and API knowledge for optimization solvers."], "transferable_to": ["Other domain-specific optimization problems (e.g., chemical process optimization, supply chain optimization).", "Other engineering domains requiring natural language to executable code translation for complex systems.", "Automated modeling for other types of optimization problems (e.g., scheduling, routing) with domain-specific constraints.", "Development of self-improving LLM agents for expert-level problem formulation in specialized fields."], "open_weaknesses": ["LLMs struggle significantly with semantic parameter inference, failing to map qualitative descriptors to quantitative standards.", "LLMs exhibit fragility in implementing structural modifications, indicating a gap between conceptual understanding and coding proficiency.", "Current LLMs lack specific resolution specification and API knowledge required for professional-grade optimization modeling.", "LLMs show a catastrophic collapse in performance when tasks involve both semantic inference and structural modifications."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": true}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.33, "experiments": {"benchmarks": ["ProOPF-B", "NL4Opt", "MAMO", "Optibench"], "baselines": ["GPT-5.2", "GPT-5.1", "Claude 4.5 Sonnet", "Claude 3.5 Sonnet", "DeepSeek V3.2", "DeepSeek-r1", "Gemini 3.0 Pro", "Qwen3-Coder", "Qwen3-30B-A3B"], "hardware": "8 x A100 (80GB) GPUs", "instance_sizes": [4, 9, 14, 30, 39, 57, 118, 300, 2000, 9241]}, "results": {"vs_baselines": {"Existing Benchmarks": "SOTA LLMs achieve >90% accuracy, significantly higher than ProOPF-B performance", "GPT-5.2": "14.05% average accuracy on ProOPF-B (few-shot)", "Claude 4.5 Sonnet": "33.88% average accuracy on ProOPF-B (few-shot)", "DeepSeek V3.2": "29.75% average accuracy on ProOPF-B (few-shot)", "Gemini 3.0 Pro": "37.19% average accuracy on ProOPF-B (few-shot)", "Qwen3-30B-A3B (SFT)": "35.53% average accuracy on ProOPF-B (few-shot), improving Level 2 from 0% to 33.30% and Level 4 from 0% to 11.54%"}, "scalability": "Model performance exhibits a sharp negative correlation with task abstraction and complexity, with LLMs struggling significantly on tasks requiring semantic parameter inference and structural modifications.", "statistical_rigor": "Evaluated using objective value accuracy from generated implementations, across few-shot and zero-shot settings, with correctness defined by a tolerance threshold (epsilon > 0) for objective value difference.", "limitations_acknowledged": ["struggle with semantic parameter inference and structural modeling", "deficiency in specific resolution specification and API knowledge", "fragility in basic coding proficiency despite high-level domain logic", "inability to map qualitative descriptors to quantitative domain standards without explicit numerical grounding"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2602.03318", "arxiv_url": "https://arxiv.org/abs/2602.03318", "title": "MIRROR: A Multi-Agent Framework with Iterative Adaptive Revision and Hierarchical Retrieval for Optimization Modeling in Operations Research", "authors": ["Yifan"], "abstract": "", "published_date": "2026-02-03", "affiliations": "Xi'an Jiaotong University, Northwestern Polytechnical University", "category": "Generative AI for OR", "relevance": {"methodological": 5, "problem": 9, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "Establishes a new SOTA baseline for our OR modeling benchmarks (IndustryOR, NL4Opt) using RAG+Agents rather than fine-tuning. The structured error-handling mechanism is a clean implementation pattern relevant to our coding agents."}, "brief": "MIRROR is a multi-agent framework that translates natural language OR problems into Gurobi code using Hierarchical RAG (metadata filtering + semantic search) and an iterative repair loop. It achieves ~72% pass@1 across five benchmarks, outperforming Chain-of-Experts and fine-tuned models like LLMOPT without task-specific training. The key takeaway is their **structured revision tip mechanism**: upon execution failure, the agent generates a JSON object explicitly isolating the `error_statement`, `incorrect_code_snippet`, and `correct_code_snippet`, which serves as a precise memory artifact for subsequent retries. This structured reflection pattern is superior to raw error logs and could be immediately adopted in our own code generation pipelines.", "methodology": {"core_method": "Multi-Agent Framework with Iterative Adaptive Revision (IAR) and Hierarchical Retrieval-Augmented Generation (HRAG)", "llm_role": "code_writer, decomposition_guide, evaluator", "llm_model_used": "qwen-plus-2025-09-11", "search_type": "hybrid", "novelty_claim": "MIRROR is a fine-tuning-free, end-to-end multi-agent framework that translates natural language optimization problems into mathematical models and solver code, integrating execution-driven iterative adaptive revision and hierarchical retrieval.", "components": ["Multi-agent system", "Iterative Adaptive Revision (IAR) mechanism", "Hierarchical Retrieval-Augmented Generation (HRAG) mechanism", "Dual Memory (Local and Global)", "Exemplar Library", "Parameter Extraction Agent", "Modeling Advisor Agent", "Mathematical Modeling Agent", "Code Generation Agent", "Modeling Revision Agent", "Code Revision Agent", "Executor"], "training_required": false}, "tags": {"methods": ["multi_agent_system", "llm_code_generation", "llm_as_evaluator", "llm_in_the_loop", "retrieval_augmented_generation", "program_synthesis", "self_improving_search", "iterative_adaptive_revision", "hierarchical_retrieval_augmented_generation"], "problems": ["optimization_modeling", "mixed_integer_linear_programming", "linear_programming", "integer_programming", "non_linear_programming", "combinatorial_routing", "discrete_scheduling_assignment"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "chain_of_experts", "specific_domain": "optimization_modeling", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Optimization Modeling in Operations Research", "short": "OR Modeling", "class_": "program_synthesis", "properties": ["natural_language_input", "mathematical_model_output", "solver_code_output", "multi_agent_system", "iterative_refinement", "retrieval_augmented"], "scale": "Up to 489 instances per dataset, with varying complexity quantified by number of variables and constraints"}, "lineage": {"direct_ancestors": [{"paper": "Chain-of-Experts (Xiao et al., 2023)", "relationship": "improves upon multi-agent framework limitations"}, {"paper": "OptiMUS (Ahmaditeshnizi et al., 2024)", "relationship": "improves upon multi-agent framework limitations"}, {"paper": "ORMind (Wang et al., 2025)", "relationship": "improves upon multi-agent framework limitations"}], "closest_prior_work": "Chain-of-Experts", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Further improve error diagnosis for more subtle logical flaws in complex models.", "Extend the framework to handle non-linear and stochastic optimization problems more robustly.", "Integrate with real-time data sources for dynamic problem updates and re-optimization.", "Explore human-in-the-loop mechanisms for critical revisions or ambiguous problem statements."], "transferable_to": ["Automated scientific modeling in other domains (e.g., physics, chemistry simulations).", "Automated code generation for other specialized programming tasks beyond OR.", "Automated data science pipeline generation from natural language descriptions."], "open_weaknesses": ["The quality and coverage of the curated exemplar library (HRAG) can limit performance on novel problems.", "Potential for LLM hallucinations if external knowledge is insufficient or misaligned with task requirements.", "Scalability to extremely large-scale optimization problems or highly novel, unseen scenarios.", "Human oversight remains essential for critical applications due to potential for flawed outputs from ambiguous inputs."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.33, "experiments": {"benchmarks": ["NL4Opt", "Mamo-EasyLP", "Mamo-ComplexLP", "IndustryOR", "ComplexOR"], "baselines": ["Chain-of-Thought (CoT) on qwen-plus-2025-09-11", "Chain-of-Thought (CoT) on DeepSeek-v3", "Chain-of-Thought (CoT) on qwen3-30b-a3b-instruct-2507", "MiniOpt (14B)", "LLMOPT (14B)", "OptMATH (7B)", "ORLM (8B)", "SIRL (32B)", "OptiMUS", "Chain-of-Experts (COE)", "ORMind"], "hardware": "Not specified, uses Gurobi solver and LLMs (qwen-plus-2025-09-11, qwen3-30b-a3b-instruct-2507)", "instance_sizes": [245, 489, 203, 100, 18]}, "results": {"vs_baselines": {"COE": "+3.68% Macro Avg", "SIRL (32B)": "+6.4% on Mamo-ComplexLP, +9.0% on IndustryOR", "Deepseek-v3 (CoT)": "+7.43% Macro Avg", "qwen-plus (CoT)": "+13.36% Macro Avg", "qwen3-30B (CoT)": "MIRROR (30B) +12.15% Macro Avg"}, "scalability": "MIRROR consistently improves performance on complex datasets and effectively boosts capabilities of small open-source language models without fine-tuning.", "statistical_rigor": "Pass@1 metric used; temperature set to 0 for deterministic and reproducible outputs. No explicit mention of multiple runs per instance, variance, or significance tests.", "limitations_acknowledged": ["Incorrect or ambiguous inputs could lead to flawed outputs, requiring human oversight.", "Intended solely as a decision-support aid."]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2602.03132", "arxiv_url": "https://arxiv.org/abs/2602.03132", "title": "Contrastive Concept-Tree Search for LLM-Assisted Algorithm Discovery", "authors": ["Timothee"], "abstract": "", "published_date": "2026-02-03", "affiliations": "", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 8, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper introduces a structural upgrade to the standard LLM evolutionary loop (AlphaEvolve) by inserting an explicit 'semantic concept' layer. It solves the 'black box' mutation problem by learning which algorithmic strategies (concepts) correlate with performance, directly addressing our goals for sample efficiency, memory, and observability in AlgoEvo."}, "brief": "The authors introduce Contrastive Concept-Tree Search (CCTS), which modifies the standard evolutionary loop by prompting the LLM to extract semantic 'concepts' from every generated program, building a dynamic hierarchy. They then apply a Tree-structured Parzen Estimator (TPE) to these concepts to learn a contrastive utility model (p(concept|good)/p(concept|bad)), using this to bias parent selection towards promising algorithmic strategies. Results are rigorous, showing consistent improvements over k-elite baselines on combinatorial tasks like Circle Packing, with a synthetic ablation confirming the model learns ground-truth concept utilities. **Key Takeaway:** We should immediately implement the 'Concept TPE' loop in AlgoEvo—asking the LLM to tag generated heuristics with concepts and maintaining a weight vector over these concepts provides a cheap, interpretable 'process reward model' to guide search.", "methodology": {"core_method": "Contrastive Concept-Tree Search (CCTS) using a hierarchical Bernoulli model and Tree-structured Parzen Estimator (TPE) for likelihood-ratio based parent reweighting, combined with cross-entropy updates for concept utility estimation.", "llm_role": "heuristic_generator", "llm_model_used": "gemini-2.0-flash", "search_type": "hybrid", "novelty_claim": "CCTS extracts a hierarchical concept representation from generated programs and learns a contrastive concept model that guides parent selection by reweighting parents using a likelihood-ratio score between high- and low-performing solutions.", "components": ["hierarchical concept representation", "contrastive concept model", "likelihood-ratio score", "parent selection policy", "cross-entropy update", "Tree-structured Parzen Estimator (TPE)", "semantic feature extractor", "concept-level exploration mechanism", "novelty bias", "mixture-based concept selection", "Beta priors"], "training_required": true}, "tags": {"methods": ["llm_as_heuristic", "llm_code_generation", "evolution_of_heuristics", "program_synthesis", "self_improving_search", "tree_structured_parzen_estimator", "cross_entropy_method", "contrastive_learning", "concept_learning", "bayesian_optimization", "hybrid_search"], "problems": ["algorithm_discovery", "heuristic_evolution", "combinatorial_optimization", "circle_packing_problem", "arithmetic_kakeya_conjecture", "heilbronns_triangle_problem", "squares_in_square_problem"], "contribution_type": ["new_method", "framework", "new_benchmark", "empirical_study"], "framework_lineage": null, "specific_domain": null, "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "LLM-Assisted Algorithm Discovery for Combinatorial Optimization", "short": "Algorithm Discovery", "class_": "algorithm_discovery", "properties": ["black_box_optimization", "iterative_refinement", "concept_guided"], "scale": "26 circles, 11 points, 6-55 squares"}, "lineage": {"direct_ancestors": [{"paper": "Tree-structured Parzen Estimator", "relationship": "integrates as a component for parent reweighting"}, {"paper": "FunSearch", "relationship": "builds on the paradigm of LLM-assisted algorithm discovery"}], "closest_prior_work": "FunSearch", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Strengthening guidance in the upper tail of algorithm performance", "Combining CCTS with population-structuring mechanisms like island-based evolution or quality-diversity methods", "Exploring larger LLM models and search budgets", "Improving robustness to insufficient task-specific information in prompts"], "transferable_to": ["Other combinatorial optimization problems", "Automated program synthesis for different domains", "Discovery of algorithms for other problem classes"], "open_weaknesses": ["Primarily improves the lower tail of algorithm performance", "Restricted to relatively small, low-cost LLM models and modest search budgets", "Sensitivity to initial prompt quality and task-specific information", "Lack of integration with population-based search methods"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": true}, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.4, "experiments": {"benchmarks": ["Circle packing problem", "Arithmetic Kakeya Conjecture", "Heilbronn’s Triangle Problem", "Squares-in-Square problem"], "baselines": ["Greedy selection", "k-elite selection (k=5)", "Uniform selection"], "hardware": "gemini-flash-2.0, gpt-o4-mini, gemini-2.5-flash-lite, gpt-4.1-nano", "instance_sizes": [26, 11, 6, 55]}, "results": {"vs_baselines": {"Greedy selection": "CCTS consistently achieves higher scores and faster improvement across all tasks compared to Greedy selection, with a reduced width of the lower tail of the performance distribution.", "k-elite selection (k=5)": "CCTS consistently achieves higher scores and faster improvement across all tasks compared to k-elite selection, with a reduced width of the lower tail of the performance distribution.", "Uniform selection": "CCTS consistently achieves higher scores and faster improvement across all tasks compared to Uniform selection, with a reduced width of the lower tail of the performance distribution."}, "scalability": "The optimal search strategy (exploitation probability) depends on the structure of the underlying latent concept tree, with exploitation being more effective for wider trees and exploration for narrower ones.", "statistical_rigor": "Results are averaged over 60 runs for real tasks and 500 runs for synthetic tasks, with 95% confidence intervals reported.", "limitations_acknowledged": ["CCTS primarily improves the lower tail of the algorithms’ score distribution, with strengthening guidance in the upper tail remaining future work.", "The benchmark's primary goal was to study algorithm discovery dynamics, not to solve underlying problems optimally.", "Experiments were restricted to relatively small, low-cost LLM models and modest search budgets.", "Insufficient task-specific information in the initial prompt can lead to overly generic concept trees and stalled search.", "Combining CCTS with population-structuring mechanisms like island-based evolution or quality-diversity methods was not explored but could improve robustness and scalability."]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2602.02724", "arxiv_url": "https://arxiv.org/abs/2602.02724", "title": "Automatic Design of Optimization Test Problems with Large Language Models", "authors": ["Wojciech"], "abstract": "", "published_date": "2026-02-02", "affiliations": "AGH University of Krakow, Warsaw University of Technology", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 5, "problem": 9, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "While the evolutionary method is standard (EoH-style), the application to generating targeted benchmark instances is critical for our AlgoEvo project. We currently rely on static suites like BBOB; this framework allows us to generate infinite, diverse, and 'adversarial' training data to prevent our evolved algorithms from overfitting."}, "brief": "Achtelik et al. adapt LLM-driven evolutionary search (EoH) to generate interpretable Python functions that match specific landscape features (ELA), effectively creating synthetic benchmarks on demand. Unlike prior neural network approaches that fail to scale, this method performs robustly in higher dimensions (3D-5D) and produces portable code. The key takeaway is the capability to procedurally generate 'hard' or specific-property instances; we should immediately adopt this to create a dynamic training curriculum for AlgoEvo, ensuring our evolved metaheuristics generalize beyond standard libraries like BBOB.", "methodology": {"core_method": "LLM-driven evolutionary search for Python function generation", "llm_role": "evolutionary_search", "llm_model_used": "Gemini 2.0 Flash, Gemini 2.5 Flash, Gemini 3.0 Flash", "search_type": "improvement", "novelty_claim": "Introduction and validation of ELA-guided benchmark generation using LLMs to produce novel, non-trivial optimization problems conforming to target ELA feature vectors.", "components": ["LLM-driven evolutionary architecture", "initialization strategy", "exploration operators", "mutation operators", "ELA feature extraction (pflacco)", "NumPy for function implementation"], "training_required": false}, "tags": {"methods": ["llm_evolutionary_search", "eoh", "exploratory_landscape_analysis", "llm_code_generation", "program_synthesis", "evolution_of_heuristics"], "problems": ["test_function_generation", "continuous_optimization", "black_box_optimization", "benchmark_design"], "contribution_type": ["new_method", "framework", "empirical_study", "sota_result", "new_benchmark"], "framework_lineage": "eoh", "specific_domain": "continuous_black_box_optimization", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Automatic Generation of Continuous Optimization Test Functions", "short": "Test Function Generation", "class_": "test_function_design", "properties": ["continuous", "single_objective", "landscape_feature_matching", "interpretable", "high_dimensional"], "scale": "2-5 dimensions"}, "lineage": {"direct_ancestors": [{"paper": "Evolution of heuristics: towards efficient automatic algorithm design using large language model", "relationship": "extends LLM-driven evolutionary architecture from"}, {"paper": "Neural Networks as Black-Box Benchmark Functions Optimized for Exploratory Landscape Features", "relationship": "extends problem formulation of ELA-guided test function generation from"}], "closest_prior_work": "Evolution of heuristics: towards efficient automatic algorithm design using large language model", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["systematic categorization of structural motifs in LLM-generated functions", "extension to constrained and multi-objective optimization landscapes", "development of prompting strategies for specific analytical properties (optima, gradients)", "hybrid approaches combining NNs and LLMs for benchmark generation"], "transferable_to": ["constrained_optimization", "multi_objective_optimization", "algorithm_selection", "other black_box_optimization_problems"], "open_weaknesses": ["NNs retain accuracy advantage in lowest-dimensional regime (2D)", "Generated functions may simplify complex multimodal landscapes", "Decision-making process of LLM during generation remains opaque", "Model capability improvements yield incremental rather than transformative gains"]}, "artifacts": {"code_url": "https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020", "models_released": false, "new_benchmark": true}, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.48, "experiments": {"benchmarks": ["BBOB suite", "MA-BBOB hybrid functions"], "baselines": ["NN-based method [20]", "LlaMEA [14]", "Zero Shot baseline"], "hardware": "CPU for ELA calculation, LLM inference hardware not specified", "instance_sizes": [2, 3, 4, 5]}, "results": {"vs_baselines": {"NN-based method [20]": "2D: NNs win on 100% of problems; 3D: EoTF wins on >75% of problems; 5D: EoTF achieves better solution quality than NNs in 3D", "LlaMEA [14]": "3D: EoTF slightly outperforms LlaMEA", "Zero Shot baseline": "2D: EoTF outperforms Zero Shot on >90% of problems; 3D: EoTF outperforms Zero Shot on 75% of problems", "Gemini 2.5 Flash": "Gemini 3.0 Flash wins on 70.8% of functions", "Gemini 2.0 Flash": "Gemini 3.0 Flash wins on 54.2% of functions; Gemini 2.5 Flash wins on 54.2% of functions"}, "scalability": "EoTF demonstrates remarkable stability in solution quality as dimensionality increases from 2D to 5D, with average median ELA distance remaining consistently between 0.17 and 0.20.", "statistical_rigor": "ELA vectors recalculated over 100 independent samples, median distance used for comparison. Optimizer rankings compared using Critical Difference diagrams from Friedman test with Nemenyi post-hoc analysis. Error bars represent interquartile range (q25–q75).", "limitations_acknowledged": ["NNs retain accuracy advantage in lowest-dimensional regime", "Generated functions may simplify complex multimodal landscapes (e.g., FID 21)", "Decision-making process of LLM during generation remains opaque", "Model capability improvements yield incremental rather than transformative gains"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.02919", "arxiv_url": "https://arxiv.org/abs/2602.02919", "title": "DeltaEvolve: Accelerating Scientific Discovery through Momentum-Driven Evolution", "authors": ["Jiachen"], "abstract": "", "published_date": "2026-02-02", "affiliations": "Microsoft, The Ohio State University", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 10, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper proposes a concrete architectural fix (Semantic Deltas) for the context bottleneck in AlphaEvolve, a core method we use. The ablation study demonstrating that 'context selection dominates scalar feedback' challenges our current assumption that precise reward signals are the primary driver of evolution."}, "brief": "DeltaEvolve replaces the standard full-code history in evolutionary search with 'semantic deltas'—structured text summaries capturing the 'from/to' logic of modifications and their hypotheses. Across 5 domains (including BBOB and Symbolic Regression), they demonstrate superior objective scores over AlphaEvolve while reducing token consumption by ~37%. The critical takeaway is the 'Progressive Disclosure' mechanism: treating history as a momentum vector (deltas) rather than a state archive (snapshots) allows us to fit a deeper evolutionary trajectory into the context window. We should immediately test their 'Delta Plan' prompt structure in AlgoEvo to improve sample efficiency and reduce costs.", "methodology": {"core_method": "Momentum-driven evolutionary framework with semantic delta and progressive disclosure within an Expectation-Maximization (EM) process", "llm_role": "program_synthesizer", "llm_model_used": "gpt-5-mini, o3-mini, gemini-2.5-flash-lite, gemini-2.5-flash", "search_type": "improvement", "novelty_claim": "We propose DeltaEvolve, a momentum-driven evolutionary framework that replaces full-code history with structured semantic delta capturing how and why modifications between successive nodes affect performance.", "components": ["Expectation–Maximization framework", "Semantic Delta", "Multi-Level Database", "Progressive Disclosure Sampler"], "training_required": false}, "tags": {"methods": ["llm_evolutionary_search", "llm_code_generation", "llm_as_heuristic", "expectation_maximization", "semantic_delta", "multi_level_database", "progressive_disclosure", "map_elites", "cma_es"], "problems": ["black_box_optimization", "hexagon_packing", "symbolic_regression", "pde_solver", "efficient_convolution", "heuristic_evolution", "algorithm_discovery"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "alphaevolve", "specific_domain": null, "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Programmatic Scientific Discovery and Black-box Optimization", "short": "LLM-driven Program Evolution", "class_": "llm_evolutionary_search", "properties": ["discrete", "extremely_large_space", "non_differentiable", "iterative_refinement", "compositional"], "scale": "3-40 dimensions (Blackbox Opt), 11 hexagons (Hexagon Packing), up to 10 coefficients (Symbolic Regression), 50-200 grid sizes (PDE Solver), 30n x 30n matrices (Efficient Convolution)"}, "lineage": {"direct_ancestors": [{"paper": "AlphaEvolve", "relationship": "extends by replacing full-code history with semantic delta"}], "closest_prior_work": "AlphaEvolve", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["integrating llm weight updates (e-step optimization) into deltaevolve", "applying semantic delta concept to other llm-driven search paradigms", "improving the generation of semantic deltas for more complex modifications"], "transferable_to": ["symbolic constructions and mathematical proofs", "drug discovery and materials design", "other open_ended scientific discovery problems"], "open_weaknesses": ["potential limitations in the granularity or quality of semantic delta generation for highly complex changes", "scalability to extremely long programs where even deltas might become large", "optimality of the fixed window size in the progressive disclosure mechanism for all tasks"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.9, "experiments": {"benchmarks": ["BBOB (Black-Box Optimization Benchmarking) suite", "Hexagon Packing (N=11)", "Symbolic Regression (Nonlinear Harmonic Oscillator)", "PDE Solver (2D Poisson Equation)", "Efficient Convolution (2D kernel)"], "baselines": ["Parallel Sampling", "Greedy Refine", "AlphaEvolve (Novikov et al., 2025)", "OpenEvolve (Sharma, 2025)"], "hardware": "LLM ensemble using gpt-5-mini/o3-mini and gemini-2.5-flash-lite/gemini-2.5-flash APIs", "instance_sizes": [50, 100, 200]}, "results": {"vs_baselines": {"Parallel Sampling": "DeltaEvolve achieves significantly higher scores (e.g., +557.7% on Blackbox Optimization) and lower token consumption (e.g., -64.4% on Blackbox Optimization).", "Greedy Refine": "DeltaEvolve achieves significantly higher scores (e.g., +20.68% on Blackbox Optimization) and lower token consumption (e.g., -30.0% on Blackbox Optimization).", "AlphaEvolve": "DeltaEvolve achieves average +2.16% higher score and -30.91% lower token consumption across 5 tasks."}, "scalability": "DeltaEvolve improves token efficiency by replacing full-code history with semantic deltas, allowing it to achieve superior solution quality with significantly fewer tokens, especially for complex problems requiring longer evolutionary histories.", "statistical_rigor": "Experiments run with 3 random seeds (11, 42, 100); maximum best score and average token consumption reported.", "limitations_acknowledged": []}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.02029", "arxiv_url": "https://arxiv.org/abs/2602.02029", "title": "Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation", "authors": ["Zhongyuan"], "abstract": "", "published_date": "2026-02-02", "affiliations": "The Hong Kong Polytechnic University, InfiX.ai", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 8, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper introduces a structured intermediate representation (CIR) for OR modeling that directly competes with and complements our OR-Bench project. The CIR schema offers a potential 'genotype' for our evolutionary search (AlgoEvo) that is far more robust than raw code."}, "brief": "Lyu et al. propose a 'Canonical Intermediate Representation' (CIR) to decouple natural language operational rules from their mathematical instantiation, explicitly forcing the LLM to select modeling paradigms (e.g., time-indexed vs. continuous flow) before coding. They achieve state-of-the-art accuracy (47.2% vs 22.4% baseline) on a new, complex benchmark (ORCOpt-Bench) by using a multi-agent pipeline that retrieves and adapts constraint templates. The key takeaway is the 'Mapper' agent's paradigm selection logic, which prevents common formulation errors in VRPs and scheduling; we should evaluate CIR as a structured mutation space for AlgoEvo to replace brittle code evolution. The new benchmark is immediately relevant for our OR-Bench evaluation suite.", "methodology": {"core_method": "Multi-agent pipeline with Canonical Intermediate Representation (CIR) and Retrieval-Augmented Generation (RAG)", "llm_role": "Decomposition guide, paradigm selector, code writer, and verifier", "llm_model_used": "DeepSeek-v3.2, Qwen3-32B", "search_type": "constructive", "novelty_claim": "We introduce the Canonical Intermediate Representation (CIR): a schema that LLMs explicitly generate between problem descriptions and optimization models, thereby decoupling rule logic from its mathematical instantiation.", "components": ["Extractor Agent", "Mapper Agent", "Formalizer Agent", "Checker Agent", "CIR knowledge base", "Retrieval-Augmented Generation (RAG)"], "training_required": false}, "tags": {"methods": ["llm_as_evaluator", "llm_code_generation", "llm_in_the_loop", "retrieval_augmented_generation", "multi_agent_system", "canonical_intermediate_representation"], "problems": ["optimization_problem_formulation", "code_generation", "milp_general"], "contribution_type": ["new_method", "framework", "new_benchmark", "sota_result", "empirical_study"], "framework_lineage": null, "specific_domain": "optimization_problem_formulation", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "LLM-based Optimization Problem Formulation and Code Generation", "short": "LLM-based Opt Model Formulation", "class_": "optimization_problem_formulation", "properties": ["complex_operational_rules", "composite_constraints", "paradigm_sensitive", "natural_language_input", "code_generation"], "scale": "50 problems across 10 domains"}, "lineage": {"direct_ancestors": [{"paper": "AhmadiTeshnizi et al., 2024", "relationship": "builds on multi-agent LLM systems for optimization formulation (OptiMUS)"}, {"paper": "Zhang & Luo, 2025", "relationship": "builds on multi-agent LLM systems for optimization formulation (OR-LLM-Agent)"}, {"paper": "Jiang et al., 2025", "relationship": "builds on RAG for optimization problem solving (DRoC)"}, {"paper": "Liang et al., 2025", "relationship": "builds on multi-agent RAG for large-scale optimization formulation (LEAN-LLM-OPT)"}], "closest_prior_work": "LEAN-LLM-OPT (Liang et al., 2025)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["automate the construction and expansion of the CIR knowledge base", "incorporate constraint refinement mechanisms into the framework", "enrich CIR paradigm annotations with empirical computational profiles for efficiency-aware pattern selection"], "transferable_to": ["broader classes of optimization problems and domains", "problems with complex loading sequences or sequence-dependent constraints", "problems with complex skill interdependencies or multiple skill level requirements", "operational decision-making processes requiring explicit dispatching priorities"], "open_weaknesses": ["the CIR knowledge corpus is bounded, limiting domain coverage", "CIR soundness guarantee ensures no rule-violating solutions but does not achieve completeness, potentially leading to overly restrictive constraints", "performance is contingent on the curated CIR library’s domain coverage"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": true}, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.83, "experiments": {"benchmarks": ["ORCOpt-Bench", "IndustryOR", "BWOR", "OptMATH"], "baselines": ["Standard prompt", "Chain-of-Experts", "Reflexion", "Vanilla RAG", "Self-RAG", "GPT-5", "Grok-4", "ORLM", "OR-LLM-Agent"], "hardware": "null", "instance_sizes": [50]}, "results": {"vs_baselines": {"Standard prompt (deepseek-v3.2)": "+24.8% AR on ORCOpt-Bench", "Chain-of-Experts": "+17.0% AR on ORCOpt-Bench", "Reflexion": "+9.4% AR on ORCOpt-Bench", "GPT5": "+14.2% AR on ORCOpt-Bench (with reflection), -2.0% AR on IndustryOR, +1.2% AR on BWOR (with reflection), +6.0% AR on OptMATH (with reflection)", "Grok4": "+8.6% AR on ORCOpt-Bench (with reflection), -4.0% AR on IndustryOR, -1.3% AR on BWOR (with reflection), +4.8% AR on OptMATH (with reflection)", "OR-LLM-Agent": "-3.7% AR on BWOR (with reflection)"}, "scalability": "Not explicitly discussed in terms of problem instance size; performance improves with increased generation attempts (pass@k).", "statistical_rigor": "Five independent runs were conducted for each evaluation, and average performance is reported.", "limitations_acknowledged": ["The knowledge corpus is bounded, and performance is contingent on CIR library’s domain coverage.", "CIR soundness guarantee ensures no rule-violating solutions but does not achieve completeness, potentially excluding some valid solutions due to overly restrictive constraints."]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2602.01404", "arxiv_url": "https://arxiv.org/abs/2602.01404", "title": "BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation", "authors": ["Zhouzi"], "abstract": "", "published_date": "2026-02-01", "affiliations": "Carnegie Mellon University, University of Warwick, UNC Chapel Hill", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 7, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": true, "team_discussion": true, "reasoning": "While focused on training rather than inference, the rigorous OR formulation (convex optimization) outperforms RL/heuristics (Pollux) and theoretically debunks 'cluster efficiency' as a metric. This directly informs our 'GPUSched' project and offers a scheduling model applicable to our parallel evolutionary search workers."}, "brief": "This paper derives a 'Budget-Optimal Allocation' (BOA) policy for ML training jobs, proving via queueing theory that a 'fixed-width' policy (no queueing, constant allocation per epoch) is optimal under general stochastic assumptions. They validate this on AWS, showing a 1.75x improvement in JCT and 2.2x budget reduction compared to Pollux, demonstrating that maximizing cluster utilization/efficiency is actually suboptimal for JCT. The key takeaway is the rigorous convex optimization formulation that replaces heuristic autoscaling, along with a practical 'Epoch Gluing' technique to handle rescaling overheads—both transferable to scheduling our malleable evolutionary search workloads.", "methodology": {"core_method": "Budget-Optimal Allocation (BOA) policy derived from convex optimization", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "We prove new theoretical results for optimally solving the GPU rental problem that advance the current theoretical understanding of scheduling for parallelizable jobs.", "components": ["Budget-Optimal Allocation (BOA) policy", "BOA Width Calculator", "Epoch Gluing", "Budget Partitioning", "AdaptDL framework", "performance model"], "training_required": true}, "tags": {"methods": ["budget_optimal_allocation", "convex_optimization", "stochastic_modeling", "queueing_theory", "mathematical_analysis", "autoscaling", "resource_allocation", "job_scheduling", "fixed_width_policy", "epoch_gluing", "budget_partitioning", "performance_modeling"], "problems": ["gpu_scheduling", "cloud_resource_scheduling", "ml_training_resource_allocation", "job_completion_time_minimization", "cost_performance_tradeoff_optimization", "parallelizable_job_scheduling"], "contribution_type": ["new_method", "theoretical_result", "sota_result", "framework", "empirical_study"], "framework_lineage": "adaptdl", "specific_domain": "ml_training_resource_allocation", "llm_coupling": null}, "problem": {"formal_name": "Budget-Constrained GPU Rental and Allocation Problem for ML Training", "short": "GPU Allocation", "class_": "cloud_resource_scheduling", "properties": ["budget_constrained", "stochastic", "dynamic", "parallelizable_jobs", "sublinear_speedup", "multi_epoch_jobs", "rescaling_overheads"], "scale": "85-960 jobs, up to 300 GPUs on 80 nodes"}, "lineage": {"direct_ancestors": [{"paper": "AdaptDL", "relationship": "builds on"}, {"paper": "Pollux", "relationship": "outperforms and extends autoscaling variant of"}], "closest_prior_work": "Pollux [26]", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["full_evaluation_on_heterogeneous_gpu_clusters", "evaluation_with_physically_partitioned_gpus", "refining_boa_width_calculator_heuristics_for_rescaling_overheads", "investigating_tighter_integration_of_hyperparameter_tuning_with_scheduling"], "transferable_to": ["general_cloud_resource_scheduling", "ml_inference_workload_scheduling", "scheduling_other_types_of_parallelizable_jobs", "heterogeneous_computing_environments"], "open_weaknesses": ["rescaling_overheads_are_approximated_by_heuristics", "platform_dependent_node_reclamation_time_is_not_optimized", "baselines_performance_degrades_with_prediction_errors_and_high_variability", "model_assumes_fractional_gpu_allocations_which_are_rounded_in_practice"]}, "artifacts": {"code_url": "https://github.com/petuum/adaptdl/tree/osdi21-artifact/simulator", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.54, "experiments": {"benchmarks": ["newTrace", "workload-1 trace", "filterTrace"], "baselines": ["Pollux [26]", "Pollux with Autoscaling"], "hardware": "AWS EKS cluster with g4dn.12xlarge instances (4 NVIDIA T4 GPUs each)", "instance_sizes": [85, 918, 960]}, "results": {"vs_baselines": {"Pollux": "up to 1.75x improvement in average JCT and 1.7x improvement in P95 JCT (compared to Pollux with autoscaling)", "Pollux with Autoscaling": "up to 1.8x improvement in average JCT, up to 1.7x improvement in P95 JCT, and up to 2.2x reduction in budget for same JCT"}, "scalability": "BOA Constrictor maintains its advantage with increased job types, job size variability, and handles intense bursts in traffic more effectively than baselines.", "statistical_rigor": "Reports average JCT and P95 JCT from real-world implementation experiments and detailed simulations, including analysis of interarrival time variability (C2_I).", "limitations_acknowledged": ["Full evaluation on heterogeneous clusters deferred to future work", "Evaluation with physically partitioned GPUs (e.g., NVIDIA MIG) is outside the scope", "BOA Width Calculator uses a heuristic to approximate optimal solution for rescaling overheads", "Excludes platform-dependent node reclamation time from GPU usage statistics", "Baselines (Pollux's optimizer) encountered stability issues and crashes at low usage levels", "Shorter traces in implementation experiments limit learning time for speedup functions", "Speedup models are imperfect, though BOA Constrictor is robust to errors"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2602.01237", "arxiv_url": "https://arxiv.org/abs/2602.01237", "title": "Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models", "authors": ["Katrina"], "abstract": "", "published_date": "2026-02-01", "affiliations": "Harvard University", "category": "OR for Generative AI", "relevance": {"methodological": 7, "problem": 8, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "Directly intersects our 'OR for AI systems' and 'sample efficiency' tracks. The specific finding that middle transformer layers (12-17) hold the strongest predictive signal for reasoning complexity is immediately actionable for our process reward models and evolutionary search termination criteria."}, "brief": "Brown et al. propose a 'Predictive Scheduling' framework that trains lightweight predictors (MLP on hidden states or LoRA) to estimate required CoT length before generation, using a greedy algorithm to allocate tokens under a global budget. Results show a 7.9% accuracy gain on GSM8K over uniform batching, backed by a systematic layer-wise analysis. **The key takeaway for us is that middle transformer layers (12-17)—not the final layer—contain the highest signal-to-noise ratio for predicting reasoning difficulty.** We should immediately test extracting features from these layers for our AlgoEvo value functions to improve sample efficiency. While the greedy scheduling algorithm itself is standard OR, the application to internal model states for pre-run allocation is a valid efficiency win for our serving optimization work.", "methodology": {"core_method": "Predictive Scheduling framework using lightweight MLP predictors on intermediate transformer hidden states or LoRA-fine-tuned classifiers on raw text, combined with a greedy batch allocation algorithm", "llm_role": "predictor", "llm_model_used": "DeepSeek-R1-Distill-Qwen-1.5B, GPT-4-mini", "search_type": "improvement", "novelty_claim": "We are the first to systematically analyze which transformer layers yield the most informative signals for such predictions, demonstrating that middle layers provide superior accuracy in reasoning-length estimation.", "components": ["MLP predictors", "LoRA fine-tuned classifiers", "greedy batch allocation algorithm", "early stopping probability vectors", "difficulty classification"], "training_required": true}, "tags": {"methods": ["supervised_learning", "mlp", "lora", "greedy_algorithm", "llm_fine_tuned", "llm_in_context_learning", "early_stopping", "difficulty_classification", "chain_of_thought", "transformer_hidden_states_analysis"], "problems": ["llm_serving_optimization", "resource_allocation", "arithmetic_reasoning", "optimal_reasoning_length_prediction", "query_difficulty_estimation"], "contribution_type": ["new_method", "framework", "empirical_study", "sota_result"], "framework_lineage": null, "specific_domain": "arithmetic_reasoning", "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models", "short": "Predictive Scheduling", "class_": "resource_allocation", "properties": ["adaptive", "dynamic", "inference-time", "cost-efficient", "multi-trace reasoning"], "scale": "GSM8K dataset (1,294 test examples), 16-256 token budgets"}, "lineage": {"direct_ancestors": [{"paper": "Damani et al., 2024", "relationship": "extends adaptive allocation of LLM computation"}, {"paper": "Fu et al., 2024a", "relationship": "extends dynamic scheduling framework (Dynasor)"}], "closest_prior_work": "Damani et al., 2024", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["investigate hybrid approaches combining hidden-state features and linguistic pattern recognition", "extend predictive scheduling to multi-trace aggregation methods (e.g., self-consistency, tree-of-thoughts)", "integrate uncertainty estimation to guard against misallocation", "deploy in latency-sensitive, cost-constrained production systems"], "transferable_to": ["multi-trace aggregation methods (e.g., self-consistency, tree-of_thoughts)", "latency-sensitive, cost-constrained production systems", "other complex reasoning tasks", "on_device_deployments"], "open_weaknesses": ["prediction errors become more consequential at higher token budgets", "substantial room for improvement in early stopping predictions, especially at higher token budgets", "linguistic features alone are insufficient for precise continuous early stopping probability prediction", "difficulty classifiers struggle with medium category problems due to ambiguity"]}, "artifacts": {"code_url": "https://aneeshers.github.io/predictive-scheduling/", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.82, "experiments": {"benchmarks": ["GSM8K"], "baselines": ["Non-adaptive uniform budgeting", "Oracle allocation"], "hardware": "1 H100 GPUs", "instance_sizes": [1294]}, "results": {"vs_baselines": {"Non-adaptive uniform budgeting": "+7.9% accuracy on GSM8K (absolute gain)", "Oracle allocation": "closes >50% of gap to oracle"}, "scalability": "The adaptive allocation strategy effectively prioritizes easy problems at low budgets and shifts resources to harder problems as the budget increases, demonstrating efficient resource distribution across varying problem complexities.", "statistical_rigor": "Early stopping probabilities derived from 100 independent reasoning traces per question; original GSM8K train/test split maintained; extensive hyperparameter sweeps for MLP and LoRA models; early stopping based on validation performance.", "limitations_acknowledged": ["Prediction errors become more consequential at higher token budgets, leading to underperformance against non-adaptive baseline.", "Substantial room for improvement in early stopping predictions, especially at higher token budgets.", "Moderate correlation (0.742) suggests additional factors influence reasoning difficulty beyond hidden states.", "Linguistic features alone are insufficient for precise continuous early stopping probability prediction.", "Both few-shot and LoRA models struggle with classifying medium difficulty problems due to inherent ambiguity at category boundaries.", "Limited scope of LoRA adaptation may be insufficient for complex linguistic-to-probability mapping."]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.01410", "arxiv_url": "https://arxiv.org/abs/2602.01410", "title": "SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training", "authors": ["Yunjie"], "abstract": "", "published_date": "2026-02-01", "affiliations": "University of Michigan, NTT Research, Inc., University of Massachusetts Amherst", "category": "OR for Generative AI", "relevance": {"methodological": 7, "problem": 5, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While we do not focus on pretraining, this is a textbook example of 'OR for AI Systems'—specifically using runtime Integer Linear Programming (ILP) to dynamically optimize system configurations. This architecture is directly transferable to our GPUSched and dynamic resource allocation projects."}, "brief": "Pan et al. introduce SNIP, a framework that periodically solves a Knapsack-style Integer Linear Program (ILP) to assign layer-wise precision (FP4/FP8) during training, minimizing a custom 'divergence' metric subject to FLOPs constraints. Results are simulated via fake quantization (proxy FLOPs) rather than wall-clock time on native hardware, but the method scales to 70B models and outperforms heuristic baselines. **Key Takeaway:** The design pattern of 'collect sensitivity stats -> solve lightweight ILP -> dynamic reconfiguration' is highly relevant for our work on optimizing LLM serving and agent compute budgets; it proves standard OR solvers are fast enough to operate within the runtime loop of high-performance AI systems.", "methodology": {"core_method": "Integer Linear Programming (ILP) for layer-wise precision optimization guided by loss divergence and weight divergence metrics", "llm_role": "none", "llm_model_used": null, "search_type": "exact", "novelty_claim": "This paper introduces SNIP, a fine-grained adaptive mixed-precision training framework for LLM pretraining that supports subbyte precision, which periodically collects statistics to assess precision loss impact and defines loss divergence and weight divergence metrics to guide an ILP problem for optimizing layerwise precision.", "components": ["statistics collection", "divergence analysis", "Integer Linear Programming (ILP) solver", "asynchronous quantization update"], "training_required": true}, "tags": {"methods": ["mixed_precision_training", "quantization", "integer_linear_programming", "adaptive_precision", "statistics_collection", "divergence_analysis", "adamw_optimizer", "pipeline_parallelism", "distributed_data_parallelism", "deepspeed_zero", "tensor_parallelism", "fully_sharded_data_parallelism"], "problems": ["llm_pretraining", "llm_training_optimization", "layerwise_precision_optimization"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": null, "specific_domain": "llm_pretraining", "llm_coupling": null}, "problem": {"formal_name": "Adaptive Mixed Precision Framework for Subbyte Large Language Model Training", "short": "LLM Mixed Precision Training", "class_": "llm_training_optimization", "properties": ["subbyte precision", "adaptive", "fine-grained", "layer-wise", "quantization", "pretraining"], "scale": "1B, 3B, 7B, 70B Llama-like models"}, "lineage": {"direct_ancestors": [{"paper": "Mixed Precision Training", "relationship": "builds upon foundational mixed precision techniques from"}, {"paper": "DeepSeek-V3 Technical Report", "relationship": "extends FP8 training recipes and quantization strategies from"}, {"paper": "Optimizing Large Language Model Training Using FP4 Quantization", "relationship": "integrates and extends FP4 training approaches from"}, {"paper": "FGMP: Fine-Grained Mixed-Precision Weight and Activation Quantization for Hardware-Accelerated LLM Inference", "relationship": "builds on ideas of layer-wise precision policies from"}], "closest_prior_work": "DeepSeek-V3 Technical Report", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Extend low-precision support to reduce-scatter operations for communication overhead reduction", "Explore integration with other emerging quantization techniques and formats", "Evaluate end-to-end performance on hardware with native FP4/FP8 support", "Deepen analysis of different quantization granularities beyond tile-wise and block-wise"], "transferable_to": ["Other deep learning model architectures (e.g., vision, multimodal models)", "LLM fine-tuning and adaptation tasks", "Different hardware architectures with varying precision capabilities", "Quantization for LLM inference, adapting training-specific metrics"], "open_weaknesses": ["Reliance on FLOPs as a proxy for efficiency due to lack of native hardware access", "Computational overhead of statistics collection and ILP solving, despite asynchronous execution", "Potential noise and inconsistencies in baseline evaluations", "Approximation of second-order derivatives for weight divergence estimation"]}, "artifacts": {"code_url": "https://github.com/pyjhzwh/SNIP", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.07, "experiments": {"benchmarks": ["ARC Easy (ARC-e)", "ARC Challenge (ARC-c)", "MMLU", "BoolQ", "PiQA", "HellaSwag", "OpenBookQA", "WinoGrande"], "baselines": ["Uniform Precision (BF16)", "Uniform Precision (FP8)", "Uniform Precision (FP4)", "Empirical Coarse-grained Quantization (E-layer-id)", "Empirical Coarse-grained Quantization (E-layer-type)", "Fine-Grained Quantization by Absolute Error Minimization (min-abs-err)", "Fine-Grained Quantization by Relative Error Minimization (min-rel-err)", "Random Fine-grained Quantization"], "hardware": "4 NVIDIA A40 GPUs (40GB each), 8 NVIDIA A100 GPUs (80GB each), 64 H100 GPUs", "instance_sizes": [1, 3, 7, 70]}, "results": {"vs_baselines": {"BF16": "SNIP achieves near BF16 accuracy (e.g., TinyLlama 1B average 44.21 vs 44.22 at 75% FP4 FLOPs)", "FP8": "SNIP outperforms FP8 in HellaSwag (+0.59% vs +0.24% for Llama 70B at 50% FP4 FLOPs)", "FP4": "SNIP significantly outperforms uniform FP4 (e.g., TinyLlama 1B average 44.27 vs 33.18 at 80% FP4 FLOPs)", "min-abs-err": "SNIP consistently outperforms min-abs-err (e.g., TinyLlama 1B average 44.16 vs 33.70 at 50% FP4 FLOPs)", "min-rel-err": "SNIP consistently outperforms min-rel-err (e.g., TinyLlama 1B average 44.16 vs 34.32 at 50% FP4 FLOPs)", "E-layer-type": "SNIP consistently outperforms E-layer-type (e.g., TinyLlama 1B average 44.16 vs 33.11 at 50% FP4 FLOPs)", "E-layer-id": "SNIP consistently outperforms E-layer-id (e.g., TinyLlama 1B average 44.16 vs 33.26 at 50% FP4 FLOPs)", "Random Fine-grained Quantization": "SNIP significantly outperforms random schemes, which often fail to converge or show large performance drops."}, "scalability": "SNIP demonstrates scalability to larger models (1B, 3B, 7B, 70B) and maintains stable performance across various training checkpoints.", "statistical_rigor": "Training loss curves for BF16 and SNIP nearly overlap, with SNIP exhibiting slightly higher training loss. Authors acknowledge potential noise in baselines outperforming BF16.", "limitations_acknowledged": ["Lack of access to hardware natively supporting both FP8 and FP4 for end-to-end runtime measurements, thus using fraction of FP4 FLOPs as a proxy for efficiency.", "Potential noise in evaluations of some baselines (min-abs-err, min-rel-err) where they sometimes outperform BF16 baseline in test accuracy."]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2601.17899", "arxiv_url": "https://arxiv.org/abs/2601.17899", "title": "Evolving Interdependent Operators with Large Language Models for Multi-Objective Combinatorial Optimization", "authors": ["Junhao"], "abstract": "", "published_date": "2026-02-01", "affiliations": "", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 8, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper directly addresses the 'coupling' problem in evolutionary search (where optimizing components in isolation fails), which is a known bottleneck in our AlgoEvo project. The hierarchical approach of searching 'design thoughts' (strategies) via MCTS before generating code is a concrete architectural pattern we should adopt to handle interdependent heuristics."}, "brief": "E2OC introduces a hierarchical search framework where MCTS optimizes 'design thoughts' (textual strategies) rather than raw code, subsequently using these strategies to guide a coordinate-descent-style evolution of interdependent operators. While the computational cost is high due to the inner-loop operator rotation, the results on FJSP/TSP (+20% HV vs expert) and comparisons against FunSearch/EoH demonstrate that explicitly modeling operator coupling is superior to isolated evolution. The critical takeaway for us is the **'strategy-first' search layer**: evolving a semantic blueprint for component interaction *before* code generation prevents the local optima trap of independent component optimization, a technique we should immediately test in AlgoEvo.", "methodology": {"core_method": "Monte Carlo Tree Search for progressive design strategy search with operator rotation evolution", "llm_role": "heuristic_generator", "llm_model_used": "deepseek-chat", "search_type": "hybrid", "novelty_claim": "E2OC is a new algorithm design paradigm that supports the LLM-based co-evolution of design strategies and codes, achieving automated design of multi-operators in MOEAs with minimum hand-craft.", "components": ["Monte Carlo Tree Search", "Operator Rotation Mechanism", "LLM-based Algorithm Generator", "LLM-based Prompt Generator", "Multi-objective Evaluator", "Warm-start initialization"], "training_required": true}, "tags": {"methods": ["monte_carlo_tree_search", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "evolutionary_algorithms", "program_synthesis", "multi_objective_optimization"], "problems": ["heuristic_evolution", "operator_discovery", "job_shop_scheduling", "tsp", "multi_objective_combinatorial_optimization"], "contribution_type": ["new_method", "framework", "sota_result"], "framework_lineage": null, "specific_domain": "multi_objective_combinatorial_optimization", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Automated Heuristic Design for Multi-Objective Evolutionary Algorithms", "short": "AHD for MOEAs", "class_": "algorithm_design", "properties": ["multi_objective", "combinatorial", "np_hard", "interdependent_operators", "evolutionary_algorithms"], "scale": "FJSP (15 instances), TSP (20-200 nodes)"}, "lineage": {"direct_ancestors": [{"paper": "MCTS-AHD", "relationship": "extends Monte Carlo Tree Search for automated heuristic design"}, {"paper": "FunSearch", "relationship": "builds on LLM-based evolutionary search for algorithms"}, {"paper": "EoH", "relationship": "builds on LLM-based evolutionary search for algorithms"}], "closest_prior_work": "MCTS-AHD", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Improve efficiency of the design thoughts search to reduce high evaluation costs.", "Enhance LLM capabilities and reduce dependence on explicit domain knowledge for broader applicability.", "Explore more complex structures for interdependent operators.", "Apply the framework to a wider range of multi-objective combinatorial optimization problems."], "transferable_to": ["Other multi-objective combinatorial optimization problems (e.g., VRP variants, complex scheduling problems).", "Automated design of other types of metaheuristics or exact algorithm components.", "Problems requiring the discovery and co-evolution of interdependent components beyond operators."], "open_weaknesses": ["High computational cost associated with the design search process.", "Dependence on the inherent capabilities and domain knowledge boundaries of the LLMs used.", "Potential for generating sub-optimal solutions if LLM suggestions are of low quality.", "Scalability challenges for extremely large problem instances or highly complex operator search spaces."]}, "artifacts": {"code_url": "null", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_status": "stable", "bridge_score": 0.6752, "is_bridge": true, "priority_score": 8.14, "experiments": {"benchmarks": ["Brandimarte benchmark set (FJSP)", "M-objective TSP instances"], "baselines": ["NSGA-II (Deb et al. 2002)", "NSGA-III (Deb and Jain 2014)", "MOEA/D (Qingfu Zhang and Hui Li 2007)", "Random (Zhang et al. 2024b)", "FunSearch (Romera-Paredes et al. 2024)", "EoH (Liu et al. 2024a)", "ReEvo (Ye et al. 2024)", "MCTS-AHD (Zheng et al. 2025b)", "Coordinate-Descent (CD)", "Upper-Confidence-Bound (UCB)", "LLM (multi-heuristic)", "Win-UCB", "MCTS OC", "MCTS Tuple", "MCTS Sample"], "hardware": "null", "instance_sizes": [20, 50, 100, 150, 200]}, "results": {"vs_baselines": {"NSGA-II (Expert, Bi-FJSP)": "+22.00% HV", "MCTS-AHD (Bi-FJSP)": "+7.3% HV on all instances", "Win-UCB (Bi-FJSP)": "+7.9% HV on all instances", "2opt (Bi-TSP100)": "+1.6% HV", "oropt 2opt 3opt (Bi-TSP200)": "+22.06% HV"}, "scalability": "E2OC demonstrates strong generalization performance across different problem scales, consistently achieving the best results on small-scale (Bi-TSP20-200) and large-scale (Bi-TSP150-200) instances.", "statistical_rigor": "All algorithms are independently executed five times, and mean HV and IGD, along with confidence intervals, are reported.", "limitations_acknowledged": ["High Evaluation Cost of Design Thoughts Search", "Dependence on Domain Knowledge and Capability Boundaries of LLMs"]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2507.11352", "arxiv_url": "https://arxiv.org/abs/2507.11352", "title": "Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces", "authors": ["Yunhao"], "abstract": "", "published_date": "2026-01-30", "affiliations": "Neurosymbolic Intelligence, The University of Texas at Austin, University of Colorado Boulder", "category": "OR for Generative AI", "relevance": {"methodological": 6, "problem": 5, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "The paper demonstrates a closed-loop refinement system where uncertainty (measured via latent space distance) drives DPO and prompt optimization. While their specific task (3-class intent classification) is trivial compared to our code generation needs, the architecture of using a 'probabilistic guarantee' as a proxy reward signal is directly transferable to improving sample efficiency in AlgoEvo."}, "brief": "Yang et al. introduce a neurosymbolic agent that translates natural language into PDDL goals, using a learned latent space to estimate 'intent uncertainty' (distance to class centroids) which gates downstream execution. They use this uncertainty signal to drive both Direct Preference Optimization (DPO) and prompt optimization (TextGrad), achieving higher accuracy than GPT-5 on a lightweight model. **Takeaway:** The concept of deriving a 'probabilistic guarantee' from latent embeddings to serve as a cheap proxy reward or filter is a concrete technique we should test in AlgoEvo to reduce expensive evaluations. However, be skeptical of the topline results as they rely on a simplistic 3-class classification task rather than complex reasoning.", "methodology": {"core_method": "Neurosymbolic Vision-Language Logistics (VLL) agent with uncertainty-aware intent-verification loop, using latent space learning, probabilistic guarantees, DPO, and TextGrad for refinement.", "llm_role": "goal_interpreter_and_refiner", "llm_model_used": "GPT-4o-mini", "search_type": "constructive", "novelty_claim": "The paper introduces a neurosymbolic Vision-Language Logistics (VLL) agent with an uncertainty-aware intent-verification loop that provides probabilistic guarantees on goal interpretation, enabling proactive clarification and uncertainty-guided refinement.", "components": ["Vision models", "Language model (GPT-4o-mini backbone)", "Symbolic PDDL solver", "Symbolic verifier", "Probabilistic field-level uncertainty estimator", "Embedding model P (latent space learning)", "Supervised contrastive loss", "Calibration distribution FC", "Direct Preference Optimization (DPO)", "TextGrad for prompt optimization"], "training_required": true}, "tags": {"methods": ["neurosymbolic_ai", "multimodal_ai", "uncertainty_estimation", "probabilistic_guarantee", "formal_verification", "llm_fine_tuned", "direct_preference_optimization", "llm_prompt_optimization", "textgrad", "contrastive_learning", "symbolic_planning", "intent_classification", "llm_in_the_loop"], "problems": ["logistics_planning", "airlift_planning", "intent_classification", "natural_language_interface"], "contribution_type": ["new_method", "framework", "empirical_study", "sota_result"], "framework_lineage": null, "specific_domain": "airlift_logistics_planning", "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Uncertainty-Aware Intent Verification for Vision-Language Logistics Planning", "short": "VLL Intent Verification", "class_": "intent_classification_for_logistics", "properties": ["multimodal", "neurosymbolic", "uncertainty-aware", "certifiable", "conversational"], "scale": "3 goal types, 400 training samples, 200 testing samples"}, "lineage": {"direct_ancestors": [{"paper": "Rafailov et al., 2023", "relationship": "uses direct preference optimization (DPO) for refinement"}, {"paper": "Yuksekgonul et al., 2024", "relationship": "uses TextGrad for prompt optimization"}, {"paper": "Bhatt et al., 2025", "relationship": "builds on formal framework for uncertainty in multimodal foundation models"}, {"paper": "Yang et al., 2024a", "relationship": "builds on fine-tuning language models using formal methods feedback"}], "closest_prior_work": "Bhatt et al., 2025", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["extend to end-to-end certifiable logistics pipelines", "integrate symbolic proof engines for plan correctness", "support real-time grounding from streaming perceptual inputs", "enable cache-guided plan updates under changing conditions"], "transferable_to": ["robotic_execution", "autonomous_driving", "database_querying", "decision_support_systems"], "open_weaknesses": ["focuses on intent verification, not full end-to-end certifiable logistics", "downstream optimization quality not primary focus of evaluation", "relies on manually labeled data for latent space learning and calibration", "probabilistic guarantee is a lower bound, not an exact probability"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.75, "experiments": {"benchmarks": ["Custom airlift logistics planning domain (for intent classification)"], "baselines": ["GPT-4.1", "GPT-5", "GPT-4o-mini (zero-shot)", "Llama-2-70B", "Mistral-7B", "Mixtral-8x7B"], "hardware": "null", "instance_sizes": [400, 200, 100, 200]}, "results": {"vs_baselines": {"GPT-4.1": "VLL (combined) outperforms by ~16% accuracy (0.96 vs 0.8)", "GPT-5": "VLL (combined) outperforms by ~11% accuracy (0.96 vs 0.85)", "GPT-4o-mini (zero-shot)": "VLL (fine-tuned) achieves 0.96 accuracy vs 0.73 for zero-shot", "Llama-2-70B": "VLL (combined) outperforms by ~21% accuracy (0.96 vs 0.75)", "Mistral-7B": "VLL (combined) outperforms by ~31% accuracy (0.96 vs 0.65)", "Mixtral-8x7B": "VLL (combined) outperforms by ~26% accuracy (0.96 vs 0.7)"}, "scalability": "The lightweight VLL model, fine-tuned on limited data, surpasses 20x larger models in goal classification accuracy while cutting inference latency by nearly 50%.", "statistical_rigor": "Evaluated on 200 independent test samples; accuracy and re-query frequency reported across varying thresholds.", "limitations_acknowledged": ["The current work focuses on intent verification rather than the full end-to-end certifiable logistics pipeline.", "Downstream optimization quality was not the primary focus of evaluation."]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2601.21372", "arxiv_url": "https://arxiv.org/abs/2601.21372", "title": "NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents", "authors": ["Yang"], "abstract": "", "published_date": "2026-01-29", "affiliations": "Carnegie Mellon University, C3 AI", "category": "Generative AI for OR", "relevance": {"methodological": 9, "problem": 9, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper introduces a 'Simulator-Optimizer' asymmetric validation loop that solves the ground-truth-free evaluation problem in code generation. This architecture directly addresses our need for better fitness signals in AlgoEvo and robust modeling in OR-Bench, offering a concrete pattern to replace brittle unit tests."}, "brief": "NEMO achieves SOTA on 8/9 optimization benchmarks by deploying autonomous coding agents that generate both a declarative optimizer (solver code) and an imperative simulator (verification code). The key innovation is using the simulator to validate the optimizer's results in a closed loop, detecting logical errors without ground truth—a technique that beats fine-tuned models like SIRL by up to 28%. The most stealable insight is this asymmetric validation: imperative Python simulation is often less error-prone than declarative constraint formulation, making it a robust 'critic' for generated solvers. This is immediately applicable to our OR-Bench and AlgoEvo projects for generating reliable reward signals.", "methodology": {"core_method": "Execution-Aware Optimization Modeling via Autonomous Coding Agents (ACAs) with asymmetric simulator-optimizer validation loop", "llm_role": "code_writer", "llm_model_used": "OpenAI o3, Claude 3.7 Sonnet, Qwen3-Embedding-8B", "search_type": "hybrid", "novelty_claim": "NEMO introduces an execution-aware, agentic framework using remote ACAs and an asymmetric simulator-optimizer validation loop for robust optimization modeling from natural language.", "components": ["Autonomous Coding Agents (ACAs)", "Decision Process Extractor", "Simulator", "Solver Recommender", "Optimizer", "External Memory for Few-shot Learning", "Minimum Bayes Risk (MBR) Decoding", "Self-Consistency for Solution Trajectories", "Asymmetric Validation Loop"], "training_required": false}, "tags": {"methods": ["llm_code_generation", "llm_in_the_loop", "llm_prompt_optimization", "in_context_learning", "autonomous_coding_agents", "minimum_bayes_risk_decoding", "self_consistency", "asymmetric_validation", "execution_aware_modeling", "program_synthesis", "vector_embedding", "retrieval_augmented_generation"], "problems": ["optimization_model_synthesis", "linear_programming", "mixed_integer_linear_programming", "non_linear_programming", "second_order_cone_programming", "knapsack", "scheduling", "routing", "facility_location", "assignment", "transportation", "network_flow", "tsp", "vehicle_routing", "resource_allocation", "production_planning", "inventory_management", "cutting_stock", "bin_packing"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "nemo", "specific_domain": "optimization_model_synthesis", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Automated Optimization Modeling from Natural Language Descriptions", "short": "Automated Optimization Modeling", "class_": "program_synthesis", "properties": ["linear_programming", "mixed_integer_linear_programming", "non_linear_programming", "second_order_cone_programming"], "scale": "unspecified individual instance dimensions, covers LP, MILP, NLP, SOCP problem types"}, "lineage": {"direct_ancestors": [{"paper": "OptimAI", "relationship": "improves upon agent-based framework"}, {"paper": "OptiMUS", "relationship": "improves upon agent-based framework"}, {"paper": "OR-LLM-Agent", "relationship": "improves upon agent-based framework"}, {"paper": "Chain-of-Experts", "relationship": "improves upon agent-based framework"}, {"paper": "OpenHands", "relationship": "uses as autonomous coding agent platform"}], "closest_prior_work": "OptimAI", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["acceleration_strategies_for_acas", "parallelizing_aca_runs", "distilling_recurring_patterns_into_components", "learning_from_execution_grounded_feedback"], "transferable_to": ["scientific_model_synthesis", "software_engineering_code_generation_with_validation", "automated_decision_support_systems"], "open_weaknesses": ["high_computational_overhead", "energy_consumption_at_scale", "requires_human_oversight_in_high_stakes_domains", "data_quality_issues_in_benchmarks"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.62, "experiments": {"benchmarks": ["OptiBench", "OptMATH-Bench", "NL4OPT", "NLP4LP", "BWOR", "IndustryOR", "MAMO-Easy", "MAMO-Complex", "ComplexOR"], "baselines": ["OptimAI", "OptiMUS", "OR-LLM-Agent", "CoE", "OptMATH", "ORLM", "LLMOPT", "SIRL"], "hardware": "OpenAI o3, Claude 3.7 Sonnet (via OpenHands), Qwen3-Embedding-8B on unspecified hardware; 5-10 minutes inference time per instance.", "instance_sizes": [605, 166, 245, 269, 82, 100, 652, 211, 18]}, "results": {"vs_baselines": {"OptimAI": "+8.1% on OptiBench", "OptiMUS": "+19.6% on NL4OPT, +9.4% on NLP4LP, +11.1% on ComplexOR", "OR-LLM-Agent": "+22.5% on NL4OPT, 0% on BWOR, +27.0% on IndustryOR, +1.2% on MAMO-Easy, +20.4% on MAMO-Complex", "CoE": "+34.2% on NL4OPT, +28.3% on NLP4LP, +39.7% on ComplexOR", "OptMATH": "+24.3% on OptiBench, +31.0% on OptMATH-Bench, +2.5% on NL4OPT, -6.5% on MAMO-Easy, +17.9% on MAMO-Complex", "ORLM": "+11.9% on NL4OPT, +25.0% on IndustryOR, -1.8% on MAMO-Easy", "LLMOPT": "+24.0% on OptiBench, +25.7% on OptMATH-Bench, +19.0% on IndustryOR, -13.8% on MAMO-Complex, +5.1% on ComplexOR", "SIRL": "+23.0% on OptiBench, +19.9% on OptMATH-Bench"}, "scalability": "Incurs 5-10 minutes computational overhead per instance due to iterative code generation and validation loops, with increased reasoning effort yielding higher solution quality.", "statistical_rigor": "No statistical significance or variability measures reported; internal non-determinism mitigated via MBR and self-consistency protocols.", "limitations_acknowledged": ["High computational overhead (5-10 minutes per instance)", "Requires human oversight in high-stakes domains", "Potential for substantial energy consumption at scale", "Ethical concerns with automated decision-making", "Reliance on existing benchmarks with data quality issues"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2601.21239", "arxiv_url": "https://arxiv.org/abs/2601.21239", "title": "TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design", "authors": ["Chentong"], "abstract": "", "published_date": "2026-01-29", "affiliations": "", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 10, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper proposes a concrete architectural fix for 'numerical blindness' in LLM evolutionary search by embedding a Differential Evolution layer. It directly addresses our sample efficiency and precision bottlenecks in AlgoEvo."}, "brief": "TIDE introduces a nested evolutionary framework that strictly decouples algorithmic structure generation (via LLM) from numerical parameter tuning (via Differential Evolution), managed by a Tree Similarity Edit Distance (TSED) guided island model. Results on 9 COPs (TSP, BPP, etc.) show it consistently outperforms ReEvo and EoH, primarily because the DE layer optimizes constants at zero token cost, preventing the discard of structurally sound but poorly tuned heuristics. The critical takeaway is the necessity of a gradient-free tuning layer for LLM-generated code; relying on LLMs for numerical constants is inefficient and imprecise. We should immediately implement a similar parameter-tuning inner loop in our AlgoEvo framework.", "methodology": {"core_method": "Nested evolutionary framework with TSED-guided island model and co-evolutionary inner loop (UCB-based LLM logic generation + differential mutation for parameter tuning)", "llm_role": "heuristic_generator", "llm_model_used": "Qwen3-Max-2025-09-23", "search_type": "hybrid", "novelty_claim": "TIDE decouples structural reasoning from parameter optimization using a nested evolutionary framework with a TSED-guided island model and a UCB-based scheduler for prompt strategies.", "components": ["TSED-guided Island Model", "Adaptive Migration Strategy", "Constructive Fusion Reset", "Co-Evolutionary Inner Loop", "UCB-based Scheduler", "LLM-based Logic Generation", "Differential Mutation Operator", "Abstract Syntax Tree (AST) parsing", "Structural Normalization", "APTED algorithm"], "training_required": false}, "tags": {"methods": ["evolutionary_algorithm", "island_model_ea", "co_evolution", "differential_evolution", "llm_as_heuristic", "llm_code_generation", "ucb", "multi_armed_bandit", "program_synthesis", "ast_manipulation", "evolution_of_heuristics", "automated_algorithm_design"], "problems": ["heuristic_design", "tsp", "knapsack", "bin_packing", "assignment_problem", "cvrp", "open_pit_mining", "multi_dimensional_knapsack", "dynamic_programming_problem", "mountain_car"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "eoh", "specific_domain": null, "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Automated Heuristic Design", "short": "AHD", "class_": "algorithm_design", "properties": ["NP-hard", "combinatorial_optimization", "parameterized_heuristics", "stochastic_generation", "non-stationary_rewards"], "scale": "50-654 nodes (TSP, CVRP, OP, TSPLIB), 50-200 items (KP, MKP), 1k-10k items (BPP), 12-21 dimensions (ASP), Mountain Car control task"}, "lineage": {"direct_ancestors": [{"paper": "EoH", "relationship": "extends the concept of LLM-based heuristic evolution from"}, {"paper": "ReEvo", "relationship": "extends the concept of LLM-based heuristic evolution from"}], "closest_prior_work": "EoH", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["explore_multi_objective_optimization", "investigate_transferability_of_learned_insights"], "transferable_to": ["other_combinatorial_optimization_problems", "continuous_optimization_problems"], "open_weaknesses": ["limited_to_single_objective_optimization", "limited_transferability_across_disparate_domains", "potential_computational_cost_of_llm_interactions"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.72, "experiments": {"benchmarks": ["TSP", "KP", "online BPP", "ASP", "TSP GLS", "TSP KGLS", "CVRP", "OP", "MKP", "offline BPP", "DPP", "Mountain Car-v0", "TSPLIB"], "baselines": ["Greedy", "POMO", "EoH", "ReEvo", "HSEvo", "MCTS-AHD", "LKH3", "OR-Tools", "NeuOpt", "DeepACO", "First Fit", "Best Fit"], "hardware": "Intel Core i7-12700 CPU", "instance_sizes": [12, 15, 21, 50, 51, 70, 76, 99, 100, 101, 107, 124, 144, 150, 152, 159, 195, 200, 225, 226, 318, 400, 417, 500, 654, 1000, 5000, 10000]}, "results": {"vs_baselines": {"EoH": "TIDE achieves -7.35% gap on Constructive TSP N=50 (4.76% vs 12.11%) and -0.6% on Constructive KP N=50 (0.04% vs 0.60%)", "ReEvo": "TIDE achieves -3.51% gap on Constructive TSP N=50 (4.76% vs 8.27%) and -0.17% on Constructive KP N=50 (0.04% vs 0.21%)", "HSEvo": "TIDE achieves -3.28% gap on Constructive TSP N=50 (4.76% vs 8.04%) and -0.13% on Constructive KP N=50 (0.04% vs 0.17%)", "MCTS-AHD": "TIDE achieves -3.43% gap on Constructive TSP N=50 (4.76% vs 8.19%) and -0.13% on Constructive KP N=50 (0.04% vs 0.17%)", "POMO": "TIDE outperforms by +2.08% on Constructive KP N=50 (0.04% vs 2.12%)", "NeuOpt": "TIDE outperforms by +0.278% on Improvement TSP GLS N=100 (0.026% vs 0.304%)", "DeepACO": "TIDE outperforms by +0.04 on ACO TSP N=50 (5.80 vs 5.84)", "Greedy": "TIDE outperforms by +22.58% on Constructive TSP N=50 (4.76% vs 22.62%)", "First Fit": "TIDE outperforms by +1.44% on Online BPP 1k (3.33% vs 4.77%)", "Best Fit": "TIDE outperforms by +1.69% on Online BPP 1k (3.33% vs 5.02%)"}, "scalability": "TIDE maintains robust performance across various problem scales, with its adaptive mechanisms preventing premature convergence and enabling continued improvement on larger instances.", "statistical_rigor": "All LLM-based method results are averaged over three runs. Ablation studies and comparative experiments are performed under identical settings to ensure fair comparison.", "limitations_acknowledged": ["Future work will explore applying TIDE to multi-objective optimization scenarios", "Future work will investigate the transferability of learned structural insights across disparate problem domains"]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2601.20539", "arxiv_url": "https://arxiv.org/abs/2601.20539", "title": "PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs", "authors": ["Oguzhan"], "abstract": "", "published_date": "2026-01-29", "affiliations": "", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 8, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper directly addresses your specific interest in 'memory' and 'planning' within LLM evolutionary search. It replaces standard population-based mutation with a structured 'Entailment Graph' and a Policy/World-Model split, offering a concrete architecture to upgrade AlgoEvo's stateless nature."}, "brief": "PathWise reformulates heuristic discovery as a sequential planning problem over an 'Entailment Graph,' where a Policy Agent generates high-level evolutionary directives (rationales) and a World Model executes the code, guided by specific Critic reflections. The results are robust: it outperforms ReEvo, FunSearch, and MCTS-AHD on TSP, CVRP, and Bin Packing while using half the evaluation budget (500 vs 1000), demonstrating genuine sample efficiency. The key takeaway is the **Entailment Graph** structure: explicitly storing the *derivation rationale* and lineage allows the LLM to reason about the search trajectory and avoid redundant failures, a mechanism we should immediately adapt for AlgoEvo to fix our memory bottleneck.", "methodology": {"core_method": "Multi-agent reasoning framework (Policy Agent, World Model Agent, Critic Agents) for Automated Heuristic Design, formulated as a sequential decision process over an entailment graph.", "llm_role": "heuristic_generator", "llm_model_used": "GPT-4o-mini", "search_type": "hybrid", "novelty_claim": "We propose a novel multi-agent reasoning framework, referred to as Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs (PathWise), which formulates heuristic generation as a sequential decision process over an entailment graph serving as a compact, stateful memory of the search trajectory.", "components": ["Policy Agent", "World Model Agent", "Policy Critic", "World Model Critic", "Entailment Graph", "Prompt-level Diversity (Prompt Perturbation)", "State Shuffling", "Population Management"], "training_required": false}, "tags": {"methods": ["multi_agent_reasoning", "llm_evolutionary_search", "evolution_of_heuristics", "llm_as_heuristic", "llm_code_generation", "entailment_graph", "prompt_engineering", "population_management", "program_synthesis"], "problems": ["automated_heuristic_design", "heuristic_evolution", "operator_discovery", "tsp", "knapsack_problem", "cvrp", "multiple_knapsack_problem", "orienteering_problem", "bin_packing_problem", "combinatorial_optimization_problems"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": null, "specific_domain": null, "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Automated Heuristic Design for Combinatorial Optimization Problems", "short": "AHD", "class_": "automated_heuristic_design", "properties": ["NP-hard", "sequential_decision_process", "multi-agent_reasoning", "stateful_memory"], "scale": "50-10000 nodes/items"}, "lineage": {"direct_ancestors": [{"paper": "FunSearch", "relationship": "builds on concept of LLM-based automated heuristic design"}, {"paper": "EoH", "relationship": "builds on concept of LLM-based automated heuristic design"}], "closest_prior_work": "FunSearch", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["address stochasticity and improve diversity of LLM outputs", "explore different LLM backbones and reasoning levels", "extend to other complex combinatorial optimization problems", "investigate more sophisticated population management or search strategies"], "transferable_to": ["other combinatorial optimization problems", "automated algorithm design for continuous optimization", "multi_agent planning systems"], "open_weaknesses": ["stochasticity of LLM outputs leading to instability and reduced diversity", "performance variability across problem domains and LLM backbones", "computational cost of LLM calls", "lack of theoretical guarantees for generated heuristics"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.92, "experiments": {"benchmarks": ["Traveling Salesman Problem (TSP)", "Knapsack Problem (KP)", "Capacitated Vehicle Routing Problem (CVRP)", "Multiple Knapsack Problem (MKP)", "Orienteering Problem (OP)", "Bin Packing Problem (BPP)", "TSPLIB"], "baselines": ["Greedy Construct", "POMO", "Best Fit", "First Fit", "ACO", "DeepACO", "Knowledge-guided Local Search (KGLS)", "VRP-DACT", "NeuOpt", "NeuralGLS", "GNNGLS", "FunSearch", "EoH", "ReEvo", "HSEvo", "MCTS-AHD"], "hardware": "single AMD Ryzen Threadripper PRO 7985WX CPU", "instance_sizes": [50, 100, 200, 300, 500, 1000, 5000, 10000]}, "results": {"vs_baselines": {"Greedy Construct": "Consistently outperforms on TSP and KP (constructive framework)", "POMO": "Outperforms on TSP (N=200) and KP (constructive framework)", "Best Fit": "Achieves higher MRGI (54.48% GPT-4o-mini) on Online BPP", "First Fit": "Achieves higher MRGI (54.48% GPT-4o-mini) on Online BPP", "ACO": "Consistently outperforms manually designed ACO heuristics across all test sets and LLM backbones", "DeepACO": "Outperforms on OP (N=200) and MKP (ACO framework)", "FunSearch": "Achieves average MRGI of 20.38% on TSP (constructive) and 49.89% on KP (constructive) with GPT-4o-mini", "EoH": "Achieves average MRGI of 20.38% on TSP (constructive) and 49.89% on KP (constructive) with GPT-4o-mini", "ReEvo": "Achieves average MRGI of 20.38% on TSP (constructive) and 49.89% on KP (constructive) with GPT-4o-mini", "HSEvo": "Achieves average MRGI of 20.38% on TSP (constructive) and 49.89% on KP (constructive) with GPT-4o-mini", "MCTS-AHD": "Achieves average MRGI of 20.38% on TSP (constructive) and 49.89% on KP (constructive) with GPT-4o-mini", "KGLS": "Achieves average MRGI of 3.77% on TSP (GLS framework) with GPT-4o-mini", "VRP-DACT": "Outperforms on TSP (GLS framework)", "NeuOpt": "Outperforms on TSP (GLS framework)", "NeuralGLS": "Outperforms on TSP (GLS framework)", "GNNGLS": "Outperforms on TSP (GLS framework)"}, "scalability": "PathWise exhibits strong scalability with respect to problem size, with effectiveness growing as problem complexity increases (e.g., MRGI on KP increases from 31.67% on ID to 81.34% on largest OOD instances).", "statistical_rigor": "Mean performance is reported over 3 independent runs for LLM-based AHD methods, with evolution curves showing lower variance.", "limitations_acknowledged": ["Heuristic generation relies on stochastic LLM outputs, leading to temporary instability and reduced effective diversity.", "Heuristic performance varies across problem domains when different LLM backbones or reasoning levels are used, due to differences in model training and post-processing."]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2601.21511", "arxiv_url": "https://arxiv.org/abs/2601.21511", "title": "LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI", "authors": ["Niki"], "abstract": "", "published_date": "2026-01-29", "affiliations": "", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 9, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper introduces a concrete, implementable mechanism (AST features -> Surrogate -> SHAP -> Prompt) to provide structural feedback to the LLM, directly addressing our focus on 'better signal for search' and sample efficiency in AlgoEvo. It moves beyond simple fitness feedback to guiding the 'shape' of the code."}, "brief": "LLaMEA-SAGE augments LLM-based evolutionary search by extracting AST features (complexity, graph metrics) from generated code, training a surrogate model to predict fitness from these features, and using SHAP analysis to generate natural language prompts that guide the LLM to modify specific structural properties (e.g., 'increase cyclomatic complexity'). On the MA-BBOB benchmark, it outperforms state-of-the-art methods (MCTS-AHD, LHNS) and converges faster than vanilla LLaMEA, although the authors honestly report that statistical significance was limited (p=0.44) due to small sample sizes (5 runs). The critical takeaway for us is the pipeline of using static code analysis as a feedback signal—we can immediately steal this 'SAGE' loop to guide AlgoEvo or EvoCut by telling the LLM *how* to structurally mutate code based on surrogate correlations, rather than just hoping for random improvements.", "methodology": {"core_method": "LLaMEA-SAGE, an LLM-driven evolutionary algorithm that integrates structural feedback from Explainable AI (SHAP) analysis of Abstract Syntax Tree (AST) code features to guide mutations.", "llm_role": "evolutionary_search", "llm_model_used": "GPT-5-mini", "search_type": "hybrid", "novelty_claim": "We propose LLaMEA-SAGE which guides the algorithm discovery process using explainable AI analysis of code features, providing structured feedback to the LLM.", "components": ["LLM-based code generation", "Abstract Syntax Tree (AST) feature extraction", "Graph-theoretic statistics", "Code-level complexity indicators (cyclomatic complexity, token counts, parameter counts)", "Archive-based surrogate model (XGBoost)", "SHAP analysis for feature importance", "Natural-language mutation prompt augmentation", "Evolutionary strategy (elitist selection)"], "training_required": true}, "tags": {"methods": ["llm_evolutionary_search", "llm_code_generation", "shap_analysis", "explainable_ai", "abstract_syntax_tree_analysis", "static_code_analysis", "xgboost", "surrogate_modeling", "evolutionary_strategy", "llamea"], "problems": ["black_box_optimization", "continuous_optimization", "heuristic_evolution"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "llamea", "specific_domain": "black_box_optimization", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Automated Algorithm Design for Black-Box Optimization Algorithms", "short": "AAD", "class_": "algorithm_design", "properties": ["template-free", "black-box optimization", "continuous optimization", "noiseless", "unconstrained"], "scale": "d=10 (training), d=5, 10, 20 (validation)"}, "lineage": {"direct_ancestors": [{"paper": "LLaMEA", "relationship": "extends with structural feedback from XAI"}], "closest_prior_work": "LLaMEA", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Explore dynamic execution behavior and interaction effects of generated code", "Apply to other LLM backends and evaluate robustness", "Extend to other optimization domains beyond continuous black-box optimization", "Investigate more sophisticated code feature sets for XAI feedback"], "transferable_to": ["Discrete optimization problems", "Combinatorial optimization problems", "Automated algorithm design for other problem classes (e.g., graph algorithms)", "Multi-objective optimization"], "open_weaknesses": ["Evaluation limited to a single LLM backend (GPT-5-mini)", "Current evaluation focuses on continuous black-box optimization at moderate dimensionalities", "Code feature set captures only static structural properties, not dynamic execution behavior", "Limited number of statistical runs (5 seeds) leading to wide confidence intervals"]}, "artifacts": {"code_url": "https://anonymous.4open.science/r/LLaMEA-SAGE/README.md", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_5", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.32, "experiments": {"benchmarks": ["SBOX-COST benchmark suite", "Many Affine BBOB (MA-BBOB) benchmark"], "baselines": ["Vanilla LLaMEA", "MCTS-AHD [23]", "LHNS [22]", "NeighborhoodAdaptiveDE [16] (for validation only)"], "hardware": "null", "instance_sizes": [5, 10, 20]}, "results": {"vs_baselines": {"LLaMEA (SBOX-COST)": "LLaMEA-SAGE achieves similar performance faster, with an average improvement of 11.1 AUC units and a large effect size (Cliff’s δ=0.60).", "LLaMEA (MA-BBOB)": "LLaMEA-SAGE achieves better performance faster, with a medium effect size (Cliff’s δ=0.36), reaching fitness values LLaMEA cannot achieve within the full budget.", "MCTS-AHD (MA-BBOB)": "LLaMEA-SAGE and vanilla LLaMEA clearly outperform MCTS-AHD.", "LHNS (MA-BBOB)": "LLaMEA-SAGE and vanilla LLaMEA clearly outperform LHNS.", "NeighborhoodAdaptiveDE (MA-BBOB validation)": "The best LLaMEA-SAGE algorithm beats NeighborhoodAdaptiveDE on 10d and 20d, but not on 5d (where NeighborhoodAdaptiveDE was specifically tuned)."}, "scalability": "LLaMEA-SAGE preserves scalability relative to vanilla LLaMEA, with only a marginal increase in LLM token cost, and generalizes to different dimensionalities (5d, 10d, 20d).", "statistical_rigor": "Experiments repeated for 5 independent runs with different random seeds. Performance measured using AOCC. Statistical verification includes paired Wilcoxon signed-rank test, Cliff’s δ effect sizes, and 95% bootstrap confidence intervals for mean AUC difference. Authors acknowledge limited runs (5 seeds) lead to wide CIs and non-significant p-values (p=0.44).", "limitations_acknowledged": ["Experiments conducted using a single LLM backend (though ablation on another shows consistent outcome)", "Current evaluation focuses on continuous black-box optimization benchmarks at moderate dimensionalities", "Code feature set captures only static structural properties of code and does not account for dynamic execution behavior or interaction effects during optimization runs"]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2601.21847", "arxiv_url": "https://arxiv.org/abs/2601.21847", "title": "READY: Reward Discovery for Meta-Black-Box Optimization", "authors": ["Zechuan"], "abstract": "", "published_date": "2026-01-29", "affiliations": "", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 8, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper provides a concrete architecture (multi-task niches + explicit transfer) that directly upgrades the standard Eureka/EoH loops we currently use. The specific prompt engineering for 'History-Reflection' and 'Knowledge Transfer' addresses our active struggles with sample efficiency and preventing stagnation in AlgoEvo."}, "brief": "READY introduces a multi-task evolutionary framework where LLMs evolve reward functions for multiple MetaBBO algorithms simultaneously, utilizing explicit 'Knowledge Transfer' operators to translate successful logic between distinct tasks. The results are robust, demonstrating superior performance over Eureka and EoH on BBOB benchmarks with a 2-4x reduction in search time due to parallelization and shared heuristics. The most stealable insights are the 'History-Reflection' operator—which prompts the LLM to extrapolate trends from the evolutionary trajectory rather than just mutating the current state—and the cross-niche transfer mechanism, both of which should be implemented in our multi-agent optimization stack immediately.", "methodology": {"core_method": "LLM-based program evolution with a multi-task niche-based architecture, fine-grained evolutionary operators, and explicit knowledge transfer", "llm_role": "evolutionary_search", "llm_model_used": "DeepSeek-V3.2", "search_type": "improvement", "novelty_claim": "READY is the first LLM-driven multitask framework for automating reward discovery in MetaBBO, synergizing a niche-based architecture with specialized LLM operators to overcome the limitations of manual reward engineering in effectiveness and efficiency.", "components": ["Multitask Program Evolution", "Fine-grained Evolution Operators (Local-Reflection Mutation, History-Reflection Mutation, Global-Reflection Mutation, Exploitative Crossover, Exploratory Crossover)", "Knowledge Transfer scheme", "Greedy Niche-based Population Sampling (Expert Anchoring, Iterative In-Context Generation, Performance-Based Rejection Sampling)"], "training_required": true}, "tags": {"methods": ["llm_evolutionary_search", "program_synthesis", "multi_task_learning", "evolutionary_algorithms", "knowledge_transfer", "population_based_optimization", "eureka", "eoh", "reevo"], "problems": ["reward_discovery", "meta_black_box_optimization", "black_box_optimization", "program_synthesis"], "contribution_type": ["new_method", "sota_result", "framework"], "framework_lineage": null, "specific_domain": "meta_black_box_optimization", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Reward Discovery for Meta-Black-Box Optimization", "short": "Automated MetaBBO Reward Discovery", "class_": "program_synthesis", "properties": ["meta-learning", "bi-level optimization", "black-box optimization", "reward engineering", "multi-task"], "scale": "3 MetaBBO tasks, 16 unseen BBOB test functions"}, "lineage": {"direct_ancestors": [{"paper": "Eureka (Ma et al., 2023)", "relationship": "extends LLM-driven reward discovery to multitask settings"}, {"paper": "EoH (Liu et al., 2024b)", "relationship": "builds on LLM-driven evolutionary search principles"}, {"paper": "ReEvo (Ye et al., 2024)", "relationship": "builds on LLM-driven evolutionary search principles"}], "closest_prior_work": "Eureka (Ma et al., 2023)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Explore READY's applicability to a broader range of MetaBBO tasks and domains.", "Investigate the impact of different LLM architectures and sizes on performance.", "Further optimize the fine-grained evolutionary operators and sampling strategies.", "Apply the multitask reward discovery approach to other program synthesis challenges."], "transferable_to": ["Automated algorithm design for various optimization problems.", "General program synthesis for scientific discovery.", "Automated hyperparameter optimization for complex systems.", "Reward engineering in other reinforcement learning domains."], "open_weaknesses": ["Scalability to significantly more complex or higher-dimensional MetaBBO tasks.", "Generalizability to entirely novel domains without prior knowledge transfer.", "Potential for the evolutionary search to converge to local optima.", "Dependency on the quality of initial prompts and population for effective search."]}, "artifacts": {"code_url": "https://anonymous.4open.science/r/ICML_READY-747F", "models_released": true, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.25, "experiments": {"benchmarks": ["BBOB test suite"], "baselines": ["Handcrafted rewards", "Eureka (Ma et al., 2023)", "EoH (Liu et al., 2024b)", "ReEvo (Ye et al., 2024)"], "hardware": "Ray framework for parallel evaluation, 7 hours for entire discovery process across three tasks", "instance_sizes": [16]}, "results": {"vs_baselines": {"DEDQN": "READY achieves an average rank of 2.00, outperforming Handcrafted (4.06), EoH (2.31), ReEvo (3.44), and Eureka (3.19).", "RLEPSO": "READY achieves an average rank of 2.38, outperforming Handcrafted (2.75), EoH (3.44), ReEvo (3.06), and Eureka (3.38).", "RLDAS": "READY achieves an average rank of 2.06, outperforming Handcrafted (3.31), EoH (2.93), ReEvo (3.56), and Eureka (3.13).", "Overall": "READY outperforms Handcrafted rewards in 36 out of 48 test scenarios and achieves the lowest mean cost in 24 cases against all state-of-the-art automated baselines."}, "scalability": "READY completes the entire discovery process across three tasks in only 7 hours, representing a 2x to 4x speedup over baselines. Discovered rewards operate within the microsecond regime (e.g., 3.47µs per call for DEDQN) for inference, and the framework scales effectively with backend LLM reasoning capability.", "statistical_rigor": "Results are reported as mean minimum cost \b1 standard deviation over 51 independent runs for evaluation. During evolutionary search, each reward is tested over 3 independent runs to mitigate stochasticity.", "limitations_acknowledged": []}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2601.21096", "arxiv_url": "https://arxiv.org/abs/2601.21096", "title": "Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve", "authors": ["Hongzheng"], "abstract": "", "published_date": "2026-01-28", "affiliations": "Google DeepMind, Google, Cornell University", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 7, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper introduces a 'Hierarchical Search' pattern that solves a major bottleneck in LLM evolutionary search (LLMs are bad at tuning numerical constants). It successfully applies AlphaEvolve to production-scale problems by decoupling logic synthesis from parameter optimization."}, "brief": "Magellan couples AlphaEvolve with a black-box autotuner (Vizier) to evolve C++ compiler heuristics, achieving >5% binary size reduction in LLVM and beating both human experts and prior neural policies. The results are rigorous, validated on production workloads and showing temporal generalization. **The critical takeaway is the 'Hierarchical Search' strategy:** rather than asking the LLM to write fully specified code, they prompt it to generate *templates* with exposed parameters (flags), delegating numerical tuning to a cheap external optimizer. This directly addresses the sample efficiency issues we face in AlgoEvo; we should immediately steal this architecture to separate structural evolution from parameter tuning.", "methodology": {"core_method": "LLM-powered coding agent (AlphaEvolve) with evolutionary search and black-box autotuning (Vizier)", "llm_role": "code_writer", "llm_model_used": "Gemini-2.5-Pro", "search_type": "hybrid", "novelty_claim": "Magellan evolves the compiler pass itself by synthesizing executable C++ decision logic, coupling an LLM coding agent with evolutionary search and autotuning in a closed loop.", "components": ["AlphaEvolve", "LLM coding agent", "Vizier autotuner", "LLVM compiler", "XLA compiler"], "training_required": true}, "tags": {"methods": ["llm_code_generation", "evolutionary_search", "autotuning", "alphaevolve", "program_synthesis", "black_box_optimization", "evolution_of_heuristics"], "problems": ["compiler_optimization_heuristics_discovery", "function_inlining", "register_allocation", "e_graph_extraction", "auto_sharding"], "contribution_type": ["framework", "new_method", "sota_result", "empirical_study"], "framework_lineage": "alphaevolve", "specific_domain": null, "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Autonomous Discovery of Novel Compiler Optimization Heuristics", "short": "Compiler Heuristics Discovery", "class_": "program_synthesis", "properties": ["NP-hard", "heuristic-driven", "production-scale", "multi-objective"], "scale": "production-scale workloads, internal search application, multiple production binaries"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2506.13131", "relationship": "extends AlphaEvolve for compiler pass evolution"}, {"paper": "arXiv:2101.04808", "relationship": "contrasts with and partially replaces MLGO neural policies"}], "closest_prior_work": "arXiv:2506.13131", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["scaling up evaluations with longer search iterations", "automating feature discovery for compiler optimizations", "applying to green-field problems in different compiler infrastructures", "evolving into a general platform for adaptive compiler heuristics"], "transferable_to": ["gpu_compilers", "npu_compilers", "kernel_mapping", "operator_scheduling", "task_assignment"], "open_weaknesses": ["search loop constrained by high compilation and evaluation cost", "unclear when LLM-driven pass evolution is preferable to neural-policy approaches", "compactness of policies for broader multi-objective optimization", "scalability with search budget, model choice, and objective noise"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.81, "experiments": {"benchmarks": ["internal search application", "clang benchmark", "large-scale production search workload", "Enzyme-JAX", "ASPLOS’25 IOPDDL contest setting"], "baselines": ["upstream LLVM inlining heuristic", "neural network (MLGO)", "human-crafted heuristics (RegAllocGreedy)", "manually designed policies (e-graph extraction)", "top contest submissions (ASPLOS’25 IOPDDL)"], "hardware": "Google's internal clang/LLVM toolchain, 1.5 days to one week sequential evaluation", "instance_sizes": [38, 5, 20]}, "results": {"vs_baselines": {"LLVM upstream heuristic (inlining size, partial)": "4.27% size reduction", "LLVM upstream heuristic (inlining size, full)": "5.23% size reduction", "LLVM upstream heuristic (inlining size, autotune)": ">5% size reduction", "NN Model (inlining size, temporal)": "slightly outperforms, 5.75%-5.95% size reduction", "NN Model (inlining size, domain)": "average 8.79% size reduction vs 8.52%", "LLVM upstream heuristic (inlining performance)": "0.61% speedup", "Human-crafted heuristics (RegAllocGreedy)": "improves from -0.55% to -0.15%", "Manually designed policies (e-graph extraction)": "7% improvement", "ASPLOS’25 IOPDDL top submissions": "comparable performance"}, "scalability": "The API-based search explores more aggressively and achieves higher eventual gains, and the discovered heuristics generalize robustly across time and heterogeneous codebases.", "statistical_rigor": "Experiments involve multiple runs over 1.5 days to one week of sequential evaluation, using batches of 10-40 candidate hyperparameter configurations for autotuning, and evaluating generalization across four temporal snapshots and more than ten production binaries.", "limitations_acknowledged": ["Search loop constrained by compilation/evaluation cost", "Unclear when LLM-driven pass evolution is preferable to neural-policy approaches", "Compactness of policies for multi-objective optimization is unclear"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2601.19793", "arxiv_url": "https://arxiv.org/abs/2601.19793", "title": "CASTER: Breaking the Cost-Performance Barrier in Multi-Agent Orchestration via Context-Aware Strategy for Task Efficient Routing", "authors": ["Shanyv"], "abstract": "", "published_date": "2026-01-27", "affiliations": "University of Houston, China University of Petroleum (East China), Southwest Jiaotong University", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 6, "problem": 8, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While the neural architecture is standard, the 'negative feedback' training loop (relabeling weak-model failures as strong-model positives) is a highly practical trick for training proxy evaluators in our evolutionary search pipelines."}, "brief": "CASTER implements a context-aware neural router for multi-agent systems that dynamically selects between weak and strong models, reducing inference costs by ~72% compared to a GPT-4o-only baseline. The authors validate this on a custom benchmark across four domains, showing it outperforms cascading strategies (FrugalGPT) by avoiding the 'double-billing' of failed weak calls. The standout takeaway for us is the 'On-Policy Negative Feedback' mechanism: training the router by explicitly relabeling instances where the weak model failed as 'Strong-Required'. We should adapt this active learning logic to train our proxy reward models in AlgoEvo, allowing us to reliably offload expensive evaluations to cheaper proxies without manual annotation.", "methodology": {"core_method": "Dual-Branch Feature Fusion Network for task difficulty estimation with On-Policy iterative training via negative feedback", "llm_role": "task_generator, evaluator", "llm_model_used": "GPT-4o", "search_type": "classification", "novelty_claim": "CASTER introduces a lightweight neural router for dynamic LLM selection in multi-agent systems, combining semantic and structural features with an on-policy negative feedback training mechanism to optimize cost and performance.", "components": ["Dual-Branch Feature Fusion Network", "Semantic Embedding Encoder", "Meta-feature Extractor", "Fusion Classifier", "Cold Start Pre-training", "Iterative Fine-tuning via Negative Feedback", "Dynamic Adversarial Task Generation Pipeline", "LLM-as-a-Judge Evaluation Framework"], "training_required": true}, "tags": {"methods": ["neural_networks", "feature_fusion", "text_embeddings", "supervised_learning", "on_policy_learning", "negative_feedback_learning", "active_learning", "pre_training", "fine_tuning", "data_augmentation", "adversarial_generation", "llm_as_evaluator", "llm_orchestration_router", "dynamic_model_selection"], "problems": ["llm_resource_allocation", "cost_optimization", "multi_agent_systems", "software_engineering", "data_analysis", "scientific_discovery", "cybersecurity"], "contribution_type": ["new_method", "framework", "new_benchmark", "sota_result", "empirical_study"], "framework_lineage": null, "specific_domain": null, "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Dynamic Model Selection in Multi-Agent Systems", "short": "MAS Dynamic Model Selection", "class_": "llm_orchestration", "properties": ["multi-agent", "graph-based", "dynamic", "cost-aware", "context-aware", "long-horizon"], "scale": "multi-agent workflows with complex cyclic dependencies"}, "lineage": {"direct_ancestors": [{"paper": "FrugalGPT", "relationship": "improves upon cascading strategy of"}, {"paper": "RouteLLM", "relationship": "improves upon preference-based methods of"}, {"paper": "LangGraph", "relationship": "builds multi-agent system on"}], "closest_prior_work": "FrugalGPT", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Explore adaptive router architectures beyond fixed dual-branch.", "Integrate with real-time LLM performance and cost monitoring.", "Apply to other domains such as legal analysis and creative writing.", "Investigate multi-objective optimization for routing beyond cost and success rate."], "transferable_to": ["Heterogeneous computing resource allocation systems.", "Dynamic task scheduling in distributed systems.", "Any system with a trade-off between resource cost and performance.", "Other multi-agent systems not based on LangGraph."], "open_weaknesses": ["Potential biases in LLM-as-a-Judge evaluation for ground truth.", "Performance in scenarios with non-tiered LLM pricing (e.g., DeepSeek).", "Scalability to a much larger number of agent types or models.", "The 'Cold Start' relies on synthetic rule-based data, which might not fully capture real-world nuances."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": true}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.48, "experiments": {"benchmarks": ["Software Engineering", "Data Analysis", "Scientific Discovery", "Cybersecurity"], "baselines": ["Force Strong (GPT-4o)", "Force Weak (GPT-4o-mini)", "FrugalGPT (Cascade)"], "hardware": "LLMs accessed via standardized APIs", "instance_sizes": [10, 20]}, "results": {"vs_baselines": {"Force Strong (GPT-4o)": "-72.4% cost on OpenAI Software, matching or surpassing quality (e.g., +0.1% on Science, +0.7% on Security)", "Force Weak (GPT-4o-mini)": "+1.2% quality on Software, +1.2% on Data, +5.1% on Science, +2.7% on Security, with higher cost", "FrugalGPT (Cascade)": "+1.0% quality on Software, +0.7% on Data, +1.2% on Science, +0.8% on Security, with -20.7% to -48.0% cost"}, "scalability": "CASTER effectively flattens the cost curve compared to the Force Strong baseline and demonstrates robust generalizability across diverse LLM providers, achieving significant cost reductions while maintaining high performance.", "statistical_rigor": "Evaluated using LLM-as-a-Judge framework with stratified difficulty levels and held-out test sets. Cost distributions include standard deviations. No explicit mention of multiple runs for variance or significance tests beyond average scores.", "limitations_acknowledged": ["No specific ethical risks beyond those inherent to the underlying LLMs."]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2601.19622", "arxiv_url": "https://arxiv.org/abs/2601.19622", "title": "Algorithmic Prompt-Augmentation for Efficient LLM-Based Heuristic Design for A* Search", "authors": ["Thomas"], "abstract": "", "published_date": "2026-01-27", "affiliations": "", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 7, "problem": 5, "inspirational": 8}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "Provides a concrete, low-cost prompt engineering tactic (injecting the driver code) that allows smaller open-weights models (Qwen2.5) to outperform GPT-4o in evolutionary search. Directly actionable for reducing costs and improving sample efficiency in our AlgoEvo pipelines."}, "brief": "This paper introduces 'Algorithmic-Contextual EoH' (A-CEoH), which injects the actual source code of the search algorithm (e.g., the A* driver loop, neighbor generation) into the LLM prompt alongside the problem description. Experiments on the Unit-Load Pre-Marshalling Problem and Sliding Puzzle Problem demonstrate that this algorithmic context allows a 32B parameter model (Qwen2.5-Coder) to generate heuristics superior to those from GPT-4o and human experts. The results are credible and backed by comparisons against optimal baselines. The key takeaway is a transferable 'prompt trick': explicitly showing the LLM the code that *calls* its generated function aligns the heuristic significantly better with the search dynamics than natural language descriptions alone. We should immediately test injecting our ALNS/search driver code into our evolutionary prompt templates.", "methodology": {"core_method": "Evolutionary Heuristic Design (EoH) framework with Algorithmic-Contextual Prompt Augmentation (A-CEoH)", "llm_role": "heuristic_generator", "llm_model_used": "Qwen2.5-Coder:32b, GPT4o:2024-08-06, Gemma2:27b", "search_type": "improvement", "novelty_claim": "A novel domain-agnostic prompt augmentation strategy (A-CEoH) is introduced that embeds A* algorithm code into the prompt to guide LLM-based heuristic generation.", "components": ["Evolution of Heuristics (EoH) framework", "Algorithmic-Contextual EoH (A-CEoH) prompt augmentation", "Problem-Contextual EoH (P-CEoH) prompt augmentation", "A* search algorithm", "Python code generation", "Natural language thought generation", "Evolutionary procedure (initialization, exploration, modification strategies)"], "training_required": false}, "tags": {"methods": ["evolution_of_heuristics", "llm_evolutionary_search", "llm_as_heuristic", "llm_code_generation", "llm_prompt_optimization", "llm_in_the_loop", "program_synthesis"], "problems": ["unit_load_pre_marshalling_problem", "sliding_puzzle_problem", "heuristic_evolution", "operator_discovery"], "contribution_type": ["new_method", "sota_result", "framework", "new_benchmark"], "framework_lineage": "eoh", "specific_domain": null, "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Unit-Load Pre-Marshalling Problem, Sliding Puzzle Problem", "short": "UPMP, SPP", "class_": "algorithm_design", "properties": ["combinatorial", "A*_search_guiding_heuristic", "minimal_moves", "NP-hard", "priority_based", "grid_based", "constrained"], "scale": "UPMP: 5x5 layout; SPP: 20x20 puzzles"}, "lineage": {"direct_ancestors": [{"paper": "Evolution of Heuristics (EoH)", "relationship": "extends the core framework of"}, {"paper": "Problem-Contextual EoH (P-CEoH)", "relationship": "builds upon the prompt augmentation concept of"}], "closest_prior_work": "Problem-Contextual EoH (P-CEoH)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Investigate methods to improve convergence of A-CEoH in later generations for UPMP.", "Explore strategies to help LLMs escape local optima during heuristic generation.", "Analyze why PA-CEoH did not improve performance for the Sliding Puzzle Problem.", "Evaluate the framework with other LLMs and larger problem instances."], "transferable_to": ["Other A*-search based combinatorial optimization problems.", "Heuristic design for different search algorithms beyond A*.", "Automated algorithm design for other classic CO problems.", "Generating heuristics for real-world logistics and planning problems."], "open_weaknesses": ["A-CEoH alone struggles with convergence in later generations for UPMP.", "Certain LLMs (e.g., GPT4o) can converge to local optima.", "Some LLMs (e.g., Gemma2) struggle to produce competitive heuristics for specific problems.", "PA-CEoH did not outperform A-CEoH for the well-studied SPP."]}, "artifacts": {"code_url": "https://github.com/tb-git-tud/a-ceoh-evolution-of-heuristics", "models_released": false, "new_benchmark": true}, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.18, "experiments": {"benchmarks": ["Custom UPMP instances (5x5 layout, 5 priority classes, 60% fill rate)", "Custom SPP instances (20x20 puzzles, 200 random moves)"], "baselines": ["EoH [21]", "P-CEoH [1]", "Optimal A* [25] (for UPMP)", "Hybrid Heuristic A* [13] (for SPP)", "Optimal A* [11] (for SPP)"], "hardware": "AMD EPYC 7401P with 64 GB RAM", "instance_sizes": [5, 20]}, "results": {"vs_baselines": {"PA-CEoH Qwen2.5-Coder:32b vs Optimal A* [25] (UPMP)": "Achieves optimal fitness (0.0815) on training instances with faster runtime (0.0888s vs 0.2773s); solves 30/30 test instances (fitness 0.1388, time 0.151) vs 27/30 (fitness 0.0968, time 0.44) for baseline.", "A-CEoH Qwen2.5-Coder:32b vs Optimal A* [11] (SPP)": "Solves 10/10 instances (fitness 0.445, time 17.417) vs 5/10 (fitness 0.252, time 137.55) on seeds 0-9; solves 7/10 (fitness 0.392, time 16.272) vs 6/10 (fitness 0.375, time 187.91) on seeds 10-19. Solves more instances with significantly shorter runtime.", "A-CEoH Qwen2.5-Coder:32b vs Hybrid Heuristic A* [13] (SPP)": "Solves 10/10 instances (fitness 0.445, time 17.417) vs 5/10 (fitness 0.539, time 56.587) on seeds 0-9; solves 7/10 (fitness 0.392, time 16.272) vs 6/10 (fitness 0.479, time 189.51) on seeds 10-19. Solves more instances with significantly shorter runtime.", "A-CEoH vs EoH (UPMP)": "Consistently outperforms EoH baseline in best-found and median fitness across all models.", "PA-CEoH vs EoH (UPMP)": "Delivers the best overall results (0.0815 fitness) for UPMP.", "A-CEoH vs P-CEoH (SPP)": "A-CEoH significantly improves Qwen2.5-Coder:32b performance (best fitness 0.445), while P-CEoH does not improve it."}, "scalability": "The method successfully generates heuristics for large 20x20 SPP instances where traditional PDB-based methods are infeasible due to memory and preprocessing demands.", "statistical_rigor": "Experiments were executed ten times for UPMP and five times for SPP per LLM, with results presented as box plots showing fitness distributions. No explicit statistical significance tests are reported.", "limitations_acknowledged": ["A-CEoH alone exhibits difficulty converging toward better fitness values in later generations for UPMP.", "GPT4o:2024-08-06 sometimes converges to local optima.", "Gemma2:27b struggles to produce competitive heuristics for SPP.", "For the well-studied SPP, PA-CEoH did not exceed A-CEoH performance."]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2601.17670", "arxiv_url": "https://arxiv.org/abs/2601.17670", "title": "Grammar-Aware Literate Generative Mathematical Programming with Compiler-in-the-Loop", "authors": ["Roberto"], "abstract": "", "published_date": "2026-01-25", "affiliations": "University of Edinburgh, University College Cork", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 9, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "The paper directly addresses our active 'OR-Bench' and 'RobustMAS' projects by introducing a rigorous OR modeling framework and, crucially, a new StochasticOR benchmark. The cost/latency analysis provides a strong counter-argument to the heavy multi-agent approaches (CoE) we have been skeptical of."}, "brief": "SyntAGM is a framework for translating natural language into Algebraic Modeling Language (PyOPL) code using a 'compiler-in-the-loop' approach, where the LLM is constrained by an in-context BNF grammar and iteratively repairs code based on compiler diagnostics. They demonstrate that this approach matches the accuracy of expensive multi-agent systems (like Chain-of-Experts) while being significantly faster and cheaper. The immediate takeaways for us are the **StochasticOR benchmark** (which we should adopt for RobustMAS) and the technique of **injecting explicit BNF grammars** into prompts to enforce syntax in evolutionary search without fine-tuning. The 'literate modeling' approach—embedding reasoning as comments directly next to code constraints—is also a clever memory mechanism we could steal for AlgoEvo.", "methodology": {"core_method": "Iterative generate–compile–assess–revise loop with compiler-in-the-loop and LLM-based alignment judge", "llm_role": "generator, evaluator, revision policy", "llm_model_used": "GPT-4o-2024-08-06, GPT-4.1-2025-04-14, GPT-5-2025-08-06, GPT-5-nano-2025-08-07, gpt-oss-20b", "search_type": "hybrid", "novelty_claim": "SyntAGM is a grammar-aware system that translates natural language problem descriptions into PyOPL models via an iterative generate–compile–assess–revise loop, leveraging in-context PyOPL BNF grammar and few-shot retrieval of literate PyOPL model exemplars.", "components": ["PyOPL compiler", "LLM generator", "LLM alignment assessor", "Retrieval-Augmented Generation (RAG) for few-shot exemplars", "Self-reflection module (revision prompt)", "Literate programming for memory/auditability"], "training_required": false}, "tags": {"methods": ["llm_as_heuristic", "llm_as_evaluator", "llm_in_the_loop", "llm_prompt_optimization", "retrieval_augmented_generation", "in_context_learning", "compiler_in_the_loop", "literate_programming", "program_synthesis", "self_improving_search"], "problems": ["generative_mathematical_programming", "linear_programming", "mixed_integer_linear_programming", "stochastic_programming", "two_stage_stochastic_programming", "multi_stage_stochastic_programming", "combinatorial_optimization"], "contribution_type": ["new_method", "new_benchmark", "empirical_study", "framework"], "framework_lineage": null, "specific_domain": "stochastic_programming", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Generative Mathematical Programming (Natural Language to AML Model Synthesis)", "short": "GenMP", "class_": "program_synthesis", "properties": ["natural_language_input", "mathematical_program_output", "declarative_language_target", "deterministic", "stochastic"], "scale": "Benchmarks range from 10 to 351 instances, covering varying complexity of linear and stochastic programming problems."}, "lineage": {"direct_ancestors": [{"paper": "Shinn et al., 2023", "relationship": "similar to Reflexion-style agents, extending generate-evaluate-reflect loop with compiler feedback"}, {"paper": "Wei et al., 2022", "relationship": "builds on Chain-of-Thought prompting for internal reasoning"}, {"paper": "Ramamonjison et al., 2022a", "relationship": "addresses Generative Mathematical Programming, building on early work and benchmarks like NL4Opt"}], "closest_prior_work": "Shinn et al., 2023", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Extend PyOPL to support nonlinear programming", "Investigate endogenous-order or multi-runway formulations for problems like ALP", "Explore more complex stochastic programming structures beyond two-stage/multi-stage", "Further improve cost-accuracy trade-offs for GenMP systems"], "transferable_to": ["Other Algebraic Modelling Languages (AMLs) beyond OPL/PyOPL", "Code generation for other domain-specific languages with compilers", "Automated algorithm design for other problem classes requiring structured code generation", "Scientific discovery tasks requiring structured code generation and validation"], "open_weaknesses": ["PyOPL currently does not support nonlinear programming", "Performance (accuracy) drops with smaller LLMs", "Objective-wise evaluation might not fully capture model correctness or quality", "High token usage and latency for some advanced LLM reasoning methods (e.g., CoE)"]}, "artifacts": {"code_url": "https://gwr3n.github.io/rhetor/", "models_released": false, "new_benchmark": true}, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_status": "stable", "bridge_score": 0.5251, "is_bridge": true, "priority_score": 7.18, "experiments": {"benchmarks": ["NL4OPT", "ComplexOR", "ReSocratic", "IndustryOR", "StochasticOR"], "baselines": ["Standard", "Chain-of-Thought", "Tree-of-Thoughts", "Reflexion", "Chain-of-Experts"], "hardware": "Intel(R) Xeon(R) w5-2465X, 3096 MHz, 16 Core(s), 32 Logical Processor(s) with 128 GB of RAM and NVIDIA RTX 2000 Ada Generation with 16 GB GDDR6 (for gpt-oss-20b)", "instance_sizes": [214, 18, 351, 42, 10]}, "results": {"vs_baselines": {"Standard": "SyntAGM achieves significantly higher accuracy (e.g., 61.6% vs 30.3% on NL4Opt) with comparable latency and token usage.", "Chain-of-Thought": "SyntAGM achieves higher accuracy (e.g., 61.6% vs 46.7% on NL4Opt) with comparable latency and token usage.", "Tree-of-Thoughts": "SyntAGM offers competitive accuracy (e.g., 50% vs 50% on ComplexOR, 69% vs 64.2% on IndustryOR) with lower latency and cost.", "Reflexion": "SyntAGM shows competitive accuracy (e.g., 61.6% vs 59.8% on NL4Opt, 69% vs 71.43% on IndustryOR) with lower latency and cost.", "Chain-of-Experts": "SyntAGM achieves slightly lower accuracy (e.g., 61.6% vs 69.1% on NL4Opt) but with significantly superior latency (114s vs 651s) and token usage."}, "scalability": "SyntAGM demonstrates competitive accuracy while consistently being faster and cheaper (lower latency and token/cost) than other advanced methods as problem complexity increases.", "statistical_rigor": "Results are from single runs for most benchmarks; for StochasticOR, averages over 3 runs are reported. Accuracy is based on objective function value comparison within a tolerance (relative 1e-6, absolute 1e-9).", "limitations_acknowledged": ["PyOPL does not currently support nonlinear programming.", "Performance drops with smaller LLMs (e.g., GPT-5-nano)."]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2602.13218", "arxiv_url": "https://arxiv.org/abs/2602.13218", "title": "Scaling the Scaling Logic: Agentic Meta-Synthesis of Logic Reasoning", "authors": ["Bowen"], "abstract": "", "published_date": "2026-01-23", "affiliations": "Tencent, The Hong Kong University of Science and Technology (Guangzhou)", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 6, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper demonstrates a robust 'meta-synthesis' loop that evolves problem generators rather than instances, directly applicable to our 'Evolving the Evolver' and 'Latent Heuristic Search' projects. The 'Code-Augmented Blind Review' is a critical innovation for verifying evolved code without human intervention."}, "brief": "Liu et al. introduce SS-Logic, an agentic framework that evolves Python 'Generator-Validator' pairs to scale logic task families, using a rigorous 'Code-Augmented Blind Review' where independent agents must write code to solve generated tasks to verify their validity. They expand 400 seed families to over 21k instances, achieving consistent gains on AIME (+3.0) and SynLogic (+5.2) via RLVR. **Crucial Takeaway:** We should steal the 'Blind Review' mechanism for AlgoEvo—using the solvability of a generated problem (by an independent code agent) as a strict fitness filter for the generator itself. This directly addresses our bottleneck in filtering invalid or hallucinated heuristics during evolutionary search.", "methodology": {"core_method": "Agentic Meta-Synthesis framework using a Generate–Validate–Repair closed loop for Generator–Validator program pairs", "llm_role": "program_synthesis, code_writer, decomposition_guide, evaluator, evolutionary_search", "llm_model_used": "DeepSeek-V3.1-Terminus, o4-mini", "search_type": "improvement", "novelty_claim": "SS-Logic is an agentic meta-synthesis framework that scales at the task-family level by iteratively synthesizing and repairing executable Generator–Validator program pairs in a closed Generate–Validate–Repair loop, enabling continuous family evolution with controllable difficulty.", "components": ["Main Agent", "Sub-Agents", "Context Playbook", "Multi-Gate Validation Protocol", "Quality Agent", "Auto Validator Pool", "Reviewer Agents", "Generator program", "Validator program", "Experience Manager"], "training_required": false}, "tags": {"methods": ["llm_as_heuristic", "llm_code_generation", "llm_as_evaluator", "llm_in_the_loop", "llm_evolutionary_search", "program_synthesis", "evolution_of_heuristics", "multi_agent_system"], "problems": ["logical_reasoning", "program_synthesis", "automated_algorithm_design", "heuristic_evolution", "task_family_synthesis"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study", "new_benchmark"], "framework_lineage": "cognitive_kernel_pro", "specific_domain": "logical_reasoning", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Verifiable Task-Family Synthesis for Logical Reasoning", "short": "Logic Task Synthesis", "class_": "program_synthesis", "properties": ["verifiable", "agentic", "meta_synthesis", "closed_loop", "controllable_difficulty", "task_family_level"], "scale": "400-953 task families, 5,718-21,389 instances"}, "lineage": {"direct_ancestors": [{"paper": "Cognitive Kernel-Pro (Fang et al., 2025)", "relationship": "builds upon as agent backbone"}, {"paper": "AgentFrontier (Chen et al., 2025b)", "relationship": "extends agentic synthesis pipeline for task evolution"}, {"paper": "AgentEvolver (Zhai et al., 2025)", "relationship": "extends agentic synthesis pipeline for task evolution"}], "closest_prior_work": "AgentFrontier (Chen et al., 2025b)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Improve efficiency and cost-effectiveness of the validation protocol", "Extend meta-synthesis to other complex reasoning domains beyond logic", "Develop more sophisticated agentic strategies for program evolution", "Investigate integration of RL-trained LLMs for generator/validator synthesis"], "transferable_to": ["Mathematical reasoning and problem generation", "Scientific discovery and hypothesis generation", "Automated theorem proving and proof generation", "Code generation for diverse algorithmic problems"], "open_weaknesses": ["High resource consumption for rejected task families during synthesis", "Statistical limitations due to small sample sizes in some analyses", "Potential for homophily bias in evaluation benchmarks", "Framework's ability to generate truly paradigm-shifting problems from limited seeds"]}, "artifacts": {"code_url": "https://github.com/AdAstraAbyssoque/Scaling-the-Scaling-Logic", "models_released": false, "new_benchmark": true}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.83, "experiments": {"benchmarks": ["SynLogic", "ARC-AGI", "BBH", "BBEH", "Enigmata Eval", "AIME24", "AIME25", "Brumo25", "HMMT25", "Math500"], "baselines": ["Seed (human-annotated tasks)", "AIME (for data mixing)", "Qwen3-8B-Base", "Qwen3-8B(Thinking)"], "hardware": "null", "instance_sizes": [400, 953, 5718, 21389, 236, 170]}, "results": {"vs_baselines": {"SynLogic": "+5.2", "BBEH": "+1.4", "AIME25": "+3.0", "Brumo25": "+3.7"}, "scalability": "The framework expanded task families from 400 to 953 and instances from 5,718 to 21,389, with an amortized cost of $1.18 per accepted task family.", "statistical_rigor": "Pass@k metric, 95% bootstrap confidence intervals (2,000 iterations), and error bars are used.", "limitations_acknowledged": ["Confidence intervals frequently cross zero due to limited sample size (N=170)", "Homophily bias can inflate evaluation scores by ~6.6%"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2601.16849", "arxiv_url": "https://arxiv.org/abs/2601.16849", "title": "The Art of Being Difficult: Combining Human and AI Strengths to Find Adversarial Instances for Heuristics", "authors": ["Henri"], "abstract": "", "published_date": "2026-01-23", "affiliations": "Google DeepMind, University of Bonn, University of Manitoba", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 5, "problem": 8, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While methodologically standard (FunSearch + human), it provides a concrete case study on the gap between evolved code and theoretical insight. It highlights a specific module we are missing in AlgoEvo: an automated 'simplifier/generalizer' agent to convert messy evolved constants into symbolic proofs."}, "brief": "This paper applies FunSearch to generate adversarial instances for classical OR heuristics (Knapsack, Bin Packing, k-median), successfully breaking long-standing theoretical lower bounds. The results are rigorous: they disprove the output-polynomial time of the Nemhauser-Ullmann algorithm and improve the Best-Fit bin packing bound to 1.5. The key takeaway for our AlgoEvo work is the workflow: the LLM finds 'messy' structural patterns (e.g., repeated floats) which humans then manually generalize into asymptotic proofs. This validates Program Search over vector search but exposes the 'generalization gap'—we should implement a post-processing agent to automate this manual refinement step.", "methodology": {"core_method": "Human-LLM collaborative program search (Co-FunSearch)", "llm_role": "code_writer", "llm_model_used": "gpt-4.1-mini, gpt-4.1-nano, open-mistral-nemo", "search_type": "sampling", "novelty_claim": "The paper introduces Co-FunSearch, a human-LLM collaborative framework that refines LLM-generated programs to derive mathematically rigorous adversarial instances and improved lower bounds for combinatorial optimization heuristics.", "components": ["FunSearch Cycle", "Large Language Model", "Program Evaluation", "Program Database", "Human Expert Analysis", "Manual Refinement"], "training_required": false}, "tags": {"methods": ["human_llm_collaboration", "program_search", "evolutionary_algorithm", "manual_refinement", "llm_code_generation", "llm_in_the_loop", "funsearch", "local_search", "dynamic_programming", "iterative_rounding", "best_fit_heuristic"], "problems": ["adversarial_instance_generation", "knapsack_problem", "bin_packing", "hierarchical_k_median_clustering", "generalized_gasoline_problem", "lower_bound_discovery", "heuristic_analysis"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "funsearch", "specific_domain": null, "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Generation of Adversarial Instances for Combinatorial Optimization Heuristics", "short": "Adversarial Instance Generation (AIG)", "class_": "algorithm_design", "properties": ["adversarial", "combinatorial_optimization", "heuristic_analysis", "lower_bound_discovery", "multi-problem"], "scale": "Small to medium instances (e.g., 8-20 items/points) for discovery, generalizable to asymptotic bounds"}, "lineage": {"direct_ancestors": [{"paper": "Romera-Paredes et al., Nature 2023", "relationship": "refines outputs from FunSearch"}], "closest_prior_work": "funsearch", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["apply co_funsearch to other heuristics and combinatorial optimization problems", "further automate the human refinement and generalization steps", "investigate reasons for lack of improvement on certain heuristics/problems", "develop formal asymptotic proofs for all discovered lower bounds"], "transferable_to": ["other areas of algorithm analysis beyond worst_case", "scientific discovery problems requiring interpretable solutions", "automated algorithm design for other problem classes"], "open_weaknesses": ["not universally superior to existing state_of_the_art for all heuristics", "difficulty in generalizing results for some problems (e.g., k_means, page_replacement)", "lack of asymptotic proofs for some improved lower bounds (e.g., generalized_gasoline_problem)", "computational cost of llm inference in the search loop"]}, "artifacts": {"code_url": "https://github.com/lumi-a/funsearch", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.15, "experiments": {"benchmarks": ["Knapsack Problem (Nemhauser-Ullmann Heuristic)", "Bin Packing (Best-Fit Heuristic)", "Hierarchical k-median clustering", "Generalized Gasoline Problem (Iterative Rounding Algorithm)"], "baselines": ["Local Search", "FunSearch"], "hardware": "No specific CPU/GPU mentioned; search terminated after 3 minutes per trial.", "instance_sizes": [20, 13, 8, 14]}, "results": {"vs_baselines": {"Knapsack (Previous SOTA)": "Co-FunSearch achieved nO(√n), improving on 2.0", "Knapsack (Local Search)": "Co-FunSearch achieved nO(√n), improving on 1.93", "Knapsack (FunSearch)": "Co-FunSearch achieved nO(√n), improving on 646.92", "Bin-Packing (Previous SOTA)": "Co-FunSearch achieved 1.5, improving on 1.3", "Bin-Packing (Local Search)": "Co-FunSearch achieved 1.5, improving on 1.478", "Bin-Packing (FunSearch)": "Co-FunSearch achieved 1.5, improving on 1.497", "k-median (Previous SOTA)": "Co-FunSearch achieved 1.618, improving on 1.0", "k-median (Local Search)": "Co-FunSearch achieved 1.618, improving on 1.36", "k-median (FunSearch)": "Co-FunSearch achieved 1.618, improving on 1.538", "Gasoline (Previous SOTA)": "Co-FunSearch achieved 4.65, improving on 2.0", "Gasoline (Local Search)": "Co-FunSearch achieved 4.65, improving on 2.11", "Gasoline (FunSearch)": "Co-FunSearch achieved 4.65, improving on 3.05"}, "scalability": "The methodology generates generic Python programs that scale with instance parameters, leading to generalizable and asymptotic lower bounds for various problems.", "statistical_rigor": "Results are based on maxima across 30 trials for FunSearch and local search, with standard error reported. Bin packing heuristic performance is averaged over 10,000 random shuffles.", "limitations_acknowledged": ["Not always superior to SOTA", "Failed to generalize or replicate known lower bounds for some heuristics (e.g., page replacement, k-means, Ward's method, asymptotic Best-Fit)", "Gasoline problem proof strategy does not apply for larger k, preventing asymptotic proof"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2511.16485", "arxiv_url": "https://arxiv.org/abs/2511.16485", "title": "Online Operator Design in Evolutionary Optimization for Flexible Job Shop Scheduling via Large Language Models", "authors": ["Rongjie"], "abstract": "", "published_date": "2026-01-22", "affiliations": "City University of Hong Kong, Guangdong University of Technology", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 7, "problem": 8, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper implements 'online' operator evolution (intervening during the run), which directly aligns with our goals for 'Evolving the Evolver' and 'observability' in AlgoEvo. The 'Perception' module provides a concrete template for diagnosing search stagnation."}, "brief": "LLM4EO embeds an LLM directly into the Genetic Algorithm loop to dynamically generate and replace gene-selection operators whenever the population stagnates, rather than training them offline. Results on FJSP benchmarks (Brandimarte, Fattahi) show a 3-4% improvement over static GA and GP, with convergence plots demonstrating that LLM interventions successfully break local optima. The most stealable insight is the 'Perception and Analysis' prompt structure: it forces the LLM to explicitly diagnose *why* the current population is stuck (based on fitness stats) before generating new code, a mechanism we should port to AlgoEvo to handle search stagnation. This validates the viability of online, state-aware LLM intervention in OR scheduling problems.", "methodology": {"core_method": "Genetic Algorithm with LLM-driven online operator design and adaptive operator evolution", "llm_role": "evolutionary_search", "llm_model_used": "GPT-4.1-mini", "search_type": "hybrid", "novelty_claim": "The approach achieves co-evolution of solutions and operators within a unified optimization framework, introducing a novel paradigm for enhancing the efficiency and adaptability of EAs.", "components": ["knowledge-transfer-based operator design", "evolution perception and analysis", "adaptive operator evolution", "Genetic Algorithm"], "training_required": false}, "tags": {"methods": ["evolutionary_algorithm", "genetic_algorithm", "llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "evolution_of_heuristics", "program_synthesis", "local_search"], "problems": ["flexible_job_shop_scheduling", "distributed_flexible_job_shop_scheduling"], "contribution_type": ["new_method", "sota_result", "empirical_study", "framework"], "framework_lineage": "eoh", "specific_domain": "flexible_job_shop_scheduling", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Flexible Job Shop Scheduling Problem", "short": "FJSP", "class_": "scheduling", "properties": ["flexible", "distributed"], "scale": "10-20 jobs, 4-15 machines"}, "lineage": {"direct_ancestors": [{"paper": "Evolution of heuristics: towards efficient automatic algorithm design using large language model", "relationship": "extends LLM-driven heuristic evolution to online operator design"}, {"paper": "Reevo: large language models as hyper-heuristics with reflective evolution", "relationship": "builds on LLM as hyper-heuristics with reflective evolution"}], "closest_prior_work": "Evolution of heuristics: towards efficient automatic algorithm design using large language model", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Investigate different LLM architectures and their impact on operator quality and cost.", "Explore integration with multi-objective evolutionary algorithms.", "Develop more efficient LLM invocation strategies to reduce computational overhead.", "Generalize the framework to other complex combinatorial optimization problems."], "transferable_to": ["Other scheduling problems (e.g., flow shop, project scheduling).", "Logistics optimization (e.g., VRP variants).", "Resource allocation problems.", "Automated algorithm design for other metaheuristics."], "open_weaknesses": ["Computational latency and execution overhead of frequent LLM calls.", "Dependence on prompt engineering for effective operator generation.", "Potential for LLM 'response inertia' despite mutation strategies.", "Scalability to extremely large-scale problems where LLM interaction might become a bottleneck."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.75, "experiments": {"benchmarks": ["Brandimarte, 1993", "Hurink et al., 1994", "Fattahi et al., 2007", "De Giovanni and Pezzella, 2010"], "baselines": ["LLM4OD", "LLM4OPD", "Genetic Programming (GP)", "Gene Expression Programming (GEP)", "SLABC", "SLGA", "HGIN-RS", "GRU-DRL", "HTS/SA", "AIA", "IGAR", "DDEA-PMI", "GA", "P'GA", "Lu'GA", "CRO", "Giovanni'GA"], "hardware": "Intel core i5-12400 @ 2.50 GHz and 32 GB of RAM", "instance_sizes": [10, 15, 20]}, "results": {"vs_baselines": {"LLM4OD": "3.0% RPD_BM improvement on Brandimarte", "LLM4OPD": "2.0% RPD_BM improvement on Brandimarte", "Genetic Programming (GP)": "Outperforms on Fattahi MFJS instances", "Gene Expression Programming (GEP)": "Outperforms on Fattahi MFJS instances", "SLABC": "45.3% RPD_BM improvement on Brandimarte", "SLGA": "19.2% RPD_BM improvement on Brandimarte", "HGIN-RS": "32.8% RPD_BM improvement on Brandimarte", "GRU-DRL": "21.5% RPD_BM improvement on Brandimarte", "HTS/SA": "29.4% RPD_BM improvement on Fattahi", "AIA": "3.3% RPD_BM improvement on Fattahi", "IGAR": "2.9% RPD_BM improvement on Fattahi", "DDEA-PMI": "44.3% RPD_BM improvement on Fattahi", "GA": "3.08% RPD_BM improvement on Brandimarte, 3.64% RPDaver_BM improvement on Brandimarte", "P'GA": "30.8% RPDaver_BM improvement on Brandimarte", "Lu'GA": "Achieves optimal BM in LA06, LA08-LA15 for DFJSP", "CRO": "Achieves optimal BM in LA06, LA08-LA15 for DFJSP", "Giovanni'GA": "Achieves optimal BM in LA06, LA08-LA15 for DFJSP"}, "scalability": "For larger-scale problems, the proportion of time consumed by operator evolution is significantly reduced, demonstrating advantages in optimization speed and solution quality.", "statistical_rigor": "Each instance was solved 10 times, reporting best makespan (BM), average makespan (AM), and average relative percent deviation (RPDaver); convergence curves and box plots were used for visualization.", "limitations_acknowledged": []}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2601.16175", "arxiv_url": "https://arxiv.org/abs/2601.16175", "title": "Learning to Discover at Test Time", "authors": ["Mert"], "abstract": "", "published_date": "2026-01-22", "affiliations": "Stanford University, NVIDIA, UC San Diego, Together AI, Astera Institute", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 10, "inspirational": 10}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper directly challenges and improves upon the AlphaEvolve/ShinkaEvolve paradigms central to our research by demonstrating that test-time weight updates (RL) significantly outperform frozen-model context evolution. It provides a concrete recipe (entropic objective + PUCT) for 'RL-infused evolution' that we list as a primary focus."}, "brief": "TTT-Discover introduces a method to fine-tune an LLM (gpt-oss-120b) *during* inference on a single test problem using RL, replacing the frozen-model evolutionary search of AlphaEvolve. They employ a novel 'entropic objective' that optimizes for the single best solution (discovery) rather than expected return, combined with PUCT-based state reuse. The results are empirically rigorous, setting new SOTA on Erdős’ problem, GPU kernel optimization, and AtCoder contests, directly beating AlphaEvolve and ShinkaEvolve. The critical takeaway is that for hard discovery tasks, shifting the model's distribution via online updates is superior to context-based search; we should immediately test their entropic objective in our AlgoEvo pipeline.", "methodology": {"core_method": "Reinforcement Learning with Entropic Objective and PUCT-based Reuse at Test Time", "llm_role": "Policy being optimized to generate solutions/code", "llm_model_used": "gpt-oss-120b", "search_type": "hybrid", "novelty_claim": "TTT-Discover performs reinforcement learning at test time, allowing the LLM to continually train on problem-specific experience, using a tailored entropic objective and PUCT-based reuse to prioritize promising solutions for scientific discovery.", "components": ["Reinforcement Learning", "Entropic Objective", "PUCT-inspired state selection", "LoRA fine-tuning", "Importance Sampling Ratio Correction"], "training_required": true}, "tags": {"methods": ["reinforcement_learning", "entropic_objective", "puct_search", "lora_fine_tuning", "importance_sampling", "llm_as_policy", "llm_code_generation", "evolutionary_search", "gradient_descent", "random_hill_climbing", "simulated_annealing", "linear_programming", "slsqp", "data_diffusion", "truncated_svd"], "problems": ["erdos_minimum_overlap_problem", "autocorrelation_inequalities", "gpu_kernel_optimization", "triangular_matrix_multiplication", "multi_head_latent_attention", "automated_algorithm_design", "atcoder_heuristic_contest", "purse_seine_fishing", "apple_incremental_game", "single_cell_rna_seq_denoising", "circle_packing"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "thetaevolve", "specific_domain": null, "llm_coupling": "rl_trained"}, "problem": {"formal_name": "Scientific Discovery Optimization", "short": "Discovery Problem", "class_": "scientific_discovery", "properties": ["continuous_rewards", "test_time_optimization", "single_solution_focus", "out_of_distribution"], "scale": "Diverse, problem-specific scales (e.g., up to 30000-piece functions, 1024x1024 matrices, 5000 entities)"}, "lineage": {"direct_ancestors": [{"paper": "AlphaEvolve [50]", "relationship": "extends test-time search from"}, {"paper": "ThetaEvolve [78]", "relationship": "improves upon test-time RL of"}], "closest_prior_work": "ThetaEvolve [78]", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Extend to problems with sparse or binary rewards", "Extend to problems in non-verifiable domains", "Further hyperparameter tuning for specific tasks", "Extensive auto-tuning of block sizes for GPU kernels"], "transferable_to": ["Other mathematical conjectures and open problems", "Other GPU architectures and kernel types", "Other combinatorial optimization problems (e.g., vehicle routing, scheduling)", "Other biological data analysis tasks beyond denoising"], "open_weaknesses": ["Currently limited to problems with continuous rewards", "Generated GPU kernels may lack fine-grained Triton optimization", "Some SOTA results (e.g., MLA-Decode) not statistically significant", "Benchmark metrics for biological tasks may not guarantee biological validity", "Lack of extensive auto-tuning for GPU kernel block sizes"]}, "artifacts": {"code_url": "https://github.com/test-time-training/discover", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.97, "experiments": {"benchmarks": ["Erdős’ Minimum Overlap Problem", "Autocorrelation Inequalities (First)", "Autocorrelation Inequalities (Second)", "Circle Packing (n=26)", "Circle Packing (n=32)", "GPUMode TriMul Competition (A100)", "GPUMode TriMul Competition (H100)", "GPUMode TriMul Competition (B200)", "GPUMode TriMul Competition (AMD MI300X)", "GPUMode DeepSeek MLA Competition (AMD MI300X)", "AtCoder Heuristic Contest 039 (ahc039)", "AtCoder Heuristic Contest 058 (ahc058)", "OpenProblems Denoising Benchmark (PBMC)", "OpenProblems Denoising Benchmark (Tabula Muris Senis Lung)"], "baselines": ["Best Human", "AlphaEvolve [50]", "AlphaEvolve V2 [14]", "ThetaEvolve [78]", "OpenEvolve [61]", "Best-of-25600", "ShinkaEvolve [37]", "ALE-Agent [27]", "MAGIC [75]", "ALRA [40]"], "hardware": "NVIDIA H100, H200, A100, B200, AMD MI300X GPUs on Tinker/Modal cloud platform; 2 CPUs for math problems. Training cost ~ $500 per problem for 50 steps.", "instance_sizes": [26, 32, 51, 95, 500, 600, 1024, 1319, 5000, 30000]}, "results": {"vs_baselines": {"Erdős’ Minimum Overlap Problem": "-0.0126% vs AlphaEvolve [50]", "Autocorrelation Inequality 1": "-0.0179% vs ThetaEvolve [78]", "Autocorrelation Inequality 2": "+0.197% vs AlphaEvolve V2 [14]", "TriMul H100": "-15.31% vs 1st human", "TriMul A100": "-51.48% vs 1st human", "TriMul B200": "-12.00% vs 1st human", "TriMul AMD MI300X": "-38.17% vs 1st human", "MLA-Decode AMD MI300X": "+0.92% vs 1st human (not statistically significant)", "AtCoder Heuristic Contest 039": "+0.011% vs 1st human", "AtCoder Heuristic Contest 058": "+0.0048% vs ALE-Agent [27]", "Denoising PBMC": "+1.43% vs OpenEvolve [61]", "Denoising Tabula": "+2.82% vs OpenEvolve [61]"}, "scalability": "TTT-Discover is effective across diverse problem scales and types, consistently improving solutions for specific instances rather than generalizing across problem distributions.", "statistical_rigor": "Results are reported for every attempted problem. For GPU kernel engineering, 10 trials with 95% confidence intervals are provided for some hardware. Ablations show reward distributions over 50 steps with 512 rollouts per step.", "limitations_acknowledged": ["Method only applicable to problems with continuous rewards", "MLA-Decode kernels do not leverage Triton for fine-grained optimization", "MLA-Decode results not statistically significant vs human SOTA", "Benchmark metrics for denoising do not guarantee biological validity", "TriMul kernels lack extensive auto-tuning of block size", "Potential for further hyperparameter tuning in ablations"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2601.15738", "arxiv_url": "https://arxiv.org/abs/2601.15738", "title": "LLM-Assisted Automatic Dispatching Rule Design for Dynamic Flexible Assembly Flow Shop Scheduling", "authors": ["Junhao"], "abstract": "", "published_date": "2026-01-22", "affiliations": "", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 7, "problem": 6, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "The paper implements a clean 'Dual-Expert' evolutionary loop where an Evaluator LLM provides semantic critiques that explicitly drive specific crossover and mutation prompt templates. This 'semantic crossover' approach is a concrete technique to improve operator quality in our AlgoEvo work."}, "brief": "LLM4DRD employs a dual-agent framework (Generator & Evaluator) to evolve priority dispatching rules for dynamic flexible assembly flow shops. The core contribution is the **Hybrid Evaluation** mechanism, where the Evaluator generates qualitative critiques (strengths/weaknesses) that are injected into the Generator's prompts to guide specific operators like 'Dominance-Fusion Crossover' and 'Directed Optimization.' Empirical results show it outperforms FunSearch and EOH, avoiding the premature convergence seen in other methods. The most stealable insight is the prompt structure for crossover: rather than blindly combining code, it uses the Evaluator's analysis of parent strengths to direct the merger, a technique we should implement to improve sample efficiency in our evolutionary search.", "methodology": {"core_method": "LLM-assisted evolutionary algorithm for automatic dispatching rule design with dual-expert mechanism and feature-fitting rule evolution", "llm_role": "heuristic_generator, evaluator", "llm_model_used": "gpt-4o-mini", "search_type": "improvement", "novelty_claim": "This study develops an LLM-assisted Dynamic Rule Design framework (LLM4DRD) that automatically evolves integrated online scheduling rules adapted to scheduling features using a dual-expert LLM mechanism and elite knowledge-guided initialization.", "components": ["Heterogeneous graph-based MDP formulation", "Elite knowledge guided initialization mechanism", "Dual-expert mechanism (LLM-A for rule generation, LLM-S for scheduling evaluation)", "Hybrid evaluation strategy (objective + empirical)", "Feature fitting based rule evolution mechanism", "Prompt strategies (Dominance-Fusion Crossover, Exploratory Crossover, Directed Optimization, Parameter Tuning)"], "training_required": false}, "tags": {"methods": ["llm_evolutionary_search", "evolutionary_algorithm", "llm_as_heuristic", "llm_as_evaluator", "program_synthesis", "heuristic_evolution", "llm_prompt_optimization", "mdp_formulation", "funsearch", "eoh", "reevo", "mcts"], "problems": ["dynamic_flexible_assembly_flow_shop_scheduling", "flow_shop_scheduling", "dispatching_rule_design"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study", "new_benchmark"], "framework_lineage": "funsearch", "specific_domain": "dynamic_flexible_assembly_flow_shop_scheduling", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Dynamic Flexible Assembly Flow Shop Scheduling Problem with Multi-Product Delivery and Dual Kitting Constraints", "short": "DFAFSP-MPD", "class_": "scheduling", "properties": ["dynamic", "multi-stage", "assembly", "flow_shop", "flexible", "multi_product_delivery", "dual_kitting_constraints", "stochastic_arrivals"], "scale": "3-5 assembly lines, 6-12 processing machines, 20-50 dynamic orders"}, "lineage": {"direct_ancestors": [{"paper": "Funsearch", "relationship": "extends the concept of LLM-driven heuristic search from"}, {"paper": "EOH", "relationship": "extends the concept of LLM-driven heuristic search from"}], "closest_prior_work": "Funsearch", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["incorporate more complex constraint structures and dynamic response events", "address independent design and integrated solution of dispatching rules for different subproblems", "explore human-machine interaction mechanisms for collaborative rule design"], "transferable_to": ["other dynamic scheduling problems", "other combinatorial optimization problems requiring heuristic design", "real_time_decision_making_systems"], "open_weaknesses": ["limited to current constraint structures and dynamic events", "sub-optimal for heterogeneous decision preferences due to integrated rule design", "lacks human-in-the-loop collaboration"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": true}, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_status": "stable", "bridge_score": 0.6801, "is_bridge": true, "priority_score": 5.45, "experiments": {"benchmarks": ["Industrial instances from a Chinese home-appliance manufacturer", "Synthetically generated instances based on existing dynamic scheduling studies"], "baselines": ["GP", "GEP", "Funsearch", "EOH", "MCTS-AHD", "ReEvo", "RandSample", "EDD", "FIFO+SPT", "FIFO+EET", "MOPNR+SPT", "MOPNR+EET", "LWKR+SPT", "LWKR+EET", "MWKR+SPT", "MWKR+EET"], "hardware": "Intel Core i5-12600KF processor", "instance_sizes": [3, 5, 6, 12, 20, 50]}, "results": {"vs_baselines": {"EDD": "16.81% average tardiness reduction", "GP": "6.35% average tardiness reduction", "GEP": "12.39% average tardiness reduction", "RandSample": "3.19% average tardiness reduction", "FunSearch": "3.17% average tardiness reduction", "EOH": "4.12% average tardiness reduction", "MCTS-AHD": "6.02% average tardiness reduction", "ReEvo": "6.48% average tardiness reduction", "Hybrid Heuristics (e.g., LWKR+SPT)": "10-30% average tardiness reduction on robustness tests"}, "scalability": "LLM4DRD demonstrates strong robustness under high-load conditions and maintains stable performance across diverse and complex scenarios, consistently outperforming baselines with lower variability.", "statistical_rigor": "Experiments conducted for 5 independent runs; average performance, variance (standard deviation), and average relative improvement (ARI) are reported.", "limitations_acknowledged": ["Future research will consider more complex constraint structures and dynamic response events (secondary resources, transportation, multi-type disturbances).", "Future work will address independent design and integrated solution of dispatching rules for different subproblems to balance heterogeneous decision preferences.", "Human-machine interaction mechanisms will be explored for collaborative rule design."]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2504.08930", "arxiv_url": "https://arxiv.org/abs/2504.08930", "title": "VectorLiteRAG: Latency-Aware and Fine-Grained Resource Partitioning for Efficient RAG", "authors": ["Junkyum"], "abstract": "", "published_date": "2026-01-19", "affiliations": "Georgia Institute of Technology", "category": "OR for Generative AI", "relevance": {"methodological": 5, "problem": 7, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "Relevant specifically for our 'OR for AI systems' and 'GPUSched' tracks. It addresses the exact resource contention problem (KV cache vs. other GPU memory) we study, but uses a heuristic binary search rather than formal optimization. Good baseline to compare against."}, "brief": "VectorLiteRAG optimizes RAG serving throughput by dynamically partitioning vector indices between CPU and GPU memory based on access skew and latency SLOs. The results are credible, showing up to 1.5x throughput gains on H100/L40S setups by balancing retrieval speed against LLM KV-cache capacity. The most stealable insight is their use of a Beta distribution to analytically model the *minimum* hit rate within a batch (the bottleneck) to predict tail latency without full simulation—a technique we could adapt for stochastic constraints in our serving formulations. It solves a resource allocation problem we care about, though via systems engineering rather than the rigorous OR methods we prefer.", "methodology": {"core_method": "Analytical performance modeling and latency-bounded partitioning algorithm for hybrid CPU-GPU vector index, combined with a distributed runtime pipeline featuring query- and shard-aware routing and dynamic dispatcher.", "llm_role": "target_of_optimization", "llm_model_used": null, "search_type": "improvement", "novelty_claim": "VECTORLITERAG is the first solution to provide fine-grained resource control for co-located RAG pipelines, using an analytical model for latency and hit rate to enable principled GPU memory partitioning under explicit SLOs.", "components": ["Performance Modeling", "Tail Query Hit Rate Estimation", "Latency-Bounded Partitioning Algorithm", "Index Splitter", "Router", "Dynamic Dispatcher", "Adaptive Runtime Index Update", "Hybrid CPU-GPU vector index"], "training_required": false}, "tags": {"methods": ["retrieval_augmented_generation_serving", "vector_similarity_search", "approximate_nearest_neighbor_search", "inverted_file_index", "product_quantization", "fast_scanning", "analytical_modeling", "performance_estimation", "latency_bounded_partitioning", "resource_partitioning", "resource_allocation", "scheduling", "dynamic_dispatching", "adaptive_index_update", "beta_distribution_modeling", "queuing_theory"], "problems": ["rag_serving_optimization", "llm_serving_optimization", "resource_contention", "latency_optimization", "throughput_maximization", "approximate_nearest_neighbor_search"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "hedrarag", "specific_domain": "rag_serving_optimization", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Latency-Aware and Fine-Grained Resource Partitioning for Efficient Retrieval-Augmented Generation", "short": "RAG Serving", "class_": "resource_allocation", "properties": ["latency_aware", "fine_grained", "heterogeneous_hardware", "skewed_access_patterns", "dynamic_workloads", "SLO_compliant", "distributed"], "scale": "88M vectors, 40GB-80GB indexes, Llama3-8B to Llama3-70B LLMs"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2507.09138", "relationship": "builds on co-location approach of"}], "closest_prior_work": "HedraRAG", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["extend to prefill_decode_disaggregation_frameworks", "jointly model vector search and prefill/decode throughput", "more precisely model dynamic dispatcher impact with full order statistics"], "transferable_to": ["iterative_retrieval_rag_applications", "multi_stage_generation_rag_applications", "embedding_access_patterns_in_recommendation_systems", "any_clustered_gpu_accelerated_index"], "open_weaknesses": ["precise modeling of dynamic dispatcher complexity", "current approach not directly applicable to prefill_decode_disaggregation", "potential for temporal bias in query distribution drift", "coarse interpolation of throughput-cache curve"]}, "artifacts": {"code_url": "https://github.com/sitar-lab/VectorLiteRAG-AE", "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_3", "front_status": "growing", "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.89, "experiments": {"benchmarks": ["Wiki-All", "ORCAS 1K", "ORCAS 2K"], "baselines": ["FAISS-CPU IVF FastScan (CPU-Only)", "FAISS-GPU IVF (DED-GPU)", "Sharded FAISS-GPU IVF (ALL-GPU)", "HedraRAG"], "hardware": "L40S node (8 NVIDIA L40S GPUs 48GB GDDR, dual Xeon 6426Y CPUs), H100 node (8 NVIDIA H100 GPUs 80GB HBM, Xeon Platinum 8462Y CPUs)", "instance_sizes": [88000000, 8000000000, 32000000000, 70000000000]}, "results": {"vs_baselines": {"CPU-Only": "Consistent SLO violations, poor tail response due to limited per-request performance.", "DED-GPU": "Performs poorly with large models due to rigid model parallelism constraints and resource oversubscription.", "ALL-GPU": "Sharp latency increase at high arrival rates, up to 2x end-to-end latency increase for large indexes/models.", "HedraRAG": "Lower TTFT at low request rates, but narrower operable range and higher overall end-to-end latency at higher rates."}, "scalability": "VECTORLITERAG consistently expands the range of SLO-compliant request rate across all tested configurations, extending throughput roughly in proportion to the number of GPUs and maintaining serviceability over a wider range across varying input/output lengths.", "statistical_rigor": "Evaluations use 90th and 95th percentile latency targets, Poisson distribution for request arrival, and results are averaged over multiple runs. Minor variations due to system-level factors are acknowledged.", "limitations_acknowledged": ["Precise modeling of the dynamic dispatcher's impact on per-request completion times would increase complexity significantly.", "Future work is needed to extend the approach to prefill–decode disaggregation frameworks, requiring joint modeling of vector search and both prefill/decode stages."]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2505.23970", "arxiv_url": "https://arxiv.org/abs/2505.23970", "title": "Cache Your Prompt When It's Green: Carbon-Aware Caching for Large Language Model Serving", "authors": ["Yuyang"], "abstract": "", "published_date": "2026-01-19", "affiliations": "University of Waterloo, Purdue University", "category": "OR for Generative AI", "relevance": {"methodological": 5, "problem": 7, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "The paper applies standard OR methods (ILP) to LLM serving, which directly aligns with our 'OR for AI systems' interest. While the optimization technique is not novel, the objective function formulation (trading embodied vs. operational carbon) and the cache replacement heuristic are actionable for our resource allocation projects."}, "brief": "Tian et al. propose GreenCache, a framework using Integer Linear Programming (ILP) to dynamically resize KV caches for LLM serving, balancing operational carbon (compute) against embodied carbon (SSD storage). They demonstrate ~15% carbon reduction on Llama-3 70B using Azure traces, though the reliance on simulation rather than live deployment weakens the claims slightly. For our 'OR for AI systems' work, the key takeaway is their 'Least Carbon Savings' (LCS) eviction policy—a heuristic that weighs computation saved against storage cost and recency—which we could adapt for optimizing memory-constrained multi-agent systems (HERMES) or general serving resource allocation.", "methodology": {"core_method": "Integer Linear Programming (ILP) based dynamic cache size reconfiguration with SARIMA load prediction and carbon-aware Least Carbon Savings (LCS) cache replacement policy", "llm_role": "none", "llm_model_used": null, "search_type": "exact", "novelty_claim": "This is the first work that systematically studies the embodied carbon emissions of storage due to caching in LLM serving systems.", "components": ["Profiler", "Performance Monitor", "CI Predictor (EnsembleCI)", "Load Predictor (SARIMA)", "Constraint Solver (ILP)", "Cache Manager", "Least Carbon Savings (LCS) Replacement Policy"], "training_required": true}, "tags": {"methods": ["integer_linear_programming", "sarima", "time_series_forecasting", "cache_replacement_policy", "least_carbon_savings", "profiling", "kv_cache", "llm_serving_optimization", "resource_allocation"], "problems": ["llm_serving_optimization", "carbon_emission_reduction", "sustainable_computing", "slo_satisfaction", "cache_management", "multi_turn_conversation", "document_reading_comprehension"], "contribution_type": ["new_method", "framework", "empirical_study", "sota_result"], "framework_lineage": "lmcache", "specific_domain": "llm_serving_optimization", "llm_coupling": null}, "problem": {"formal_name": "Carbon-Aware Cache Management for Large Language Model Serving", "short": "GreenCache", "class_": "resource_allocation", "properties": ["carbon_aware", "dynamic_workloads", "SLO_constrained", "tradeoff_operational_embodied_carbon"], "scale": "Llama-3 70B, Llama-3 8B, up to 16 TB SSD cache"}, "lineage": {"direct_ancestors": [{"paper": "LMCache [45]", "relationship": "builds on LLM caching system"}], "closest_prior_work": "EcoServe [39]", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["integrate more advanced load and CI predictors", "apply to Mixture-of-Expert (MoE) models", "explore other caching mediums (DRAM, CXL-attached memory, HDD)", "investigate embodied carbon of other hardware components beyond storage"], "transferable_to": ["other cloud computing services with significant storage embodied carbon", "general data caching systems", "other LLM architectures and models"], "open_weaknesses": ["limited real-world LLM serving traces for prediction", "inaccuracies in load and carbon intensity prediction", "profiling errors between evaluation and actual execution", "assumptions in ILP formulation (discrete cache sizes, constant CI)", "embodied carbon accounting assumptions for cloud resources"]}, "artifacts": {"code_url": "https://greencache.persistentmemory.org", "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_status": "growing", "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.89, "experiments": {"benchmarks": ["ShareGPT", "TriviaQA"], "baselines": ["No Cache", "Full Cache"], "hardware": "4x NVIDIA L40 GPUs, AMD 7453 CPU, 512 GB DDR4 Memory, up to 16 TB SSD storage", "instance_sizes": [70, 8, 1, 2, 4, 8, 12, 16]}, "results": {"vs_baselines": {"Full Cache": "12.6% avg carbon reduction (Llama-3 70B, multi-turn), 9.4% avg carbon reduction (Llama-3 70B, doc comp alpha=0.4), 5.6% avg carbon reduction (Llama-3 70B, doc comp alpha=0.7), 10.8% avg carbon reduction (Llama-3 8B, multi-turn), 7.6% avg carbon reduction (Llama-3 8B, doc comp alpha=0.4), 9.7% avg carbon reduction (Llama-3 8B, doc comp alpha=0.7). Higher savings (14.4-20.3%) in low-CI grids.", "No Cache": "Fails to meet >90% SLO attainment."}, "scalability": "Carbon savings decrease with longer cache resizing intervals. Benefits decrease at higher request rates as larger caches are needed for SLO. Shorter SSD lifespans (3 years) yield higher carbon savings (up to 11.9%). Higher SSD embodied carbon (90 kgCO2e/TB) increases carbon savings (up to 25%).", "statistical_rigor": "Load predictor MAPE 4.3%, CI predictor MAPE 6.8-15.3%, profiling context length difference 1.13-5.78%. Overall errors result in 0.79% average carbon savings reduction.", "limitations_acknowledged": ["Limited real-world LLM serving traces", "Prediction and profiling inaccuracies", "ILP assumptions (discrete cache sizes, constant CI)", "Embodied carbon accounting assumptions (cloud resource utilization)"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2602.11164", "arxiv_url": "https://arxiv.org/abs/2602.11164", "title": "Automated Optimization Modeling via a Localizable Error-Driven Perspective", "authors": ["Weiting"], "abstract": "", "published_date": "2026-01-17", "affiliations": "Huawei Noah’s Ark Lab, Fudan University, University of Science and Technology of China", "category": "Generative AI for OR", "relevance": {"methodological": 8, "problem": 7, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "The proposed DFPO (Dynamic Supervised Fine-Tuning Policy Optimization) directly addresses the sparse reward problem in code generation RL—a key bottleneck in our AlgoEvo work—by using a teacher to correct failed rollouts rather than discarding them. Additionally, the 'reverse data synthesis' strategy (generating problems specifically to trigger known error patterns) offers a novel approach for our 'Evolving the Evolver' test case generation."}, "brief": "This paper introduces MIND, a framework for automated optimization modeling that combines error-driven data synthesis with a novel post-training method called DFPO. Instead of standard RLVR which suffers from sparse rewards on hard problems, DFPO uses a teacher model to minimally correct the student's *failed* rollouts, converting them into on-policy(ish) positive samples for SFT/RL. Results show a 7B model outperforming GPT-4 on IndustryOR and OptMATH benchmarks. **Key Takeaway:** We should steal the DFPO mechanism for AlgoEvo: rather than wasting failed evolutionary samples, use a stronger model (or oracle) to fix the code and feed it back as a reward signal, drastically improving sample efficiency in our RL loops.", "methodology": {"core_method": "Error-driven learning framework (MIND) combining Dynamic Supervised Fine-Tuning Policy Optimization (DFPO) with an error-driven reverse data synthesis pipeline", "llm_role": "code_writer, decomposition_guide, evolutionary_search, prompt_optimizer", "llm_model_used": "Qwen2.5-7B-Instruct, Qwen3-8B, DeepSeek-R1, DeepSeek-V3", "search_type": "hybrid", "novelty_claim": "We propose a novel error-driven learning framework (MIND) that customizes the entire model training framework from data synthesis to post-training, leveraging localizable error patterns.", "components": ["Error-driven reverse data synthesis pipeline", "Dynamic Supervised Fine-Tuning Policy Optimization (DFPO)", "Error pattern identification", "Single-error reverse data synthesis", "Multi-error reverse data synthesis", "Quality control (Code validation, Bidirectional validation)", "Reward design (modeling fidelity reward, accuracy reward)"], "training_required": true}, "tags": {"methods": ["error_driven_learning", "data_synthesis", "reverse_data_synthesis", "supervised_fine_tuning", "reinforcement_learning", "dynamic_supervised_fine_tuning_policy_optimization", "group_relative_policy_optimization", "proximal_policy_optimization", "llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "llm_as_evaluator", "llm_fine_tuned"], "problems": ["automated_optimization_modeling", "program_synthesis", "milp_general", "linear_programming", "mixed_integer_linear_programming", "integer_programming", "nonlinear_programming", "second_order_cone_programming", "job_shop_scheduling", "diet_problem", "facility_location", "production_planning", "resource_allocation"], "contribution_type": ["new_method", "new_benchmark", "sota_result", "framework", "empirical_study"], "framework_lineage": "mind", "specific_domain": null, "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Automated Optimization Modeling", "short": "AOM", "class_": "program_synthesis", "properties": ["natural_language_input", "mathematical_formulation_output", "executable_code_output", "chain_of_thought_reasoning", "verifiable_solutions", "localized_errors"], "scale": "Small to medium complexity problems (e.g., diet, scheduling, facility location, production planning)"}, "lineage": {"direct_ancestors": [{"paper": "ORLM", "relationship": "builds on data synthesis methods from"}, {"paper": "OptMATH", "relationship": "builds on data synthesis methods from"}, {"paper": "SIRL", "relationship": "extends RLVR approach for AOM from"}, {"paper": "DAPO", "relationship": "extends policy optimization from"}, {"paper": "GRPO", "relationship": "extends policy optimization from"}], "closest_prior_work": "SIRL", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["explore more advanced error patterns and synthesis strategies", "extend MIND to other complex reasoning tasks beyond optimization modeling", "investigate methods to improve performance on tabular data problems", "develop techniques to bridge the gap with larger foundation models"], "transferable_to": ["other code generation tasks", "mathematical reasoning problems", "automated algorithm design for specific OR problems", "scientific discovery tasks requiring structured output generation"], "open_weaknesses": ["limited generalization on tabular data problems", "LLMs struggle to learn effectively from highly challenging datasets (multi-error synthesis)", "still lags behind much larger foundation models on some benchmarks", "automated optimization modeling cannot guarantee complete accuracy"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": true}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.61, "experiments": {"benchmarks": ["NL4Opt", "IndustryOR", "MAMO EasyLP", "MAMO ComplexLP", "OptMATH-Bench", "OptiBench", "MIND-Bench"], "baselines": ["GPT-4", "OpenAI o3", "Deepseek-V3", "Deepseek-R1", "Qwen2.5-7B-Instruct", "Qwen3-8B", "Autoformulator", "Chain-of-Experts", "Step-Opt", "OptiMUS", "ORLM", "LLMOPT", "OptMATH", "SIRL"], "hardware": "single computing node with four NVIDIA A100 GPUs (80 GB each)", "instance_sizes": []}, "results": {"vs_baselines": {"Qwen2.5-7B-Instruct": "+14.3% Macro AVG", "Qwen3-8B": "+31.0% Macro AVG", "SIRL-Qwen2.5-7B": "+2.5% Macro AVG, +4.3% on MIND-Bench", "SIRL-Qwen2.5-32B": "+0.1% Macro AVG, +2.9% on MIND-Bench", "Deepseek-V3": "+3.8% Macro AVG, +1.4% on MIND-Bench", "Deepseek-R1": "+0.8% Macro AVG, -7.3% on MIND-Bench (MIND-Qwen3-8B vs Deepseek-R1)", "OptMATH-Qwen2.5-7B": "+6.9% Macro AVG", "OptMATH-Qwen2.5-32B": "+6.3% Macro AVG"}, "scalability": "MIND shows significant performance gains on challenging benchmarks (e.g., IndustryOR, ComplexLP, OptMATH) and generalizes well to out-of-distribution problems (MIND-Bench), with larger base models (Qwen3-8B) achieving competitive performance against much larger proprietary models, though it shows marginal improvement on tabular data benchmarks (OptiBench).", "statistical_rigor": "Evaluated using pass@1 accuracy, where a solution is correct if the relative error between LLM-generated and ground-truth objective values is less than 10^-6. No explicit mention of multiple runs, variance, or significance tests for main results.", "limitations_acknowledged": ["Automated optimization modeling cannot guarantee complete accuracy", "MIND-Qwen2.5-7B shows marginal improvement on OptiBench due to limited representation of similar problem types in training data", "LLMs struggle to learn effectively when trained directly on highly challenging datasets", "MIND-Qwen2.5-7B still lags behind 671B-parameter foundation models (Deepseek-V3, Deepseek-R1) on MIND-Bench"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2511.16947", "arxiv_url": "https://arxiv.org/abs/2511.16947", "title": "Fine-grained MoE Load Balancing with Linear Programming", "authors": ["Chenqi"], "abstract": "", "published_date": "2026-01-15", "affiliations": "Peking University, Institute of Computing Technology Chinese Academy of Sciences", "category": "OR for Generative AI", "relevance": {"methodological": 7, "problem": 8, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper provides a successful existence proof for using formal Linear Programming solvers in the runtime critical path of LLM systems (per micro-batch), directly validating the approach of our 'GPUSched' and 'OR for AI' projects. It demonstrates that LP overhead can be negligible (<1ms) with warm-starting, offering a superior alternative to the heuristics we typically rely on for resource allocation."}, "brief": "FineMoE replaces heuristic load balancing in MoE training with a Linear Programming (LP) formulation solved per micro-batch to minimize maximum GPU load, achieving ~37-47% throughput gains over Megatron-LM. They utilize warm-started simplex solvers to keep optimization time under 1ms and employ Cayley graphs to optimize static expert placement. For our `GPUSched` work, this is a critical data point: it proves that formal OR solvers can replace heuristics in real-time LLM infrastructure without becoming a latency bottleneck.", "methodology": {"core_method": "FineEP, a novel expert parallelism strategy leveraging token scheduling formulated as a linear programming problem, combined with tailored expert placement strategies (symmetric Cayley graphs, asymmetric Monte Carlo sampling) and an adaptive replacement mechanism.", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "We propose FineEP to achieve dynamic and fine-grained load balancing among GPUs with token scheduling.", "components": ["token scheduling", "linear programming", "locality-aware routing", "overlapping scheduling", "symmetric expert placement (Cayley graphs)", "asymmetric expert placement (Monte Carlo sampling)", "adaptive replacement mechanism"], "training_required": false}, "tags": {"methods": ["expert_parallelism", "expert_data_parallelism", "token_scheduling", "linear_programming", "locality_aware_routing", "overlapping_scheduling", "pipelining", "cayley_graphs", "monte_carlo_sampling", "adaptive_replacement", "distributed_systems", "highs_solver"], "problems": ["moe_load_balancing", "gpu_scheduling", "distributed_training", "resource_allocation"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "finemoe", "specific_domain": "moe_load_balancing", "llm_coupling": null}, "problem": {"formal_name": "Fine-grained Load Balancing for Mixture-of-Experts with Token Scheduling", "short": "MoE Load Balancing", "class_": "load_balancing", "properties": ["dynamic", "distributed", "fine-grained", "micro-batch-level"], "scale": "16-32 GPUs, 8-256 experts, up to 8x7B models"}, "lineage": {"direct_ancestors": [{"paper": "Megatron-LM", "relationship": "implementation base"}, {"paper": "SmartMoE", "relationship": "systematic solution for expert placement"}, {"paper": "FlexMoE", "relationship": "systematic solution for dynamic replica counts"}, {"paper": "LPLB", "relationship": "similar token scheduling approach"}], "closest_prior_work": "LPLB", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["group_level_scheduling_for_larger_scale", "integration_with_fsdp", "further_optimization_of_communication_aware_scheduling", "refining_pipelining_for_latency_hiding"], "transferable_to": ["distributed_systems_with_dynamic_resource_allocation", "llm_inference_load_balancing", "general_distributed_computing_resource_management"], "open_weaknesses": ["computational_overhead_of_lpp_solving_at_extreme_scales", "inter_node_communication_overhead_in_certain_network_topologies", "data_format_incompatibilities_with_specific_communication_backends", "training_suspension_during_adaptive_replacement"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.74, "experiments": {"benchmarks": ["GPT 32x1.3B", "GPT 16x3.2B", "GPT 8x6.7B", "Mixtral 16x2B", "Mixtral 8x7B"], "baselines": ["Megatron-LM", "SmartMoE", "FlexMoE", "DeepSpeed"], "hardware": "4 nodes, each with 8 NVIDIA H100 80GB SXM GPUs, 900 GBps NVLink, two 400 Gbps Infiniband NICs per node", "instance_sizes": [8, 16, 32, 64, 128, 256]}, "results": {"vs_baselines": {"Megatron-LM": "up to 47.6% throughput improvement, average 36.9% throughput improvement", "FlexMoE": "average 13.9% throughput improvement over FlexMoE", "DeepSpeed": "poor performance with 16/32 experts, outperforms Megatron-LM with 8 experts", "SmartMoE": "sometimes worse than Megatron-LM"}, "scalability": "FineMoE consistently achieves complete load balance across varied skewness and scales well, with scheduling overhead remaining below 1ms even with 64 GPUs and 256 experts.", "statistical_rigor": "Expert loads generated using Zipfian distribution with varied skewness. Results presented as averages across models and configurations.", "limitations_acknowledged": ["Scaling to large clusters and increased expert counts increases computational overhead in scheduling", "FineEP may convert some intra-node communication into inter-node, leading to extra overhead", "Data format incompatibilities between DeepEP and Megatron-LM for DeepEP backend", "Pipelining FineEP introduces additional system overhead", "Model re-initialization for adaptive replacement causes temporary suspension of training"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2502.07115", "arxiv_url": "https://arxiv.org/abs/2502.07115", "title": "Online Scheduling for LLM Inference with KV Cache Constraints", "authors": ["Patrick"], "abstract": "", "published_date": "2026-01-15", "affiliations": "Massachusetts Institute of Technology, Microsoft Research, HKUST", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 10, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper provides the exact mathematical foundation (IP formulation + hardness proofs) for our active 'GPUSched' project. It demonstrates why standard scheduling fails due to dynamic KV-cache growth and offers a proven heuristic (MC-SF) that we should immediately implement as a baseline."}, "brief": "This paper formulates LLM inference scheduling as an Integer Program (IP) that explicitly models the linear memory growth of KV caches, and proposes a 'Memory Constrained Shortest First' (MC-SF) algorithm. The results are rigorous, showing MC-SF achieves near-optimal performance (within 5% of hindsight optimal) on synthetic data and significantly outperforms standard FCFS/threshold heuristics on real traces. The critical takeaway is the 'future feasibility check' (Eq. 5), which validates that a batch will *remain* within memory limits throughout the generation process based on predicted output lengths—a necessary deviation from standard static-size scheduling. This is foundational reading for our GPUSched project, providing both the exact IP baseline we need and a strong heuristic to benchmark against.", "methodology": {"core_method": "Memory Constrained Shortest First (MC-SF) online batching and scheduling algorithm", "llm_role": "none", "llm_model_used": null, "search_type": "constructive", "novelty_claim": "A novel online batching and scheduling algorithm (MC-SF) that minimizes inference latency by managing KV cache memory constraints and anticipating future memory usage.", "components": ["online_batching", "online_scheduling", "KV_cache_memory_constraint_checking", "shortest-first_prioritization_by_predicted_output_length", "non-preemptive_execution"], "training_required": false}, "tags": {"methods": ["online_optimization", "scheduling", "batching", "integer_programming", "competitive_ratio_analysis", "shortest_first", "memory_constrained_scheduling", "online_algorithms"], "problems": ["llm_inference_scheduling", "resource_allocation", "kv_cache_management", "online_scheduling"], "contribution_type": ["new_method", "framework", "theoretical_result", "empirical_study", "sota_result"], "framework_lineage": null, "specific_domain": "llm_inference_scheduling", "llm_coupling": null}, "problem": {"formal_name": "Online Scheduling for LLM Inference with KV Cache Constraints", "short": "LLM Inference Scheduling", "class_": "scheduling", "properties": ["online", "KV_cache_constraints", "memory_constraints", "batching", "non-preemptive", "sequential_token_generation", "single_GPU"], "scale": "40-60 requests (synthetic), 1000-10000 requests (real-world)"}, "lineage": {"direct_ancestors": [], "closest_prior_work": "Bari et al., 2025", "novelty_type": "new_problem"}, "extensions": {"next_steps": ["Jointly design prediction mechanisms for output lengths and batching-scheduling policies", "Extend framework to multiple computational workers (heterogeneous GPUs, load balancing)", "Study other arrival models (heavy-tailed, mixed stochastic-adversarial workloads)", "Incorporate lower bounds on output length predictions for tighter memory control"], "transferable_to": ["Other resource-constrained online scheduling problems", "Scheduling for other autoregressive models with dynamic resource usage", "Multi-server job scheduling"], "open_weaknesses": ["Reliance on output length predictions (and their accuracy)", "Current model is for a single computational worker", "Theoretical competitive ratio proof relies on specific assumptions (all prompts same size, simultaneous arrival)", "Deterministic online algorithms cannot achieve constant competitive ratio in general"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_6", "front_status": "emerging", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.61, "experiments": {"benchmarks": ["Synthetic datasets", "LMSYS-Chat-1M (Zheng et al., 2023)"], "baselines": ["Hindsight Optimal", "α-protection greedy algorithms", "α-protection, β-clearing algorithms", "Memory Constrained Benchmark (MC-Benchmark)", "FCFS"], "hardware": "Microsoft Surface Laptop with Snapdragon X Elite (12 Core) processor (simulating Llama2-70B on A100 GPUs)", "instance_sizes": [40, 60, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]}, "results": {"vs_baselines": {"Hindsight Optimal (All-at-once)": "1.005x average latency, 1.074x worst-case", "Hindsight Optimal (Online stochastic)": "1.047x average latency, 1.227x worst-case", "MC-Benchmark (High Demand)": "3x lower latency growth rate", "MC-Benchmark (Low Demand)": "8x lower latency growth rate", "Benchmark α=0.3 (1000 requests, λ=50)": "+61.7% latency", "Benchmark α=0.25 (1000 requests, λ=50)": "+58.9% latency", "Benchmark α=0.2, β=0.2 (1000 requests, λ=50)": "+57.0% latency", "Benchmark α=0.2, β=0.1 (1000 requests, λ=50)": "+57.0% latency", "Benchmark α=0.1, β=0.2 (1000 requests, λ=50)": "+66.2% latency", "Benchmark α=0.1, β=0.1 (1000 requests, λ=50)": "+58.4% latency", "FCFS (with ϵ=0.8 prediction error)": "Significantly lower latency"}, "scalability": "MC-SF shows superior scalability with significantly lower linear latency growth rates (3x to 8x better) compared to baselines in both high and low demand settings.", "statistical_rigor": "Synthetic experiments used 200 independent trials. Real-world simulations used 50 independent runs, reporting average, standard deviation, max, and min latency.", "limitations_acknowledged": ["Assumes predicted output length upper bounds true length", "Future work to jointly design prediction mechanisms and scheduling policies", "Framework currently limited to single computational worker", "Future work to study other arrival models (e.g., heavy-tailed, mixed workloads)"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2601.05943", "arxiv_url": "https://arxiv.org/abs/2601.05943", "title": "Global Optimization for Combinatorial Geometry Problems Revisited in the Era of LLMs", "authors": ["Timo"], "abstract": "", "published_date": "2026-01-15", "affiliations": "", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 4, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper debunks the difficulty of the AlphaEvolve benchmarks (a core focus for us), showing they are better solved by off-the-shelf NLP solvers than by LLM evolution. Ignoring this puts us at risk of publishing 'novel' results that are actually inferior to standard OR baselines."}, "brief": "Berthold et al. demonstrate that standard global NLP solvers (SCIP, Xpress) outperform DeepMind's AlphaEvolve on its own benchmarks (circle/hexagon packing, min-max distance) without any learning or evolution. The results are rigorous, improving on 'newly discovered' solutions within minutes using default solver settings. **CRITICAL TAKEAWAY:** We must validate our AlgoEvo results against classical global solvers to ensure we aren't claiming 'discovery' on problems that are trivial for SCIP; furthermore, it suggests a hybrid path where LLMs generate NLP models for solvers rather than evolving raw heuristic code. This is a necessary reality check for our benchmarking strategy.", "methodology": {"core_method": "Global Nonlinear Programming (NLP) using spatial branch-and-bound", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "Off-the-shelf global optimization solvers, without modification, can match and often improve upon state-of-the-art LLM-driven and other literature solutions for hard combinatorial geometry problems.", "components": ["Spatial branch-and-bound", "Automatic linearization", "Convexification", "Presolving", "Primal heuristics"], "training_required": false}, "tags": {"methods": ["global_optimization", "nonlinear_programming", "spatial_branch_and_bound", "automatic_linearization", "convexification", "presolving", "primal_heuristics"], "problems": ["combinatorial_geometry_problems", "min_max_distance_ratio", "circle_packing", "hexagon_packing", "geometric_optimization", "packing"], "contribution_type": ["sota_result", "empirical_study"], "framework_lineage": null, "specific_domain": "combinatorial_geometry_problems", "llm_coupling": null}, "problem": {"formal_name": "Combinatorial Geometry Problems (Min-Max Distance Ratio, Circle Packing, Hexagon Packing)", "short": "CGP", "class_": "geometric_optimization", "properties": ["nonconvex", "nonlinear", "continuous_variables", "geometric_constraints", "packing", "distance_minimization"], "scale": "up to 30 points (MMRP), 32 circles (CPP), 16 hexagons (HPP)"}, "lineage": {"direct_ancestors": [], "closest_prior_work": "AlphaEvolve", "novelty_type": "incremental"}, "extensions": {"next_steps": ["explore_larger_instances", "analyze_dual_bounds_more_deeply", "expand_problem_set_for_comparison"], "transferable_to": ["other_geometric_optimization_problems", "general_nonconvex_nlp", "other_packing_problems"], "open_weaknesses": ["limited_problem_set_for_comparison", "scalability_to_very_large_instances", "lack_of_dual_bound_analysis"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.91, "experiments": {"benchmarks": ["AlphaEvolve benchmark suite"], "baselines": ["AlphaEvolve [19,25]", "David Cantrell [16,17]", "Audet et al. [1]", "Friedman (Hexagons) [18]"], "hardware": "Not specified, solutions found within seconds or minutes on default settings.", "instance_sizes": [3, 7, 10, 11, 12, 13, 14, 15, 16, 20, 21, 22, 26, 27, 29, 30, 32]}, "results": {"vs_baselines": {"AlphaEvolve": "Improved MMRP n=16 (12.88927->12.88924), MMRP n=14 (4.16585->4.16578), CPP n=32 (2.93794->2.93957), HPP n=11 (3.93010->3.92485), HPP n=12 (3.94192->3.94165).", "Cantrell": "Improved MMRP n=21 (17.776->17.77499), MMRP n=22 (19.055->19.05398), CPP n=26 (2.638->2.63930), CPP n=27 (2.687->2.69015).", "Audet et al.": "Improved MMRP n=29 (25.929->25.92460).", "Friedman (Hexagons)": "Improved HPP n=14 (4.27240->4.26900), HPP n=15 (4.45406->4.44769), HPP n=16 (4.53633->4.52788)."}, "scalability": "Solutions found within seconds or minutes for instances up to 32 circles and 16 hexagons; performance gains are more significant for highly constrained problems.", "statistical_rigor": "Solutions verified using AlphaEvolve validation code; results reported to 5 decimal places; no statistical runs or significance tests mentioned.", "limitations_acknowledged": ["Discussion of dual bounds and their improvement is omitted due to space constraints.", "Conclusions on problem type competitiveness are based on a limited set of examples."]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2601.06520", "arxiv_url": "https://arxiv.org/abs/2601.06520", "title": "SkyNomad: On Using Multi-Region Spot Instances to Minimize AI Batch Job Cost", "authors": ["Zhifei"], "abstract": "", "published_date": "2026-01-10", "affiliations": "UC Berkeley, Shanghai Jiao Tong University, AMD, ICSI", "category": "OR for Generative AI", "relevance": {"methodological": 6, "problem": 9, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper directly solves the cost/scalability bottleneck for our massive evolutionary search experiments (AlgoEvo). Implementing their scheduling logic could triple our compute budget efficiency."}, "brief": "SkyNomad presents a multi-region scheduler for AI batch jobs that minimizes cost by dynamically migrating spot instances based on real-time availability probing and survival-analysis-based lifetime prediction. The authors propose a 'Unified Cost Model' that quantifies the monetary value of deadline slack, allowing the system to mathematically trade off migration egress costs against cheaper spot prices. Empirical results on AWS and GCP are strong, demonstrating 1.25-3.96x cost savings over single-region baselines while guaranteeing deadlines. We should immediately adapt their 'Value of Progress' heuristic and lifetime prediction module to optimize our own large-scale parallel evolution infrastructure.", "methodology": {"core_method": "Multi-region scheduling policy guided by a unified monetary cost model, incorporating online availability probing, survival analysis for spot lifetime prediction, and deadline pressure estimation", "llm_role": "none", "llm_model_used": null, "search_type": "improvement", "novelty_claim": "The design of a scheduling policy that draws spot capacity from multiple regions while guaranteeing deadlines.", "components": ["Online availability probing", "Spot lifetime prediction (using survival analysis)", "Future progress value estimation", "Unified monetary cost model", "Deadline-aware scheduling policy", "Checkpoint migration"], "training_required": false}, "tags": {"methods": ["multi_region_scheduling", "cost_modeling", "online_probing", "resource_monitoring", "survival_analysis", "time_series_prediction", "deadline_management", "data_migration"], "problems": ["ai_batch_job_scheduling", "resource_cost_minimization", "deadline_constrained_scheduling", "spot_instance_management"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "skypilot", "specific_domain": "ai_batch_job_scheduling", "llm_coupling": null}, "problem": {"formal_name": "Minimizing AI Batch Job Cost using Multi-Region Spot Instances while guaranteeing deadlines", "short": "AI Batch Job Cost Minimization", "class_": "resource_allocation", "properties": ["multi_region", "spot_instances", "deadline_constrained", "interruptible", "checkpoint_based_recovery", "spatial_heterogeneity", "temporal_heterogeneity", "cold_start_delay", "egress_cost"], "scale": "1-16 GPU instances, 30-100 hour jobs, 45-150 hour deadlines, 0-4 TB checkpoints, 1-13 regions"}, "lineage": {"direct_ancestors": [{"paper": "arxiv:2404.09900", "relationship": "adapts deadline-aware rules from"}, {"paper": "arxiv:2303.08740", "relationship": "builds system on"}], "closest_prior_work": "arxiv:2404.09900", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["multi_cloud_extension", "adaptive_checkpointing", "task_as_a_service_pricing_model"], "transferable_to": ["other_cloud_providers", "workloads_with_dynamic_parallelism", "online_serving_systems"], "open_weaknesses": ["higher_cross_cloud_migration_costs", "provider_specific_probing_semantics", "fixed_number_of_instances_gang_scheduled_preemption", "progress_loss_after_preemptions"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.69, "experiments": {"benchmarks": ["Qwen3-4B on Orca-Math dataset", "Qwen3-14B on Orca-Math dataset", "GCP H100 spot availability trace", "AWS V100 spot availability trace"], "baselines": ["Uniform Progress (UP) [50]", "AWS SageMaker Managed Spot (ASM) [4]", "UP(S) (multi-region extension of UP, SkyPilot's production failover policy [41])", "Optimal (omniscient policy)", "SkyNomad (o) (SkyNomad with oracle for next spot lifetime)", "UP(A) (multi-region UP with availability probing)", "UP(AP) (multi-region UP with availability/price balancing)"], "hardware": "AWS: 4xL4 (g6.12xlarge), 8xA100 (p4d.24xlarge), 4xA10G (g5.12xlarge); GCP: 16 1xH100 instances (a3-highgpu-1g). Total cost of end-to-end experiments approximately $8,000. Total cost of GCP H100 trace collection approximately $9,000.", "instance_sizes": [1, 4, 8, 16]}, "results": {"vs_baselines": {"UP": "10-55% cost savings for SkyNomad vs best single-region UP, 47-69% vs average single-region UP. 2.6x higher cost than SkyNomad at 2.0x deadline ratio. 57% cheaper than UP at 4TB checkpoint.", "ASM": "1.09-3.96x cost savings for SkyNomad vs ASM across configurations.", "UP(S)": "4-44% cost savings for SkyNomad vs UP(S).", "Optimal": "SkyNomad within 10-12% of Optimal cost across various configurations.", "SkyNomad(o)": "SkyNomad within 2-5% of SkyNomad(o) cost across various configurations.", "UP(A)": "SkyNomad significantly outperforms UP(A), e.g., 3.6x lower cost in Asia-only scenario.", "UP(AP)": "SkyNomad outperforms UP(AP), e.g., $707 vs $806 at 8 regions."}, "scalability": "SkyNomad scales gracefully with increasing checkpoint sizes and number of regions, achieving near-optimal cost, with diminishing returns beyond 6 regions for aggregated availability.", "statistical_rigor": "Results are averaged over 20 jobs with different start times, showing standard error or 95% confidence intervals.", "limitations_acknowledged": ["Multi-Cloud Extension challenges (heterogeneous APIs, higher cross-cloud migration costs, provider-specific probing semantics)", "Availability Signals (portability trade-off with native signals)", "Autoscaling Integration (fixed number of instances, gang-scheduled preemption)", "Progress Loss After Preemptions (computation after last checkpoint is lost)"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2601.05770", "arxiv_url": "https://arxiv.org/abs/2601.05770", "title": "Weights to Code: Extracting Interpretable Algorithms from the Discrete Transformer", "authors": ["Yifan"], "abstract": "", "published_date": "2026-01-09", "affiliations": "", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 7, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper presents a gradient-based alternative to LLM evolutionary search for algorithm discovery. It successfully bridges continuous optimization and discrete symbolic code without using an LLM, which is directly relevant to your 'Latent Heuristic Search' and 'AlgoEvo' projects as a potential hybrid or competing method."}, "brief": "Zhang et al. introduce the 'Discrete Transformer,' a constrained architecture that learns algorithmic tasks via gradient descent and allows for the post-hoc extraction of exact, human-readable Python code. By enforcing functional disentanglement (using attention strictly for routing and MLPs for arithmetic) and employing temperature-annealed sampling, they recover symbolic laws for arithmetic and physics tasks with near-zero error. The critical takeaway is their 'continuous-to-discrete homotopy' strategy—annealing from soft to hard selection during training—which enables differentiable search to converge on discrete, symbolic solutions. This suggests a viable path to discover heuristics via continuous optimization rather than purely stochastic LLM evolution.", "methodology": {"core_method": "Discrete Transformer with functional disentanglement, temperature-annealed sampling, hypothesis testing for attention, and symbolic regression for MLP", "llm_role": "none", "llm_model_used": null, "search_type": "sampling", "novelty_claim": "We propose the Discrete Transformer, an architecture explicitly engineered to bridge the gap between continuous representations and discrete symbolic logic by enforcing strict functional disentanglement and employing temperature-annealed sampling to facilitate the extraction of human-readable programs.", "components": ["Discrete Transformer architecture", "Numerical Attention", "Numerical MLP", "Linear Output Head", "Temperature Annealing", "Differentiable Sampling", "Hypothesis Testing (for Attention)", "Symbolic Regression (for MLP)", "Piecewise Linear Encoding", "Discretized Reading Mechanism", "Magnitude-based Pruning"], "training_required": true}, "tags": {"methods": ["discrete_transformer", "transformer", "temperature_annealing", "differentiable_sampling", "hypothesis_testing", "symbolic_regression", "piecewise_linear_encoding", "magnitude_based_pruning", "functional_disentanglement"], "problems": ["algorithm_extraction", "symbolic_algorithm_discovery"], "contribution_type": ["new_method", "sota_result"], "framework_lineage": "discrete_transformer", "specific_domain": "algorithm_extraction", "llm_coupling": null}, "problem": {"formal_name": "Algorithm Extraction", "short": "AE", "class_": "algorithm_extraction", "properties": ["interpretable", "continuous_variables", "demonstration_free", "symbolic_logic"], "scale": "sequence length 10"}, "lineage": {"direct_ancestors": [{"paper": "MIPS (Michaud et al., 2024)", "relationship": "extends algorithm extraction to continuous variables"}], "closest_prior_work": "MIPS (Michaud et al., 2024)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["improve scalability to higher-dimensional interactions", "enhance expressivity beyond hard attention constraints", "explore alternative disentanglement strategies", "apply to different types of symbolic programs"], "transferable_to": ["extracting algorithms from scientific domains", "automated program synthesis", "interpretable AI models"], "open_weaknesses": ["complexity ceiling for high-dimensional interactions", "restricted expressivity due to hard attention", "capacity trade-off from disentangled representations"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.61, "experiments": {"benchmarks": ["MIPS benchmark (Michaud et al., 2024)"], "baselines": ["RNN-based baselines", "MIPS (Michaud et al., 2024)"], "hardware": "Not specified (PyTorch)", "instance_sizes": [10]}, "results": {"vs_baselines": {"RNN-based baselines": "comparable performance (near-zero MSE)", "MIPS (Michaud et al., 2024)": "matches capabilities, extends to continuous variables"}, "scalability": "Applicability restricted to tasks decomposable into low-dimensional, sparse interactions; excessive capacity inflates program length due to redundancy.", "statistical_rigor": "Average performance across three random seeds; hyperparameters selected via grid search.", "limitations_acknowledged": ["Complexity Ceiling (restricted to low-dimensional, sparse interactions)", "Restricted Expressivity (hard attention constrains model expressiveness)", "Capacity Trade-off (disentangled scalar representations sacrifice compressive efficiency of superposition)"]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2510.14150", "arxiv_url": "https://arxiv.org/abs/2510.14150", "title": "CodeEvolve: an open source evolutionary coding agent for algorithm discovery and optimization", "authors": ["Henrique"], "abstract": "", "published_date": "2026-01-06", "affiliations": "Inter&Co., Worcester Polytechnic Institute, Universidade Federal de Minas Gerais", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 8, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper presents a direct, open-source competitor to AlphaEvolve that achieves SOTA results using open-weight models, directly impacting our AlgoEvo roadmap. It validates specific architectural choices (MAP-Elites, inspiration-based crossover) that we are currently investigating or should be implementing."}, "brief": "CodeEvolve couples islands-based genetic algorithms with LLMs, utilizing CVT-MAP-Elites for diversity and a specific 'inspiration-based' crossover operator where the LLM integrates logic from high-ranking peer solutions. The results are strong and backed by numbers: they beat AlphaEvolve on 5/9 benchmarks and demonstrate that Qwen3-Coder-30B matches Gemini-2.5 performance at ~10% of the cost. The single most useful takeaway is the implementation of the 'inspiration' operator and the necessity of MAP-Elites over simple elitism to escape local optima in code space. We should immediately benchmark their open-source framework against our internal AlgoEvo builds.", "methodology": {"core_method": "Islands-based genetic algorithm with modular LLM orchestration, context-aware recombination, adaptive meta-prompting, and depth-based exploitation", "llm_role": "code_writer", "llm_model_used": "GEMINI-2.5 FLASH/PRO, Qwen3-Coder-30B", "search_type": "hybrid", "novelty_claim": "An open-source framework for algorithmic discovery that integrates islands-based evolutionary search with modular LLM orchestration, designed for transparency and reproducibility.", "components": ["Islands-based genetic algorithm", "weighted LLM ensemble", "depth exploitation operator", "meta-prompting exploration operator", "inspiration-based crossover", "MAP-Elites archive", "exploration scheduling"], "training_required": false}, "tags": {"methods": ["genetic_algorithm", "llm_code_generation", "llm_as_heuristic", "llm_evolutionary_search", "program_synthesis", "map_elites", "meta_prompting", "evolution_of_heuristics"], "problems": ["algorithm_discovery", "packing_problems", "distance_optimization", "autocorrelation_inequalities"], "contribution_type": ["new_method", "framework", "sota_result"], "framework_lineage": "alphaevolve", "specific_domain": null, "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Algorithmic Discovery and Optimization", "short": "Algorithmic Discovery", "class_": "algorithm_design", "properties": ["LLM-driven", "evolutionary", "program synthesis", "meta-optimization"], "scale": "evolving single functions to entire codebases"}, "lineage": {"direct_ancestors": [{"paper": "AlphaEvolve", "relationship": "extends framework of"}, {"paper": "ThetaEvolve", "relationship": "extends framework of"}, {"paper": "OpenEvolve", "relationship": "extends framework of"}, {"paper": "ShinkaEvolve", "relationship": "extends framework of"}], "closest_prior_work": "AlphaEvolve", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Conduct full-scale ablation studies for LLM ensemble components", "Explore heterogeneous LLM orchestration for different stages of evolution", "Develop adaptive hyperparameters for LLM generation", "Investigate methods for reducing inference budget for large-scale evolution"], "transferable_to": ["Other combinatorial optimization problems", "Scientific discovery tasks beyond current benchmarks", "General program synthesis for diverse domains", "Automated algorithm design for different computational paradigms"], "open_weaknesses": ["Reliance on reported baseline results due to budget constraints", "Current framework uses static hyperparameters for LLM generation", "High inference budget for large-scale evolution remains non-trivial", "New hyperparameters require tuning for novel domains"]}, "artifacts": {"code_url": "https://github.com/inter-co/science-codeevolve", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.86, "experiments": {"benchmarks": ["CirclePackingSquare", "CirclePackingRect", "HexagonPacking", "MinimizeMaxMinDist", "FirstAutocorrIneq", "SecondAutocorrIneq"], "baselines": ["AlphaEvolve", "ThetaEvolve", "OpenEvolve", "ShinkaEvolve"], "hardware": "AWS Sagemaker (vCPUS and RAM)", "instance_sizes": [26, 32, 21, 11, 12, 16, 14]}, "results": {"vs_baselines": {"AlphaEvolve": "Matches or surpasses in 5 out of 9 benchmark instances, establishing new state-of-the-art marks in MinimizeMaxMinDist and CirclePackingSquare (n=32).", "ThetaEvolve": "CODEEVOLVE shows consistency across packing and distance optimization problems, while ThetaEvolve performs strongly on Autocorrelation inequalities but lacks reported results on other benchmarks.", "ShinkaEvolve": "Matches CODEEVOLVE on CirclePackingSquare (n=26).", "OpenEvolve": "Slightly inferior to CODEEVOLVE on CirclePackingSquare (n=26)."}, "scalability": "Open-weight models (Qwen3-Coder-30B) often match or exceed closed-source baselines (GEMINI-2.5) at a fraction of the compute cost (e.g., 6 USD vs 35 USD for CirclePackingSquare n=26), suggesting modular orchestration is the primary driver of success rather than raw model scale.", "statistical_rigor": "Experiments were executed 3 times, reporting best and worst results across runs rather than standard deviation, as standard deviation is considered an unreliable estimator for small sample sizes.", "limitations_acknowledged": ["Budget constraints prevented full-scale ablation study using Gemini ensemble", "Reported results for baselines were used instead of reproduction due to budget", "Potential for heterogeneous orchestration (e.g., different LLMs for different stages)", "Current framework uses static hyperparameters for LLM generation", "New hyperparameters require tuning for novel domains", "Inference budget for large-scale evolution remains non-trivial"]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2505.03802", "arxiv_url": "https://arxiv.org/abs/2505.03802", "title": "Balancing Fidelity and Plasticity: Aligning Mixed-Precision Fine-Tuning with Linguistic Hierarchies", "authors": ["Changhai"], "abstract": "", "published_date": "2026-01-05", "affiliations": "Fudan University, Yale University, Zhejiang University", "category": "OR for Generative AI", "relevance": {"methodological": 7, "problem": 6, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While focused on quantization, the paper demonstrates a successful application of Evolutionary Algorithms (NSGA-II) + Bayesian Optimization to expensive LLM hyperparameter search. The specific techniques for reducing search cost (entropy-based initialization and proxy tuning) are directly transferable to our AlgoEvo sample efficiency problems."}, "brief": "QR-Adaptor employs a three-stage gradient-free search (Entropy Profiling → NSGA-II → Bayesian Optimization) to jointly optimize per-layer quantization bits and LoRA ranks. The results are empirically strong, showing that strategic mixed-precision (avg ~3.5 bits) can rival 16-bit baselines by preserving fidelity in deep semantic layers. We should steal their **Fidelity Sensitivity Profiling** (using information entropy to bias the initial evolutionary population) and **Proxy Tuning** (using few-step training as a cheap fitness proxy); these are concrete mechanisms to improve sample efficiency in our own evolutionary search pipelines.", "methodology": {"core_method": "Three-stage gradient-free search pipeline: Fidelity Sensitivity Profiling (information entropy based initialization), global exploration via Pareto-ranking genetic algorithm (NSGA-II), and local refinement using Bayesian Optimization (Gaussian Process with Matérn-5/2 Kernel and Expected Improvement acquisition function).", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "Introduces QR-Adaptor, a unified gradient-free framework that automates the joint search for per-layer bit-width and LoRA rank by formulating resource allocation as a multi-objective optimization problem aligned with the model’s intrinsic linguistic hierarchy.", "components": ["Fidelity Sensitivity Profiling", "Pareto-ranking Genetic Algorithm (NSGA-II)", "Bayesian Optimization", "Proxy Tuning for evaluation"], "training_required": true}, "tags": {"methods": ["quantization_aware_fine_tuning", "mixed_precision_quantization", "low_rank_adaptation", "parameter_efficient_fine_tuning", "multi_objective_optimization", "discrete_optimization", "evolutionary_algorithm", "genetic_algorithm", "nsga_ii", "bayesian_optimization", "gaussian_process", "expected_improvement", "information_entropy_profiling", "proxy_tuning"], "problems": ["llm_resource_allocation", "llm_efficiency", "quantized_llm_fine_tuning", "natural_language_understanding", "mathematical_reasoning"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "qr_adaptor", "specific_domain": "llm_resource_allocation", "llm_coupling": null}, "problem": {"formal_name": "Joint Optimization of Mixed-Precision Quantization Bit-width and LoRA Rank for Large Language Models Fine-tuning", "short": "Joint Bit-Rank Allocation for QLoRA", "class_": "llm_resource_allocation", "properties": ["quantized", "low_rank_adaptation", "mixed_precision", "multi_objective", "layer_wise_heterogeneity"], "scale": "1B to 13B parameters"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2305.14314", "relationship": "extends fixed bit-width QLoRA to adaptive joint bit-rank optimization"}, {"paper": "arXiv:2303.10512", "relationship": "extends rank-only adaptive AdaLoRA to joint bit-rank optimization"}, {"paper": "arXiv:2509.12019", "relationship": "extends bit-only automated mixed-precision (AMQ) for inference to fine-tuning with joint rank optimization"}, {"paper": "A fast and elitist multiobjective genetic algorithm: Nsga-ii", "relationship": "adapts NSGA-II framework for discrete landscape exploration"}], "closest_prior_work": "arXiv:2305.14314", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Explore predictor-based neural architecture search (NAS) to accelerate profiling.", "Apply to other compression techniques like structural pruning or knowledge distillation."], "transferable_to": ["Pruned LLMs for enhanced adaptability.", "Other LLM architectures and tasks.", "Other compression techniques (e.g., knowledge distillation, structural pruning)."], "open_weaknesses": ["Non-zero computational overhead compared to heuristic-based methods.", "Potential bottleneck for rapid, one-shot adaptation in continuously changing tasks."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.24, "experiments": {"benchmarks": ["ARC-C", "ARC-E", "HellaSwag", "PIQA", "WinoGrande", "BoolQ", "OBQA", "MMLU", "WikiText-2", "C4", "GSM8K", "HC3"], "baselines": ["LoRA (FP16)", "QLoRA (4-bit)", "AdaLoRA", "AMQ+LoRA", "LoftQ", "LQ-LoRA", "ApiQ", "RILQ", "QuaRot", "SpinQuant"], "hardware": "NVIDIA L20, NVIDIA A100", "instance_sizes": [1, 3, 4, 7, 8, 13]}, "results": {"vs_baselines": {"QLoRA (4-bit)": "QR-Adaptor-4 (avg ~3.5 bits) outperforms standard 4-bit QLoRA despite using approx 12% less parameter memory. On Qwen3-8B, it improves average accuracy from 67.8% to 68.4%, and on LLaMA-3-8B, it gains +0.8% accuracy (69.1% vs. 68.3%).", "LoRA (FP16)": "QR-Adaptor-6 (avg ~5.2 bits) surpasses the FP16 LoRA upper bound on both 8B models, achieving 70.6% on Qwen3-8B (vs. 70.2%) and 71.2% on LLaMA-3-8B (vs. 70.8%).", "QLoRA (3-bit)": "On GSM8K, QR-Adaptor (3.4 bits) recovers most of the performance drop, reaching 77.8% (vs 55.4% for QLoRA 3-bit, 78.5% for FP16 LoRA).", "LoftQ": "QR-Adaptor (<=4-bit) achieves 69.73% average accuracy, surpassing LoftQ (1 iter) at 68.82% by nearly 1%.", "ApiQ": "QR-Adaptor (Mixed 2/4-bit) outperforms ApiQ with 64.40% accuracy (vs 62.53%).", "RILQ": "QR-Adaptor (Mixed 2/4-bit) outperforms RILQ with 64.40% accuracy (vs 63.16%)."}, "scalability": "Consistently establishes a new Pareto frontier across varying model scales (1B to 8B parameters) and reduces peak VRAM to 12.8 GB, fitting within consumer-grade hardware and surpassing 4-bit QLoRA's 14.2 GB footprint.", "statistical_rigor": "Extensive experiments across multiple models and datasets. Search convergence tracked by validation PPL. Final allocated bit-width shows strong Pearson correlation (r > 0.8) with Fidelity Sensitivity Score. No explicit p-values or confidence intervals for main results.", "limitations_acknowledged": ["Non-zero computational overhead compared to heuristic-based methods like QLoRA", "Potential bottleneck in scenarios requiring rapid, one-shot adaptation for continuously changing tasks"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2504.11320", "arxiv_url": "https://arxiv.org/abs/2504.11320", "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints", "authors": ["Ruicheng"], "abstract": "", "published_date": "2026-01-05", "affiliations": "Massachusetts Institute of Technology, Peking University, Alibaba Group", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 10, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper is a direct hit for our 'GPUSched' project. It provides a rigorous Operations Research formulation (fluid dynamics + queueing theory) for LLM inference that solves the unknown output length problem without requiring predictive ML models, offering a theoretical guarantee against memory eviction."}, "brief": "This paper formulates LLM inference as a multi-stage stochastic scheduling problem, introducing 'Nested WAIT'—a threshold-based algorithm that handles unknown output lengths by letting prompts classify themselves as they survive into deeper decode segments. Unlike heuristic baselines (vLLM, Sarathi), they provide rigorous asymptotic optimality proofs and high-probability bounds against memory overflow, validated on A100 simulations. The key takeaway is the 'nested segment' mechanism: instead of predicting job size, structure the queue so short jobs exit early and long jobs naturally migrate to lower-priority/protected tiers, effectively decoupling the memory risk. We should immediately evaluate this threshold logic for our GPUSched formulations, as it likely outperforms our current predictive or FCFS approaches for handling KV cache growth.", "methodology": {"core_method": "Fluid dynamics approximation and threshold-based online scheduling (WAIT and Nested WAIT algorithms)", "llm_role": "none", "llm_model_used": null, "search_type": "constructive", "novelty_claim": "Applies operations research principles to establish a theoretical framework for LLM deployment under memory constraints, developing fluid dynamics approximation and threshold-based online scheduling algorithms (WAIT and Nested WAIT) to prevent eviction.", "components": ["Fluid dynamics approximation", "WAIT algorithm", "Nested WAIT algorithm", "threshold-based admission control", "on-the-fly type classification", "safety buffer"], "training_required": false}, "tags": {"methods": ["fluid_dynamics_approximation", "online_scheduling", "threshold_based_scheduling", "queueing_theory", "asymptotic_analysis", "martingale_theory", "doobs_inequality", "union_bound", "binomial_thinning", "lindley_recursion", "kingmans_bound", "wait_algorithm", "nested_wait_algorithm"], "problems": ["llm_inference_scheduling", "memory_constrained_scheduling", "online_scheduling", "llm_serving_optimization", "resource_allocation"], "contribution_type": ["new_method", "theoretical_result", "framework", "sota_result"], "framework_lineage": null, "specific_domain": "llm_inference_scheduling", "llm_coupling": null}, "problem": {"formal_name": "LLM inference optimization as a multi-stage online scheduling problem", "short": "LLM inference scheduling", "class_": "online_scheduling", "properties": ["memory_constrained", "dynamic_kv_cache", "multi_stage", "online", "unknown_output_lengths", "eviction_recomputation"], "scale": "Llama-7B, Llama-13B models; up to 500 output tokens; up to 50,000 prompts"}, "lineage": {"direct_ancestors": [{"paper": "Jaillet et al. (2025)", "relationship": "provides an alternative theoretical framework for online LLM inference scheduling to"}, {"paper": "Wang et al. (2025)", "relationship": "provides an alternative theoretical framework for online LLM inference scheduling to"}], "closest_prior_work": "Jaillet et al. (2025)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Optimal threshold parameter determination for specific deployment scenarios", "Extension to multi-GPU systems (considering communication costs and parallelization)", "Analytical frameworks for up-to-date LLM inference models (e.g., multi-head latent attention, KV cache compression)", "Performance analysis under more volatile arrival rates and demand surges"], "transferable_to": ["Other dynamic resource allocation problems with growing memory footprints", "Cloud scheduling with dynamic resource demands", "Prefill-Decode Disaggregated systems (e.g., DistServe, Splitwise)", "General queueing systems with state-dependent service times and eviction"], "open_weaknesses": ["Determining optimal threshold parameters for specific deployment scenarios remains an open problem", "Current research is limited to single-GPU systems", "Lack of analytical framework for state-of-the-art LLM inference models (e.g., multi-head latent attention)", "Performance under highly volatile (non-stationary) arrival rates needs further study"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_6", "front_status": "emerging", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.61, "experiments": {"benchmarks": ["vLLM", "Sarathi"], "baselines": ["vLLM (Kwon et al. 2023)", "Sarathi (Agrawal et al. 2023)"], "hardware": "NVIDIA A100 GPU (simulated via Microsoft Vidur), L20 GPU", "instance_sizes": [2, 3, 4, 10, 50, 60, 100, 160, 200, 300, 500, 1000, 4000, 6000, 50000]}, "results": {"vs_baselines": {"vLLM": "Superior throughput and reduced latency across synthetic and real-world datasets.", "Sarathi": "Superior throughput and reduced latency across synthetic and real-world datasets."}, "scalability": "Achieves near-optimal throughput in the asymptotic regime with logarithmic memory overhead for safety buffers; performance depends primarily on arrival rates and output lengths, not number of types.", "statistical_rigor": "Theoretical analysis establishes near-optimal performance in the asymptotic regime using fluid dynamics approximation, coupling arguments, exponential martingales, Doob's inequality, and union bounds for high-probability memory guarantees.", "limitations_acknowledged": ["Determining optimal threshold parameters for specific deployment scenarios", "Extension to multi-GPU scenarios", "Analytical frameworks for up-to-date LLM inference models (e.g., multi-head latent attention)", "Performance under more volatile arrival rates (e.g., demand surges)"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2512.24077", "arxiv_url": "https://arxiv.org/abs/2512.24077", "title": "LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm", "authors": ["Chunhui"], "abstract": "", "published_date": "2025-12-30", "affiliations": "", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 10, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper directly addresses our primary bottleneck in AlgoEvo: the inefficiency of 'blind' LLM mutations. It successfully integrates a ReAct-style cognitive loop (Plan-Execute-Summarize) into the evolutionary cycle, using lineage-based memory to guide mutations. The results on AlphaEvolve benchmarks explicitly outperform our current baselines (OpenEvolve/ShinkaEvolve) by 60% in efficiency."}, "brief": "LoongFlow replaces the standard stochastic mutation operator in LLM evolutionary search with a 'Plan-Execute-Summarize' (PES) cognitive loop. Instead of random code changes, a Planner retrieves the 'intent' and 'summary' of the parent solution's lineage to generate a directed hypothesis, which is then executed and summarized for the next generation. The authors demonstrate a 60% reduction in evaluations and a 100% success rate on AlphaEvolve tasks where standard methods fail or stagnate. The critical takeaway is the 'Lineage-Based Context Retrieval' mechanism: explicitly passing the parent's plan and retrospective summary to the child allows for directed rather than random walks in the search space. We must implement this PES loop in AlgoEvo immediately to fix our sample efficiency issues.", "methodology": {"core_method": "Plan-Execute-Summarize (PES) paradigm integrated with Hybrid Evolutionary Memory (Multi-Island, MAP-Elites, Adaptive Boltzmann Selection)", "llm_role": "planner, executor, summarizer", "llm_model_used": "DeepSeek-r1-0528, Gemini-3-Pro-Preview, Gemini-3.0-flash, Claude-Opus-4.5", "search_type": "hybrid", "novelty_claim": "LoongFlow integrates LLMs into a cognitive 'Plan-Execute-Summarize' (PES) paradigm and a hybrid evolutionary memory system to achieve directed evolutionary search, overcoming limitations of traditional 'blind' mutation operators.", "components": ["Plan-Execute-Summarize (PES) paradigm", "Hybrid Evolutionary Memory", "Multi-Island Model", "MAP-Elites", "Adaptive Boltzmann Selection", "Lineage-Based Context Retrieval", "Polymorphic Execution Strategies", "Local Verification Loop", "Abductive Reflection"], "training_required": false}, "tags": {"methods": ["plan_execute_summarize", "hybrid_evolutionary_memory", "multi_island_model", "map_elites", "adaptive_boltzmann_selection", "lineage_based_context_retrieval", "polymorphic_execution_strategies", "local_verification_loop", "abductive_reflection", "llm_as_planner", "llm_as_executor", "llm_as_summarizer", "llm_evolutionary_search", "evolution_of_heuristics", "program_synthesis", "llm_code_generation", "self_improving_search"], "problems": ["algorithm_discovery", "operator_discovery", "ml_pipeline_optimization", "mathematical_problems", "circle_packing"], "contribution_type": ["new_method", "sota_result", "framework"], "framework_lineage": "alphaevolve", "specific_domain": "operator_discovery", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm for Algorithmic Discovery and Pipeline Optimization", "short": "Algorithmic Discovery", "class_": "llm_evolutionary_search", "properties": ["self_improving", "code_generation", "high_dimensional_search_space", "open_ended"], "scale": "Various mathematical problems and complex ML pipelines"}, "lineage": {"direct_ancestors": [{"paper": "AlphaEvolve", "relationship": "extends the concept of LLM-driven evolutionary search from"}, {"paper": "OpenEvolve", "relationship": "improves upon the evolutionary efficiency and success rate of"}], "closest_prior_work": "OpenEvolve", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Explore application to more complex and diverse problem domains beyond current benchmarks.", "Investigate the impact of different LLM architectures and sizes on PES performance.", "Develop more sophisticated hybrid evolutionary memory strategies.", "Formalize the Plan-Execute-Summarize paradigm for broader theoretical understanding."], "transferable_to": ["automated_machine_learning_pipeline_design", "discovery_of_algorithms_for_other_combinatorial_optimization_problems", "scientific_discovery_requiring_iterative_hypothesis_generation", "program_synthesis_for_domain_specific_languages"], "open_weaknesses": ["Potential reliance on LLM quality and susceptibility to hallucinations.", "Scalability limitations for extremely large search spaces or very long program generation.", "Generalizability of the PES paradigm across all types of algorithmic discovery tasks.", "Computational cost associated with multiple LLM interactions in the loop."]}, "artifacts": {"code_url": "https://github.com/baidu-baige/LoongFlow", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.81, "experiments": {"benchmarks": ["AlphaEvolve Suite", "MLEBench"], "baselines": ["OpenEvolve", "ShinkaEvolve"], "hardware": "Not specified", "instance_sizes": []}, "results": {"vs_baselines": {"OpenEvolve": "Up to 60% improvement in evolutionary efficiency, significantly outperformed on AlphaEvolve Suite (e.g., Autocorrelation II: 0.9027 vs 0.8962), 100% success rate vs 33% on Circle Packing (Square), broke theoretical barrier vs failed on high-difficulty tasks.", "ShinkaEvolve": "Failed to break 0.99 barrier on Circle Packing (Square), failed to reach score of 1.0 on high-difficulty tasks."}, "scalability": "Achieves state-of-the-art solution quality with significantly reduced computational costs and superior sample efficiency, especially on high-difficulty tasks.", "statistical_rigor": "Evaluated across 3 independent runs for efficiency, reporting average and individual trajectories.", "limitations_acknowledged": []}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2512.21884", "arxiv_url": "https://arxiv.org/abs/2512.21884", "title": "Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models", "authors": ["Tingyang"], "abstract": "", "published_date": "2025-12-26", "affiliations": "Pennsylvania State University, Virginia Tech, Indian Institute of Science", "category": "OR for Generative AI", "relevance": {"methodological": 5, "problem": 9, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper provides a rigorous MILP formulation for the exact problem defined in your 'GPUSched' project (LLM serving optimization). It demonstrates that explicit OR modeling of GPU memory (specifically KV cache) significantly outperforms the heuristics currently used in systems like PETALS."}, "brief": "This paper formulates geographically distributed LLM inference as a joint block placement and request routing problem, solved via a decomposed MILP heuristic (greedy placement + shortest path routing). The results are real and validated on A100 clusters, showing 60-80% latency reduction over PETALS' native heuristics. The key takeaway for us is their explicit modeling of 'attention cache' memory consumption as a function of concurrent requests—treating this as a dynamic constraint rather than a static buffer is the primary driver of their performance gains. This is a direct blueprint for the constraints we need in our 'GPUSched' formulations, though the algorithmic techniques themselves are standard OR fare.", "methodology": {"core_method": "Three-step heuristic algorithm decomposing MILP: Conservative Greedy Block Placement (CG-BP) for block allocation and Waiting-penalized Shortest-path Request Routing (WS-RR) for request routing.", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "First systematic study of the resource allocation problem in distributed LLM inference, focusing on block placement and request routing, with experimentally validated performance models and a polynomial-complexity algorithm with guaranteed performance.", "components": ["Conservative Greedy Block Placement (CG-BP)", "Waiting-penalized Shortest-path Request Routing (WS-RR)"], "training_required": false}, "tags": {"methods": ["milp_formulation", "np_hardness_proof", "decomposition_algorithm", "greedy_algorithm", "shortest_path_routing", "robust_optimization", "performance_modeling", "simulation"], "problems": ["resource_allocation", "llm_serving_optimization", "block_placement", "request_routing"], "contribution_type": ["new_method", "theoretical_result", "empirical_study", "framework", "sota_result"], "framework_lineage": "petals", "specific_domain": "llm_serving_optimization", "llm_coupling": null}, "problem": {"formal_name": "Joint Block Placement and Request Routing for Distributed LLM Inference", "short": "BPRR", "class_": "resource_allocation", "properties": ["geographically_distributed", "pipeline_parallel", "capacitated", "online"], "scale": "Up to 149 servers/nodes, up to 96 LLM blocks, up to 200 concurrent requests"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2307.03161", "relationship": "optimizes resource allocation for the distributed LLM inference system PETALS"}], "closest_prior_work": "Distributed inference and fine-tuning of large language models over the internet (PETALS)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["optimize for other performance measures like cost or robustness", "improve suboptimality of greedy block placement and myopic request scheduling", "adapt block placement dynamically to observed concurrent requests"], "transferable_to": ["any_pipeline_parallel_llm_inference_systems", "systems_with_direct_server_server_communications", "heterogeneous_input_output_lengths"], "open_weaknesses": ["suboptimality_of_cg_bprr", "conservative_block_assignment_may_miss_feasible_solutions", "complex_runtime_factors_not_fully_captured_by_model", "individually_optimal_scheduling_may_not_be_globally_optimal"]}, "artifacts": {"code_url": "https://github.com/TingyangSunJeff/LLM_inference_simulator/tree/main", "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_status": "growing", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.54, "experiments": {"benchmarks": ["PETALS system", "BLOOM-176B", "Internet Topology Zoo (AboveNet, BellCanada, GTS-CE)"], "baselines": ["PETALS original algorithm [8]", "Optimized Number (PETALS with optimized block count)", "Optimized Order (PETALS with optimized server order)", "Optimized RR (PETALS with MILP routing)"], "hardware": "3 A100 (80 GB) GPUs, 8 A100 (80 GB) GPUs, NVIDIA MIGs (virtual GPUs), CPU-only simulator", "instance_sizes": [6, 9, 10, 15, 20, 22, 25, 26, 40, 100, 149, 200]}, "results": {"vs_baselines": {"PETALS original algorithm": "60-80% reduction in average per-token inference time, with up to 86% reduction for the first token, across diverse settings.", "Optimized Number": "Can improve inference time in most cases, particularly under high demands.", "Optimized Order": "May help in some cases, but does not always improve inference time.", "Optimized RR": "May help in some cases, but can worsen performance for long sequences."}, "scalability": "Performance improvement achieved by the proposed solution is larger in more resource-constrained scenarios and shows a widening performance gap in larger deployments.", "statistical_rigor": "Mean values reported from 5 Monte Carlo runs for experiments and 20 Monte Carlo runs for simulations. Runtime figures include standard deviations, but no explicit variance or significance tests are reported for inference time.", "limitations_acknowledged": ["Suboptimality of the greedy block placement and myopic request scheduling strategy", "Complex runtime factors (e.g., memory fragmentation and low-level scheduling) not captured by the system model"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2512.21487", "arxiv_url": "https://arxiv.org/abs/2512.21487", "title": "Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism", "authors": ["Xinglin"], "abstract": "", "published_date": "2025-12-25", "affiliations": "The Hong Kong University of Science and Technology, Harbin Institute of Technology, Hong Kong Baptist University", "category": "OR for Generative AI", "relevance": {"methodological": 6, "problem": 7, "inspirational": 5}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While not relevant to our primary evolutionary search focus, this is a strong reference for the 'GPUSched' and 'OR for AI systems' projects. It demonstrates how to formulate LLM inference scheduling as a tractable optimization problem using simple linear performance models and convexity proofs to enable online solving."}, "brief": "FinDEP optimizes distributed Mixture-of-Experts (MoE) inference by partitioning tasks (attention, experts, communication) into fine-grained micro-batches and solving a scheduling problem to maximize overlap. The authors achieve 1.02x-1.61x speedups on H20/A6000 clusters compared to PPPipe, backed by solid empirical data. The key takeaway for our 'GPUSched' work is their methodology: deriving analytical properties (monotonicity and convexity) of the scheduling objective to reduce a complex search space into an $O(1)$ online solver, rather than relying on heavy solvers or RL. This confirms that simple linear performance models ($\\alpha + \\beta x$) are sufficient for accurate online resource allocation in LLM serving.", "methodology": {"core_method": "Fine-grained task scheduling algorithm for disaggregated expert parallelism (DEP) with maximal task overlap, guided by linear performance models and analytical properties (monotonicity, convexity)", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "We propose FinDEP, a fine-grained task scheduling algorithm for DEP with maximal task overlap to improve the inference throughput of MoE models.", "components": ["Fine-grained task partitioning", "Performance models (GEMM, Attention, Communication)", "Optimization problem formulation", "Efficient algorithm for near-optimal solution", "AASS and ASAS scheduling strategies", "Offline planning phase", "Online adaptive phase"], "training_required": false}, "tags": {"methods": ["task_scheduling", "fine_grained_scheduling", "disaggregated_expert_parallelism", "ping_pong_pipeline", "performance_modeling", "linear_performance_models", "optimization_problem_formulation", "heuristic_algorithm_design", "mathematical_analysis", "convex_optimization", "scheduling_strategies"], "problems": ["moe_inference", "llm_serving_optimization", "gpu_resource_allocation", "communication_optimization", "task_scheduling"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "pppipe", "specific_domain": "llm_serving_optimization", "llm_coupling": null}, "problem": {"formal_name": "Mixture-of-Experts Inference Scheduling with Disaggregated Expert Parallelism", "short": "MoE Inference Scheduling", "class_": "scheduling", "properties": ["memory_intensive", "disaggregated_expert_parallelism", "shared_experts", "fine_grained_task_pipelining", "communication_overhead"], "scale": "8-32 GPUs, sequence length up to 8192, large MoE models (236B, 235B parameters)"}, "lineage": {"direct_ancestors": [{"paper": "MegaScale-Infer [36]", "relationship": "extends the PPPipe algorithm from"}], "closest_prior_work": "MegaScale-Infer [36]", "novelty_type": "incremental"}, "extensions": {"next_steps": ["Optimize for communication-dominated scenarios at extreme scales", "Enable dynamic adaptation of AG/EG partitioning in online settings", "Integrate with other LLM inference optimizations like quantization or offloading"], "transferable_to": ["Other distributed deep learning inference architectures", "General parallel computing workloads with computation-communication overlap challenges", "Mixture-of-Experts training optimization"], "open_weaknesses": ["Limited performance gains on high-bandwidth interconnects where communication is not the primary bottleneck", "Diminished relative improvement at extremely large scales where communication dominates", "Reboot costs limit frequent dynamic changes to AG and EG in online settings", "Limited improvement for shorter sequences due to communication optimizations"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_0", "front_status": "emerging", "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.13, "experiments": {"benchmarks": ["DeepSeek-V2 236B", "Qwen3-235B-A22B"], "baselines": ["PPPipe [36]", "Naive DEP"], "hardware": "8x NVIDIA A6000, 8x NVIDIA A10, 8x NVIDIA H20, 32x NVIDIA H20 GPUs; Ubuntu 22.04, Python 3.10, CUDA 11.3, PyTorch 2.4, NCCL 2.27.5, FlashInfer 0.3.0", "instance_sizes": [1024, 2048, 4096, 8192, 1, 2, 4, 8, 12, 16, 24, 48, 3072, 6144]}, "results": {"vs_baselines": {"PPPipe": "up to 1.61x throughput improvement (e.g., Qwen, S=8192 on Testbed B)", "Naive DEP": "1.7x reduction in non-overlapped communication vs PPPipe (DeepSeek-V2, S=4096 on Testbed A)"}, "scalability": "FinDEP shows robust speedups up to 1.24x on 32-GPU systems, demonstrating efficiency at large scales, especially when communication and computation are balanced. However, at extremely large scales where communication dominates, relative gains may diminish.", "statistical_rigor": "30 trials per data point (10 warm-up, 20 for statistics) for micro-benchmarks. Average of 3 independent runs for end-to-end throughput.", "limitations_acknowledged": ["Limited improvement for shorter sequences due to communication optimizations", "Diminished gains on high-bandwidth interconnects (e.g., H20 NVLink) where communication is not the primary bottleneck", "Relative improvement diminishes at extremely large scales where communication dominates", "Reboot costs limit frequent changes to AG and EG in online settings"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2512.14806", "arxiv_url": "https://arxiv.org/abs/2512.14806", "title": "Let the Barbarians In: How AI Can Accelerate Systems Performance Research", "authors": ["Audrey"], "abstract": "", "published_date": "2025-12-22", "affiliations": "UC Berkeley", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 7, "problem": 10, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper establishes the current SOTA baselines for LLM-driven evolution in systems (our exact niche). While it uses existing frameworks (OpenEvolve, GEPA), its extensive ablation studies on feedback granularity, seed diversity, and edit constraints provide a 'tuning manual' that we should immediately apply to AlgoEvo and EvoCut."}, "brief": "Cheng et al. (UC Berkeley) perform a rigorous empirical evaluation of LLM evolutionary search (ADRS) across 10 systems problems, achieving SOTA results on MoE load balancing (13x speedup via rediscovering Hamilton's Apportionment) and cloud scheduling. The results are real and backed by code, comparing frameworks like OpenEvolve, GEPA, and ShinkaEvolve. **Key Takeaway:** Their 'Best Practices' section offers concrete engineering constraints we should adopt: specifically, that 'moderate' feedback (worst-k cases) outperforms 'detailed' feedback (prevents overfitting), and that restricting mutations to diff-based edits is essential to prevent reward hacking. This paper validates our core research thesis while providing the benchmarks we now need to beat.", "methodology": {"core_method": "Evolutionary search using LLM-based Prompt Generator, Solution Generator, Evaluator, Storage, and Solution Selector components", "llm_role": "solution_generator, prompt_generator, evaluator", "llm_model_used": "GPT-5, Gemini-3.0-Pro-Preview", "search_type": "improvement", "novelty_claim": "This paper evaluates existing open-source AI-Driven Research for Systems (ADRS) frameworks across ten systems performance problems, demonstrating that AI-generated algorithms can match or outperform human state-of-the-art solutions and outlining best practices.", "components": ["Prompt Generator", "Solution Generator", "Evaluator", "Storage", "Solution Selector"], "training_required": false}, "tags": {"methods": ["evolution_of_heuristics", "program_synthesis", "llm_code_generation", "llm_as_heuristic", "llm_as_evaluator", "llm_prompt_optimization", "llm_evolutionary_search", "openevolve", "gepa", "shinkaevolve", "empirical_evaluation"], "problems": ["systems_performance_optimization", "telemetry_repair", "multi_cloud_data_transfer_cost_optimization", "expert_parallelism_load_balancing", "model_to_gpu_placement", "llm_inference_optimization", "transaction_scheduling", "deadline_driven_job_scheduling", "multi_agent_system_optimization", "datacenter_tcp_congestion_control"], "contribution_type": ["empirical_study", "framework", "sota_result"], "framework_lineage": null, "specific_domain": null, "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "AI-Driven Research for Systems for Systems Performance Problems", "short": "ADRS", "class_": "research_automation", "properties": ["performance_optimization", "algorithm_discovery", "iterative_refinement", "systems_research"], "scale": "ten case studies across diverse sub-domains (networking, databases, core systems), multi-region cloud scheduling, mixture-of-experts load balancing, LLM-based SQL, transaction scheduling"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2510.06189", "relationship": "extends prior empirical study of ADRS"}, {"paper": "OpenEvolve", "relationship": "evaluates and provides best practices for"}, {"paper": "GEPA", "relationship": "evaluates and provides best practices for"}, {"paper": "ShinkaEvolve", "relationship": "evaluates and provides best practices for"}], "closest_prior_work": "arXiv:2510.06189", "novelty_type": "incremental"}, "extensions": {"next_steps": ["develop_better_evaluators_for_fidelity_generality_speed", "improve_problem_specification_and_prompting_with_retrieval_and_evolution", "enhance_solution_generators_for_multi_file_reasoning_and_ensembles", "automate_hyperparameter_tuning_for_exploration_exploitation"], "transferable_to": ["other_systems_performance_optimization_problems", "automated_algorithm_discovery_in_other_scientific_domains", "designing_distributed_protocols"], "open_weaknesses": ["requires_isolated_changes_in_small_system_parts", "difficulty_with_coordinated_changes_across_multiple_modules", "requires_reliable_and_efficient_evaluators", "discovery_process_is_a_black_box"]}, "artifacts": {"code_url": "https://github.com/codelion/openevolve", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.54, "experiments": {"benchmarks": ["Telemetry Repair", "Cloudcast", "Expert Parallelism Load Balancer (EPLB)", "Model Placement (Prism)", "LLM-SQL", "Transaction Scheduling (TXN)", "Can’t Be Late (CBL)", "Can’t Be Late Multi-Region Extension (CBL-Multi)", "Multi-Agent System Optimization (MAS)", "Datacenter TCP Congestion Control (NS3)", "ShareGPT", "GSM8K", "movies", "beer", "BIRD", "PDMX", "products", "Epinions", "SmallBank", "TPC-C", "TAOBench", "YCSB"], "baselines": ["HotNets’24 solution", "Naive direct replication", "DeepSeek open-source EPLB (greedy bin-packing)", "Proprietary EPLB implementation", "Prism algorithm [84]", "Greedy recursive group algorithm (GGR) [43]", "Original table ordering", "Random scheduler", "Shortest Makespan First (SMF) [13]", "Greedy policy [80]", "Uniform Progress algorithm [80]", "Multi-region Uniform Progress variant", "MetaGPT [30]", "θ-PowerTCP [4]"], "hardware": "1xK80, 1xV100, 8xK80, 8xV100 GPUs; costs $15-$30 per case study for 100 iterations", "instance_sizes": [500, 10]}, "results": {"vs_baselines": {"Telemetry Repair": "+11.8% better counter repair score, +47.9% higher confidence calibration vs HotNets’24 solution", "Cloudcast": "No improvement vs SOTA, 31.1% cost reduction vs naive direct replication", "Expert Parallelism Load Balancer (EPLB)": "Same load balance, 13x faster runtime vs proprietary implementation", "Model Placement (Prism)": "20.9% cheaper than published solution", "LLM-SQL": "Comparable hit rate, 3.9x faster runtime vs GGR", "Transaction Scheduling (TXN)": "60% better than greedy (offline), matches SMF (online)", "Can’t Be Late (CBL)": "Up to 16% (average 7%) higher cost savings vs SOTA, 6% higher average cost savings than Uniform Progress (and 20% over the greedy policy)", "Can’t Be Late Multi-Region Extension (CBL-Multi)": "26% lower cost vs single-region baseline, 17% improvements over a strong baseline", "Multi-Agent System Optimization (MAS)": "7% improvement on ProgramDev vs MetaGPT", "Datacenter TCP Congestion Control (NS3)": "49% lower queue length, similar throughput vs θ-PowerTCP"}, "scalability": "ADRS frameworks are efficient, with most case studies requiring only a few hours and tens of dollars, leveraging simulators for faster iteration, but are best suited for problems with isolated changes and efficient, reliable evaluations.", "statistical_rigor": "Experiments were repeated three times, reporting mean ± standard deviation over these runs.", "limitations_acknowledged": ["ADRS works best with isolated changes in small, self-contained system parts", "Problems requiring coordinated changes across multiple modules are difficult due to limited multi-file reasoning and context lengths", "Reliable evaluators are needed for unambiguous ranking, difficult for semantic equivalence verification", "Efficient evaluations are crucial, as thousands of iterations can be prohibitively slow/expensive", "Not effective for problems solvable by existing optimization solvers (e.g., ILP)", "Building inexpensive yet faithful simulators for complex systems is non-trivial", "The discovery process of ADRS frameworks remains largely a black box", "Current evolutionary searches are often inefficient, looping over failed heuristics or repeated errors", "Formalizing intuitive trade-offs into numerical weights for objectives can be difficult", "Feedback granularity (e.g., moderate vs. minimal/detailed) is user-determined and impacts overfitting", "Hyperparameter automation for balancing exploration and exploitation is needed", "Optimal balance between synchronous and asynchronous human-ADRS interaction is an open question"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2511.02864", "arxiv_url": "https://arxiv.org/abs/2511.02864", "title": "Mathematical exploration and discovery at scale", "authors": ["Bogdan"], "abstract": "", "published_date": "2025-12-22", "affiliations": "Google DeepMind, UCLA, Brown University, Institute for Advanced Study", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 7, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This is the definitive empirical playbook for scaling LLM evolutionary search (our primary focus). It validates 'Search Mode' (evolving heuristics) over direct solution generation and demonstrates emergent agentic behaviors (prompt injection) when evolved code is allowed to call LLMs."}, "brief": "DeepMind applies AlphaEvolve to 67 math problems, formalizing the distinction between 'Search Mode' (evolving heuristics for fixed instances) and 'Generalizer Mode' (evolving algorithms that extrapolate from small to large n). Results are rigorous, establishing new bounds on Kakeya sets and 10+ other problems by exploiting verifier loopholes and heuristic specialization. The most critical takeaway for AlgoEvo is Section 44: evolving code that *calls* other LLMs leads to emergent prompt optimization and injection strategies, suggesting a path for our multi-agent optimization work. We must adopt their 'Generalizer' training curriculum (train on small n, test on large n) to fix our scalability bottlenecks.", "methodology": {"core_method": "LLM-guided evolutionary search for programs (search heuristics) that find mathematical constructions", "llm_role": "Generates and mutates code for search heuristics; guides meta-level evolution of search strategies", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "AlphaEvolve introduces a meta-level evolutionary framework that optimizes the search process itself, combining LLM-guided code generation with automated evaluation to discover and refine algorithmic solutions for diverse mathematical construction problems at scale.", "components": ["LLM-based code generator", "automated evaluator (fitness function)", "evolutionary algorithm (selection, mutation)", "population of programs (search heuristics)", "search mode", "generalizer mode", "parallel computation"], "training_required": false}, "tags": {"methods": ["llm_evolutionary_search", "llm_code_generation", "llm_as_heuristic", "alphaevolve", "evolutionary_algorithm", "program_synthesis", "meta_level_evolution", "local_search", "gradient_descent", "convex_programming", "linear_programming", "monte_carlo_simulation", "automatic_differentiation", "jax_framework", "sympy_library", "numpy_library", "shapely_library", "binary_search", "sat_solvers", "in_context_learning"], "problems": ["mathematical_construction_discovery", "finite_field_kakeya_sets", "finite_field_nikodym_sets", "autocorrelation_inequalities", "difference_bases", "kissing_numbers", "kakeya_needle_problem", "sphere_packing", "uncertainty_principles", "hausdorff_young_inequality", "gagliardo_nirenberg_inequality", "youngs_convolution_inequality", "hardy_littlewood_maximal_inequality", "ovals_problem", "sendovs_conjecture", "schmeissers_conjecture", "borceas_conjecture", "smales_problem", "de_bruin_sharma_inequalities", "crouzeixs_conjecture", "sidorenkos_conjecture", "prime_number_theorem", "golays_merit_factor_conjecture", "blocks_stacking_problem", "arithmetic_kakeya_conjecture", "furstenberg_sarkozy_theorem", "spherical_designs", "thomson_problem", "tammes_problem", "packing_in_a_dilate", "circle_packing_in_a_square", "turan_hypergraph_problem", "factoring_factorials", "beat_the_average_game", "erdos_discrepancy_problem", "fejes_toth_problem", "sums_and_differences_of_sets", "sum_product_problem", "minimal_triangle_density", "matrix_multiplication_inequalities", "heilbronn_problem", "max_to_min_distance_ratios", "erdos_gyarfas_conjecture", "erdos_squarefree_problem", "equidistant_points_in_convex_polygons", "pairwise_touching_cylinders", "erdos_squares_in_a_square_problem", "szemeredi_trotter_theorem", "rudin_problem_for_polynomials", "erdos_szekeres_happy_ending_problem", "subsets_of_grid_no_isosceles_triangles", "no_5_on_a_sphere_problem", "ring_loading_problem", "moving_sofa_problem", "imo_tiling_problem", "function_guessing_game", "smullyan_type_logic_puzzles"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study", "negative_result"], "framework_lineage": "alphaevolve", "specific_domain": null, "llm_coupling": "llm_as_heuristic"}, "problem": {"formal_name": "Algorithmic Discovery for Mathematical Constructions", "short": "Mathematical Construction Discovery", "class_": "algorithm_design", "properties": ["generative", "iterative", "evolutionary", "LLM-guided", "automated_evaluation", "multi-level_abstraction", "heuristic_search", "optimization"], "scale": "Diverse mathematical problems, from small (e.g., n=3) to large (e.g., n=500, 100x100 grids, degree 300 polynomials)"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:224", "relationship": "introduces AlphaEvolve"}, {"paper": "arXiv:242", "relationship": "predecessor, FunSearch"}], "closest_prior_work": "arXiv:242", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Further incorporate computer-assisted proof into AlphaEvolve output", "Systematically assess difficulty of mathematical bounds/conjectures for classification", "Enable AlphaEvolve to select its own hyperparameters dynamically", "Explore finding better generalizable constructions for Kakeya needle problem", "Extract tighter bounds on lower order coefficients in Thomson problem energy asymptotics", "Solve IMO tiling problem for all n, not just perfect squares"], "transferable_to": ["Other scientific discovery problems", "Other mathematical bounds or conjectures", "Problem variants (e.g., different potentials for Thomson, different shapes for packing)", "Computer-assisted proof generation and formal verification"], "open_weaknesses": ["Struggles with problems not formulated as smooth score function optimization", "Susceptible to 'cheating phenomenon' by exploiting verifier loopholes", "Potential loss of interpretability in evolved search processes", "Performance can deteriorate for larger problem instances in some domains", "Slow asymptotic convergence can lead to misinterpretation of small-scale improvements", "Reliance on cheap LLMs for internal calls can introduce noise and unreliability", "Does not always figure out 'obvious' human ideas", "Does not currently handle proving optimality"]}, "artifacts": {"code_url": "https://colab.research.google.com/github/google-deepmind/alphaevolve_results/blob/master/mathematical_results.ipynb", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.06, "experiments": {"benchmarks": ["67 diverse mathematical problems (e.g., Kakeya sets, Autocorrelation inequalities, Kissing numbers, Sphere packing, Heilbronn problems, Sum-product problems, Erdős discrepancy problem, IMO problems)"], "baselines": ["Best known solutions from mathematical literature", "FunSearch", "traditional computational/theoretical methods", "Keich's construction", "Cohn-Elkies baseline", "Sutherland's code", "Cantrell's constructions"], "hardware": "CPUs (up to 20 parallel threads), GPUs (for specific evaluators, e.g., JAX-based), typical runs lasting hours to days", "instance_sizes": [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 24, 25, 30, 32, 36, 40, 50, 64, 70, 80, 91, 100, 128, 140, 150, 180, 200, 205, 210, 240, 250, 282, 292, 300, 306, 310, 320, 350, 360, 400, 420, 430, 450, 460, 500, 600, 1160, 2025, 6166]}, "results": {"vs_baselines": {"Kakeya sets (d=3)": "C_K(3,p) <= 1/4 p^3 + 7/8 p^2 - 1/8 (refining previous O(p) term)", "Nikodym sets (d>=3)": "C_N(d,p) <= p^d - (((d-2)/log 2) + 1 + o(1)) p^(d-1) log p (improving best known construction)", "Autocorrelation (Prob 6.2)": "C6.2 <= 1.5032 (from 1.50992)", "Difference bases (Prob 6.7)": "C6.7 <= 2.6390 (from 2.6571)", "Kissing numbers (C6.8(11))": "Improved lower bound from 592 to 593", "Kakeya needle (2D)": "Obtained constructions with better union area than Keich's within 1-2 hours", "Sphere packing (C6.13(n))": "Competitive with Cohn-Elkies, matching up to 4 digits for lower n, improving for higher n", "Sums and differences (C6.44)": "C6.44 >= 1.1584 (from 1.14465)", "Touching cylinders (n=7)": "Found construction with loss O(10^-23) in 2 hours (vs 4 months CPU for previous work)", "Erdős squares in a square (C6.55(11,P))": "Improved upper bound from 2.912096 to 2.894531", "Subsets of grid (64x64)": "Found 112 points (from 110) without isosceles triangles", "No 5 on a sphere (C6.60(8))": "Improved lower bound from 22 to 23", "Ring Loading Problem (C6.61)": "C6.61 >= 1.119 (from 1.1)", "Moving sofa (3D)": "Found construction with estimated volume 1.84 (rigorous lower bound 1.81) vs 1.7699", "IMO 2025 Problem 6 (C6.65(n))": "Found optimal solution for all perfect square n"}, "scalability": "AlphaEvolve scales to study large classes of problems simultaneously, accelerating time-to-discovery with increased parallelism, though at a higher total computational cost (LLM queries).", "statistical_rigor": "Experiments on Problem 6.2 averaged over 10-100 runs with varying CPU threads; LLM model comparisons averaged over 20-50 runs. Reproducibility is expected with sufficient experiments due to evolutionary randomness.", "limitations_acknowledged": ["Struggles with problems not formulated as smooth score function optimization", "Did not always attain optimal or near-optimal results", "Susceptible to 'cheating phenomenon' by exploiting verifier loopholes", "Potential loss of interpretability in evolved search processes", "Performance can deteriorate for larger problem instances in some domains", "Slow asymptotic convergence can lead to misinterpretation of small-scale improvements", "Reliance on cheap LLMs for internal calls can introduce noise and unreliability"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2512.18682", "arxiv_url": "https://arxiv.org/abs/2512.18682", "title": "Solver-Independent Automated Problem Formulation via LLMs for High-Cost Simulation-Driven Design", "authors": ["Yuchen"], "abstract": "", "published_date": "2025-12-21", "affiliations": "Xidian University, Victoria University of Wellington, Westlake University", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 5, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "The paper introduces a 'solver-independent' evaluation metric that uses LLM-predicted rankings of historical data as a proxy for code correctness. This is a concrete, actionable technique to reduce evaluation costs in our own evolutionary search and reward modeling pipelines."}, "brief": "Li et al. propose APF, a framework to fine-tune LLMs for translating engineering requirements into optimization code without running expensive simulations during training. They generate synthetic training data and filter it by checking if the generated code ranks historical data instances similarly to how an LLM 'judge' ranks them based on the text requirements. Results show 7B models outperforming GPT-4o on antenna design tasks, validated by actual simulation. **Key Takeaway:** We can replace expensive ground-truth evaluations in our process reward models by checking consistency between generated code outputs and LLM-predicted rankings on cached historical data—a direct method to improve sample efficiency in AlgoEvo.", "methodology": {"core_method": "Supervised fine-tuning of LLMs on a synthetically generated dataset, created via data augmentation (semantic paraphrasing, order permutation) and LLM-based test instance annotation and selection, to convert natural language requirements into executable Python optimization functions.", "llm_role": "code_writer", "llm_model_used": "LLAMA3.1-8B-Instruct, Qwen2.5-7B-Instruct, Mistral-7B-Instruct", "search_type": "constructive", "novelty_claim": "APF proposes a solver-independent framework for automated problem formulation via LLMs, featuring an innovative pipeline for automatically generating high-quality fine-tuning data without relying on expensive solver feedback.", "components": ["Data Generation Module", "Test Instance Annotation Module", "Data Evaluation and Selection Module", "Supervised Fine-Tuning Module", "Semantic Paraphrasing", "Order Permutation", "LLM-based Ranking (listwise)"], "training_required": true}, "tags": {"methods": ["supervised_learning", "llm_fine_tuned", "synthetic_data_generation", "data_augmentation", "llm_code_generation", "llm_as_evaluator"], "problems": ["automated_problem_formulation", "expensive_black_box_optimization", "antenna_design", "constrained_optimization"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "apf", "specific_domain": "antenna_design", "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Automated Problem Formulation for High-Cost Simulation-Driven Design", "short": "Problem Formulation", "class_": "program_synthesis", "properties": ["high_cost_simulation_driven", "natural_language_requirements", "solver_independent", "multi_objective", "constrained"], "scale": "Multiple interdependent/conflicting requirements, high-dimensional curves"}, "lineage": {"direct_ancestors": [{"paper": "ORLM: A Customizable Framework in Training Large Models for Automated Optimization Modeling", "relationship": "extends fine-tuning for problem formulation by removing solver dependency"}, {"paper": "OptMATH: A Scalable Bidirectional Data Synthesis Framework for Optimization Modeling", "relationship": "extends fine-tuning for problem formulation by removing solver dependency"}, {"paper": "LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch", "relationship": "extends fine-tuning for problem formulation by removing solver dependency"}], "closest_prior_work": "ORLM: A Customizable Framework in Training Large Models for Automated Optimization Modeling", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["validate the framework in broader physics-based engineering fields (e.g., aerodynamics, structural optimization)", "address the context window limitation for highly complex problem descriptions or large-scale validation datasets", "further improve the data generation and selection pipeline", "explore other LLM architectures or fine-tuning strategies"], "transferable_to": ["aerodynamics", "structural_optimization", "micro_electronics", "robotics"], "open_weaknesses": ["evaluation focused exclusively on antenna design, limiting cross-domain generalizability", "solver_independent_evaluation_bounded_by_llm_context_window", "potential for low-quality synthetic data if filtering is not strict enough"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_9", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.99, "experiments": {"benchmarks": ["Antenna Design (case study)"], "baselines": ["GPT-4o", "DeepSeek-V3", "Chain-of-Experts", "OptiMUS", "LLAMA3.1-8B (base)", "Qwen2.5-7B-Instruct (base)", "Mistral-7B-Instruct (base)"], "hardware": "4 NVIDIA V100 Tensor Core GPUs (32 GB each)", "instance_sizes": [2300]}, "results": {"vs_baselines": {"DeepSeek-V3": "APF (LLAMA3.1-8B) achieved +4.58% higher overall alignment score (0.7976 vs 0.7518)", "GPT-4o": "APF (LLAMA3.1-8B) achieved +13.25% higher overall alignment score (0.7976 vs 0.6651)", "Chain-of-Experts": "APF (LLAMA3.1-8B) achieved +7.24% higher overall alignment score (0.7976 vs 0.7252)", "OptiMUS": "APF (LLAMA3.1-8B) achieved +12.89% higher overall alignment score (0.7976 vs 0.6687)", "LLAMA3.1-8B (base)": "APF (LLAMA3.1-8B) achieved +254.8% higher overall alignment score (0.7976 vs 0.2248)", "Qwen2.5-7B (base)": "APF (Qwen2.5-7B) achieved +50.4% higher overall alignment score (0.7961 vs 0.5292)", "Mistral-7B (base)": "APF (Mistral-7B) achieved +163.3% higher overall alignment score (0.7918 vs 0.3007)"}, "scalability": "The framework provides a scalable and efficient path for automating domain-specific problem formalization, enabling smaller open-source models to match or exceed larger state-of-the-art models.", "statistical_rigor": "Results are reported as mean (standard deviation) across multiple runs.", "limitations_acknowledged": ["Evaluation focused exclusively on antenna design, limiting cross-domain generalizability", "Solver-independent evaluation is bounded by LLM context window, limiting processing of highly complex problem descriptions or large-scale validation datasets"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2512.18134", "arxiv_url": "https://arxiv.org/abs/2512.18134", "title": "Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs", "authors": ["Rupanshu"], "abstract": "", "published_date": "2025-12-19", "affiliations": "Stanford University, NVIDIA", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper successfully applies formal OR methods (ILP/SMT) to solve low-level GPU instruction scheduling and warp specialization, a domain typically dominated by heuristics or manual tuning. It directly addresses your interest in 'OR formulations for AI systems' and provides a rigorous alternative (or complement) to evolutionary code search for kernel optimization."}, "brief": "Twill formulates the complex interplay of software pipelining and warp specialization on modern GPUs (Hopper/Blackwell) as a joint SMT/ILP optimization problem, automatically rediscovering expert-tuned Flash Attention schedules without heuristics. The results are rigorous, matching hand-tuned performance within 1-2% and handling new hardware constraints (Blackwell TMEM) automatically. The key takeaway is the 'cost normalization' technique via ILP to make the scheduling search space tractable, and the demonstration that exact constraint solvers can replace human intuition for complex kernel generation. This is essential reading for your work on OR formulations for GPU scheduling and LLM serving optimization, offering a deterministic baseline to compare against evolutionary approaches.", "methodology": {"core_method": "Joint optimization of modulo scheduling and warp specialization formulated as a constraint satisfaction problem, solved by Integer Linear Programming (ZLP) for initial modulo schedule and Satisfiability Modulo Theories (SMT) solver for combined SWP and WS", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "We introduce a novel formulation of SWP and WS as a joint optimization problem that can be solved holistically by off-the-shelf constraint solvers.", "components": ["Modulo scheduling", "Integer Linear Programming (ZLP)", "Satisfiability Modulo Theories (SMT) solver", "Memory allocation constraints", "Warp assignment constraints", "Cost normalization (ZLP)", "Variable latency optimizations"], "training_required": false}, "tags": {"methods": ["software_pipelining", "warp_specialization", "modulo_scheduling", "integer_linear_programming", "satisfiability_modulo_theories", "constraint_satisfaction_problem", "compiler_optimization", "code_generation", "resource_allocation", "cost_normalization"], "problems": ["gpu_scheduling", "tensor_core_scheduling", "flash_attention", "matrix_multiplication_optimization"], "contribution_type": ["new_method", "framework", "sota_result", "theoretical_result", "empirical_study"], "framework_lineage": "twill", "specific_domain": "tensor_core_scheduling", "llm_coupling": null}, "problem": {"formal_name": "Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs", "short": "SWP and WS for TC GPUs", "class_": "compiler_optimization", "properties": ["joint_optimization", "heuristic-free", "optimal_schedules", "constraint-based", "architecture-specific"], "scale": "Flash Attention on Hopper and Blackwell architectures, sequence length 2048-16384"}, "lineage": {"direct_ancestors": [{"paper": "Lam 1988", "relationship": "adapts modulo scheduling from"}, {"paper": "Rau 1994", "relationship": "adapts modulo scheduling from"}, {"paper": "Bauer et al. 2011", "relationship": "builds on warp specialization concept from"}, {"paper": "Bauer et al. 2014", "relationship": "builds on warp specialization concept from"}], "closest_prior_work": "PipeThreader", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["support for loops with control flow using hierarchical reduction techniques", "automatic determination of tile size"], "transferable_to": ["other deep learning kernels", "new gpu architectures", "other hardware accelerators"], "open_weaknesses": ["limited to singly_nested_loops without control flow", "requires manual tile size selection", "solution times range from tens of seconds to a few minutes"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.11, "experiments": {"benchmarks": ["Flash Attention Forward Pass", "Flash Attention Backward Pass"], "baselines": ["Triton", "Triton-Tiled", "CUDA-Default", "cuDNN", "Flash Attention 3 (FA3)", "Flash Attention 4 (FA4)", "Twill-SWP", "Twill-SWP-Triton-WS", "CUDA-Triton-WS", "Twill-LR"], "hardware": "NVIDIA H100 SXM5 80 GB, NVIDIA B200 180 GB, Intel Xeon Platinum 8570", "instance_sizes": [2048, 4096, 8192, 16384]}, "results": {"vs_baselines": {"Flash Attention Forward Pass (Hopper)": "within 1% of official FA3 implementation; significantly outperformed Triton, Triton-Tiled, CUDA-Default", "Flash Attention Forward Pass (Blackwell)": "within 2% of FA4; competitive with cuDNN; significantly better than Triton, Triton-Tiled, CUDA-Default; Twill-SWP-Triton-WS and CUDA-Triton-WS yielded no benefit", "Flash Attention Backward Pass (Hopper)": "11% worse than FA3 (due to tile size); competitive with cuDNN on highest sequence length", "Flash Attention Backward Pass (Blackwell)": "Twill-LR yielded small speedup over default implementation, competitive with FA4 and cuDNN"}, "scalability": "Solution times for Twill’s joint problem range from tens of seconds to a few minutes. Performance (TFLOPS/s) generally increases with sequence length for all methods.", "statistical_rigor": "All experiments use CUDA 13.0. No explicit mention of multiple runs, variance, or significance tests.", "limitations_acknowledged": ["only supports singly-nested loops without additional control flow", "tile size is not automatically determined", "solution times range from tens of seconds to a few minutes"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2509.18057", "arxiv_url": "https://arxiv.org/abs/2509.18057", "title": "Reinforced Generation of Combinatorial Structures: Hardness of Approximation", "authors": ["Ansh"], "abstract": "", "published_date": "2025-12-19", "affiliations": "Google DeepMind, Google, University of California, Berkeley", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 5, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper introduces a critical innovation for our AlphaEvolve work: using the LLM to evolve the *verification/evaluation code* itself for performance (achieving 10,000x speedups). This directly addresses the evaluation bottleneck in our evolutionary search pipelines, allowing us to scale to larger search spaces."}, "brief": "Nagda et al. utilize AlphaEvolve to discover combinatorial gadgets that improve hardness of approximation bounds for MAX-CUT and TSP, validating findings with formal proofs. The standout contribution is not the hardness results themselves, but the methodology: they tasked AlphaEvolve with optimizing the *verification code* (checking correctness against a slow ground truth), achieving a 10,000x speedup that enabled searching gadgets of size 19 (vs. 11 previously). We should immediately adopt this 'evolve the verifier' loop for our computationally expensive fitness functions in AlgoEvo to break current scalability limits.", "methodology": {"core_method": "LLM-guided evolutionary search for combinatorial structures and verification procedures", "llm_role": "evolutionary_search", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "AlphaEvolve, an LLM code mutation agent, is used to discover and verify finite combinatorial structures (Ramanujan graphs and gadgets) that improve state-of-the-art hardness of approximation results.", "components": ["LLM-based code mutation agent", "code snippet for structure generation", "evaluation function (verifier)", "LLM-evolved verifier"], "training_required": false}, "tags": {"methods": ["alphaevolve", "llm_evolutionary_search", "llm_code_generation", "llm_as_evaluator", "evolution_of_heuristics", "branch_and_bound", "linear_programming", "mixed_integer_programming", "spectral_graph_theory", "genetic_algorithm"], "problems": ["max_cut", "max_independent_set", "max_k_cut", "metric_tsp", "hardness_of_approximation", "constraint_satisfaction_problems", "ramanujan_graphs", "gadget_design", "algorithm_discovery"], "contribution_type": ["sota_result", "new_method", "theoretical_result", "framework", "empirical_study"], "framework_lineage": "alphaevolve", "specific_domain": null, "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Maximum Cut, Maximum Independent Set, Maximum k-Cut, Metric Traveling Salesman Problem", "short": "MAX-CUT, MAX-IND-SET, MAX-k-CUT, TSP", "class_": "graph_optimization", "properties": ["sparse", "regular", "random_graphs", "Ramanujan", "NP-hard_approximation", "gadget_reduction", "metric"], "scale": "graphs up to 163 vertices; gadgets up to 19 variables (MAX-k-CUT) and 12 vertices (TSP)"}, "lineage": {"direct_ancestors": [{"paper": "KY24", "relationship": "improves lower bounds for MAX-CUT and MAX-IND-SET certification"}, {"paper": "AOTW14", "relationship": "improves inapproximability for MAX-4-CUT"}, {"paper": "CC20", "relationship": "improves inapproximability for metric TSP"}, {"paper": "TSSW00", "relationship": "builds on gadget reduction framework"}], "closest_prior_work": "CC20", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["further improvements to tsp bound", "routinely run gadget proofs through ai optimization", "complete characterizations of sigma_mc_d and sigma_is_d", "beating sota for max-3-cut (custom pcps)", "coupling llm reasoning with alphaevolve"], "transferable_to": ["other d-regular ramanujan graphs (d > 4)", "other gadget_based hardness proofs", "other combinatorial optimization problems", "mathematical discovery beyond or/ai"], "open_weaknesses": ["max-3-cut result not sota (vs custom pcps)", "alphaevolve failed on hadamard matrix construction", "direct llm prompting for structures met with failure", "verification remains a bottleneck for larger gadgets", "human intuition for asymmetric gadgets is challenging"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.56, "experiments": {"benchmarks": ["G(n,d) random graphs", "3LIN(3)", "3LIN(4)", "Hybrid-3LIN(2)"], "baselines": ["KY24", "Hof03", "Hae21", "AOTW14", "KKLP96", "GS09", "CC20", "PY93", "Eng99", "BHK+00", "PV06", "Lam14", "KLS15"], "hardware": "Optimized tensor contraction (numpy) and branch-and-bound strategies for verification, achieving 10,000x speedup for gadgets.", "instance_sizes": [12, 14, 19, 20, 36, 124, 163]}, "results": {"vs_baselines": {"KY24 (MAX-CUT d=4)": "+3.6% (0.911 vs 0.875)", "KY24 (MAX-IND-SET d=3)": "+1.4% (0.472 vs 0.458)", "KY24 (MAX-IND-SET d=4)": "+2.5% (0.453 vs 0.428)", "Hof03/Hae21 (MAX-CUT d=3)": "-1.8% (0.953 vs 0.971)", "Hof03/Hae21 (MAX-CUT d=4)": "-1.7% (0.916 vs 0.933)", "Hof03/Hae21 (MAX-IND-SET d=3)": "-0.9% (0.476 vs 0.485)", "Hof03/Hae21 (MAX-IND-SET d=4)": "-0.7% (0.457 vs 0.464)", "AOTW14 (MAX-4-CUT)": "improves SOTA from 0.9883 to 0.987", "KKLP96 (MAX-3-CUT gadget-based)": "improves from 0.9853 to 0.9649", "CC20 (Metric TSP)": "improves SOTA from 117/116 to 111/110"}, "scalability": "Scalability is achieved by AlphaEvolve's ability to search larger combinatorial spaces and by LLM-evolved verifiers that significantly speed up exponential-time verification procedures.", "statistical_rigor": "The results are based on formal proofs of NP-hardness, with gadgets verified by brute-force algorithms, and analytical arguments for upper bounds.", "limitations_acknowledged": ["MAX-3-CUT result does not beat SOTA relying on custom PCPs", "AlphaEvolve failed on Hadamard-668 and H428x428 constructions", "Direct LLM prompting failed to generate structures", "Verification remains a bottleneck", "Human experts or highly customized solvers might find solutions, but not with simple methods"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2512.16134", "arxiv_url": "https://arxiv.org/abs/2512.16134", "title": "Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference", "authors": ["Jian"], "abstract": "", "published_date": "2025-12-18", "affiliations": "Baidu Inc.", "category": "OR for Generative AI", "relevance": {"methodological": 6, "problem": 9, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper directly targets the core problem of your 'GPUSched' project (LLM inference scheduling) specifically for the latest DP+EP architectures (DeepSeek-V3). It provides production-verified heuristics for handling the 'coupled load imbalance' of KV-cache memory and compute, which we can directly port into our OR formulations."}, "brief": "Tian et al. introduce Staggered Batch Scheduling (SBS) for DP+EP architectures, enforcing a buffering window to enable global bin-packing rather than immediate dispatch, which they prove causes Head-of-Line blocking in non-preemptive prefill phases. Tested on a production H800 cluster serving DeepSeek-V3, they demonstrate a 30-40% reduction in TTFT and a ~20% throughput increase backed by clear utilization metrics. The most valuable takeaway for our GPUSched project is their 'IQR-aware lexicographical' scheduling heuristic for the Decode phase, which robustly balances batch size against KV-cache memory variance—a constraint logic we should immediately adopt. This work validates that discrete batching is superior to continuous dispatch for MoE models, necessitating an update to our queuing theory models.", "methodology": {"core_method": "Staggered Batch Scheduling (SBS) with Throughput-Adaptive Interval Control, Multi-tier State Synchronization, Prioritized Batch Allocation Algorithm (PBAA) for Prefill, and IQR-Aware Lexicographical Decode Scheduling for Decode", "llm_role": "none", "llm_model_used": "DeepSeek-V3", "search_type": "constructive", "novelty_claim": "Staggered Batch Scheduling (SBS) deliberately buffers requests to form optimal execution batches, eliminating internal queuing and parallelization bubbles, and enabling load-aware global allocation for both Prefill and Decode phases.", "components": ["Throughput-Adaptive Interval Control", "Multi-tier State Synchronization Protocol", "Prioritized Batch Allocation Algorithm (PBAA)", "IQR-Aware Lexicographical Decode Scheduling", "Dynamic Capacity Model", "Length-Based Pre-Sorting"], "training_required": false}, "tags": {"methods": ["scheduling_algorithms", "batch_scheduling", "adaptive_control", "state_synchronization", "greedy_algorithm", "bin_packing", "outlier_detection", "lexicographical_optimization", "queuing_theory", "load_balancing", "resource_allocation"], "problems": ["llm_inference_scheduling", "distributed_scheduling", "time_to_first_token_optimization", "throughput_optimization", "gpu_scheduling", "kv_cache_management", "llm_serving_optimization"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "staggered_batch_scheduling", "specific_domain": "llm_inference_scheduling", "llm_coupling": null}, "problem": {"formal_name": "Large Language Model Inference Scheduling in P/D-separated DP+EP Architectures", "short": "LLM Inference Scheduling", "class_": "scheduling", "properties": ["distributed", "P/D-separated", "DP+EP architecture", "high internal synchronization costs", "non-preemptive prefill", "discrete batch processing", "load imbalance", "co-optimization (TTFT and throughput)"], "scale": "large-scale distributed LLM inference (trillion parameters, 32+ GPUs, 0-64K tokens)"}, "lineage": {"direct_ancestors": [{"paper": "Splitwise (Patel et al., 2024)", "relationship": "pioneered P/D disaggregation architecture"}, {"paper": "Llumnix (Sun et al., 2024)", "relationship": "addressed dynamic scheduling and TTFT in LLM serving"}], "closest_prior_work": "Llumnix (Sun et al., 2024)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["integrate with more advanced flow control mechanisms", "extend to other hybrid parallelism architectures beyond DP+EP", "dynamically adapt to varying network latencies", "optimize for other metrics like cost or energy efficiency"], "transferable_to": ["distributed deep learning training", "cloud resource scheduling for microservices", "high_performance_computing_job_scheduling"], "open_weaknesses": ["performance under extremely bursty or unpredictable workloads", "scalability limits of global state and feedback system in ultra-large clusters", "potential overhead of multi-tier synchronization protocol in certain failure scenarios", "greedy allocation might not be globally optimal in all scenarios"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_6", "front_status": "emerging", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.61, "experiments": {"benchmarks": ["DeepSeek-V3 inference workload (0-3K input tokens)", "DeepSeek-V3 inference workload (3K-64K input tokens)"], "baselines": ["Immediate Dispatch Baseline"], "hardware": "NVIDIA H800 GPUs", "instance_sizes": [3000, 16000, 64000]}, "results": {"vs_baselines": {"Immediate Dispatch Baseline": "TTFT reduced by 30-40%, overall throughput improved by 15-20%. Prefill throughput boosted by 12.9-22.8%, Decode throughput improved by ~15%. Prefill Chunk Utilization increased from 52% to 88%. Decode KV Cache load variance (±1σ range) reduced by 40%."}, "scalability": "SBS maintains performance advantage under high variance in input lengths and is presented as a scalable path for next-generation inference systems.", "statistical_rigor": "Average TTFT, QPS, and chunk utilization are reported across various load levels. Standard deviation is used for KV Cache load distribution.", "limitations_acknowledged": []}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2512.15705", "arxiv_url": "https://arxiv.org/abs/2512.15705", "title": "Dynamic Rebatching for Efficient Early-Exit Inference with DREX", "authors": ["Xuting"], "abstract": "", "published_date": "2025-12-17", "affiliations": "Microsoft Research, University of Pennsylvania", "category": "OR for Generative AI", "relevance": {"methodological": 5, "problem": 8, "inspirational": 6}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "While not relevant to our primary evolutionary search focus, this is critical for our 'GPUSched' and 'OR for AI systems' workstreams. It invalidates static batching assumptions for dynamic models and provides a concrete cost model for rebatching overhead that we must incorporate into our scheduling formulations."}, "brief": "DREX introduces a system for 'Early-Exit' LLMs that dynamically splits and regroups batches at intermediate layers, using a cost-benefit heuristic (Adaptive Rebatching Threshold) to decide when rebatching is profitable versus forcing execution. Results are solid (2-12% throughput gain on A100s) and backed by real system measurements, not just simulations. The key takeaway for us is the analytical model for rebatching overhead (Eq. 6)—we can lift this constraint directly into our integer programming formulations for the GPUSched project to accurately model the trade-off between batch fragmentation and compute savings. Essential reading only for the serving optimization sub-team; irrelevant for the core evolutionary search group.", "methodology": {"core_method": "Dynamic Rebatching with copy-free rebatching buffer and SLA-aware scheduler", "llm_role": "inference_target", "llm_model_used": null, "search_type": "improvement", "novelty_claim": "DREX is the first serving framework to deeply examine and operationalize early exits into end-to-end LLM serving pipelines through Dynamic Rebatching.", "components": ["Dynamic Rebatching", "copy-free rebatching buffer", "EE and SLA-aware scheduler", "memory-efficient state-copying", "Adaptive Rebatching Threshold", "SLA-aware forced flushing"], "training_required": false}, "tags": {"methods": ["early_exit_llms", "dynamic_rebatching", "llm_inference_serving", "continuous_batching", "kv_caching", "virtual_memory_management", "attention_kernels", "sla_aware_scheduling", "adaptive_thresholding", "state_copying"], "problems": ["llm_serving_optimization", "inference_acceleration", "resource_utilization", "latency_throughput_optimization", "quality_of_service_guarantees"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "sarathi_serve", "specific_domain": "llm_serving_optimization", "llm_coupling": null}, "problem": {"formal_name": "Efficient Early-Exit Inference with Large Language Models", "short": "EE LLM Inference", "class_": "inference_acceleration", "properties": ["early_exit", "batched", "dynamic_rebatching", "kv_cache_management", "sla_aware"], "scale": "Llama-EE-13B/70B, Qwen-EE-14B models, batch sizes 4-8, up to 4000 tokens"}, "lineage": {"direct_ancestors": [{"paper": "Sarathi-Serve [3]", "relationship": "built on top of"}, {"paper": "vLLM [22]", "relationship": "scheduler builds upon continuous batching from"}, {"paper": "Apparate [11]", "relationship": "adapts EE ramp architecture from"}, {"paper": "Miao et al. [31]", "relationship": "improves upon grouped exit policies from"}], "closest_prior_work": "An efficient inference framework for early-exit large language models (Miao et al. [31])", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Improve EE models to better align confidence scores with end-to-end quality metrics", "Apply Dynamic Rebatching to other dynamic computation LLMs (e.g., Mixture-of-Depths, ShortGPT)", "Integrate with speculative decoding for further inference acceleration", "Extend DREX to distributed LLM serving environments"], "transferable_to": ["Other early-exit deep neural networks beyond LLMs", "LLMs with Mixture-of-Depths or other dynamic layer execution", "Systems using speculative decoding for inference acceleration", "Any batched inference system where individual items might complete at different stages"], "open_weaknesses": ["Mismatch between EE confidence scores and end-to-end quality metrics (e.g., BERT score)", "Trade-off between throughput and tail Request Completion Time (RCT)", "Overhead of rebatching, especially for smaller models, despite Adaptive Rebatching Threshold (ART)", "Potential for request starvation under specific, low-probability conditions (mitigated by SLA-aware flushing)"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_0", "front_status": "emerging", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.24, "experiments": {"benchmarks": ["HELM", "CNN/Daily Mail dataset"], "baselines": ["Latency-only (Apparate)", "Consensus (Miao et al.)", "Majority", "Greedy", "Non-EE"], "hardware": "NVIDIA A100 node with 80GB VRAM, NVIDIA H200 node with 141GB VRAM", "instance_sizes": [4, 8]}, "results": {"vs_baselines": {"Non-EE": "2-12% throughput improvement on Llama-EE-70B", "Latency-only": "2-10.3% throughput improvement", "Consensus": "2-10.3% throughput improvement", "Majority": "2-10.3% throughput improvement", "Greedy": "lower P95 confidence score (96% worse) despite higher throughput", "Adaptive Rebatching Threshold": "9% throughput improvement over naive rebatching", "SLA-aware scheduling": "11.4% throughput improvement over Consensus (no SLA pressure), up to 58.4% average RCT increase (high SLA pressure)", "Memory-efficient state-copying": "5.7% average reduction in CUDA memory operations, up to 18.3% with Greedy policy"}, "scalability": "The gain for early exiting is more significant in larger models, leading to a lower Adaptive Rebatching Threshold and more EE opportunities. Rebatching consistently increases throughput by 4.5–8.6% across various batch sizes with 2 exits.", "statistical_rigor": "Reports P95 confidence scores, average and tail Request Completion Times (RCT), and memory operations from 3 samples. Implies multiple runs for averages and consistency claims.", "limitations_acknowledged": ["BERT score tends to decrease as a model makes more early exits, even with high confidence, highlighting a limitation in current EE models and configurations.", "Dynamic Rebatching introduces a trade-off with request completion time, as requests are not processed on a FCFS basis but dynamically rearranged to optimize for throughput."]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2512.13857", "arxiv_url": "https://arxiv.org/abs/2512.13857", "title": "EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery", "authors": ["Kamer"], "abstract": "", "published_date": "2025-12-17", "affiliations": "aiXplain Inc", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 8, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper introduces a representational shift (DAG of alternatives vs. single program) that directly solves the credit assignment problem in LLM-based evolution—a core bottleneck in our AlgoEvo and AlphaEvolve work. It effectively implements a structural Process Reward Model without training a separate value network."}, "brief": "EvoLattice replaces the standard 'overwrite-based' evolution of monolithic programs with a persistent DAG where each node holds multiple alternative implementations, evaluating all valid combinatorial paths to compute fine-grained performance statistics for every micro-operator. The results are strong: it outperforms AlphaEvolve and FunSearch styles on NAS-Bench-Zero by explicitly preserving diversity and enabling surgical, data-driven pruning rather than blind mutation. The critical takeaway is the 'alternative-level statistic' mechanism: by aggregating performance across all paths a component participates in, they generate a high-fidelity signal that tells the LLM exactly which lines of code are working, effectively solving the sparse reward problem in code evolution. We should immediately discuss refactoring our AlgoEvo representation to support this multi-alternative graph structure, as it maximizes signal extraction per LLM call.", "methodology": {"core_method": "LLM-guided evolutionary search on a multi-alternative Quality-Diversity Directed Acyclic Graph (DAG) representation with alternative-level performance statistics and deterministic self-repair", "llm_role": "evolutionary_search", "llm_model_used": "GPT-OSS-20B", "search_type": "hybrid", "novelty_claim": "EvoLattice introduces a framework that represents an entire population of candidate programs or agent behaviors within a single directed acyclic graph, where each node stores multiple persistent alternatives and every valid path through the graph defines a distinct executable candidate.", "components": ["Directed Acyclic Graph (DAG) representation", "multi-alternative nodes", "path enumeration/sampling", "memoized execution", "alternative-level performance statistics computation", "LLM-guided mutation", "deterministic self-repair pipeline", "structured hypothesis generation", "two-level prompting architecture"], "training_required": false}, "tags": {"methods": ["llm_evolutionary_search", "program_synthesis", "evolution_of_heuristics", "quality_diversity", "directed_acyclic_graph", "memoization", "llm_code_generation", "llm_in_the_loop", "llm_as_heuristic", "self_improving_search"], "problems": ["program_synthesis", "agent_design", "neural_architecture_search_proxy_discovery", "optimizer_discovery"], "contribution_type": ["new_method", "framework", "sota_result"], "framework_lineage": "evolattice", "specific_domain": null, "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "LLM-Guided Program and Agent Synthesis", "short": "null", "class_": "llm_evolutionary_search", "properties": ["multi-alternative", "graph-based", "persistent_population", "quality-diversity_implicit", "self-repairing", "training-free"], "scale": "384 architectures (for NAS-Bench-Suite-Zero)"}, "lineage": {"direct_ancestors": [{"paper": "FunSearch", "relationship": "departs from single-path overwrite-based evolution"}, {"paper": "AlphaEvolve", "relationship": "departs from single-path overwrite-based evolution"}, {"paper": "MAP-Elites", "relationship": "realizes quality-diversity implicitly and internally, unlike external archives"}], "closest_prior_work": "FunSearch", "novelty_type": "paradigm_shift"}, "extensions": {"next_steps": ["Develop a principled theory of optimal path sampling, importance sampling, or surrogate modeling for path evaluation", "Design adaptive graph-regularization schemes to control structural complexity", "Implement novelty-aware pruning strategies", "Explore sparsity-promoting evolution policies"], "transferable_to": ["Multi-agent systems evolution", "General program synthesis tasks beyond proxies and optimizers", "Other automated algorithm design problems", "Symbolic regression"], "open_weaknesses": ["Scalability pressure from combinatorial growth of executable paths", "Cost of evaluating all paths as the number of nodes and alternatives increases", "Potential for excessively large structures in long evolutionary runs", "Tension between exploration and representational compactness during pruning"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.31, "experiments": {"benchmarks": ["NAS-Bench-Suite-Zero (Medium)", "Training-free optimizer update rule discovery"], "baselines": ["Spectral Energy (mean)", "Covariance Trace", "Spectral + Covariance (fixed)", "ZeroLM (Chen et al., 2025)", "LPZero (Dong et al., 2024)", "FunSearch-style proxy generation", "AlphaEvolve-style diff evolution", "ShinkaEvolve-like parent sampling", "SGD (-g)", "SignSGD (Bernstein et al., 2018)", "Lion (Chen et al., 2023)", "Curvature-normalized (-g/√h)", "Sign + curvature (linear blend)", "Sign + curvature (nonlinear gating)"], "hardware": "GPUs (FP16 inference)", "instance_sizes": [384]}, "results": {"vs_baselines": {"Spectral Energy (mean)": "+158.3% ρ on NAS-Bench-Suite-Zero", "Covariance Trace": "+121.4% ρ on NAS-Bench-Suite-Zero", "Spectral + Covariance (fixed)": "+93.75% ρ on NAS-Bench-Suite-Zero", "ZeroLM (Chen et al., 2025)": "+72.2% ρ on NAS-Bench-Suite-Zero", "LPZero (Dong et al., 2024)": "+55.0% ρ on NAS-Bench-Suite-Zero", "FunSearch-style proxy generation": "+72.2% ρ on NAS-Bench-Suite-Zero", "AlphaEvolve-style diff evolution": "+40.9% ρ on NAS-Bench-Suite-Zero", "ShinkaEvolve-like parent sampling": "+29.1% ρ on NAS-Bench-Suite-Zero", "SGD (-g)": "+89.4% score on Optimizer Discovery", "SignSGD (Bernstein et al., 2018)": "+50.3% score on Optimizer Discovery", "Lion (Chen et al., 2023)": "+44.6% score on Optimizer Discovery", "Curvature-normalized (-g/√h)": "+40.8% score on Optimizer Discovery", "Sign + curvature (linear blend)": "+31.8% score on Optimizer Discovery", "Sign + curvature (nonlinear gating)": "+23.9% score on Optimizer Discovery"}, "scalability": "Memoization of shared subgraphs ensures runtime comparable to single-path methods, achieving multiplicative expansion of effective population size without linear increases in evaluation cost, despite combinatorial growth of executable paths.", "statistical_rigor": "Spearman rank correlation (ρ) reported with confidence intervals (CI Half-Width) over 384 architectures in Phase B; lower variance and faster stabilization observed.", "limitations_acknowledged": ["Combinatorial growth of executable paths creates scalability pressure", "Optimal path sampling, importance sampling, or surrogate modeling for path evaluation is unexplored", "Long evolutionary runs may produce excessively large structures", "Adaptive graph-regularization, novelty-aware pruning, or sparsity-promoting evolution policies are needed to control complexity"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2512.11306", "arxiv_url": "https://arxiv.org/abs/2512.11306", "title": "RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training", "authors": ["Tianyuan"], "abstract": "", "published_date": "2025-12-15", "affiliations": "Alibaba Group, Hong Kong University of Science and Technology, UIUC", "category": "OR for Generative AI", "relevance": {"methodological": 7, "problem": 8, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "This is a high-quality reference for our 'OR for AI systems' track, demonstrating how to formulate and solve scheduling for stochastic, multi-phase AI workloads. The engineering patterns for handling long-tail latency are directly transferable to optimizing our distributed evolutionary search infrastructure."}, "brief": "ROLLMUX proposes a cluster scheduler that interleaves the rollout and training phases of multiple RL jobs to eliminate the idle 'dependency bubbles' inherent in synchronous on-policy learning. Tested on a production-scale cluster (328 H800s + 328 H20s), they demonstrate a 1.84x cost reduction with real-world traces, validating the approach beyond simulation. The most stealable insight is 'long-tail migration': dynamically detecting straggler requests during generation and migrating them to a small subset of nodes, freeing the main cluster to proceed immediately. We should implement this logic in our AlgoEvo evaluation loops to mitigate stochastic evaluation times.", "methodology": {"core_method": "Two-tier cluster scheduling framework with inter-group conservative stochastic planning and intra-group round-robin orchestration", "llm_role": "none", "llm_model_used": null, "search_type": "constructive", "novelty_claim": "ROLLMUX is the first phase-centric co-scheduling system for stochastic RL post-training jobs.", "components": ["co_execution_group_abstraction", "inter_group_scheduler", "intra_group_scheduler", "long_tail_migration", "phase_centric_control_model", "topology_aware_model_synchronization", "warm_start_mechanism"], "training_required": false}, "tags": {"methods": ["cluster_scheduling", "resource_allocation", "time_multiplexing", "co_scheduling", "inter_group_scheduling", "intra_group_scheduling", "round_robin_scheduling", "conservative_stochastic_planning", "long_tail_migration", "warm_start_mechanism", "phase_centric_control", "topology_aware_model_synchronization"], "problems": ["rl_post_training", "llm_post_training", "cluster_scheduling", "resource_management", "dependency_bubble_mitigation", "workload_heterogeneity", "stochastic_workloads", "context_switching_overhead", "cross_cluster_synchronization"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "roll", "specific_domain": "rl_post_training", "llm_coupling": null}, "problem": {"formal_name": "Cross-Cluster Scheduling for Disaggregated Reinforcement Learning Post-Training", "short": "null", "class_": "cluster_scheduling", "properties": ["disaggregated", "on_policy", "stochastic", "heterogeneous", "multi_tenant", "phase_level"], "scale": "328 H20 and 328 H800 GPUs, 3B-32B LLM models, up to 2000 concurrent jobs"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2506.06122", "relationship": "built atop"}], "closest_prior_work": "Heterogeneity-Aware cluster scheduling policies for deep learning workloads (Gavel+) arXiv:2006.09927", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["explore_dynamic_training_pool_scaling", "investigate_more_advanced_inter_group_scheduling_policies", "integrate_with_other_intra_job_optimizations_like_speculative_decoding"], "transferable_to": ["llm_inference_serving_with_prefill_decode_disaggregation", "general_distributed_computing_with_phase_dependent_resource_bottlenecks", "multi_tenant_cloud_environments_with_heterogeneous_hardware"], "open_weaknesses": ["conservative_planning_in_inter_group_scheduling_may_be_overly_pessimistic", "lack_of_dynamic_training_pool_scaling_limits_flexibility", "optimality_proof_for_round_robin_assumes_non_overloaded_groups", "potential_overhead_of_phase_centric_control_model_beyond_context_switching"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.73, "experiments": {"benchmarks": ["DeepMath-103K", "MathOrz57K", "Microsoft Philly multi-tenant training cluster trace"], "baselines": ["Solo Disaggregation", "veRL [41]", "Gavel+ [29]", "Offline Optimal", "Random", "Greedy (Most-Idle)"], "hardware": "NVIDIA H20 GPUs, NVIDIA H800 GPUs, 400 Gbps InfiniBand (intra-cluster), 20 Gbps Ethernet (inter-cluster)", "instance_sizes": [2, 3, 5, 8, 16, 200, 300, 328, 2000]}, "results": {"vs_baselines": {"Solo Disaggregation": "1.84x cost reduction, 24.4% rollout bubble reduction, 43.1% training bubble reduction", "veRL [41]": "1.38x cost reduction, 7.87x-8.33x faster sync (single-node), 2.62x-2.75x faster sync (multi-node)", "Gavel+ [29]": "1.14x-1.85x cost efficiency improvement", "Offline Optimal": "1.01x-1.12x cost, 100% SLO attainment", "Random": "1.72x-2.00x higher cost, 37-58% SLO attainment", "Greedy (Most-Idle)": "1.38x-1.89x higher cost, 42-61% SLO attainment"}, "scalability": "Achieves near-linear scalability for scheduling decisions (591ms for 2000 jobs) and robustly scales performance benefits with increasing cluster size and job heterogeneity.", "statistical_rigor": "Evaluated with production workload traces and large-scale simulations; SLOs sampled uniformly; scheduler optimality compared to brute-force search.", "limitations_acknowledged": ["None explicitly acknowledged in a dedicated section."]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2512.09963", "arxiv_url": "https://arxiv.org/abs/2512.09963", "title": "GoodSpeed: Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference", "authors": ["Phuong"], "abstract": "", "published_date": "2025-12-14", "affiliations": "The University of Sydney, Kyung Hee University", "category": "OR for Generative AI", "relevance": {"methodological": 5, "problem": 7, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "This is a textbook application of network utility maximization (Stolyar's algorithm) to LLM inference. While not methodologically novel, it is a clean execution of 'OR for AI systems' that directly addresses our interest in stochastic resource allocation for LLM serving (GPUSched)."}, "brief": "GoodSpeed uses gradient-based scheduling to dynamically allocate token generation budgets across distributed draft servers, maximizing a logarithmic utility function to balance throughput and fairness. The authors provide rigorous fluid sample path analysis to prove convergence, backed by experiments on H100/L4 clusters, although the baselines (fixed/random allocation) are relatively weak. The most useful takeaway is the mechanism of using exponentially smoothed acceptance rate estimates to drive real-time control in a stochastic system—a robust pattern we should adopt for our own stochastic resource allocation and RobustMAS projects.", "methodology": {"core_method": "Gradient-based scheduling algorithm maximizing logarithmic utility for proportional fairness with adaptive speculative decoding", "llm_role": "heuristic_generator, evaluator", "llm_model_used": "Qwen3-14B, Llama3.1-70B-Instruct-AWQ-INT4, Qwen3-0.6B, Qwen3-1.7B, Llama 3.2-1B-Instruct, Llama 3.2-3B-Instruct", "search_type": "improvement", "novelty_claim": "GoodSpeed introduces a novel distributed edge inference architecture for speculative decoding, employing gradient scheduling with smoothed estimates to ensure fair and efficient resource allocation, proven asymptotically optimal via fluid sample path analysis.", "components": ["Central verification server", "Heterogeneous draft servers (SLMs)", "Gradient scheduling algorithm", "Exponential smoothing for acceptance rates", "Exponential smoothing for goodput estimates", "Logarithmic utility function"], "training_required": false}, "tags": {"methods": ["speculative_decoding", "gradient_scheduling", "resource_allocation", "exponential_smoothing", "fluid_model_analysis", "distributed_inference", "llm_in_the_loop"], "problems": ["distributed_llm_inference", "goodput_optimization", "fairness_optimization", "latency_reduction", "edge_inference"], "contribution_type": ["new_method", "framework", "theoretical_result", "empirical_study", "sota_result"], "framework_lineage": null, "specific_domain": "distributed_llm_inference_resource_allocation", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference", "short": "Distributed LLM Inference Goodput Optimization", "class_": "resource_allocation", "properties": ["multi-user", "resource-constrained", "heterogeneous_draft_servers", "dynamic_workloads", "latency-sensitive", "fairness", "distributed_inference"], "scale": "4-8 draft servers, 50-150 max token length, C={6, 16, 20, 24, 28} token budget"}, "lineage": {"direct_ancestors": [{"paper": "Fast inference from transformers via speculative decoding", "relationship": "builds on the core technique of speculative decoding"}, {"paper": "On the asymptotic optimality of the gradient scheduling algorithm for multiuser throughput allocation", "relationship": "adapts gradient scheduling for LLM goodput optimization"}, {"paper": "DSSD: Efficient edge-device deployment and collaborative inference via distributed split speculative decoding", "relationship": "proposes a novel architecture in the space of distributed speculative decoding"}], "closest_prior_work": "DSSD: Efficient edge-device deployment and collaborative inference via distributed split speculative decoding", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Dynamically adjust smoothing parameters (eta, beta) for acceptance rates and goodput estimates", "Explore more complex utility functions beyond logarithmic for diverse fairness objectives", "Integrate with other LLM serving optimizations (e.g., PagedAttention, continuous batching) for further efficiency", "Extend theoretical analysis to more general non-stationary conditions and system capacity variations"], "transferable_to": ["Distributed inference for other large AI models (e.g., multi-modal, diffusion models) in edge environments", "General distributed task scheduling and resource management in heterogeneous edge computing", "Optimizing fair resource allocation in other multi-user, resource-constrained distributed systems"], "open_weaknesses": ["Increased receiving time due to variable drafting lengths, requiring the verification server to wait for the slowest client", "Theoretical convergence relies on assumptions (ergodicity, Lipschitz continuity) that may not hold in all extreme dynamic scenarios", "Fixed smoothing parameters (eta, beta) may not be optimal for all dynamic workloads and require careful tuning", "Potential for further optimization regarding inter-model communication costs and hardware locality in distributed settings"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_3", "front_status": "growing", "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.89, "experiments": {"benchmarks": ["Alpaca", "Awesome-ChatGPT-Prompts", "CNN/DailyMail", "OpenOrca", "Chatbot Arena", "GSM8K", "SPIDER", "HLE"], "baselines": ["Fixed-S", "Random-S"], "hardware": "NVIDIA H100 GPU (verification server), NVIDIA L4 GPU (draft servers)", "instance_sizes": [4, 8, 50, 150, 6, 16, 20, 24, 28]}, "results": {"vs_baselines": {"Fixed-S": "Comparable total wall time, 5% higher verification time, 1.8-2x lower utility", "Random-S": "5-25% higher total wall time, 2-3x lower utility"}, "scalability": "GoodSpeed provides a scalable, fair, and efficient solution for multi-server speculative decoding in distributed LLM inference systems, demonstrating stable utility convergence within 600 iterations.", "statistical_rigor": "Rigorous fluid sample path analysis proves asymptotic optimality. Experimental results show strong alignment between estimated and real goodput with empirical standard deviation confidence bands, and utility convergence within 600 iterations.", "limitations_acknowledged": []}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2512.12476", "arxiv_url": "https://arxiv.org/abs/2512.12476", "title": "HetRL: Efficient Reinforcement Learning for LLMs in Heterogeneous Environments", "authors": ["Yongjun"], "abstract": "", "published_date": "2025-12-13", "affiliations": "Amazon Web Services, ETH Zurich", "category": "OR for Generative AI", "relevance": {"methodological": 6, "problem": 8, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper directly targets your 'GPUSched' and 'Scalability' interests by applying OR-style optimization (GA + SHA) to LLM workflow scheduling. It demonstrates how to effectively utilize heterogeneous/fragmented compute for complex RLHF loops, which is critical for scaling your evolutionary search experiments cost-effectively."}, "brief": "HetRL formulates the scheduling of RLHF workflows (PPO/GRPO) across heterogeneous GPUs and networks as a constrained joint optimization problem, solved via a multi-level search combining Successive Halving and Genetic Algorithms. The authors validate this with 20,000 GPU-hours of experiments, demonstrating 3-9x throughput gains over standard systems like 'verl' in heterogeneous settings. The key takeaway is the hierarchical decomposition of the search space (Task Grouping → Coarse Assignment → Fine-grained Assignment) and the use of SHA to efficiently allocate search budget among candidate configurations. This is directly actionable for your 'GPUSched' project and offers a concrete strategy to scale 'AlgoEvo' runs across cheaper, fragmented GPU resources.", "methodology": {"core_method": "Multi-level search framework with nested successive halving and genetic algorithm with two-level swaps for constrained joint optimization of partitioning and assignment strategies", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "We formulate RL training scheduling in heterogeneous environments as a constrained joint optimization problem and propose a novel scheduling algorithm that decomposes the complex search space with a multi-level search framework and allocates the search budget via successive halving.", "components": ["Multi-level search framework", "Cost model", "Nested successive halving", "Genetic algorithm with two-level swaps", "Profiler", "Load balancer", "Distributed execution engine"], "training_required": false}, "tags": {"methods": ["constrained_optimization", "joint_optimization", "multi_level_search", "successive_halving", "genetic_algorithm", "cost_modeling", "distributed_systems", "resource_scheduling", "load_balancing", "data_parallelism", "pipeline_parallelism", "tensor_parallelism", "graph_partitioning"], "problems": ["llm_post_training", "reinforcement_learning_from_human_feedback", "gpu_scheduling", "heterogeneous_computing", "distributed_deep_learning", "resource_allocation"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "verl", "specific_domain": "llm_rlhf_scheduling", "llm_coupling": null}, "problem": {"formal_name": "Reinforcement Learning Training Scheduling in Heterogeneous Environments", "short": "HetRL Scheduling", "class_": "scheduling", "properties": ["heterogeneous_GPUs", "heterogeneous_networks", "multi_model", "multi_task", "complex_dependencies", "distributed"], "scale": "64 GPUs (24 A100s, 24 L40Ss, 16 L4s) for Qwen-4B, 8B, 14B models"}, "lineage": {"direct_ancestors": [{"paper": "verl (Sheng et al., 2025)", "relationship": "builds system on top of"}, {"paper": "StreamRL (Zhong et al., 2025a)", "relationship": "addresses limitations of"}, {"paper": "Megatron-LM (Shoeybi et al., 2019)", "relationship": "uses as training engine"}, {"paper": "vLLM (Kwon et al., 2023)", "relationship": "uses as serving engine"}, {"paper": "Yuan et al., 2022", "relationship": "applies genetic algorithm from"}, {"paper": "Jamieson & Talwalkar, 2016", "relationship": "applies successive halving from"}], "closest_prior_work": "StreamRL (Zhong et al., 2025a)", "novelty_type": "new_problem"}, "extensions": {"next_steps": ["Integrate more advanced load balancing strategies", "Extend support to other GPU generations, vendors, and networking stacks", "Investigate convergence and potential precision issues with heterogeneous GPUs", "Conduct cost-efficiency comparisons"], "transferable_to": ["General distributed LLM training and serving in heterogeneous environments", "Other multi-model/multi-task AI workflows", "Resource management for other types of distributed heterogeneous compute"], "open_weaknesses": ["Limited hardware and networking stack support (NVIDIA, AWS OFI NCCL/EFA)", "No analysis of RL convergence or precision effects in heterogeneous settings", "Lack of cost-efficiency comparison", "Load balancing gains are slightly lower than some related work"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.44, "experiments": {"benchmarks": ["Qwen-4B", "Qwen-8B", "Qwen-14B", "GSM8k dataset"], "baselines": ["verl (Sheng et al., 2025)", "StreamRL (Zhong et al., 2025a)"], "hardware": "64 GPUs (24 A100s, 24 L40Ss, 16 L4s), 20,000 GPU-hours for evaluation", "instance_sizes": [4, 8, 14, 1024]}, "results": {"vs_baselines": {"verl-Sync": "1.4x to 5.46x throughput increase", "verl-Async": "1.71x to 10.76x throughput increase", "StreamRL-Async": "1.1x to 3.72x throughput increase"}, "scalability": "HetRL effectively leverages heterogeneous GPUs and networks, showing larger performance gains with increased heterogeneity and outperforming homogeneous setups by 1.09-2.0x.", "statistical_rigor": "Extensive evaluation consuming 20,000 GPU-hours; no explicit mention of multiple runs or statistical significance tests.", "limitations_acknowledged": ["Evaluation only uses three NVIDIA GPUs (A100, L40S, L4) and AWS networking stacks; no support for other GPU generations, vendors, or networking stacks.", "Focuses on throughput; does not investigate convergence or potential precision issues with heterogeneous GPUs.", "Does not include a cost-efficiency comparison."]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2512.11270", "arxiv_url": "https://arxiv.org/abs/2512.11270", "title": "A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation", "authors": ["Hong"], "abstract": "", "published_date": "2025-12-12", "affiliations": "Sejong University", "category": "Generative AI for OR", "relevance": {"methodological": 5, "problem": 7, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While the multi-agent decomposition is standard prompt engineering, the specific pipeline for extracting OR/MDP components (Parameters -> Variables -> Constraints) is a clean template for our OR-Bench project. It demonstrates that structural decomposition allows smaller models (Gemma-27B) to handle modeling tasks usually reserved for GPT-4, which is relevant for our scalability goals."}, "brief": "A-LAMP decomposes the translation of natural language task descriptions into executable RL environments via a multi-agent pipeline, separating parameter extraction, variable definition, and constraint formulation before code generation. The results show that this structured approach allows a 27B model to rival GPT-4o on simple tasks, though the benchmarks (e.g., grid-world drone delivery, trivial wireless scheduling) are toy-scale and the RL application is sometimes forced. The primary takeaway is the specific decomposition schema for symbolic modeling: we should steal their granular extraction pipeline (Parameters -> Objectives -> Variables -> Constraints) to improve the reliability of our automated problem instantiation in OR-Bench and AlgoEvo without relying solely on expensive frontier models.", "methodology": {"core_method": "Modular multi-agent LLM framework (A-LAMP) decomposing MDP formulation and policy generation into specialized LLM agents", "llm_role": "Automates MDP modeling, environment code generation, and policy training pipeline", "llm_model_used": "Multiple (GPT-4o, Gemma3-27B)", "search_type": "sampling", "novelty_claim": "A-LAMP decomposes the MDP formulation and policy generation process into specialized LLM agents, constructing precise mathematical representations and corresponding policy structures, yielding more reliable and consistent policies than a single large LLM.", "components": ["Parameter Agent", "Objective Agent", "Variable Agent", "Constraint Agent", "Modeling Agent", "SAR Agent", "Env Agent", "Coding Agent", "Code Executor", "Error Correction Module"], "training_required": true}, "tags": {"methods": ["multi_agent_llm_system", "llm_code_generation", "llm_as_evaluator", "llm_in_the_loop", "program_synthesis", "reinforcement_learning", "deep_q_networks", "mdp_modeling"], "problems": ["automated_mdp_modeling", "automated_policy_generation", "cart_pole", "mountain_car", "wireless_network_scheduling", "drone_delivery", "inventory_management"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": null, "specific_domain": null, "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Automated MDP Modeling and Policy Generation", "short": "Automated RL Pipeline Generation", "class_": "program_synthesis", "properties": ["sequential_decision_making", "dynamic", "uncertain", "from_natural_language", "discrete_actions"], "scale": "4 users, 10 items, 50x50 grid world"}, "lineage": {"direct_ancestors": [{"paper": "AhmadiTeshnizi et al., 2024", "relationship": "incorporates error correction module from OptiMUS"}], "closest_prior_work": "Ma et al., 2023 (EUREKA)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["develop finer-grained coding agents for environment construction and training loop generation", "incorporate adaptive mechanisms for hyperparameter tuning and validation", "integrate domain-informed priors and structured knowledge"], "transferable_to": ["network_resource_scheduling", "industrial_automation", "supply_chain_optimization", "new_rl_tasks_from_natural_language"], "open_weaknesses": ["coding stage fragility due to syntactic or structural mismatches", "lack of adaptive hyperparameter tuning", "limited integration of domain-informed priors and structured knowledge"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.74, "experiments": {"benchmarks": ["Cart-pole", "Mountain-car", "Wireless", "Drone-delivery", "Inventory-management"], "baselines": ["GPT-4o (single-model)", "Gemma3-27B (single-model)", "A-LAMP without error correction (EC)"], "hardware": "null", "instance_sizes": [4, 10, 50]}, "results": {"vs_baselines": {"GPT-4o (single-model)": "A-LAMP (with/without EC) achieves 1.8x to 6x higher policy generation success rates than single GPT-4o across tasks.", "Gemma3-27B (single-model)": "Light A-LAMP (Gemma3-27B) significantly outperforms single Gemma3-27B, achieving up to 3x higher success rates and approaching GPT-4o performance."}, "scalability": "A-LAMP enables reliable policy generation for more complex RL problems than single LLMs.", "statistical_rigor": "Results are reported as success rates (modeling, coding, policy generation) over 20 trials per task, with failure distributions analyzed.", "limitations_acknowledged": ["Coding stage fragility due to syntactic/structural mismatches", "Lack of adaptive hyperparameter tuning", "Limited integration of domain-informed priors and structured knowledge"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2512.10271", "arxiv_url": "https://arxiv.org/abs/2512.10271", "title": "Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters", "authors": ["Shruti"], "abstract": "", "published_date": "2025-12-11", "affiliations": "Virginia Tech, Kuwait University, Northwestern Polytechnical University", "category": "OR for Generative AI", "relevance": {"methodological": 7, "problem": 8, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper directly targets the core problem of your 'GPUSched' project (OR formulations for GPU scheduling) with a hybrid architecture that successfully integrates RL with MILP. It provides a concrete blueprint for overcoming the limitations of pure RL in constraint-heavy environments."}, "brief": "RLTune introduces a hybrid scheduling architecture where an RL agent (PPO) handles dynamic job prioritization based on cluster state, while a MILP solver optimizes the specific job-to-node packing constraints for the top-K jobs. The results are robust, demonstrating a ~25% makespan reduction over Slurm on a physical cluster and significant gains over pure RL baselines on standard traces (Philly, Helios). The critical takeaway is the architectural separation of concerns: delegating 'fuzzy' long-term objectives to RL and 'hard' constraint satisfaction to a symbolic solver. We should evaluate this 'RL-guided Solver' pattern for our `GPUSched` and `EvoCut` projects to improve constraint handling without losing adaptivity.", "methodology": {"core_method": "Proximal Policy Optimization (PPO) with Actor-Critic network for dynamic prioritization coupled with Mixed-Integer Linear Programming (MILP) for multi-resource allocation", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "RLTune uniquely separates and couples RL for proactive prioritization and MILP for multi-dimensional look-ahead allocation, forming a unified hybrid framework that learns without per-application profiling and generalizes across heterogeneous clusters.", "components": ["Feature Building Module", "Feature Sampling Module", "RL Agent (Actor-Critic, PPO)", "MILP-based Allocation Optimizer (CVXPY, GLPK_MI)"], "training_required": true}, "tags": {"methods": ["reinforcement_learning", "proximal_policy_optimization", "actor_critic", "mixed_integer_linear_programming", "feature_engineering", "feature_sampling", "multi_layer_perceptron", "cvxpy", "glpk_mi"], "problems": ["gpu_scheduling", "cloud_scheduling", "resource_allocation", "deep_learning_workload_scheduling", "heterogeneous_gpu_clusters", "job_scheduling"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "rltune", "specific_domain": "deep_learning_workload_scheduling", "llm_coupling": null}, "problem": {"formal_name": "Hybrid Learning and Optimization-Based Dynamic Scheduling for Deep Learning Workloads on Heterogeneous GPU Clusters", "short": "DL Workload Scheduling", "class_": "resource_allocation", "properties": ["heterogeneous", "dynamic", "application_agnostic", "profiling_free", "multi_resource", "multi_tenant", "NP_hard"], "scale": "60k-1.75M jobs, 2490-6.5K GPUs, 552-1.8K nodes"}, "lineage": {"direct_ancestors": [{"paper": "RLScheduler", "relationship": "extends RL-based CPU scheduling to GPU-aware multi-resource scheduling"}, {"paper": "SchedInspector", "relationship": "extends RL-based CPU scheduling to GPU-aware multi-resource scheduling"}], "closest_prior_work": "RLScheduler", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["incorporate improved runtime predictors for enhanced performance", "explore elastic resource modification (GPU type or count) or preemption", "further leverage transfer learning to reduce training overhead"], "transferable_to": ["other compute-intensive workloads beyond deep learning", "multi-tenant cloud environments with diverse resource types", "scheduling problems with dynamic resource availability"], "open_weaknesses": ["lack of support for elastic resource modification or preemption", "performance degradation with certain cross-policy transfer learning scenarios", "scalability of MILP solver under extremely bursty arrivals (requires K parameter adjustment)"]}, "artifacts": {"code_url": "https://github.com/dshruti20/RLTune", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.74, "experiments": {"benchmarks": ["Microsoft Philly", "Helios", "Alibaba PAI"], "baselines": ["Slurm Multifactor Priority", "QSSF", "FIFO", "SJF", "WFP3", "UNICEP", "F1", "RLScheduler", "SchedInspector"], "hardware": "P100, K80, M40 GPUs; Training takes 3-8 hours", "instance_sizes": [256, 1024, 10000]}, "results": {"vs_baselines": {"Overall": "GPU utilization up to 20%, queueing delay up to 81%, JCT up to 70%", "Naive-RLTune": "Pro-RLTune achieves 52.59% improvement in BSLD over naive-RLTune", "RL-FIFO (Philly)": "87.5% reduction in wait time", "F1 (Helios)": "BSLD reduced by at least 5.28%", "SJF (Alibaba)": "BSLD reduced by up to 72.32%", "FIFO (Philly)": "JCT improved by up to 70.68%", "F1 (Philly)": "GPU utilization increased by 13.62%", "SJF (Helios)": "GPU utilization increased by 19.71%", "Slurm (Philly)": "BSLD reduced by 71.54%", "Slurm (Helios)": "BSLD reduced by 81.18%", "QSSF (Philly)": "25% improvement in wait time, 3.25x better performance on BSLD, 48.43% improvement in JCT", "RLScheduler/SchedInspector": "1.2x faster job completions, up to 35% lower waiting times, 20% higher utilization", "Slurm (Real Deployment)": "Makespan reduced by 24.8%, GPU utilization increased by 3.9%, Average wait time reduced by 30.5%"}, "scalability": "Decision latency increases sub-linearly from 7.8 s for 128 jobs to 22.8 s for 1024 jobs, corresponding to 1.26x-1.59x growth as queue doubles.", "statistical_rigor": "Each experiment runs ten times with random sequences of 1,024 jobs, and the average wait time is reported.", "limitations_acknowledged": ["Evaluation does not support elastic resource modification (GPU type or count) or preemption.", "Performance degradation is observed when trained on WFP3 and tested on other policies like SJF and F1."]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2512.09209", "arxiv_url": "https://arxiv.org/abs/2512.09209", "title": "Beyond Algorithm Evolution: An LLM-Driven Framework for the Co-Evolution of Swarm Intelligence Optimization Algorithms and Prompts", "authors": ["Shipeng"], "abstract": "", "published_date": "2025-12-10", "affiliations": "Peking University", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 8, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper directly implements 'Evolving the Evolver' by co-evolving prompt templates alongside code, a key missing piece in our AlgoEvo stack. The specific fitness metric for prompts (algorithm performance delta) is a concrete, transferable heuristic that addresses our bottleneck with complex constraints."}, "brief": "The authors introduce a co-evolutionary framework where both the optimization algorithm (Fireworks Algorithm operators) and the prompt templates used to generate them are evolved simultaneously by the LLM. The results demonstrate a massive performance jump on constrained Aircraft Landing problems (from ~56% with FunSearch to 100% with their method), suggesting that static prompts are a primary failure mode for complex OR constraints. The critical takeaway is their prompt fitness function: evaluating a prompt template based on the *performance improvement* (`child - parent`) of the code it generates, rather than absolute performance. We should immediately implement this 'prompt-delta' fitness signal in AlgoEvo to automate our prompt engineering loop.", "methodology": {"core_method": "Collaborative evolution of Fireworks Algorithm operators and prompt templates, driven by a single LLM", "llm_role": "evolutionary_search", "llm_model_used": "GPT-4o-mini, Qwen3-32B, GPT-5", "search_type": "hybrid", "novelty_claim": "This paper proposes a novel framework for the collaborative evolution of both swarm intelligence algorithms and guiding prompts using a single LLM.", "components": ["Swarm intelligence algorithm evolution (FWA operators)", "Prompt template evolution", "Prompt template evaluation method", "Algorithm selection", "Individual generation", "Task evaluation", "Algorithm update"], "training_required": false}, "tags": {"methods": ["llm_evolutionary_search", "evolution_of_heuristics", "program_synthesis", "llm_prompt_optimization", "swarm_intelligence", "metaheuristics", "llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "fireworks_algorithm"], "problems": ["combinatorial_optimization", "aircraft_landing_problem", "equitable_partition_problem", "flow_shop_scheduling", "uncapacitated_p_median_problem"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": null, "specific_domain": "combinatorial_optimization", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "LLM-Driven Framework for the Co-Evolution of Swarm Intelligence Optimization Algorithms and Prompts", "short": "LLM-SIOA-PE", "class_": "llm_evolutionary_search", "properties": ["NP-hard", "combinatorial_optimization", "multi-population"], "scale": "various combinatorial optimization problem instances"}, "lineage": {"direct_ancestors": [{"paper": "EoH", "relationship": "extends the concept of algorithm evolution"}, {"paper": "Funsearch", "relationship": "extends the concept of algorithm evolution"}, {"paper": "Reevo", "relationship": "combines with prompt evolution"}], "closest_prior_work": "EoH", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Explore other swarm intelligence algorithms for co-evolution", "Investigate multi-objective co-evolution", "Apply to more complex, real-world combinatorial optimization problems", "Develop more sophisticated prompt evaluation metrics"], "transferable_to": ["Other metaheuristic paradigms (e.g., genetic algorithms, simulated annealing)", "Other types of optimization problems (e.g., continuous optimization, mixed-integer programming)", "Automated algorithm configuration"], "open_weaknesses": ["Computational cost of LLM interactions", "Sensitivity to initial prompt design", "Lack of theoretical guarantees for evolved algorithms", "Interpretability of the co-evolutionary process and evolved components"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.86, "experiments": {"benchmarks": ["CO-Benchmark", "Aircraft Landing Problem", "Equitable Partition Problem", "Flow Shop Scheduling Problem", "Uncapacitated p-Median Problem"], "baselines": ["EoH", "Funsearch", "Reevo", "PSO", "DE"], "hardware": "", "instance_sizes": []}, "results": {"vs_baselines": {"EoH": "Ours outperformed EoH by a large margin (e.g., 105.34% vs 43.04% on Aircraft Landing with GPT-4o-mini).", "ReEvo": "Ours outperformed ReEvo by a large margin (e.g., 105.34% vs 56.04% on Aircraft Landing with GPT-4o-mini).", "Funsearch": "Ours outperformed Funsearch by a large margin (e.g., 105.34% vs 56.04% on Aircraft Landing with GPT-4o-mini), but was competitive on Flow Shop Scheduling.", "PSO": "Ours significantly outperformed PSO (e.g., 100% vs 37.56% on Equitable Partition).", "DE": "Ours significantly outperformed DE (e.g., 100% vs 25.62% on Equitable Partition)."}, "scalability": "Maintains robust performance across diverse problem instances and scales, effectively leveraging more powerful LLMs for further gains.", "statistical_rigor": "Each experiment was run 5 times, and the best algorithm's performance was reported.", "limitations_acknowledged": []}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2512.07898", "arxiv_url": "https://arxiv.org/abs/2512.07898", "title": "MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement", "authors": ["Hongwei"], "abstract": "", "published_date": "2025-12-05", "affiliations": "ZTE", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 6, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "The theoretical proof that a minimal batch size (M=2) maximizes expected gain under fixed budgets directly challenges our current population sizing strategies for local search. Additionally, their 'segment-level integration' mechanism offers a concrete blueprint for a semantic crossover operator in AlgoEvo."}, "brief": "MARINE proposes a multi-agent framework that iteratively refines a single 'reference trajectory' by generating small batches of candidates, verifying logical/factual conflicts, and merging superior segments rather than regenerating the whole chain. Results are impressive, with an 80B model matching 1000B baselines on retrieval tasks, backed by a theoretical derivation showing that batch size M=2 is optimal for fixed-budget refinement. The critical takeaway is the 'conflict-aware meta-verification' and segment merger, which functions effectively as a process-reward-guided mutation operator. We should immediately test the M=2 configuration in our evolutionary loops and consider adapting their merger logic to replace random crossover in our code generation agents.", "methodology": {"core_method": "Multi-Agent Recursive IN-context Enhancement (MARINE) framework for iterative refinement of a persistent reference trajectory via a theoretically-grounded refinement operator", "llm_role": "trajectory_generator, evaluator, refiner", "llm_model_used": "DeepSeek-V3.2, Qwen3-next-80B-A3B", "search_type": "improvement", "novelty_claim": "MARINE reconceptualizes test-time reasoning as iterative refinement of a persistent reference trajectory, fundamentally departing from conventional one-shot or multi-sample paradigms.", "components": ["layered multi-agent architecture", "structured trajectory representation", "conflict-aware meta-verification", "segment-level integration", "batch-size optimization"], "training_required": false}, "tags": {"methods": ["multi_agent_system", "iterative_refinement", "in_context_learning", "llm_as_refiner", "llm_as_evaluator", "llm_as_generator", "theoretical_analysis", "batch_size_optimization", "conflict_aware_meta_verification", "structured_trajectory_representation"], "problems": ["llm_reasoning_enhancement", "multi_hop_retrieval", "parameter_efficient_reasoning"], "contribution_type": ["new_method", "sota_result", "theoretical_result", "framework", "empirical_study"], "framework_lineage": "marine", "specific_domain": "multi_hop_retrieval", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "LLM Test-Time Reasoning Enhancement", "short": "LLM-TTE", "class_": "llm_reasoning_enhancement", "properties": ["iterative", "in_context", "multi_agent", "parameter_efficient", "test_time_inference"], "scale": "685B and 80B parameter LLMs"}, "lineage": {"direct_ancestors": [{"paper": "Lu et al., 2025", "relationship": "enhances conflict-aware meta-verification mechanism from"}], "closest_prior_work": "Madaan et al., 2023 (Self-Refine)", "novelty_type": "paradigm_shift"}, "extensions": {"next_steps": ["integrate MARINE outputs for post-training alignment and optimization workflows", "develop robust safeguards against misinformation propagation", "implement continuous monitoring for failure modes", "define explicit constraints for high-stakes applications"], "transferable_to": ["diverse complex reasoning tasks", "reliability_sensitive_applications", "other LLM-based agent systems"], "open_weaknesses": ["diminishing returns with increasing refinement rounds", "potential for misinformation propagation", "vulnerability to strategic manipulation"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.36, "experiments": {"benchmarks": ["BrowserComp-ZH"], "baselines": ["Tongyi DeepResearch", "OpenAI DeepResearch", "Kimi-K2", "Best-of-N (BoN)", "Self-Refine", "Chain-of-Thought (CoT)", "pass@N"], "hardware": "null", "instance_sizes": [685, 80]}, "results": {"vs_baselines": {"Tongyi DeepResearch": "-0.7% on BrowserComp-ZH (685B)", "OpenAI DeepResearch": "+3.1% on BrowserComp-ZH (685B)", "Kimi-K2": "+17.2% on BrowserComp-ZH (685B); matches 80B model performance", "Best-of-N (BoN)": "+10.7% on BrowserComp-ZH (685B); +5.2% (80B)", "Self-Refine": "+5.5% on BrowserComp-ZH (685B); +16.9% (80B)", "Chain-of-Thought (CoT)": "+20.0% on BrowserComp-ZH (685B); +18.0% (80B)", "pass@N": "-10.4% on BrowserComp-ZH (685B, K=4); -4.9% (80B, K=4)"}, "scalability": "An 80B-parameter model augmented with MARINE matches the performance of standalone 1000B-parameter agents, reducing parameter requirements by over an order of magnitude; diminishing returns are observed with increasing refinement rounds and larger batch sizes.", "statistical_rigor": "All reported results are obtained by averaging over three independent samples to mitigate stochastic fluctuations.", "limitations_acknowledged": ["potential misinformation propagation", "strategic manipulation"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2512.03762", "arxiv_url": "https://arxiv.org/abs/2512.03762", "title": "RoCo: Role-Based LLMs Collaboration for Automatic Heuristic Design", "authors": ["Jiawei"], "abstract": "", "published_date": "2025-12-04", "affiliations": "South China University of Technology", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 8, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper proposes a Multi-Agent Evolutionary Search architecture that directly competes with our AlgoEvo and MASPRM projects. It provides a concrete implementation of replacing standard mutation operators with a structured multi-agent debate loop, which we need to benchmark against."}, "brief": "RoCo replaces standard evolutionary mutation operators with a 4-agent collaboration loop (Explorer, Exploiter, Critic, Integrator) that iteratively refines heuristics and accumulates long-term reflection memory across generations. While the empirical gains over ReEvo are marginal (often <1%) and likely expensive in token cost, the architecture successfully demonstrates how to embed structured multi-agent reasoning into the evolutionary loop to stabilize black-box search. The key takeaway is their Long-term Reflection mechanism, which aggregates critic feedback into a persistent memory buffer to guide future mutations—a technique we should immediately test to improve sample efficiency in AlgoEvo.", "methodology": {"core_method": "Multi-Agent Role-Based System (RoCo) for Automatic Heuristic Design (AHD) integrated into an Evolutionary Program Search (EoH) framework", "llm_role": "evolutionary_search", "llm_model_used": "GPT-4o-mini", "search_type": "hybrid", "novelty_claim": "RoCo, a novel Multi-Agent Role-Based System, coordinates four specialized LLM-guided agents—explorer, exploiter, critic, and integrator—to collaboratively generate high-quality heuristics.", "components": ["Explorer agent", "Exploiter agent", "Critic agent", "Integrator agent", "Multi-round collaboration", "Short-term reflection", "Long-term reflection", "Memory-guided elite mutation", "EoH framework (Exploration Operators, Modification Operators)"], "training_required": false}, "tags": {"methods": ["llm_evolutionary_search", "multi_agent_system", "llm_as_heuristic", "llm_as_evaluator", "llm_code_generation", "evolution_of_heuristics", "program_synthesis", "reflection", "genetic_algorithm", "ant_colony_optimization", "guided_local_search"], "problems": ["automatic_heuristic_design", "combinatorial_optimization", "tsp", "cvrp", "orienteering_problem", "multiple_knapsack_problem", "bin_packing"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "eoh", "specific_domain": null, "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Automatic Heuristic Design for Combinatorial Optimization Problems", "short": "AHD for COPs", "class_": "llm_evolutionary_search", "properties": ["NP-hard", "white-box", "black-box"], "scale": "20-1000 nodes/items"}, "lineage": {"direct_ancestors": [{"paper": "Evolution of Heuristics", "relationship": "extends population-based architecture from"}, {"paper": "LLM-based Multi-Agent Systems", "relationship": "draws inspiration for collaborative agent framework from"}, {"paper": "Multi-agent debate systems", "relationship": "draws inspiration for critic and integrator design from"}], "closest_prior_work": "Evolution of Heuristics", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["applying_roco_to_more_np_hard_problems", "applying_roco_to_continuous_optimization", "applying_roco_to_mixed_integer_optimization", "exploring_real_world_applications_in_logistics", "exploring_real_world_applications_in_scheduling"], "transferable_to": ["continuous_optimization", "mixed_integer_optimization", "logistics_planning", "job_scheduling"], "open_weaknesses": ["computational_cost_of_multi_agent_interaction", "dependency_on_prompt_engineering_quality", "generalization_to_entirely_new_problem_classes", "scalability_of_multi_agent_system_with_more_agents"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.04, "experiments": {"benchmarks": ["Traveling Salesman Problem (TSP)", "Capacitated Vehicle Routing Problem (CVRP)", "Orienteering Problem (OP)", "Multiple Knapsack Problem (MKP)", "Offline Bin Packing Problem (BPP)"], "baselines": ["EoH", "ReEvo", "HSEvo", "MCTS-AHD", "DeepACO", "ACO", "NeuOpt", "GNNGLS", "KGLS", "KGLS-ReEvo", "KGLS-MCTS-AHD"], "hardware": "null", "instance_sizes": [20, 50, 100, 200, 500, 700, 1000]}, "results": {"vs_baselines": {"White-box TSP-50 vs ReEvo": "-0.004", "White-box TSP-100 vs MCTS-AHD": "+0.063", "White-box TSP-200 vs MCTS-AHD": "+0.292", "White-box OP-50 vs MCTS-AHD": "+0.066", "White-box OP-100 vs ReEvo": "-0.492", "White-box OP-200 vs ReEvo": "-0.985", "White-box CVRP-50 vs ReEvo": "-0.129", "White-box CVRP-100 vs DeepACO": "-0.094", "White-box CVRP-200 vs DeepACO": "+0.205", "White-box MKP-100 vs ReEvo": "+0.210", "White-box MKP-200 vs ReEvo": "+0.903", "White-box MKP-500 vs ReEvo": "+4.648", "White-box BPP-500 vs MCTS-AHD": "-0.015", "White-box BPP-700 vs MCTS-AHD": "-0.063", "White-box BPP-1000 vs MCTS-AHD": "+0.032", "Black-box TSP-50 vs ReEvo": "+0.070", "Black-box TSP-100 vs ReEvo": "+0.148", "Black-box TSP-200 vs ReEvo": "+0.166", "Black-box OP-50 vs ReEvo": "-0.071", "Black-box OP-100 vs EoH": "-0.077", "Black-box OP-200 vs HsEvo": "-4.842", "Black-box CVRP-50 vs MCTS-AHD": "-0.010", "Black-box CVRP-100 vs EoH": "-0.026", "Black-box CVRP-200 vs MCTS-AHD": "-0.023", "Black-box MKP-100 vs ReEvo": "+0.021", "Black-box MKP-200 vs ReEvo": "+0.082", "Black-box MKP-500 vs EoH": "-2.685", "Black-box BPP-500 vs EoH": "-0.001", "Black-box BPP-700 vs HsEvo": "-0.110", "Black-box BPP-1000 vs HsEvo": "+0.016", "GLS TSP20 vs NeuOpt": "0.000", "GLS TSP50 vs NeuOpt": "+0.018", "GLS TSP100 vs KGLS-ReEvo": "+0.001", "GLS TSP200 vs KGLS-MCTS-AHD": "-0.026"}, "scalability": "RoCo exhibits robust scalability across problem sizes, consistently maintaining leading or near-leading performance as the instance scale increases across all five combinatorial optimization problems.", "statistical_rigor": "Results are averaged over 3 or 4 runs, with mean and standard deviation reported, and small variances observed.", "limitations_acknowledged": []}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2509.20412", "arxiv_url": "https://arxiv.org/abs/2509.20412", "title": "Structuring Collective Action with LLM-Guided Evolution: From Ill-Structured Problems to Executable Heuristics", "authors": ["Kevin"], "abstract": "", "published_date": "2025-12-03", "affiliations": "University of Waterloo, Royal Bank of Canada", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 7, "problem": 5, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "The paper introduces a novel bi-level architecture (ECHO-MIMIC) that couples code evolution with 'message evolution' (nudging), effectively treating mechanism design as a search problem. This is a creative angle for our multi-agent coordination work, specifically for evolving communication/incentive protocols alongside optimization logic."}, "brief": "ECHO-MIMIC presents a framework that first uses LLM-guided evolution to generate Python heuristics for agents (ECHO), and subsequently evolves natural language 'nudges' (MIMIC) to persuade simulated agents to adopt these global-optimal policies. While the experiments rely on synthetic data for agriculture and EV charging, the approach outperforms DSPy and AutoGen baselines in driving collective action. The most valuable takeaway is the architectural separation of 'policy discovery' (code evolution) and 'adoption mechanism' (message evolution)—a pattern we could adapt to evolve incentive structures or negotiation protocols in our multi-agent optimization systems (MASPRM/HERMES). The analysis of code complexity (Halstead metrics) versus fitness also provides a useful empirical reference for our observability work.", "methodology": {"core_method": "LLM-driven evolutionary search for Python code heuristics and natural language messages (ECHO-MIMIC framework)", "llm_role": "variation engine, generator, modifier, fixer, agent (simulation) LLM", "llm_model_used": "Gemini 2.0 Flash Thinking, Gemini 2.5 Flash, Gemini 2.5 Pro, GPT-5 nano, GPT-5 mini", "search_type": "improvement", "novelty_claim": "We introduce ECHO-MIMIC, a general framework that deconstructs complex collective action ISPs into executable behavioral heuristics that are well-structured for individual agents, and then nudges the agents to implement these heuristics.", "components": ["ECHO", "MIMIC", "Domain Creation Agent", "Generator LLM", "Modifier LLM", "Fixer LLM", "Policy Generator LLM", "Policy Modifier LLM", "Agent (Simulation) LLM"], "training_required": true}, "tags": {"methods": ["evolutionary_algorithms", "large_language_models", "llm_code_generation", "llm_as_evaluator", "llm_prompt_optimization", "llm_in_the_loop", "evolution_of_heuristics", "program_synthesis", "genetic_algorithm", "multi_agent_systems"], "problems": ["collective_action_problems", "ill_structured_problems", "multi_agent_coordination", "heuristic_evolution", "agricultural_landscape_management", "carbon_aware_ev_charging"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "funsearch", "specific_domain": "multi_agent_coordination", "llm_coupling": "llm_in_the_loop"}, "problem": {"formal_name": "Ill-Structured Collective Action Problems", "short": "ISPs", "class_": "multi_agent_coordination", "properties": ["ill-structured", "multi-agent", "conflicting objectives", "bounded-rationality", "non-separable global objective"], "scale": "5 farms; 5 agents, 4 time slots, 7-day horizons"}, "lineage": {"direct_ancestors": [{"paper": "Romera-Paredes et al., 2024", "relationship": "builds on LLM-driven program discovery from FunSearch"}, {"paper": "Liu et al., 2024a", "relationship": "builds on the evolution of heuristics paradigm"}], "closest_prior_work": "Romera-Paredes et al., 2024", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["field_validation_with_real_participants", "online_iterative_refinement_with_real_world_feedback", "interpretability_of_heuristics_via_regularization", "adaptive_persona_modeling_with_bayesian_meta_learning"], "transferable_to": ["decentralized_water_rangelands_management", "supply_chains_logistics_coordination", "urban_mobility_traffic_management", "disaster_risk_mitigation"], "open_weaknesses": ["agent_simulation_abstracts_human_behavior_requires_validation", "non_stationarity_distribution_shift_can_stale_heuristics_nudges", "persuasive_mechanisms_risk_manipulation_unequal_burden_sharing", "complex_opaque_heuristics_erode_interpretability_trust"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.99, "experiments": {"benchmarks": ["Agricultural Landscape Management", "Carbon-Aware EV Charging Coordination"], "baselines": ["DSPy MIPROv2", "AutoGen"], "hardware": "null", "instance_sizes": [5, 4, 7]}, "results": {"vs_baselines": {"DSPy MIPROv2": "ECHO/MIMIC consistently outperforms, achieving up to 2.3x higher accuracy on Farm and 1.9x on EV.", "AutoGen": "ECHO/MIMIC consistently outperforms, achieving up to 2.3x higher accuracy on Farm and 2.4x on EV."}, "scalability": "Not explicitly evaluated for performance with increasing instance size.", "statistical_rigor": "Mean accuracy averaged over 5 agents and 2 seeds per domain; comprehensive logging of metrics.", "limitations_acknowledged": ["Agent simulation abstracts human behavior and requires real-world validation.", "Non-stationarity and distribution shift can stale learned heuristics and nudges.", "Persuasive mechanisms risk manipulation, unequal burden sharing, or disparate impacts.", "Evolution can produce complex, opaque heuristics eroding interpretability and trust."]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2510.09330", "arxiv_url": "https://arxiv.org/abs/2510.09330", "title": "Safety Game: Balancing Safe and Informative Conversations with Blackbox Agentic AI using LP Solvers", "authors": ["Tuan"], "abstract": "", "published_date": "2025-12-02", "affiliations": "University of Warwick", "category": "OR for Generative AI", "relevance": {"methodological": 7, "problem": 4, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While safety alignment isn't our primary domain, the paper successfully demonstrates embedding a lightweight LP solver into the inference loop to enforce hard constraints on soft probabilistic outputs. This validates our 'OR for AI' thesis and offers a concrete mechanism for constrained selection in evolutionary search."}, "brief": "The authors formulate LLM response selection as a zero-sum game, solving a small Linear Program (LP) at inference time to mix candidate answers such that the expected risk never exceeds a 'safe fallback' baseline. Results are statistically significant, showing ~15% accuracy gains on SafetyBench by effectively managing the trade-off between helpfulness and safety probes. The key takeaway is the 'Adaptation Safety' constraint formulation: using an LP to guarantee that a stochastic policy is no worse than a heuristic baseline is a powerful, lightweight control mechanism we could adapt for selecting evolved algorithms or managing constraints in multi-agent optimization.", "methodology": {"core_method": "Two-player zero-sum game formulation solved by a linear programming (LP) solver at inference time to compute minimax equilibrium strategies, using binary probes for helpfulness and safety scores, with a sigmoid penalty for risk.", "llm_role": "agent_response_selection, evaluator", "llm_model_used": null, "search_type": "equilibrium search", "novelty_claim": "A novel game-theoretic formulation of the safety vs. helpfulness dilemma for black-box LLM agents, solved by an LP solver at inference time without retraining or access to model internals.", "components": ["two-player zero-sum game", "linear programming solver", "binary helpfulness probe", "binary safety probe", "sigmoid penalty function", "safe fallback option"], "training_required": false}, "tags": {"methods": ["game_theory", "zero_sum_game", "linear_programming", "llm_in_the_loop", "llm_as_evaluator", "inference_time_alignment", "black_box_optimization", "equilibrium_search", "adaptation_safety", "optimization_penalty_function"], "problems": ["llm_safety_alignment", "safety_helpfulness_tradeoff", "multiple_choice_qa"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "safety_game", "specific_domain": "llm_safety_alignment", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Black-box Safety-Helpfulness Alignment for LLM Agents", "short": "LLM Safety-Helpfulness Alignment", "class_": "llm_alignment", "properties": ["black-box", "model-independent", "inference-time", "zero-sum game", "multiple-choice QA", "safety-helpfulness trade-off"], "scale": "up to 11,435 items"}, "lineage": {"direct_ancestors": [{"paper": "Jacob et al., 2024a", "relationship": "builds on game-theoretic LLM generation/selection"}, {"paper": "Ge et al., 2024", "relationship": "draws inspiration from adaptation safety"}, {"paper": "Brown and Sandholm, 2017", "relationship": "draws inspiration from adaptation safety"}], "closest_prior_work": "The Consensus Game: Language Model Generation via Equilibrium Search (Jacob et al., 2024a)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["extend to sequential dialogues/debates", "relax discrete action space to open-ended questions", "develop multi-player Safety Game", "integrate with inference-time steering and control methods"], "transferable_to": ["sequential_dialogue_systems", "open_ended_question_answering", "multi_agent_llm_systems", "llm_inference_steering"], "open_weaknesses": ["limited_to_multiple_choice_qa", "assumes_discrete_action_space", "single_player_safety_cap", "complex_integration_with_steering_methods"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_32", "front_status": "growing", "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.89, "experiments": {"benchmarks": ["HHH", "TruthfulQA", "SafetyBench"], "baselines": ["G (Generative ranking)", "D (Discriminative ranking)", "MI (Mutual-information style)", "SC (Self-contrastive)", "ER-G (Equilibrium-ranking variant)", "ER-D (Equilibrium-ranking variant)"], "hardware": "", "instance_sizes": [200, 662, 11435]}, "results": {"vs_baselines": {"overall": "Outperforms baselines in 11 of 15 test cases, with up to two-fold improvement in accuracy on SafetyBench, and near-best performance in another."}, "scalability": "The LP is small in size and computationally efficient, enabling scalable, model-agnostic safety alignment at inference time.", "statistical_rigor": "Uses binomial standard errors for 95% confidence intervals and McNemar’s test with continuity-corrected χ2 statistic for pairwise comparisons.", "limitations_acknowledged": ["Limited to multiple-choice QA settings", "Does not handle sequential dialogues/debates", "Assumes discrete and known action space", "Single-player safety cap (not multi-player)", "Integration with inference-time steering methods is challenging"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2508.01002", "arxiv_url": "https://arxiv.org/abs/2508.01002", "title": "Optimal Scheduling Algorithms for LLM Inference: Theory and Practice", "authors": ["Agrim"], "abstract": "", "published_date": "2025-12-01", "affiliations": "The University of Texas at Austin", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 8, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper provides a rigorous theoretical foundation for LLM inference scheduling that explicitly accounts for GPU hardware constraints (GeMM tiling), directly relevant to our 'GPUSched' project. It offers a concrete, open-source scheduler (SLAI) that outperforms Sarathi-Serve, providing a strong baseline and implementation reference for our resource allocation work."}, "brief": "Bari et al. develop a queueing-theoretic framework for LLM inference that proves throughput optimality requires satisfying two conditions: optimal GeMM tiling (batch sizes matching hardware tensor core dimensions) and dynamic resource allocation between prefill/decode phases. They propose RAD (theoretical) and SLAI (practical), where SLAI uses a 'last schedulable time' heuristic to delay decode iterations for non-critical requests, thereby freeing up compute for prefill to reduce TTFT. Results are strong, showing a 53% reduction in median TTFT and 26% capacity increase over Sarathi-Serve on Mistral-7B. For our GPUSched project, the key takeaway is the explicit coupling of batch sizes to LCM(tile_dims) for theoretical optimality and the dynamic slack-based scheduling logic for heterogeneous SLOs.", "methodology": {"core_method": "Resource-Aware Dynamic (RAD) scheduler for throughput optimality based on optimal GeMM tiling and dynamic prefill/decode resource allocation; SLO-Aware LLM Inference (SLAI) scheduler for practical SLOs using real-time TBT deadline tracking, SPF prefill ordering, and dynamic offset adjustment based on GPU memory utilization.", "llm_role": "none", "llm_model_used": null, "search_type": "constructive", "novelty_claim": "We propose the throughput-optimal RAD scheduler and the SLO-aware SLAI scheduler which dynamically prioritizes decode requests and reorders prefill requests to meet heterogeneous TBT constraints and reduce TTFT.", "components": ["optimal_gemm_tiling", "dynamic_resource_allocation", "real_time_tbt_deadline_tracking", "shortest_prefill_first_ordering", "dynamic_offset_adjustment", "token_budgeting", "active_request_capping", "decode_limit"], "training_required": false}, "tags": {"methods": ["resource_aware_dynamic_scheduler", "slo_aware_llm_inference_scheduler", "optimal_gemm_tiling", "dynamic_resource_allocation", "real_time_tbt_deadline_tracking", "shortest_prefill_first_ordering", "dynamic_offset_adjustment", "token_budgeting", "active_request_capping", "decode_limit", "queueing_theory", "matrix_multiplication", "gemm", "gemv", "scheduling", "load_balancing", "first_come_first_serve", "llm_serving_systems"], "problems": ["llm_inference_scheduling", "throughput_optimization", "latency_minimization", "slo_aware_scheduling", "resource_allocation", "kv_cache_management", "gpu_scheduling", "llm_serving_optimization"], "contribution_type": ["new_method", "theoretical_result", "framework", "sota_result", "empirical_study"], "framework_lineage": "sarathi_serve", "specific_domain": "llm_serving_optimization", "llm_coupling": null}, "problem": {"formal_name": "Optimal Scheduling Algorithms for LLM Inference", "short": "LLM Scheduling", "class_": "scheduling", "properties": ["two_phase_computation", "throughput_optimization", "slo_aware", "heterogeneous_latency_constraints", "dynamic_resource_allocation", "kv_cache_management"], "scale": "Mistral-7B model, up to 8192 tokens, 128 concurrent requests, 0.2-1.6 requests/second"}, "lineage": {"direct_ancestors": [{"paper": "Sarathi-serve", "relationship": "builds on"}, {"paper": "vLLM", "relationship": "builds on"}, {"paper": "arXiv:2504.07347", "relationship": "extends queueing theory from"}], "closest_prior_work": "Sarathi-serve", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["flashattention_adaptation", "sophisticated_routing_strategies_for_heterogeneous_nodes", "handling_bursty_workloads", "adapting_gpu_computation_model_for_flashattention"], "transferable_to": ["other_ai_inference_systems", "heterogeneous_gpu_nodes", "multi_gpu_inference", "bursty_workloads"], "open_weaknesses": ["tradeoff_between_optimal_tiling_and_resource_allocation_for_slos", "python_implementation_not_fully_optimized", "simplified_random_routing_assumption", "non_optimal_tiling_for_non_multiple_prompt_lengths"]}, "artifacts": {"code_url": "https://github.com/agrimUT/SLAI", "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_0", "front_status": "emerging", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.29, "experiments": {"benchmarks": ["openchat_shareGPT4 dataset"], "baselines": ["Sarathi-serve (FCFS)", "Sarathi-serve (SPF)", "vLLM"], "hardware": "single NVIDIA RTX 6000 Ada GPU with 48 GB of memory", "instance_sizes": [128, 8192]}, "results": {"vs_baselines": {"Sarathi-serve (FCFS)": "53% reduction in median TTFT and 26% increase in serving capacity (from 1.15 to 1.45 req/s) at 0.5s median TTFT target", "vLLM": "Violates P99 TBT at low loads, ineffective for QoS"}, "scalability": "SLAI increases maximum sustainable request rate by 26% (from 1.15 to 1.45 req/s) while meeting latency constraints, though benefits diminish with higher proportions of strict TBT requests.", "statistical_rigor": "Single run for 35 minutes with fixed random seeds for reproducibility; no explicit statistical tests or variance reported.", "limitations_acknowledged": ["tradeoff between optimal tiling and resource allocation for SLOs", "python implementation not fully optimized", "simplified random routing assumption", "FlashAttention adaptation left for future work", "non-optimal tiling for non-multiple prompt lengths"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2511.23473", "arxiv_url": "https://arxiv.org/abs/2511.23473", "title": "ThetaEvolve: Test-time Learning on Open Problems", "authors": ["Yiping"], "abstract": "", "published_date": "2025-11-28", "affiliations": "Microsoft, University of Washington, Carnegie Mellon University, University of Wisconsin-Madison, University of California, San Diego", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 10, "problem": 9, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper directly implements 'RL-infused evolution'—a primary focus of our research profile—and demonstrates that training on the dynamic evolutionary trajectory allows an 8B model to outperform Gemini-Pro ensembles. It provides a concrete, successful recipe for integrating GRPO with AlphaEvolve, which we must replicate immediately."}, "brief": "ThetaEvolve integrates test-time reinforcement learning (GRPO) directly into an AlphaEvolve-style loop, allowing a single 8B model to learn from its own successful mutations and achieve new SOTA bounds on Circle Packing and Autocorrelation inequalities. The results are rigorous, showing that RL applied to the *dynamic* environment (sampling from the evolving database) vastly outperforms RL on static prompts or pure inference search. The most stealable insight is the 'lazy penalty' mechanism—penalizing semantically equivalent code or stagnation—which forces the RL policy to learn genuine exploration strategies rather than memorization. This is a blueprint for the 'RL-infused evolution' milestone in our AlgoEvo roadmap.", "methodology": {"core_method": "Program evolution with test-time Reinforcement Learning (RL) using GRPO algorithm", "llm_role": "code_writer", "llm_model_used": "DeepSeek-R1-0528-Qwen3-8B", "search_type": "improvement", "novelty_claim": "An open-source framework that integrates program evolution with test-time Reinforcement Learning, enabling small open-source LLMs to achieve new best-known bounds on open mathematical optimization problems.", "components": ["single LLM", "large program database", "batch sampling", "lazy penalties", "reward shaping", "GRPO algorithm", "verifier", "parser", "evaluator", "prompt builder"], "training_required": true}, "tags": {"methods": ["program_evolution", "reinforcement_learning", "grpo", "llm_code_generation", "llm_as_heuristic", "llm_in_the_loop", "evolution_of_heuristics", "program_synthesis", "test_time_learning", "batch_sampling", "reward_shaping", "large_program_database"], "problems": ["mathematical_discovery", "open_mathematical_optimization", "circle_packing", "autocorrelation_inequalities", "hadamard_matrix", "geometric_problems", "black_box_optimization", "symbolic_regression"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "alphaevolve", "specific_domain": "mathematical_discovery", "llm_coupling": "rl_trained"}, "problem": {"formal_name": "Improving Bounds on Open Mathematical Optimization Problems via Program Evolution", "short": "OMOP-PE", "class_": "llm_evolutionary_search", "properties": ["open_problems", "mathematical_discovery", "continuous_objectives", "program_synthesis", "circle_packing", "autocorrelation_inequalities", "hadamard_matrix"], "scale": "N=26 circles, N=29 Hadamard matrix, step functions with up to 50,000 pieces"}, "lineage": {"direct_ancestors": [{"paper": "AlphaEvolve", "relationship": "simplifies and extends"}, {"paper": "OpenEvolve", "relationship": "builds upon open-source implementation"}, {"paper": "AlphaProof", "relationship": "inspired test-time RL integration"}], "closest_prior_work": "AlphaEvolve", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["train on multiple targets simultaneously (e.g., different instances or tasks)", "extend to more general post-training workflows", "replace static environments with dynamic, co-evolving ones for better exploration", "further improvement and optimization of the RL-in-dynamic-environment paradigm"], "transferable_to": ["optimization problems with continuous rewards", "different instances of the same task with varying parameters", "a wide range of real-world optimization tasks", "other scientific discovery problems requiring program synthesis"], "open_weaknesses": ["reward shaping parameters require tuning per model and task", "current RL setup does not incorporate dynamic sampling", "further improvements may be difficult when starting from an already strong local minimum", "database management strategies (MAP-Elites, islands) are critical and simplification degrades performance"]}, "artifacts": {"code_url": "https://github.com/ypwang61/ThetaEvolve", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.86, "experiments": {"benchmarks": ["CirclePacking-T", "FirstAutoCorrIneq", "SecondAutoCorrIneq", "ThirdAutoCorrIneq", "HadamardMatrix"], "baselines": ["AlphaEvolve", "ShinkaEvolve", "ThetaEvolve w/o RL", "RL with static environment", "Format reward baseline"], "hardware": "8x80G A100s", "instance_sizes": [26, 29, 600, 50000]}, "results": {"vs_baselines": {"AlphaEvolve": "ThetaEvolve improves best-known bounds on Circle Packing (2.6359857 vs 2.63586276) and First Autocorrelation Inequality (1.503133 vs 1.503164).", "ShinkaEvolve": "ThetaEvolve improves best-known bound on Circle Packing (2.6359857 vs 2.63598283).", "ThetaEvolve w/o RL": "ThetaEvolve w/ RL consistently outperforms inference-only runs across all tasks (e.g., CirclePacking-T best 2.6359857 vs 2.6359834 for Distill-Qwen3-8B).", "RL with static environment": "Performs much worse than ThetaEvolve w/ RL and even worse than ThetaEvolve w/o RL.", "Format reward baseline": "Ineffective and performs worse than pure inference."}, "scalability": "Performance improves with increased test-time compute and larger program database size; batch sampling significantly increases inference throughput compared to sequential sampling.", "statistical_rigor": "Experiments are run with three random seeds (42, 1234, 3407); mean and best scores are reported, and standard deviations are shown in plots.", "limitations_acknowledged": ["Reward shaping parameters require tuning per model and task", "Current RL setup does not incorporate dynamic sampling, which could further improve stability and performance", "Further improvements may be difficult when starting from an already strong local minimum", "Database management strategies (MAP-Elites, islands) are important and simplification leads to weaker performance"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2511.22217", "arxiv_url": "https://arxiv.org/abs/2511.22217", "title": "Optimizing NetGPT via Routing-Based Synergy and Reinforcement Learning", "authors": ["Yuxuan"], "abstract": "", "published_date": "2025-11-27", "affiliations": "Zhejiang University, Huawei Technologies Co., Ltd., Zhejiang Lab, Macau University of Science and Technology, The University of Electro-Communications, Shenzhen CyberAray Network Technology Co., Ltd", "category": "OR for Generative AI", "relevance": {"methodological": 5, "problem": 6, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While the network-aware routing is specific to edge computing, the mechanism for 'SFT-anchored PPO' to maintain schema validity during online RL is a transferable technique for our code-generation evolutionary agents."}, "brief": "Chen et al. propose a cloud-edge routing framework that dynamically offloads tool-calling tasks based on network conditions (RTT/Bandwidth) and a learned confidence score, while simultaneously updating the edge model via PPO. Results on 8,000 tasks show that dynamic thresholds outperform static baselines like FrugalGPT, and crucially, that interleaving SFT updates is required to prevent JSON schema collapse during RL. The primary takeaway for us is the 'SFT-anchored' update strategy: alternating between RL (for reward maximization) and SFT (on valid outputs) is a simple, effective stabilizer for maintaining structural constraints (like code syntax or JSON) during optimization. We should test this anchoring technique in AlgoEvo to keep evolved heuristics syntactically valid while maximizing fitness.", "methodology": {"core_method": "Unified router score with state-dependent fallback threshold and schema-preserving reinforcement learning (PPO with SFT anchor) for edge LLM policy update", "llm_role": "heuristic_generator", "llm_model_used": "DeepSeek-R1-Distill-Qwen-7B (edge), DeepSeek-V3.2-Exp (cloud), Qwen2.5-1.5B-Instruct (reward model)", "search_type": "improvement", "novelty_claim": "The paper proposes a network-aware, schema-preserving routing pipeline with continual improvement for cloud-edge LLM agents, unifying state-dependent routing with online adaptation via an SFT-anchored RL objective.", "components": ["Edge LLM policy", "Cloud LLM", "Reward Model", "Dynamic Fallback Threshold (FuncDyn)", "PolicyNet (MLP router)", "Proximal Policy Optimization (PPO)", "Supervised Finetuning (SFT) anchor", "Caching mechanism"], "training_required": true}, "tags": {"methods": ["adaptive_routing", "dynamic_thresholding", "reinforcement_learning", "ppo", "supervised_finetuning", "llm_in_the_loop", "llm_as_heuristic", "llm_as_evaluator", "mlp", "online_learning", "continual_learning", "rule_based_routing", "funcdyn", "policynet"], "problems": ["llm_serving_optimization", "resource_allocation", "tool_calling", "task_offloading", "network_optimization"], "contribution_type": ["new_method", "sota_result", "theoretical_result", "empirical_study", "framework"], "framework_lineage": "netgpt", "specific_domain": "llm_serving_optimization", "llm_coupling": "rl_trained"}, "problem": {"formal_name": "Cloud-Edge LLM Routing and Online Adaptation for NetGPT", "short": "NetGPT Routing", "class_": "routing", "properties": ["cloud-edge", "tool-calling", "network-aware", "dynamic", "schema-constrained", "online_adaptation", "multi-step"], "scale": "8000 tasks, 10-20 candidate tools, 0-8 steps"}, "lineage": {"direct_ancestors": [{"paper": "RouteLLM: Learning to route llms from preference data", "relationship": "extends by adding network awareness, online adaptation, and schema adherence"}, {"paper": "FrugalGPT: How to use large language models while reducing cost and improving performance", "relationship": "extends by making routing dynamic, network-aware, and enabling online improvement"}], "closest_prior_work": "RouteLLM: Learning to route llms from preference data", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["multi_edge_multi_cloud_orchestration", "reward_signal_robustness_under_distribution_shift", "hardware_in_the_loop_evaluation_on_real_network_traces"], "transferable_to": ["distributed_ai_systems", "edge_computing_task_offloading", "multi_agent_systems_with_heterogeneous_capabilities"], "open_weaknesses": ["current_system_is_single_edge_cloud", "reward_signal_drift_under_distribution_shift", "reliance_on_simulated_network_conditions"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.63, "experiments": {"benchmarks": ["private corpus of tool-calling tasks"], "baselines": ["RouteLLM [14]", "FrugalGPT [11]", "All Edge", "All Cloud"], "hardware": "8x NVIDIA A800 (80 GB) GPUs", "instance_sizes": [8000, 2000, 8000]}, "results": {"vs_baselines": {"FuncDyn": "Highest utility (J) across GOOD/MID/BAD network regimes", "PolicyNet": "Highest quality (Q) across GOOD/MID/BAD network regimes, with higher offload and cost", "FrugalGPT": "Intermediate performance between dynamic controllers and RouteLLM", "RouteLLM": "Lowest utility (J) across GOOD/MID/BAD network regimes", "All Edge": "Lower utility compared to dynamic controllers", "All Cloud": "Lower utility compared to dynamic controllers"}, "scalability": "The dynamic fallback threshold adapts smoothly and predictably to time-varying network conditions (RTT, bandwidth), maintaining higher utility and adjusting offloading decisions to optimize quality-cost trade-offs across different network states.", "statistical_rigor": "Metrics are averaged over 8,000 evaluation tasks; PPO training shows lower variance with SFT anchoring.", "limitations_acknowledged": ["Future work includes multi-edge/multi-cloud orchestration", "robustness of reward signals under distribution shift", "hardware-in-the-loop evaluations"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2511.21572", "arxiv_url": "https://arxiv.org/abs/2511.21572", "title": "BAMAS: Structuring Budget-Aware Multi-Agent Systems", "authors": ["Liming"], "abstract": "", "published_date": "2025-11-26", "affiliations": "Tsinghua University, Peking University, University of Illinois Urbana-Champaign, Nanyang Technological University", "category": "OR for Generative AI", "relevance": {"methodological": 7, "problem": 8, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper validates our thesis on 'OR for AI' by successfully applying Integer Linear Programming (ILP) to multi-agent system configuration. It provides a concrete, proven formulation for budget-constrained agent selection that we can directly adapt for our RobustMAS and resource allocation projects."}, "brief": "BAMAS decouples agent resource provisioning from coordination strategy, using an Integer Linear Programming (ILP) solver to select the optimal set of LLMs under a strict budget and offline RL to select a fixed interaction topology. They demonstrate ~80% cost reduction on GSM8K and MBPP while matching SOTA accuracy, proving that formal optimization beats greedy heuristics for agent allocation. The key takeaway for us is the 'lexicographically optimal' ILP formulation for tier-based LLM selection, which we should steal immediately for our inference resource managers. While their topology search is limited to a fixed library (unlike our evolutionary approach), the hybrid ILP+RL architecture is a strong template for our 'OR for Generative AI' work.", "methodology": {"core_method": "Joint optimization of LLM selection via Integer Linear Programming (ILP) and agent collaboration topology selection via offline reinforcement learning (REINFORCE)", "llm_role": "agents", "llm_model_used": "DeepSeek-V3, GPT-4.1 nano", "search_type": "hybrid", "novelty_claim": "We introduce BAMAS, a novel framework for constructing multi-agent systems under budget constraints, which jointly optimizes LLM selection and agent collaboration topology through Integer Linear Programming and reinforcement learning, respectively, to maximize task performance within a fixed cost budget.", "components": ["Budget-Constrained LLM Provisioning (ILP)", "Agent Collaboration Topology Selection (Offline RL)", "Agent Instantiation"], "training_required": true}, "tags": {"methods": ["integer_linear_programming", "reinforcement_learning", "offline_reinforcement_learning", "policy_gradient", "llm_in_the_loop", "llm_as_heuristic"], "problems": ["multi_agent_coordination", "resource_allocation", "llm_serving_optimization", "code_generation", "mathematical_reasoning", "budget_constrained_multi_agent_system_structuring"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": null, "specific_domain": "budget_constrained_multi_agent_system_structuring", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Budget-Constrained Multi-Agent System Structuring", "short": "BC-MAS Structuring", "class_": "multi_agent_coordination", "properties": ["budget-constrained", "LLM-based", "multi-agent", "cost-performance trade-off"], "scale": "Small number of LLM agents (2-5) solving up to 1,319 tasks"}, "lineage": {"direct_ancestors": [{"paper": "AutoGen (Wu et al. 2024)", "relationship": "addresses limitations of existing multi-agent frameworks regarding cost-efficiency"}, {"paper": "MetaGPT (Hong et al. 2024)", "relationship": "addresses limitations of existing multi-agent frameworks regarding cost-efficiency"}, {"paper": "ChatDev (Qian et al. 2024)", "relationship": "addresses limitations of existing multi-agent frameworks regarding cost-efficiency"}], "closest_prior_work": "AutoGen (Wu et al. 2024)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Explore more complex collaboration topologies beyond predefined ones", "Investigate dynamic budget allocation and re-planning during execution", "Incorporate uncertainty in LLM cost estimation for more robust provisioning", "Extend to other types of agents or heterogeneous agent capabilities"], "transferable_to": ["Resource-constrained AI system design and deployment", "Cloud resource management for LLM workloads", "Automated system design with heterogeneous components and budget limits"], "open_weaknesses": ["Occasional budget overruns due to non-deterministic execution costs", "Limited set of predefined collaboration topologies may restrict optimal solutions", "Offline RL might not fully capture real-time dynamics and emergent behaviors", "The Planner-Driven topology was never selected, indicating potential suboptimality or instability"]}, "artifacts": {"code_url": "https://github.com/chunfenri/BAMAS", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.74, "experiments": {"benchmarks": ["GSM8K", "MBPP", "MATH"], "baselines": ["AutoGen (Wu et al. 2024)", "MetaGPT (Hong et al. 2024)", "ChatDev (Qian et al. 2024)", "Naive-CostAware"], "hardware": "null", "instance_sizes": [1319, 500, 1000]}, "results": {"vs_baselines": {"AutoGen (DeepSeek-V3)": "GSM8K: -62% cost at comparable accuracy (95.3% vs 95.4%); MBPP: -80% cost at +1.8% accuracy (82.6% vs 80.8%)", "MetaGPT (DeepSeek-V3)": "GSM8K: -83% cost at +1.8% accuracy (95.3% vs 93.5%); MBPP: -86% cost at +0.4% accuracy (82.6% vs 82.2%)", "ChatDev (DeepSeek-V3)": "GSM8K: -80% cost at +0.3% accuracy (95.3% vs 95.0%); MBPP: -85% cost at +1.4% accuracy (82.6% vs 81.2%)", "AutoGen (GPT-4.1 nano)": "MATH: -19% cost at +3.6% accuracy (81.2% vs 77.6%)", "MetaGPT (GPT-4.1 nano)": "MATH: -53% cost at +14.1% accuracy (81.2% vs 67.1%)", "ChatDev (GPT-4.1 nano)": "MATH: -52% cost at +5.7% accuracy (81.2% vs 75.5%)", "Naive-CostAware (DeepSeek-V3, L5)": "GSM8K: -67% cost at comparable accuracy (95.3% vs 95.3%)", "Naive-CostAware (DeepSeek-V3, L2)": "MBPP: -62% cost at +1.0% accuracy (82.6% vs 81.6%)"}, "scalability": "BAMAS exhibits an overall cost-performance trade-off, where increasing the budget consistently leads to higher accuracy, and it adaptively selects simpler topologies under tight budgets.", "statistical_rigor": "Single run per experiment with global random seed; ILP solver and LLM decoding steps are deterministic.", "limitations_acknowledged": ["The exact cost of executing a multi-agent system is non-deterministic, leading to occasional budget overruns."]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2511.16383", "arxiv_url": "https://arxiv.org/abs/2511.16383", "title": "An Agent-Based Framework for the Automatic Validation of Mathematical Optimization Models", "authors": ["Alexander"], "abstract": "", "published_date": "2025-11-20", "affiliations": "IBM Research", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 8, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper directly addresses the 'oracle problem' in our OR-Bench and AlgoEvo projects: how to validate LLM-generated code/models without a ground truth. The mutation testing approach provides a concrete, automated mechanism to filter hallucinated constraints that we should implement immediately."}, "brief": "Zadorojniy et al. introduce a multi-agent framework for validating LLM-generated optimization models by generating a test suite and verifying the suite's quality via mutation testing (ensuring tests detect deliberate errors injected into the model). On 100 NLP4LP instances, they achieve a 76% mutation kill ratio and successfully classify external models where simple objective value comparisons fail. The critical takeaway is the 'bootstrapped validation' workflow: using mutation analysis to validate the generated unit tests themselves before using them to score the model. We should steal this mutation-based verification loop to create a robust, ground-truth-free fitness signal for our evolutionary search and OR benchmarking pipelines.", "methodology": {"core_method": "Multi-agent LLM framework for automatic validation of optimization models using problem-level API generation, unit test generation, and optimization-specific mutation testing", "llm_role": "code_writer", "llm_model_used": "o1-preview, gpt-4o", "search_type": "sampling", "novelty_claim": "We propose a novel agent-based method for automatic validation of optimization models that builds upon and extends methods from software testing to address optimization modeling.", "components": ["Business-Interface Generator", "Tests Generator", "Optimization Modeler", "Mutation Agent", "Test Adjuster"], "training_required": false}, "tags": {"methods": ["multi_agent_system", "llm_as_heuristic", "llm_code_generation", "software_testing", "mutation_testing", "test_case_generation", "iterative_refinement", "optimization_model_validation"], "problems": ["optimization_model_validation", "llm_generated_code_validation", "linear_programming", "resource_allocation", "scheduling"], "contribution_type": ["new_method", "framework", "empirical_study"], "framework_lineage": null, "specific_domain": "optimization_model_validation", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Automatic Validation of Mathematical Optimization Models", "short": "OptModel Validation", "class_": "optimization_model_validation", "properties": ["LLM-generated models", "agent-based", "mutation testing", "linear programs"], "scale": "100 problems from NLP4LP benchmark"}, "lineage": {"direct_ancestors": [{"paper": "Optimus: scalable optimization modeling with (mi)lp solvers and large language models", "relationship": "addresses validation shortcomings of LLM-generated models from"}, {"paper": "Mutation testing advances: An analysis and survey", "relationship": "adapts principles from"}], "closest_prior_work": "Optimus: scalable optimization modeling with (mi)lp solvers and large language models", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["refining the mutation process to achieve higher coverage", "evaluating the framework on more complex, real-world problems"], "transferable_to": ["general mathematical_programming_problems", "mixed_integer_linear_programming", "nonlinear_programming"], "open_weaknesses": ["LLM susceptibility to hallucination", "potential for false positives in test suite", "single run may not produce high-quality outcome"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.74, "experiments": {"benchmarks": ["NLP4LP benchmark"], "baselines": ["all o1-preview configuration", "hybrid o1-preview + gpt-4o configuration"], "hardware": "11th Gen Intel(R) Core(TM) i7-11850H CPU @ 2.50GHz and 64GB RAM", "instance_sizes": [100]}, "results": {"vs_baselines": {"all o1-preview configuration": "Achieved 76% mutation coverage, outperforming hybrid configuration.", "hybrid o1-preview + gpt-4o configuration": "Achieved 69% mutation coverage, 7% lower than all o1-preview configuration."}, "scalability": "The vast majority of instances (76%) converge quickly, requiring no more than 3.5 iterations on average.", "statistical_rigor": "Shapiro–Wilk test (p-value 2.17 × 10−6) and Wilcoxon signed-rank test (p-value approximately 0.4) were used to assess statistical significance of iteration differences.", "limitations_acknowledged": ["refining the mutation process to achieve higher coverage", "evaluating the framework on more complex, real-world problems"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2511.15898", "arxiv_url": "https://arxiv.org/abs/2511.15898", "title": "Global Resolution: Optimal Multi-Draft Speculative Sampling via Convex Minimization", "authors": ["Rahul"], "abstract": "", "published_date": "2025-11-19", "affiliations": "Stanford University, Ritual", "category": "OR for Generative AI", "relevance": {"methodological": 9, "problem": 8, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper perfectly intersects your 'OR for AI systems' interest by applying heavy OR theory (polymatroids, max-flow) to solve LLM serving latency. It provides a mathematically optimal solution to the speculative sampling transport problem, which directly impacts the throughput of your massive evolutionary search experiments."}, "brief": "The authors solve the Optimal Transport Linear Program (OTLP) for multi-draft speculative sampling by reducing it to a convex minimization problem using polymatroid theory and max-flow, rather than using slow general LP solvers. They prove this 'Global Resolution' algorithm is exact for i.i.d. drafts and achieves >90% acceptance with negligible overhead (<100ms), running 10,000x faster than baselines. **Key Takeaway:** The reduction of a discrete token selection problem to a convex optimization problem via polymatroids is a brilliant theoretical trick we could potentially adapt for selecting diverse solution subsets in AlgoEvo. This is a definitive 'OR for LLM infra' paper that obsoletes heuristic verification strategies.", "methodology": {"core_method": "Global Resolution algorithm via convex minimization and max-flow", "llm_role": "none", "llm_model_used": null, "search_type": "exact", "novelty_claim": "The first efficient algorithm to compute the optimal transport (OT) to arbitrary accuracy when n >= 2 draft tokens are i.i.d. sampled from a draft model.", "components": ["Max-flow algorithm", "Polymatroid theory", "Convex minimization", "Complementary slackness", "Subset selection", "L-BFGS-B gradient descent"], "training_required": false}, "tags": {"methods": ["speculative_sampling", "optimal_transport", "linear_programming", "max_flow", "polymatroid_theory", "convex_minimization", "complementary_slackness", "subset_selection", "gradient_descent", "l_bfgs_b", "global_resolution"], "problems": ["llm_inference_acceleration", "autoregressive_decoding_latency_reduction", "optimal_multi_draft_speculative_sampling"], "contribution_type": ["new_method", "sota_result", "theoretical_result", "empirical_study"], "framework_lineage": "global_resolution", "specific_domain": "llm_inference_acceleration", "llm_coupling": null}, "problem": {"formal_name": "Optimal Multi-Draft Speculative Sampling", "short": "OTLP", "class_": "llm_inference_acceleration", "properties": ["multi-draft", "single-step", "optimal_transport", "i.i.d. draft sampling", "autoregressive_decoding"], "scale": "V up to 10000 (truncated), n up to 10"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2502.18779", "relationship": "reverses derivation of subset selection formulation from"}, {"paper": "Sun et al., 2023", "relationship": "builds on OTLP formulation from"}, {"paper": "Khisti et al., 2025", "relationship": "unifies canonical decomposition from"}], "closest_prior_work": "arXiv:2502.18779", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Examine efficacy in multi-step speculative decoding systems", "Extend to non-i.i.d. draft settings (e.g., sampling without replacement, distinct independent drafts)", "Improve runtime and reduce early terminations by using non-fixed truncation thresholds or alternative gradient descent methods", "Modify choice of k in top-k draft sampling for failure scenarios"], "transferable_to": ["Multi-step speculative decoding systems (SpecTr)", "Single-draft speculative sampling variants", "LLM inference acceleration techniques requiring optimal transport solutions", "Optimization problems reducible to submodular minimization or max-flow"], "open_weaknesses": ["Runtime of convex minimization (step 3) is variable and can be too large for practical use in some settings", "Global resolution is situational and cannot always be used (early termination/failure cases)", "Current algorithm is limited to i.i.d. draft settings", "Failure rates of global resolution increase significantly with higher target model temperatures"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.01, "experiments": {"benchmarks": ["GSM8K", "HumanEval", "IfEval", "PopQA", "WildJailbreak"], "baselines": ["General LP solver (HiGHS)", "Max-flow solver (graph-tools)", "Optimized max-flow solver"], "hardware": "4xA6000, Intel Xeon Gold 5315Y CPU (32 cores, 3.20 GHz); A100-80GB, Intel Xeon Gold 6342 CPU (12 cores, 2.80 GHz)", "instance_sizes": [10000, 10]}, "results": {"vs_baselines": {"General LP solver (HiGHS)": "Global Resolution is 10,000+x faster; up to +1.71% acceptance gain", "Max-flow solver (graph-tools)": "Global Resolution is 10,000+x faster; up to +1.71% acceptance gain"}, "scalability": "Global Resolution is 10,000+x faster than baselines, with runtimes under 100 ms/token for tested sizes, but failure rates increase with temperature.", "statistical_rigor": "Averages reported over 40 random prompts from 5 datasets; no variance or significance tests mentioned.", "limitations_acknowledged": ["Runtime of convex minimization (step 3) is variable and can be too large", "Global resolution is situational and cannot always be used", "Current algorithm limited to i.i.d. draft settings", "Future work needed for multi-step setting efficacy and non-i.i.d. drafts"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2511.17592", "arxiv_url": "https://arxiv.org/abs/2511.17592", "title": "GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms", "authors": ["Valentin"], "abstract": "", "published_date": "2025-11-17", "affiliations": "Sber, Artificial Intelligence Research Institute (AIRI)", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 8, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This is a direct open-source competitor/blueprint for our AlgoEvo project. It provides validated engineering decisions (rewrite vs. diffs, lineage tracking) that we can immediately adopt or critique to save development time."}, "brief": "GigaEvo is an open-source reproduction of the AlphaEvolve framework that implements MAP-Elites with an asynchronous DAG execution engine, successfully reproducing SOTA results on Heilbronn triangles and beating FunSearch on Weibull Bin Packing. The results are credible and backed by code, specifically highlighting that 'rewrite-based' mutation outperforms 'diff-based' approaches for open-weights models—a crucial engineering constraint for us. The most actionable takeaway is their 'bidirectional lineage tracking' mechanism, which enriches mutation prompts by analyzing both how a program improved over its ancestor and how its descendants further improved, a technique we should steal for AlgoEvo's mutation operator. Their negative result regarding multi-island MAP-Elites (added complexity, no gain) suggests we should deprioritize similar complex topologies.", "methodology": {"core_method": "MAP-Elites quality-diversity algorithm with LLM-driven mutation operators (rewrite-based or diff-based) and bidirectional lineage tracking", "llm_role": "evolutionary_search", "llm_model_used": "Qwen3-235B-A22B-Thinking-2507", "search_type": "hybrid", "novelty_claim": "GigaEvo is an extensible open-source framework that enables researchers to study and experiment with hybrid LLM-evolution approaches inspired by AlphaEvolve, providing modular implementations of key components.", "components": ["MAP-Elites", "asynchronous DAG-based evaluation pipeline", "LLM-driven mutation operator", "bidirectional lineage tracking", "multi-island evolutionary strategies", "Redis database", "LangGraph-based agent", "Hydra configuration system", "insight generation", "lineage analysis"], "training_required": false}, "tags": {"methods": ["llm_evolutionary_search", "map_elites", "quality_diversity", "llm_as_heuristic", "llm_code_generation", "dag_execution", "insight_generation", "lineage_tracking", "multi_island_evolution", "langgraph", "hydra_configuration", "program_synthesis", "simulated_annealing", "quasi_random_sampling", "embedding_similarity", "sigmoid_calibration"], "problems": ["heilbronn_triangle_problem", "geometric_optimization", "circle_packing", "kissing_number_problem", "discrete_optimization", "bin_packing", "online_optimization", "text_classification", "prompt_engineering", "agent_design", "algorithm_discovery", "heuristic_evolution"], "contribution_type": ["framework", "reproducibility_study", "sota_result", "new_method"], "framework_lineage": "alphaevolve", "specific_domain": null, "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Heilbronn Triangle Problem", "short": "Heilbronn Triangle", "class_": "geometric_optimization", "properties": ["non-convex", "high-dimensional", "point_placement", "maximize_minimum_area"], "scale": "11 points"}, "lineage": {"direct_ancestors": [{"paper": "Novikov et al., 2025", "relationship": "implements and extends concepts from"}, {"paper": "Georgiev et al., 2025", "relationship": "implements and extends concepts from"}], "closest_prior_work": "AlphaEvolve (Novikov et al., 2025)", "novelty_type": "incremental"}, "extensions": {"next_steps": ["investigate multi_island_map_elites behavior space design", "integrate continuous_local_search for geometric problems", "extend to multi_file projects and other programming languages", "further research on kissing_number_problem as a benchmark"], "transferable_to": ["other geometric_optimization problems", "other combinatorial_optimization problems", "other algorithm_discovery tasks", "other prompt_and_agent_evolution tasks"], "open_weaknesses": ["multi_island_map_elites did not show clear benefit", "lack of continuous_local_search for fine-grained numerical optimization", "limited to single_file_python_programs", "some llms struggled with certain problems"]}, "artifacts": {"code_url": "https://github.com/AIRI-Institute/gigaevo-core", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.04, "experiments": {"benchmarks": ["Heilbronn Triangle Problem (n=11)", "Circle Packing in Squares (n=26)", "Circle Packing in Squares (n=32)", "Kissing Numbers in High Dimensions (n=12)", "One-dimensional Online Bin Packing (Uniform)", "One-dimensional Online Bin Packing (Weibull)", "Jigsaw - Agile Community Rules Classification"], "baselines": ["AlphaEvolve (Novikov et al., 2025; Georgiev et al., 2025)", "Friedman (2019)", "FunSearch (Romera-Paredes et al., 2024)", "Baseline Prompt"], "hardware": "null", "instance_sizes": [11, 26, 32, 12, 2048]}, "results": {"vs_baselines": {"AlphaEvolve (Heilbronn n=11)": "-0.0001 on min triangle area (0.0364 vs 0.0365)", "AlphaEvolve (Circle Packing n=26)": "+0.00098 on sum of radii (2.63598 vs 2.635)", "Previous SOTA (Circle Packing n=32)": "+0.002 on sum of radii (2.939 vs 2.937)", "Google Gemini-2.5-Flash (Kissing Numbers n=12)": "recovered established lower bound of 840, no improvement", "FunSearch (Bin Packing Uniform)": "replicated state-of-the-art", "FunSearch (Bin Packing Weibull)": "-0.13% excess bin usage (0.55% vs 0.68%)", "Baseline Prompt (Jigsaw)": "+11.3 percentage points AUC on test set (0.783 vs 0.670)", "Evolved Prompt (Jigsaw)": "+2.0 percentage points AUC on test set (0.803 vs 0.783)"}, "scalability": "The framework's modular and concurrent architecture, utilizing asynchronous DAGs and Redis for shared state, supports efficient processing of evolutionary runs and concurrent evaluation of programs.", "statistical_rigor": "Experiments involved multiple restarts (25 for Heilbronn) and generations (20 for Heilbronn, 60 for Prompt/Agent evolution), but no explicit statistical significance tests or variance reporting across multiple independent runs for final results.", "limitations_acknowledged": ["No clear benefit from multi-island MAP-Elites observed", "Hybrid approaches integrating continuous local search may be beneficial for geometric problems", "Framework currently focuses on single-file Python programs, not multi-file projects or other languages"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2508.03661", "arxiv_url": "https://arxiv.org/abs/2508.03661", "title": "Automated Algorithmic Discovery for Scientific Computing through LLM-Guided Evolutionary Search: A Case Study in Gravitational-Wave Detection", "authors": ["He"], "abstract": "", "published_date": "2025-11-16", "affiliations": "Tsinghua University, University of Chinese Academy of Sciences", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 7, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper successfully integrates MCTS with LLM-based evolutionary operators, demonstrating a 59% improvement over ReEvo (pure evolution) and MCTS-AHD. It provides a concrete architecture for 'RL-infused evolution' and 'Memory' (via Path-wise Crossover) that we can directly transplant into AlgoEvo to solve our exploration-exploitation bottlenecks."}, "brief": "Evo-MCTS introduces a hybrid search architecture where MCTS manages the exploration-exploitation balance of an evolutionary process, using LLMs for node expansion via novel operators like 'Path-wise Crossover' (synthesizing code from full root-to-leaf trajectories). The results are empirically strong, outperforming standard LLM-evolution baselines (ReEvo) by ~150% on a complex signal processing task. We learned that structuring the evolutionary lineage as a tree and using MCTS Q-values to select parents—rather than standard population selection—drastically improves sample efficiency and solution quality. This is a blueprint for the 'RL-infused evolution' and 'persistent memory' features we have been planning for our own framework.", "methodology": {"core_method": "LLM-guided Evolutionary Monte Carlo Tree Search (Evo-MCTS) with reflective code synthesis and multi-scale evolutionary operations", "llm_role": "code_writer, heuristic_generator, evaluator, evolutionary_search", "llm_model_used": "o3-mini-medium, o1-2024-12-17, gpt-4o-2024-11-20, claude-3-7-sonnet-20250219-thinking, deepseek-r1-250120", "search_type": "hybrid", "novelty_claim": "Evo-MCTS integrates LLMs with tree-structured evolutionary search and multi-scale evolutionary operations for interpretable algorithm discovery in scientific computing, achieving synergistic performance beyond individual components.", "components": ["Monte Carlo Tree Search (MCTS)", "Evolutionary Algorithm", "Reflective Code Synthesis", "LLM Code Generation", "Domain Knowledge Integration", "UCT selection", "Parent Crossover", "Sibling Crossover", "Path-wise Crossover", "Point Mutation", "Error Handling and Iterative Refinement", "Population Management"], "training_required": false}, "tags": {"methods": ["monte_carlo_tree_search", "evolutionary_algorithm", "llm_code_generation", "llm_as_heuristic", "llm_as_evaluator", "llm_evolutionary_search", "program_synthesis", "evolution_of_heuristics", "reflective_code_synthesis", "evo_mcts"], "problems": ["gravitational_wave_detection", "operator_discovery", "heuristic_evolution"], "contribution_type": ["new_method", "sota_result", "empirical_study", "framework"], "framework_lineage": "reevo", "specific_domain": "gravitational_wave_detection", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Automated Algorithm Discovery for Gravitational-Wave Detection", "short": "GW-AAD", "class_": "algorithm_design", "properties": ["continuous_parameter_spaces", "strict_physical_constraints", "interpretable_solutions", "non_stationary_noise", "non_Gaussian_noise"], "scale": "time-series data from dual detectors, up to 1.18 days continuous, sampled at 2048 Hz"}, "lineage": {"direct_ancestors": [{"paper": "Monte Carlo Tree Search", "relationship": "integrates MCTS for tree-structured search"}, {"paper": "Evolutionary Algorithms", "relationship": "incorporates multi-scale evolutionary operations"}, {"paper": "ReEvo", "relationship": "builds upon LLM-guided evolutionary search principles"}, {"paper": "MCTS-AHD", "relationship": "extends MCTS for automated heuristic discovery"}], "closest_prior_work": "ReEvo", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Develop real-time processing capabilities for gravitational wave detection", "Integrate comprehensive parameter estimation into discovered algorithms", "Extend to multi-messenger astronomy applications", "Explore algorithm discovery for other scientific computing domains with strict physical constraints"], "transferable_to": ["Materials science for discovering new simulation algorithms", "Drug discovery for optimizing molecular dynamics simulations", "Other physics simulations requiring interpretable algorithms", "Automated algorithm design for signal processing in other scientific fields"], "open_weaknesses": ["Current benchmarks (MLGWSC-1) do not fully capture real-world operational complexities", "Discovered algorithms are not yet production-ready pipelines", "Computational cost of LLM-guided search can be high", "Ensuring interpretability for increasingly complex discovered algorithms"]}, "artifacts": {"code_url": "https://github.com/iphysresearch/evo-mcts", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.06, "experiments": {"benchmarks": ["MLGWSC-1 Dataset 4"], "baselines": ["Sage [14]", "Virgo-AUTh [30, 31]", "PyCBC [30, 32]", "TPI FSU Jena [30, 33]", "cWB [30, 34]", "MFCNN [30, 35]", "CNN-Coinc [30, 36, 37]", "MCTS-AHD [29]", "ReEvo [28]"], "hardware": "ORISE Supercomputer with 32-core x86 processors and 4 GPGPU accelerators per node", "instance_sizes": [39, 3782]}, "results": {"vs_baselines": {"Sage": "+20.2% AUC (5241.37 vs 4359.27)", "Virgo-AUTh": "+27.8% AUC (5241.37 vs 4101.48)", "PyCBC": "+28.8% AUC (5241.37 vs 4069.90)", "TPI FSU Jena": "+39.9% AUC (5241.37 vs 3744.99)", "cWB": "+62.5% AUC (5241.37 vs 3225.01)", "MFCNN": "+81.3% AUC (5241.37 vs 2890.33)", "CNN-Coinc": "+162.5% AUC (5241.37 vs 1997.02)", "MCTS-AHD": "+59.2% fitness (2670.37 vs 1677.73)", "ReEvo": "+146.6% fitness (2670.37 vs 1082.61)"}, "scalability": "The framework consistently discovers algorithms that generalize robustly to independent datasets and maintain performance on extended-duration signals, with more sophisticated algorithms emerging in deeper MCTS layers showing enhanced generalization.", "statistical_rigor": "5 independent runs with different random seeds; Shannon diversity index and Complexity Index of Diversity analysis; 100 independent re-executions for edge robustness; Adaptive Statistical Testing Protocol (Shapiro-Wilk, Welch’s t-test, Mann-Whitney U test, Cohen’s d, rank-biserial correlation) with resampling for component analysis.", "limitations_acknowledged": ["Benchmarks like MLGWSC-1 have limitations in capturing full complexity of operational gravitational wave detection, lacking real-time processing, comprehensive parameter estimation, or multi-messenger astronomy integration.", "Framework's primary contribution is methodology for automated algorithm discovery, not production-ready gravitational wave detection pipelines."]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2511.09092", "arxiv_url": "https://arxiv.org/abs/2511.09092", "title": "OR-R1: Automating Modeling and Solving of Operations Research Optimization Problem via Test-Time Reinforcement Learning", "authors": ["Zezhen"], "abstract": "", "published_date": "2025-11-12", "affiliations": "The Hong Kong University of Science and Technology, Arizona State University, University of North Carolina at Chapel Hill", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper successfully applies Group Relative Policy Optimization (GRPO)—a critic-free RL method—to OR code generation, achieving state-of-the-art results with minimal data (100 samples). It provides a concrete blueprint for implementing 'RL-infused evolution' without the overhead of a value network, which is directly applicable to our AlgoEvo and MASPRM projects."}, "brief": "OR-R1 introduces a data-efficient framework that fine-tunes Qwen3-8B using Supervised Fine-Tuning (SFT) followed by Test-Time Group Relative Policy Optimization (TGRPO) on unlabeled data. The results are empirically strong: it outperforms ORLM and LLMOPT while using only 1/10th of the synthetic training data, specifically narrowing the consistency gap between Pass@1 and Pass@8. The key takeaway for us is the effectiveness of GRPO (normalizing rewards within a sampled group to estimate baselines) combined with majority-voting rewards; this eliminates the need for a separate critic model while significantly improving code generation consistency. We should immediately evaluate GRPO as a lightweight alternative to PPO for the 'RL-infused' components of our evolutionary search methods.", "methodology": {"core_method": "Supervised Fine-tuning (SFT) followed by Test-Time Group Relative Policy Optimization (TGRPO) with a composite reward function", "llm_role": "code_writer, heuristic_generator, evaluator", "llm_model_used": "Qwen3-8B", "search_type": "hybrid", "novelty_claim": "OR-R1 is a novel framework that, for the first time, integrates SFT and TGRPO for automated operations research problems modeling and solving.", "components": ["Supervised Fine-tuning (SFT)", "Test-Time Group Relative Policy Optimization (TGRPO)", "Format Reward", "Valid-Code Reward", "Majority Voting Reward"], "training_required": true}, "tags": {"methods": ["supervised_fine_tuning", "reinforcement_learning", "proximal_policy_optimization", "test_time_adaptation", "llm_code_generation", "llm_as_evaluator", "llm_fine_tuned", "llm_in_the_loop", "program_synthesis", "self_improving_search", "group_relative_policy_optimization", "test_time_reinforcement_learning"], "problems": ["operations_research_problem_solving", "optimization_modeling", "linear_programming", "mixed_integer_linear_programming", "resource_allocation", "scheduling", "logistics"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": null, "specific_domain": "operations_research_problem_solving", "llm_coupling": "rl_trained"}, "problem": {"formal_name": "Automated Operations Research Optimization Problem Modeling and Solving", "short": "OR Problem Modeling and Solving", "class_": "operations_research_modeling", "properties": ["linear_programming", "mixed_integer_linear_programming"], "scale": "Small to medium-sized linear and mixed-integer linear programming problems"}, "lineage": {"direct_ancestors": [{"paper": "ORLM (Huang et al. 2025)", "relationship": "improves data efficiency and consistency over"}, {"paper": "LLMOPT (Jiang et al. 2024a)", "relationship": "improves performance over"}, {"paper": "DeepSeekMath (Shao et al. 2024)", "relationship": "TGRPO is consistent with its Group Relative Policy Optimization (GRPO)"}, {"paper": "TTRL (Zuo et al. 2025)", "relationship": "TGRPO extends Test-Time Reinforcement Learning principles from"}], "closest_prior_work": "ORLM (Huang et al. 2025)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Extend training duration to potentially reach performance plateau", "Explore application to a wider range of OR problem types beyond LP/MILP", "Integrate with more advanced or specialized OR solvers", "Further improve data synthesis frameworks for SFT to reduce reliance on even small labeled datasets"], "transferable_to": ["Automated algorithm design for other computational domains", "Code generation for scientific computing and other problem-solving tasks", "Optimization problems with non-linear or stochastic elements", "Automated problem solving in other formal domains (e.g., theorem proving, constraint satisfaction)"], "open_weaknesses": ["Computational resource constraints limited training duration, suggesting unreached performance potential", "While reduced, a gap between single-attempt (Pass@1) and multi-attempt (Pass@8) accuracy still exists", "The initial SFT stage still relies on a (small) labeled dataset, which could be further minimized or eliminated", "Generalization to highly novel or out-of-distribution OR problems might still be a challenge"]}, "artifacts": {"code_url": "https://github.com/SCUTE-ZZ/OR-R1", "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_6", "front_status": "emerging", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.36, "experiments": {"benchmarks": ["NL4Opt", "MAMO Easy LP", "MAMO Complex LP", "NLP4LP", "ComplexOR", "IndustryOR", "OptiBench", "ICML Competition"], "baselines": ["Qwen3-8B SFT(3K)", "Qwen2.5-7B SFT(3K)", "Llama3-8B SFT(3K)", "Qwen3-8B SFT(100)", "LLMOPT(Qwen2.5-14B)", "ORLM(Llama3-8B)"], "hardware": "4x A100 (40G) GPUs", "instance_sizes": [230, 652, 211, 242, 18, 100, 605, 410]}, "results": {"vs_baselines": {"ORLM(Llama3-8B)": "+4.2% average accuracy", "LLMOPT(Qwen2.5-14B)": "+7.6% average accuracy", "Qwen3-8B SFT(3K)": "+3.1% average accuracy", "Qwen3-8B SFT(100)": "+6.4% average accuracy"}, "scalability": "TGRPO achieves significant performance gains with a relatively small amount of data, and increasing the data size further brings diminishing returns, demonstrating high data efficiency.", "statistical_rigor": "Results are reported as mean and standard deviation over three independent training runs.", "limitations_acknowledged": ["Training duration was limited due to computational resource constraints, suggesting potential for further improvement with extended training."]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2502.14617", "arxiv_url": "https://arxiv.org/abs/2502.14617", "title": "SageServe: Optimizing LLM Serving on Cloud Data Centers with Forecast Aware Auto-Scaling", "authors": ["Shashwat"], "abstract": "", "published_date": "2025-11-12", "affiliations": "Microsoft, University of Illinois Urbana-Champaign, Georgia Institute of Technology, Indian Institute of Science", "category": "OR for Generative AI", "relevance": {"methodological": 7, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper directly addresses your 'GPUSched' and 'OR formulations for LLM inference' active projects with a production-grade ILP formulation and, crucially, releases real-world Microsoft O365 inference traces. It validates the OR approach to system optimization which you advocate for, providing a definitive benchmark."}, "brief": "SageServe optimizes LLM inference resource allocation across regions using an Integer Linear Programming (ILP) model coupled with ARIMA-based traffic forecasting, specifically targeting mixed interactive and non-interactive workloads. They validate this on real Microsoft O365 production traces (which they release), demonstrating a 25% reduction in GPU hours and $2.5M/month savings compared to reactive baselines. The primary value for us is the release of the production workload traces—allowing us to benchmark our 'GPUSched' formulations against real-world data rather than synthetic distributions—and their specific ILP formulation for unified capacity management, which directly competes with our internal OR models.", "methodology": {"core_method": "Integer Linear Programming (ILP) for resource allocation, combined with ARIMA-based time-series forecasting and reactive heuristics for dynamic scaling and scheduling", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "SageServe introduces a unified framework for LLM inference serving that combines multi-timescale control knobs, forecast-aware predictive scaling via ILP, and reactive heuristics to co-optimize routing and resource allocation across regions and diverse workload tiers.", "components": ["Load Predictor (ARIMA)", "Optimization Module (ILP Solver)", "Autoscaler (Scaling Logic: LT-I, LT-U, LT-UA)", "Queue Manager (for NIW)", "Routing Logic (Global, Regional, Instance-level)", "Request Scheduler (FCFS, EDF, PF, DPA)", "LLM Inference Simulator (Splitwise extension)"], "training_required": true}, "tags": {"methods": ["integer_linear_programming", "arima_time_series_forecasting", "reactive_heuristics", "discrete_event_simulation"], "problems": ["llm_serving_optimization", "resource_allocation", "gpu_scheduling", "cloud_scheduling"], "contribution_type": ["new_method", "sota_result", "empirical_study", "framework", "new_benchmark"], "framework_lineage": null, "specific_domain": "llm_serving_optimization", "llm_coupling": null}, "problem": {"formal_name": "Optimizing LLM Serving on Cloud Data Centers with Forecast Aware Auto-Scaling", "short": "LLM Inference Serving Optimization", "class_": "resource_allocation", "properties": ["mixed_workloads", "multi_model", "multi_region", "forecast_aware", "auto_scaling", "SLA_constrained", "GPU_accelerated", "cold_start_delays"], "scale": "10 million requests per day, 3 regions, 4-5 LLM models, thousands of GPU VMs"}, "lineage": {"direct_ancestors": [{"paper": "Splitwise: Efficient generative llm inference using phase splitting", "relationship": "extends simulation harness from"}, {"paper": "Hierarchical Autoscaling for Large Language Model Serving with Chiron", "relationship": "addresses limitations of regional-level deployments and reactive autoscaling"}], "closest_prior_work": "Hierarchical Autoscaling for Large Language Model Serving with Chiron", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Extend to accommodate workloads with a continuum of SLAs", "Conduct studies with deployments across heterogeneous hardware types", "Implement SLA differentiation at the routing level by dedicating specific instances to high-priority workloads"], "transferable_to": ["General cloud resource management for other AI models", "Distributed system scheduling with diverse SLA requirements", "Edge computing resource allocation"], "open_weaknesses": ["Theoretical performance limits for LLM types differ from real-world achievements", "Current SLA differentiation could be improved at the routing level"]}, "artifacts": {"code_url": "https://github.com/shashwatj07/SageServe", "models_released": false, "new_benchmark": true}, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_status": "growing", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.96, "experiments": {"benchmarks": ["Microsoft Office 365 production traces (July 2025)", "Microsoft Office 365 production traces (November 2024)"], "baselines": ["Unified Reactive Heuristic", "Chiron [36]"], "hardware": "NVIDIA H100-80GB VMs", "instance_sizes": [10000000]}, "results": {"vs_baselines": {"Unified Reactive Heuristic": "24.21% fewer instance hours (LT-I), 19.65% fewer (LT-U), 23.38% fewer (LT-UA) for Llama-2; 25% overall GPU-hours savings; 80% reduction in VM cold-start.", "Chiron [36]": "Higher instance demand and lower hardware utilization compared to SageServe and Reactive."}, "scalability": "SageServe scales effectively for diverse model architectures (e.g., MoE models) and longer traces, adapting to unique efficiency characteristics while maintaining benefits for traditional dense models and different request rates.", "statistical_rigor": "MAPE of <3% for Splitwise performance model; 95th percentile (P95) latency reported; comparison of mean, median, P95 latency; standard deviation in latency shown; periodicity score (ps) for workload patterns.", "limitations_acknowledged": ["Future work includes extending SageServe to accommodate workloads with a continuum of SLAs.", "Conducting studies on the proposed approach with deployments across heterogeneous hardware types.", "Theoretical performance limits for LLM types differ from real-world achievements.", "Cloud providers could implement SLA differentiation at the routing level as well by dedicating specific instances to high-priority workloads."]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2511.08522", "arxiv_url": "https://arxiv.org/abs/2511.08522", "title": "AlphaResearch: Accelerating New Algorithm Discovery with Language Models", "authors": ["Zhaojian"], "abstract": "", "published_date": "2025-11-11", "affiliations": "Yale, NYU, Tsinghua, ByteDance", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 7, "problem": 6, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While the empirical results are mixed (fails on 6/8 tasks), the architecture of using a learned 'Idea Reward Model' (trained on literature/reviews) to filter natural language mutations before code generation is a concrete implementation of 'RL-infused evolution' that we should discuss for AlgoEvo."}, "brief": "AlphaResearch introduces a 'dual environment' for algorithm discovery: it generates natural language research ideas, filters them using a reward model fine-tuned on ICLR peer reviews, and then executes the surviving ideas. While it claims to beat human baselines on Packing Circles, the improvement is marginal (<0.1%) and it fails to improve upon baselines in 6/8 benchmark problems. The key takeaway for us is the mechanism of an 'Idea Critic'—using a learned reward model to filter the search space at the prompt level before wasting compute on execution—which directly addresses our sample efficiency goals in evolutionary search.", "methodology": {"core_method": "Autonomous research agent with dual research environment combining execution-based verification and simulated real-world peer review", "llm_role": "research_agent", "llm_model_used": "o4-mini", "search_type": "improvement", "novelty_claim": "AlphaResearch, an autonomous research agent, discovers new algorithms on open-ended problems by synergizing feasibility and innovation through a novel dual research environment combining execution-based verification and simulated real-world peer review.", "components": ["LLM Ensemble (for idea/program generation)", "AlphaResearch-RM-7B (Reward Model)", "Program-based Execution Environment", "MetaData (trajectory update)"], "training_required": true}, "tags": {"methods": ["llm_research_agent", "llm_code_generation", "reward_model", "llm_fine_tuned", "program_synthesis", "execution_based_verification", "llm_as_evaluator", "llm_evolutionary_search", "evolution_of_heuristics"], "problems": ["algorithm_discovery", "packing_circles", "spherical_code", "littlewood_polynomials", "mstd"], "contribution_type": ["new_method", "framework", "sota_result", "new_benchmark", "empirical_study"], "framework_lineage": null, "specific_domain": null, "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "New Algorithm Discovery for Open-Ended Algorithmic Problems", "short": "Algorithm Discovery", "class_": "algorithm_design", "properties": ["open_ended", "program_based_verification", "iterative_optimization", "dual_environment_feedback"], "scale": "8 algorithmic problems, instances up to n=512"}, "lineage": {"direct_ancestors": [{"paper": "AlphaEvolve", "relationship": "builds on the agentic discovery paradigm of"}], "closest_prior_work": "AlphaEvolve", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Expand to more realistic applications like tensor computations", "Augment research agents with external tools", "Enhance reward model with larger parameters and datasets"], "transferable_to": ["Other open-ended algorithmic problems", "Scientific discovery", "Automated theorem proving"], "open_weaknesses": ["Limited to current problem scope", "Lack of external tools integration", "Reward model uses small models and limited dataset"]}, "artifacts": {"code_url": "https://github.com/answers111/alpha-research", "models_released": true, "new_benchmark": true}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.24, "experiments": {"benchmarks": ["AlphaResearchComp", "packing circles (n=26)", "packing circles (n=32)", "minimizing max-min distance ratio (d=2, n=16)", "third autocorrelation inequality", "spherical code (d=3, n=30)", "autoconvolution peak minimization (upper bound)", "littlewood polynomials (n=512)", "MSTD (n=30)"], "baselines": ["Human researchers", "AlphaEvolve", "OpenEvolve", "ShinkaEvolve"], "hardware": "Training for AlphaResearch-RM-7B used bfloat16 precision under FSDP; no specific GPU/CPU for agent execution.", "instance_sizes": [26, 32, 16, 30, 512]}, "results": {"vs_baselines": {"Human researchers": "Outperforms on packing circles (n=26, n=32), matches on Littlewood Polynomials (n=512) and MSTD (n=30), underperforms on 4 other problems.", "AlphaEvolve": "Outperforms on packing circles (n=26) by +0.04% and (n=32) by +0.07%.", "OpenEvolve": "Better on packing circles (n=26).", "ShinkaEvolve": "Slightly better on packing circles (n=26)."}, "scalability": "The discovery process shows consistent improvement over iterations, with execution-based reward growing rapidly then plateauing.", "statistical_rigor": "Single run for each problem; reward model evaluated on 100 samples with human annotator comparison.", "limitations_acknowledged": ["Limited to current problem scope, needs expansion to more realistic applications like tensor computations.", "Research agents could be augmented with external tools for more complex problems.", "Reward model training uses small models and limited dataset, requiring enhancement in parameter and dataset size."]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2511.05915", "arxiv_url": "https://arxiv.org/abs/2511.05915", "title": "CoEdge-RAG: Optimizing Hierarchical Scheduling for Retrieval-Augmented LLMs in Collaborative Edge Computing", "authors": ["Guihang"], "abstract": "", "published_date": "2025-11-08", "affiliations": "Sun Yat-sen University", "category": "OR for Generative AI", "relevance": {"methodological": 6, "problem": 8, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "Directly relevant to our 'GPUSched' and 'OR for AI systems' tracks. The hierarchical decomposition (RL for routing, Convex Optimization for resource allocation) offers a concrete architecture for distributed LLM serving that balances semantic quality with latency constraints."}, "brief": "Hong et al. introduce CoEdge-RAG, a hierarchical scheduling framework for distributed edge RAG that combines PPO-based query routing with Online Convex Optimization (OCO) for local resource management. They empirically validate that a quadratic function best approximates LLM inference latency for OCO, allowing them to dynamically resize models and memory allocations under strict SLOs. The standout takeaway is the feedback loop: using PPO to learn a 'semantic routing policy' based on downstream generation quality (Rouge/BERTScore) rather than just load, effectively solving the 'black box' data distribution problem in privacy-preserving multi-agent systems. This hybrid RL/OR control stack is a transferable pattern for our distributed inference and multi-agent optimization work.", "methodology": {"core_method": "Hierarchical scheduling combining PPO for online query identification, linear regression for inter-node capacity estimation, and online convex optimization with quadratic approximation for intra-node latency-quality trade-off", "llm_role": "generation, evaluation", "llm_model_used": "DeepSeek-V3", "search_type": "hybrid", "novelty_claim": "A novel hierarchical scheduling framework for multi-node collaborative edge computing that jointly optimizes query scheduling, model deployment, and resource allocation for RAG-LLMs, featuring an online PPO-based query identification mechanism.", "components": ["Online Query Identification Mechanism (PPO)", "Dynamic Inter-node Scheduling Strategy", "Adaptive Intra-node Scheduling (Online Convex Optimization)", "Retrieval-Augmented Generation (RAG)", "VLLM inference backend"], "training_required": true}, "tags": {"methods": ["hierarchical_scheduling", "retrieval_augmented_generation", "proximal_policy_optimization", "online_convex_optimization", "reinforcement_learning", "linear_regression", "vllm", "llm_in_the_loop", "llm_as_evaluator"], "problems": ["llm_serving_optimization", "resource_allocation", "workload_balancing", "edge_computing", "distributed_systems", "scheduling", "latency_optimization", "quality_optimization", "combinatorial_optimization"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": null, "specific_domain": "hierarchical_rag_scheduling_in_edge_computing", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Optimizing Hierarchical Scheduling for Retrieval-Augmented LLMs in Collaborative Edge Computing", "short": "Hierarchical RAG Scheduling", "class_": "hierarchical_scheduling", "properties": ["real-time", "privacy-preserving", "resource-constrained", "distributed", "heterogeneous", "online", "latency-constrained", "quality-aware"], "scale": "3-4 edge nodes, 1-2 GPUs per node, 1B-8B LLM parameters, up to 2000 queries"}, "lineage": {"direct_ancestors": [{"paper": "Retrieval-augmented generation for knowledge-intensive nlp tasks", "relationship": "extends RAG to multi-node collaborative edge environments"}, {"paper": "Helix: Distributed serving of large language models via max-flow on heterogeneous gpus", "relationship": "integrates RAG and data-awareness into distributed LLM serving"}], "closest_prior_work": "Helix: Distributed serving of large language models via max-flow on heterogeneous gpus", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["extend_to_multi_document_queries", "explore_more_complex_latency_quality_functions", "investigate_adaptive_thresholding_for_ppo_updates", "integrate_with_dynamic_network_conditions"], "transferable_to": ["cloud_edge_hybrid_llm_serving", "distributed_inference_for_other_large_ai_models", "multi_agent_systems_with_resource_constraints", "federated_learning_for_rag_systems"], "open_weaknesses": ["focus_on_single_document_queries", "approximation_of_generation_quality_and_latency_functions", "computational_cost_of_transient_request_level_quality_estimations", "static_configuration_of_ppo_update_threshold"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.43, "experiments": {"benchmarks": ["DomainQA", "Personalized-Proactive-Conversations (PPC)"], "baselines": ["Random Allocation", "MAB-based Allocation", "Oracle Allocation", "Small-Param", "Mid-Param", "Mixed-Param.1", "Mixed-Param.2", "w/o inter-node scheduling"], "hardware": "NVIDIA RTX 4090 GPUs (single and dual configurations across 4 nodes)", "instance_sizes": [400, 500, 1000, 1500, 2000]}, "results": {"vs_baselines": {"Random Allocation": "+4.23% to +91.39% across metrics on DomainQA and PPC", "MAB-based Allocation": "Significant improvements across metrics on DomainQA and PPC", "Oracle Allocation": "Comparable performance (within 2-3% on key metrics)", "w/o inter-node scheduling": "+7.13% to +12.65% on Rouge-L and BERTScore", "Small-Param": "Comparable performance under strict latency, superior under relaxed latency", "Mid-Param": "Significantly lower drop rates (e.g., <3% vs >44%) and higher quality under strict latency", "Mixed-Param.1": "Significantly lower drop rates (e.g., <3% vs >23%) and higher quality under strict latency", "Mixed-Param.2": "Significantly lower drop rates (e.g., <3% vs >60%) and higher quality under strict latency"}, "scalability": "The system dynamically adjusts query and resource allocation to maintain performance and quality under varying query loads, domain skew, and latency constraints, outperforming baselines across different scales of latency requirements.", "statistical_rigor": "Performance is evaluated using multiple metrics (Rouge, Bleu, Meteor, BERTScore, DropRate) across diverse datasets and simulated query patterns; RMSE is used for latency model accuracy, and weight factors are manually tuned.", "limitations_acknowledged": ["Focus on single-document queries", "Absence of closed-form expressions for latency and quality functions", "Computational cost and inaccuracy of transient request-level quality estimations"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2511.00685", "arxiv_url": "https://arxiv.org/abs/2511.00685", "title": "SOCRATES: Simulation Optimization with Correlated Replicas and Adaptive Trajectory Evaluations", "authors": ["Haoting"], "abstract": "", "published_date": "2025-11-01", "affiliations": "Columbia, UC Berkeley, Amazon", "category": "Generative AI for OR", "relevance": {"methodological": 8, "problem": 7, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "The concept of 'trajectory-aware meta-optimization'—feeding loss curves to an LLM to dynamically switch strategies—is a concrete solution to the stagnation detection problem in our evolutionary search. It effectively turns the LLM into a runtime controller rather than just a code generator."}, "brief": "SOCRATES introduces a two-stage framework: first constructing 'Operational AI Replicas' (surrogates) via LLM-guided causal discovery, then using an LLM to analyze optimization trajectories on these surrogates to schedule hybrid algorithms (e.g., running BO then switching to GA). While the benchmarks (inventory, queuing) are simple and the causal inference step seems fragile, the core innovation of **trajectory-based reasoning** is highly transferable. We can steal this mechanism for AlgoEvo: instead of blind evolution, our planner agent should consume the optimization trajectory to dynamically swap operators or restart populations when stagnation is detected, effectively using the LLM as a process reward model.", "methodology": {"core_method": "Two-stage procedure: Stage 1 constructs an ensemble of Operational AI Replicas (OARs) via LLM-guided causal skeleton inference and EM-type structural learning. Stage 2 employs an LLM as a trajectory-aware meta-optimizer to iteratively revise and compose a hybrid SO algorithm schedule on the OAR ensemble.", "llm_role": "causal_discovery, meta_optimizer, schedule_reviser", "llm_model_used": null, "search_type": "improvement", "novelty_claim": "SOCRATES introduces a novel two-stage LLM-driven meta-optimization procedure that automates the design of tailored simulation optimization algorithms by constructing operational AI replicas and iteratively revising algorithm schedules based on trajectory analysis.", "components": ["LLM-guided causal skeleton inference (BFS)", "EM-type structural learning", "Operational AI Replica (OAR) ensemble", "LLM-based trajectory-aware meta-optimizer", "Iterative revision loop", "Online adaptation procedure", "Baseline SO algorithms (BO-EI, BO-UCB, BO-PI, GA, PSO)"], "training_required": true}, "tags": {"methods": ["simulation_optimization", "llm_in_the_loop", "llm_as_meta_optimizer", "causal_discovery", "expectation_maximization", "ensemble_methods", "bayesian_optimization", "genetic_algorithm", "particle_swarm", "metaheuristics", "digital_replicas", "program_synthesis", "evolution_of_heuristics", "meta_optimization"], "problems": ["automated_algorithm_design", "heuristic_evolution", "simulation_optimization", "multi_sku_base_stock_optimization", "multi_server_queuing_network_resource_routing"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "llm_driven_meta_optimizer", "specific_domain": null, "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Automated Design of Simulation Optimization Algorithms for Complex Stochastic Systems", "short": "Auto-SO Algorithm Design", "class_": "algorithm_design", "properties": ["complex", "expensive-to-sample", "stochastic", "adaptive", "tailored"], "scale": "general complex stochastic systems"}, "lineage": {"direct_ancestors": [{"paper": "Optimizing optimization for design optimization", "relationship": "builds on the concept of meta-optimization"}, {"paper": "arXiv:2402.01207", "relationship": "leverages efficient causal graph discovery using LLMs"}, {"paper": "arXiv:2503.11720", "relationship": "incorporates adaptive-feedback loop similar to Rich Preference Optimization"}, {"paper": "Evolution of heuristics: Towards efficient automatic algorithm design using large language model", "relationship": "builds on the paradigm of LLM-driven methodological discovery and program synthesis"}], "closest_prior_work": "Optimizing optimization for design optimization", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Investigate alternative LLM architectures and prompting strategies for causal discovery and schedule revision.", "Extend the OAR framework to handle dynamic system changes and real-time data streams more robustly.", "Develop theoretical guarantees for the performance and convergence of LLM-driven meta-optimization.", "Automate the selection and parameter tuning of baseline SO algorithms within the schedule."], "transferable_to": ["Automated algorithm configuration (Auto-ML for operations research).", "Design of control policies for complex adaptive systems.", "Digital twin development for various industrial and scientific domains.", "Resource allocation and scheduling problems in dynamic environments."], "open_weaknesses": ["Vulnerability to LLM hallucinations in causal skeleton inference and schedule revision.", "Potential for OARs to mismatch the real system, despite online adaptation.", "Computational expense of generating and evaluating multiple OARs and iterative schedules.", "Generalizability of learned schedules to entirely new problem instances or domains not seen during OAR training."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.94, "experiments": {"benchmarks": ["Multi-SKU Single-Echelon Warehouse and Base-Stock Optimization", "Multi-Server Queuing Network and Resource–Routing Optimization"], "baselines": ["BO-EI", "BO-UCB", "BO-PI", "GA", "PSO"], "hardware": "null", "instance_sizes": []}, "results": {"vs_baselines": {"BO-EI": "Mean cost 28.27 +/- 1.16", "BO-UCB": "Mean cost 28.50 +/- 0.89", "BO-PI": "Mean cost 28.20 +/- 1.22 (best single algorithm baseline)", "GA": "Mean cost 28.68 +/- 1.79", "PSO": "Mean cost 43.97 +/- 19.90 (highest variance)", "SOCRATES_best_schedule": "Mean cost 26.52 +/- 0.85 (6.0% reduction vs best single baseline BO-PI)"}, "scalability": "The paper states that Operational AI Replicas (OARs) are computationally efficient and scalable for evaluating algorithms, but does not explicitly discuss the scalability of the overall SOCRATES method with respect to the size of the original stochastic system.", "statistical_rigor": "Results are reported as mean and one-standard-deviation error across 5 independent runs. The learning framework involves multiple outer runs, epochs, and intra-epoch revision steps.", "limitations_acknowledged": ["Potential mismatches between the learned OAR and the real system (addressed by online adaptation)", "LLMs cannot always strictly follow all requirements, necessitating admissibility checks"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2510.27610", "arxiv_url": "https://arxiv.org/abs/2510.27610", "title": "ORGEval: Graph-Theoretic Evaluation of LLMs in Optimization Modeling", "authors": ["Zhuohan"], "abstract": "", "published_date": "2025-10-31", "affiliations": "The Chinese University of Hong Kong, Shenzhen, Shenzhen Research Institute of Big Data, Shenzhen International Center for Industrial and Applied Mathematics, Shenzhen Loop Area Institute", "category": "Generative AI for OR", "relevance": {"methodological": 8, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper introduces a structural evaluation method (graph isomorphism) that replaces expensive solver execution for checking model correctness. This directly addresses the evaluation bottleneck in our 'OR benchmarking' and 'LLM reasoning evaluation' work, offering a way to validate generated models in seconds rather than hours."}, "brief": "Wang et al. propose ORGEval, a framework that evaluates LLM-generated optimization models by converting them into bipartite graphs and using the Weisfeiler-Lehman (WL) test to detect isomorphism with a ground truth, rather than solving the instances. They prove that for 'symmetric decomposable' graphs, this method is guaranteed to detect equivalence correctly, achieving 100% consistency and running in seconds compared to hours for solver-based checks on hard MIPLIB instances. The critical takeaway is the shift from execution-based to **structural evaluation**: we can validate model logic via graph topology ($O(k(m+n)^2)$) without incurring the cost of solving NP-hard problems. This is immediately actionable for our OR benchmarking pipelines and could serve as a rapid 'pre-solve' filter in our evolutionary search loops to reject structurally invalid candidates instantly.", "methodology": {"core_method": "Graph-theoretic evaluation framework using Weisfeiler-Lehman (WL) test with Symmetric Decomposable (SD) graph condition for model isomorphism detection", "llm_role": "none", "llm_model_used": null, "search_type": "sampling", "novelty_claim": "ORGEval is a graph-theoretic evaluation framework that assesses LLMs' optimization modeling capabilities by reducing model equivalence to graph isomorphism testing, identifying and proving a sufficient condition (symmetric decomposable graphs) for the Weisfeiler-Lehman test's correctness.", "components": ["Bipartite graph representation", "Weisfeiler-Lehman (WL) test", "Symmetric Decomposable (SD) detection algorithm", "Hashing for node coloring"], "training_required": false}, "tags": {"methods": ["graph_theory", "weisfeiler_lehman_test", "graph_isomorphism", "bipartite_graphs", "hashing", "symmetric_decomposable_graphs"], "problems": ["llm_evaluation", "milp_modeling", "lp_modeling", "model_equivalence_detection"], "contribution_type": ["new_method", "new_benchmark", "empirical_study", "theoretical_result", "framework"], "framework_lineage": "wl_test_for_milp_graphs", "specific_domain": "llm_optimization_modeling_evaluation", "llm_coupling": null}, "problem": {"formal_name": "Optimization Model Equivalence Detection for Linear and Mixed-Integer Linear Programs", "short": "MILP/LP Model Equivalence", "class_": "llm_evaluation", "properties": ["linear", "mixed_integer", "graph_isomorphism", "model_data_separation", "symmetric_decomposable_graphs"], "scale": "1848-17050 variables+constraints"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2209.12288", "relationship": "extends graph representation and WL-test application for MILP/LP"}], "closest_prior_work": "On representing linear programs by graph neural networks (Chen et al., 2022b)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Extend ORGEval to non-linear or stochastic optimization models.", "Develop methods to provide more granular feedback to LLMs for error correction.", "Investigate the limits of the symmetric decomposable condition and explore alternatives for non-SD graphs.", "Integrate ORGEval into an iterative LLM refinement loop for automated model improvement."], "transferable_to": ["Automated verification of mathematical model transformations.", "Evaluating code generation for other structured programming tasks.", "Debugging and quality assurance for human-written optimization models.", "Benchmarking other AI systems that generate structured code or models."], "open_weaknesses": ["Model isomorphism is a strict equivalence; some functionally equivalent but structurally different models might be missed.", "Reliance on random sampling to obtain symmetric decomposable instances for theoretical guarantees.", "Does not evaluate the practical performance or solution quality of the generated models, only structural correctness.", "Limited to linear and mixed-integer linear programs."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": true}, "front_id": "generative_ai_for_or_2026-02-18_front_1", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.11, "experiments": {"benchmarks": ["Bench4Opt", "MIPLIB"], "baselines": ["Solver-based evaluation (optimal value comparison)", "Canonical accuracy"], "hardware": "Not specified for ORGEval; solvers require significant CPU resources (e.g., hundreds and thousands of CPUs)", "instance_sizes": [1848, 10463, 17050]}, "results": {"vs_baselines": {"Solver-based evaluation": "100% consistency vs 94.11% average consistency on LLM-generated models; runtime 0.21s-32.07s vs >1 hour for hard/open problems."}, "scalability": "ORGEval consistently produces evaluation results within 30 seconds on average, even for large and challenging problems (up to 17050 variables+constraints) where solvers require over an hour or fail to solve.", "statistical_rigor": "Evaluated consistency across five random data configurations for all models in Bench4Opt, achieving 100% consistency. Sampled 75 MIPLIB instances (25 easy, 25 hard, 25 open) for efficiency tests.", "limitations_acknowledged": ["Model isomorphism is a stricter form of equivalence than model-lossless-reduction, which may not capture all forms of functional equivalence."]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2510.18428", "arxiv_url": "https://arxiv.org/abs/2510.18428", "title": "AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library", "authors": ["Minwei"], "abstract": "", "published_date": "2025-10-21", "affiliations": "Massachusetts Institute of Technology, London School of Economics and Political Science, University of Florida, Northeastern University, Singapore Management University, Singapore-MIT Alliance for Research and Technology", "category": "Generative AI for OR", "relevance": {"methodological": 8, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper sits exactly at the intersection of our two primary interests: OR formulation (OR-Bench) and evolutionary memory systems (AlgoEvo). The 'Library Evolution' mechanism—specifically refining retrieval conditions based on solver feedback—offers a concrete solution to the 'memory pollution' and 'misapplication' problems we face in long-running evolutionary search."}, "brief": "AlphaOPT introduces a 'Library Evolution' mechanism that iteratively refines the *applicability conditions* of cached optimization insights based on solver feedback, allowing it to learn from answers alone (no gold programs). On OOD benchmarks like OptiBench, it beats fine-tuned models (ORLM) by ~13% and shows consistent scaling with data size. **Key Takeaway:** The specific mechanism of diagnosing 'unretrieved' vs. 'negative' tasks to rewrite retrieval triggers is a transferable technique for our AlgoEvo memory; it solves the problem of heuristic misapplication in long-term search. We should implement this 'condition refinement' loop immediately to improve our multi-agent memory systems.", "methodology": {"core_method": "Self-improving experience library framework with a continual two-phase cycle: Library Learning (insight extraction and consolidation) and Library Evolution (applicability condition refinement)", "llm_role": "Research agent for insight extraction, condition refinement, and program generation, operating within an evolutionary library learning framework", "llm_model_used": "GPT-4o", "search_type": "improvement", "novelty_claim": "AlphaOPT is the first experience-library learning framework for natural language optimization formulation tasks, formally grounded in a mathematical view, capable of learning solely from answers without requiring gold-standard programs.", "components": ["Library Learning phase", "Library Evolution phase", "Insight Extraction", "Library Storage and Retrieval", "Library Diagnosis", "Library Refinement", "Hierarchical Taxonomy", "Solver-guided self-exploration"], "training_required": false}, "tags": {"methods": ["llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "llm_evolutionary_search", "llm_as_evaluator", "retrieval_augmented_generation", "program_synthesis", "evolution_of_heuristics", "self_improving_search"], "problems": ["optimization_program_formulation", "milp_general", "nlp_general", "combinatorial_optimization", "heuristic_evolution"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": null, "specific_domain": "optimization_program_formulation", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Optimization Program Formulation", "short": "OPF", "class_": "optimization_modeling", "properties": ["natural_language_input", "mathematical_formulation_output", "solver_code_generation", "diverse_problem_types", "ambiguous_specifications"], "scale": "454 problem instances across multiple benchmarks"}, "lineage": {"direct_ancestors": [{"paper": "Reflexion (Shinn et al., 2023)", "relationship": "adapts experience learning from"}, {"paper": "STaR (Zelikman et al., 2022)", "relationship": "adapts experience learning from"}, {"paper": "ExpeL (Zhao et al., 2024)", "relationship": "adapts experience learning from"}, {"paper": "AlphaEvolve (Novikov et al., 2025)", "relationship": "adapts LLM-driven evolutionary methods from"}], "closest_prior_work": "ExpeL (Zhao et al., 2024)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["reasoning_oriented_test_time_scaling", "strengthening_datasets_with_real_world_problems", "improving_formulation_efficiency"], "transferable_to": ["complex_combinatorial_optimization", "large_scale_optimization", "dynamic_optimization", "multi_level_spatiotemporal_logic_problems"], "open_weaknesses": ["improving_retrieval_precision", "clearer_explanations_and_examples_for_insights", "better_llm_adaptation_to_unseen_scenarios", "lack_of_cross_structural_transfer_and_context_adaptation"]}, "artifacts": {"code_url": "https://github.com/Minw913/AlphaOPT", "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.11, "experiments": {"benchmarks": ["NLP4LP", "NL4OPT", "IndustryOR", "MAMO (ComplexLP)", "LogiOR", "Optibench"], "baselines": ["Standard", "Reflexion", "OptiMUS", "ORThought", "ORLM", "LLMOPT"], "hardware": "GPT-4o (OpenAI 2024) with temperature 0", "instance_sizes": [242, 289, 82, 111, 403, 92]}, "results": {"vs_baselines": {"ORThought": "AlphaOPT (self-exploration) +7.7% on OptiBench", "ORLM": "AlphaOPT (full) +13.6% on OptiBench", "LLMOPT": "AlphaOPT (full) +25.4% on OptiBench, but -13.7% on NLP4LP (in-distribution)"}, "scalability": "AlphaOPT steadily improves with increasing data size (65% to 72% MacroAvg from 100 to 300 training items) while maintaining a compact library.", "statistical_rigor": "Experiments use success rate as the primary evaluation metric, with three independent trials per task to mitigate stochasticity.", "limitations_acknowledged": ["Retrieval precision can still be improved through enhanced semantic disambiguation and structural filtering.", "Insights with high invalid proportions suggest the need for clearer explanations and better-designed examples.", "Strengthening the LLM’s ability to adapt and generalize retrieved insights to unseen, complex optimization scenarios.", "Current system lacks robust cross-structural transfer and context adaptation capabilities.", "Primarily focuses on static, linear formulations."]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2506.15707", "arxiv_url": "https://arxiv.org/abs/2506.15707", "title": "Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling", "authors": ["Xinglin"], "abstract": "", "published_date": "2025-10-20", "affiliations": "Beijing Institute of Technology, Xiaohongshu Inc", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 8, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper provides a theoretically grounded, embedding-based mechanism to solve the 'redundancy' problem in LLM search, which is the exact bottleneck in our AlgoEvo and code generation pipelines. It reframes diversity preservation as optimal resource allocation, directly applicable to our sample efficiency goals."}, "brief": "Wang et al. introduce Direction-Oriented Resource Allocation (DORA), which uses embedding-based soft clustering to group semantically similar reasoning paths and allocates compute budget to distinct 'directions' rather than individual solutions. They prove solution-level allocation (like REBASE) is suboptimal when paths are correlated and show DORA achieves state-of-the-art accuracy on MATH500 with 3.5x fewer FLOPs. **Key Takeaway:** We can immediately steal the 'semantic uniqueness reweighting' mechanism for AlgoEvo. By clustering generated heuristics via embeddings before expensive evaluation, we can drastically improve sample efficiency and stop wasting compute on minor variations of the same code.", "methodology": {"core_method": "Direction-Oriented Resource Allocation (DORA)", "llm_role": "reasoning_path_generator", "llm_model_used": "Llama-3.2-1B-Instruct, Llama-3.2-3B-Instruct, Qwen2.5-1.5B-Instruct, LLaMA-3.1-8B-Instruct", "search_type": "improvement", "novelty_claim": "First to formulate test-time search as an optimal resource allocation problem and propose DORA, a provably optimal direction-level allocation strategy that decouples direction quality from candidate count.", "components": ["policy model", "Process Reward Model (PRM)", "resource allocation strategy", "voting method", "embedding model", "semantic similarity matrix", "softmax with temperature"], "training_required": false}, "tags": {"methods": ["direction_oriented_resource_allocation", "dora", "resource_allocation", "bayesian_optimization", "convex_optimization", "llm_in_the_loop", "llm_as_heuristic", "process_reward_model", "embedding_model", "semantic_similarity", "soft_clustering", "beam_search", "temperature_sampling", "diverse_verifier_tree_search", "reward_balanced_search", "rebase"], "problems": ["mathematical_reasoning", "llm_test_time_scaling", "llm_inference_optimization", "resource_allocation"], "contribution_type": ["new_method", "sota_result", "theoretical_result", "empirical_study", "framework"], "framework_lineage": "rebase", "specific_domain": "mathematical_reasoning", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Optimal Resource Allocation for Test-Time Search", "short": "TTS Resource Allocation", "class_": "resource_allocation", "properties": ["optimal", "Bayesian", "uncertainty-aware", "direction-oriented", "mathematical_reasoning"], "scale": "rollout budgets of 16-256"}, "lineage": {"direct_ancestors": [{"paper": "Inference scaling laws: An empirical analysis of compute-optimal inference for llm problem-solving", "relationship": "improves upon solution-level allocation of REBASE"}, {"paper": "Scaling llm test-time compute optimally can be more effective than scaling model parameters", "relationship": "builds on parallel search strategies for LLM test-time scaling"}, {"paper": "Scaling test-time compute with open models", "relationship": "builds on parallel search strategies for LLM test-time scaling"}], "closest_prior_work": "Inference scaling laws: An empirical analysis of compute-optimal inference for llm problem-solving", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Incorporate alternative intermediate feedback signals beyond PRM.", "Develop allocation strategies that adapt to increasing confidence during multi-step reasoning.", "Extend DORA to other complex reasoning tasks beyond mathematics.", "Investigate integration with dynamic decomposition or branch-and-prune strategies."], "transferable_to": ["Code generation and program synthesis.", "Scientific discovery tasks requiring multi-step reasoning.", "Any LLM application benefiting from efficient test-time search with a fixed budget.", "General resource allocation problems under uncertainty."], "open_weaknesses": ["Reliance on the availability and quality of a Process Reward Model (PRM).", "Theoretical analysis assumes a low-confidence setting, which may not always hold.", "Potential computational overhead from the embedding model (though claimed efficient).", "Sensitivity to temperature hyperparameters (Ts, Tb) in certain ranges."]}, "artifacts": {"code_url": "https://github.com/WangXinglin/DORA", "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_3", "front_status": "growing", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.16, "experiments": {"benchmarks": ["MATH500", "AIME2024", "AIME2025", "HMMT24", "HMMT25", "AMC23", "AMC24"], "baselines": ["Temperature Sampling", "Beam Search", "Diverse Verifier Tree Search (DVTS)", "Reward Balanced Search (REBASE)"], "hardware": "32 NVIDIA A100 GPUs (40G)", "instance_sizes": [16, 32, 64, 128, 256]}, "results": {"vs_baselines": {"Temperature Sampling": "DORA consistently outperforms Temperature Sampling in accuracy.", "Beam Search": "DORA consistently outperforms Beam Search in accuracy.", "DVTS": "DORA consistently outperforms DVTS in accuracy.", "REBASE": "DORA consistently outperforms REBASE in accuracy, achieving higher accuracy with 3.5x fewer FLOPs and 4x lower latency at 64 rollouts compared to REBASE at 256 rollouts."}, "scalability": "The performance gap between DORA and REBASE widens as the rollout budget increases, with DORA achieving stronger performance with substantially less compute.", "statistical_rigor": "Experiments on MATH500 were repeated five times, and on AIME2024/AIME2025 ten times, reporting average performance. FLOPs/latency results are mean (standard deviation) over three runs.", "limitations_acknowledged": ["relies on a process reward model (PRM) being available", "theoretical analysis assumes a low-confidence setting"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2510.17015", "arxiv_url": "https://arxiv.org/abs/2510.17015", "title": "Justitia: Fair and Efficient Scheduling for LLM Applications", "authors": ["Mingyan"], "abstract": "", "published_date": "2025-10-19", "affiliations": "Shanghai Jiao Tong University", "category": "OR for Generative AI", "relevance": {"methodological": 7, "problem": 9, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper directly addresses the core problem of your 'GPUSched' project (scheduling LLM applications/agents). It provides a concrete, theoretically grounded heuristic (Virtual Time Fair Queuing) and a specific cost metric that outperforms current baselines."}, "brief": "Justitia introduces a scheduler for LLM agents that prioritizes applications based on their 'virtual finish time' (derived from a theoretical fair-sharing model) but executes them with full resource saturation to minimize completion time. The authors demonstrate a ~60% reduction in average job completion time compared to state-of-the-art fair schedulers (VTC) on vLLM, backed by rigorous experiments and theoretical delay bounds. The key takeaway is the 'KV token-time' cost metric (pd + d^2/2) which accurately captures memory bottlenecks in auto-regressive generation, and the insight that 'long-term fairness' allows for short-term resource saturation. This is immediately actionable for your GPUSched project and relevant for optimizing the serving infrastructure of AlgoEvo.", "methodology": {"core_method": "Virtual-time based fair queuing with memory-centric cost modeling and MLP-based demand prediction", "llm_role": "none", "llm_model_used": null, "search_type": "constructive", "novelty_claim": "Justitia is a novel scheduler for LLM applications that uses memory-centric cost modeling, MLP-based demand prediction, and virtual-time based fair queuing to achieve fair and efficient scheduling with worst-case delay guarantees.", "components": ["Memory-centric cost modeling", "MLP-based demand prediction", "Virtual-time based fair queuing", "TF-IDF vectorization"], "training_required": true}, "tags": {"methods": ["fair_queuing", "virtual_time_scheduling", "memory_centric_cost_modeling", "mlp", "demand_prediction", "tf_idf", "non_preemptive_scheduling"], "problems": ["llm_application_scheduling", "gpu_scheduling", "cloud_scheduling", "kv_cache_optimization", "resource_starvation"], "contribution_type": ["new_method", "framework", "sota_result", "theoretical_result", "empirical_study"], "framework_lineage": null, "specific_domain": "llm_application_scheduling", "llm_coupling": null}, "problem": {"formal_name": "Fair and Efficient Scheduling for LLM Applications", "short": "LLM Application Scheduling", "class_": "scheduling", "properties": ["fair", "efficient", "memory-centric", "application-level", "non-preemptive", "KV_cache_bottleneck_aware", "auto-regressive_workload"], "scale": "300-500 LLM applications, LLaMA-7B/13B models, up to 100 APP/min arrival rate"}, "lineage": {"direct_ancestors": [{"paper": "vLLM [12]", "relationship": "implemented atop"}, {"paper": "VTC [10]", "relationship": "improves upon state-of-the-art fair scheduling"}, {"paper": "A generalized processor sharing approach to flow control in integrated services networks: the single-node case [14]", "relationship": "borrows virtual-time based fair queuing algorithm from network scheduling"}], "closest_prior_work": "Fairness in Serving Large Language Models [10]", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Address memory fragmentation in theoretical analysis and implementation", "Extend to larger, distributed GPU clusters for global queue management", "Integrate with other LLM acceleration techniques (e.g., FlashAttention, quantization)", "Explore more sophisticated demand prediction models for evolving workloads"], "transferable_to": ["Scheduling for other auto-regressive models (e.g., diffusion models)", "General cloud resource scheduling with dynamic, variable demands", "Network packet scheduling (potential for improvements to transfer back)", "Scheduling for other AI workloads with dynamic resource requirements"], "open_weaknesses": ["Theoretical analysis neglects memory fragmentation problem", "Potential for demand prediction errors to impact scheduling order in extreme cases", "Scalability to extremely large and heterogeneous clusters may introduce new challenges", "Assumptions about prompt size relative to KV cache might not hold for all future models/workloads"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_6", "front_status": "emerging", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.24, "experiments": {"benchmarks": ["Mooncake dataset", "MapReduce Summarization (MRS)", "Plan-and-Execution (PE)", "Code Checking (CC)", "Knowledge-Based-Query-Answering Verification (KBQAV)", "Equation Verification (EV)", "Fact Verification (FV)", "ALFWorld Interaction (ALFWI)", "Document Merging (DM)", "Self Consistency (SC)"], "baselines": ["vLLM [12]", "vLLM-SJF [26]", "Parrot [7]", "VTC [10]", "SRJF"], "hardware": "single NVIDIA A100 PCIe 40 GB GPU, four 16-core AMD EPYC 7302 CPUs, 128 GB RAM; four Tesla V100 PCIe 16 GB GPUs, four 20-core Intel Xeon Gold 6133 CPUs, 258 GB RAM", "instance_sizes": [300, 500]}, "results": {"vs_baselines": {"Parrot": "61.1% better average JCT", "VTC": "57.5% better average JCT, 92% applications complete no later than VTC, worst-case delay 26.0%", "SRJF": "very close average JCT, avoids starvation of large applications", "Justitia/C": "up to 42.3% JCT performance degradation compared to Justitia", "Justitia-S3": "significantly higher average relative error (452% vs 53%), inference overhead (55.7ms vs 2.16ms), and JCT (366.7s vs 151.1s) compared to Justitia"}, "scalability": "Scheduling overhead remains consistently low (under 10 ms) up to 100 APP/min arrival rate, and delay is bounded regardless of competing applications.", "statistical_rigor": "Experiments conducted with 300 LLM applications, MLP trained on 100 samples per application, and demand predictability tested over 100 trial runs. CDFs are presented.", "limitations_acknowledged": ["Assumes prompt size is much smaller than total KV cache space, neglecting fragmentation problem for theoretical analysis."]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2510.16916", "arxiv_url": "https://arxiv.org/abs/2510.16916", "title": "SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search", "authors": ["Dong"], "abstract": "", "published_date": "2025-10-19", "affiliations": "NEC Labs America, Baylor University, University of Texas at Dallas, Augusta University, Southern Illinois University", "category": "Generative AI for OR", "relevance": {"methodological": 8, "problem": 7, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "The paper introduces two specific mechanisms—Prompt Backpropagation and Uncertainty Backpropagation—that directly address the 'noisy reward' and 'search guidance' bottlenecks in our AlgoEvo and MASPRM projects. It demonstrates how to integrate textual feedback loops into tree search, which is transferable to our evolutionary frameworks."}, "brief": "SolverLLM frames optimization problem formulation as a hierarchical Monte Carlo Tree Search (MCTS), decomposing the task into six layers (variables, constraints, etc.) and using test-time compute to beat fine-tuned baselines like LLMOPT. The results appear robust, showing ~10% gains on complex datasets, though inference cost is high. **The critical takeaway for us is the 'Prompt Backpropagation' mechanism:** instead of just updating numerical values, they propagate textual error analysis from leaf nodes back up the tree to dynamically modify the prompts of parent nodes, effectively creating 'short-term memory' for the search. We should immediately test this technique in AlgoEvo to prevent the recurrence of failed code patterns during mutation steps. Additionally, their use of semantic entropy to down-weight uncertain rewards in MCTS is a practical solution to the noisy evaluation problem we face in process reward models.", "methodology": {"core_method": "LLM-guided Monte Carlo Tree Search (MCTS) with dynamic expansion, prompt backpropagation, and uncertainty backpropagation for optimization problem formulation and code generation", "llm_role": "decomposition_guide, heuristic_generator, evaluator, code_writer, evolutionary_search", "llm_model_used": "GPT-4o", "search_type": "improvement", "novelty_claim": "SolverLLM is a training-free framework that leverages test-time scaling to solve diverse optimization problems by generating mathematical formulations and translating them into solver-ready code, guided by a novel Monte Carlo Tree Search (MCTS) strategy with dynamic expansion, prompt backpropagation, and uncertainty backpropagation.", "components": ["Monte Carlo Tree Search", "Dynamic Expansion", "Prompt Backpropagation", "Uncertainty Backpropagation", "Type Element", "Element-Based Formulation (Type, Sets, Parameters, Variables, Objective, Constraints)", "LLM-based Evaluator", "Error Backpropagation for Code Generation", "Pruning Module"], "training_required": false}, "tags": {"methods": ["monte_carlo_tree_search", "llm_as_heuristic", "llm_as_evaluator", "llm_code_generation", "llm_in_the_loop", "llm_prompt_optimization", "program_synthesis", "test_time_scaling"], "problems": ["optimization_problem_formulation", "linear_programming", "mixed_integer_linear_programming", "traveling_salesman_problem"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "autoformulation", "specific_domain": "optimization_problem_formulation", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Optimization Problem Formulation", "short": "OPF", "class_": "optimization_formulation", "properties": ["training_free", "test_time_scaling", "LLM_guided", "MCTS_based", "code_generation", "formulation_generation"], "scale": "Problems with 3-10+ variables/constraints"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2411.01679", "relationship": "extends test-time scaling and MCTS for formulation from"}, {"paper": "arXiv:2410.13213", "relationship": "inspired element-based formulation and error backpropagation from"}, {"paper": "arXiv:2303.05510", "relationship": "builds on MCTS for code generation/reasoning from"}], "closest_prior_work": "arXiv:2411.01679", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["extend to more complex optimization settings", "explore hybrid inference paradigms", "improve robustness under ambiguous or noisy natural language descriptions"], "transferable_to": ["operations_research", "supply_chain_management", "education", "general_mathematical_modeling"], "open_weaknesses": ["high inference latency on complex problems", "subjectivity and non-determinism of LLM-based reward estimation", "limited evaluation on highly ambiguous or adversarial problem descriptions"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.94, "experiments": {"benchmarks": ["NL4Opt", "MamoEasy", "MamoComplex", "NLP4LP", "ComplexOR", "IndustryOR"], "baselines": ["GPT-4 Directly", "GPT-4o Directly", "Reflexion", "Chain-of-Experts", "OptiMUS", "AutoFormulation", "ORLM-Mistral", "ORLM-Deepseek", "ORLM-LLaMa3", "LLMOPT"], "hardware": "null", "instance_sizes": []}, "results": {"vs_baselines": {"GPT-4 Directly": "+49.7% on NL4Opt, +51.2% on NLP4LP, +68.3% on ComplexOR, +29.5% on MamoEasy, +61.4% on MamoComplex, +28.0% on IndustryOR", "GPT-4o Directly": "+16.0% on NL4Opt, +54.6% on NLP4LP, +50.5% on ComplexOR, +5.0% on MamoEasy, +42.0% on MamoComplex, +22.0% on IndustryOR", "Reflexion": "+44.0% on NL4Opt, +40.7% on NLP4LP, +58.7% on ComplexOR", "Chain-of-Experts": "+32.8% on NL4Opt, +33.9% on NLP4LP, +39.7% on ComplexOR", "OptiMUS": "+18.2% on NL4Opt, +15.0% on NLP4LP, +11.1% on ComplexOR", "ORLM-Mistral": "+12.6% on MamoEasy, +12.6% on NL4Opt, +44.0% on MamoComplex, +29.0% on IndustryOR", "ORLM-Deepseek": "+13.8% on MamoEasy, +10.5% on NL4Opt, +38.1% on MamoComplex, +23.0% on IndustryOR", "ORLM-LLaMa3": "+13.7% on MamoEasy, +11.3% on NL4Opt, +38.6% on MamoComplex, +18.0% on IndustryOR", "LLMOPT": "-1.0% on MamoEasy, +4.0% on NL4Opt, +8.0% on MamoComplex, +10.0% on IndustryOR", "AutoFormulation": "+7.0% on MamoEasy, +4.0% on NL4Opt, +10.0% on NLP4LP, +11.1% on ComplexOR, +39.0% on MamoComplex, +9.0% on IndustryOR"}, "scalability": "SolverLLM consistently achieves higher solving accuracy while consuming fewer tokens across all search budgets compared to AutoFormulation, but may incur relatively high inference latency due to its reliance on MCTS, especially on complex problems with large formulation spaces.", "statistical_rigor": "The mean of solving accuracy and execution rate is reported over 3 runs for each method across all benchmark datasets.", "limitations_acknowledged": ["High inference latency due to MCTS on complex problems", "Effectiveness of reward estimation bounded by subjective and non-deterministic nature of LLMs", "Evaluation restricted to well-structured and moderately constrained optimization problems"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2510.14825", "arxiv_url": "https://arxiv.org/abs/2510.14825", "title": "Programmatic Representation Learning with Language Models", "authors": ["Gabriel"], "abstract": "", "published_date": "2025-10-16", "affiliations": "Harvard University, Stanford University", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 6, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "The D-ID3 algorithm introduces a 'local evolution' paradigm—generating code conditioned on specific data subsets (tree nodes)—which directly addresses the heterogeneity problem in our evolutionary search work. This is a structural advance over global FunSearch/AlphaEvolve loops."}, "brief": "The authors propose two algorithms, F2 (Features FunSearch) and D-ID3 (Dynamic ID3), to learn programmatic features for decision trees. D-ID3 is particularly novel: instead of evolving a global heuristic, it calls the LLM at *each split node* to generate a feature that discriminates the specific data subset at that leaf. Results are strong on Chess (matching Transformers trained on 250x more data) and Text, though the Image results (MNIST) are trivial. **Key Takeaway:** The D-ID3 architecture—using the solver's current state (leaf node data) to prompt the LLM for *local* code generation—is a powerful pattern we should steal for our VRP solvers (e.g., evolving local repair operators for specific route bottlenecks) and EvoCut work.", "methodology": {"core_method": "LLM-synthesized programmatic feature functions combined with decision tree predictors, using Features FunSearch (F2) and Dynamic ID3 (D-ID3) algorithms", "llm_role": "code_writer", "llm_model_used": "GPT 4o-mini, GPT 5-mini", "search_type": "hybrid", "novelty_claim": "We propose jointly learning programmatic features, represented as LLM-generated functions from the input domain to the reals, together with decision tree predictors, thus obtaining fast, interpretable predictors.", "components": ["LLM-generated Python functions (features)", "Decision Trees", "Random Forest (for F2 evaluation and final prediction)", "FunSearch-style loop (F2)", "ID3-style decision tree learning (D-ID3)"], "training_required": true}, "tags": {"methods": ["llm_code_generation", "llm_as_heuristic", "llm_in_the_loop", "funsearch", "evolution_of_heuristics", "id3_algorithm", "decision_trees", "random_forest", "representation_learning"], "problems": ["supervised_learning", "representation_learning", "chess_position_evaluation", "image_classification", "mnist", "fashion_mnist", "text_classification", "ai_text_detection", "model_debugging"], "contribution_type": ["new_method", "framework", "empirical_study", "sota_result"], "framework_lineage": "funsearch", "specific_domain": "representation_learning", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Learned Programmatic Representation Models", "short": "LeaPR", "class_": "representation_learning", "properties": ["interpretable", "neural_network_free", "code-based_features", "decision_tree_predictors", "high-dimensional_inputs"], "scale": "200k data points (LeaPR), 50M data points (Transformer baseline)"}, "lineage": {"direct_ancestors": [{"paper": "FunSearch", "relationship": "adapts and extends"}, {"paper": "ID3 algorithm", "relationship": "novel variant of"}], "closest_prior_work": "FunSearch", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["learn_deep_hierarchical_features", "improve_scalability_to_data_rich_domains", "develop_explainable_chess_policies"], "transferable_to": ["general_supervised_learning_tasks", "data_exploration", "model_debugging_for_other_domains", "scientific_discovery", "algorithmic_discovery"], "open_weaknesses": ["lack_of_deep_hierarchical_features", "scalability_limitations_for_large_datasets", "limited_experimental_scale", "limited_performance_scaling_with_data"]}, "artifacts": {"code_url": "https://github.com/gpoesia/leapr/", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.81, "experiments": {"benchmarks": ["Lichess", "MNIST", "Fashion-MNIST", "Ghostbuster", "Waterbird"], "baselines": ["Transformer (Ruoss et al., 2024)", "Random Forest (raw board)", "ResNet-50", "EfficientNetV2", "GPTZero", "RoBERTa", "Ghostbuster (Verma et al., 2024)", "Perplexity only"], "hardware": "CPU-only commodity machine (LeaPR), 4x H100 NVIDIA GPUs (Transformer), single H100 GPU (ResNet-50/EfficientNetV2)", "instance_sizes": [200000, 50000000]}, "results": {"vs_baselines": {"Transformer (Ruoss et al., 2024)": "LeaPR D-ID3 + GPT 5-mini achieves +3.2% move accuracy and LeaPR D-ID3 + GPT 4o-mini achieves -0.005 RMSE, +0.011 ρ on chess compared to Transformer (5e7)", "Random Forest (raw board)": "LeaPR D-ID3 + GPT 5-mini achieves +19.0% move accuracy on chess", "ResNet-50": "LeaPR D-ID3 + GPT 5-mini achieves -1.8% on MNIST, -1.03% on Fashion-MNIST", "EfficientNetV2": "LeaPR D-ID3 + GPT 5-mini achieves -1.89% on MNIST, -2.43% on Fashion-MNIST", "Random Forest (raw pixels)": "LeaPR D-ID3 + GPT 5-mini achieves +1.31% on MNIST, +0.22% on Fashion-MNIST", "Ghostbuster (Verma et al., 2024)": "LeaPR D-ID3 + GPT 5-mini achieves -0.2% F1 on text classification", "RoBERTa": "LeaPR D-ID3 + GPT 5-mini achieves +0.7% F1 on text classification", "GPTZero": "LeaPR D-ID3 + GPT 5-mini achieves +5.7% F1 on text classification", "Perplexity only": "LeaPR D-ID3 + GPT 5-mini achieves +17.3% F1 on text classification"}, "scalability": "Performance does not scale well beyond 200k training data points, unlike neural networks, and faces scalability challenges for data-rich domains.", "statistical_rigor": "Results are reported from single runs for LeaPR models, with baselines trained to convergence; no explicit statistical significance tests are mentioned.", "limitations_acknowledged": ["Lack of deep hierarchical feature learning", "Limited scalability to large, data-rich domains", "Small-scale experiments"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2510.11331", "arxiv_url": "https://arxiv.org/abs/2510.11331", "title": "Efficient LLM Inference over Heterogeneous Edge Networks with Speculative Decoding", "authors": ["Bingjie"], "abstract": "", "published_date": "2025-10-13", "affiliations": "Queen Mary University of London, Kyung Hee University, Xidian University, Guangzhou Institute of Technology", "category": "OR for Generative AI", "relevance": {"methodological": 5, "problem": 7, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": false, "reasoning": "While this is a competent OR formulation for LLM serving (a stated interest), the specific focus on wireless edge constraints and the simplifying assumption of fixed output lengths for the DP solver limit its direct transferability to our high-performance cloud scheduling or evolutionary search infrastructure."}, "brief": "Zhu et al. propose a distributed Speculative Decoding framework for edge networks, formulating a Mixed-Integer Nonlinear Programming problem to jointly optimize task batching, speculation length, and wireless bandwidth. They solve the batching subproblem using a Dynamic Programming (DP) algorithm, achieving ~30-45% latency reduction over heuristics in simulations, though the approach relies on a rigid assumption of fixed maximum output lengths to remain tractable. The primary takeaway for our 'GPUSched' work is their DP formulation for optimizing batch boundaries in a pipelined draft-verify system, which offers a cleaner mathematical alternative to greedy heuristics for serving schedules. However, the heavy reliance on wireless channel modeling makes the full system less relevant to our datacenter-centric optimization problems.", "methodology": {"core_method": "Speculative Decoding (SD) with pipeline parallelism, combined with joint optimization of speculation length, task batching, and wireless communication resource allocation", "llm_role": "inference engine", "llm_model_used": "LLaMA", "search_type": "hybrid", "novelty_claim": "A novel SD-based LLM inference framework for heterogeneous edge networks with pipeline parallelism and a joint optimization for speculation length, batching, and wireless resource allocation to minimize serving latency.", "components": ["Speculative Decoding", "Pipeline Parallelism", "Latency Model", "Dynamic Programming for Batching", "Closed-form Wireless Resource Allocation", "Speculation Length Enumeration"], "training_required": false}, "tags": {"methods": ["speculative_decoding", "pipeline_parallelism", "dynamic_programming", "wireless_resource_allocation", "task_batching", "optimization", "autoregressive_decoding"], "problems": ["llm_serving_optimization", "latency_minimization", "edge_llm_inference", "resource_allocation", "batch_scheduling"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": null, "specific_domain": "llm_serving_optimization", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Joint Optimization for Speculation Length, Task Batching, and Wireless Communication Resource Allocation to Minimize Total Serving Latency for SD-based LLM Inference at the Edge", "short": "SD-LLM Edge Inference", "class_": "LLM serving optimization", "properties": ["heterogeneous_edge_networks", "speculative_decoding", "pipeline_parallelism", "task_batching", "wireless_resource_allocation", "latency_minimization"], "scale": "100 tasks, 2 heterogeneous edge nodes, LLaMA models (68M-13B parameters), input up to 1792 tokens, output up to 2048 tokens"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2302.13971", "relationship": "uses LLaMA models as base LLMs"}, {"paper": "Fast inference from transformers via speculative decoding", "relationship": "adopts speculative decoding technique from"}, {"paper": "Speculative decoding with big little decoder", "relationship": "adopts speculative decoding technique from"}, {"paper": "SpecInfer: Accelerating Large Language Model Serving with Tree-Based Speculative Inference and Verification", "relationship": "improves upon fixed speculation length approaches from"}, {"paper": "Pipeinfer: Accelerating llm inference using asynchronous pipelined speculation", "relationship": "builds on pipelined speculation concepts from"}], "closest_prior_work": "SpecInfer: Accelerating Large Language Model Serving with Tree-Based Speculative Inference and Verification", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Explore dynamic adaptation of speculation length and batch size in real-time based on system load and task characteristics.", "Integrate other LLM acceleration techniques (e.g., quantization, model parallelism) within the SD framework for further efficiency.", "Develop methods to dynamically predict or optimize the token acceptance rate (alpha) for improved performance.", "Extend the framework to more complex heterogeneous edge network topologies and multi-cloud environments."], "transferable_to": ["Distributed inference for other large AI models (e.g., large vision models, multimodal models) on edge networks.", "Real-time AI applications with strict latency requirements, such as autonomous vehicles and robotic control.", "General multi-user, multi-task edge computing scenarios requiring efficient resource management.", "Cloud-edge collaborative inference systems where models are partitioned across different compute tiers."], "open_weaknesses": ["The current batching algorithm relies on a conservative maximum possible output length, which may not be optimal for all tasks.", "The assumption of negligible fronthaul transmission latency might not hold in all practical scenarios, especially with lower-speed links.", "The latency model for pipeline scheduling, while comprehensive, may not fully capture all real-world system overheads and complexities.", "The paper does not address the potential accuracy degradation if the draft model is too weak, or the trade-off between latency and accuracy."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_status": "growing", "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.55, "experiments": {"benchmarks": ["LLaMA-68M, LLaMA-7B", "LLaMA-1.1B, LLaMA-7B", "LLaMA-1.1B, LLaMA-13B"], "baselines": ["SD w/o Pipeline [35]", "No batching [5]", "Fixed speculation length (FSL) [20]", "AD-based serving mechanism (ADS)", "Static batching [9]", "Max batching [11]", "Heuristic batching [34]", "Uniform resource allocation scheme"], "hardware": "NVIDIA GeForce RTX 3080 GPU (SBS), NVIDIA RTX 4500 GPU (MBS)", "instance_sizes": [100, 1792, 2048]}, "results": {"vs_baselines": {"AD-based serving mechanism (ADS)": "Consistently lower latency across all model combinations and acceptance rates", "SD w/o Pipeline": "Up to 31.6% latency reduction (Imax=512), 30.7% (Omax=1792)", "FSL": "Lower latency across task numbers", "No Batching": "Lower latency across task numbers", "Max Batching": "Up to 21.4% latency reduction (K=90)", "Heuristic Batching": "Up to 19.6% latency reduction (Imax=1792), 20.5% (Omax=1024)", "Uniform resource allocation scheme": "Up to 44.9% latency reduction (LLaMA-68M, K=100, Bw=25MHz), 29.3% (LLaMA-1.1B), 25.2% (LLaMA-1.1B, LLaMA-13B)"}, "scalability": "Latency of all serving mechanisms exhibits an approximately linear increase with the task number, maximum input length, and maximum output length. Performance advantage of the proposed mechanism becomes more pronounced with increasing output length.", "statistical_rigor": "Not explicitly detailed, but 'extensive simulations' are mentioned. No variance or significance tests reported.", "limitations_acknowledged": []}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2510.10962", "arxiv_url": "https://arxiv.org/abs/2510.10962", "title": "MC#: Mixture Compressor for Mixture-of-Experts Large Models", "authors": ["Wei"], "abstract": "", "published_date": "2025-10-13", "affiliations": "NVIDIA Research, National University of Singapore, The University of Hong Kong, Beihang University, Hangzhou Innovation Institute", "category": "OR for Generative AI", "relevance": {"methodological": 6, "problem": 7, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While primarily a systems paper, it successfully applies Integer Programming (OR) to optimize LLM quantization, directly validating our 'OR for AI' research track. The learnable router mechanism is also transferable to our multi-agent routing problems."}, "brief": "Huang et al. propose MC#, a compression framework for MoE models that combines static mixed-precision quantization with dynamic expert pruning. They formulate bit-width allocation as an Integer Linear Programming (ILP) problem—optimizing expert importance vs. quantization error—and use a Gumbel-Softmax router for dynamic pruning. Results are strong, achieving 6.2x weight reduction on DeepSeek-VL2 with <2% accuracy loss. **Takeaway:** The ILP formulation (Eq. 7) is a clean, successful application of OR to AI infrastructure that we should replicate for our own resource allocation/scheduling problems; additionally, the differentiable router offers a template for dynamic agent selection in our multi-agent systems.", "methodology": {"core_method": "Hybrid compression combining Pre-Loading Mixed-Precision Quantization (PMQ) via Linear Programming and Online Top-any Pruning (OTP) via Gumbel-Softmax sampling", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "MC# is a unified framework combining static quantization and dynamic expert pruning by leveraging expert and token significance for aggressive MoE-LLM/VLM compression.", "components": ["Pre-Loading Mixed-Precision Quantization (PMQ)", "Online Top-any Pruning (OTP)", "Linear Programming (LP)", "Gumbel-Softmax sampling", "GPTQ quantization", "HQQ tool", "Learnable router"], "training_required": true}, "tags": {"methods": ["mixed_precision_quantization", "dynamic_expert_pruning", "linear_programming", "gumbel_softmax", "post_training_quantization", "gptq", "pruning", "model_compression", "binary_quantization", "learnable_mask", "hqq"], "problems": ["mixture_of_experts_compression", "llm_compression", "vlm_compression"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "mc#", "specific_domain": "mixture_of_experts_compression", "llm_coupling": null}, "problem": {"formal_name": "Mixture-of-Experts Large Model Compression", "short": "MoE Compression", "class_": "model_compression", "properties": ["sparse_activation", "multimodal", "mixed_precision_quantization", "dynamic_expert_pruning"], "scale": "3B-141B parameters"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2410.06270", "relationship": "extended from preliminary version"}, {"paper": "arXiv:2210.17323", "relationship": "uses GPTQ as foundational quantization tool"}], "closest_prior_work": "arXiv:2410.06270", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Adaptation for specific multimodal applications", "Optimization for specific hardware platforms", "Further improving performance on challenging reasoning and long-context tasks"], "transferable_to": ["Other Mixture-of-Experts (MoE) architectures", "Deployment on consumer-grade and edge-level applications", "Integration with other quantization techniques"], "open_weaknesses": ["Performance degradation on challenging tasks (e.g., GSM8K, HumanEval, Needle-in-a-haystack)", "Limitations of rule-based expert pruning for larger numbers of experts", "Challenges of uniform bit-width quantization under extreme compression ratios"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_34", "front_status": "emerging", "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.43, "experiments": {"benchmarks": ["C4", "M4", "WikiText2", "EleutherAI LM Harness", "VLMEvalKit", "GSM8K", "HumanEval", "Needle-in-a-haystack"], "baselines": ["GPTQ (uniform)", "Block Sparsity Predictor (BSP)", "Hessian-based quantization", "Rule-based ODP"], "hardware": "NVIDIA A100-80GB, NVIDIA RTX 3090", "instance_sizes": [3, 16, 27, 49, 141]}, "results": {"vs_baselines": {"GPTQ (uniform) 2.00-bit": "28.6%↓ on Mixtral 8x7b LM-Eval, 14.1%↓ on DeepSeek-VL2-L VLM-Eval, 29.6%↓ on DeepSeek-VL2-S VLM-Eval, 61.9%↓ on DeepSeek-VL2-T VLM-Eval", "BSP 2.54-bit": "PMQ 18.4% better on Mixtral 8x7b LM-Eval", "Hessian 2.54-bit": "PMQ 0.2% worse on Mixtral 8x7b LM-Eval, PMQ 4.7% better on DeepSeek-VL2-L VLM-Eval", "Rule-based ODP 2.05-bit": "PMQ+OTP achieves PPL 6.45 vs ODP's 6.62 on Mixtral 8x7b", "PMQ+OTP": "reduces activated parameters by 20-33% with ~1% accuracy loss"}, "scalability": "Quantization loss decreases as model size increases, showing greater benefits for larger MoE-VLMs; OTP scales robustly to more experts.", "statistical_rigor": "Calibrated on 128 random sequences (2048 tokens) for PMQ; OTP trained on 4096 random samples; throughput measured with 2048 input/512 output tokens.", "limitations_acknowledged": ["Performance drop more pronounced on challenging tasks (GSM8K, HumanEval, long-context NIAH)", "Future work needed for multimodal application adaptation and hardware optimization"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2510.11121", "arxiv_url": "https://arxiv.org/abs/2510.11121", "title": "Refining Hybrid Genetic Search for CVRP via Reinforcement Learning-Finetuned LLM", "authors": ["Rongjie"], "abstract": "", "published_date": "2025-10-13", "affiliations": "Nanyang Technological University, Singapore, Singapore Management University, Singapore, Nanjing University of Information Science and Technology, China", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 10, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper validates a critical hypothesis for our AlgoEvo work: that RL-finetuning small models on code execution feedback outperforms prompting SOTA models (GPT-4o) for heuristic discovery. It provides a concrete, successful recipe for 'RL-infused evolution' applied to our exact problem domain (HGS for CVRP)."}, "brief": "Zhu et al. fine-tune a Qwen-14B model using Reinforcement Learning (DAPO) to generate C++ crossover operators for the state-of-the-art HGS solver. Unlike typical prompting papers, they demonstrate that a small, specialized model can improve upon expert-designed components in a highly optimized solver, achieving superior results on CVRPLIB (up to 1000 nodes) where GPT-4o fails. The most stealable insight is their **AST-based anti-plagiarism reward**, which penalizes the model for generating code structurally identical to the prompt examples, effectively forcing exploration and preventing mode collapse—a technique we should immediately adopt for our evolutionary search agents. This confirms we should pivot from pure prompting to RL-finetuning for our code-generation agents.", "methodology": {"core_method": "Reinforcement Learning (DAPO) fine-tuning of LLM for crossover operator generation within Hybrid Genetic Search (HGS)", "llm_role": "heuristic_generator", "llm_model_used": "Qwen-14B reasoning LLM", "search_type": "improvement", "novelty_claim": "To the best of our knowledge, this work provides the first empirical evidence that a compact reasoning LLM (14B parameters) can be fine-tuned via RL to generate critical algorithmic components that exceed the performance of those in state-of-the-art, expert-engineered solvers.", "components": ["Hybrid Genetic Search (HGS)", "Reinforcement Learning (DAPO)", "Multi-tiered reward function", "Anti-Plagiarism Cache with Abstract Syntax Tree", "Incremental compilation", "Few-shot CoT prompting"], "training_required": true}, "tags": {"methods": ["genetic_algorithm", "hgs", "reinforcement_learning", "rl_dapo", "llm_as_heuristic", "llm_code_generation", "llm_fine_tuned", "llm_rl_trained", "llm_in_context_learning", "evolution_of_heuristics"], "problems": ["cvrp", "combinatorial_routing", "operator_discovery", "heuristic_evolution"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "hgs", "specific_domain": "cvrp", "llm_coupling": "rl_trained"}, "problem": {"formal_name": "Capacitated Vehicle Routing Problem", "short": "CVRP", "class_": "routing", "properties": ["capacitated"], "scale": "up to 1000 nodes"}, "lineage": {"direct_ancestors": [{"paper": "Hybrid Genetic Search (HGS)", "relationship": "refines the crossover operator within"}], "closest_prior_work": "HGS-PyVRP", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Explore LLM-generated mutation or local search operators for HGS.", "Investigate different LLM architectures or sizes for operator generation.", "Develop more robust validation and debugging mechanisms for generated code.", "Apply the RL-finetuning approach to other metaheuristic components."], "transferable_to": ["Other vehicle routing problem variants (e.g., VRPTW, PDPTW, MDVRP).", "Other combinatorial optimization problems (e.g., scheduling, bin packing).", "Other metaheuristic frameworks (e.g., ALNS, Tabu Search) for operator discovery."], "open_weaknesses": ["Potential for LLM-generated operators to contain bugs or be inefficient.", "Computational cost associated with RL fine-tuning and LLM inference during search.", "Generalizability of generated operators to problem instances significantly different from training data.", "Dependency on a specific LLM model (Qwen-14B) and its inherent biases."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.81, "experiments": {"benchmarks": ["CVRPLIB X instances"], "baselines": ["HGS-PyVRP800", "HGS-PyVRP1000", "OR-Tools", "LKH", "POMO", "MTPOMO", "MVMoE", "RF-POMO", "RF-MoE-L", "AM", "DeepACO", "NeuroLKH", "NeuOpt", "MCTS-AHD", "ReEvo", "GPT4o800", "GPT4o1000", "GPT-o3800", "GPT-o31000", "GPT-o4-mini800", "GPT-o4-mini1000"], "hardware": "Not specified, but mentions incremental compilation for speed", "instance_sizes": [100, 200, 400, 600, 800, 1000]}, "results": {"vs_baselines": {"HGS-PyVRP1000": "Outperforms by 0.03-0.08% gap across all instance sizes (n=100-1000)", "HGS-PyVRP800": "Slightly worse on n<200 (+0.08% gap), but better on n>=200 (up to -0.05% gap)", "GPT4o/o3/o4-mini": "Outperforms, as these LLMs match HGS-PyVRP performance but RFTHGS is superior", "OR-Tools": "Significantly outperforms (RFTHGS gap < 3% vs OR-Tools gap > 4%)", "LKH": "Significantly outperforms (RFTHGS gap < 3% vs LKH gap > 1%)", "NCO methods": "Significantly outperforms (e.g., RFTHGS gap < 3% vs POMO > 13%)", "Prompting-Based LLM methods": "Significantly outperforms (e.g., RFTHGS gap < 3% vs ReEvo > 72%)"}, "scalability": "Performance advantage is consistent from small-scale instances and generalizes effectively to large-scale problems of up to 1000 nodes, more than twice the size of training instances.", "statistical_rigor": "Uses a fixed set of 30 CVRP instances for training reward calculation. During testing, 16 operators are sampled, and the best one's performance is reported. Benchmarking is done on a fixed random seed over several CVRP instances.", "limitations_acknowledged": ["Operators generated by LLMs may contain bugs which, if deployed without thorough validation, could cause losses."]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2510.15969", "arxiv_url": "https://arxiv.org/abs/2510.15969", "title": "LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems", "authors": ["Paul-Niklas"], "abstract": "", "published_date": "2025-10-12", "affiliations": "Karlsruhe Institute of Technology, Reutlingen University", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 8, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper provides a concrete, successful architecture for automating symbolic OR reformulations, a key component for our 'OR-Bench' and automated modeling work. The strategy of mixing LLM agents with deterministic structural policies to handle nesting is a pattern we should adopt."}, "brief": "LinearizeLLM is a multi-agent framework that converts LaTeX nonlinear optimization problems into exact MILP formulations by detecting nonlinear terms and processing them bottom-up based on nesting depth. On 40 benchmark instances, it achieves 73% end-to-end success compared to <15% for one-shot LLMs and Pyomo baselines, demonstrating that structural decomposition is essential for handling complex nested terms. The key takeaway is the 'Structural Policy': rather than letting the LLM plan the reformulation order, they enforce a deterministic bottom-up traversal (linearizing children before parents). We should steal this hybrid approach—using deterministic graph traversal to orchestrate LLM manipulation steps—to improve reliability in our symbolic modeling and EvoCut pipelines.", "methodology": {"core_method": "Agent-based LLM framework with specialized reformulation agents and a depth-based processing policy", "llm_role": "decomposition_guide, reformulation_expert", "llm_model_used": "Gemini 2.5 Flash", "search_type": "constructive", "novelty_claim": "An agent-based LLM framework with specialized reformulation agents and a depth-based processing policy for exact linear reformulation of nonlinear optimization problems from LaTeX input.", "components": ["Detection agent", "Structural policy", "Reformulation agents", "Code generation agent"], "training_required": false}, "tags": {"methods": ["llm_research_agent", "multi_agent_system", "exact_linearization", "milp_reformulation", "pattern_detection", "rule_based_reformulation", "llm_in_the_loop", "llm_code_generation", "milp_solver", "modeling_language", "gdp_transformation", "bilinear_linearization", "min_max_linearization", "absolute_value_linearization", "linear_fractional_linearization", "monotone_transformation_linearization"], "problems": ["nonlinear_optimization_problem_linearization", "minlp", "bilinear_products", "min_operator", "max_operator", "absolute_value", "linear_fractional", "monotone_transformations"], "contribution_type": ["new_method", "framework", "new_benchmark", "empirical_study", "sota_result"], "framework_lineage": "linearizellm", "specific_domain": "nonlinear_optimization_problem_linearization", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Exact Linear Reformulation of Nonlinear Optimization Problems", "short": "NLOP Linearization", "class_": "optimization_reformulation", "properties": ["exact", "nonlinear", "mixed_integer", "bilinear_products", "min_max_operators", "absolute_value", "linear_fractional", "monotone_transformations"], "scale": "40 instances with varying nonlinearity depths (1 to >=3)"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2407.06000", "relationship": "builds on multi-agent LLM framework for OR problems"}, {"paper": "arXiv:2504.16918", "relationship": "builds on LLM-driven MILP formulation and refinement"}, {"paper": "arXiv:2504.16918", "relationship": "addresses open research direction of LLM-based nonlinear optimization problem reformulation"}], "closest_prior_work": "arXiv:2407.06000", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["extend to inexact linearization techniques (e.g., McCormick envelopes)", "expand the set of supported exact nonlinear patterns", "improve robustness to syntactic refactoring and macro definitions in LaTeX input", "further research into optimal context provision for LLM agents"], "transferable_to": ["general MILP/LP modeling from natural language descriptions", "integration into decomposition algorithms (e.g., Benders, column generation)", "other types of optimization problem reformulations (e.g., convexification, relaxation)", "making optimization tools more accessible to non-experts"], "open_weaknesses": ["currently handles a fixed set of nonlinear patterns", "does not support general nonlinear functions or relaxations when exact reformulations are not possible", "main automatic metric (OSR) does not certify argmin-set equivalence", "results are based on a limited dataset (40 instances) and a single LLM (Gemini 2.5 Flash)", "sensitivity to macro-based refactoring in LaTeX input"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": true}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.73, "experiments": {"benchmarks": ["ComplexOR-NL dataset"], "baselines": ["One-shot LLM", "Deterministic rules-based linearizer (Pyomo)"], "hardware": "AMD Ryzen Threadripper PRO 7955WX processor @ 4.5 GHz, 512 GB RAM, PNY NVIDIA T1000 8 GB graphics card", "instance_sizes": [40]}, "results": {"vs_baselines": {"One-shot LLM": "8.3x lower OSR on average across depths", "Deterministic rules-based linearizer (Pyomo)": "4.3x lower OSR on average across depths"}, "scalability": "LinearizeLLM is stable across depths, maintaining an OSR of 73.33% even at d >=3, while baselines struggle as nesting increases.", "statistical_rigor": "Each experiment is evaluated over three independent runs; results are averaged over 3 random seeds.", "limitations_acknowledged": ["fixed set of nonlinear patterns", "does not certify argmin-set equivalence", "limited to 40 instances and one LLM"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2510.06189", "arxiv_url": "https://arxiv.org/abs/2510.06189", "title": "Barbarians at the Gate: How AI is Upending Systems Research", "authors": ["Audrey"], "abstract": "", "published_date": "2025-10-10", "affiliations": "UC Berkeley", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 6, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper is a direct, high-profile validation of your core research thesis (LLM evolutionary search for systems/OR). It applies methods nearly identical to yours (OpenEvolve/AlphaEvolve) to problems exactly in your scope (GPU scheduling, MoE placement, cloud resource allocation) with strong empirical results."}, "brief": "The authors apply OpenEvolve (an AlphaEvolve-style framework) to 11 computer systems problems, achieving significant gains over human baselines, such as a 5.0x speedup in MoE expert placement and 26% cost reduction in cloud scheduling. The results are empirically rigorous, relying on high-fidelity simulators rather than toy problems. For us, the key takeaway is the engineering recipe: using an ensemble of reasoning models (o3) for exploration and fast models (Gemini) for diversity, combined with a specific 'failure taxonomy' to debug search stagnation. This is immediate proof-of-concept for your 'GPUSched' and 'AlgoEvo' projects; we should adopt their ensemble strategy and simulator-first evaluation pipeline.", "methodology": {"core_method": "LLM-driven evolutionary search (MAP-Elites and island models) with automated code generation and empirical evaluation", "llm_role": "code_writer, reasoning_agent, feedback_generator", "llm_model_used": "GPT-4o, o3, Gemini 2.5 Pro, Gemini 2.5 Flash, Gemini 2.5 Flash Lite, GPT-5, Claude Opus", "search_type": "evolutionary_search", "novelty_claim": "ADRS automates algorithm discovery and evaluation for systems performance problems by iteratively generating, evaluating, and refining solutions using LLMs and empirical verification.", "components": ["Prompt Generator", "Solution Generator", "Evaluator", "Storage", "Solution Selector"], "training_required": false}, "tags": {"methods": ["llm_as_heuristic", "llm_as_evaluator", "llm_code_generation", "evolutionary_search", "map_elites", "island_models"], "problems": ["algorithm_discovery", "systems_performance_optimization", "spot_instance_scheduling", "llm_serving_optimization", "transaction_scheduling", "telemetry_repair", "global_model_placement"], "contribution_type": ["new_method", "framework", "sota_result"], "framework_lineage": "funsearch", "specific_domain": "systems_performance_optimization", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Automated Algorithm Discovery for Systems Performance Optimization", "short": "ADRS", "class_": "algorithm_design", "properties": ["performance_optimization", "empirical_verification", "simulator_based_evaluation", "iterative_refinement"], "scale": "up to 500 transactions, Llama3.2-1B model, multi-region/multi-cloud configurations, multi-agent LLM systems"}, "lineage": {"direct_ancestors": [{"paper": "FunSearch", "relationship": "builds on the concept of LLM-driven evolutionary search for algorithm discovery"}, {"paper": "MAP-Elites", "relationship": "integrates MAP-Elites for diversity in evolutionary search"}], "closest_prior_work": "FunSearch", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Conduct systematic ablation studies for optimal LLM and evolutionary search configurations", "Develop methods to formalize intuitive trade-offs into numerical weights for evaluators", "Explore non-monolithic evolutionary searches for modular program synthesis", "Improve hyperparameter tuning strategies for LLM exploration/exploitation balance"], "transferable_to": ["Other resource allocation and scheduling problems", "Network optimization and design", "Scientific discovery requiring algorithm design", "Hardware design optimization"], "open_weaknesses": ["Limited size and complexity of code LLMs can analyze", "Prone to runtime, search, and algorithm failures (e.g., reward hacking, overfitting)", "Building faithful and inexpensive simulators is non-trivial", "Difficulty in hyperparameter tuning for LLM exploration/exploitation"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.84, "experiments": {"benchmarks": ["Solomon R101 (adapted for spot instances)", "ShareGPT", "GSM8K", "Wikitext-2 perplexity", "PTB PPL", "Epinions", "SmallBank", "TPC-C", "TAOBench", "YCSB", "ProgramDev-v1 benchmark"], "baselines": ["Uniform Progress (Wu et al. (2024))", "greedy policy (Wu et al. (2024))", "DeepSeek EPLB (DeepSeek AI (2024))", "internal frontier lab implementation", "greedy recursive group algorithm (Liu et al. (2024b))", "Shortest Makespan First (Cheng et al. (2024))", "HotNets’24 solution (Krentsel et al. (2024))", "direct replication strategy", "algorithm from Yu et al. (2025)", "MetaGPT (Hong et al. (2024))"], "hardware": "Unspecified GPUs/CPUs, typically few hours (1-12h) and tens of dollars (less than $20) per case study", "instance_sizes": [500]}, "results": {"vs_baselines": {"Uniform Progress (Can't Be Late Single Region)": "+7% average cost savings, up to 16.7%", "Single-region baseline (Can't Be Late Multi-Region)": "26% lower cost", "Internal reference implementation (EPLB)": "5.0x faster runtime", "GGR (LLM-SQL)": "3x faster runtime, similar hit rate", "SMF (Transaction Scheduling Offline)": "34% makespan reduction", "HotNets'24 solution (Telemetry Repair)": "+9% better counter repair score, +30% higher confidence calibration score", "Published solution (Global Model Placement)": "18.5% cheaper"}, "scalability": "The ADRS process itself is scalable due to simulator-based evaluation, which is orders of magnitude faster and cheaper than real systems, enabling hundreds to thousands of iterations.", "statistical_rigor": "Evaluations use sampled traces (e.g., 30% for feedback, full set for final results), report average cost savings, per-workload improvements, and per-configuration statistics (mean, deviation, count).", "limitations_acknowledged": ["Size and complexity of code LLMs can analyze", "Prone to runtime, search, and algorithm failures (e.g., reward hacking, overfitting)", "Overly strong baselines can limit search to micro-optimizations", "Too much guidance can risk premature convergence", "Building faithful and inexpensive simulators is non-trivial", "Lack of systematic ablation studies for optimal configurations", "Inefficiency of evolutionary search (looping over failed heuristics)", "Difficulty in formalizing intuitive trade-offs into numerical weights for evaluators", "Monolithic nature of current evolutionary searches (single final program)", "Hyperparameter tuning for LLM exploration/exploitation is difficult and relies on trial and error", "Optimal balance between synchronous and asynchronous human-ADRS interaction is an open question"]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2510.08755", "arxiv_url": "https://arxiv.org/abs/2510.08755", "title": "Robust Heuristic Algorithm Design with LLMs", "authors": ["Pantea"], "abstract": "", "published_date": "2025-10-09", "affiliations": "Microsoft, MIT, Microsoft Research, University of Southern California, The University of Texas at Austin", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 8, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper provides a concrete architecture for 'white-box' evolutionary search, replacing FunSearch's blind mutation with solver-guided debugging. It directly addresses our need for better feedback signals in AlgoEvo by integrating formal counter-examples and natural language explanations into the loop."}, "brief": "Karimi et al. introduce 'Robusta', an enhancement to FunSearch that uses a Heuristic Analyzer (solver-based) to identify adversarial inputs and a Suggester LLM to explain *why* the current heuristic fails before generating new code. They demonstrate a 28x improvement in worst-case performance over FunSearch on traffic engineering tasks, with results backed by rigorous comparison against optimal solvers. The critical takeaway is the 'Suggester' intermediate step: converting raw failure instances into natural language coding strategies significantly improves the LLM's ability to fix logic bugs compared to raw samples alone. We should immediately attempt to replicate this 'Analyzer -> Explainer -> Coder' loop for our VRP work, using small-scale solvers to generate counter-examples for our evolved ALNS operators.", "methodology": {"core_method": "Explanation-guided genetic search for heuristic design", "llm_role": "evolutionary_search, decomposition_guide, code_writer, prompt_optimizer", "llm_model_used": "Open-AI’s o4-mini", "search_type": "improvement", "novelty_claim": "We propose a novel architecture for LLM-based heuristic design through explanation-guided genetic search.", "components": ["Heuristic Analyzer and Explainer", "Suggester LLM", "Heuristic Generator (LLM-Based)", "Pattern Analysis LLM", "Suggest Improvement LLM", "Heuristic Writer"], "training_required": true}, "tags": {"methods": ["llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "funsearch", "evolution_of_heuristics", "genetic_algorithm", "heuristic_analysis", "combinatorial_reasoning", "smt_solvers", "optimization_based_analyzers", "ensemble_heuristics", "input_space_partitioning", "llm_prompt_optimization", "program_synthesis"], "problems": ["robust_algorithm_design", "heuristic_design", "traffic_engineering", "network_routing"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "funsearch", "specific_domain": "traffic_engineering", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Robust Heuristic Algorithm Design", "short": "RHAD", "class_": "algorithm_design", "properties": ["NP-hard", "approximate", "robustness", "edge-cases"], "scale": "20-node, 30-link sub-graphs; evaluated on 196-node, 486-edge graphs"}, "lineage": {"direct_ancestors": [{"paper": "FunSearch", "relationship": "extends LLM-based genetic search with explanation-guided mechanisms"}, {"paper": "MetaOpt [49]", "relationship": "uses as a heuristic analyzer for adversarial sample generation"}, {"paper": "XPlain [31]", "relationship": "uses for partitioning input space into regions and generating explanations"}], "closest_prior_work": "FunSearch", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Automate mathematical model generation for heuristics using LLMs", "Develop techniques for automatic selection and application of scaling strategies for heuristic analysis", "Investigate dynamic re-evaluation of input regions during the search process", "Develop methods to automatically generate explanations for LLM-generated code", "Explore advanced strategies for integrating multiple LLM suggestions throughout the search"], "transferable_to": ["Capacity planning under network failures", "Other networking problems requiring robust heuristics", "General NP-hard problems where robust approximate solutions are needed", "Program synthesis tasks with critical robustness requirements"], "open_weaknesses": ["Scalability limitations of SMT-based and optimization-based heuristic analyzers", "Difficulty in dynamically revising adversarial inputs at each search iteration", "Static definition of input regions based on the initial base heuristic", "Uncertainty regarding the optimal role and effectiveness of LLMs in this domain", "Balancing average performance improvements with worst-case robustness objectives"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.31, "experiments": {"benchmarks": ["Microsoft’s traffic engineering heuristic", "CogentCo topology from the Topology Zoo"], "baselines": ["FunSearch"], "hardware": "OpenAI API for LLM calls; heuristic evaluation runtime limited to 120 seconds per instance", "instance_sizes": [20, 196]}, "results": {"vs_baselines": {"FunSearch": "Robusta achieves 0.5% max suboptimality vs 14% for FunSearch; 0.01% mean suboptimality vs 2.07% for FunSearch; 1.00x runtime vs 0.96x for FunSearch"}, "scalability": "Heuristics designed on smaller 20-node topologies reduced worst suboptimality from 46% to 27% on larger 196-node topologies without modification.", "statistical_rigor": "Experiments repeated 20 times to account for randomness, reporting mean and standard deviation for average suboptimality.", "limitations_acknowledged": ["Unclear whether LLMs are the right tool for this problem", "Scalability of SMT-based and optimization-based heuristic analyzers", "Difficulty in revising adversarial inputs at each iteration of the search process", "Static definition of regions based on the base heuristic", "Need for more research to balance average performance and runtime objectives"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2510.07417", "arxiv_url": "https://arxiv.org/abs/2510.07417", "title": "FLEET: Formal Language-Grounded Scheduling for Heterogeneous Robot Teams", "authors": ["Corban"], "abstract": "", "published_date": "2025-10-08", "affiliations": "JHU APL, JHU, DEVCOM ARL", "category": "OR for Generative AI", "relevance": {"methodological": 5, "problem": 7, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "Validates the 'LLM as Translator, MILP as Solver' paradigm for heterogeneous VRP/scheduling, which aligns with our OR formulations work. It confirms that pure LLM planning is inferior to hybrid approaches for combinatorial constraints."}, "brief": "FLEET implements a hybrid pipeline where an LLM extracts a task dependency graph and a 'fitness matrix' (capability scores) from natural language, which then populate a standard MILP for multi-robot scheduling. Results on the PARTNR benchmark show it outperforms pure LLM planners (SMART-LLM) by ~7% on heterogeneous tasks, though overall gains are modest. The actionable takeaway is the **fitness matrix extraction**: using the LLM to generate dense cost coefficients ($c_{ij}$) for the optimization model rather than just binary constraints. We should adopt this technique for handling soft semantic preferences in our heterogeneous VRP formulations.", "methodology": {"core_method": "Hybrid generative–formal framework combining LLM-based task decomposition and fitness estimation with Mixed-Integer Linear Programming (MILP) for makespan minimization.", "llm_role": "decomposition_guide", "llm_model_used": "gpt-4o, gpt-oss-20b", "search_type": "hybrid", "novelty_claim": "We introduce a hybrid framework that embeds LLM-derived task graphs and capability-aware fitness matrices into a makespan-minimizing scheduler for multi-robot coordination.", "components": ["LLM front-end", "MILP scheduler", "Auction allocator", "World Model", "ConceptAgent (reasoning agents for execution)"], "training_required": false}, "tags": {"methods": ["milp_general", "llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "multi_robot_task_allocation", "makespan_minimization", "anytime_algorithm", "auction_algorithm", "closed_loop_control", "tool_use_agents"], "problems": ["multi_robot_scheduling", "makespan_minimization", "heterogeneous_robot_teams", "natural_language_understanding", "task_decomposition", "resource_constrained_scheduling"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": null, "specific_domain": "multi_robot_scheduling", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Formal Language-Grounded Scheduling for Heterogeneous Robot Teams", "short": "Multi-Robot Scheduling", "class_": "scheduling", "properties": ["heterogeneous", "natural_language_instructions", "makespan_minimization", "precedence_constraints", "non_overlap_constraints", "capability_aware", "closed_loop", "dynamic"], "scale": "2-3 agents"}, "lineage": {"direct_ancestors": [{"paper": "ConceptAgent: Llm-driven precondition grounding and tree search for robust task planning and execution", "relationship": "uses for robot execution"}, {"paper": "Twostep: Multi-agent task planning using classical planners and large language models", "relationship": "extends hybrid LLM-classical planning to multi-robot context"}], "closest_prior_work": "SMART-LLM: Smart multi-agent robot task planning using large language models", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Scaling to larger robot teams and more complex environments", "Integrating more sophisticated world models for dynamic replanning", "Exploring different LLM models or fine-tuning for specific domains", "Handling more complex, dynamic, and uncertain constraints"], "transferable_to": ["Logistics and warehouse automation with heterogeneous fleets", "Disaster response and search & rescue operations", "Space exploration and planetary surface operations", "Automated manufacturing and assembly lines"], "open_weaknesses": ["MILP solver limitations in finding optimal solutions within strict time limits", "Potential for LLM hallucinations or errors in task decomposition and fitness estimation", "Scalability challenges for very large robot teams or highly dynamic scenarios", "Sensitivity to prompt engineering for LLM performance"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.74, "experiments": {"benchmarks": ["PARTNR [1]"], "baselines": ["PARTNR decentralized baseline [1]", "SMART-LLM [22]"], "hardware": "Boston Dynamic Spot robots", "instance_sizes": [2, 3]}, "results": {"vs_baselines": {"PARTNR decentralized baseline [1]": "+0.31 overall success rate (0.59 vs 0.28)", "SMART-LLM [22]": "+0.02 overall success rate (0.59 vs 0.57), +0.07 on heterogeneous tasks"}, "scalability": "The system supports MILP with time/gap caps and warm-starts, falling back to Auction allocator for scalability and guaranteed progress.", "statistical_rigor": "Scores are macro-averaged across PARTNR categories and averaged over runs with gpt-4o and gpt-oss-20b.", "limitations_acknowledged": ["MILP might not always find optimal solutions within time limits", "Evaluation omitted strictly spatial-relation tasks"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2510.05774", "arxiv_url": "https://arxiv.org/abs/2510.05774", "title": "ConstraintLLM: A Neuro-Symbolic Framework for Industrial-Level Constraint Programming", "authors": ["Weichun"], "abstract": "", "published_date": "2025-10-07", "affiliations": "University of Oxford, University of Chinese Academy of Sciences, Hangzhou Institute for Advanced Study, ISCAS, University of Science and Technology Beijing", "category": "OR for Generative AI", "relevance": {"methodological": 7, "problem": 8, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "The paper demonstrates that for formal modeling (OR/CP), retrieving based on logical structure (constraint types) vastly outperforms semantic RAG. This is a concrete technique we can immediately apply to improve our OR-Bench and AlgoEvo code generation pipelines."}, "brief": "ConstraintLLM fine-tunes a 32B model for Constraint Programming (CP) modeling, utilizing a \"Constraint-Aware Retrieval Module\" (CARM) that retrieves few-shot examples based on extracted constraint signatures (e.g., `AllDifferent`, `Cumulative`) rather than text embeddings. They also employ a Tree-of-Thoughts search pruned by test case execution and an iterative self-correction mechanism that retrieves \"correction paths\" (error-to-fix trajectories). Results are strong: on their new industrial benchmark (IndusCP), they achieve ~51% accuracy with a 32B model, matching or beating GPT-4o and DeepSeek-V3. **Key Takeaway:** The shift from semantic retrieval to *structural* retrieval (matching constraint profiles) is the \"stealable\" insight; we should implement this for our OR modeling tasks immediately, ignoring surface-level problem descriptions in favor of logical signatures. This directly impacts our OR-Bench and automated formulation work.", "methodology": {"core_method": "Neuro-Symbolic Framework integrating Multi-Instruction Supervised Fine-Tuning (SFT) of an open-source LLM, Constraint-Aware Retrieval Module (CARM), Tree-of-Thoughts (ToT) exploration, and Iterative Self-Correction with Guided Retrieval.", "llm_role": "code_writer", "llm_model_used": "Qwen2.5-Coder-32B-Instruct", "search_type": "hybrid", "novelty_claim": "ConstraintLLM is the first LLM specifically designed for CP modeling, trained on an open-source LLM with multi-instruction supervised fine-tuning, and integrates a novel Constraint-Aware Retrieval Module (CARM).", "components": ["Multi-Instruction Supervised Fine-Tuning (SFT)", "Constraint-Aware Retrieval Module (CARM)", "Tree-of-Thoughts (ToT) framework", "Iterative Self-Correction with Guided Retrieval"], "training_required": true}, "tags": {"methods": ["neuro_symbolic_ai", "llm_code_generation", "llm_as_heuristic", "llm_as_evaluator", "llm_fine_tuned", "llm_in_the_loop", "tree_of_thoughts", "retrieval_augmented_generation", "supervised_fine_tuning", "parameter_efficient_fine_tuning", "qlora", "self_correction", "constraint_programming_solver", "in_context_learning"], "problems": ["automated_constraint_programming_modeling", "program_synthesis", "constraint_optimization_problems", "constraint_satisfaction_problems", "scheduling", "resource_allocation", "routing", "bin_packing", "combinatorial_puzzles_and_games", "cryptography_and_algorithmic_puzzles", "manufacturing_and_production_planning", "telecommunications_and_network_design", "layout_packing_and_cutting", "design_and_configuration", "data_driven_optimization_and_analytics"], "contribution_type": ["new_method", "new_benchmark", "sota_result", "framework"], "framework_lineage": "constraintllm", "specific_domain": null, "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Automated Constraint Programming Modeling", "short": "CP Modeling", "class_": "program_synthesis", "properties": ["industrial-level", "complex combinatorial", "NP-hard", "declarative"], "scale": "up to 21000 constraints and 3300 variables"}, "lineage": {"direct_ancestors": [{"paper": "CP-LLM-ICL (Michailidis et al., 2024)", "relationship": "improves upon by adding SFT and CARM"}, {"paper": "Tree-of-Thoughts (Yao et al., 2023)", "relationship": "integrates as a core component"}, {"paper": "Retrieval-Augmented Generation (Lewis et al., 2020)", "relationship": "builds upon with a constraint-aware variant (CARM)"}], "closest_prior_work": "CP-LLM-ICL (Michailidis et al., 2024)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Expand to broader problem domains beyond current scope.", "Refine the self-correction mechanism for enhanced robustness.", "Train the framework on larger and more diverse LLM models.", "Extend the IndusCP benchmark with more challenging tasks."], "transferable_to": ["Other open-source LLMs for CP modeling.", "Different constraint programming libraries or languages (e.g., MiniZinc, XCSP).", "Other symbolic reasoning paradigms like SAT or SMT for automated modeling."], "open_weaknesses": ["Evaluation is currently limited to the Python API of PyCSP3 library.", "The framework's performance is primarily validated on the Qwen2.5 series of models.", "Reliance on CARM for in-context exemplars, ideally aiming for zero-shot or few-shot capabilities without external retrieval."]}, "artifacts": {"code_url": "https://github.com/william4s/ConstraintLLM", "models_released": true, "new_benchmark": true}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.74, "experiments": {"benchmarks": ["NL4OPT", "LGPs", "LogicDeduction", "IndusCP"], "baselines": ["Direct Solving", "CoT (one-shot)", "RAG (four-shot)", "CP-LLM-ICL", "LLMOPT", "Logic-LM", "ChatGPT-4o", "DeepSeek-V3"], "hardware": "3 NVIDIA RTX A6000 GPUs", "instance_sizes": [350, 400, 900, 970, 1100, 1650, 3300, 21000]}, "results": {"vs_baselines": {"Qwen2.5-Coder-32B Direct Solving": "IndusCP: 23.3%, NL4OPT: 10.1%, LGPs: 3.3%, LogicDeduction: 51.0%", "Qwen2.5-Coder-32B CoT": "IndusCP: 17.5%, NL4OPT: 85.2%, LGPs: 2.0%, LogicDeduction: 84.5%", "Qwen2.5-Coder-32B RAG": "IndusCP: 21.8%, NL4OPT: 88.6%, LGPs: 82.0%, LogicDeduction: 92.0%", "CP-LLM-ICL": "IndusCP: 22.7%, NL4OPT: 87.5%, LGPs: 76.0%, LogicDeduction: 93.5%", "LLMOPT": "IndusCP: 3.3%, NL4OPT: 93.0%, LGPs: 0.0%, LogicDeduction: 16.0%", "Logic-LM": "LogicDeduction: 87.6%", "ChatGPT-4o CoT": "IndusCP: 26.5%, NL4OPT: 82.3%, LGPs: 41.0%, LogicDeduction: 67.0%", "ChatGPT-4o RAG": "IndusCP: 33.5%, NL4OPT: 91.5%, LGPs: 87.0%, LogicDeduction: 97.0%", "DeepSeek-V3 CoT": "IndusCP: 38.5%, NL4OPT: 84.9%, LGPs: 42.0%, LogicDeduction: 85.0%", "DeepSeek-V3 RAG": "IndusCP: 51.0%, NL4OPT: 96.7%, LGPs: 87.0%, LogicDeduction: 99.0%", "ConstraintLLM (w/ ToT)": "IndusCP: 51.3%, NL4OPT: 99.26%, LGPs: 92.0%, LogicDeduction: 100.0%"}, "scalability": "Demonstrates stable performance across major problem categories and generalizes effectively to domains with limited external knowledge by matching constraint profiles.", "statistical_rigor": "Solving Accuracy (SA) is reported as the proportion of problems with verified correct solutions; no explicit statistical tests or variance are reported.", "limitations_acknowledged": ["Evaluation is only done on the Python API of PyCSP3 library due to the lack of data in other forms of code.", "Methods were trained and evaluated based on the Qwen2.5 series of models, suggesting a benefit to extend to a broader range of open-source LLMs.", "Relies on CARM for in-context exemplars, ideally the model should be capable of high-quality CP modeling directly under zero-shot or few-shot conditions."]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2510.06056", "arxiv_url": "https://arxiv.org/abs/2510.06056", "title": "Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep Research", "authors": ["Gang"], "abstract": "", "published_date": "2025-10-07", "affiliations": "MIT-IBM Watson AI Lab, IBM Research, University of Notre Dame", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 8, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper directly extends AlphaEvolve (our primary focus) by integrating a 'Deep Research' retrieval loop and a robust debugging agent. It addresses two key bottlenecks we face: LLMs running out of novel ideas (plateauing) and high failure rates in generated code."}, "brief": "DeepEvolve augments the standard evolutionary coding loop (AlphaEvolve) with two critical components: a 'Deep Research' module that searches the web/literature to generate grounded mutation proposals, and an iterative debugging agent that fixes execution errors. While the '666%' improvement on Circle Packing is likely due to a weak baseline (fixed-size vs. generalized), the engineering results are compelling: the debugging agent raises execution success rates from ~13% to ~99% in complex tasks. The key takeaway for our AlgoEvo work is the architecture of generating a text-based 'research proposal' via RAG before attempting code generation, rather than mutating code directly. We should immediately adopt their debugging loop and consider injecting external literature search into our mutation operators to prevent search stagnation.", "methodology": {"core_method": "Agent-based framework integrating deep research (planning, searching, writing) with algorithm evolution (coding, evaluation, evolutionary selection) and iterative debugging.", "llm_role": "heuristic_generator, code_writer, evaluator, decomposition_guide, debugger", "llm_model_used": "o4-mini, gpt-4o, o3-mini", "search_type": "hybrid", "novelty_claim": "DeepEvolve integrates deep research with algorithm evolution, uniting external knowledge retrieval, cross-file code editing, and systematic debugging under a feedback-driven iterative loop.", "components": ["Plan module", "Search module", "Write module", "Coding agent", "Evaluation module", "Evolutionary Selection module", "Debugging agent", "Reflection mechanism", "Evolutionary Database (MAP-Elites, Island-based populations)"], "training_required": false}, "tags": {"methods": ["agent_based_framework", "llm_as_heuristic", "llm_code_generation", "llm_as_evaluator", "llm_in_the_loop", "llm_research_agent", "evolution_of_heuristics", "program_synthesis", "map_elites", "island_model_ea", "iterative_debugging"], "problems": ["algorithm_discovery", "molecular_prediction", "molecular_translation", "circle_packing", "pde_solving", "image_classification", "time_series_prediction", "scientific_discovery"], "contribution_type": ["new_method", "framework", "sota_result"], "framework_lineage": "alphaevolve", "specific_domain": "scientific_discovery", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Scientific Algorithm Discovery", "short": "Algorithm Discovery", "class_": "algorithm_design", "properties": ["multi-domain", "multi-modal_data", "feedback-driven", "iterative", "code_implementation"], "scale": "Diverse scientific domains and data modalities, including molecules, geometries, PDEs, images, time series, mRNA, and text."}, "lineage": {"direct_ancestors": [{"paper": "AlphaEvolve", "relationship": "extends and augments"}], "closest_prior_work": "AlphaEvolve", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["optimizing LLM call efficiency and runtime budget", "exploring more complex and challenging scientific problems", "further automating the research and debugging process", "improving generalization to handle variable-sized inputs more robustly"], "transferable_to": ["automated algorithm design for operations research problems", "discovery of general machine learning models", "drug discovery and materials science", "complex engineering design problems"], "open_weaknesses": ["runtime budget constraints limit search space for some problems", "limited room for improvement on certain state-of-the-art baselines", "novel ideas may have higher implementation difficulty", "lack of explicit statistical significance tests for quantitative results"]}, "artifacts": {"code_url": "https://github.com/liugangcode/deepevolve", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.86, "experiments": {"benchmarks": ["Molecular Prediction", "Molecular Translation", "Circle Packing", "Burgers’ Equation", "Parkinson’s Disease", "Nuclei Image", "Open Vaccine", "Polymer Prediction", "USP P2P"], "baselines": ["Initial Algorithm (AlphaEvolve, OpenEvolve, GREA, CodePDE, Kaggle winning solutions)"], "hardware": "single GPU (2080-Ti or A6k)", "instance_sizes": [26, 27, 28, 29, 30, 31, 32]}, "results": {"vs_baselines": {"Initial Algorithm": "Improved performance across 9 benchmarks (0.39% to 666.02% gain), with specific gains like 666.02% on Circle Packing and 35.94% on Molecular Translation."}, "scalability": "DeepEvolve improves generalization for problems like Circle Packing by discovering algorithms that handle variable-sized constructions, and achieves performance improvements across diverse problem types and maturities.", "statistical_rigor": "Performance improvements are reported as percentages over initial algorithms. Execution success rates and average debugging counts are provided. Qualitative evaluation of originality, future potential, and implementation difficulty is performed using an LLM-as-a-judge approach. No explicit statistical significance tests (e.g., p-values, confidence intervals) are reported for the quantitative performance metrics in Table 2, which show single values.", "limitations_acknowledged": ["For Open Vaccine, runtime budget constraints limit the search space.", "For Burgers’ Equation, the state-of-the-art baseline leaves limited room for further improvement.", "Novel ideas may have higher implementation difficulty."]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2510.04204", "arxiv_url": "https://arxiv.org/abs/2510.04204", "title": "CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling", "authors": ["Zhengyang"], "abstract": "", "published_date": "2025-10-05", "affiliations": "Qwen Team, Alibaba Inc., The Chinese University of Hong Kong, Shenzhen, Southern University of Science and Technology, Shanghai University of Finance and Economics, Shenzhen Loop Area Institute (SLAI)", "category": "Generative AI for OR", "relevance": {"methodological": 8, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper directly addresses the 'Code Utilization Distrust' failure mode—where models simulate logic in text rather than using tools—which is a primary bottleneck in our AlgoEvo and OR-Bench work. It provides a proven, automated 'Intervener' workflow to fix this via RL, achieving SOTA with a 4B model, offering a concrete blueprint to improve our agent reliability."}, "brief": "Tang et al. propose CALM, a framework that uses an expert 'Intervener' model to inject corrective hints into a small LRM's reasoning trace (e.g., forcing it to use Python instead of manual calculation), followed by SFT and RL (GRPO). Results are strong and verified: a 4B model matches DeepSeek-R1 (671B) on OR benchmarks, specifically fixing the 'Code Utilization Distrust' we see in our own agents. The key takeaway is the 'Intervener' loop: instead of discarding failed traces, they repair them with hints to create a 'golden' reasoning dataset that preserves the 'thinking' process while enforcing tool use. This is a direct, actionable method for improving our AlgoEvo agents' reliability in generating executable heuristics without massive human annotation.", "methodology": {"core_method": "Corrective Adaptation with Lightweight Modification (CALM) framework with two-stage training (SFT + RL)", "llm_role": "generates optimization models and solver code, performs reflective reasoning, and receives corrective hints from an expert LLM (Intervener)", "llm_model_used": "Qwen3-4B-Thinking-2507", "search_type": "improvement", "novelty_claim": "CALM is a lightweight and scalable framework that leverages solver code execution and dynamic, hint-based data synthesis to correct and strengthen LRM reasoning trajectories, preserving and amplifying native reasoning patterns for optimization modeling.", "components": ["CALM framework", "Reasoner LLM", "Intervener LLM", "Code Interpreter", "Supervised Fine-Tuning", "Reinforcement Learning (GRPO)"], "training_required": true}, "tags": {"methods": ["llm_as_heuristic", "llm_as_evaluator", "llm_code_generation", "llm_in_the_loop", "llm_fine_tuned", "reinforcement_learning", "supervised_learning", "program_synthesis", "group_relative_policy_optimization", "code_interpreter_feedback"], "problems": ["automated_optimization_modeling", "mathematical_modeling", "milp_general", "lp_formulation"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "calm", "specific_domain": "automated_optimization_modeling", "llm_coupling": "rl_trained"}, "problem": {"formal_name": "Automated optimization modeling", "short": "OM", "class_": "optimization_modeling", "properties": ["mathematical modeling", "code generation", "reflective", "solver-integrated", "multi-step reasoning"], "scale": "diverse problems from high-school to industrial complexity"}, "lineage": {"direct_ancestors": [{"paper": "ORLM (Huang et al., 2025)", "relationship": "improves upon the non-reflective paradigm of"}, {"paper": "LLMOPT (Jiang et al., 2024)", "relationship": "improves upon the non-reflective paradigm of"}, {"paper": "SIRL (Chen et al., 2025)", "relationship": "improves upon the non-reflective paradigm of"}, {"paper": "CoRT (Li et al., 2025a)", "relationship": "inspired by solver access mechanism in"}, {"paper": "Qwen3-4B-Thinking-2507", "relationship": "adapts and fine-tunes"}], "closest_prior_work": "SIRL (Chen et al., 2025)", "novelty_type": "paradigm_shift"}, "extensions": {"next_steps": ["Extend STORM to broader optimization modeling agent frameworks like OptiMUS", "Further reduce 'Code Utilization Distrust' and 'Lack of OR Expertise' flaws", "Explore dynamic intervention limits beyond fixed N=5", "Investigate methods to reduce computational resource constraints for RL training"], "transferable_to": ["Inventory management", "Airline crew scheduling", "Other complex real-world operations research problems", "Domains requiring reflective reasoning and code execution (e.g., scientific discovery)"], "open_weaknesses": ["Reliance on a powerful, closed-source Intervener model (Gemini-2.5-Pro) for data curation", "Fixed maximum number of interventions (N=5) may limit correction for deeply flawed trajectories", "Computational resource intensity of reinforcement learning stage", "Scalability of expert intervener feedback to entirely new, unseen problem types without human oversight"]}, "artifacts": {"code_url": "https://github.com/tangzhy/STORM", "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_status": "stable", "bridge_score": 0.5271, "is_bridge": true, "priority_score": 8.27, "experiments": {"benchmarks": ["NL4Opt", "MAMO-Easy", "MAMO-Complex", "IndustryOR", "OptMath"], "baselines": ["GPT-3.5-Turbo", "GPT-4", "DeepSeek-V3", "DeepSeek-R1-0528", "Qwen3-235B-A22B-Thinking-2507", "Chain-of-Experts", "OptiMUS", "LLMOPT-Qwen2.5-14B", "ORLM-LLaMA-3-8B", "OptMATH-Qwen2.5-7B", "SIRL-Qwen2.5-7B", "Qwen3-4B-Thinking-2507 (Base)"], "hardware": "4x 8x NVIDIA H800 (80GB) GPUs", "instance_sizes": []}, "results": {"vs_baselines": {"GPT-3.5-Turbo": "+23.6% Macro AVG (68.9% vs 45.3%)", "GPT-4": "+13.9% Macro AVG (68.9% vs 55.0%)", "DeepSeek-V3": "+7.9% Macro AVG (68.9% vs 61.0%)", "DeepSeek-R1-0528": "+1.4% Macro AVG (68.9% vs 67.5%)", "Qwen3-235B-A22B-Thinking-2507": "+5.0% Macro AVG (68.9% vs 63.9%)", "OptiMUS": "+19.5% Macro AVG (68.9% vs 49.4%)", "LLMOPT-Qwen2.5-14B": "+17.8% Macro AVG (68.9% vs 51.1%)", "ORLM-LLaMA-3-8B": "+19.7% Macro AVG (68.9% vs 49.2%)", "OptMATH-Qwen2.5-7B": "+13.5% Macro AVG (68.9% vs 55.4%)", "SIRL-Qwen2.5-7B": "+6.8% Macro AVG (68.9% vs 62.1%)", "Qwen3-4B-Thinking-2507 (Base)": "+11.8% Macro AVG (68.9% vs 57.1%) and +23.8% on MAMO-Complex (70.3% vs 46.5%)"}, "scalability": "The 4B STORM model achieves performance comparable to a 671B LRM, demonstrating high parameter efficiency and sample-efficient learning with CALM.", "statistical_rigor": "Pass@1 accuracy is averaged over 8 independent samples per problem with temperature 0.6 and top-p 0.95.", "limitations_acknowledged": []}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2510.02679", "arxiv_url": "https://arxiv.org/abs/2510.02679", "title": "Automated Constraint Specification for Job Scheduling by Regulating Generative Model with Domain-Specific Representation", "authors": ["Yu-Zhe"], "abstract": "", "published_date": "2025-10-03", "affiliations": "Peking University, The Hong Kong University of Science and Technology, Huazhong University of Science and Technology, University of Science and Technology of China", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 6, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While the application is specific to manufacturing scheduling, the method for *automatically learning* a domain-specific language (DSL) to constrain LLM outputs is highly relevant to our work on automated algorithm design and OR-Bench. The use of Dirichlet Process Mixture Models to infer semantic structures offers a concrete technique to reduce manual engineering in our evolutionary search spaces."}, "brief": "This paper proposes a constraint-centric architecture that translates natural language manufacturing descriptions into Job Shop Scheduling (JSP) constraints by mediating through a learned Domain-Specific Language (DSL). Unlike standard prompting, they implement an automated DSL adaptation algorithm using non-parametric modeling (DPMM) and Expectation-Maximization to learn the syntax and semantics of the intermediate representation from data, which is then verified via a Pushdown Automaton. While the experiments rely on synthetic data augmented from standard benchmarks (a weakness), the methodology for **automatically deriving the intermediate representation** rather than hand-coding it is a transferable insight. We could steal this 'automated DSL design' approach to dynamically construct search spaces for AlgoEvo or to improve the robustness of NL-to-OR translation in OR-Bench.", "methodology": {"core_method": "Constraint-centric architecture regulating LLMs with Domain-Specific Languages (DSLs) and an automated DSL adaptation algorithm", "llm_role": "constraint_generator", "llm_model_used": "GPT-4o", "search_type": "constructive", "novelty_claim": "We address the automatic, entire-workflow constraint specification for production scheduling, based on a domain-specific representation to regulate LLM outputs, to achieve a balance between the generative power of GenAI and the guardrail of reliability.", "components": ["Constraint-centric architecture", "Domain-Specific Languages (DSLs)", "Automated DSL adaptation algorithm", "Constraint Abstraction Module", "Constraint Generation Module", "Schedule Grounding Module", "Pushdown Automaton (PDA) for verification"], "training_required": true}, "tags": {"methods": ["generative_models", "large_language_models", "domain_specific_languages", "llm_in_the_loop", "llm_as_heuristic", "llm_code_generation", "non_parametric_modeling", "dirichlet_process_mixture_model", "expectation_maximization", "pushdown_automaton", "constraint_programming", "gaussian_process", "program_synthesis"], "problems": ["job_shop_scheduling", "constraint_specification", "automated_dsl_design"], "contribution_type": ["new_method", "framework", "sota_result", "new_benchmark"], "framework_lineage": null, "specific_domain": "job_shop_scheduling", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Automated Constraint Specification for Job Scheduling by Regulating Generative Model with Domain-Specific Representation", "short": "Automated Constraint Specification for JSP", "class_": "constraint_specification", "properties": ["heterogeneous_data_input", "domain_specific", "hierarchical_structure", "adaptable_to_scenarios", "precision_critical"], "scale": "classical Job Shop Scheduling Problems"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2406.00000", "relationship": "inspired translation and verification methodology from"}, {"paper": "arXiv:2406.00000", "relationship": "inspired automated DSL syntax design from"}], "closest_prior_work": "Chain-of-experts: When llms meet complex operations research problems", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Explore adaptation boundary to more complex manufacturing scenarios", "Integrate with other smart manufacturing systems"], "transferable_to": ["Flow shop scheduling", "Project scheduling", "Resource allocation problems", "Other constraint satisfaction problems requiring precise specification from ambiguous data"], "open_weaknesses": ["Scalability to extremely large-scale manufacturing systems", "Robustness to highly ambiguous or incomplete raw manufacturing data", "Further reducing reliance on human expert supervision for DSL refinement"]}, "artifacts": {"code_url": "null", "models_released": false, "new_benchmark": true}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.24, "experiments": {"benchmarks": ["Augmented classical Job Shop Scheduling Problems (JSPs) from OR literature"], "baselines": ["MULTI-STAGE-LLM (MSL)", "TWO-STAGE-LLM (TSL)"], "hardware": "null", "instance_sizes": []}, "results": {"vs_baselines": {"MSL": "Ours significantly outperforms MSL (p < .0001) across all evaluation metrics (BLEU, EMKVP-Precision, EMKVP-Recall, EMKVP-F1, Constraint-Acc, Runtime-ER) in both full pipeline and module-specific tests.", "TSL": "Ours significantly outperforms TSL (p < .0001) across all evaluation metrics (BLEU, EMKVP-Precision, EMKVP-Recall, EMKVP-F1, Constraint-Acc, Runtime-ER) in both full pipeline and module-specific tests."}, "scalability": "The architecture demonstrates consistent and exceptional performance across ten diverse manufacturing scenarios, with automated DSL adaptation enabling broad applicability.", "statistical_rigor": "Paired samples t-tests (p < .0001 or p < .05) and Kruskal-Wallis H-Test (p = .978) were used for significance testing across ten scenarios. Variance-to-Mean Ratio (VMR) was used for multi-domain evaluation.", "limitations_acknowledged": ["Adaptation boundary to more complex manufacturing scenarios", "Integration with other smart manufacturing systems"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2510.00821", "arxiv_url": "https://arxiv.org/abs/2510.00821", "title": "Logical Consistency Between Disagreeing Experts and Its Role in AI Safety", "authors": ["Andrés"], "abstract": "", "published_date": "2025-10-01", "affiliations": "Data Engines", "category": "OR for Generative AI", "relevance": {"methodological": 7, "problem": 6, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper frames LLM-as-a-Judge evaluation as an Integer Linear Programming problem, a technique that aligns perfectly with our OR expertise. It offers a mathematically rigorous way to bound the reliability of our multi-agent evaluators without needing ground truth."}, "brief": "Corrada-Emmanuel formulates the unsupervised evaluation of classifiers as an Integer Linear Programming problem, defining the geometric space of possible ground truths consistent with observed agent disagreements. While the results are primarily theoretical demonstrations on MT-Bench (showing that certain disagreement patterns mathematically preclude accuracy >46%), the methodology is sound. The key takeaway is the concept of 'no-knowledge alarms': using LP constraints to flag when a multi-agent system or process reward model has become logically incoherent. We could implement this as a cheap, rigorous filter in our evolutionary search loops to prune branches where the evaluator agents are demonstrably unreliable.", "methodology": {"core_method": "Formulating and solving a Linear Programming problem in integer space to identify logically consistent group evaluations for classifiers based on observed agreement/disagreement statistics and universal linear equalities (axioms).", "llm_role": "subject_of_evaluation", "llm_model_used": "gpt4", "search_type": "exact", "novelty_claim": "Introduces a logic of unsupervised evaluation for classifiers by computing the set of group evaluations logically consistent with observed agreement/disagreement statistics using universally applicable linear equalities (axioms).", "components": ["Linear Programming", "Logical Axioms", "Inequality Constraints", "Statistical Summaries of Responses"], "training_required": false}, "tags": {"methods": ["integer_linear_programming", "formal_verification", "logical_inference", "axiomatic_system", "unsupervised_evaluation", "black_box_evaluation"], "problems": ["unsupervised_evaluation", "llm_evaluation", "meta_evaluation", "ai_safety_monitoring", "llm_comparison"], "contribution_type": ["new_method", "framework", "empirical_study"], "framework_lineage": null, "specific_domain": "llm_evaluation", "llm_coupling": null}, "problem": {"formal_name": "Logical Consistency in Unsupervised Evaluation for Classifiers", "short": "Logical Consistency Evaluation", "class_": "formal_verification", "properties": ["unsupervised", "no_ground_truth", "black_box_classifiers", "finite_labels", "integer_programming_formulation"], "scale": "Q=10-25 items, R=2-3 labels, N=1-2 classifiers"}, "lineage": {"direct_ancestors": [{"paper": "Platanios et al. (2014, 2016)", "relationship": "builds on logical aspects of unsupervised evaluation from"}], "closest_prior_work": "Estimating accuracy from unlabeled data: A Bayesian approach (Platanios et al., 2014)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Extend the logic to unsupervised evaluation for regressors", "Develop alarms with different accuracy thresholds per label", "Investigate the scientific validity and usefulness of these alarms in various contexts", "Explore M > 1 summaries and correlated experts"], "transferable_to": ["Any classifier evaluation without ground truth", "Multiple-choice exams", "Medical diagnosis", "Human expert evaluation"], "open_weaknesses": ["Cannot detect that classifiers agreeing in their answers are incorrect", "Cannot establish the scientific validity or usefulness of the alarms", "Limited to finite labels and factual questions", "Does not resolve the scientific question of whether a test has a correct answer key"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.24, "experiments": {"benchmarks": ["MT-Bench dataset"], "baselines": [], "hardware": "null", "instance_sizes": [10, 25]}, "results": {"vs_baselines": {"gpt4": "Worst performing accuracy of ~14% on tied judgments; alarm triggers for >46% accuracy threshold (jointly with authors)", "authors": "Alarm triggers for >46% accuracy threshold (jointly with gpt4)"}, "scalability": "The filtering effect of the axioms on possible evaluations strengthens with increasing problem size (Q) and number of labels (R), reducing the dimensionality of the possible set by R-1.", "statistical_rigor": "Exhaustive enumeration of possible evaluations for small instances (Q=10, Q=25) and analysis of multiplicity counts and min-max accuracy plots across the Q-complex.", "limitations_acknowledged": ["Cannot detect that classifiers agreeing in their answers are incorrect", "Cannot establish the scientific validity or usefulness of using these alarms in any context"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2509.24509", "arxiv_url": "https://arxiv.org/abs/2509.24509", "title": "Experience-Guided Reflective Co-Evolution of Prompts and Heuristics for Automatic Algorithm Design", "authors": ["Yihong"], "abstract": "", "published_date": "2025-09-30", "affiliations": "Tencent, Renmin University of China, City University of Hong Kong", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 7, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper directly implements 'Evolving the Evolver' (co-evolving prompts) and dynamic operator selection, which are active goals for our AlgoEvo project. It provides a concrete architecture for integrating prompt optimization into the evolutionary loop."}, "brief": "EvoPH introduces a co-evolutionary framework where both the heuristic code and the LLM prompts are evolved, utilizing an island model for diversity and a 'strategy sampling' mechanism that dynamically selects mutation types (e.g., parameter tuning vs. rewrite) based on feedback. They report dominating performance over FunSearch and ReEvo on TSP and BPP (e.g., reducing Christofides gap from ~20% to ~5%), though the static performance of baselines suggests the gain comes largely from automating prompt engineering. The most stealable insight is the **Strategy Sampling** module: explicitly defining a pool of mutation operators and using an 'experience' buffer to select them is a practical implementation of the 'planner' concept we need for AlgoEvo. We should also adopt their island migration topology to improve diversity in our parallelized search.", "methodology": {"core_method": "Experience-Guided Reflective Co-Evolution of Prompts and Heuristics (EvoPH) with island-based elites selection", "llm_role": "heuristic_generator", "llm_model_used": "Gemini-2.5-pro", "search_type": "evolutionary_search", "novelty_claim": "EvoPH is a novel framework that co-evolves prompts and heuristics, integrating an island migration model with elites selection and experience-guided adaptive prompt refinement to escape local optima.", "components": ["Heuristics Evolution", "Prompt Evolution", "Island-based Elites Selection", "Experience Summarization", "Strategy Sampling"], "training_required": false}, "tags": {"methods": ["evolutionary_search", "llm_as_heuristic", "llm_code_generation", "llm_prompt_optimization", "program_synthesis", "self_improving_search", "genetic_algorithm", "evoph"], "problems": ["tsp", "bin_packing", "heuristic_evolution", "operator_discovery"], "contribution_type": ["new_method", "framework", "new_benchmark", "sota_result"], "framework_lineage": "funsearch", "specific_domain": null, "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Traveling Salesman Problem and Bin Packing Problem", "short": "TSP, BPP", "class_": "algorithm_design", "properties": ["NP-hard", "offline", "fully_weighted_graph"], "scale": "instances solvable by Gurobi/OR-Tools within 600s"}, "lineage": {"direct_ancestors": [{"paper": "Funsearch (Romera-Paredes et al., 2024)", "relationship": "extends the paradigm of LLM-driven evolutionary search for algorithms"}, {"paper": "EoH (Liu et al., 2024a)", "relationship": "extends the paradigm of LLM-driven evolutionary search for algorithms"}, {"paper": "Reevo (Ye et al., 2024)", "relationship": "extends the paradigm of LLM-driven evolutionary search for algorithms"}], "closest_prior_work": "Funsearch (Romera-Paredes et al., 2024)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Apply EvoPH to a wider range of combinatorial optimization problems", "Investigate different LLM architectures or larger models for heuristic generation", "Explore more sophisticated prompt evolution strategies and experience summarization techniques", "Analyze the computational efficiency and scalability of EvoPH on larger instances"], "transferable_to": ["Vehicle Routing Problems (VRP variants)", "Scheduling problems (e.g., job_shop_scheduling)", "Graph algorithms (e.g., graph_coloring)", "Other NP-hard problems requiring specialized heuristic design"], "open_weaknesses": ["Lack of explicit statistical significance tests for main results", "Scalability limitations for very large or complex problem instances", "Generalizability of generated heuristics across diverse instance distributions", "Computational cost of the co-evolutionary process"]}, "artifacts": {"code_url": "null", "models_released": false, "new_benchmark": true}, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.94, "experiments": {"benchmarks": ["TSP-Gurobi-Bench (TGB)", "BPP-Ortools-Bench (BOB)"], "baselines": ["Funsearch (Romera-Paredes et al., 2024)", "EoH (Liu et al., 2024a)", "mEoH (Yao et al., 2025)", "Reevo (Ye et al., 2024)"], "hardware": "null", "instance_sizes": []}, "results": {"vs_baselines": {"Funsearch": "EvoPH achieved 5.17% vs 19.71% (Christofides, TGB) and 1.65% vs 25.49% (best-fit, BOB)", "EoH": "EvoPH achieved 5.17% vs 9.64% (Christofides, TGB) and 1.65% vs 17.20% (best-fit, BOB)", "mEoH": "EvoPH achieved 5.17% vs 16.90% (Christofides, TGB) and 1.65% vs 23.45% (best-fit, BOB)", "Reevo": "EvoPH achieved 5.17% vs 20.60% (Christofides, TGB) and 1.65% vs 26.77% (best-fit, BOB)"}, "scalability": "null", "statistical_rigor": "Results are reported as relative error against optimal solutions. Robustness experiments show proportion of executable code and convergence trajectories over 20 iterations, but no explicit statistical significance tests or variance for main results.", "limitations_acknowledged": []}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2507.23390", "arxiv_url": "https://arxiv.org/abs/2507.23390", "title": "FMIP: Joint Continuous-Integer Flow For Mixed-Integer Linear Programming", "authors": ["Hongpei"], "abstract": "", "published_date": "2025-09-29", "affiliations": "Stanford University, Princeton University, National University of Singapore, Shanghai Jiao Tong University, Shanghai University of Finance and Economics", "category": "OR for Generative AI", "relevance": {"methodological": 6, "problem": 6, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While this uses GNNs/Flow Matching rather than LLMs, the 'holistic guidance' mechanism for handling hybrid discrete-continuous spaces is a transferable inference-time optimization technique relevant to our multi-agent and hybrid search problems."}, "brief": "FMIP introduces a flow matching framework that jointly generates integer and continuous variables for MILP, utilizing a tripartite graph and inference-time guidance. Empirical results on MIPLIB and other benchmarks show a ~40% reduction in primal gap compared to integer-only neural baselines (DIFUSCO), though it remains a heuristic warm-start for solvers. The most valuable takeaway is the **hybrid guidance mechanism** (Eq. 6 & 7): it combines gradient descent for continuous variables with a sampling-and-reweighting scheme for discrete variables based on constraint violations. We should consider stealing this reweighting logic for guiding hybrid evolutionary operators or multi-agent action spaces where gradients are available for only part of the state.", "methodology": {"core_method": "Generative framework based on conditional flow matching for joint continuous-integer variable distribution modeling", "llm_role": "none", "llm_model_used": null, "search_type": "sampling", "novelty_claim": "FMIP is the first generative framework that models the joint distribution of both integer and continuous variables for MILP solutions.", "components": ["conditional flow matching", "time-dependent tripartite graph representation", "joint training objective", "holistic guidance mechanism", "gradient-based guidance for continuous variables", "sampling-and-reweighting for integer variables", "graph backbone network (e.g., Tri-GCN, Bi-GCN, GAT, ClusterGCN)"], "training_required": true}, "tags": {"methods": ["generative_models", "flow_matching", "conditional_flow_matching", "graph_neural_networks", "tri_gcn", "bi_gcn", "gat", "clustergcn", "gradient_based_optimization", "sampling_and_reweighting", "warm_starting", "neural_diving", "predict_and_search", "pmvb", "apollo_milp"], "problems": ["milp_general", "combinatorial_optimization", "combinatorial_auctions", "generalized_independent_set", "maximum_independent_set", "fixed_charge_multi_commodity_network_flow", "set_covering", "load_balancing", "item_placement"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "flow_matching", "specific_domain": "milp_general", "llm_coupling": null}, "problem": {"formal_name": "Mixed-Integer Linear Programming", "short": "MILP", "class_": "mathematical_optimization", "properties": ["NP-hard", "discrete choices", "continuous quantities", "linear constraints", "bounded integer variables"], "scale": "Various sizes represented by eight standard MILP benchmarks"}, "lineage": {"direct_ancestors": [{"paper": "Flow matching for generative modeling (Lipman et al., 2023; Albergo and Vanden-Eijnden, 2023; Liu et al., 2023)", "relationship": "builds on conditional flow matching"}, {"paper": "DIFUSCO (Sun and Yang, 2023; Feng et al., 2024)", "relationship": "extends integer-only generative modeling to mixed-integer"}, {"paper": "IP-Guided-Diff (Zeng et al., 2024)", "relationship": "extends integer-only generative modeling to mixed-integer"}, {"paper": "TFG-Flow (Lin et al., 2025)", "relationship": "adapts guidance mechanism from"}], "closest_prior_work": "DIFUSCO (Sun and Yang, 2023; Feng et al., 2024)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Enhance graph representation to better capture task-specific features", "Develop a customized solver tailored to the FMIP framework"], "transferable_to": ["Other MILP variants", "Energy systems optimization", "Supply chain design problems"], "open_weaknesses": ["Excessive sampling steps can reduce solution diversity", "Excessive sampling steps can prematurely trap search in local optima"]}, "artifacts": {"code_url": "https://github.com/Lhongpei/FMIP", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.93, "experiments": {"benchmarks": ["Combinatorial Auctions (CA)", "Generalized Independent Set (GIS)", "Maximum Independent Set (MIS)", "Fixed-Charge Multi-Commodity Network Flow (FCMNF)", "Set Covering (SC)", "Load Balancing (LB)", "Item Placement (IP)", "MIPLIB2017"], "baselines": ["SL (Standard supervised learning)", "DIFUSCO", "IP-Guided-Diff", "Gurobi"], "hardware": "Single NVIDIA H100 GPU for training; 12 cores Intel Xeon Platinum 8469C at 2.60 GHz CPU with 512 GB RAM for CPU tasks", "instance_sizes": []}, "results": {"vs_baselines": {"SL": "FMIP consistently achieves lower primal gaps across all benchmarks and solvers.", "DIFUSCO": "FMIP consistently achieves lower primal gaps across all benchmarks and solvers.", "IP-Guided-Diff": "FMIP consistently achieves lower primal gaps across most benchmarks and solvers.", "Gurobi": "FMIP-warm-started solvers achieve comparable or better primal gaps within shorter time limits on many instances."}, "scalability": "Inference time is comparable to other generative baselines and constitutes a negligible fraction (<1%) of the total downstream solver time; solution quality scaling with increasing problem size is not explicitly detailed.", "statistical_rigor": "Results are reported as objective values and primal gaps (absolute or relative) against a best-known solution, without explicit mention of multiple runs, variance, or statistical significance tests.", "limitations_acknowledged": ["Excessive sampling steps can reduce solution diversity and lead to local optima."]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2509.24323", "arxiv_url": "https://arxiv.org/abs/2509.24323", "title": "MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems", "authors": ["Kun"], "abstract": "", "published_date": "2025-09-29", "affiliations": "NTU, NUS, USTC, ZJU, BUAA, PKU", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 6, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper proposes a concrete solution to the 'brittleness' of LLM-generated code via a trained 'Rectifier' agent and replaces blind search with offline RL (Collaborative Tree Optimization). This directly addresses the crash-rate and sample-efficiency bottlenecks in our AlgoEvo and AILS-II projects."}, "brief": "MAS2 trains a tri-agent system (Generator, Implementer, Rectifier) using offline RL on decision trees to dynamically construct and repair multi-agent workflows. The results are strong, outperforming ADAS and MaAS on standard benchmarks while maintaining Pareto efficiency. The critical takeaway for us is the **Rectifier agent**: rather than discarding failed evolutionary candidates (as we currently do in AlgoEvo), we should implement a dedicated loop to patch runtime errors (e.g., API failures, dimension mismatches). Additionally, their 'Collaborative Tree Optimization' offers a rigorous method to fine-tune the 'Evolver' model using trajectory data, which could replace our current prompt-based meta-heuristics.", "methodology": {"core_method": "Recursive self-generation of multi-agent systems using a generator-implementer-rectifier tri-agent team, trained via Collaborative Tree Optimization (CTO) with offline reinforcement learning and value-scaled preference alignment.", "llm_role": "multi_agent_system_orchestration_and_rectification", "llm_model_used": "Qwen3-8B", "search_type": "hybrid", "novelty_claim": "MAS2 introduces a paradigm of recursive self-generation, where a multi-agent system autonomously architects bespoke multi-agent systems for diverse problems, transcending the limitations of the 'generate-once-and-deploy' paradigm.", "components": ["generator agent", "implementor agent", "rectifier agent", "Collaborative Tree Optimization (CTO)", "path credit propagation", "value-guided preference construction", "value-scaled optimization"], "training_required": true}, "tags": {"methods": ["multi_agent_systems", "llm_in_the_loop", "meta_agents", "tri_agent_architecture", "self_generative_mas", "self_configuring_mas", "self_rectifying_mas", "collaborative_tree_optimization", "offline_reinforcement_learning", "path_credit_propagation", "preference_alignment", "value_scaled_optimization", "llm_fine_tuned", "llm_as_heuristic", "llm_code_generation", "llm_as_evaluator"], "problems": ["automated_multi_agent_system_design", "multi_hop_question_answering", "deep_research", "code_generation", "mathematical_reasoning"], "contribution_type": ["new_method", "framework", "empirical_study", "sota_result", "generalization_study"], "framework_lineage": null, "specific_domain": null, "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Automating Multi-Agent System Design with Self-Generation, Self-Configuration, and Self-Rectification", "short": "MAS Design", "class_": "multi_agent_system_design", "properties": ["self-generative", "self-configuring", "self-rectifying", "dynamic", "adaptive", "error-prone_environments"], "scale": "diverse problems across multi-hop search, deep research, code generation, and mathematical reasoning"}, "lineage": {"direct_ancestors": [{"paper": "MAS-GPT", "relationship": "extends agent-based MAS generation from"}, {"paper": "ScoreFlow", "relationship": "extends agent-based MAS generation from"}, {"paper": "FlowReasoner", "relationship": "extends meta-agent training from"}], "closest_prior_work": "ScoreFlow", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Explore more complex and proactive rectification strategies beyond reactive adjustments.", "Investigate online learning and continuous adaptation for meta-agents in dynamic environments.", "Extend the MAS2 paradigm to multi-modal agent systems and other complex AI system designs.", "Formalize and explore multi-level recursive self-generation for MAS."], "transferable_to": ["Automated workflow generation for scientific discovery and data analysis pipelines.", "Adaptive control systems requiring real-time self-configuration and error correction.", "Designing and optimizing multi-modal LLM agent systems.", "Resource-constrained edge AI systems requiring dynamic task-specific agent configurations."], "open_weaknesses": ["The scalability of Collaborative Tree Optimization for extremely large or open-ended search spaces.", "Potential limitations of the rectification agent in handling novel, unforeseen system failures or paradigm shifts.", "The cost-performance trade-off for extremely long-horizon tasks, despite current Pareto efficiency claims.", "Generalizability to entirely new problem domains without any prior training data for meta-agents."]}, "artifacts": {"code_url": "https://github.com/yeyeyeah2/MAS2", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.36, "experiments": {"benchmarks": ["HotpotQA", "Bamboogle", "Natural Question (NQ)", "BrowseComp+", "HumanEval", "MBPP", "MATH"], "baselines": ["Qwen3-14B", "GPT-4o-mini", "QwQ-32B", "Qwen-2.5-72B", "GPT-4o", "CoT (GPT-4o)", "SC (GPT-4o)", "MedPrompt", "MultiPersona", "LLM-Debate", "DyLAN", "ADAS", "MaAS", "AFlow", "ScoreFlow"], "hardware": "Qwen3-8B finetuned via LoRA (rank=8, alpha=16)", "instance_sizes": []}, "results": {"vs_baselines": {"Qwen3-14B": "+27.1% on HotpotQA", "GPT-4o-mini": "+23.7% on HotpotQA", "QwQ-32B": "+37.5% on BrowseComp+", "Qwen-2.5-72B": "+17.1% on HotpotQA", "GPT-4o": "+19.8% on HotpotQA", "CoT (GPT-4o)": "+23.1% on HotpotQA", "SC (GPT-4o)": "+22.9% on HotpotQA", "MedPrompt": "+19.2% on Bamboogle", "MultiPersona": "+18.2% on HotpotQA", "LLM-Debate": "+22.4% on HotpotQA", "DyLAN": "+8.5% on HotpotQA", "ADAS": "+19.6% on MATH", "MaAS": "+5.7% on HotpotQA", "AFlow": "+11.4% on HotpotQA", "ScoreFlow": "+9.3% on BrowseComp+"}, "scalability": "MAS2 consistently establishes a new cost-performance Pareto frontier by dynamically assigning LLMs and configuring system architectures based on task complexity, and generalizes to unseen LLM backbones.", "statistical_rigor": "Results are reported as the average of three random runs.", "limitations_acknowledged": []}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2506.04203", "arxiv_url": "https://arxiv.org/abs/2506.04203", "title": "Cascadia: An Efficient Cascade Serving System for Large Language Models", "authors": ["Youhe"], "abstract": "", "published_date": "2025-09-29", "affiliations": "Princeton University, University of Cambridge, Tsinghua University, HKUST, Shanghai Jiaotong University", "category": "OR for Generative AI", "relevance": {"methodological": 7, "problem": 8, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper directly overlaps with our 'GPUSched' and 'OR for Generative AI' workstreams, specifically using MILP for LLM resource allocation. It establishes a strong baseline for cascade serving that we must benchmark against or differentiate from."}, "brief": "Jiang et al. propose CASCADIA, a bi-level optimization framework for LLM cascade serving that iterates between an MILP solver for hardware deployment (choosing DP/TP/PP strategies) and a Chebyshev-guided solver for routing thresholds. They demonstrate 2.3x average throughput gains over SGLang and CascadeServe on H100 clusters, backed by rigorous ablation studies. The key takeaway is the effective decomposition of the NP-hard joint optimization problem: freezing routing to solve deployment via MILP, then optimizing routing against that deployment. This is a direct reference point for our 'GPUSched' project, validating the efficacy of formal integer programming in LLM resource allocation.", "methodology": {"core_method": "Bi-level optimization with MILP for deployment and Chebyshev-guided method for routing", "llm_role": "evaluator", "llm_model_used": "GPT-4o", "search_type": "hybrid", "novelty_claim": "We propose a bi-level approach that jointly optimizes deployment and routing for LLM cascade serving, formulated as a constrained optimization problem.", "components": ["Bi-level optimization", "MILP deployment solver", "Chebyshev-guided routing solver", "Parallelism strategy search (DP, TP, PP)", "Threshold-based routing", "LLM-as-a-Judge", "System simulator", "Re-scheduling mechanism"], "training_required": false}, "tags": {"methods": ["bi_level_optimization", "mixed_integer_linear_programming", "chebyshev_guided_optimization", "data_parallelism", "tensor_parallelism", "pipeline_parallelism", "llm_as_evaluator", "system_algorithm_co_design", "resource_allocation_optimization", "load_balancing", "threshold_based_routing", "dynamic_scheduling"], "problems": ["llm_serving_optimization", "resource_allocation", "latency_quality_tradeoff", "gpu_scheduling", "multi_model_inference", "workload_heterogeneity_management"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": null, "specific_domain": "llm_serving_optimization", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "LLM Cascade Serving Optimization", "short": "LLM Cascade Serving", "class_": "resource_allocation", "properties": ["model_heterogeneity", "workload_heterogeneity", "latency_quality_tradeoff", "co_optimization", "dynamic"], "scale": "3-model cascades, up to 80 GPUs"}, "lineage": {"direct_ancestors": [{"paper": "FrugalGPT", "relationship": "builds on threshold-based routing from"}, {"paper": "AutoMix", "relationship": "builds on threshold-based routing from"}, {"paper": "CascadeServe", "relationship": "improves upon existing cascade serving systems like"}, {"paper": "DistServe", "relationship": "analogous re-scheduling mechanism to"}], "closest_prior_work": "CascadeServe", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["explore more complex cascade architectures beyond sequential", "integrate advanced LLM inference optimizations like speculative decoding", "extend to multi-tenant or disaggregated LLM serving environments", "dynamic adaptation to changing quality requirements or resource availability"], "transferable_to": ["other multi_model_serving scenarios", "edge_llm_serving", "general_resource_allocation_problems_with_latency_quality_tradeoffs"], "open_weaknesses": ["scheduling overhead for extremely rapid workload changes", "reliance on simulator for latency estimation (potential real-world variability)", "complexity of configuring and maintaining bi-level optimization in production", "judger (GPT-4o) latency, cost, and potential biases"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_status": "growing", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.89, "experiments": {"benchmarks": ["MT-Bench (workload traces)"], "baselines": ["SGLang (stand-alone LLMs)", "CascadeServe", "Sarathi-Serve"], "hardware": "NVIDIA H100-80GB GPUs (32 total), NVLink, Infiniband, 12-core CPU", "instance_sizes": [16, 32, 48, 64, 80]}, "results": {"vs_baselines": {"SGLang (stand-alone LLMs)": "up to 4x lower latency, up to 5x higher throughput (2.8x/3x average)", "CascadeServe": "up to 2.5x lower latency, up to 3.3x higher throughput (1.7x average)", "Sarathi-Serve": "up to 1.95x higher throughput (1.64x average)"}, "scalability": "Scheduling algorithm scales to 80 GPUs within one minute; system shows consistent performance improvements under fluctuating workloads.", "statistical_rigor": "Reports 95% SLO attainment, average quality requirements, and average throughput. Simulator accuracy within 2-7% absolute error.", "limitations_acknowledged": ["Scheduling overhead (mitigated by parallelization)"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2510.05115", "arxiv_url": "https://arxiv.org/abs/2510.05115", "title": "SAC-Opt: Semantic Anchors for Iterative Correction in Optimization Modeling", "authors": ["Yansen"], "abstract": "", "published_date": "2025-09-28", "affiliations": "Huawei Noah’s Ark Lab, Huawei’s Supply Chain Management Department, City University of Hong Kong", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 8, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "The 'backward-guided' verification (code-to-text reconstruction) is a practical, high-impact engineering solution to the 'silent error' problem in OR modeling. It directly complements our OR-Bench work and could be integrated into our code generation pipelines to filter semantically incorrect but syntactically valid models."}, "brief": "SAC-Opt introduces a verification loop where generated Gurobi code is back-translated into natural language ('semantic anchors') to check for alignment with the original problem description. Empirical results are strong, demonstrating a ~22% accuracy improvement on the ComplexLP dataset over OptiMUS-0.3 by catching logic errors that solver feedback misses. The primary takeaway is the utility of granular, constraint-level back-translation as a process reward signal, which we should adopt to improve the reliability of our automated modeling agents.", "methodology": {"core_method": "Backward-guided iterative semantic alignment and correction using LLM agents", "llm_role": "code_writer, evaluator, decomposition_guide", "llm_model_used": "GPT-4o", "search_type": "improvement", "novelty_claim": "SAC-Opt is the first optimization modeling framework that performs proactive semantic verification to detect silent semantic errors that solver-driven checks cannot capture.", "components": ["Extract Agent", "Trans Agent", "Recons Agent", "Verif Agent", "Optimization Solver (Gurobi)"], "training_required": false}, "tags": {"methods": ["llm_code_generation", "llm_as_evaluator", "llm_in_the_loop", "iterative_correction", "semantic_alignment", "prompt_engineering", "multi_agent_system", "sentence_embedding", "optimization_solver", "sac_opt"], "problems": ["optimization_modeling", "program_synthesis", "semantic_error_detection", "linear_programming", "combinatorial_optimization"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "optimus", "specific_domain": "optimization_modeling", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Optimization Modeling", "short": "OM", "class_": "program_synthesis", "properties": ["semantic_fidelity", "iterative_correction", "natural_language_to_code", "solver_executable"], "scale": "various, from simple to complex optimization problems"}, "lineage": {"direct_ancestors": [{"paper": "AhmadiTeshnizi et al., 2024a", "relationship": "extends correction mechanisms from OptiMUS-0.3 and adopts its extraction strategy"}, {"paper": "Shinn et al., 2023", "relationship": "improves upon solver-driven feedback in Reflexion with semantic feedback"}, {"paper": "Xiao et al., 2024", "relationship": "improves upon forward-pass LLM-based optimization workflows"}], "closest_prior_work": "OptiMUS-0.3", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Develop more accurate and computationally efficient semantic alignment strategies", "Explore strategies to balance verification quality with computational efficiency", "Extend to other optimization problem types beyond those covered by current datasets", "Investigate methods to further improve the robustness and accuracy of structured data extraction"], "transferable_to": ["Automated code generation and debugging in other programming domains", "Formal verification and semantic validation of AI-generated content", "Systems requiring iterative refinement based on semantic feedback", "Different mathematical modeling languages or solver interfaces"], "open_weaknesses": ["Annotation noise in existing public datasets", "Potential for computational overhead on simpler problems where initial generation is often correct", "Reliance on the quality of initial structured data extraction", "Coarse similarity signals in verification can introduce noise or misalignment"]}, "artifacts": {"code_url": "https://github.com/Forrest-Stone/SAC-Opt", "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.58, "experiments": {"benchmarks": ["NL4OPT", "IndustryOR", "EasyLP", "ComplexLP", "NLP4LP", "ReSocratic", "ComplexOR"], "baselines": ["Standard", "Chain-of-Thought (CoT)", "Chain-of-Experts (CoE)", "CAFA", "Reflexion", "OptiMUS-0.2", "OptiMUS-0.3"], "hardware": "Cloud-based LLM (GPT-4o), local CPU for SentenceTransformer, Gurobi solver", "instance_sizes": [214, 42, 545, 111, 178, 403, 18]}, "results": {"vs_baselines": {"Standard": "+25.6% (NL4OPT)", "CoT": "+24.6% (NL4OPT)", "CoE": "+2.1% (EasyLP)", "CAFA": "+18.7% (NL4OPT)", "Reflexion": "+10.7% (EasyLP)", "OptiMUS-0.2": "+7.3% (EasyLP)", "OptiMUS-0.3": "+27.5% (ComplexLP)"}, "scalability": "SAC-Opt shows robust performance and larger gains on more complex datasets, while baselines degrade.", "statistical_rigor": "Results averaged over 5 independent runs; mean and standard deviation reported for correction/debugging attempts.", "limitations_acknowledged": ["Annotation noise in public datasets", "Dependence on structured data extraction accuracy (though shown to be robust)"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2509.23331", "arxiv_url": "https://arxiv.org/abs/2509.23331", "title": "C-Evolve: Consensus-based Evolution for Prompt Groups", "authors": ["Tiancheng"], "abstract": "", "published_date": "2025-09-27", "affiliations": "Westlake University", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 7, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper bridges our two primary research pillars—evolutionary search and multi-agent systems—by demonstrating how to evolve *teams* of agents rather than single optima. The 'voting score' mechanism is a concrete, transferable technique to induce complementarity in our heuristic portfolios without complex diversity objectives."}, "brief": "C-Evolve modifies island-based evolution to optimize a group of prompts that maximize consensus accuracy (majority vote) rather than individual performance. The authors introduce a 'voting score' fitness function—calculated via Exponential Moving Average (EMA) of an individual's contribution to sampled groups—which successfully drives the population toward diverse, complementary strategies that outperform ensembles of individually optimized prompts (beating AlphaEvolve by ~4% on Qwen3-8B). The single most actionable takeaway is the **EMA voting score mechanism**: we can steal this exact fitness formulation to evolve portfolios of complementary VRP heuristics in AlgoEvo, replacing our current focus on converging to a single 'best' solver. While the benchmarks are standard (MATH, HotpotQA), the method offers a robust solution to the 'single heuristic limitation' we face in OR.", "methodology": {"core_method": "Island-based evolutionary algorithm with Exponential Moving Average (EMA) voting score as fitness, optimizing groups of prompts for consensus via majority voting or LLM-based aggregation.", "llm_role": "evolver, consensus_aggregator", "llm_model_used": "Qwen3-8B, GPT-4.1-mini", "search_type": "improvement", "novelty_claim": "C-Evolve is the first algorithm developing the group consensus among prompts to drive their evolution for making more accurate predictions.", "components": ["island-based evolutionary algorithm", "voting score (EMA)", "LLM-based evolver", "consensus aggregator (majority voting)", "consensus aggregator (LLM-based)", "feedback mechanism", "warm-up stage", "voting stage", "migration"], "training_required": true}, "tags": {"methods": ["evolutionary_algorithm", "island_based_evolutionary_algorithm", "genetic_algorithm", "prompt_optimization", "llm_in_the_loop", "llm_as_evolver", "llm_as_aggregator", "majority_voting", "exponential_moving_average", "program_synthesis", "llm_evolutionary_search"], "problems": ["multi_hop_question_answering", "instruction_following", "fact_extraction", "claim_verification", "mathematical_problem_solving", "multiple_choice_question_answering", "llm_prompt_optimization"], "contribution_type": ["new_method", "sota_result", "empirical_study", "framework"], "framework_lineage": "alphaevolve", "specific_domain": "llm_prompt_optimization", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Consensus-Based Prompt Group Optimization for Compound AI Systems", "short": "Prompt Group Optimization", "class_": "llm_evolutionary_search", "properties": ["black_box_llms", "group_optimization", "consensus_driven", "island_based_evolution", "feedback_driven"], "scale": "5 tasks, 3 islands, 10 prompts/island, 100-300 questions/task"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2507.19457", "relationship": "extends prompt evolution from GEPA by incorporating group consensus"}, {"paper": "arXiv:2506.13131", "relationship": "extends island-based evolutionary algorithm from AlphaEvolve to optimize prompt groups for consensus"}], "closest_prior_work": "AlphaEvolve", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Explore more sophisticated consensus mechanisms beyond majority voting and LLM-selection.", "Investigate explicit diversity-promoting mechanisms in multi-island evolution.", "Apply C-Evolve to other types of compound AI systems or black-box optimization problems.", "Further optimize computational efficiency for larger-scale problems."], "transferable_to": ["Automated algorithm design for other domains (e.g., traditional OR heuristics).", "Multi-agent LLM systems requiring coordinated decision-making.", "Black-box optimization problems where ensemble methods are beneficial.", "Optimizing other forms of LLM components (e.g., tool use, control flow logic)."], "open_weaknesses": ["Computational overhead, especially for open-ended tasks requiring LLM-based aggregation.", "Sensitivity to hyperparameters like EMA smoothing factor (alpha).", "Limitations of current consensus aggregators (LLM-summary vs LLM-selection).", "Scalability to a much larger number of islands or population sizes."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.76, "experiments": {"benchmarks": ["HotpotQA", "IFBench", "Hover", "MATH", "GPQA"], "baselines": ["Baseline", "GEPA", "AlphaEvolve"], "hardware": "8xH800 GPUs (for Qwen3-8B), OpenAI API (for GPT-4.1-mini)", "instance_sizes": [300, 294, 300, 300, 264]}, "results": {"vs_baselines": {"Baseline_Qwen3-8B": "+20.64% on HotpotQA, +12.59% on IFBench, +12.67% on Hover, +17.67% on MATH, +5.72% on GPQA", "GEPA_Qwen3-8B": "+4.95% on HotpotQA, +9.87% on IFBench, +7.00% on Hover", "AlphaEvolve_Qwen3-8B": "+5.36% on HotpotQA, +2.73% on IFBench, +5.67% on Hover, +2.67% on MATH, +4.07% on GPQA", "Baseline_GPT-4.1-mini": "+26.40% on HotpotQA, +7.83% on IFBench, +9.66% on Hover, +16.67% on MATH, +19.92% on GPQA", "GEPA_GPT-4.1-mini": "+2.25% on HotpotQA, +1.37% on IFBench, +12.66% on Hover", "AlphaEvolve_GPT-4.1-mini": "+3.33% on HotpotQA, +2.72% on IFBench, +1.33% on Hover, +2.67% on MATH, +3.25% on GPQA"}, "scalability": "C-Evolve consistently outperforms single prompts on harder problems (MATH Levels 3-5) due to consensus, and employs caching and asynchronous prompting to mitigate computational overhead.", "statistical_rigor": "Performance is reported as average accuracy on test sets. Evolutionary trajectories (EMA voting score) are shown. Ablation studies include comparisons of different aggregators and EMA smoothing factors. MATH Level 5 results are analyzed by problem case (all answers same, two of three same, all distinct) to show consensus benefits.", "limitations_acknowledged": ["Computational overhead during evolution and inference (mitigated by optimizations)"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2506.11087", "arxiv_url": "https://arxiv.org/abs/2506.11087", "title": "Enhancing Delta Compression in LLMs via SVD-based Quantization Error Minimization", "authors": ["Boya"], "abstract": "", "published_date": "2025-09-27", "affiliations": "Tsinghua University, Fudan University, Southern University of Science and Technology, Shanghai University of Finance and Economics", "category": "OR for Generative AI", "relevance": {"methodological": 5, "problem": 7, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While the ILP formulation is standard OR, it validates our 'OR for AI' thesis by significantly beating heuristics. Crucially, their ILP solver takes ~30 minutes, making this a perfect potential benchmark/application for our EvoCut or AlgoEvo optimization methods to accelerate."}, "brief": "PRINMIX replaces heuristic quantization of LLM delta-weights with a 0/1 Integer Linear Programming (ILP) formulation to minimize reconstruction error. The results are strong and backed by numbers, showing ~22% improvement on AIME2024 and 6x storage savings compared to Delta-CoMe. For us, the key takeaway is not the compression itself, but the formulation: it proves that exact OR modeling outperforms heuristics in LLM serving infrastructure. Additionally, the reported 30-minute solving time suggests this problem could serve as a valuable testbed for our own evolutionary solver acceleration (EvoCut/AlgoEvo).", "methodology": {"core_method": "SVD-based mixed-precision delta compression formulated as a 0/1 Integer Linear Programming (ILP) problem for quantization error minimization, integrated with Reconstruction Target Correction (RTC)", "llm_role": "none", "llm_model_used": null, "search_type": "exact", "novelty_claim": "PRINMIX is a rigorous SVD-based framework that models quantization as an optimization problem, grounding the design in mathematical mechanisms.", "components": ["Singular Value Decomposition (SVD)", "Quantization Error Derivation", "0/1 Integer Linear Programming (ILP)", "Reconstruction Target Correction (RTC)", "GPTQ"], "training_required": false}, "tags": {"methods": ["singular_value_decomposition", "mixed_precision_quantization", "integer_linear_programming", "optimization", "post_training_quantization", "gptq", "delta_compression", "reconstruction_target_correction", "quantization_error_minimization"], "problems": ["llm_compression", "delta_weight_compression", "llm_deployment_efficiency", "reasoning", "math_word_problems", "code_generation", "multimodal_qa"], "contribution_type": ["new_method", "framework", "sota_result", "theoretical_result", "empirical_study"], "framework_lineage": "gptq", "specific_domain": "llm_compression", "llm_coupling": null}, "problem": {"formal_name": "SVD-based Delta Compression of Large Language Model Delta Parameters", "short": "LLM Delta Compression", "class_": "llm_compression", "properties": ["SVD-based", "mixed-precision", "post-training", "delta_weights", "quantization_error_minimization"], "scale": "7B-14B LLMs"}, "lineage": {"direct_ancestors": [{"paper": "Delta-CoMe (Ping et al., 2024)", "relationship": "improves theoretical foundation of mixed-precision SVD delta-compression from"}, {"paper": "GPTQ (Frantar et al., 2022)", "relationship": "follows the structure of post-training quantization from"}], "closest_prior_work": "Delta-CoMe (Ping et al., 2024)", "novelty_type": "incremental"}, "extensions": {"next_steps": ["Further minimize performance degradation on challenging tasks", "Accelerate ILP solving time with commercial solvers or reduced solution space", "Explore applicability to other compression ratios"], "transferable_to": ["Other neural network compression tasks beyond LLMs", "Other types of delta weights from different fine-tuning methods", "Full model quantization"], "open_weaknesses": ["Lossy compression leading to performance degradation on certain tasks", "Computational cost of ILP solving", "Dependency on calibration data"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.74, "experiments": {"benchmarks": ["Math500", "AIME2024", "GSM8K", "HumanEval", "MBPP", "GQA", "ScienceQA"], "baselines": ["SVD-based low-rank compression", "BitDelta", "Delta-CoMe"], "hardware": "NVIDIA L20 GPU, NVIDIA L40 GPU", "instance_sizes": [7, 13, 14]}, "results": {"vs_baselines": {"Delta-CoMe": "Outperforms by 2.9% avg (7B) and 2.2% avg (13-14B), with up to 26.9% on AIME2024.", "Low-Rank": "Significantly outperforms (e.g., +53.5% avg on 7B models).", "BitDelta": "Significantly outperforms (e.g., +82.7% avg on 7B models)."}, "scalability": "PRINMIX improves throughput 6x and decreases end-to-end latency 100x compared to naive methods, scaling better than baselines at high model counts.", "statistical_rigor": "Results reported as mean(std) over three runs for Delta-CoMe and PRINMIX.", "limitations_acknowledged": ["PRINMIX remains a lossy compression method for certain tasks.", "Performance degradation can be substantial in some cases."]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2505.22954", "arxiv_url": "https://arxiv.org/abs/2505.22954", "title": "Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents", "authors": ["Jenny"], "abstract": "", "published_date": "2025-09-26", "affiliations": "Sakana AI, Vector Institute, University of British Columbia, Canada CIFAR AI Chair", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 10, "problem": 8, "inspirational": 10}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper successfully demonstrates 'Evolving the Evolver'—a core goal of our AlgoEvo project—by enabling agents to modify their own source code rather than just prompts. It provides a concrete, empirically validated blueprint for self-referential evolutionary search that we can immediately adapt."}, "brief": "DGM implements a population-based evolutionary loop where agents modify their own Python source code (tools, memory, flow) to improve performance on coding benchmarks, rather than just optimizing prompts or parameters. Results are strong and verified: it boosts a base agent from 20% to 50% on SWE-bench Verified, matching handcrafted SoTA, with ablations proving the necessity of the population archive (open-endedness) over single-lineage hill climbing. **Key Takeaway:** The 'self-diagnosis' mechanism—feeding execution logs to a model to propose specific *architectural* code changes (e.g., implementing a 'str_replace' tool to fix granular editing errors)—is the exact mechanism we need to implement for evolving our heuristic searchers. This validates that LLM-driven code evolution is viable for complex logic improvement, not just toy tasks.", "methodology": {"core_method": "Darwin Gödel Machine (DGM) with iterative self-modification, empirical validation, and population-based open-ended exploration", "llm_role": "coding_agent, self_modifier, problem_solver, diagnosis_agent", "llm_model_used": "Claude 3.5 Sonnet (New), o3-mini, OpenAI o1", "search_type": "improvement", "novelty_claim": "We introduce the Darwin Gödel Machine (DGM), a novel self-improving system that iteratively modifies its own code (thereby also improving its ability to modify its own codebase) and empirically validates each change using coding benchmarks.", "components": ["coding agent", "archive of agents", "self-modification module", "empirical evaluation module", "parent selection mechanism", "Bash tool", "Edit tool (with granular file viewing/editing, string replacement, text insertion, undo)"], "training_required": false}, "tags": {"methods": ["darwin_godel_machine", "open_ended_evolution", "evolution_of_heuristics", "llm_code_generation", "llm_as_heuristic", "llm_as_evaluator", "llm_in_the_loop", "program_synthesis", "self_improving_ai", "tool_use", "quality_diversity", "meta_learning"], "problems": ["software_engineering_tasks", "code_generation", "automated_algorithm_design", "heuristic_evolution", "llm_hallucination_mitigation"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "dgm", "specific_domain": "software_engineering_tasks", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Open-Ended Evolution of Self-Improving Agents", "short": "Self-Improving AI", "class_": "llm_evolutionary_search", "properties": ["self-improving", "open-ended", "code-modifying", "empirical_validation", "tool_use"], "scale": "SWE-bench (Python, multi-file edits), Polyglot (multi-language, single-file implementations)"}, "lineage": {"direct_ancestors": [{"paper": "Schmidhuber, 2007", "relationship": "relaxes theoretical requirements of Gödel machine"}, {"paper": "Darwin, 2023", "relationship": "inspired by Darwinian evolution principles"}, {"paper": "Wang et al., 2019", "relationship": "inspired by open-endedness research"}], "closest_prior_work": "Robeyns et al., 2025", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["allow dgm to modify its own open_ended_exploration process", "extend self_modification to update foundation models", "develop self_improving_ai systems beyond coding domain", "co_evolve target task distribution"], "transferable_to": ["other_programming_languages", "other_ai_domains", "ai_safety_settings", "complex_software_engineering_problems"], "open_weaknesses": ["requires_extensive_compute", "limited_by_underlying_fm_capabilities", "fixed_open_ended_exploration_process", "coding_benchmarks_may_not_fully_reflect_self_improvement", "potential_for_objective_hacking", "falls_short_of_closed_source_sota"]}, "artifacts": {"code_url": "https://github.com/jennyzzt/dgm", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.76, "experiments": {"benchmarks": ["SWE-bench Verified", "Polyglot"], "baselines": ["DGM without self-improving agents", "DGM without open-ended exploration", "OpenHands + CodeAct v2.1 (claude-3-5-sonnet-20241022)", "Aider", "DGM Greedy"], "hardware": "unspecified (estimated 2 weeks and $22,000 API costs for SWE-bench run)", "instance_sizes": [10, 50, 200]}, "results": {"vs_baselines": {"DGM without self-improving agents": "outperforms, gains taper off quickly", "DGM without open-ended exploration": "outperforms, makes very little progress", "OpenHands + CodeAct v2.1 (claude-3-5-sonnet-20241022)": "DGM 50.0% vs SoTA 51.0% on 200 tasks (comparable)", "Aider": "DGM 30.7% vs Aider (approx 15-18% on pass@1) (DGM far surpasses)", "DGM Greedy": "DGM 50.0% (SWE-bench) / 38.0% (Polyglot) vs DGM Greedy 39.7% (SWE-bench) / 30.0% (Polyglot)"}, "scalability": "DGM automatically improves its coding capabilities, increasing performance on SWE-bench from 20.0% to 50.0% and on Polyglot from 14.2% to 30.7%. Improvements generalize across FMs, benchmarks, and programming languages.", "statistical_rigor": "DGM algorithm run three times on Polyglot benchmark, achieving mean accuracy of 40.7% with a standard deviation of 2.3%.", "limitations_acknowledged": ["falls short of closed-source SoTA SWE-bench solutions", "requires extensive compute", "limited by the capabilities of the underlying FM", "open-ended exploration process is fixed and not modifiable by the DGM", "coding benchmarks may not fully reflect self-improvement ability"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2509.22979", "arxiv_url": "https://arxiv.org/abs/2509.22979", "title": "OptiMind: Teaching LLMs to Think Like Optimization Experts", "authors": ["Zeyi"], "abstract": "", "published_date": "2025-09-26", "affiliations": "Microsoft Research, Stanford University, University of Washington", "category": "Generative AI for OR", "relevance": {"methodological": 5, "problem": 9, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper invalidates current baselines on IndustryOR and OptMATH by revealing 30-50% error rates in the ground truth. It directly impacts our 'OR-Bench' project validity. We must adopt their cleaned datasets and can leverage their error taxonomy."}, "brief": "The authors fine-tune a 20B model for MILP formulation, but the critical contribution is a rigorous audit of standard benchmarks (IndustryOR, OptMATH), revealing that 30-50% of instances are flawed (missing data, wrong ground truth, infeasible). They introduce a 'class-based error analysis' where the model classifies a problem (e.g., TSP) and retrieves specific, expert-written hints to avoid common pitfalls, boosting accuracy by ~20%. **Takeaway:** We must immediately replace our benchmark versions with their cleaned sets for the OR-Bench project. Additionally, their library of 'error hints' per problem class is a high-value artifact we can scrape and inject into AlgoEvo's prompt templates to improve initial population quality.", "methodology": {"core_method": "Supervised fine-tuning (SFT) of a 20B-parameter LLM (GPT-OSS-20B variant) on a semi-automatically cleaned, class-specific error-analyzed training dataset, combined with error-aware prompting and multi-turn self-correction at inference.", "llm_role": "code_writer", "llm_model_used": "GPT-OSS-20B", "search_type": "hybrid", "novelty_claim": "We systematically integrate optimization expertise to improve formulation accuracy for mixed-integer linear programming, leveraging semi-automated, class-based error analysis to guide both training and inference.", "components": ["class-based error analysis", "training data cleaning (solution regeneration, missing data infilling)", "supervised fine-tuning", "error-aware prompting with class-specific hints", "self-consistency (majority voting)", "multi-turn correction with solver feedback"], "training_required": true}, "tags": {"methods": ["supervised_fine_tuning", "llm_code_generation", "llm_in_the_loop", "llm_as_evaluator", "error_analysis", "data_cleaning", "self_consistency", "majority_voting", "multi_turn_feedback"], "problems": ["mixed_integer_linear_programming", "optimization_modeling", "traveling_salesman_problem", "set_cover", "flow_shop_scheduling", "capacitated_facility_location_problem", "diet_problem", "network_optimization", "transportation_problem", "production_planning_problem", "job_shop_scheduling"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "llm_optimization_modeling", "specific_domain": "mixed_integer_linear_programming", "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Mixed-Integer Linear Programming Formulation from Natural Language", "short": "MILP Formulation", "class_": "program_synthesis", "properties": ["mixed_integer", "linear", "natural_language_input", "code_generation_output"], "scale": "up to 6 nodes (TSP), 3 products/5 periods (production planning)"}, "lineage": {"direct_ancestors": [{"paper": "GPT-OSS-20B", "relationship": "fine_tunes_base_model"}, {"paper": "SIRL (Chen et al., 2025)", "relationship": "improves_upon_sota_llm_for_optimization"}, {"paper": "OptMATH (Lu et al., 2025)", "relationship": "uses_and_cleans_training_data_from"}, {"paper": "ORLM (Tang et al., 2024)", "relationship": "uses_and_cleans_training_data_from"}], "closest_prior_work": "SIRL (Chen et al., 2025)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["apply to supply_chain_management", "adapt to enterprise_specific_scenarios", "extend to non_linear_optimization_problems", "embed more expert_knowledge into future_llms"], "transferable_to": ["other_mathematical_programming_paradigms", "other_code_generation_tasks", "scientific_experiment_design", "financial_modeling"], "open_weaknesses": ["reliance_on_expert_curated_error_analysis", "scalability_to_very_large_problem_instances", "diminishing_returns_of_multi_turn_correction_and_majority_voting", "handling_of_non_linear_problems_due_to_data_scarcity"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_9", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.06, "experiments": {"benchmarks": ["IndustryOR", "Mamo-Complex", "OptMATH"], "baselines": ["GPT-OSS-20B base model", "QWEN3-32B", "SIRL-GUROBI32B", "O4-MINI", "GPT-5"], "hardware": "single node with eight NVIDIA HGX B200 GPUs (SFT), 4 compute nodes with eight 80GB NVIDIA H100 GPUs (data cleaning/evaluation)", "instance_sizes": [3, 5, 6]}, "results": {"vs_baselines": {"GPT-OSS-20B base model": "+2.7% to +20.7% (SFT, no hints, 1 turn)", "QWEN3-32B": "+7% to +23% (SFT, no hints, 1 turn)", "SIRL-GUROBI32B": "+13% to +26% (SFT, no hints, 1 turn)", "O4-MINI": "within 1.9% on IndustryOR, +13.9% on Mamo-Complex, +3.8% on OptMATH (SFT+hints, 1 turn vs O4-MINI, no hints, 1 turn)", "GPT-5": "within 7% on IndustryOR, 1.8% on OptMATH, +4.2% on Mamo-Complex (SFT+hints, 5 turns vs GPT-5, no hints, 5 turns)"}, "scalability": "The paper primarily discusses the scalability of its multi-turn inference process, noting diminishing returns with more turns, but does not explicitly detail how formulation accuracy scales with increasing problem instance size (e.g., number of variables or constraints).", "statistical_rigor": "All reported results are averaged across 10 independent experiments using different random seeds, with solutions grouped within a relative and absolute tolerance of 10^-6.", "limitations_acknowledged": ["Future LLMs may embed more expert knowledge.", "Second-order cone programming problems were out of scope due to scarce and unrepresentative training data.", "Marginal benefit of majority voting diminishes with multiple turns, and gains from additional turns also exhibit diminishing returns."]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2509.22558", "arxiv_url": "https://arxiv.org/abs/2509.22558", "title": "StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models", "authors": ["Chenyu"], "abstract": "", "published_date": "2025-09-26", "affiliations": "Shanghai Jiao Tong University", "category": "Generative AI for OR", "relevance": {"methodological": 9, "problem": 8, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper directly attacks the limitations of discriminative Process Reward Models (PRMs) which we currently use. The proposed Generative PRM (GenPRM) and Weighted DPO (W-DPO) offer a concrete architectural upgrade for our AlgoEvo and MASPRM projects to handle long-horizon dependencies."}, "brief": "Zhou et al. propose StepORLM, a framework where an 8B policy and a **Generative Process Reward Model (GenPRM)** co-evolve. Unlike standard discriminative PRMs that score steps in isolation, their GenPRM generates a reasoning trace to evaluate the full trajectory's logic before assigning credit, addressing the interdependency of OR constraints. They align the policy using **Weighted DPO**, where preference weights are derived from the GenPRM's process scores. They claim to beat GPT-4o and DeepSeek-V3 on 6 OR benchmarks (e.g., NL4Opt, MAMO) with an 8B model. **Key Takeaway:** We should test **Generative PRMs** immediately for AlgoEvo; asking the critic to 'explain then score' (generative) rather than just 'score' (discriminative) likely fixes the credit assignment noise in our long-horizon search.", "methodology": {"core_method": "Self-evolving framework with generative process supervision using Weighted Direct Preference Optimization (W-DPO) and Supervised Fine-Tuning (SFT)", "llm_role": "code_writer, evaluator, decomposition_guide, evolutionary_search", "llm_model_used": "Qwen3-8B", "search_type": "hybrid", "novelty_claim": "We are the first to introduce generative process supervision to holistically evaluate entire reasoning trajectories on OR problems, mitigating the credit assignment problem and myopia issue of existing RL-based works.", "components": ["Policy model", "Generative Process Reward Model (GenPRM)", "External solver", "Data synthesis pipeline", "Supervised Fine-Tuning (SFT)", "Weighted Direct Preference Optimization (W-DPO)"], "training_required": true}, "tags": {"methods": ["llm_as_heuristic", "llm_as_evaluator", "llm_code_generation", "llm_in_the_loop", "llm_evolutionary_search", "reinforcement_learning", "weighted_direct_preference_optimization", "supervised_fine_tuning", "generative_process_supervision", "process_reward_model", "self_improving_search", "program_synthesis"], "problems": ["operations_research_modeling", "linear_programming", "mixed_integer_linear_programming", "traveling_salesman_problem", "combinatorial_optimization"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "steporlm", "specific_domain": "combinatorial_optimization", "llm_coupling": "rl_trained"}, "problem": {"formal_name": "Operations Research Optimization Modeling", "short": "OR Modeling", "class_": "optimization_modeling", "properties": ["long_horizon", "interdependent_reasoning", "combinatorial", "mixed_integer_linear_programs"], "scale": "Various scales of OR problems, including complex MILPs"}, "lineage": {"direct_ancestors": [{"paper": "Solver-Informed RL (SIRL) (Chen et al., 2025b)", "relationship": "addresses limitations of outcome-based RL in"}, {"paper": "Process Reward Models (PRMs)", "relationship": "addresses limitations of discriminative process supervision in"}, {"paper": "Direct Preference Optimization (DPO) (Zheng et al., 2025a)", "relationship": "extends DPO with weighted objective for"}], "closest_prior_work": "Process Reward Models (PRMs)", "novelty_type": "paradigm_shift"}, "extensions": {"next_steps": ["Further investigation into performance on challenging datasets like IndustryOR", "Applying the co-evolved GenPRM as a universal verifier to other domains beyond OR", "Exploring more flexible output formats and implicit reasoning patterns in self-evolving iterations"], "transferable_to": ["Non-linear optimization problems", "Real-world, large-scale industrial optimization problems", "Other complex reasoning tasks requiring structured problem-solving"], "open_weaknesses": ["Credit assignment problem in outcome-based rewards remains a general challenge", "Myopia of conventional discriminative process supervision (addressed but context for comparison)", "Performance variability on small and challenging test sets (e.g., IndustryOR)", "Lack of explicit statistical significance tests or variance measures in experimental results"]}, "artifacts": {"code_url": null, "models_released": true, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_status": "stable", "bridge_score": 0.5012, "is_bridge": true, "priority_score": 8.46, "experiments": {"benchmarks": ["NL4Opt", "MAMO EasyLP", "MAMO ComplexLP", "NLP4LP", "ComplexOR", "IndustryOR", "ReSocratic"], "baselines": ["GPT-4o", "DeepSeek-V3", "Qwen3-32B", "Qwen2.5-72B-Instruct", "ORLM", "LLMOPT", "OptMATH", "OptiMUS-v0.3", "CoT", "CoE", "CAFA"], "hardware": "single node with 8 × NVIDIA H100 80 GB GPUs", "instance_sizes": []}, "results": {"vs_baselines": {"GPT-4o": "+29.6% Pass@1 accuracy", "DeepSeek-V3": "+10.2% Pass@1 accuracy", "Qwen3-32B": "+13.1% Pass@1 accuracy", "Qwen2.5-72B-Instruct": "+14.9% Pass@1 accuracy", "ORLM": "+20.6% Pass@1 accuracy", "LLMOPT": "+49.8% Pass@1 accuracy", "OptiMUS-v0.3": "+18.5% Pass@1 accuracy", "CoT": "+35.3% Pass@1 accuracy", "CoE": "+20.1% Pass@1 accuracy", "CAFA": "+34.0% Pass@1 accuracy"}, "scalability": "The co-evolved GenPRM acts as a universal inference-time verifier, substantially boosting the performance of other OR language models and enabling inference scaling.", "statistical_rigor": "Pass@1 accuracy is reported across benchmarks, with performance tracked over training iterations, but no explicit statistical significance tests or variance measures are provided.", "limitations_acknowledged": ["Credit assignment problem in outcome-based rewards", "Myopia of conventional discriminative process supervision", "Small size and challenging nature of IndustryOR test set leading to non-monotonic performance trends"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2509.21091", "arxiv_url": "https://arxiv.org/abs/2509.21091", "title": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute", "authors": ["Junpei"], "abstract": "", "published_date": "2025-09-25", "affiliations": "Mohamed bin Zayed University of Artificial Intelligence, New York University, RIKEN AIP, Institute of Science Tokyo, NEC Corporation", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 7, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "The adaptive sampling mechanism is a direct, mathematically grounded solution to the sample efficiency bottleneck in our evolutionary search evaluations. Additionally, the MILP formulation for ensemble weights is a perfect example of 'OR for AI' that aligns with our specific expertise."}, "brief": "This paper introduces a Bayesian adaptive stopping criterion (using Dirichlet process priors and Bayes factors) for majority voting, reducing test-time compute by 2-5x while maintaining asymptotic 'Best-of-Infinity' accuracy. They further demonstrate that optimizing weights for an ensemble of LLMs can be formulated as a Mixed-Integer Linear Program (MILP) by treating the decision boundaries as polytopes. **What we learned:** The Bayesian stopping logic is immediately transferable to AlgoEvo to reduce the cost of fitness evaluations—we can stop evaluating candidate solutions early if their performance distribution is statistically distinct. The MILP approach for ensembles also offers a concrete formulation we could adapt for our GPU scheduling and model serving optimization work.", "methodology": {"core_method": "Adaptive sampling for majority voting using Bayesian modeling (Dirichlet process prior and Bayes factor) to determine stopping criteria, combined with optimally weighted LLM ensembles formulated as a Mixed-Integer Linear Program (MILP) with a max-margin solution.", "llm_role": "answer_generator", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "We propose an adaptive generation scheme that selects N based on answer agreement, thereby efficiently allocating inference-time computation, and extend the framework to optimally weighted ensembles of multiple LLMs using a mixed-integer linear program.", "components": ["Adaptive sampling", "Bayesian modeling", "Dirichlet process prior", "Bayes factor", "Majority voting", "LLM ensemble", "Mixed-Integer Linear Program (MILP)", "Max-margin solution"], "training_required": false}, "tags": {"methods": ["best_of_n", "majority_voting", "adaptive_sampling", "bayesian_modeling", "dirichlet_process_prior", "bayes_factor", "llm_ensemble", "mixed_integer_linear_programming", "max_margin_optimization", "monte_carlo_methods", "llm_in_the_loop", "llm_as_answer_generator"], "problems": ["llm_inference_optimization", "llm_serving_optimization", "mathematical_reasoning", "scientific_reasoning", "ensemble_optimization"], "contribution_type": ["new_method", "framework", "empirical_study", "sota_result", "new_benchmark"], "framework_lineage": null, "specific_domain": "llm_reasoning_tasks", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Best-of-Infinity Asymptotic Performance of Test-Time Compute for Large Language Models", "short": "Best-of-∞", "class_": "llm_inference_optimization", "properties": ["adaptive_sampling", "majority_voting", "weighted_ensemble", "mixed_integer_linear_programming_formulation", "Bayesian_stopping_criteria"], "scale": "11 LLMs, up to 500 problems, at least 80 generations per LLM-problem pair"}, "lineage": {"direct_ancestors": [{"paper": "Wang et al., 2023", "relationship": "builds on majority voting for LLMs"}, {"paper": "Soto et al., 2016", "relationship": "extends Bayesian adaptive sampling (Urn model) to LLM generation"}], "closest_prior_work": "Soto et al., 2016", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Explore alternative Bayesian priors or frequentist stopping criteria for adaptive sampling.", "Extend MILP formulation to dynamically select N for each LLM in the ensemble.", "Investigate the impact of Chain-of-Thought (CoT) on adaptive sampling and ensemble performance.", "Apply the adaptive ensemble framework to other LLM tasks beyond reasoning."], "transferable_to": ["Other LLM tasks requiring robust answer aggregation (e.g., code generation, summarization).", "Crowdsourcing systems for aggregating human judgments from multiple annotators.", "General machine learning ensemble methods for adaptive resource allocation."], "open_weaknesses": ["Answer parsing can be sub-optimal for varied output formats (e.g., MATH500).", "Scalability of MILP for very large numbers of LLMs or answer categories.", "Theorem 1's assumption of a finite answer space might not hold for all open-ended LLM generations.", "The current method does not leverage Chain-of-Thought (CoT) prompting."]}, "artifacts": {"code_url": "https://github.com/jkomiyama/BoInf-code-publish", "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_3", "front_status": "growing", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.09, "experiments": {"benchmarks": ["AIME2024", "AIME2025", "GPQA-DIAMOND", "MATH500"], "baselines": ["Best-of-N (fixed budget)", "Single LLM (Bo1)", "Random selection", "Self-certainty", "INF-ORM-Llama3.1-70B", "Skywork-Reward-V2-Llama-3.1-8B", "Skywork-Reward-V2-Qwen3-8B", "LLM-as-a-judge (tournament)", "LLM-as-a-judge (set)"], "hardware": "local environment (for open-weight LLMs), highspy (MILP solver)", "instance_sizes": [30, 198, 500]}, "results": {"vs_baselines": {"Best-of-N (fixed budget)": "Adaptive sampling achieves same accuracy with substantially fewer samples (2x-5x reduction) and tokens.", "Single LLM (Bo1)": "LLM ensemble outperforms any single LLM (e.g., 93.3% vs 90.0% for GPT-OSS-20B on AIME2025).", "Random": "Majority voting outperforms random selection on AIME2025 (85.42% vs 76.25%).", "Self-certainty": "Majority voting outperforms self-certainty on AIME2025 (85.42% vs 75.83%).", "INF-ORM-Llama3.1-70B": "Majority voting outperforms INF-ORM-Llama3.1-70B on AIME2025 (85.42% vs 79.79%).", "Skywork-Reward-V2-Llama-3.1-8B": "Majority voting outperforms Skywork-Reward-V2-Llama-3.1-8B on AIME2025 (85.42% vs 79.79%).", "Skywork-Reward-V2-Qwen3-8B": "Majority voting outperforms Skywork-Reward-V2-Qwen3-8B on AIME2025 (85.42% vs 80.00%).", "LLM-as-a-judge (tournament)": "Majority voting outperforms LLM-as-a-judge (tournament) on AIME2025 (85.42% vs 82.92%).", "LLM-as-a-judge (set)": "Majority voting outperforms LLM-as-a-judge (set) on AIME2025 (85.42% vs 81.25%)."}, "scalability": "Adaptive sampling reduces computation times by 2x-5x compared to fixed BoN for the same accuracy. MILP solver scales smoothly to K≈10 LLMs and N≈1000 problems.", "statistical_rigor": "Results estimated from 100 independent runs. Bayes factor calculated with 1,000 Monte Carlo samples. Two-sigma confidence intervals reported.", "limitations_acknowledged": ["Answer parser might be sub-optimal for some models (e.g., MATH500)", "No tool call option specified", "GPT-OSS-20B reasoning mode set to medium (not best)", "GSM8K not tested as it is too easy for tested LLMs"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2509.21593", "arxiv_url": "https://arxiv.org/abs/2509.21593", "title": "GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models", "authors": ["Peng"], "abstract": "", "published_date": "2025-09-25", "affiliations": "Massachusetts Institute of Technology, Stanford University, Technical University of Munich", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 7, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper provides a concrete, successful architecture for integrating RAG with AlphaEvolve-style search to solve the 'domain grounding' problem. It directly addresses our need for better observability and signal in evolutionary search by injecting theoretical constraints via an outer loop."}, "brief": "GeoEvolve augments standard LLM-based evolutionary search (OpenEvolve) with an outer 'researcher' loop that queries a domain-specific RAG (textbooks/papers) to inject theoretical constraints into mutation prompts. On geospatial interpolation tasks, they report 13-21% error reduction over standard evolution, with ablations confirming that retrieved domain knowledge—not just iterative feedback—drives the performance gain. The critical takeaway is the architectural pattern of 'Knowledge-Guided Evolution': instead of relying on the LLM's internal weights for domain theory, they explicitly retrieve and inject theoretical priors (e.g., valid variogram definitions) to steer the search. We should adapt this 'Theory-RAG' outer loop for our AlgoEvo pipeline to force evolved VRP heuristics to respect OR theoretical bounds.", "methodology": {"core_method": "Multi-agent LLM framework combining OpenEvolve-based evolutionary search with GeoKnowRAG for geospatial domain knowledge injection", "llm_role": "code_writer, evaluator, prompt_optimizer, evolutionary_search, decomposition_guide", "llm_model_used": "GPT-4", "search_type": "hybrid", "novelty_claim": "We integrate evolutionary search with domain knowledge by coupling GeoEvolve’s evolutionary code generation with retrieval-augmented geospatial knowledge.", "components": ["code evolver (OpenEvolve)", "evolved code analyzer", "geospatial knowledge retriever (GeoKnowRAG)", "geo-informed prompt generator", "outer agentic controller"], "training_required": false}, "tags": {"methods": ["multi_agent_llm", "evolutionary_search", "retrieval_augmented_generation", "llm_code_generation", "llm_as_evaluator", "llm_prompt_optimization", "llm_evolutionary_search", "program_synthesis", "openevolve"], "problems": ["geospatial_modeling", "spatial_interpolation", "uncertainty_quantification", "algorithm_discovery"], "contribution_type": ["new_method", "framework", "sota_result"], "framework_lineage": "openevolve", "specific_domain": "geospatial_modeling", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Automating Geospatial Model Discovery", "short": "GeoMD", "class_": "geospatial_modeling", "properties": ["spatial_autocorrelation", "spatial_heterogeneity", "scale_effect", "multi_modal_data", "algorithm_design"], "scale": "regional geospatial datasets, up to 21613 points"}, "lineage": {"direct_ancestors": [{"paper": "OpenEvolve", "relationship": "builds upon and extends"}], "closest_prior_work": "OpenEvolve", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["evaluate performance with different foundation models", "incorporate a broader geospatial knowledge database", "explore other geospatial model discovery tasks"], "transferable_to": ["scientific discovery in other domains requiring model discovery", "automated algorithm design for other data types", "complex system modeling beyond geospatial"], "open_weaknesses": ["limited evaluation with diverse LLM models", "scope of geospatial knowledge base", "lack of explicit statistical significance tests"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.94, "experiments": {"benchmarks": ["Spatial Interpolation (Kriging) on Australian trace-element data", "Spatial Uncertainty Quantification (GeoCP) on Seattle housing price data"], "baselines": ["Original Kriging", "OpenEvolve (Kriging)", "OpenEvolve with GeoKnowledge Prompt (Kriging)", "GeoEvolve without GeoKnowRAG (Kriging)", "Original GeoCP", "OpenEvolve (GeoCP)", "OpenEvolve with GeoKnowledge Prompt (GeoCP)", "GeoEvolve without GeoKnowRAG (GeoCP)"], "hardware": "GPT-4 based LLM inference", "instance_sizes": [21613]}, "results": {"vs_baselines": {"Original Kriging": "reduces RMSE by 15.4% (Cu), 21.2% (Pb), 13.0% (Zn)", "OpenEvolve Kriging": "reduces RMSE by 11.3% (Cu), 20.9% (Pb), 13.5% (Zn)", "Original GeoCP": "reduces Interval Score by 16.7%", "OpenEvolve GeoCP": "reduces Interval Score by 11.9%"}, "scalability": "Localized kriging reduces computational complexity from O(n^3) to O(K^3); GeoCP shows improved scalability on large test sets.", "statistical_rigor": "Results reported from 100 evolutionary iterations for GeoEvolve and 10 for OpenEvolve; no explicit statistical significance tests mentioned.", "limitations_acknowledged": ["Performance evaluation with different foundation models", "Incorporation of a broader and more comprehensive geospatial knowledge database"]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2511.11576", "arxiv_url": "https://arxiv.org/abs/2511.11576", "title": "DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs", "authors": ["WenZhuo"], "abstract": "", "published_date": "2025-09-24", "affiliations": "Zhejiang University, University of Toronto, Peking University", "category": "Generative AI for OR", "relevance": {"methodological": 6, "problem": 8, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper directly impacts our 'RobustMAS' and 'OR-Bench' projects by demonstrating that LLMs fail at mathematical derivations for robust optimization but succeed when using specialized DSLs (RSOME). It provides a concrete path to move our benchmarking beyond deterministic toy problems."}, "brief": "Zhu et al. propose DAOpt, a framework for modeling optimization under uncertainty that integrates LLMs with the RSOME library to handle robust and stochastic formulations. Their experiments on a new dataset (OptU) convincingly demonstrate that standard LLM-generated deterministic models suffer from the 'optimizer's curse,' achieving only ~27% out-of-sample feasibility, whereas their robust approach achieves >70%. The critical takeaway for us is to **stop asking LLMs to derive mathematical duals or robust counterparts**; instead, we should train them to use high-level DSLs (like RSOME) that handle the duality internally. This is an immediate action item for our RobustMAS project to ensure generated solutions are actually executable in stochastic environments.", "methodology": {"core_method": "LLM-based multi-agent framework for optimization modeling, integrating few-shot learning with OR domain knowledge (RSOME toolbox) and a Reflexion-based checker", "llm_role": "code_writer", "llm_model_used": "GPT-4o", "search_type": "improvement", "novelty_claim": "DAOpt is a novel LLM-based multi-agent framework for modeling and evaluating data-driven optimization under uncertainty, featuring a new dataset OptU, few-shot learning with OR domain knowledge, and evaluation focused on out-of-sample feasibility and robustness.", "components": ["OptU dataset", "Data-driven Optimization Identifier (LLM agent)", "OR Domain Knowledge Learner (LLM agent)", "Reflexion-based Checker (program)", "RSOME toolbox", "Few-shot learning"], "training_required": true}, "tags": {"methods": ["llm_code_generation", "llm_as_evaluator", "multi_agent_system", "few_shot_learning", "stochastic_optimization", "robust_optimization", "distributionally_robust_optimization", "lagrangian_duality", "sample_average_approximation", "reflexion", "rsome"], "problems": ["optimization_modeling", "data_driven_optimization_under_uncertainty", "stochastic_optimization_modeling", "robust_optimization_modeling", "distributionally_robust_optimization_modeling", "inventory_network_problem", "transportation_problem"], "contribution_type": ["new_method", "new_benchmark", "framework", "empirical_study"], "framework_lineage": null, "specific_domain": "data_driven_optimization_under_uncertainty", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Data-Driven Optimization under Uncertainty", "short": "DDOU", "class_": "optimization_modeling", "properties": ["data_driven", "uncertainty", "stochastic", "robust", "distributionally_robust", "multi_agent"], "scale": "small to medium scale instances (e.g., 5 warehouses, 7 stores; 50-103 samples)"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2402.10172", "relationship": "extends LLM-based optimization modeling from deterministic to uncertain settings"}, {"paper": "arXiv:2405.00000", "relationship": "extends LLM-based optimization modeling from deterministic to uncertain settings"}, {"paper": "arXiv:2500.00000", "relationship": "extends LLM-based optimization modeling from deterministic to uncertain settings"}, {"paper": "arXiv:2406.00000", "relationship": "extends LLM-based optimization modeling from deterministic to uncertain settings"}, {"paper": "arXiv:2402.00000", "relationship": "extends LLM-based optimization modeling from deterministic to uncertain settings"}, {"paper": "Advances in Neural Information Processing Systems, 36", "relationship": "integrates Reflexion for code checking"}, {"paper": "Management Science, 66(8):3329–3339, 2020", "relationship": "integrates RSOME toolbox for robust and stochastic optimization"}], "closest_prior_work": "OptiMUS: Scalable optimization modeling with (mi) lp solvers and large language models", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["design a multi-agent system for further fine-tuning of components", "integrate additional operations research domain knowledge", "explore two-stage optimization problems"], "transferable_to": ["supply_chain_optimization_under_uncertainty", "financial_portfolio_optimization", "resource_allocation_under_uncertainty"], "open_weaknesses": ["current multi-agent system could benefit from further fine-tuning", "limited integration of advanced OR domain knowledge", "does not yet address two-stage optimization problems"]}, "artifacts": {"code_url": "https://anonymous.4open.science/r/LLM-for-data-driven-optimization-problems-9528", "models_released": false, "new_benchmark": true}, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.44, "experiments": {"benchmarks": ["OptU dataset"], "baselines": ["Direct prompting", "Chain-of-Thought (CoT) prompting", "DOpt-LLM w.o. Domain knowledge"], "hardware": "null", "instance_sizes": [5, 7, 50, 103]}, "results": {"vs_baselines": {"Direct prompting": "DAOpt-LLM (GPT-4o) achieves significantly higher out-of-sample feasibility (e.g., DRO 0.80 vs 0.33) and lower over-optimistic rates (e.g., DRO 0.01 vs 0.40).", "CoT prompting": "DAOpt-LLM (GPT-4o) achieves significantly higher out-of-sample feasibility (e.g., DRO 0.80 vs 0.40) and lower over-optimistic rates (e.g., DRO 0.01 vs 0.42).", "DOpt-LLM w.o. Domain knowledge": "DAOpt-LLM with domain knowledge (RO) achieves higher feasibility (0.89 vs 0.56) and lower over-optimistic rates (0.00 vs 0.30)."}, "scalability": "The OptU dataset decouples problem descriptions from underlying data, allowing for scaling with sample size, and experiments use in-sample sizes up to 50 and out-of-sample up to 103.", "statistical_rigor": "Experiments are repeated three times for each LLM, and average performance is reported. For the inventory network problem, samples are generated using 10 different random seeds, and average performance is reported.", "limitations_acknowledged": ["Future work will focus on designing a multi-agent system that promotes further fine-tuning of each component", "integrating additional operations research (OR) domain knowledge to further enhance performance", "exploring two-stage optimization problems"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2503.21476", "arxiv_url": "https://arxiv.org/abs/2503.21476", "title": "Robust DNN Partitioning and Resource Allocation Under Uncertain Inference Time", "authors": ["Zhaojun"], "abstract": "", "published_date": "2025-09-23", "affiliations": "Tsinghua University", "category": "OR for Generative AI", "relevance": {"methodological": 6, "problem": 7, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While the application is Edge AI, the formulation for handling uncertain inference times via Chance-Constrained Programming (CCP) is directly transferable to our 'GPUSched' and 'RobustMAS' projects. It offers a tractable deterministic approximation for stochastic duration that avoids the computational cost of simulation or the inefficiency of worst-case bounds."}, "brief": "Nan et al. propose a robust optimization framework for DNN partitioning that handles uncertain inference times by converting probabilistic deadlines into deterministic constraints using mean/variance information (Chance-Constrained Programming). They decompose the resulting MINLP into a convex resource allocation problem and a partitioning problem solved via the Penalty Convex-Concave Procedure (PCCP). Experiments on real hardware (Jetson/RTX) demonstrate ~50% energy savings over worst-case baselines while maintaining violation probabilities below the risk threshold. For our 'GPUSched' and 'RobustMAS' work, the key takeaway is the specific analytic transformation of the chance constraint and the use of PCCP as a heuristic for the binary subproblem—a potential alternative to heavy evolutionary search for real-time scheduling components.", "methodology": {"core_method": "Problem decomposition into resource allocation and DNN partitioning subproblems, solved iteratively using Chance-Constrained Programming (CCP), convex optimization, and Penalty Convex-Concave Procedure (PCCP)", "llm_role": "none", "llm_model_used": null, "search_type": "improvement", "novelty_claim": "To the best of our knowledge, this is the first work explicitly considering inference time uncertainty in optimizing DNN partitioning.", "components": ["Problem decomposition", "Chance-Constrained Programming (CCP)", "Convex optimization", "Penalty Convex-Concave Procedure (PCCP)", "Nonlinear least squares", "Mean/variance estimation"], "training_required": false}, "tags": {"methods": ["mixed_integer_nonlinear_programming", "problem_decomposition", "chance_constrained_programming", "convex_optimization", "penalty_convex_concave_procedure", "nonlinear_least_squares", "mean_variance_estimation"], "problems": ["dnn_partitioning", "resource_allocation", "edge_intelligence", "probabilistic_deadlines", "uncertainty_quantification"], "contribution_type": ["new_method", "empirical_study", "sota_result", "framework"], "framework_lineage": null, "specific_domain": "dnn_partitioning_resource_allocation", "llm_coupling": null}, "problem": {"formal_name": "Robust DNN Partitioning and Resource Allocation Under Uncertain Inference Time", "short": "DNN-PRA-UIT", "class_": "resource_allocation", "properties": ["uncertain_inference_time", "probabilistic_deadlines", "energy_minimization", "multi_device", "joint_optimization", "mixed_integer", "non_linear", "chance_constrained"], "scale": "6-36 mobile devices, 7-10 partitioning points"}, "lineage": {"direct_ancestors": [{"paper": "arxiv:1907.03009", "relationship": "extends chance-constrained programming (CCP) technique from"}, {"paper": "arxiv:1602.04017", "relationship": "applies penalty convex-concave procedure (PCCP) from"}, {"paper": "arxiv:2201.00000", "relationship": "extends deterministic DNN partitioning and resource allocation from (e.g., Shi et al. 2023)"}], "closest_prior_work": "Multiuser co-inference with batch processing capable edge server", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Extend to high-speed mobile scenarios (e.g., connected vehicles, drones) with dynamic uncertainties.", "Joint optimization considering VM resource competition and associated delays in multi-user edge systems.", "Extend to cross-continuous inference tasks (e.g., video streaming), incorporating time-related information and autocorrelation."], "transferable_to": ["Other resource allocation problems in edge computing with uncertain task execution times.", "Distributed machine learning inference in dynamic environments.", "Real-time systems requiring probabilistic performance guarantees."], "open_weaknesses": ["Does not consider channel state uncertainty.", "Variance approximation method introduces some errors.", "VM resource competition at the edge server is not explicitly modeled.", "Limited to static mobile device scenarios, not high-speed mobility."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.03, "experiments": {"benchmarks": ["AlexNet", "ResNet152", "ViT-B/32", "CIFAR-10"], "baselines": ["Random policy", "Optimal policy (exhaustive search)", "Worst-case optimization", "Mean-value optimization"], "hardware": "Jetson Xavier NX CPU/GPU, Jetson Nano GPU, GeForce RTX 4080 (for measurements); Intel Core i7-8700 CPU, 16GB RAM (for simulations)", "instance_sizes": [6, 7, 8, 9, 10, 12, 18, 24, 30, 36]}, "results": {"vs_baselines": {"Random policy": "Algorithm 1 significantly lower energy consumption", "Optimal policy (exhaustive search)": "Algorithm 1 very close to optimal energy consumption", "Worst-case optimization": "Algorithm 2 saves 44.8-53.1% energy for AlexNet, 9.3% for ResNet152, 5.8% for ViT-B/32, while maintaining violation probability below risk level.", "Mean-value optimization": "Algorithm 2 has higher energy consumption but guarantees probabilistic deadlines (e.g., mean-value has >40% violation for AlexNet, Algorithm 2 keeps it below 0.5% for \n=0.06)."}, "scalability": "The runtime of the proposed algorithm increases linearly with the number of mobile devices, despite the exponentially growing search space, demonstrating polynomial time complexity.", "statistical_rigor": "Mean inference time for each block is obtained through 500 experiments; variance and covariance are calculated. Average iterations and convergence trajectories are shown. Violation probabilities are reported against risk levels.", "limitations_acknowledged": ["channel state uncertainty not considered", "variance approximation introduces errors", "not extended to high-speed mobile scenarios", "VM resource competition not modeled", "not extended to cross-continuous inference tasks"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2507.15877", "arxiv_url": "https://arxiv.org/abs/2507.15877", "title": "Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning", "authors": ["Simon"], "abstract": "", "published_date": "2025-09-21", "affiliations": "", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 6, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper empirically demonstrates that Test-Time Fine-Tuning (TTFT) fails at structural generalization, relying instead on memorization, while execution-guided search succeeds. It explicitly highlights a failure mode of basic AlphaEvolve implementations (0% success on OOD) that we must address by integrating intermediate execution states (process rewards) into our search."}, "brief": "Ouellette implements an Execution-Guided Neural Program Synthesis (EG-NPS) system for ARC-AGI that conditions the search on the intermediate execution state of every instruction, achieving 80% success on out-of-distribution tasks where TTFT (10%) and standard AlphaEvolve (0-14%) fail. The results are rigorous, using controlled OOD tasks to prove that TTFT relies on in-distribution priors rather than reasoning. The critical takeaway for our AlgoEvo work is the architecture of the 'state-conditioned decoder': instead of blind code generation, we should inject the tokenized execution result of step $t$ into the context for step $t+1$. This is effectively a dense process reward model that solves the sample efficiency bottleneck we face in evolutionary search.", "methodology": {"core_method": "Execution-guided multi-step neural program synthesis (EG-NPS) with Encoder-Decoder Transformer and tree search", "llm_role": "none", "llm_model_used": null, "search_type": "constructive", "novelty_claim": "We present an implementation of an execution-guided multi-step neural program synthesis (EG-NPS) algorithm for ARC-AGI, along with a new DSL and program syntax that facilitates the latter.", "components": ["Encoder-Decoder Transformer", "Custom DSL", "Tree search algorithm", "Execution-guided feedback", "Entropy-based exploration"], "training_required": true}, "tags": {"methods": ["neural_program_synthesis", "execution_guided_program_synthesis", "transformer", "tree_search", "domain_specific_language", "test_time_fine_tuning", "alphaevolve", "gridcoder", "llm_code_generation", "llm_as_heuristic"], "problems": ["arc_agi", "compositional_generalization", "out_of_distribution_generalization", "program_synthesis", "visual_reasoning", "grid_manipulation"], "contribution_type": ["new_method", "framework", "empirical_study", "sota_result", "negative_result"], "framework_lineage": "gridcoder", "specific_domain": "arc_agi", "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Out-of-Distribution Generalization in the ARC-AGI Domain", "short": "ARC-AGI", "class_": "program_synthesis", "properties": ["compositional_generalization", "out_of_distribution", "visual_reasoning", "grid_manipulation"], "scale": "up to 30x30 grids"}, "lineage": {"direct_ancestors": [{"paper": "Zohar and Wolf 2018", "relationship": "modifies execution-guided program synthesis from"}, {"paper": "Ouellette 2024", "relationship": "extends non-execution-guided program synthesis from"}], "closest_prior_work": "Zohar and Wolf 2018", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Extend DSL to include object detection and manipulation", "Investigate the 'entropy' exploration mechanism further", "Apply execution-guided neural program synthesis to other complex compositional tasks", "Analyze the interplay between LLM pretraining and fine-tuning for OOD generalization"], "transferable_to": ["Other visual program synthesis domains", "Automated algorithm discovery for grid-based puzzles", "Tasks requiring compositional generalization in structured environments"], "open_weaknesses": ["Current DSL limitations regarding object detection and manipulation", "Scalability of tree search for extremely large program spaces", "Limited impact of the 'entropy' exploration mechanism on aggregate success rates", "The core method does not leverage large pre-trained LLMs directly"]}, "artifacts": {"code_url": "https://github.com/SimonOuellette35/OODGenARC-AGI", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.36, "experiments": {"benchmarks": ["ARC-AGI-like OOD tasks", "ARC-AGI-2 training tasks"], "baselines": ["GridCoder1", "NN-Only", "Omni-ARC (Qwen2-0.5B-Instruct)", "AlphaEvolve (gpt-4.1-mini)"], "hardware": "A40 RunPod instance (48Gb VRAM, 50Gb RAM, 9 vCPUs Intel Xeon Gold 6342 @ 2.80GHz)", "instance_sizes": [3, 30]}, "results": {"vs_baselines": {"GridCoder1": "GridCoder2 median 80% vs 42.86% on OOD tasks", "NN-Only": "GridCoder2 median 80% vs 10% on OOD tasks", "TTFT": "GridCoder2 median 80% vs 10% on OOD tasks", "AlphaEvolve": "GridCoder2 median 80% vs 14.29% on OOD tasks", "ARC-AGI-2 training tasks": "GridCoder2 83.33% vs NN-Only 27.78% of solvable tasks"}, "scalability": "Demonstrates strong compositional generalization to structurally novel tasks and longer programs not seen during training, even finding more efficient solutions.", "statistical_rigor": "Results reported as median, min, and max over 5 distinct runs for 10 task instances per OOD task.", "limitations_acknowledged": ["Current DSL limits solvable ARC-AGI tasks (e.g., no object detection/manipulation)", "TTFT primarily elicits in-distribution knowledge, not novel generalization"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2509.19349", "arxiv_url": "https://arxiv.org/abs/2509.19349", "title": "ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution", "authors": ["Robert"], "abstract": "", "published_date": "2025-09-17", "affiliations": "Sakana AI", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper directly attacks the sample efficiency bottleneck of AlphaEvolve-style search, achieving SOTA results with 150 samples where we typically spend thousands. It provides concrete, low-hanging engineering fixes (embedding rejection, bandit selection) that we should immediately port to AlgoEvo."}, "brief": "ShinkaEvolve presents an open-source evolutionary framework that drastically improves sample efficiency (e.g., beating AlphaEvolve on Circle Packing with only 150 evaluations vs. thousands) by integrating embedding-based novelty rejection, adaptive parent sampling, and bandit-based LLM selection. The results are credible, backed by code from Sakana AI, and directly target our primary pain point of high API costs/sample inefficiency in evolutionary search. **Key Takeaway:** We must implement their 'novelty rejection sampling' immediately—using a cheap embedding model to filter out semantically similar code mutations (threshold 0.95) before execution is a trivial but high-impact optimization for our AlgoEvo pipeline. This paper proves that smart filtering is superior to the brute-force compute strategies we have been relying on.", "methodology": {"core_method": "Evolutionary framework with adaptive parent sampling, code novelty rejection-sampling, and bandit-based LLM ensemble selection", "llm_role": "mutation_operator, evaluator, decomposition_guide", "llm_model_used": "gemini-2.5-pro, claude-sonnet-4, o4-mini, gpt-4.1-nano, gpt-4.1, gpt-4.1-mini, gemini-2.5-flash, gpt-5, gpt-5-mini", "search_type": "improvement", "novelty_claim": "We introduce ShinkaEvolve, an evolutionary framework that substantially improves sample efficiency through three key algorithmic innovations: a novel parent program sampling strategy, code novelty rejection-sampling, and adaptive performance-based LLM ensemble selection.", "components": ["adaptive parent sampling (power law, weighted)", "code novelty rejection-sampling (embedding similarity, LLM-as-a-novelty-judge)", "bandit-based LLM ensemble selection (UCB1)", "island model", "meta-scratchpad system", "diff-based edits", "full rewrites", "crossover mutation"], "training_required": false}, "tags": {"methods": ["evolutionary_algorithms", "llm_as_mutation_operator", "llm_as_evaluator", "llm_code_generation", "llm_evolutionary_search", "program_synthesis", "evolution_of_heuristics", "simulated_annealing", "slsqp", "kd_tree", "multi_armed_bandit", "island_model", "novelty_search", "adaptive_sampling", "code_embedding", "in_context_learning", "meta_learning"], "problems": ["circle_packing", "mathematical_reasoning", "agent_design", "competitive_programming", "heuristic_evolution", "load_balancing", "llm_architecture_design", "constrained_optimization", "combinatorial_optimization"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "alphaevolve", "specific_domain": null, "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Open-Ended And Sample-Efficient Program Evolution", "short": "Program Evolution", "class_": "program_evolution", "properties": ["open_ended", "sample_efficient", "LLM_guided", "programmatic"], "scale": "26 circles, 30 AIME questions, 10 ALE-Bench tasks, MoE 556M-2.7B parameters"}, "lineage": {"direct_ancestors": [{"paper": "AlphaEvolve", "relationship": "improves sample efficiency over"}, {"paper": "OpenEvolve", "relationship": "related evolutionary coding agent"}, {"paper": "LLM4AD", "relationship": "related platform for algorithm design"}], "closest_prior_work": "AlphaEvolve", "novelty_type": "incremental"}, "extensions": {"next_steps": ["automated_task_specification", "true_open_endedness", "self_referential_refinement", "online_meta_learning"], "transferable_to": ["problems_with_ill_defined_numerical_objectives", "other_scientific_discovery_domains", "other_agentic_system_designs", "other_combinatorial_optimization_problems"], "open_weaknesses": ["fixed_configurations_for_exploration_exploitation_balance", "manual_human_expertise_for_objective_functions", "constrained_to_problems_with_well_defined_numerical_objectives", "api_costs_from_large_scale_llm_usage"]}, "artifacts": {"code_url": "https://github.com/SakanaAI/ShinkaEvolve", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.41, "experiments": {"benchmarks": ["Circle Packing", "AIME 2024", "ALE-Bench LITE", "Mixture-of-Expert Load Balancing Loss"], "baselines": ["AlphaEvolve", "OpenEvolve", "LLM4AD", "ALE-Agent", "global-batch LBL", "simple single-query agents", "majority-voting approaches"], "hardware": "Not specified for ShinkaEvolve execution; MoE models trained with bfloat16", "instance_sizes": [26, 30, 10, 556, 2700]}, "results": {"vs_baselines": {"AlphaEvolve": "new SOTA on circle packing with <150 samples", "AIME baselines": "significantly outperforms hand-designed agents", "ALE-Agent": "+2.3% average improvement, 5th to 2nd on ahc039", "global-batch LBL": "consistent edge in perplexity and task performance"}, "scalability": "ShinkaEvolve achieves broad applicability with exceptional sample efficiency, requiring orders of magnitude fewer evaluations than existing approaches.", "statistical_rigor": "Each candidate for AIME was evaluated across three independent runs. MoE results report average task performance, perplexity, and missrouted token fraction.", "limitations_acknowledged": ["fixed configurations with limited automatic control over exploration-exploitation balance", "task specification requires manual human expertise for objective functions and evaluation", "framework is constrained to problems with well-defined numerical objectives", "API costs from large-scale LLM usage could create economic barriers"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2507.10259", "arxiv_url": "https://arxiv.org/abs/2507.10259", "title": "Temporal-Aware GPU Resource Allocation for Distributed LLM Inference via Reinforcement Learning", "authors": ["Chengze"], "abstract": "", "published_date": "2025-09-16", "affiliations": "Shenzhen University of Advanced Technology, China Mobile Research Institute", "category": "OR for Generative AI", "relevance": {"methodological": 6, "problem": 9, "inspirational": 6}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper directly targets the 'GPUSched' project in your profile. It provides a strong critique of 'reactive' OR formulations (single-timeslot optimization) by quantifying the high cost of state transitions (model switching/migration), which static Integer Programming approaches often neglect."}, "brief": "TORTA introduces a hierarchical scheduler for distributed LLM inference that uses a macro-level RL agent (PPO) supervised by an Optimal Transport (OT) baseline to manage inter-region allocation, followed by a micro-level greedy allocator. Results on simulated clusters (up to 50 servers) demonstrate a ~15% reduction in latency compared to reactive baselines (like SkyLB) specifically by optimizing for temporal smoothness and reducing switching costs. The key technical takeaway is the use of an exact OR solver (OT) as a dense supervision signal to train a faster RL policy, effectively combining the optimality of OR with the temporal foresight of RL. We should review our GPUSched formulations to ensure we aren't falling into the 'reactive' trap described here; if we are, this hybrid RL-OT architecture is a viable alternative.", "methodology": {"core_method": "Proximal Policy Optimization (PPO) with Optimal Transport supervision", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "Proposes TORTA, a two-layer spatiotemporal scheduling framework integrating reinforcement learning and optimal transport to address temporal dependencies and decision continuity in distributed GPU resource allocation for LLM inference.", "components": ["Macro-level RL-OT optimizer", "Micro-level greedy task-server matching", "Demand predictor", "Dynamic server activation"], "training_required": true}, "tags": {"methods": ["reinforcement_learning", "proximal_policy_optimization", "optimal_transport", "greedy_algorithm", "demand_forecasting", "neural_network"], "problems": ["llm_inference_scheduling", "gpu_resource_allocation", "distributed_systems", "load_balancing", "multi_objective_optimization", "resource_allocation"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": null, "specific_domain": "llm_serving_optimization", "llm_coupling": null}, "problem": {"formal_name": "Temporal-Aware GPU Resource Allocation for Distributed LLM Inference", "short": "Distributed LLM Inference Scheduling", "class_": "resource_allocation", "properties": ["distributed", "temporal-aware", "multi-objective", "dynamic workloads", "heterogeneous resources"], "scale": "Multi-region, up to 100,000 tasks (real-world), 50 servers (experimental)"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:1707.06347", "relationship": "uses PPO for policy optimization"}], "closest_prior_work": "arXiv:2507.00000 (SkyLB)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["extend framework to multi-tenant scenarios", "investigate adaptive prediction mechanisms for improved forecasting accuracy", "develop lightweight variants for resource-constrained environments"], "transferable_to": ["general cloud resource scheduling", "distributed AI model serving (e.g., diffusion models)", "edge computing resource management"], "open_weaknesses": ["performance gains depend on prediction quality (diminishing returns below 40% accuracy)", "computational overhead for extremely large-scale deployments"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_status": "growing", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.69, "experiments": {"benchmarks": ["Abilene", "Cost2", "Gabriel", "Polska"], "baselines": ["SkyLB", "SDIB (Standard Deviation and Idle-time Balanced)", "RR (Round-Robin)"], "hardware": "Intel i5-13490F CPU (for MILP tests), V100, H100, RTX 4090, RTX 3090, A100, T4 GPUs (for inference/simulation)", "instance_sizes": [200, 500, 1000, 2000, 5000]}, "results": {"vs_baselines": {"SkyLB": "Up to 15% lower response time, 7-16% lower power cost, 32-72% lower operational overhead, 3.6-4.4% higher load balance.", "SDIB": "Up to 17% lower response time, 12-16% lower power cost, 71-79% lower operational overhead, 4.1-6.9% higher load balance.", "RR": "Up to 24% lower response time, 8-17% lower power cost, 73-78% lower operational overhead, 6.8-11.5% higher load balance."}, "scalability": "TORTA's two-layer architecture and RL approach enable it to scale to larger problem sizes than traditional MILP methods, maintaining sub-second decision times for up to 5000 tasks.", "statistical_rigor": "Mean values and confidence intervals are reported from multiple experimental runs, but no formal significance tests are detailed.", "limitations_acknowledged": ["Performance depends on prediction quality (diminishing returns below 40% accuracy)", "Computational overhead for extremely large-scale deployments"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2507.17668", "arxiv_url": "https://arxiv.org/abs/2507.17668", "title": "How Should We Meta-Learn Reinforcement Learning Algorithms?", "authors": ["Alexander"], "abstract": "", "published_date": "2025-09-10", "affiliations": "University of Oxford", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 7, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper establishes concrete failure modes for LLM-based algorithm discovery (specifically regarding high-dimensional feature inputs) compared to Evolution Strategies (ES). It provides a rigorous empirical basis for choosing between LLM and ES components in our AlgoEvo pipeline."}, "brief": "Goldie et al. perform a rigorous empirical benchmark comparing LLM-based algorithm proposal against Black-box Evolution Strategies (ES) and various distillation methods. They find that while LLMs are sample-efficient for simple functions, they catastrophically fail to incorporate high-dimensional input features (e.g., the 20+ inputs in OPEN), where Black-box ES remains superior. The most actionable takeaway is 'Same-Size Distillation': distilling a learned black-box algorithm into a fresh network of identical size using synthetic data consistently improves out-of-distribution generalization with zero additional environment samples. We should implement this distillation step immediately and reconsider using LLMs for feature-heavy heuristic components.", "methodology": {"core_method": "Empirical comparison of meta-learning algorithms for reinforcement learning algorithm discovery", "llm_role": "code_writer", "llm_model_used": "GPT o3-mini", "search_type": "hybrid", "novelty_claim": "First empirical comparison of different meta-learning algorithms for reinforcement learning algorithm discovery.", "components": ["Black-Box Meta-Learning (Evolution Strategies)", "Black-Box Meta-Learning (Meta-gradients)", "Black-Box Distillation (Same-Size)", "Black-Box Distillation (Smaller)", "Symbolic Distillation", "LLM Proposal"], "training_required": true}, "tags": {"methods": ["meta_learning", "evolution_strategies", "meta_gradients", "black_box_distillation", "symbolic_distillation", "llm_proposal", "reinforcement_learning", "ppo", "actor_critic", "program_synthesis", "llm_code_generation", "llm_in_the_loop"], "problems": ["reinforcement_learning_algorithm_design", "algorithm_discovery", "learned_optimizers", "learned_policy_optimization", "learned_policy_gradient"], "contribution_type": ["empirical_study", "framework"], "framework_lineage": null, "specific_domain": "reinforcement_learning_algorithm_design", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Meta-Learning Reinforcement Learning Algorithms", "short": "Meta-RL Algorithm Design", "class_": "algorithm_design", "properties": ["empirical_comparison", "generalization", "sample_efficiency", "interpretability", "computational_cost"], "scale": "Diverse RL environments (MinAtar, Brax, Cartpole, Craftax) with varying algorithm complexities (7-20 inputs, 1k-200k parameters)"}, "lineage": {"direct_ancestors": [{"paper": "Lu et al. (2022)", "relationship": "meta-learned algorithm (LPO) studied in comparison"}, {"paper": "Oh et al. (2020)", "relationship": "meta-learned algorithm (LPG) studied in comparison"}, {"paper": "Goldie et al. (2024)", "relationship": "meta-learned algorithm (OPEN) studied in comparison"}, {"paper": "Lu et al. (2024)", "relationship": "LLM proposal framework (DiscoPOP) evaluated"}, {"paper": "Zheng et al. (2022)", "relationship": "symbolic distillation method evaluated"}], "closest_prior_work": "Oh et al. (2020)", "novelty_type": "new_problem"}, "extensions": {"next_steps": ["Explore hyperparameter sensitivity of LLM-proposed algorithms", "Investigate different architectures for black-box distillation (e.g., transformers)", "Blend different meta-learning algorithms (e.g., symbolic distillation with black-box encoding, LLM warm-started from symbolic)", "Study different prompting styles or agentic frameworks for LLMs"], "transferable_to": ["Meta-learning algorithms for other machine learning domains (e.g., supervised learning, unsupervised learning)", "Automated algorithm design for other optimization problems (e.g., combinatorial optimization, continuous optimization)", "Designing components beyond optimizers/drift functions (e.g., new loss functions, exploration strategies)"], "open_weaknesses": ["LLM-proposed algorithms' reliance on hyperparameter selection", "Symbolic distillation's poor scalability to high-dimensional inputs", "LLM proposal's need for a good warm-start and struggle with many features", "Black-box learning is practically the only scalable option for many features/long rollouts"]}, "artifacts": {"code_url": "https://github.com/AlexGoldie/learn-rl-algorithms", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.46, "experiments": {"benchmarks": ["MinAtar", "Brax (Ant, Humanoid, Hopper, Walker)", "OpenAI Gym (Cartpole)", "Craftax-Classic", "Gridworlds"], "baselines": ["Black-Box Learning", "LLM Proposal", "Same-Size Distillation", "Smaller Distillation", "Symbolic Distillation"], "hardware": "GPU (NVIDIA equipment grant)", "instance_sizes": [7, 8, 19, 20, 100]}, "results": {"vs_baselines": {"LLM Proposal": "Superior OOD generalization for simple algorithms, but worse ID. Fails for complex inputs.", "Same-Size Distillation": "Minor OOD generalization gains, no ID harm. Improved LPG.", "Smaller Distillation": "Similar to Same-Size, but can degrade performance. Improved LPG.", "Symbolic Distillation": "Poor scalability with many inputs, generally no performance improvement."}, "scalability": "Black-box learning is most scalable for algorithms with many features and long rollouts; symbolic distillation scales poorly with input size; LLM proposal has medium scalability, requiring warm-starting and struggling with many features.", "statistical_rigor": "IQM of return with 95% stratified bootstrap confidence intervals for 16 environment seeds; meta-learning algorithms trained with a single seed without meta-hyperparameter tuning.", "limitations_acknowledged": ["LLM-proposed algorithms' reliance on hyperparameter selection", "Unexplored architectures in black-box distillation (e.g., transformers)", "Symbolic distillation's inability to scale to high-dimensional problems", "LLM proposal needs a good warm-start", "LLM proposal requires hyperparameter tuning in meta-test environment", "In-distribution performance of LLM proposal is likely worse than black-box", "Smaller distillation can cause bigger drops in performance", "Black-box learning is practically the only way for many features"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2509.08970", "arxiv_url": "https://arxiv.org/abs/2509.08970", "title": "Gala: Global LLM Agents for Text-to-Model Translation", "authors": ["Junyang"], "abstract": "", "published_date": "2025-09-10", "affiliations": "University of Southern California, Brown University, Fidelity Investments", "category": "Generative AI for OR", "relevance": {"methodological": 5, "problem": 8, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "Directly relevant to our OR-Bench and symbolic modeling work. The strategy of decomposing agents by mathematical primitives (global constraints) rather than functional roles is a concrete architectural choice we should evaluate for our own modeling pipelines."}, "brief": "GALA decomposes text-to-MiniZinc translation into a multi-agent system where specialized agents detect specific Constraint Programming global constraints (e.g., all_different, cumulative) before an assembler unifies them. Results on 110 TEXT2ZINC instances show a modest improvement over CoT (57% vs 52% execution rate with o3-mini), though the sample size is small and lacks statistical rigor. The key takeaway is the architectural shift from generic 'coder/reviewer' roles to 'primitive-specific' agents, which aligns LLM reasoning with the target formalism's structure. We should test this 'primitive-based decomposition' in our OR-Bench pipeline to see if it reduces hallucination of complex constraints better than our current methods.", "methodology": {"core_method": "Multi-agent LLM framework for global constraint detection and assembly", "llm_role": "code_writer", "llm_model_used": "OpenAI o3-mini, gpt-4o-mini, gpt-oss:20b, Phi4", "search_type": "constructive", "novelty_claim": "We introduce GALA, a framework that addresses this challenge with a global agentic approach: multiple specialized large language model (LLM) agents decompose the modeling task by global constraint type.", "components": ["Specialized LLM agents (per global constraint type)", "Assembler LLM"], "training_required": false}, "tags": {"methods": ["llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "multi_agent_llm_system", "constraint_programming", "global_constraints", "program_synthesis"], "problems": ["text_to_model_translation", "constraint_programming_modeling", "natural_language_for_optimization"], "contribution_type": ["new_method", "framework", "empirical_study"], "framework_lineage": null, "specific_domain": "text_to_minizinc_translation", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Text-to-Model Translation of Satisfaction and Optimization Problems", "short": "Text-to-MINIZINC Translation", "class_": "program_synthesis", "properties": ["constraint_programming", "natural_language_processing", "multi_agent"], "scale": "TEXT2ZINC instances"}, "lineage": {"direct_ancestors": [{"paper": "Chain-of-Experts by Xiao et al. [2023]", "relationship": "refines multi-agent LLM decomposition from"}, {"paper": "OptiMUS by AhmadiTeshnizi et al. [2023]", "relationship": "refines multi-agent LLM decomposition from"}], "closest_prior_work": "Chain-of-Experts by Xiao et al. [2023]", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["optimize global agents through systematic prompt optimization, few-shot exemplars, or fine-tuning", "implement compile-time snippet validation for agents", "enhance assembler with supervisor for variable/objective extraction or post-hoc linker for unification", "develop systematic error taxonomy and feedback loops for targeted fixes and regenerations"], "transferable_to": ["other constraint modeling languages (e.g., CPMpy, OPL)", "general program synthesis tasks requiring structured decomposition", "translation of natural language to other formal problem representations"], "open_weaknesses": ["difficulty for individual agents in distinguishing counting patterns from other numerical constraints", "assembler agent has the most complex task, requiring robust integration logic", "current TEXT2ZINC dataset limits full showcase of global constraint approach (70% lack global constraints)", "initial evaluation lacked model tuning, prompt optimization, or hyperparameter search"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_7", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.99, "experiments": {"benchmarks": ["TEXT2ZINC"], "baselines": ["One-shot prompting", "Chain-of-Thought (CoT) prompting"], "hardware": "null", "instance_sizes": [110, 567]}, "results": {"vs_baselines": {"o3-mini CoT": "GALA outperforms CoT on execution, solve, and average score.", "gpt-4o-mini CoT": "GALA outperforms CoT on execution, solve, and average score.", "gpt-oss:20b CoT": "GALA slightly outperforms CoT on execution rate, but slightly underperforms on solve rate and average score.", "gpt-oss:20b baseline": "GALA significantly outperforms baseline on all metrics."}, "scalability": "The modular, agentic decomposition aims to reduce cognitive load and complexity, suggesting good scalability, though not explicitly tested for large problem sizes.", "statistical_rigor": "Initial evaluation on 110 instances; no statistical significance tests or variance reported.", "limitations_acknowledged": ["Difficulty in distinguishing counting patterns for individual agents", "Room for improvement in base models and prompt optimization", "Assembler agent has the most complex task", "Current TEXT2ZINC dataset limits full showcase of global constraint approach (70% lack global constraints)", "No model tuning, prompt optimization, or hyperparameter search performed yet"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2509.07367", "arxiv_url": "https://arxiv.org/abs/2509.07367", "title": "Autonomous Code Evolution Meets NP-Completeness", "authors": ["Cunxi"], "abstract": "", "published_date": "2025-09-09", "affiliations": "NVIDIA Research, University of Maryland", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 9, "inspirational": 10}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper demonstrates that LLM evolutionary search can scale from isolated kernels (AlphaEvolve) to full repository maintenance and innovation, beating human SOTA on the canonical NP-complete problem. It introduces a 'self-evolving rule system' that solves the stability/memory bottleneck we face in AlgoEvo, making it a blueprint for our next architecture update."}, "brief": "SATLUTION extends LLM evolutionary search to full-scale C++ repositories, autonomously evolving SAT solvers that outperform 2025 human competition winners using only 2024 training data. The results are highly rigorous, backed by 90k CPU hours of distributed evaluation and strict correctness proofs (DRAT), showing a clear monotonic improvement trajectory. The single most stealable insight is the **self-evolving rule system**: the agent autonomously updates a persistent set of markdown constraints (e.g., forbidden patterns, testing protocols) based on post-cycle failure analysis, effectively creating 'institutional memory' that prevents regression in long-horizon search. We must implement this meta-learning loop in AlgoEvo immediately to move beyond single-file optimization.", "methodology": {"core_method": "Autonomous agent-based code evolution system with Planning and Coding LLM agents", "llm_role": "evolutionary_search", "llm_model_used": "Claude models", "search_type": "improvement", "novelty_claim": "SATLUTION is the first framework to extend LLM-based code evolution to the full repository scale, encompassing hundreds of files and tens of thousands of lines of C/C++ code, and the first to achieve champion-level performance in NP-complete problem solving.", "components": ["Planning Agent", "Coding Agent", "Rule System (static and self-evolving)", "Distributed Runtime Evaluator", "Two-stage Verification Pipeline"], "training_required": true}, "tags": {"methods": ["llm_evolutionary_search", "llm_code_generation", "llm_in_the_loop", "program_synthesis", "self_improving_search", "evolution_of_heuristics", "conflict_driven_clause_learning", "multi_armed_bandit", "multi_uip_clause_learning", "bandit_tuned_uip_depth", "vivification_sensitivity", "bandit_tuned_vivification", "reward_design", "adaptive_sliding_window", "multi_domain_bandit_control", "symmetry_breaking_preprocessing", "compressed_watch_architecture"], "problems": ["boolean_satisfiability", "np_complete_problem", "combinatorial_optimization", "heuristic_evolution", "algorithm_discovery"], "contribution_type": ["new_method", "sota_result", "framework"], "framework_lineage": "alphaevolve", "specific_domain": "boolean_satisfiability", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Boolean Satisfiability Problem", "short": "SAT", "class_": "satisfiability", "properties": ["NP-complete", "combinatorial_optimization", "repository_scale"], "scale": "hundreds of files, tens of thousands of lines of C/C++ code, 400 instances"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2506.13131", "relationship": "extends AlphaEvolve from isolated kernels to full repository scale"}], "closest_prior_work": "AlphaEvolve: A coding agent for scientific and algorithmic discovery", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Develop agents to autonomously construct, adapt, and optimize their own verifiers.", "Extend the framework to new problem domains where correctness guarantees are critical.", "Automate Electronic Design Automation (EDA) tasks through agent-driven verifier evolution."], "transferable_to": ["electronic_design_automation", "large_scale_circuit_design_optimization", "other_np_complete_problems"], "open_weaknesses": ["Agents struggled in fully automated operation and required human intervention for correctness checks and memory errors.", "Agents lacked sufficient domain-specific knowledge at the idea level for nuanced SAT solving aspects.", "Controlled ablation studies of learned components are challenging due to their highly entangled nature.", "Human expertise is currently indispensable in designing robust verification pipelines."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.71, "experiments": {"benchmarks": ["SAT Competition 2024 benchmark", "SAT Competition 2025 benchmark"], "baselines": ["AE kissat2025 MAB (Winner SC25)", "kissat-public (2nd place SC25)", "kissat-sc2024 (Winner SC24)", "kissat-mab-dc", "kissat-mab-binary", "BreakID-Kissat", "AMSAT", "cadical public", "CaDiCaL", "Kissat MAB Binary", "Kissat MAB ESA"], "hardware": "LSF-managed cluster of 800 AMD EPYC 7F72 nodes operating at 3.6 GHz, 90,000 CPU hours", "instance_sizes": [400]}, "results": {"vs_baselines": {"AE kissat2025 MAB (Winner SC25)": "SATLUTION attained the lowest PAR-2, outperforming the top-2 2025 winning solvers on SAT 2025 benchmark. Solved 347, 345, and 344 instances vs 334 for AE kissat2025 MAB on SAT 2025 benchmark. Substantially outperformed all original 2024 solvers and even surpassed the 2025 champion when run on the 2024 instances.", "kissat-public (2nd place SC25)": "SATLUTION attained the lowest PAR-2, outperforming the top-2 2025 winning solvers on SAT 2025 benchmark. Solved 347, 345, and 344 instances vs 331 for kissat-public on SAT 2025 benchmark.", "kissat-sc2024 (Winner SC24)": "SATLUTION achieves the lowest overall PAR-2 score, substantially outperforming all original 2024 solvers and even surpassing the 2025 champion when run on the 2024 instances."}, "scalability": "The evolved solvers exhibit stronger scaling and continue to close additional instances while the competition winners plateau in the medium-to-hard region (1000–4000 seconds), steadily accumulating small performance gains into a significant overall advantage.", "statistical_rigor": "Evaluated on the full SAT Competition 2024 benchmark suite (400 instances) using a cluster of 800 CPU nodes in parallel over 70 iterative evolution cycles. Correctness checks include SAT assignment verification and UNSAT DRAT proof validation. Monotonic trend of the SATLUTION accumulated mean performance was observed.", "limitations_acknowledged": ["Agents struggled in fully automated operation (YOLO mode) and were prone to SAT/UNSAT correctness check failures and deep memory errors (segmentation faults), requiring human intervention.", "Agents lacked sufficient domain-specific knowledge at the idea level for nuanced SAT solving aspects.", "Human guidance was critical for higher-level strategies, with agents handling lower-level implementation.", "Controlled ablation studies of learned components are challenging due to their highly entangled nature in a complex SAT solving system."]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2508.19373", "arxiv_url": "https://arxiv.org/abs/2508.19373", "title": "HAP: Hybrid Adaptive Parallelism for Efficient Mixture-of-Experts Inference", "authors": ["Haoran"], "abstract": "", "published_date": "2025-08-26", "affiliations": "Huawei Noah’s Ark Lab, Shandong University", "category": "OR for Generative AI", "relevance": {"methodological": 7, "problem": 8, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper is a direct validation of our 'OR formulations for AI systems' thesis. It successfully replaces static heuristics with an Integer Linear Programming (ILP) formulation for LLM inference configuration, achieving significant speedups. It provides a concrete template for our 'GPUSched' project regarding how to model system latency and transition costs in an optimization solver."}, "brief": "HAP replaces static parallelization heuristics in MoE inference with an Integer Linear Programming (ILP) solver that dynamically selects optimal strategies (TP, EP, DP) for Attention and Expert modules. They achieve verified ~1.6x speedups on A100/A6000 GPUs by modeling the inference process as a two-stage problem (prefill vs. decoding) with explicit transition costs, allowing the system to switch parallelism strategies mid-inference. For our work on OR-based resource allocation (GPUSched), the key takeaway is their formulation of **transition overheads** within the ILP constraints—a technique we should steal to model dynamic reconfiguration in our scheduling solvers. This confirms that symbolic OR methods can outperform standard systems heuristics in the LLM serving stack.", "methodology": {"core_method": "Integer Linear Programming (ILP) for dynamic selection of hybrid parallel strategies (DP, TP, EP) for Attention and Expert modules, guided by module-specific inference latency simulation models", "llm_role": "none", "llm_model_used": null, "search_type": "exact", "novelty_claim": "HAP dynamically selects hybrid parallel strategies to enhance MoE inference efficiency by hierarchically decomposing MoE architectures into Attention and Expert modules, each with specialized latency simulation, and leveraging Integer Linear Programming to find optimal configurations.", "components": ["Module Decomposition (Attention and Expert modules)", "Inference Latency Simulation Models (computational FLOPs-based, communication bandwidth-based)", "Random Forest Regression (for η and ρ in simulation models)", "Integer Linear Programming (ILP) solver", "Dynamic Parallelism Transition Strategy (weight redistribution or quantized backup/dequantization)"], "training_required": true}, "tags": {"methods": ["integer_linear_programming", "random_forest_regression", "data_parallelism", "tensor_parallelism", "expert_parallelism", "dynamic_parallelism", "latency_prediction", "model_quantization"], "problems": ["llm_serving_optimization", "resource_allocation", "moe_inference_optimization"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "alpa", "specific_domain": "moe_llm_inference_optimization", "llm_coupling": null}, "problem": {"formal_name": "Efficient Mixture-of-Experts Inference Parallelization", "short": "MoE Inference Parallelization", "class_": "system_optimization", "properties": ["dynamic_parallelism_selection", "hybrid_parallelism", "latency_minimization", "memory_constrained", "heterogeneous_hardware_adaptive"], "scale": "Mixtral-8x7B, Qwen1.5-MoE-A2.7B, Qwen2-57B-A14B models; 4-8 GPUs; 256-4096 token context; 64-2048 token generation; batch sizes 8-64"}, "lineage": {"direct_ancestors": [{"paper": "DeepSpeed-MoE", "relationship": "improves upon static hybrid parallel strategies for MoE inference"}, {"paper": "DeepEP", "relationship": "improves upon static expert parallelism for MoE inference"}, {"paper": "Alpa", "relationship": "extends ILP-based parallelism configuration to MoE inference, addressing its limitations for inference tasks"}, {"paper": "Tutel", "relationship": "extends switchable parallelism to MoE inference, achieving zero-cost switching and simultaneous optimization for Attention and Expert modules"}], "closest_prior_work": "Alpa", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["multi_node_inference_optimization", "real_time_inference_serving_optimization", "more_sophisticated_search_mechanisms_for_multi_node"], "transferable_to": ["other_large_model_architectures_parallelization", "distributed_deep_learning_training_optimization", "heterogeneous_computing_environments"], "open_weaknesses": ["performance_degradation_from_quantization_in_some_cases", "overhead_of_dynamic_transition_strategy", "limited_search_space_for_expert_module_parallelism"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.74, "experiments": {"benchmarks": ["Mixtral-8x7B", "Qwen1.5-MoE-A2.7B", "Qwen2-57B-A14B"], "baselines": ["TP-based inference (Tensor Parallelism)", "EP-based inference (Expert Parallelism)"], "hardware": "NVIDIA A100 GPUs, NVIDIA A6000 GPUs, NVIDIA V100 GPUs (single node with 4 A6000, 4 A100, 8 A100, or 8 V100 GPUs)", "instance_sizes": [8, 16, 32, 64]}, "results": {"vs_baselines": {"TP-based inference": "up to 1.77x speedup (A6000), 1.68x (A100), 1.57x (V100) across various MoE models and inference scenarios", "EP-based inference": "comparable prefill performance to EP with minimal transition overhead, while maintaining decoding efficiency equivalent to TP"}, "scalability": "HAP showcases remarkable generalization capability, maintaining performance effectiveness across diverse MoE model configurations and GPU platforms (A100, A6000, V100).", "statistical_rigor": "Communication simulation models exhibit an error margin within 5%, while computational simulation models maintain an error below 10%, based on empirically measured latency profiles.", "limitations_acknowledged": ["Future work: multi-node inference", "Future work: dynamic, real-time inference serving scenarios"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2508.14544", "arxiv_url": "https://arxiv.org/abs/2508.14544", "title": "Adaptively Robust LLM Inference Optimization under Prediction Uncertainty", "authors": ["Zixi"], "abstract": "", "published_date": "2025-08-20", "affiliations": "Stanford University, Peking University, HKUST", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 9, "inspirational": 6}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper directly addresses the core uncertainty challenge in your active 'GPUSched' and 'LLM serving optimization' projects. It provides a provably optimal online policy that renders conservative robust formulations obsolete for this specific problem."}, "brief": "Chen et al. propose $A_{min}$, an online scheduling algorithm for LLM inference that handles unknown output lengths by optimistically assuming the lower bound and evicting jobs (based on accumulated length) if memory overflows. They prove a logarithmic competitive ratio and show via simulations on LMSYS-Chat-1M that this approach nearly matches hindsight-optimal scheduling, vastly outperforming conservative upper-bound baselines. **Key Takeaway:** For our **GPUSched** project, we should abandon conservative memory reservation for output tokens; instead, implement an optimistic scheduler that oversubscribes memory and handles overflows via their ordered eviction policy, as the cost of restart is theoretically bounded and empirically negligible compared to the throughput gains.", "methodology": {"core_method": "Adaptive online scheduling with dynamic lower bound estimation, ordered eviction, and greedy batch formation (Amin algorithm)", "llm_role": "none", "llm_model_used": null, "search_type": "constructive", "novelty_claim": "We introduce Amin, an adaptive algorithm that initially treats the predicted lower bound as the output length and dynamically refines this estimate during inferencing, achieving a log-scale competitive ratio.", "components": ["adaptive lower bound estimation", "ordered eviction", "greedy batch formation", "competitive analysis"], "training_required": false}, "tags": {"methods": ["online_scheduling", "robust_optimization", "competitive_analysis", "greedy_algorithm", "adaptive_algorithms", "ordered_eviction", "lower_bound_estimation"], "problems": ["llm_inference_scheduling", "resource_constrained_scheduling", "online_scheduling", "latency_minimization"], "contribution_type": ["new_method", "theoretical_result", "empirical_study"], "framework_lineage": null, "specific_domain": "llm_inference_scheduling", "llm_coupling": null}, "problem": {"formal_name": "Optimizing Large Language Model (LLM) inference scheduling to minimize total latency under prediction uncertainty", "short": "LLM Inference Scheduling", "class_": "scheduling", "properties": ["online", "multi_task", "prediction_uncertainty", "memory_constraints", "cancellations_allowed", "robust"], "scale": "n requests (up to 2000 in experiments)"}, "lineage": {"direct_ancestors": [{"paper": "Jaillet et al. (2025)", "relationship": "extends the LLM inference scheduling model from"}], "closest_prior_work": "Jaillet et al. (2025)", "novelty_type": "incremental"}, "extensions": {"next_steps": ["develop new algorithms leveraging specific output distributions (e.g., geometric, two-point)", "extend to multi-metric scheduling constraints", "design algorithms for heterogeneous input sizes", "characterize stability conditions for LLM inference systems"], "transferable_to": ["online resource allocation with predicted demand", "online caching with access predictions", "secretary problems with machine learned advice", "general resource-constrained online scheduling with prediction uncertainty"], "open_weaknesses": ["Amax's poor robustness under wide prediction intervals", "difficulty in providing theoretical competitive ratio for Amin under geometric distributions", "complexity of memory-preserving combinatorial proof techniques for scheduling problems", "Amin's competitive ratio is logarithmic, not constant"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.51, "experiments": {"benchmarks": ["H-SF (Hindsight-Shortest First)"], "baselines": ["Amax (Max-Length Based Algorithm)"], "hardware": "LLaMA2-70B model on two linked A100 GPUs (for batch processing time estimation)", "instance_sizes": [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}, "results": {"vs_baselines": {"Amax": "significantly higher latency than H-SF, especially under high prediction uncertainty (e.g., +80% in Example 1, large gaps in Figures 5, 7); performance improves with more accurate predictions (Figure 6).", "Amin": "consistently matches or closely approaches H-SF performance across all prediction accuracy levels (e.g., nearly identical in Figure 5, close in Figures 6, 7)."}, "scalability": "Amin achieves asymptotic optimality with a competitive ratio of O(log(α−1)) as memory capacity M approaches infinity.", "statistical_rigor": "Average end-to-end latency reported over random subsets of conversations; no explicit confidence intervals or significance tests mentioned.", "limitations_acknowledged": ["Amax is overly conservative and non-robust to wide prediction intervals", "Under geometric distribution, it is hard to provide a theoretical result for the competitive ratio of Amin"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2508.03082", "arxiv_url": "https://arxiv.org/abs/2508.03082", "title": "EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic Design", "authors": ["Fei"], "abstract": "", "published_date": "2025-08-20", "affiliations": "Huawei Noah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArk Lab, City University of Hong Kong", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper fundamentally shifts the goal of LLM-based evolution from 'finding the single best algorithm' to 'finding the optimal portfolio,' which directly maps to your ALNS and multi-agent optimization work. The proposed selection mechanism is a concrete, theoretically grounded improvement for population diversity that we can immediately implement."}, "brief": "EoH-S reformulates Automated Heuristic Design (AHD) to evolve a complementary *set* of heuristics rather than a single robust one, proving the objective is submodular and solvable via a greedy strategy. Results are strong and credible: on TSPLib and CVRPLib, their set of 10 heuristics reduces the optimality gap by ~40-60% compared to the top 10 heuristics from FunSearch or ReEvo. **KEY TAKEAWAY:** We should replace standard elitist selection in AlgoEvo with their 'Complementary Population Management' (CPM). By greedily selecting individuals based on marginal contribution to instance coverage (using instance-wise performance vectors), we can automatically generate diverse operator pools for ALNS instead of relying on hand-crafted diversity metrics.", "methodology": {"core_method": "Evolutionary search framework with complementary population management and diversity-aware memetic search", "llm_role": "heuristic_generator", "llm_model_used": "DEEPSEEK-V3", "search_type": "improvement", "novelty_claim": "We introduce Automated Heuristic Set Design (AHSD), a new formulation for LLM-driven AHD, and propose Evolution of Heuristic Set (EoH-S) to automatically generate a small-sized complementary heuristic set to serve diverse problem instances.", "components": ["Complementary Population Management (CPM)", "Complementary-aware Search (CS)", "Local Search (LS)", "LLM-based heuristic generation"], "training_required": true}, "tags": {"methods": ["evolutionary_algorithm", "llm_as_heuristic", "llm_code_generation", "memetic_algorithm", "population_management", "evolution_of_heuristics", "funsearch", "reevo"], "problems": ["bin_packing", "tsp", "cvrp", "heuristic_evolution", "combinatorial_optimization"], "contribution_type": ["new_method", "framework", "sota_result"], "framework_lineage": "eoh", "specific_domain": "combinatorial_optimization", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Automated Heuristic Set Design", "short": "AHSD", "class_": "algorithm_design", "properties": ["complementary", "small-sized set", "diverse instances", "NP-hard objective", "monotone objective", "supermodular objective"], "scale": "10-10000 items/nodes"}, "lineage": {"direct_ancestors": [{"paper": "EoH", "relationship": "extends the Evolution of Heuristics framework"}, {"paper": "FunSearch", "relationship": "adapts LLM-based program synthesis concepts from"}], "closest_prior_work": "EoH", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["explore heuristic collaboration strategies", "improve performance on low-diversity instances"], "transferable_to": ["other combinatorial optimization problems", "automated algorithm design for different problem classes", "multi_objective_optimization"], "open_weaknesses": ["less effective when instance diversity is low", "heuristic collaboration strategies not explored"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_status": "stable", "bridge_score": 0.6253, "is_bridge": true, "priority_score": 8.3, "experiments": {"benchmarks": ["BPPLib", "TSPLib", "CVRPLib"], "baselines": ["First Fit", "Best Fit", "Random", "1+1 EPS", "FunSearch", "EoH", "MEoH", "CALM", "ReEvo", "MCTS-AHD"], "hardware": "one Intel i7-9700 CPU", "instance_sizes": [10, 15, 16, 19, 20, 21, 22, 23, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 46, 48, 50, 51, 52, 53, 54, 55, 56, 57, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 76, 77, 78, 79, 80, 90, 99, 100, 101, 105, 106, 107, 110, 115, 120, 121, 125, 127, 129, 130, 134, 135, 136, 139, 140, 143, 144, 148, 150, 151, 152, 153, 157, 159, 160, 162, 167, 170, 172, 176, 180, 181, 186, 190, 195, 198, 199, 200, 201, 204, 209, 214, 219, 223, 225, 228, 233, 237, 242, 247, 251, 256, 261, 262, 264, 266, 270, 275, 280, 284, 289, 294, 298, 300, 400, 417, 439, 493, 500, 574, 575, 654, 724, 783, 1000, 1002, 5000, 10000]}, "results": {"vs_baselines": {"FunSearch Top10": "EoH-S 0.0515 vs FunSearch Top10 0.0608 on OBP training n200-500", "EoH Top10": "EoH-S 0.040 vs EoH Top10 0.079 on TSP test 50", "ReEvo Top10": "EoH-S 0.040 vs ReEvo Top10 0.092 on TSP test 50", "ReEvo Top10 (CVRPLib B)": "EoH-S 0.183 vs ReEvo Top10 0.250 on CVRPLib Set B", "ReEvo Top10 (TSPLib Avg)": "EoH-S 0.093 vs ReEvo Top10 0.165 on TSPLib average"}, "scalability": "EoH-S consistently outperforms baselines across diverse problem sizes and distributions, showing strong generalization capabilities.", "statistical_rigor": "Average performance reported over three independent runs; relative gap to lower bound or LKH baseline is used.", "limitations_acknowledged": ["Less effective when instance diversity is low", "Heuristic collaboration strategies not explored"]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2508.13380", "arxiv_url": "https://arxiv.org/abs/2508.13380", "title": "Batching-Aware Joint Model Onloading and Offloading for Hierarchical Multi-Task Inference", "authors": ["Seohyeon"], "abstract": "", "published_date": "2025-08-18", "affiliations": "The University of Texas at Austin, DEVCOM Army Research Laboratory", "category": "OR for Generative AI", "relevance": {"methodological": 5, "problem": 8, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While not an evolutionary method, this paper provides a rigorous OR formulation for 'batching-aware' inference scheduling that directly overlaps with our 'GPUSched' project. The linearization technique for batching constraints is immediately useful."}, "brief": "Cha et al. propose an alternating optimization framework (J3O) for joint model placement and query routing in hierarchical inference systems, decomposing the MINLP into greedy Lagrangian submodular maximization and linear programming. They explicitly model batching latency at the edge using a linear surrogate to handle the non-convex batch setup costs, achieving ~97% of Gurobi's optimal accuracy with <15% of the runtime. **Takeaway:** We should steal their linear surrogate formulation for batching overhead (approximating the L0-norm of task arrival) for our 'GPUSched' integer programs; it offers a tractable way to model batching efficiency in serving systems without full non-linear solvers.", "methodology": {"core_method": "Alternating optimization combining greedy submodular maximization with Lagrangian relaxation for onloading and constrained linear programming for offloading", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "We formulate a new joint model onloading and offloading problem for hierarchical multi-task inference, aiming to maximize system-wide accuracy under memory, compute, and communication constraints.", "components": ["Alternating Optimization", "Greedy Submodular Maximization", "Lagrangian Relaxation", "Linear Programming", "Subgradient Method", "Linear Surrogate for Batching Constraints"], "training_required": false}, "tags": {"methods": ["mixed_integer_nonlinear_programming", "alternating_optimization", "greedy_algorithm", "submodular_maximization", "lagrangian_relaxation", "linear_programming", "subgradient_method", "linear_approximation", "batching_optimization"], "problems": ["ml_inference_systems", "resource_allocation", "computation_offloading", "model_placement", "multi_task_learning", "edge_computing", "latency_optimization"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": null, "specific_domain": "ml_inference_systems", "llm_coupling": null}, "problem": {"formal_name": "Joint Model Onloading and Hierarchical Offloading for Multi-Task Inference", "short": "J3O/BAJ3O", "class_": "resource_allocation", "properties": ["hierarchical", "multi_task", "resource_constrained", "batching_aware"], "scale": "30 clients, 3 edge servers, 3-6 tasks, 7-31 models"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2508.13380v1", "relationship": "extends multi-task model deployment to hierarchical offloading and batching"}, {"paper": "2024.03.00023", "relationship": "extends joint model placement and task offloading from single-task to multi-task, multi-tier, and batching-aware"}], "closest_prior_work": "2024.03.00023", "novelty_type": "new_problem"}, "extensions": {"next_steps": ["Dynamic adaptation to evolving task demands", "Handling noisy load estimates", "Online learning for resource availability"], "transferable_to": ["Other hierarchical distributed computing systems", "Federated learning resource management", "Edge AI deployments with diverse ML workloads"], "open_weaknesses": ["Assumes slowly evolving task demands", "Assumes non-noisy load estimates", "Linearization of batching constraints is an approximation", "Potential for further latency reduction beyond 2Tb guarantee"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.13, "experiments": {"benchmarks": ["Taskonomy-5", "DomainNet-6", "Cityscape3D-3"], "baselines": ["MINLP Solver", "Greedy-AO", "OPT-AO", "Rand-AO", "Full Local"], "hardware": "NVIDIA Jetson Nano, NVIDIA TX2, A10-class GPUs, NVIDIA RTX 2080, Mobile 500-class devices, H200-class GPUs", "instance_sizes": [30, 3, 31]}, "results": {"vs_baselines": {"MINLP Solver": "J3O achieves 97.7% (Taskonomy), 97.5% (DomainNet), 99.7% (Cityscape3D) of optimal accuracy/loss, with 1.21%-14.70% of runtime.", "Greedy-AO": "J3O achieves better accuracy/loss (e.g., 0.094 vs 0.151 loss on Taskonomy) with comparable runtime.", "OPT-AO": "J3O achieves slightly lower accuracy but 2.6x-2.15x faster runtime on Taskonomy/DomainNet; outperforms on both accuracy and runtime on Cityscape3D.", "Rand-AO": "J3O significantly outperforms in accuracy/loss (e.g., 0.094 vs 0.276 loss on Taskonomy).", "Full Local": "J3O significantly outperforms in accuracy/loss (e.g., 0.094 vs 0.507 loss on Taskonomy)."}, "scalability": "J3O's runtime scales linearly with the number of models and memory budgets, remaining orders of magnitude faster than optimal solvers on larger problems.", "statistical_rigor": "Results are averaged over 20 random seeds, with standard deviations reported.", "limitations_acknowledged": ["Current approach assumes slowly evolving or non-noisy task demands and resource availability."]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2508.11850", "arxiv_url": "https://arxiv.org/abs/2508.11850", "title": "EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models", "authors": ["M."], "abstract": "", "published_date": "2025-08-16", "affiliations": "Huawei Technologies Canada, University of British Columbia, University of Toronto", "category": "Generative AI for OR", "relevance": {"methodological": 9, "problem": 10, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper appears to be a direct collision with your active project 'EvoCut' (same name, same methodology). It successfully demonstrates that LLMs can generate effective MILP cuts using empirical verification rather than formal proofs, a strategy we must evaluate immediately."}, "brief": "Yazdani et al. introduce EvoCut, an evolutionary framework where LLMs generate Python code for MILP cuts, filtered by a 'usefulness check' (does it cut the current LP relaxation?) and an 'empirical validity check' (does it preserve known integer optima?). They report 17-57% gap reductions on TSPLIB and JSSP compared to Gurobi defaults, backed by strong ablation studies on the evolutionary operators. **Key Takeaway:** The reliance on 'acceleration cuts'—constraints verified empirically on small datasets rather than formally proven—bypasses the bottleneck of automated theorem proving while still delivering valid speedups. We should immediately adopt their 'LP separation' check as a cheap, high-signal reward for our own evolutionary search loops.", "methodology": {"core_method": "Evolutionary algorithm powered by multiple LLM-based agents for iterative generation and refinement of acceleration cuts", "llm_role": "heuristic_generator", "llm_model_used": "DeepSeek-R1", "search_type": "evolutionary_search", "novelty_claim": "EVOCUT is the first framework that, given only an MILP model and a small set of instances, automatically produces and verifies acceleration cuts with no human in the loop.", "components": ["LLM-based initializer agent", "LLM-based crossover agents", "LLM-based mutation agents", "Verifier (code check, optimal solution preservation check, usefulness check)", "Evaluator (fitness computation)", "Evolutionary process (elitism, selection, crossover, mutation)"], "training_required": false}, "tags": {"methods": ["evolutionary_algorithm", "llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "evolution_of_heuristics", "funsearch", "program_synthesis", "cutting_planes", "branch_and_bound"], "problems": ["MILP_general", "TSP", "MCND", "CWLP", "job_shop_scheduling", "tiling_problem"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "funsearch", "specific_domain": null, "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Automated Generation of Acceleration Cuts for Mixed Integer Linear Programming", "short": "MILP Cut Generation", "class_": "algorithm_design", "properties": ["NP-hard", "combinatorial", "mixed_integer_linear", "formulation_strengthening"], "scale": "up to 250 cities (TSP), up to 20 nodes/318 arcs/200 commodities (MCND), 100 facilities/1000 customers (CWLP), up to 100 jobs/20 machines (JSSP), N up to 25 (IMO P6)"}, "lineage": {"direct_ancestors": [{"paper": "Romera-Paredes et al. 2024", "relationship": "extends the LLM-evolutionary search paradigm from heuristic discovery to acceleration cut generation"}, {"paper": "Liu et al. 2024a", "relationship": "extends the LLM-evolutionary search paradigm from heuristic discovery to acceleration cut generation"}, {"paper": "Ye et al. 2024", "relationship": "extends the LLM-evolutionary search paradigm from heuristic discovery to acceleration cut generation"}], "closest_prior_work": "Romera-Paredes et al. 2024", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Integrate automated proof systems to generate provably optimality-preserving or valid cuts", "Implement dynamic cut separation and addition via callbacks during solver execution", "Develop efficient separation algorithms for generated cuts", "Explore the use of different LLM models or fine-tuning strategies"], "transferable_to": ["Any MILP solver supporting user-defined cuts", "Other combinatorial optimization problems beyond MILP", "Generation of other types of problem-specific constraints or formulation improvements", "Real-world, large-scale MILP applications"], "open_weaknesses": ["Optimal-solution preservation is currently verified empirically, not formally proven", "Cuts are inserted statically before solving, not dynamically", "Computational cost of LLM calls and evolutionary search can be high", "Reliance on a small verification set for empirical checks"]}, "artifacts": {"code_url": "https://github.com/milad1378yz/EvoCut", "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.81, "experiments": {"benchmarks": ["TSPLIB", "CommaLAB R MCND instances", "OR-LIB CWLP instances", "Taillard JSSP benchmark", "IMO 2025 P6 tiling problem"], "baselines": ["Gurobi Optimizer without cuts", "Cuts produced solely by the initializer agent"], "hardware": "Gurobi Optimizer 10.0.0 (Linux 64-bit) on eight threads, 20 hours wall-clock time for a full run", "instance_sizes": [250, 20, 318, 200, 100, 1000, 100, 20, 25]}, "results": {"vs_baselines": {"Gurobi Optimizer without cuts": "TSP: 57.4% gap reduction, 74% time saving for 10% gap; MCND: 17.1% gap reduction; CWLP: 46.2% gap reduction; JSSP: 37.3% gap reduction"}, "scalability": "EVOCUT reliably generates cuts that generalize to unseen instances and different problem distributions without instance-specific tuning.", "statistical_rigor": "Mean relative gap improvement and mean relative time-saving are reported with standard deviation (σ) over test instances.", "limitations_acknowledged": ["Empirical verification of optimal-solution preservation instead of formal proof", "Static cut insertion instead of dynamic separation via callbacks"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2508.09229", "arxiv_url": "https://arxiv.org/abs/2508.09229", "title": "Cluster Topology-Driven Placement of Experts Reduces Network Traffic in MoE Inference", "authors": ["Danil"], "abstract": "", "published_date": "2025-08-12", "affiliations": "AIRI, Skoltech, Avito", "category": "OR for Generative AI", "relevance": {"methodological": 5, "problem": 8, "inspirational": 6}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "Directly relevant to our 'GPUSched' and 'OR for Generative AI' projects. It provides a concrete, successful ILP formulation for MoE serving that we can adapt, validating that OR methods outperform heuristics in this domain."}, "brief": "This paper formulates the placement of MoE experts (specifically DeepSeek-R1/V3) onto distributed GPU clusters as an Integer Linear Program (ILP) to minimize network hops. While the results are simulation-based (counting hops rather than measuring real latency), they demonstrate that ILP-based placement reduces traffic by ~14-30% compared to Round-Robin, but *only* when the objective function is weighted by historical expert activation frequency; unweighted ILP performs poorly. The key takeaway for our GPUSched project is the specific formulation of the load-aware objective function and the finding that topology-aware placement requires usage statistics to beat simple heuristics. We should adapt this ILP formulation for our resource allocation work.", "methodology": {"core_method": "Integer Linear Programming (ILP) with load-aware objective function", "llm_role": "none", "llm_model_used": null, "search_type": "exact", "novelty_claim": "We formalize the problem of placing MoE layers over a cluster using integer linear programming and demonstrate that exploiting statistics of experts’ load significantly improves placement.", "components": ["Integer Linear Program formulation", "network graph representation", "shortest path calculation", "expert load frequency statistics", "CVXPy solver"], "training_required": false}, "tags": {"methods": ["integer_linear_programming", "optimization", "network_topology_modeling", "shortest_path_algorithms", "cvxpy"], "problems": ["llm_serving_optimization", "moe_expert_placement", "resource_allocation", "network_traffic_minimization", "distributed_system_optimization"], "contribution_type": ["new_method", "sota_result", "empirical_study", "framework"], "framework_lineage": null, "specific_domain": "llm_serving_optimization", "llm_coupling": null}, "problem": {"formal_name": "Cluster Topology-Driven Placement of Experts to Reduce Network Traffic in MoE Inference", "short": "MoE Expert Placement", "class_": "resource_allocation", "properties": ["topology-aware", "MoE LLMs", "inference-stage", "minimizing network traffic", "imbalanced expert load", "distributed"], "scale": "DeepSeekMoE 16B, DeepSeek-R1 671B models on up to 256 GPUs"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2502.06643", "relationship": "extends ILP-based MoE placement from"}], "closest_prior_work": "arXiv:2502.06643", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["combine with adaptive balancing methods for expert load", "analyze reasons for basic ILP sometimes underperforming greedy", "explore performance gains with even larger GPU counts"], "transferable_to": ["other distributed deep learning models", "general distributed system resource allocation problems", "different network architectures and topologies"], "open_weaknesses": ["lack of access to a standalone cluster for real-world benchmarking", "ILP assumptions focus on high-level distributions of experts and attention layers"]}, "artifacts": {"code_url": "https://github.com/svtdanny/moe_topology_pack", "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_status": "growing", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.14, "experiments": {"benchmarks": ["OASST1 dataset"], "baselines": ["Round-Robin (RR) placement", "Greedy placement", "MoETuner [7]"], "hardware": "128-core Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz CPU and 512Gb DDR3 RAM", "instance_sizes": [256, 64, 32]}, "results": {"vs_baselines": {"ILPLoad": "consistently outperforms RR, Greedy, and basic ILP across all tested topologies and Clayer values, achieving up to 39.1% reduction in average network hops compared to RR.", "Greedy": "outperforms RR, but is consistently worse than ILPLoad.", "ILP": "performance varies, sometimes worse than RR or Greedy, but generally better than RR for larger Clayer.", "MoETuner": "timeout after 12 hours on toy DeepSeek-16b model, making it impractical."}, "scalability": "Increasing the total number of GPUs is expected to yield more significant gains for ILPLoad; the performance gap between algorithms narrows as Clayer increases.", "statistical_rigor": "Results are reported as mean ± standard deviation over multiple runs, indicating variability but no explicit significance tests are mentioned.", "limitations_acknowledged": ["lack of access to a standalone cluster for real-world benchmarking", "assumptions in ILP focus on high-level distributions of experts and attention layers only"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2508.07768", "arxiv_url": "https://arxiv.org/abs/2508.07768", "title": "Pareto Multi-Objective Alignment for Language Models", "authors": ["Qiang"], "abstract": "", "published_date": "2025-08-11", "affiliations": "Ruhr University Bochum", "category": "OR for Generative AI", "relevance": {"methodological": 7, "problem": 5, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "The paper offers a mathematically grounded, O(n) complexity alternative to MGDA for multi-objective optimization. The closed-form gradient aggregation is a 'stealable' technique for our RL-infused search components that need to balance conflicting metrics (e.g., quality vs. latency) without manual weight tuning."}, "brief": "PAMA introduces a computationally efficient algorithm for multi-objective alignment by reformulating the expensive gradient-norm minimization of MGDA into a convex optimization problem with a closed-form solution, reducing complexity from O(n^2d) to O(n). Empirical results on LLaMA-2-7B are robust, showing stable convergence on conflicting objectives (e.g., harmlessness vs. length) where baselines like MGDA-UB oscillate or fail. The single most useful takeaway is the analytical derivation for optimal objective weighting (Theorem 1) and the 'Noon PPO' heuristic (clipping negative advantages); we could port this logic to our multi-objective process reward models in AlgoEvo to balance search signals efficiently. While the NLP experiments are trivial, the gradient balancing mechanism is directly applicable to our multi-objective RL controllers.", "methodology": {"core_method": "PAMA (PAreto Multi-Objective Alignment) algorithm, which transforms multi-objective RLHF into a convex optimization problem with a closed-form solution, combined with Noon PPO.", "llm_role": "subject_of_optimization", "llm_model_used": "GPT-2, GPT-2 XL, LLaMA-2", "search_type": "improvement", "novelty_claim": "PAMA is a principled and computationally efficient algorithm designed explicitly for MOA in LLMs, transforming multi-objective RLHF into a convex optimization problem with a closed-form solution, reducing complexity from O(n^2d) to O(n).", "components": ["PAMA algorithm", "Noon PPO", "Reward Models"], "training_required": true}, "tags": {"methods": ["rlhf", "ppo", "noon_ppo", "multi_objective_optimization", "convex_optimization", "gradient_descent", "gradient_aggregation", "llm_alignment", "llm_fine_tuning"], "problems": ["multi_objective_llm_alignment", "sentiment_control", "text_length_control", "humor_generation", "harmlessness_control"], "contribution_type": ["new_method", "sota_result", "theoretical_result", "framework"], "framework_lineage": "pama", "specific_domain": "multi_objective_llm_alignment", "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Multi-Objective Alignment for Language Models", "short": "MOA", "class_": "llm_alignment", "properties": ["multi_objective", "conflicting_objectives", "large_scale", "pareto_optimality"], "scale": "125M to 7B parameters"}, "lineage": {"direct_ancestors": [{"paper": "Proximal Policy Optimization Algorithms", "relationship": "extends PPO with Noon PPO variant"}, {"paper": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "relationship": "extends RLHF to multi-objective setting"}, {"paper": "Multiple-Gradient Descent Algorithm (MGDA)", "relationship": "addresses scalability limitations of gradient-based MOO methods like MGDA"}], "closest_prior_work": "Multi-task learning as multi-objective optimization (MGDA-UB)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Explore more complex and diverse objective combinations beyond current experimental settings.", "Investigate adaptive weighting schemes for objectives within PAMA to further enhance flexibility.", "Extend theoretical guarantees to non-convex settings or different reward structures.", "Develop MOA without explicit pre-trained reward models."], "transferable_to": ["Multi-task learning in other deep learning domains (e.g., vision, speech).", "Multi-objective reinforcement learning beyond LLMs.", "Personalized LLM alignment with individual user preferences.", "Resource allocation or scheduling problems where LLMs are involved."], "open_weaknesses": ["The current formulation relies on pre-trained reward models, which may limit its applicability in scenarios without such models.", "The paper uses a single fixed seed for larger models due to computational cost, which could impact the statistical robustness of those results.", "The focus is on text generation; applicability to other LLM capabilities (e.g., reasoning, code generation) with multiple objectives needs further exploration.", "The choice and potential interactions of objectives (e.g., harmlessness vs. creativity) can be complex and require careful consideration."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_32", "front_status": "growing", "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.99, "experiments": {"benchmarks": ["IMDb", "HH-RLHF"], "baselines": ["MORLHF", "MGDA-UB"], "hardware": "Intel i9-14900K CPU, single NVIDIA RTX A6000 GPU", "instance_sizes": [125, 1500, 7000]}, "results": {"vs_baselines": {"MORLHF": "Consistently underperforms PAMA across most objectives (sentiment, humor, harmlessness), often plateauing at lower reward levels. For LLaMA-2 7B, MORLHF achieves higher length reward (5.3502 vs 3.1400) but significantly lower harmlessness (-0.1313 vs 0.4406).", "MGDA-UB": "Consistently underperforms PAMA across all objectives, often exhibiting instability, severe performance degradation, and pronounced oscillations during training. For LLaMA-2 7B, MGDA-UB achieves higher length reward (9.0842 vs 3.1400) but significantly lower harmlessness (-0.2844 vs 0.4406)."}, "scalability": "PAMA demonstrates robust and effective multi-objective alignment capabilities across language models ranging from 125M to 7B parameters, maintaining stable optimization and superior performance.", "statistical_rigor": "For GPT-2 (125M) experiments, evaluations were conducted using eight fixed random seeds, with shaded areas representing standard deviation. For GPT-2 XL (1.5B) and LLaMA-2 (7B), a single fixed seed was used due to computational cost.", "limitations_acknowledged": []}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2508.07468", "arxiv_url": "https://arxiv.org/abs/2508.07468", "title": "CP-Agent: Agentic Constraint Programming", "authors": ["Stefan"], "abstract": "", "published_date": "2025-08-10", "affiliations": "TU Wien", "category": "Generative AI for OR", "relevance": {"methodological": 5, "problem": 9, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "Directly relevant to our OR-Bench project. The paper demonstrates that 100% accuracy on constraint modeling is possible via simple iterative refinement IF the benchmark ambiguities are fixed. We need to examine their benchmark clarifications and the negative result regarding task management tools."}, "brief": "Szeider implements a standard ReAct agent with a persistent IPython kernel to iteratively generate and refine CPMpy models, claiming 100% accuracy on CP-Bench. However, this perfect score is achieved on a *modified* version of the benchmark where the author manually fixed 31 ambiguous problem statements and 19 ground-truth errors—making the '100%' result an artifact of dataset cleaning rather than pure model capability. The most actionable takeaways are the negative result for explicit 'task management' tools (which hurt performance on hard problems) and the effectiveness of a minimal (<50 lines) domain prompt over complex scaffolding. We should review their clarified benchmark for our OR-Bench work.", "methodology": {"core_method": "Agentic Python coding agent using ReAct framework with persistent IPython kernel for iterative refinement of CPMpy constraint models", "llm_role": "code_writer", "llm_model_used": "Claude Sonnet 4.5", "search_type": "hybrid", "novelty_claim": "CP-Agent, a Python coding agent that uses the ReAct framework with a persistent IPython kernel to iteratively execute code, observe solver feedback, and refine constraint models from natural language.", "components": ["ReAct loop controller", "persistent IPython kernel", "python exec tool", "save code tool", "task management tool (optional)", "custom kernel manager", "OpenRouter API", "system prompt", "project prompt", "task prompt", "CPMpy library"], "training_required": false}, "tags": {"methods": ["llm_agent", "react_framework", "ipython_kernel", "iterative_refinement", "constraint_programming", "cpmpy", "neurosymbolic_ai", "prompt_engineering", "model_context_protocol", "program_synthesis", "llm_as_code_generator", "llm_in_the_loop"], "problems": ["natural_language_to_code", "constraint_modeling", "combinatorial_optimization", "combinatorial_satisfaction", "scheduling", "assignment_problems", "graph_problems", "combinatorial_design", "tsp", "job_shop_scheduling", "bin_packing", "knapsack", "n_queens", "sudoku", "cryptarithmetic_puzzle", "wolf_goat_cabbage"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study", "new_benchmark"], "framework_lineage": "agentic_python_coder", "specific_domain": "constraint_modeling", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Constraint Modeling from Natural Language", "short": "Constraint Modeling", "class_": "program_synthesis", "properties": ["combinatorial", "satisfaction", "optimization", "diverse constraint types"], "scale": "101 diverse combinatorial problems"}, "lineage": {"direct_ancestors": [{"paper": "ReAct: Synergizing reasoning and acting in language models", "relationship": "implements the ReAct framework for agentic workflow"}, {"paper": "CP-Bench: Evaluating large language models for constraint modelling", "relationship": "uses and clarifies the CP-Bench benchmark"}, {"paper": "CP-Bench: Evaluating large language models for constraint modelling", "relationship": "improves upon fixed workflow LLM approaches for constraint modeling"}], "closest_prior_work": "CP-Bench: Evaluating large language models for constraint modelling", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["adapt to other scientific computing domains", "optimize prompts for specific LLM models (e.g., using DSPy)", "further investigate task management tool utility for specific problem families", "integrate with more MCP-compatible clients"], "transferable_to": ["logic_programming", "automated_program_repair", "general_code_generation_tasks", "other_neurosymbolic_ai_applications"], "open_weaknesses": ["task_management_tool_can_be_counterproductive_increasing_token_usage_and_time", "performance_varies_across_different_llm_models_without_specific_prompt_optimization", "stochasticity_in_llm_responses_leads_to_variability_in_execution_times", "results_not_directly_comparable_to_original_cp_bench_due_to_benchmark_clarifications"]}, "artifacts": {"code_url": "https://github.com/szeider/agentic-python-coder", "models_released": false, "new_benchmark": true}, "front_id": "generative_ai_for_or_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.39, "experiments": {"benchmarks": ["CP-Bench (clarified version)"], "baselines": ["Michailidis et al. [14] fixed workflow (GPT-4-turbo with repeated sampling and self-verification)"], "hardware": "Not specified", "instance_sizes": [101]}, "results": {"vs_baselines": {"CP-Agent": "100% accuracy on 101 problems", "Michailidis et al. [14] fixed workflow (GPT-4-turbo with repeated sampling and self-verification)": "70% accuracy on original CP-Bench (not directly comparable due to benchmark clarifications)"}, "scalability": "The number of python exec calls varies from 1 to 28, and execution times range from 20 to 542 seconds, depending on problem complexity.", "statistical_rigor": "All experiments ran three times, and results are averaged across three runs to account for randomness.", "limitations_acknowledged": ["Direct comparison with original CP-Bench results is not possible due to benchmark clarifications", "Task management tool can be counterproductive for some problems, increasing token usage and execution time", "Prompt optimization for Claude Sonnet 4.5 might affect performance of other LLMs"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2505.18602", "arxiv_url": "https://arxiv.org/abs/2505.18602", "title": "LLM-Meta-SR: In-Context Learning for Evolving Selection Operators in Symbolic Regression", "authors": ["Hengzhe"], "abstract": "", "published_date": "2025-08-08", "affiliations": "Victoria University of Wellington, Michigan State University", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 7, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "The 'semantics-aware' crossover mechanism is a concrete, algorithmic improvement to LLM evolutionary search that directly addresses our need for better signal and sample efficiency. It moves beyond random or fitness-proportional parent selection to explicitly pairing solutions with complementary performance profiles."}, "brief": "Zhang et al. develop a meta-evolutionary framework to evolve selection operators for symbolic regression, achieving state-of-the-art results on SRBench by outperforming expert-designed methods like ε-lexicase. The standout contribution is **semantics-aware crossover**: rather than selecting parents based solely on scalar fitness, they compute complementarity scores using performance vectors across instances, explicitly retrieving parents that solve different subsets of the problem. This effectively treats parent selection as a retrieval task based on behavioral signatures, ensuring the LLM combines distinct functional capabilities. We should immediately implement this complementarity-based parent retrieval in AlgoEvo to improve how we merge heuristics.", "methodology": {"core_method": "LLM-driven meta-evolutionary framework for designing selection operators, incorporating semantics-aware selection, bloat control, and domain knowledge into prompts", "llm_role": "evolutionary_search", "llm_model_used": "GPT-4.1 Mini", "search_type": "evolutionary_search", "novelty_claim": "The paper proposes an LLM-driven meta-symbolic regression framework to automatically design selection operators, addressing semantic awareness and code bloat through semantic-based feedback, complementary selection, prompt-based length control, multi-objective survival selection, and domain knowledge incorporation.", "components": ["meta-evolution workflow", "population initialization", "synthetic evaluation", "solution evaluation", "survival selection", "solution selection", "solution generation (crossover, mutation)", "semantics-aware evolution", "bloat control", "domain knowledge incorporation", "genetic programming (inner loop)"], "training_required": true}, "tags": {"methods": ["llm_evolutionary_search", "genetic_programming", "evolutionary_algorithm", "llm_as_heuristic", "llm_code_generation", "in_context_learning", "metaheuristics"], "problems": ["symbolic_regression", "operator_discovery", "heuristic_evolution"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": null, "specific_domain": "symbolic_regression", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Automated Design of Selection Operators for Evolutionary Symbolic Regression", "short": "Meta-SR Selection Operator Design", "class_": "algorithm_design", "properties": ["meta-learning", "evolutionary_algorithm_component_design", "interpretable_output", "semantic_awareness_required", "bloat_control_required"], "scale": "240-1059 observations, 50-124 features (for meta-evolution); up to 10,000 training instances (for SR evaluation)"}, "lineage": {"direct_ancestors": [{"paper": "RAG-SR", "relationship": "improves upon LLM-based symbolic regression"}, {"paper": "lexicase selection", "relationship": "extends and designs new forms of selection operators"}], "closest_prior_work": "RAG-SR", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Address LLM-generated code logical bugs on small datasets", "Explore design of other evolutionary operators (e.g., mutation, crossover)", "Investigate more robust semantic feedback mechanisms", "Optimize LLM call efficiency and computational cost"], "transferable_to": ["Automated design of other evolutionary algorithm components", "Algorithm design for other domains requiring interpretable models", "Meta-learning for other optimization problems"], "open_weaknesses": ["LLM-generated code may contain subtle logical bugs on unseen or small datasets", "Computational cost associated with LLM interactions", "Generalizability of evolved operators to vastly different problem distributions"]}, "artifacts": {"code_url": "https://anonymous.4open.science/r/LLM-Meta-SR/", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_5", "front_status": "stable", "bridge_score": 0.6513, "is_bridge": true, "priority_score": 7.14, "experiments": {"benchmarks": ["SRBench", "OpenML 505", "OpenML 4544", "OpenML 588", "OpenML 650"], "baselines": ["automatic \n-lexicase selection (AutoLex)", "probabilistic lexicase selection (PLex)", "lexicase-like selection via diverse aggregation (DALex)", "\n-lexicase selection with dynamic split (D-Split)", "random sampling with tournament selection (RDS-Tour)", "complementary phenotype selection (CPS)", "tournament selection (size 3)", "tournament selection (size 7)", "Boltzmann selection with temperature scheduling", "RAG-SR"], "hardware": "AMD Milan 7713 CPU", "instance_sizes": [240, 500, 1000, 1059, 10000]}, "results": {"vs_baselines": {"AutoLex": "+2.3% R2 on SRBench (Omni 0.86 vs AutoLex 0.84)", "PLex": "+3.6% R2 on SRBench (Omni 0.86 vs PLex 0.83)", "DALex": "+3.6% R2 on SRBench (Omni 0.86 vs DALex 0.83)", "D-Split": "+3.6% R2 on SRBench (Omni 0.86 vs D-Split 0.83)", "RDS-Tour": "+4.9% R2 on SRBench (Omni 0.86 vs RDS-Tour 0.82)", "CPS": "+4.9% R2 on SRBench (Omni 0.86 vs CPS 0.82)", "tournament selection (size 3)": "+6.2% R2 on SRBench (Omni 0.86 vs Tournament-3 0.81)", "tournament selection (size 7)": "+6.2% R2 on SRBench (Omni 0.86 vs Tournament-7 0.81)", "Boltzmann selection with temperature scheduling": "+6.2% R2 on SRBench (Omni 0.86 vs Boltzmann 0.81)", "RAG-SR": "+1.2% R2 on SRBench (RAG-SR-Omni 0.87 vs RAG-SR 0.86), also improved model size and training time"}, "scalability": "The evolved Omni selection operator maintains population diversity and suffers less from rapid growth in tree size, leading to smaller, more interpretable models and reduced computational cost during evaluation and deployment.", "statistical_rigor": "Experiments were repeated 3 times for meta-evolution and 10 independent runs per dataset for symbolic regression, reporting medians and confidence intervals, with statistical significance judged using Wilcoxon signed-rank test with Benjamini\n-Hochberg correction.", "limitations_acknowledged": ["The LLM-generated code may contain subtle logical bugs if not exposed to a wide range of instances during meta-evolution, as demonstrated by poor performance on small datasets."]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2508.03464", "arxiv_url": "https://arxiv.org/abs/2508.03464", "title": "Learning to Incentivize: LLM-Empowered Contract for AIGC Offloading in Teleoperation", "authors": ["Zijun"], "abstract": "", "published_date": "2025-08-05", "affiliations": "University of Houston, The Pennsylvania State University, University of Florida, Kyung Hee University, China University of Petroleum (East China), Prairie View A&M University", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 5, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "The paper implements a hierarchical 'verbal gradient' architecture (short-term pair analysis + long-term memory) for evolving code, which directly addresses our focus on improving search signals and memory in AlgoEvo."}, "brief": "Zhan et al. propose an LLM-based evolutionary framework to generate Python solvers for inferring hidden agent parameters in contract design (a bilevel OR problem). While the experiments are toy-scale (N=7 actions) and benchmarks are weak, the methodological architecture is highly relevant: they separate 'short-term reflectors' (analyzing parent pairs) from a 'long-term reflector' (aggregating insights across generations) to guide the Mutation LLM. This is a concrete, transferable implementation of evolutionary memory that we should test to improve sample efficiency in our own code-evolving agents.", "methodology": {"core_method": "LLM-evolved solver for ASP setting inference (P2) combined with convex optimization for contract derivation (P3)", "llm_role": "evolutionary_search", "llm_model_used": "gpt-4.1-mini-2025-04-14", "search_type": "hybrid", "novelty_claim": "This is the first work to utilize LLM in designing incentive mechanisms under information asymmetry.", "components": ["Seed solver (Algorithm 1)", "Generator LLM", "Evaluator LLM", "Short-term reflector LLM", "Crossover LLM", "Long-term reflector LLM", "Mutation LLM", "KMeans clustering", "Convex Optimization"], "training_required": false}, "tags": {"methods": ["llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_in_the_loop", "evolution_of_heuristics", "program_synthesis", "convex_optimization", "k_means_clustering", "mixed_integer_nonlinear_programming"], "problems": ["online_learning_contract_design", "moral_hazard", "incentive_mechanism_design", "aigc_offloading", "teleoperation"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "evolution_of_heuristics", "specific_domain": "aigc_offloading_teleoperation", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Online Learning Contract Design Problem for AIGC Offloading in Teleoperation", "short": "OLCD for AIGC Offloading", "class_": "incentive_mechanism_design", "properties": ["information_asymmetry", "moral_hazard", "online_learning", "APX-hard", "NP-hard", "unknown_variable_sizes", "discrete_action_space", "discrete_outcome_space"], "scale": "M up to 20 outcomes, N up to 1000 actions, K up to 100 historical logs"}, "lineage": {"direct_ancestors": [{"paper": "Adaptive contract design for crowdsourcing markets: Bandit algorithms for repeated principal-agent problems", "relationship": "builds on online learning contract theory from"}, {"paper": "Deep contract design via discontinuous networks", "relationship": "improves sample efficiency of deep learning-based online contract design from"}, {"paper": "Evolution of heuristics: Towards efficient automatic algorithm design using large language model", "relationship": "inspired LLM-empowered solver evolution from"}, {"paper": "Reevo: Large language models as hyper-heuristics with reflective evolution", "relationship": "inspired LLM-empowered solver evolution from"}], "closest_prior_work": "Deep contract design via discontinuous networks", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Investigate LLM fine-tuning or RL training for improved solver evolution.", "Extend the framework to address adverse selection or multi-agent contract design.", "Develop theoretical guarantees for LLM-empowered contract design.", "Optimize LLM inference cost and latency for real-time deployment."], "transferable_to": ["Resource allocation in edge computing and federated learning.", "Crowdsourcing markets with hidden actions.", "General principal-agent problems in various economic settings.", "Incentive mechanism design for other AIGC service applications."], "open_weaknesses": ["Sensitivity of performance to the size of the outcome space (M).", "Reliance on a sufficient number of historical interaction logs (K).", "Potential for sub-optimal ASP utility compared to benchmarks (though still positive).", "Computational cost and latency of LLM interactions in practical systems."]}, "artifacts": {"code_url": "https://github.com/Zijun0819/llm4contract", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.44, "experiments": {"benchmarks": ["Seed", "Zero-shot", "Bandit"], "baselines": ["Seed (Algorithm 1)", "Zero-shot (Algorithm 2 under simulated scenario)", "Bandit (Multi-armed bandit method)"], "hardware": "6-core AMD Ryzen 5 processor, 32 GB of RAM, an RTX 4060 GPU, Windows 11 OS", "instance_sizes": [2, 3, 4, 5, 6, 7, 8, 10, 12, 20, 25, 50, 100, 1000]}, "results": {"vs_baselines": {"Seed": "+16-58% teleoperator utility (π%T)", "Bandit": "+5-38% teleoperator utility (π%T)", "Zero-shot": "Comparable to Ours for M=2, inferior for N=2, otherwise Ours is better"}, "scalability": "Performance improves with more historical logs (K), declines with larger outcome (M) and action (N) spaces, but maintains η=75% for K=100 and η=95% for M=2, K={50,100} across varying N.", "statistical_rigor": "Experiments ran multiple times, and average values were taken to enhance reliability.", "limitations_acknowledged": ["Performance (π%T) deteriorates with increasing size of outcome space (M), indicating sensitivity to M."]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2508.03117", "arxiv_url": "https://arxiv.org/abs/2508.03117", "title": "Toward a Trustworthy Optimization Modeling Agent via Verifiable Synthetic Data Generation", "authors": ["Vinicius"], "abstract": "", "published_date": "2025-08-05", "affiliations": "IBM Research AI", "category": "Generative AI for OR", "relevance": {"methodological": 6, "problem": 7, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While not an evolutionary search paper, the 'Symbolic -> NL' synthetic data pipeline is the gold standard for creating verifiable OR benchmarks, which is directly relevant to our OR-Bench project. The multi-language majority voting is a concrete engineering trick we can adopt immediately."}, "brief": "Lima et al. introduce a pipeline to generate synthetic optimization datasets by starting with symbolic MILP instances (ground truth) and using LLMs to generate natural language descriptions, ensuring full verifiability. They fine-tune a small model (Granite 8B) that beats GPT-4 on 6/7 benchmarks, largely due to a 'majority vote' mechanism where the agent generates code in 5 different modeling languages (Pyomo, Gurobi, etc.) and checks for result consistency. **Takeaway:** We should steal the multi-language execution voting to boost robustness in our code generation agents. Furthermore, their reverse-generation (Symbolic $\\to$ NL) strategy is the correct approach for generating infinite, error-free test cases for our OR-Bench work.", "methodology": {"core_method": "Verifiable Synthetic Data Generation (SDG) pipeline combined with a modular LLM agent (OptiTrust) employing multi-stage translation, multi-language inference, and majority-vote cross-validation", "llm_role": "data_generator, code_writer, decomposition_guide, formulation_generator, evaluator", "llm_model_used": "Llama3.3-70B-Instruct, Llama 4 Maverick, Phi-4, DeepSeek-R1-Distill-Llama-70B, OpenAI o3-mini, Granite 3.2 8B Instruct", "search_type": "hybrid", "novelty_claim": "Our framework introduces a verifiable synthetic data generation pipeline for optimization modeling and a modular LLM agent, OptiTrust, that performs multi-stage translation with multi-language inference and majority-vote cross-validation.", "components": ["Decomposition Agent", "Formulation Agent", "Code Agent", "Synthetic Data Generation Pipeline", "Multi-language Inference", "Majority Voting", "Self-reflection", "Debugging"], "training_required": true}, "tags": {"methods": ["llm_as_heuristic", "llm_code_generation", "llm_fine_tuned", "llm_in_the_loop", "llm_as_evaluator", "multi_agent_system", "synthetic_data_generation", "majority_voting", "self_reflection", "debugging", "supervised_fine_tuning"], "problems": ["natural_language_to_optimization_modeling", "linear_programming", "mixed_integer_linear_programming", "combinatorial_optimization", "dataset_curation"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study", "dataset_curation"], "framework_lineage": null, "specific_domain": "natural_language_to_optimization_modeling", "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Natural Language to Optimization Modeling", "short": "NL2Opt", "class_": "program_synthesis", "properties": ["linear_programming", "mixed_integer_linear_programming", "natural_language_input", "multi_language_output", "verifiable_solutions"], "scale": "varying, including complex and lengthy descriptions"}, "lineage": {"direct_ancestors": [{"paper": "Xiao et al. 2023", "relationship": "adopts multi-agent design pattern from Chain-of-Experts"}, {"paper": "AhmadiTeshnizi, Gao, and Udell 2024", "relationship": "adopts multi-agent design pattern from OptiMUS"}, {"paper": "Lu et al. 2025", "relationship": "builds on synthetic data generation from mathematical formulations, adding verifiability"}], "closest_prior_work": "Lu et al. 2025", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Extend to nonlinear programming problems", "Incorporate more complex problem structures and abstract semantics", "Improve handling of ambiguous or underspecified natural language descriptions", "Integrate with real-time data sources for dynamic optimization modeling"], "transferable_to": ["Other optimization paradigms (e.g., constraint programming, stochastic programming)", "Automated algorithm design for other computational domains", "Code generation for other scientific computing tasks", "Automated data science pipeline generation"], "open_weaknesses": ["Performance drops for complex and lengthy problem descriptions", "Limited structural diversity for certain structured problems due to templated generation", "Reliance on teacher models for demonstration generation (potential for bias/errors)", "Scalability to extremely large-scale optimization problems"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_status": "stable", "bridge_score": 0.6495, "is_bridge": true, "priority_score": 5.38, "experiments": {"benchmarks": ["NL4Opt", "EasyLP", "ComplexLP", "IndustryOR", "NLP4LP", "ComplexOR", "ReSocratic"], "baselines": ["GPT-4", "CoT", "Chain-of-Experts", "CAFA", "ORLM-LLaMA-3 8B"], "hardware": "NVIDIA H100 GPUs", "instance_sizes": []}, "results": {"vs_baselines": {"GPT-4": "91.6% (OptiTrust) vs 61.2% (GPT-4) on NL4Opt; 92.3% vs 70.3% on EasyLP; 94.4% vs 73.6% on NLP4LP; 81.4% vs 48.4% on ReSocratic; 61.1% vs 42.9% on ComplexOR; 42.9% vs 38.1% on IndustryOR; 63.1% vs 57.7% on ComplexLP", "CoT": "91.6% (OptiTrust) vs 62.2% (CoT) on NL4Opt; 92.3% vs 49.5% on EasyLP; 94.4% vs 74.7% on NLP4LP; 81.4% vs 43.6% on ReSocratic; 61.1% vs 39.2% on ComplexOR; 42.9% vs 40.5% on IndustryOR; 63.1% vs 42.3% on ComplexLP", "Chain-of-Experts": "91.6% (OptiTrust) vs 66.7% (Chain-of-Experts) on NL4Opt; 92.3% vs 94.4% on EasyLP; 94.4% vs 87.4% on NLP4LP; 81.4% vs 71.2% on ReSocratic; 61.1% vs 57.1% on ComplexOR; 42.9% vs 31.2% on IndustryOR; 63.1% vs 50.6% on ComplexLP", "CAFA": "91.6% (OptiTrust) vs 68.1% (CAFA) on NL4Opt; 92.3% vs 71.2% on EasyLP; 94.4% vs 50.0% on NLP4LP; 81.4% vs 40.1% on ReSocratic; 61.1% vs 46.4% on ComplexOR; 42.9% vs 41.1% on IndustryOR; 63.1% vs 44.5% on ComplexLP", "ORLM-LLaMA-3 8B": "91.6% (OptiTrust) vs 73.8% (ORLM-LLaMA-3 8B) on NL4Opt; 92.3% vs 90.4% on EasyLP; 94.4% vs 76.4% on NLP4LP; 81.4% vs 61.8% on ReSocratic; 61.1% vs 50.0% on ComplexOR; 42.9% vs 42.9% on IndustryOR; 63.1% vs 59.5% on ComplexLP"}, "scalability": "Solution accuracy consistently drops for all evaluated methods, including OptiTrust, when tackling optimization problems characterized by complex and lengthy descriptions.", "statistical_rigor": "Solution Accuracy is the primary metric, widely accepted in prior benchmarking studies. Random seed was set to zero and temperature to 0.7 during inference. No explicit mention of multiple runs, variance, or significance tests for results.", "limitations_acknowledged": ["Optimization problems with complex and lengthy descriptions remain challenging for all methods.", "Simplified generation procedures for certain structured problems restrict structural diversity."]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2504.05108", "arxiv_url": "https://arxiv.org/abs/2504.05108", "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning", "authors": ["Anja"], "abstract": "", "published_date": "2025-08-04", "affiliations": "EPFL, Apple", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 8, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper successfully executes 'RL-infused evolution'—a specific item in our primary focus list—by integrating DPO into the FunSearch loop. It provides a concrete technical solution (Forward KL regularization) to the diversity collapse problem that typically plagues self-improving search."}, "brief": "EvoTune augments LLM-based evolutionary search (FunSearch) by iteratively fine-tuning the LLM weights using Direct Preference Optimization (DPO) on the generated programs. The results are robust, consistently outperforming static FunSearch on Bin Packing, TSP, and Hash Code benchmarks by discovering better heuristics faster. The critical takeaway is the use of **Forward KL regularization** in DPO instead of the standard Reverse KL; this prevents the mode collapse that usually kills evolutionary diversity, allowing the model to learn from high-fitness samples while maintaining exploration. This is a direct blueprint for implementing the 'RL-infused evolution' component of our AlgoEvo project.", "methodology": {"core_method": "Evolutionary Search with Reinforcement Learning (DPO) fine-tuning", "llm_role": "heuristic_generator", "llm_model_used": "Llama3.2 1B Instruct, Phi 3.5 Mini Instruct, Granite 3.1 2B Instruct", "search_type": "hybrid", "novelty_claim": "First to tightly integrate LLM-based evolutionary search with RL fine-tuning in the loop to optimize the LLM policy.", "components": ["LLM", "Evolutionary Search", "Reinforcement Learning (DPO)", "Program Database (island-based)", "Prompt Construction (Chain-of-Thought)", "Forward KL regularization"], "training_required": true}, "tags": {"methods": ["evolutionary_search", "reinforcement_learning", "dpo", "llm_as_heuristic", "llm_code_generation", "llm_prompt_optimization", "program_synthesis", "llm_evolutionary_search", "evolution_of_heuristics"], "problems": ["bin_packing", "tsp", "datacenter_optimization", "ridesharing_optimization", "symbolic_regression", "heuristic_evolution", "program_synthesis"], "contribution_type": ["new_method", "sota_result", "framework"], "framework_lineage": "evotune", "specific_domain": null, "llm_coupling": "rl_trained"}, "problem": {"formal_name": "Algorithm Discovery for Combinatorial Optimization and Symbolic Regression", "short": "Algorithm Discovery", "class_": "algorithm_design", "properties": ["heuristic_design", "program_synthesis", "online", "graph_optimization", "packing"], "scale": "500 items, 100-200 nodes, 9x9 to 15x15 grids"}, "lineage": {"direct_ancestors": [{"paper": "FunSearch", "relationship": "extends LLM-based evolutionary search by integrating RL fine-tuning"}], "closest_prior_work": "FunSearch", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Explore larger LLM models and increased sampling budgets", "Optimize computational costs of the RL training phase", "Investigate trade-offs between compute costs and inference performance", "Apply the framework to a broader range of complex combinatorial optimization problems"], "transferable_to": ["Other NP-hard combinatorial optimization problems", "More complex scientific discovery tasks beyond current symbolic regression", "Automated design of other types of algorithms (e.g., graph algorithms)", "Resource allocation and scheduling problems in various domains"], "open_weaknesses": ["High computational cost associated with the RL training phase", "Scalability limitations with very large LLM models", "Generalizability to problems requiring highly specialized domain knowledge", "Potential for local optima in the evolutionary search process"]}, "artifacts": {"code_url": "https://claire-labo.github.io/EvoTune/", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.86, "experiments": {"benchmarks": ["OR-Library (Bin Packing)", "TSPLib (Traveling Salesman Problem)", "Google Hash Code (Datacenter Optimization)", "Google Hash Code (Self-Driving Rides)", "LLM-SR (Material Stress Behavior)", "LLM-SR (Bacterial Growth Modeling)", "Custom TSP instances", "Custom Flatpack instances"], "baselines": ["FunSearch", "POMO", "LEHD", "NeuralGLS", "KGLS", "Best-Fit heuristic", "Greedy heuristic", "Mixtral 8x7B", "GPT-3.5-turbo"], "hardware": "null", "instance_sizes": [500, 100, 200, 9, 11, 15]}, "results": {"vs_baselines": {"FunSearch": "Consistently higher top-50 reward scores across all tasks and models; up to 15% better optimality gap on BP/FP, 1.2% on TSP.", "POMO": "Outperforms on TSPLib (0.32 vs 2.02 optimality gap at tmax=1000).", "LEHD": "Outperforms on TSPLib (0.32 vs 1.92 optimality gap at tmax=1000).", "NeuralGLS": "Outperforms on TSPLib (0.32 vs 0.96 optimality gap at tmax=1000).", "KGLS": "Outperforms on TSPLib (0.32 vs 0.36 optimality gap at tmax=1000).", "Best-Fit heuristic": "Outperforms on Bin Packing (2.06 vs 5.37 optimality gap).", "Greedy heuristic": "Outperforms on Flatpack (0.0829 vs 0.1092 optimality gap).", "Mixtral 8x7B": "Outperforms on LLM-SR Stress-Strain (0.0033 vs 0.0162 NMSE).", "GPT-3.5-turbo": "Outperforms on LLM-SR Stress-Strain (0.0033 vs 0.0210 NMSE).", "Human (Hash Code Datacenter)": "Achieves 418 vs 407 (top human score)."}, "scalability": "Performance gap to baseline widens with larger sampling budgets; method scales effectively to real-world settings.", "statistical_rigor": "Results averaged over 10 random seeds with standard error reported.", "limitations_acknowledged": ["Limited LLM model sizes (1B-3.8B parameters) and sampling budget (up to 22.4k outputs) explored.", "Additional compute costs due to RL training phase; trade-offs with inference costs need further investigation."]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2508.01558", "arxiv_url": "https://arxiv.org/abs/2508.01558", "title": "EvoVLMA: Evolutionary Vision-Language Model Adaptation", "authors": ["Kun"], "abstract": "", "published_date": "2025-08-03", "affiliations": "Chinese Academy of Sciences", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 7, "problem": 4, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While the application is Computer Vision (irrelevant), the paper provides two concrete engineering insights for our AlgoEvo platform: 1) Joint evolution of complex components failed while sequential evolution succeeded, and 2) A robust 'web-service' sandboxing architecture to handle CUDA errors from LLM-generated code."}, "brief": "This paper proposes EvoVLMA, an LLM-based evolutionary framework that searches for Python code to adapt Vision-Language Models (feature selection and logits computation). They demonstrate that **jointly** evolving two coupled algorithmic components fails (worse than random), whereas a **sequential** two-stage evolution strategy yields SOTA results (beating manual baselines by ~1-2%). For our AlgoEvo work, the key takeaway is the infrastructure design: they wrap code execution in restartable web services with a process monitor to handle the high rate of CUDA errors/timeouts in generated code—a practical 'trick' we should adopt to improve our search stability.", "methodology": {"core_method": "LLM-assisted two-stage evolutionary algorithm with crossover and mutation operators for optimizing feature selection and logits computation functions in code space", "llm_role": "code_writer", "llm_model_used": "DeepSeek", "search_type": "evolutionary_search", "novelty_claim": "We introduce automatic algorithm design to the domain of efficient adaptation of pre-trained VLMs and propose a two-stage evolutionary method performing algorithm searching in code space.", "components": ["initialization", "crossover", "mutation", "selection", "low-precision code conversion", "web based code execution", "process monitoring", "fitness function"], "training_required": false}, "tags": {"methods": ["evolutionary_algorithm", "llm_code_generation", "llm_as_heuristic", "program_synthesis", "evolution_of_heuristics"], "problems": ["algorithm_discovery", "vision_language_model_adaptation", "few_shot_learning", "multimodal_learning"], "contribution_type": ["new_method", "sota_result", "framework"], "framework_lineage": "funsearch", "specific_domain": "vision_language_model_adaptation", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Automatic Algorithm Design for Training-Free Vision-Language Model Adaptation", "short": "AutoVLMAdaptation", "class_": "algorithm_design", "properties": ["training_free", "few_shot", "multimodal", "code_generation_based"], "scale": "1, 2, 4, 8, 16 shots per class"}, "lineage": {"direct_ancestors": [{"paper": "FunSearch", "relationship": "builds on the paradigm of LLM-assisted evolutionary search for code"}], "closest_prior_work": "FunSearch", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["explore more complex code structures for VLM adaptation", "investigate alternative LLM prompting strategies for code generation", "apply to other training-free multimodal adaptation tasks", "improve initialization strategies for the evolutionary process"], "transferable_to": ["other multimodal adaptation tasks", "automated algorithm design for other machine learning problems", "zero-shot learning scenarios"], "open_weaknesses": ["dependence on good initialization", "constrained creativity of the LLM in generating novel code", "challenges in generating complex multi-function code"]}, "artifacts": {"code_url": "https://github.com/kding1225/EvoVLMA", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.74, "experiments": {"benchmarks": ["ImageNet", "Caltech101", "UCF101", "EuroSAT", "SUN397", "DTD", "Food101", "OxfordPets", "StanfordCars", "Flowers102", "FGVCAircraft", "ImageNetV2", "ImageNet-Sketch"], "baselines": ["Tip-Adapter", "APE", "GDA"], "hardware": "2x RTX 3090 GPUs", "instance_sizes": [1, 2, 4, 8, 16]}, "results": {"vs_baselines": {"Tip-Adapter": "Improved average few-shot classification accuracy by +0.57% and average domain generalization by +0.10%.", "APE": "Improved average few-shot classification accuracy by +1.25% (up to +1.91% on 8-shot) and average domain generalization by +0.91%.", "GDA": "Improved average few-shot classification accuracy by +0.38% (up to +1.01% on 1-shot) and average domain generalization by +0.87%."}, "scalability": "The found algorithms generalize well to different visual backbones (ResNet-101, ViT-B/16) and maintain computational efficiency.", "statistical_rigor": "Average results over three seeds are reported; Optuna-based hyper-parameter optimization is used.", "limitations_acknowledged": ["dependence on good initialization", "constrained creativity", "challenges in generating complex multi-function code"]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2507.15615", "arxiv_url": "https://arxiv.org/abs/2507.15615", "title": "DHEvo: Data-Algorithm Based Heuristic Evolution for Generalizable MILP Solving", "authors": ["Zhihao"], "abstract": "", "published_date": "2025-07-21", "affiliations": "Harbin Institute of Technology, Huawei Noah’s Ark Lab, Nanyang Technological University", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper directly addresses the generalization bottleneck in LLM evolutionary search (a key issue for AlgoEvo) by introducing a dynamic data-selection loop. The application to MILP diving heuristics overlaps perfectly with our EvoCut project, offering a concrete mechanism to improve our heuristic generation pipelines."}, "brief": "DHEvo introduces a 'data-algorithm co-evolution' framework that iteratively evolves heuristic code while simultaneously filtering the training instance set to retain only 'representative' instances (those where current heuristics perform well/stably). Empirical results on SCIP diving heuristics show it outperforms FunSearch and EoH by ~60% on Setcover while significantly reducing performance variance, validating the claim that dynamic data curation prevents overfitting. The key takeaway is the counter-intuitive curriculum strategy: rather than training on the hardest instances, filtering for instances with 'regular' feasible regions (high fitness) stabilizes the evolutionary search for code. We should immediately test this dynamic instance filtering in AlgoEvo to improve sample efficiency and generalization.", "methodology": {"core_method": "Data-Algorithm Co-evolution Framework (DHEvo) with LLM-based Multi-Agent Evolution System (MA-Evolution System)", "llm_role": "code_writer", "llm_model_used": "GPT-4o mini", "search_type": "improvement", "novelty_claim": "We propose a unified evolutionary computation framework based on data-algorithm co-evolution that enables better approximation of the instance distribution and enhances the representational capacity of the learned heuristics, leading to improved generalization.", "components": ["Data-algorithm co-evolution", "Multi-Agent Evolution System (MA-Evolution System)", "Prompt engineering", "Evolutionary operations (initialization, crossover, mutation, parent selection)"], "training_required": false}, "tags": {"methods": ["evolutionary_computation", "llm_code_generation", "llm_evolutionary_search", "multi_agent_system", "data_algorithm_co_evolution", "diving_heuristics", "branch_and_bound", "prompt_engineering"], "problems": ["MILP_general", "heuristic_evolution", "combinatorial_optimization", "set_cover", "combinatorial_auctions", "facility_location", "maximum_independent_set", "load_balancing", "neural_network_verification"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "eoh", "specific_domain": "MILP_general", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Mixed-Integer Linear Programming", "short": "MILP", "class_": "llm_evolutionary_search", "properties": ["integer-constrained", "generalizable", "intra-class diversity"], "scale": "up to 2000 rows/columns, 1500 bids, 400 customers, 1500 nodes"}, "lineage": {"direct_ancestors": [{"paper": "Funsearch [22]", "relationship": "extends LLM-based evolutionary algorithm design"}, {"paper": "EoH [24]", "relationship": "extends LLM-based evolutionary algorithm design"}], "closest_prior_work": "EoH [24]", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Extend data-algorithm co-evolution to other types of primal heuristics (e.g., relaxation-induced neighborhood search, local search).", "Apply the DHEvo framework to other components of MILP solvers (e.g., branching rules, cutting plane selection).", "Explore different multi-agent architectures or LLM models for the MA-Evolution System.", "Investigate theoretical guarantees for the generalization achieved by data-algorithm co-evolution."], "transferable_to": ["Other exact solvers (e.g., Gurobi, CPLEX) for MILP.", "Other combinatorial optimization problems requiring specialized heuristics (e.g., VRP, scheduling).", "Automated algorithm design for other domains beyond MILP.", "Problems where instance-specific characteristics significantly impact heuristic performance."], "open_weaknesses": ["The computational cost of the iterative data-algorithm co-evolution process.", "Scalability of the framework to extremely large-scale or highly diverse MILP problem classes.", "Potential for LLM-generated code to contain subtle bugs or inefficiencies not caught by the evaluation.", "All methods fail to obtain the optimal solution on the Load balancing dataset within the time limit."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.11, "experiments": {"benchmarks": ["cauctions", "setcover", "facilities", "indset", "LoadBalance", "NNVerify", "MIPLIB"], "baselines": ["LLM4Solver [25]", "Funsearch [22]", "HillClimb [46]", "EoH [24]", "Coefficient (SCIP)", "Distributional (SCIP)", "Farkas (SCIP)", "Pseudocost (SCIP)", "Linesearch (SCIP)", "Vectorlength (SCIP)", "L2DIVE [45]", "Default SCIP", "Tuned SCIP"], "hardware": "Intel(R) Xeon(R) CPU E5-2667 v4 @ 3.20GHz and NVIDIA A100", "instance_sizes": [100, 300, 400, 500, 1000, 1500, 2000]}, "results": {"vs_baselines": {"LLM4Solver [25]": "Outperforms on Cauctions, Setcover, Facilities, Indset by significant margins in primal gap.", "Funsearch [22]": "Outperforms on all datasets in primal gap, e.g., 77.99% vs 9.74% on Setcover.", "HillClimb [46]": "Outperforms on all datasets in primal gap, e.g., 81.55% vs 9.74% on Setcover.", "EoH [24]": "Outperforms on Cauctions, Facilities, Setcover in primal gap and solving efficiency; slightly worse on Indset primal gap.", "Hand-crafted Heuristics": "Significantly outperforms, e.g., 56.04% improvement on Indset primal gap over best manual heuristic.", "L2DIVE [45]": "Outperforms on Cauctions, Facilities, Indset primal gap; L2DIVE better on Setcover primal gap.", "Default SCIP": "Outperforms in solving time and PDI across all benchmarks.", "Tuned SCIP": "Outperforms in solving time and PDI across all benchmarks.", "Scip-off-cut": "Outperforms in solving time and PDI on LoadBalance, NNVerify, MIPLIB.", "Scip-on-cut": "Outperforms in solving time and PDI on LoadBalance, NNVerify, MIPLIB, e.g., 26.1% PDI improvement on LoadBalance."}, "scalability": "The method demonstrates robust generalization across diverse problem instances within the same problem class, achieving lowest performance variance.", "statistical_rigor": "Reports average primal gap and standard error across multiple runs (3 random seeds) for each method and dataset.", "limitations_acknowledged": ["Failure to obtain optimal solutions on the Load balancing dataset within given time limits."]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2507.14995", "arxiv_url": "https://arxiv.org/abs/2507.14995", "title": "LLM-Enhanced Multi-Agent Reinforcement Learning with Expert Workflow for Real-Time P2P Energy Trading", "authors": ["C."], "abstract": "", "published_date": "2025-07-20", "affiliations": "China Agricultural University, University of Glasgow, Guangdong University of Foreign Studies", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 6, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "The 'Differential Attention' mechanism in the critic network is a concrete architectural improvement for scaling multi-agent systems that we should test in our MASPRM/HERMES work. Additionally, the pipeline of using LLMs to generate CVXPY code to create synthetic expert data for RL is a robust neurosymbolic pattern applicable to our VRP and scheduling problems."}, "brief": "This paper proposes a neurosymbolic MARL framework for P2P energy trading where LLMs generate CVXPY optimization models to act as 'experts' for RL agents to imitate via Wasserstein distance. They introduce a 'Differential Attention' mechanism in the critic that subtracts attention maps to filter noise, enabling scalability to 100 agents where standard baselines fail. **Takeaway:** We should steal the Differential Attention architecture for our multi-agent critics to handle irrelevant interactions in large-scale optimization. The workflow of using LLMs to write the *solver* (generating reliable synthetic data) rather than the *solution* is a transferable strategy for bootstrapping RL in our OR domains.", "methodology": {"core_method": "LLM-Enhanced Multi-Agent Reinforcement Learning (MARL) with CTDE-based imitative expert MARL algorithm, using a differential multi-head attention-based critic network and Wasserstein metric for imitation", "llm_role": "heuristic_generator", "llm_model_used": "Claude-4-Opus", "search_type": "hybrid", "novelty_claim": "A novel LLM-MARL integrated framework is proposed for real-time P2P electricity markets, introducing LLMs as experts to generate personalized strategies, guiding MARL training through imitation, and an enhanced critic network with differential attention to improve policy evaluation accuracy and accelerate learning.", "components": ["LLM expert workflow", "Multi-agent actor-critic framework", "Wasserstein-2 metric", "Differential multi-head attention critic network", "Prioritized Experience Replay", "Lagrangian dual theory for constrained optimization"], "training_required": true}, "tags": {"methods": ["multi_agent_reinforcement_learning", "actor_critic", "imitation_learning", "deep_reinforcement_learning", "llm_as_expert", "llm_code_generation", "llm_in_the_loop", "llm_prompt_optimization", "attention_mechanism", "multi_head_attention", "differential_attention", "lagrangian_relaxation", "mathematical_optimization", "wasserstein_metric", "prioritized_experience_replay", "centralized_training_decentralized_execution"], "problems": ["p2p_energy_trading", "resource_allocation", "multi_agent_coordination", "voltage_control", "distribution_network_security"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": null, "specific_domain": "p2p_energy_trading", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Real-Time Peer-to-Peer Energy Trading", "short": "P2P Energy Trading", "class_": "multi_agent_coordination", "properties": ["real-time", "network-constrained", "personalized prosumers", "stochastic renewable energy", "dynamic demand", "voltage constraints"], "scale": "20-100 prosumers on IEEE 141-bus"}, "lineage": {"direct_ancestors": [{"paper": "arxiv:1707.09183", "relationship": "builds on centralized training with decentralized execution (CTDE) framework"}, {"paper": "arxiv:2410.05258", "relationship": "adapts differential transformer for critic network"}, {"paper": "Online optimal power scheduling of a microgrid via imitation learning", "relationship": "incorporates imitation learning techniques"}], "closest_prior_work": "Carbon-aware peer-to-peer energy trading in an unbalanced distribution network via a nash equilibrium discovery deep reinforcement learning approach", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Extend to larger-scale prosumer groups", "Expand external expert knowledge repositories", "Explore adaptability of diverse LLM workflow architectures in complex scenarios"], "transferable_to": ["microgrid_operation", "distribution_network_reconfiguration", "active_voltage_control", "autonomous_driving"], "open_weaknesses": ["Limited generalization capability, requiring retraining for substantial environmental changes", "LLM may generate inaccurate models/code for device types or scenarios not covered by its knowledge base", "Suboptimal expert actions from LLMs with fewer than 8B parameters in local deployment settings"]}, "artifacts": {"code_url": "https://github.com/jzk0806/P2P-llm-supplementary", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.24, "experiments": {"benchmarks": ["Modified IEEE 141-bus distribution network"], "baselines": ["MADDPG", "MAAC", "MATD3+BC", "MAGAIL", "OP"], "hardware": "NVIDIA RTX 5070Ti GPU, AMD Ryzen ThreadRipper 3970X CPU, 64GB of RAM", "instance_sizes": [20, 100]}, "results": {"vs_baselines": {"MADDPG": "26.4% lower operational cost and 71.0% lower voltage violation rate than MADDPG", "MAAC": "18.5% lower operational cost and 47.5% lower voltage violation rate than MAAC", "MATD3+BC": "11.8% lower operational cost and 34.2% lower voltage violation rate than MATD3+BC", "MAGAIL": "24.1% lower operational cost and 62.5% lower voltage violation rate than MAGAIL", "OP": "5.9% lower operational cost and 23.7% lower voltage violation rate than OP"}, "scalability": "The proposed method demonstrates strong scalability, effectively handling 100 prosumers where conventional CTDE baselines fail, due to its shared critic and Differential Attention mechanism.", "statistical_rigor": "Each algorithm is executed independently five times, and both the mean reward and the standard deviation of the resulting rewards are recorded.", "limitations_acknowledged": ["Limited generalization capability, requiring retraining for substantial environmental changes (e.g., major topology reconfiguration or new market mechanisms)", "LLM may generate inaccurate models/code for device types or scenarios not covered by its knowledge base", "Suboptimal expert actions from LLMs with fewer than 8B parameters in local deployment settings"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2507.11737", "arxiv_url": "https://arxiv.org/abs/2507.11737", "title": "Auto-Formulating Dynamic Programming Problems with Large Language Models", "authors": ["Chenyu"], "abstract": "", "published_date": "2025-07-15", "affiliations": "University of Chicago, Cornell University, Shanghai Jiao Tong University, Shanghai University of Finance and Economics, Cardinal Operations", "category": "Generative AI for OR", "relevance": {"methodological": 8, "problem": 7, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "The 'DualReflect' data synthesis pipeline—specifically the Backward Generation (Code -> Problem) component—is a concrete strategy we can adopt to generate verifiable training data for our AlgoEvo models. Additionally, the release of DP-Bench complements our OR-Bench work."}, "brief": "Zhou et al. introduce DPLM, a 7B model fine-tuned to formulate Dynamic Programming models, achieving performance comparable to o1 on their new DP-Bench. Their key contribution is 'DualReflect,' a synthetic data pipeline that combines Forward Generation (Problem→Code) for diversity with Backward Generation (Code→Problem) for correctness. **Takeaway:** We should steal the Backward Generation approach for AlgoEvo: instead of relying on noisy forward generation, we can take valid heuristics/OR code (which we have in abundance) and reverse-engineer problem descriptions to create massive, verifiable synthetic datasets for fine-tuning our code generation models. The paper proves this method is superior for 'cold-starting' small models in data-scarce domains.", "methodology": {"core_method": "DPLM, a 7B-parameter specialized model fine-tuned on Qwen-2.5-7B-Instruct using synthetic data generated by DualReflect, combining Supervised Fine-Tuning (SFT) with Reinforcement Learning (GRPO/DPO) alignment.", "llm_role": "model_formulator, code_writer, synthetic_data_generator, refinement_agent", "llm_model_used": "Qwen-2.5-7B-Instruct", "search_type": "hybrid", "novelty_claim": "This work is the first to systematically investigate the automatic formulation of DP problems using LLMs, introducing DPLM and DualReflect, a scalable data generation framework that balances accuracy and diversity.", "components": ["DualReflect synthetic data generation framework", "Forward Generation", "Backward Generation", "Reflected CoT mechanism", "RAG-based solution generator (using GPT-4o for data generation)", "Scenario Expansion", "Supervised Fine-Tuning (SFT)", "Reinforcement Learning (RL) alignment", "Group-Relative Policy Optimization (GRPO)", "Direct Preference Optimization (DPO)"], "training_required": true}, "tags": {"methods": ["dynamic_programming", "llm_code_generation", "llm_as_evaluator", "llm_fine_tuned", "llm_in_the_loop", "supervised_fine_tuning", "reinforcement_learning_alignment", "direct_preference_optimization", "group_relative_policy_optimization", "retrieval_augmented_generation", "synthetic_data_generation", "chain_of_thought", "program_synthesis", "self_instruct", "dualreflect", "forward_generation", "backward_generation", "reflected_cot"], "problems": ["dynamic_programming_formulation", "stochastic_dynamic_programming", "finite_horizon_dp", "infinite_horizon_dp", "operations_research_modeling", "inventory_management", "resource_allocation", "transportation_and_logistics", "investment_and_risk", "game_strategy", "automated_modeling", "benchmark_creation"], "contribution_type": ["new_method", "new_benchmark", "sota_result", "empirical_study", "framework"], "framework_lineage": "dplm", "specific_domain": "dynamic_programming_formulation", "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Auto-Formulating Dynamic Programming Problems", "short": "DP Auto-Formulation", "class_": "operations_research_modeling", "properties": ["stochastic_transitions", "multi_period", "sequential_decision_making", "finite_horizon", "infinite_horizon", "textbook_level"], "scale": "132 textbook-level problems"}, "lineage": {"direct_ancestors": [{"paper": "Huang et al. 2025", "relationship": "extends LLM-based optimization modeling to DP from LP/MIP"}, {"paper": "Lu et al. 2025", "relationship": "extends LLM-based optimization modeling to DP from LP/MIP"}, {"paper": "Zelikman et al. 2022", "relationship": "inspires Reflected CoT from STaR"}], "closest_prior_work": "Huang et al. 2025", "novelty_type": "new_problem"}, "extensions": {"next_steps": ["Expand initial seed data and scale up synthetic data generation for broader coverage and improved performance on easy problems.", "Explore more advanced reinforcement learning techniques or hybrid approaches for LLM alignment.", "Apply DPLM to real-world, complex dynamic programming problems beyond textbook level.", "Investigate the transferability of DualReflect to other operations research problem types, such as stochastic or robust mathematical programming."], "transferable_to": ["Auto-formulation of other optimization problem classes (e.g., stochastic linear/integer programming, robust optimization).", "Automated algorithm design and discovery for other sequential decision-making problems.", "Educational tools for teaching dynamic programming formulation and operations research modeling.", "Decision-support systems requiring automated mathematical modeling from natural language descriptions."], "open_weaknesses": ["Limited coverage of specialized domain knowledge within the current dataset, impacting performance on some easy problems.", "RL-only training approaches for smaller models often lead to instability and prohibitive computational costs.", "Backward generation inherently limits problem diversity, as it is constrained by the fixed model structures of initial seeds.", "Forward generation is inherently less accurate, requiring extensive refinement and filtering steps.", "GRPO requires significantly more wall time compared to DPO due to on-policy rollout generation and group-normalized advantage computation.", "A 7B-parameter base model may lack the capacity to reliably discover high-quality reasoning trajectories from scratch without strong SFT initialization."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": true}, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_status": "stable", "bridge_score": 0.6957, "is_bridge": true, "priority_score": 7.15, "experiments": {"benchmarks": ["DP-Bench"], "baselines": ["o1", "GPT-4o", "DeepSeek-R1", "DeepSeek-V3", "Qwen-2.5-72B-Instruct", "Qwen-2.5-32B-Instruct", "Gemma-2-9B-It", "LLama-3.1-8B-Instruct", "Qwen-2.5-7B-Instruct", "ORLM-LLaMA-3-8B"], "hardware": "eight NVIDIA H100 GPUs (80GB each), interconnected via NVLink", "instance_sizes": []}, "results": {"vs_baselines": {"DeepSeek-R1": "+0.9% macro-average, +9.5% on hard problems, -7.7% on easy problems", "GPT-4o": "+19.6% macro-average", "o1": "+7.5% macro-average", "Qwen-2.5-7B-Instruct": "+45.7% macro-average", "ORLM-LLaMA-3-8B": "+51.3% macro-average"}, "scalability": "Model performance improves with increased training data and model size up to 7B parameters, with forward generation enhancing generalization at scale and backward generation providing rapid initial gains; hard problems show logarithmic accuracy increase with more samples.", "statistical_rigor": "Pass@1, micro-average, and macro-average accuracies are reported. Inference scaling uses up to 16 independent completions, with results evaluated by pass@k and self-consistency@k (majority voting).", "limitations_acknowledged": ["limited coverage of specialized domain knowledge within our current dataset", "RL-only approaches often lead to training instability and prohibitive computational costs", "backward generation inherently limits problem diversity", "forward generation is inherently less accurate", "a 7B-parameter base model lacks the capacity to reliably discover high-quality reasoning trajectories from scratch", "GRPO requires approximately eight times more wall time due to on-policy rollout generation and group-normalized advantage computation"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2507.10614", "arxiv_url": "https://arxiv.org/abs/2507.10614", "title": "Fine-tuning Large Language Model for Automated Algorithm Design", "authors": ["Fei"], "abstract": "", "published_date": "2025-07-13", "affiliations": "City University of Hong Kong", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 10, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper directly addresses our core workflow (LLM-based evolutionary search) and provides a concrete recipe for reducing inference costs by fine-tuning smaller models (1B) to match larger ones (8B). The proposed sampling strategy for DPO is a specific, actionable technique we can immediately apply to our existing experiment logs."}, "brief": "Liu et al. introduce a fine-tuning pipeline for LLMs in automated algorithm design, utilizing a 'Diversity-Aware Rank-based' sampling strategy to construct DPO preference pairs from evolutionary search histories. By partitioning the population into ranked subsets and sampling pairs with a guaranteed quality gap (skipping adjacent tiers), they ensure training signals are both clear and diverse. Empirically, they show that a fine-tuned Llama-3.2-1B matches the performance of a base Llama-3.1-8B on ASP and CVRP tasks, effectively compressing the search capability into a much cheaper model. We should implement this sampling strategy to recycle our AlgoEvo run logs into specialized 'mutator' models, potentially allowing us to downscale to 1B/3B models for the inner search loop without losing quality.", "methodology": {"core_method": "Direct Preference Optimization (DPO) with Diversity-Aware Rank-based (DAR) Sampling for LLM fine-tuning", "llm_role": "code_writer", "llm_model_used": "Llama-3.2-1B-Instruct, Llama-3.1-8B-Instruct", "search_type": "hybrid", "novelty_claim": "Introduction of Diversity-Aware Rank-based (DAR) sampling strategy and Direct Preference Optimization (DPO) for fine-tuning LLMs specifically for algorithm design tasks.", "components": ["Direct Preference Optimization (DPO)", "Diversity-Aware Rank-based (DAR) Sampling", "Low-Rank Adaptation (LoRA)", "Evolution of Heuristics (EoH)", "FunSearch"], "training_required": true}, "tags": {"methods": ["llm_fine_tuned", "llm_in_the_loop", "llm_code_generation", "direct_preference_optimization", "diversity_aware_rank_based_sampling", "low_rank_adaptation", "evolution_of_heuristics", "funsearch"], "problems": ["automated_algorithm_design", "admissible_set_problem", "cvrp", "tsp", "heuristic_evolution"], "contribution_type": ["new_method", "empirical_study", "sota_result", "framework"], "framework_lineage": null, "specific_domain": null, "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Automated Algorithm Design", "short": "AAD", "class_": "algorithm_design", "properties": ["LLM_driven", "heuristic_synthesis", "iterative_refinement"], "scale": "n=15, w=10 (ASP); 50-100 nodes (CVRP/TSP)"}, "lineage": {"direct_ancestors": [{"paper": "Evolution of Heuristics", "relationship": "improves LLM component for"}, {"paper": "FunSearch", "relationship": "improves LLM component for"}, {"paper": "Direct Preference Optimization", "relationship": "applies and adapts"}, {"paper": "EvoTune", "relationship": "builds upon prior online fine-tuning efforts"}], "closest_prior_work": "EvoTune (ˇSurina et al., 2025)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Explore other preference learning methods or RL-based fine-tuning for AAD", "Investigate adaptive prompt generation for DPO training", "Apply the fine-tuning approach to more complex or novel algorithm design tasks", "Analyze the learned inductive biases in the fine-tuned LLMs"], "transferable_to": ["Other combinatorial optimization problems", "Automated algorithm design for continuous or black-box optimization", "General code generation tasks requiring quality and diversity"], "open_weaknesses": ["High computational cost of algorithm evaluation for data generation", "Reliance on existing AAD frameworks for initial data collection", "Generalization to entirely novel problem types not related to the training data", "The fixed prompt template might limit the LLM's adaptability"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.39, "experiments": {"benchmarks": ["Admissible Set Problem (ASP)", "Capacitated Vehicle Routing Problem (CVRP)", "Traveling Salesman Problem (TSP)"], "baselines": ["Llama-3.2-1B-Instruct (base model)", "Llama-3.1-8B-Instruct (base model)", "Top-1 Sampling", "Top-1% Sampling", "Top-5% Sampling", "Top-10% Sampling", "EoH with base LLMs", "FunSearch with base LLMs"], "hardware": "NVIDIA L20 GPUs", "instance_sizes": [15, 10, 50, 100]}, "results": {"vs_baselines": {"Base LLMs (random sampling)": "Fine-tuned Llama-3.2-1B-Instruct significantly reduced average gap and matched Llama-3.1-8B-Instruct on ASP and CVRP.", "Base LLMs in EoH/FunSearch": "Fine-tuned LLMs consistently outperformed base models, achieving faster convergence and smaller optimality gaps (e.g., FunSearch Llama-1B-FT improved ASP Top-1 from 19.48% to 9.19%).", "Top-k Sampling Strategies": "Proposed DAR sampling significantly increased pairwise distances and enhanced algorithm design capability compared to Top-k methods.", "Generalization (CVRP-100)": "Fine-tuned Llama-3.1-8B-Instruct improved performance (e.g., FunSearch Top-1 from 26.27% to 24.1%).", "Generalization (TSP-50)": "Fine-tuned Llama-3.1-8B-Instruct improved performance (e.g., FunSearch Top-1 from 15.53% to 11.85%)."}, "scalability": "Fine-tuned LLMs generalize to variant settings of the same problem and transfer to related but distinct algorithm design tasks. The fine-tuned 1B LLM achieves competitive performance to 8B LLM.", "statistical_rigor": "Mean and standard deviation aggregated over three independent runs are reported for top-1 and top-10 heuristics, and shown in convergence curves.", "limitations_acknowledged": ["Collecting sufficient algorithm data is expensive, requiring computationally expensive evaluation.", "Training data size may not be sufficient to support full fine-tuning without overfitting, necessitating LoRA.", "Improvement on some generalization tasks (e.g., TSP with EoH) is less pronounced because the base model already converges efficiently."]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2507.03605", "arxiv_url": "https://arxiv.org/abs/2507.03605", "title": "Behaviour Space Analysis of LLM-driven Meta-heuristic Discovery", "authors": ["Niki"], "abstract": "", "published_date": "2025-07-04", "affiliations": "Leiden University, University of Stirling", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 7, "problem": 8, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper moves beyond 'we beat the benchmark' to explain *why* LLM evolution succeeds or fails using novel observability tools (Code Evolution Graphs, Search Trajectory Networks). It offers a concrete, empirically backed heuristic (1+1 elitist with explicit 'simplify' prompts) that outperforms complex population methods, directly informing our AlgoEvo operator design."}, "brief": "The authors introduce a behavioral analysis framework for LLM-driven algorithm discovery, mapping the 'behavior space' of generated heuristics using Search Trajectory Networks (STNs) and Code Evolution Graphs (CEGs). Results on BBOB (5D) show that a simple 1+1 elitist strategy alternating between 'simplify code' and 'random new' prompts significantly outperforms population-based approaches, effectively balancing exploitation and exploration while preventing code bloat. The primary takeaway is the critical role of a 'simplify' mutation operator—without it, LLM-generated code tends to drift into complexity without performance gains. We should immediately adopt their visualization metrics to debug our own evolutionary search trajectories and implement their 'simplify' prompt strategy in AlgoEvo.", "methodology": {"core_method": "LLaMEA framework with 1+1 elitist evolution strategy and dual mutation prompts (code simplification and random perturbation)", "llm_role": "evolutionary_search", "llm_model_used": "o4-mini-2025-04-16", "search_type": "evolutionary_search", "novelty_claim": "First to combine dynamic search-trace analysis, static code metrics, and standardized anytime performance measures within a single benchmark for LLM-driven automated algorithm design.", "components": ["LLaMEA framework", "GPT o4-mini LLM", "mutation prompts", "1+1 elitist strategy", "population-based evolutionary strategy", "behaviour metrics", "Code Evolution Graphs", "Search Trajectory Networks"], "training_required": false}, "tags": {"methods": ["llamea", "evolution_strategy", "elitist_strategy", "llm_evolutionary_search", "evolution_of_heuristics", "program_synthesis", "llm_code_generation", "llm_as_heuristic", "search_space_analysis", "algorithm_analysis"], "problems": ["automated_algorithm_design", "black_box_optimization", "expensive_continuous_optimization", "heuristic_evolution", "operator_discovery"], "contribution_type": ["framework", "empirical_study"], "framework_lineage": "llamea", "specific_domain": "expensive_continuous_optimization", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Automated Algorithm Design for Black-Box Optimization", "short": "AAD", "class_": "algorithm_design", "properties": ["black_box", "continuous", "noiseless", "single_objective"], "scale": "5-dimensional functions"}, "lineage": {"direct_ancestors": [{"paper": "LLaMEA framework", "relationship": "builds upon and analyzes"}], "closest_prior_work": "LLaMEA framework", "novelty_type": "incremental"}, "extensions": {"next_steps": ["Explore higher-dimensional black-box optimization problems", "Test with a wider range of LLM models and architectures", "Investigate different evolutionary strategies beyond 1+1 elitist", "Apply the analysis framework to other LLM-driven algorithm design systems"], "transferable_to": ["Automated algorithm design for discrete optimization problems", "LLM-driven code generation for other programming tasks", "Analysis of search behavior in other meta-heuristic discovery systems"], "open_weaknesses": ["Limited to relatively low-dimensional problems (5D)", "Only one type of LLM (GPT o4-mini) was used", "Limited number of 5 independent runs for statistical analysis"]}, "artifacts": {"code_url": "https://doi.org/10.5281/zenodo.15675581", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_5", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.56, "experiments": {"benchmarks": ["BBOB benchmark suite (10 noiseless functions)"], "baselines": ["LLaMEA-1", "LLaMEA-2", "LLaMEA-3", "LLaMEA-5", "LLaMEA-6"], "hardware": "null", "instance_sizes": [5]}, "results": {"vs_baselines": {"LLaMEA-1": "outperformed by LLaMEA-4", "LLaMEA-2": "outperformed by LLaMEA-4", "LLaMEA-3": "outperformed by LLaMEA-4", "LLaMEA-5": "outperformed by LLaMEA-4", "LLaMEA-6": "outperformed by LLaMEA-4"}, "scalability": "Study limited to 5-dimensional problems; no explicit scalability analysis for the methodology.", "statistical_rigor": "5 independent runs with different random seeds; mean AOCC and 95% confidence intervals reported.", "limitations_acknowledged": ["limited to relatively low-dimensional problems (5D)", "one type of LLM", "limited number of 5 runs"]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2506.13131", "arxiv_url": "https://arxiv.org/abs/2506.13131", "title": "AlphaEvolve: A coding agent for scientific and algorithmic discovery", "authors": ["Alexander"], "abstract": "", "published_date": "2025-06-16", "affiliations": "Google DeepMind", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 10, "problem": 10, "inspirational": 10}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper defines the new state-of-the-art for your primary research focus (LLM evolutionary search). It transitions from single-function evolution (FunSearch) to full-codebase evolution via diffs and introduces meta-prompt evolution, directly addressing your goals of 'Evolving the Evolver' and scalability."}, "brief": "AlphaEvolve extends FunSearch by evolving entire code files (rather than single functions) using a 'search/replace' diff format and Gemini 2.0, achieving SOTA results across matrix multiplication (beating Strassen), 50+ open math problems, and Google's production scheduling. The results are exceptionally strong and verified, including deployed improvements to Google's Borg scheduler (0.7% resource recovery) and TPU circuits. The critical takeaway is the move to **diff-based full-file evolution** and **meta-prompt evolution** (evolving the prompt instructions alongside the code), which allows the system to modify architecture and logic rather than just heuristics. This is a mandatory blueprint for the next iteration of our AlgoEvo and EvoCut projects.", "methodology": {"core_method": "LLM-guided evolutionary algorithm for code superoptimization, orchestrating an autonomous pipeline of LLMs for code generation, critique, and evolution, grounded by code execution and automatic evaluation.", "llm_role": "evolutionary_search", "llm_model_used": "Gemini 2.0 Flash, Gemini 2.0 Pro", "search_type": "improvement", "novelty_claim": "AlphaEvolve leverages state-of-the-art LLMs to evolve large pieces of code that implement complex algorithms spanning multiple functions and components, going significantly beyond its predecessors in scale and generality.", "components": ["LLMs ensemble", "Prompt sampler", "Evaluators pool", "Program database", "Distributed Controller Loop", "evolutionary database (inspired by MAP elites and island-based population models)", "evaluation cascade (hypothesis testing)", "LLM-generated feedback", "parallelized evaluation"], "training_required": false}, "tags": {"methods": ["llm_evolutionary_search", "evolutionary_algorithm", "code_generation", "program_synthesis", "llm_as_heuristic", "llm_as_evaluator", "automated_evaluation", "map_elites", "island_model_ea", "llm_ensemble", "alphaevolve"], "problems": ["scientific_discovery", "algorithm_discovery", "matrix_multiplication", "mathematical_discovery", "combinatorial_packing", "geometric_problems", "resource_allocation", "scheduling", "kernel_optimization", "circuit_design", "heuristic_evolution", "operator_discovery"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "alphaevolve", "specific_domain": null, "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Scientific and Algorithmic Discovery via Code Superoptimization", "short": "AlphaEvolve", "class_": "llm_evolutionary_search", "properties": ["code_evolution", "automated_evaluation", "multi_objective", "provably_correct_algorithms", "real_world_impact"], "scale": "up to hundreds of lines of code, 50+ mathematical problems, matrix sizes up to ⟨6,6,6⟩, Google-scale computational stacks"}, "lineage": {"direct_ancestors": [{"paper": "Fawzi et al. [26] (AlphaTensor)", "relationship": "extends the approach of AlphaTensor for matrix multiplication to a broader range of problems and code structures"}, {"paper": "MAP-Elites", "relationship": "incorporates principles from"}, {"paper": "island-based population models", "relationship": "incorporates principles from"}], "closest_prior_work": "AlphaTensor", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Address memory limitations for larger problem instances", "Optimize LLM-provided evaluation of ideas for better efficiency and accuracy", "Extend to tasks requiring manual experimentation or human-in-the-loop feedback", "Explore alternative LLM models or advanced ensemble strategies"], "transferable_to": ["Other scientific domains requiring code-based discovery (e.g., physics, chemistry)", "More complex algorithm design problems beyond current scope", "Hardware design for different architectures or specialized circuits", "Optimization of other critical software kernels and system components"], "open_weaknesses": ["Requires automated evaluators for all target problems", "Tasks requiring manual experimentation are currently out of scope", "Memory limitations encountered for large matrix multiplication instances", "LLM-provided evaluation of ideas is not yet fully optimized"]}, "artifacts": {"code_url": "https://colab.research.google.com/github/google-deepmind/alphaevolve_results/blob/master/mathematical_results.ipynb", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.5958, "is_bridge": true, "priority_score": 9.44, "experiments": {"benchmarks": ["Tensor decomposition for matrix multiplication (54 sizes ⟨m,n,p⟩)", "Autocorrelation inequalities", "Uncertainty principles", "Erdős’s minimum overlap problem", "Kissing number problem (11 dimensions)", "Packing problems (hexagons, points, circles)", "Heilbronn problem (triangles, convex regions)", "Sums and differences of finite sets", "Google’s cluster management system (Borg)", "Gemini kernel engineering (Pallas kernels for matrix multiplication)", "TPU arithmetic circuit design (Verilog)", "FlashAttention kernel (XLA-generated IR)"], "baselines": ["Strassen (1969)", "Fawzi et al. [26] (AlphaTensor)", "Hopcroft and Kerr [42]", "Smirnov [93, 91, 92]", "Kauers and Moosbauer [47, 48]", "Laderman [52]", "Moosbauer and Poole [72]", "Previous SOTA for mathematical problems [17, 70, 104, 33, 39, 107, 31, 29]", "Existing production heuristic (Google data centers)", "Expert-designed heuristic (Gemini kernel)", "Highly optimized Verilog implementation (TPU circuit)", "Compiler-generated and highly optimized XLA IR (FlashAttention)"], "hardware": "single GPU accelerator (for evaluation), TPU accelerators (for Gemini kernel), evaluation cluster (for parallelized evaluation)", "instance_sizes": [2, 3, 4, 5, 6, 11, 12, 13, 14, 16, 21, 26, 32, 50, 400, 600]}, "results": {"vs_baselines": {"Strassen (1969)": "4x4 complex-valued matrices: 48 scalar multiplications (vs 49)", "Fawzi et al. [26]": "4x4 matrices: 48 scalar multiplications (vs 47 for field with 2 elements, AlphaEvolve is for any field)", "Previous SOTA (mathematics)": "surpassed on ~20% of 50+ problems, matched on ~75%", "Erdős’s minimum overlap problem [40]": "0.380924 (vs 0.380927)", "Kissing number (11D) [31]": "593 (vs 592)", "Packing 11 unit hexagons [29]": "3.931 side length (vs 3.943)", "Packing 12 unit hexagons [29]": "3.942 side length (vs 4.0)", "Min/Max distance (16 pts, 2D) [29]": "sqrt(12.889266112) (vs sqrt(12.890))", "Min/Max distance (14 pts, 3D) [29]": "sqrt(4.165849767) (vs sqrt(4.168))", "Heilbronn (triangle, 11 pts) [29]": ">=0.0365 area (vs 0.036)", "Heilbronn (convex, 13 pts) [29]": ">=0.0309 area (vs 0.0306)", "Heilbronn (convex, 14 pts) [29]": ">=0.0278 area (vs 0.0277)", "Packing circles (26, unit square) [29]": ">=2.635 sum of radii (vs 2.634)", "Packing circles (32, unit square) [29]": ">=2.937 sum of radii (vs 2.936)", "Packing circles (21, perimeter 4) [29]": ">=2.3658 sum of radii (vs 2.364)", "Existing production heuristic (Google data centers)": "recovers 0.7% fleet-wide compute resources", "Expert-designed heuristic (Gemini kernel)": "23% kernel speedup, 1% reduction in Gemini training time", "Highly optimized Verilog (TPU circuit)": "simple code rewrite removing unnecessary bits", "XLA-generated IR (FlashAttention)": "kernel sped up by 32%, pre/postprocessing by 15%"}, "scalability": "AlphaEvolve goes significantly beyond its predecessors in scale and generality, evolving up to hundreds of lines of code and tackling a diverse range of problems. However, memory limitations were encountered for matrix multiplication beyond ⟨5,5,5⟩.", "statistical_rigor": "Performance is measured as the best (lowest) rank achieved on each target as well as the fraction of seeds that achieved this rank. Ablation results are averaged over three independent runs, initialized with different random seeds, with shades indicating intra-target standard deviation.", "limitations_acknowledged": ["Requires automated evaluator for problems", "Tasks requiring manual experimentation are out of scope", "Memory limitations encountered for large matrix multiplication instances (beyond ⟨5,5,5⟩)", "LLM-provided evaluation of ideas not yet optimized for"]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2502.13575", "arxiv_url": "https://arxiv.org/abs/2502.13575", "title": "ETS: Efficient Tree Search for Inference-Time Scaling", "authors": ["Coleman"], "abstract": "", "published_date": "2025-06-11", "affiliations": "University of California, Berkeley, ICSI, LBNL", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 7, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper sits exactly at the intersection of our two core competencies: Operations Research (ILP) and LLM Search. It replaces heuristic pruning (top-k) with a formal optimization problem (ILP) to manage memory constraints, which is a superior paradigm for our AlgoEvo and serving optimization work."}, "brief": "ETS formulates the tree search pruning step as a lightweight Integer Linear Program (ILP) that maximizes the reward of retained nodes while penalizing total KV cache size and enforcing semantic diversity via clustering. Unlike standard beam search or REBASE, it explicitly optimizes the trade-off between memory consumption (KV sharing) and exploration coverage. The authors demonstrate a 1.8x reduction in KV cache size and 1.4x throughput increase on MATH500 with minimal accuracy loss. We should steal the 'ILP-in-the-loop' mechanism for population management in our evolutionary search frameworks to optimize hardware utilization dynamically.", "methodology": {"core_method": "Efficient Tree Search (ETS) using a linear programming cost model with KV cache sharing penalty and semantic coverage term", "llm_role": "candidate_generator, process_reward_model, search_guidance", "llm_model_used": "Llemma-34B, Llemma-Reward-34B, Mistral-7B-SFT, Mistral-7B PRM, Llama-3.2-1B-Instruct, Llama3.1-8B-PRM-Deepseek-Data", "search_type": "hybrid", "novelty_claim": "ETS promotes KV sharing by pruning redundant trajectories while maintaining necessary diverse trajectories, incorporating a linear programming cost model with a semantic coverage term.", "components": ["Linear Programming cost model", "semantic coverage term", "REBASE sampling", "Pulp optimization library", "CBC solver", "BERT embedding model", "hierarchical agglomerative clustering"], "training_required": false}, "tags": {"methods": ["tree_search", "llm_in_the_loop", "llm_as_heuristic", "llm_as_evaluator", "linear_programming", "integer_linear_programming", "clustering", "hierarchical_agglomerative_clustering", "bert_embeddings", "rebase", "process_reward_model", "kv_cache_optimization", "inference_optimization", "memory_optimization"], "problems": ["llm_inference_optimization", "llm_serving_optimization", "mathematical_reasoning"], "contribution_type": ["new_method", "sota_result", "empirical_study", "framework"], "framework_lineage": "rebase", "specific_domain": "mathematical_reasoning", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Optimizing LLM Inference Tree Search for Efficiency and Accuracy", "short": "LLM Tree Search Optimization", "class_": "llm_inference_optimization", "properties": ["memory_bound", "inference_time_scaling", "diversity_vs_efficiency_tradeoff", "process_reward_model_guided"], "scale": "search widths of 16, 64, 256"}, "lineage": {"direct_ancestors": [{"paper": "REBASE", "relationship": "extends sampling strategy and improves efficiency"}, {"paper": "Beam Search", "relationship": "improves upon efficiency and diversity trade-off"}, {"paper": "Diverse Verifier Tree Search (DVTS)", "relationship": "improves upon efficiency and diversity trade-off"}], "closest_prior_work": "REBASE (Wu et al. [2024])", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["leverage instruction_tuned_models for reward signal instead of trained PRM", "extend to other problem_solving_tasks beyond math"], "transferable_to": ["code_generation", "general_llm_problem_solving"], "open_weaknesses": ["requires a trained process_reward_model (PRM)", "evaluation limited to mathematical_reasoning_tasks"]}, "artifacts": {"code_url": "https://github.com/SqueezeAILab/ETS", "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_3", "front_status": "growing", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.76, "experiments": {"benchmarks": ["MATH500", "GSM8K"], "baselines": ["REBASE", "Beam Search", "Diverse Verifier Tree Search (DVTS)"], "hardware": "NVIDIA H100 NVL GPUs", "instance_sizes": [16, 64, 256]}, "results": {"vs_baselines": {"REBASE": "up to 1.8x KV cache reduction, 1.4x throughput increase, with minimal accuracy degradation (e.g., +0.8% on Llemma-34B MATH500 width 256)", "Beam Search": "Improved accuracy-efficiency trade-off compared to standard beam search", "DVTS": "Improved accuracy-efficiency trade-off compared to DVTS"}, "scalability": "Benefits are consistent across different model families and datasets, and for various search widths, achieving 1.4x increased throughput.", "statistical_rigor": "Profiled throughput on 100 samples from MATH500 test set, averaged time across all steps in search process, using 8 parallel threads (for profiling) and 32 threads (for batched serving context).", "limitations_acknowledged": ["requires the use of a trained process reward model (PRM)", "evaluation results constrained to mathematical reasoning tasks"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2410.15639", "arxiv_url": "https://arxiv.org/abs/2410.15639", "title": "Can Large Language Models Invent Algorithms to Improve Themselves?: Algorithm Discovery for Recursive Self-Improvement through Reinforcement Learning", "authors": ["Yoichi"], "abstract": "", "published_date": "2025-06-10", "affiliations": "NEC Corporation", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 8, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper successfully implements 'RL-infused evolution' by using Direct Preference Optimization (DPO) to fine-tune the code generator based on search results. This provides a concrete, proven mechanism to achieve our 'Evolving the Evolver' objective in AlgoEvo, moving beyond static prompting."}, "brief": "Ishibashi et al. propose 'Self-Developing,' a framework where an LLM generates Python code for model merging, evaluates the results, and uses the performance data to fine-tune the generator via DPO in a recursive loop. The results are empirically strong, outperforming human-designed baselines (Task Arithmetic) by 4.3% on GSM8k and demonstrating that the generator explicitly learns better strategies over iterations. **Key Takeaway:** We can replace the static mutation operators in our evolutionary search with a DPO-trained model that learns from the search history—effectively implementing 'learning to search.' This is a direct, actionable upgrade for our AlgoEvo and AlphaEvolve pipelines.", "methodology": {"core_method": "Direct Preference Optimization (DPO) for iterative refinement of an algorithm-generating LLM", "llm_role": "evolutionary_search", "llm_model_used": "openchat-3.5-1210", "search_type": "improvement", "novelty_claim": "Self-Developing is a framework enabling LLMs to autonomously discover, implement, and refine their own improvement algorithms through an iterative cycle of generating algorithmic candidates as executable code, evaluating effectiveness, and using DPO to recursively improve strategies.", "components": ["Algorithm Factory (LLM)", "Algorithm Generation (Python code)", "Algorithm Evaluation (task scores)", "Direct Preference Optimization (DPO)", "Temperature Decay", "LoRA fine-tuning"], "training_required": true}, "tags": {"methods": ["rl_dpo", "llm_code_generation", "llm_as_evaluator", "llm_fine_tuned", "llm_evolutionary_search", "program_synthesis", "self_improving_search"], "problems": ["algorithm_discovery", "mathematical_reasoning", "model_merging"], "contribution_type": ["new_method", "framework", "empirical_study", "sota_result"], "framework_lineage": "self_developing", "specific_domain": "mathematical_reasoning", "llm_coupling": "rl_trained"}, "problem": {"formal_name": "Algorithm Discovery for Recursive Self-Improvement through Reinforcement Learning", "short": "LLM Algorithm Discovery", "class_": "algorithm_design", "properties": ["recursive_self_improvement", "model_merging", "code_generation", "llm_driven", "meta_learning"], "scale": "7B parameter models, 3 candidate models"}, "lineage": {"direct_ancestors": [{"paper": "Direct Preference Optimization", "relationship": "applies and extends"}], "closest_prior_work": "FunSearch", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Validate discovered algorithms in broader domains beyond mathematical reasoning", "Scale the framework to larger LLMs and more complex tasks", "Investigate theoretical underpinnings of discovered algorithms", "Explore different DPO variants or other RL methods for algorithm discovery"], "transferable_to": ["Other LLM-based tasks requiring self-improvement (e.g., code generation, creative writing)", "Different LLM architectures and sizes", "Algorithm discovery for other computational problems (e.g., combinatorial optimization, scientific discovery)"], "open_weaknesses": ["Limited to mathematical reasoning tasks due to computational constraints", "Lack of explicit variance or significance tests for results", "Generalizability to vastly different domains not fully explored", "Computational cost of the self-improvement loop"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.31, "experiments": {"benchmarks": ["GSM8k", "MATH"], "baselines": ["Task Arithmetic (Ilharco et al., 2023)", "TIES Merging (Yadav et al., 2023)", "Model Stock (Jang et al., 2024)"], "hardware": "NVIDIA A100 GPUs", "instance_sizes": [3]}, "results": {"vs_baselines": {"Task Arithmetic (original)": "+4.2% on GSM8k (76.1% vs 71.9%), comparable on MATH (8.5% vs 8.5%)", "TIES Merge (original)": "+4.3% on GSM8k (76.1% vs 71.8%), +0.1% on MATH (8.5% vs 8.4%)", "Model Stock (original)": "+36.6% on GSM8k (76.1% vs 39.5%), +2.4% on MATH (8.5% vs 6.1%)", "Task Arithmetic (transfer)": "+7.4% on GSM8k (78.8% vs 71.4%), +108% on MATH (2.5% vs 1.2%)"}, "scalability": "Discovered algorithms demonstrate strong generalization and transferability to out-of-domain models without re-optimization, maintaining high performance.", "statistical_rigor": "Top three performances across all iterations are displayed, with a single evaluation on the test set for the top 15 models selected on the development set. No explicit variance or significance tests are reported.", "limitations_acknowledged": ["Focus solely on mathematical reasoning tasks", "Computational resource constraints prevented experiments in other domains", "Need for validation across broader domains"]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2506.07972", "arxiv_url": "https://arxiv.org/abs/2506.07972", "title": "HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization", "authors": ["Hongzheng"], "abstract": "", "published_date": "2025-06-09", "affiliations": "Cornell University, Harvard University, NVIDIA", "category": "Generative AI for OR", "relevance": {"methodological": 5, "problem": 9, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper establishes a rigorous benchmark (HeuriGym) for LLM-based heuristic generation on problems we care about (PDPTW, Scheduling). Crucially, it provides empirical evidence that current open-source evolutionary methods (ReEvo, EoH) fail on these complex tasks, creating a perfect opening for our AlgoEvo work."}, "brief": "The authors introduce HeuriGym, a benchmark suite of 9 hard combinatorial optimization problems (including PDPTW, EDA scheduling, and routing) coupled with an agentic evaluation loop. Results are backed by extensive experiments showing that SOTA LLMs saturate at ~60% of expert performance and, significantly, that existing evolutionary frameworks (ReEvo, EoH) perform *worse* than simple prompting on these large-context tasks (300+ lines of code). The key takeaway is the failure mode of current evolutionary methods: they cannot handle the context fragmentation and feedback integration required for complex heuristic design. We should immediately adopt this benchmark to demonstrate AlgoEvo's superiority, as the current baselines are weak and the problem set aligns perfectly with our focus.", "methodology": {"core_method": "Agentic framework for evaluating and iteratively refining LLM-generated heuristic algorithms via code execution feedback", "llm_role": "heuristic_generator", "llm_model_used": null, "search_type": "improvement", "novelty_claim": "HeuriGym is an agentic framework for evaluating LLM-crafted heuristics in combinatorial optimization, featuring an interactive loop for solution generation, execution feedback, and iterative refinement.", "components": ["LLM agent", "problem description", "compiler/interpreter", "verifier", "evaluator", "feedback loop", "demonstration set", "evaluation set"], "training_required": false}, "tags": {"methods": ["llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "in_context_learning", "agentic_framework", "iterative_refinement", "tool_use", "dynamic_programming"], "problems": ["operator_scheduling", "technology_mapping", "global_routing", "e_graph_extraction", "intra_operator_parallelism", "protein_sequence_design", "mendelian_error_detection", "airline_crew_pairing", "pickup_and_delivery_with_time_windows"], "contribution_type": ["new_benchmark", "framework", "empirical_study", "new_method"], "framework_lineage": null, "specific_domain": null, "llm_coupling": null}, "problem": {"formal_name": "Heuristic Generation for Combinatorial Optimization Problems", "short": "Heuristic Generation", "class_": "heuristic_algorithm_design", "properties": ["agentic", "iterative_refinement", "tool_use", "multi_step_planning", "instruction_fidelity", "large_solution_space", "NP-hard"], "scale": "dozens of large-scale instances (up to 218 total), with search spaces up to 10^65000"}, "lineage": {"direct_ancestors": [{"paper": "ALE-Bench (Imajuku et al., 2025)", "relationship": "improves upon evaluation methodology of"}, {"paper": "CO-Bench (Sun et al., 2025)", "relationship": "improves upon evaluation methodology of"}, {"paper": "HumanEval (Chen et al., 2021b)", "relationship": "addresses limitations of"}], "closest_prior_work": "ALE-Bench (Imajuku et al., 2025)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["extend benchmark with more combinatorial optimization problems and larger instances", "integrate advanced prompting and multi-agent strategies", "incorporate test_time search strategies like best_of_N sampling and evolutionary algorithms", "improve C++ integration for performance"], "transferable_to": ["automated_algorithm_design_for_computationally_hard_problems", "benchmarking_llms_for_general_agentic_reasoning_and_tool_use", "evaluation_of_llms_in_scientific_discovery_and_engineering_design"], "open_weaknesses": ["llm_hallucinations_and_instruction_following_failures", "llm_inefficiency_in_navigating_large_search_spaces_for_high_quality_solutions", "performance_overheads_of_python_execution_and_challenging_c++_integration", "reliance_on_proxy_metrics_that_may_not_fully_reflect_real_world_performance"]}, "artifacts": {"code_url": "https://github.com/cornell-zhang/heurigym", "models_released": false, "new_benchmark": true}, "front_id": "generative_ai_for_or_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.6538, "is_bridge": true, "priority_score": 6.59, "experiments": {"benchmarks": ["Operator scheduling", "Technology mapping", "Global routing", "E-graph extraction", "Intra-operator parallelism", "Protein sequence design", "Mendelian error detection", "Airline crew pairing", "Pickup and delivery problem with time windows"], "baselines": ["Expert baseline", "HSEvo (Dat et al., 2025)", "ReEvo (Ye et al., 2024)", "EoH (Liu et al., 2024b)"], "hardware": "CPU server with 8 CPU cores per instance for Python execution; preliminary C++ experiments also conducted.", "instance_sizes": [24, 31, 24, 23, 28, 24, 20, 14, 30]}, "results": {"vs_baselines": {"Expert baseline": "Top LLMs (e.g., Gemini-2.5-Pro) achieve a QYI of ~0.62, significantly below the expert baseline of 1.0.", "HSEvo": "QYI of 0.4491, worse than Gemini-2.5-Pro's 0.6170.", "ReEvo": "QYI of 0.4486, worse than Gemini-2.5-Pro's 0.6170.", "EoH": "QYI of 0.4492, worse than Gemini-2.5-Pro's 0.6170."}, "scalability": "LLM-generated heuristics show limited effectiveness on large-scale instances, achieving significantly lower quality than expert solutions, and benefit from more informative demonstrations for complex tasks.", "statistical_rigor": "Experiments use a fixed generation temperature of 0 for deterministic outputs, evaluate multi-round performance with SOLVEs@i (1, 5, 10 iterations), and report weighted QYI aggregated across tasks. Ablation studies on temperature, few-shot demonstrations, and feedback rounds are performed.", "limitations_acknowledged": ["LLM correctness (hallucinations, instruction-following) and performance (navigating large search spaces)", "Python execution incurs overheads; C++ integration is challenging", "Current agentic pipeline uses standard configuration, not advanced prompting or multi-agent strategies", "Proxy metrics may not fully reflect real-world performance, especially in scientific/engineering domains"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2506.07759", "arxiv_url": "https://arxiv.org/abs/2506.07759", "title": "REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models", "authors": ["Diego"], "abstract": "", "published_date": "2025-06-09", "affiliations": "Vicomtech Foundation, University of the Basque Country, Universidad EAFIT, HiTZ Basque Center for Language Technology", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 6, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "While the FJSSP results lag behind CP solvers (12% vs 1% gap), the 'Clustering Reflection' mechanism is a transferable innovation for our AlgoEvo pipeline. It solves the context-window bottleneck in population-based reflection by grouping phenotypically similar solutions, ensuring the LLM receives structured, diverse feedback rather than a noisy list of programs."}, "brief": "Forniés-Tabuenca et al. propose REMoH, an LLM-driven evolutionary framework for multi-objective FJSSP that uses K-Means to cluster the population by objective performance before generating reflections. While their optimality gaps (~12%) trail behind state-of-the-art CP solvers (~1.5%), the ablation study confirms that their reflection mechanism significantly improves Pareto front diversity (Hypervolume). **The killer feature is the phenotypic clustering step:** instead of reflecting on a random or elitist subset, they group solutions by trade-offs (e.g., 'low makespan' vs 'balanced') to generate targeted prompts. We should implement this clustering-based context construction in AlgoEvo to improve diversity maintenance in multi-objective search without exploding token costs.", "methodology": {"core_method": "Hybrid framework integrating NSGA-II with LLM-based heuristic generation and a reflection mechanism", "llm_role": "evolutionary_search", "llm_model_used": "Gemini 2.0 Flash", "search_type": "hybrid", "novelty_claim": "A novel framework integrating NSGA-II with LLM-based heuristic generation, featuring a reflection mechanism that uses clustering and search-space reflection to guide the creation of diverse, high-quality heuristics, improving convergence and maintaining solution diversity.", "components": ["NSGA-II", "LLM-generated heuristic operators", "Reflection mechanism", "K-Means clustering", "Silhouette method", "Reflective crossover", "Elitist mutation"], "training_required": false}, "tags": {"methods": ["nsga_ii", "llm_as_heuristic", "llm_code_generation", "k_means_clustering", "evolutionary_algorithm", "metaheuristics", "crossover", "mutation", "reflection_mechanism", "evolution_of_heuristics", "program_synthesis"], "problems": ["flexible_job_shop_scheduling", "multi_objective_scheduling", "makespan_minimization", "workload_balancing", "operation_separation", "sequence_dependent_setup_times"], "contribution_type": ["new_method", "framework", "empirical_study", "sota_result"], "framework_lineage": "eoh", "specific_domain": "flexible_job_shop_scheduling", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Flexible Job Shop Scheduling Problem", "short": "FJSSP", "class_": "scheduling", "properties": ["multi_objective", "flexible", "sequence_dependent_setup_times", "non_linear"], "scale": "10-20 jobs, 10-15 machines"}, "lineage": {"direct_ancestors": [{"paper": "NSGA-II", "relationship": "integrates with"}, {"paper": "Evolution of Heuristics (EoH) [9]", "relationship": "builds on principles of"}, {"paper": "ReEvo [34]", "relationship": "builds on principles of"}, {"paper": "HSEvo [35]", "relationship": "draws strategy for population initialization from"}], "closest_prior_work": "Evolution of Heuristics (EoH) [9]", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["explore the evolution of full metaheuristic architectures", "extend to high-level algorithm generation", "improve computational efficiency of LLM interactions"], "transferable_to": ["other multi_objective_optimization_problems", "other scheduling_problems", "combinatorial_optimization"], "open_weaknesses": ["computational_cost_of_llm_inference", "premature_convergence_of_some_llms", "llm_biases_and_inaccuracies"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.6417, "is_bridge": true, "priority_score": 6.43, "experiments": {"benchmarks": ["Brandimarte", "Barnes", "Dauzere"], "baselines": ["Mixed-Integer Linear Programming (MILP)", "Constraint Programming (CP) with Google OR-Tools CP-SAT", "Constraint Programming (CP) with IBM CPLEX CP Optimizer", "Greedy dispatching rule (DR)", "Ho et al. [49] (RL1)", "Echeverria et al. [50] (RL2)"], "hardware": "8-core Intel i7 processor, 16 GB of RAM, no dedicated GPU", "instance_sizes": [10, 15, 20]}, "results": {"vs_baselines": {"MILPmb": "REMoH +6.14% GAP on makespan", "DCmb": "REMoH +11.17% GAP on makespan", "ORmb": "REMoH +10.87% GAP on makespan", "DR": "REMoH -42.18% GAP on makespan", "RL1": "REMoH +4.55% GAP on makespan (Mk01-Mk10)", "RL2": "REMoH +4.03% GAP on makespan (Mk01-Mk10)", "MILPbm": "REMoH +12.11% GAP on workload balance", "DCbm": "REMoH +12.11% GAP on workload balance", "ORbm": "REMoH +12.11% GAP on workload balance", "DR_WB": "REMoH -15.64% GAP on workload balance"}, "scalability": "REMoH-generated heuristics become more relevant as instance complexity increases, showing stable performance across all instances.", "statistical_rigor": "Each metric is obtained by averaging the results from three independent runs.", "limitations_acknowledged": ["Computational cost of the chosen LLM (Gemini 2.0 Flash) for best solution quality."]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2506.06052", "arxiv_url": "https://arxiv.org/abs/2506.06052", "title": "DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems", "authors": ["Kostis"], "abstract": "", "published_date": "2025-06-06", "affiliations": "KU Leuven, University of Western Macedonia", "category": "Generative AI for OR", "relevance": {"methodological": 5, "problem": 8, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": true, "team_discussion": true, "reasoning": "Directly relevant to our 'OR-Bench' and 'AlgoEvo' projects. It quantifies the danger of evaluating LLM-generated code on single instances (showing massive overfitting) and provides strong evidence that Python-based CP frameworks (CPMpy/OR-Tools) significantly outperform MiniZinc for LLM generation, contradicting some standard assumptions."}, "brief": "This paper introduces DCP-Bench-Open, a benchmark of 164 discrete combinatorial problems, to evaluate LLMs on translating natural language into constraint models (CPMpy, MiniZinc, OR-Tools). The results are rigorous and highlight a critical failure mode: LLMs overfit to the specific data values in the prompt's example instance, causing a ~30% performance drop when evaluated on hidden instances (Multi-Instance Accuracy). Crucially for our pipeline design, they find that Retrieval-Augmented In-Context Learning (RAICL) is ineffective or harmful compared to simply including library documentation in the system prompt. We should adopt their 'Multi-Instance Accuracy' metric immediately for OR-Bench and switch any MiniZinc generation efforts to Python-based frameworks like CPMpy or OR-Tools, which LLMs handle much better.", "methodology": {"core_method": "LLM-driven constraint model generation", "llm_role": "code_writer, decomposition_guide, evaluator", "llm_model_used": "gpt-5.1-2025-11-13, gpt-oss-120B, DeepSeek-V3.2, Qwen3-Coder-480B-A35B, Qwen3 235B A22B Instruct 2507, Kimi K2 Instruct, Cogito v2.1 671B", "search_type": "hybrid", "novelty_claim": "This work introduces DCP-Bench-Open, a novel benchmark for evaluating LLM-driven constraint modelling, and systematically evaluates prompt-based and inference-time compute methods across diverse LLMs and modelling frameworks.", "components": ["System Prompt (Basic, Guidelines, Documentation)", "Retrieval-Augmented In-Context Learning (RAICL)", "Reasoning (Low Reasoning Effort/specialized variants)", "Repeated Sampling with Solution Majority Voting", "Self-Verification"], "training_required": false}, "tags": {"methods": ["llm_code_generation", "llm_prompt_optimization", "llm_in_the_loop", "retrieval_augmented_in_context_learning", "repeated_sampling", "solution_majority_voting", "self_verification", "constraint_programming", "cp_sat", "minizinc", "cpmpy"], "problems": ["constraint_modelling", "program_synthesis", "discrete_combinatorial_problems"], "contribution_type": ["new_benchmark", "empirical_study", "sota_result", "framework"], "framework_lineage": null, "specific_domain": "constraint_modelling", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Constraint Modelling of Discrete Combinatorial Problems", "short": "LLM-driven Constraint Modelling", "class_": "program_synthesis", "properties": ["discrete", "combinatorial", "NP-hard", "optimization", "satisfaction", "multi_instance", "multi_solution"], "scale": "1-2463 constraints, 2-716 variables"}, "lineage": {"direct_ancestors": [{"paper": "Michailidis et al. (2025b)", "relationship": "extends benchmark from"}, {"paper": "Michailidis et al. (2024)", "relationship": "builds on retrieval-augmented in-context learning from"}], "closest_prior_work": "Michailidis et al. (2025b)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["evaluate newer llms", "evaluate additional modelling or solving frameworks (e.g., ILP, SAT)", "explore multi-turn interactions for modelling with llms", "evaluate modelling efficiency across multiple instances"], "transferable_to": ["integer_linear_programming", "boolean_satisfiability", "mixed_integer_programming", "planning_domain_definition_language"], "open_weaknesses": ["llms overfit to default data instances", "lack of large-scale industrial problems in benchmarks", "limited multi-turn interaction capabilities"]}, "artifacts": {"code_url": "https://github.com/kostis-init/CP-Bench", "models_released": false, "new_benchmark": true}, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_status": "stable", "bridge_score": 0.7019, "is_bridge": true, "priority_score": 6.02, "experiments": {"benchmarks": ["DCP-Bench-Open", "CSPLib", "CPMpy examples", "Håkan Kjellerstrand repository", "Modelling course-based set of problems", "ComplexOR dataset"], "baselines": ["gpt-5.1-2025-11-13 (Baseline)", "gpt-oss-120B (Baseline)", "DeepSeek-V3.2 (Baseline)", "Qwen3-Coder-480B-A35B (Baseline)", "Qwen3 235B A22B Instruct 2507 (Baseline)", "Kimi K2 Instruct (Baseline)", "Cogito v2.1 671B (Baseline)"], "hardware": "Ubuntu 24.04.3 system with 32GB RAM and an Intel Core Ultra 7 165Hx22 processor", "instance_sizes": [2, 716, 1, 2463]}, "results": {"vs_baselines": {"CPMpy (Level 3)": "up to 70.1% (gpt-5.1, DeepSeek-V3.2)", "OR-Tools (Level 3)": "up to 75.0% (gpt-oss-120b)", "MiniZinc (Level 3)": "up to 57.3% (Qwen3-Coder)", "RAICL": "ineffective or degraded performance for most LLMs", "Reasoning": "mixed results, gpt-5.1 gained substantially, others dropped", "Sampling": "10%+ accuracy increase across all LLMs", "Self-Verification": "10%+ accuracy increase across all LLMs", "Sampling & Self-Verification (combined)": "up to 91% accuracy (gpt-5.1 on CPMpy)"}, "scalability": "LLMs frequently overfit to the specific values or structure of the default data instance, leading to performance decline on hidden test instances (e.g., gpt-5.1 and Kimi-K2-I show 17.4% drop in strict robustness from SIA to MIA).", "statistical_rigor": "Reproducibility seed set to 42 for all API calls; temperature parameter τ set to 0 for deterministic outputs, except τ=0.8 for repeated sampling.", "limitations_acknowledged": ["LLMs can over-encode the example instance given in the prompt, leading to incorrect results on other instances of the same problem type.", "Large-scale industrial problems typically involve more data, many constraints and objectives, and elaborate descriptions, though these are rarely publicly available.", "Future work could also explore multi-turn interactions for modelling with LLMs."]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2502.00722", "arxiv_url": "https://arxiv.org/abs/2502.00722", "title": "Demystifying Cost-Efficiency in LLM Serving over Heterogeneous GPUs", "authors": ["Youhe"], "abstract": "", "published_date": "2025-06-05", "affiliations": "University of Cambridge, ETH Zurich, Peking University, The Hong Kong University of Science and Technology, Purdue University", "category": "OR for Generative AI", "relevance": {"methodological": 5, "problem": 9, "inspirational": 6}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper addresses the exact problem of our 'GPUSched' project (OR formulations for LLM inference) using a very similar methodology (MILP). It serves as a critical baseline and source of heuristics for configuration pruning."}, "brief": "Jiang et al. formulate LLM serving on heterogeneous clouds as a Mixed-Integer Linear Programming (MILP) problem, co-optimizing GPU rental composition, parallelism strategies (TP/PP), and workload routing. They demonstrate ~25% throughput gains over SOTA systems (Helix, HexGen) using vLLM benchmarks, validating the approach with strong empirical ablations. For our **GPUSched** project, the key takeaway is their solver strategy: pre-generating valid configurations to linearize the problem and using a binary search wrapper on the makespan to avoid direct minimization overhead. We should adopt their heuristics for pruning the configuration space (e.g., restricting TP to intra-node) to improve our own solver times.", "methodology": {"core_method": "Mixed-Integer Linear Programming (MILP) for scheduling", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "A novel MILP-based scheduling algorithm that co-optimizes GPU composition, deployment configurations, and workload assignments for cost-efficient LLM serving under real-time cloud GPU availability and budget constraints.", "components": ["Mixed-Integer Linear Programming (MILP) formulation", "Binary search on makespan (T)", "Practical heuristics (memory/connectivity constraints, TP/PP restrictions)", "One-time profiling for throughput estimation", "Multi-model extension"], "training_required": false}, "tags": {"methods": ["mixed_integer_linear_programming", "binary_search", "heuristics", "profiling", "data_parallelism", "tensor_parallelism", "pipeline_parallelism"], "problems": ["llm_serving_optimization", "resource_allocation", "gpu_scheduling", "milp_general", "black_box_optimization"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": null, "specific_domain": "llm_serving_optimization", "llm_coupling": null}, "problem": {"formal_name": "Cost-Efficient LLM Serving over Heterogeneous GPUs", "short": "LLM-HGS", "class_": "resource_allocation", "properties": ["heterogeneous_GPUs", "cost_efficiency", "workload_heterogeneity", "dynamic_GPU_availability", "budget_constrained", "multi_model_serving"], "scale": "up to 104 GPUs, 2 LLM models, 9 workload types"}, "lineage": {"direct_ancestors": [{"paper": "JIANG et al.", "relationship": "improves upon the scheduling and GPU composition of HexGen"}, {"paper": "Mei et al., 2024", "relationship": "improves upon the MILP formulation and scope of Helix"}], "closest_prior_work": "Helix (Mei et al., 2024)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["implement online replanning for dynamic workloads", "integrate with other LLM serving optimizations like advanced batching", "explore more sophisticated profiling techniques", "address trade-offs between cost-efficiency and latency more dynamically"], "transferable_to": ["heterogeneous computing environments beyond GPUs (e.g., CPUs, TPUs)", "serving other types of large AI models (e.g., vision models)", "general cloud resource allocation and scheduling problems", "multi-tenant cloud environments with diverse service level objectives"], "open_weaknesses": ["profiling estimations have 4-7% error", "performance gains narrow at very high budgets due to cloud resource availability limits", "online scheduling for dynamic workloads is not directly addressed", "handling extreme budget constraints where optimal GPUs are unavailable"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_status": "growing", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.39, "experiments": {"benchmarks": ["Swiss AI Center real-world workload traces", "WildChat dataset", "Azure-Trace dataset", "ShareGPT dataset"], "baselines": ["Homogeneous H100 setup", "Homogeneous RTX A6000 setup", "Homogeneous RTX 4090 setup", "HexGen", "Helix"], "hardware": "H100, A100 (NVLink 300 GB/s); A40, RTX A6000, L40, RTX 4090 (PCIe 60 GB/s); Ethernet 5 Gb/s inter-server connection; vLLM serving framework.", "instance_sizes": [62, 91, 100, 104]}, "results": {"vs_baselines": {"H100 (Homo)": "up to +41% throughput, up to -54% latency", "RTX A6000 (Homo)": "up to +41% throughput, up to -54% latency", "RTX 4090 (Homo)": "up to +41% throughput, up to -54% latency", "HexGen (uniform composition)": "up to +35% throughput", "HexGen (optimal composition)": "up to +18% throughput", "Helix (Llama-30B)": "+35% throughput", "Helix (Llama3-70B)": "+25% throughput"}, "scalability": "The scheduling algorithm scales efficiently for large-scale problems using binary search, achieving 4x speedup in solution time with minimal performance loss; overall system performance scales with budget but its advantage over baselines narrows at very high budgets due to cloud GPU availability limits.", "statistical_rigor": "Average performance reported across multiple workload traces and GPU availability scenarios. Latency reported using percentiles (P10-P100). No explicit statistical significance tests or variance for individual data points.", "limitations_acknowledged": ["Online scheduling for dynamic workloads is not directly addressed (though replanning is discussed as a future extension).", "Cost-efficiency optimization might lead to slightly higher latency (though P99 latency is lowest).", "Performance gains over baselines narrow at very high budgets due to cloud resource availability limits.", "Profiling estimations have 4-7% error, though deemed sufficient."]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2411.19146", "arxiv_url": "https://arxiv.org/abs/2411.19146", "title": "Puzzle: Distillation-Based NAS for Inference-Optimized LLMs", "authors": ["Akhiad"], "abstract": "", "published_date": "2025-06-03", "affiliations": "NVIDIA", "category": "OR for Generative AI", "relevance": {"methodological": 7, "problem": 8, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This is a prime example of 'OR for AI systems'—specifically using Mixed-Integer Programming (MIP) to optimize LLM inference infrastructure. It offers a rigorous, constraint-based alternative to evolutionary search for modular optimization problems."}, "brief": "Bercovich et al. introduce Puzzle, a framework that optimizes LLM architectures for specific hardware by training a library of block variants (via local distillation) and using Mixed-Integer Programming (MIP) to select the optimal layer-wise configuration under strict latency and memory constraints. The results are robust: they compress Llama-70B to 51B, fitting on a single H100 with 2.17x throughput gain and 98.4% accuracy retention, significantly outperforming pruning baselines like Wanda. **Key takeaway:** The 'decomposed search' strategy—replacing expensive end-to-end evolutionary evaluation loops with local proxy scores (KL divergence) and a global MIP solver—is a highly efficient method for modular system configuration. This directly informs our 'GPUSched' and serving optimization work by demonstrating how to mathematically formulate hardware constraints (KV-cache, batch size, compute) into the model design process itself.", "methodology": {"core_method": "Decomposed Neural Architecture Search (NAS) using Blockwise Local Knowledge Distillation (BLD) for parallel architecture exploration and Mixed-Integer Programming (MIP) for precise constraint optimization, followed by Global Knowledge Distillation (GKD)", "llm_role": "none", "llm_model_used": "Llama-3.1-70B-Instruct, Llama-3.3-70B-Instruct, Llama-3.1-8B-Instruct", "search_type": "hybrid", "novelty_claim": "Our work pioneers the large-scale use of blockwise distillation and MIP-based architecture search for LLMs, successfully scaling these techniques to tens of billions of parameters while requiring only a fraction of the original training compute.", "components": ["Blockwise Local Distillation (BLD)", "Mixed-Integer Programming (MIP)", "Global Knowledge Distillation (GKD)", "Channel Contribution pruning", "TensorRT-LLM modifications for non-uniform blocks"], "training_required": true}, "tags": {"methods": ["neural_architecture_search", "knowledge_distillation", "blockwise_local_distillation", "global_knowledge_distillation", "mixed_integer_programming", "pruning", "low_rank_approximation", "structured_sparsity", "transformer_architecture_optimization", "grouped_query_attention", "multi_head_attention", "reinforcement_learning_from_human_feedback"], "problems": ["llm_inference_optimization", "hardware_aware_optimization", "constrained_optimization", "model_compression", "neural_network_architecture_design"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "lana", "specific_domain": "llm_inference_optimization", "llm_coupling": null}, "problem": {"formal_name": "Distillation-Based Neural Architecture Search for Inference-Optimized Large Language Models", "short": "LLM Inference Optimization", "class_": "llm_optimization", "properties": ["hardware_aware", "inference_optimized", "constrained", "heterogeneous_architecture", "distillation_based"], "scale": "8B-405B parameters, up to 128K context length"}, "lineage": {"direct_ancestors": [{"paper": "LANA: latency aware network acceleration", "relationship": "inspired decomposed NAS strategy from"}, {"paper": "DNA: Block-wisely supervised neural architecture search with knowledge distillation", "relationship": "similar decomposed NAS methods"}, {"paper": "DONNA: Distilling optimal neural networks: Rapid search in diverse spaces", "relationship": "similar decomposed NAS methods"}], "closest_prior_work": "LANA: latency aware network acceleration", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Explore novel operations (e.g., variable window attention, state-space models) as alternative blocks in the search space", "Optimize models for specific capabilities like Chain-of-Thought reasoning", "Extend framework to multimodal tasks including vision-language models and retrieval-augmented generation"], "transferable_to": ["Other transformer-based models beyond LLMs", "Vision-language models", "Retrieval-augmented generation models", "LLM deployment on edge devices and diverse data center hardware"], "open_weaknesses": ["Blockwise Local Distillation (BLD) does not inherently ensure compatibility between different blocks, requiring Global Knowledge Distillation (GKD)", "Performance degradation for long contexts (e.g., beyond 64K tokens) without specific long-context uptraining", "Sensitivity of GKD loss composition, where Language Modeling (LM) loss can be unnecessary or even harmful for optimal uptraining"]}, "artifacts": {"code_url": null, "models_released": true, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.74, "experiments": {"benchmarks": ["Winogrande", "ARC Challenge", "MMLU", "HellaSwag", "GSM8K", "TruthfulQA", "XLSum English", "Instruct HumanEval", "MT-Bench", "RULER", "Arena Hard"], "baselines": ["Llama-3.1-70B-Instruct", "Llama-3.3-70B-Instruct", "Llama-3.1-8B-Instruct", "Mixtral 8x22B", "Llama-3.2-3B-Instruct", "Wanda", "Low-rank approximation", "Random-from-block-library", "Fully Random", "Parent-Randomized", "Greedy Algorithm", "Maximizing Parameters (heuristic)"], "hardware": "NVIDIA H100 GPU, NVIDIA RTX 4090 GPU", "instance_sizes": [8, 47, 49, 51, 56, 70, 253, 340, 405, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072]}, "results": {"vs_baselines": {"Llama-3.1-70B-Instruct": "Nemotron-51B achieves 2.17x inference throughput speedup and retains 98.4% of original model's benchmark accuracies.", "Llama-3.3-70B-Instruct": "Nemotron-49B-Base maintains or exceeds performance up to 16K tokens, retains >98% at 64K, and >94% at 128K context.", "Llama-3.2-3B-Instruct": "Llama-3.1-8B-Instruct child derivative achieves equivalent throughput and far better accuracy (73.98 vs 70.36 weighted accuracy).", "Mixtral 8x22B": "Nemotron-51B pushes beyond its efficient frontier in accuracy vs throughput.", "Wanda": "Nemotron-51B achieves 99.49% of parent's average accuracy vs Wanda's 92.23%.", "Low-rank approximation": "Nemotron-51B achieves 99.49% of parent's average accuracy vs low-rank's 88.96%.", "Greedy Algorithm": "MIP-derived model (78.39% MMLU) significantly outperforms greedy (70.74% MMLU).", "Maximizing Parameters (heuristic)": "MIP-derived model (78.39% MMLU) significantly outperforms parameter-maximization (23.12% MMLU).", "Random-from-block-library": "Nemotron-51B (98.61% relative to Llama-70B) outperforms random-from-library (86.58%).", "Fully Random": "Nemotron-51B (98.61% relative to Llama-70B) outperforms fully random (18.73%).", "Parent-Randomized": "Nemotron-51B (98.61% relative to Llama-70B) outperforms parent-randomized (19.25%)."}, "scalability": "Puzzle enables efficient deployment of LLMs across data centers and edge devices by optimizing for specific hardware and usage profiles, achieving significant throughput speedups and fitting larger models on single GPUs.", "statistical_rigor": "Human evaluation conducted with 3 independent annotators per sample (169 samples), randomized completion order. Ablation studies on dataset size, composition, scoring metrics, and search algorithms. Multiple runs for MIP with different batch sizes to explore trade-offs. Performance metrics include average scores, percentages, and specific benchmark results.", "limitations_acknowledged": ["BLD does not ensure compatibility between different blocks, requiring GKD.", "Nemotron-51B performance degraded beyond 64K tokens without specific long-context uptraining.", "LM loss is unnecessary for optimal GKD uptraining and can harm downstream tasks."]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2506.02049", "arxiv_url": "https://arxiv.org/abs/2506.02049", "title": "EvoGit: Decentralized Code Evolution via Git-Based Multi-Agent Collaboration", "authors": ["Beichen"], "abstract": "", "published_date": "2025-06-01", "affiliations": "The Hong Kong Polytechnic University", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 6, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "While the experimental rigor is low (lack of hard baselines), the architectural proposal—using Git's native DAG for asynchronous population management and lineage tracking—is a brilliant engineering solution to the concurrency and state problems we face in AlgoEvo."}, "brief": "Huang et al. introduce EvoGit, a framework where LLM agents asynchronously evolve code by treating Git commits as the population and using 3-way merges (based on Lowest Common Ancestor) as crossover. While the experiments (web app, bin packing generator) are largely qualitative and lack rigorous statistical benchmarking against baselines like MetaGPT, the architectural contribution is significant. The key takeaway is using Git's native DAG structure to handle lineage, persistence, and asynchronous concurrency 'for free,' replacing complex custom population managers. This is directly actionable for our AlgoEvo infrastructure to enable massive parallelism and better memory/traceability without reinventing the wheel.", "methodology": {"core_method": "Decentralized multi-agent evolutionary process using a Git-based phylogenetic graph with mutation and three-way crossover operations", "llm_role": "code_writer, evaluator", "llm_model_used": null, "search_type": "evolutionary_search", "novelty_claim": "EvoGit introduces a decentralized multi-agent framework for collaborative software development driven by autonomous code evolution, coordinating solely through a Git-based phylogenetic graph without centralized control or explicit message passing.", "components": ["Autonomous Agents", "Human Product Manager", "Phylogenetic Graph", "Git Backend", "Code Mutation", "Three-way Crossover", "Pairwise Comparison"], "training_required": false}, "tags": {"methods": ["llm_code_generation", "llm_as_evaluator", "multi_agent_system", "evolutionary_algorithm", "program_synthesis", "version_control_system", "git_based_coordination", "reward_free_evolution", "phylogenetic_graph", "mutation", "crossover", "three_way_crossover", "pairwise_comparison", "human_in_the_loop"], "problems": ["software_development", "web_application_development", "automated_algorithm_design", "bin_packing"], "contribution_type": ["new_method", "framework", "empirical_study"], "framework_lineage": "evolution_of_heuristics", "specific_domain": null, "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Decentralized Code Evolution via Git-based Multi-Agent Collaboration", "short": "EvoGit", "class_": "multi_agent_software_evolution", "properties": ["decentralized", "multi_agent", "asynchronous", "reward_free", "Git_based", "traceable", "lineage_aware_crossover", "incremental", "advantageous"], "scale": "16 agents, 120 iterations"}, "lineage": {"direct_ancestors": [{"paper": "NEAT Stanley & Miikkulainen (2002)", "relationship": "generalizes lineage tracking concept from neuroevolution to software evolution"}, {"paper": "MetaGPT Hong et al. (2024)", "relationship": "improves upon centralized coordination of multi-agent LLM software development frameworks"}], "closest_prior_work": "MetaGPT Hong et al. (2024)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["adaptive pruning of the version graph", "automated evaluation of divergent branches", "integration with more capable agents equipped with temporal reasoning", "long-horizon planning for agents"], "transferable_to": ["hardware design", "scientific experiment design", "legal document generation", "self_improving_general_purpose_development_agents"], "open_weaknesses": ["inherent non-determinism from LLM outputs", "randomized conflict resolution during crossover", "random selection of target file and editable region for mutation", "potential for excessive divergence leading to unproductive versions"]}, "artifacts": {"code_url": "https://github.com/BillHuang2001/evogit", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_5", "front_status": "stable", "bridge_score": 0.6696, "is_bridge": true, "priority_score": 7.56, "experiments": {"benchmarks": ["Web Application Development (Task 1)", "Meta-Level Code Synthesis (Task 2)"], "baselines": [], "hardware": "Not specified, 16 agents run in parallel", "instance_sizes": [16, 120]}, "results": {"vs_baselines": {}, "scalability": "EvoGit supports scalable agent interaction and highly scalable parallel exploration by enabling asynchronous, decentralized collaboration through a shared Git-based version graph.", "statistical_rigor": "Experiments involved 16 agents running for 120 iterations, acknowledging inherent non-determinism from LLM outputs and randomized conflict resolution during crossover, but no formal statistical tests or variance reporting for performance comparisons were provided.", "limitations_acknowledged": ["Adaptive pruning of the version graph", "Automated evaluation of divergent branches", "Integration with more capable agents with temporal reasoning and long-horizon planning", "Inherent non-determinism due to LLM outputs and randomized conflict resolution"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2505.21775", "arxiv_url": "https://arxiv.org/abs/2505.21775", "title": "DualSchool: How Reliable are LLMs for Optimization Education?", "authors": ["Michael"], "abstract": "", "published_date": "2025-05-27", "affiliations": "Georgia Institute of Technology", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 5, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While the specific task (Primal-to-Dual) is educational, the proposed Canonical Graph Edit Distance (CGED) metric directly addresses a flaw in our OR-Bench evaluation pipeline: objective value checks yield false positives for structurally incorrect models. We should consider adapting CGED to verify symbolic formulations."}, "brief": "This paper evaluates LLMs on Primal-to-Dual Conversion (P2DC), introducing a 'Canonical Graph Edit Distance' (CGED) to verify structural correctness while ignoring benign differences like variable ordering or slack conventions. Results show that even strong LLMs often fail (<50% accuracy) and, crucially, that standard execution-based evaluation (checking objective values) produces frequent false positives by missing errors in redundant constraints. The primary takeaway for us is the CGED methodology: a robust way to score symbolic OR model generation that captures structural validity better than execution alone, which we could steal for our benchmarking and evolutionary search fitness functions.", "methodology": {"core_method": "DUALSCHOOL framework for generating and verifying P2DC instances using Canonical Graph Edit Distance (CGED)", "llm_role": "problem_solver", "llm_model_used": "Mistral-7B, Phi 4-14B, Gemma 3-12B, Qwen 2.5–7B, Qwen 2.5–14B, Llama 3.1-8B, Llama 3.3-70B", "search_type": "constructive", "novelty_claim": "The paper proposes DUALSCHOOL, a comprehensive framework to evaluate the reliability of LLMs for a relatively simple optimization task, and designs a robust automatic evaluation using Canonical Graph Edit Distance (CGED) that allows for differences in dualization convention.", "components": ["DUALSCHOOL framework", "automatic symbolic dualization", "Canonical Graph Edit Distance (CGED)", "graph-based equivalence detection", "canonicalization step", "prompt template", "gurobipy code generation"], "training_required": false}, "tags": {"methods": ["canonical_graph_edit_distance", "graph_edit_distance", "llm_evaluation", "llm_code_generation", "automatic_symbolic_dualization", "canonicalization", "evaluation_framework"], "problems": ["primal_to_dual_conversion", "linear_programming_duality", "llm_reliability_evaluation", "optimization_education", "error_correction", "error_classification", "model_verification"], "contribution_type": ["new_method", "new_benchmark", "empirical_study", "framework"], "framework_lineage": null, "specific_domain": "primal_to_dual_conversion", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Primal-to-Dual Conversion", "short": "P2DC", "class_": "optimization_model_conversion", "properties": ["linear_programming", "duality", "structured_data", "llm_evaluation"], "scale": "up to 9 variables, 14 constraints"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2405.13144", "relationship": "uses datasets from"}, {"paper": "Xing et al. [5]", "relationship": "extends NGED from"}], "closest_prior_work": "Xing et al. [5]", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["extend DUALSCHOOL to quadratic and conic formulations", "evaluate DUALSCHOOL's efficacy as a fine-tuning dataset", "improve LLM performance on P2DC tasks"], "transferable_to": ["quadratic_programming_duality", "conic_programming_duality", "general_optimization_model_verification", "llm_evaluation_for_structured_tasks"], "open_weaknesses": ["low_llm_accuracy_on_p2dc", "llm_inability_to_correct_errors", "bias_in_llm_verification", "limited_in_context_learning_benefit"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": true}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.83, "experiments": {"benchmarks": ["ComplexOR", "Easy LP", "NL4OPT", "NLP4LP", "2D LPs", "CO-Small"], "baselines": ["Mistral-7B", "Phi 4-14B", "Gemma 3-12B", "Qwen 2.5–7B", "Qwen 2.5–14B", "Llama 3.1-8B", "Llama 3.3-70B"], "hardware": "Intel Xeon 6426Y (2.5GHz) CPUs, 8 NVIDIA L40S 48GB GPUs", "instance_sizes": [9, 14]}, "results": {"vs_baselines": {"Phi 4-14B": "47.8% CGED accuracy on NL4OPT (0-shot)", "Llama 3.3-70B": "39.9% CGED accuracy on NL4OPT (0-shot)", "Other open LLMs": "Consistently lower CGED accuracies, often 0%"}, "scalability": "State-of-the-art open LLMs struggle even for the smallest two-variable instances, indicating poor performance across problem sizes.", "statistical_rigor": "Reports mean and max variables/constraints for datasets, and accuracies (CGED, NGED, OBJ, Exec%) across four benchmark datasets under 0-shot and 1-shot prompting conditions.", "limitations_acknowledged": ["LLMs fail to consistently produce correct duals even for small instances", "LLMs often produce duals with correct objective value but incorrect or malformed structure", "Limited applicability of in-context learning (one-shot prompting)", "Bias towards predicting 'no' in the VERIFICATION task"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2506.11057", "arxiv_url": "https://arxiv.org/abs/2506.11057", "title": "STRCMP: Integrating Graph Structural Priors with Language Models for Combinatorial Optimization", "authors": ["Xijun"], "abstract": "", "published_date": "2025-05-22", "affiliations": "Shanghai Key Laboratory of Scalable Computing and Systems, School of Computer Science, Shanghai Jiao Tong University", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper directly upgrades the standard LLM-evolutionary search loop (our primary focus) by injecting GNN-based structural priors, addressing our specific bottleneck of sample efficiency. It targets our exact problem domains (MILP/SAT) and outperforms baselines we likely track (AutoSAT, LLM4Solver)."}, "brief": "STRCMP introduces a composite architecture where a GNN encodes CO problem instances (MILP/SAT) into embeddings that condition an LLM (fine-tuned via SFT and DPO) to generate solver-specific heuristics within an evolutionary loop. The results are strong and empirically backed, showing significant reductions in convergence time and timeouts compared to text-only evolutionary methods like AutoSAT and LLM4Solver. The key takeaway is the architectural blueprint for fusing instance-specific structural embeddings (via soft prompting) with LLM code generation to drastically improve the sample efficiency of evolutionary search. This is immediately relevant to our EvoCut and AlgoEvo projects, suggesting we should move beyond pure text prompts for topology-heavy problems.", "methodology": {"core_method": "Structure-aware LLM-based algorithm discovery framework combining Graph Neural Network (GNN) for structural embeddings and LLM for solver-specific code generation, refined by an evolutionary algorithm.", "llm_role": "code_writer", "llm_model_used": "Qwen2.5-Coder-7B-Instructor", "search_type": "hybrid", "novelty_claim": "STRCMP is the first framework to explicitly integrate structural priors of CO problems into LLM-driven algorithm discovery, jointly improving solution quality and computational efficiency.", "components": ["Graph Neural Network (GNN)", "Large Language Model (LLM)", "Evolutionary Algorithm", "Supervised Fine-Tuning (SFT)", "Direct Preference Optimization (DPO)"], "training_required": true}, "tags": {"methods": ["gnn", "llm_in_the_loop", "evolutionary_algorithm", "supervised_fine_tuning", "direct_preference_optimization", "llm_code_generation", "program_synthesis", "evolution_of_heuristics"], "problems": ["milp_general", "sat", "set_covering", "maximum_independent_set", "multiple_knapsack", "load_balancing"], "contribution_type": ["new_method", "sota_result", "framework"], "framework_lineage": null, "specific_domain": null, "llm_coupling": "rl_trained"}, "problem": {"formal_name": "Combinatorial Optimization (Mixed Integer Linear Programming and Boolean Satisfiability)", "short": "CO (MILP, SAT)", "class_": "algorithm_design", "properties": ["NP-hard", "constrained", "discrete", "structural_priors"], "scale": "500-node graphs, 500x1000 matrices, up to 317,635 variables and 2,120,983 constraints"}, "lineage": {"direct_ancestors": [{"paper": "LLM4Solver", "relationship": "extends the concept of LLM-driven algorithm discovery"}], "closest_prior_work": "LLM4Solver", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["optimize end-to-end joint training of the composite model", "resolve conflicts in post-training data distribution for uniform performance", "automate the definition of target components to reduce human expertise reliance"], "transferable_to": ["other_combinatorial_optimization_problems", "graph_optimization_problems", "constraint_satisfaction_problems"], "open_weaknesses": ["end_to_end_joint_training_challenges", "inconsistent_performance_across_ablations", "reliance_on_human_expertise_for_component_definition"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.11, "experiments": {"benchmarks": ["Set Covering", "Maximum Independent Set", "Multiple Knapsack", "MIK", "CORLAT", "Load Balancing", "Chromatic-Number-of-the-Plane (CNP)", "Profitable-Robust-Production (PRP)", "CoinsGrid", "Zamkeller"], "baselines": ["L2B", "HEM", "NeuroSAT", "AutoSAT", "LLM4Solver", "EasySAT"], "hardware": "dual AMD EPYC 9534 64-core processors @ 2.45GHz and two NVIDIA H800 80GB GPUs", "instance_sizes": [1000, 500, 60, 413, 466, 61000, 8798, 154828, 317635, 24592]}, "results": {"vs_baselines": {"AutoSAT": "Outperforms on most SAT benchmarks (e.g., 71.8% better PAR-2 on Zamkeller, 66.7% fewer timeouts on PRP).", "EasySAT": "Significantly outperforms on SAT benchmarks (e.g., 72.5% better PAR-2 on Zamkeller).", "NeuroSAT": "Achieves significantly fewer timeouts on SAT benchmarks (e.g., 93.5% fewer on PRP).", "L2B": "Maintains strong performance parity on MILP benchmarks.", "HEM": "Maintains strong performance parity on MILP benchmarks.", "LLM4Solver": "Maintains strong performance parity on MILP benchmarks."}, "scalability": "STRCMP achieves significantly faster convergence and higher-quality solutions compared to evolutionary-based baselines, and facilitates generalization across problem classes.", "statistical_rigor": "Extensive experiments across two CO domains and nine benchmark datasets. Comprehensive results, convergence comparisons, ablation studies, and statistical significance analysis (with variance plots) are provided in appendices.", "limitations_acknowledged": ["End-to-end joint training of the composite model presents significant optimization challenges.", "The full STRCMP model does not uniformly outperform its ablations (SFT Only, DPO Only) across all benchmarks, suggesting potential conflicts in post-training data distribution.", "Current methodologies rely on human expertise to predefine target components."]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2505.12285", "arxiv_url": "https://arxiv.org/abs/2505.12285", "title": "CALM: Co-evolution of Algorithms and Language Model for Automatic Heuristic Design", "authors": ["Ziyao"], "abstract": "", "published_date": "2025-05-18", "affiliations": "City University of Hong Kong, Southeast University, University of Victoria, Hon Hai Research Institute", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 8, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper successfully implements 'RL-infused evolution'—a key item on our research agenda—by fine-tuning the LLM weights during the search using GRPO. It proves that a small, locally adapted model (7B) can outperform frozen SOTA models (GPT-4o) in heuristic generation, directly challenging our reliance on large, frozen APIs for AlgoEvo."}, "brief": "CALM introduces a hybrid evolutionary framework that fine-tunes the LLM generator *during* the search process using Group Relative Policy Optimization (GRPO), rather than relying solely on prompt evolution. Using a quantized Qwen-7B model on a single consumer GPU, it outperforms GPT-4o-based baselines (FunSearch, EoH) on Bin Packing and VRP benchmarks. The critical takeaway is their reward function design: instead of absolute performance, they reward the *relative improvement* of the generated code over the specific 'parent' heuristics in the prompt, stabilizing the RL signal. We should immediately test this 'online fine-tuning' approach to reduce our API costs and improve sample efficiency in AlgoEvo.", "methodology": {"core_method": "Hybrid framework combining verbal and numerical guidance for heuristic evolution, achieved by fine-tuning an LLM via reinforcement learning (GRPO) based on heuristic quality, co-evolving the LLM with the search process.", "llm_role": "heuristic_generator_and_fine_tuned_agent", "llm_model_used": "Qwen2.5-7B-Instruct-INT4", "search_type": "hybrid", "novelty_claim": "CALM is the first LLM-based AHD framework that jointly optimizes both the prompt generation process and the LLM model itself, overcoming the limitations of fixed-model approaches.", "components": ["GRPO (Reinforcement Learning)", "Evolutionary Operators (Injection, Replacement, Crossover, Simplification, Initialization)", "Diversity-Aware Crossover", "Collapse Mechanism", "Reward Function (for feasible/infeasible responses)", "Unsloth (LLM framework)"], "training_required": true}, "tags": {"methods": ["evolutionary_search", "heuristic_evolution", "llm_fine_tuned", "rl_trained", "grpo", "llm_evolutionary_search", "genetic_algorithm", "llm_as_heuristic", "program_synthesis"], "problems": ["bin_packing", "tsp", "cvrp", "orienteering_problem", "algorithm_discovery", "heuristic_evolution", "operator_discovery"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "evolution_of_heuristics", "specific_domain": null, "llm_coupling": "rl_trained"}, "problem": {"formal_name": "Automatic Heuristic Design", "short": "AHD", "class_": "algorithm_design", "properties": ["LLM-based", "evolutionary search", "meta-level", "heuristic generation"], "scale": "1,000-2,000 LLM queries for heuristic generation, applied to OBP (1k-10k items), TSP/CVRP/OP (50-200 nodes)"}, "lineage": {"direct_ancestors": [{"paper": "FunSearch", "relationship": "improves upon"}, {"paper": "Evolution of Heuristics (EoH)", "relationship": "improves upon"}, {"paper": "MCTS-AHD", "relationship": "improves upon"}], "closest_prior_work": "FunSearch", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Explore reward mechanisms for non-heuristic LLM trajectories to broaden search space.", "Scale CALM to larger LLM models and distributed computing environments.", "Investigate the co-evolution of LLMs and algorithms for other types of algorithms beyond heuristics.", "Develop more sophisticated diversity-aware mechanisms for evolutionary operators."], "transferable_to": ["Other combinatorial optimization problems (e.g., scheduling, facility location).", "Automated design of approximation algorithms.", "Discovery of algorithms for graph problems.", "Heuristic design for real-world logistics and supply chain problems."], "open_weaknesses": ["LLM evolution is heavily dependent on explicit performance signals from heuristics, limiting exploration of non-heuristic solutions.", "Current evaluation uses a compact LLM on limited hardware, potentially underestimating full scalability and performance with larger models.", "The framework's ability to generalize to entirely new problem classes (beyond CO) is not fully explored.", "The computational cost of co-evolving the LLM and heuristics can be substantial."]}, "artifacts": {"code_url": "https://github.com/whxru/CALM", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.31, "experiments": {"benchmarks": ["Online Bin Packing (OBP)", "Traveling Salesman Problem (TSP)", "Capacitated Vehicle Routing Problem (CVRP)", "Orienteering Problem (OP)"], "baselines": ["Best-Fit [38]", "First Fit", "Greedy-Construct (GC) [39]", "ACO [40]", "POMO [14]", "DeepACO [41]", "FunSearch [13]", "EoH [8]", "ReEvo [9]", "HSEvo [10]", "MCTS-AHD [11]", "EvoTune [33]"], "hardware": "single 24GB NVIDIA A30 GPU with an Intel Xeon Gold 5220R CPU", "instance_sizes": [1000, 5000, 10000, 50, 100, 200]}, "results": {"vs_baselines": {"MCTS-AHD (GPT-4o-mini)": "Outperforms on OBP (-20.2% avg gap), CVRP (-16% to -30% avg gap), OP (-4% to -23% avg gap). Outperforms on TSP N=200 (-2.2% gap).", "POMO": "Outperforms on TSP N=200 (-34.4% gap); POMO is superior on TSP N=50/100.", "FunSearch (GPT-4o-mini)": "Outperforms on OBP (-28.3% avg gap).", "EvoTune (Phi3.5-mini-Instruct)": "Outperforms on OBP (-69.9% avg gap)."}, "scalability": "CALM consistently outperforms baselines on both in-domain and out-of-domain scales, demonstrating strong generalization to new problem sizes.", "statistical_rigor": "Results are averaged over three runs. Standard deviation is shaded in training curves (Figure 2). Optimality gaps are reported.", "limitations_acknowledged": ["LLM evolution depends heavily on explicit performance signals from heuristics in prompts/responses (no reward for non-heuristic trajectories)", "Evaluated with a compact LLM on a single 24GB GPU due to limited resources, full potential/scalability on larger models not fully explored."]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2505.11792", "arxiv_url": "https://arxiv.org/abs/2505.11792", "title": "Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling", "authors": ["Yitian"], "abstract": "", "published_date": "2025-05-17", "affiliations": "Stanford University, Shanghai Jiao Tong University, The University of Hong Kong, Shanghai University of Finance and Economics, Cardinal Operations", "category": "Generative AI for OR", "relevance": {"methodological": 9, "problem": 8, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "The 'Partial KL' technique (relaxing KL constraints on reasoning traces while enforcing them on code) is a directly transferable innovation for our RL-infused evolutionary search. Additionally, using .lp file structural analysis for reward signals and self-consistency offers a concrete way to improve our process reward models for OR tasks."}, "brief": "Chen et al. introduce SIRL, a framework for training LLMs to generate optimization models using Reinforcement Learning with Verifiable Rewards (RLVR) and a novel 'Partial KL' surrogate objective. By removing the KL penalty from the reasoning (CoT) section while retaining it for the code generation section, they balance exploration with syntactic stability, achieving SOTA on OptMATH and IndustryOR against OpenAI-o3 and DeepSeek-R1. The critical takeaway for us is the Partial KL strategy: it allows the model to 'think' freely outside the reference distribution while adhering to strict coding standards—a technique we should immediately test in AlgoEvo. Furthermore, their method of parsing .lp files to extract structural features (variable counts, constraint types) for 'instance-enhanced self-consistency' provides a much richer signal than our current binary success/failure metrics.", "methodology": {"core_method": "Reinforcement Learning with Verifiable Reward (RLVR) using REINFORCE++ with a Partial KL surrogate function", "llm_role": "code_writer", "llm_model_used": "Qwen2.5-7B-Instruct, Qwen2.5-32B-Instruct", "search_type": "constructive", "novelty_claim": "First application of Reinforcement Learning with Verifiable Reward (RLVR) to enhance LLMs' proficiency in optimization modeling, introducing a novel Partial KL surrogate function and a two-stage reward system.", "components": ["Reinforcement Learning with Verifiable Reward (RLVR)", "Partial KL surrogate function", "Two-stage reward system", "Instance-enhanced self-consistency", "Data synthesis pipeline", "LLM-as-a-judge"], "training_required": true}, "tags": {"methods": ["reinforcement_learning", "reinforce_plus_plus", "llm_code_generation", "llm_in_the_loop", "llm_as_evaluator", "data_synthesis", "self_consistency", "milp_solver", "partial_kl_divergence", "two_stage_reward_system"], "problems": ["automated_optimization_modeling", "milp_general", "lp_general", "ip_general", "nlp_general", "socp_general", "program_synthesis"], "contribution_type": ["new_method", "sota_result", "framework"], "framework_lineage": "sirl", "specific_domain": "automated_optimization_modeling", "llm_coupling": "rl_trained"}, "problem": {"formal_name": "Automated Optimization Modeling from Natural Language", "short": "OptModelGen", "class_": "program_synthesis", "properties": ["linear", "integer", "mixed_integer", "nonlinear", "second_order_cone"], "scale": "245-642 instances"}, "lineage": {"direct_ancestors": [{"paper": "ORLM [25]", "relationship": "builds on data synthesis pipeline from"}, {"paper": "Reinforcement Learning with Verifiable Reward (RLVR) [29, 32]", "relationship": "applies and extends the paradigm of"}, {"paper": "REINFORCE++ [75]", "relationship": "uses as core policy gradient algorithm"}], "closest_prior_work": "ORLM [25]", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["address reward hacking", "systematic error analysis for difficult problems", "targeted improvements for difficult benchmarks like IndustryOR and OptMATH"], "transferable_to": ["tool_augmented_tasks requiring exploration and validity", "mathematical_theorem_proving", "code_generation_for_other_domains"], "open_weaknesses": ["reward hacking persists", "performance on difficult benchmarks like IndustryOR and OptMATH remains limited"]}, "artifacts": {"code_url": "https://github.com/Cardinal-Operations/SIRL", "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_1", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.31, "experiments": {"benchmarks": ["NL4OPT", "MAMO Easy", "MAMO Complex", "IndustryOR", "OptMATH"], "baselines": ["GPT-4", "DeepSeek-V3.1", "DeepSeek-R1", "OpenAI-o3", "OptiMUS", "ORLM-LLaMA-3-8B", "LLMOpt-Qwen2.5-14B", "OptMATH-Qwen2.5-7B", "OptMATH-Qwen2.5-32B"], "hardware": "single compute node with eight 80GB NVIDIA H100 GPUs (384 GPU hours total)", "instance_sizes": [245, 642, 203, 100, 193]}, "results": {"vs_baselines": {"DeepSeek-V3.1": "SIRL-32B achieved +3.3% Macro AVG (68.3% vs 65.0%)", "DeepSeek-R1": "SIRL-32B achieved +3.7% Macro AVG (68.3% vs 64.6%)", "OpenAI-o3": "SIRL-32B achieved +11.2% Macro AVG (68.3% vs 57.1%)", "OptiMUS": "SIRL-7B achieved +11.2% Macro AVG (60.6% vs 49.4%)", "ORLM-LLaMA-3-8B": "SIRL-7B achieved +14.2% Macro AVG (60.6% vs 46.4%)", "LLMOpt-Qwen2.5-14B": "SIRL-7B achieved +9.5% Macro AVG (60.6% vs 51.1%)", "OptMATH-Qwen2.5-7B": "SIRL-7B achieved +5.2% Macro AVG (60.6% vs 55.4%)", "OptMATH-Qwen2.5-32B": "SIRL-32B achieved +7.2% Macro AVG (68.3% vs 61.1%)", "GPT-4": "SIRL-32B achieved +13.3% Macro AVG (68.3% vs 55.0%)"}, "scalability": "SIRL-Qwen2.5-32B demonstrated superior performance across all evaluated benchmarks, achieving a higher Macro Average than much larger models, highlighting efficiency in tackling complex optimization modeling challenges.", "statistical_rigor": "Mean accuracy and standard deviation across multiple runs are reported under top-p sampling conditions.", "limitations_acknowledged": ["reward hacking persists", "performance on difficult benchmarks like IndustryOR and OptMATH remains limited"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2505.10117", "arxiv_url": "https://arxiv.org/abs/2505.10117", "title": "Learning Virtual Machine Scheduling in Cloud Computing through Language Agents", "authors": ["Jiehao"], "abstract": "", "published_date": "2025-05-15", "affiliations": "Shanghai Jiao Tong University, East China Normal University, Tongji University", "category": "Generative AI for OR", "relevance": {"methodological": 8, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper successfully applies a hierarchical SMDP framework to LLM-based evolutionary search, solving the 'generalization vs. specialization' trade-off that plagues our current AlgoEvo work. It demonstrates that evolving a 'Composer' (selector) alongside a 'Miner' (heuristic library) is superior to evolving a single monolithic policy for non-stationary environments."}, "brief": "Wu et al. introduce MiCo, a hierarchical framework that uses LLMs to evolve both a library of scenario-specific scheduling heuristics ('Options') and a master policy ('Composer') that dynamically switches between them based on system state. Tested on large-scale Huawei/Azure VM traces, it achieves a 96.9% competitive ratio against Gurobi, significantly outperforming Deep RL (SchedRL) by ~11% in dynamic scenarios. **Key Insight:** Instead of evolving a single robust heuristic (which often fails in non-stationary environments), explicitly evolve a *portfolio* of specialized heuristics and a separate *selector* function. This SMDP-based decomposition is a concrete architectural pattern we should adopt in AlgoEvo to handle diverse problem instances and non-stationary distributions effectively.", "methodology": {"core_method": "Hierarchical Language Agent Framework (MiCo) for LLM-driven heuristic design, formulated as Semi-Markov Decision Process with Options (SMDP-Option), using LLM-based function optimization for policy discovery and composition.", "llm_role": "heuristic_generator, evolutionary_search, decomposition_guide", "llm_model_used": "GPT-4", "search_type": "improvement", "novelty_claim": "We pioneer an LLM-driven heuristic design paradigm for ODMBP that automatically generates context-aware scheduling policies through LLM reasoning, eliminating reliance on domain-expert predefined rules while maintaining decision interpretability.", "components": ["Option Miner", "Option Composer", "Scenario Generation Tool", "Pruning Tool", "LLM-based Function Optimization"], "training_required": true}, "tags": {"methods": ["llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "llm_evolutionary_search", "evolution_of_heuristics", "program_synthesis", "in_context_learning", "semi_markov_decision_process", "options_framework", "hierarchical_reinforcement_learning", "heuristic_design", "weighted_sum_method", "dynamic_weight_adjustment", "lookahead_mechanism"], "problems": ["virtual_machine_scheduling", "online_dynamic_multidimensional_bin_packing", "cloud_scheduling"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "funsearch", "specific_domain": "virtual_machine_scheduling", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Online Dynamic Multidimensional Bin Packing", "short": "ODMBP", "class_": "packing", "properties": ["online", "dynamic", "multidimensional", "NP-hard", "stochastic_arrivals", "nonstationary_demands"], "scale": "10,000-400,000 VMs, 50 PMs"}, "lineage": {"direct_ancestors": [{"paper": "Romera-Paredes et al. 2024", "relationship": "extends LLM-based program search from FunSearch"}, {"paper": "Sutton et al. 1999", "relationship": "builds on Semi-Markov Decision Process with Options (SMDP-Option) framework"}], "closest_prior_work": "Romera-Paredes et al. 2024", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Leverage fine-tuning techniques for domain-specific LLM models with expert knowledge in cloud scheduling.", "Address the cold start problem using unsupervised or self-supervised learning for initial policy sets."], "transferable_to": ["Various combinatorial optimization problems.", "Future cloud computing scenarios and emerging technological trends."], "open_weaknesses": ["The cold start problem for generating initial policy sets without predefined options.", "LLMs struggle to balance performance across all scenarios with a single policy.", "Limited prompt context in LLMs for capturing overall patterns and nonstationary trends."]}, "artifacts": {"code_url": "https://github.com/jettbrains/-L-", "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.11, "experiments": {"benchmarks": ["Huawei-East-1", "AzurePublicDatasetV2"], "baselines": ["Best-Fit", "First-Fit", "Hindsight", "SchedRL", "Gurobi (offline optimal)"], "hardware": "four AMD EPYC 7R32 48-Core processors, 192 cores, 514 GB RAM, NVIDIA A100 GPU", "instance_sizes": [20000, 130000, 400000]}, "results": {"vs_baselines": {"Best-Fit": "MiCo +4.3 percentage points (mean)", "First-Fit": "MiCo +7.2 percentage points (mean)", "Hindsight": "MiCo +6.4 percentage points (mean)", "SchedRL": "MiCo +11.1 percentage points (mean)"}, "scalability": "MiCo achieves a 96.9% competitive ratio in large-scale scenarios involving more than 10,000 virtual machines and maintains high performance under nonstationary request flows and diverse configurations.", "statistical_rigor": "Results for SchedRL include confidence intervals; 30 independent experiments were conducted, and mean performance ratios are calculated across scenarios.", "limitations_acknowledged": ["cold start problem for initial policy sets", "potential for fine-tuning domain-specific models with expert knowledge"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2505.06608", "arxiv_url": "https://arxiv.org/abs/2505.06608", "title": "RideAgent: An LLM-Enhanced Optimization Framework for Automated Taxi Fleet Operations", "authors": ["Xinyu"], "abstract": "", "published_date": "2025-05-10", "affiliations": "Tsinghua University, McGill University, George Washington University, JD Intelligent Cities Research, Beijing Technology and Business University", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 6, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "The paper introduces a concrete mechanism (LLM-based variable fixing via historical solution analysis) that directly complements our work on EvoCut and AlgoEvo. It offers a specific heuristic strategy to accelerate MIP solving that we haven't fully explored."}, "brief": "RideAgent employs an LLM to analyze a small set of historical optimal solutions, identifying and fixing 'low-sensitivity' decision variables to shrink the MIP search space before handing it to Gurobi. The results are empirically solid, showing a ~50% time reduction with <2.5% optimality gap, outperforming standard cutting plane baselines on NYC taxi data. **Key Takeaway:** We should adapt their 'Small-Sample Guided Optimization' strategy—specifically using LLMs to infer *variable fixing constraints* from elite archive solutions—to accelerate the inner solvers in our AlgoEvo and EvoCut pipelines. This offers a concrete, data-driven way to prune search spaces that complements our current evolutionary approaches.", "methodology": {"core_method": "LLM-guided variable fixing heuristic for Mixed-Integer Programming (MIP) with an embedded Random Forest (RF) objective, solved lexicographically", "llm_role": "objective_formulation", "llm_model_used": "GPT-4o mini", "search_type": "hybrid", "novelty_claim": "Introduces a novel LLM-guided acceleration heuristic that performs model reduction by identifying and fixing low-sensitivity decision variables, termed Small-Sample Guided Optimization.", "components": ["LLM (GPT-4o mini)", "Mixed-Integer Programming (MIP) solver (Gurobi)", "Random Forest (RF) model", "Problem Matcher", "Indicator Generator", "Problem Tailor", "Code Safeguard", "Response Prompter"], "training_required": true}, "tags": {"methods": ["mixed_integer_programming", "random_forest", "supervised_learning", "llm_agents", "llm_as_heuristic", "llm_objective_formulation", "llm_code_generation", "llm_in_the_loop", "variable_fixing_heuristic", "lexicographical_optimization"], "problems": ["taxi_pre_allocation", "dynamic_pricing", "spatio_temporal_supply_demand_balancing", "fleet_management", "resource_allocation", "milp_general"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "rideagent", "specific_domain": "joint_taxi_pre_allocation_and_pricing", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Joint Taxi Pre-Allocation and Pricing Problem", "short": "JTPP", "class_": "resource_allocation", "properties": ["electric", "spatio-temporal", "dynamic", "feature-driven", "uncertain_demand", "multi-objective"], "scale": "50 zones, 3 SOC levels, up to 1032 decision variables"}, "lineage": {"direct_ancestors": [{"paper": "Biggs et al. (2022)", "relationship": "extends RF-to-MIP conversion technique from"}, {"paper": "Hao et al. (2020)", "relationship": "model setup informed by taxi pre-allocation framework from"}], "closest_prior_work": "OptiMUS, ORPO, OptiGuide (LLM-assisted OR tools)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["enhancing_underlying_predictive_models", "investigating_more_sophisticated_dialogue_protocols_between_llm_and_solver", "deploying_and_testing_in_other_real_world_operational_environments"], "transferable_to": ["supply_chain_management", "logistics", "resource_scheduling"], "open_weaknesses": ["effectiveness_of_heuristic_depends_on_alignment_between_historical_guidance_data_and_new_user_query"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.24, "experiments": {"benchmarks": ["New York City yellow taxi trip records 2016"], "baselines": ["FULL model (complete feature-driven MIP)", "CliqueCuts", "CoverCuts", "GomoryCuts", "GUBCoverCuts", "MIRCuts"], "hardware": "macOS system with Apple M1 Pro CPU and 16GB RAM", "instance_sizes": [50, 3, 1032]}, "results": {"vs_baselines": {"FULL model (linear objectives)": "53.15% CPU time reduction with 2.42% average optimality gap", "FULL model (nonlinear objectives)": "50.04% average CPU time reduction with 1.51% RF-Obj Gap", "Cutting plane methods": "42.32% average time saving with 0.94% RF-Obj Gap and 5.15% QR-Obj Gap"}, "scalability": "RideAgent's solution time is consistently lower than the FULL model's, and this time advantage widens as more variables are fixed.", "statistical_rigor": "Each query was executed 10 times, with reported metrics averaged; box plots illustrate median objective gaps and variance across different fixed variable scales.", "limitations_acknowledged": ["Effectiveness of heuristic depends on alignment between historical guidance data and new user query", "Need for enhancing underlying predictive models", "Need for investigating more sophisticated dialogue protocols between LLM and solver", "Need for deployment and testing in other real-world operational environments"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2505.04354", "arxiv_url": "https://arxiv.org/abs/2505.04354", "title": "Optimization Problem Solving Can Transition to Evolutionary Agentic Workflows", "authors": ["Wenhao"], "abstract": "", "published_date": "2025-05-07", "affiliations": "University of Minnesota, Tongji University, East China Normal University", "category": "Generative AI for OR", "relevance": {"methodological": 5, "problem": 9, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "This is a position paper from a competing group working on the exact same intersection (LLM-based evolutionary search for OR/Systems). While the methodology is largely a repackaging of FunSearch/ReEvo concepts into an 'agentic' framework, the specific application to evolving symbolic update rules for ADMM is relevant to our EvoCut and Latent Heuristic Search projects."}, "brief": "Li et al. propose an 'Evolutionary Agentic Workflow' that combines LLMs (DeepSeek) with evolutionary search to automate algorithm design, demonstrating it on VM scheduling and ADMM parameter tuning. The empirical rigor is low; they compare against weak baselines (BestFit for bin packing, a 2000-era heuristic for ADMM) and frame it as a position paper. However, the application of LLM-evolution to discover symbolic mathematical update rules (for ADMM step sizes) rather than just procedural code is a concrete use case we should consider for our EvoCut work. This serves primarily as competitor intelligence—validating our AlgoEvo direction—rather than a source of novel methodology.", "methodology": {"core_method": "Evolutionary Agentic Workflow combining Foundation Agents (Memory, Reasoning, World Modeling, Action modules) and Evolutionary Search (Distributed Population Management, Solution Diversity Preservation, Knowledge-Guided Evolution)", "llm_role": "evolutionary_search", "llm_model_used": "Deepseek coder LLM model", "search_type": "hybrid", "novelty_claim": "This paper contends that an “evolutionary agentic workflow,” underpinned by foundation models (FMs), is uniquely suited to surmount these obstacles in ways that conventional optimization pipelines cannot.", "components": ["Foundation Agents (Memory module, Reasoning module, World Modeling module, Action module)", "Evolutionary Framework (Distributed Population Management, Solution Diversity Preservation, Knowledge-Guided Evolution)", "Human-Centered Evaluation"], "training_required": true}, "tags": {"methods": ["evolutionary_algorithms", "llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "llm_evolutionary_search", "llm_fine_tuned", "reinforcement_learning", "agentic_workflow", "hyperparameter_optimization", "automated_algorithm_design", "problem_formulation", "ADMM", "bin_packing_heuristics", "meta_optimization", "retrieval_augmented_generation", "continual_learning"], "problems": ["virtual_machine_scheduling", "online_vector_bin_packing", "ADMM_parameter_adaptation", "structured_convex_optimization", "automated_algorithm_design", "hyperparameter_optimization", "problem_formulation"], "contribution_type": ["new_method", "framework", "empirical_study", "sota_result"], "framework_lineage": "evolutionary_agentic_workflow", "specific_domain": null, "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Evolutionary Agentic Optimization Workflow Design", "short": "AutoOpt", "class_": "llm_evolutionary_search", "properties": ["meta-optimization", "agentic", "evolutionary", "LLM-driven", "adaptive", "human-in-the-loop"], "scale": "VM scheduling (30-200 VMs), ADMM problems (from LibADMM)"}, "lineage": {"direct_ancestors": [{"paper": "Romera-Paredes et al., 2024", "relationship": "builds on LLM-evolutionary algorithm integration from FunSearch"}, {"paper": "Liu et al., 2024a", "relationship": "builds on LLM-evolutionary algorithm integration from Evolution of Heuristics"}, {"paper": "Ye et al., 2024", "relationship": "enhances evolutionary framework with reflection mechanisms from ReEvo"}, {"paper": "He et al., 2000", "relationship": "extends adaptive parameter tuning for ADMM from"}], "closest_prior_work": "Romera-Paredes et al., 2024", "novelty_type": "paradigm_shift"}, "extensions": {"next_steps": ["Integrate with automated proof assistants (e.g., Lean4) for theoretical verification", "Reduce LLM inference cost via quantization, distillation, and small FMs", "Improve scalability to large-scale problems using HPC/GPU acceleration and bottom-up operator optimization"], "transferable_to": ["Supply chain management", "Manufacturing optimization", "Energy systems optimization", "General machine learning optimization tasks"], "open_weaknesses": ["Lack of built-in mechanisms for theoretical verification", "High inference cost of FMs", "Scalability to large-scale industrial optimization problems"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.24, "experiments": {"benchmarks": ["Huawei Cloud VM placement dataset", "LibADMM"], "baselines": ["BestFit", "ADMM-expert"], "hardware": "One A100 (80G) GPU", "instance_sizes": [30, 50, 100, 150, 200]}, "results": {"vs_baselines": {"BestFit": "NewAlgorithm achieved higher scheduling length (e.g., 1526.1 vs 1386.8 on VM size 50) and consistently outperformed BestFit across all tested VM sizes (30-200).", "ADMM-expert": "NewADMM generally required significantly fewer iterations than ADMM-expert (e.g., L1: 60 vs 227, Elasticnet: 12 vs 229, LRR: 3 vs 198), though it required more iterations for LRMC (122 vs 83)."}, "scalability": "The workflow is heuristic-focused and generalizes to different VM sizes and ADMM problems. Computational demands for huge instances are a potential bottleneck but can be addressed by HPC/GPU acceleration.", "statistical_rigor": "Preliminary experiments with average performance reported. No explicit mention of multiple runs, variance, or significance tests.", "limitations_acknowledged": ["Lack of built-in mechanisms for theoretical verification", "High inference cost of FMs", "Scalability to large-scale industrial optimization problems"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2505.01485", "arxiv_url": "https://arxiv.org/abs/2505.01485", "title": "CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code", "authors": ["Tasnim"], "abstract": "", "published_date": "2025-05-02", "affiliations": "Queen's University", "category": "Generative AI for OR", "relevance": {"methodological": 5, "problem": 7, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While the RAG architecture is standard, the specific technique of indexing code via generated metadata (keywords/summaries) rather than raw syntax is a high-value engineering trick for our OR-Bench and coding agent pipelines. It directly addresses the vocabulary mismatch in translating natural language to solver APIs."}, "brief": "CHORUS introduces a RAG framework for generating Gurobi code that replaces standard code retrieval with a metadata-based approach, indexing code examples by generated keywords and summaries rather than raw syntax. On the NL4Opt-Code benchmark, this allows open-source models like Llama-3-70B to match GPT-4 performance (improving accuracy from ~23% to ~57%). The key takeaway for us is the effectiveness of 'metadata-augmented indexing'—bridging the semantic gap between natural language problem descriptions and rigid solver APIs by retrieving based on functional descriptions rather than code embeddings. We should apply this metadata indexing strategy to the code retrieval modules in our OR-Bench and AlgoEvo agents.", "methodology": {"core_method": "Retrieval-Augmented Generation (RAG) framework with hierarchical chunking, metadata-augmented indexing, two-stage retrieval, cross-encoder reranking, expert prompting, and structured output parsing", "llm_role": "code_writer", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "CHORUS is a novel zero-shot RAG framework that enhances LLMs’ retrieval abilities by leveraging hierarchical chunking and contextual metadata from LP solver documentation for code generation, and is the first work to apply a retrieval mechanism to this task.", "components": ["Hierarchical tree indexing", "Metadata-augmented indexing", "Two-stage context retrieval", "Cross-encoder reranking", "Expert prompting", "Structured output parser with reasoning steps"], "training_required": false}, "tags": {"methods": ["llm_code_generation", "retrieval_augmented_generation", "hierarchical_chunking", "metadata_augmented_indexing", "two_stage_retrieval", "cross_encoder_reranking", "expert_prompting", "structured_output_parsing", "llm_in_the_loop"], "problems": ["linear_programming_code_generation", "program_synthesis"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study", "new_benchmark"], "framework_lineage": "rag", "specific_domain": "linear_programming_code_generation", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Linear Programming Code Generation from Natural Language", "short": "LP Code Gen", "class_": "program_synthesis", "properties": ["linear", "solver_specific"], "scale": "unspecified"}, "lineage": {"direct_ancestors": [{"paper": "Tsouros et al. 2023", "relationship": "extends end-to-end LP solver code generation from"}, {"paper": "Lewis et al. 2020", "relationship": "applies and enhances Retrieval-Augmented Generation (RAG) from"}], "closest_prior_work": "Tsouros et al. 2023", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["explore adaptive retrieval strategies", "implement multi-step reasoning", "develop agentic approaches for code synthesis", "extend to integer linear, mixed, or non-linear programming problems"], "transferable_to": ["other mathematical programming problems (e.g., MILP, NLP)", "code generation for other optimization solvers (e.g., CPLEX, OR-Tools)", "code synthesis for other domain-specific APIs or libraries"], "open_weaknesses": ["code generation remains highly sensitive to prompt engineering", "smaller models struggle with limited context windows", "current framework is focused on LP, not other optimization types"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": true}, "front_id": "generative_ai_for_or_2026-02-18_front_7", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.74, "experiments": {"benchmarks": ["NL4Opt-Code dataset"], "baselines": ["GPT3.5", "GPT4", "Llama3.1 (8B) baseline", "Llama3.3 (70B) baseline", "Phi4 (14B) baseline", "Deepseek-r1 (32B) baseline", "Qwen2.5-coder (32B) baseline", "Traditional RAG"], "hardware": "Intel Xeon Platinum 8358 processor, NVIDIA H100 NVL GPU, 251 GB of RAM", "instance_sizes": []}, "results": {"vs_baselines": {"Llama3.1 (8B) baseline": "+69.5% accuracy with CHORUS (0.1349 vs 0.0796)", "Llama3.3 (70B) baseline": "+147.9% accuracy with CHORUS (0.5675 vs 0.2289)", "Phi4 (14B) baseline": "+216.0% accuracy with CHORUS (0.6125 vs 0.1938)", "Deepseek-r1 (32B) baseline": "+444.9% accuracy with CHORUS (0.5848 vs 0.1073)", "Qwen2.5-coder (32B) baseline": "+28.9% accuracy with CHORUS (0.5986 vs 0.4644)", "GPT3.5": "CHORUS-enhanced open-source LLMs surpass its 0.5260 accuracy", "GPT4": "CHORUS-enhanced open-source LLMs approach its 0.6367 accuracy", "Traditional RAG": "CHORUS outperforms by 46.14-89.33% accuracy"}, "scalability": "Smaller models struggle to fully incorporate all contextual elements within limited context windows, indicating potential scalability challenges with increasing problem complexity or context requirements.", "statistical_rigor": "Preliminary experiments noted significant variability in model responses despite low sampling temperature; however, explicit details on multiple runs or significance tests for main results are not provided.", "limitations_acknowledged": ["Code generation remains highly sensitive to prompt engineering", "Smaller models struggle to fully incorporate all contextual elements within limited context windows", "Alignment of LLMs with other optimization topics (e.g., integer linear, mixed, or non-linear problems) is left for future research"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2504.07347", "arxiv_url": "https://arxiv.org/abs/2504.07347", "title": "Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents", "authors": ["Yueying"], "abstract": "", "published_date": "2025-04-24", "affiliations": "Cornell University, Columbia University", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper provides the rigorous queueing-theoretic foundation for our 'GPUSched' project, moving beyond heuristics to prove stability conditions. Crucially, it demonstrates that while work-conserving policies optimize single-node throughput, they can cause instability in multi-agent networks (Rybko-Stolyar anomaly)—a critical insight for our multi-agent system architecture."}, "brief": "Li et al. formulate a batch queueing model for LLM inference, proving that 'work-conserving' algorithms (like Sarathi-Serve) which mix prefill and decode tokens are throughput-optimal, whereas separated strategies (vanilla vLLM, FasterTransformer) are theoretically unstable. The results are rigorous, combining fluid limit proofs with empirical validation on A100s showing queue blow-ups in non-optimal schedulers. The key takeaway is the precise definition of stability for token-level batching and the counter-intuitive finding that these locally optimal policies can fail in multi-agent networks due to cyclic resource dependencies. This is foundational reading for our GPUSched project and directly informs how we should model resource allocation for our multi-agent optimization systems.", "methodology": {"core_method": "Queueing-theoretic framework with discrete-time Markov chains and fluid limits for analyzing work-conserving scheduling algorithms", "llm_role": "none", "llm_model_used": null, "search_type": "exact", "novelty_claim": "A formal queueing-theoretic framework for LLM inference scheduling that captures the unique characteristics of the prefill and decode phases, explicitly modeling the batch processing time.", "components": ["Queueing model", "Discrete-time Markov chains (DTMCs)", "Lyapunov function", "Fluid limit technique", "Work-conserving scheduling algorithms", "(Kp, Kd)-FCFS algorithms"], "training_required": false}, "tags": {"methods": ["queueing_theory", "discrete_time_markov_chains", "fluid_limits", "lyapunov_function", "work_conserving_scheduling", "fcfs_scheduling", "decode_prioritized_scheduling", "prefill_prioritized_scheduling", "mixed_batching", "mathematical_modeling", "stochastic_processes", "orca", "sarathi_serve", "fastertransformer", "vllm"], "problems": ["llm_inference_scheduling", "gpu_scheduling", "resource_allocation", "ai_agent_workloads", "multi_agent_coordination", "throughput_optimization", "latency_optimization"], "contribution_type": ["new_method", "theoretical_result", "empirical_study", "framework"], "framework_lineage": null, "specific_domain": "llm_inference_scheduling", "llm_coupling": null}, "problem": {"formal_name": "Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents", "short": "LLM Inference Scheduling", "class_": "scheduling", "properties": ["dual_phase", "dynamic_batch_formation", "request_interdependencies", "work_conserving", "autoregressive", "discrete_time", "multi_agent"], "scale": "CodeLlama-34B, Llama-70B, token budget 128-2048, up to 1000 tokens"}, "lineage": {"direct_ancestors": [{"paper": "batch queueing theory", "relationship": "extends"}, {"paper": "queueing theory for scheduling and resource allocation", "relationship": "applies principles from"}, {"paper": "Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills", "relationship": "models and validates performance of"}, {"paper": "Orca: A distributed serving system for transformer-based generative models", "relationship": "models and validates performance of"}, {"paper": "FasterTransformer", "relationship": "models and analyzes limitations of"}, {"paper": "Efficient memory management for large language model serving with pagedattention", "relationship": "models and analyzes limitations of"}], "closest_prior_work": "Queueing, predictions, and llms: Challenges and open problems", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Optimal scheduling policies for tail latency (TBT, TTFT) under various load scenarios", "KV cache memory management for longer contexts and test-time workloads", "Scheduling policies under multi-tenancy with best-effort and latency-critical requests", "Joint optimization of model autoscaling, resource allocation, KV cache policies, and load-balancing"], "transferable_to": ["Cloud computing scheduling", "Operating systems and networking scheduling", "Telecommunications scheduling", "Manufacturing and healthcare operations scheduling", "General distributed AI systems"], "open_weaknesses": ["Optimal scheduling for tail latency not comprehensively studied", "KV cache memory management for longer contexts is future work", "Multi-tenancy scheduling not fully explored", "Memory constraints ignored in the primary queueing model", "Non-linear batch processing time for long contexts not fully captured in the simplified model"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_6", "front_status": "emerging", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.36, "experiments": {"benchmarks": ["Orca", "Sarathi-Serve", "FasterTransformer", "vLLM", "CodeLlama-34B", "Llama-70B", "ShareGPT dataset"], "baselines": ["FasterTransformer", "vanilla vLLM", "Orca", "Sarathi-Serve", "latest version of vLLM with chunk-prefill"], "hardware": "A100, two A100 80GB GPUs with NVLink", "instance_sizes": [34000000000, 70000000000, 128, 256, 512, 1024, 2048]}, "results": {"vs_baselines": {"Orca": "throughput-optimal", "Sarathi-Serve": "throughput-optimal", "FasterTransformer": "not maximally stable, unstable under moderate load", "vanilla vLLM": "not maximally stable, unstable under moderate load", "latest version of vLLM with chunk-prefill": "throughput optimal"}, "scalability": "Work-conserving algorithms achieve maximum throughput in single-instance LLM inference, but may fail to attain optimality in some multi-agent network scenarios.", "statistical_rigor": "Empirical validation with plots showing requests in system over time, batch processing time with R2 values, and CCDF for latency. Theoretical proofs provide rigorous stability guarantees.", "limitations_acknowledged": ["optimal scheduling policy for tail latency not comprehensively studied", "KV cache memory management for longer contexts is future work", "multi-tenancy scheduling not fully explored", "joint optimization with autoscaling, resource allocation, KV cache, and load-balancing is future work", "memory constraints ignored in model", "non-linear batch processing time for long contexts not fully captured"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2504.16918", "arxiv_url": "https://arxiv.org/abs/2504.16918", "title": "OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents", "authors": ["Raghav"], "abstract": "", "published_date": "2025-04-23", "affiliations": "University of Maryland at College Park", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 7, "inspirational": 8}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "The UCB-based debug scheduling is a concrete, transferable mechanism for managing search breadth vs. depth in code generation agents. It directly addresses our 'sample efficiency' and 'search control' interests for AlgoEvo without requiring complex RL training."}, "brief": "OptimAI introduces a multi-agent framework for translating natural language to optimization models, featuring a 'plan-before-code' stage and a novel **UCB-based debug scheduler**. Instead of linearly debugging a single solution, it treats debugging as a multi-armed bandit problem, dynamically allocating compute to different solution strategies based on a 'Decider' score and exploration term. While the combinatorial results (TSP a280) are trivial, the bandit mechanism is a highly effective heuristic for search control. We should steal this UCB scheduling logic for AlgoEvo to prevent agents from wasting tokens debugging fundamentally flawed heuristics.", "methodology": {"core_method": "LLM-powered multi-agent system (formulator, planner, coder, code critic, decider, verifier) with UCB-based debug scheduling for adaptive plan selection and iterative code refinement.", "llm_role": "decomposition_guide, code_writer, evaluator, evolutionary_search", "llm_model_used": "GPT-4o, o1-mini, QwQ (by Qwen), DeepSeek-R1, Llama 3.3 70B, Gemma 2 27B", "search_type": "hybrid", "novelty_claim": "OptimAI is a novel LLM-powered multi-agent framework for solving natural language optimization problems, featuring a plan-before-code strategy, UCB-based debug scheduling for adaptive plan selection, and multi-agent collaboration for iterative code refinement.", "components": ["Formulator agent", "Planner agent", "Coder agent", "Code Critic agent", "Decider agent", "Verifier agent", "UCB-based debug scheduling"], "training_required": false}, "tags": {"methods": ["llm_as_heuristic", "llm_as_evaluator", "llm_code_generation", "llm_in_the_loop", "multi_agent_llm_system", "multi_armed_bandit"], "problems": ["optimization_modeling", "linear_programming", "mixed_integer_linear_programming", "nonlinear_programming", "mixed_integer_nonlinear_programming", "traveling_salesperson_problem", "job_shop_scheduling", "set_covering_problem"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": null, "specific_domain": "optimization_modeling", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Optimization Problem Solving from Natural Language", "short": "NL-to-Opt", "class_": "optimization_modeling", "properties": ["natural_language_input", "multi_agent", "adaptive_planning", "multi_solver_support", "iterative_debugging"], "scale": "65-605 general optimization problems, up to 280 cities for TSP, 20 jobs/5 machines for JSP"}, "lineage": {"direct_ancestors": [{"paper": "OptiMUS", "relationship": "extends capabilities for natural language to optimization problem solving"}, {"paper": "Optibench", "relationship": "extends capabilities for natural language to optimization problem solving"}, {"paper": "Chain-of-Experts", "relationship": "extends multi-agent LLM approach for operations research tasks"}, {"paper": "OR-LLM-Agent", "relationship": "extends agentic LLM approach for operations research problems"}], "closest_prior_work": "Optibench", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Reinforce the framework with RL, especially for fine-tuning the decider component.", "Scale up OptimAI to tackle large-scale problems requiring human expert teams.", "Explore dynamic roles and interactions among agents for adaptation and specialization."], "transferable_to": ["Other NP-hard combinatorial optimization problems beyond TSP, JSP, SCP.", "Complex real-world decision-making problems in various scientific and engineering disciplines.", "Automated scientific discovery tasks requiring optimization."], "open_weaknesses": ["The decider component's performance could be substantially improved with RL training.", "Current performance is comparable to a single skilled programmer, not a team of experts for large-scale problems.", "High token usage, indicating potential for cost optimization.", "Optimal hyperparameters (e.g., number of plans, exploration constant) require careful tuning."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_status": "stable", "bridge_score": 0.5313, "is_bridge": true, "priority_score": 5.79, "experiments": {"benchmarks": ["NLP4LP", "Optibench", "TSPLIB", "SelfJSP", "IBM ILOG CPLEX Optimization Studio documentation"], "baselines": ["OptiMUS [4]", "Optibench [35]"], "hardware": "null", "instance_sizes": [65, 605, 280, 20, 5]}, "results": {"vs_baselines": {"OptiMUS": "OptimAI w/ GPT-4o+o1-mini achieves 88.1% accuracy on NLP4LP vs 71.6%, reducing error rate by 58%. OptimAI outperforms OptiMUS across all metrics (Executability: 3.5 vs 3.1, Productivity: 2.32 vs 0.72).", "Optibench": "OptimAI w/ DeepSeek-R1 achieves 82.3% overall accuracy on Optibench, outperforming previous best by 8.1 standard deviations. Error rate reductions on Optibench subsets range from 41% to 68%. OptimAI surpasses Optibench in executability (3.5 vs 3.4)."}, "scalability": "OptimAI is broadly applicable and demonstrates strong generality by effectively solving NP-hard combinatorial optimization problems like TSP, JSP, and Set Covering without problem-specific customization.", "statistical_rigor": "Evaluations conducted in a zero-shot prompting setting, measuring Pass@1 accuracy, Executability (human-evaluated 1-4 score), Token Usage, Productivity, and Revisions. Ablation studies confirm the effectiveness of individual roles and UCB-based debug scheduling.", "limitations_acknowledged": ["Reinforcing the framework with RL, especially for fine-tuning the decider component, could yield substantial gains.", "Scaling up OptimAI to tackle large-scale problems that typically require a team of human experts and engineers, moving beyond the current scope where its performance is comparable to a single skilled programmer."]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2504.04310", "arxiv_url": "https://arxiv.org/abs/2504.04310", "title": "CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization", "authors": ["Weiwei"], "abstract": "", "published_date": "2025-04-06", "affiliations": "Carnegie Mellon University", "category": "Generative AI for OR", "relevance": {"methodological": 4, "problem": 9, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper establishes the standard benchmark (CO-Bench) for the exact problem we are solving (LLM evolutionary search for CO). We should adopt this suite to evaluate AlgoEvo instead of curating our own ad-hoc instances."}, "brief": "Sun et al. introduce CO-Bench, a suite of 36 diverse combinatorial optimization problems (packing, scheduling, routing) designed specifically to benchmark LLM agents in generating algorithms (code), not just solutions. They evaluate 9 frameworks (including FunSearch, ReEvo, AIDE), finding that FunSearch combined with reasoning models (o3-mini) yields the most robust performance, though agents still struggle significantly with strict feasibility constraints (valid solution rates often <60%). **Takeaway:** We should immediately integrate CO-Bench into our pipeline to benchmark AlgoEvo against ReEvo and FunSearch; this saves us months of data curation and provides a standardized metric to prove our method's superiority.", "methodology": {"core_method": "LLM-based algorithm search using agentic frameworks with iterative refinement and evolutionary search", "llm_role": "evolutionary_search", "llm_model_used": "o3-mini-medium", "search_type": "improvement", "novelty_claim": "We introduce CO-Bench, the first comprehensive benchmark to evaluate the capability of LLMs to develop algorithms for diverse and challenging real-world CO problems.", "components": ["LLM prompting", "evolutionary algorithm", "iterative refinement", "external tools (coding environment, evaluation system)", "BestOfN sampling", "Chain of Experts", "Greedy Refinement", "EoH", "AIDE", "ReEvo", "MSTC-AHD"], "training_required": false}, "tags": {"methods": ["llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "llm_evolutionary_search", "funsearch", "eoh", "reevo", "aide", "mstc_ahd", "bestofn_sampling", "greedy_refinement", "chain_of_experts", "metaheuristics", "program_synthesis"], "problems": ["aircraft_landing", "assignment_problem", "assortment_problem", "bin_packing_one_dimensional", "capacitated_warehouse_location", "common_due_date_scheduling", "constrained_guillotine_cutting", "constrained_non_guillotine_cutting", "container_loading", "container_loading_with_weight_restrictions", "corporate_structuring", "crew_scheduling", "equitable_partitioning_problem", "euclidean_steiner_problem", "flow_shop_scheduling", "generalised_assignment_problem", "graph_coloring", "hybrid_reentrant_shop_scheduling", "job_shop_scheduling", "maximum_independent_set", "multi_demand_multidimensional_knapsack_problem", "multidimensional_knapsack_problem", "open_shop_scheduling", "packing_unequal_circles", "packing_unequal_circles_area", "packing_unequal_rectangles_and_squares", "packing_unequal_rectangles_and_squares_area", "resource_constrained_shortest_path", "set_covering", "set_partitioning", "traveling_salesman_problem", "uncapacitated_warehouse_location", "unconstrained_guillotine_cutting", "period_vehicle_routing_problem", "capacitated_p_median_problem", "uncapacitated_p_median_problem"], "contribution_type": ["new_benchmark", "empirical_study", "framework"], "framework_lineage": null, "specific_domain": null, "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Algorithm Search for Combinatorial Optimization", "short": "ASCO", "class_": "llm_evolutionary_search", "properties": ["NP-hard", "discrete", "structured", "constraint-rich"], "scale": "up to 11000 variables"}, "lineage": {"direct_ancestors": [{"paper": "Romera-Paredes et al. 2023", "relationship": "benchmarks and evaluates FunSearch"}, {"paper": "Liu et al. 2024", "relationship": "benchmarks and evaluates Evolution of Heuristics (EoH)"}, {"paper": "Ye et al. 2024", "relationship": "benchmarks and evaluates ReEvo"}, {"paper": "Jiang et al. 2025", "relationship": "benchmarks and evaluates AIDE"}, {"paper": "Zheng et al. 2025", "relationship": "benchmarks and evaluates MSTC-AHD"}], "closest_prior_work": "Romera-Paredes et al. 2023", "novelty_type": "new_problem"}, "extensions": {"next_steps": ["Enhance LLM agents' problem comprehension for CO tasks.", "Improve LLM agents' creative reasoning abilities for CO tasks.", "Develop agents that can consistently improve under longer search budgets.", "Address limitations in handling feasibility constraints and generating efficient solutions."], "transferable_to": ["Automated algorithm design for other scientific domains.", "LLM-driven research agents for general problem-solving.", "Benchmarking of LLM agents in other complex engineering tasks."], "open_weaknesses": ["Limited algorithmic novelty from LLM agents.", "Insufficient handling of feasibility constraints by agents.", "Struggle with solution reliability and valid solution rates.", "Underdeveloped planning mechanisms in current agents.", "Lack of knowledge in agents to select best-performing classical solvers."]}, "artifacts": {"code_url": "https://github.com/sunnweiwei/CO-Bench", "models_released": false, "new_benchmark": true}, "front_id": "generative_ai_for_or_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.5662, "is_bridge": true, "priority_score": 6.26, "experiments": {"benchmarks": ["Aircraft landing", "Assignment problem", "Assortment problem", "Bin packing - one-dimensional", "Capacitated warehouse location", "Common due date scheduling", "Constrained guillotine cutting", "Constrained non-guillotine cutting", "Container loading", "Container loading with weight restrictions", "Corporate structuring", "Crew scheduling", "Equitable partitioning problem", "Euclidean Steiner problem", "Flow shop scheduling", "Generalised assignment problem", "Graph colouring", "Hybrid Reentrant Shop Scheduling", "Job shop scheduling", "MIS", "Multi-Demand Multidimensional Knapsack problem", "Multidimensional knapsack problem", "Open shop scheduling", "Packing unequal circles", "Packing unequal circles area", "Packing unequal rectangles and squares", "Packing unequal rectangles and squares area", "Resource constrained shortest path", "Set covering", "Set partitioning", "TSP", "Uncapacitated warehouse location", "Unconstrained guillotine cutting", "Vehicle routing: period routing", "p-median - capacitated", "p-median - uncapacitated"], "baselines": ["Classical Solver", "Llama-3.3-70B-Instruct", "GPT-4o-mini", "Qwen-2.5-Code-32B-Instruct", "GPT-4o", "DeepSeek-V3", "QwQ-32B", "DeepSeek-R1", "Grok-3-Thinking", "Gemini 2.5 Pro", "o3-mini-medium", "o1-high", "o1-medium", "o3-mini-high", "Claude-3.7 Sonnet", "DIMES (Qiu, Sun, and Yang 2022)", "DIFUSCO (Sun and Yang 2023)", "T2T (Li et al. 2023)", "LEHD + ReEvo (Ye et al. 2024)"], "hardware": "single CPU core of a dual AMD EPYC 7313 16-Core processor", "instance_sizes": [500, 1000, 10000, 11000]}, "results": {"vs_baselines": {"FunSearch (o3-mini)": "+5.65% Avg Score vs Classical Solver (0.797)", "Greedy Refine (o3-mini)": "+5.39% Avg Score vs Classical Solver (0.797)", "EoH (o3-mini)": "+5.39% Avg Score vs Classical Solver (0.797)", "ReEvo (o3-mini)": "-2.89% Avg Score vs Classical Solver (0.797)", "BestOfN (o3-mini)": "-2.89% Avg Score vs Classical Solver (0.797)", "MSTC-AHD (o3-mini)": "-4.39% Avg Score vs Classical Solver (0.797)", "AIDE (o3-mini)": "-5.52% Avg Score vs Classical Solver (0.797)", "Claude-3.7 Sonnet": "-18.32% Avg Score vs Classical Solver (0.797)", "neural solvers (DIMES, DIFUSCO, T2T, LEHD + ReEvo)": "Competitive performance, often outperforming existing neural solvers under similar time budget and approaching best results given extended search time."}, "scalability": "All agents improve with more iteration steps, but performance saturates after ~30 steps. LLMs excel in applying established techniques to improve efficiency and implementation quality.", "statistical_rigor": "Scores are averaged across all test instances for each problem, then averaged across all 36 problems. No explicit mention of multiple runs for agents or significance tests.", "limitations_acknowledged": ["limited algorithmic novelty", "insufficient handling of feasibility constraints", "struggle with solution feasibility and reliability", "current planning mechanisms in agents are still underdeveloped and may not reliably outperform random sampling", "agents often struggle to understand the problem constraints", "lack the necessary knowledge to select the best-performing solver"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2405.17743", "arxiv_url": "https://arxiv.org/abs/2405.17743", "title": "ORLM: A Customizable Framework in Training Large Models for Automated Optimization Modeling", "authors": ["Chenyu"], "abstract": "", "published_date": "2025-04-04", "affiliations": "Columbia University, Duke University, Shanghai Jiao Tong University, The Chinese University of Hong Kong, Shenzhen, Shenzhen Research Institute of Big Data, Shanghai University of Finance and Economics, Cardinal Operations", "category": "OR for Generative AI", "relevance": {"methodological": 5, "problem": 8, "inspirational": 6}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper introduces 'IndustryOR' and a synthetic data pipeline that directly competes with or complements our active 'OR-Bench' project. We must evaluate their benchmark and potentially adopt their data synthesis prompts to scale our own datasets."}, "brief": "The authors propose OR-Instruct, a framework that uses GPT-4 to synthesize over 32k optimization modeling pairs (natural language to COPT code) to fine-tune 7B-scale models (ORLM). They demonstrate that these fine-tuned models outperform GPT-4 on their new 'IndustryOR' benchmark, a result that appears robust given the specialized nature of the task. The most valuable takeaway is their specific data augmentation strategy—iteratively altering constraints and injecting specific modeling techniques (e.g., Big M)—which provides a concrete recipe we can steal to generate diverse instances for our OR-Bench project. While the methodology is standard instruction tuning, the resulting artifacts (benchmark and model) establish a new baseline for automated OR modeling that we cannot ignore.", "methodology": {"core_method": "Instruction tuning of open-source LLMs using semi-automated synthetic data generated by OR-Instruct framework", "llm_role": "data_synthesis, model_generator, code_writer", "llm_model_used": "GPT-4, Mistral-7B, Deepseek-Math-7B-Base, LLaMA-3-8B, Qwen-2.5-7B", "search_type": "constructive", "novelty_claim": "First to propose a viable path for training open-source LLMs that are capable of optimization modeling and developing solver codes.", "components": ["OR-Instruct data synthesis framework", "Iterative bootstrapping algorithm", "Data expansion (using GPT-4)", "Data augmentation (altering objectives/constraints, rephrasing questions, incorporating multiple techniques)", "Heuristic filtering", "Instruction tuning", "COPT solver integration"], "training_required": true}, "tags": {"methods": ["instruction_tuning", "llm_fine_tuned", "synthetic_data_generation", "data_augmentation", "bootstrapping", "heuristic_filtering", "program_synthesis", "llm_as_model_generator", "llm_code_generation", "reinforcement_learning"], "problems": ["automated_optimization_modeling", "mathematical_modeling", "linear_programming", "integer_programming", "mixed_integer_programming", "non_linear_programming", "multi_objective_programming", "quadratic_programming", "dynamic_programming", "stochastic_programming", "code_generation"], "contribution_type": ["new_method", "new_benchmark", "sota_result", "framework", "empirical_study"], "framework_lineage": "or_instruct", "specific_domain": "automated_optimization_modeling", "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Automated Optimization Modeling", "short": "AOM", "class_": "optimization_modeling_automation", "properties": ["linear_programming", "integer_programming", "mixed_integer_programming", "non_linear_programming", "multi_objective_programming", "quadratic_programming", "dynamic_programming", "stochastic_programming"], "scale": "small to medium-scale instances (up to ~30 variables/constraints)"}, "lineage": {"direct_ancestors": [{"paper": "OptiMUS: Scalable optimization modeling with (MI)LP solvers and large language models", "relationship": "addresses limitations of proprietary LLMs in automated optimization modeling"}, {"paper": "Chain-of-Experts: When LLMs meet complex operations research problems", "relationship": "addresses limitations of proprietary LLMs in automated optimization modeling"}, {"paper": "NL4Opt competition: Formulating optimization problems based on their natural language descriptions", "relationship": "builds on benchmark and problem formulation task from"}, {"paper": "How far can camels go? Exploring the state of instruction tuning on open resources", "relationship": "adapts instruction tuning methodology from"}], "closest_prior_work": "OptiMUS: Scalable optimization modeling with (MI)LP solvers and large language models", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["incorporate reinforcement learning (RLHF, DPO, KTO) for ranking optimal solutions", "develop multi-agent collaboration and internal chain of thought structures", "construct new datasets for reinforcement learning (e.g., preference lists)", "develop efficient data exploration strategies to minimize reliance on large-scale data"], "transferable_to": ["supply_chain_management", "scheduling", "inventory_management", "operations_research_education", "mathematical_modeling_competitions"], "open_weaknesses": ["overly simplistic output for complex problems (low model completeness)", "weak ability to rank optimal solutions (Pass@1 vs Pass@8 gap)", "poor learning capability from data (marginal gains at later stages of data scaling)", "limited performance on non-linear programming and other rare question types", "semantic misunderstanding and errors in objective/constraint translation"]}, "artifacts": {"code_url": "https://github.com/cardinal-operations/orlm", "models_released": true, "new_benchmark": true}, "front_id": "or_for_generative_ai_2026-02-18_front_6", "front_status": "emerging", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.24, "experiments": {"benchmarks": ["NL4OPT", "MAMO (EasyLP)", "MAMO (ComplexLP)", "IndustryOR"], "baselines": ["tag-BART", "Standard (GPT-3.5)", "Reflexion (GPT-3.5)", "Chain-of-Experts (GPT-3.5)", "Standard (GPT-4)", "Reflexion (GPT-4)", "Chain-of-Experts (GPT-4)", "OptiMUS (GPT-4)", "Llama-3.1-Instruct (405B)", "DeepSeek-V2-Chat (236B)", "Qwen2-Instruct (72B)", "DeepSeek-R1-Distill (32B)", "Mistral-Nemo (12B)", "Senior Undergraduates", "Experts"], "hardware": "null", "instance_sizes": [289, 652, 211, 100]}, "results": {"vs_baselines": {"Standard (GPT-4)": "ORLM-LLaMA-3 outperforms Standard GPT-4 by 38.4% on NL4OPT, 15.8% on MAMO EasyLP, 22.8% on MAMO ComplexLP, and 10.0% on IndustryOR.", "Senior Undergraduates": "ORLM-LLaMA-3 outperforms Senior Undergraduates on NL4OPT and MAMO EasyLP, but underperforms on MAMO ComplexLP and IndustryOR."}, "scalability": "ORLM's accuracy shows an upward trend with increasing model size or data volume, following a power-law trend for micro average performance and model size scaling.", "statistical_rigor": "Statistical analysis (Shapiro-Wilk, Levene, Mann-Whitney U, t-test) with Hedge's g and statistical power confirms significant improvements (p<0.05, Hedge's g > 1, power > 0.8) in accuracy and time efficiency for human-AI collaboration.", "limitations_acknowledged": ["Overly simplistic output for complex problems", "Weak ability to rank optimal solutions (Pass@1 vs Pass@8 gap)", "Poor learning capability from data (marginal gains at later stages of data scaling)", "Limited performance on non-linear programming and other rare question types", "Errors in semantic understanding and objective/constraint translation"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2504.00613", "arxiv_url": "https://arxiv.org/abs/2504.00613", "title": "LLM-Guided Search for Deletion-Correcting Codes", "authors": ["Franziska"], "abstract": "", "published_date": "2025-04-01", "affiliations": "Technical University of Munich, Munich Center for Machine Learning", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 7, "problem": 4, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "The functional deduplication strategy is a concrete, high-ROI engineering improvement for our AlgoEvo pipeline that directly addresses our sample efficiency bottleneck."}, "brief": "Weindel and Heckel adapt FunSearch to discover priority functions for the Maximum Independent Set problem (applied to deletion-correcting codes), achieving new SOTA lower bounds for specific lengths (n=12, 13, 16). The critical takeaway for us is their **functional deduplication** step: they hash function outputs on a small subset of data to discard syntactically unique but logically identical programs, which significantly improves sample efficiency by preventing the evaluator from wasting cycles on 'comment changes' or variable renames. Additionally, they demonstrate that optimizing for the single hardest instance generalizes better than averaging performance across a curriculum—a counter-intuitive finding we should test in our reward modeling.", "methodology": {"core_method": "LLM-guided evolutionary search (FunSearch adaptation) for priority functions", "llm_role": "evolutionary_search", "llm_model_used": "StarCoder2-15B", "search_type": "improvement", "novelty_claim": "Proposes an LLM-guided evolutionary search (FunSearch adaptation) to find deletion-correcting codes, discovering new maximum-size codes for single deletions and improved lower bounds for two and three deletions.", "components": ["LLM", "Evolutionary Search Framework", "Program Database (islands, clusters)", "Evaluator (greedy independent set construction)", "Deduplication Step"], "training_required": false}, "tags": {"methods": ["llm_guided_evolutionary_search", "funsearch", "evolutionary_search", "program_synthesis", "greedy_algorithm", "deduplication", "few_shot_prompting"], "problems": ["deletion_correcting_codes", "independent_set", "code_design", "information_theory"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "funsearch", "specific_domain": "deletion_correcting_codes", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Constructing Deletion-Correcting Codes", "short": "DCC", "class_": "code_design", "properties": ["deletion_correcting", "binary_sequences", "fixed_deletions", "finite_length"], "scale": "n=6-25 for s=1, n=7-16 for s=2,3"}, "lineage": {"direct_ancestors": [{"paper": "Mathematical discoveries from program search with large language models", "relationship": "adapts FunSearch from"}], "closest_prior_work": "Mathematical discoveries from program search with large language models", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["scale evaluator for longer code lengths", "mathematically analyze discovered priority functions", "apply to other error types (e.g., insertion, substitution)", "find a single priority function optimal for both single and multiple deletions"], "transferable_to": ["other error_correcting_code problems", "combinatorial_optimization problems solvable by greedy search with priority functions", "automated_algorithm_design for information_theory problems"], "open_weaknesses": ["poor scalability of evaluator for moderate to large code lengths", "memory-prohibitive graph construction for larger instances", "no single priority function found that is optimal for both single and multiple deletions across all tested lengths"]}, "artifacts": {"code_url": "https://github.com/MLI-lab/FunDCC", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.89, "experiments": {"benchmarks": ["Deletion-correcting code instances (n,s) for s=1 (n=6-25), s=2 (n=7-16), s=3 (n=7-16)"], "baselines": ["Trivial lexicographic baseline", "Landjev and Haralambiev (LH07)", "Khajouei et al. (KZK11)", "Varshamov-Tenengolts (VT) codes"], "hardware": "NVIDIA A100 (80GB) or H100 (94GB) GPUs for LLM, 2 CPU cores per evaluator", "instance_sizes": [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 20, 25]}, "results": {"vs_baselines": {"Trivial lexicographic baseline": "Outperforms significantly across all (n,s) pairs.", "Landjev and Haralambiev (LH07)": "Outperforms for s=2 (n=12,13,16) and s=3 (n=11,13,14,16).", "Varshamov-Tenengolts (VT) codes": "Matches conjectured optimal sizes for s=1 up to n=25.", "Best known (s=2)": "Achieves new best-known sizes for n=12 (34 vs 32), n=13 (50 vs 49), n=16 (204 vs 201).", "Best known (s=3)": "Achieves new best-known sizes for n=11 (8 vs 7), n=13 (14 vs 12), n=14 (20 vs 15), n=16 (38 vs 31)."}, "scalability": "Evaluator's exponential complexity with code length limits scalability, especially with graph construction; however, discovered priority functions generalize to longer codes to some extent.", "statistical_rigor": "20+ evolutionary search runs with varying hyperparameters were conducted; success was measured as a binary outcome (finding an optimal function), with 2 out of 3 identical runs achieving optimality.", "limitations_acknowledged": ["Poor scalability of the evaluator for moderate to large code lengths", "Evaluator's exponential complexity with code length", "Memory-prohibitive graph construction for larger instances", "No single priority function found that is optimal for both single and multiple deletions across all tested lengths"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2503.10721", "arxiv_url": "https://arxiv.org/abs/2503.10721", "title": "From Understanding to Excelling: Template-Free Algorithm Design through Structural-Functional Co-Evolution", "authors": ["Zhe"], "abstract": "", "published_date": "2025-03-13", "affiliations": "Princeton University, Nanyang Technological University, City University of Hong Kong, University of Science and Technology of China, The Hong Kong University of Science and Technology (Guangzhou)", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 9, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper directly targets the primary bottleneck of our current research (FunSearch/EoH's reliance on fixed templates) by introducing a 'structural' evolution layer. It effectively proposes the next generation of the architecture we are currently building."}, "brief": "Zhao et al. propose CAE, a framework that co-evolves algorithm structure (workflow/call graphs) alongside function implementations, aiming to eliminate the fixed templates required by SOTA methods like FunSearch and EoH. On TSP benchmarks, they report reducing optimality gaps by ~2-5% compared to ReEvo, and in quadratic optimization, the system autonomously discovered numerical stability fixes (e.g., replacing matrix inversion with solvers) that human baselines missed. The critical takeaway is the 'bi-dimensional co-evolution' strategy: explicitly maintaining and mutating a population of control flow graphs separate from the function bodies, which allows the system to escape the local optima imposed by a fixed human-designed harness. We must evaluate if this structural search approach can be integrated into AlgoEvo to automate our harness design.", "methodology": {"core_method": "LLM-driven bi-dimensional structural-functional co-evolutionary algorithm", "llm_role": "code_writer, heuristic_generator, decomposition_guide, prompt_optimizer", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "The framework achieves template-free algorithm design through bi-dimensional structural-functional co-evolution, enabling global optimization and innovative structural design beyond fixed templates.", "components": ["LLM-based semantic understanding", "LLM-based code generation", "Validation mechanism", "Bi-dimensional co-evolution", "Functional dimension optimization", "Structural dimension optimization", "Reflection modules", "Crossover", "Mutation", "Rewriting", "Function combination"], "training_required": false}, "tags": {"methods": ["llm_code_generation", "llm_as_heuristic", "llm_in_the_loop", "evolutionary_algorithm", "genetic_algorithm", "evolution_of_heuristics", "program_synthesis", "hyper_heuristics", "metaheuristics", "numerical_stability", "gradient_correction", "adaptive_scaling"], "problems": ["algorithm_discovery", "heuristic_evolution", "tsp", "cvrp", "quadratic_optimization", "expensive_continuous_optimization"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "cae", "specific_domain": null, "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Template-Free Algorithm Design", "short": "AAD", "class_": "algorithm_design", "properties": ["template-free", "structural-functional co-evolution", "global optimization", "adaptive", "end-to-end"], "scale": "20-439 nodes (TSP), 1000 variables (Quadratic Optimization)"}, "lineage": {"direct_ancestors": [{"paper": "Evolution of heuristics: Towards efficient automatic algorithm design using large language model", "relationship": "extends and overcomes limitations of"}, {"paper": "Mathematical discoveries from program search with large language models", "relationship": "extends and overcomes limitations of"}, {"paper": "Reevo: Large language models as hyper-heuristics with reflective evolution", "relationship": "extends and overcomes limitations of"}], "closest_prior_work": "Evolution of heuristics: Towards efficient automatic algorithm design using large language model", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["develop_lightweight_llm_variants_or_hybrid_methods", "integrate_parallel_and_distributed_optimization", "develop_self_learning_systems_for_continual_improvement", "integrate_with_broader_scientific_discovery_pipelines"], "transferable_to": ["other_combinatorial_optimization_problems", "other_continuous_optimization_problems", "general_scientific_discovery_pipelines", "automated_algorithm_design_for_ml_algorithms"], "open_weaknesses": ["computational_efficiency_for_extensive_llm_queries", "computational_efficiency_for_intensive_validation_steps", "lack_of_self_learning_and_continual_improvement", "need_for_explainable_ai_in_generated_solutions", "addressing_ethical_considerations_bias_fairness_safety"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.41, "experiments": {"benchmarks": ["ts225", "eil51", "d657", "rat99", "d493", "kroA150", "rl1889", "kroB100", "fl1577", "u1817", "kroC100", "u724", "d1655", "ch130", "pr264", "bier127", "pr299", "pr226", "lin318", "fl417", "pr439", "TSP20", "TSP50", "TSP100", "Quadratic Optimization"], "baselines": ["GHPP [41]", "ReEvo [4]", "GA", "GA+EOH [1]", "GA+ReEvo [4]", "ACO", "ACO+EOH [1]", "ACO+ReEvo [4]", "KGLS [42]", "KGLS+EOH", "KGLS+ReEvo", "IQN", "SLIQN", "LISR-1", "LISR-k"], "hardware": "null", "instance_sizes": [20, 50, 51, 99, 100, 127, 130, 150, 225, 226, 264, 299, 318, 417, 439, 493, 657, 724, 1000, 1577, 1655, 1817, 1889]}, "results": {"vs_baselines": {"GHPP [41]": "CAE achieves significantly lower optimality gaps (e.g., 4.6% vs 7.7% on ts225, 3.5% vs 10.2% on eil51)", "ReEvo [4]": "CAE achieves significantly lower optimality gaps (e.g., 4.6% vs 6.6% on ts225, 3.5% vs 6.5% on eil51)", "GA": "CAE achieves higher optimality gaps (e.g., 6.6% vs 0.0% on TSP20, 10.3% vs 0.0% on TSP50)", "GA+EOH [1]": "CAE achieves higher optimality gaps (e.g., 6.6% vs 1.9% on TSP20, 10.3% vs 2.3% on TSP50)", "GA+ReEvo [4]": "CAE achieves higher optimality gaps (e.g., 6.6% vs 1.9% on TSP20, 10.3% vs 1.3% on TSP50)", "ACO": "ACO+CAE achieves higher optimality gaps (e.g., 0.5% vs 0.0% on TSP20, 1.8% vs 0.0% on TSP50)", "ACO+EOH [1]": "ACO+CAE achieves higher optimality gaps (e.g., 0.5% vs -0.7% on TSP20, 1.8% vs 0.8% on TSP50)", "ACO+ReEvo [4]": "ACO+CAE achieves higher optimality gaps (e.g., 0.5% vs -0.2% on TSP20, 1.8% vs 0.3% on TSP50)", "KGLS [42]": "KGLS+CAE achieves higher optimality gaps (e.g., 11.2% vs 0.0% on TSP20, 11.7% vs 0.0% on TSP50)", "KGLS+EOH": "KGLS+CAE achieves higher optimality gaps (e.g., 11.2% vs 0.6% on TSP20, 11.7% vs -0.2% on TSP50)", "KGLS+ReEvo": "KGLS+CAE achieves higher optimality gaps (e.g., 11.2% vs 0.2% on TSP20, 11.7% vs -1.0% on TSP50)", "IQN": "LISR-k(CAE) shows faster convergence and better objective values, especially for ill-conditioned problems (e.g., optimal convergence within 50s for ξ=12, while IQN plateaus early).", "SLIQN": "LISR-k(CAE) shows faster convergence and better objective values, especially for ill-conditioned problems (e.g., optimal convergence within 50s for ξ=12, while SLIQN plateaus early).", "LISR-1": "LISR-k(CAE) shows faster convergence and better objective values, especially for ill-conditioned problems.", "LISR-k": "LISR-k(CAE) shows faster convergence and better objective values, especially for ill-conditioned problems."}, "scalability": "The framework demonstrates strong scalability and adaptability, effectively addressing both small-scale and large-scale problems and maintaining robust performance under varying complexity and ill-conditioning.", "statistical_rigor": "Results are averaged over 3 independent runs with different starting nodes to ensure robustness and consistency.", "limitations_acknowledged": ["Computational efficiency for extensive LLM queries", "Computational efficiency for intensive validation steps", "Lack of self-learning and continual improvement mechanisms", "Limited integration with broader scientific discovery pipelines", "Need for explainable AI", "Ethical considerations (bias, fairness, safety)"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2503.09357", "arxiv_url": "https://arxiv.org/abs/2503.09357", "title": "Automatic Operator-level Parallelism Planning for Distributed Deep Learning -- A Mixed-Integer Programming Approach", "authors": ["Ruifeng"], "abstract": "", "published_date": "2025-03-12", "affiliations": "Huawei", "category": "OR for Generative AI", "relevance": {"methodological": 7, "problem": 8, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper provides a concrete, successful MIP formulation for LLM parallelism that beats expert human designs (DeepSeek DualPipe). It directly validates and informs our 'GPUSched' and 'OR formulations for AI systems' work, offering a specific graph-coarsening strategy to make the MIP tractable."}, "brief": "She et al. formulate distributed LLM training/inference as a Flexible Distributed Job Shop Scheduling Problem (FDJSSP) solved via Mixed-Integer Programming (MIP) combined with a heuristic graph coarsening step. They demonstrate that this automated approach not only reproduces DeepSeek V3's expert-designed \"DualPipe\" strategy but, when allowed to search longer, discovers a schedule with 50% fewer pipeline bubbles. The primary takeaway is the effectiveness of the bi-level optimization framework (greedy merging + MIP) to handle the scale of operator-level graphs, proving that formal OR methods can outperform manual system design for LLM infrastructure. This is a mandatory read for our GPUSched project, offering a concrete formulation for operator-level constraints we can directly adapt.", "methodology": {"core_method": "Mixed-Integer Programming (MIP) formulation with a bi-level solution framework including a heuristic operation merging step", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "We propose formulating parallelism planning as a scheduling optimization problem using mixed-integer programming and a bi-level solution framework balancing optimality with computational efficiency, automatically generating effective distributed plans for heterogeneous neural networks and hardware constraints.", "components": ["Mixed-Integer Programming (MIP) formulation", "Bi-level solution framework", "Heuristic operation merging (Algorithm 1)", "Commercial MIP solver (Gurobi)"], "training_required": false}, "tags": {"methods": ["mixed_integer_programming", "bi_level_optimization", "heuristic_search", "graph_partitioning", "gurobi"], "problems": ["distributed_deep_learning_scheduling", "llm_training_optimization", "llm_inference_optimization", "job_shop_scheduling", "flexible_distributed_job_shop_scheduling", "resource_allocation", "makespan_minimization", "memory_management"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": null, "specific_domain": "llm_training_and_inference_scheduling", "llm_coupling": null}, "problem": {"formal_name": "Automatic Operator-level Parallelism Planning for Distributed Deep Learning", "short": "FDJSSP", "class_": "scheduling", "properties": ["operator_level", "heterogeneous_neural_networks", "hardware_constraints", "memory_constraints", "non_linear_architectures", "data_dependencies", "communication_costs", "non_preemptive", "flexible", "distributed"], "scale": "2-8 machines, 40-200 operations"}, "lineage": {"direct_ancestors": [{"paper": "DeepSeek-AI, “Deepseek-v3 technical report,” 2024.", "relationship": "reproduces and improves upon DualPipe strategy from"}], "closest_prior_work": "DeepSeek-AI, “Deepseek-v3 technical report,” 2024.", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["incorporating heterogeneous hardware considerations", "dynamic load balancing for unpredictable workloads", "automated exploration of hybrid parallelization strategies", "improving graph partitioning heuristic"], "transferable_to": ["general distributed computing scheduling", "cloud resource scheduling", "manufacturing scheduling"], "open_weaknesses": ["scalability for very large models due to MIP complexity", "greedy nature of heuristic operation merging", "assumes homogeneous machines and communication channels for simplicity"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_0", "front_status": "emerging", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.99, "experiments": {"benchmarks": ["DeepSeek V3 pipeline parallelism configurations", "Random computational graph"], "baselines": ["DeepSeek’s DualPipe"], "hardware": "PC with an Intel Core i7 processor, 32 GB of RAM", "instance_sizes": [2, 4, 8, 40, 200]}, "results": {"vs_baselines": {"Dualpipe": "Comparable or superior performance, reducing computational bubbles by half under the same memory constraints."}, "scalability": "Computational complexity grows significantly with operations; a heuristic pre-merges operations to reduce problem size to fewer than 100 nodes for efficient MIP solving.", "statistical_rigor": "None explicitly reported; results presented via Gantt charts and direct comparisons of pipeline bubble counts.", "limitations_acknowledged": ["Heuristic operation merging is inherently greedy and may not find optimal graph partitioning", "Future work needed for heterogeneous hardware considerations", "Future work needed for dynamic load balancing for unpredictable workloads", "Future work needed for automated exploration of hybrid parallelization strategies"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2503.08796", "arxiv_url": "https://arxiv.org/abs/2503.08796", "title": "Robust Multi-Objective Controlled Decoding of Large Language Models", "authors": ["Seongho"], "abstract": "", "published_date": "2025-03-11", "affiliations": "University College London, University of Basel, Ulsan National Institute of Science and Technology", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 6, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper bridges robust optimization (OR) and process reward models (LLM search) via a clean game-theoretic formulation. It offers a concrete, mathematically sound algorithm for dynamic multi-objective balancing that we can port directly to our evolutionary search and multi-agent optimization frameworks."}, "brief": "RMOD formulates multi-objective decoding as a zero-sum game between a policy and adversarial weights, solving a convex optimization problem at each decoding step to maximize the worst-case value estimate (essentially a Process Reward Model). The results are empirically strong, outperforming MO-DPO and scalarized baselines on alignment benchmarks by dynamically preventing any single objective from collapsing. **Key Takeaway:** The efficient inference-time weight optimization algorithm (Eq. 10) is a 'stealable' mechanism for **AlgoEvo** and **RobustMAS**. We should implement this dynamic adversarial weighting to balance conflicting code metrics (e.g., runtime vs. solution quality) during evolutionary search, replacing our current static scalarization methods.", "methodology": {"core_method": "Maximin two-player game between adversarially computed reward weights and sampling policy, solvable through Nash equilibrium, reduced to convex optimization, with blockwise best-of-K sampling", "llm_role": "controlled_decoding_target", "llm_model_used": "gemma-2-2b-it", "search_type": "hybrid", "novelty_claim": "We introduce Robust Multi-Objective Decoding (RMOD), a novel inference-time algorithm that robustly aligns Large Language Models (LLMs) to multiple human objectives (e.g., instruction-following, helpfulness, safety) by maximizing the worst-case rewards.", "components": ["Value functions (trained via reward models)", "Reference policy (πref)", "Iterative weight update (projected gradient descent)", "Block-wise best-of-K sampling", "KL divergence regularization"], "training_required": true}, "tags": {"methods": ["controlled_decoding", "multi_objective_optimization", "robust_optimization", "game_theory", "maximin_optimization", "convex_optimization", "gradient_descent", "best_of_k_sampling", "blockwise_decoding", "reward_modeling", "value_function_learning", "llm_as_evaluator", "supervised_fine_tuning"], "problems": ["llm_alignment", "instruction_following", "helpfulness", "safety", "truthfulness", "honesty", "multi_objective_decision_making"], "contribution_type": ["new_method", "empirical_study", "sota_result", "framework"], "framework_lineage": "controlled_decoding", "specific_domain": "llm_alignment", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Robust Multi-Objective Controlled Decoding of Large Language Models", "short": "RMOD", "class_": "llm_alignment", "properties": ["robust", "multi_objective", "inference_time", "controlled_decoding", "maximin_game"], "scale": "up to 10 objectives, up to 256 tokens per response, block sizes 4-256 tokens"}, "lineage": {"direct_ancestors": [{"paper": "Controlled Decoding (Mudgal et al., 2023)", "relationship": "extends blockwise decoding and inference-time alignment from"}, {"paper": "CD-FUDGE (Yang & Klein, 2021)", "relationship": "adopts value function training methodology from"}], "closest_prior_work": "Controlled Decoding (Mudgal et al., 2023)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Investigate dynamic tuning of the trade-off parameter lambda", "Improve value function prediction accuracy for very short blocks", "Scale RMOD to an even larger number of objectives", "Explore RMOD's application in other LLM generation tasks"], "transferable_to": ["Other multi-objective decision-making problems with generative components", "Other generative AI models (e.g., image, code generation) requiring robust multi-objective control", "Real-time adaptive control systems"], "open_weaknesses": ["Performance degradation as the number of objectives increases", "Challenges in value function prediction for very short blocks", "Dependence on accurate pre-trained reward models/value functions", "Potential for reward overoptimization in best-of-K variants"]}, "artifacts": {"code_url": "https://github.com/williambankes/robust-multi-objective-decoding", "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_32", "front_status": "growing", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.84, "experiments": {"benchmarks": ["Anthropic Helpfulness-Harmless (HH)", "UltraFeedback", "ValuePrism"], "baselines": ["Reference", "CD-harmless", "CD-helpful", "CD-uniform", "GRPO", "MO-GRPO", "DPO", "MO-DPO", "MOD", "RS", "DISTILL-RMOD", "MAXMIN-GRPO"], "hardware": "single A100 80GB GPU", "instance_sizes": [1000, 1024]}, "results": {"vs_baselines": {"DISTILL-RMOD": "RMOD +1.2% WCWR (LLM-as-Judge)", "CD-UNIFORM": "RMOD +1.5% WCWR (LLM-as-Judge) and +~0.17 worst-case reward (HH)", "MO-GRPO": "RMOD +4.5% WCWR (LLM-as-Judge)", "MO-DPO": "RMOD +6.3% WCWR (LLM-as-Judge)", "Reference": "RMOD +~0.77 worst-case reward (HH)"}, "scalability": "RMOD scales effectively with the number of objectives, consistently outperforming baselines, though overall performance decreases as the number of objectives increases.", "statistical_rigor": "Average win rate across 1024 test prompts for HH and 1000 for UltraFeedback/ValuePrism; latency measured on 100 prompts.", "limitations_acknowledged": ["Performance decreases with increasing number of objectives", "Value function prediction accuracy decreases for very short blocks"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2406.01566", "arxiv_url": "https://arxiv.org/abs/2406.01566", "title": "Helix: Serving Large Language Models over Heterogeneous GPUs and Network via Max-Flow", "authors": ["Yixuan"], "abstract": "", "published_date": "2025-03-05", "affiliations": "Carnegie Mellon University", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper is a definitive reference for our 'GPUSched' and 'OR formulations for AI systems' active projects. It successfully demonstrates that rigorous OR formulations (Max-Flow/MILP) significantly outperform standard systems heuristics for LLM serving, validating our research thesis."}, "brief": "Helix formulates distributed LLM serving on heterogeneous clusters as a max-flow problem, using MILP to optimize model placement and deriving a per-request weighted round-robin scheduler from the flow solution. Unlike standard static pipeline parallelism, it routes every request dynamically based on edge capacities, achieving up to 3.3x throughput gains over Swarm on mixed GPU clusters (L4/T4/A100). The results are rigorous, backed by both physical cluster experiments and high-fidelity simulations. The critical takeaway is the 'per-request pipeline' abstraction: decoupling request routing from static device assignment allows exact OR methods to maximize utilization of weaker hardware—a technique we should immediately evaluate for our GPUSched project.", "methodology": {"core_method": "Max-flow problem formulation on directed, weighted graphs with Mixed Integer Linear Programming (MILP) for joint model placement and per-request pipeline scheduling", "llm_role": "none", "llm_model_used": null, "search_type": "exact", "novelty_claim": "Helix formulates LLM inference over heterogeneous GPUs and networks as a max-flow problem and uses MILP to jointly optimize model placement and per-request scheduling for high-throughput and low-latency serving.", "components": ["Max-Flow formulation", "Mixed Integer Linear Programming (MILP) algorithm", "Per-request pipelines", "Interleaved Weighted Round-Robin (IWRR) scheduler", "KV-Cache Estimation", "Cluster pruning", "Heuristic-guided MILP initialization"], "training_required": false}, "tags": {"methods": ["max_flow_optimization", "milp", "pipeline_parallelism", "tensor_parallelism", "scheduling_algorithms", "weighted_round_robin", "kv_cache_management", "dynamic_batching", "milp_acceleration", "heuristic_initialization"], "problems": ["llm_serving_optimization", "model_placement", "resource_scheduling"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": null, "specific_domain": "llm_serving_optimization", "llm_coupling": null}, "problem": {"formal_name": "Large Language Model Serving over Heterogeneous GPUs and Network", "short": "Heterogeneous LLM Serving", "class_": "distributed_systems_optimization", "properties": ["high_throughput", "low_latency", "heterogeneous_gpus", "heterogeneous_network", "geo_distributed", "model_placement", "request_scheduling"], "scale": "24-42 GPU nodes, LLaMA 30B-70B, up to 7 node types"}, "lineage": {"direct_ancestors": [{"paper": "vLLM [22]", "relationship": "builds on vLLM for LLM inference optimizations"}], "closest_prior_work": "Petals [5]", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Scaling MILP to hundreds or thousands of nodes using heuristic partitioning", "Integrating with other LLM serving optimizations (e.g., speculative inference)", "Dynamic adaptation to changing cluster conditions (GPU availability, network bandwidth)", "Exploring LP relaxation with robust rounding for faster MILP solutions"], "transferable_to": ["Distributed training of large ML models on heterogeneous clusters", "Serving other large deep learning models (e.g., diffusion models) on heterogeneous infrastructure", "General resource allocation and scheduling in heterogeneous distributed computing environments"], "open_weaknesses": ["MILP solving time for very large clusters", "Higher latency compared to systems using only high-end GPUs (inherent to using less powerful GPUs)", "KV-cache estimation based on average output length might be suboptimal for highly variable request patterns"]}, "artifacts": {"code_url": "https://github.com/Thesys-lab/Helix-ASPLOS25", "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_status": "growing", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.26, "experiments": {"benchmarks": ["Azure Conversation dataset"], "baselines": ["Swarm [45]", "separate pipelines (SP)", "Petals [5]", "random scheduling", "Shortest Queue First scheduling (SQ)"], "hardware": "NVIDIA A100, L4, T4, V100 GPUs; 10Gb/s intra-cluster network; 100Mb/s inter-cluster network; 14-core CPU for MILP solver", "instance_sizes": [30, 70, 24, 42]}, "results": {"vs_baselines": {"Swarm": "up to 3.3x decode throughput (online, high-heterogeneity cluster, LLaMA 70B), up to 66% prompt latency reduction (online, geo-distributed, LLaMA 30B)", "separate pipelines (SP)": "up to 3.29x decode throughput (online, high-heterogeneity cluster, LLaMA 70B)", "Petals": "up to 1.49x decode throughput (offline, geo-distributed, LLaMA 70B)", "random scheduling": "up to 29% decode throughput (offline, single cluster, LLaMA 70B)", "Shortest Queue First scheduling (SQ)": "up to 19% decode throughput (offline, geo-distributed, LLaMA 70B)"}, "scalability": "Achieves consistently high performance with increasing GPU heterogeneity and scales linearly with number of nodes and connections in MILP formulation, with potential for heuristic partitioning for very large clusters.", "statistical_rigor": "Results report median, 5th, 25th, 75th, and 95th percentiles from 30-minute (online) or 10-minute (offline) test runs after warm-up.", "limitations_acknowledged": ["Higher latency compared to systems using only high-end GPUs due to use of less powerful GPUs (T4, L4).", "MILP solver may take hours for large clusters, suggesting heuristic partitioning for hundreds/thousands of nodes."]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2406.04508", "arxiv_url": "https://arxiv.org/abs/2406.04508", "title": "OCCAM: Towards Cost-Efficient and Accuracy-Aware Classification Inference", "authors": ["Dujian"], "abstract": "", "published_date": "2025-02-25", "affiliations": "University of British Columbia", "category": "OR for Generative AI", "relevance": {"methodological": 6, "problem": 7, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper provides a clean, rigorous OR formulation (ILP) for dynamic model selection, which directly aligns with our 'OR formulations for AI systems' and 'LLM serving optimization' tracks. The training-free estimator is a transferable technique for our resource allocation problems."}, "brief": "OCCAM formulates the inference model selection problem as an Integer Linear Program (ILP), using a nearest-neighbor estimator on validation embeddings to predict query-specific model accuracy. The authors provide theoretical guarantees for the estimator's bias and variance, demonstrating 40% cost reduction on ImageNet with <1% accuracy drop compared to heuristic baselines. The key takeaway is the **training-free, NNS-based accuracy estimator** combined with ILP; this avoids training complex routers and provides statistical guarantees. This is directly applicable to our **LLM serving optimization** (GPUSched) work for routing prompts between models of varying costs, and potentially for estimating fitness in **AlgoEvo** without full execution.", "methodology": {"core_method": "Integer Linear Programming for optimal classifier assignment with nearest neighbor-based accuracy estimation and variance regularization", "llm_role": "none", "llm_model_used": null, "search_type": "exact", "novelty_claim": "We propose a novel training-free approach, OCCAM, that uses an unbiased and low-variance accuracy estimator with asymptotic guarantees to compute the optimal assignment of diverse classifiers to given queries under user-specified cost budgets.", "components": ["unbiased accuracy estimator", "low-variance accuracy estimator", "nearest neighbor search", "Integer Linear Programming solver", "variance regularization"], "training_required": false}, "tags": {"methods": ["integer_linear_programming", "nearest_neighbor_search", "accuracy_estimation", "variance_regularization", "algorithm_selection", "hybrid_inference", "lipschitz_continuity"], "problems": ["optimal_model_portfolio_problem", "classification_inference_optimization", "algorithm_selection", "cost_constrained_optimization", "image_classification"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study", "theoretical_result"], "framework_lineage": null, "specific_domain": "image_classification", "llm_coupling": null}, "problem": {"formal_name": "Optimal Model Portfolio Problem", "short": "OMPP", "class_": "algorithm_selection", "properties": ["cost-constrained", "accuracy-maximization", "multi-model", "classification_inference"], "scale": "10-1000 classes, 10,000 queries, 7-101 classifiers"}, "lineage": {"direct_ancestors": [{"paper": "Chen et al., 2022", "relationship": "improves upon model-based accuracy prediction and lack of guarantees"}, {"paper": "Ding et al., 2022", "relationship": "extends hybrid inference from two models to multiple models with explicit cost budgets"}, {"paper": "Ding et al., 2024", "relationship": "extends hybrid inference from two models to multiple models with explicit cost budgets"}, {"paper": "Kag et al., 2022", "relationship": "extends hybrid inference from two models to multiple models with explicit cost budgets"}], "closest_prior_work": "Chen et al., 2022", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["adapt to non-well_separated_tasks", "accommodate data_drifts_or_out_of_distribution_data", "optimize for min_accuracy instead of average_accuracy", "further reduce accuracy_estimation_errors"], "transferable_to": ["other_classification_tasks", "general_resource_constrained_decision_making", "dynamic_model_selection_for_inference"], "open_weaknesses": ["assumes_well_separated_tasks", "assumes_iid_data", "optimizes_average_accuracy_not_min_accuracy", "estimation_error_room_for_improvement"]}, "artifacts": {"code_url": "https://github.com/DujianDing/OCCAM.git", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.03, "experiments": {"benchmarks": ["CIFAR-10", "CIFAR-100", "Tiny ImageNet", "ImageNet-1K"], "baselines": ["Single Best", "Random", "FrugalMCT [Chen et al., 2022]"], "hardware": "one NVIDIA V100 GPU of 32GB GPU RAM", "instance_sizes": [10, 100, 200, 1000, 7, 101, 10000]}, "results": {"vs_baselines": {"Single Best": "OCCAM achieves 40% cost reduction with 0.51% accuracy drop on CIFAR10, compared to 2.22% for Single Best. On ImageNet-1K (101 models), OCCAM is 8.35% more accurate at 40% cost reduction.", "Random": "OCCAM achieves 40% cost reduction with 0.51% accuracy drop on CIFAR10, compared to 2.86% for Random. On ImageNet-1K (101 models), OCCAM is 6.25% more accurate at 40% cost reduction.", "FrugalMCT": "OCCAM achieves 40% cost reduction with 0.51% accuracy drop on CIFAR10, compared to 1.22% for FrugalMCT. OCCAM is 1.1% more accurate than FrugalMCT on Tiny ImageNet at 40% cost reduction. FrugalMCT did not finish on 101 models."}, "scalability": "OCCAM's accuracy improves with increasing sample size (K*s) and maintains effectiveness across various problem difficulties (10-1000 classes) and number of classifiers (7-101).", "statistical_rigor": "Results are averaged over K i.i.d. samples; variance of the estimator is shown to be asymptotically low-variance (proportional to 1/√K).", "limitations_acknowledged": ["assumes well-separated tasks", "assumes IID data", "optimizes average accuracy (not min accuracy)"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2410.06270", "arxiv_url": "https://arxiv.org/abs/2410.06270", "title": "Mixture Compressor for Mixture-of-Experts LLMs Gains More", "authors": ["Wei"], "abstract": "", "published_date": "2025-02-22", "affiliations": "The University of Hong Kong, The Chinese University of Hong Kong, Beihang University, Centre for Perceptual and Interactive Intelligence, Hong Kong", "category": "OR for Generative AI", "relevance": {"methodological": 5, "problem": 7, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper demonstrates a successful application of Integer Programming (OR) to optimize LLM inference (AI), directly validating our 'OR for AI' research interest. Practically, the method enables running Mixtral-8x7B on single consumer GPUs, which could significantly reduce cloud costs for our large-scale evolutionary search experiments."}, "brief": "Huang et al. propose a compression framework for MoE-LLMs that uses Integer Programming to optimally allocate mixed bit-widths (1-3 bits) to experts based on activation frequency and routing weights. They achieve strong empirical results, compressing Mixtral 8x7b to ~16GB (fitting on a single RTX 3090) with only a ~4% drop in zero-shot accuracy, significantly outperforming uniform quantization. The key takeaway is the explicit IP formulation for minimizing quantization error under memory constraints—a clean 'OR for AI' pattern we can adapt for our GPU scheduling or memory allocation formulations. While not a methodological advance in evolution, this is highly relevant for our infrastructure: it enables deploying high-quality MoE models on cheaper hardware for our massive AlgoEvo loops.", "methodology": {"core_method": "Hybrid Post-Training Quantization and Dynamic Pruning for MoE-LLMs using Linear Programming for bit-width allocation and significance-aware token protection", "llm_role": "none", "llm_model_used": "Mixtral 8x7b, Mixtral 8x22b", "search_type": "hybrid", "novelty_claim": "We are the first to explore extreme training-free mixture compression for MoE-LLMs, efficiently combining static expert quantization with dynamic expert pruning using a combination of expert importance metrics to achieve ultra-lightweight MoE-LLMs without significantly sacrificing performance.", "components": ["Pre-Loading Mixed-Precision Quantization (PMQ)", "Online Dynamic Pruning (ODP)", "Linear Programming (LP) for adaptive bit-width allocation", "GPTQ (as base Post-Training Quantization)", "Significance-aware token protection", "Weighted evaluation function for expert importance"], "training_required": false}, "tags": {"methods": ["post_training_quantization", "mixed_precision_quantization", "expert_quantization", "dynamic_pruning", "expert_pruning", "token_pruning", "linear_programming", "integer_programming", "gptq", "moe_llm_compression"], "problems": ["llm_compression", "memory_optimization", "inference_efficiency"], "contribution_type": ["new_method", "framework", "sota_result"], "framework_lineage": "mc", "specific_domain": "mixture_of_experts_llm_compression", "llm_coupling": null}, "problem": {"formal_name": "Extreme Compression of Mixture-of-Experts Large Language Models", "short": "MoE-LLM Compression", "class_": "model_compression", "properties": ["training-free", "mixed-precision", "dynamic_pruning", "post-training", "expert-wise", "token-wise"], "scale": "Mixtral 8x7b (49B total, 13B activated) and Mixtral 8x22b (141B total, 39B activated) parameters"}, "lineage": {"direct_ancestors": [{"paper": "Frantar et al., 2022", "relationship": "uses GPTQ as foundational PTQ tool"}, {"paper": "Lu et al., 2024", "relationship": "builds on dynamic expert pruning strategy"}], "closest_prior_work": "Li et al., 2024", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["adapting strategy for multimodal applications", "optimizing for specific hardware platforms", "future exploration of MoE LLM compression on complex tasks"], "transferable_to": ["multimodal_llms", "other_moe_architectures", "other_llm_compression_techniques"], "open_weaknesses": ["performance_degradation_on_complex_reasoning_tasks", "lack_of_specific_hardware_optimization", "potential_for_further_accuracy_at_extreme_compression"]}, "artifacts": {"code_url": "https://github.com/Aaronhuang-778/MC-MoE", "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_34", "front_status": "emerging", "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.99, "experiments": {"benchmarks": ["C4", "Math", "WikiText2", "EleutherAI LM Harness", "PIQA", "ARC-e", "ARC-c", "BoolQ", "HellaSwag", "Winogrande", "MathQA", "MMLU", "GSM8K", "HumanEval", "Needle-in-a-haystack (NIAH)"], "baselines": ["GPTQ (uniform quantization)", "Block Score Predictor (BSP) (Li et al., 2024)", "Hessian-based (Dong et al., 2020)", "LLaMA2-7b (16-bit)", "LLaMA2-13b (16-bit)"], "hardware": "NVIDIA A100-80GB GPU (2x for Mixtral 8x7b, 4x for Mixtral 8x22b, 1x for quantized models), RTX 3090 GPU; Mixtral 8x7b quantization in 90 minutes", "instance_sizes": [13, 39, 26]}, "results": {"vs_baselines": {"GPTQ (uniform 2.00-bit)": "+20.58% on LM-Eval (PMQ 2.05-bit)", "Block Score Predictor (BSP 2.54-bit)": "+18.43% on LM-Eval (PMQ 2.54-bit)", "Hessian-based (2.54-bit)": "+0.32% on LM-Eval (PMQ 2.54-bit)", "LLaMA2-13b (16-bit)": "+1.75% on LM-Eval (MC 2.54-bit Mixtral 8x7b)", "Mixtral 8x7b (16-bit)": "-4.35% on LM-Eval (MC 2.54-bit Mixtral 8x7b)"}, "scalability": "The performance boost from model weight compression remains consistent regardless of input sequence length and batch size; speed advantage increases with sequence length due to ODP efficiency.", "statistical_rigor": "Evaluated on 128 random sequences for calibration; average accuracy reported across eight zero-shot benchmarks.", "limitations_acknowledged": ["Future work on adapting strategy for multimodal applications and specific hardware platforms", "Larger performance losses on complex CoT tasks due to intricate reasoning demands and lack of strong pre-training inference capabilities in open-source MoE/dense LLMs"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2503.10642", "arxiv_url": "https://arxiv.org/abs/2503.10642", "title": "Text2Zinc: A Cross-Domain Dataset for Modeling Optimization and Satisfaction Problems in MiniZinc", "authors": ["Akash"], "abstract": "", "published_date": "2025-02-22", "affiliations": "Brown University, Fidelity Investments", "category": "Generative AI for OR", "relevance": {"methodological": 3, "problem": 8, "inspirational": 4}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper releases a direct competitor/precursor dataset (TEXT2ZINC) to our active 'OR-Bench' project. We must review their data quality and baselines to ensure our benchmarking work is properly positioned and potentially assimilate their instances."}, "brief": "Singirikonda et al. introduce TEXT2ZINC, a dataset of 110 Natural Language-to-MiniZinc problems, and benchmark GPT-4 using Vanilla, CoT, and Compositional prompting. Their results are poor (max ~25% solution accuracy), confirming that off-the-shelf LLMs struggle significantly with MiniZinc syntax and logical translation. Crucially, they attempt using Knowledge Graphs as an intermediate representation, but report that it actually *reduced* solution accuracy compared to basic CoT—a valuable negative result for our symbolic modeling work. We should examine their dataset for inclusion in OR-Bench, but their prompting methods are rudimentary baselines we should easily outperform.", "methodology": {"core_method": "LLM-based MiniZinc model generation using various prompting strategies (Vanilla, Chain-of-Thought, Compositional)", "llm_role": "code_writer", "llm_model_used": "GPT-4", "search_type": "sampling", "novelty_claim": "The paper introduces TEXT2ZINC, a unified, cross-domain dataset integrating both satisfaction and optimization problems using MiniZinc's solver-agnostic modeling, and explores knowledge graphs as novel intermediate representations for natural language to model translation.", "components": ["Vanilla Prompting", "Chain-of-Thought Prompting", "Compositional Prompting", "Knowledge Graphs (intermediate representation)"], "training_required": false}, "tags": {"methods": ["llm_code_generation", "llm_prompt_optimization", "llm_in_the_loop", "vanilla_prompting", "chain_of_thought", "compositional_prompting", "knowledge_graphs", "minizinc_modeling"], "problems": ["program_synthesis", "natural_language_to_code_generation", "automated_optimization_modeling", "automated_constraint_programming_modeling", "linear_programming", "mixed_integer_linear_programming", "constraint_programming", "scheduling", "bin_packing", "supply_chain_optimization"], "contribution_type": ["new_benchmark", "empirical_study"], "framework_lineage": null, "specific_domain": "natural_language_to_minizinc_modeling", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Natural Language to MiniZinc Model Generation", "short": "NL2MZN", "class_": "program_synthesis", "properties": ["optimization", "satisfaction", "solver_agnostic", "cross_domain"], "scale": "110 problem instances (dataset), 63 problems (experiments)"}, "lineage": {"direct_ancestors": [{"paper": "Ramamonjison et al. 2022a", "relationship": "builds on early efforts of LLMs for LP problem formulation"}, {"paper": "Tsouros et al. 2023", "relationship": "builds on blueprint for conversational modeling assistants and decomposition-based prompting"}, {"paper": "Dakle et al. 2023; Kadıo˘glu et al. 2024", "relationship": "builds on LLM generated MiniZinc code with in-line annotation"}, {"paper": "Xiao et al. 2023", "relationship": "builds on multi-agent frameworks for complex OR problems"}, {"paper": "AhmadiTeshnizi et al. 2024", "relationship": "builds on modular LLM systems for complex problem descriptions"}], "closest_prior_work": "Dakle et al. 2023; Kadıo˘glu et al. 2024", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["explore alternative intermediate representations (e.g., named entities, semantic graphs, agentic frameworks)", "expand TEXT2ZINC dataset with additional models and problem types", "address LLM misinterpretation of optimization vs. satisfiability problems", "improve LLM training on MiniZinc's specialized syntax and constraints"], "transferable_to": ["automated modeling for other high-level modeling languages (e.g., GAMS, CPMpy, Pyomo)", "program synthesis from natural language in other declarative domains", "automated generation of other domain-specific languages"], "open_weaknesses": ["LLMs are not yet a push-button solution for combinatorial modeling", "consistently low solution accuracy compared to execution accuracy", "LLMs occasionally misinterpret optimization problems as satisfiability problems", "syntax errors are a primary cause of execution failures due to limited LLM training on MiniZinc", "intermediate representations like knowledge graphs require more research for optimal utilization"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": true}, "front_id": "generative_ai_for_or_2026-02-18_front_6", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.09, "experiments": {"benchmarks": ["TEXT2ZINC (NLP4LP subset)"], "baselines": ["Vanilla Prompting (Basic)", "Vanilla Prompting (+ Data & examples)", "Vanilla Prompting (+ Shape)", "Vanilla Prompting (+ Knowledge Graph)", "Chain-of-Thought (CoT with data)", "Chain-of-Thought (CoT + Examples)", "Chain-of-Thought (CoT + Shape)", "Compositional (Multi-Call + Composition)"], "hardware": "Not specified", "instance_sizes": [63]}, "results": {"vs_baselines": {"Vanilla Prompting (Basic)": "19.04% Execution Accuracy, 6.34% Solution Accuracy", "Vanilla Prompting (+ Data & examples)": "36.50% Execution Accuracy, 19.04% Solution Accuracy", "Vanilla Prompting (+ Shape)": "19.04% Execution Accuracy, 12.69% Solution Accuracy", "Vanilla Prompting (+ Knowledge Graph)": "34.92% Execution Accuracy, 11.11% Solution Accuracy", "Chain-of-Thought (CoT with data)": "42.85% Execution Accuracy, 17.46% Solution Accuracy", "Chain-of-Thought (CoT + Examples)": "58.73% Execution Accuracy, 25.39% Solution Accuracy", "Chain-of-Thought (CoT + Shape)": "55.55% Execution Accuracy, 20.63% Solution Accuracy", "Compositional (Multi-Call + Composition)": "60.31% Execution Accuracy, 22.22% Solution Accuracy"}, "scalability": "The paper does not explicitly discuss scalability with respect to problem size, but notes that LLMs are not yet a push-button technology for combinatorial problems.", "statistical_rigor": "Results are reported as execution and solution accuracy percentages over 63 problems. No variance or significance tests are explicitly mentioned.", "limitations_acknowledged": ["LLMs are not yet a push-button technology to model combinatorial problems from text.", "LLMs struggle with consistency and precision required in declarative approaches.", "Consistently lower solution accuracies (typically 30-50% of execution accuracies) across different strategies.", "LLMs occasionally misinterpret constraint optimization problems as satisfiability problems.", "Syntax errors are the primary cause of execution failures due to LLM's limited training on MiniZinc's specialized syntax.", "Both too little and too much information can be detrimental to performance.", "Intermediate representations like Knowledge Graphs show potential but require more research for best utilization."]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2412.20694", "arxiv_url": "https://arxiv.org/abs/2412.20694", "title": "QUBE: Enhancing Automatic Heuristic Design via Quality-Uncertainty Balanced Evolution", "authors": ["Zijie"], "abstract": "", "published_date": "2025-02-21", "affiliations": "Westlake University, Zhejiang University, University of Electronic Science and Technology of China", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 8, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper identifies a fundamental flaw in FunSearch's parent selection (score-based) and fixes it with a UCB bandit approach (offspring-potential-based). This is a drop-in improvement for our AlgoEvo and evolutionary search pipelines that directly targets sample efficiency."}, "brief": "QUBE replaces FunSearch's naive score-based parent selection with a UCB algorithm that selects parents based on the *average quality of their offspring* (exploitation) plus an uncertainty term (exploration). The authors demonstrate that a parent's own score is a poor predictor of its ability to evolve further; treating parents as 'bandit arms' based on their lineage statistics yields significantly better results on Bin Packing and TSP with fewer samples. While they fail to beat DeepMind's massive-scale Cap Set record, the methodological insight regarding 'offspring-aware' selection is statistically validated and immediately transferable to our evolutionary search frameworks.", "methodology": {"core_method": "Evolutionary Algorithm with LLM as variation operator, guided by Quality-Uncertainty Trade-off Criterion (QUTC) using Uncertainty-Inclusive Quality (UIQ) metric", "llm_role": "variation_operator", "llm_model_used": "OpenCoder-8B-Instruct", "search_type": "hybrid", "novelty_claim": "We introduce Quality-Uncertainty Balanced Evolution (QUBE), a novel approach that enhances LLM+EA methods by redefining the priority criterion within the FunSearch framework.", "components": ["Evolutionary Algorithm", "Large Language Model (LLM)", "Multi-population (islands) framework", "Quality-Uncertainty Trade-off Criterion (QUTC)", "Uncertainty-Inclusive Quality (UIQ) metric", "Parent selection mechanism", "Island reset mechanism"], "training_required": false}, "tags": {"methods": ["evolutionary_algorithm", "llm_code_generation", "llm_as_heuristic", "llm_in_the_loop", "funsearch", "qube", "upper_confidence_bound", "exploitation_exploration_tradeoff", "multi_population_evolutionary_algorithm", "hyper_heuristics", "program_synthesis"], "problems": ["online_bin_packing", "tsp", "cap_set_problem", "heuristic_evolution"], "contribution_type": ["new_method", "sota_result", "empirical_study", "framework"], "framework_lineage": "funsearch", "specific_domain": "heuristic_evolution", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Automatic Heuristic Design for NP-complete problems", "short": "Heuristic Design", "class_": "algorithm_design", "properties": ["NP-complete", "heuristic_optimization", "exploitation_exploration_balance", "multi_population_evolutionary_search"], "scale": "Online Bin Packing (1k-10k items), Cap Set (n=8), TSP (20-100 cities)"}, "lineage": {"direct_ancestors": [{"paper": "Romera-Paredes et al., 2024", "relationship": "redefines priority criterion within FunSearch framework"}], "closest_prior_work": "Romera-Paredes et al., 2024", "novelty_type": "incremental"}, "extensions": {"next_steps": ["apply to more complex NP-hard problems", "investigate larger or better LLMs for enhanced performance", "improve computational efficiency for large-scale problems", "address safety and explainability of LLM-generated code"], "transferable_to": ["other combinatorial optimization problems", "automated algorithm design for diverse scientific domains", "hyper-heuristic design for different problem classes"], "open_weaknesses": ["failure to outperform original FunSearch on cap set problem (n=8)", "high computational requirements for certain experiments", "performance sensitivity to LLM choice and random factors", "risks associated with unpredictable and unexplainable LLM-generated code"]}, "artifacts": {"code_url": "https://github.com/zzjchen/QUBE_code", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.86, "experiments": {"benchmarks": ["OR-Library (OR1, OR2, OR3, OR4)", "Weibull distribution instances", "Cap Set n=8", "TSP20", "TSP50", "TSP100"], "baselines": ["FunSearch", "FunSearch*", "EoH"], "hardware": "single server with 8 NVIDIA A100 GPUs and 2 Intel(R) Xeon(R) Platinum 8358 CPUs", "instance_sizes": [20, 50, 100, 1000, 5000, 10000, 8]}, "results": {"vs_baselines": {"FunSearch*": "Significantly outperforms across OBP (9.36-41.73% lower excess ratio), Cap Set (3.4% larger), and TSP (20.7% lower excess ratio on TSP100).", "FunSearch": "Significantly outperforms across OBP (10.98-42.44% lower excess ratio), but finds a 6.25% smaller cap set on n=8.", "EoH": "Significantly outperforms across OBP Weibull (31.3-52.5% lower excess ratio) and TSP (8.0% lower excess ratio on TSP100)."}, "scalability": "Consistently finds better heuristics and maintains performance improvements in later stages of evolution, demonstrating superior exploitation and adaptive exploration.", "statistical_rigor": "Multiple runs (10 for most, 5 for cap set) are performed, reporting best and average performance with standard deviation for robustness.", "limitations_acknowledged": ["Fails to outperform original FunSearch on cap set problem (n=8)", "High computational requirements for cap set experiments", "Performance is related to LLM choice, number of samples, and random factors", "Risk of unpredictable and hard-to-explain code generated by LLMs", "LLM-generated code observed attempting to modify local files"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2502.14760", "arxiv_url": "https://arxiv.org/abs/2502.14760", "title": "EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of Optimization Formulations", "authors": ["Haotian"], "abstract": "", "published_date": "2025-02-20", "affiliations": "Stanford University, The University of Texas at Austin", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 9, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper solves the 'correctness verification' bottleneck for our OR-Bench and generative modeling projects. It renders our current reliance on 'execution accuracy' (comparing objective values) obsolete by proving it fails on trivial transformations like rescaling."}, "brief": "Zhai et al. propose EquivaMap, a framework that evaluates whether two MILP formulations are equivalent by using an LLM to discover a linear mapping between their decision variables, which is then rigorously verified by a solver. Unlike 'execution accuracy' (which fails on unit scaling) or 'canonical accuracy' (which fails on variable permutation), they achieve 100% accuracy on a new dataset of equivalent formulations including cuts and slack variables. The core insight is replacing output comparison with a 'propose-mapping-and-verify' loop, effectively using the LLM to construct a proof of equivalence. We must adopt this methodology for the OR-Bench evaluation pipeline immediately, as it eliminates the false negatives currently plaguing our generation benchmarks.", "methodology": {"core_method": "LLM-based discovery of linear mapping functions between decision variables, followed by MILP solver-based verification of feasibility and optimality", "llm_role": "heuristic_generator", "llm_model_used": "GPT-4", "search_type": "constructive", "novelty_claim": "We propose EquivaMap, a framework that leverages large language models to automatically discover such mappings for scalable, reliable equivalence checking, with a verification stage that ensures mapped solutions preserve feasibility and optimality without additional solver calls.", "components": ["Large Language Model (mapping finder)", "MILP solver", "Verification module (feasibility and optimality check)"], "training_required": false}, "tags": {"methods": ["llm_as_heuristic", "llm_in_the_loop", "milp_solver", "linear_programming", "karp_reductions", "program_synthesis"], "problems": ["equivalence_checking_of_optimization_formulations", "milp_general", "lp_general"], "contribution_type": ["new_method", "framework", "new_benchmark", "sota_result"], "framework_lineage": null, "specific_domain": "equivalence_checking_of_optimization_formulations", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Equivalence Checking of Optimization Formulations", "short": "Equivalence Checking", "class_": "equivalence_checking", "properties": ["MILP", "LP", "instance_specific", "feasible", "bounded"], "scale": "Not explicitly quantified, but designed for diverse and potentially large-scale MILP/LP formulations"}, "lineage": {"direct_ancestors": [{"paper": "Karp reductions", "relationship": "inspired the formal criterion of Quasi-Karp equivalence"}], "closest_prior_work": "WL-test", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Leverage EquivaMap to check equivalences across diverse optimization problems (e.g., max-flow and min-cut)", "Extend EquivaMap to handle more intricate reformulations (e.g., from decomposition algorithms)", "Optimize LLM prompting strategies for improved efficiency", "Explore the use of smaller fine-tuned models or future LLM advancements to reduce runtime"], "transferable_to": ["Equivalence checking for other types of optimization problems (e.g., non-linear, stochastic)", "Automated debugging of optimization models by identifying reasons for non-equivalence", "Evaluation of other LLM-generated artifacts in operations research and AI"], "open_weaknesses": ["LLM interaction is currently the primary bottleneck for runtime performance", "Current work focuses on relatively straightforward transformations, omitting more intricate reformulations", "Quasi-Karp equivalence is directional, meaning α' being equivalent to α does not necessarily imply α is equivalent to α'"]}, "artifacts": {"code_url": "https://github.com/HumainLab/EquivaMap", "models_released": false, "new_benchmark": true}, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_status": "stable", "bridge_score": 0.7615, "is_bridge": true, "priority_score": 7.89, "experiments": {"benchmarks": ["EquivaFormulation", "NLP4LP dataset"], "baselines": ["Canonical Accuracy", "Execution Accuracy", "WL-test", "naive-LLM"], "hardware": "No specific GPU/CPU specifications provided; runtime analysis performed.", "instance_sizes": []}, "results": {"vs_baselines": {"Canonical Accuracy": "0% on equivalent, 100% on non-equivalent", "Execution Accuracy": "0-100% (fails on Rescaling/Feasibility)", "WL-test": "0% on equivalent, 100% on non-equivalent", "naive-LLM": "3.3-98.9%", "EquivaMap": "100% across all transformations"}, "scalability": "EquivaMap's prompt length scales with the number of sets of variables, not individual variables, managing LLM interaction cost for larger problems. LLM call time is the bottleneck (avg 11.88s per instance).", "statistical_rigor": "Algorithm 1 run K=3 times; equivalence declared if at least one valid mapping is found.", "limitations_acknowledged": ["Focuses on relatively straightforward transformations, omitting intricate reformulations (e.g., from decomposition algorithms)", "LLM interaction is currently the bottleneck for runtime"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2502.11102", "arxiv_url": "https://arxiv.org/abs/2502.11102", "title": "OptMATH: A Scalable Bidirectional Data Synthesis Framework for Optimization Modeling", "authors": ["Hongliang"], "abstract": "", "published_date": "2025-02-16", "affiliations": "Peking University", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper directly impacts our 'OR-Bench' project by demonstrating that synthetic, solver-verified data can train models to outperform GPT-4 on OR modeling tasks. It provides a concrete blueprint for solving the data scarcity issue in symbolic OR benchmarking."}, "brief": "The authors introduce OptMATH, a framework for generating synthetic optimization datasets by creating mathematical instances from seed generators, back-translating them to natural language via LLMs, and validating the pairs using a solver-based rejection sampling loop (checking if the re-generated model yields the same optimal value). They demonstrate that a Qwen-32B model fine-tuned on this data beats GPT-4 on NL4Opt and MAMO benchmarks. The critical takeaway is the **solver-verified reverse generation pipeline**: we should immediately steal this workflow to populate OR-Bench and generate diverse, verified training environments for AlgoEvo, replacing manual curation with scalable synthesis.", "methodology": {"core_method": "Scalable bidirectional data synthesis framework integrating feedback-driven PD generation, LLM-based backtranslation with self-criticism/refinement, and AutoFormulator with rejection sampling.", "llm_role": "data synthesizer", "llm_model_used": "Deepseek-V3, Qwen2.5 series", "search_type": "sampling", "novelty_claim": "We propose a scalable bidirectional synthesis framework that addresses the critical challenge of data scarcity in optimization modeling through triplet-aligned (NL, MF, PD) data generation and rigorous validation. Our framework uniquely integrates a closed-loop workflow with optimal value matching, ensuring semantic equivalence between NL, MF, and PD.", "components": ["seed problem generators", "feedback-driven PD generation", "LLM-based backtranslation pipeline", "self-criticism", "self-refinement", "AutoFormulator", "rejection sampling", "data augmentation", "supervised fine-tuning (SFT)"], "training_required": true}, "tags": {"methods": ["llm_as_data_synthesizer", "llm_fine_tuned", "llm_in_the_loop", "llm_code_generation", "llm_as_evaluator", "supervised_learning", "data_augmentation", "rejection_sampling", "prompt_engineering", "instance_generation", "self_refinement"], "problems": ["optimization_modeling", "lp_modeling", "milp_modeling", "ip_modeling", "nlp_modeling", "socp_modeling", "data_scarcity"], "contribution_type": ["new_method", "new_benchmark", "sota_result", "framework"], "framework_lineage": "optmath", "specific_domain": null, "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Data Synthesis for Optimization Modeling", "short": "OptMATH Data Synthesis", "class_": "data_synthesis", "properties": ["scalable", "bidirectional", "triplet-aligned (NL, MF, PD)", "rigorous validation", "closed-loop workflow", "optimal value matching", "semantic equivalence"], "scale": "600,000 LP files, 53 problem types, LP file lengths 1,000-25,000 characters"}, "lineage": {"direct_ancestors": [{"paper": "ORLM: Training large language models for optimization modeling", "relationship": "improves upon fine-tuning approach for optimization modeling"}, {"paper": "Autoformulation of mathematical optimization models using LLMs", "relationship": "extends automated formulation methods"}, {"paper": "Towards foundation models for mixed integer linear programming", "relationship": "extends data synthesis for MILP to include natural language translation"}], "closest_prior_work": "ORLM: Training large language models for optimization modeling", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["investigate exact equivalence between mathematical formulations", "integrate with advanced AI techniques like reinforcement learning or Monte Carlo Tree Search", "further improve the self-refinement loop for natural language generation", "explore methods to address ambiguity in variable types (integer/continuous)"], "transferable_to": ["code generation for other domains", "data synthesis for other structured prediction tasks", "automated problem formulation in other scientific or engineering fields"], "open_weaknesses": ["determining exact equivalence between mathematical formulations", "ambiguity in problem statements regarding variable types (integer/continuous)", "computational cost of rejection sampling for very large-scale data generation", "potential for non-perfect semantic equivalence despite optimal value matching"]}, "artifacts": {"code_url": "https://github.com/AuroraLHL/OptMATH", "models_released": false, "new_benchmark": true}, "front_id": "generative_ai_for_or_2026-02-18_front_1", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.81, "experiments": {"benchmarks": ["NL4OPT", "MAMO EasyLP", "MAMO ComplexLP", "OptMATH-Bench"], "baselines": ["GPT-3.5-turbo", "GPT-4", "Deepseek-V3", "Chain-of-Experts", "Optimus", "ORLM-LLaMA-3-8B"], "hardware": "null", "instance_sizes": [500000000, 1500000000, 3000000000, 7000000000, 14000000000, 32000000000]}, "results": {"vs_baselines": {"GPT-3.5-turbo": "OptMATH-Qwen2.5-32B surpasses (68.7% Macro AVG vs 51.4%)", "GPT-4": "OptMATH-Qwen2.5-32B surpasses (68.7% Macro AVG vs 60.6%)", "Deepseek-V3": "OptMATH-Qwen2.5-32B surpasses (68.7% Macro AVG vs 67.0%)", "Chain-of-Experts": "OptMATH-Qwen2.5-32B surpasses on NL4OPT (95.9% vs 64.2%)", "Optimus": "OptMATH-Qwen2.5-32B surpasses on NL4OPT (95.9% vs 78.8%)", "ORLM-LLaMA-3-8B": "OptMATH-Qwen2.5-7B outperforms on all benchmarks (64.2% Macro AVG vs 51.4%)"}, "scalability": "The framework demonstrates exceptional scalability, with all models exhibiting substantial performance improvements after OptMATH-Train fine-tuning, though relative gains diminish for larger models.", "statistical_rigor": "Pass@1 accuracy is used as the evaluation metric, with manual analysis of 1% randomly sampled instances revealing 99.6% accuracy. No explicit statistical significance tests reported for main results.", "limitations_acknowledged": ["Exact equivalence between mathematical formulations is an open research question", "LLMs may struggle with integer vs. continuous variable determination due to problem statement ambiguities"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2502.15763", "arxiv_url": "https://arxiv.org/abs/2502.15763", "title": "Hybrid Offline-online Scheduling Method for Large Language Model Inference Optimization", "authors": ["Bowen"], "abstract": "", "published_date": "2025-02-14", "affiliations": "Noah’s Ark Lab, Huawei, Tsinghua University", "category": "OR for Generative AI", "relevance": {"methodological": 6, "problem": 10, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper is a direct competitor to our 'GPUSched' project, explicitly applying OR formulations (MIP) to LLM inference. We need to benchmark our approaches against their Lagrangian heuristic for prefill-decode interleaving."}, "brief": "Pang et al. formulate LLM inference scheduling as a Mixed-Integer Programming (MIP) model, solving it via a hybrid approach: offline bin-packing for request assignment and an online Lagrangian heuristic for prefill-decode preemption. They report a ~9% utilization increase (80.2% to 89.1%) over a vLLM-style baseline on LLaMA-65B, though the evaluation is limited to a single 8-GPU node and assumes deterministic output lengths for the offline component. The most actionable takeaway is their derivation of a simple cost-comparison threshold (prefill cost vs. decode wait cost) to dynamically inject prefill tasks into decoding streams. This provides a concrete, low-overhead heuristic baseline for our GPUSched work.", "methodology": {"core_method": "Hybrid offline-online method combining Minimizing Makespan Bin Packing (offline) with sorting, online preemption, and a Lagrangian-based heuristic (online)", "llm_role": "none", "llm_model_used": null, "search_type": "hybrid", "novelty_claim": "First to formulate the LLM inference scheduling problem as a mathematical model and propose a two-stage hybrid offline-online approach with sorting, preemption, and a Lagrangian-based heuristic.", "components": ["Mixed-Integer Programming (MIP) formulation", "Minimizing Makespan Bin Packing Problem", "Sorting and Online Preemptive Method", "Lagrangian-based heuristic"], "training_required": false}, "tags": {"methods": ["mixed_integer_programming", "bin_packing", "makespan_minimization", "online_scheduling", "preemptive_scheduling", "lagrangian_heuristic", "hybrid_optimization", "continuous_batching", "heuristic_search"], "problems": ["llm_serving_optimization", "gpu_scheduling", "resource_allocation", "scheduling", "bin_packing", "makespan_minimization"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "vllm", "specific_domain": "llm_serving_optimization", "llm_coupling": null}, "problem": {"formal_name": "Large Language Model Inference Optimization Problem", "short": "LLM Inference Optimization", "class_": "scheduling", "properties": ["hybrid", "offline_online", "preemptive", "real_time_decisions", "uncertain_output_length", "continuous_batching"], "scale": "1319-10000 requests, up to 200 clients"}, "lineage": {"direct_ancestors": [{"paper": "vLLM", "relationship": "provides foundational LLM inference serving system, identifies limitations in its FCFS scheduler"}, {"paper": "ORCA", "relationship": "provides foundational LLM inference serving system"}], "closest_prior_work": "vLLM", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["stochastic_programming_for_uncertainty", "rl_for_iteration_scheduling", "dynamic_hardware_utilization_optimization"], "transferable_to": ["other_ai_model_serving", "general_gpu_workload_scheduling", "cloud_resource_management"], "open_weaknesses": ["deterministic_model_lacks_uncertainty_handling", "online_iteration_scheduling_is_heuristic", "static_client_numbers_lead_to_underutilization", "mip_solver_scalability_issues"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "or_for_generative_ai_2026-02-18_front_0", "front_status": "emerging", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.19, "experiments": {"benchmarks": ["GSM8K dataset"], "baselines": ["Baseline method (vLLM FCFS prefill-first)", "Offline Request Scheduling (their offline component)", "Online only Scheduling (their online component)"], "hardware": "Eight Ascend processing units, each with 64 GB memory", "instance_sizes": [1319, 200]}, "results": {"vs_baselines": {"Baseline method (vLLM FCFS prefill-first)": "Improved utilization by 8.86% (from 80.2% to 89.06%) and reduced total inference time by 10.42s (from 201.00s to 190.58s). Achieved 8.0% average utilization improvement and 100.63 tokens/s generation speed increase over 100 cases."}, "scalability": "Demonstrated consistent performance improvements across 100 randomly generated cases, indicating adaptability to varying workloads and problem instances, and claimed adaptability to various LLM models, hardware platforms, and datasets.", "statistical_rigor": "Results are based on a single real-world case study and an average over 100 randomly generated cases; no variance or significance tests reported.", "limitations_acknowledged": ["Deterministic MIP model does not account for uncertainties (e.g., output length)", "Current online iteration scheduling relies on a heuristic method", "Hardware underutilization with static client numbers due to stochastic output length variations"]}, "analysis_date": "2026-02-18"}, {"arxiv_id": "2502.09544", "arxiv_url": "https://arxiv.org/abs/2502.09544", "title": "Explainable AI-assisted Optimization for Feynman Integral Reduction", "authors": ["Zhuo-Yang"], "abstract": "", "published_date": "2025-02-13", "affiliations": "Peking University, Universit\nZ\nrich, Beijing Computational Science Research Center", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 7, "problem": 3, "inspirational": 8}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While the domain is physics, the paper provides strong empirical evidence for a strategy we struggle with: evolving heuristics on 'toy' instances (1-loop) that successfully generalize to intractable 'grand challenge' instances (5-loop). This directly addresses the evaluation bottleneck in our AlgoEvo work."}, "brief": "Song et al. apply FunSearch to evolve priority functions for Feynman integral reduction, achieving up to 3058x reduction in seeding integrals compared to standard heuristics. The results are rigorous, enabling previously impossible multi-loop calculations. The critical insight for us is the successful transfer of heuristics evolved on trivial 1-loop instances (fast evaluation) to complex 5-loop problems without retraining. We should adopt this 'evolve-on-toy, deploy-on-giant' evaluation protocol to drastically reduce compute costs in our VRP and SAT solver evolutionary search pipelines.", "methodology": {"core_method": "FunSearch algorithm for developing a priority function to optimize seeding integrals in Integration-by-Parts (IBP) reduction", "llm_role": "heuristic_generator", "llm_model_used": "CPM-2B model", "search_type": "hybrid", "novelty_claim": "A novel approach optimizing Feynman integral reduction via a FunSearch-developed priority function, combining LLMs and genetic algorithms for improved memory and computational efficiency.", "components": ["FunSearch algorithm", "Large Language Models", "Genetic Algorithms", "Priority function", "Laporta algorithm", "Improved seeding strategy", "LiteRed", "Mathematica", "FiniteFlow sparse solver"], "training_required": true}, "tags": {"methods": ["funsearch", "genetic_algorithm", "large_language_models", "llm_as_heuristic", "program_synthesis", "integration_by_parts_reduction", "laporta_algorithm", "improved_seeding", "binary_search_algorithm", "sparse_solver", "evolutionary_algorithms", "priority_function_design"], "problems": ["feynman_integral_reduction", "high_energy_physics_calculations", "one_loop_integrals", "multi_loop_integrals", "planar_integrals", "non_planar_integrals", "combinatorial_optimization"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "funsearch", "specific_domain": "feynman_integral_reduction", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Feynman Integral Reduction", "short": "IBP reduction", "class_": "physics_calculation_optimization", "properties": ["integration_by_parts_identities", "multi-loop", "planar", "non-planar", "dots_and_numerators", "high_precision"], "scale": "1-loop to 5-loop, 6-particle, up to 20 propagators, with dots and numerators"}, "lineage": {"direct_ancestors": [{"paper": "Nature 625, 468 (2024)", "relationship": "builds on FunSearch algorithm from"}, {"paper": "arXiv:2405.14621", "relationship": "improves upon improved seeding strategies from"}], "closest_prior_work": "Nature 625, 468 (2024)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["optimize priority function for equation_solving_time", "conduct full performance testing of multi_target_priority_function", "apply funsearch to more complex ibp reductions (e.g. two_loop)", "explore further applications of explainable_ai in theoretical_physics"], "transferable_to": ["other complex mathematical problems easy_to_evaluate_but_difficult_to_solve", "other problems in high_energy_theoretical_physics", "other ibp_reduction problems with different integral_families", "higher_loop_and_leg_feynman_integrals"], "open_weaknesses": ["priority_function not optimized for equation_solving_time", "funsearch did not surpass box_priority for two_loop_ibp_reductions within current computational_limits", "multi_target_priority_function requires full performance_testing", "potential for overfitting with excessively long_output_lengths in llm_generated_code"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.63, "experiments": {"benchmarks": ["one-loop massless bubble integral", "planar six-particle phase-space integral family", "non-planar six-particle phase-space integral family"], "baselines": ["Laporta seeding (FIRE6)", "Improved seeding (FiniteFlow)"], "hardware": "Xeon Gold 6148 CPU and 768 GB of available memory", "instance_sizes": [1, 5]}, "results": {"vs_baselines": {"Laporta seeding": "Factor of 5 reduction in seeding integrals for 1-loop I(15,10); up to 3058x reduction for planar d5.", "Improved seeding": "Up to 24.8x reduction for planar s12, 3058x for planar d5; 5.3x for non-planar s6, 1060x for non-planar d5; ~1.98x for multi-target."}, "scalability": "Demonstrates remarkable scalability; improvement factor grows larger with increasing Feynman integral complexity, achieving up to 3058x reduction for complex multi-loop integrals.", "statistical_rigor": "FunSearch runs were repeated 10 times, each for 5000 epochs. Priority functions were evaluated on average seeding integrals across 30 distinct target integrals. 7 out of 10 runs converged to the best function.", "limitations_acknowledged": ["Priority function primarily minimizes seeding integrals, not equation-solving time; optimizing for time is future work.", "FunSearch did not surpass Box priority for two-loop IBP reductions within 2 days/2000 epochs.", "Full performance testing of multi-target priority function is left for future work."]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2412.01523", "arxiv_url": "https://arxiv.org/abs/2412.01523", "title": "FlexSP: Accelerating Large Language Model Training via Flexible Sequence Parallelism", "authors": ["Yujie"], "abstract": "", "published_date": "2025-02-11", "affiliations": "Peking University, ByteDance Inc., Beihang University", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 6, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "This is a textbook execution of 'OR for AI Systems,' directly relevant to our GPUSched and resource allocation projects. It validates that exact solvers (MILP) can run in the critical path of AI infrastructure if the problem space is cleverly reduced."}, "brief": "FlexSP optimizes distributed LLM training by dynamically assigning varied-length sequences to heterogeneous Sequence Parallelism (SP) groups using a Mixed-Integer Linear Programming (MILP) solver in the loop. The results are solid, showing up to 1.98x speedup on A100 clusters by mitigating communication bottlenecks for short sequences while preventing OOM for long ones. **Key Takeaway:** The authors use Dynamic Programming to 'bucket' similar sequences, drastically reducing the variable count for the MILP solver; this specific technique—reducing problem granularity to make exact solvers feasible in real-time systems—is directly applicable to our 'GPUSched' and inference resource allocation work. While we focus on evolution, this is a definitive reference for our 'OR for AI Systems' track, proving that formal optimization can beat heuristics in dynamic GPU scheduling.", "methodology": {"core_method": "Heterogeneity-adaptive sequence parallelism using MILP and dynamic programming for optimal strategy selection", "llm_role": "none", "llm_model_used": null, "search_type": "exact", "novelty_claim": "FlexSP is the first to adaptively adjust parallelism strategies to match heterogeneous workloads of varied-length sequences with heterogeneous parallelism.", "components": ["Parallelism Planner", "Sequence Blaster", "Sequence Bucketing (Dynamic Programming)", "MILP Solver (SCIP)", "Cost Estimation (extended alpha-beta model)", "Hot Switching and Group Management", "Disaggregated Solving and Training"], "training_required": false}, "tags": {"methods": ["sequence_parallelism", "distributed_training", "system_optimization", "adaptive_parallelism", "linear_programming", "mixed_integer_linear_programming", "dynamic_programming", "cost_modeling", "resource_allocation", "sequence_packing", "gradient_accumulation", "flash_attention"], "problems": ["llm_training_efficiency", "long_context_llm_training", "workload_heterogeneity", "communication_optimization", "memory_optimization"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "deepspeed_ulysses", "specific_domain": "llm_training_efficiency", "llm_coupling": null}, "problem": {"formal_name": "Efficient Training of Large Language Models with Varied-Length Sequences", "short": "LLM Training with Varied Sequence Lengths", "class_": "distributed_system_optimization", "properties": ["varied-length sequences", "long-tail distribution", "heterogeneous workloads", "memory constraints", "communication overhead", "adaptive parallelism"], "scale": "GPT-7B to 30B models, up to 384K tokens per sequence"}, "lineage": {"direct_ancestors": [{"paper": "DeepSpeed-Ulysses", "relationship": "improves upon homogeneous Ulysses-style sequence parallelism from"}, {"paper": "Megatron-LM", "relationship": "improves upon homogeneous parallelism strategies from"}, {"paper": "sequence_packing", "relationship": "builds upon sequence packing techniques like Best-fit Packing"}], "closest_prior_work": "DeepSpeed-Ulysses", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["integrate context parallelism into FlexSP", "extend adaptive parallelism to other heterogeneous data types (e.g., images/videos)", "further optimize for extremely long sequences to reduce computation overhead"], "transferable_to": ["other distributed training paradigms (e.g., pipeline parallelism, tensor parallelism)", "other deep learning models with varied input sizes", "heterogeneous hardware environments"], "open_weaknesses": ["solver time can exceed training time for very large GPU clusters (though amortized)", "reduced optimization opportunities for datasets with less pronounced long-tail distributions (shorter context lengths)", "significant computation overhead of extremely long sequences still limits overall speedup"]}, "artifacts": {"code_url": "https://github.com/PKU-DAIR/Hetu-Galvatron", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.54, "experiments": {"benchmarks": ["GitHub", "CommonCrawl", "Wikipedia"], "baselines": ["Megatron-LM", "DeepSpeed", "FlexSP-BatchAda"], "hardware": "8 nodes, each with 8 NVIDIA A100 40GB GPUs, NVLink, 400Gbps InfiniBand", "instance_sizes": [192000, 384000]}, "results": {"vs_baselines": {"DeepSpeed": "FlexSP up to 1.72x faster", "Megatron-LM": "FlexSP up to 1.98x faster", "FlexSP-BatchAda": "FlexSP up to 1.42x faster"}, "scalability": "FlexSP shows robust scalability with increasing GPU count (1.91x speedup from 16 to 32 GPUs, 1.82x from 32 to 64 GPUs) and maintains optimal performance across varying context lengths (1.42x-1.51x speedup).", "statistical_rigor": "Average iteration time recorded over 40 iterations after 10-iteration warm-up; cost estimation error below 6%.", "limitations_acknowledged": ["Context parallelism integration is future work", "Reduced optimization opportunities for shorter context lengths", "Significant computation overhead for extremely long sequences reduces speedup"]}, "analysis_date": "2026-02-19"}, {"arxiv_id": "2502.06643", "arxiv_url": "https://arxiv.org/abs/2502.06643", "title": "MoETuner: Optimized Mixture of Expert Serving with Balanced Expert Placement and Token Routing", "authors": ["Seokjin"], "abstract": "", "published_date": "2025-02-10", "affiliations": "Georgia Institute of Technology", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 9, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper is a direct execution of 'OR formulations for AI systems'—specifically applying Integer Linear Programming to LLM serving optimization. It validates the viability of static ILP for MoE expert placement, which is immediately relevant to our GPUSched and resource allocation projects."}, "brief": "Go et al. formulate the MoE expert placement problem as a two-stage Integer Linear Program (ILP) to balance token load and minimize communication tail latency, exploiting stable token routing dependencies across layers. They demonstrate real-world speedups of 17.5% on multi-node H200 clusters running Mixtral-8x7B, validating the approach with concrete systems measurements rather than just simulation. The key takeaway is the effectiveness of a min-max ILP objective for reducing tail latency in distributed inference, proving that static optimization based on profiling is sufficient for significant gains. This directly supports our 'OR for AI systems' track and provides a strong baseline formulation for our GPU scheduling work.", "methodology": {"core_method": "Integer Linear Programming (ILP) for expert clustering and cluster-to-GPU assignment", "llm_role": "none", "llm_model_used": "Mixtral-8x7B", "search_type": "exact", "novelty_claim": "We introduce MOETUNER, a novel optimization approach for expert parallelism in MoE models leveraging ILP to minimize both communication and compute-induced tail latency.", "components": ["Token Routing Profiling", "ILP Optimization (Load-Balanced Expert Clustering)", "ILP Optimization (Communication-aware Cluster-GPU Assignment)", "Custom Expert Parallelism Initialization"], "training_required": false}, "tags": {"methods": ["integer_linear_programming", "resource_allocation", "load_balancing", "system_level_optimization", "expert_placement", "token_routing_profiling", "distributed_systems"], "problems": ["llm_serving_optimization", "distributed_system_optimization", "resource_allocation", "load_balancing", "tail_latency_reduction", "communication_optimization"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "megatron_lm", "specific_domain": "llm_serving_optimization", "llm_coupling": null}, "problem": {"formal_name": "Optimized Mixture of Expert Serving with Balanced Expert Placement and Token Routing", "short": "MoE Serving Optimization", "class_": "distributed_system_optimization", "properties": ["distributed", "sparse_activation", "load_balancing", "communication_aware", "tail_latency_reduction", "cross_layer_dependencies", "capacitated"], "scale": "8 experts per layer, 8-16 GPUs"}, "lineage": {"direct_ancestors": [{"paper": "Megatron-LM", "relationship": "modifies expert placement and communication modules in"}, {"paper": "arXiv:2404.19429", "relationship": "addresses limitations of computation-communication overlapping in"}, {"paper": "arXiv:2203.13262", "relationship": "addresses limitations of congestion-avoiding expert selection in"}, {"paper": "2023 USENIX Annual Technical Conference (USENIX ATC 23)", "relationship": "addresses limitations of dynamic resource scheduling in"}, {"paper": "2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "relationship": "builds on insight of inter-layer expert affinity from"}], "closest_prior_work": "2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Investigate dynamic expert placement strategies that adapt to changing workloads at runtime.", "Extend the ILP formulation to incorporate other parallelism techniques (e.g., pipeline parallelism) for holistic optimization.", "Explore approximate or heuristic solutions for ILP to scale to even larger models/clusters where exact ILP might be too slow.", "Evaluate MOETUNER's impact on energy consumption and cost efficiency."], "transferable_to": ["Other distributed sparse models beyond MoE (e.g., models with conditional computation).", "General cloud resource scheduling and task placement problems.", "Optimizing data transfer and computation in other distributed computing frameworks."], "open_weaknesses": ["The current approach relies on profiling token routing, which adds an overhead and assumes stable routing patterns.", "The ILP solution is static and does not adapt to dynamic changes in token routing or expert activation during inference.", "The ILP solver's runtime might become a bottleneck for extremely large numbers of experts or GPUs.", "The interaction with tensor parallelism's all-gather operations in multi-node setups still presents a challenge for further latency reduction."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.29, "experiments": {"benchmarks": ["WikiText-103", "MiniPile", "LAMBADA", "enwik8"], "baselines": ["Megatron-LM (naive expert placement)"], "hardware": "8x NVIDIA H100 SXM5 80GB (single-node), 8x NVIDIA H200 SXM5 142GB (multi-node)", "instance_sizes": [8, 16]}, "results": {"vs_baselines": {"Megatron-LM": "9.3% end-to-end speedup on single-node (8 H100 GPUs), 17.5% end-to-end speedup on multi-node (16 H200 GPUs)"}, "scalability": "MOETUNER demonstrates consistent benefits across setups, effectively addressing inefficiencies in token processing to deliver predictable and efficient computation, with more significant speedup in multi-node due to lower inter-node bandwidth.", "statistical_rigor": "End-to-end speedup measured by running 100 inference steps and averaging results across datasets, with 100 warmup steps. Latency measurements used PyTorch Profiler, averaging results over 10 inference steps with 100 warm-up steps on WikiText-103. Tail latency is average of maximum GPU execution times per layer; average latency is mean execution time across all GPUs.", "limitations_acknowledged": ["In the multi-node setup, while token processing time is reduced, the improvements are less pronounced compared to the single-node configuration due to additional inter-node communication overhead from all-gather operations introduced by tensor parallelism.", "Fluctuations in latency in some layers (e.g., 12–22) are likely due to short profiling iterations.", "Improvement in tail latency is less pronounced in certain layers (e.g., 15 and 30) due to high inter-node token dispatching, which introduces additional communication overhead."]}, "analysis_date": "2026-02-19"}, {"arxiv_id": "2409.16867", "arxiv_url": "https://arxiv.org/abs/2409.16867", "title": "Multi-objective Evolution of Heuristic Using Large Language Model", "authors": ["Shunyu"], "abstract": "", "published_date": "2025-02-04", "affiliations": "City University of Hong Kong, Southern University of Science and Technology", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper directly addresses the population diversity and 'mode collapse' issues we face in AlgoEvo by formalizing a code-space diversity metric (AST). It is a rigorous application of MOEA principles to LLM code generation by a top group (Qingfu Zhang)."}, "brief": "MEoH extends LLM-based heuristic evolution (like FunSearch/EoH) to multi-objective scenarios (e.g., Gap vs. Runtime) by introducing a 'Dominance-Dissimilarity' mechanism that selects parents based on both Pareto dominance and Abstract Syntax Tree (AST) code distance. The results are credible and strong: on TSP, they find heuristics matching EoH's quality but running 16x faster (1.37s vs 22.4s) by effectively navigating the complexity-performance trade-off. The single most useful takeaway is the **AST-based dissimilarity metric** for population management; we should immediately steal this to prune semantically identical code in our evolutionary loops, thereby forcing exploration and improving sample efficiency. This is a direct upgrade to our current single-objective evolutionary search methods.", "methodology": {"core_method": "LLM-based Multi-objective Evolutionary Algorithm with Dominance-Dissimilarity Mechanism", "llm_role": "heuristic_generator", "llm_model_used": "GPT3.5-turbo", "search_type": "improvement", "novelty_claim": "MEoH is the first LLM-based multi-objective heuristic search framework that generates a non-dominated set of heuristics using a novel dominance-dissimilarity mechanism for population management.", "components": ["LLM-based search operators", "dominance-dissimilarity mechanism", "Abstract Syntax Tree (AST) for code dissimilarity", "Pareto dominance", "population management", "parent selection"], "training_required": false}, "tags": {"methods": ["llm_evolutionary_search", "llm_as_heuristic", "evolutionary_algorithm", "multi_objective_optimization", "program_synthesis", "pareto_dominance"], "problems": ["heuristic_evolution", "bin_packing", "tsp"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "eoh", "specific_domain": null, "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Multi-objective Automated Heuristic Design", "short": "MOAHD", "class_": "algorithm_design", "properties": ["multi-objective", "LLM-based", "evolutionary", "zero-shot LLM"], "scale": "5k-100k items (BPP), 100-1002 nodes (TSP)"}, "lineage": {"direct_ancestors": [{"paper": "FunSearch (Romera-Paredes et al. 2024)", "relationship": "extends single-objective LLM-based heuristic search from"}, {"paper": "EoH (Liu et al. 2024a)", "relationship": "extends single-objective LLM-based heuristic search from"}], "closest_prior_work": "EoH (Liu et al. 2024a)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Extend the framework to handle more than two or three objectives", "Apply the multi-objective heuristic design to a broader range of combinatorial optimization problems", "Explore the use of different LLM architectures or fine-tuning strategies for heuristic generation", "Investigate methods to improve computational efficiency for even larger problem instances"], "transferable_to": ["Vehicle Routing Problems (VRP variants)", "Scheduling problems (e.g., job shop scheduling, flow shop scheduling)", "Other combinatorial optimization problems requiring custom heuristic design"], "open_weaknesses": ["Limited scalability of the current framework to a high number of objectives", "Applicability currently restricted to specific types of heuristic design tasks", "Potential for further performance gains by moving beyond off-the-shelf LLMs", "Computational cost associated with LLM interactions for large populations or many generations"]}, "artifacts": {"code_url": "https://github.com/Optima-CityU/LLM4AD", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.44, "experiments": {"benchmarks": ["Weibull instances", "TSPLIB"], "baselines": ["FunSearch (Romera-Paredes et al. 2024)", "EoH (Liu et al. 2024a)", "NSGA-II (Deb et al. 2002)", "MOEA/D (Zhang and Li 2007)"], "hardware": "Intel Core i7-11700 processor and 32GB of memory", "instance_sizes": [5000, 10000, 100000, 100, 500, 1000, 1002]}, "results": {"vs_baselines": {"FunSearch": "MEoH achieves significantly better optimal gap and running time on BPP (up to 10x faster, 20x better gap on C500), and superior performance on small TSP instances (0.018% gap vs 0.050%, 2.354s vs 3.418s), competitive on large TSP instances with faster running time.", "EoH": "MEoH achieves competitive or slightly worse optimal gap on BPP but is significantly faster (up to 10x faster), and superior performance on small TSP instances (0.018% gap vs 0.093%, 2.354s vs 25.917s), competitive on large TSP instances with significantly faster running time (355.754s vs 1515.992s).", "NSGA-II": "MEoH obtains best Hypervolume (HV) and Inverted Generational Distance (IGD) on BPP and TSP.", "MOEA/D": "MEoH obtains best Hypervolume (HV) and Inverted Generational Distance (IGD) on BPP and TSP."}, "scalability": "MEoH demonstrates improved efficiency and competitive performance on larger instances, achieving up to 10x faster running times compared to baselines.", "statistical_rigor": "Experiments were repeated three times; Hypervolume (HV) and Inverted Generational Distance (IGD) metrics were used to assess convergence and diversity.", "limitations_acknowledged": ["Limited to two or three objectives", "Limited range of heuristic design tasks"]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2407.12117", "arxiv_url": "https://arxiv.org/abs/2407.12117", "title": "MEMO: Fine-grained Tensor Management For Ultra-long Context LLM Training", "authors": ["Pinxue"], "abstract": "", "published_date": "2025-01-15", "affiliations": "Peking University, Tencent Inc.", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 5, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper successfully applies Mixed Integer Programming (MIP) to low-level GPU memory management, providing a concrete, successful template for our 'OR for AI systems' and 'GPUSched' research tracks."}, "brief": "Memo enables training 7B LLMs with 1M context on 8 GPUs by combining token-wise activation swapping with a bi-level Mixed Integer Programming (MIP) approach to eliminate memory fragmentation. The results are strong (52% MFU vs ~30% for DeepSpeed) and demonstrate that static memory planning via OR solvers outperforms dynamic allocators for repetitive Transformer workloads. The key takeaway is the bi-level MIP strategy—solving the allocation for one layer and broadcasting it—which makes the NP-hard memory planning tractable. We should adapt this MIP formulation for our own GPU scheduling and inference resource allocation (GPUSched) projects.", "methodology": {"core_method": "Fine-grained activation memory management combining token-wise recomputation and swapping with bi-level Mixed Integer Programming (MIP) for memory planning", "llm_role": "none", "llm_model_used": null, "search_type": "exact", "novelty_claim": "Memo is a novel LLM training framework designed for fine-grained activation memory management, enabling efficient training of 7B LLM with 1 million context length on only 8 A800 GPUs.", "components": ["token-wise activation recomputation", "token-wise activation swapping", "bi-level Mixed Integer Programming (MIP) for memory planning", "rounding buffers", "CUDA streams", "job profiler", "memory planner", "runtime executor"], "training_required": false}, "tags": {"methods": ["activation_recomputation", "activation_swapping", "mixed_integer_programming", "memory_management", "tensor_management", "distributed_training", "data_parallelism", "tensor_parallelism", "pipeline_parallelism", "sequence_parallelism", "context_parallelism", "gpu_memory_optimization", "cpu_offloading", "cuda_streams", "memory_defragmentation"], "problems": ["llm_training_efficiency", "long_context_llm_training", "activation_memory_optimization", "memory_fragmentation"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "megatron_lm", "specific_domain": "long_context_llm_training", "llm_coupling": null}, "problem": {"formal_name": "Fine-grained Tensor Management For Ultra-long Context LLM Training", "short": "LLM Long Context Training Memory Management", "class_": "tensor_management", "properties": ["long_context", "memory_constrained", "activation_memory", "memory_fragmentation"], "scale": "7B LLM with 1 million sequence length on 8 A800 GPUs, up to 8 million sequence length"}, "lineage": {"direct_ancestors": [{"paper": "Megatron-LM", "relationship": "built on top of"}, {"paper": "TransformerEngine", "relationship": "built on top of"}, {"paper": "activation_recomputation", "relationship": "improves upon naive application of"}, {"paper": "activation_swapping", "relationship": "improves upon naive application of"}, {"paper": "OLLA", "relationship": "extends memory planning concepts from"}], "closest_prior_work": "OLLA: Optimizing the Lifetime and Location of Arrays to Reduce the Memory Usage of Neural Networks", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Dynamic adjustment of the offloading fraction alpha during training", "Integration with more advanced or novel distributed parallelism strategies", "Extension of fine-grained memory management to other deep learning architectures", "Optimization of the initial profiling step for extremely large models"], "transferable_to": ["Other memory-intensive deep learning models (e.g., large CNNs, GNNs)", "General large-scale data processing with memory hierarchies (CPU-GPU, GPU-disk)", "Training LLMs on heterogeneous hardware environments"], "open_weaknesses": ["The profiling step, while mitigated, can still be memory-intensive for certain extreme cases", "Reliance on an external MIP solver (e.g., Gurobi) which may have licensing or availability constraints", "The offloading fraction alpha is determined statically before training, not adaptively during runtime", "Potential for CPU Out-Of-Host-Memory (OOHM) errors for excessively long sequences if alpha is not optimally chosen"]}, "artifacts": {"code_url": "https://github.com/pinxuezhao/MEMO", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.29, "experiments": {"benchmarks": ["GPT-7B", "GPT-13B", "GPT-30B", "GPT-65B"], "baselines": ["Megatron-LM", "DeepSpeed (DeepSpeed-Ulysses)"], "hardware": "NVIDIA A800 GPUs (80GB), 2TB CPU memory, NVLink, Infiniband", "instance_sizes": [4000, 8000, 16000, 32000, 64000, 128000, 256000, 384000, 512000, 640000, 768000, 896000, 1024000, 1152000, 1280000, 1408000, 8192000]}, "results": {"vs_baselines": {"Megatron-LM": "1.97x MFU improvement, supports longer sequences", "DeepSpeed": "1.80x MFU improvement, supports longer sequences"}, "scalability": "The maximum sequence length supported by Memo increases linearly with the number of GPUs, consistently maintaining an MFU of over 50%.", "statistical_rigor": "Convergence experiments were conducted for 1000 iterations, showing aligned loss curves across different configurations.", "limitations_acknowledged": []}, "analysis_date": "2026-02-19"}, {"arxiv_id": "2501.03508", "arxiv_url": "https://arxiv.org/abs/2501.03508", "title": "A Sequential Optimal Learning Approach to Automated Prompt Engineering in Large Language Models", "authors": ["Shuyang"], "abstract": "", "published_date": "2025-01-07", "affiliations": "Northwestern University, Stevens Institute of Technology", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 7, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper perfectly bridges your interests in 'OR formulations for AI' and 'LLM search efficiency.' It demonstrates that for limited-budget scenarios, rigorous Bayesian optimal learning (solved via MIP) beats the standard evolutionary approaches (EvoPrompt) we typically use."}, "brief": "Wang et al. treat prompt engineering as a Bayesian optimal experimental design problem, representing prompts as discrete feature vectors (template, tone, examples) and selecting the next candidate using a Knowledge-Gradient (KG) policy solved via mixed-integer second-order cone programming. Results are rigorous and show that this OR-based approach outperforms evolutionary (EvoPrompt) and bandit baselines on instruction induction tasks, specifically in low-sample regimes (N=30). The critical takeaway is the **replacement of random evolutionary mutation with a KG policy over a structured feature space** to maximize information gain per step. We should steal this formulation to optimize high-level meta-parameters or strategy selection in AlgoEvo, leveraging our team's OR background to solve our sample efficiency bottleneck.", "methodology": {"core_method": "Sequential Optimal Learning with Knowledge-Gradient (KG) policy using Bayesian regression", "llm_role": "instruction_generator, evaluator", "llm_model_used": "GPT-3.5", "search_type": "improvement", "novelty_claim": "We introduce a sequential optimal learning framework for automated prompt engineering to guide through the process of designing a sequence of prompts that effectively elicit accurate responses from an LLM.", "components": ["feature-based prompt representation", "Bayesian regression model", "Knowledge-Gradient (KG) policy", "mixed-integer second-order cone optimization", "logit link function"], "training_required": true}, "tags": {"methods": ["optimal_learning", "knowledge_gradient_policy", "bayesian_regression", "sequential_decision_making", "mixed_integer_conic_optimization", "llm_as_evaluator", "llm_as_instruction_generator", "feature_engineering", "black_box_optimization", "bayesian_optimization"], "problems": ["llm_prompt_optimization", "instruction_induction", "automated_prompt_engineering", "constrained_optimization"], "contribution_type": ["new_method", "sota_result", "framework"], "framework_lineage": "knowledge_gradient", "specific_domain": "instruction_induction", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Automated Prompt Engineering in Large Language Models", "short": "Automated Prompt Engineering", "class_": "LLM_prompt_optimization", "properties": ["sequential", "feature_based", "limited_evaluation_budget", "black_box_LLM_compatible", "interpretable_prompts", "constrained_search_space"], "scale": "27360 prompt feature combinations"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2402.09723", "relationship": "builds on prompt selection methods like TRIPLE"}, {"paper": "arXiv:2406.14449", "relationship": "builds on edit-based prompt refinement methods like EvoPrompt"}, {"paper": "INFORMS journal on Computing, 21(4):599–613, 2009", "relationship": "applies Knowledge-Gradient policy from"}, {"paper": "INFORMS Journal on Computing, 28(4):721–735, 2016", "relationship": "leverages optimal learning with combinatorial feature selection from"}, {"paper": "Management Science, 66(9):4226–4245, 2020", "relationship": "leverages scalable KG computation methods from"}], "closest_prior_work": "INFORMS Journal on Computing, 28(4):721–735, 2016", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["investigation of continuous representations of prompts by embedding vectors", "explore other optimal learning policies for prompt selection", "apply to other LLM tasks with costly evaluation", "further improve computational efficiency for extremely large feature spaces"], "transferable_to": ["other_llm_tasks_with_costly_evaluation", "black_box_optimization_with_limited_budget", "automated_design_of_other_llm_components", "scientific_discovery_with_expensive_experiments"], "open_weaknesses": ["limited_to_discrete_feature_representations", "less_advantageous_for_easy_tasks_with_flat_performance_functions", "potential_computational_cost_for_extremely_large_feature_spaces", "reliance_on_validation_data_for_scoring"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.61, "experiments": {"benchmarks": ["instruction induction tasks [15]"], "baselines": ["EvoPrompt [12]", "TRIPLE [33]", "SOPL-TS (Thompson Sampling)", "SOPL-Greedy (Adaptive Myopic)"], "hardware": "Unspecified HPC facility (Northwestern University Quest)", "instance_sizes": [10, 100]}, "results": {"vs_baselines": {"EvoPrompt": "6.47% improvement in average test score", "TRIPLE": "11.99% improvement in average test score", "SOPL-TS": "5.60% improvement in average test score", "SOPL-Greedy": "9.23% improvement in average test score"}, "scalability": "The SOPL-KG is capable of discovering high-quality prompts within given prompt evaluations and outperforms other methods with fewer iterations (N=10, N=20).", "statistical_rigor": "Average test score across 20 replications with different random seeds, reporting mean and standard deviation.", "limitations_acknowledged": ["Current framework is limited to discrete feature-based prompt representations, with future work on continuous embedding vectors."]}, "analysis_date": "2026-02-19"}, {"arxiv_id": "2412.14995", "arxiv_url": "https://arxiv.org/abs/2412.14995", "title": "HSEvo: Elevating Automatic Heuristic Design with Diversity-Driven Harmony Search and Genetic Algorithm Using LLMs", "authors": ["Pham"], "abstract": "", "published_date": "2024-12-19", "affiliations": "George Mason University, Hanoi University of Science and Technology", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 7, "problem": 8, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While the use of 'Harmony Search' specifically is somewhat arbitrary, the architectural improvements—specifically 'Flash Reflection' for token efficiency and the hybrid numerical parameter tuning step—are directly transferable engineering optimizations for our AlgoEvo pipeline."}, "brief": "HSEvo extends LLM-based evolutionary search (LLM-EPS) by integrating a numerical parameter tuning step (Harmony Search) and a token-efficient 'Flash Reflection' mechanism that batches analysis of parent pairs. They report superior results over ReEvo and FunSearch on Bin Packing and TSP, validated by proposed diversity metrics based on code embeddings. **Key Takeaway:** We should implement the hybrid tuning pattern: explicitly parsing LLM-generated code to extract constants and tuning them with a cheap numerical optimizer (rather than asking the LLM to tune parameters), and adopt batched reflections to reduce inference costs.", "methodology": {"core_method": "Adaptive LLM-based Evolutionary Program Search (LLM-EPS) framework combining Harmony Search and Genetic Algorithm with diversity-driven mechanisms", "llm_role": "evolutionary_search", "llm_model_used": "gpt-4o-mini-2024-07-18", "search_type": "hybrid", "novelty_claim": "HSEvo is a novel LLM-EPS framework that balances diversity and objective performance using a harmony search algorithm and introduces two new diversity measurement metrics (SWDI, CDI) for LLM-EPS.", "components": ["Initialization", "Selection", "Flash Reflection", "Crossover", "Elitist Mutation", "Harmony Search"], "training_required": false}, "tags": {"methods": ["llm_evolutionary_search", "llm_as_heuristic", "llm_code_generation", "harmony_search", "genetic_algorithm", "evolution_of_heuristics", "program_synthesis", "shannon_wiener_diversity_index", "cumulative_diversity_index", "code_embedding", "abstract_syntax_tree", "in_context_learning", "llm_prompt_optimization"], "problems": ["automated_heuristic_design", "bin_packing_online", "traveling_salesman_problem", "orienteering_problem"], "contribution_type": ["new_method", "sota_result", "empirical_study", "framework"], "framework_lineage": "reevo", "specific_domain": null, "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Automatic Heuristic Design", "short": "AHD", "class_": "algorithm_design", "properties": ["NP-hard", "combinatorial_optimization", "black-box"], "scale": "5k items (BPO), 100 nodes (TSP), 50 nodes (OP)"}, "lineage": {"direct_ancestors": [{"paper": "FunSearch", "relationship": "improves upon LLM-EPS approach of"}, {"paper": "Evolution of Heuristics (EoH)", "relationship": "improves upon LLM-EPS approach of"}, {"paper": "ReEvo", "relationship": "extends and improves LLM-EPS framework of"}], "closest_prior_work": "ReEvo", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Explore other diversity-maintaining mechanisms in LLM-EPS", "Apply HSEvo to a wider range of AHD problems", "Investigate the theoretical properties of heuristic search spaces in LLM-EPS", "Further optimize LLM token usage for cost-effectiveness"], "transferable_to": ["Other combinatorial optimization problems", "Automated program synthesis for general algorithms", "Multi-objective optimization problems"], "open_weaknesses": ["Scalability to very large problem instances", "Generalizability of diversity metrics across different LLM-EPS architectures", "Further reducing LLM inference costs", "The trade-off between diversity and exploitation still requires careful balancing"]}, "artifacts": {"code_url": "https://github.com/datphamvn/HSEvo", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.73, "experiments": {"benchmarks": ["Bin Packing Online (BPO)", "Traveling Salesman Problem (TSP)", "Orienteering Problem (OP)"], "baselines": ["FunSearch", "EoH", "ReEvo"], "hardware": "single core of an Xeon Processors CPU", "instance_sizes": [5000, 100, 50]}, "results": {"vs_baselines": {"FunSearch": "Outperforms on BPO (1.07 vs 2.05) and TSP (0.02 vs 0.09)", "EoH": "Outperforms on BPO (1.07 vs 3.17) and TSP (0.02 vs 1.09), competitive on OP (-14.62 vs -14.62)", "ReEvo": "Outperforms on BPO (1.07 vs 2.48), TSP (0.02 vs 0.05), and OP (-14.62 vs -14.54)"}, "scalability": "HSEvo maintains a good balance between diversity and optimization performance while remaining cost-effective, but specific scalability with problem size is not detailed.", "statistical_rigor": "Experiments were conducted with 3 independent runs, and standard deviations are reported for objective scores and diversity indices.", "limitations_acknowledged": []}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2407.15504", "arxiv_url": "https://arxiv.org/abs/2407.15504", "title": "Fundamental Limits of Prompt Compression: A Rate-Distortion Framework for Black-Box Language Models", "authors": ["Alliot"], "abstract": "", "published_date": "2024-12-11", "affiliations": "UT Austin, EPFL", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 8, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper applies rigorous OR techniques (Linear Programming, Duality) to define the theoretical limits of prompt compression, directly aligning with your 'OR for AI' interest. It proves that current fixed-rate heuristics are suboptimal and offers a concrete, variable-rate alternative that could significantly improve context efficiency in our evolutionary search frameworks."}, "brief": "Nagle et al. formalize prompt compression as a rate-distortion problem, deriving the fundamental theoretical limit via a dual linear program and proposing 'Adaptive QuerySelect,' a variable-rate compression technique. The results are rigorous: they calculate exact limits on synthetic data and use beam search approximations for NLP, demonstrating that existing fixed-rate methods leave significant performance on the table. The key takeaway is that **variable-rate compression**—keeping tokens based on a confidence threshold rather than a fixed percentage—is essential for approaching optimality; this allows 'hard' queries to retain more context while aggressively compressing 'easy' ones. This is immediately actionable for our AlgoEvo work: we should replace fixed-window history truncation with a query-aware, variable-rate compressor to maximize the useful information in our limited context window.", "methodology": {"core_method": "Rate-distortion theory formalized as a linear program, solved via its dual using a geometric algorithm; Adaptive QuerySelect (query-aware, variable-rate token classification)", "llm_role": "token classifier", "llm_model_used": "XLM RoBERTa Large", "search_type": "exact", "novelty_claim": "We formalize prompt compression as a rate-distortion problem, derive its fundamental limits via a dual linear program, and propose Adaptive QuerySelect, a query-aware, variable-rate method that significantly reduces the gap to optimality.", "components": ["Rate-distortion function", "Linear programming", "Dual linear program", "Geometric algorithm", "Adaptive QuerySelect"], "training_required": true}, "tags": {"methods": ["rate_distortion_theory", "linear_programming", "dual_linear_program", "geometric_algorithm", "token_classification", "llm_in_the_loop", "llm_as_heuristic", "llm_fine_tuned", "beam_search", "adaptive_queryselect", "queryselect"], "problems": ["prompt_compression", "llm_inference_efficiency", "black_box_optimization"], "contribution_type": ["new_method", "theoretical_result", "empirical_study", "framework", "sota_result"], "framework_lineage": "llmlingua_2", "specific_domain": "prompt_compression", "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Prompt Compression for Large Language Models", "short": "Prompt Compression", "class_": "llm_optimization", "properties": ["black_box", "token_level", "hard_prompts", "query_aware", "rate_distortion_framework"], "scale": "4-10 tokens (synthetic), up to 15 tokens (small natural language), hundreds-thousands tokens (large natural language)"}, "lineage": {"direct_ancestors": [{"paper": "PWJ+24", "relationship": "adapts LLMLingua-2 from"}], "closest_prior_work": "LLMLingua-2", "novelty_type": "new_problem"}, "extensions": {"next_steps": ["exhaustively study adaptive_queryselect on natural_language_datasets", "develop_better_semantic_distortion_functions", "explore_tokenization_impact_further", "investigate_generative_compression_schemes"], "transferable_to": ["other_llm_efficiency_problems", "black_box_model_compression", "different_llm_tasks"], "open_weaknesses": ["exact_optimal_curve_computation_intractable_for_large_prompts_vocabularies", "quality_of_gpt4_generated_labels_for_natural_language_impacts_performance_gap", "proper_semantic_distortion_function_is_an_open_problem", "standard_tokenization_limits_low_rates_on_synthetic_data"]}, "artifacts": {"code_url": "https://github.com/acnagle/fundamental-limits", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.71, "experiments": {"benchmarks": ["Synthetic dataset (binary prompts)", "Small natural language dataset", "NarrativeQA"], "baselines": ["Selective Context [LDLG23]", "LLMLingua [JWLYQ23]", "LLMLingua Query [JWL+23]", "LLMLingua-2 [PWJ+24]"], "hardware": "AMD Ryzen Threadripper PRO 5975WX CPU, 2x Nvidia RTX 4090 GPUs; AMD EPYC 7742 64-Core Processor, 4x 80GB SMX4 A100 GPUs", "instance_sizes": [4, 10, 15, 100, 1000]}, "results": {"vs_baselines": {"Adaptive QuerySelect": "outperforms all baselines on synthetic and small natural language datasets, significantly reducing gap to theoretical limit", "QuerySelect": "outperforms query-agnostic LLMLingua-2", "LLMLingua-2": "achieves lower distortion than no compression for higher rates on natural language"}, "scalability": "Exact optimal curve computation is intractable for large vocabulary sizes and long prompts; beam search approximates for hundreds-thousands of tokens with O(BN^2) inference calls.", "statistical_rigor": "Evaluated on 1400 synthetic examples (200 per query); hyperparameter grid search with separate test set for model selection; average rate and distortion reported.", "limitations_acknowledged": ["Exact optimal curve computation intractable for large prompts/vocabularies", "Quality of GPT-4 generated labels for natural language impacts performance gap", "Proper semantic distortion function is an open problem", "Standard tokenization limits low rates on synthetic data"]}, "analysis_date": "2026-02-19"}, {"arxiv_id": "2411.19744", "arxiv_url": "https://arxiv.org/abs/2411.19744", "title": "Amplifying human performance in combinatorial competitive programming", "authors": ["Petar"], "abstract": "", "published_date": "2024-11-29", "affiliations": "Google DeepMind", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 6, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This is a direct, successful application of LLM evolutionary search (FunSearch) to the exact class of OR/VRP problems we study. While the core algorithm is standard FunSearch, the engineering pattern (human backbone + evolved scoring function with state switching) is a blueprint we should replicate immediately."}, "brief": "DeepMind applies FunSearch (using Gemini 1.5 Flash) to evolve scoring functions within human-written greedy backbones for Hash Code and AtCoder problems, achieving top-1% or rank-1 performance against humans. The results are robust, beating top human teams on 5/8 historical contests using a generic evolutionary setup. The critical takeaway is the 'switching variable' technique: using a single evolved function to handle multiple distinct decision points (e.g., selecting a vehicle vs. selecting a route) by passing a state flag, rather than evolving multiple interacting functions. This validates that generalist models (Flash) are sufficient for high-end OR evolution without code-specific fine-tuning. We should adopt their 'Backbone + Scorer' architecture for our VRP/Scheduling work immediately.", "methodology": {"core_method": "FunSearch, an evolutionary algorithm for program search using LLMs and a systematic evaluator", "llm_role": "evolutionary_search", "llm_model_used": "Gemini 1.5 Flash 002", "search_type": "improvement", "novelty_claim": "By applying FunSearch on a human-designed solution backbone across several Hash Code competition tasks, we are able to significantly amplify the scores obtained by the backbone.", "components": ["FunSearch", "human-designed greedy algorithm backbone", "LLM-evolved scoring function", "systematic evaluator"], "training_required": false}, "tags": {"methods": ["funsearch", "evolutionary_algorithm", "program_synthesis", "llm_evolutionary_search", "llm_code_generation", "llm_as_heuristic", "greedy_algorithm", "hill_climbing"], "problems": ["combinatorial_optimization", "data_center_optimization", "delivery_routing", "video_streaming_optimization", "self_driving_rides_scheduling", "photo_slideshow_optimization", "book_scanning_optimization", "traffic_signaling_scheduling", "mentorship_and_teamwork_assignment", "purse_seine_fishing"], "contribution_type": ["new_method", "sota_result", "empirical_study", "framework"], "framework_lineage": "funsearch", "specific_domain": null, "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Combinatorial Competitive Programming", "short": "CCP", "class_": "algorithm_design", "properties": ["NP-hard", "intractable", "heuristic-based", "scoring_function_optimization"], "scale": "Grid sizes up to 5000x5000, very large inputs for Hash Code problems"}, "lineage": {"direct_ancestors": [{"paper": "Romera-Paredes et al., 2024", "relationship": "leverages FunSearch from"}], "closest_prior_work": "Romera-Paredes et al., 2024", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["further_engineering_to_optimize_under_time_constraints", "application_to_other_competitive_programming_paradigms", "exploring_more_complex_backbone_structures", "investigating_interpretability_of_evolved_functions"], "transferable_to": ["other_np_hard_optimization_problems", "automated_algorithm_design_for_diverse_algorithms", "human_ai_collaborative_problem_solving_in_complex_domains", "real_world_software_engineering_problems"], "open_weaknesses": ["potential_llm_exposure_to_training_data", "internal_fitness_mismatch_with_contest_evaluation", "scalability_for_extremely_large_search_spaces", "interpretability_of_complex_evolved_heuristics"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.84, "experiments": {"benchmarks": ["Hash Code 2015 Qualification: Optimize a Data Center", "Hash Code 2016 Qualification: Delivery", "Hash Code 2017 Qualification: Streaming videos", "Hash Code 2018 Qualification: Self-driving rides", "Hash Code 2019 Qualification: Photo slideshow", "Hash Code 2020 Qualification: Book scanning", "Hash Code 2021 Qualification: Traffic signaling", "Hash Code 2022 Qualification: Mentorship and Teamwork", "AtCoder Heuristic Contest 039"], "baselines": ["Human contestants (top teams)", "Backbone solution with base scoring function", "Backbone solution with 2-hour evolved scoring function"], "hardware": "Multiple devices (memory limit 10GB, execution time limit 1800s)", "instance_sizes": [1500, 2000, 3000, 4000, 5000]}, "results": {"vs_baselines": {"Backbone with base scoring function": "Not in top percentile or finals qualification for any Hash Code year.", "Backbone with 2-hour evolved scoring function": "Sufficient to qualify for finals in all but two Hash Code iterations.", "Top human teams (Hash Code)": "Outperforms in 5/8 Hash Code qualification rounds (2015, 2018, 2020, 2021, 2022), consistently in top percentile.", "Top human teams (AtCoder AHC 039)": "Ranks 9th-17th (95% CI)."}, "scalability": "Performance improves with more computational resources, with 2-hour evolved solutions showing strong merit.", "statistical_rigor": "Reports average score with standard deviation (3,521.9 ± 424.4) and 95% confidence interval (528,285.4 ± 10,396.6) for AtCoder AHC 039.", "limitations_acknowledged": ["LLM (Gemini 1.5 Flash 002) might have been exposed to Hash Code subroutines in training data (mitigated by prompt structure and iterative improvement)", "Internal fitness function for AtCoder might not correspond to actual contest evaluation (mitigated by new dataset and bootstrap estimates)"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2411.17404", "arxiv_url": "https://arxiv.org/abs/2411.17404", "title": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical Modeling Problem Solving", "authors": ["Teng"], "abstract": "", "published_date": "2024-11-26", "affiliations": "Huawei, The University of Hong Kong", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 8, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "The paper directly addresses the lack of process-labeled data for OR modeling, which is a bottleneck for our PRM work. The proposed 'Pairwise Preference' mechanism at the leaf nodes is a concrete architectural fix for the noisy reward signal problem we face in AlgoEvo."}, "brief": "Wang et al. propose BPP-Search, combining Beam Search, a Process Reward Model (PRM), and a final Pairwise Preference Model to generate LP/MIP models from natural language. While their new 'StructuredOR' dataset is small (38 test instances), it uniquely provides intermediate modeling labels (sets, parameters, variables) essential for training PRMs in this domain. The key takeaway is their finding that PRMs are effective for pruning but imprecise for final ranking; they solve this by adding a pairwise preference model at the leaf layer—a technique we should immediately steal to improve selection robustness in our MASPRM and evolutionary search pipelines. This is a competent execution of 'LLM + Search' applied specifically to our OR niche.", "methodology": {"core_method": "BPP-Search: Tree-of-Thought with Beam Search, Process Reward Model, and Pairwise Preference Algorithm", "llm_role": "policy_model_for_generation, evaluator_for_search_guidance, data_generator", "llm_model_used": "GPT-4o (policy), Qwen2.5-Math-1.5B (PRM/Preference)", "search_type": "hybrid", "novelty_claim": "BPP-Search integrates reinforcement learning into a tree-of-thought structure using Beam search, a Process reward model, and a pairwise Preference algorithm to enhance mathematical modeling problem solving.", "components": ["Tree-of-Thought", "Beam Search", "Process Reward Model", "Pairwise Preference Model", "Policy Model (LLM)"], "training_required": true}, "tags": {"methods": ["llm_as_heuristic", "llm_as_evaluator", "llm_code_generation", "llm_in_the_loop", "tree_of_thought", "beam_search", "process_reward_model", "pairwise_preference_model", "supervised_learning", "greedy_search", "epsilon_greedy_search", "program_synthesis"], "problems": ["natural_language_to_mathematical_model", "linear_programming", "mixed_integer_programming", "optimization_problem_formulation", "mathematical_reasoning"], "contribution_type": ["new_method", "new_benchmark", "sota_result", "framework"], "framework_lineage": "tree_of_thought", "specific_domain": "optimization_problem_formulation", "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Mathematical Modeling Problem Solving from Natural Language", "short": "NL2MM", "class_": "program_synthesis", "properties": ["linear_programming", "mixed_integer_programming", "abstract_modeling", "concrete_instance_modeling", "natural_language_input"], "scale": "up to 289 problems, with abstract models having average 2.22 sets, 3.95 parameters, 1.40 variables, 2.38 constraints, and concrete models having up to 22 variables per abstract variable"}, "lineage": {"direct_ancestors": [{"paper": "Yao et al., 2023", "relationship": "extends Tree-of-Thought"}, {"paper": "Lowerre and Reddy, 1976", "relationship": "integrates Beam Search"}, {"paper": "Uesato et al., 2022", "relationship": "integrates Process Reward Model"}], "closest_prior_work": "Yao et al., 2023", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["explore wider and deeper tree-of-thought structures", "improve LLM handling of large numerical inputs in OR problems", "reduce computational cost of tree-of-thought search", "extend to other types of mathematical models beyond LP/MIP"], "transferable_to": ["program_synthesis_for_other_domains", "complex_reasoning_tasks", "automated_algorithm_design", "scientific_discovery_problems"], "open_weaknesses": ["high computational cost for extensive tree exploration", "LLM limitations in accurately reproducing large numerical values", "PRM scoring imprecision for regression-like tasks", "trade-off between tree-of-thought depth/width and computational cost"]}, "artifacts": {"code_url": "https://github.com/LLM4OR/StructuredOR", "models_released": false, "new_benchmark": true}, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_status": "stable", "bridge_score": 0.6191, "is_bridge": true, "priority_score": 6.93, "experiments": {"benchmarks": ["StructuredOR", "NL4OPT", "MAMO-ComplexLP"], "baselines": ["CoT-BMLD", "CoT-SPVOC", "SC", "ToT-Randomly-Chosen", "ToT-Rethink", "ToT-Fully-Traverse", "Greedy Search Variant", "Beam Search Variant"], "hardware": "null", "instance_sizes": [30, 72, 143]}, "results": {"vs_baselines": {"CoT": "+47.4% correct rate on StructuredOR", "SC": "+33.3% correct rate on StructuredOR", "ToT-Randomly-Chosen": "+33.3% correct rate on StructuredOR", "ToT-Rethink": "+21.8% correct rate on StructuredOR", "Greedy Search Variant": "+12.0% correct rate on StructuredOR", "Beam Search Variant": "+16.6% correct rate on StructuredOR"}, "scalability": "BPP-Search significantly reduces reasoning steps compared to full ToT traversal, improving efficiency, but LLMs themselves face limitations with large numerical inputs in problem instances.", "statistical_rigor": "Results are reported as correct rates on fixed test sets (30, 72, 143 problems) without explicit mention of multiple runs, variance, or significance tests.", "limitations_acknowledged": ["Trade-offs in ToT structure (width/depth) vs. computational cost", "LLM capability limitations with large numerical values in OR problems", "Limited computational resources for extensive tree exploration"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2411.01679", "arxiv_url": "https://arxiv.org/abs/2411.01679", "title": "Autoformulation of Mathematical Optimization Models Using LLMs", "authors": ["Nicolás"], "abstract": "", "published_date": "2024-11-03", "affiliations": "University of Cambridge, University of Hawaii at Manoa", "category": "Generative AI for OR", "relevance": {"methodological": 8, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper introduces a rigorous mechanism (SMT-based symbolic pruning) to handle redundancy in LLM-generated mathematical structures, which is directly transferable to our evolutionary search pipelines (AlgoEvo/EvoCut) to drastically improve sample efficiency. It also establishes a new baseline for our OR benchmarking work."}, "brief": "Astorga et al. frame optimization modeling as a hierarchical Monte-Carlo Tree Search (MCTS) problem, using LLMs to generate components and—crucially—employing SMT solvers to prune mathematically equivalent branches (e.g., recognizing `x+y` and `y+x` as identical). They achieve SOTA results on NL4OPT and IndustryOR, outperforming fine-tuned models like ORLM while using significantly fewer samples than naive approaches. **Key Takeaway:** The integration of symbolic equivalence checking (SMT) to prune the search tree is a technique we should immediately steal; implementing this in AlgoEvo would allow us to discard functionally identical code/math mutants before expensive evaluation, directly addressing our sample efficiency bottleneck.", "methodology": {"core_method": "LLM-enhanced Monte-Carlo Tree Search with symbolic pruning and LLM-based evaluation", "llm_role": "conditional_hypothesis_generator, evaluator", "llm_model_used": "GPT4-0613", "search_type": "constructive", "novelty_claim": "A novel approach combining LLMs, symbolic tools, and MCTS for efficient and systematic exploration of optimization model space, using LLMs as hypothesis generators and correctness evaluators, and symbolic pruning.", "components": ["Monte-Carlo Tree Search (MCTS)", "Large Language Models (LLMs)", "Symbolic pruning (Satisfiability Modulo Theories solvers)", "Deterministic parser", "Dual reward system (LLM evaluation + solver feedback)"], "training_required": false}, "tags": {"methods": ["monte_carlo_tree_search", "llm_in_the_loop", "llm_as_heuristic", "llm_as_evaluator", "symbolic_pruning", "smt_solvers", "hierarchical_decomposition", "dual_reward_system", "uct", "deterministic_parser"], "problems": ["automated_optimization_modeling", "program_synthesis", "linear_programming", "mixed_integer_programming", "convex_optimization", "non_convex_optimization"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study", "reproducibility_study"], "framework_lineage": null, "specific_domain": "automated_optimization_modeling", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Autoformulation of Mathematical Optimization Models", "short": "Autoformulation", "class_": "program_synthesis", "properties": ["linear programming", "mixed-integer programming", "convex", "non-convex"], "scale": "10-100 variables"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2303.11366", "relationship": "builds on LLM-based autoformulation frameworks like Reflexion"}, {"paper": "arXiv:2309.08532", "relationship": "builds on LLM-based autoformulation frameworks like Chain-of-Experts"}, {"paper": "arXiv:2402.10172", "relationship": "builds on LLM-based autoformulation frameworks like OptiMUS"}], "closest_prior_work": "arXiv:2309.08532", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["develop collaborative frameworks with human-in-the-loop", "integrate active acquisition techniques", "explore retrieval-augmented generation for LLMs", "inform LLM finetuning via outcome- or process-level supervision"], "transferable_to": ["wider range of problem types and complexities beyond LP/MILP", "problems requiring advanced or creative reformulations", "non_linear_programming", "problems requiring convex reformulations or relaxation strategies"], "open_weaknesses": ["subtle errors in LLM-generated formulations", "challenges in verifying correctness for complex formulations", "objective-value proxy not always capturing semantic correctness", "need for sophisticated approaches for complex parser transformations", "potential redundant paths due to conservative SMT pruning", "diminishing returns on correct solution discovery with increased computational budget"]}, "artifacts": {"code_url": "https://github.com/jumpynitro/AutoFormulator", "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_1", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.11, "experiments": {"benchmarks": ["NL4OPT", "IndustryOR", "MAMO (ComplexLP)", "ComplexOR"], "baselines": ["ORLMLlama3-8B", "Standard (zero-shot prompting)", "Reflexion", "Chain-of-Experts", "OptiMUS", "Tree-of-Thought (DFS)", "Sequential (naive sampling)"], "hardware": "null", "instance_sizes": [10, 100]}, "results": {"vs_baselines": {"ORLMLlama3-8B": "Autoformulator (All) outperforms by +6.92% on NL4OPT, +10.0% on IndustryOR, +23.0% on MAMO (ComplexLP), +27.8% on ComplexOR.", "OptiMUS": "Autoformulator (All) outperforms by +13.82% on NL4OPT and +5.5% on ComplexOR.", "Chain-of-Experts": "Autoformulator (All) outperforms by +22.1% on MAMO (ComplexLP).", "Tree-of-Thought (N=3)": "Autoformulator (N=3) significantly outperforms across all benchmarks (e.g., +25.68% on NL4OPT).", "Sequential (N=3)": "Autoformulator (N=3) significantly outperforms across all benchmarks (e.g., +31.39% on NL4OPT)."}, "scalability": "The method achieves substantial efficiency gains (1000x fewer generations) compared to non-hierarchical search, though marginal gains per rollout diminish with increased rollouts.", "statistical_rigor": "Accuracy is reported as the proportion of problems yielding optimal objective values. Pass@N and Best-of-N metrics are used. Statistical significance (p-value) and correlation coefficients are reported for evaluation methods.", "limitations_acknowledged": ["LLM-generated formulations may contain subtle errors leading to incorrect solutions.", "Verifying correctness becomes increasingly challenging for complex formulations.", "Accuracy based on objective values does not always capture semantic correctness.", "More complex transformations may require sophisticated approaches.", "Conservative treatment of non-decidable SMT equivalence checks may lead to redundant paths.", "Diminishing returns on correct solution discovery with increased computational budget."]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2410.22657", "arxiv_url": "https://arxiv.org/abs/2410.22657", "title": "Automatic programming via large language models with population self-evolution for dynamic job shop scheduling problem", "authors": ["Jin"], "abstract": "", "published_date": "2024-10-30", "affiliations": "Huazhong University of Science and Technology", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 6, "problem": 8, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While the problem (Dynamic JSSP) is a perfect match, the methodological advance ('Self-Evolution') yields negligible gains (<1%) over the existing ReEvo framework. However, the prompt structure for individual history reflection is a concrete tactic we could test in AlgoEvo."}, "brief": "This paper introduces SeEvo, an LLM-based evolutionary search for Dynamic Job Shop Scheduling heuristics that adds an 'individual self-reflection' loop—prompting the LLM to analyze performance differences of a specific rule before and after mutation—alongside standard population-level reflection. While they claim significant improvements over GP/GEP and DRL, the ablation study reveals only a marginal <1% improvement over the existing ReEvo framework on benchmark instances. The primary takeaway for us is the specific prompt engineering technique of injecting an individual's mutation history (previous code vs. current code performance) into the context to guide the next mutation, which could potentially improve sample efficiency in our own evolutionary loops despite their weak empirical validation.", "methodology": {"core_method": "LLM-based population self-evolutionary (SeEvo) method for automatic heuristic dispatching rules (HDRs) design", "llm_role": "heuristic_generator", "llm_model_used": "gpt-3.5-turbo-0125, GLM-3-Turbo", "search_type": "hybrid", "novelty_claim": "This paper proposes a novel population self-evolutionary (SeEvo) method, a general search framework inspired by the self-reflective design strategies of human experts.", "components": ["Individual Encoding", "Initialization", "Individual Co-Evolution Reflection", "Individual Self-Evolution Reflection", "Collective Evolution Reflection", "Crossover", "Mutation", "Individual Evaluation"], "training_required": true}, "tags": {"methods": ["llm_evolutionary_search", "evolution_of_heuristics", "program_synthesis", "self_improving_search", "genetic_algorithm", "llm_as_heuristic_generator", "llm_in_the_loop"], "problems": ["job_shop_scheduling", "dynamic_scheduling", "heuristic_evolution", "operator_discovery"], "contribution_type": ["new_method", "framework", "sota_result"], "framework_lineage": "reevo", "specific_domain": "job_shop_scheduling", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Dynamic Job Shop Scheduling Problem", "short": "DJSSP", "class_": "scheduling", "properties": ["dynamic", "random_job_arrivals", "minimizing_makespan"], "scale": "15-100 jobs, 10-20 machines"}, "lineage": {"direct_ancestors": [{"paper": "ReEvo", "relationship": "extends reflective evolutionary optimization concepts from"}], "closest_prior_work": "ReEvo", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["fine_tune_llms_for_djssp_knowledge", "explore_multiple_dispatching_rules_in_dynamic_cases", "investigate_other_scheduling_objectives"], "transferable_to": ["flow_shop_scheduling", "vehicle_routing_problems", "other_combinatorial_optimization_problems"], "open_weaknesses": ["llms_not_fine_tuned_for_djssp_specific_knowledge", "only_single_hdr_employed_in_dynamic_cases", "limited_statistical_rigor_in_experimental_evaluation"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.28, "experiments": {"benchmarks": ["Taillard (TA) datasets", "Demirkol (DMU) datasets"], "baselines": ["Random", "LPT", "SPT", "STPT", "MPSR", "LSO", "SPT/TWKR", "SPT×TWK", "SPT+SSO", "SPT/LSO", "TWKR", "SRM", "SSO", "GEP", "MTGP", "DRL-Liu", "DRL-Chen", "DRL-Zhang", "DRL-GAT", "DRL-GCN"], "hardware": "Intel(R) Xeon(R) W-3365 CPU @ 2.70GHz running Ubuntu 20.04", "instance_sizes": [15, 20, 30, 40, 50, 100]}, "results": {"vs_baselines": {"DRL-Liu": "SeEvo(GPT3.5) achieved -1.9% mean makespan vs DRL-Liu on DMU, SeEvo(GLM3) achieved -4.2% vs DRL-Liu on TA.", "GEP": "SeEvo(GPT3.5) achieved -5.2% mean makespan vs GEP on DMU, SeEvo(GLM3) achieved -4.6% vs GEP on TA.", "MTGP": "SeEvo(GPT3.5) achieved -3.2% mean makespan vs MTGP on DMU, SeEvo(GLM3) achieved -5.4% vs MTGP on TA.", "ReEvo": "SeEvo(GLM3) achieved -0.3% to -0.6% mean makespan vs ReEvo on ablation study instances.", "HDRs": "SeEvo consistently outperforms common HDRs across static and dynamic benchmarks."}, "scalability": "SeEvo effectively schedules and generalizes across JSSP cases of different sizes and is highly capable of handling unseen dynamic environments and uncertainties.", "statistical_rigor": "Results are reported as makespan values on benchmark instances and average makespan on dynamic cases. No explicit mention of multiple runs per instance or statistical significance tests for comparisons, but convergence curves show average makespan over datasets.", "limitations_acknowledged": ["LLMs are used via scheduling APIs without direct fine-tuning for specific DJSSP knowledge.", "Only a single HDR is employed in dynamic cases, limiting performance compared to static cases."]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2410.22296", "arxiv_url": "https://arxiv.org/abs/2410.22296", "title": "Generalists vs. Specialists: Evaluating LLMs on Highly-Constrained Biophysical Sequence Optimization Tasks", "authors": ["Angelica"], "abstract": "", "published_date": "2024-10-29", "affiliations": "Genentech, New York University", "category": "Generative AI for OR", "relevance": {"methodological": 8, "problem": 6, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper provides a concrete, mathematically motivated loss function (MargE) that outperforms DPO and PPO for 'LLM as optimizer' loops. It directly addresses your focus on 'RL-infused evolution' and 'sample efficiency' by demonstrating exactly why DPO causes mode collapse in constrained optimization."}, "brief": "The authors propose LLOME, a bilevel optimization framework that fine-tunes an LLM using 'MargE' (Margin-Aligned Expectation), a loss function that weights gradient updates by the magnitude of reward improvement (margin) rather than simple preference rankings. Results are rigorous and demonstrate that while DPO leads to generator collapse and infeasibility in constrained spaces, MargE maintains diversity and significantly improves sample efficiency, matching specialized solvers like LaMBO-2 on medium-difficulty tasks. The critical takeaway is that standard alignment methods (DPO/RLHF) are ill-suited for optimization because they discard information about *how much* better a solution is; MargE fixes this by satisfying the Strong Interpolation Criteria. We should immediately evaluate replacing the RL/update component in AlgoEvo with the MargE objective to improve the stability and quality of our evolved heuristics.", "methodology": {"core_method": "LLOME (Language Model Optimization with Margin Expectation) bilevel optimization with Margin-Aligned Expectation (MargE) loss", "llm_role": "optimization_driver", "llm_model_used": "PYTHIA 2.8B", "search_type": "hybrid", "novelty_claim": "LLOME is a bilevel optimization algorithm enabling LLMs to learn from online feedback for constrained sequence optimization, paired with the novel Margin-Aligned Expectation (MargE) loss.", "components": ["Bilevel Optimization", "Margin-Aligned Expectation (MargE) Loss", "Iterative Refinement", "Dataset Formatting (PropEn)", "Genetic Algorithm (pre-solver)", "Greedy Decoding", "Temperature-adjusted Sampling"], "training_required": true}, "tags": {"methods": ["bilevel_optimization", "marge_loss", "llm_as_optimizer", "llm_in_the_loop", "preference_learning", "supervised_fine_tuning", "direct_preference_optimization", "rlhf", "ppo", "reinforce", "iterative_refinement", "propen", "genetic_algorithm", "greedy_decoding", "temperature_sampling"], "problems": ["biophysical_sequence_optimization", "protein_design", "antibody_design", "ehrlich_functions", "black_box_optimization", "constrained_optimization", "discrete_optimization"], "contribution_type": ["new_method", "new_benchmark", "empirical_study", "framework"], "framework_lineage": "bilevel_optimization", "specific_domain": "biophysical_sequence_optimization", "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Biophysical Sequence Optimization Tasks (Ehrlich Functions)", "short": "Ehrlich Functions", "class_": "sequence_optimization", "properties": ["highly_constrained", "discrete", "black_box", "non_additive_mutational_effects", "procedurally_generated", "adjustable_difficulty"], "scale": "Sequence length 32-128, vocabulary size 8-32"}, "lineage": {"direct_ancestors": [{"paper": "Ma et al. (2024)", "relationship": "employs similar bilevel optimization with LLMs"}, {"paper": "Tagasovska et al. (2024)", "relationship": "adapts PropEn for dataset formatting"}], "closest_prior_work": "Ma et al. (2024)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Investigate the effect of the epistasis factor (a > 0) in Ehrlich functions", "Improve LLM calibration between likelihood and true objective", "Optimize multi-stage training strategies (e.g., SFT then MargE)", "Develop methods to reduce the computational cost of LLM-based optimizers"], "transferable_to": ["Drug discovery and materials design", "General highly-constrained discrete black-box optimization problems", "Controllable text generation with fine-grained, hard constraints"], "open_weaknesses": ["High computational cost of LLMs compared to specialized solvers", "LLMs' difficulty in satisfying precise biophysical constraints", "Miscalibration between LLM likelihoods and true rewards", "Limited inference-time extrapolation capabilities of LLMs", "DPO's tendency for mode collapse and unsuitability for constrained problems"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": true}, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_status": "stable", "bridge_score": 0.5714, "is_bridge": true, "priority_score": 7.53, "experiments": {"benchmarks": ["Ehr(8, 128)-8-8-8 (f1)", "Ehr(32, 32)-4-4-4 (f2)", "Ehr(32, 128)-4-4-4 (f3)", "Ehr(32, 128)-8-8-8 (f4)"], "baselines": ["LaMBO-2", "Genetic Algorithm (GA)", "LLOME-SFT", "LLOME-DPO", "LLOME-REINFORCE", "OpenAI's o1", "Google's Gemini 2 Flash Thinking"], "hardware": "two A100 GPUs", "instance_sizes": [32, 128]}, "results": {"vs_baselines": {"LLOME-MARGE": "outperforms LaMBO-2 on f2, comparable on f1, f3, f4", "LaMBO-2": "outperforms LLOME-MARGE on f3, comparable on f1, f2, f4", "LLOME-SFT": "lags behind LLOME-MARGE and LaMBO-2 on f2, f3", "LLOME-DPO": "lags behind all methods on f2, f3, struggles with feasibility and diversity", "Genetic Algorithm (GA)": "lags behind LLOME-MARGE and LaMBO-2 on f2, f3", "OpenAI's o1": "0.9375 minimum regret on f2 (prompting alone)", "Google's Gemini 2 Flash Thinking": "0 minimum regret on f2 (prompting alone, but most outputs infeasible)"}, "scalability": "Performance differentiation between methods is highest on medium difficulty tasks; LLOME-MARGE with smaller LLM plateaus earlier due to limited capacity.", "statistical_rigor": "Median cumulative regret estimated from 8 trials; 95% confidence intervals shown in plots.", "limitations_acknowledged": ["LLMs incur heavy computational costs", "LLMs struggle to satisfy precise constraints", "LLMs exhibit likelihood-reward miscalibration", "LLMs struggle without explicit rewards", "LLMs' inference-time extrapolation is limited beyond a threshold", "DPO is ill-suited for constrained problems", "Specialized solvers require custom tuning for each test function", "Smaller LLMs show training instability and limited capacity"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2410.13213", "arxiv_url": "https://arxiv.org/abs/2410.13213", "title": "LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch", "authors": ["Caigao"], "abstract": "", "published_date": "2024-10-17", "affiliations": "Ant Group, East China Normal University, Nanjing University", "category": "Generative AI for OR", "relevance": {"methodological": 5, "problem": 7, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper establishes a new SOTA baseline for our 'OR-Bench' project, demonstrating that open-weight models (Qwen-14B) can outperform GPT-4o on NL-to-Solver tasks if aligned correctly. It validates KTO as a viable technique for reducing symbolic formulation errors."}, "brief": "The authors fine-tune Qwen1.5-14B to translate natural language optimization problems into Pyomo code via a structured 'five-element' intermediate representation (Sets, Parameters, Variables, Objective, Constraints) and KTO alignment. They achieve ~11% accuracy gains over GPT-4o and ORLM on benchmarks like NL4Opt and IndustryOR, primarily by reducing formulation hallucinations through the structured intermediate step and preference optimization. For our OR-Bench work, the key takeaway is the concrete recipe for using KTO to align symbolic modeling agents, which appears more effective than standard SFT for enforcing constraints in smaller models. While not an evolutionary search paper, it provides a strong, locally runnable baseline for our OR modeling evaluations.", "methodology": {"core_method": "Multi-instruction supervised fine-tuning and KTO model alignment with self-correction", "llm_role": "Generates problem formulations, writes solver code, and performs error analysis for self-correction", "llm_model_used": "Qwen1.5-14B", "search_type": "hybrid", "novelty_claim": "LLMOPT introduces a five-element formulation for universal problem definition, combined with multi-instruction tuning, KTO model alignment, and a self-correction mechanism to enhance optimization generalization from natural language descriptions.", "components": ["five-element formulation", "data augmentation", "multi-instruction supervised fine-tuning (SFT)", "Kahneman-Tversky Optimization (KTO) model alignment", "auto-testing with self-correction mechanism"], "training_required": true}, "tags": {"methods": ["llm_as_heuristic", "llm_as_evaluator", "llm_code_generation", "llm_fine_tuned", "llm_in_the_loop", "supervised_learning", "kahneman_tversky_optimization", "data_augmentation", "self_correction", "program_synthesis"], "problems": ["linear_programming", "integer_programming", "mixed_integer_programming", "nonlinear_programming", "combinatorial_optimization", "multi_objective_programming", "natural_language_to_optimization_problem_formulation", "solver_code_generation", "general_optimization_problems"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": null, "specific_domain": "general_optimization_problems", "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "General Optimization Problems", "short": "OP", "class_": "llm_optimization_modeling", "properties": ["linear_programming", "nonlinear_programming", "mixed_integer_programming", "integer_programming", "combinatorial_optimization", "multi_objective_optimization"], "scale": "diverse problem types, typically small to medium scale"}, "lineage": {"direct_ancestors": [{"paper": "Tang et al., 2024", "relationship": "improves upon learning-based optimization modeling approach of ORLM"}, {"paper": "Ma et al., 2024", "relationship": "improves upon learning-based optimization modeling approach of LLaMoCo"}, {"paper": "Ethayarajh et al., 2024", "relationship": "adopts Kahneman-Tversky Optimization (KTO) for model alignment"}, {"paper": "Chen et al., 2024", "relationship": "inspired the self-correction mechanism"}], "closest_prior_work": "ORLM (Tang et al., 2024)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Efficiently gather, synthesize, and generate more diverse and well-labeled high-quality training data for optimization problems", "Explore understanding and utilizing data stored in specific databases or files for large-scale problems", "Address varying accuracy across different problem formulating difficulties"], "transferable_to": ["More complex real-world optimization scenarios not yet covered", "Optimization problems requiring integration with external structured data sources", "Automated algorithm design for specific problem classes beyond general OR"], "open_weaknesses": ["Scarcity of high-quality, diverse, and well-labeled training data for optimization problems", "Limited ability to handle large-scale problems where data is stored in structured databases/files", "Performance variability across different problem formulating difficulties"]}, "artifacts": {"code_url": "https://github.com/caigaojiang/LLMOPT", "models_released": false, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_1", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.74, "experiments": {"benchmarks": ["NL4Opt", "Mamo (EasyLP)", "Mamo (ComplexLP)", "IndustryOR", "NLP4LP", "ComplexOR"], "baselines": ["Reflexion", "Chain-of-Experts", "OptiMUS", "ORLM (Mistral-7B)", "Deepseek-Math-7B-Base", "LLaMa3-8B", "GPT-4", "GPT-4o", "GPT-4-Turbo"], "hardware": "NVIDIA 8*A100 Tensor Core GPUs with 80 GB each for training, 1*A100 GPU for inference", "instance_sizes": []}, "results": {"vs_baselines": {"ORLM (Mistral-7B)": "LLMOPT achieves an average SA improvement of 19.8% over 4 datasets (NL4Opt, Mamo Easy, Mamo Complex, IndustryOR).", "ORLM (Deepseek-Math-7B-Base)": "+14.8% SA on Mamo Easy (97.0% vs 82.2%)", "ORLM (LLaMa3-8B)": "+8.0% SA on IndustryOR (46.0% vs 38.0%)", "Reflexion": "+40.0% SA on NL4Opt (93.0% vs 53.0%)", "Chain-of-Experts": "+28.8% SA on NL4Opt (93.0% vs 64.2%)", "OptiMUS": "+14.2% SA on NL4Opt (93.0% vs 78.8%)", "GPT-4 Directly": "+45.7% SA on NL4Opt (93.0% vs 47.3%)", "GPT-4o": "Outperforms in SA and AST across all 6 datasets (e.g., +12.0% SA on NL4Opt)", "GPT-4-Turbo": "Outperforms in SA and AST across all 6 datasets (e.g., +18.0% SA on NL4Opt)", "Overall": "11.08% average SA improvement over state-of-the-art methods"}, "scalability": "Demonstrates strong generality across diverse optimization problem types; specific scalability to larger instance sizes is not explicitly detailed.", "statistical_rigor": "Reports Execution Rate (ER), Solving Accuracy (SA), and Average Solving Times (AST) on held-out test sets; no explicit mention of multiple runs or statistical significance tests for variance.", "limitations_acknowledged": ["Scarcity of high-quality, diverse, and well-labeled training data for optimization problems", "Difficulty in handling large-scale problems with data stored in structured databases/files", "Varying accuracy across different problem formulating difficulties"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2402.01145", "arxiv_url": "https://arxiv.org/abs/2402.01145", "title": "ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution", "authors": ["Haoran"], "abstract": "", "published_date": "2024-10-14", "affiliations": "Peking University, KAIST, Singapore Management University, Southeast University, PKU-Wuhan Institute for AI", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper directly implements 'verbal gradients' and 'memory' in LLM evolutionary search—two of our primary focus areas. The 'Reflective Evolution' mechanism (comparing parent pairs to guide mutation) is a concrete, transferable technique to improve sample efficiency in AlgoEvo."}, "brief": "ReEvo integrates a 'Reflector LLM' into genetic programming that analyzes pairs of heuristics (better vs. worse) to generate textual 'verbal gradients' for crossover and mutation, maintaining a long-term memory of these insights. The results are strong and relevant: they outperform EoH (Evolution of Heuristics) and NCO baselines on TSP, CVRP, and Bin Packing with significantly higher sample efficiency (only ~100 evaluations). The single most useful takeaway is the 'Short-term Reflection' prompting strategy—explicitly asking the LLM to derive a mutation direction by comparing the logic of high-fitness vs. low-fitness parents—which we should immediately test in our AlgoEvo framework to reduce sample costs. This is a direct methodological upgrade for our current evolutionary search pipelines.", "methodology": {"core_method": "Genetic Programming with LLM-based Reflective Evolutionary Search", "llm_role": "heuristic_generator, decomposition_guide", "llm_model_used": "gpt-3.5-turbo", "search_type": "hybrid", "novelty_claim": "ReEvo couples evolutionary computation with LLM-based humanoid reflections to create Language Hyper-Heuristics for automated heuristic design.", "components": ["Generator LLM", "Reflector LLM", "Individual encoding (code snippets)", "Selection", "Short-term reflection", "Crossover", "Long-term reflection", "Elitist mutation"], "training_required": false}, "tags": {"methods": ["hyper_heuristics", "language_hyper_heuristics", "reflective_evolution", "evolutionary_computation", "genetic_programming", "llm_code_generation", "llm_as_heuristic", "llm_in_the_loop", "llm_evolutionary_search", "self_reflection", "guided_local_search", "ant_colony_optimization", "genetic_algorithm", "neural_combinatorial_optimization", "black_box_optimization"], "problems": ["combinatorial_optimization", "heuristic_evolution", "traveling_salesman_problem", "capacitated_vehicle_routing_problem", "orienteering_problem", "multiple_knapsack_problem", "bin_packing", "decap_placement_problem", "operator_discovery"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "reevo", "specific_domain": "heuristic_evolution", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Heuristic Design for Combinatorial Optimization Problems", "short": "HD-COP", "class_": "algorithm_design", "properties": ["NP-hard", "heterogeneous", "black-box", "white-box", "constrained"], "scale": "heuristics designed for COPs with up to 1000 nodes/items"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2401.02051", "relationship": "builds on LHH concept from"}, {"paper": "Mathematical discoveries from program search with large language models", "relationship": "builds on LLM program search from"}], "closest_prior_work": "arXiv:2401.02051", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["explore broader applications of LHHs", "develop better dual-level optimization architectures for LHHs", "establish theoretical foundations for LHHs", "enrich evolutionary computation by interpreting genetic cues using LLMs", "experiment with more capable LLMs like OpenAI o1"], "transferable_to": ["other string_based_optimization_scenarios", "prompt_tuning"], "open_weaknesses": ["results may not scale beyond 100 heuristic evaluations", "effectiveness of reflection depends on capable LLMs", "open-source LLMs may not be sufficient for reflection"]}, "artifacts": {"code_url": "https://github.com/AI4CO/ReEvo", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.44, "experiments": {"benchmarks": ["TSP", "CVRP", "OP", "MKP", "BPP", "DPP", "TSPLIB"], "baselines": ["NeuOpt", "GNNGLS", "NeuralGLS", "EoH", "KGLS", "DeepACO", "GA (expert)", "DevFormer", "Nearest Neighbour", "GHPP", "POMO", "POMO + DAR", "LEHD", "LEHD + DAR"], "hardware": "AMD EPYC 7742 CPU, NVIDIA GeForce RTX 3090 GPU", "instance_sizes": [20, 51, 99, 100, 120, 127, 130, 150, 200, 225, 226, 264, 299, 300, 318, 417, 439, 493, 500, 657, 700, 724, 1000, 1577, 1655, 1817, 1889]}, "results": {"vs_baselines": {"NeuOpt": "KGLS-ReEvo outperforms on TSP200", "GNNGLS": "KGLS-ReEvo outperforms on all sizes", "NeuralGLS": "KGLS-ReEvo outperforms on all sizes", "EoH": "KGLS-ReEvo outperforms on all sizes", "KGLS": "KGLS-ReEvo improves on all sizes", "DeepACO": "ReEvo outperforms on 3/5 COPs", "GA (expert)": "GA-ReEvo outperforms GA-expert on DPP", "DevFormer": "GA-ReEvo outperforms DevFormer-CSE on DPP", "Nearest Neighbour": "ReEvo outperforms on TSPLIB", "GHPP": "ReEvo outperforms on TSPLIB", "POMO": "POMO + ReEvo improves POMO", "POMO + DAR": "POMO + ReEvo outperforms POMO + DAR", "LEHD": "LEHD + ReEvo improves LEHD", "LEHD + DAR": "LEHD + ReEvo outperforms LEHD + DAR"}, "scalability": "ReEvo-generated heuristics show consistent performance across problem sizes and distributions, improving generalization, especially with increased distributional shift.", "statistical_rigor": "Results are averaged over 3 runs; small variances are observed, and correlation length is averaged over 3 runs with 40 random walk steps.", "limitations_acknowledged": ["Results may not scale up beyond 100 heuristic evaluations", "Effectiveness of reflection depends on capable LLMs; open-source LLMs may not be sufficient"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2409.08692", "arxiv_url": "https://arxiv.org/abs/2409.08692", "title": "B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests", "authors": ["Mouxiang"], "abstract": "", "published_date": "2024-09-13", "affiliations": "Zhejiang University, Singapore Management University", "category": "OR for Generative AI", "relevance": {"methodological": 8, "problem": 8, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper provides a mathematically rigorous replacement for the heuristic evaluation metrics (like pass rate or majority voting) we currently use in evolutionary search when ground truth is unavailable. It directly addresses the 'noisy fitness' problem in AlgoEvo."}, "brief": "Chen et al. derive a Bayesian posterior estimator (B4) for selecting correct code solutions using unreliable (LLM-generated) tests, explicitly modeling the probability of incorrect code passing incorrect tests. They demonstrate statistically significant improvements (up to 50% relative gain on hard problems) over state-of-the-art heuristics like CodeT and MaxPass on HumanEval and APPS. The key takeaway is the B4 scoring formula: a product of four Beta functions that weighs consensus sets based on priors about test reliability (e.g., incorrect code rarely passes incorrect tests). This is immediately actionable for AlgoEvo: we can replace our naive fitness aggregation with B4 to improve selection accuracy when using generated unit tests, directly boosting sample efficiency.", "methodology": {"core_method": "Bayesian Maximum A Posteriori (MAP) estimation approximated by Beta functions (B4)", "llm_role": "none", "llm_model_used": null, "search_type": "heuristic_search", "novelty_claim": "Proposing a Bayesian framework to define an optimal code solution selection strategy and an efficient approximation (B4) with an error bound.", "components": ["Bayesian framework", "Maximum A Posteriori (MAP) estimation", "Beta functions", "integer programming formulation", "consensus set partitioning"], "training_required": false}, "tags": {"methods": ["bayesian_inference", "maximum_a_posteriori", "beta_distribution", "integer_programming", "consensus_set_partitioning", "heuristics_analysis"], "problems": ["code_generation_evaluation", "plausible_solution_selection", "plausible_test_assessment"], "contribution_type": ["new_method", "sota_result", "theoretical_result", "empirical_study", "framework"], "framework_lineage": "b4", "specific_domain": "code_generation_evaluation", "llm_coupling": null}, "problem": {"formal_name": "Optimal Assessment of Plausible Code Solutions with Plausible Tests", "short": "Code Solution Selection", "class_": "code_assessment", "properties": ["plausible_solutions", "plausible_tests", "Bayesian_framework", "integer_programming"], "scale": "up to 400 solutions, up to 400 test cases"}, "lineage": {"direct_ancestors": [{"paper": "CodeT", "relationship": "extends consensus set partitioning from"}, {"paper": "Bayesian statistics", "relationship": "applies framework of"}], "closest_prior_work": "CodeT", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["adapt_to_automatic_program_repair", "adapt_to_code_translation", "explore_alternative_priors", "optimize_hyperparameters_for_specific_applications"], "transferable_to": ["automatic_program_repair", "code_translation", "general_software_engineering_generation_tasks", "plausible_solution_selection_with_unreliable_validators"], "open_weaknesses": ["independence_assumptions_violation", "prior_robustness_to_llm_misinterpretation", "hyperparameter_tuning_across_contexts", "theoretical_simplifications_for_error_analysis"]}, "artifacts": {"code_url": "https://github.com/ZJU-CTAG/B4", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 7.19, "experiments": {"benchmarks": ["HumanEval", "MBPP (sanitized version)", "APPS introductory", "APPS interview", "APPS competition"], "baselines": ["Random", "MaxPass", "MBR-exec", "CodeT"], "hardware": "CPU (execution time less than 1 second for N=M=400)", "instance_sizes": [10, 30, 50, 100, 200, 300, 400]}, "results": {"vs_baselines": {"CodeT": "+6.1% to +12.0% relative improvement on average", "Random": "+246% relative improvement in most challenging scenarios"}, "scalability": "B4 is efficient and scales well, with computation time less than one second for 400 solutions/tests.", "statistical_rigor": "Wilcoxon signed-rank significance test used (p-values < 0.001); simulated experiments repeated 20,000 times for stability.", "limitations_acknowledged": ["Assumptions of independence (code/test correctness, passing states) can be violated", "Prior for theta_0 assumes low probability, which LLMs might increase", "Priors for theta_x and theta_y efficacy may diminish when conditions (large N, high theta_x) are not met", "Hyperparameters tuning challenges across different usage scenarios", "Theoretical results rely on Law of Large Numbers and neglect complex interactions of multiple incorrect sets"]}, "analysis_date": "2026-02-19"}, {"arxiv_id": "2409.07045", "arxiv_url": "https://arxiv.org/abs/2409.07045", "title": "Beyond IID: Optimizing Instruction Learning from the Perspective of Instruction Interaction and Dependency", "authors": ["Hanyu"], "abstract": "", "published_date": "2024-09-11", "affiliations": "Beijing Academy of Artificial Intelligence", "category": "OR for Generative AI", "relevance": {"methodological": 6, "problem": 5, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "While SFT data selection isn't our primary focus, the formulation of data mixing as a Linear Programming problem based on an 'interaction matrix' is a perfect example of 'OR for AI'. We could adapt this LP approach to optimize population diversity in AlgoEvo or prompt buffers in our multi-agent systems."}, "brief": "The authors propose optimizing SFT data mixtures using Linear Programming (EE-CPO) by modeling the 'interaction' (synergy/antagonism) between instruction categories, rather than treating them as IID. They empirically derive a dependency taxonomy showing Math and Code are fundamental 'root' capabilities required before learning complex tasks, validating this via curriculum learning experiments that beat DEITA. The results are solid (+1.73 AlpacaEval over DEITA), though the cost of deriving the interaction matrix (training N models) is high. **Takeaway:** The 'Effect Equivalence Coefficient' matrix combined with an LP solver is a rigorous OR formulation for resource/data allocation that we should steal to optimize heuristic populations in our evolutionary search frameworks.", "methodology": {"core_method": "Causal Intervention based Instruction Correlation Analysis and Ability Taxonomy Induction; Effect Equivalence-based Linear Programming for Category Proportion Optimization (EE-CPO); Dependency Taxonomy Guided Curriculum Supervised Fine-Tuning (DT-CSFT)", "llm_role": "tagger, base_model_for_analysis_and_finetuning", "llm_model_used": "Qwen-1.5-72B-Instruction, Qwen-1.5-7B, Llama3-8B", "search_type": "hybrid", "novelty_claim": "Systematically investigates instruction interaction and dependency patterns and optimizes instruction sets via linear programming and curriculum learning.", "components": ["Automatic Ability Tagging System", "Causal Intervention based Instruction Correlation Analysis", "Causal Intervention based Ability Taxonomy Induction", "Effect Equivalence-based Category Proportion Optimization (EE-CPO)", "Dependency Taxonomy Guided Curriculum Supervised Fine-Tuning (DT-CSFT)"], "training_required": true}, "tags": {"methods": ["causal_inference", "causal_intervention", "linear_programming", "curriculum_learning", "supervised_fine_tuning", "llm_as_tagger", "text_embedding", "hierarchical_clustering", "statistical_testing", "instruction_tagging_system"], "problems": ["instruction_set_optimization", "llm_data_optimization", "llm_alignment", "causal_discovery", "ability_taxonomy_induction"], "contribution_type": ["new_method", "sota_result", "empirical_study", "framework"], "framework_lineage": null, "specific_domain": "instruction_set_optimization", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Instruction Set Optimization for Supervised Fine-Tuning of Large Language Models", "short": "SFT Instruction Set Optimization", "class_": "llm_data_optimization", "properties": ["instruction_interaction", "instruction_dependency", "non_iid_data_distribution", "curriculum_learning_guided"], "scale": "Instruction sets of 10k-50k instances, 29 instruction categories"}, "lineage": {"direct_ancestors": [{"paper": "Wang, Chen, and Zhu, 2021", "relationship": "builds on curriculum learning principles from"}], "closest_prior_work": "DEITA (Liu et al., 2023)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["More comprehensive exploration of complex instruction interaction patterns", "Dynamic adaptation of curriculum learning based on real-time model performance", "Extending the methodology to other LLM tasks or modalities"], "transferable_to": ["Data selection and optimization for other machine learning models", "Curriculum design for multi-modal LLMs", "Optimizing training sequences in other sequential learning tasks"], "open_weaknesses": ["Scalability of correlation and dependency analysis to a much larger number of instruction categories", "Sensitivity of effect equivalence coefficients to base model and existing dataset choices", "Potential bias from manual selection of instruction categories for analysis"]}, "artifacts": {"code_url": "https://github.com/BAAI-DIPL/sft-set-optimization-via-instruction-interaction-and-dependency", "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.68, "experiments": {"benchmarks": ["MT-Bench", "AlpacaEval 2.0"], "baselines": ["Random Selection", "Instag", "IFD", "DEITA"], "hardware": "null", "instance_sizes": [10000, 20000, 50000]}, "results": {"vs_baselines": {"DEITA": "EE-CPO consistently outperforms DEITA, with EE-CPO (50k) achieving up to +0.38 on MT-Bench and +1.73 on AlpacaEval2.0 over DEITA (50k). DT-CSFT also generally improves performance over DEITA.", "Random Selection": "EE-CPO (50k) outperforms Random Selection by up to +0.68 on MT-Bench and +5.58 on AlpacaEval2.0.", "Instag": "EE-CPO (50k) outperforms Instag by up to +0.82 on MT-Bench and +4.8 on AlpacaEval2.0.", "IFD": "EE-CPO (50k) outperforms IFD by up to +0.98 on MT-Bench and +6.04 on AlpacaEval2.0."}, "scalability": "The method shows consistent performance advantages across instruction set sizes (10k-50k), indicating its benefits are not offset by larger data scales.", "statistical_rigor": "Statistical significance is assessed using Wilcoxon signed-rank test with Benjamini-Hochberg adjusted P-values (<0.05).", "limitations_acknowledged": ["Complexity of interaction patterns for numerous instruction categories", "Need for more comprehensive exploration of interaction patterns"]}, "analysis_date": "2026-02-19"}, {"arxiv_id": "2508.10047", "arxiv_url": "https://arxiv.org/abs/2508.10047", "title": "A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions", "authors": ["Ziyang"], "abstract": "", "published_date": "2024-08-01", "affiliations": "Zhejiang University, Huawei Noah’s Ark Lab, Singapore University of Social Sciences, Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security", "category": "Generative AI for OR", "relevance": {"methodological": 5, "problem": 9, "inspirational": 6}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper invalidates the baselines of several major datasets (IndustryOR, NL4Opt) by identifying massive error rates (up to 54%). As we are developing 'OR-Bench', ignoring this cleaning effort would render our own benchmarking comparisons scientifically flawed."}, "brief": "This survey and empirical audit reveals that standard optimization modeling benchmarks (NL4Opt, IndustryOR) suffer from critical error rates ranging from 16% to 54%, rendering prior leaderboards unreliable. The authors manually cleaned these datasets and re-evaluated methods, finding that Chain-of-Thought (CoT) often degrades performance compared to standard prompting, while fine-tuned models (ORLM) and multi-agent systems (Chain-of-Experts) perform best. The immediate takeaway is that we must adopt their cleaned datasets for our OR-Bench project; using the original open-source versions is no longer defensible. Additionally, the failure of CoT on these tasks suggests we should prioritize multi-agent or fine-tuned approaches for symbolic formulation tasks.", "methodology": {"core_method": "Systematic Literature Review and Empirical Re-evaluation", "llm_role": "evaluator", "llm_model_used": "gpt-4o-2024-08-06", "search_type": "analysis", "novelty_claim": "First systematic review of LLM-based optimization modeling, including data cleaning of existing benchmarks, construction of a new leaderboard, and an online resource portal.", "components": ["systematic literature review", "benchmark data cleaning", "unified performance re-evaluation", "online portal development"], "training_required": false}, "tags": {"methods": ["systematic_literature_review", "empirical_study", "data_cleaning", "benchmark_creation", "llm_as_evaluator"], "problems": ["benchmark_quality_assessment", "evaluation_standardization", "natural_language_for_optimization", "optimization_modeling"], "contribution_type": ["survey", "new_benchmark", "empirical_study", "framework"], "framework_lineage": null, "specific_domain": "optimization_modeling", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Natural Language for Optimization (NL4Opt)", "short": "NL4Opt", "class_": "program_synthesis", "properties": ["translating natural language to mathematical models", "domain-specific terminology", "implicit constraints", "abstract models", "concrete models", "mixed integer programming", "nonlinear integer programming"], "scale": "Instances ranging from 37 to 652, with complexity measured by number of variables and constraints"}, "lineage": {"direct_ancestors": [], "closest_prior_work": "NL4Opt", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["develop reasoning models for optimization modeling using RL", "enhance explainability of LLM-based modeling processes", "improve domain knowledge injection into LLMs via knowledge graphs", "explore human-in_the_loop modeling paradigms"], "transferable_to": ["natural_language_to_code_generation_for_other_domains", "scientific_discovery_automation", "automated_algorithm_design_for_other_problem_classes", "formal_language_synthesis_from_natural_language"], "open_weaknesses": ["high_error_rates_in_existing_benchmarks", "lack_of_truly_complex_cases_in_benchmarks", "inconsistent_evaluation_standards_across_works", "black_box_nature_of_llms_in_modeling"]}, "artifacts": {"code_url": "https://llm4or.github.io/LLM4OR", "models_released": false, "new_benchmark": true}, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_status": "stable", "bridge_score": 0.6697, "is_bridge": true, "priority_score": 7.11, "experiments": {"benchmarks": ["NL4Opt", "IndustryOR", "MAMO EasyLP", "MAMO ComplexLP", "ReSocratic", "NLP4LP", "ComplexOR"], "baselines": ["Standard prompting", "CoT", "Chain-of-Experts", "CAFA", "ORLM-LLaMA-3 8B"], "hardware": "OpenAI API (gpt-4o-2024-08-06)", "instance_sizes": [37, 100, 211, 269, 289, 605, 652]}, "results": {"vs_baselines": {"Standard prompting": "Baseline performance, sometimes outperforms CoT.", "CoT": "Inconsistent, sometimes performs worse than standard prompting.", "Chain-of-Experts": "Competitive, works well for simpler tasks (e.g., 94.4% on EasyLP), but surpassed by ORLM on complex datasets.", "CAFA": "Performance comparable to CoT.", "ORLM-LLaMA-3 8B": "Competitive, surpasses Chain-of-Experts on complex datasets (e.g., IndustryOR, ComplexLP)."}, "scalability": "The paper notes that existing benchmarks lack truly complex cases, making it hard to fully assess scalability of methods, though some methods perform better on 'complex' datasets.", "statistical_rigor": "11 human experts manually identified errors in benchmarks, with each error case cross-validated by at least three different experts. All evaluation results were reproduced using a standardized method, reporting accuracy.", "limitations_acknowledged": ["High error rates in current benchmarks", "Existing benchmarks are dominated by simple and moderate problems, with very few challenging cases", "Evaluation results reported in existing works lack a unified standard", "Open-source landscape in optimization modeling remains limited", "Difficulty in annotating long chain-of-thought data for training reasoning models", "Black-box nature of LLMs, lacking explainability", "Difficulty in injecting domain knowledge into LLMs", "Lack of human intervention exploration during inference (human-in-the-loop modeling)"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2407.19633", "arxiv_url": "https://arxiv.org/abs/2407.19633", "title": "OptiMUS-0.3: Using Large Language Models to Model and Solve Optimization Problems at Scale", "authors": ["Ali"], "abstract": "", "published_date": "2024-07-29", "affiliations": "", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 9, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper sets the current SOTA for LLM-to-MILP formulation, directly impacting our 'OR-Bench' and 'Generative AI for OR' tracks. The modular architecture (specifically the connection graph and structure detection) offers a concrete blueprint for handling the large-scale instances and complex descriptions that currently break our baselines."}, "brief": "OptiMUS-0.3 is a modular multi-agent system that translates natural language into Gurobi code, utilizing a 'connection graph' to manage variable-constraint relationships in long contexts and specialized agents to detect solver-specific structures (SOS, indicators) or implement sifting. The results are rigorous, introducing a new hard benchmark (NLP4LP) where they outperform GPT-4o by ~40% and beat Chain-of-Experts. The most stealable insight is the 'Structure Detection Agent': instead of relying on the LLM to write generic constraints, we should explicitly prompt for and map high-level structures to efficient solver APIs (like SOS constraints) to improve performance in our EvoCut and AlgoEvo pipelines. This is a necessary read for the OR-Bench team.", "methodology": {"core_method": "Modular LLM-based agent (OptiMUS-0.3) employing a connection graph and self-reflective error correction for sequential optimization model formulation and code generation using Gurobi API", "llm_role": "optimization_model_synthesis", "llm_model_used": "GPT-4o", "search_type": "hybrid", "novelty_claim": "OptiMUS-0.3 employs a modular structure with a connection graph to process constraints and objectives independently, enabling it to handle long descriptions and large data without excessively long prompts, and incorporates self-reflective error correction and advanced optimization modeling techniques.", "components": ["Modular agent architecture", "Connection graph", "State management (JSON)", "Self-reflective error correction (reflective prompts, confidence-based feedback)", "Debugging module", "Structure Detection Agent (for SOS, indicator variables, piecewise-linear constraints)", "Advanced Optimization Coding Agent (for callbacks, sifting)", "Gurobi Python API integration"], "training_required": false}, "tags": {"methods": ["llm_code_generation", "llm_as_evaluator", "llm_prompt_optimization", "llm_in_the_loop", "ai_agents", "multi_agent_coordination", "mixed_integer_linear_programming", "linear_programming", "gurobi", "self_reflective_error_correction", "debugging", "structure_detection", "special_ordered_sets", "indicator_variables", "piecewise_linear_constraints", "sifting", "callbacks", "in_context_learning"], "problems": ["optimization_modeling", "milp_general", "facility_location", "flight_assignment", "security_constrained_unit_commitment", "scheduling", "cutting", "routing", "blending", "packing"], "contribution_type": ["new_method", "sota_result", "new_benchmark", "framework", "empirical_study"], "framework_lineage": "optimus", "specific_domain": "milp_general", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Mixed-Integer Linear Programming (MILP) problem formulation and solving from natural language descriptions", "short": "MILP", "class_": "optimization_modeling", "properties": ["mixed_integer", "linear"], "scale": "Up to 2500 units/variables, 500 locations, 100 flights, with long descriptions and complex data"}, "lineage": {"direct_ancestors": [{"paper": "AhmadiTeshnizi et al. 2024", "relationship": "extends"}], "closest_prior_work": "Chain-of-Experts (CoE)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["develop_reliability_guarantees_for_llm_systems", "improve_human_llm_feedback_loops", "handle_ambiguous_problem_descriptions", "automated_solver_selection_and_customization"], "transferable_to": ["constraint_programming", "convex_optimization", "traveling_salesman_problem_solvers", "non_expert_optimization_users"], "open_weaknesses": ["improve_constraint_and_objective_extraction", "reduce_mathematical_formulation_errors", "enhance_code_generation_robustness", "mitigate_llm_hallucinations_and_instability"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": true}, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_status": "stable", "bridge_score": 0.7387, "is_bridge": true, "priority_score": 7.21, "experiments": {"benchmarks": ["NLP4LP", "NL4OPT", "IndustryOR"], "baselines": ["Standard prompting (GPT-4o)", "Standard prompting (o1)", "Reflexion (GPT-4o)", "LLMOPT (Qwen1.5-14B)", "ORLM (Deepseek-Math)", "Chain-of-Experts (CoE) (GPT-4o)", "OptiMUS-0.2 (GPT-4o)"], "hardware": "Not specified, experiments run using GPT-4o API calls.", "instance_sizes": [1101, 355, 100]}, "results": {"vs_baselines": {"Standard prompting (GPT-4o)": "NL4OPT: +39.3% accuracy (vs 47.3%), NLP4LP: +40.5% accuracy (vs 33.2%), IndustryOR: +9.0% accuracy (vs 28.0%)", "Reflexion (GPT-4o)": "NL4OPT: +33.6% accuracy (vs 53.0%), NLP4LP: +31.1% accuracy (vs 42.6%)", "LLMOPT (Qwen1.5-14B)": "NL4OPT: -6.4% accuracy (vs 93.0%), NLP4LP: -10.1% accuracy (vs 83.8%), IndustryOR: -9.0% accuracy (vs 46.0%)", "ORLM (Deepseek-Math)": "NL4OPT: +0.1% accuracy (vs 86.5%), NLP4LP: +0.8% accuracy (vs 72.9%), IndustryOR: -1.0% accuracy (vs 38.0%)", "Chain-of-Experts (CoE) (GPT-4o)": "NL4OPT: +22.4% accuracy (vs 64.2%), NLP4LP: +24.5% accuracy (vs 49.2%)", "OptiMUS-0.2 (GPT-4o)": "NL4OPT: +7.8% accuracy (vs 78.8%), NLP4LP: +5.7% accuracy (vs 68.0%)", "OptiMUS-0.3 (o1) vs LLMOPT (Qwen1.5-14B)": "NLP4LP: -3.2% accuracy (vs 83.8%), IndustryOR: 0.0% accuracy (vs 46.0%)", "OptiMUS-0.3 (o1) vs ORLM (Deepseek-Math)": "NLP4LP: +7.7% accuracy (vs 72.9%), IndustryOR: +8.0% accuracy (vs 38.0%)"}, "scalability": "OptiMUS-0.3 scales to problems with large amounts of data and long descriptions, solving instances within a median of 108 seconds and a maximum of 350 seconds.", "statistical_rigor": "Performance consistency across 5 random seeds for 10 hard instances of NLP4LP; LLM confidence scores calibrated against manual annotations by a doctoral student (28/40 instances with 5/5 confidence were 100% correct, 12/40 instances with <5 confidence were 91.7% correct).", "limitations_acknowledged": ["Reliability of LLMs", "Trust and interpretability for human users", "Ambiguity in natural language problem descriptions", "Need for faster specialized solvers", "Lack of larger, realistic datasets", "Extension beyond MILPs to other modeling frameworks (e.g., CP, convex optimization)"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2407.13126", "arxiv_url": "https://arxiv.org/abs/2407.13126", "title": "Improving GPU Multi-Tenancy Through Dynamic Multi-Instance GPU Reconfiguration", "authors": ["Tianyu"], "abstract": "", "published_date": "2024-07-18", "affiliations": "UC San Diego, University of Pittsburgh, University of Arizona, University of Georgia", "category": "OR for Generative AI", "relevance": {"methodological": 6, "problem": 7, "inspirational": 6}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper is a direct reference for our 'GPUSched' project. It demonstrates that fine-grained MIG reconfiguration (usually considered too expensive due to ~6s latency) is viable if modeled correctly within an ILP that accounts for reconfiguration penalties."}, "brief": "MIGRator formulates dynamic NVIDIA MIG partitioning as an Integer Linear Program (ILP) to optimize a compound 'Goodput' metric (SLO + accuracy) for continuous learning workloads. The results on A100s show ~20% gains over baselines like Ekya and PARIS, largely by mitigating the massive ~6s MIG reconfiguration overhead via a 'pre-initialization' lookahead strategy. For our GPUSched project, the key takeaway is the explicit modeling of reconfiguration penalties in the ILP and the technique of pre-assembling instances during idle time to hide latency. While the reliance on 200-second traffic prediction is a potential fragility, the rigorous handling of hardware constraints makes this a strong reference for our OR-based resource allocation work.", "methodology": {"core_method": "Integer Linear Programming (ILP) for dynamic Multi-Instance GPU (MIG) reconfiguration with Goodput objective", "llm_role": "none", "llm_model_used": null, "search_type": "exact", "novelty_claim": "MIGRator is a novel GPU reconfiguration runtime that dynamically performs GPU reconfiguration for multi-tenancy CL workloads, leveraging the 'Goodput' metric in the ILP objective function to consider both inference SLO attainment and model accuracy.", "components": ["NVIDIA Multi-Instance GPU (MIG)", "Integer Linear Programming (ILP)", "Goodput metric", "Gurobi solver", "Informer (transformer-based prediction model)", "Pre-initialization optimization"], "training_required": true}, "tags": {"methods": ["integer_linear_programming", "gurobi_solver", "transformer", "supervised_learning", "gpu_resource_management", "dynamic_resource_allocation", "multi_instance_gpu", "pre_initialization"], "problems": ["gpu_scheduling", "cloud_scheduling", "multi_tenancy", "continuous_learning", "service_level_objective_optimization", "model_accuracy_optimization"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "nvidia_mig", "specific_domain": "gpu_multi_tenancy_scheduling", "llm_coupling": null}, "problem": {"formal_name": "Improving GPU Multi-Tenancy Through Dynamic Multi-Instance GPU Reconfiguration for Continuous Learning Workloads", "short": "Dynamic MIG Reconfiguration for CL", "class_": "resource_allocation", "properties": ["multi_tenancy", "continuous_learning", "dynamic", "SLO_aware", "accuracy_aware", "MIG_partitioning", "fine_grained_reconfiguration"], "scale": "2 co-running tenants, 7 GPCs"}, "lineage": {"direct_ancestors": [{"paper": "NVIDIA MIG", "relationship": "builds upon"}, {"paper": "Ekya [83]", "relationship": "improves upon resource allocation for CL workloads"}, {"paper": "PARIS [19]", "relationship": "improves upon static MIG partitioning"}], "closest_prior_work": "Ekya [83]", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Integrate with other continuous learning optimization techniques (e.g., data filtering, hyperparameter tuning).", "Scale the dynamic reconfiguration to support a larger number of co-running tenants.", "Explore adaptive reconfiguration granularity to dynamically balance performance and ILP overheads."], "transferable_to": ["Other multi-tenant GPU workloads beyond continuous learning.", "Resource allocation and scheduling in general cloud/edge computing environments.", "Optimization problems with multiple, conflicting objectives (e.g., latency vs. throughput)."], "open_weaknesses": ["ILP solver overhead can increase significantly at very fine reconfiguration granularities.", "Scalability of the ILP formulation to a much larger number of tenants is not fully explored.", "Performance is dependent on the accuracy of the inference request prediction model (Informer)."]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": null, "front_status": null, "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.03, "experiments": {"benchmarks": ["NC-CIFAR-10", "NC-CORe50", "NC-20-Newsgroups"], "baselines": ["Ekya [83]", "Astraea [17]", "PARIS [19]"], "hardware": "NVIDIA A100 GPU with 40 GB memory, CUDA 12.1, AlmaLinux-8.7 OS", "instance_sizes": [2, 7]}, "results": {"vs_baselines": {"Ekya": "+17% Goodput", "Astraea": "+21% Goodput", "PARIS": "+20% Goodput"}, "scalability": "MIGRator automatically determines beneficial GPC allocations for different batch sizes (1 and 4) and achieves higher Goodput with finer reconfiguration granularity (0.5s to 1s).", "statistical_rigor": "Results are reported as average percentages across 16 workloads and multiple retraining windows. No explicit mention of variance, confidence intervals, or significance tests.", "limitations_acknowledged": []}, "analysis_date": "2026-02-19"}, {"arxiv_id": "2407.10873", "arxiv_url": "https://arxiv.org/abs/2407.10873", "title": "Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models", "authors": ["Rui"], "abstract": "", "published_date": "2024-07-15", "affiliations": "City University of Hong Kong, Southern University of Science and Technology", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 4, "problem": 10, "inspirational": 6}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper exposes a critical weakness in the field: complex LLM-evolution methods (FunSearch, ReEvo) often fail to significantly outperform a trivial (1+1)-ES hill climber. We must validate our AlgoEvo/EvoCut architectures against this simple baseline immediately to justify our complexity."}, "brief": "Zhang et al. perform a rigorous benchmarking of major LLM-based evolutionary program search (EPS) methods (FunSearch, EoH, ReEvo) against a simple (1+1)-EPS baseline across four problems and nine LLMs. The results are empirically solid and sobering: the simple (1+1)-EPS baseline—iterative improvement via one-shot prompting—frequently matches or outperforms the complex population-based methods, particularly on bin packing, though EoH remains superior on TSP. **Crucial Takeaway:** We are likely over-engineering our search mechanisms; we must implement a (1+1)-EPS baseline in all future experiments (AlgoEvo, EvoCut) because if our multi-agent systems cannot beat this simple hill-climber, our papers will be rejected for unnecessary complexity. Additionally, they find that larger models (GPT-4) do not strictly guarantee better heuristic search performance compared to smaller, code-specialized models like CodeLlama-7B.", "methodology": {"core_method": "LLM-based Evolutionary Program Search (EPS)", "llm_role": "evolutionary_search", "llm_model_used": "UniXcoder, StarCoder, CodeLlama-7B, CodeLlama-34B, DeepSeek-Coder-6.7B, DeepSeek-Coder-33B, GPT-3.5, GPT-4, Claude 3 Opus", "search_type": "improvement", "novelty_claim": "This work provides empirical grounding for the importance of evolutionary search in LLM-based AHD approaches through a large-scale benchmark and detailed analysis.", "components": ["Executable program representation", "Evolutionary Computation paradigm", "Large Language Models", "Genetic Algorithm", "Island Model", "(1+1)-ES", "Few-shot prompting", "Chain of Thought prompting", "Reflection"], "training_required": false}, "tags": {"methods": ["llm_evolutionary_search", "program_synthesis", "evolutionary_computation", "genetic_algorithm", "island_model", "one_plus_one_es", "llm_as_heuristic", "llm_code_generation", "llm_prompt_optimization", "chain_of_thought", "reflection"], "problems": ["automated_heuristic_design", "admissible_set_problem", "online_bin_packing", "tsp"], "contribution_type": ["empirical_study", "new_benchmark"], "framework_lineage": "evolution_of_heuristics", "specific_domain": null, "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Automated Heuristic Design", "short": "AHD", "class_": "algorithm_design", "properties": ["automating heuristic development", "evolutionary program search"], "scale": "15 nodes (AS), 250-5000 items (OBP), 100 locations (TSP)"}, "lineage": {"direct_ancestors": [], "closest_prior_work": "FunSearch", "novelty_type": "incremental"}, "extensions": {"next_steps": ["Reduce computational and API usage costs of LLM-based EPS methods", "Develop more robust and universally effective LLM-based EPS algorithms", "Investigate the impact of different LLM architectures and fine-tuning strategies on heuristic design"], "transferable_to": ["Other combinatorial optimization problems (e.g., VRP, scheduling)", "Automated algorithm design for other domains (e.g., graph algorithms, scientific discovery)", "Automated program synthesis beyond heuristics"], "open_weaknesses": ["No universally effective LLM-based EPS method exists (No Free Lunch theorem applies)", "High computational and API usage costs for current LLM-based EPS methods", "Performance is highly dependent on the specific LLM choice", "LLM-based EPS algorithmic development is still in early stages"]}, "artifacts": {"code_url": "https://github.com/zhichao-lu/llm-eps", "models_released": false, "new_benchmark": true}, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.86, "experiments": {"benchmarks": ["Admissible Set A(15, 10)", "Online Bin Packing (OR dataset)", "Online Bin Packing (Weibull dataset)", "Traveling Salesman Problem (TSP100 instances)"], "baselines": ["FunSearch [12]", "Evolution of Heuristic (EoH) [14]", "Reflective Evolution (ReEvo) [15]", "(1+1)-EPS (proposed baseline)", "Standalone LLMs (Random Search)"], "hardware": "16 NVIDIA V100 GPU cards (for open-source LLMs), OpenAI and Anthropic APIs (for closed-source LLMs)", "instance_sizes": [15, 250, 5000, 100]}, "results": {"vs_baselines": {"FunSearch": "Performance varies, no consistent superiority.", "EoH": "Best on TSP; performance varies across problems and LLMs.", "ReEvo": "Performance varies, no consistent superiority.", "(1+1)-EPS": "Competitive overall, except OBP (Weibull); often outperforms standalone LLMs.", "Standalone LLMs": "Generally inferior to EPS methods on AS, OBP (Weibull), TSP; comparable on OBP (OR)."}, "scalability": "Existing LLM-based EPS methods incur significant computational and API usage costs, indicating a need for further optimization and query reduction.", "statistical_rigor": "Evaluated over five independent runs; mean relative distance (∆d) and standard deviations are reported.", "limitations_acknowledged": ["No universally effective LLM-based EPS method (No Free Lunch theorem applies)", "Performance depends on specific LLM choice", "Significant computational and API usage costs", "LLM-based EPS algorithmic development is still in early stages"]}, "analysis_date": "2026-02-13"}, {"arxiv_id": "2407.09887", "arxiv_url": "https://arxiv.org/abs/2407.09887", "title": "OptiBench Meets ReSocratic: Measure and Improve LLMs for Optimization Modeling", "authors": ["Zhicheng"], "abstract": "", "published_date": "2024-07-13", "affiliations": "The Hong Kong University of Science and Technology, ETH Zurich, Huawei Noah’s Ark Lab, City University of Hong Kong, Sun Yat-sen University, MBZUAI, University of California Merced, Chongqing University", "category": "Generative AI for OR", "relevance": {"methodological": 6, "problem": 9, "inspirational": 7}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper directly competes with and informs your 'OR-Bench' project. It establishes a new baseline (OptiBench) for LLM optimization modeling that includes nonlinear and tabular data, which previous benchmarks (NL4OPT) lacked. The data synthesis method is directly applicable to your work."}, "brief": "The authors propose OptiBench, a benchmark of 605 optimization problems (linear/nonlinear, tabular/text), and ReSocratic, a data synthesis method that generates formal models first and back-translates them into natural language questions. Results are strong: fine-tuning Llama-3-8B on their 29k synthetic samples improves accuracy from 13.6% to 51.1%, validating the data quality. **Key Takeaway:** The 'Reverse Socratic' synthesis pipeline (Formal Model → Code → NL Question) is the superior strategy for generating synthetic OR datasets because it guarantees solvability and ground truth by construction, unlike forward generation. We should steal this pipeline for generating robust test instances for OR-Bench and potentially for training our OR agents.", "methodology": {"core_method": "ReSocratic data synthesis for optimization problems, followed by Supervised Fine-Tuning of LLMs for Python code generation using PySCIPOpt solver", "llm_role": "evolutionary_search", "llm_model_used": "DeepSeek-V2", "search_type": "constructive", "novelty_claim": "We propose ReSocratic, a novel method for generating diverse and reliable data for optimization problems, which synthesizes complex reasoning data from scratch in a reverse manner.", "components": ["ReSocratic data synthesis", "Supervised Fine-Tuning", "Python code generation", "PySCIPOpt solver"], "training_required": true}, "tags": {"methods": ["llm_code_generation", "llm_as_solver", "llm_in_the_loop", "llm_fine_tuned", "data_synthesis", "reverse_socratic_method", "supervised_fine_tuning", "pyscipopt", "mathematical_modeling"], "problems": ["optimization_modeling", "linear_programming", "nonlinear_programming", "integer_programming", "mixed_integer_programming", "program_synthesis"], "contribution_type": ["new_method", "new_benchmark", "sota_result", "empirical_study", "framework"], "framework_lineage": "resocratic", "specific_domain": "mathematical_optimization", "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "End-to-End Optimization Problem Solving", "short": "OPT", "class_": "program_synthesis", "properties": ["linear", "nonlinear", "integer_variables", "mixed_integer_variables", "tabular_data", "end_to_end"], "scale": "605 instances (up to 7+ variables)"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2205.08232", "relationship": "builds on Socratic method for data synthesis from"}, {"paper": "NL4OPT (Ramamonjison et al., 2022b)", "relationship": "extends benchmark by adding end-to-end solving, nonlinear, and tabular data"}, {"paper": "ComplexOR (Xiao et al., 2023)", "relationship": "extends benchmark by adding larger scale, nonlinear, and tabular data"}, {"paper": "NLP4LP (AhmadiTeshnizi et al., 2024)", "relationship": "extends benchmark by adding larger scale, nonlinear, and tabular data"}, {"paper": "MAMO (Huang et al., 2024a)", "relationship": "extends benchmark by adding nonlinear and tabular data"}], "closest_prior_work": "GSM8K Socratic dataset (Cobbe et al., 2021)", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Extend ReSocratic to other complex reasoning tasks such as math word problem-solving", "Evaluate more large language models on OPTIBENCH", "Enable LLM self-improvement for complex problem-solving abilities"], "transferable_to": ["math_word_problem_solving", "resource_allocation", "supply_chain_management", "power_energy_scheduling"], "open_weaknesses": ["LLMs struggle with accurate mathematical formulation", "LLMs struggle with error-free code generation", "Performance degrades with more variables and longer text (especially for smaller LLMs)", "Lower code pass rate for nonlinear problems"]}, "artifacts": {"code_url": "https://github.com/yangzhch6/ReSocratic", "models_released": false, "new_benchmark": true}, "front_id": "generative_ai_for_or_2026-02-18_front_1", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.69, "experiments": {"benchmarks": ["OPTIBENCH", "NL4OPT-E"], "baselines": ["GPT-3.5-Turbo", "GPT-4", "GPT-4o", "GPT-4o-mini", "Llama-3-8B-Instruct", "Llama-3-70B-Instruct", "Mistral-7B-Instruct-v0.3", "Qwen2-7b-Instruct", "DeepSeek-V2", "DeepSeek-V2.5", "Llama-2-7B-Chat"], "hardware": "two A800 GPUs", "instance_sizes": [2, 3, 4, 5, 6, 7]}, "results": {"vs_baselines": {"Llama-2-7B-Chat": "improved from 0.0% to 30.6% on OPTIBENCH", "Llama-3-8B-Instruct": "improved from 13.6% to 51.1% on OPTIBENCH", "GPT-4o": "achieved 69.4% (few-shot SOTA on OPTIBENCH)"}, "scalability": "Model performance decreases as the number of variables increases, and GPT-3.5-Turbo shows significant degradation on long text questions, while GPT-4 is more balanced.", "statistical_rigor": "Pass@k results show performance improves with more generation attempts. Human-LLM consistency check on 20 samples showed 19/20 consistency for LLM-as-a-judge.", "limitations_acknowledged": ["LLMs face principal challenges in accurate mathematical formulation and error-free code generation", "Current work focuses on optimization problems, future work includes extending to other complex reasoning tasks like math word problem-solving"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2406.04824", "arxiv_url": "https://arxiv.org/abs/2406.04824", "title": "FunBO: Discovering Acquisition Functions for Bayesian Optimization with FunSearch", "authors": ["Virginia"], "abstract": "", "published_date": "2024-07-01", "affiliations": "Google DeepMind", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 6, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This is a direct, high-profile application of LLM evolutionary search (FunSearch) to discover optimization algorithms (Acquisition Functions). It validates our core thesis in 'AlgoEvo' but applies it to continuous optimization. We need to benchmark our methods against this approach."}, "brief": "FunBO applies FunSearch to evolve Python code for Bayesian Optimization acquisition functions, evaluating fitness by running full BO loops on synthetic functions. The results are empirically strong, showing that evolved AFs generalize well to out-of-distribution functions and outperform standard baselines like EI and UCB. The most stealable insight is their 'few-shot' adaptation strategy, where a general-purpose heuristic is rapidly fine-tuned on a small set of target instances—a technique we should immediately test for our VRP heuristics. While the method is computationally expensive (brute-forcing the inner loop), the interpretable code outputs provide concrete ideas for dynamic exploration-exploitation trade-offs.", "methodology": {"core_method": "FunSearch-based evolutionary algorithm for discovering acquisition functions in Python code", "llm_role": "evolutionary_search", "llm_model_used": "Codey (based on PaLM model family)", "search_type": "evolutionary_search", "novelty_claim": "This is the first work exploring AFs represented in computer code, thus demonstrating a novel approach to harness the power of LLMs for sampling policy design.", "components": ["LLM (Codey)", "Evaluator (Bayesian Optimization loop)", "Programs Database (island model)", "Prompt creation"], "training_required": true}, "tags": {"methods": ["bayesian_optimization", "acquisition_functions", "gaussian_processes", "funsearch", "llm_code_generation", "llm_evolutionary_search", "evolutionary_algorithm", "meta_learning", "reinforcement_learning", "proximal_policy_optimization", "few_shot_learning"], "problems": ["acquisition_function_discovery", "black_box_optimization", "hyperparameter_optimization", "global_optimization_benchmarks", "heuristic_evolution", "operator_discovery"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "funsearch", "specific_domain": "black_box_optimization", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Discovering Acquisition Functions for Bayesian Optimization", "short": "AF Discovery", "class_": "algorithm_design", "properties": ["program_search", "code_generation", "iterative_improvement", "fitness_evaluation_via_BO"], "scale": "1-6 dimensions, up to 10000 evaluation points, 20-30 BO trials per objective function"}, "lineage": {"direct_ancestors": [{"paper": "arxiv:2307.09967", "relationship": "extends FunSearch by adapting it for acquisition function discovery"}], "closest_prior_work": "arxiv:2307.09967", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Extend to constrained optimization problems", "Extend to noisy evaluations", "Extend to alternative surrogate models beyond GPs", "Explore encoding assumptions and prior knowledge into AFs", "Rigorously test properties of discovered AFs for inclusion in standard BO packages"], "transferable_to": ["Multi-objective Bayesian optimization", "Algorithm design for other optimization components (e.g., surrogate models)", "Automated design of heuristics for other OR problems", "Optimization problems with different types of surrogate models"], "open_weaknesses": ["High computational overhead due to full BO loop evaluation", "Limited scalability for large training sets (G) or complex optimization problems", "Simple score metric (Eq. 1) may not fully characterize convergence path", "High overall cost due to large number of LLM samples and compute resources", "High variance in the quality of discovered AFs"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.84, "experiments": {"benchmarks": ["Ackley", "Levy", "Schwefel", "Rosenbrock", "Sphere", "Styblinski-Tang", "Weierstrass", "Beale", "Branin", "Michalewicz", "Goldstein-Price", "Hartmann-3", "Hartmann-6", "RBF-based SVM HPO", "AdaBoost HPO", "GP samples"], "baselines": ["Expected Improvement (EI)", "Upper Confidence Bound (UCB)", "Probability of Improvement (PofI)", "Posterior Mean (MEAN)", "Random", "MetaBO [40]", "FSAF [14]"], "hardware": "5 Codey instances on TPUs, 100 CPUs evaluators per LLM instance", "instance_sizes": [1, 2, 3, 4, 6]}, "results": {"vs_baselines": {"EI": "Outperforms on OOD-Bench, ID-Bench, HPO-ID (AdaBoost), GPs-ID (d=4), FEW-SHOT; competitive elsewhere.", "UCB": "Outperforms on OOD-Bench, ID-Bench, HPO-ID (AdaBoost), GPs-ID (d=4), FEW-SHOT; competitive elsewhere.", "PofI": "Outperforms on ID-Bench, HPO-ID (AdaBoost), GPs-ID (d=4), FEW-SHOT; competitive elsewhere.", "MEAN": "Outperforms on ID-Bench, HPO-ID (AdaBoost), GPs-ID (d=4), FEW-SHOT; competitive elsewhere.", "Random": "Outperforms on ID-Bench, HPO-ID (AdaBoost), GPs-ID (d=4), FEW-SHOT; competitive elsewhere.", "MetaBO [40]": "Outperforms on AdaBoost HPO, GPs-ID (d=4), FEW-SHOT; competitive on ID-Bench, SVM HPO, GPs-ID (d=3).", "FSAF [14]": "Outperforms on FEW-SHOT."}, "scalability": "Computational overhead of running full BO loop for each function in G limits scalability for larger sets G and more complex optimization problems (e.g., multi-objective).", "statistical_rigor": "Results reported as normalized average simple regret (¯Rt) over 100 function instances, with shaded areas indicating ± standard deviations/2.", "limitations_acknowledged": ["Computational overhead of full BO loop evaluation", "Limited scalability for large G or complex problems (e.g., multi-objective)", "Simple score metric (Eq. 1) may not fully characterize convergence path", "High overall cost due to LLM samples", "High variance in quality of discovered AFs due to randomness"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2401.02051", "arxiv_url": "https://arxiv.org/abs/2401.02051", "title": "Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model", "authors": ["Fei"], "abstract": "", "published_date": "2024-06-01", "affiliations": "Huawei Noah’s Ark Lab, City University of Hong Kong, Southern University of Science and Technology", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 9, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper directly targets our core focus (LLM evolutionary search for OR) and claims to beat FunSearch with orders of magnitude better sample efficiency (2k queries vs millions). The 'Thought + Code' co-evolution mechanism is a concrete architectural change we can immediately test in AlgoEvo to reduce our inference costs."}, "brief": "EoH introduces a dual-track evolutionary framework that evolves both natural language 'thoughts' (heuristic logic) and their corresponding Python code, rather than code alone. On Online Bin Packing, it claims to outperform DeepMind's FunSearch while using only ~2,000 LLM queries (vs FunSearch's millions), and achieves SOTA gaps on TSP and FSSP via Guided Local Search. The critical takeaway is the 'E2' prompt strategy: explicitly asking the LLM to extract common ideas from parent heuristics into a natural language 'thought' before generating code, which acts as a genetic Chain-of-Thought to stabilize mutation. We should immediately implement this 'Thought-then-Code' mutation operator in our AlgoEvo pipeline to address our sample efficiency bottlenecks.", "methodology": {"core_method": "LLM-assisted evolutionary algorithm for co-evolving natural language heuristic descriptions ('thoughts') and executable code implementations", "llm_role": "heuristic_generator", "llm_model_used": "GPT-3.5-turbo", "search_type": "hybrid", "novelty_claim": "We propose EoH, a novel paradigm that uses LLMs to evolution both thoughts and codes for the automatic design of heuristics with minimum hand-craft design and no domain model training.", "components": ["Evolutionary Computation framework", "Large Language Models", "Natural language descriptions (thoughts)", "Executable code implementations", "Five prompt strategies (E1, E2, M1, M2, M3)", "Fitness evaluation on problem instances", "Population management (selection)"], "training_required": false}, "tags": {"methods": ["evolutionary_computation", "large_language_models", "llm_code_generation", "llm_in_the_loop", "llm_evolutionary_search", "evolution_of_heuristics", "program_synthesis", "guided_local_search", "local_search", "llm_prompt_optimization", "in_context_learning"], "problems": ["automatic_heuristic_design", "online_bin_packing", "traveling_salesman_problem", "flow_shop_scheduling_problem", "heuristic_evolution"], "contribution_type": ["new_method", "sota_result", "framework", "empirical_study"], "framework_lineage": "eoh", "specific_domain": null, "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Automatic Heuristic Design", "short": "AHD", "class_": "algorithm_design", "properties": ["heuristic_generation", "code_synthesis", "natural_language_reasoning", "evolutionary_search"], "scale": "Online bin packing (up to 10k items), TSP (up to 200 nodes), FSSP (up to 100 jobs, 20 machines)"}, "lineage": {"direct_ancestors": [{"paper": "Mathematical discoveries from program search with large language models", "relationship": "improves upon the LLM+EC paradigm for AHD"}, {"paper": "Algorithm evolution using large language model", "relationship": "preliminary version"}], "closest_prior_work": "Mathematical discoveries from program search with large language models", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["pre_trained_domain_llm", "understanding_of_heuristic_search_space", "efficient_human_llm_interaction"], "transferable_to": ["other_combinatorial_optimization_problems", "algorithm_design_for_other_domains", "hyperparameter_optimization"], "open_weaknesses": ["costly_heuristic_evaluation", "early_stage_of_llm_ec_integration", "computational_expense_for_llm_queries"]}, "artifacts": {"code_url": "https://github.com/FeiLiu36/EoH", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 8.11, "experiments": {"benchmarks": ["Weibull instances (online bin packing)", "TSP100 instances (randomly generated)", "TSPLib (TSP)", "Randomly generated instances (FSSP)", "Taillard instances (FSSP)"], "baselines": ["First Fit", "Best Fit", "FunSearch", "Nearest Insertion", "Farthest Insertion", "Google Or-Tools", "Attention Model (AM)", "POMO", "LEHD", "GUPTA", "CDS", "NEH", "NEHFF", "PFSPNet", "PFSPNet NEH", "GCN", "LKH3", "NN", "BQ", "LS", "GLS", "EBGLS", "KGLS", "GNNGLS", "NeuralGLS", "ILS1", "ILS2"], "hardware": "single CPU i7-9700", "instance_sizes": [20, 50, 51, 52, 70, 76, 99, 100, 101, 105, 107, 124, 127, 130, 136, 144, 150, 152, 159, 195, 198, 200, 1000, 5000, 10000]}, "results": {"vs_baselines": {"First Fit": "EoH achieves 1.18% average gap vs 4.51% for First Fit on online bin packing.", "Best Fit": "EoH achieves 1.18% average gap vs 4.13% for Best Fit on online bin packing.", "FunSearch": "EoH achieves 1.18% average gap vs 2.23% for FunSearch on online bin packing, using significantly fewer LLM queries.", "Nearest Insertion": "EoH achieves 0.28% average gap vs 16.77% for NI on TSPLib instances.", "Farthest Insertion": "EoH achieves 0.28% average gap vs 4.01% for FI on TSPLib instances.", "Google Or-Tools": "EoH achieves 0.28% average gap vs 0.78% for Or-Tools on TSPLib instances, and 0% on several instances.", "Attention Model (AM)": "EoH achieves 0.28% average gap vs 16.77% for AM on TSPLib instances.", "POMO": "EoH achieves 0.28% average gap vs 2.02% for POMO on TSPLib instances.", "LEHD": "EoH achieves 0.28% average gap vs 1.92% for LEHD on TSPLib instances.", "GUPTA": "EoH achieves 0.23% average gap vs 16.81% for GUPTA on Taillard FSSP instances.", "CDS": "EoH achieves 0.23% average gap vs 10.37% for CDS on Taillard FSSP instances.", "NEH": "EoH achieves 0.23% average gap vs 2.71% for NEH on Taillard FSSP instances.", "NEHFF": "EoH achieves 0.23% average gap vs 2.49% for NEHFF on Taillard FSSP instances.", "PFSPNet": "EoH achieves 0.23% average gap vs 2.48% for PFSPNet on Taillard FSSP instances.", "PFSPNet NEH": "EoH achieves 0.23% average gap vs 2.48% for PFSPNet NEH on Taillard FSSP instances.", "GCN": "EoH achieves 0.025% gap vs 1.880% for GCN on TSP100 random instances.", "LKH3": "EoH achieves 0.025% gap vs 0.011% for LKH3 on TSP100 random instances.", "NN": "EoH achieves 0.025% gap vs 25.104% for NN on TSP100 random instances.", "BQ": "EoH achieves 0.025% gap vs 0.579% for BQ on TSP100 random instances.", "LS": "EoH achieves 0.025% gap vs 4.004% for LS on TSP100 random instances.", "GLS": "EoH achieves 0.025% gap vs 0.659% for GLS on TSP100 random instances.", "EBGLS": "EoH achieves 0.025% gap vs 0.155% for EBGLS on TSP100 random instances.", "KGLS": "EoH achieves 0.025% gap vs 0.035% for KGLS on TSP100 random instances.", "GNNGLS": "EoH achieves 0.28% average gap vs 0.96% for GNNGLS on TSPLib instances.", "NeuralGLS": "EoH achieves 0.28% average gap vs 0.42% for NeuralGLS on TSPLib instances.", "ILS1": "EoH achieves 0.23% average gap vs 1.00% for ILS1 on Taillard FSSP instances.", "ILS2": "EoH achieves 0.23% average gap vs 0.36% for ILS2 on Taillard FSSP instances."}, "scalability": "EoH demonstrates strong generalization to out-of-distribution instances and maintains competitive performance across varying problem sizes and capacities, often outperforming baselines where they struggle with larger or different distributions.", "statistical_rigor": "Results are averaged over multiple runs (3 for ablation, 5 for bin packing, 10 for FSSP Taillard, 64 for TSP training, 1000 for TSP random) and reported as average gaps or makespans; no explicit variance or significance tests are detailed beyond averages.", "limitations_acknowledged": ["Evaluation of heuristics is costly due to running on instance sets", "Early infancy of LLM-based heuristic evolution", "Potential for further improvement with domain-specific LLMs or better understanding of heuristic search space"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2402.02456", "arxiv_url": "https://arxiv.org/abs/2402.02456", "title": "tnGPS: Discovering Unknown Tensor Network Structure Search Algorithms via Large Language Models (LLMs", "authors": ["Junhua"], "abstract": "", "published_date": "2024-06-01", "affiliations": "RIKEN Center for Advanced Intelligence Project, Tencent Inc., Guangdong University of Technology", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 3, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "This paper implements a structured LLM-evolution loop (similar to our AlgoEvo) but explicitly tackles population diversity via LLM-based semantic clustering ('Knowledge Categorization'). This is a concrete, transferable technique to improve the diversity and quality of our own evolutionary search methods."}, "brief": "The authors propose tnGPS, a FunSearch-style framework that evolves Python code for Tensor Network Structure Search by mimicking human innovation stages (categorization, recombination, diversity injection). While the application (Tensor Networks) is niche, the results outperform standard heuristics like TNGA and TNLS. The critical takeaway for us is the 'Knowledge Categorization' phase: they use the LLM to semantically cluster the population of generated algorithms to manage diversity and guide the 'Diversity Injection' step. We should immediately implement this LLM-based population clustering in AlgoEvo to prevent convergence on similar code patterns.", "methodology": {"core_method": "LLM-driven automation framework for algorithm discovery via iterative refinement and enhancement using a prompting pipeline", "llm_role": "evolutionary_search", "llm_model_used": "gpt-4-1106-preview", "search_type": "hybrid", "novelty_claim": "We propose tnGPS, a large language model (LLM)-driven automation framework designed to automatically generate novel and effective TN-SS algorithms tailored to specific downstream tasks.", "components": ["Knowledge Categorization (KC)", "Idea Dropout (ID)", "Knowledge Recombination (KR)", "Incremental Innovation (II)", "Diversity Injection (DI)", "Sandbox Environment", "Prompt Architecture"], "training_required": false}, "tags": {"methods": ["llm_code_generation", "llm_in_the_loop", "llm_evolutionary_search", "program_synthesis", "evolution_of_heuristics", "prompt_engineering"], "problems": ["tensor_network_structure_search", "automatic_algorithm_discovery", "hyperparameter_optimization", "model_selection"], "contribution_type": ["new_method", "framework", "sota_result"], "framework_lineage": "funsearch", "specific_domain": "tensor_network_structure_search", "llm_coupling": "off_the_shelf"}, "problem": {"formal_name": "Tensor Network Structure Search", "short": "TN-SS", "class_": "algorithm_discovery", "properties": ["high-dimensional", "discrete", "model_selection", "hyperparameter_optimization"], "scale": "256x256 pixel images (order-8 tensors), order-4 tensors (dim 12), order-6 tensors (dim 8), ranks up to 10"}, "lineage": {"direct_ancestors": [{"paper": "Liu et al., 2023", "relationship": "extends LLM-driven algorithm evolution to tensor network structure search"}, {"paper": "Romera-Paredes et al., 2024", "relationship": "builds on LLM-driven program synthesis for algorithm discovery"}], "closest_prior_work": "Liu et al., 2023", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["explore other LLM models and architectures for algorithm discovery", "automate prompt engineering and refinement within the framework", "improve robustness against LLM hallucinations and unreliable code generation", "apply the framework to discover algorithms for other high-dimensional discrete optimization problems"], "transferable_to": ["automated machine learning (AutoML) tasks beyond neural architecture search", "discovery of algorithms for other combinatorial optimization problems", "scientific discovery tasks requiring novel algorithmic approaches", "design of specialized heuristics for various domains"], "open_weaknesses": ["performance variation due to changes in LLM models", "susceptibility to LLM hallucination leading to unreliable results", "manual adjustments required for prompt architecture when using different LLMs", "computational cost of iterative evaluation and LLM interactions"]}, "artifacts": {"code_url": "https://github.com/ChaoLiAtRIKEN/tngps", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 5.94, "experiments": {"benchmarks": ["BSD500", "CCPP", "MG"], "baselines": ["TNGA (Li & Sun, 2020)", "TNLS (Li et al., 2022)", "GREEDY (Hashemizadeh et al., 2020)", "TnALE (Li et al., 2023)"], "hardware": "", "instance_sizes": [256, 8, 4, 12, 6, 10]}, "results": {"vs_baselines": {"TNGA": "Outperformed by tnGPS-discovered algorithms (e.g., 0.1102 vs 0.1558 objective, 2.24 vs 2.64 parameters on CCPP random, 3.01 vs 12.69 on MG random).", "TNLS": "Outperformed by tnGPS-discovered algorithms (e.g., 0.1102 vs 0.1558 objective, 2.24 vs 2.36 parameters on CCPP random, 3.01 vs 17.25 on MG random).", "GREEDY": "Outperformed by tnGPS-discovered algorithms (e.g., 0.1102 vs 0.1558 objective, 2.24 vs 2.50 parameters on CCPP random, 3.01 vs failed on MG random).", "TnALE": "Outperformed by tnGPS-discovered algorithms (e.g., 0.1102 vs 0.1558 objective, 2.24 vs 2.60 parameters on CCPP random, 3.01 vs failed on MG random)."}, "scalability": "Discovered algorithms consistently outperform baselines across various problem scales and initialization types, demonstrating effectiveness on both in-domain and out-of-domain tasks.", "statistical_rigor": "Results are averaged over training images; number of samples to find structure is reported. No explicit statistical significance tests mentioned for final comparisons.", "limitations_acknowledged": ["performance is subject to variation due to changes in LLMs", "LLMs generating unreliable results due to the hallucination issue", "machines may eventually replace human expertise entirely"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2403.11446", "arxiv_url": "https://arxiv.org/abs/2403.11446", "title": "LLM Guided Evolution -- The Automation of Models Advancing Models", "authors": ["Clint"], "abstract": "", "published_date": "2024-03-18", "affiliations": "Georgia Tech Research Institute", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 5, "problem": 6, "inspirational": 7}, "significance": {"must_read": false, "changes_thinking": false, "team_discussion": true, "reasoning": "The paper is empirically weak (single trials on CIFAR-10), but the 'Evolution of Thought' (EoT) prompting strategy—using the diff between an elite mutant and its seed to ground future mutations—is a concrete, transferable mechanism for implementing feedback loops in our AlgoEvo framework without training a value model."}, "brief": "Morris et al. propose 'Guided Evolution,' an LLM-based NAS framework that introduces 'Evolution of Thought' (EoT) and 'Character Role Play' to guide code mutations. While the results are statistically negligible (single trials, ~0.8% gain on CIFAR-10), the EoT mechanism offers a specific, actionable prompt engineering technique: explicitly prompting the LLM to compare a successful elite individual against its original seed to extract 'reasoning' before applying mutations to new individuals. This serves as a lightweight, prompt-based memory/feedback mechanism that could immediately improve sample efficiency in our evolutionary search agents. The 'Character Role Play' (e.g., asking the LLM to act as 'Dr. MaGoo' for unorthodox ideas) is a gimmicky but potentially useful heuristic for maintaining population diversity.", "methodology": {"core_method": "LLM-guided Genetic Algorithm for Neural Architecture Search (NAS) with multi-objective optimization (accuracy and parameter count), incorporating Evolution of Thought (EoT) and Character Role Play (CRP) for mutation and mating.", "llm_role": "evolutionary_search, code_writer, evaluator", "llm_model_used": "Mixtral 8x7B", "search_type": "improvement", "novelty_claim": "Our study introduces \"Guided Evolution\" (GE), a novel framework that utilizes Large Language Models (LLMs) to directly modify code, and \"Evolution of Thought\" (EoT) which enables LLMs to reflect on and learn from previous mutation outcomes.", "components": ["Guided Evolution (GE) framework", "Evolution of Thought (EoT)", "Character Role Play (CRP)", "Genetic Algorithms", "SPEA-2 elite selection", "NSGA-II selection", "Mixtral 8x7B LLM"], "training_required": false}, "tags": {"methods": ["genetic_algorithm", "llm_code_generation", "llm_as_heuristic", "llm_as_evaluator", "llm_in_the_loop", "llm_prompt_optimization", "evolution_of_heuristics", "program_synthesis", "self_improving_search", "neural_architecture_search", "spea2", "nsga2", "character_role_play", "evolution_of_thought"], "problems": ["neural_architecture_search", "image_classification"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "guided_evolution", "specific_domain": "image_classification", "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Neural Architecture Search", "short": "NAS", "class_": "neural_architecture_search", "properties": ["LLM-guided", "evolutionary", "multi-objective", "code-based", "feedback_loop"], "scale": "low-parameter neural networks for 32x32 image classification"}, "lineage": {"direct_ancestors": [{"paper": "arxiv:2205.11916", "relationship": "extends Zero-Shot Chain-of-Thought"}, {"paper": "arxiv:2305.10601", "relationship": "extends Tree-of-Thought"}, {"paper": "Automatic Chain of Thought Prompting in Large Language Models", "relationship": "extends Auto-CoT"}, {"paper": "arxiv:2306.01779", "relationship": "builds on insights for Character Role Play"}, {"paper": "SPEA2: Improving the strength pareto evolutionary algorithm for multiobjective optimization", "relationship": "uses SPEA-2"}, {"paper": "A fast and elitist multiobjective genetic algorithm: NSGA-II", "relationship": "uses NSGA-II"}], "closest_prior_work": "funsearch", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Expand GE to broader datasets and domains", "Explore the impact of GE on NAS efficiency", "Investigate the role of instructive commenting in code gene sequences", "Study the effects of different LLM models on GE's parameter space exploration"], "transferable_to": ["Other machine learning model architectures beyond ExquisiteNetV2", "Different types of code generation and modification tasks", "Multi-objective optimization problems in other domains"], "open_weaknesses": ["Ablation studies were conducted with a single trial for each variant due to time constraints", "Evolutionary process was cut off due to time constraints, not convergence", "Further work is needed to fully assess the impact of EoT and CRP"]}, "artifacts": {"code_url": "https://github.com/clint-kristopher-morris/llm-guided-evolution", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 4.63, "experiments": {"benchmarks": ["CIFAR10"], "baselines": ["ExquisiteNetV2", "MobileNetV2", "EfficientNetB0", "ResNet18", "DenseNet121"], "hardware": "null", "instance_sizes": [518230]}, "results": {"vs_baselines": {"ExquisiteNetV2": "GE-Evolved-L achieved +0.82% accuracy (93.34% vs 92.52%) with 0% parameter change; GE-Evolved-M achieved +0.64% accuracy (93.16% vs 92.52%) with 43.1% parameter reduction."}, "scalability": "The GE framework demonstrated potential for further advancement over longer evolutionary periods, attributed to the vast array of design pathways explorable by the LLM-guided framework. The evolutionary process was cut short due to time constraints, not convergence.", "statistical_rigor": "Ablation studies were conducted with a single trial for each variant due to time constraints.", "limitations_acknowledged": ["Ablation studies conducted with a single trial for each variant due to time constraints", "Future work needed to fully assess the impact of EoT and CRP", "Evolutionary process cutoff due to time constraints, not convergence"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2403.02985", "arxiv_url": "https://arxiv.org/abs/2403.02985", "title": "Evolution Transformer: In-Context Evolutionary Optimization", "authors": ["Robert"], "abstract": "", "published_date": "2024-03-05", "affiliations": "Google DeepMind, TU Berlin", "category": "LLMs for Algorithm Design", "relevance": {"methodological": 8, "problem": 7, "inspirational": 9}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper provides a concrete neural architecture and training loop (SR-EAD) for 'learning to optimize' without a teacher. It directly challenges our reliance on static LLM prompts for search logic and offers a blueprint for 'Evolving the Evolver' via self-generated data."}, "brief": "Lange et al. introduce the Evolution Transformer, a causal architecture that learns to perform evolutionary strategy updates by attending to optimization history, effectively 'distilling' algorithms like CMA-ES into a neural network. Crucially, they propose 'Self-Referential Algorithm Distillation' (SR-EAD), where the model improves itself by perturbing its own weights, generating trajectories, and filtering for the best ones to retrain on—eliminating the need for a teacher. The results are strong, showing generalization to unseen Brax control tasks and successful (though sometimes unstable) self-bootstrapping. The key takeaway for us is the SR-EAD loop as a mechanism for open-ended optimizer improvement, and their use of Perceiver cross-attention to handle variable population sizes—a technique we should immediately steal for our multi-agent memory architectures.", "methodology": {"core_method": "Evolution Transformer, a causal Transformer architecture with self-attention and Perceiver cross-attention for search distribution updates", "llm_role": "evolutionary_search", "llm_model_used": null, "search_type": "improvement", "novelty_claim": "We introduce Evolution Transformer, an architecture inducing a population-order invariant and dimension-order equivariant search update.", "components": ["Evolution Transformer (causal Transformer, self-attention, Perceiver cross-attention, MLP)", "Evolutionary Algorithm Distillation (EAD)", "Self-Referential Evolutionary Algorithm Distillation (SR-EAD)", "JAX", "evosax"], "training_required": true}, "tags": {"methods": ["evolution_strategies", "transformer", "self_attention", "perceiver_attention", "algorithm_distillation", "supervised_learning", "meta_learning", "meta_optimization", "self_improving_search", "evolution_of_heuristics"], "problems": ["black_box_optimization", "neuroevolution", "continuous_control", "expensive_continuous_optimization", "algorithm_discovery"], "contribution_type": ["new_method", "framework", "empirical_study", "sota_result"], "framework_lineage": "evolution_of_heuristics", "specific_domain": "expensive_continuous_optimization", "llm_coupling": null}, "problem": {"formal_name": "In-Context Evolutionary Optimization", "short": "EO", "class_": "meta_optimization", "properties": ["black_box", "gradient_free", "in_context", "population_order_invariant", "dimension_order_equivariant"], "scale": "up to 1000 dimensions, up to 128 population members"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2303.09478", "relationship": "extends meta-learned attention-based EOs to a full Transformer-based ES"}, {"paper": "arXiv:2210.14215", "relationship": "extends Transformer-based algorithm distillation to zero-order optimization"}], "closest_prior_work": "arXiv:2303.09478", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["open_ended_algorithm_discovery_with_sread", "apply_to_other_sequential_algorithms", "integrate_state_space_models_for_long_context", "further_benchmarking_on_diverse_tasks"], "transferable_to": ["reinforcement_learning_policies", "neural_architecture_search", "other_zero_order_optimization_problems"], "open_weaknesses": ["high_memory_requirements_for_large_dimensions_or_populations", "limited_in_context_learning_due_to_sliding_context_window", "lack_of_mechanistic_understanding_of_black_box_bbo", "instability_of_self_referential_training"]}, "artifacts": {"code_url": "https://github.com/RobertTLange/evosax", "models_released": false, "new_benchmark": false}, "front_id": "llms_for_algorithm_d_2026-02-18_front_5", "front_status": "stable", "bridge_score": 0.564, "is_bridge": true, "priority_score": 7.93, "experiments": {"benchmarks": ["BBOB benchmark functions", "Brax tasks", "neuroevolution tasks (ant control, MNIST CNN Classification, Pendulum MLP Control, CartPole)"], "baselines": ["OpenAI-ES", "Sep-CMA-ES", "Gaussian Hill Climbing", "Natural Evolution Strategies (SNES)"], "hardware": "1-4 NVIDIA A100 GPUs for training, 1 NVIDIA V100S GPU for inference", "instance_sizes": [3, 5, 10, 128, 1000]}, "results": {"vs_baselines": {"Evolutionary Optimization baselines": "robustly outperforms on Brax and neuroevolution tasks", "OpenAI-ES": "outperforms on Brax tasks (negative optimality gap)", "SNES": "competitive performance on Brax tasks"}, "scalability": "The Evolution Transformer generalizes to unseen numbers of search dimensions and population members, but deployment has large memory requirements for high dimensions/population, necessitating a sliding context window.", "statistical_rigor": "Results are averaged across 3 or 5 independent runs, and interquartile mean is reported for some tasks.", "limitations_acknowledged": ["Large memory requirements for high search dimensions or population members", "Sliding context window limits in-context learning power", "Lack of full mechanistic understanding of the 'black-box BBO'", "Self-referential training can be unstable and jump between local optima"]}, "analysis_date": "2026-02-17"}, {"arxiv_id": "2403.01131", "arxiv_url": "https://arxiv.org/abs/2403.01131", "title": "LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation", "authors": ["Zeyuan"], "abstract": "", "published_date": "2024-03-02", "affiliations": "Singapore Management University, Nanyang Technological University, South China University of Technology", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 8, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": true, "team_discussion": true, "reasoning": "This paper demonstrates that a fine-tuned 350M parameter model can drastically outperform GPT-4 Turbo in generating optimization code by learning to select and configure the right solver (e.g., CMA-ES vs. SLSQP) based on problem structure. This directly addresses the scalability bottleneck in our AlgoEvo project, suggesting we should fine-tune local models on our search logs rather than relying on expensive, generic frontier models."}, "brief": "LLaMoCo fine-tunes small LLMs (down to 350M) to generate executable Python optimization code by training on a synthetic dataset where the 'ground truth' is the empirically best-performing solver identified via exhaustive benchmarking. The results are compelling: the fine-tuned 350M model achieves ~85% normalized performance on benchmarks where GPT-4 Turbo only reaches ~14-30%, largely because the small model learns to select specialized evolutionary strategies (like BIPOP-CMA-ES) while GPT-4 defaults to generic gradient-based solvers. **Key Takeaway:** We can replace the expensive GPT-4 calls in our evolutionary search loop with a specialized, fine-tuned local model (CodeLlama-7B) trained on our historical search successes, significantly improving both sample efficiency and scalability. The paper's 'contrastive warm-up' strategy for aligning diverse problem descriptions is also a transferable technique for our problem encoding work.", "methodology": {"core_method": "Instruction tuning of LLMs with a two-phase learning strategy incorporating contrastive learning-based warm-up and sequence-to-sequence loss", "llm_role": "code_writer", "llm_model_used": "CodeGen-Mono (350M), Phi-2 (2.7B), Code Llama (7B)", "search_type": "sampling", "novelty_claim": "LLaMoCo is the first instruction-tuning framework designed to adapt LLMs for solving optimization problems in a code-to-code manner.", "components": ["Instruction set construction", "Two-phase instruction tuning", "Contrastive warm-up", "Balanced data sampling", "Homogeneous batch sampling"], "training_required": true}, "tags": {"methods": ["llm_code_generation", "instruction_tuning", "contrastive_learning", "supervised_learning", "llm_fine_tuned", "program_synthesis"], "problems": ["optimization_code_generation", "numerical_optimization", "constrained_optimization", "unconstrained_optimization", "optimizer_discovery"], "contribution_type": ["new_method", "framework", "sota_result", "empirical_study"], "framework_lineage": "llamo_co", "specific_domain": null, "llm_coupling": "fine_tuned"}, "problem": {"formal_name": "Optimization Code Generation", "short": "OptCodeGen", "class_": "algorithm_design", "properties": ["constrained", "unconstrained", "multimodal", "nonseparable", "high-dimensional"], "scale": "2-50 dimensions (synthetic); high-dimensional (realistic)"}, "lineage": {"direct_ancestors": [{"paper": "arXiv:2309.03409", "relationship": "improves upon solution-to-solution LLM optimization (OPRO)"}, {"paper": "arXiv:2310.19046", "relationship": "improves upon solution-to-solution LLM optimization (LMEA)"}, {"paper": "arXiv:2310.06116", "relationship": "improves upon prompt-based LLM optimizer generation (OptiMus)"}], "closest_prior_work": "OptiMus (arXiv:2310.06116)", "novelty_type": "paradigm_shift"}, "extensions": {"next_steps": ["augment instruction set with more instances for enhanced generalization", "further alignment tuning of fine-tuned LLMs", "explore multi-objective optimization code generation"], "transferable_to": ["code generation for other scientific computing tasks", "automated algorithm design for other domains (e.g., combinatorial optimization, machine learning models)", "code generation for specific domain-specific languages (DSLs)"], "open_weaknesses": ["generalization to highly out-of-distribution problems", "potential for LLM underfitting on specific problem types without explicit hints", "efficiency for extremely large-scale problems (though improved over solution-to-solution)"]}, "artifacts": {"code_url": "https://anonymous.4open.science/r/LLaMoCo-722A", "models_released": true, "new_benchmark": false}, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_status": "stable", "bridge_score": 0.7541, "is_bridge": true, "priority_score": 7.79, "experiments": {"benchmarks": ["Synthetic problem instances", "Kumar et al. (2020) realistic problem set"], "baselines": ["OPRO", "LMEA", "GPT-4 Turbo", "Code Llama-7B", "Llama 2-70B"], "hardware": "Intel(R) Xeon(R) Gold 6348 CPU, 504GB RAM and a Nvidia A800 (80GB) GPU", "instance_sizes": [2, 50]}, "results": {"vs_baselines": {"OPRO": "LLaMoCo-L +368.3% on Ieval; LLaMoCo-S +17.9% on realistic problems", "LMEA": "LLaMoCo-L +465.9% on Ieval", "GPT-4 Turbo": "LLaMoCo-L +12.4% on Ieval; LLaMoCo-S +47.4% on realistic problems", "Code Llama-7B": "LLaMoCo-L +180.8% on Ieval", "Llama 2-70B": "LLaMoCo-L +305.5% on Ieval"}, "scalability": "The solution information-free setting allows handling large-scale problems with higher efficiency than solution-to-solution methods.", "statistical_rigor": "Experiments were conducted with 5 independent runs, reporting average error rate, recovery cost, optimization performance, and computational cost.", "limitations_acknowledged": ["Need to augment the instruction set with more instances to enhance generalization performance", "Further alignment tuning of fine-tuned LLMs is a promising future direction"]}, "analysis_date": "2026-02-15"}, {"arxiv_id": "2402.10172", "arxiv_url": "https://arxiv.org/abs/2402.10172", "title": "OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models", "authors": ["Ali"], "abstract": "", "published_date": "2024-02-15", "affiliations": "Stanford University", "category": "Generative AI for OR", "relevance": {"methodological": 7, "problem": 8, "inspirational": 8}, "significance": {"must_read": true, "changes_thinking": false, "team_discussion": true, "reasoning": "Directly competes with/complements our OR-Bench project by releasing the NLP4LP benchmark. The 'Connection Graph' architecture is a concrete, transferable solution to the context-window bottleneck in code generation agents, highly relevant for our AlgoEvo and MAS memory architectures."}, "brief": "OptiMUS is a multi-agent framework for translating natural language into Gurobi code, achieving SOTA performance by using a 'Connection Graph' to map variables and parameters to specific constraints. This graph allows the agents to dynamically filter context and construct minimal prompts, enabling success on problems with long descriptions where baselines like Chain-of-Experts fail. They release NLP4LP, a hard benchmark of 67 complex instances, which we must immediately compare against our OR-Bench efforts. The **Connection Graph** is the key stealable insight: a structured dependency tracking mechanism that solves context pollution in iterative code generation, directly applicable to our AlgoEvo and HERMES memory designs.", "methodology": {"core_method": "Modular LLM-based multi-agent system (OptiMUS) with connection graph", "llm_role": "multi-agent orchestration for model formulation, code generation, evaluation, and debugging", "llm_model_used": "GPT-4", "search_type": "hybrid", "novelty_claim": "OptiMUS introduces a novel connection graph and modular LLM agent structure to process problems with long descriptions and large data without long prompts, outperforming SOTA.", "components": ["Preprocessor", "Manager agent", "Formulator agent", "Programmer agent", "Evaluator agent", "Connection graph"], "training_required": false}, "tags": {"methods": ["llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "llm_as_evaluator", "llm_prompt_optimization", "multi_agent_system", "program_synthesis", "mixed_integer_linear_programming", "linear_programming", "gurobi"], "problems": ["optimization_modeling", "facility_location", "network_flow", "scheduling", "portfolio_management", "energy_optimization", "MILP_general", "LP_general"], "contribution_type": ["new_method", "new_benchmark", "sota_result", "framework"], "framework_lineage": "autogen", "specific_domain": null, "llm_coupling": "in_context_learning"}, "problem": {"formal_name": "Automated Optimization Modeling from Natural Language", "short": "NL-to-MILP", "class_": "program_synthesis", "properties": ["natural_language_input", "mixed_integer_linear_programming", "linear_programming", "long_descriptions", "large_data", "debugging"], "scale": "67-1101 instances, with long descriptions and large data"}, "lineage": {"direct_ancestors": [{"paper": "Anonymous, 2024", "relationship": "extends Chain-of-Experts (CoE) for NL-to-OR modeling"}, {"paper": "Ramamonjison et al, 2023", "relationship": "improves upon NL4Opt competition approach"}, {"paper": "Wu et al., 2023", "relationship": "inspired multi-agent manager design (Autogen)"}], "closest_prior_work": "Anonymous, 2024", "novelty_type": "combinatorial_novelty"}, "extensions": {"next_steps": ["Fine-tuning smaller LLMs for improved performance and cost-efficiency", "Integrating user feedback into the agent interaction process", "Automatically selecting the best optimization solver based on problem characteristics", "Enhancing the manager agent's decision-making with reinforcement learning"], "transferable_to": ["Other mathematical programming paradigms (e.g., non-linear programming, constraint programming)", "Automated modeling for other scientific or engineering domains", "Code generation for other domain-specific languages or APIs"], "open_weaknesses": ["Suboptimal performance when using smaller, less capable LLMs", "Errors in mathematical modeling, including incorrect models or missing constraints", "Persistent coding errors even after the debugging process", "Challenges in interpreting ambiguous natural language descriptions"]}, "artifacts": {"code_url": null, "models_released": false, "new_benchmark": true}, "front_id": "generative_ai_for_or_2026-02-18_front_1", "front_status": "stable", "bridge_score": 0.0, "is_bridge": false, "priority_score": 6.89, "experiments": {"benchmarks": ["NL4OPT", "ComplexOR", "NLP4LP"], "baselines": ["Standard prompting", "Reflexion", "Chain-of-Experts (CoE)"], "hardware": "null", "instance_sizes": [1101, 37, 67]}, "results": {"vs_baselines": {"Standard prompting": "+31.5% accuracy on NL4OPT, +57.2% on ComplexOR, +36.2% on NLP4LP", "Reflexion": "+25.8% accuracy on NL4OPT, +47.6% on ComplexOR, +25.7% on NLP4LP", "Chain-of-Experts (CoE)": "+14.6% accuracy on NL4OPT, +28.6% on ComplexOR, +18.9% on NLP4LP"}, "scalability": "Maintains stable prompt length and scales to problems with long descriptions and large data due to modularity and connection graph.", "statistical_rigor": "Reports average accuracy; no explicit statistical tests or variance analysis mentioned.", "limitations_acknowledged": ["Poor performance with smaller LLMs out-of-the-box", "Need for fine-tuning smaller models", "Potential for integrating user feedback", "Automatic solver selection", "Enhancing manager agent with RL", "Failure cases include incorrect modeling, missing constraints, and coding errors"]}, "analysis_date": "2026-02-15"}], "fronts": [{"front_id": "llms_for_algorithm_d_2026-02-18_front_6", "category": "LLMs for Algorithm Design", "snapshot_date": "2026-02-18", "core_papers": ["2510.14150", "2511.02864", "2601.21847", "2506.13131", "2507.17668", "2601.21096", "2510.06056", "2602.05688", "2505.22954", "2509.23331", "2511.17592", "2602.10233", "2601.05943", "2509.19349", "2602.03545", "2602.13218", "2509.18057", "2602.02919", "2601.16175", "2511.23473", "2509.07367", "2511.08522", "2512.24077"], "size": 23, "internal_density": 0.2925, "dominant_methods": ["llm_code_generation", "program_synthesis", "llm_evolutionary_search", "evolution_of_heuristics", "llm_as_heuristic"], "dominant_problems": ["algorithm_discovery", "heuristic_evolution", "circle_packing", "autocorrelation_inequalities", "automated_algorithm_design"], "growth_rate": 0.0, "stability": 1.0, "status": "stable", "name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "summary": "This research front explores advanced architectural innovations in LLM-guided algorithm design, moving beyond basic Evolution of Heuristics (EoH) and FunSearch paradigms. It focuses on sophisticated co-evolutionary strategies, multi-agent systems, and novel population representations to improve the discovery and performance of heuristics for complex combinatorial optimization problems. Key themes include evolving interdependent operators, dynamically adapting search strategies, and co-evolving problem instances or prompt templates alongside the algorithms themselves.\n\nKey contributions include E2OC, which uses MCTS to co-evolve interdependent operators, achieving up to +22% Hypervolume on FJSP/TSP. LLM4EO demonstrates online operator design for Flexible Job Shop Scheduling, yielding 3-4% RPD_BM improvement. ASRO introduces a game-theoretic framework for co-evolving solvers and adversarial instance generators, outperforming EoH by 0.5-30% on OBP, TSP, and CVRP. A-CEoH enhances prompts with algorithmic context, enabling smaller LLMs to generate superior A* heuristics for UPMP and SPP. EvoLattice proposes a DAG-based population representation with alternative-level statistics, boosting performance on NAS-Bench-Zero by over 150%. Other notable work includes LLM-driven test function generation (EoTF), co-evolution of prompts and Fireworks Algorithm operators (achieving 100% on Aircraft Landing vs. 56% for ReEvo), the dual-expert LLM4DRD for dynamic scheduling, TIDE's nested evolution for decoupling structure and parameter tuning (reducing TSP gap by 7.35%), RoCo's multi-agent system with long-term reflection, and EoH-S, which evolves complementary heuristic *sets* to reduce optimality gaps by 40-60% compared to single-heuristic approaches.\n\nThis front is rapidly emerging and maturing, characterized by a shift towards more complex, integrated LLM-driven systems. The trajectory indicates a strong focus on improving the efficiency, robustness, and generalization capabilities of generated algorithms. Future work will likely integrate these diverse architectural advancements, such as combining nested evolutionary loops with graph-based population representations, and expanding to more challenging multi-objective, constrained, and real-world dynamic optimization problems.", "future_directions": ["Integrate nested evolutionary frameworks (e.g., TIDE's structure/parameter decoupling) with advanced population representations (e.g., EvoLattice's DAG) to improve sample efficiency and convergence.", "Develop and evaluate multi-agent and game-theoretic co-evolutionary systems (e.g., ASRO, RoCo, LLM4DRD) for generating both algorithms and adversarial problem instances.", "Extend the co-evolution of prompts and operators (e.g., 2512.09209) to multi-objective and constrained real-world problems, focusing on prompt fitness functions.", "Investigate methods for dynamic, online LLM intervention and operator design (e.g., LLM4EO) within active search loops, leveraging algorithmic context (A-CEoH) and long-term reflection (RoCo).", "Explore the evolution of *sets* of complementary heuristics (EoH-S) and their application in adaptive large neighborhood search (ALNS) or other metaheuristics for diverse problem instances."], "top_affiliations": [["DeepMind", 6], ["Google", 2], ["Sakana AI", 2], ["University of California", 2], ["Microsoft", 2]], "method_overlap_with": {"llms_for_algorithm_d_2026-02-18_front_2": ["evolution_of_heuristics", "evolutionary_algorithm", "evolutionary_algorithms", "evolutionary_search", "genetic_algorithm", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_fine_tuned", "llm_in_the_loop", "local_search", "map_elites", "meta_learning", "program_synthesis", "reinforcement_learning", "self_improving_search", "simulated_annealing"], "llms_for_algorithm_d_2026-02-18_front_1": ["black_box_optimization", "code_embedding", "evolution_of_heuristics", "evolutionary_algorithm", "evolutionary_search", "genetic_algorithm", "genetic_programming", "grpo", "in_context_learning", "island_model", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_fine_tuned", "llm_in_the_loop", "local_search", "program_synthesis", "reinforcement_learning", "self_improving_search"], "llms_for_algorithm_d_2026-02-18_front_7": ["eoh", "evolution_of_heuristics", "evolutionary_algorithm", "evolutionary_algorithms", "genetic_algorithm", "island_model_ea", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_in_the_loop", "local_search", "multi_agent_system", "multi_armed_bandit", "program_synthesis", "quality_diversity", "reevo", "self_improving_search"], "llms_for_algorithm_d_2026-02-18_front_5": ["evolution_of_heuristics", "evolution_strategies", "evolutionary_algorithm", "genetic_programming", "in_context_learning", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "meta_learning", "multi_agent_system", "program_synthesis", "self_improving_search"]}}, {"front_id": "llms_for_algorithm_d_2026-02-18_front_2", "category": "LLMs for Algorithm Design", "snapshot_date": "2026-02-18", "core_papers": ["2510.14825", "2502.09544", "2504.00613", "2508.01558", "2602.03132", "2602.08253", "2510.06189", "2412.20694", "2402.02456", "2601.16849", "2411.19744", "2509.24509", "2510.08755", "2406.04824", "2510.11121"], "size": 15, "internal_density": 0.4952, "dominant_methods": ["llm_code_generation", "llm_as_heuristic", "program_synthesis", "funsearch", "evolution_of_heuristics"], "dominant_problems": ["heuristic_evolution", "operator_discovery", "combinatorial_optimization", "algorithm_discovery", "combinatorial_routing"], "growth_rate": 0.0, "stability": 1.0, "status": "stable", "name": "Enhanced LLM Evolutionary Search via Concept Learning and Co-Evolution", "summary": "This research front significantly advances LLM-guided evolutionary search for algorithm design, primarily building upon and enhancing the FunSearch framework. It explores various architectural and methodological improvements to boost the efficiency, robustness, and generalization of automatically discovered algorithms. Key themes include evolving specific components like acquisition functions (FunBO), Vision-Language Model adaptation strategies (EvoVLMA), and competitive programming scoring functions, as well as discovering tensor network structures (tnGPS) and deletion-correcting codes.\n\nSeveral papers introduce significant contributions. Contrastive Concept-Tree Search (CCTS) extracts hierarchical concepts from generated programs to guide search, showing consistent improvements over k-elite selection on combinatorial tasks. Robusta enhances FunSearch by using a Heuristic Analyzer and Suggester LLM to explain failures, achieving a 28x improvement in worst-case performance on traffic engineering. G-LNS co-evolves destroy and repair operators for Large Neighborhood Search, outperforming OR-Tools on large CVRP instances. EvoPH co-evolves prompts and heuristics using an island model and strategy sampling, dominating FunSearch on TSP and BPP. QUBE improves parent selection in FunSearch by using an uncertainty-inclusive quality metric based on offspring performance, leading to better results on Bin Packing and TSP. Zhu et al. demonstrate RL-finetuning of a Qwen-14B model to generate C++ crossover operators for HGS, outperforming GPT-4o and expert-designed components on CVRPLIB.\n\nThis front is rapidly maturing, moving beyond basic LLM-as-code-generator paradigms to sophisticated, self-improving search architectures. The trajectory indicates a shift towards more robust, interpretable, and efficient algorithm discovery. Future work will likely focus on integrating more advanced LLM reasoning capabilities, developing better feedback mechanisms (e.g., automated generalization of adversarial instances), and scaling these methods to even more complex, real-world problems with higher computational demands. The emphasis on co-evolution, concept learning, and failure analysis suggests a move towards more \"white-box\" and adaptive evolutionary systems.", "future_directions": ["Integrate Contrastive Concept-Tree Search (CCTS) with quality-diversity methods to improve guidance in the upper tail of algorithm performance.", "Implement the 'Analyzer -> Explainer -> Coder' loop (Robusta) for VRP, using small-scale solvers to generate counter-examples for evolved ALNS operators.", "Adopt the 'evolve-on-toy, deploy-on-giant' evaluation protocol for VRP and SAT solver evolutionary search pipelines to reduce compute costs.", "Apply AST-based anti-plagiarism rewards in RL-finetuning for code generation agents to enhance exploration and prevent mode collapse.", "Extend the G-LNS framework to multi-objective optimization problems beyond routing, focusing on co-evolving synergistic destroy and repair operators."], "top_affiliations": [["DeepMind", 3], ["Singapore", 2], ["Harvard University", 1], ["Stanford University", 1], ["Peking University", 1]], "method_overlap_with": {"llms_for_algorithm_d_2026-02-18_front_6": ["evolution_of_heuristics", "evolutionary_algorithm", "evolutionary_algorithms", "evolutionary_search", "genetic_algorithm", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_fine_tuned", "llm_in_the_loop", "local_search", "map_elites", "meta_learning", "program_synthesis", "reinforcement_learning", "self_improving_search", "simulated_annealing"], "llms_for_algorithm_d_2026-02-18_front_1": ["evolution_of_heuristics", "evolutionary_algorithm", "evolutionary_search", "genetic_algorithm", "hyper_heuristics", "large_language_models", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_fine_tuned", "llm_in_the_loop", "llm_prompt_optimization", "local_search", "program_synthesis", "reinforcement_learning", "self_improving_search"], "llms_for_algorithm_d_2026-02-18_front_7": ["co_evolution", "evolution_of_heuristics", "evolutionary_algorithm", "evolutionary_algorithms", "funsearch", "genetic_algorithm", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_in_the_loop", "llm_prompt_optimization", "local_search", "program_synthesis", "self_improving_search"], "llms_for_algorithm_d_2026-02-18_front_5": ["evolution_of_heuristics", "evolutionary_algorithm", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "meta_learning", "program_synthesis", "self_improving_search"]}}, {"front_id": "llms_for_algorithm_d_2026-02-18_front_1", "category": "LLMs for Algorithm Design", "snapshot_date": "2026-02-18", "core_papers": ["2401.02051", "2412.14995", "2407.10873", "2403.11446", "2504.05108", "2503.10721", "2508.03661", "2506.11057", "2402.01145", "2409.16867", "2410.22657", "2505.12285"], "size": 12, "internal_density": 0.7576, "dominant_methods": ["program_synthesis", "llm_code_generation", "llm_evolutionary_search", "llm_as_heuristic", "evolution_of_heuristics"], "dominant_problems": ["heuristic_evolution", "tsp", "bin_packing", "operator_discovery", "traveling_salesman_problem"], "growth_rate": 0.0, "stability": 1.0, "status": "stable", "name": "LLM-Enhanced Evolutionary Search: Diversity, RL, and Structural Co-evolution", "summary": "This research front focuses on advancing LLM-based evolutionary search for automated algorithm design, moving beyond initial frameworks like FunSearch and EoH. The unifying theme involves integrating sophisticated mechanisms to enhance population diversity, incorporate reinforcement learning (RL) for iterative LLM fine-tuning, and enable structural co-evolution of algorithm components. Key frameworks include MEoH for multi-objective search, EvoTune and CALM for RL-infused evolution, CAE and STRCMP for structural priors, and ReEvo for advanced reflective evolution.\n\nSignificant contributions include MEoH's 'Dominance-Dissimilarity' mechanism, achieving up to 20x better gaps on Bin Packing and 16x faster TSP heuristics. EvoTune demonstrated up to 15% better optimality gaps on Bin Packing/Flow Shop with DPO and Forward KL regularization. CAE reduced TSP optimality gaps by 2-5% through bi-dimensional structural-functional co-evolution, while STRCMP fused GNNs with LLMs to significantly reduce convergence times on MILP/SAT. CALM achieved superior performance on Bin Packing and VRP by online LLM fine-tuning with relative improvement rewards, outperforming GPT-4o baselines. EoH's 'E2' prompt strategy enabled dual-track evolution of 'thoughts' and code, outperforming FunSearch with significantly fewer LLM queries. Empirically, Zhang et al. highlighted that simple (1+1)-EPS often matches complex methods, underscoring the need for robust baselines.\n\nThis front is rapidly maturing, characterized by a shift from foundational LLM-evolutionary concepts to highly specialized and integrated approaches. The trajectory indicates a strong emphasis on improving sample efficiency, reducing computational costs, and enhancing the robustness and generalizability of discovered algorithms. Future work will likely converge on hybrid frameworks that combine the strengths of RL-based fine-tuning, structural co-evolution, and advanced diversity-maintaining mechanisms to tackle increasingly complex and real-world combinatorial optimization problems.", "future_directions": ["Extend multi-objective heuristic design frameworks, such as MEoH, to handle more than three objectives and apply them to a broader range of combinatorial optimization problems.", "Optimize and scale RL-infused evolutionary search methods like EvoTune and CALM to larger LLM models and distributed computing environments, while investigating trade-offs between compute costs and inference performance.", "Integrate structural priors and bi-dimensional co-evolution strategies (from STRCMP and CAE) into existing LLM-evolutionary pipelines to automate harness design and improve performance on topology-heavy problems.", "Develop more robust and universally effective LLM-based evolutionary program search algorithms by combining diversity-driven mechanisms (e.g., AST-based dissimilarity, Harmony Search) with efficient reflection and memory (e.g., ReEvo's verbal gradients, EoT).", "Investigate the theoretical properties of heuristic search spaces in LLM-based evolutionary program search and develop self-learning systems for continual algorithm improvement."], "top_affiliations": [["City University of Hong Kong", 5], ["Southern University of Science and Technology", 3], ["Southeast University", 2], ["Huawei Noah’s Ark Lab", 1], ["George Mason University", 1]], "method_overlap_with": {"llms_for_algorithm_d_2026-02-18_front_6": ["black_box_optimization", "code_embedding", "evolution_of_heuristics", "evolutionary_algorithm", "evolutionary_search", "genetic_algorithm", "genetic_programming", "grpo", "in_context_learning", "island_model", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_fine_tuned", "llm_in_the_loop", "local_search", "program_synthesis", "reinforcement_learning", "self_improving_search"], "llms_for_algorithm_d_2026-02-18_front_2": ["evolution_of_heuristics", "evolutionary_algorithm", "evolutionary_search", "genetic_algorithm", "hyper_heuristics", "large_language_models", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_fine_tuned", "llm_in_the_loop", "llm_prompt_optimization", "local_search", "program_synthesis", "reinforcement_learning", "self_improving_search"], "llms_for_algorithm_d_2026-02-18_front_7": ["ant_colony_optimization", "evolution_of_heuristics", "evolutionary_algorithm", "genetic_algorithm", "guided_local_search", "heuristic_evolution", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_in_the_loop", "llm_prompt_optimization", "local_search", "metaheuristics", "monte_carlo_tree_search", "multi_objective_optimization", "program_synthesis", "reflection", "self_improving_search"], "llms_for_algorithm_d_2026-02-18_front_5": ["evolution_of_heuristics", "evolutionary_algorithm", "genetic_programming", "in_context_learning", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "metaheuristics", "program_synthesis", "self_improving_search"]}}, {"front_id": "llms_for_algorithm_d_2026-02-18_front_7", "category": "LLMs for Algorithm Design", "snapshot_date": "2026-02-18", "core_papers": ["2601.19622", "2511.16485", "2512.09209", "2601.22896", "2602.02724", "2508.03082", "2512.13857", "2512.03762", "2601.15738", "2601.17899", "2601.21239"], "size": 11, "internal_density": 0.4909, "dominant_methods": ["llm_code_generation", "program_synthesis", "evolution_of_heuristics", "llm_as_heuristic", "llm_evolutionary_search"], "dominant_problems": ["combinatorial_optimization", "tsp", "heuristic_evolution", "bin_packing", "cvrp"], "growth_rate": 0.0, "stability": 1.0, "status": "stable", "name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "summary": "This research front explores advanced architectural innovations in LLM-guided algorithm design, moving beyond basic Evolution of Heuristics (EoH) and FunSearch paradigms. It focuses on sophisticated co-evolutionary strategies, multi-agent systems, and novel population representations to improve the discovery and performance of heuristics for complex combinatorial optimization problems. Key themes include evolving interdependent operators, dynamically adapting search strategies, and co-evolving problem instances or prompt templates alongside the algorithms themselves.\n\nKey contributions include E2OC, which uses MCTS to co-evolve interdependent operators, achieving up to +22% Hypervolume on FJSP/TSP. LLM4EO demonstrates online operator design for Flexible Job Shop Scheduling, yielding 3-4% RPD_BM improvement. ASRO introduces a game-theoretic framework for co-evolving solvers and adversarial instance generators, outperforming EoH by 0.5-30% on OBP, TSP, and CVRP. A-CEoH enhances prompts with algorithmic context, enabling smaller LLMs to generate superior A* heuristics for UPMP and SPP. EvoLattice proposes a DAG-based population representation with alternative-level statistics, boosting performance on NAS-Bench-Zero by over 150%. Other notable work includes LLM-driven test function generation (EoTF), co-evolution of prompts and Fireworks Algorithm operators (achieving 100% on Aircraft Landing vs. 56% for ReEvo), the dual-expert LLM4DRD for dynamic scheduling, TIDE's nested evolution for decoupling structure and parameter tuning (reducing TSP gap by 7.35%), RoCo's multi-agent system with long-term reflection, and EoH-S, which evolves complementary heuristic *sets* to reduce optimality gaps by 40-60% compared to single-heuristic approaches.\n\nThis front is rapidly emerging and maturing, characterized by a shift towards more complex, integrated LLM-driven systems. The trajectory indicates a strong focus on improving the efficiency, robustness, and generalization capabilities of generated algorithms. Future work will likely integrate these diverse architectural advancements, such as combining nested evolutionary loops with graph-based population representations, and expanding to more challenging multi-objective, constrained, and real-world dynamic optimization problems.", "future_directions": ["Integrate nested evolutionary frameworks (e.g., TIDE's structure/parameter decoupling) with advanced population representations (e.g., EvoLattice's DAG) to improve sample efficiency and convergence.", "Develop and evaluate multi-agent and game-theoretic co-evolutionary systems (e.g., ASRO, RoCo, LLM4DRD) for generating both algorithms and adversarial problem instances.", "Extend the co-evolution of prompts and operators (e.g., 2512.09209) to multi-objective and constrained real-world problems, focusing on prompt fitness functions.", "Investigate methods for dynamic, online LLM intervention and operator design (e.g., LLM4EO) within active search loops, leveraging algorithmic context (A-CEoH) and long-term reflection (RoCo).", "Explore the evolution of *sets* of complementary heuristics (EoH-S) and their application in adaptive large neighborhood search (ALNS) or other metaheuristics for diverse problem instances."], "top_affiliations": [["City University of Hong Kong", 2], ["Guangdong University of Technology", 1], ["Peking University", 1], ["Tsinghua University", 1], ["Chinese Academy of Sciences", 1]], "method_overlap_with": {"llms_for_algorithm_d_2026-02-18_front_6": ["eoh", "evolution_of_heuristics", "evolutionary_algorithm", "evolutionary_algorithms", "genetic_algorithm", "island_model_ea", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_in_the_loop", "local_search", "multi_agent_system", "multi_armed_bandit", "program_synthesis", "quality_diversity", "reevo", "self_improving_search"], "llms_for_algorithm_d_2026-02-18_front_2": ["co_evolution", "evolution_of_heuristics", "evolutionary_algorithm", "evolutionary_algorithms", "funsearch", "genetic_algorithm", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_in_the_loop", "llm_prompt_optimization", "local_search", "program_synthesis", "self_improving_search"], "llms_for_algorithm_d_2026-02-18_front_1": ["ant_colony_optimization", "evolution_of_heuristics", "evolutionary_algorithm", "genetic_algorithm", "guided_local_search", "heuristic_evolution", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_in_the_loop", "llm_prompt_optimization", "local_search", "metaheuristics", "monte_carlo_tree_search", "multi_objective_optimization", "program_synthesis", "reflection", "self_improving_search"], "llms_for_algorithm_d_2026-02-18_front_5": ["evolution_of_heuristics", "evolutionary_algorithm", "exploratory_landscape_analysis", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "metaheuristics", "multi_agent_system", "program_synthesis", "self_improving_search"]}}, {"front_id": "llms_for_algorithm_d_2026-02-18_front_5", "category": "LLMs for Algorithm Design", "snapshot_date": "2026-02-18", "core_papers": ["2601.21511", "2507.03605", "2403.02985", "2506.02049", "2602.04529", "2505.18602"], "size": 6, "internal_density": 0.4667, "dominant_methods": ["llm_code_generation", "llm_evolutionary_search", "llamea", "evolutionary_algorithm", "evolution_strategy"], "dominant_problems": ["black_box_optimization", "heuristic_evolution", "automated_algorithm_design", "expensive_continuous_optimization", "operator_discovery"], "growth_rate": 0.0, "stability": 1.0, "status": "stable", "name": "Advanced LLM-Driven Algorithm Evolution via Behavioral and Structural Guidance", "summary": "This front explores advanced techniques for LLM-driven algorithm discovery, moving beyond basic prompt engineering to incorporate sophisticated feedback mechanisms and architectural innovations. Key themes include leveraging semantics-aware selection (LLM-Meta-SR), analyzing behavioral spaces (LLaMEA), enabling decentralized code evolution (EvoGit), decoupling discovery from expensive evaluations using landscape-aware proxies (LLaMEA), integrating structural feedback from Explainable AI (LLaMEA-SAGE), and developing self-referential learning architectures (Evolution Transformer). These approaches aim to make the LLM-driven evolution process more efficient, robust, and interpretable across domains like symbolic regression and expensive continuous optimization.\n\nSpecific contributions include Zhang et al.'s LLM-Meta-SR, which achieved +2.3% R2 on SRBench for symbolic regression by using semantics-aware crossover. Huang et al.'s EvoGit introduced a novel Git-based multi-agent framework for decentralized code evolution. Yin et al. demonstrated that LLaMEA, guided by GP-evolved symbolic proxies, can discover algorithms for photonics problems that outperform baselines like LSHADE with 50x fewer real evaluations. Lange et al.'s Evolution Transformer, employing Self-Referential Algorithm Distillation (SR-EAD), learned to perform evolutionary strategy updates and generalized to unseen Brax control tasks. Furthermore, two LLaMEA papers advanced the understanding and guidance of LLM evolution: one by analyzing behavioral spaces on BBOB (5D) to show the importance of 'simplify' mutations, and another (LLaMEA-SAGE) by using SHAP analysis of AST features to guide mutations, leading to faster convergence on MA-BBOB.\n\nThis front is rapidly maturing, transitioning from demonstrating the feasibility of LLM-driven algorithm design to developing principled methods for its efficiency, robustness, and interpretability. The emphasis is shifting towards understanding why certain LLM-generated algorithms perform well and how to systematically guide their evolution. The next wave of research will likely focus on integrating multiple forms of feedback (semantic, behavioral, structural, landscape) within unified frameworks, scaling these methods to tackle higher-dimensional and more complex real-world problems, and developing more robust, open-ended self-improvement loops that can autonomously discover and refine algorithms over extended periods.", "future_directions": ["Integrate diverse feedback mechanisms, such as semantics-aware selection (LLM-Meta-SR) and structural feedback from XAI (LLaMEA-SAGE), to create more sophisticated and interpretable LLM-guided mutation strategies.", "Scale LLM-driven algorithm discovery to higher-dimensional and computationally expensive real-world problems by further developing landscape-aware symbolic proxy generation (Yin et al.) and efficient transfer learning techniques.", "Advance self-referential algorithm distillation (SR-EAD from Evolution Transformer) to enable more robust and open-ended autonomous improvement of optimizers, including generalization to novel problem classes.", "Enhance decentralized code evolution frameworks (EvoGit) with adaptive version graph pruning and automated evaluation of divergent branches to support large-scale, asynchronous multi-agent collaboration.", "Investigate the dynamic execution behavior and interaction effects of LLM-generated code to identify and mitigate subtle logical bugs, especially on small datasets, and improve the overall robustness and transferability of discovered algorithms."], "top_affiliations": [["Leiden University", 1], ["University of Stirling", 1], ["DeepMind", 1], ["TU Berlin", 1], ["The Hong Kong Polytechnic University", 1]], "method_overlap_with": {"llms_for_algorithm_d_2026-02-18_front_6": ["evolution_of_heuristics", "evolution_strategies", "evolutionary_algorithm", "genetic_programming", "in_context_learning", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "meta_learning", "multi_agent_system", "program_synthesis", "self_improving_search"], "llms_for_algorithm_d_2026-02-18_front_2": ["evolution_of_heuristics", "evolutionary_algorithm", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "meta_learning", "program_synthesis", "self_improving_search"], "llms_for_algorithm_d_2026-02-18_front_1": ["evolution_of_heuristics", "evolutionary_algorithm", "genetic_programming", "in_context_learning", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "metaheuristics", "program_synthesis", "self_improving_search"], "llms_for_algorithm_d_2026-02-18_front_7": ["evolution_of_heuristics", "evolutionary_algorithm", "exploratory_landscape_analysis", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "metaheuristics", "multi_agent_system", "program_synthesis", "self_improving_search"]}}, {"front_id": "generative_ai_for_or_2026-02-18_front_0", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "core_papers": ["2410.22296", "2602.02029", "2508.11850", "2502.14760", "2508.03117", "2507.10614", "2507.11737", "2510.16916", "2411.17404", "2505.10117", "2403.01131"], "size": 11, "internal_density": 0.5091, "dominant_methods": ["llm_in_the_loop", "llm_code_generation", "program_synthesis", "llm_as_heuristic", "llm_as_evaluator"], "dominant_problems": ["optimization_problem_formulation", "linear_programming", "constrained_optimization", "milp_general", "mixed_integer_linear_programming"], "growth_rate": 0.0, "stability": 1.0, "status": "stable", "name": "Self-Improving LLM Agents for Iterative OR Program Synthesis", "summary": "This research front focuses on developing advanced LLM-powered frameworks for robust optimization modeling and program synthesis in Operations Research. Central to these efforts are iterative generation, evaluation, and refinement mechanisms, moving beyond simple prompting. Key approaches include generative process supervision (StepORLM), multi-agent systems with adaptive revision (MIRROR, OptimAI, DAOpt), self-improving experience libraries (AlphaOPT), semantic verification (SAC-Opt), and grammar-aware generation with compiler feedback (SyntAGM). The overarching goal is to reliably translate complex natural language OR problems into executable mathematical programming code.\n\nSignificant contributions include StepORLM's Generative Process Reward Models, achieving a +29.6% Pass@1 accuracy over GPT-4o on NL4Opt. MIRROR's multi-agent framework, using Hierarchical RAG and structured revision tips, improved Macro Avg by +3.68% over Chain-of-Experts. AlphaOPT's 'Library Evolution' mechanism, which refines applicability conditions, boosted performance by +13.6% on OptiBench. SAC-Opt demonstrated ~22% accuracy improvement on ComplexLP via backward-guided semantic verification. OptimAI introduced UCB-based debug scheduling, reducing NLP4LP error rates by 58%. SyntAGM achieved 61.6% accuracy on NL4Opt using compiler-in-the-loop grammar enforcement, while DAOpt integrated LLMs with RSOME for robust optimization, achieving >70% out-of-sample feasibility. CALM enabled a 4B model to match DeepSeek-R1 on OR benchmarks through corrective adaptation. A critical survey highlighted significant error rates in existing benchmarks (16-54%) and the ineffectiveness of Chain-of-Thought for symbolic formulation, while DCP-Bench-Open introduced 'Multi-Instance Accuracy' to address LLM overfitting to example data.\n\nThis front is rapidly maturing, with a clear trajectory towards more autonomous, verifiable, and adaptive LLM agents for OR. Future work will likely emphasize integrating deeper OR domain knowledge, scaling to real-world industrial problems, and enhancing explainability. The focus is shifting from merely generating code to building systems that can intelligently debug, verify, and continually improve their modeling capabilities, moving towards human-level proficiency in complex OR problem formulation.", "future_directions": ["Extend Generative Process Reward Models (GenPRMs) to improve credit assignment in long-horizon search tasks within evolutionary algorithms, by asking the critic to 'explain then score'.", "Implement UCB-based debug scheduling in multi-agent systems to dynamically allocate computational resources and prevent agents from pursuing fundamentally flawed solution strategies.", "Adopt AlphaOPT's 'condition refinement' loop to enhance multi-agent memory systems by iteratively refining the applicability conditions of cached insights based on solver feedback.", "Develop LLM frameworks that leverage high-level Domain-Specific Languages (DSLs) like RSOME for robust and stochastic optimization, rather than attempting to derive complex mathematical duals directly.", "Integrate explicit BNF grammars and compiler-in-the-loop feedback into LLM code generation pipelines to enforce syntactic correctness and improve the reliability of generated optimization models."], "top_affiliations": [["Shanghai Jiao Tong University", 2], ["Genentech", 1], ["New York University", 1], ["The Hong Kong Polytechnic University", 1], ["InfiX.ai", 1]], "method_overlap_with": {"generative_ai_for_or_2026-02-18_front_14": ["debugging", "evolution_of_heuristics", "group_relative_policy_optimization", "in_context_learning", "linear_programming", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_fine_tuned", "llm_in_the_loop", "llm_prompt_optimization", "multi_agent_system", "process_reward_model", "program_synthesis", "retrieval_augmented_generation", "supervised_fine_tuning", "supervised_learning"], "generative_ai_for_or_2026-02-18_front_6": ["chain_of_thought", "dynamic_programming", "evolution_of_heuristics", "evolutionary_algorithm", "funsearch", "in_context_learning", "iterative_refinement", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_fine_tuned", "llm_in_the_loop", "llm_prompt_optimization", "multi_agent_system", "program_synthesis", "retrieval_augmented_generation"], "generative_ai_for_or_2026-02-18_front_1": ["linear_programming", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_fine_tuned", "llm_in_the_loop", "llm_prompt_optimization", "milp_solver", "monte_carlo_tree_search", "multi_agent_system", "program_synthesis", "supervised_fine_tuning", "supervised_learning"], "generative_ai_for_or_2026-02-18_front_7": ["llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "program_synthesis", "retrieval_augmented_generation"], "generative_ai_for_or_2026-02-18_front_9": ["llm_as_evaluator", "llm_code_generation", "llm_fine_tuned", "llm_in_the_loop", "majority_voting", "supervised_fine_tuning", "supervised_learning", "synthetic_data_generation"]}}, {"front_id": "generative_ai_for_or_2026-02-18_front_14", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "core_papers": ["2506.06052", "2508.10047", "2510.04204", "2510.18428", "2510.05115", "2407.19633", "2511.11576", "2602.03318", "2509.22558", "2601.17670", "2504.16918"], "size": 11, "internal_density": 0.4727, "dominant_methods": ["llm_as_evaluator", "llm_code_generation", "llm_in_the_loop", "llm_as_heuristic", "program_synthesis"], "dominant_problems": ["optimization_modeling", "linear_programming", "combinatorial_optimization", "mixed_integer_linear_programming", "milp_general"], "growth_rate": 0.1, "stability": 0.9091, "status": "stable", "name": "Self-Improving LLM Frameworks for Robust Optimization Model Synthesis", "summary": "This research front focuses on developing advanced LLM-driven frameworks for automated optimization modeling, specifically translating natural language problem descriptions into executable mathematical programming code (e.g., Gurobi, CPMpy, PyOPL). The unifying theme is the move beyond simple prompt engineering to sophisticated multi-agent architectures, self-correction mechanisms, and iterative refinement loops that enhance the accuracy, robustness, and reliability of generated optimization models across various problem domains, including discrete combinatorial problems, MILP, and data-driven optimization under uncertainty.\n\nKey contributions include the introduction of novel benchmarks like DCP-Bench-Open for constraint modeling and OptU for data-driven optimization under uncertainty, alongside critical re-evaluations of existing datasets (NL4Opt, IndustryOR) to address high error rates. Methodologically, frameworks like CALM leverage corrective adaptation with lightweight modification and RL to achieve significant accuracy gains (e.g., +23.6% Macro AVG for GPT-3.5-Turbo). AlphaOPT introduces a self-improving experience library that refines applicability conditions, outperforming fine-tuned models by 13%. Multi-agent systems like OptiMUS-0.3 (achieving +40% accuracy on NLP4LP) and OptimAI (88.1% accuracy on NLP4LP with UCB-based debug scheduling) integrate connection graphs, structure detection, and adaptive debugging. SAC-Opt employs backward-guided semantic verification for a 22% accuracy improvement, while SyntAGM utilizes a compiler-in-the-loop with grammar-aware generation to match multi-agent systems in accuracy. StepORLM introduces a Generative Process Reward Model (GenPRM) and Weighted DPO, enabling an 8B model to surpass GPT-4o on several OR benchmarks, and DAOpt demonstrates robust modeling under uncertainty, achieving >70% out-of-sample feasibility compared to 27% for deterministic models.\n\nThis front is rapidly maturing, demonstrating a clear trajectory from basic LLM code generation to highly sophisticated, self-correcting, and domain-aware systems. The next wave of papers will likely focus on scaling these frameworks to tackle larger, more complex industrial-scale problems, integrating deeper OR domain knowledge through specialized DSLs or knowledge graphs, and enhancing the explainability and trustworthiness of the automated modeling process to facilitate real-world adoption.", "future_directions": ["Extend current LLM-based frameworks (e.g., CALM, OptiMUS, SyntAGM) to robustly handle non-linear, multi-objective, and more complex stochastic programming structures beyond two-stage models.", "Develop more accurate and computationally efficient semantic alignment and verification strategies (e.g., building on SAC-Opt, StepORLM's GenPRM) to reduce subtle logical flaws and improve explainability in complex optimization models.", "Integrate real-time data sources and dynamic problem updates into LLM-generated models to enable adaptive re-optimization for highly dynamic operational environments.", "Scale up LLM-based optimization modeling to large-scale industrial problems, moving beyond current benchmark limitations by addressing data volume, computational efficiency, and human-in-the-loop collaboration.", "Refine memory and learning mechanisms (e.g., AlphaOPT's condition refinement, MIRROR's structured revision tips, SyntAGM's literate modeling) to improve long-term reliability and prevent misapplication of learned optimization insights."], "top_affiliations": [["Zhejiang University", 2], ["Huawei Noah’s Ark Lab", 2], ["KU Leuven", 1], ["University of Western Macedonia", 1], ["Singapore University of Social Sciences", 1]], "method_overlap_with": {"generative_ai_for_or_2026-02-18_front_0": ["debugging", "evolution_of_heuristics", "group_relative_policy_optimization", "in_context_learning", "linear_programming", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_fine_tuned", "llm_in_the_loop", "llm_prompt_optimization", "multi_agent_system", "process_reward_model", "program_synthesis", "retrieval_augmented_generation", "supervised_fine_tuning", "supervised_learning"], "generative_ai_for_or_2026-02-18_front_6": ["constraint_programming", "cpmpy", "evolution_of_heuristics", "in_context_learning", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_fine_tuned", "llm_in_the_loop", "llm_prompt_optimization", "multi_agent_system", "program_synthesis", "prompt_engineering", "reinforcement_learning", "retrieval_augmented_generation", "robust_optimization"], "generative_ai_for_or_2026-02-18_front_1": ["gurobi", "linear_programming", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_fine_tuned", "llm_in_the_loop", "llm_prompt_optimization", "mixed_integer_linear_programming", "multi_agent_system", "program_synthesis", "prompt_engineering", "reinforcement_learning", "supervised_fine_tuning", "supervised_learning"], "generative_ai_for_or_2026-02-18_front_7": ["constraint_programming", "llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "multi_agent_llm_system", "program_synthesis", "retrieval_augmented_generation"], "generative_ai_for_or_2026-02-18_front_9": ["data_cleaning", "llm_as_evaluator", "llm_code_generation", "llm_fine_tuned", "llm_in_the_loop", "supervised_fine_tuning", "supervised_learning"]}}, {"front_id": "generative_ai_for_or_2026-02-18_front_6", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "core_papers": ["2505.04354", "2506.07759", "2504.04310", "2508.14544", "2508.07468", "2503.10642", "2511.16383", "2506.07972", "2601.21372"], "size": 9, "internal_density": 0.3611, "dominant_methods": ["llm_code_generation", "llm_in_the_loop", "llm_as_heuristic", "program_synthesis", "iterative_refinement"], "dominant_problems": ["scheduling", "bin_packing", "linear_programming", "job_shop_scheduling", "tsp"], "growth_rate": 0.0, "stability": 1.0, "status": "stable", "name": "Agentic LLM Frameworks for Robust OR Model Synthesis and Validation", "summary": "This research front centers on the development of agentic LLM frameworks for automating various aspects of Operations Research, primarily focusing on robust optimization model synthesis and the design of novel heuristics. Key frameworks like NEMO, CP-Agent, and HeuriGym leverage LLMs to generate and iteratively refine optimization models (e.g., MiniZinc, CPMpy) and heuristic algorithms for complex combinatorial problems such as Flexible Job Shop Scheduling and various routing tasks. A significant unifying theme is the use of iterative, execution-aware feedback loops and agentic workflows to enhance the reliability and performance of LLM-generated OR artifacts.\n\nNotable contributions include Zadorojniy et al.'s multi-agent framework for automatic validation of optimization models, achieving 76% mutation coverage on NLP4LP. Szeider's CP-Agent demonstrates 100% accuracy on a clarified CP-Bench for CPMpy model generation, highlighting the power of persistent IPython kernels and iterative refinement. NEMO (2601.21372) achieves SOTA on 8/9 optimization benchmarks by employing an asymmetric simulator-optimizer validation loop, outperforming fine-tuned models by up to 28%. Forniés-Tabuenca et al.'s REMoH introduces a reflection mechanism with phenotypic clustering for multi-objective FJSSP, improving Pareto front diversity. Benchmarks like HeuriGym and CO-Bench are established to rigorously evaluate LLM-crafted heuristics, revealing that current SOTA LLMs saturate at ~60% of expert performance and highlighting challenges in handling context fragmentation and strict feasibility constraints.\n\nThis front is rapidly maturing, characterized by a shift from basic LLM prompting to sophisticated agentic workflows with robust validation mechanisms. The emphasis on execution-aware feedback, mutation testing, and simulator-optimizer loops signifies a move towards more reliable and verifiable LLM-generated OR solutions. The next wave of papers will likely focus on improving the computational efficiency of these agentic frameworks, scaling them to larger and more complex real-world problems, and integrating them with formal verification tools to address theoretical guarantees, pushing beyond empirical performance to provable correctness.", "future_directions": ["Refine mutation testing processes in agentic frameworks (e.g., Zadorojniy et al.) to achieve higher coverage and evaluate on more complex, real-world optimization models.", "Extend the asymmetric simulator-optimizer validation loop (NEMO) to a broader range of combinatorial optimization problems, focusing on acceleration strategies for autonomous coding agents.", "Integrate phenotypic clustering (REMoH) into other LLM-driven evolutionary frameworks to enhance diversity maintenance in multi-objective heuristic search.", "Develop LLM agents that can consistently improve under longer search budgets and better handle strict feasibility constraints, as identified by HeuriGym and CO-Bench.", "Explore the integration of automated proof assistants (e.g., Lean4) with evolutionary agentic workflows to provide theoretical verification for LLM-generated algorithms and models."], "top_affiliations": [["Carnegie Mellon University", 2], ["University of Minnesota", 1], ["Tongji University", 1], ["East China Normal University", 1], ["Vicomtech Foundation", 1]], "method_overlap_with": {"generative_ai_for_or_2026-02-18_front_0": ["chain_of_thought", "dynamic_programming", "evolution_of_heuristics", "evolutionary_algorithm", "funsearch", "in_context_learning", "iterative_refinement", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_fine_tuned", "llm_in_the_loop", "llm_prompt_optimization", "multi_agent_system", "program_synthesis", "retrieval_augmented_generation"], "generative_ai_for_or_2026-02-18_front_14": ["constraint_programming", "cpmpy", "evolution_of_heuristics", "in_context_learning", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_fine_tuned", "llm_in_the_loop", "llm_prompt_optimization", "multi_agent_system", "program_synthesis", "prompt_engineering", "reinforcement_learning", "retrieval_augmented_generation", "robust_optimization"], "generative_ai_for_or_2026-02-18_front_1": ["llm_as_heuristic", "llm_code_generation", "llm_fine_tuned", "llm_in_the_loop", "llm_prompt_optimization", "multi_agent_system", "program_synthesis", "prompt_engineering", "reinforcement_learning", "self_consistency"], "generative_ai_for_or_2026-02-18_front_7": ["constraint_programming", "llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "program_synthesis", "retrieval_augmented_generation"], "generative_ai_for_or_2026-02-18_front_9": ["llm_code_generation", "llm_fine_tuned", "llm_in_the_loop", "self_consistency"]}}, {"front_id": "generative_ai_for_or_2026-02-18_front_1", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "core_papers": ["2505.11792", "2410.13213", "2502.11102", "2411.01679", "2402.10172", "2510.27610", "2407.09887"], "size": 7, "internal_density": 0.9048, "dominant_methods": ["llm_in_the_loop", "llm_code_generation", "llm_as_evaluator", "llm_as_heuristic", "llm_fine_tuned"], "dominant_problems": ["program_synthesis", "linear_programming", "mixed_integer_programming", "optimization_modeling", "automated_optimization_modeling"], "growth_rate": -0.125, "stability": 0.875, "status": "stable", "name": "LLM-Enhanced Optimization Modeling via SIRL, MCTS, and Graph-Theoretic Evaluation", "summary": "This research front focuses on advancing Large Language Models (LLMs) for automated optimization modeling, data synthesis, and rigorous evaluation. Key frameworks include Solver-Informed RL (SIRL) with a Partial KL strategy, LLM-enhanced Monte-Carlo Tree Search (MCTS) with symbolic pruning, modular multi-agent systems like OptiMUS using a Connection Graph, and graph-theoretic evaluation frameworks such as ORGEval. The unifying theme is to develop more robust, scalable, and verifiable methods for LLM-driven generation and refinement of optimization models, often leveraging structured intermediate representations or search processes.\n\nKey contributions include SIRL's Reinforcement Learning with Verifiable Reward (RLVR) achieving +3.3% Macro AVG on benchmarks like NL4OPT, and LLMOPT's multi-instruction SFT and KTO yielding up to +19.8% SA improvement over ORLM. OptMATH and ReSocratic introduce scalable bidirectional data synthesis frameworks, with OptMATH-Qwen2.5-32B surpassing GPT-4 on NL4Opt. The Autoformulator integrates LLMs with MCTS and SMT-based symbolic pruning, outperforming ORLMLlama3-8B by up to +27.8% on ComplexOR. OptiMUS utilizes a modular multi-agent system with a Connection Graph for context management, achieving significant accuracy gains (e.g., +57.2% on ComplexOR). ORGEval provides a graph-theoretic evaluation using the Weisfeiler-Lehman test for model isomorphism, achieving 100% consistency with solver-based methods in seconds for hard MIPLIB instances.\n\nThis front is emerging and rapidly maturing, with a high density and stable status indicating active, high-impact research. The trajectory suggests a strong push towards integrating these diverse techniques, such as combining RL with symbolic pruning or incorporating graph-theoretic evaluation into RL reward functions. Future work will likely address current limitations like reward hacking, data scarcity for large-scale problems, and extending capabilities to non-linear or stochastic optimization, emphasizing reliable and generalizable LLM capabilities in Operations Research.", "future_directions": ["Integrate SMT-based symbolic pruning (from Autoformulator) and graph-theoretic evaluation (from ORGEval) into Reinforcement Learning with Verifiable Reward (SIRL) to enhance sample efficiency and mitigate reward hacking.", "Extend bidirectional data synthesis frameworks like OptMATH and ReSocratic to generate high-quality, diverse datasets for non-linear, stochastic, and multi-objective optimization problems.", "Develop LLM-based approaches capable of understanding and utilizing data stored in structured databases or files for large-scale optimization problems, moving beyond in-context problem descriptions.", "Investigate methods for providing more granular, structural feedback to LLMs for error correction, moving beyond binary success/failure metrics to improve model formulation accuracy.", "Explore the application of the Connection Graph mechanism (from OptiMUS) to manage context in other complex, iterative LLM-driven code generation and reasoning tasks."], "top_affiliations": [["Stanford University", 2], ["Shanghai Jiao Tong University", 1], ["The University of Hong Kong", 1], ["Shanghai University of Finance and Economics", 1], ["Cardinal Operations", 1]], "method_overlap_with": {"generative_ai_for_or_2026-02-18_front_0": ["linear_programming", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_fine_tuned", "llm_in_the_loop", "llm_prompt_optimization", "milp_solver", "monte_carlo_tree_search", "multi_agent_system", "program_synthesis", "supervised_fine_tuning", "supervised_learning"], "generative_ai_for_or_2026-02-18_front_14": ["gurobi", "linear_programming", "llm_as_evaluator", "llm_as_heuristic", "llm_code_generation", "llm_fine_tuned", "llm_in_the_loop", "llm_prompt_optimization", "mixed_integer_linear_programming", "multi_agent_system", "program_synthesis", "prompt_engineering", "reinforcement_learning", "supervised_fine_tuning", "supervised_learning"], "generative_ai_for_or_2026-02-18_front_6": ["llm_as_heuristic", "llm_code_generation", "llm_fine_tuned", "llm_in_the_loop", "llm_prompt_optimization", "multi_agent_system", "program_synthesis", "prompt_engineering", "reinforcement_learning", "self_consistency"], "generative_ai_for_or_2026-02-18_front_7": ["llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "program_synthesis"], "generative_ai_for_or_2026-02-18_front_9": ["data_augmentation", "llm_as_evaluator", "llm_code_generation", "llm_fine_tuned", "llm_in_the_loop", "self_consistency", "supervised_fine_tuning", "supervised_learning"]}}, {"front_id": "generative_ai_for_or_2026-02-18_front_7", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "core_papers": ["2505.01485", "2509.08970"], "size": 2, "internal_density": 1.0, "dominant_methods": ["llm_code_generation", "llm_in_the_loop", "retrieval_augmented_generation", "hierarchical_chunking", "metadata_augmented_indexing"], "dominant_problems": ["linear_programming_code_generation", "program_synthesis", "text_to_model_translation", "constraint_programming_modeling", "natural_language_for_optimization"], "growth_rate": 0.0, "stability": 1.0, "status": "stable", "name": "Hierarchical RAG and Multi-Agent LLMs for Optimization Model Synthesis", "summary": "This research front focuses on leveraging advanced LLM architectures, specifically Retrieval-Augmented Generation (RAG) and multi-agent systems, for the automated synthesis of optimization models from natural language descriptions. CHORUS introduces a RAG framework with hierarchical retrieval and metadata-augmented indexing for generating Linear Programming (LP) code (e.g., Gurobi). GALA proposes a multi-agent LLM framework for text-to-MiniZinc translation, specializing agents in detecting and assembling global Constraint Programming (CP) constraints.\n\nCHORUS significantly improves LP code generation accuracy, achieving a +147.9% increase for Llama3.3 (70B) on the NL4Opt-Code dataset, by using a novel metadata-augmented indexing strategy that bridges the semantic gap between natural language and solver APIs. GALA demonstrates the effectiveness of decomposing complex translation tasks into primitive-specific agents, showing a modest improvement in execution rate (57% vs 52% with o3-mini) over Chain-of-Thought on the TEXT2ZINC benchmark for MiniZinc model generation. Both approaches highlight the power of structured LLM interaction and specialized knowledge retrieval/processing for robust program synthesis in optimization.\n\nThis front is emerging, demonstrating early successes in applying sophisticated LLM architectures to the challenging domain of automated optimization modeling. The trajectory suggests a move towards more structured, agentic, and knowledge-augmented LLM systems that can better understand and formalize complex problem descriptions into executable optimization models. The likely next paper will focus on integrating these architectural innovations, perhaps combining advanced retrieval with multi-agent reasoning, or extending them to more complex problem types and solver paradigms beyond LP and CP.", "future_directions": ["Integrate CHORUS's metadata-augmented indexing with GALA's multi-agent framework for enhanced constraint detection and model assembly across optimization paradigms.", "Develop adaptive retrieval strategies and multi-step reasoning agents to improve context-aware generation of complex optimization models.", "Implement compile-time snippet validation and post-hoc linking mechanisms within agentic frameworks to ensure correctness and unification of generated models.", "Extend these structured LLM approaches to generate models for Mixed-Integer Linear Programming (MILP) or Satisfiability Modulo Theories (SMT).", "Systematically optimize agent prompts and leverage few-shot exemplars or fine-tuning to improve text-to-model translation performance and reduce hallucination."], "top_affiliations": [["Queen's University", 1], ["University of Southern California", 1], ["Brown University", 1], ["Fidelity Investments", 1]], "method_overlap_with": {"generative_ai_for_or_2026-02-18_front_0": ["llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "program_synthesis", "retrieval_augmented_generation"], "generative_ai_for_or_2026-02-18_front_14": ["constraint_programming", "llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "multi_agent_llm_system", "program_synthesis", "retrieval_augmented_generation"], "generative_ai_for_or_2026-02-18_front_6": ["constraint_programming", "llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "program_synthesis", "retrieval_augmented_generation"], "generative_ai_for_or_2026-02-18_front_1": ["llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "program_synthesis"], "generative_ai_for_or_2026-02-18_front_9": ["llm_code_generation", "llm_in_the_loop"]}}, {"front_id": "generative_ai_for_or_2026-02-18_front_9", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "core_papers": ["2512.18682", "2509.22979"], "size": 2, "internal_density": 1.0, "dominant_methods": ["llm_code_generation", "llm_as_evaluator", "supervised_learning", "llm_fine_tuned", "synthetic_data_generation"], "dominant_problems": ["automated_problem_formulation", "expensive_black_box_optimization", "antenna_design", "constrained_optimization", "mixed_integer_linear_programming"], "growth_rate": 0.0, "stability": 1.0, "status": "stable", "name": "LLM-Guided Search and Fine-Tuning for OR Model and Algorithm Synthesis", "summary": "This research front focuses on advanced techniques for leveraging Large Language Models (LLMs) to automate and enhance Operations Research (OR) problem formulation and algorithm design. The core theme revolves around integrating LLMs into sophisticated search algorithms, employing specialized fine-tuning strategies with preference learning and synthetic data, and utilizing structured intermediate representations or hierarchical decomposition to improve the robustness, verifiability, and efficiency of generated OR solutions. Specific frameworks like BPP-Search, MiCo (FunSearch), EvoCut, SolverLLM, EquivaMap, DPLM (DualReflect), CIR, LLaMoCo, and LLOME (MargE) are central to these efforts.\n\nKey contributions include Liu et al.'s DPO with Diversity-Aware Rank-based sampling, enabling a fine-tuned Llama-3.2-1B to match an 8B model on ASP and CVRP. LLaMoCo demonstrates that a 350M parameter model can significantly outperform GPT-4 in optimization code generation through instruction tuning. LLOME introduces the MargE loss function for stable and diverse fine-tuning in constrained optimization, outperforming DPO. In search, BPP-Search enhances Tree-of-Thought for mathematical modeling, while MiCo applies a hierarchical SMDP framework to VM scheduling, achieving an 11% improvement over Deep RL. EvoCut uses LLMs for MILP cut generation, yielding 17-57% gap reductions. SolverLLM introduces Prompt Backpropagation within MCTS for robust formulation. For verification and representation, EquivaMap achieves 100% accuracy in checking formulation equivalence, OptiTrust uses multi-language execution voting for verifiable synthetic data, and CIR introduces a Canonical Intermediate Representation, boosting accuracy to 47.2% on the ORCOpt-Bench.\n\nThis front is rapidly emerging and maturing, characterized by a strong emphasis on moving beyond generic LLM prompting towards highly specialized and verifiable integration. The trajectory indicates a clear focus on improving the reliability and performance of LLM-generated OR artifacts. Future work will likely concentrate on combining these advanced techniques, such as integrating MargE-style preference learning into MCTS or evolutionary search frameworks, and expanding CIR-like structured generation to encompass more complex, real-world OR problems, potentially with dynamic cut separation and formal proof systems.", "future_directions": ["Integrate the MargE objective (LLOME) into evolutionary search frameworks (EvoCut, MiCo) and MCTS (SolverLLM) to improve the stability, diversity, and quality of evolved heuristics and policies in constrained optimization tasks.", "Adopt 'Prompt Backpropagation' (SolverLLM) and 'LP separation check' (EvoCut) mechanisms within LLM-guided evolutionary search pipelines (e.g., AlgoEvo) to dynamically modify prompts, prevent recurrence of failed code patterns, and provide high-signal rewards for mutation steps.", "Extend the Canonical Intermediate Representation (CIR) framework to serve as a structured, paradigm-aware mutation space for LLM-based algorithm evolution, replacing brittle code evolution with more robust, domain-specific generation for complex OR problems like VRPs and scheduling.", "Apply 'Backward Generation' (DualReflect in DPLM) and 'Symbolic -> NL' synthetic data generation (OptiTrust) strategies to create massive, verifiable datasets for fine-tuning specialized LLMs (LLaMoCo, Liu et al.) for specific OR sub-problems or algorithm design tasks, particularly for cold-starting small models.", "Develop methods to dynamically integrate LLM-generated acceleration cuts (EvoCut) via solver callbacks during execution and incorporate automated proof systems to generate provably optimality-preserving or valid cuts, moving beyond empirical verification."], "top_affiliations": [["Xidian University", 1], ["Victoria University of Wellington", 1], ["Westlake University", 1], ["Microsoft Research", 1], ["Stanford University", 1]], "method_overlap_with": {"generative_ai_for_or_2026-02-18_front_0": ["llm_as_evaluator", "llm_code_generation", "llm_fine_tuned", "llm_in_the_loop", "majority_voting", "supervised_fine_tuning", "supervised_learning", "synthetic_data_generation"], "generative_ai_for_or_2026-02-18_front_14": ["data_cleaning", "llm_as_evaluator", "llm_code_generation", "llm_fine_tuned", "llm_in_the_loop", "supervised_fine_tuning", "supervised_learning"], "generative_ai_for_or_2026-02-18_front_6": ["llm_code_generation", "llm_fine_tuned", "llm_in_the_loop", "self_consistency"], "generative_ai_for_or_2026-02-18_front_1": ["data_augmentation", "llm_as_evaluator", "llm_code_generation", "llm_fine_tuned", "llm_in_the_loop", "self_consistency", "supervised_fine_tuning", "supervised_learning"], "generative_ai_for_or_2026-02-18_front_7": ["llm_code_generation", "llm_in_the_loop"]}}, {"front_id": "or_for_generative_ai_2026-02-18_front_10", "category": "OR for Generative AI", "snapshot_date": "2026-02-18", "core_papers": ["2602.14516", "2510.11331", "2502.00722", "2508.09229", "2506.04203", "2502.14617", "2507.10259", "2512.21884", "2505.23970", "2406.01566"], "size": 10, "internal_density": 0.7111, "dominant_methods": ["integer_linear_programming", "pipeline_parallelism", "data_parallelism", "tensor_parallelism", "performance_modeling"], "dominant_problems": ["llm_serving_optimization", "resource_allocation", "gpu_scheduling", "llm_scheduling", "service_level_objective_optimization"], "growth_rate": 0.6667, "stability": 0.4545, "status": "growing", "name": "Integer Linear Programming for Heterogeneous LLM Serving Resource Allocation", "summary": "This research front is unified by the application of Integer Linear Programming (ILP) and Mixed-Integer Linear Programming (MILP) to optimize various aspects of Large Language Model (LLM) serving. The core theme revolves around efficient resource allocation, scheduling, and deployment strategies for LLM inference, particularly addressing challenges posed by heterogeneous GPU clusters, disaggregated serving architectures, and geographically distributed systems. Specific problem domains include multi-round inference, Mixture-of-Expert (MoE) model placement, cascade serving, and carbon-aware caching.\n\nKey contributions include Dynamo's ILP-based offline deployment for multi-round inference, achieving up to 340% SLO attainment improvement (Paper 1). Jiang et al. (Paper 3) demonstrated ~25% throughput gains over SOTA systems like Helix by co-optimizing GPU composition and parallelism using MILP for heterogeneous clouds. CASCADIA (Paper 5) introduced a bi-level optimization (MILP for deployment, Chebyshev for routing) for cascade serving, yielding 2.3x average throughput gains. SageServe (Paper 6) achieved 25% GPU-hours savings and $2.5M/month savings by coupling ILP with ARIMA forecasting for auto-scaling. Helix (Paper 10) formulated distributed LLM serving as a max-flow MILP, achieving up to 3.3x throughput gains on mixed GPU clusters by dynamic per-request routing. Other notable works include ILP for MoE expert placement (Paper 4), carbon-aware KV cache management (Paper 9), and hybrid RL-Optimal Transport for temporal-aware GPU allocation (Paper 7).\n\nThis front is rapidly maturing, driven by the increasing complexity and scale of LLM deployments. The consistent success of ILP/MILP in achieving significant performance, cost, and energy efficiency gains across diverse LLM serving scenarios indicates a strong and active research trajectory. Future work will likely focus on developing more scalable and dynamic OR solutions, integrating these with advanced LLM-specific optimizations, and expanding to multi-objective and real-time adaptive systems.", "future_directions": ["Develop scalable and dynamic ILP/MILP solvers capable of real-time adaptation to changing cluster conditions, heterogeneous hardware, and highly dynamic LLM workloads.", "Integrate ILP/MILP-based resource allocation frameworks with advanced LLM inference optimizations such as speculative decoding, quantization, and multi-agent workflow management.", "Extend optimization objectives beyond traditional latency and throughput to include cost, carbon emissions, and diverse Service Level Objectives (SLAs) in multi-tenant and disaggregated serving environments.", "Improve heuristic decomposition strategies for complex MILP problems in distributed LLM serving, specifically for MoE expert placement and dynamic attention cache block allocation.", "Investigate hybrid Operations Research-Reinforcement Learning (OR-RL) approaches for temporal-aware GPU resource allocation, leveraging OR for optimal supervision and RL for dynamic foresight."], "top_affiliations": [["University of Cambridge", 3], ["Peking University", 2], ["Purdue University", 2], ["Indian Institute of Science", 2], ["Shanghai Jiao Tong University", 1]], "method_overlap_with": {"or_for_generative_ai_2026-02-18_front_6": ["greedy_algorithm", "llm_as_evaluator", "load_balancing", "proximal_policy_optimization", "queueing_theory", "reinforcement_learning", "resource_allocation", "scheduling_algorithms"], "or_for_generative_ai_2026-02-18_front_0": ["bi_level_optimization", "continuous_batching", "load_balancing", "performance_modeling", "queueing_theory"], "or_for_generative_ai_2026-02-18_front_3": ["greedy_algorithm", "integer_linear_programming", "llm_as_evaluator", "load_balancing", "mixed_integer_linear_programming", "resource_allocation", "speculative_decoding"], "or_for_generative_ai_2026-02-18_front_32": ["llm_as_evaluator", "robust_optimization"]}}, {"front_id": "or_for_generative_ai_2026-02-18_front_6", "category": "OR for Generative AI", "snapshot_date": "2026-02-18", "core_papers": ["2504.11320", "2512.16134", "2511.09092", "2502.07115", "2504.07347", "2510.17015", "2405.17743"], "size": 7, "internal_density": 0.5714, "dominant_methods": ["queueing_theory", "reinforcement_learning", "llm_code_generation", "llm_fine_tuned", "program_synthesis"], "dominant_problems": ["llm_inference_scheduling", "resource_allocation", "gpu_scheduling", "online_scheduling", "llm_serving_optimization"], "growth_rate": 0.0, "stability": 0.25, "status": "emerging", "name": "Optimizing LLM Inference and Automated OR Problem Generation", "summary": "This research front explores the application of advanced Operations Research (OR) principles and AI techniques to two critical challenges within the LLM ecosystem: optimizing LLM inference serving and automating the generation of OR models. For LLM inference, papers focus on sophisticated scheduling algorithms to manage GPU resources, KV cache memory, and request batching under various constraints. Concurrently, other works leverage large language models themselves to automatically formulate and solve complex OR problems, bridging the gap between natural language problem descriptions and executable optimization models.\n\nKey contributions in LLM inference scheduling include Nested WAIT (Paper 1) for multi-stage online scheduling, achieving superior throughput and reduced latency on vLLM/Sarathi. Staggered Batch Scheduling (SBS) (Paper 2) for DP+EP architectures reduced Time-to-First-Token by 30-40% and increased throughput by 15-20% on DeepSeek-V3. Memory Constrained Shortest First (MC-SF) (Paper 4) achieved near-optimal latency (within 5% of hindsight optimal) for KV cache-aware online scheduling. Justitia (Paper 6) introduced a virtual-time based fair scheduler, reducing average job completion time by ~60%. In automated OR modeling, OR-R1 (Paper 3) integrated SFT and Test-Time Group Relative Policy Optimization (TGRPO) to improve modeling accuracy by +4.2% over ORLM. ORLM (Paper 7) and the OR-Instruct framework fine-tuned LLMs with synthetic data, outperforming GPT-4 by up to 38.4% on benchmarks like NL4OPT and IndustryOR.\n\nThis front is emerging, with significant activity in both LLM inference optimization and automated OR modeling. The trajectory suggests continued innovation in developing more robust and adaptive scheduling policies for increasingly complex LLM architectures and deployment scenarios. For automated OR, the next papers will likely focus on enhancing LLM capabilities for solution ranking, handling more diverse and complex problem types, and integrating multi-agent collaboration for sophisticated problem-solving.", "future_directions": ["Extend LLM inference scheduling frameworks (e.g., Nested WAIT, SBS, MC-SF) to multi-GPU, heterogeneous, and distributed clusters, addressing communication costs and memory fragmentation.", "Develop joint prediction and scheduling policies for LLM inference, dynamically adapting to varying workloads and KV cache constraints for optimal tail latency.", "Enhance automated OR modeling frameworks (e.g., OR-R1, ORLM) by incorporating reinforcement learning for solution ranking and extending to a wider range of complex OR problem types beyond LP/MILP.", "Investigate multi-agent collaboration and internal chain-of-thought structures within LLMs for more sophisticated OR problem formulation and solving.", "Integrate advanced flow control mechanisms and LLM acceleration techniques (e.g., FlashAttention, quantization) into existing scheduling policies for improved efficiency."], "top_affiliations": [["Massachusetts Institute of Technology", 2], ["Columbia University", 2], ["Shanghai Jiao Tong University", 2], ["Peking University", 1], ["Alibaba Group", 1]], "method_overlap_with": {"or_for_generative_ai_2026-02-18_front_10": ["greedy_algorithm", "llm_as_evaluator", "load_balancing", "proximal_policy_optimization", "queueing_theory", "reinforcement_learning", "resource_allocation", "scheduling_algorithms"], "or_for_generative_ai_2026-02-18_front_0": ["bin_packing", "load_balancing", "lyapunov_function", "online_scheduling", "queueing_theory", "scheduling"], "or_for_generative_ai_2026-02-18_front_3": ["greedy_algorithm", "llm_as_evaluator", "llm_in_the_loop", "load_balancing", "queuing_theory", "resource_allocation", "scheduling"], "or_for_generative_ai_2026-02-18_front_32": ["llm_as_evaluator", "llm_in_the_loop", "supervised_fine_tuning"], "or_for_generative_ai_2026-02-18_front_34": ["integer_programming"]}}, {"front_id": "or_for_generative_ai_2026-02-18_front_0", "category": "OR for Generative AI", "snapshot_date": "2026-02-18", "core_papers": ["2602.02987", "2512.21487", "2503.09357", "2508.01002", "2502.15763", "2512.15705"], "size": 6, "internal_density": 0.5333, "dominant_methods": ["mixed_integer_programming", "heuristic_search", "continuous_batching", "stochastic_control", "queueing_network"], "dominant_problems": ["llm_serving_optimization", "resource_allocation", "llm_inference_scheduling", "scheduling", "makespan_minimization"], "growth_rate": 0.0, "stability": 0.0909, "status": "emerging", "name": "Optimal LLM Inference Scheduling with Queueing Theory and MIP", "summary": "This research front unifies advanced Operations Research techniques, specifically many-server queueing network models, Mixed-Integer Programming (MIP), and stochastic control, to achieve optimal or near-optimal LLM inference scheduling and resource allocation. Key themes include managing prefill-decode contention, optimizing disaggregated expert parallelism in Mixture-of-Experts (MoE) models, and automatic operator-level parallelism planning for distributed deep learning.\n\nKey contributions include Lin et al.'s (2026) 'Gate-and-Route' policy derived from a fluid LP, demonstrating ~30% lower revenue loss than OPT on Dolly-15k. She et al. (2025) used MIP for operator-level parallelism, reducing pipeline bubbles by 50% for DeepSeek V3. Bari et al. (2025) introduced RAD and SLAI schedulers, achieving a 53% reduction in median TTFT and 26% capacity increase over Sarathi-Serve on Mistral-7B. Pang et al. (2025) combined offline bin-packing with an online Lagrangian heuristic, improving utilization by 8.86% over vLLM FCFS on LLaMA-65B. FinDEP (2025) optimized MoE inference with fine-grained scheduling, yielding up to 1.61x throughput improvement on Qwen3-235B, while DREX (2025) showed 2-12% throughput gains on Llama-EE-70B using dynamic rebatching for early-exit LLMs.\n\nThis front is emerging, characterized by a strong trend towards rigorous mathematical modeling to solve complex LLM serving challenges, moving beyond heuristic-driven approaches. The next papers will likely focus on relaxing simplifying assumptions (e.g., exponential service times), integrating stochastic programming for uncertainty, and extending these optimal control strategies to more heterogeneous and dynamic LLM architectures, such as those with speculative decoding or Mixture-of-Depths.", "future_directions": ["Relax exponential service time assumptions in queueing models to general distributions using measure-valued processes, as suggested by Lin et al. (2026).", "Develop diffusion approximations for rigorous tail-latency Service Level Objective (SLO) guarantees, building on the work by Lin et al. (2026) and Bari et al. (2025).", "Incorporate stochastic programming into MIP models to account for uncertainties in LLM output lengths and dynamic workloads, addressing limitations identified by Pang et al. (2025).", "Generalize optimal scheduling models to heterogeneous GPU infrastructures and dynamic agent architectures, extending the scope of Lin et al. (2026) and She et al. (2025).", "Integrate dynamic rebatching strategies (like DREX) with speculative decoding and other dynamic computation LLMs (e.g., Mixture-of-Depths) for further inference acceleration."], "top_affiliations": [["The Hong Kong University of Science and Technology", 2], ["Huawei", 2], ["Harbin Institute of Technology", 1], ["Hong Kong Baptist University", 1], ["The University of Texas at Austin", 1]], "method_overlap_with": {"or_for_generative_ai_2026-02-18_front_10": ["bi_level_optimization", "continuous_batching", "load_balancing", "performance_modeling", "queueing_theory"], "or_for_generative_ai_2026-02-18_front_6": ["bin_packing", "load_balancing", "lyapunov_function", "online_scheduling", "queueing_theory", "scheduling"], "or_for_generative_ai_2026-02-18_front_3": ["convex_optimization", "linear_programming", "load_balancing", "scheduling"], "or_for_generative_ai_2026-02-18_front_32": ["convex_optimization", "linear_programming"], "or_for_generative_ai_2026-02-18_front_34": ["linear_programming"]}}, {"front_id": "or_for_generative_ai_2026-02-18_front_3", "category": "OR for Generative AI", "snapshot_date": "2026-02-18", "core_papers": ["2509.21091", "2512.09963", "2602.00509", "2502.13575", "2504.08930", "2506.15707"], "size": 6, "internal_density": 0.4667, "dominant_methods": ["llm_in_the_loop", "resource_allocation", "llm_as_heuristic", "rebase", "process_reward_model"], "dominant_problems": ["llm_serving_optimization", "llm_inference_optimization", "mathematical_reasoning", "scientific_reasoning", "ensemble_optimization"], "growth_rate": 2.0, "stability": 0.3333, "status": "growing", "name": "OR-Driven Adaptive Resource Allocation for LLM Inference and Test-Time Search", "summary": "This research front focuses on applying advanced Operations Research techniques to optimize various aspects of Large Language Model (LLM) inference and serving. Key approaches include Bayesian adaptive stopping for efficient ensemble evaluation, gradient-based scheduling in GoodSpeed for distributed speculative decoding, PROBE's predictive Lookahead Pipelining for MoE inference, and ETS's Integer Linear Programming for KV cache optimization in tree search. Additionally, DORA employs embedding-based resource allocation for test-time search, and VectorLiteRAG uses analytical modeling for RAG serving.\n\nContributions include Bayesian adaptive sampling (2-5x compute reduction on AIME/GPQA) and MILP for LLM ensembles. GoodSpeed demonstrates gradient-based scheduling for distributed speculative decoding, achieving fair goodput on H100/L4 clusters. PROBE's Lookahead Pipelining yields a 1.3x speedup for Qwen3-MoE-235B inference, while ETS leverages ILP to achieve 1.8x KV cache reduction and 1.4x throughput increase on MATH500 compared to REBASE. VectorLiteRAG provides 1.5x throughput gains for RAG serving on H100/L40S, and DORA achieves state-of-the-art accuracy on MATH500 with 3.5x fewer FLOPs by optimizing test-time search.\n\nThis front is rapidly maturing, characterized by the increasing sophistication of OR methods integrated directly into LLM serving and reasoning pipelines. The trajectory points towards more tightly coupled, real-time optimization, where OR solvers dynamically adapt resource allocation and search strategies. Future work will likely focus on developing unified frameworks that combine predictive modeling, adaptive sampling, and advanced combinatorial optimization to handle the stochastic and dynamic nature of LLM workloads across diverse hardware architectures and reasoning tasks.", "future_directions": ["Integrate Bayesian adaptive stopping and semantic uniqueness reweighting (from DORA) into evolutionary search frameworks like AlgoEvo to reduce fitness evaluation costs and improve sample efficiency.", "Extend PROBE's Lookahead Pipelining architecture by injecting specialized combinatorial optimization solvers (e.g., ALNS or IP formulations) to replace greedy resource allocation in deterministic execution windows.", "Develop unified MILP/ILP formulations for dynamic resource allocation in LLM ensembles and test-time search, considering KV cache sharing, semantic diversity, and real-time performance metrics.", "Adapt VectorLiteRAG's analytical modeling of tail latency using Beta distributions to predict and manage stochastic constraints in dynamic LLM serving and resource partitioning problems.", "Explore the dynamic adjustment of control parameters (e.g., smoothing parameters in GoodSpeed, adaptive stopping criteria) and the use of instruction-tuned models for reward signals in adaptive LLM inference systems."], "top_affiliations": [["Mohamed bin Zayed University of Artificial Intelligence", 1], ["New York University", 1], ["RIKEN AIP", 1], ["Institute of Science Tokyo", 1], ["NEC Corporation", 1]], "method_overlap_with": {"or_for_generative_ai_2026-02-18_front_10": ["greedy_algorithm", "integer_linear_programming", "llm_as_evaluator", "load_balancing", "mixed_integer_linear_programming", "resource_allocation", "speculative_decoding"], "or_for_generative_ai_2026-02-18_front_6": ["greedy_algorithm", "llm_as_evaluator", "llm_in_the_loop", "load_balancing", "queuing_theory", "resource_allocation", "scheduling"], "or_for_generative_ai_2026-02-18_front_0": ["convex_optimization", "linear_programming", "load_balancing", "scheduling"], "or_for_generative_ai_2026-02-18_front_32": ["convex_optimization", "linear_programming", "llm_as_evaluator", "llm_in_the_loop"], "or_for_generative_ai_2026-02-18_front_34": ["linear_programming"]}}, {"front_id": "or_for_generative_ai_2026-02-18_front_32", "category": "OR for Generative AI", "snapshot_date": "2026-02-18", "core_papers": ["2508.07768", "2503.08796", "2510.09330"], "size": 3, "internal_density": 1.0, "dominant_methods": ["multi_objective_optimization", "convex_optimization", "gradient_descent", "game_theory", "llm_as_evaluator"], "dominant_problems": ["multi_objective_llm_alignment", "sentiment_control", "text_length_control", "humor_generation", "harmlessness_control"], "growth_rate": 0.5, "stability": 0.6667, "status": "growing", "name": "Convex Optimization and Game Theory for Robust Multi-Objective LLM Alignment", "summary": "This research front unifies recent advancements in applying Operations Research techniques, specifically convex optimization and game theory, to the challenging problem of multi-objective alignment for Large Language Models (LLMs). Papers introduce frameworks like PAMA, Safety Game, and Robust Multi-Objective Decoding (RMOD) to manage conflicting objectives such as harmlessness, helpfulness, sentiment, and length control, often at inference time.\n\nKey contributions include the PAMA algorithm, which transforms multi-objective RLHF into an O(n) convex optimization problem with a closed-form solution, outperforming MORLHF and MGDA-UB on LLaMA-2 7B for harmlessness. The Safety Game formulates black-box LLM agent alignment as a zero-sum game solvable by an LP solver at inference, achieving up to two-fold accuracy improvement on SafetyBench. RMOD introduces a maximin two-player game for robust multi-objective decoding, solving a convex optimization problem at each step to maximize worst-case value, outperforming MO-DPO and scalarized baselines by +1.2% WCWR on Anthropic HH.\n\nThis front is rapidly growing, demonstrating the power of OR principles to bring robustness and efficiency to LLM alignment. The trajectory indicates a strong focus on mathematically grounded, inference-time control mechanisms. Future work will likely focus on extending these frameworks to more complex, dynamic, and multi-agent scenarios, improving their scalability to a greater number of objectives, and integrating these control mechanisms into broader agentic architectures.", "future_directions": ["Extend PAMA's theoretical guarantees to non-convex settings or different reward structures for multi-objective RLHF.", "Adapt the Safety Game framework to handle sequential dialogues and open-ended questions for black-box LLM agents, and explore multi-player game formulations.", "Scale RMOD to manage a larger number of conflicting objectives and improve value function prediction accuracy for very short decoding blocks.", "Investigate integrating dynamic adversarial weighting mechanisms (from RMOD) into evolutionary search algorithms like AlgoEvo to balance conflicting code metrics.", "Develop adaptive weighting schemes for objectives within PAMA to further enhance flexibility in real-world LLM alignment tasks."], "top_affiliations": [["Ruhr University Bochum", 1], ["University College London", 1], ["University of Basel", 1], ["Ulsan National Institute of Science and Technology", 1], ["University of Warwick", 1]], "method_overlap_with": {"or_for_generative_ai_2026-02-18_front_10": ["llm_as_evaluator", "robust_optimization"], "or_for_generative_ai_2026-02-18_front_6": ["llm_as_evaluator", "llm_in_the_loop", "supervised_fine_tuning"], "or_for_generative_ai_2026-02-18_front_0": ["convex_optimization", "linear_programming"], "or_for_generative_ai_2026-02-18_front_3": ["convex_optimization", "linear_programming", "llm_as_evaluator", "llm_in_the_loop"], "or_for_generative_ai_2026-02-18_front_34": ["linear_programming"]}}, {"front_id": "or_for_generative_ai_2026-02-18_front_34", "category": "OR for Generative AI", "snapshot_date": "2026-02-18", "core_papers": ["2410.06270", "2510.10962"], "size": 2, "internal_density": 1.0, "dominant_methods": ["post_training_quantization", "mixed_precision_quantization", "linear_programming", "gptq", "expert_quantization"], "dominant_problems": ["llm_compression", "memory_optimization", "inference_efficiency", "mixture_of_experts_compression", "vlm_compression"], "growth_rate": 0.0, "stability": 0.0, "status": "emerging", "name": "Linear Programming for MoE LLM Mixed-Precision Quantization and Pruning", "summary": "This research front centers on the Mixture Compressor (MC and MC#) frameworks, which leverage Operations Research techniques, specifically Linear Programming, for the extreme compression of Mixture-of-Experts (MoE) Large Language Models. The core theme involves optimizing mixed-precision quantization and dynamic expert pruning to significantly reduce model size while preserving performance, enabling deployment on resource-constrained hardware.\n\nThe initial paper, \"Mixture Compressor for Mixture-of-Experts LLMs Gains More\" (Huang et al., 2025-02), introduces a hybrid post-training quantization and dynamic pruning approach. It uses Linear Programming to optimally allocate mixed bit-widths (1-3 bits) to experts based on activation frequency, achieving strong empirical results such as compressing Mixtral 8x7b to ~16GB with only a ~4% drop in zero-shot accuracy, outperforming uniform GPTQ. Building on this, \"MC#: Mixture Compressor for Mixture-of-Experts Large Models\" (Huang et al., 2025-10) refines the framework by combining Pre-Loading Mixed-Precision Quantization (PMQ) via Linear Programming with Online Top-any Pruning (OTP) using Gumbel-Softmax sampling. This unified approach achieves a 6.2x weight reduction on DeepSeek-VL2 with less than 2% accuracy loss, further demonstrating the efficacy of OR-driven compression.\n\nThis front is currently emerging, with two closely related papers published in 2025, the second building directly on the first. The trajectory suggests a continued focus on refining these OR-driven compression techniques. The likely next paper would explore the adaptation of these methods to new model architectures or more challenging, complex reasoning tasks, while also addressing hardware-specific optimizations.", "future_directions": ["Adapt the MC/MC# compression strategies for multimodal applications, such as Vision-Language Models (VLMs), to achieve similar memory and inference efficiency gains.", "Optimize the mixed-precision quantization and pruning algorithms for specific hardware platforms (e.g., edge devices, specialized AI accelerators) to maximize real-world deployment efficiency.", "Further improve performance on challenging reasoning tasks (e.g., GSM8K, HumanEval) and long-context scenarios, where current methods show more pronounced performance drops.", "Extend the Integer Linear Programming (ILP) formulations from MoE compression to other critical AI infrastructure problems, such as dynamic GPU scheduling or memory allocation for large-scale model serving.", "Investigate the application of differentiable routers (like Gumbel-Softmax) for dynamic agent selection and resource allocation in multi-agent AI systems beyond model compression."], "top_affiliations": [["The University of Hong Kong", 2], ["Beihang University", 2], ["The Chinese University of Hong Kong", 1], ["Centre for Perceptual and Interactive Intelligence", 1], ["Hong Kong", 1]], "method_overlap_with": {"or_for_generative_ai_2026-02-18_front_6": ["integer_programming"], "or_for_generative_ai_2026-02-18_front_0": ["linear_programming"], "or_for_generative_ai_2026-02-18_front_3": ["linear_programming"], "or_for_generative_ai_2026-02-18_front_32": ["linear_programming"]}}], "bridges": [{"paper_id": "2601.15738", "category": "LLMs for Algorithm Design", "snapshot_date": "2026-02-18", "home_front_id": "llms_for_algorithm_d_2026-02-18_front_7", "connected_fronts": ["llms_for_algorithm_d_2026-02-18_front_1", "llms_for_algorithm_d_2026-02-18_front_2"], "bridge_score": 0.6801, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2601.17899", "category": "LLMs for Algorithm Design", "snapshot_date": "2026-02-18", "home_front_id": "llms_for_algorithm_d_2026-02-18_front_7", "connected_fronts": ["llms_for_algorithm_d_2026-02-18_front_1", "llms_for_algorithm_d_2026-02-18_front_2"], "bridge_score": 0.6752, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2506.02049", "category": "LLMs for Algorithm Design", "snapshot_date": "2026-02-18", "home_front_id": "llms_for_algorithm_d_2026-02-18_front_5", "connected_fronts": ["llms_for_algorithm_d_2026-02-18_front_1"], "bridge_score": 0.6696, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2505.18602", "category": "LLMs for Algorithm Design", "snapshot_date": "2026-02-18", "home_front_id": "llms_for_algorithm_d_2026-02-18_front_5", "connected_fronts": ["llms_for_algorithm_d_2026-02-18_front_2", "llms_for_algorithm_d_2026-02-18_front_1"], "bridge_score": 0.6513, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2508.03082", "category": "LLMs for Algorithm Design", "snapshot_date": "2026-02-18", "home_front_id": "llms_for_algorithm_d_2026-02-18_front_7", "connected_fronts": ["llms_for_algorithm_d_2026-02-18_front_1", "llms_for_algorithm_d_2026-02-18_front_6", "llms_for_algorithm_d_2026-02-18_front_2", "llms_for_algorithm_d_2026-02-18_front_5"], "bridge_score": 0.6253, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2506.13131", "category": "LLMs for Algorithm Design", "snapshot_date": "2026-02-18", "home_front_id": "llms_for_algorithm_d_2026-02-18_front_6", "connected_fronts": ["llms_for_algorithm_d_2026-02-18_front_1", "llms_for_algorithm_d_2026-02-18_front_7", "llms_for_algorithm_d_2026-02-18_front_2"], "bridge_score": 0.5958, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2403.02985", "category": "LLMs for Algorithm Design", "snapshot_date": "2026-02-18", "home_front_id": "llms_for_algorithm_d_2026-02-18_front_5", "connected_fronts": ["llms_for_algorithm_d_2026-02-18_front_1", "llms_for_algorithm_d_2026-02-18_front_7"], "bridge_score": 0.564, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2502.14760", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "home_front_id": "generative_ai_for_or_2026-02-18_front_0", "connected_fronts": ["generative_ai_for_or_2026-02-18_front_1", "generative_ai_for_or_2026-02-18_front_14", "generative_ai_for_or_2026-02-18_front_6"], "bridge_score": 0.7615, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2403.01131", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "home_front_id": "generative_ai_for_or_2026-02-18_front_0", "connected_fronts": ["generative_ai_for_or_2026-02-18_front_1", "generative_ai_for_or_2026-02-18_front_14", "generative_ai_for_or_2026-02-18_front_6"], "bridge_score": 0.7541, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2407.19633", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "home_front_id": "generative_ai_for_or_2026-02-18_front_14", "connected_fronts": ["generative_ai_for_or_2026-02-18_front_1", "generative_ai_for_or_2026-02-18_front_0", "generative_ai_for_or_2026-02-18_front_6"], "bridge_score": 0.7387, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2506.06052", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "home_front_id": "generative_ai_for_or_2026-02-18_front_14", "connected_fronts": ["generative_ai_for_or_2026-02-18_front_6", "generative_ai_for_or_2026-02-18_front_1", "generative_ai_for_or_2026-02-18_front_0"], "bridge_score": 0.7019, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2507.11737", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "home_front_id": "generative_ai_for_or_2026-02-18_front_0", "connected_fronts": ["generative_ai_for_or_2026-02-18_front_1", "generative_ai_for_or_2026-02-18_front_14"], "bridge_score": 0.6957, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2508.10047", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "home_front_id": "generative_ai_for_or_2026-02-18_front_14", "connected_fronts": ["generative_ai_for_or_2026-02-18_front_1", "generative_ai_for_or_2026-02-18_front_0", "generative_ai_for_or_2026-02-18_front_6"], "bridge_score": 0.6697, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2506.07972", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "home_front_id": "generative_ai_for_or_2026-02-18_front_6", "connected_fronts": ["generative_ai_for_or_2026-02-18_front_1", "generative_ai_for_or_2026-02-18_front_0", "generative_ai_for_or_2026-02-18_front_14"], "bridge_score": 0.6538, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2508.03117", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "home_front_id": "generative_ai_for_or_2026-02-18_front_0", "connected_fronts": ["generative_ai_for_or_2026-02-18_front_1", "generative_ai_for_or_2026-02-18_front_14"], "bridge_score": 0.6495, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2506.07759", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "home_front_id": "generative_ai_for_or_2026-02-18_front_6", "connected_fronts": ["generative_ai_for_or_2026-02-18_front_1", "generative_ai_for_or_2026-02-18_front_0", "generative_ai_for_or_2026-02-18_front_14"], "bridge_score": 0.6417, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2411.17404", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "home_front_id": "generative_ai_for_or_2026-02-18_front_0", "connected_fronts": ["generative_ai_for_or_2026-02-18_front_1", "generative_ai_for_or_2026-02-18_front_14"], "bridge_score": 0.6191, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2410.22296", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "home_front_id": "generative_ai_for_or_2026-02-18_front_0", "connected_fronts": ["generative_ai_for_or_2026-02-18_front_1", "generative_ai_for_or_2026-02-18_front_14"], "bridge_score": 0.5714, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2504.04310", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "home_front_id": "generative_ai_for_or_2026-02-18_front_6", "connected_fronts": ["generative_ai_for_or_2026-02-18_front_1", "generative_ai_for_or_2026-02-18_front_0", "generative_ai_for_or_2026-02-18_front_14"], "bridge_score": 0.5662, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2504.16918", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "home_front_id": "generative_ai_for_or_2026-02-18_front_14", "connected_fronts": ["generative_ai_for_or_2026-02-18_front_1", "generative_ai_for_or_2026-02-18_front_0", "generative_ai_for_or_2026-02-18_front_6"], "bridge_score": 0.5313, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2510.04204", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "home_front_id": "generative_ai_for_or_2026-02-18_front_14", "connected_fronts": ["generative_ai_for_or_2026-02-18_front_1"], "bridge_score": 0.5271, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2601.17670", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "home_front_id": "generative_ai_for_or_2026-02-18_front_14", "connected_fronts": ["generative_ai_for_or_2026-02-18_front_1"], "bridge_score": 0.5251, "verdict": "TRUE SYNTHESIS"}, {"paper_id": "2509.22558", "category": "Generative AI for OR", "snapshot_date": "2026-02-18", "home_front_id": "generative_ai_for_or_2026-02-18_front_14", "connected_fronts": ["generative_ai_for_or_2026-02-18_front_1", "generative_ai_for_or_2026-02-18_front_0"], "bridge_score": 0.5012, "verdict": "TRUE SYNTHESIS"}], "genealogy": {"frameworks": {"adaptdl": {"parent": null, "children": [], "first_paper_date": "2026-02-01", "paper_count": 1, "latest_paper_date": "2026-02-01", "is_active": true}, "aflow": {"parent": null, "children": [], "first_paper_date": "2026-02-04", "paper_count": 1, "latest_paper_date": "2026-02-04", "is_active": true}, "agentic_python_coder": {"parent": null, "children": [], "first_paper_date": "2025-08-10", "paper_count": 1, "latest_paper_date": "2025-08-10", "is_active": false}, "alpa": {"parent": null, "children": [], "first_paper_date": "2025-08-26", "paper_count": 1, "latest_paper_date": "2025-08-26", "is_active": false}, "alphaevolve": {"parent": null, "children": [], "first_paper_date": "2025-06-16", "paper_count": 16, "latest_paper_date": "2026-02-10", "is_active": true}, "apf": {"parent": null, "children": [], "first_paper_date": "2025-12-21", "paper_count": 1, "latest_paper_date": "2025-12-21", "is_active": false}, "autoformulation": {"parent": null, "children": [], "first_paper_date": "2025-10-19", "paper_count": 1, "latest_paper_date": "2025-10-19", "is_active": false}, "autogen": {"parent": null, "children": [], "first_paper_date": "2024-02-15", "paper_count": 1, "latest_paper_date": "2024-02-15", "is_active": false}, "b4": {"parent": null, "children": [], "first_paper_date": "2024-09-13", "paper_count": 1, "latest_paper_date": "2024-09-13", "is_active": false}, "bilevel_optimization": {"parent": null, "children": [], "first_paper_date": "2024-10-29", "paper_count": 1, "latest_paper_date": "2024-10-29", "is_active": false}, "cae": {"parent": null, "children": [], "first_paper_date": "2025-03-13", "paper_count": 1, "latest_paper_date": "2025-03-13", "is_active": false}, "calm": {"parent": null, "children": [], "first_paper_date": "2025-10-05", "paper_count": 1, "latest_paper_date": "2025-10-05", "is_active": false}, "chain_of_experts": {"parent": null, "children": [], "first_paper_date": "2026-02-03", "paper_count": 1, "latest_paper_date": "2026-02-03", "is_active": true}, "cognitive_kernel_pro": {"parent": null, "children": [], "first_paper_date": "2026-01-23", "paper_count": 1, "latest_paper_date": "2026-01-23", "is_active": true}, "constraintllm": {"parent": null, "children": [], "first_paper_date": "2025-10-07", "paper_count": 1, "latest_paper_date": "2025-10-07", "is_active": false}, "controlled_decoding": {"parent": null, "children": [], "first_paper_date": "2025-03-11", "paper_count": 1, "latest_paper_date": "2025-03-11", "is_active": false}, "deepspeed_ulysses": {"parent": null, "children": [], "first_paper_date": "2025-02-11", "paper_count": 1, "latest_paper_date": "2025-02-11", "is_active": false}, "dgm": {"parent": null, "children": [], "first_paper_date": "2025-09-26", "paper_count": 1, "latest_paper_date": "2025-09-26", "is_active": false}, "discrete_transformer": {"parent": null, "children": [], "first_paper_date": "2026-01-09", "paper_count": 1, "latest_paper_date": "2026-01-09", "is_active": false}, "dplm": {"parent": null, "children": [], "first_paper_date": "2025-07-15", "paper_count": 1, "latest_paper_date": "2025-07-15", "is_active": false}, "dynamo": {"parent": null, "children": [], "first_paper_date": "2026-02-16", "paper_count": 1, "latest_paper_date": "2026-02-16", "is_active": true}, "eoh": {"parent": null, "children": [], "first_paper_date": "2024-06-01", "paper_count": 11, "latest_paper_date": "2026-02-09", "is_active": true}, "evolattice": {"parent": null, "children": [], "first_paper_date": "2025-12-17", "paper_count": 1, "latest_paper_date": "2025-12-17", "is_active": false}, "evolution_of_heuristics": {"parent": null, "children": [], "first_paper_date": "2024-03-05", "paper_count": 5, "latest_paper_date": "2025-08-05", "is_active": false}, "evolutionary_agentic_workflow": {"parent": null, "children": [], "first_paper_date": "2025-05-07", "paper_count": 1, "latest_paper_date": "2025-05-07", "is_active": false}, "evotune": {"parent": null, "children": [], "first_paper_date": "2025-08-04", "paper_count": 1, "latest_paper_date": "2025-08-04", "is_active": false}, "finemoe": {"parent": null, "children": [], "first_paper_date": "2026-01-15", "paper_count": 1, "latest_paper_date": "2026-01-15", "is_active": false}, "flow_matching": {"parent": null, "children": [], "first_paper_date": "2025-09-29", "paper_count": 1, "latest_paper_date": "2025-09-29", "is_active": false}, "funsearch": {"parent": null, "children": [], "first_paper_date": "2024-06-01", "paper_count": 17, "latest_paper_date": "2026-02-09", "is_active": true}, "global_resolution": {"parent": null, "children": [], "first_paper_date": "2025-11-19", "paper_count": 1, "latest_paper_date": "2025-11-19", "is_active": false}, "gptq": {"parent": null, "children": [], "first_paper_date": "2025-09-27", "paper_count": 1, "latest_paper_date": "2025-09-27", "is_active": false}, "gridcoder": {"parent": null, "children": [], "first_paper_date": "2025-09-21", "paper_count": 1, "latest_paper_date": "2025-09-21", "is_active": false}, "grpo": {"parent": null, "children": [], "first_paper_date": "2026-02-08", "paper_count": 1, "latest_paper_date": "2026-02-08", "is_active": true}, "guided_evolution": {"parent": null, "children": [], "first_paper_date": "2024-03-18", "paper_count": 1, "latest_paper_date": "2024-03-18", "is_active": false}, "hedrarag": {"parent": "rag", "children": [], "first_paper_date": "2026-01-19", "paper_count": 1, "latest_paper_date": "2026-01-19", "is_active": false}, "hgs": {"parent": null, "children": [], "first_paper_date": "2025-10-13", "paper_count": 1, "latest_paper_date": "2025-10-13", "is_active": false}, "knowledge_gradient": {"parent": null, "children": [], "first_paper_date": "2025-01-07", "paper_count": 1, "latest_paper_date": "2025-01-07", "is_active": false}, "lago": {"parent": null, "children": [], "first_paper_date": "2026-02-17", "paper_count": 1, "latest_paper_date": "2026-02-17", "is_active": true}, "lana": {"parent": null, "children": [], "first_paper_date": "2025-06-03", "paper_count": 1, "latest_paper_date": "2025-06-03", "is_active": false}, "linearizellm": {"parent": null, "children": [], "first_paper_date": "2025-10-12", "paper_count": 1, "latest_paper_date": "2025-10-12", "is_active": false}, "llamea": {"parent": null, "children": [], "first_paper_date": "2025-07-04", "paper_count": 3, "latest_paper_date": "2026-02-04", "is_active": true}, "llamo_co": {"parent": null, "children": [], "first_paper_date": "2024-03-02", "paper_count": 1, "latest_paper_date": "2024-03-02", "is_active": false}, "llm_driven_meta_optimizer": {"parent": null, "children": [], "first_paper_date": "2025-11-01", "paper_count": 1, "latest_paper_date": "2025-11-01", "is_active": false}, "llm_optimization_modeling": {"parent": null, "children": [], "first_paper_date": "2025-09-26", "paper_count": 1, "latest_paper_date": "2025-09-26", "is_active": false}, "llmlingua_2": {"parent": null, "children": [], "first_paper_date": "2024-12-11", "paper_count": 1, "latest_paper_date": "2024-12-11", "is_active": false}, "lmcache": {"parent": "mc", "children": [], "first_paper_date": "2026-01-19", "paper_count": 1, "latest_paper_date": "2026-01-19", "is_active": false}, "madevolve": {"parent": null, "children": [], "first_paper_date": "2026-02-17", "paper_count": 1, "latest_paper_date": "2026-02-17", "is_active": true}, "many_server_queueing_theory": {"parent": null, "children": [], "first_paper_date": "2026-02-03", "paper_count": 1, "latest_paper_date": "2026-02-03", "is_active": true}, "marine": {"parent": null, "children": [], "first_paper_date": "2025-12-05", "paper_count": 1, "latest_paper_date": "2025-12-05", "is_active": false}, "mc": {"parent": null, "children": ["lmcache", "mc#"], "first_paper_date": "2025-02-22", "paper_count": 1, "latest_paper_date": "2025-02-22", "is_active": false}, "mc#": {"parent": "mc", "children": [], "first_paper_date": "2025-10-13", "paper_count": 1, "latest_paper_date": "2025-10-13", "is_active": false}, "megatron_lm": {"parent": null, "children": [], "first_paper_date": "2025-01-15", "paper_count": 2, "latest_paper_date": "2025-02-10", "is_active": false}, "mind": {"parent": null, "children": [], "first_paper_date": "2026-01-17", "paper_count": 1, "latest_paper_date": "2026-01-17", "is_active": false}, "mlora": {"parent": null, "children": [], "first_paper_date": "2026-02-06", "paper_count": 1, "latest_paper_date": "2026-02-06", "is_active": true}, "nemo": {"parent": null, "children": [], "first_paper_date": "2026-01-29", "paper_count": 1, "latest_paper_date": "2026-01-29", "is_active": true}, "netgpt": {"parent": null, "children": [], "first_paper_date": "2025-11-27", "paper_count": 1, "latest_paper_date": "2025-11-27", "is_active": false}, "nvidia_mig": {"parent": null, "children": [], "first_paper_date": "2024-07-18", "paper_count": 1, "latest_paper_date": "2024-07-18", "is_active": false}, "openevolve": {"parent": null, "children": [], "first_paper_date": "2025-09-25", "paper_count": 1, "latest_paper_date": "2025-09-25", "is_active": false}, "optimus": {"parent": null, "children": [], "first_paper_date": "2024-07-29", "paper_count": 2, "latest_paper_date": "2025-09-28", "is_active": false}, "optmath": {"parent": null, "children": [], "first_paper_date": "2025-02-16", "paper_count": 1, "latest_paper_date": "2025-02-16", "is_active": false}, "or_instruct": {"parent": null, "children": [], "first_paper_date": "2025-04-04", "paper_count": 1, "latest_paper_date": "2025-04-04", "is_active": false}, "pama": {"parent": null, "children": [], "first_paper_date": "2025-08-11", "paper_count": 1, "latest_paper_date": "2025-08-11", "is_active": false}, "petals": {"parent": null, "children": [], "first_paper_date": "2025-12-26", "paper_count": 1, "latest_paper_date": "2025-12-26", "is_active": false}, "pppipe": {"parent": null, "children": [], "first_paper_date": "2025-12-25", "paper_count": 1, "latest_paper_date": "2025-12-25", "is_active": false}, "proopf": {"parent": null, "children": [], "first_paper_date": "2026-02-03", "paper_count": 1, "latest_paper_date": "2026-02-03", "is_active": true}, "proxywar": {"parent": null, "children": [], "first_paper_date": "2026-02-04", "paper_count": 1, "latest_paper_date": "2026-02-04", "is_active": true}, "qr_adaptor": {"parent": null, "children": [], "first_paper_date": "2026-01-05", "paper_count": 1, "latest_paper_date": "2026-01-05", "is_active": false}, "rag": {"parent": null, "children": ["hedrarag"], "first_paper_date": "2025-05-02", "paper_count": 1, "latest_paper_date": "2025-05-02", "is_active": false}, "rebase": {"parent": null, "children": [], "first_paper_date": "2025-06-11", "paper_count": 2, "latest_paper_date": "2025-10-20", "is_active": false}, "reevo": {"parent": null, "children": [], "first_paper_date": "2024-10-14", "paper_count": 4, "latest_paper_date": "2025-11-16", "is_active": false}, "reloop": {"parent": null, "children": [], "first_paper_date": "2026-02-17", "paper_count": 1, "latest_paper_date": "2026-02-17", "is_active": true}, "resocratic": {"parent": null, "children": [], "first_paper_date": "2024-07-13", "paper_count": 1, "latest_paper_date": "2024-07-13", "is_active": false}, "rideagent": {"parent": null, "children": [], "first_paper_date": "2025-05-10", "paper_count": 1, "latest_paper_date": "2025-05-10", "is_active": false}, "rltune": {"parent": null, "children": [], "first_paper_date": "2025-12-11", "paper_count": 1, "latest_paper_date": "2025-12-11", "is_active": false}, "roll": {"parent": null, "children": [], "first_paper_date": "2025-12-15", "paper_count": 1, "latest_paper_date": "2025-12-15", "is_active": false}, "safety_game": {"parent": null, "children": [], "first_paper_date": "2025-12-02", "paper_count": 1, "latest_paper_date": "2025-12-02", "is_active": false}, "sarathi_serve": {"parent": null, "children": [], "first_paper_date": "2025-12-01", "paper_count": 2, "latest_paper_date": "2025-12-17", "is_active": false}, "self_developing": {"parent": null, "children": [], "first_paper_date": "2025-06-10", "paper_count": 1, "latest_paper_date": "2025-06-10", "is_active": false}, "sglang": {"parent": null, "children": [], "first_paper_date": "2026-02-03", "paper_count": 1, "latest_paper_date": "2026-02-03", "is_active": true}, "sirl": {"parent": null, "children": [], "first_paper_date": "2025-05-17", "paper_count": 1, "latest_paper_date": "2025-05-17", "is_active": false}, "skypilot": {"parent": null, "children": [], "first_paper_date": "2026-01-10", "paper_count": 1, "latest_paper_date": "2026-01-10", "is_active": false}, "staggered_batch_scheduling": {"parent": null, "children": [], "first_paper_date": "2025-12-18", "paper_count": 1, "latest_paper_date": "2025-12-18", "is_active": false}, "steporlm": {"parent": null, "children": [], "first_paper_date": "2025-09-26", "paper_count": 1, "latest_paper_date": "2025-09-26", "is_active": false}, "thetaevolve": {"parent": null, "children": [], "first_paper_date": "2026-01-22", "paper_count": 1, "latest_paper_date": "2026-01-22", "is_active": true}, "tree_of_thought": {"parent": null, "children": [], "first_paper_date": "2024-11-26", "paper_count": 1, "latest_paper_date": "2024-11-26", "is_active": false}, "twill": {"parent": null, "children": [], "first_paper_date": "2025-12-19", "paper_count": 1, "latest_paper_date": "2025-12-19", "is_active": false}, "verl": {"parent": null, "children": [], "first_paper_date": "2025-12-13", "paper_count": 1, "latest_paper_date": "2025-12-13", "is_active": false}, "vllm": {"parent": null, "children": [], "first_paper_date": "2025-02-14", "paper_count": 1, "latest_paper_date": "2025-02-14", "is_active": false}, "wl_test_for_milp_graphs": {"parent": null, "children": [], "first_paper_date": "2025-10-31", "paper_count": 1, "latest_paper_date": "2025-10-31", "is_active": false}}, "wordcloud_data": [{"text": "funsearch", "weight": 17, "is_active": true, "must_read_ratio": 0.706, "must_read_count": 12}, {"text": "alphaevolve", "weight": 16, "is_active": true, "must_read_ratio": 1.0, "must_read_count": 16}, {"text": "eoh", "weight": 11, "is_active": true, "must_read_ratio": 0.818, "must_read_count": 9}, {"text": "evolution_of_heuristics", "weight": 5, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 5}, {"text": "reevo", "weight": 4, "is_active": false, "must_read_ratio": 0.5, "must_read_count": 2}, {"text": "llamea", "weight": 3, "is_active": true, "must_read_ratio": 1.0, "must_read_count": 3}, {"text": "optimus", "weight": 2, "is_active": false, "must_read_ratio": 0.5, "must_read_count": 1}, {"text": "sarathi_serve", "weight": 2, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 2}, {"text": "rebase", "weight": 2, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 2}, {"text": "megatron_lm", "weight": 2, "is_active": false, "must_read_ratio": 0.5, "must_read_count": 1}, {"text": "madevolve", "weight": 1, "is_active": true, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "lago", "weight": 1, "is_active": true, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "proxywar", "weight": 1, "is_active": true, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "cognitive_kernel_pro", "weight": 1, "is_active": true, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "thetaevolve", "weight": 1, "is_active": true, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "discrete_transformer", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "evolattice", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "hgs", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "dgm", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "openevolve", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "gridcoder", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "evotune", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "self_developing", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "cae", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "guided_evolution", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "reloop", "weight": 1, "is_active": true, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "chain_of_experts", "weight": 1, "is_active": true, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "proopf", "weight": 1, "is_active": true, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "nemo", "weight": 1, "is_active": true, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "mind", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "apf", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "llm_driven_meta_optimizer", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "wl_test_for_milp_graphs", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "autoformulation", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "linearizellm", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "calm", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "steporlm", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "llm_optimization_modeling", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "agentic_python_coder", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "dplm", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "sirl", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "rideagent", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "evolutionary_agentic_workflow", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "rag", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "optmath", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "tree_of_thought", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "bilevel_optimization", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "resocratic", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "llamo_co", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "autogen", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "dynamo", "weight": 1, "is_active": true, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "grpo", "weight": 1, "is_active": true, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "mlora", "weight": 1, "is_active": true, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "aflow", "weight": 1, "is_active": true, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "many_server_queueing_theory", "weight": 1, "is_active": true, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "sglang", "weight": 1, "is_active": true, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "adaptdl", "weight": 1, "is_active": true, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "lmcache", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "hedrarag", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "finemoe", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "skypilot", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "qr_adaptor", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "petals", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "pppipe", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "twill", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "staggered_batch_scheduling", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "roll", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "verl", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "rltune", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "marine", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "safety_game", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "netgpt", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "global_resolution", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "mc#", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "constraintllm", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "flow_matching", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "gptq", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "alpa", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "pama", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "lana", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "or_instruct", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "controlled_decoding", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "mc", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "vllm", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "deepspeed_ulysses", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}, {"text": "knowledge_gradient", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "llmlingua_2", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "b4", "weight": 1, "is_active": false, "must_read_ratio": 1.0, "must_read_count": 1}, {"text": "nvidia_mig", "weight": 1, "is_active": false, "must_read_ratio": 0.0, "must_read_count": 0}], "summary": {"total_frameworks": 89, "root_frameworks": 86, "active_last_30d": 20, "top_frameworks": [["funsearch", 17], ["alphaevolve", 16], ["eoh", 11], ["evolution_of_heuristics", 5], ["reevo", 4]], "newest_frameworks": [["lago", "2026-02-17"], ["madevolve", "2026-02-17"], ["reloop", "2026-02-17"], ["dynamo", "2026-02-16"], ["grpo", "2026-02-08"]], "roots": ["adaptdl", "aflow", "agentic_python_coder", "alpa", "alphaevolve", "apf", "autoformulation", "autogen", "b4", "bilevel_optimization", "cae", "calm", "chain_of_experts", "cognitive_kernel_pro", "constraintllm", "controlled_decoding", "deepspeed_ulysses", "dgm", "discrete_transformer", "dplm", "dynamo", "eoh", "evolattice", "evolution_of_heuristics", "evolutionary_agentic_workflow", "evotune", "finemoe", "flow_matching", "funsearch", "global_resolution", "gptq", "gridcoder", "grpo", "guided_evolution", "hgs", "knowledge_gradient", "lago", "lana", "linearizellm", "llamea", "llamo_co", "llm_driven_meta_optimizer", "llm_optimization_modeling", "llmlingua_2", "madevolve", "many_server_queueing_theory", "marine", "mc", "megatron_lm", "mind", "mlora", "nemo", "netgpt", "nvidia_mig", "openevolve", "optimus", "optmath", "or_instruct", "pama", "petals", "pppipe", "proopf", "proxywar", "qr_adaptor", "rag", "rebase", "reevo", "reloop", "resocratic", "rideagent", "rltune", "roll", "safety_game", "sarathi_serve", "self_developing", "sglang", "sirl", "skypilot", "staggered_batch_scheduling", "steporlm", "thetaevolve", "tree_of_thought", "twill", "verl", "vllm", "wl_test_for_milp_graphs"]}}, "category_tables": {"LLMs for Algorithm Design": [{"arxiv_id": "2602.16038", "arxiv_url": "https://arxiv.org/abs/2602.16038", "title": "Heuristic Search as Language-Guided Program Optimization", "authors_short": "Mingxin Yu et.al.", "published_date": "2026-02-17", "m_score": 9, "p_score": 10, "i_score": 9, "priority_score": 9.0, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_in_the_loop", "llm_as_evaluator"], "problems": ["combinatorial_optimization", "pickup_and_delivery_problem_with_time_windows", "airline_crew_pairing_problem", "technology_mapping_problem", "intra_operator_parallelism_scheduling"], "code_url": null, "brief": "LaGO decomposes automated heuristic design into three explicit modules: evaluation, a code-writing 'Analyst' (backward pass), and a diversity-aware 'Generator' (update), while co-evolving constructive and refinement heuristics. The authors demonstrate significant gains (+0.17 QYI) on PDPTW and Crew Pairing against ReEvo and EoH, showing that joint optimization of initialization and improvement prevents local optima. The critical takeaway is the 'Analyst' module: instead of asking the LLM for text critiques, they ask it to write Python feature extraction functions to statistically characterize solution quality—a technique we should immediately adopt to upgrade our fitness signals in AlgoEvo.", "affiliations": "Massachusetts Institute of Technology", "analysis_date": "2026-02-19"}, {"arxiv_id": "2602.15951", "arxiv_url": "https://arxiv.org/abs/2602.15951", "title": "MadEvolve: Evolutionary Optimization of Cosmological Algorithms with Large Language Models", "authors_short": "Tianyi Li et.al.", "published_date": "2026-02-17", "m_score": 9, "p_score": 6, "i_score": 9, "priority_score": 8.0, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["llm_as_heuristic", "llm_code_generation", "llm_as_evaluator", "llm_evolutionary_search", "evolution_of_heuristics"], "problems": ["cosmological_initial_condition_reconstruction", "bao_reconstruction", "21cm_foreground_reconstruction", "baryonic_physics_approximation", "tsz_prediction"], "code_url": "https://github.com/tianyi-stack/MadEvolve-Cosmo", "brief": "MadEvolve extends AlphaEvolve by embedding a gradient-based optimization loop (via JAX) inside the fitness evaluation, allowing the LLM to focus purely on code structure while an optimizer (Adam) handles continuous parameters. They demonstrate 20-30% performance gains on complex cosmological reconstruction tasks, validated on held-out simulations. The critical takeaway is the architectural pattern: prompt the LLM to write differentiable code rather than tuning constants, and use a UCB1 bandit to dynamically select between cheap and expensive models. We should immediately adopt the differentiable inner-loop strategy for our continuous heuristic search projects.", "affiliations": "University of Wisconsin-Madison", "analysis_date": "2026-02-19"}, {"arxiv_id": "2602.10233", "arxiv_url": "https://arxiv.org/abs/2602.10233", "title": "ImprovEvolve: Ask AlphaEvolve to Improve the Input Solution and Then Improvise", "authors_short": "Alexey Kravatskiy et.al.", "published_date": "2026-02-10", "m_score": 8, "p_score": 5, "i_score": 8, "priority_score": 7.25, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["llm_evolutionary_search", "alphaevolve", "map_elites", "basin_hopping", "l_bfgs_b"], "problems": ["hexagon_packing", "second_autocorrelation_inequality", "packing", "functional_optimization"], "code_url": null, "brief": "Kravatskiy et al. introduce ImprovEvolve, a framework that restricts the LLM to evolving `improve()` (local search) and `perturb()` (mutation) operators, which are then executed by a fixed basin-hopping algorithm. They achieve new state-of-the-art results on Hexagon Packing and the Second Autocorrelation Inequality, demonstrating that this modular approach generalizes to unseen problem sizes where monolithic AlphaEvolve solutions fail. The critical insight is that LLMs are poor at designing global search logic and tuning hyperparameters (LLM edits actively harmed performance), so we should isolate the LLM to generating local moves while keeping the meta-heuristic framework deterministic. We should immediately apply this 'operator-only' evolution strategy to our ALNS research for VRP.", "affiliations": "MIRIAI, FusionBrain Lab, Institute of Numerical Mathematics", "analysis_date": "2026-02-17"}, {"arxiv_id": "2601.22896", "arxiv_url": "https://arxiv.org/abs/2601.22896", "title": "Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery", "authors_short": "Xinyi Ke et.al.", "published_date": "2026-02-09", "m_score": 9, "p_score": 9, "i_score": 9, "priority_score": 8.69, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["game_theory", "zero_sum_game", "policy_space_response_oracles", "algorithm_space_response_oracles", "llm_code_generation"], "problems": ["automated_heuristic_discovery", "combinatorial_optimization", "online_bin_packing", "traveling_salesman_problem", "capacitated_vehicle_routing_problem"], "code_url": null, "brief": "ASRO adapts Policy Space Response Oracles (PSRO) to code generation, treating heuristic discovery as a zero-sum game where a 'Solver' evolves to minimize gaps and a 'Generator' evolves to create adversarial instances. The results are compelling: it consistently beats the static EoH baseline on TSPLIB and CVRPLIB, proving that adversarial training yields better generalization than training on fixed distributions. The critical takeaway is the architecture: explicitly co-evolving an 'Instance Generator' program alongside the solver prevents overfitting and exposes edge cases (like specific geometric traps in TSP) that static benchmarks miss. This is a direct upgrade to our AlgoEvo/AlphaEvolve pipelines, though it incurs higher computational costs due to the evaluation matrix required for the meta-game.", "affiliations": "Tsinghua University, Chinese Academy of Sciences, University of Chinese Academy of Sciences, AiRiA", "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.08253", "arxiv_url": "https://arxiv.org/abs/2602.08253", "title": "G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design", "authors_short": "Baoyun Zhao et.al.", "published_date": "2026-02-09", "m_score": 8, "p_score": 9, "i_score": 8, "priority_score": 8.24, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_name": "Enhanced LLM Evolutionary Search via Concept Learning and Co-Evolution", "front_status": "stable", "methods": ["large_neighborhood_search", "adaptive_large_neighborhood_search", "evolutionary_algorithm", "llm_code_generation", "llm_as_variation_operator"], "problems": ["traveling_salesman_problem", "capacitated_vehicle_routing_problem", "open_vehicle_routing_problem", "combinatorial_routing", "operator_discovery"], "code_url": "https://github.com/ZBoyn/G-LNS", "brief": "G-LNS extends LLM-based evolutionary search to ALNS by co-evolving Python code for Destroy and Repair operators rather than constructive priority rules. The authors introduce a 'Synergy Matrix' that tracks the performance of specific operator pairs during evaluation, using this data to guide a 'Synergistic Joint Crossover' where the LLM optimizes the coupling between destroy and repair logic. Results are strong: it significantly outperforms FunSearch and EoH on TSP/CVRP and beats OR-Tools on large-scale instances (N=200) under time constraints. The key takeaway for AlgoEvo is the synergy-aware co-evolution mechanism—explicitly tracking and prompting for component interaction is a concrete technique we can apply to multi-agent optimization systems.", "affiliations": "Tsinghua University, University of Chinese Academy of Sciences, Northeastern University", "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.05688", "arxiv_url": "https://arxiv.org/abs/2602.05688", "title": "Mining Generalizable Activation Functions", "authors_short": "Alex Vitvitskyi et.al.", "published_date": "2026-02-05", "m_score": 8, "p_score": 5, "i_score": 8, "priority_score": 6.55, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["evolutionary_search", "alphaevolve", "llm_evolutionary_search", "llm_code_generation", "multilayer_perceptron"], "problems": ["activation_function_search", "operator_discovery", "out_of_distribution_generalization", "image_classification", "algorithmic_reasoning"], "code_url": "https://github.com/Aastha2104/Parkinson-Disease-Prediction", "brief": "Vitvitskyi et al. (DeepMind) utilize AlphaEvolve to discover novel activation functions by evolving Python code on small, synthetic datasets explicitly designed to test OOD generalization (e.g., polynomials, Feynman equations). The results are credible and backed by downstream transfer: discovered functions like `GELU * (1 + 0.5 sinc(x))` outperform baselines on algorithmic reasoning tasks (CLRS-30) while matching standard vision benchmarks. **Key Takeaway:** The 'Small-Scale Lab' methodology—optimizing on cheap, synthetic proxy tasks to find generalizable logic—is a validated strategy to bypass the computational bottleneck of evaluating evolved candidates on large-scale instances. We should steal this 'proxy evolution' setup for AlgoEvo to drastically reduce evaluation costs while targeting generalization in VRP heuristics.", "affiliations": "Google DeepMind", "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.04296", "arxiv_url": "https://arxiv.org/abs/2602.04296", "title": "ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas", "authors_short": "Wenjun Peng et.al.", "published_date": "2026-02-04", "m_score": 5, "p_score": 7, "i_score": 7, "priority_score": 4.98, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["llm_code_generation", "llm_as_agent", "llm_in_the_loop", "iterative_repair", "multi_agent_systems"], "problems": ["llm_code_generation_evaluation", "game_ai", "sudoku", "2048", "tower_of_hanoi"], "code_url": "https://github.com/xinke-wang/ProxyWar", "brief": "ProxyWar introduces a tournament-based evaluation framework for LLM-generated code, using TrueSkill ratings from game simulations (Sudoku, Poker, etc.) instead of static unit tests. The results are robust (10k+ matches) and reveal a low correlation between Pass@1 and actual win rates; notably, 'reasoning' models like DeepSeek-R1 crush 'coding' models like Qwen-Coder in strategic tasks despite lower static scores. For our evolutionary search work, this confirms that we must move beyond static benchmarks to dynamic, competitive evaluation signals to avoid optimizing for syntax over strategy. We should also prioritize reasoning models over code-specialized ones for our agentic logic generation.", "affiliations": "", "analysis_date": "2026-02-13"}, {"arxiv_id": "2602.04529", "arxiv_url": "https://arxiv.org/abs/2602.04529", "title": "Landscape-aware Automated Algorithm Design: An Efficient Framework for Real-world Optimization", "authors_short": "Haoran Yin et.al.", "published_date": "2026-02-04", "m_score": 8, "p_score": 7, "i_score": 9, "priority_score": 7.86, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_5", "front_name": "Advanced LLM-Driven Algorithm Evolution via Behavioral and Structural Guidance", "front_status": "stable", "methods": ["genetic_programming", "evolutionary_algorithm", "llm_evolutionary_search", "llamea", "exploratory_landscape_analysis"], "problems": ["automated_algorithm_design", "expensive_continuous_optimization", "black_box_optimization"], "code_url": "10.5281/zenodo.18385405", "brief": "Yin et al. introduce a framework that decouples algorithm discovery from expensive evaluations by using Genetic Programming to evolve symbolic proxy functions that statistically match the target problem's landscape (via ELA features). Empirical results on photonics problems confirm that algorithms evolved on these cheap proxies transfer successfully to the real tasks, outperforming standard baselines like LSHADE with only 50×D real evaluations. **Key Takeaway:** We can synthesize 'symbolic gyms' that statistically mimic our target problems to run thousands of LLM iterations at near-zero cost. This directly addresses the sample efficiency bottleneck in AlgoEvo and suggests we should move beyond standard neural surrogates to evolved symbolic proxies.", "affiliations": "", "analysis_date": "2026-02-12"}, {"arxiv_id": "2602.03545", "arxiv_url": "https://arxiv.org/abs/2602.03545", "title": "Persona Generators: Generating Diverse Synthetic Personas at Scale", "authors_short": "Davide Paglieri et.al.", "published_date": "2026-02-03", "m_score": 8, "p_score": 3, "i_score": 8, "priority_score": 6.03, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_in_the_loop", "alphaevolve"], "problems": ["synthetic_persona_generation", "synthetic_data_generation", "diversity_maximization", "support_coverage", "stress_testing_ai_systems"], "code_url": "https://github.com/akhatua2/synthpersona", "brief": "Paglieri et al. (DeepMind) apply AlphaEvolve to optimize Python code that generates synthetic personas, explicitly maximizing diversity metrics (convex hull, coverage) in embedding space rather than just fidelity. They achieve >80% coverage of the behavioral space compared to <50% for baselines, proving that evolving the *generator function* is more effective than prompting for diversity. The key takeaway is their two-stage architecture (autoregressive high-level trait generation $\\to$ parallel detail expansion), which we should steal to evolve 'Solution Generators' for VRP/OR that inherently resist mode collapse. This validates our direction with AlgoEvo but offers a concrete architectural pattern for maintaining population diversity.", "affiliations": "Google DeepMind", "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.03132", "arxiv_url": "https://arxiv.org/abs/2602.03132", "title": "Contrastive Concept-Tree Search for LLM-Assisted Algorithm Discovery", "authors_short": "Timothee Leleu et.al.", "published_date": "2026-02-03", "m_score": 9, "p_score": 8, "i_score": 9, "priority_score": 8.4, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_name": "Enhanced LLM Evolutionary Search via Concept Learning and Co-Evolution", "front_status": "stable", "methods": ["llm_as_heuristic", "llm_code_generation", "evolution_of_heuristics", "program_synthesis", "self_improving_search"], "problems": ["algorithm_discovery", "heuristic_evolution", "combinatorial_optimization", "circle_packing_problem", "arithmetic_kakeya_conjecture"], "code_url": null, "brief": "The authors introduce Contrastive Concept-Tree Search (CCTS), which modifies the standard evolutionary loop by prompting the LLM to extract semantic 'concepts' from every generated program, building a dynamic hierarchy. They then apply a Tree-structured Parzen Estimator (TPE) to these concepts to learn a contrastive utility model (p(concept|good)/p(concept|bad)), using this to bias parent selection towards promising algorithmic strategies. Results are rigorous, showing consistent improvements over k-elite baselines on combinatorial tasks like Circle Packing, with a synthetic ablation confirming the model learns ground-truth concept utilities. **Key Takeaway:** We should immediately implement the 'Concept TPE' loop in AlgoEvo—asking the LLM to tag generated heuristics with concepts and maintaining a weight vector over these concepts provides a cheap, interpretable 'process reward model' to guide search.", "affiliations": "", "analysis_date": "2026-02-13"}, {"arxiv_id": "2602.02724", "arxiv_url": "https://arxiv.org/abs/2602.02724", "title": "Automatic Design of Optimization Test Problems with Large Language Models", "authors_short": "Wojciech Achtelik et.al.", "published_date": "2026-02-02", "m_score": 5, "p_score": 9, "i_score": 7, "priority_score": 6.48, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["llm_evolutionary_search", "eoh", "exploratory_landscape_analysis", "llm_code_generation", "program_synthesis"], "problems": ["test_function_generation", "continuous_optimization", "black_box_optimization", "benchmark_design"], "code_url": "https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020", "brief": "Achtelik et al. adapt LLM-driven evolutionary search (EoH) to generate interpretable Python functions that match specific landscape features (ELA), effectively creating synthetic benchmarks on demand. Unlike prior neural network approaches that fail to scale, this method performs robustly in higher dimensions (3D-5D) and produces portable code. The key takeaway is the capability to procedurally generate 'hard' or specific-property instances; we should immediately adopt this to create a dynamic training curriculum for AlgoEvo, ensuring our evolved metaheuristics generalize beyond standard libraries like BBOB.", "affiliations": "AGH University of Krakow, Warsaw University of Technology", "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.02919", "arxiv_url": "https://arxiv.org/abs/2602.02919", "title": "DeltaEvolve: Accelerating Scientific Discovery through Momentum-Driven Evolution", "authors_short": "Jiachen Jiang et.al.", "published_date": "2026-02-02", "m_score": 9, "p_score": 10, "i_score": 9, "priority_score": 8.9, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["llm_evolutionary_search", "llm_code_generation", "llm_as_heuristic", "expectation_maximization", "semantic_delta"], "problems": ["black_box_optimization", "hexagon_packing", "symbolic_regression", "pde_solver", "efficient_convolution"], "code_url": null, "brief": "DeltaEvolve replaces the standard full-code history in evolutionary search with 'semantic deltas'—structured text summaries capturing the 'from/to' logic of modifications and their hypotheses. Across 5 domains (including BBOB and Symbolic Regression), they demonstrate superior objective scores over AlphaEvolve while reducing token consumption by ~37%. The critical takeaway is the 'Progressive Disclosure' mechanism: treating history as a momentum vector (deltas) rather than a state archive (snapshots) allows us to fit a deeper evolutionary trajectory into the context window. We should immediately test their 'Delta Plan' prompt structure in AlgoEvo to improve sample efficiency and reduce costs.", "affiliations": "Microsoft, The Ohio State University", "analysis_date": "2026-02-17"}, {"arxiv_id": "2601.17899", "arxiv_url": "https://arxiv.org/abs/2601.17899", "title": "Evolving Interdependent Operators with Large Language Models for Multi-Objective Combinatorial Optimization", "authors_short": "Junhao Qiu et.al.", "published_date": "2026-02-01", "m_score": 8, "p_score": 8, "i_score": 8, "priority_score": 8.14, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["monte_carlo_tree_search", "llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "evolutionary_algorithms"], "problems": ["heuristic_evolution", "operator_discovery", "job_shop_scheduling", "tsp", "multi_objective_combinatorial_optimization"], "code_url": "null", "brief": "E2OC introduces a hierarchical search framework where MCTS optimizes 'design thoughts' (textual strategies) rather than raw code, subsequently using these strategies to guide a coordinate-descent-style evolution of interdependent operators. While the computational cost is high due to the inner-loop operator rotation, the results on FJSP/TSP (+20% HV vs expert) and comparisons against FunSearch/EoH demonstrate that explicitly modeling operator coupling is superior to isolated evolution. The critical takeaway for us is the **'strategy-first' search layer**: evolving a semantic blueprint for component interaction *before* code generation prevents the local optima trap of independent component optimization, a technique we should immediately test in AlgoEvo.", "affiliations": "", "analysis_date": "2026-02-13"}, {"arxiv_id": "2601.21239", "arxiv_url": "https://arxiv.org/abs/2601.21239", "title": "TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design", "authors_short": "Chentong Chen et.al.", "published_date": "2026-01-29", "m_score": 9, "p_score": 10, "i_score": 8, "priority_score": 8.72, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["evolutionary_algorithm", "island_model_ea", "co_evolution", "differential_evolution", "llm_as_heuristic"], "problems": ["heuristic_design", "tsp", "knapsack", "bin_packing", "assignment_problem"], "code_url": null, "brief": "TIDE introduces a nested evolutionary framework that strictly decouples algorithmic structure generation (via LLM) from numerical parameter tuning (via Differential Evolution), managed by a Tree Similarity Edit Distance (TSED) guided island model. Results on 9 COPs (TSP, BPP, etc.) show it consistently outperforms ReEvo and EoH, primarily because the DE layer optimizes constants at zero token cost, preventing the discard of structurally sound but poorly tuned heuristics. The critical takeaway is the necessity of a gradient-free tuning layer for LLM-generated code; relying on LLMs for numerical constants is inefficient and imprecise. We should immediately implement a similar parameter-tuning inner loop in our AlgoEvo framework.", "affiliations": "", "analysis_date": "2026-02-13"}, {"arxiv_id": "2601.20539", "arxiv_url": "https://arxiv.org/abs/2601.20539", "title": "PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs", "authors_short": "Oguzhan Gungordu et.al.", "published_date": "2026-01-29", "m_score": 8, "p_score": 8, "i_score": 8, "priority_score": 7.92, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["multi_agent_reasoning", "llm_evolutionary_search", "evolution_of_heuristics", "llm_as_heuristic", "llm_code_generation"], "problems": ["automated_heuristic_design", "heuristic_evolution", "operator_discovery", "tsp", "knapsack_problem"], "code_url": null, "brief": "PathWise reformulates heuristic discovery as a sequential planning problem over an 'Entailment Graph,' where a Policy Agent generates high-level evolutionary directives (rationales) and a World Model executes the code, guided by specific Critic reflections. The results are robust: it outperforms ReEvo, FunSearch, and MCTS-AHD on TSP, CVRP, and Bin Packing while using half the evaluation budget (500 vs 1000), demonstrating genuine sample efficiency. The key takeaway is the **Entailment Graph** structure: explicitly storing the *derivation rationale* and lineage allows the LLM to reason about the search trajectory and avoid redundant failures, a mechanism we should immediately adapt for AlgoEvo to fix our memory bottleneck.", "affiliations": "", "analysis_date": "2026-02-13"}, {"arxiv_id": "2601.21511", "arxiv_url": "https://arxiv.org/abs/2601.21511", "title": "LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI", "authors_short": "Niki van Stein et.al.", "published_date": "2026-01-29", "m_score": 8, "p_score": 9, "i_score": 9, "priority_score": 8.32, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_5", "front_name": "Advanced LLM-Driven Algorithm Evolution via Behavioral and Structural Guidance", "front_status": "stable", "methods": ["llm_evolutionary_search", "llm_code_generation", "shap_analysis", "explainable_ai", "abstract_syntax_tree_analysis"], "problems": ["black_box_optimization", "continuous_optimization", "heuristic_evolution"], "code_url": "https://anonymous.4open.science/r/LLaMEA-SAGE/README.md", "brief": "LLaMEA-SAGE augments LLM-based evolutionary search by extracting AST features (complexity, graph metrics) from generated code, training a surrogate model to predict fitness from these features, and using SHAP analysis to generate natural language prompts that guide the LLM to modify specific structural properties (e.g., 'increase cyclomatic complexity'). On the MA-BBOB benchmark, it outperforms state-of-the-art methods (MCTS-AHD, LHNS) and converges faster than vanilla LLaMEA, although the authors honestly report that statistical significance was limited (p=0.44) due to small sample sizes (5 runs). The critical takeaway for us is the pipeline of using static code analysis as a feedback signal—we can immediately steal this 'SAGE' loop to guide AlgoEvo or EvoCut by telling the LLM *how* to structurally mutate code based on surrogate correlations, rather than just hoping for random improvements.", "affiliations": "", "analysis_date": "2026-02-13"}, {"arxiv_id": "2601.21847", "arxiv_url": "https://arxiv.org/abs/2601.21847", "title": "READY: Reward Discovery for Meta-Black-Box Optimization", "authors_short": "Zechuan Huang et.al.", "published_date": "2026-01-29", "m_score": 8, "p_score": 8, "i_score": 8, "priority_score": 7.25, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["llm_evolutionary_search", "program_synthesis", "multi_task_learning", "evolutionary_algorithms", "knowledge_transfer"], "problems": ["reward_discovery", "meta_black_box_optimization", "black_box_optimization", "program_synthesis"], "code_url": "https://anonymous.4open.science/r/ICML_READY-747F", "brief": "READY introduces a multi-task evolutionary framework where LLMs evolve reward functions for multiple MetaBBO algorithms simultaneously, utilizing explicit 'Knowledge Transfer' operators to translate successful logic between distinct tasks. The results are robust, demonstrating superior performance over Eureka and EoH on BBOB benchmarks with a 2-4x reduction in search time due to parallelization and shared heuristics. The most stealable insights are the 'History-Reflection' operator—which prompts the LLM to extrapolate trends from the evolutionary trajectory rather than just mutating the current state—and the cross-niche transfer mechanism, both of which should be implemented in our multi-agent optimization stack immediately.", "affiliations": "", "analysis_date": "2026-02-13"}, {"arxiv_id": "2601.21096", "arxiv_url": "https://arxiv.org/abs/2601.21096", "title": "Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve", "authors_short": "Hongzheng Chen et.al.", "published_date": "2026-01-28", "m_score": 8, "p_score": 7, "i_score": 9, "priority_score": 7.81, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["llm_code_generation", "evolutionary_search", "autotuning", "alphaevolve", "program_synthesis"], "problems": ["compiler_optimization_heuristics_discovery", "function_inlining", "register_allocation", "e_graph_extraction", "auto_sharding"], "code_url": null, "brief": "Magellan couples AlphaEvolve with a black-box autotuner (Vizier) to evolve C++ compiler heuristics, achieving >5% binary size reduction in LLVM and beating both human experts and prior neural policies. The results are rigorous, validated on production workloads and showing temporal generalization. **The critical takeaway is the 'Hierarchical Search' strategy:** rather than asking the LLM to write fully specified code, they prompt it to generate *templates* with exposed parameters (flags), delegating numerical tuning to a cheap external optimizer. This directly addresses the sample efficiency issues we face in AlgoEvo; we should immediately steal this architecture to separate structural evolution from parameter tuning.", "affiliations": "Google DeepMind, Google, Cornell University", "analysis_date": "2026-02-17"}, {"arxiv_id": "2601.19793", "arxiv_url": "https://arxiv.org/abs/2601.19793", "title": "CASTER: Breaking the Cost-Performance Barrier in Multi-Agent Orchestration via Context-Aware Strategy for Task Efficient Routing", "authors_short": "Shanyv Liu et.al.", "published_date": "2026-01-27", "m_score": 6, "p_score": 8, "i_score": 7, "priority_score": 5.48, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["neural_networks", "feature_fusion", "text_embeddings", "supervised_learning", "on_policy_learning"], "problems": ["llm_resource_allocation", "cost_optimization", "multi_agent_systems", "software_engineering", "data_analysis"], "code_url": null, "brief": "CASTER implements a context-aware neural router for multi-agent systems that dynamically selects between weak and strong models, reducing inference costs by ~72% compared to a GPT-4o-only baseline. The authors validate this on a custom benchmark across four domains, showing it outperforms cascading strategies (FrugalGPT) by avoiding the 'double-billing' of failed weak calls. The standout takeaway for us is the 'On-Policy Negative Feedback' mechanism: training the router by explicitly relabeling instances where the weak model failed as 'Strong-Required'. We should adapt this active learning logic to train our proxy reward models in AlgoEvo, allowing us to reliably offload expensive evaluations to cheaper proxies without manual annotation.", "affiliations": "University of Houston, China University of Petroleum (East China), Southwest Jiaotong University", "analysis_date": "2026-02-17"}, {"arxiv_id": "2601.19622", "arxiv_url": "https://arxiv.org/abs/2601.19622", "title": "Algorithmic Prompt-Augmentation for Efficient LLM-Based Heuristic Design for A* Search", "authors_short": "Thomas Bömer et.al.", "published_date": "2026-01-27", "m_score": 7, "p_score": 5, "i_score": 8, "priority_score": 5.18, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["evolution_of_heuristics", "llm_evolutionary_search", "llm_as_heuristic", "llm_code_generation", "llm_prompt_optimization"], "problems": ["unit_load_pre_marshalling_problem", "sliding_puzzle_problem", "heuristic_evolution", "operator_discovery"], "code_url": "https://github.com/tb-git-tud/a-ceoh-evolution-of-heuristics", "brief": "This paper introduces 'Algorithmic-Contextual EoH' (A-CEoH), which injects the actual source code of the search algorithm (e.g., the A* driver loop, neighbor generation) into the LLM prompt alongside the problem description. Experiments on the Unit-Load Pre-Marshalling Problem and Sliding Puzzle Problem demonstrate that this algorithmic context allows a 32B parameter model (Qwen2.5-Coder) to generate heuristics superior to those from GPT-4o and human experts. The results are credible and backed by comparisons against optimal baselines. The key takeaway is a transferable 'prompt trick': explicitly showing the LLM the code that *calls* its generated function aligns the heuristic significantly better with the search dynamics than natural language descriptions alone. We should immediately test injecting our ALNS/search driver code into our evolutionary prompt templates.", "affiliations": "", "analysis_date": "2026-02-13"}, {"arxiv_id": "2602.13218", "arxiv_url": "https://arxiv.org/abs/2602.13218", "title": "Scaling the Scaling Logic: Agentic Meta-Synthesis of Logic Reasoning", "authors_short": "Bowen Liu et.al.", "published_date": "2026-01-23", "m_score": 9, "p_score": 6, "i_score": 9, "priority_score": 7.83, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["llm_as_heuristic", "llm_code_generation", "llm_as_evaluator", "llm_in_the_loop", "llm_evolutionary_search"], "problems": ["logical_reasoning", "program_synthesis", "automated_algorithm_design", "heuristic_evolution", "task_family_synthesis"], "code_url": "https://github.com/AdAstraAbyssoque/Scaling-the-Scaling-Logic", "brief": "Liu et al. introduce SS-Logic, an agentic framework that evolves Python 'Generator-Validator' pairs to scale logic task families, using a rigorous 'Code-Augmented Blind Review' where independent agents must write code to solve generated tasks to verify their validity. They expand 400 seed families to over 21k instances, achieving consistent gains on AIME (+3.0) and SynLogic (+5.2) via RLVR. **Crucial Takeaway:** We should steal the 'Blind Review' mechanism for AlgoEvo—using the solvability of a generated problem (by an independent code agent) as a strict fitness filter for the generator itself. This directly addresses our bottleneck in filtering invalid or hallucinated heuristics during evolutionary search.", "affiliations": "Tencent, The Hong Kong University of Science and Technology (Guangzhou)", "analysis_date": "2026-02-17"}, {"arxiv_id": "2601.16849", "arxiv_url": "https://arxiv.org/abs/2601.16849", "title": "The Art of Being Difficult: Combining Human and AI Strengths to Find Adversarial Instances for Heuristics", "authors_short": "Henri Nikoleit et.al.", "published_date": "2026-01-23", "m_score": 5, "p_score": 8, "i_score": 7, "priority_score": 5.15, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_name": "Enhanced LLM Evolutionary Search via Concept Learning and Co-Evolution", "front_status": "stable", "methods": ["human_llm_collaboration", "program_search", "evolutionary_algorithm", "manual_refinement", "llm_code_generation"], "problems": ["adversarial_instance_generation", "knapsack_problem", "bin_packing", "hierarchical_k_median_clustering", "generalized_gasoline_problem"], "code_url": "https://github.com/lumi-a/funsearch", "brief": "This paper applies FunSearch to generate adversarial instances for classical OR heuristics (Knapsack, Bin Packing, k-median), successfully breaking long-standing theoretical lower bounds. The results are rigorous: they disprove the output-polynomial time of the Nemhauser-Ullmann algorithm and improve the Best-Fit bin packing bound to 1.5. The key takeaway for our AlgoEvo work is the workflow: the LLM finds 'messy' structural patterns (e.g., repeated floats) which humans then manually generalize into asymptotic proofs. This validates Program Search over vector search but exposes the 'generalization gap'—we should implement a post-processing agent to automate this manual refinement step.", "affiliations": "Google DeepMind, University of Bonn, University of Manitoba", "analysis_date": "2026-02-17"}, {"arxiv_id": "2511.16485", "arxiv_url": "https://arxiv.org/abs/2511.16485", "title": "Online Operator Design in Evolutionary Optimization for Flexible Job Shop Scheduling via Large Language Models", "authors_short": "Rongjie Liao et.al.", "published_date": "2026-01-22", "m_score": 7, "p_score": 8, "i_score": 7, "priority_score": 5.75, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["evolutionary_algorithm", "genetic_algorithm", "llm_as_heuristic", "llm_code_generation", "llm_in_the_loop"], "problems": ["flexible_job_shop_scheduling", "distributed_flexible_job_shop_scheduling"], "code_url": null, "brief": "LLM4EO embeds an LLM directly into the Genetic Algorithm loop to dynamically generate and replace gene-selection operators whenever the population stagnates, rather than training them offline. Results on FJSP benchmarks (Brandimarte, Fattahi) show a 3-4% improvement over static GA and GP, with convergence plots demonstrating that LLM interventions successfully break local optima. The most stealable insight is the 'Perception and Analysis' prompt structure: it forces the LLM to explicitly diagnose *why* the current population is stuck (based on fitness stats) before generating new code, a mechanism we should port to AlgoEvo to handle search stagnation. This validates the viability of online, state-aware LLM intervention in OR scheduling problems.", "affiliations": "City University of Hong Kong, Guangdong University of Technology", "analysis_date": "2026-02-17"}, {"arxiv_id": "2601.16175", "arxiv_url": "https://arxiv.org/abs/2601.16175", "title": "Learning to Discover at Test Time", "authors_short": "Mert Yuksekgonul et.al.", "published_date": "2026-01-22", "m_score": 9, "p_score": 10, "i_score": 10, "priority_score": 8.97, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["reinforcement_learning", "entropic_objective", "puct_search", "lora_fine_tuning", "importance_sampling"], "problems": ["erdos_minimum_overlap_problem", "autocorrelation_inequalities", "gpu_kernel_optimization", "triangular_matrix_multiplication", "multi_head_latent_attention"], "code_url": "https://github.com/test-time-training/discover", "brief": "TTT-Discover introduces a method to fine-tune an LLM (gpt-oss-120b) *during* inference on a single test problem using RL, replacing the frozen-model evolutionary search of AlphaEvolve. They employ a novel 'entropic objective' that optimizes for the single best solution (discovery) rather than expected return, combined with PUCT-based state reuse. The results are empirically rigorous, setting new SOTA on Erdős’ problem, GPU kernel optimization, and AtCoder contests, directly beating AlphaEvolve and ShinkaEvolve. The critical takeaway is that for hard discovery tasks, shifting the model's distribution via online updates is superior to context-based search; we should immediately test their entropic objective in our AlgoEvo pipeline.", "affiliations": "Stanford University, NVIDIA, UC San Diego, Together AI, Astera Institute", "analysis_date": "2026-02-17"}, {"arxiv_id": "2601.15738", "arxiv_url": "https://arxiv.org/abs/2601.15738", "title": "LLM-Assisted Automatic Dispatching Rule Design for Dynamic Flexible Assembly Flow Shop Scheduling", "authors_short": "Junhao Qiu et.al.", "published_date": "2026-01-22", "m_score": 7, "p_score": 6, "i_score": 7, "priority_score": 5.45, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["llm_evolutionary_search", "evolutionary_algorithm", "llm_as_heuristic", "llm_as_evaluator", "program_synthesis"], "problems": ["dynamic_flexible_assembly_flow_shop_scheduling", "flow_shop_scheduling", "dispatching_rule_design"], "code_url": null, "brief": "LLM4DRD employs a dual-agent framework (Generator & Evaluator) to evolve priority dispatching rules for dynamic flexible assembly flow shops. The core contribution is the **Hybrid Evaluation** mechanism, where the Evaluator generates qualitative critiques (strengths/weaknesses) that are injected into the Generator's prompts to guide specific operators like 'Dominance-Fusion Crossover' and 'Directed Optimization.' Empirical results show it outperforms FunSearch and EOH, avoiding the premature convergence seen in other methods. The most stealable insight is the prompt structure for crossover: rather than blindly combining code, it uses the Evaluator's analysis of parent strengths to direct the merger, a technique we should implement to improve sample efficiency in our evolutionary search.", "affiliations": "", "analysis_date": "2026-02-13"}, {"arxiv_id": "2601.05943", "arxiv_url": "https://arxiv.org/abs/2601.05943", "title": "Global Optimization for Combinatorial Geometry Problems Revisited in the Era of LLMs", "authors_short": "Timo Berthold et.al.", "published_date": "2026-01-15", "m_score": 4, "p_score": 9, "i_score": 8, "priority_score": 6.91, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["global_optimization", "nonlinear_programming", "spatial_branch_and_bound", "automatic_linearization", "convexification"], "problems": ["combinatorial_geometry_problems", "min_max_distance_ratio", "circle_packing", "hexagon_packing", "geometric_optimization"], "code_url": null, "brief": "Berthold et al. demonstrate that standard global NLP solvers (SCIP, Xpress) outperform DeepMind's AlphaEvolve on its own benchmarks (circle/hexagon packing, min-max distance) without any learning or evolution. The results are rigorous, improving on 'newly discovered' solutions within minutes using default solver settings. **CRITICAL TAKEAWAY:** We must validate our AlgoEvo results against classical global solvers to ensure we aren't claiming 'discovery' on problems that are trivial for SCIP; furthermore, it suggests a hybrid path where LLMs generate NLP models for solvers rather than evolving raw heuristic code. This is a necessary reality check for our benchmarking strategy.", "affiliations": "", "analysis_date": "2026-02-13"}, {"arxiv_id": "2601.05770", "arxiv_url": "https://arxiv.org/abs/2601.05770", "title": "Weights to Code: Extracting Interpretable Algorithms from the Discrete Transformer", "authors_short": "Yifan Zhang et.al.", "published_date": "2026-01-09", "m_score": 8, "p_score": 7, "i_score": 8, "priority_score": 7.61, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["discrete_transformer", "transformer", "temperature_annealing", "differentiable_sampling", "hypothesis_testing"], "problems": ["algorithm_extraction", "symbolic_algorithm_discovery"], "code_url": null, "brief": "Zhang et al. introduce the 'Discrete Transformer,' a constrained architecture that learns algorithmic tasks via gradient descent and allows for the post-hoc extraction of exact, human-readable Python code. By enforcing functional disentanglement (using attention strictly for routing and MLPs for arithmetic) and employing temperature-annealed sampling, they recover symbolic laws for arithmetic and physics tasks with near-zero error. The critical takeaway is their 'continuous-to-discrete homotopy' strategy—annealing from soft to hard selection during training—which enables differentiable search to converge on discrete, symbolic solutions. This suggests a viable path to discover heuristics via continuous optimization rather than purely stochastic LLM evolution.", "affiliations": "", "analysis_date": "2026-02-13"}, {"arxiv_id": "2510.14150", "arxiv_url": "https://arxiv.org/abs/2510.14150", "title": "CodeEvolve: an open source evolutionary coding agent for algorithm discovery and optimization", "authors_short": "Henrique Assumpção et.al.", "published_date": "2026-01-06", "m_score": 8, "p_score": 8, "i_score": 8, "priority_score": 7.86, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["genetic_algorithm", "llm_code_generation", "llm_as_heuristic", "llm_evolutionary_search", "program_synthesis"], "problems": ["algorithm_discovery", "packing_problems", "distance_optimization", "autocorrelation_inequalities"], "code_url": "https://github.com/inter-co/science-codeevolve", "brief": "CodeEvolve couples islands-based genetic algorithms with LLMs, utilizing CVT-MAP-Elites for diversity and a specific 'inspiration-based' crossover operator where the LLM integrates logic from high-ranking peer solutions. The results are strong and backed by numbers: they beat AlphaEvolve on 5/9 benchmarks and demonstrate that Qwen3-Coder-30B matches Gemini-2.5 performance at ~10% of the cost. The single most useful takeaway is the implementation of the 'inspiration' operator and the necessity of MAP-Elites over simple elitism to escape local optima in code space. We should immediately benchmark their open-source framework against our internal AlgoEvo builds.", "affiliations": "Inter&Co., Worcester Polytechnic Institute, Universidade Federal de Minas Gerais", "analysis_date": "2026-02-13"}, {"arxiv_id": "2512.24077", "arxiv_url": "https://arxiv.org/abs/2512.24077", "title": "LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm", "authors_short": "Chunhui Wan et.al.", "published_date": "2025-12-30", "m_score": 9, "p_score": 10, "i_score": 9, "priority_score": 8.81, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["plan_execute_summarize", "hybrid_evolutionary_memory", "multi_island_model", "map_elites", "adaptive_boltzmann_selection"], "problems": ["algorithm_discovery", "operator_discovery", "ml_pipeline_optimization", "mathematical_problems", "circle_packing"], "code_url": "https://github.com/baidu-baige/LoongFlow", "brief": "LoongFlow replaces the standard stochastic mutation operator in LLM evolutionary search with a 'Plan-Execute-Summarize' (PES) cognitive loop. Instead of random code changes, a Planner retrieves the 'intent' and 'summary' of the parent solution's lineage to generate a directed hypothesis, which is then executed and summarized for the next generation. The authors demonstrate a 60% reduction in evaluations and a 100% success rate on AlphaEvolve tasks where standard methods fail or stagnate. The critical takeaway is the 'Lineage-Based Context Retrieval' mechanism: explicitly passing the parent's plan and retrospective summary to the child allows for directed rather than random walks in the search space. We must implement this PES loop in AlgoEvo immediately to fix our sample efficiency issues.", "affiliations": "", "analysis_date": "2026-02-13"}, {"arxiv_id": "2512.14806", "arxiv_url": "https://arxiv.org/abs/2512.14806", "title": "Let the Barbarians In: How AI Can Accelerate Systems Performance Research", "authors_short": "Audrey Cheng et.al.", "published_date": "2025-12-22", "m_score": 7, "p_score": 10, "i_score": 9, "priority_score": 7.54, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["evolution_of_heuristics", "program_synthesis", "llm_code_generation", "llm_as_heuristic", "llm_as_evaluator"], "problems": ["systems_performance_optimization", "telemetry_repair", "multi_cloud_data_transfer_cost_optimization", "expert_parallelism_load_balancing", "model_to_gpu_placement"], "code_url": "https://github.com/codelion/openevolve", "brief": "Cheng et al. (UC Berkeley) perform a rigorous empirical evaluation of LLM evolutionary search (ADRS) across 10 systems problems, achieving SOTA results on MoE load balancing (13x speedup via rediscovering Hamilton's Apportionment) and cloud scheduling. The results are real and backed by code, comparing frameworks like OpenEvolve, GEPA, and ShinkaEvolve. **Key Takeaway:** Their 'Best Practices' section offers concrete engineering constraints we should adopt: specifically, that 'moderate' feedback (worst-k cases) outperforms 'detailed' feedback (prevents overfitting), and that restricting mutations to diff-based edits is essential to prevent reward hacking. This paper validates our core research thesis while providing the benchmarks we now need to beat.", "affiliations": "UC Berkeley", "analysis_date": "2026-02-17"}, {"arxiv_id": "2511.02864", "arxiv_url": "https://arxiv.org/abs/2511.02864", "title": "Mathematical exploration and discovery at scale", "authors_short": "Bogdan Georgiev et.al.", "published_date": "2025-12-22", "m_score": 9, "p_score": 7, "i_score": 9, "priority_score": 8.06, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["llm_evolutionary_search", "llm_code_generation", "llm_as_heuristic", "alphaevolve", "evolutionary_algorithm"], "problems": ["mathematical_construction_discovery", "finite_field_kakeya_sets", "finite_field_nikodym_sets", "autocorrelation_inequalities", "difference_bases"], "code_url": "https://github.com/Aryia-Behroziuan/Robot-learning", "brief": "DeepMind applies AlphaEvolve to 67 math problems, formalizing the distinction between 'Search Mode' (evolving heuristics for fixed instances) and 'Generalizer Mode' (evolving algorithms that extrapolate from small to large n). Results are rigorous, establishing new bounds on Kakeya sets and 10+ other problems by exploiting verifier loopholes and heuristic specialization. The most critical takeaway for AlgoEvo is Section 44: evolving code that *calls* other LLMs leads to emergent prompt optimization and injection strategies, suggesting a path for our multi-agent optimization work. We must adopt their 'Generalizer' training curriculum (train on small n, test on large n) to fix our scalability bottlenecks.", "affiliations": "Google DeepMind, UCLA, Brown University, Institute for Advanced Study", "analysis_date": "2026-02-17"}, {"arxiv_id": "2509.18057", "arxiv_url": "https://arxiv.org/abs/2509.18057", "title": "Reinforced Generation of Combinatorial Structures: Hardness of Approximation", "authors_short": "Ansh Nagda et.al.", "published_date": "2025-12-19", "m_score": 9, "p_score": 5, "i_score": 9, "priority_score": 7.56, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["alphaevolve", "llm_evolutionary_search", "llm_code_generation", "llm_as_evaluator", "evolution_of_heuristics"], "problems": ["max_cut", "max_independent_set", "max_k_cut", "metric_tsp", "hardness_of_approximation"], "code_url": null, "brief": "Nagda et al. utilize AlphaEvolve to discover combinatorial gadgets that improve hardness of approximation bounds for MAX-CUT and TSP, validating findings with formal proofs. The standout contribution is not the hardness results themselves, but the methodology: they tasked AlphaEvolve with optimizing the *verification code* (checking correctness against a slow ground truth), achieving a 10,000x speedup that enabled searching gadgets of size 19 (vs. 11 previously). We should immediately adopt this 'evolve the verifier' loop for our computationally expensive fitness functions in AlgoEvo to break current scalability limits.", "affiliations": "Google DeepMind, Google, University of California, Berkeley", "analysis_date": "2026-02-17"}, {"arxiv_id": "2512.13857", "arxiv_url": "https://arxiv.org/abs/2512.13857", "title": "EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery", "authors_short": "Kamer Ali Yuksel et.al.", "published_date": "2025-12-17", "m_score": 9, "p_score": 8, "i_score": 9, "priority_score": 8.31, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["llm_evolutionary_search", "program_synthesis", "evolution_of_heuristics", "quality_diversity", "directed_acyclic_graph"], "problems": ["program_synthesis", "agent_design", "neural_architecture_search_proxy_discovery", "optimizer_discovery"], "code_url": null, "brief": "EvoLattice replaces the standard 'overwrite-based' evolution of monolithic programs with a persistent DAG where each node holds multiple alternative implementations, evaluating all valid combinatorial paths to compute fine-grained performance statistics for every micro-operator. The results are strong: it outperforms AlphaEvolve and FunSearch styles on NAS-Bench-Zero by explicitly preserving diversity and enabling surgical, data-driven pruning rather than blind mutation. The critical takeaway is the 'alternative-level statistic' mechanism: by aggregating performance across all paths a component participates in, they generate a high-fidelity signal that tells the LLM exactly which lines of code are working, effectively solving the sparse reward problem in code evolution. We should immediately discuss refactoring our AlgoEvo representation to support this multi-alternative graph structure, as it maximizes signal extraction per LLM call.", "affiliations": "aiXplain Inc", "analysis_date": "2026-02-17"}, {"arxiv_id": "2512.09209", "arxiv_url": "https://arxiv.org/abs/2512.09209", "title": "Beyond Algorithm Evolution: An LLM-Driven Framework for the Co-Evolution of Swarm Intelligence Optimization Algorithms and Prompts", "authors_short": "Shipeng Cen et.al.", "published_date": "2025-12-10", "m_score": 8, "p_score": 8, "i_score": 8, "priority_score": 7.86, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["llm_evolutionary_search", "evolution_of_heuristics", "program_synthesis", "llm_prompt_optimization", "swarm_intelligence"], "problems": ["combinatorial_optimization", "aircraft_landing_problem", "equitable_partition_problem", "flow_shop_scheduling", "uncapacitated_p_median_problem"], "code_url": null, "brief": "The authors introduce a co-evolutionary framework where both the optimization algorithm (Fireworks Algorithm operators) and the prompt templates used to generate them are evolved simultaneously by the LLM. The results demonstrate a massive performance jump on constrained Aircraft Landing problems (from ~56% with FunSearch to 100% with their method), suggesting that static prompts are a primary failure mode for complex OR constraints. The critical takeaway is their prompt fitness function: evaluating a prompt template based on the *performance improvement* (`child - parent`) of the code it generates, rather than absolute performance. We should immediately implement this 'prompt-delta' fitness signal in AlgoEvo to automate our prompt engineering loop.", "affiliations": "Peking University", "analysis_date": "2026-02-13"}, {"arxiv_id": "2512.03762", "arxiv_url": "https://arxiv.org/abs/2512.03762", "title": "RoCo: Role-Based LLMs Collaboration for Automatic Heuristic Design", "authors_short": "Jiawei Xu et.al.", "published_date": "2025-12-04", "m_score": 8, "p_score": 8, "i_score": 7, "priority_score": 7.04, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["llm_evolutionary_search", "multi_agent_system", "llm_as_heuristic", "llm_as_evaluator", "llm_code_generation"], "problems": ["automatic_heuristic_design", "combinatorial_optimization", "tsp", "cvrp", "orienteering_problem"], "code_url": null, "brief": "RoCo replaces standard evolutionary mutation operators with a 4-agent collaboration loop (Explorer, Exploiter, Critic, Integrator) that iteratively refines heuristics and accumulates long-term reflection memory across generations. While the empirical gains over ReEvo are marginal (often <1%) and likely expensive in token cost, the architecture successfully demonstrates how to embed structured multi-agent reasoning into the evolutionary loop to stabilize black-box search. The key takeaway is their Long-term Reflection mechanism, which aggregates critic feedback into a persistent memory buffer to guide future mutations—a technique we should immediately test to improve sample efficiency in AlgoEvo.", "affiliations": "South China University of Technology", "analysis_date": "2026-02-17"}, {"arxiv_id": "2509.20412", "arxiv_url": "https://arxiv.org/abs/2509.20412", "title": "Structuring Collective Action with LLM-Guided Evolution: From Ill-Structured Problems to Executable Heuristics", "authors_short": "Kevin Bradley Dsouza et.al.", "published_date": "2025-12-03", "m_score": 7, "p_score": 5, "i_score": 7, "priority_score": 4.99, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["evolutionary_algorithms", "large_language_models", "llm_code_generation", "llm_as_evaluator", "llm_prompt_optimization"], "problems": ["collective_action_problems", "ill_structured_problems", "multi_agent_coordination", "heuristic_evolution", "agricultural_landscape_management"], "code_url": null, "brief": "ECHO-MIMIC presents a framework that first uses LLM-guided evolution to generate Python heuristics for agents (ECHO), and subsequently evolves natural language 'nudges' (MIMIC) to persuade simulated agents to adopt these global-optimal policies. While the experiments rely on synthetic data for agriculture and EV charging, the approach outperforms DSPy and AutoGen baselines in driving collective action. The most valuable takeaway is the architectural separation of 'policy discovery' (code evolution) and 'adoption mechanism' (message evolution)—a pattern we could adapt to evolve incentive structures or negotiation protocols in our multi-agent optimization systems (MASPRM/HERMES). The analysis of code complexity (Halstead metrics) versus fitness also provides a useful empirical reference for our observability work.", "affiliations": "University of Waterloo, Royal Bank of Canada", "analysis_date": "2026-02-17"}, {"arxiv_id": "2511.23473", "arxiv_url": "https://arxiv.org/abs/2511.23473", "title": "ThetaEvolve: Test-time Learning on Open Problems", "authors_short": "Yiping Wang et.al.", "published_date": "2025-11-28", "m_score": 10, "p_score": 9, "i_score": 9, "priority_score": 8.86, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["program_evolution", "reinforcement_learning", "grpo", "llm_code_generation", "llm_as_heuristic"], "problems": ["mathematical_discovery", "open_mathematical_optimization", "circle_packing", "autocorrelation_inequalities", "hadamard_matrix"], "code_url": "https://github.com/ypwang61/ThetaEvolve", "brief": "ThetaEvolve integrates test-time reinforcement learning (GRPO) directly into an AlphaEvolve-style loop, allowing a single 8B model to learn from its own successful mutations and achieve new SOTA bounds on Circle Packing and Autocorrelation inequalities. The results are rigorous, showing that RL applied to the *dynamic* environment (sampling from the evolving database) vastly outperforms RL on static prompts or pure inference search. The most stealable insight is the 'lazy penalty' mechanism—penalizing semantically equivalent code or stagnation—which forces the RL policy to learn genuine exploration strategies rather than memorization. This is a blueprint for the 'RL-infused evolution' milestone in our AlgoEvo roadmap.", "affiliations": "Microsoft, University of Washington, Carnegie Mellon University, University of Wisconsin-Madison, University of California, San Diego", "analysis_date": "2026-02-17"}, {"arxiv_id": "2511.17592", "arxiv_url": "https://arxiv.org/abs/2511.17592", "title": "GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms", "authors_short": "Valentin Khrulkov et.al.", "published_date": "2025-11-17", "m_score": 8, "p_score": 8, "i_score": 7, "priority_score": 7.04, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["llm_evolutionary_search", "map_elites", "quality_diversity", "llm_as_heuristic", "llm_code_generation"], "problems": ["heilbronn_triangle_problem", "geometric_optimization", "circle_packing", "kissing_number_problem", "discrete_optimization"], "code_url": "https://github.com/FusionBrainLab/gigaevo-core", "brief": "GigaEvo is an open-source reproduction of the AlphaEvolve framework that implements MAP-Elites with an asynchronous DAG execution engine, successfully reproducing SOTA results on Heilbronn triangles and beating FunSearch on Weibull Bin Packing. The results are credible and backed by code, specifically highlighting that 'rewrite-based' mutation outperforms 'diff-based' approaches for open-weights models—a crucial engineering constraint for us. The most actionable takeaway is their 'bidirectional lineage tracking' mechanism, which enriches mutation prompts by analyzing both how a program improved over its ancestor and how its descendants further improved, a technique we should steal for AlgoEvo's mutation operator. Their negative result regarding multi-island MAP-Elites (added complexity, no gain) suggests we should deprioritize similar complex topologies.", "affiliations": "Sber, Artificial Intelligence Research Institute (AIRI)", "analysis_date": "2026-02-17"}, {"arxiv_id": "2508.03661", "arxiv_url": "https://arxiv.org/abs/2508.03661", "title": "Automated Algorithmic Discovery for Scientific Computing through LLM-Guided Evolutionary Search: A Case Study in Gravitational-Wave Detection", "authors_short": "He Wang et.al.", "published_date": "2025-11-16", "m_score": 9, "p_score": 7, "i_score": 9, "priority_score": 8.06, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_name": "LLM-Enhanced Evolutionary Search: Diversity, RL, and Structural Co-evolution", "front_status": "stable", "methods": ["monte_carlo_tree_search", "evolutionary_algorithm", "llm_code_generation", "llm_as_heuristic", "llm_as_evaluator"], "problems": ["gravitational_wave_detection", "operator_discovery", "heuristic_evolution"], "code_url": "https://github.com/iphysresearch/evo-mcts", "brief": "Evo-MCTS introduces a hybrid search architecture where MCTS manages the exploration-exploitation balance of an evolutionary process, using LLMs for node expansion via novel operators like 'Path-wise Crossover' (synthesizing code from full root-to-leaf trajectories). The results are empirically strong, outperforming standard LLM-evolution baselines (ReEvo) by ~150% on a complex signal processing task. We learned that structuring the evolutionary lineage as a tree and using MCTS Q-values to select parents—rather than standard population selection—drastically improves sample efficiency and solution quality. This is a blueprint for the 'RL-infused evolution' and 'persistent memory' features we have been planning for our own framework.", "affiliations": "Tsinghua University, University of Chinese Academy of Sciences", "analysis_date": "2026-02-13"}, {"arxiv_id": "2511.08522", "arxiv_url": "https://arxiv.org/abs/2511.08522", "title": "AlphaResearch: Accelerating New Algorithm Discovery with Language Models", "authors_short": "Zhaojian Yu et.al.", "published_date": "2025-11-11", "m_score": 7, "p_score": 6, "i_score": 7, "priority_score": 5.24, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["llm_research_agent", "llm_code_generation", "reward_model", "llm_fine_tuned", "program_synthesis"], "problems": ["algorithm_discovery", "packing_circles", "spherical_code", "littlewood_polynomials", "mstd"], "code_url": "https://github.com/answers111/alpha-research", "brief": "AlphaResearch introduces a 'dual environment' for algorithm discovery: it generates natural language research ideas, filters them using a reward model fine-tuned on ICLR peer reviews, and then executes the surviving ideas. While it claims to beat human baselines on Packing Circles, the improvement is marginal (<0.1%) and it fails to improve upon baselines in 6/8 benchmark problems. The key takeaway for us is the mechanism of an 'Idea Critic'—using a learned reward model to filter the search space at the prompt level before wasting compute on execution—which directly addresses our sample efficiency goals in evolutionary search.", "affiliations": "Yale, NYU, Tsinghua, ByteDance", "analysis_date": "2026-02-13"}, {"arxiv_id": "2510.14825", "arxiv_url": "https://arxiv.org/abs/2510.14825", "title": "Programmatic Representation Learning with Language Models", "authors_short": "Gabriel Poesia et.al.", "published_date": "2025-10-16", "m_score": 9, "p_score": 6, "i_score": 9, "priority_score": 7.81, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_name": "Enhanced LLM Evolutionary Search via Concept Learning and Co-Evolution", "front_status": "stable", "methods": ["llm_code_generation", "llm_as_heuristic", "llm_in_the_loop", "funsearch", "evolution_of_heuristics"], "problems": ["supervised_learning", "representation_learning", "chess_position_evaluation", "image_classification", "mnist"], "code_url": "https://github.com/gpoesia/leapr", "brief": "The authors propose two algorithms, F2 (Features FunSearch) and D-ID3 (Dynamic ID3), to learn programmatic features for decision trees. D-ID3 is particularly novel: instead of evolving a global heuristic, it calls the LLM at *each split node* to generate a feature that discriminates the specific data subset at that leaf. Results are strong on Chess (matching Transformers trained on 250x more data) and Text, though the Image results (MNIST) are trivial. **Key Takeaway:** The D-ID3 architecture—using the solver's current state (leaf node data) to prompt the LLM for *local* code generation—is a powerful pattern we should steal for our VRP solvers (e.g., evolving local repair operators for specific route bottlenecks) and EvoCut work.", "affiliations": "Harvard University, Stanford University", "analysis_date": "2026-02-17"}, {"arxiv_id": "2510.11121", "arxiv_url": "https://arxiv.org/abs/2510.11121", "title": "Refining Hybrid Genetic Search for CVRP via Reinforcement Learning-Finetuned LLM", "authors_short": "Rongjie Zhu et.al.", "published_date": "2025-10-13", "m_score": 9, "p_score": 10, "i_score": 9, "priority_score": 8.81, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_name": "Enhanced LLM Evolutionary Search via Concept Learning and Co-Evolution", "front_status": "stable", "methods": ["genetic_algorithm", "hgs", "reinforcement_learning", "rl_dapo", "llm_as_heuristic"], "problems": ["cvrp", "combinatorial_routing", "operator_discovery", "heuristic_evolution"], "code_url": null, "brief": "Zhu et al. fine-tune a Qwen-14B model using Reinforcement Learning (DAPO) to generate C++ crossover operators for the state-of-the-art HGS solver. Unlike typical prompting papers, they demonstrate that a small, specialized model can improve upon expert-designed components in a highly optimized solver, achieving superior results on CVRPLIB (up to 1000 nodes) where GPT-4o fails. The most stealable insight is their **AST-based anti-plagiarism reward**, which penalizes the model for generating code structurally identical to the prompt examples, effectively forcing exploration and preventing mode collapse—a technique we should immediately adopt for our evolutionary search agents. This confirms we should pivot from pure prompting to RL-finetuning for our code-generation agents.", "affiliations": "Nanyang Technological University, Singapore, Singapore Management University, Singapore, Nanjing University of Information Science and Technology, China", "analysis_date": "2026-02-13"}, {"arxiv_id": "2510.06189", "arxiv_url": "https://arxiv.org/abs/2510.06189", "title": "Barbarians at the Gate: How AI is Upending Systems Research", "authors_short": "Audrey Cheng et.al.", "published_date": "2025-10-10", "m_score": 6, "p_score": 9, "i_score": 8, "priority_score": 6.84, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_name": "Enhanced LLM Evolutionary Search via Concept Learning and Co-Evolution", "front_status": "stable", "methods": ["llm_as_heuristic", "llm_as_evaluator", "llm_code_generation", "evolutionary_search", "map_elites"], "problems": ["algorithm_discovery", "systems_performance_optimization", "spot_instance_scheduling", "llm_serving_optimization", "transaction_scheduling"], "code_url": null, "brief": "The authors apply OpenEvolve (an AlphaEvolve-style framework) to 11 computer systems problems, achieving significant gains over human baselines, such as a 5.0x speedup in MoE expert placement and 26% cost reduction in cloud scheduling. The results are empirically rigorous, relying on high-fidelity simulators rather than toy problems. For us, the key takeaway is the engineering recipe: using an ensemble of reasoning models (o3) for exploration and fast models (Gemini) for diversity, combined with a specific 'failure taxonomy' to debug search stagnation. This is immediate proof-of-concept for your 'GPUSched' and 'AlgoEvo' projects; we should adopt their ensemble strategy and simulator-first evaluation pipeline.", "affiliations": "UC Berkeley", "analysis_date": "2026-02-13"}, {"arxiv_id": "2510.08755", "arxiv_url": "https://arxiv.org/abs/2510.08755", "title": "Robust Heuristic Algorithm Design with LLMs", "authors_short": "Pantea Karimi et.al.", "published_date": "2025-10-09", "m_score": 9, "p_score": 8, "i_score": 9, "priority_score": 8.31, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_name": "Enhanced LLM Evolutionary Search via Concept Learning and Co-Evolution", "front_status": "stable", "methods": ["llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "funsearch", "evolution_of_heuristics"], "problems": ["robust_algorithm_design", "heuristic_design", "traffic_engineering", "network_routing"], "code_url": null, "brief": "Karimi et al. introduce 'Robusta', an enhancement to FunSearch that uses a Heuristic Analyzer (solver-based) to identify adversarial inputs and a Suggester LLM to explain *why* the current heuristic fails before generating new code. They demonstrate a 28x improvement in worst-case performance over FunSearch on traffic engineering tasks, with results backed by rigorous comparison against optimal solvers. The critical takeaway is the 'Suggester' intermediate step: converting raw failure instances into natural language coding strategies significantly improves the LLM's ability to fix logic bugs compared to raw samples alone. We should immediately attempt to replicate this 'Analyzer -> Explainer -> Coder' loop for our VRP work, using small-scale solvers to generate counter-examples for our evolved ALNS operators.", "affiliations": "Microsoft, MIT, Microsoft Research, University of Southern California, The University of Texas at Austin", "analysis_date": "2026-02-17"}, {"arxiv_id": "2510.06056", "arxiv_url": "https://arxiv.org/abs/2510.06056", "title": "Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep Research", "authors_short": "Gang Liu et.al.", "published_date": "2025-10-07", "m_score": 8, "p_score": 8, "i_score": 8, "priority_score": 7.86, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["agent_based_framework", "llm_as_heuristic", "llm_code_generation", "llm_as_evaluator", "llm_in_the_loop"], "problems": ["algorithm_discovery", "molecular_prediction", "molecular_translation", "circle_packing", "pde_solving"], "code_url": "https://github.com/liugangcode/deepevolve", "brief": "DeepEvolve augments the standard evolutionary coding loop (AlphaEvolve) with two critical components: a 'Deep Research' module that searches the web/literature to generate grounded mutation proposals, and an iterative debugging agent that fixes execution errors. While the '666%' improvement on Circle Packing is likely due to a weak baseline (fixed-size vs. generalized), the engineering results are compelling: the debugging agent raises execution success rates from ~13% to ~99% in complex tasks. The key takeaway for our AlgoEvo work is the architecture of generating a text-based 'research proposal' via RAG before attempting code generation, rather than mutating code directly. We should immediately adopt their debugging loop and consider injecting external literature search into our mutation operators to prevent search stagnation.", "affiliations": "MIT-IBM Watson AI Lab, IBM Research, University of Notre Dame", "analysis_date": "2026-02-13"}, {"arxiv_id": "2509.24509", "arxiv_url": "https://arxiv.org/abs/2509.24509", "title": "Experience-Guided Reflective Co-Evolution of Prompts and Heuristics for Automatic Algorithm Design", "authors_short": "Yihong Liu et.al.", "published_date": "2025-09-30", "m_score": 8, "p_score": 7, "i_score": 8, "priority_score": 6.94, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_name": "Enhanced LLM Evolutionary Search via Concept Learning and Co-Evolution", "front_status": "stable", "methods": ["evolutionary_search", "llm_as_heuristic", "llm_code_generation", "llm_prompt_optimization", "program_synthesis"], "problems": ["tsp", "bin_packing", "heuristic_evolution", "operator_discovery"], "code_url": "null", "brief": "EvoPH introduces a co-evolutionary framework where both the heuristic code and the LLM prompts are evolved, utilizing an island model for diversity and a 'strategy sampling' mechanism that dynamically selects mutation types (e.g., parameter tuning vs. rewrite) based on feedback. They report dominating performance over FunSearch and ReEvo on TSP and BPP (e.g., reducing Christofides gap from ~20% to ~5%), though the static performance of baselines suggests the gain comes largely from automating prompt engineering. The most stealable insight is the **Strategy Sampling** module: explicitly defining a pool of mutation operators and using an 'experience' buffer to select them is a practical implementation of the 'planner' concept we need for AlgoEvo. We should also adopt their island migration topology to improve diversity in our parallelized search.", "affiliations": "Tencent, Renmin University of China, City University of Hong Kong", "analysis_date": "2026-02-13"}, {"arxiv_id": "2509.24323", "arxiv_url": "https://arxiv.org/abs/2509.24323", "title": "MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems", "authors_short": "Kun Wang et.al.", "published_date": "2025-09-29", "m_score": 8, "p_score": 6, "i_score": 8, "priority_score": 7.36, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["multi_agent_systems", "llm_in_the_loop", "meta_agents", "tri_agent_architecture", "self_generative_mas"], "problems": ["automated_multi_agent_system_design", "multi_hop_question_answering", "deep_research", "code_generation", "mathematical_reasoning"], "code_url": "https://github.com/yeyeyeah2/MAS2", "brief": "MAS2 trains a tri-agent system (Generator, Implementer, Rectifier) using offline RL on decision trees to dynamically construct and repair multi-agent workflows. The results are strong, outperforming ADAS and MaAS on standard benchmarks while maintaining Pareto efficiency. The critical takeaway for us is the **Rectifier agent**: rather than discarding failed evolutionary candidates (as we currently do in AlgoEvo), we should implement a dedicated loop to patch runtime errors (e.g., API failures, dimension mismatches). Additionally, their 'Collaborative Tree Optimization' offers a rigorous method to fine-tune the 'Evolver' model using trajectory data, which could replace our current prompt-based meta-heuristics.", "affiliations": "NTU, NUS, USTC, ZJU, BUAA, PKU", "analysis_date": "2026-02-17"}, {"arxiv_id": "2509.23331", "arxiv_url": "https://arxiv.org/abs/2509.23331", "title": "C-Evolve: Consensus-based Evolution for Prompt Groups", "authors_short": "Tiancheng Li et.al.", "published_date": "2025-09-27", "m_score": 8, "p_score": 7, "i_score": 9, "priority_score": 7.76, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["evolutionary_algorithm", "island_based_evolutionary_algorithm", "genetic_algorithm", "prompt_optimization", "llm_in_the_loop"], "problems": ["multi_hop_question_answering", "instruction_following", "fact_extraction", "claim_verification", "mathematical_problem_solving"], "code_url": null, "brief": "C-Evolve modifies island-based evolution to optimize a group of prompts that maximize consensus accuracy (majority vote) rather than individual performance. The authors introduce a 'voting score' fitness function—calculated via Exponential Moving Average (EMA) of an individual's contribution to sampled groups—which successfully drives the population toward diverse, complementary strategies that outperform ensembles of individually optimized prompts (beating AlphaEvolve by ~4% on Qwen3-8B). The single most actionable takeaway is the **EMA voting score mechanism**: we can steal this exact fitness formulation to evolve portfolios of complementary VRP heuristics in AlgoEvo, replacing our current focus on converging to a single 'best' solver. While the benchmarks are standard (MATH, HotpotQA), the method offers a robust solution to the 'single heuristic limitation' we face in OR.", "affiliations": "Westlake University", "analysis_date": "2026-02-17"}, {"arxiv_id": "2505.22954", "arxiv_url": "https://arxiv.org/abs/2505.22954", "title": "Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents", "authors_short": "Jenny Zhang et.al.", "published_date": "2025-09-26", "m_score": 10, "p_score": 8, "i_score": 10, "priority_score": 8.76, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["darwin_godel_machine", "open_ended_evolution", "evolution_of_heuristics", "llm_code_generation", "llm_as_heuristic"], "problems": ["software_engineering_tasks", "code_generation", "automated_algorithm_design", "heuristic_evolution", "llm_hallucination_mitigation"], "code_url": "https://github.com/jennyzzt/dgm", "brief": "DGM implements a population-based evolutionary loop where agents modify their own Python source code (tools, memory, flow) to improve performance on coding benchmarks, rather than just optimizing prompts or parameters. Results are strong and verified: it boosts a base agent from 20% to 50% on SWE-bench Verified, matching handcrafted SoTA, with ablations proving the necessity of the population archive (open-endedness) over single-lineage hill climbing. **Key Takeaway:** The 'self-diagnosis' mechanism—feeding execution logs to a model to propose specific *architectural* code changes (e.g., implementing a 'str_replace' tool to fix granular editing errors)—is the exact mechanism we need to implement for evolving our heuristic searchers. This validates that LLM-driven code evolution is viable for complex logic improvement, not just toy tasks.", "affiliations": "Sakana AI, Vector Institute, University of British Columbia, Canada CIFAR AI Chair", "analysis_date": "2026-02-17"}, {"arxiv_id": "2509.21593", "arxiv_url": "https://arxiv.org/abs/2509.21593", "title": "GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models", "authors_short": "Peng Luo et.al.", "published_date": "2025-09-25", "m_score": 8, "p_score": 7, "i_score": 8, "priority_score": 6.94, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["multi_agent_llm", "evolutionary_search", "retrieval_augmented_generation", "llm_code_generation", "llm_as_evaluator"], "problems": ["geospatial_modeling", "spatial_interpolation", "uncertainty_quantification", "algorithm_discovery"], "code_url": null, "brief": "GeoEvolve augments standard LLM-based evolutionary search (OpenEvolve) with an outer 'researcher' loop that queries a domain-specific RAG (textbooks/papers) to inject theoretical constraints into mutation prompts. On geospatial interpolation tasks, they report 13-21% error reduction over standard evolution, with ablations confirming that retrieved domain knowledge—not just iterative feedback—drives the performance gain. The critical takeaway is the architectural pattern of 'Knowledge-Guided Evolution': instead of relying on the LLM's internal weights for domain theory, they explicitly retrieve and inject theoretical priors (e.g., valid variogram definitions) to steer the search. We should adapt this 'Theory-RAG' outer loop for our AlgoEvo pipeline to force evolved VRP heuristics to respect OR theoretical bounds.", "affiliations": "Massachusetts Institute of Technology, Stanford University, Technical University of Munich", "analysis_date": "2026-02-13"}, {"arxiv_id": "2507.15877", "arxiv_url": "https://arxiv.org/abs/2507.15877", "title": "Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning", "authors_short": "Simon Ouellette et.al.", "published_date": "2025-09-21", "m_score": 8, "p_score": 6, "i_score": 8, "priority_score": 7.36, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["neural_program_synthesis", "execution_guided_program_synthesis", "transformer", "tree_search", "domain_specific_language"], "problems": ["arc_agi", "compositional_generalization", "out_of_distribution_generalization", "program_synthesis", "visual_reasoning"], "code_url": "https://github.com/SimonOuellette35/OODGenARC-AGI", "brief": "Ouellette implements an Execution-Guided Neural Program Synthesis (EG-NPS) system for ARC-AGI that conditions the search on the intermediate execution state of every instruction, achieving 80% success on out-of-distribution tasks where TTFT (10%) and standard AlphaEvolve (0-14%) fail. The results are rigorous, using controlled OOD tasks to prove that TTFT relies on in-distribution priors rather than reasoning. The critical takeaway for our AlgoEvo work is the architecture of the 'state-conditioned decoder': instead of blind code generation, we should inject the tokenized execution result of step $t$ into the context for step $t+1$. This is effectively a dense process reward model that solves the sample efficiency bottleneck we face in evolutionary search.", "affiliations": "", "analysis_date": "2026-02-17"}, {"arxiv_id": "2509.19349", "arxiv_url": "https://arxiv.org/abs/2509.19349", "title": "ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution", "authors_short": "Robert Tjarko Lange et.al.", "published_date": "2025-09-17", "m_score": 9, "p_score": 9, "i_score": 8, "priority_score": 8.41, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["evolutionary_algorithms", "llm_as_mutation_operator", "llm_as_evaluator", "llm_code_generation", "llm_evolutionary_search"], "problems": ["circle_packing", "mathematical_reasoning", "agent_design", "competitive_programming", "heuristic_evolution"], "code_url": "https://github.com/SakanaAI/ShinkaEvolve", "brief": "ShinkaEvolve presents an open-source evolutionary framework that drastically improves sample efficiency (e.g., beating AlphaEvolve on Circle Packing with only 150 evaluations vs. thousands) by integrating embedding-based novelty rejection, adaptive parent sampling, and bandit-based LLM selection. The results are credible, backed by code from Sakana AI, and directly target our primary pain point of high API costs/sample inefficiency in evolutionary search. **Key Takeaway:** We must implement their 'novelty rejection sampling' immediately—using a cheap embedding model to filter out semantically similar code mutations (threshold 0.95) before execution is a trivial but high-impact optimization for our AlgoEvo pipeline. This paper proves that smart filtering is superior to the brute-force compute strategies we have been relying on.", "affiliations": "Sakana AI", "analysis_date": "2026-02-17"}, {"arxiv_id": "2507.17668", "arxiv_url": "https://arxiv.org/abs/2507.17668", "title": "How Should We Meta-Learn Reinforcement Learning Algorithms?", "authors_short": "Alexander David Goldie et.al.", "published_date": "2025-09-10", "m_score": 8, "p_score": 7, "i_score": 7, "priority_score": 7.46, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["meta_learning", "evolution_strategies", "meta_gradients", "black_box_distillation", "symbolic_distillation"], "problems": ["reinforcement_learning_algorithm_design", "algorithm_discovery", "learned_optimizers", "learned_policy_optimization", "learned_policy_gradient"], "code_url": "https://github.com/AlexGoldie/learn-rl-algorithms", "brief": "Goldie et al. perform a rigorous empirical benchmark comparing LLM-based algorithm proposal against Black-box Evolution Strategies (ES) and various distillation methods. They find that while LLMs are sample-efficient for simple functions, they catastrophically fail to incorporate high-dimensional input features (e.g., the 20+ inputs in OPEN), where Black-box ES remains superior. The most actionable takeaway is 'Same-Size Distillation': distilling a learned black-box algorithm into a fresh network of identical size using synthetic data consistently improves out-of-distribution generalization with zero additional environment samples. We should implement this distillation step immediately and reconsider using LLMs for feature-heavy heuristic components.", "affiliations": "University of Oxford", "analysis_date": "2026-02-17"}, {"arxiv_id": "2509.07367", "arxiv_url": "https://arxiv.org/abs/2509.07367", "title": "Autonomous Code Evolution Meets NP-Completeness", "authors_short": "Cunxi Yu et.al.", "published_date": "2025-09-09", "m_score": 9, "p_score": 9, "i_score": 10, "priority_score": 8.71, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["llm_evolutionary_search", "llm_code_generation", "llm_in_the_loop", "program_synthesis", "self_improving_search"], "problems": ["boolean_satisfiability", "np_complete_problem", "combinatorial_optimization", "heuristic_evolution", "algorithm_discovery"], "code_url": null, "brief": "SATLUTION extends LLM evolutionary search to full-scale C++ repositories, autonomously evolving SAT solvers that outperform 2025 human competition winners using only 2024 training data. The results are highly rigorous, backed by 90k CPU hours of distributed evaluation and strict correctness proofs (DRAT), showing a clear monotonic improvement trajectory. The single most stealable insight is the **self-evolving rule system**: the agent autonomously updates a persistent set of markdown constraints (e.g., forbidden patterns, testing protocols) based on post-cycle failure analysis, effectively creating 'institutional memory' that prevents regression in long-horizon search. We must implement this meta-learning loop in AlgoEvo immediately to move beyond single-file optimization.", "affiliations": "NVIDIA Research, University of Maryland", "analysis_date": "2026-02-17"}, {"arxiv_id": "2508.03082", "arxiv_url": "https://arxiv.org/abs/2508.03082", "title": "EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic Design", "authors_short": "Fei Liu et.al.", "published_date": "2025-08-20", "m_score": 8, "p_score": 9, "i_score": 8, "priority_score": 8.3, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_7", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["evolutionary_algorithm", "llm_as_heuristic", "llm_code_generation", "memetic_algorithm", "population_management"], "problems": ["bin_packing", "tsp", "cvrp", "heuristic_evolution", "combinatorial_optimization"], "code_url": null, "brief": "EoH-S reformulates Automated Heuristic Design (AHD) to evolve a complementary *set* of heuristics rather than a single robust one, proving the objective is submodular and solvable via a greedy strategy. Results are strong and credible: on TSPLib and CVRPLib, their set of 10 heuristics reduces the optimality gap by ~40-60% compared to the top 10 heuristics from FunSearch or ReEvo. **KEY TAKEAWAY:** We should replace standard elitist selection in AlgoEvo with their 'Complementary Population Management' (CPM). By greedily selecting individuals based on marginal contribution to instance coverage (using instance-wise performance vectors), we can automatically generate diverse operator pools for ALNS instead of relying on hand-crafted diversity metrics.", "affiliations": "Huawei Noah\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArk Lab, City University of Hong Kong", "analysis_date": "2026-02-13"}, {"arxiv_id": "2505.18602", "arxiv_url": "https://arxiv.org/abs/2505.18602", "title": "LLM-Meta-SR: In-Context Learning for Evolving Selection Operators in Symbolic Regression", "authors_short": "Hengzhe Zhang et.al.", "published_date": "2025-08-08", "m_score": 8, "p_score": 7, "i_score": 8, "priority_score": 7.14, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_5", "front_name": "Advanced LLM-Driven Algorithm Evolution via Behavioral and Structural Guidance", "front_status": "stable", "methods": ["llm_evolutionary_search", "genetic_programming", "evolutionary_algorithm", "llm_as_heuristic", "llm_code_generation"], "problems": ["symbolic_regression", "operator_discovery", "heuristic_evolution"], "code_url": "https://anonymous.4open.science/r/LLM-Meta-SR/", "brief": "Zhang et al. develop a meta-evolutionary framework to evolve selection operators for symbolic regression, achieving state-of-the-art results on SRBench by outperforming expert-designed methods like ε-lexicase. The standout contribution is **semantics-aware crossover**: rather than selecting parents based solely on scalar fitness, they compute complementarity scores using performance vectors across instances, explicitly retrieving parents that solve different subsets of the problem. This effectively treats parent selection as a retrieval task based on behavioral signatures, ensuring the LLM combines distinct functional capabilities. We should immediately implement this complementarity-based parent retrieval in AlgoEvo to improve how we merge heuristics.", "affiliations": "Victoria University of Wellington, Michigan State University", "analysis_date": "2026-02-13"}, {"arxiv_id": "2504.05108", "arxiv_url": "https://arxiv.org/abs/2504.05108", "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning", "authors_short": "Anja Surina et.al.", "published_date": "2025-08-04", "m_score": 8, "p_score": 8, "i_score": 8, "priority_score": 7.86, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_name": "LLM-Enhanced Evolutionary Search: Diversity, RL, and Structural Co-evolution", "front_status": "stable", "methods": ["evolutionary_search", "reinforcement_learning", "dpo", "llm_as_heuristic", "llm_code_generation"], "problems": ["bin_packing", "tsp", "datacenter_optimization", "ridesharing_optimization", "symbolic_regression"], "code_url": "https://claire-labo.github.io/EvoTune/", "brief": "EvoTune augments LLM-based evolutionary search (FunSearch) by iteratively fine-tuning the LLM weights using Direct Preference Optimization (DPO) on the generated programs. The results are robust, consistently outperforming static FunSearch on Bin Packing, TSP, and Hash Code benchmarks by discovering better heuristics faster. The critical takeaway is the use of **Forward KL regularization** in DPO instead of the standard Reverse KL; this prevents the mode collapse that usually kills evolutionary diversity, allowing the model to learn from high-fitness samples while maintaining exploration. This is a direct blueprint for implementing the 'RL-infused evolution' component of our AlgoEvo project.", "affiliations": "EPFL, Apple", "analysis_date": "2026-02-13"}, {"arxiv_id": "2508.01558", "arxiv_url": "https://arxiv.org/abs/2508.01558", "title": "EvoVLMA: Evolutionary Vision-Language Model Adaptation", "authors_short": "Kun Ding et.al.", "published_date": "2025-08-03", "m_score": 7, "p_score": 4, "i_score": 7, "priority_score": 4.74, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_name": "Enhanced LLM Evolutionary Search via Concept Learning and Co-Evolution", "front_status": "stable", "methods": ["evolutionary_algorithm", "llm_code_generation", "llm_as_heuristic", "program_synthesis", "evolution_of_heuristics"], "problems": ["algorithm_discovery", "vision_language_model_adaptation", "few_shot_learning", "multimodal_learning"], "code_url": "https://github.com/kding1225/EvoVLMA", "brief": "This paper proposes EvoVLMA, an LLM-based evolutionary framework that searches for Python code to adapt Vision-Language Models (feature selection and logits computation). They demonstrate that **jointly** evolving two coupled algorithmic components fails (worse than random), whereas a **sequential** two-stage evolution strategy yields SOTA results (beating manual baselines by ~1-2%). For our AlgoEvo work, the key takeaway is the infrastructure design: they wrap code execution in restartable web services with a process monitor to handle the high rate of CUDA errors/timeouts in generated code—a practical 'trick' we should adopt to improve our search stability.", "affiliations": "Chinese Academy of Sciences", "analysis_date": "2026-02-13"}, {"arxiv_id": "2507.03605", "arxiv_url": "https://arxiv.org/abs/2507.03605", "title": "Behaviour Space Analysis of LLM-driven Meta-heuristic Discovery", "authors_short": "Niki van Stein et.al.", "published_date": "2025-07-04", "m_score": 7, "p_score": 8, "i_score": 8, "priority_score": 7.56, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_5", "front_name": "Advanced LLM-Driven Algorithm Evolution via Behavioral and Structural Guidance", "front_status": "stable", "methods": ["llamea", "evolution_strategy", "elitist_strategy", "llm_evolutionary_search", "evolution_of_heuristics"], "problems": ["automated_algorithm_design", "black_box_optimization", "expensive_continuous_optimization", "heuristic_evolution", "operator_discovery"], "code_url": "https://doi.org/10.5281/zenodo.15675581", "brief": "The authors introduce a behavioral analysis framework for LLM-driven algorithm discovery, mapping the 'behavior space' of generated heuristics using Search Trajectory Networks (STNs) and Code Evolution Graphs (CEGs). Results on BBOB (5D) show that a simple 1+1 elitist strategy alternating between 'simplify code' and 'random new' prompts significantly outperforms population-based approaches, effectively balancing exploitation and exploration while preventing code bloat. The primary takeaway is the critical role of a 'simplify' mutation operator—without it, LLM-generated code tends to drift into complexity without performance gains. We should immediately adopt their visualization metrics to debug our own evolutionary search trajectories and implement their 'simplify' prompt strategy in AlgoEvo.", "affiliations": "Leiden University, University of Stirling", "analysis_date": "2026-02-13"}, {"arxiv_id": "2506.13131", "arxiv_url": "https://arxiv.org/abs/2506.13131", "title": "AlphaEvolve: A coding agent for scientific and algorithmic discovery", "authors_short": "Alexander Novikov et.al.", "published_date": "2025-06-16", "m_score": 10, "p_score": 10, "i_score": 10, "priority_score": 9.44, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_6", "front_name": "Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design", "front_status": "stable", "methods": ["llm_evolutionary_search", "evolutionary_algorithm", "code_generation", "program_synthesis", "llm_as_heuristic"], "problems": ["scientific_discovery", "algorithm_discovery", "matrix_multiplication", "mathematical_discovery", "combinatorial_packing"], "code_url": "https://github.com/zdmwi/open-alpha-evolve", "brief": "AlphaEvolve extends FunSearch by evolving entire code files (rather than single functions) using a 'search/replace' diff format and Gemini 2.0, achieving SOTA results across matrix multiplication (beating Strassen), 50+ open math problems, and Google's production scheduling. The results are exceptionally strong and verified, including deployed improvements to Google's Borg scheduler (0.7% resource recovery) and TPU circuits. The critical takeaway is the move to **diff-based full-file evolution** and **meta-prompt evolution** (evolving the prompt instructions alongside the code), which allows the system to modify architecture and logic rather than just heuristics. This is a mandatory blueprint for the next iteration of our AlgoEvo and EvoCut projects.", "affiliations": "Google DeepMind", "analysis_date": "2026-02-13"}, {"arxiv_id": "2410.15639", "arxiv_url": "https://arxiv.org/abs/2410.15639", "title": "Can Large Language Models Invent Algorithms to Improve Themselves?: Algorithm Discovery for Recursive Self-Improvement through Reinforcement Learning", "authors_short": "Yoichi Ishibashi et.al.", "published_date": "2025-06-10", "m_score": 9, "p_score": 8, "i_score": 9, "priority_score": 8.31, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["rl_dpo", "llm_code_generation", "llm_as_evaluator", "llm_fine_tuned", "llm_evolutionary_search"], "problems": ["algorithm_discovery", "mathematical_reasoning", "model_merging"], "code_url": null, "brief": "Ishibashi et al. propose 'Self-Developing,' a framework where an LLM generates Python code for model merging, evaluates the results, and uses the performance data to fine-tune the generator via DPO in a recursive loop. The results are empirically strong, outperforming human-designed baselines (Task Arithmetic) by 4.3% on GSM8k and demonstrating that the generator explicitly learns better strategies over iterations. **Key Takeaway:** We can replace the static mutation operators in our evolutionary search with a DPO-trained model that learns from the search history—effectively implementing 'learning to search.' This is a direct, actionable upgrade for our AlgoEvo and AlphaEvolve pipelines.", "affiliations": "NEC Corporation", "analysis_date": "2026-02-13"}, {"arxiv_id": "2506.02049", "arxiv_url": "https://arxiv.org/abs/2506.02049", "title": "EvoGit: Decentralized Code Evolution via Git-Based Multi-Agent Collaboration", "authors_short": "Beichen Huang et.al.", "published_date": "2025-06-01", "m_score": 8, "p_score": 6, "i_score": 8, "priority_score": 7.56, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_5", "front_name": "Advanced LLM-Driven Algorithm Evolution via Behavioral and Structural Guidance", "front_status": "stable", "methods": ["llm_code_generation", "llm_as_evaluator", "multi_agent_system", "evolutionary_algorithm", "program_synthesis"], "problems": ["software_development", "web_application_development", "automated_algorithm_design", "bin_packing"], "code_url": "https://github.com/billhuang2001/evogit", "brief": "Huang et al. introduce EvoGit, a framework where LLM agents asynchronously evolve code by treating Git commits as the population and using 3-way merges (based on Lowest Common Ancestor) as crossover. While the experiments (web app, bin packing generator) are largely qualitative and lack rigorous statistical benchmarking against baselines like MetaGPT, the architectural contribution is significant. The key takeaway is using Git's native DAG structure to handle lineage, persistence, and asynchronous concurrency 'for free,' replacing complex custom population managers. This is directly actionable for our AlgoEvo infrastructure to enable massive parallelism and better memory/traceability without reinventing the wheel.", "affiliations": "The Hong Kong Polytechnic University", "analysis_date": "2026-02-17"}, {"arxiv_id": "2506.11057", "arxiv_url": "https://arxiv.org/abs/2506.11057", "title": "STRCMP: Integrating Graph Structural Priors with Language Models for Combinatorial Optimization", "authors_short": "Xijun Li et.al.", "published_date": "2025-05-22", "m_score": 8, "p_score": 9, "i_score": 8, "priority_score": 8.11, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_name": "LLM-Enhanced Evolutionary Search: Diversity, RL, and Structural Co-evolution", "front_status": "stable", "methods": ["gnn", "llm_in_the_loop", "evolutionary_algorithm", "supervised_fine_tuning", "direct_preference_optimization"], "problems": ["milp_general", "sat", "set_covering", "maximum_independent_set", "multiple_knapsack"], "code_url": null, "brief": "STRCMP introduces a composite architecture where a GNN encodes CO problem instances (MILP/SAT) into embeddings that condition an LLM (fine-tuned via SFT and DPO) to generate solver-specific heuristics within an evolutionary loop. The results are strong and empirically backed, showing significant reductions in convergence time and timeouts compared to text-only evolutionary methods like AutoSAT and LLM4Solver. The key takeaway is the architectural blueprint for fusing instance-specific structural embeddings (via soft prompting) with LLM code generation to drastically improve the sample efficiency of evolutionary search. This is immediately relevant to our EvoCut and AlgoEvo projects, suggesting we should move beyond pure text prompts for topology-heavy problems.", "affiliations": "Shanghai Key Laboratory of Scalable Computing and Systems, School of Computer Science, Shanghai Jiao Tong University", "analysis_date": "2026-02-13"}, {"arxiv_id": "2505.12285", "arxiv_url": "https://arxiv.org/abs/2505.12285", "title": "CALM: Co-evolution of Algorithms and Language Model for Automatic Heuristic Design", "authors_short": "Ziyao Huang et.al.", "published_date": "2025-05-18", "m_score": 9, "p_score": 8, "i_score": 9, "priority_score": 8.31, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_name": "LLM-Enhanced Evolutionary Search: Diversity, RL, and Structural Co-evolution", "front_status": "stable", "methods": ["evolutionary_search", "heuristic_evolution", "llm_fine_tuned", "rl_trained", "grpo"], "problems": ["bin_packing", "tsp", "cvrp", "orienteering_problem", "algorithm_discovery"], "code_url": "https://github.com/whxru/CALM", "brief": "CALM introduces a hybrid evolutionary framework that fine-tunes the LLM generator *during* the search process using Group Relative Policy Optimization (GRPO), rather than relying solely on prompt evolution. Using a quantized Qwen-7B model on a single consumer GPU, it outperforms GPT-4o-based baselines (FunSearch, EoH) on Bin Packing and VRP benchmarks. The critical takeaway is their reward function design: instead of absolute performance, they reward the *relative improvement* of the generated code over the specific 'parent' heuristics in the prompt, stabilizing the RL signal. We should immediately test this 'online fine-tuning' approach to reduce our API costs and improve sample efficiency in AlgoEvo.", "affiliations": "City University of Hong Kong, Southeast University, University of Victoria, Hon Hai Research Institute", "analysis_date": "2026-02-13"}, {"arxiv_id": "2504.00613", "arxiv_url": "https://arxiv.org/abs/2504.00613", "title": "LLM-Guided Search for Deletion-Correcting Codes", "authors_short": "Franziska Weindel et.al.", "published_date": "2025-04-01", "m_score": 7, "p_score": 4, "i_score": 8, "priority_score": 5.89, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_name": "Enhanced LLM Evolutionary Search via Concept Learning and Co-Evolution", "front_status": "stable", "methods": ["llm_guided_evolutionary_search", "funsearch", "evolutionary_search", "program_synthesis", "greedy_algorithm"], "problems": ["deletion_correcting_codes", "independent_set", "code_design", "information_theory"], "code_url": "https://github.com/MLI-lab/FunDCC", "brief": "Weindel and Heckel adapt FunSearch to discover priority functions for the Maximum Independent Set problem (applied to deletion-correcting codes), achieving new SOTA lower bounds for specific lengths (n=12, 13, 16). The critical takeaway for us is their **functional deduplication** step: they hash function outputs on a small subset of data to discard syntactically unique but logically identical programs, which significantly improves sample efficiency by preventing the evaluator from wasting cycles on 'comment changes' or variable renames. Additionally, they demonstrate that optimizing for the single hardest instance generalizes better than averaging performance across a curriculum—a counter-intuitive finding we should test in our reward modeling.", "affiliations": "Technical University of Munich, Munich Center for Machine Learning", "analysis_date": "2026-02-17"}, {"arxiv_id": "2503.10721", "arxiv_url": "https://arxiv.org/abs/2503.10721", "title": "From Understanding to Excelling: Template-Free Algorithm Design through Structural-Functional Co-Evolution", "authors_short": "Zhe Zhao et.al.", "published_date": "2025-03-13", "m_score": 9, "p_score": 9, "i_score": 8, "priority_score": 8.41, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_name": "LLM-Enhanced Evolutionary Search: Diversity, RL, and Structural Co-evolution", "front_status": "stable", "methods": ["llm_code_generation", "llm_as_heuristic", "llm_in_the_loop", "evolutionary_algorithm", "genetic_algorithm"], "problems": ["algorithm_discovery", "heuristic_evolution", "tsp", "cvrp", "quadratic_optimization"], "code_url": null, "brief": "Zhao et al. propose CAE, a framework that co-evolves algorithm structure (workflow/call graphs) alongside function implementations, aiming to eliminate the fixed templates required by SOTA methods like FunSearch and EoH. On TSP benchmarks, they report reducing optimality gaps by ~2-5% compared to ReEvo, and in quadratic optimization, the system autonomously discovered numerical stability fixes (e.g., replacing matrix inversion with solvers) that human baselines missed. The critical takeaway is the 'bi-dimensional co-evolution' strategy: explicitly maintaining and mutating a population of control flow graphs separate from the function bodies, which allows the system to escape the local optima imposed by a fixed human-designed harness. We must evaluate if this structural search approach can be integrated into AlgoEvo to automate our harness design.", "affiliations": "Princeton University, Nanyang Technological University, City University of Hong Kong, University of Science and Technology of China, The Hong Kong University of Science and Technology (Guangzhou)", "analysis_date": "2026-02-17"}, {"arxiv_id": "2412.20694", "arxiv_url": "https://arxiv.org/abs/2412.20694", "title": "QUBE: Enhancing Automatic Heuristic Design via Quality-Uncertainty Balanced Evolution", "authors_short": "Zijie Chen et.al.", "published_date": "2025-02-21", "m_score": 8, "p_score": 8, "i_score": 8, "priority_score": 7.86, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_name": "Enhanced LLM Evolutionary Search via Concept Learning and Co-Evolution", "front_status": "stable", "methods": ["evolutionary_algorithm", "llm_code_generation", "llm_as_heuristic", "llm_in_the_loop", "funsearch"], "problems": ["online_bin_packing", "tsp", "cap_set_problem", "heuristic_evolution"], "code_url": "https://github.com/zzjchen/QUBE_code", "brief": "QUBE replaces FunSearch's naive score-based parent selection with a UCB algorithm that selects parents based on the *average quality of their offspring* (exploitation) plus an uncertainty term (exploration). The authors demonstrate that a parent's own score is a poor predictor of its ability to evolve further; treating parents as 'bandit arms' based on their lineage statistics yields significantly better results on Bin Packing and TSP with fewer samples. While they fail to beat DeepMind's massive-scale Cap Set record, the methodological insight regarding 'offspring-aware' selection is statistically validated and immediately transferable to our evolutionary search frameworks.", "affiliations": "Westlake University, Zhejiang University, University of Electronic Science and Technology of China", "analysis_date": "2026-02-17"}, {"arxiv_id": "2502.09544", "arxiv_url": "https://arxiv.org/abs/2502.09544", "title": "Explainable AI-assisted Optimization for Feynman Integral Reduction", "authors_short": "Zhuo-Yang Song et.al.", "published_date": "2025-02-13", "m_score": 7, "p_score": 3, "i_score": 8, "priority_score": 4.63, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_name": "Enhanced LLM Evolutionary Search via Concept Learning and Co-Evolution", "front_status": "stable", "methods": ["funsearch", "genetic_algorithm", "large_language_models", "llm_as_heuristic", "program_synthesis"], "problems": ["feynman_integral_reduction", "high_energy_physics_calculations", "one_loop_integrals", "multi_loop_integrals", "planar_integrals"], "code_url": null, "brief": "Song et al. apply FunSearch to evolve priority functions for Feynman integral reduction, achieving up to 3058x reduction in seeding integrals compared to standard heuristics. The results are rigorous, enabling previously impossible multi-loop calculations. The critical insight for us is the successful transfer of heuristics evolved on trivial 1-loop instances (fast evaluation) to complex 5-loop problems without retraining. We should adopt this 'evolve-on-toy, deploy-on-giant' evaluation protocol to drastically reduce compute costs in our VRP and SAT solver evolutionary search pipelines.", "affiliations": "Peking University, Universit\nZ\nrich, Beijing Computational Science Research Center", "analysis_date": "2026-02-17"}, {"arxiv_id": "2409.16867", "arxiv_url": "https://arxiv.org/abs/2409.16867", "title": "Multi-objective Evolution of Heuristic Using Large Language Model", "authors_short": "Shunyu Yao et.al.", "published_date": "2025-02-04", "m_score": 8, "p_score": 9, "i_score": 8, "priority_score": 7.44, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_name": "LLM-Enhanced Evolutionary Search: Diversity, RL, and Structural Co-evolution", "front_status": "stable", "methods": ["llm_evolutionary_search", "llm_as_heuristic", "evolutionary_algorithm", "multi_objective_optimization", "program_synthesis"], "problems": ["heuristic_evolution", "bin_packing", "tsp"], "code_url": "https://github.com/Optima-CityU/LLM4AD", "brief": "MEoH extends LLM-based heuristic evolution (like FunSearch/EoH) to multi-objective scenarios (e.g., Gap vs. Runtime) by introducing a 'Dominance-Dissimilarity' mechanism that selects parents based on both Pareto dominance and Abstract Syntax Tree (AST) code distance. The results are credible and strong: on TSP, they find heuristics matching EoH's quality but running 16x faster (1.37s vs 22.4s) by effectively navigating the complexity-performance trade-off. The single most useful takeaway is the **AST-based dissimilarity metric** for population management; we should immediately steal this to prune semantically identical code in our evolutionary loops, thereby forcing exploration and improving sample efficiency. This is a direct upgrade to our current single-objective evolutionary search methods.", "affiliations": "City University of Hong Kong, Southern University of Science and Technology", "analysis_date": "2026-02-13"}, {"arxiv_id": "2412.14995", "arxiv_url": "https://arxiv.org/abs/2412.14995", "title": "HSEvo: Elevating Automatic Heuristic Design with Diversity-Driven Harmony Search and Genetic Algorithm Using LLMs", "authors_short": "Pham Vu Tuan Dat et.al.", "published_date": "2024-12-19", "m_score": 7, "p_score": 8, "i_score": 7, "priority_score": 5.73, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_name": "LLM-Enhanced Evolutionary Search: Diversity, RL, and Structural Co-evolution", "front_status": "stable", "methods": ["llm_evolutionary_search", "llm_as_heuristic", "llm_code_generation", "harmony_search", "genetic_algorithm"], "problems": ["automated_heuristic_design", "bin_packing_online", "traveling_salesman_problem", "orienteering_problem"], "code_url": "https://github.com/datphamvn/HSEvo", "brief": "HSEvo extends LLM-based evolutionary search (LLM-EPS) by integrating a numerical parameter tuning step (Harmony Search) and a token-efficient 'Flash Reflection' mechanism that batches analysis of parent pairs. They report superior results over ReEvo and FunSearch on Bin Packing and TSP, validated by proposed diversity metrics based on code embeddings. **Key Takeaway:** We should implement the hybrid tuning pattern: explicitly parsing LLM-generated code to extract constants and tuning them with a cheap numerical optimizer (rather than asking the LLM to tune parameters), and adopt batched reflections to reduce inference costs.", "affiliations": "George Mason University, Hanoi University of Science and Technology", "analysis_date": "2026-02-17"}, {"arxiv_id": "2411.19744", "arxiv_url": "https://arxiv.org/abs/2411.19744", "title": "Amplifying human performance in combinatorial competitive programming", "authors_short": "Petar Veličković et.al.", "published_date": "2024-11-29", "m_score": 6, "p_score": 9, "i_score": 8, "priority_score": 6.84, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_name": "Enhanced LLM Evolutionary Search via Concept Learning and Co-Evolution", "front_status": "stable", "methods": ["funsearch", "evolutionary_algorithm", "program_synthesis", "llm_evolutionary_search", "llm_code_generation"], "problems": ["combinatorial_optimization", "data_center_optimization", "delivery_routing", "video_streaming_optimization", "self_driving_rides_scheduling"], "code_url": null, "brief": "DeepMind applies FunSearch (using Gemini 1.5 Flash) to evolve scoring functions within human-written greedy backbones for Hash Code and AtCoder problems, achieving top-1% or rank-1 performance against humans. The results are robust, beating top human teams on 5/8 historical contests using a generic evolutionary setup. The critical takeaway is the 'switching variable' technique: using a single evolved function to handle multiple distinct decision points (e.g., selecting a vehicle vs. selecting a route) by passing a state flag, rather than evolving multiple interacting functions. This validates that generalist models (Flash) are sufficient for high-end OR evolution without code-specific fine-tuning. We should adopt their 'Backbone + Scorer' architecture for our VRP/Scheduling work immediately.", "affiliations": "Google DeepMind", "analysis_date": "2026-02-17"}, {"arxiv_id": "2410.22657", "arxiv_url": "https://arxiv.org/abs/2410.22657", "title": "Automatic programming via large language models with population self-evolution for dynamic job shop scheduling problem", "authors_short": "Jin Huang et.al.", "published_date": "2024-10-30", "m_score": 6, "p_score": 8, "i_score": 6, "priority_score": 5.28, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_name": "LLM-Enhanced Evolutionary Search: Diversity, RL, and Structural Co-evolution", "front_status": "stable", "methods": ["llm_evolutionary_search", "evolution_of_heuristics", "program_synthesis", "self_improving_search", "genetic_algorithm"], "problems": ["job_shop_scheduling", "dynamic_scheduling", "heuristic_evolution", "operator_discovery"], "code_url": null, "brief": "This paper introduces SeEvo, an LLM-based evolutionary search for Dynamic Job Shop Scheduling heuristics that adds an 'individual self-reflection' loop—prompting the LLM to analyze performance differences of a specific rule before and after mutation—alongside standard population-level reflection. While they claim significant improvements over GP/GEP and DRL, the ablation study reveals only a marginal <1% improvement over the existing ReEvo framework on benchmark instances. The primary takeaway for us is the specific prompt engineering technique of injecting an individual's mutation history (previous code vs. current code performance) into the context to guide the next mutation, which could potentially improve sample efficiency in our own evolutionary loops despite their weak empirical validation.", "affiliations": "Huazhong University of Science and Technology", "analysis_date": "2026-02-13"}, {"arxiv_id": "2402.01145", "arxiv_url": "https://arxiv.org/abs/2402.01145", "title": "ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution", "authors_short": "Haoran Ye et.al.", "published_date": "2024-10-14", "m_score": 8, "p_score": 9, "i_score": 8, "priority_score": 7.44, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_name": "LLM-Enhanced Evolutionary Search: Diversity, RL, and Structural Co-evolution", "front_status": "stable", "methods": ["hyper_heuristics", "language_hyper_heuristics", "reflective_evolution", "evolutionary_computation", "genetic_programming"], "problems": ["combinatorial_optimization", "heuristic_evolution", "traveling_salesman_problem", "capacitated_vehicle_routing_problem", "orienteering_problem"], "code_url": "https://github.com/ai4co/reevo", "brief": "ReEvo integrates a 'Reflector LLM' into genetic programming that analyzes pairs of heuristics (better vs. worse) to generate textual 'verbal gradients' for crossover and mutation, maintaining a long-term memory of these insights. The results are strong and relevant: they outperform EoH (Evolution of Heuristics) and NCO baselines on TSP, CVRP, and Bin Packing with significantly higher sample efficiency (only ~100 evaluations). The single most useful takeaway is the 'Short-term Reflection' prompting strategy—explicitly asking the LLM to derive a mutation direction by comparing the logic of high-fitness vs. low-fitness parents—which we should immediately test in our AlgoEvo framework to reduce sample costs. This is a direct methodological upgrade for our current evolutionary search pipelines.", "affiliations": "Peking University, KAIST, Singapore Management University, Southeast University, PKU-Wuhan Institute for AI", "analysis_date": "2026-02-17"}, {"arxiv_id": "2407.10873", "arxiv_url": "https://arxiv.org/abs/2407.10873", "title": "Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models", "authors_short": "Rui Zhang et.al.", "published_date": "2024-07-15", "m_score": 4, "p_score": 10, "i_score": 6, "priority_score": 6.86, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_name": "LLM-Enhanced Evolutionary Search: Diversity, RL, and Structural Co-evolution", "front_status": "stable", "methods": ["llm_evolutionary_search", "program_synthesis", "evolutionary_computation", "genetic_algorithm", "island_model"], "problems": ["automated_heuristic_design", "admissible_set_problem", "online_bin_packing", "tsp"], "code_url": "https://github.com/zhichao-lu/llm-eps", "brief": "Zhang et al. perform a rigorous benchmarking of major LLM-based evolutionary program search (EPS) methods (FunSearch, EoH, ReEvo) against a simple (1+1)-EPS baseline across four problems and nine LLMs. The results are empirically solid and sobering: the simple (1+1)-EPS baseline—iterative improvement via one-shot prompting—frequently matches or outperforms the complex population-based methods, particularly on bin packing, though EoH remains superior on TSP. **Crucial Takeaway:** We are likely over-engineering our search mechanisms; we must implement a (1+1)-EPS baseline in all future experiments (AlgoEvo, EvoCut) because if our multi-agent systems cannot beat this simple hill-climber, our papers will be rejected for unnecessary complexity. Additionally, they find that larger models (GPT-4) do not strictly guarantee better heuristic search performance compared to smaller, code-specialized models like CodeLlama-7B.", "affiliations": "City University of Hong Kong, Southern University of Science and Technology", "analysis_date": "2026-02-13"}, {"arxiv_id": "2406.04824", "arxiv_url": "https://arxiv.org/abs/2406.04824", "title": "FunBO: Discovering Acquisition Functions for Bayesian Optimization with FunSearch", "authors_short": "Virginia Aglietti et.al.", "published_date": "2024-07-01", "m_score": 6, "p_score": 9, "i_score": 8, "priority_score": 6.84, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_name": "Enhanced LLM Evolutionary Search via Concept Learning and Co-Evolution", "front_status": "stable", "methods": ["bayesian_optimization", "acquisition_functions", "gaussian_processes", "funsearch", "llm_code_generation"], "problems": ["acquisition_function_discovery", "black_box_optimization", "hyperparameter_optimization", "global_optimization_benchmarks", "heuristic_evolution"], "code_url": null, "brief": "FunBO applies FunSearch to evolve Python code for Bayesian Optimization acquisition functions, evaluating fitness by running full BO loops on synthetic functions. The results are empirically strong, showing that evolved AFs generalize well to out-of-distribution functions and outperform standard baselines like EI and UCB. The most stealable insight is their 'few-shot' adaptation strategy, where a general-purpose heuristic is rapidly fine-tuned on a small set of target instances—a technique we should immediately test for our VRP heuristics. While the method is computationally expensive (brute-forcing the inner loop), the interpretable code outputs provide concrete ideas for dynamic exploration-exploitation trade-offs.", "affiliations": "Google DeepMind", "analysis_date": "2026-02-17"}, {"arxiv_id": "2401.02051", "arxiv_url": "https://arxiv.org/abs/2401.02051", "title": "Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model", "authors_short": "Fei Liu et.al.", "published_date": "2024-06-01", "m_score": 8, "p_score": 9, "i_score": 8, "priority_score": 8.11, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_name": "LLM-Enhanced Evolutionary Search: Diversity, RL, and Structural Co-evolution", "front_status": "stable", "methods": ["evolutionary_computation", "large_language_models", "llm_code_generation", "llm_in_the_loop", "llm_evolutionary_search"], "problems": ["automatic_heuristic_design", "online_bin_packing", "traveling_salesman_problem", "flow_shop_scheduling_problem", "heuristic_evolution"], "code_url": "https://github.com/FeiLiu36/EoH", "brief": "EoH introduces a dual-track evolutionary framework that evolves both natural language 'thoughts' (heuristic logic) and their corresponding Python code, rather than code alone. On Online Bin Packing, it claims to outperform DeepMind's FunSearch while using only ~2,000 LLM queries (vs FunSearch's millions), and achieves SOTA gaps on TSP and FSSP via Guided Local Search. The critical takeaway is the 'E2' prompt strategy: explicitly asking the LLM to extract common ideas from parent heuristics into a natural language 'thought' before generating code, which acts as a genetic Chain-of-Thought to stabilize mutation. We should immediately implement this 'Thought-then-Code' mutation operator in our AlgoEvo pipeline to address our sample efficiency bottlenecks.", "affiliations": "Huawei Noah’s Ark Lab, City University of Hong Kong, Southern University of Science and Technology", "analysis_date": "2026-02-17"}, {"arxiv_id": "2402.02456", "arxiv_url": "https://arxiv.org/abs/2402.02456", "title": "tnGPS: Discovering Unknown Tensor Network Structure Search Algorithms via Large Language Models (LLMs", "authors_short": "Junhua Zeng et.al.", "published_date": "2024-06-01", "m_score": 8, "p_score": 3, "i_score": 8, "priority_score": 5.94, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_2", "front_name": "Enhanced LLM Evolutionary Search via Concept Learning and Co-Evolution", "front_status": "stable", "methods": ["llm_code_generation", "llm_in_the_loop", "llm_evolutionary_search", "program_synthesis", "evolution_of_heuristics"], "problems": ["tensor_network_structure_search", "automatic_algorithm_discovery", "hyperparameter_optimization", "model_selection"], "code_url": "https://github.com/ChaoLiAtRIKEN/tngps", "brief": "The authors propose tnGPS, a FunSearch-style framework that evolves Python code for Tensor Network Structure Search by mimicking human innovation stages (categorization, recombination, diversity injection). While the application (Tensor Networks) is niche, the results outperform standard heuristics like TNGA and TNLS. The critical takeaway for us is the 'Knowledge Categorization' phase: they use the LLM to semantically cluster the population of generated algorithms to manage diversity and guide the 'Diversity Injection' step. We should immediately implement this LLM-based population clustering in AlgoEvo to prevent convergence on similar code patterns.", "affiliations": "RIKEN Center for Advanced Intelligence Project, Tencent Inc., Guangdong University of Technology", "analysis_date": "2026-02-17"}, {"arxiv_id": "2403.11446", "arxiv_url": "https://arxiv.org/abs/2403.11446", "title": "LLM Guided Evolution -- The Automation of Models Advancing Models", "authors_short": "Clint Morris et.al.", "published_date": "2024-03-18", "m_score": 5, "p_score": 6, "i_score": 7, "priority_score": 4.63, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_1", "front_name": "LLM-Enhanced Evolutionary Search: Diversity, RL, and Structural Co-evolution", "front_status": "stable", "methods": ["genetic_algorithm", "llm_code_generation", "llm_as_heuristic", "llm_as_evaluator", "llm_in_the_loop"], "problems": ["neural_architecture_search", "image_classification"], "code_url": "https://github.com/clint-kristopher-morris/llm-guided-evolution", "brief": "Morris et al. propose 'Guided Evolution,' an LLM-based NAS framework that introduces 'Evolution of Thought' (EoT) and 'Character Role Play' to guide code mutations. While the results are statistically negligible (single trials, ~0.8% gain on CIFAR-10), the EoT mechanism offers a specific, actionable prompt engineering technique: explicitly prompting the LLM to compare a successful elite individual against its original seed to extract 'reasoning' before applying mutations to new individuals. This serves as a lightweight, prompt-based memory/feedback mechanism that could immediately improve sample efficiency in our evolutionary search agents. The 'Character Role Play' (e.g., asking the LLM to act as 'Dr. MaGoo' for unorthodox ideas) is a gimmicky but potentially useful heuristic for maintaining population diversity.", "affiliations": "Georgia Tech Research Institute", "analysis_date": "2026-02-17"}, {"arxiv_id": "2403.02985", "arxiv_url": "https://arxiv.org/abs/2403.02985", "title": "Evolution Transformer: In-Context Evolutionary Optimization", "authors_short": "Robert Tjarko Lange et.al.", "published_date": "2024-03-05", "m_score": 8, "p_score": 7, "i_score": 9, "priority_score": 7.93, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "llms_for_algorithm_d_2026-02-18_front_5", "front_name": "Advanced LLM-Driven Algorithm Evolution via Behavioral and Structural Guidance", "front_status": "stable", "methods": ["evolution_strategies", "transformer", "self_attention", "perceiver_attention", "algorithm_distillation"], "problems": ["black_box_optimization", "neuroevolution", "continuous_control", "expensive_continuous_optimization", "algorithm_discovery"], "code_url": "https://github.com/RobertTLange/evosax", "brief": "Lange et al. introduce the Evolution Transformer, a causal architecture that learns to perform evolutionary strategy updates by attending to optimization history, effectively 'distilling' algorithms like CMA-ES into a neural network. Crucially, they propose 'Self-Referential Algorithm Distillation' (SR-EAD), where the model improves itself by perturbing its own weights, generating trajectories, and filtering for the best ones to retrain on—eliminating the need for a teacher. The results are strong, showing generalization to unseen Brax control tasks and successful (though sometimes unstable) self-bootstrapping. The key takeaway for us is the SR-EAD loop as a mechanism for open-ended optimizer improvement, and their use of Perceiver cross-attention to handle variable population sizes—a technique we should immediately steal for our multi-agent memory architectures.", "affiliations": "Google DeepMind, TU Berlin", "analysis_date": "2026-02-17"}], "Generative AI for OR": [{"arxiv_id": "2602.15983", "arxiv_url": "https://arxiv.org/abs/2602.15983", "title": "ReLoop: Structured Modeling and Behavioral Verification for Reliable LLM-Based Optimization", "authors_short": "Junbo Jacob Lian et.al.", "published_date": "2026-02-17", "m_score": 8, "p_score": 9, "i_score": 8, "priority_score": 8.3, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["chain_of_thought", "behavioral_verification", "solver_based_perturbation", "iis_diagnostics", "diagnosis_guided_repair"], "problems": ["optimization_modeling", "silent_failures", "multi_period_retail_inventory_optimization", "milp_general", "program_synthesis"], "code_url": "https://github.com/junbolian/ReLoop", "brief": "ReLoop proposes a verification pipeline for LLM-generated optimization models that detects 'silent failures' (code that runs but solves the wrong problem) by perturbing input parameters and checking for expected solver objective shifts. They demonstrate that standard execution feasibility is a poor proxy for correctness (90% gap) on their new RetailOpt-190 benchmark, and that this perturbation testing significantly improves reliability. The critical takeaway is the use of sensitivity analysis as a ground-truth-free process reward signal: we can validate evolved algorithms in AlgoEvo by asserting that specific input perturbations *must* trigger output changes, filtering out semantically invalid candidates before expensive evaluation.", "affiliations": "National University of Singapore, Northwestern University, City University of Hong Kong, Wenzhou University, Wenzhou Buyi Pharmacy Chain Co., Ltd.", "analysis_date": "2026-02-19"}, {"arxiv_id": "2602.10450", "arxiv_url": "https://arxiv.org/abs/2602.10450", "title": "Constructing Industrial-Scale Optimization Modeling Benchmark", "authors_short": "Zhong Li et.al.", "published_date": "2026-02-11", "m_score": 7, "p_score": 9, "i_score": 8, "priority_score": 7.96, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["benchmark_design", "dataset_generation", "reverse_engineering", "structure_aware_modeling", "mixed_integer_linear_programming"], "problems": ["natural_language_to_optimization", "industrial_optimization", "benchmark_creation", "mixed_integer_linear_programming", "combinatorial_optimization"], "code_url": "https://github.com/optsuite/MIPLIB-NL", "brief": "Li et al. introduce MIPLIB-NL, a benchmark of 223 industrial-scale MILP instances (up to 10^7 variables) reverse-engineered from MIPLIB 2017, enforcing strict model-data separation. Results are sobering: SOTA models like GPT-4 and fine-tuned OR-LLMs drop from ~90% accuracy on existing toy benchmarks to ~18% here, failing primarily on structural consistency and index handling at scale. For us, the key takeaway is their \"Loop-Based Structural Scaffold\" taxonomy—a method to compress massive industrial formulations into compact LLM prompts via model-data separation. This is a mandatory read for our OR-Bench project, as it demonstrates that current evaluations are effectively measuring overfitting to toy problems rather than genuine modeling capability.", "affiliations": "Peking University, Huawei Technologies Co., Ltd., Great Bay University", "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.03070", "arxiv_url": "https://arxiv.org/abs/2602.03070", "title": "ProOPF: Benchmarking and Improving LLMs for Professional-Grade Power Systems Optimization Modeling", "authors_short": "Chao Shen et.al.", "published_date": "2026-02-03", "m_score": 7, "p_score": 6, "i_score": 7, "priority_score": 5.33, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["llm_code_generation", "llm_as_evaluator", "llm_fine_tuned", "data_synthesis", "benchmark_design"], "problems": ["optimal_power_flow", "power_system_optimization", "unit_commitment", "economic_dispatch", "optimal_transmission_switching"], "code_url": null, "brief": "Shen et al. propose a benchmark (ProOPF) for translating natural language into Optimal Power Flow (OPF) models, treating instances as parametric or structural modifications to a canonical base model rather than generating code from scratch. They introduce a rigorous data synthesis pipeline using 'scenario trees' to map qualitative descriptions (e.g., 'heatwave') to quantitative parameter deltas, and define structural extensions (e.g., adding security constraints) as modular patches. Results are sobering: SOTA models (GPT-4, Claude 3.5) score 0% on the hardest level (semantic inference + structural change), though SFT recovers ~11-35%. **Key Takeaway:** We should steal their 'Base + Delta' synthesis approach for our VRP variant generation and OR-Bench work; it allows for scalable, physically valid data generation without requiring an LLM to hallucinate full solvers, and effectively benchmarks 'ambiguity' handling.", "affiliations": "", "analysis_date": "2026-02-15"}, {"arxiv_id": "2602.03318", "arxiv_url": "https://arxiv.org/abs/2602.03318", "title": "MIRROR: A Multi-Agent Framework with Iterative Adaptive Revision and Hierarchical Retrieval for Optimization Modeling in Operations Research", "authors_short": "Yifan Shi et.al.", "published_date": "2026-02-03", "m_score": 5, "p_score": 9, "i_score": 6, "priority_score": 5.33, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_name": "Self-Improving LLM Frameworks for Robust Optimization Model Synthesis", "front_status": "stable", "methods": ["multi_agent_system", "llm_code_generation", "llm_as_evaluator", "llm_in_the_loop", "retrieval_augmented_generation"], "problems": ["optimization_modeling", "mixed_integer_linear_programming", "linear_programming", "integer_programming", "non_linear_programming"], "code_url": null, "brief": "MIRROR is a multi-agent framework that translates natural language OR problems into Gurobi code using Hierarchical RAG (metadata filtering + semantic search) and an iterative repair loop. It achieves ~72% pass@1 across five benchmarks, outperforming Chain-of-Experts and fine-tuned models like LLMOPT without task-specific training. The key takeaway is their **structured revision tip mechanism**: upon execution failure, the agent generates a JSON object explicitly isolating the `error_statement`, `incorrect_code_snippet`, and `correct_code_snippet`, which serves as a precise memory artifact for subsequent retries. This structured reflection pattern is superior to raw error logs and could be immediately adopted in our own code generation pipelines.", "affiliations": "Xi'an Jiaotong University, Northwestern Polytechnical University", "analysis_date": "2026-02-15"}, {"arxiv_id": "2602.02029", "arxiv_url": "https://arxiv.org/abs/2602.02029", "title": "Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation", "authors_short": "Zhongyuan Lyu et.al.", "published_date": "2026-02-02", "m_score": 7, "p_score": 8, "i_score": 7, "priority_score": 6.83, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_name": "Self-Improving LLM Agents for Iterative OR Program Synthesis", "front_status": "stable", "methods": ["llm_as_evaluator", "llm_code_generation", "llm_in_the_loop", "retrieval_augmented_generation", "multi_agent_system"], "problems": ["optimization_problem_formulation", "code_generation", "milp_general"], "code_url": null, "brief": "Lyu et al. propose a 'Canonical Intermediate Representation' (CIR) to decouple natural language operational rules from their mathematical instantiation, explicitly forcing the LLM to select modeling paradigms (e.g., time-indexed vs. continuous flow) before coding. They achieve state-of-the-art accuracy (47.2% vs 22.4% baseline) on a new, complex benchmark (ORCOpt-Bench) by using a multi-agent pipeline that retrieves and adapts constraint templates. The key takeaway is the 'Mapper' agent's paradigm selection logic, which prevents common formulation errors in VRPs and scheduling; we should evaluate CIR as a structured mutation space for AlgoEvo to replace brittle code evolution. The new benchmark is immediately relevant for our OR-Bench evaluation suite.", "affiliations": "The Hong Kong Polytechnic University, InfiX.ai", "analysis_date": "2026-02-15"}, {"arxiv_id": "2601.21372", "arxiv_url": "https://arxiv.org/abs/2601.21372", "title": "NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents", "authors_short": "Yang Song et.al.", "published_date": "2026-01-29", "m_score": 9, "p_score": 9, "i_score": 9, "priority_score": 8.62, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_6", "front_name": "Agentic LLM Frameworks for Robust OR Model Synthesis and Validation", "front_status": "stable", "methods": ["llm_code_generation", "llm_in_the_loop", "llm_prompt_optimization", "in_context_learning", "autonomous_coding_agents"], "problems": ["optimization_model_synthesis", "linear_programming", "mixed_integer_linear_programming", "non_linear_programming", "second_order_cone_programming"], "code_url": null, "brief": "NEMO achieves SOTA on 8/9 optimization benchmarks by deploying autonomous coding agents that generate both a declarative optimizer (solver code) and an imperative simulator (verification code). The key innovation is using the simulator to validate the optimizer's results in a closed loop, detecting logical errors without ground truth—a technique that beats fine-tuned models like SIRL by up to 28%. The most stealable insight is this asymmetric validation: imperative Python simulation is often less error-prone than declarative constraint formulation, making it a robust 'critic' for generated solvers. This is immediately applicable to our OR-Bench and AlgoEvo projects for generating reliable reward signals.", "affiliations": "Carnegie Mellon University, C3 AI", "analysis_date": "2026-02-15"}, {"arxiv_id": "2601.17670", "arxiv_url": "https://arxiv.org/abs/2601.17670", "title": "Grammar-Aware Literate Generative Mathematical Programming with Compiler-in-the-Loop", "authors_short": "Roberto Rossi et.al.", "published_date": "2026-01-25", "m_score": 7, "p_score": 9, "i_score": 7, "priority_score": 7.18, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_name": "Self-Improving LLM Frameworks for Robust Optimization Model Synthesis", "front_status": "stable", "methods": ["llm_as_heuristic", "llm_as_evaluator", "llm_in_the_loop", "llm_prompt_optimization", "retrieval_augmented_generation"], "problems": ["generative_mathematical_programming", "linear_programming", "mixed_integer_linear_programming", "stochastic_programming", "two_stage_stochastic_programming"], "code_url": "https://gwr3n.github.io/rhetor/", "brief": "SyntAGM is a framework for translating natural language into Algebraic Modeling Language (PyOPL) code using a 'compiler-in-the-loop' approach, where the LLM is constrained by an in-context BNF grammar and iteratively repairs code based on compiler diagnostics. They demonstrate that this approach matches the accuracy of expensive multi-agent systems (like Chain-of-Experts) while being significantly faster and cheaper. The immediate takeaways for us are the **StochasticOR benchmark** (which we should adopt for RobustMAS) and the technique of **injecting explicit BNF grammars** into prompts to enforce syntax in evolutionary search without fine-tuning. The 'literate modeling' approach—embedding reasoning as comments directly next to code constraints—is also a clever memory mechanism we could steal for AlgoEvo.", "affiliations": "University of Edinburgh, University College Cork", "analysis_date": "2026-02-15"}, {"arxiv_id": "2602.11164", "arxiv_url": "https://arxiv.org/abs/2602.11164", "title": "Automated Optimization Modeling via a Localizable Error-Driven Perspective", "authors_short": "Weiting Liu et.al.", "published_date": "2026-01-17", "m_score": 8, "p_score": 7, "i_score": 8, "priority_score": 7.61, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["error_driven_learning", "data_synthesis", "reverse_data_synthesis", "supervised_fine_tuning", "reinforcement_learning"], "problems": ["automated_optimization_modeling", "program_synthesis", "milp_general", "linear_programming", "mixed_integer_linear_programming"], "code_url": null, "brief": "This paper introduces MIND, a framework for automated optimization modeling that combines error-driven data synthesis with a novel post-training method called DFPO. Instead of standard RLVR which suffers from sparse rewards on hard problems, DFPO uses a teacher model to minimally correct the student's *failed* rollouts, converting them into on-policy(ish) positive samples for SFT/RL. Results show a 7B model outperforming GPT-4 on IndustryOR and OptMATH benchmarks. **Key Takeaway:** We should steal the DFPO mechanism for AlgoEvo: rather than wasting failed evolutionary samples, use a stronger model (or oracle) to fix the code and feed it back as a reward signal, drastically improving sample efficiency in our RL loops.", "affiliations": "Huawei Noah’s Ark Lab, Fudan University, University of Science and Technology of China", "analysis_date": "2026-02-17"}, {"arxiv_id": "2512.18682", "arxiv_url": "https://arxiv.org/abs/2512.18682", "title": "Solver-Independent Automated Problem Formulation via LLMs for High-Cost Simulation-Driven Design", "authors_short": "Yuchen Li et.al.", "published_date": "2025-12-21", "m_score": 7, "p_score": 5, "i_score": 7, "priority_score": 4.99, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_9", "front_name": "LLM-Guided Search and Fine-Tuning for OR Model and Algorithm Synthesis", "front_status": "stable", "methods": ["supervised_learning", "llm_fine_tuned", "synthetic_data_generation", "data_augmentation", "llm_code_generation"], "problems": ["automated_problem_formulation", "expensive_black_box_optimization", "antenna_design", "constrained_optimization"], "code_url": null, "brief": "Li et al. propose APF, a framework to fine-tune LLMs for translating engineering requirements into optimization code without running expensive simulations during training. They generate synthetic training data and filter it by checking if the generated code ranks historical data instances similarly to how an LLM 'judge' ranks them based on the text requirements. Results show 7B models outperforming GPT-4o on antenna design tasks, validated by actual simulation. **Key Takeaway:** We can replace expensive ground-truth evaluations in our process reward models by checking consistency between generated code outputs and LLM-predicted rankings on cached historical data—a direct method to improve sample efficiency in AlgoEvo.", "affiliations": "Xidian University, Victoria University of Wellington, Westlake University", "analysis_date": "2026-02-15"}, {"arxiv_id": "2512.11270", "arxiv_url": "https://arxiv.org/abs/2512.11270", "title": "A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation", "authors_short": "Hong Je-Gal et.al.", "published_date": "2025-12-12", "m_score": 5, "p_score": 7, "i_score": 6, "priority_score": 4.74, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["multi_agent_llm_system", "llm_code_generation", "llm_as_evaluator", "llm_in_the_loop", "program_synthesis"], "problems": ["automated_mdp_modeling", "automated_policy_generation", "cart_pole", "mountain_car", "wireless_network_scheduling"], "code_url": null, "brief": "A-LAMP decomposes the translation of natural language task descriptions into executable RL environments via a multi-agent pipeline, separating parameter extraction, variable definition, and constraint formulation before code generation. The results show that this structured approach allows a 27B model to rival GPT-4o on simple tasks, though the benchmarks (e.g., grid-world drone delivery, trivial wireless scheduling) are toy-scale and the RL application is sometimes forced. The primary takeaway is the specific decomposition schema for symbolic modeling: we should steal their granular extraction pipeline (Parameters -> Objectives -> Variables -> Constraints) to improve the reliability of our automated problem instantiation in OR-Bench and AlgoEvo without relying solely on expensive frontier models.", "affiliations": "Sejong University", "analysis_date": "2026-02-15"}, {"arxiv_id": "2511.16383", "arxiv_url": "https://arxiv.org/abs/2511.16383", "title": "An Agent-Based Framework for the Automatic Validation of Mathematical Optimization Models", "authors_short": "Alexander Zadorojniy et.al.", "published_date": "2025-11-20", "m_score": 7, "p_score": 8, "i_score": 7, "priority_score": 6.74, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_6", "front_name": "Agentic LLM Frameworks for Robust OR Model Synthesis and Validation", "front_status": "stable", "methods": ["multi_agent_system", "llm_as_heuristic", "llm_code_generation", "software_testing", "mutation_testing"], "problems": ["optimization_model_validation", "llm_generated_code_validation", "linear_programming", "resource_allocation", "scheduling"], "code_url": null, "brief": "Zadorojniy et al. introduce a multi-agent framework for validating LLM-generated optimization models by generating a test suite and verifying the suite's quality via mutation testing (ensuring tests detect deliberate errors injected into the model). On 100 NLP4LP instances, they achieve a 76% mutation kill ratio and successfully classify external models where simple objective value comparisons fail. The critical takeaway is the 'bootstrapped validation' workflow: using mutation analysis to validate the generated unit tests themselves before using them to score the model. We should steal this mutation-based verification loop to create a robust, ground-truth-free fitness signal for our evolutionary search and OR benchmarking pipelines.", "affiliations": "IBM Research", "analysis_date": "2026-02-15"}, {"arxiv_id": "2511.00685", "arxiv_url": "https://arxiv.org/abs/2511.00685", "title": "SOCRATES: Simulation Optimization with Correlated Replicas and Adaptive Trajectory Evaluations", "authors_short": "Haoting Zhang et.al.", "published_date": "2025-11-01", "m_score": 8, "p_score": 7, "i_score": 8, "priority_score": 6.94, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["simulation_optimization", "llm_in_the_loop", "llm_as_meta_optimizer", "causal_discovery", "expectation_maximization"], "problems": ["automated_algorithm_design", "heuristic_evolution", "simulation_optimization", "multi_sku_base_stock_optimization", "multi_server_queuing_network_resource_routing"], "code_url": null, "brief": "SOCRATES introduces a two-stage framework: first constructing 'Operational AI Replicas' (surrogates) via LLM-guided causal discovery, then using an LLM to analyze optimization trajectories on these surrogates to schedule hybrid algorithms (e.g., running BO then switching to GA). While the benchmarks (inventory, queuing) are simple and the causal inference step seems fragile, the core innovation of **trajectory-based reasoning** is highly transferable. We can steal this mechanism for AlgoEvo: instead of blind evolution, our planner agent should consume the optimization trajectory to dynamically swap operators or restart populations when stagnation is detected, effectively using the LLM as a process reward model.", "affiliations": "Columbia, UC Berkeley, Amazon", "analysis_date": "2026-02-15"}, {"arxiv_id": "2510.27610", "arxiv_url": "https://arxiv.org/abs/2510.27610", "title": "ORGEval: Graph-Theoretic Evaluation of LLMs in Optimization Modeling", "authors_short": "Zhuohan Wang et.al.", "published_date": "2025-10-31", "m_score": 8, "p_score": 9, "i_score": 8, "priority_score": 8.11, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_1", "front_name": "LLM-Enhanced Optimization Modeling via SIRL, MCTS, and Graph-Theoretic Evaluation", "front_status": "stable", "methods": ["graph_theory", "weisfeiler_lehman_test", "graph_isomorphism", "bipartite_graphs", "hashing"], "problems": ["llm_evaluation", "milp_modeling", "lp_modeling", "model_equivalence_detection"], "code_url": null, "brief": "Wang et al. propose ORGEval, a framework that evaluates LLM-generated optimization models by converting them into bipartite graphs and using the Weisfeiler-Lehman (WL) test to detect isomorphism with a ground truth, rather than solving the instances. They prove that for 'symmetric decomposable' graphs, this method is guaranteed to detect equivalence correctly, achieving 100% consistency and running in seconds compared to hours for solver-based checks on hard MIPLIB instances. The critical takeaway is the shift from execution-based to **structural evaluation**: we can validate model logic via graph topology ($O(k(m+n)^2)$) without incurring the cost of solving NP-hard problems. This is immediately actionable for our OR benchmarking pipelines and could serve as a rapid 'pre-solve' filter in our evolutionary search loops to reject structurally invalid candidates instantly.", "affiliations": "The Chinese University of Hong Kong, Shenzhen, Shenzhen Research Institute of Big Data, Shenzhen International Center for Industrial and Applied Mathematics, Shenzhen Loop Area Institute", "analysis_date": "2026-02-15"}, {"arxiv_id": "2510.18428", "arxiv_url": "https://arxiv.org/abs/2510.18428", "title": "AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library", "authors_short": "Minwei Kong et.al.", "published_date": "2025-10-21", "m_score": 8, "p_score": 9, "i_score": 8, "priority_score": 8.11, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_name": "Self-Improving LLM Frameworks for Robust Optimization Model Synthesis", "front_status": "stable", "methods": ["llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "llm_evolutionary_search", "llm_as_evaluator"], "problems": ["optimization_program_formulation", "milp_general", "nlp_general", "combinatorial_optimization", "heuristic_evolution"], "code_url": "https://github.com/Minw913/AlphaOPT", "brief": "AlphaOPT introduces a 'Library Evolution' mechanism that iteratively refines the *applicability conditions* of cached optimization insights based on solver feedback, allowing it to learn from answers alone (no gold programs). On OOD benchmarks like OptiBench, it beats fine-tuned models (ORLM) by ~13% and shows consistent scaling with data size. **Key Takeaway:** The specific mechanism of diagnosing 'unretrieved' vs. 'negative' tasks to rewrite retrieval triggers is a transferable technique for our AlgoEvo memory; it solves the problem of heuristic misapplication in long-term search. We should implement this 'condition refinement' loop immediately to improve our multi-agent memory systems.", "affiliations": "Massachusetts Institute of Technology, London School of Economics and Political Science, University of Florida, Northeastern University, Singapore Management University, Singapore-MIT Alliance for Research and Technology", "analysis_date": "2026-02-15"}, {"arxiv_id": "2510.16916", "arxiv_url": "https://arxiv.org/abs/2510.16916", "title": "SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search", "authors_short": "Dong Li et.al.", "published_date": "2025-10-19", "m_score": 8, "p_score": 7, "i_score": 8, "priority_score": 6.94, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_name": "Self-Improving LLM Agents for Iterative OR Program Synthesis", "front_status": "stable", "methods": ["monte_carlo_tree_search", "llm_as_heuristic", "llm_as_evaluator", "llm_code_generation", "llm_in_the_loop"], "problems": ["optimization_problem_formulation", "linear_programming", "mixed_integer_linear_programming", "traveling_salesman_problem"], "code_url": null, "brief": "SolverLLM frames optimization problem formulation as a hierarchical Monte Carlo Tree Search (MCTS), decomposing the task into six layers (variables, constraints, etc.) and using test-time compute to beat fine-tuned baselines like LLMOPT. The results appear robust, showing ~10% gains on complex datasets, though inference cost is high. **The critical takeaway for us is the 'Prompt Backpropagation' mechanism:** instead of just updating numerical values, they propagate textual error analysis from leaf nodes back up the tree to dynamically modify the prompts of parent nodes, effectively creating 'short-term memory' for the search. We should immediately test this technique in AlgoEvo to prevent the recurrence of failed code patterns during mutation steps. Additionally, their use of semantic entropy to down-weight uncertain rewards in MCTS is a practical solution to the noisy evaluation problem we face in process reward models.", "affiliations": "NEC Labs America, Baylor University, University of Texas at Dallas, Augusta University, Southern Illinois University", "analysis_date": "2026-02-15"}, {"arxiv_id": "2510.15969", "arxiv_url": "https://arxiv.org/abs/2510.15969", "title": "LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems", "authors_short": "Paul-Niklas Ken Kandora et.al.", "published_date": "2025-10-12", "m_score": 7, "p_score": 8, "i_score": 7, "priority_score": 5.73, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["llm_research_agent", "multi_agent_system", "exact_linearization", "milp_reformulation", "pattern_detection"], "problems": ["nonlinear_optimization_problem_linearization", "minlp", "bilinear_products", "min_operator", "max_operator"], "code_url": null, "brief": "LinearizeLLM is a multi-agent framework that converts LaTeX nonlinear optimization problems into exact MILP formulations by detecting nonlinear terms and processing them bottom-up based on nesting depth. On 40 benchmark instances, it achieves 73% end-to-end success compared to <15% for one-shot LLMs and Pyomo baselines, demonstrating that structural decomposition is essential for handling complex nested terms. The key takeaway is the 'Structural Policy': rather than letting the LLM plan the reformulation order, they enforce a deterministic bottom-up traversal (linearizing children before parents). We should steal this hybrid approach—using deterministic graph traversal to orchestrate LLM manipulation steps—to improve reliability in our symbolic modeling and EvoCut pipelines.", "affiliations": "Karlsruhe Institute of Technology, Reutlingen University", "analysis_date": "2026-02-15"}, {"arxiv_id": "2510.04204", "arxiv_url": "https://arxiv.org/abs/2510.04204", "title": "CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling", "authors_short": "Zhengyang Tang et.al.", "published_date": "2025-10-05", "m_score": 8, "p_score": 9, "i_score": 8, "priority_score": 8.27, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_name": "Self-Improving LLM Frameworks for Robust Optimization Model Synthesis", "front_status": "stable", "methods": ["llm_as_heuristic", "llm_as_evaluator", "llm_code_generation", "llm_in_the_loop", "llm_fine_tuned"], "problems": ["automated_optimization_modeling", "mathematical_modeling", "milp_general", "lp_formulation"], "code_url": "https://github.com/tangzhy/STORM", "brief": "Tang et al. propose CALM, a framework that uses an expert 'Intervener' model to inject corrective hints into a small LRM's reasoning trace (e.g., forcing it to use Python instead of manual calculation), followed by SFT and RL (GRPO). Results are strong and verified: a 4B model matches DeepSeek-R1 (671B) on OR benchmarks, specifically fixing the 'Code Utilization Distrust' we see in our own agents. The key takeaway is the 'Intervener' loop: instead of discarding failed traces, they repair them with hints to create a 'golden' reasoning dataset that preserves the 'thinking' process while enforcing tool use. This is a direct, actionable method for improving our AlgoEvo agents' reliability in generating executable heuristics without massive human annotation.", "affiliations": "Qwen Team, Alibaba Inc., The Chinese University of Hong Kong, Shenzhen, Southern University of Science and Technology, Shanghai University of Finance and Economics, Shenzhen Loop Area Institute (SLAI)", "analysis_date": "2026-02-15"}, {"arxiv_id": "2510.02679", "arxiv_url": "https://arxiv.org/abs/2510.02679", "title": "Automated Constraint Specification for Job Scheduling by Regulating Generative Model with Domain-Specific Representation", "authors_short": "Yu-Zhe Shi et.al.", "published_date": "2025-10-03", "m_score": 7, "p_score": 6, "i_score": 7, "priority_score": 5.24, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["generative_models", "large_language_models", "domain_specific_languages", "llm_in_the_loop", "llm_as_heuristic"], "problems": ["job_shop_scheduling", "constraint_specification", "automated_dsl_design"], "code_url": "null", "brief": "This paper proposes a constraint-centric architecture that translates natural language manufacturing descriptions into Job Shop Scheduling (JSP) constraints by mediating through a learned Domain-Specific Language (DSL). Unlike standard prompting, they implement an automated DSL adaptation algorithm using non-parametric modeling (DPMM) and Expectation-Maximization to learn the syntax and semantics of the intermediate representation from data, which is then verified via a Pushdown Automaton. While the experiments rely on synthetic data augmented from standard benchmarks (a weakness), the methodology for **automatically deriving the intermediate representation** rather than hand-coding it is a transferable insight. We could steal this 'automated DSL design' approach to dynamically construct search spaces for AlgoEvo or to improve the robustness of NL-to-OR translation in OR-Bench.", "affiliations": "Peking University, The Hong Kong University of Science and Technology, Huazhong University of Science and Technology, University of Science and Technology of China", "analysis_date": "2026-02-15"}, {"arxiv_id": "2510.05115", "arxiv_url": "https://arxiv.org/abs/2510.05115", "title": "SAC-Opt: Semantic Anchors for Iterative Correction in Optimization Modeling", "authors_short": "Yansen Zhang et.al.", "published_date": "2025-09-28", "m_score": 7, "p_score": 8, "i_score": 6, "priority_score": 5.58, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_name": "Self-Improving LLM Frameworks for Robust Optimization Model Synthesis", "front_status": "stable", "methods": ["llm_code_generation", "llm_as_evaluator", "llm_in_the_loop", "iterative_correction", "semantic_alignment"], "problems": ["optimization_modeling", "program_synthesis", "semantic_error_detection", "linear_programming", "combinatorial_optimization"], "code_url": "https://github.com/Forrest-Stone/SAC-Opt", "brief": "SAC-Opt introduces a verification loop where generated Gurobi code is back-translated into natural language ('semantic anchors') to check for alignment with the original problem description. Empirical results are strong, demonstrating a ~22% accuracy improvement on the ComplexLP dataset over OptiMUS-0.3 by catching logic errors that solver feedback misses. The primary takeaway is the utility of granular, constraint-level back-translation as a process reward signal, which we should adopt to improve the reliability of our automated modeling agents.", "affiliations": "Huawei Noah’s Ark Lab, Huawei’s Supply Chain Management Department, City University of Hong Kong", "analysis_date": "2026-02-15"}, {"arxiv_id": "2509.22979", "arxiv_url": "https://arxiv.org/abs/2509.22979", "title": "OptiMind: Teaching LLMs to Think Like Optimization Experts", "authors_short": "Zeyi Chen et.al.", "published_date": "2025-09-26", "m_score": 5, "p_score": 9, "i_score": 7, "priority_score": 7.06, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_9", "front_name": "LLM-Guided Search and Fine-Tuning for OR Model and Algorithm Synthesis", "front_status": "stable", "methods": ["supervised_fine_tuning", "llm_code_generation", "llm_in_the_loop", "llm_as_evaluator", "error_analysis"], "problems": ["mixed_integer_linear_programming", "optimization_modeling", "traveling_salesman_problem", "set_cover", "flow_shop_scheduling"], "code_url": null, "brief": "The authors fine-tune a 20B model for MILP formulation, but the critical contribution is a rigorous audit of standard benchmarks (IndustryOR, OptMATH), revealing that 30-50% of instances are flawed (missing data, wrong ground truth, infeasible). They introduce a 'class-based error analysis' where the model classifies a problem (e.g., TSP) and retrieves specific, expert-written hints to avoid common pitfalls, boosting accuracy by ~20%. **Takeaway:** We must immediately replace our benchmark versions with their cleaned sets for the OR-Bench project. Additionally, their library of 'error hints' per problem class is a high-value artifact we can scrape and inject into AlgoEvo's prompt templates to improve initial population quality.", "affiliations": "Microsoft Research, Stanford University, University of Washington", "analysis_date": "2026-02-15"}, {"arxiv_id": "2509.22558", "arxiv_url": "https://arxiv.org/abs/2509.22558", "title": "StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models", "authors_short": "Chenyu Zhou et.al.", "published_date": "2025-09-26", "m_score": 9, "p_score": 8, "i_score": 9, "priority_score": 8.46, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_name": "Self-Improving LLM Frameworks for Robust Optimization Model Synthesis", "front_status": "stable", "methods": ["llm_as_heuristic", "llm_as_evaluator", "llm_code_generation", "llm_in_the_loop", "llm_evolutionary_search"], "problems": ["operations_research_modeling", "linear_programming", "mixed_integer_linear_programming", "traveling_salesman_problem", "combinatorial_optimization"], "code_url": null, "brief": "Zhou et al. propose StepORLM, a framework where an 8B policy and a **Generative Process Reward Model (GenPRM)** co-evolve. Unlike standard discriminative PRMs that score steps in isolation, their GenPRM generates a reasoning trace to evaluate the full trajectory's logic before assigning credit, addressing the interdependency of OR constraints. They align the policy using **Weighted DPO**, where preference weights are derived from the GenPRM's process scores. They claim to beat GPT-4o and DeepSeek-V3 on 6 OR benchmarks (e.g., NL4Opt, MAMO) with an 8B model. **Key Takeaway:** We should test **Generative PRMs** immediately for AlgoEvo; asking the critic to 'explain then score' (generative) rather than just 'score' (discriminative) likely fixes the credit assignment noise in our long-horizon search.", "affiliations": "Shanghai Jiao Tong University", "analysis_date": "2026-02-15"}, {"arxiv_id": "2511.11576", "arxiv_url": "https://arxiv.org/abs/2511.11576", "title": "DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs", "authors_short": "WenZhuo Zhu et.al.", "published_date": "2025-09-24", "m_score": 6, "p_score": 8, "i_score": 7, "priority_score": 6.44, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_name": "Self-Improving LLM Frameworks for Robust Optimization Model Synthesis", "front_status": "stable", "methods": ["llm_code_generation", "llm_as_evaluator", "multi_agent_system", "few_shot_learning", "stochastic_optimization"], "problems": ["optimization_modeling", "data_driven_optimization_under_uncertainty", "stochastic_optimization_modeling", "robust_optimization_modeling", "distributionally_robust_optimization_modeling"], "code_url": "https://anonymous.4open.science/r/LLM-for-data-driven-optimization-problems-9528", "brief": "Zhu et al. propose DAOpt, a framework for modeling optimization under uncertainty that integrates LLMs with the RSOME library to handle robust and stochastic formulations. Their experiments on a new dataset (OptU) convincingly demonstrate that standard LLM-generated deterministic models suffer from the 'optimizer's curse,' achieving only ~27% out-of-sample feasibility, whereas their robust approach achieves >70%. The critical takeaway for us is to **stop asking LLMs to derive mathematical duals or robust counterparts**; instead, we should train them to use high-level DSLs (like RSOME) that handle the duality internally. This is an immediate action item for our RobustMAS project to ensure generated solutions are actually executable in stochastic environments.", "affiliations": "Zhejiang University, University of Toronto, Peking University", "analysis_date": "2026-02-15"}, {"arxiv_id": "2509.08970", "arxiv_url": "https://arxiv.org/abs/2509.08970", "title": "Gala: Global LLM Agents for Text-to-Model Translation", "authors_short": "Junyang Cai et.al.", "published_date": "2025-09-10", "m_score": 5, "p_score": 8, "i_score": 6, "priority_score": 4.99, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_7", "front_name": "Hierarchical RAG and Multi-Agent LLMs for Optimization Model Synthesis", "front_status": "stable", "methods": ["llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "multi_agent_llm_system", "constraint_programming"], "problems": ["text_to_model_translation", "constraint_programming_modeling", "natural_language_for_optimization"], "code_url": null, "brief": "GALA decomposes text-to-MiniZinc translation into a multi-agent system where specialized agents detect specific Constraint Programming global constraints (e.g., all_different, cumulative) before an assembler unifies them. Results on 110 TEXT2ZINC instances show a modest improvement over CoT (57% vs 52% execution rate with o3-mini), though the sample size is small and lacks statistical rigor. The key takeaway is the architectural shift from generic 'coder/reviewer' roles to 'primitive-specific' agents, which aligns LLM reasoning with the target formalism's structure. We should test this 'primitive-based decomposition' in our OR-Bench pipeline to see if it reduces hallucination of complex constraints better than our current methods.", "affiliations": "University of Southern California, Brown University, Fidelity Investments", "analysis_date": "2026-02-15"}, {"arxiv_id": "2508.14544", "arxiv_url": "https://arxiv.org/abs/2508.14544", "title": "Adaptively Robust LLM Inference Optimization under Prediction Uncertainty", "authors_short": "Zixi Chen et.al.", "published_date": "2025-08-20", "m_score": 7, "p_score": 9, "i_score": 6, "priority_score": 7.51, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_6", "front_name": "Agentic LLM Frameworks for Robust OR Model Synthesis and Validation", "front_status": "stable", "methods": ["online_scheduling", "robust_optimization", "competitive_analysis", "greedy_algorithm", "adaptive_algorithms"], "problems": ["llm_inference_scheduling", "resource_constrained_scheduling", "online_scheduling", "latency_minimization"], "code_url": null, "brief": "Chen et al. propose $A_{min}$, an online scheduling algorithm for LLM inference that handles unknown output lengths by optimistically assuming the lower bound and evicting jobs (based on accumulated length) if memory overflows. They prove a logarithmic competitive ratio and show via simulations on LMSYS-Chat-1M that this approach nearly matches hindsight-optimal scheduling, vastly outperforming conservative upper-bound baselines. **Key Takeaway:** For our **GPUSched** project, we should abandon conservative memory reservation for output tokens; instead, implement an optimistic scheduler that oversubscribes memory and handles overflows via their ordered eviction policy, as the cost of restart is theoretically bounded and empirically negligible compared to the throughput gains.", "affiliations": "Stanford University, Peking University, HKUST", "analysis_date": "2026-02-17"}, {"arxiv_id": "2508.11850", "arxiv_url": "https://arxiv.org/abs/2508.11850", "title": "EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models", "authors_short": "M. Yazdani et.al.", "published_date": "2025-08-16", "m_score": 9, "p_score": 10, "i_score": 9, "priority_score": 8.81, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_name": "Self-Improving LLM Agents for Iterative OR Program Synthesis", "front_status": "stable", "methods": ["evolutionary_algorithm", "llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "evolution_of_heuristics"], "problems": ["MILP_general", "TSP", "MCND", "CWLP", "job_shop_scheduling"], "code_url": "https://github.com/milad1378yz/EvoCut", "brief": "Yazdani et al. introduce EvoCut, an evolutionary framework where LLMs generate Python code for MILP cuts, filtered by a 'usefulness check' (does it cut the current LP relaxation?) and an 'empirical validity check' (does it preserve known integer optima?). They report 17-57% gap reductions on TSPLIB and JSSP compared to Gurobi defaults, backed by strong ablation studies on the evolutionary operators. **Key Takeaway:** The reliance on 'acceleration cuts'—constraints verified empirically on small datasets rather than formally proven—bypasses the bottleneck of automated theorem proving while still delivering valid speedups. We should immediately adopt their 'LP separation' check as a cheap, high-signal reward for our own evolutionary search loops.", "affiliations": "Huawei Technologies Canada, University of British Columbia, University of Toronto", "analysis_date": "2026-02-15"}, {"arxiv_id": "2508.07468", "arxiv_url": "https://arxiv.org/abs/2508.07468", "title": "CP-Agent: Agentic Constraint Programming", "authors_short": "Stefan Szeider et.al.", "published_date": "2025-08-10", "m_score": 5, "p_score": 9, "i_score": 7, "priority_score": 6.39, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_6", "front_name": "Agentic LLM Frameworks for Robust OR Model Synthesis and Validation", "front_status": "stable", "methods": ["llm_agent", "react_framework", "ipython_kernel", "iterative_refinement", "constraint_programming"], "problems": ["natural_language_to_code", "constraint_modeling", "combinatorial_optimization", "combinatorial_satisfaction", "scheduling"], "code_url": "https://github.com/szeider/agentic-python-coder", "brief": "Szeider implements a standard ReAct agent with a persistent IPython kernel to iteratively generate and refine CPMpy models, claiming 100% accuracy on CP-Bench. However, this perfect score is achieved on a *modified* version of the benchmark where the author manually fixed 31 ambiguous problem statements and 19 ground-truth errors—making the '100%' result an artifact of dataset cleaning rather than pure model capability. The most actionable takeaways are the negative result for explicit 'task management' tools (which hurt performance on hard problems) and the effectiveness of a minimal (<50 lines) domain prompt over complex scaffolding. We should review their clarified benchmark for our OR-Bench work.", "affiliations": "TU Wien", "analysis_date": "2026-02-15"}, {"arxiv_id": "2508.03117", "arxiv_url": "https://arxiv.org/abs/2508.03117", "title": "Toward a Trustworthy Optimization Modeling Agent via Verifiable Synthetic Data Generation", "authors_short": "Vinicius Lima et.al.", "published_date": "2025-08-05", "m_score": 6, "p_score": 7, "i_score": 7, "priority_score": 5.38, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_name": "Self-Improving LLM Agents for Iterative OR Program Synthesis", "front_status": "stable", "methods": ["llm_as_heuristic", "llm_code_generation", "llm_fine_tuned", "llm_in_the_loop", "llm_as_evaluator"], "problems": ["natural_language_to_optimization_modeling", "linear_programming", "mixed_integer_linear_programming", "combinatorial_optimization", "dataset_curation"], "code_url": null, "brief": "Lima et al. introduce a pipeline to generate synthetic optimization datasets by starting with symbolic MILP instances (ground truth) and using LLMs to generate natural language descriptions, ensuring full verifiability. They fine-tune a small model (Granite 8B) that beats GPT-4 on 6/7 benchmarks, largely due to a 'majority vote' mechanism where the agent generates code in 5 different modeling languages (Pyomo, Gurobi, etc.) and checks for result consistency. **Takeaway:** We should steal the multi-language execution voting to boost robustness in our code generation agents. Furthermore, their reverse-generation (Symbolic $\\to$ NL) strategy is the correct approach for generating infinite, error-free test cases for our OR-Bench work.", "affiliations": "IBM Research AI", "analysis_date": "2026-02-15"}, {"arxiv_id": "2507.14995", "arxiv_url": "https://arxiv.org/abs/2507.14995", "title": "LLM-Enhanced Multi-Agent Reinforcement Learning with Expert Workflow for Real-Time P2P Energy Trading", "authors_short": "C. Lou et.al.", "published_date": "2025-07-20", "m_score": 7, "p_score": 6, "i_score": 7, "priority_score": 5.24, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["multi_agent_reinforcement_learning", "actor_critic", "imitation_learning", "deep_reinforcement_learning", "llm_as_expert"], "problems": ["p2p_energy_trading", "resource_allocation", "multi_agent_coordination", "voltage_control", "distribution_network_security"], "code_url": "https://github.com/jzk0806/P2P-llm-supplementary", "brief": "This paper proposes a neurosymbolic MARL framework for P2P energy trading where LLMs generate CVXPY optimization models to act as 'experts' for RL agents to imitate via Wasserstein distance. They introduce a 'Differential Attention' mechanism in the critic that subtracts attention maps to filter noise, enabling scalability to 100 agents where standard baselines fail. **Takeaway:** We should steal the Differential Attention architecture for our multi-agent critics to handle irrelevant interactions in large-scale optimization. The workflow of using LLMs to write the *solver* (generating reliable synthetic data) rather than the *solution* is a transferable strategy for bootstrapping RL in our OR domains.", "affiliations": "China Agricultural University, University of Glasgow, Guangdong University of Foreign Studies", "analysis_date": "2026-02-15"}, {"arxiv_id": "2507.11737", "arxiv_url": "https://arxiv.org/abs/2507.11737", "title": "Auto-Formulating Dynamic Programming Problems with Large Language Models", "authors_short": "Chenyu Zhou et.al.", "published_date": "2025-07-15", "m_score": 8, "p_score": 7, "i_score": 8, "priority_score": 7.15, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_name": "Self-Improving LLM Agents for Iterative OR Program Synthesis", "front_status": "stable", "methods": ["dynamic_programming", "llm_code_generation", "llm_as_evaluator", "llm_fine_tuned", "llm_in_the_loop"], "problems": ["dynamic_programming_formulation", "stochastic_dynamic_programming", "finite_horizon_dp", "infinite_horizon_dp", "operations_research_modeling"], "code_url": null, "brief": "Zhou et al. introduce DPLM, a 7B model fine-tuned to formulate Dynamic Programming models, achieving performance comparable to o1 on their new DP-Bench. Their key contribution is 'DualReflect,' a synthetic data pipeline that combines Forward Generation (Problem→Code) for diversity with Backward Generation (Code→Problem) for correctness. **Takeaway:** We should steal the Backward Generation approach for AlgoEvo: instead of relying on noisy forward generation, we can take valid heuristics/OR code (which we have in abundance) and reverse-engineer problem descriptions to create massive, verifiable synthetic datasets for fine-tuning our code generation models. The paper proves this method is superior for 'cold-starting' small models in data-scarce domains.", "affiliations": "University of Chicago, Cornell University, Shanghai Jiao Tong University, Shanghai University of Finance and Economics, Cardinal Operations", "analysis_date": "2026-02-15"}, {"arxiv_id": "2507.10614", "arxiv_url": "https://arxiv.org/abs/2507.10614", "title": "Fine-tuning Large Language Model for Automated Algorithm Design", "authors_short": "Fei Liu et.al.", "published_date": "2025-07-13", "m_score": 7, "p_score": 10, "i_score": 8, "priority_score": 7.39, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_name": "Self-Improving LLM Agents for Iterative OR Program Synthesis", "front_status": "stable", "methods": ["llm_fine_tuned", "llm_in_the_loop", "llm_code_generation", "direct_preference_optimization", "diversity_aware_rank_based_sampling"], "problems": ["automated_algorithm_design", "admissible_set_problem", "cvrp", "tsp", "heuristic_evolution"], "code_url": null, "brief": "Liu et al. introduce a fine-tuning pipeline for LLMs in automated algorithm design, utilizing a 'Diversity-Aware Rank-based' sampling strategy to construct DPO preference pairs from evolutionary search histories. By partitioning the population into ranked subsets and sampling pairs with a guaranteed quality gap (skipping adjacent tiers), they ensure training signals are both clear and diverse. Empirically, they show that a fine-tuned Llama-3.2-1B matches the performance of a base Llama-3.1-8B on ASP and CVRP tasks, effectively compressing the search capability into a much cheaper model. We should implement this sampling strategy to recycle our AlgoEvo run logs into specialized 'mutator' models, potentially allowing us to downscale to 1B/3B models for the inner search loop without losing quality.", "affiliations": "City University of Hong Kong", "analysis_date": "2026-02-15"}, {"arxiv_id": "2506.07972", "arxiv_url": "https://arxiv.org/abs/2506.07972", "title": "HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization", "authors_short": "Hongzheng Chen et.al.", "published_date": "2025-06-09", "m_score": 5, "p_score": 9, "i_score": 7, "priority_score": 6.59, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_6", "front_name": "Agentic LLM Frameworks for Robust OR Model Synthesis and Validation", "front_status": "stable", "methods": ["llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "in_context_learning", "agentic_framework"], "problems": ["operator_scheduling", "technology_mapping", "global_routing", "e_graph_extraction", "intra_operator_parallelism"], "code_url": "https://github.com/cornell-zhang/heurigym", "brief": "The authors introduce HeuriGym, a benchmark suite of 9 hard combinatorial optimization problems (including PDPTW, EDA scheduling, and routing) coupled with an agentic evaluation loop. Results are backed by extensive experiments showing that SOTA LLMs saturate at ~60% of expert performance and, significantly, that existing evolutionary frameworks (ReEvo, EoH) perform *worse* than simple prompting on these large-context tasks (300+ lines of code). The key takeaway is the failure mode of current evolutionary methods: they cannot handle the context fragmentation and feedback integration required for complex heuristic design. We should immediately adopt this benchmark to demonstrate AlgoEvo's superiority, as the current baselines are weak and the problem set aligns perfectly with our focus.", "affiliations": "Cornell University, Harvard University, NVIDIA", "analysis_date": "2026-02-15"}, {"arxiv_id": "2506.07759", "arxiv_url": "https://arxiv.org/abs/2506.07759", "title": "REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models", "authors_short": "Diego Forni'es-Tabuenca et.al.", "published_date": "2025-06-09", "m_score": 7, "p_score": 6, "i_score": 7, "priority_score": 6.43, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_6", "front_name": "Agentic LLM Frameworks for Robust OR Model Synthesis and Validation", "front_status": "stable", "methods": ["nsga_ii", "llm_as_heuristic", "llm_code_generation", "k_means_clustering", "evolutionary_algorithm"], "problems": ["flexible_job_shop_scheduling", "multi_objective_scheduling", "makespan_minimization", "workload_balancing", "operation_separation"], "code_url": null, "brief": "Forniés-Tabuenca et al. propose REMoH, an LLM-driven evolutionary framework for multi-objective FJSSP that uses K-Means to cluster the population by objective performance before generating reflections. While their optimality gaps (~12%) trail behind state-of-the-art CP solvers (~1.5%), the ablation study confirms that their reflection mechanism significantly improves Pareto front diversity (Hypervolume). **The killer feature is the phenotypic clustering step:** instead of reflecting on a random or elitist subset, they group solutions by trade-offs (e.g., 'low makespan' vs 'balanced') to generate targeted prompts. We should implement this clustering-based context construction in AlgoEvo to improve diversity maintenance in multi-objective search without exploding token costs.", "affiliations": "Vicomtech Foundation, University of the Basque Country, Universidad EAFIT, HiTZ Basque Center for Language Technology", "analysis_date": "2026-02-15"}, {"arxiv_id": "2506.06052", "arxiv_url": "https://arxiv.org/abs/2506.06052", "title": "DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems", "authors_short": "Kostis Michailidis et.al.", "published_date": "2025-06-06", "m_score": 5, "p_score": 8, "i_score": 7, "priority_score": 6.02, "must_read": false, "changes_thinking": true, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_name": "Self-Improving LLM Frameworks for Robust Optimization Model Synthesis", "front_status": "stable", "methods": ["llm_code_generation", "llm_prompt_optimization", "llm_in_the_loop", "retrieval_augmented_in_context_learning", "repeated_sampling"], "problems": ["constraint_modelling", "program_synthesis", "discrete_combinatorial_problems"], "code_url": "https://github.com/kostis-init/CP-Bench", "brief": "This paper introduces DCP-Bench-Open, a benchmark of 164 discrete combinatorial problems, to evaluate LLMs on translating natural language into constraint models (CPMpy, MiniZinc, OR-Tools). The results are rigorous and highlight a critical failure mode: LLMs overfit to the specific data values in the prompt's example instance, causing a ~30% performance drop when evaluated on hidden instances (Multi-Instance Accuracy). Crucially for our pipeline design, they find that Retrieval-Augmented In-Context Learning (RAICL) is ineffective or harmful compared to simply including library documentation in the system prompt. We should adopt their 'Multi-Instance Accuracy' metric immediately for OR-Bench and switch any MiniZinc generation efforts to Python-based frameworks like CPMpy or OR-Tools, which LLMs handle much better.", "affiliations": "KU Leuven, University of Western Macedonia", "analysis_date": "2026-02-15"}, {"arxiv_id": "2505.21775", "arxiv_url": "https://arxiv.org/abs/2505.21775", "title": "DualSchool: How Reliable are LLMs for Optimization Education?", "authors_short": "Michael Klamkin et.al.", "published_date": "2025-05-27", "m_score": 7, "p_score": 5, "i_score": 6, "priority_score": 4.83, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["canonical_graph_edit_distance", "graph_edit_distance", "llm_evaluation", "llm_code_generation", "automatic_symbolic_dualization"], "problems": ["primal_to_dual_conversion", "linear_programming_duality", "llm_reliability_evaluation", "optimization_education", "error_correction"], "code_url": null, "brief": "This paper evaluates LLMs on Primal-to-Dual Conversion (P2DC), introducing a 'Canonical Graph Edit Distance' (CGED) to verify structural correctness while ignoring benign differences like variable ordering or slack conventions. Results show that even strong LLMs often fail (<50% accuracy) and, crucially, that standard execution-based evaluation (checking objective values) produces frequent false positives by missing errors in redundant constraints. The primary takeaway for us is the CGED methodology: a robust way to score symbolic OR model generation that captures structural validity better than execution alone, which we could steal for our benchmarking and evolutionary search fitness functions.", "affiliations": "Georgia Institute of Technology", "analysis_date": "2026-02-15"}, {"arxiv_id": "2505.11792", "arxiv_url": "https://arxiv.org/abs/2505.11792", "title": "Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling", "authors_short": "Yitian Chen et.al.", "published_date": "2025-05-17", "m_score": 9, "p_score": 8, "i_score": 9, "priority_score": 8.31, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_1", "front_name": "LLM-Enhanced Optimization Modeling via SIRL, MCTS, and Graph-Theoretic Evaluation", "front_status": "stable", "methods": ["reinforcement_learning", "reinforce_plus_plus", "llm_code_generation", "llm_in_the_loop", "llm_as_evaluator"], "problems": ["automated_optimization_modeling", "milp_general", "lp_general", "ip_general", "nlp_general"], "code_url": "https://github.com/Cardinal-Operations/SIRL", "brief": "Chen et al. introduce SIRL, a framework for training LLMs to generate optimization models using Reinforcement Learning with Verifiable Rewards (RLVR) and a novel 'Partial KL' surrogate objective. By removing the KL penalty from the reasoning (CoT) section while retaining it for the code generation section, they balance exploration with syntactic stability, achieving SOTA on OptMATH and IndustryOR against OpenAI-o3 and DeepSeek-R1. The critical takeaway for us is the Partial KL strategy: it allows the model to 'think' freely outside the reference distribution while adhering to strict coding standards—a technique we should immediately test in AlgoEvo. Furthermore, their method of parsing .lp files to extract structural features (variable counts, constraint types) for 'instance-enhanced self-consistency' provides a much richer signal than our current binary success/failure metrics.", "affiliations": "Stanford University, Shanghai Jiao Tong University, The University of Hong Kong, Shanghai University of Finance and Economics, Cardinal Operations", "analysis_date": "2026-02-15"}, {"arxiv_id": "2505.10117", "arxiv_url": "https://arxiv.org/abs/2505.10117", "title": "Learning Virtual Machine Scheduling in Cloud Computing through Language Agents", "authors_short": "Jiehao Wu et.al.", "published_date": "2025-05-15", "m_score": 8, "p_score": 9, "i_score": 8, "priority_score": 8.11, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_name": "Self-Improving LLM Agents for Iterative OR Program Synthesis", "front_status": "stable", "methods": ["llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "llm_evolutionary_search", "evolution_of_heuristics"], "problems": ["virtual_machine_scheduling", "online_dynamic_multidimensional_bin_packing", "cloud_scheduling"], "code_url": "https://github.com/jettbrains/-L-", "brief": "Wu et al. introduce MiCo, a hierarchical framework that uses LLMs to evolve both a library of scenario-specific scheduling heuristics ('Options') and a master policy ('Composer') that dynamically switches between them based on system state. Tested on large-scale Huawei/Azure VM traces, it achieves a 96.9% competitive ratio against Gurobi, significantly outperforming Deep RL (SchedRL) by ~11% in dynamic scenarios. **Key Insight:** Instead of evolving a single robust heuristic (which often fails in non-stationary environments), explicitly evolve a *portfolio* of specialized heuristics and a separate *selector* function. This SMDP-based decomposition is a concrete architectural pattern we should adopt in AlgoEvo to handle diverse problem instances and non-stationary distributions effectively.", "affiliations": "Shanghai Jiao Tong University, East China Normal University, Tongji University", "analysis_date": "2026-02-15"}, {"arxiv_id": "2505.06608", "arxiv_url": "https://arxiv.org/abs/2505.06608", "title": "RideAgent: An LLM-Enhanced Optimization Framework for Automated Taxi Fleet Operations", "authors_short": "Xinyu Jiang et.al.", "published_date": "2025-05-10", "m_score": 7, "p_score": 6, "i_score": 7, "priority_score": 5.24, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["mixed_integer_programming", "random_forest", "supervised_learning", "llm_agents", "llm_as_heuristic"], "problems": ["taxi_pre_allocation", "dynamic_pricing", "spatio_temporal_supply_demand_balancing", "fleet_management", "resource_allocation"], "code_url": null, "brief": "RideAgent employs an LLM to analyze a small set of historical optimal solutions, identifying and fixing 'low-sensitivity' decision variables to shrink the MIP search space before handing it to Gurobi. The results are empirically solid, showing a ~50% time reduction with <2.5% optimality gap, outperforming standard cutting plane baselines on NYC taxi data. **Key Takeaway:** We should adapt their 'Small-Sample Guided Optimization' strategy—specifically using LLMs to infer *variable fixing constraints* from elite archive solutions—to accelerate the inner solvers in our AlgoEvo and EvoCut pipelines. This offers a concrete, data-driven way to prune search spaces that complements our current evolutionary approaches.", "affiliations": "Tsinghua University, McGill University, George Washington University, JD Intelligent Cities Research, Beijing Technology and Business University", "analysis_date": "2026-02-15"}, {"arxiv_id": "2505.04354", "arxiv_url": "https://arxiv.org/abs/2505.04354", "title": "Optimization Problem Solving Can Transition to Evolutionary Agentic Workflows", "authors_short": "Wenhao Li et.al.", "published_date": "2025-05-07", "m_score": 5, "p_score": 9, "i_score": 6, "priority_score": 5.24, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_6", "front_name": "Agentic LLM Frameworks for Robust OR Model Synthesis and Validation", "front_status": "stable", "methods": ["evolutionary_algorithms", "llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "llm_evolutionary_search"], "problems": ["virtual_machine_scheduling", "online_vector_bin_packing", "ADMM_parameter_adaptation", "structured_convex_optimization", "automated_algorithm_design"], "code_url": null, "brief": "Li et al. propose an 'Evolutionary Agentic Workflow' that combines LLMs (DeepSeek) with evolutionary search to automate algorithm design, demonstrating it on VM scheduling and ADMM parameter tuning. The empirical rigor is low; they compare against weak baselines (BestFit for bin packing, a 2000-era heuristic for ADMM) and frame it as a position paper. However, the application of LLM-evolution to discover symbolic mathematical update rules (for ADMM step sizes) rather than just procedural code is a concrete use case we should consider for our EvoCut work. This serves primarily as competitor intelligence—validating our AlgoEvo direction—rather than a source of novel methodology.", "affiliations": "University of Minnesota, Tongji University, East China Normal University", "analysis_date": "2026-02-15"}, {"arxiv_id": "2505.01485", "arxiv_url": "https://arxiv.org/abs/2505.01485", "title": "CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code", "authors_short": "Tasnim Ahmed et.al.", "published_date": "2025-05-02", "m_score": 5, "p_score": 7, "i_score": 6, "priority_score": 4.74, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_7", "front_name": "Hierarchical RAG and Multi-Agent LLMs for Optimization Model Synthesis", "front_status": "stable", "methods": ["llm_code_generation", "retrieval_augmented_generation", "hierarchical_chunking", "metadata_augmented_indexing", "two_stage_retrieval"], "problems": ["linear_programming_code_generation", "program_synthesis"], "code_url": null, "brief": "CHORUS introduces a RAG framework for generating Gurobi code that replaces standard code retrieval with a metadata-based approach, indexing code examples by generated keywords and summaries rather than raw syntax. On the NL4Opt-Code benchmark, this allows open-source models like Llama-3-70B to match GPT-4 performance (improving accuracy from ~23% to ~57%). The key takeaway for us is the effectiveness of 'metadata-augmented indexing'—bridging the semantic gap between natural language problem descriptions and rigid solver APIs by retrieving based on functional descriptions rather than code embeddings. We should apply this metadata indexing strategy to the code retrieval modules in our OR-Bench and AlgoEvo agents.", "affiliations": "Queen's University", "analysis_date": "2026-02-15"}, {"arxiv_id": "2504.16918", "arxiv_url": "https://arxiv.org/abs/2504.16918", "title": "OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents", "authors_short": "Raghav Thind et.al.", "published_date": "2025-04-23", "m_score": 7, "p_score": 7, "i_score": 8, "priority_score": 5.79, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_name": "Self-Improving LLM Frameworks for Robust Optimization Model Synthesis", "front_status": "stable", "methods": ["llm_as_heuristic", "llm_as_evaluator", "llm_code_generation", "llm_in_the_loop", "multi_agent_llm_system"], "problems": ["optimization_modeling", "linear_programming", "mixed_integer_linear_programming", "nonlinear_programming", "mixed_integer_nonlinear_programming"], "code_url": null, "brief": "OptimAI introduces a multi-agent framework for translating natural language to optimization models, featuring a 'plan-before-code' stage and a novel **UCB-based debug scheduler**. Instead of linearly debugging a single solution, it treats debugging as a multi-armed bandit problem, dynamically allocating compute to different solution strategies based on a 'Decider' score and exploration term. While the combinatorial results (TSP a280) are trivial, the bandit mechanism is a highly effective heuristic for search control. We should steal this UCB scheduling logic for AlgoEvo to prevent agents from wasting tokens debugging fundamentally flawed heuristics.", "affiliations": "University of Maryland at College Park", "analysis_date": "2026-02-15"}, {"arxiv_id": "2504.04310", "arxiv_url": "https://arxiv.org/abs/2504.04310", "title": "CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization", "authors_short": "Weiwei Sun et.al.", "published_date": "2025-04-06", "m_score": 4, "p_score": 9, "i_score": 7, "priority_score": 6.26, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_6", "front_name": "Agentic LLM Frameworks for Robust OR Model Synthesis and Validation", "front_status": "stable", "methods": ["llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "llm_evolutionary_search", "funsearch"], "problems": ["aircraft_landing", "assignment_problem", "assortment_problem", "bin_packing_one_dimensional", "capacitated_warehouse_location"], "code_url": "https://github.com/sunnweiwei/CO-Bench", "brief": "Sun et al. introduce CO-Bench, a suite of 36 diverse combinatorial optimization problems (packing, scheduling, routing) designed specifically to benchmark LLM agents in generating algorithms (code), not just solutions. They evaluate 9 frameworks (including FunSearch, ReEvo, AIDE), finding that FunSearch combined with reasoning models (o3-mini) yields the most robust performance, though agents still struggle significantly with strict feasibility constraints (valid solution rates often <60%). **Takeaway:** We should immediately integrate CO-Bench into our pipeline to benchmark AlgoEvo against ReEvo and FunSearch; this saves us months of data curation and provides a standardized metric to prove our method's superiority.", "affiliations": "Carnegie Mellon University", "analysis_date": "2026-02-15"}, {"arxiv_id": "2503.10642", "arxiv_url": "https://arxiv.org/abs/2503.10642", "title": "Text2Zinc: A Cross-Domain Dataset for Modeling Optimization and Satisfaction Problems in MiniZinc", "authors_short": "Akash Singirikonda et.al.", "published_date": "2025-02-22", "m_score": 3, "p_score": 8, "i_score": 4, "priority_score": 5.09, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_6", "front_name": "Agentic LLM Frameworks for Robust OR Model Synthesis and Validation", "front_status": "stable", "methods": ["llm_code_generation", "llm_prompt_optimization", "llm_in_the_loop", "vanilla_prompting", "chain_of_thought"], "problems": ["program_synthesis", "natural_language_to_code_generation", "automated_optimization_modeling", "automated_constraint_programming_modeling", "linear_programming"], "code_url": null, "brief": "Singirikonda et al. introduce TEXT2ZINC, a dataset of 110 Natural Language-to-MiniZinc problems, and benchmark GPT-4 using Vanilla, CoT, and Compositional prompting. Their results are poor (max ~25% solution accuracy), confirming that off-the-shelf LLMs struggle significantly with MiniZinc syntax and logical translation. Crucially, they attempt using Knowledge Graphs as an intermediate representation, but report that it actually *reduced* solution accuracy compared to basic CoT—a valuable negative result for our symbolic modeling work. We should examine their dataset for inclusion in OR-Bench, but their prompting methods are rudimentary baselines we should easily outperform.", "affiliations": "Brown University, Fidelity Investments", "analysis_date": "2026-02-15"}, {"arxiv_id": "2502.14760", "arxiv_url": "https://arxiv.org/abs/2502.14760", "title": "EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of Optimization Formulations", "authors_short": "Haotian Zhai et.al.", "published_date": "2025-02-20", "m_score": 7, "p_score": 9, "i_score": 7, "priority_score": 7.89, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_name": "Self-Improving LLM Agents for Iterative OR Program Synthesis", "front_status": "stable", "methods": ["llm_as_heuristic", "llm_in_the_loop", "milp_solver", "linear_programming", "karp_reductions"], "problems": ["equivalence_checking_of_optimization_formulations", "milp_general", "lp_general"], "code_url": "https://github.com/HumainLab/EquivaMap", "brief": "Zhai et al. propose EquivaMap, a framework that evaluates whether two MILP formulations are equivalent by using an LLM to discover a linear mapping between their decision variables, which is then rigorously verified by a solver. Unlike 'execution accuracy' (which fails on unit scaling) or 'canonical accuracy' (which fails on variable permutation), they achieve 100% accuracy on a new dataset of equivalent formulations including cuts and slack variables. The core insight is replacing output comparison with a 'propose-mapping-and-verify' loop, effectively using the LLM to construct a proof of equivalence. We must adopt this methodology for the OR-Bench evaluation pipeline immediately, as it eliminates the false negatives currently plaguing our generation benchmarks.", "affiliations": "Stanford University, The University of Texas at Austin", "analysis_date": "2026-02-15"}, {"arxiv_id": "2502.11102", "arxiv_url": "https://arxiv.org/abs/2502.11102", "title": "OptMATH: A Scalable Bidirectional Data Synthesis Framework for Optimization Modeling", "authors_short": "Hongliang Lu et.al.", "published_date": "2025-02-16", "m_score": 7, "p_score": 9, "i_score": 8, "priority_score": 7.81, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_1", "front_name": "LLM-Enhanced Optimization Modeling via SIRL, MCTS, and Graph-Theoretic Evaluation", "front_status": "stable", "methods": ["llm_as_data_synthesizer", "llm_fine_tuned", "llm_in_the_loop", "llm_code_generation", "llm_as_evaluator"], "problems": ["optimization_modeling", "lp_modeling", "milp_modeling", "ip_modeling", "nlp_modeling"], "code_url": "https://github.com/AuroraLHL/OptMATH", "brief": "The authors introduce OptMATH, a framework for generating synthetic optimization datasets by creating mathematical instances from seed generators, back-translating them to natural language via LLMs, and validating the pairs using a solver-based rejection sampling loop (checking if the re-generated model yields the same optimal value). They demonstrate that a Qwen-32B model fine-tuned on this data beats GPT-4 on NL4Opt and MAMO benchmarks. The critical takeaway is the **solver-verified reverse generation pipeline**: we should immediately steal this workflow to populate OR-Bench and generate diverse, verified training environments for AlgoEvo, replacing manual curation with scalable synthesis.", "affiliations": "Peking University", "analysis_date": "2026-02-15"}, {"arxiv_id": "2411.17404", "arxiv_url": "https://arxiv.org/abs/2411.17404", "title": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical Modeling Problem Solving", "authors_short": "Teng Wang et.al.", "published_date": "2024-11-26", "m_score": 7, "p_score": 8, "i_score": 7, "priority_score": 6.93, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_name": "Self-Improving LLM Agents for Iterative OR Program Synthesis", "front_status": "stable", "methods": ["llm_as_heuristic", "llm_as_evaluator", "llm_code_generation", "llm_in_the_loop", "tree_of_thought"], "problems": ["natural_language_to_mathematical_model", "linear_programming", "mixed_integer_programming", "optimization_problem_formulation", "mathematical_reasoning"], "code_url": "https://github.com/LLM4OR/StructuredOR", "brief": "Wang et al. propose BPP-Search, combining Beam Search, a Process Reward Model (PRM), and a final Pairwise Preference Model to generate LP/MIP models from natural language. While their new 'StructuredOR' dataset is small (38 test instances), it uniquely provides intermediate modeling labels (sets, parameters, variables) essential for training PRMs in this domain. The key takeaway is their finding that PRMs are effective for pruning but imprecise for final ranking; they solve this by adding a pairwise preference model at the leaf layer—a technique we should immediately steal to improve selection robustness in our MASPRM and evolutionary search pipelines. This is a competent execution of 'LLM + Search' applied specifically to our OR niche.", "affiliations": "Huawei, The University of Hong Kong", "analysis_date": "2026-02-15"}, {"arxiv_id": "2411.01679", "arxiv_url": "https://arxiv.org/abs/2411.01679", "title": "Autoformulation of Mathematical Optimization Models Using LLMs", "authors_short": "Nicolás Astorga et.al.", "published_date": "2024-11-03", "m_score": 8, "p_score": 9, "i_score": 8, "priority_score": 8.11, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_1", "front_name": "LLM-Enhanced Optimization Modeling via SIRL, MCTS, and Graph-Theoretic Evaluation", "front_status": "stable", "methods": ["monte_carlo_tree_search", "llm_in_the_loop", "llm_as_heuristic", "llm_as_evaluator", "symbolic_pruning"], "problems": ["automated_optimization_modeling", "program_synthesis", "linear_programming", "mixed_integer_programming", "convex_optimization"], "code_url": "https://github.com/jumpynitro/AutoFormulator", "brief": "Astorga et al. frame optimization modeling as a hierarchical Monte-Carlo Tree Search (MCTS) problem, using LLMs to generate components and—crucially—employing SMT solvers to prune mathematically equivalent branches (e.g., recognizing `x+y` and `y+x` as identical). They achieve SOTA results on NL4OPT and IndustryOR, outperforming fine-tuned models like ORLM while using significantly fewer samples than naive approaches. **Key Takeaway:** The integration of symbolic equivalence checking (SMT) to prune the search tree is a technique we should immediately steal; implementing this in AlgoEvo would allow us to discard functionally identical code/math mutants before expensive evaluation, directly addressing our sample efficiency bottleneck.", "affiliations": "University of Cambridge, University of Hawaii at Manoa", "analysis_date": "2026-02-15"}, {"arxiv_id": "2410.22296", "arxiv_url": "https://arxiv.org/abs/2410.22296", "title": "Generalists vs. Specialists: Evaluating LLMs on Highly-Constrained Biophysical Sequence Optimization Tasks", "authors_short": "Angelica Chen et.al.", "published_date": "2024-10-29", "m_score": 8, "p_score": 6, "i_score": 8, "priority_score": 7.53, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_name": "Self-Improving LLM Agents for Iterative OR Program Synthesis", "front_status": "stable", "methods": ["bilevel_optimization", "marge_loss", "llm_as_optimizer", "llm_in_the_loop", "preference_learning"], "problems": ["biophysical_sequence_optimization", "protein_design", "antibody_design", "ehrlich_functions", "black_box_optimization"], "code_url": null, "brief": "The authors propose LLOME, a bilevel optimization framework that fine-tunes an LLM using 'MargE' (Margin-Aligned Expectation), a loss function that weights gradient updates by the magnitude of reward improvement (margin) rather than simple preference rankings. Results are rigorous and demonstrate that while DPO leads to generator collapse and infeasibility in constrained spaces, MargE maintains diversity and significantly improves sample efficiency, matching specialized solvers like LaMBO-2 on medium-difficulty tasks. The critical takeaway is that standard alignment methods (DPO/RLHF) are ill-suited for optimization because they discard information about *how much* better a solution is; MargE fixes this by satisfying the Strong Interpolation Criteria. We should immediately evaluate replacing the RL/update component in AlgoEvo with the MargE objective to improve the stability and quality of our evolved heuristics.", "affiliations": "Genentech, New York University", "analysis_date": "2026-02-15"}, {"arxiv_id": "2410.13213", "arxiv_url": "https://arxiv.org/abs/2410.13213", "title": "LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch", "authors_short": "Caigao Jiang et.al.", "published_date": "2024-10-17", "m_score": 5, "p_score": 7, "i_score": 6, "priority_score": 4.74, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_1", "front_name": "LLM-Enhanced Optimization Modeling via SIRL, MCTS, and Graph-Theoretic Evaluation", "front_status": "stable", "methods": ["llm_as_heuristic", "llm_as_evaluator", "llm_code_generation", "llm_fine_tuned", "llm_in_the_loop"], "problems": ["linear_programming", "integer_programming", "mixed_integer_programming", "nonlinear_programming", "combinatorial_optimization"], "code_url": "https://github.com/caigaojiang/llmopt", "brief": "The authors fine-tune Qwen1.5-14B to translate natural language optimization problems into Pyomo code via a structured 'five-element' intermediate representation (Sets, Parameters, Variables, Objective, Constraints) and KTO alignment. They achieve ~11% accuracy gains over GPT-4o and ORLM on benchmarks like NL4Opt and IndustryOR, primarily by reducing formulation hallucinations through the structured intermediate step and preference optimization. For our OR-Bench work, the key takeaway is the concrete recipe for using KTO to align symbolic modeling agents, which appears more effective than standard SFT for enforcing constraints in smaller models. While not an evolutionary search paper, it provides a strong, locally runnable baseline for our OR modeling evaluations.", "affiliations": "Ant Group, East China Normal University, Nanjing University", "analysis_date": "2026-02-15"}, {"arxiv_id": "2508.10047", "arxiv_url": "https://arxiv.org/abs/2508.10047", "title": "A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions", "authors_short": "Ziyang Xiao et.al.", "published_date": "2024-08-01", "m_score": 5, "p_score": 9, "i_score": 6, "priority_score": 7.11, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_name": "Self-Improving LLM Frameworks for Robust Optimization Model Synthesis", "front_status": "stable", "methods": ["systematic_literature_review", "empirical_study", "data_cleaning", "benchmark_creation", "llm_as_evaluator"], "problems": ["benchmark_quality_assessment", "evaluation_standardization", "natural_language_for_optimization", "optimization_modeling"], "code_url": "https://llm4or.github.io/LLM4OR", "brief": "This survey and empirical audit reveals that standard optimization modeling benchmarks (NL4Opt, IndustryOR) suffer from critical error rates ranging from 16% to 54%, rendering prior leaderboards unreliable. The authors manually cleaned these datasets and re-evaluated methods, finding that Chain-of-Thought (CoT) often degrades performance compared to standard prompting, while fine-tuned models (ORLM) and multi-agent systems (Chain-of-Experts) perform best. The immediate takeaway is that we must adopt their cleaned datasets for our OR-Bench project; using the original open-source versions is no longer defensible. Additionally, the failure of CoT on these tasks suggests we should prioritize multi-agent or fine-tuned approaches for symbolic formulation tasks.", "affiliations": "Zhejiang University, Huawei Noah’s Ark Lab, Singapore University of Social Sciences, Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security", "analysis_date": "2026-02-15"}, {"arxiv_id": "2407.19633", "arxiv_url": "https://arxiv.org/abs/2407.19633", "title": "OptiMUS-0.3: Using Large Language Models to Model and Solve Optimization Problems at Scale", "authors_short": "Ali AhmadiTeshnizi et.al.", "published_date": "2024-07-29", "m_score": 7, "p_score": 9, "i_score": 7, "priority_score": 7.21, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_14", "front_name": "Self-Improving LLM Frameworks for Robust Optimization Model Synthesis", "front_status": "stable", "methods": ["llm_code_generation", "llm_as_evaluator", "llm_prompt_optimization", "llm_in_the_loop", "ai_agents"], "problems": ["optimization_modeling", "milp_general", "facility_location", "flight_assignment", "security_constrained_unit_commitment"], "code_url": null, "brief": "OptiMUS-0.3 is a modular multi-agent system that translates natural language into Gurobi code, utilizing a 'connection graph' to manage variable-constraint relationships in long contexts and specialized agents to detect solver-specific structures (SOS, indicators) or implement sifting. The results are rigorous, introducing a new hard benchmark (NLP4LP) where they outperform GPT-4o by ~40% and beat Chain-of-Experts. The most stealable insight is the 'Structure Detection Agent': instead of relying on the LLM to write generic constraints, we should explicitly prompt for and map high-level structures to efficient solver APIs (like SOS constraints) to improve performance in our EvoCut and AlgoEvo pipelines. This is a necessary read for the OR-Bench team.", "affiliations": "", "analysis_date": "2026-02-15"}, {"arxiv_id": "2407.09887", "arxiv_url": "https://arxiv.org/abs/2407.09887", "title": "OptiBench Meets ReSocratic: Measure and Improve LLMs for Optimization Modeling", "authors_short": "Zhicheng YANG et.al.", "published_date": "2024-07-13", "m_score": 6, "p_score": 9, "i_score": 7, "priority_score": 6.69, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_1", "front_name": "LLM-Enhanced Optimization Modeling via SIRL, MCTS, and Graph-Theoretic Evaluation", "front_status": "stable", "methods": ["llm_code_generation", "llm_as_solver", "llm_in_the_loop", "llm_fine_tuned", "data_synthesis"], "problems": ["optimization_modeling", "linear_programming", "nonlinear_programming", "integer_programming", "mixed_integer_programming"], "code_url": "https://github.com/yangzhch6/ReSocratic", "brief": "The authors propose OptiBench, a benchmark of 605 optimization problems (linear/nonlinear, tabular/text), and ReSocratic, a data synthesis method that generates formal models first and back-translates them into natural language questions. Results are strong: fine-tuning Llama-3-8B on their 29k synthetic samples improves accuracy from 13.6% to 51.1%, validating the data quality. **Key Takeaway:** The 'Reverse Socratic' synthesis pipeline (Formal Model → Code → NL Question) is the superior strategy for generating synthetic OR datasets because it guarantees solvability and ground truth by construction, unlike forward generation. We should steal this pipeline for generating robust test instances for OR-Bench and potentially for training our OR agents.", "affiliations": "The Hong Kong University of Science and Technology, ETH Zurich, Huawei Noah’s Ark Lab, City University of Hong Kong, Sun Yat-sen University, MBZUAI, University of California Merced, Chongqing University", "analysis_date": "2026-02-15"}, {"arxiv_id": "2403.01131", "arxiv_url": "https://arxiv.org/abs/2403.01131", "title": "LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation", "authors_short": "Zeyuan Ma et.al.", "published_date": "2024-03-02", "m_score": 7, "p_score": 8, "i_score": 8, "priority_score": 7.79, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_0", "front_name": "Self-Improving LLM Agents for Iterative OR Program Synthesis", "front_status": "stable", "methods": ["llm_code_generation", "instruction_tuning", "contrastive_learning", "supervised_learning", "llm_fine_tuned"], "problems": ["optimization_code_generation", "numerical_optimization", "constrained_optimization", "unconstrained_optimization", "optimizer_discovery"], "code_url": "https://anonymous.4open.science/r/LLaMoCo-722A", "brief": "LLaMoCo fine-tunes small LLMs (down to 350M) to generate executable Python optimization code by training on a synthetic dataset where the 'ground truth' is the empirically best-performing solver identified via exhaustive benchmarking. The results are compelling: the fine-tuned 350M model achieves ~85% normalized performance on benchmarks where GPT-4 Turbo only reaches ~14-30%, largely because the small model learns to select specialized evolutionary strategies (like BIPOP-CMA-ES) while GPT-4 defaults to generic gradient-based solvers. **Key Takeaway:** We can replace the expensive GPT-4 calls in our evolutionary search loop with a specialized, fine-tuned local model (CodeLlama-7B) trained on our historical search successes, significantly improving both sample efficiency and scalability. The paper's 'contrastive warm-up' strategy for aligning diverse problem descriptions is also a transferable technique for our problem encoding work.", "affiliations": "Singapore Management University, Nanyang Technological University, South China University of Technology", "analysis_date": "2026-02-15"}, {"arxiv_id": "2402.10172", "arxiv_url": "https://arxiv.org/abs/2402.10172", "title": "OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models", "authors_short": "Ali AhmadiTeshnizi et.al.", "published_date": "2024-02-15", "m_score": 7, "p_score": 8, "i_score": 8, "priority_score": 6.89, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "generative_ai_for_or_2026-02-18_front_1", "front_name": "LLM-Enhanced Optimization Modeling via SIRL, MCTS, and Graph-Theoretic Evaluation", "front_status": "stable", "methods": ["llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "llm_as_evaluator", "llm_prompt_optimization"], "problems": ["optimization_modeling", "facility_location", "network_flow", "scheduling", "portfolio_management"], "code_url": null, "brief": "OptiMUS is a multi-agent framework for translating natural language into Gurobi code, achieving SOTA performance by using a 'Connection Graph' to map variables and parameters to specific constraints. This graph allows the agents to dynamically filter context and construct minimal prompts, enabling success on problems with long descriptions where baselines like Chain-of-Experts fail. They release NLP4LP, a hard benchmark of 67 complex instances, which we must immediately compare against our OR-Bench efforts. The **Connection Graph** is the key stealable insight: a structured dependency tracking mechanism that solves context pollution in iterative code generation, directly applicable to our AlgoEvo and HERMES memory designs.", "affiliations": "Stanford University", "analysis_date": "2026-02-15"}], "OR for Generative AI": [{"arxiv_id": "2602.14516", "arxiv_url": "https://arxiv.org/abs/2602.14516", "title": "Efficient Multi-round LLM Inference over Disaggregated Serving", "authors_short": "Wenhao He et.al.", "published_date": "2026-02-16", "m_score": 7, "p_score": 9, "i_score": 7, "priority_score": 7.32, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_name": "Integer Linear Programming for Heterogeneous LLM Serving Resource Allocation", "front_status": "growing", "methods": ["adaptive_scheduling", "resource_management", "integer_linear_programming", "performance_modeling", "distributed_systems"], "problems": ["llm_serving_optimization", "resource_allocation", "llm_scheduling", "service_level_objective_optimization", "prefill_decode_disaggregation"], "code_url": null, "brief": "AMPD introduces a disaggregated serving framework tailored for multi-round LLM agents, utilizing an offline ILP solver to optimize resource allocation (TP/DP configurations) and an online adaptive routing mechanism to handle incremental prefill tasks. The results are strong, showing 67-340% improvements in SLO attainment over vLLM and NVIDIA Dynamo by dynamically routing incremental prefill to decode workers when slack exists. For our 'GPUSched' project, the key takeaway is the specific ILP formulation (Eq. 5) for partitioning prefill/decode resources under global GPU constraints, and the insight that multi-agent workflows create a unique 'incremental prefill' bottleneck that standard disaggregation handles poorly.", "affiliations": "University of Cambridge, Peking University, Shanghai Jiao Tong University, Ant Group, Southeast University", "analysis_date": "2026-02-18"}, {"arxiv_id": "2602.08585", "arxiv_url": "https://arxiv.org/abs/2602.08585", "title": "Predicting Future Utility: Global Combinatorial Optimization for Task-Agnostic KV Cache Eviction", "authors_short": "Ziyao Tang et.al.", "published_date": "2026-02-09", "m_score": 8, "p_score": 9, "i_score": 7, "priority_score": 7.42, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["combinatorial_optimization", "convex_hull_relaxation", "greedy_algorithm", "isotonic_regression", "pava"], "problems": ["kv_cache_eviction", "llm_inference_optimization", "resource_allocation"], "code_url": null, "brief": "Tang et al. formulate KV cache eviction not as a heuristic filtering task, but as a global combinatorial optimization problem maximizing 'Oracle Importance' (future utility) across all attention heads. They solve this NP-hard problem efficiently by applying Isotonic Regression (via PAVA) to create a convex surrogate of the eviction loss, enabling an optimal greedy allocation strategy that is deployed via an offline-computed lookup table. Results are strong: they achieve 80% cache reduction on LongBench and RULER with minimal degradation, significantly outperforming dynamic heuristics like AdaKV. **Key Takeaway:** The decomposition of error into 'ranking error' vs. 'allocation error'—and solving the latter via convex-hull relaxation—is a powerful OR pattern we should apply to our own resource allocation and scheduling problems.", "affiliations": "Baidu, Fudan University", "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.08603", "arxiv_url": "https://arxiv.org/abs/2602.08603", "title": "OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval", "authors_short": "Teng Wang et.al.", "published_date": "2026-02-09", "m_score": 9, "p_score": 3, "i_score": 9, "priority_score": 7.19, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["mixed_integer_programming", "set_operations", "llm_in_the_loop", "llm_as_planner", "llm_as_evaluator"], "problems": ["composed_image_retrieval", "text_to_image_retrieval"], "code_url": "https://anonymous.4open.science/r/OSCAR-3A55/README.md", "brief": "Wang et al. formulate agentic tool-use planning not as a heuristic search (ReAct), but as a two-stage Mixed-Integer Programming (MIP) problem that solves for the mathematically optimal trajectory (tool selection + set operations) on training data. These 'golden trajectories' are then used as retrieved in-context demonstrations to steer the VLM at inference time, achieving SOTA on CIR benchmarks with only 10% of training data. **Key Takeaway:** We can steal this 'Offline MIP $\\to$ Online ICL' paradigm. Instead of relying on noisy online RL or expensive evolutionary loops to guide our AlgoEvo agents, we can solve MIPs on training instances to generate optimal reasoning traces, effectively 'solving' the prompt engineering problem via OR.", "affiliations": "Shanghai Jiao Tong University, OPPO", "analysis_date": "2026-02-17"}, {"arxiv_id": "2601.21008", "arxiv_url": "https://arxiv.org/abs/2601.21008", "title": "Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and Behavioral Rationality in Operations Research", "authors_short": "Ruicheng Ao et.al.", "published_date": "2026-02-08", "m_score": 9, "p_score": 8, "i_score": 9, "priority_score": 8.44, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "supervised_learning", "rl_ppo"], "problems": ["llm_evaluation_for_or", "infeasibility_diagnosis", "newsvendor_problem", "llm_self_correction", "behavioral_bias_mitigation"], "code_url": null, "brief": "Ao et al. introduce a framework for iterative OR model debugging that trains an 8B model using Group Relative Policy Optimization (GRPO) and a Process Reward Model (PRM) to outperform GPT-4o-mini. They utilize Gurobi's Irreducible Infeasible Subsystem (IIS) not just as text feedback, but as a dense reward signal (IIS size reduction) for the PRM, achieving a 95.3% recovery rate versus 86.2% for frontier APIs. **Key Takeaway:** We should steal their PRM construction method—specifically using solver diagnostics (like IIS reduction or compiler error counts) as dense step-level rewards—and their 'faithfulness penalty' to prevent overfitting in our evolutionary search. This is a direct validation of RLVR (Reinforcement Learning with Verifiable Rewards) for OR, proving it superior to large-scale prompting.", "affiliations": "Massachusetts Institute of Technology, Alibaba Group", "analysis_date": "2026-02-18"}, {"arxiv_id": "2602.07663", "arxiv_url": "https://arxiv.org/abs/2602.07663", "title": "A Two-Layer Framework for Joint Online Configuration Selection and Admission Control", "authors_short": "Owen Shen et.al.", "published_date": "2026-02-07", "m_score": 8, "p_score": 9, "i_score": 8, "priority_score": 8.23, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["online_optimization", "bandits_with_knapsacks", "online_linear_programming", "saddle_point_optimization", "ucb"], "problems": ["resource_allocation", "admission_control", "configuration_selection", "llm_serving_optimization", "gpu_scheduling"], "code_url": null, "brief": "The authors introduce a 'switching-aware' primal-dual framework for joint configuration selection (e.g., quantization, parallelism) and admission control, demonstrating that dynamically mixing configurations allows for higher resource utilization than any single fixed configuration. Results are rigorous, backed by $\\tilde{O}(\\sqrt{T})$ regret bounds and experiments on Alibaba cluster traces where the method achieves ~97% competitive ratio (vs. ~85% for greedy). The key takeaway is the 'switching-aware fluid oracle' concept: our resource allocation models for LLM serving must optimize over the convex hull of configurations (mixing CPU-heavy and Mem-heavy setups) rather than searching for a single static optimum. We should adapt their saddle-point formulation for the GPUSched project to handle heterogeneous resource constraints more effectively.", "affiliations": "Massachusetts Institute of Technology, Stanford University", "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.07263", "arxiv_url": "https://arxiv.org/abs/2602.07263", "title": "tLoRA: Efficient Multi-LoRA Training with Elastic Shared Super-Models", "authors_short": "Kevin Li et.al.", "published_date": "2026-02-06", "m_score": 5, "p_score": 8, "i_score": 6, "priority_score": 5.1, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["low_rank_adaptation", "distributed_training", "model_parallelism", "pipeline_parallelism", "gpu_kernel_optimization"], "problems": ["multi_lora_training_optimization", "gpu_scheduling", "llm_resource_management"], "code_url": null, "brief": "tLoRA optimizes multi-tenant LoRA training by fusing heterogeneous adapters into a 'Shared Super-Model' and employing an online scheduler that groups jobs based on residual GPU capacity and urgency. They report 1.2–1.8x throughput improvements and ~5x faster job completion times on A100 clusters compared to mLoRA, backed by realistic trace-driven experiments. For our GPUSched and resource allocation work, their hierarchical incremental grouping strategy serves as the state-of-the-art heuristic baseline we must outperform; additionally, their adaptive nano-batching (AIMD controller) is a transferable engineering trick for overlapping communication in distributed LLM workloads.", "affiliations": "University of Illinois Urbana-Champaign", "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.04431", "arxiv_url": "https://arxiv.org/abs/2602.04431", "title": "MaMa: A Game-Theoretic Approach for Designing Safe Agentic Systems", "authors_short": "Jonathan Nöther et.al.", "published_date": "2026-02-04", "m_score": 8, "p_score": 8, "i_score": 8, "priority_score": 7.96, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["game_theory", "stackelberg_security_game", "llm_adversarial_search", "automated_system_design", "multi_agent_systems"], "problems": ["automated_agentic_system_design", "llm_safety", "adversarial_robustness", "agent_safety", "multi_agent_coordination"], "code_url": null, "brief": "MaMa automates the design of multi-agent systems by formulating the problem as a Stackelberg Security Game: a Meta-Agent evolves system architectures (tools, communication graphs) while a Meta-Adversary iteratively optimizes worst-case agent compromises to break them. Empirical results on the BAD-ACTS benchmark show this adversarial co-evolution reduces attack success rates from ~50% (static baselines) to ~15-25% without degrading task quality. The critical takeaway is the implementation of an **adversarial co-evolution loop** within the architecture search—optimizing the 'threat' alongside the 'solution'—which directly addresses the robustness objectives in our RobustMAS project. We should implement this 'Meta-Adversary' concept to stress-test our evolved algorithms during the search phase rather than post-hoc.", "affiliations": "Max Planck Institute for Software Systems", "analysis_date": "2026-02-18"}, {"arxiv_id": "2602.00509", "arxiv_url": "https://arxiv.org/abs/2602.00509", "title": "PROBE: Co-Balancing Computation and Communication in MoE Inference via Real-Time Predictive Prefetching", "authors_short": "Qianchao Zhu et.al.", "published_date": "2026-02-03", "m_score": 5, "p_score": 9, "i_score": 7, "priority_score": 6.63, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_3", "front_name": "OR-Driven Adaptive Resource Allocation for LLM Inference and Test-Time Search", "front_status": "growing", "methods": ["llm_inference_optimization", "mixture_of_experts", "expert_parallelism", "load_balancing", "predictive_scheduling"], "problems": ["moe_inference_optimization", "llm_serving_optimization", "latency_critical_inference", "throughput_optimization", "distributed_system_performance"], "code_url": null, "brief": "PROBE optimizes MoE inference by using a distilled MLP to predict next-layer expert activation, enabling proactive load balancing and weight prefetching hidden behind the current layer's computation. The results are strong (1.3x speedup on 235B models) and demonstrate that control plane overheads can be fully masked. The critical takeaway for our `GPUSched` project is the **Lookahead Pipelining** architecture: it carves out a deterministic execution window where we could inject our own specialized solvers (e.g., fast ALNS or IP formulations) to outperform their basic greedy resource allocator. This transforms the stochastic serving problem into a short-horizon deterministic routing problem we are well-equipped to solve.", "affiliations": "Kling Infra, Kuaishou Technology", "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.02943", "arxiv_url": "https://arxiv.org/abs/2602.02943", "title": "3D-Learning: Diffusion-Augmented Distributionally Robust Decision-Focused Learning", "authors_short": "Jiaqi Wen et.al.", "published_date": "2026-02-03", "m_score": 8, "p_score": 9, "i_score": 8, "priority_score": 7.53, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["diffusion_model", "distributionally_robust_optimization", "decision_focused_learning", "dual_learning", "proximal_policy_optimization"], "problems": ["resource_provisioning", "llm_serving_optimization", "demand_response", "edge_data_center_selection", "out_of_distribution_generalization"], "code_url": "https://github.com/CIGLAB-Houston/3DLearning.git", "brief": "Wen et al. introduce '3D-Learning,' a framework that replaces analytic ambiguity sets (Wasserstein/KL) in Distributionally Robust Optimization (DRO) with a diffusion model trained via PPO to generate worst-case scenarios. Applied to LLM resource provisioning, they claim ~40-50% regret reduction on OOD Azure traces compared to standard DRO, though training computational cost is high (6.8GB memory vs 35MB). The critical takeaway is the methodology of parameterizing the ambiguity set with a generative model to find 'realistic' adversarial edge cases that respect the data manifold, solving the support shift issue of KL-DRO. We should steal this 'generative ambiguity set' concept for benchmarking our heuristics in RobustMAS and AlgoEvo.", "affiliations": "University of Houston", "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.02987", "arxiv_url": "https://arxiv.org/abs/2602.02987", "title": "Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control", "authors_short": "Ruihan Lin et.al.", "published_date": "2026-02-03", "m_score": 8, "p_score": 9, "i_score": 7, "priority_score": 8.3, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_0", "front_name": "Optimal LLM Inference Scheduling with Queueing Theory and MIP", "front_status": "emerging", "methods": ["stochastic_control", "queueing_network", "many_server_queueing", "fluid_approximation", "linear_programming"], "problems": ["llm_inference_scheduling", "resource_allocation", "queueing_theory", "revenue_management", "scheduling"], "code_url": null, "brief": "Lin et al. formulate LLM inference scheduling as a multiclass many-server queueing network, deriving a 'Gate-and-Route' policy from a steady-state fluid LP that explicitly manages prefill-decode contention. Calibrated on A100s, their approach proves that separating prefill admission (via occupancy tracking) from decode routing (work-conserving) eliminates decode backlogs and maximizes revenue. **Key Takeaway:** The decomposition of scheduling into 'static planning' (solving an LP for target occupancies) and 'dynamic control' (a simple gate tracking those targets) is a scalable alternative to online combinatorial optimization for your GPUSched work. It mathematically formalizes the intuition that prefill is the bottleneck and decode should be kept strictly critical but not backlogged.", "affiliations": "The Hong Kong University of Science and Technology", "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.01404", "arxiv_url": "https://arxiv.org/abs/2602.01404", "title": "BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation", "authors_short": "Zhouzi Li et.al.", "published_date": "2026-02-01", "m_score": 8, "p_score": 7, "i_score": 7, "priority_score": 6.54, "must_read": false, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["budget_optimal_allocation", "convex_optimization", "stochastic_modeling", "queueing_theory", "mathematical_analysis"], "problems": ["gpu_scheduling", "cloud_resource_scheduling", "ml_training_resource_allocation", "job_completion_time_minimization", "cost_performance_tradeoff_optimization"], "code_url": "https://github.com/petuum/adaptdl/tree/osdi21-artifact/simulator", "brief": "This paper derives a 'Budget-Optimal Allocation' (BOA) policy for ML training jobs, proving via queueing theory that a 'fixed-width' policy (no queueing, constant allocation per epoch) is optimal under general stochastic assumptions. They validate this on AWS, showing a 1.75x improvement in JCT and 2.2x budget reduction compared to Pollux, demonstrating that maximizing cluster utilization/efficiency is actually suboptimal for JCT. The key takeaway is the rigorous convex optimization formulation that replaces heuristic autoscaling, along with a practical 'Epoch Gluing' technique to handle rescaling overheads—both transferable to scheduling our malleable evolutionary search workloads.", "affiliations": "Carnegie Mellon University, University of Warwick, UNC Chapel Hill", "analysis_date": "2026-02-18"}, {"arxiv_id": "2602.01237", "arxiv_url": "https://arxiv.org/abs/2602.01237", "title": "Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models", "authors_short": "Katrina Brown et.al.", "published_date": "2026-02-01", "m_score": 7, "p_score": 8, "i_score": 7, "priority_score": 6.82, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["supervised_learning", "mlp", "lora", "greedy_algorithm", "llm_fine_tuned"], "problems": ["llm_serving_optimization", "resource_allocation", "arithmetic_reasoning", "optimal_reasoning_length_prediction", "query_difficulty_estimation"], "code_url": "https://aneeshers.github.io/predictive-scheduling/", "brief": "Brown et al. propose a 'Predictive Scheduling' framework that trains lightweight predictors (MLP on hidden states or LoRA) to estimate required CoT length before generation, using a greedy algorithm to allocate tokens under a global budget. Results show a 7.9% accuracy gain on GSM8K over uniform batching, backed by a systematic layer-wise analysis. **The key takeaway for us is that middle transformer layers (12-17)—not the final layer—contain the highest signal-to-noise ratio for predicting reasoning difficulty.** We should immediately test extracting features from these layers for our AlgoEvo value functions to improve sample efficiency. While the greedy scheduling algorithm itself is standard OR, the application to internal model states for pre-run allocation is a valid efficiency win for our serving optimization work.", "affiliations": "Harvard University", "analysis_date": "2026-02-17"}, {"arxiv_id": "2602.01410", "arxiv_url": "https://arxiv.org/abs/2602.01410", "title": "SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training", "authors_short": "Yunjie Pan et.al.", "published_date": "2026-02-01", "m_score": 7, "p_score": 5, "i_score": 7, "priority_score": 5.07, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["mixed_precision_training", "quantization", "integer_linear_programming", "adaptive_precision", "statistics_collection"], "problems": ["llm_pretraining", "llm_training_optimization", "layerwise_precision_optimization"], "code_url": "https://github.com/pyjhzwh/SNIP", "brief": "Pan et al. introduce SNIP, a framework that periodically solves a Knapsack-style Integer Linear Program (ILP) to assign layer-wise precision (FP4/FP8) during training, minimizing a custom 'divergence' metric subject to FLOPs constraints. Results are simulated via fake quantization (proxy FLOPs) rather than wall-clock time on native hardware, but the method scales to 70B models and outperforms heuristic baselines. **Key Takeaway:** The design pattern of 'collect sensitivity stats -> solve lightweight ILP -> dynamic reconfiguration' is highly relevant for our work on optimizing LLM serving and agent compute budgets; it proves standard OR solvers are fast enough to operate within the runtime loop of high-performance AI systems.", "affiliations": "University of Michigan, NTT Research, Inc., University of Massachusetts Amherst", "analysis_date": "2026-02-17"}, {"arxiv_id": "2507.11352", "arxiv_url": "https://arxiv.org/abs/2507.11352", "title": "Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces", "authors_short": "Yunhao Yang et.al.", "published_date": "2026-01-30", "m_score": 6, "p_score": 5, "i_score": 7, "priority_score": 4.75, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["neurosymbolic_ai", "multimodal_ai", "uncertainty_estimation", "probabilistic_guarantee", "formal_verification"], "problems": ["logistics_planning", "airlift_planning", "intent_classification", "natural_language_interface"], "code_url": null, "brief": "Yang et al. introduce a neurosymbolic agent that translates natural language into PDDL goals, using a learned latent space to estimate 'intent uncertainty' (distance to class centroids) which gates downstream execution. They use this uncertainty signal to drive both Direct Preference Optimization (DPO) and prompt optimization (TextGrad), achieving higher accuracy than GPT-5 on a lightweight model. **Takeaway:** The concept of deriving a 'probabilistic guarantee' from latent embeddings to serve as a cheap proxy reward or filter is a concrete technique we should test in AlgoEvo to reduce expensive evaluations. However, be skeptical of the topline results as they rely on a simplistic 3-class classification task rather than complex reasoning.", "affiliations": "Neurosymbolic Intelligence, The University of Texas at Austin, University of Colorado Boulder", "analysis_date": "2026-02-17"}, {"arxiv_id": "2504.08930", "arxiv_url": "https://arxiv.org/abs/2504.08930", "title": "VectorLiteRAG: Latency-Aware and Fine-Grained Resource Partitioning for Efficient RAG", "authors_short": "Junkyum Kim et.al.", "published_date": "2026-01-19", "m_score": 5, "p_score": 7, "i_score": 6, "priority_score": 4.89, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_3", "front_name": "OR-Driven Adaptive Resource Allocation for LLM Inference and Test-Time Search", "front_status": "growing", "methods": ["retrieval_augmented_generation_serving", "vector_similarity_search", "approximate_nearest_neighbor_search", "inverted_file_index", "product_quantization"], "problems": ["rag_serving_optimization", "llm_serving_optimization", "resource_contention", "latency_optimization", "throughput_maximization"], "code_url": "https://github.com/sitar-lab/VectorLiteRAG-AE", "brief": "VectorLiteRAG optimizes RAG serving throughput by dynamically partitioning vector indices between CPU and GPU memory based on access skew and latency SLOs. The results are credible, showing up to 1.5x throughput gains on H100/L40S setups by balancing retrieval speed against LLM KV-cache capacity. The most stealable insight is their use of a Beta distribution to analytically model the *minimum* hit rate within a batch (the bottleneck) to predict tail latency without full simulation—a technique we could adapt for stochastic constraints in our serving formulations. It solves a resource allocation problem we care about, though via systems engineering rather than the rigorous OR methods we prefer.", "affiliations": "Georgia Institute of Technology", "analysis_date": "2026-02-18"}, {"arxiv_id": "2505.23970", "arxiv_url": "https://arxiv.org/abs/2505.23970", "title": "Cache Your Prompt When It's Green: Carbon-Aware Caching for Large Language Model Serving", "authors_short": "Yuyang Tian et.al.", "published_date": "2026-01-19", "m_score": 5, "p_score": 7, "i_score": 6, "priority_score": 4.89, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_name": "Integer Linear Programming for Heterogeneous LLM Serving Resource Allocation", "front_status": "growing", "methods": ["integer_linear_programming", "sarima", "time_series_forecasting", "cache_replacement_policy", "least_carbon_savings"], "problems": ["llm_serving_optimization", "carbon_emission_reduction", "sustainable_computing", "slo_satisfaction", "cache_management"], "code_url": "https://greencache.persistentmemory.org", "brief": "Tian et al. propose GreenCache, a framework using Integer Linear Programming (ILP) to dynamically resize KV caches for LLM serving, balancing operational carbon (compute) against embodied carbon (SSD storage). They demonstrate ~15% carbon reduction on Llama-3 70B using Azure traces, though the reliance on simulation rather than live deployment weakens the claims slightly. For our 'OR for AI systems' work, the key takeaway is their 'Least Carbon Savings' (LCS) eviction policy—a heuristic that weighs computation saved against storage cost and recency—which we could adapt for optimizing memory-constrained multi-agent systems (HERMES) or general serving resource allocation.", "affiliations": "University of Waterloo, Purdue University", "analysis_date": "2026-02-18"}, {"arxiv_id": "2511.16947", "arxiv_url": "https://arxiv.org/abs/2511.16947", "title": "Fine-grained MoE Load Balancing with Linear Programming", "authors_short": "Chenqi Zhao et.al.", "published_date": "2026-01-15", "m_score": 7, "p_score": 8, "i_score": 7, "priority_score": 6.74, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["expert_parallelism", "expert_data_parallelism", "token_scheduling", "linear_programming", "locality_aware_routing"], "problems": ["moe_load_balancing", "gpu_scheduling", "distributed_training", "resource_allocation"], "code_url": null, "brief": "FineMoE replaces heuristic load balancing in MoE training with a Linear Programming (LP) formulation solved per micro-batch to minimize maximum GPU load, achieving ~37-47% throughput gains over Megatron-LM. They utilize warm-started simplex solvers to keep optimization time under 1ms and employ Cayley graphs to optimize static expert placement. For our `GPUSched` work, this is a critical data point: it proves that formal OR solvers can replace heuristics in real-time LLM infrastructure without becoming a latency bottleneck.", "affiliations": "Peking University, Institute of Computing Technology Chinese Academy of Sciences", "analysis_date": "2026-02-18"}, {"arxiv_id": "2502.07115", "arxiv_url": "https://arxiv.org/abs/2502.07115", "title": "Online Scheduling for LLM Inference with KV Cache Constraints", "authors_short": "Patrick Jaillet et.al.", "published_date": "2026-01-15", "m_score": 8, "p_score": 10, "i_score": 8, "priority_score": 8.61, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_6", "front_name": "Optimizing LLM Inference and Automated OR Problem Generation", "front_status": "emerging", "methods": ["online_optimization", "scheduling", "batching", "integer_programming", "competitive_ratio_analysis"], "problems": ["llm_inference_scheduling", "resource_allocation", "kv_cache_management", "online_scheduling"], "code_url": null, "brief": "This paper formulates LLM inference scheduling as an Integer Program (IP) that explicitly models the linear memory growth of KV caches, and proposes a 'Memory Constrained Shortest First' (MC-SF) algorithm. The results are rigorous, showing MC-SF achieves near-optimal performance (within 5% of hindsight optimal) on synthetic data and significantly outperforms standard FCFS/threshold heuristics on real traces. The critical takeaway is the 'future feasibility check' (Eq. 5), which validates that a batch will *remain* within memory limits throughout the generation process based on predicted output lengths—a necessary deviation from standard static-size scheduling. This is foundational reading for our GPUSched project, providing both the exact IP baseline we need and a strong heuristic to benchmark against.", "affiliations": "Massachusetts Institute of Technology, Microsoft Research, HKUST", "analysis_date": "2026-02-18"}, {"arxiv_id": "2601.06520", "arxiv_url": "https://arxiv.org/abs/2601.06520", "title": "SkyNomad: On Using Multi-Region Spot Instances to Minimize AI Batch Job Cost", "authors_short": "Zhifei Li et.al.", "published_date": "2026-01-10", "m_score": 6, "p_score": 9, "i_score": 7, "priority_score": 6.69, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["multi_region_scheduling", "cost_modeling", "online_probing", "resource_monitoring", "survival_analysis"], "problems": ["ai_batch_job_scheduling", "resource_cost_minimization", "deadline_constrained_scheduling", "spot_instance_management"], "code_url": null, "brief": "SkyNomad presents a multi-region scheduler for AI batch jobs that minimizes cost by dynamically migrating spot instances based on real-time availability probing and survival-analysis-based lifetime prediction. The authors propose a 'Unified Cost Model' that quantifies the monetary value of deadline slack, allowing the system to mathematically trade off migration egress costs against cheaper spot prices. Empirical results on AWS and GCP are strong, demonstrating 1.25-3.96x cost savings over single-region baselines while guaranteeing deadlines. We should immediately adapt their 'Value of Progress' heuristic and lifetime prediction module to optimize our own large-scale parallel evolution infrastructure.", "affiliations": "UC Berkeley, Shanghai Jiao Tong University, AMD, ICSI", "analysis_date": "2026-02-17"}, {"arxiv_id": "2505.03802", "arxiv_url": "https://arxiv.org/abs/2505.03802", "title": "Balancing Fidelity and Plasticity: Aligning Mixed-Precision Fine-Tuning with Linguistic Hierarchies", "authors_short": "Changhai Zhou et.al.", "published_date": "2026-01-05", "m_score": 7, "p_score": 6, "i_score": 7, "priority_score": 5.24, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["quantization_aware_fine_tuning", "mixed_precision_quantization", "low_rank_adaptation", "parameter_efficient_fine_tuning", "multi_objective_optimization"], "problems": ["llm_resource_allocation", "llm_efficiency", "quantized_llm_fine_tuning", "natural_language_understanding", "mathematical_reasoning"], "code_url": null, "brief": "QR-Adaptor employs a three-stage gradient-free search (Entropy Profiling → NSGA-II → Bayesian Optimization) to jointly optimize per-layer quantization bits and LoRA ranks. The results are empirically strong, showing that strategic mixed-precision (avg ~3.5 bits) can rival 16-bit baselines by preserving fidelity in deep semantic layers. We should steal their **Fidelity Sensitivity Profiling** (using information entropy to bias the initial evolutionary population) and **Proxy Tuning** (using few-step training as a cheap fitness proxy); these are concrete mechanisms to improve sample efficiency in our own evolutionary search pipelines.", "affiliations": "Fudan University, Yale University, Zhejiang University", "analysis_date": "2026-02-18"}, {"arxiv_id": "2504.11320", "arxiv_url": "https://arxiv.org/abs/2504.11320", "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints", "authors_short": "Ruicheng Ao et.al.", "published_date": "2026-01-05", "m_score": 8, "p_score": 10, "i_score": 8, "priority_score": 8.61, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_6", "front_name": "Optimizing LLM Inference and Automated OR Problem Generation", "front_status": "emerging", "methods": ["fluid_dynamics_approximation", "online_scheduling", "threshold_based_scheduling", "queueing_theory", "asymptotic_analysis"], "problems": ["llm_inference_scheduling", "memory_constrained_scheduling", "online_scheduling", "llm_serving_optimization", "resource_allocation"], "code_url": null, "brief": "This paper formulates LLM inference as a multi-stage stochastic scheduling problem, introducing 'Nested WAIT'—a threshold-based algorithm that handles unknown output lengths by letting prompts classify themselves as they survive into deeper decode segments. Unlike heuristic baselines (vLLM, Sarathi), they provide rigorous asymptotic optimality proofs and high-probability bounds against memory overflow, validated on A100 simulations. The key takeaway is the 'nested segment' mechanism: instead of predicting job size, structure the queue so short jobs exit early and long jobs naturally migrate to lower-priority/protected tiers, effectively decoupling the memory risk. We should immediately evaluate this threshold logic for our GPUSched formulations, as it likely outperforms our current predictive or FCFS approaches for handling KV cache growth.", "affiliations": "Massachusetts Institute of Technology, Peking University, Alibaba Group", "analysis_date": "2026-02-17"}, {"arxiv_id": "2512.21884", "arxiv_url": "https://arxiv.org/abs/2512.21884", "title": "Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models", "authors_short": "Tingyang Sun et.al.", "published_date": "2025-12-26", "m_score": 5, "p_score": 9, "i_score": 7, "priority_score": 6.54, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_name": "Integer Linear Programming for Heterogeneous LLM Serving Resource Allocation", "front_status": "growing", "methods": ["milp_formulation", "np_hardness_proof", "decomposition_algorithm", "greedy_algorithm", "shortest_path_routing"], "problems": ["resource_allocation", "llm_serving_optimization", "block_placement", "request_routing"], "code_url": "https://github.com/TingyangSunJeff/LLM_inference_simulator/tree/main", "brief": "This paper formulates geographically distributed LLM inference as a joint block placement and request routing problem, solved via a decomposed MILP heuristic (greedy placement + shortest path routing). The results are real and validated on A100 clusters, showing 60-80% latency reduction over PETALS' native heuristics. The key takeaway for us is their explicit modeling of 'attention cache' memory consumption as a function of concurrent requests—treating this as a dynamic constraint rather than a static buffer is the primary driver of their performance gains. This is a direct blueprint for the constraints we need in our 'GPUSched' formulations, though the algorithmic techniques themselves are standard OR fare.", "affiliations": "Pennsylvania State University, Virginia Tech, Indian Institute of Science", "analysis_date": "2026-02-17"}, {"arxiv_id": "2512.21487", "arxiv_url": "https://arxiv.org/abs/2512.21487", "title": "Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism", "authors_short": "Xinglin Pan et.al.", "published_date": "2025-12-25", "m_score": 6, "p_score": 7, "i_score": 5, "priority_score": 5.13, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_0", "front_name": "Optimal LLM Inference Scheduling with Queueing Theory and MIP", "front_status": "emerging", "methods": ["task_scheduling", "fine_grained_scheduling", "disaggregated_expert_parallelism", "ping_pong_pipeline", "performance_modeling"], "problems": ["moe_inference", "llm_serving_optimization", "gpu_resource_allocation", "communication_optimization", "task_scheduling"], "code_url": null, "brief": "FinDEP optimizes distributed Mixture-of-Experts (MoE) inference by partitioning tasks (attention, experts, communication) into fine-grained micro-batches and solving a scheduling problem to maximize overlap. The authors achieve 1.02x-1.61x speedups on H20/A6000 clusters compared to PPPipe, backed by solid empirical data. The key takeaway for our 'GPUSched' work is their methodology: deriving analytical properties (monotonicity and convexity) of the scheduling objective to reduce a complex search space into an $O(1)$ online solver, rather than relying on heavy solvers or RL. This confirms that simple linear performance models ($\\alpha + \\beta x$) are sufficient for accurate online resource allocation in LLM serving.", "affiliations": "The Hong Kong University of Science and Technology, Harbin Institute of Technology, Hong Kong Baptist University", "analysis_date": "2026-02-18"}, {"arxiv_id": "2512.18134", "arxiv_url": "https://arxiv.org/abs/2512.18134", "title": "Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs", "authors_short": "Rupanshu Soi et.al.", "published_date": "2025-12-19", "m_score": 8, "p_score": 9, "i_score": 8, "priority_score": 8.11, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["software_pipelining", "warp_specialization", "modulo_scheduling", "integer_linear_programming", "satisfiability_modulo_theories"], "problems": ["gpu_scheduling", "tensor_core_scheduling", "flash_attention", "matrix_multiplication_optimization"], "code_url": null, "brief": "Twill formulates the complex interplay of software pipelining and warp specialization on modern GPUs (Hopper/Blackwell) as a joint SMT/ILP optimization problem, automatically rediscovering expert-tuned Flash Attention schedules without heuristics. The results are rigorous, matching hand-tuned performance within 1-2% and handling new hardware constraints (Blackwell TMEM) automatically. The key takeaway is the 'cost normalization' technique via ILP to make the scheduling search space tractable, and the demonstration that exact constraint solvers can replace human intuition for complex kernel generation. This is essential reading for your work on OR formulations for GPU scheduling and LLM serving optimization, offering a deterministic baseline to compare against evolutionary approaches.", "affiliations": "Stanford University, NVIDIA", "analysis_date": "2026-02-18"}, {"arxiv_id": "2512.16134", "arxiv_url": "https://arxiv.org/abs/2512.16134", "title": "Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference", "authors_short": "Jian Tian et.al.", "published_date": "2025-12-18", "m_score": 6, "p_score": 9, "i_score": 7, "priority_score": 7.61, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_6", "front_name": "Optimizing LLM Inference and Automated OR Problem Generation", "front_status": "emerging", "methods": ["scheduling_algorithms", "batch_scheduling", "adaptive_control", "state_synchronization", "greedy_algorithm"], "problems": ["llm_inference_scheduling", "distributed_scheduling", "time_to_first_token_optimization", "throughput_optimization", "gpu_scheduling"], "code_url": null, "brief": "Tian et al. introduce Staggered Batch Scheduling (SBS) for DP+EP architectures, enforcing a buffering window to enable global bin-packing rather than immediate dispatch, which they prove causes Head-of-Line blocking in non-preemptive prefill phases. Tested on a production H800 cluster serving DeepSeek-V3, they demonstrate a 30-40% reduction in TTFT and a ~20% throughput increase backed by clear utilization metrics. The most valuable takeaway for our GPUSched project is their 'IQR-aware lexicographical' scheduling heuristic for the Decode phase, which robustly balances batch size against KV-cache memory variance—a constraint logic we should immediately adopt. This work validates that discrete batching is superior to continuous dispatch for MoE models, necessitating an update to our queuing theory models.", "affiliations": "Baidu Inc.", "analysis_date": "2026-02-17"}, {"arxiv_id": "2512.15705", "arxiv_url": "https://arxiv.org/abs/2512.15705", "title": "Dynamic Rebatching for Efficient Early-Exit Inference with DREX", "authors_short": "Xuting Liu et.al.", "published_date": "2025-12-17", "m_score": 5, "p_score": 8, "i_score": 6, "priority_score": 6.24, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_0", "front_name": "Optimal LLM Inference Scheduling with Queueing Theory and MIP", "front_status": "emerging", "methods": ["early_exit_llms", "dynamic_rebatching", "llm_inference_serving", "continuous_batching", "kv_caching"], "problems": ["llm_serving_optimization", "inference_acceleration", "resource_utilization", "latency_throughput_optimization", "quality_of_service_guarantees"], "code_url": null, "brief": "DREX introduces a system for 'Early-Exit' LLMs that dynamically splits and regroups batches at intermediate layers, using a cost-benefit heuristic (Adaptive Rebatching Threshold) to decide when rebatching is profitable versus forcing execution. Results are solid (2-12% throughput gain on A100s) and backed by real system measurements, not just simulations. The key takeaway for us is the analytical model for rebatching overhead (Eq. 6)—we can lift this constraint directly into our integer programming formulations for the GPUSched project to accurately model the trade-off between batch fragmentation and compute savings. Essential reading only for the serving optimization sub-team; irrelevant for the core evolutionary search group.", "affiliations": "Microsoft Research, University of Pennsylvania", "analysis_date": "2026-02-18"}, {"arxiv_id": "2512.11306", "arxiv_url": "https://arxiv.org/abs/2512.11306", "title": "RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training", "authors_short": "Tianyuan Wu et.al.", "published_date": "2025-12-15", "m_score": 7, "p_score": 8, "i_score": 7, "priority_score": 5.73, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["cluster_scheduling", "resource_allocation", "time_multiplexing", "co_scheduling", "inter_group_scheduling"], "problems": ["rl_post_training", "llm_post_training", "cluster_scheduling", "resource_management", "dependency_bubble_mitigation"], "code_url": null, "brief": "ROLLMUX proposes a cluster scheduler that interleaves the rollout and training phases of multiple RL jobs to eliminate the idle 'dependency bubbles' inherent in synchronous on-policy learning. Tested on a production-scale cluster (328 H800s + 328 H20s), they demonstrate a 1.84x cost reduction with real-world traces, validating the approach beyond simulation. The most stealable insight is 'long-tail migration': dynamically detecting straggler requests during generation and migrating them to a small subset of nodes, freeing the main cluster to proceed immediately. We should implement this logic in our AlgoEvo evaluation loops to mitigate stochastic evaluation times.", "affiliations": "Alibaba Group, Hong Kong University of Science and Technology, UIUC", "analysis_date": "2026-02-18"}, {"arxiv_id": "2512.09963", "arxiv_url": "https://arxiv.org/abs/2512.09963", "title": "GoodSpeed: Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference", "authors_short": "Phuong Tran et.al.", "published_date": "2025-12-14", "m_score": 5, "p_score": 7, "i_score": 6, "priority_score": 4.89, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_3", "front_name": "OR-Driven Adaptive Resource Allocation for LLM Inference and Test-Time Search", "front_status": "growing", "methods": ["speculative_decoding", "gradient_scheduling", "resource_allocation", "exponential_smoothing", "fluid_model_analysis"], "problems": ["distributed_llm_inference", "goodput_optimization", "fairness_optimization", "latency_reduction", "edge_inference"], "code_url": null, "brief": "GoodSpeed uses gradient-based scheduling to dynamically allocate token generation budgets across distributed draft servers, maximizing a logarithmic utility function to balance throughput and fairness. The authors provide rigorous fluid sample path analysis to prove convergence, backed by experiments on H100/L4 clusters, although the baselines (fixed/random allocation) are relatively weak. The most useful takeaway is the mechanism of using exponentially smoothed acceptance rate estimates to drive real-time control in a stochastic system—a robust pattern we should adopt for our own stochastic resource allocation and RobustMAS projects.", "affiliations": "The University of Sydney, Kyung Hee University", "analysis_date": "2026-02-18"}, {"arxiv_id": "2512.12476", "arxiv_url": "https://arxiv.org/abs/2512.12476", "title": "HetRL: Efficient Reinforcement Learning for LLMs in Heterogeneous Environments", "authors_short": "Yongjun He et.al.", "published_date": "2025-12-13", "m_score": 6, "p_score": 8, "i_score": 7, "priority_score": 6.44, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["constrained_optimization", "joint_optimization", "multi_level_search", "successive_halving", "genetic_algorithm"], "problems": ["llm_post_training", "reinforcement_learning_from_human_feedback", "gpu_scheduling", "heterogeneous_computing", "distributed_deep_learning"], "code_url": null, "brief": "HetRL formulates the scheduling of RLHF workflows (PPO/GRPO) across heterogeneous GPUs and networks as a constrained joint optimization problem, solved via a multi-level search combining Successive Halving and Genetic Algorithms. The authors validate this with 20,000 GPU-hours of experiments, demonstrating 3-9x throughput gains over standard systems like 'verl' in heterogeneous settings. The key takeaway is the hierarchical decomposition of the search space (Task Grouping → Coarse Assignment → Fine-grained Assignment) and the use of SHA to efficiently allocate search budget among candidate configurations. This is directly actionable for your 'GPUSched' project and offers a concrete strategy to scale 'AlgoEvo' runs across cheaper, fragmented GPU resources.", "affiliations": "Amazon Web Services, ETH Zurich", "analysis_date": "2026-02-18"}, {"arxiv_id": "2512.10271", "arxiv_url": "https://arxiv.org/abs/2512.10271", "title": "Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters", "authors_short": "Shruti Dongare et.al.", "published_date": "2025-12-11", "m_score": 7, "p_score": 8, "i_score": 7, "priority_score": 6.74, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["reinforcement_learning", "proximal_policy_optimization", "actor_critic", "mixed_integer_linear_programming", "feature_engineering"], "problems": ["gpu_scheduling", "cloud_scheduling", "resource_allocation", "deep_learning_workload_scheduling", "heterogeneous_gpu_clusters"], "code_url": "https://github.com/dshruti20/RLTune", "brief": "RLTune introduces a hybrid scheduling architecture where an RL agent (PPO) handles dynamic job prioritization based on cluster state, while a MILP solver optimizes the specific job-to-node packing constraints for the top-K jobs. The results are robust, demonstrating a ~25% makespan reduction over Slurm on a physical cluster and significant gains over pure RL baselines on standard traces (Philly, Helios). The critical takeaway is the architectural separation of concerns: delegating 'fuzzy' long-term objectives to RL and 'hard' constraint satisfaction to a symbolic solver. We should evaluate this 'RL-guided Solver' pattern for our `GPUSched` and `EvoCut` projects to improve constraint handling without losing adaptivity.", "affiliations": "Virginia Tech, Kuwait University, Northwestern Polytechnical University", "analysis_date": "2026-02-18"}, {"arxiv_id": "2512.07898", "arxiv_url": "https://arxiv.org/abs/2512.07898", "title": "MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement", "authors_short": "Hongwei Zhang et.al.", "published_date": "2025-12-05", "m_score": 8, "p_score": 6, "i_score": 8, "priority_score": 7.36, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["multi_agent_system", "iterative_refinement", "in_context_learning", "llm_as_refiner", "llm_as_evaluator"], "problems": ["llm_reasoning_enhancement", "multi_hop_retrieval", "parameter_efficient_reasoning"], "code_url": null, "brief": "MARINE proposes a multi-agent framework that iteratively refines a single 'reference trajectory' by generating small batches of candidates, verifying logical/factual conflicts, and merging superior segments rather than regenerating the whole chain. Results are impressive, with an 80B model matching 1000B baselines on retrieval tasks, backed by a theoretical derivation showing that batch size M=2 is optimal for fixed-budget refinement. The critical takeaway is the 'conflict-aware meta-verification' and segment merger, which functions effectively as a process-reward-guided mutation operator. We should immediately test the M=2 configuration in our evolutionary loops and consider adapting their merger logic to replace random crossover in our code generation agents.", "affiliations": "ZTE", "analysis_date": "2026-02-18"}, {"arxiv_id": "2510.09330", "arxiv_url": "https://arxiv.org/abs/2510.09330", "title": "Safety Game: Balancing Safe and Informative Conversations with Blackbox Agentic AI using LP Solvers", "authors_short": "Tuan Nguyen et.al.", "published_date": "2025-12-02", "m_score": 7, "p_score": 4, "i_score": 7, "priority_score": 4.89, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_32", "front_name": "Convex Optimization and Game Theory for Robust Multi-Objective LLM Alignment", "front_status": "growing", "methods": ["game_theory", "zero_sum_game", "linear_programming", "llm_in_the_loop", "llm_as_evaluator"], "problems": ["llm_safety_alignment", "safety_helpfulness_tradeoff", "multiple_choice_qa"], "code_url": null, "brief": "The authors formulate LLM response selection as a zero-sum game, solving a small Linear Program (LP) at inference time to mix candidate answers such that the expected risk never exceeds a 'safe fallback' baseline. Results are statistically significant, showing ~15% accuracy gains on SafetyBench by effectively managing the trade-off between helpfulness and safety probes. The key takeaway is the 'Adaptation Safety' constraint formulation: using an LP to guarantee that a stochastic policy is no worse than a heuristic baseline is a powerful, lightweight control mechanism we could adapt for selecting evolved algorithms or managing constraints in multi-agent optimization.", "affiliations": "University of Warwick", "analysis_date": "2026-02-17"}, {"arxiv_id": "2508.01002", "arxiv_url": "https://arxiv.org/abs/2508.01002", "title": "Optimal Scheduling Algorithms for LLM Inference: Theory and Practice", "authors_short": "Agrim Bari et.al.", "published_date": "2025-12-01", "m_score": 8, "p_score": 8, "i_score": 7, "priority_score": 7.29, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_0", "front_name": "Optimal LLM Inference Scheduling with Queueing Theory and MIP", "front_status": "emerging", "methods": ["resource_aware_dynamic_scheduler", "slo_aware_llm_inference_scheduler", "optimal_gemm_tiling", "dynamic_resource_allocation", "real_time_tbt_deadline_tracking"], "problems": ["llm_inference_scheduling", "throughput_optimization", "latency_minimization", "slo_aware_scheduling", "resource_allocation"], "code_url": "https://github.com/agrimUT/SLAI", "brief": "Bari et al. develop a queueing-theoretic framework for LLM inference that proves throughput optimality requires satisfying two conditions: optimal GeMM tiling (batch sizes matching hardware tensor core dimensions) and dynamic resource allocation between prefill/decode phases. They propose RAD (theoretical) and SLAI (practical), where SLAI uses a 'last schedulable time' heuristic to delay decode iterations for non-critical requests, thereby freeing up compute for prefill to reduce TTFT. Results are strong, showing a 53% reduction in median TTFT and 26% capacity increase over Sarathi-Serve on Mistral-7B. For our GPUSched project, the key takeaway is the explicit coupling of batch sizes to LCM(tile_dims) for theoretical optimality and the dynamic slack-based scheduling logic for heterogeneous SLOs.", "affiliations": "The University of Texas at Austin", "analysis_date": "2026-02-18"}, {"arxiv_id": "2511.22217", "arxiv_url": "https://arxiv.org/abs/2511.22217", "title": "Optimizing NetGPT via Routing-Based Synergy and Reinforcement Learning", "authors_short": "Yuxuan Chen et.al.", "published_date": "2025-11-27", "m_score": 5, "p_score": 6, "i_score": 7, "priority_score": 4.63, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["adaptive_routing", "dynamic_thresholding", "reinforcement_learning", "ppo", "supervised_finetuning"], "problems": ["llm_serving_optimization", "resource_allocation", "tool_calling", "task_offloading", "network_optimization"], "code_url": null, "brief": "Chen et al. propose a cloud-edge routing framework that dynamically offloads tool-calling tasks based on network conditions (RTT/Bandwidth) and a learned confidence score, while simultaneously updating the edge model via PPO. Results on 8,000 tasks show that dynamic thresholds outperform static baselines like FrugalGPT, and crucially, that interleaving SFT updates is required to prevent JSON schema collapse during RL. The primary takeaway for us is the 'SFT-anchored' update strategy: alternating between RL (for reward maximization) and SFT (on valid outputs) is a simple, effective stabilizer for maintaining structural constraints (like code syntax or JSON) during optimization. We should test this anchoring technique in AlgoEvo to keep evolved heuristics syntactically valid while maximizing fitness.", "affiliations": "Zhejiang University, Huawei Technologies Co., Ltd., Zhejiang Lab, Macau University of Science and Technology, The University of Electro-Communications, Shenzhen CyberAray Network Technology Co., Ltd", "analysis_date": "2026-02-18"}, {"arxiv_id": "2511.21572", "arxiv_url": "https://arxiv.org/abs/2511.21572", "title": "BAMAS: Structuring Budget-Aware Multi-Agent Systems", "authors_short": "Liming Yang et.al.", "published_date": "2025-11-26", "m_score": 7, "p_score": 8, "i_score": 7, "priority_score": 6.74, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["integer_linear_programming", "reinforcement_learning", "offline_reinforcement_learning", "policy_gradient", "llm_in_the_loop"], "problems": ["multi_agent_coordination", "resource_allocation", "llm_serving_optimization", "code_generation", "mathematical_reasoning"], "code_url": "https://github.com/chunfenri/BAMAS", "brief": "BAMAS decouples agent resource provisioning from coordination strategy, using an Integer Linear Programming (ILP) solver to select the optimal set of LLMs under a strict budget and offline RL to select a fixed interaction topology. They demonstrate ~80% cost reduction on GSM8K and MBPP while matching SOTA accuracy, proving that formal optimization beats greedy heuristics for agent allocation. The key takeaway for us is the 'lexicographically optimal' ILP formulation for tier-based LLM selection, which we should steal immediately for our inference resource managers. While their topology search is limited to a fixed library (unlike our evolutionary approach), the hybrid ILP+RL architecture is a strong template for our 'OR for Generative AI' work.", "affiliations": "Tsinghua University, Peking University, University of Illinois Urbana-Champaign, Nanyang Technological University", "analysis_date": "2026-02-17"}, {"arxiv_id": "2511.15898", "arxiv_url": "https://arxiv.org/abs/2511.15898", "title": "Global Resolution: Optimal Multi-Draft Speculative Sampling via Convex Minimization", "authors_short": "Rahul Krishna Thomas et.al.", "published_date": "2025-11-19", "m_score": 9, "p_score": 8, "i_score": 7, "priority_score": 8.01, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["speculative_sampling", "optimal_transport", "linear_programming", "max_flow", "polymatroid_theory"], "problems": ["llm_inference_acceleration", "autoregressive_decoding_latency_reduction", "optimal_multi_draft_speculative_sampling"], "code_url": null, "brief": "The authors solve the Optimal Transport Linear Program (OTLP) for multi-draft speculative sampling by reducing it to a convex minimization problem using polymatroid theory and max-flow, rather than using slow general LP solvers. They prove this 'Global Resolution' algorithm is exact for i.i.d. drafts and achieves >90% acceptance with negligible overhead (<100ms), running 10,000x faster than baselines. **Key Takeaway:** The reduction of a discrete token selection problem to a convex optimization problem via polymatroids is a brilliant theoretical trick we could potentially adapt for selecting diverse solution subsets in AlgoEvo. This is a definitive 'OR for LLM infra' paper that obsoletes heuristic verification strategies.", "affiliations": "Stanford University, Ritual", "analysis_date": "2026-02-17"}, {"arxiv_id": "2511.09092", "arxiv_url": "https://arxiv.org/abs/2511.09092", "title": "OR-R1: Automating Modeling and Solving of Operations Research Optimization Problem via Test-Time Reinforcement Learning", "authors_short": "Zezhen Ding et.al.", "published_date": "2025-11-12", "m_score": 8, "p_score": 9, "i_score": 8, "priority_score": 8.36, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_6", "front_name": "Optimizing LLM Inference and Automated OR Problem Generation", "front_status": "emerging", "methods": ["supervised_fine_tuning", "reinforcement_learning", "proximal_policy_optimization", "test_time_adaptation", "llm_code_generation"], "problems": ["operations_research_problem_solving", "optimization_modeling", "linear_programming", "mixed_integer_linear_programming", "resource_allocation"], "code_url": "https://github.com/SCUTE-ZZ/OR-R1", "brief": "OR-R1 introduces a data-efficient framework that fine-tunes Qwen3-8B using Supervised Fine-Tuning (SFT) followed by Test-Time Group Relative Policy Optimization (TGRPO) on unlabeled data. The results are empirically strong: it outperforms ORLM and LLMOPT while using only 1/10th of the synthetic training data, specifically narrowing the consistency gap between Pass@1 and Pass@8. The key takeaway for us is the effectiveness of GRPO (normalizing rewards within a sampled group to estimate baselines) combined with majority-voting rewards; this eliminates the need for a separate critic model while significantly improving code generation consistency. We should immediately evaluate GRPO as a lightweight alternative to PPO for the 'RL-infused' components of our evolutionary search methods.", "affiliations": "The Hong Kong University of Science and Technology, Arizona State University, University of North Carolina at Chapel Hill", "analysis_date": "2026-02-18"}, {"arxiv_id": "2502.14617", "arxiv_url": "https://arxiv.org/abs/2502.14617", "title": "SageServe: Optimizing LLM Serving on Cloud Data Centers with Forecast Aware Auto-Scaling", "authors_short": "Shashwat Jaiswal et.al.", "published_date": "2025-11-12", "m_score": 7, "p_score": 9, "i_score": 8, "priority_score": 7.96, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_name": "Integer Linear Programming for Heterogeneous LLM Serving Resource Allocation", "front_status": "growing", "methods": ["integer_linear_programming", "arima_time_series_forecasting", "reactive_heuristics", "discrete_event_simulation"], "problems": ["llm_serving_optimization", "resource_allocation", "gpu_scheduling", "cloud_scheduling"], "code_url": "https://github.com/shashwatj07/SageServe", "brief": "SageServe optimizes LLM inference resource allocation across regions using an Integer Linear Programming (ILP) model coupled with ARIMA-based traffic forecasting, specifically targeting mixed interactive and non-interactive workloads. They validate this on real Microsoft O365 production traces (which they release), demonstrating a 25% reduction in GPU hours and $2.5M/month savings compared to reactive baselines. The primary value for us is the release of the production workload traces—allowing us to benchmark our 'GPUSched' formulations against real-world data rather than synthetic distributions—and their specific ILP formulation for unified capacity management, which directly competes with our internal OR models.", "affiliations": "Microsoft, University of Illinois Urbana-Champaign, Georgia Institute of Technology, Indian Institute of Science", "analysis_date": "2026-02-17"}, {"arxiv_id": "2511.05915", "arxiv_url": "https://arxiv.org/abs/2511.05915", "title": "CoEdge-RAG: Optimizing Hierarchical Scheduling for Retrieval-Augmented LLMs in Collaborative Edge Computing", "authors_short": "Guihang Hong et.al.", "published_date": "2025-11-08", "m_score": 6, "p_score": 8, "i_score": 7, "priority_score": 5.43, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["hierarchical_scheduling", "retrieval_augmented_generation", "proximal_policy_optimization", "online_convex_optimization", "reinforcement_learning"], "problems": ["llm_serving_optimization", "resource_allocation", "workload_balancing", "edge_computing", "distributed_systems"], "code_url": null, "brief": "Hong et al. introduce CoEdge-RAG, a hierarchical scheduling framework for distributed edge RAG that combines PPO-based query routing with Online Convex Optimization (OCO) for local resource management. They empirically validate that a quadratic function best approximates LLM inference latency for OCO, allowing them to dynamically resize models and memory allocations under strict SLOs. The standout takeaway is the feedback loop: using PPO to learn a 'semantic routing policy' based on downstream generation quality (Rouge/BERTScore) rather than just load, effectively solving the 'black box' data distribution problem in privacy-preserving multi-agent systems. This hybrid RL/OR control stack is a transferable pattern for our distributed inference and multi-agent optimization work.", "affiliations": "Sun Yat-sen University", "analysis_date": "2026-02-17"}, {"arxiv_id": "2506.15707", "arxiv_url": "https://arxiv.org/abs/2506.15707", "title": "Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling", "authors_short": "Xinglin Wang et.al.", "published_date": "2025-10-20", "m_score": 8, "p_score": 8, "i_score": 9, "priority_score": 8.16, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_3", "front_name": "OR-Driven Adaptive Resource Allocation for LLM Inference and Test-Time Search", "front_status": "growing", "methods": ["direction_oriented_resource_allocation", "dora", "resource_allocation", "bayesian_optimization", "convex_optimization"], "problems": ["mathematical_reasoning", "llm_test_time_scaling", "llm_inference_optimization", "resource_allocation"], "code_url": "https://github.com/WangXinglin/DORA", "brief": "Wang et al. introduce Direction-Oriented Resource Allocation (DORA), which uses embedding-based soft clustering to group semantically similar reasoning paths and allocates compute budget to distinct 'directions' rather than individual solutions. They prove solution-level allocation (like REBASE) is suboptimal when paths are correlated and show DORA achieves state-of-the-art accuracy on MATH500 with 3.5x fewer FLOPs. **Key Takeaway:** We can immediately steal the 'semantic uniqueness reweighting' mechanism for AlgoEvo. By clustering generated heuristics via embeddings before expensive evaluation, we can drastically improve sample efficiency and stop wasting compute on minor variations of the same code.", "affiliations": "Beijing Institute of Technology, Xiaohongshu Inc", "analysis_date": "2026-02-18"}, {"arxiv_id": "2510.17015", "arxiv_url": "https://arxiv.org/abs/2510.17015", "title": "Justitia: Fair and Efficient Scheduling for LLM Applications", "authors_short": "Mingyan Yang et.al.", "published_date": "2025-10-19", "m_score": 7, "p_score": 9, "i_score": 7, "priority_score": 7.24, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_6", "front_name": "Optimizing LLM Inference and Automated OR Problem Generation", "front_status": "emerging", "methods": ["fair_queuing", "virtual_time_scheduling", "memory_centric_cost_modeling", "mlp", "demand_prediction"], "problems": ["llm_application_scheduling", "gpu_scheduling", "cloud_scheduling", "kv_cache_optimization", "resource_starvation"], "code_url": null, "brief": "Justitia introduces a scheduler for LLM agents that prioritizes applications based on their 'virtual finish time' (derived from a theoretical fair-sharing model) but executes them with full resource saturation to minimize completion time. The authors demonstrate a ~60% reduction in average job completion time compared to state-of-the-art fair schedulers (VTC) on vLLM, backed by rigorous experiments and theoretical delay bounds. The key takeaway is the 'KV token-time' cost metric (pd + d^2/2) which accurately captures memory bottlenecks in auto-regressive generation, and the insight that 'long-term fairness' allows for short-term resource saturation. This is immediately actionable for your GPUSched project and relevant for optimizing the serving infrastructure of AlgoEvo.", "affiliations": "Shanghai Jiao Tong University", "analysis_date": "2026-02-18"}, {"arxiv_id": "2510.11331", "arxiv_url": "https://arxiv.org/abs/2510.11331", "title": "Efficient LLM Inference over Heterogeneous Edge Networks with Speculative Decoding", "authors_short": "Bingjie Zhu et.al.", "published_date": "2025-10-13", "m_score": 5, "p_score": 7, "i_score": 6, "priority_score": 4.55, "must_read": false, "changes_thinking": false, "team_discussion": false, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_name": "Integer Linear Programming for Heterogeneous LLM Serving Resource Allocation", "front_status": "growing", "methods": ["speculative_decoding", "pipeline_parallelism", "dynamic_programming", "wireless_resource_allocation", "task_batching"], "problems": ["llm_serving_optimization", "latency_minimization", "edge_llm_inference", "resource_allocation", "batch_scheduling"], "code_url": null, "brief": "Zhu et al. propose a distributed Speculative Decoding framework for edge networks, formulating a Mixed-Integer Nonlinear Programming problem to jointly optimize task batching, speculation length, and wireless bandwidth. They solve the batching subproblem using a Dynamic Programming (DP) algorithm, achieving ~30-45% latency reduction over heuristics in simulations, though the approach relies on a rigid assumption of fixed maximum output lengths to remain tractable. The primary takeaway for our 'GPUSched' work is their DP formulation for optimizing batch boundaries in a pipelined draft-verify system, which offers a cleaner mathematical alternative to greedy heuristics for serving schedules. However, the heavy reliance on wireless channel modeling makes the full system less relevant to our datacenter-centric optimization problems.", "affiliations": "Queen Mary University of London, Kyung Hee University, Xidian University, Guangzhou Institute of Technology", "analysis_date": "2026-02-18"}, {"arxiv_id": "2510.10962", "arxiv_url": "https://arxiv.org/abs/2510.10962", "title": "MC#: Mixture Compressor for Mixture-of-Experts Large Models", "authors_short": "Wei Huang et.al.", "published_date": "2025-10-13", "m_score": 6, "p_score": 7, "i_score": 7, "priority_score": 5.43, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_34", "front_name": "Linear Programming for MoE LLM Mixed-Precision Quantization and Pruning", "front_status": "emerging", "methods": ["mixed_precision_quantization", "dynamic_expert_pruning", "linear_programming", "gumbel_softmax", "post_training_quantization"], "problems": ["mixture_of_experts_compression", "llm_compression", "vlm_compression"], "code_url": null, "brief": "Huang et al. propose MC#, a compression framework for MoE models that combines static mixed-precision quantization with dynamic expert pruning. They formulate bit-width allocation as an Integer Linear Programming (ILP) problem—optimizing expert importance vs. quantization error—and use a Gumbel-Softmax router for dynamic pruning. Results are strong, achieving 6.2x weight reduction on DeepSeek-VL2 with <2% accuracy loss. **Takeaway:** The ILP formulation (Eq. 7) is a clean, successful application of OR to AI infrastructure that we should replicate for our own resource allocation/scheduling problems; additionally, the differentiable router offers a template for dynamic agent selection in our multi-agent systems.", "affiliations": "NVIDIA Research, National University of Singapore, The University of Hong Kong, Beihang University, Hangzhou Innovation Institute", "analysis_date": "2026-02-17"}, {"arxiv_id": "2510.07417", "arxiv_url": "https://arxiv.org/abs/2510.07417", "title": "FLEET: Formal Language-Grounded Scheduling for Heterogeneous Robot Teams", "authors_short": "Corban Rivera et.al.", "published_date": "2025-10-08", "m_score": 5, "p_score": 7, "i_score": 6, "priority_score": 4.74, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["milp_general", "llm_as_heuristic", "llm_code_generation", "llm_in_the_loop", "multi_robot_task_allocation"], "problems": ["multi_robot_scheduling", "makespan_minimization", "heterogeneous_robot_teams", "natural_language_understanding", "task_decomposition"], "code_url": null, "brief": "FLEET implements a hybrid pipeline where an LLM extracts a task dependency graph and a 'fitness matrix' (capability scores) from natural language, which then populate a standard MILP for multi-robot scheduling. Results on the PARTNR benchmark show it outperforms pure LLM planners (SMART-LLM) by ~7% on heterogeneous tasks, though overall gains are modest. The actionable takeaway is the **fitness matrix extraction**: using the LLM to generate dense cost coefficients ($c_{ij}$) for the optimization model rather than just binary constraints. We should adopt this technique for handling soft semantic preferences in our heterogeneous VRP formulations.", "affiliations": "JHU APL, JHU, DEVCOM ARL", "analysis_date": "2026-02-17"}, {"arxiv_id": "2510.05774", "arxiv_url": "https://arxiv.org/abs/2510.05774", "title": "ConstraintLLM: A Neuro-Symbolic Framework for Industrial-Level Constraint Programming", "authors_short": "Weichun Shi et.al.", "published_date": "2025-10-07", "m_score": 7, "p_score": 8, "i_score": 7, "priority_score": 6.74, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["neuro_symbolic_ai", "llm_code_generation", "llm_as_heuristic", "llm_as_evaluator", "llm_fine_tuned"], "problems": ["automated_constraint_programming_modeling", "program_synthesis", "constraint_optimization_problems", "constraint_satisfaction_problems", "scheduling"], "code_url": "https://github.com/william4s/ConstraintLLM", "brief": "ConstraintLLM fine-tunes a 32B model for Constraint Programming (CP) modeling, utilizing a \"Constraint-Aware Retrieval Module\" (CARM) that retrieves few-shot examples based on extracted constraint signatures (e.g., `AllDifferent`, `Cumulative`) rather than text embeddings. They also employ a Tree-of-Thoughts search pruned by test case execution and an iterative self-correction mechanism that retrieves \"correction paths\" (error-to-fix trajectories). Results are strong: on their new industrial benchmark (IndusCP), they achieve ~51% accuracy with a 32B model, matching or beating GPT-4o and DeepSeek-V3. **Key Takeaway:** The shift from semantic retrieval to *structural* retrieval (matching constraint profiles) is the \"stealable\" insight; we should implement this for our OR modeling tasks immediately, ignoring surface-level problem descriptions in favor of logical signatures. This directly impacts our OR-Bench and automated formulation work.", "affiliations": "University of Oxford, University of Chinese Academy of Sciences, Hangzhou Institute for Advanced Study, ISCAS, University of Science and Technology Beijing", "analysis_date": "2026-02-18"}, {"arxiv_id": "2510.00821", "arxiv_url": "https://arxiv.org/abs/2510.00821", "title": "Logical Consistency Between Disagreeing Experts and Its Role in AI Safety", "authors_short": "Andrés Corrada-Emmanuel et.al.", "published_date": "2025-10-01", "m_score": 7, "p_score": 6, "i_score": 7, "priority_score": 5.24, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["integer_linear_programming", "formal_verification", "logical_inference", "axiomatic_system", "unsupervised_evaluation"], "problems": ["unsupervised_evaluation", "llm_evaluation", "meta_evaluation", "ai_safety_monitoring", "llm_comparison"], "code_url": null, "brief": "Corrada-Emmanuel formulates the unsupervised evaluation of classifiers as an Integer Linear Programming problem, defining the geometric space of possible ground truths consistent with observed agent disagreements. While the results are primarily theoretical demonstrations on MT-Bench (showing that certain disagreement patterns mathematically preclude accuracy >46%), the methodology is sound. The key takeaway is the concept of 'no-knowledge alarms': using LP constraints to flag when a multi-agent system or process reward model has become logically incoherent. We could implement this as a cheap, rigorous filter in our evolutionary search loops to prune branches where the evaluator agents are demonstrably unreliable.", "affiliations": "Data Engines", "analysis_date": "2026-02-17"}, {"arxiv_id": "2507.23390", "arxiv_url": "https://arxiv.org/abs/2507.23390", "title": "FMIP: Joint Continuous-Integer Flow For Mixed-Integer Linear Programming", "authors_short": "Hongpei Li et.al.", "published_date": "2025-09-29", "m_score": 6, "p_score": 6, "i_score": 7, "priority_score": 4.93, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["generative_models", "flow_matching", "conditional_flow_matching", "graph_neural_networks", "tri_gcn"], "problems": ["milp_general", "combinatorial_optimization", "combinatorial_auctions", "generalized_independent_set", "maximum_independent_set"], "code_url": "https://github.com/Lhongpei/FMIP", "brief": "FMIP introduces a flow matching framework that jointly generates integer and continuous variables for MILP, utilizing a tripartite graph and inference-time guidance. Empirical results on MIPLIB and other benchmarks show a ~40% reduction in primal gap compared to integer-only neural baselines (DIFUSCO), though it remains a heuristic warm-start for solvers. The most valuable takeaway is the **hybrid guidance mechanism** (Eq. 6 & 7): it combines gradient descent for continuous variables with a sampling-and-reweighting scheme for discrete variables based on constraint violations. We should consider stealing this reweighting logic for guiding hybrid evolutionary operators or multi-agent action spaces where gradients are available for only part of the state.", "affiliations": "Stanford University, Princeton University, National University of Singapore, Shanghai Jiao Tong University, Shanghai University of Finance and Economics", "analysis_date": "2026-02-18"}, {"arxiv_id": "2506.04203", "arxiv_url": "https://arxiv.org/abs/2506.04203", "title": "Cascadia: An Efficient Cascade Serving System for Large Language Models", "authors_short": "Youhe Jiang et.al.", "published_date": "2025-09-29", "m_score": 7, "p_score": 8, "i_score": 7, "priority_score": 6.89, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_name": "Integer Linear Programming for Heterogeneous LLM Serving Resource Allocation", "front_status": "growing", "methods": ["bi_level_optimization", "mixed_integer_linear_programming", "chebyshev_guided_optimization", "data_parallelism", "tensor_parallelism"], "problems": ["llm_serving_optimization", "resource_allocation", "latency_quality_tradeoff", "gpu_scheduling", "multi_model_inference"], "code_url": null, "brief": "Jiang et al. propose CASCADIA, a bi-level optimization framework for LLM cascade serving that iterates between an MILP solver for hardware deployment (choosing DP/TP/PP strategies) and a Chebyshev-guided solver for routing thresholds. They demonstrate 2.3x average throughput gains over SGLang and CascadeServe on H100 clusters, backed by rigorous ablation studies. The key takeaway is the effective decomposition of the NP-hard joint optimization problem: freezing routing to solve deployment via MILP, then optimizing routing against that deployment. This is a direct reference point for our 'GPUSched' project, validating the efficacy of formal integer programming in LLM resource allocation.", "affiliations": "Princeton University, University of Cambridge, Tsinghua University, HKUST, Shanghai Jiaotong University", "analysis_date": "2026-02-17"}, {"arxiv_id": "2506.11087", "arxiv_url": "https://arxiv.org/abs/2506.11087", "title": "Enhancing Delta Compression in LLMs via SVD-based Quantization Error Minimization", "authors_short": "Boya Xiong et.al.", "published_date": "2025-09-27", "m_score": 5, "p_score": 7, "i_score": 6, "priority_score": 4.74, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["singular_value_decomposition", "mixed_precision_quantization", "integer_linear_programming", "optimization", "post_training_quantization"], "problems": ["llm_compression", "delta_weight_compression", "llm_deployment_efficiency", "reasoning", "math_word_problems"], "code_url": null, "brief": "PRINMIX replaces heuristic quantization of LLM delta-weights with a 0/1 Integer Linear Programming (ILP) formulation to minimize reconstruction error. The results are strong and backed by numbers, showing ~22% improvement on AIME2024 and 6x storage savings compared to Delta-CoMe. For us, the key takeaway is not the compression itself, but the formulation: it proves that exact OR modeling outperforms heuristics in LLM serving infrastructure. Additionally, the reported 30-minute solving time suggests this problem could serve as a valuable testbed for our own evolutionary solver acceleration (EvoCut/AlgoEvo).", "affiliations": "Tsinghua University, Fudan University, Southern University of Science and Technology, Shanghai University of Finance and Economics", "analysis_date": "2026-02-17"}, {"arxiv_id": "2509.21091", "arxiv_url": "https://arxiv.org/abs/2509.21091", "title": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute", "authors_short": "Junpei Komiyama et.al.", "published_date": "2025-09-25", "m_score": 8, "p_score": 7, "i_score": 8, "priority_score": 7.09, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_3", "front_name": "OR-Driven Adaptive Resource Allocation for LLM Inference and Test-Time Search", "front_status": "growing", "methods": ["best_of_n", "majority_voting", "adaptive_sampling", "bayesian_modeling", "dirichlet_process_prior"], "problems": ["llm_inference_optimization", "llm_serving_optimization", "mathematical_reasoning", "scientific_reasoning", "ensemble_optimization"], "code_url": "https://github.com/jkomiyama/BoInf-code-publish", "brief": "This paper introduces a Bayesian adaptive stopping criterion (using Dirichlet process priors and Bayes factors) for majority voting, reducing test-time compute by 2-5x while maintaining asymptotic 'Best-of-Infinity' accuracy. They further demonstrate that optimizing weights for an ensemble of LLMs can be formulated as a Mixed-Integer Linear Program (MILP) by treating the decision boundaries as polytopes. **What we learned:** The Bayesian stopping logic is immediately transferable to AlgoEvo to reduce the cost of fitness evaluations—we can stop evaluating candidate solutions early if their performance distribution is statistically distinct. The MILP approach for ensembles also offers a concrete formulation we could adapt for our GPU scheduling and model serving optimization work.", "affiliations": "Mohamed bin Zayed University of Artificial Intelligence, New York University, RIKEN AIP, Institute of Science Tokyo, NEC Corporation", "analysis_date": "2026-02-17"}, {"arxiv_id": "2503.21476", "arxiv_url": "https://arxiv.org/abs/2503.21476", "title": "Robust DNN Partitioning and Resource Allocation Under Uncertain Inference Time", "authors_short": "Zhaojun Nan et.al.", "published_date": "2025-09-23", "m_score": 6, "p_score": 7, "i_score": 6, "priority_score": 5.03, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["mixed_integer_nonlinear_programming", "problem_decomposition", "chance_constrained_programming", "convex_optimization", "penalty_convex_concave_procedure"], "problems": ["dnn_partitioning", "resource_allocation", "edge_intelligence", "probabilistic_deadlines", "uncertainty_quantification"], "code_url": null, "brief": "Nan et al. propose a robust optimization framework for DNN partitioning that handles uncertain inference times by converting probabilistic deadlines into deterministic constraints using mean/variance information (Chance-Constrained Programming). They decompose the resulting MINLP into a convex resource allocation problem and a partitioning problem solved via the Penalty Convex-Concave Procedure (PCCP). Experiments on real hardware (Jetson/RTX) demonstrate ~50% energy savings over worst-case baselines while maintaining violation probabilities below the risk threshold. For our 'GPUSched' and 'RobustMAS' work, the key takeaway is the specific analytic transformation of the chance constraint and the use of PCCP as a heuristic for the binary subproblem—a potential alternative to heavy evolutionary search for real-time scheduling components.", "affiliations": "Tsinghua University", "analysis_date": "2026-02-18"}, {"arxiv_id": "2507.10259", "arxiv_url": "https://arxiv.org/abs/2507.10259", "title": "Temporal-Aware GPU Resource Allocation for Distributed LLM Inference via Reinforcement Learning", "authors_short": "Chengze Du et.al.", "published_date": "2025-09-16", "m_score": 6, "p_score": 9, "i_score": 6, "priority_score": 6.69, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_name": "Integer Linear Programming for Heterogeneous LLM Serving Resource Allocation", "front_status": "growing", "methods": ["reinforcement_learning", "proximal_policy_optimization", "optimal_transport", "greedy_algorithm", "demand_forecasting"], "problems": ["llm_inference_scheduling", "gpu_resource_allocation", "distributed_systems", "load_balancing", "multi_objective_optimization"], "code_url": null, "brief": "TORTA introduces a hierarchical scheduler for distributed LLM inference that uses a macro-level RL agent (PPO) supervised by an Optimal Transport (OT) baseline to manage inter-region allocation, followed by a micro-level greedy allocator. Results on simulated clusters (up to 50 servers) demonstrate a ~15% reduction in latency compared to reactive baselines (like SkyLB) specifically by optimizing for temporal smoothness and reducing switching costs. The key technical takeaway is the use of an exact OR solver (OT) as a dense supervision signal to train a faster RL policy, effectively combining the optimality of OR with the temporal foresight of RL. We should review our GPUSched formulations to ensure we aren't falling into the 'reactive' trap described here; if we are, this hybrid RL-OT architecture is a viable alternative.", "affiliations": "Shenzhen University of Advanced Technology, China Mobile Research Institute", "analysis_date": "2026-02-18"}, {"arxiv_id": "2508.19373", "arxiv_url": "https://arxiv.org/abs/2508.19373", "title": "HAP: Hybrid Adaptive Parallelism for Efficient Mixture-of-Experts Inference", "authors_short": "Haoran Lin et.al.", "published_date": "2025-08-26", "m_score": 7, "p_score": 8, "i_score": 7, "priority_score": 6.74, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["integer_linear_programming", "random_forest_regression", "data_parallelism", "tensor_parallelism", "expert_parallelism"], "problems": ["llm_serving_optimization", "resource_allocation", "moe_inference_optimization"], "code_url": null, "brief": "HAP replaces static parallelization heuristics in MoE inference with an Integer Linear Programming (ILP) solver that dynamically selects optimal strategies (TP, EP, DP) for Attention and Expert modules. They achieve verified ~1.6x speedups on A100/A6000 GPUs by modeling the inference process as a two-stage problem (prefill vs. decoding) with explicit transition costs, allowing the system to switch parallelism strategies mid-inference. For our work on OR-based resource allocation (GPUSched), the key takeaway is their formulation of **transition overheads** within the ILP constraints—a technique we should steal to model dynamic reconfiguration in our scheduling solvers. This confirms that symbolic OR methods can outperform standard systems heuristics in the LLM serving stack.", "affiliations": "Huawei Noah’s Ark Lab, Shandong University", "analysis_date": "2026-02-17"}, {"arxiv_id": "2508.13380", "arxiv_url": "https://arxiv.org/abs/2508.13380", "title": "Batching-Aware Joint Model Onloading and Offloading for Hierarchical Multi-Task Inference", "authors_short": "Seohyeon Cha et.al.", "published_date": "2025-08-18", "m_score": 5, "p_score": 8, "i_score": 7, "priority_score": 5.13, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["mixed_integer_nonlinear_programming", "alternating_optimization", "greedy_algorithm", "submodular_maximization", "lagrangian_relaxation"], "problems": ["ml_inference_systems", "resource_allocation", "computation_offloading", "model_placement", "multi_task_learning"], "code_url": null, "brief": "Cha et al. propose an alternating optimization framework (J3O) for joint model placement and query routing in hierarchical inference systems, decomposing the MINLP into greedy Lagrangian submodular maximization and linear programming. They explicitly model batching latency at the edge using a linear surrogate to handle the non-convex batch setup costs, achieving ~97% of Gurobi's optimal accuracy with <15% of the runtime. **Takeaway:** We should steal their linear surrogate formulation for batching overhead (approximating the L0-norm of task arrival) for our 'GPUSched' integer programs; it offers a tractable way to model batching efficiency in serving systems without full non-linear solvers.", "affiliations": "The University of Texas at Austin, DEVCOM Army Research Laboratory", "analysis_date": "2026-02-17"}, {"arxiv_id": "2508.09229", "arxiv_url": "https://arxiv.org/abs/2508.09229", "title": "Cluster Topology-Driven Placement of Experts Reduces Network Traffic in MoE Inference", "authors_short": "Danil Sivtsov et.al.", "published_date": "2025-08-12", "m_score": 5, "p_score": 8, "i_score": 6, "priority_score": 6.14, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_name": "Integer Linear Programming for Heterogeneous LLM Serving Resource Allocation", "front_status": "growing", "methods": ["integer_linear_programming", "optimization", "network_topology_modeling", "shortest_path_algorithms", "cvxpy"], "problems": ["llm_serving_optimization", "moe_expert_placement", "resource_allocation", "network_traffic_minimization", "distributed_system_optimization"], "code_url": "https://github.com/svtdanny/moe_topology_pack", "brief": "This paper formulates the placement of MoE experts (specifically DeepSeek-R1/V3) onto distributed GPU clusters as an Integer Linear Program (ILP) to minimize network hops. While the results are simulation-based (counting hops rather than measuring real latency), they demonstrate that ILP-based placement reduces traffic by ~14-30% compared to Round-Robin, but *only* when the objective function is weighted by historical expert activation frequency; unweighted ILP performs poorly. The key takeaway for our GPUSched project is the specific formulation of the load-aware objective function and the finding that topology-aware placement requires usage statistics to beat simple heuristics. We should adapt this ILP formulation for our resource allocation work.", "affiliations": "AIRI, Skoltech, Avito", "analysis_date": "2026-02-17"}, {"arxiv_id": "2508.07768", "arxiv_url": "https://arxiv.org/abs/2508.07768", "title": "Pareto Multi-Objective Alignment for Language Models", "authors_short": "Qiang He et.al.", "published_date": "2025-08-11", "m_score": 7, "p_score": 5, "i_score": 6, "priority_score": 4.99, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_32", "front_name": "Convex Optimization and Game Theory for Robust Multi-Objective LLM Alignment", "front_status": "growing", "methods": ["rlhf", "ppo", "noon_ppo", "multi_objective_optimization", "convex_optimization"], "problems": ["multi_objective_llm_alignment", "sentiment_control", "text_length_control", "humor_generation", "harmlessness_control"], "code_url": null, "brief": "PAMA introduces a computationally efficient algorithm for multi-objective alignment by reformulating the expensive gradient-norm minimization of MGDA into a convex optimization problem with a closed-form solution, reducing complexity from O(n^2d) to O(n). Empirical results on LLaMA-2-7B are robust, showing stable convergence on conflicting objectives (e.g., harmlessness vs. length) where baselines like MGDA-UB oscillate or fail. The single most useful takeaway is the analytical derivation for optimal objective weighting (Theorem 1) and the 'Noon PPO' heuristic (clipping negative advantages); we could port this logic to our multi-objective process reward models in AlgoEvo to balance search signals efficiently. While the NLP experiments are trivial, the gradient balancing mechanism is directly applicable to our multi-objective RL controllers.", "affiliations": "Ruhr University Bochum", "analysis_date": "2026-02-17"}, {"arxiv_id": "2508.03464", "arxiv_url": "https://arxiv.org/abs/2508.03464", "title": "Learning to Incentivize: LLM-Empowered Contract for AIGC Offloading in Teleoperation", "authors_short": "Zijun Zhan et.al.", "published_date": "2025-08-05", "m_score": 8, "p_score": 5, "i_score": 8, "priority_score": 6.44, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["llm_as_heuristic", "llm_code_generation", "llm_evolutionary_search", "llm_in_the_loop", "evolution_of_heuristics"], "problems": ["online_learning_contract_design", "moral_hazard", "incentive_mechanism_design", "aigc_offloading", "teleoperation"], "code_url": "https://github.com/Zijun0819/llm4contract", "brief": "Zhan et al. propose an LLM-based evolutionary framework to generate Python solvers for inferring hidden agent parameters in contract design (a bilevel OR problem). While the experiments are toy-scale (N=7 actions) and benchmarks are weak, the methodological architecture is highly relevant: they separate 'short-term reflectors' (analyzing parent pairs) from a 'long-term reflector' (aggregating insights across generations) to guide the Mutation LLM. This is a concrete, transferable implementation of evolutionary memory that we should test to improve sample efficiency in our own code-evolving agents.", "affiliations": "University of Houston, The Pennsylvania State University, University of Florida, Kyung Hee University, China University of Petroleum (East China), Prairie View A&M University", "analysis_date": "2026-02-17"}, {"arxiv_id": "2507.15615", "arxiv_url": "https://arxiv.org/abs/2507.15615", "title": "DHEvo: Data-Algorithm Based Heuristic Evolution for Generalizable MILP Solving", "authors_short": "Zhihao Zhang et.al.", "published_date": "2025-07-21", "m_score": 8, "p_score": 9, "i_score": 8, "priority_score": 8.11, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["evolutionary_computation", "llm_code_generation", "llm_evolutionary_search", "multi_agent_system", "data_algorithm_co_evolution"], "problems": ["MILP_general", "heuristic_evolution", "combinatorial_optimization", "set_cover", "combinatorial_auctions"], "code_url": null, "brief": "DHEvo introduces a 'data-algorithm co-evolution' framework that iteratively evolves heuristic code while simultaneously filtering the training instance set to retain only 'representative' instances (those where current heuristics perform well/stably). Empirical results on SCIP diving heuristics show it outperforms FunSearch and EoH by ~60% on Setcover while significantly reducing performance variance, validating the claim that dynamic data curation prevents overfitting. The key takeaway is the counter-intuitive curriculum strategy: rather than training on the hardest instances, filtering for instances with 'regular' feasible regions (high fitness) stabilizes the evolutionary search for code. We should immediately test this dynamic instance filtering in AlgoEvo to improve sample efficiency and generalization.", "affiliations": "Harbin Institute of Technology, Huawei Noah’s Ark Lab, Nanyang Technological University", "analysis_date": "2026-02-17"}, {"arxiv_id": "2502.13575", "arxiv_url": "https://arxiv.org/abs/2502.13575", "title": "ETS: Efficient Tree Search for Inference-Time Scaling", "authors_short": "Coleman Hooper et.al.", "published_date": "2025-06-11", "m_score": 8, "p_score": 7, "i_score": 8, "priority_score": 7.76, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_3", "front_name": "OR-Driven Adaptive Resource Allocation for LLM Inference and Test-Time Search", "front_status": "growing", "methods": ["tree_search", "llm_in_the_loop", "llm_as_heuristic", "llm_as_evaluator", "linear_programming"], "problems": ["llm_inference_optimization", "llm_serving_optimization", "mathematical_reasoning"], "code_url": "https://github.com/SqueezeAILab/ETS", "brief": "ETS formulates the tree search pruning step as a lightweight Integer Linear Program (ILP) that maximizes the reward of retained nodes while penalizing total KV cache size and enforcing semantic diversity via clustering. Unlike standard beam search or REBASE, it explicitly optimizes the trade-off between memory consumption (KV sharing) and exploration coverage. The authors demonstrate a 1.8x reduction in KV cache size and 1.4x throughput increase on MATH500 with minimal accuracy loss. We should steal the 'ILP-in-the-loop' mechanism for population management in our evolutionary search frameworks to optimize hardware utilization dynamically.", "affiliations": "University of California, Berkeley, ICSI, LBNL", "analysis_date": "2026-02-18"}, {"arxiv_id": "2502.00722", "arxiv_url": "https://arxiv.org/abs/2502.00722", "title": "Demystifying Cost-Efficiency in LLM Serving over Heterogeneous GPUs", "authors_short": "Youhe Jiang et.al.", "published_date": "2025-06-05", "m_score": 5, "p_score": 9, "i_score": 6, "priority_score": 6.39, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_name": "Integer Linear Programming for Heterogeneous LLM Serving Resource Allocation", "front_status": "growing", "methods": ["mixed_integer_linear_programming", "binary_search", "heuristics", "profiling", "data_parallelism"], "problems": ["llm_serving_optimization", "resource_allocation", "gpu_scheduling", "milp_general", "black_box_optimization"], "code_url": null, "brief": "Jiang et al. formulate LLM serving on heterogeneous clouds as a Mixed-Integer Linear Programming (MILP) problem, co-optimizing GPU rental composition, parallelism strategies (TP/PP), and workload routing. They demonstrate ~25% throughput gains over SOTA systems (Helix, HexGen) using vLLM benchmarks, validating the approach with strong empirical ablations. For our **GPUSched** project, the key takeaway is their solver strategy: pre-generating valid configurations to linearize the problem and using a binary search wrapper on the makespan to avoid direct minimization overhead. We should adopt their heuristics for pruning the configuration space (e.g., restricting TP to intra-node) to improve our own solver times.", "affiliations": "University of Cambridge, ETH Zurich, Peking University, The Hong Kong University of Science and Technology, Purdue University", "analysis_date": "2026-02-18"}, {"arxiv_id": "2411.19146", "arxiv_url": "https://arxiv.org/abs/2411.19146", "title": "Puzzle: Distillation-Based NAS for Inference-Optimized LLMs", "authors_short": "Akhiad Bercovich et.al.", "published_date": "2025-06-03", "m_score": 7, "p_score": 8, "i_score": 7, "priority_score": 6.74, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["neural_architecture_search", "knowledge_distillation", "blockwise_local_distillation", "global_knowledge_distillation", "mixed_integer_programming"], "problems": ["llm_inference_optimization", "hardware_aware_optimization", "constrained_optimization", "model_compression", "neural_network_architecture_design"], "code_url": null, "brief": "Bercovich et al. introduce Puzzle, a framework that optimizes LLM architectures for specific hardware by training a library of block variants (via local distillation) and using Mixed-Integer Programming (MIP) to select the optimal layer-wise configuration under strict latency and memory constraints. The results are robust: they compress Llama-70B to 51B, fitting on a single H100 with 2.17x throughput gain and 98.4% accuracy retention, significantly outperforming pruning baselines like Wanda. **Key takeaway:** The 'decomposed search' strategy—replacing expensive end-to-end evolutionary evaluation loops with local proxy scores (KL divergence) and a global MIP solver—is a highly efficient method for modular system configuration. This directly informs our 'GPUSched' and serving optimization work by demonstrating how to mathematically formulate hardware constraints (KV-cache, batch size, compute) into the model design process itself.", "affiliations": "NVIDIA", "analysis_date": "2026-02-18"}, {"arxiv_id": "2504.07347", "arxiv_url": "https://arxiv.org/abs/2504.07347", "title": "Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents", "authors_short": "Yueying Li et.al.", "published_date": "2025-04-24", "m_score": 8, "p_score": 9, "i_score": 8, "priority_score": 8.36, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_6", "front_name": "Optimizing LLM Inference and Automated OR Problem Generation", "front_status": "emerging", "methods": ["queueing_theory", "discrete_time_markov_chains", "fluid_limits", "lyapunov_function", "work_conserving_scheduling"], "problems": ["llm_inference_scheduling", "gpu_scheduling", "resource_allocation", "ai_agent_workloads", "multi_agent_coordination"], "code_url": null, "brief": "Li et al. formulate a batch queueing model for LLM inference, proving that 'work-conserving' algorithms (like Sarathi-Serve) which mix prefill and decode tokens are throughput-optimal, whereas separated strategies (vanilla vLLM, FasterTransformer) are theoretically unstable. The results are rigorous, combining fluid limit proofs with empirical validation on A100s showing queue blow-ups in non-optimal schedulers. The key takeaway is the precise definition of stability for token-level batching and the counter-intuitive finding that these locally optimal policies can fail in multi-agent networks due to cyclic resource dependencies. This is foundational reading for our GPUSched project and directly informs how we should model resource allocation for our multi-agent optimization systems.", "affiliations": "Cornell University, Columbia University", "analysis_date": "2026-02-18"}, {"arxiv_id": "2405.17743", "arxiv_url": "https://arxiv.org/abs/2405.17743", "title": "ORLM: A Customizable Framework in Training Large Models for Automated Optimization Modeling", "authors_short": "Chenyu Huang et.al.", "published_date": "2025-04-04", "m_score": 5, "p_score": 8, "i_score": 6, "priority_score": 6.24, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_6", "front_name": "Optimizing LLM Inference and Automated OR Problem Generation", "front_status": "emerging", "methods": ["instruction_tuning", "llm_fine_tuned", "synthetic_data_generation", "data_augmentation", "bootstrapping"], "problems": ["automated_optimization_modeling", "mathematical_modeling", "linear_programming", "integer_programming", "mixed_integer_programming"], "code_url": "https://github.com/cardinal-operations/orlm", "brief": "The authors propose OR-Instruct, a framework that uses GPT-4 to synthesize over 32k optimization modeling pairs (natural language to COPT code) to fine-tune 7B-scale models (ORLM). They demonstrate that these fine-tuned models outperform GPT-4 on their new 'IndustryOR' benchmark, a result that appears robust given the specialized nature of the task. The most valuable takeaway is their specific data augmentation strategy—iteratively altering constraints and injecting specific modeling techniques (e.g., Big M)—which provides a concrete recipe we can steal to generate diverse instances for our OR-Bench project. While the methodology is standard instruction tuning, the resulting artifacts (benchmark and model) establish a new baseline for automated OR modeling that we cannot ignore.", "affiliations": "Columbia University, Duke University, Shanghai Jiao Tong University, The Chinese University of Hong Kong, Shenzhen, Shenzhen Research Institute of Big Data, Shanghai University of Finance and Economics, Cardinal Operations", "analysis_date": "2026-02-18"}, {"arxiv_id": "2503.09357", "arxiv_url": "https://arxiv.org/abs/2503.09357", "title": "Automatic Operator-level Parallelism Planning for Distributed Deep Learning -- A Mixed-Integer Programming Approach", "authors_short": "Ruifeng She et.al.", "published_date": "2025-03-12", "m_score": 7, "p_score": 8, "i_score": 7, "priority_score": 6.99, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_0", "front_name": "Optimal LLM Inference Scheduling with Queueing Theory and MIP", "front_status": "emerging", "methods": ["mixed_integer_programming", "bi_level_optimization", "heuristic_search", "graph_partitioning", "gurobi"], "problems": ["distributed_deep_learning_scheduling", "llm_training_optimization", "llm_inference_optimization", "job_shop_scheduling", "flexible_distributed_job_shop_scheduling"], "code_url": null, "brief": "She et al. formulate distributed LLM training/inference as a Flexible Distributed Job Shop Scheduling Problem (FDJSSP) solved via Mixed-Integer Programming (MIP) combined with a heuristic graph coarsening step. They demonstrate that this automated approach not only reproduces DeepSeek V3's expert-designed \"DualPipe\" strategy but, when allowed to search longer, discovers a schedule with 50% fewer pipeline bubbles. The primary takeaway is the effectiveness of the bi-level optimization framework (greedy merging + MIP) to handle the scale of operator-level graphs, proving that formal OR methods can outperform manual system design for LLM infrastructure. This is a mandatory read for our GPUSched project, offering a concrete formulation for operator-level constraints we can directly adapt.", "affiliations": "Huawei", "analysis_date": "2026-02-18"}, {"arxiv_id": "2503.08796", "arxiv_url": "https://arxiv.org/abs/2503.08796", "title": "Robust Multi-Objective Controlled Decoding of Large Language Models", "authors_short": "Seongho Son et.al.", "published_date": "2025-03-11", "m_score": 8, "p_score": 6, "i_score": 8, "priority_score": 6.84, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_32", "front_name": "Convex Optimization and Game Theory for Robust Multi-Objective LLM Alignment", "front_status": "growing", "methods": ["controlled_decoding", "multi_objective_optimization", "robust_optimization", "game_theory", "maximin_optimization"], "problems": ["llm_alignment", "instruction_following", "helpfulness", "safety", "truthfulness"], "code_url": "https://github.com/williambankes/robust-multi-objective-decoding", "brief": "RMOD formulates multi-objective decoding as a zero-sum game between a policy and adversarial weights, solving a convex optimization problem at each decoding step to maximize the worst-case value estimate (essentially a Process Reward Model). The results are empirically strong, outperforming MO-DPO and scalarized baselines on alignment benchmarks by dynamically preventing any single objective from collapsing. **Key Takeaway:** The efficient inference-time weight optimization algorithm (Eq. 10) is a 'stealable' mechanism for **AlgoEvo** and **RobustMAS**. We should implement this dynamic adversarial weighting to balance conflicting code metrics (e.g., runtime vs. solution quality) during evolutionary search, replacing our current static scalarization methods.", "affiliations": "University College London, University of Basel, Ulsan National Institute of Science and Technology", "analysis_date": "2026-02-17"}, {"arxiv_id": "2406.01566", "arxiv_url": "https://arxiv.org/abs/2406.01566", "title": "Helix: Serving Large Language Models over Heterogeneous GPUs and Network via Max-Flow", "authors_short": "Yixuan Mei et.al.", "published_date": "2025-03-05", "m_score": 8, "p_score": 9, "i_score": 8, "priority_score": 8.26, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_10", "front_name": "Integer Linear Programming for Heterogeneous LLM Serving Resource Allocation", "front_status": "growing", "methods": ["max_flow_optimization", "milp", "pipeline_parallelism", "tensor_parallelism", "scheduling_algorithms"], "problems": ["llm_serving_optimization", "model_placement", "resource_scheduling"], "code_url": "https://github.com/Thesys-lab/Helix-ASPLOS25", "brief": "Helix formulates distributed LLM serving on heterogeneous clusters as a max-flow problem, using MILP to optimize model placement and deriving a per-request weighted round-robin scheduler from the flow solution. Unlike standard static pipeline parallelism, it routes every request dynamically based on edge capacities, achieving up to 3.3x throughput gains over Swarm on mixed GPU clusters (L4/T4/A100). The results are rigorous, backed by both physical cluster experiments and high-fidelity simulations. The critical takeaway is the 'per-request pipeline' abstraction: decoupling request routing from static device assignment allows exact OR methods to maximize utilization of weaker hardware—a technique we should immediately evaluate for our GPUSched project.", "affiliations": "Carnegie Mellon University", "analysis_date": "2026-02-17"}, {"arxiv_id": "2406.04508", "arxiv_url": "https://arxiv.org/abs/2406.04508", "title": "OCCAM: Towards Cost-Efficient and Accuracy-Aware Classification Inference", "authors_short": "Dujian Ding et.al.", "published_date": "2025-02-25", "m_score": 6, "p_score": 7, "i_score": 6, "priority_score": 5.03, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["integer_linear_programming", "nearest_neighbor_search", "accuracy_estimation", "variance_regularization", "algorithm_selection"], "problems": ["optimal_model_portfolio_problem", "classification_inference_optimization", "algorithm_selection", "cost_constrained_optimization", "image_classification"], "code_url": "https://github.com/DujianDing/OCCAM", "brief": "OCCAM formulates the inference model selection problem as an Integer Linear Program (ILP), using a nearest-neighbor estimator on validation embeddings to predict query-specific model accuracy. The authors provide theoretical guarantees for the estimator's bias and variance, demonstrating 40% cost reduction on ImageNet with <1% accuracy drop compared to heuristic baselines. The key takeaway is the **training-free, NNS-based accuracy estimator** combined with ILP; this avoids training complex routers and provides statistical guarantees. This is directly applicable to our **LLM serving optimization** (GPUSched) work for routing prompts between models of varying costs, and potentially for estimating fitness in **AlgoEvo** without full execution.", "affiliations": "University of British Columbia", "analysis_date": "2026-02-18"}, {"arxiv_id": "2410.06270", "arxiv_url": "https://arxiv.org/abs/2410.06270", "title": "Mixture Compressor for Mixture-of-Experts LLMs Gains More", "authors_short": "Wei Huang et.al.", "published_date": "2025-02-22", "m_score": 5, "p_score": 7, "i_score": 6, "priority_score": 4.99, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_34", "front_name": "Linear Programming for MoE LLM Mixed-Precision Quantization and Pruning", "front_status": "emerging", "methods": ["post_training_quantization", "mixed_precision_quantization", "expert_quantization", "dynamic_pruning", "expert_pruning"], "problems": ["llm_compression", "memory_optimization", "inference_efficiency"], "code_url": "https://github.com/aaronhuang-778/mc-moe", "brief": "Huang et al. propose a compression framework for MoE-LLMs that uses Integer Programming to optimally allocate mixed bit-widths (1-3 bits) to experts based on activation frequency and routing weights. They achieve strong empirical results, compressing Mixtral 8x7b to ~16GB (fitting on a single RTX 3090) with only a ~4% drop in zero-shot accuracy, significantly outperforming uniform quantization. The key takeaway is the explicit IP formulation for minimizing quantization error under memory constraints—a clean 'OR for AI' pattern we can adapt for our GPU scheduling or memory allocation formulations. While not a methodological advance in evolution, this is highly relevant for our infrastructure: it enables deploying high-quality MoE models on cheaper hardware for our massive AlgoEvo loops.", "affiliations": "The University of Hong Kong, The Chinese University of Hong Kong, Beihang University, Centre for Perceptual and Interactive Intelligence, Hong Kong", "analysis_date": "2026-02-18"}, {"arxiv_id": "2502.15763", "arxiv_url": "https://arxiv.org/abs/2502.15763", "title": "Hybrid Offline-online Scheduling Method for Large Language Model Inference Optimization", "authors_short": "Bowen Pang et.al.", "published_date": "2025-02-14", "m_score": 6, "p_score": 10, "i_score": 7, "priority_score": 7.19, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": "or_for_generative_ai_2026-02-18_front_0", "front_name": "Optimal LLM Inference Scheduling with Queueing Theory and MIP", "front_status": "emerging", "methods": ["mixed_integer_programming", "bin_packing", "makespan_minimization", "online_scheduling", "preemptive_scheduling"], "problems": ["llm_serving_optimization", "gpu_scheduling", "resource_allocation", "scheduling", "bin_packing"], "code_url": null, "brief": "Pang et al. formulate LLM inference scheduling as a Mixed-Integer Programming (MIP) model, solving it via a hybrid approach: offline bin-packing for request assignment and an online Lagrangian heuristic for prefill-decode preemption. They report a ~9% utilization increase (80.2% to 89.1%) over a vLLM-style baseline on LLaMA-65B, though the evaluation is limited to a single 8-GPU node and assumes deterministic output lengths for the offline component. The most actionable takeaway is their derivation of a simple cost-comparison threshold (prefill cost vs. decode wait cost) to dynamically inject prefill tasks into decoding streams. This provides a concrete, low-overhead heuristic baseline for our GPUSched work.", "affiliations": "Noah’s Ark Lab, Huawei, Tsinghua University", "analysis_date": "2026-02-18"}, {"arxiv_id": "2412.01523", "arxiv_url": "https://arxiv.org/abs/2412.01523", "title": "FlexSP: Accelerating Large Language Model Training via Flexible Sequence Parallelism", "authors_short": "Yujie Wang et.al.", "published_date": "2025-02-11", "m_score": 8, "p_score": 6, "i_score": 7, "priority_score": 5.54, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["sequence_parallelism", "distributed_training", "system_optimization", "adaptive_parallelism", "linear_programming"], "problems": ["llm_training_efficiency", "long_context_llm_training", "workload_heterogeneity", "communication_optimization", "memory_optimization"], "code_url": "https://github.com/AFDWang/ASPLOS25-FlexSP-Supplemental-Material", "brief": "FlexSP optimizes distributed LLM training by dynamically assigning varied-length sequences to heterogeneous Sequence Parallelism (SP) groups using a Mixed-Integer Linear Programming (MILP) solver in the loop. The results are solid, showing up to 1.98x speedup on A100 clusters by mitigating communication bottlenecks for short sequences while preventing OOM for long ones. **Key Takeaway:** The authors use Dynamic Programming to 'bucket' similar sequences, drastically reducing the variable count for the MILP solver; this specific technique—reducing problem granularity to make exact solvers feasible in real-time systems—is directly applicable to our 'GPUSched' and inference resource allocation work. While we focus on evolution, this is a definitive reference for our 'OR for AI Systems' track, proving that formal optimization can beat heuristics in dynamic GPU scheduling.", "affiliations": "Peking University, ByteDance Inc., Beihang University", "analysis_date": "2026-02-19"}, {"arxiv_id": "2502.06643", "arxiv_url": "https://arxiv.org/abs/2502.06643", "title": "MoETuner: Optimized Mixture of Expert Serving with Balanced Expert Placement and Token Routing", "authors_short": "Seokjin Go et.al.", "published_date": "2025-02-10", "m_score": 8, "p_score": 9, "i_score": 7, "priority_score": 7.29, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["integer_linear_programming", "resource_allocation", "load_balancing", "system_level_optimization", "expert_placement"], "problems": ["llm_serving_optimization", "distributed_system_optimization", "resource_allocation", "load_balancing", "tail_latency_reduction"], "code_url": null, "brief": "Go et al. formulate the MoE expert placement problem as a two-stage Integer Linear Program (ILP) to balance token load and minimize communication tail latency, exploiting stable token routing dependencies across layers. They demonstrate real-world speedups of 17.5% on multi-node H200 clusters running Mixtral-8x7B, validating the approach with concrete systems measurements rather than just simulation. The key takeaway is the effectiveness of a min-max ILP objective for reducing tail latency in distributed inference, proving that static optimization based on profiling is sufficient for significant gains. This directly supports our 'OR for AI systems' track and provides a strong baseline formulation for our GPU scheduling work.", "affiliations": "Georgia Institute of Technology", "analysis_date": "2026-02-19"}, {"arxiv_id": "2407.12117", "arxiv_url": "https://arxiv.org/abs/2407.12117", "title": "MEMO: Fine-grained Tensor Management For Ultra-long Context LLM Training", "authors_short": "Pinxue Zhao et.al.", "published_date": "2025-01-15", "m_score": 8, "p_score": 5, "i_score": 7, "priority_score": 5.29, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["activation_recomputation", "activation_swapping", "mixed_integer_programming", "memory_management", "tensor_management"], "problems": ["llm_training_efficiency", "long_context_llm_training", "activation_memory_optimization", "memory_fragmentation"], "code_url": "https://github.com/pinxuezhao/MEMO", "brief": "Memo enables training 7B LLMs with 1M context on 8 GPUs by combining token-wise activation swapping with a bi-level Mixed Integer Programming (MIP) approach to eliminate memory fragmentation. The results are strong (52% MFU vs ~30% for DeepSpeed) and demonstrate that static memory planning via OR solvers outperforms dynamic allocators for repetitive Transformer workloads. The key takeaway is the bi-level MIP strategy—solving the allocation for one layer and broadcasting it—which makes the NP-hard memory planning tractable. We should adapt this MIP formulation for our own GPU scheduling and inference resource allocation (GPUSched) projects.", "affiliations": "Peking University, Tencent Inc.", "analysis_date": "2026-02-19"}, {"arxiv_id": "2501.03508", "arxiv_url": "https://arxiv.org/abs/2501.03508", "title": "A Sequential Optimal Learning Approach to Automated Prompt Engineering in Large Language Models", "authors_short": "Shuyang Wang et.al.", "published_date": "2025-01-07", "m_score": 8, "p_score": 7, "i_score": 8, "priority_score": 7.61, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["optimal_learning", "knowledge_gradient_policy", "bayesian_regression", "sequential_decision_making", "mixed_integer_conic_optimization"], "problems": ["llm_prompt_optimization", "instruction_induction", "automated_prompt_engineering", "constrained_optimization"], "code_url": null, "brief": "Wang et al. treat prompt engineering as a Bayesian optimal experimental design problem, representing prompts as discrete feature vectors (template, tone, examples) and selecting the next candidate using a Knowledge-Gradient (KG) policy solved via mixed-integer second-order cone programming. Results are rigorous and show that this OR-based approach outperforms evolutionary (EvoPrompt) and bandit baselines on instruction induction tasks, specifically in low-sample regimes (N=30). The critical takeaway is the **replacement of random evolutionary mutation with a KG policy over a structured feature space** to maximize information gain per step. We should steal this formulation to optimize high-level meta-parameters or strategy selection in AlgoEvo, leveraging our team's OR background to solve our sample efficiency bottleneck.", "affiliations": "Northwestern University, Stevens Institute of Technology", "analysis_date": "2026-02-19"}, {"arxiv_id": "2407.15504", "arxiv_url": "https://arxiv.org/abs/2407.15504", "title": "Fundamental Limits of Prompt Compression: A Rate-Distortion Framework for Black-Box Language Models", "authors_short": "Alliot Nagle et.al.", "published_date": "2024-12-11", "m_score": 8, "p_score": 8, "i_score": 7, "priority_score": 7.71, "must_read": true, "changes_thinking": true, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["rate_distortion_theory", "linear_programming", "dual_linear_program", "geometric_algorithm", "token_classification"], "problems": ["prompt_compression", "llm_inference_efficiency", "black_box_optimization"], "code_url": "https://github.com/acnagle/fundamental-limits", "brief": "Nagle et al. formalize prompt compression as a rate-distortion problem, deriving the fundamental theoretical limit via a dual linear program and proposing 'Adaptive QuerySelect,' a variable-rate compression technique. The results are rigorous: they calculate exact limits on synthetic data and use beam search approximations for NLP, demonstrating that existing fixed-rate methods leave significant performance on the table. The key takeaway is that **variable-rate compression**—keeping tokens based on a confidence threshold rather than a fixed percentage—is essential for approaching optimality; this allows 'hard' queries to retain more context while aggressively compressing 'easy' ones. This is immediately actionable for our AlgoEvo work: we should replace fixed-window history truncation with a query-aware, variable-rate compressor to maximize the useful information in our limited context window.", "affiliations": "UT Austin, EPFL", "analysis_date": "2026-02-19"}, {"arxiv_id": "2409.08692", "arxiv_url": "https://arxiv.org/abs/2409.08692", "title": "B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests", "authors_short": "Mouxiang Chen et.al.", "published_date": "2024-09-13", "m_score": 8, "p_score": 8, "i_score": 8, "priority_score": 7.19, "must_read": true, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["bayesian_inference", "maximum_a_posteriori", "beta_distribution", "integer_programming", "consensus_set_partitioning"], "problems": ["code_generation_evaluation", "plausible_solution_selection", "plausible_test_assessment"], "code_url": "https://github.com/zju-ctag/b4", "brief": "Chen et al. derive a Bayesian posterior estimator (B4) for selecting correct code solutions using unreliable (LLM-generated) tests, explicitly modeling the probability of incorrect code passing incorrect tests. They demonstrate statistically significant improvements (up to 50% relative gain on hard problems) over state-of-the-art heuristics like CodeT and MaxPass on HumanEval and APPS. The key takeaway is the B4 scoring formula: a product of four Beta functions that weighs consensus sets based on priors about test reliability (e.g., incorrect code rarely passes incorrect tests). This is immediately actionable for AlgoEvo: we can replace our naive fitness aggregation with B4 to improve selection accuracy when using generated unit tests, directly boosting sample efficiency.", "affiliations": "Zhejiang University, Singapore Management University", "analysis_date": "2026-02-19"}, {"arxiv_id": "2409.07045", "arxiv_url": "https://arxiv.org/abs/2409.07045", "title": "Beyond IID: Optimizing Instruction Learning from the Perspective of Instruction Interaction and Dependency", "authors_short": "Hanyu Zhao et.al.", "published_date": "2024-09-11", "m_score": 6, "p_score": 5, "i_score": 7, "priority_score": 4.68, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["causal_inference", "causal_intervention", "linear_programming", "curriculum_learning", "supervised_fine_tuning"], "problems": ["instruction_set_optimization", "llm_data_optimization", "llm_alignment", "causal_discovery", "ability_taxonomy_induction"], "code_url": "https://github.com/BAAI-DIPL/sft-set-optimization-via-instruction-interaction-and-dependency", "brief": "The authors propose optimizing SFT data mixtures using Linear Programming (EE-CPO) by modeling the 'interaction' (synergy/antagonism) between instruction categories, rather than treating them as IID. They empirically derive a dependency taxonomy showing Math and Code are fundamental 'root' capabilities required before learning complex tasks, validating this via curriculum learning experiments that beat DEITA. The results are solid (+1.73 AlpacaEval over DEITA), though the cost of deriving the interaction matrix (training N models) is high. **Takeaway:** The 'Effect Equivalence Coefficient' matrix combined with an LP solver is a rigorous OR formulation for resource/data allocation that we should steal to optimize heuristic populations in our evolutionary search frameworks.", "affiliations": "Beijing Academy of Artificial Intelligence", "analysis_date": "2026-02-19"}, {"arxiv_id": "2407.13126", "arxiv_url": "https://arxiv.org/abs/2407.13126", "title": "Improving GPU Multi-Tenancy Through Dynamic Multi-Instance GPU Reconfiguration", "authors_short": "Tianyu Wang et.al.", "published_date": "2024-07-18", "m_score": 6, "p_score": 7, "i_score": 6, "priority_score": 5.03, "must_read": false, "changes_thinking": false, "team_discussion": true, "front_id": null, "front_name": "", "front_status": null, "methods": ["integer_linear_programming", "gurobi_solver", "transformer", "supervised_learning", "gpu_resource_management"], "problems": ["gpu_scheduling", "cloud_scheduling", "multi_tenancy", "continuous_learning", "service_level_objective_optimization"], "code_url": null, "brief": "MIGRator formulates dynamic NVIDIA MIG partitioning as an Integer Linear Program (ILP) to optimize a compound 'Goodput' metric (SLO + accuracy) for continuous learning workloads. The results on A100s show ~20% gains over baselines like Ekya and PARIS, largely by mitigating the massive ~6s MIG reconfiguration overhead via a 'pre-initialization' lookahead strategy. For our GPUSched project, the key takeaway is the explicit modeling of reconfiguration penalties in the ILP and the technique of pre-assembling instances during idle time to hide latency. While the reliance on 200-second traffic prediction is a potential fragility, the rigorous handling of hardware constraints makes this a strong reference for our OR-based resource allocation work.", "affiliations": "UC San Diego, University of Pittsburgh, University of Arizona, University of Georgia", "analysis_date": "2026-02-19"}]}}