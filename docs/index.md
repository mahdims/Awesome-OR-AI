---
layout: default
---

## Updated on 2026.02.19
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#llms-for-algorithm-design>LLMs for Algorithm Design</a></li>
    <li><a href=#generative-ai-for-or>Generative AI for OR</a></li>
    <li><a href=#or-for-generative-ai>OR for Generative AI</a></li>
  </ol>
</details>
## LLMs for Algorithm Design

### üÜï Most Recent

| Date | Title | Authors | Affiliation | Links |
|------|-------|---------|-------------|-------|
| 2026-02-17 | <details><summary>**MadEvolve: Evolutionary Optimization of Cosmological Algorithms with Large Language Models**</summary>MadEvolve extends AlphaEvolve by embedding a gradient-based optimization loop (via JAX) inside the fitness evaluation, allowing the LLM to focus purely on code structure while an optimizer (Adam) handles continuous parameters. They demonstrate 20-30% performance gains on complex cosmological reconstruction tasks, validated on held-out simulations. The critical takeaway is the architectural pattern: prompt the LLM to write differentiable code rather than tuning constants, and use a UCB1 bandit to dynamically select between cheap and expensive models. We should immediately adopt the differentiable inner-loop strategy for our continuous heuristic search projects.</details> | Tianyi Li et.al. | University of Wisconsin-Madison | [pdf](http://arxiv.org/abs/2602.15951) / [code](https://github.com/tianyi-stack/MadEvolve-Cosmo) |
| 2026-02-17 | <details><summary>**Heuristic Search as Language-Guided Program Optimization**</summary>LaGO decomposes automated heuristic design into three explicit modules: evaluation, a code-writing 'Analyst' (backward pass), and a diversity-aware 'Generator' (update), while co-evolving constructive and refinement heuristics. The authors demonstrate significant gains (+0.17 QYI) on PDPTW and Crew Pairing against ReEvo and EoH, showing that joint optimization of initialization and improvement prevents local optima. The critical takeaway is the 'Analyst' module: instead of asking the LLM for text critiques, they ask it to write Python feature extraction functions to statistically characterize solution quality‚Äîa technique we should immediately adopt to upgrade our fitness signals in AlgoEvo.</details> | Mingxin Yu et.al. | Massachusetts Institute of Technology | [pdf](http://arxiv.org/abs/2602.16038) / code |
| 2026-02-10 | <details><summary>**ImprovEvolve: Ask AlphaEvolve to Improve the Input Solution and Then Improvise**</summary>Kravatskiy et al. introduce ImprovEvolve, a framework that restricts the LLM to evolving `improve()` (local search) and `perturb()` (mutation) operators, which are then executed by a fixed basin-hopping algorithm. They achieve new state-of-the-art results on Hexagon Packing and the Second Autocorrelation Inequality, demonstrating that this modular approach generalizes to unseen problem sizes where monolithic AlphaEvolve solutions fail. The critical insight is that LLMs are poor at designing global search logic and tuning hyperparameters (LLM edits actively harmed performance), so we should isolate the LLM to generating local moves while keeping the meta-heuristic framework deterministic. We should immediately apply this 'operator-only' evolution strategy to our ALNS research for VRP.</details> | Alexey Kravatskiy et.al. | MIRIAI, FusionBrain Lab, Institute of Numerical Mathematics | [pdf](http://arxiv.org/abs/2602.10233) / code |
| 2026-02-09 | <details><summary>**G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design**</summary>G-LNS extends LLM-based evolutionary search to ALNS by co-evolving Python code for Destroy and Repair operators rather than constructive priority rules. The authors introduce a 'Synergy Matrix' that tracks the performance of specific operator pairs during evaluation, using this data to guide a 'Synergistic Joint Crossover' where the LLM optimizes the coupling between destroy and repair logic. Results are strong: it significantly outperforms FunSearch and EoH on TSP/CVRP and beats OR-Tools on large-scale instances (N=200) under time constraints. The key takeaway for AlgoEvo is the synergy-aware co-evolution mechanism‚Äîexplicitly tracking and prompting for component interaction is a concrete technique we can apply to multi-agent optimization systems.</details> | Baoyun Zhao et.al. | Tsinghua University, University of Chinese Academy of Sciences, Northeastern University | [pdf](http://arxiv.org/abs/2602.08253) / [code](https://github.com/zboyn/G-LNS) |
| 2026-02-09 | <details><summary>**Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery**</summary>ASRO adapts Policy Space Response Oracles (PSRO) to code generation, treating heuristic discovery as a zero-sum game where a 'Solver' evolves to minimize gaps and a 'Generator' evolves to create adversarial instances. The results are compelling: it consistently beats the static EoH baseline on TSPLIB and CVRPLIB, proving that adversarial training yields better generalization than training on fixed distributions. The critical takeaway is the architecture: explicitly co-evolving an 'Instance Generator' program alongside the solver prevents overfitting and exposes edge cases (like specific geometric traps in TSP) that static benchmarks miss. This is a direct upgrade to our AlgoEvo/AlphaEvolve pipelines, though it incurs higher computational costs due to the evaluation matrix required for the meta-game.</details> | Xinyi Ke et.al. | Tsinghua University, Chinese Academy of Sciences, University of Chinese Academy of Sciences, AiRiA | [pdf](http://arxiv.org/abs/2601.22896) / code |

### ‚≠ê Best Papers

| Score | Date | Title | Authors | Affiliation | Links |
|-------|------|-------|---------|-------------|-------|
| 30/30 | 2025-06-16 | <details><summary>**AlphaEvolve: A coding agent for scientific and algorithmic discovery**</summary>AlphaEvolve extends FunSearch by evolving entire code files (rather than single functions) using a 'search/replace' diff format and Gemini 2.0, achieving SOTA results across matrix multiplication (beating Strassen), 50+ open math problems, and Google's production scheduling. The results are exceptionally strong and verified, including deployed improvements to Google's Borg scheduler (0.7% resource recovery) and TPU circuits. The critical takeaway is the move to **diff-based full-file evolution** and **meta-prompt evolution** (evolving the prompt instructions alongside the code), which allows the system to modify architecture and logic rather than just heuristics. This is a mandatory blueprint for the next iteration of our AlgoEvo and EvoCut projects.</details> | Alexander Novikov et.al. | Google DeepMind | [pdf](http://arxiv.org/abs/2506.13131) / [code](https://colab.research.google.com/github/google-deepmind/alphaevolve_results/blob/master/mathematical_results.ipynb) |
| 29/30 | 2026-01-22 | <details><summary>**Learning to Discover at Test Time**</summary>TTT-Discover introduces a method to fine-tune an LLM (gpt-oss-120b) *during* inference on a single test problem using RL, replacing the frozen-model evolutionary search of AlphaEvolve. They employ a novel 'entropic objective' that optimizes for the single best solution (discovery) rather than expected return, combined with PUCT-based state reuse. The results are empirically rigorous, setting new SOTA on Erd≈ës‚Äô problem, GPU kernel optimization, and AtCoder contests, directly beating AlphaEvolve and ShinkaEvolve. The critical takeaway is that for hard discovery tasks, shifting the model's distribution via online updates is superior to context-based search; we should immediately test their entropic objective in our AlgoEvo pipeline.</details> | Mert Yuksekgonul et.al. | Stanford University, NVIDIA, UC San Diego, Together AI, Astera Institute | [pdf](http://arxiv.org/abs/2601.16175) / [code](https://github.com/test-time-training/discover) |
| 28/30 | 2026-02-17 | <details><summary>**Heuristic Search as Language-Guided Program Optimization**</summary>LaGO decomposes automated heuristic design into three explicit modules: evaluation, a code-writing 'Analyst' (backward pass), and a diversity-aware 'Generator' (update), while co-evolving constructive and refinement heuristics. The authors demonstrate significant gains (+0.17 QYI) on PDPTW and Crew Pairing against ReEvo and EoH, showing that joint optimization of initialization and improvement prevents local optima. The critical takeaway is the 'Analyst' module: instead of asking the LLM for text critiques, they ask it to write Python feature extraction functions to statistically characterize solution quality‚Äîa technique we should immediately adopt to upgrade our fitness signals in AlgoEvo.</details> | Mingxin Yu et.al. | Massachusetts Institute of Technology | [pdf](http://arxiv.org/abs/2602.16038) / code |
| 28/30 | 2026-02-02 | <details><summary>**DeltaEvolve: Accelerating Scientific Discovery through Momentum-Driven Evolution**</summary>DeltaEvolve replaces the standard full-code history in evolutionary search with 'semantic deltas'‚Äîstructured text summaries capturing the 'from/to' logic of modifications and their hypotheses. Across 5 domains (including BBOB and Symbolic Regression), they demonstrate superior objective scores over AlphaEvolve while reducing token consumption by ~37%. The critical takeaway is the 'Progressive Disclosure' mechanism: treating history as a momentum vector (deltas) rather than a state archive (snapshots) allows us to fit a deeper evolutionary trajectory into the context window. We should immediately test their 'Delta Plan' prompt structure in AlgoEvo to improve sample efficiency and reduce costs.</details> | Jiachen Jiang et.al. | Microsoft, The Ohio State University | [pdf](http://arxiv.org/abs/2602.02919) / code |
| 28/30 | 2025-12-30 | <details><summary>**LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm**</summary>LoongFlow replaces the standard stochastic mutation operator in LLM evolutionary search with a 'Plan-Execute-Summarize' (PES) cognitive loop. Instead of random code changes, a Planner retrieves the 'intent' and 'summary' of the parent solution's lineage to generate a directed hypothesis, which is then executed and summarized for the next generation. The authors demonstrate a 60% reduction in evaluations and a 100% success rate on AlphaEvolve tasks where standard methods fail or stagnate. The critical takeaway is the 'Lineage-Based Context Retrieval' mechanism: explicitly passing the parent's plan and retrospective summary to the child allows for directed rather than random walks in the search space. We must implement this PES loop in AlgoEvo immediately to fix our sample efficiency issues.</details> | Chunhui Wan et.al. |  | [pdf](http://arxiv.org/abs/2512.24077) / [code](https://github.com/baidu-baige/LoongFlow) |

### üî¨ Research Fronts

| Status | Front Name | Papers | Front Analysis |
|--------|-----------|--------|----------------|
| ‚úÖ Stable | Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design | <details><summary>23</summary><ol><li><a href="http://arxiv.org/abs/2506.13131">AlphaEvolve: A coding agent for scientific and algorithmic discovery</a> <sub>(30)</sub></li><li><a href="http://arxiv.org/abs/2601.16175">Learning to Discover at Test Time</a> <sub>(29)</sub></li><li><a href="http://arxiv.org/abs/2505.22954">Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents</a> <sub>(28)</sub></li><li><a href="http://arxiv.org/abs/2602.02919">DeltaEvolve: Accelerating Scientific Discovery through Momentum-Driven Evolution</a> <sub>(28)</sub></li><li><a href="http://arxiv.org/abs/2511.23473">ThetaEvolve: Test-time Learning on Open Problems</a> <sub>(28)</sub></li><li><a href="http://arxiv.org/abs/2509.07367">Autonomous Code Evolution Meets NP-Completeness</a> <sub>(28)</sub></li><li><a href="http://arxiv.org/abs/2512.24077">LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm</a> <sub>(28)</sub></li><li><a href="http://arxiv.org/abs/2509.19349">ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution</a> <sub>(26)</sub></li><li><a href="http://arxiv.org/abs/2511.02864">Mathematical exploration and discovery at scale</a> <sub>(25)</sub></li><li><a href="http://arxiv.org/abs/2510.14150">CodeEvolve: an open source evolutionary coding agent for algorithm discovery and optimization</a> <sub>(24)</sub></li><li><a href="http://arxiv.org/abs/2601.21847">READY: Reward Discovery for Meta-Black-Box Optimization</a> <sub>(24)</sub></li><li><a href="http://arxiv.org/abs/2601.21096">Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve</a> <sub>(24)</sub></li><li><a href="http://arxiv.org/abs/2510.06056">Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep Research</a> <sub>(24)</sub></li><li><a href="http://arxiv.org/abs/2509.23331">C-Evolve: Consensus-based Evolution for Prompt Groups</a> <sub>(24)</sub></li><li><a href="http://arxiv.org/abs/2602.13218">Scaling the Scaling Logic: Agentic Meta-Synthesis of Logic Reasoning</a> <sub>(24)</sub></li><li><a href="http://arxiv.org/abs/2511.17592">GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2509.18057">Reinforced Generation of Combinatorial Structures: Hardness of Approximation</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2507.17668">How Should We Meta-Learn Reinforcement Learning Algorithms?</a> <sub>(22)</sub></li><li><a href="http://arxiv.org/abs/2602.05688">Mining Generalizable Activation Functions</a> <sub>(21)</sub></li><li><a href="http://arxiv.org/abs/2602.10233">ImprovEvolve: Ask AlphaEvolve to Improve the Input Solution and Then Improvise</a> <sub>(21)</sub></li><li><a href="http://arxiv.org/abs/2601.05943">Global Optimization for Combinatorial Geometry Problems Revisited in the Era of LLMs</a> <sub>(21)</sub></li><li><a href="http://arxiv.org/abs/2511.08522">AlphaResearch: Accelerating New Algorithm Discovery with Language Models</a> <sub>(20)</sub></li><li><a href="http://arxiv.org/abs/2602.03545">Persona Generators: Generating Diverse Synthetic Personas at Scale</a> <sub>(19)</sub></li></ol></details> | <details><summary>View analysis</summary>This research front explores advanced architectural innovations in LLM-guided algorithm design, moving beyond basic Evolution of Heuristics (EoH) and FunSearch paradigms. It focuses on sophisticated co-evolutionary strategies, multi-agent systems, and novel population representations to improve the discovery and performance of heuristics for complex combinatorial optimization problems. Key themes include evolving interdependent operators, dynamically adapting search strategies, and co-evolving problem instances or prompt templates alongside the algorithms themselves.  Key contributions include E2OC, which uses MCTS to co-evolve interdependent operators, achieving up to +22% Hypervolume on FJSP/TSP. LLM4EO demonstrates online operator design for Flexible Job Shop Scheduling, yielding 3-4% RPD_BM improvement. ASRO introduces a game-theoretic framework for co-evolving solvers and adversarial instance generators, outperforming EoH by 0.5-30% on OBP, TSP, and CVRP. A-CEoH enhances prompts with algorithmic context, enabling smaller LLMs to generate superior A* heuristics for UPMP and SPP. EvoLattice proposes a DAG-based population representation with alternative-level statistics, boosting performance on NAS-Bench-Zero by over 150%. Other notable work includes LLM-driven test function generation (EoTF), co-evolution of prompts and Fireworks Algorithm operators (achieving 100% on Aircraft Landing vs. 56% for ReEvo), the dual-expert LLM4DRD for dynamic scheduling, TIDE's nested evolution for decoupling structure and parameter tuning (reducing TSP gap by 7.35%), RoCo's multi-agent system with long-term reflection, and EoH-S, which evolves complementary heuristic *sets* to reduce optimality gaps by 40-60% compared to single-heuristic approaches.  This front is rapidly emerging and maturing, characterized by a shift towards more complex, integrated LLM-driven systems. The trajectory indicates a strong focus on improving the efficiency, robustness, and generalization capabilities of generated algorithms. Future work will likely integrate these diverse architectural advancements, such as combining nested evolutionary loops with graph-based population representations, and expanding to more challenging multi-objective, constrained, and real-world dynamic optimization problems.</details> |
| ‚úÖ Stable | Enhanced LLM Evolutionary Search via Concept Learning and Co-Evolution | <details><summary>15</summary><ol><li><a href="http://arxiv.org/abs/2510.11121">Refining Hybrid Genetic Search for CVRP via Reinforcement Learning-Finetuned LLM</a> <sub>(28)</sub></li><li><a href="http://arxiv.org/abs/2602.03132">Contrastive Concept-Tree Search for LLM-Assisted Algorithm Discovery</a> <sub>(26)</sub></li><li><a href="http://arxiv.org/abs/2510.08755">Robust Heuristic Algorithm Design with LLMs</a> <sub>(26)</sub></li><li><a href="http://arxiv.org/abs/2602.08253">G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design</a> <sub>(25)</sub></li><li><a href="http://arxiv.org/abs/2510.14825">Programmatic Representation Learning with Language Models</a> <sub>(24)</sub></li><li><a href="http://arxiv.org/abs/2412.20694">QUBE: Enhancing Automatic Heuristic Design via Quality-Uncertainty Balanced Evolution</a> <sub>(24)</sub></li><li><a href="http://arxiv.org/abs/2510.06189">Barbarians at the Gate: How AI is Upending Systems Research</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2411.19744">Amplifying human performance in combinatorial competitive programming</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2509.24509">Experience-Guided Reflective Co-Evolution of Prompts and Heuristics for Automatic Algorithm Design</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2406.04824">FunBO: Discovering Acquisition Functions for Bayesian Optimization with FunSearch</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2601.16849">The Art of Being Difficult: Combining Human and AI Strengths to Find Adversarial Instances for Heuristics</a> <sub>(20)</sub></li><li><a href="http://arxiv.org/abs/2504.00613">LLM-Guided Search for Deletion-Correcting Codes</a> <sub>(19)</sub></li><li><a href="http://arxiv.org/abs/2402.02456">tnGPS: Discovering Unknown Tensor Network Structure Search Algorithms via Large Language Models (LLMs)</a> <sub>(19)</sub></li><li><a href="http://arxiv.org/abs/2502.09544">Explainable AI-assisted Optimization for Feynman Integral Reduction</a> <sub>(18)</sub></li><li><a href="http://arxiv.org/abs/2508.01558">EvoVLMA: Evolutionary Vision-Language Model Adaptation</a> <sub>(18)</sub></li></ol></details> | <details><summary>View analysis</summary>This research front significantly advances LLM-guided evolutionary search for algorithm design, primarily building upon and enhancing the FunSearch framework. It explores various architectural and methodological improvements to boost the efficiency, robustness, and generalization of automatically discovered algorithms. Key themes include evolving specific components like acquisition functions (FunBO), Vision-Language Model adaptation strategies (EvoVLMA), and competitive programming scoring functions, as well as discovering tensor network structures (tnGPS) and deletion-correcting codes.  Several papers introduce significant contributions. Contrastive Concept-Tree Search (CCTS) extracts hierarchical concepts from generated programs to guide search, showing consistent improvements over k-elite selection on combinatorial tasks. Robusta enhances FunSearch by using a Heuristic Analyzer and Suggester LLM to explain failures, achieving a 28x improvement in worst-case performance on traffic engineering. G-LNS co-evolves destroy and repair operators for Large Neighborhood Search, outperforming OR-Tools on large CVRP instances. EvoPH co-evolves prompts and heuristics using an island model and strategy sampling, dominating FunSearch on TSP and BPP. QUBE improves parent selection in FunSearch by using an uncertainty-inclusive quality metric based on offspring performance, leading to better results on Bin Packing and TSP. Zhu et al. demonstrate RL-finetuning of a Qwen-14B model to generate C++ crossover operators for HGS, outperforming GPT-4o and expert-designed components on CVRPLIB.  This front is rapidly maturing, moving beyond basic LLM-as-code-generator paradigms to sophisticated, self-improving search architectures. The trajectory indicates a shift towards more robust, interpretable, and efficient algorithm discovery. Future work will likely focus on integrating more advanced LLM reasoning capabilities, developing better feedback mechanisms (e.g., automated generalization of adversarial instances), and scaling these methods to even more complex, real-world problems with higher computational demands. The emphasis on co-evolution, concept learning, and failure analysis suggests a move towards more "white-box" and adaptive evolutionary systems.</details> |
| ‚úÖ Stable | LLM-Enhanced Evolutionary Search: Diversity, RL, and Structural Co-evolution | <details><summary>12</summary><ol><li><a href="http://arxiv.org/abs/2503.10721">From Understanding to Excelling: Template-Free Algorithm Design through Structural-Functional Co-Evolution</a> <sub>(26)</sub></li><li><a href="http://arxiv.org/abs/2505.12285">CALM: Co-evolution of Algorithms and Language Model for Automatic Heuristic Design</a> <sub>(26)</sub></li><li><a href="http://arxiv.org/abs/2401.02051">Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model</a> <sub>(25)</sub></li><li><a href="http://arxiv.org/abs/2508.03661">Automated Algorithmic Discovery for Scientific Computing through LLM-Guided Evolutionary Search: A Case Study in Gravitational-Wave Detection</a> <sub>(25)</sub></li><li><a href="http://arxiv.org/abs/2506.11057">STRCMP: Integrating Graph Structural Priors with Language Models for Combinatorial Optimization</a> <sub>(25)</sub></li><li><a href="http://arxiv.org/abs/2402.01145">ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution</a> <sub>(25)</sub></li><li><a href="http://arxiv.org/abs/2409.16867">Multi-objective Evolution of Heuristic Using Large Language Model</a> <sub>(25)</sub></li><li><a href="http://arxiv.org/abs/2504.05108">Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning</a> <sub>(24)</sub></li><li><a href="http://arxiv.org/abs/2412.14995">HSEvo: Elevating Automatic Heuristic Design with Diversity-Driven Harmony Search and Genetic Algorithm Using LLMs</a> <sub>(22)</sub></li><li><a href="http://arxiv.org/abs/2407.10873">Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models</a> <sub>(20)</sub></li><li><a href="http://arxiv.org/abs/2410.22657">Automatic programming via large language models with population self-evolution for dynamic job shop scheduling problem</a> <sub>(20)</sub></li><li><a href="http://arxiv.org/abs/2403.11446">LLM Guided Evolution -- The Automation of Models Advancing Models</a> <sub>(18)</sub></li></ol></details> | <details><summary>View analysis</summary>This research front focuses on advancing LLM-based evolutionary search for automated algorithm design, moving beyond initial frameworks like FunSearch and EoH. The unifying theme involves integrating sophisticated mechanisms to enhance population diversity, incorporate reinforcement learning (RL) for iterative LLM fine-tuning, and enable structural co-evolution of algorithm components. Key frameworks include MEoH for multi-objective search, EvoTune and CALM for RL-infused evolution, CAE and STRCMP for structural priors, and ReEvo for advanced reflective evolution.  Significant contributions include MEoH's 'Dominance-Dissimilarity' mechanism, achieving up to 20x better gaps on Bin Packing and 16x faster TSP heuristics. EvoTune demonstrated up to 15% better optimality gaps on Bin Packing/Flow Shop with DPO and Forward KL regularization. CAE reduced TSP optimality gaps by 2-5% through bi-dimensional structural-functional co-evolution, while STRCMP fused GNNs with LLMs to significantly reduce convergence times on MILP/SAT. CALM achieved superior performance on Bin Packing and VRP by online LLM fine-tuning with relative improvement rewards, outperforming GPT-4o baselines. EoH's 'E2' prompt strategy enabled dual-track evolution of 'thoughts' and code, outperforming FunSearch with significantly fewer LLM queries. Empirically, Zhang et al. highlighted that simple (1+1)-EPS often matches complex methods, underscoring the need for robust baselines.  This front is rapidly maturing, characterized by a shift from foundational LLM-evolutionary concepts to highly specialized and integrated approaches. The trajectory indicates a strong emphasis on improving sample efficiency, reducing computational costs, and enhancing the robustness and generalizability of discovered algorithms. Future work will likely converge on hybrid frameworks that combine the strengths of RL-based fine-tuning, structural co-evolution, and advanced diversity-maintaining mechanisms to tackle increasingly complex and real-world combinatorial optimization problems.</details> |
| ‚úÖ Stable | Co-Evolutionary LLM Architectures for Enhanced Heuristic and Algorithm Design | <details><summary>11</summary><ol><li><a href="http://arxiv.org/abs/2601.22896">Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery</a> <sub>(27)</sub></li><li><a href="http://arxiv.org/abs/2601.21239">TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design</a> <sub>(27)</sub></li><li><a href="http://arxiv.org/abs/2512.13857">EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery</a> <sub>(26)</sub></li><li><a href="http://arxiv.org/abs/2508.03082">EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic Design</a> <sub>(25)</sub></li><li><a href="http://arxiv.org/abs/2512.09209">Beyond Algorithm Evolution: An LLM-Driven Framework for the Co-Evolution of Swarm Intelligence Optimization Algorithms and Prompts</a> <sub>(24)</sub></li><li><a href="http://arxiv.org/abs/2601.17899">Evolving Interdependent Operators with Large Language Models for Multi-Objective Combinatorial Optimization</a> <sub>(24)</sub></li><li><a href="http://arxiv.org/abs/2512.03762">RoCo: Role-Based LLMs Collaboration for Automatic Heuristic Design</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2511.16485">Online Operator Design in Evolutionary Optimization for Flexible Job Shop Scheduling via Large Language Models</a> <sub>(22)</sub></li><li><a href="http://arxiv.org/abs/2602.02724">Automatic Design of Optimization Test Problems with Large Language Models</a> <sub>(21)</sub></li><li><a href="http://arxiv.org/abs/2601.19622">Algorithmic Prompt-Augmentation for Efficient LLM-Based Heuristic Design for A Search</a> <sub>(20)</sub></li><li><a href="http://arxiv.org/abs/2601.15738">LLM-Assisted Automatic Dispatching Rule Design for Dynamic Flexible Assembly Flow Shop Scheduling</a> <sub>(20)</sub></li></ol></details> | <details><summary>View analysis</summary>This research front explores advanced architectural innovations in LLM-guided algorithm design, moving beyond basic Evolution of Heuristics (EoH) and FunSearch paradigms. It focuses on sophisticated co-evolutionary strategies, multi-agent systems, and novel population representations to improve the discovery and performance of heuristics for complex combinatorial optimization problems. Key themes include evolving interdependent operators, dynamically adapting search strategies, and co-evolving problem instances or prompt templates alongside the algorithms themselves.  Key contributions include E2OC, which uses MCTS to co-evolve interdependent operators, achieving up to +22% Hypervolume on FJSP/TSP. LLM4EO demonstrates online operator design for Flexible Job Shop Scheduling, yielding 3-4% RPD_BM improvement. ASRO introduces a game-theoretic framework for co-evolving solvers and adversarial instance generators, outperforming EoH by 0.5-30% on OBP, TSP, and CVRP. A-CEoH enhances prompts with algorithmic context, enabling smaller LLMs to generate superior A* heuristics for UPMP and SPP. EvoLattice proposes a DAG-based population representation with alternative-level statistics, boosting performance on NAS-Bench-Zero by over 150%. Other notable work includes LLM-driven test function generation (EoTF), co-evolution of prompts and Fireworks Algorithm operators (achieving 100% on Aircraft Landing vs. 56% for ReEvo), the dual-expert LLM4DRD for dynamic scheduling, TIDE's nested evolution for decoupling structure and parameter tuning (reducing TSP gap by 7.35%), RoCo's multi-agent system with long-term reflection, and EoH-S, which evolves complementary heuristic *sets* to reduce optimality gaps by 40-60% compared to single-heuristic approaches.  This front is rapidly emerging and maturing, characterized by a shift towards more complex, integrated LLM-driven systems. The trajectory indicates a strong focus on improving the efficiency, robustness, and generalization capabilities of generated algorithms. Future work will likely integrate these diverse architectural advancements, such as combining nested evolutionary loops with graph-based population representations, and expanding to more challenging multi-objective, constrained, and real-world dynamic optimization problems.</details> |
| ‚úÖ Stable | Advanced LLM-Driven Algorithm Evolution via Behavioral and Structural Guidance | <details><summary>6</summary><ol><li><a href="http://arxiv.org/abs/2601.21511">LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI</a> <sub>(26)</sub></li><li><a href="http://arxiv.org/abs/2403.02985">Evolution Transformer: In-Context Evolutionary Optimization</a> <sub>(24)</sub></li><li><a href="http://arxiv.org/abs/2602.04529">Landscape-aware Automated Algorithm Design: An Efficient Framework for Real-world Optimization</a> <sub>(24)</sub></li><li><a href="http://arxiv.org/abs/2507.03605">Behaviour Space Analysis of LLM-driven Meta-heuristic Discovery</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2505.18602">LLM-Meta-SR: In-Context Learning for Evolving Selection Operators in Symbolic Regression</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2506.02049">EvoGit: Decentralized Code Evolution via Git-Based Multi-Agent Collaboration</a> <sub>(22)</sub></li></ol></details> | <details><summary>View analysis</summary>This front explores advanced techniques for LLM-driven algorithm discovery, moving beyond basic prompt engineering to incorporate sophisticated feedback mechanisms and architectural innovations. Key themes include leveraging semantics-aware selection (LLM-Meta-SR), analyzing behavioral spaces (LLaMEA), enabling decentralized code evolution (EvoGit), decoupling discovery from expensive evaluations using landscape-aware proxies (LLaMEA), integrating structural feedback from Explainable AI (LLaMEA-SAGE), and developing self-referential learning architectures (Evolution Transformer). These approaches aim to make the LLM-driven evolution process more efficient, robust, and interpretable across domains like symbolic regression and expensive continuous optimization.  Specific contributions include Zhang et al.'s LLM-Meta-SR, which achieved +2.3% R2 on SRBench for symbolic regression by using semantics-aware crossover. Huang et al.'s EvoGit introduced a novel Git-based multi-agent framework for decentralized code evolution. Yin et al. demonstrated that LLaMEA, guided by GP-evolved symbolic proxies, can discover algorithms for photonics problems that outperform baselines like LSHADE with 50x fewer real evaluations. Lange et al.'s Evolution Transformer, employing Self-Referential Algorithm Distillation (SR-EAD), learned to perform evolutionary strategy updates and generalized to unseen Brax control tasks. Furthermore, two LLaMEA papers advanced the understanding and guidance of LLM evolution: one by analyzing behavioral spaces on BBOB (5D) to show the importance of 'simplify' mutations, and another (LLaMEA-SAGE) by using SHAP analysis of AST features to guide mutations, leading to faster convergence on MA-BBOB.  This front is rapidly maturing, transitioning from demonstrating the feasibility of LLM-driven algorithm design to developing principled methods for its efficiency, robustness, and interpretability. The emphasis is shifting towards understanding why certain LLM-generated algorithms perform well and how to systematically guide their evolution. The next wave of research will likely focus on integrating multiple forms of feedback (semantic, behavioral, structural, landscape) within unified frameworks, scaling these methods to tackle higher-dimensional and more complex real-world problems, and developing more robust, open-ended self-improvement loops that can autonomously discover and refine algorithms over extended periods.</details> |

<details>
<summary>üìã Full list (79 papers, sorted by date)</summary>

<table><colgroup><col width="5%"><col width="7%"><col width="34%"><col width="13%"><col width="16%"><col width="5%"><col width="20%"></colgroup>
<thead><tr><th>Score</th><th>Date</th><th>Title</th><th>Authors</th><th>Affiliation</th><th>Venue</th><th>Links</th></tr></thead>
<tbody>
<tr><td><small>24/30</small></td><td><small>2026-02-17</small></td><td><details><summary><strong>MadEvolve: Evolutionary Optimization of Cosmological Algorithms with Large Language Models</strong></summary>MadEvolve extends AlphaEvolve by embedding a gradient-based optimization loop (via JAX) inside the fitness evaluation, allowing the LLM to focus purely on code structure while an optimizer (Adam) handles continuous parameters. They demonstrate 20-30% performance gains on complex cosmological reconstruction tasks, validated on held-out simulations. The critical takeaway is the architectural pattern: prompt the LLM to write differentiable code rather than tuning constants, and use a UCB1 bandit to dynamically select between cheap and expensive models. We should immediately adopt the differentiable inner-loop strategy for our continuous heuristic search projects.</details></td><td><small>Tianyi Li et.al.</small></td><td><small>University of Wisconsin-Madison</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.15951">pdf</a> / <a href="https://github.com/tianyi-stack/MadEvolve-Cosmo">code</a></small></td></tr>
<tr><td><small>28/30</small></td><td><small>2026-02-17</small></td><td><details><summary><strong>Heuristic Search as Language-Guided Program Optimization</strong></summary>LaGO decomposes automated heuristic design into three explicit modules: evaluation, a code-writing 'Analyst' (backward pass), and a diversity-aware 'Generator' (update), while co-evolving constructive and refinement heuristics. The authors demonstrate significant gains (+0.17 QYI) on PDPTW and Crew Pairing against ReEvo and EoH, showing that joint optimization of initialization and improvement prevents local optima. The critical takeaway is the 'Analyst' module: instead of asking the LLM for text critiques, they ask it to write Python feature extraction functions to statistically characterize solution quality‚Äîa technique we should immediately adopt to upgrade our fitness signals in AlgoEvo.</details></td><td><small>Mingxin Yu et.al.</small></td><td><small>Massachusetts Institute of Technology</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.16038">pdf</a> / code</small></td></tr>
<tr><td><small>21/30</small></td><td><small>2026-02-10</small></td><td><details><summary><strong>ImprovEvolve: Ask AlphaEvolve to Improve the Input Solution and Then Improvise</strong></summary>Kravatskiy et al. introduce ImprovEvolve, a framework that restricts the LLM to evolving `improve()` (local search) and `perturb()` (mutation) operators, which are then executed by a fixed basin-hopping algorithm. They achieve new state-of-the-art results on Hexagon Packing and the Second Autocorrelation Inequality, demonstrating that this modular approach generalizes to unseen problem sizes where monolithic AlphaEvolve solutions fail. The critical insight is that LLMs are poor at designing global search logic and tuning hyperparameters (LLM edits actively harmed performance), so we should isolate the LLM to generating local moves while keeping the meta-heuristic framework deterministic. We should immediately apply this 'operator-only' evolution strategy to our ALNS research for VRP.</details></td><td><small>Alexey Kravatskiy et.al.</small></td><td><small>MIRIAI, FusionBrain Lab, Institute of Numerical Mathematics</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.10233">pdf</a> / code</small></td></tr>
<tr><td><small>25/30</small></td><td><small>2026-02-09</small></td><td><details><summary><strong>G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design</strong></summary>G-LNS extends LLM-based evolutionary search to ALNS by co-evolving Python code for Destroy and Repair operators rather than constructive priority rules. The authors introduce a 'Synergy Matrix' that tracks the performance of specific operator pairs during evaluation, using this data to guide a 'Synergistic Joint Crossover' where the LLM optimizes the coupling between destroy and repair logic. Results are strong: it significantly outperforms FunSearch and EoH on TSP/CVRP and beats OR-Tools on large-scale instances (N=200) under time constraints. The key takeaway for AlgoEvo is the synergy-aware co-evolution mechanism‚Äîexplicitly tracking and prompting for component interaction is a concrete technique we can apply to multi-agent optimization systems.</details></td><td><small>Baoyun Zhao et.al.</small></td><td><small>Tsinghua University, University of Chinese Academy of Sciences, Northeastern University</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.08253">pdf</a> / <a href="https://github.com/zboyn/G-LNS">code</a></small></td></tr>
<tr><td><small>27/30</small></td><td><small>2026-02-09</small></td><td><details><summary><strong>Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery</strong></summary>ASRO adapts Policy Space Response Oracles (PSRO) to code generation, treating heuristic discovery as a zero-sum game where a 'Solver' evolves to minimize gaps and a 'Generator' evolves to create adversarial instances. The results are compelling: it consistently beats the static EoH baseline on TSPLIB and CVRPLIB, proving that adversarial training yields better generalization than training on fixed distributions. The critical takeaway is the architecture: explicitly co-evolving an 'Instance Generator' program alongside the solver prevents overfitting and exposes edge cases (like specific geometric traps in TSP) that static benchmarks miss. This is a direct upgrade to our AlgoEvo/AlphaEvolve pipelines, though it incurs higher computational costs due to the evaluation matrix required for the meta-game.</details></td><td><small>Xinyi Ke et.al.</small></td><td><small>Tsinghua University, Chinese Academy of Sciences, University of Chinese Academy of Sciences, AiRiA</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2601.22896">pdf</a> / code</small></td></tr>
<tr><td><small>21/30</small></td><td><small>2026-02-05</small></td><td><details><summary><strong>Mining Generalizable Activation Functions</strong></summary>Vitvitskyi et al. (DeepMind) utilize AlphaEvolve to discover novel activation functions by evolving Python code on small, synthetic datasets explicitly designed to test OOD generalization (e.g., polynomials, Feynman equations). The results are credible and backed by downstream transfer: discovered functions like `GELU * (1 + 0.5 sinc(x))` outperform baselines on algorithmic reasoning tasks (CLRS-30) while matching standard vision benchmarks. **Key Takeaway:** The 'Small-Scale Lab' methodology‚Äîoptimizing on cheap, synthetic proxy tasks to find generalizable logic‚Äîis a validated strategy to bypass the computational bottleneck of evaluating evolved candidates on large-scale instances. We should steal this 'proxy evolution' setup for AlgoEvo to drastically reduce evaluation costs while targeting generalization in VRP heuristics.</details></td><td><small>Alex Vitvitskyi et.al.</small></td><td><small>Google DeepMind</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.05688">pdf</a> / <a href="https://github.com/Aastha2104/Parkinson-Disease-Prediction">code</a></small></td></tr>
<tr><td><small>24/30</small></td><td><small>2026-02-04</small></td><td><details><summary><strong>Landscape-aware Automated Algorithm Design: An Efficient Framework for Real-world Optimization</strong></summary>Yin et al. introduce a framework that decouples algorithm discovery from expensive evaluations by using Genetic Programming to evolve symbolic proxy functions that statistically match the target problem's landscape (via ELA features). Empirical results on photonics problems confirm that algorithms evolved on these cheap proxies transfer successfully to the real tasks, outperforming standard baselines like LSHADE with only 50√óD real evaluations. **Key Takeaway:** We can synthesize 'symbolic gyms' that statistically mimic our target problems to run thousands of LLM iterations at near-zero cost. This directly addresses the sample efficiency bottleneck in AlgoEvo and suggests we should move beyond standard neural surrogates to evolved symbolic proxies.</details></td><td><small>Haoran Yin et.al.</small></td><td><small></small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.04529">pdf</a> / <a href="10.5281/zenodo.18385405">code</a></small></td></tr>
<tr><td><small>19/30</small></td><td><small>2026-02-04</small></td><td><details><summary><strong>ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas</strong></summary>ProxyWar introduces a tournament-based evaluation framework for LLM-generated code, using TrueSkill ratings from game simulations (Sudoku, Poker, etc.) instead of static unit tests. The results are robust (10k+ matches) and reveal a low correlation between Pass@1 and actual win rates; notably, 'reasoning' models like DeepSeek-R1 crush 'coding' models like Qwen-Coder in strategic tasks despite lower static scores. For our evolutionary search work, this confirms that we must move beyond static benchmarks to dynamic, competitive evaluation signals to avoid optimizing for syntax over strategy. We should also prioritize reasoning models over code-specialized ones for our agentic logic generation.</details></td><td><small>Wenjun Peng et.al.</small></td><td><small></small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.04296">pdf</a> / <a href="https://github.com/xinke-wang/ProxyWar">code</a></small></td></tr>
<tr><td><small>26/30</small></td><td><small>2026-02-03</small></td><td><details><summary><strong>Contrastive Concept-Tree Search for LLM-Assisted Algorithm Discovery</strong></summary>The authors introduce Contrastive Concept-Tree Search (CCTS), which modifies the standard evolutionary loop by prompting the LLM to extract semantic 'concepts' from every generated program, building a dynamic hierarchy. They then apply a Tree-structured Parzen Estimator (TPE) to these concepts to learn a contrastive utility model (p(concept|good)/p(concept|bad)), using this to bias parent selection towards promising algorithmic strategies. Results are rigorous, showing consistent improvements over k-elite baselines on combinatorial tasks like Circle Packing, with a synthetic ablation confirming the model learns ground-truth concept utilities. **Key Takeaway:** We should immediately implement the 'Concept TPE' loop in AlgoEvo‚Äîasking the LLM to tag generated heuristics with concepts and maintaining a weight vector over these concepts provides a cheap, interpretable 'process reward model' to guide search.</details></td><td><small>Timothee Leleu et.al.</small></td><td><small></small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.03132">pdf</a> / code</small></td></tr>
<tr><td><small>19/30</small></td><td><small>2026-02-03</small></td><td><details><summary><strong>Persona Generators: Generating Diverse Synthetic Personas at Scale</strong></summary>Paglieri et al. (DeepMind) apply AlphaEvolve to optimize Python code that generates synthetic personas, explicitly maximizing diversity metrics (convex hull, coverage) in embedding space rather than just fidelity. They achieve >80% coverage of the behavioral space compared to <50% for baselines, proving that evolving the *generator function* is more effective than prompting for diversity. The key takeaway is their two-stage architecture (autoregressive high-level trait generation $\to$ parallel detail expansion), which we should steal to evolve 'Solution Generators' for VRP/OR that inherently resist mode collapse. This validates our direction with AlgoEvo but offers a concrete architectural pattern for maintaining population diversity.</details></td><td><small>Davide Paglieri et.al.</small></td><td><small>Google DeepMind</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.03545">pdf</a> / <a href="https://github.com/akhatua2/synthpersona">code</a></small></td></tr>
<tr><td><small>28/30</small></td><td><small>2026-02-02</small></td><td><details><summary><strong>DeltaEvolve: Accelerating Scientific Discovery through Momentum-Driven Evolution</strong></summary>DeltaEvolve replaces the standard full-code history in evolutionary search with 'semantic deltas'‚Äîstructured text summaries capturing the 'from/to' logic of modifications and their hypotheses. Across 5 domains (including BBOB and Symbolic Regression), they demonstrate superior objective scores over AlphaEvolve while reducing token consumption by ~37%. The critical takeaway is the 'Progressive Disclosure' mechanism: treating history as a momentum vector (deltas) rather than a state archive (snapshots) allows us to fit a deeper evolutionary trajectory into the context window. We should immediately test their 'Delta Plan' prompt structure in AlgoEvo to improve sample efficiency and reduce costs.</details></td><td><small>Jiachen Jiang et.al.</small></td><td><small>Microsoft, The Ohio State University</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.02919">pdf</a> / code</small></td></tr>
<tr><td><small>21/30</small></td><td><small>2026-02-02</small></td><td><details><summary><strong>Automatic Design of Optimization Test Problems with Large Language Models</strong></summary>Achtelik et al. adapt LLM-driven evolutionary search (EoH) to generate interpretable Python functions that match specific landscape features (ELA), effectively creating synthetic benchmarks on demand. Unlike prior neural network approaches that fail to scale, this method performs robustly in higher dimensions (3D-5D) and produces portable code. The key takeaway is the capability to procedurally generate 'hard' or specific-property instances; we should immediately adopt this to create a dynamic training curriculum for AlgoEvo, ensuring our evolved metaheuristics generalize beyond standard libraries like BBOB.</details></td><td><small>Wojciech Achtelik et.al.</small></td><td><small>AGH University of Krakow, Warsaw University of Technology</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.02724">pdf</a> / <a href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">code</a></small></td></tr>
<tr><td><small>24/30</small></td><td><small>2026-02-01</small></td><td><details><summary><strong>Evolving Interdependent Operators with Large Language Models for Multi-Objective Combinatorial Optimization</strong></summary>E2OC introduces a hierarchical search framework where MCTS optimizes 'design thoughts' (textual strategies) rather than raw code, subsequently using these strategies to guide a coordinate-descent-style evolution of interdependent operators. While the computational cost is high due to the inner-loop operator rotation, the results on FJSP/TSP (+20% HV vs expert) and comparisons against FunSearch/EoH demonstrate that explicitly modeling operator coupling is superior to isolated evolution. The critical takeaway for us is the **'strategy-first' search layer**: evolving a semantic blueprint for component interaction *before* code generation prevents the local optima trap of independent component optimization, a technique we should immediately test in AlgoEvo.</details></td><td><small>Junhao Qiu et.al.</small></td><td><small></small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2601.17899">pdf</a> / <a href="null">code</a></small></td></tr>
<tr><td><small>24/30</small></td><td><small>2026-01-29</small></td><td><details><summary><strong>READY: Reward Discovery for Meta-Black-Box Optimization</strong></summary>READY introduces a multi-task evolutionary framework where LLMs evolve reward functions for multiple MetaBBO algorithms simultaneously, utilizing explicit 'Knowledge Transfer' operators to translate successful logic between distinct tasks. The results are robust, demonstrating superior performance over Eureka and EoH on BBOB benchmarks with a 2-4x reduction in search time due to parallelization and shared heuristics. The most stealable insights are the 'History-Reflection' operator‚Äîwhich prompts the LLM to extrapolate trends from the evolutionary trajectory rather than just mutating the current state‚Äîand the cross-niche transfer mechanism, both of which should be implemented in our multi-agent optimization stack immediately.</details></td><td><small>Zechuan Huang et.al.</small></td><td><small></small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2601.21847">pdf</a> / <a href="https://anonymous.4open.science/r/ICML_READY-747F">code</a></small></td></tr>
<tr><td><small>26/30</small></td><td><small>2026-01-29</small></td><td><details><summary><strong>LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI</strong></summary>LLaMEA-SAGE augments LLM-based evolutionary search by extracting AST features (complexity, graph metrics) from generated code, training a surrogate model to predict fitness from these features, and using SHAP analysis to generate natural language prompts that guide the LLM to modify specific structural properties (e.g., 'increase cyclomatic complexity'). On the MA-BBOB benchmark, it outperforms state-of-the-art methods (MCTS-AHD, LHNS) and converges faster than vanilla LLaMEA, although the authors honestly report that statistical significance was limited (p=0.44) due to small sample sizes (5 runs). The critical takeaway for us is the pipeline of using static code analysis as a feedback signal‚Äîwe can immediately steal this 'SAGE' loop to guide AlgoEvo or EvoCut by telling the LLM *how* to structurally mutate code based on surrogate correlations, rather than just hoping for random improvements.</details></td><td><small>Niki van Stein et.al.</small></td><td><small></small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2601.21511">pdf</a> / <a href="https://anonymous.4open.science/r/LLaMEA-SAGE/README.md">code</a></small></td></tr>
<tr><td><small>27/30</small></td><td><small>2026-01-29</small></td><td><details><summary><strong>TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design</strong></summary>TIDE introduces a nested evolutionary framework that strictly decouples algorithmic structure generation (via LLM) from numerical parameter tuning (via Differential Evolution), managed by a Tree Similarity Edit Distance (TSED) guided island model. Results on 9 COPs (TSP, BPP, etc.) show it consistently outperforms ReEvo and EoH, primarily because the DE layer optimizes constants at zero token cost, preventing the discard of structurally sound but poorly tuned heuristics. The critical takeaway is the necessity of a gradient-free tuning layer for LLM-generated code; relying on LLMs for numerical constants is inefficient and imprecise. We should immediately implement a similar parameter-tuning inner loop in our AlgoEvo framework.</details></td><td><small>Chentong Chen et.al.</small></td><td><small></small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2601.21239">pdf</a> / code</small></td></tr>
<tr><td><small>24/30</small></td><td><small>2026-01-29</small></td><td><details><summary><strong>PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs</strong></summary>PathWise reformulates heuristic discovery as a sequential planning problem over an 'Entailment Graph,' where a Policy Agent generates high-level evolutionary directives (rationales) and a World Model executes the code, guided by specific Critic reflections. The results are robust: it outperforms ReEvo, FunSearch, and MCTS-AHD on TSP, CVRP, and Bin Packing while using half the evaluation budget (500 vs 1000), demonstrating genuine sample efficiency. The key takeaway is the **Entailment Graph** structure: explicitly storing the *derivation rationale* and lineage allows the LLM to reason about the search trajectory and avoid redundant failures, a mechanism we should immediately adapt for AlgoEvo to fix our memory bottleneck.</details></td><td><small>Oguzhan Gungordu et.al.</small></td><td><small></small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2601.20539">pdf</a> / code</small></td></tr>
<tr><td><small>24/30</small></td><td><small>2026-01-28</small></td><td><details><summary><strong>Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve</strong></summary>Magellan couples AlphaEvolve with a black-box autotuner (Vizier) to evolve C++ compiler heuristics, achieving >5% binary size reduction in LLVM and beating both human experts and prior neural policies. The results are rigorous, validated on production workloads and showing temporal generalization. **The critical takeaway is the 'Hierarchical Search' strategy:** rather than asking the LLM to write fully specified code, they prompt it to generate *templates* with exposed parameters (flags), delegating numerical tuning to a cheap external optimizer. This directly addresses the sample efficiency issues we face in AlgoEvo; we should immediately steal this architecture to separate structural evolution from parameter tuning.</details></td><td><small>Hongzheng Chen et.al.</small></td><td><small>Google DeepMind, Google, Cornell University</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2601.21096">pdf</a> / code</small></td></tr>
<tr><td><small>20/30</small></td><td><small>2026-01-27</small></td><td><details><summary><strong>Algorithmic Prompt-Augmentation for Efficient LLM-Based Heuristic Design for A Search</strong></summary>This paper introduces 'Algorithmic-Contextual EoH' (A-CEoH), which injects the actual source code of the search algorithm (e.g., the A* driver loop, neighbor generation) into the LLM prompt alongside the problem description. Experiments on the Unit-Load Pre-Marshalling Problem and Sliding Puzzle Problem demonstrate that this algorithmic context allows a 32B parameter model (Qwen2.5-Coder) to generate heuristics superior to those from GPT-4o and human experts. The results are credible and backed by comparisons against optimal baselines. The key takeaway is a transferable 'prompt trick': explicitly showing the LLM the code that *calls* its generated function aligns the heuristic significantly better with the search dynamics than natural language descriptions alone. We should immediately test injecting our ALNS/search driver code into our evolutionary prompt templates.</details></td><td><small>Thomas B√∂mer et.al.</small></td><td><small></small></td><td><small>EvoStar conference; Code: https://github</small></td><td><small><a href="http://arxiv.org/abs/2601.19622">pdf</a> / <a href="https://github.com/tb-git-tud/a-ceoh-evolution-of-heuristics">code</a></small></td></tr>
<tr><td><small>21/30</small></td><td><small>2026-01-27</small></td><td><details><summary><strong>CASTER: Breaking the Cost-Performance Barrier in Multi-Agent Orchestration via Context-Aware Strategy for Task Efficient Routing</strong></summary>CASTER implements a context-aware neural router for multi-agent systems that dynamically selects between weak and strong models, reducing inference costs by ~72% compared to a GPT-4o-only baseline. The authors validate this on a custom benchmark across four domains, showing it outperforms cascading strategies (FrugalGPT) by avoiding the 'double-billing' of failed weak calls. The standout takeaway for us is the 'On-Policy Negative Feedback' mechanism: training the router by explicitly relabeling instances where the weak model failed as 'Strong-Required'. We should adapt this active learning logic to train our proxy reward models in AlgoEvo, allowing us to reliably offload expensive evaluations to cheaper proxies without manual annotation.</details></td><td><small>Shanyv Liu et.al.</small></td><td><small>University of Houston, China University of Petroleum (East China), Southwest Jiaotong University</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2601.19793">pdf</a> / code</small></td></tr>
<tr><td><small>20/30</small></td><td><small>2026-01-23</small></td><td><details><summary><strong>The Art of Being Difficult: Combining Human and AI Strengths to Find Adversarial Instances for Heuristics</strong></summary>This paper applies FunSearch to generate adversarial instances for classical OR heuristics (Knapsack, Bin Packing, k-median), successfully breaking long-standing theoretical lower bounds. The results are rigorous: they disprove the output-polynomial time of the Nemhauser-Ullmann algorithm and improve the Best-Fit bin packing bound to 1.5. The key takeaway for our AlgoEvo work is the workflow: the LLM finds 'messy' structural patterns (e.g., repeated floats) which humans then manually generalize into asymptotic proofs. This validates Program Search over vector search but exposes the 'generalization gap'‚Äîwe should implement a post-processing agent to automate this manual refinement step.</details></td><td><small>Henri Nikoleit et.al.</small></td><td><small>Google DeepMind, University of Bonn, University of Manitoba</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2601.16849">pdf</a> / <a href="https://github.com/lumi-a/funsearch">code</a></small></td></tr>
<tr><td><small>24/30</small></td><td><small>2026-01-23</small></td><td><details><summary><strong>Scaling the Scaling Logic: Agentic Meta-Synthesis of Logic Reasoning</strong></summary>Liu et al. introduce SS-Logic, an agentic framework that evolves Python 'Generator-Validator' pairs to scale logic task families, using a rigorous 'Code-Augmented Blind Review' where independent agents must write code to solve generated tasks to verify their validity. They expand 400 seed families to over 21k instances, achieving consistent gains on AIME (+3.0) and SynLogic (+5.2) via RLVR. **Crucial Takeaway:** We should steal the 'Blind Review' mechanism for AlgoEvo‚Äîusing the solvability of a generated problem (by an independent code agent) as a strict fitness filter for the generator itself. This directly addresses our bottleneck in filtering invalid or hallucinated heuristics during evolutionary search.</details></td><td><small>Bowen Liu et.al.</small></td><td><small>Tencent, The Hong Kong University of Science and Technology (Guangzhou)</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.13218">pdf</a> / <a href="https://github.com/AdAstraAbyssoque/Scaling-the-Scaling-Logic">code</a></small></td></tr>
<tr><td><small>20/30</small></td><td><small>2026-01-22</small></td><td><details><summary><strong>LLM-Assisted Automatic Dispatching Rule Design for Dynamic Flexible Assembly Flow Shop Scheduling</strong></summary>LLM4DRD employs a dual-agent framework (Generator & Evaluator) to evolve priority dispatching rules for dynamic flexible assembly flow shops. The core contribution is the **Hybrid Evaluation** mechanism, where the Evaluator generates qualitative critiques (strengths/weaknesses) that are injected into the Generator's prompts to guide specific operators like 'Dominance-Fusion Crossover' and 'Directed Optimization.' Empirical results show it outperforms FunSearch and EOH, avoiding the premature convergence seen in other methods. The most stealable insight is the prompt structure for crossover: rather than blindly combining code, it uses the Evaluator's analysis of parent strengths to direct the merger, a technique we should implement to improve sample efficiency in our evolutionary search.</details></td><td><small>Junhao Qiu et.al.</small></td><td><small></small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2601.15738">pdf</a> / code</small></td></tr>
<tr><td><small>29/30</small></td><td><small>2026-01-22</small></td><td><details><summary><strong>Learning to Discover at Test Time</strong></summary>TTT-Discover introduces a method to fine-tune an LLM (gpt-oss-120b) *during* inference on a single test problem using RL, replacing the frozen-model evolutionary search of AlphaEvolve. They employ a novel 'entropic objective' that optimizes for the single best solution (discovery) rather than expected return, combined with PUCT-based state reuse. The results are empirically rigorous, setting new SOTA on Erd≈ës‚Äô problem, GPU kernel optimization, and AtCoder contests, directly beating AlphaEvolve and ShinkaEvolve. The critical takeaway is that for hard discovery tasks, shifting the model's distribution via online updates is superior to context-based search; we should immediately test their entropic objective in our AlgoEvo pipeline.</details></td><td><small>Mert Yuksekgonul et.al.</small></td><td><small>Stanford University, NVIDIA, UC San Diego, Together AI, Astera Institute</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2601.16175">pdf</a> / <a href="https://github.com/test-time-training/discover">code</a></small></td></tr>
<tr><td><small>22/30</small></td><td><small>2026-01-22</small></td><td><details><summary><strong>Online Operator Design in Evolutionary Optimization for Flexible Job Shop Scheduling via Large Language Models</strong></summary>LLM4EO embeds an LLM directly into the Genetic Algorithm loop to dynamically generate and replace gene-selection operators whenever the population stagnates, rather than training them offline. Results on FJSP benchmarks (Brandimarte, Fattahi) show a 3-4% improvement over static GA and GP, with convergence plots demonstrating that LLM interventions successfully break local optima. The most stealable insight is the 'Perception and Analysis' prompt structure: it forces the LLM to explicitly diagnose *why* the current population is stuck (based on fitness stats) before generating new code, a mechanism we should port to AlgoEvo to handle search stagnation. This validates the viability of online, state-aware LLM intervention in OR scheduling problems.</details></td><td><small>Rongjie Liao et.al.</small></td><td><small>City University of Hong Kong, Guangdong University of Technology</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2511.16485">pdf</a> / code</small></td></tr>
<tr><td><small>21/30</small></td><td><small>2026-01-15</small></td><td><details><summary><strong>Global Optimization for Combinatorial Geometry Problems Revisited in the Era of LLMs</strong></summary>Berthold et al. demonstrate that standard global NLP solvers (SCIP, Xpress) outperform DeepMind's AlphaEvolve on its own benchmarks (circle/hexagon packing, min-max distance) without any learning or evolution. The results are rigorous, improving on 'newly discovered' solutions within minutes using default solver settings. **CRITICAL TAKEAWAY:** We must validate our AlgoEvo results against classical global solvers to ensure we aren't claiming 'discovery' on problems that are trivial for SCIP; furthermore, it suggests a hybrid path where LLMs generate NLP models for solvers rather than evolving raw heuristic code. This is a necessary reality check for our benchmarking strategy.</details></td><td><small>Timo Berthold et.al.</small></td><td><small></small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2601.05943">pdf</a> / code</small></td></tr>
<tr><td><small>23/30</small></td><td><small>2026-01-09</small></td><td><details><summary><strong>Weights to Code: Extracting Interpretable Algorithms from the Discrete Transformer</strong></summary>Zhang et al. introduce the 'Discrete Transformer,' a constrained architecture that learns algorithmic tasks via gradient descent and allows for the post-hoc extraction of exact, human-readable Python code. By enforcing functional disentanglement (using attention strictly for routing and MLPs for arithmetic) and employing temperature-annealed sampling, they recover symbolic laws for arithmetic and physics tasks with near-zero error. The critical takeaway is their 'continuous-to-discrete homotopy' strategy‚Äîannealing from soft to hard selection during training‚Äîwhich enables differentiable search to converge on discrete, symbolic solutions. This suggests a viable path to discover heuristics via continuous optimization rather than purely stochastic LLM evolution.</details></td><td><small>Yifan Zhang et.al.</small></td><td><small></small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2601.05770">pdf</a> / code</small></td></tr>
<tr><td><small>24/30</small></td><td><small>2026-01-06</small></td><td><details><summary><strong>CodeEvolve: an open source evolutionary coding agent for algorithm discovery and optimization</strong></summary>CodeEvolve couples islands-based genetic algorithms with LLMs, utilizing CVT-MAP-Elites for diversity and a specific 'inspiration-based' crossover operator where the LLM integrates logic from high-ranking peer solutions. The results are strong and backed by numbers: they beat AlphaEvolve on 5/9 benchmarks and demonstrate that Qwen3-Coder-30B matches Gemini-2.5 performance at ~10% of the cost. The single most useful takeaway is the implementation of the 'inspiration' operator and the necessity of MAP-Elites over simple elitism to escape local optima in code space. We should immediately benchmark their open-source framework against our internal AlgoEvo builds.</details></td><td><small>Henrique Assump√ß√£o et.al.</small></td><td><small>Inter&Co., Worcester Polytechnic Institute, Universidade Federal de Minas Gerais</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2510.14150">pdf</a> / <a href="https://github.com/inter-co/science-codeevolve">code</a></small></td></tr>
<tr><td><small>28/30</small></td><td><small>2025-12-30</small></td><td><details><summary><strong>LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm</strong></summary>LoongFlow replaces the standard stochastic mutation operator in LLM evolutionary search with a 'Plan-Execute-Summarize' (PES) cognitive loop. Instead of random code changes, a Planner retrieves the 'intent' and 'summary' of the parent solution's lineage to generate a directed hypothesis, which is then executed and summarized for the next generation. The authors demonstrate a 60% reduction in evaluations and a 100% success rate on AlphaEvolve tasks where standard methods fail or stagnate. The critical takeaway is the 'Lineage-Based Context Retrieval' mechanism: explicitly passing the parent's plan and retrospective summary to the child allows for directed rather than random walks in the search space. We must implement this PES loop in AlgoEvo immediately to fix our sample efficiency issues.</details></td><td><small>Chunhui Wan et.al.</small></td><td><small></small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2512.24077">pdf</a> / <a href="https://github.com/baidu-baige/LoongFlow">code</a></small></td></tr>
<tr><td><small>25/30</small></td><td><small>2025-12-22</small></td><td><details><summary><strong>Mathematical exploration and discovery at scale</strong></summary>DeepMind applies AlphaEvolve to 67 math problems, formalizing the distinction between 'Search Mode' (evolving heuristics for fixed instances) and 'Generalizer Mode' (evolving algorithms that extrapolate from small to large n). Results are rigorous, establishing new bounds on Kakeya sets and 10+ other problems by exploiting verifier loopholes and heuristic specialization. The most critical takeaway for AlgoEvo is Section 44: evolving code that *calls* other LLMs leads to emergent prompt optimization and injection strategies, suggesting a path for our multi-agent optimization work. We must adopt their 'Generalizer' training curriculum (train on small n, test on large n) to fix our scalability bottlenecks.</details></td><td><small>Bogdan Georgiev et.al.</small></td><td><small>Google DeepMind, UCLA, Brown University, Institute for Advanced Study</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2511.02864">pdf</a> / <a href="https://colab.research.google.com/github/google-deepmind/alphaevolve_results/blob/master/mathematical_results.ipynb">code</a></small></td></tr>
<tr><td><small>26/30</small></td><td><small>2025-12-22</small></td><td><details><summary><strong>Let the Barbarians In: How AI Can Accelerate Systems Performance Research</strong></summary>Cheng et al. (UC Berkeley) perform a rigorous empirical evaluation of LLM evolutionary search (ADRS) across 10 systems problems, achieving SOTA results on MoE load balancing (13x speedup via rediscovering Hamilton's Apportionment) and cloud scheduling. The results are real and backed by code, comparing frameworks like OpenEvolve, GEPA, and ShinkaEvolve. **Key Takeaway:** Their 'Best Practices' section offers concrete engineering constraints we should adopt: specifically, that 'moderate' feedback (worst-k cases) outperforms 'detailed' feedback (prevents overfitting), and that restricting mutations to diff-based edits is essential to prevent reward hacking. This paper validates our core research thesis while providing the benchmarks we now need to beat.</details></td><td><small>Audrey Cheng et.al.</small></td><td><small>UC Berkeley</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2512.14806">pdf</a> / <a href="https://github.com/codelion/openevolve">code</a></small></td></tr>
<tr><td><small>23/30</small></td><td><small>2025-12-19</small></td><td><details><summary><strong>Reinforced Generation of Combinatorial Structures: Hardness of Approximation</strong></summary>Nagda et al. utilize AlphaEvolve to discover combinatorial gadgets that improve hardness of approximation bounds for MAX-CUT and TSP, validating findings with formal proofs. The standout contribution is not the hardness results themselves, but the methodology: they tasked AlphaEvolve with optimizing the *verification code* (checking correctness against a slow ground truth), achieving a 10,000x speedup that enabled searching gadgets of size 19 (vs. 11 previously). We should immediately adopt this 'evolve the verifier' loop for our computationally expensive fitness functions in AlgoEvo to break current scalability limits.</details></td><td><small>Ansh Nagda et.al.</small></td><td><small>Google DeepMind, Google, University of California, Berkeley</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2509.18057">pdf</a> / code</small></td></tr>
<tr><td><small>26/30</small></td><td><small>2025-12-17</small></td><td><details><summary><strong>EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery</strong></summary>EvoLattice replaces the standard 'overwrite-based' evolution of monolithic programs with a persistent DAG where each node holds multiple alternative implementations, evaluating all valid combinatorial paths to compute fine-grained performance statistics for every micro-operator. The results are strong: it outperforms AlphaEvolve and FunSearch styles on NAS-Bench-Zero by explicitly preserving diversity and enabling surgical, data-driven pruning rather than blind mutation. The critical takeaway is the 'alternative-level statistic' mechanism: by aggregating performance across all paths a component participates in, they generate a high-fidelity signal that tells the LLM exactly which lines of code are working, effectively solving the sparse reward problem in code evolution. We should immediately discuss refactoring our AlgoEvo representation to support this multi-alternative graph structure, as it maximizes signal extraction per LLM call.</details></td><td><small>Kamer Ali Yuksel et.al.</small></td><td><small>aiXplain Inc</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2512.13857">pdf</a> / code</small></td></tr>
<tr><td><small>24/30</small></td><td><small>2025-12-10</small></td><td><details><summary><strong>Beyond Algorithm Evolution: An LLM-Driven Framework for the Co-Evolution of Swarm Intelligence Optimization Algorithms and Prompts</strong></summary>The authors introduce a co-evolutionary framework where both the optimization algorithm (Fireworks Algorithm operators) and the prompt templates used to generate them are evolved simultaneously by the LLM. The results demonstrate a massive performance jump on constrained Aircraft Landing problems (from ~56% with FunSearch to 100% with their method), suggesting that static prompts are a primary failure mode for complex OR constraints. The critical takeaway is their prompt fitness function: evaluating a prompt template based on the *performance improvement* (`child - parent`) of the code it generates, rather than absolute performance. We should immediately implement this 'prompt-delta' fitness signal in AlgoEvo to automate our prompt engineering loop.</details></td><td><small>Shipeng Cen et.al.</small></td><td><small>Peking University</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2512.09209">pdf</a> / code</small></td></tr>
<tr><td><small>23/30</small></td><td><small>2025-12-04</small></td><td><details><summary><strong>RoCo: Role-Based LLMs Collaboration for Automatic Heuristic Design</strong></summary>RoCo replaces standard evolutionary mutation operators with a 4-agent collaboration loop (Explorer, Exploiter, Critic, Integrator) that iteratively refines heuristics and accumulates long-term reflection memory across generations. While the empirical gains over ReEvo are marginal (often <1%) and likely expensive in token cost, the architecture successfully demonstrates how to embed structured multi-agent reasoning into the evolutionary loop to stabilize black-box search. The key takeaway is their Long-term Reflection mechanism, which aggregates critic feedback into a persistent memory buffer to guide future mutations‚Äîa technique we should immediately test to improve sample efficiency in AlgoEvo.</details></td><td><small>Jiawei Xu et.al.</small></td><td><small>South China University of Technology</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2512.03762">pdf</a> / code</small></td></tr>
<tr><td><small>19/30</small></td><td><small>2025-12-03</small></td><td><details><summary><strong>Structuring Collective Action with LLM-Guided Evolution: From Ill-Structured Problems to Executable Heuristics</strong></summary>ECHO-MIMIC presents a framework that first uses LLM-guided evolution to generate Python heuristics for agents (ECHO), and subsequently evolves natural language 'nudges' (MIMIC) to persuade simulated agents to adopt these global-optimal policies. While the experiments rely on synthetic data for agriculture and EV charging, the approach outperforms DSPy and AutoGen baselines in driving collective action. The most valuable takeaway is the architectural separation of 'policy discovery' (code evolution) and 'adoption mechanism' (message evolution)‚Äîa pattern we could adapt to evolve incentive structures or negotiation protocols in our multi-agent optimization systems (MASPRM/HERMES). The analysis of code complexity (Halstead metrics) versus fitness also provides a useful empirical reference for our observability work.</details></td><td><small>Kevin Bradley Dsouza et.al.</small></td><td><small>University of Waterloo, Royal Bank of Canada</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2509.20412">pdf</a> / code</small></td></tr>
<tr><td><small>28/30</small></td><td><small>2025-11-28</small></td><td><details><summary><strong>ThetaEvolve: Test-time Learning on Open Problems</strong></summary>ThetaEvolve integrates test-time reinforcement learning (GRPO) directly into an AlphaEvolve-style loop, allowing a single 8B model to learn from its own successful mutations and achieve new SOTA bounds on Circle Packing and Autocorrelation inequalities. The results are rigorous, showing that RL applied to the *dynamic* environment (sampling from the evolving database) vastly outperforms RL on static prompts or pure inference search. The most stealable insight is the 'lazy penalty' mechanism‚Äîpenalizing semantically equivalent code or stagnation‚Äîwhich forces the RL policy to learn genuine exploration strategies rather than memorization. This is a blueprint for the 'RL-infused evolution' milestone in our AlgoEvo roadmap.</details></td><td><small>Yiping Wang et.al.</small></td><td><small>Microsoft, University of Washington, Carnegie Mellon University, University of Wisconsin-Madison, University of California, San Diego</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2511.23473">pdf</a> / <a href="https://github.com/ypwang61/ThetaEvolve">code</a></small></td></tr>
<tr><td><small>23/30</small></td><td><small>2025-11-17</small></td><td><details><summary><strong>GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms</strong></summary>GigaEvo is an open-source reproduction of the AlphaEvolve framework that implements MAP-Elites with an asynchronous DAG execution engine, successfully reproducing SOTA results on Heilbronn triangles and beating FunSearch on Weibull Bin Packing. The results are credible and backed by code, specifically highlighting that 'rewrite-based' mutation outperforms 'diff-based' approaches for open-weights models‚Äîa crucial engineering constraint for us. The most actionable takeaway is their 'bidirectional lineage tracking' mechanism, which enriches mutation prompts by analyzing both how a program improved over its ancestor and how its descendants further improved, a technique we should steal for AlgoEvo's mutation operator. Their negative result regarding multi-island MAP-Elites (added complexity, no gain) suggests we should deprioritize similar complex topologies.</details></td><td><small>Valentin Khrulkov et.al.</small></td><td><small>Sber, Artificial Intelligence Research Institute (AIRI)</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2511.17592">pdf</a> / <a href="https://github.com/AIRI-Institute/gigaevo-core">code</a></small></td></tr>
<tr><td><small>25/30</small></td><td><small>2025-11-16</small></td><td><details><summary><strong>Automated Algorithmic Discovery for Scientific Computing through LLM-Guided Evolutionary Search: A Case Study in Gravitational-Wave Detection</strong></summary>Evo-MCTS introduces a hybrid search architecture where MCTS manages the exploration-exploitation balance of an evolutionary process, using LLMs for node expansion via novel operators like 'Path-wise Crossover' (synthesizing code from full root-to-leaf trajectories). The results are empirically strong, outperforming standard LLM-evolution baselines (ReEvo) by ~150% on a complex signal processing task. We learned that structuring the evolutionary lineage as a tree and using MCTS Q-values to select parents‚Äîrather than standard population selection‚Äîdrastically improves sample efficiency and solution quality. This is a blueprint for the 'RL-infused evolution' and 'persistent memory' features we have been planning for our own framework.</details></td><td><small>He Wang et.al.</small></td><td><small>Tsinghua University, University of Chinese Academy of Sciences</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2508.03661">pdf</a> / <a href="https://github.com/iphysresearch/evo-mcts">code</a></small></td></tr>
<tr><td><small>20/30</small></td><td><small>2025-11-11</small></td><td><details><summary><strong>AlphaResearch: Accelerating New Algorithm Discovery with Language Models</strong></summary>AlphaResearch introduces a 'dual environment' for algorithm discovery: it generates natural language research ideas, filters them using a reward model fine-tuned on ICLR peer reviews, and then executes the surviving ideas. While it claims to beat human baselines on Packing Circles, the improvement is marginal (<0.1%) and it fails to improve upon baselines in 6/8 benchmark problems. The key takeaway for us is the mechanism of an 'Idea Critic'‚Äîusing a learned reward model to filter the search space at the prompt level before wasting compute on execution‚Äîwhich directly addresses our sample efficiency goals in evolutionary search.</details></td><td><small>Zhaojian Yu et.al.</small></td><td><small>Yale, NYU, Tsinghua, ByteDance</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2511.08522">pdf</a> / <a href="https://github.com/answers111/alpha-research">code</a></small></td></tr>
<tr><td><small>24/30</small></td><td><small>2025-10-16</small></td><td><details><summary><strong>Programmatic Representation Learning with Language Models</strong></summary>The authors propose two algorithms, F2 (Features FunSearch) and D-ID3 (Dynamic ID3), to learn programmatic features for decision trees. D-ID3 is particularly novel: instead of evolving a global heuristic, it calls the LLM at *each split node* to generate a feature that discriminates the specific data subset at that leaf. Results are strong on Chess (matching Transformers trained on 250x more data) and Text, though the Image results (MNIST) are trivial. **Key Takeaway:** The D-ID3 architecture‚Äîusing the solver's current state (leaf node data) to prompt the LLM for *local* code generation‚Äîis a powerful pattern we should steal for our VRP solvers (e.g., evolving local repair operators for specific route bottlenecks) and EvoCut work.</details></td><td><small>Gabriel Poesia et.al.</small></td><td><small>Harvard University, Stanford University</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2510.14825">pdf</a> / <a href="https://github.com/gpoesia/leapr/">code</a></small></td></tr>
<tr><td><small>28/30</small></td><td><small>2025-10-13</small></td><td><details><summary><strong>Refining Hybrid Genetic Search for CVRP via Reinforcement Learning-Finetuned LLM</strong></summary>Zhu et al. fine-tune a Qwen-14B model using Reinforcement Learning (DAPO) to generate C++ crossover operators for the state-of-the-art HGS solver. Unlike typical prompting papers, they demonstrate that a small, specialized model can improve upon expert-designed components in a highly optimized solver, achieving superior results on CVRPLIB (up to 1000 nodes) where GPT-4o fails. The most stealable insight is their **AST-based anti-plagiarism reward**, which penalizes the model for generating code structurally identical to the prompt examples, effectively forcing exploration and preventing mode collapse‚Äîa technique we should immediately adopt for our evolutionary search agents. This confirms we should pivot from pure prompting to RL-finetuning for our code-generation agents.</details></td><td><small>Rongjie Zhu et.al.</small></td><td><small>Nanyang Technological University, Singapore, Singapore Management University, Singapore, Nanjing University of Information Science and Technology, China</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2510.11121">pdf</a> / code</small></td></tr>
<tr><td><small>23/30</small></td><td><small>2025-10-10</small></td><td><details><summary><strong>Barbarians at the Gate: How AI is Upending Systems Research</strong></summary>The authors apply OpenEvolve (an AlphaEvolve-style framework) to 11 computer systems problems, achieving significant gains over human baselines, such as a 5.0x speedup in MoE expert placement and 26% cost reduction in cloud scheduling. The results are empirically rigorous, relying on high-fidelity simulators rather than toy problems. For us, the key takeaway is the engineering recipe: using an ensemble of reasoning models (o3) for exploration and fast models (Gemini) for diversity, combined with a specific 'failure taxonomy' to debug search stagnation. This is immediate proof-of-concept for your 'GPUSched' and 'AlgoEvo' projects; we should adopt their ensemble strategy and simulator-first evaluation pipeline.</details></td><td><small>Audrey Cheng et.al.</small></td><td><small>UC Berkeley</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2510.06189">pdf</a> / code</small></td></tr>
<tr><td><small>26/30</small></td><td><small>2025-10-09</small></td><td><details><summary><strong>Robust Heuristic Algorithm Design with LLMs</strong></summary>Karimi et al. introduce 'Robusta', an enhancement to FunSearch that uses a Heuristic Analyzer (solver-based) to identify adversarial inputs and a Suggester LLM to explain *why* the current heuristic fails before generating new code. They demonstrate a 28x improvement in worst-case performance over FunSearch on traffic engineering tasks, with results backed by rigorous comparison against optimal solvers. The critical takeaway is the 'Suggester' intermediate step: converting raw failure instances into natural language coding strategies significantly improves the LLM's ability to fix logic bugs compared to raw samples alone. We should immediately attempt to replicate this 'Analyzer -> Explainer -> Coder' loop for our VRP work, using small-scale solvers to generate counter-examples for our evolved ALNS operators.</details></td><td><small>Pantea Karimi et.al.</small></td><td><small>Microsoft, MIT, Microsoft Research, University of Southern California, The University of Texas at Austin</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2510.08755">pdf</a> / code</small></td></tr>
<tr><td><small>24/30</small></td><td><small>2025-10-07</small></td><td><details><summary><strong>Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep Research</strong></summary>DeepEvolve augments the standard evolutionary coding loop (AlphaEvolve) with two critical components: a 'Deep Research' module that searches the web/literature to generate grounded mutation proposals, and an iterative debugging agent that fixes execution errors. While the '666%' improvement on Circle Packing is likely due to a weak baseline (fixed-size vs. generalized), the engineering results are compelling: the debugging agent raises execution success rates from ~13% to ~99% in complex tasks. The key takeaway for our AlgoEvo work is the architecture of generating a text-based 'research proposal' via RAG before attempting code generation, rather than mutating code directly. We should immediately adopt their debugging loop and consider injecting external literature search into our mutation operators to prevent search stagnation.</details></td><td><small>Gang Liu et.al.</small></td><td><small>MIT-IBM Watson AI Lab, IBM Research, University of Notre Dame</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2510.06056">pdf</a> / <a href="https://github.com/liugangcode/deepevolve">code</a></small></td></tr>
<tr><td><small>23/30</small></td><td><small>2025-09-30</small></td><td><details><summary><strong>Experience-Guided Reflective Co-Evolution of Prompts and Heuristics for Automatic Algorithm Design</strong></summary>EvoPH introduces a co-evolutionary framework where both the heuristic code and the LLM prompts are evolved, utilizing an island model for diversity and a 'strategy sampling' mechanism that dynamically selects mutation types (e.g., parameter tuning vs. rewrite) based on feedback. They report dominating performance over FunSearch and ReEvo on TSP and BPP (e.g., reducing Christofides gap from ~20% to ~5%), though the static performance of baselines suggests the gain comes largely from automating prompt engineering. The most stealable insight is the **Strategy Sampling** module: explicitly defining a pool of mutation operators and using an 'experience' buffer to select them is a practical implementation of the 'planner' concept we need for AlgoEvo. We should also adopt their island migration topology to improve diversity in our parallelized search.</details></td><td><small>Yihong Liu et.al.</small></td><td><small>Tencent, Renmin University of China, City University of Hong Kong</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2509.24509">pdf</a> / <a href="null">code</a></small></td></tr>
<tr><td><small>22/30</small></td><td><small>2025-09-29</small></td><td><details><summary><strong>MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems</strong></summary>MAS2 trains a tri-agent system (Generator, Implementer, Rectifier) using offline RL on decision trees to dynamically construct and repair multi-agent workflows. The results are strong, outperforming ADAS and MaAS on standard benchmarks while maintaining Pareto efficiency. The critical takeaway for us is the **Rectifier agent**: rather than discarding failed evolutionary candidates (as we currently do in AlgoEvo), we should implement a dedicated loop to patch runtime errors (e.g., API failures, dimension mismatches). Additionally, their 'Collaborative Tree Optimization' offers a rigorous method to fine-tune the 'Evolver' model using trajectory data, which could replace our current prompt-based meta-heuristics.</details></td><td><small>Kun Wang et.al.</small></td><td><small>NTU, NUS, USTC, ZJU, BUAA, PKU</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2509.24323">pdf</a> / <a href="https://github.com/yeyeyeah2/MAS2">code</a></small></td></tr>
<tr><td><small>24/30</small></td><td><small>2025-09-27</small></td><td><details><summary><strong>C-Evolve: Consensus-based Evolution for Prompt Groups</strong></summary>C-Evolve modifies island-based evolution to optimize a group of prompts that maximize consensus accuracy (majority vote) rather than individual performance. The authors introduce a 'voting score' fitness function‚Äîcalculated via Exponential Moving Average (EMA) of an individual's contribution to sampled groups‚Äîwhich successfully drives the population toward diverse, complementary strategies that outperform ensembles of individually optimized prompts (beating AlphaEvolve by ~4% on Qwen3-8B). The single most actionable takeaway is the **EMA voting score mechanism**: we can steal this exact fitness formulation to evolve portfolios of complementary VRP heuristics in AlgoEvo, replacing our current focus on converging to a single 'best' solver. While the benchmarks are standard (MATH, HotpotQA), the method offers a robust solution to the 'single heuristic limitation' we face in OR.</details></td><td><small>Tiancheng Li et.al.</small></td><td><small>Westlake University</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2509.23331">pdf</a> / code</small></td></tr>
<tr><td><small>28/30</small></td><td><small>2025-09-26</small></td><td><details><summary><strong>Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents</strong></summary>DGM implements a population-based evolutionary loop where agents modify their own Python source code (tools, memory, flow) to improve performance on coding benchmarks, rather than just optimizing prompts or parameters. Results are strong and verified: it boosts a base agent from 20% to 50% on SWE-bench Verified, matching handcrafted SoTA, with ablations proving the necessity of the population archive (open-endedness) over single-lineage hill climbing. **Key Takeaway:** The 'self-diagnosis' mechanism‚Äîfeeding execution logs to a model to propose specific *architectural* code changes (e.g., implementing a 'str_replace' tool to fix granular editing errors)‚Äîis the exact mechanism we need to implement for evolving our heuristic searchers. This validates that LLM-driven code evolution is viable for complex logic improvement, not just toy tasks.</details></td><td><small>Jenny Zhang et.al.</small></td><td><small>Sakana AI, Vector Institute, University of British Columbia, Canada CIFAR AI Chair</small></td><td><small>Robotics</small></td><td><small><a href="http://arxiv.org/abs/2505.22954">pdf</a> / <a href="https://github.com/jennyzzt/dgm">code</a></small></td></tr>
<tr><td><small>23/30</small></td><td><small>2025-09-25</small></td><td><details><summary><strong>GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models</strong></summary>GeoEvolve augments standard LLM-based evolutionary search (OpenEvolve) with an outer 'researcher' loop that queries a domain-specific RAG (textbooks/papers) to inject theoretical constraints into mutation prompts. On geospatial interpolation tasks, they report 13-21% error reduction over standard evolution, with ablations confirming that retrieved domain knowledge‚Äînot just iterative feedback‚Äîdrives the performance gain. The critical takeaway is the architectural pattern of 'Knowledge-Guided Evolution': instead of relying on the LLM's internal weights for domain theory, they explicitly retrieve and inject theoretical priors (e.g., valid variogram definitions) to steer the search. We should adapt this 'Theory-RAG' outer loop for our AlgoEvo pipeline to force evolved VRP heuristics to respect OR theoretical bounds.</details></td><td><small>Peng Luo et.al.</small></td><td><small>Massachusetts Institute of Technology, Stanford University, Technical University of Munich</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2509.21593">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2025-09-21</small></td><td><details><summary><strong>Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning</strong></summary>Ouellette implements an Execution-Guided Neural Program Synthesis (EG-NPS) system for ARC-AGI that conditions the search on the intermediate execution state of every instruction, achieving 80% success on out-of-distribution tasks where TTFT (10%) and standard AlphaEvolve (0-14%) fail. The results are rigorous, using controlled OOD tasks to prove that TTFT relies on in-distribution priors rather than reasoning. The critical takeaway for our AlgoEvo work is the architecture of the 'state-conditioned decoder': instead of blind code generation, we should inject the tokenized execution result of step $t$ into the context for step $t+1$. This is effectively a dense process reward model that solves the sample efficiency bottleneck we face in evolutionary search.</details></td><td><small>Simon Ouellette et.al.</small></td><td><small></small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2507.15877">pdf</a> / <a href="https://github.com/SimonOuellette35/OODGenARC-AGI">code</a></small></td></tr>
<tr><td><small>26/30</small></td><td><small>2025-09-17</small></td><td><details><summary><strong>ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution</strong></summary>ShinkaEvolve presents an open-source evolutionary framework that drastically improves sample efficiency (e.g., beating AlphaEvolve on Circle Packing with only 150 evaluations vs. thousands) by integrating embedding-based novelty rejection, adaptive parent sampling, and bandit-based LLM selection. The results are credible, backed by code from Sakana AI, and directly target our primary pain point of high API costs/sample inefficiency in evolutionary search. **Key Takeaway:** We must implement their 'novelty rejection sampling' immediately‚Äîusing a cheap embedding model to filter out semantically similar code mutations (threshold 0.95) before execution is a trivial but high-impact optimization for our AlgoEvo pipeline. This paper proves that smart filtering is superior to the brute-force compute strategies we have been relying on.</details></td><td><small>Robert Tjarko Lange et.al.</small></td><td><small>Sakana AI</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2509.19349">pdf</a> / <a href="https://github.com/SakanaAI/ShinkaEvolve">code</a></small></td></tr>
<tr><td><small>22/30</small></td><td><small>2025-09-10</small></td><td><details><summary><strong>How Should We Meta-Learn Reinforcement Learning Algorithms?</strong></summary>Goldie et al. perform a rigorous empirical benchmark comparing LLM-based algorithm proposal against Black-box Evolution Strategies (ES) and various distillation methods. They find that while LLMs are sample-efficient for simple functions, they catastrophically fail to incorporate high-dimensional input features (e.g., the 20+ inputs in OPEN), where Black-box ES remains superior. The most actionable takeaway is 'Same-Size Distillation': distilling a learned black-box algorithm into a fresh network of identical size using synthetic data consistently improves out-of-distribution generalization with zero additional environment samples. We should implement this distillation step immediately and reconsider using LLMs for feature-heavy heuristic components.</details></td><td><small>Alexander David Goldie et.al.</small></td><td><small>University of Oxford</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2507.17668">pdf</a> / <a href="https://github.com/AlexGoldie/learn-rl-algorithms">code</a></small></td></tr>
<tr><td><small>28/30</small></td><td><small>2025-09-09</small></td><td><details><summary><strong>Autonomous Code Evolution Meets NP-Completeness</strong></summary>SATLUTION extends LLM evolutionary search to full-scale C++ repositories, autonomously evolving SAT solvers that outperform 2025 human competition winners using only 2024 training data. The results are highly rigorous, backed by 90k CPU hours of distributed evaluation and strict correctness proofs (DRAT), showing a clear monotonic improvement trajectory. The single most stealable insight is the **self-evolving rule system**: the agent autonomously updates a persistent set of markdown constraints (e.g., forbidden patterns, testing protocols) based on post-cycle failure analysis, effectively creating 'institutional memory' that prevents regression in long-horizon search. We must implement this meta-learning loop in AlgoEvo immediately to move beyond single-file optimization.</details></td><td><small>Cunxi Yu et.al.</small></td><td><small>NVIDIA Research, University of Maryland</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2509.07367">pdf</a> / code</small></td></tr>
<tr><td><small>25/30</small></td><td><small>2025-08-20</small></td><td><details><summary><strong>EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic Design</strong></summary>EoH-S reformulates Automated Heuristic Design (AHD) to evolve a complementary *set* of heuristics rather than a single robust one, proving the objective is submodular and solvable via a greedy strategy. Results are strong and credible: on TSPLib and CVRPLib, their set of 10 heuristics reduces the optimality gap by ~40-60% compared to the top 10 heuristics from FunSearch or ReEvo. **KEY TAKEAWAY:** We should replace standard elitist selection in AlgoEvo with their 'Complementary Population Management' (CPM). By greedily selecting individuals based on marginal contribution to instance coverage (using instance-wise performance vectors), we can automatically generate diverse operator pools for ALNS instead of relying on hand-crafted diversity metrics.</details></td><td><small>Fei Liu et.al.</small></td><td><small>Huawei Noah

































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Ark Lab, City University of Hong Kong</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2508.03082">pdf</a> / code</small></td></tr>
<tr><td><small>23/30</small></td><td><small>2025-08-08</small></td><td><details><summary><strong>LLM-Meta-SR: In-Context Learning for Evolving Selection Operators in Symbolic Regression</strong></summary>Zhang et al. develop a meta-evolutionary framework to evolve selection operators for symbolic regression, achieving state-of-the-art results on SRBench by outperforming expert-designed methods like Œµ-lexicase. The standout contribution is **semantics-aware crossover**: rather than selecting parents based solely on scalar fitness, they compute complementarity scores using performance vectors across instances, explicitly retrieving parents that solve different subsets of the problem. This effectively treats parent selection as a retrieval task based on behavioral signatures, ensuring the LLM combines distinct functional capabilities. We should immediately implement this complementarity-based parent retrieval in AlgoEvo to improve how we merge heuristics.</details></td><td><small>Hengzhe Zhang et.al.</small></td><td><small>Victoria University of Wellington, Michigan State University</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2505.18602">pdf</a> / <a href="https://anonymous.4open.science/r/LLM-Meta-SR/">code</a></small></td></tr>
<tr><td><small>24/30</small></td><td><small>2025-08-04</small></td><td><details><summary><strong>Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning</strong></summary>EvoTune augments LLM-based evolutionary search (FunSearch) by iteratively fine-tuning the LLM weights using Direct Preference Optimization (DPO) on the generated programs. The results are robust, consistently outperforming static FunSearch on Bin Packing, TSP, and Hash Code benchmarks by discovering better heuristics faster. The critical takeaway is the use of **Forward KL regularization** in DPO instead of the standard Reverse KL; this prevents the mode collapse that usually kills evolutionary diversity, allowing the model to learn from high-fitness samples while maintaining exploration. This is a direct blueprint for implementing the 'RL-infused evolution' component of our AlgoEvo project.</details></td><td><small>Anja Surina et.al.</small></td><td><small>EPFL, Apple</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2504.05108">pdf</a> / <a href="https://claire-labo.github.io/EvoTune/">code</a></small></td></tr>
<tr><td><small>18/30</small></td><td><small>2025-08-03</small></td><td><details><summary><strong>EvoVLMA: Evolutionary Vision-Language Model Adaptation</strong></summary>This paper proposes EvoVLMA, an LLM-based evolutionary framework that searches for Python code to adapt Vision-Language Models (feature selection and logits computation). They demonstrate that **jointly** evolving two coupled algorithmic components fails (worse than random), whereas a **sequential** two-stage evolution strategy yields SOTA results (beating manual baselines by ~1-2%). For our AlgoEvo work, the key takeaway is the infrastructure design: they wrap code execution in restartable web services with a process monitor to handle the high rate of CUDA errors/timeouts in generated code‚Äîa practical 'trick' we should adopt to improve our search stability.</details></td><td><small>Kun Ding et.al.</small></td><td><small>Chinese Academy of Sciences</small></td><td><small>ACM Multimedia 2025 (ACM MM 2025)</small></td><td><small><a href="http://arxiv.org/abs/2508.01558">pdf</a> / <a href="https://github.com/kding1225/EvoVLMA">code</a></small></td></tr>
<tr><td><small>23/30</small></td><td><small>2025-07-04</small></td><td><details><summary><strong>Behaviour Space Analysis of LLM-driven Meta-heuristic Discovery</strong></summary>The authors introduce a behavioral analysis framework for LLM-driven algorithm discovery, mapping the 'behavior space' of generated heuristics using Search Trajectory Networks (STNs) and Code Evolution Graphs (CEGs). Results on BBOB (5D) show that a simple 1+1 elitist strategy alternating between 'simplify code' and 'random new' prompts significantly outperforms population-based approaches, effectively balancing exploitation and exploration while preventing code bloat. The primary takeaway is the critical role of a 'simplify' mutation operator‚Äîwithout it, LLM-generated code tends to drift into complexity without performance gains. We should immediately adopt their visualization metrics to debug our own evolutionary search trajectories and implement their 'simplify' prompt strategy in AlgoEvo.</details></td><td><small>Niki van Stein et.al.</small></td><td><small>Leiden University, University of Stirling</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2507.03605">pdf</a> / <a href="https://doi.org/10.5281/zenodo.15675581">code</a></small></td></tr>
<tr><td><small>30/30</small></td><td><small>2025-06-16</small></td><td><details><summary><strong>AlphaEvolve: A coding agent for scientific and algorithmic discovery</strong></summary>AlphaEvolve extends FunSearch by evolving entire code files (rather than single functions) using a 'search/replace' diff format and Gemini 2.0, achieving SOTA results across matrix multiplication (beating Strassen), 50+ open math problems, and Google's production scheduling. The results are exceptionally strong and verified, including deployed improvements to Google's Borg scheduler (0.7% resource recovery) and TPU circuits. The critical takeaway is the move to **diff-based full-file evolution** and **meta-prompt evolution** (evolving the prompt instructions alongside the code), which allows the system to modify architecture and logic rather than just heuristics. This is a mandatory blueprint for the next iteration of our AlgoEvo and EvoCut projects.</details></td><td><small>Alexander Novikov et.al.</small></td><td><small>Google DeepMind</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2506.13131">pdf</a> / <a href="https://colab.research.google.com/github/google-deepmind/alphaevolve_results/blob/master/mathematical_results.ipynb">code</a></small></td></tr>
<tr><td><small>26/30</small></td><td><small>2025-06-10</small></td><td><details><summary><strong>Can Large Language Models Invent Algorithms to Improve Themselves?: Algorithm Discovery for Recursive Self-Improvement through Reinforcement Learning</strong></summary>Ishibashi et al. propose 'Self-Developing,' a framework where an LLM generates Python code for model merging, evaluates the results, and uses the performance data to fine-tune the generator via DPO in a recursive loop. The results are empirically strong, outperforming human-designed baselines (Task Arithmetic) by 4.3% on GSM8k and demonstrating that the generator explicitly learns better strategies over iterations. **Key Takeaway:** We can replace the static mutation operators in our evolutionary search with a DPO-trained model that learns from the search history‚Äîeffectively implementing 'learning to search.' This is a direct, actionable upgrade for our AlgoEvo and AlphaEvolve pipelines.</details></td><td><small>Yoichi Ishibashi et.al.</small></td><td><small>NEC Corporation</small></td><td><small>NAACL 2025 (main)</small></td><td><small><a href="http://arxiv.org/abs/2410.15639">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2025-06-01</small></td><td><details><summary><strong>EvoGit: Decentralized Code Evolution via Git-Based Multi-Agent Collaboration</strong></summary>Huang et al. introduce EvoGit, a framework where LLM agents asynchronously evolve code by treating Git commits as the population and using 3-way merges (based on Lowest Common Ancestor) as crossover. While the experiments (web app, bin packing generator) are largely qualitative and lack rigorous statistical benchmarking against baselines like MetaGPT, the architectural contribution is significant. The key takeaway is using Git's native DAG structure to handle lineage, persistence, and asynchronous concurrency 'for free,' replacing complex custom population managers. This is directly actionable for our AlgoEvo infrastructure to enable massive parallelism and better memory/traceability without reinventing the wheel.</details></td><td><small>Beichen Huang et.al.</small></td><td><small>The Hong Kong Polytechnic University</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2506.02049">pdf</a> / <a href="https://github.com/BillHuang2001/evogit">code</a></small></td></tr>
<tr><td><small>25/30</small></td><td><small>2025-05-22</small></td><td><details><summary><strong>STRCMP: Integrating Graph Structural Priors with Language Models for Combinatorial Optimization</strong></summary>STRCMP introduces a composite architecture where a GNN encodes CO problem instances (MILP/SAT) into embeddings that condition an LLM (fine-tuned via SFT and DPO) to generate solver-specific heuristics within an evolutionary loop. The results are strong and empirically backed, showing significant reductions in convergence time and timeouts compared to text-only evolutionary methods like AutoSAT and LLM4Solver. The key takeaway is the architectural blueprint for fusing instance-specific structural embeddings (via soft prompting) with LLM code generation to drastically improve the sample efficiency of evolutionary search. This is immediately relevant to our EvoCut and AlgoEvo projects, suggesting we should move beyond pure text prompts for topology-heavy problems.</details></td><td><small>Xijun Li et.al.</small></td><td><small>Shanghai Key Laboratory of Scalable Computing and Systems, School of Computer Science, Shanghai Jiao Tong University</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2506.11057">pdf</a> / code</small></td></tr>
<tr><td><small>26/30</small></td><td><small>2025-05-18</small></td><td><details><summary><strong>CALM: Co-evolution of Algorithms and Language Model for Automatic Heuristic Design</strong></summary>CALM introduces a hybrid evolutionary framework that fine-tunes the LLM generator *during* the search process using Group Relative Policy Optimization (GRPO), rather than relying solely on prompt evolution. Using a quantized Qwen-7B model on a single consumer GPU, it outperforms GPT-4o-based baselines (FunSearch, EoH) on Bin Packing and VRP benchmarks. The critical takeaway is their reward function design: instead of absolute performance, they reward the *relative improvement* of the generated code over the specific 'parent' heuristics in the prompt, stabilizing the RL signal. We should immediately test this 'online fine-tuning' approach to reduce our API costs and improve sample efficiency in AlgoEvo.</details></td><td><small>Ziyao Huang et.al.</small></td><td><small>City University of Hong Kong, Southeast University, University of Victoria, Hon Hai Research Institute</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2505.12285">pdf</a> / <a href="https://github.com/whxru/CALM">code</a></small></td></tr>
<tr><td><small>19/30</small></td><td><small>2025-04-01</small></td><td><details><summary><strong>LLM-Guided Search for Deletion-Correcting Codes</strong></summary>Weindel and Heckel adapt FunSearch to discover priority functions for the Maximum Independent Set problem (applied to deletion-correcting codes), achieving new SOTA lower bounds for specific lengths (n=12, 13, 16). The critical takeaway for us is their **functional deduplication** step: they hash function outputs on a small subset of data to discard syntactically unique but logically identical programs, which significantly improves sample efficiency by preventing the evaluator from wasting cycles on 'comment changes' or variable renames. Additionally, they demonstrate that optimizing for the single hardest instance generalizes better than averaging performance across a curriculum‚Äîa counter-intuitive finding we should test in our reward modeling.</details></td><td><small>Franziska Weindel et.al.</small></td><td><small>Technical University of Munich, Munich Center for Machine Learning</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2504.00613">pdf</a> / <a href="https://github.com/MLI-lab/FunDCC">code</a></small></td></tr>
<tr><td><small>26/30</small></td><td><small>2025-03-13</small></td><td><details><summary><strong>From Understanding to Excelling: Template-Free Algorithm Design through Structural-Functional Co-Evolution</strong></summary>Zhao et al. propose CAE, a framework that co-evolves algorithm structure (workflow/call graphs) alongside function implementations, aiming to eliminate the fixed templates required by SOTA methods like FunSearch and EoH. On TSP benchmarks, they report reducing optimality gaps by ~2-5% compared to ReEvo, and in quadratic optimization, the system autonomously discovered numerical stability fixes (e.g., replacing matrix inversion with solvers) that human baselines missed. The critical takeaway is the 'bi-dimensional co-evolution' strategy: explicitly maintaining and mutating a population of control flow graphs separate from the function bodies, which allows the system to escape the local optima imposed by a fixed human-designed harness. We must evaluate if this structural search approach can be integrated into AlgoEvo to automate our harness design.</details></td><td><small>Zhe Zhao et.al.</small></td><td><small>Princeton University, Nanyang Technological University, City University of Hong Kong, University of Science and Technology of China, The Hong Kong University of Science and Technology (Guangzhou)</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2503.10721">pdf</a> / code</small></td></tr>
<tr><td><small>24/30</small></td><td><small>2025-02-21</small></td><td><details><summary><strong>QUBE: Enhancing Automatic Heuristic Design via Quality-Uncertainty Balanced Evolution</strong></summary>QUBE replaces FunSearch's naive score-based parent selection with a UCB algorithm that selects parents based on the *average quality of their offspring* (exploitation) plus an uncertainty term (exploration). The authors demonstrate that a parent's own score is a poor predictor of its ability to evolve further; treating parents as 'bandit arms' based on their lineage statistics yields significantly better results on Bin Packing and TSP with fewer samples. While they fail to beat DeepMind's massive-scale Cap Set record, the methodological insight regarding 'offspring-aware' selection is statistically validated and immediately transferable to our evolutionary search frameworks.</details></td><td><small>Zijie Chen et.al.</small></td><td><small>Westlake University, Zhejiang University, University of Electronic Science and Technology of China</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2412.20694">pdf</a> / <a href="https://github.com/zzjchen/QUBE_code">code</a></small></td></tr>
<tr><td><small>18/30</small></td><td><small>2025-02-13</small></td><td><details><summary><strong>Explainable AI-assisted Optimization for Feynman Integral Reduction</strong></summary>Song et al. apply FunSearch to evolve priority functions for Feynman integral reduction, achieving up to 3058x reduction in seeding integrals compared to standard heuristics. The results are rigorous, enabling previously impossible multi-loop calculations. The critical insight for us is the successful transfer of heuristics evolved on trivial 1-loop instances (fast evaluation) to complex 5-loop problems without retraining. We should adopt this 'evolve-on-toy, deploy-on-giant' evaluation protocol to drastically reduce compute costs in our VRP and SAT solver evolutionary search pipelines.</details></td><td><small>Zhuo-Yang Song et.al.</small></td><td><small>Peking University, Universit
Z
rich, Beijing Computational Science Research Center</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2502.09544">pdf</a> / code</small></td></tr>
<tr><td><small>25/30</small></td><td><small>2025-02-04</small></td><td><details><summary><strong>Multi-objective Evolution of Heuristic Using Large Language Model</strong></summary>MEoH extends LLM-based heuristic evolution (like FunSearch/EoH) to multi-objective scenarios (e.g., Gap vs. Runtime) by introducing a 'Dominance-Dissimilarity' mechanism that selects parents based on both Pareto dominance and Abstract Syntax Tree (AST) code distance. The results are credible and strong: on TSP, they find heuristics matching EoH's quality but running 16x faster (1.37s vs 22.4s) by effectively navigating the complexity-performance trade-off. The single most useful takeaway is the **AST-based dissimilarity metric** for population management; we should immediately steal this to prune semantically identical code in our evolutionary loops, thereby forcing exploration and improving sample efficiency. This is a direct upgrade to our current single-objective evolutionary search methods.</details></td><td><small>Shunyu Yao et.al.</small></td><td><small>City University of Hong Kong, Southern University of Science and Technology</small></td><td><small>AAAI Conference on Artificial Intelligence</small></td><td><small><a href="http://arxiv.org/abs/2409.16867">pdf</a> / <a href="https://github.com/Optima-CityU/LLM4AD">code</a></small></td></tr>
<tr><td><small>22/30</small></td><td><small>2024-12-19</small></td><td><details><summary><strong>HSEvo: Elevating Automatic Heuristic Design with Diversity-Driven Harmony Search and Genetic Algorithm Using LLMs</strong></summary>HSEvo extends LLM-based evolutionary search (LLM-EPS) by integrating a numerical parameter tuning step (Harmony Search) and a token-efficient 'Flash Reflection' mechanism that batches analysis of parent pairs. They report superior results over ReEvo and FunSearch on Bin Packing and TSP, validated by proposed diversity metrics based on code embeddings. **Key Takeaway:** We should implement the hybrid tuning pattern: explicitly parsing LLM-generated code to extract constants and tuning them with a cheap numerical optimizer (rather than asking the LLM to tune parameters), and adopt batched reflections to reduce inference costs.</details></td><td><small>Pham Vu Tuan Dat et.al.</small></td><td><small>George Mason University, Hanoi University of Science and Technology</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2412.14995">pdf</a> / <a href="https://github.com/datphamvn/HSEvo">code</a></small></td></tr>
<tr><td><small>23/30</small></td><td><small>2024-11-29</small></td><td><details><summary><strong>Amplifying human performance in combinatorial competitive programming</strong></summary>DeepMind applies FunSearch (using Gemini 1.5 Flash) to evolve scoring functions within human-written greedy backbones for Hash Code and AtCoder problems, achieving top-1% or rank-1 performance against humans. The results are robust, beating top human teams on 5/8 historical contests using a generic evolutionary setup. The critical takeaway is the 'switching variable' technique: using a single evolved function to handle multiple distinct decision points (e.g., selecting a vehicle vs. selecting a route) by passing a state flag, rather than evolving multiple interacting functions. This validates that generalist models (Flash) are sufficient for high-end OR evolution without code-specific fine-tuning. We should adopt their 'Backbone + Scorer' architecture for our VRP/Scheduling work immediately.</details></td><td><small>Petar Veliƒçkoviƒá et.al.</small></td><td><small>Google DeepMind</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2411.19744">pdf</a> / code</small></td></tr>
<tr><td><small>20/30</small></td><td><small>2024-10-30</small></td><td><details><summary><strong>Automatic programming via large language models with population self-evolution for dynamic job shop scheduling problem</strong></summary>This paper introduces SeEvo, an LLM-based evolutionary search for Dynamic Job Shop Scheduling heuristics that adds an 'individual self-reflection' loop‚Äîprompting the LLM to analyze performance differences of a specific rule before and after mutation‚Äîalongside standard population-level reflection. While they claim significant improvements over GP/GEP and DRL, the ablation study reveals only a marginal <1% improvement over the existing ReEvo framework on benchmark instances. The primary takeaway for us is the specific prompt engineering technique of injecting an individual's mutation history (previous code vs. current code performance) into the context to guide the next mutation, which could potentially improve sample efficiency in our own evolutionary loops despite their weak empirical validation.</details></td><td><small>Jin Huang et.al.</small></td><td><small>Huazhong University of Science and Technology</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2410.22657">pdf</a> / code</small></td></tr>
<tr><td><small>25/30</small></td><td><small>2024-10-14</small></td><td><details><summary><strong>ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution</strong></summary>ReEvo integrates a 'Reflector LLM' into genetic programming that analyzes pairs of heuristics (better vs. worse) to generate textual 'verbal gradients' for crossover and mutation, maintaining a long-term memory of these insights. The results are strong and relevant: they outperform EoH (Evolution of Heuristics) and NCO baselines on TSP, CVRP, and Bin Packing with significantly higher sample efficiency (only ~100 evaluations). The single most useful takeaway is the 'Short-term Reflection' prompting strategy‚Äîexplicitly asking the LLM to derive a mutation direction by comparing the logic of high-fitness vs. low-fitness parents‚Äîwhich we should immediately test in our AlgoEvo framework to reduce sample costs. This is a direct methodological upgrade for our current evolutionary search pipelines.</details></td><td><small>Haoran Ye et.al.</small></td><td><small>Peking University, KAIST, Singapore Management University, Southeast University, PKU-Wuhan Institute for AI</small></td><td><small>NeurIPS 2024</small></td><td><small><a href="http://arxiv.org/abs/2402.01145">pdf</a> / <a href="https://github.com/AI4CO/ReEvo">code</a></small></td></tr>
<tr><td><small>20/30</small></td><td><small>2024-07-15</small></td><td><details><summary><strong>Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models</strong></summary>Zhang et al. perform a rigorous benchmarking of major LLM-based evolutionary program search (EPS) methods (FunSearch, EoH, ReEvo) against a simple (1+1)-EPS baseline across four problems and nine LLMs. The results are empirically solid and sobering: the simple (1+1)-EPS baseline‚Äîiterative improvement via one-shot prompting‚Äîfrequently matches or outperforms the complex population-based methods, particularly on bin packing, though EoH remains superior on TSP. **Crucial Takeaway:** We are likely over-engineering our search mechanisms; we must implement a (1+1)-EPS baseline in all future experiments (AlgoEvo, EvoCut) because if our multi-agent systems cannot beat this simple hill-climber, our papers will be rejected for unnecessary complexity. Additionally, they find that larger models (GPT-4) do not strictly guarantee better heuristic search performance compared to smaller, code-specialized models like CodeLlama-7B.</details></td><td><small>Rui Zhang et.al.</small></td><td><small>City University of Hong Kong, Southern University of Science and Technology</small></td><td><small>the 18th International Conference on Parallel Problem Solving From Nature (PPSN 2024)</small></td><td><small><a href="http://arxiv.org/abs/2407.10873">pdf</a> / <a href="https://github.com/zhichao-lu/llm-eps">code</a></small></td></tr>
<tr><td><small>23/30</small></td><td><small>2024-07-01</small></td><td><details><summary><strong>FunBO: Discovering Acquisition Functions for Bayesian Optimization with FunSearch</strong></summary>FunBO applies FunSearch to evolve Python code for Bayesian Optimization acquisition functions, evaluating fitness by running full BO loops on synthetic functions. The results are empirically strong, showing that evolved AFs generalize well to out-of-distribution functions and outperform standard baselines like EI and UCB. The most stealable insight is their 'few-shot' adaptation strategy, where a general-purpose heuristic is rapidly fine-tuned on a small set of target instances‚Äîa technique we should immediately test for our VRP heuristics. While the method is computationally expensive (brute-forcing the inner loop), the interpretable code outputs provide concrete ideas for dynamic exploration-exploitation trade-offs.</details></td><td><small>Virginia Aglietti et.al.</small></td><td><small>Google DeepMind</small></td><td><small>International Conference on Machine Learning</small></td><td><small><a href="http://arxiv.org/abs/2406.04824">pdf</a> / code</small></td></tr>
<tr><td><small>19/30</small></td><td><small>2024-06-01</small></td><td><details><summary><strong>tnGPS: Discovering Unknown Tensor Network Structure Search Algorithms via Large Language Models (LLMs)</strong></summary>The authors propose tnGPS, a FunSearch-style framework that evolves Python code for Tensor Network Structure Search by mimicking human innovation stages (categorization, recombination, diversity injection). While the application (Tensor Networks) is niche, the results outperform standard heuristics like TNGA and TNLS. The critical takeaway for us is the 'Knowledge Categorization' phase: they use the LLM to semantically cluster the population of generated algorithms to manage diversity and guide the 'Diversity Injection' step. We should immediately implement this LLM-based population clustering in AlgoEvo to prevent convergence on similar code patterns.</details></td><td><small>Junhua Zeng et.al.</small></td><td><small>RIKEN Center for Advanced Intelligence Project, Tencent Inc., Guangdong University of Technology</small></td><td><small>ICML2024</small></td><td><small><a href="http://arxiv.org/abs/2402.02456">pdf</a> / <a href="https://github.com/ChaoLiAtRIKEN/tngps">code</a></small></td></tr>
<tr><td><small>25/30</small></td><td><small>2024-06-01</small></td><td><details><summary><strong>Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model</strong></summary>EoH introduces a dual-track evolutionary framework that evolves both natural language 'thoughts' (heuristic logic) and their corresponding Python code, rather than code alone. On Online Bin Packing, it claims to outperform DeepMind's FunSearch while using only ~2,000 LLM queries (vs FunSearch's millions), and achieves SOTA gaps on TSP and FSSP via Guided Local Search. The critical takeaway is the 'E2' prompt strategy: explicitly asking the LLM to extract common ideas from parent heuristics into a natural language 'thought' before generating code, which acts as a genetic Chain-of-Thought to stabilize mutation. We should immediately implement this 'Thought-then-Code' mutation operator in our AlgoEvo pipeline to address our sample efficiency bottlenecks.</details></td><td><small>Fei Liu et.al.</small></td><td><small>Huawei Noah‚Äôs Ark Lab, City University of Hong Kong, Southern University of Science and Technology</small></td><td><small>International Conference on Machine Learning</small></td><td><small><a href="http://arxiv.org/abs/2401.02051">pdf</a> / <a href="https://github.com/FeiLiu36/EoH">code</a></small></td></tr>
<tr><td><small>18/30</small></td><td><small>2024-03-18</small></td><td><details><summary><strong>LLM Guided Evolution -- The Automation of Models Advancing Models</strong></summary>Morris et al. propose 'Guided Evolution,' an LLM-based NAS framework that introduces 'Evolution of Thought' (EoT) and 'Character Role Play' to guide code mutations. While the results are statistically negligible (single trials, ~0.8% gain on CIFAR-10), the EoT mechanism offers a specific, actionable prompt engineering technique: explicitly prompting the LLM to compare a successful elite individual against its original seed to extract 'reasoning' before applying mutations to new individuals. This serves as a lightweight, prompt-based memory/feedback mechanism that could immediately improve sample efficiency in our evolutionary search agents. The 'Character Role Play' (e.g., asking the LLM to act as 'Dr. MaGoo' for unorthodox ideas) is a gimmicky but potentially useful heuristic for maintaining population diversity.</details></td><td><small>Clint Morris et.al.</small></td><td><small>Georgia Tech Research Institute</small></td><td><small>Annual Conference on Genetic and Evolutionary Computation</small></td><td><small><a href="http://arxiv.org/abs/2403.11446">pdf</a> / <a href="https://github.com/clint-kristopher-morris/llm-guided-evolution">code</a></small></td></tr>
<tr><td><small>24/30</small></td><td><small>2024-03-05</small></td><td><details><summary><strong>Evolution Transformer: In-Context Evolutionary Optimization</strong></summary>Lange et al. introduce the Evolution Transformer, a causal architecture that learns to perform evolutionary strategy updates by attending to optimization history, effectively 'distilling' algorithms like CMA-ES into a neural network. Crucially, they propose 'Self-Referential Algorithm Distillation' (SR-EAD), where the model improves itself by perturbing its own weights, generating trajectories, and filtering for the best ones to retrain on‚Äîeliminating the need for a teacher. The results are strong, showing generalization to unseen Brax control tasks and successful (though sometimes unstable) self-bootstrapping. The key takeaway for us is the SR-EAD loop as a mechanism for open-ended optimizer improvement, and their use of Perceiver cross-attention to handle variable population sizes‚Äîa technique we should immediately steal for our multi-agent memory architectures.</details></td><td><small>Robert Tjarko Lange et.al.</small></td><td><small>Google DeepMind, TU Berlin</small></td><td><small>GECCO Companion</small></td><td><small><a href="http://arxiv.org/abs/2403.02985">pdf</a> / <a href="https://github.com/RobertTLange/evosax">code</a></small></td></tr>
</tbody></table>

</details>

## Generative AI for OR

### üÜï Most Recent

| Date | Title | Authors | Affiliation | Links |
|------|-------|---------|-------------|-------|
| 2026-02-17 | <details><summary>**ReLoop: Structured Modeling and Behavioral Verification for Reliable LLM-Based Optimization**</summary>ReLoop proposes a verification pipeline for LLM-generated optimization models that detects 'silent failures' (code that runs but solves the wrong problem) by perturbing input parameters and checking for expected solver objective shifts. They demonstrate that standard execution feasibility is a poor proxy for correctness (90% gap) on their new RetailOpt-190 benchmark, and that this perturbation testing significantly improves reliability. The critical takeaway is the use of sensitivity analysis as a ground-truth-free process reward signal: we can validate evolved algorithms in AlgoEvo by asserting that specific input perturbations *must* trigger output changes, filtering out semantically invalid candidates before expensive evaluation.</details> | Junbo Jacob Lian et.al. | National University of Singapore, Northwestern University, City University of Hong Kong, Wenzhou University, Wenzhou Buyi Pharmacy Chain Co., Ltd. | [pdf](http://arxiv.org/abs/2602.15983) / [code](https://github.com/junbolian/ReLoop) |
| 2026-02-11 | <details><summary>**Constructing Industrial-Scale Optimization Modeling Benchmark**</summary>Li et al. introduce MIPLIB-NL, a benchmark of 223 industrial-scale MILP instances (up to 10^7 variables) reverse-engineered from MIPLIB 2017, enforcing strict model-data separation. Results are sobering: SOTA models like GPT-4 and fine-tuned OR-LLMs drop from ~90% accuracy on existing toy benchmarks to ~18% here, failing primarily on structural consistency and index handling at scale. For us, the key takeaway is their "Loop-Based Structural Scaffold" taxonomy‚Äîa method to compress massive industrial formulations into compact LLM prompts via model-data separation. This is a mandatory read for our OR-Bench project, as it demonstrates that current evaluations are effectively measuring overfitting to toy problems rather than genuine modeling capability.</details> | Zhong Li et.al. | Peking University, Huawei Technologies Co., Ltd., Great Bay University | [pdf](http://arxiv.org/abs/2602.10450) / [code](https://github.com/optsuite/MIPLIB-NL) |
| 2026-02-03 | <details><summary>**MIRROR: A Multi-Agent Framework with Iterative Adaptive Revision and Hierarchical Retrieval for Optimization Modeling in Operations Research**</summary>MIRROR is a multi-agent framework that translates natural language OR problems into Gurobi code using Hierarchical RAG (metadata filtering + semantic search) and an iterative repair loop. It achieves ~72% pass@1 across five benchmarks, outperforming Chain-of-Experts and fine-tuned models like LLMOPT without task-specific training. The key takeaway is their **structured revision tip mechanism**: upon execution failure, the agent generates a JSON object explicitly isolating the `error_statement`, `incorrect_code_snippet`, and `correct_code_snippet`, which serves as a precise memory artifact for subsequent retries. This structured reflection pattern is superior to raw error logs and could be immediately adopted in our own code generation pipelines.</details> | Yifan Shi et.al. | Xi'an Jiaotong University, Northwestern Polytechnical University | [pdf](http://arxiv.org/abs/2602.03318) / code |
| 2026-02-03 | <details><summary>**ProOPF: Benchmarking and Improving LLMs for Professional-Grade Power Systems Optimization Modeling**</summary>Shen et al. propose a benchmark (ProOPF) for translating natural language into Optimal Power Flow (OPF) models, treating instances as parametric or structural modifications to a canonical base model rather than generating code from scratch. They introduce a rigorous data synthesis pipeline using 'scenario trees' to map qualitative descriptions (e.g., 'heatwave') to quantitative parameter deltas, and define structural extensions (e.g., adding security constraints) as modular patches. Results are sobering: SOTA models (GPT-4, Claude 3.5) score 0% on the hardest level (semantic inference + structural change), though SFT recovers ~11-35%. **Key Takeaway:** We should steal their 'Base + Delta' synthesis approach for our VRP variant generation and OR-Bench work; it allows for scalable, physically valid data generation without requiring an LLM to hallucinate full solvers, and effectively benchmarks 'ambiguity' handling.</details> | Chao Shen et.al. |  | [pdf](http://arxiv.org/abs/2602.03070) / code |
| 2026-02-02 | <details><summary>**Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation**</summary>Lyu et al. propose a 'Canonical Intermediate Representation' (CIR) to decouple natural language operational rules from their mathematical instantiation, explicitly forcing the LLM to select modeling paradigms (e.g., time-indexed vs. continuous flow) before coding. They achieve state-of-the-art accuracy (47.2% vs 22.4% baseline) on a new, complex benchmark (ORCOpt-Bench) by using a multi-agent pipeline that retrieves and adapts constraint templates. The key takeaway is the 'Mapper' agent's paradigm selection logic, which prevents common formulation errors in VRPs and scheduling; we should evaluate CIR as a structured mutation space for AlgoEvo to replace brittle code evolution. The new benchmark is immediately relevant for our OR-Bench evaluation suite.</details> | Zhongyuan Lyu et.al. | The Hong Kong Polytechnic University, InfiX.ai | [pdf](http://arxiv.org/abs/2602.02029) / code |

### ‚≠ê Best Papers

| Score | Date | Title | Authors | Affiliation | Links |
|-------|------|-------|---------|-------------|-------|
| 28/30 | 2025-08-16 | <details><summary>**EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models**</summary>Yazdani et al. introduce EvoCut, an evolutionary framework where LLMs generate Python code for MILP cuts, filtered by a 'usefulness check' (does it cut the current LP relaxation?) and an 'empirical validity check' (does it preserve known integer optima?). They report 17-57% gap reductions on TSPLIB and JSSP compared to Gurobi defaults, backed by strong ablation studies on the evolutionary operators. **Key Takeaway:** The reliance on 'acceleration cuts'‚Äîconstraints verified empirically on small datasets rather than formally proven‚Äîbypasses the bottleneck of automated theorem proving while still delivering valid speedups. We should immediately adopt their 'LP separation' check as a cheap, high-signal reward for our own evolutionary search loops.</details> | M. Yazdani et.al. | Huawei Technologies Canada, University of British Columbia, University of Toronto | [pdf](http://arxiv.org/abs/2508.11850) / [code](https://github.com/milad1378yz/EvoCut) |
| 27/30 | 2026-01-29 | <details><summary>**NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents**</summary>NEMO achieves SOTA on 8/9 optimization benchmarks by deploying autonomous coding agents that generate both a declarative optimizer (solver code) and an imperative simulator (verification code). The key innovation is using the simulator to validate the optimizer's results in a closed loop, detecting logical errors without ground truth‚Äîa technique that beats fine-tuned models like SIRL by up to 28%. The most stealable insight is this asymmetric validation: imperative Python simulation is often less error-prone than declarative constraint formulation, making it a robust 'critic' for generated solvers. This is immediately applicable to our OR-Bench and AlgoEvo projects for generating reliable reward signals.</details> | Yang Song et.al. | Carnegie Mellon University, C3 AI | [pdf](http://arxiv.org/abs/2601.21372) / code |
| 26/30 | 2025-09-26 | <details><summary>**StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models**</summary>Zhou et al. propose StepORLM, a framework where an 8B policy and a **Generative Process Reward Model (GenPRM)** co-evolve. Unlike standard discriminative PRMs that score steps in isolation, their GenPRM generates a reasoning trace to evaluate the full trajectory's logic before assigning credit, addressing the interdependency of OR constraints. They align the policy using **Weighted DPO**, where preference weights are derived from the GenPRM's process scores. They claim to beat GPT-4o and DeepSeek-V3 on 6 OR benchmarks (e.g., NL4Opt, MAMO) with an 8B model. **Key Takeaway:** We should test **Generative PRMs** immediately for AlgoEvo; asking the critic to 'explain then score' (generative) rather than just 'score' (discriminative) likely fixes the credit assignment noise in our long-horizon search.</details> | Chenyu Zhou et.al. | Shanghai Jiao Tong University | [pdf](http://arxiv.org/abs/2509.22558) / code |
| 26/30 | 2025-05-17 | <details><summary>**Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling**</summary>Chen et al. introduce SIRL, a framework for training LLMs to generate optimization models using Reinforcement Learning with Verifiable Rewards (RLVR) and a novel 'Partial KL' surrogate objective. By removing the KL penalty from the reasoning (CoT) section while retaining it for the code generation section, they balance exploration with syntactic stability, achieving SOTA on OptMATH and IndustryOR against OpenAI-o3 and DeepSeek-R1. The critical takeaway for us is the Partial KL strategy: it allows the model to 'think' freely outside the reference distribution while adhering to strict coding standards‚Äîa technique we should immediately test in AlgoEvo. Furthermore, their method of parsing .lp files to extract structural features (variable counts, constraint types) for 'instance-enhanced self-consistency' provides a much richer signal than our current binary success/failure metrics.</details> | Yitian Chen et.al. | Stanford University, Shanghai Jiao Tong University, The University of Hong Kong, Shanghai University of Finance and Economics, Cardinal Operations | [pdf](http://arxiv.org/abs/2505.11792) / [code](https://github.com/Cardinal-Operations/SIRL) |
| 25/30 | 2026-02-17 | <details><summary>**ReLoop: Structured Modeling and Behavioral Verification for Reliable LLM-Based Optimization**</summary>ReLoop proposes a verification pipeline for LLM-generated optimization models that detects 'silent failures' (code that runs but solves the wrong problem) by perturbing input parameters and checking for expected solver objective shifts. They demonstrate that standard execution feasibility is a poor proxy for correctness (90% gap) on their new RetailOpt-190 benchmark, and that this perturbation testing significantly improves reliability. The critical takeaway is the use of sensitivity analysis as a ground-truth-free process reward signal: we can validate evolved algorithms in AlgoEvo by asserting that specific input perturbations *must* trigger output changes, filtering out semantically invalid candidates before expensive evaluation.</details> | Junbo Jacob Lian et.al. | National University of Singapore, Northwestern University, City University of Hong Kong, Wenzhou University, Wenzhou Buyi Pharmacy Chain Co., Ltd. | [pdf](http://arxiv.org/abs/2602.15983) / [code](https://github.com/junbolian/ReLoop) |

### üî¨ Research Fronts

| Status | Front Name | Papers | Front Analysis |
|--------|-----------|--------|----------------|
| ‚úÖ Stable | Self-Improving LLM Agents for Iterative OR Program Synthesis | <details><summary>11</summary><ol><li><a href="http://arxiv.org/abs/2508.11850">EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models</a> <sub>(28)</sub></li><li><a href="http://arxiv.org/abs/2507.10614">Fine-tuning Large Language Model for Automated Algorithm Design</a> <sub>(25)</sub></li><li><a href="http://arxiv.org/abs/2505.10117">Learning Virtual Machine Scheduling in Cloud Computing through Language Agents</a> <sub>(25)</sub></li><li><a href="http://arxiv.org/abs/2502.14760">EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of Optimization Formulations</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2507.11737">Auto-Formulating Dynamic Programming Problems with Large Language Models</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2510.16916">SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2403.01131">LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2410.22296">Generalists vs. Specialists: Evaluating LLMs on Highly-Constrained Biophysical Sequence Optimization Tasks</a> <sub>(22)</sub></li><li><a href="http://arxiv.org/abs/2602.02029">Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation</a> <sub>(22)</sub></li><li><a href="http://arxiv.org/abs/2411.17404">BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical Modeling Problem Solving</a> <sub>(22)</sub></li><li><a href="http://arxiv.org/abs/2508.03117">Toward a Trustworthy Optimization Modeling Agent via Verifiable Synthetic Data Generation</a> <sub>(20)</sub></li></ol></details> | <details><summary>View analysis</summary>This research front focuses on developing advanced LLM-powered frameworks for robust optimization modeling and program synthesis in Operations Research. Central to these efforts are iterative generation, evaluation, and refinement mechanisms, moving beyond simple prompting. Key approaches include generative process supervision (StepORLM), multi-agent systems with adaptive revision (MIRROR, OptimAI, DAOpt), self-improving experience libraries (AlphaOPT), semantic verification (SAC-Opt), and grammar-aware generation with compiler feedback (SyntAGM). The overarching goal is to reliably translate complex natural language OR problems into executable mathematical programming code.  Significant contributions include StepORLM's Generative Process Reward Models, achieving a +29.6% Pass@1 accuracy over GPT-4o on NL4Opt. MIRROR's multi-agent framework, using Hierarchical RAG and structured revision tips, improved Macro Avg by +3.68% over Chain-of-Experts. AlphaOPT's 'Library Evolution' mechanism, which refines applicability conditions, boosted performance by +13.6% on OptiBench. SAC-Opt demonstrated ~22% accuracy improvement on ComplexLP via backward-guided semantic verification. OptimAI introduced UCB-based debug scheduling, reducing NLP4LP error rates by 58%. SyntAGM achieved 61.6% accuracy on NL4Opt using compiler-in-the-loop grammar enforcement, while DAOpt integrated LLMs with RSOME for robust optimization, achieving >70% out-of-sample feasibility. CALM enabled a 4B model to match DeepSeek-R1 on OR benchmarks through corrective adaptation. A critical survey highlighted significant error rates in existing benchmarks (16-54%) and the ineffectiveness of Chain-of-Thought for symbolic formulation, while DCP-Bench-Open introduced 'Multi-Instance Accuracy' to address LLM overfitting to example data.  This front is rapidly maturing, with a clear trajectory towards more autonomous, verifiable, and adaptive LLM agents for OR. Future work will likely emphasize integrating deeper OR domain knowledge, scaling to real-world industrial problems, and enhancing explainability. The focus is shifting from merely generating code to building systems that can intelligently debug, verify, and continually improve their modeling capabilities, moving towards human-level proficiency in complex OR problem formulation.</details> |
| ‚úÖ Stable | Self-Improving LLM Frameworks for Robust Optimization Model Synthesis | <details><summary>11</summary><ol><li><a href="http://arxiv.org/abs/2509.22558">StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models</a> <sub>(26)</sub></li><li><a href="http://arxiv.org/abs/2510.04204">CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling</a> <sub>(25)</sub></li><li><a href="http://arxiv.org/abs/2510.18428">AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library</a> <sub>(25)</sub></li><li><a href="http://arxiv.org/abs/2407.19633">OptiMUS-0.3: Using Large Language Models to Model and Solve Optimization Problems at Scale</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2601.17670">Grammar-Aware Literate Generative Mathematical Programming with Compiler-in-the-Loop</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2504.16918">OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents</a> <sub>(22)</sub></li><li><a href="http://arxiv.org/abs/2510.05115">SAC-Opt: Semantic Anchors for Iterative Correction in Optimization Modeling</a> <sub>(21)</sub></li><li><a href="http://arxiv.org/abs/2511.11576">DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs</a> <sub>(21)</sub></li><li><a href="http://arxiv.org/abs/2506.06052">DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems</a> <sub>(20)</sub></li><li><a href="http://arxiv.org/abs/2508.10047">A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions</a> <sub>(20)</sub></li><li><a href="http://arxiv.org/abs/2602.03318">MIRROR: A Multi-Agent Framework with Iterative Adaptive Revision and Hierarchical Retrieval for Optimization Modeling in Operations Research</a> <sub>(20)</sub></li></ol></details> | <details><summary>View analysis</summary>This research front focuses on developing advanced LLM-driven frameworks for automated optimization modeling, specifically translating natural language problem descriptions into executable mathematical programming code (e.g., Gurobi, CPMpy, PyOPL). The unifying theme is the move beyond simple prompt engineering to sophisticated multi-agent architectures, self-correction mechanisms, and iterative refinement loops that enhance the accuracy, robustness, and reliability of generated optimization models across various problem domains, including discrete combinatorial problems, MILP, and data-driven optimization under uncertainty.  Key contributions include the introduction of novel benchmarks like DCP-Bench-Open for constraint modeling and OptU for data-driven optimization under uncertainty, alongside critical re-evaluations of existing datasets (NL4Opt, IndustryOR) to address high error rates. Methodologically, frameworks like CALM leverage corrective adaptation with lightweight modification and RL to achieve significant accuracy gains (e.g., +23.6% Macro AVG for GPT-3.5-Turbo). AlphaOPT introduces a self-improving experience library that refines applicability conditions, outperforming fine-tuned models by 13%. Multi-agent systems like OptiMUS-0.3 (achieving +40% accuracy on NLP4LP) and OptimAI (88.1% accuracy on NLP4LP with UCB-based debug scheduling) integrate connection graphs, structure detection, and adaptive debugging. SAC-Opt employs backward-guided semantic verification for a 22% accuracy improvement, while SyntAGM utilizes a compiler-in-the-loop with grammar-aware generation to match multi-agent systems in accuracy. StepORLM introduces a Generative Process Reward Model (GenPRM) and Weighted DPO, enabling an 8B model to surpass GPT-4o on several OR benchmarks, and DAOpt demonstrates robust modeling under uncertainty, achieving >70% out-of-sample feasibility compared to 27% for deterministic models.  This front is rapidly maturing, demonstrating a clear trajectory from basic LLM code generation to highly sophisticated, self-correcting, and domain-aware systems. The next wave of papers will likely focus on scaling these frameworks to tackle larger, more complex industrial-scale problems, integrating deeper OR domain knowledge through specialized DSLs or knowledge graphs, and enhancing the explainability and trustworthiness of the automated modeling process to facilitate real-world adoption.</details> |
| ‚úÖ Stable | Agentic LLM Frameworks for Robust OR Model Synthesis and Validation | <details><summary>9</summary><ol><li><a href="http://arxiv.org/abs/2601.21372">NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents</a> <sub>(27)</sub></li><li><a href="http://arxiv.org/abs/2508.14544">Adaptively Robust LLM Inference Optimization under Prediction Uncertainty</a> <sub>(22)</sub></li><li><a href="http://arxiv.org/abs/2511.16383">An Agent-Based Framework for the Automatic Validation of Mathematical Optimization Models</a> <sub>(22)</sub></li><li><a href="http://arxiv.org/abs/2508.07468">CP-Agent: Agentic Constraint Programming</a> <sub>(21)</sub></li><li><a href="http://arxiv.org/abs/2506.07972">HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization</a> <sub>(21)</sub></li><li><a href="http://arxiv.org/abs/2505.04354">Optimization Problem Solving Can Transition to Evolutionary Agentic Workflows</a> <sub>(20)</sub></li><li><a href="http://arxiv.org/abs/2506.07759">REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models</a> <sub>(20)</sub></li><li><a href="http://arxiv.org/abs/2504.04310">CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization</a> <sub>(20)</sub></li><li><a href="http://arxiv.org/abs/2503.10642">Text2Zinc: A Cross-Domain Dataset for Modeling Optimization and Satisfaction Problems in MiniZinc</a> <sub>(15)</sub></li></ol></details> | <details><summary>View analysis</summary>This research front centers on the development of agentic LLM frameworks for automating various aspects of Operations Research, primarily focusing on robust optimization model synthesis and the design of novel heuristics. Key frameworks like NEMO, CP-Agent, and HeuriGym leverage LLMs to generate and iteratively refine optimization models (e.g., MiniZinc, CPMpy) and heuristic algorithms for complex combinatorial problems such as Flexible Job Shop Scheduling and various routing tasks. A significant unifying theme is the use of iterative, execution-aware feedback loops and agentic workflows to enhance the reliability and performance of LLM-generated OR artifacts.  Notable contributions include Zadorojniy et al.'s multi-agent framework for automatic validation of optimization models, achieving 76% mutation coverage on NLP4LP. Szeider's CP-Agent demonstrates 100% accuracy on a clarified CP-Bench for CPMpy model generation, highlighting the power of persistent IPython kernels and iterative refinement. NEMO (2601.21372) achieves SOTA on 8/9 optimization benchmarks by employing an asymmetric simulator-optimizer validation loop, outperforming fine-tuned models by up to 28%. Forni√©s-Tabuenca et al.'s REMoH introduces a reflection mechanism with phenotypic clustering for multi-objective FJSSP, improving Pareto front diversity. Benchmarks like HeuriGym and CO-Bench are established to rigorously evaluate LLM-crafted heuristics, revealing that current SOTA LLMs saturate at ~60% of expert performance and highlighting challenges in handling context fragmentation and strict feasibility constraints.  This front is rapidly maturing, characterized by a shift from basic LLM prompting to sophisticated agentic workflows with robust validation mechanisms. The emphasis on execution-aware feedback, mutation testing, and simulator-optimizer loops signifies a move towards more reliable and verifiable LLM-generated OR solutions. The next wave of papers will likely focus on improving the computational efficiency of these agentic frameworks, scaling them to larger and more complex real-world problems, and integrating them with formal verification tools to address theoretical guarantees, pushing beyond empirical performance to provable correctness.</details> |
| ‚úÖ Stable | LLM-Enhanced Optimization Modeling via SIRL, MCTS, and Graph-Theoretic Evaluation | <details><summary>7</summary><ol><li><a href="http://arxiv.org/abs/2505.11792">Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling</a> <sub>(26)</sub></li><li><a href="http://arxiv.org/abs/2411.01679">Autoformulation of Mathematical Optimization Models Using LLMs</a> <sub>(25)</sub></li><li><a href="http://arxiv.org/abs/2510.27610">ORGEval: Graph-Theoretic Evaluation of LLMs in Optimization Modeling</a> <sub>(25)</sub></li><li><a href="http://arxiv.org/abs/2502.11102">OptMATH: A Scalable Bidirectional Data Synthesis Framework for Optimization Modeling</a> <sub>(24)</sub></li><li><a href="http://arxiv.org/abs/2402.10172">OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2407.09887">OptiBench Meets ReSocratic: Measure and Improve LLMs for Optimization Modeling</a> <sub>(22)</sub></li><li><a href="http://arxiv.org/abs/2410.13213">LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch</a> <sub>(18)</sub></li></ol></details> | <details><summary>View analysis</summary>This research front focuses on advancing Large Language Models (LLMs) for automated optimization modeling, data synthesis, and rigorous evaluation. Key frameworks include Solver-Informed RL (SIRL) with a Partial KL strategy, LLM-enhanced Monte-Carlo Tree Search (MCTS) with symbolic pruning, modular multi-agent systems like OptiMUS using a Connection Graph, and graph-theoretic evaluation frameworks such as ORGEval. The unifying theme is to develop more robust, scalable, and verifiable methods for LLM-driven generation and refinement of optimization models, often leveraging structured intermediate representations or search processes.  Key contributions include SIRL's Reinforcement Learning with Verifiable Reward (RLVR) achieving +3.3% Macro AVG on benchmarks like NL4OPT, and LLMOPT's multi-instruction SFT and KTO yielding up to +19.8% SA improvement over ORLM. OptMATH and ReSocratic introduce scalable bidirectional data synthesis frameworks, with OptMATH-Qwen2.5-32B surpassing GPT-4 on NL4Opt. The Autoformulator integrates LLMs with MCTS and SMT-based symbolic pruning, outperforming ORLMLlama3-8B by up to +27.8% on ComplexOR. OptiMUS utilizes a modular multi-agent system with a Connection Graph for context management, achieving significant accuracy gains (e.g., +57.2% on ComplexOR). ORGEval provides a graph-theoretic evaluation using the Weisfeiler-Lehman test for model isomorphism, achieving 100% consistency with solver-based methods in seconds for hard MIPLIB instances.  This front is emerging and rapidly maturing, with a high density and stable status indicating active, high-impact research. The trajectory suggests a strong push towards integrating these diverse techniques, such as combining RL with symbolic pruning or incorporating graph-theoretic evaluation into RL reward functions. Future work will likely address current limitations like reward hacking, data scarcity for large-scale problems, and extending capabilities to non-linear or stochastic optimization, emphasizing reliable and generalizable LLM capabilities in Operations Research.</details> |
| ‚úÖ Stable | Hierarchical RAG and Multi-Agent LLMs for Optimization Model Synthesis | <details><summary>2</summary><ol><li><a href="http://arxiv.org/abs/2509.08970">Gala: Global LLM Agents for Text-to-Model Translation</a> <sub>(19)</sub></li><li><a href="http://arxiv.org/abs/2505.01485">CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code</a> <sub>(18)</sub></li></ol></details> | <details><summary>View analysis</summary>This research front focuses on leveraging advanced LLM architectures, specifically Retrieval-Augmented Generation (RAG) and multi-agent systems, for the automated synthesis of optimization models from natural language descriptions. CHORUS introduces a RAG framework with hierarchical retrieval and metadata-augmented indexing for generating Linear Programming (LP) code (e.g., Gurobi). GALA proposes a multi-agent LLM framework for text-to-MiniZinc translation, specializing agents in detecting and assembling global Constraint Programming (CP) constraints.  CHORUS significantly improves LP code generation accuracy, achieving a +147.9% increase for Llama3.3 (70B) on the NL4Opt-Code dataset, by using a novel metadata-augmented indexing strategy that bridges the semantic gap between natural language and solver APIs. GALA demonstrates the effectiveness of decomposing complex translation tasks into primitive-specific agents, showing a modest improvement in execution rate (57% vs 52% with o3-mini) over Chain-of-Thought on the TEXT2ZINC benchmark for MiniZinc model generation. Both approaches highlight the power of structured LLM interaction and specialized knowledge retrieval/processing for robust program synthesis in optimization.  This front is emerging, demonstrating early successes in applying sophisticated LLM architectures to the challenging domain of automated optimization modeling. The trajectory suggests a move towards more structured, agentic, and knowledge-augmented LLM systems that can better understand and formalize complex problem descriptions into executable optimization models. The likely next paper will focus on integrating these architectural innovations, perhaps combining advanced retrieval with multi-agent reasoning, or extending them to more complex problem types and solver paradigms beyond LP and CP.</details> |
| ‚úÖ Stable | LLM-Guided Search and Fine-Tuning for OR Model and Algorithm Synthesis | <details><summary>2</summary><ol><li><a href="http://arxiv.org/abs/2509.22979">OptiMind: Teaching LLMs to Think Like Optimization Experts</a> <sub>(21)</sub></li><li><a href="http://arxiv.org/abs/2512.18682">Solver-Independent Automated Problem Formulation via LLMs for High-Cost Simulation-Driven Design</a> <sub>(19)</sub></li></ol></details> | <details><summary>View analysis</summary>This research front focuses on advanced techniques for leveraging Large Language Models (LLMs) to automate and enhance Operations Research (OR) problem formulation and algorithm design. The core theme revolves around integrating LLMs into sophisticated search algorithms, employing specialized fine-tuning strategies with preference learning and synthetic data, and utilizing structured intermediate representations or hierarchical decomposition to improve the robustness, verifiability, and efficiency of generated OR solutions. Specific frameworks like BPP-Search, MiCo (FunSearch), EvoCut, SolverLLM, EquivaMap, DPLM (DualReflect), CIR, LLaMoCo, and LLOME (MargE) are central to these efforts.  Key contributions include Liu et al.'s DPO with Diversity-Aware Rank-based sampling, enabling a fine-tuned Llama-3.2-1B to match an 8B model on ASP and CVRP. LLaMoCo demonstrates that a 350M parameter model can significantly outperform GPT-4 in optimization code generation through instruction tuning. LLOME introduces the MargE loss function for stable and diverse fine-tuning in constrained optimization, outperforming DPO. In search, BPP-Search enhances Tree-of-Thought for mathematical modeling, while MiCo applies a hierarchical SMDP framework to VM scheduling, achieving an 11% improvement over Deep RL. EvoCut uses LLMs for MILP cut generation, yielding 17-57% gap reductions. SolverLLM introduces Prompt Backpropagation within MCTS for robust formulation. For verification and representation, EquivaMap achieves 100% accuracy in checking formulation equivalence, OptiTrust uses multi-language execution voting for verifiable synthetic data, and CIR introduces a Canonical Intermediate Representation, boosting accuracy to 47.2% on the ORCOpt-Bench.  This front is rapidly emerging and maturing, characterized by a strong emphasis on moving beyond generic LLM prompting towards highly specialized and verifiable integration. The trajectory indicates a clear focus on improving the reliability and performance of LLM-generated OR artifacts. Future work will likely concentrate on combining these advanced techniques, such as integrating MargE-style preference learning into MCTS or evolutionary search frameworks, and expanding CIR-like structured generation to encompass more complex, real-world OR problems, potentially with dynamic cut separation and formal proof systems.</details> |

<details>
<summary>üìã Full list (54 papers, sorted by date)</summary>

<table><colgroup><col width="5%"><col width="7%"><col width="34%"><col width="13%"><col width="16%"><col width="5%"><col width="20%"></colgroup>
<thead><tr><th>Score</th><th>Date</th><th>Title</th><th>Authors</th><th>Affiliation</th><th>Venue</th><th>Links</th></tr></thead>
<tbody>
<tr><td><small>25/30</small></td><td><small>2026-02-17</small></td><td><details><summary><strong>ReLoop: Structured Modeling and Behavioral Verification for Reliable LLM-Based Optimization</strong></summary>ReLoop proposes a verification pipeline for LLM-generated optimization models that detects 'silent failures' (code that runs but solves the wrong problem) by perturbing input parameters and checking for expected solver objective shifts. They demonstrate that standard execution feasibility is a poor proxy for correctness (90% gap) on their new RetailOpt-190 benchmark, and that this perturbation testing significantly improves reliability. The critical takeaway is the use of sensitivity analysis as a ground-truth-free process reward signal: we can validate evolved algorithms in AlgoEvo by asserting that specific input perturbations *must* trigger output changes, filtering out semantically invalid candidates before expensive evaluation.</details></td><td><small>Junbo Jacob Lian et.al.</small></td><td><small>National University of Singapore, Northwestern University, City University of Hong Kong, Wenzhou University, Wenzhou Buyi Pharmacy Chain Co., Ltd.</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.15983">pdf</a> / <a href="https://github.com/junbolian/ReLoop">code</a></small></td></tr>
<tr><td><small>24/30</small></td><td><small>2026-02-11</small></td><td><details><summary><strong>Constructing Industrial-Scale Optimization Modeling Benchmark</strong></summary>Li et al. introduce MIPLIB-NL, a benchmark of 223 industrial-scale MILP instances (up to 10^7 variables) reverse-engineered from MIPLIB 2017, enforcing strict model-data separation. Results are sobering: SOTA models like GPT-4 and fine-tuned OR-LLMs drop from ~90% accuracy on existing toy benchmarks to ~18% here, failing primarily on structural consistency and index handling at scale. For us, the key takeaway is their &quot;Loop-Based Structural Scaffold&quot; taxonomy‚Äîa method to compress massive industrial formulations into compact LLM prompts via model-data separation. This is a mandatory read for our OR-Bench project, as it demonstrates that current evaluations are effectively measuring overfitting to toy problems rather than genuine modeling capability.</details></td><td><small>Zhong Li et.al.</small></td><td><small>Peking University, Huawei Technologies Co., Ltd., Great Bay University</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.10450">pdf</a> / <a href="https://github.com/optsuite/MIPLIB-NL">code</a></small></td></tr>
<tr><td><small>20/30</small></td><td><small>2026-02-03</small></td><td><details><summary><strong>MIRROR: A Multi-Agent Framework with Iterative Adaptive Revision and Hierarchical Retrieval for Optimization Modeling in Operations Research</strong></summary>MIRROR is a multi-agent framework that translates natural language OR problems into Gurobi code using Hierarchical RAG (metadata filtering + semantic search) and an iterative repair loop. It achieves ~72% pass@1 across five benchmarks, outperforming Chain-of-Experts and fine-tuned models like LLMOPT without task-specific training. The key takeaway is their **structured revision tip mechanism**: upon execution failure, the agent generates a JSON object explicitly isolating the `error_statement`, `incorrect_code_snippet`, and `correct_code_snippet`, which serves as a precise memory artifact for subsequent retries. This structured reflection pattern is superior to raw error logs and could be immediately adopted in our own code generation pipelines.</details></td><td><small>Yifan Shi et.al.</small></td><td><small>Xi'an Jiaotong University, Northwestern Polytechnical University</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.03318">pdf</a> / code</small></td></tr>
<tr><td><small>20/30</small></td><td><small>2026-02-03</small></td><td><details><summary><strong>ProOPF: Benchmarking and Improving LLMs for Professional-Grade Power Systems Optimization Modeling</strong></summary>Shen et al. propose a benchmark (ProOPF) for translating natural language into Optimal Power Flow (OPF) models, treating instances as parametric or structural modifications to a canonical base model rather than generating code from scratch. They introduce a rigorous data synthesis pipeline using 'scenario trees' to map qualitative descriptions (e.g., 'heatwave') to quantitative parameter deltas, and define structural extensions (e.g., adding security constraints) as modular patches. Results are sobering: SOTA models (GPT-4, Claude 3.5) score 0% on the hardest level (semantic inference + structural change), though SFT recovers ~11-35%. **Key Takeaway:** We should steal their 'Base + Delta' synthesis approach for our VRP variant generation and OR-Bench work; it allows for scalable, physically valid data generation without requiring an LLM to hallucinate full solvers, and effectively benchmarks 'ambiguity' handling.</details></td><td><small>Chao Shen et.al.</small></td><td><small></small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.03070">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2026-02-02</small></td><td><details><summary><strong>Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation</strong></summary>Lyu et al. propose a 'Canonical Intermediate Representation' (CIR) to decouple natural language operational rules from their mathematical instantiation, explicitly forcing the LLM to select modeling paradigms (e.g., time-indexed vs. continuous flow) before coding. They achieve state-of-the-art accuracy (47.2% vs 22.4% baseline) on a new, complex benchmark (ORCOpt-Bench) by using a multi-agent pipeline that retrieves and adapts constraint templates. The key takeaway is the 'Mapper' agent's paradigm selection logic, which prevents common formulation errors in VRPs and scheduling; we should evaluate CIR as a structured mutation space for AlgoEvo to replace brittle code evolution. The new benchmark is immediately relevant for our OR-Bench evaluation suite.</details></td><td><small>Zhongyuan Lyu et.al.</small></td><td><small>The Hong Kong Polytechnic University, InfiX.ai</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.02029">pdf</a> / code</small></td></tr>
<tr><td><small>27/30</small></td><td><small>2026-01-29</small></td><td><details><summary><strong>NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents</strong></summary>NEMO achieves SOTA on 8/9 optimization benchmarks by deploying autonomous coding agents that generate both a declarative optimizer (solver code) and an imperative simulator (verification code). The key innovation is using the simulator to validate the optimizer's results in a closed loop, detecting logical errors without ground truth‚Äîa technique that beats fine-tuned models like SIRL by up to 28%. The most stealable insight is this asymmetric validation: imperative Python simulation is often less error-prone than declarative constraint formulation, making it a robust 'critic' for generated solvers. This is immediately applicable to our OR-Bench and AlgoEvo projects for generating reliable reward signals.</details></td><td><small>Yang Song et.al.</small></td><td><small>Carnegie Mellon University, C3 AI</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2601.21372">pdf</a> / code</small></td></tr>
<tr><td><small>23/30</small></td><td><small>2026-01-25</small></td><td><details><summary><strong>Grammar-Aware Literate Generative Mathematical Programming with Compiler-in-the-Loop</strong></summary>SyntAGM is a framework for translating natural language into Algebraic Modeling Language (PyOPL) code using a 'compiler-in-the-loop' approach, where the LLM is constrained by an in-context BNF grammar and iteratively repairs code based on compiler diagnostics. They demonstrate that this approach matches the accuracy of expensive multi-agent systems (like Chain-of-Experts) while being significantly faster and cheaper. The immediate takeaways for us are the **StochasticOR benchmark** (which we should adopt for RobustMAS) and the technique of **injecting explicit BNF grammars** into prompts to enforce syntax in evolutionary search without fine-tuning. The 'literate modeling' approach‚Äîembedding reasoning as comments directly next to code constraints‚Äîis also a clever memory mechanism we could steal for AlgoEvo.</details></td><td><small>Roberto Rossi et.al.</small></td><td><small>University of Edinburgh, University College Cork</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2601.17670">pdf</a> / <a href="https://gwr3n.github.io/rhetor/">code</a></small></td></tr>
<tr><td><small>23/30</small></td><td><small>2026-01-17</small></td><td><details><summary><strong>Automated Optimization Modeling via a Localizable Error-Driven Perspective</strong></summary>This paper introduces MIND, a framework for automated optimization modeling that combines error-driven data synthesis with a novel post-training method called DFPO. Instead of standard RLVR which suffers from sparse rewards on hard problems, DFPO uses a teacher model to minimally correct the student's *failed* rollouts, converting them into on-policy(ish) positive samples for SFT/RL. Results show a 7B model outperforming GPT-4 on IndustryOR and OptMATH benchmarks. **Key Takeaway:** We should steal the DFPO mechanism for AlgoEvo: rather than wasting failed evolutionary samples, use a stronger model (or oracle) to fix the code and feed it back as a reward signal, drastically improving sample efficiency in our RL loops.</details></td><td><small>Weiting Liu et.al.</small></td><td><small>Huawei Noah‚Äôs Ark Lab, Fudan University, University of Science and Technology of China</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.11164">pdf</a> / code</small></td></tr>
<tr><td><small>‚Äî</small></td><td><small>2026-01-09</small></td><td><strong>Global Optimization for Combinatorial Geometry Problems Revisited in the Era of LLMs</strong></td><td><small>Timo Berthold et.al.</small></td><td><small></small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2601.05943">pdf</a> / code</small></td></tr>
<tr><td><small>19/30</small></td><td><small>2025-12-21</small></td><td><details><summary><strong>Solver-Independent Automated Problem Formulation via LLMs for High-Cost Simulation-Driven Design</strong></summary>Li et al. propose APF, a framework to fine-tune LLMs for translating engineering requirements into optimization code without running expensive simulations during training. They generate synthetic training data and filter it by checking if the generated code ranks historical data instances similarly to how an LLM 'judge' ranks them based on the text requirements. Results show 7B models outperforming GPT-4o on antenna design tasks, validated by actual simulation. **Key Takeaway:** We can replace expensive ground-truth evaluations in our process reward models by checking consistency between generated code outputs and LLM-predicted rankings on cached historical data‚Äîa direct method to improve sample efficiency in AlgoEvo.</details></td><td><small>Yuchen Li et.al.</small></td><td><small>Xidian University, Victoria University of Wellington, Westlake University</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2512.18682">pdf</a> / code</small></td></tr>
<tr><td><small>18/30</small></td><td><small>2025-12-12</small></td><td><details><summary><strong>A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation</strong></summary>A-LAMP decomposes the translation of natural language task descriptions into executable RL environments via a multi-agent pipeline, separating parameter extraction, variable definition, and constraint formulation before code generation. The results show that this structured approach allows a 27B model to rival GPT-4o on simple tasks, though the benchmarks (e.g., grid-world drone delivery, trivial wireless scheduling) are toy-scale and the RL application is sometimes forced. The primary takeaway is the specific decomposition schema for symbolic modeling: we should steal their granular extraction pipeline (Parameters -> Objectives -> Variables -> Constraints) to improve the reliability of our automated problem instantiation in OR-Bench and AlgoEvo without relying solely on expensive frontier models.</details></td><td><small>Hong Je-Gal et.al.</small></td><td><small>Sejong University</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2512.11270">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2025-11-20</small></td><td><details><summary><strong>An Agent-Based Framework for the Automatic Validation of Mathematical Optimization Models</strong></summary>Zadorojniy et al. introduce a multi-agent framework for validating LLM-generated optimization models by generating a test suite and verifying the suite's quality via mutation testing (ensuring tests detect deliberate errors injected into the model). On 100 NLP4LP instances, they achieve a 76% mutation kill ratio and successfully classify external models where simple objective value comparisons fail. The critical takeaway is the 'bootstrapped validation' workflow: using mutation analysis to validate the generated unit tests themselves before using them to score the model. We should steal this mutation-based verification loop to create a robust, ground-truth-free fitness signal for our evolutionary search and OR benchmarking pipelines.</details></td><td><small>Alexander Zadorojniy et.al.</small></td><td><small>IBM Research</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2511.16383">pdf</a> / code</small></td></tr>
<tr><td><small>23/30</small></td><td><small>2025-11-01</small></td><td><details><summary><strong>SOCRATES: Simulation Optimization with Correlated Replicas and Adaptive Trajectory Evaluations</strong></summary>SOCRATES introduces a two-stage framework: first constructing 'Operational AI Replicas' (surrogates) via LLM-guided causal discovery, then using an LLM to analyze optimization trajectories on these surrogates to schedule hybrid algorithms (e.g., running BO then switching to GA). While the benchmarks (inventory, queuing) are simple and the causal inference step seems fragile, the core innovation of **trajectory-based reasoning** is highly transferable. We can steal this mechanism for AlgoEvo: instead of blind evolution, our planner agent should consume the optimization trajectory to dynamically swap operators or restart populations when stagnation is detected, effectively using the LLM as a process reward model.</details></td><td><small>Haoting Zhang et.al.</small></td><td><small>Columbia, UC Berkeley, Amazon</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2511.00685">pdf</a> / code</small></td></tr>
<tr><td><small>25/30</small></td><td><small>2025-10-31</small></td><td><details><summary><strong>ORGEval: Graph-Theoretic Evaluation of LLMs in Optimization Modeling</strong></summary>Wang et al. propose ORGEval, a framework that evaluates LLM-generated optimization models by converting them into bipartite graphs and using the Weisfeiler-Lehman (WL) test to detect isomorphism with a ground truth, rather than solving the instances. They prove that for 'symmetric decomposable' graphs, this method is guaranteed to detect equivalence correctly, achieving 100% consistency and running in seconds compared to hours for solver-based checks on hard MIPLIB instances. The critical takeaway is the shift from execution-based to **structural evaluation**: we can validate model logic via graph topology ($O(k(m+n)^2)$) without incurring the cost of solving NP-hard problems. This is immediately actionable for our OR benchmarking pipelines and could serve as a rapid 'pre-solve' filter in our evolutionary search loops to reject structurally invalid candidates instantly.</details></td><td><small>Zhuohan Wang et.al.</small></td><td><small>The Chinese University of Hong Kong, Shenzhen, Shenzhen Research Institute of Big Data, Shenzhen International Center for Industrial and Applied Mathematics, Shenzhen Loop Area Institute</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2510.27610">pdf</a> / code</small></td></tr>
<tr><td><small>25/30</small></td><td><small>2025-10-21</small></td><td><details><summary><strong>AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library</strong></summary>AlphaOPT introduces a 'Library Evolution' mechanism that iteratively refines the *applicability conditions* of cached optimization insights based on solver feedback, allowing it to learn from answers alone (no gold programs). On OOD benchmarks like OptiBench, it beats fine-tuned models (ORLM) by ~13% and shows consistent scaling with data size. **Key Takeaway:** The specific mechanism of diagnosing 'unretrieved' vs. 'negative' tasks to rewrite retrieval triggers is a transferable technique for our AlgoEvo memory; it solves the problem of heuristic misapplication in long-term search. We should implement this 'condition refinement' loop immediately to improve our multi-agent memory systems.</details></td><td><small>Minwei Kong et.al.</small></td><td><small>Massachusetts Institute of Technology, London School of Economics and Political Science, University of Florida, Northeastern University, Singapore Management University, Singapore-MIT Alliance for Research and Technology</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2510.18428">pdf</a> / <a href="https://github.com/Minw913/AlphaOPT">code</a></small></td></tr>
<tr><td><small>23/30</small></td><td><small>2025-10-19</small></td><td><details><summary><strong>SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search</strong></summary>SolverLLM frames optimization problem formulation as a hierarchical Monte Carlo Tree Search (MCTS), decomposing the task into six layers (variables, constraints, etc.) and using test-time compute to beat fine-tuned baselines like LLMOPT. The results appear robust, showing ~10% gains on complex datasets, though inference cost is high. **The critical takeaway for us is the 'Prompt Backpropagation' mechanism:** instead of just updating numerical values, they propagate textual error analysis from leaf nodes back up the tree to dynamically modify the prompts of parent nodes, effectively creating 'short-term memory' for the search. We should immediately test this technique in AlgoEvo to prevent the recurrence of failed code patterns during mutation steps. Additionally, their use of semantic entropy to down-weight uncertain rewards in MCTS is a practical solution to the noisy evaluation problem we face in process reward models.</details></td><td><small>Dong Li et.al.</small></td><td><small>NEC Labs America, Baylor University, University of Texas at Dallas, Augusta University, Southern Illinois University</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2510.16916">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2025-10-12</small></td><td><details><summary><strong>LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems</strong></summary>LinearizeLLM is a multi-agent framework that converts LaTeX nonlinear optimization problems into exact MILP formulations by detecting nonlinear terms and processing them bottom-up based on nesting depth. On 40 benchmark instances, it achieves 73% end-to-end success compared to <15% for one-shot LLMs and Pyomo baselines, demonstrating that structural decomposition is essential for handling complex nested terms. The key takeaway is the 'Structural Policy': rather than letting the LLM plan the reformulation order, they enforce a deterministic bottom-up traversal (linearizing children before parents). We should steal this hybrid approach‚Äîusing deterministic graph traversal to orchestrate LLM manipulation steps‚Äîto improve reliability in our symbolic modeling and EvoCut pipelines.</details></td><td><small>Paul-Niklas Ken Kandora et.al.</small></td><td><small>Karlsruhe Institute of Technology, Reutlingen University</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2510.15969">pdf</a> / code</small></td></tr>
<tr><td><small>25/30</small></td><td><small>2025-10-05</small></td><td><details><summary><strong>CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling</strong></summary>Tang et al. propose CALM, a framework that uses an expert 'Intervener' model to inject corrective hints into a small LRM's reasoning trace (e.g., forcing it to use Python instead of manual calculation), followed by SFT and RL (GRPO). Results are strong and verified: a 4B model matches DeepSeek-R1 (671B) on OR benchmarks, specifically fixing the 'Code Utilization Distrust' we see in our own agents. The key takeaway is the 'Intervener' loop: instead of discarding failed traces, they repair them with hints to create a 'golden' reasoning dataset that preserves the 'thinking' process while enforcing tool use. This is a direct, actionable method for improving our AlgoEvo agents' reliability in generating executable heuristics without massive human annotation.</details></td><td><small>Zhengyang Tang et.al.</small></td><td><small>Qwen Team, Alibaba Inc., The Chinese University of Hong Kong, Shenzhen, Southern University of Science and Technology, Shanghai University of Finance and Economics, Shenzhen Loop Area Institute (SLAI)</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2510.04204">pdf</a> / <a href="https://github.com/tangzhy/STORM">code</a></small></td></tr>
<tr><td><small>20/30</small></td><td><small>2025-10-03</small></td><td><details><summary><strong>Automated Constraint Specification for Job Scheduling by Regulating Generative Model with Domain-Specific Representation</strong></summary>This paper proposes a constraint-centric architecture that translates natural language manufacturing descriptions into Job Shop Scheduling (JSP) constraints by mediating through a learned Domain-Specific Language (DSL). Unlike standard prompting, they implement an automated DSL adaptation algorithm using non-parametric modeling (DPMM) and Expectation-Maximization to learn the syntax and semantics of the intermediate representation from data, which is then verified via a Pushdown Automaton. While the experiments rely on synthetic data augmented from standard benchmarks (a weakness), the methodology for **automatically deriving the intermediate representation** rather than hand-coding it is a transferable insight. We could steal this 'automated DSL design' approach to dynamically construct search spaces for AlgoEvo or to improve the robustness of NL-to-OR translation in OR-Bench.</details></td><td><small>Yu-Zhe Shi et.al.</small></td><td><small>Peking University, The Hong Kong University of Science and Technology, Huazhong University of Science and Technology, University of Science and Technology of China</small></td><td><small>IEEE Transactions on Automation Science and Engineering</small></td><td><small><a href="http://arxiv.org/abs/2510.02679">pdf</a> / <a href="null">code</a></small></td></tr>
<tr><td><small>21/30</small></td><td><small>2025-09-28</small></td><td><details><summary><strong>SAC-Opt: Semantic Anchors for Iterative Correction in Optimization Modeling</strong></summary>SAC-Opt introduces a verification loop where generated Gurobi code is back-translated into natural language ('semantic anchors') to check for alignment with the original problem description. Empirical results are strong, demonstrating a ~22% accuracy improvement on the ComplexLP dataset over OptiMUS-0.3 by catching logic errors that solver feedback misses. The primary takeaway is the utility of granular, constraint-level back-translation as a process reward signal, which we should adopt to improve the reliability of our automated modeling agents.</details></td><td><small>Yansen Zhang et.al.</small></td><td><small>Huawei Noah‚Äôs Ark Lab, Huawei‚Äôs Supply Chain Management Department, City University of Hong Kong</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2510.05115">pdf</a> / <a href="https://github.com/Forrest-Stone/SAC-Opt">code</a></small></td></tr>
<tr><td><small>26/30</small></td><td><small>2025-09-26</small></td><td><details><summary><strong>StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models</strong></summary>Zhou et al. propose StepORLM, a framework where an 8B policy and a **Generative Process Reward Model (GenPRM)** co-evolve. Unlike standard discriminative PRMs that score steps in isolation, their GenPRM generates a reasoning trace to evaluate the full trajectory's logic before assigning credit, addressing the interdependency of OR constraints. They align the policy using **Weighted DPO**, where preference weights are derived from the GenPRM's process scores. They claim to beat GPT-4o and DeepSeek-V3 on 6 OR benchmarks (e.g., NL4Opt, MAMO) with an 8B model. **Key Takeaway:** We should test **Generative PRMs** immediately for AlgoEvo; asking the critic to 'explain then score' (generative) rather than just 'score' (discriminative) likely fixes the credit assignment noise in our long-horizon search.</details></td><td><small>Chenyu Zhou et.al.</small></td><td><small>Shanghai Jiao Tong University</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2509.22558">pdf</a> / code</small></td></tr>
<tr><td><small>21/30</small></td><td><small>2025-09-26</small></td><td><details><summary><strong>OptiMind: Teaching LLMs to Think Like Optimization Experts</strong></summary>The authors fine-tune a 20B model for MILP formulation, but the critical contribution is a rigorous audit of standard benchmarks (IndustryOR, OptMATH), revealing that 30-50% of instances are flawed (missing data, wrong ground truth, infeasible). They introduce a 'class-based error analysis' where the model classifies a problem (e.g., TSP) and retrieves specific, expert-written hints to avoid common pitfalls, boosting accuracy by ~20%. **Takeaway:** We must immediately replace our benchmark versions with their cleaned sets for the OR-Bench project. Additionally, their library of 'error hints' per problem class is a high-value artifact we can scrape and inject into AlgoEvo's prompt templates to improve initial population quality.</details></td><td><small>Zeyi Chen et.al.</small></td><td><small>Microsoft Research, Stanford University, University of Washington</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2509.22979">pdf</a> / code</small></td></tr>
<tr><td><small>21/30</small></td><td><small>2025-09-24</small></td><td><details><summary><strong>DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs</strong></summary>Zhu et al. propose DAOpt, a framework for modeling optimization under uncertainty that integrates LLMs with the RSOME library to handle robust and stochastic formulations. Their experiments on a new dataset (OptU) convincingly demonstrate that standard LLM-generated deterministic models suffer from the 'optimizer's curse,' achieving only ~27% out-of-sample feasibility, whereas their robust approach achieves >70%. The critical takeaway for us is to **stop asking LLMs to derive mathematical duals or robust counterparts**; instead, we should train them to use high-level DSLs (like RSOME) that handle the duality internally. This is an immediate action item for our RobustMAS project to ensure generated solutions are actually executable in stochastic environments.</details></td><td><small>WenZhuo Zhu et.al.</small></td><td><small>Zhejiang University, University of Toronto, Peking University</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2511.11576">pdf</a> / <a href="https://anonymous.4open.science/r/LLM-for-data-driven-optimization-problems-9528">code</a></small></td></tr>
<tr><td><small>19/30</small></td><td><small>2025-09-10</small></td><td><details><summary><strong>Gala: Global LLM Agents for Text-to-Model Translation</strong></summary>GALA decomposes text-to-MiniZinc translation into a multi-agent system where specialized agents detect specific Constraint Programming global constraints (e.g., all_different, cumulative) before an assembler unifies them. Results on 110 TEXT2ZINC instances show a modest improvement over CoT (57% vs 52% execution rate with o3-mini), though the sample size is small and lacks statistical rigor. The key takeaway is the architectural shift from generic 'coder/reviewer' roles to 'primitive-specific' agents, which aligns LLM reasoning with the target formalism's structure. We should test this 'primitive-based decomposition' in our OR-Bench pipeline to see if it reduces hallucination of complex constraints better than our current methods.</details></td><td><small>Junyang Cai et.al.</small></td><td><small>University of Southern California, Brown University, Fidelity Investments</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2509.08970">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2025-08-20</small></td><td><details><summary><strong>Adaptively Robust LLM Inference Optimization under Prediction Uncertainty</strong></summary>Chen et al. propose $A_{min}$, an online scheduling algorithm for LLM inference that handles unknown output lengths by optimistically assuming the lower bound and evicting jobs (based on accumulated length) if memory overflows. They prove a logarithmic competitive ratio and show via simulations on LMSYS-Chat-1M that this approach nearly matches hindsight-optimal scheduling, vastly outperforming conservative upper-bound baselines. **Key Takeaway:** For our **GPUSched** project, we should abandon conservative memory reservation for output tokens; instead, implement an optimistic scheduler that oversubscribes memory and handles overflows via their ordered eviction policy, as the cost of restart is theoretically bounded and empirically negligible compared to the throughput gains.</details></td><td><small>Zixi Chen et.al.</small></td><td><small>Stanford University, Peking University, HKUST</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2508.14544">pdf</a> / code</small></td></tr>
<tr><td><small>28/30</small></td><td><small>2025-08-16</small></td><td><details><summary><strong>EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models</strong></summary>Yazdani et al. introduce EvoCut, an evolutionary framework where LLMs generate Python code for MILP cuts, filtered by a 'usefulness check' (does it cut the current LP relaxation?) and an 'empirical validity check' (does it preserve known integer optima?). They report 17-57% gap reductions on TSPLIB and JSSP compared to Gurobi defaults, backed by strong ablation studies on the evolutionary operators. **Key Takeaway:** The reliance on 'acceleration cuts'‚Äîconstraints verified empirically on small datasets rather than formally proven‚Äîbypasses the bottleneck of automated theorem proving while still delivering valid speedups. We should immediately adopt their 'LP separation' check as a cheap, high-signal reward for our own evolutionary search loops.</details></td><td><small>M. Yazdani et.al.</small></td><td><small>Huawei Technologies Canada, University of British Columbia, University of Toronto</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2508.11850">pdf</a> / <a href="https://github.com/milad1378yz/EvoCut">code</a></small></td></tr>
<tr><td><small>21/30</small></td><td><small>2025-08-10</small></td><td><details><summary><strong>CP-Agent: Agentic Constraint Programming</strong></summary>Szeider implements a standard ReAct agent with a persistent IPython kernel to iteratively generate and refine CPMpy models, claiming 100% accuracy on CP-Bench. However, this perfect score is achieved on a *modified* version of the benchmark where the author manually fixed 31 ambiguous problem statements and 19 ground-truth errors‚Äîmaking the '100%' result an artifact of dataset cleaning rather than pure model capability. The most actionable takeaways are the negative result for explicit 'task management' tools (which hurt performance on hard problems) and the effectiveness of a minimal (<50 lines) domain prompt over complex scaffolding. We should review their clarified benchmark for our OR-Bench work.</details></td><td><small>Stefan Szeider et.al.</small></td><td><small>TU Wien</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2508.07468">pdf</a> / <a href="https://github.com/szeider/agentic-python-coder">code</a></small></td></tr>
<tr><td><small>20/30</small></td><td><small>2025-08-05</small></td><td><details><summary><strong>Toward a Trustworthy Optimization Modeling Agent via Verifiable Synthetic Data Generation</strong></summary>Lima et al. introduce a pipeline to generate synthetic optimization datasets by starting with symbolic MILP instances (ground truth) and using LLMs to generate natural language descriptions, ensuring full verifiability. They fine-tune a small model (Granite 8B) that beats GPT-4 on 6/7 benchmarks, largely due to a 'majority vote' mechanism where the agent generates code in 5 different modeling languages (Pyomo, Gurobi, etc.) and checks for result consistency. **Takeaway:** We should steal the multi-language execution voting to boost robustness in our code generation agents. Furthermore, their reverse-generation (Symbolic $\to$ NL) strategy is the correct approach for generating infinite, error-free test cases for our OR-Bench work.</details></td><td><small>Vinicius Lima et.al.</small></td><td><small>IBM Research AI</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2508.03117">pdf</a> / code</small></td></tr>
<tr><td><small>20/30</small></td><td><small>2025-07-20</small></td><td><details><summary><strong>LLM-Enhanced Multi-Agent Reinforcement Learning with Expert Workflow for Real-Time P2P Energy Trading</strong></summary>This paper proposes a neurosymbolic MARL framework for P2P energy trading where LLMs generate CVXPY optimization models to act as 'experts' for RL agents to imitate via Wasserstein distance. They introduce a 'Differential Attention' mechanism in the critic that subtracts attention maps to filter noise, enabling scalability to 100 agents where standard baselines fail. **Takeaway:** We should steal the Differential Attention architecture for our multi-agent critics to handle irrelevant interactions in large-scale optimization. The workflow of using LLMs to write the *solver* (generating reliable synthetic data) rather than the *solution* is a transferable strategy for bootstrapping RL in our OR domains.</details></td><td><small>C. Lou et.al.</small></td><td><small>China Agricultural University, University of Glasgow, Guangdong University of Foreign Studies</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2507.14995">pdf</a> / <a href="https://github.com/jzk0806/P2P-llm-supplementary">code</a></small></td></tr>
<tr><td><small>23/30</small></td><td><small>2025-07-15</small></td><td><details><summary><strong>Auto-Formulating Dynamic Programming Problems with Large Language Models</strong></summary>Zhou et al. introduce DPLM, a 7B model fine-tuned to formulate Dynamic Programming models, achieving performance comparable to o1 on their new DP-Bench. Their key contribution is 'DualReflect,' a synthetic data pipeline that combines Forward Generation (Problem‚ÜíCode) for diversity with Backward Generation (Code‚ÜíProblem) for correctness. **Takeaway:** We should steal the Backward Generation approach for AlgoEvo: instead of relying on noisy forward generation, we can take valid heuristics/OR code (which we have in abundance) and reverse-engineer problem descriptions to create massive, verifiable synthetic datasets for fine-tuning our code generation models. The paper proves this method is superior for 'cold-starting' small models in data-scarce domains.</details></td><td><small>Chenyu Zhou et.al.</small></td><td><small>University of Chicago, Cornell University, Shanghai Jiao Tong University, Shanghai University of Finance and Economics, Cardinal Operations</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2507.11737">pdf</a> / code</small></td></tr>
<tr><td><small>25/30</small></td><td><small>2025-07-13</small></td><td><details><summary><strong>Fine-tuning Large Language Model for Automated Algorithm Design</strong></summary>Liu et al. introduce a fine-tuning pipeline for LLMs in automated algorithm design, utilizing a 'Diversity-Aware Rank-based' sampling strategy to construct DPO preference pairs from evolutionary search histories. By partitioning the population into ranked subsets and sampling pairs with a guaranteed quality gap (skipping adjacent tiers), they ensure training signals are both clear and diverse. Empirically, they show that a fine-tuned Llama-3.2-1B matches the performance of a base Llama-3.1-8B on ASP and CVRP tasks, effectively compressing the search capability into a much cheaper model. We should implement this sampling strategy to recycle our AlgoEvo run logs into specialized 'mutator' models, potentially allowing us to downscale to 1B/3B models for the inner search loop without losing quality.</details></td><td><small>Fei Liu et.al.</small></td><td><small>City University of Hong Kong</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2507.10614">pdf</a> / code</small></td></tr>
<tr><td><small>20/30</small></td><td><small>2025-06-09</small></td><td><details><summary><strong>REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models</strong></summary>Forni√©s-Tabuenca et al. propose REMoH, an LLM-driven evolutionary framework for multi-objective FJSSP that uses K-Means to cluster the population by objective performance before generating reflections. While their optimality gaps (~12%) trail behind state-of-the-art CP solvers (~1.5%), the ablation study confirms that their reflection mechanism significantly improves Pareto front diversity (Hypervolume). **The killer feature is the phenotypic clustering step:** instead of reflecting on a random or elitist subset, they group solutions by trade-offs (e.g., 'low makespan' vs 'balanced') to generate targeted prompts. We should implement this clustering-based context construction in AlgoEvo to improve diversity maintenance in multi-objective search without exploding token costs.</details></td><td><small>Diego Forni'es-Tabuenca et.al.</small></td><td><small>Vicomtech Foundation, University of the Basque Country, Universidad EAFIT, HiTZ Basque Center for Language Technology</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2506.07759">pdf</a> / code</small></td></tr>
<tr><td><small>21/30</small></td><td><small>2025-06-09</small></td><td><details><summary><strong>HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization</strong></summary>The authors introduce HeuriGym, a benchmark suite of 9 hard combinatorial optimization problems (including PDPTW, EDA scheduling, and routing) coupled with an agentic evaluation loop. Results are backed by extensive experiments showing that SOTA LLMs saturate at ~60% of expert performance and, significantly, that existing evolutionary frameworks (ReEvo, EoH) perform *worse* than simple prompting on these large-context tasks (300+ lines of code). The key takeaway is the failure mode of current evolutionary methods: they cannot handle the context fragmentation and feedback integration required for complex heuristic design. We should immediately adopt this benchmark to demonstrate AlgoEvo's superiority, as the current baselines are weak and the problem set aligns perfectly with our focus.</details></td><td><small>Hongzheng Chen et.al.</small></td><td><small>Cornell University, Harvard University, NVIDIA</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2506.07972">pdf</a> / <a href="https://github.com/cornell-zhang/heurigym">code</a></small></td></tr>
<tr><td><small>20/30</small></td><td><small>2025-06-06</small></td><td><details><summary><strong>DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems</strong></summary>This paper introduces DCP-Bench-Open, a benchmark of 164 discrete combinatorial problems, to evaluate LLMs on translating natural language into constraint models (CPMpy, MiniZinc, OR-Tools). The results are rigorous and highlight a critical failure mode: LLMs overfit to the specific data values in the prompt's example instance, causing a ~30% performance drop when evaluated on hidden instances (Multi-Instance Accuracy). Crucially for our pipeline design, they find that Retrieval-Augmented In-Context Learning (RAICL) is ineffective or harmful compared to simply including library documentation in the system prompt. We should adopt their 'Multi-Instance Accuracy' metric immediately for OR-Bench and switch any MiniZinc generation efforts to Python-based frameworks like CPMpy or OR-Tools, which LLMs handle much better.</details></td><td><small>Kostis Michailidis et.al.</small></td><td><small>KU Leuven, University of Western Macedonia</small></td><td><small>ECAI25)</small></td><td><small><a href="http://arxiv.org/abs/2506.06052">pdf</a> / <a href="https://github.com/kostis-init/CP-Bench">code</a></small></td></tr>
<tr><td><small>18/30</small></td><td><small>2025-05-27</small></td><td><details><summary><strong>DualSchool: How Reliable are LLMs for Optimization Education?</strong></summary>This paper evaluates LLMs on Primal-to-Dual Conversion (P2DC), introducing a 'Canonical Graph Edit Distance' (CGED) to verify structural correctness while ignoring benign differences like variable ordering or slack conventions. Results show that even strong LLMs often fail (<50% accuracy) and, crucially, that standard execution-based evaluation (checking objective values) produces frequent false positives by missing errors in redundant constraints. The primary takeaway for us is the CGED methodology: a robust way to score symbolic OR model generation that captures structural validity better than execution alone, which we could steal for our benchmarking and evolutionary search fitness functions.</details></td><td><small>Michael Klamkin et.al.</small></td><td><small>Georgia Institute of Technology</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2505.21775">pdf</a> / code</small></td></tr>
<tr><td><small>26/30</small></td><td><small>2025-05-17</small></td><td><details><summary><strong>Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling</strong></summary>Chen et al. introduce SIRL, a framework for training LLMs to generate optimization models using Reinforcement Learning with Verifiable Rewards (RLVR) and a novel 'Partial KL' surrogate objective. By removing the KL penalty from the reasoning (CoT) section while retaining it for the code generation section, they balance exploration with syntactic stability, achieving SOTA on OptMATH and IndustryOR against OpenAI-o3 and DeepSeek-R1. The critical takeaway for us is the Partial KL strategy: it allows the model to 'think' freely outside the reference distribution while adhering to strict coding standards‚Äîa technique we should immediately test in AlgoEvo. Furthermore, their method of parsing .lp files to extract structural features (variable counts, constraint types) for 'instance-enhanced self-consistency' provides a much richer signal than our current binary success/failure metrics.</details></td><td><small>Yitian Chen et.al.</small></td><td><small>Stanford University, Shanghai Jiao Tong University, The University of Hong Kong, Shanghai University of Finance and Economics, Cardinal Operations</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2505.11792">pdf</a> / <a href="https://github.com/Cardinal-Operations/SIRL">code</a></small></td></tr>
<tr><td><small>25/30</small></td><td><small>2025-05-15</small></td><td><details><summary><strong>Learning Virtual Machine Scheduling in Cloud Computing through Language Agents</strong></summary>Wu et al. introduce MiCo, a hierarchical framework that uses LLMs to evolve both a library of scenario-specific scheduling heuristics ('Options') and a master policy ('Composer') that dynamically switches between them based on system state. Tested on large-scale Huawei/Azure VM traces, it achieves a 96.9% competitive ratio against Gurobi, significantly outperforming Deep RL (SchedRL) by ~11% in dynamic scenarios. **Key Insight:** Instead of evolving a single robust heuristic (which often fails in non-stationary environments), explicitly evolve a *portfolio* of specialized heuristics and a separate *selector* function. This SMDP-based decomposition is a concrete architectural pattern we should adopt in AlgoEvo to handle diverse problem instances and non-stationary distributions effectively.</details></td><td><small>Jiehao Wu et.al.</small></td><td><small>Shanghai Jiao Tong University, East China Normal University, Tongji University</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2505.10117">pdf</a> / <a href="https://github.com/jettbrains/-L-">code</a></small></td></tr>
<tr><td><small>20/30</small></td><td><small>2025-05-10</small></td><td><details><summary><strong>RideAgent: An LLM-Enhanced Optimization Framework for Automated Taxi Fleet Operations</strong></summary>RideAgent employs an LLM to analyze a small set of historical optimal solutions, identifying and fixing 'low-sensitivity' decision variables to shrink the MIP search space before handing it to Gurobi. The results are empirically solid, showing a ~50% time reduction with <2.5% optimality gap, outperforming standard cutting plane baselines on NYC taxi data. **Key Takeaway:** We should adapt their 'Small-Sample Guided Optimization' strategy‚Äîspecifically using LLMs to infer *variable fixing constraints* from elite archive solutions‚Äîto accelerate the inner solvers in our AlgoEvo and EvoCut pipelines. This offers a concrete, data-driven way to prune search spaces that complements our current evolutionary approaches.</details></td><td><small>Xinyu Jiang et.al.</small></td><td><small>Tsinghua University, McGill University, George Washington University, JD Intelligent Cities Research, Beijing Technology and Business University</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2505.06608">pdf</a> / code</small></td></tr>
<tr><td><small>20/30</small></td><td><small>2025-05-07</small></td><td><details><summary><strong>Optimization Problem Solving Can Transition to Evolutionary Agentic Workflows</strong></summary>Li et al. propose an 'Evolutionary Agentic Workflow' that combines LLMs (DeepSeek) with evolutionary search to automate algorithm design, demonstrating it on VM scheduling and ADMM parameter tuning. The empirical rigor is low; they compare against weak baselines (BestFit for bin packing, a 2000-era heuristic for ADMM) and frame it as a position paper. However, the application of LLM-evolution to discover symbolic mathematical update rules (for ADMM step sizes) rather than just procedural code is a concrete use case we should consider for our EvoCut work. This serves primarily as competitor intelligence‚Äîvalidating our AlgoEvo direction‚Äîrather than a source of novel methodology.</details></td><td><small>Wenhao Li et.al.</small></td><td><small>University of Minnesota, Tongji University, East China Normal University</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2505.04354">pdf</a> / code</small></td></tr>
<tr><td><small>18/30</small></td><td><small>2025-05-02</small></td><td><details><summary><strong>CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code</strong></summary>CHORUS introduces a RAG framework for generating Gurobi code that replaces standard code retrieval with a metadata-based approach, indexing code examples by generated keywords and summaries rather than raw syntax. On the NL4Opt-Code benchmark, this allows open-source models like Llama-3-70B to match GPT-4 performance (improving accuracy from ~23% to ~57%). The key takeaway for us is the effectiveness of 'metadata-augmented indexing'‚Äîbridging the semantic gap between natural language problem descriptions and rigid solver APIs by retrieving based on functional descriptions rather than code embeddings. We should apply this metadata indexing strategy to the code retrieval modules in our OR-Bench and AlgoEvo agents.</details></td><td><small>Tasnim Ahmed et.al.</small></td><td><small>Queen's University</small></td><td><small>Learning and Intelligent Optimization</small></td><td><small><a href="http://arxiv.org/abs/2505.01485">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2025-04-23</small></td><td><details><summary><strong>OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents</strong></summary>OptimAI introduces a multi-agent framework for translating natural language to optimization models, featuring a 'plan-before-code' stage and a novel **UCB-based debug scheduler**. Instead of linearly debugging a single solution, it treats debugging as a multi-armed bandit problem, dynamically allocating compute to different solution strategies based on a 'Decider' score and exploration term. While the combinatorial results (TSP a280) are trivial, the bandit mechanism is a highly effective heuristic for search control. We should steal this UCB scheduling logic for AlgoEvo to prevent agents from wasting tokens debugging fundamentally flawed heuristics.</details></td><td><small>Raghav Thind et.al.</small></td><td><small>University of Maryland at College Park</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2504.16918">pdf</a> / code</small></td></tr>
<tr><td><small>20/30</small></td><td><small>2025-04-06</small></td><td><details><summary><strong>CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization</strong></summary>Sun et al. introduce CO-Bench, a suite of 36 diverse combinatorial optimization problems (packing, scheduling, routing) designed specifically to benchmark LLM agents in generating algorithms (code), not just solutions. They evaluate 9 frameworks (including FunSearch, ReEvo, AIDE), finding that FunSearch combined with reasoning models (o3-mini) yields the most robust performance, though agents still struggle significantly with strict feasibility constraints (valid solution rates often <60%). **Takeaway:** We should immediately integrate CO-Bench into our pipeline to benchmark AlgoEvo against ReEvo and FunSearch; this saves us months of data curation and provides a standardized metric to prove our method's superiority.</details></td><td><small>Weiwei Sun et.al.</small></td><td><small>Carnegie Mellon University</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2504.04310">pdf</a> / <a href="https://github.com/sunnweiwei/CO-Bench">code</a></small></td></tr>
<tr><td><small>15/30</small></td><td><small>2025-02-22</small></td><td><details><summary><strong>Text2Zinc: A Cross-Domain Dataset for Modeling Optimization and Satisfaction Problems in MiniZinc</strong></summary>Singirikonda et al. introduce TEXT2ZINC, a dataset of 110 Natural Language-to-MiniZinc problems, and benchmark GPT-4 using Vanilla, CoT, and Compositional prompting. Their results are poor (max ~25% solution accuracy), confirming that off-the-shelf LLMs struggle significantly with MiniZinc syntax and logical translation. Crucially, they attempt using Knowledge Graphs as an intermediate representation, but report that it actually *reduced* solution accuracy compared to basic CoT‚Äîa valuable negative result for our symbolic modeling work. We should examine their dataset for inclusion in OR-Bench, but their prompting methods are rudimentary baselines we should easily outperform.</details></td><td><small>Akash Singirikonda et.al.</small></td><td><small>Brown University, Fidelity Investments</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2503.10642">pdf</a> / code</small></td></tr>
<tr><td><small>23/30</small></td><td><small>2025-02-20</small></td><td><details><summary><strong>EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of Optimization Formulations</strong></summary>Zhai et al. propose EquivaMap, a framework that evaluates whether two MILP formulations are equivalent by using an LLM to discover a linear mapping between their decision variables, which is then rigorously verified by a solver. Unlike 'execution accuracy' (which fails on unit scaling) or 'canonical accuracy' (which fails on variable permutation), they achieve 100% accuracy on a new dataset of equivalent formulations including cuts and slack variables. The core insight is replacing output comparison with a 'propose-mapping-and-verify' loop, effectively using the LLM to construct a proof of equivalence. We must adopt this methodology for the OR-Bench evaluation pipeline immediately, as it eliminates the false negatives currently plaguing our generation benchmarks.</details></td><td><small>Haotian Zhai et.al.</small></td><td><small>Stanford University, The University of Texas at Austin</small></td><td><small>International Conference on Machine Learning</small></td><td><small><a href="http://arxiv.org/abs/2502.14760">pdf</a> / <a href="https://github.com/HumainLab/EquivaMap">code</a></small></td></tr>
<tr><td><small>24/30</small></td><td><small>2025-02-16</small></td><td><details><summary><strong>OptMATH: A Scalable Bidirectional Data Synthesis Framework for Optimization Modeling</strong></summary>The authors introduce OptMATH, a framework for generating synthetic optimization datasets by creating mathematical instances from seed generators, back-translating them to natural language via LLMs, and validating the pairs using a solver-based rejection sampling loop (checking if the re-generated model yields the same optimal value). They demonstrate that a Qwen-32B model fine-tuned on this data beats GPT-4 on NL4Opt and MAMO benchmarks. The critical takeaway is the **solver-verified reverse generation pipeline**: we should immediately steal this workflow to populate OR-Bench and generate diverse, verified training environments for AlgoEvo, replacing manual curation with scalable synthesis.</details></td><td><small>Hongliang Lu et.al.</small></td><td><small>Peking University</small></td><td><small>International Conference on Machine Learning</small></td><td><small><a href="http://arxiv.org/abs/2502.11102">pdf</a> / <a href="https://github.com/AuroraLHL/OptMATH">code</a></small></td></tr>
<tr><td><small>22/30</small></td><td><small>2024-11-26</small></td><td><details><summary><strong>BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical Modeling Problem Solving</strong></summary>Wang et al. propose BPP-Search, combining Beam Search, a Process Reward Model (PRM), and a final Pairwise Preference Model to generate LP/MIP models from natural language. While their new 'StructuredOR' dataset is small (38 test instances), it uniquely provides intermediate modeling labels (sets, parameters, variables) essential for training PRMs in this domain. The key takeaway is their finding that PRMs are effective for pruning but imprecise for final ranking; they solve this by adding a pairwise preference model at the leaf layer‚Äîa technique we should immediately steal to improve selection robustness in our MASPRM and evolutionary search pipelines. This is a competent execution of 'LLM + Search' applied specifically to our OR niche.</details></td><td><small>Teng Wang et.al.</small></td><td><small>Huawei, The University of Hong Kong</small></td><td><small>Annual Meeting of the Association for Computational Linguistics</small></td><td><small><a href="http://arxiv.org/abs/2411.17404">pdf</a> / <a href="https://github.com/LLM4OR/StructuredOR">code</a></small></td></tr>
<tr><td><small>25/30</small></td><td><small>2024-11-03</small></td><td><details><summary><strong>Autoformulation of Mathematical Optimization Models Using LLMs</strong></summary>Astorga et al. frame optimization modeling as a hierarchical Monte-Carlo Tree Search (MCTS) problem, using LLMs to generate components and‚Äîcrucially‚Äîemploying SMT solvers to prune mathematically equivalent branches (e.g., recognizing `x+y` and `y+x` as identical). They achieve SOTA results on NL4OPT and IndustryOR, outperforming fine-tuned models like ORLM while using significantly fewer samples than naive approaches. **Key Takeaway:** The integration of symbolic equivalence checking (SMT) to prune the search tree is a technique we should immediately steal; implementing this in AlgoEvo would allow us to discard functionally identical code/math mutants before expensive evaluation, directly addressing our sample efficiency bottleneck.</details></td><td><small>Nicol√°s Astorga et.al.</small></td><td><small>University of Cambridge, University of Hawaii at Manoa</small></td><td><small>ICML</small></td><td><small><a href="http://arxiv.org/abs/2411.01679">pdf</a> / <a href="https://github.com/jumpynitro/AutoFormulator">code</a></small></td></tr>
<tr><td><small>22/30</small></td><td><small>2024-10-29</small></td><td><details><summary><strong>Generalists vs. Specialists: Evaluating LLMs on Highly-Constrained Biophysical Sequence Optimization Tasks</strong></summary>The authors propose LLOME, a bilevel optimization framework that fine-tunes an LLM using 'MargE' (Margin-Aligned Expectation), a loss function that weights gradient updates by the magnitude of reward improvement (margin) rather than simple preference rankings. Results are rigorous and demonstrate that while DPO leads to generator collapse and infeasibility in constrained spaces, MargE maintains diversity and significantly improves sample efficiency, matching specialized solvers like LaMBO-2 on medium-difficulty tasks. The critical takeaway is that standard alignment methods (DPO/RLHF) are ill-suited for optimization because they discard information about *how much* better a solution is; MargE fixes this by satisfying the Strong Interpolation Criteria. We should immediately evaluate replacing the RL/update component in AlgoEvo with the MargE objective to improve the stability and quality of our evolved heuristics.</details></td><td><small>Angelica Chen et.al.</small></td><td><small>Genentech, New York University</small></td><td><small>International Conference on Machine Learning</small></td><td><small><a href="http://arxiv.org/abs/2410.22296">pdf</a> / code</small></td></tr>
<tr><td><small>18/30</small></td><td><small>2024-10-17</small></td><td><details><summary><strong>LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch</strong></summary>The authors fine-tune Qwen1.5-14B to translate natural language optimization problems into Pyomo code via a structured 'five-element' intermediate representation (Sets, Parameters, Variables, Objective, Constraints) and KTO alignment. They achieve ~11% accuracy gains over GPT-4o and ORLM on benchmarks like NL4Opt and IndustryOR, primarily by reducing formulation hallucinations through the structured intermediate step and preference optimization. For our OR-Bench work, the key takeaway is the concrete recipe for using KTO to align symbolic modeling agents, which appears more effective than standard SFT for enforcing constraints in smaller models. While not an evolutionary search paper, it provides a strong, locally runnable baseline for our OR modeling evaluations.</details></td><td><small>Caigao Jiang et.al.</small></td><td><small>Ant Group, East China Normal University, Nanjing University</small></td><td><small>International Conference on Learning Representations</small></td><td><small><a href="http://arxiv.org/abs/2410.13213">pdf</a> / <a href="https://github.com/caigaojiang/LLMOPT">code</a></small></td></tr>
<tr><td><small>20/30</small></td><td><small>2024-08-01</small></td><td><details><summary><strong>A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions</strong></summary>This survey and empirical audit reveals that standard optimization modeling benchmarks (NL4Opt, IndustryOR) suffer from critical error rates ranging from 16% to 54%, rendering prior leaderboards unreliable. The authors manually cleaned these datasets and re-evaluated methods, finding that Chain-of-Thought (CoT) often degrades performance compared to standard prompting, while fine-tuned models (ORLM) and multi-agent systems (Chain-of-Experts) perform best. The immediate takeaway is that we must adopt their cleaned datasets for our OR-Bench project; using the original open-source versions is no longer defensible. Additionally, the failure of CoT on these tasks suggests we should prioritize multi-agent or fine-tuned approaches for symbolic formulation tasks.</details></td><td><small>Ziyang Xiao et.al.</small></td><td><small>Zhejiang University, Huawei Noah‚Äôs Ark Lab, Singapore University of Social Sciences, Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security</small></td><td><small>International Joint Conference on Artificial Intelligence</small></td><td><small><a href="http://arxiv.org/abs/2508.10047">pdf</a> / <a href="https://llm4or.github.io/LLM4OR">code</a></small></td></tr>
<tr><td><small>23/30</small></td><td><small>2024-07-29</small></td><td><details><summary><strong>OptiMUS-0.3: Using Large Language Models to Model and Solve Optimization Problems at Scale</strong></summary>OptiMUS-0.3 is a modular multi-agent system that translates natural language into Gurobi code, utilizing a 'connection graph' to manage variable-constraint relationships in long contexts and specialized agents to detect solver-specific structures (SOS, indicators) or implement sifting. The results are rigorous, introducing a new hard benchmark (NLP4LP) where they outperform GPT-4o by ~40% and beat Chain-of-Experts. The most stealable insight is the 'Structure Detection Agent': instead of relying on the LLM to write generic constraints, we should explicitly prompt for and map high-level structures to efficient solver APIs (like SOS constraints) to improve performance in our EvoCut and AlgoEvo pipelines. This is a necessary read for the OR-Bench team.</details></td><td><small>Ali AhmadiTeshnizi et.al.</small></td><td><small></small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2407.19633">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2024-07-13</small></td><td><details><summary><strong>OptiBench Meets ReSocratic: Measure and Improve LLMs for Optimization Modeling</strong></summary>The authors propose OptiBench, a benchmark of 605 optimization problems (linear/nonlinear, tabular/text), and ReSocratic, a data synthesis method that generates formal models first and back-translates them into natural language questions. Results are strong: fine-tuning Llama-3-8B on their 29k synthetic samples improves accuracy from 13.6% to 51.1%, validating the data quality. **Key Takeaway:** The 'Reverse Socratic' synthesis pipeline (Formal Model ‚Üí Code ‚Üí NL Question) is the superior strategy for generating synthetic OR datasets because it guarantees solvability and ground truth by construction, unlike forward generation. We should steal this pipeline for generating robust test instances for OR-Bench and potentially for training our OR agents.</details></td><td><small>Zhicheng YANG et.al.</small></td><td><small>The Hong Kong University of Science and Technology, ETH Zurich, Huawei Noah‚Äôs Ark Lab, City University of Hong Kong, Sun Yat-sen University, MBZUAI, University of California Merced, Chongqing University</small></td><td><small>International Conference on Learning Representations</small></td><td><small><a href="http://arxiv.org/abs/2407.09887">pdf</a> / <a href="https://github.com/yangzhch6/ReSocratic">code</a></small></td></tr>
<tr><td><small>23/30</small></td><td><small>2024-03-02</small></td><td><details><summary><strong>LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation</strong></summary>LLaMoCo fine-tunes small LLMs (down to 350M) to generate executable Python optimization code by training on a synthetic dataset where the 'ground truth' is the empirically best-performing solver identified via exhaustive benchmarking. The results are compelling: the fine-tuned 350M model achieves ~85% normalized performance on benchmarks where GPT-4 Turbo only reaches ~14-30%, largely because the small model learns to select specialized evolutionary strategies (like BIPOP-CMA-ES) while GPT-4 defaults to generic gradient-based solvers. **Key Takeaway:** We can replace the expensive GPT-4 calls in our evolutionary search loop with a specialized, fine-tuned local model (CodeLlama-7B) trained on our historical search successes, significantly improving both sample efficiency and scalability. The paper's 'contrastive warm-up' strategy for aligning diverse problem descriptions is also a transferable technique for our problem encoding work.</details></td><td><small>Zeyuan Ma et.al.</small></td><td><small>Singapore Management University, Nanyang Technological University, South China University of Technology</small></td><td><small>IEEE Transactions on Evolutionary Computation</small></td><td><small><a href="http://arxiv.org/abs/2403.01131">pdf</a> / <a href="https://anonymous.4open.science/r/LLaMoCo-722A">code</a></small></td></tr>
<tr><td><small>23/30</small></td><td><small>2024-02-15</small></td><td><details><summary><strong>OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models</strong></summary>OptiMUS is a multi-agent framework for translating natural language into Gurobi code, achieving SOTA performance by using a 'Connection Graph' to map variables and parameters to specific constraints. This graph allows the agents to dynamically filter context and construct minimal prompts, enabling success on problems with long descriptions where baselines like Chain-of-Experts fail. They release NLP4LP, a hard benchmark of 67 complex instances, which we must immediately compare against our OR-Bench efforts. The **Connection Graph** is the key stealable insight: a structured dependency tracking mechanism that solves context pollution in iterative code generation, directly applicable to our AlgoEvo and HERMES memory designs.</details></td><td><small>Ali AhmadiTeshnizi et.al.</small></td><td><small>Stanford University</small></td><td><small>International Conference on Machine Learning</small></td><td><small><a href="http://arxiv.org/abs/2402.10172">pdf</a> / code</small></td></tr>
</tbody></table>

</details>

## OR for Generative AI

### üÜï Most Recent

| Date | Title | Authors | Affiliation | Links |
|------|-------|---------|-------------|-------|
| 2026-02-16 | <details><summary>**Efficient Multi-round LLM Inference over Disaggregated Serving**</summary>AMPD introduces a disaggregated serving framework tailored for multi-round LLM agents, utilizing an offline ILP solver to optimize resource allocation (TP/DP configurations) and an online adaptive routing mechanism to handle incremental prefill tasks. The results are strong, showing 67-340% improvements in SLO attainment over vLLM and NVIDIA Dynamo by dynamically routing incremental prefill to decode workers when slack exists. For our 'GPUSched' project, the key takeaway is the specific ILP formulation (Eq. 5) for partitioning prefill/decode resources under global GPU constraints, and the insight that multi-agent workflows create a unique 'incremental prefill' bottleneck that standard disaggregation handles poorly.</details> | Wenhao He et.al. | University of Cambridge, Peking University, Shanghai Jiao Tong University, Ant Group, Southeast University | [pdf](http://arxiv.org/abs/2602.14516) / code |
| 2026-02-09 | <details><summary>**OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval**</summary>Wang et al. formulate agentic tool-use planning not as a heuristic search (ReAct), but as a two-stage Mixed-Integer Programming (MIP) problem that solves for the mathematically optimal trajectory (tool selection + set operations) on training data. These 'golden trajectories' are then used as retrieved in-context demonstrations to steer the VLM at inference time, achieving SOTA on CIR benchmarks with only 10% of training data. **Key Takeaway:** We can steal this 'Offline MIP $\to$ Online ICL' paradigm. Instead of relying on noisy online RL or expensive evolutionary loops to guide our AlgoEvo agents, we can solve MIPs on training instances to generate optimal reasoning traces, effectively 'solving' the prompt engineering problem via OR.</details> | Teng Wang et.al. | Shanghai Jiao Tong University, OPPO | [pdf](http://arxiv.org/abs/2602.08603) / [code](https://anonymous.4open.science/r/OSCAR-3A55/README.md) |
| 2026-02-09 | <details><summary>**Predicting Future Utility: Global Combinatorial Optimization for Task-Agnostic KV Cache Eviction**</summary>Tang et al. formulate KV cache eviction not as a heuristic filtering task, but as a global combinatorial optimization problem maximizing 'Oracle Importance' (future utility) across all attention heads. They solve this NP-hard problem efficiently by applying Isotonic Regression (via PAVA) to create a convex surrogate of the eviction loss, enabling an optimal greedy allocation strategy that is deployed via an offline-computed lookup table. Results are strong: they achieve 80% cache reduction on LongBench and RULER with minimal degradation, significantly outperforming dynamic heuristics like AdaKV. **Key Takeaway:** The decomposition of error into 'ranking error' vs. 'allocation error'‚Äîand solving the latter via convex-hull relaxation‚Äîis a powerful OR pattern we should apply to our own resource allocation and scheduling problems.</details> | Ziyao Tang et.al. | Baidu, Fudan University | [pdf](http://arxiv.org/abs/2602.08585) / code |
| 2026-02-08 | <details><summary>**Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and Behavioral Rationality in Operations Research**</summary>Ao et al. introduce a framework for iterative OR model debugging that trains an 8B model using Group Relative Policy Optimization (GRPO) and a Process Reward Model (PRM) to outperform GPT-4o-mini. They utilize Gurobi's Irreducible Infeasible Subsystem (IIS) not just as text feedback, but as a dense reward signal (IIS size reduction) for the PRM, achieving a 95.3% recovery rate versus 86.2% for frontier APIs. **Key Takeaway:** We should steal their PRM construction method‚Äîspecifically using solver diagnostics (like IIS reduction or compiler error counts) as dense step-level rewards‚Äîand their 'faithfulness penalty' to prevent overfitting in our evolutionary search. This is a direct validation of RLVR (Reinforcement Learning with Verifiable Rewards) for OR, proving it superior to large-scale prompting.</details> | Ruicheng Ao et.al. | Massachusetts Institute of Technology, Alibaba Group | [pdf](http://arxiv.org/abs/2601.21008) / code |
| 2026-02-07 | <details><summary>**A Two-Layer Framework for Joint Online Configuration Selection and Admission Control**</summary>The authors introduce a 'switching-aware' primal-dual framework for joint configuration selection (e.g., quantization, parallelism) and admission control, demonstrating that dynamically mixing configurations allows for higher resource utilization than any single fixed configuration. Results are rigorous, backed by $\tilde{O}(\sqrt{T})$ regret bounds and experiments on Alibaba cluster traces where the method achieves ~97% competitive ratio (vs. ~85% for greedy). The key takeaway is the 'switching-aware fluid oracle' concept: our resource allocation models for LLM serving must optimize over the convex hull of configurations (mixing CPU-heavy and Mem-heavy setups) rather than searching for a single static optimum. We should adapt their saddle-point formulation for the GPUSched project to handle heterogeneous resource constraints more effectively.</details> | Owen Shen et.al. | Massachusetts Institute of Technology, Stanford University | [pdf](http://arxiv.org/abs/2602.07663) / code |

### ‚≠ê Best Papers

| Score | Date | Title | Authors | Affiliation | Links |
|-------|------|-------|---------|-------------|-------|
| 26/30 | 2026-02-08 | <details><summary>**Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and Behavioral Rationality in Operations Research**</summary>Ao et al. introduce a framework for iterative OR model debugging that trains an 8B model using Group Relative Policy Optimization (GRPO) and a Process Reward Model (PRM) to outperform GPT-4o-mini. They utilize Gurobi's Irreducible Infeasible Subsystem (IIS) not just as text feedback, but as a dense reward signal (IIS size reduction) for the PRM, achieving a 95.3% recovery rate versus 86.2% for frontier APIs. **Key Takeaway:** We should steal their PRM construction method‚Äîspecifically using solver diagnostics (like IIS reduction or compiler error counts) as dense step-level rewards‚Äîand their 'faithfulness penalty' to prevent overfitting in our evolutionary search. This is a direct validation of RLVR (Reinforcement Learning with Verifiable Rewards) for OR, proving it superior to large-scale prompting.</details> | Ruicheng Ao et.al. | Massachusetts Institute of Technology, Alibaba Group | [pdf](http://arxiv.org/abs/2601.21008) / code |
| 26/30 | 2026-01-15 | <details><summary>**Online Scheduling for LLM Inference with KV Cache Constraints**</summary>This paper formulates LLM inference scheduling as an Integer Program (IP) that explicitly models the linear memory growth of KV caches, and proposes a 'Memory Constrained Shortest First' (MC-SF) algorithm. The results are rigorous, showing MC-SF achieves near-optimal performance (within 5% of hindsight optimal) on synthetic data and significantly outperforms standard FCFS/threshold heuristics on real traces. The critical takeaway is the 'future feasibility check' (Eq. 5), which validates that a batch will *remain* within memory limits throughout the generation process based on predicted output lengths‚Äîa necessary deviation from standard static-size scheduling. This is foundational reading for our GPUSched project, providing both the exact IP baseline we need and a strong heuristic to benchmark against.</details> | Patrick Jaillet et.al. | Massachusetts Institute of Technology, Microsoft Research, HKUST | [pdf](http://arxiv.org/abs/2502.07115) / code |
| 26/30 | 2026-01-05 | <details><summary>**Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints**</summary>This paper formulates LLM inference as a multi-stage stochastic scheduling problem, introducing 'Nested WAIT'‚Äîa threshold-based algorithm that handles unknown output lengths by letting prompts classify themselves as they survive into deeper decode segments. Unlike heuristic baselines (vLLM, Sarathi), they provide rigorous asymptotic optimality proofs and high-probability bounds against memory overflow, validated on A100 simulations. The key takeaway is the 'nested segment' mechanism: instead of predicting job size, structure the queue so short jobs exit early and long jobs naturally migrate to lower-priority/protected tiers, effectively decoupling the memory risk. We should immediately evaluate this threshold logic for our GPUSched formulations, as it likely outperforms our current predictive or FCFS approaches for handling KV cache growth.</details> | Ruicheng Ao et.al. | Massachusetts Institute of Technology, Peking University, Alibaba Group | [pdf](http://arxiv.org/abs/2504.11320) / code |
| 25/30 | 2026-02-07 | <details><summary>**A Two-Layer Framework for Joint Online Configuration Selection and Admission Control**</summary>The authors introduce a 'switching-aware' primal-dual framework for joint configuration selection (e.g., quantization, parallelism) and admission control, demonstrating that dynamically mixing configurations allows for higher resource utilization than any single fixed configuration. Results are rigorous, backed by $\tilde{O}(\sqrt{T})$ regret bounds and experiments on Alibaba cluster traces where the method achieves ~97% competitive ratio (vs. ~85% for greedy). The key takeaway is the 'switching-aware fluid oracle' concept: our resource allocation models for LLM serving must optimize over the convex hull of configurations (mixing CPU-heavy and Mem-heavy setups) rather than searching for a single static optimum. We should adapt their saddle-point formulation for the GPUSched project to handle heterogeneous resource constraints more effectively.</details> | Owen Shen et.al. | Massachusetts Institute of Technology, Stanford University | [pdf](http://arxiv.org/abs/2602.07663) / code |
| 25/30 | 2026-02-03 | <details><summary>**3D-Learning: Diffusion-Augmented Distributionally Robust Decision-Focused Learning**</summary>Wen et al. introduce '3D-Learning,' a framework that replaces analytic ambiguity sets (Wasserstein/KL) in Distributionally Robust Optimization (DRO) with a diffusion model trained via PPO to generate worst-case scenarios. Applied to LLM resource provisioning, they claim ~40-50% regret reduction on OOD Azure traces compared to standard DRO, though training computational cost is high (6.8GB memory vs 35MB). The critical takeaway is the methodology of parameterizing the ambiguity set with a generative model to find 'realistic' adversarial edge cases that respect the data manifold, solving the support shift issue of KL-DRO. We should steal this 'generative ambiguity set' concept for benchmarking our heuristics in RobustMAS and AlgoEvo.</details> | Jiaqi Wen et.al. | University of Houston | [pdf](http://arxiv.org/abs/2602.02943) / [code](https://github.com/CIGLAB-Houston/3DLearning.git) |

### üî¨ Research Fronts

| Status | Front Name | Papers | Front Analysis |
|--------|-----------|--------|----------------|
| üìà Growing | Integer Linear Programming for Heterogeneous LLM Serving Resource Allocation | <details><summary>10</summary><ol><li><a href="http://arxiv.org/abs/2406.01566">Helix: Serving Large Language Models over Heterogeneous GPUs and Network via Max-Flow</a> <sub>(25)</sub></li><li><a href="http://arxiv.org/abs/2502.14617">SageServe: Optimizing LLM Serving on Cloud Data Centers with Forecast Aware Auto-Scaling</a> <sub>(24)</sub></li><li><a href="http://arxiv.org/abs/2602.14516">Efficient Multi-round LLM Inference over Disaggregated Serving</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2506.04203">Cascadia: An Efficient Cascade Serving System for Large Language Models</a> <sub>(22)</sub></li><li><a href="http://arxiv.org/abs/2507.10259">Temporal-Aware GPU Resource Allocation for Distributed LLM Inference via Reinforcement Learning</a> <sub>(21)</sub></li><li><a href="http://arxiv.org/abs/2512.21884">Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models</a> <sub>(21)</sub></li><li><a href="http://arxiv.org/abs/2502.00722">Demystifying Cost-Efficiency in LLM Serving over Heterogeneous GPUs</a> <sub>(20)</sub></li><li><a href="http://arxiv.org/abs/2508.09229">Cluster Topology-Driven Placement of Experts Reduces Network Traffic in MoE Inference</a> <sub>(19)</sub></li><li><a href="http://arxiv.org/abs/2510.11331">Efficient LLM Inference over Heterogeneous Edge Networks with Speculative Decoding</a> <sub>(18)</sub></li><li><a href="http://arxiv.org/abs/2505.23970">Cache Your Prompt When It's Green: Carbon-Aware Caching for Large Language Model Serving</a> <sub>(18)</sub></li></ol></details> | <details><summary>View analysis</summary>This research front is unified by the application of Integer Linear Programming (ILP) and Mixed-Integer Linear Programming (MILP) to optimize various aspects of Large Language Model (LLM) serving. The core theme revolves around efficient resource allocation, scheduling, and deployment strategies for LLM inference, particularly addressing challenges posed by heterogeneous GPU clusters, disaggregated serving architectures, and geographically distributed systems. Specific problem domains include multi-round inference, Mixture-of-Expert (MoE) model placement, cascade serving, and carbon-aware caching.  Key contributions include Dynamo's ILP-based offline deployment for multi-round inference, achieving up to 340% SLO attainment improvement (Paper 1). Jiang et al. (Paper 3) demonstrated ~25% throughput gains over SOTA systems like Helix by co-optimizing GPU composition and parallelism using MILP for heterogeneous clouds. CASCADIA (Paper 5) introduced a bi-level optimization (MILP for deployment, Chebyshev for routing) for cascade serving, yielding 2.3x average throughput gains. SageServe (Paper 6) achieved 25% GPU-hours savings and $2.5M/month savings by coupling ILP with ARIMA forecasting for auto-scaling. Helix (Paper 10) formulated distributed LLM serving as a max-flow MILP, achieving up to 3.3x throughput gains on mixed GPU clusters by dynamic per-request routing. Other notable works include ILP for MoE expert placement (Paper 4), carbon-aware KV cache management (Paper 9), and hybrid RL-Optimal Transport for temporal-aware GPU allocation (Paper 7).  This front is rapidly maturing, driven by the increasing complexity and scale of LLM deployments. The consistent success of ILP/MILP in achieving significant performance, cost, and energy efficiency gains across diverse LLM serving scenarios indicates a strong and active research trajectory. Future work will likely focus on developing more scalable and dynamic OR solutions, integrating these with advanced LLM-specific optimizations, and expanding to multi-objective and real-time adaptive systems.</details> |
| üöÄ Emerging | Optimizing LLM Inference and Automated OR Problem Generation | <details><summary>7</summary><ol><li><a href="http://arxiv.org/abs/2504.11320">Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints</a> <sub>(26)</sub></li><li><a href="http://arxiv.org/abs/2502.07115">Online Scheduling for LLM Inference with KV Cache Constraints</a> <sub>(26)</sub></li><li><a href="http://arxiv.org/abs/2511.09092">OR-R1: Automating Modeling and Solving of Operations Research Optimization Problem via Test-Time Reinforcement Learning</a> <sub>(25)</sub></li><li><a href="http://arxiv.org/abs/2504.07347">Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents</a> <sub>(25)</sub></li><li><a href="http://arxiv.org/abs/2510.17015">Justitia: Fair and Efficient Scheduling for LLM Applications</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2512.16134">Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference</a> <sub>(22)</sub></li><li><a href="http://arxiv.org/abs/2405.17743">ORLM: A Customizable Framework in Training Large Models for Automated Optimization Modeling</a> <sub>(19)</sub></li></ol></details> | <details><summary>View analysis</summary>This research front explores the application of advanced Operations Research (OR) principles and AI techniques to two critical challenges within the LLM ecosystem: optimizing LLM inference serving and automating the generation of OR models. For LLM inference, papers focus on sophisticated scheduling algorithms to manage GPU resources, KV cache memory, and request batching under various constraints. Concurrently, other works leverage large language models themselves to automatically formulate and solve complex OR problems, bridging the gap between natural language problem descriptions and executable optimization models.  Key contributions in LLM inference scheduling include Nested WAIT (Paper 1) for multi-stage online scheduling, achieving superior throughput and reduced latency on vLLM/Sarathi. Staggered Batch Scheduling (SBS) (Paper 2) for DP+EP architectures reduced Time-to-First-Token by 30-40% and increased throughput by 15-20% on DeepSeek-V3. Memory Constrained Shortest First (MC-SF) (Paper 4) achieved near-optimal latency (within 5% of hindsight optimal) for KV cache-aware online scheduling. Justitia (Paper 6) introduced a virtual-time based fair scheduler, reducing average job completion time by ~60%. In automated OR modeling, OR-R1 (Paper 3) integrated SFT and Test-Time Group Relative Policy Optimization (TGRPO) to improve modeling accuracy by +4.2% over ORLM. ORLM (Paper 7) and the OR-Instruct framework fine-tuned LLMs with synthetic data, outperforming GPT-4 by up to 38.4% on benchmarks like NL4OPT and IndustryOR.  This front is emerging, with significant activity in both LLM inference optimization and automated OR modeling. The trajectory suggests continued innovation in developing more robust and adaptive scheduling policies for increasingly complex LLM architectures and deployment scenarios. For automated OR, the next papers will likely focus on enhancing LLM capabilities for solution ranking, handling more diverse and complex problem types, and integrating multi-agent collaboration for sophisticated problem-solving.</details> |
| üöÄ Emerging | Optimal LLM Inference Scheduling with Queueing Theory and MIP | <details><summary>6</summary><ol><li><a href="http://arxiv.org/abs/2602.02987">Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control</a> <sub>(24)</sub></li><li><a href="http://arxiv.org/abs/2508.01002">Optimal Scheduling Algorithms for LLM Inference: Theory and Practice</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2502.15763">Hybrid Offline-online Scheduling Method for Large Language Model Inference Optimization</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2503.09357">Automatic Operator-level Parallelism Planning for Distributed Deep Learning -- A Mixed-Integer Programming Approach</a> <sub>(22)</sub></li><li><a href="http://arxiv.org/abs/2512.15705">Dynamic Rebatching for Efficient Early-Exit Inference with DREX</a> <sub>(19)</sub></li><li><a href="http://arxiv.org/abs/2512.21487">Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism</a> <sub>(18)</sub></li></ol></details> | <details><summary>View analysis</summary>This research front unifies advanced Operations Research techniques, specifically many-server queueing network models, Mixed-Integer Programming (MIP), and stochastic control, to achieve optimal or near-optimal LLM inference scheduling and resource allocation. Key themes include managing prefill-decode contention, optimizing disaggregated expert parallelism in Mixture-of-Experts (MoE) models, and automatic operator-level parallelism planning for distributed deep learning.  Key contributions include Lin et al.'s (2026) 'Gate-and-Route' policy derived from a fluid LP, demonstrating ~30% lower revenue loss than OPT on Dolly-15k. She et al. (2025) used MIP for operator-level parallelism, reducing pipeline bubbles by 50% for DeepSeek V3. Bari et al. (2025) introduced RAD and SLAI schedulers, achieving a 53% reduction in median TTFT and 26% capacity increase over Sarathi-Serve on Mistral-7B. Pang et al. (2025) combined offline bin-packing with an online Lagrangian heuristic, improving utilization by 8.86% over vLLM FCFS on LLaMA-65B. FinDEP (2025) optimized MoE inference with fine-grained scheduling, yielding up to 1.61x throughput improvement on Qwen3-235B, while DREX (2025) showed 2-12% throughput gains on Llama-EE-70B using dynamic rebatching for early-exit LLMs.  This front is emerging, characterized by a strong trend towards rigorous mathematical modeling to solve complex LLM serving challenges, moving beyond heuristic-driven approaches. The next papers will likely focus on relaxing simplifying assumptions (e.g., exponential service times), integrating stochastic programming for uncertainty, and extending these optimal control strategies to more heterogeneous and dynamic LLM architectures, such as those with speculative decoding or Mixture-of-Depths.</details> |
| üìà Growing | OR-Driven Adaptive Resource Allocation for LLM Inference and Test-Time Search | <details><summary>6</summary><ol><li><a href="http://arxiv.org/abs/2506.15707">Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling</a> <sub>(25)</sub></li><li><a href="http://arxiv.org/abs/2509.21091">Best-of-$\infty$ -- Asymptotic Performance of Test-Time Compute</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2502.13575">ETS: Efficient Tree Search for Inference-Time Scaling</a> <sub>(23)</sub></li><li><a href="http://arxiv.org/abs/2602.00509">PROBE: Co-Balancing Computation and Communication in MoE Inference via Real-Time Predictive Prefetching</a> <sub>(21)</sub></li><li><a href="http://arxiv.org/abs/2512.09963">GoodSpeed: Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference</a> <sub>(18)</sub></li><li><a href="http://arxiv.org/abs/2504.08930">VectorLiteRAG: Latency-Aware and Fine-Grained Resource Partitioning for Efficient RAG</a> <sub>(18)</sub></li></ol></details> | <details><summary>View analysis</summary>This research front focuses on applying advanced Operations Research techniques to optimize various aspects of Large Language Model (LLM) inference and serving. Key approaches include Bayesian adaptive stopping for efficient ensemble evaluation, gradient-based scheduling in GoodSpeed for distributed speculative decoding, PROBE's predictive Lookahead Pipelining for MoE inference, and ETS's Integer Linear Programming for KV cache optimization in tree search. Additionally, DORA employs embedding-based resource allocation for test-time search, and VectorLiteRAG uses analytical modeling for RAG serving.  Contributions include Bayesian adaptive sampling (2-5x compute reduction on AIME/GPQA) and MILP for LLM ensembles. GoodSpeed demonstrates gradient-based scheduling for distributed speculative decoding, achieving fair goodput on H100/L4 clusters. PROBE's Lookahead Pipelining yields a 1.3x speedup for Qwen3-MoE-235B inference, while ETS leverages ILP to achieve 1.8x KV cache reduction and 1.4x throughput increase on MATH500 compared to REBASE. VectorLiteRAG provides 1.5x throughput gains for RAG serving on H100/L40S, and DORA achieves state-of-the-art accuracy on MATH500 with 3.5x fewer FLOPs by optimizing test-time search.  This front is rapidly maturing, characterized by the increasing sophistication of OR methods integrated directly into LLM serving and reasoning pipelines. The trajectory points towards more tightly coupled, real-time optimization, where OR solvers dynamically adapt resource allocation and search strategies. Future work will likely focus on developing unified frameworks that combine predictive modeling, adaptive sampling, and advanced combinatorial optimization to handle the stochastic and dynamic nature of LLM workloads across diverse hardware architectures and reasoning tasks.</details> |
| üìà Growing | Convex Optimization and Game Theory for Robust Multi-Objective LLM Alignment | <details><summary>3</summary><ol><li><a href="http://arxiv.org/abs/2503.08796">Robust Multi-Objective Controlled Decoding of Large Language Models</a> <sub>(22)</sub></li><li><a href="http://arxiv.org/abs/2508.07768">Pareto Multi-Objective Alignment for Language Models</a> <sub>(18)</sub></li><li><a href="http://arxiv.org/abs/2510.09330">Safety Game: Balancing Safe and Informative Conversations with Blackbox Agentic AI using LP Solvers</a> <sub>(18)</sub></li></ol></details> | <details><summary>View analysis</summary>This research front unifies recent advancements in applying Operations Research techniques, specifically convex optimization and game theory, to the challenging problem of multi-objective alignment for Large Language Models (LLMs). Papers introduce frameworks like PAMA, Safety Game, and Robust Multi-Objective Decoding (RMOD) to manage conflicting objectives such as harmlessness, helpfulness, sentiment, and length control, often at inference time.  Key contributions include the PAMA algorithm, which transforms multi-objective RLHF into an O(n) convex optimization problem with a closed-form solution, outperforming MORLHF and MGDA-UB on LLaMA-2 7B for harmlessness. The Safety Game formulates black-box LLM agent alignment as a zero-sum game solvable by an LP solver at inference, achieving up to two-fold accuracy improvement on SafetyBench. RMOD introduces a maximin two-player game for robust multi-objective decoding, solving a convex optimization problem at each step to maximize worst-case value, outperforming MO-DPO and scalarized baselines by +1.2% WCWR on Anthropic HH.  This front is rapidly growing, demonstrating the power of OR principles to bring robustness and efficiency to LLM alignment. The trajectory indicates a strong focus on mathematically grounded, inference-time control mechanisms. Future work will likely focus on extending these frameworks to more complex, dynamic, and multi-agent scenarios, improving their scalability to a greater number of objectives, and integrating these control mechanisms into broader agentic architectures.</details> |
| üöÄ Emerging | Linear Programming for MoE LLM Mixed-Precision Quantization and Pruning | <details><summary>2</summary><ol><li><a href="http://arxiv.org/abs/2510.10962">MC#: Mixture Compressor for Mixture-of-Experts Large Models</a> <sub>(20)</sub></li><li><a href="http://arxiv.org/abs/2410.06270">Mixture Compressor for Mixture-of-Experts LLMs Gains More</a> <sub>(18)</sub></li></ol></details> | <details><summary>View analysis</summary>This research front centers on the Mixture Compressor (MC and MC#) frameworks, which leverage Operations Research techniques, specifically Linear Programming, for the extreme compression of Mixture-of-Experts (MoE) Large Language Models. The core theme involves optimizing mixed-precision quantization and dynamic expert pruning to significantly reduce model size while preserving performance, enabling deployment on resource-constrained hardware.  The initial paper, "Mixture Compressor for Mixture-of-Experts LLMs Gains More" (Huang et al., 2025-02), introduces a hybrid post-training quantization and dynamic pruning approach. It uses Linear Programming to optimally allocate mixed bit-widths (1-3 bits) to experts based on activation frequency, achieving strong empirical results such as compressing Mixtral 8x7b to ~16GB with only a ~4% drop in zero-shot accuracy, outperforming uniform GPTQ. Building on this, "MC#: Mixture Compressor for Mixture-of-Experts Large Models" (Huang et al., 2025-10) refines the framework by combining Pre-Loading Mixed-Precision Quantization (PMQ) via Linear Programming with Online Top-any Pruning (OTP) using Gumbel-Softmax sampling. This unified approach achieves a 6.2x weight reduction on DeepSeek-VL2 with less than 2% accuracy loss, further demonstrating the efficacy of OR-driven compression.  This front is currently emerging, with two closely related papers published in 2025, the second building directly on the first. The trajectory suggests a continued focus on refining these OR-driven compression techniques. The likely next paper would explore the adaptation of these methods to new model architectures or more challenging, complex reasoning tasks, while also addressing hardware-specific optimizations.</details> |

<details>
<summary>üìã Full list (94 papers, sorted by date)</summary>

<table><colgroup><col width="5%"><col width="7%"><col width="34%"><col width="13%"><col width="16%"><col width="5%"><col width="20%"></colgroup>
<thead><tr><th>Score</th><th>Date</th><th>Title</th><th>Authors</th><th>Affiliation</th><th>Venue</th><th>Links</th></tr></thead>
<tbody>
<tr><td><small>23/30</small></td><td><small>2026-02-16</small></td><td><details><summary><strong>Efficient Multi-round LLM Inference over Disaggregated Serving</strong></summary>AMPD introduces a disaggregated serving framework tailored for multi-round LLM agents, utilizing an offline ILP solver to optimize resource allocation (TP/DP configurations) and an online adaptive routing mechanism to handle incremental prefill tasks. The results are strong, showing 67-340% improvements in SLO attainment over vLLM and NVIDIA Dynamo by dynamically routing incremental prefill to decode workers when slack exists. For our 'GPUSched' project, the key takeaway is the specific ILP formulation (Eq. 5) for partitioning prefill/decode resources under global GPU constraints, and the insight that multi-agent workflows create a unique 'incremental prefill' bottleneck that standard disaggregation handles poorly.</details></td><td><small>Wenhao He et.al.</small></td><td><small>University of Cambridge, Peking University, Shanghai Jiao Tong University, Ant Group, Southeast University</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.14516">pdf</a> / code</small></td></tr>
<tr><td><small>21/30</small></td><td><small>2026-02-09</small></td><td><details><summary><strong>OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval</strong></summary>Wang et al. formulate agentic tool-use planning not as a heuristic search (ReAct), but as a two-stage Mixed-Integer Programming (MIP) problem that solves for the mathematically optimal trajectory (tool selection + set operations) on training data. These 'golden trajectories' are then used as retrieved in-context demonstrations to steer the VLM at inference time, achieving SOTA on CIR benchmarks with only 10% of training data. **Key Takeaway:** We can steal this 'Offline MIP $\to$ Online ICL' paradigm. Instead of relying on noisy online RL or expensive evolutionary loops to guide our AlgoEvo agents, we can solve MIPs on training instances to generate optimal reasoning traces, effectively 'solving' the prompt engineering problem via OR.</details></td><td><small>Teng Wang et.al.</small></td><td><small>Shanghai Jiao Tong University, OPPO</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.08603">pdf</a> / <a href="https://anonymous.4open.science/r/OSCAR-3A55/README.md">code</a></small></td></tr>
<tr><td><small>24/30</small></td><td><small>2026-02-09</small></td><td><details><summary><strong>Predicting Future Utility: Global Combinatorial Optimization for Task-Agnostic KV Cache Eviction</strong></summary>Tang et al. formulate KV cache eviction not as a heuristic filtering task, but as a global combinatorial optimization problem maximizing 'Oracle Importance' (future utility) across all attention heads. They solve this NP-hard problem efficiently by applying Isotonic Regression (via PAVA) to create a convex surrogate of the eviction loss, enabling an optimal greedy allocation strategy that is deployed via an offline-computed lookup table. Results are strong: they achieve 80% cache reduction on LongBench and RULER with minimal degradation, significantly outperforming dynamic heuristics like AdaKV. **Key Takeaway:** The decomposition of error into 'ranking error' vs. 'allocation error'‚Äîand solving the latter via convex-hull relaxation‚Äîis a powerful OR pattern we should apply to our own resource allocation and scheduling problems.</details></td><td><small>Ziyao Tang et.al.</small></td><td><small>Baidu, Fudan University</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.08585">pdf</a> / code</small></td></tr>
<tr><td><small>26/30</small></td><td><small>2026-02-08</small></td><td><details><summary><strong>Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and Behavioral Rationality in Operations Research</strong></summary>Ao et al. introduce a framework for iterative OR model debugging that trains an 8B model using Group Relative Policy Optimization (GRPO) and a Process Reward Model (PRM) to outperform GPT-4o-mini. They utilize Gurobi's Irreducible Infeasible Subsystem (IIS) not just as text feedback, but as a dense reward signal (IIS size reduction) for the PRM, achieving a 95.3% recovery rate versus 86.2% for frontier APIs. **Key Takeaway:** We should steal their PRM construction method‚Äîspecifically using solver diagnostics (like IIS reduction or compiler error counts) as dense step-level rewards‚Äîand their 'faithfulness penalty' to prevent overfitting in our evolutionary search. This is a direct validation of RLVR (Reinforcement Learning with Verifiable Rewards) for OR, proving it superior to large-scale prompting.</details></td><td><small>Ruicheng Ao et.al.</small></td><td><small>Massachusetts Institute of Technology, Alibaba Group</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2601.21008">pdf</a> / code</small></td></tr>
<tr><td><small>25/30</small></td><td><small>2026-02-07</small></td><td><details><summary><strong>A Two-Layer Framework for Joint Online Configuration Selection and Admission Control</strong></summary>The authors introduce a 'switching-aware' primal-dual framework for joint configuration selection (e.g., quantization, parallelism) and admission control, demonstrating that dynamically mixing configurations allows for higher resource utilization than any single fixed configuration. Results are rigorous, backed by $\tilde{O}(\sqrt{T})$ regret bounds and experiments on Alibaba cluster traces where the method achieves ~97% competitive ratio (vs. ~85% for greedy). The key takeaway is the 'switching-aware fluid oracle' concept: our resource allocation models for LLM serving must optimize over the convex hull of configurations (mixing CPU-heavy and Mem-heavy setups) rather than searching for a single static optimum. We should adapt their saddle-point formulation for the GPUSched project to handle heterogeneous resource constraints more effectively.</details></td><td><small>Owen Shen et.al.</small></td><td><small>Massachusetts Institute of Technology, Stanford University</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.07663">pdf</a> / code</small></td></tr>
<tr><td><small>19/30</small></td><td><small>2026-02-06</small></td><td><details><summary><strong>tLoRA: Efficient Multi-LoRA Training with Elastic Shared Super-Models</strong></summary>tLoRA optimizes multi-tenant LoRA training by fusing heterogeneous adapters into a 'Shared Super-Model' and employing an online scheduler that groups jobs based on residual GPU capacity and urgency. They report 1.2‚Äì1.8x throughput improvements and ~5x faster job completion times on A100 clusters compared to mLoRA, backed by realistic trace-driven experiments. For our GPUSched and resource allocation work, their hierarchical incremental grouping strategy serves as the state-of-the-art heuristic baseline we must outperform; additionally, their adaptive nano-batching (AIMD controller) is a transferable engineering trick for overlapping communication in distributed LLM workloads.</details></td><td><small>Kevin Li et.al.</small></td><td><small>University of Illinois Urbana-Champaign</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.07263">pdf</a> / code</small></td></tr>
<tr><td><small>‚Äî</small></td><td><small>2026-02-04</small></td><td><strong>MIRROR: A Multi-Agent Framework with Iterative Adaptive Revision and Hierarchical Retrieval for Optimization Modeling in Operations Research</strong></td><td><small>Yifan Shi et.al.</small></td><td><small></small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.03318">pdf</a> / code</small></td></tr>
<tr><td><small>24/30</small></td><td><small>2026-02-04</small></td><td><details><summary><strong>MaMa: A Game-Theoretic Approach for Designing Safe Agentic Systems</strong></summary>MaMa automates the design of multi-agent systems by formulating the problem as a Stackelberg Security Game: a Meta-Agent evolves system architectures (tools, communication graphs) while a Meta-Adversary iteratively optimizes worst-case agent compromises to break them. Empirical results on the BAD-ACTS benchmark show this adversarial co-evolution reduces attack success rates from ~50% (static baselines) to ~15-25% without degrading task quality. The critical takeaway is the implementation of an **adversarial co-evolution loop** within the architecture search‚Äîoptimizing the 'threat' alongside the 'solution'‚Äîwhich directly addresses the robustness objectives in our RobustMAS project. We should implement this 'Meta-Adversary' concept to stress-test our evolved algorithms during the search phase rather than post-hoc.</details></td><td><small>Jonathan N√∂ther et.al.</small></td><td><small>Max Planck Institute for Software Systems</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.04431">pdf</a> / code</small></td></tr>
<tr><td><small>24/30</small></td><td><small>2026-02-03</small></td><td><details><summary><strong>Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control</strong></summary>Lin et al. formulate LLM inference scheduling as a multiclass many-server queueing network, deriving a 'Gate-and-Route' policy from a steady-state fluid LP that explicitly manages prefill-decode contention. Calibrated on A100s, their approach proves that separating prefill admission (via occupancy tracking) from decode routing (work-conserving) eliminates decode backlogs and maximizes revenue. **Key Takeaway:** The decomposition of scheduling into 'static planning' (solving an LP for target occupancies) and 'dynamic control' (a simple gate tracking those targets) is a scalable alternative to online combinatorial optimization for your GPUSched work. It mathematically formalizes the intuition that prefill is the bottleneck and decode should be kept strictly critical but not backlogged.</details></td><td><small>Ruihan Lin et.al.</small></td><td><small>The Hong Kong University of Science and Technology</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.02987">pdf</a> / code</small></td></tr>
<tr><td><small>25/30</small></td><td><small>2026-02-03</small></td><td><details><summary><strong>3D-Learning: Diffusion-Augmented Distributionally Robust Decision-Focused Learning</strong></summary>Wen et al. introduce '3D-Learning,' a framework that replaces analytic ambiguity sets (Wasserstein/KL) in Distributionally Robust Optimization (DRO) with a diffusion model trained via PPO to generate worst-case scenarios. Applied to LLM resource provisioning, they claim ~40-50% regret reduction on OOD Azure traces compared to standard DRO, though training computational cost is high (6.8GB memory vs 35MB). The critical takeaway is the methodology of parameterizing the ambiguity set with a generative model to find 'realistic' adversarial edge cases that respect the data manifold, solving the support shift issue of KL-DRO. We should steal this 'generative ambiguity set' concept for benchmarking our heuristics in RobustMAS and AlgoEvo.</details></td><td><small>Jiaqi Wen et.al.</small></td><td><small>University of Houston</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.02943">pdf</a> / <a href="https://github.com/CIGLAB-Houston/3DLearning.git">code</a></small></td></tr>
<tr><td><small>21/30</small></td><td><small>2026-02-03</small></td><td><details><summary><strong>PROBE: Co-Balancing Computation and Communication in MoE Inference via Real-Time Predictive Prefetching</strong></summary>PROBE optimizes MoE inference by using a distilled MLP to predict next-layer expert activation, enabling proactive load balancing and weight prefetching hidden behind the current layer's computation. The results are strong (1.3x speedup on 235B models) and demonstrate that control plane overheads can be fully masked. The critical takeaway for our `GPUSched` project is the **Lookahead Pipelining** architecture: it carves out a deterministic execution window where we could inject our own specialized solvers (e.g., fast ALNS or IP formulations) to outperform their basic greedy resource allocator. This transforms the stochastic serving problem into a short-horizon deterministic routing problem we are well-equipped to solve.</details></td><td><small>Qianchao Zhu et.al.</small></td><td><small>Kling Infra, Kuaishou Technology</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.00509">pdf</a> / code</small></td></tr>
<tr><td><small>‚Äî</small></td><td><small>2026-02-02</small></td><td><strong>Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation</strong></td><td><small>Zhongyuan Lyu et.al.</small></td><td><small></small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.02029">pdf</a> / code</small></td></tr>
<tr><td><small>19/30</small></td><td><small>2026-02-01</small></td><td><details><summary><strong>SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training</strong></summary>Pan et al. introduce SNIP, a framework that periodically solves a Knapsack-style Integer Linear Program (ILP) to assign layer-wise precision (FP4/FP8) during training, minimizing a custom 'divergence' metric subject to FLOPs constraints. Results are simulated via fake quantization (proxy FLOPs) rather than wall-clock time on native hardware, but the method scales to 70B models and outperforms heuristic baselines. **Key Takeaway:** The design pattern of 'collect sensitivity stats -> solve lightweight ILP -> dynamic reconfiguration' is highly relevant for our work on optimizing LLM serving and agent compute budgets; it proves standard OR solvers are fast enough to operate within the runtime loop of high-performance AI systems.</details></td><td><small>Yunjie Pan et.al.</small></td><td><small>University of Michigan, NTT Research, Inc., University of Massachusetts Amherst</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.01410">pdf</a> / <a href="https://github.com/pyjhzwh/SNIP">code</a></small></td></tr>
<tr><td><small>22/30</small></td><td><small>2026-02-01</small></td><td><details><summary><strong>Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models</strong></summary>Brown et al. propose a 'Predictive Scheduling' framework that trains lightweight predictors (MLP on hidden states or LoRA) to estimate required CoT length before generation, using a greedy algorithm to allocate tokens under a global budget. Results show a 7.9% accuracy gain on GSM8K over uniform batching, backed by a systematic layer-wise analysis. **The key takeaway for us is that middle transformer layers (12-17)‚Äînot the final layer‚Äîcontain the highest signal-to-noise ratio for predicting reasoning difficulty.** We should immediately test extracting features from these layers for our AlgoEvo value functions to improve sample efficiency. While the greedy scheduling algorithm itself is standard OR, the application to internal model states for pre-run allocation is a valid efficiency win for our serving optimization work.</details></td><td><small>Katrina Brown et.al.</small></td><td><small>Harvard University</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.01237">pdf</a> / <a href="https://aneeshers.github.io/predictive-scheduling/">code</a></small></td></tr>
<tr><td><small>‚Äî</small></td><td><small>2026-02-01</small></td><td><strong>EvoOpt-LLM: Evolving industrial optimization models with large language models</strong></td><td><small>Yiliu He et.al.</small></td><td><small></small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.01082">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2026-02-01</small></td><td><details><summary><strong>BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation</strong></summary>This paper derives a 'Budget-Optimal Allocation' (BOA) policy for ML training jobs, proving via queueing theory that a 'fixed-width' policy (no queueing, constant allocation per epoch) is optimal under general stochastic assumptions. They validate this on AWS, showing a 1.75x improvement in JCT and 2.2x budget reduction compared to Pollux, demonstrating that maximizing cluster utilization/efficiency is actually suboptimal for JCT. The key takeaway is the rigorous convex optimization formulation that replaces heuristic autoscaling, along with a practical 'Epoch Gluing' technique to handle rescaling overheads‚Äîboth transferable to scheduling our malleable evolutionary search workloads.</details></td><td><small>Zhouzi Li et.al.</small></td><td><small>Carnegie Mellon University, University of Warwick, UNC Chapel Hill</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2602.01404">pdf</a> / <a href="https://github.com/petuum/adaptdl/tree/osdi21-artifact/simulator">code</a></small></td></tr>
<tr><td><small>18/30</small></td><td><small>2026-01-30</small></td><td><details><summary><strong>Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces</strong></summary>Yang et al. introduce a neurosymbolic agent that translates natural language into PDDL goals, using a learned latent space to estimate 'intent uncertainty' (distance to class centroids) which gates downstream execution. They use this uncertainty signal to drive both Direct Preference Optimization (DPO) and prompt optimization (TextGrad), achieving higher accuracy than GPT-5 on a lightweight model. **Takeaway:** The concept of deriving a 'probabilistic guarantee' from latent embeddings to serve as a cheap proxy reward or filter is a concrete technique we should test in AlgoEvo to reduce expensive evaluations. However, be skeptical of the topline results as they rely on a simplistic 3-class classification task rather than complex reasoning.</details></td><td><small>Yunhao Yang et.al.</small></td><td><small>Neurosymbolic Intelligence, The University of Texas at Austin, University of Colorado Boulder</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2507.11352">pdf</a> / code</small></td></tr>
<tr><td><small>‚Äî</small></td><td><small>2026-01-28</small></td><td><strong>DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems</strong></td><td><small>Kostis Michailidis et.al.</small></td><td><small></small></td><td><small>ECAI25)</small></td><td><small><a href="http://arxiv.org/abs/2506.06052">pdf</a> / <a href="https://github.com/kostis-init/CP-Bench">code</a></small></td></tr>
<tr><td><small>18/30</small></td><td><small>2026-01-19</small></td><td><details><summary><strong>Cache Your Prompt When It's Green: Carbon-Aware Caching for Large Language Model Serving</strong></summary>Tian et al. propose GreenCache, a framework using Integer Linear Programming (ILP) to dynamically resize KV caches for LLM serving, balancing operational carbon (compute) against embodied carbon (SSD storage). They demonstrate ~15% carbon reduction on Llama-3 70B using Azure traces, though the reliance on simulation rather than live deployment weakens the claims slightly. For our 'OR for AI systems' work, the key takeaway is their 'Least Carbon Savings' (LCS) eviction policy‚Äîa heuristic that weighs computation saved against storage cost and recency‚Äîwhich we could adapt for optimizing memory-constrained multi-agent systems (HERMES) or general serving resource allocation.</details></td><td><small>Yuyang Tian et.al.</small></td><td><small>University of Waterloo, Purdue University</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2505.23970">pdf</a> / <a href="https://greencache.persistentmemory.org">code</a></small></td></tr>
<tr><td><small>18/30</small></td><td><small>2026-01-19</small></td><td><details><summary><strong>VectorLiteRAG: Latency-Aware and Fine-Grained Resource Partitioning for Efficient RAG</strong></summary>VectorLiteRAG optimizes RAG serving throughput by dynamically partitioning vector indices between CPU and GPU memory based on access skew and latency SLOs. The results are credible, showing up to 1.5x throughput gains on H100/L40S setups by balancing retrieval speed against LLM KV-cache capacity. The most stealable insight is their use of a Beta distribution to analytically model the *minimum* hit rate within a batch (the bottleneck) to predict tail latency without full simulation‚Äîa technique we could adapt for stochastic constraints in our serving formulations. It solves a resource allocation problem we care about, though via systems engineering rather than the rigorous OR methods we prefer.</details></td><td><small>Junkyum Kim et.al.</small></td><td><small>Georgia Institute of Technology</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2504.08930">pdf</a> / <a href="https://github.com/sitar-lab/VectorLiteRAG-AE">code</a></small></td></tr>
<tr><td><small>26/30</small></td><td><small>2026-01-15</small></td><td><details><summary><strong>Online Scheduling for LLM Inference with KV Cache Constraints</strong></summary>This paper formulates LLM inference scheduling as an Integer Program (IP) that explicitly models the linear memory growth of KV caches, and proposes a 'Memory Constrained Shortest First' (MC-SF) algorithm. The results are rigorous, showing MC-SF achieves near-optimal performance (within 5% of hindsight optimal) on synthetic data and significantly outperforms standard FCFS/threshold heuristics on real traces. The critical takeaway is the 'future feasibility check' (Eq. 5), which validates that a batch will *remain* within memory limits throughout the generation process based on predicted output lengths‚Äîa necessary deviation from standard static-size scheduling. This is foundational reading for our GPUSched project, providing both the exact IP baseline we need and a strong heuristic to benchmark against.</details></td><td><small>Patrick Jaillet et.al.</small></td><td><small>Massachusetts Institute of Technology, Microsoft Research, HKUST</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2502.07115">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2026-01-15</small></td><td><details><summary><strong>Fine-grained MoE Load Balancing with Linear Programming</strong></summary>FineMoE replaces heuristic load balancing in MoE training with a Linear Programming (LP) formulation solved per micro-batch to minimize maximum GPU load, achieving ~37-47% throughput gains over Megatron-LM. They utilize warm-started simplex solvers to keep optimization time under 1ms and employ Cayley graphs to optimize static expert placement. For our `GPUSched` work, this is a critical data point: it proves that formal OR solvers can replace heuristics in real-time LLM infrastructure without becoming a latency bottleneck.</details></td><td><small>Chenqi Zhao et.al.</small></td><td><small>Peking University, Institute of Computing Technology Chinese Academy of Sciences</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2511.16947">pdf</a> / code</small></td></tr>
<tr><td><small>‚Äî</small></td><td><small>2026-01-14</small></td><td><strong>OptiMind: Teaching LLMs to Think Like Optimization Experts</strong></td><td><small>Xinzhi Zhang et.al.</small></td><td><small></small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2509.22979">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2026-01-10</small></td><td><details><summary><strong>SkyNomad: On Using Multi-Region Spot Instances to Minimize AI Batch Job Cost</strong></summary>SkyNomad presents a multi-region scheduler for AI batch jobs that minimizes cost by dynamically migrating spot instances based on real-time availability probing and survival-analysis-based lifetime prediction. The authors propose a 'Unified Cost Model' that quantifies the monetary value of deadline slack, allowing the system to mathematically trade off migration egress costs against cheaper spot prices. Empirical results on AWS and GCP are strong, demonstrating 1.25-3.96x cost savings over single-region baselines while guaranteeing deadlines. We should immediately adapt their 'Value of Progress' heuristic and lifetime prediction module to optimize our own large-scale parallel evolution infrastructure.</details></td><td><small>Zhifei Li et.al.</small></td><td><small>UC Berkeley, Shanghai Jiao Tong University, AMD, ICSI</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2601.06520">pdf</a> / code</small></td></tr>
<tr><td><small>‚Äî</small></td><td><small>2026-01-09</small></td><td><strong>OPT-Engine: Benchmarking the Limits of LLMs in Optimization Modeling via Complexity Scaling</strong></td><td><small>Yitian Chen et.al.</small></td><td><small></small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2601.19924">pdf</a> / <a href="https://github.com/Cardinal-Operations/OPTEngine">code</a></small></td></tr>
<tr><td><small>26/30</small></td><td><small>2026-01-05</small></td><td><details><summary><strong>Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints</strong></summary>This paper formulates LLM inference as a multi-stage stochastic scheduling problem, introducing 'Nested WAIT'‚Äîa threshold-based algorithm that handles unknown output lengths by letting prompts classify themselves as they survive into deeper decode segments. Unlike heuristic baselines (vLLM, Sarathi), they provide rigorous asymptotic optimality proofs and high-probability bounds against memory overflow, validated on A100 simulations. The key takeaway is the 'nested segment' mechanism: instead of predicting job size, structure the queue so short jobs exit early and long jobs naturally migrate to lower-priority/protected tiers, effectively decoupling the memory risk. We should immediately evaluate this threshold logic for our GPUSched formulations, as it likely outperforms our current predictive or FCFS approaches for handling KV cache growth.</details></td><td><small>Ruicheng Ao et.al.</small></td><td><small>Massachusetts Institute of Technology, Peking University, Alibaba Group</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2504.11320">pdf</a> / code</small></td></tr>
<tr><td><small>20/30</small></td><td><small>2026-01-05</small></td><td><details><summary><strong>Balancing Fidelity and Plasticity: Aligning Mixed-Precision Fine-Tuning with Linguistic Hierarchies</strong></summary>QR-Adaptor employs a three-stage gradient-free search (Entropy Profiling ‚Üí NSGA-II ‚Üí Bayesian Optimization) to jointly optimize per-layer quantization bits and LoRA ranks. The results are empirically strong, showing that strategic mixed-precision (avg ~3.5 bits) can rival 16-bit baselines by preserving fidelity in deep semantic layers. We should steal their **Fidelity Sensitivity Profiling** (using information entropy to bias the initial evolutionary population) and **Proxy Tuning** (using few-step training as a cheap fitness proxy); these are concrete mechanisms to improve sample efficiency in our own evolutionary search pipelines.</details></td><td><small>Changhai Zhou et.al.</small></td><td><small>Fudan University, Yale University, Zhejiang University</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2505.03802">pdf</a> / code</small></td></tr>
<tr><td><small>21/30</small></td><td><small>2025-12-26</small></td><td><details><summary><strong>Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models</strong></summary>This paper formulates geographically distributed LLM inference as a joint block placement and request routing problem, solved via a decomposed MILP heuristic (greedy placement + shortest path routing). The results are real and validated on A100 clusters, showing 60-80% latency reduction over PETALS' native heuristics. The key takeaway for us is their explicit modeling of 'attention cache' memory consumption as a function of concurrent requests‚Äîtreating this as a dynamic constraint rather than a static buffer is the primary driver of their performance gains. This is a direct blueprint for the constraints we need in our 'GPUSched' formulations, though the algorithmic techniques themselves are standard OR fare.</details></td><td><small>Tingyang Sun et.al.</small></td><td><small>Pennsylvania State University, Virginia Tech, Indian Institute of Science</small></td><td><small>Performance Evaluation, Vol. 170, pp. 102527, November 2025</small></td><td><small><a href="http://arxiv.org/abs/2512.21884">pdf</a> / <a href="https://github.com/TingyangSunJeff/LLM_inference_simulator/tree/main">code</a></small></td></tr>
<tr><td><small>18/30</small></td><td><small>2025-12-25</small></td><td><details><summary><strong>Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism</strong></summary>FinDEP optimizes distributed Mixture-of-Experts (MoE) inference by partitioning tasks (attention, experts, communication) into fine-grained micro-batches and solving a scheduling problem to maximize overlap. The authors achieve 1.02x-1.61x speedups on H20/A6000 clusters compared to PPPipe, backed by solid empirical data. The key takeaway for our 'GPUSched' work is their methodology: deriving analytical properties (monotonicity and convexity) of the scheduling objective to reduce a complex search space into an $O(1)$ online solver, rather than relying on heavy solvers or RL. This confirms that simple linear performance models ($\alpha + \beta x$) are sufficient for accurate online resource allocation in LLM serving.</details></td><td><small>Xinglin Pan et.al.</small></td><td><small>The Hong Kong University of Science and Technology, Harbin Institute of Technology, Hong Kong Baptist University</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2512.21487">pdf</a> / code</small></td></tr>
<tr><td><small>25/30</small></td><td><small>2025-12-19</small></td><td><details><summary><strong>Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs</strong></summary>Twill formulates the complex interplay of software pipelining and warp specialization on modern GPUs (Hopper/Blackwell) as a joint SMT/ILP optimization problem, automatically rediscovering expert-tuned Flash Attention schedules without heuristics. The results are rigorous, matching hand-tuned performance within 1-2% and handling new hardware constraints (Blackwell TMEM) automatically. The key takeaway is the 'cost normalization' technique via ILP to make the scheduling search space tractable, and the demonstration that exact constraint solvers can replace human intuition for complex kernel generation. This is essential reading for your work on OR formulations for GPU scheduling and LLM serving optimization, offering a deterministic baseline to compare against evolutionary approaches.</details></td><td><small>Rupanshu Soi et.al.</small></td><td><small>Stanford University, NVIDIA</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2512.18134">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2025-12-18</small></td><td><details><summary><strong>Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference</strong></summary>Tian et al. introduce Staggered Batch Scheduling (SBS) for DP+EP architectures, enforcing a buffering window to enable global bin-packing rather than immediate dispatch, which they prove causes Head-of-Line blocking in non-preemptive prefill phases. Tested on a production H800 cluster serving DeepSeek-V3, they demonstrate a 30-40% reduction in TTFT and a ~20% throughput increase backed by clear utilization metrics. The most valuable takeaway for our GPUSched project is their 'IQR-aware lexicographical' scheduling heuristic for the Decode phase, which robustly balances batch size against KV-cache memory variance‚Äîa constraint logic we should immediately adopt. This work validates that discrete batching is superior to continuous dispatch for MoE models, necessitating an update to our queuing theory models.</details></td><td><small>Jian Tian et.al.</small></td><td><small>Baidu Inc.</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2512.16134">pdf</a> / code</small></td></tr>
<tr><td><small>19/30</small></td><td><small>2025-12-17</small></td><td><details><summary><strong>Dynamic Rebatching for Efficient Early-Exit Inference with DREX</strong></summary>DREX introduces a system for 'Early-Exit' LLMs that dynamically splits and regroups batches at intermediate layers, using a cost-benefit heuristic (Adaptive Rebatching Threshold) to decide when rebatching is profitable versus forcing execution. Results are solid (2-12% throughput gain on A100s) and backed by real system measurements, not just simulations. The key takeaway for us is the analytical model for rebatching overhead (Eq. 6)‚Äîwe can lift this constraint directly into our integer programming formulations for the GPUSched project to accurately model the trade-off between batch fragmentation and compute savings. Essential reading only for the serving optimization sub-team; irrelevant for the core evolutionary search group.</details></td><td><small>Xuting Liu et.al.</small></td><td><small>Microsoft Research, University of Pennsylvania</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2512.15705">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2025-12-15</small></td><td><details><summary><strong>RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training</strong></summary>ROLLMUX proposes a cluster scheduler that interleaves the rollout and training phases of multiple RL jobs to eliminate the idle 'dependency bubbles' inherent in synchronous on-policy learning. Tested on a production-scale cluster (328 H800s + 328 H20s), they demonstrate a 1.84x cost reduction with real-world traces, validating the approach beyond simulation. The most stealable insight is 'long-tail migration': dynamically detecting straggler requests during generation and migrating them to a small subset of nodes, freeing the main cluster to proceed immediately. We should implement this logic in our AlgoEvo evaluation loops to mitigate stochastic evaluation times.</details></td><td><small>Tianyuan Wu et.al.</small></td><td><small>Alibaba Group, Hong Kong University of Science and Technology, UIUC</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2512.11306">pdf</a> / code</small></td></tr>
<tr><td><small>18/30</small></td><td><small>2025-12-14</small></td><td><details><summary><strong>GoodSpeed: Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference</strong></summary>GoodSpeed uses gradient-based scheduling to dynamically allocate token generation budgets across distributed draft servers, maximizing a logarithmic utility function to balance throughput and fairness. The authors provide rigorous fluid sample path analysis to prove convergence, backed by experiments on H100/L4 clusters, although the baselines (fixed/random allocation) are relatively weak. The most useful takeaway is the mechanism of using exponentially smoothed acceptance rate estimates to drive real-time control in a stochastic system‚Äîa robust pattern we should adopt for our own stochastic resource allocation and RobustMAS projects.</details></td><td><small>Phuong Tran et.al.</small></td><td><small>The University of Sydney, Kyung Hee University</small></td><td><small>INFOCOM 2026</small></td><td><small><a href="http://arxiv.org/abs/2512.09963">pdf</a> / code</small></td></tr>
<tr><td><small>21/30</small></td><td><small>2025-12-13</small></td><td><details><summary><strong>HetRL: Efficient Reinforcement Learning for LLMs in Heterogeneous Environments</strong></summary>HetRL formulates the scheduling of RLHF workflows (PPO/GRPO) across heterogeneous GPUs and networks as a constrained joint optimization problem, solved via a multi-level search combining Successive Halving and Genetic Algorithms. The authors validate this with 20,000 GPU-hours of experiments, demonstrating 3-9x throughput gains over standard systems like 'verl' in heterogeneous settings. The key takeaway is the hierarchical decomposition of the search space (Task Grouping ‚Üí Coarse Assignment ‚Üí Fine-grained Assignment) and the use of SHA to efficiently allocate search budget among candidate configurations. This is directly actionable for your 'GPUSched' project and offers a concrete strategy to scale 'AlgoEvo' runs across cheaper, fragmented GPU resources.</details></td><td><small>Yongjun He et.al.</small></td><td><small>Amazon Web Services, ETH Zurich</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2512.12476">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2025-12-11</small></td><td><details><summary><strong>Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters</strong></summary>RLTune introduces a hybrid scheduling architecture where an RL agent (PPO) handles dynamic job prioritization based on cluster state, while a MILP solver optimizes the specific job-to-node packing constraints for the top-K jobs. The results are robust, demonstrating a ~25% makespan reduction over Slurm on a physical cluster and significant gains over pure RL baselines on standard traces (Philly, Helios). The critical takeaway is the architectural separation of concerns: delegating 'fuzzy' long-term objectives to RL and 'hard' constraint satisfaction to a symbolic solver. We should evaluate this 'RL-guided Solver' pattern for our `GPUSched` and `EvoCut` projects to improve constraint handling without losing adaptivity.</details></td><td><small>Shruti Dongare et.al.</small></td><td><small>Virginia Tech, Kuwait University, Northwestern Polytechnical University</small></td><td><small>ACM Symposium on Cloud Computing</small></td><td><small><a href="http://arxiv.org/abs/2512.10271">pdf</a> / <a href="https://github.com/dshruti20/RLTune">code</a></small></td></tr>
<tr><td><small>22/30</small></td><td><small>2025-12-05</small></td><td><details><summary><strong>MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement</strong></summary>MARINE proposes a multi-agent framework that iteratively refines a single 'reference trajectory' by generating small batches of candidates, verifying logical/factual conflicts, and merging superior segments rather than regenerating the whole chain. Results are impressive, with an 80B model matching 1000B baselines on retrieval tasks, backed by a theoretical derivation showing that batch size M=2 is optimal for fixed-budget refinement. The critical takeaway is the 'conflict-aware meta-verification' and segment merger, which functions effectively as a process-reward-guided mutation operator. We should immediately test the M=2 configuration in our evolutionary loops and consider adapting their merger logic to replace random crossover in our code generation agents.</details></td><td><small>Hongwei Zhang et.al.</small></td><td><small>ZTE</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2512.07898">pdf</a> / code</small></td></tr>
<tr><td><small>18/30</small></td><td><small>2025-12-02</small></td><td><details><summary><strong>Safety Game: Balancing Safe and Informative Conversations with Blackbox Agentic AI using LP Solvers</strong></summary>The authors formulate LLM response selection as a zero-sum game, solving a small Linear Program (LP) at inference time to mix candidate answers such that the expected risk never exceeds a 'safe fallback' baseline. Results are statistically significant, showing ~15% accuracy gains on SafetyBench by effectively managing the trade-off between helpfulness and safety probes. The key takeaway is the 'Adaptation Safety' constraint formulation: using an LP to guarantee that a stochastic policy is no worse than a heuristic baseline is a powerful, lightweight control mechanism we could adapt for selecting evolved algorithms or managing constraints in multi-agent optimization.</details></td><td><small>Tuan Nguyen et.al.</small></td><td><small>University of Warwick</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2510.09330">pdf</a> / code</small></td></tr>
<tr><td><small>23/30</small></td><td><small>2025-12-01</small></td><td><details><summary><strong>Optimal Scheduling Algorithms for LLM Inference: Theory and Practice</strong></summary>Bari et al. develop a queueing-theoretic framework for LLM inference that proves throughput optimality requires satisfying two conditions: optimal GeMM tiling (batch sizes matching hardware tensor core dimensions) and dynamic resource allocation between prefill/decode phases. They propose RAD (theoretical) and SLAI (practical), where SLAI uses a 'last schedulable time' heuristic to delay decode iterations for non-critical requests, thereby freeing up compute for prefill to reduce TTFT. Results are strong, showing a 53% reduction in median TTFT and 26% capacity increase over Sarathi-Serve on Mistral-7B. For our GPUSched project, the key takeaway is the explicit coupling of batch sizes to LCM(tile_dims) for theoretical optimality and the dynamic slack-based scheduling logic for heterogeneous SLOs.</details></td><td><small>Agrim Bari et.al.</small></td><td><small>The University of Texas at Austin</small></td><td><small>Proceedings of the ACM on Measurement and Analysis of Computing Systems</small></td><td><small><a href="http://arxiv.org/abs/2508.01002">pdf</a> / <a href="https://github.com/agrimUT/SLAI">code</a></small></td></tr>
<tr><td><small>18/30</small></td><td><small>2025-11-27</small></td><td><details><summary><strong>Optimizing NetGPT via Routing-Based Synergy and Reinforcement Learning</strong></summary>Chen et al. propose a cloud-edge routing framework that dynamically offloads tool-calling tasks based on network conditions (RTT/Bandwidth) and a learned confidence score, while simultaneously updating the edge model via PPO. Results on 8,000 tasks show that dynamic thresholds outperform static baselines like FrugalGPT, and crucially, that interleaving SFT updates is required to prevent JSON schema collapse during RL. The primary takeaway for us is the 'SFT-anchored' update strategy: alternating between RL (for reward maximization) and SFT (on valid outputs) is a simple, effective stabilizer for maintaining structural constraints (like code syntax or JSON) during optimization. We should test this anchoring technique in AlgoEvo to keep evolved heuristics syntactically valid while maximizing fitness.</details></td><td><small>Yuxuan Chen et.al.</small></td><td><small>Zhejiang University, Huawei Technologies Co., Ltd., Zhejiang Lab, Macau University of Science and Technology, The University of Electro-Communications, Shenzhen CyberAray Network Technology Co., Ltd</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2511.22217">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2025-11-26</small></td><td><details><summary><strong>BAMAS: Structuring Budget-Aware Multi-Agent Systems</strong></summary>BAMAS decouples agent resource provisioning from coordination strategy, using an Integer Linear Programming (ILP) solver to select the optimal set of LLMs under a strict budget and offline RL to select a fixed interaction topology. They demonstrate ~80% cost reduction on GSM8K and MBPP while matching SOTA accuracy, proving that formal optimization beats greedy heuristics for agent allocation. The key takeaway for us is the 'lexicographically optimal' ILP formulation for tier-based LLM selection, which we should steal immediately for our inference resource managers. While their topology search is limited to a fixed library (unlike our evolutionary approach), the hybrid ILP+RL architecture is a strong template for our 'OR for Generative AI' work.</details></td><td><small>Liming Yang et.al.</small></td><td><small>Tsinghua University, Peking University, University of Illinois Urbana-Champaign, Nanyang Technological University</small></td><td><small>AAAI 2026 (oral paper)</small></td><td><small><a href="http://arxiv.org/abs/2511.21572">pdf</a> / <a href="https://github.com/chunfenri/BAMAS">code</a></small></td></tr>
<tr><td><small>24/30</small></td><td><small>2025-11-19</small></td><td><details><summary><strong>Global Resolution: Optimal Multi-Draft Speculative Sampling via Convex Minimization</strong></summary>The authors solve the Optimal Transport Linear Program (OTLP) for multi-draft speculative sampling by reducing it to a convex minimization problem using polymatroid theory and max-flow, rather than using slow general LP solvers. They prove this 'Global Resolution' algorithm is exact for i.i.d. drafts and achieves >90% acceptance with negligible overhead (<100ms), running 10,000x faster than baselines. **Key Takeaway:** The reduction of a discrete token selection problem to a convex optimization problem via polymatroids is a brilliant theoretical trick we could potentially adapt for selecting diverse solution subsets in AlgoEvo. This is a definitive 'OR for LLM infra' paper that obsoletes heuristic verification strategies.</details></td><td><small>Rahul Krishna Thomas et.al.</small></td><td><small>Stanford University, Ritual</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2511.15898">pdf</a> / code</small></td></tr>
<tr><td><small>‚Äî</small></td><td><small>2025-11-13</small></td><td><strong>LM4Opt-RA: A Multi-Candidate LLM Framework with Structured Ranking for Automating Network Resource Allocation</strong></td><td><small>Tasnim Ahmed et.al.</small></td><td><small></small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2512.00039">pdf</a> / code</small></td></tr>
<tr><td><small>24/30</small></td><td><small>2025-11-12</small></td><td><details><summary><strong>SageServe: Optimizing LLM Serving on Cloud Data Centers with Forecast Aware Auto-Scaling</strong></summary>SageServe optimizes LLM inference resource allocation across regions using an Integer Linear Programming (ILP) model coupled with ARIMA-based traffic forecasting, specifically targeting mixed interactive and non-interactive workloads. They validate this on real Microsoft O365 production traces (which they release), demonstrating a 25% reduction in GPU hours and $2.5M/month savings compared to reactive baselines. The primary value for us is the release of the production workload traces‚Äîallowing us to benchmark our 'GPUSched' formulations against real-world data rather than synthetic distributions‚Äîand their specific ILP formulation for unified capacity management, which directly competes with our internal OR models.</details></td><td><small>Shashwat Jaiswal et.al.</small></td><td><small>Microsoft, University of Illinois Urbana-Champaign, Georgia Institute of Technology, Indian Institute of Science</small></td><td><small>Proceedings of the ACM on Measurement and Analysis of Computing Systems, Vol. 9, No. 3, Article 61. December 2025</small></td><td><small><a href="http://arxiv.org/abs/2502.14617">pdf</a> / <a href="https://github.com/shashwatj07/SageServe">code</a></small></td></tr>
<tr><td><small>25/30</small></td><td><small>2025-11-12</small></td><td><details><summary><strong>OR-R1: Automating Modeling and Solving of Operations Research Optimization Problem via Test-Time Reinforcement Learning</strong></summary>OR-R1 introduces a data-efficient framework that fine-tunes Qwen3-8B using Supervised Fine-Tuning (SFT) followed by Test-Time Group Relative Policy Optimization (TGRPO) on unlabeled data. The results are empirically strong: it outperforms ORLM and LLMOPT while using only 1/10th of the synthetic training data, specifically narrowing the consistency gap between Pass@1 and Pass@8. The key takeaway for us is the effectiveness of GRPO (normalizing rewards within a sampled group to estimate baselines) combined with majority-voting rewards; this eliminates the need for a separate critic model while significantly improving code generation consistency. We should immediately evaluate GRPO as a lightweight alternative to PPO for the 'RL-infused' components of our evolutionary search methods.</details></td><td><small>Zezhen Ding et.al.</small></td><td><small>The Hong Kong University of Science and Technology, Arizona State University, University of North Carolina at Chapel Hill</small></td><td><small>AAAI</small></td><td><small><a href="http://arxiv.org/abs/2511.09092">pdf</a> / <a href="https://github.com/SCUTE-ZZ/OR-R1">code</a></small></td></tr>
<tr><td><small>21/30</small></td><td><small>2025-11-08</small></td><td><details><summary><strong>CoEdge-RAG: Optimizing Hierarchical Scheduling for Retrieval-Augmented LLMs in Collaborative Edge Computing</strong></summary>Hong et al. introduce CoEdge-RAG, a hierarchical scheduling framework for distributed edge RAG that combines PPO-based query routing with Online Convex Optimization (OCO) for local resource management. They empirically validate that a quadratic function best approximates LLM inference latency for OCO, allowing them to dynamically resize models and memory allocations under strict SLOs. The standout takeaway is the feedback loop: using PPO to learn a 'semantic routing policy' based on downstream generation quality (Rouge/BERTScore) rather than just load, effectively solving the 'black box' data distribution problem in privacy-preserving multi-agent systems. This hybrid RL/OR control stack is a transferable pattern for our distributed inference and multi-agent optimization work.</details></td><td><small>Guihang Hong et.al.</small></td><td><small>Sun Yat-sen University</small></td><td><small>RTSS 2025 (Real-Time Systems Symposium</small></td><td><small><a href="http://arxiv.org/abs/2511.05915">pdf</a> / code</small></td></tr>
<tr><td><small>‚Äî</small></td><td><small>2025-11-04</small></td><td><strong>An LLM-powered MILP modelling engine for workforce scheduling guided by expert knowledge</strong></td><td><small>Qingyang Li et.al.</small></td><td><small></small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2511.02364">pdf</a> / code</small></td></tr>
<tr><td><small>25/30</small></td><td><small>2025-10-20</small></td><td><details><summary><strong>Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling</strong></summary>Wang et al. introduce Direction-Oriented Resource Allocation (DORA), which uses embedding-based soft clustering to group semantically similar reasoning paths and allocates compute budget to distinct 'directions' rather than individual solutions. They prove solution-level allocation (like REBASE) is suboptimal when paths are correlated and show DORA achieves state-of-the-art accuracy on MATH500 with 3.5x fewer FLOPs. **Key Takeaway:** We can immediately steal the 'semantic uniqueness reweighting' mechanism for AlgoEvo. By clustering generated heuristics via embeddings before expensive evaluation, we can drastically improve sample efficiency and stop wasting compute on minor variations of the same code.</details></td><td><small>Xinglin Wang et.al.</small></td><td><small>Beijing Institute of Technology, Xiaohongshu Inc</small></td><td><small>NeurIPS2025</small></td><td><small><a href="http://arxiv.org/abs/2506.15707">pdf</a> / <a href="https://github.com/WangXinglin/DORA">code</a></small></td></tr>
<tr><td><small>23/30</small></td><td><small>2025-10-19</small></td><td><details><summary><strong>Justitia: Fair and Efficient Scheduling for LLM Applications</strong></summary>Justitia introduces a scheduler for LLM agents that prioritizes applications based on their 'virtual finish time' (derived from a theoretical fair-sharing model) but executes them with full resource saturation to minimize completion time. The authors demonstrate a ~60% reduction in average job completion time compared to state-of-the-art fair schedulers (VTC) on vLLM, backed by rigorous experiments and theoretical delay bounds. The key takeaway is the 'KV token-time' cost metric (pd + d^2/2) which accurately captures memory bottlenecks in auto-regressive generation, and the insight that 'long-term fairness' allows for short-term resource saturation. This is immediately actionable for your GPUSched project and relevant for optimizing the serving infrastructure of AlgoEvo.</details></td><td><small>Mingyan Yang et.al.</small></td><td><small>Shanghai Jiao Tong University</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2510.17015">pdf</a> / code</small></td></tr>
<tr><td><small>20/30</small></td><td><small>2025-10-13</small></td><td><details><summary><strong>MC#: Mixture Compressor for Mixture-of-Experts Large Models</strong></summary>Huang et al. propose MC#, a compression framework for MoE models that combines static mixed-precision quantization with dynamic expert pruning. They formulate bit-width allocation as an Integer Linear Programming (ILP) problem‚Äîoptimizing expert importance vs. quantization error‚Äîand use a Gumbel-Softmax router for dynamic pruning. Results are strong, achieving 6.2x weight reduction on DeepSeek-VL2 with <2% accuracy loss. **Takeaway:** The ILP formulation (Eq. 7) is a clean, successful application of OR to AI infrastructure that we should replicate for our own resource allocation/scheduling problems; additionally, the differentiable router offers a template for dynamic agent selection in our multi-agent systems.</details></td><td><small>Wei Huang et.al.</small></td><td><small>NVIDIA Research, National University of Singapore, The University of Hong Kong, Beihang University, Hangzhou Innovation Institute</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2510.10962">pdf</a> / code</small></td></tr>
<tr><td><small>18/30</small></td><td><small>2025-10-13</small></td><td><details><summary><strong>Efficient LLM Inference over Heterogeneous Edge Networks with Speculative Decoding</strong></summary>Zhu et al. propose a distributed Speculative Decoding framework for edge networks, formulating a Mixed-Integer Nonlinear Programming problem to jointly optimize task batching, speculation length, and wireless bandwidth. They solve the batching subproblem using a Dynamic Programming (DP) algorithm, achieving ~30-45% latency reduction over heuristics in simulations, though the approach relies on a rigid assumption of fixed maximum output lengths to remain tractable. The primary takeaway for our 'GPUSched' work is their DP formulation for optimizing batch boundaries in a pipelined draft-verify system, which offers a cleaner mathematical alternative to greedy heuristics for serving schedules. However, the heavy reliance on wireless channel modeling makes the full system less relevant to our datacenter-centric optimization problems.</details></td><td><small>Bingjie Zhu et.al.</small></td><td><small>Queen Mary University of London, Kyung Hee University, Xidian University, Guangzhou Institute of Technology</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2510.11331">pdf</a> / code</small></td></tr>
<tr><td><small>18/30</small></td><td><small>2025-10-08</small></td><td><details><summary><strong>FLEET: Formal Language-Grounded Scheduling for Heterogeneous Robot Teams</strong></summary>FLEET implements a hybrid pipeline where an LLM extracts a task dependency graph and a 'fitness matrix' (capability scores) from natural language, which then populate a standard MILP for multi-robot scheduling. Results on the PARTNR benchmark show it outperforms pure LLM planners (SMART-LLM) by ~7% on heterogeneous tasks, though overall gains are modest. The actionable takeaway is the **fitness matrix extraction**: using the LLM to generate dense cost coefficients ($c_{ij}$) for the optimization model rather than just binary constraints. We should adopt this technique for handling soft semantic preferences in our heterogeneous VRP formulations.</details></td><td><small>Corban Rivera et.al.</small></td><td><small>JHU APL, JHU, DEVCOM ARL</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2510.07417">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2025-10-07</small></td><td><details><summary><strong>ConstraintLLM: A Neuro-Symbolic Framework for Industrial-Level Constraint Programming</strong></summary>ConstraintLLM fine-tunes a 32B model for Constraint Programming (CP) modeling, utilizing a &quot;Constraint-Aware Retrieval Module&quot; (CARM) that retrieves few-shot examples based on extracted constraint signatures (e.g., `AllDifferent`, `Cumulative`) rather than text embeddings. They also employ a Tree-of-Thoughts search pruned by test case execution and an iterative self-correction mechanism that retrieves &quot;correction paths&quot; (error-to-fix trajectories). Results are strong: on their new industrial benchmark (IndusCP), they achieve ~51% accuracy with a 32B model, matching or beating GPT-4o and DeepSeek-V3. **Key Takeaway:** The shift from semantic retrieval to *structural* retrieval (matching constraint profiles) is the &quot;stealable&quot; insight; we should implement this for our OR modeling tasks immediately, ignoring surface-level problem descriptions in favor of logical signatures. This directly impacts our OR-Bench and automated formulation work.</details></td><td><small>Weichun Shi et.al.</small></td><td><small>University of Oxford, University of Chinese Academy of Sciences, Hangzhou Institute for Advanced Study, ISCAS, University of Science and Technology Beijing</small></td><td><small>Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 15999-16019</small></td><td><small><a href="http://arxiv.org/abs/2510.05774">pdf</a> / <a href="https://github.com/william4s/ConstraintLLM">code</a></small></td></tr>
<tr><td><small>20/30</small></td><td><small>2025-10-01</small></td><td><details><summary><strong>Logical Consistency Between Disagreeing Experts and Its Role in AI Safety</strong></summary>Corrada-Emmanuel formulates the unsupervised evaluation of classifiers as an Integer Linear Programming problem, defining the geometric space of possible ground truths consistent with observed agent disagreements. While the results are primarily theoretical demonstrations on MT-Bench (showing that certain disagreement patterns mathematically preclude accuracy >46%), the methodology is sound. The key takeaway is the concept of 'no-knowledge alarms': using LP constraints to flag when a multi-agent system or process reward model has become logically incoherent. We could implement this as a cheap, rigorous filter in our evolutionary search loops to prune branches where the evaluator agents are demonstrably unreliable.</details></td><td><small>Andr√©s Corrada-Emmanuel et.al.</small></td><td><small>Data Engines</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2510.00821">pdf</a> / code</small></td></tr>
<tr><td><small>‚Äî</small></td><td><small>2025-10-01</small></td><td><strong>StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models</strong></td><td><small>Chenyu Zhou et.al.</small></td><td><small>Shanghai Jiao Tong Univeristy</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2509.22558">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2025-09-29</small></td><td><details><summary><strong>Cascadia: An Efficient Cascade Serving System for Large Language Models</strong></summary>Jiang et al. propose CASCADIA, a bi-level optimization framework for LLM cascade serving that iterates between an MILP solver for hardware deployment (choosing DP/TP/PP strategies) and a Chebyshev-guided solver for routing thresholds. They demonstrate 2.3x average throughput gains over SGLang and CascadeServe on H100 clusters, backed by rigorous ablation studies. The key takeaway is the effective decomposition of the NP-hard joint optimization problem: freezing routing to solve deployment via MILP, then optimizing routing against that deployment. This is a direct reference point for our 'GPUSched' project, validating the efficacy of formal integer programming in LLM resource allocation.</details></td><td><small>Youhe Jiang et.al.</small></td><td><small>Princeton University, University of Cambridge, Tsinghua University, HKUST, Shanghai Jiaotong University</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2506.04203">pdf</a> / code</small></td></tr>
<tr><td><small>19/30</small></td><td><small>2025-09-29</small></td><td><details><summary><strong>FMIP: Joint Continuous-Integer Flow For Mixed-Integer Linear Programming</strong></summary>FMIP introduces a flow matching framework that jointly generates integer and continuous variables for MILP, utilizing a tripartite graph and inference-time guidance. Empirical results on MIPLIB and other benchmarks show a ~40% reduction in primal gap compared to integer-only neural baselines (DIFUSCO), though it remains a heuristic warm-start for solvers. The most valuable takeaway is the **hybrid guidance mechanism** (Eq. 6 & 7): it combines gradient descent for continuous variables with a sampling-and-reweighting scheme for discrete variables based on constraint violations. We should consider stealing this reweighting logic for guiding hybrid evolutionary operators or multi-agent action spaces where gradients are available for only part of the state.</details></td><td><small>Hongpei Li et.al.</small></td><td><small>Stanford University, Princeton University, National University of Singapore, Shanghai Jiao Tong University, Shanghai University of Finance and Economics</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2507.23390">pdf</a> / <a href="https://github.com/Lhongpei/FMIP">code</a></small></td></tr>
<tr><td><small>18/30</small></td><td><small>2025-09-27</small></td><td><details><summary><strong>Enhancing Delta Compression in LLMs via SVD-based Quantization Error Minimization</strong></summary>PRINMIX replaces heuristic quantization of LLM delta-weights with a 0/1 Integer Linear Programming (ILP) formulation to minimize reconstruction error. The results are strong and backed by numbers, showing ~22% improvement on AIME2024 and 6x storage savings compared to Delta-CoMe. For us, the key takeaway is not the compression itself, but the formulation: it proves that exact OR modeling outperforms heuristics in LLM serving infrastructure. Additionally, the reported 30-minute solving time suggests this problem could serve as a valuable testbed for our own evolutionary solver acceleration (EvoCut/AlgoEvo).</details></td><td><small>Boya Xiong et.al.</small></td><td><small>Tsinghua University, Fudan University, Southern University of Science and Technology, Shanghai University of Finance and Economics</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2506.11087">pdf</a> / code</small></td></tr>
<tr><td><small>23/30</small></td><td><small>2025-09-25</small></td><td><details><summary><strong>Best-of-$\infty$ -- Asymptotic Performance of Test-Time Compute</strong></summary>This paper introduces a Bayesian adaptive stopping criterion (using Dirichlet process priors and Bayes factors) for majority voting, reducing test-time compute by 2-5x while maintaining asymptotic 'Best-of-Infinity' accuracy. They further demonstrate that optimizing weights for an ensemble of LLMs can be formulated as a Mixed-Integer Linear Program (MILP) by treating the decision boundaries as polytopes. **What we learned:** The Bayesian stopping logic is immediately transferable to AlgoEvo to reduce the cost of fitness evaluations‚Äîwe can stop evaluating candidate solutions early if their performance distribution is statistically distinct. The MILP approach for ensembles also offers a concrete formulation we could adapt for our GPU scheduling and model serving optimization work.</details></td><td><small>Junpei Komiyama et.al.</small></td><td><small>Mohamed bin Zayed University of Artificial Intelligence, New York University, RIKEN AIP, Institute of Science Tokyo, NEC Corporation</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2509.21091">pdf</a> / <a href="https://github.com/jkomiyama/BoInf-code-publish">code</a></small></td></tr>
<tr><td><small>‚Äî</small></td><td><small>2025-09-24</small></td><td><strong>OR-Toolformer: Modeling and Solving Operations Research Problems with Tool Augmented Large Language Models</strong></td><td><small>Jianzhang Zhang et.al.</small></td><td><small></small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2510.01253">pdf</a> / code</small></td></tr>
<tr><td><small>19/30</small></td><td><small>2025-09-23</small></td><td><details><summary><strong>Robust DNN Partitioning and Resource Allocation Under Uncertain Inference Time</strong></summary>Nan et al. propose a robust optimization framework for DNN partitioning that handles uncertain inference times by converting probabilistic deadlines into deterministic constraints using mean/variance information (Chance-Constrained Programming). They decompose the resulting MINLP into a convex resource allocation problem and a partitioning problem solved via the Penalty Convex-Concave Procedure (PCCP). Experiments on real hardware (Jetson/RTX) demonstrate ~50% energy savings over worst-case baselines while maintaining violation probabilities below the risk threshold. For our 'GPUSched' and 'RobustMAS' work, the key takeaway is the specific analytic transformation of the chance constraint and the use of PCCP as a heuristic for the binary subproblem‚Äîa potential alternative to heavy evolutionary search for real-time scheduling components.</details></td><td><small>Zhaojun Nan et.al.</small></td><td><small>Tsinghua University</small></td><td><small>IEEE Transactions on Mobile Computing</small></td><td><small><a href="http://arxiv.org/abs/2503.21476">pdf</a> / code</small></td></tr>
<tr><td><small>21/30</small></td><td><small>2025-09-16</small></td><td><details><summary><strong>Temporal-Aware GPU Resource Allocation for Distributed LLM Inference via Reinforcement Learning</strong></summary>TORTA introduces a hierarchical scheduler for distributed LLM inference that uses a macro-level RL agent (PPO) supervised by an Optimal Transport (OT) baseline to manage inter-region allocation, followed by a micro-level greedy allocator. Results on simulated clusters (up to 50 servers) demonstrate a ~15% reduction in latency compared to reactive baselines (like SkyLB) specifically by optimizing for temporal smoothness and reducing switching costs. The key technical takeaway is the use of an exact OR solver (OT) as a dense supervision signal to train a faster RL policy, effectively combining the optimality of OR with the temporal foresight of RL. We should review our GPUSched formulations to ensure we aren't falling into the 'reactive' trap described here; if we are, this hybrid RL-OT architecture is a viable alternative.</details></td><td><small>Chengze Du et.al.</small></td><td><small>Shenzhen University of Advanced Technology, China Mobile Research Institute</small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2507.10259">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2025-08-26</small></td><td><details><summary><strong>HAP: Hybrid Adaptive Parallelism for Efficient Mixture-of-Experts Inference</strong></summary>HAP replaces static parallelization heuristics in MoE inference with an Integer Linear Programming (ILP) solver that dynamically selects optimal strategies (TP, EP, DP) for Attention and Expert modules. They achieve verified ~1.6x speedups on A100/A6000 GPUs by modeling the inference process as a two-stage problem (prefill vs. decoding) with explicit transition costs, allowing the system to switch parallelism strategies mid-inference. For our work on OR-based resource allocation (GPUSched), the key takeaway is their formulation of **transition overheads** within the ILP constraints‚Äîa technique we should steal to model dynamic reconfiguration in our scheduling solvers. This confirms that symbolic OR methods can outperform standard systems heuristics in the LLM serving stack.</details></td><td><small>Haoran Lin et.al.</small></td><td><small>Huawei Noah‚Äôs Ark Lab, Shandong University</small></td><td><small>International Conference on Parallel and Distributed Systems</small></td><td><small><a href="http://arxiv.org/abs/2508.19373">pdf</a> / code</small></td></tr>
<tr><td><small>20/30</small></td><td><small>2025-08-18</small></td><td><details><summary><strong>Batching-Aware Joint Model Onloading and Offloading for Hierarchical Multi-Task Inference</strong></summary>Cha et al. propose an alternating optimization framework (J3O) for joint model placement and query routing in hierarchical inference systems, decomposing the MINLP into greedy Lagrangian submodular maximization and linear programming. They explicitly model batching latency at the edge using a linear surrogate to handle the non-convex batch setup costs, achieving ~97% of Gurobi's optimal accuracy with <15% of the runtime. **Takeaway:** We should steal their linear surrogate formulation for batching overhead (approximating the L0-norm of task arrival) for our 'GPUSched' integer programs; it offers a tractable way to model batching efficiency in serving systems without full non-linear solvers.</details></td><td><small>Seohyeon Cha et.al.</small></td><td><small>The University of Texas at Austin, DEVCOM Army Research Laboratory</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2508.13380">pdf</a> / code</small></td></tr>
<tr><td><small>‚Äî</small></td><td><small>2025-08-16</small></td><td><strong>EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models</strong></td><td><small>Milad Yazdani et.al.</small></td><td><small></small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2508.11850">pdf</a> / <a href="https://github.com/milad1378yz/EvoCut">code</a></small></td></tr>
<tr><td><small>19/30</small></td><td><small>2025-08-12</small></td><td><details><summary><strong>Cluster Topology-Driven Placement of Experts Reduces Network Traffic in MoE Inference</strong></summary>This paper formulates the placement of MoE experts (specifically DeepSeek-R1/V3) onto distributed GPU clusters as an Integer Linear Program (ILP) to minimize network hops. While the results are simulation-based (counting hops rather than measuring real latency), they demonstrate that ILP-based placement reduces traffic by ~14-30% compared to Round-Robin, but *only* when the objective function is weighted by historical expert activation frequency; unweighted ILP performs poorly. The key takeaway for our GPUSched project is the specific formulation of the load-aware objective function and the finding that topology-aware placement requires usage statistics to beat simple heuristics. We should adapt this ILP formulation for our resource allocation work.</details></td><td><small>Danil Sivtsov et.al.</small></td><td><small>AIRI, Skoltech, Avito</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2508.09229">pdf</a> / <a href="https://github.com/svtdanny/moe_topology_pack">code</a></small></td></tr>
<tr><td><small>18/30</small></td><td><small>2025-08-11</small></td><td><details><summary><strong>Pareto Multi-Objective Alignment for Language Models</strong></summary>PAMA introduces a computationally efficient algorithm for multi-objective alignment by reformulating the expensive gradient-norm minimization of MGDA into a convex optimization problem with a closed-form solution, reducing complexity from O(n^2d) to O(n). Empirical results on LLaMA-2-7B are robust, showing stable convergence on conflicting objectives (e.g., harmlessness vs. length) where baselines like MGDA-UB oscillate or fail. The single most useful takeaway is the analytical derivation for optimal objective weighting (Theorem 1) and the 'Noon PPO' heuristic (clipping negative advantages); we could port this logic to our multi-objective process reward models in AlgoEvo to balance search signals efficiently. While the NLP experiments are trivial, the gradient balancing mechanism is directly applicable to our multi-objective RL controllers.</details></td><td><small>Qiang He et.al.</small></td><td><small>Ruhr University Bochum</small></td><td><small>ECML/PKDD 2025</small></td><td><small><a href="http://arxiv.org/abs/2508.07768">pdf</a> / code</small></td></tr>
<tr><td><small>21/30</small></td><td><small>2025-08-05</small></td><td><details><summary><strong>Learning to Incentivize: LLM-Empowered Contract for AIGC Offloading in Teleoperation</strong></summary>Zhan et al. propose an LLM-based evolutionary framework to generate Python solvers for inferring hidden agent parameters in contract design (a bilevel OR problem). While the experiments are toy-scale (N=7 actions) and benchmarks are weak, the methodological architecture is highly relevant: they separate 'short-term reflectors' (analyzing parent pairs) from a 'long-term reflector' (aggregating insights across generations) to guide the Mutation LLM. This is a concrete, transferable implementation of evolutionary memory that we should test to improve sample efficiency in our own code-evolving agents.</details></td><td><small>Zijun Zhan et.al.</small></td><td><small>University of Houston, The Pennsylvania State University, University of Florida, Kyung Hee University, China University of Petroleum (East China), Prairie View A&M University</small></td><td><small>IEEE Transactions on Network Science and Engineering</small></td><td><small><a href="http://arxiv.org/abs/2508.03464">pdf</a> / <a href="https://github.com/Zijun0819/llm4contract">code</a></small></td></tr>
<tr><td><small>‚Äî</small></td><td><small>2025-08-05</small></td><td><strong>Toward a Trustworthy Optimization Modeling Agent via Verifiable Synthetic Data Generation</strong></td><td><small>Vinicius Lima et.al.</small></td><td><small></small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2508.03117">pdf</a> / code</small></td></tr>
<tr><td><small>‚Äî</small></td><td><small>2025-08-01</small></td><td><strong>OR-LLM-Agent: Automating Modeling and Solving of Operations Research Optimization Problems with Reasoning LLM</strong></td><td><small>Bowen Zhang et.al.</small></td><td><small></small></td><td><small></small></td><td><small><a href="http://arxiv.org/abs/2503.10009">pdf</a> / <a href="https://github.com/bwz96sco/or_llm_agent">code</a></small></td></tr>
<tr><td><small>25/30</small></td><td><small>2025-07-21</small></td><td><details><summary><strong>DHEvo: Data-Algorithm Based Heuristic Evolution for Generalizable MILP Solving</strong></summary>DHEvo introduces a 'data-algorithm co-evolution' framework that iteratively evolves heuristic code while simultaneously filtering the training instance set to retain only 'representative' instances (those where current heuristics perform well/stably). Empirical results on SCIP diving heuristics show it outperforms FunSearch and EoH by ~60% on Setcover while significantly reducing performance variance, validating the claim that dynamic data curation prevents overfitting. The key takeaway is the counter-intuitive curriculum strategy: rather than training on the hardest instances, filtering for instances with 'regular' feasible regions (high fitness) stabilizes the evolutionary search for code. We should immediately test this dynamic instance filtering in AlgoEvo to improve sample efficiency and generalization.</details></td><td><small>Zhihao Zhang et.al.</small></td><td><small>Harbin Institute of Technology, Huawei Noah‚Äôs Ark Lab, Nanyang Technological University</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2507.15615">pdf</a> / code</small></td></tr>
<tr><td><small>‚Äî</small></td><td><small>2025-06-30</small></td><td><strong>Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice</strong></td><td><small>Akshit Kumar et.al.</small></td><td><small></small></td><td><small>Online World Conference on Soft Computing in Industrial Applications</small></td><td><small><a href="http://arxiv.org/abs/2506.23924">pdf</a> / code</small></td></tr>
<tr><td><small>23/30</small></td><td><small>2025-06-11</small></td><td><details><summary><strong>ETS: Efficient Tree Search for Inference-Time Scaling</strong></summary>ETS formulates the tree search pruning step as a lightweight Integer Linear Program (ILP) that maximizes the reward of retained nodes while penalizing total KV cache size and enforcing semantic diversity via clustering. Unlike standard beam search or REBASE, it explicitly optimizes the trade-off between memory consumption (KV sharing) and exploration coverage. The authors demonstrate a 1.8x reduction in KV cache size and 1.4x throughput increase on MATH500 with minimal accuracy loss. We should steal the 'ILP-in-the-loop' mechanism for population management in our evolutionary search frameworks to optimize hardware utilization dynamically.</details></td><td><small>Coleman Hooper et.al.</small></td><td><small>University of California, Berkeley, ICSI, LBNL</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2502.13575">pdf</a> / <a href="https://github.com/SqueezeAILab/ETS">code</a></small></td></tr>
<tr><td><small>‚Äî</small></td><td><small>2025-06-05</small></td><td><strong>Autoformulation of Mathematical Optimization Models Using LLMs</strong></td><td><small>Nicol√°s Astorga et.al.</small></td><td><small></small></td><td><small>ICML</small></td><td><small><a href="http://arxiv.org/abs/2411.01679">pdf</a> / code</small></td></tr>
<tr><td><small>20/30</small></td><td><small>2025-06-05</small></td><td><details><summary><strong>Demystifying Cost-Efficiency in LLM Serving over Heterogeneous GPUs</strong></summary>Jiang et al. formulate LLM serving on heterogeneous clouds as a Mixed-Integer Linear Programming (MILP) problem, co-optimizing GPU rental composition, parallelism strategies (TP/PP), and workload routing. They demonstrate ~25% throughput gains over SOTA systems (Helix, HexGen) using vLLM benchmarks, validating the approach with strong empirical ablations. For our **GPUSched** project, the key takeaway is their solver strategy: pre-generating valid configurations to linearize the problem and using a binary search wrapper on the makespan to avoid direct minimization overhead. We should adopt their heuristics for pruning the configuration space (e.g., restricting TP to intra-node) to improve our own solver times.</details></td><td><small>Youhe Jiang et.al.</small></td><td><small>University of Cambridge, ETH Zurich, Peking University, The Hong Kong University of Science and Technology, Purdue University</small></td><td><small>International Conference on Machine Learning</small></td><td><small><a href="http://arxiv.org/abs/2502.00722">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2025-06-03</small></td><td><details><summary><strong>Puzzle: Distillation-Based NAS for Inference-Optimized LLMs</strong></summary>Bercovich et al. introduce Puzzle, a framework that optimizes LLM architectures for specific hardware by training a library of block variants (via local distillation) and using Mixed-Integer Programming (MIP) to select the optimal layer-wise configuration under strict latency and memory constraints. The results are robust: they compress Llama-70B to 51B, fitting on a single H100 with 2.17x throughput gain and 98.4% accuracy retention, significantly outperforming pruning baselines like Wanda. **Key takeaway:** The 'decomposed search' strategy‚Äîreplacing expensive end-to-end evolutionary evaluation loops with local proxy scores (KL divergence) and a global MIP solver‚Äîis a highly efficient method for modular system configuration. This directly informs our 'GPUSched' and serving optimization work by demonstrating how to mathematically formulate hardware constraints (KV-cache, batch size, compute) into the model design process itself.</details></td><td><small>Akhiad Bercovich et.al.</small></td><td><small>NVIDIA</small></td><td><small>International Conference on Machine Learning</small></td><td><small><a href="http://arxiv.org/abs/2411.19146">pdf</a> / code</small></td></tr>
<tr><td><small>25/30</small></td><td><small>2025-04-24</small></td><td><details><summary><strong>Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents</strong></summary>Li et al. formulate a batch queueing model for LLM inference, proving that 'work-conserving' algorithms (like Sarathi-Serve) which mix prefill and decode tokens are throughput-optimal, whereas separated strategies (vanilla vLLM, FasterTransformer) are theoretically unstable. The results are rigorous, combining fluid limit proofs with empirical validation on A100s showing queue blow-ups in non-optimal schedulers. The key takeaway is the precise definition of stability for token-level batching and the counter-intuitive finding that these locally optimal policies can fail in multi-agent networks due to cyclic resource dependencies. This is foundational reading for our GPUSched project and directly informs how we should model resource allocation for our multi-agent optimization systems.</details></td><td><small>Yueying Li et.al.</small></td><td><small>Cornell University, Columbia University</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2504.07347">pdf</a> / code</small></td></tr>
<tr><td><small>19/30</small></td><td><small>2025-04-04</small></td><td><details><summary><strong>ORLM: A Customizable Framework in Training Large Models for Automated Optimization Modeling</strong></summary>The authors propose OR-Instruct, a framework that uses GPT-4 to synthesize over 32k optimization modeling pairs (natural language to COPT code) to fine-tune 7B-scale models (ORLM). They demonstrate that these fine-tuned models outperform GPT-4 on their new 'IndustryOR' benchmark, a result that appears robust given the specialized nature of the task. The most valuable takeaway is their specific data augmentation strategy‚Äîiteratively altering constraints and injecting specific modeling techniques (e.g., Big M)‚Äîwhich provides a concrete recipe we can steal to generate diverse instances for our OR-Bench project. While the methodology is standard instruction tuning, the resulting artifacts (benchmark and model) establish a new baseline for automated OR modeling that we cannot ignore.</details></td><td><small>Chenyu Huang et.al.</small></td><td><small>Columbia University, Duke University, Shanghai Jiao Tong University, The Chinese University of Hong Kong, Shenzhen, Shenzhen Research Institute of Big Data, Shanghai University of Finance and Economics, Cardinal Operations</small></td><td><small>Operations Research (2025), published online ahead of print</small></td><td><small><a href="http://arxiv.org/abs/2405.17743">pdf</a> / <a href="https://github.com/cardinal-operations/orlm">code</a></small></td></tr>
<tr><td><small>22/30</small></td><td><small>2025-03-12</small></td><td><details><summary><strong>Automatic Operator-level Parallelism Planning for Distributed Deep Learning -- A Mixed-Integer Programming Approach</strong></summary>She et al. formulate distributed LLM training/inference as a Flexible Distributed Job Shop Scheduling Problem (FDJSSP) solved via Mixed-Integer Programming (MIP) combined with a heuristic graph coarsening step. They demonstrate that this automated approach not only reproduces DeepSeek V3's expert-designed &quot;DualPipe&quot; strategy but, when allowed to search longer, discovers a schedule with 50% fewer pipeline bubbles. The primary takeaway is the effectiveness of the bi-level optimization framework (greedy merging + MIP) to handle the scale of operator-level graphs, proving that formal OR methods can outperform manual system design for LLM infrastructure. This is a mandatory read for our GPUSched project, offering a concrete formulation for operator-level constraints we can directly adapt.</details></td><td><small>Ruifeng She et.al.</small></td><td><small>Huawei</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2503.09357">pdf</a> / code</small></td></tr>
<tr><td><small>22/30</small></td><td><small>2025-03-11</small></td><td><details><summary><strong>Robust Multi-Objective Controlled Decoding of Large Language Models</strong></summary>RMOD formulates multi-objective decoding as a zero-sum game between a policy and adversarial weights, solving a convex optimization problem at each decoding step to maximize the worst-case value estimate (essentially a Process Reward Model). The results are empirically strong, outperforming MO-DPO and scalarized baselines on alignment benchmarks by dynamically preventing any single objective from collapsing. **Key Takeaway:** The efficient inference-time weight optimization algorithm (Eq. 10) is a 'stealable' mechanism for **AlgoEvo** and **RobustMAS**. We should implement this dynamic adversarial weighting to balance conflicting code metrics (e.g., runtime vs. solution quality) during evolutionary search, replacing our current static scalarization methods.</details></td><td><small>Seongho Son et.al.</small></td><td><small>University College London, University of Basel, Ulsan National Institute of Science and Technology</small></td><td><small>ICLR</small></td><td><small><a href="http://arxiv.org/abs/2503.08796">pdf</a> / <a href="https://github.com/williambankes/robust-multi-objective-decoding">code</a></small></td></tr>
<tr><td><small>25/30</small></td><td><small>2025-03-05</small></td><td><details><summary><strong>Helix: Serving Large Language Models over Heterogeneous GPUs and Network via Max-Flow</strong></summary>Helix formulates distributed LLM serving on heterogeneous clusters as a max-flow problem, using MILP to optimize model placement and deriving a per-request weighted round-robin scheduler from the flow solution. Unlike standard static pipeline parallelism, it routes every request dynamically based on edge capacities, achieving up to 3.3x throughput gains over Swarm on mixed GPU clusters (L4/T4/A100). The results are rigorous, backed by both physical cluster experiments and high-fidelity simulations. The critical takeaway is the 'per-request pipeline' abstraction: decoupling request routing from static device assignment allows exact OR methods to maximize utilization of weaker hardware‚Äîa technique we should immediately evaluate for our GPUSched project.</details></td><td><small>Yixuan Mei et.al.</small></td><td><small>Carnegie Mellon University</small></td><td><small>International Conference on Architectural Support for Programming Languages and Operating Systems</small></td><td><small><a href="http://arxiv.org/abs/2406.01566">pdf</a> / <a href="https://github.com/Thesys-lab/Helix-ASPLOS25">code</a></small></td></tr>
<tr><td><small>19/30</small></td><td><small>2025-02-25</small></td><td><details><summary><strong>OCCAM: Towards Cost-Efficient and Accuracy-Aware Classification Inference</strong></summary>OCCAM formulates the inference model selection problem as an Integer Linear Program (ILP), using a nearest-neighbor estimator on validation embeddings to predict query-specific model accuracy. The authors provide theoretical guarantees for the estimator's bias and variance, demonstrating 40% cost reduction on ImageNet with <1% accuracy drop compared to heuristic baselines. The key takeaway is the **training-free, NNS-based accuracy estimator** combined with ILP; this avoids training complex routers and provides statistical guarantees. This is directly applicable to our **LLM serving optimization** (GPUSched) work for routing prompts between models of varying costs, and potentially for estimating fitness in **AlgoEvo** without full execution.</details></td><td><small>Dujian Ding et.al.</small></td><td><small>University of British Columbia</small></td><td><small>ICLR</small></td><td><small><a href="http://arxiv.org/abs/2406.04508">pdf</a> / <a href="https://github.com/DujianDing/OCCAM.git">code</a></small></td></tr>
<tr><td><small>18/30</small></td><td><small>2025-02-22</small></td><td><details><summary><strong>Mixture Compressor for Mixture-of-Experts LLMs Gains More</strong></summary>Huang et al. propose a compression framework for MoE-LLMs that uses Integer Programming to optimally allocate mixed bit-widths (1-3 bits) to experts based on activation frequency and routing weights. They achieve strong empirical results, compressing Mixtral 8x7b to ~16GB (fitting on a single RTX 3090) with only a ~4% drop in zero-shot accuracy, significantly outperforming uniform quantization. The key takeaway is the explicit IP formulation for minimizing quantization error under memory constraints‚Äîa clean 'OR for AI' pattern we can adapt for our GPU scheduling or memory allocation formulations. While not a methodological advance in evolution, this is highly relevant for our infrastructure: it enables deploying high-quality MoE models on cheaper hardware for our massive AlgoEvo loops.</details></td><td><small>Wei Huang et.al.</small></td><td><small>The University of Hong Kong, The Chinese University of Hong Kong, Beihang University, Centre for Perceptual and Interactive Intelligence, Hong Kong</small></td><td><small>ICLR</small></td><td><small><a href="http://arxiv.org/abs/2410.06270">pdf</a> / <a href="https://github.com/Aaronhuang-778/MC-MoE">code</a></small></td></tr>
<tr><td><small>23/30</small></td><td><small>2025-02-14</small></td><td><details><summary><strong>Hybrid Offline-online Scheduling Method for Large Language Model Inference Optimization</strong></summary>Pang et al. formulate LLM inference scheduling as a Mixed-Integer Programming (MIP) model, solving it via a hybrid approach: offline bin-packing for request assignment and an online Lagrangian heuristic for prefill-decode preemption. They report a ~9% utilization increase (80.2% to 89.1%) over a vLLM-style baseline on LLaMA-65B, though the evaluation is limited to a single 8-GPU node and assumes deterministic output lengths for the offline component. The most actionable takeaway is their derivation of a simple cost-comparison threshold (prefill cost vs. decode wait cost) to dynamically inject prefill tasks into decoding streams. This provides a concrete, low-overhead heuristic baseline for our GPUSched work.</details></td><td><small>Bowen Pang et.al.</small></td><td><small>Noah‚Äôs Ark Lab, Huawei, Tsinghua University</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2502.15763">pdf</a> / code</small></td></tr>
<tr><td><small>21/30</small></td><td><small>2025-02-11</small></td><td><details><summary><strong>FlexSP: Accelerating Large Language Model Training via Flexible Sequence Parallelism</strong></summary>FlexSP optimizes distributed LLM training by dynamically assigning varied-length sequences to heterogeneous Sequence Parallelism (SP) groups using a Mixed-Integer Linear Programming (MILP) solver in the loop. The results are solid, showing up to 1.98x speedup on A100 clusters by mitigating communication bottlenecks for short sequences while preventing OOM for long ones. **Key Takeaway:** The authors use Dynamic Programming to 'bucket' similar sequences, drastically reducing the variable count for the MILP solver; this specific technique‚Äîreducing problem granularity to make exact solvers feasible in real-time systems‚Äîis directly applicable to our 'GPUSched' and inference resource allocation work. While we focus on evolution, this is a definitive reference for our 'OR for AI Systems' track, proving that formal optimization can beat heuristics in dynamic GPU scheduling.</details></td><td><small>Yujie Wang et.al.</small></td><td><small>Peking University, ByteDance Inc., Beihang University</small></td><td><small>International Conference on Architectural Support for Programming Languages and Operating Systems</small></td><td><small><a href="http://arxiv.org/abs/2412.01523">pdf</a> / <a href="https://github.com/PKU-DAIR/Hetu-Galvatron">code</a></small></td></tr>
<tr><td><small>24/30</small></td><td><small>2025-02-10</small></td><td><details><summary><strong>MoETuner: Optimized Mixture of Expert Serving with Balanced Expert Placement and Token Routing</strong></summary>Go et al. formulate the MoE expert placement problem as a two-stage Integer Linear Program (ILP) to balance token load and minimize communication tail latency, exploiting stable token routing dependencies across layers. They demonstrate real-world speedups of 17.5% on multi-node H200 clusters running Mixtral-8x7B, validating the approach with concrete systems measurements rather than just simulation. The key takeaway is the effectiveness of a min-max ILP objective for reducing tail latency in distributed inference, proving that static optimization based on profiling is sufficient for significant gains. This directly supports our 'OR for AI systems' track and provides a strong baseline formulation for our GPU scheduling work.</details></td><td><small>Seokjin Go et.al.</small></td><td><small>Georgia Institute of Technology</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2502.06643">pdf</a> / code</small></td></tr>
<tr><td><small>20/30</small></td><td><small>2025-01-15</small></td><td><details><summary><strong>MEMO: Fine-grained Tensor Management For Ultra-long Context LLM Training</strong></summary>Memo enables training 7B LLMs with 1M context on 8 GPUs by combining token-wise activation swapping with a bi-level Mixed Integer Programming (MIP) approach to eliminate memory fragmentation. The results are strong (52% MFU vs ~30% for DeepSpeed) and demonstrate that static memory planning via OR solvers outperforms dynamic allocators for repetitive Transformer workloads. The key takeaway is the bi-level MIP strategy‚Äîsolving the allocation for one layer and broadcasting it‚Äîwhich makes the NP-hard memory planning tractable. We should adapt this MIP formulation for our own GPU scheduling and inference resource allocation (GPUSched) projects.</details></td><td><small>Pinxue Zhao et.al.</small></td><td><small>Peking University, Tencent Inc.</small></td><td><small>Proc. ACM Manag. Data</small></td><td><small><a href="http://arxiv.org/abs/2407.12117">pdf</a> / <a href="https://github.com/pinxuezhao/MEMO">code</a></small></td></tr>
<tr><td><small>23/30</small></td><td><small>2025-01-07</small></td><td><details><summary><strong>A Sequential Optimal Learning Approach to Automated Prompt Engineering in Large Language Models</strong></summary>Wang et al. treat prompt engineering as a Bayesian optimal experimental design problem, representing prompts as discrete feature vectors (template, tone, examples) and selecting the next candidate using a Knowledge-Gradient (KG) policy solved via mixed-integer second-order cone programming. Results are rigorous and show that this OR-based approach outperforms evolutionary (EvoPrompt) and bandit baselines on instruction induction tasks, specifically in low-sample regimes (N=30). The critical takeaway is the **replacement of random evolutionary mutation with a KG policy over a structured feature space** to maximize information gain per step. We should steal this formulation to optimize high-level meta-parameters or strategy selection in AlgoEvo, leveraging our team's OR background to solve our sample efficiency bottleneck.</details></td><td><small>Shuyang Wang et.al.</small></td><td><small>Northwestern University, Stevens Institute of Technology</small></td><td><small>Conference on Empirical Methods in Natural Language Processing</small></td><td><small><a href="http://arxiv.org/abs/2501.03508">pdf</a> / code</small></td></tr>
<tr><td><small>23/30</small></td><td><small>2024-12-11</small></td><td><details><summary><strong>Fundamental Limits of Prompt Compression: A Rate-Distortion Framework for Black-Box Language Models</strong></summary>Nagle et al. formalize prompt compression as a rate-distortion problem, deriving the fundamental theoretical limit via a dual linear program and proposing 'Adaptive QuerySelect,' a variable-rate compression technique. The results are rigorous: they calculate exact limits on synthetic data and use beam search approximations for NLP, demonstrating that existing fixed-rate methods leave significant performance on the table. The key takeaway is that **variable-rate compression**‚Äîkeeping tokens based on a confidence threshold rather than a fixed percentage‚Äîis essential for approaching optimality; this allows 'hard' queries to retain more context while aggressively compressing 'easy' ones. This is immediately actionable for our AlgoEvo work: we should replace fixed-window history truncation with a query-aware, variable-rate compressor to maximize the useful information in our limited context window.</details></td><td><small>Alliot Nagle et.al.</small></td><td><small>UT Austin, EPFL</small></td><td><small>NeurIPS</small></td><td><small><a href="http://arxiv.org/abs/2407.15504">pdf</a> / <a href="https://github.com/acnagle/fundamental-limits">code</a></small></td></tr>
<tr><td><small>24/30</small></td><td><small>2024-09-13</small></td><td><details><summary><strong>B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests</strong></summary>Chen et al. derive a Bayesian posterior estimator (B4) for selecting correct code solutions using unreliable (LLM-generated) tests, explicitly modeling the probability of incorrect code passing incorrect tests. They demonstrate statistically significant improvements (up to 50% relative gain on hard problems) over state-of-the-art heuristics like CodeT and MaxPass on HumanEval and APPS. The key takeaway is the B4 scoring formula: a product of four Beta functions that weighs consensus sets based on priors about test reliability (e.g., incorrect code rarely passes incorrect tests). This is immediately actionable for AlgoEvo: we can replace our naive fitness aggregation with B4 to improve selection accuracy when using generated unit tests, directly boosting sample efficiency.</details></td><td><small>Mouxiang Chen et.al.</small></td><td><small>Zhejiang University, Singapore Management University</small></td><td><small>ASE' 24 (full paper)</small></td><td><small><a href="http://arxiv.org/abs/2409.08692">pdf</a> / <a href="https://github.com/ZJU-CTAG/B4">code</a></small></td></tr>
<tr><td><small>18/30</small></td><td><small>2024-09-11</small></td><td><details><summary><strong>Beyond IID: Optimizing Instruction Learning from the Perspective of Instruction Interaction and Dependency</strong></summary>The authors propose optimizing SFT data mixtures using Linear Programming (EE-CPO) by modeling the 'interaction' (synergy/antagonism) between instruction categories, rather than treating them as IID. They empirically derive a dependency taxonomy showing Math and Code are fundamental 'root' capabilities required before learning complex tasks, validating this via curriculum learning experiments that beat DEITA. The results are solid (+1.73 AlpacaEval over DEITA), though the cost of deriving the interaction matrix (training N models) is high. **Takeaway:** The 'Effect Equivalence Coefficient' matrix combined with an LP solver is a rigorous OR formulation for resource/data allocation that we should steal to optimize heuristic populations in our evolutionary search frameworks.</details></td><td><small>Hanyu Zhao et.al.</small></td><td><small>Beijing Academy of Artificial Intelligence</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2409.07045">pdf</a> / <a href="https://github.com/BAAI-DIPL/sft-set-optimization-via-instruction-interaction-and-dependency">code</a></small></td></tr>
<tr><td><small>19/30</small></td><td><small>2024-07-18</small></td><td><details><summary><strong>Improving GPU Multi-Tenancy Through Dynamic Multi-Instance GPU Reconfiguration</strong></summary>MIGRator formulates dynamic NVIDIA MIG partitioning as an Integer Linear Program (ILP) to optimize a compound 'Goodput' metric (SLO + accuracy) for continuous learning workloads. The results on A100s show ~20% gains over baselines like Ekya and PARIS, largely by mitigating the massive ~6s MIG reconfiguration overhead via a 'pre-initialization' lookahead strategy. For our GPUSched project, the key takeaway is the explicit modeling of reconfiguration penalties in the ILP and the technique of pre-assembling instances during idle time to hide latency. While the reliance on 200-second traffic prediction is a potential fragility, the rigorous handling of hardware constraints makes this a strong reference for our OR-based resource allocation work.</details></td><td><small>Tianyu Wang et.al.</small></td><td><small>UC San Diego, University of Pittsburgh, University of Arizona, University of Georgia</small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2407.13126">pdf</a> / code</small></td></tr>
<tr><td><small>‚Äî</small></td><td><small>2024-06-18</small></td><td><strong>City-LEO: Toward Transparent City Management Using LLM with End-to-End Optimization</strong></td><td><small>Zihao Jiao et.al.</small></td><td><small></small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2406.10958">pdf</a> / code</small></td></tr>
<tr><td><small>‚Äî</small></td><td><small>2024-05-30</small></td><td><strong>Large Language Model Watermark Stealing With Mixed Integer Programming</strong></td><td><small>Zhaoxi Zhang et.al.</small></td><td><small></small></td><td><small>arXiv.org</small></td><td><small><a href="http://arxiv.org/abs/2405.19677">pdf</a> / code</small></td></tr>
</tbody></table>

</details>
