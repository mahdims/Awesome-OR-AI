You are a senior research scientist with deep expertise in operations research, combinatorial optimization, machine learning, and LLM-based approaches. Read this paper carefully and extract structured information.

IMPORTANT: Be concise and precise. Be technical and precise. Use short, clear values â€” no verbose descriptions. Keep lists short. Focus on concrete details, not vague descriptions.

Extract the following information (your response will be automatically structured as JSON):

0. AFFILIATIONS
   - Extract author affiliations as a single comma-separated string, sorted by institutional prominence (most famous/prestigious first). Use short names (e.g., "DeepMind, MIT, Stanford, Tsinghua"). If not found, use empty string.

1. PROBLEM DEFINITION
   - Formal problem name (full name, not just initials)
   - Short abbreviation (e.g., CVRPTW, JSP, TSP)
   - Problem class: Use a descriptive label. Common examples: routing, scheduling, packing, network_design, facility_location, resource_allocation, algorithm_design, llm_evolutionary_search, program_synthesis, satisfiability, graph_optimization, multi_agent_coordination, scientific_discovery. If none fit, create a new descriptive label.
   - Properties list (e.g., ["capacitated", "time_windows", "stochastic", "dynamic"])
   - Scale of instances tackled (e.g., "50-200 nodes", "up to 1000 jobs")

2. METHODOLOGY
   - Core method: Be VERY specific (e.g., "PPO with graph attention network policy" not just "reinforcement learning")
   - LLM role: "none" | "heuristic_generator" | "evaluator" | "code_writer" | "decomposition_guide" | "prompt_optimizer" | "evolutionary_search" | "research_agent" | or describe a new role if none fit
   - LLM model used: Exact model name if applicable (GPT-4, Claude, Llama-3, etc.) or null
   - Search type: "constructive" | "improvement" | "hybrid" | "exact" | "sampling"
   - Novelty claim: ONE sentence describing what's genuinely new
   - Components: List all algorithmic components used
   - Training required: true/false

3. EXPERIMENTAL SETUP
   - Benchmarks: Exact names (e.g., ["Solomon R101", "Gehring-Homberger", "TSPLIB"])
   - Baselines: List all methods compared against (with citations if available)
   - Hardware: GPU/CPU specs and training time
   - Instance sizes: List of tested sizes as integers

4. RESULTS
   - vs_baselines: Dictionary mapping each baseline to gap description (e.g., {"LKH3": "-2.3% on Solomon-100", "HGS": "+0.8%"})
   - scalability: One sentence on how performance changes with problem size
   - statistical_rigor: Describe statistical reporting (number of runs, variance, significance tests)
   - limitations_acknowledged: List any limitations authors explicitly mention

5. ARTIFACTS
   - code_url: GitHub/GitLab URL if code is released, else null
   - models_released: true if trained models are shared
   - new_benchmark: true if paper introduces new benchmark dataset

6. CONFIDENCE
   - For each section above, rate your extraction confidence: "high" (clearly stated in paper), "medium" (inferred from context), or "low" (uncertain/ambiguous). Flag any fields where you are guessing.

CRITICAL: If information is not in the paper, use null or empty lists. DO NOT INVENT DATA.
