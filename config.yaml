base_url: "https://arxiv.paperswithcode.com/api/v0/papers/"
user_name: "YOUR_USERNAME"
repo_name: "or-llm-daily"
show_authors: True
show_links: True
max_results: 100

publish_readme: True
publish_gitpage: True

# file paths
json_readme_path: './docs/or-llm-daily.json'
json_gitpage_path: './docs/or-llm-daily-web.json'

md_readme_path: 'README.md'
md_gitpage_path: './docs/index.md'

# keywords to search (order here controls display order in output)
keywords:
    "LLMs for Algorithm Design":
        description: "Using large language models for automatic algorithm design, evolutionary search for heuristics, automated heuristic generation, program synthesis, and LLM-guided optimization algorithm discovery"
        filters:
            - "Automatic Algorithm Design"
            - "LLM Algorithm Design"
            - "Evolutionary Algorithm LLM"
            - "LLM Evolutionary Search"
            - "Evolution of Heuristics"
            - "FunSearch"
            - "ReEvo"
            - "AlphaEvolve"
            - "LLM Program Synthesis"
            - "LLM Program Search"
            - "Automated Heuristic Design"
            - "LLM Metaheuristic"
            - "LLM Algorithm Generation"
            - "Algorithm Evolution"
            - "Algorithm Discovery"
            - "Evolutionary Computation LLM"
            - "LLM Genetic Algorithm"
            - "LLM Code Generation Optimization"
    "Generative AI for OR":
        description: "Using generative AI and large language models for operations research, mathematical optimization modeling, combinatorial optimization, and automated formulation of optimization problems"
        seed_papers:
            - "2405.17743"  # ORLM: A Customizable Framework in Training Large Models for Automated Optimization Modeling
            - "2310.06116"  # OptiMUS: Optimization Modeling Using MIP Solvers and large language models
            - "2303.08233"  # NL4Opt: Competition on Natural Language Processing for Optimization
    "OR for Generative AI":
        description: "Using operations research and optimization techniques to improve LLM serving, inference efficiency, GPU scheduling, KV cache management, memory optimization, and resource allocation for large language models"
        filters:
            - "LLM Serving Optimization"
            - "LLM Inference Optimization"
            - "Efficient LLM Inference OR"
            - "LLM Scheduling"
            - "GPU Scheduling"
            - "KV Cache Optimization"
            - "Model Serving Optimization"
            - "LLM Resource Allocation"
            - "LLM Throughput Optimization"
            - "LLM Latency Optimization"
            - "LLM Deployment Optimization"
            - "LLM Memory Optimization"
            - "Batch Scheduling Inference"
            - "Request Scheduling LLM"
            - "Optimization LLM Serving"
            - "Integer Programming LLM"
            - "Mixed Integer Programming LLM"
            - "Linear Programming LLM"
            - "Combinatorial Reasoning"
