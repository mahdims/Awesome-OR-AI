--- Page 1 ---
ARLO: A Tailorable Approach for Transforming
Natural Language Software Requirements into
Architecture using LLMs
Tooraj Helmi, University of Southern California, thelmi@usc.edu
Abstract—Software requirements expressed in natural lan-
guage (NL) frequently suffer from verbosity, ambiguity, and
inconsistency. This creates a range of challenges, including
selecting an appropriate architecture for a system and assessing
different architectural alternatives. Relying on human expertise
to accomplish the task of mapping NL requirements to archi-
tecture is time-consuming and error-prone. This paper proposes
ARLO, an approach that automates this task by leveraging (1)
a set of NL requirements for a system, (2) an existing standard
that specifies architecturally relevant software quality attributes,
and (3) a readily available Large Language Model (LLM).
Specifically, ARLO determines the subset of NL requirements
for a given system that is architecturally relevant and maps
that subset to a tailorable matrix of architectural choices. ARLO
applies integer linear programming on the architectural-choice
matrix to determine the optimal architecture for the current
requirements. We demonstrate ARLO’s efficacy using a set of
real-world examples. We highlight ARLO’s ability (1) to trace
the selected architectural choices to the requirements and (2) to
isolate NL requirements that exert a particular influence on a
system’s architecture. This allows the identification, comparative
assessment, and exploration of alternative architectural choices
based on the requirements and constraints expressed therein.
I. INTRODUCTION
Software systems are ever-evolving, with increases in size
and complexity that call for careful elicitation of require-
ments and architectural choices [1]. While it has been long
recognized that requirements and architecture co-evolve [2],
[3], understanding their interactions, especially early in the
software development process, is still an open challenge.
More specifically, there is a scarcity of knowledge regarding
their alignment, architecture-to-requirements traceability, and
conserving architectural knowledge [4].
Most software requirements are still captured using natural
language (NL) [5]–[7]. The informal nature of NLs is a signif-
icant obstacle in machine processing of such requirements [8].
In the past, researchers have proposed approaches to classify
the NL requirements into different categories of functional
and non-functional requirements [5], [6], to identify quality
attributes from NL requirements, to facilitate architectural
decisions by leveraging machine learning [7], and so on. While
promising, these approaches require large, curated datasets and
high model-building effort [9].
Translating NL requirements to architecture-related design
decisions is a cumbersome task that has long been recognized
in the research community [1]. In practice, architects’ deci-
sions often rely entirely on their intuition, experience, and
deep understanding of the software product [10], [11]. While
they may have the requisite domain knowledge, architects
must manually deal with large numbers of NL requirements
that may be ambiguous, incomplete, and internally inconsis-
tent [12], [13]. In addition, the architects’ decisions tend to
reflect their familiarity with and preference for specific design
and/or technological choices. All this frequently leads to sub-
optimal choices for the software system under development.
These challenges underscore the importance of an effective
methodology for analyzing NL requirements and making cor-
responding architectural decisions. Recent research on apply-
ing LLMs in software engineering, particularly in processing
requirements, has shown promise. For example, a study by
White et al. [14] shows how LLMs like ChatGPT improve soft-
ware engineering tasks such as generating API specifications
from requirements lists. Another study by Belzner et al. [15]
highlights how LLMs can assist in generating class design and
corresponding pseudocode of the important classes and their
relationships. While these approaches demonstrate specific
applications of LLMs in addressing certain aspects of software
engineering, they do not provide a comprehensive solution for
using LLMs to make broader architectural decisions.
This paper introduces ARLO, a novel approach for arriving
at an effective Architecture from NL Requirements using
LLMs and Optimization algorithms. The use of LLMs is
central to our approach. While current state-of-the-art zero-
shot LLMs are not perfect, and the quality of their output
can vary, they outperform humans when processing large
text corpora. On the other hand, while LLMs excel in pro-
cessing textual information, they do not possess the expert
understanding that a human may have in deriving informed
architectural decisions from specific requirements. To address
this, we suggest utilizing Quality Attributes (QAs) to bridge
the gap between NL requirements and architectural choices.
This approach enables an Large Language Models (LLMs)
to concentrate on extracting QAs, which we hypothesize the
LLM can do effectively. The QAs used in our approach are
based on those defined in the ISO/IEC 25010 standard [16]:
functional suitability, performance efficiency, compatibility,
interaction capability, reliability, security, maintainability, and
flexibility. Our approach is motivated by the observation that,
while there are potentially countless domain- and application-
specific architectural choices in practice, existing zero-shot
LLMs can deal much more effectively with the small number
of well-understood QAs.
ARLO uses QAs to arrive at the most suitable architectural
arXiv:2504.06143v1  [cs.SE]  8 Apr 2025


--- Page 2 ---
choices. An architectural choice is a distinct design decision
made or strategy pursued in the development of a software
system, shaping its structure, constituent components, their
interactions, and overall behavior. For instance, the deploy-
ment choices for a system may be monolithic, microservices,
or peer-to-peer.
We employ a systematic approach to extract QAs and
organize them in a way that ARLO can utilize effectively
to derive architectural choices. The process begins with us-
ing LLMs to identify architecturally significant requirements
(ASRs), which are requirements that specify key decisions
regarding high-level software architecture. We then extract
crucial information for each ASR, including implied QAs
and existing conditional statements. Next, we apply a specific
algorithm to group similar conditions into Condition Groups
(CGs). Subsequently, we use LLMs to identify Concurrent
Condition Groups (CCGs), which are CGs that can be true
simultaneously. ARLO will then suggests distinct architectural
choices for each CCG.
ARLO relies on a matrix that captures architectural knowl-
edge to select architectural choices based on the desired QAs.
Values in the matrix indicate an architect’s assessment of the
impact of each choice on a QA. For example, selecting the
always-on data caching strategy may impose a performance
penalty compared to offline first. ARLO’s matrix is designed
to be simple to create, flexible to include varying architectural
needs and preferences, and tailorable to capture new knowl-
edge or system constraints. Considering the desired QAs and
their respective weights, calculated based on their frequency
of implication in the requirements, ARLO uses the matrix to
determine the optimal architectural choices by leveraging an
Integer Linear Programming technique.
We have applied ARLO in three real-world examples,
demonstrating 1) its scalability measured as its execution
time for software systems with requirements ranging from
several hundred to several thousand and 2) its sensitivity to
the chosen matrix and specific requirements called architecture
influencing requirements (AIRs). An AIR (or a set of AIRs)
is an ASR that, when removed, will change the architectural
choices selected by ARLO.
In summary, our contributions are as follows:
1) A novel approach for automating software architecture
decision-making based on arbitrary natural language
requirements by leveraging zero-shot LLMs.
2) Establishing a connection between architectural deci-
sions and requirements through quality attributes while
incorporating the architect’s insights into how these
attributes influence architectural choices.
3) Introducing the concept of architecturally influencing
requirements (AIRs), driving the architectural decisions.
4) Publicly available implementation of ARLO.
The remainder of the paper is organized as follows. Sec-
tion II delves into existing research on the role of LLMs
in requirements engineering and the influence of QAs on
software architecture. Section III presents ARLO’s design
and implementation. Section IV details its application on
three real-world projects. Limitations and threats to validity
are discussed in Section V. Lastly, conclusions and future
directions are outlined in Section VI.
II. RELATED WORK
Standards like ISO/IEC 25010 define essential QAs [16].
The influence of QAs on architectural choices [17] and
software design decisions [7] is well-documented [18]–[20].
While some RE approaches recognize the importance of QAs
and Non-Functional Requirements (NFRs) [21], [22], they are
often implicitly integrated with functional requirements [23],
[24]. Quality concerns like security, performance, and main-
tainability are key in shaping a system’s architecture and often
require trade-offs between functional and quality requirements;
for instance, the Twin Peaks model [4] highlights the iterative
relationship between requirements and architecture.
Historically, identifying QAs has been manual and labor-
intensive [25], [26]. Recent advancements in NLP have en-
abled automated approaches [27]–[29], such as extracting
QAs from user stories [30] and classifying user reviews from
app stores [31]. The automated extraction and visualization
of quality concerns in [32], [33] uses data mining and ML
to identify quality-related requirements from specifications,
helping stakeholders understand their impact across projects.
Research shows that while precise prediction of a system’s
architecture that achieves desired QAs is challenging [34],
assessments of how architectural decisions impact QAs are
feasible [35]. This can be accomplished via architectural
tactics [36], decision maps [7], fuzzy mathematics [9], and
economic modeling [37]. However, these approaches do not
prescribe how to arrive at architectural choices.
Design Structure Matrices (DSM) and design rule the-
ory (DRT) are widely used to analyze software modularity
and evolution, as demonstrated by MacCormack et al. and
Baldwin [38], [39]. DSM also supports decision-making and
analyzes architectural decay [40]. Tools like Titan [41] link
software design with quality analysis, while Huynh et al. [42]
automate UML transformations into decision models, and Cai
et al. [43] offers a framework for deriving DSMs to understand
modularity better. In ARLO, we use a matrix to link QAs to
architectural choices to determine the optimum decisions.
Another contribution of ARLO is traceability. Software
traceability links artifacts like requirements, code, and tests,
ensuring systems meet client specifications. Recent methods
improve manual processes by using ML, ontology, and in-
formation retrieval (IR), achieving up to 95% accuracy [44].
Hybrid approaches combining ML and reasoning outperform
traditional IR by accounting for artifact relationships [45].
ARLO automates traceability by mapping architecturally sig-
nificant and influential requirements to QAs using LLMs
and optimization, bypassing static associations via textual
similarity used in earlier methods.
Recent work has endeavored to use LLMs to derive software
architecture aspects. Eisenreich et al. [46] propose a method
to generate a system’s architecture semi-automatically based
on requirements using modern AI techniques using manual


--- Page 3 ---
Fig. 1: Overview of ARLO
iterations, with the architect using prompts to tell the tooling
what aspects of the architecture candidate need to be changed.
[47] recover software architecture from code units using
deductive reasoning supported by LLMs. Karetnikov et al. [48]
use knowledge graphs for analyzing architecture decision
records, allowing them to be reused on a cross-project basis.
Soliman et al. [49] explore manual retrieval of architectural
knowledge from web search engines, allowing architects to
make well-founded design decisions. ARLO enhances existing
approaches by automating the extraction of ASRs and mapping
them to QAs using LLMs. This automation streamlines a
traditionally labor-intensive process.
III. THE ARLO APPROACH
ARLO automates the selection of an appropriate software
Architecture from a set of NL Requirements by leveraging
LLMs and Optimization methods. ARLO supports decision-
making by providing appropriate recommendations for archi-
tectural choices. It provides a process of architecture dis-
covery [3] by leveraging the power of LLMs to identify
architecturally significant requirements (ASRs) and a set of
impacted quality attributes (QAs).
ARLO maps the ASRs to corresponding QAs and conditions
under which each ASR may occur. Consequently, ARLO
generates a collection of concurrent condition groups, each
comprising a set of conditions that can simultaneously hold
during the system’s operation. Lastly, ARLO leverages an
off-the-shelf integer linear programming optimizer to select
the best architectural choices to recommend to the architect.
Figure 1 highlights the three key steps involved in ARLO:
(1) extraction of ASRs, (2) traceability of architectural de-
cisions by grouping desired QAs, and (3) selecting optimal
architectural choices. ARLO also employs various concepts to
tackle the complex problem of making architectural choices.
Table I provides an overview of these key concepts, while
Figure 2 presents a UML diagram illustrating the relationships
between them.
Before detailing ARLO’s major steps and implementation
details,
we
introduce
a
running
example
to
illustrate
the discussion. To this end, we use Urban Messaging
System (UMS), designed for high-density areas and capable of
adapting between normal and disaster-response modes. UMS
was chosen to demonstrate ARLO’s ability to adapt to different
operational conditions, each with specific desired QAs.
TABLE I: Key Concepts in ARLO’s Approach
Concept
Description
Architecturally
Significant
Requirement (ASR)
A requirement that directly impacts the archi-
tecture of the system. ASRs imply one or more
QAs that need to be fulfilled.
Quality
Attribute
(QA)
Characteristics that measure the quality of a sys-
tem, such as performance, reliability, security,
etc. In ARLO, QAs serve as the bridge between
requirements and architectural choices.
Condition
Group
(CG)
A group of equivalent conditions from the lin-
gual perspective.
Concurrent
Condi-
tion Group (CCG)
A collection of CGs that can be true simultane-
ously.
Decision Group
A category of architectural decisions, such as
deployment strategy, data caching method, etc.
Each group contains multiple choices.
Choice
A specific architectural option within a Decision
Group, such as "Monolith" or "Microservices"
within the "Deployment" Decision Group.
Decision
A choice selected within a decision group.
Table II lists ten requirements for UMS, with corresponding
relevant QAs. Some requirements (R4, R5, and R10) may not
imply any QAs; in other words, ARLO should not detect
them as ASRs. On the other hand, multiple ASRs may
target the same QA, impacting its weight. Among UMS’s ten
requirements, six are identified as ASRs and mapped to two
conditions groups (right-most column)—normal operation (1)
and infrastructure failure (2)—as elaborated below.
A. ARLO’s Step 1: Extraction of ASRs Using LLM
We use the LLM with Prompt 1 from Table III to extract
a list of ASRs with associated QAs and, if applicable, any
conditional statements. Each prompt in Table III is designed to
clearly define the LLM’s expected task based on the provided
data, followed by an explanation of specialized concepts,
such as the definition of ASRs or a list of QAs and their
meanings. This structured prompting aligns with approaches
demonstrated to be effective in instruction-tuning LLMs [50].
ARLO employs the eight QAs shown in Table IV, the
first seven of which are design-oriented QAs defined in the
ISO/IEC 25010 standard that targets software product qual-
ity [16]. The standard also includes functional suitability and
safety, which we have not included. Functional suitability does
not directly relate to architectural choices but addresses the
Fig. 2: ARLO’s view of a software system


--- Page 4 ---
TABLE II: UMS requirements and ARLO’s outputs.
ID
Description
ASR
QA
CG
R1
System
must
support
sub-second
message delivery under normal op-
erating conditions.
Y
Performance
1
R2
During infrastructure failure, the sys-
tem must automatically utilize neigh-
boring devices to ensure delivery.
Y
Reliability
2
R3
System should prioritize message de-
livery based on urgency during an
emergency, as determined by embed-
ded NLP algorithms.
Y
Usability,
Reliability
2
R4
System should offer both biometrics
and social authentication.
N
N/A
-
R5
System updates should be deploy-
able remotely without interrupting
ongoing messaging services.
N
N/A
-
R6
During a disaster, the system guaran-
tees eventual message delivery with-
out a specific time constraint.
Y
Reliability
2
R7
In normal conditions, the system
must encrypt messages end-to-end to
ensure privacy and security.
Y
Security
1
R8
It should integrate with existing so-
cial platforms for wider accessibility.
N
N/A
-
R9
In case of a disaster, the system
should send alerts to all users.
Y
Usability,
Reliability
2
R10 It must allow messages up to 10K
words during normal conditions and
up to 100 words during emergencies.
N
N/A
-
characteristics of application-specific functional requirements,
while safety is process- and runtime-oriented as defined in
the standard. As an added eighth QA, we have incorporated
cost efficiency since architectural decisions often depend on
cost considerations of project implementation, operation, and
maintenance.
While the list of QAs in Table IV is broadly applicable, it
is important to note that ARLO does not mandate or depend
on the specific choice of QAs; the list in Table IV can be
augmented as appropriate. For example, ARLO could readily
incorporate the subdimensions of the QAs from the ISO/IEC
TABLE III: GPT Prompts used by ARLO
Prompt 1. Parsing Requirements
I have provided a set of software requirements. I want you to extract the
following information:
• Whether it is architecturally significant. A requirement is
architecturally significant if it satisfies both of these conditions: 1)
It explicitly states a key decision regarding high-level software
architecture. 2) It specifies one or more quality attributes
regarding software architecture: [List of QAs and their
descriptions from Table IV provided here]
• Find the QAs mentioned in the list above.
• The condition that should be true when the QAs are expected.
Prompt 2. Determining Condition Groups
If the following conditions [List of conditions] mean the same thing,
one can infer another or be considered a subset of another, return ‘True.’
Otherwise, return ‘False.’
Prompt 3. Determining Concurrent Condition Groups
Organize the provided set of conditions into groups where conditions in
the same group can be true simultaneously. Once grouped, simply return
the IDs of the conditions in each group enclosed in parentheses.
TABLE IV: QAs defined in ISO/IEC 25010, used by ARLO
QA
Description
Performance
Efficiency
(PE)
Relates to the performance relative to the resources
used under stated conditions. Sub-characteristics in-
clude time behavior, resource utilization, and capacity.
Compatibility
(CO)
Assesses software’s ability to co-exist with independent
software in a common environment sharing resources. It
includes interoperability, co-existence, and compliance.
Interaction
Capability
(IC)
Measures how easy and satisfying the software is. It
covers appropriateness recognizability, learnability, op-
erability, user error protection, user interface aesthetics,
and accessibility.
Reliability
(RE)
Measures the software’s capacity to maintain its perfor-
mance level under stated conditions for a stated period.
It includes maturity, fault tolerance, and recoverability.
Security (SE)
Covers the software’s ability to protect informa-
tion and data, ensuring confidentiality, integrity, non-
repudiation, accountability, and authenticity.
Maintainability
(MA)
Measures how easy it is to modify the software. It
includes modularity, reusability, analyzability, modifi-
ability, and testability.
Flexibility
(FL)
Measures the ease with which the software can be
transferred from one environment to another. It includes
adaptability, installability, replaceability, and flexibility
compliance.
Cost
Efficiency
(CE)
Emphasizes minimizing financial resources in software
development, maintenance, and operation to stay within
budget.
25010 standard (such as recoverability, a subdimension of
reliability, or modularity, a subdimension of maintainability)
by updating Prompt 1. Similarly, although the motivation
behind this work is to explore any zero-shot LLM, we have
opted to employ GPT4o model APIs since GPT-4o is known
for its proficiency in processing and interpreting complex
language data [51].
As shown in Figure 3, the output of this step of ARLO
is a set of requirements classified as ASRs, each associated
with specific QAs and possibly conditions under which the
requirement occurs. If an ASR is unrelated to any condition
(“N/A” in Table II), ARLO implicitly assigns it the default
condition “under any circumstances”. For instance, the first
[ {
"Id": 1,
"IsArchitecturallySignificant": true,
"QualityAttributes": ["Performance Efficiency"],
"ConditionText": "under normal operating conditions" },
{
"Id": 2,
"IsArchitecturallySignificant": true,
"QualityAttributes": ["Reliability"],
"ConditionText": "In the event of infrastructure
,→failure" },
{
"Id": 3,
"IsArchitecturallySignificant": true,
"QualityAttributes": ["Usability", "Reliability"],
"ConditionText": "during an emergency" },
{
"Id": 4,
"IsArchitecturallySignificant": false,
"QualityAttributes": [],
"ConditionText": "N/A" },
...
]
Fig. 3: An excerpt from ARLO’s Step 1 Output for UMS


--- Page 5 ---
requirement in Table II is correctly classified as ASR: it
implies performance efficiency as a QA since it demands a
sub-second message delivery. ARLO also correctly identifies
its condition as “under normal operating conditions” as shown
in Figure 3. This step shapes subsequent architectural decision-
making, as detailed below.
B. ARLO’s Step 2: Traceability by Grouping Desired QAs
Once the ASRs, their corresponding QAs, and the condi-
tions under which they hold are obtained, they are grouped
based on the conditions so that ARLO can select a set of
architectural choices for each group. As illustrated in Figure 4,
ARLO first uses LLM APIs [52] to generate embeddings
for each requirement. These embeddings serve as input to
hierarchical clustering [53], which groups requirements based
on similar conditions. Notably, clustering is employed to
minimize the number of LLM calls, which is the most time-
intensive part of ARLO’s process, as shown later in this
section. Hierarchical clustering is particularly well-suited here
because it does not require a predetermined number of clusters,
allowing for flexibility in grouping based on a varying number
of conditions.
Fig. 4: Clustering ASRs before Applying Algorithm 1
Algorithm 1 Categorize ASRs into Condition Groups
1: Input: List of ASRs within each cluster.
2: Output: List of condition groups within each cluster.
3: groups ←[ ]
4: for all asr in asrs do
5:
cond ←extract_cond(asr)
6:
group_found ←false
7:
for all group in groups do
8:
if logic_equiv(cond, group[’nominal_cond’]) then
9:
group[‘asrs’].append(asr)
10:
group_found ←true
11:
break
12:
end if
13:
end for
14:
if not group_found then
15:
new_group ←{nominal_cond: cond, asrs: [asr]}
16:
groups.append(new_group)
17:
end if
18: end for
ARLO runs Algorithm 1 within each cluster with Prompt 2
from Table III to determine the condition groups (CGs). The
algorithm creates a list variable called groups (line 1). The
algorithm forms a new group in the first iteration. It adds it to
groups since no existing condition group exists. In subsequent
iterations, it tries to add more ASRs to an existing group or
add a new group if needed. For each group, we keep one of the
conditions as the “nominal condition”. Each ASR’s condition
is extracted and compared with the nominal conditions of
existing groups (lines 2-6). If the condition of the ASR is
logically equivalent to the nominal condition of an existing
group (based on LLM’s determination using prompt 2 in
Table III), the ASR is added to that group (lines 7-10). If
there is no logical equivalence with any existing group, a
new group is formed with this new condition as its nominal
condition, and the ASR is added to this group (lines 12-15).
It is possible for a requirement to contain multiple conditions,
meaning it could belong to more than one CG. Although the
current implementation of Algorithm 1 does not handle this
scenario, a careful tracking mechanism will be necessary to
avoid double-counting the implied QAs.
Note that Algorithm 1 can be used directly to form CGs.
However, doing so requires O(n2) pairwise comparisons,
invoking the LLM API, which takes l time units (with
l ≈100ms/token for GPT-4o). The total time would thus be
O(n2 · l). To reduce API calls, we apply clustering, which is
CPU-bound with complexity O(n3) [54]. Once the clustering
is done, we run Algorithm 1 within each cluster. For k clus-
ters, each with n/k requirements on average, the complexity
becomes O(n2/k) per cluster. The overall complexity is,
therefore,
O(n3) + O
 n2
k · l

In the above formula, the first term specifies the cluster-
ing complexity, and the second term specifies the reduced
complexity in calling LLM APIs. The l factor is orders of
magnitude larger than the time required for CPU operations,
thus reducing API calls shifts the main workload to local CPU
resources. Clustering provides practical efficiency, especially
for large-scale systems. However, it is important to note that
this may slightly reduce the accuracy of generating CGs, as
clustering based on embedding similarity may not be as precise
as the lingual equivalency determined by LLMs [55].
As an example of applying Algorithm 1, consider UMS
requirements R1 and R6 from Table II, both of which are
ASRs. Initially, since there is no existing group, R1 is added to
a newly created condition group CG1 with “normal operating
conditions” as its nominal condition. When R2 is evaluated,
its condition “In the event of infrastructure failure” is deemed
not logically equivalent to the condition of the first group.
Hence, it is added to a newly created condition group CG2.
For each condition group, ARLO picks the QAs associated
with the conditions included in that group. For example, since
both R1 and R7 are members of CG1, their respective QAs,
performance efficiency and security, are associated with CG1.
Once the sets of CGs are determined, the LLM is used with
Prompt 3 from Table III to determine which CGs can be


--- Page 6 ---
simultaneously true. Recall that we refer to such CGs as
concurrent condition groups (CCGs).
In the case of UMS, three nominal conditions are derived
from ASRs: normal condition (NC), disaster response (DR),
under any circumstances (UAC). We pass these nominal con-
ditions as the input to LLM: 1) NC, 2) DR, and 3) UAC. We
get (1, 3) and (2, 3) as the output, denoting pairs (NC, UAC)
and (DR, UAC) can co-occur, thus forming two CCGs which
we refer to as CCG1 and CCG2 subsequently. Note that it is
impossible to have a case where both NC and DR are true
simultaneously, while UAC always holds and can coexist with
any other condition.
In our approach, determining the weight of each QA is
critical, as it reduces architects’ bias and improves architec-
ture alignment with ASRs. While stakeholders could directly
provide these weights to reflect their priorities, ARLO derives
these weights empirically based on the ASRs. Specifically,
ARLO currently quantifies the weight of each QA by counting
the number of ASRs that imply this particular QA. This can
indicate the QA’s priority as emphasized by several require-
ment. More sophisticated approaches could also be explored to
determine QA priorities. For instance, analyzing the linguistic
nuances within the requirements could reveal the criticality
associated with a QA. For UMS, we ob the following weights:
Performance: 1 (implied by R1), Reliability: 4 (implied by
(R2, R3, R6, and R9), Usability: 2 (implied by R3 and R9),
and Security: 1 (implied by R7). These weights are then used
in the next step to guide the selection architectural choices.
C. ARLO’s Step 3: Architectural Choice Selection
In this step, the CCGs identified in the previous phase are
leveraged to guide architectural decisions. ARLO employs
a decision matrix to align the QAs with architectural de-
cisions. In this matrix, rows represent architectural decision
categories (e.g., deployment strategies, caching mechanisms),
while columns represent the QAs. Note that architectural
categories and choices used in rows are neither exhaustive
nor prescriptive. They are illustrative examples intended to
demonstrate how our approach can be applied and tailored
to suit different projects or organizations’ specific needs and
contexts. Table V shows a subset of the decision matrix rele-
vant to the UMS case whose ASRs imply 4 QAs. (Section IV
shows cases where all QAs are implied.). The values shown in
the table are based on the authors’ experience; as previously
mentioned, they can be customized to align with the unique
requirements of specific projects.
The matrix values (+1), (0), or (-1) indicate whether a
particular architectural choice supports, is neutral toward, or
conflicts with a specific QA, respectively. An architect can
specify these values based on their understanding of how
decisions impact different QAs. For instance, based on the
values we have chosen and shown in Table V, Microservices
positively impact Performance Efficiency (+1). This positive
impact is justified because Microservices enable independent
scaling of different services based on their load, thereby opti-
mizing resource allocation to achieve the desired performance
TABLE V: QA-Arch Choice Mapping Matrix used with UMS
Group
Choice
PE
IC
RE
SE
Deployment
Monolith
0
0
-1
0
Microservices
1
0
0
0
P2P
0
0
1
-1
Data
Caching
Always-on
-1
1
1
1
Offline First
1
1
1
-1
Comm.
API-Call
0
0
0
1
Message-Based
1
0
1
0
WebSocket
1
1
0
-1
Data Repl.
Central
0
0
-1
0
Hot-Hot
1
0
1
0
Hot-Cold
0
0
1
0
Database
Mgmt.
SQL
0
0
0
0
NoSQL
1
0
1
0
Polyglot Persistence
1
0
1
0
Security
Proactive
1
0
0
1
Reactive
0
1
0
0
Data
Synch.
Real-time
1
0
0
0
Batch Processing
0
1
1
0
by dedicating resources only where they are needed [56].
In contrast, Offline-first data caching can negatively impact
Security (-1). This negative impact is justified because storing
multiple copies of data locally increases the risk of exposure,
heightening the chances of unauthorized access [57] or side-
channel attacks [58] and thereby, potential security breaches.
We then use integer linear programming (ILP) to maximize
the QA satisfaction scores, thus selecting the optimal architec-
tural choices. ILP aligns with the nature of our optimization
goal in which each decision is binary and aims to optimize the
total weighted sum of selections [59]. The optimizer considers
the mutual exclusivity of choices within each group of choices.
For example, ILP can select only one deployment option.
Note that other approaches could be used instead of ILP. For
instance, Dynamic Programming efficiently solves problems
with overlapping sub-problems but requires substantial mem-
ory [60]. Genetic Algorithms are also suitable for exploring
large search spaces without needing gradient information, but
they do not guarantee a globally optimal solution [61].
Below is a formal definition of the optimization problem.
The goal is to maximize the overall satisfaction score, defined
as the sum of weighted satisfaction scores for each QA given
a matrix M with dimensions n × m, each row represents an
architectural choice, and each column represents a QA. The
matrix element Mij denotes the effectiveness of choice i in
achieving QA j. Choices are grouped, and exactly one choice
per group must be chosen. In the following formula xi is a
binary decision variable indicating the selection of choice i
and Wj is the weights for QAj:
Scorej =
n
X
i=1
Mij × xi
(1)
Maximize
m
X
j=1
(Scorej × Wj) over xi
(2)
It is important to stress that the core of our work is not
in the optimization algorithm itself but in how we model
architectural decisions and formulate the objective function.
The novelty lies in defining the decision matrix, which allows


--- Page 7 ---
TABLE VI: Choices for the Messaging System
Choice Group
CCG1
CCG2
Deployment
Microservices
P2P
Data Caching
Always-on
Offline First
Communication
API-Call
Message-Based
Data Replication
Hot-Hot
Hot-Hot
DB Mgmt
NoSQL
NoSQL
Security Strategy
Proactive
Proactive
Data Synchronization
Real-Time
Batch Processing
TABLE VII: QA Satisfaction Scores for the Messaging System
CCG1 (Normal)
CCG2 (Disaster)
Performance
4
0
Reliability
0
24
Usability
0
2
Security
3
0
the problem to be treated as a linear optimization problem
given by Equation 2. We use Google’s linear optimization tool
(OR-tools) [62] to determine the most suitable architectural
choices based on the objective derived from the desired QAs
and their weights for each condition group.
Returning to the UMS case, we run ILP once for each
condition group. The results of this process, reflecting the
most effective alignment of architectural choices with the
specified QAs under each condition group, are presented in
Table VI. These architectural choices are particularly tailored
for scenarios such as disaster response, where the likelihood
of a centralized infrastructure supporting other deployment
strategies (e.g., microservices) is significantly reduced.
Table VII presents the satisfaction scores for QAs under
each condition. Under normal conditions, the desired QAs are
performance and security, with weighted scores of 4 and 3. The
choice of a microservices deployment enhances performance,
while an always-on data caching and API-call communication
further contribute to this QA. The proactive strategy bolsters
the system’s security capabilities.
By contrast, operation during disaster response emphasizes
reliability with a higher weighted score of 24 and interaction
capability with a weighted score of 2. Note that these scores
are calculated using Equation 1. This emphasis on reliability
makes sense, given its higher weight. The optimal configu-
ration for this group includes a P2P deployment, which sig-
nificantly enhances reliability. Message-based communication
and batch data synchronization also support this emphasis
on reliability. An offline-first data caching strategy enhances
Interaction Capability by allowing the system to function
seamlessly even with intermittent or no internet connectivity.
It achieves this by utilizing cached data and synchronization
once connectivity is restored [63].
This precise alignment of architectural choices with priori-
tized QAs mapped to various CCGs demonstrates the rigor of
our method in architectural decision-making. It is important to
note that CCGs are not mutually exclusive: a CG can belong
to multiple CCGs. Each CCG provides a set of architectural
decisions optimized for the nominal conditions of its associ-
ated CGs. The development team can review these CCGs and
proactively implement the system to accommodate decisions
from multiple CCGs. Furthermore, at runtime, a monitoring
system can assess the current operational conditions and
activate the appropriate architecture based on which CCG’s
conditions are met.
D. ARLO’s Implementation
ARLO has been implemented in C#. The current prototype
implementation consists of 1,626 source lines of code (SLOC).
ARLO relies on two third-party packages: OpenAI’s GPT-
4 Completions API [64] and Google’s linear optimization
package [65]. ARLO’s complete source code is available at
https://anonymous.4open.science/r/ARLO-0BED.
IV. EVALUATION
To establish our approach’s practical relevance, we evalu-
ated three distinct cases, thoroughly examined in this section.
The criteria for selecting these projects were full accessibility
to requirements and a considerable ratio of ASRs (in our
case, 15% or higher). This threshold was chosen to ensure
a substantial focus on architectural considerations.
With these cases, we aim to investigate the following re-
search questions, with answers provided based on our findings
from three case studies:
• RQ1 – How does ARLO scale when applied to software
with varying numbers of requirements? This RQ aims
to determine ARLO’s applicability to large, real-world
systems.
• RQ2 – How does change of requirements or the input
matrix influence ARLO’s outcomes? This RQ’s aim is to
evaluate ARLO’s sensitivity to requirements changes.
• RQ3 – How do different conditions in requirements in-
fluence architectural choices? This RQ aims to evaluate
ARLO’s tailorability to different requirements.
A. Dataset Description
We selected three software systems—Atlassian Bamboo,
Appcelerator Aptana, and Spring XD—with publicly available
requirements [66] to evaluate ARLO under different contexts
and numbers of requirements. Table VIII summarizes the three
systems’ key statistics.
As shown in Table VIII, the selected systems vary in size,
with Bamboo having 522 requirements and Spring XD much
larger with 3,759 requirements. Notably, in all three systems,
at least 20% of the requirements are identified as ASRs by
ARLO and utilized to determine architectural choices.
B. Experiment Results
We executed ARLO’s steps 1 and 2 as outlined in Section III
to derive ASRs, identify the QAs associated with each ASR,
group ASRs into CGs based on their conditions, and establish
TABLE VIII: Case Statistics
REs
Cleaned-Up
ASRs
Conditional
CCGs
Bamboo
522
374
94
84
11
Aptana
830
771
186
231
17
Spring XD
3759
3056
969
728
12


--- Page 8 ---
CCGs. As shown in Table VIII, each case contains several
CCGs. Due to space constraints, we only show the results for
the first CCG within each case, and the rest can be found in
the appendix.1 Table IX displays the QA count for each case,
revealing that some QAs are implied more frequently than
others. ARLO selects the best options that satisfy the most
frequently occurring QAs. This is a significant advantage of
using ARLO, as human architects typically do not prioritize
preferred QAs and instead aim to design an architecture that
satisfies most QAs regardless of their priority [67].
TABLE IX: QA Counts for Three Cases
Case
PE
CO
IC
RE
SE
MA
FL
CE
Bamboo
8
7
15
13
9
16
4
2
Aptana
32
4
36
6
1
42
7
3
Spring XD
143
71
201
99
54
245
82
9
Finally, we applied ARLO’s step 3 to identify the archi-
tectural decisions for each case. Table X shows the choices
made for each case. As we can see, the results vary across
cases. The main driver for the results is the QA counts given
in Table IX. For instance, we see that a monolith deployment
has been chosen for both Spring XD and Aptana since they
have a relatively high count for performance efficiency (PE).
As we can see in the matrix, PE is positively linked with
a monolith choice for deployment. This is not the case for
Bamboo, which has a relatively low weight for PE.
TABLE X: ARLO’s Results for Three Cases
Group
Bamboo
Aptana
Spring XD
Deployment
Microservices
Monolith
Monolith
Caching
Offline First
Offline First
Offline First
Communication
Message-Based
Message-Based
API-Call
Data Repl.
Hot-Cold
Central
Hot-Cold
DBMS
SQL
SQL
SQL
Security
Proactive
Proactive
Proactive
Data Synch.
Batch Processing
Real-time Sync
Real-time Sync
The significance of these findings lies in ARLO’s ability
to adjust architectural decisions based on the specific QA
profiles of each project. By leveraging the QA counts, ARLO
ensures that the architecture is tailored to meet the most critical
QAs. This contrasts with traditional manual approaches, where
architects may overlook the nuanced importance of individual
QAs, potentially leading to sub-optimal choices.
1) RQ1 – ARLO’s Scalability: Our analytical treatment
(provided in online appendix2) proves that selecting CGs is
the most computationally intensive component within ARLO
with an O(n2) complexity. Table XI shows ARLO’s expected
execution times for software with different numbers of re-
quirements. In this table, the first column shows the number
of requirements, the second column shows the iteration count
required in Algorithm 1, and the last two columns show the
best and worst-case running time. In the worst case, each
requirement is an ASR and has a condition, and all conditions
1https://anonymous.4open.science/r/ARLO-0BED/Appendix%20B.pdf
2https://anonymous.4open.science/r/ARLO-0BED/Appendix%20A.pdf
TABLE XI: ARLO’s Scalability
System Size
# Algo 1 Iters
Best Case
Worst Case
Small ∼100 REs
4,950
10 Sec
8 Min
Medium ∼1000 REs
499,500
16 Min
13 Hours
Large ∼2000 REs
1,999,000
1 Hour
55 Hours
X Large ∼5000 REs
12,497,500
7 Hours
14 Days
are mutually exclusive. This means the inner loop is Algo-
rithm 1 has to repeat equal to the number of n groups added
before it. So the total will be 0+1+...+n−1 = n(n−1)/2.
Based on the three cases presented earlier, the ASR ratio
ranges from 18% to 25%, and the conditional requirements
ratio ranges from 16% to 27%. Therefore, for the best-
case scenario, we assumed a 15% ASR ratio and a 15%
conditional ratio, which is strictly less than the range of
values in the dataset. To calculate the times, we assumed it
would take approximately 1s per LLM API call (20 tokens
per requirement times 50ms per token
[68]). The time is
calculated as the number of Iterations (column 2) times 1s
for the worst case and scaled down by 0.0225 (15% ASRs
times 15% conditionals) to obtain the best case values.
We ran ARLO with the matrix shown in Table XII. We
then measured the time needed to complete the process. The
results align with the analytical expectations. ARLO processes
Bamboo, which has 374 requirements (REs), in 10 minutes,
and Aptana, with 771 REs, in 4 minutes. Both processing times
fall within the expected range of 10 seconds to 16 minutes,
as detailed in Table XI, which shows that it takes 10 seconds
for 100 REs and 16 minutes for 1000 REs. Similarly, ARLO
processes Spring XD, with 3056 REs, in 20 minutes, falling
within the expected range of 16 minutes to 1 hour.
TABLE XII: The Matrix Used to Experiment with Three Cases
Group
Choice
PE
CO IC
RE
SE
MA FL
CE
Deployment
Monolith
1
0
1
-1
1
0
-1
-1
Microservices
0
1
-1
0
0
-1
1
0
Peer-to-Peer
-1
-1
0
1
-1
1
0
1
Caching
Always-On
-1
0
1
-1
1
-1
0
1
Offline-First
1
0
-1
1
-1
1
0
-1
Communi-
cation
API Calls
-1
-1
1
0
1
1
-1
0
Messaging
0
0
0
1
-1
-1
1
0
Web-Socket
1
1
-1
-1
0
0
0
0
Data
Replication
Centralized
-1
0
0
-1
0
1
0
1
Hot-Hot
1
0
0
0
1
-1
0
-1
Hot-Cold
0
0
0
1
-1
0
0
0
DBMS
SQL
-1
1
0
-1
-1
1
0
1
NoSQL
1
-1
0
1
1
-1
0
-1
Security
Proactive
0
0
-1
0
1
1
0
-1
Real-time
0
0
1
0
-1
-1
0
1
Data
Synch.
Reactive
1
0
1
-1
0
-1
1
-1
Batch
-1
0
-1
1
0
1
-1
1
Based on our observations, ARLO can scale and adapt
to increasing requirements, significantly reducing the time
required compared to a manual approach. This scalability is
particularly important for large and complex projects.
2) RQ2 – ARLO’s Sensitivity: RQ2 concerns what pa-
rameters used by ARLO have the most impact on its recom-
mendations. Our experiments show that the two major factors
impacting ARLO’s results are the matrix used and a particular


--- Page 9 ---
set of requirements, which we call architecturally influential
requirements (AIRs). While ASRs contain information that
generally determine architectural decisions, AIRs are a sub-
set that directly determines which ASRs, individually or in
combination, can impact a specific decision.
a) The matrix configuration: If the matrix is configured to
prefer choices equally, with the same sum across all rows for
a given group, it won’t impact ARLO’s results, as the linear
optimization treats all options equally. However, values set in
the matrix can make a choice preferred by having a higher
row sum. This indicates that architects using ARLO should
carefully configure the matrix to obtain desirable results. If
the matrix is unbalanced, they should ensure they can justify
their preferences for any given choice.
b) Size of AIRs set: AIRs can be key in determining
ARLO’s choices. AIRs imply QAs to which the optimizer’s
solution is the most sensitive. We can have cases where a
single ASR forms an AIR or several ASRs form an AIR
set. To select AIRs, 1) we first perform a sensitivity analysis
to determine which QAs the results are most sensitive to.
This involves applying deviations in QA counts ranging from
10% to 90%. The sensitivity is determined by observing how
these adjustments affect the decisions. The QAs with most
number of changes are selected as the most sensitive, 2) ASRs
implying the most sensitive QA are removed individually.
ARLO is then run to observe if the decisions change, 3) If
the decisions change, an AIR set is identified, consisting of
all the removed ASRs till the last decision change.
Figure 5 shows that smaller systems like Bamboo have
a higher frequency of smaller AIR sets, meaning that indi-
vidual or small cluster requirements significantly influence
the architecture. In contrast, larger systems like Spring XD
tend to have AIR sets that encompass a broader range of
requirements, reflecting the increased complexity of mak-
ing the right architectural decisions in larger, more intricate
systems. Both Bamboo and Aptana have a higher number
of AIR sets consisting of a single requirement. In contrast,
Spring XD’s smallest AIR set consists of 15 ASRs, which
is approximately 1% of the total ASRs. This suggests that in
large-scale projects, architectural decisions are less sensitive to
individual requirements and more dependent on the collective
influence of multiple interrelated requirements.
Impact analysis: We analyzed two single-AIR cases (Bam-
boo, Aptana) and one multi-AIR case (Spring XD) to assess
TABLE XIII: Smallest AIR set for Spring XD
ID
Values
Description
R789
RE, IC
Validate negative pageSize values in controllers.
R853
PE, RE
Add connection pool to Redis connection factory.
R857
IC, RE
RabitMQ sink module connection props override issue.
R859
RE, IC
JDBC sink deletes table despite ’initDatabase=false’.
R898
IC, RE
Shutdown servers using –cluster-name param.
R899
IC, RE,
PE
Use cleanup app from XD-1243 to stop previous CI
runs on EC2.
R917
IC, RE
Improve stream state management.
R924
RE
Cannot access JMX endpoints with Boot Snapshot.
...
...
...
1
10
20
30
40
50
60
70
80
90
100
100
101
102
103
AIR Set Size
Occurrence
Bamboo Aptana Spring XD
Fig. 5: The AIR set size distribution
ARLO’s sensitivity to requirements changes. Single-AIR cases
are simpler to validate, while the multi-AIR case highlights
ARLO’s ability to trace complex dependencies beyond manual
methods. Note that the behavior of ARLO is independent of
the requirement’s complexity thus the chosen requirements
generally represent the ASRs in the project.
a) Bamboo R127: Reports indicate that Bamboo some-
times leaves 1orphaned’ elastic instances and detached EBS
volumes. Add functionality to allow admins to view and
shut down instances not currently connected to Bamboo.:
Removing requirement R127, addressing reliability and cost
efficiency, shifts from an ‘offline-first’ data caching to‘always-
on’ approach. R127 requires removing storage instances not
connected to Bamboo, thus enhancing cost efficiency by
reducing extra storage and ensuring reliability by avoiding
outdated data. ARLO’s decision to choose ‘always-on’ can
be explained as follows: In Table XII, ‘always-on’ is less
cost-efficient and less reliable than ‘offline-first’ [69], [70]. By
removing R127, the emphasis on cost efficiency and reliability
is reduced. Thus, while both strategies are equal for reliability,
‘always-on’, which is less cost-effective, is preferred.
Note that this transition would not have been easy for a hu-
man architect since the language of R127 focuses on a specific
operational issue—removing orphaned storage instances. A
human architect might address this requirement by implement-
ing the specific functionality without considering the broader
architectural implications. The requirement does not explicitly
mention the ‘offline-first’ or ‘always-on’ strategies, so linking
this operational fix to a change in the data caching approach
requires a deeper analysis that is not immediately obvious.
b) Aptana R437: Our current implementation of our JS
parsing infrastructure does not allow the parser to be run
outside of Eclipse. We need to extract the minimal set of
classes that will de-couple our implementation from Eclipse.:
Removing requirement R437, which addresses flexibility and
maintainability, shifts from using SQL for DBMS to NoSQL.
R437 recommends allowing Aptana to run outside of Eclipse,
enhancing flexibility by decoupling it from Eclipse and im-
proving maintainability by making it easier to run and debug
from the command line. Removing R437 causes a transition
to NoSQL because, in the Table XII, Both SQL and NoSQL


--- Page 10 ---
are equally preferred for flexibility, but SQL is favored for
maintainability [71]. (An equal degree of flexibility is assumed
because NoSQL offers greater adaptability to different types
of data [72], whereas SQL excels in terms of transferability
across environments [73]. Since both adaptability and transfer-
ability are components of the flexibility definition provided in
Table IV, this results in an overall equal impact.) With R437
removed, the emphasis on both flexibility and maintainability
is reduced. Therefore, there is less emphasis on maintainabil-
ity, making NoSQL a viable option.
Similar to the previous case, this transition would not
have been easy for a human architect to arrive at. The lan-
guage of R437 focuses on a specific technical implementation
detail—decoupling the JS parser from Eclipse. A human
architect might address this requirement by implementing
the decoupling without considering the broader architectural
implications. The requirement does not explicitly mention the
impact on the database management system, so linking this
technical change to a shift from SQL to NoSQL requires a
deeper analysis that is not immediately apparent.
ARLO’s approach systematically analyzes interdependen-
cies across requirements and provides recommendations based
on comprehensive data analysis, something that might be
infeasible manually. The automated process can detect patterns
and correlations that a human might overlook.
c) Spring XD AIR set: As evident in Table IX, the QA
counts are significantly higher for Spring XD. This increase is
due to the higher number of ASRs, implying more QAs. Con-
sequently, the likelihood of having small AIR sets decreases.
More ASRs must be removed to significantly change QA
weights that can influence ARLO’s decision-making process.
ARLO’s ability to determine AIR sets in larger systems
offers two key advantages for architects. First, identifying
AIRs manually in smaller systems is feasible but impractical
in larger ones. Table XIII shows the smallest AIR set for
Spring XD (only the first few requirements are described).
Although not immediately apparent, ARLO identifies that
these requirements all relate to reliability. Removing them
could compromise reliability, so ARLO suggests transitioning
from Hot-Cold to Centralized data replication. This demon-
strates ARLO’s ability to detect patterns and assist architects
in making informed decisions. Second, ARLO limits the
number of AIR sets, allowing for manageable reviews. Without
focusing on AIRs, architects would need to consider all ASR
combinations, which is impractical.
3) RQ3
–
Impact
of
Concurrent
Condition
Groups
(CCGs): Each CCG implies certain conditions that must be
met for the CCG to occur when the system executes. Since
each CCG includes QAs from different ASRs, we expect
ARLO’s results to vary across different CCGs. Our experi-
ments confirm this variation, but the changes are insignificant
due to the overlap among CCGs. Specifically, the “under any
circumstances” group is included in all CCGs and typically has
the most ASRs, greatly influencing QA counts. For example,
in Aptana’s CCG1 and CCG2, the choices for DBMS (SQL
vs. NoSQL) and Security (Proactive vs. Reactive) differ.
This variation is due to different QA counts under each
CCG, as shown in Table XIV. For instance, for CCG1, MA
has the highest count, while IC has the highest count for
CCG2. This results in different choices for DBMS, where MA
demands SQL, while IC demands NoSQL.
TABLE XIV: QA Counts under Different CCGs for Aptana
CCG
MA
IC
PE
RE
CO
FL
CE
SE
CCG 1
46
41
41
11
8
8
3
2
CCG 2
43
45
40
16
7
7
3
1
Specifying different architectural choices under various
CCGs upfront allows the software to be implemented for
specific conditions and transition to appropriate architectures
based on operational conditions.
V. LIMITATIONS AND THREATS TO VALIDITY
Dependence on LLM Accuracy: ARLO relies heavily on
LLMs for extracting ASRs and QAs. Any misinterpretation by
the LLMs, especially in handling complex or ambiguous re-
quirements, could lead to sub-optimal architectural decisions.
A. Internal validity
This threat involves potential biases in selecting architec-
tural choices and QAs based on ISO/IEC 25010. To enhance
validity, future work will involve broader expert input and
using multiple LLMs to ensure consistency and reduce bias.
B. Construct validity
The configuration of the QA-architecture mapping matrix
significantly shapes the decisions ARLO recommends. An
imbalanced matrix can introduce biases, making certain archi-
tectural choices appear more favorable. This requires careful
calibration to ensure balanced and objective outcomes.
C. External validity
ARLO’s approach has been tested on specific software
systems, which may not fully represent the diversity of all
projects. Further research is needed to validate its applicability
to other domains or more complex systems.
VI. CONCLUSION AND FUTURE DIRECTIONS
In this paper, we introduced ARLO, a novel approach that
leverages LLMs and ILP to automate the translation of natu-
ral language requirements into software architectures. ARLO
uses QAs as a bridge, enabling a systematic and traceable
process for architectural decision-making. Our experiments
demonstrated ARLO’s ability to scale with the number of
requirements and highlighted its sensitivity to key architectural
decisions. ARLO offers a flexible and data-driven tool to
support decision-making processes for architects.
In future research, we would focus on refining the LLMs
to identify architecturally significant requirements more accu-
rately, deriving more sophisticated approaches to determine
QA priorities, extending ARLO’s applicability to specific
domains, exploring the impact of different matrix configura-
tions on architectural outcomes, and handling overlaps among
condition groups occurring due to multi-condition ASRs that
could belong to multiple CGs.


--- Page 11 ---
REFERENCES
[1] P. Grünbacher, A. Egyed, and N. Medvidovic, “Reconciling software
requirements and architectures with intermediate models,” Software &
Systems Modeling, vol. 3, pp. 235–253, 8 2004.
[2] B. Nuseibeh, “Weaving together requirements and architectures,” Com-
puter, vol. 34, no. 3, pp. 115–119, 2001.
[3] T. Spijkman, S. Molenaar, F. Dalpiaz, and S. Brinkkemper, “Alignment
and granularity of requirements and architecture in agile development:
A functional perspective,” Information and Software Technology, vol.
133, 5 2021.
[4] J. Cleland-Huang, R. S. Hanmer, S. Supakkul, and M. Mirakhorli, “The
twin peaks of requirements and architecture,” IEEE software, vol. 30,
no. 2, pp. 24–29, 2013.
[5] Q. A. Shreda and A. A. Hanani, “Identifying non-functional require-
ments from unconstrained documents using natural language processing
and machine learning approaches,” IEEE Access, 2021.
[6] D. Dave and V. Anu, “Identifying functional and non-functional software
requirements from user app reviews,” in 2022 IEEE International IOT,
Electronics and Mechatronics Conference (IEMTRONICS). IEEE, 2022,
pp. 1–6.
[7] P. Lago, “Architecture design decision maps for software sustainability,”
in 2019 IEEE/ACM 41st International Conference on Software Engi-
neering: Software Engineering in Society (ICSE-SEIS).
IEEE, 2019,
pp. 61–64.
[8] A. Umber and I. S. Bajwa, “Minimizing ambiguity in natural language
software requirements specification,” in 2011 Sixth International Con-
ference on Digital Information Management, 2011, pp. 102–107.
[9] N. Esfahani, S. Malek, and K. Razavi, “Guidearch: guiding the explo-
ration of architectural solution space under uncertainty,” in 2013 35th
International Conference on Software Engineering (ICSE). IEEE, 2013,
pp. 43–52.
[10] M. Svahnberg, C. Wohlin, L. Lundberg, and M. Mattsson, “A quality-
driven decision-support method for identifying software architecture
candidates,” International Journal of Software Engineering and Knowl-
edge Engineering, vol. 13, no. 05, pp. 547–573, 2003.
[11] Y. Macit, G. Giray, and E. Tüzün, “Methods for identifying architectural
debt: A systematic mapping study,” in 2020 Turkish National Software
Engineering Symposium (UYMS).
IEEE, 2020, pp. 1–6.
[12] F. Dalpiaz, A. Ferrari, X. Franch, and C. Palomares, “Natural language
processing for requirements engineering: The best is yet to come,” IEEE
software, vol. 35, no. 5, pp. 115–119, 2018.
[13] A. Yadav, A. Patel, and M. Shah, “A comprehensive review on resolving
ambiguities in natural language processing,” AI Open, vol. 2, pp. 85–92,
2021.
[14] J. White, S. Hays, Q. Fu, J. Spencer-Smith, and D. C. Schmidt, “Chatgpt
prompt patterns for improving code quality, refactoring, requirements
elicitation, and software design,” arXiv preprint arXiv:2303.07839,
2023.
[15] L. Belzner, T. Gabor, and M. Wirsing, “Large language model assisted
software engineering: prospects, challenges, and a case study,” in In-
ternational Conference on Bridging the Gap between AI and Reality.
Springer, 2023, pp. 355–374.
[16] ISO/IEC, “Systems and software quality requirements and evalu-
ation (square),” https://www.iso.org/standard/35733.html/, 2023, ac-
cessed: 1/13/2024.
[17] M. A. Al Imran, S. P. Lee, and M. M. Ahsan, “Quality driven architec-
tural solutions selection approach through measuring impact factors,” in
2017 International Conference on Electrical Engineering and Computer
Science (ICECOS).
IEEE, 2017, pp. 131–136.
[18] G. Márquez, H. Astudillo, and R. Kazman, “Architectural tactics in
software architecture: A systematic mapping study,” Journal of Systems
and Software, vol. 197, p. 111558, 2023.
[19] L. Bass, P. Clements, and R. Kazman, Software architecture in practice.
Addison-Wesley Professional, 2003.
[20] M. Svahnberg, C. Wohlin, L. Lundberg, and M. Mattsson, “A method
for understanding quality attributes in software architecture structures,”
in Proceedings of the 14th international conference on Software engi-
neering and knowledge engineering, 2002, pp. 819–826.
[21] A. Moreira, J. Araújo, and I. Brito, “Crosscutting quality attributes
for requirements engineering,” in Proceedings of the 14th international
conference on Software engineering and knowledge engineering, 2002,
pp. 167–174.
[22] D. Domah and F. J. Mitropoulos, “The nerv methodology: A lightweight
process for addressing non-functional requirements in agile software
development,” in SoutheastCon 2015.
IEEE, 2015, pp. 1–7.
[23] A. Borg, A. Yong, P. Carlshamre, and K. Sandahl, “The bad conscience
of requirements engineering: an investigation in real-world treatment of
non-functional requirements,” 2003.
[24] F. Paetsch, A. Eberlein, and F. Maurer, “Requirements engineering and
agile software development,” in WET ICE 2003. Proceedings. Twelfth
IEEE International Workshops on Enabling Technologies: Infrastructure
for Collaborative Enterprises, 2003.
IEEE, 2003, pp. 308–313.
[25] L. Rosenhainer, “Identifying crosscutting concerns in requirements spec-
ifications,” in Proceedings of OOPSLA Early Aspects, vol. 10. Citeseer,
2004.
[26] R. J. Kusters, R. van Solingen, and J. J. Trienekens, “Identifying
embedded software quality: two approaches,” Quality and Reliability
Engineering International, vol. 15, no. 6, pp. 485–492, 1999.
[27] M. Younas, D. N. Jawawi, I. Ghani, and M. A. Shah, “Extraction of
non-functional requirement using semantic similarity distance,” Neural
Computing and Applications, vol. 32, pp. 7383–7397, 2020.
[28] M. Ahmed, S. U. R. Khan, and K. A. Alam, “An nlp-based quality at-
tributes extraction and prioritization framework in agile-driven software
development,” Automated Software Engineering, vol. 30, no. 1, p. 7,
2023.
[29] B. Li, Z. Li, and Y. Yang, “Nfrnet: a deep neural network for automatic
classification of non-functional requirements,” in 2021 IEEE 29th Inter-
national Requirements Engineering Conference (RE).
IEEE, 2021, pp.
434–435.
[30] F. Gilson, M. Galster, and F. Georis, “Extracting quality attributes from
user stories for early architecture decision making,” in 2019 IEEE
international conference on software architecture companion (ICSA-C).
IEEE, 2019, pp. 129–136.
[31] M. Lu and P. Liang, “Automatic classification of non-functional require-
ments from augmented app user reviews,” in Proceedings of the 21st
International Conference on Evaluation and Assessment in Software
Engineering, 2017, pp. 344–353.
[32] M. Rahimi, M. Mirakhorli, and J. Cleland-Huang, “Automated extraction
and visualization of quality concerns from requirements specifications,”
in 2014 IEEE 22nd international requirements engineering conference
(RE).
IEEE, 2014, pp. 253–262.
[33] M. Fazelnia, V. Koscinski, S. Herzog, and M. Mirakhorli, “Lessons from
the use of natural language inference (nli) in requirements engineering
tasks,” arXiv preprint arXiv:2405.05135, 2024.
[34] L. Chung, B. A. Nixon, E. Yu, and J. Mylopoulos, Non-functional
requirements in software engineering.
Springer Science & Business
Media, 2012, vol. 5.
[35] F. Bachmann, L. Bass, M. Klein, and C. Shelton, “Designing software ar-
chitectures to achieve quality attribute requirements,” IEE Proceedings-
Software, vol. 152, no. 4, pp. 153–165, 2005.
[36] F. Bachmann, L. Bass, and M. Klein, “Moving from quality attribute
requirements to architectural decisions.” in STRAW, 2003, pp. 122–129.
[37] R. Kazman, J. Asundi, and M. Klein, “Quantifying the costs and benefits
of architectural decisions,” in Proceedings of the 23rd International
Conference on Software Engineering. ICSE 2001.
IEEE, 2001, pp.
297–306.
[38] A. MacCormack, C. Baldwin, and J. Rusnak, “Exploring the duality be-
tween product and organizational architectures: A test of the “mirroring”
hypothesis,” Research policy, vol. 41, no. 8, pp. 1309–1324, 2012.
[39] C. Y. Baldwin and K. B. Clark, “The option value of modularity in
design,” Harvard NOM Research Paper, vol. 1, pp. 1–14, 2002.
[40] M. J. LaMantia, Y. Cai, A. MacCormack, and J. Rusnak, “Analyzing the
evolution of large-scale software systems using design structure matrices
and design rule theory: Two exploratory cases,” in Seventh Working
IEEE/IFIP Conference on Software Architecture (WICSA 2008).
IEEE,
2008, pp. 83–92.
[41] L. Xiao, Y. Cai, and R. Kazman, “Titan: A toolset that connects
software architecture with quality analysis,” in Proceedings of the 22nd
ACM SIGSOFT International Symposium on Foundations of Software
Engineering, 2014, pp. 763–766.
[42] S. Huynh, Y. Cai, and W. Shen, “Automatic transformation of uml
models into analytical decision models,” Technical Report DU-CS-08-
01, 2008.
[43] Y. Cai and K. J. Sullivan, “Software design spaces: Logical modeling and
formal dependence analysis,” Technical Report CS-2004-19, University
of Virginia, Charlottesville, VA, Tech. Rep., 2004.


--- Page 12 ---
[44] V. Adithya and G. Deepak, “Ontoreq: an ontology focused collective
knowledge approach for requirement traceability modelling,” in Euro-
pean, Asian, Middle Eastern, North African Conference on Management
& Information Systems.
Springer, 2021, pp. 358–370.
[45] T. Li, S. Wang, D. Lillis, and Z. Yang, “Combining machine learning
and logical reasoning to improve requirements traceability recovery,”
Applied Sciences, vol. 10, no. 20, p. 7253, 2020.
[46] T. Eisenreich, S. Speth, and S. Wagner, “From requirements to archi-
tecture: An ai-based journey to semi-automatically generate software
architectures,” arXiv preprint arXiv:2401.14079, 2024.
[47] S. A. Rukmono, L. Ochoa, and M. Chaudron, “Deductive software
architecture recovery via chain-of-thought prompting,” in Proceedings
of the 2024 ACM/IEEE 44th International Conference on Software
Engineering: New Ideas and Emerging Results, 2024, pp. 92–96.
[48] A. Karetnikov, L. Ehrlinger, G. Buchgeher, and V. Geist, “Semantic
modeling of architecture decision records to enable ai-based analysis,”
in 2024 IEEE International Conference on Software Analysis, Evolution
and Reengineering (SANER).
IEEE, 2024, pp. 62–66.
[49] M. Soliman, M. Wiese, Y. Li, M. Riebisch, and P. Avgeriou, “Exploring
web search engines to find architectural knowledge,” in 2021 IEEE 18th
International Conference on Software Architecture (ICSA). IEEE, 2021,
pp. 162–172.
[50] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,
A. M. Dai, and Q. V. Le, “Finetuned language models are zero-shot
learners,” arXiv preprint arXiv:2109.01652, 2021.
[51] Y. Liu, D. Iter, Y. Xu, S. Wang, R. Xu, and C. Zhu, “Gpteval: Nlg
evaluation using gpt-4 with better human alignment,” arXiv preprint
arXiv:2303.16634, 2023.
[52] OpenAI, “New embedding models and api updates,” https://openai.com/
index/new-embedding-models-and-api-updates/, 2024, accessed: 2024-
11-15.
[53] T. Hastie, R. Tibshirani, J. H. Friedman, and J. H. Friedman, The
elements of statistical learning: data mining, inference, and prediction.
Springer, 2009, vol. 2.
[54] W. contributors, “Hierarchical clustering,” https://en.wikipedia.org/wiki/
Hierarchical_clustering, 2024, accessed: September 11, 2024.
[55] M. Freestone and S. K. K. Santu, “Word embeddings revisited: Do llms
offer something new?” arXiv preprint arXiv:2402.11094, 2024.
[56] H. Khazaei, N. Mahmoudi, C. Barna, and M. Litoiu, “Performance
modeling of microservice platforms,” IEEE Transactions on Cloud
Computing, vol. 10, no. 4, pp. 2848–2862, 2020.
[57] B. Ali, M. A. Gregory, and S. Li, “Multi-access edge computing
architecture, data security and privacy: A review,” IEEE Access, vol. 9,
pp. 18 706–18 721, 2021.
[58] F. Liu, H. Wu, K. Mai, and R. B. Lee, “Newcache: Secure cache
architecture thwarting cache side-channel attacks,” IEEE Micro, vol. 36,
no. 5, pp. 8–16, 2016.
[59] A. Ashouri, S. S. Fux, M. J. Benz, and L. Guzzella, “Optimal design and
operation of building services using mixed-integer linear programming
techniques,” Energy, vol. 59, pp. 365–376, 2013.
[60] V. R. Vadiyala and P. R. Baddam, “Exploring the symbiosis: Dynamic
programming and its relationship with data structures,” Asian Journal
of Applied Science and Engineering, vol. 7, no. 1, pp. 101–112, 2018.
[61] G. D’Angelo and F. Palmieri, “Gga: A modified genetic algorithm
with gradient-based local search for solving constrained optimization
problems,” Information Sciences, vol. 547, pp. 136–162, 2021.
[62] G. Inc, “About or-tools or-tools is an open source software suite for
optimization, tuned for tackling the world’s toughest problems in vehicle
routing, flows, integer and linear programming, and constraint pro-
gramming.” https://developers.google.com/optimization, 2023, accessed:
1/15/2024.
[63] Y. Wang, Y. Zhang, X. Han, P. Wang, C. Xu, J. Horton, and J. Culberson,
“Cost-driven data caching in the cloud: An algorithmic approach,” in
IEEE INFOCOM 2021-IEEE Conference on Computer Communications.
IEEE, 2021, pp. 1–10.
[64] OpenAI, “Openai api documentation,” https://platform.openai.com/docs/
api-reference, 2024, accessed: 2024-11-15.
[65] Google, “Google or-tools linear optimization,” https://developers.google.
com/optimization/lp, 2024, accessed: 2024-11-15.
[66] M. Choetkiertikul, H. K. Dam, T. Tran, T. T. M. Pham, A. Ghose, and
T. Menzies, “A deep learning model for estimating story points,” IEEE
Transactions on Software Engineering, vol. PP, no. 99, p. 1, 2018.
[67] M. Kassab and N. Kilicay-Ergin, “Applying analytical hierarchy process
to system quality requirements prioritization,” Innovations in Systems
and Software Engineering, vol. 11, pp. 303–312, 2015.
[68] O.
Community,
“Gpt-3.5
and
gpt-4
api
response
time
measurements
(fyi),”
2023,
accessed:
2024-
11-13.
[Online].
Available:
https://community.openai.com/t/
gpt-3-5-and-gpt-4-api-response-time-measurements-fyi/237394
[69] L.
Onyekwere,
“Offline-first
development,”
2024,
accessed:
2024-11-14.
[Online].
Available:
https://blog.openreplay.com/
offline-first-development
[70] T. N. Stack, “Build more reliable web apps with offline-first principles,”
2023, accessed: 2024-11-14. [Online]. Available: https://thenewstack.io/
build-better-customer-experience-applications-using-offline-first-principles/
[71] S. Bharany, K. Kaur, S. E. M. Eltaher, A. O. Ibrahim, S. Sharma, and
M. M. M. Abd Elsalam, “A comparative study of cloud data portability
frameworks for analyzing object to nosql database mapping from ondm’s
perspective,” International Journal of Advanced Computer Science and
Applications, vol. 14, no. 10, 2023.
[72] J.
Roddam,
“System
design:
Sql
vs
nosql
databases:
A
deep
dive,”
DEV
Community,
2023,
accessed:
2024-
11-14.
[Online].
Available:
https://dev.to/jayaprasanna_roddam/
system-design-sql-vs-nosql-databases-a-deep-dive-922
[73] S. Wickramasinghe, “Sql vs. nosql today: Databases, differences &
when to use which,” Splunk Blog, 2023, accessed: 2024-11-14. [Online].
Available: https://www.splunk.com/en_us/blog/learn/sql-vs-nosql.html
