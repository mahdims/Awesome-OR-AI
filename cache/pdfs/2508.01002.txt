--- Page 1 ---
Optimal Scheduling Algorithms for LLM Inference: Theory
and Practice
AGRIM BARIâˆ—, The University of Texas at Austin, United States
PARIKSHIT HEGDEâˆ—, The University of Texas at Austin, United States
GUSTAVO DE VECIANA, The University of Texas at Austin, United States
With the growing use of Large Language Model (LLM)-based tools like ChatGPT, Perplexity, and Gemini
across industries, there is a rising need for e!cient LLM inference systems. These systems handle requests
with a unique two-phase computation structure: a pre"ll-phase that processes the full input prompt and a
decode-phase that autoregressively generates tokens one at a time. This structure calls for new strategies for
routing and scheduling requests.
In this paper, we take a comprehensive approach to this challenge by developing a theoretical framework
that models routing and scheduling in LLM inference systems. We identify two key design principlesâ€”optimal
tiling and dynamic resource allocationâ€”that are essential for achieving high throughput. Guided by these
principles, we propose the Resource-Aware Dynamic (RAD) scheduler and prove that it achieves throughput
optimality under mild conditions. To address practical Service Level Objectives (SLOs) such as serving requests
with di#erent Time Between Token (TBT) constraints, we design the SLO-Aware LLM Inference (SLAI)
scheduler. SLAI uses real-time measurements to prioritize decode requests that are close to missing their TBT
deadlines and reorders pre"ll requests based on known prompt lengths to further reduce the Time To First
Token (TTFT) delays.
We evaluate SLAI on the openchat_shareGPT4 dataset using the Mistral-7B model on an NVIDIA RTX ADA
6000 GPU. Compared to Sarathi-Serve, SLAI reduces the median TTFT by 53% and increases the maximum
serving capacity by 26% such that median TTFT is below 0.5 seconds, while meeting tail TBT latency constraints.
The complete source code is available at: https://github.com/agrimUT/SLAI.
CCS Concepts: â€¢ Computing methodologies â†’Arti!cial intelligence; Distributed algorithms; Machine
learning; Model development and analysis; â€¢ Theory of computation â†’Theory and algorithms for
application domains.
Additional Key Words and Phrases: LLM inference serving systems, Throughput-optimal systems, SLO aware
schedulers
ACM Reference Format:
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana. 2025. Optimal Scheduling Algorithms for LLM Inference:
Theory and Practice. Proc. ACM Meas. Anal. Comput. Syst. 9, 3, Article 59 (December 2025), 43 pages. https:
//doi.org/10.1145/3771574
âˆ—Both authors contributed equally to this work.
Authorsâ€™ Contact Information: Agrim Bari, agrimchaudhry@gmail.com, Chandra Department of Electrical and Computer
Engineering, The University of Texas at Austin, Austin, TX, United States; Parikshit Hegde, parihegde96@gmail.com,
Chandra Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, United States;
Gustavo de Veciana, deveciana@utexas.edu, Chandra Department of Electrical and Computer Engineering, The University
of Texas at Austin, Austin, TX, United States.
This work is licensed under a Creative Commons Attribution 4.0 International License.
Â© 2025 Copyright held by the owner/author(s).
ACM 2476-1249/2025/12-ART59
https://doi.org/10.1145/3771574
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 2 ---
59:2
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
1
Introduction
LLM inference systems. The core problem in Large Language Model (LLM) inference is to generate
a response autoregressively, one token1 at a time, given a promptâ€”for example, "What is the capital
of France?" producing the output "It is Paris." Modern LLMs such as GPT-4, Llama 3, and Gemini now
power a wide range of services, including chatbots, coding assistants, and search engines. These
services handle millions of user requests daily, and private deployments are rapidly increasing. As
a result, there is growing interest in optimizing how requests are processed across one or more
Graphics Processing Unit (GPU)-enabled nodes in data centers, since improved e!ciency can lead
to signi"cant reductions in infrastructure and operating costs.
Objectives for LLM serving systems. To meet growing demand, LLM systems must be care-
fully designed to make e!cient use of hardware. A well-designed system keeps each active GPU
busyâ€”fully utilizing both its compute and memoryâ€”while also keeping response times low. This
leads to two main goals: (1) achieving high throughput, measured in requests per second, to reduce
the cost per request, and (2) maintaining low latency, which directly a#ects user experience. Latency
is typically measured using two Service-Level Objectives (SLOs)2: Time To First Token (TTFT), which
is the delay between a requestâ€™s arrival and the generation of the "rst output token, capturing how
long a user waits before the LLM starts responding; and Time Between Tokens (TBT), which is the
time between successive output tokens, indicating the rate at which the response is streamed to the
user. In real-world systems, request routing, scheduling, and caching are used to meet these goals.
This paper focuses on scheduling, which plays a critical role in increasing throughput, reducing
TTFT, and keeping TBT within acceptable limits.
Phases of an LLM request and scheduler decisions. Each request to an LLM based on the
Transformer architecture goes through two main phases: pre!ll and decode. In the pre!ll-phase,
the model processes the entire prompt and generates the "rst output token. After that, the request
enters the decode-phase, where it produces one token at a time in an auto-regressive manner until
a stop token is generated. These two phases have distinct characteristics. The pre"ll-phase is
highly parallelizable and can fully utilize the GPUâ€™s compute resources. By contrast, the decode
phase is sequential and has low parallelism, which means that multiple decode-phase requests
must be batched together to make e!cient use of the GPU. Additionally, Transformer models store
intermediate representations of tokensâ€”called the Key-Value (KV) cacheâ€”which grow with the
number of tokens processed and consume GPU memory. The schedulerâ€™s job is to select a mix
of pre"ll-phase and decode-phase requests to include in each GPU batch. These decisions must
balance GPU compute and memory bandwidth usage, stay within the memory budget, and meet
latency SLOs.
Challenges in scheduler design. Designing an e#ective scheduler for LLM inference presents
several key challenges. First, GPU compute e!ciency depends heavily on the composition of each
batchâ€”that is, the mix of pre"ll and decode-phase requests scheduled together. As a result, the
scheduler must construct batches carefully to make the most of available resources. Second, the
scheduler must balance resource allocation between the pre"ll and decode phases, as both phases
have their respective SLOs: TTFT and TBT. Prioritizing pre"ll-phase requests can reduce TTFT
but may delay decodes and worsen TBT. Conversely, prioritizing decode-phase requests keeps
TBT low but can lower compute utilization and increase TTFT for new requests. Third, while the
prompt length is known upon a request arrival the output length is unknown, making memory
1A token in a LLM is a unit of text-such as a word, subword, or character-used as the basic input element for processing and
generation.
2Thresholds may vary by application, but these two metrics are commonly used.
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 3 ---
Optimal Scheduling for LLM Inference
59:3
management complexâ€”particularly since GPU memory is often a bottleneck. Finally, many LLM
serving systems support multiple user tiers which have heterogeneous performance needs.
Our approach. In this work, we approach the LLM scheduling problem from two complementary
perspectives. From a theoretical standpoint, we develop a rigorous framework for analyzing and
achieving throughput-optimal scheduling. From a practical perspective, we design a scheduler that
dynamically adapts to diverse latency SLOs across heterogeneous user tiers.
Contributions. Our key contributions are:
(1) A Theoretical Model for LLM Inference Systems. We develop a comprehensive analytical
framework for request routing and scheduling in LLM inference systems, capable of capturing
salient features of practical policies considered in the literature. Additionally, we introduce a
model to represent inference computation times on modern GPU architectures. This uni"ed
framework enables rigorous analysis and comparison of routing and scheduling policies in
such systems. See Section 3.
(2) A Throughput Optimal Scheduler. We identify two key design principles that characterize
optimal resource-utilization in LLM inference systems; optimal tiling and optimal dynamic
resource allocation across pre"ll and decode workloads. Guided by these insights, we design
a simple load-balancing routing strategy and introduce a Resource-Aware Dynamic (RAD)
scheduler. We rigorously prove that, under mild assumptions, this combination achieves
throughput optimality. See Section 4.
(3) Insights for Practical Systems. Recognizing that in practical LLM inference systems it is
desirable to meet latency SLOs, we provide insights into how real-world systems approximate
and adapt the proposed design principles to satisfy these constraints. See Section 5.
(4) A SLO-Aware Scheduler. We design a practical scheduler called SLO-Aware LLM Inference
(SLAI) scheduler that aims to minimize online median TTFT when serving requests with
heterogeneous TBT constraints. To achieve this, SLAI uses online measurements to decide
when the execution of a decode-iteration has become time-critical, i.e., should be prioritized
for scheduling. In addition, it uses the known prompt length information to order pre"ll-phase
requests so as to reduce the median TTFT. See Section 6.
(5) Experimental Performance. We evaluate SLAI on the openchat_shareGPT4 dataset using
the Mistral-7B LLM on an NVIDIA RTX ADA 6000 GPU. Our results show that SLAI can
reduce the median TTFT by 53% while meeting TBT requirements compared to Sarathi-serve,
the current state-of-the-art scheduler. Additionally, when median TTFT can not exceed 0.5
seconds, SLAI increases the serving capacity by 26%. See Section 7.
1.1
Related Work
LLM serving systems must make decisions about when to run the pre"ll and decode phases of
a request, where to execute each request, and how to manage the growing KV cache produced
by the model. Based on these challenges, prior work can be broadly categorized into three areas:
(a) scheduling within a single inference node, (b) routing across multiple inference nodes, and (c)
managing the KV cache. In this section, we focus exclusively on scheduling. For a discussion of
related works on routing and KV cache management, we refer the reader to Appendix A.
1.1.1
!eueing based analysis for schedulers. The queueing-theoretic study of LLM inference
remains in its early stages, especially when compared to the more mature systems-oriented literature.
A recent survey by Mitzenmacher et al. [19] highlights the distinctive characteristics of LLM
inference workloads and emphasizes the importance of queueing-based analysis for such systems.
A closely related work is [17], in which the authors develop a queueing-theoretic model for
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 4 ---
59:4
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
LLM inference schedulers and analyze throughput optimality. The authors approximate the batch-
processing time, which is observed to have a staircase-behaviour in practice, by a linear function.
They observe under their model that work conserving schedulers are throughput optimal. In our
work, we model the detailed computations on a GPU to perform LLM inference, and obtain a
formula for the batch processing time that re$ects the staircase-behaviour observed in practice. Our
more detailed modeling framework yields novel structural insightsâ€”particularly on the optimal
tiling and resource allocation strategies required for throughput optimality. These insights o#er
new avenues for improving the performance of practical LLM inference systems. In [20], the authors
study throughput-maximization in a setting where the system is backed up with requests, and the
token generation times depend on the batch size, but not on the token positions.
1.1.2
Scheduling policies. Schedulers di#er in how they prioritize pre"ll-phase and decode-phase
requests, and in their batching granularity. Below, we highlight some of the key work in this
literature.
Decode-prioritizing (request-level) schedulers. Frameworks such as FasterTransformer [23, 24]
and the request-level mode of TensorRT-LLM process a set of requests till completion, before
admitting any new requests. Since new pre"ll-phase requests never interrupt ongoing decode-
phase requests, these schedulers perform well on TBT. However, they have low throughput when
there is a imbalance in the total (prompt and output) length of requests and thus GPU may be
under-utilized.
Pre!ll-prioritizing (iteration-level) schedulers. Iteration-level batching, "rst introduced by
Orca [33], enables dynamic admission and completion of requests at each forward pass. However, it
relies on static memory allocation for the KV cache, which limits the number of concurrent requests
to 16 on an A100 GPU. vLLM [15] overcomes this limitation using paged attention, allowing more
$exible memory management and increasing the maximum number of concurrent requests to 128.
Additionally, like FlashDecoding++ [8] and DeepSpeed-FastGen [7], vLLM aggressively admits
new pre"ll-phase requests to improve throughput. However, this eager admission policy can delay
decode iterationsâ€”especially for long promptsâ€”leading to higher TBT latencies.
Hybrid schedulers. Sarathi-Serve [2] introduces a token-budgeted, chunked-pre"ll strategy
to balance throughput and TBT, e#ectively reducing decode-iteration stalls that are common in
pre"ll-prioritizing schedulers. Beyond such scheduling-focused methods, recent systems propose
orthogonal techniques aimed at improving overall LLM inference performance. BlendServe [34]
targets o%ine workloads by reordering requests based on their resource usage pro"les to improve
hardware e!ciency. POD-Attention [14] enables pipelined execution of pre"ll and decode phases
to increase kernel overlap and improve GPU utilization. HydraGen [13] reduces redundant compu-
tation by identifying and merging shared prompt pre"xes across requests. DistServe [36] adopts
a disaggregated architecture that separates pre"ll and decode execution across di#erent nodes,
thereby eliminating intra-GPU contention; however, it introduces communication overhead due to
the transfer of large KV caches.
2
Background
2.1
Transformer
We focus on the widely used decoder-only Transformer architectureâ€”referred to simply as the
Transformer in this workâ€”that forms the basis of GPT, LLaMA, PaLM and other families of LLM
models. The Transformer is an autoregressive model that generates token ğ¿ğ¿+1 by conditioning on
all previous tokens ğ¿1, . . . ,ğ¿ğ¿. Token ğ¿ğ¿is processed by the Transformer as shown in Fig. 1. A token
is "rst converted into an embedding and then passed through ğ‘€layers. Let the input to the ğ‘th
layer be a vector ğœ´ğ‘€â†‘Rğ‘ğ¿of dimension ğ‘‚ğ‘‚. Each layer comprises of three distinct sub-layers:
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 5 ---
Optimal Scheduling for LLM Inference
59:5
Fig. 1. Transformer architecture.
(1) QKV Projection: This sub-layer is parametrized by three matrices ğ‘ƒğ‘€
ğ‘ƒ,ğ‘ƒğ‘€
ğ‘„and ğ‘ƒğ‘€
ğ‘…each of
dimension ğ‘‚â†“ğ‘‚ğ‘‚. The corresponding Query, Key and Value (or, QKV) vectors are computed
using a linear transformation as, ğœ¶ğ‘€= ğ‘ƒğ‘€
ğ‘ƒğœ´ğ‘€, ğœ·ğ‘€= ğ‘ƒğ‘€
ğ‘„ğœ´ğ‘€and ğœ¸ğ‘€= ğ‘ƒğ‘€
ğ‘…ğœ´ğ‘€, where ğœ¶ğ‘€, ğœ·ğ‘€, ğœ¸ğ‘€
are all vectors of dimension ğ‘‚â†“1.
(2) Self Attention: For the ğ‘„th token, the key and value vectors of all tokens up to and including
token ğ‘„are stacked into matrices ğ‘…ğ‘€,ğ¿and ğ‘†ğ‘€,ğ¿of dimension ğ‘‚â†“ğ‘„, and the self-attention is
computed as,
ğœ¹ğ‘€= ğ‘†ğ‘€,ğ¿softmax
!
ğ‘…ğ‘€,ğ¿â†”ğœ¶ğ‘€
â†—
ğ‘‚
"
,
(1)
where ğœ¹ğ‘€â†‘Rğ‘is the resulting context vector. (1) represents a single attention head. In
practice, Transformers employ multiple attention heads [30], meaning that multiple sets of ğ‘‡,
ğ‘…, and ğ‘†vectors are used in parallel in a layer. The resulting outputs are then concatenated
and passed to a Feed-Forward Network (FFN) described below. For simplicity, and without
loss of generality, we focus on a single attention head in our theoretical analysis. Our results
are also applicable to memory-e!cient variants such as multi-query attention [27] and
grouped-query attention [3].
(3) Feed Forward Net (FFN): The output of the self-attention sublayer is passed through an FFN
with one hidden-layer. The dimensions of the weight matrices of the FFN, ğ‘ƒğ‘€
ğ‘†1 and ğ‘ƒğ‘€
ğ‘†2, are
ğ‘‚ğ‘‡ğ‘‡â†“ğ‘‚and ğ‘‚ğ‘‚â†“ğ‘‚ğ‘‡ğ‘‡respectively. The output of the FFN serves as the input to the (ğ‘+1)th
layer.
The output of layer N is passed through a linear transform followed by a softmax operation. The
probability distribution output by the softmax is used to sample the next token in the sequence.
2.2
Inference on a Transformer
An LLM inference system receives requests in the form of prompts, consisting of a variable length
sequence of tokens ğ¿1,ğ¿2, . . . ,ğ¿ğ‘ˆğ‘€, where ğ‘ˆğ‘‰denotes the prompt length. The inference task is to
autoregressively sample output tokens ğ¿ğ‘ˆğ‘€+1,ğ¿ğ‘ˆğ‘€+2 . . . ,ğ¿ğ‘ˆğ‘€+ğ‘ˆğ‘from the Transformer until a stop
token is sampled at an apriori unknown position ğ‘ˆğ‘‰+ ğ‘ˆğ‘Š+ 1, where ğ‘ˆğ‘Šdenotes the output length.
A central technique used to speed up LLM inference is KV caching, wherein the key and value
vectors (KV vectors) that have been computed for a token ğ¿ğ¿are cached in GPU memory. This
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 6 ---
59:6
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
allows subsequent tokens to directly retrieve the relevant KV vectors during attention computation,
avoiding recomputation and signi"cantly improving inference e!ciency.
The computation associated with completing an LLM inference request can be divided into two
distinct phases,
(1) Pre!ll-Phase: Although prompt tokens are speci"ed by the request and thus do not need to
be sampled, the KV vectors for all prompt tokens do need to computed and cached for all
layers since they are required to perform the self-attention computation for later tokens. The
computation of the KV vectors corresponding to prompt tokens is called the pre"ll-phase.
Since all the prompt tokens are available, the pre"ll-phase can be done in parallel for all the
prompt tokens at once, or a chunk of tokens at a time. This is discussed further in Sections
3.1 and 3.2.
(2) Decode-Phase: The decode-phase denotes the computations involved in generating output
tokens ğ¿ğ‘ˆğ‘€+1,ğ¿ğ‘ˆğ‘€+2, . . . ,ğ¿ğ‘ˆğ‘€+ğ‘ˆğ‘. The KV vectors computed in this phase are also cached in
memory for use in the self-attention computation of future tokens. Since this is the token
generation phase, it is autoregressive in nature unlike the pre"ll-phase, and thus due to
dependencies cannot be parallelized across tokens of the request.
See Fig. 2 for a visualization of the two phases of LLM inference.
Fig. 2. Prefill and decode phases of a request during inference on a Transformer.
2.3
Computation on a GPU
A signi"cant fraction of computations associated with LLM inference and, more generally, modern
AI workloads involve matrix-matrix multiplication which can be sped up by specialized hardware.
As such, modern GPUs have several components to perform matrix-matrix multiplication that is
distinct from components used to perform general parallel computations. For concreteness, in this
paper we focus our analysis on the terminology and convention used by NVIDIA TensorCore GPUs
[25], while noting that GPUs produced by Intel[10], AMD[1] etc., have analogous components.
From hereon, unless otherwise mentioned, by GPU we mean NVIDIA TensorCore GPU.
A GPU consists of a set of ğ‘‰independent processors called Streaming Multiprocessors (SMs). An
SM in turn consists of a number of fundamental computational units, which can be categorized
into two categories: 1) CUDA cores which are general purpose parallel computation units, and 2)
Tensor Cores that are speci"cally designed to perform (generalized) matrix-matrix multiplications.
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 7 ---
Optimal Scheduling for LLM Inference
59:7
Two types of computational tasks on a GPU are especially relevant in the context of LLM inference,
Generalized Matrix-Matrix Multiplication (GeMM) and Generalized Matrix Vector Multiplication
(GeMV), which are detailed below.
GeMM. Let ğ‘Šâ†‘Rğ‘rowâ†“ğ‘red, ğ‘‹â†‘Rğ‘redâ†“ğ‘col, ğ‘Œâ†‘Rğ‘rowâ†“ğ‘col and let ğ‘, ğ‘â†‘R; the GeMM operation
computes ğ‘Œâ†˜ğ‘ğ‘Šğ‘‹+ ğ‘ğ‘Œ. In this formulation ğ‘‚row and ğ‘‚col denote the row and column dimensions
of the output matrix ğ‘Œ, while ğ‘‚red is the reduction dimension, corresponding to the shared inner
dimension of input matrices ğ‘Šand ğ‘‹.
Fig. 3. Computation of a tile in a GeMM, ğ‘Œâ†˜ğ‘ğ‘Šğ‘‹+ ğ‘ğ‘Œ, requires a sweep of a row of tiles of ğ‘Šand a column
of tiles of ğ‘‹.
The GPU GeMM algorithm [22] partitions the output matrixğ‘Œinto tiles of size ğ‘rowâ†“ğ‘col (padding
zeros if necessary to "t dimensions). The computation of a tile of ğ‘Œis assigned to an SM. The input
matrices ğ‘Šand ğ‘‹are tiled into tiles of dimensions ğ‘row â†“ğ‘red and ğ‘red â†“ğ‘col. Here, we shall refer to
ğ‘red as the reduction tile dimension. Each SM loads the corresponding input tiles into local memory
to compute its assigned output tile. See Fig. 3 for a visual representation of this process.
Tensor Cores can only be used if the output tile dimensions belong to a particular GPU-dependent
set, Tout and the reduction tile dimension belongs to a set Tred. Tile dimensions in Tout and Tred
are always powers of 2. For instance, on the A100 GPU, T ğ‘‹100
out
= {(256, 128), (128, 256), (128, 128),
(256, 64), (64, 256), (128, 64), (64, 128), (64, 64)}, and typically Tred = {32, 64}. Moreover, Tensor
Cores provide a substantially higher FLOP/s rate for GeMM with any of the allowed tile con"g-
urations as compared to CUDA cores. Therefore it is most often bene"cial to pad the input and
output matrices with zeros in order to partition it into tiles that "t in Tout and Tred, even though it
leads to redundant (zero-padding) computation. When zero padding is used, the FLOP/s rate only
counts the useful computations and excludes redundant computations, i.e., the theoretical peak is
computed using the original matrix sizes, not the padded sizes. For a detailed discussion on the
characteristics of matrix sizes and FLOP/s rate of GeMMs, refer [22].
Suppose the GeMM computation uses output tile dimensions ğ‘row â†“ğ‘col and reduction tile
dimension ğ‘red. The output matrix ğ‘Œthen contains â‰ƒğ‘‚row/ğ‘rowâ‡Â· â‰ƒğ‘‚col/ğ‘colâ‡tiles, and each output tile
requires â‰ƒğ‘‚red/ğ‘redâ‡tile-pairs of ğ‘Šand ğ‘‹to be multiplied (ref. Fig. 3). Let ğ‘(ğ‘row,ğ‘col,ğ‘red) denote the
speed of computing the GeMM of a tile-pair with dimensions ğ‘row â†“ğ‘red and ğ‘red â†“ğ‘col, and recall
that ğ‘‰is the number of SMs on the GPU. Then, we model the total time to complete the GeMM as
ğ‘‘GeMM(ğ‘‚row,ğ‘‚col,ğ‘‚red,ğ‘row,ğ‘col,ğ‘red) =
1
ğ‘‰ğ‘(ğ‘row,ğ‘col,ğ‘red)
#ğ‘‚row
ğ‘row
$ #ğ‘‚col
ğ‘col
$ #ğ‘‚red
ğ‘red
$
.
(2)
Here, 1/ğ‘‰accounts for the parallelism provided by theğ‘‰SMs, 1/ğ‘(ğ‘row,ğ‘col,ğ‘red) captures the e#ective
time to compute one tile-pair multiplication, and the remaining terms count the total number of
tile-pair multiplications needed.
A!!"#$%&â€™( 1. There exists output tile dimensions (ğ‘â‡’
row,ğ‘â‡’
col) â†‘Tout and a reduction tile size
ğ‘â‡’
red â†‘Tred such that if a GeMMâ€™s matrices are a) su"ciently large, i.e., can fully utilize the GPUâ€™s
compute resources, and b) are perfectly tiled i.e., there is no padding overhead, then the GPU achieves
its maximal FLOP/s rate.
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 8 ---
59:8
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
Assumption 1 is well supported by empirical studies of GeMM performance on modern GPUs
[22]. Intuitively, when matrices are su!ciently large and partition exactly into the tile size ğ‘â‡’
row â†“ğ‘â‡’
col
along the output dimensions and ğ‘â‡’
red along the reduction dimension, the GPU can fully utilize its
SMs by creating a large number of independent tile computations. These output tile dimensions
ğ‘â‡’
row â†“ğ‘â‡’
col are typically the largest in Tout which leads to higher arithmetic intensityâ€“the ratio of
$oating-point operations to memory accessesâ€“thereby enabling more e!cient use of memory
bandwidth and allowing the GPU to operate at its peak throughput. For instance, in the A100 GPU,
ğ‘â‡’
row â†“ğ‘â‡’
col is either 128 â†“256 or 256 â†“128 and ğ‘â‡’
red is 32.
GeMV. Let ğ‘Šâ†‘Rğ‘rowâ†“ğ‘col, ğœ´â†‘Rğ‘col, and ğœ¹â†‘Rğ‘row. Then, for scalars ğ‘, ğ‘, the GeMV operation
computes ğœ¹â†˜ğ‘ğ‘Šğœ´+ ğ‘ğœ¹. The GeMV is also implemented by tiling the output vector ğœ¹into
sub-vectors of size ğ‘row, which are assigned to individual SMs and computed using CUDA cores.
Correspondingly, the matrix ğ‘Šis partitioned into tiles of size ğ‘row â†“ğ‘col, and the input vector ğœ´into
tiles of size ğ‘col. Let ğ‘(ğ‘row,ğ‘col) denote the speed of computing a single tile-pair GeMV. Then the
total time to complete the GeMV is modeled as
ğ‘‘GeMV(ğ‘‚row,ğ‘‚col,ğ‘row,ğ‘col) =
1
ğ‘‰ğ‘(ğ‘row,ğ‘col)
#ğ‘‚row
ğ‘row
$ #ğ‘‚col
ğ‘col
$
.
(3)
Because GeMV performs fewer $oating-point operations per byte of memory tra!c than GeMM,
its arithmetic intensity is lower and thus it is less e!cient in terms of memory bandwidth utilization.
This motivates the following assumption.
A!!"#$%&â€™( 2. Let ğ‘‚row,ğ‘‚col,ğ‘‚red â†‘N. Let ğ‘Šâ†‘Rğ‘rowâ†“ğ‘red. Let, ğœ´(1), . . . , ğœ´(ğ‘‚col) â†‘Rğ‘redâ†“1
and ğœ¹(1), . . . ,ğœ¹(ğ‘‚col) â†‘Rğ‘rowâ†“1 be 2ğ‘‚col vectors. Denote the respective stacked vectors as, ğ‘’=
[ğœ´(1) ğœ´(2) . . . , ğœ´(ğ‘‚col)] and ğ‘“= [ğœ¹(1) ğœ¹(2) . . . ğœ¹(ğ‘‚col)]. Let ğ‘, ğ‘â†‘R. We will assume, that the
time required to compute the set of GeMVs, ğœ¹(ğ‘„) â†˜ğ‘ğ‘Šğœ´(ğ‘„) + ğ‘ğœ¹(ğ‘„) for all ğ‘„â†‘{1, . . . ,ğ‘‚col}, is greater
than the time required to compute the GeMM, ğ‘“â†˜ğ‘ğ‘Šğ‘’+ ğ‘ğ‘“.
3
LLM Inference System Model
We shall consider an LLM inference system consisting of ğ‘”inference nodes coordinated by a central
resource planner as shown in Fig. 4 which function as follows.
Inference node: An inference node is the basic computational unit responsible for performing
LLM inference. For large models such as GPT-3, PaLM, etc., inference is typically performed using
multiple GPUs due to memory constraints. However, all GPUs within a node participate in a
"xed execution pipeline using tensor-parallelism[21] or pipeline-parallelism[9]. Without loss of
generality, it is convenient to model an inference node as operating on a single GPU.
The primary decision process at an inference node lies in its LLM inference scheduler which
decides the order in which to schedule tasks on the nodeâ€™s GPU. The design of LLM inference
schedulers is the focus of this work, and, we describe a general framework to study them in Section
3.1, after a brief description of the resource planner below.
Resource planner: The resource planner receives incoming requests in the form of prompts and
assigns each one to an inference node, where the request joins a queue and waits to be served.
The planner continuously monitors the status of all inference nodesâ€”tracking metrics such as the
number of pending requests, request latencies, and memory usage (including risks of over$ows).
Based on this information, it may choose to reassign waiting requests or even migrate partially
processed requests to other nodes. When a partially completed request is migrated, the source node
must transfer the partially computed KV cache to the destination node, which might incur a cost
such as the time required to complete the transfer.
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 9 ---
Optimal Scheduling for LLM Inference
59:9
Fig. 4. An LLM inference system.
A resource planner in some systems, such as in NVIDIA Dynamo[4], has the ability to turn o#
some inference nodes when they are being underutilized, and turn them on when the system is
congested. This feature is especially useful towards achieving energy e!ciency when the request
arrival pattern is heterogeneous and may vary through time. We focus on stationary request arrivals
in our work for analytical tractability, and do not study policies to turn nodes on or o#.
3.1
A General Framework for LLM Inference Schedulers
We consider a class of schedulers that schedule batches of request iterations non-preemptively that
captures a range of existing schedulers in the literature. In this context, the meaning of an iteration
of a request depends on whether it is in the pre"ll-phase or the decode-phase, as de"ned below.
D)*&(&%&â€™( 1 (P+)*&,,-I%)+.%&â€™(). For a request ğ‘•, a chunk-size ğ‘–, and starting token index ğ‘„, a
pre!ll-iteration, denoted ğ‘—ğ‘˜(ğ‘•,ğ‘„,ğ‘–), involves computing the KV vectors for each layer corresponding
to prompt token indices in the range ğ‘„to ğ‘„+ ğ‘–â‡‘1 (both inclusive). Once the iteration is complete, the
starting token index is updates to ğ‘„+ ğ‘–.
In order to schedule ğ‘—ğ‘˜(ğ‘•,ğ‘„,ğ‘–) on the GPU, the KV vectors for tokens up to index ğ‘„â‡‘1 must
have been previously computed and cached in GPU memory, and ğ‘„+ ğ‘–â‡‘1 should not exceed the
prompt length. Moreover, if ğ‘„+ğ‘–â‡‘1 corresponds to the last prompt token, then the pre"ll-iteration
generates the "rst output token.
D)*&(&%&â€™( 2 (D)/â€™0)-I%)+.%&â€™(). An iteration of a request ğ‘•in the decode-phase, denoted
ğ‘™ğ‘˜(ğ‘•,ğ‘„), corresponds to the generation of the next output token which is at index ğ‘„+ 1.
In order to schedule ğ‘™ğ‘˜(ğ‘•,ğ‘„), all the KV vectors for tokens up to index ğ‘„â‡‘1 must be previously
computed and cached in GPU memory. Moreover, index ğ‘„should not correspond to a prompt token,
and should not correspond to a token that follows the stop token.
Let Pğ‘Œand Dğ‘Œbe the sets of requests that are in their pre"ll-phase and decode-phase respectively
just prior to the construction of batch ğ‘š. Let Bğ‘Œdenote the set of iterations to be scheduled in
batch ğ‘š. Initialize Bğ‘Œ= â‡“. A scheduler selects a subset Sğ‘‰
ğ‘Œof ğ‘›ğ‘‰
ğ‘Œpre"ll-phase requests from Pğ‘Œ
and subset Sğ‘Š
ğ‘Œof ğ‘›ğ‘Š
ğ‘Œdecode-phase requests from Dğ‘Œand constructs the batch as follows: 1) for
each selected pre"ll-phase request ğ‘•ğ‘â†‘Sğ‘‰
ğ‘Œwith starting token index ğ‘„ğ‘, it chooses a chunk size
ğ‘–ğ‘and adds ğ‘—ğ‘˜(ğ‘•ğ‘,ğ‘„ğ‘,ğ‘–ğ‘) to Bğ‘Œ; 2) for each selected decode-phase request ğ‘•ğ‘â†‘Sğ‘Š
ğ‘Œwith the latest
token index ğ‘„ğ‘, it adds ğ‘™ğ‘˜(ğ‘•ğ‘,ğ‘„ğ‘) to Bğ‘Œ. At most one decode-iteration can be scheduled per request
in a batch. Then, it schedules Bğ‘Œon the GPU. Note that all the iterations within the batch may be
processed in parallel by the GPU. A description of some existing LLM schedulers expressed in this
framework is provided in Appendix E.
Next, we discuss how computations corresponding to a batch are performed on the GPU.
3.2
LLM Inference Computation on the GPU
We split the operations involved into 3 categories.
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 10 ---
59:10
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
3.2.1
Linear transformations: This category includes the QKV projections in each layer, the
linear computations in the FFN sub-layer of each layer and also the "nal linear projection. In all
computations in this category, a weight matrix of the transformer multiplies a token feature vector.
In particular, these computations involve the features of the present token, and do not involve any
previous tokens in the sequence.
In line with Assumption 2, this operation may be scheduled e!ciently as a GeMM instead of
multiple GeMVs as follows. Let ğ‘ƒdenote the weight matrix of the transformer. Let ğ‘œğ‘Œdenote
the token-count of the ğ‘šth batch, which is, ğ‘œğ‘Œ= ğ‘›ğ‘Š
ğ‘Œ+ %
ğ‘:ğ‘ğ‘‚â†‘Sğ‘€ğ‘ƒğ‘–ğ‘. Stack all the ğ‘œğ‘Œcorresponding
feature vectors into a matrix ğ‘ğ¿ğ‘€and compute ğ‘ğ‘ğ‘ğ‘‘= ğ‘ƒğ‘ğ¿ğ‘€. Let (ğ‘row,ğ‘col) â†‘Tout and ğ‘red â†‘Tred
denote the output tile dimensions and reduction tile dimension used respectively for all linear
transformations. For simplicity, we use a common tile con"guration across transformations, as
Assumption 1 implies a single tile con"guration can achieve optimal performance across di#erent
GeMM dimensions.
Consider the QKV projection in layer ğ‘. The dimensions of ğ‘ƒğ‘€
ğ‘ƒ, ğ‘ƒğ‘€
ğ‘„and ğ‘ƒğ‘€
ğ‘…are ğ‘‚â†“ğ‘‚ğ‘‚. Then,
ğ‘’ğ‘€, which is the stack of ğ‘œğ‘Œnumber of ğœ´ğ‘€â€™s, has a dimension of ğ‘‚ğ‘‚â†“ğ‘œğ‘Œ. And the 3 output matrices,
which are the stacks of ğœ¶ğ‘€â€™s, ğœ·ğ‘€â€™s and ğœ¸ğ‘€â€™s respectively of tokens in the batch, have a dimension
of ğ‘‚â†“ğ‘œğ‘Œeach. Since the model dimensions are designed with hardware in mind, ğ‘‚,ğ‘‚ğ‘‚(as well as
other model dimensions) are divisible by the tile dimensions. Therefore, according to (2), the time
required to complete the 3 GeMMs is,
3
ğ‘’ğ‘“(ğ‘‘row,ğ‘‘col,ğ‘‘red)
ğ‘ğ¿
ğ‘‘red
ğ‘
ğ‘‘row
&
ğ‘”ğ‘ƒ
ğ‘‘col
'
. The two FFN sub-layer matrices
ğ‘ƒğ‘€
ğ‘†1 and ğ‘ƒğ‘€
ğ‘†2 have dimensions ğ‘‚# â†“ğ‘‚and ğ‘‚ğ‘‚â†“ğ‘‚#, respectively. Denote the dimension of the "nal
linear projection matrix as ğ‘‚outâ†“ğ‘‚ğ‘‚. Recalling that there are ğ‘€layers in the transformer, the amount
of time required to compute the linear transformations for a batch with token count ğ‘œğ‘Œis,
1
ğ‘‰ğ‘(ğ‘row,ğ‘col,ğ‘red)
(
3ğ‘€ğ‘‚ğ‘‚
ğ‘red
ğ‘‚
ğ‘row
+ ğ‘€ğ‘‚
ğ‘red
ğ‘‚#
ğ‘row
+ ğ‘€ğ‘‚#
ğ‘red
ğ‘‚ğ‘‚
ğ‘row
+ ğ‘‚ğ‘‚
ğ‘red
ğ‘‚out
ğ‘row
) # ğ‘œğ‘Œ
ğ‘col
$
In the above formula, the scheduler can only modify the token count of the batch, but not any
of the transformerâ€™s parameters. So, we can abstract out "xed components, and express the time
required to complete the linear transformation computations for the tokens in a batch as,
1
ğ‘Lin(ğ‘row,ğ‘col,ğ‘red)
# ğ‘œğ‘Œ
ğ‘col
$
.
(4)
3.2.2
Self-a"ention: This computation has di#erent characteristics in decode and pre"ll iterations.
The self-attention for a decode-iteration of a request consists of performing GeMV operations
which cannot be scheduled as a GeMM using batching across requests because each request has a
unique KV cache. Consider a decode-iteration ğ‘™ğ‘˜(ğ‘•ğ‘,ğ‘„ğ‘) in the batch. Its self-attention computation
involves 2 GeMVâ€™s: one with a matrix of size ğ‘‚â†“ğ‘„ğ‘and a vector of length ğ‘„ğ‘, and another with a
matrix of size ğ‘„ğ‘â†“ğ‘‚and a vector of length ğ‘‚. Let (ğ‘row,ğ‘col) be the optimal tile con"guration for the
GeMV. Then, from (3), the time required to perform Self-Attenion at layer ğ‘for this decode-iteration
is,
1
ğ‘(ğ‘row,ğ‘col)
( ğ‘‚
ğ‘col
# ğ‘„ğ‘
ğ‘row
$
+
# ğ‘„ğ‘
ğ‘col
$ ğ‘‚
ğ‘row
)
âŠ‹ğ‘‘ğ‘Š,ğ‘•ğ‘‹(ğ‘„ğ‘).
(5)
Above, we useğ‘‘ğ‘Š,ğ‘•ğ‘‹(Â·) to encapsulate the "xed components of the self-attention computation time.
On the other hand, since pre"ll-iterations may be scheduled for a chunk of tokens at a time, their
self-attention may be computed as a GeMM as follows. Consider a pre"ll-iteration ğ‘—ğ‘˜(ğ‘•ğ‘,ğ‘„ğ‘,ğ‘–ğ‘)
at layer ğ‘of the Transformer. Stack the ğ‘„ğ‘+ ğ‘–ğ‘â‡‘1 each of key and value vectors of dimension ğ‘‚
of the "rst ğ‘„ğ‘+ ğ‘–ğ‘â‡‘1 tokens into matrices ğ‘…ğ‘€and ğ‘†ğ‘€. Stack the ğ‘–ğ‘query vectors of dimension ğ‘‚
corresponding to the current chunk into the matrix ğ‘‡ğ‘€. Let ğ‘be a masking matrix of dimension
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 11 ---
Optimal Scheduling for LLM Inference
59:11
(ğ‘„ğ‘+ ğ‘–ğ‘â‡‘1) â†“ğ‘–ğ‘, where each entry is 1 except in indices (ğ‘Ÿ1,ğ‘Ÿ2) satisfying ğ‘Ÿ1 > ğ‘„and ğ‘Ÿ2 â‡”ğ‘Ÿ1 â‡‘ğ‘„,
where it is set to 0. The masking matrix allows a token to only attend to previous tokens in
the prompt sequence. Then, the stack of output vectors of the self-attention may be computed
as, ğ‘“ğ‘€= ğ‘†ğ‘€softmax
*
ğ‘„ğ‘„â†”ğ‘ƒğ‘„
â†—
ğ‘
â†–ğ‘
+
, where â†–denotes an element-wise product. Observe that this
requires: 1) a GeMM between matrices of dimensions (ğ‘„ğ‘+ğ‘–ğ‘â‡‘1) â†“ğ‘‚and ğ‘‚â†“ğ‘–ğ‘, 2) element-wise
multiplication with the mask ğ‘followed by a non-linearity (softmax), and 3) another GeMM
between ğ‘‚â†“(ğ‘„ğ‘+ğ‘–ğ‘â‡‘1) and (ğ‘„ğ‘+ğ‘–ğ‘â‡‘1) â†“ğ‘–ğ‘matrices. Let (ğ‘row,ğ‘col) â†‘Tout and ğ‘red â†‘Tred denote the
output tile dimensions and reduction tile dimension used respectively for both the GeMMs. We
assume the cost of masking and softmax is negligible for simplicity. Then, according to (2), the time
required to compute the self-attention across all ğ‘€layers is:
ğ‘€
ğ‘‰ğ‘(ğ‘row,ğ‘col,ğ‘red)
(#ğ‘„ğ‘+ ğ‘–ğ‘â‡‘1
ğ‘row
$ # ğ‘–ğ‘
ğ‘col
$ ğ‘‚
ğ‘red
+
ğ‘‚
ğ‘row
# ğ‘–ğ‘
ğ‘col
$ #ğ‘„ğ‘+ ğ‘–ğ‘â‡‘1
ğ‘red
$)
.
(6)
R)#.+1 1. For simplicity, we describe the computation of self-attention as two successive GeMMâ€™s
interleaved with non-linear operations. In practice, systems usually use FlashAttention [5]-a hardware-
e"cient algorithm which minimizes the number of memory transfers by fusing operations and re-
ordering computation using tiling. Our approach and analysis can be adapted for FlashAttention with
appropriate modi!cations to (6). We leave this to future work.
3.2.3
Non-Linear Operations: Each SM in a GPU has a Special-Function-Unit that e!ciently
computes non-linear functions. Transformers include non-linear operations such as ReLU, softmax,
etc., whichâ€”like Linear Transformationsâ€”operate only on the tokens being processed in the batch.
The time to compute all non-linear operations for a single token is denoted by
1
ğ‘“nLin . Thus, for a
batch containing ğ‘œğ‘Œtokens, the total computation time is
1
ğ‘“nLinğ‘œğ‘Œ, where ğ‘nLin denotes the rate at
which non-linear operations are performed per token.
Batch execution time: Recall that Bğ‘Œdenotes theğ‘šth batch with pre"ll-iterations from requests
in Sğ‘‰
ğ‘Œand decode-iterations from requests in Sğ‘Š
ğ‘Œ. Then, summing up the computations times from
(4), (5) and (6) for all the iterations, we express the batch computation time as,
ğ‘‘ğ‘–(Bğ‘Œ) =
1
ğ‘Lin(ğ‘row,ğ‘col,ğ‘red)
# ğ‘œğ‘Œ
ğ‘col
$
+
1
ğ‘nLin
ğ‘œğ‘Œ+ ğ‘€
,
ğ‘:ğ‘ğ‘‚â†‘Sğ‘
ğ‘ƒ
ğ‘‘ğ‘Š,ğ‘•ğ‘‹(ğ‘„ğ‘)
+
ğ‘€
ğ‘‰ğ‘(ğ‘row,ğ‘col,ğ‘red)
,
ğ‘:ğ‘ğ‘‚â†‘Sğ‘€ğ‘ƒ
(#ğ‘„ğ‘+ ğ‘–ğ‘â‡‘1
ğ‘row
$ # ğ‘–ğ‘
ğ‘col
$ ğ‘‚
ğ‘red
+
ğ‘‚
ğ‘row
# ğ‘–ğ‘
ğ‘col
$ #ğ‘„ğ‘+ ğ‘–ğ‘â‡‘1
ğ‘red
$)
.
(7)
The ceiling function in the batch execution time formula captures the staircase-behaviour observed
in practice. See [17, Figure 4].
Next, we present our main theoretical contribution: a simple resource planner and a scheduler that
maximizes the throughputâ€”measured in requests per secondâ€”under stationary request arrivals.
4
A Throughput Optimal Inference System
4.1
Throughput optimal resource planner
For simplicity, we assume that all ğ‘”Inference Nodes in the system are identical and are running
the same Transformer model. We consider a simple uniformly random resource planner, which
distributes each incoming request uniformly at random to one of the ğ‘”inference nodes.
The random-routing assumption represents a simpli"ed setting. The multi-node model is included
to re$ect large-scale inference architectures, which typically involve multiple GPUs or servers.
Within the homogeneous setup considered here, random routing su!ces to achieve throughput
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 12 ---
59:12
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
optimality. In more realistic deploymentsâ€”featuring heterogeneous nodes, bursty workloads, or
latency-sensitive objectivesâ€”more sophisticated routing strategies may be required. This model
thus serves as a foundational step toward analyzing such richer settings.
A general resource planner in our model (see Fig. 4) has the ability to monitor node statuses
and transfer pending or partially processed requests from one node to another. While the simple
resource planner above does neither of these, we show that, combined with our proposed scheduler
in Section 4.2, it maximizes resource utilization and is thus throughput-optimal, even when the
cost of transferring the KV cache of partially processed requests between nodes is 0. The ability to
monitor node status and transfer requests becomes particularly useful in practical systems that
aim to minimize delay or satisfy strict latency SLO constraints [4].
4.2
RAD: A throughput optimal scheduler
Our proposed Resource-Aware-Dynamic (RAD) Scheduler is shown in Algorithm 1. The time
horizon of RAD can be partitioned into cycles, where in a cycle, RAD schedules a sequence of
batches that start and complete up to ğ‘requests, with ğ‘being a parameter. Now, in addition to ğ‘,
RAD also accepts the optimal output tile dimensions, (ğ‘â‡’
row,ğ‘â‡’
col) â†‘Tout and reduction tile dimension
ğ‘â‡’
red â†‘Tred as in Assumption 1, for optimal GeMM computation on the GPU. The central design
principle of RAD is two-fold.
Optimal GeMM tiling. RAD schedules batches such that all GeMM computations in the batch are
optimally tiled. It does so by either scheduling ğ‘â‡’
col decode-iterations in a batch, or, scheduling a
pre"ll-iteration with chunk size equal to LCM(ğ‘â‡’
row,ğ‘â‡’
col,ğ‘â‡’
red), where LCM stands for least-common-
multiple. This tiling strategy is violated only in rare casesâ€”speci"cally, near the end of a cycleâ€”as
further explained in the description of Algorithm 1.
Optimal resource allocation via dynamic scheduling of pre"ll and decode. RAD dynamically
prioritizes pre"ll or decode iterations based on the request arrival pattern and its characteristics.
For instance, if the requests have very long prompts but short output lengths, it spends more time
performing pre"ll iterations. On the other hand, if the requests have relatively shorter prompt
lengths and longer output lengths, it spends more time performing decode iterations. In this context,
pre"ll and decode phases may be viewed as two classes of tasks, and that RAD partitions the nodeâ€™s
service capacity such that both pre"ll and decode tasks may be completed.
In addition to the above principles, RAD also manages GPU memory and prevents over$ow
by limiting the number of active requests â€”requests that currently store their KV cache in GPU
memoryâ€” to ğ‘â‡’
col, the number required for optimal tiling. See Assumption 4 for further details.
At a high level, RAD alternates between two operational modesâ€”Pre"ll Mode and Decode
Modeâ€”as illustrated in Figure 5. At the start of a cycle, RAD enters Pre"ll Mode, completing ğ‘â‡’
col
pre"ll-phase requests, after which the same number of decode-phase requests become active. It then
transitions to Decode Mode, scheduling decode iterations for these active requests in optimally tiled
batches. When one or more decode-phase requests complete, RAD switches back to Pre"ll Mode
to process additional pre"ll-phase requests until there are again exactly ğ‘â‡’
col active decode-phase
requests, thereby re-enabling e!cient decode batching. Once a su!cient number of requests have
been completed in the current cycle, RAD "nishes the remaining onesâ€”even if their batches are
not optimally tiledâ€”before beginning the next cycle.
Next, we provide a detailed explanation of RAD as shown in Algorithm 1. Let P and D denote
the set of requests in pre"ll and decode phases respectively. A variable, ğ‘”ğ‘ ğ‘¡ğ‘¢ğ‘ ğ‘‰ğ‘ğ‘‰_ğ‘„ğ‘_ğ‘–ğ‘£ğ‘–ğ‘ˆğ‘ keeps
track of how many requests have started processing (i.e., have had an iteration scheduled) in the
current cycle. During a cycle, we continue to update P with any requests that arrive. Then at the
completion of a batch, the next batch is selected as follows. If there are no requests in P or Q, the
scheduler simply waits until a new arrival happens (lines 5). Otherwise, 1) If there are enough decode
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 13 ---
Optimal Scheduling for LLM Inference
59:13
Fig. 5. High-level operation of the RAD scheduler. In Prefill Mode, the scheduler completes prefill iterations
with optimal tiling until there are ğ‘â‡’
col active decode-phase requests, at which point it switches to Decode
Mode. In Decode Mode, it schedules decode iterations for these active requests until at least one completes,
a!er which it switches back to Prefill Mode.
phase requests, or there are no pre!ll phase requests, or ğ‘requests have started processing in the current
cycle (Lines 6-11): RAD selects one decode iteration per request in D and runs them together as a
batch. After the GPU completes processing the batch, any requests that sampled the stop token
are removed. If D is now empty, it signals the end of the current cycle, and ğ‘”ğ‘ ğ‘¡ğ‘¢ğ‘ ğ‘‰ğ‘ğ‘‰_ğ‘„ğ‘_ğ‘–ğ‘£ğ‘–ğ‘ˆğ‘ is
set to 0. 2) Else (Lines 12-21): RAD chooses a request in a "rst-come-"rst-serve (FCFS) manner
from P and schedules its pre"ll-iterations in chunks of size LCM(ğ‘row,ğ‘col,ğ‘red), unless there are
fewer tokens, until its pre"ll phase is complete. The request is then moved to D, and the number
ğ‘”ğ‘ ğ‘¡ğ‘¢ğ‘ ğ‘‰ğ‘ğ‘‰_ğ‘„ğ‘_ğ‘–ğ‘£ğ‘–ğ‘ˆğ‘ is updated.
Algorithm 1 Resource Aware Dynamic (RAD) Scheduler
Require: Parameter ğ‘
// Max number of prefills per cycle
Require: Output tile size (ğ‘â‡’
row,ğ‘â‡’
col) and reduction tile dimension ğ‘â‡’
red // For optimal tiling
1: P â†˜â‡“, D â†˜â‡“
// Set of requests in prefill and decode phase
2: ğ‘”ğ‘ ğ‘¡ğ‘¢ğ‘ ğ‘‰ğ‘ğ‘‰_ğ‘„ğ‘_ğ‘–ğ‘£ğ‘–ğ‘ˆğ‘ â†˜0
3: while True do
4:
Update P based on new arrivals
5:
if P = â‡“and D = â‡“then continue
// Do nothing
6:
else if |D| = ğ‘â‡’
col or P = â‡“or ğ‘”ğ‘ ğ‘¡ğ‘¢ğ‘ ğ‘‰ğ‘ğ‘‰_ğ‘„ğ‘_ğ‘–ğ‘£ğ‘–ğ‘ˆğ‘ = ğ‘then
7:
Schedule a DI with appropriate token index for each request in D in a batch
8:
Remove requests that sampled the stop token from D
9:
If D = â‡“then ğ‘”ğ‘ ğ‘¡ğ‘¢ğ‘ ğ‘‰ğ‘ğ‘‰_ğ‘„ğ‘_ğ‘–ğ‘£ğ‘–ğ‘ˆğ‘ â†˜0
// End of cycle
10:
else
11:
Choose a request ğ‘•"rst-come-"rst-serve from P and set ğ‘„â†˜1
12:
while pre"ll phase of the chosen request is incomplete do
13:
ğ‘–= min{ğ‘â‡’
lcm, ğ‘¤ğ‘‰(ğ‘•) â‡‘ğ‘„}
// ğ‘â‡’
lcm = LCM(ğ‘â‡’
row,ğ‘â‡’
col,ğ‘â‡’
red)
14:
Schedule ğ‘—ğ‘˜(ğ‘•,ğ‘„,ğ‘–) in a batch
15:
ğ‘„â†˜ğ‘„+ ğ‘–
16:
end while
17:
Move the request from P to D
18:
ğ‘”ğ‘ ğ‘¡ğ‘¢ğ‘ ğ‘‰ğ‘ğ‘‰_ğ‘„ğ‘_ğ‘–ğ‘£ğ‘–ğ‘ˆğ‘ â†˜ğ‘”ğ‘ ğ‘¡ğ‘¢ğ‘ ğ‘‰ğ‘ğ‘‰_ğ‘„ğ‘_ğ‘–ğ‘£ğ‘–ğ‘ˆğ‘ + 1
19:
end if
20: end while
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 14 ---
59:14
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
4.3
Throughput Optimality Characterization
The following theorem characterizes the maximum arrival rate under which any routing and
scheduling policy could be stable.
T2)â€™+)# 1 (U$$)+ Bâ€™"(0). Consider a system with a resource planner and ğ‘”identical inference
nodes. Let requests arrive to the system according to a Poisson-Point-Process (PPP) of rate ğ‘¥. Let the
ğ‘¦th request arrival, ğ‘•ğ‘, have prompt and output lengths, ğ‘¤ğ‘‰
ğ‘and ğ‘¤ğ‘Š
ğ‘, sampled according to a bounded
joint-distribution ğ‘§ğ‘—ğ‘€,ğ‘—ğ‘, i.i.d., across requests.
Let (ğ‘â‡’
row,ğ‘â‡’
col) â†‘Tout and ğ‘â‡’
col â†‘Tred be the optimal output tile dimensions and reduction tile
dimension respectively from Assumption 1, and let, ğ‘â‡’
lcm = LCM(ğ‘â‡’
row,ğ‘â‡’
col,ğ‘â‡’
red). De!ne,
Â¯ğ‘‘ğ‘=
1
ğ‘Lin(ğ‘â‡’row,ğ‘â‡’
col,ğ‘â‡’
red)
E
-
ğ‘¤ğ‘‰+ ğ‘¤ğ‘Š.
ğ‘â‡’
col
+
E
-
ğ‘¤ğ‘‰+ ğ‘¤ğ‘Š.
ğ‘nLin
+ ğ‘€E
ï£®ï£«ï£«ï£«ï£«ï£¶
ğ‘—ğ‘
,
ğ¿=1
ğ‘‘ğ‘Š,ğ‘•ğ‘‹(ğ‘¤ğ‘‰+ ğ‘„)
ï£®ï£¹ï£¹ï£¹ï£¹ï£°
+
ğ‘€ğ‘‚
ğ‘‰ğ‘(ğ‘â‡’row,ğ‘â‡’
col,ğ‘â‡’
red)ğ‘â‡’rowğ‘â‡’
colğ‘â‡’
red
E
-
ğ‘¤ğ‘‰(ğ‘¤ğ‘‰+ ğ‘â‡’
lcm)
.
.
Denote ğ‘‡(ğ‘) to be the number of pending requests in the system at time ğ‘, i.e., those that have arrived
but not yet completed service. Under Assumptions 1 and 2, if ğ‘¥Â¯ğ‘‘ğ‘> ğ‘”, there exists an ğ‘> 0 such that
for any resource planner and scheduler, limğ‘‘â†’â†™
ğ‘ƒ(ğ‘‘)
ğ‘‘
> ğ‘almost surely.
The proof of Theorem 1 can be found in Appendix B. Intuitively, the argument proceeds by
showing that, in the best case, an inference node may be viewed as a single server system with
expected service time Â¯ğ‘‘ğ‘. Then, instability may be shown to occur when the load on the system
exceeds its service capacity.
Next, we proceed to analyze the range of arrival rates under which the RAD scheduler stabilizes
the system. First, we make two assumptions.
A!!"#$%&â€™( 3. Let (ğ‘â‡’
row,ğ‘â‡’
col) and ğ‘â‡’
red be output tile dimensions and reduction tile dimension used
by RAD. Then, prompt lengths ğ‘¤ğ‘‰âˆğ‘§ğ‘—ğ‘€are multiples of LCM(ğ‘â‡’
row,ğ‘â‡’
col,ğ‘â‡’
row) almost surely. Finally,
assume the prompt and output lengths are upper bounded by ğ‘ˆğ‘‰,max and ğ‘ˆğ‘Š,max respectively.
The upper bound on the prompt and output lengths is natural since LLMs only support bounded
context lengths. If the prompt length is not a multiple of LCM(ğ‘â‡’
row,ğ‘â‡’
col,ğ‘â‡’
row), then any scheduler
may have to complete a part of the pre"ll-phase with non-optimal tiling. We make this assumption
to simplify our scheduler and its analysis.
A!!"#$%&â€™( 4. Let (ğ‘â‡’
row,ğ‘â‡’
col) be the output tile dimensions used by RAD. Each inference node is
provisioned with su"cient GPU memory to store the KV cache for ğ‘â‡’
col concurrent requests, each with
prompt length ğ‘ˆğ‘‰,max and output length ğ‘ˆğ‘Š,max, where ğ‘ˆğ‘‰,max and ğ‘ˆğ‘Š,max are de!ned in Assumption 3.
This assumption ensures that su!cient GPU memory is available to accommodate the worst-case
KV cache requirements needed by the RAD scheduler. The above assumption is also a mild one
because ğ‘â‡’
col is typically as small as 64 or 128. Modern LLM inference nodes that use paged-attention
[15] are able to accommodate the KV-cache of these many requests in their GPU memory.
Next, we state our result which characterizes the loads that the RAD scheduler combined with a
simple resource planner can stabilize.
T2)â€™+)# 2 (T2+â€™"32$"% O$%&#.,&%4 â€™* RAD). Consider a system with a uniformly random
resource planner, and ğ‘”identical Inference Nodes running the RAD scheduler. Let requests arrive
according to a Poisson Point Process (PPP) of rate ğ‘¥. Let the ğ‘¦th request arrival, ğ‘•ğ‘, have prompt and
output lengths, ğ‘¤ğ‘‰
ğ‘and ğ‘¤ğ‘Š
ğ‘, sampled according to a joint-distribution ğ‘§ğ‘—ğ‘€,ğ‘—ğ‘, i.i.d., across requests.
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 15 ---
Optimal Scheduling for LLM Inference
59:15
Suppose Assumptions 1, 2, 3, and 4 hold. Let Â¯ğ‘‘ğ‘be as de!ned in Theorem 1. De!ne the worst-case
time to complete a request as:
ğ‘‘max âŠ‹
max
(ğ‘‘row,ğ‘‘col)â†‘Tout,ğ‘‘redâ†‘Tred
ï£®ï£«ï£«ï£«ï£«ï£¶
ğ‘ˆğ‘‰,max + ğ‘ˆğ‘Š,max
ğ‘Lin(ğ‘row,ğ‘col,ğ‘red) + ğ‘ˆğ‘‰,max + ğ‘ˆğ‘Š,max
ğ‘nLin
+ ğ‘€
ğ‘ˆğ‘€,max+ğ‘ˆğ‘,max
,
ğ¿=1
ğ‘‘ğ‘Š,ğ‘•ğ‘‹(ğ‘„)
ï£®ï£¹ï£¹ï£¹ï£¹ï£°
.
Suppose there exists ğ›¥> 0 such that ğ‘¥Â¯ğ‘‘ğ‘â‡”ğ‘”(1 â‡‘ğ›¥). Then, RAD with parameter ğ‘satisfying
ğ‘>
(ğ‘‘â‡’
colâ‡‘1) ğ‘˜max
ğ‘™Â¯ğ‘˜ğ‘…
, is stable in the following sense. Let ğ‘‡ğ‘šâ€² (ğ‘) denote the number of pending requests at
node ğ‘”â€² at time ğ‘. For every ğ‘”â€², (ğ‘‡ğ‘šâ€² (ğ‘))ğ‘‘âˆ0 forms a positive-recurrent regenerative process[28].
We outline the arguments used to prove the statement above. First we construct a discrete-time
Markov chain (DTMC) for each node that tracks the number of requests at the node at the start of
each cycle. We show that the DTMC is positive recurrent under the conditions of the Theorem.
Further, we show that the expected time of a cycle is "nite. This leads us to conclude that the
number of pending requests ğ‘‡ğ‘šâ€² (ğ‘) at any time ğ‘at a node ğ‘”â€² forms a positive-recurrent regenerative
processâ€” it visits 0 in "nite expected time. The proof can be found in Appendix C.
5
Practical Insights from Theory
In Section 4, we identi"ed two key principles that make a system throughput optimal: 1) optimal
GeMM tiling when there are su!cient requests in the system, 2) optimal resource allocation for
pre"ll and decode iterations to account for variability and uncertainty of prompt and output lengths.
In practice, LLM inference systems also have to meet other latency-based SLO constraints, such as
TTFT and TBT as explained in the Section 1. We believe that designing a system that simultaneously
achieves optimal GeMM tiling and optimal resource allocation, while maintaining low TTFT and
TBT is challenging. As an illustrative example, consider an instance of running the RAD scheduler
where in every batch of ğ‘â‡’
col decode-iterations, one or more requests generate their stop token. In
this case, the RAD scheduler has to complete pre"ll-phases of an equal number of new requests
in between every consecutive batch of decode-iterations. Since pre"ll-phases may take a while to
complete because of long prompt lengths, all requests may experience high TBT. See Appendix D
for a throughput-optimal scheduler with low TBT but which incurs high TTFT.
In real-world deployments, meeting both TTFT and TBT constraints often necessitates a tradeo#
between optimal tiling and optimal resource allocation. We examine two state-of-the-art LLM
inference paradigmsâ€”single-node serving, where both the pre"ll and decode phases of each request
are executed on the same inference node, and disaggregated serving, where these phases are
distributed across di#erent nodesâ€”and present a novel perspective on how these systems adopt
complementary strategies to navigate this tradeo#.
Single node serving maintains optimal resource allocation but sacri!ces optimal GeMM tiling.
Consider, Sarathi-serve [2] a state-of-the-art single node serving scheduler. When enough requests
are present, Sarathi-serve prioritizes "lling a batch with available decode iterations, then adds pre"ll
iterations such that the total token count equals ğ‘œbudget, a multiple of ğ‘â‡’
col. This dynamic batching
strategy is similar to the RAD schedulerâ€™s, and preserves optimal allocation across pre"ll and decode.
However, since it does not enforce that pre"ll chunk sizes are multiples of LCM(ğ‘â‡’
row,ğ‘â‡’
col,ğ‘â‡’
red), it
does not guarantee optimal tiling. Other schedulers, such as vLLM [15] and Orca [33], also fall into
the category of single-node serving schedulers.
Disaggregated serving, by contrast, maintains optimal GeMM tiling but sacri!ces optimal resource
allocation. Distserve [36], a state-of-the-art disaggregated serving scheduler, statically partitions
inference nodes into pre"ll-nodes and decode-nodes. Pre"ll-nodes exclusively handle pre"ll itera-
tions, which allows them to, in most cases, schedule with optimal chunk sizes, helping minimize
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 16 ---
59:16
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
TTFT. Completed pre"ll-phase requests are transferred to decode-nodes. Decode-nodes handle
decode iterations and can batch ğ‘â‡’
col decode requests when available, keeping TBT low. However,
static partitioning may lead to resource imbalance: for instance, when prompt lengths are short
but output lengths are long, decode-nodes may become overloaded while pre"ll-nodes remain
underutilized.
To address practical needs, we next consider the design of a scheduler that serves heterogeneous
request classes subject to latency constraints at a single node. Since single node serving systems
dynamically allocate resources and are therefore more robust to heterogeneity in request arrivals,
we focus on this paradigm in the sequel.
6
SLO Aware LLM Inference Scheduler
Request classes. In real-world LLM serving systems, requests may come from di#erent classes of
user with heterogeneous latency SLOs. For example, paying users typically expect fast and smooth
responses, especially during token generation, which requires stricter TBT deadlines. By contrast,
free-tier users are generally more tolerant of delays and can be served with more relaxed TBT
constraints. Managing these mixed latency requirements well is important to keep users satis"ed
while making e!cient use of system resources.
Recall that we consider a class of schedulers that executes pre"ll-phase and decode-phase requests
as sequences of pre"ll-iterations and decode-iterations, respectively, see De"nitions 1 and 2.
Motivation. The throughput-optimal RAD scheduler described in Section 4 focuses on maximiz-
ing throughput, but it does not consider latency SLOs during either the pre"ll or decode phases due
to the challenges mentioned in Section 5. However, in practice, meeting these latency constraints is
critical to deliver a good user experience.
Sarathi-Serve, the current state-of-the-art scheduler, addresses this by chunking long pre"ll-phase
requests into smaller chunks and interleaving them with decode-phase requests in each batch. Each
batch is constrained by a token budgetâ€”the maximum number of tokens it can process. Sarathi-
Serve includes all active decode-phase requests from the previous batch in the current one and uses
the remaining token budget to schedule pre"ll-iterations. However, it treats all decode-iterations of
associated decode-phase requests as if they had the strictest TBT deadline, even when actual TBT
deadlines vary across requests. While this conservative strategy ensures tail TBT latency is below
some threshold, it can lead to ine!cient use of batch capacity and does not address reducing TTFT
for pre"ll-phase requests. To overcome this limitation, we propose the SLO-Aware LLM Inference
(SLAI) Scheduler. SLAI tracks each decode-iterationâ€™s TBT deadline and delays its inclusion in a
batch until necessary. This allows the scheduler to allocate more of the batchâ€™s token budget to
pre"ll-iterations earlier, without missing TBT deadlines on decode-iterations. As we will show, this
approach better aligns scheduling decisions with request-speci"c needs, resulting in lower median
TTFT for pre"ll-phase requests while still meeting tail TBT latency constraints on decode-iterations.
Key concepts and parameters. We begin by explaining how SLAI dynamically decides when to
include a decode-iteration in a batch. The key idea in this process is the last schedulable time, which
determines when a decode-iteration becomes critical and must be included to meet its latency
target.
Let ğ›©> 0 be an o#set parameter for SLAI that de"nes how early a decode-iteration should
be considered critical. Consider the ğ‘„th decode iteration for a given request ğ‘¦which has an SLO
requirement of TBTğ‘, SLAI notes the end time of the most recent batch in which its (ğ‘„â‡‘1)th decode-
iteration was included, denoted ğ‘ ğ¿â‡‘1,ğ‘. It also maintains the running average of batch execution
times observed so far, denoted by ğ‘batch. We de"ne its last schedulable time as:
ğ‘Œğ¿,ğ‘= ğ‘ ğ¿â‡‘1,ğ‘+ TBTğ‘â‡‘ğ›©Â· ğ‘batch
(8)
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 17 ---
Optimal Scheduling for LLM Inference
59:17
This is the latest wall-clock time by which decode-iteration ğ‘¦must be included in a batch to meet its
TBT deadline. When constructing batch ğ‘šat time ğ‘ğ‘Œ, the scheduler checks each active (in progress
that currently occupies GPU memory) decode-phase request and labels its decode-iteration as
critical if ğ‘ğ‘Œâˆğ‘Œğ¿,ğ‘; otherwise, it is considered as non-critical and can be deferred to a later batch.
Besides this dynamic prioritization, SLAI uses several key parameters to balance latency targets
with e!cient GPU use. The token budget ğ‘œbudget sets the maximum token count allowed in a batch,
ensuring e#ective use of the GPUâ€™s compute resources without violating TBT SLOs. A cap on
the number of active requests ğ‘limits how many active requests are allowed at once, helping
prevent memory over$ows for large models or long prompts. A decode limit ğ‘restricts how many
of decode-iterations can be included in a batch, avoiding long batch execution times due to too
many decode-iterations in a batch. Finally, the o#set parameter ğ›©provides a safety margin for the
last schedulable time computation to absorb variability in batch execution and reduce the risk of
missing TBT SLOs. We will later discuss how these parameters interact and in$uence the behavior
and performance of the scheduler. Next, we describe how the SLAI scheduler works.
Batch construction. We now describe how a batch is constructed by the SLAI scheduler. At
each decision point, the scheduler forms a batch from the set of active requests and new requests
that have not yet been processed. While forming a batch, it must respect several system constraints:
the token budget (ğ‘œbudget), the cap on active requests (ğ‘), and the decode request limit (ğ‘). The batch
construction follows these steps:
(1) Identify critical decode-iterations: For each active decode-phase request, compute the last
schedulable time for its decode-iteration. A decode-iteration is marked as critical if current
time is past its last schedulable time; otherwise, it is marked as non-critical.
(2) Add critical decode-iterations: Include critical decode-iterations in the batch, in the increasing
order of last schedulable time. This ensures that decode-iterations that are closest to their
TBT deadline are scheduled "rst.
(3) Add pre!ll-phase requests: Next, add pre"ll-phase requests in a non-preemptive manner.
Among these, requests that have already been scheduled at least once (i.e., active pre"ll-phase
requests) are given a higher priority. If token budget and cap on number of active requests has
not been exceeded, new pre"ll-phase requests are considered. We consider two possibilities
for ordering the incoming requests: Shortest Pre"ll First (SPF) to reduce the average or median
TTFT or First Come First Serve (FCFS) order to ensure fairness.
(4) Add non-critical decode-iterations: Finally, if there further token budget remains and number
of decodes in the batch are less than the decode limit, include additional non-critical decode-
iterations in increasing order of their last schedulable time.
Next, we discuss the impact of parameters other than the o#set (ğ›©), which has already been
covered in our earlier discussion of the scheduler.
6.1
Impact of di!erent scheduler parameters
6.1.1
Token budget (ğ‘œbudget). The token budget places an upper limit on the number of tokens that
can be processed in a single batchâ€”one token per decode-phase request and ğ‘–âˆ1 tokens per pre"ll
chunk. Fig. 6a shows how the choice of token budget ğ‘œbudget a#ects TTFT and batch-execution
time. When ğ‘œbudget is small, a 2048-token pre"ll must be split into many small chunks. This leads
to frequent kernel launches and synchronization, causing the GPU to spend more time idling. As
a result, TTFT increases. Conversely, when ğ‘œbudget is large, the scheduler can process the entire
request in fewer large batches. This improves GPU utilization but each batch takes longer to execute.
If a decode-iteration is scheduled during such a long batch, it must wait, increasing the risk of
violating its TBT constraint. The scheduler must thus choose a token budget ğ‘œbudget that balances
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 18 ---
59:18
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
(a) Median TTFT and mean batch
execution time for 100 indepen-
dent requests (each with a 2048-
token prefill and 1-token decode)
as a function of the token budget.
(b) GPU memory usage versus
number of concurrent decode-
phase requests (each with a 2048-
token prefill and 1-token decode).
(c) Batch execution time for ğ‘€
decode-phase requests (token po-
sition 513), co-scheduled with one
prefill-only request to fully utilize
the token budget of 512 tokens.
Fig. 6. Impact of token budget, concurrency, and batch composition on request execution latency and GPU
memory usage for Mistral-7B on a single NVIDIA RTX ADA 6000 GPU.
e!ciency and responsiveness. Batches should be large enough to use the GPU e#ectively, but not
so long that they excessively delay latency-sensitive decode-phase requests. Choosing this value
carefully is a key part of designing an e#ective scheduling policy.
6.1.2
Cap on the number of active requests (ğ‘). Each request generates KV tensors that must be
stored in GPU memory until the request is completed. Fig. 6b shows how GPU memory utilization
grows with the number of active decode-phase requests, based on runs with ğ‘€concurrent decode-
phase requests (each with a 2048-token pre"ll followed by a 1-token decode iteration) on Mistral-7B.
As more requests become active, memory usage increases steadily.
When too many requests are active, the scheduler may need to evict KV tensors to make room
for new ones. If a requestâ€™s KV tensors are evicted, they must be recomputed before the request can
resume decoding. This adds unnecessary delay and increases TTFT, even for requests that have not
yet been scheduled. To avoid this, the scheduler should cap the number of active requests, ensuring
that all necessary KV tensors can remain in memory without eviction. Doing so helps maintain
low latency and avoids unnecessary recomputation overheads.
6.1.3
Decode limit (ğ‘). The decode limit sets an upper bound on how many decode-iterations can
be included in a single batch. Fig. 6c shows how batch-execution time changes as the number of
decode-iterations increases, while keeping the token budget "xed at 512. When only a few decode
iterations are present, the batch "nishes quickly. However, as more decode-iterations are added,
the batch takes signi"cantly longer to "nish due to increased pressure on compute and memory
resources. Each decode-iteration triggers self-attention computation, which involves GeMVs per
requestâ€”an ine!cient computation on GPU (see Section 3.2.2).
When many decode-phase requests are active, limiting the number of decode-iterations in each
batch helps control latency. In order to meet strict TBT deadlines, the scheduler can cap the number
of decode-iterations per batch, reducing the number of TBT violations and maintaining better
responsiveness under load.
7
Experimental results
Implementation. We built SLAI on top of the open-source implementations of Sarathi-serve [2]
and vLLM [15]. The complete source code is available at: https://github.com/agrimUT/SLAI.
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 19 ---
Optimal Scheduling for LLM Inference
59:19
Evaluation. We evaluate SLAI using the Mistral-7B [12] model (mistralai/Mistral-7B-Instruct-
v0.2) and run all experiments on a single NVIDIA RTX 6000 Ada GPU with 48 GB of memory. The en-
vironment uses CUDA 12.1 and Python 3.10. For our workload, we use the openchat_shareGPT4 [31]
dataset, which contains multi-round conversations between users and ChatGPT4 [26]. Each round
is treated as a separate request.
Our baseline is Sarathi-serve con"gured with FCFS ordering for pre"ll-phase requests, referred to
as Sarathi-serve (FCFS), which represents the current state-of-the-art in LLM inference scheduling
on a single node. We also evaluate a variant of Sarathi-serve that uses shortest pre"ll "rst ordering,
referred to as Sarathi-serve (SPF).
Similarly, we assess SLAI under both FCFS and SPF pre"ll orderings, referred as SLAI (FCFS,
"xed o#set) and SLAI (SPF, "xed o#set), respectively. In addition, we evaluate a dynamic version of
SLAI, called SLAI (SPF, dynamic o#set), where the o#set ğ›©is adjusted at runtime based on GPU
memory utilization measurements. When GPU memory usage is low, a small o#set is used to allow
more pre"ll-phase requests into the system. When memory usage is high, a larger o#set is applied
so that decode-iterations are marked critical earlier and prioritized accordingly, thus clearing out
memory. For completeness, we also include vLLM in our comparisons, a scheduler that prioritizes
pre"ll-phase requests and serves as another relevant baseline.
Metrics. We evaluate two key metrics. The "rst measures the median TTFT since this is measured
only once per request. This re$ects how well the scheduler meets responsiveness objectives across
requests. The second metric is the 99th percentile of the TBT, which is computed once per generated
token and captures tail latency during the decode-iterations. This helps assess how smoothly tokens
are generated over time.
Workload. To emulate realistic tra!c, we generate synthetic traces based on request length
distributions observed in the openchat_shareGPT4 dataset (see Table 1). Each synthetic request is
generated by sampling one row from the dataset, which provides the number of pre"ll tokens and
decode tokens for that request. We then cap the total length at 8192 tokens to respect our maximum
sequence length. The sampled pre"ll and decode lengths are used directly in our trace generator.
Inference requests are generated according to a Poisson process with rates ğ‘¥â†‘{0.2, 0.4, . . . , 1.6}
requests per second. Each experiment runs for 35 minutes of wall-clock time, including a short
warm-up period before measurement begins. To ensure reproducibility, we "x random seeds for
both the trace generation and scheduler runs. We consider two types of requests associated with
paying and free-tier users. Paying users expect faster and smoother generation compared to free-tier
users. To re$ect this, we assign a TBT SLO threshold of 0.1 seconds for paying users and 0.5 seconds
for free-tier user. These values are slightly relaxed compared to real-world production settings
because our implementation is in Python (which is not fully optimized), includes telemetry overhead,
and also re$ects the inherent performance limitations of the modelâ€“hardware combination. Each
incoming request is randomly marked as associated with a paying or free-tier user with some
probability.
Dataset
Prompt length (tokens)
Decode length (tokens)
Median
P90
Std.
Median
P90
Std.
openchat_sharegpt4
1730
5696
2088
415
834
101
Table 1. Prompt and decode length (token) statistics for requests in the openchat_sharegpt4 dataset.
Results discussion. We "rst consider a scenario where each request has a 5% chance of coming
from a paying user. This low percentage re$ects the user distribution seen on platforms like
ChatGPT, where most users belong to the free tier and Figures 7a and 7b present the 99th percentile
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 20 ---
59:20
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
TBT (P99 TBT) for paying and free-tier users, respectively. Figure 7c illustrates the median TTFT
as a function of the request rate. In all experiments, we con"gure Sarathi-serve (both FCFS and
SPF variants) with a token budget of 512 to ensure that the 0.1-second TBT target for paying users
is met. For all SLAI variants, we use the same token budget, and set both the number of active
requests and concurrent decode-phase request limit to 128. For SLAI (FCFS/SPF, "xed o#set), we
set the o#set parameter to 10, which controls when a decode-phase request becomes time-critical,
whereas for SLAI (SPF, dynamic o#set), the o#set is set to 5 if GPU memory utilization is below
96%, and to 10 otherwise.
(a) 99th percentile TBT for paying users across
di"erent request rates for a target of 0.1 seconds.
(b) 99th percentile TBT for free-tier users across
di"erent request rates for a target of 0.5 seconds.
(c) Median TTFT for all users as a function of request
rate.
Fig. 7. Performance comparison of SLAI, Sarathi-serve, and vLLM under mixed user workloads with 5%
paying users. SLAI (SPF, dynamic o"set) achieves the best latency-throughput trade-o".
TBT Behavior. Figures 7a and 7b show how SLAI dynamically prioritizes requests during their
decode phase based on their TBT targets. Under Sarathi-serve, the 99th percentile TBT steadily
increases for both paying and free-tier requests as the system load grows. This happens because
every decode-iteration is included in every batch, and as load increases, so does the batch execution
time, leading to higher delays for all requests. By contrast, SLAI handles decode-iterations di#erently.
Requests from paying users have strict (low) TBT targets, which in most cases are always considered
time-critical. As the load increases and batches take longer to run, their P99 TBT naturally increases.
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 21 ---
Optimal Scheduling for LLM Inference
59:21
Free-tier requests, however, have more relaxed TBT targets. At lower loads, the scheduler can
defer these decode-iterations to prioritize pre"lls, since they are not immediately time-critical. This
initially causes their P99 TBT to rise. But as the load continues to increase and batch execution
time becomes longer, the window during which a free-tier decode-iteration remains non-critical
shortens. As a result, these decode-iterations become time-critical sooner and are prioritized earlier
in scheduling. This leads to a drop in their P99 TBT. Eventually, at high loads, all free-tier decode-
iterations are immediately marked as time-critical, and their TBT increases againâ€”now dominated
by the growing batch execution time, similar to paying users. Lastly, vLLM since it is a pre"ll-
prioritizing scheduler ends up violating P99 TBT at a relatively low load and thus is not e#ective at
managing decode-phase requests.
TTFT behaviour. Figure 7c shows the median TTFT as a function of requests per second. The
vLLM policy, which prioritizes pre"ll-phase requests, achieves the lowest median TTFT at low
loads. However, it does so by aggressively batching pre"ll requests at the expense of violating 99th
percentile TBT latency constraints, making it unsuitable for scenarios with strict QoS requirements.
Sarathi-serve improves upon this by balancing pre"ll and decode phases to maintain both acceptable
median TTFT and TBT tail latencies. When Sarathi-serve is combined with the SPF-based policy
it yields a better median TTFT than its FCFS counterpart, highlighting the bene"t of reordering
pre"ll requests by prompt length. However, Sarathi-serve does not adapt to heterogeneous TBT
deadlines across requests. SLAI (SPF, "xed o#set) addresses this by selectively deferring decode-
phase requests with relaxed deadlines, achieving further improvements in median TTFT. Finally,
SLAI (SPF, dynamic o#set) introduces dynamic decode-iteration deferral based on real-time GPU
memory utilization, allowing the system to better utilize available token budget of a batch. As a
result, SLAI delivers signi"cant performance improvements: it reduces the median TTFT from 1.5
seconds (under Sarathi-Serve (FCFS)) to 0.7 secondsâ€”a 53% improvement under high loadâ€”and
increases the maximum sustainable request rate from 1.15 to 1.45 requests per second while
maintaining a "xed median TTFT constraint of 0.5 seconds and meeting tail TBT latency targets,
representing a 26% increase in serving capacity.
See Appendix F for additional experimental results that highlight several important aspects: i)
the performance of di#erent policies as a function of prompt lengths, ii) the impact of prioritizing
paying users over free-tier users during the pre"ll phase, and iii) how the policies compare when
the proportion of paying users increases to 50% or 95%.
7.1
Choosing the parameters for SLAI (SPF, dynamic o!set)
Role of ğ›©. As described in Section 6, the last schedulable time for the ğ‘„th decode-iteration of request
ğ‘¦is given by:
ğ‘Œğ¿,ğ‘= ğ‘ ğ¿â‡‘1,ğ‘+ TBTğ‘â‡‘ğ›©Â· ğ‘batch
(9)
At the time of constructing batch ğ‘š(at time ğ‘ğ‘Œ), we mark a decode-iteration as time-critical if
ğ‘ğ‘Œâˆğ‘Œğ¿,ğ‘. Otherwise, it is considered non-critical and can be deferred to a future batch. The o#set
parameter ğ›©controls how aggressively we prioritize decode-iterations of decode-phase requests: a
smaller ğ›©delays when decode-iterations are considered critical (favoring pre"ll-phase requests),
while a larger ğ›©marks them critical sooner (favoring decode-phase requests).
Empirical motivation. Recall that our goal is to reduce the median TTFT while ensuring that
tail TBT constraints are always respected. Figure 8 shows GPU memory utilization under three
di#erent policies: SLAI (SPF, "xed o#set 10), SLAI (SPF, "xed o#set 5), and SLAI (SPF, dynamic
o#set). All experiments were conducted at a load of 1.6 requests per second.
With a "xed o#set of ğ›©= 10, decode-phase iterations are prioritized. This keeps TBT well within
bounds but underutilizes the GPU memory. For example, requests from users with a relaxed TBT
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 22 ---
59:22
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
Fig. 8. GPU memory utilization over time for SLAI (SPF, fixed o"set 10), SLAI (SPF, fixed o"set 5), and SLAI
(SPF, dynamic o"set: ğ›©= 5 below 96%, ğ›©= 10 otherwise) for a fixed load of 1.6 requests per second.
constraint of 0.5 seconds consistently achieve tail TBT values far below the target (see Fig. 7b).
This suggests we are being too conservative and missing the chance to initiate more pre"ll-phase
requests.
We then tried a "xed o#set of ğ›©= 5, which gives higher priority to pre"ll-phase requests and
delays scheduling of decode-iterations of decode-phase requests. However, at high load (e.g., 1.6
requests per second), this led to memory saturation, as seen in Fig. 8. This happens because decode-
phase requestsâ€™ KV tensors stayed in memory for too long, preventing the system from admitting
new pre"ll-phase requests. As a result, TTFT blows up, and the SLAI policy with "xed o#set 5 is
unable to sustain the 1.6 requests per second load.
These results show a clear trade-o#: we want to be aggressive in admitting pre"ll-phase requests
when memory allows, but we also need to ensure decode-phase requests do not build up and block
future admissions.
Dynamic o"set policy. To balance these needs, we propose a simple dynamic policy that
switches ğ›©based on current GPU memory usage:
ğ›©=
ï£»
5,
if GPU memory utilization < 96%
(favor pre"ll-phase requests);
10,
otherwise
(favor decode-phase iterations).
When memory usage is below 96%, we assume there is enough headroom to safely admit more
pre"ll-phase requests. When usage crosses 96%, we shift priority to decode-phase iterations to
eventually complete requests and free up memory.
The 96% threshold was chosen based on empirical observations. In our setup, each new pre"ll-
phase request is allocated memory for its KV tensors at the time of admissionâ€”even before all
pre"ll tokens are processed. We found that admitting a new pre"ll-phase request can consume up
to 4% of GPU memory. To avoid admission failures, we ensure that this much headroom is always
available, and thus set the switching threshold to 96%.
Observed e"ect. As shown in Figure 8, the dynamic policy maintains high GPU memory utiliza-
tion similar to "xed ğ›©= 5, but avoids hitting 100%, which can cause admission stalls. This dynamic
adjustment ensures that TBT constraints are met while also improving TTFT and overall system
throughput.
8
Conclusion
This paper presented a framework for designing e!cient LLM inference systems. By modeling the
unique two-phase structure of LLM inference, we identi"ed two key design principlesâ€”optimal
tiling and optimal resource allocationâ€”as essential for achieving high throughput. Based on these
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 23 ---
Optimal Scheduling for LLM Inference
59:23
insights, we introduced the RAD scheduler, which, when combined with a uniformly random
resource planner, provably achieves throughput optimality. In real-world deployments, however,
systems must also meet SLOs. We argued that practical schedulers often approximate only one of
the two design principles, thereby sacri"cing some throughput to satisfy these SLOs. To handle
heterogeneous request classes with di#erent latency needs, we proposed the SLAI scheduler. SLAI
reduces TTFT by intelligently prioritizing requests while still meeting tail TBT constraints. In
comparison to existing state-of-the-art, SLAI reduced the median TTFT by 53% and increased the
maximum serving capacity by 26% for a "xed median TTFT, while meeting the TBT constraints.
Acknowledgments
This work was supported in part by NSF Award CNS-2212202 and NSF Award 2148224, which
receives funding from OUSD R&E, NIST, and industry partners through the Resilient & Intelligent
NextG Systems (RINGS) program. We would also like to thank the reviewers and shepherd of our
ACM SIGMETRICS 2026 submission. Their thoughtful feedback and suggestions were instrumental
in improving the quality and clarity of this paper.
References
[1] 2023. Introducing AMD CDNAâ„¢3 Architecture. White Paper. Advanced Micro Devices, Inc. https://www.amd.com/
content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-3-white-paper.pdf Accessed: 2025-06-18.
[2] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gulavani, Alexey Tumanov,
and Ramachandran Ramjee. 2024. Taming Throughput-Latency Tradeo# in LLM Inference with Sarathi-Serve. In 18th
USENIX Symposium on Operating Systems Design and Implementation (OSDI 24). USENIX Association, Santa Clara, CA,
117â€“134. https://www.usenix.org/conference/osdi24/presentation/agrawal
[3] Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico LebrÃ³n, and Sumit Sanghai. 2023. Gqa:
Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245
(2023).
[4] NVIDIA Corporation. 2025. NVIDIA Dynamo. https://developer.nvidia.com/dynamo. Accessed: 2025-07-07.
[5] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. 2022. FlashAttention: Fast and Memory-E!cient
Exact Attention with IO-Awareness. arXiv:2205.14135 [cs.LG] https://arxiv.org/abs/2205.14135
[6] Frederic G Foster. 1953. On the stochastic matrices associated with certain queuing processes. The Annals of
Mathematical Statistics 24, 3 (1953), 355â€“360.
[7] Connor Holmes, Masahiro Tanaka, Michael Wyatt, Ammar Ahmad Awan, Je# Rasley, Samyam Rajbhandari, Reza Yaz-
dani Aminabadi, Heyang Qin, Arash Bakhtiari, Lev Kurilenko, and Yuxiong He. 2024. DeepSpeed-FastGen: High-
throughput Text Generation for LLMs via MII and DeepSpeed-Inference. arXiv:2401.08671 [cs.PF] https://arxiv.org/
abs/2401.08671
[8] Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong, and Yu Wang. 2024.
FlashDecoding++: Faster Large Language Model Inference on GPUs. arXiv:2311.01282 [cs.LG] https://arxiv.org/abs/
2311.01282
[9] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan
Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. 2019. GPipe: e"cient training of giant neural networks using
pipeline parallelism. Curran Associates Inc., Red Hook, NY, USA.
[10] Intel Corporation. 2024. IntelÂ® GaudiÂ® 3 AI Accelerator White Paper. Technical Report. Intel Corporation. https:
//www.intel.com/content/www/us/en/content-details/817486/intel-gaudi-3-ai-accelerator-white-paper.html Accessed:
2025-06-18.
[11] Kunal Jain, Anjaly Parayil, Ankur Mallick, Esha Choukse, Xiaoting Qin, Jue Zhang, ÃÃ±igo Goiri, Rujia Wang, Chetan
Bansal, Victor RÃ¼hle, Anoop Kulkarni, Steve Kofsky, and Saravan Rajmohan. 2025. Intelligent Router for LLM
Workloads: Improving Performance Through Workload-Aware Load Balancing. arXiv:2408.13510 [cs.DC] https:
//arxiv.org/abs/2408.13510
[12] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, LÃ©lio Renard Lavaud, Marie-Anne Lachaux,
Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, and William El Sayed. 2023. Mistral 7B.
arXiv:2310.06825 [cs.CL] https://arxiv.org/abs/2310.06825
[13] Jordan Juravsky, Bradley Brown, Ryan Ehrlich, Daniel Y. Fu, Christopher RÃ©, and Azalia Mirhoseini. 2024. Hydragen:
High-Throughput LLM Inference with Shared Pre"xes. arXiv:2402.05099 [cs.LG] https://arxiv.org/abs/2402.05099
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 24 ---
59:24
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
[14] Aditya K. Kamath, Ramya Prabhu, Jayashree Mohan, Simon Peter, Ramachandran Ramjee, and Ashish Panwar. 2025.
POD-Attention: Unlocking Full Pre"ll-Decode Overlap for Faster LLM Inference. In Proceedings of the 30th ACM
International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS
â€™25). ACM, 897â€“912. doi:10.1145/3676641.3715996
[15] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao
Zhang, and Ion Stoica. 2023. E!cient Memory Management for Large Language Model Serving with PagedAttention.
arXiv:2309.06180 [cs.LG] https://arxiv.org/abs/2309.06180
[16] Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast Inference from Transformers via Speculative Decoding.
arXiv:2211.17192 [cs.LG] https://arxiv.org/abs/2211.17192
[17] Yueying Li, Jim Dai, and Tianyi Peng. 2025. Throughput-optimal scheduling algorithms for llm inference and ai agents.
arXiv preprint arXiv:2504.07347 (2025).
[18] Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng Zhang, Kuntai Du, Jiayi Yao, Shan Lu,
Ganesh Ananthanarayanan, Michael Maire, Henry Ho#mann, Ari Holtzman, and Junchen Jiang. 2024. CacheGen:
KV Cache Compression and Streaming for Fast Large Language Model Serving. arXiv:2310.07240 [cs.NI] https:
//arxiv.org/abs/2310.07240
[19] Michael Mitzenmacher and Rana Shahout. 2025. Queueing, Predictions, and LLMs: Challenges and Open Problems.
arXiv preprint arXiv:2503.07545 (March 2025). arXiv:2503.07545 [cs.AI] https://arxiv.org/abs/2503.07545
[20] Moonmoon Mohanty, Gautham Bolar, Preetam Patil, UmaMaheswari Devi, Felix George, Pratibha Moogi, and Parimal
Parag. 2025. Deferred pre"ll for throughput maximization in LLM inference. In Proceedings of The 5th Workshop on
Machine Learning and Systems (EuroMLSysâ€™2025). ACM, New York, NY, USA, 100â€“106. doi:10.1145/3721146.3721962
[21] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri
Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia. 2021. E!cient
large-scale language model training on GPU clusters using megatron-LM. In Proceedings of the International Conference
for High Performance Computing, Networking, Storage and Analysis (St. Louis, Missouri) (SC â€™21). Association for
Computing Machinery, New York, NY, USA, Article 58, 15 pages. doi:10.1145/3458817.3476209
[22] NVIDIA. [n. d.]. Matrix Multiplication Background Userâ€™s Guide. https://docs.nvidia.com/deeplearning/performance/dl-
performance-matrix-multiplication/index.html. Accessed: February 27, 2025.
[23] NVIDIA. 2025. FasterTransformer. https://github.com/NVIDIA/FasterTransformer
[24] NVIDIA. 2025. TensorRT-LLM: A TensorRT toolbox for optimized large-language-model inference. https://github.
com/NVIDIA/TensorRT-LLM
[25] NVIDIA Corporation. 2020. NVIDIA A100 Tensor Core GPU Architecture. Technical Report. NVIDIA Corporation.
https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf Version
1.0.
[26] OpenAI. 2025. ChatGPT. https://chat.openai.com
[27] Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150 (2019).
[28] Karl Sigman and Ronald W Wol#. 1993. A review of regenerative processes. SIAM review 35, 2 (1993), 269â€“288.
[29] Biao Sun, Ziming Huang, Hanyu Zhao, Wencong Xiao, Xinyi Zhang, Yong Li, and Wei Lin. 2024. Llumnix: Dynamic
Scheduling for Large Language Model Serving. In 18th USENIX Symposium on Operating Systems Design and Imple-
mentation (OSDI 24). USENIX Association, Santa Clara, CA, 173â€“191. https://www.usenix.org/conference/osdi24/
presentation/sun-biao
[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, &ukasz Kaiser, and Illia
Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).
[31] Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2024. OpenChat: Advancing Open-
source Language Models with Mixed-Quality Data. arXiv:2309.11235 [cs.CL] https://arxiv.org/abs/2309.11235
[32] Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod
Grover, Arvind Krishnamurthy, and Luis Ceze. 2025. FlashInfer: E!cient and Customizable Attention Engine for LLM
Inference Serving. arXiv:2501.01005 [cs.DC] https://arxiv.org/abs/2501.01005
[33] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. 2022. Orca: A distributed
serving system for {Transformer-Based} generative models. In 16th USENIX Symposium on Operating Systems Design
and Implementation (OSDI 22). 521â€“538.
[34] Yilong Zhao, Shuo Yang, Kan Zhu, Lianmin Zheng, Baris Kasikci, Yang Zhou, Jiarong Xing, and Ion Stoica. 2024.
BlendServe: Optimizing O%ine Inference for Auto-regressive Large Models with Resource-aware Batching. arXiv
preprint arXiv:2411.16102 (2024).
[35] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Je# Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis,
Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. 2024. SGLang: E!cient Execution of Structured Language
Model Programs. arXiv:2312.07104 [cs.AI] https://arxiv.org/abs/2312.07104
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 25 ---
Optimal Scheduling for LLM Inference
59:25
[36] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. 2024. {DistServe}:
Disaggregating pre"ll and decoding for goodput-optimized large language model serving. In 18th USENIX Symposium
on Operating Systems Design and Implementation (OSDI 24). 193â€“210.
A
Other Related Work
A.1
Cluster-level Routing
Per-GPU schedulers rely on the router to provide a well-balanced stream of requests. Most produc-
tion systems still use simple strategies like round-robin or shortest-queue routing. These methods
overlook the complex relationship between request length, prompt size, and current GPU state. The
Intelligent Router for LLM Workloads [11] addresses this by framing routing as a sequential decision
problem. It trains a workload-aware reinforcement learning agent to minimize overall latency by
predicting each requestâ€™s response length and estimating how much delay each placement would
cause.
A.2
KV-Cache Management
In transformer models, self-attention reuses all previous tokens, making KV-cache management
critical for both speed and capacity.
Memory layout. vLLM [15] uses paged attention, which divides GPU memory into "xed-size
blocks that are dynamically assigned to requests. This reduces fragmentation and allows hundreds
of requests to run in parallel. Llumnix [29] builds on this by migrating KV tensors across replicas
in real time, balancing memory usage and lowering preemption costs.
Pre!x reuse and compression. Some systems try to reduce the amount of KV data stored or
recomputed. SGLang [35] introduces a radix-tree cache and orders batches to maximize pre"x
reuse across multi-turn chats and speculative decoding. CacheGen [18] compresses KV blocks and
streams them on demand, while FlashInfer [32] creates custom GPU kernels that operate directly
on the compressed format. These techniques are independent of scheduling and routing and can be
used alongside paged layouts or distributed setups.
Preemption strategies. When GPU memory runs out, systems must either recompute or o%oad
paused requests. vLLM [15] and Sarathi-Serve [2] evict stalled requests, splice their outputs back
into the prompt, and later rebuild the KV cache. DistServe [36], on the other hand, moves the
KV tensors to host memory and resumes decoding once space is available. Each method involves
a trade-o# between memory tra!c and computation, and interacts closely with the schedulerâ€™s
design.
A.3
Speculative decoding
Speculative decoding [16] o#ers a complementary approach that accelerates decode-phase compu-
tation by generating token drafts using a smaller auxiliary model, which are then validated by the
larger target model. While originally proposed to reduce per-request latency, this technique can
also bene"t scheduling by reducing decode durations, improving GPU throughput, and enabling
more e!cient batch formation under tight latency constraints.
B
Upper Bound Proof
In Section B.1, we establish a lower bound on the time required for any LLM inference system to
serve a given set of requests. Building on this, Section B.2 derives a lower bound on the number of
pending requests at a given time in any system. Finally, in Section B.3, we use these results to prove
Theorem 1, showing that if the request arrival rate exceeds ğ‘”/ Â¯ğ‘‘ğ‘, the number of pending requests
grows linearly over timeâ€”thereby establishing an upper bound on the systemâ€™s capacity region..
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 26 ---
59:26
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
B.1
Request Set Drain Time
The "rst part of the proof is to consider a more powerful class of resource planners and schedulers
(as described below) and lower bound the time required by them to start and complete ğ‘Ÿrequests.
Consider a set, R = {ğ‘•1, . . . , ğ‘•ğ‘›}, of ğ‘Ÿrequests, where each request ğ‘•ğ‘â€™s prompt and output
lengths are, ğ‘¤ğ‘‰
ğ‘, ğ‘¤ğ‘Š
ğ‘âˆğ‘§ğ‘—ğ‘€,ğ‘—ğ‘, and are i.i.d., across requests. Further, consider a more powerful class
of resource planners and schedulers which are able to, a) commence serving of all requests at time
0, b) serve pre"ll and decode iterations of requests in an arbitrary order, c) serve the di#erent
iterations of requests at any node, even simultaneously, while paying zero cost for request transfers.
Clearly, the realistic class of schedulers considered in Theorem 1 whichâ€” a) may commence serving
requests once they arrive, b) have to serve pre"ll and decode iterations in order according to
De"nitions 1 and 2, and, c) may only transfer requests once the respective computations at the
source node are completedâ€” are a subset of this more powerful class of systems. To develop our
bounds, we will compute the least time required by this powerful class of systems to completely
serve all the ğ‘Ÿrequests.
Consider a sequence of batches ï£¯Bğ‘Œ,ğ‘šâ€²ï£ºğ‘œğ‘†â€²
ğ‘Œ=1 at node ğ‘”â€², for every ğ‘”â€² â†‘{1, . . . ,ğ‘”}. Let the set of
batch sequences completely serve all the ğ‘Ÿrequests. Then, the request set drain time which is the
time required to complete ğ‘Ÿrequests using all ğ‘”nodes, denoted ğ‘‘(R), is given by,
ğ‘‘(ğ‘•) =
max
ğ‘šâ€²â†‘{1,...,ğ‘š}
ğ‘œğ‘†â€²
,
ğ‘Œ=1
ğ‘‘ğ‘–(Bğ‘Œ,ğ‘šâ€²),
âˆ1
ğ‘”
ğ‘š,
ğ‘šâ€²=1
ğ‘œğ‘†â€²
,
ğ‘Œ=1
ğ‘‘ğ‘–(Bğ‘Œ,ğ‘šâ€²).
(10)
Let ğ‘œ(B) be the token count of batch B. Let Sğ‘‰(B) and Sğ‘Š(B) be the sets of pre"ll and decode
phase requests in the batch. For a decode-phase request ğ‘•ğ‘let ğ‘„ğ‘denote the token position of its
decode-iteration in the batch. And, similarly for a pre"ll-phase request ğ‘•ğ‘, let ğ‘„ğ‘and ğ‘–ğ‘denote the
starting token index and chunk size of its pre"ll-iteration in the batch. Let ğ‘row, ğ‘col denote the
output tile dimensions and ğ‘red denotes the reduction tile dimension for the GeMM. Recall the batch
computation time formula for a batch B,
ğ‘‘ğ‘–(B) =
1
ğ‘Lin(ğ‘row,ğ‘col,ğ‘red)
#ğ‘œ(B)
ğ‘col
$
+
1
ğ‘nLin
ğ‘œ(B) + ğ‘€
,
ğ‘:ğ‘ğ‘‚â†‘Sğ‘(B)
ğ‘‘ğ‘Š,ğ‘•ğ‘‹(ğ‘„ğ‘)
+
ğ‘€
ğ‘‰ğ‘(ğ‘row,ğ‘col,ğ‘red)
,
ğ‘:ğ‘ğ‘‚â†‘Sğ‘€(B)
(#ğ‘„ğ‘+ ğ‘–ğ‘â‡‘1
ğ‘row
$ # ğ‘–ğ‘
ğ‘col
$ ğ‘‚
ğ‘red
+
ğ‘‚
ğ‘row
# ğ‘–ğ‘
ğ‘col
$ #ğ‘„ğ‘+ ğ‘–ğ‘â‡‘1
ğ‘red
$)
,
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 27 ---
Optimal Scheduling for LLM Inference
59:27
Then,
ğ‘‘ğ‘–(B)
(ğ‘)
âˆ
1
ğ‘Lin(ğ‘row,ğ‘col,ğ‘red)
ğ‘œ(B)
ğ‘col
+
1
ğ‘nLin
ğ‘œ(B) + ğ‘€
,
ğ‘:ğ‘ğ‘‚â†‘Sğ‘(B)
ğ‘‘ğ‘Š,ğ‘•ğ‘‹(ğ‘„ğ‘)
+
ğ‘€
ğ‘‰ğ‘(ğ‘row,ğ‘col,ğ‘red)
,
ğ‘:ğ‘ğ‘‚â†‘Sğ‘€(B)
(ğ‘„ğ‘+ ğ‘–ğ‘â‡‘1
ğ‘row
ğ‘–ğ‘
ğ‘col
ğ‘‚
ğ‘red
+
ğ‘‚
ğ‘row
ğ‘–ğ‘
ğ‘col
ğ‘„ğ‘+ ğ‘–ğ‘â‡‘1
ğ‘red
)
,
(ğ‘)
âˆ
1
ğ‘Lin(ğ‘â‡’row,ğ‘â‡’
col,ğ‘â‡’
red)
ğ‘œ(B)
ğ‘â‡’
col
+
1
ğ‘nLin
ğ‘œ(B) + ğ‘€
,
ğ‘:ğ‘ğ‘‚â†‘Sğ‘(B)
ğ‘‘ğ‘Š,ğ‘•ğ‘‹(ğ‘„ğ‘)
+
ğ‘€
ğ‘‰ğ‘(ğ‘â‡’row,ğ‘â‡’
col,ğ‘â‡’
red)
,
ğ‘:ğ‘ğ‘‚â†‘Sğ‘€(B)
(ğ‘„ğ‘+ ğ‘–ğ‘â‡‘1
ğ‘â‡’row
ğ‘–ğ‘
ğ‘â‡’
col
ğ‘‚
ğ‘â‡’
red
+
ğ‘‚
ğ‘â‡’row
ğ‘–ğ‘
ğ‘â‡’
col
ğ‘„ğ‘+ ğ‘–ğ‘â‡‘1
ğ‘â‡’
red
)
,
(11)
where (a) follows by simply removing the ceiling function, and (b) follows from Assumption 1.
Consider request ğ‘•ğ‘. Let ğ‘â‡’
lcm = LCM(ğ‘â‡’
row,ğ‘â‡’
col,ğ‘â‡’
red). By Assumption 3, ğ‘¤ğ‘‰
ğ‘is a multiple of ğ‘â‡’
lcm.
Therefore, the optimal set of chunk positions and chunk size to complete the self-attention in the
pre"ll phase in minimal time is,
Iğ‘=
ï£±
(1,ğ‘â‡’
lcm, (ğ‘â‡’
lcm + 1,ğ‘â‡’
lcm), . . . , (ğ‘¤ğ‘‰â‡‘ğ‘â‡’
lcm,ğ‘â‡’
lcm)
ï£¼
.
Then, de"ne its â€œoptimal e#ective completion timeâ€ as,
ğ‘‘ğ‘
â‡’(ğ‘¦) =
1
ğ‘Lin(ğ‘â‡’row,ğ‘â‡’
col,ğ‘â‡’
red)
ğ‘¤ğ‘‰
ğ‘+ ğ‘¤ğ‘Š
ğ‘
ğ‘â‡’
col
+
1
ğ‘nLin
(ğ‘¤ğ‘‰
ğ‘+ ğ‘¤ğ‘Š
ğ‘) + ğ‘€
ğ‘—ğ‘€
ğ‘‚+ğ‘—ğ‘
ğ‘‚
,
ğ¿=ğ‘—ğ‘€
ğ‘‚+1
ğ‘‘ğ‘Š,ğ‘•ğ‘‹(ğ‘„)
+
ğ‘€
ğ‘‰ğ‘(ğ‘â‡’row,ğ‘â‡’
col,ğ‘â‡’
red)
,
(ğ¿ğ‘‚,ğ‘Ÿğ‘‚)â†‘Iğ‘‚
(ğ‘„ğ‘+ ğ‘–ğ‘â‡‘1
ğ‘â‡’row
ğ‘–ğ‘
ğ‘â‡’
col
ğ‘‚
ğ‘â‡’
red
+
ğ‘‚
ğ‘â‡’row
ğ‘–ğ‘
ğ‘â‡’
col
ğ‘„ğ‘+ ğ‘–ğ‘â‡‘1
ğ‘â‡’
red
)
,
(12)
=
1
ğ‘Lin(ğ‘â‡’row,ğ‘â‡’
col,ğ‘â‡’
red)
ğ‘¤ğ‘‰
ğ‘+ ğ‘¤ğ‘Š
ğ‘
ğ‘â‡’
col
+
1
ğ‘nLin
(ğ‘¤ğ‘‰
ğ‘+ ğ‘¤ğ‘Š
ğ‘) + ğ‘€
ğ‘—ğ‘€
ğ‘‚+ğ‘—ğ‘
ğ‘‚
,
ğ¿=ğ‘—ğ‘€
ğ‘‚+1
ğ‘‘ğ‘Š,ğ‘•ğ‘‹(ğ‘„)
+
ğ‘€ğ‘‚
ğ‘‰ğ‘(ğ‘â‡’row,ğ‘â‡’
col,ğ‘â‡’
red)ğ‘â‡’rowğ‘â‡’
colğ‘â‡’
red
ğ‘¤ğ‘‰
ğ‘
*
ğ‘¤ğ‘‰
ğ‘+ ğ‘â‡’
lcm
+
.
(13)
Then, we have the following alternate lower bound for the request set drain time.
P+â€™$â€™!&%&â€™( 1. ğ‘‘(R) âˆ1
ğ‘š
%ğ‘›
ğ‘=1ğ‘‘ğ‘
â‡’(ğ‘¦).
P+â€™â€™*. From (10) we have,
ğ‘‘(R) âˆ1
ğ‘”
ğ‘š,
ğ‘šâ€²=1
ğ‘œğ‘†â€²
,
ğ‘Œ=1
ğ‘‘ğ‘–ï£¯Bğ‘Œ,ğ‘šâ€²ï£º,
where recall that ï£¯Bğ‘Œ,ğ‘šâ€² : ğ‘”â€² â†‘{1, . . . ,ğ‘”},ğ‘šâ†‘{1, . . . , ğ‘ğ‘šâ€²}ï£ºis a set of batches that complete all the
pre"ll and decode iterations of all the ğ‘Ÿrequests in R. The batch execution time of a batch is
lower-bounded in (11). By rearranging the terms in the summation of batch execution times, and
using (12), we obtain the result.
â«…Ì¸
Then, the following limit follows by the strong law of large numbers.
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 28 ---
59:28
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
P+â€™$â€™!&%&â€™( 2. For any ğ‘Ÿâ†‘N, let the set of ğ‘Ÿrequests ğ‘•1, . . . , ğ‘•ğ‘›have prompt and output lengths
distributed according to ğ‘§ğ‘—ğ‘€,ğ‘—ğ‘, and let these lengths be i.i.d., across requests. Then,
lim
ğ‘›â†’â†™
1
ğ‘š
%ğ‘›
ğ‘=1ğ‘‘ğ‘
â‡’(ğ‘¦)
ğ‘Ÿ
=
Â¯ğ‘‘ğ‘
ğ‘”,
where Â¯ğ‘‘ğ‘is as in Theorem 1.
B.2
A lower bound on the pending requests in the system
Let ï£¯Acont.(ğ‘)ï£º
ğ‘‘âˆ0 denote the arrival process of requests to the system, where Acont.(ğ‘) denotes the
set of requests that have arrived by time ğ‘. Let, ğ‘Š(ğ‘) âŠ‹
ï£³ï£³Acont.(ğ‘)
ï£³ï£³denote the number of arrivals
by time ğ‘. Then, recall that by the premise of the theorem, (ğ‘Š(ğ‘))ğ‘‘âˆ0 is a Poisson point process of
rate ğ‘¥, and each request has prompt and output lengths distributed according to ğ‘§ğ‘—ğ‘€,ğ‘—ğ‘i.i.d., across
requests. Finally, recall that ğ‘‡(ğ‘) denotes the number of pending requests in the system at time ğ‘.
Consider the worst-case amount of time, ğ‘‘max, required to complete a request. This is when
the prompt and output lengths are the longest possible, i.e., ğ‘ˆğ‘‰,max and ğ‘ˆğ‘Š,max. Moreover, in this
worst case, the scheduler schedules pre"ll-iterations with chunk size 1 at a time without batching
to complete the pre"ll-phase, and schedules all the decode-iterations as well without batching.
Therefore, we have,
ğ‘‘max =
max
(ğ‘‘row,ğ‘‘col)â†‘Tout,ğ‘‘redâ†‘Tred
ï£®ï£«ï£«ï£«ï£«ï£¶
ğ‘ˆğ‘‰,max + ğ‘ˆğ‘Š,max
ğ‘Lin(ğ‘row,ğ‘col,ğ‘red) + ğ‘ˆğ‘‰,max + ğ‘ˆğ‘Š,max
ğ‘nLin
+ ğ‘€
ğ‘ˆğ‘€,max+ğ‘ˆğ‘,max
,
ğ¿=1
ğ‘‘ğ‘Š,ğ‘•ğ‘‹(ğ‘„)
ï£®ï£¹ï£¹ï£¹ï£¹ï£°
.
(14)
Then, we can prove the following lower bound on the number of pending requests in the system.
L)##. 1. Consider a system with any resource planner and scheduler. Let Acont.(ğ‘) denote the set
of requests that have arrived by time ğ‘. Let the â€œoptimal request completion timeâ€ function ğ‘‘ğ‘
â‡’(Â·) be as
in (13). Let ğ‘‘max be as in (14). Then, the number of pending requests in the system at any time ğ‘is
lower bounded as,
ğ‘‡(ğ‘) âˆ
ï£¯1
ğ‘š
%
ğ‘â‡”ğ‘‹(ğ‘‘) ğ‘‘ğ‘
â‡’(ğ‘¦) â‡‘ğ‘ï£º
ğ‘‘max
.
P+â€™â€™*. The proof is by contradiction. Suppose there is a time time ğ‘when,
ğ‘‡(ğ‘) <
ï£¯1
ğ‘š
%
ğ‘â‡”ğ‘‹(ğ‘‘) ğ‘‘ğ‘
â‡’(ğ‘¦) â‡‘ğ‘ï£º
ğ‘‘max
.
If 1
ğ‘š
%
ğ‘â‡”ğ‘‹cont.(ğ‘‘) ğ‘‘ğ‘
â‡’(ğ‘¦) â‡”ğ‘, this immediately leads to a contradiction since it implies ğ‘‡(ğ‘) < 0.
The number of pending requests in the system has to be non-negative.
Consider 1
ğ‘š
%
ğ‘â‡”ğ‘‹cont.(ğ‘‘) ğ‘‘ğ‘
â‡’(ğ‘¦) â‡”ğ‘. In this case, the worst-case amount of time taken to complete
the ğ‘‡(ğ‘) requests happens is, a) the prompt and output lengths are the longest possible, b) none of
the requests have started processing by time ğ‘, and c) the iterations of each request are scheduled
without batching. Due to (14), the worst-case amount of time taken to complete the ğ‘‡(ğ‘) requests
is ğ‘‡(ğ‘)ğ‘‘max. This implies that the amount of time it took to complete all the requests in Acont.(ğ‘) is,
ğ‘‘(Acont.(ğ‘)) â‡”ğ‘+ ğ‘‡(ğ‘)ğ‘‘max,
< ğ‘+
ï£¯1
ğ‘š
%
ğ‘â‡”ğ‘‹(ğ‘‘) ğ‘‘ğ‘
â‡’(ğ‘¦) â‡‘ğ‘ï£º
ğ‘‘max
ğ‘‘max,
< 1
ğ‘”
,
ğ‘â‡”ğ‘‹(ğ‘‘)
ğ‘‘ğ‘
â‡’(ğ‘¦).
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 29 ---
Optimal Scheduling for LLM Inference
59:29
This is a contradiction to Proposition 1 which lowers bound the time to complete a set of requests
for any pair of resource planner and scheduler.
This completes the proof by contradiction.
â«…Ì¸
B.3
Proof of Theorem 1
Recall that ï£¯Acont.(ğ‘)ï£º
ğ‘‘âˆ0 denotes the arrival process of requests to the system, where Acont.(ğ‘)
denotes the set of requests that have arrived by time ğ‘. And, ğ‘Š(ğ‘) âŠ‹
ï£³ï£³Acont.(ğ‘)
ï£³ï£³denotes the number
of arrivals by time ğ‘. By the premise of the Theorem, ğ‘Š(ğ‘) is a Poisson point process of rate ğ‘¥.
Therefore, by the strong law of large numbers,
lim
ğ‘‘â†’â†™
ğ‘Š(ğ‘)
ğ‘
= ğ‘¥,
almost surely.
(15)
Consider the sequence of requests (ğ‘•ğ‘)â†™
ğ‘=1, such that ğ‘¤ğ‘‰
ğ‘, ğ‘¤ğ‘Š
ğ‘âˆğ‘§ğ‘—ğ‘€,ğ‘—ğ‘i.i.d., across requests. Then,
again by the strong law of large numbers,
lim
ğ‘›â†’â†™
1
ğ‘Ÿ
ğ‘›
,
ğ‘=1
ğ‘‘ğ‘
â‡’(ğ‘¦) = E
-
ğ‘‘ğ‘
â‡’(1)
.
,
almost surely,
(ğ‘)= Â¯ğ‘‘ğ‘,
where (a) follows from the formula for ğ‘‘ğ‘
â‡’() in (13) and the expression for Â¯ğ‘‘ğ‘in Theorem 1.
For any ğ‘âˆ0, requests ï£¯ğ‘•ğ‘
ï£ºğ‘‹(ğ‘‘)
ğ‘=1 arrive by time ğ‘. Moreover, since ğ‘Š(ğ‘) is a Poisson point process,
ğ‘Š(ğ‘) â†’â†™almost surely as ğ‘â†’â†™. Therefore,
lim
ğ‘‘â†’â†™
1
ğ‘Š(ğ‘)
,
ğ‘â‡”ğ‘‹(ğ‘‘)
ğ‘‘ğ‘
â‡’(ğ‘¦) = Â¯ğ‘‘ğ‘,
almost surely.
(16)
Combining (15) and (16), we get,
lim
ğ‘‘â†’â†™
1
ğ‘
,
ğ‘â‡”ğ‘‹(ğ‘‘)
ğ‘‘ğ‘
â‡’(ğ‘¦) = lim
ğ‘‘â†’â†™
ï£¾ï£²
ï£½
1
ğ‘Š(ğ‘)
,
ğ‘â‡”ğ‘‹(ğ‘‘)
ğ‘‘ğ‘
â‡’(ğ‘¦)ï£´ï£¦
ï£­
(ğ‘Š(ğ‘)
ğ‘
)
,
= ï£¾ï£²
ï£½
lim
ğ‘‘â†’â†™
1
ğ‘Š(ğ‘)
,
ğ‘â‡”ğ‘‹(ğ‘‘)
ğ‘‘ğ‘
â‡’(ğ‘¦)ï£´ï£¦
ï£­
(
lim
ğ‘‘â†’â†™
ğ‘Š(ğ‘)
ğ‘
)
,
= ğ‘¥Â¯ğ‘‘ğ‘.
(17)
From Lemma 1 we have,
1
ğ‘ğ‘‡(ğ‘) âˆ
ï£¯1
ğ‘š
1
ğ‘‘
%
ğ‘â‡”ğ‘‹(ğ‘‘) ğ‘‘ğ‘
â‡’(ğ‘¦) â‡‘1ï£º
ğ‘‘max
.
Taking the limit and substituting (17), we get,
lim
ğ‘‘â†’â†™
1
ğ‘ğ‘‡(ğ‘) âˆ
ï£¯1
ğ‘š
ï£¯limğ‘‘â†’â†™1
ğ‘‘
%
ğ‘â‡”ğ‘‹(ğ‘‘) ğ‘‘ğ‘
â‡’(ğ‘¦)ï£ºâ‡‘1ï£º
ğ‘‘max
,
=
ğ‘ Â¯ğ‘˜ğ‘…
ğ‘š
â‡‘1
ğ‘‘max ,
almost surely.
Since, ğ‘¥Â¯ğ‘‘ğ‘> ğ‘”, the right hand side of the above equation is positive. Therefore, substituting
ğ‘=
ğ‘‡Â¯ğ‘ˆğ‘…
ğ‘†
â‡‘1
ğ‘˜max
completes the proof of Theorem 1.
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 30 ---
59:30
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
C
Throughput Optimality Of RAD Proof
This section contains the proof of Theorem 2. The outline of the proof is as follows:
(1) In Section C.1, we construct a discrete-time process that tracks the number of pending
requests at a node at the start of cycles of RAD. We show that this forms a discrete-time
Markov chain (DTMC).
(2) In Section C.2, we bound the expected cycle times for RAD when there are at least ğ‘pending
requests at the start of the cycle.
(3) Building on this, in Section C.3, we show that under the conditions in Theorem 2, the DTMC
tracking the number of requests at the start of cycles is positive-recurrent.
(4) Finally, in Section C.4 we show that the number of pending requests at any node forms a
positive-recurrent regenerative process by showing that the expected cycle time is bounded.
C.1
The Discrete Time Markov Chain (DTMC) Model
Here, we analyze the discrete-time process that tracks the number of pending requests at a node at
the start of cycles. To do so, we introduce some stochastic processes. Consider node ğ‘”â€².
(1) Let ï£¯Acont.
ğ‘šâ€²
(ğ‘)ï£º
ğ‘‘âˆ0 denote the arrival process at the node. The arrival times form a Poisson
point process of rate ğ‘¥/ğ‘”, and the ğ‘¦th request arrival to this node, ğ‘•ğ‘, has prompt and output
lengths, ğ‘¤ğ‘‰
ğ‘, ğ‘¤ğ‘Š
ğ‘âˆğ‘§ğ‘—ğ‘€,ğ‘—ğ‘, independent of other requests.
(2) Let (Fğ‘šâ€² (ğ‘))ğ‘‘âˆ0 denote the corresponding "ltration with Fğ‘šâ€² (ğ‘) = ğ›¬(Acont.
ğ‘šâ€²
(ğ‘â€²) : 0 â‡”ğ‘â€² â‡”ğ‘).
(3) Let ğ‘Œâ‡‘1
ğ‘šâ€² [ğ‘–] denote the start time of the ğ‘–th cycle of the RAD scheduler, for a cycle ğ‘–â†‘N.
Since the RAD scheduler only uses causal information, it can be easily shown that ğ‘Œâ‡‘1
ğ‘šâ€² [ğ‘–] is
a stopping-time with respect to (Fğ‘šâ€² (ğ‘))ğ‘‘âˆ0 for any ğ‘–.
(4) Let ğ‘Œğ‘šâ€² (ğ‘) denote the cycle that the RAD scheduler is in at time ğ‘, i.e., ğ‘Œğ‘šâ€² (ğ‘) = max{ğ‘–â†‘N :
ğ‘Œâ‡‘1
ğ‘šâ€² [ğ‘–] â‡”ğ‘}.
(5) Let Xcont.
ğ‘šâ€²
(ğ‘) denote the set of pending requests at time ğ‘.
(6) Let Xğ‘šâ€² [ğ‘–] âŠ‹Xcont.
ğ‘šâ€²
(ğ‘Œâ‡‘1
ğ‘š[ğ‘–]) denote the set of pending requests at the start of cycle ğ‘–.
(7) Let Ağ‘šâ€² [ğ‘–] denote the set of arrivals that happen during cycle ğ‘–. That is, it is the set of arrivals
in ï£¯Acont.
ğ‘šâ€²
(ğ‘)ï£º
ğ‘‘âˆ0 between times
ï£¸
ğ‘Œâ‡‘1
ğ‘šâ€² [ğ‘–],ğ‘Œâ‡‘1
ğ‘šâ€² [ğ‘–+ 1]
+
.
(8) Let Rğ‘šâ€² [ğ‘–] denote the set of requests that complete in cycle ğ‘–.
(9) Let ğ‘‡ğ‘šâ€² (ğ‘) âŠ‹
ï£³ï£³Xcont.
ğ‘šâ€²
(ğ‘)
ï£³ï£³denote the number of pending requests at the node at time ğ‘.
(10) Let ğ‘’ğ‘šâ€² [ğ‘–] = |Xğ‘šâ€² [ğ‘–]| denote the number of pending requests at the node at the start of cycle
ğ‘–.
We prove the following.
L)##. 2. (ğ‘’ğ‘šâ€² [ğ‘–])â†™
ğ‘Ÿ=1 is a DTMC. That is,
ğ‘—(ğ‘’ğ‘šâ€² [ğ‘–+ 1] = ğ›¯[ğ‘–+ 1] | ğ‘’ğ‘šâ€² [ğ‘–] = ğ›¯[ğ‘–],ğ‘’ğ‘šâ€² [ğ‘–â‡‘1] = ğ›¯[ğ‘–â‡‘1], . . . ) =
ğ‘—(ğ‘’ğ‘šâ€² [ğ‘–+ 1] = ğ›¯[ğ‘–+ 1] | ğ‘’ğ‘šâ€² [ğ‘–] = ğ›¯[ğ‘–]),
for any ğ‘–â†‘N and ğ›¯[0], . . . ,ğ›¯[ğ‘–+ 1] â†‘N.
The result will hold true due to the following three reasons,
(1) The arrivals follow a Poisson point process, making future arrivals independent of past
arrivals.
(2) The RAD scheduler is â€œmemorylessâ€ across cycles. It does not use information about past
cycles in order to schedule the requests in the latest cycle.
(3) The prompt and output lengths are i.i.d., across requests, and the RAD scheduler chooses
requests in FCFS. Therefore, after knowing the number of requests at the start of the latest
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 31 ---
Optimal Scheduling for LLM Inference
59:31
cycle, the prompt and output lengths of these requests are independent of the number of
requests in previous cycles.
We provide a formal proof below.
P+â€™â€™*. Since ğ‘Œâ‡‘1
ğ‘šâ€² [ğ‘–] is a stopping time, by the strong Markov property of the Poisson point
process, we have,
ï£¯Acont.
ğ‘šâ€²
(ğ‘)ï£º
ğ‘‘>ğ‘¡â‡‘1
ğ‘†â€² [ğ‘Ÿ] âˆˆâˆˆFğ‘šâ€² (ğ‘Œâ‡‘1
ğ‘šâ€² [ğ‘–]),
âˆ‹ğ‘–â†‘N,
(18)
where âˆˆâˆˆdenotes statistical independence. Since RAD is a causal scheduler, we have that Xcont.
ğ‘šâ€²
(ğ‘)
is causal, i.e., it is measurable in Fğ‘šâ€² (ğ‘). This is denoted as Xcont.
ğ‘šâ€²
(ğ‘) â†‘ğ‘ŒFğ‘šâ€² (ğ‘). Therefore, we have,
Xğ‘šâ€² [ğ‘–] â†‘ğ‘ŒFğ‘šâ€² (ğ‘Œâ‡‘1
ğ‘šâ€² [ğ‘–]),
âˆ‹ğ‘–â†‘N,
(19)
In the ğ‘–th cycle, the set of requests completed by the RAD scheduler is a function of the
set of requests at the start of the cycle, Xğ‘šâ€² [ğ‘–], and arrival pattern after the start of the cycle,
ï£¯Acont.
ğ‘šâ€²
(ğ‘)ï£º
ğ‘‘>ğ‘¡â‡‘1
ğ‘†â€² [ğ‘Ÿ]. Therefore, the start time of the (ğ‘–+ 1)th cycle, ğ‘Œâ‡‘1
ğ‘šâ€² [ğ‘–+ 1], the sets of arrived
and completed requests in the ğ‘–th cycle, Ağ‘šâ€² [ğ‘–] and Rğ‘šâ€² [ğ‘–], are all measurable functions of,
(1) the set of requests at the node at the start of the ğ‘–th cycle Xğ‘šâ€² [ğ‘–],
(2) the arrival process after the start of the ğ‘–th cycle, ï£¯Acont.
ğ‘šâ€²
(ğ‘)ï£º
ğ‘‘>ğ‘¡â‡‘1
ğ‘†â€² [ğ‘Ÿ].
Moreover, Xğ‘šâ€² [ğ‘–+ 1] = Xğ‘šâ€² [ğ‘–] ï£¬Ağ‘šâ€² [ğ‘–] \ Rğ‘šâ€² [ğ‘–]. So, for some function, ğ›±, that is measurable with
respect to ğ›¬
*
Xğ‘šâ€² [ğ‘–], ï£¯Acont.
ğ‘šâ€²
(ğ‘)ï£º
ğ‘‘>ğ‘¡â‡‘1
ğ‘†â€² [ğ‘Ÿ]
+
,
Xğ‘šâ€² [ğ‘–+ 1] = ğ›±
*
Xğ‘šâ€² [ğ‘–], ï£¯Acont.
ğ‘šâ€²
(ğ‘)ï£º
ğ‘‘>ğ‘¡â‡‘1
ğ‘†â€² [ğ‘Ÿ]
+
.
(20)
Recall, ğ‘’ğ‘šâ€² [ğ‘–] = |Xğ‘šâ€² [ğ‘–]|. From (19), for any ğ‘–â€² < ğ‘–, we have ğ‘’ğ‘šâ€² [ğ‘–â€²] â†‘ğ‘ŒFğ‘šâ€² ï£¯ğ‘Œâ‡‘1
ğ‘šâ€² [ğ‘–]ï£º. Therefore,
from (20) and the strong Markov property in (18), we can establish the following property,
ğ‘—(ğ‘’ğ‘šâ€² [ğ‘–+ 1] = ğ›¯[ğ‘–+ 1] | Xğ‘šâ€² [ğ‘–] = S,ğ‘’ğ‘šâ€² [ğ‘–â‡‘1] = ğ›¯[ğ‘–â‡‘1], . . . ) =
ğ‘—(ğ‘’ğ‘šâ€² [ğ‘–+ 1] = ğ›¯[ğ‘–+ 1] | Xğ‘šâ€² [ğ‘–] = S),
for any ğ‘–â†‘N and ğœ´[0], . . . , ğœ´[ğ‘–+ 1] â†‘N and any set of requests S. Also, since ğ‘—(ğ‘’= ğ›¯|ğ‘“=
ğ‘£,ğ›´(ğ‘“) = ğ›´(ğ‘£)) = ğ‘—(ğ‘’= ğ›¯|ğ‘“= ğ‘£) for any random variables ğ‘’and ğ‘“and measurable function ğ›´(Â·),
we can add ğ‘’ğ‘šâ€² [ğ‘–] = |S| to the conditioning on both sides,
ğ‘—(ğ‘’ğ‘šâ€² [ğ‘–+ 1] = ğ›¯[ğ‘–+ 1] | Xğ‘šâ€² [ğ‘–] = S,ğ‘’ğ‘šâ€² [ğ‘–] = |S| ,ğ‘’ğ‘šâ€² [ğ‘–â‡‘1] = ğ›¯[ğ‘–â‡‘1], . . . ) =
ğ‘—(ğ‘’ğ‘šâ€² [ğ‘–+ 1] = ğ›¯[ğ‘–+ 1] | Xğ‘šâ€² [ğ‘–] = S,ğ‘’ğ‘šâ€² [ğ‘–] = |S|),
(21)
Since RAD chooses requests FCFS, given ğ‘’ğ‘šâ€² [ğ‘–] = ğ›¯, Xğ‘šâ€² [ğ‘–] is simply the set of the last ğ›¯requests
in ï£¯Acont.
ğ‘šâ€²
(ğ‘)ï£º
ğ‘‘â‡”ğ‘¡â‡‘1
ğ‘†â€² [ğ‘Ÿ]. Given the cardinality of Xğ‘šâ€² [ğ‘–], which is ğ‘’ğ‘šâ€² [ğ‘–], the prompt and output
lengths of the requests in Xğ‘šâ€² [ğ‘–] is i.i.d., by the premise of the Theorem. Therefore, we have,
ğ‘—(Xğ‘šâ€² [ğ‘–] = S | ğ‘’ğ‘šâ€² [ğ‘–] = |S|) =
ï£·
ğ‘:ğ‘ğ‘‚â†‘S
ğ‘§ğ‘—ğ‘€,ğ‘—ğ‘
*
ğ‘ˆğ‘‰
ğ‘,ğ‘ˆğ‘Š
ğ‘
+
(22)
where ğ‘ˆğ‘‰
ğ‘and ğ‘ˆğ‘Š
ğ‘denotes the prompt and output length realizations of request ğ‘•ğ‘.
Again, since given ğ‘’ğ‘šâ€² [ğ‘–] = ğ›¯, Xğ‘šâ€² [ğ‘–] is simply the set of the last ğ›¯requests in ï£¯Acont.
ğ‘šâ€²
(ğ‘)ï£º
ğ‘‘â‡”ğ‘¡â‡‘1
ğ‘†â€² [ğ‘Ÿ],
we have that Xğ‘šâ€² [ğ‘–] is independent of the number of requests at the start of previous cycles.
Therefore we have,
ğ‘—(Xğ‘šâ€² [ğ‘–] = S | ğ‘’ğ‘šâ€² [ğ‘–] = |S| ,ğ‘’ğ‘šâ€² [ğ‘–â‡‘1] = ğ›¯[ğ‘–â‡‘1], . . . ) =
ï£·
ğ‘:ğ‘ğ‘‚â†‘S
ğ‘§ğ‘—ğ‘€,ğ‘—ğ‘…(ğ‘ˆğ‘‰
ğ‘,ğ‘ˆğ‘Š
ğ‘),
(23)
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 32 ---
59:32
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
Then, multiplying (22) on the right-hand-side of (21) and multiplying (23) on the left-hand-size,
and integrating over all sets S of size ğ›¯[ğ‘–], we obtain the desired Markov property,
ğ‘—(ğ‘’ğ‘šâ€² [ğ‘–+ 1] = ğ›¯[ğ‘–+ 1] | ğ‘’ğ‘šâ€² [ğ‘–] = ğ›¯[ğ‘–],ğ‘’ğ‘šâ€² [ğ‘–â‡‘1] = ğ›¯[ğ‘–â‡‘1], . . . ) =
ğ‘—(ğ‘’ğ‘šâ€² [ğ‘–+ 1] = ğ›¯[ğ‘–+ 1] | ğ‘’ğ‘šâ€² [ğ‘–] = ğ›¯[ğ‘–]),
â«…Ì¸
C.2
Cycle Times
The RAD scheduler in Algorithm 1 proceeds in cycles. We will bound the expected time of a cycle
given that there are at least ğ‘requests at the start of the cycle whose prompt and output lengths
are distributed according to ğ‘§ğ‘—ğ‘€,ğ‘—ğ‘. Recall that in this case RAD starts and completes ğ‘requests
chosen in FCFS manner in the cycle.
Let the set ofğ‘chosen requests be denoted by R = {ğ‘•1, . . . , ğ‘•ğ‘€} (where the indexing is without loss
of generality). Consider a requestğ‘•ğ‘â†‘R. Recall that, due to Assumption 3,ğ‘â‡’
lcm = LCM(ğ‘â‡’
row,ğ‘â‡’
col,ğ‘â‡’
red)
divides the prompt length ğ‘¤ğ‘‰
ğ‘. In this case, the RAD scheduler schedules chunks of size ğ‘â‡’
lcm for the
request, ğ‘•ğ‘alone in a batch. Let,
Iğ‘=
ï£±
(1,ğ‘â‡’
lcm), (ğ‘â‡’
lcm + 1,ğ‘â‡’
lcm), . . . , (ğ‘¤ğ‘‰
ğ‘â‡‘ğ‘â‡’
lcm,ğ‘â‡’
lcm)
ï£¼
denote the set of chunk positions and chunk sizes scheduled to complete the pre"ll-phase. Then,
required time to complete its pre"ll-phase, once scheduled, is,
ğ‘‘ğ‘‰
ğ‘=
1
ğ‘Lin(ğ‘â‡’row,ğ‘â‡’
col,ğ‘â‡’
red)
ğ‘¤ğ‘‰
ğ‘
ğ‘â‡’
col
+
1
ğ‘nLin
ğ‘¤ğ‘‰
ğ‘
+
ğ‘€
ğ‘‰ğ‘(ğ‘â‡’row,ğ‘â‡’
col,ğ‘â‡’
red)
,
(ğ¿ğ‘‚,ğ‘Ÿğ‘‚)â†‘Iğ‘‚
(ğ‘„ğ‘+ ğ‘–ğ‘â‡‘1
ğ‘â‡’row
ğ‘–ğ‘
ğ‘â‡’
col
ğ‘‚
ğ‘â‡’
red
+
ğ‘‚
ğ‘â‡’row
ğ‘–ğ‘
ğ‘â‡’
col
ğ‘„ğ‘+ ğ‘–ğ‘â‡‘1
ğ‘â‡’
red
)
,
Taking the sum and the expectation, the expected amount of time to complete the pre"ll-phases of
all the ğ‘requests can be expressed as,
E
-
ğ‘‘ğ‘‰ğ‘œ(R)
.
= ğ‘Â·
D
1
ğ‘Lin(ğ‘â‡’row,ğ‘â‡’
col,ğ‘â‡’
red)
E
-
ğ‘¤ğ‘‰.
ğ‘â‡’
col
+
E
-
ğ‘¤ğ‘‰.
ğ‘nLin
+
ğ‘€ğ‘‚
ğ‘‰ğ‘(ğ‘â‡’row,ğ‘â‡’
col,ğ‘â‡’
red)ğ‘â‡’rowğ‘â‡’
colğ‘â‡’
red
E
-
ğ‘¤ğ‘‰(ğ‘¤ğ‘‰+ ğ‘â‡’
lcm)
.
E
.
(24)
Next, we will focus on bounding the e#ective execution time of decode-iterations. For this,
consider the set of batch indices, Mğ‘Š, where only decode-iterations are scheduled, and denote the
batches by ï£¯Bğ‘Š
ğ‘Œ
ï£º
ğ‘Œâ†‘Mğ‘. Moreover, let Mğ‘Š
â‡’denote the subset of these batch indices that contain
exactly ğ‘â‡’
col decode-iterations. In other words, these batches are optimally tiled. Therefore, for
ğ‘šâ†‘Mğ‘Š
â‡’we have the batch execution time given as,
ğ‘‘ğ‘–
â‡’(Bğ‘Š
ğ‘Œ) =
1
ğ‘Lin(ğ‘â‡’row,ğ‘â‡’
col,ğ‘â‡’
red)
ğ‘â‡’
col
ğ‘â‡’
col
+
1
ğ‘nLin
ğ‘â‡’
col + ğ‘€
,
ğ‘:ğ‘ğ‘‚â†‘Sğ‘(Bğ‘
ğ‘ƒ)
ğ‘‘ğ‘Š,ğ‘•ğ‘‹(ğ‘„ğ‘).
where recall that ğ‘„ğ‘is the token position corresponding to the decode-iteration of request ğ‘•ğ‘in the
batch.
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 33 ---
Optimal Scheduling for LLM Inference
59:33
For all other batches ğ‘šâ†‘Mğ‘Š\ Mğ‘Š
â‡’, the batch execution times are upper bounded as,
ğ‘‘ğ‘–(Bğ‘Š
ğ‘Œ) â‡”
max
(ğ‘‘row,ğ‘‘col)â†‘Tout,ğ‘‘redâ†‘Tout
1
ğ‘Lin(ğ‘row,ğ‘col,ğ‘red)
Fï£³ï£³Bğ‘Š
ğ‘Œ
ï£³ï£³
ğ‘col
G
+
1
ğ‘nLin
ï£³ï£³Bğ‘Š
ğ‘Œ
ï£³ï£³+ ğ‘€
,
ğ‘:ğ‘ğ‘‚â†‘Sğ‘(Bğ‘
ğ‘ƒ)
ğ‘‘ğ‘Š,ğ‘•ğ‘‹(ğ‘„ğ‘).
Let Râ‡’denote the set of requests whose every decode iteration is scheduled in an optimally tiled
batch. For ğ‘•ğ‘â‡’â†‘Râ‡’, denote the â€œe#ective completion timeâ€ it took to complete its decode-phase as,
ğ‘‘ğ‘Š
â‡’(ğ‘¦â‡’) =
1
ğ‘Lin(ğ‘â‡’row,ğ‘â‡’
col,ğ‘â‡’
red)
ğ‘¤ğ‘Š
ğ‘â‡’
ğ‘â‡’
col
+
1
ğ‘nLin
ğ‘¤ğ‘Š
ğ‘â‡’+ ğ‘€
ğ‘—ğ‘€
ğ‘‚â‡’+ğ‘—ğ‘
ğ‘‚â‡’
,
ğ¿=ğ‘—ğ‘€
ğ‘‚â‡’+1
ğ‘‘ğ‘Š,ğ‘•ğ‘‹(ğ‘„).
For all other requests, ğ‘•ğ‘â€² â†‘R \ Râ‡’, their â€œe#ective completion timesâ€ can be bounded as,
ğ‘‘ğ‘Š,max(ğ‘¦â€²) â‡”
max
(ğ‘‘row,ğ‘‘col)â†‘Tout,ğ‘‘redâ†‘Tout
1
ğ‘Lin(ğ‘row,ğ‘col,ğ‘red) ğ‘¤ğ‘Š
ğ‘â€² +
1
ğ‘nLin
ğ‘¤ğ‘Š
ğ‘â€² + ğ‘€
ğ‘—ğ‘€
ğ‘‚â€²+ğ‘—ğ‘
ğ‘‚â€²
,
ğ¿=ğ‘—ğ‘€
ğ‘‚â€²+1
ğ‘‘ğ‘Š,ğ‘•ğ‘‹(ğ‘„).
Due to Assumption 3,
ğ‘‘ğ‘Š,max(ğ‘¦â€²) â‡”ğ‘‘max,
almost surely,
where ğ‘‘max is as de"ned in Theorem 2.
The condition in Line 6 of Algorithm 1 ensures that when ğ‘requests are being processed in the
cycle, i.e., |R| = ğ‘, then all the decode-iterations of at least ğ‘â‡‘ğ‘â‡’
col + 1 requests are optimally tiled,
i.e., |Râ‡’| âˆğ‘â‡‘ğ‘â‡’
col. Therefore, |R \ Râ‡’| â‡”ğ‘â‡’
col â‡‘1.
The amount of time required to complete the decode-phase of requests in this cycle can thus be
bounded as,
ğ‘‘ğ‘Šğ‘œ=
,
ğ‘Œâ†‘Mğ‘
ğ‘‘ğ‘–(Bğ‘Š
ğ‘Œ),
=
,
ğ‘Œâ†‘Mğ‘
â‡’
ğ‘‘ğ‘–(Bğ‘Š
ğ‘Œ) +
,
ğ‘Œâ†‘Mğ‘\Mğ‘
â‡’
ğ‘‘ğ‘–(Bğ‘Š
ğ‘Œ),
â‡”
,
ğ‘â‡’:ğ‘ğ‘‚â‡’â†‘Râ‡’
ğ‘‘ğ‘Š
â‡’(ğ‘¦â‡’) +
,
ğ‘â€²:ğ‘ğ‘‚â€² â†‘R\Râ‡’
ğ‘‘ğ‘Š,max(ğ‘¦â€²),
â‡”
,
ğ‘:ğ‘ğ‘‚â†‘R
ğ‘‘ğ‘Š
â‡’(ğ‘¦) +
,
ğ‘:ğ‘ğ‘‚â€² â†‘R\Râ‡’
ğ‘‘ğ‘Š,max(ğ‘¦â€²),
â‡”
,
ğ‘:ğ‘ğ‘‚â†‘R
ğ‘‘ğ‘Š
â‡’(ğ‘¦) + (ğ‘â‡’
col â‡‘1)ğ‘‘max.
Since each request has prompt and output length distributed according to ğ‘§ğ‘—ğ‘€,ğ‘—ğ‘, the expected time
to complete the decode-phase of all the requests in the cycle can be upper bounded as,
E
-
ğ‘‘ğ‘Šğ‘œ.
â‡”ğ‘ï£¾ï£²
ï£½
1
ğ‘Lin(ğ‘â‡’row,ğ‘â‡’
col,ğ‘â‡’
red)
E
-
ğ‘¤ğ‘Š.
ğ‘â‡’
col
+
1
ğ‘nLin
E
-
ğ‘¤ğ‘Š.
+ ğ‘€E
ï£®ï£«ï£«ï£«ï£«ï£¶
ğ‘—ğ‘€+ğ‘—ğ‘
,
ğ¿=ğ‘—ğ‘€+1
ğ‘‘ğ‘Š,ğ‘•ğ‘‹(ğ‘„)
ï£®ï£¹ï£¹ï£¹ï£¹ï£°
ï£´ï£¦
ï£­
+ (ğ‘â‡’
col â‡‘1)ğ‘‘max.
(25)
Therefore, the expected cycle time is upper bounded as,
E
-
ğ‘‘cycle.
= E
-
ğ‘‘ğ‘‰ğ‘œ.
+ E
-
ğ‘‘ğ‘Šğ‘œ.
,
â‡”ğ‘Â¯ğ‘‘ğ‘+ (ğ‘â‡’
col â‡‘1)ğ‘‘max,
(26)
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 34 ---
59:34
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
where the upper bound is obtained by combining (24), (25), and the de"nition of Â¯ğ‘‘ğ‘from Theorem
1.
C.3
Positive recurrence of the DTMC
L)##. 3. Under the same conditions as in Theorem 2, (ğ‘’ğ‘šâ€² [ğ‘–])â†™
ğ‘Ÿ=1 is positive recurrent.
P+â€™â€™*. Consider a state ğ‘’ğ‘šâ€² [ğ‘–] with ğ‘’ğ‘šâ€² [ğ‘–] âˆğ‘. Denote ğ‘Šğ‘šâ€² [ğ‘–] = |Ağ‘šâ€² [ğ‘–]| to be the number of
request arrivals that happen during the ğ‘–th cycle of RAD. Then, ğ‘’ğ‘šâ€² [ğ‘–+ 1] = ğ‘’ğ‘šâ€² [ğ‘–] â‡‘ğ‘+ ğ‘Šğ‘šâ€² [ğ‘–],
since RAD completes exactly ğ‘requests in a cycle if there are at least ğ‘requests at the start of the
cycle.
Since there are already at least ğ‘requests at the start of the cycle, and RAD chooses requests in
a FCFS manner, the set of requests completed in the cycle is a subset of these requests. That is,
Rğ‘šâ€² [ğ‘–] â–³Xğ‘šâ€² [ğ‘–]
and
|ğ‘•ğ‘šâ€² [ğ‘–]| = ğ‘,
if ğ‘’ğ‘šâ€² [ğ‘–] âˆğ‘.
Therefore, in this case, the duration of this cycle is independent of the arrivals after the cyle starts,
i.e.,
ğ‘Œâ‡‘1
ğ‘šâ€² [ğ‘–+ 1] â‡‘ğ‘Œâ‡‘1
ğ‘šâ€² [ğ‘–] âˆˆâˆˆï£¯Acont.
ğ‘šâ€²
(ğ‘)ï£º
ğ‘‘>ğ‘¡â‡‘1
ğ‘†â€² [ğ‘Ÿ] ,
if ğ‘’ğ‘šâ€² [ğ‘–] âˆğ‘.
Requests arrive according to a Poisson point process of rate ğ‘¥/ğ‘”. Therefore, by the independence
of the cycle duration to the arrival process, we have the expected number of arrivals during the
cycle given as,
E [ğ‘Šğ‘šâ€² [ğ‘–]] = ğ‘¥E
-
ğ‘Œâ‡‘1
ğ‘šâ€² [ğ‘–+ 1] â‡‘ğ‘Œâ‡‘1
ğ‘šâ€² [ğ‘–]
.
,
if ğ‘’ğ‘šâ€² [ğ‘–] âˆğ‘.
From the upper bound on the expected duration of the cycle in (26), we have,
E [ğ‘Šğ‘šâ€² [ğ‘–]] â‡”ğ‘¥
ğ‘”
*
ğ‘Â¯ğ‘‘ğ‘+ (ğ‘â‡’
col â‡‘1)ğ‘‘max+
,
if ğ‘’ğ‘šâ€² [ğ‘–] âˆğ‘.
Then the drift for all the states ğ›¯with ğ›¯âˆğ‘is,
E
-
ğ‘’ğ‘šâ€² [ğ‘–+ 1] â‡‘ğ‘’ğ‘šâ€² [ğ‘–]
ï£³ï£³ğ‘’ğ‘šâ€² [ğ‘–] = ğ›¯
.
= ğ‘
(ğ‘¥
ğ‘”
Â¯ğ‘‘ğ‘â‡‘1
)
+ ğ‘¥(ğ‘â‡’
col â‡‘1)ğ‘‘max,
(ğ‘)
â‡”â‡‘ğ‘ğ›¥+ (ğ‘â‡’
col â‡‘1)ğ‘¥ğ‘‘max,
(ğ‘)< 0,
âˆ‹ğ‘–â†‘N.
(a) follows by the premise, ğ‘¥Â¯ğ‘‘ğ‘â‡”ğ‘”(1 â‡‘ğ›¥), and (b) follows from the choice of ğ‘in Theorem 2.
And clearly E
-
ğ‘’ğ‘šâ€² [ğ‘–+ 1]
ï£³ï£³ğ‘’ğ‘šâ€² [ğ‘–] = ğ›¯
.
< â†™for any ğ›¯â†‘N.
Therefore, by Fosterâ€™s Theorem[6], (ğ‘’ğ‘šâ€² [ğ‘–])â†™
ğ‘Ÿ=1 is positive recurrent.
â«…Ì¸
C.4
Proof of Theorem 2
First we show that the expected cycle times are "nite.
L)##. 4. Under the same conditions as Theorem 2, for any cycle index ğ‘–â†‘N:
E
-
ğ‘Œâ‡‘1
ğ‘šâ€² [ğ‘–+ 1] â‡‘ğ‘Œâ‡‘1
ğ‘šâ€² [ğ‘–]
.
â‡”ğ‘”
ğ‘¥+ ğ‘ğ‘‘max.
P+â€™â€™*. Consider ğ‘’ğ‘šâ€² [ğ‘–] > 0. Then, the RAD scheduler completes at most ğ‘requests in this cycle.
The worst case time that a scheduler may take to complete ğ‘requests is when, a) the prompt and
output lengths of all requests are the longest possible, and, b) the scheduler processes the requests
one after the other. ğ‘‘max bounds the worst-case time required to complete a request. Therefore, the
worst-case cycle time of the RAD scheduler is bounded by ğ‘ğ‘‘max.
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 35 ---
Optimal Scheduling for LLM Inference
59:35
Consider ğ‘’ğ‘šâ€² [ğ‘–] = 0. In this case, the RAD scheduler waits for a request to arrive, which takes
expected time ğ‘”/ğ‘¥. Then, similar to above, the RAD scheduler may complete up to ğ‘requests (in
the case where more requests arrive before RAD proceeds to the next cycle). We know that that
worst-case cycle time to complete ğ‘requests is ğ‘ğ‘‘max. Therefore, in this case, the expected cycle
time is bounded as ğ‘”/ğ‘¥+ ğ‘ğ‘‘max.
â«…Ì¸
Finally, we using the previous Lemmas proved in this section, we present the proof of Theorem
2.
P+â€™â€™* â€™* T2)â€™+)# 2. By de"nition, ğ‘‡ğ‘šâ€² ï£¯ğ‘Œâ‡‘1[ğ‘–]ï£º= 0, if and only if ğ‘’ğ‘šâ€² [ğ‘–] = 0. And, by Lemma
3, (ğ‘’ğ‘šâ€² [ğ‘–])â†™
ğ‘Ÿ=1 is positive-recurrent. And, by Lemma 4, for any ğ‘–1,ğ‘–2 â†‘N such that ğ‘–1 < ğ‘–2,
E
-
ğ‘Œâ‡‘1
ğ‘šâ€² [ğ‘–2] â‡‘ğ‘Œâ‡‘1
ğ‘šâ€² [ğ‘–1]
.
â‡”(ğ‘–2 â‡‘ğ‘–1)ğ‘”
ğ‘¥
+ (ğ‘–2 â‡‘ğ‘–1)ğ‘ğ‘‘max.
We know the systems starts empty: ğ‘‡ğ‘šâ€² (0) = 0. De"ne, ğ‘‘regen = min{ğ‘> 0 : ğ‘‡ğ‘šâ€² (ğ‘) = 0}. Then
by the above results, E [ğ‘‘regen] < â†™. Moreover, RAD starts a new cycle whenever ğ‘‡ğ‘šâ€² (ğ‘) = 0.
Therefore, ğ‘‘regen is aligned with the start of a new cycle: ğ‘‘regen = ğ‘Œâ‡‘1[ğ‘Œ(ğ‘‘regen)]. Since (ğ‘’ğ‘šâ€² [ğ‘–])â†™
ğ‘Ÿ=0
is a DTMC, this implies that (ğ‘‡ğ‘šâ€² (ğ‘))ğ‘‘>ğ‘˜regen is independent of (ğ‘‡ğ‘šâ€² (ğ‘))ğ‘‘â‡”ğ‘˜regen. Therefore, (ğ‘‡ğ‘šâ€² (ğ‘))ğ‘‘âˆ0
is a regenerative process, with regeneration point 0. Further it is positive recurrent since E [ğ‘‘regen] <
â†™.
â«…Ì¸
D
Another Throughput-Optimal Scheduler
Another throughput-optimal scheduler is presented in Algorithm 2. It has the same high level
design principle as the RAD scheduler. It proceeds in cycles, where in each cycle it starts and
completes up to ğ‘requests. This scheduler di#ers from RAD in the order in which it schedules
pre"ll and decode iterations.
Algorithm 2 "rst completes up to ğ‘pre"ll-phase requests (if enough are available) with optimal
tiling (see Lines 3-13). Then it moves to the decode mode, where it "rst chooses the "rst ğ‘â‡’
col of the
decode phase requests, D, and marks them as active, denoted Dactive. It then proceeds to schedules
batches of decode-iterations of each request in Dactive. After each request it removes requests that
sampled their stop token, and replaces them with remaining requests in D, if there are any. This
proceeds until all the requests in this cycle have completed.
It may be shown that Algorithm 2 schedules optimally tiled batches most of the time, and also
performs dynamic optimal resource allocation between pre"ll and decode phase requests just
like the RAD scheduler. As such, Theorem 2 applies when the RAD scheduler is replaced by the
scheduler in Algorithm 2 as well. The proof would be very similar to the one presented in Appendix
C for the RAD scheduler. We omit the proof to avoid repetition.
The primary di#erence in the operation of the two schedulers would be the TTFT and TBT
observed by requests. For the scheduler in Algorithm 2, consider that the "rst output token of a
request is only produced when it has been moved to the active decode set, Dactive. In this case, the
decode-iterations of this request keep getting scheduled in consecutive batches until the request is
complete. Therefore, the TBT of requests will be low. However, since the scheduler has to complete
a large number, ğ‘, of requests before it starts generating the tokens for any of the requests, the TTFT
of the requests will be very high. This is complementary to the RAD scheduler, where requests
observe a hight TBT, but a relatively lower TTFT.
E
Other Schedulers in the Framework
As a way of illustration, we provide a description of request-level batching, Sarathi-serve and
DistServe in our scheduler framework introduced in Section 3.1.
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 36 ---
59:36
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
Algorithm 2 Another Throughput Optimal Scheduler
Require: Parameter ğ‘
1: P â†˜â‡“, D â†˜â‡“
2: while True do
// Cycle starts in PREFILL MODE
3:
ğ‘”ğ‘ ğ‘¡ğ‘¢ğ‘ ğ‘‰ğ‘ğ‘‰_ğ‘„ğ‘_ğ‘–ğ‘£ğ‘–ğ‘ˆğ‘ â†˜0
4:
while P Ï‰ â‡“and ğ‘”ğ‘ ğ‘¡ğ‘¢ğ‘ ğ‘‰ğ‘ğ‘‰_ğ‘„ğ‘_ğ‘–ğ‘£ğ‘–ğ‘ˆğ‘ < ğ‘do
5:
Choose a request ğ‘•"rst-come-"rst-serve from P and set ğ‘„â†˜1
6:
while pre"ll phase of the chosen request is incomplete do
7:
ğ‘–= min{LCM(ğ‘â‡’
row,ğ‘â‡’
col,ğ‘â‡’
red), ğ‘¤ğ‘‰(ğ‘•) â‡‘ğ‘„}
8:
Schedule ğ‘—ğ‘˜(ğ‘•,ğ‘„,ğ‘–) in a batch.
9:
ğ‘„â†˜ğ‘„+ ğ‘–
10:
end while
11:
Move the request from P to D
12:
ğ‘”ğ‘ ğ‘¡ğ‘¢ğ‘ ğ‘‰ğ‘ğ‘‰_ğ‘„ğ‘_ğ‘–ğ‘£ğ‘–ğ‘ˆğ‘ â†˜ğ‘”ğ‘ ğ‘¡ğ‘¢ğ‘ ğ‘‰ğ‘ğ‘‰_ğ‘„ğ‘_ğ‘–ğ‘£ğ‘–ğ‘ˆğ‘ + 1
13:
end while
//Change mode to DECODE MODE
14:
Dactive = â‡“
15:
Move min(ğ‘â‡’
col, |D|) requests from D to Dactive
16:
while |Dactive| > 0 do
17:
Schedule one DI for each request in Dactive in a batch.
18:
Remove requests that sampled the !stop! token from Dactive
19:
Move min(ğ‘â‡’
col â‡‘|Dactive| , |D|) requests from D to Dactive.
20:
end while
21: end while
E.1
Request level batching
This scheduler is inspired by FasterTransformer[23], which schedules a batch of ğ‘›requests at a
time, and only proceeds to other requests after all these requests have completed. We adapt this
scheduler and express it in the â€œsequences of batches of iterationsâ€ framework in Algorithm 3. Here
the scheduler alternates between two modes. It starts in Pre"ll Mode in which it selects ğ‘›requests
from the pre"ll-queue in FCFS order and completes their pre"ll phases. It then switches to Decode
Mode where it schedules one decode iteration for all active decode-phase requests until all of them
complete. It then switches back to Pre"ll Mode.
E.2
Sarathi-serve
This scheduler, as shown in Algorithm 4, enforces a "xed token budget per batch and prioritizes
decode-phase requests. It "rsts selects a decode iteration for each decode-phase request (the number
of these is guaranteed to be below the budget). It then "lls the remaining capacity with chunked
pre"ll iterations. Completed pre"lls are transitioned to the decode queue, while completed decodes
are removed.
E.3
DistServe
This is a distributed scheduler, as shown in Algorithm 5, and runs on dedicated pre"ll and decode
nodes. In pre"ll nodes, pre"ll requests are handled one at a time in FCFS order, with entire prompts
processed in a single iteration. Completed requests are transferred to the decode node. At the
decode node, it continuously batches and schedules decode iterations. Here we note that it is
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 37 ---
Optimal Scheduling for LLM Inference
59:37
Algorithm 3 Request-Level Batching Scheduler [23]
Require: Batch size ğ‘›
1: P â†˜â‡“, D â†˜â‡“
// Prefill and decode phase queues
2: ğ‘šğ›¶ğ‘‚ğ‘ â†˜Decode
3: while True do
4:
Update P based on arrivals
5:
if ğ‘šğ›¶ğ‘‚ğ‘ = Prefill then
6:
if D Ï‰ â‡“then
7:
Schedule ğ‘™ğ‘˜for each ğ‘•â†‘D in a batch with appropriate token positions
8:
Remove from D any request that sampled the stop token
9:
else
10:
ğ‘šğ›¶ğ‘‚ğ‘ â†˜Prefill
11:
end if
12:
else if ğ‘šğ›¶ğ‘‚ğ‘ = Prefill then
13:
if P Ï‰ â‡“then
14:
Choose up to ğ‘›requests from P in FCFS order
15:
for all selected requests ğ‘•ğ‘do
16:
Schedule ğ‘—ğ‘˜(ğ‘•, 1, ğ‘¤ğ‘‰
ğ‘)
// Full prompt in one iteration
17:
Move ğ‘•from P to D
18:
end for
19:
end if
20:
ğ‘šğ›¶ğ‘‚ğ‘ â†˜Decode
21:
end if
22: end while
possible to run both pre"ll and decode requests with optimal tiling at the pre"ll and decode nodes
respectively.
F
Additional experimental results
F.1
Results for the scenario of 5% split
Figure 9 presents a comparative evaluation of scheduling policies under heterogeneous TTFT and
TBT constraints, with a workload comprising 5% paying users. Figure 9a shows the median TTFT
for all requests as a function of request rate, plotted on a log-scaled y-axis to highlight di#erences
at low load. This view reveals how various schedulers handle contention-free versus saturated
conditions. Figure 9b reports the number of requests completed at the peak load of 1.6 requests
per second, bucketed by prompt length. Notably, SLAI (SPF, dynamic o#set) serves nearly as many
requests as Sarathi-serve while achieving substantially lower median TTFT, whereas vLLM exhibits
instability and fails to maintain throughput under high load. Finally, Figure 9c plots the mean
TTFT at 1.6 requests per second as a function of prompt length. Despite favoring shorter prompts,
SPF-based schedulers yield a lower overall TTFT compared to FCFS variants, demonstrating the
bene"t of prioritizing short requests even in the presence of heterogeneous job sizes.
F.2
Prioritizing prefill-phase requests of paying users over free-tier users
In this section, we evaluate an additional policy: SLAI (SPF with priority, dynamic o#set). This
policy gives strict priority to pre"ll-phase requests from paying users over those from free-tier
users, regardless of prompt length. In other words, it always schedules a paying userâ€™s request
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 38 ---
59:38
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
Algorithm 4 Sarathi-Serve Scheduler[2]
Require: Token budget ğ‘œbudget
// Max tokens per batch
1: P â†˜â‡“, D â†˜â‡“
// Requests in prefill and decode phases
2: while True do
3:
Update P based on arrivals
4:
B â†˜â‡“
// Current batch and used tokens
5:
for all requests ğ‘•â†‘D do
6:
Add ğ‘™ğ‘˜(ğ‘•,ğ‘„) to batch B
7:
Mark ğ‘„â†˜ğ‘„+ 1 for ğ‘•
8:
end for
9:
ğ‘œâ†˜|D|
10:
for all ğ‘•â†‘P in arrival order do
11:
while ğ‘œ< ğ‘œbudget do
12:
ğ‘–â†˜min{ğ‘œbudget â‡‘ğ‘œ, ğ‘¤ğ‘‰
ğ‘â‡‘ğ‘„}
// i is starting token index left to prefill
13:
Add ğ‘—ğ‘˜(ğ‘•,ğ‘„,ğ‘–) to batch B
14:
ğ‘œâ†˜ğ‘œ+ ğ‘–
15:
Mark ğ‘„â†˜ğ‘„+ ğ‘–for ğ‘•
16:
end while
17:
end for
18:
Schedule batch B
19:
for all requests ğ‘•â†‘P do
20:
if pre"ll of ğ‘•complete then
21:
Move ğ‘•from P to D
22:
end if
23:
end for
24:
for all requests ğ‘•â†‘D do
25:
if decode of ğ‘•sampled stop token then
26:
Remove ğ‘•from D
27:
end if
28:
end for
29: end while
"rst. All other parameters are the same as in SLAI (SPF, dynamic o#set). Figure 10 compares this
priority-based policy with other scheduling strategies. At high load (1.6 requests per second), we
observe that the mean TTFT for paying users is lower than that for free-tier users. However, the
improvement in TTFT for paying users is relatively small.
F.3
Results for the scenario of 50% and 95% split
In this section, we present additional results for scenarios with less heterogeneity in user workloads.
Figures 11 and 12 show results similar to those discussed earlier, but for cases where the percentage
of paying users is 50% and 95%, respectively. As the proportion of paying users increases, the
improvement in serving capacity under SLAI (SPF, dynamic o#set) compared to Sarathi-serve
(FCFS) becomes smaller. This is because a larger share of tra!c now has stricter TBT constraints,
leaving fewer opportunities for SLAI to defer decode-phase requests dynamically.
Received July 2025; revised September 2025; accepted October 2025
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 39 ---
Optimal Scheduling for LLM Inference
59:39
Algorithm 5 DistServe Scheduler [36]
Require: Separate compute nodes for pre"ll and decode phases
â€” Pre!ll Node â€”
1: P â†˜â‡“
// Requests awaiting prefill
2: while True do
3:
Update P based on new arrivals
4:
if P = â‡“then
5:
continue
6:
end if
7:
Choose request ğ‘•ğ‘from P in "rst-come-"rst-serve order
8:
Schedule ğ‘—ğ‘˜(ğ‘•ğ‘, 1, ğ‘¤ğ‘‰
ğ‘)
// chunked prefill-iteration may be done too
9:
Transfer KV-cache of ğ‘•to a Decode Node
10:
Remove ğ‘•from P
11: end while
â€” Decode Node â€”
12: D â†˜â‡“
// Requests in decode phase
13: while True do
14:
Update D received from a Pre"ll Node
15:
if D = â‡“then
16:
continue
17:
end if
18:
Schedule ğ‘™ğ‘˜(ğ‘•) for each ğ‘•â†‘D in a batch
// ğ‘â‡’
col requests may also be scheduled
for optimal tiling
19:
Remove from D any request that sampled the stop token
20: end while
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 40 ---
59:40
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
(a) Median TTFT for all users as a function of request
rate, shown on a log-scaled ğ‘£-axis to illustrate the gap
at lower request rates.
(b) Requests served at high load (1.6 req/s) versus
prompt length.
(c) Mean TTFT at the high load (1.6 req/s) versus prompt
length.
Fig. 9. Performance comparison of di"erent policies under mixed user workloads with 5% paying users.
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 41 ---
Optimal Scheduling for LLM Inference
59:41
(a) Mean TTFT for paying users as a
function of requests per second.
(b) Mean TTFT for free-tier users as a
function of requests per second.
Fig. 10. Performance comparison of di"erent policies under mixed user workloads with 5% paying users.
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 42 ---
59:42
Agrim Bari, Parikshit Hegde, and Gustavo de Veciana
(a) 99th percentile TBT for paying users across
di"erent request rates for a target of 0.1 seconds.
(b) 99th percentile TBT for free-tier users across
di"erent request rates for a target of 0.5 seconds.
(c) Median TTFT for all users as a function of request
rate. SLAI (SPF, dynamic o"set) reduces TTFT from 1.5
seconds (under Sarathi-serve (FCFS)) to 0.73 seconds,
and increases peak serving capacity from 1.15 to 1.4
requests per second subject to latency constraints.
Fig. 11. Performance comparison of SLAI, Sarathi-serve, and vLLM under mixed user workloads with 50%
paying users. SLAI (SPF, dynamic o"set) achieves the best latency-throughput trade-o".
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.


--- Page 43 ---
Optimal Scheduling for LLM Inference
59:43
(a) 99th percentile TBT for paying users across
di"erent request rates for a target of 0.1 seconds.
(b) 99th percentile TBT for free-tier users across
di"erent request rates for a target of 0.5 seconds.
(c) Median TTFT for all users as a function of request
rate. SLAI (SPF, dynamic o"set) reduces TTFT from 2
seconds (under Sarathi-serve (FCFS)) to 0.75 seconds,
and increases peak serving capacity from 1.15 to 1.25
requests per second subject to latency constraints.
Fig. 12. Performance comparison of SLAI, Sarathi-serve, and vLLM under mixed user workloads with 95%
paying users. SLAI (SPF, dynamic o"set) achieves the best latency-throughput trade-o".
Proc. ACM Meas. Anal. Comput. Syst., Vol. 9, No. 3, Article 59. Publication date: December 2025.
