--- Page 1 ---
Investigating the Potential of Using Large Language Models for
Scheduling
Deddy Jobson
deddy@mercari.com
Mercari
Tokyo, Japan
Li Yilin
y-li@mercari.com
Mercari
Tokyo, Japan
ABSTRACT
The inaugural ACM International Conference on AI-powered Soft-
ware introduced the AIware Challenge, prompting researchers to ex-
plore AI-driven tools for optimizing conference programs through
constrained optimization. We investigate the use of Large Language
Models (LLMs) for program scheduling, focusing on zero-shot learn-
ing and integer programming to measure paper similarity. Our study
reveals that LLMs, even under zero-shot settings, create reasonably
good first drafts of conference schedules. When clustering papers,
using only titles as LLM inputs produces results closer to human
categorization than using titles and abstracts with TFIDF. The code
has been made publicly available1.
CCS CONCEPTS
â€¢ Applied computing â†’Multi-criterion optimization and
decision-making; â€¢ Information systems â†’Data mining.
KEYWORDS
large language models, scheduling, mathematical optimization, clus-
tering
ACM Reference Format:
Deddy Jobson and Li Yilin. 2024. Investigating the Potential of Using Large
Language Models for Scheduling. In Proceedings of the 1st ACM International
Conference on AI-Powered Software (AIware â€™24), July 15â€“16, 2024, Porto de
Galinhas, Brazil. ACM, New York, NY, USA, 3 pages. https://doi.org/10.1145/
3664646.3665084
1
INTRODUCTION
The inaugural ACM International Conference on AI-powered Soft-
ware debuts the AIware Challenge, encouraging researchers and
practitioners to implement AI-driven tools to create an optimized
conference program, a fundamental issue in constraint optimiza-
tion. Constrained optimization is a prevalent mathematical task,
needing intensive computational resources and tailored algorithms
to address the unique complexities of the decision space. Applying
1https://github.com/deddyjobson/llms-for-scheduling
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
AIware â€™24, July 15â€“16, 2024, Porto de Galinhas, Brazil
Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0685-1/24/07
https://doi.org/10.1145/3664646.3665084
Large Language Models (LLMs) has demonstrated significant suc-
cess in various domains such as mathematical problem-solving and
reasoning.
This report investigates the feasibility of leveraging LLMs to ad-
dress the program scheduling challenge. Our investigation revolves
around two primary approaches: 1) zero-shot learning to generate
the schedules and 2) integer programming with LLMs to measure
the similarity between papers.
2
PROBLEM STATEMENT
Our paper focuses on automating the allocation of papers to pre-
determined sessions, treating it as an allocation and constrained
clustering problem. We do not factor in the specific timing of paper
presentations or the impact of parallel tracks, earmarking these
as potential areas for future investigation on the broader goal of
automating conference scheduling.
3
EXPERIMENTS
3.1
Zero-Shot Scheduling by LLMs
Our first AIware approach directly prompts a large language model
to make the schedule. While LLMs have a lot of room to improve on
arithmetic reasoning tasks, recent approaches have been proposed
to improve their abilities for math problems[1, 4, 5]. Furthermore,
LLMs are expected to improve at solving LLMs at zero-shot config-
urations, too, due to the scaling laws[3].
However, from our experiments, we find the LLMs are still unable
to create schedules without violating constraints. We, therefore,
additionally experiment with smaller-scale problems by downsam-
pling the number of sessions or papers within each session. To
measure the performance of our approach, we measure the follow-
ing metrics:
â€¢ Completeness and Homogeneity wrt the ground truth[2]
â€¢ % of constraints broken, for various types of constraints.
We use a temperature setting of 0.8 in our experiments.
We find that GPT-4 in zero-shot settings can generate reason-
ably good conference schedules. We also counted the number of
violations according to the schedule created by the GPT -4. Looking
at violations on the number of sessions and papers in the proposed
schedule, we find that, on average, less than 3 papers were missing,
and two sessions were added. Furthermore, checking if the session
durations are within bounds, we find that while many sessions
proposed were too long (in some cases, more than 50%), in most
cases, the proposed session was off by less than 10% of the length of
the session. This suggests that by moving around a couple of papers
between sessions, a human can finish the work mostly done by an
LLM, suggesting a collaboration with LLMs be the best approach.
arXiv:2406.07573v1  [cs.AI]  4 Jun 2024


--- Page 2 ---
AIware â€™24, July 15â€“16, 2024, Porto de Galinhas, Brazil
Jobson and Li
Figure 1: Comparing the homogeneity and completeness
score for different number of papers per session. Homogene-
ity deteriorates when the scale of the data is larger, but not
completeness.
Computing the homogeneity and completeness scores with re-
spect to the actual schedule of MSR 2022 (Figure 1), we find that
while homogeneity drops with an increase in problem size, com-
pleteness is not as much affected.
3.2
Integer Program using LLM to measure
similarity
In Section 3.1, we discover that LLMs face challenges in strict ad-
herence to all constraints, particularly when dealing with many
papers. We plan to explore how to leverage the text-understanding
capabilities of LLMs to facilitate the problem.
We incorporate the similarity of papers as an objective in our
optimization process to build an integer programming problem and
use LLM to generate the similarity. Recognizing that similarity is a
subjective concept, we have utilized the existing session arrange-
ment as the benchmark for comparison. To assess the proximity of
our results to the original schedule, we employ the completeness
score and homogeneity score metrics [2].
Leveraging LLMsâ€™ text-understanding capabilities, we cluster
the papers into 5 groups and contrast the outcomes with a Bag of
Words approach featuring TFIDF normalization. The comparison,
outlined in Table 1, demonstrates the superior performance of LLM
when utilizing only the paper titles compared to TFIDF with both
titles and abstracts.
To further evaluate the clustering outcomes, we formulate an in-
teger programming problem (formula see Appendix) using the best
results from LLM with text alone and TFIDF with text and abstract
as the similarity parameters. The similarity metric indicates that
papers ğ‘–and ğ‘—sharing the same cluster exhibit a similarity value of
1, and 0 otherwise. completeness score and homogeneity score are
outlined in Table 2. Notably, the comprehensive and homogeneity
scores align LLMâ€™s clustering result closely with TFIDF. This align-
ment reflects a common scenario in optimization problems where
the parameters are derived from predicted outcomes.
4
CONCLUSION
Our experimentation shows that LLMs are a promising tool in
managing optimization problems even when they lack precision
Approaches
Completeness
Score
Homogeneity
Score
TFIDF (title only)
0.33
0.14
TFIDF (title and
abstract)
0.37
0.20
LLM (title only)
0.41
0.22
Table 1: clustering results applying LLM and TFIDF. This
is the average results from 5 random trials. LLM with title
only exhibits the highest performance in completeness score
and homogeneity score when considering the current session
label as the ground truth.
Approaches
Completeness
Score
Homogeneity
Score
TFIDF (title and
abstract)
0.42
0.42
LLM (title only)
0.41
0.41
Table 2: The results of integer programming optimization
using similarity measures from LLM and TFIDF. Due to the
inferior performance of TFIDF with title-only, it is omitted
from this comparison. The results show that the LLM has a
similar result to the TFIDF(title and abstract).
when the number of decision variables exceeds a certain threshold.
To address this limitation, a potential strategy involves involving
humans in the loop, or combining the strengths of LLMs and numer-
ical solvers as we demonstrated through our clustering + Integer
Programming approach.
5
ACKNOWLEDGEMENTS
We would like to thank Mercari Inc. for supporting this research.
LLMs were used to reword sections and to reformat tables in the
paper. However, we did not generate entire sections through LLMs.
REFERENCES
[1] Zachary Levonian, Chenglu Li, Wangda Zhu, Anoushka Gade, Owen Henkel,
Millie-Ellen Postle, and Wanli Xing. 2023. Retrieval-augmented Generation to Im-
prove Math Question-Answering: Trade-offs Between Groundedness and Human
Preference. https://doi.org/10.48550/arXiv.2310.03184 arXiv:2310.03184 [cs].
[2] Andrew Rosenberg and Julia Hirschberg. 2007. V-measure: A conditional entropy-
based external cluster evaluation measure. In Proceedings of the 2007 joint conference
on empirical methods in natural language processing and computational natural
language learning (EMNLP-CoNLL). 410â€“420.
[3] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud,
Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tat-
sunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022.
Emergent Abilities of Large Language Models. http://arxiv.org/abs/2206.07682
arXiv:2206.07682 [cs].
[4] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou,
and Xinyun Chen. 2023. Large Language Models as Optimizers. https://doi.org/
10.48550/arXiv.2309.03409 arXiv:2309.03409 [cs].
[5] Tianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra, Yiming
Yang, Niket Tandon, and Uri Alon. 2024. In-Context Principle Learning from
Mistakes. https://doi.org/10.48550/arXiv.2402.05403 arXiv:2402.05403 [cs].


--- Page 3 ---
Investigating the Potential of Using Large Language Models for Scheduling
AIware â€™24, July 15â€“16, 2024, Porto de Galinhas, Brazil
A
PROMPT DESIGN
The following is the template of the prompt we used. Note that the
terms surrounded by curly braces ({}) show where the list of papers
was added.
Program creation is the process of taking all the
accepted papers to a conference and allocating a
presentation slot for each paper with parallel sessions.
The PC chairs of a conference typically do this manually.
Assign sessions to the following papers based on
the following constraints:
1. The total length of all paper presentations
within a session should be less than or equal to
the length of the session they are in.
2. No new sessions should be added.
3. The "Discussions and Q/A" event should only be
at the end of a session if present.
4. All papers must be assigned to some session.
The output should contain the schedule in the form
of the csv representation of a data frame. This
csv representation should be in three quotes (```)
on both sides so that I can easily extract it
from your result and make a data frame.
Example output format(use as many rows as the
actual number of papers):
```
session@talk_title@duration
231@An Empirical Study on Maintainable Method ...@7
223@Improve Quality of Cloud Serverless ...@7
15@Extracting corrective actions from code ...@7
15@How to Improve Deep Learning for Software ...@7
11@ReCover: a Curated Dataset for Regression...@4
```
The list of paper and session titles are below:
Session Lengths:
{sessions_df_string}
Paper durations:
{papers_df_string}
Make sure, above all else, that your response
is formatted as requested, with the proper headers.
Some points to note:
â€¢ On top of giving the list of papers, we gave a list of sessions
with session length.
â€¢ We shuffled the list of papers so the LLM would not copy the
original ordering.
â€¢ We opted for values separated by the commercial at sym-
bol (@) rather than commas to make handling titles with
commas easier.
â€¢ The final statement was added to increase the chances of
getting the output in the right format.
B
INTEGER PROGRAMMING FORMULATION
Define binary decision variables ğ‘¥ğ‘–,ğ‘—:
ğ‘¥ğ‘–,ğ‘—=
(
1
if paper ğ‘–is scheduled in session ğ‘—,
0
otherwise.
We introduce a new variable to indicate whether two papers are
scheduled for the same session. Define binary decision variables
ğ‘§ğ‘–,ğ‘—,ğ‘š:
ğ‘§ğ‘–,ğ‘—,ğ‘š=
(
1
if papers ğ‘–and ğ‘—are in the same session ğ‘š,
0
otherwise.
B.1
Objective Function
Maximize the similarity between papers scheduled in the same
session (if the paper is clustered into the same cluster, the similarity
is 1 otherwise it is 0):
Maximize
ğ‘
âˆ‘ï¸
ğ‘–=1
ğ‘
âˆ‘ï¸
ğ‘—=1
ğ‘€
âˆ‘ï¸
ğ‘š=1
simlarity(ğ‘–, ğ‘—) Â· ğ‘§ğ‘–,ğ‘—,ğ‘š
B.2
Constraints
B.2.1
Linkğ‘§andğ‘¥variables. For each pair of papers both scheduled
in the same session:
ğ‘§ğ‘–,ğ‘—,ğ‘šâ‰¥ğ‘¥ğ‘–,ğ‘š+ ğ‘¥ğ‘—,ğ‘šâˆ’1
ğ‘§ğ‘–,ğ‘—,ğ‘šâ‰¤ğ‘¥ğ‘–,ğ‘š
ğ‘§ğ‘–,ğ‘—,ğ‘šâ‰¤ğ‘¥ğ‘—,ğ‘š
B.2.2
Each paper scheduled exactly once. Ensure every paper is
scheduled in exactly one session:
ğ‘€
âˆ‘ï¸
ğ‘—=1
ğ‘¥ğ‘–,ğ‘—= 1
âˆ€ğ‘–
B.2.3
Session length constraints. Ensure the total duration of pa-
pers in any session does not exceed the available time:
ğ‘
âˆ‘ï¸
ğ‘–=1
ğ‘¥ğ‘–,ğ‘—Â· papers_durationğ‘–â‰¤session_lengthsğ‘—
âˆ€ğ‘—
Received 2024-04-24; accepted 2024-05-10; revised 20 February 2007; revised
12 March 2009; accepted 5 June 2009
