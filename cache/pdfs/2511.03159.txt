--- Page 1 ---
1
Joint Optimization of DNN Model Caching and
Request Routing in Mobile Edge Computing
Shuting Qiu, Fang Dong, Member, IEEE, Siyu Tan, Ruiting Zhou, Member, IEEE, Dian Shen, Member, IEEE,
Patrick P. C. Lee, Senior Member, IEEE and Qilin Fan, Member, IEEE,
Abstractâ€” Mobile edge computing (MEC) can pre-cache deep
neural networks (DNNs) near end-users, providing low-latency
services and improving usersâ€™ quality of experience (QoE).
However, caching all DNN models at edge servers with limited
capacity is difficult, and the impact of model loading time on
QoE remains underexplored. Hence, we introduce dynamic DNNs
in edge scenarios, disassembling a complete DNN model into
interrelated submodels for more fine-grained and flexible model
caching and request routing solutions. This raises the pressing
issue of jointly deciding request routing and submodel caching for
dynamic DNNs to balance model inference precision and loading
latency for QoE optimization. In this paper, we study the joint
dynamic model caching and request routing problem in MEC
networks, aiming to maximize user request inference precision
under constraints of server resources, latency, and model loading
time. To tackle this problem, we propose CoCaR, an offline
algorithm based on linear programming and random rounding
that leverages dynamic DNNs to optimize caching and routing
schemes, achieving near-optimal performance. Furthermore, we
develop an online variant of CoCaR, named CoCaR-OL, enabling
effective adaptation to dynamic and unpredictable online request
patterns. The simulation results demonstrate that the proposed
CoCaR improves the average inference precision of user requests
by 46% compared to state-of-the-art baselines. In addition, in
online scenarios, CoCaR-OL achieves an improvement of no less
than 32.3% in user QoE over competitive baselines.
Index Termsâ€”Mobile edge computing, dynamic DNN, model
caching, request routing, joint optimization
I. INTRODUCTION
R
ECENTLY, machine learning methods, especially deep
neural networks (DNNs), have been transforming various
application domains [2]â€“[4], such as computer vision [5],
This work is supported by National Natural Science Foundation of China
under Grants, No. 62232004, Jiangsu Provincial Frontier Technology Re-
search and Development Program under Grant BF2024070, Shenzhen Science
and Technology Program under Grant KJZD20240903100814018, Jiangsu
Provincial Key Laboratory of Network and Information Security under Grants
No.BM2003201, Key Laboratory of Computer Network and Information
Integration of Ministry of Education of China under Grants No.93K-9, and
partially supported by Collaborative Innovation Center of Novel Software
Technology and Industrialization. (Corresponding authors: Fang Dong.)
This paper is an extended version of our preliminary work presented at
IEEE INFOCOM 2025 [DOI: 10.1109/INFOCOM55648.2025.11044457].
Shuting Qiu, Fang Dong, Siyu Tan, Dian Shen and Ruiting Zhou are
with the School of Computer Science and Engineering, Southeast University,
Nanjing 211189, China (email: qiushuting@seu.edu.cn; fdong@seu.edu.cn;
sytan@seu.edu.cn; dshen@seu.edu.cn; ruitingzhou@seu.edu.cn).
Patrick P. C. Lee is with the Department of Computer Science and
Engineering, The Chinese University of Hong Kong, Hong Kong (email:
pclee@cse.cuhk.edu.hk).
Qilin
Fan
is
with
the
School
of
Big
Data
and
Software
Engi-
neering, Chongqing University, Chongqing 400044, China (email: fan-
qilin@cqu.edu.cn).
speech recognition [6], and autonomous driving [7]. To pro-
mote efficient task processing, mobile edge computing (MEC)
[8]â€“[11], has emerged as a promising paradigm to provide end-
users with low-latency computation, caching, and transmission
capabilities [12]â€“[14]. In MEC, DNN models can be pre-
cached at base stations (BSs) with edge servers, enabling
rapid responses to user requests [15], [16]. However, unlike
cloud servers, due to the limited resources of a BS, only a
few popular DNN models can be cached simultaneously to
service some users [17], [18]. Additionally, request routing
is also a key factor affecting usersâ€™ quality of experience
(QoE) in multi-edge collaboration scenarios [19]â€“[21]. There-
fore, routing requests to BSs with relevant cached models is
essential for maximizing user coverage and system efficiency.
Obviously, the model caching mechanism is tightly coupled
with the request routing scheme. Thus, jointly optimizing
model caching and request routing decisions is crucial for
improving usersâ€™ QoE in MEC.
Existing joint optimization schemes for model caching and
request routing mainly focus on caching complete models at
resource-limited BSs [22]â€“[26], leading to inefficient resource
utilization and poor user QoE [27]. Fortunately, dynamic
DNNs [28], [29] provide a promising solution by enabling
flexible model structure adjustments. In the context of this
paper, we define dynamic DNNs as a model versioning frame-
work, where a base model is partitioned into multiple submod-
els of varying depth and precision for fine-grained caching.
Specifically, we disassemble a complete DNN model into a
set of interrelated submodels by dividing the feature extraction
layer, allowing for switches between them by adjusting the
number of layers. This approach enables fine-grained resource
allocation in a multi-server caching environment, thereby
distinguishing our method from device-edge split computing
[30], [31], which partitions a single inference task across two
locations for collaborative execution. Consequently, our use of
dynamic DNNs allows for more flexible model caching and
request routing solutions, utilizing BS resources in a more
fine-grained manner.
However, incorporating dynamic DNNs complicates the
joint optimization decisions, leading to two primary chal-
lenges: 1) Caching schemes for submodels of the same
dynamic DNN at a BS are mutually exclusive, permitting
only one submodel to be cached at a time. This necessitates
determining not only which DNN models to cache but also
which submodel of these dynamic DNNs to cache, thereby
increasing the complexity of the variable-coupled joint opti-
mization problem. 2) Inference requests for different DNN
arXiv:2511.03159v1  [cs.NI]  5 Nov 2025


--- Page 2 ---
2
ğ‡ğ¢ğğğğ­ğŸ
Transformer
Encoder 
Ã—L(2)
ğ‡ğ¢ğğğğ­ğŸ
Transformer
Encoder 
Ã—L(6)
EntNet
Input
Submodel 2 of ViT
2 to 1 
ğ‡ğ¢ğğğğ­ğŸ
Transformer
Encoder 
Ã—L(6)
EntNet
Submodel 1 of ViT
ğ„ğ±ğ­ğğğ­ğŸ
1 to 2 
3 to 2 
Submodel 3 of ViT
3 to 1 
1 to 3 
2 to 3 
ğ‡ğ¢ğğğğ­ğŸ
Transformer
Encoder 
Ã—L(2)
ğ‡ğ¢ğğğğ­ğŸ
Transformer
Encoder 
Ã—L(6)
EntNet
ğ‡ğ¢ğğğğ­ğŸ‘
Transformer
Encoder 
Ã—L(4)
Input
Input
ğ„ğ±ğ­ğğğ­ğŸ
ğ„ğ±ğ­ğğğ­ğŸ‘
Fig. 1.
Submodels division and switching of ViT. When switching from
submodel 1 to submodel 2 of ViT, we only need to remove ExtNet1 of
submodel 1 and then connect HidNet2 and ExtNet2 to form submodel 2.
models arrive in real-time, requiring timely adjustments to
caching schemes [32]. Moreover, caching models at BSs
incurs loading latency [33], and different caching schemes for
dynamic DNNs yield different model valid service times and
inference precisions, thereby affecting usersâ€™ QoE. Therefore,
the adverse impact of model loading time on QoE must be
considered and minimized.
Although some related studies consider model loading time
[33], [34] and explore its impact on the joint optimization
of model caching and request routing problem in multi-edge
collaborative MEC to maximize throughput under latency and
accuracy constraints, they only use model loading costs to
refine the solution, without considering its impact on decision-
making. Therefore, existing solutions are not efficient and
flexible enough in MEC environments with limited resources
and dynamically changing user requests.
In this paper, we study a Joint Dynamic model Caching and
request Routing problem (JDCR) with the goal of maximizing
user request inference precision. Based on linear programming
(LP) and randomized rounding, we first propose a novel offline
multi-edge Collaborative dynamic model Caching and request
Routing optimization algorithm (CoCaR), which provides an
approximate optimal solution to the JDCR problem with theo-
retical guarantees. Subsequently, we extend CoCaR to CoCaR-
OL to adapt to online scenarios where user requests are
hard to predict. Specifically, we divide time into consecutive
observation windows. When making decisions in each window,
we first consider the BS resource and latency constraints, as
well as the impact of model loading time on QoE due to the
switching of caching schemes. Then, based on BS caching
results from the previous window, we utilize the capability of
rapid switching between submodels of the same dynamic DNN
to flexibly adjust the caching scheme, thereby increasing the
valid service time of the model. Additionally, by leveraging the
fine-grained resource utilization feature of dynamic DNNs, BS
resources can be used more efficiently, enabling the caching
of more DNN models to satisfy a greater number of user
demands. The contributions of this paper are as follows:
â€¢ We formulate the JDCR problem in MEC networks. The
goal is to maximize the total inference precision for user
requests while adhering to constraints on BS resources,
latency, and model loading time. As this JDCR problem
{0.98, 1.2}
{0.96, 1.5}
{0.94, 0.8}
{0.9, 0.9}
{0.84, 0.5}
{0.96, 1.5}
Request 
Sequence
Dynamic 
DNN
Static 
DNN
Time
Ã—60
Ã—40
Ã—20
Ã—80
â€¦â€¦
0.24 s
0.04 s
0.04 s
0.43 s
â€¦
â€¦
â€¦â€¦
â€¦â€¦
â€¦â€¦
Number of user requests for model A or model B
ğ€ğ’Šğğ’ŠThe i-th submodel of model A or model B
Observation Window ğœObservation Window ğœ+ 1
Average 
Inference 
Precision 
Average 
Hit 
Rate
0.45
0.46
0.87
0.91
{Inference precision, model size}
Loading time 
of model
ğğŸ‘
ğğŸ‘
ğ€ğŸ
ğ€ğŸ
ğ€ğŸ
ğğŸ
ğğŸ‘
ğğŸ‘
ğ€ğŸ‘
1.06 s
1.33 s
â€¦
Valid service time of model
Fig. 2. Examples of static DNN and dynamic DNN schemes.
is a non-convex, nonlinear integer programming problem
that poses significant challenges for resolution, we perform
equivalent transformations and relaxations.
â€¢ We propose a novel offline algorithm, CoCaR, based on
LP and random rounding to solve the JDCR problem.
Specifically, CoCaR can leverage dynamic DNNs to provide
a more flexible and fine-grained caching scheme. Theoretical
analyses show that CoCaR has strict performance guarantees
and achieves an approximation ratio of (1 âˆ’
q
4 ln |H|
Pâ€ 
)2 to
the optimal integer solution, where |H| represents the total
number of submodels, and Pâ€  is the objective value of the
optimal fractional solution.
â€¢ We further extend CoCaR to handle online scenarios where
future user requests are difficult to predict in advance and
both caching and routing decisions must be made in real-
time. Thus, we propose CoCaR-OL, an online variant that
determines dynamic DNN model caching strategies based
on expected future gains estimated from historical user
request patterns. It further adopts a greedy algorithm for
routing decisions, enabling effective adaptation to dynamic
and unpredictable demands.
â€¢ Extensive simulations validate the effectiveness of the pro-
posed CoCaR and CoCaR-OL. The results show that CoCaR
achieves at least a 46% improvement in average inference
precision and a 44% increase in cache hit rate over four
state-of-the-art algorithms, utilizing at least 86% of BS
resources. In online scenarios, CoCaR-OL attains at least
a 1.71Ã— improvement in usersâ€™ QoE and a 1.73Ã— increase
in cache hit rate over the baseline.
II. BACKGROUND AND MOTIVATION
Dynamic DNN. As shown in Fig. 1, taking Vision Trans-
former (ViT) [35] as an example, we divide a DNN into three
parts based on its network architecture: the entry network
(EntNet), the hidden network (HidNet), and the exit network
(ExtNet). To ensure a certain level of inference precision, the
DNN is divided into multiple submodels of different sizes by
partitioning HidNet, and each submodel has its own trained
ExtNet. Additionally, switching between submodels of a dy-
namic DNN involves adding or removing the corresponding
HidNet and ExtNet.
Next, we use an example to compare a scheme that caches
only complete models (i.e., static DNN) with a scheme based


--- Page 3 ---
3
â€¦
EntNet
â‹¯
EntNet
EntNet
ğ‡ğ¢ğğğğ­
ğ‡ğ¢ğğğğ­
ğ‡ğ¢ğğğğ­
â€¦
Base Station
User
Wireless Link
Wired Link
Cloud Server
Dynamic DNN 
cached at BS
â€¦
Model ğŸ:
Model ğŸ:
Model ğ‘´:
ğ„ğ±ğ­ğğğ­
ğ„ğ±ğ­ğğğ­
ğ„ğ±ğ­ğğğ­
Fig. 3. System model. The same color indicates the dynamic DNN associated
with a model type, while different lengths of that color represent different
submodels of the dynamic DNN.
on a dynamic DNN at a BS. Apart from the specific exper-
imental settings described below, the remaining parameters
and performance results are obtained under the experimental
environment and configurations described in Sec. VI-A.
Motivating Example. As shown in Fig. 2, there are two
DNN model types, A and B, each divided into three sub-
models of different sizes. A period of 50s is divided into 10
observation windows, with 100 users randomly and uniformly
initiating model inference requests in each window. The cache
size is set to 2, with cached models loaded into the BS memory
at the start of each window, and providing valid services
once fully cached. In the observation window Ï„ + 1, the
static DNN scheme takes 1.33s to fully cache model B (size
1.5), but cannot fully cache model A (size 1.2) due to the
limited cache size at the same time. In contrast, the dynamic
DNN-based scheme utilizes cache results from a window Ï„,
taking only 0.43s to cache model B from submodel 2 to
submodel 3, providing 4.57s of valid service. Additionally,
it can also cache submodel 1 of model A, achieving an
inference precision of 0.84, thereby providing more services
to more users. Ultimately, using a dynamic DNN improves
the average inference precision and cache hit rate by at least
48% compared to a static DNN. This example underscores the
significance of developing novel dynamic DNN-based model
caching and request routing schemes.
III. SYSTEM MODEL AND PROBLEM FORMULATION
A. System Overview
As shown in Fig. 3, we consider an MEC system consisting
of N = {1, 2, Â· Â· Â· , N} BSs equipped with edge servers. These
BSs have caching and computing capabilities and can com-
municate with each other through high-speed wired networks.
They can provide some of the M services for end users
U = {1, 2, Â· Â· Â· , U}. We assume that users are randomly and
uniformly distributed within the BS coverage area, with the
closest BS to each user u (called the home BS) denoted as
Ë†nu. We focus on joint model caching and request routing
in MEC networks. Let M = {1, 2, Â· Â· Â· , M} represent the
set of different DNN model types, and mu âˆˆM the DNN
model inference request generated by user u. Let H(m) =
ğ‘›!
ğ‘›"
ğ‘›#
Perform model 
inference at BS ğ‘›#
1
2
3
Target BS 
for user ğ‘¢
ğ‘¢
User
Dynamic DNN 
cached at BS
Request 
model 
type
4
5
6
Fig. 4. Communication latency: Routing user uâ€™s request involves wireless
transmission from u to home BS n1, wired transmission from n1 to target BS
n3, and a total of 6 hops from initiating the request to receive the inference
result.
{hm
0 , hm
1 , Â· Â· Â· , hm
H(m)} denote the set of submodels {hm
j }H(m)
j=1
of the dynamic DNN associated with m, along with an empty
submodel hm
0
1. Then, denote by H = S
mâˆˆM H(m) the set
of all submodels, and |H| the total number of submodels. For
each model type m, a BS can cache at most one submodel
concurrently to provide service. Furthermore, we define a
partial order (H, âª¯) over the set of submodels H, where
each H(m) is linearly ordered, and elements from different
H(m) are incomparable. That is, for any hi, hj âˆˆH, we have
hi âª¯hj if and only if hi, hj âˆˆH(m), and hi is a submodel
no larger than hj. Similarly, hi â‰»hj holds if and only if hi
and hj belong to the same H(m), and hi is a larger submodel
than hj.
Considering the dynamic nature of the MEC network,
time is divided into discrete time slots, denoted as T
=
{1, 2, . . . , t, . . . , T}. Multiple consecutive time slots form an
observation window, denoted as Î“ = {1, 2, . . . , Ï„, . . . , |Î“|}.
Each observation window updates the caching and routing
decisions based on user requests. If user uâ€™s request is routed
to the target BS that caches the submodel of the dynamic DNN
of mu, it is called a hit, and model inference is performed to
obtain a result with corresponding precision; otherwise, it is a
miss, and the user uâ€™s request will be sent to the cloud with
an inference precision of 0 at the edge.
Next, we focus on the joint offline caching and routing
decisions within an observation window Ï„. For brevity, we
omit Ï„ when there is no ambiguity.
B. Model Caching Model
It is crucial to decide which DNN models to cache on
resource-limited BSs. Let binary variable xn,h âˆˆ{0, 1} be the
model caching decision for submodel h in BS n at window Ï„,
where xnh = 1 if submodel h has been cached at BS n, and
xn,h = 0 otherwise. Each BS is required to cache a submodel
of the dynamic DNN associated with each model type:
X
hâˆˆH(m)
xn,h = 1, âˆ€n âˆˆN, m âˆˆM,
(1)
where if xn,hm
0 = 1, it indicates that the empty submodel of
model type m is cached (which is equivalent to not caching
the model type m).
1Here, hm
0 is introduced for modeling convenience, with no impact on BS
resource usage or model inference precision.


--- Page 4 ---
4
In addition, the total cached models cannot exceed the
memory capacity of BS n:
X
hâˆˆH
xn,h Â· rh â‰¤Rn, âˆ€n âˆˆN,
(2)
where rh is the required memory space to cache submodel h,
and Rn is the total memory capacity of BS n.
C. Request Routing Model
In general, there are many methods to pre-fetch user re-
quests [23], [36], which can then be routed to the target BS or
the cloud for inference. Let yn,u âˆˆ{0, 1} be the user request
routing decision variable at window Ï„, where yn,u = 1 if the
request from user u is routed to BS n, otherwise yn,u = 0.
Let du, ddlu, and su denote the request data size of user u,
the maximum perceived latency user u can tolerate, and the
initiation time of user uâ€™s request in the observation window
Ï„, respectively.
First, each user uâ€™s request is routed to at most one BS, and
requests that cannot be routed are sent to the cloud:
X
nâˆˆN
yn,u â‰¤1, âˆ€u âˆˆU.
(3)
Second, users experience end-to-end inference latency from
request initiation to receiving the result, which includes:
â€¢ Communication latency. As shown in Fig. 4, the communi-
cation latency for routing user requests can be calculated as
T off
u
= P
nâˆˆN yn,u Â· ( du
Ï†Ë†nu +
du
rË†nu,n + Î»u,n), âˆ€u âˆˆU, where
Ï†Ë†nu = Wclog2(1+ guEn,u
N0
) is the wireless transmission rate,
gu is user uâ€™s transmission power, En,u is the channel gain
from user u to BS n, Wc is the channel bandwidth, and N0
is the noise power 2; rË†nu,n is the average wired transmission
rate between home BS Ë†nu and target BS n; and Î»u,n denotes
the propagation latency of user uâ€™s request routed to BS n,
measured from its initiation to the reception of the inference
result, which depends on the number of hops.
â€¢ Inference latency. The inference latency of user uâ€™s re-
quest at the target BS n can be calculated as T infer
u
=
P
nâˆˆN
P
hâˆˆH(mu) yn,u Â· xn,h Â· chÂ·du
Cn , âˆ€u âˆˆU, where ch
denotes the computational flops required for model h to
process one unit of data request, and Cn denotes the
maximum computational capacity that BS n can provide.
Thus, user uâ€™s end-to-end inference latency Tu can be
calculated as Tu = T off
u +T infer
u
, âˆ€u âˆˆU. To ensure the QoE of
end-users, Tu must not exceed the maximum perceived latency
ddlu that user u can tolerate:
Tu â‰¤ddlu, âˆ€u âˆˆU.
(4)
Additionally, the time required to load the DNN model
into the BSâ€™s memory at each observation window cannot
be ignored. In offline scenarios, we can proactively pre-
download the required DNN models from the cloud to the
secondary storage of the BS based on the expected user
requests in the upcoming window, and then load the models
2In this paper, we simplify the transmission model by assuming that BSs
adopt a fixed transmission rate Ï†Ë†nu for all users u [37].
TABLE I
LIST OF NOTATIONS.
Notation
Description
N, U
Set of base stations and users
M, H
Set of DNN model types and submodels
H(m)
Set of submodels associated with model type m
Ë†nu
Closest BS to user u (home BS)
mu
User uâ€™s requested DNN model type m
xn,h
Caching variable for submodel h at BS n in window Ï„
yn,u
Routing variable for user u to BS n in window Ï„
du, ddlu
User uâ€™s data size and maximum tolerable latency
su
User uâ€™s request initiation time in window Ï„
Rn
Memory capacity of BS n
Cn
Computation capacity of BS n
rh
Size of submodel h
ch
Flops of submodel h per data unit
Ï†n
Wireless transmission rate of BS n
rnâ€²,n
Wired transmission rate between BS nâ€² and BS n
Î»u,n
Propagation latency for user u to BS n in window Ï„
T off
u
User uâ€™s communication latency in window Ï„
T infer
u
User uâ€™s inference latency in window Ï„
Tu
User uâ€™s total end-to-end inference latency in window Ï„
T load
n,m
Load latency of model type m at BS n in window Ï„
ph
Expected inference precision of submodel h
into the memory at the beginning of the next observation
window. Let T load
n,m denote the loading latency for caching the
submodel of the dynamic DNN associated with model type
m at BS n. When caching submodel h of a model type that
is already cached in the previous window, let Dswit
m
(hâ€², h)
denote the switching latency from submodel hâ€² to submodel
h; conversely, when caching a model that has not been cached
before, let Dnew
m
(hm
0 , h) represent the latency to add submodel
h of model m. For the observation window Ï„, the model
loading latency of the model type m at BS n can be calculated
as:
T load
n,m =
X
h,hâ€²âˆˆ{H(m)\hm
0 }
xn,hâ€²(Ï„ âˆ’1) Â· Dswit
m
(hâ€², h) Â· xn,h
+
X
hâˆˆ{H(m)\hm
0 }
xn,h Â· Dnew
m
(hm
0 , h) Â· xn,hm
0 (Ï„ âˆ’1)
= xT
n,m Â· Dm Â· xâ€²
n,m(Ï„ âˆ’1),
(5)
where vector xn,m = [xn,hm
0 , xn,hm
1 , Â· Â· Â· , xn,hm
H(m)], and Dm
can be regarded as the transit time of model m, including
the possible switching or addition of the model state during
the transit process from the previous window to the current
window.
Since DNNs must be cached before providing services, user
uâ€™s request can be routed to BS n with the cached submodel
of mu for inference only if the model loading completion time
is earlier than the request initiation time su:
X
nâˆˆN
yn,u Â· T load
n,mu â‰¤su, âˆ€u âˆˆU.
(6)


--- Page 5 ---
5
D. Problem Formulation
In the offline scenario where user requests can be predicted
in advance, the purpose is to jointly decide on model caching
and request routing variables in a multi-edge collaborative
scenario to maximize the total inference precision of all user
requests in each observation window, while the constraints
associated with the decision variables are not violated. Let ph
be the expected inference precision of the submodel h. The
problem can be formulated as follows:
(P) max
x,y
X
uâˆˆU
X
nâˆˆN
X
hâˆˆH(mu)
xn,h Â· yn,u Â· ph
(7)
s.t.
(1), (2), (3), (4), (6),
xn,h âˆˆ{0, 1}, yn,u âˆˆ{0, 1}, âˆ€n âˆˆN, u âˆˆU, h âˆˆH,
(8)
where constraint (1) ensures that each BS caches one submodel
per model type; constraint (2) limits the BS memory used by
cached models; constraint (3) restricts each user request to at
most one BS; constraint (4) keeps the end-to-end inference
latency within user tolerance; and constraint (6) requires the
DNN models to be cached before executing user requests at
BSs. For ease of reference, important notations are listed in
Table I.
IV. THE COCAR APPROACH
In this section, we transform problem P, formulated
under the assumption in Sec. III-C that user requests for
the upcoming window are known in advance, into a linear
programming problem. The assumption allows proactive
downloading of the required models from the cloud to the
edge server, followed by loading them into the serverâ€™s
memory. Then, we propose CoCaR to solve the transformed
problem. Furthermore, we provide a detailed theoretical
analysis and complexity evaluation for CoCaR, and extend it
to practical scenarios.
A. Problem Transformation
The term xn,h Â· yn,u in problem P makes it a nonlinear
integer programming problem, which is difficult to solve
directly. To address this, we introduce the virtual binary integer
variable An,u,h to transform problem P into an equivalent
integer linear programming problem P1. We define
An,u,h = xn,h Â· yn,u, âˆ€n âˆˆN, u âˆˆU, h âˆˆH(mu).
(9)
Then, we introduce three additional constraints to equiva-
lently represent Eq. (9):
An,u,h â‰¤xn,h, An,u,h â‰¤yn,u,
xn,h + yn,u âˆ’1 â‰¤An,u,h.
(10)
Subsequently, from Eq. (9), we can observe that An,u,h has
practical significance, i.e., An,u,h = 1 indicates that BS n has
cached submodel h and user uâ€™s request is routed to BS n.
Thus, we can eliminate y to reduce the number of variables
and simplify the problem.
Algorithm 1 CoCaR Algorithm
Input: {mu, du, ddlu, su}, Ï„, Ëœxn,h(Ï„ âˆ’1), N, M
Output: Ëœxn,h, ËœAn,u,h, Ëœyn,u
1: Solve problem P1-LR and get the optimal fractional solution
xâ€ 
n,h, Aâ€ 
n,u,h.
2: for n âˆˆN, m âˆˆM do
3:
Sample Ëœxn,m from the multinoulli distribution Pr[Ëœxn,m =
I(h)] = xâ€ 
n,h, h âˆˆH(m).
4:
Denote the sample result by Ëœxn,m = I(Ë†h).
5:
Set Ëœxn,Ë†h = 1, Ëœxn,h = 0, h âˆˆH(m)\{Ë†h}.
6: end for
7: for u âˆˆU, n âˆˆN do
8:
for h âˆˆH(mu) do
9:
Set ËœÏ•n,u,h = 1 with probability
Aâ€ 
n,u,h
xâ€ 
n,h
.
10:
Let ËœAn,u,h = Ëœxn,h Â· ËœÏ•n,u,h.
11:
end for
12:
Let Ëœyn,u = 1(P
hâˆˆH(mu) ËœAn,u,h > 0).
13: end for
14: return Ëœxn,h, ËœAn,u,h, Ëœyn,u
Therefore, problem P1 involves only the variables xn,h and
An,u,h. For each user u âˆˆU, there is
X
nâˆˆN
X
hâˆˆH(mu)
An,u,h =
X
nâˆˆN
X
hâˆˆH(mu)
xn,h Â· yn,u =
X
nâˆˆN
yn,u,
(11)
which holds due to Eq. (1). Thus, the constraint (3) can be
equivalently transformed into:
X
nâˆˆN
X
hâˆˆH(mu)
An,u,h â‰¤1, âˆ€u âˆˆU.
(12)
Furthermore, for the T off
u
part of the end-to-end inference
latency in Eq. (4), let T off
u
= P
nâˆˆN
P
hâˆˆH(mu) xn,h Â· yn,u Â·
( du
Ï†Ë†nu +
du
rË†nu,n +Î»u,n), âˆ€u âˆˆU, which is equivalent to the orig-
inal constraint. Consequently, problem P1 can be formulated
as:
(P1) max
x,A
X
uâˆˆU
X
nâˆˆN
X
hâˆˆH(mu)
An,u,h Â· ph
(13)
s.t.
(1), (2), (12),
An,u,h â‰¤xn,h, âˆ€n âˆˆN, u âˆˆU, h âˆˆH(mu), (14)
X
nâˆˆN
X
hâˆˆH(mu)
An,u,h Â· (( du
Ï†Ë†nu
+
du
rË†nu,n
+ Î»u,n)
+ ch Â· du
Cn
) â‰¤ddlu, âˆ€u âˆˆU,
(15)
X
nâˆˆN
X
h,hâ€²âˆˆH(mu)
An,u,h Â· xn,hâ€²(Ï„ âˆ’1) Â· Dmu(hâ€², h)
â‰¤su, âˆ€u âˆˆU,
(16)
xn,h âˆˆ{0, 1}, âˆ€n âˆˆN, h âˆˆH,
(17)
An,u,h âˆˆ{0, 1}, âˆ€n âˆˆN, u âˆˆU, h âˆˆH(mu).
(18)
However, problem P1 is still difficult to solve in polynomial
time due to its non-convexity. To address this, we relax the


--- Page 6 ---
6
variables in P1 and transform it into an LP problem:
(P1-LR) max
x,A
X
uâˆˆU
X
nâˆˆN
X
hâˆˆH(mu)
An,u,h Â· ph
(19)
s.t.
(1), (2), (12), (14), (15), (16),
xn,h âˆˆ[0, 1], âˆ€n âˆˆN, h âˆˆH,
(20)
An,u,h âˆˆ[0, 1], âˆ€n âˆˆN, u âˆˆU, h âˆˆH(mu).
(21)
B. Approximation Algorithm
We propose the CoCaR algorithm to solve the JDCR
problem using random rounding and linear programming. The
details are provided below and summarized in Alg. 1.
First, by applying a linear programming solver [38] to
solve problem P1-LR, we can obtain the optimal fractional
solutions xâ€ 
n,,h and Aâ€ 
n,u,h (Line 1). Then, we round xâ€ 
n,h
and Aâ€ 
n,u,h to obtain vector Ëœxn,m and intermediate variable
ËœÏ•n,u,h, where Ëœxn,m contains the integer solutions Ëœxn,h, and
ËœÏ•n,u,h indicates whether user uâ€™s request attempts to route to
submodel h at BS n for inference, serving to determine the
integer solution ËœAn,u,h. Specifically, let the vector Ëœxn,m be the
indicator vector I(h) with probability xâ€ 
n,h (Lines 3-5) and the
rounding probability of ËœÏ•n,u,h be Ï•â€ 
n,u,h (Lines 9-10):
Pr[Ëœxn,m = I(h)] = xâ€ 
n,h,
Pr[ËœÏ•n,u,h = 1] = Ï•â€ 
n,u,h =
Aâ€ 
n,u,h
xâ€ 
n,h
,
(22)
where I(h) is a one-hot vector of size |H(m)|, with the h-th
element being 1 and 0 otherwise, indicating that the submodel
h of model m is cached at BS n, i.e., Ëœxn,h = 1. Finally, let
ËœAn,u,h = Ëœxn,h Â· ËœÏ•n,u,h.
(23)
Thus, we can revert Ëœyn,u based on ËœAn,u,h (Line 12).
Then, we provide theoretical guarantees on the quality of
the solutions returned by the CoCaR algorithm.
Lemma 1. The solutions returned by CoCaR satisfy the
constraints in problem P1 in expectation.
Proof. First, the inequality ËœAn,u,h â‰¤Ëœxn,h holds according to
Eq. (23), which satisfies constraint (14).
Second, for integer solutions Ëœxn,h, P
hâˆˆH(m) Ëœxn,h
=
âˆ¥Ëœxn,mâˆ¥1 = 1 holds. Therefore, constraint (1) can be satisfied,
where âˆ¥Ëœxn,mâˆ¥1 is the L1 norm of vector Ëœxn,m.
Next, the expected amount of memory consumed by model
caching at BS n, n âˆˆN is given by: E[P
hâˆˆH Ëœxn,h Â· rh] =
E[P
mâˆˆM Ëœxn,m Â· rm] = P
mâˆˆM
P
hâˆˆH(m) xâ€ 
n,h Â· rh â‰¤Rn,
where vector rm
= [rhm
0 , rhm
1 , Â· Â· Â· , rhm
H(m)], and the last
inequality holds due to constraint (2).
Then, constraint (12) is satisfied in expectation due to:
E[
X
nâˆˆN
X
hâˆˆH(mu)
ËœAn,u,h] =
X
nâˆˆN
E[Ëœxn,mu Â· âƒ—ËœÏ•n,u]
=
X
nâˆˆN
X
hâˆˆH(mu)
Pr[Ëœxn,mu = I(h)] Â· Pr[ËœÏ•n,u,h = 1]
(24)
=
X
nâˆˆN
X
hâˆˆH(mu)
xâ€ 
n,h Â·
Aâ€ 
n,u,h
xâ€ 
n,h
=
X
nâˆˆN
X
hâˆˆH(mu)
Aâ€ 
n,u,h â‰¤1,
where vector âƒ—ËœÏ•n,u = [ËœÏ•n,u,hmu
0
, ËœÏ•n,u,hmu
1
, Â· Â· Â· , ËœÏ•n,u,hmu
H(mu)].
The first equation holds due to vector operations, and the third
due to Eq. (22). The inequality holds by constraint (12).
Furthermore, the end-to-end inference latency perceived by
user u âˆˆU is expected to be:
E[
X
nâˆˆN
X
hâˆˆH(mu)
ËœAn,u,h Â· (( du
Ï†Ë†nu
+
du
rË†nu,n
+ Î»u,n) + ch Â· du
Cn
)]
= E[
X
nâˆˆN
âƒ—ËœÏ•n,u Â· (Ëœxn,mu âŠ™âƒ—Ë†Tn,u)]
(25)
=
X
nâˆˆN
X
hâˆˆH(mu)
Aâ€ 
n,u,h Â· Ë†Tn,u,h â‰¤ddlu,
where Ë†Tn,u,h = (( du
Ï†Ë†nu +
du
rË†nu,n +Î»u,n)+ chÂ·du
Cn ), vector âƒ—Ë†Tn,u =
[Ë†Tn,u,hmu
0
, Ë†Tn,u,hmu
1
, Â· Â· Â· , Ë†Tn,u,hmu
H(mu)] and âŠ™represents the
element-wise vector multiplication. The second equation holds
due to Eq. (22), and the inequality is by constraint (15).
Finally, the model loading latency for user u, u âˆˆU is
expected to be:
E[
X
nâˆˆN
X
hâˆˆH(mu)
X
hâ€²âˆˆH(mu)
ËœAn,u,h Â· xn,hâ€²(Ï„ âˆ’1) Â· Dmu(hâ€², h)]
= E[
X
nâˆˆN
âƒ—ËœÏ•n,u Â· (Ëœxn,mu âŠ™âƒ—Ë†Dn,mu)]
(26)
=
X
nâˆˆN
X
h,hâ€²âˆˆH(mu)
Aâ€ 
n,u,h Â· xn,hâ€²(Ï„ âˆ’1) Â· Ë†Dn,h â‰¤su,
where Ë†Dn,h = P
hâ€²âˆˆH(mu) xn,hâ€²(Ï„ âˆ’1) Â· Dmu(hâ€², h), and
vector âƒ—Ë†Dn,mu = [ Ë†Dn,hmu
0
, Ë†Dn,hmu
1
, Â· Â· Â· , Ë†Dn,hmu
H(mu)]. The last
inequality holds due to constraint (16).
Lemma 2. The objective value obtained by the CoCaR is
equal to that of the optimal fractional solution in expectation.
Proof. First, let Pâ€  be the objective value in the optimal
fractional solution, Pâ€  = P
uâˆˆU
P
nâˆˆN
P
hâˆˆH(mu) Aâ€ 
n,u,hÂ·ph.
In expectation, the total inference precision of user requests
that the CoCaR algorithm can obtain is:
E[ËœP] = E[
X
uâˆˆU
X
nâˆˆN
X
hâˆˆH(mu)
ËœAn,u,h Â· ph]
=
X
uâˆˆU
X
nâˆˆN
E[âƒ—ËœÏ•n,u Â· (Ëœxn,mu âŠ™âƒ—Pmu)]
=
X
uâˆˆU
X
nâˆˆN
X
hâˆˆH(mu)
Aâ€ 
n,u,h Â· ph = Pâ€ ,
(27)
where vector âƒ—Pmu = [pmu
0
, pmu
1
, Â· Â· Â· , pmu
H(mu)].
The above lemmas show that CoCaR has theoretical guar-
antees in expectation. However, in practice, random rounding
may lead to constraint violations. Next, we analyze in detail
using the Chernoff Bound theorem [39].
Definition 1 (Chernoff Bound [39]). Given I independent
random variables z1, z2, . . . , zI, where for all zi obey multi-
noulli distribution, and zi âˆˆ[0, 1]. Let Âµ = E[PI
i=1 zi]. Then,
it holds that Pr[PI
i=1 zi â‰¥(1 + Î´)Âµ] â‰¤exp
âˆ’Î´2Âµ
2+Î´ , âˆ€Î´ > 0
and Pr[PI
i=1 zi â‰¤(1 âˆ’Î´)Âµ] â‰¤exp
âˆ’Î´2Âµ
2
, âˆ€0 < Î´ < 1.


--- Page 7 ---
7
Theorem 1. There is a high probability that the solution
returned by the CoCaR algorithm has at least (1âˆ’
q
4 ln |H|
Pâ€ 
)2
approximation ratio with the optimal integer solution, where
Pâ€  is the objective value obtained from the optimal fractional
solution, under the assumption Pâ€  â‰¥4 ln |H|.
Proof. According to Eq. (22), Pâ€  is equivalent to Pâ€  =
P
uâˆˆU
P
nâˆˆN
P
hâˆˆH(mu) xâ€ 
n,hÂ·Ï•â€ 
n,u,hÂ·ph. Next, to satisfy the
independence requirement for the Chernoff Bound, we provide
a two-stage proof.
In stage I, we first keep Ï•â€ 
n,u,h in Pâ€  unchanged and round
xâ€ 
n,h to Ëœxn,h to obtain Pâ€²:
Pâ€² =
X
uâˆˆU
X
nâˆˆN
X
hâˆˆH(mu)
Ëœxn,h Â· Ï•â€ 
n,u,h Â· ph
=
X
nâˆˆN
X
mâˆˆM
(Ëœxn,m Â·
X
uâˆˆ{u|mu=m}
âƒ—Pu),
(28)
where âƒ—Pu = [
Aâ€ 
n,u,hmu
0
xâ€ 
n,hmu
0
Â· phmu
0
, Â· Â· Â· ,
Aâ€ 
n,u,hmu
H(mu)
xâ€ 
n,hmu
H(mu)
Â· phmu
H(mu)].
Denote vn,m = Ëœxn,m Â· P
uâˆˆ{u|mu=m} âƒ—Pu, the determinism
of P
uâˆˆ{u|mu=m} âƒ—Pu and the independence of Ëœxn,m ensure
that vn,m are independent. Clearly, we have E[Pâ€²] = Pâ€ , and
by the Chernoff Bound theorem, when Î´1 âˆˆ[0, 1], we have
Pr[Pâ€² â‰¤(1 âˆ’Î´1)Pâ€ ] = q1 â‰¤exp(âˆ’Î´2
1Pâ€ 
2
).
(29)
Next, find a Î´1 value for the right side of Eq. (29) to make
it very small. Specifically, we require,
exp(âˆ’Î´2
1Pâ€ 
2
) â‰¤
1
|H|2 .
(30)
This means that as the number of submodels increases, the
probability bound converges to zero quickly. Eq. (30) holds
when Î´1 â‰¥
q
4 ln |H|
Pâ€ 
, which is true if we pick Î´1 =
q
4 ln |H|
Pâ€ 
.
In stage II, building upon the rounded result Ëœxn,h in stage
I, we round Ï•â€ 
n,u,h to ËœÏ•n,u,h, resulting in the equation ËœP: ËœP =
P
uâˆˆU
P
nâˆˆN
P
hâˆˆH(mu) Ëœxn,h Â· ËœÏ•n,u,h Â· ph. Since Ëœxn,h Â· ph
are deterministic and ËœÏ•n,u,h are independent, it follows that:
E[ËœP] = P
uâˆˆU
P
nâˆˆN
P
hâˆˆH(mu) Ëœxn,h Â· Pr[ËœÏ•n,u,h = 1] Â· ph =
Pâ€². By the Chernoff Bound theorem, when Î´2 âˆˆ[0, 1], we
have:
Pr[ËœP â‰¤(1 âˆ’Î´2)Pâ€²] = q2 â‰¤exp(âˆ’Î´2
2Pâ€²
2
).
(31)
Since stage I and stage II are independent, when the
condition Pâ€² â‰¥(1 âˆ’Î´1)Pâ€  in Eq. (29) is satisfied, we have:
q2 = Pr[ËœP â‰¤(1 âˆ’Î´2)Pâ€²|Pâ€² â‰¥(1 âˆ’Î´1)Pâ€ ]
â‰¤exp(âˆ’Î´2
2Pâ€²
2
) â‰¤exp(âˆ’Î´2
2(1 âˆ’Î´1)Pâ€ 
2
).
(32)
Next, a value of Î´2 is specified to make the right side of
Eq. (32) become very small. Specifically, we require:
exp(âˆ’Î´2
2(1 âˆ’Î´1)Pâ€ 
2
) â‰¤
1
|H|2(1âˆ’Î´1) ,
(33)
Eq. (33) holds when Î´2 satisfies Î´2 â‰¥
q
4 ln |H|
Pâ€ 
. We select
Î´2 =
q
4 ln |H|
Pâ€ 
to make the inequality hold.
Under the conditions specified in (30) and (33), by combin-
ing (29) and (31), we obtain:
Pr[ËœP â‰¥(1 âˆ’Î´1)(1 âˆ’Î´2)Pâ€ ]
â‰¥Pr[Pâ€² â‰¥(1 âˆ’Î´1)Pâ€ ]Pr[ËœP â‰¥(1 âˆ’Î´2)Pâ€²|Pâ€² â‰¥(1 âˆ’Î´1)Pâ€ ]
= (1 âˆ’q1)(1 âˆ’q2) â‰¥(1 âˆ’
1
|H|2 )(1 âˆ’
1
|H|2(1âˆ’Î´1) ).
(34)
Since ËœPâˆ—â‰¤Pâ€ , where ËœPâˆ—is the optimal integer solution, it
follows that: Pr[ËœP â‰¥(1 âˆ’Î´1)(1 âˆ’Î´2)ËœPâˆ—] â‰¥Pr[ËœP â‰¥(1 âˆ’
Î´1)(1 âˆ’Î´2)Pâ€ ] â‰¥(1 âˆ’
1
|H|2 )(1 âˆ’
1
|H|2(1âˆ’Î´1) ).
In practice, as the number of user requests, BSs, and models
increases, Pâ€  related to u âˆˆU, n âˆˆN, h âˆˆH will significantly
exceed 4 ln |H| (Pâ€  â‰«4 ln |H|). Therefore, the objective value
obtained by the CoCaR algorithm has an approximation ratio
of at least (1 âˆ’Î´1)(1 âˆ’Î´2) = (1 âˆ’
q
4 ln |H|
Pâ€ 
)2 to the optimal
integer solution ËœPâˆ—with a high probability.
Theorem 2. For BS n âˆˆN, the memory consumption of the
cached model returned by the CoCaR algorithm has a high
probability of not exceeding its memory capacity by more than
a factor of (
q
2 ln |H|
Î¶â€ 
n
+
1
âˆš
2)2 + 1
2, where Î¶â€ 
n is the memory
consumption of BS n in the optimal fractional solution, under
the assumption Î¶â€ 
n â‰¥ln |H|.
Proof. According to Lemma 1, E[P
hâˆˆH Ëœxn,h Â· rh]
=
E[P
mâˆˆM Ëœxn,m Â· rm] = P
mâˆˆM
P
hâˆˆH(m) xâ€ 
n,h Â· rh, where
Ëœxn,m Â· rm are independent random variables and can be
normalized into values within [0,1]. Thus, we apply the
Chernoff Bound theorem to show that for any Î´
>
0:
Pr[P
hâˆˆH Ëœxn,h Â· rh â‰¥(1 + Î´) P
mâˆˆM
P
hâˆˆH(m) xâ€ 
n,h Â· rh] â‰¤
e
âˆ’Î´2 P
mâˆˆM
P
hâˆˆH(m) xâ€ 
n,hÂ·rh
2+Î´
.
Denote Î¶â€ 
n = P
mâˆˆM
P
hâˆˆH(m) xâ€ 
n,h Â· rh. Since Î¶â€ 
n â‰¤Rn,
it follows that: Pr[P
hâˆˆH Ëœxn,h Â· rh â‰¥(1 + Î´)Rn] â‰¤e
âˆ’Î´2Î¶â€ 
n
2+Î´ .
Then, similar to the proof in Theorem 1, to make the right
side of the inequality small, i.e., e
âˆ’Î´2Î¶â€ 
n
2+Î´
â‰¤
1
|H|2 , Î´ must satisfy
Î´ â‰¥ln |H|
Î¶â€ 
n
+
q
ln2 |H|
Î¶â€ 2
n
+ 4 ln |H|
Î¶â€ 
n
.
We select Î´ = 2 ln |H|
Î¶â€ 
n
+ 2
q
ln |H|
Î¶â€ 
n . Since Î¶â€ 
n â‰¥ln |H| holds
in practice, with a high probability, the memory capacity of BS
n will not exceed by more than a factor of 1+Î´ = (
q
2 ln |H|
Î¶â€ 
n
+
1
âˆš
2)2 + 1
2.
By similar proofs, we can prove the following three theo-
rems about constraints (12), (15), and (16).
Theorem 3. For user u âˆˆU, there is a high probability
that P
nâˆˆN
P
hâˆˆH(mu) ËœAn,u,h is no greater than (
q
2 ln |H|
Î·â€ 
+
1
âˆš
2)2 + 1
2, where, Î·â€  = P
nâˆˆN
P
hâˆˆH(mu) Aâ€ 
n,u,h.
Theorem 4. For user u âˆˆU, CoCaRâ€™s end-to-end infer-
ence latency has a high probability of not exceeding the
userâ€™s tolerable perceived latency by a factor larger than
(
q
2 ln |H|
Tâ€ 
u
+
1
âˆš
2)2 + 1
2, where Tâ€ 
u is the end-to-end inference
latency perceived by user u in the optimal fractional solution.


--- Page 8 ---
8
Theorem 5. For model type mu requested by user u, u âˆˆU,
there is a high probability that CoCaRâ€™s model loading latency
will not exceed user uâ€™s request initiation time by more than
a factor of (
r
2 ln |H|
T depâ€ 
mu
+
1
âˆš
2)2 + 1
2, where T depâ€ 
mu
is the model
loading latency of model mu in the optimal fractional solution.
C. Complexity Analysis
First, we analyze the scale of the variables and constraints
in problem P1-LR. Let hâˆ—= maxm |H(m)|, m âˆˆM, thus the
number of variables xn,h and An,u,h are bounded by O(N|H|)
and O(NUhâˆ—), respectively. Therefore, the total number of
decision variables is bound by O(N|H|) + O(NUhâˆ—) â‰¤
O(NMhâˆ—) + O(NUhâˆ—) â‰¤O(NUhâˆ—), given that M < U in
practice. Similarly, the number of constraints is bounded by
O(NM)+O(N)+3O(U)+O(NUhâˆ—). Since M < U < Uhâˆ—,
the total number of constraints is also O(NUhâˆ—).
Then, considering the three loops in Lines 2, 7, and 8 of
Algorithm 1, the time complexity of CoCaR is O(NM) +
O(NUhâˆ—) â‰¤O(NUhâˆ—), as M < U holds.
D. Extension to Practice
Since the CoCaR algorithmâ€™s rounded solution may violate
a few constraints, we employ a heuristic strategy to convert
the rounded solutions Ëœxn,h, Ëœyn,u into feasible ones xn,h, yn,u.
First, for BS n âˆˆN that violates the memory capacity
constraint, we evaluate the benefit of each model type m âˆˆM
based on user requests and inference precision ph (where
Ëœxn,h = 1, h âˆˆH(m)). We remove the least beneficial sub-
model and try to cache smaller ones. If no suitable submodel
can be cached, user requests previously routed to that BS
are redirected to the cloud. This process continues until the
memory constraint (2) is satisfied. Next, for users u âˆˆU
violating constraints on maximum tolerable perceived latency
or model loading latency, requests are redirected to the cloud
until constraints (15) and (16) are met. Finally, if multiple
yn,u = 1 for a user u, the request is routed to the BS with
the highest inference precision. These steps ensure all caching
and routing decisions satisfy the constraints.
V. ADAPTATION TO ONLINE SCENARIOS
In real-world scenarios, the assumption made in Sec. III-C,
i.e., future user requests can be predicted in advance and the
required models can be pre-downloaded from the cloud to
edge servers, does not always hold [40], [41]. Thus, caching
decisions can only be made after receiving the current user
request, and the user must wait until the required model is
cached before being served. To address this, we further extend
the CoCaR approach to an online version, CoCaR-OL, which
adjusts the dynamic model caching strategy according to real-
time user request patterns.
A. System Model
As shown in Fig. 5, in the online scenario, model caching
decisions are made based on historical user request patterns
in each time slot. Therefore, when deciding to cache a larger
Cache ğ€ğŸ: 
Switch ğğŸ‘â†’ğğŸ
Cache Status
Time Slots
ğğŸ‘
ğ€ğŸ
ğğŸ
ğ€ğŸ‘
ğğŸ
Hit with ğ€ğŸ
Hit with ğğŸ‘
Hit with ğ€ğŸ
Hit with ğğŸ
Hit with ğ€ğŸ
Hit with ğğŸ
Hit with ğ€ğŸ
Hit with ğğŸ
Hit with ğ€ğŸ‘
Hit with ğğŸ
User Request
â€¦â€¦
ğğŸ
ğğŸ
ğ‘¡$
ğ‘¡%
ğ‘¡&
ğ‘¡'
ğ‘¡(
Cache Decision
Keep Current 
Cache Status
Cache ğ€ğŸ‘:
Switch ğğŸâ†’ğğŸ
Keep Current 
Cache Status
ğ€ğŸ
ğ€ğŸğš«ğ€ğŸ
ğš«ğ€ğŸ‘
ğ€ğŸ
ğš«ğ€ğŸ
ğš«ğ€ğŸ‘
Fig. 5. Illustration of model caching in an online scenario. There are two types
of models, A and B, each comprising three distinct submodels. At time slot
t1, the system decides to cache submodel A2, which requires downloading
the additional component âˆ†A2 from the cloud to switch from A1 to A2. To
satisfy the cache capacity constraint, submodel B3 is simultaneously reduced
to B2. Since downloading âˆ†A2 takes two time slots, A2 becomes available
to serve users only from time slot t3 onward. In contrast, model eviction is
fast, allowing B2 to serve users immediately at t2.
submodel, the components required for the submodel switch-
ing need to be downloaded from the cloud, which may not be
completed within the current time slot. Since each submodel
hm
i+1 is typically constructed by incrementally adding com-
ponents to hm
i , the system can better utilize cached models
to serve users by downloading and caching the intermediate
submodels in sequence. For example, if a cached submodel hm
1
is decided to be switched to submodel hm
3 , it will first switch
to submodel hm
2 , and then to submodel hm
3 as the additional
required components are downloaded. To model this online
scenario, we introduce some new notations. Let Wn be the
bandwidth between the cloud and base station n, and âˆ†t be
the duration of time slot t. Let ËœOt
n,h and Ot
n,h denote the sizes
of data for submodel h to be downloaded at BS n before and
after the caching decision in time slot t, respectively. Similarly,
ËœXt
n,h and Xt
n,h represent the caching status of submodel h at
BS n before and after the caching decision in time slot t,
respectively.
Thus, at time slot t, the remaining data size of submodel h
to be downloaded at BS n can be calculated as:
ËœOt
n,h =
(
max(Otâˆ’1
n,h âˆ’Â¯t Â· Wn, 0),
if Otâˆ’1
n,h Ì¸= 0,
0,
otherwise,
(35)
where Â¯t = âˆ†t âˆ’min
 P
hâ€²â‰ºh Otâˆ’1
n,hâ€²
Wn
, âˆ†t

, meaning that each
submodel to be downloaded can start downloading immedi-
ately after the previous submodel is downloaded.
Then, whether submodel h has finished downloading at time
slot t can be denoted as:
gt
n,h = 1{Otâˆ’1
n,h Ì¸= 0 âˆ§ËœOt
n,h = 0},
(36)
where1{a} = 1 if condition a is true, and 0 otherwise.
Next, based on the downloading status of submodels, the
caching state can be calculated as:
ËœXt
n,h =
ï£±
ï£´
ï£²
ï£´
ï£³
1,
if gt
n,h = 1 âˆ§P
hâ€²â‰»h gt
n,hâ€² = 0,
0,
if P
hâ€²â‰»h gt
n,hâ€² â‰¥1,
Xtâˆ’1
n,h ,
otherwise.
(37)
Specifically, if at time slot t âˆ’1, submodel h at BS n has just
been downloaded, and no submodel larger than h has been
downloaded in this slot, then submodel h will be cached and
ËœXt
n,h is set to 1. Conversely, if at time slot t, a larger submodel


--- Page 9 ---
9
than h that has been downloaded at BS n, so that submodel
h should not be cached, i.e., ËœXt
n,h is set to 0. Otherwise, the
caching state of submodel h at BS n remains unchanged.
Consequently, based on the above caching state, the infer-
ence precision that model type m can provide at BS n during
time slot t can be calculated as:
Pn,m(t) =
X
hâˆˆH(m)
ËœXt
n,h Â· ph.
(38)
In addition, similar to Eq. (4), the total end-to-end inference
latency at time slot t for routing user requests of model type
m from home BS nâ€² to target BS n for inference can be
calculated as:
Tt
m(nâ€², n) = dm
Ï†nâ€² + dm
rnâ€²,n
+ Î»nâ€²n +
X
hâˆˆH(m)
ËœXt
n,h Â· ch Â· dm
Cn
,
(39)
where dm represents the input data size of model type m.
Finally, let Qt
m(nâ€², n) be the Quality of Experience (QoE)
at time slot t for routing a user request of model type m with
home BS nâ€² to target BS n for inference. To comprehensively
evaluate the trade-off between inference precision and the user-
perceived end-to-end inference latency, we define the QoE
function as follows:
Qt
m(nâ€², n) = Pn,m(t)Â·max(0, 1âˆ’(Tt
m(nâ€², n)âˆ’Î¸)Â·Î±), (40)
where Î¸ is a normalization factor (i.e., the minimum end-
to-end inference latency), and Î± is a smoothing factor that
controls the degradation of precision due to end-to-end in-
ference latency. Specifically, when Tt
m(nâ€², n) > Î¸, the QoE
value Qt
m(nâ€², n) falls below the original precision Pn,m(t),
reflecting the negative impact of latency; conversely, if the
inference latency is equal to Î¸, the QoE will not be degraded.
Accordingly, the target BS that maximizes the QoE for a
user requesting model m with home BS nâ€² at time slot t can
be denoted as:
ntâˆ—
m(nâ€²) = arg max
nâˆˆN Qt
m(nâ€², n).
(41)
Problem Formulation. In the online model caching and
request routing optimization problem, we route user requests to
the BS that maximizes their QoE. The optimization objective
is to maximize the total QoE of all users by designing an
effective caching policy. The problem is formulated as follows:
(P2)
max
X
X
tâˆˆT
X
uâˆˆU
Qt
mu(Ë†nu, ntâˆ—
m(Ë†nu))
(42)
s.t.
X
hâˆˆH
Xt
n,h Â· rh â‰¤Rn, âˆ€t âˆˆT , n âˆˆN
(43)
Tt
mu(Ë†nu, ntâˆ—
mu(Ë†nu)) â‰¤ddlu, âˆ€t âˆˆT , u âˆˆU.
(44)
B. Algorithm Design
To solve problem P2, we propose CoCaR-OL, a heuristic
caching algorithm based on predictive future gain, which
enables fine-grained perception of changes in user request pat-
terns in each time slot and allows flexible adjustment of cached
models. In CoCaR-OL, the action space for enlarging cached
submodels is defined as the set of all submodels from the
Algorithm 2 CoCaR-OL Algorithm
Input: U, N, M, round
Output: Xt
n,h, Ot
n,h
1: Initialize time slot variable t = 0.
2: Initialize model caching and download status X0
n,h, O0
n,h.
3: while True do
â–·Start online monitoring and deciding
4:
Step to new time slot: t â†t + 1.
5:
Routine Update: Calculate temporary download status ËœOt
n,h
according to Eq. (48).
6:
Routine Update: Calculate download-finishing flag variable
gt
n,h and temporary cache status Ëœ
Xt
n,h according to Eq. (49).
7:
Receive current user requests {mt
u | u âˆˆU}.
8:
Calculate the best routing destination ntâˆ—
m(nâ€²) from Eq. (41).
9:
for u âˆˆU do
â–·Route user requests and calc. total QoE
10:
Route use request mu to the best destination ntâˆ—
mu(Ë†nu).
11:
Calculate the corresponding QoE value Qt
mu by Eq. (40).
12:
end for
13:
Update the recent proportion of user requests f t
n,m by Eq. (45).
14:
Ot
n,h := ËœOt
n,h, Xt
n,h := Ëœ
Xt
n,h.
15:
for r = 1 â†’round do
â–·Make model caching decision
16:
Randomly choose a BS n for caching adjustment.
17:
Compute the future expected gain âˆ†R for each possible
cached model switch based on Ot
n,h, Xt
n,h according to
Eq. (47).
18:
Find the best scheme for model switching (caching and
evicting) by solving a memory-constrained knapsack prob-
lem.
19:
Switch Update: Update download status Ot
n,h by Eq. (48).
20:
Switch Update: Update cache status Xt
n,h by Eq. (49).
21:
end for
22: end while
currently cached one up to the first submodel whose additional
components, relative to the cached submodels, cannot be fully
downloaded within a time slot.
Specifically, we first use the user request patterns from the
past âˆ†T P time slots to simulate user requests for the future
âˆ†T F time slots. In particular, the proportion of user requests
for model type m with home BS n over the past âˆ†T P time
slots can be denoted as:
f t
n,m =
Pi=t
i=tâˆ’âˆ†T P
P
uâˆˆU 1{mi
u = m âˆ§Ë†ni
u = n}
Pi=t
i=tâˆ’âˆ†T P U
.
(45)
Next, let Ï€t
n,m(Xt; Ot) âˆˆ{(hâ€², h) | Xt
n,hâ€² = 1, h âˆˆH(m)}
denote the action taken at time slot t for a cached submodel
hâ€² of model type m at BS n, under the observation of system
states Xt and Ot. When hâ€² â‰»h, it indicates a switch from
the cached submodel hâ€² to a smaller submodel h; conversely,
it indicates a switch to a larger submodel h or remaining
unchanged. Then, at time slot t, the expected future reward
of switching a cached submodel hâ€² of model m at BS n to a
submodel h, while keeping all other system states unchanged,
can be computed as:
R(Ï€t
n,m = (hâ€², h)) =
âˆ†T F
X
tâ€²=t+1
X
nâ€²âˆˆN
Î³(tâ€²âˆ’t) Â· f tâ€²
nâ€²,m
Â· Qtâ€²
m
 nâ€², ntâ€²âˆ—
m (nâ€²)

,
(46)
where Î³t denotes the discount factor of the userâ€™s QoE in the
tth future time slot.


--- Page 10 ---
10
Therefore, the expected future gain from replacing the
cached submodel hâ€² of model m at BS n with h can be
computed as:
âˆ†R(Ï€t
n,m = (hâ€², h)) = R(Ï€t
n,m = (hâ€², h))
âˆ’R(Ï€t
n,m = (hâ€², hâ€²)).
(47)
Finally, after making the caching decision at time slot t, if it
is decided to switch from the cached submodel hâ€² at BS n to
a larger target submodel Ë†ht, the download status of submodel
h can be computed as:
Ot
n,h =
ï£±
ï£²
ï£³
âˆ†rh,
if ËœOt
n,h = 0 âˆ§P
hâ€²â‰ºh
ËœXt
n,hâ€² = 1 âˆ§h âª¯Ë†ht,
ËœOt
n,h,
otherwise,
(48)
where âˆ†rh represents the additional data size that each
submodel h between the cached submodel hâ€² and the target
submodel Ë†ht needs to download relative to its preceding
submodel. Otherwise, if the decision is to switch from the
cached submodel hâ€² at BS n to a smaller target submodel Ë†ht,
the modelâ€™s cache state can be updated directly as the time
required for cache eviction is negligible:
Xt
n,h =
ï£±
ï£´
ï£²
ï£´
ï£³
1,
if h = Ë†ht,
0,
if h = hâ€²,
ËœXt
n,h,
otherwise.
(49)
The procedure of the CoCaR-OL algorithm is shown in Alg.
2. At the beginning of each time slot t, the download and cache
states of each submodel, ËœOt
n,h and ËœXt
n,h, are updated through
a routine process (Lines 5-6). Then, the maximum QoE for
each user during the current time slot is calculated, and the
historical request frequency f t
n,m of each model m at each BS
n is updated (Lines 7-13). In each iteration, a BS n is randomly
selected, and all submodels of models that are not being down-
loaded at BS n are traversed. For each candidate submodel, it
is hypothetically enlarged, and a knapsack problem is solved
under the constraint of the remaining memory capacity of BS
n. This step selects a combination of cached submodels from
other candidate models and their smaller submodels, such that
the expected future gain of cache switching is maximized
(Lines 16-18). The same process is then repeated for other
candidate submodels. Finally, the model switching scheme that
yields the maximum expected future gain is chosen from all
the options, and the corresponding model states Ot
n,h and Xt
n,h
are updated accordingly (Lines 19-20).
VI. PERFORMANCE EVALUATION
In this section, we conduct simulation experiments to
evaluate the proposed CoCaR algorithmâ€™s performance after
converting its solution into feasible ones (Sec. IV-D) as well
as the performance of the CoCaR-OL algorithm in online
scenarios.
A. Evaluation Setup
Following a setup similar to [22] and [25], the duration of
each observation window is set to âˆ†Ï„ = 3s and the number
of windows |Î“| = 10, i.e., the total time is 30s. We consider
TABLE II
ATTRIBUTES OF THE THREE VIT SUBMODELS
Submodel
Memory (MB)
FLOPs (GFlops)
Precision
1
174.32
5.70
0.8417
2
227.42
7.56
0.9413
3
342.05
11.29
0.9894
TABLE III
LOADING TIME OF THE THREE VIT SUBMODELS (S)
Original
Submodels
Final Submodels
1
2
3
/
0.68860
0.87696
1.05821
1
0.00000
0.24794
0.46098
2
0.04238
0.00000
0.25082
3
0.04725
0.04242
0.00000
an MEC network with N = 5 BSs and U = 600 users in each
window. The coverage range of a BS is 150 m, and the connec-
tion between BSs is established using the ErdËosâ€“RÃ©nyi random
graph model [42]. The wireless uplink transmission rate is set
to Ï†Ë†nu = 20 Mbps, and the transmission rate between BSs
is set to rË†nun = 100 Mbps. The bandwidth between the cloud
and the base station is set to Wn = 800 Mbps. The propagation
time for one hop is set to 0.01s. We set the memory capacity
for each BS n to Rn = 500MB and the maximum computing
power Cn = 70 Gflops/s.
We use M = 8 different DNN model types (e.g., ViT,
swintransformer [43], etc.), with each model type having
three submodels. Each submodel is trained and tested on the
CIFAR-10 dataset [44]. Taking ViT as an example. Tables
II and III show the attributes and loading times of the ViT
submodels, respectively. Table II shows the differences in
memory consumption and inference precision among the ViT
submodels. Table III indicates that loading submodel 2 directly
from the secondary storage takes 0.877s, whereas switching
from submodel 1 to submodel 2 takes only 0.248s, thereby
increasing the valid service time of ViT. The popularity of
model types follows a Zipf distribution with a skewness
coefficient of 0.8 [45], and each user generates a request
for a random model type. The data size of each request and
model input is du = dm = 0.144 MB. The maximum tolerable
latency for the user is set to 0.3s. We implemented the CoCaR
using Python 3.10.14 and conducted all experiments on an
Intel(R) Xeon(R) Gold 6230R CPU @ 2.10 GHz with eight
NVIDIA GeForce RTX 3070 GPUs.
B. Benchmarks and Evaluation Metrics
We compare CoCaR with the following algorithms.
â€¢ Linear-Relaxation (LR). The optimal fractional solution
of problem P1-LR, obtained via a linear programming
solver, serves as an upper bound for the optimal integer
solution but is typically unattainable in practice.
â€¢ SPR3 [22]. It uses random rounding to solve the joint ser-
vice caching and request routing problem under memory,
computation, and communication constraints.


--- Page 11 ---
11
100
200
300
400
500
Memory Capacity (MB)
0.0
0.2
0.4
0.6
0.8
1.0
Average Precision
(a)
100
200
300
400
500
Memory Capacity (MB)
0.0
0.2
0.4
0.6
0.8
1.0
Average Hit Rate
(b)
LR
CoCaR
Greedy
Random
SPR3
GatMARL
Fig. 6.
Impact of different BS memory capacities: (a) Average inference
precision; (b) Average hit rate.
1
2
4
5
10
20
Popularity Change Frequency
0.0
0.2
0.4
0.6
0.8
1.0
Average Precision
(a)
1
2
4
5
10
20
Popularity Change Frequency
0.0
0.2
0.4
0.6
0.8
1.0
Average Hit Rate
(b)
LR
CoCaR
Greedy
Random
SPR3
GatMARL
Fig. 7.
Impact of popularity change frequency: (a) Average inference
precision; (b) Average hit rate. The x-axis indicates how many observation
windows the popularity changes once.
â€¢ GatMARL [46]. GatMARL constructs the MEC environ-
ment as an undirected graph, applying a graph attention-
based multi-agent reinforcement learning algorithm to
learn task offloading and service caching policies.
â€¢ Greedy. The model type is selected based on popularity,
and one of its submodels is cached in descending order
of precision. User requests are routed to the home BS.
â€¢ Random. The models cached at each BS are randomly
selected from the submodels associated with each model
type. User requests are randomly routed to a BS.
Note that SPR3 and GatMARL only cache the complete
service, and all benchmarks except LR ignore the impact of
model loading time on decision-making.
Our evaluation is based on the following metrics:
â€¢ Average inference precision. It reflects the average
inference precision for all user requests over all windows.
â€¢ Average QoE of Users. It reflects the average QoE for all
user requests under the precision and latency tradeoff over
all time slots, which is calculated according to Eq. (40)
in Sec. V-A.
â€¢ Average hit rate. It reflects the ratio of requests served
by the BS over all windows.
â€¢ Average memory utilization. It reflects the memory
usage ratio of cached models at the BS over all windows.
0
0.2
0.4
0.6
0.8
1
Zipf Skewness Coefficient
0.0
0.2
0.4
0.6
0.8
1.0
Average Precision
(a)
0
0.2
0.4
0.6
0.8
1
Zipf Skewness Coefficient
0.0
0.2
0.4
0.6
0.8
1.0
Average Hit Rate
(b)
LR
CoCaR
Greedy
Random
SPR3
GatMARL
Fig. 8. Impact of different Zipf skewness coefficients:(a) Average inference
precision; (b) Average hit rate.
C. Evaluation Results and Analysis of CoCaR
Impact of BS Memory Capacity. Fig. 6 shows the results
of average inference precision and average hit rate as the
BS memory capacity varies. As the BS memory capacity
increases from 100 MB to 500 MB, the performance of all
methods improves. This is because a larger memory capacity
allows BSs to cache more models, thereby satisfying more
user requests. When R = 500 MB, CoCaRâ€™s average inference
precision gap with LR is only 8.1%, outperforming other
baselines by at least 46.5%. Therefore, CoCaR, with its
superior decision-making mechanism, can quickly adapt to
changes in memory resources, consistently outperform other
baselines, and steadily converge to the optimal LR algorithm.
Impact of Popularity Change Frequency. In this experi-
ment, we set the number of observation windows to Î“ = 20
and configured the other parameters to their default values
(Sec. VI-A). Fig. 7 shows how often the model popularity
changes, i.e., the number of observation windows in which
the DNN modelsâ€™ popularity changes once, affecting the al-
gorithmsâ€™ performance. CoCaR consistently achieves the best
results after LR, owing to its ability to leverage BS caching
results from the previous observation window and balance the
impact of model loading time by switching between dynamic
DNN submodels, thereby quickly caching DNN models and
adapting to environmental changes. In contrast, the other
baselines make independent decisions for each observation
window, resulting in more fluctuating results in response to
popularity changes. Even when popularity changes in every
window, CoCaRâ€™s average precision exceeds other baselines
by more than 51.1%, second only to the LR, while maintaining
the highest hit rate.
Impact of Zipf Skewness Coefficient. Fig. 8 illustrates the
performance of algorithms across Zipf skewness coefficients
ranging from 0 to 1. CoCaR consistently achieves higher
average precision and hit rate as Zipf skewness increases, with
slight fluctuations due to random rounding. Additionally, since
a higher coefficient results in more frequent requests for a few
highly popular model types, the upward trend in the greedy
algorithm becomes more evident. When the Zipf skewness
coefficient is zero, the popularity vector is uniform, indicating
that user requests are completely randomized. Despite this,
CoCaR achieves favourable results by leveraging a multi-


--- Page 12 ---
12
1
2
3
4
5
6
7
Observation Window Duration (s)
0.0
0.2
0.4
0.6
0.8
1.0
Average Precision
(a)
1
2
3
4
5
6
7
Observation Window Duration (s)
0.0
0.2
0.4
0.6
0.8
1.0
Average Hit Rate
(b)
LR
CoCaR
Greedy
Random
SPR3
GatMARL
Fig. 9. Impact of different observation window duration: (a) Average inference
precision; (b) Average hit rate.
100
200
300
400
500
(a) Memory Capacity (MB)
0.0
0.5
1.0
1
2
4
5
10
20
(b) Popularity Change Frequency
0.0
0.5
1.0
0
0.2
0.4
0.6
0.8
1
(c) Zipf Skewness Coefficient
0.0
0.5
1.0
1
2
3
4
5
6
7
(d) Observation Window Duration (s)
0.0
0.5
1.0
Average Memory Utilization
CoCaR
Greedy
Random
SPR3
GatMARL
Fig. 10. Average BS memory resource utilization.
edge collaboration mechanism and quickly adjusting caching
schemes via submodel switching to serve more users.
Impact of Observation Window Duration. Fig. 9 shows
the impact on algorithm performance as the observation win-
dow duration is varied, while keeping the other default settings
constant (Sec. VI-A). According to the parameter settings in
Sec. VI-A, given the observation window duration âˆ†Ï„ = 1s,
there are Î“ = 30 windows, each with U = 200 users. LR,
CoCaR, and SPR3 exhibit an initial increase followed by a
decrease, achieving optimal performance at a duration of 3s.
This is due to the imbalance between model loading time and
observation window duration. An window that is too short sig-
nificantly increases the time spent on loading models, thereby
reducing the valid model service time, inference precision, and
hit rate. Conversely, an overly long window may prevent the
algorithm from promptly adapting to changing user requests,
thus affecting performance. Additionally, fixed hyperparame-
ters, such as the number of users during GarMARL training,
limit its decision-making in this experiment.
BS Resource Utilization. Fig. 10 shows the average BS
resource utilization of all algorithms under various influencing
factors. Combining the experimental analysis above, CoCaR
demonstrates a more effective, finer-grained utilization of BS
resources, with each unit of resource usage resulting in higher
0
20
40
60
80
100
Time Slot
0
20
40
60
80
100
120
Count of User Requests (BS 4)
Model 0
Model 1
Model 2
Model 3
Model 4
Model 5
Model 6
Model 7
Fig. 11. Count of user requests for DNN models at BS 4 in each time.
average inference precision and hit rate, thereby improving
overall service quality and user coverage. Specifically, under
the experimental setting in Sec. VI-A, CoCaR can utilize
at least 86% of BS memory resources. For the remaining
algorithms, the coarse-grained caching scheme and the neglect
of model loading time result in fewer cached DNN models and
shorter valid model service time for the same BS resource
utilization, ultimately leading to decreased algorithm perfor-
mance.
D. Evaluation Results and Analysis of CoCaR-OL
Extended Settings. We set the duration of each time slot to
âˆ†t = 0.5s, with a total of T = 100 time slots. In each time
slot, round = 3 BSs are randomly selected, and a caching
decision is made based on the past âˆ†T P = 10 user requests
and the expected future gain in the next âˆ†T F = 5 time slots.
The smoothing factor Î± and the discount factor Î³ are both set
to 0.9. In each time slot, U = 600 users simultaneously initiate
requests for model type M. The popularity of DNN models at
each BS is changed every 20 time slots, with a warm-up phase
starting 5 time slots earlier to gradually adjust the popularity.
Other parameters are the same as those defined in Sec. VI-A.
Fig. 11 illustrates the distribution of user requests at BS 4
under the above experimental setup.
In the online scenario, we introduce the following online
caching algorithms for comparison:
â€¢ LFU [47]. In each time slot, round BSs are randomly
and independently selected. For each selected BS, model
request frequencies over the past âˆ†T P time slots at
the BS and its one-hop neighbors are computed. The
cached submodel of the most frequent model is enlarged,
while the cached submodel of the least frequent model is
gradually reduced until the memory constraint is met.
â€¢ LFU-MAD [48]. It accounts for delayed hits in caching
and adjusts the strategy by ranking content. In our exper-
iment, ranks are based on weighted request frequency,
with higher weights to more recent user requests. The
caching decision is similar to the LFU in this paper.


--- Page 13 ---
13
100
300
500
700
900
Memory Capacity (MB)
0.0
0.2
0.4
0.6
0.8
Average QoE of Users
CoCaR-OL w/ Partition
CoCaR-OL w/o Partition
LFU-MAD w/ Partition
LFU-MAD w/o Partition
LFU w/ Partition
LFU w/o Partition
Random w/ Partition
Random w/o Partition
Fig. 12. Effect of BS Memory Capacity on the average userâ€™s QoE.
100
300
500
700
900
Memory Capacity (MB)
0.0
0.2
0.4
0.6
0.8
1.0
Average Hit Rate
CoCaR-OL w/ Partition
CoCaR-OL w/o Partition
LFU-MAD w/ Partition
LFU-MAD w/o Partition
LFU w/ Partition
LFU w/o Partition
Random w/ Partition
Random w/o Partition
Fig. 13. Effect of BS Memory Capacity on the cache hit rate.
â€¢ Random. In each timeslot, round BSs are randomly
selected. At each selected BS, a cached submodel is
randomly chosen and enlarged, and a combination of the
remaining submodels that satisfies the memory constraint
is then randomly selected for caching.
In our evaluation, for the above algorithms, only submodels
of models that are not currently being downloaded can be
switched, and the memory occupied by downloading submod-
els is considered in each decision. Additionally, we considered
versions of CoCaR-OL and the above baselines without the
dynamic DNN switching mechanism (i.e., model partitioning),
meaning that either the model is not cached at all or the
complete, original model is cached.
Effect of BS Memory Capacity. We evaluated the per-
formance of each algorithm with cache sizes ranging from
100MB to 500 MB. As shown in Figs. 12 and 13, when the
cache is small (e.g., 100 MB), only the smallest submodel
of each model can be cached, resulting in low user QoE
and cache hit rates across all algorithms. As the cache size
increases, larger submodels can be cached, providing better
service to more users, and hence improving QoE and hit rate
for all algorithms. Moreover, algorithms with dynamic DNN
10
20
25
33
50
100
Popularity Change Frequency
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Average QoE of Users
CoCaR-OL w/ Partition
CoCaR-OL w/o Partition
LFU-MAD w/ Partition
LFU-MAD w/o Partition
LFU w/ Partition
LFU w/o Partition
Random w/ Partition
Random w/o Partition
Fig. 14. Effect of popularity change frequency on the average userâ€™s QoE.
10
20
25
33
50
100
Popularity Change Frequency
0.0
0.2
0.4
0.6
0.8
1.0
Average Hit Rate
CoCaR-OL w/ Partition
CoCaR-OL w/o Partition
LFU-MAD w/ Partition
LFU-MAD w/o Partition
LFU w/ Partition
LFU w/o Partition
Random w/ Partition
Random w/o Partition
Fig. 15. Effect of popularity change frequency on the cache hit rate.
submodel switching consistently outperform their counterparts
that cache only the full original models. For instance, with
a cache size of 500 MB, CoCaR-OL with model partitioning
achieves a 32.3% higher user QoE than LFU-MAD with
partitioning, and a 36.5% higher QoE than CoCaR-OL without
partitioning. This improvement stems from our dynamic DNN-
based caching mechanism, which continues serving users with
current submodels while new submodels are being down-
loaded. These results demonstrate that the dynamic DNN
caching mechanism can adapt to different caching schemes,
leveraging fine-grained memory usage and enabling flexible,
rapid model switching to enhance user service.
Effect of Popularity Change Frequency. Figs. 14 and 15
illustrate the impact of the frequency of DNN model popularity
changes, with the popularity changing from every 10 time slots
to every 100 slots. Overall, the userâ€™s QoE and cache hit rate of
all algorithms gradually increase as the frequency of popularity
changes decreases. Furthermore, since the proposed CoCaR-
OL algorithm considers the impact of each caching decision
on the global expected future gain, while others rely solely on
partial information, it exhibits greater stability and consistently
outperforms other algorithms regardless of popularity change


--- Page 14 ---
14
0.0
0.4
0.8
1.2
1.6
Zipf Skewness Coefficient
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Average QoE of Users
CoCaR-OL w/ Partition
CoCaR-OL w/o Partition
LFU-MAD w/ Partition
LFU-MAD w/o Partition
LFU w/ Partition
LFU w/o Partition
Random w/ Partition
Random w/o Partition
Fig. 16. Effect of the Zipf skewness coefficient on the average userâ€™s QoE.
0.0
0.4
0.8
1.2
1.6
Zipf Skewness Coefficient
0.0
0.2
0.4
0.6
0.8
1.0
Average Hit Rate
CoCaR-OL w/ Partition
CoCaR-OL w/o Partition
LFU-MAD w/ Partition
LFU-MAD w/o Partition
LFU w/ Partition
LFU w/o Partition
Random w/ Partition
Random w/o Partition
Fig. 17. Effect of the Zipf skewness coefficient on the cache hit rate.
frequency. When popularity changes every 10 time slots,
LFU with model partitioning achieves 3.3% and 3.2% lower
user QoE than LFU-MAD and Random, respectively. This
is because LFU computes request frequencies over the past
10 time slots, so when popularity shifts abruptly, its caching
decisions lag behind the new distribution. In contrast, LFU-
MAD gives more weight to recent requests, enabling faster
adaptation to local popularity changes.
Effect of the Zipf Skewness Coefficient. The impact of the
Zipf skewness coefficient on algorithm performance is shown
in Figs. 16 and 17. As the skewness coefficient increases, the
proposed CoCaR-OL consistently leverages global information
to generate high-quality caching decisions, maintaining stable
performance. In contrast, LFU and LFU-MAD with model
partitioning exhibit much weaker performance. This is because
the LFU strategy relies exclusively on frequency for its deci-
sions, disregarding other critical factors such as model size and
precision. These limitations further underscore the advantages
of our gain-oriented CoCaR-OL in balancing precision and
latency. When the Zipf coefficient is zero, popular models
change randomly, and LFU tends to switch to caching smaller
submodels of different models to provide some services. As
the Zipf coefficient increases to 0.4, LFU gradually caches the
largest submodels of popular models, but the requests for these
models may not greatly exceed those for less popular models,
leading to overall performance degradation. This degradation
is mitigated by a further increase in the coefficient, ultimately
resulting in improved performance.
VII. RELATED WORK
Caching & Routing in MEC. In recent years, mobile
edge computing (MEC) has received increasing attention in
networking. Since service caching and request routing are
critical in MEC, jointly optimizing them to guarantee usersâ€™
QoE remains a major challenge. Poularakis et al. [22] studied
the joint optimization problem of service placement and re-
quest routing with multidimensional constraints and proposed
an algorithm to achieve near-optimal performance. Yao et al.
[25] proposed a graph attention-based intelligent cooperative
task offloading and service caching scheme to maximize the
utility of quality of service-based systems. Zhang et al. [23]
jointly considered service caching, computation offloading,
transmission, and computation resource allocation to minimize
the overall computation and latency costs for all users. Yao
et al. [33] aimed to maximize throughput under budget,
accuracy, and latency constraints by jointly optimizing model
caching and request routing. Chu et al. [19] investigated
how to maximize user QoE in MEC-based networks by
jointly optimizing service caching, resource allocation, and
task offloading decisions. Zhao et al. [17] investigated how
to efficiently offload dependent tasks to edge nodes with
limited service caches and designed an efficient algorithm
based on convex programming to solve this problem. Bi et
al. [49] consider the problem of joint optimization of service
cache locations, computation offloading decisions, and system
resource allocation on a single edge server that assists mobile
users in performing a series of computational tasks. However,
these studies focus on caching complete services at BSs, which
limits the effective utilization of BSsâ€™ resources. Additionally,
they neglect the impact of service loading time on caching
and routing decision-making. In practice, user requests have
dynamically changing patterns and significantly impact usersâ€™
QoE, and hence must be addressed.
Dynamic DNNs. Dynamic DNNs can meet various require-
ments under different resource constraints by adjusting the
number of channels and layers. MutualNet [50] and MSDNet
[51] achieve adaptive precision-latency tradeoffs by varying
DNN network width and depth, respectively. Dynamic-OFA
[28] provides an efficient architecture for GPUs and CPUs by
using a shared backbone model that adjusts DNN width, depth,
filter sizes, and input resolution. SubFlow [52] dynamically
builds and executes DNN sub-networks for flexible time-
bound inference and training. Smart-MOMEPS [27] leverages
a multi-exit mechanism and dynamic service migration to al-
leviate DNN performance degradation. SN-Net [53] generates
numerous new stitched networks by combining pre-trained
models from a model family, enabling adaptation to different
resource constraints. ESTA [54], on the other hand, uses effi-
cient parameter fine-tuning and training-time gradient statistics


--- Page 15 ---
15
to assemble stitched models of various sizes from existing pre-
trained DNNs, achieving a stable balance between precision
and efficiency. Different from existing studies, we introduce
dynamic DNNs into edge caching by dividing a complete
DNN into multiple interrelated and switchable submodels.
This approach provides a novel solution for fine-grained,
resource-efficient, and flexible edge-based model caching.
VIII. CONCLUSION AND FUTURE WORK
In this paper, we study the joint dynamic model caching
and request routing optimization problem in MEC networks,
aiming to maximize the total inference precision of user re-
quests. Then, we perform multiple equivalent transformations
and relaxation for the joint optimization problem and propose
a novel random rounding and LP-based algorithm, CoCaR, to
solve it. Moreover, to adapt to the online scenario where user
requests are difficult to predict, we further propose CoCaR-
OL as an extension of CoCaR. Theoretical analysis and
experimental results show that the proposed CoCaR achieves
near-optimal performance and CoCaR-OL outperforms other
benchmark algorithms in online settings. Therefore, our pro-
posed dynamic DNN-based caching mechanism effectively
exploits memory resources in a fine-grained manner, flexibly
adjusting caching strategies to enhance service delivery for
users. In the future, we will further explore BSsâ€™ resource
allocation and distributed decision-making scheme in the joint
model caching and request routing problem.
ACKNOWLEDGMENTS
The authors also thank the Big Data Computing Center of
Southeast University for providing the experiment environ-
ment and computing facility. The authors are also grateful to
SF Technology Co., Ltd. for their valuable discussions and
technical support.
REFERENCES
[1] S. Qiu, F. Dong, S. Tan, D. Shen, R. Zhou, and Q. Fan, â€œCoCaR:
Enabling efficient dynamic DNN-based model caching and request
routing in MEC,â€ in proc. IEEE INFOCOM, London, United Kingdom,
May, 2025, pp. 1â€“10.
[2] X. Hu, L. Liang, S. Li, L. Deng, P. Zuo, Y. Ji, X. Xie, Y. Ding, C. Liu,
T. Sherwood et al., â€œDeepSniffer: A DNN model extraction framework
based on learning architectural hints,â€ in proc. ACM 25th ASPLOS, New
York, USA, Mar, 2020, pp. 385â€“399.
[3] T. Wang, L. Shen, Q. Fan, T. Xu, T. Liu, and H. Xiong, â€œJoint
admission control and resource allocation of virtual network embedding
via hierarchical deep reinforcement learning,â€ IEEE Transactions on
Services Computing, vol. 17, no. 03, pp. 1001â€“1015, 2024.
[4] Z. Wang, R. Zhang, S. Zhang, W. Cheng, W. Wang, and Y. Cui, â€œEdge-
assisted adaptive configuration for serverless-based video analytics,â€
IEEE Transactions on Networking, vol. 33, no. 3, pp. 1144â€“1159, 2025.
[5] A. Krizhevsky, I. Sutskever, and G. E. Hinton, â€œImagenet classification
with deep convolutional neural networks,â€ Communications of the ACM,
vol. 60, no. 6, pp. 84â€“90, 2017.
[6] G. Saon, Z. TÃ¼ske, D. Bolanos, and B. Kingsbury, â€œAdvancing RNN
transducer technology for speech recognition,â€ in proc. IEEE ICASSP,
Toronto, Canada, Jun, 2021, pp. 5654â€“5658.
[7] Z. Cao, S. Xu, H. Peng, D. Yang, and R. Zidek, â€œConfidence-aware
reinforcement learning for self-driving cars,â€ IEEE Transactions on
Intelligent Transportation Systems, vol. 23, no. 7, pp. 7419â€“7430, 2021.
[8] Z. Ning, H. Hu, X. Wang, L. Guo, S. Guo, G. Wang, and X. Gao,
â€œMobile edge computing and machine learning in the internet of
unmanned aerial vehicles: a survey,â€ ACM Computing Surveys, vol. 56,
no. 1, pp. 1â€“31, 2023.
[9] B. Huang, X. Liu, Y. Xiang, D. Yu, S. Deng, and S. Wang, â€œRein-
forcement learning for cost-effective IoT service caching at the edge,â€
Journal of Parallel and Distributed Computing, vol. 168, pp. 120â€“136,
2022.
[10] R. Zhang, R. Zhou, Y. Wang, H. Tan, and K. He, â€œIncentive mechanisms
for online task offloading with privacy-preserving in UAV-assisted mo-
bile edge computing,â€ IEEE/ACM Transactions on Networking, vol. 32,
no. 3, pp. 2646â€“2661, 2024.
[11] X. Wang, J. Ye, and J. C. Lui, â€œMean field graph based D2D collab-
oration and offloading pricing in mobile edge computing,â€ IEEE/ACM
Transactions on Networking, vol. 32, no. 1, pp. 491â€“505, 2023.
[12] T. Ren, Z. Hu, H. He, J. Niu, and X. Liu, â€œFEAT: Towards fast
environment-adaptive task offloading and power allocation in MEC,â€
in proc. IEEE INFOCOM, New York City, USA, May, 2023, pp. 1â€“10.
[13] Z. Xu, L. Zhou, S. C.-K. Chau, W. Liang, H. Dai, L. Chen, W. Xu,
Q. Xia, and P. Zhou, â€œNear-optimal and collaborative service caching in
mobile edge clouds,â€ IEEE Transactions on Mobile Computing, vol. 22,
no. 7, pp. 4070â€“4085, 2023.
[14] H. Tan, Y. Wang, C. Zhang, G. Li, H. Du, Z. Han, S. H.-C. Jiang, and
X.-Y. Li, â€œAsymptotically tight approximation for online file caching
with delayed hits and bypassing,â€ IEEE Transactions on Networking,
vol. 33, no. 4, pp. 1886â€“1899, 2025.
[15] R. Zhou, X. Wu, H. Tan, and R. Zhang, â€œTwo time-scale joint service
caching and task offloading for UAV-assisted mobile edge computing,â€
in proc. IEEE INFOCOM, London, United Kingdom, May, 2022, pp.
1189â€“1198.
[16] M. Li, Z. Han, C. Zhang, R. Zhou, Y. Liu, and H. Tan, â€œDynamic
resource allocation for deep learning clusters with separated compute
and storage,â€ in proc. IEEE INFOCOM, New York City, USA, May,
2023, pp. 1â€“10.
[17] G. Zhao, H. Xu, Y. Zhao, C. Qiao, and L. Huang, â€œOffloading tasks
with dependency and service caching in mobile edge computing,â€ IEEE
Transactions on Parallel and Distributed Systems, vol. 32, no. 11, pp.
2777â€“2792, 2021.
[18] L. Qin, H. Lu, Y. Lu, C. Zhang, and F. Wu, â€œJoint optimization of
base station clustering and service caching in user-centric MEC,â€ IEEE
Transactions on Mobile Computing, vol. 23, no. 5, pp. 6455â€“6469, 2023.
[19] W. Chu, X. Jia, Z. Yu, J. C. Lui, and Y. Lin, â€œJoint service caching, re-
source allocation and task offloading for MEC-based networks: A multi-
layer optimization approach,â€ IEEE Transactions on Mobile Computing,
vol. 23, no. 4, pp. 2958â€“2975, 2024.
[20] J. Ren, W. Zhang, H. Wang, D. Yang, S. Wang, H. Zhang, and S. Cui,
â€œToward deterministic wide-area networks via deadline-aware routing
and scheduling,â€ IEEE Transactions on Networking, vol. 33, no. 4, pp.
1762â€“1778, 2025.
[21] T. Wang, Q. Fan, C. Wang, L. Ding, N. J. Yuan, and H. Xiong,
â€œFlagVNE: A flexible and generalizable RL framework for network
resource allocation,â€ in proc. 33rd IJCAI, 2024.
[22] K. Poularakis, J. Llorca, A. M. Tulino, I. Taylor, and L. Tassiulas,
â€œService placement and request routing in MEC networks with storage,
computation, and communication constraints,â€ IEEE/ACM Transactions
on Networking, vol. 28, no. 3, pp. 1047â€“1060, 2020.
[23] G. Zhang, S. Zhang, W. Zhang, Z. Shen, and L. Wang, â€œJoint service
caching, computation offloading and resource allocation in mobile edge
computing systems,â€ IEEE Transactions on Wireless Communications,
vol. 20, no. 8, pp. 5288â€“5300, 2021.
[24] M. Tang and V. W. Wong, â€œDeep reinforcement learning for task
offloading in mobile edge computing systems,â€ IEEE Transactions on
Mobile Computing, vol. 21, no. 6, pp. 1985â€“1997, 2020.
[25] Z. Yao, S. Xia, Y. Li, and G. Wu, â€œCooperative task offloading and
service caching for digital twin edge networks: A graph attention multi-
agent reinforcement learning approach,â€ IEEE Journal on Selected Areas
in Communications, vol. 41, no. 11, pp. 3401â€“3413, 2023.
[26] K. Peng, L. Wang, J. He, C. Cai, and M. Hu, â€œJoint optimization of
service deployment and request routing for microservices in mobile edge
computing,â€ IEEE Transactions on Services Computing, vol. 17, no. 3,
pp. 1016â€“1028, 2024.
[27] P. Wang, T. Ouyang, G. Liao, J. Gong, S. Yu, and X. Chen, â€œEdge
intelligence in motion: Mobility-aware dynamic DNN inference service
migration with downtime in mobile edge computing,â€ Journal of Systems
Architecture, vol. 130, p. 102664, 2022.
[28] W. Lou, L. Xun, A. Sabet, J. Bi, J. Hare, and G. V. Merrett, â€œDynamic-
OFA: Runtime DNN architecture switching for performance scaling
on heterogeneous embedded platforms,â€ in proc. IEEE/CVF CVPR,
Nashville, USA, Jun, 2021, pp. 3110â€“3118.
[29] C. Singhal, Y. Wu, F. Malandrino, M. Levorato, and C. F. Chiasserini,
â€œDistributing inference tasks over interconnected systems through dy-


--- Page 16 ---
16
namic DNNs,â€ IEEE Transactions on Networking, vol. 33, no. 4, pp.
1717â€“1730, 2025.
[30] Z. Liu, H. Du, J. Lin, Z. Gao, L. Huang, S. Hosseinalipour, and
D. Niyato, â€œDnn partitioning, task offloading, and resource allocation
in dynamic vehicular networks: A lyapunov-guided diffusion-based rein-
forcement learning approach,â€ IEEE Transactions on Mobile Computing,
vol. 24, no. 3, pp. 1945â€“1962, 2025.
[31] H. Li, X. Li, Q. Fan, Q. He, X. Wang, and V. C. Leung, â€œDistributed dnn
inference with fine-grained model partitioning in mobile edge computing
networks,â€ IEEE Transactions on Mobile Computing, vol. 23, no. 10,
pp. 9060â€“9074, 2024.
[32] S. Qiu, Q. Fan, X. Li, X. Zhang, G. Min, and Y. Lyu, â€œOA-Cache:
Oracle approximation-based cache replacement at the network edge,â€
IEEE Transactions on Network and Service Management, vol. 20, no. 3,
pp. 3177â€“3189, 2023.
[33] M. Yao, L. Chen, Y. Wu, and J. Wu, â€œLoading cost-aware model caching
and request routing in edge-enabled wireless sensor networks,â€ The
Computer Journal, vol. 66, no. 10, pp. 2409â€“2425, 2023.
[34] M. Yao, L. Chen, J. Zhang, J. Huang, and J. Wu, â€œLoading cost-aware
model caching and request routing for cooperative edge inference,â€ in
proc. IEEE ICC, Seoul, Korea, Republic of, May, 2022, pp. 2327â€“2332.
[35] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, and N. Houlsby, â€œAn image is worth 16x16 words:
Transformers for image recognition at scale,â€ in proc. 9th ICLR. Virtual
Event: OpenReview.net, May, 2021.
[36] T. Ouyang, K. Zhao, G. Hong, X. Zhang, Z. Zhou, and X. Chen, â€œDy-
namic edge-centric resource provisioning for online and offline services
co-location via reactive and predictive approaches,â€ IEEE Transactions
on Networking, vol. 33, no. 3, pp. 1388â€“1403, 2025.
[37] L. Chen, C. Shen, P. Zhou, and J. Xu, â€œCollaborative service placement
for edge computing in dense small cell networks,â€ IEEE Transactions
on Mobile Computing, vol. 20, no. 2, pp. 377â€“390, 2019.
[38] Y. T. Lee and A. Sidford, â€œEfficient inverse maintenance and faster
algorithms for linear programming,â€ in proc. IEEE 56th FOCS, Berkeley,
USA, Oct, 2015, pp. 230â€“249.
[39] M. Mitzenmacher and E. Upfal, Probability and computing: Random-
ization and probabilistic techniques in algorithms and data analysis.
Cambridge university press, 2017.
[40] P. Wang, S. Li, Y. Han, F. Ye, and Q. Zhang, â€œFast-response edge caching
scheme for graph data,â€ IEEE Transactions on Networking, vol. 33,
no. 4, pp. 1962â€“1975, 2025.
[41] Z. Tang, F. Mou, J. Lou, W. Jia, Y. Wu, and W. Zhao, â€œMulti-user layer-
aware online container migration in edge-assisted vehicular networks,â€
IEEE/ACM Transactions on Networking, vol. 32, no. 2, pp. 1807â€“1822,
2023.
[42] P. ErdËos, A. RÃ©nyi et al., â€œOn the evolution of random graphs,â€ Publ.
math. inst. hung. acad. sci, vol. 5, no. 1, pp. 17â€“60, 1960.
[43] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,
â€œSwin Transformer: Hierarchical vision transformer using shifted win-
dows,â€ in proc. IEEE/CVF ICCV, Virtual Event, Oct, 2021, pp. 10 012â€“
10 022.
[44] A. Krizhevsky, G. Hinton et al., â€œLearning multiple layers of features
from tiny images,â€ Masterâ€™s thesis, Department of Computer Science,
University of Toronto, 2009.
[45] K. Shanmugam, N. Golrezaei, A. G. Dimakis, A. F. Molisch, and
G. Caire, â€œFemtoCaching: Wireless content delivery through distributed
caching helpers,â€ IEEE Transactions on Information Theory, vol. 59,
no. 12, pp. 8402â€“8413, 2013.
[46] Z. Yao, S. Xia, Y. Li, and G. Wu, â€œCooperative task offloading and
service caching for digital twin edge networks: A graph attention multi-
agent reinforcement learning approach,â€ IEEE Journal on Selected Areas
in Communications, vol. 41, no. 11, pp. 3401â€“3413, 2023.
[47] J. Dilley and M. Arlitt, â€œImproving proxy cache performance: Analysis
of three replacement policies,â€ IEEE Internet Computing, vol. 3, no. 6,
pp. 44â€“50, 2002.
[48] N. Atre, J. Sherry, W. Wang, and D. S. Berger, â€œCaching with delayed
hits,â€ in proc. ACM SIGCOMM, Virtual Event USA, Jul, 2020, pp. 495â€“
513.
[49] S. Bi, L. Huang, and Y. J. A. Zhang, â€œJoint optimization of service
caching placement and computation offloading in mobile edge comput-
ing systems,â€ IEEE Transactions on Wireless Communications, vol. 19,
no. 7, pp. 4947â€“4963, 2020.
[50] T. Yang, S. Zhu, C. Chen, S. Yan, M. Zhang, and A. Willis, â€œMutu-
alNet: Adaptive convnet via mutual learning from network width and
resolution,â€ in proc. ECCV.
Glasgow, UK: Springer, Aug, 2020, pp.
299â€“315.
[51] G. Huang, D. Chen, T. Li, F. Wu, L. van der Maaten, and K. Weinberger,
â€œMulti-scale dense networks for resource efficient image classification,â€
in proc. ICLR.
Vancouver, Canada: OpenReview.net, Apr, 2018.
[52] S. Lee and S. Nirjon, â€œSubflow: A dynamic induced-subgraph strategy
toward real-time DNN inference and training,â€ in proc. IEEE RTAS,
Sydney, Australia, Apr, 2020, pp. 15â€“29.
[53] Z. Pan, J. Cai, and B. Zhuang, â€œStitchable neural networks,â€ in proc.
IEEE/CVF CVPR, Vancouver, Canada, Jun, 2023, pp. 16 102â€“16 112.
[54] H. He, Z. Pan, J. Liu, J. Cai, and B. Zhuang, â€œEfficient stitchable task
adaptation,â€ in proc. IEEE/CVF CVPR, Seattle, USA, Jun, 2024, pp.
28 555â€“28 565.
