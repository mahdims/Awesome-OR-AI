--- Page 1 ---
ITADATA2024: The 3rd Italian Conference on Big Data and Data Science
From User Preferences to Optimization
Constraints Using Large Language Models
Manuela Sanguinetti[0000−0002−0147−2208]
, Alessandra Perniciano[0009−0003−8956−5058], Luca Zedda[0009−0001−8488−1612],
Andrea Loddo[0000−0002−6571−3816], Cecilia Di Ruberto[0000−0003−4641−0307],
and Maurizio Atzori[0000−0001−6112−7310]
University of Cagliari, Department of Mathematics and Computer Science, Cagliari,
Italy
{manuela.sanguinetti;alessandra.pernician;luca.zedda;andrea.loddo;dirubert;atzori}@unica.it
Abstract. This work explores using Large Language Models (LLMs) to
translate user preferences into energy optimization constraints for home
appliances. We describe a task where natural language user utterances
are converted into formal constraints for smart appliances, within the
broader context of a renewable energy community (REC) and in the
Italian scenario. We evaluate the effectiveness of various LLMs currently
available for Italian in translating these preferences resorting to clas-
sical zero-shot, one-shot, and few-shot learning settings, using a pilot
dataset of Italian user requests paired with corresponding formal con-
straint representation. Our contributions include establishing a baseline
performance for this task, publicly releasing the dataset and code for
further research, and providing insights on observed best practices and
limitations of LLMs in this particular domain.
Keywords: natural language · energy optimization · large language
models.
1
Introduction and Motivations
The use of conversational agents in home automation and energy monitoring can
be pivotal in household management, by making it easier for users to understand
and control their energy consumption. The usefulness of such tools becomes
even more evident in emerging contexts such as Renewable Energy Communi-
ties (RECs), in which members are also prosumers, i.e., both consumers and
producers of energy (typically from photovoltaic (PV) systems, sometimes also
associated with storage batteries), and can share their excess energy flexibly
with other members of the same community, thereby reducing the load from the
main grid and overall energy costs. In such a scenario, the functions of a natural
language-based interface can thus range from monitoring consumption and pro-
duction trends, both at the level of the individual prosumer and at the level of
the entire community, to generating advice, e.g., on the most appropriate times
to use certain appliances in such a way as to maximize self-consumption (i.e.,
arXiv:2503.21360v1  [cs.CL]  27 Mar 2025


--- Page 2 ---
2
the use of energy from alternative sources than the main grid). The latter in
particular can provide effective feedback when linked to an optimizer that can
maximize self-consumption and thus enhance community-level efficiency.
The work presented in this paper is part of a broader research project that
aims to create an energy management system capable of managing large volumes
of data related to both energy consumption and generation within a REC. In
this context, the usefulness of a conversational interface lies in the possibility
given to the user to consult this complex amount of data in an accessible and
immediate way, taking advantage of the potential provided by dialogue systems
(such as chatbots and virtual assistants). There are numerous challenges involved
in the development of such a tool; these include the need to generate natural
language messages from structured numerical data (relating to trends in energy
consumption or production) [4], or even the possibility of exploiting such data to
create user profiles and tailoring the content to the appropriate profile, in order
to deliver more effective customized guidance [19]. Concerning in particular the
optimization task, it is worth pointing out that user engagement and willingness
to follow such guidance can also be influenced by how closely the agent’s recom-
mendations actually reflect the individual user’s habits and preferences. Recent
user studies have revealed how the optimization needs of a REC might poorly
align with the actual needs of individual community members, especially when
it comes to using certain appliances [7,13]. Therefore, it is essential to incorpo-
rate these preferences directly into the optimization function. This allows the
optimizer to take such factors into account by using them as actual constraints,
be they related to indoor comfort or time schedules related to users’ individual
routines. The ability of the conversational agent to properly encode such kind of
information, and respond accordingly, is therefore essential for enhancing user’s
compliance with more resource-aware practices.
The integration of user preferences into energy management systems and
appliance scheduling processes has been considered in previous studies within
the field of operational research. This has been done either through the direct
collection of preferences via static surveys [9] or by considering user actions
as a form of feedback with respect to suggested proposals [18]. Little to no
attention has been drawn so far to the extraction of constraints dynamically
expressed by users through natural language. In fact, other frameworks have
proposed the use of Natural Language Processing (NLP) techniques to extract
relevant information from unstructured text regarding optimization problems,
though pertaining to different topics [16] (see Section 3). Our objective is in-
spired by the latter approach and it specifically aims to tackle a similar task
within the broader project scenario mentioned above, and specifically within the
Italian landscape. In particular, the main task we aim to address in our study
is that of converting natural language usage preferences, expressed directly by
users through the interaction with a conversational agent, into energy optimiza-
tion constraints for household appliances. To address this task, we explore the
potential offered by Large Language Models (LLMs) and in-context learning us-
ing a handcrafted pilot dataset of domain-specific user utterances and formal
ITADATA2024: The 3rd Italian Conference on Big Data and Data Science


--- Page 3 ---
3
constraint representation. We prompted several LLMs among those available for
Italian using classical zero-shot, one-shot and few-shot settings in order to assess
their usefulness in adequately handling the mentioned task. The contribution we
aim to propose is thus to provide an initial baseline on this task, also making
available the data and code used for the experiments, along with some insights
on recommended practices and approaches for effectively using LLMs in similar
contexts, including the limitations we observed in our experiments.
The remainder of this the paper thus offers an introductory overview of the
whole project along with a definition of the problem we intend to address in this
work and the relevant body of literature. It will then provide a description of
the data and the models used in our experiments and a discussion of the results
obtained.
2
Background and Problem Definition
As mentioned above, this work is part of a larger project that involves, among
other tasks, the development of a conversational tool to deliver energy feedback
to members of a REC in Italy. The conversational interface has been designed
to dynamically manage the various user requests, which can be related either to
their own consumption trends or to the availability of alternative energy from
their photovoltaic system or (if present) a battery storage system. Among the
functions provided for this agent is the one related precisely to requesting sug-
gestions on the best way to exploit one’s energy resources, so as to maximize
self-consumption. For this purpose the agent will then be connected to an ex-
ternal optimizer that aims to maximize the amount of shared energy within
the REC as a whole. A prototype of this optimizer has been developed and
described in Messilem et al. [11] and Moré et al. [12]. This work aims to recon-
cile energy community-level goals with those of individual members, using the
conversational interface to enable greater interaction between the user and the
system.
In light of the objectives and the usage scenario outlined above, we then
defined the task at hand, i.e., given a natural language utterance expressing
user’s preferences or needs with respect to the use of a household appliance,
the goal is to convert the user’s expressed preferences into energy optimization
constraints. The allowed constraints can be of two types: 1) time constraints,
aiming at expressing that the user wants the appliance to be running at a cer-
tain time; 2) temperature setting constraints, that aim to further model the
optimization module with preferences over the desired settings when the appli-
ance of interest is a heat pump for both air conditioning and water heating. The
current formulation assumes that the available input data comprises both the
full user utterance (provided as context) and the text span (or spans) express-
ing the user preferences. The intended output is thus a representation of the
given preferences in the form of time or temperature constraints. More formally,
given a user utterance of n words w U = {w1, ..., wn}, and a set of text spans
C = {c1, ..., ci}, where each span c ⊂U expresses a constraint in terms of usage
ITADATA2024: The 3rd Italian Conference on Big Data and Data Science


--- Page 4 ---
4
preference, the objective is to map each constraint into a formal constraint rep-
resentation FR(c), thus having FR(c) = map(c), ∀c ∈C. The problem can thus
be addressed as a classical sequence-to-sequence task, as it involves the conver-
sion of a given natural language input into a structured format representing the
constraints.
3
Related Work
Our research approach builds upon recent work aiming at converting natural
language descriptions into formal representations of optimization problems. The
main rationale that lies behind such line of research consists in enabling a larger
audience, possibly without any prior background on operational research, to
successfully address optimization problems that may regard various aspects of
everyday life. The primary contribution to this task can be found in the work by
Ramamonijson et al. [17], who developed a benchmark dataset in English con-
sisting of Linear Programming (LP) problems with textual descriptions of such
problems. The dataset includes 1101 problems from domains such as advertising,
investment, sales, manufacturing, science, and transportation.
In this framework, the objective and constraint functions are linear with
respect to variables in the LP problems, and constraints are always expressed
as inequalities. Three main representation formats have been designed to fully
map, with a two-stage approach, the linguistic description into the actual math-
ematical formulation, thus going through an XML-like Intermediate Represen-
tation (IR) and a canonical (tabular) form. As a result, each example has a text
description of the problem and is annotated with its corresponding IR, math
representation, and canonical formulation.
The dataset was then used as benchmark within the NL4Opt shared task,
that took place in 2022 at NeurIPS [16].1 The competition precisely consisted in
translating textual descriptions of optimization problems into Linear Program-
ming (LP) formulations. It was organized into two distinct sub-tasks: a sequence
labeling task aimed at identifying all the entities in the text that were relevant
for the optimization problem formulation (i.e., variables, objective function and
constraints) and a sequence-to-sequence task aimed at generating the mathemat-
ical problem formulation. Sub-task 2 in particular uses both the input linguistic
description along with the text spans identified in the previous sub-task to gener-
ate the problem formulation. The task organizers developed a separate baseline
for each sub-task, using XLM-RoBERTa-base for the sequence labeling step and
BART with a copy mechanism for generating the LP formulation, obtaining a
macro-averaged F1 = 0.906 in the former and a declaration-level mapping ac-
curacy of 0.610 in the latter. Once the competition closed, the organizers finally
investigated how ChatGPT would perform on the same data. They bypassed the
competition’s subtasks and directly asked ChatGPT (model GPT-3.5-turbo) to
generate the problem formulations from the given text descriptions, with human
1 https://github.com/nl4opt. The second edition of the competition has recently
been launched: https://nl4opt.github.io/neurips-2024/
ITADATA2024: The 3rd Italian Conference on Big Data and Data Science


--- Page 5 ---
5
experts assessing the output quality of the model on the test set. The model
reached a declaration-level accuracy of 0.927.
The best-performing system in Subtask 2 [6] obtained an accuracy of 0.899
with a “decode all-at-once” strategy; the textual input was enhanced surrounding
variable entities with XML-tags, and both objective and constraint declarations
were generated simultaneously using BART-large.
Li et al. [10] further extended the NL4Opt approach by expanding the dataset
with brand new descriptions that would include logic constraints (e.g., if A then B)
and equality constraints. To address the task, the authors adopted a three-stage
method that employs ChatGPT for variable identification, a fine-tuned GPT-3
model for constraint classification, and again ChatGPT for constraint gener-
ation. For stages 1 and 3 ChatGPT was prompted with a zero-shot setting.
Alternatively, PaLM 2 was also fine-tuned for all stages, though obtaining lower
results overall.
Although our work shares the same basic goal with those just described,
the specifics of our use case mean that our approach diverges significantly with
respect to several points: (1) the type of input expected is a single user utter-
ance, not the description of an optimization problem; (2) the task definition, at
least at this stage, involves the conversion of constraints only, whether they are
related to temperature or to the desired operation times of the appliance; and
(3) the representation of constraints, as described below, involves an abstract
formulation that does not conform with the canonical form of linear constraints.
4
Data and Annotation Scheme
The data sample used in these experiments comes from a larger domain-specific
corpus of user utterances and labeled intents, that was primarily used to train the
NLU module of the conversational agent introduced above.2 The overall corpus
consists of 157 utterances in Italian grouped into three intent categories, two
for monitoring energy consumption and PV production, and one for requesting
optimization advice from the agent.3 Out of the former category, some user
requests involve general questions on how best to use their appliances, as in
Example 1 below, while a pilot sub-sample (26 utterances) includes requests
where specific preferences, in terms of time ranges or temperature settings, are
expressed, as in Example 2:
(1)
quando mi conviene aumentare l’acqua al massimo?
EN: "When is it convenient for me to set the water heater to the maxi-
mum?"
2 The backbone of the agent has been implemented with RASA [2], and for the NLU
module the built-in Dual Intent and Entity Transformer (DIET) [3] classifier was
used.
3 The whole dataset in YAML format uset to train DIET is available here:
https://github.com/msang/nl-interface/tree/main/data
ITADATA2024: The 3rd Italian Conference on Big Data and Data Science


--- Page 6 ---
6
(2)
come risparmiare tenendo il climatizzatore sempre acceso?
EN: "How to save money by keeping the air conditioner always on?"
This pilot sub-sample of 26 utterances then underwent a two-step manual
annotation to identify and label the text spans within each utterance that express
such kind of preferences, and then to convert these preferences into possible
constraint representations, that can be further processed and fed to the external
optimization module.
The constraint annotation scheme considers the following parameters and
variables, some of which are derived directly from the pre-existing definitions of
the optimizer. In particular, we consider:
1. Indices and variables:
– T = {1, ..., t}: discrete set of intervals that constitute the time horizon T
– st ∈{0, 1}: binary variable indicating the state of the appliance at time
t
– ht ∈[Hmin, Hmax]: continuous variable representing the desired temper-
ature at time t, where Hmin and Hmax are the minimum and maximum
temperatures allowed by the system
2. User-specified parameters:
– userstart, userend: desired start and shutdown times of the appliance
(either both or only one of them can be expressed by the user)
– usertemp: temperature desired by the user
The intermediate constraint representation format involves first assigning the
value of the decision variable (be it st or ht), and then the value of t for which
that variable takes the assigned value.
Based on the recurring patterns observed in the pilot set of utterances, the
following conditions can be outlined:
– The user wants the household appliance to run all the time:
st = 1∀t
– The user wants the household appliance to be on/off at precise time intervals:
st = 1 ∨0∀userstart ≤t ≤userend
– The user wants the appliance to be turned on/off starting from a specified
time:
st = 1 ∨0∀t ≥userstart
– The user wants the appliance to be turned on/off until a specified time:
st = 1 ∨0∀t ≤userend
The same scheme can be applied to the temperature variable, where the value
assigned will be the one specified by the user, thus having ht = usertemp.
The two-step annotation pipeline along with an actual example from the
dataset are shown in Figure 1.
In the next sections we will describe how we used the annotated data with a
set of Large Language Models currently available for Italian.
ITADATA2024: The 3rd Italian Conference on Big Data and Data Science


--- Page 7 ---
7
Fig. 1. Annotation pipeline and example for the user utterance "ho bisogno che l’acqua
calda sia disponibile dalle 7 alle 8,30" (EN: "I need hot water to be available from 7:00
to 8:30.")
5
Models and Settings
Given the wide variety of tasks and application domains for which they can be
used, we decided to explore the capabilities of Large Language Models (LLMs) on
this specific task as well. In particular, we chose to use more recent instruction-
tuned models, which allow a description of the task to be provided within the
prompt fed to the models. This approach was taken to optimize model per-
formance in the absence of sufficient data for tackling standard fine-tuning ap-
proaches, as in our case. In addition, the scarcity of training data is compounded
by the limited availability of usable models of this type for the Italian language.
We thus preliminary selected for our experiments the following LLMs: Chat-
GPT (model GPT-3.5-turbo), LLaMAntino-2-UltraChat [1]4 and LLaMAntino-
3-ANITA [14] (fine-tuned versions of Llama2 and Llama3, respectively), and
Zefiro5, a fine-tuned model of Mistral [8]. For the former model we used the
online interface,6 while for the others we used the model checkpoints available
through the HuggingFace platform and using the Transformers library.
Prompt structure and settings Drawing inspiration from the approach followed
in Gangwar and Kani [6] (see Section 3), the input utterances included in each
prompt were labeled with XML tags that incorporate the text spans express-
ing users’ preferences. In order to guide the models towards the appropriate
constraint generation, the prompts were structured so as to include: i) a brief
introduction to the task; ii) the meaning of the XML tags; iii) the constraint
4 Due to hardware constraints, the smaller version – 7B parameters – was used.
5 https://huggingface.co/giux78/zefiro-7b-beta-ITA-v0.1
6 https://openai.com/chatgpt
ITADATA2024: The 3rd Italian Conference on Big Data and Data Science


--- Page 8 ---
8
representation format; iv) in the case of one and few-shot prompts, one or five
pairs of XML-tagged utterances and corresponding constraint formal represen-
tation; v) the input utterance. As mentioned above, we queried each LLM with
different prompts, with the aim of further enhancing the models’ performance.
We empolyed common one-shot and few-shot approaches, including additional
examples (up to 5 in the few-shot setting) within the prompt.
In order to limit randomness in the responses generated by the various mod-
els, while ensuring they select a diverse yet relevant set of tokens, for the open
models (thus excluding ChatGPT) we used the following combination of decod-
ing parameters: temperature = 0.1, top-k = 20, and top-p = 0.9. In addition, to
keep the responses short and concise, we set the maximum generation length to
30 tokens.7
Evaluation metrics Due to the specific nature of this task, standard text-to-text
generation metrics, such as BLEU, ROUGE or METEOR, were not deemed
suitable. To measure the closeness between the output generated by the various
models tested and the formal constraint representation described in the previous
section, we adopted two specific metrics. One is ChrF [15], which considers the
overlap of character n-grams between reference and generated constraints. The
measure is computed as follows:
ChrFβ = (1+β2)·ChrP·ChrR
ChrR+β2·ChrP
where ChrP and ChrR are character n-gram precision and recall averaged
over all n-grams, and β is a weight assigned to recall (with β = 1, precision and
recall have equal importance). Although the task at hand cannot be properly
defined as text-to-code, the adherence to a format with a specific set of symbols
and a precise syntax makes it somehow similar to the latter. Hence the choice
to use a metric that could faithfully measure such adherence in the generated
constraints. Recent studies also suggest that, namely in the context of text-to-
code generation, this measure is preferable to other standard metrics [5].
In addition to character matching, we also aimed to evaluate the accuracy
of the model in correctly identifying both the variables involved (i.e. st for us-
age timing preferences and ht for temperature preferences) and the conditions
expressing the time value t. To do this, we parsed the generated responses to
extract these two aspects and calculated the accuracy of the models with respect
to their correct generation. We thus computed accuracy separately for variables
and conditions:
AccV ariables = 1
N
N
X
i=1
V ariablesgen
i
V ariablesi
(3)
7 Both annotated data in JSONL format, prompts and Python code used for the ex-
periments are available at the following GitHub page: https://github.com/msang/
nl-interface/tree/main/nl2optim
ITADATA2024: The 3rd Italian Conference on Big Data and Data Science


--- Page 9 ---
9
AccConditions = 1
N
N
X
i=1
Conditionsgen
i
Conditionsi
(4)
where N is the number of utterances used for evaluation, V ariablesi and
Conditionsi represent the total number of variables and conditions, respec-
tively, in the constraint formulations for utterance i, and V ariablesgen
i
and
Conditionsgen
i
represent the number of variables and conditions correctly gen-
erated by the model for utterance i.
Next section reports and discusses the results obtained.
6
Results and Discussion
As mentioned above, four models available for Italian were tested with the vari-
ous prompts. However, of these, both LLaMAntino-2 and Zefiro returned incon-
sistent and noise-rich results. We therefore considered their outputs to be invalid
for evaluation and were discarded. On the other hand, the outputs generated by
ChatGPT and LLaMAntino-3 were evaluated using the metrics reported in the
previous section. The results obtained are shown in Table 1.
prompt
ChrF
AccV ariables AccConditions AccAvg
Cerbero
0s
43.0734
0.7381
0.1190
0.4286
1s
35.9382
0.2619
0
0.1310
fs
–
–
–
–
ChatGPT
0s
51.0562
0.7857
0.2143
0.5
1s
60.8065
0.7857
0.119
0.4524
fs
69.0289
0.7619
0.3571
0.5595
LLaMAntino-3-ANITA
0s
37.9288
0.5
0.0714
0.2857
1s
66.238
0.7222
0.2857
0.504
fs
74.5472
0.8571
0.4286
0.6429
Maestrale
0s
33.8217
0.2063
0.0238
0.1151
1s
59.0048
0.7619
0.3571
0.5595
fs
–
–
–
–
IT5
–
47.0815
0.4545
0.2
0.3273
Table 1. Results of the tested models with zero-shot (0s), one-shot (1s) and few-shot
(fs) prompts.
These results show the overall inadequacy of the tested LLMs to address this
task using only the in-context learning approach; given the particular nature
ITADATA2024: The 3rd Italian Conference on Big Data and Data Science


--- Page 10 ---
10
of the task, further supervised fine-tuning is therefore required. On the other
hand, going into more detail about the results with respect to individual metrics
and prompt settings, it is possible to see how ChatGPT obtains better results
than LLaMAntino-3 when only the task instructions and the input utterance are
provided with the preferences embedded within the XML tags. However, when
further examples are given, LLaMAntino-3 shows better performance in terms
of both overlapping char-grams and average accuracy. As for the accuracy in
generating both the variables and the conditions that determine the value of t,
it can be observed that both models have more problems generating conditions
correctly than variables. In the case of ChatGPT in particular, the addition of a
single annotation example reduced the model’s performance in generating condi-
tions, while leaving those related to variable generation unchanged. Performance
did not benefit from the additional context provided, suggesting that ChatGPT
may need more examples or different prompt optimization strategies to improve
significantly. In case of LLaMAntino-3, on the other hand, the model improves
incrementally – both in the generation of variables and conditions – as the num-
ber of examples in the prompt increases, showing, at least in this setting, greater
capabilities when provided with more context.
7
Conclusions
In this paper, we explored the capabilities of some LLMs available for Italian
in translating user preferences into energy optimization constraints for house-
hold appliances. Given the current scarcity of data, in this phase of the work
we preliminarily evaluated the effectiveness of these models using only prompt
engineering techniques. We gradually increased the number of examples pro-
vided to each model, analyzing the differences in performance at each step. The
results obtained clearly indicate that, for this specific task, a more in-depth
knowledge of the specific domain is required. This involves the use of supervised
fine-tuning techniques. As future work, we plan to further expand the dataset of
utterances expressing user preferences and annotate the related constraints; we
intend to do so replicating the two-step approach also adopted for developing
the pilot dataset, as also described in Section 4, thus first manually identifying
the text spans expressing the user preferences and then converting them into
formal constraint representations. Finally, concerning the evaluation part, a fur-
ther extension could be applied by adopting, once the model is effectively trained
and properly connected with the actual optimizer, measures to also evaluate the
functional correctness of the constraints produced. This approach could take
inspiration from metrics such as pass@k, which harks back to the practices of
human developers, who judge the correctness of code based on passing a series
of unit tests. This in particular would serve to highlight the effectiveness of the
task in relation to the specific application scenario of the overarching project.
Acknowledgments. This work has been developed within the framework of the
project e.INS- Ecosystem of Innovation for Next Generation Sardinia (cod. ECS 00000038)
ITADATA2024: The 3rd Italian Conference on Big Data and Data Science


--- Page 11 ---
11
funded by the Italian Ministry for Research and Education (MUR) under the National
Recovery and Resilience Plan (NRRP) - MISSION 4 COMPONENT 2, "From research
to business" INVESTMENT 1.5, "Creation and strengthening of Ecosystems of inno-
vation" and construction of "Territorial R&D Leaders". This work was also partially
funded under the National Recovery and Resilience Plan (NRRP) - Mission 4 Com-
ponent 2 Investment 1.3, Project code PE0000021, “Network 4 Energy Sustainable
Transition–NEST”.
References
1. Basile, P., Musacchio, E., Polignano, M., Siciliani, L., Fiameni, G., Semeraro, G.:
Llamantino: Llama 2 models for effective text generation in italian language (2023)
2. Bocklisch, T., Faulkner, J., Pawlowski, N., Nichol, A.: Rasa: Open source language
understanding and dialogue management. CoRR abs/1712.05181 (2017), http:
//arxiv.org/abs/1712.05181
3. Bunk, T., Varshneya, D., Vlasov, V., Nichol, A.: DIET: lightweight language under-
standing for dialogue systems. CoRR abs/2004.09936 (2020), https://arxiv.
org/abs/2004.09936
4. Conde-Clemente, P., Alonso, J.M., Trivino, G.: Toward automatic generation
of linguistic advice for saving energy at home. Soft Computing 22(2), 345–
359 (2018). https://doi.org/10.1007/s00500-016-2430-5, https://doi.org/
10.1007/s00500-016-2430-5
5. Evtikhiev, M., Bogomolov, E., Sokolov, Y., Bryksin, T.: Out of the bleu: How
should we assess quality of the code generation models? Journal of Systems and
Software 203, 111741 (2023). https://doi.org/10.1016/j.jss.2023.111741
6. Gangwar, N., Kani, N.: Highlighting named entities in input for auto-formulation
of optimization problems. In: Dubois, C., Kerber, M. (eds.) Intelligent Computer
Mathematics. pp. 130–141. Springer Nature Switzerland, Cham (2023)
7. Jensen, R.H., Teli, M., Jensen, S.B., Gram, M., Harboe Sørensen, M.: Design-
ing Eco-Feedback Systems for Communities: Interrogating a Techno-solutionist
Vision for Sustainable Communal Energy. In: 10th International Conference on
Communities & Technologies - Wicked Problems in the Age of Tech. pp. 245–
257. C&T ’21, Association for Computing Machinery, New York, NY, USA (2021).
https://doi.org/10.1145/3461564.3461581
8. Jiang, A.Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.S., de las Casas,
D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L.R., Lachaux,
M.A., Stock, P., Scao, T.L., Lavril, T., Wang, T., Lacroix, T., Sayed, W.E.: Mistral
7b (2023), https://arxiv.org/abs/2310.06825
9. Jin, X., Baker, K., Christensen, D., Isley, S.: Foresee: A user-centric home energy
management system for energy efficiency and demand response. Applied Energy
205, 1583–1595 (2017). https://doi.org/10.1016/j.apenergy.2017.08.166
10. Li, Q., Zhang, L., Mak-Hau, V.: Synthesizing mixed-integer linear programming
models from natural language descriptions (2023)
11. Messilem, M.A.M., Deplano, D., Franceschelli, M., Usai, E., Carli, R.: Distributed
optimization for networks of battery energy storage systems in energy communities
with shared energy incentives. In: International Conference on Automation Science
and Engineering. IEEE (2024)
12. Moré, J.J., Deplano, D., Pilloni, A., Pisano, A., Franceschelli, M.: Online coordi-
nation of bess and thermostatically control loads for shared energy optimization
ITADATA2024: The 3rd Italian Conference on Big Data and Data Science


--- Page 12 ---
12
in energy communities. In: International Conference on Automation Science and
Engineering. IEEE (2024)
13. Peña, E.G., Jensen, R.H.: The character of eco-feedback systems for energy com-
munities. In: 11th International Conference on Communities and Technologies. p.
203–214. C&ampT ’23, Association for Computing Machinery, New York, NY,
USA (2023). https://doi.org/10.1145/3593743.3593783
14. Polignano, M., Basile, P., Semeraro, G.: Advanced natural-based interaction for
the italian language: Llamantino-3-anita (2024)
15. Popovic, M.: chrf: character n-gram f-score for automatic mt evaluation. In:
Proceedings of the Tenth Workshop on Statistical Machine Translation (2015),
https://api.semanticscholar.org/CorpusID:15349458
16. Ramamonjison, R., Yu, T., Li, R., Li, H., Carenini, G., Ghaddar, B., He, S., Mosta-
jabdaveh, M., Banitalebi-Dehkordi, A., Zhou, Z., Zhang, Y.: Nl4opt competition:
Formulating optimization problems based on their natural language descriptions.
In: Proceedings of the NeurIPS 2022 Competitions Track. pp. 189–203 (2023)
17. Ramamonjison, R., Li, H., Yu, T., He, S., Rengan, V., Banitalebi-dehkordi, A.,
Zhou, Z., Zhang, Y.: Augmenting operations research with auto-formulation of op-
timization models from problem descriptions. In: Li, Y., Lazaridou, A. (eds.) Pro-
ceedings of the 2022 Conference on Empirical Methods in Natural Language Pro-
cessing: Industry Track. pp. 29–62. Association for Computational Linguistics, Abu
Dhabi, UAE (Dec 2022). https://doi.org/10.18653/v1/2022.emnlp-industry.
4
18. Shuvo, S.S., Yilmaz, Y.: Home energy recommendation system (hers): A deep
reinforcement learning method based on residents’ feedback and activity. IEEE
Transactions on Smart Grid 13(4), 2812–2821 (2022). https://doi.org/10.1109/
TSG.2022.3158814
19. Trivino, G., Sanchez-Valdes, D.: Generation of linguistic advices for saving en-
ergy: Architecture. In: Dediu, A.H., Magdalena, L., Martín-Vide, C. (eds.) Theory
and Practice of Natural Computing. pp. 83–94. Springer International Publishing,
Cham (2015)
ITADATA2024: The 3rd Italian Conference on Big Data and Data Science
