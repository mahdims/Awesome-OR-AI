--- Page 1 ---
Published as a conference paper at ICLR 2026
DASH:
DETERMINISTIC
ATTENTION
SCHEDUL-
ING FOR HIGH-THROUGHPUT REPRODUCIBLE LLM
TRAINING
Xinwei Qiang1,3, Hongmin Chen2, Shixuan Sun1‚àó, Jingwen Leng1, Xin Liu2, Minyi Guo1
1School of Computer Science, Shanghai Jiao Tong University
2ByteDance Seed, 3Zhiyuan College, Shanghai Jiao Tong University
1{qiangxinwei, sunshixuan, leng-jw, guo-my}@sjtu.edu.cn
2{chenhongmin.will, liuxin.ai}@bytedance.com
ABSTRACT
Determinism is indispensable for reproducibility in large language model (LLM)
training, yet it often exacts a steep performance cost. In widely used attention
implementations such as FlashAttention-3, the deterministic backward pass can
incur up to a 37.9% throughput reduction relative to its non-deterministic coun-
terpart, primarily because gradient accumulation operations must be serialized to
guarantee numerical consistency. This performance loss stems from suboptimal
scheduling of compute and gradient-reduction phases, leading to significant hard-
ware underutilization.
To address this challenge, we formulate the backward pass of deterministic at-
tention as a scheduling problem on a Directed Acyclic Graph (DAG) and derive
schedules that minimize the critical path length. Building on this formulation, we
present DASH(Deterministic Attention Scheduling for High-Throughput), which
encapsulates two complementary scheduling strategies: (i) Descending Q-Tile It-
eration, a reversed query-block traversal that shrinks pipeline stalls in causal atten-
tion, and (ii) Shift Scheduling, a theoretically optimal schedule within our DAG
model that reduces pipeline stalls for both full and causal masks.
Our empirical evaluations on NVIDIA H800 GPUs demonstrate that DASH nar-
rows the performance gap of deterministic attention. The proposed strategies im-
prove the throughput of the attention backward pass by up to 1.28√ó compared to
the baseline, significantly advancing the efficiency of reproducible LLM training.
Our code is open-sourced at https://github.com/SJTU-Liquid/
deterministic-FA3.
1
INTRODUCTION
The pursuit of consistent and verifiable outcomes is a cornerstone of rigorous scientific research and
large-scale engineering. In the domain of large language model (LLM) training (Wu et al., 2024),
where experiments span thousands of GPUs (Grattafiori et al., 2024; DeepSeek-AI et al., 2025)
and incur enormous costs, this principle of reproducibility becomes indispensable. Reproducibility
empowers practitioners to diagnose training instabilities, such as loss divergence, and to evaluate
the impact of architectural modifications. Consequently, deterministic training, which guarantees
bitwise identical results across runs, is increasingly adopted as a standard practice for industry.
The origin of the non-determinism in attention of LLM training can be traced back to a fundamental
yet often overlooked characteristic of computer arithmetic: the non-associativity of floating-point
(FP) operations (Villa et al., 2009). For instance, (108 + 10‚àí6) ‚àí108 evaluates to 0.0 in single-
precision, whereas 108 ‚àí108 + 10‚àí6 yields the correct 10‚àí6. This sensitivity is magnified in the
massively parallel environment of GPUs (Shanmugavelu et al., 2024).
‚àóShixuan Sun is the corresponding author.
1
arXiv:2601.21824v1  [cs.LG]  29 Jan 2026


--- Page 2 ---
Published as a conference paper at ICLR 2026
Local Reduction
Global Reduction
(Inter SMs)
Global Reduction
(Intra  SM)
KV0 KV1 KV2 KV3 KV4 KV5 KV6 KV7
Computation of FlashAttention
Backward
ùëÑ0
ùëÑ1
ùëÑ2
ùëÑ3
ùëÑ4
ùëÑ5
ùëÑ6
ùëÑ7
Local 
Buffers
Global 
Buffers
SM0  SM1  SM2  SM3  SM4  SM5  SM6  SM7
SM0
SM1
SM2
SM3
SM4
SM5
SM6
SM7
Reduction with Atomic Operation
Performance Drop
SM0
Reduction with Barrier
SM1
SM5
SM2
SM3
SM4
SM6
SM7
Figure 1: Overview of the deterministic FlashAttention. Left: Tiled computation structure of the
backward pass, highlighting the local and global reductions. Middle: Comparison between the
non-deterministic (atomic-based) and deterministic (ordered) global reduction. Right: Performance
degradation under causal and full attention masks, HD stands for head dimension.
In high-performance attention (Vaswani et al., 2023) mechanisms like FlashAttention (Dao et al.,
2022), the backward pass computation is parallelized across hundreds of GPU Streaming Multipro-
cessors (SMs) (NVIDIA, 2022) to maximize throughput. Each SM, running a Cooperative Thread
Array (CTA), accumulates a partial contribution to gradient tensors (e.g., the gradient for the query
matrix, dQ). The default high-speed approach allows these CTAs to concurrently update the final
gradient in global memory via non-deterministic atomicAdd operations, as shown in Figure 1 mid-
dle. This creates a non-deterministic accumulation order: the final accumulated value depends on
the uncontrolled completion order of the CTAs, leading to bit-wise variations between runs.
To enforce reproducibility, FlashAttention-3 (Shah et al., 2024) provides a deterministic mode. It
enforces a fixed accumulation order by using synchronization barriers to force CTAs to perform their
additions in a serialized order (e.g., ordered by CTA index). However, this guarantee of consistency
imposes a significant performance penalty. As illustrated in Figure 1 right, enabling deterministic
mode may lower throughput by up to 37.9%, leading to severe training costs when scaling LLMs
across hundreds of thousands of GPUs.
This performance gap is not an inherent consequence of serialization itself. Instead, it stems from
a direct conflict between the tile scheduling and a rigid, pre-determined accumulation order. As
illustrated in the middle of Figure 1, the full mask scenario, commonly employed in multi-modal
tasks, highlights a key inefficiency: the naive schedule creates a bottleneck by forcing reductions
to start sequentially. An ideal schedule, however, would parallelize this process, allowing CTAs to
begin reduction on different tiles concurrently. Crucially, this reveals that the computation schedule
and the accumulation order are tightly coupled and cannot be optimized in isolation.
To address this, we introduce Deterministic Attention Scheduling for High-throughput (DASH),
a framework that formulates deterministic attention backward execution as an explicit scheduling
optimization problem. We model the deterministic backward pass as a Directed Acyclic Graph
(DAG), and formalize the objective as minimizing the DAG‚Äôs critical path length. Based on this
model, we design two complementary scheduling strategies. The first, Descending Q-Tile Iteration,
is a heuristic that processes query tiles in reverse order to advance dependency resolution and shrink
pipeline bubbles in causal attention. The second strategy, a theoretically optimal algorithm we term
Shift Scheduling is provably optimal under our DAG model. It employs a phase-shifted assignment
of computational tasks to GPU multiprocessors, creating a perfectly staggered execution pattern.
This ensures that the workload is perfectly balanced and that the serialized reduction operations
proceed without contention while approaching the model‚Äôs theoretical utilization bound.
Our empirical evaluations on NVIDIA H800 GPUs show that DASH significantly narrows the per-
formance gap relative to the FlashAttention-3 deterministic baseline. The two strategies deliver
up to a 1.28√ó speedup for the deterministic attention backward pass, significantly improving the
efficiency of reproducible LLM training.
In summary, we made the following contributions in this paper:
2


--- Page 3 ---
Published as a conference paper at ICLR 2026
‚Ä¢ We identify the misalignment between tile execution and accumulation ordering as the principal
source of performance degradation in deterministic attention.
‚Ä¢ We provide the first DAG-based formalization of deterministic attention backward scheduling,
enabling principled optimization of critical path length.
‚Ä¢ We introduce two complementary scheduling strategies, Descending Q-Tile Iteration and Shift
Scheduling, that achieve up to a 1.28√ó speedup over the FlashAttention-3 deterministic baseline
on H800 GPUs.
2
BACKGROUND
2.1
DETERMINISTIC FLASHATTENTION BACKWARD PASS
We first outline the core gradient computations in the FlashAttention backward pass: dQ, dK, and
dV (Figure 1, left). During backpropagation, the gradients dK and dV are accumulated across all
queries for each key (or value) position, i.e., they are reduced along the Q axis. In contrast, dQ
requires a reduction across all key‚Äìvalue (KV) positions for each query, i.e., along the KV axis. To
expose parallelism, the implementation partitions the KV dimension across SMs, allowing dK and
dV to be computed within each SM via a local reduction. However, this strategy distributes partial
contributions to dQ over multiple SMs, necessitating a global reduction to produce the final gradient.
A conventional implementation performs this reduction using atomic additions (Figure 1, middle),
which induces run-to-run variation because floating-point addition is non-associative. The resulting
numerical nondeterminism undermines strict reproducibility in large-scale training. To guarantee
determinism, one must enforce a prescribed accumulation order. FlashAttention-3 achieves this by
performing a tile-wise sequential accumulation of dQ along the KV dimension.
2.2
GPU ARCHITECTURE
On modern GPUs, the memory hierarchy comprises registers, shared memory, L2 cache, and global
memory (NVIDIA, 2022), reflecting a fundamental capacity‚Äìlatency trade-off: smaller and faster
storage resides closer to the compute units. Shared memory is private to each SM, enabling low-
latency intra-SM data reuse, whereas the L2 cache is globally shared, mediating inter-SM data ex-
change and coherence. In datacenter-class GPUs, the L2 cache may be physically segmented, with
each segment preferentially serving a subset of SMs; remote-segment accesses typically incur higher
latency than local ones. This hierarchical organization materially shapes the attainable performance
and the efficiency of memory-bound GPU kernels.
2.3
DETERMINISM IN OTHER OPERATIONS OF THE TRANSFORMER
Other components, such as GEMMs, attention forward and normalization, also involve reduction op-
erations; however, the computational cost of enforcing determinism in these cases is generally min-
imal during typical LLM training. GEMMs may exhibit nondeterministic behavior only when the
reduction axis (i.e., the K-dimension) is partitioned across multiple blocks, as in split-K (NVIDIA
Corporation, 2025) or stream-K (Osama et al., 2023) parallelization modes. In large-batch LLM
training, parallelism along the M and N dimensions is typically sufficient to fully utilize the GPU,
rendering split-K or stream-K modes unnecessary; therefore, disabling these modes generally re-
sults in only a minor reduction in throughput. Similarly, other operations involving reduction, such
as attention forward passes and normalizations, typically perform reductions within a single block,
thereby ensuring a deterministic reduction order. Purely elementwise operations, including activa-
tion functions and bias additions, are inherently deterministic.
3
DASH: SCHEDULING STRATEGIES FOR DETERMINISTIC ATTENTION
In this section, we introduce optimized scheduling strategies for deterministic attention. Without
loss of generality, we assume that the number of KV tiles equals the number of SMs, denoted by
n. When the actual number of KV tiles differs from the number of SMs, we conceptually refine or
aggregate attention heads so that all SMs remain fully utilized under the same analytical framework.
3


--- Page 4 ---
Published as a conference paper at ICLR 2026
ùê∂i, j
ùëÖi, j
SM0
SM1
Critical Path Length = 2c + 3r
Idle
ùê∂0,0
ùëÖ0,0
ùê∂0,1
ùëÖ0,1
ùê∂1,1
ùëÖ1,1
S
T
Compute
Reduction
Dependency
Critical Path
Compute
Reduction
Dependency
Critical Path
c
r
0
Figure 2: Visualization of the Deterministic Scheduling Problem. The Gantt chart (left) shows a
naive execution schedule for a problem with two KV-tiles (i-index) and two Q-tiles (j-index). Each
task consists of a compute phase C(i, j) and a reduction phase R(i, j). Local reductions enforce
contiguous execution on a single SM (e.g., all tasks for i = 0 on SM0). A deterministic global
reduction order introduces a cross-SM dependency (red arrow), forcing SM1 to idle and creating
a pipeline bubble. The corresponding DAG (right) abstracts this schedule, where the critical path
determines the end-to-end latency.
3.1
PROBLEM FORMULATION
We formalize the deterministic attention backward scheduling problem as an optimization over a
directed acyclic graph (DAG), as shown in Figure 2. The DAG‚Äôs structure is constrained jointly by
the dataflow of FlashAttention and the architectural characteristics of the target GPU. Our model
represents a simplified abstraction of actual GPU execution; its primary purpose is to offer insights
into more effective scheduling decisions, rather than to accurately predict real execution times. As
such, there remain significant differences between our theoretical model and the complexities of
real-world GPU behavior.
Graph Construction.
Each tile-processing task is modeled as a linear path of nodes connected by
edges that encode two successive phases: (i) the tile‚Äôs computation and (ii) the subsequent global re-
duction. These phase edges are weighted by their respective execution times, which are assumed to
be constants. To encode legal accumulation orderings and data dependencies across tiles, we insert
zero-weight dependency edges between nodes of different task paths. In this way, edge weights
capture quantitative duration, while the topology captures qualitative ordering constraints. The
scheduling objective is to minimize the critical-path length of the resulting DAG, thereby reduc-
ing end-to-end latency and improving overall execution efficiency.
Optimization Constraint.
Data movement across different memory levels incurs substantial over-
head, while registers provide the fastest storage in GPUs. To leverage fast register-resident accumu-
lation of dK and dV , all operations for a given KV tile must run contiguously on a single SM.
Consequently, the edges associated with this tile form an unbroken chain, which imposes a key
constraint on our optimization.
3.2
ANALYSIS OF FLASHATTENTION-3 DETERMINISTIC BACKWARD SCHEDULE
SM0
SM1
SM2
SM3
c0
c0
c0
c0
r0
r0
r0
r0
c1
c1
c1
c1
r1
r1
r1
r1
c2
c2
c2
c2
r2
r2
r2
r2
c3
c3
c3
c3
r3
r3
r3
r3
(a) Schedule for Full Mask
SM0
SM1
SM2
SM3
c0
c1
c2
c3
r0
c1
r1
r1
c2
c2
r2
r2
r2
c3
c3
c3
r3
r3
r3
r3
(b) Schedule for Causal Mask
Figure 3: Backward scheduling of FlashAttention-3 for both mask shapes. Each colored segment
denotes one block‚Äôs computation (cost c) followed by a reduction (cost r). Idle gaps correspond to
pipeline bubbles. For clarity, since we assume the number of KV tiles equals the number of SMs,
each SM processes exactly one KV tile; thus we omit the KV index in the visualization and show
only the query index for each block.
Under a full attention mask, the FlashAttention-3 backward schedule achieves reasonable pipeline
utilization (Figure 3a). Observable bubbles (SM idle periods) arise only during the startup phase
4


--- Page 5 ---
Published as a conference paper at ICLR 2026
of the first computation stage, before steady-state overlap is established. Let each stage incur a
computation cost c followed by a reduction cost r. After the initial fill, each attention head sustains
n sequential (computation + reduction) pairs, giving Tsteady = n ¬∑ (c + r) where n is the number of
SMs. The startup overhead contributes an additional (n‚àí1)¬∑r due to staggered completion of the first
sequence of reductions. Hence, for m heads, Tfull = mTsteady+Tstartup = m¬∑n¬∑(c+r)+(n‚àí1)¬∑r,
up to negligible control and synchronization overhead.
In contrast, when a causal mask is applied, the data dependencies inherent in the schedule lead to
significant inefficiencies. As shown in Figure 3b, this schedule introduces a substantial bubble within
the execution of each attention head, preventing effective pipelining. The critical path for a single
head becomes Thead causal = n ¬∑ (c + r) + (n ‚àí1) ¬∑ r. Since this inefficient pattern repeats for every
head, the total execution time for m heads is approximately Tcausal = m¬∑Thead causal +Tstartup ‚âà
m ¬∑ n ¬∑ (c + r) + (n ‚àí1) ¬∑ r.
3.3
DESCENDING Q-TILE ITERATION: A ROBUST HEURISTIC FOR CAUSAL MASKS
SM0
SM1
SM2
SM3
c3
c3
c3
c3
r3
r3
r3
r3
c2
c2
c2
r2
r2
r2
c1
c1
r1
r1
c0
r0
c3
c3
c3
c3
r3
r3
c2
r3
c2
r2
r3
c2
r2
c1
r2
c1
r1
r1
c0
r0
Figure 4: Descending (reverse-order) query tile schedule for the causal mask. Reversing the Q-block
traversal accelerates dependency resolution. Colors distinguish attention heads in the pipeline.
To mitigate the pipeline bubbles caused by causal masking, we propose a simple yet effective mod-
ification: reversing the processing order of the query (Q) blocks. As illustrated in Figure 4, this
reversed schedule allows most SMs to begin their computation earlier by resolving dependencies
more quickly.
The crucial advantage of this approach is its impact on pipeline efficiency for subsequent atten-
tion heads.
By reversing the order, the short tasks are completed first, freeing up their SMs
much earlier. Consequently, the second head can immediately begin to utilize these available re-
sources, creating a tightly coupled pipeline that almost eliminates the idle gaps between heads.
This sustained high utilization across an even number of m heads yields a total execution time of:
Treversed ‚âàm¬∑(n+1)(c+r)
2
+ (n ‚àí1) ¬∑ r.
3.4
SHIFT SCHEDULING
Reduction
Compute
Dependency
Critical Path Length = 2c + 2r
S
S
T
T
Proceeding dependency
Reverse dependency
Critical Path Length = 2c + 3r
c
r
0
Figure 5: Illustrative example for Lemma 1. Left: Added dependency (zero-weight) edges pre-
serve non-decreasing depth order and do not lengthen the critical path. Right: A backward (depth-
decreasing) dependency edge violates the lemma‚Äôs condition and increases the critical path.
Although the Descending Q-Tile Iteration significantly improves performance, it is natural to ask
whether a theoretically optimal schedule exists. To address this, we examine the impact of introduc-
ing reduction-induced inter-SM dependencies on the computation DAG‚Äôs critical path.
Disregarding (for the moment) the accumulation edges required for dQ updates, the graph decom-
poses into n independent chains whose total time is minimized when their cumulative workloads
are perfectly balanced. In this idealized scenario, all chains are also isomorphic, as they share an
identical task structure and number of tasks. The core challenge is thus to insert the necessary
5


--- Page 6 ---
Published as a conference paper at ICLR 2026
SM0
SM1
SM2
SM3
Assigned SM ID
Q0
Q1
Q2
Q3
Query Tile
0
3
2
1
1
0
3
2
2
1
0
3
3
2
1
0
KV0
KV1
KV2
KV3
Key-Value Tile
SM0
SM1
SM2
SM3
c0
c1
c2
c3
r0
r1
r2
r3
c1
c2
c3
c0
r1
r2
r3
r0
c2
c3
c0
c1
r2
r3
r0
r1
c3
c0
c1
c2
r3
r0
r1
r2
Figure 6: Optimal full-mask schedule via cyclic shifting. Left: Cyclic visiting order of Q tiles
per SM; distinct timestamps (i.e., the value in each box) on each row induce a natural, conflict-
free reduction sequence for every dQ block. Right: Simulated timeline showing fully balanced
utilization without additional bubbles.
zero-weight dependency edges without lengthening the original critical path. The lemma below
characterizes precisely when this is possible; its proof is deferred to Appendix B for brevity.
lemma 1. Let G0 = (V, E0) be a DAG consisting of a single source node s, a single sink node t, and
n ‚â•1 parallel, isomorphic chains connecting s to t. All edge weights in E0 are strictly positive. Let
the depth of a node v, denoted depth(v), be the number of edges on the unique path from s to v within
its chain in G0. Let a sequence of graphs G1, . . . , Gk be generated such that Gi = (V, Ei‚àí1‚à™{ei}),
where each ei = (ui, vi) is a zero-weight edge. We add the explicit condition that every new graph
Gi in the sequence must remain a DAG.
Under this condition, the critical path length of Gk is equal to that of G0 if and only if for every
added edge ei = (ui, vi) for i ‚àà{1, . . . , k}, the condition depth(ui) ‚â§depth(vi) holds.
As illustrated in Figure 5, Lemma 1 dictates that to preserve the original critical path length, any
added dependency edge (u, v) must satisfy the condition depth(u) ‚â§depth(v). This formal con-
straint translates to a critical physical limitation: for any given query tile Qj, the tasks involving it
cannot be executed in parallel.
A schedule that assigns two tiles contributing to the same dQj‚Äîsay (KVi, Qj) and (KVk, Qj)‚Äîto
execute concurrently on different SMs would create a resource conflict during their reduction phases.
Resolving this conflict requires serializing the reductions, for instance, forcing the reduction for
(KVk, Qj) to wait for the one from (KVi, Qj) to complete, or vice versa. Because the conflicting
reduction tasks would otherwise start at the same depth in the DAG, this forced serialization in-
troduces a dependency edge (u, v) where depth(u) > depth(v)‚Äîfrom the completion of the first
reduction to the start of the second. This directly violates the lemma‚Äôs condition and sub-optimally
extends the critical path.
Our objective is thus twofold: first, to balance the workload across SMs, and second, to devise a
conflict-free reduction order that adheres to the lemma‚Äôs constraint.
Optimal Schedule for Full Masks
Under a full mask, per-KV-tile workloads are uniform, allow-
ing for immediate balancing. To satisfy the second objective, we employ a Shift Scheduling, as
illustrated in Figure 6. In this schedule, SMi processes KV blocks in the order (i, i + 1, . . . , n ‚àí
1, 0, . . . , i ‚àí1). This cyclical assignment inherently creates a conflict-free, sequential ordering for
the reductions on any given dQ block, directly satisfying the lemma‚Äôs condition. As both workload
balancing and conflict-free reduction are achieved, this schedule is theoretically optimal.
Symmetric Shift Scheduling for Causal Masks
Causal masking induces a strongly imbalanced
workload: early KV blocks participate in the full set of query interactions, whereas later blocks
contribute progressively fewer operations, yielding workloads that decrease linearly across the se-
quence.
We address this by Symmetric Shift Scheduling. Its core is a symmetric pairing principle: SMs
jointly handle KV blocks i and n ‚àí1 ‚àíi, pairing the longest with the shortest, the second-longest
with the second-shortest, and so forth. This pairing equalizes task chain lengths per SM, restoring
near-perfect balance.
6


--- Page 7 ---
Published as a conference paper at ICLR 2026
We operationalize symmetric pairing via a two-phase schedule. In Phase 1, a cyclic shift is applied to
the dense lower-left rectangle, efficiently filling the pipeline. Phase 2 addresses the residual triangles
using a purely analytical model of workload folding, where tasks from the lower-right are logically
mapped to the upper-left‚Äôs masked slots to form a conceptual square without any data movement.
The operational sequence‚Äîa top-down traversal of the left triangle and a bottom-up traversal of the
right‚Äîis algebraically equivalent to a diagonal-initialized shift schedule on this conceptual square.
This equivalence is key: it preserves workload balance, ensures contiguous computation for each
KV block, enforces depth-monotone accumulation to satisfy Lemma 1, and ultimately eliminates all
pipeline bubbles.
SM0
SM1
SM2
SM3
SM3
SM2
SM1
SM0
Paired SM Alloc
Q0
Q1
Q2
Q3
Q4
Q5
Q6
Q7
Query Tile
5
6
5
7
6
5
0
4
3
2
1
0
4
3
8
2
1
0
4
7
8
3
2
1
0
6
7
8
4
3
2
1
5
6
7
8
Phase 1
Phase 2
KV0
KV1
KV2
KV3
KV4
KV5
KV6
KV7
Key-Value Tile
Figure 7: Optimal causal-mask sched-
ule using symmetric shift and two-phase
workload folding.
Phase 1 processes
the dense lower-left rectangle; Phase
2 folds the remaining triangles into a
logical square and traverses it starting
from the main diagonal, first covering
the upper-left portion before the lower-
right, ensuring each KV block is exe-
cuted contiguously.
Summary of Optimal Performance
In summary, the
proposed scheduling strategies achieve theoretical opti-
mality for both scenarios. By perfectly balancing work-
loads and eliminating pipeline bubbles, the total execu-
tion time for m heads is: Full Mask: Tfull opt = m ¬∑ n ¬∑
(c + r); Causal Mask: Tcausal opt = m¬∑(n+1)¬∑(c+r)
2
4
EXPERIMENTS
In this section, we empirically evaluate the performance
of our proposed scheduling strategies under full and
causal masks. We measure throughput under various se-
quence lengths and analyze how architectural factors in-
teract with different scheduling choices.
4.1
EXPERIMENTAL SETUP
Hardware and Software.
All experiments are con-
ducted on a server equipped with NVIDIA H800 GPUs,
CUDA version 12.6 and Triton (Tillet et al., 2019) ver-
sion 3.4. All kernels are implemented by extending the
FlashAttention-3 implementation.
Baseline and Proposed Methods.
We compare our
methods against the deterministic backward pass of
FlashAttention-3, which serves as our primary baseline.
We also benchmark against the Triton tutorial‚Äôs implementation for causal attention, as its pub-
lic version lacks a full-mask counterpart.
We omit FlashAttention-2 because prior published
benchmarks (Shah et al., 2024) on Hopper-class GPUs show it is consistently outperformed by
FlashAttention-3, and thus it no longer constitutes a competitive baseline. The methods under eval-
uation are:
‚Ä¢ Descending Q-Tile Iteration (for both masks)
‚Ä¢ Shift Scheduling (for full masks)
‚Ä¢ Symmetric Shift Scheduling (for causal masks)
Benchmark Settings
Following the methodology of the FlashAttention-3 study, we evaluate per-
formance by fixing the total number of tokens at 16,384 while varying the sequence length from 512
to 16,384. Similarly, we fix the hidden dimension to be 2,048, and test different head dimensions in
64 and 128. All the results are tested using BF16 precision random inputs.
4.2
PERFORMANCE ON FULL ATTENTION MASKS
Figure 8 presents the throughput comparison for the full attention mask scenario. Our Shift Schedul-
ing consistently outperforms the FlashAttention-3 baseline across most sequence lengths, demon-
strating the effectiveness of our theoretically optimal approach. However, a notable exception occurs
7


--- Page 8 ---
Published as a conference paper at ICLR 2026
512
1024
2048
4096
8192
16384
Sequence Length (seqlen)
200
300
400
500
600
Performance (TFLOPS)
Baseline
Descending
Shift Scheduling
Non-deterministic
(a) Full mask, headdim = 64.
512
1024
2048
4096
8192
16384
Sequence Length (seqlen)
200
300
400
500
600
Performance (TFLOPS)
Baseline
Descending
Shift Scheduling
Non-deterministic
(b) Full mask, headdim = 128
Figure 8: Backward-pass throughput under full attention masks.
at the maximum sequence length of 16,384, where its performance slightly degrades relative to the
baseline.
This phenomenon highlights a divergence between our theoretical model and practical hardware
execution. Our model assumes zero-cost dependency edges, but in reality, inter-SM communication
for synchronizing reduction operations is mediated by the L2 cache. This incurs significant latency,
ranging from approximately 200 cycles for accesses to the local L2 cache segment to over 500 cycles
for remote segment accesses on H800-class GPUs (Luo et al., 2025). This latency differential is a
direct consequence of the distributed L2 cache architecture described in Section 2.
At a sequence length of 16,384 and a KV block size of 128, the computation for a single head is dis-
tributed across 128 blocks, often mapped to 128 SMs. This high degree of parallelism necessitates
frequent cross-SM communication to signal task completion. Given the large number of partici-
pating SMs, a substantial portion of these synchronization signals must traverse the higher-latency
links to a remote L2 cache segment. The Shift Scheduling, with its more intricate dependency graph
compared to the simpler, linear dependency of the baseline, becomes more sensitive to this com-
munication overhead at extreme parallelism. This increased synchronization cost, dominated by
remote L2 accesses, ultimately outweighs the computational benefits of the schedule in this specific
high-parallelism, long-sequence scenario, leading to the observed performance degradation.
4.3
PERFORMANCE ON CAUSAL ATTENTION MASKS
512
1024
2048
4096
8192
16384
Sequence Length (seqlen)
0
100
200
300
400
500
600
Performance (TFLOPS)
Triton
Baseline
Descending
Symmetric Shift Scheduling
Non-deterministic
(a) Causal mask, headdim = 64
512
1024
2048
4096
8192
16384
Sequence Length (seqlen)
0
100
200
300
400
500
600
Performance (TFLOPS)
Triton
Baseline
Descending
Symmetric Shift Scheduling
Non-deterministic
(b) Causal mask, headdim = 128
Figure 9: Backward-pass throughput under causal attention masks.
The performance evaluation for causal attention masks, presented in Figure 9, confirms the efficacy
of our proposed methods. Both the Descending Q-Tile Iteration and our theoretically optimal Sym-
metric Shift Scheduling demonstrate a throughput improvement over the FlashAttention-3 baseline
across all tested configurations.
An interesting trade-off emerges when comparing our two proposed methods at different head di-
mensions (headdim). At headdim = 64, the Symmetric Shift Scheduling achieves the highest
performance, validating the benefits of its superior workload balancing. However, the descending
schedule does not perform very well in this case. This is because in the FlashAttention-3 causal
8


--- Page 9 ---
Published as a conference paper at ICLR 2026
backward kernel, the L2-aware LPT scheduler interleaves multiple heads across SMs. When head-
dim = 64 and the sequence length is short, each head‚Äôs L2 footprint remains small, allowing many
heads to reside in cache with only 1‚Äì2 tiles in flight per head. Consequently, the causal stalls tar-
geted by Descending Q-Tile Iteration are largely masked by cross-head interleaving, resulting in
only marginal net performance gains.
However, at headdim = 128, Symmetric Shift Scheduling‚Äôs performance is surpassed by the sim-
pler Descending Q-Tile Iteration. This performance inversion is attributable to a critical interaction
between algorithmic complexity and GPU resource limitations, specifically register pressure. The
Symmetric Shift Scheduling, while algorithmically optimal, requires a more complex implementa-
tion to manage the state of the folded task space. This complexity translates to higher register usage
per thread to maintain additional loop counters and intermediate states.
When headdim = 128, the base register requirement for storing accumulators and other interme-
diate values is already substantial. The additional overhead (around 10 registers) from our optimal
schedule can push the total register count per thread beyond the hardware‚Äôs physical limit, as shown
by Nsight Compute (NVIDIA Corporation, 2024). This forces the compiler to generate code that
spills registers, offloading their contents to the much slower local memory. The high latency incurred
by these spill-induced memory operations introduces significant execution stalls, which negate the
algorithmic benefits of the more balanced workload and lead to degraded performance. In con-
trast, the simpler Descending Q-Tile Iteration operates below this critical register pressure threshold,
thereby avoiding spilling and achieving better effective performance in this high-resource-demand
scenario. Therefore, the two schedules for causal masks are complementary: Symmetric Shift is
theoretically optimal under our DAG model, while Descending is the practically preferred choice
for large head dimensions on current GPUs.
In the future, Symmetric Shift‚Äôs theoretical advantages are expected to be fully realized on newer
architectures with greater on-chip resources (such as Blackwell GPUs with TMEM, or devices
equipped with larger register files), or under kernel designs that are less constrained by register
allocation than the present FlashAttention-3 implementation.
4.4
END-TO-END PERFORMANCE
LLaMA3-8B
Qwen2.5-7B Mistral-8x7B
1.000
1.025
1.050
1.075
1.100
Speedup
LLMs
Seq Len
8k
16k
32k
LLaDA-1b
SAM-ViT-Huge
SD3.5-Large
SD3.5-Medium
1.00
1.02
1.04
1.06
Full Mask Models
(a) Speedup for an entire transformer block.
Llama3-8B
Qwen2.5-7B
Mistral 8x7B
LLaDA-1b
SAM-ViT-Huge
SD3.5-Large
SD3.5-Medium
0.00
0.25
0.50
0.75
1.00
Ratio
Attn-fwd
Attn-bwd
FFN (fwd+bwd)
Others
Baseline
Ours
(b) Time breakdown.
Figure 10: End-to-end performance of a transformer block.
To assess the performance gains delivered by DASH during training, we measured the runtime
required to process an entire transformer block, accounting for both forward and backward passes.
We evaluated DASH across a range of widely adopted models. For causal mask scenarios, we
selected famous LLMs: LLaMA3-8b (Grattafiori et al., 2024), Qwen2.5-7b (Qwen et al., 2025), and
Mistral-8√ó7b (Jiang et al., 2024). For full mask scenarios, we included the vision model SAM-
huge (Kirillov et al., 2023), the diffusion models StableDiffusion3.5 (medium and large) (AI, 2024),
and the diffusion-based language model LLaDA-1b (Nie et al., 2025).
For LLMs, we employ a batch size of 1 with sequence lengths of 8k, 16k, and 32k. In the case of full
mask models, a batch size of 16 is used, with the training sequence length fixed at 4k in accordance
with standard architectural configurations. The relative speedup achieved by our approach compared
to the baseline is illustrated in Figure 10a. For causal models, we observe end-to-end performance
improvements ranging from 2% to 10%. Full mask models also exhibit a speedup of approximately
4%. In summary we achieved an average speedup of around 5%, which aligns with our internal
training experience on thousands of GPUs. Additionally, Figure 10b provides a detailed breakdown
9


--- Page 10 ---
Published as a conference paper at ICLR 2026
of computation time across different kernel operations, with causal models evaluated at a sequence
length of 16k.
4.5
IMPACT OF DETERMINISM ON NUMERICAL STABILITY
Table 1: Max gradient deviation averaged over 10 identical backward passes; Mr = max |gr ‚àígref|.
Masking Scheme
Non-deterministic
Deterministic
Full
2.4 √ó 10‚àí4
0
Causal
4.9 √ó 10‚àí4
0
Our analysis of backward passes indicates that non-deterministic kernels cause run-to-run gradient
deviations of O(10‚àí4), while deterministic ones guarantee bitwise identical outcomes (Table 1).
Although small, this variability can accumulate, so determinism is key to achieving reproducibility.
5
RELATED WORKS
FlashAttention and Kernel-Level I/O Optimization
Early optimization of attention focused on
mitigating the I/O bottleneck imposed by the quadratic attention matrix. FlashAttention (Dao et al.,
2022) introduced an I/O-aware tiled and fused kernel that avoids materializing the full attention
matrix in HBM. FlashAttention-2 and 3 (Dao, 2023; Shah et al., 2024) further improved utilization
via refined work partitioning and leveraged specialized hardware for asynchronous data movement.
Low-Precision Attention
Low-precision methods further reduce bandwidth and memory cost.
The SageAttention series (Zhang et al., 2025b;a;c) systematically explores progressively lower for-
mats while maintaining accuracy.
Inference-Oriented Attention Kernels
Inference-specialized kernels include FlashDecoding and
FlashDecoding++(Dao et al., 2023; Hong et al., 2024) for autoregressive decoding, PodAtten-
tion(Kamath et al., 2025) for mixed prefilling/decoding, and DeFT (Yao et al., 2025) and Fast-
Tree (Pan et al., 2025) for tree-structured generation.
Distributed Cyclic Scheduling
Our shift-based scheduling is inspired by cyclic (ring-
style) phase-shift patterns long used in distributed systems.
Distributed attention algo-
rithms‚ÄîRingAttention (Liu et al., 2023), StripedAttention (Brandon et al., 2023), and Loong-
Train (Gu et al., 2024)‚Äîadopt related cyclic schemes to overlap communication and computation
across devices, whereas we apply a shift strategy intra-GPU to co-optimize deterministic accumula-
tion and work balance.
Deterministic Implementations
Existing deterministic implementations either split dK, dV and
dQ calculation into different passes (e.g., Triton tutorials (Tillet et al., 2019))‚Äîforcing a second K/V
read‚Äîor materialize per-tile dQ partials for later consolidation (FlashAttention-2), adding memory
footprint and an extra reduction kernel. These designs trade bandwidth or memory rather than co-
optimizing execution and accumulation order, which is the focus of our approach.
Determinism in Inference
Determinism for inference has also been examined: He & Lab (2025)
attribute non-reproducibility to lack of ‚Äúbatch invariance,‚Äù where outputs depend on batch size, and
design batch-invariant kernels. Their goal differs from ours: we target training time run-to-run
determinism, where batch configurations are fixed to ensure reproducibility.
6
CONCLUSION
In this work, we addressed the significant performance penalty associated with the deterministic
backward pass in modern attention mechanisms. By formulating the computation as a scheduling
problem on a DAG, we introduced DASH, a framework featuring two distinct and complementary
10


--- Page 11 ---
Published as a conference paper at ICLR 2026
strategies. The first, Descending Q-Tile Iteration, provides a simple yet remarkably effective heuris-
tic that accelerates causal attention. The second, derived from our conflict-free scheduling lemma,
represents a theoretically optimal solution.
Our empirical evaluation not only demonstrates that DASH significantly narrows the performance
gap, improving throughput by up to 1.28√ó over the baseline, but more importantly, it reveals a
crucial insight: theoretical optimality does not always translate to practical superiority. We iden-
tified hardware realities, such as register pressure and inter-SM communication latency, as critical
factors that can override the benefits of a more complex, algorithmically perfect schedule. By pro-
viding a suite of solutions catering to different scenarios, DASH enables practitioners to achieve
high throughput attention in reproducible LLM training.
REFERENCES
Stability AI. Stable diffusion 3.5, 2024. URL https://github.com/Stability-AI/sd3.
5. Accessed: Nov. 2025.
William Brandon, Aniruddha Nrusimha, Kevin Qian, Zachary Ankner, Tian Jin, Zhiye Song, and
Jonathan Ragan-Kelley. Striped attention: Faster ring attention for causal transformers, 2023.
URL https://arxiv.org/abs/2311.09431.
Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023. URL
https://arxiv.org/abs/2307.08691.
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R¬¥e. Flashattention: Fast and
memory-efficient exact attention with io-awareness, 2022. URL https://arxiv.org/abs/
2205.14135.
Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov.
Flash-decoding for long-
context inference.
PyTorch Blog, October 2023.
URL https://pytorch.org/blog/
flash-decoding/.
DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Cheng-
gang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang,
Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting
Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui
Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi
Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li,
Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang,
Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun
Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan
Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J.
Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang,
Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng
Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shut-
ing Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao,
Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue
Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xi-
aokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin
Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang,
Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang
Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui
Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying
Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu,
Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan
Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F.
Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda
Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao,
Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li,
Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2025. URL
https://arxiv.org/abs/2412.19437.
11


--- Page 12 ---
Published as a conference paper at ICLR 2026
Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad
Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan,
Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Ko-
renev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava
Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,
Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret,
Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius,
Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary,
Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab
AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco
Guzm¬¥an, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind That-
tai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Kore-
vaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra,
Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Ma-
hadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu,
Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jong-
soo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala,
Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid
El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren
Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin,
Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi,
Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew
Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Ku-
mar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoy-
chev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur C¬∏ elebi, Patrick Alrassy, Pengchuan
Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan,
Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ra-
mon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Ro-
hit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan
Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell,
Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng
Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer
Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman,
Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mi-
haylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor
Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, V¬¥ƒ±tor Albiero, Vladan Petrovic, Weiwei
Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang
Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Gold-
schlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning
Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh,
Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria,
Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein,
Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, An-
drew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, An-
nie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel,
Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leon-
hardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu
Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Mon-
talvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao
Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia
Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide
Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le,
Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily
Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smoth-
ers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni,
Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia
Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan,
Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harri-
son Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj,
12


--- Page 13 ---
Published as a conference paper at ICLR 2026
Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James
Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jen-
nifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang,
Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Jun-
jie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy
Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang,
Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell,
Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa,
Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias
Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L.
Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike
Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari,
Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan
Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong,
Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent,
Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar,
Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Ro-
driguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy,
Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin
Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon,
Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ra-
maswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha,
Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal,
Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satter-
field, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj
Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo
Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook
Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Ku-
mar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov,
Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiao-
jian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia,
Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao,
Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhao-
duo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL
https://arxiv.org/abs/2407.21783.
Diandian Gu, Peng Sun, Qinghao Hu, Ting Huang, Xun Chen, Yingtong Xiong, Guoteng Wang,
Qiaoling Chen, Shangchun Zhao, Jiarui Fang, Yonggang Wen, Tianwei Zhang, Xin Jin, and Xu-
anzhe Liu. Loongtrain: Efficient training of long-sequence llms with head-context parallelism,
2024. URL https://arxiv.org/abs/2406.18485.
Horace
He
and
Thinking
Machines
Lab.
Defeating
nondeterminism
in
llm
infer-
ence.
Thinking Machines Lab:
Connectionism, 2025.
doi:
10.64434/tml.20250910.
https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/.
Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong,
and Yu Wang. Flashdecoding++: Faster large language model inference on gpus, 2024. URL
https://arxiv.org/abs/2311.01282.
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris
Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gi-
anna Lengyel, Guillaume Bour, Guillaume Lample, L¬¥elio Renard Lavaud, Lucile Saulnier, Marie-
Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le
Scao, Th¬¥eophile Gervet, Thibaut Lavril, Thomas Wang, Timoth¬¥ee Lacroix, and William El Sayed.
Mixtral of experts, 2024. URL https://arxiv.org/abs/2401.04088.
Aditya K. Kamath, Ramya Prabhu, Jayashree Mohan, Simon Peter, Ramachandran Ramjee, and
Ashish Panwar. Pod-attention: Unlocking full prefill-decode overlap for faster llm inference. In
Proceedings of the 30th ACM International Conference on Architectural Support for Program-
ming Languages and Operating Systems, Volume 2, ASPLOS ‚Äô25, pp. 897‚Äì912. ACM, March
13


--- Page 14 ---
Published as a conference paper at ICLR 2026
2025. doi: 10.1145/3676641.3715996. URL http://dx.doi.org/10.1145/3676641.
3715996.
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete
Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll¬¥ar, and Ross Girshick.
Segment anything, 2023. URL https://arxiv.org/abs/2304.02643.
Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-
infinite context, 2023. URL https://arxiv.org/abs/2310.01889.
Weile Luo, Ruibo Fan, Zeyu Li, Dayou Du, Hongyuan Liu, Qiang Wang, and Xiaowen Chu. Dis-
secting the nvidia hopper architecture through microbenchmarking and multiple level analysis,
2025. URL https://arxiv.org/abs/2501.12084.
Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin,
Ji-Rong Wen, and Chongxuan Li. Large language diffusion models, 2025. URL https://
arxiv.org/abs/2502.09992.
NVIDIA.
NVIDIA H100 Tensor Core GPU Architecture.
Technical report, NVIDIA,
mar 2022.
URL https://www.nvidia.com/content/dam/en-zz/Solutions/
gtc22/data-center/h100/gtc22-whitepaper-hopper.pdf. White paper.
NVIDIA Corporation. NVIDIA Nsight Compute, 2024. URL https://developer.nvidia.
com/nsight-compute. Version 2022.4.
NVIDIA Corporation. Split-k gemm. https://github.com/NVIDIA/cutlass/tree/
main/examples/06_splitK_gemm, 2025.
Muhammad Osama, Duane Merrill, Cris Cecka, Michael Garland, and John D. Owens. Stream-
k: Work-centric parallel decomposition for dense matrix-matrix multiplication on the gpu. In
Proceedings of the 28th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel
Programming, PPoPP ‚Äô23, pp. 429‚Äì431, New York, NY, USA, 2023. Association for Computing
Machinery. ISBN 9798400700156. doi: 10.1145/3572848.3577479. URL https://doi.
org/10.1145/3572848.3577479.
Zaifeng Pan, Yitong Ding, Yue Guan, Zheng Wang, Zhongkai Yu, Xulong Tang, Yida Wang, and
Yufei Ding. Fasttree: Optimizing attention kernel and runtime for tree-structured LLM inference.
In Eighth Conference on Machine Learning and Systems, 2025. URL https://openreview.
net/forum?id=BwvHcHZ3kJ.
Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan
Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang,
Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin
Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li,
Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang,
Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025.
URL https://arxiv.org/abs/2412.15115.
Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao.
Flashattention-3: Fast and accurate attention with asynchrony and low-precision, 2024. URL
https://arxiv.org/abs/2407.08608.
Sanjif Shanmugavelu, Mathieu Taillefumier, Christopher Culver, Oscar Hernandez, Mark Coletti,
and Ada Sedova. Impacts of floating-point non-associativity on reproducibility for hpc and deep
learning applications, 2024. URL https://arxiv.org/abs/2408.05148.
Philippe Tillet, H. T. Kung, and David Cox. Triton: an intermediate language and compiler for
tiled neural network computations.
In Proceedings of the 3rd ACM SIGPLAN International
Workshop on Machine Learning and Programming Languages, MAPL 2019, pp. 10‚Äì19, New
York, NY, USA, 2019. Association for Computing Machinery.
ISBN 9781450367196.
doi:
10.1145/3315508.3329973. URL https://doi.org/10.1145/3315508.3329973.
14


--- Page 15 ---
Published as a conference paper at ICLR 2026
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. URL https://arxiv.
org/abs/1706.03762.
Oreste Villa, Daniel Chavarr¬¥ƒ±a-Miranda, Vidhya Gurumoorthi, Andres Marquez, and Sriram Kr-
ishnamoorthy. Effects of floating-point non-associativity on numerical computations on mas-
sively multithreaded systems. Pacific Northwest National Laboratory (PNNL), Richland, WA
(US), Cray User Group, Inc., Corvallis, OR, United States(US)., 05 2009.
URL https:
//www.osti.gov/biblio/976992.
Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari.
Continual learning for large language models: A survey, 2024. URL https://arxiv.org/
abs/2402.01364.
Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke Wang, and Tao Lin.
Deft: Decoding with flash tree-attention for efficient tree-structured llm inference, 2025. URL
https://arxiv.org/abs/2404.00242.
Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, and Jianfei Chen. Sageattention2:
Efficient attention with thorough outlier smoothing and per-thread int4 quantization, 2025a. URL
https://arxiv.org/abs/2411.10958.
Jintao Zhang, Jia Wei, Haofeng Huang, Pengle Zhang, Jun Zhu, and Jianfei Chen.
Sageatten-
tion: Accurate 8-bit attention for plug-and-play inference acceleration, 2025b. URL https:
//arxiv.org/abs/2410.02367.
Jintao Zhang, Jia Wei, Pengle Zhang, Xiaoming Xu, Haofeng Huang, Haoxu Wang, Kai Jiang,
Jun Zhu, and Jianfei Chen.
Sageattention3: Microscaling fp4 attention for inference and an
exploration of 8-bit training, 2025c. URL https://arxiv.org/abs/2505.11594.
15


--- Page 16 ---
Published as a conference paper at ICLR 2026
A
THE USE OF LARGE LANGUAGE MODELS
During the preparation of this manuscript, the authors employed a large language model (LLM) for
two primary purposes. First, the LLM was used as a tool to improve the grammar, spelling, and
overall clarity of the text. Second, it was used to assist in the initial stages of the literature search.
The role of the LLM was strictly that of an assistant. All language suggestions were reviewed and
edited by the authors to ensure they accurately reflected the intended scientific meaning. Further-
more, any literature identified with the assistance of the LLM was independently retrieved, reviewed,
and vetted for relevance and accuracy by the authors. All intellectual contributions, including the
conception of the research, methodology, and final conclusions, are the exclusive work of the human
authors, who take full responsibility for the final content of this paper.
B
PROOF OF LEMMA 1
Proof. Let LPi(x) denote the length of the longest path from the source node s to node x in graph
Gi. The critical path length of Gi is CP(Gi) = LPi(t).
Due to the isomorphic structure of the chains in G0, all nodes at the same depth j have the same
longest path length from s. Let‚Äôs denote this common length as Lj = LP0(v) for any node v with
depth(v) = j. Since all original edge weights in E0 are strictly positive, it follows that for any two
depths j1 and j2, if j1 < j2, then Lj1 < Lj2. This implies j1 ‚â§j2 ‚áê‚áíLj1 ‚â§Lj2.
The proof proceeds by induction on the number of added edges, k.
Base Case (k=1): We prove the statement for the addition of a single edge e1 = (u, v) to G0 to
form G1.
Sufficient Condition ( =‚áí): Assume depth(u) ‚â§depth(v). By the lemma‚Äôs premise, we are
given that adding e1 results in G1 being a DAG. We must show that CP(G1) = CP(G0).
The longest path to any node x in G1 is given by the recurrence LP1(x) = max(w,x)‚ààE1{LP1(w)+
weight(w, x)}. For node v, this becomes:
LP1(v) = max(LP0(v), LP0(u) + 0)
By definition, LP0(v) = Ldepth(v) and LP0(u) = Ldepth(u). The condition depth(u) ‚â§depth(v)
implies Ldepth(u) ‚â§Ldepth(v).
Thus, LP1(v) = max(Ldepth(v), Ldepth(u)) = Ldepth(v) =
LP0(v). Since the longest path to v is unchanged, and this is the only modification, the longest
paths to all successors of v also remain unchanged. Therefore, LP1(x) = LP0(x) for all x ‚ààV ,
which implies CP(G1) = CP(G0).
Necessary Condition (‚áê=): Assume CP(G1) = CP(G0) and (as per the lemma‚Äôs premise) G1 is
a DAG. We prove the contrapositive: if depth(u) > depth(v), then CP(G1) > CP(G0).
Since G1 is a DAG, adding the edge (u, v) did not create a cycle. The longest path to v becomes:
LP1(v) = max(LP0(v), LP0(u) + 0) = max(Ldepth(v), Ldepth(u))
Since we assume depth(u) > depth(v) and all original edge weights are strictly positive, we have
Ldepth(u) > Ldepth(v). This leads to LP1(v) = Ldepth(u) > Ldepth(v) = LP0(v). The longest
path to v has strictly increased. This increase propagates to all successors of v, including the sink t.
Therefore, LP1(t) > LP0(t), which means CP(G1) > CP(G0). This contradicts our assumption.
Thus, the condition depth(u) ‚â§depth(v) is necessary.
Inductive Hypothesis (IH): Assume for some k ‚â•1, the lemma holds. That is, given that Gk
is a DAG, CP(Gk) = CP(G0) if and only if the condition depth(ui) ‚â§depth(vi) held for all
i ‚àà{1, . . . , k}. We make the stronger hypothesis that if the condition held, then LPk(x) = LP0(x)
for all nodes x ‚ààV .
Inductive Step: We prove the lemma for the addition of the (k + 1)-th edge, ek+1 = (u, v), to Gk
to form Gk+1.
16


--- Page 17 ---
Published as a conference paper at ICLR 2026
Sufficient Condition ( =‚áí): Assume depth(u) ‚â§depth(v). By the lemma‚Äôs premise, we are
given that Gk+1 is a DAG. We must show CP(Gk+1) = CP(Gk).
The longest path to v in Gk+1 is LPk+1(v) = max(LPk(v), LPk(u)+0). By the IH, since the con-
ditions held for the first k edges, we have LPk(v) = LP0(v) = Ldepth(v) and LPk(u) = LP0(u) =
Ldepth(u). The calculation is identical to the base case: LPk+1(v) = max(Ldepth(v), Ldepth(u)) =
Ldepth(v) = LPk(v). The longest path to v is unchanged, and by propagation, LPk+1(x) = LPk(x)
for all x ‚ààV . This maintains our strong hypothesis and proves CP(Gk+1) = CP(Gk) = CP(G0).
Necessary Condition (‚áê=): Assume CP(Gk+1) = CP(Gk) and (as per the lemma‚Äôs premise)
Gk+1 is a DAG. We prove the contrapositive: if depth(u) > depth(v), then CP(Gk+1) >
CP(Gk).
Since Gk+1 is a DAG, adding (u, v) did not create a cycle. We compute LPk+1(v):
LPk+1(v) = max(LPk(v), LPk(u) + 0)
Using the IH (CP(Gk) = CP(G0) implies the conditions held for the first k edges, so LPk(x) =
LP0(x) for all x):
LPk+1(v) = max(LP0(v), LP0(u)) = max(Ldepth(v), Ldepth(u))
Since we assume depth(u) > depth(v), we have Ldepth(u) > Ldepth(v). This leads to LPk+1(v) =
Ldepth(u) > Ldepth(v) = LP0(v) = LPk(v). The longest path to v strictly increases. This in-
crease propagates to the sink node t, so CP(Gk+1) > CP(Gk). This contradicts our assumption.
Therefore, the condition is necessary.
By the principle of induction, the lemma holds for any k ‚â•1.
C
EXACT ALGORITHM AND MODIFICATIONS
We present the exact algorithm in Algorithm 1 in this section. The following pseudocode is adapted
from the original FlashAttention-3 paper (Shah et al., 2024), with all modifications introduced by
DASH explicitly marked.
17


--- Page 18 ---
Published as a conference paper at ICLR 2026
Algorithm 1 DASH algorithm
Require: Matrices Q, K, V, O, dO ‚ààRN√ód in HBM, logsumexp vector L ‚ààRN in HBM, block
sizes Bc, Br.
1: In a preprocessing kernel, compute D = rowsum(dO ‚ó¶O) ‚ààRd (pointwise multiply), write
D to HBM and divide it into Tr blocks D1, . . . , DTr of size Br each.
2: Divide Q into Tr =
l
N
Br
m
blocks Q1, . . . , QTr of size Br √ó d each, and divide K, V in to
Tc =
l
N
Bc
m
blocks K1, . . . , KTc and V1, . . . , VTc, of size Bc √ó d each.
3: Divide dO into Tr blocks dOi, . . . , dOTr of size Br √ó d each, and divide L into Tr blocks
Li, . . . , LTr of size Br each.
4: Initialize pipeline object to manage barrier synchronization with s-stage circular SMEM buffer.
5: if in producer warpgroup then
6:
Deallocate predetermined number of registers.
7:
Issue load Kj and Vj from HBM to shared memory.
8:
Upon completion, commit to notify consumer of the load of Kj and Vj.
9:
for i in assigned Q-tile schedule do [DASH]
10:
Wait for the (i % s)th stage of the buffer to be consumed.
11:
Issue loads of Qi, dOi from HBM to shared memory at the (i % s)th stage of the buffer.
12:
Upon completion, commit to notify consumers of the loads of Qi, dOi.
13:
end for
14: else if in consumer warpgroups then
15:
Reallocate predetermined number of registers as function of number of consumer warps.
16:
On-chip, Initialize dKj = (0)Bc√ód, dVj = (0)Bc√ód .
17:
Wait for Kj and Vj to be loaded in shared memory.
18:
for i in assigned Q-tile schedule do [DASH]
19:
Wait for Qi to be loaded in shared memory.
20:
Load Li, Di from HBM to on-chip SRAM.
21:
On chip, compute S(j)
i
= QiKT
j ‚ààRBr√óBc (SS-GEMM). Commit.
22:
Wait for dOi to be loaded in shared memory.
23:
On chip, compute dP(j)
i
= dOiV‚ä§
j ‚ààRBr√óBc (SS-GEMM). Commit.
24:
On chip, wait for S(j)
i , then compute P(j)
i
= exp(Sij ‚àíLi) ‚ààRBr√óBc.
25:
On chip, wait for dP(j)
i , then compute dS(j)
i
= P(j)
i
‚ó¶(dP(j)
i
‚àíDi) ‚ààRBr√óBc.
26:
On chip, compute dVj ‚ÜêdVj + (P(j)
i )‚ä§dOi ‚ààRBc√ód (RS-GEMM). Commit.
27:
On chip, compute dKj ‚ÜêdKj + dS(j)
i
‚ä§Qi ‚ààRBc√ód (RS-GEMM). Commit and wait
for both dVj and dKj.
28:
On chip, compute dQ(local)
i
= dS(j)
i Kj ‚ààRBr√ód (SS-GEMM), and write dQ(local)
i
to
smem. Notify the dQ-writer.
29:
end for
30: else if in dQ-writer warp then
31:
for i in assigned Q-tile schedule do [DASH]
32:
Wait for dQ(local)
i
to be ready in smem.
33:
Wait until the global order grants this block its turn to reduce.
[DASH]
34:
Using a semaphore, atomically add dQ(local)
i
to dQi in global memory.
35:
Advance the global order.
[DASH]
36:
end for
37: end if
18
