--- Page 1 ---
LLaMEA-SAGE: Guiding Automated Algorithm Design with
Structural Feedback from Explainable AI
Niki van Stein
n.van.stein@liacs.leidenuniv.nl
LIACS, Leiden University
Netherlands
Anna V. Kononova
a.kononova@liacs.leidenuniv.nl
LIACS, Leiden University
Netherlands
Lars Kotthoff
lk223@st-andrews.ac.uk
University of St Andrews
United Kingdom
Thomas B√§ck
t.h.w.baeck@liacs.leidenuniv.nl
LIACS, Leiden University
Netherlands
Abstract
Large language models have enabled automated algorithm de-
sign (AAD) by generating optimization algorithms directly from
natural-language prompts. While evolutionary frameworks such
as LLaMEA demonstrate strong exploratory capabilities across the
algorithm design space, their search dynamics are entirely driven
by fitness feedback, leaving substantial information about the gen-
erated code unused. We propose a mechanism for guiding AAD
using feedback constructed from graph-theoretic and complexity
features extracted from the abstract syntax trees of the generated
algorithms, based on a surrogate model learned over an archive of
evaluated solutions. Using explainable AI techniques, we identify
features that substantially affect performance and translate them
into natural-language mutation instructions that steer subsequent
LLM-based code generation without restricting expressivity.
We propose LLaMEA-SAGE, which integrates this feature-driven
guidance into LLaMEA, and evaluate it across several benchmarks.
We show that the proposed structured guidance achieves the same
performance faster than vanilla LLaMEA in a small controlled exper-
iment. In a larger-scale experiment using the MA-BBOB suite from
the GECCO-MA-BBOB competition, our guided approach achieves
superior performance compared to state-of-the-art AAD methods.
These results demonstrate that signals derived from code can ef-
fectively bias LLM-driven algorithm evolution, bridging the gap
between code structure and human-understandable performance
feedback in automated algorithm design.
CCS Concepts
‚Ä¢ Computing methodologies ‚ÜíHeuristic function construc-
tion; Continuous space search; Natural language generation.
Keywords
Automated Algorithm Design, Large Language Models, Evolution-
ary Computation, Structural code analysis, Explainable AI‚Äìguided
optimization
1
Introduction
Designing effective and efficient optimization algorithms remains
a core challenge in Evolutionary Computation (EC) [2]. While
decades of research have produced highly successful hand-crafted
methods, such as CMA-ES [8], their design typically relies on sub-
stantial human expertise, iterative trial-and-error and implicit do-
main knowledge. This makes systematic algorithm innovation slow,
difficult to replicate and hard to scale across problem classes.
Recent advances in large language models (LLMs) have opened
up a new paradigm for Automated Algorithm Design (AAD), where
algorithm discovery itself is framed as a search problem over exe-
cutable programs. A growing body of work demonstrates that LLMs
can generate, tune hyperparameters, modify and refine heuristics
or algorithms when embedded in structured search procedures
[6, 14, 19, 22, 23]. These approaches show that LLMs can contribute
to algorithmic innovation rather than merely code synthesis.
Despite this progress, several important shortcomings remain.
First, many LLM-based heuristic design methods rely on fixed algo-
rithmic templates, limiting the scope of discoverable algorithms and
biasing the search toward known structures. Second, search strate-
gies often struggle to balance exploration and exploitation in large,
heterogeneous code spaces, leading to either premature stagnation
or inefficient use of expensive LLM queries. Third, while promising
results have been reported, there is still limited understanding of
how to systematically improve LLM-generated algorithms after the
initial generation of the code and how such improvements com-
pare to state-of-the-art LLM-based heuristic design methods under
controlled benchmarking [1].
In this paper, we address some of these limitations by extending
the LLaMEA framework with a structured feedback mechanism.
During the algorithm discovery process, it learns which code fea-
tures of the discovered algorithms are correlated with performance
and may lead to better results. We focus on scalable, template-free
evolution of full black-box optimization algorithms, while explicitly
positioning our approach relative to recent state-of-the-art LLM-
based heuristic design methods, including MCTS-AHD [23] and
LHNS [22].
Our main contributions are as follows:
‚Ä¢ We propose LLaMEA-SAGE which guides the algorithm
discovery process using explainable AI analysis of code
features, providing structured feedback to the LLM.
‚Ä¢ We conduct a systematic experimental comparison against
state-of-the-art LLM-based heuristic design methods, in-
cluding MCTS-AHD and LHNS, using them as strong base-
lines alongside the vanilla LLaMEA.
arXiv:2601.21511v1  [cs.AI]  29 Jan 2026


--- Page 2 ---
van Stein et al.
Document
Section Level 1
1. Introduction
Section Level 1
2. Analysis
Section Level 1
3. Conclusion
Section Level 2
2.1 Title A
Section Level 2
2.2 Title B
Paragraph 1
Paragraph 2
Paragraph 3
Sentence 2.1
Sentence 2.2
Sentence 2.N
Word 2.1.1
Word 2.1,2
Word 2.1.M
Generate an evolutionary
optimization algorithm...
LLM Code Generation
...
def EvolutionaryOptimizer(...):
... if f(x) < best:
Performance Evaluation
Fitness
Evaluate on
Benchmarks
Aggregated Fitness Score
...
Try decreasing the 
cyclometic complexity..
Solution Archive
SHAP Analysis
Example Code
Selected Solution
Population
Selected Solution
Code Features Extraction
Surrogate Modelling
SHAP Analysis
of selected solution
SHAP value
AST Feature
Figure 1: Overview of the proposed method LLaMEA-SAGE.
‚Ä¢ We empirically demonstrate that the proposed approach
improves algorithm quality and robustness on black-box
optimization benchmarks and provide an extensive analysis
of the AAD runs.
All code and results are available in our repository1.
2
Related Work
One of the earliest frameworks in the AAD area is Evolution of
Heuristics (EoH) [6]. EoH integrates LLMs into an evolutionary
computation loop, where candidate heuristics are represented by
both natural-language descriptions and corresponding code frag-
ments. New heuristics are generated through LLM-driven mutation
and crossover operations, guided by performance-based selection.
EoH has demonstrated strong performance on a range of combina-
torial optimization problems and established LLM-based heuristic
evolution as a competitive alternative to hand-crafted designs and
other program search approaches such as FunSearch [13].
Several follow-up works have explored alternative search strate-
gies to better cope with the size and complexity of the heuristic
search space [3]. MCTS-AHD replaces the population-based EC loop
with a Monte Carlo Tree Search (MCTS) formulation, organizing
all generated heuristics in a tree structure and using MCTS to bal-
ance exploration and exploitation [23]. By allowing temporarily
underperforming heuristics to be revisited and refined, MCTS-AHD
mitigates premature stagnation.
Another recent state-of-the-art method in LLM-based AAD is
LLM-driven Heuristic Neighborhood Search (LHNS) [22], which aban-
dons population-based evolution in favor of a single-solution neigh-
borhood search paradigm, iteratively applying ruin-and-recreate
operations to partially destroy and reconstruct heuristic code using
an LLM.
In contrast to these heuristic-centric approaches, LLaMEA [14]
targets automated discovery of complete optimization algorithms
1https://anonymous.4open.science/r/LLaMEA-SAGE/README.md
rather than individual heuristic components. LLaMEA embeds
LLMs within the framework of evolutionary strategies, such as
(1, 1), (1+1) and (ùúá+ùúÜ) schemes, and directly optimizes executable
algorithm implementations using runtime performance on selected
benchmarks as feedback. Unlike EoH-style methods, LLaMEA does
not rely on predefined heuristic slots or algorithmic templates, en-
abling the generation and optimization of larger and more flexible
code bases. On continuous black-box optimization benchmarks,
LLaMEA has been shown to produce algorithms that are compet-
itive with, and in some cases outperform, established methods
such as CMA-ES [7] and differential evolution [15]. Subsequent
extensions, such as LLaMEA-HPO [19], further decouple algorithm
structure discovery from hyperparameter tuning, improving sample
efficiency.
Positioning. The methods above illustrate complementary trade-
offs in LLM-based algorithm design. EoH, MCTS-AHD and LHNS
focus on structured heuristic search within predefined frameworks
and have achieved strong results on combinatorial optimization
problems. LLaMEA, in contrast, emphasizes end-to-end discovery
of full algorithms in continuous black-box optimization. In this
work, we build on the open-source and modular framework of
LLaMEA as a flexible foundation for black-box optimization algo-
rithm design, while using vanilla LLaMEA, MCTS-AHD and LHNS
as state-of-the-art baselines for comparison. This allows us to assess
progress incrementally, relative to leading LLM-based heuristic de-
sign approaches while focusing on scalable, template-free algorithm
discovery.
Code features for algorithm design. Static code analysis is a well-
established technique in software engineering and has been im-
portant in assessing the maintainability and quality of source code
[11]. The features classically used for static code analysis have
also shown their use for better selecting and configuring algo-
rithms [12]. Pulatov et al. [12] introduce an approach to improving
algorithm selection by analysing structural features of algorithms,


--- Page 3 ---
LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI
Algorithm 1 LLaMEA-SAGE
Require: Evaluation function ùëì(¬∑), LLM M, mutation prompts P,
Feature computation function cf(¬∑), Population size ùúá, offspring
size ùúÜ, evaluation budget ùêµ
1: Initialize population ùëÉusing M
2: Evaluate, compute code features (cf) and store (ùë†, ùëì(ùë†), cf(ùë†))
for all ùë†‚ààùëÉin archive A
3: while |A| < ùêµdo
4:
Train surrogate model ÀÜùëìon code features from A
5:
ùëÇ‚Üê‚àÖ
6:
for ùëñ= 1 to ùúÜdo
7:
Select parent ùëù‚ààùëÉ
8:
Sample mutation prompt ùëû‚ààP
9:
Augment ùëûto increase/decrease code feature value with
highest impact on performance given ùëù
10:
Generate offspring ùë†‚Ä≤ using LLM M, ùëùand prompt ùëû
11:
Evaluate ùëì(ùë†‚Ä≤) and extract code features
12:
Add (ùë†‚Ä≤, ùëì(ùë†‚Ä≤), cf(ùë†‚Ä≤)) to archive A
13:
Add ùë†‚Ä≤ to offspring set ùëÇ
14:
end for
15:
Select next population ùëÉfrom ùëÉ‚à™ùëÇ(elitist selection)
16: end while
17: return best solution in A
moving beyond traditional ‚Äúblack-box‚Äù methods that rely solely
on performance observations. The authors demonstrate that incor-
porating algorithm features into the selection process can lead to
performance improvements. We further this idea by integrating
code-based features into automated algorithm design with the goal
of making the complex and open-ended evolution of code more ef-
ficient and using the information gathered during the evolutionary
process as effectively as possible.
3
LLaMEA-SAGE: Structural Code-feature
Guided Algorithm Evolution
We propose a code-features-driven mutation guidance mechanism
for automated algorithm design that augments LLM-based evolu-
tionary search with structural signals extracted from program code
(see Algorithm 1). The method is implemented in the open-source
LLaMEA framework [14] and introduces an archive-based surrogate
model that learns relations between algorithmic code structure and
optimization performance. These relations are analysed and turned
into lightweight natural language descriptions, which augment the
LLM prompt to guide subsequent modifications.
3.1
Algorithm Representation via Abstract
Syntax Trees
Each candidate algorithm is represented as executable Python code.
The correctness of the code is assessed during the evaluation. To
enable learning over structural properties of algorithms, we extract
a rich set of features from the code‚Äôs Abstract Syntax Tree (AST).
Given a solution code ùëê, we parse it into an AST and construct a
directed graph
ùê∫ùëê= (ùëâ, ùê∏),
where nodes correspond to code-feature elements and edges encode
parent‚Äìchild relations in the syntax tree [10].
From ùê∫ùëê, we compute graph-theoretic statistics capturing size,
depth, connectivity and heterogeneity, including:
‚Ä¢ node and edge counts,
‚Ä¢ degree statistics (mean, variance, entropy),
‚Ä¢ tree depth statistics (min, mean, max),
‚Ä¢ clustering coefficients and assortativity,
‚Ä¢ path-based metrics such as diameter and average shortest
path.
In addition, we extract code-level complexity indicators using
static analysis, including cyclomatic complexity, function token
counts and parameter counts aggregated across functions. The final
feature vector for a solution ùë†is
xùë†‚ààRùëë,
where ùëëdenotes the total number of AST and complexity features.
The full code features extraction pipeline is deterministic and light-
weight, enabling reuse during evolution without re-calculating code
features for unchanged solutions per iteration.
3.2
Archive-Based Surrogate Modelling
During evolution, all evaluated solutions including extracted code
features (cf) are stored in an archive A = {(ùë†ùëñ, ùëìùëñ, cfùëñ)}ùëÅ
ùëñ=1, where ùëìùëñ
denotes the observed fitness and cfùëñthe features derived from the
AST and code of ùëñ.
Using the archive, we train a gradient-boosted regression tree
model
ÀÜùëì(ùëêùëì) ‚âàùëì,
using XGBoost [4] with squared-error loss to approximate the code-
features‚Äìfitness relation function ùëìgiven code featuresùëêùëì. Training
is triggered once a minimum archive size is reached to avoid degen-
erate models. This surrogate captures non-linear relations between
code structure and empirical performance.
3.3
Feature Importance via SHAP
To derive actionable mutation guidance, we apply SHAP [9] to
the trained surrogate model. For each feature ùëó, SHAP provides an
attribution value ùúôùëóquantifying its contribution to the predicted
fitness:
ÀÜùëì(x) = ùúô0 +
ùëë
‚àëÔ∏Å
ùëó=1
ùúôùëó.
The feature with the largest absolute attribution is selected and
the sign of its SHAP value determines whether the feature value
should be increased or decreased.
3.4
Guided Mutation Prompting
The resulting guidance is translated into a natural-language in-
struction that augments the standard mutation prompt provided
to the LLM. Formally, if feature ùëòis selected with action ùëé‚àà
{increase, decrease}, the mutation operator is extended with:


--- Page 4 ---
van Stein et al.
Guided Mutation Prompt
‚ÄúBased on archive analysis, try to <a> the <k> of the solution.‚Äù
This guidance does not constrain the LLM syntactically but biases
the generated mutation toward structural regions that are empiri-
cally correlated with improved performance. We do not claim that
these found correlations are causally correct, but even imperfect,
noisy correlations are sufficient to bias exploration beneficially in
early AAD stages as we will show in the results section. The evolu-
tionary loop remains the same as LLaMEA otherwise, preserving
the exploratory nature of the LLM-driven algorithm synthesis.
Overall, the method closes the loop between code structure, ob-
served optimization performance and future algorithm generation,
yielding a data-driven form of inductive bias for automated algo-
rithm design.
Note that while we evaluated our structured guidance approach
with LLaMEA, the general approach is not specific to any particular
framework and could also be used with EoH, MCTS-AHD or LHNS.
4
Experimental setup
We evaluate the proposed code-features driven guidance, LLaMEA-
SAGE, in two AAD experiments designed to isolate its effect and
assess scalability to established benchmarking scenarios. All exper-
iments are done using the IOH-BLADE [18] benchmarking toolbox;
raw results, code and post-processing notebooks are available in
our repository2. Final discovered algorithms are additionally val-
idated on unseen instances using different dimensionalities and
evaluation budgets to assess generalizability. In addition we assess
their performance against SOTA baselines.
4.1
Performance Metric
For all experiments, we use an anytime performance metric, AOCC,
to access the quality of generated black-box optimizers on selected
benchmarks and as a fitness score feedback to the LLM. Eq. 1 gives
the definition of AOCC:
ùê¥ùëÇùê∂ùê∂(ùë¶ùëé,ùëì) = 1
ùêµ
√çùêµ
ùëñ=1

1 ‚àímin(max((ùë¶ùëñ),ùëôùëè),ùë¢ùëè)‚àíùëôùëè
ùë¢ùëè‚àíùëôùëè

(1)
where ùë¶ùëé,ùëìis a series of log-scaled current-best fitness value of ùëì
during the optimization algorithm run ùëé, ùêµis the evaluation budget,
ùë¶ùëñis the ùëñ-th element of ùë¶ùëé,ùëì, ùë¢ùëèis the upper bound of ùëìand ùëôùëèis
the lower bound of ùëì.
4.2
Experiment 1: Vanilla LLaMEA vs.
Feature-Guided LLaMEA-SAGE
Objective. The first experiment isolates the effect of our proposed
feature-guided mutation by comparing a vanilla LLaMEA-based
evolutionary strategy (LLaMEA) against an identical strategy aug-
mented with code-features-based guidance (LLaMEA-SAGE).
Benchmark. We use a subset of the SBOX-COST benchmark
suite [20], focusing on multiple noiseless single objective box-
constrained optimization problems. Functions ùëì‚àà{1, . . . , 5} are
used
2https://anonymous.4open.science/r/LLaMEA-SAGE/README.md
(all separable functions), each evaluated across 5 instances.
Dimensionality is fixed to ùëë= 10.
Methods. Two methods are compared:
‚Ä¢ LLaMEA: Standard LLaMEA with random parent selection
and standard mutation prompts.
‚Ä¢ LLaMEA-SAGE: Identical configuration but now with fea-
ture guided mutation enabled.
Both methods use ùúá= 8 parents, ùúÜ= 8 offspring, elitist selection
and an identical LLM backend (GPT-5-mini). The total evolutionary
budget is fixed to 200 evaluations. The two mutation prompts used
were ‚ÄúGenerate a new algorithm that is different from the algorithms
you have tried before.‚Äù and ‚ÄúRefine the strategy of the selected solution
to improve it.‚Äù For the remainder of the paper we refer to these
prompts as random new and refine respectively. We use the initial
population size (in this case 8) as the minimum archive size before
training a surrogate model.
Evaluation Protocol. Each method (LLaMEA and LLaMEA-SAGE)
is repeated for 5 independent runs with different random seeds. Per-
formance is measured using current-best fitness (AOCC) over each
run and aggregated in the figures using mean and 95% confidence
intervals across runs.
4.3
Experiment 2: Comparison against SOTA
Objective. The second experiment evaluates whether feature-
guided mutation improves performance in a multi-algorithm, multi-
function setting comparing to the latest state-of-the-art methods
in automated algorithm design.
Benchmark. We use the Many Affine BBOB benchmark (MA-
BBOB) [21] as this is also an official GECCO competition and con-
tains a large variety of different functions. In this setup, we evaluate
generated algorithms across multiple affine BBOB functions. Train-
ing is performed on 10 instances, with dimensionality ùëë= 10 and a
budget of 5000 ¬∑ ùëëevaluations per algorithm.
Methods. This experiment positions the proposed method against
established automated algorithm design baselines.
‚Ä¢ LLaMEA-SAGE: LLaMEA with code feature‚Äìdriven muta-
tion guidance. 4 parents and 16 offspring, elitism enabled.
‚Ä¢ LLaMEA: vanilla LLaMEA without feature guidance. 4 par-
ents and 16 offspring, elitism enabled.
‚Ä¢ MCTS-based algorithm synthesis using recommended
parameters from [23].
‚Ä¢ LHNS (Large Neighborhood Search over code), using rec-
ommended parameters from [22].
Each method is executed for 5 independent runs with seeds
{1, . . . , 5}, all using the same LLM backend (GPT-5-mini) and eval-
uation budget. Each generated algorithm as a 1-hour time-limit to
complete (to prevent non-terminating algorithms). All methods are
evaluated on MA-BBOB with identical training instances, dimen-
sionality, evaluation budgets and random seeds. Each method is
allowed the same number of algorithm evaluations (200), ensuring
a fair comparison.
Each method uses the exact same problem formulation prompt
to start with (See Appendix A). Performance is evaluated using
aggregated anytime performance (AOCC) over MA-BBOB tasks,


--- Page 5 ---
LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI
Figure 2: Average best-so-far fitness (AOCC) over time of
the baseline LLaMEA and the proposed code-feature guided
approach (LLaMEA-SAGE) on the SBOX-COST suite. Aggre-
gated over 5 independent runs, the 95% confidence interval
is shown as shaded area.
Figure 3: Average speed-up of LLaMEA-SAGE versus vanilla
LLaMEA. The speed-up is how much faster than LLaMEA
the proposed method is, i.e. if LLaMEA-SAGE reaches fitness
value ùëìùëñin ùëöevaluations and LLaMEA reaches that same
fitness value in ùëõevaluations, the speed-up is ùëõ/ùëö.
following the standard IOHProfiler logging and aggregation proto-
col [5].
5
Results
5.1
Proof of concept experiment
In Figure 2, we can observe the aggregated performance of the
proposed method versus vanilla LLaMEA (see Section 4.2 for the
setup description). We can see that, especially at the beginning of
the AAD process, the code-feature guided mutation seems very
beneficial. While both methods reach similar performance at the
end of the budget, LLaMEA-SAGE achieves better performance
faster and performance varies less across different runs. In Figure
3 we clearly see that LLaMEA-SAGE is beneficial especially at the
beginning of the runs, when looking at the relative speed-up of
LLaMEA-SAGE versus LLaMEA.
Figure 4: Code features and direction used during the 5 dif-
ferent runs to guide the automated design of black-box opti-
mization algorithms.
Figure 5: Consistency between code feature‚Äìguided sugges-
tions and actual code feature mutation by LLaMEA-SAGE for
different mutation prompts (left the random new and right
the refine prompt).
To statistically verify the effect size and significance of our re-
sults, we calculated the area under the convergence curve (AUC)
per seed for LLaMEA-SAGE and LLaMEA. On these per-seed AUC
values, we applied a paired Wilcoxon signed-rank test and addition-
ally report effect sizes (Cliff‚Äôs ùõø) and bootstrap confidence intervals
for the mean AUC difference to account for the limited number of
runs. On SBOX-COST, LLaMEA-SAGE achieves a higher AUC than
LLaMEA, with an average improvement of 11.1 AUC units. The
effect size is large (Cliff‚Äôs ùõø= 0.60), indicating a consistently faster
convergence across runs. However, due to the limited number of
seeds, the 95% bootstrap confidence interval of the AUC difference
remains wide and the paired Wilcoxon test does not reach statistical
significance (ùëù= 0.44).
Next, we examine which code features have been used to steer the
AAD process and with what suggested direction (increase versus
decrease) in the 5 runs. In Figure 4, we can observe that a large
variety of code features has been used to at least some extent, with
total cyclomatic complexity and total parameter count being the
most frequently used code-features. It is notable that the suggested
direction is almost always ‚Äúincrease‚Äù, indicating that increasing
the complexity (one way or another) of the generated solutions
is beneficial for the performance on this task. We emphasize that
this trend is benchmark-specific and should not be interpreted as a
general preference for more complex algorithms.


--- Page 6 ---
van Stein et al.
Figure 6: Code evolution graph of the total cyclomatic com-
plexity of designed algorithms over time for one run of
LLaMEA and LLaMEA-SAGE. Node colour represents nor-
malized fitness where yellow nodes are of high fitness and
dark blue nodes of low fitness. A yellow edge denotes the
use of the refine prompt, light blue denotes the use of the
random new prompt, green denotes an increase of the code
feature and the use of the refine prompt, dark blue denotes
an increase and the use of the random new prompt.
To further investigate our newly proposed code feature‚Äìguidance
method, we calculate the code feature difference between parent
and offspring and how often this is consistent with the provided
prompt guidance message. For example, if LLaMEA-SAGE asks the
LLM to refine the given solution and increase the total token count,
the resulting individual can either have a larger token count than
its parent (Match) or a lower (Mismatch). The results of this anal-
ysis in Figure 5 tells a clear message: when the LLM is informed
to generate a completely new algorithm, it ignores the code fea-
ture guidance completely, while when the refine prompt is used, it
mostly follows orders (but not always).
Using Code Evolution Graphs (CEG) [17], we can analyse in detail
what happens during the different mutation steps. A CEG is a graph
where we display each generated algorithm as a node and each
parent-child relation in the AAD process as an edge between its
parent and child node. The node color represents normalized fitness
where yellow nodes are of high fitness and dark blue nodes of low
fitness. For this paper, we have modified the standard CEG to also
show information about what kind of mutation prompt has been
used, by coloring the edges either yellow (random new prompt)
or light blue (refine prompt). In addition, we use a dark green
color when the prompt contains a guidance to increase a given code
feature together with the refine prompt and a dark blue color when
the guidance contains an increase together with the random new
prompt.
In Figure 6, we can see how the total cyclomatic complexity of
generated algorithms evolves over the run using different muta-
tion prompts for LLaMEA and LLaMEA-SAGE. We track the total
cyclomatic code complexity as the selected feature guidance
metric for the coloring mentioned above. From Figure 6, we can see
that the code feature guidance does cause a larger exploratory effect
in total complexity versus a non-guided mutation (vanilla LLaMEA).
We can also clearly observe that the random new prompt almost al-
ways lead to lower complexity solutions, while the refine prompt is
Figure 7: Average best-so-far fitness (AOCC) over time of
the SOTA baselines and the proposed code feature guided
approach (LLaMEA-SAGE) on the MA-BBOB suite. Averaged
over 5 independent runs.
Figure 8: Average speed-up of LLaMEA-SAGE versus vanilla
LLaMEA on MA-BBOB. The speed-up is how much faster
than LLaMEA the proposed method is, i.e. if LLaMEA-SAGE
reaches fitness value ùëìùëñinùëöevaluations and LLaMEA reaches
that same fitness value in ùëõevaluations, the speed-up is ùëõ/ùëö.
Speed-up values after 50 evaluations are missing because
LLaMEA never reaches the fitness value of LLaMEA-SAGE.
most often increasing the code complexity. Additional CEGs for dif-
ferent code features for all runs are provided in the supplementary
material (Appendix C).
5.2
Many Affine BBOB Competition
Now that we better understand how our proposed mutation guid-
ance technique influences the AAD process, we perform a larger
benchmarking exercise on the MA-BBOB suite. This experiment
aims to validate the proposed method against strong state-of-the-art
baselines.
In Figure 7, we can see the aggregated fitness over time for the
proposed LLaMEA-SAGE and SOTA baselines. Again, we see a per-
formance difference, especially in the beginning of the evolutionary
process, between vanilla LLaMEA and LLaMEA-SAGE as can also
be observed by the speed-up shown in Figure 8. There we can also


--- Page 7 ---
LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI
Figure 9: Code evolution graph of the mean parameter count of designed algorithms over time for the different methods on
MA-BBOB. Colors of nodes and edges are according to Figure 6
see that at only 50 evaluations, LLaMEA-SAGE reaches an average
fitness that LLaMEA cannot achieve even after the full 200 eval-
uations. Both LLaMEA-based algorithms clearly outperform the
LHNS and MCTS-AHD methods on this black-box optimization
algorithm design task.
Also for MA-BBOB, statistical testing reveals that LLaMEA-SAGE
achieves a higher mean AUC than vanilla LLaMEA but this time the
effect size is medium (Cliff‚Äôs ùõø= 0.36), indicating a consistent but
modest improvement. Due to the small number of runs (5 seeds), the
95% bootstrap confidence interval of the AUC difference is wide and
includes zero and the paired Wilcoxon test does not reach statistical
significance (p = 0.44). Due to the substantial computational and
API costs associated with large language model‚Äìbased optimization,
the number of runs was necessarily limited.
Figure 9 shows the CEG for each method, analysing the mean
parameter count code feature on the ùë¶-axis. Note that for LHNS
there is no parent-child relation logged and such there are no edges
in the graph for that method. We observe is that for this particular
code feature again there is a lot of exploration going on in LLaMEA-
SAGE, especially when this feature is mentioned in the guidance
prompt (dark blue and dark green colors). For LHNS, the value of
the feature is relatively stable and low, while for MCTS-AHD, there
are very large jumps in its feature value.
5.3
Token Costs per Method
As the usage of LLMs can be expensive, either in compute or in
API costs, we also compare the number of LLM tokens required
to do a full AAD run per method. In Figure 10, the average total
number of tokens per run is shown. We can observe that LHNS is
the most token efficient method, this is most likely not because of
greater efficiency of the approach, but because it tends to generate
less complex code, which consumes fewer tokens. LLaMEA and
LLaMEA-SAGE are very close to each other, with the additional
code feature guidance increasing the token cost only marginally.
This shows that LLaMEA-SAGE preserves scalability relative to
vanilla LLaMEA. MCTS-AHD has by far the highest token cost
(more than twice as high as LLaMEA) as it queries the LLM several
times per solution, while the other methods only query the LLM
once per generated algorithm.
Figure 10: Average number of LLM tokens used per AAD
method for one run of generating 200 algorithms.
5.4
Validation of Final Best Algorithms
We validate the final best generated algorithm from each approach,
i.e. the generated algorithm with the highest fitness over the 5
independent runs per method (LLaMEA, LLaMEA-SAGE, LHNS and
MCTS). For the final evaluation we use 50 instances from MA-BBOB
(instances 100 to 150) and ùëë= 5, ùëë= 10 and ùëë= 20 with a budget of
2000 ¬∑ùëëfunction evaluations. Two out of the three comparisons are
performed on dimensionalities different from the one used for the
AAD to assess how well the generated algorithms generalise. We
also include in the comparison the winner of the 2025 MA-BBOB
competition (NeighborhoodAdaptiveDE) [16] (specifically tuned
on MA-BBOB 5ùëë).
The empirical attainment function (EAF) based empirical cumu-
lative distribution function (ECDF) curves from the three validation
experiments can be seen in Figure 11. The EAF estimates the per-
centage of runs that attain a given target value no later than a given
runtime. Taking the partial integral of the EAF results in a more
accurate version of the Empirical Cumulative Distribution Function,
as it does not rely on discretization of the targets. Larger EAF values
means that the algorithm performs better. While the best generated
algorithm from the approach we propose here does not beat the
competition winner in 5ùëë(on which NeighborHoodAdaptiveDE
was tuned), it does 10ùëëand 20ùëëand still performs well on 5ùëë, in
particular beating all other automatically designed algorithms.


--- Page 8 ---
van Stein et al.
Figure 11: Empirical attainment function based ECDF curves (higher is better) on 50 MA-BBOB instances in 5ùëë(left plot), 10ùëë
(middle plot) and 20ùëë(right plot) for all best discovered algorithms during the runs of different AAD methods. All algorithms
have been trained only on 10ùëëproblem instances apart from the competition winner, which has been trained on 5ùëë.
6
Conclusions
We introduced LLaMEA-SAGE, a structured feedback mechanism
for automated algorithm design that augments LLM-driven evolu-
tionary search with information extracted from program code. By
analysing abstract syntax tree (AST) and complexity code features
of generated algorithms, learning a surrogate model over an archive
of evaluated solutions and using explainable AI techniques to iden-
tify influential structural properties, we close the loop between code
structure and black-box optimization performance. Crucially, this
feedback is translated into lightweight natural-language guidance
prompts that steers LLM-based mutations without constraining
expressivity or imposing rigid templates.
Our experimental results demonstrate that code feature guided
mutation improves the efficiency and stability of automated algo-
rithm discovery. In controlled experiments, our proposed LLaMEA-
SAGE achieves good performance faster than vanilla LLaMEA under
identical budgets, while extensive analysis using code evolution
graphs shows that the guidance meaningfully biases structural ex-
ploration. In a large-scale comparison on the MA-BBOB benchmark
suite, LLaMEA-SAGE consistently outperforms strong state-of-
the-art baselines, including MCTS-based and large-neighborhood
search approaches, while retaining the flexibility of template-free,
end-to-end algorithm generation. These findings suggest that even
relatively simple structural signals can provide a powerful inductive
bias for LLM-driven algorithm evolution.
Limitations and future work. While the results are promising, sev-
eral limitations remain. First, our experiments are conducted using a
single LLM backend, while ablation studies (see the Supplementary
material Appendix B) on a different LLM backend (Gemini-2.0-flash-
lite) show a consistent outcome; a more systematic study across
more models would be beneficial to assess robustness. Second,
the current evaluation focuses on continuous black-box optimiza-
tion benchmarks at moderate dimensionalities; it remains an open
question how well the proposed guidance generalizes to higher-
dimensional settings, noisy objectives or fundamentally different
problem classes. Third, the code feature set, while deliberately
generic, captures only static structural properties of code and does
not account for dynamic execution behavior or interaction effects
during optimization runs.
These limitations point to several directions for future research.
Promising extensions include combining structural code features
with runtime or behavioral descriptors, exploring adaptive or multi-
feature guidance strategies instead of single-feature attribution
and integrating the proposed mechanism into other LLM-based
algorithm design frameworks beyond LLaMEA. More broadly, we
believe that closing the loop between program analysis, explainable
models and generative LLMs is a key step toward more principled,
interpretable and scalable automated algorithm design.
References
[1] Thomas Bartz-Beielstein, Carola Doerr, Jakob Bossek, Sowmya Chandrasekaran,
Tome Eftimov, Andreas Fischbach, Pascal Kerschke, Manuel Lopez-Ibanez,
Katherine M. Malan, Jason H. Moore, Boris Naujoks, Patryk Orzechowski,
Vanessa Volz, Markus Wagner, and Thomas Weise. 2020. Benchmarking in
Optimization: Best Practice and Open Issues. arXiv.
[2] Thomas H. W. B√§ck, Anna V. Kononova, Bas van Stein, Hao Wang, Kirill A.
Antonov, Roman T. Kalkreuth, Jacob de Nobel, Diederick Vermetten, Roy de
Winter, and Furong Ye. 2023. Evolutionary Algorithms for Parameter Opti-
mization‚ÄîThirty Years Later. Evolutionary Computation 31, 2 (06 2023), 81‚Äì122.
doi:10.1162/evco_a_00325
[3] Dikshit Chauhan, Bapi Dutta, Indu Bala, Niki van Stein, Thomas B√§ck, and
Anupam Yadav. 2025. Evolutionary Computation and Large Language Models: A
Survey of Methods, Synergies, and Applications. arXiv preprint arXiv:2505.15741
(2025).
[4] Tianqi Chen, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu
Cho, Kailong Chen, Rory Mitchell, Ignacio Cano, Tianyi Zhou, et al. 2015. Xg-
boost: extreme gradient boosting. R package version 0.4-2 1, 4 (2015), 1‚Äì4.
[5] Carola Doerr, Hao Wang, Furong Ye, Sander van Rijn, and Thomas B√§ck. 2018.
IOHprofiler: A Benchmarking and Profiling Tool for Iterative Optimization
Heuristics. arXiv e-prints:1810.05281 (oct 2018). arXiv:1810.05281 https://arxiv.
org/abs/1810.05281
[6] Liu Fei, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao
Lu, and Qingfu Zhang. 2024. Evolution of Heuristics: Towards Efficient Automatic
Algorithm Design Using Large Language Model. In International Conference on
Machine Learning (ICML). https://arxiv.org/abs/2401.02051
[7] Nikolaus Hansen. 2023.
The CMA Evolution Strategy: A Tutorial.
arXiv:1604.00772 doi:10.48550/arXiv.1604.00772
[8] Nikolaus Hansen, Sibylle D M√ºller, and Petros Koumoutsakos. 2003. Reducing
the time complexity of the derandomized evolution strategy with covariance
matrix adaptation (CMA-ES). Evolutionary computation 11, 1 (2003), 1‚Äì18.
[9] Scott M. Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M. Prutkin,
Bala Nair, Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. 2020.
From local explanations to global understanding with explainable AI for trees.
Nature Machine Intelligence 2, 1 (2020), 2522‚Äì5839.
[10] Iulian Neamtiu, Jeffrey S Foster, and Michael Hicks. 2005. Understanding source
code evolution using abstract syntax tree matching. In Proceedings of the 2005


--- Page 9 ---
LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI
international workshop on Mining software repositories. 1‚Äì5.
[11] Alberto S Nu√±ez-Varela, H√©ctor G P√©rez-Gonzalez, Francisco E Mart√≠nez-Perez,
and Carlos Soubervielle-Montalvo. 2017. Source code metrics: A systematic
mapping study. Journal of Systems and Software 128 (2017), 164‚Äì197.
[12] Damir Pulatov, Marie Anastacio, Lars Kotthoff, and Holger Hoos. 2022. Opening
the black box: Automated software analysis for algorithm selection. In Interna-
tional Conference on Automated Machine Learning. PMLR, 6‚Äì1.
[13] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov,
Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S
Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, and Alhussein Fawzi.
2024. Mathematical discoveries from program search with large language models.
Nature 625 (01 2024), 468‚Äì475. Issue 7995.
[14] Niki van Stein and Thomas B√§ck. 2025. LLaMEA: A Large Language Model
Evolutionary Algorithm for Automatically Generating Metaheuristics. IEEE
Transactions on Evolutionary Computation 29, 2 (2025), 331‚Äì345. doi:10.1109/
TEVC.2024.3497793
[15] Rainer Storn and Kenneth Price. 1997. Differential evolution‚Äìa simple and
efficient heuristic for global optimization over continuous spaces. Journal of
global optimization 11, 4 (1997), 341‚Äì359.
[16] Niki van Stein. 2025. Neighborhood Adaptive Differential Evolution. In Pro-
ceedings of the Genetic and Evolutionary Computation Conference Companion.
1‚Äì2.
[17] Niki van Stein, Anna V. Kononova, Lars Kotthoff, and Thomas B√§ck. 2025. Code
evolution graphs: Understanding large language model driven design of algo-
rithms. In Proceedings of the Genetic and Evolutionary Computation Conference.
943‚Äì951.
[18] Niki van Stein, Anna V. Kononova, Haoran Yin, and Thomas B√§ck. 2025. BLADE:
Benchmark suite for LLM-driven Automated Design and Evolution of iterative
optimisation heuristics. In Proceedings of the Genetic and Evolutionary Computa-
tion Conference Companion. 2336‚Äì2344.
[19] Niki van Stein, Diederick Vermetten, and Thomas B√§ck. 2025. In-the-loop Hyper-
Parameter Optimization for LLM-Based Automated Design of Heuristics. ACM
Trans. Evol. Learn. Optim. (April 2025). doi:10.1145/3731567 Just Accepted.
[20] Diederick Vermetten, Manuel L√≥pez-Ib√°√±ez, Olaf Mersmann, Richard All-
mendinger, and Anna V. Kononova. 2023. Analysis of modular CMA-ES on
strict box-constrained problems in the SBOX-COST benchmarking suite. In
Proceedings of the Companion Conference on Genetic and Evolutionary Compu-
tation (Lisbon, Portugal) (GECCO ‚Äô23 Companion). Association for Computing
Machinery, New York, NY, USA, 2346‚Äì2353. doi:10.1145/3583133.3596419
[21] Diederick Vermetten, Furong Ye, Thomas B√§ck, and Carola Doerr. 2023. Ma-bbob:
Many-affine combinations of bbob functions for evaluating automl approaches in
noiseless numerical black-box optimization contexts. In International Conference
on Automated Machine Learning. PMLR, 7‚Äì1.
[22] Zhuoliang Xie, Fei Liu, Zhenkun Wang, and Qingfu Zhang. 2025. LLM-Driven
Neighborhood Search for Efficient Heuristic Design. In 2025 IEEE Congress on
Evolutionary Computation (CEC). 1‚Äì8. doi:10.1109/CEC65147.2025.11043025
[23] Zhi Zheng, Zhuoliang Xie, Zhenkun Wang, and Bryan Hooi. 2025.
Monte
Carlo Tree Search for Comprehensive Exploration in LLM-Based Automatic
Heuristic Design. In Proceedings of the 42nd International Conference on Ma-
chine Learning (Proceedings of Machine Learning Research, Vol. 267), Aarti Singh,
Maryam Fazel, Daniel Hsu, Simon Lacoste-Julien, Felix Berkenkamp, Tegan
Maharaj, Kiri Wagstaff, and Jerry Zhu (Eds.). PMLR, 78338‚Äì78373.
https:
//proceedings.mlr.press/v267/zheng25o.html


--- Page 10 ---
van Stein et al.
Supplementary Material for LLaMEA-SAGE
A
Problem Specific Prompts used
This appendix provides the exact problem specification prompts used to initialize the AAD process in our experiments. Including these
prompts verbatim is essential for reproducibility, as they define the optimization task, interface constraints and permissible implementation
details exposed to the LLM.
Appendix A.1 documents the prompt used for the SBOX-COST benchmark in Experiment 1, while Appendix A.2 reports the corresponding
prompt for the MA-BBOB benchmark used in Experiment 2.
All AAD methods compared in this paper (LLaMEA, LLaMEA-SAGE, LHNS, and MCTS-AHD) were initialized using the same problem
specification prompts for a given benchmark. Differences in performance therefore arise from the search strategies and mutation mechanisms
rather than differences in task formulation.
A.1
SBOX
Problem specification prompt
You are an excellent Python programmer.
You are a Python expert working on a new optimization algorithm.
You can use numpy v2 and some other standard libraries.
Your task is to develop a novel heuristic optimization algorithm for continuous optimization problems.
The optimization algorithm should work on different instances of noiseless unconstrained functions.
Your task is to write the optimization algorithm in Python code.
Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound).
The dimensionality can be varied.
The code should contain an `__init__(self, budget, dim)` function with optional additional arguments
and the function `def __call__(self, func)`,
which should optimize the black box function `func` using `self.budget` function evaluations.
The func() can only be called as many times as the budget allows, not more.
An example of such code (a simple random search), is as follows:
```python
import numpy as np
import math
class RandomSearch:
def __init__(self, budget=10000, dim=10):
self.budget = budget
self.dim = dim
def __call__(self, func):
self.f_opt = np.inf
self.x_opt = None
for i in range(self.budget):
x = np.random.uniform(func.bounds.lb, func.bounds.ub)
f = func(x)
if f < self.f_opt:
self.f_opt = f
self.x_opt = x
return self.f_opt, self.x_opt
```
Give an excellent and novel heuristic algorithm to solve this task and also give it a one-line description,
describing the main idea. Give the response in the format:
# Description: <short-description>
# Code:
```python
<code>
```
A.2
MA-BBOB
Problem specification prompt
You are an excellent Python programmer.
You are a Python developer working on a new optimization algorithm.
Your task is to develop a novel heuristic optimization algorithm for continuous optimization problems.
The optimization algorithm should handle a wide range of noiseless functions.
Your task is to write the optimization algorithm in Python code.
Each of the optimization functions has a search space between -5.0 (lower bound) and 5.0 (upper bound).
The dimensionality can be varied.
The code should contain an `__init__(self, budget, dim)` function with optional additional arguments
and the function `def __call__(self, func)`, which should optimize the black box function `func` using `self.budget` function evaluations.
The func() can only be called as many times as the budget allows, not more.
An example of such code (a simple random search), is as follows:
```python
import numpy as np
class RandomSearch:
def __init__(self, budget=10000, dim=10):
self.budget = budget
self.dim = dim
def __call__(self, func):
self.f_opt = np.inf
self.x_opt = None


--- Page 11 ---
LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI
for i in range(self.budget):
x = np.random.uniform(func.bounds.lb, func.bounds.ub)
f = func(x)
if f < self.f_opt:
self.f_opt = f
self.x_opt = x
return self.f_opt, self.x_opt
```
Give an excellent and novel heuristic algorithm to solve this task and also give it a one-line description,
describing the main idea. Give the response in the format:
# Description: <short-description>
# Code:
```python
<code>
```
", "cost": 0.0001131, "tokens": 0}
B
LLM Ablation studies
This appendix reports an additional ablation experiment assessing the robustness of the proposed code feature guided mutation mechanism
on a different LLM backend. While the main paper focuses on results obtained using the GPT-5-mini model, we repeat experiment 2 using
the Gemini-2.0-flash-lite model to evaluate whether the observed performance trends are model-specific.
Figure 12 compares the anytime performance (AOCC) of LLaMEA-SAGE and baseline methods on the MA-BBOB benchmark under both
LLM backends. Although absolute performance levels differ, the relative advantage of code feature guided mutation remains consistent.
These results support the claim that the proposed guidance mechanism captures structural signals that generalize across LLM implemen-
tations, rather than exploiting idiosyncrasies of a single model.
Figure 12: Average best-so-far fitness (AOCC) over time of the SOTA baselines and the proposed code feature guided approach
(LLaMEA-SAGE) on the MA-BBOB suite using on the left gpt-5-nano vs on the right gemini-flash-2.0-lite. Averaged over 5
independent runs.
C
Additional Code Evolution Graphs
This appendix provides additional Code Evolution Graphs (CEGs) to complement the analyses presented in Section 5. While the main paper
highlights representative runs and selected code features, the figures in this appendix offer a more comprehensive view across benchmarks,
runs and structural properties.
First we show CEGs for the SBOX-COST benchmark, including the evolution of features such as mean parameter count, maximum degree
and token count over time. Next we report the corresponding graphs for the MA-BBOB benchmark.
Across these figures, node color encodes normalized fitness, while edge color indicates the mutation prompt type and whether code
feature guidance requested an increase or decrease of the selected feature (only for LLaMEA-SAGE). Together, these visualizations illustrate
how code feature guidance systematically biases structural exploration during the AAD process, while preserving substantial diversity in the
generated algorithms.


--- Page 12 ---
van Stein et al.
Figure 13: Code evolution graph of the max degree code feature of designed algorithms over time for SBOX-COST. Node colour
represents normalized fitness where yellow nodes are of high fitness and dark blue nodes of relative low fitness. A yellow edge
denotes the use of the refine prompt, light blue denotes the use of the random new prompt, green denotes an increase of the
code feature and the use of the refine prompt, dark blue denotes an increase and the use of the random new prompt
Figure 14: Code evolution graph of the mean parameter count code feature of designed algorithms over time for SBOX-COST.
Node colour represents normalized fitness where yellow nodes are of high fitness and dark blue nodes of relative low fitness.
A yellow edge denotes the use of the refine prompt, light blue denotes the use of the random new prompt, green denotes an
increase of the code feature and the use of the refine prompt, dark blue denotes an increase and the use of the random new
prompt


--- Page 13 ---
LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI
Figure 15: Code evolution graph of the mean parameter count code feature of designed algorithms over time for MA-BBOB.
Node colour represents normalized fitness where yellow nodes are of high fitness and dark blue nodes of relative low fitness.
A yellow edge denotes the use of the refine prompt, light blue denotes the use of the random new prompt, green denotes an
increase of the code feature and the use of the refine prompt, dark blue denotes an increase and the use of the random new
prompt


--- Page 14 ---
van Stein et al.
Figure 16: Code evolution graph of the total number of code tokens of designed algorithms over time for MA-BBOB. Node
colour represents normalized fitness where yellow nodes are of high fitness and dark blue nodes of relative low fitness. A yellow
edge denotes the use of the refine prompt, light blue denotes the use of the random new prompt, green denotes an increase of
the code feature and the use of the refine prompt, dark blue denotes an increase and the use of the random new prompt
