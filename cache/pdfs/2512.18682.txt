--- Page 1 ---
Solver-Independent Automated Problem Formulation via LLMs for
High-Cost Simulation-Driven Design
Yuchen Li1, Handing Wang1, Bing Xue2, Mengjie Zhang2 and Yaochu Jin3
1School of Artificial Intelligence, Xidian University, China
2School of Engineering and Computer Science, Victoria University of Wellington, New Zealand
3School of Engineering, Westlake University, China
ycli_7@stu.xidian.edu.cn, hdwang@xidian.edu.cn, bing.xue@ecs.vuw.ac.nz
mengjie.zhang@ecs.vuw.ac.nz, jinyaochu@westlake.edu.cn
Abstract
In the high-cost simulation-driven design do-
main, translating ambiguous design require-
ments into a mathematical optimization formu-
lation is a bottleneck for optimizing product
performance. This process is time-consuming
and heavily reliant on expert knowledge. While
large language models (LLMs) offer potential
for automating this task, existing approaches
either suffer from poor formalization that fails
to accurately align with the design intent or rely
on solver feedback for data filtering, which is
unavailable due to the high simulation costs.
To address this challenge, we propose APF, a
framework for solver-independent, automated
problem formulation via LLMs designed to au-
tomatically convert engineers’ natural language
requirements into executable optimization mod-
els. The core of this framework is an innova-
tive pipeline for automatically generating high-
quality data, which overcomes the difficulty of
constructing suitable fine-tuning datasets in the
absence of high-cost solver feedback with the
help of data generation and test instance an-
notation. The generated high-quality dataset
is used to perform supervised fine-tuning on
LLMs, significantly enhancing their ability to
generate accurate and executable optimization
problem formulations. Experimental results
on antenna design demonstrate that APF sig-
nificantly outperforms the existing methods in
both the accuracy of requirement formalization
and the quality of resulting radiation efficiency
curves in meeting the design goals.
1
Introduction
High-cost simulation-driven design is prevalent in
numerous fields, such as antenna (Lei et al., 2019),
aerospace (Lyu et al., 2015; Li et al., 2022), micro-
electronics (Barros et al., 2010), and robotics (Li
and Feng, 2020). As illustrated in Figure 1(a), a
common task in product design across these fields
is to optimize design parameters, ensuring that
Requirements
Equations Code
Solver
Human
(a) Manual Formulation of Requirements
Feedback Result
Mathematical Model: 
• Objective  
 
• Constraint 
Fi(Px(z))
Gi(Px(z))
Black-Box  
Simulation
Design  
Parameter x
Performance Metric 
Px(z) :
 (Unoptimizable)
x →Px(z )
 (Optimizable)
x →Fi(Px(z ))/Gi(Px(z ))
z
z
:
Requirements
 Actual Result
Expected Result
Requirements
Equations Code
Solver
API-based LLM
(b) API-based LLM Formalization of Requirements 
 Actual Result
Expected Result
LLM
LLM
Requirements
Equations Code
Solver
Fine-tuned LLM
Training Data
SFT
LLM
Training Data
SFT
Score
Solver-based Evaluation
Solver-independent Evaluation
Test Instance 
Annotation
Score
Ours
Others
 Actual Result
Expected Result
(c) Fine-tuned LLM Formalization of Requirements 
Figure 1: Formalizing Requirements in High-Cost
Simulation-Driven Design: From Manual Expertise to
LLM-based Workflow
the performance distribution under given evalua-
tion variables (e.g., frequency, angle) satisfies spe-
cific design requirements. In practice, this perfor-
mance distribution is typically obtained through
high-fidelity simulations and manifests itself as
high-dimensional curves, which are difficult for op-
timization algorithms to use directly. Therefore, it
is necessary to formalize the design requirements
into an executable mathematical model to serve as
the objectives or constraints for an optimization
algorithm. However, since design requirements are
often provided in unstructured natural language,
and the formalization process is time-consuming,
1
arXiv:2512.18682v1  [cs.CL]  21 Dec 2025


--- Page 2 ---
and highly dependent on engineering expertise, au-
tomating this procedure remains a significant chal-
lenge.
The development of large language models
(LLMs), such as GPT (OpenAI et al., 2024),
Gemini (Team et al., 2025, 2024), and DeepSeek
(DeepSeek-AI et al., 2025), offers a promising av-
enue to automatically formulate optimization prob-
lems. The existing work in this domain can be
broadly divided into prompt-based (Figure 1(b))
and fine-tuning-based (Figure 1(c)) methods. Early
work such as Chain-of-Experts (Xiao et al., 2023)
and OptiMUS (Ahmaditeshnizi et al., 2024) ex-
plore prompt-based methods, where the model
is guided to generate optimization problems by
carefully designing input prompts. Fine-tuning-
based methods such as LLaMoCo (Ma et al., 2024),
ORLM (Huang et al., 2025), LLMOPT (Jiang et al.,
2024), and SIRL (Chen et al., 2025) improve the
performance of LLMs by training on task-specific
data, enabling better understanding and generation
of optimization problems. These studies show that
fine-tuned models with moderate sizes (e.g., 7 bil-
lion parameters) can sometimes perform better than
larger general-purpose models like GPT-4. This
highlights the advantage of fine-tuning in improv-
ing the accuracy and reliability of problem formu-
lation for optimization tasks.
Although the previous studies have explored the
methods for automatically converting optimization
problems described in natural language into mathe-
matical models using LLMs, most of these meth-
ods focus on operational optimization problems
such as linear programming and integer program-
ming, which differ significantly in their problem
description and evaluation costs from the high-cost
simulation-driven design scenarios. Therefore, the
application of these methods is limited in complex
industrial design scenarios. In particular, prompt-
based methods struggle to accurately identify ob-
jectives and constraints when faced with natural
language requirements that are vague or heavily
reliant on domain-specific knowledge. While fine-
tuning-based methods improve the LLM’s ability
to handle structured optimization tasks, they face
limitations in high-cost simulation-driven design
scenarios where the cost of solver feedback pre-
vents effective data quality filtering.
To address the modeling challenges in high-cost
simulation-driven design, we propose a framework
for automated problem formulation, called APF.
This framework utilizes a fine-tuned LLM to auto-
matically convert engineers’ natural language re-
quirements into accurate and executable optimiza-
tion problem formulations. Our method comprises
three main contributions: (1) We develop an au-
tomated framework combining data augmentation
and instance annotation to construct high-quality
datasets for LLM fine-tuning. (2) We introduce a
solver-independent evaluation module that utilizes
LLM reasoning to assess formulation quality, elim-
inating the need for expensive solver feedback. (3)
We apply SFT on the dataset and evaluate APF on
industrial optimization task. Results demonstrate
that our method consistently outperforms existing
approaches in accuracy.
2
Related Work
2.1
LLMs for Automated Problem
Formulation
The NL4Opt competition (Ramamonjison et al.,
2023, 2022) provides an early benchmark for trans-
lating natural-language descriptions into formal
optimization representations.
Early studies fo-
cused on leveraging zero-shot or few-shot reason-
ing through prompt engineering to automate the
modeling process. For instance, Chain-of-Experts
(Xiao et al., 2023) employs a multi-agent frame-
work to decompose complex reasoning tasks, while
OptiMUS (Ahmaditeshnizi et al., 2024) designs
domain-specific prompt templates tailored to linear
and mixed-integer programming. More recent ef-
forts have shifted toward fine-tuning LLMs through
supervised or reinforcement learning.
ORLM
(Huang et al., 2025) and OPTMATH (Lu et al.,
2025) construct large-scale synthetic datasets using
semi-automated pipelines to enable SFT. LLMOPT
(Jiang et al., 2024) combines multi-instruction
learning with alignment techniques to improve the
generalization of LLMs in mathematical modeling
tasks. SIRL (Chen et al., 2025) further introduces
reinforcement learning with verifiable rewards to
enhance the reliability of LLMs in optimization
problem formulation. However, these approaches
are often constrained by the need for extensive ex-
pert annotation or instantaneous solver feedback,
both of which are impractical in simulation-driven
design due to the scarcity of domain experts and
the high computational cost of evaluation.
2.2
LLM-Based Synthetic Data Generation
In recent years, LLM-based synthetic data genera-
tion has been widely adopted across a range of tasks
2


--- Page 3 ---
For a frequency 
within the range of 
4.18 GHz to 4.31 
GHz, I expect the 
radiated power should  
be maximized.
Requirements 
For a frequency within 
the range of 4.31 GHz 
to 4.35 GHz, I expect 
the radiated power 
should be less than 
10.5 db.
…
Data Generation
Equations
def obj1(data):
   mask = (4.18 <= data[:,0]) & (data[:,0] <= 4.31)
   return -np.max(data[mask, 1])
def c1(data):
   mask = (4.31 <= data[:,0]) & (data[:,0] <= 4.35)
   return np.max(data[mask, 1])-10.5
…
Rewriter
The radiated power shall be maximized across the 4.18 GHz 
to 4.31 GHz frequency range.
The radiated power shall not exceed 10.5 dB within the 4.31 
GHz to 4.35 GHz band.
…
Rewrite Requirements 
Original Sample
Input Context:
Output Context:
Augment Sample
Input Context:
Output Context:
Generator
Simulation 
Dataset
Data Evaluation and Selection
Reject
Accept
Equations
…
def ...
def ...
0.9
0.5
0.2
Equation Rank
Score
#Task Instruction:…
{human example}
{Test Instances Data}
{Requirements}
Judge Prompt
1
2
3
Human Rank 
Example
Index
Test Instances
Judge
Test Instance Annotation
Highly 
correlated  
Weakly 
correlated  
LLM Rank
Score = 0.5
Supervised Fine-Tuning
Training 
Dataset
Supervised 
Fine-Tuning
...
1
3
2
1
2
3
Data 
Sample
Figure 2: Overview of the APF framework. (a) Data Generation: Design requirements are derived from
the simulation dataset and rewritten by the LLM to produce corresponding model equations. (b) Test Instance
Annotation: For each requirement, a set of test instances is generated and annotated with reference rankings by
the LLM. (c) Data Evaluation and Selection: Generated equations are evaluated against LLM-based rankings,
and high-quality samples are selected to construct the training set. (d) Supervised Fine-Tuning: Dataset is used to
fine-tune an open-source LLM, significantly enhancing its capability to generate accurate and executable design
formulations.
(Long et al., 2024; Liu et al., 2024). For domain-
specific fine-tuning, generating high-quality and
task-relevant synthetic data is crucial for enhancing
model performance and generalization (Wang et al.,
2023; Zelikman et al., 2022). Current approaches
to synthetic data generation using LLMs can be
broadly categorized into two types: one generates
structured task data from scratch by leveraging the
internal knowledge of LLMs (Li et al., 2024; Xu
et al., 2024); the other uses seed inputs to guide
the generation process, allowing for greater control
and diversity in the output (Luo et al., 2023; Mitra
et al., 2024; Abdin et al., 2024).
In the field of automated theorem proving, the
methods such as DeepSeek-Prover (Xin et al.,
2024) and LLM-ATPH (Lai et al., 2025) construct
formalized reasoning trajectories to produce high-
quality synthetic datasets, significantly improving
the proof capabilities of LLMs. In the domain
of automated problem formulation, recent works
like ORLM (Huang et al., 2025), OptMATH (Lu
et al., 2025) and LLMOPT (Jiang et al., 2024)
have adapted these strategies to synthesize large-
scale modeling datasets by combining expert labels
with LLM augmentations. Nonetheless, their re-
liance on execution-based filtering is not scalable
for simulation-driven design, where verification
entails expensive physics simulations.
3
The New Method
To address the misalignment between design intent
and optimization models in simulation-driven de-
sign, we introduce APF. A primary challenge in
this domain is the acquisition of reliable training
data, where reliance on numerical solvers makes
large-scale validation extremely expensive. APF
addresses this with a test instance-based strategy
that avoids solver-based verification and enables
data selection without expensive simulations. We
fine-tune LLMs to align natural language design
specifications with formal modeling equations. As
illustrated in Figure 2, our framework consists of
four integral modules: data generation, test in-
stance annotation, data evaluation and selection,
and supervised fine-tuning, culminating in a model
capable of automating high-cost simulation prob-
lem formulation.
3.1
Representation
Both the data generation and test instance anno-
tation modules rely on extracting design require-
ments and test instances from the historical sim-
ulation dataset.
However, industrial specifica-
tions are typically unstructured and highly context-
dependent. To address this and ensure the frame-
work’s generalizability across various design con-
texts, we introduce a unified abstract definition for
both requirements and instances in APF.
3


--- Page 4 ---
3.1.1
Natural Language Requirement
Representation
To standardize the design descriptions contained
in the simulation dataset into consistent natural-
language requirements, we formalize each require-
ment using a structured tuple:
r = (Z, M, C),
(1)
where Z is a specific subregion of the evaluation
variable z (e.g., the passband of a frequency do-
main), M : z ∈Z →R is a metric function
(e.g., radiation efficiency), and C specifies the de-
sign intent, such as a threshold constraint (e.g.,
minz∈Z M(z) ≥1.5) or an optimization goal (e.g.,
maxz∈Z M(z)). While this formalization captures
the essential meaning of each requirement, real-
world engineering designs rarely hinge on just one.
Instead, they often involve multiple requirements
that may be interdependent or even conflicting. To
handle this complexity, we define a complete de-
sign requirements set R composed of multiple re-
quirement statements.
R = {r1, r2, . . . , rn},
(2)
To construct physically feasible requirement sets,
we derive these values directly from a simulation
dataset. Instead of randomly synthesizing parame-
ters, we treat existing simulation outcomes as de-
sign targets. Specifically, for a given simulation
sample, we extract its operating conditions to de-
fine Z and its performance metrics to populate
C. This data-driven approach ensures that every
constructed R corresponds to a valid, physically
solvable design.
3.1.2
Test Instance Representation
We define a test instance I as the performance
response of a design solution derived from high-
fidelity simulations. Formally, this is expressed
as:
I = S(x),
(3)
where x is design parameter, and S represents the
computationally expensive solver (e.g., an elec-
tromagnetic full-wave simulator).
Unlike sim-
ple scalar metrics, I typically appears as a high-
dimensional curve or vector over the evaluation
domain. Since designers often impose specific con-
straints on different regions of this curve, the qual-
ity of a design depends on how well its response
shape matches these targets. Given a requirement
set R, we can determine the quality of each in-
stance based on its compliance. Thus, for a set of
test instances I = {I1, I2, . . . , Im}, the require-
ments R determine a ranking πR, ordering the in-
stances by their satisfaction of the design intent.
3.2
Data Generation
To effectively train LLMs for specialized domains
like industrial design, the primary challenge lies in
generating a large, diverse, and structured training
dataset from ambiguous, human-expressed require-
ments. Based on the representation defined above,
we construct a diverse set of training samples from
the historical simulation dataset. Specifically, we
first extract the structured requirement tuples from
simulation records, and then leverage LLMs to gen-
erate the corresponding mathematical equations.
Moreover, we perform data augmentation to gener-
ate diverse training samples and enhance the gener-
alization ability of LLMs.
3.2.1
Equation Generation
To translate natural language design requirements
R into precise mathematical equations E
=
{e1, e2, . . . , en}, we proceed as follows.
Our
method leverages an LLM guided by a structured
prompt template. This template embeds R along
with clear instructions and rich contextual informa-
tion, enabling the LLM to process the prompt and
generate the corresponding mathematical formula-
tion. The detailed prompt template is provided in
Appendix A.1. We generate a base dataset of N
samples, denoted as Dbase = {(Ri, Ei)}N
i=1.
3.2.2
Data Augmentation
We introduce a data augmentation strategy to align
the dataset with simulation-driven design com-
plexity. Our strategy couples semantic paraphras-
ing, which captures linguistic variations in require-
ments, with order permutation to model structural
diversity inherent in engineering specifications.
1. Semantic Paraphrasing. To enhance robust-
ness in handling diverse requirement descrip-
tions, we employ an LLM to rewrite each re-
quirement ri in the original set R into v se-
mantically equivalent variants. We constrain
this process using prompt engineering to en-
sure that all variables, constants, and units re-
main unchanged. Subsequently, we randomly
sample combinations of these variants to con-
struct an augmented set Raug containing l de-
sign requirements.
4


--- Page 5 ---
2. Order Permutation.
While the semantic
meaning of requirements is invariant to their
order, LLMs frequently demonstrate sensitiv-
ity to input sequences. To mitigate this, we
randomly permute the requirements and their
corresponding equations in tandem. This ap-
proach prevents the model from relying on
spurious positional cues, ensuring it attends to
the semantic content rather than the sequence
order.
Formally, for each base sample (Ri, Ei) ∈Dbase,
we generate l augmented samples {(R′
i,j, Ei)}l
j=1.
The final training dataset is constructed by com-
bining the base and augmented data: D = Dbase ∪
SN
i=1{(R′
i,j, Ei)}l
j=1.
3.3
Solver-independent Evaluation
Given the modality gap between natural language
requirements and mathematical equations, direct
assessment of semantic alignment is computation-
ally intractable. To address this, we introduce a
set of test instances I as a bridge, reframing the
alignment challenge into a quantifiable ranking con-
sistency problem. Our core assumption is that an
equation faithful to the design intent must yield an
execution ranking on I that consistently matches
the reference ranking defined by the requirements.
3.3.1
Test Instance Annotation
Obtaining a reliable reference ranking conditioned
on specific requirements R is essential for evalua-
tion. Given the limited scalability of manual anno-
tation, we use LLMs to automatically generate ref-
erence rankings. Specifically, we adapt a listwise
ranking strategy rather than pairwise comparisons.
Unlike pairwise methods that often lack global
context, the listwise approach enables the LLM to
evaluate the entire test instance set I jointly. This
perspective minimizes logical inconsistencies and
captures complex trade-offs effectively. We formu-
late this procedure as a conditional generation task
guided by a structured prompt P. To simulate ex-
pert reasoning, the prompt consists of four logical
components:
P = Ptask ⊕Pexpert ⊕ψtab(I) ⊕ψreq(R),
(4)
where ⊕denotes string concatenation, and ψ(·)
represents the functions that convert structured data
into text. The components are defined as follows:
1. Task Instruction Ptask: Provides the system-
level prompt that defines the problem scope.
It directs the LLM to function as a domain
expert, outlining the logical procedure for bal-
ancing conflicting requirements to produce a
reliable ranking.
2. Human Example Pexpert: Contains a one-shot
human expert example. This part guides the
model to focus on key features by showing
how experts balance conflicting metrics.
3. Instances Data ψtab(I): Lists the physical at-
tributes of all test instances I in a table format.
This structure supports horizontal comparison
and helps generate the listwise ranking.
4. Requirements Query ψreq(R): Includes the
current R in the context. This serves as the
basis for ranking and ensures the evaluation
standards fit the current task.
Finally, the reference ranking πLLM is obtained
by maximizing the posterior probability under the
distribution Pθ of the LLM:
πLLM = arg max
π
Pθ(π | P).
(5)
This generated sequence serves as the reference
ranking for evaluating the quality of the generated
equations.
3.3.2
Data Evaluation and Selection
To guarantee the reliability of our training data,
we propose a ranking-based metric to evaluate the
alignment between the generated formulation E
and the ground-truth design intent. Given a set
of test instances I, we calculate the objective val-
ues and constraint violations using formulation E.
The predicted ranking πE is derived through a hi-
erarchical sorting strategy: feasible solutions are
prioritized over infeasible ones, followed by non-
dominated sorting based on Pareto dominance (Yu,
1974). This maps the mathematical properties of
E directly to a sequence of solution quality. We
define the quality score of E as the Spearman cor-
relation coefficient between the predicted ranking
πE and the reference ranking πLLM:
S(E) = ρ(πE, πLLM),
(6)
where a higher correlation implies that E accu-
rately captures the intent of the design require-
ments.
Based on this metric, we filter Dbase to construct
a high-quality dataset, DHQ. We only retain sam-
ples that show a strong correlation with the design
5


--- Page 6 ---
requirements, discarding any triplets (Ri, Ei) that
fail to meet this standard. To ensure consistency, if
a base sample is removed, its corresponding l aug-
mented samples are also excluded. Finally, DHQ is
used for model fine-tuning.
4
Case Study: Antenna Design
To validate our method on a high-cost simulation-
driven design task, we select antenna design as
a case study. This problem involves optimizing
structural parameters x to shape the radiation effi-
ciency curve across the frequency domain (El Mis-
ilmani et al., 2020). A critical challenge in this
domain is the gap between high-level engineering
intent and executable mathematical formulations.
Engineers typically describe requirements quali-
tatively (e.g., demanding a flat passband or deep
stopband rejection) rather than providing explicit
objective functions. Manually translating these
ambiguous, multi-regional requirements into pre-
cise constraints for algorithmic optimization is both
error-prone and labor-intensive. Specifically, we
focus on a representative three-layer filtering patch
antenna (Liang et al., 2022). As illustrated in Fig-
ure 3, the radiation efficiency curve is characterized
by five distinct frequency bands {Zi}5
i=1: a Low
Stopband, a Low Radiation Null, a Passband, a
High Radiation Null, and a High Stopband. This
case constitutes a rigorous test for our method be-
cause the design goals across these bands are het-
erogeneous and strictly conflicting: 1) Passband:
Maximize radiation efficiency while maintaining
curve flatness to ensure stable signal transmission.
2) Radiation Nulls: Suppress efficiency to mini-
mal levels within two narrow bands to filter specific
interference. 3) Stopbands: Consistently suppress
efficiency across wide flanking bands to prevent
out-of-band noise.
4.1
Experiment Settings
To comprehensively assess APF, we conduct sys-
tematic comparisons against two mainstream cat-
egories of advanced methods. The first category
includes prompting-based approaches, which eval-
uate the zero-shot reasoning ability of large lan-
guage models. Specifically, we test direct zero-
shot prompting on proprietary models (e.g., GPT-
4o, DeepSeek-V3), as well as strong optimization-
oriented prompting frameworks such as Chain-of-
Experts (Xiao et al., 2023) and OptiMUS (Ahma-
diteshnizi et al., 2024). The second category cov-
Frequency(GHz)
Efficiency(dB)
Passband
Radiation Nulls
Stopbands
Expect Direction
Figure 3: An example radiation efficiency curve is di-
vided into five key frequency bands, each with distinct
design requirements.
ers our proposed fine-tuning-based APF methods.
To ensure fairness, we adapt several state-of-the-
art open-source LLMs as unified backbones, in-
cluding Llama-3.1-8B-Instruct (Grattafiori et al.,
2024), Qwen2.5-7B-Instruct (Qwen et al., 2025),
and Mistral-7B-Instruct (Jiang et al., 2023). These
models are used both for fine-tuning in APF and
for evaluating prompting-based baselines, allowing
us to directly assess the gains introduced by fine-
tuning. It is worth noting that other fine-tuning
strategies such as ORLM (Huang et al., 2025),
which require frequent interactions with external
solvers, are excluded from our comparison due to
the prohibitively high cost of running simulations
in our industrial setting.
We sampled 2,300 design requirement sets from
historical antenna simulations. This collection was
randomly split into a test set of 300 samples (en-
suring zero overlap) and a candidate training set
of 2,000 samples. Subsequently, APF generated
corresponding mathematical equations specifically
for the 2,000 training requirement sets to construct
the base dataset Dbase. To ensure the reliability of
the training data, we analyzed the quality scores of
Dbase, as shown in Figure 4. The results indicate
that most scores cluster between 0.9 and 1.0. This
high consistency between the generated equations
and design requirements confirms the effectiveness
of our data generation process. To strictly maintain
data quality for fine-tuning, we applied a selec-
tion threshold of 0.7, a value generally considered
the lower bound for strong correlation. Samples
exceeding this threshold were retained and subse-
quently augmented, resulting in a final high-quality
dataset DHQ, comprising 7,879 samples.
6


--- Page 7 ---
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
Quality Score
0
200
400
600
800
Number of Samples
Reject
Accept
1.0
0.8
0.6
0.4
0.2
0.0
0.2
0.4
0.6
0
25
50
75
100
125
Figure 4: The distribution of quality scores for the sam-
ples.
4.1.1
Evaluation Metrics
We propose an alignment metric A(E) to quantify
how well the generated equation set E satisfies
the design requirements. The set E is partitioned
into a subset of objective functions Eobj (size n1)
and constraints Econ (size n2). We conduct evalua-
tions on a test instance set comprising K selected
antenna radiation efficiency curves. For objective
functions, the relative ordering of candidates is
paramount. We utilize the Spearman rank corre-
lation (ρ) to measure the agreement between the
ranking induced by each generated objective ei (ˆπi)
and the ground truth ranking (π∗):
Aobj(E) = 1
n1
X
ei∈Eobj
ρ(ˆπi, π∗).
(7)
For constraints, the focus is on the accurate deter-
mination of feasibility. We define the alignment
score as the classification accuracy. For each con-
straint ej, we compute the agreement between the
predicted feasibility vector ˆyj and the ground truth
y∗:
Acon(E) = 1
n2
X
ej∈Econ

1 −1
m∥ˆyj −y∗∥1

,
(8)
where ∥· ∥1 denotes the L1 norm. Finally, the
total alignment score is calculated as A(E) =
αAobj(E) + (1 −α)Acon(E), where the hyperpa-
rameter α ∈[0, 1] balances the importance of ob-
jectives and constraints. For all these metrics, a
larger value indicates better performance. In our
experiments, we set α = 0.5.
Method
Aobj
Acon
A
Baselines
GPT-4o
0.6055 (0.2425)
0.7075 (0.2270)
0.6651 (0.1879)
DeepSeek-V3
0.7404 (0.2688)
0.7690 (0.1839)
0.7518 (0.2129)
Chain-of-Experts
0.7426 (0.2410)
0.7453 (0.1865)
0.7252 (0.2309)
Optimus
0.6341 (0.2051)
0.6986 (0.2433)
0.6687 (0.1737)
Open-Source LLMs
LLAMA3.1-8B
-0.0453 (0.6470)
0.5029 (0.1985)
0.2248 (0.4288)
Qwen2.5-7B
0.3542 (0.3208)
0.7333 (0.1129)
0.5292 (0.1458)
Mistral-7B
0.0733 (0.4403)
0.4936 (0.1713)
0.3007 (0.3644)
Ours (APF) based on open-source LLMs
LLAMA3.1-8B
0.8012 (0.1059)
0.7969 (0.1720)
0.7976 (0.1228)
Qwen2.5-7B
0.7990 (0.1106)
0.7959 (0.1739)
0.7961 (0.1243)
Mistral-7B
0.7974 (0.1171)
0.7883 (0.1912)
0.7918 (0.1345)
Table 1: Overall performance of APF fine-tuned models
compared to baselines and open-source LLMs. Results
are reported as mean (std), with best in bold.
4.2
Results
4.2.1
Quality of Generated Formulations
We evaluate our proposed APF method by
fine-tuning three open-source instruct models
(LLAMA3.1-8B, Qwen2.5-7B, and Mistral-7B) on
our dataset DHQ. We then compare these fine-tuned
models on the test set against four strong baselines
and their original base models. As shown in Ta-
ble 1, our method consistently and significantly im-
proves performance across all 7B and 8B models.
Significantly, the fine-tuned LLAMA3.1-8B model
demonstrates the most notable improvement, with
its overall score increasing from 0.2248 to 0.7976.
This represents a notable improvement over its base
model and outperforms the evaluated baselines.
These results demonstrate two key findings.
First, the synthetic data from APF significantly im-
proves models’ ability to formalize design require-
ments, overcoming limitations of pretrained mod-
els. Second, our approach enables smaller open-
source models to match or exceed the performance
of larger state-of-the-art models on domain-specific
industrial design tasks.
4.2.2
Antenna Design Performance
To assess the real-world utility of our method, we
design an optimization task based on the practical
antenna design scenario. The task requires LLMs
to generate mathematical formulations for a con-
strained optimization problem, which aims to max-
imize passband radiation efficiency while adhering
to strict power limits in stopband regions. The
natural language description of the design require-
ments is shown in Figure 5(a). The mathematical
formulations corresponding to the generated code
of each method are illustrated in Figure 5(b). We
7


--- Page 8 ---
Minimize
max
f∈[0.95, 1.08] −Px( f )
Subject to
max
f∈[0.95, 1.08] (−Px( f ) −4.49) ≤0
max
f∈[0.80, 0.92] (Px( f ) + 4.39) ≤0
max
f∈[1.08, 1.12] (Px( f ) + 11.74) ≤0
Minimize
avgf∈[0.95, 1.08] −Px( f )
Subject to
min
f∈[0.95, 1.08] −Px( f ) + 4.49 ≤0
max
f∈[0.80, 0.92] Px( f ) + 4.39 ≤0
max
f∈[1.08, 1.12] Px( f ) + 11.74 ≤0
Minimize
max
f∈[0.95, 1.08] −Px( f )
Subject to
max
f∈[0.95, 1.08] (−Px( f ) + 4.49) ≤0
max
f∈[0.80, 0.92] (Px( f ) + 4.39) ≤0
max
f∈[1.08, 1.12] (Px( f ) + 11.74) ≤0
Minimize
avgf∈[0.95, 1.08] −Px( f )
Subject to
min
f∈[0.95, 1.08] −Px( f ) −4.49 ≤0
max
f∈[0.80, 0.92] Px( f ) + 4.39 ≤0
max
f∈[1.08, 1.12] Px( f ) + 11.74 ≤0
Minimize
avgf∈[0.95, 1.08] −Px( f )
Subject to
min
f∈[0.95, 1.08] −Px( f ) −4.49 ≤0
max
f∈[0.80, 0.92] Px( f ) + 4.39 ≤0
min
f∈[1.08, 1.12] Px( f ) + 11.74 ≤0
DeepSeek-V3
APF(LLAMA3.1-8B)
GPT-4o
Chain-of-Experts
Optimus
Minimize
avgf∈[0.95, 1.08] −Px( f )
Subject to
avgf∈[0.95, 1.08]Px( f ) + 4.49 ≤0
avgf∈[0.80, 0.92]Px( f ) + 4.39 ≤0
avgf∈[1.08, 1.12]Px( f ) −11.74 ≤0
LLAMA3.1-8B
Design Requirement: “The radiated power within the frequency 
range of 0.95 Hz to 1.08 Hz in the passband should exceed -4.49 
dB. The objective is to maximize the radiated power for passband 
frequencies from 0.95 Hz to 1.08 Hz. In the 0.8 Hz to 0.92 Hz low-
frequency stopband, the radiated power must be below -4.39 dB. 
Radiated power should be under -11.74 dB for the high-frequency 
radiation null between 1.08 Hz and 1.12 Hz.”
(a) Natural Language Description of Design Requirements.
(b) Optimization Models Generated by Different Methods.
(c) Radiation Efficiency of Different Methods.
 Passband Compliant: LLM-APF
 Low Stopband Compliant: All
 High Radiation Null Compliant:  
  LLM-APF
Figure 5: Comparison of radiation efficiency curves optimized using formulations generated by different methods.
then feed each generated formulation into a scal-
able constrained Bayesian optimization (SCBO)
solver (Eriksson and Poloczek, 2021) to obtain the
final optimized antenna performance. The results
are shown in Figure 5(c). As shown in Figure 5(c),
the radiation efficiency curve obtained from the
optimization model generated by SF-APF satisfies
all design requirements. In contrast, the formula-
tions from other methods fail to meet the design
requirements for the passband and high radiation
null. This demonstrates the superior ability of SF-
APF to accurately formulate design requirements
into effective mathematical models, leading to op-
timized designs that better align with design intent.
4.3
Ablation Study
We ablate Data Augmentation and Data Selection
by removing each component and fine-tuning the
same backbone. Table 2 shows that both compo-
nents contribute to Aobj, Acon, and overall A.
Impact of Data Augmentation As shown in
Table 2, removing the data augmentation module
(denoted as "w/o Augmentation") leads to a de-
crease in performance across all metrics. This indi-
cates that data augmentation positively influences
the model’s performance. By generating diverse
phrasings of design requirements, it improves the
model’s generalization ability to handle varied user
inputs.
Impact of Data Selection According to Table
2, removing the data selection module (denoted as
"w/o Selection") also results in performance degra-
dation. This demonstrates that filtering synthetic
Method
Aobj
Acon
A
w/o Augmentation
0.7656
0.7555
0.7553
w/o Selection
0.7603
0.7800
0.7653
APF
0.8009
0.7971
0.7976
Table 2: Ablation results of APF with Augmentation
and Selection removed.
data is a crucial step in the fine-tuning process. The
synthetic dataset may contain low-quality or inac-
curate design requirement-equation pairs, which
introduce noise. By filtering out these inferior ex-
amples, the data selection module ensures that the
model focuses on high-quality data, leading to bet-
ter overall performance.
5
Conclusions
This paper addresses the challenge of translat-
ing ambiguous natural language requirements into
precise mathematical optimization models. We
present APF, a solver-independent framework that
enhances formulation accuracy by synthesizing
high-quality datasets for fine-tuning, eliminating
the need for costly solver feedback. Experimental
results on antenna design demonstrate that APF sig-
nificantly outperforms baselines, yielding design
outcomes that align more closely with engineering
intent. This work provides a scalable and efficient
path for automating domain-specific problem for-
malization.
8


--- Page 9 ---
Limitations
While APF shows promise in automating prob-
lem formulation, two constraints warrant men-
tion. First, our current evaluation focuses exclu-
sively on antenna design. Although this domain
effectively represents high-cost, simulation-driven
tasks, we plan to validate the framework in broader
physics-based engineering fields, such as aerody-
namics and structural optimization, to rigorously
assess its cross-domain generalizability. Second,
the solver-independent evaluation relies on con-
structing prompts with detailed test instances. This
approach is inherently bounded by the context win-
dow of current LLMs, potentially limiting the capa-
bility to process highly complex problem descrip-
tions or large-scale validation datasets.
References
Marah Abdin, Jyoti Aneja, Harkirat Behl, S\ ’ebastien
Bubeck, Ronen Eldan, Suriya Gunasekar, Michael
Harrison, Russell J. Hewett, Mojan Javaheripi, Piero
Kauffmann, and 1 others. 2024. Phi-4 Technical
Report. Preprint, arXiv:2412.08905.
Ali Ahmaditeshnizi, Wenzhi Gao, and Madeleine Udell.
2024. OptiMUS: Scalable Optimization Modeling
with (MI)LP Solvers and Large Language Models.
In Proceedings of the 41st International Conference
on Machine Learning, pages 577–596. PMLR.
Manuel Barros, Jorge Guilherme, and Nuno Horta. 2010.
Analog circuits optimization based on evolutionary
computation techniques. Integration, 43(1):136–155.
Yitian Chen, Jingfan Xia, Siyu Shao, Dongdong Ge, and
Yinyu Ye. 2025. Solver-Informed RL: Grounding
Large Language Models for Authentic Optimization
Modeling. Preprint, arXiv:2505.11792.
DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingx-
uan Wang, Bochao Wu, Chengda Lu, Chenggang
Zhao, Chengqi Deng, Chenyu Zhang, and 1 others.
2025. DeepSeek-V3 Technical Report. Preprint,
arXiv:2412.19437.
Hilal M. El Misilmani, Tarek Naous, and Salwa K.
Al Khatib. 2020. A review on the design and op-
timization of antennas using machine learning al-
gorithms and techniques. International Journal of
RF and Microwave Computer-Aided Engineering,
30(10):e22356.
David Eriksson and Matthias Poloczek. 2021. Scalable
Constrained Bayesian Optimization. In Proceedings
of The 24th International Conference on Artificial
Intelligence and Statistics, pages 730–738. PMLR.
Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,
Abhinav Pandey, Abhishek Kadian, Ahmad Al-
Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten,
Alex Vaughan, and 1 others. 2024. The Llama 3 Herd
of Models. Preprint, arXiv:2407.21783.
Chenyu Huang, Zhengyang Tang, Shixi Hu, Ruoqing
Jiang, Xin Zheng, Dongdong Ge, Benyou Wang, and
Zizhuo Wang. 2025. ORLM: A Customizable Frame-
work in Training Large Models for Automated Op-
timization Modeling. Operations Research, page
opre.2024.1233.
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, and 1 others. 2023.
Mistral 7B. Preprint, arXiv:2310.06825.
Caigao Jiang, Xiang Shu, Hong Qian, Xingyu Lu, Jun
Zhou, Aimin Zhou, and Yang Yu. 2024. LLMOPT:
Learning to Define and Solve General Optimization
Problems from Scratch. In The Thirteenth Interna-
tional Conference on Learning Representations.
Junyu Lai, Jiakun Zhang, Shuo Xu, Taolue Chen, Zi-
hang Wang, Yao Yang, Jiarui Zhang, Chun Cao, and
Jingwei Xu. 2025. LLM-based Automated Theorem
Proving Hinges on Scalable Synthetic Data Genera-
tion. Preprint, arXiv:2505.12031.
Shiwen Lei, Yaohui Yang, Haoquan Hu, Zhiqin Zhao,
Bo Chen, and Xiangdong Qiu. 2019. Power Gain
Optimization Method for Wide-Beam Array Antenna
via Convex Optimization. IEEE Transactions on
Antennas and Propagation, 67(3):1620–1629.
Haoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun
Wang, Xingxing Zhang, Haoyang Huang, Shaohan
Huang, Xiaolong Huang, Zeqiang Huang, Dongdong
Zhang, and 1 others. 2024. Synthetic Data (Almost)
from Scratch: Generalized Instruction Tuning for
Language Models. Preprint, arXiv:2402.13064.
Jichao Li, Xiaosong Du, and Joaquim R.R.A. Mar-
tins. 2022. Machine learning in aerodynamic shape
optimization.
Progress in Aerospace Sciences,
134:100849.
Shengqin Li and Xinyuan Feng. 2020. Study of struc-
tural optimization design on a certain vehicle body-
in-white based on static performance and modal anal-
ysis. Mechanical Systems and Signal Processing,
135:106405.
Gen-Zhu Liang, Fu-Chang Chen, Hang Yuan, Kai-Ran
Xiang, and Qing-Xin Chu. 2022. A High Selectivity
and High Efficiency Filtering Antenna With Con-
trollable Radiation Nulls Based on Stacked Patches.
IEEE Transactions on Antennas and Propagation,
70(1):708–713.
Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe
Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng,
Diyi Yang, Denny Zhou, and 1 others. 2024. Best
Practices and Lessons Learned on Synthetic Data.
Preprint, arXiv:2404.07503.
9


--- Page 10 ---
Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao
Ding, Gang Chen, and Haobo Wang. 2024.
On
LLMs-Driven Synthetic Data Generation, Curation,
and Evaluation: A Survey. In Findings of the As-
sociation for Computational Linguistics: ACL 2024,
pages 11065–11082, Bangkok, Thailand. Association
for Computational Linguistics.
Hongliang Lu, Zhonglin Xie, Yaoyu Wu, Can Ren,
Yuxuan Chen, and Zaiwen Wen. 2025.
Opt-
MATH: A Scalable Bidirectional Data Synthesis
Framework for Optimization Modeling. Preprint,
arXiv:2502.11102.
Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo
Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qing-
wei Lin, and Daxin Jiang. 2023. WizardCoder: Em-
powering Code Large Language Models with Evol-
Instruct. In The Twelfth International Conference on
Learning Representations.
Zhoujie Lyu, Gaetan K. W. Kenway, and Joaquim R.
R. A. Martins. 2015. Aerodynamic Shape Optimiza-
tion Investigations of the Common Research Model
Wing Benchmark. AIAA Journal, 53(4):968–985.
Zeyuan Ma, Hongshu Guo, Jiacheng Chen, Guojun
Peng, Zhiguang Cao, Yining Ma, and Yue-Jiao Gong.
2024. LLaMoCo: Instruction Tuning of Large Lan-
guage Models for Optimization Code Generation.
Preprint, arXiv:2403.01131.
Arindam Mitra, Luciano Del Corro, Guoqing Zheng,
Shweti Mahajan, Dany Rouhana, Andres Codas,
Yadong Lu, Wei-ge Chen, Olga Vrousgos, Corby
Rosset, and 1 others. 2024. AgentInstruct: Toward
Generative Teaching with Agentic Flows. Preprint,
arXiv:2407.03502.
OpenAI, Josh Achiam, Steven Adler, Sandhini Agar-
wal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
Aleman, Diogo Almeida, Janko Altenschmidt, Sam
Altman, and 1 others. 2024. GPT-4 Technical Report.
Preprint, arXiv:2303.08774.
Qwen, An Yang, Baosong Yang, Beichen Zhang,
Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan
Li, Dayiheng Liu, Fei Huang, and 1 others.
2025.
Qwen2.5 Technical Report.
Preprint,
arXiv:2412.15115.
Rindra Ramamonjison, Haley Li, Timothy Yu, Shiqi
He, Vishnu Rengan, Amin Banitalebi-dehkordi, Zirui
Zhou, and Yong Zhang. 2022. Augmenting Opera-
tions Research with Auto-Formulation of Optimiza-
tion Models From Problem Descriptions. In Proceed-
ings of the 2022 Conference on Empirical Methods in
Natural Language Processing: Industry Track, pages
29–62, Abu Dhabi, UAE. Association for Computa-
tional Linguistics.
Rindranirina Ramamonjison, Timothy Yu, Raymond
Li, Haley Li, Giuseppe Carenini, Bissan Ghaddar,
Shiqi He, Mahdi Mostajabdaveh, Amin Banitalebi-
Dehkordi, Zirui Zhou, and 1 others. 2023. NL4Opt
Competition: Formulating Optimization Problems
Based on Their Natural Language Descriptions. In
Proceedings of the NeurIPS 2022 Competitions Track,
pages 189–203. PMLR.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-
Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan
Schalkwyk, Andrew M. Dai, Anja Hauth, Katie
Millican, and 1 others. 2025. Gemini: A Family
of Highly Capable Multimodal Models. Preprint,
arXiv:2312.11805.
Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan
Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer,
Damien Vincent, Zhufeng Pan, Shibo Wang, and 1
others. 2024. Gemini 1.5: Unlocking multimodal
understanding across millions of tokens of context.
Preprint, arXiv:2403.05530.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa
Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh
Hajishirzi. 2023. Self-Instruct: Aligning Language
Models with Self-Generated Instructions. Preprint,
arXiv:2212.10560.
Ziyang Xiao, Dongxiang Zhang, Yangjun Wu, Lilin Xu,
Yuan Jessica Wang, Xiongwei Han, Xiaojin Fu, Tao
Zhong, Jia Zeng, Mingli Song, and 1 others. 2023.
Chain-of-Experts: When LLMs Meet Complex Op-
erations Research Problems. In The Twelfth Interna-
tional Conference on Learning Representations.
Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren,
Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and
Xiaodan Liang. 2024. DeepSeek-Prover: Advanc-
ing Theorem Proving in LLMs through Large-Scale
Synthetic Data. Preprint, arXiv:2405.14333.
Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yun-
tian Deng, Radha Poovendran, Yejin Choi, and
Bill Yuchen Lin. 2024. Magpie: Alignment Data
Synthesis from Scratch by Prompting Aligned LLMs
with Nothing. In The Thirteenth International Con-
ference on Learning Representations.
P. L. Yu. 1974. Cone convexity, cone extreme points,
and nondominated solutions in decision problems
with multiobjectives. Journal of Optimization Theory
and Applications, 14(3):319–377.
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.
Goodman. 2022. STaR: Bootstrapping Reasoning
With Reasoning. Preprint, arXiv:2203.14465.
10


--- Page 11 ---
A
Prompt Templates for APF Framework
This appendix presents the specific prompt tem-
plates used in our Automated Problem Formulation
(APF) framework. To ensure reproducibility and
transparency, we provide the exact instructions in-
put to the Large Language Models (LLMs). These
prompts cover two distinct phases of our pipeline:
1. Data Generation Phase: Used to generate
training dataset mapping natural language re-
quirements to executable code (Section A.1).
2. Evaluation Phase: Used to establish refer-
ence rankings for test instances via an LLM-
as-a-Judge approach (Section A.2).
A.1
Data Generation
This section provides the detailed prompt templates
used for data generation in our framework. For data
augmentation, we instruct the language model to
rewrite each technical requirement into multiple
alternative phrasings while strictly preserving the
core technical meaning, all numerical values, and
units. This ensures that the model is exposed to a
wide range of natural language expressions without
altering the underlying semantics. For equation
generation, we provide a prompt that guides the
model to translate natural language requirements
into executable Python functions suitable for nu-
merical optimization. The prompt specifies the ex-
pected input and output formats, as well as the con-
ventions for defining objective and constraint func-
tions. The full prompt templates for both stages are
shown in Listings 1 and 2.
You are an expert antenna systems
engineer with a strong command of
technical English. Your task is to
act as a writing assistant.
You will be given a numbered list of
technical design requirements. For
each requirement in the list , you
must generate exactly {num_versions}
distinct , alternative phrasings.
Critical Rules:
Preserve Core Meaning: The fundamental
physical and technical meaning of
each requirement must be preserved
EXACTLY. Keep All Numbers and Units:
All numerical values (e.g.,
frequencies like 2.45 GHz , power
levels like
-10 dB, thresholds like
-4.5) and their units (GHz , Hz, dB ,
etc.) MUST remain UNCHANGED.
Maintain Conditions: The type of
condition ('less than ', 'greater
than ', 'maximized ', 'minimized ')
must not be altered.
Format:
I will provide the requirements as a
numbered list. Your response MUST be
a single , valid JSON object. Do not
add any introductory text or
explanations outside of the JSON
structure. The keys of the JSON
object must be the number from the
input list. The value for each key
must be a JSON array of strings ,
where each string is one of the
rewritten versions of the
requirement.
Now , please generate {num_versions}
versions for the following
requirements:
{requirements_text}
Listing 1: Prompt for Data Augmentation
You are an expert in engineering
optimization and scientific
computing. Your task is to act as a
bridge between natural language
engineering specifications and
executable Python code. Translate
the following engineering design
requirements into Python functions
suitable for a numerical
optimization library like NumPy.
Each function should be self -
contained and operate on a 2D NumPy
array.
Following requirements:
{requirements_text}
You should adhere to the following rules
General Rules:
1.
Your output MUST be a valid JSON
array
`[...]`.
2.
Each object in the array corresponds
to one numbered design requirement
from the input.
3.
Do not output any text or code
outside of the main JSON array.
JSON Object Schema:
Each object in the array must contain
the following five keys:
1.
`"requirement_index"` (integer): The
original number of the requirement
(e.g., 1, 2).
2.
`"function_type"` (string): Either
`"objective"` or `"constraint"`.
Choose the most appropriate one.
3.
`"function_name"` (string): The name
of the function (e.g., `"obj1"`, `"
c2"`).
5.
`"code"` (string): The complete ,
self -contained Python function. This
string MUST be properly escaped for
JSON , especially newlines (`\\n`)
and quotes (`\\"`).
11


--- Page 12 ---
Python Function Rules:
1.
Signature: Use type hints , e.g., `
def obj1(data: np.ndarray) -> float
:`.
2.
Input: The function must accept a
single argument `data `, which is a 2
D NumPy array.
3.
Objectives: Must be for minimization
. To maximize a metric , minimize its
negative.
4.
Constraints: Must be satisfied when
the function 's return value is `<
0`.
5.
Dependencies: Use `numpy ` for all
array operations. Assume `import
numpy as np` is already executed.
Listing 2: Prompt for Equation Generation
A.2
Test Instance Annotation
This section provides the detailed prompt template
used for test instance annotation in our solver-
independent evaluation. Given a requirement set
R and a collection of test instances I (simulated
performance curves), we instruct the LLM to act
as a domain expert and produce a listwise ranking
over all curves. The prompt explicitly separates
objective and constraint requirements and enforces
a two-stage procedure: (1) check feasibility against
all constraints; (2) rank feasible curves by how well
they satisfy the objectives, while ranking infeasible
curves by the degree of constraint satisfaction. This
structured instruction reduces logical inconsisten-
cies and yields a reliable reference ranking πLLM
for downstream correlation-based data selection.
The full prompt template is shown in Listing 3.
Task Instruction:
You are {{ Expert_Role }}.
Your task is to comprehensively evaluate
and rank multiple curves
considering both objective and
constraint requirements.
[Optional: Few -Shot Example]
{{ Example_Section }}
Now , please evaluate the following
curves:
Design Requirements:
Objective Requirements (to be optimized)
:
{{ List_of_Objectives }}
Constraint Requirements (must be
satisfied):
{{ List_of_Constraints }}
Evaluation Strategy:
Rule 1: Check Feasibility.
Evaluate each curve against all
Constraint Requirements , separate
the curves into two groups: 1.
Feasible curves: those that satisfy
all Constraint Requirements. 2.
Violating curves: those that fail to
meet one or more Constraint
Requirements.
The feasible curves will be ranked later
according to 'Rule 2: Rank the
Satisfying Curves '.
The violating curves will be ranked
later according to 'Rule 3: Rank the
Violating Curves '.
Rule 2: Rank Feasible Curves.
Among the curves that satisfy all
Constraint Requirements , rank them
solely based on how well they
collectively satisfy all Objective
Requirements , without further
considering the constraints.
Rule 3: Rank Violating Curves.
If some curves violate the constraints ,
they should be ranked at the bottom.
If all curves fail to satisfy every
constraint , you should still rank
them. In this case , rank them based
on how closely they satisfy all
constraint requirements overall.
The curve that most nearly meets the
full set of constraints should be
ranked highest within this group.
Do not rank them according to the
objective requirements in this
situation.
Curve Data (JSON Format):
Each curve is represented as an object
with: "curve ": identifier , "data":
list of [{{ Axis_X_Name }}, {{
Axis_Y_Name }}]. {{ Data_Description }}
```json
{{ JSON_Data_of_Curves }}
Listing 3: Prompt for Test Instance Annotation
B
Detail Setting of APF
In the data augmentation phase of APF, for a set
of design requirements R, the number of rewritten
versions of the design requirements is set to 3, and
we select l = 5 design requirement sets from all
possible combinations as the augmented result of
R.
We implement all model training using the
PyTorch framework, with base models being
LLAMA3.1-8B, Qwen2.5-7B, and Mistral-7B,
which have 7-8 billion parameters.
We use 4
NVIDIA V100 Tensor Core GPUs with 32 GB each
for model training and 4 V100 GPUs for model in-
ference. The hyperparameters during training are
shown in Table 3.
12


--- Page 13 ---
Table 3: Detail training settings for SFT.
Parameter
Value
LoRA_Dropout
0.05
LoRA_R
16
LoRA_Alpha
32
LearningRate
2.00E-04
BatchSize
16
MaxLength
1600
Epochs
2
C
Detailed Results
In this section, we present the executable code gen-
erated by different automatic problem formulation
methods for the same set of design requirements
in a real antenna design task. We include results
from several mainstream approaches, including
APF (LLAMA3.1-8B), Optimus, Chain-of-Experts,
GPT-4o, DeepSeek-V3, and the base LLAMA3.1-
8B, covering both objective and constraint func-
tions implemented in Python.
Each code example is given as a function, where
the input is a two-dimensional NumPy array: the
first column represents the evaluation variable
(such as frequency), and the second column is the
corresponding performance metric (such as radia-
tion efficiency). The objective functions are imple-
mented for minimization, and the constraint func-
tions are satisfied when their return value is less
than zero. These code samples allow for a direct
comparison of the accuracy, readability, and practi-
cal usability of different methods in modeling.
C.1
Results of APF(LLAMA3.1-8B)
Listing 4: Code for APF(LLAMA3.1-8B)


def obj1(data: np.ndarray) -> float:
"""Minimizes the negative of the
mean power in the 0.95 -1.08 Hz
passband."""
mask = (data[:, 0] >= 0.95) & (data
[:, 0] <= 1.08)
if not np.any(mask):
return 0.0
passband_power = data[mask , 1]
return -np.mean(passband_power)
def c1(data: np.ndarray) -> float:
"""Checks if the minimum power in
the 0.95 -1.08 Hz passband is
greater than
-4.49 dB."""
mask = (data[:, 0] >= 0.95) & (data
[:, 0] <= 1.08)
if not np.any(mask):
return 0.0
min_power_in_passband = np.min(data[
mask , 1])
limit = -4.49
return limit - min_power_in_passband
def c2(data: np.ndarray) -> float:
"""Checks if the maximum power in
the 0.8 -0.92 Hz stopband is less
than
-4.39 dB."""
mask = (data[:, 0] >= 0.8) & (data
[:, 0] <= 0.92)
if not np.any(mask):
return 0.0
max_power_in_stopband = np.max(data[
mask , 1])
limit = -4.39
return max_power_in_stopband - limit
def c3(data: np.ndarray) -> float:
"""Checks if the minimum power in
the 1.08 -1.12 Hz radiation zero
points is less than
-11.74 dB.""
"
mask = (data[:, 0] >= 1.08) & (data
[:, 0] <= 1.12)
if not np.any(mask):
return 0.0
min_power_in_zero_points = np.min(
data[mask , 1])
limit =
-11.74
return min_power_in_zero_points -
limit


C.2
Results of Optimus
Listing 5: Code for Optimus


def passband_power_maximization(data: np
.ndarray) -> float:
# Requirement 2: Maximize the
radiated power for passband
frequencies (0.95 Hz to 1.08 Hz)
mask = (data[:, 0] >= 0.95) & (data
[:, 0] <= 1.08)
power_in_range = data[mask , 1]
if len(power_in_range) == 0:
return np.inf
return -np.max(power_in_range)
def passband_power_constraint(data: np.
ndarray) -> float:
# Requirement 1: Ensure power >
-4.49 dB for passband
frequencies (0.95 Hz to 1.08 Hz)
mask = (data[:, 0] >= 0.95) & (data
[:, 0] <= 1.08)
power_in_range = data[mask , 1]
if len(power_in_range) == 0:
return np.inf
return np.max(-power_in_range -
4.49)
def low_stopband_power_constraint(data:
np.ndarray) -> float:
# Requirement 3: Ensure power <
-4.39 dB for low -frequency
stopband (0.8 Hz to 0.92 Hz)
13


--- Page 14 ---
mask = (data[:, 0] >= 0.8) & (data
[:, 0] <= 0.92)
power_in_range = data[mask , 1]
if len(power_in_range) == 0:
return np.inf
return np.max(power_in_range -
( -4.39))
def high_radiation_zero_power_constraint
(data: np.ndarray) -> float:
# Requirement 4: Ensure power <
-11.74 dB for high -frequency
radiation zero points (1.08 Hz
to 1.12 Hz)
mask = (data[:, 0] >= 1.08) & (data
[:, 0] <= 1.12)
power_in_range = data[mask , 1]
if len(power_in_range) == 0:
return np.inf
return np.max(power_in_range -
( -11.74))


C.3
Results of Chain-of-Experts
Listing 6: Code for Chain-of-Experts


def obj2(data: np.ndarray) -> float:
"""Minimizes the negative of the
mean power in the 0.95 -1.08 Hz
passband to maximize power."""
mask = (data[:, 0] >= 0.95) & (data
[:, 0] <= 1.08)
passband_power = data[mask , 1]
return -np.mean(passband_power)
def constraint1(data: np.ndarray) ->
float:
"""Ensures the power is greater than
-4.49 dB in the 0.95 -1.08 Hz
passband."""
mask = (data[:, 0] >= 0.95) & (data
[:, 0] <= 1.08)
passband_power = data[mask , 1]
return -np.min(passband_power) +
4.49
def constraint3(data: np.ndarray) ->
float:
"""Ensures the power is less than
-4.39 dB in the 0.8 -0.92 Hz low -
frequency stopband."""
mask = (data[:, 0] >= 0.8) & (data
[:, 0] <= 0.92)
stopband_power = data[mask , 1]
return np.max(stopband_power) + 4.39
def constraint4(data: np.ndarray) ->
float:
"""Ensures the power is less than
-11.74 dB in the 1.08 -1.12 Hz
high -frequency radiation zero
points."""
mask = (data[:, 0] >= 1.08) & (data
[:, 0] <= 1.12)
zero_points_power = data[mask , 1]
return np.max(zero_points_power) +
11.74


C.4
Results of GPT-4o
Listing 7: Code for GPT-4o


def maximize_passband_power(data: np.
ndarray):
mask = (data[:, 0] >= 0.95) & (data
[:, 0] <= 1.08)
selected_power = data[mask , 1]
return -np.max(selected_power)
def constraint_passband_power(data: np.
ndarray):
mask = (data[:, 0] >= 0.95) & (data
[:, 0] <= 1.08)
selected_power = data[mask , 1]
return np.max(-selected_power +
4.49)
def constraint_low_frequency_stopband(
data: np.ndarray):
mask = (data[:, 0] >= 0.8) & (data
[:, 0] <= 0.92)
selected_power = data[mask , 1]
return np.max(selected_power + 4.39)
def constraint_radiation_zero_points(
data: np.ndarray):
mask = (data[:, 0] >= 1.08) & (data
[:, 0] <= 1.12)
selected_power = data[mask , 1]
return np.max(selected_power +
11.74)


C.5
Results of DeepSeek-V3
Listing 8: Code for DeepSeek-V3


def objective_maximize_passband_power(
data: np.ndarray) -> float:
mask = (data[:, 0] >= 0.95) & (data
[:, 0] <= 1.08)
passband_power = data[mask , 1]
return -np.mean(passband_power)
def constraint_passband_min_power(data:
np.ndarray) -> float:
mask = (data[:, 0] >= 0.95) & (data
[:, 0] <= 1.08)
passband_power = data[mask , 1]
return -np.min(passband_power) -
4.49
def constraint_low_stopband_max_power(
data: np.ndarray) -> float:
mask = (data[:, 0] >= 0.8) & (data
[:, 0] <= 0.92)
stopband_power = data[mask , 1]
return np.max(stopband_power) + 4.39
def constraint_high_stopband_max_power(
data: np.ndarray) -> float:
mask = (data[:, 0] >= 1.08) & (data
[:, 0] <= 1.12)
stopband_power = data[mask , 1]
return np.max(stopband_power) +
11.74


14


--- Page 15 ---
C.6
Results of LLAMA3.1-8B
Listing 9: Code for LLAMA3.1-8B


def obj1(data: np.ndarray) -> float:
"""Minimizes the negative of the
mean power in the 0.95 -1.08 GHz
passband."""
mask = (data[:, 0] >= 0.95) & (data
[:, 0] <= 1.08)
passband_power = data[mask , 1]
return -np.mean(passband_power)
def con1(data: np.ndarray) -> float:
"""Ensures the power in the
0.95 -1.08 GHz passband is
greater than
-4.49 dB."""
mask = (data[:, 0] >= 0.95) & (data
[:, 0] <= 1.08)
passband_power = data[mask , 1]
return np.mean(passband_power) +
4.49
def con2(data: np.ndarray) -> float:
"""Ensures the power in the
0.80 -0.92 Hz stopband is less
than
-4.39 dB."""
mask = (data[:, 0] >= 0.80) & (data
[:, 0] <= 0.92)
stopband_power = data[mask , 1]
return np.mean(stopband_power) -
4.39
def con3(data: np.ndarray) -> float:
"""Ensures the power in the
1.08 -1.12 Hz stopband is less
than
-11.74 dB."""
mask = (data[:, 0] >= 1.08) & (data
[:, 0] <= 1.12)
stopband_power = data[mask , 1]
return np.mean(stopband_power) -
11.74


15
