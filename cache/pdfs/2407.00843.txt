--- Page 1 ---
A UNIFIED APPROACH TO EXTRACT INTERPRETABLE RULES
FROM TREE ENSEMBLES VIA INTEGER PROGRAMMING
Lorenzo Bonasera
Department of Mathematics “Felice Casorati”
University of Pavia
Pavia 27100
lorenzo.bonasera01@universitadipavia.it
Emilio Carrizosa
Institute of Mathematics of the University of Seville
University of Seville
Seville 41012
ecarrizosa@us.es
ABSTRACT
Tree ensembles are very popular machine learning models, known for their effectiveness in supervised
classification and regression tasks. Their performance derives from aggregating predictions of multiple
decision trees, which are renowned for their interpretability properties. However, tree ensemble
models do not reliably exhibit interpretable output. Our work aims to extract an optimized list of rules
from a trained tree ensemble, providing the user with a condensed, interpretable model that retains
most of the predictive power of the full model. Our approach consists of solving a set partitioning
problem formulated through Integer Programming. The proposed method works with either tabular
or time series data, for both classification and regression tasks, and its flexible formulation can
include any arbitrary loss or regularization functions. Our extensive computational experiments offer
statistically significant evidence that our method is competitive with other rule extraction methods in
terms of predictive performance and fidelity towards the tree ensemble. Moreover, we empirically
show that the proposed method effectively extracts interpretable rules from tree ensemble that are
designed for time series data.
Keywords Machine learning · Integer programming · Interpretability · Explanation fidelity · Rule extraction · Tree
ensemble · Time series classification
1
Introduction
The demand for interpretable and explainable machine learning models has arisen across various applications. Indeed,
decision-makers need to understand the causal mechanisms behind a model’s output in order to justify the choices
made, especially in high-risk domains [3]. Consequently, research in interpretability has expanded significantly within
both the machine learning and operations research communities over the past decade. Explainable Artificial Intelligence
(XAI) has emerged as a rapidly growing research field [45], primarily focusing on post-hoc explanation methods to
facilitate the understanding of outputs provided by black-box models such as deep neural networks. Although XAI
methods can be highly effective in capturing and representing the internal decision-making processes of black-box
models, their application in high-risk domains is controversial [70]. Therefore, researchers have gradually shifted their
attention to developing inherently interpretable methods.
Commonly regarded as one of the most interpretable machine learning models, decision trees have been for decades
the tool of choice for providing highly intuitive and understandable predictions, thanks to their affinity with human
reasoning [74]. However, their interpretability is counterbalanced by poor out-of-sample performance, mainly due to
overfitting and inconsistent predictions. Aggregating trees into ensembles solved this problem, giving rise to machine
learning models such as random forest [17] or gradient boosted trees [25, 39], which significantly improved performance
and robustness for both classification and regression tasks. However, combining a large number of decision trees
naturally hinders their original interpretability, producing black-box models. As a consequence, research in the field of
interpretable machine learning has focused on developing methods and techniques to interpret both the predictions
and the internal mechanisms of tree ensemble models. More in general, methods for interpreting ensemble models
such as random forests can be grouped into two categories [3]. The first one refers to internal processing techniques,
arXiv:2407.00843v3  [math.OC]  13 Jan 2025


--- Page 2 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
which involve computing ensemble-specific measures to gain interpretable information about predictions. This category
includes various measurement of the feature importance, such as Mean Decrease Accuracy, Mean Decrease Impurity,
and Minimal Depth [48, 73]. From a statistical inference viewpoint, feature importance consists of an estimate for the
quantity that would have been obtained by making the same prediction with unknown true response function [47]. As
discussed in previous research, model diagnostics based on feature importance can be misleading and biased towards
correlated features [47, 75, 93]. The second category comprehends all the post-hoc approaches that aim to identify
and explain the relationship between tree ensembles and their outputs [3]. These approaches include the so-called
born-again tree methods, which aim to induce a single decision tree that replicates the behavior of a given tree ensemble
[18, 34, 85]. Another class of post-hoc techniques for interpreting tree ensembles consists of rule extraction methods,
which involve inducing a rule-based predictor to capture the predictive power of the ensemble model [3, 34].
Arguably, rule lists (or rule sets) represent the only competitor of decision trees in terms of interpretability [90], as they
both consist of logical models in the form of “if-then” statements that are easy to understand [70]. A list of rules can be
built from scratch or by exploiting association rules previously mined from given data [1]. An interesting connection
between decision trees and rule lists involves extracting rules from previously trained tree ensembles. By doing so, the
search space for selecting rules is restricted to the set of branch nodes in the tree ensemble, which represent the logical
conditions used to split the training data and make predictions. Thus, extracting rules from a trained tree ensemble
avoids exploring impractically large search spaces. Existing methods involve solving various optimization problems
to select the best performing rules, along with their corresponding optimal weights [9, 40, 60, 64]. However, these
methods lack the flexibility to incorporate diverse loss functions, as they primarily focus on minimizing the squared
prediction error. This limitation makes existing approaches difficult to customize and adapt to different learning tasks.
Indeed, their extension to both classification and regression tasks is challenging, and their implementations are currently
incompatible with complex data types such as time series, images, or text. This can become a significant drawback,
especially given the growing academic and industrial interest in developing machine learning models for these types of
data. For these reasons, our aim is to provide an explanation method that condenses any trained tree ensemble into an
interpretable list of rules, retaining most of its predictive power and faithfully representing its inner structure, regardless
of the type of data.
To cope with the lack of flexibility, Mathematical Programming has recently played a fundamental role in developing
interpretable and customizable methods, inducing more sparse and accurate models [22]. A new field belonging
to operations research started focusing on formulating Mixed-Integer Linear Programming models to learn optimal
classification and regression trees [11, 21], as well as optimal rule lists [56, 86, 87]. The ability to fully control how
models learn while minimizing their global prediction error has made this research field growing rapidly, particularly
thanks to the recent hardware and algorithmic improvements on solving Integer Programming (IP) problems [11, 56].
However, due to their complexity, most interpretable methods based on IP problems face scalability issues, both in
terms of the number of records and features contained in data. In fact, dealing with large-scale data is becoming an
increasingly common requirement for machine learning methods related to data-driven decision making [80]. For this
reason, developing computationally efficient interpretable methods is a challenging problem. While tree ensemble and
heuristic rule extraction methods do not suffer from scalability issues, they lack the expressive power and flexibility that
Mathematical Programming can offer.
1.1
Our contribution
In light of all the above, we present as our contribution a novel method for extracting an optimal list of rules from
any tree ensemble model by the means of Mathematical Programming. The proposed method relies on solving a
well-known IP formulation of the set partitioning problem [6]. The solution of such problem yields an optimal subset of
rules in terms of interpretability and predictive power. The use of Mathematical Programming makes the proposed
method highly flexible and easily adaptable to various applications. It can be applied to both regression and multi-class
classification tasks, explicitly handling tabular and time series data. Remarkably, its formulation allows users to easily
incorporate multiple custom loss functions and robustness measures. Through extensive computational experiments,
we offer statistically significant evidence that our method is a valid competitor to the state-of-the-art methods for
extracting rules from tree ensembles. As fidelity and interpretability are essential for producing valuable explanations
that accurately describe black-box models [43, 92], we introduce two novel measure to assess explanation fidelity of
surrogate models from tree ensembles. Obtained results show that our approach excels in terms of internal fidelity
towards the ensemble model, while achieving competitive scores in external fidelity.
Outline
The remainder of this paper is organized as follows. In Section 2, we present an overview on previous
works about learning rule lists from data, with a specific focus on related works on rule extraction methods from tree
ensembles, discussing their main advantages and drawbacks. Additionally, we discuss the role of explanation fidelity
in the context of rule extraction, and we review existing works on explainable methods for time series analysis. In
2


--- Page 3 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
Section 3, we provide the mathematical notation and preliminary concept for introducing the problem, proposing two
novel measures of internal fidelity for surrogate models from tree ensembles. In Section 4, we introduce our framework
and present the optimization problem, outlining the procedures for validating the proposed method and performing
data inference. In Section 5, we report and discuss our computational results in terms of predictive performance and
explanation fidelity on classification and regression tasks using benchmark tabular datasets. Furthermore, we conduct
a computational evaluation of the proposed method on time series multi-class classification tasks. Finally, Section 6
concludes the paper with a perspective on future works and further research directions.
2
Related work
In this section, we review and discuss previous works on learning of rule sets from tabular data, with a particular focus
on the past literature about rule extraction from ensemble models. Moreover, we discuss the role of explanation fidelity
of surrogate models, such as rule sets, with respect to the black-box models from which they are derived. Finally,
we explore related work on explainable methods for time series data, aiming to highlight the position of this paper
in relation to existing research gaps. Remarkably, earlier works about learning rule sets span multiple fields [41].
Various approaches have been explored, including Disjunctive Normal Form learning theory [38, 53, 76, 81], contrast
set learning [5, 7, 54] and frequent pattern mining [36, 42]. The problem of learning rule lists directly from data has
been well studied [35, 55], involving Bayesian frameworks [57], heuristic methods [24, 26, 27, 28] and exact methods
based on Mathematical Programming [56, 86, 87].
2.1
Rule extraction from tree ensembles
Extracting rules from a trained tree ensemble avoids exploring impractically large search spaces and is considered a
state-of-the-art post-hoc approach for interpreting random forests [3, 50]. Friedman and Popescu [40] first proposed a
specialized method for extracting lists of rules from both classification and regression tree ensembles, called RuleFit.
Their method extracts a subset of rules by minimizing a LASSO optimization problem, selecting the best regularized
weight for each rule. However, despite its flexibility, RuleFit tends to perform poorly when extracting rules from deeper
trees with highly correlated features [60]. Meinshausen [64] was the first to consider the tradeoff between predictive
performance and interpretability with NodeHarvest. The author proposed to extract rules from a tree ensemble by
solving a set partitioning problem of non-negative weights through Quadratic Programming. NodeHarvest demonstrates
good predictive performance and computational efficiency, offering users an inherently interpretable and sparse model.
However, its extension to classification problems is limited to handling no more than two classes. More recently, Bénard
et al. proposed a rule extraction method for both classification [9] and regression tasks [8] called SIRUS. Both variants
of SIRUS involve a preprocessing step where random forests are restricted to split nodes based on the q-empirical
quantiles for each feature, with q being a user-defined parameter. Then, the extracted rules are filtered based on an
user-defined threshold frequency p0. These filtered rules are then aggregated either by taking their average prediction
or by solving a ridge regression problem, depending on the task at hand. Both variants demonstrate good predictive
performance and robustness to data perturbation. Nonetheless, its classification variant cannot handle multi-class
classification problems. Despite providing interpretable output, the aforementioned methods are incapable of combining
extracted rules by imposing an order on features, failing to resemble a tree-like structure. This issue has been addressed
by Liu and Mazumder with FIRE [60], which outperforms existing methods in terms of regression performance and
interpretability. The authors proposed a quadratic problem introducing a non-smooth fusion penalty. Their framework
promotes solutions that exhibit a tree-like structure, thereby improving interpretability and sparsity of extracted rules.
The problem is then solved through an ad-hoc optimization algorithm that leverages its block structure. However, while
FIRE excels in regression performance, its extension to classification tasks is not straightforward and has not been
investigated by the authors. Moreover, FIRE does not offer a direct control on the number of extracted rules.
2.2
Explanation fidelity
In the literature, the discrepancy between a black-box (or opaque) model and a surrogate model extracted from it is
commonly referred to as explanation fidelity [50, 65, 83] or completeness [43]. A surrogate model whose predictions
closely match those of the opaque model is considered highly faithful. In the context of rule extraction, explanation
fidelity requires the extracted rules to faithfully represent the behavior of the trained black-box model. As argued by
Zhou [94] and Johansson et al. [50], rule extraction methods should be used to accomplish two distinct tasks. The
first one is to understand the underlying reasoning behind individual predictions made by a black-box model, while
the second one consists of extracting an interpretable model that serves as a more understandable predictor. Methods
handling the former task are evaluated based on fidelity, whereas out-of-sample accuracy is the primary criterion
3


--- Page 4 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
for methods focused on the latter task. Related works on rule extraction methods from tree ensembles fall into the
second category. Indeed, the reported results are compared based on predictive performance, while any form of fidelity
measure is neglected. According to Gilpin et al. [43], fidelity is necessary to produce explanations that accurately
describe the black-box model. In fact, interpretability and fidelity are both needed to obtain valuable explanations [92].
However, they can conflict, as developing an accurate surrogate often requires a more complex model, which can reduce
its interpretability [43, 50, 65, 94]. Therefore, a tradeoff between interpretability and fidelity comes into play, and
explanation methods should not be evaluated based on a single point along this tradeoff [43]. As first discussed by
Messalas et al. [65], two types of fidelity can be identified. The first, known as external fidelity [84], refers to the degree
to which the predictions of the surrogate model agree with those of the opaque model. Common measures of external
fidelity include calculating the disagreement between the two models, often expressed as the fraction of unexplained
variance or through mean squared error. While external fidelity evaluates the consistency between the predictions of
the surrogate and the opaque models, it does not guarantee that the surrogate’s decision-making process is faithful
to the original one [65]. This aspect is captured by the internal fidelity, which is notably hard to assess [84]. The
model-agnostic approach to assessing internal fidelity involves measuring the alignment between the feature importance
rankings provided by the surrogate and opaque models. For example, this can be done by sorting the features of a
dataset according to their importance as determined by both models, and then computing the rank correlation (e.g.,
Kendall’s τ) between the two orderings [65, 84]. Currently, there is a lack of model-specific methods for assessing the
internal fidelity of rule extraction techniques applied to tree ensembles.
2.3
Explainable methods and tree ensembles for temporal data
Explainable machine learning methods for temporal data mainly focus on providing users with explanations derived
from the data itself, such as time points, subsequences, or instance-based methods [79]. Indeed, the majority of
earlier works about knowledge extraction from temporal data in terms of logical rules involve transitions between time
points [61, 80], subsequences [29, 30, 51, 66] or Allen’s temporal logic [2, 12, 72]. Explainable methods designed to
discover knowledge from time series include various extensions of the decision tree induction algorithms. For instance,
Sciavicco and Stan developed a time series extension of the C4.5 algorithm [71], following the work of Brunello et
al. [19] extending the ID3 algorithm. Similarly, previous works introduced tree ensemble models specifically designed
to operate with time series, such as interval-based boosted classifiers [69] and random forests [33], time points-based
random forests [4, 37] and Proximity Forests [62]. Arguably, the most explainable tree ensemble model for time series
is represented by the generalized Random Shapelet Forest (gRSF), introduced by Karlsson et al. [52]. The authors
were the first to propose a tree ensemble classification and regression method based on shapelets [44, 91], which are
generally defined as class-discriminative subsequences of time series. More specifically, these subsequences are the
ones that vary the most among different classes of the dataset. The key idea of shapelets-based methods is to extract
interesting features by mining local subsequences from time series datasets and utilize them for learning tasks such
as classification, regression and clustering. Since shapelets are short time series subsequences, they can be visually
inspected, providing users with interpretable output. While the generalized Random Shapelet Forest (gRSF) framework
is among the state-of-the-art explainable methods for temporal data, its underlying machine learning model is not
interpretable due to its ensemble structure. To the best of our knowledge, our work represents the first rule set extraction
method whose implementation is directly capable of handling tree ensembles designed for temporal data.
3
Preliminaries
In this section, we introduce the main notation and formalism about time series, decision trees and tree ensembles,
to prepare the ground for presenting our rules extraction problem. Our formalism allows us to represent trees and
ensemble models as feature-agnostic sets of rules for both classification and regression settings. We also present two
novel measures of internal fidelity for surrogate models based on rule extraction from tree ensembles.
3.1
Time series
A time series x = [x1, x2, . . . , xP ] is an ordered collection of P real values. Given a time series x ∈RP , a time series
subsequence of x of length J ≤P starting at position p is the ordered collection of values xJ
p = [xp, . . . , xp+J−1].
Please note that there are a total of P −J + 1 subsequences. Given two time series x, z ∈RP , we consider as time
series distance the Euclidean norm between the two corresponding vectors ||x −z||2. In a general setting, distance
measures different from the Euclidean norm are allowed. Given two time series x ∈RP and z ∈RJ, with J ≤P, the
time series subsequence distance is the minimum distance between z and any subsequence xJ
p of x, that is
4


--- Page 5 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
dist(x, z) =
min
1≤p≤P −J+1 ||xJ
p −z||2.
(1)
3.2
Decision trees
We consider standard supervised classification and regression problems, with a given a training dataset of N i.i.d.
samples D = {(xi, yi), i = 1, . . . , N}, where data points xi ∈RP are expressed as vectors containing P tabular
features or time series of length P. In the case of classification task, targets yi ∈C represent a categorical variable,
where C is a set of class labels. In the case of regression task, targets yi ∈R represent a numerical variable. The
given dataset is used to construct a predictor that maps each data point to the codomain of the corresponding task.
A binary decision tree of maximal depth D is a predictor that recursively divides the feature space into at most 2D
non-overlapping partitions called leaf nodes. Each partition is defined by an ordered collection of disjunctive splittings
called branch nodes. Splittings are defined by a tuple (at(x), bt), for each t ∈B, where B is the set of branch nodes.
The first element of the tuple represents the feature selector, whereas the second element corresponds to the threshold
value. Each splitting compares the selected feature to the threshold value. If a data point exhibits a value lower or equal
than the threshold, it is assigned to the left child of the branch node; otherwise, it is assigned to the right one. In the
case of univariate decision trees for tabular data, the feature selector is the scalar product between point x ∈RP and
one canonical base e(p) of the same feature space RP . Figure 1 depicts an example of an univariate binary decision
tree of maximal depth D = 2 for the classification of tabular data. It contains branch nodes B = {1, 2, 3} and leaf
nodes L = {1, 2, 3, 4}. The root node selects the 10th feature of each point contained in the dataset, and compares each
value to the threshold b1 = 0.7. The other branch nodes operate in a similar way. Each leaf node is associated to a
colour representing one of two class labels in C = {Class 0, Class 1}.
1
2
1
2
3
3
4
⟨e(10), x⟩≤0.7
⟨e(10), x⟩> 0.7
⟨e(8), x⟩≤12.2
⟨e(8), x⟩> 12.2
⟨e(2), x⟩≤97.8
⟨e(2), x⟩> 97.8
Figure 1: Example of a binary decision tree of depth 2 for the classification of tabular data.
3.2.1
Shapelets-based trees
If points contained in the dataset are time series, the first element of each splitting (at(x), bt) represents an extractor
of temporal features. Without loss of generality, we consider as temporal feature the distance (1) computed between
time series and meaningful subsequences sampled from the dataset. We show how to use data-mined shapelets as
subsequences to learn a decision tree for time series interpretable classification [13, 91]. As an example, we use the
ItalyPowerDemand dataset from the UCR repository [31]. The training set includes 67 time series, each 24 units long,
representing twelve monthly electrical power demand patterns from Italy. The classification task is to distinguish
days between October and March from those between April and September, which correspond to the two class labels
in C = {Class 0, Class 1}. The top two subplots in Figure 2 show the training data for the two classes. Using one of the
available data mining approaches [44, 78], it is possible to extract shapelets s1 and s2 from the training data. These
shapelets are depicted in dark green in the rightmost part of Figure 2. By computing distance (1) between each time
series x and each of the two shapelets s1 and s2, we can represent each time series as a point in the scatter plot in
Figure 2. The horizontal axis represents dist(x, s1), while the vertical axis represents dist(x, s2). Finally, using the
two distances as features for each time series, we can build a decision tree that partitions the feature space associating
threshold values b1 and b2 with shapelets s1 and s2, respectively. Figure 3 shows a shapelets-based decision tree of
maximal depth D = 2, with branch nodes B = {1, 2} and leaf nodes L = {1, 2, 3}. Threshold values correspond to
those shown in the scatter plot in Figure 2.
5


--- Page 6 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
0
5
10
15
20
25
2.5
1.0
0.5
2.0
3.5
Training data: Class 0
0
5
10
15
20
25
2.5
1.0
0.5
2.0
3.5
Training data: Class 1
0
1
2
3
4
5
6
7
2.5
0.0
2.5
5.0
Shapelet s1
0
1
2
3
4
5
6
7
2.5
0.0
2.5
5.0
Shapelet s2
3.5
4.0
4.5
5.0
dist(x, s1)
3.0
3.5
4.0
4.5
5.0
5.5
dist(x, s2)
b2 = 4.30
b1 = 4.65
Distance-transformed time series
Class 0
Class 1
Figure 2: Example of shapelets-based decision tree built upon the ItalyPowerDemand dataset from the UCR reposi-
tory [31]. The upper subplots show the training time series in red (Class 0) and blue (Class 1). The right subplots show
the two shapelets s1, s2 in dark green. The left subplot displays the scatter plot of the time series based on dist(x, s1)
and dist(x, s2).
3.2.2
Rule extraction
We define as condition the splitting of a given branch node t ∈B paired with the left (≤) or right (>) sign. For instance,
we indicate with (a1(x), b1, ≤) the condition that a given point x has to satisfy to get to the left child of the root node,
that is whether or not the inequality a1(x) ≤b1 holds. Starting from the root node, each point is recursively evaluated
at branch nodes until it is finally assigned to a leaf node to get a prediction. In the case of a classification tree, the
prediction corresponds to the most frequent class contained in the leaf node. For regression, the prediction is the average
value of the target variables of the assigned data points. Since the path from the root node to each leaf node is unique,
we can represent each leaf node l ∈L as a rule, that is the ordered collection of conditions along its path
Rl = [(a1(x), b1, ♢), . . . , (aP a(l)(x), bP a(l), ♢)],
(2)
where ♢stands for the sign of each condition, while Pa(l) ∈B represents the parent node of leaf node l ∈L. Since
splittings are disjunctive, each point x that satisfies all the conditions of a rule is uniquely assigned to the corresponding
leaf node. Given a rule Rl, we define the set that contains all the corresponding splitting as follows
Sl = {(a1(x), b1), . . . , (aP a(l)(x), bP a(l))}.
(3)
As example, let us consider the decision tree depicted in Figure 1. The rule corresponding to the first leaf node is
R1 = [(⟨e(10), x⟩, 0.7, ≤), (⟨e(8), x⟩, 12.2, ≤)],
whereas the corresponding set of splittings is
S1 = {(⟨e(10), x, ⟩, 0.7), (⟨e(8), x⟩, 12.2)}.
6


--- Page 7 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
1
2
1
2
3
dist(x, s1) ≤4.65
dist(x, s1) > 4.65
dist(x, s2) ≤4.30
dist(x, s2) > 4.30
Figure 3: Example of a shapelets-based decision tree of depth 2 for the classification of temporal data.
Finally, we can represent a decision tree T by all the paths it contains, expressing it as the set containing the
corresponding rules
T = {Rl | l ∈L}.
(4)
For instance, the decision tree in Figure 1 can be represented as the following set of rules
T = {[(⟨e(10), x⟩, 0.7, ≤), (⟨e(8), x⟩, 12.2, ≤)],
[(⟨e(10), x⟩, 0.7, ≤), (⟨e(8), x⟩, 12.2, >)],
[(⟨e(10), x⟩, 0.7, >), (⟨e(2), x⟩, 97.8, ≤)],
[(⟨e(10), x⟩, 0.7, >), (⟨e(2), x⟩, 97.8, >)]}.
whereas the decision tree for time series in Figure 3 can be represented as
T = {[(dist(x, s1), 4.65, ≤), (dist(x, s2), 4.30, ≤)],
[(dist(x, s1), 4.65, ≤), (dist(x, s2), 4.30, >)]
[(dist(x, s1), 4.65, >)]}.
3.3
Tree ensembles
A tree ensemble classification or regression model consists of learning multiple decision trees by bagging [16] or
boosting [39], leveraging their aggregate predictions to obtain more accurate and robust results. The former approach
consists of training a collection of decision trees with different samples bootstrapped from the same training dataset,
obtaining a random forest [17]. Instead, in the latter approach trees are sequentially trained by exploiting the prediction
error of the previous ones, obtaining boosted learners like XGBoost [25]. For sake of simplicity, in the reminder of this
paper we focus on random forests as representative of tree ensemble models. Exploiting the introduced formalism, we
can express a random forest RF containing decision trees Tk, with k ∈K as their disjoint union, that is
RF =
G
k∈K
Tk.
(5)
In the classification setting, the prediction of a random forest corresponds to the majority vote of its decision trees.
For regression tasks, the prediction is the average of all the predictions obtained by its decision trees. A generalized
Random Shapelet Forest [52] is obtained by bagging decision trees whose feature selectors correspond to (1) applied to
sampled shapelets. Similarly, a Time Series Forest [33] can be obtained by applying the same procedure choosing as
feature one among mean, standard deviation and slope computed on sampled intervals. With this formalism, we can
represent any random forest as a set of rules.
7


--- Page 8 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
3.4
Rule fidelity
Our goal is to extract a surrogate model from a trained random forest in the form of a set (or list) Z = {Rl}ℓ
l=1
containing ℓrules. By doing so, we aim to effectively retain the predictive power of the tree ensemble and faithfully
represent its inner structure in an interpretable way. To measure the internal fidelity of a set of rules towards the tree
ensemble, we introduce two novel measures of representativeness. The first one is the percentage of trees whose paths
are represented by a list of rules. A tree T is path-represented by a list of rules Z if Z ∩T ̸= ∅. In other words, a tree
of the random forest is path-represented by a list if the latter contains at least one of the paths from the root node of the
tree to its leaf nodes. The second fidelity measure corresponds to the percentage of trees whose conditions (or branch
nodes) are represented by a list of rules. A tree T is node-represented by a list of rules Z if the latter contains at least
one logical condition (or branch node) of the tree. In this way, we can measure how closely a list of rules resembles
the random forest, depending on the amount of path-represented and node-represented trees. For example, the tree in
Figure 1
T = {[(⟨e(10), x⟩, 0.7, ≤), (⟨e(8), x⟩, 12.2, ≤)],
[(⟨e(10), x⟩, 0.7, ≤), (⟨e(8), x⟩, 12.2, >)],
[(⟨e(10), x⟩, 0.7, >), (⟨e(2), x⟩, 97.8, ≤)],
[(⟨e(10), x⟩, 0.7, >), (⟨e(2), x⟩, 97.8, >)]}
is path-represented and node-represented by the rule
R1 = [(⟨e(10), x⟩, 0.7, ≤), (⟨e(8), x⟩, 12.2, ≤)],
while it is only node-represented by the rule
R2 = [(⟨e(10), x⟩, 0.7, ≤), (⟨e(5), x⟩, −4.1, ≤)],
as they only share the condition (⟨e(10), x⟩, 0.7, ≤).
4
Methodology
In this section, we present and discuss the proposed method. Given a trained random forest containing a total of L
rules, our approach consists of selecting an optimal list of rules Z by solving a set partitioning problem. In this way,
we can guarantee that each point is assigned to only one of the rules it satisfies, mimicking the partition of the feature
space provided by decision trees and enhancing overall interpretability. The key idea is to select the best subset of
rules according to their robustness to data perturbation and predictive power, measured by computing rule-specific
regularization and loss function, respectively.
4.1
Preprocessing step
Since the set partitioning problem is NP-hard [58], we alleviate the computational burden of our approach by separating
the computation of regularization and loss functions from the optimization problem. Therefore, we characterize each
rule by three quantities that are computed during the following preprocessing step. We define the stability of the j-th
rule as the weighted sum of the Sørensen-Dice index [23, 59] computed over all the rules contained in the random
forest, ignoring their inequality sign. Given the set of splittings in the form (3), for each l = 1, . . . , L, where L is the
amount of rules in the random forest, we compute the stability ϕj of rule Rj as follows
ϕj =
L
X
l̸=j, l=1
|wl| 2|Sj ∩Sl|
|Sj| + |Sl|,
(6)
where wl is the weight of the l-th rule in case of weighted tree ensembles (wl = 1 if unweighted). This measures the
total amount of shared splittings between rule Rj and all the other rules in the tree ensemble. As example, consider the
following rules extracted from a random forest
R1 = [(⟨e(10), x⟩, 0.7, ≤), (⟨e(8), x⟩, 12.2, ≤)],
R2 = [(⟨e(10), x⟩, 0.7, ≤), (⟨e(8), x⟩, 12.2, >)],
R3 = [(⟨e(10), x⟩, 0.7, >)],
8


--- Page 9 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
with their corresponding sets of splittings
S1 = {(⟨e(10), x⟩, 0.7), (⟨e(8), x⟩, 12.2)},
S2 = {(⟨e(10), x⟩, 0.7), (⟨e(8), x⟩, 12.2)},
S3 = {(⟨e(10), x⟩, 0.7)}.
The stability of each rule is computed as follows
ϕ1 = 2|S1 ∩S2|
|S1| + |S2| + 2|S1 ∩S3|
|S1| + |S3| = 2(2)
2 + 2 + 2(1)
2 + 1 = 1.67,
ϕ2 = 2|S2 ∩S1|
|S2| + |S1| + 2|S2 ∩S3|
|S2| + |S3| = 2(2)
2 + 2 + 2(1)
2 + 1 = 1.67,
ϕ3 = 2|S3 ∩S1|
|S3| + |S1| + 2|S3 ∩S2|
|S3| + |S2| = 2(1)
1 + 2 + 2(1)
1 + 2 = 1.33.
In other words, we measure how much a logical condition is replicated through the whole ensemble. A rule made of
conditions that are consistently replicated inside the random forest is more stable than a rule containing less frequent
conditions. By maximizing their stability, selected rules are less sensitive to data perturbation, and they can more
likely resemble a decision tree structure. This approach combines the effects of the fusion penalty of FIRE [60] and
the occurrence frequency of SIRUS [8]. Indeed, the former penalty term promotes the learning of rules with more
shared splittings, improving their interpretability. Instead, the latter method prunes the rules that entirely occur less than
the frequency threshold parameter p0. In order to quantify the predictive power of the j-th rule, we compute its loss
depending on the learning task as follows
ξj =
Nj −maxc∈C Ncj,
if classification,
MSE(Rj),
if regression,
(7)
where Nj is the number of data points that satisfy the rule, Ncj is the number of data points of class c ∈C among them,
and MSE(Rj) corresponds to the Mean Squared Error (MSE) computed on the response values yi ∈R of data points
that satisfy rule Rj. In other words, we quantify the loss of the j-th rule as the number of misclassified points in the
case of classification tasks. Instead, we consider the MSE computed on assigned points as the loss of the j-th rule in
the case of regression tasks. By computing loss and regularization values during the preprocessing step, we manage
to easily handle non-linear and multiple functions. After their computation, vectors ϕj and ξj are normalized to the
same scale. Lastly, we keep track of the assignment for each of the N data point to each of the L rules through the
matrix A ∈{0, 1}N×L, whose entries are computed as follows
Aij =
1,
if point xi satisfies rule Rj,
0,
otherwise.
(8)
4.2
Optimization problem
After employing the preprocessing step, we formulate the problem of selecting the best list of rules as a set partitioning
problem through Integer Programming. We introduce a binary decision variable for each rule of the tree ensemble, that
is zj ∈{0, 1}, with j = 1, . . . , L. Then, we exploit the corresponding quantities ϕj, ξj and assignment A to write the
following IP problem
max
z
λ
L
X
j=1
ϕjzj −(1 −λ)
L
X
j=1
ξjzj
(9a)
s.t.
L
X
j=1
Aijzj = 1,
i = 1, . . . , N,
(9b)
L
X
j=1
zj ≤ℓ,
(9c)
zj ∈{0, 1},
j = 1, . . . , L,
(9d)
where λ ∈[0, 1] represents a weight parameter to balance loss and stability, and ℓ∈N represents the maximum number
of rules to extract. Solving this problem produces an optimal list of rules in the form Z = {Rj | zj = 1, j = 1, . . . , L},
9


--- Page 10 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
containing the rules whose corresponding binary variable are included in the solution. The objective function (9a)
maximizes the stability of rules while minimizing their aggregated loss. Constraint (9b) ensures that each sample
is assigned to exactly one rule. Constraint (9c) imposes an upper bound on the number of selected rules. Finally,
constraint (9d) specifies the domain of the decision variables. We remark that the proposed method is independent of
how stability and loss are measured. Indeed, our approach is flexible and easy to customize, allowing the end-user to
compute multiple custom, non-linear stability and loss functions during the preprocessing step described in Section 4.1.
In this way, the optimization problem is agnostic to the learning task, efficiently handling both regression and multi-class
classification.
4.3
Validation procedure
In this section, we describe the procedure to train and validate the proposed method when the maximum number of
rules to extract is not specified or cannot be inferred. Starting from an already trained random forest with given training
and validation sets, we need to choose a proper value for parameter ℓ. Therefore, we validate this parameter through an
exhaustive search over a discrete set

ℓ, . . . , ℓ
	
, providing both a lower and an upper bound for its range. We compute
the lower bound ℓby solving the optimization problem obtained from (9) by dropping constraint (9c) and modifying
the objective function as follows
ℓ=
min
z
L
X
j=1
zj
(10a)
s.t.
L
X
j=1
Aijzj = 1,
i = 1, . . . , N,
(10b)
zj ∈{0, 1},
j = 1, . . . , L.
(10c)
We compute the upper bound ℓby solving the same problem with the opposite objective function. The objective values
obtained from these solutions correspond to the desired bounds. For possibly impractical problems, we propose two
heuristic strategies to compute sufficiently tight bounds for both values. Given an ensemble of K trees, we can compute
a heuristic value for the lower bound ℓas follows
ℓheur = min
k∈K |Tk|,
(11)
that is the number of rules (or leaf nodes) contained in the smallest tree of the ensemble. This approach yields lower
bounds that are close to the exact values, as shown in Figure 4. To compute a heuristic value for the upper bound ℓ,
we can construct a decision tree with low cost complexity on the input data and count the number of generated leaf
nodes. We note that obtained results can significantly vary depending on the dataset. After computing bounds, we
solve Problem (9) for each value of ℓ∈

ℓ, . . . , ℓ
	
over the training set. We remark that reoptimizing the same IP
problem with a different right-hand side is computationally efficient, since modern solvers can exploit effective warm
start techniques [88]. Finally, we select the value of ℓthat achieves the lowest loss on the validation set.
4.4
Inference procedure
One of the major differences between the proposed method and existing works on rule extraction consists of the lack of
weighted rules. Indeed, our approach generates a list of rules that corresponds to a set partition, without incorporating
any weighting strategy. We argue that this approach is more interpretable than competing ones, as it resembles a
decision tree and it partitions the training data into a rule-based clustering structure, which is considered a challenging
problem in the field of interpretable machine learning [20]. However, the application of an unweighted list of rules may
fail on unseen data points, especially if they significantly differ from the training data. This can result in out-of-sample
data falling into multiple rules or none at all, preventing the proposed method from generating a meaningful prediction.
We remark that, in rule extraction for black-box models, the purpose is to explain the predictions obtained from a
black-box model [94]. Therefore, if the surrogate model fails to obtain an individual prediction as in the case of multiple
or absent rules, the unseen data point can still be predicted by the tree ensemble [50]. Nonetheless, we propose a simple
heuristic to address the issue. If an unseen data point falls within two or more rules, we apply the rule that covers the
largest amount of training data. Instead, if an unseen data point does not fall within any rules of the list, the prediction
is the average of the response vector in case of regression, or the majority class in case of classification.
10


--- Page 11 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
0
5
10
15
20
25
30
35
Dataset
0
2
4
6
8
10
Lower bound
Exact
Heuristic
Figure 4: Comparison between exact and heuristic values for the lower bound ℓover 36 benchmark datasets.
5
Experiments and results
In this section, we present and discuss the experimental evaluation conducted to assess the performance of the proposed
method against the state-of-the-art, for regression and classification tasks of tabular data and time series classification.
To ensure a fair comparison, we selected as competing approaches the state-of-the-art methods that are reproducible.
For the same reason, we excluded FIRE from the comparison, since it does not provide a fixed upper limit on the number
of extracted rules. We discuss more in detail in A the challenges of establishing a fair comparison for rule extraction
methods. We implemented all methods in Python 3.9, except for SIRUS, which was implemented in R 4.3.1. We ran all
the experiments on the same machine equipped with a 3.3 GHz 8-core processor and 16 GB of memory. The proposed
IP problems (9) and (10) are solved through Gurobi 11.0 [46]. From preliminary results, the impact of validating the
weight parameter λ resulted in negligible effects. Thus, in the following experimental evaluation we fixed λ = 0.5,
ensuring a balanced tradeoff between loss and stability of extracted rules. In order to compare multiple methods in
terms of predictive power over the selected datasets, we employed the non-parametric Friedman test [32] combined
with the signed-rank Wilcoxon-Holm post-hoc test [49], with a level of statistical significance equal to 0.05. According
to Benavoli et al. [10], the Wilcoxon signed-rank test is more appropriate than the more common post-hoc Nemenyi
test [68]. Details about benchmark datasets are contained in Table 2, Table 5 and Table 8. During data preprocessing,
we applied one-hot encoding to tabular data with categorical features and discarded samples containing missing values.
For regression tasks, we standardized the response vector. No data preprocessing was applied to time series datasets.
5.1
Tabular regression
In this experiment, we compare the proposed method to the state-of-the-art about rule extraction for regression tasks
of tabular data. In particular, competing methods are RuleFit [40], SIRUS [8] and NodeHarvest (NH) [64]. The
experimental plan involves running each method on a selected benchmark of 22 datasets from the OpenML public
repository [82]. Following the experimental plan of earlier works [60, 64], we extract rules from a regression forest of
500 trees of depth 3 implemented through the scikit-learn package with default settings [67], and fix the maximum
amount of selected rules to 15. Since this quantity coincides with parameter ℓ, the proposed validation procedure is
unnecessary. Instead, we discard rules that cover fewer than a minimum percentage Nmin of the training samples, where
Nmin ∈[0.001, 0.01] is tuned through cross-validation. For competing methods, we fix the number of extracted rules
(or nodes) to 15 and the maximal interaction order (or length of rules) to 3. Consequently, NodeHarvest does not need
further parameters tuning [64]. For SIRUS, we tune the frequency threshold p0 through cross-validation, and we set
the q parameter to 10, following the authors’ guidelines [8]. For RuleFit, we tune the regularization parameter for the
LASSO regression through cross-validation [40]. We divide each dataset into a 75/25 train-test split. We run each
regression task with 30 different random seeds, and we report average and standard deviation of the out-of-sample Mean
Squared Error (MSE) in Table 2. From obtained results, we find that differences in average predictive performance
are not statistically significant (p ≈0.08). We show the pairwise statistical comparison through the critical difference
diagram comparing ranks in Figure 5. Remarkably, our approach achieves statistically equivalent performance to
RuleFit and SIRUS, while presenting a significant improvement over NodeHarvest in terms of MSE (p ≈0.002). The
11


--- Page 12 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
median value of computing times of the proposed method is ≈7 minutes, and the average runtime of the Gurobi IP
solver is ≈5% of the total computing time. On the selected benchmark, competing methods are more efficient than
ours.
Table 1: Details of datasets from the OpenML repository for tabular data regression.
Dataset
Records
Features
Mercedes_Benz
4209
563
Moneyball
114
54
abalone
4177
10
autoMpg
392
25
bank32nh
8192
32
boston
506
22
cpu_small
8192
12
elevators
16599
18
house_16H
22784
16
houses
20640
8
kin8nm
8192
8
mtp
4450
202
no2
500
7
pol
15000
48
puma32H
8192
32
socmob
1156
39
space_ga
3107
6
stock
950
9
tecator
240
124
us_crime
123
249
wind
6574
14
wine_quality
6497
11
1
2
3
4
NH
SIRUS
RuleFit
Our method
Figure 5: Critical distance diagram obtained by running the Friedman test [32] combined with the signed-rank Wilcoxon-
Holm post-hoc test [49] on the results about regression of tabular data of Table 2.
5.1.1
Comparison in terms of fidelity
To assess internal fidelity, we compute the two representativeness measures introduced in Section 3.4, and the F1-score
between the features selected by each rule extraction methods and the top 5% of features ranked by their importance
in the random forest, following the work of Velmurugan et al. [84]. To evaluate external fidelity, we report the
disagreement in terms of MSE computed between the predictions of each competing method and those of the random
forest. We report in Table 3 the average of each fidelity measure computed over the benchmark datasets, for each
rule extraction method. For the sake of completion, we also report the average number of extracted rules. Notably,
the proposed method obtains the best scores of representativeness and disagreement, while obtaining the second best
F1-score. Instead, RuleFit achieves the lowest disagreement value and the best F1-score, and obtains the second best
values in terms of representativeness. On average, the proposed method and NodeHarvest extract the lowest amount of
rules from the tree ensemble. By contrast, SIRUS extracts the largest number of rules, due to its tendency to select rules
with a very limited amount of logical conditions. We conclude that the choice of the interpretable method depends
12


--- Page 13 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
Table 2: Out-of-sample MSE values computed on benchmark datasets for tabular regression.
Dataset
Our method
RuleFit
SIRUS
NH
Mercedes_Benz
0.44 ± 0.10
0.46 ± 0.11
0.51 ± 0.10
0.45 ± 0.10
Moneyball
0.26 ± 0.07
0.22 ± 0.08
0.20 ± 0.07
0.26 ± 0.08
abalone
0.61 ± 0.06
0.61 ± 0.06
0.68 ± 0.06
0.60 ± 0.06
autoMpg
0.31 ± 0.06
0.30 ± 0.08
0.28 ± 0.05
0.32 ± 0.08
bank32nh
0.65 ± 0.03
0.64 ± 0.04
0.61 ± 0.04
0.65 ± 0.04
boston
0.28 ± 0.04
0.27 ± 0.06
0.29 ± 0.09
0.31 ± 0.06
cpu_small
0.08 ± 0.01
0.08 ± 0.01
0.11 ± 0.01
0.08 ± 0.01
elevators
0.52 ± 0.03
0.54 ± 0.05
0.64 ± 0.04
0.53 ± 0.03
house_16H
0.68 ± 0.05
0.69 ± 0.06
0.66 ± 0.06
0.69 ± 0.05
houses
0.51 ± 0.01
0.52 ± 0.03
0.56 ± 0.02
0.51 ± 0.01
kin8nm
0.67 ± 0.02
0.68 ± 0.02
0.65 ± 0.03
0.68 ± 0.05
mtp
0.74 ± 0.03
0.73 ± 0.04
0.74 ± 0.03
0.75 ± 0.06
no2
0.59 ± 0.09
0.57 ± 0.12
0.54 ± 0.10
0.60 ± 0.13
pol
0.29 ± 0.02
0.28 ± 0.05
0.28 ± 0.02
0.30 ± 0.01
puma32H
0.41 ± 0.02
0.42 ± 0.08
0.56 ± 0.06
0.42 ± 0.01
socmob
0.33 ± 0.09
0.32 ± 0.11
0.37 ± 0.12
0.37 ± 0.25
space_ga
0.53 ± 0.08
0.54 ± 0.10
0.53 ± 0.10
0.56 ± 0.09
stock
0.09 ± 0.01
0.12 ± 0.03
0.24 ± 0.08
0.10 ± 0.01
tecator
0.05 ± 0.01
0.06 ± 0.02
0.08 ± 0.02
0.04 ± 0.02
us_crime
0.46 ± 0.07
0.33 ± 0.04
0.35 ± 0.09
0.50 ± 0.09
wind
0.36 ± 0.01
0.36 ± 0.03
0.39 ± 0.02
0.36 ± 0.03
wine_quality
0.74 ± 0.04
0.74 ± 0.04
0.78 ± 0.04
0.75 ± 0.02
Average
0.44 ± 0.04
0.43 ± 0.06
0.46 ± 0.06
0.45 ± 0.05
Rank (p ≈0.08)
2.11
2.23
2.73
2.93
on the desired result. Indeed, SIRUS is the preferred method to obtain a list containing a large amount of short rules.
On the contrary, the proposed approach is the most suitable to extract a list that contains a smaller amount of more
representative rules. We show an example of extracted rules in B.
Table 3: Average fidelity values and number of extracted rules computed on the tabular regression benchmark.
Measure
Our method
RuleFit
SIRUS
NH
Represented trees
0.731
0.607
0.307
0.514
Represented paths
0.069
0.032
0.005
0.042
F1-score
0.543
0.586
0.447
0.505
Disagreement
0.047
0.047
0.092
0.065
Extracted rules
8.44
10.21
11.50
7.99
5.2
Tabular classification
Similarly to the previous experiment, we compare the proposed method to the state-of-the-art about rule extraction for
classification tasks of tabular data. As competing methods, we consider the previously mentioned approaches used
for regression tasks. However, NodeHarvest and SIRUS can only handle binary classification tasks. Therefore, we
select datasets with no more than 2 different classes. In particular, the experimental plan involves running each method
on a selected benchmark of 14 datasets from the OpenML public repository [82], following the approach of earlier
works [56, 87]. To assess the effectiveness of methods in extracting very short and sparse rules that resemble a decision
tree, we extract rules from a random forest of 500 trees of depth 2, and fix the maximum amount of selected rules to 4.
Similar to the previous experiment, this quantity coincides with parameter ℓ, making the proposed validation procedure
unnecessary. For competing methods, we fix the number of extracted rules to 4 and the maximal interaction order
to 2. NodeHarvest does not need further parameter tuning. For SIRUS, we tune the frequency threshold p0 through
cross-validation, and we set the q parameter to 10. For RuleFit, we tune the regularization parameter for the logistic
model through cross-validation. We divide each dataset into a 75/25 train-test split. We run each classification task
with 30 different random seeds, reporting the average and standard deviation of the out-of-sample accuracy in Table 5.
From the critical difference diagram comparing ranks in Figure 6, we observe that all methods achieve statistically
equivalent classification performance, with the exception of NodeHarvest, which performs poorly when running with
a very limited number of nodes. The median value of computing times of the proposed method is ≈1 minute, with
Gurobi’s average runtime being ≈8.5% of the total. Compared to competing methods, it is less efficient, as it requires
13


--- Page 14 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
more computational resources. However, our approach is more flexible than competing methods, as it can handle
different loss functions and multi-class classification tasks.
Table 4: Details of datasets from the OpenML repository for tabular data classification.
Dataset
Records
Features
Classes
adult
45222
120
2
bank-marketing
45211
51
2
banknote
1372
4
2
blood-transfusion
748
4
2
diabetes
768
8
2
FICO
9871
37
2
heart-c
296
25
2
ilpd
583
11
2
ionosphere
351
34
2
MagicTelescope
19020
10
2
mushroom
5644
98
2
musk
6598
268
2
tic-tac-toe
958
27
2
wdbc
569
30
2
Table 5: Out-of-sample accuracy values computed on benchmark datasets for tabular classification.
Dataset
Our method
RuleFit
SIRUS
NH
adult
0.81 ± 0.01
0.77 ± 0.02
0.76 ± 0.02
0.75 ± 0.00
bank-mark.
0.89 ± 0.00
0.89 ± 0.01
0.88 ± 0.00
0.88 ± 0.00
banknote
0.91 ± 0.01
0.85 ± 0.08
0.87 ± 0.03
0.56 ± 0.02
blood-transf.
0.74 ± 0.02
0.76 ± 0.02
0.76 ± 0.02
0.75 ± 0.03
diabetes
0.70 ± 0.03
0.70 ± 0.05
0.70 ± 0.04
0.65 ± 0.02
FICO
0.69 ± 0.01
0.67 ± 0.03
0.71 ± 0.01
0.52 ± 0.01
heart-c
0.73 ± 0.04
0.75 ± 0.06
0.77 ± 0.05
0.54 ± 0.07
ilpd
0.70 ± 0.03
0.72 ± 0.03
0.72 ± 0.03
0.72 ± 0.03
ionosphere
0.89 ± 0.02
0.83 ± 0.07
0.88 ± 0.04
0.70 ± 0.09
MagicTeles.
0.77 ± 0.02
0.74 ± 0.03
0.73 ± 0.01
0.65 ± 0.01
mushroom
0.91 ± 0.02
0.88 ± 0.07
0.90 ± 0.01
0.62 ± 0.01
musk
0.86 ± 0.01
0.87 ± 0.02
0.93 ± 0.04
0.85 ± 0.01
tic-tac-toe
0.68 ± 0.02
0.67 ± 0.05
0.67 ± 0.03
0.65 ± 0.02
wdbc
0.95 ± 0.03
0.92 ± 0.03
0.91 ± 0.02
0.63 ± 0.03
Average
0.80 ± 0.02
0.79 ± 0.04
0.80 ± 0.03
0.68 ± 0.03
Rank (p < 10−3)
1.89
2.25
2.11
3.75
5.2.1
Comparison in terms of fidelity
To assess internal fidelity, we compute the same measures as in the previous experiment. For external fidelity, we
calculate the disagreement as the fraction of misaligned classifications between each competing method and the random
forest. We report in Table 6 the average of each fidelity measure computed over the benchmark datasets, for each
rule extraction method. For the sake of completion, we also report the average number of extracted rules. From
obtained results, it turns out that the proposed method excels in representativeness, while obtaining the second best
F1-score. Instead, SIRUS achieves the lowest disagreement value and the best F1-score, but performs poorly in terms
of representativeness. This behavior is ascribable to its tendency to extract rules with a limited number of logical
conditions, as shown in B.
5.3
Time series classification
The aim of this experiment is to assess the effectiveness of the proposed method in handling tree ensembles specifically
designed for time series. We choose the generalized Random Shapelet Forest (gRSF) [52] as the tree ensemble model
14


--- Page 15 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
1
2
3
4
NH
RuleFit
SIRUS
Our method
Figure 6: Critical distance diagram obtained by running the Friedman test [32] combined with the signed-rank Wilcoxon-
Holm post-hoc test [49] on the results about classification of tabular data of Table 5.
Table 6: Average fidelity values and number of extracted rules computed on the tabular classification benchmark.
Measure
Our method
RuleFit
SIRUS
NH
Represented trees
0.355
0.178
0.064
0.112
Represented paths
0.021
0.008
0.006
0.005
F1-score
0.546
0.259
0.571
0.152
Disagreement
0.107
0.097
0.054
0.179
Extracted rules
3.98
3.24
3.61
2.48
for this purpose, providing interpretable features (shapelets). Currently, our approach is the only rule extraction
method whose implementation works on temporal data. The experimental plan consists of running gRSF on a selected
benchmark of 27 time series datasets from the UCR public repository [31] following the methodology of Karlsson
et al. [52], and applying the proposed method to extract an optimal list of rules from it. We remark that the UCR
repository provides fixed train-test splits for each dataset, avoiding the need for additional splitting strategies. For
this reason, we conduct each classification task using only 10 different random seeds. The gRSF is trained using the
tuned hyperparameters provided by its authors, which control the size and number of sampled shapelets. To maintain
consistency with the previous experiments, we fix the maximal depth to 3. Therefore, we select time series datasets with
no more than 23 different classes. As the number of rules to extract is not fixed, we train our method by running the
validation procedure with exact bounds discussed in Section 4.3, and we apply 5-fold cross-validation. The average and
standard deviation of the out-of-sample accuracy are reported in Table 8. On average, results show that the gRSF (Full
model) outperforms the proposed method, while the latter manages to extract an average of 6 interpretable rules. The
median computation time with the training procedure is ≈17 minutes. On average, the runtime of the Gurobi IP solver
is negligible. Remarkably, with given hyperparameters, gRSF is significantly faster, running in less than 30 seconds per
task. Nevertheless, the computational burden of our method is justified by its interpretability and condensed predictive
power. In B, we show an example of a shapelet-based list of rules extracted from gRSF.
15


--- Page 16 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
Table 7: Details of datasets from the UCR repository for time series classification.
Dataset
Train size
Test size
Length
Classes
Beef
30
30
470
5
CBF
30
900
128
3
ChlorineConcentration
467
3840
166
3
CinCECGTorso
40
1380
1639
4
Coffee
28
28
286
2
DiatomSizeReduction
16
306
345
4
ECG200
100
100
96
2
ECGFiveDays
23
861
136
2
FaceFour
24
88
350
4
GunPoint
50
150
150
2
Haptics
155
308
1092
5
ItalyPowerDemand
67
1029
24
2
Lightning2
60
61
637
2
Lightning7
70
73
319
7
MoteStrain
20
1252
84
2
OliveOil
30
30
570
4
SonyAIBORobotSurface1
20
601
70
2
SonyAIBORobotSurface2
27
953
65
2
Symbols
25
995
398
6
SyntheticControl
300
300
60
6
Trace
100
100
275
4
TwoLeadECG
23
1139
82
2
TwoPatterns
1000
4000
128
4
UWaveGestureX
896
3582
315
8
UWaveGestureY
896
3582
315
8
UWaveGestureZ
896
3582
315
8
Wafer
1000
6164
152
2
16


--- Page 17 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
Table 8: Out-of-sample accuracy computed on benchmark datasets for time series classification.
Dataset
Our method
gRSF (Full model)
Beef
0.45 ± 0.10
0.70 ± 0.02
CBF
0.83 ± 0.06
0.99 ± 0.00
ChlorineConcentration
0.56 ± 0.01
0.58 ± 0.00
CinCECGTorso
0.62 ± 0.05
0.79 ± 0.00
Coffee
0.87 ± 0.06
0.90 ± 0.02
DiatomSizeReduction
0.83 ± 0.05
0.96 ± 0.01
ECG200
0.81 ± 0.05
0.84 ± 0.01
ECGFiveDays
0.97 ± 0.03
1.00 ± 0.00
FaceFour
0.74 ± 0.07
0.98 ± 0.00
GunPoint
0.88 ± 0.06
0.99 ± 0.00
Haptics
0.39 ± 0.03
0.43 ± 0.02
ItalyPowerDemand
0.93 ± 0.01
0.94 ± 0.00
Lightning2
0.77 ± 0.03
0.79 ± 0.01
Lightning7
0.55 ± 0.05
0.66 ± 0.01
MoteStrain
0.73 ± 0.03
0.87 ± 0.01
OliveOil
0.69 ± 0.05
0.86 ± 0.01
SonyAIBORobotSurface1 0.91 ± 0.05
0.96 ± 0.00
SonyAIBORobotSurface2 0.86 ± 0.04
0.94 ± 0.00
Symbols
0.61 ± 0.10
0.92 ± 0.01
SyntheticControl
0.93 ± 0.02
0.99 ± 0.00
Trace
0.95 ± 0.02
0.99 ± 0.00
TwoLeadECG
0.91 ± 0.03
0.95 ± 0.00
TwoPatterns
0.62 ± 0.04
0.86 ± 0.01
UWaveGestureX
0.63 ± 0.03
0.70 ± 0.00
UWaveGestureY
0.53 ± 0.03
0.62 ± 0.00
UWaveGestureZ
0.78 ± 0.02
0.64 ± 0.01
Wafer
0.96 ± 0.01
0.97 ± 0.00
Average
0.74 ± 0.04
0.84 ± 0.01
17


--- Page 18 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
6
Conclusions and future work
In this paper, we introduce a novel method to extract interpretable rules from tree ensembles. Our approach involves
solving a set partitioning problem, separating the computation of loss and regularization functions from the Integer
Programming model. Leveraging the flexibility of Mathematical Programming, our method can incorporate various
non-linear functions, and it can handle both multi-class classification and regression tasks for tabular and temporal
data. We present extensive and statistically significant computational results for the classification and regression of
tabular data, showing that the proposed method achieves comparable performance to other competing rule extraction
methods for tree ensembles. We also conduct a comparison in terms of explanation fidelity, introducing two novel
measures of internal fidelity for surrogate models from tree ensembles. The results demonstrate that the proposed
approach outperforms others in internal fidelity, while maintaining competitive external fidelity. Additionally, we
provide empirical evidence that the proposed method can effectively extract interpretable rules from tree ensembles
designed for the multi-class classification of temporal data. Experiments also show that our method is computationally
demanding and less efficient than competing approaches. Nevertheless, this behavior is almost completely ascribable to
the preprocessing steps, empirically proving the efficiency of the proposed IP model. Future research may focus on
improving the preprocessing phase and extending the implementation of the proposed method to handle tree ensembles
for more complex tasks, such as image [14, 63, 89] or text classification [15, 77].
Acknowledgments
This work was supported by Fedegari Autoclavi S.p.A. The authors thank Prof. Stefano Gualandi and Prof. Isak
Samsten for their valuable support. The authors express their gratitude to all the researchers who generously provided
the datasets used in this work.
A
Comparison of rule extraction methods
We note that competing methods can generate significantly different types of rules. RuleFit is the only method that
produces rules similar in form to those generated by our approach. In contrast, SIRUS generates rules in an “if-then-else”
format, where each rule could be interpreted as two separate rules according to our definition, due to the inclusion of
the “else” part. Finally, NodeHarvest generates rules in the form of weighted trees, where each node contributes to the
prediction and is weighted depending on the tree structure. In the case of NodeHarvest, branch nodes of weighted trees
can be viewed as rules, as they may become leaf nodes after the pruning step. As a result, ensuring a fair comparison
across methods is not trivial. Nonetheless, each of the methods considered allows for direct control over the upper limit
of rules to be extracted. Therefore, in our experimental evaluation we choose to maintain the same fixed number of
rules according to the corresponding definitions used by the competing methods, as specified by their authors.
B
Interpretable output
We present examples of the interpretable rules extracted by different methods. The target datasets selected for this
qualitative analysis are those from the benchmark where all methods performed similarly. For simplicity, we consider
RuleFit and SIRUS as competing methods, since their lists of rules are easier to represent.
Tecator
For this tabular dataset, we trained a random forest with maximal depth 2 and we limited the number of
extracted rules up to 4. We do not apply any train-test split, as we are not evaluating out-of-sample performance. For
completeness, we report the prediction values of SIRUS rules obtained by the “else” clause. In Table 9, we present the
lists of rules obtained from our method, RuleFit, and SIRUS, along with their corresponding weights and prediction
values. Notably, the rules extracted by our method are the only ones that construct a complete decision tree, as depicted
in Figure 7. Instead, RuleFit tends to extract logical conditions with slightly different thresholds, and SIRUS tends to
produce shorter rules, due to the inclusion of “else” conditions.
Houses
For this tabular dataset, we trained a random forest with maximal depth 3 and we limited the number of
extracted rules up to 15. We do not apply any train-test split, as we are not evaluating out-of-sample performance. For
18


--- Page 19 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
Our method
Rule
Weight
Prediction
(x(122), 59.20, ≤), (x(122), 47.25, ≤)
-
2.12
(x(122), 59.20, ≤), (x(122), 47.25, >)
-
0.84
(x(122), 59.20, >), (x(122), 67.20, ≤)
-
-0.13
(x(122), 59.20, >), (x(122), 67.20, >)
-
-0.80
RuleFit
Rule
Weight
Prediction
(x(122), 57.95, ≤), (x(122), 46.25, ≤)
1.78
2.21
(x(122), 59.20, ≤), (x(122), 47.25, >)
0.54
0.87
(x(122), 57.95, >)
-0.45
-0.54
(x(122), 59.20, >), (x(122), 67.20, >)
-0.67
-0.80
SIRUS
Rule
Weight
Prediction
Prediction (else)
(x(122), 57.97, ≤)
0.57
1.32
-0.57
(x(122), 44.81, ≤)
0.44
2.16
-0.24
(x(123), 16.00, ≤)
0.27
1.19
-0.50
Table 9: List of rules obtained by competing methods on the tecator dataset.
1
2
2.12
0.84
3
-0.13
-0.80
x(122) ≤59.20
x(122) > 59.20
x(122) ≤47.25
x(122) > 47.25
x(122) ≤67.20
x(122) > 67.20
Figure 7: Rules extracted from tecator by our method in the form of a decision tree.
completeness, we report the prediction values of SIRUS rules obtained by the “else” clause. In Table 10, we present the
lists of rules obtained from our method, RuleFit, and SIRUS, along with their corresponding weights and prediction
values. Again, the rules extracted by our method are the only ones that form a complete decision tree, as shown in
Figure 8. Instead, RuleFit tends to extract shorter rules containing logical conditions with slightly different thresholds,
while SIRUS tends to produce more rules made of only one condition.
ECGFiveDays
This temporal dataset contains ECG signals recorded from a 67-year-old male. The two classes
correspond to the dates 12/11/1990 and 17/11/1990 [31]. For this dataset, we first trained the gRSF over the training
set by using the provided hyperparameters [52] and setting the maximal depth to 3. Since no competing methods
currently address time series classification, we applied our rule extraction method with the ℓparameter fixed at 2. The
resulting list of rules is shown in Figure 9, presented as a shapelet-based decision tree with unitary depth. The first leaf
node corresponds to the date 17/11/1990, while the second leaf node corresponds to 12/11/1990. We also present two
examples of time series from different classes, overlaying the single shapelet extracted from the gRSF and reporting the
distance values computed using the subsequence distance measure (1). Notably, the temporal positions of the two peaks
near x = 80 are sufficient for accurately categorizing the signals. The extracted rules effectively capture and explain
the predictions of the shapelet-based tree ensemble, obtaining a test accuracy of 99%.
19


--- Page 20 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
Our method
Rule
Weight
Prediction
(x(0), 5.08, ≤), (x(0), 3.11, ≤), (x(6), 34.45, ≤)
-
-0.42
(x(0), 5.08, ≤), (x(0), 3.11, ≤), (x(6), 34.45, >)
-
-0.79
(x(0), 5.08, ≤), (x(0), 3.11, >), (x(1), 38.50, ≤)
-
-0.08
(x(0), 5.08, ≤), (x(0), 3.11, >), (x(1), 38.50, >)
-
0.45
(x(0), 5.08, >), (x(0), 6.87, ≤), (x(1), 36.50, ≤)
-
0.64
(x(0), 5.08, >), (x(0), 6.87, ≤), (x(1), 36.50, >)
-
1.35
(x(0), 5.08, >), (x(0), 6.87, >), (x(0), 7.82, ≤)
-
1.45
(x(0), 5.08, >), (x(0), 6.87, >), (x(0), 7.82, >)
-
2.17
RuleFit
Rule
Weight
Prediction
(x(0), 5.04, ≤), (x(0), 2.85, ≤)
-0.33
-0.67
(x(0), 5.04, >)
0.38
1.06
(x(0), 5.08, ≤), (x(0), 3.12, ≤)
-0.36
-0.60
(x(0), 5.08, >), (x(0), 6.82, >)
1.02
1.86
(x(0), 5.09, ≤)
-0.05
-0.28
(x(0), 5.37, ≤)
-0.34
-0.24
(x(0), 5.12, ≤)
-0.04
-0.27
SIRUS
Rule
Weight
Prediction
Prediction (else)
(x(0), 5.11, ≤)
0.41
-0.28
1.11
(x(6), 38.5, ≤)
0.45
0.08
-0.75
(x(7), 122, ≤)
0.17
0.33
-0.08
(x(1), 46, ≤)
0.12
-0.04
0.35
(x(0), 5.11, ≤), (x(0), 3.14, ≤)
0.53
-0.60
0.40
(x(2), 2460, ≤)
0.11
-0.13
0.20
(x(0), 5.11, ≤), (x(1), 46, ≤)
0.29
-0.33
0.79
(x(2), 1840, ≤)
0.12
-0.20
0.13
(x(2), 2130, ≤)
0.11
-0.16
0.16
(x(0), 5.11, ≥), (x(0), 3.14, ≤), (x(6), 38.5, ≤)
0.07
0.10
-0.06
(x(0), 5.11, ≤), (x(7), 122, ≥)
0.21
-0.34
0.63
Table 10: List of rules obtained by competing methods on the houses dataset.
20


--- Page 21 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
1
2
3
-0.42
-0.79
4
-0.08
0.45
5
6
0.64
1.35
7
1.45
2.17
x(0) ≤5.08
x(0) > 5.08
x(0) ≤3.11
x(0) > 3.11
x(0) ≤6.87
x(0) > 6.87
x(6) ≤34.45
x(6) > 34.45
x(1) ≤38.5
x(1) > 38.5
x(1) ≤36.5
x(1) > 36.5
x(0) ≤7.82
x(0) > 7.82
Figure 8: Rules extracted from house by our method in the form of a decision tree.
21


--- Page 22 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
1
17/11
12/11
dist(x, s1) ≤3.53
dist(x, s1) > 3.53
0
20
40
60
80
100 120
−6
−4
−2
0
2
dist(x3, s1) = 2.60
s1
x3
0
20
40
60
80
100 120
−6
−4
−2
0
2
dist(x4, s1) = 4.85
s1
x4
Figure 9: Rules extracted from ECGFiveDays by our method in the form of a shapelet-based decision tree.
22


--- Page 23 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
References
[1] Agrawal, R., Imieli´nski, T., Swami, A., 1993. Mining association rules between sets of items in large databases,
in: Proceedings of the 1993 ACM SIGMOD international conference on Management of data, pp. 207–216.
[2] Allen, J.F., 1983. Maintaining knowledge about temporal intervals. Commun. ACM 26, 832–843.
[3] Aria, M., Cuccurullo, C., Gnasso, A., 2021. A comparison among interpretative proposals for random forests.
Machine Learning with Applications 6, 100094.
[4] Auret, L., Aldrich, C., 2010. Change point detection in time series data with random forests. Control Engineering
Practice 18, 990–1002.
[5] Azevedo, P.J., 2010. Rules for contrast sets. Intelligent Data Analysis 14, 623–640.
[6] Balas, E., Padberg, M.W., 1976. Set partitioning: A survey. SIAM review 18, 710–760.
[7] Bay, S.D., Pazzani, M.J., 1999. Detecting change in categorical data: Mining contrast sets, in: Proceedings of the
Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 302–306.
[8] Bénard, C., Biau, G., Da Veiga, S., Scornet, E., 2021a. Interpretable random forests via rule extraction, in:
International Conference on Artificial Intelligence and Statistics, PMLR. pp. 937–945.
[9] Bénard, C., Biau, G., Da Veiga, S., Scornet, E., 2021b. Sirus: Stable and interpretable rule set for classification.
Electronic Journal of Statistics 15, 427–505.
[10] Benavoli, A., Corani, G., Mangili, F., 2016. Should we really use post-hoc tests based on mean-ranks? The Journal
of Machine Learning Research 17, 152–161. URL: http://jmlr.org/papers/v17/benavoli16a.html.
[11] Bertsimas, D., Dunn, J., 2017. Optimal classification trees. Machine Learning 106, 1039–1082.
[12] Bonasera, L., Duma, D., Gualandi, S., 2024. Learning sparse-lets for interpretable classification of event-interval
sequences, in: Metaheuristics International Conference, Springer. pp. 3–18.
[13] Bonasera, L., Gualandi, S., 2024. Optimal shapelets tree for time series interpretable classification. EURO Journal
on Computational Optimization , 100091.
[14] Bosch, A., Zisserman, A., Munoz, X., 2007. Image classification using random forests and ferns, in: 2007 IEEE
11th international conference on computer vision, Ieee. pp. 1–8.
[15] Bouaziz, A., Dartigues-Pallez, C., da Costa Pereira, C., Precioso, F., Lloret, P., 2014. Short text classification
using semantic random forest, in: Data Warehousing and Knowledge Discovery: 16th International Conference,
DaWaK 2014, Munich, Germany, September 2-4, 2014. Proceedings 16, Springer. pp. 288–299.
[16] Breiman, L., 1996. Bagging predictors. Machine learning 24, 123–140.
[17] Breiman, L., 2001. Random forests. Machine learning 45, 5–32.
[18] Breiman, L., Shang, N., 1996. Born again trees. University of California, Berkeley, Berkeley, CA, Technical
Report 1, 4.
[19] Brunello, A., Sciavicco, G., Stan, I.E., 2019. Interval temporal logic decision tree learning, in: European
Conference on Logics in Artificial Intelligence, Springer. pp. 778–793.
[20] Carrizosa, E., Kurishchenko, K., Marín, A., Morales, D.R., 2023. On clustering and interpreting with rules by
means of mathematical optimization. Computers & Operations Research 154, 106180.
[21] Carrizosa, E., Molero-Rio, C., Romero Morales, D., 2021. Mathematical optimization in classification and
regression trees. TOP 29, 5–33.
[22] Carrizosa, E., Morales, D.R., 2013. Supervised classification and mathematical optimization. Computers &
Operations Research 40, 150–165.
[23] Chao, A., Chazdon, R.L., Colwell, R.K., Shen, T.J., 2006. Abundance-based similarity indices and their estimation
when there are unseen species in samples. Biometrics 62, 361–371.
[24] Chen, G., Liu, H., Yu, L., Wei, Q., Zhang, X., 2006. A new approach to classification based on association rule
mining. Decision Support Systems 42, 674–689.
[25] Chen, T., He, T., Benesty, M., Khotilovich, V., Tang, Y., Cho, H., Chen, K., Mitchell, R., Cano, I., Zhou, T., et al.,
2015. Xgboost: extreme gradient boosting. R package version 0.4-2 1, 1–4.
[26] Cheng, H., Yan, X., Han, J., Hsu, C.W., 2006. Discriminative frequent pattern analysis for effective classification,
in: 2007 IEEE 23rd international conference on data engineering, IEEE. pp. 716–725.
[27] Clark, P., Niblett, T., 1989. The cn2 induction algorithm. Machine learning 3, 261–283.
23


--- Page 24 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
[28] Cohen, W.W., Singer, Y., 1999. A simple, fast, and effective rule learner. AAAI/IAAI 99, 3.
[29] Cotofrei, P., Stoffel, K., 2002. Rule extraction from time series databases using classification trees, in: Proceedings
of the IASTED International Conference on Applied Informatics, Citeseer. pp. 327–332.
[30] Das, G., Lin, K.I., Mannila, H., Renganathan, G., Smyth, P., 1998. Rule discovery from time series, in: Proceedings
of the 4th International Conference on Knowledge Discovery and Data Mining, KDD 1998, p. 16 – 22.
[31] Dau, H.A., Keogh, E., Kamgar, K., Yeh, C.C.M., Zhu, Y., Gharghabi, S., Ratanamahatana, C.A., Yanping, Hu, B.,
Begum, N., Bagnall, A., Mueen, A., Batista, G., Hexagon-ML, 2018. The ucr time series classification archive.
[32] Demšar, J., 2006. Statistical comparisons of classifiers over multiple data sets. The Journal of Machine Learning
Research 7, 1–30.
[33] Deng, H., Runger, G., Tuv, E., Vladimir, M., 2013. A time series forest for classification and feature extraction.
Information Sciences 239, 142–153.
[34] Di Teodoro, G., Monaci, M., Palagi, L., 2024. Unboxing tree ensembles for interpretability: a hierarchical
visualization tool and a multivariate optimal re-built tree. EURO Journal on Computational Optimization 12,
100084.
[35] Domingos, P., 1996. Unifying instance-based and rule-based induction. Machine Learning 24, 141–168.
[36] Dong, G., Li, J., 1999. Efficient mining of emerging patterns: Discovering trends and differences, in: Proceedings
of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 43–52.
[37] Dudek, G., 2015. Short-term load forecasting using random forests, in: Intelligent Systems’ 2014: Proceedings of
the 7th IEEE International Conference Intelligent Systems IS’2014, September 24-26, 2014, Warsaw, Poland,
Volume 2: Tools, Architectures, Systems, Applications, Springer. pp. 821–828.
[38] Feldman, V., 2012. Learning dnf expressions from fourier spectrum, in: Conference on Learning Theory, JMLR
Workshop and Conference Proceedings. pp. 17–1.
[39] Friedman, J.H., 2001. Greedy function approximation: a gradient boosting machine. Annals of statistics ,
1189–1232.
[40] Friedman, J.H., Popescu, B.E., 2008. Predictive learning via rule ensembles. The Annals of Applied Statistics ,
916–954.
[41] Fürnkranz, J., Gamberger, D., Lavraˇc, N., 2012. Foundations of rule learning. Springer Science & Business
Media.
[42] García-Borroto, M., Martínez-Trinidad, J.F., Carrasco-Ochoa, J.A., 2010. A new emerging pattern mining
algorithm and its application in supervised classification, in: Advances in Knowledge Discovery and Data Mining:
14th Pacific-Asia Conference, Springer. pp. 150–157.
[43] Gilpin, L.H., Bau, D., Yuan, B.Z., Bajwa, A., Specter, M., Kagal, L., 2018. Explaining explanations: An overview
of interpretability of machine learning, in: 2018 IEEE 5th International Conference on Data Science and Advanced
Analytics (DSAA), pp. 80–89.
[44] Grabocka, J., Schilling, N., Wistuba, M., Schmidt-Thieme, L., 2014.
Learning time-series shapelets.
Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
doi:doi:10.1145/2623330.2623613.
[45] Gunning, D., Stefik, M., Choi, J., Miller, T., Stumpf, S., Yang, G.Z., 2019. Xai—explainable artificial intelligence.
Science robotics 4, eaay7120.
[46] Gurobi Optimization, L., 2022. Gurobi optimizer reference manual. URL: https://www.gurobi.com.
[47] Hooker, G., Mentch, L., Zhou, S., 2021. Unrestricted permutation forces extrapolation: variable importance
requires at least one more model, or there is no free variable importance. Statistics and Computing 31.
[48] Ishwaran, H., Kogalur, U.B., Gorodeski, E.Z., Minn, A.J., Lauer, M.S., 2010. High-dimensional variable selection
for survival data. Journal of the American Statistical Association 105, 205–217.
[49] Ismail Fawaz, H., Forestier, G., Weber, J., Idoumghar, L., Muller, P.A., 2019. Deep learning for time series
classification: a review. Data Mining and Knowledge Discovery 33, 917–963.
[50] Johansson, U., Sönströd, C., Löfström, T., Boström, H., 2022. Rule extraction with guarantees from regression
models. Pattern Recognition 126, 108554.
[51] Kadous, M.W., 1999. Learning comprehensible descriptions of multivariate time series, in: Proceedings of the
Sixteenth International Conference on Machine Learning, p. 463.
24


--- Page 25 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
[52] Karlsson, I., Papapetrou, P., Boström, H., 2016. Generalized random shapelet forests. Data Mining and Knowledge
Discovery 30. doi:doi:https://doi.org/10.1007/s10618-016-0473-y.
[53] Klivans, A.R., Servedio, R., 2001. Learning dnf in time, in: Proceedings of the thirty-third annual ACM
symposium on Theory of Computing, pp. 258–265.
[54] Kralj, P., Lavraˇc, N., Gamberger, D., Krstaˇci´c, A., 2007. Contrast set mining for distinguishing between similar
diseases, in: Artificial Intelligence in Medicine: 11th Conference on Artificial Intelligence in Medicine, Springer.
pp. 109–118.
[55] Lakkaraju, H., Bach, S.H., Leskovec, J., 2016. Interpretable decision sets: A joint framework for description and
prediction, in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pp. 1675–1684.
[56] Lawless, C., Dash, S., Gunluk, O., Wei, D., 2023. Interpretable and fair boolean rule sets via column generation.
Journal of Machine Learning Research 24, 1–50.
[57] Letham, B., Rudin, C., McCormick, T.H., Madigan, D., 2015. Interpretable classifiers using rules and bayesian
analysis: Building a better stroke prediction model. The Annals of Applied Statistics 9, 1350 – 1371.
[58] Lewis, H.R., 1983. Michael r. πgarey and david s. johnson. computers and intractability. a guide to the theory of
np-completeness. wh freeman and company, san francisco1979, x+ 338 pp. The Journal of Symbolic Logic 48,
498–500.
[59] Li, X., Wang, C., Zhang, X., Sun, W., 2020. Generic sao similarity measure via extended sørensen-dice index.
IEEE Access 8, 66538–66552.
[60] Liu, B., Mazumder, R., 2023. Fire: An optimization approach for fast interpretable rule extraction, in: Proceedings
of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 1396–1405.
[61] Lu, H., Han, J., Feng, L., 1998. Stock movement prediction and n-dimensional inter-transaction association rules,
in: 1998 ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, Seattle, WA,
USA, ACM, New York, USA.
[62] Lucas, B., Shifaz, A., Pelletier, C., O’Neill, L., Zaidi, N., Goethals, B., Petitjean, F., Webb, G.I., 2019. Proximity
forest: an effective and scalable distance-based classifier for time series. Data Mining and Knowledge Discovery
33, 607–635.
[63] Man, W., Ji, Y., Zhang, Z., 2018. Image classification based on improved random forest algorithm, in: 2018 IEEE
3rd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA), IEEE. pp. 346–350.
[64] Meinshausen, N., 2010. Node harvest. The Annals of Applied Statistics , 2049–2072.
[65] Messalas, A., Kanellopoulos, Y., Makris, C., 2019. Model-agnostic interpretability with shapley values, in:
2019 10th International Conference on Information, Intelligence, Systems and Applications (IISA), pp. 1–7.
doi:doi:10.1109/IISA.2019.8900669.
[66] Ozden, B., Ramaswamy, S., Silberschatz, A., 1998. Cyclic association rules, in: Proceedings 14th International
Conference on Data Engineering, IEEE. pp. 412–421.
[67] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E., 2011.
Scikit-learn: Machine learning in Python. Journal of Machine Learning Research 12, 2825–2830.
[68] Pohlert, T., 2014. The pairwise multiple comparison of mean ranks package (pmcmr). R package 27, 9.
[69] Rodríguez, J.J., Alonso, C.J., Boström, H., 2001. Boosting interval based literals. Intelligent Data Analysis 5,
245–262.
[70] Rudin, C., 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable
models instead. Nature machine intelligence 1, 206–215.
[71] Sciavicco, G., Stan, I.E., 2020. Knowledge extraction with interval temporal logic decision trees, in: 27th
International Symposium on Temporal Representation and Reasoning (TIME 2020), Schloss Dagstuhl-Leibniz-
Zentrum für Informatik.
[72] Sciavicco, G., Stan, I.E., Vaccari, A., 2019. Towards a general method for logical rule extraction from time series,
in: International Work-Conference on the Interplay Between Natural and Artificial Computation, Springer. pp.
3–12.
[73] Seifert, S., Gundlach, S., Szymczak, S., 2019. Surrogate minimal depth as an importance measure for variables in
random forests. Bioinformatics 35, 3663–3671.
25


--- Page 26 ---
Extract Intepretable Rules from Tree Ensembles via Integer Programming
[74] Silva, A.P.D., 2017. Optimization approaches to supervised classification. European Journal of Operational
Research 261, 772–788.
[75] Strobl, C., Boulesteix, A.L., Kneib, T., Augustin, T., Zeileis, A., 2008. Conditional variable importance for
random forests. BMC bioinformatics 9, 1–11.
[76] Su, G., Wei, D., Varshney, K.R., Malioutov, D.M., 2016. Learning sparse two-level boolean rules, in: 2016 IEEE
26th International Workshop on Machine Learning for Signal Processing (MLSP), IEEE. pp. 1–6.
[77] Sun, Y., Li, Y., Zeng, Q., Bian, Y., 2020. Application research of text classification based on random forest
algorithm, in: 2020 3rd international conference on advanced electronic materials, computers and software
engineering (aemcse), IEEE. pp. 370–374.
[78] Tavenard, R., Faouzi, J., Vandewiele, G., Divo, F., Androz, G., Holtz, C., Payne, M., Yurchak, R., Rußwurm, M.,
Kolar, K., Woods, E., 2020. Tslearn, a machine learning toolkit for time series data. Journal of Machine Learning
Research 21, 1–6.
[79] Theissler, A., Spinnato, F., Schlegel, U., Guidotti, R., 2022. Explainable ai for time series classification: a review,
taxonomy and research directions. IEEE Access 10, 100700–100724.
[80] Ullah, A., Harib, K.H., 2006. Knowledge extraction from time series and its application to surface roughness
simulation. Information Knowledge Systems Management 5, 117–134.
[81] Valiant, L.G., 1984. A theory of the learnable. Communications of the ACM 27, 1134–1142.
[82] Vanschoren, J., Van Rijn, J.N., Bischl, B., Torgo, L., 2014. Openml: networked science in machine learning.
ACM SIGKDD Explorations Newsletter 15, 49–60.
[83] Velmurugan, M., Ouyang, C., Moreira, C., Sindhgatta, R., 2021. Evaluating fidelity of explainable methods for
predictive process analytics, in: Intelligent Information Systems, pp. 64–72.
[84] Velmurugan, M., Ouyang, C., Sindhgatta, R., Moreira, C., 2023. Through the looking glass: evaluating post hoc
explanations using transparent models. International Journal of Data Science and Analytics , 1–21.
[85] Vidal, T., Schiffer, M., 2020. Born-again tree ensembles, in: International conference on machine learning, PMLR.
pp. 9743–9753.
[86] Wang, T., Rudin, C., 2015. Learning optimized or’s of and’s. arXiv preprint arXiv:1511.02210 .
[87] Wei, D., Dash, S., Gao, T., Gunluk, O., 2019. Generalized linear rule models, in: International conference on
machine learning, PMLR. pp. 6687–6696.
[88] Witzig, J., 2014. Reoptimization techniques in MIP solvers. Ph.D. thesis. Zuse Institute Berlin.
[89] Xu, B., Ye, Y., Nie, L., 2012. An improved random forest classifier for image classification, in: 2012 IEEE
International Conference on Information and Automation, IEEE. pp. 795–800.
[90] Yang, H., Rudin, C., Seltzer, M., 2017. Scalable bayesian rule lists, in: International conference on machine
learning, PMLR. pp. 3921–3930.
[91] Ye, L., Keogh, E., 2009.
Time series shapelets: A new primitive for data mining, in: Proceedings of
the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 947–956.
doi:doi:10.1145/1557019.1557122.
[92] Zhou, J., Gandomi, A.H., Chen, F., Holzinger, A., 2021. Evaluating the quality of machine learning explanations:
A survey on methods and metrics. Electronics 10, 593.
[93] Zhou, Z., Hooker, G., 2021. Unbiased measurement of feature importance in tree-based methods. ACM
Transactions on Knowledge Discovery from Data (TKDD) 15, 1–21.
[94] Zhou, Z.H., 2004. Rule extraction: Using neural networks or for neural networks? Journal of Computer Science
and Technology 19, 249–253.
26
