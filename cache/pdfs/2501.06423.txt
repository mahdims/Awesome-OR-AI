--- Page 1 ---
arXiv:2501.06423v1  [cs.AI]  11 Jan 2025
AlgoPilot: Fully Autonomous Program Synthesis
Without Human-Written Programs
Xiaoxin Yin1*
1*Independent Researcher.
Corresponding author(s). E-mail(s): xiaoxin@gmail.com;
Abstract
Program synthesis has traditionally relied on human-provided speciﬁcations,
examples, or prior knowledge to generate functional algorithms. Existing meth-
ods either emulate human-written algorithms or solve speciﬁc tasks without
generating reusable programmatic logic, limiting their ability to create novel algo-
rithms. We introduce AlgoPilot, a groundbreaking approach for fully automated
program synthesis without human-written programs or trajectories. AlgoPilot
leverages reinforcement learning (RL) guided by a Trajectory Language Model
(TLM) to synthesize algorithms from scratch. The TLM, trained on trajecto-
ries generated by random Python functions, serves as a soft constraint during
the RL process, aligning generated sequences with patterns likely to represent
valid algorithms. Using sorting as a test case, AlgoPilot demonstrates its abil-
ity to generate trajectories that are interpretable as classical algorithms, such
as Bubble Sort, while operating without prior algorithmic knowledge. This work
establishes a new paradigm for algorithm discovery and lays the groundwork for
future advancements in autonomous program synthesis.
Keywords: Program Synthesis, Reinforcement Learning
1 Introduction
Program synthesis is the task of automatically generating programs that satisfy a
given speciﬁcation, typically expressed as input-output examples, formal constraints,
or high-level descriptions. The ultimate goal of program synthesis is to bridge the
gap between human intent and executable code, enabling systems that can generate
reliable and correct programs with minimal human intervention. The development of
deep learning has signiﬁcantly advanced program synthesis by enabling models to learn
1


--- Page 2 ---
complex program structures and semantics directly from data. Sequence-to-sequence
models and transformers have been successfully applied to generate programs from
natural language descriptions, input-output examples, or partial speciﬁcations [1, 2].
For instance, DeepCoder by Balog et al. [2] demonstrated the potential of neural-
guided search by predicting program sketches and eﬃciently guiding symbolic solvers.
Similarly, RobustFill by Devlin et al. [1] employed sequence-to-sequence models to
synthesize string transformation programs from input-output pairs, showcasing the
capabilities of deep neural networks in capturing program patterns.
In recent years there are two main directions of using deep learning for program
synthesis:
• Neural Reasoning Models, which learn to program with training data being the
“trajectories” of human-written algorithms.
Neural reasoning models aim at emulating algorithmic and programmatic reasoning
through end-to-end learning paradigms. Approaches such as Neural Programmer-
Interpreters (NPI) [3], A Generalist Neural Algorithmic Learner (GNAL) [4], and
Neural Algorithmic Reasoning with Causal Regularisation [5] share a common
methodology: they train neural models using trajectories derived from intermediate
states of human-written programs.
However, these approaches can only rebuild a program based on a human-written
program. They cannot create algorithms or programs on their own (e.g., based on
inputs and desired outputs). Thus their practical application is limited.
• End-to-End Neural Task Solvers, which train models to directly solve tasks (such
as sorting) instead of creating programs.
Another line of research focuses on training models to complete tasks typically
performed by classical algorithms, such as sorting. Notable examples include Neural
Turing Machines (NTMs) [6], Diﬀerentiable Neural Computers (DNCs) [7], and
AlphaDev [8]. Unlike traditional approaches that construct algorithms using explicit
operations like ”Compare” and ”Swap,” these models operate diﬀerently. They are
trained end-to-end and can observe all inputs simultaneously, enabling them to learn
task-speciﬁc behaviors directly.
However, this is very diﬀerent from what we mean by “algorithms”. For example, a
sorting algorithm should only use “Compare” and “Swap” (or “Copy”) operations,
instead of using a DNN to check all inputs at the same time and select the smallest
element. The above approaches cannot create algorithms that can be implemented
by a typical programming language.
We can see that neither of the two lines of research can automatically create a
program to solve an algorithmic task on its own. Neural Reasoning Models rely on
human-created programs to provide training data. End-to-End Neural Task Solvers
create models instead of programs. Here comes our key question: Is it possible to
train a model that can create an algorithm on its own?
An early work along this direction is presented by Abolaﬁa et al. [9], who used
RNNs to synthesize programs in BF, a very simple programming language. It only
requires the inputs and desired outputs of the program, without any human-written
2


--- Page 3 ---
programs. However, it can only solve very simple problems such as arithmetic opera-
tions and basic string manipulations, and is not powerful enough to solve algorithmic
tasks such as sorting.
In this paper we present AlgoPilot, the ﬁrst approach that can learn to create an
algorithm purely on its own, without human-created algorithms or trajectories. Algo-
Pilot uses reinforcement learning to learn to accomplish a algorithmic task, such as
sorting. Comparing with existing methods that learns from trajectories of human-
written algorithms [3] [4] [5], AlgoPilot uses a language model to build a model for
predicting the next token in a trajectory of a randomly generated python function.
This language model’s prediction becomes a soft constraint for its reinforcement learn-
ing (a.k.a. RL) process, which guides the RL model to generate trajectories that can
likely be generated by a real algorithm. This enables AlgoPilot to train a model which
generates trajectories that are highly similar to those of a real algorithm. One can
easily ﬁnd an algorithm (or a python function) that generates such trajectories (pos-
sibly with the help of LLMs), and in this way AlgoPilot achieves automated program
synthesis without human instructions.
Let us take sorting with double-loop as an example. Here is the procedure of
AlgoPilot in generating a sorting algorithm with a double-loop:
1. Random Function Generator
We create a random function generator that can generate a random python function
with a double-loop, and randomly add the two operations “Compare” and “Swap”.
2. Trajectory Language Model (TLM)
Many random functions are generated, each producing a trajectory of relevant
operations (Compare, Swap, and the indices involved). AlgoPilot trains a language
model of the trajectories, which can predict the probability of observing a token
after a preﬁx of tokens.
3. Reinforcement Learning
AlgoPilot uses reinforcement learning to train a transformer model for sorting.
Given the input’s length, the model can only use Compare and Swap operations,
and the environment only returns one feedback for each operation. For “Compare
i j”, the environment returns if the element at position i is smaller than, equal to,
or greater than that at position j. For Swap, the environment returns a boolean
indicating whether the array is sorted. Rewards are given to the model when it
ﬁnds a pair of elements that need to be swapped, or successfully sorts the array.
4. Guided Reinforcement Learning
The above reinforcement learning can train a model that can sort an array. How-
ever, its trajectory does not follow any obvious pattern, and it does not seem feasible
to create an algorithm based on the trajectory. Therefore, AlgoPilot uses the Tra-
jectory Language Model (TLM) to enhance the reinforcement learning process. It
adds an additional reward when the next operation has a high probability accord-
ing to the TLM, and adds a penalty for a low probability. This guides the model
to learn to generate a trajectory that can be produced by an algorithm.
5. Algorithm Creation
The trajectory generated in Step 4 usually follows obvious algorithmic patterns.
Currently AlgoPilot uses LLMs (such as GPT-4o) to generate an algorithm or a
3


--- Page 4 ---
python function. But we can certain train a language model to generate them from
the trajectories of randomly generated functions.
The rest of the paper is organized as follows. Related work is reviewed in Section 2.
Section 3 presents AlgoPilot, including the reinforcement learning environment, ran-
dom function generator, trajectory language model, guided reinforcement learning,
and ﬁnally the creation of algorithm. Experiment results are included in each subsec-
tion. We then discuss about the future work in Section 4, and conclude this paper in
Section 5.
2 Related Work
Historically, program synthesis can be traced back to early theoretical work in the
1960s, such as Church’s synthesis problem, which posed the challenge of ﬁnding a
program satisfying a logical speciﬁcation. Since then, the ﬁeld has evolved signiﬁ-
cantly, driven by advances in computational power, formal veriﬁcation techniques, and
learning-based methods.
In the early 2000s, program synthesis gained momentum with the introduction
of inductive program synthesis approaches, where programs are synthesized from
input-output examples. Notable among these is the Programming by Example (PBE)
paradigm, which allows users to specify program behavior through examples rather
than formal speciﬁcations. Tools such as FlashFill by Gulwani et al. [10] demonstrated
the practical feasibility of program synthesis in spreadsheet automation, enabling non-
programmers to perform complex tasks via example-driven speciﬁcations. Around the
same time, SKETCH [11] introduced a counterexample-guided inductive synthesis
(CEGIS) approach, which iteratively reﬁnes candidate programs based on counterex-
amples generated by a veriﬁer. Additionally, Angelic Programming [12] explored
angelic non-determinism to simplify program search by focusing on high-level intent.
These pioneering eﬀorts laid the foundation for modern approaches to program syn-
thesis, setting the stage for incorporating machine learning, neural networks, and
reinforcement learning techniques in subsequent years.
In recent years, deep learning has signiﬁcantly advanced program synthesis by
enabling models to learn complex program structures and semantics directly from data.
Traditional search-based and symbolic methods often struggle with large search spaces,
while neural approaches oﬀer a more scalable and ﬂexible alternative. Deep learning
models, particularly sequence-to-sequence architectures and transformers, have been
successfully applied to generate programs from natural language descriptions, input-
output examples, or partial speciﬁcations [1, 2]. For instance, DeepCoder by Balog
et al. [2] demonstrated the potential of neural-guided search by predicting program
sketches and eﬃciently guiding symbolic solvers. Similarly, RobustFill by Devlin et
al. [1] employed sequence-to-sequence models to synthesize string transformation pro-
grams from input-output pairs, showcasing the capabilities of deep neural networks
in capturing program patterns. The advent of large-scale pre-trained language mod-
els, such as OpenAI’s Codex [13], has further pushed the boundaries by generating
executable code from natural language prompts. These advancements illustrate the
growing synergy between deep learning techniques and program synthesis, opening up
4


--- Page 5 ---
new possibilities for automating software development and improving programming
productivity.
Abolaﬁa et al. [9] introduced an approach leveraging recurrent neural networks
(RNNs) to synthesize programs in BF, a very simple programming language. Their
method employs Priority Queue Training (PQT), an iterative optimization strategy
that maintains a queue of top-performing programs, focusing the training on promising
samples. This approach addresses the challenge of sparse rewards in program synthesis
and demonstrates improved generalization across algorithmic tasks, including string
manipulation and arithmetic operations. The results highlight the potential of neu-
ral networks in capturing logical reasoning patterns essential for end-to-end program
synthesis.
3 AlgoPilot: Automated Learning of Algorithm
without Human Help
In this section we present AlgoPilot, the ﬁrst approach that can learn to create an algo-
rithm purely on its own, without human-created algorithms or trajectories. AlgoPilot
uses reinforcement learning to learn to accomplish a algorithmic task. In this study
we use sorting with double-loop as the example, and the approach can be applied to
many other types of algorithms.
Comparing with existing methods that learns from trajectories of human-written
algorithms [3] [4] [5], we generate many random python functions with double-loops,
and train a language model to predict the next token in a trajectory of a randomly
generated python function. The prediction of this language model becomes a soft con-
straint for its reinforcement learning process, which guides the RL model to generate
trajectories that can likely be generated by a real algorithm. This enables AlgoPilot
to train a model which generates trajectories that are highly similar to those of a real
algorithm.
3.1 Learning to Sort with Reinforcement Learning
3.1.1 Environment of Reinforcement Learning
Before going into the details of AlgoPilot, we ﬁrst explain our setting of learning to sort
with reinforcement learning. We use a Sorting Environment to manage the state and
dynamics of the sorting task. Each episode begins with generation of a random array
of integers with a speciﬁed length (6 to 14). The agent interacts with the environment
through a series of actions, which includes actions like “Compare” and “Swap”, along
with tokens representing various comparison outcomes and list indices.
When the agent performs a Compare action, it selects two indices from the list to
compare their values, receiving feedback on whether the ﬁrst is less than, equal to, or
greater than the second. If the agent chooses to Swap two elements, the environment
swaps them in the underlying array, and gives a reward or penalty based on whether
the two elements are swapped into the right order. The environment also imposes a
maximum number of steps per episode to encourage eﬃciency and prevent endless
interactions.
5


--- Page 6 ---
Reward mechanisms are designed to guide the agent toward eﬀective sorting. Pos-
itive rewards are given for successful swaps and achieving a sorted list, while negative
rewards penalize invalid actions. A small negative reward is given to every step, in
order to penalize ineﬃcient moves. The environment employs a combination of long-
term gamma (for ﬁnally achieving a sorted array) and short-term gamma (for all
other rewards) in the Bellman equations, in order to balance immediate and long-term
rewards, fostering strategies that are both eﬀective and eﬃcient over the course of an
episode.
3.1.2 Reinforcement Learning Agent
We use a simple transformer model for this reinforcement learning task, which pre-
dicts actions within the sorting environment. Unless otherwise speciﬁed, our default
transformer has 4 layers, 8 heads, and a hidden dimension of 192. Sinusoidal positional
vectors are used for the position embeddings. We use the TransformerEncoderLayer
of Pytorch, and Relu activation function.
Our reinforcement learning model is very diﬀerent from many existing approaches
such as Neural Turing Machines (NTMs) [6], Diﬀerentiable Neural Computers (DNCs)
[7], and AlphaDev [8]. The above models observe the whole input array simultaneously,
which enables them to directly select the smallest element or sorting the array. In
contrast, our transformer-based agent can only use “Compare” operations to learn
about the underlying array, which is consistent with most sorting algorithms including
Selection Sort, Bubble Sort, QuickSort and Merge Sort.
Please note that although our agent can learn to produce a sequence of “Compare”
and “Swap” actions to sort an array, such sequences seldom follow any algorithmic
patterns, and it is diﬃcult to create an algorithm based on such sequences.
3.1.3 Experiment Results
Before going into more details of AlgoPilot, we ﬁrst present our experiment results of
our simple reinforcement learning agent in this sorting environment. All our experi-
ments were done using a computer with a NVIDIA A6000 GPU, an Intel i7-12700K
CPU, with Ubuntu 18.04 and Pytorch 2.0.0.
The sorting environment is conﬁgured as follows. The reward of sorting success is
0.5, which uses a gamma of 0.99 (i.e., it barely decays over the steps). All other rewards
have a gamma of 0.7, which decays quickly over several steps. Each step has a reward
of -0.3, and each swap that moves a smaller element to the front has a reward of 1.0.
We study random arrays of diﬀerent sizes (from 6 to 14, step 2). For each array
size, we allow the model to perform at most 3 · array size2 operations. If the array is
sorted before this limit on #operations is hit, we say the model successfully sorts this
array.
We use an Epsilon-greedy policy for exploration-exploitation trade-oﬀ. Epsilon is
set to 0.5 at the beginning, and will gradually drop to 0.05, which decays by a factor
of e every 1000 episodes. We run 50,000 episodes in each test. Learning rate is set to
1e −4.
We focus on two metrics for our reinforcement learning model: Success rate and
Number of operations (both Compare and Swap). Figure 1 shows the success rate
6


--- Page 7 ---
vs. number of episodes, which illustrates how the model learns to sort an array during
the training process of reinforcement learning. We can see the success rate is close to
100% for array sizes 6, 8 and 10. The success rate is around 95% for array sizes 12
and 141.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
·105
0
0.2
0.4
0.6
0.8
1
#Episode
Success Rate
Success Rate vs. #Episode for Diﬀerent Array Sizes
Size 6
Size 8
Size 10
Size 12
Size 14
Fig. 1: Success Rate vs. #Episode for various array sizes. We run 100,000 #Episode
for each array size from 6 to 14 (step 2), and have at least about 95% success rate in
each setting, which means our model successfully sort the array within 3 · array size2
operations.
Now let us see how our model compares with QuickSort, a close-to-optimal sorting
algorithm using only “Compare” and “Swap” operations, in term of number of oper-
ations. The expected numbers of “Compare” and “Swap” operations of QuickSort are
shown below, with details in Appendix A.
E[C(n)] = 2 (n + 1) Hn −2 n,
E[S(n)] = (n + 1) Hn −2 n,
where
Hn =
n
X
k=1
1
k .
Figure 2 shows the number of operations used to successfully sort an array of each
size. Each horizontal line represents the expected number of operations of Quicksort
1We tried array size of 16 and the model did not achieve a signiﬁcant success rate
7


--- Page 8 ---
for each array size, and the plot in the same color represents the number of operations
of our model. We can see that for each array size, the number of operations keeps
getting lower in the training process (i.e., as #Episodes grows). The model uses fewer
operations for smaller array sizes, and more operations than Quicksort for larger array
sizes. It seems that our model can learn a procedure that is quite optimal for smaller
arrays. While for larger arrays the model’s procedure is less optimal.
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
·105
0
20
40
60
80
100
120
140
160
#Episode
#Operation
#Operation vs. #Episode for Diﬀerent Array Sizes
Model 6
Quicksort 6
Model 8
Quicksort 8
Model 10
Quicksort 10
Model 12
Quicksort 12
Model 14
Quicksort 14
Fig. 2: #Operations vs. #Episode for various array sizes. We run 100,000 #Episode
for each array size from 6 to 14 (step 2). Each horizontal line represents the expected
number of operations of Quicksort for each array size, and the plot in the same color
represents the number of operations of our model. We can see that the model uses
fewer operations for smaller array sizes, and more operations than Quicksort for larger
array sizes.
Although the model can learn to sort an array eﬃciently and eﬀectively, its
sequence of operations does not seem to follow any algorithmic pattern. Here is an
example in which the model successfully sorts an array of size 6. The operations seems
rather arbitrary and we cannot identify any pattern.
len6 Compare 0 4 more Swap Compare 2 3 more Swap Compare 4 5 more Swap
Compare 1 2 more Swap Compare 3 4 more Swap Compare 2 3 less Compare 0 1
less Compare 3 4 less Compare 4 5 more Swap
In the following subsections we will describe AlgoPilot, which guides the model to
learn to generate sequences of operations that follow algorithmic patterns.
8


--- Page 9 ---
3.2 Random Function Generator
In order to train a language model for trajectories (which are sequences of operations in
our study) of programs, we need to create a random function generator which generates
random functions of a particular type. In this study we focus on functions with double-
loops. With help from ChatGPT-o1, we create a random function generator that can
generate a python function with a double-loop, as shown in Algorithm 1 and Algorithm
2.
Algorithm 1 GENERATE RANDOM CONDITION
Input: None
Output: A single line containing a random if statement using Compare(...).
possible refs ←[“arr[i]”, “arr[i-1]”, “arr[i+1]”, “arr[j]”, “arr[j-1]”, “arr[j+1]”, “y”]
left expr ←RandomChoice(possible refs)
right expr ←RandomChoice(possible refs)
Ensure left expr ̸= right expr if possible
possible return checks ←[“> 0”, “< 0”, “== 0”, “>= 0”, “<= 0”]
op to zero ←RandomChoice(possible return checks)
return ‘‘if self.Compare(left expr, right expr) op to zero:’’
Algorithm 2 GENERATE RANDOM FUNCTION
Output: A string deﬁning a new method with nested loops and a random condition.
func name ←“random func doubleloop”
func signature ←“def random func doubleloop(self):”
arr ←self.arr
⊲Reference the instance array
n ←len(arr)
Outer Loop:
Randomly choose:
for i in range(n - 1)
for i in range(n)
With some probability:
y ←arr[i]
Inner Loop:
Randomly choose:
for j in range(i+1, n)
for j in range(n - i - 1)
Condition Inside Inner Loop:
condition line ←GENERATE RANDOM CONDITION()
if self.Compare(...):
Call self.Swap()
return The entire function deﬁnition as a string.
9


--- Page 10 ---
Listing 1 shows an example random function generated, and it deﬁnitely cannot
sort an array. In order to generate a trajectory, this function randomly outputs Com-
pare and Swap operations. Please note that AlgoPilot is not aware of what a random
function can do. It only uses trajectories of such randomly generated functions to
train a language model for trajectories, and never knows whether these trajectories
lead to sorted array or not. The learning to sort is done using the reinforcement learn-
ing environment by rewarding useful operations (described in Section 3.1) , which is
completely independent from the generation of random functions.
Here is an example trajectory:
len14 Compare 0 1 more Compare 1 1 less Swap Compare 2 1 more Compare 3 1
more Compare 4 1 more Compare 5 1 more Compare 6 1 more Compare 7 1 more
Compare 8 1 more Compare 9 1 more Compare 10 1 more Compare 11 1 more
Compare 12 1 more Compare 13 1 more Compare 1 2 less Compare 2 2 less Swap
Compare 3 2 more ...
10


--- Page 11 ---
Listing 1: A randomly generated function with a double-loop
class RandomDoubleLoopFunction :
def
i n i t
( s e l f , n=14):
s e l f . arr = l i s t ( range (1 , n + 1))
random . s h u f f l e ( s e l f . arr )
s e l f . o r i g a r r = l i s t ( s e l f . arr )
s e l f . idx1 ,
s e l f . idx2 = −1, −1
s e l f . output = [ ’ len16 ’ ]
def Swap( s e l f ) :
i ,
j = s e l f . idx1 ,
s e l f . idx2
s e l f . arr [ i ] ,
s e l f . arr [ j ] = s e l f . arr [ j ] ,
s e l f . arr [ i ]
s e l f . output . append ( ”Swap” )
def Compare( s e l f , a , b ) :
s e l f . idx1 ,
s e l f . idx2 = s e l f . o r i g a r r . index ( a ) ,
s e l f . o r i g a r r . index (b)
outcome = ” equal”
i f
a > b :
outcome = ”more”
else :
outcome = ” l e s s ”
s e l f . output . append ( f ”Compare  { s e l f . idx1 }  { s e l f . idx2 }  {outcome}” )
return ( a > b) −( a < b)
def random func doubleloop ( s e l f ) :
arr = s e l f . arr
n = len ( arr )
for
i
in range (n ) :
y = arr [ i ]
for
j
in range ( i + 1 , n ) :
i f
s e l f . Compare( arr [ j −1] ,
arr [ i +1]) == 0 :
s e l f . Swap ()
3.3 Trajectory Language Model (TLM)
Given a sequence like “len16 Compare 0 1 more Compare 1 1 less Swap Compare
2 1 more Swap Compare 3 1 more ...”, we want to train a language model to pre-
dict the probability of seeing each token given the preﬁx. Since the vocabulary is very
small (with 36 diﬀerent tokens), we prefer a medium-sized language model. We choose
QWen-0.5B [14], which has the best benchmark results among all models with < 1B
parameters on the Hugging Face Open LLM Leaderboard2.
2https://huggingface.co/spaces/open-llm-leaderboard/open llm leaderboard
11


--- Page 12 ---
All our experiments were done using a computer with a NVIDIA A6000 GPU,
an Intel i7-12700K CPU, with Ubuntu 18.04 and Pytorch 2.0.0. We created 1.5M
randomly generated functions with double-loop, and generated a trajectory using each
of them. These 1.5M trajectories were randomly split into a training set (1.44M) and
a testing set (0.06M).
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
·105
6 · 10−2
7 · 10−2
8 · 10−2
9 · 10−2
0.1
0.11
0.12
0.13
0.14
0.15
Steps (with batch size 16)
Loss
Training and Validation Loss vs Steps
Training Loss
Validation Loss
Fig. 3: Comparison of Training Loss and Validation Loss over Training Steps with
batch size 16, for training Trajectory Language Model.
3.4 Guided Reinforcement Learning
In this subsection we will describe how AlgoPilot uses the Trajectory Language Model
(TLM ) to guide the training of its reinforcement learning model described in Section
3.1. As shown in Section 3.1, although the model can learn to sort an array using
Compare and Swap operations, the trajectory of such operations does not follow any
algorithmic pattern. Therefore, we would like to add an additional reward (or penalty)
to the reinforcement learning environment according to the TLM. If an operation has
high probability according to the TLM, a relatively high reward should be given to
agent of reinforcement learning. If an operation has low probability, a low or negative
reward should be given.
Here we revisit the reward settings of the sorting environment. The reward of
sorting success is 0.5, which uses a gamma of 0.99 (i.e., it barely decays over the steps).
All other rewards have a gamma of 0.7, which decays quickly over several steps. Each
12


--- Page 13 ---
step has a reward of -0.3, and each swap that moves a smaller element to the front
has a reward of 1.0.
Suppose prob(a) is probability of an action a according to the TLM. We deﬁne the
loss of action a as
loss(a) = −ln prob(a)
(1)
The additional reward is set to
T LMreward(a) = −0.02 · loss(a)
Let us ﬁrst look at the success rate and number of operations for the above
approach (for arrays of size 8). As shown in Figure 4, AlgoPilot achieves about 95%
of success rate, with a reasonably low number of operations.
1
2
3
4
5
6
7
8
9
·104
0.2
0.4
0.6
0.8
1
#Episodes
Success Rate
Success Rate
34
36
38
40
42
44
46
#Operations
#Operations
Fig. 4: Success Rate and #Operations vs. #Episodes for AlgoPilot with Guided
Reinforcement Learning
Here is an example trajectory after training for 10,000 episodes: len8 Compare 0 1
more Swap Compare 1 2 more Swap Compare 2 3 more Swap Compare 3 4 more
Swap Compare 4 5 more Swap Compare 5 6 more Swap Compare 6 7 more Swap
Compare 1 2 less Compare 2 3 more Swap Compare 3 4 less Compare 4 5
more Swap Compare 5 6 more Swap Compare 0 1 more Swap Compare 2 3 less
Compare 3 4 less Compare 4 5 more Swap Compare 1 2 more Swap Compare
2 3 more Swap Compare 3 4 more Swap Compare 4 5 more Swap Compare 1 2
less Compare 2 3 more Swap.
We feed the above trajectory into GPT-4o-mini, and ask it to generate a python
function that is likely generate the above trajectory (with discrepancies allowed). The
13


--- Page 14 ---
prompt is in Appendix B. The python function generated by GPT-4o-mini is shown
below:
def generate_actions(a):
actions = []
n = len(a)
# Compare and swap logic, similar to a bubble sort approach
for i in range(n - 1):
for j in range(n - i - 1):
actions.append(f"Compare {j} {j + 1}")
if a[j] > a[j + 1]:
actions.append("more")
# Swap elements if the left one is greater
a[j], a[j + 1] = a[j + 1], a[j]
actions.append("Swap")
elif a[j] < a[j + 1]:
actions.append("less")
else:
actions.append("equal")
return actions
# Example usage
a = [8, 7, 6, 5, 4, 3, 2, 1]
actions = generate_actions(a)
for action in actions:
print(action)
We can see that the above function is a perfect implementation of Bubble Sort. It
is possible that GPT-4o-mini generates this algorithm because it knows Bubble Sort.
But if we look at the trajectory, anybody with basic programming experiences would
be able to ﬁgure out the algorithm, even she does not know Bubble Sort.
len8 Compare 0 1 more Swap Compare 1 2 more Swap Compare 2 3 more Swap Compare
3 4 more Swap Compare 4 5 more Swap Compare 5 6 more Swap Compare 6 7 more Swap
(missing Compare 0 1) Compare 1 2 less Compare 2 3 more Swap Compare 3 4 less
Compare 4 5 more Swap Compare 5 6 more Swap Compare 0 1 more Swap (missing
Compare 1 2) Compare 2 3 less Compare 3 4 less Compare 4 5 more Swap (missing
Compare 0 1) Compare 1 2 more Swap Compare 2 3 more Swap Compare 3 4 more Swap
Compare 4 5 more Swap (missing Compare 0 1) Compare 1 2 less Compare 2 3 more
Swap
As shown above, the trajectory generated by our model (after 10,000 episodes)
misses 4 comparisons, possibly because in most random arrays it is unnecessary to
repeatedly compare the ﬁrst two elements in each outer loop. We show the number of
discrepancies for the trajectory generated by AlgoPilot’s model trained for diﬀerent
numbers of episodes in Figure 5. The number of discrepancies remains rather stable
(between 3 and 5). We put each of the ten trajectories into GPT-4o-mini, and in each
case a correct Bubble Sort algorithm is created.
14


--- Page 15 ---
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
·105
0
1
2
3
4
5
#Episodes
Discrepancies
Fig. 5: Discrepancies vs. #Episodes for trajectories generated by AlgoPilot compared
with Bubble Sort
4 Discussions and Future Work
Let us revisit the procedure of AlgoPilot in creating an algorithm for using a double-
loop to sort an array:
1. Random Function Generator
Use a random function generator to generate millions of random python function
with a double-loop, and randomly add the two operations “Compare” and “Swap”.
2. Trajectory Language Model (TLM)
Train a language model using the trajectories of the random functions, which can
predict the probability of observing the next token after a preﬁx.
3. Reinforcement Learning
Use reinforcement learning to train a transformer model for sorting, using only
Compare and Swap operations, with the environment only returning a simple feed-
back for each operation (greater, equal, less for Compare and sorted for
Swap).
4. Guided Reinforcement Learning
Use the Trajectory Language Model (TLM) to enhance the reinforcement learning
process by adding an additional reward when the next operation has a high proba-
bility according to the TLM. This guides the model to learn to generate a trajectory
that can be produced by an algorithm.
5. Algorithm Creation
Use an LLM (such as GPT-4o-mini) to generate an algorithm or a python function
based on the trajectory.
We can see that the above procedure does not use any prior knowledge of a sorting
algorithm, except that in Step 5 (Algorithm Creation) where the LLM is probably
aware of all popular algorithms. Although it often looks obvious and trivial to create
15


--- Page 16 ---
an algorithm based on a good trajectory, we still hope to get rid of any prior knowledge
to achieve fully autonomy in algorithm creation.
Therefore, we will consider the creation of an algorithm based on a trajectory as
our future work. We will use our random function generator to generate millions of
functions and corresponding trajectories, and add random noise to the trajectories.
Then we will train a sequnece-to-sequence model to predict the function based on the
trajectory. It will be the last piece of puzzle to be added to the AI that can create an
algorithm in a fully automated fashion.
Appendix A
Expected Numbers of Compare and
Swap operations of Quicksort
A.1
Model of QuickSort
We consider the classic (textbook) QuickSort algorithm with the following character-
istics:
1. Pivot Selection: Choose one pivot uniformly at random from the current subarray.
2. Partitioning Scheme: Utilize the Lomuto partition scheme:
• Compare each of the other n −1 elements to the pivot.
• Each time an element smaller than the pivot is found, increment a pointer and
perform a swap.
• Finally, perform one additional swap to place the pivot in its correct position at
the boundary.
3. Recursion: Recursively sort the subarrays to the left and right of the pivot.
We aim to count:
• #Comparisons: The number of times array elements are compared against the
pivot.
• #Swaps: The number of times two elements in memory are exchanged.
Both counts are treated as random variables due to the random selection of pivots.
A.2
Expected Number of Comparisons
Let C(n) denote the (random) number of comparisons QuickSort makes on an array
of size n. The recurrence relation for the expected number of comparisons is:
C(n) = (n −1) + C(k) + C(n −k −1),
where:
• (n −1) is the number of comparisons in the current partition.
• k is the rank of the chosen pivot (0 ≤k ≤n −1), selected uniformly at random.
• C(k) and C(n −k −1) are the recursive calls on the resulting subarrays.
Taking expectations on both sides:
16


--- Page 17 ---
E[C(n)] = (n −1) + 1
n
n−1
X
k=0
(E[C(k)] + E[C(n −k −1)]) .
Solving this recurrence leads to the closed-form expression involving harmonic
numbers:
E[C(n)] = 2(n + 1)Hn −2n
where Hn = 1 + 1
2 + · · · + 1
n is the n-th harmonic number. For large n, Hn
approximates to ln n + γ (Euler-Mascheroni constant, γ ≈0.5772), yielding:
E[C(n)] ∼2n ln n
In base-2 logarithms, this can be expressed as:
E[C(n)] ≈1.386 n log2 n
A.3
Expected Number of Swaps
Let S(n) denote the (random) number of swaps performed using the Lomuto partition
scheme. The expected number of swaps in a single partition step is approximately:
n −1
2
+ 1 = n + 1
2
Considering recursion, the recurrence relation is:
S(n) = S(k) + S(n −k −1) + (swaps in current partition) ,
Taking expectations:
E[S(n)] = 1
n
n−1
X
k=0
(E[S(k)] + E[S(n −k −1)]) + n + 1
2
Solving this recurrence yields:
E[S(n)] = (n + 1)Hn −2n
For large n, this simpliﬁes to:
E[S(n)] ∼n ln n
In base-2 logarithms:
E[S(n)] ≈0.693 n log2 n
17


--- Page 18 ---
A.4
Final Exact Formulas
• Expected Number of Comparisons:
E[#Comparisons] = 2(n + 1)Hn −2n
• Expected Number of Swaps:
E[#Swaps] = (n + 1)Hn −2n
Where Hn is the n-th harmonic number, deﬁned as:
Hn = 1 + 1
2 + 1
3 + · · · + 1
n
As n →∞, we have:
Hn ∼ln n + γ,
where γ is Euler’s constant. Therefore, the asymptotic behaviors are:
E[#Comparisons] ∼2n ln n,
E[#Swaps] ∼n ln n.
A.5
Caveats and Variations
1. Partition Scheme: The exact constants diﬀer if using Hoare’s partition scheme
instead of Lomuto’s. Hoare’s scheme generally results in fewer swaps.
2. Pivot Selection Strategy: Diﬀerent pivot selection strategies, such as median-
of-three, can alter the constants and lower-order terms in the expected counts,
although the overall Θ(n log n) scaling remains unchanged.
3. Worst-Case Behavior: Deterministic worst-case scenarios (e.g., always selecting
the largest element as pivot) can lead to Θ(n2) comparisons and swaps. However,
this is avoided in expectation with random pivot selection.
Appendix B
LLM prompt for algorithm
generation from a trajectory
Given a trajectory generated by our model in guided reinforcement learning, we use
the following prompt to let GPT generate a python function that is likely to generate
the trajectory (with small discrepancies allowed):
Below is the sequence of actions of a python function that operates
on an array of integers named "a", and its will receive feedbacks from
an environment after it takes actions. Action "Compare i j" means the
python function compares a[i] and a[j]. If a[i] is greater than a[j],
the environment will return "more". If a[i] is smaller than a[j], it
will return "less". Otherwise it will return "equal". Action "Swap"
18


--- Page 19 ---
means the python function swaps a[i] and a[j]. Note this sequence of
actions may contain noises, i.e., some actions may be missing and some
others added.
Sequence of actions and feedbacks:
‘‘len8 Compare 0 1 more Swap Compare 1 2 more Swap Compare 2 3 more
Swap Compare 3 4 more Swap Compare 4 5 more Swap Compare 5 6 more Swap
Compare 6 7 more Swap Compare 1 2 less Compare 2 3 more Swap Compare
3 4 less Compare 4 5 more Swap Compare 5 6 more Swap Compare 0 1 more
Swap Compare 2 3 less Compare 3 4 less Compare 4 5 more Swap Compare
1 2 more Swap Compare 2 3 more Swap Compare 3 4 more Swap Compare 4 5
more Swap Compare 1 2 less Compare 2 3 more Swap’’
Now, please write a python function that is likely to generate this
sequence of actions. Your function should not generate the noises.
References
[1] Devlin, J., Bunel, R., Singh, R., Hausknecht, M., Kohli, P.: Robustﬁll: Neu-
ral program learning under noisy i/o. In: Proceedings of the 34th International
Conference on Machine Learning (ICML’17), pp. 990–998 (2017)
[2] Balog, M., Gaunt, A.L., Brockschmidt, M., Nowozin, S., Tarlow, D.: Deepcoder:
Learning to write programs. In: Proceedings of the 5th International Conference
on Learning Representations (ICLR’17) (2017)
[3] Reed, S., Freitas, N.: Neural programmer-interpreters. International Conference
on Learning Representations (2015)
[4] Veliˇckovi´c, P., Blundell, C., Rae, J.: A generalist neural algorithmic learner.
International Conference on Learning Representations (2021)
[5] Zhang, Y., Bengio, Y.: Neural algorithmic reasoning with causal regularisation.
Advances in Neural Information Processing Systems (2022)
[6] Graves, A., Wayne, G., Danihelka, I.: Neural turing machines. arXiv preprint
arXiv:1410.5401 (2014)
[7] Graves, A., Wayne, G., Danihelka, I.: Hybrid computing using a neural network
with dynamic external memory. Nature (2016)
[8] DeepMind: Alphadev: A general ai system for optimizing algorithms. arXiv
preprint arXiv:2305.13550 (2023)
[9] Abolaﬁa, D.A., Norouzi, M., Shen, J., Zhao, R., Le, Q.V.: Neural program
synthesis with priority queue training. arXiv preprint arXiv:1801.03526 (2018)
[10] Gulwani, S.: Automating string processing in spreadsheets using input-output
examples. In: Proceedings of the 38th ACM SIGPLAN-SIGACT Symposium
19


--- Page 20 ---
on Principles of Programming Languages (POPL’11), pp. 317–330 (2011).
https://doi.org/10.1145/1926385.1926423
[11] Solar-Lezama, A.: Program synthesis by sketching. PhD thesis, University of
California, Berkeley (2008)
[12] Bodik, R., Phothilimthana, P., Torlak, E.: Angelic programming. In: Proceedings
of the 4th International Workshop on Practical Aspects of Declarative Languages
(PADL’07), pp. 201–215 (2007). https://doi.org/10.1007/978-3-540-69611-7 14
[13] Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H.P.d.O., Kaplan, J., Edwards,
H., Burda, Y., Joseph, N., Brockman, G.: Evaluating large language models
trained on code. arXiv preprint arXiv:2107.03374 (2021)
[14] Yang,
A.,
et
al.:
Qwen2.5
technical
report
(2024)
https://doi.org/10.48550/arXiv.2412.15115 . arXiv preprint arXiv:2412.15115
20


--- Page 21 ---


--- Page 22 ---
