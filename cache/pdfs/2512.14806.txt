--- Page 1 ---
Let the Barbarians In:
How AI Can Accelerate Systems Performance Research
Audrey Cheng∗, Shu Liu*, Melissa Pan, Zhifei Li, Shubham Agarwal, Mert Cemri,
Bowen Wang, Alexander Krentsel, Tian Xia, Jongseok Park, Shuo Yang, Jeff Chen,
Lakshya Agrawal, Ashwin Naren, Shulu Li, Ruiying Ma, Aditya Desai, Jiarong Xing,
Koushik Sen, Matei Zaharia, Ion Stoica
UC Berkeley
Abstract
Artificial Intelligence (AI) is beginning to transform the re-
search process by automating the discovery of new solutions.
This shift depends on the availability of reliable verifiers,
which AI-driven approaches require to validate candidate so-
lutions. Research focused on improving systems performance
is especially well-suited to this paradigm because system per-
formance problems naturally admit such verifiers: candidates
can be implemented in real systems or simulators and evalu-
ated against predefined workloads. We term this iterative cycle
of generation, evaluation, and refinement AI-Driven Research
for Systems (ADRS). Using several open-source ADRS
instances (i.e., OpenEvolve, GEPA, and ShinkaEvolve), we
demonstrate across ten case studies (e.g., multi-region cloud
scheduling, mixture-of-experts load balancing, LLM-based
SQL, transaction scheduling) that ADRS-generated solutions
can match or even outperform human state-of-the-art designs.
Based on these findings, we outline best practices (e.g.,
level of prompt specification, amount of feedback, robust
evaluation) for effectively using ADRS, and we discuss future
research directions and their implications. Although we do not
yet have a universal recipe for applying ADRS across all of
systems research, we hope our preliminary findings, together
with the challenges we identify, offer meaningful guidance
for future work as researcher effort shifts increasingly toward
problem formulation and strategic oversight.
Note: This paper is an extension of our prior work [14]. It
adds extensive evaluation across multiple ADRS frameworks
and provides deeper analysis and insights into best practices.
1
Introduction
One of the most ambitious goals of artificial intelligence (AI)
is to revolutionize scientific discovery by automating algo-
rithm design, experiment execution, and even the research
process itself. While the realization of this goal will likely be
uneven, with certain domains being transformed earlier and
∗Equal contribution, ordered alphabetically.
more profoundly than others, AI-driven approaches [5, 26, 38,
58, 71] have already reached a level of capability where they
can meaningfully contribute to computer systems research.
A significant portion
of systems research, spanning
networking, databases, and distributed systems, is dedicated
to enhancing performance. This is typically achieved through
the meticulous, human-driven design of new algorithms for
tasks such as routing [2, 9, 76], scheduling [13, 80], and
resource management [22, 34]. Crucially, the novelty and
efficacy of these algorithms are often the primary metrics for
a publishable paper.
Our first inquiry in this paper is to explore whether a new
class of AI-driven approaches, which we term AI-Driven
Research for Systems (ADRS), can generate algorithms that
surpass human state-of-the-art solutions for systems perfor-
mance problems. To answer this inquiry, we use three emerg-
ing open-source ADRS frameworks (e.g., OpenEvolve [71],
GEPA [5], and ShinkaEvolve [38]) across ten real research
tasks and show that these frameworks can already generate
solutions that match or even exceed the performance of
state-of-the-art, human-designed solutions. For example, in
a load-balancing problem for a Mixture-of-Experts (MoE)
model, OpenEvolve discovers an algorithm to rebalance
experts across GPUs that is 13× faster than the best-known
baseline. In a job scheduling problem aimed at reducing
costs by using spot instances across multiple cloud regions,
OpenEvolve generates a solution that achieved roughly 35%
greater savings than an expert-developed baseline (Table 2).
Given these compelling results, we turn to the question of
how to best apply these frameworks for solution discovery.
We conduct extensive ablation studies across all components
of the evolutionary process and based on these findings,
outline best practices along three axes: problem specification,
solution evaluation, and feedback. For specification, we
find that “less is more and more is less”: the amount of
information we provide is critical to navigate the trade-off
between exploration and exploitation, i.e., between quickly
finding a solution and spending additional time and resources
searching for a better solution. For evaluation, we find
that the “generated solutions are only as strong as their
arXiv:2512.14806v4  [cs.SE]  22 Dec 2025


--- Page 2 ---
evaluators”: discovering effective solutions requires ensuring
robust verification and diverse test sets. Finally, we show
that for feedback, the “devil is in the details”: maximizing
performance depends on calibrating feedback granularity to
provide actionable guidance without overfitting.
Finally, we discuss the consequences of AI-driven research
for the systems community. As AI increasingly takes
on the role of algorithm discovery and optimization, the
emphasis for human researchers will likely pivot to problem
formulation, high-level ideation, and strategic direction. In
this new model, the researcher acts as an advisor to powerful
AI research assistants:
defining meaningful problems,
proposing creative starting points, and distilling insights from
generated solutions. This approach can create a powerful
virtuous cycle: the same AI-driven methodologies can be
applied to improve the AI systems themselves, leading to a
compounding acceleration of the pace of discovery.
In summary, we introduce no new problems, algorithms, or
mechanisms in this paper. Instead, we evaluate how emerging
AI-driven approaches may substantially accelerate systems
research and articulate why embracing this approach may be
essential as the field evolves. We make three contributions:
• We study the capabilities of existing open-source ADRS
frameworks for systems performance problems across ten
case studies, showing that AI-generated algorithms can
outperform human state-of-the-art solutions (e.g., 13×
faster load balancing and 35% lower cloud costs).
• We suggest a set of best practices for problem specification,
evaluation, and feedback to help researchers effectively
apply ADRS.
• We discuss the implications of this shift for the systems
community, outlining the evolving role of researchers, and
identifying open challenges in AI-assisted discovery.
2
Why AI-Driven Research for Systems?
In this paper, we advocate for an AI-driven approach
to systems performance problems. While performance
optimization is not the sole focus of systems research,
it remains a central one—a brief survey of top systems,
networking, and database venues (NSDI, OSDI, SIGMOD,
SOSP, and VLDB) shows that over one-third of published
papers feature performance optimization algorithms as their
core contribution. We argue that AI-driven methods are
particularly well-suited to this domain because candidate
solutions can be verified robustly and at low cost.
First, verifying whether a solution improves performance is
straightforward. Such solutions typically introduce new tech-
niques or algorithms implemented directly within the systems
they aim to optimize. Verification amounts to running these
systems under representative workloads and checking for im-
provements over baselines on relevant performance metrics.
Second, solutions to systems performance problems
typically preserve the correctness of the original system, or
at least, verifying this is straightforward. For instance, it is
straightforward to check whether a load-balancing algorithm
schedules all assigned tasks or whether a network router
forwards all packets it receives.
Third, the algorithmic code targeted for evolution is often
relatively small, e.g., the core logic of a scheduler, load
balancer, or resource allocator. This makes the generated
code easier for humans to interpret, which can further help
with correctness verification. In all our case studies, we
readily understood the generated solutions and identified
their main ideas and techniques (see Section 4). As these
tools mature, we expect the scope of modification to expand
across multiple components (e.g., when designing complex
distributed protocols). Maintaining code interpretability in
such cases is an important topic for future research.
Finally, systems researchers often use simulators to develop
and evaluate solutions before deployment. Since simulator-
based verification is relatively cheap, evaluation remains
practical even if the search process generates excessive candi-
date solutions. For example, most of our case studies required
only a few hours and cost less than several tens of dollars. That
said, building inexpensive yet faithful simulators for complex
systems (e.g., operating systems, databases) is far from trivial
and remains a topic for future research (Section 6.2.1).
3
Using AI to Accelerate Systems Research
This section provides an overview of the systems research
process and then introduces the AI-Driven Research for
Systems (ADRS) approach, which accelerates this process
through automatic solution discovery and evaluation.
3.1
Systems Performance Research Process
The typical systems research process spans weeks or months
and consists of five stages (Figure 1a):
• Problem Formulation: Precisely define the problem to
solve (e.g., improving system throughput). This problem
anchors the entire research process and the final results.
• Evaluation Framework: Develop a framework to imple-
ment and evaluate potential solutions. This may be the
system itself or a simulator that approximates the system’s
behavior. Even when the system exists, a simulator may be
used to accelerate iteration. Researchers also assemble or
reuse workloads (traces) or benchmarks to drive evaluation.
• Solution: Design a solution (e.g., an algorithm) for the
problem, such as a new scheduling or search technique.
• Evaluation: Implement the solution in the simulator or sys-
tem, evaluate its performance using the selected workloads,
and compare it against baseline(s). If the solution does not


--- Page 3 ---
000
Solution 
Design new 
solution to solve 
the problem
Paper 
Write-Up
Communicate 
findings in 
research paper
Evaluation
Parametric evaluation 
(formula, cost model, 
etc.) in an emulator, 
simulator, or real system
Scientist 
Develop the system or 
a simulator, integrate 
workload traces, 
implement baselines
Evaluation 
Framework
Experimentation
Loop
Problem 
Formulation
Literature review,
problem setup: 
hypothesis, objective, 
constraints, workloads
-
(a)
00000
Solution
Generator
LLM ensemble to 
generate solutions
Evaluator
Tests solutions and 
assigns scores &
feedback
Configs (e.g., LLMs)
Scientist 
Problem 
Formulation
Paper 
Write-Up
Evaluation 
Framework
Solution
Selector
Select promising 
solutions to refine 
Prompt
Generator
Creates context-
rich prompts
Storage
Stores solutions 
scores & 
feedbacks
Inner loop
AI-Driven Research Systems (ADRS) 
Observations 
(e.g., solution, feedback)
Outer loop
Evaluator +
Initial Solution  
Problem
statement 
(b)
Figure 1: (a) The five stages of a typical systems research process. (b) The AI-driven Research for Systems (ADRS) architecture
instantiated in the same process. ADRS (Grey area) augments the scientist by automating the Solution and Evaluation stages
while leaving the other stages unchanged.
improve performance, researchers return to the Solution
stage to refine the approach or develop alternatives.
• Paper Write-Up: Once a solution achieves the desired
results, document and communicate the findings.
3.2
AI-Driven Research for Systems (ADRS)
As Large Language Models (LLMs) have progressed from
simple text completion to sophisticated reasoning and tool
use, new architectures have been developed to enhance the
reliability and scope of tasks to which they can be applied.
In this paper, we focus on how LLMs can be used to design,
implement, and evaluate new solutions (e.g., algorithms)
to solve systems research problems. We call this approach
AI-Driven Research for Systems (ADRS) and depict it in
Figure 1b. ADRS operationalizes the two iterative stages
of the systems research process—Solution and Evalua-
tion—shown in Figure 1a. Together, these stages account
for roughly 40% of total research time (based on a survey
of over 30 systems researchers, Figure 7 in Appendix A).
At its core, ADRS implements an iterative loop that
constructs or refines prompts for LLMs, generates new
solutions or improves existing ones, and evaluates these
solutions in either a real system or a simulator. This loop
continues until a satisfactory solution emerges, the resource
budget is exhausted, or the researcher terminates it. ADRS
consists of five components:
• Prompt Generator: Creates the prompt used to generate
the solution. This prompt contains the problem statement,
context (e.g., simulator or system code), and previous
solutions (from the Solution Selector) to guide refinement.
• Solution Generator: Feeds the prompt from the Prompt
Generator to one or more LLMs to generate a new solution
or refine an existing one, typically by directly editing the
code in the simulator or real system.
• Evaluator: Takes the solution from the Solution Generator
and runs it against a predefined set of workloads. The
solution is scored based on performance and can use an
LLM to provide qualitative feedback.
• Storage:
Persists
solutions, outputs, scores, and the
feedback provided by the Evaluator.
• Solution Selector: Chooses a subset of solutions from
Storage and provides them to the Prompt Generator to
seed the next generation.
Together, these components form an automated inner
feedback loop that enables ADRS to iteratively refine
solutions. This can be paired with an outer loop in which
a human can observe the generated solutions and provide
high-level guidance for future prompts.
In most cases, ADRS leverages simulators rather than
real systems for two main reasons. First, the real system’s
codebase is often too large to fit within the context window
of current LLMs. Second, running evaluations in a simulator
can be orders of magnitude faster than on the real system,
greatly accelerating iteration.
As mentioned earlier, ADRS automates the Solution
and Evaluation stages of the research process depicted in
Figure 1a. As such, ADRS can accelerate two of the most
time-consuming stages (represented in orange in Figure 1a):
Solution Development and Evaluation by iteratively
proposing and refining solutions until a satisfactory one
is discovered or the iteration limit is reached, optionally
incorporating researcher guidance.
In addition to speed, we believe ADRS is valuable because
it transcends the domain-specific expertise of human
researchers. LLMs are trained on vast, diverse datasets and
can discover novel solutions by identifying patterns from un-
related domains–solutions that a human expert might simply
overlook due to their specialized focus. In our experiments,
LLMs combined techniques from diverse domains such as
Political Science and Economics to outperform human SOTA
solutions (Table 4, Appendix 4.6). For example, for the
MoE load balancing algorithm (EPLB), GEPA with GPT-5
generated a solution that uses Hamilton’s Apportionment


--- Page 4 ---
Framework
Parent Selection
Evolution Context for Generation
Evaluation & Cost
OpenEvolve
Sample a parent per island using ex-
ploration, archive-based exploitation,
or random selection.
Parent program plus a few top and
diverse archive programs from the
same island.
Run full validation; higher-scoring pro-
gram is added to the archive. One LLM
call per iteration; no correctness gate.
GEPA
Sample parents from a per-instance
Pareto frontier over validation in-
stances.
Parent code together with minibatch
evaluation traces and metrics.
First require improvement on a small
minibatch, then run full validation.
Approximately one LLM call and
6–11 evaluations per iteration.
ShinkaEvolve
Weighted sampling from a Pareto
archive, with bonuses for novelty and
programs with fewer children.
Parent
plus
six
archive
inspi-
rations,
combined
with
meta-
recommendations based on program
summaries.
Only correct programs are used for
evolution. One LLM plus one embed-
ding call per iteration. 12 LLM calls
for meta-learning every 10 iterations.
Table 1: ADRS frameworks we evaluate in our experiments. All use the same island-based population structure; we highlight
only the design choices and costs relevant to our evaluation.
from Political Science to efficiently assign replica counts for
uniform workloads without expensive search iterations.
Finally, it is important to note that for other stages of the
research process, ADRS has a neutral effect: it makes it
neither harder nor easier for researchers to choose problems,
build evaluation frameworks, or document results. Thus,
ADRS advances the Pareto frontier of the research process.
3.3
ADRS Examples
ADRS aligns with several recent systems that pair LLM-
generated
candidates
with
programmatic
evaluation.
AlphaEvolve [58] is a proprietary ADRS framework from
Google DeepMind that discovers new algorithms using an
evolutionary loop over LLM-generated candidates managed
via MAP-Elites and an island model.
We
evaluate
three
open-source
ADRS
frameworks:
OpenEvolve
[71], GEPA
[5], and ShinkaEvolve
[38]
(Table 1). OpenEvolve re-implements AlphaEvolve’s core
asynchronous pipeline and adds developer utilities, such as
evolution tree visualization. GEPA takes a prompt-centric
approach, using natural-language reflection to mutate
prompts and Pareto filtering to retain diverse high-performing
solutions. ShinkaEvolve emphasizes structured introspection
through weighted archive sampling, a correctness gate that
admits only valid programs, and periodic meta-reflection.
Coding assistants, such as Claude Code [8] and Cursor [31],
can also be viewed as ADRS instances that leverage codebase
context and natural-language prompts to evolve and verify
algorithms through an interactive feedback loop.
4
Evaluation and Case Studies
To rigorously evaluate the capability of ADRS in solving
system performance problems, we investigate ten tasks across
diverse sub-domains, including networking, databases, and
core systems. We summarize our findings in Table 2. Each
case study follows a common schema that captures the prob-
lem setup, environment, evolutionary process, model usage,
and final outcome, as detailed in Table 8 in the Appendix.
In this section, we present a subset of representative case
studies selected to highlight insights to guide future research.
These examples illustrate both the limitations of current
frameworks and the best practices required to overcome
them (detailed further in Section 5). We evaluate three
open-source ADRS frameworks (GEPA, OpenEvolve, and
ShinkaEvolve) using their default configurations. To ensure
a fair comparison, we equalize the generation budget by
capping each run at 100 iterations using the GPT-5 and
Gemini-3.0-Pro-Preview models. We repeat each experiment
three times. The specific configs are provided in Appendix D.
We present the aggregate performance results across all
ADRS use cases in Table 3. OpenEvolve achieved the highest
success rate, delivering the best solution in 9 out of 20 total
cases, followed by ShinkaEvolve with eight and GEPA with
six (noting a tie between OpenEvolve and GEPA on the EPLB
task with Gemini-3). When analyzing model sensitivity, we
observe distinct behaviors: GEPA performs significantly bet-
ter with GPT-5 (four top results) than with Gemini-3 (two top
results), whereas ShinkaEvolve exhibits the inverse, favoring
Gemini-3 (five top results) over GPT-5 (three top results). In
contrast, OpenEvolve demonstrates the greatest versatility,
maintaining consistent performance across both foundation
models (four top results with GPT-5 and five with Gemini-3).
The five case studies cover distributed systems, databases,
and LLM systems, summarized as follows:
• CBL - Optimizing Spot Instance Savings under Deadlines
(Single Region): Given a job with a deadline, the solution
aims to maximize the use of cheaper spot instances in
a public cloud without violating the deadline. ADRS
improves the SOTA result by up to 35% for a single region.


--- Page 5 ---
Task & SOTA Publication
Objective
SOTA / Baseline
Time / Cost
Telemetry Repair [36]
Repair buggy network telemetry.
+11.8% better counter repair score, +47.9%
higher confidence calibration.
8h (100 iters), ≤$15
Cloudcast [79]
Optimize multi-cloud data transfer cost.
No improvement.
1h (100 iters), ≤$15
Expert Parallelism Load
Balancer [6] (EPLB)
Balance expert-parallel load across GPUs.
Same load balance, 13× faster runtime vs.
proprietary implementation.
5h (100 iters), ≤$15
Model Placement [84] (Prism)
Optimize cost for model-to-GPU placement globally.
20.9% cheaper than published solution.
1h (100 iters), ≤$15
LLM-SQL [43]
Reorder tabular data to improve prefix hit rate.
Comparable hit rate, 3.9× faster runtime.
1h (100 iters), ≤$20
Transaction Scheduling [13]
(TXN)
Minimize makespan in transaction scheduling.
60% better than greedy (offline).
<2h (100 iters), ≤$20
Can’t Be Late [80] (CBL)
Schedule deadline-driven jobs on single-region spot
instances.
Up to 16% (average 7%) higher cost savings
vs. SOTA.
5h (100 iters), ≤$30
Can’t Be Late Multi-Region
Extension [WIP] (CBL-Multi)
Schedule deadline-driven jobs on multi-region spot
instances.
26% lower cost vs. single-region baseline.
3h (100 iters), ≤$25
Multi-Agent System Optimiza-
tion [30] (MAS)
Improve multi-agent collaboration using MAST
taxonomy.
7% improvement on ProgramDev.
<2h (100 iters), ≤$15
Datacenter TCP
Congestion
Control [4] (NS3)
Maximize throughput and minimize queue latency.
49% lower queue length, similar throughput.
1h (100 iters), ≤$15
Table 2: Summary of project task objectives and corresponding SOTA publications, performance relative to SOTA and
baseline solutions, and overall time/cost efficiency. Most tasks achieve near-SOTA performance within hours at modest cost,
demonstrating the practicality of the ADRS approach.1
Strategy
Telemetry
Cloudcast ↓
EPLB
Prism
LLM-SQL
TXN
CBL ↓CBL-Multi ↓
MAS
NS3
Human SOTA
0.8222
626.24
0.127
21.892
0.6920
2724.8
101.68
92.332
0.380
68.97
GPT-5
OE
0.930 ± 0.04 926.9 ± 170.7 0.127 ± 0.00 26.23 ± 0.00 0.710 ± 0.01
4238.6 ± 89.8
112.7 ± 4.2
81.47 ± 0.05 0.333 ± 0.15
92.24 ± 4.76
GEPA
0.916 ± 0.05 689.9 ± 73.50 0.134 ± 0.01 26.19 ± 0.07 0.713 ± 0.00 3752.5 ± 204.4 99.13 ± 1.49
79.51 ± 1.50 0.290 ± 0.00
68.97 ± 0.00
Shinka
0.923 ± 0.04 954.8 ± 124.6 0.118 ± 0.01 26.26 ± 0.00 0.712 ± 0.00 4090.0 ± 337.7
107.2 ± 1.7
79.84 ± 0.65 0.271 ± 0.11 89.50 ± 18.73
Gemini-3
OE
0.954 ± 0.01
707.8 ± 40.1 0.127 ± 0.00 26.24 ± 0.01 0.729 ± 0.01 4109.2 ± 253.9
107.4 ± 2.7
80.77 ± 0.71 0.188 ± 0.02
115.2 ± 13.2
GEPA
0.850 ± 0.00
720.4 ± 46.2 0.127 ± 0.00 26.16 ± 0.03 0.713 ± 0.00 3615.6 ± 481.2 95.69 ± 0.50
81.45 ± 0.31 0.206 ± 0.01
74.47 ± 9.53
Shinka
0.918 ± 0.03
949.8 ± 73.4 0.120 ± 0.01 26.25 ± 0.01 0.721 ± 0.00 3931.7 ± 343.4
105.0 ± 5.0
80.26 ± 1.07 0.243 ± 0.02
84.72 ± 7.70
Table 3: Comparison of OpenEvolve (OE), GEPA, and Shinka across GPT-5 and Gemini-3. We report mean ± standard deviation
over three runs for each framework on downstream benchmarks. Higher the scores the better except for cases with ↓label.
• CBL-Multi - Optimizing Spot Instance Savings under Dead-
lines for Multi-Region: An extension of the single-region
spot instance scheduling problem to multiple regions,
where the policy must also choose migration timing and
region placement. ADRS achieves 17% improvements over
a strong baseline in a multi-region setting.
• EPLB - Optimizing Expert Placement in MoE Inference:
The solution seeks to balance the load across GPUs by map-
ping the expert replicas across GPUs. ADRS provides a five-
fold improvement in the time it takes to rebalance experts
compared with the best-known proprietary implementation.
• LLM-SQL - Optimizing LLM Inference for SQL Queries:
The solution to this problem reorders rows and columns
in a table to maximize the hit rate in a KV cache when
performing LLM inference. ADRS achieves a similar
hit rate to SOTA, while reducing the running time of the
reordering algorithm by 3×.
• TXN - Optimizing Transaction Scheduling: The solution
aims to reorder transactions to minimize conflicts and
hence improve the makespan and throughput. ADRS
“rediscovers” the SOTA solution for the online case and
improves a strong baseline by 34% for the offline case, for
which we are not aware of any published solution.
1Reported “time” reflects the automated solution-iteration phase
(model–evaluator loop). It excludes researcher effort related to problem
specification (e.g., refining prompts, evaluators, or experimental setup),
which typically ranges from a few hours to a few days—still orders of


--- Page 6 ---
4.1
Case Study #1: Can’t Be Late (CBL)
[NSDI ‘24]
Our first case study focuses on reducing the cost of deadline-
driven jobs by exploiting cheaper but unreliable spot instances
in the cloud. This problem was studied in an NSDI ‘24 out-
standing paper [80], which introduced the current state-of-the-
art policy. We report the performance of the best framework
(GEPA) and show how it discovers an algorithm that improves
the cost savings compared to the human SOTA by an average
of 6%, with per-workload improvements of up to 35%.
Problem setup. The task is to minimize the cost of running
deadline-aware jobs on a single node by using spot instances
in one cloud region, while ensuring jobs still meets their dead-
lines. Spot instances are typically 60% to 90% cheaper than
on-demand instances, but they may not always be available
and can be preempted at any time. Each preemption incurs
a changeover delay due to the setup time on a new instance.
Objective and constraints. We evaluate the average cost
savings across a range of workload traces. We require that
all deadlines be met for an algorithm to be considered valid.
Initial program and baselines. The initial program is
the greedy policy from the original paper [80], which uses
spot instances until preemption risks missing a deadline. We
compare against the Uniform Progress algorithm, the paper’s
state-of-the-art solution, which tracks expected progress and
switches between spot and on-demand instances based on
whether it is ahead of or behind schedule.
Solution generator and selector. We run GEPA with
Gemini-3.0 for 100 iterations. The search completes in about
five hours and costs less than $30.
Evaluator. We use the simulator from the NSDI paper
and use configurations covering different job fractions,
changeover delays, regions, and accelerator types. For each
configuration, we sample 30% of the traces as a feedback sub-
set used during GEPA search to reduce overfitting to specific
traces. We report final results on the full evaluation set.
The evaluator checks syntax and interface compliance
and tests valid solutions on sampled traces. It reports
average cost savings over the Uniform Progress baseline and
per-configuration statistics (mean, deviation, count). Trace
features, such as availability and average spot duration, are
also included to provide richer context.
GEPA results. The best policy achieves 6% higher average
cost savings than Uniform Progress (and 20% over the greedy
policy) while meeting all deadlines. Per-trace improvements
reach up to 35% compared to Uniform Progress.
As shown in Figure 2b, the evolved policy fundamentally
differs from Uniform Progress shown in Figure 2a. While
Uniform Progress follows a fixed formula to maintain steady
magnitude less than manually developing a solution. Because this effort is
difficult to measure consistently, we do not report it here.
Figure 2: Side-by-side comparison of the initial Uniform
Progress policy and the evolved adaptive strategy. Key
innovations in the evolved policy are highlighted.
1
def UniformProgress(has_spot, state, env, task):
2
# Uniform progress assumption
3
rate = task.duration / task.deadline
4
expected = env.elapsed_time * rate
5
actual = task.progress_made
6
# Behind schedule: must run now
7
if actual < expected:
8
return SPOT if has_spot else ON_DEMAND
9
# Simple hysteresis for on-demand
10
if state == ON_DEMAND:
11
buffer = expected + 2 * task.overhead
12
if actual < buffer:
13
return ON_DEMAND
14
# Default preference
15
return SPOT if has_spot else NONE
(a) Before: Uniform Progress Policy
1
def AdaptiveStrategy(has_spot, state, env, task):
2
# Track spot availability history
3
self.history.append(has_spot)
4
avail = avg(self.history)
5
trend = recent_avg(self.history) - avail
6
# Stability via run-length statistics
7
runs = extract_runs(self.history)
8
stable = percentile(runs, 25)
9
recover = percentile(runs, 50)
10
cur_run = trailing_run(self.history)
11
# Slack = remaining time - required time
12
slack = time_remaining(env) - time_needed(task)
13
# No slack: force ON_DEMAND
14
if slack <= 0:
15
return ON_DEMAND
16
# Seal ON_DEMAND under tight slack
17
if slack <= lock_margin(stable):
18
self.sealed = True
19
if self.sealed:
20
if has_spot and cur_run >= recover:
21
if slack > unlock_margin(recover):
22
self.sealed = False
23
if self.sealed:
24
return ON_DEMAND
25
# Anti-flapping dwell
26
if state == ON_DEMAND and self.od_dwell < min_dwell():
27
return ON_DEMAND
28
# Buffer zone
29
if slack <= retry_buffer():
30
return SPOT if has_spot else ON_DEMAND
31
# Safe zone with trend-aware switching
32
if has_spot:
33
if state == ON_DEMAND:
34
thresh = switch_threshold(avail, trend)
35
return SPOT if max(runs) >= thresh else ON_DEMAND
36
return SPOT
37
if state == ON_DEMAND:
38
return ON_DEMAND
39
# Adaptive waiting
40
wait = slack * wait_factor(avail, trend)
41
return NONE if self.wait_time < wait else ON_DEMAND
(b) After: Evolved Adaptive Policy
progress, the evolved policy tracks spot availability patterns
and detects trends in availability over time (lines 3-6). It
estimates spot stability using percentiles of historical run
lengths, making the thresholds robust to outliers (lines
7-10). When the deadline approaches, the policy switches
to on-demand instances and remains locked in that mode
until spot availability is consistently stable (lines 16–24).
To avoid repeatedly switching between instance types, the


--- Page 7 ---
policy waits a minimum period after moving to on-demand
before considering a switch back (lines 25–27). When there
is sufficient time before the deadline, the policy factors in
availability trends, returning to spot instances more readily
when availability is high and improving (lines 32–36).
The main limitation of Uniform Progress is its inflexibility:
it must use every available spot instance regardless of how
short-lived it might be, leading to frequent switches with little
progress. The evolved policy avoids this through adaptive
waiting (lines 39–41): it waits longer when spot availability
is high and improving, but switches to on-demand proactively
when availability drops or begins declining.
Evolution process. The search explores the policy space
through iterative refinement. Early candidates learn to track
spot availability using a sliding window and compute the
remaining time buffer. Subsequent candidates introduce a
locking mechanism that switches to on-demand when the
deadline approaches and only returns to spot instances after
observing consistent availability. Later candidates refine
stability estimation by switching from maximum run length
to percentile-based thresholds, making the policy more robust
to outliers. The final policy adds trend detection and adaptive
waiting: it adjusts switching thresholds and wait times based
on whether spot availability is improving or declining.
4.2
Case Study #2:
Multi-Region Can’t Be Late
In this section, we show that OpenEvolve develops an algo-
rithm on an expanded multi-region setting, where no prior pol-
icy has been published. In this setting, OpenEvolve discovers
an algorithm that outperforms a hand-tuned baseline by 17%.
Problem setup. The original Uniform Progress assumes
one region with uniform spot prices. In practice, spot prices
and availability differ across regions ([49]), so a policy must
decide when to switch spot and on-demand, which region
to use, and when to migrate jobs. We use OpenEvolve to
explore this multi-region space and derive an better policy.
Objective and constraints. We evaluate the total cost in
a multi-region setup, accounting for spot and on-demand
instances and migration costs. A policy is valid only if all
job deadlines are met.
Initial program and baselines. As no baseline exists,
we adopt a Uniform Progress variant that first assigns spot
instances locally and, if none are available, move to other
regions in a round-robin manner.
Solution generator and selector. We run OpenEvolve for
100 iterations with GPT-5, using the same island setup as
before. The evaluator then reports both overall and per-trace
scores, similar to Section 4.1.
Evaluator. For the multi-region setting, we extend the
Uniform Progress simulator and evaluate cost savings on 106
traces.
OpenEvolve results. The final policy achieves 17% cost
savings, on average, compared to the multi-region Uniform
Progress baseline. It balances cost efficiency with deadline
guarantees using a simple principle: when a job is not urgent
(i.e., not at risk of missing its deadline), it explores additional
regions to seek lower-cost spot capacity; if a job is urgent, it
prioritizes immediate progress, selecting spot instances when
available or falling back to on-demand. This adaptive logic en-
ables opportunistic exploration under slack conditions while
ensuring reliability when deadlines are at risk, effectively
managing the trade-off between exploration and guaranteed
progress in a multi-region environment. In addition, the
policy leverages a dynamic view of regional capacity to
opportunistically migrate when conditions are favorable.
Evolution process. The search process demonstrates iter-
ative improvement of the deadline monitoring mechanisms
from multi-region scheduling policies. Initial strategies imple-
ment basic progress tracking by comparing task completion
against elapsed time. The system discovers key insights
through failure analysis. At iteration 7, the system introduces
region caching and urgency calculation. Iteration 5-12 at-
tempts with aggressive cost reduction initially show promise,
but ultimately fail when accumulated delays cannot be re-
covered within deadline constraints. These failures guide the
search toward more balanced approaches. The final evolved
strategy at iteration 63 implements a two-stage urgency
detection system. Rather than applying uniform resource allo-
cation rules, it combines schedule-based progress monitoring
with direct deadline pressure analysis. This design enables
adaptive behavior: immediate allocation of on-demand
instances when deadlines are at risk, while maintaining
intelligent region exploration when deadline permits. The
insight is the separation of deadline assessment from resource
provisioning decisions, enabling adaptive region selection.
4.3
Case Study #3: Expert Placement in MoE
Inference (EPLB)
In this section, we use ADRS framworks to design algo-
rithms to balance inference load across multiple GPUs in
Mixture-of-Experts (MoE) architectures. In our evaluation,
ShinkaEvolve discovers an implementation that is 13.0×
faster than a proprietary, frontier-lab baseline while achieving
a similar load-balance factor.
Problem setup. The basic Expert Parallelism Load
Balancing (EPLB) algorithm operates in three stages: (i)
distributing expert groups across nodes to balance the load,
(ii) creating replicas for hot (popular) experts, and (iii)
assigning these replicas to GPUs to maximize load balance.
The problem is to determine the replica count per expert and
map expert replicas to GPUs to maximize load balance for
a given query workload, MoE model, and GPU set.
Objective and constraints. Our goal is twofold: maximize
load balance (i.e., the ratio of average to maximum tokens


--- Page 8 ---
Figure 3: Side-by-side comparison of the initial greedy policy
and final evolved heuristic. Key innovations in the evolved
policy are highlighted.
1
def InitialStrategy(...):
2
...
3
for item in sorted(items, reverse=True):
4
# Greedily choose least-loaded pack
5
# Plain iterative assignment
6
available = filter_nonfull(packs)
7
target = min(available)
8
target.add(item)
9
...
(a) Before: Initial Program
1
def EvolvedStrategy(...):
2
...
3
# Items are pre-sorted by weight
4
idx = arange(num_items)
5
6
block
= idx // num_packs
7
offset = idx % num_packs
8
9
# Vectorized snake assignment
10
# even blocks: 0 -> P-1
11
# odd blocks:
P-1 -> 0
12
pack_id = where(
13
block % 2 == 0,
14
offset,
15
num_packs - 1 - offset
16
)
17
18
assign(items, pack_id)
19
...
(b) After: Evolved Policy
generated per GPU) and minimize the runtime of the
algorithm when the load distribution shifts.
Initial program and baselines. The initial program is
the open-source implementation from DeepSeek [6], which
performs expert placement using a greedy bin-packing.
Specifically, it sorts experts by load and assigns them
iteratively to the least-loaded feasible GPU. While simple,
its Python-based linear search using a for-loop is slow,
averaging 693 ms for a 0.66 balance factor. We also include
a non-public reference implementation from a frontier lab
as a baseline. This implementation avoids explicit iteration
and reduces the runtime to 19.5 ms while achieving the same
balance factor as the open-source algorithm.
Solution generator and selector. We employ the same
setup used for all use cases (Section 4). For each framework,
evolution takes roughly five hours and costs under $15.
Evaluator. Our simulator models a distributed GPU infer-
ence engine for MoE models. The simulator is implemented in
168 lines of PyTorch code. Our evaluation trace models load
changes over the ShareGPT and GSM8K datasets [15, 18].
The evaluator’s output metrics are: (a) the balance factor
and (b) the time it takes to rearrange the expert replicas
during the load changes. We compute the combined score
as the equally weighted average of the load balance factor
and a normalized lower-is-better runtime term.
ShinkaEvolve results. The evolved algorithm replaces
the linear Python for-loop with vectorized tensor operations:
instead of explicit greedy bin packing, it constructs a tensor
of expert indices and assigns experts via a zigzag (“snake”)
pattern across GPUs. This assignment alternates direction
across contiguous blocks of experts, interleaving higher- and
lower-load experts across GPU slots. The resulting algorithm
matches the load balance factor of the other baselines while
reducing runtime to just 1.51 ms, yielding a 13.0× speedup
over the proprietary implementation.
Evolution process. ShinkaEvolve’s evolution trajectory
can be characterized by two major steps in improving
runtime: first, replacing Python for-loops with PyTorch tensor
operations, and second, discovering the zigzag placement
pattern (Figure 3b). Interestingly, the initial introduction
of the zigzag pattern did not yield immediate gains—the
balance factor sometimes worsened, and rearrangement
costs fluctuated. The breakthrough came later, when the
evolution process learned to systematically reuse the zigzag
partitioning heuristic across multiple stages of EPLB, rather
than only in the initial group distribution. At this point, both
runtime and stability improved substantially in our trace.”
The expert replication stage, by contrast, remained the most
unstable throughout evolution. The system oscillated between
strategies such as copying the least-used experts, overloading
popular ones, or attempting proportional spreads. These
experiments rarely improved the score, and ultimately the
intuitive rule of replicating only overloaded experts prevailed.
Consequently, many iterations were unproductive, with the
main speed improvements coming from global reorganization
logic that exploited PyTorch’s batched operations and the
zigzag layout.
In summary, ShinkaEvolve rediscovered and exploited a
tensorized zigzag partitioning scheme, yielding an evolved
EPLB algorithm that achieves a 13.0× speedup while
maintaining the same balance factor.
4.4
Case Study #4: LLM Inference on SQL
Queries (LLM-SQL) [MLSys ‘25]
This research problem [43] arises in relational analytics,
where SQL queries invoke LLMs over entire tables, with each
row triggering a separate LLM inference operation. At scale,
this is prohibitively expensive. The state-of-the-art solution
mitigates cost by reordering rows and fields to maximize
prefix KV cache reuse. Using OpenEvolve, we evolve such a
reordering policy, achieving similar hit rates while delivering
a 3× runtime speedup.
Problem setup. To minimize inference time and cost, we
aim to maximize the prefix cache hit rate (PHR) by reordering
both rows and fields in the table before performing inference.
This problem is combinatorial: for a table with n rows and m
fields, there are n!×(m!n) possible orderings, making a naive


--- Page 9 ---
Figure 4: Side-by-side comparison of the greedy recursive
grouping (GGR) and the evolved prefix-aware reordering pol-
icy. Key innovations in the evolved algorithm are highlighted.
1
def GGR(df):
2
# 1. Compute value counts for all cells
3
counts = Counter(df.stack())
4
val_len = {v: len(str(v))**2 for v in counts}
5
6
# 2. Pick value maximizing len^2 * (count-1)
7
v_star = argmax_v [ val_len[v] * (counts[v]-1) ]
8
if v_star is None: return fixed_reorder(df)
9
10
# 3. Split rows with/without v_star
11
G = rows with v_star
12
R = rows without v_star
13
14
# 4. Reorder columns in G (v_star front, deps after)
15
for row in G:
16
cols_with_val = [c for c in df.columns if row[c]==v_star]
17
reordered = cols_with_val + (others)
18
row = row[reordered]
19
20
# 5. Recurse on G remainder and R
21
G = QuickGreedy(G remainder)
22
R = QuickGreedy(R)
23
return concat(G,R)
(a) GGR baseline.
1
def EvolvedPolicy(df):
2
# 1. Precompute value statistics ONCE
3
counts
= Counter(df.values.ravel())
4
val_len = {v: len(str(v))**2 for v in counts}
5
6
BASE = 5000
# size threshold to cap recursion depth
7
8
# 2. Select best grouping value (GGR score)
9
score = lambda v: val_len[v] * (counts[v] - 1)
10
v_star = max(counts, key=score, default=None)
11
if v_star is None or counts[v_star] <= 1:
12
return FixedReorder(df)
13
14
# 3. Split rows by whether they contain v_star
15
grouped
= df[df.eq(v_star).any(axis=1)]
16
remaining = df[~df.eq(v_star).any(axis=1)]
17
18
# 4. Column recursion: pull v_star forward (stable)
19
grouped = StableColumnReorder(grouped, v_star)
20
21
# 5. Recursive descent with early stopping
22
if len(df) > BASE:
23
top, bot = split(df)
24
return concat(EvolvedPolicy(top),
25
EvolvedPolicy(bot))
26
27
# 6. Recurse on remainder and concatenate
28
if not remaining.empty:
29
return concat(
30
grouped,
31
EvolvedPolicy(remaining)
32
)
33
34
return grouped
(b) Evolved prefix-aware policy.
brute-force search infeasible. Thus, the goal is to design a
reordering algorithm that achieves high PHR while keeping
its runtime small relative to the overall inference time.
Objective and constraints. Our objective is to maximize
the prefix hit rate (PHR) while keeping the runtime of the
reordering algorithm low. Since PHR measures the fraction
of token prefixes shared across consecutive rows, and serves
as a proxy for inference cost and latency.
Initial program and baselines. As initial program, we
use the greedy recursive group algorithm (GGR) [43], a
heuristic that recursively groups rows by common field
values and reorders fields using schema statistics with early
stopping. This approach approximates the optimal reordering
algorithm while running more efficiently. For comparison,
we also include a simple baseline: the table in its original
ordering, i.e., the default row/field order of the input table.
This program is implemented in Pandas [52], an open-source
Python library for data manipulation and analysis.
Solution generator and selector. We configure OpenEvolve
with three islands, and we use an LLM ensemble of 80%
OpenAI o3 and 20% Gemini 2.5 Pro, and we run it for 100
iterations. The entire evolution takes about one hour and
costs less than $7.
Evaluator. We leverage the publicly available simulator
from the paper that measures prefix cache hit rate (PHR) given
dataset table. The simulator is written in Python (200 LOC)
and evaluates a benchmark of representative LLM queries
across five recommendation datasets (e.g., movies [63],
beer [51], BIRD [39], PDMX [45], products [28]).
The evaluator reports four key metrics. First, it reports a
combine score = 0.95×PHR+0.05×
1
1+runtime, defined as
the equally weighted average of PHR and algorithm runtime
across datasets, where higher PHR and lower runtime yield
a higher score. To preserve query semantics, the reordering
algorithm must not alter which rows or fields are included,
only their order. Second, the evaluator records a binary
flag indicating whether the candidate program executes
end-to-end. Third, it reports the detailed prefix hit rate
for each dataset. Finally, it measures the total runtime of
the policy. The combined score serves as the optimization
objective during evolution.
OpenEvolve
results.
The
program
synthesized by
OpenEvolve attains a comparable average PHR to the GGR
algorithm while reducing end-to-end runtime by approxi-
mately 3×, resulting in a higher overall combined score. The
greedy baseline incurs substantial overhead due to repeated
value-count recomputation and deep recursive traversal
over the table. In contrast, the evolved implementation
incorporates several structural optimizations. First, it avoids
redundant scans by maintaining a cached global value-
frequency map and lazily updating value-length statistics,
eliminating repeated full-table passes. Second, it replaces ex-
pensive Pandas-based indexing and grouping operations with
direct NumPy and Python-level array manipulations, reducing
the dominant computation to simple O(Nrows×Ncols) loops.
Finally, rather than globally re-sorting the table at each step,
the algorithm applies a localized, prefix-aware reordering
heuristic that prioritizes continuity with the previous row
while weighting values by squared string length. These
optimizations, illustrated in Figure 4, jointly explain the
substantial runtime improvements without sacrificing PHR.


--- Page 10 ---
Evolution process. The search begins with the published
GGR algorithm, which achieves a good Prefix Hit Rate
(PHR) but suffers from repeated counter operations and deep
recursion. Early in the search, by iteration 32, OpenEvolve
discovers a faster heuristic that orders columns by (frequency
× squared length) instead of using recursive splitting. This
change eliminates redundant value counting and improves
runtime, though at the cost of some grouping accuracy.
Later, by iteration 72, the heuristic is refined. It now uses
normalized weights (frequency ratio × squared length) and
limits multi-key sorting to only the most informative columns,
which improves hit rates while maintaining efficiency.
By iteration 97, the final program strikes an optimal
balance between speed and accuracy. It raises the recursion
base threshold, reuses cached counts and string lengths,
reintroduces selective recursion for rows, and incorporates
a NumPy-based reordering to replace costly pandas lookups.
Overall, the evolution progresses from early runtime
improvements to mid-stage refinements that recover accuracy,
culminating in a final design that integrates both for the best
overall score.
4.5
Case Study #5: Transaction Scheduling
(TXN) [VLDB ‘24]
This research problem [13] aims to find efficient schedules
to reduce conflicts for transactional workloads. Optimizing
execution order of transactions can significantly improve
throughput by minimizing overall execution time. We apply
OpenEvolve and find it is unable to find a solution that
outperforms the state-of-the-art policy under the original
online setting constraints. However, OpenEvolve discovers
a superior scheduling algorithm in the offline setting,
demonstrating the utility of ADRS frameworks for rapidly
exploring problem variations.
Problem setup. Conflicts on shared data cause performance
bottlenecks in many transactional workloads [12]. One
approach to minimize conflicts is to carefully schedule the
transactions. The problem we aim to solve is: given a set of
transactions, find a schedule that minimizes the conflicts and
improves the throughput.
Objective and constraints. Maximizing throughput in this
setting is equivalent to minimizing the schedule makespan,
i.e., the total time to execute all transactions. We consider
both the online and the offline settings. In the online setting,
we assume that the transaction order is fixed once the
schedule is determined (i.e., committed transactions cannot
be rollbacked). We constrain the scheduling algorithm to
O(n) runtime (where n is the number of transactions to be
scheduled) and assume the read/write operations are not
known apriori (only hot keys can be predicted). We also
consider the offline scheduling problem, which is relevant
Figure 5: Side-by-side comparison of the evolved constant-
time greedy policy (SMF) and the offline evolved policy. Vari-
able names are abbreviated for brevity without losing logic.
1
# --- 1. Candidate sampling --- #
2
K = 8
3
rng = np.random.default_rng()
4
while rem:
5
# --- 2. Draw candidates --- #
6
# Handle tail end of queue
7
if len(rem) <= K: cands = rem
8
else: cands = rng.choice(rem, K, replace=False)
9
# --- 3. Evaluate incremental cost --- #
10
best_cand = None
11
best_add = math.inf
12
best_times = (0, 0)
13
for cand in cands:
14
delta, t0, t1 = calc_cost(cand, state)
15
if delta < best_add:
16
best_add = delta
17
best_cand = cand
18
best_times = (t0, t1)
19
# --- 4. Commit update global state --- #
20
schedule.append(best_cand)
21
rem.remove(best_cand)
22
# Update locks/occupancy maps
23
update_state(best_cand, best_times)
24
total_cost += best_add
25
return total_cost, schedule
(a) SMF / greedy policy.
1
def get_best_schedule(workload, n_seqs):
2
n = workload.num_txns
3
# --- 1. Cost Cache --- #
4
memo = {}
5
def get_cost(s):
6
return memo.setdefault(tuple(s), workload.eval(s))
7
8
# --- 2. Pairwise Scoring (O(N^2)) --- #
9
score = [0] * n
10
for i in range(n):
11
for j in range(i + 1, n):
12
d = get_cost([j, i]) - get_cost([i, j])
13
score[i] += d; score[j] -= d
14
best_c, best_s = float("inf"), None
15
16
# --- 3. Multi-Start Strategy --- #
17
for k in range(max(6, n_seqs)):
18
mode = k % 3
19
20
if mode == 0:
# A. Greedy w/ Random Sampling
21
seq = [random.choice(range(n))]
22
rem = list(set(range(n)) - set(seq))
23
while rem:
24
batch = random.sample(rem, min(len(rem), 12))
25
# ... (evaluate best insertion pos) ...
26
seq.insert(best_pos, best_cand)
27
elif mode == 1: # B. Beam Search
28
seq = beam_search_helper(n, get_cost)
29
else:
# C. Borda Count Sort
30
seq = sorted(range(n), key=lambda x: -score[x])
31
32
# --- 4. Deterministic Polish --- #
33
improved = True
34
35
while improved:
36
# ... (scan swaps/moves, update seq) ...
37
improved = try_local_moves(seq)
38
# --- 5. Final Refinement --- #
39
# Perturb + final deterministic polish
40
seq = escape_local_optima(seq, get_cost)
41
42
if (c := get_cost(seq)) < best_c:
43
best_c, best_s = c, seq
44
45
return best_c, best_s
(b) Offline evolved policy.


--- Page 11 ---
Problem
Technique
Domain Origin
How it helps?
EPLB
(GEPA, GPT 5)
Hamilton’s Apportionment
Political Science
(US Congress History)
Fast-Path Allocation: Provides a baseline for assigning integer replica
counts using the “Largest Remainder Method.” It is used for uniform
workloads to avoid expensive search iterations.
Transaction Scheduling
(OpenEvolve, GPT-5)
Condorcet / Borda Count
Economics
(Social Choice Theory)
Preference Aggregation: Constructs a matrix where every transaction
“votes” on its precedence. The score is the aggregated cost savings of
essentially running an election to determine the “consensus” order.
Telemetry Repair
(OpenEvolve, Gemini-3)
Kirchhoff’s Current Law
(KCL)
Electrical Engineering
(Fluid Dynamics)
Constraint Satisfaction: Enforces the physical law of “flow conser-
vation” to validate noisy telemetry counters. Deviations indicate data
errors.
Can’t Be Late Multi-Region
(GEPA, GPT-5)
Multi-Armed Bandit with UCB-
style Scoring
Artificial Intelligence
(Reinforcement Learning)
Explore-Exploit Region Selection: Maintains per-region success/fail-
ure counters (succ, fail) to compute a “success rate” score. Unknown
regions are prioritized for exploration, while high-success regions are
exploited. This mirrors the Upper Confidence Bound (UCB) algorithm
for balancing exploration vs. exploitation in slot machine selection.
Datacenter TCP Congestion
Control
(OpenEvolve, Gemini-3)
Gradient-based Power Control
Physics / Electrical Engineering
(Electrical Circuits & Power)
Congestion Anticipation: Adapts the physical concept of “Power”
(P = V · I) to networking, treating throughput as Current and delay
as Voltage. It computes the RTT gradient ( dRTT
dt
) to measure the
“velocity” of queue growth, allowing the system to react to the energy
of congestion bursts before queues overflow.
Table 4: Cross-domain techniques discovered by ADRS frameworks on different problems. The “Problem” column denotes
the framework and model that generated the best-performing program for the given systems problem. The table is not exhaustive
and presents only one representative technique per problem.
to deterministic databases [66, 77] that schedule batches of
transactions, a setting with no previously known results.
Solution generator and selector. We configure OpenEvolve
to run with three islands and use OpenAI’s GPT-5 for both
problem settings. We run for 100 iterations, which takes less
than two hours and costs less than $20.
Initial program and baselines. We use a random scheduler
for the initial program. We compare against a number of
transactional scheduling algorithms, including the SOTA
algorithm, Shortest Makespan First (SMF), which greedily
chooses transactions to schedule.
Evaluator. We use the Python simulator from the SMF
paper [13], which assumes that each operation takes one unit
of time. The simulator calculates the makespan of a given
transaction schedule and also provides statistical bounds
on the makespan of the schedule for a given workload. We
measure total makespan over five traces from the OLTP
benchmarks used in the original paper (Epinions, SmallBank,
TPC-C, TAOBench, YCSB) with 500 transactions each. We
use the random-scheme baseline as the initial program.
OpenEvolve results. In the online setting, OpenEvolve’s
best policy matches SMF. OpenEvolve rediscovered this
algorithm from a random baseline, likely due to training data
contamination from the SMF paper. While this result is not as
interesting, this confirms ADRS frameworks can reproduce
state-of-the-art solutions. Surpassing SMF is difficult because
transaction scheduling involves complex operation groups,
dependencies, and correctness constraints (e.g., serializabil-
ity). Prior work showed that complex heuristics (hill climbing,
simulated annealing) did not perform better due to these con-
straints [13]. Instead, SMF’s strategy of leveraging conflict
costs provides a generalizable solution across workloads.
At a high level, SMF minimizes contention by separating
transactions with high conflict costs. The incremental
makespan increase of adding a transaction to the schedule
accounts for all potential conflicts with the current ordering.
Specifically, SMF starts with a random transaction. At each
iteration (Figure 5a, lines 5), it selects the transaction that
increases makespan the least among k sampled unscheduled
requests (lines 8-22) and appends it to the schedule (lines
25-30). Ties are broken randomly. The algorithm runs in
linear time O(n×k), where n is the transaction count and k
is the sample size.
In the offline setting, OpenEvolve discovers a novel
algorithm that reduces makespan by 60% compared to SMF.
This result reduces concerns about contamination since
it is not a previously known solution. The evolved policy
employs a multi-stage strategy. First, it computes pairwise
conflict costs to assign a score to each transaction (Figure
11, lines 9-14). Second, it iterates through a multi-start loop,
generating candidates via randomized greedy insertion, beam
search, or score-based sorting (lines 18–32). Each candidate
undergoes deterministic polishing to resolve local minima
(lines 36–38) and a final perturbation step to escape optima
(line 42). This algorithm extends the greedy intuition of SMF
while maintaining O(n2) runtime. This result indicates that
scheduling based on conflict costs is the effective approach
for reducing makespan. Furthermore, OpenEvolve rapidly
adapts the algorithm for altered constraints, showing it can
assist researchers in customizing solutions for different
settings (which typically require manual re-design).
Evolution Process In the online setting, OpenEvolve’s
search shifts from random schedules and length-only
heuristics toward conflict-aware solutions. Recognizing


--- Page 12 ---
Start with 
Diverse 
Seeds
Choose the 
Right  
Abstraction
Iterative 
Hint 
Refinement
Granular 
Feedback
Precise 
Scoring 
Function
Prevent 
Reward 
Hacking
Learn From 
Failures
Diverse 
Tests and 
Safeguards
Leverage 
Meta-
Feedback
Three-Part 
Prompt
Design
LLM
GPT-5
Gemini-3
Multi-technique?
Few techinques?
Aggregate?
1
2
3
Choose Generator
Design Evaluator &
Define the Specs
Determine Form of Feedback
Per-result?
GEPA
OpenEvolve/
ShinkaEvolve
Non-Deter. 
Evaluation
No?
Yes?
OpenEvolve
GEPA/
ShinkaEvolve
Figure 6: Workflow diagram that illustrates how these insights integrate into the end-to-end ADRS process.2
ADRS Axis
Actionable Insights
Specifications: "Less is More and More is Less"
• Three-part prompt with problem, evaluation criteria, and context.
• Choose the right abstraction to match the optimization goal.
• Start evolution with diverse, domain-specialized seeds.
• Iteratively refine hints to balance explore-exploit.
Evaluation: "Solution is Only as Good as Evaluator"
• Enforce diverse test sets and safeguards.
• Prevent reward hacking by restricting edits.
• Use precise scoring function that is smooth and deterministic.
• Select LLM based on solution composition.
Feedback: "The Devil is in the Details"
• Calibrate feedback granularity to provide actionable guidance.
• Learn from failures to gain insights for evolution.
• Meta-feedback helps guide search when accurate.
• Encode resilience for non-deterministic evaluators.
Table 5: Summary of actionable insights for ADRS, covering specification, evaluation, and feedback.
that contention increases makespan, candidates explore
write-count bucketing and greedy contention minimization.
Common pitfalls included over-reliance on transaction length
(which correlates poorly with contention) and expensive
greedy selection that violated the O(n) constraint. Early
candidates often converge prematurely on single heuristics
(e.g., length-first, write-first). The final policy generalizes
these insights, incorporating conflict costs to match SMF.
For the offline setting, the evolution also shifts from random
sampling to structured construction heuristics (e.g., beam
search, pairwise scoring). The search eventually centers in
two directions: (i) multi-start construction strategies that build
schedules incrementally and (ii) guided local search that re-
fines them. The search often finds that standard hill climb-
ing plateaued quickly in local minima. To overcome this,
the framework discovers the importance of pairwise prece-
dence by analyzing whether executing transaction i before j is
cheaper than the reverse. The best solution combines these in-
sights: it employs a mix of initialization strategies (greedy in-
sertion, beam search, and Borda sorting based on precedence
scores) followed by a polishing phase that uses pairwise-
guided swaps and block moves to escape local optima.
4.6
Cross-Domain Analysis
One of the most compelling capabilities of ADRS is its ability
to generate solutions by drawing on techniques from different
domains. While systems researchers typically specialize in a
particular subfield (e.g., networking or databases), LLMs are
trained on vast, diverse corpora that span the entire spectrum
of human knowledge [10, 16]. This breadth allows ADRS
to identify structural parallels between systems performance
problems and those in other fields, revealing connections that
may be overlooked by a human expert focused on a single
domain. As hypothesized in Section 3.2, this cross-domain
reasoning enables ADRS to import techniques from fields
like political science, economics, and physics to solve modern
systems challenges efficiently. Table 4 shows representative
cross-domain techniques discovered by ADRS frameworks
across our case studies. For each problem, the table reports
one example technique, its domain of origin, and how it is
used in the generated solution. The listed techniques originate
from areas such as political apportionment, voting systems,
electrical engineering, combinatorial optimization, real-time
scheduling, reinforcement learning, and congestion-control
modeling. The table is not exhaustive for space and provides
a single example technique per problem.


--- Page 13 ---
In EPLB (Section 4.3), the core challenge is to assign
integer replica counts to experts to balance load. This is a
structurally identical problem to allocating congressional
seats to states based on the voting population. GEPA,
using GPT-5, “rediscovered” Hamilton’s Apportionment
method (also known as the Largest Remainder Method) from
political science. By applying this 18th-century algorithm,
the system implemented a fast-path allocation strategy for
uniform workloads that avoided expensive search iterations,
contributing to the 13× speedup over the baseline.
Similarly, in TXN (Section 4.5), OpenEvolve leverages
concepts from social choice theory and economics to
minimize conflicts. To determine the optimal execution order,
the generated solution constructs a precedence matrix. where
transactions effectively “vote” on their preferred ordering
based on conflict costs, a technique mirroring the Condorcet or
Borda Count methods used in voting systems. This approach
allowed the scheduler to determine an order that minimizes
global makespan, effectively translating a preference aggre-
gation problem into a high-performance database schedule.
5
Best Practices
Having demonstrated the potential of ADRS to surpass
state-of-the-art solutions, we now focus on the most effective
strategies for applying these frameworks to algorithm
discovery. Drawing from extensive ablation studies across
the evolutionary process, we consolidate our findings into a
set of actionable best practices. We categorize these strategies
across three critical axes, specifications, evaluation, and
feedback and summarize them in Table 5. Additionally, we
provide a workflow guide in Figure 6 for applying ADRS
frameworks to new research problems.
5.1
Specs: Less is More and More is Less
Rigorously defined specifications are a prerequisite for
effective search. We find that ambiguity in prompts is a
primary source of execution and algorithmic failures (e.g.,
missing API details). Effective prompts should be highly
specific and structured, clearly defining three key areas:
• The problem: the core problem definition.
• The evaluation criteria: optimization goals and correctness
constraints.
• The context: essential background, such as required APIs.
We recommend instantiating a concise, three-section
template for each problem and polishing the draft with
external LLMs (e.g., ChatGPT and Gemini) prior to evolution.
In our experience, this has been crucial to reducing invalid
generations and preventing evaluator hacking.
[Prompt Generator] High-level or Low-level abstrac-
tions? Selecting the appropriate level of abstraction for
evolution is crucial to balancing implementation convenience
with genuine problem-solving.
Unrestricted access to
high-level external library APIs can lead to suboptimal
optimizations. With EPLB, replacing custom operators
with PyTorch primitives yielded only trivial speedups. To
encourage algorithmic advances, we can restrict API access
to help the ADRS framework explore new strategies rather
than relying on pre-built solutions. Conversely, providing
high-level APIs can help the ADRS framework to focus on
the desired logic to optimize. With Cloudcast, exposing sim-
ulator internals, boilerplate state management, or concurrency
primitives in the prompt causes the LLM to waste iterations
tuning irrelevant implementation details rather than making
algorithmic improvements. The right abstraction is a minimal,
higher-level interface containing only the target function
signature (e.g., search_algorithm()) and essential helper
classes (e.g., make_nx_graph() to return cost profiles); this
provides a focused starting point for more effective search.
Takeaway 1. Choose the right abstraction by carefully
scoping available APIs to match the optimization goal.
[Prompt Generator] Basic or SOTA initial programs?
The choice of base program shapes the trajectory of algorithm
evolution. Buggy or weak baselines waste iterations on trivial
fixes, while strong baselines can accelerate progress. For ex-
ample, in LLM-SQL, starting from an overly simple baseline
(no column reordering) causes evolution to stagnate and yield
poor hit rates, as the system fails to discover row-specific
reordering. In contrast, starting from the published SOTA,
which already achieves high hit rates, enables OpenEvolve
to target the runtime bottleneck, evolving a solution in 100
iterations that is 3× faster.
On the other hand, overly strong baselines that encode
near-SOTA logic or rely on high-level APIs can limit the
search to shallow micro-optimizations. In Can’t-Be-Late
(Section 4.1), evolution from a simple greedy baseline
produced better results than starting from the stronger
Uniform Progress policy, which restricted exploration.
To navigate this trade-off, we recommend seeding evolution
with three to five diverse baselines, potentially generated
using coding assistants. We validate this with an ablation
study on LLM-SQL using OpenEvolve. Seeding all islands
with the same base program (the existing SOTA) caps
combined scores at 0.74. In contrast, initializing islands with
structurally diverse programs, ranging from basic designs
that capture the core column-reordering strategy to stronger
heuristics that incorporate lightweight statistical signals (e.g.,
column-frequency and prefix-match patterns), allows the
2Specific model recommendations (e.g., GPT-5 vs. Gemini-3.0) represent
a snapshot based on ablation studies across our 10 use cases. These serve
to illustrate design space trade-offs; we expect optimal generator choices
to evolve alongside model capabilities.


--- Page 14 ---
islands to start from different regions of the search space. This
diversity enabled evolutions from one island to reach a com-
bined score of 0.7755. Notably, only the runs that included
these diverse seeds had a final score higher than 0.74.
Takeaway 2. Start evolution with diverse, domain-
specialized seeds.
[Prompt Generator] More or fewer hints? While detailed
problem specifications are generally beneficial, the utility of
solution hints (i.e., specific suggestions for how to approach
the problem) is nuanced. Too much guidance can risk
premature convergence and prevent the discovery of novel
solutions, while too little can make the search inefficient. For
example, in EPLB, hints could have helped avoid wasted
iterations on “extreme” replication strategies. Conversely,
in Transaction Scheduling, hints about batching biased
the search toward sub-optimal designs, whereas leaving
it unconstrained enabled OpenEvolve to discover a 30%
faster greedy policy. This illustrates a fundamental trade-off:
providing more hints accelerates by pruning the search
space, whereas withholding hints fosters broader exploration,
potentially at the cost of efficiency.
We recommend a two-phase approach to balance exploration
and exploitation. In the first phase, we start with a generic,
problem-only prompt with no specific algorithmic hints to
allow the framework to explore freely. In the second, we
inject targeted algorithmic hints into the prompt so that the
search can escape local minima. We switch to this phase if
the best combined score does not improve over a number of
iterations. In practice, this prompting approach effectively
unblocks runs that would otherwise plateau. For both EPLB
and Cloudcast, initial progress using generic prompts (“use
optimization techniques”) plateaued after 12 and 8 iterations,
respectively. For EPLB, injecting a hint on proportional
allocation strategies that guided the model to discover
Hamilton apportionment, breaking the plateau (+28%) and
yielding +60% total improvement. For CloudCast, suggesting
a shared-tree multicast routing topology (replacing naive
per-destination paths) led to a program that improved the
score by +48%. In contrast, the evolution that continued with
the generic prompt failed to make further progress.
Takeaway 3. Iteratively refine hints to balance the
exploration-exploitation trade-off.
5.2
Evaluation: The Solution is Only as Good
as the Evaluator
[Evaluator] Avoid overfitting. To prevent overfitting (i.e.,
generated solutions hard-code behaviors to narrow work-
loads), evaluation feedback should be robust. Specifically,
we highlight two strategies. First, ensure test set diversity
via broad test sets that cover edge cases. Our ablation study
on Can’t-Be-Late (Appendix C.1) shows that limiting spot
scheduling evaluation to a single availability pattern degraded
performance on unseen workloads, whereas including diverse
traces mitigated this issue. Second, employ validation
safeguards, such as separate validation sets and a Lines of
Code (LOC) penalty to discourage hard-coded solutions.
Takeaway 4. Enforce diverse test sets and safeguards.
[Evaluator] Prevent reward hacking. Robust evaluators
are essential to prevent reward hacking in which generated
solutions exploit loopholes to maximize scores without
solving the problem. For example, in MAS, some candidates
bypassed one stage of the evaluation pipeline to achieve high
scores without performing the full task. On EPLB, the LLM
achieved high load balance by assigning zero weight to 97%
of experts. We later corrected this by explicitly enforcing that
every expert must receive at least one replica. To mitigate such
failures, evaluators should combine multiple signals (e.g.,
correctness, efficiency, robustness) and include adversarial
tests. This prevents the model from “gaming” the evaluation
and aligns candidate solutions with intended constraints.
In particular, we observe that allowing full-file rewrites
significantly increases the frequency and subtlety of reward
hacking compared to scoped (i.e., diff-based) edits. In Prism,
full rewrites on OpenEvolve enabled the LLM to delete the
GPU memory limit variable, which would be immutable
under diff-based edits (since it can be outside of the evolve
block). The generated program caught the exception triggered
by the undefined memory limit and continued execution,
effectively bypassing the constraint. Similarly, in Cloudcast,
where the model should only optimize find_path(), full
rewrites allowed it to omit broadcasting some data partitions,
illegitimately reducing costs by 90%.
Consequently, diff-based edits provide tighter control against
reward hacking. By restricting the LLM’s ability to change
code, we enforce “correctness by construction” and reduce the
burden on the evaluator to catch correctness violations. Fur-
thermore, limiting the number of changed LoC constrains de-
viation from the parent solution, analogous to KL-divergence
penalties in RL (e.g., TRPO [69], PPO [70]) that stabilize
policy updates. Extending this logic, adjusting the edit scope
allows us to regulate diversity, ensuring that child programs
diverge sufficiently from their parents to drive exploration.
We recommend starting with diff-based edits and selectively
transitioning to full rewrites when greater structural diversity
is needed. To mitigate the increased risk of reward hacking
during full rewrites, we advise refactoring the target program
to remove code not needed for evolution.
Takeaway 5. Prevent reward hacking by restricting edits.


--- Page 15 ---
[Evaluator] Score carefully. Having a precise scoring
function is key to obtaining good evolution results. We
recommend designing smooth reward functions to facilitate
incremental search. For instance, assigning penalized scores
to invalid programs, rather than zeroing them out, preserves
useful logic for future iterations (see "Learn from Failures").
Moreover, evaluation consistency across runs is crucial. For
example, EPLB algorithm runtime is hardware-dependent,
so we had to carefully isolate experiments to prevent
interference from affecting measurements.
Takeaway 6. Use smooth and deterministic scoring
functions.
[Generator] Select the right model. Given that models
exhibit distinct “thinking” and coding styles that impact
their effectiveness across different problems, we recommend
conducting preliminary evolution runs with multiple models
to select an effective LLM. GPT-5 tends to produce longer,
modular code with extensive edge case handling, whereas
Gemini-3.0 generates shorter, minimal code with less
abstraction. Tables 6 and 7 provide a detailed breakdown
of these trends. Among frameworks, OpenEvolve generates
the most concise implementations (averaging 262 LOC),
roughly 1.5-1.7× shorter than GEPA (406 LOC) and Shinka
(458 LOC). This framework-level efficiency compounds
with model selection:
Gemini-3.0 yields consistently
shorter programs (289 LOC avg.) compared to GPT-5 (461
LOC avg.), which were 1.6x longer. We analyze two key
implications: modularity and program length.
Modular programs are more effective for problems requiring
the synthesis of multiple techniques. For GEPA, GPT-5 out-
performs Gemini-3.0 on all tasks requiring the combination
of three or more techniques (EPLB, Cloudcast, Transaction
Scheduling, Table 4), producing 3–4× more code and 3–8
× more helper functions. Conversely, Gemini-3 excels on
simpler algorithmic tasks (CBL, Telemetry Repair).
While program length does not inherently correlate with
effectiveness, we recommend filtering overly long programs
(e.g., by tuning the maximum character limit parameter in
OpenEvolve). For instance, for NS3 with ShinkaEvolve,
GPT-5’s complex code introduced memory safety bugs
(e.g., use-after-erase, null dereference) and type confusion
absent in Gemini’s simpler and shorter implementations.
Recent studies confirm that excessive length often signals
unreliability [53], correlating with higher error rates and
low-quality patterns [83].
Across all problems (Tables 6 and 7), OpenEvolve produces
the shortest programs on average (262 LOC), followed by
GEPA (406 LOC) and Shinka (458 LOC). Across models,
Gemini-3 yields shorter programs on average (289 LOC)
than GPT-5 (461 LOC).
Takeaway 7. Select LLM based on solution composition.
5.3
Feedback: The “Devil” is in the Details
[Evaluator] Calibrate feedback granularity. To efficiently
guide the search, we recommend matching the feedback
granularity to the problem structure. For problems for which
the evaluation generates multiple results, we should provide
per-result feedback (e.g., per-expert in EPLB, per-trace in
CBL). This granular approach exposes workload-specific
failure modes and guides improvements on under-performing
cases. For example, GEPA’s ability to leverage per-result
feedback enabled it to return the best programs in multi-result
problems (EPLB, CBL, CBL-Multi, Cloudcast) compared
to the two other ADRS frameworks that we evaluate.
However, as mentioned before, excessive feedback can be
counterproductive. We perform an ablation study on CBL,
measuring three levels of feedback: minimal (aggregate cost
only), moderate (worst five scenarios), and detailed (per-trace
multi-dimensional breakdowns). Moderate feedback achieves
the best cost reduction (13.0%), outperforming minimal
(7.7%) and detailed (10.2%) feedback; this confirms that
while actionable guidance helps, too much detail can lead
to overfitting.
Takeaway 8. Calibrate feedback granularity to provide
actionable guidance without causing overfitting.
[Evaluator] Learn from failures. Rather than discarding
failed programs, it can be useful to retain them (i.e., via appro-
priately low scores or repair loops) as they might contain good
ideas that contribute to future improvements. We validate this
through an ablation study that implements an automatic retry
mechanism in OpenEvolve. In Transaction Scheduling, re-
pairing a child program that failed with a Python syntax error
improved the final score by 1.1×. Similarly, for LLM-SQL, a
candidate causing a 600s timeout was repaired into a 2.6s vari-
ant that became the run’s best program. These cases illustrate
that first-attempt failures can yield high-quality descendants if
given a chance for consideration and refinement. We note that
the general trade-off, rich feedback for faster convergence, and
prematurely narrowing the search space apply here as well.
Takeaway 9. Learn from failures to gain insights for
evolution.
[Evaluator] Leverage meta-feedback selectively. Incorpo-
rating meta-feedback from evolutionary history can accelerate
convergence in some cases. Frameworks like ShinkaEvolve
extract insights during evolution by periodically summarizing
recent successful strategies, generating recommendations
with additional model calls, and using these to tune subse-
quent prompts. On Prism, the meta-feedback helped identify


--- Page 16 ---
Framework
EPLB
PRISM
TELEMETRY
TXN
CLOUDCAST
NS3
CBL
CBL-Multi
LLM-SQL
MAS
Overall Avg
OpenEvolve
248
73
230
242
175
396
226
160
301
565
262
GEPA
499
314
634
334
537
∼380
219
181
364
600
406
Shinka
489
385
463
574
402
427
368
344
505
624
458
Table 6: LOC aggregated by framework (all models).
Model
EPLB
PRISM
TELEMETRY
TXN
CLOUDCAST
NS3
CBL
CBL-Multi
LLM-SQL
MAS
Overall Avg
Gemini-3
337
186
310
243
223
387
163
143
331
566
289
GPT-5
487
328
574
523
519
415
378
314
449
626
461
Table 7: LOC aggregated by model (all frameworks).
when greedy and local search strategies stagnated, explicitly
steering the model toward a more effective binary search
solution. Conversely, on CloudCast, the meta-feedback
provided incorrect guidance: it recommended “edge reuse
to minimize cost,” overlooking the fact that concurrent use
of network edges creates congestion, which increases transfer
time and leads to lower scores.
Takeaway 10. Meta-feedback helps guide search when
accurate.
[Evaluator] Handle non-deterministic evaluation with
care. Non-deterministic evaluators, where identical code
yields varying scores across runs, amplify framework-specific
trade-offs. For instance, GEPA’s subsample filtering with
noisy partial comparisons can lead to rejection of valid propos-
als due to variance in scores. Shinka’s meta-learning can lock
in early noise, mistaking randomness for signal that can bias
future generations. OpenEvolve was most resilient on our non-
deterministic use cases (NS3, Telemetry Repair), utilizing
population diversity to average out fitness noise and diff-
based constraints to prevent data overfitting. On NS3, where
scores can vary due to randomized flow start times within
a 10ms, OpenEvolve’s population approach smoothed the sig-
nal over runs while Shinka’s early lock-in led to high variance
between candidate solutions and GEPA rejected 89–97% of
proposals. On Telemetry Repair, where input measurements
contain synthetic errors, OpenEvolve’s diff-based edits led
to simpler code that generalized better over noisy data.
(simulator noise), Shinka’s early lock-in created a 14.71
point variance between identical runs, while GEPA rejected
89–97% of proposals; OpenEvolve’s population approach
successfully smoothed the signal. On Telemetry Repair
(data noise), OpenEvolve’s simplicity pressure produced
significantly shorter code (174 LOC vs GEPA’s 868
LOC) that generalized better. We recommend delaying
meta-feedback and using median scores from multiple
evaluations to mitigate these risks. Beyond framework
selection, we recommend two additional mitigations: (i) run
more iterations before applying meta-feedback, and (ii) use
median scores from multiple evaluations of each candidate.
Takeaway 11. Encode resilience for non-deterministic
evaluators.
6
Limitations and Open Challenges
In this section, we characterize the limitations of ADRS,
distinguishing between the types of problems for which the
approach is well suited and those for which it is not. We then
outline several open challenges aimed at improving ADRS
so that it can address a broader range of problems.
6.1
Which Problems Are Best Suited?
Our experience suggests that existing ADRS frameworks
excel on system problems with three key properties:
• Isolated changes: ADRS works best when improvements
can be made in a small, self-contained part of the system
(e.g., schedulers, cache managers, load balancers). In
contrast, problems that require coordinated changes
across multiple modules, e.g., distributed protocols with
interacting state machines [37, 59], remain difficult due to
limited multi-file reasoning and limited context lengths.
• Reliable evaluations: ADRS requires evaluators that can un-
ambiguously rank candidate solutions based on correctness
and performance. This is straightforward when correctness
relies on clear metrics or invariants (e.g., load balance fac-
tor). It is far harder when semantic equivalence is difficult
to verify, such as in arbitrary database query plan rewrites,
where checking equivalence is generally undecidable [3].
• Efficient evaluations:
Since
evolution
may require
thousands of iterations, each evaluation must be cheap.
Problems that require hours of GPU time or large distributed
testbeds (e.g., weight compression, large-scale training; see
Section 4) make evolution prohibitively slow or expensive.


--- Page 17 ---
As such, we do not expect ADRS to be as effective for
problems that can be cast as optimization problems and
solved directly using existing solvers, such as integer linear
programming (ILP) solvers.
6.2
Open Challenges
As ADRS emerges as a promising approach to accelerate
systems research, two natural questions arise: what should we
build to better support ADRS, and how should we improve
ADRS itself?
6.2.1
Supporting ADRS with Better Evaluators
ADRS frameworks are only as good as the evaluators guiding
them. Successful discovery requires evaluators that provide
accurate, fast, and detailed feedback:
• Fidelity: Evaluators must capture the salient system behav-
iors that are relevant to the problem being solved (e.g., flow-
level or packet-level fidelity in networking). Crucially, the
desired level of fidelity depends not only on the system un-
der study but also on the solution space that ADRS explores.
• Generality: Evaluators must support diverse workloads to
provide robust feedback signals and prevent overfitting to
a single configuration or dataset.
• Speed and reliability: Rapid evaluation is key to ADRS
scalability. Building infrastructure that supports rapid
forking and rollback (e.g., lightweight VMs, container
snapshots, or database cloning) can drastically accelerate
feedback cycles [44].
Next, we highlight two directions for improving efficiency.
Problem-specific
simulators.
Simulators
offer fast,
low-cost evaluation but are hard to design, as they must
balance fidelity and simplicity. A practical solution is to
build domain-specific simulators that capture only behaviors
relevant to the task (e.g., modeling CPU scheduling without
full operating system details). Tools like OpenEvolve can
iteratively refine these simulators until their target metrics
(e.g., latency, throughput) align with real systems. Though
costly to build, such simulators can be reused across related
problems, amortizing development effort.
Cascading evaluators. Fast and accurate evaluation can
be achieved via cascading evaluators: progressing from
fast, coarse-grained cost models to slower but high-fidelity
simulators. For example, one might begin with a simple
cost model (e.g., for database queries [72]), then progress to
simulators of increasing granularity, followed by emulators,
and finally tests on the real system. In networking, for
instance, session-level simulators offer coarser evaluation
compared to packet-level simulators. This mirrors standard
research practice: prototype quickly, then validate precisely.
6.2.2
Improving ADRS
To extend the reach of ADRS, we need advances across the
following key components.
Problem specification and prompting. Existing ADRS
frameworks often rely on simple prompts that provide only a
problem description. Just as human researchers ground their
ideas in prior work, ADRS frameworks should incorporate
retrieval techniques [62] to draw from a broader body of
knowledge (e.g., academic literature, documentation, and
related examples) to better guide their search. Prompt
evolution [5, 58], which allows the LLM to refine its own
context, can help manage the trade-off between exploration
and exploitation in providing hints by starting with generic
guidance and then dynamically incorporating more targeted
hints or examples when search stagnates.
Solution generator. A strong solution generator should
act like an autonomous developer, capable of navigating,
reasoning over, and modifying the full system codebase
rather than isolated code snippets. For example, modern AI
workloads often rely on workload analysis and cross-layer
optimizations to reduce hardware costs [17]. Similarly,
optimizations for distributed communication protocols
require coordinated changes to both sender and receiver
logic [57, 59]. Achieving this demands further agentic
capabilities: understanding dependencies, invoking analysis
tools, and reasoning coherently across non-contiguous code
modules. Beyond a single model, ADRS frameworks support
ensembles of specialized agents that can be dynamically
composed based on the given problem, much like forming
a research team with complementary skills.
6.2.3
Solution Selector and Search Process
The discovery process of ADRS frameworks remains largely
a black box. Current evolutionary searches are monolithic,
often producing a single final program that can mix good
ideas with poor implementation and thus penalize promising
concepts. Prior work [67] suggests separating ideation from
code generation to address this issue.
Moreover, evolutionary search is often inefficient, fre-
quently looping over failed heuristics or repeated errors
(Appendix C.3). We need more flexible frameworks that
support finer-grained feedback, enabling humans or LLMs to
lock in working code, boost diversity to escape local optima,
or roll back to prior versions when evolution stalls.
6.2.4
Objectives, Feedback, and Preference Learning
Some ADRS frameworks, such as OpenEvolve, require
researchers to formalize intuitive trade-offs into numerical
weights, which can be difficult in practice. For example, in
EPLB, we struggled with how to weigh the importance of
load balance against algorithm runtime. Furthermore, feed-
back granularity, which is currently determined by users, also


--- Page 18 ---
shapes evolution: our Can’t-Be-Late ablation confirms that
moderate, scenario-level feedback outperforms both minimal
(weak guidance) and detailed (overfitting-prone) feedback.
Future systems could instead learn user preferences auto-
matically. One approach is preference learning: ADRS could
present researchers with various solutions (e.g., “Solution
A is faster but less fair; Solution B is fairer but slower”) and
infer the underlying objective function from user choices.
Another approach is inverse reinforcement learning (IRL):
ADRS could infer reward functions from user-provided
examples of “good” solutions.
Framework-level changes.
Finally, we discuss open
challenges related to the overall ADRS framework.
Hyperparameter automation. Balancing exploration and
exploitation remains difficult and currently relies on trial and
error. Future work should focus on automating this tuning
process. For instance, a meta-learning layer could allow the
system dynamically adjust during evolution, making the
entire framework more reliable and accessible.
Human-ADRS interaction. The optimal balance between
synchronous (interactive assistants like Cursor) and asyn-
chronous (autonomous frameworks like OpenEvolve) user
interfaces remains an open question. A key challenge is
defining when human guidance adds values versus when
ADRS should act autonomously.
7
ADRS’s Impact on the Research Process
Despite their limitations (see Section 6.1), ADRS frameworks
can help researchers in two key ways:
• Accelerate
discovery:
ADRS
frameworks
automate
tedious tasks, such as implementation and, to some extent,
debugging, freeing researchers to focus on problem
selection and system design. Even imperfect solutions are
useful, as they often reveal new directions; in our telemetry
repair case study, AI-generated insights helped guide a
better human-designed algorithm.
• Achieve better-than-human results: ADRS tools can
explore the solution space more thoroughly. While humans
often stop after a breakthrough, AI can continue testing
variations with compounding incremental gains. In EPLB,
OpenEvolve produced an algorithm that surpassed SOTA
by exploiting potentially overlooked optimizations.
As such, ADRS has the potential to reshape systems research.
One way to think about ADRS is as providing researchers
with a virtually unbounded number of “assistants.” Much like
junior researchers, these frameworks are most effective when
given clear specifications and well-defined goals.
Consequently, ADRS adds another layer to the existing
research hierarchy.
In academia, for example, faculty
members typically advise Ph.D. students or postdocs, who
in turn mentor undergraduate students. Each of these roles
can now leverage ADRS tools to accelerate their work and
broaden the scope of what they can accomplish.
This shift raise a natural question: how will the role of a
researcher change? As ADRS tools become more effective,
we believe researchers will have more time to focus on
higher-leverage activities, namely, choosing problems and
formulating them precisely. If successful, ADRS has the
potential to elevate the entire research hierarchy, faculty and
students alike, and make them far more productive.
8
Related Work
ADRS builds on a long line of research that combines
large-scale search with AI to tackle complex problems.
Pre-LLM AI for System Optimizations. Prior to LLMs,
machine learning had already been widely applied to
optimize systems. In databases, learned models addressed
query optimization [50], cardinality estimation [81, 82],
learned indexes [35], and automated system tuning [7].
Reinforcement learning (RL) and other techniques advanced
core networking problems, including congestion control [32],
packet classification [41], and topology modeling [68]. More
broadly, RL has been applied in systems for scheduling
over data processing workloads [48], physical device
placement [54], and video streaming [19].
Automated discovery with learned approaches. AI
has increasingly powered automated discovery in complex
domains.
Early successes include AlphaGo [73] and
AlphaZero [74], which demonstrated how search and RL
can master games, and AlphaFold [33], which achieved
breakthroughs in protein structure prediction. AlphaDev [47]
extended this to low-level algorithms, while AlphaChip [24]
uses RL for chip layouts, and Big Sleep [78] applies AI
agents to detect software vulnerabilities. More recently,
benchmarks, such as AlgoTune [64], have been developed
to evaluate LLM program optimization abilities.
LLM-based coding assistants. Tools like GitHub Copi-
lot [23], Cursor [31], Codex [60], and Claude Code [8] accel-
erate research by helping researchers rapidly prototype ideas,
build simulators, and implement baselines. They enable rapid
translation of high-level concepts into working implementa-
tions. Recent work also explores using LLMs for performance-
critical code [29], such as GPU kernel code generation and op-
timization [61], further illustrating their potential for ADRS.
LLM-driven research. Beyond coding, recent work
leverages LLMs to automate broader parts of the research
process. Frameworks such as AlphaEvolve [55, 58] and
OpenEvolve [71] use the MAP-Elites algorithm and island
models to evolve new algorithms; GEPA [5] employs
reflective prompt evolution for better LLM generation; and


--- Page 19 ---
LLM4AD [42] provides a unified platform for algorithm
design. Others include ShinkaEvolve [38] (sample-efficient
evolution), EvoPrompt [25] (genetic prompt optimization),
MetaMuse [46] (self-reflective generation), and Policy-
Smith [20] (solution selection). Glia [27] presents an agentic
workflow for systems design, reporting strong results on
distributed LLM inference.
Attempts at end-to-end research automation are also
emerging. MLGym [56] offers AI research agent benchmarks,
while Code Researcher [75] explores agents for large-scale
software engineering. Work on self-evolving AI agents [21]
and broader systems applications [40, 87] is also growing.
Darwin [85] demonstrates open-ended, self-referential code
improvement, automatically evolving stronger coding agents.
Our work focuses on automated algorithm discovery
in systems, where strong evaluators enable the reliable
verification needed for productive automation.
9
Conclusion
As ADRS automates algorithm discovery, the role of the
human researcher will evolve. Much like how an academic
advisor guides a student, the researcher of the future will
direct these AI frameworks by defining problems, steering
the research process, and critically evaluating the results. One
of the most profound implications of this shift is the potential
for a virtuous cycle. We can use an ADRS to improve itself.
As recent work has shown [86], models can learn to refine
their own reasoning, debug code, and discover more effective
strategies. By rapidly iterating on their own capabilities, AI
agents will compound the pace of scientific discovery.
In this paper, we have demonstrated the potential of ADRS
in systems research. Our case studies show that the ADRS
approach can already outperform human baselines on key
performance problems. While it is still very early, we
call on the systems community to embrace these tools,
not just as accelerators of research, but as subjects of
research. Improving ADRS—making it efficient, scalable,
and reliable—is itself a systems challenge. As such, we
believe that system builders are uniquely positioned to shape
the future of AI-driven discovery.
References
[1] ns-3: A discrete-event network simulator for internet
systems. https://www.nsnam.org/, 2025. Accessed:
2025-12-15.
[2] Suryanarayan Menon A., Sanjay J Prakash, Vinayak
Naveen, Roshan Aji Cherian, Ron Regi Zacharia,
Suryapriya S., and Josna Vr.
A survey on routing
algorithms and techniques used to improve network
performance in software-defined networking.
In
2023 2nd International Conference on Computational
Systems and Communication (ICCSC), pages 1–6, 2023.
[3] Serge Abiteboul, Richard Hull, and Victor Vianu.
Foundations of Databases.
Addison-Wesley, 1995.
ISBN 0-201-53771-0.
[4] Vamsi Addanki, Oliver Michel, and Stefan Schmid.
PowerTCP:
Pushing
the
performance
limits
of
datacenter networks.
In 19th USENIX Symposium
on Networked Systems Design and Implementation
(NSDI 22), pages 51–70, Renton, WA, April 2022.
USENIX Association. ISBN 978-1-939133-27-4. URL
https://www.usenix.org/conference/nsdi22/
presentation/addanki.
[5] Lakshya A Agrawal, Shangyin Tan, Dilara Soylu,
Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav
Singhvi, Herumb Shandilya, Michael J Ryan, Meng
Jiang, et al.
Gepa: Reflective prompt evolution can
outperform reinforcement learning.
arXiv preprint
arXiv:2507.19457, 2025.
[6] DeepSeek AI. Expert parallelism load balancer (eplb).
https://github.com/deepseek-ai/eplb, 2024.
[7] Dana Van Aken, Andrew Pavlo, Geoffrey J. Gordon,
and Bohan Zhang. Automatic database management
system tuning through large-scale machine learning. In
Proceedings of the 2017 ACM SIGMOD International
Conference on Management of Data (SIGMOD ’17),
pages 1009–1024, Chicago, IL, USA, 2017. ACM. doi:
10.1145/3035918.3064029.
[8] Anthropic. Claude code: Agentic code assistant. https:
//www.anthropic.com/, 2025. Accessed: 2025-09-30.
[9] A. Boukerche, B. Turgut, N. Aydin, M.Z. Ahmad,
L. Bölöni, and D. Turgut.
Routing protocols in ad
hoc networks: a survey. Computer Networks, 55(13):
3032–3080, September 2011.
[10] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen
Krueger, Tom
Henighan, Rewon
Child, Aditya
Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric Sigler,
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei.
Lan-
guage models are few-shot learners, 2020.
URL
https://arxiv.org/abs/2005.14165.


--- Page 20 ---
[11] Mert Cemri, Melissa Z Pan, Shuyi Yang, Lakshya A
Agrawal, Bhavya
Chopra, Rishabh
Tiwari, Kurt
Keutzer, Aditya Parameswaran, Dan Klein, Kannan
Ramchandran, et al. Why do multi-agent llm systems
fail? arXiv preprint arXiv:2503.13657, 2025.
[12] Audrey Cheng, Xiao Shi, Aaron Kabcenell, Shilpa
Lawande, Hamza Qadeer, Jason Chan, Harrison Tin,
Ryan Zhao, Peter Bailis, Mahesh Balakrishnan, Nathan
Bronson, Natacha Crooks, and Ion Stoica. Taobench:
An end-to-end benchmark for social network workloads.
Proc. VLDB Endow., 15(9):1965–1977, may 2022.
ISSN 2150-8097.
[13] Audrey Cheng, Aaron Kabcenell, Jason Chan, Xiao Shi,
Peter Bailis, Natacha Crooks, and Ion Stoica. Towards
optimal transaction scheduling.
Proceedings of the
VLDB Endowment, 17(11):2694–2707, 2024.
[14] Audrey Cheng, Shu Liu, Melissa Pan, Zhifei Li, Bowen
Wang, Alex Krentsel, Tian Xia, Mert Cemri, Jongseok
Park, Shuo Yang, et al. Barbarians at the gate: How
ai is upending systems research.
arXiv preprint
arXiv:2510.06189, 2025.
[15] Wei-Lin Chiang, Zhuohan Li, Zi Lin, et al. Vicuna:
An open-source chatbot impressing gpt-4 with 90%
chatgpt quality. arXiv preprint arXiv:2304.08485, 2023.
ShareGPT conversations used as training data.
[16] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten
Bosma, Gaurav Mishra, Adam
Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian
Gehrmann, Parker Schuh, Kensen
Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao,
Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar
Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson,
Reiner Pope, James Bradbury, Jacob Austin, Michael
Isard, Guy Gur-Ari, Pengcheng
Yin, Toju
Duke,
Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev,
Henryk Michalewski, Xavier Garcia, Vedant Misra,
Kevin Robinson, Liam Fedus, Denny Zhou, Daphne
Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,
Alexander Spiridonov, Ryan Sepassi, David Dohan,
Shivani Agrawal, Mark Omernick, Andrew M. Dai,
Thanumalayan Sankaranarayana Pillai, Marie Pellat,
Aitor Lewkowycz, Erica
Moreira, Rewon
Child,
Oleksandr Polozov, Katherine Lee, Zongwei Zhou,
Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat,
Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
Palm: Scaling language modeling with pathways, 2022.
URL https://arxiv.org/abs/2204.02311.
[17] Jae-Won Chung, Nishil Talati, and Mosharaf Chowd-
hury. Toward cross-layer energy optimizations in ai
systems, 2024. arXiv:2404.06675v2.
[18] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
et al. Training verifiers to solve math word problems.
arXiv preprint arXiv:2110.14168, 2021.
[19] Kuntai Du, Ahsan Pervaiz, Xin Yuan, Aakanksha
Chowdhery, Qizheng Zhang, Henry Hoffmann, and
Junchen Jiang.
Server-driven video streaming for
deep learning inference. In Proceedings of the Annual
Conference of the ACM Special Interest Group on Data
Communication on the Applications, Technologies,
Architectures, and Protocols for Computer Communi-
cation (SIGCOMM ’20), pages 557–570, Virtual Event,
USA, 2020. ACM. doi: 10.1145/3387514.3405887.
[20] Rohit Dwivedula, Divyanshu Saxena, Aditya Akella,
Swarat Chaudhuri, and Daehyeok Kim.
Man-made
heuristics are dead. long live code generators!
In
Proceedings of the 24th ACM Workshop on Hot Topics
in Networks (HotNets ’25), 2025.
[21] Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang,
Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei
Liu, Zihao Li, et al.
A comprehensive survey of
self-evolving ai agents: A new paradigm bridging
foundation models and lifelong agentic systems. arXiv
preprint arXiv:2508.07407, 2025.
[22] Chuanchao Gao, Niraj Kumar, and Arvind Easwaran.
Energy-efficient
real-time
job
mapping
and
re-
source
management
in
mobile-edge
computing.
In
2024
IEEE
Real-Time
Systems
Symposium
(RTSS), page
15–28.
IEEE, December
2024.
doi: 10.1109/rtss62706.2024.00012.
URL http:
//dx.doi.org/10.1109/RTSS62706.2024.00012.
[23] GitHub.
Github copilot.
https://github.com/
features/copilot, 2021. Accessed: 2025-09-30.
[24] Anna Goldie and Azalia Mirhoseini.
How al-
phachip
transformed
computer
chip
design.
https://deepmind.google/discover/blog/
how-alphachip-transformed-computer-chip-
design/, September 2024. DeepMind Blog.
[25] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao
Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang.
Connecting large language models with evolutionary
algorithms yields powerful prompt optimizers. arXiv
preprint arXiv:2309.08532, 2023.
[26] Pouya
Hamadanian, Pantea
Karimi, Arash Nasr-
Esfahany, Kimia Noorbakhsh, Joseph Chandler, Ali
ParandehGheibi, Mohammad
Alizadeh, and
Hari
Balakrishnan. Glia: A human-inspired ai for automated
systems design and optimization, 2025.


--- Page 21 ---
[27] Pouya
Hamadanian, Pantea
Karimi, Arash Nasr-
Esfahany, Kimia Noorbakhsh, Joseph Chandler, Ali
ParandehGheibi, Mohammad
Alizadeh, and
Hari
Balakrishnan.
Glia: A human-inspired ai for auto-
mated systems design and optimization, 2025. URL
https://arxiv.org/abs/2510.27176.
[28] Ruining He and Julian McAuley.
Ups and downs:
Modeling the visual evolution of fashion trends
with one-class collaborative filtering.
In Proceed-
ings of the 25th International Conference on World
Wide Web, WWW ’16, page 507–517, Republic and
Canton of Geneva, CHE, 2016. International World
Wide Web Conferences Steering Committee.
ISBN
9781450341431. doi: 10.1145/2872427.2883037. URL
https://doi.org/10.1145/2872427.2883037.
[29] Charles
Hong, Sahil Bhatia, Alvin
Cheung, and
Yakun Sophia Shao.
Autocomp: Llm-driven code
optimization for tensor accelerators, 2025.
URL
https://arxiv.org/abs/2505.18574.
[30] Sirui Hong, Mingchen Zhuge, Jiaqi Chen, Xiawu
Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang,
Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang
Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and
Jürgen Schmidhuber. Metagpt: Meta programming for
a multi-agent collaborative framework, 2024.
URL
https://arxiv.org/abs/2308.00352.
[31] Cursor Inc.
Cursor: Ai coding assistant.
https:
//www.cursor.com/, 2024. Accessed: 2025-09-30.
[32] Nathan Jay, Noga H Rotman, P Godfrey, Michael
Schapira, and Aviv Tamar. Internet congestion control
via deep reinforcement learning.
arXiv preprint
arXiv:1810.03259, 2018.
[33] John Jumper, Richard Evans, Alexander Pritzel, Tim
Green, Michael Figurnov, Olaf Ronneberger, Kathryn
Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna
Potapenko, et al.
Highly accurate protein structure
prediction with alphafold. nature, 596(7873):583–589,
2021.
[34] Kamil Khan, Sudeep Pasricha, and Ryan Gary Kim.
A survey of resource management for processing-in-
memory and near-memory processing architectures.
Journal of Low Power Electronics and Applications, 10
(4):30, September 2020. doi: 10.3390/jlpea10040030.
URL https://doi.org/10.3390/jlpea10040030.
[35] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and
Neoklis Polyzotis. The case for learned index structures,
2018. URL https://arxiv.org/abs/1712.01208.
[36] Alexander Krentsel, Rishabh Iyer, Isaac Keslassy,
Sylvia Ratnasamy, Anees Shaikh, and Rob Shakir. The
case for validating inputs in software-defined wans. In
Proceedings of the 23rd ACM Workshop on Hot Topics
in Networks, pages 246–254, 2024.
[37] Leslie Lamport. Paxos made simple. ACM SIGACT
News (Distributed Computing Column) 32, 4 (Whole
Number 121, December 2001), pages 51–58, 2001.
[38] Robert Tjarko Lange, Yuki Imajuku, and Edoardo
Cetin.
Shinkaevolve: Towards open-ended and
sample-efficient program evolution, 2025.
URL
https://arxiv.org/abs/2509.19349.
[39] Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua
Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying
Geng, Nan Huo, et al.
Can llm already serve as a
database interface? a big bench for large-scale database
grounded text-to-sqls. Advances in Neural Information
Processing Systems, 36, 2024.
[40] Chieh-Jan
Mike
Liang, Haoran
Qiu, Francis
Y.
Yan, Tianyin
Xu, and
Lidong
Zhou.
The
next horizon
of system
intelligence.
https:
//www.microsoft.com/en-us/research/blog/
the-next-horizon-of-system-intelligence/,
September 2025. Accessed: 2025-09-29.
[41] Eric
Liang, Hang
Zhu, Xin
Jin, and Ion
Sto-
ica.
Neural packet classification, 2019.
URL
https://arxiv.org/abs/1902.10319.
[42] Fei Liu, Rui Zhang, Zhuoliang Xie, Rui Sun, Kai Li,
Xi Lin, Zhenkun Wang, Zhichao Lu, and Qingfu Zhang.
Llm4ad: A platform for algorithm design with large lan-
guage model. arXiv preprint arXiv:2412.17287, 2024.
[43] Shu Liu, Asim Biswal, Amog Kamsetty, Audrey Cheng,
Luis Gaspar Schroeder, Liana Patel, Shiyi Cao, Xiangxi
Mo, Ion Stoica, Joseph E Gonzalez, et al. Optimizing
llm queries in relational data analytics workloads. arXiv
preprint arXiv:2403.05821, 2024.
[44] Shu Liu, Soujanya Ponnapalli, Shreya Shankar, Sepanta
Zeighami, Alan Zhu, Shubham Agarwal, Ruiqi Chen,
Samion Suwito, Shuo Yuan, Ion Stoica, et al. Support-
ing our ai overlords: Redesigning data systems to be
agent-first. arXiv preprint arXiv:2509.00997, 2025.
[45] Phillip Long, Zachary Novack, Taylor Berg-Kirkpatrick,
and Julian McAuley. Pdmx: A large-scale public do-
main musicxml dataset for symbolic music processing,
2024. URL https://arxiv.org/abs/2409.10831.
[46] Ruiying
Ma, Chieh-Jan
Mike
Liang, Yanjie
Gao, and Francis
Y.
Yan.
Algorithm
gen-
eration
via
creative
ideation, 2025.
URL
https://arxiv.org/abs/2510.03851.


--- Page 22 ---
[47] Daniel J Mankowitz, Andrea Michi, Anton Zhernov,
Marco Gelmi, Marco Selvi, Cosmin Paduraru, Edouard
Leurent, Shariq Iqbal, Jean-Baptiste Lespiau, Alex
Ahern, et al.
Faster sorting algorithms discovered
using deep reinforcement learning. Nature, 618(7964):
257–263, 2023.
[48] Hongzi Mao, Malte Schwarzkopf, Shaileshh Bojja
Venkatakrishnan, Zili
Meng, and Mohammad Al-
izadeh.
Learning scheduling algorithms for data
processing clusters. In Proceedings of the 2019 ACM
SIGCOMM Conference, pages 270–288, Beijing, China,
2019. ACM. doi: 10.1145/3341302.3342080. URL
https://doi.org/10.1145/3341302.3342080.
[49] Ziming Mao, Tian Xia, Zhanghao Wu, Wei-Lin Chiang,
Tyler Griggs, Romil Bhardwaj, Zongheng Yang, Scott
Shenker, and Ion Stoica. Skyserve: Serving ai models
across regions and clouds with spot instances. In Pro-
ceedings of the Twentieth European Conference on Com-
puter Systems, EuroSys ’25, page 159–175, New York,
NY, USA, 2025. Association for Computing Machinery.
ISBN 9798400711961. doi: 10.1145/3689031.3717459.
URL https://doi.org/10.1145/3689031.3717459.
[50] Ryan Marcus, Parimarjan Negi, Hongzi Mao, Chi
Zhang, Mohammad Alizadeh, Tim
Kraska, Olga
Papaemmanouil, and Nesime Tatbul. Neo: a learned
query optimizer.
Proceedings of the VLDB En-
dowment, 12(11):1705–1718, July 2019.
ISSN
2150-8097. doi: 10.14778/3342263.3342644. URL
http://dx.doi.org/10.14778/3342263.3342644.
[51] Julian
McAuley, Jure
Leskovec, and
Dan
Ju-
rafsky.
Learning
attitudes
and
attributes
from
multi-aspect
reviews, 2012.
URL
https://arxiv.org/abs/1210.3926.
[52] Wes McKinney. Data structures for statistical computing
in python. In Proceedings of the 9th Python in Science
Conference (SciPy 2010), pages 51–56. SciPy, 2010.
[53] Elliot Meyerson, Giuseppe Paolo, Roberto Dailey,
Hormoz Shahrzad, Olivier Francon, Conor F. Hayes,
Xin Qiu, Babak Hodjat, and Risto Miikkulainen.
Solving a million-step llm task with zero errors, 2025.
URL https://arxiv.org/abs/2511.09030.
[54] Azalia Mirhoseini, Hieu Pham, Quoc V. Le, Benoit
Steiner, Rasmus Larsen, Yuefeng Zhou, Naveen Kumar,
Mohammad Norouzi, Samy Bengio, and Jeff Dean.
Device placement optimization with reinforcement
learning.
In Proceedings of the 34th International
Conference on Machine Learning (ICML 2017),
volume
70, pages
2430–2439, Sydney, Australia,
2017.
PMLR.
doi:
10.5555/3305890.3305932.
URL
http://proceedings.mlr.press/v70/
mirhoseini17a.html.
[55] Ansh Nadga, Abhradeep Thakurta, and Prabhakar
Raghavan.
Ai as a research partner: Advanc-
ing
theoretical
computer
science
with
alphae-
volve.
https://research.google/blog/ai-as-
a-research-partner-advancing-theoretical-
computer-science-with-alphaevolve/,
Septem-
ber 30 2025. Google DeepMind Research Blog.
[56] Deepak Nathani, Lovish Madaan, Nicholas Roberts,
Nikolay Bashlykov, Ajay Menon, Vincent Moens,
Amar Budhiraja, Despoina Magka, Vladislav Vorotilov,
Gaurav Chaurasia, et al. Mlgym: A new framework
and benchmark for advancing ai research agents. arXiv
preprint arXiv:2502.14499, 2025.
[57] Harald Ng, Seif Haridi, and Paris Carbone. Omni-paxos:
Breaking the barriers of partial connectivity.
In
Proceedings of the Eighteenth European Conference
on Computer Systems, EuroSys ’23, page 314–330,
New York, NY, USA, 2023. Association for Computing
Machinery. ISBN 9781450394871.
[58] Alexander Novikov, Ngân Vu, Marvin Eisenberger,
Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner,
Sergey Shirobokov, Borislav Kozlovskii, Francisco JR
Ruiz, Abbas Mehrabian, et al. Alphaevolve: A coding
agent for scientific and algorithmic discovery, 2025.
URL: https://arxiv. org/abs/2506.13131.
[59] Diego Ongaro and John Ousterhout. In search of an
understandable consensus algorithm. In Proceedings
of the 2014 USENIX Annual Technical Conference
(USENIX ATC ’14), pages 305–320, Philadelphia, PA,
2014. USENIX Association.
[60] OpenAI. Openai codex. https://openai.com/index/
introducing-codex/, 2025. Accessed: 2025-09-30.
[61] Anne Ouyang, Simon Guo, Simran Arora, Alex L
Zhang, William
Hu, Christopher Ré, and Azalia
Mirhoseini. Kernelbench: Can llms write efficient gpu
kernels? arXiv preprint arXiv:2502.10517, 2025.
[62] Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang,
Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez.
Memgpt: Towards LLMs as operating systems. arXiv
preprint arXiv:2310.08560, 2023. Version v2, 12 Feb
2024.
[63] Bo Pang and Lillian Lee. Seeing stars: Exploiting class
relationships for sentiment categorization with respect
to rating scales. In Proceedings of the ACL, 2005.


--- Page 23 ---
[64] Ori Press, Brandon Amos, Haoyu Zhao, Yikai Wu,
Samuel K Ainsworth, Dominik Krupke, Patrick Kidger,
Touqir Sajed, Bartolomeo Stellato, Jisun Park, et al. Al-
gotune: Can language models speed up general-purpose
numerical programs? arXiv preprint arXiv:2507.15887,
2025.
[65] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen,
Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen,
Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan
Liu, and Maosong Sun.
Chatdev: Communica-
tive agents for software development, 2024.
URL
https://arxiv.org/abs/2307.07924.
[66] Kun Ren, Alexander Thomson, Daniel Abadi, Thaddeus
Diamond, and Anthony Tomasic. Slog: Serializable,
low-latency, geo-distributed transactions. Proceedings
of the VLDB Endowment, 12(11):1747–1761, 2019.
[67] Bernardino
Romera-Paredes,
Mohammadamin
Barekatain,
Alexander
Novikov,
Matej
Balog,
M. Pawan Kumar, Emilien Dupont, Francisco J. R.
Ruiz, Jordan S. Ellenberg, Pengming Wang, Omar
Fawzi, Pushmeet Kohli, and Alhussein Fawzi. Math-
ematical discoveries
from
program
search with
large language models. Nature, 625(7995):468–475,
2024.
doi: 10.1038/s41586-023-06924-6.
URL
https://doi.org/10.1038/s41586-023-06924-6.
[68] Krzysztof Rusek, Jose Suarez-Varela, Paul Almasan,
Pere
Barlet-Ros, and
Albert
Cabellos-Aparicio.
Routenet:
Leveraging graph neural networks for
network modeling and optimization in sdn.
IEEE
Journal on
Selected Areas
in
Communications,
38(10):2260–2270, October 2020.
ISSN 1558-
0008.
doi: 10.1109/jsac.2020.3000405.
URL
http://dx.doi.org/10.1109/JSAC.2020.3000405.
[69] John Schulman, Sergey Levine, Pieter Abbeel, Michael
Jordan, and Philipp Moritz.
Trust region policy
optimization. In International conference on machine
learning, pages 1889–1897. PMLR, 2015.
[70] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
Radford, and Oleg Klimov. Proximal policy optimiza-
tion algorithms. arXiv preprint arXiv:1707.06347, 2017.
[71] Asankhaya
Sharma.
Openevolve:
an
open-
source evolutionary coding agent, 2025.
URL
https://github.com/codelion/openevolve.
[72] Tarique
Siddiqui, Alekh Jindal, Shi
Qiao, Hiren
Patel, and Wangchao Le.
Cost models for big data
query processing:
Learning, retrofitting, and our
findings. In Proceedings of the 2020 ACM SIGMOD
International Conference on Management of Data
(SIGMOD ’20), page 15–29, Portland, OR, USA,
2020. ACM. doi: 10.1145/3318464.3380584. URL
https://doi.org/10.1145/3318464.3380584.
[73] David Silver, Julian Schrittwieser, Karen Simonyan,
Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas
Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al.
Mastering the game of go without human knowledge.
nature, 550(7676):354–359, 2017.
[74] David Silver, Thomas Hubert, Julian Schrittwieser,
Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc
Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
Graepel, Timothy Lillicrap, Karen Simonyan, and
Demis Hassabis.
A general reinforcement learning
algorithm that masters chess, shogi, and go through
self-play. Science, 362(6419):1140–1144, 2018. doi:
10.1126/science.aar6404.
[75] Ramneet Singh, Sathvik Joel, Abhav Mehrotra, Nalin
Wadhwa, Ramakrishna B Bairi, Aditya Kanade, and
Nagarajan Natarajan. Code researcher: Deep research
agent for large systems code and commit history. arXiv
preprint arXiv:2506.11060, 2025.
[76] Shewaye Sirika and Smita Mahajan.
Survey on
dynamic routing protocols. International Journal of
Engineering Research & Technology (IJERT), 5(1),
January 2016. doi: 10.17577/IJERTV5IS010028. Paper
ID: IJERTV5IS010028.
[77] Alexander Thomson, Thaddeus Diamond, Shu-Chun
Weng, Kun Ren, Philip Shao, and Daniel Abadi. Calvin:
Fast distributed transactions for partitioned database
systems. In Proceedings of the 10th USENIX Sympo-
sium on Operating Systems Design and Implementation
(OSDI), 2012.
[78] Kent Walker.
A summer of security: empowering
cyber defenders with ai, July 2025.
URL https:
//blog.google/technology/safety-security/
cybersecurity-updates-summer-2025/. Accessed:
2025-10-02.
[79] Sarah Wooders, Shu Liu, Paras Jain, Xiangxi Mo,
Joseph E Gonzalez, Vincent Liu, and Ion Stoica.
Cloudcast:{High-Throughput},{Cost-Aware} overlay
multicast in the cloud. In 21st USENIX Symposium on
Networked Systems Design and Implementation (NSDI
24), pages 281–296, 2024.
[80] Zhanghao Wu, Wei-Lin Chiang, Ziming Mao, Zongheng
Yang, Eric Friedman, Scott Shenker, and Ion Stoica.
Can’t be late: optimizing spot instance savings under
deadlines. In 21st USENIX Symposium on Networked
Systems Design and Implementation (NSDI 24), pages
185–203, 2024.


--- Page 24 ---
[81] Zongheng Yang, Amog Kamsetty, Sifei Luan, Eric
Liang, Yan Duan, Xi Chen, and Ion Stoica.
Neuro-
card: one cardinality estimator for all tables.
Proc.
VLDB Endow., 14(1):61–73, September 2020. ISSN
2150-8097. doi: 10.14778/3421424.3421432. URL
https://doi.org/10.14778/3421424.3421432.
[82] Zongheng Yang, Wei-Lin Chiang, Sifei Luan, Gau-
tam Mittal, Michael Luo, and Ion Stoica.
Balsa:
Learning a query optimizer without expert demon-
strations.
In Proceedings of the 2022 International
Conference
on
Management of Data, SIGMOD-
/PODS ’22, pages 931–944, New York, NY, USA,
2022. Association for Computing Machinery. ISBN
9781450392495. doi: 10.1145/3514221.3517885. URL
https://doi.org/10.1145/3514221.3517885.
[83] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan,
Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan,
Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin,
Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong,
Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu,
Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang,
Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou,
Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan,
Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An
open-source llm reinforcement learning system at scale,
2025. URL https://arxiv.org/abs/2503.14476.
[84] Shan Yu, Jiarong Xing, Yifan Qiao, Mingyuan Ma,
Yangmin Li, Yang Wang, Shuo Yang, Zhiqiang Xie,
Shiyi Cao, Ke Bao, et al. Prism: Unleashing gpu sharing
for cost-efficient multi-llm serving.
arXiv preprint
arXiv:2505.04021, 2025.
[85] Jenny Zhang, Shengran Hu, Cong Lu, Robert Lange,
and Jeff Clune. Darwin godel machine: Open-ended
evolution of self-improving agents.
arXiv preprint
arXiv:2505.22954, 2025.
[86] Jenny Zhang, Shengran Hu, Cong Lu, Robert Lange,
and Jeff Clune. Darwin godel machine: Open-ended
evolution of self-improving agents, 2025.
URL
https://arxiv.org/abs/2505.22954.
[87] Lidong Zhou.
A match made in silicon: The co-
evolution of systems and ai. In NeurIPS 2024 Invited
Talks, 2024.
URL https://neurips.cc/virtual/
2024/invited-talk/101132. Accessed: 2025-10-07.


--- Page 25 ---
A
A Pilot Survey of Time Spent in Systems
Research
/* accheng: @Ion, do we want to keep this still? */
Prob Formulation
21.5%
Eval Framework
22.9%
Solution (Alg Design)
21.5%
Evaluation
20.1%
Writing
14.0%
Avg. Time Distribution Across Research Process
Figure 7: Time spent in various stages of the systems research
process in systems, based on a survey of 31 PhD students.
Algorithm Design (21.5%) and Evaluation (20.1%) together
account for over 40% of total effort, highlighting a significant
opportunity for leveraging AI to accelerate this process.
To understand how researchers allocate their time across
stages of the systems research process, we conducted a small
survey of 31 PhD students in broadly systems research at a
US university. Figure 7 shows the approximate fraction of
effort they report spending in each stage.
B
Additional Case Studies
We also provide descriptions on the other case studies we run,
including network telemetry repair, cloudcast, global model
placement, multi-agent system optimization, and datacenter
congestion control.
B.1
Case Study #5: Telemetry Repair
This task identified by HotNets’24 [36] studies repair of
faulty router telemetry signals, which can become buggy
due to router faults or collection infrastructure errors. Such
inconsistencies (for example, counters on the two ends of a
link not matching) can cause the network controllers to make
incorrect decisions. The objective is to detect and repair faulty
telemetry to produce a self-consistent view of network state.
We use multiple evolve frameworks (e.g. OpenEvolve,
GEPA, ShinkaEvolve) to evolve repair strategies, running
100 iterations with GPT-5 and Gemini 3.0-Pro models,
supplemented by contextual hints from the HotNets’24 paper.
The best evolved program introduces structured repair logic,
including averaging nearby counters to reduce noise, and
separating repair and confidence estimation into distinct steps.
It achieves a repair score of 95% and confidence calibration
of 95%, outperforming the HotNets’24 solution (86% repair,
65% confidence).
B.2
Case Study #6: Cloudcast
The Cloudcast problem, published in NSDI ’24 [79], studies
cost-aware multicast across multi-region and multi-cloud
topologies. The objective is to construct an overlay connect-
ing a source, multiple destinations, and optional waypoints
so as to minimize egress cost.
Our initial program is a direct replication strategy: the
source sends data independently to each destination. While
simple, this approach often incurs high egress costs when des-
tinations are located in distant or expensive regions. To guide
evolution, the evaluator tests candidate algorithms across 10
multi-region, multi-cloud configurations and assigns a total
score based on the overall egress cost of the scheduled paths.
We use our evolved frameworks to run 100 iterations in about
one hour, costing less than $10 for the entire run.
The evolved solution successfully discovers a Steiner tree
strategy, achieving an average cost reduction of 31.1% com-
pared to naive direct replication. The best performing solution
is close to the human state-of-the-art solution. This solution
constructs a cost-efficient multicast tree by introducing
intermediate waypoints. For example, data may be replicated
once at a cheaper waypoint region and then forwarded to
multiple destinations, reducing the total egress cost.
B.3
Case Study #7: Model Placement (Prism)
The research problem [84] focus on the challenge of multi-
LLM serving on shared GPUs under bursty, heterogeneous
workloads. The key metric is the KV pressure ratio (KVPR),
defined as the SLO-weighted request rate divided by available
KV cache memory. The optimization goal is to minimize the
maximum KVPR across GPUs, thereby reducing contention
and improving SLO compliance.
The base program is the algorithm from the paper [84]:
models are placed sequentially onto the first GPU with
sufficient remaining memory, without considering long-term
balance. While feasible, this approach often overloads some
GPUs while leaving others underutilized. We use OpenEvolve
to evolve improved placement strategies, guided by a scoring
function that combines execution correctness with the KVPR
objective. The evolution runs with GPT-5 and Gemini-3.
The best evolved program achieves an 18.5% higher score
compared to the state-of-the-art reported in the original paper.
The evolved strategy mirrors the SOTA algorithm from
the paper and assignment used in PRISM, but crucially
adds a local improvement stage. After the initial placement,
it repeatedly tests whether moving or swapping models
between GPUs reduces the maximum KVPR, applies such
refinements, and stops once it finds a move that can reduce
the current maximum KVPR.


--- Page 26 ---
B.4
Case Study #8:
Multi-Agent System
Optimization (MAS)
The research problem extends the work of [11] (NeurIPS’25),
which studies diagnosing and repairing failures in multi-agent
LLM systems (MAS) and proposes MAST as the taxonomy
of MAS failure modes. Such systems (e.g., MetaGPT [30],
ChatDev [65]) often suffer from breakdowns in coordination,
memory, or communication that reduce task success. The
challenge is how to improve multi-agent systems, evolving
more robust agent architectures, prompts, and inter-agent com-
munication patterns to increase reliability and performance.
The base program is a direct adaptation of MetaGPT [30],
assembled into a minimal Python implementation (roughly
400 LOC). This initial version defines fixed agent roles,
communication protocols, and system prompts, but frequently
encounters the failure modes identified in the MAST. To
guide evolution, the evaluator uses the MAST annotator,
which assigns a score of 1/(1 + total FM occurrence),
penalizing common coordination and memory failures.
We ran three evolution configurations with different
mutation scopes: (1) modifying agent definitions and commu-
nication schemes, (2) introducing new verification and com-
munication flows using GPT-5, and (3) allowing changes to
the number and types of agents, i.e., the system topology. The
evolved solutions introduced innovations such as improved
context management (v1) and verification/communication
flows (v2), though removing verification in v3 degraded per-
formance. Overall, downstream program development success
improved from 40% in the base program to 47% (v1) and 53%
(v2) on the ProgramDev-v1 benchmark, before dropping to
30% in v3. The fast that verification agent was removed in v3
was an example of reward hacking (since we penalize the ver-
ification failures, the evolution algorithm got rid of the whole
verification when it could) and an example of the importance
of carefully tuning which parts of the initial code the evolution
algorithm is allowed to change. These results show that the
evolve frameworks can automatically discover MAS design
refinements that improve robustness, though careful control
over mutation scope is critical to avoid reward hacking.
B.5
Case Study #9: Datacenter TCP Conges-
tion Control (NS3)
This research problem extends PowerTCP [4] (NSDI ‘22),
which proposes a delay-based congestion control protocol
for datacenter networks that uses a “power” signal (i.e., the
ratio of throughput to delay) to detect congestion. Traditional
datacenter TCP variants struggle with incast scenarios where
many flows simultaneously converge on a single receiver,
causing buffer bloat and packet loss. We leverage the existing
benchmark implementation in Network Simulator 3 (NS-
3) [1] to investigate the ability of ADRS frameworks to opti-
mize networking applications written in C++. The challenge
is how to improve congestion control algorithms, evolving
more responsive rate adaptation mechanisms that maximize
throughput while minimizing switch queue occupancy.
The base program is θ-PowerTCP, a variant that does not
require in-network telemetry and instead infers congestion
purely from end-to-end RTT measurements. This initial
implementation (roughly 400 LOC) defines the rate control
logic, which calculates a power signal from RTT gradients
and adjusts sending rates accordingly. The benchmark
runs on NS3, where scores can vary due to randomized
flow start times within a 10ms window, simulating a 10:1
incast scenario where ten flows are launched toward a
receiver already handling a long-running background flow.
To guide evolution, the evaluator uses a fitness function
e(throughputGbps−20) −queue_length/100, rewarding high
throughput while penalizing queue buildup.
The evolved solutions discovered several key innovations:
(1) conservative flow startup, where new flows begin at 1/10
of the maximum link rate rather than full speed, preventing
burst-induced congestion; (2) explicit congestion notification
handling with forced minimum congestion signals; and (3)
adjusted rate control parameters. The best evolved program
reduced average queue length by 49% compared to the
original implementation while maintaining nearly identical
throughput. It is worth noting that the most impactful opti-
mization was the conservative initial rate, a counterintuitive
finding since aggressive startup seems optimal for throughput
but actually triggers severe congestion before the control loop
can react. These results demonstrate that the evolution frame-
works can automatically discover protocol-level optimizations
in low-level systems code that might otherwise be overlooked.
C
Additional Analysis
We provide additional analyses that complement the main
results and offer a deeper view of how ADRS behaves across
settings and frameworks. We examine how changing train-set
coverage affects policy quality, compare program length
across frameworks and models, and present a schema table
that formalizes the components of an ADRS setup. We also
summarize recurring failure patterns observed and highlight
cross-domain techniques that emerged during evolution.
C.1
Changing Train Set Coverage
We evaluate how training-set coverage affects the learned
policies in the CBL case study. Figure 8 compares results
obtained when training on 3% of the training set versus the
full training set.
Figure 8 shows results across all hardware profiles. Each
panel reports cost-savings curves for Greedy, Uniform
Progress, and the OpenEvolve solution as a function of job
fraction. In both settings, the curves exhibit similar shapes,


--- Page 27 ---
0
20
40
60
Cost savings (%)
1xK80 (0.95)
1xV100 (0.68)
1xV100 (0.60)
8xK80 (0.59)
0.6
0.8
Job Fraction
0
20
40
60
Cost savings (%)
8xK80 (0.55)
0.6
0.8
Job Fraction
8xV100 (0.34)
0.6
0.8
Job Fraction
8xV100 (0.33)
0.6
0.8
Job Fraction
1xK80 (0.25)
Greedy
Uniform Progress
OpenEvolve Solution
(a) Training with 3% of the available training set.
0
20
40
60
Cost savings (%)
1xK80 (0.95)
1xV100 (0.68)
1xV100 (0.60)
8xK80 (0.59)
0.6
0.8
Job Fraction
0
20
40
60
Cost savings (%)
8xK80 (0.55)
0.6
0.8
Job Fraction
8xV100 (0.34)
0.6
0.8
Job Fraction
8xV100 (0.33)
0.6
0.8
Job Fraction
1xK80 (0.25)
Greedy
Uniform Progress
OpenEvolve Solution
(b) Training with the full training set.
Figure 8: Impact of training-set coverage on policy evolution.
We split the data into 30% training and 70% testing. The left
panel uses 3% of the training data, while the right panel uses
the full set.
and the ordering among the three methods remains visually
consistent across the two training-coverage conditions.
C.2
Schema Table
Table 8 summarizes the components used to define and eval-
uate an ADRS setup. It covers four main elements—Prompt
Generator, Solution Generator, Evaluator, and Solution
Selector—and lists the specific factors included in each (e.g.,
problem description, optimization objectives, model choice,
test environment, and feedback signals). The table also
includes dimensions used in our analysis of results (e.g., per-
formance and robustness) and of the evolution process (e.g.,
search trajectory, recurring failure patterns, and feedback
utility). Together, these dimensions outline the information
required to specify an ADRS problem and to evaluate both
the resulting solutions and the evolution dynamics.
C.3
Failure Patterns
Table 9 groups failures observed in ADRS pipelines into
runtime errors, search failures, and algorithm failures.
Runtime errors include syntax or interface issues and cases
where candidates exceed resource limits. Search failures
include premature convergence, repeated solution loops,
and mutation drift. Algorithm failures include misaligned
objectives, shallow optimizations, overfitting, and reward
hacking. These categories reflect the types of failure modes
observed across 420 LLM-judged traces.


--- Page 28 ---
Dimension
Key Components
Prompt
Generator
(Problem
Setup)
Problem description: problem domains – computer systems (networking, distributed systems,
databases, MLSys, etc.); problem description – performance optimization, e.g., find the most
cost-effective transfer graph
Optimization objective: e.g., latency, throughput, cost, algorithm runtime
Constraints: e.g., latency SLOs
Solution Generator
LLM type: what model used for solution generation, this includes reasoning vs. non-reasoning,
tool-use vs. non-tool-use, LLM ensemble
Number of iterations: number of rounds to iterate the solution
Evaluator
Environment and test data: testbed environment such as CPU simulator, database, GPU
environment; test data and traces
Initial Program: the initial program to start with, use public source (GitHub/paper) if available,
otherwise use simple algorithm
Additional Baselines: additional baselines for comparison if any
Evaluator Feedback: execution score, more advanced if any (e.g., error messages, human- or
agent-in-the-loop feedback)
Solution Selector
Selection algorithm: e.g., greedy, random, or island algorithm
How We Analyze Result
Performance: compare evolved result vs. initial program and SOTA algorithm
Cost-benefit: compute budget, LLM calls, simulation time (survey: was the quality gain worth
the cost?)
Other metrics: robustness, generalization
How
We
Analyze
Evolution
Process
Search trajectory: from initial program to checkpoints to final outputs (examples of key
transitions, what features are added)
Common patterns: where models get stuck, recurring failures
Feedback granularity and utility: scores, constraint violations, trace-level logs; which kinds of
feedback resolve which failure patterns
Table 8: Expanded schema for problem formulation and evaluation of AI-driven algorithm discovery. Each row corresponds
to an element in the setup. /* shu: do we still need this table? */ /* accheng: cite in S5? need to double check */


--- Page 29 ---
Category
Failure Type
Description
Runtime
Errors
Syntax & Interface Errors
Candidate solution fails to compile or integrate with evaluator.
Budget Exhaustion
Candidate exceeds resource limits (e.g., context window, API
quotas, timeouts).
Search
Failures
Premature Convergence
Search settles on a local optimal solution too early.
Stuck-in-the-Loop
Search repeats similar solutions without meaningful progress.
Mutation Drift
Search produces contradicting or random edits to the solution.
Algorithm
Failures
Misaligned Objectives
Solutions ignore key constraints (e.g., latency SLOs).
Sub-Optimal Optimizations
Shallow changes (e.g., API calls) instead of substantive
algorithmic improvement.
Overfitting
Hard-coded / narrow solutions underperform on unseen traces.
Reward Hacking
Solution exploits loopholes in the evaluator rather than solving
intended problem.
Table 9: Common failure patterns in ADRS pipelines (distribution estimated from 420 LLM-judged traces).


--- Page 30 ---
D
Config Files for ADRS Frameworks
We provide the configuration files we use for OpenEvolve, ShinkaEvolve, and GEPA.
D.1
OpenEvolve
# OpenEvolve Island-Based Evolution Configuration
# This configuration demonstrates the proper use of island-based evolution
# General settings
max_iterations: 100
checkpoint_interval: 10
log_level: "INFO"
random_seed: 42
# LLM configuration
llm:
primary_model: "gpt-5"
api_base: "https://api.openai.com/v1"
api_key: ${OPENAI_API_KEY}
primary_model_weight: 1.0
temperature: 0.7
top_p: 0.95
max_tokens: 32000
timeout: 600
# Database configuration with proper island settings
database:
population_size: 100
archive_size: 20
# Island-based evolution settings
num_islands: 5 # Number of separate populations
migration_interval: 5 # Migrate every 50 generations
migration_rate: 0.1 # Migrate 10% of top programs
# Selection parameters
elite_selection_ratio: 0.1
exploration_ratio: 0.3
exploitation_ratio: 0.7
# Note: diversity_metric fixed to "edit_distance"
# Feature map dimensions for MAP-Elites
# Default if not specified: ["complexity", "diversity"]
# Comment out the line below to use the defaults
# feature_dimensions: ["complexity", "diversity"]
feature_bins: 10
# Can also use per-dimension bins:
# feature_bins:
# performance: 20
# correctness: 10
# Prompt configuration
prompt:
num_top_programs: 3
num_diverse_programs: 2
use_template_stochasticity: true
# Evaluator configuration


--- Page 31 ---
evaluator:
timeout: 300
max_retries: 3
cascade_evaluation: false
parallel_evaluations: 4
# Evolution settings
diff_based_evolution: true
allow_full_rewrites: false
max_code_length: 60000
D.2
ShinkaEvolve
evo_config:
_target_: shinka.core.EvolutionConfig
patch_types:
- "diff"
- "full"
- "cross"
patch_type_probs:
- 0.6
- 0.3
- 0.1
num_generations: 100
max_parallel_jobs: 10
max_patch_resamples: 3
max_patch_attempts: 3
llm_models:
- "gemini-2.5-pro"
- "gemini-2.5-flash"
- "gpt-4.1-mini"
- "gpt-4.1-nano"
- "bedrock/us.anthropic.claude-sonnet-4-20250514-v1:0"
- "o4-mini"
llm_dynamic_selection: ucb
llm_kwargs:
temperatures:
- 0.0
- 0.5
- 1.0
max_tokens: 16384
meta_rec_interval: 10
meta_llm_models:
- "gpt-4.1"
meta_llm_kwargs:
temperatures:
- 0.0
embedding_model: "text-embedding-3-small"
results_dir: ${output_dir}
D.3
GEPA
reflection_minibatch_size: 3
max_iterations: 100
reflection_lm: openai/gpt-5 # gemini/gemini-3-pro-preview
