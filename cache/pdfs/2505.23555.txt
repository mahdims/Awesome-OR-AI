--- Page 1 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
1
Adaptive Federated LoRA in Heterogeneous
Wireless Networks with Independent Sampling
Yanzhao Hou, Member, IEEE, Jiaxiang Geng, Student Member, IEEE, Boyu Li, Student Member, IEEE,
Xiaofeng Tao, Senior Member, IEEE, Juncheng Wang, Member, IEEE, Xiaodong Xu, Senior Member, IEEE,
and Bing Luo, Senior Member, IEEE
Abstract—Federated LoRA has emerged as a promising tech-
nique for efficiently fine-tuning large language models (LLMs)
on distributed devices by reducing the number of trainable
parameters. However, existing approaches often inadequately
overlook the theoretical and practical implications of system and
data heterogeneity, thereby failing to optimize the overall training
efficiency, particularly in terms of wall-clock time. In this paper,
we propose an adaptive federated LoRA strategy with indepen-
dent client sampling to minimize the convergence wall-clock time
of federated fine-tuning under both computation and communi-
cation heterogeneity. We first derive a new convergence bound
for federated LoRA with arbitrary and independent client sam-
pling, notably without requiring the stringent bounded gradient
assumption. Then, we introduce an adaptive bandwidth allocation
scheme that accounts for heterogeneous client resources and
system bandwidth constraints. Based on the derived theory, we
formulate and solve a non-convex optimization problem to jointly
determine the LoRA sketching ratios and sampling probabilities,
aiming to minimize wall-clock convergence time. An efficient
and low-complexity algorithm is developed to approximate the
solution. Finally, extensive experiments demonstrate that our
approach significantly reduces wall-clock training time compared
to state-of-the-art methods across various models and datasets.
Index Terms—LLM Fine-tuning, Federated Learning, LoRA,
Client Sampling, Wireless Network.
I. INTRODUCTION
L
ARGE Language Models (LLMs), such as GPT [1], [2]
and LLaMA [3], as well as more recent architectures
[4], [5], [6] have revolutionized natural language processing by
leveraging pre-training on massive and diverse corpora to learn
broad and transferable representations. While these foundation
models demonstrate strong zero-shot and few-shot capabilities
across various downstream tasks, adapting them to specific
user needs or domain-specific applications often requires fine-
tuning, which typically depends on large amounts of data [7].
With the rapid development of 5G, the Internet of Things
Y. Hou, X. Xu and X. Tao are with the National Engineering Research
Center for Mobile Network Technologies, Beijing University of Posts and
Telecommunications, Beijing, 100876, China (e-mail: {houyanzhao, xuxi-
aodong, taoxf}@bupt.edu.cn).
J. Geng and B. Li are with the National Engineering Research Center for
Mobile Network Technologies, Beijing University of Posts and Telecommu-
nications, Beijing, 100876, China (e-mail: {lelegjx, liboyu}@bupt.edu.cn).
J. Wang is with the Hong Kong Baptist University, Hong Kong, China
(e-mail: jcwang@comp.hkbu.edu.hk).
B. Luo is affiliated with Peng Cheng Laboratory. This work was conducted
while he was a Visiting Scholar at Peng Cheng Laboratory (e-mail: luob-
ing1008@gmail.com).
Corresponding author: Yanzhao Hou and Bing Luo
(IoT), and social networking applications, massive volumes of
data are now being generated at the wireless network edge [8].
Traditional centralized fine-tuning of LLMs requires transmit-
ting this edge data to a central server, which raises significant
privacy concerns [9]. Furthermore, transferring such large-
scale data consumes substantial bandwidth, making centralized
approaches both inefficient and impractical in many real-world
scenarios [10].
As an appealing distributed machine learning (DML)
paradigm, Federated Learning (FL) enables multiple local
clients to collaboratively fine-tune a global model without
sharing their raw data [11]. By keeping data decentralized, FL
effectively preserves user privacy and reduces communication
overhead. This approach has gained widespread adoption
across various domains, including computer vision, natural
language processing, e-healthcare, and smart home applica-
tions [12], [13], [14]. However, enabling on-device fine-tuning
of LLMs in federated learning remains challenging [10]. LLMs
typically contain billions of parameters, leading to substantial
computational, communication, and memory overhead during
training. Full-parameter fine-tuning is often beyond the capa-
bilities of edge devices with limited resources [15].
Fortunately, parameter-efficient fine-tuning methods have
been proposed to reduce the computational and storage costs
of fine-tuning [16], [17], [18]. Among the methods, Low-
Rank Adaptation (LoRA) is a widely used approach due to
its flexibility that significantly reduces the number of trainable
parameters by injecting small trainable matrices into specific
layers of the model, which allows the model to be efficiently
adapted to new tasks without modifying the original pre-
trained weights [17]. To enable distributed on-device fine-
tuning of LLMs, recent studies have integrated LoRA with
the FL algorithm, forming a framework commonly referred to
as Federated LoRA [19].
Federated LoRA exhibits two distinctive characteristics.
First, the data distribution across clients is often highly non-
independent and identically distributed (non-i.i.d.) and unbal-
anced, a phenomenon known as data heterogeneity, which
can significantly hinder the convergence performance of FL
algorithms [20]. Second, local clients typically possess varying
levels of communication and computational capabilities, re-
ferred to as system heterogeneity, which leads to the straggler
problem and slows down the overall training process [21]. This
challenge becomes even more pronounced in wireless edge
networks, where limited and shared communication bandwidth
further exacerbates the impact of system heterogeneity [22].
arXiv:2505.23555v3  [cs.LG]  13 Jul 2025


--- Page 2 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
2
A. Related Works
1) FL methods for improving fine-tuning efficiency:
To address the network heterogeneity and improve com-
munication efficiency, FL algorithms such as the standard
FedAvg algorithm typically sample a subset of clients and
perform multiple local model updates [23]. Recent studies
have provided theoretical convergence analysis for FL with
client sampling1 [24], [25], [26]. However, these works have
primarily focused on the data heterogeneity but neglecting
the system heterogeneity, and therefore often exhibit slow
convergence in terms of the wall-clock time. The reason is
that these works may select a straggler under poor wireless
channel condition, resulting in a long per-round time [27].
In addressing both data and system heterogeneity, recent
works [28], [29], [30] have designed client sampling strate-
gies in wireless networks to minimize FL convergence time.
However, the convergence analysis in [28] is only valid for
convex loss function. Furthermore, their sampling probabil-
ities among clients are dependent, and a fixed number of
participating clients are selected in each round. In practice,
some FL clients may experience unexpected disconnections
due to the fluctuation of wireless networks, while others may
be unable to participate in FL training due to their higher-
priority tasks, which make the participation of each client
in FL training independent. Although [29] and [30] studied
independent client sampling, they only considered the com-
munication heterogeneity while neglecting the computation
heterogeneity. As shown in Fig. 1, when FL clients perform
multiple local iterations, computation heterogeneity also has
a significant impact on the training round time, and should
be jointly designed and optimized with the communication
heterogeneity. Moreover, their convergence results rely on a
uniform bounded gradient assumption, which may result in a
loose bound, since the gradient norm usually decays over the
training rounds.
2) Federated LoRA methods:
Federated LoRA is emerging as a promising solution for
collaborative fine-tuning of LLMs across distributed devices
[31]. Recent efforts have extended this framework in var-
ious directions. For instance, differential privacy has been
incorporated to enhance security [32], and communication
compression has been applied to reduce transmission overhead
[33]. Other methods have addressed the challenge of system
heterogeneity by proposing heterogeneous LoRA mechanisms
in [34], [35], [36], [37], [38], [39], allowing devices to use
different LoRA ranks.
However, most of these approaches lack theoretical analysis
and fail to provide a comprehensive understanding of their im-
pact on overall system efficiency, particularly in terms of wall-
clock time. For example, FlexLoRA [34] applies truncated
singular value decomposition (SVD) to align heterogeneous
LoRA modules, but this results in additional computation
and memory overhead, potentially limiting scalability. Het-
1Client sampling here means that all clients will participate in FL through-
out the training rounds, with certain probability of participation in each
round (not to casually sample a fixed subset of clients in each round), which
guarantees the model convergence and unbiasedness.
eroLoRA [35] uses zero-padding to align update dimensions
across devices, but this ad hoc approach introduces redun-
dancy and lacks optimization guarantees. ReplicationLoRA
[36] analyzes the noise error introduced by zero-padding in
HeteroLoRA and proposes a replication strategy to mitigate
it, but it lacks a convergence analysis. FedStackLoRA [37]
stacks LoRA modules across devices, which increases com-
munication cost linearly with the number of participants and
compromises the modularity and flexibility that LoRA was
originally designed to support. LoRA-A2 [38] demonstrates
robustness in challenging settings with low ranks and high
data heterogeneity, but overlooks the impact of system hetero-
geneity on overall system efficiency. FSLoRA [39] proposes
a sketching mechanism to reduce update size, but it does not
address how to optimally choose LoRA sketching ratios under
system heterogeneity to minimize wall-clock time.
Crucially, none of these methods systematically consider the
interplay between LoRA parameters’ sizes, client sampling,
data heterogeneity and system heterogeneity to optimize the
overall fine-tuning efficiency. Despite their empirical benefits,
the absence of theoretical grounding limits their applicability
to practical, large-scale, resource-constrained FL systems.
B. Our Contributions
Motivated by the above discrepancies, in this work, we
propose a new adaptive federated LoRA strategy with inde-
pendent client sampling to minimize the FL convergence wall-
clock time, while considering data heterogeneity and system
heterogeneity in both communication and computation. The
main contributions of this paper are as follows:
• Based on the sketching mechanism, we derive a new
convergence bound for federated LoRA with arbitrary
sketching ratios and independent client sampling prob-
abilities. Our convergence bound holds for general non-
convex loss function without the stringent bounded gra-
dient assumptions.
• We propose an adaptive bandwidth allocation scheme for
Federated LoRA with independent client sampling. Our
proposed approach jointly considers the impacts of het-
erogeneous computation and communication capacities,
as well as the limited system bandwidth, to characterize
the expected wall-clock time for each training round.
• Building upon the derived convergence bound and the
proposed adaptive bandwidth allocation scheme, we for-
mulate an optimization problem on the LoRA sketching
ratios and independent sampling probabilities, to mini-
mize the federated LoRA convergence wall-clock time
with both data and system heterogeneity. We develop an
efficient algorithm to approximately solve the non-convex
optimization problem with low computational complexity.
• We evaluate the performance of our proposed algorithm
under heterogeneous network settings. Our experimen-
tal results under various learning models and datasets
demonstrate that the proposed algorithm substantially
reduces the federated LoRA wall-clock finetuning time
compared with state-of-the-art methods.


--- Page 3 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
3
Fig. 1: A heterogeneous federated LoRA over wireless networks, where, clients with heterogeneous computation and
communication capacities, as well as data distribution.
II. SYSTEM MODEL
A. Federated LoRA Fine-tuning with sketching mechanism
LoRA enables efficient fine-tuning by approximating weight
updates ∆W using a low-rank decomposition ∆W = BA,
where B and A are small trainable matrices that contain
significantly fewer parameters than the original weight matrix
[17]. As illustrated in Fig. 1, we consider a federated LoRA
system operating at the wireless edge, which consists of a
set of N clients indexed by N = {1, . . . , N} and a central
server. The federated LoRA fine-tuning problem can then be
formulated as:
min
B,A F(B, A) =
N
X
n=1
anFn(B, A)
where
Fn(B, A) = Ed∼Dn[L(W0 + BA; d)].
(1)
Here, W0 denotes the frozen (pre-trained) base model. The
LoRA modules are represented by B ∈Rm×γ and A ∈Rγ×n,
which introduce a low-rank update ∆W = BA with rank γ.
Let d denote a single data sample and Dn be the local dataset
on device n. The aggregation weight an = |Dn|
|D| is set as the
data ratio of client n, where |Dn| and |D| represent the local
and global data sizes, such that PN
n=1 an = 1. The global
loss function is denoted by F(·), and the local loss function
of client n is denoted by Fn(·). The sample-level loss function
on client n is denoted by L(·; d).
However, this conventional federated LoRA paradigm typi-
cally assumes a uniform LoRA rank γ across all clients, which
fails to account for system heterogeneity. This design can lead
to significant inefficiencies: clients with limited computational
and communication capabilities may become stragglers due
to the high overhead of computing and transmitting LoRA
updates, thereby prolonging each training round and further
increasing the overall fine-tuning latency. On the other hand,
clients with stronger resources may be underutilized if con-
strained by a small, fixed rank, resulting in insufficient model
adaptation and slower convergence.
To overcome the above inefficiencies, FSLoRA develops a
sketching mechanism that preserves the flexibility of LoRA
while addressing system heterogeneity and resource con-
straints [39]. The corresponding optimization problem can be
formulated as follows:
min
B,A F S(B, A) =
N
X
n=1
anF S
n (B, A)
where
F S
n (B, A) = Ed∼Dn;S∼Sn[L(W0 + BSA; d)].
(2)
Here, S denotes a sketching matrix randomly sampled from
the diagonal matrix set Sn = S(γ, kn). The set S(γ, kn)
comprises diagonal matrices of size γ × γ with exactly kn
non-zero entries. Formally, S(γ, kn) is defined as:
S(γ, kn)=


S|S= γ
kn
X
j∈Γn
ejeT
j , Γn ⊆{1, ..., γ}, |Γn|=kn


,
(3)
where e1, . . . , eγ ∈Rγ are the standard unit basis vectors, and
the set Γn is a random subset of [γ] = {1, 2, . . . , γ} sampled
uniformly from all subsets of [γ] with cardinality kn. With S
being a matrix sampled from Sn, we have:
BSA = γ
kn
X
j∈Γn
BejeT
j A.
(4)
Here, Γn denotes the index set indicating the non-zero
positions in the sketching matrix S. The operation Bej selects
the j-th column of matrix B, while eT
j A retrieves the j-th row
of matrix A. The sketching matrix S satisfies ES∼Sn[S] = Iγ,
with Iγ denoting the identity matrix of dimension γ. This
ensures that the approximated update W0 + BSA remains
an unbiased estimator of the conventional LoRA update
W0 + BA.


--- Page 4 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
4
Given a sketching matrix S, the gradients of the loss
function L(W0 +BSA; d) with respect to the LoRA matrices
B and A are expressed as follows:
∇BL(W0 + BSA; d) = ∇L(W0 + BSA; d)AT ST
∇AL(W0 + BSA; d) = ST BT ∇L(W0 + BSA; d),
(5)
where ∇BL(W0 + BSA; d), ∇AL(W0 + BSA; d) and
∇L(W0 + BSA; d) represent the gradients of L(W0 +
BSA; d) with respect to B, A and W0 +BSA, respectively.
B. Unbiased Federated LoRA with Independent Client Sam-
pling
In practical FL systems, it is often infeasible for every client
device to participate in every training round due to factors
such as limited computational resources or intermittent net-
work connectivity. These decisions are typically independent
across devices. While many existing studies rely on dependent
sampling—assuming a fixed number of clients are sampled
per round—such assumptions are not robust to real-world
scenarios where fewer clients than expected may become
available. In such cases, theoretical guarantees derived under
dependent sampling no longer hold. Therefore, we adopt an
independent sampling model, where each client participates
in each round with an individual probability qn ∈(0, 1],
reflecting long-term participation frequencies.
We adopt the following adaptive aggregation rule in [28],
[29], [30] to ensure the aggregated global model remains
unbiased (see the proof in Appendix A):
Br+1
Ar+1

=
Br
Ar

−
N
X
n=1
anη In
r
qn
H
X
h=1
∇L(W0 +Br,h
n Sr
nAr,h
n ; d).
(6)
Here, we denote [Br; Ar] as the global LoRA matrices at
the r-th communication round, where Br and Ar represent
the shared low-rank decomposition parameters. Each sampled
client performs H local computation iterations per round. Let
h ∈1, . . . , H denote the index of the local iteration. We use
Br,h
n
and Ar,h
n
to represent the local LoRA matrices of client
n at iteration h of communication round r. The sketching
matrix used by client n in round r is denoted as Sr
n, which
remains fixed for that client throughout the round. We consider
a learning rate η > 0. At each communication round r, client n
is selected independently with probability qn via a Bernoulli
trial: In
r ∼Bernoulli(qn), where 0 < qn ≤1 denotes the
participation probability of client n, and q = q1, . . . , qN
represents the full set of sampling probabilities. If client n
participates in round r, then In
r = 1; otherwise, In
r = 0. To
ensure that the aggregated global model remains an unbiased
estimator of the full-sampled scenario under independent client
sampling, each participating client’s uploaded gradient must be
scaled by the inverse of its sampling probability qn [28], [29],
[30]. This implies that when a device is frequently sampled,
its contribution to the global model needs to be appropriately
reduced, while conversely, when a device has a low sampling
probability, its contribution to the global model needs to be
increased.
In Algorithm 1, we summarize the proposed unbiased
federated LoRA framework with independent client sampling.
The detailed procedure of the fine-tuning process is described
as follows:
1) Initialization and Broadcast (Line 2-3): At the be-
ginning of each communication round, the server generates
sketching matrices {Sr
n ∼Sn}N
n=1 for all clients, where Sn
denotes the set of feasible sketching matrices for client n.
These sketch matrices are then transmitted to the correspond-
ing clients. Additionally, the server broadcasts the current
global LoRA modules [Br; Ar] to all clients. The server also
provides each client with a prescribed independent sampling
probability qn to guide participation.
2) Local Fine-tuning (Line 4-14): Each selected client
performs H local fine-tuning iterations using its received
sketch matrix Sr
n. Specifically, in the h-th local update of the
r-th round, client n updates its LoRA parameters Br,h
n
and
Ar,h
n
guided by Sr
n, which can be formulated as:
Br,h+1
n
Ar,h+1
n

=
Br,h
n
Ar,h
n

−η
∇L(W0 + Br,h
n Sr
nAr,h
n ; d)(Ar,h
n )T (Sr
n)T
(Sr
n)T (Br,h
n )T ∇L(W0 + Br,h
n Sr
nAr,h
n ; d)

.
(7)
3) Model Update and Aggregation (Line 15-17): After
local updates, participating clients transmit their total local
gradients of ∆Br
n and ∇Ar
n to the server, which can be
formulated as: 
∆Br,h
n
∆Ar,h
n

=

Br,0
n
Ar,0
n

−

Br,H
n
Ar,H
n

.
(8)
The server aggregates these updates using the unbiased estima-
tor described in Eqn. (6), accounting for the clients’ individual
sampling probabilities qn, and then updates the global LoRA
matrices accordingly.
III. CONVERGENCE ANALYSIS
In this section, we first introduce several assumptions used
throughout the analysis. Then, based on the sketching mech-
anism, we derive a new convergence bound for federated
LoRA under arbitrary sketching ratios and independent client
sampling probabilities.
To simplify the presentation while maintaining correctness,
we define the following notations:
Let ˜L(B, A, d; S) = L(W0+BSA; d) denote the loss func-
tion under a given sketching matrix S. Define ˜Fn(B, A; S) =
Ed∼Dn[ ˜L(B, A, d; S)] as the expected loss on client n given S.
Let F S
n (B, A) = ES∼Sn[ ˜Fn(B, A; S)] denote the expectation
over the sketching distribution. For convenience, we denote
the combined LoRA parameters as X = [B; A], and rewrite
all relevant functions accordingly: F(X), Fn(X), F S(X),
F S
n (X), ˜Fn(X; S), and ˜L(X, d; S). We also define the stochas-
tic gradient ∇L(W0 + Br,h
n Sr
nAr,h
n ; d) as gr,h
n . Lastly, ∥· ∥
denotes the Frobenius norm throughout the analysis.
A. Assumptions
We present a new convergence bound for Algorithm 1
with non-convex loss functions. We make the following mild
assumptions that are commonly adopted in existing FL works
such as [39], [40], [41].


--- Page 5 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
5
Algorithm 1 Unbiased Federated LoRA with Independent
Client Sampling
Input: Sampling probabilities q = {q1, . . . , qN}, sketching
ratios k = {k1, . . . , kN}, initial LoRA parameters [B0; A0],
local iterations H, learning rate η, lora rank γ
Output: Final LoRA parameters [BR; AR]
1: for r = 0 to R −1 do
2:
Server: Generate sketching matrices {Sr
n ∼Sn =
S(γ, kn)}N
n=1
3:
Broadcast [Br; Ar] and {Sr
n}N
n=1 to all clients
4:
for each client n = 1 to N in parallel do
5:
Sample In
r ∼Bernoulli(qn)
6:
if In
r = 1 then
7:
Initialize [Br,0
n , Ar,0
n ] ←[Br, Ar]
8:
for h = 0 to H −1 do
9:
Update [Br,h+1
n
, Ar,h+1
n
] using gradient of
L(W0 + Br,h
n Sr
nAr,h
n ; d)
10:
end for
11:
Compute the total local gradients ∆Br
n, ∆Ar
n
12:
Send
1
qn ∆Br
n,
1
qn ∆Ar
n to the server
13:
end if
14:
end for
15:
Server: Aggregate updates:
16:
Br+1 ←Br −η PN
n=1 anIn
r ·
1
qn ∆Br
n
17:
Ar+1 ←Ar −η PN
n=1 anIn
r ·
1
qn ∆Ar
n
18: end for
Assumption 1. Fn(X) is differentiable and L-smooth, i.e.,
there exists a positive constant L such that ∀X, Y,
∥∇Fn(X) −∇Fn(Y) ≤L∥X −Y∥, ∀n.
Assumption 2. The variance of the gradient ∇X ˜Fn(X; S)
from the sketching matrix S ∼Sn can be bounded as:
ES∼Si∥∇X ˜Fn(X; S) −∇XF S
n (X)∥2 ≤σ2
s, ∀n,
where F S
n (X) = ES∼Si[ ˜Fn(X; S)]. In addition, for a given S,
the variance of the stochastic gradient ∇X ˜L(X, d; S) due to
data sampling d ∼Dn can be bounded as:
Ed∼Dn∥∇X ˜L(X, d; S) −∇X ˜F S
n (X; S)∥2 ≤σ2
g, ∀n,
Assumption 3. The gradient dissimilarity between the
global loss F S(X) and each local loss F S
n (X) satisfies:
∥∇XF S
n (X) −∇XF S(X)∥2 ≤ch∥∇XF S(X)∥2 + σ2
h, ∀n,
where ch > 0 and F S(X) = PN
n=1 anF S
n (X).
B. Convergence Analysis
Based on the above system model and assumptions, we
present the following Theorem 1.
Theorem
1.
When
the
stated
assumptions
are
satisfied,
and
if
the
learning
rate
η
satisfies
η
≤
min
n
1
√
18KHL,
1
6KHLQ√ch+1
o
, where K2 = maxn
n
γ2
k2
n
o
captures
the
worst-case
sketching
compression
ratio,
and Q2
=
max
nPN
n=1
a2
n
qn
o
characterizes the effect of
heterogeneous client sampling probabilities, then Algorithm
1 guarantees the following convergence behavior:
1
R
R−1
X
r=0
E[∥∇XF S(Xr)∥2] ≤4 · F S(X0) −F ∗
ηRH
+ 6ηLsN(σ2
g + σ2
s + σ2
h)
N
X
n=1
a2
n
qn
+ 36L2η2H2N(σ2
g + σ2
s + σ2
h)
N
X
n=1
a2
n
qn
· γ2
k2n
,
(9)
where Ls is the smoothness parameter of ∇F S(X), which is
proportional to L.
Proof: We present a brief proof sketch here and the
complete proof can be found in Appendix B. We first derive
an upper bound for the difference between E[F S(Xr+1)]
and E[F S(Xr)]. This is achieved by considering the local
stochastic gradient descent (SGD) updates, the aggregation
rule, and the L-smoothness of the global loss function F.
Subsequently, we proceed to further upper bound the dif-
ference within the first step. We leverage the independence
between client sampling and the randomness in data sampling
of SGD, employing tools such as Jensen’s inequality and
Young’s inequality. Finally, we sum the inequality derived
in the previous two steps over round r from 0 to R −1,
take the total expectation, and rearrange terms to obtain the
convergence bound.
Note that our convergence bound in (9) holds under ar-
bitrary and independent client sampling probabilities q, i.e.,
PN
n=1 qn
∈
(0, N], and arbitrary sketching ratios k
=
{k1, . . . , kN}, where kn corresponds to the sketching ratio
of each device n. Based on the results in Theorem 1, we can
derive the following two theoretical insights.
1) Insights for sampling probabilities q: We show that
the upper bound on the averaged expected global gradient de-
creases as the independent sampling probabilities increase. In
other words, when all devices have participation probabilities
qn = 1, i.e., full participation, the upper bound reaches its
minimum value, and the required total number of rounds R
to reach a preset convergence threshold ξ is minimized. The
upper bound in (9) further implies that in order to obtain an
unbiased global model, all clients need to participate with a
positive probability for model convergence, i.e., qn ̸= 0, for all
n. This is because when qn →0, it will take infinite number
of rounds R to achieve convergence.
2) Insights for sketching ratios k: The upper bound on
the average expected global gradient norm decreases as the
sketching ratios increase. In other words, when all devices use
the full set of LoRA parameters (i.e., kn = γ for all n), the
bound reaches its minimum, and the number of communication
rounds R required to achieve a target convergence threshold
ξ is minimized. Moreover, the bound in (9) indicates that to
ensure an unbiased global model update, each client must have
a nonzero sketching ratio, i.e., kn > 0.
Remark 1-a (About q): Although increasing qn leads to
faster convergence in terms of the number of communication
rounds R required to reach a target accuracy ξ, it does not
necessarily reduce the total wall-clock convergence time. This


--- Page 6 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
6
is because a higher participation rate per round often results
in lower bandwidth allocation per device, increasing commu-
nication time. Additionally, involving straggler devices—those
with limited computational resources—can further prolong the
round duration.
Remark 1-b (About k): As for the sketching ratio k, a
smaller kn implies fewer parameters are involved in local com-
putation, leading to lower computation and communication
costs per round. Specifically, with fewer parameters to update,
local devices compute faster, and only partial model updates
need to be transmitted, resulting in reduced communication
time. However, smaller sketching ratios generally require more
rounds to achieve convergence.
Remark 1-c (Trade-offs between q and k): Importantly,
q and k interact in nontrivial ways. For instance, a small qn
with a large kn means that client n participates infrequently,
but updates a larger portion of LoRA parameters when it
does. Conversely, a large qn with a small kn implies frequent
participation with minimal updates per round. These trade-offs
create a complex interplay between convergence rate and per-
round latency.
These observations naturally lead us to the following key
question: How can we jointly optimize the independent client
sampling probabilities q and sketching ratios k to minimize
the total wall-clock time needed for the global model to reach
a target convergence threshold?
IV. PROBLEM FORMULATION
To solve the above key question, in this section, we first
introduce the proposed adaptive bandwidth allocation scheme
for FL with independent client sampling and sketching mech-
anism. Then, we present our formulated optimization problem
on the independent sampling probabilities as well as the
sketching ratios to minimize the wall-clock training time,
subject to a convergence threshold constraint.
A. Adaptive Bandwidth Allocation
As shown in Fig. 1, we consider both heterogeneous com-
munication and computation time. For the communication
time, due to the system bandwidth limitation and wireless
interference, we assume that the selected clients are allocated
with orthogonal bandwidths. For the computation time, we
assume it is a constant and is measurable for each client.
Let τn denote the actual local computation time required by
client n to perform local LoRA matrix update under sketching
ratio kn, and let tn denote the actual communication time
required to transmit the LoRA gradients with sketching ratio
kn under unit bandwidth. Additionally, let ˆf (r)
n
represent the
bandwidth allocated to client n during communication round
r. We can show that for any independent sampling probability
q and arbitrary sketching ratios k, a minimum round time
ˆT (r)(q, k) is obtained when the sampled clients complete their
training round r at the same time [28]. The minimum round-
time ˆT (r)(q, k) can be expressed as:
ˆT (r)(q, k) = τn + tn
ˆf (r)
n
, ∀n ∈K(r)(q, k),
(10)
where K(r)(q, k) represents the set of devices participating
in round r under sampling probability q. Note that size
and elements of K(r)(q, k) vary across rounds, making the
allocation of ˆf (r)
n
to minimize the total wall-clock training
time a highly intricate problem. For analytical convenience, we
consider the expected bandwidth f (r)
n
allocation instead and
approximate ˆT (r)(q, k) with T (r)(q, k) under the expected
bandwidth. For a given round r, from (10), the expected total
bandwidth ftot can be expressed as:
ftot =
N
X
n=1
qnf (r)
n
=
N
X
n=1
qn
tn
T (r)(q, k) −τn
.
(11)
The total training time Ttot(q, k, R) after R rounds can there-
fore be expressed as:
Ttot(q, k, R) =
R
X
r=1
T (r)(q, k).
(12)
B. Problem Formulation
Our goal is to optimize the independent client sam-
pling probabilities q and sketching ratios k, to minimize
the expected total training time E[Ttot(q, k, R)], while en-
suring that the average squared norm of global gradient
1
R
PR−1
r=0 E[||∇XF S(Xr)||2] is under a preset convergence
threshold ξ after R rounds. This translates into the following
optimization problem:
P1:
min
q,k,R E[Ttot(q, k, R)]
s.t.
1
R
R−1
X
r=0
E[
∇XF S(Xr)
2] ≤ξ,
R ∈Z+,
(13)
0 < qn ≤1, kn ∈{0, 1, 2, ..., r},
∀n ∈N.
P1 is a joint and non-convex optimization problem and is
difficult to solve. First, the global loss function F S(X) is
generally defined and it is typically impossible to predict how
q, k and R affect the global loss before actually training the
model. Second, the analytical expression of the round time
T (r)(q, k) is not available.
V. OPTIMIZATION OF INDEPENDET SAMPLING AND
SKETCHING MECHANISM
In this section, we develop an efficient algorithm to solve
P1. We first obtain an upper bound on the expected round time
E[T (r)(q, k)]. Then, we formulate an approximate problem
of P1 based on the convergence upper bound in Theorem 1.
Finally, we propose an algorithm to solve the approximate
two-variable problem with low computational complexity.
A. Bounding the Expected Round Time E[T (r)(q, k)]
From (12), if we can obtain the expected round time
E[T (r)(q, k)], multiplying it by the total number of rounds
R will give us an approximation of the total training time, in
the objective of P1. Without loss of generality, we assume that
the N clients are indexed such that their actual computation
times satisfy τ1 ≤τ2 ≤· · · ≤τN.


--- Page 7 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
7
In the following theorem, we provide an upper bound for
E[T (r)(q, k)].
Theorem 2. The expected round time E[T (r)(q, k)] is upper
bounded by
E[T (r)(q, k)] ≤
PN
n=1 qntn
ftot
+ E[max τn],
(14)
where E[max τn] is the expected maximum computation time
among the clients, given by:
E[max τn] = qNτN +
N−1
X
n=1
N
Y
i=n+1
(1 −qi)qnτn ≤
N
X
n=1
qnτn.
(15)
Proof: It can be shown that the probability of client n
being the slowest one is QN
i=n+1(1 −qi)qn. This is because
client n is the slowest implying that clients n+1 to N did not
participate in the training. Due to QN
i=n+1(1 −qi) ≤1, the
upper bound of E[max τn] can be established. Furthermore,
we introduce max τn to find the maximum value of (11):
ftot =
N
X
n=1
qn
tn
T (r)(q, k) −τn
≤
PN
n=1 qntn
T (r)(q, k) −max τn
. (16)
We move ftot to the right side of the equation and T (r)(q, k)
to the left side, then take the expectation on both sides,
resulting in (14).
From the results in Theorem 2, we can derive an upper
bound on E[T (r)(q, k)] as follows:
E[T (r)(q, k)] ≤
N
X
n=1
qn( tn
ftot
+ τn).
(17)
B. Communication and Computation Time Estimation under
Sketching Mechanism
In the above descriptions, we adopt the actual communi-
cation time tn and computation time τn for each client n.
However, these values are generally not available prior to
system deployment. To solve P1 and determine the optimal
sampling probabilities q and sketching ratios k that minimize
the total wall-clock time for fine-tuning, we first introduce
the following estimations for communication and computation
time under the sketching mechanism.
1) Communication Time Estimation: Due to the sketching
mechanism, each client n only uploads the updated LoRA pa-
rameters to the server, which involves transmitting k2
n parame-
ters instead of γ2. Let tγ
n denote the time required for client n
to upload the full LoRA parameters (i.e., kn = γ) under unit
bandwidth. Since tγ
n depends on the client’s communication
capability, it can be measured before the fine-tuning process.
Then, the estimated communication time tn for sketching ratio
kn can be expressed as:
tn = tγ
n · k2
n
γ2 .
(18)
2) Computation Time Estimation: Similarly, client n only
needs to update k2
n LoRA parameters locally under sketching,
instead of γ2. Let τ γ
n be the time needed for client n to perform
full LoRA updates (kn = γ), which reflects its computation
capability and can also be measured prior to training. Follow-
ing the estimation approach in [42], the computation time τn
for a sketching ratio kn is estimated as:
τn = τ γ
n · k2
n
γ2
(19)
Therefore, the upper bound in (17) can be rewritten using
the measurable parameters τ γ
n and tγ
n as follows:
E[T (r)(q, k)] ≤
N
X
n=1
k2
n
γ2 qn
 tγ
n
ftot
+ τ γ
n

.
(20)
C. Approximate Optimization Problem for P1
As a result, we can approximate the optimal solution to
P1 by minimizing an upper bound of the problem with
Theorem 1 and 2. To simplify the expression, we denote
some constant parameters using symbols A, B, C, and D,
where A =
4(F S(X0)−F ∗)
ηNH(σ2g+σ2s+σ2
h) > 0, B =
ξ
N(σ2g+σ2s+σ2
h) > 0,
C = 6ηLs > 0 and D = 36L2η2H2 > 0, respectively.
Thus the approximated optimal solution can be expressed
as follows:
P2: min
q,k
A
B−CPN
n=1
a2n
qn −DPN
n=1
a2n
qn
γ2
k2n
·
N
X
n=1
k2
n
γ2 qn( tn
ftot
+τn)
s.t.
a2
n(C + DK2)N
B
< qn ≤1,
kn ∈{0, ..., γ},
K2 = maxn{γ2
k2n
},
∀n ∈N.
(21)
Here,
we
additionally
require
one
condition:
a2
n(C+DK2)N
B
< qn ≤1 to ensure that the denominator
of P2 is greater than zero (see the proof in appendix C). If
we recall this condition when the data is evenly distributed
among all devices, i.e., an =
1
N , the condition simplifies to
C+DK2
NB
< qn ≤1.
Remark 2: Here, we impose a tighter lower bound con-
straint on the sampling probabilities qn for each device, which
reveals the inherent relationship between sampling probabili-
ties and sketching ratios as well as the relationship between
sampling probabilities and data heterogeneity. 1) Relationship
between sampling probabilities and sketching ratios: The
selection of q and k cannot be arbitrary—otherwise, the fine-
tuning process may fail to converge. When the sketching ratio
kn of client n is small, only a limited portion of its local LoRA
matrix parameters participate in local updates. In this case, the
sampling probability qn must be larger than a threshold that
is inversely proportional to kn, so that the client participates
more frequently to ensure sufficient global model updates
and guarantee convergence. Conversely, when kn is large, the
lower bound on qn becomes smaller, meaning that even with
a lower participation frequency, the client can still contribute
effectively to the global model and convergence can still be


--- Page 8 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
8
Algorithm 2 Approximately Optimal Independent Sampling
and Sketching Mechanism for Federated LoRA with Data and
System Heterogeneity
Input: Number of clients N; measured computation time τ γ
n,
measured communication time tγ
n for each client n; total
bandwidth ftot; aggregation weight an = |Dn|/|D|; initial
global model X0; loss target Fs; step size ϵ; maximum LoRA
rank γ; initial sketching ratio vector k0
Output: Approximate optimal sampling probabilities q∗and
sketching ratios k∗
1: Parameter Estimation Phase:
2: Server runs Algorithm 1 with four different pre-defined
configurations: {q1, k1}, {q2, k2}, {q3, k3}, {q4, k4};
3: Server
records
the
required
communication
rounds
R1, R2, R3, R4 until reaching target loss Fs;
4: Compute A, B, C, D by solving (23);
5: Alternating Optimization Phase:
6: repeat
7:
Step 1: Optimize q with fixed k (Initial k0 )
8:
for M(ϵ)=Mmin, Mmin+ϵ, Mmin+2ϵ, . . . , Mmax do
9:
Substitute parameters N, τn, tn, ftot, an, M(ϵ), A,
B, C, D, γ, k into P3;
10:
Solve P3 using CVX to obtain q∗(M(ϵ));
11:
end for
12:
Step 2: Optimize k with fixed q (Greedy heuristic)
13:
Initialize kn ←γ for all n ∈N;
14:
repeat
15:
updated ←False;
16:
for each client n ∈N do
17:
if kn > 1 and reducing kn decreases P2 then
18:
kn ←kn −1;
19:
updated ←True;
20:
end if
21:
end for
22:
until updated = False
23: until convergence of q and k
24: return q∗, k∗
achieved. 2) Relationship between sampling probabilities and
data heterogeneity: If a device possesses a larger amount of
data (larger an), the lower bound for participation probability
needs to be higher.
D. Optimization Algorithm for q and k
Solving P2 poses two main challenges. The first lies in
the presence of unknown parameters, namely A, B, C and
D, which are required to evaluate the convergence condition.
The second challenge arises from the bi-variable non-convex
nature of P2, where both the sampling probabilities q and
the sketching ratios k are interdependent decision variables,
making the optimization problem particularly difficult to solve.
1) Estimate the unknown parameters:
We first propose a method to estimate the unknown pa-
rameters A, B, C, and D. This approach leverages the
convergence upper bound derived in Theorem 1 and in-
volves running Algorithm 1 with four sets of fixed sam-
pling probabilities qn and fixed sketching ratios kn as
{q1, k1}, {q2, k2}, {q3, k3}, {q4, k4}. These known param-
eter sets can be arbitrarily chosen within the feasible range.
Within each set, all clients are configured with identical
parameters to ensure consistency. The values of k and q should
differ between sets.
We only run a limited number of rounds to reach a prede-
fined estimation loss Fs instead the convergence threshold ξ.
Let’s assume that R1, R2, R3, R4 rounds are required to reach
the loss Fs with {q1, k1}, {q2, k2}, {q3, k3}, {q4, k4},
respectively. Additionally, denote the sum PN
n=1
a2
n
qn as Y1,
Y2, Y3, Y4 under {q1, k1}, {q2, k2}, {q3, k3}, {q4, k4},
respectively. We also denote the sum PN
n=1
a2
n
qn
k2
n
γ2 as Z1,
Z2, Z3, Z4 under {q1, k1}, {q2, k2}, {q3, k3}, {q4, k4},
respectively. As a result, we obtain four sets of observa-
tions {R1, Y1, Z1}, {R2, Y2, Z2}, {R3, Y3, Z3}, {R4, Y4, Z4}.
By substituting these into R =
A
B−CY −DZ ., we can obtain
a following system:
M · x = 0,
(22)
where x = [A, B, C, D]T , and M ∈R4×4 is constructed as:
M =




1
R1
−1
Y1
Z1
1
R1
−1
Y2
Z2
1
R3
−1
Y3
Z3
1
R4
−1
Y4
Z4



.
(23)
This can be solved to estimate the unknown parameters A,
B, C, and D using Singular Value Decomposition (SVD).
Specifically, we perform SVD on M as: M = UΣV T , where
Σ is a diagonal matrix containing the singular values. The
right singular vector corresponding to the smallest singular
value in Σ provides an approximate solution x in the least-
squares sense. The overall estimation process corresponds to
lines 1–4 of Algorithm 2.
2) Solve q and k:
Next, we propose an Alternating Minimization Algorithm to
solve the bi-variable non-convex problem. Firstly, with k fixed,
we solve for q, e.g., setting an arbitrary initial value (such as
kn = γ). With k fixed, problem P2 becomes a single-variable
optimization over q. We decompose the objective of P2 into
two multipliers:
A
B−PN
n=1(C+D γ2
k2n )
a2n
qn
and PN
n=1
k2
n
γ2 qn( tn
ftot +
τn). We treat the latter term as a control variable M, and use
a convex function to approximate the former term. Define the
control variable M as:
M =
N
X
n=1
k2
n
γ2 qn( tn
ftot
+ τn).
(24)
where Mmin = N·( min kn
max kn )2 min( tn
ftot +τn) ≤M ≤Mmax =
N · max( tn
ftot + τn), for ∀n ∈N. The first term in P2
can be convert to a convex function using the property that


--- Page 9 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
9
the harmonic mean is no greater than the arithmetic mean,
given by:
A
B −PN
n=1(C + D γ2
k2n ) a2n
qn
=
N · A
(B −(C + D γ2
k2
1 ) a2
1N
q1 ) + ... + (B −(C + D γ2
k2n ) a2
NN
qN )
≤
N
X
n=1
A · qn
NBqn −a2nN 2(C + D γ2
k2n )
.
(25)
The equality holds if and only if
A·qi
NBqi−a2
i N2(C+D γ2
k2
i
) =
A·qj
NBqj−a2
jN2(C+D γ2
k2
j
), ∀i, j
∈N, i ̸= j. Note that when
a2
n(C+DK2)N
B
< qn ≤1, ∀n ∈N, the right-hand side of (25)
is a convex function about q.
Given fixed k, using (24) and (25), we convert P2 to the
following optimization problem to solve for q:
P3:
min
q
N
X
n=1
A · qn
NBqn −a2nN 2(C + D γ2
k2n )
M
s.t.
M =
N
X
n=1
k2
n
γ2 qn( tn
ftot
+ τn),
a2
n(C + DK2)N
B
< qn ≤1,
K2 = maxn{γ2
k2n
},
∀n ∈N.
(26)
For any fixed feasible value of M
∈[Mmin, Mmax],
P3 is convex in q. Thus, we can solve P3 with a convex
optimization tool, such as CVX, while tackling the problem
by employing a linear search approach with a constant step
size of ϵ over the range [Mmin, Mmax][28]. This optimization
process corresponds to lines 7-11 of the proposed Algorithm 2.
Then, with q fixed, we solve for k. Since kn ∈{1, 2, . . . , γ}
is an integer variable, an exhaustive search over all possi-
ble combinations would be computationally expensive, espe-
cially when N is large. To improve efficiency, we adopt a
greedy heuristic algorithm that iteratively adjusts each kn in
a coordinate-wise manner. Starting from an initial assignment
(e.g., kn = γ), we gradually reduce each kn by one if it
leads to a lower objective value, and repeat this process until
no further improvement is observed. This approach provides
a tractable and effective method to approximate the optimal
solution of P2. Therefore, q and k can be solved iteratively
until convergence. This optimization process corresponds to
lines 12-22 of the proposed Algorithm 2.
VI. EXPERIMENTS
In this section, we present experiments to evaluate the
performance of our proposed method. All experiments are
conducted on a computing cluster equipped with NVIDIA
A800 GPUs, each with 80 GB of memory. The number of par-
ticipating mobile devices is set to 50, and the total bandwidth
in the simulated wireless communication environment is set
(a) ARC-c
(b) ARC-e
(c) OBQA
(d) SIQA
Fig. 2: Performance analysis of the optimization of q.
to 100 Mbps. Our model is based on Qwen2.5-1.5B-Instruct,
which we fine-tune and evaluate on the Commonsense170K
dataset. This dataset includes four commonsense reasoning
question-answering tasks: ARC-c, ARC-e, OBQA, and SIQA.
We use a stochastic gradient descent (SGD) batch size of 4,
with each client performing 10 local iterations per round. The
data on each device is non-i.i.d., generated according to a
Dirichlet distribution with a concentration parameter of 0.1.
A. Performance Analysis of the Optimization of q
We compare the performance of our proposed method with
four benchmark client sampling strategies: (i) full sampling,
where each client is selected with probability qn = 1; (ii) fixed
sampling, with a predefined sampling probability of qn = 0.2
for all clients; (iii) uniform sampling, where qn = 1/N for
all n; and (iv) weighted sampling, where qn = |Dn|/|D|
is proportional to the size of each client’s local dataset. In
all strategies, clients are sampled independently. In this set
of experiments, we fix the LoRA sketching ratio for each
client as kn = γ. We perform multiple runs and report the
average performance. The experimental results are presented
in Fig. 2. Our analysis focuses on wall-clock time, with all
time measurements reported in hours.
From the experimental results, we observe that our proposed
sampling strategy achieves the shortest wall-clock time to
reach model convergence across all tasks. In contrast, the full
sampling strategy suffers from severe straggler effects, as it
requires the participation of all clients in every round. This
significantly prolongs the duration of each communication
round, resulting in the longest overall fine-tuning latency
among all methods. Fixed and uniform sampling strategies as-
sign identical participation probabilities to all clients, without
considering either data or system heterogeneity. Consequently,
their training efficiency is limited. Although weighted sam-
pling takes into account the distribution of data volumes across


--- Page 10 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
10
(a) ARC-c
(b) ARC-e
(c) OBQA
(d) SIQA
Fig. 3: Performance analysis of the optimization of k.
clients, it neglects both the feature heterogeneity of the data
and the system heterogeneity, leading to suboptimal efficiency.
As each client performs full LoRA updates, all the sampling
methods eventually achieve similar final accuracies2. However,
thanks to the optimization of the sampling probabilities q, our
method reaches convergence in the shortest overall wall-clock
time.
B. Performance Analysis of the Optimization of k
We compare our proposed method with three different
LoRA sketching ratio strategies: (i) full rank, where each
client uses a sketching ratio of kn = γ, meaning all LoRA
parameters are involved in computation and communication;
(ii) normal rank, where the sketching ratios kn ∼N(µ, σ2) are
sampled independently from a truncated normal distribution
on the closed interval [0, γ], where the mean and variance are
selected to ensure values lie within the interval; (iii) uniform
rank, where the sketching ratios kn ∼U(0, γ) are sampled
independently from a uniform distribution over the interval
[0, γ]. In this experiment, the independent sampling probability
of each client is fixed at qn = 0.2. We conduct multiple runs
and report the average performance. The experimental results
are presented in Fig. 3.
From the experimental results, we observe that our proposed
method consistently outperforms all other sketching ratio
strategies across different datasets, demonstrating the effective-
ness of our optimization approach for sketching ratios. First,
our method achieves the best fine-tuning efficiency, requiring
the shortest wall-clock time to reach convergence. Second, by
fully considering data heterogeneity, our method attains higher
final accuracy than both the normal rank and uniform rank
2Notably, the full sampling method requires significantly more time to
converge and thus its final performance is not fully shown within the time
range of the figure.
TABLE I: Overall fine-tuning latency comparison.
Fine-tuning Task
ARC-e
ARC-c
OBQA
SIQA
Target Accuracy
80%
69%
60%
60%
Overall Fine-tuning Time to Target Accuracy (minutes)
HeteroLoRA [35]
6.22
5.93
7.83
7.12
FedStackLoRA [37]
3.60
3.43
4.55
4.11
FSLoRA [39]
3.35
3.20
4.23
3.83
Proposed
1.20
0.77
1.07
0.95
baselines. Although the full rank strategy achieves the highest
final accuracy, it suffers from significantly longer convergence
time, leading to a high fine-tuning cost in practical deploy-
ments3.
C. Comparison with State-Of-The-Art Methods
In this set of experiments, we compare our proposed method
with three state-of-the-art approaches: HeteroLoRA [35], Fed-
StackLoRA [37], and FSLoRA [39]. For each task, we define
a target accuracy: 80% for ARC-e, 69% for ARC-c, 60%
for OBQA, and 60% for SIQA. We evaluate the fine-tuning
efficiency of each method by measuring the overall wall-clock
time required to reach the corresponding target accuracy.
As shown in the results, our method consistently outper-
forms all baselines in terms of fine-tuning efficiency. Notably,
compared to FSLoRA, our approach achieves speedups of
2.8×, 4.2×, 4.0×, and 4.0× on ARC-e, ARC-c, OBQA, and
SIQA, respectively. This significant improvement is attributed
to our joint optimization of independent sampling and LoRA
sketching ratios, which effectively accounts for both data
heterogeneity and system heterogeneity.
VII. CONCLUSION
In this paper, we propose an adaptive Federated LoRA
strategy with independent client sampling to minimize the
convergence wall-clock time of federated fine-tuning under
both computation and communication heterogeneity. We first
derive a new convergence bound for Federated LoRA with
arbitrary and independent client sampling. Then, we introduce
an adaptive bandwidth allocation scheme that accounts for het-
erogeneous client resources and system bandwidth constraints.
Building upon the convergence bound, we formulate and solve
a non-convex optimization problem to jointly determine the
LoRA sketching ratios and sampling probabilities, aiming to
minimize wall-clock convergence time. We further uncover the
intricate relationships between independent sampling probabil-
ities, sketching ratios, and both system and data heterogene-
ity. To address the resulting bi-variable non-convex problem,
we develop an efficient algorithm with low computational
complexity. Finally, extensive experiments demonstrate that
our approach significantly reduces wall-clock training time
compared to state-of-the-art methods across various models
and datasets.
3The convergence time for full rank is too long to be displayed in the figure.


--- Page 11 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
11
REFERENCES
[1] Alec Radford, Narasimhan K, Salimans T, and Sutskever I. Improving
language understanding by generative pre-training.
Technical report,
OpenAI Technical Report, 2018.
[2] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
and Ilya Sutskever.
Language models are unsupervised multitask
learners. OpenAI blog, 1(8):9, 2019.
[3] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-
Anne Lachaux, Timoth´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric
Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. Llama: Open and efficient foundation
language models, 2023.
[4] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv
preprint arXiv:2303.08774, 2023.
[5] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai,
Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo
Wang, et al. Gemini 1.5: Unlocking multimodal understanding across
millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.
[6] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda
Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al.
Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024.
[7] Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. How to fine-tune
bert for text classification? In Chinese computational linguistics: 18th
China national conference, CCL 2019, Kunming, China, October 18–20,
2019, proceedings 18, pages 194–206. Springer, 2019.
[8] Mung Chiang and Tao Zhang. Fog and iot: An overview of research
opportunities. IEEE Internet of things journal, 3(6):854–864, 2016.
[9] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated
machine learning: Concept and applications.
ACM Transactions on
Intelligent Systems and Technology (TIST), 10(2):1–19, 2019.
[10] Tao Fan, Hanlin Gu, Xuemei Cao, Chee Seng Chan, Qian Chen, Yiqiang
Chen, Yihui Feng, Yang Gu, Jiaxiang Geng, Bing Luo, Shuoling Liu,
Win Kent Ong, Chao Ren, Jiaqi Shao, Chuan Sun, Xiaoli Tang, Hong Xi
Tae, Yongxin Tong, Shuyue Wei, Fan Wu, Wei Xi, Mingcong Xu,
He Yang, Xin Yang, Jiangpeng Yan, Hao Yu, Han Yu, Teng Zhang,
Yifei Zhang, Xiaojin Zhang, Zhenzhe Zheng, Lixin Fan, and Qiang
Yang. Ten challenging problems in federated foundation models. IEEE
Transactions on Knowledge and Data Engineering, pages 1–20, 2025.
[11] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas.
Communication-efficient learning of deep
networks from decentralized data. In Artificial intelligence and statistics,
pages 1273–1282. PMLR, 2017.
[12] Oge Marques and Oge Marques. Machine learning with core ml. Image
Processing and Computer Vision in iOS, pages 29–40, 2020.
[13] Theodora S Brisimi, Ruidi Chen, Theofanie Mela, Alex Olshevsky,
Ioannis Ch Paschalidis, and Wei Shi. Federated learning of predictive
models from federated electronic health records. International journal
of medical informatics, 112:59–67, 2018.
[14] Jiaxiang Geng, Beilong Tang, Boyan Zhang, Jiaqi Shao, and Bing Luo.
Demo: Fedcampus: A real-world privacy-preserving mobile application
for smart campus via federated learning & analytics. In Proceedings
of the Twenty-Fifth International Symposium on Theory, Algorithmic
Foundations, and Protocol Design for Mobile Networks and Mobile
Computing, MobiHoc ’24, page 377–378, New York, NY, USA, 2024.
Association for Computing Machinery.
[15] Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang,
Tong Yu, Guoyin Wang, and Yiran Chen.
Towards building the
federatedgpt: Federated instruction tuning. In ICASSP 2024 - 2024 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pages 6915–6919, 2024.
[16] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale
for parameter-efficient prompt tuning. CoRR, abs/2104.08691, 2021.
[17] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large
language models. CoRR, abs/2106.09685, 2021.
[18] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous
prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and
Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of
the Association for Computational Linguistics and the 11th Interna-
tional Joint Conference on Natural Language Processing (Volume 1:
Long Papers), pages 4582–4597, Online, August 2021. Association for
Computational Linguistics.
[19] Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin
Du, Yanfeng Wang, and Siheng Chen.
Openfedllm: Training large
language models on decentralized private data via federated learning.
In Proceedings of the 30th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining, KDD ’24, page 6137–6147, New York,
NY, USA, 2024. Association for Computing Machinery.
[20] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua
Zhang. On the convergence of fedavg on non-iid data. arXiv preprint
arXiv:1907.02189, 2019.
[21] Takayuki Nishio and Ryo Yonetani.
Client selection for federated
learning with heterogeneous resources in mobile edge. In ICC 2019-
2019 IEEE international conference on communications (ICC), pages
1–7. IEEE, 2019.
[22] Bing Luo, Xiang Li, Shiqiang Wang, Jianwei Huang, and Leandros
Tassiulas. Cost-effective federated learning in mobile edge networks.
IEEE Journal on Selected Areas in Communications, 39(12):3606–3621,
2021.
[23] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas.
Communication-efficient learning of deep
networks from decentralized data. In Artificial intelligence and statistics,
pages 1273–1282. PMLR, 2017.
[24] Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local
descent methods in federated learning. arXiv preprint arXiv:1910.14425,
2019.
[25] Wenlin Chen, Samuel Horvath, and Peter Richtarik.
Optimal client
sampling for federated learning. arXiv preprint arXiv:2010.13723, 2020.
[26] Elsa Rizk, Stefan Vlaski, and Ali H Sayed. Federated learning under im-
portance sampling. IEEE Transactions on Signal Processing, 70:5381–
5396, 2022.
[27] Bing Luo, Xiang Li, Shiqiang Wang, Jianwei Huang, and Leandros
Tassiulas. Cost-effective federated learning design. In IEEE INFOCOM
2021 - IEEE Conference on Computer Communications, pages 1–10,
2021.
[28] Bing Luo, Wenli Xiao, Shiqiang Wang, Jianwei Huang, and Leandros
Tassiulas. Tackling system and statistical heterogeneity for federated
learning with adaptive client sampling. In IEEE INFOCOM 2022-IEEE
conference on computer communications, pages 1739–1748.
[29] Wen Xu, Ben Liang, Gary Boudreau, and Hamza Sokun. Probabilistic
client sampling and power allocation for wireless federated learning. In
2023 IEEE 34th Annual International Symposium on Personal, Indoor
and Mobile Radio Communications (PIMRC), pages 1–6. IEEE, 2023.
[30] Jake Perazzone, Shiqiang Wang, Mingyue Ji, and Kevin S Chan.
Communication-efficient device scheduling for federated learning using
stochastic optimization. In IEEE INFOCOM 2022-IEEE Conference on
Computer Communications, pages 1449–1458. IEEE, 2022.
[31] Chaochao Chen, Xiaohua Feng, Yuyuan Li, Lingjuan Lyu, Jun Zhou,
Xiaolin Zheng, and Jianwei Yin. Integration of large language models
and federated learning, 2024.
[32] Youbang Sun, Zitao Li, Yaliang Li, and Bolin Ding. Improving loRA
in privacy-preserving federated learning. In The Twelfth International
Conference on Learning Representations, 2024.
[33] Kevin Kuo, Arian Raje, Kousik Rajesh, and Virginia Smith. Federated
lora with sparse communication, 2024.
[34] Jiamu Bai, Daoyuan Chen, Bingchen Qian, Liuyi Yao, and Yaliang Li.
Federated fine-tuning of large language models under heterogeneous
tasks and client resources. In The Thirty-eighth Annual Conference on
Neural Information Processing Systems, 2024.
[35] Yae Jee Cho, Luyang Liu, Zheng Xu, Aldi Fahrezi, and Gauri Joshi.
Heterogeneous LoRA for federated fine-tuning of on-device foundation
models.
In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen,
editors, Proceedings of the 2024 Conference on Empirical Methods
in Natural Language Processing, pages 12903–12913, Miami, Florida,
USA, November 2024. Association for Computational Linguistics.
[36] Yuji Byun and Jaeho Lee. Towards federated low-rank adaptation of
language models with rank heterogeneity.
In Luis Chiruzzo, Alan
Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the
Nations of the Americas Chapter of the Association for Computational
Linguistics: Human Language Technologies (Volume 2: Short Papers),
pages 356–362, Albuquerque, New Mexico, April 2025. Association for
Computational Linguistics.
[37] Ziyao Wang, Zheyu Shen, Yexiao He, Guoheng Sun, Hongyi Wang,
Lingjuan Lyu, and Ang Li. Flora: Federated fine-tuning large language
models with heterogeneous low-rank adaptations.
In A. Globerson,
L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang,
editors, Advances in Neural Information Processing Systems, volume 37,
pages 22513–22533. Curran Associates, Inc., 2024.


--- Page 12 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
12
[38] Jabin Koo, Minwoo Jang, and Jungseul Ok. Towards robust and efficient
federated low-rank adaptation with heterogeneous clients, 2024.
[39] Wenzhi Fang, Dong-Jun Han, Liangqi Yuan, Seyyedali Hosseinalipour,
and Christopher G. Brinton.
Federated sketching lora: On-device
collaborative fine-tuning of large language models, 2025.
[40] Wenzhi Fang, Dong-Jun Han, Evan Chen, Shiqiang Wang, and Christo-
pher G. Brinton. Hierarchical federated learning with multi-timescale
gradient correction. In A. Globerson, L. Mackey, D. Belgrave, A. Fan,
U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural In-
formation Processing Systems, volume 37, pages 78863–78904. Curran
Associates, Inc., 2024.
[41] Wenzhi Fang, Ziyi Yu, Yuning Jiang, Yuanming Shi, Colin N. Jones, and
Yong Zhou. Communication-efficient stochastic zeroth-order optimiza-
tion for federated learning. IEEE Transactions on Signal Processing,
70:5058–5073, 2022.
[42] Yuang Jiang, Shiqiang Wang, V´ıctor Valls, Bong Jun Ko, Wei-Han
Lee, Kin K. Leung, and Leandros Tassiulas. Model pruning enables
efficient federated learning on edge devices.
IEEE Transactions on
Neural Networks and Learning Systems, 34(12):10374–10386, 2023.
APPENDIX A
PROOF OF UNBIASED AGGREGATION WITH INDEPENDENT
SAMPLING
In this section, we prove that the aggregation method
with independent sampling in (6) is unbiased. We denote
the parameter matrix as X = [B; A], and define the local
stochastic gradient as gr,h
n
= ∇L(W0+Br,h
n Sr
nAr,h
n ; d). Thus,
given Xr, we can get the expectation of Xr+1 as follows:
Xr+1 = Xr −
N
X
n=1
ηan In
r
qn
H−1
X
h=0
gr,h
n
E[Xr+1] = Xr +
N
X
n=1
an qn
qn (Xr,H
n
−Xr)
= Xr +
N
X
n=1
anXr,H
n
−
N
X
n=1
anXr
=
N
X
n=1
anXr,H
n
.
(27)
The last term in the equation corresponds to aggregating the
local updates uploaded by the clients to the server, weighted
by an. Therefore, we can conclude that (6) is unbiased.
APPENDIX B
PROOF OF THEOREM 1
Following Appendix D.4 in [39], we can obtain that F S(X)
is Ls-smooth, where Ls is proportional to L. Thus, we have:
E[F S(Xr+1)] ≤E[F S(Xr)]
−E[< ∇XF S(Xr),
N
X
n=1
ηan In
r
qn
H−1
X
h=0
gr,h
n
>]
+ Lsη2
2
E[||
N
X
n=1
an In
r
qn
H−1
X
h=0
gr,h
n ||2]
(28)
Then, we apply Jensen’s inequality to simplify the second
term on the right-hand side of the inequality, resulting in:
−E[< ∇XF S(Xr),
N
X
n=1
ηan In
r
qn
H−1
X
h=0
gr,h
n
>]
= −ηHE[< ∇XF S(Xr), N
NH
N
X
n=1
an In
r
qn
H−1
X
h=0
∇XF S
n (Xr,h
n ) >]
=−ηH
2 E[||∇XF S(Xr)||2]−η
2H E[||
N
X
n=1
an In
r
qn
H−1
X
h=0
∇XF S
n (Xr,h
n )||2]
+ ηH
2 E[||∇XF S(Xr) −
N
NH
N
X
n=1
an In
r
qn
H−1
X
h=0
∇XF S
n (Xr,h
n )||2]
=−ηH
2 E[||∇XF S(Xr)||2]−η
2H E[||
N
X
n=1
an In
r
qn
H−1
X
h=0
∇XF S
n (Xr,h
n )||2]
+ ηN 2H
2
E[||
1
HN
N
X
n=1
H−1
X
h=0
an En
r
qn ∇XF S
n (Xr)
−
1
NH
N
X
n=1
H−1
X
h=0
an In
r
qn ∇XF S
n (Xr,h
n )||2]
≤−ηH
2 E[||∇XF S(Xr)||2]−η
2H E[||
N
X
n=1
an In
r
qn
H−1
X
h=0
∇XF S
n (Xr,h
n )||2]
+ ηN
2
N
X
n=1
H−1
X
h=0
a2
n
qn E[||∇XF S
n (Xr) −∇XF S
n (Xr,h
n )||2]
≤−ηH
2 E[||∇XF S(Xr)||2]−η
2H E[||
N
X
n=1
an In
r
qn
H−1
X
h=0
∇XF S
n (Xr,h
n )||2]
+ ηN
2
N
X
n=1
H−1
X
h=0
a2
n
qn L2 γ2
k2n
E[||Xr −Xr,h
n ||2],
(29)
where the last inequality follows from Ls-smoothness
∥∇XF S
n (X) −∇XF S
n (Y)∥≤L ·
γ
kn ∥X −Y∥, which has
been proven in Appendix D.4 of [39].
Moreover,
E[||
N
X
n=1
an In
r
qn
H−1
X
h=0
gr,h
n ||2]
= E[||
N
X
n=1
an In
r
qn
H−1
X
h=0
gr,h
n
−
N
X
n=1
an In
r
qn
H−1
X
h=0
∇X ˜Fn(Xr,h
n ; S)
+
N
X
n=1
anIr
n
qn
H−1
X
h=0
∇X ˜Fn(Xr,h
n ; S)−
N
X
n=1
anIn
r
qn
H−1
X
h=0
∇XF S
n (Xr,h
n )
+
N
X
n=1
an In
r
qn
H−1
X
h=0
∇XF S
n (Xr,h
n )||2]
≤3E[||
N
X
n=1
H−1
X
h=0
an In
r
qn (gr,h
n
−∇X ˜Fn(Xr,h
n ; S))||2]
+ 3E[||
N
X
n=1
H−1
X
h=0
an In
r
qn (∇X ˜Fn(Xr,h
n ; S) −∇XF S
n (Xr,h
n ))||2]
+ 3E[||
N
X
n=1
H−1
X
h=0
an In
r
qn ∇XF S
n (Xr,h
n )||2]
≤3NH(σ2
g +σ2
s)
N
X
n=1
a2
n
qn +3E[||
N
X
n=1
H−1
X
h=0
anIn
r
qn∇XF S
n (Xr,h
n )||2].
(30)


--- Page 13 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
13
When η ≤
1
√
18KHL, substituting the above two terms into
(28) yields:
E[F S(Xr+1)] ≤E[F S(Xr)] −ηH
2 E[||∇XF S(Xr)||2]
+ 3
2η2LsHN(σ2
g + σ2
s)
N
X
n=1
a2
n
qn
+ 1
2ηL2N
N
X
n=1
a2
n
qn
γ2
k2n
H−1
X
h=0
E[||Xr −Xr,h
n ||2].
(31)
Here, we set:
T =
N
X
n=1
a2
n
qn
γ2
k2n
H−1
X
h=0
E[||Xr −Xr,h
n ||2]
=
N
X
n=1
a2
n
qn
γ2
k2n
H−1
X
h=0
E[||η
h−1
X
µ=0
gr,µ
n ||2]
≤
N
X
n=1
a2
n
qn
γ2
k2n
η2
H−1
X
h=0
(3E[||
h−1
X
µ=0
(gr,µ
n
−∇X ˜Fn(Xr,µ
n ; S))||2]
+ 3E[||
h−1
X
µ=0
(∇X ˜Fn(Xr,µ
n ; S) −∇XF S
n (Xr,µ
n ))||2]
+ 3E[||
h−1
X
µ=0
∇XF S
n (Xr,µ
n )||2])
≤
N
X
n=1
a2
n
qn
γ2
k2n
η2
H−1
X
h=0
(3H(σ2
g + σ2
s)+3E[||
h−1
X
µ=0
∇XF S
n (Xr,µ
n )||2])
≤
N
X
n=1
a2
n
qn
γ2
k2n
η2
H−1
X
h=0
(3H(σ2
g + σ2
s)+3h
h−1
X
µ=0
(3E[||∇XF S
n(Xr,µ
n )
−∇XF S
n (Xr)||2] + 3E[||∇XF S
n (Xr) −∇XF S(Xr)||2]
+ 3E[||∇XF S(Xr)||2]))
≤(3η2H2σ2
g + 3η2H2σ2
s + 9η2H3σ2
h)
N
X
n=1
a2
n
qn
γ2
k2n
+ 9η2H3(ch + 1)
N
X
n=1
a2
n
qn
γ2
k2n
E[||∇XF S(Xr)||2]
+ 9η2H2L2K2T
(32)
Thus, when η ≤
1
√
18KHL, we can solve T as:
T ≤18η2H3(σ2
g + σ2
s + σ2
h)
N
X
n=1
a2
n
qn
γ2
k2n
+ 18η2H3(ch + 1)
N
X
n=1
a2
n
qn
γ2
k2n
E[||∇XF S(Xr)||2]
(33)
Substituting T into (31), we obtain:
(ηH
2
−9L2η3H3N(ch + 1)
N
X
n=1
a2
n
qn
γ2
k2n
)E[||∇XF S(Xr)||2]
≤E[F S(Xr)]−E[F S(Xr+1)]+ 3
2η2LsHN(σ2
g +σ2
s +σ2
h)
N
X
n=1
a2
n
qn
+ 9L2η3H3N(σ2
g + σ2
s + σ2
h)
N
X
n=1
a2
n
qn
γ2
k2n
(34)
When η ≤
1
√
18KHL, we can have ηH
2 −9L2η3H3N(ch +
1) PN
n=1
a2
n
qn
γ2
k2n ≥ηH
4 . Thus, we have:
E[||∇XF S(Xr)||2]
≤4(E[F S(Xr)]−E[F S(Xr+1)])
ηH
+6ηLsN(σ2
g +σ2
s +σ2
h)
N
X
n=1
a2
n
qn
+ 36L2η2H2N(σ2
g + σ2
s + σ2
h)
N
X
n=1
a2
n
qn
γ2
k2n
(35)
Then, we can rearrange the terms yields and telescope from
r = {1, ..., R} to get the Theorem 1.
APPENDIX C
PROOF OF THE NEW BOUND OF q
To ensure that the denominator of Problem P2 is strictly
greater than zero, we impose the constraint B −CQ2 −
DK2Q2 > 0. This implies that:
Q2 <
B
C + DK2
(36)
Since Q2 = PN
n=1
a2
n
qn , a sufficient condition is:
N
X
n=1
a2
n
qn < 1
N
N
X
n=1
B
C + DK2
(37)
which leads to the conservative per-client constraint:
a2
n
qn <
B
N(C + DK2),
∀n ∈N
(38)
Therefore, we only need to ensure that:
qn > a2
nN(C + DK2)
B
(39)
This guarantees that the denominator in P2 remains positive.
