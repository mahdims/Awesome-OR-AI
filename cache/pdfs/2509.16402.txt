--- Page 1 ---
“It Was a Magical Box”: Understanding Practitioner Workflows and Needs in
Optimization
CONNOR LAWLESS, Stanford University, USA
JAKOB SCHOEFFER, University of Groningen, The Netherlands
MADELEINE UDELL, Stanford University, USA
Optimization underpins decision-making in domains from healthcare to logistics, yet for many practitioners it remains a “magical
box”: powerful but opaque, difficult to use, and reliant on specialized expertise. While prior work has extensively studied machine
learning workflows, the everyday practices of optimization model developers (OMDs) have received little attention. We conducted
semi-structured interviews with 15 OMDs across diverse domains to examine how optimization is done in practice. Our findings
reveal a highly iterative workflow spanning six stages: problem elicitation, data processing, model development, implementation,
validation, and deployment. Importantly, we find that optimization practice is not only about algorithms that deliver better decisions,
but is equally shaped by data and dialogue—the ongoing communication with stakeholders that enables problem framing, trust, and
adoption. We discuss opportunities for future tooling that foregrounds data and dialogue alongside decision-making, opening new
directions for human-centered optimization.
Additional Key Words and Phrases: Optimization, Workflows, Artificial Intelligence, Interview Study
1
Introduction
Optimization problems lie at the heart of some of today’s most pressing challenges: coordinating sustainable energy
generation [6], delivering healthcare efficiently [7], and managing global supply chains [9]. At its core, optimization
provides a mathematical framework for selecting the best option from a vast set of alternatives under different constraints:
for example, minimizing delivery costs while ensuring every customer is served, or allocating staff to balance workload
and availability. Decades of progress in algorithms and solvers, such as branch-and-bound algorithms for mixed-integer
linear programming, have made it possible to solve large-scale problems with millions of variables and constraints [93].
Yet applying optimization in practice remains far from straightforward: it is a socio-technical process that requires
translating ambiguous business needs into precise mathematical formulations.
Little, however, is known about the workflows of optimization model developers (OMDs). This gap lies in sharp
contrast to machine learning (ML), where practitioner workflows have been extensively documented through studies of
data science practice (e.g., [63, 69, 99]), ML operations (e.g., [41, 58, 80]), and data documentation (e.g., [8, 32, 90]). Unlike
ML workflows that center on extracting insights from historical data to generate predictions, optimization workflows
revolve around generating actionable decisions. This distinction shapes practice: optimization is characterized by messy
and incomplete data that inform and constraint model formulation, pragmatic trade-offs in computation and model
fidelity, and sustained dialogue with stakeholders that both shapes problem understanding and builds the trust needed
for adoption. Understanding these distinct workflows is essential for identifying bottlenecks and designing tools that
make optimization more usable, trustworthy, and impactful.
To address this gap, we conducted semi-structured interviews with 15 OMDs working across manufacturing, logistics,
healthcare, defense, and education. OMDs recounted projects, diagrammed workflows, and reflected on pain points
and opportunities. From these accounts, we identified an iterative six-stage workflow: (i) problem elicitation, (ii) data
Authors’ Contact Information: Connor Lawless, Stanford University, Palo Alto, California, USA, lawlessc@stanford.edu; Jakob Schoeffer, University of
Groningen, Groningen, The Netherlands, j.j.schoeffer@rug.nl; Madeleine Udell, Stanford University, Palo Alto, California, USA, udell@stanford.edu.
1
arXiv:2509.16402v1  [cs.HC]  19 Sep 2025


--- Page 2 ---
2
Connor Lawless, Jakob Schoeffer, and Madeleine Udell
Problem
Elicitation
Data
Processing
Model
Development
Model
Implement.
Model
Validation
Deployment
Fig. 1.
Six-stage workflow of optimization model development. The process spans problem elicitation, data processing, model
formulation, implementation, validation, and deployment. Although shown linearly for clarity, practitioners described this workflow
as highly iterative, with frequent backtracking and refinement across stages.
processing, (iii) model formulation, (iv) implementation, (v) validation, and (vi) deployment (Figure 1). Beyond this
workflow, our findings reveal how optimization practice is shaped not only by algorithms that generate better decisions
but also by the broader realities of data and stakeholder dialogue that make those decisions usable in practice.
This paper contributes a qualitative account of optimization practice that foregrounds the socio-technical realities
of modeling work. We offer (i) an empirical characterization of how optimization workflows unfold across diverse
domains, (ii) a conceptual framing that highlights the overlooked roles of data and dialogue alongside decisions, and
(iii) design implications for tools and practices that better align with how optimization is carried out in the real world.
By surfacing these dimensions, our work provides a richer understanding of optimization practice and opens new
directions for designing systems that can more effectively support it.
The remainder of this paper is organized as follows. We begin by reviewing background work on optimization and
related studies of practitioner workflows in Section 2. We then describe our interview study, including participant
recruitment, data collection, and analysis methods in Section 3. In Section 4, we present our findings on the six stages of
the optimization workflow. Section 5 then elaborates on three cross-cutting themes that define successful optimization
projects—data, decisions, and dialogue—and considers their implications for tool design and future research. We conclude
by reflecting on the broader significance of studying optimization practice as a socio-technical endeavor.
2
Background and Related Work
This section reviews optimization as a decision-making framework (Section 2.1) and surveys related work on practitioner
tooling, interactive optimization, and emerging LLM-based approaches (Section 2.2). We also draw connections to
research on AI workflows and practitioner needs (Section 2.3), underscoring how optimization remains relatively
underexplored within Human-Computer Interaction (HCI).
2.1
Mathematical Optimization
Optimization is about making the best possible decision from a large set of alternatives, typically by maximizing or
minimizing an objective (such as cost, time, or profit) subject to feasibility constraints (such as budgets, capacities, or
schedules) [65]. Optimization problems underpin the field of operations research (OR) which leverages optimization,
among other tools, to improve management and decision-making [91]. A classic example is logistics: deciding which
warehouses to open and how to route deliveries so that costs are minimized while meeting demand and respecting
capacities. Such problems are often modeled as mixed-integer linear programs (MILPs), where variables can be continuous
or integer, linear constraints capture feasibility, and a linear objective defines the goal [93]. MILPs are widely used
because they can model diverse applications—ranging from scheduling to healthcare delivery—and are supported by
powerful commercial (e.g., Gurobi [2], CPLEX [64]) and open-source (e.g., SCIP [1]) solvers that use advanced algorithms


--- Page 3 ---
“It Was a Magical Box”: Understanding Practitioner Workflows and Needs in Optimization
3
like branch-and-cut to explore large decision spaces efficiently. Still, many real-world problems are too large to solve
exactly, which makes decomposition methods [18, 72, 87] and heuristics [25] important in practice: decomposition
breaks problems into smaller, coordinated subproblems, while heuristics provide fast, “good enough” solutions. In
logistics, for example, practitioners may solve each region separately or approximate by assigning each store to the
closest warehouse that has available capacity.
2.2
Tooling for Optimization Practitioners
Traditionally, optimization tooling has centered on modeling languages that make it easier to express problems in
solver-ready form. Systems such as AMPL [26], Convex.jl [86], JuMP [21], CVXPY [19], and solver-specific APIs like
GurobiPy [30] provide higher-level interfaces to connect with solvers. GPKit introduced a human-centered Python
package for geometric programming, demonstrating how tool design can prioritize usability in addition to functionality
[10]. These tools lower the barrier to model implementation but address only a narrow part of the workflow: translating
mathematical formulations into solver-ready inputs. In contrast, our study examines end-to-end workflows to identify
opportunities for tooling beyond implementation. Another line of work examines interactive optimization, where
humans guide solver behavior (see [60] for a detailed review). Prior studies have explored methods for managing
trade-offs between different objectives or steering heuristic search [39, 61], proposed visualization guidelines [53], and
investigated how feedback can calibrate user trust in solvers [54]. While these efforts emphasize human-in-the-loop
interactions with structured elements of a solver, our work shifts attention to the broader workflows of optimization
practitioners.
Most recently, advances in large language models (LLMs) and natural language processing have motivated “optimiza-
tion copilots” that aim to automate modeling end-to-end [3, 5, 34, 35, 73, 89, 94]. Frameworks such as Chain-of-Experts
[94] and OptiMUS [3, 4] leverage structured pipelines of LLM agents to model and solve MILP problems, while fine-tuned
LLMs have been shown to improve formulation accuracy [35]. Building on these techniques, LLM-powered systems
have been deployed in domains including supply chain optimization [50], scheduling [46], debugging [12], evaluating
formulation equivalence [98], and solver configuration [45]. However, most are designed as end-to-end solutions
for non-experts, aiming to bypass rather than support practitioner expertise. By contrast, our study examines how
optimization experts actually work, surfacing workflow bottlenecks and opportunities for tools that augment rather
than replace their expertise.
2.3
AI Practitioner Workflows and Needs
The academic optimization and OR communities traditionally value technical advances in modeling and algorithms
over the human dimensions of practice. A notable exception is the field of behavioral OR [31, 43], which combines
insights from psychology, behavioral economics, and cognitive science to understand how people interpret and use
optimization results. Yet this work focuses on behavior downstream from optimization—how decision-makers engage
with model outputs—while the upstream practices of optimization model developers themselves remain underexplored.
Motivated by this shortcoming, there has been a recent effort from a large professional organization within the OR
community to provide frameworks to guide analytics projects for practitioners that mirror the stages we identify in our
study [36].
By contrast, ML research has devoted significant attention to the workflows, practices, and needs of practitioners.
The rise of MLOps formalized lessons from software engineering and DevOps into structured processes for deploying,
monitoring, and maintaining ML systems in production [13, 41, 58]. Similarly, frameworks like CRISP-DM [92] and


--- Page 4 ---
4
Connor Lawless, Jakob Schoeffer, and Madeleine Udell
CRISP-ML [83] articulate idealized end-to-end pipelines, which highlight that technical development is inseparable
from organizational processes, infrastructure, and governance [75].
Complementing these frameworks, previous studies have provided rich, situated accounts of ML practice. Ethno-
graphic and interview-based work highlights the iterative and collaborative nature of data science [63, 99], the role of
automation in reshaping tasks and responsibilities [88], and the negotiation of trust and accountability across organiza-
tional boundaries [42, 67]. Tool-focused studies similarly reveal that adoption is often pragmatic and improvisational:
AutoML is valued less for replacing human judgment than for scaffolding it [95], fairness toolkits struggle when abstract
metrics clash with real-world messiness [17], and developers adapt tools in ways that reflect their local constraints
and goals [28, 69, 96]. Collectively, this work shows that effective tooling emerges not from abstract ideals but from
alignment with practitioners’ workflows, contexts, and values.
Another related body of research has examined practitioners’ informational and organizational needs. Studies of
documentation practices [8, 32, 90] reveal the tension between transparency ideals and workplace realities. Work on
fairness and governance [33, 51, 55] highlights misalignments between available tools and the situated practices of
development teams. Recent analyses of production ML [80] underscore the uncertainty engineers face when bridging
the gap between development environments and real-world deployment. Across these studies, a consistent lesson
emerges: tools succeed only when they are designed for the actual constraints, contexts, and goals of practitioners.
Our work extends these insights to a comparatively overlooked community: OMDs. Like ML practitioners, they must
balance technical rigor with organizational demands, but their challenges are distinct—translating domain expertise
into formal constraints, balancing tractability with fidelity, and iteratively refining formulations as requirements evolve.
By examining their workflows, we broaden the scope of human-centered AI to include optimization. In doing so, we
surface both shared concerns across AI practitioner communities and the unique requirements of optimization, pointing
toward opportunities for more effective modeling environments, documentation practices, and collaborative tools.
3
Methodology
The goal of this study was to better understand how OMDs approach their work, including the workflows they follow,
the challenges they face, and the strategies they use to address them. To this end, we conducted semi-structured
interviews with 15 OMDs working across diverse domains.
3.1
Study Participants
We recruited participants via personal connections (N=6), advertising on professional networks and social media
websites (N=2), advertising the study during a talk at technical conference (N=4), and via snow-ball sampling with
earlier participants (N=3). Participants were required to complete a screening survey to ensure that they were over 18,
based in the US, had obtained at least a bachelor’s degree, and had completed at least one real-world project involving
optimization. The latter criteria was designed to ensure that participants had practical experience with optimization
problems outside of an academic setting.
Since, to the best of our knowledge, there are no prior works that study OMDs, we focused on recruiting users
with different education backgrounds and diverse application domains. Table 1 summarizes the 15 participants of our
study. Of the 15 participants, 10 were male (66%) and 5 were female (33%). 8 of the participants (53%) had obtained their
bachelors, 4 (27%) had a masters degree, and 3 had doctoral degrees (20%) at the time of the interview. Participants had,
on average, 10 years of experience with optimization and worked on a number of applications including call center
operations, health care delivery, logistics, manufacturing, and agriculture.


--- Page 5 ---
“It Was a Magical Box”: Understanding Practitioner Workflows and Needs in Optimization
5
Table 1. Study participants.
ID
Role
Highest Degree
Experience
Application Area
P1
Data Scientist
Bachelors
3–5
Manufacturing
P2
PhD Student
Masters
6–10
Agriculture
P3
PhD Student
Bachelors
11+
Healthcare
P4
Applied Scientist
Masters
3–5
Service Operations
P5
Algorithm Development Engineer
Masters
3–5
Defense
P6
Technical Fellow
PhD
11+
Logistics
P7
Operations Research Analyst
PhD
11+
Transportation
P8
PhD Student
Masters
1–2
Logistics
P9
Software Engineer
Bachelors
1–2
Education
P10
Optimization Consultant
PhD
11+
Consulting
P11
PhD Student
Masters
3–5
Logistics
P12
PhD Student
Bachelors
3–5
Education
P13
Applied Mathematician
Masters
6–10
Transportation
P14
PhD Student
Bachelors
3–5
Logistics
P15
Applied Scientist
Masters
1–2
Logistics
3.2
Interview Protocol
Each participant attended a semi-structured interview conducted in-person (N=2) or virtually (N=13) via a video
conference tool lasting between 35 to 75 minutes (average of 44 minutes). All participants were compensated with a $20
digital gift card. During the interview, participants were asked to recall 1–2 prior projects that included the application
of optimization to solve a real-world use case. They then described the workflow of the project, from conception
to deployment, including how they interfaced with different stakeholders and what tooling they used. Participants
diagrammed the process while describing the project to encourage reflection on the entire workflow. Participants were
asked to highlight pain points and bottlenecks in the project workflow and to brainstorm tools that could have helped
improve their process. After discussing the first project, if time permitted participants were asked to describe a second
project by contrasting it with the first. The interview guide can be found in Appendix A. All interviews were captured
via audio recordings. The protocol was approved by the leading institution’s Institutional Review Board.
3.3
Coding and Analysis
In total, 654 minutes of interviews were audio-recorded and transcribed using Marvin1. We employed a grounded
theory-based approach to code and analyze the interview transcripts [11]. Each interview transcript was initially
open-coded by one of the researchers independently on a line-by-line basis. These initial codes were then merged and
grouped together to identify key patterns and relationships between different concepts. Finally, the team distilled these
codes into 17 core themes that represent the key findings of the study. Throughout the analysis, the researchers met
weekly to discuss the emerging codes, resolve discrepancies, and iteratively reach a consensus on overarching themes.
We did not calculate inter-rater reliability to avoid potential marginalization or minimization of perspectives [59].
1https://app.heymarvin.com/


--- Page 6 ---
6
Connor Lawless, Jakob Schoeffer, and Madeleine Udell
4
Findings
Our interviews revealed that optimization model development is not a straightforward technical pipeline but an iterative
socio-technical workflow. Practitioners consistently described moving back and forth across six interconnected stages:
problem elicitation, data processing, model development, implementation, validation, and deployment. Although each
stage had its own bottlenecks and strategies, collectively they constituted a cycle of refinement where iteration was
driven by changing requirements, imperfect data, and sustained dialogue with stakeholders. To present these findings,
we structure the remainder of this section around the six stages of the workflow, illustrating how practitioners approach
each stage, the challenges they encounter, and the strategies they adopt to overcome them. Table 2 summarizes our
findings along the optimization workflow.
4.1
Problem Elicitation
“The whole thing was all about this dialogue and interaction between me and the [stakeholders]. If I had
kind of gone off and just worked in my office and never had that dialogue, the project would have never
really gotten off the ground.”
— P5
The problem elicitation stage of the optimization workflow focuses on engaging stakeholders to understand, define,
and refine the problem that optimization is meant to address. This stage involves translating high-level and often vague
business needs into clear, structured requirements that can guide model development (P3, P6, P12). It requires building
a baseline understanding of the client’s current situation, determining whether optimization is the right approach, and
aligning diverse stakeholders on the problem to be solved (P10). Because knowledge is often distributed across business,
technical, and customer experts (P4, P9), elicitation relies heavily on iterative discussions, careful questioning, and
communication in non-technical language (P3, P4, P5, P6, P14, P15). A sample artifact from this phase of the workflow
could be a problem requirements document (P4, P10) that outlines all the relevant details about the decision-making
problem in natural language (see Figure 2). Although time-consuming and resource-intensive, this phase is critical:
OMDs emphasized that a thorough understanding of the problem is the foundation of any successful project.
4.1.1
Initial problem descriptions are vague, requiring iterative stakeholder engagement. Successful projects rarely begin
with a well-defined problem. Initial descriptions from clients are often vague, informal, or even contradictory (P3, P6,
P12), leaving practitioners to piece together the objectives and constraints of a consistent decision problem formulation.
This ambiguity makes elicitation a process of sustained, iterative engagement rather than one-off requirement gathering.
Practitioners described a cycle of regular meetings, document reviews, email exchanges, and site visits with business
and technical experts to gradually clarify assumptions and refine problem definitions (P3, P4, P5, P6, P14, P15). P4
likened the process to a “flywheel” of iteration:
“You need to iterate [...] the whole process is like a flywheel that we go here, then probably we’ll [...]
formulate, come up with a solution, then put it out there, gather feedback and literally come back to [...]
what’s really your goal?”
Achieving alignment on the problem definition and measures of success—such as cost reduction, increased revenue,
or more efficient processes—is essential before translating the business problem into an optimization formulation (P10).
An important question to ask at this stage, according to P10, is, “how are we going to measure that we’ve delivered
value?” P4 additionally highlighted that stakeholders respond best to specific, targeted questions, which underscores


--- Page 7 ---
“It Was a Magical Box”: Understanding Practitioner Workflows and Needs in Optimization
7
Table 2. Summary of optimization workflow insights and challenges.
Workflow Step
Insight
Exemplary Quote
Problem Elicitation
Initial problem descriptions are
vague, requiring iterative stake-
holder engagement
“If I had [...] just worked in my office and never had that dialogue [with
stakeholders], the project would have never really gotten off the ground.”
— P5
Problem understanding requires
reconciling distributed expertise
“With this project in particular it was the sheer number of stakeholders
that we had to communicate with that I think was especially challeng-
ing.” — P9
OMDs critically assess when opti-
mization is (and is not) appropriate
“Sometimes the problem they may think is optimization, sometimes it’s
just about business process change.” — P10
Data Processing
Data tends to be messy, sparse, or
incomplete
“Their data is not in a single place, or it’s like, layered on top of a bunch
of legacy systems.” — P1
Accessing data is slow and difficult
“It’s not like from textbook [...] you really need to figure out whether
you can get exactly the data you want.” — P3
Data drives problem definition and
modeling
“A lot of times the data will drive the knowledge, the understanding of
the problem, frankly.” — P7
Model Development
Modeling is iterative and evolves
with shifting requirements
“We’ll go to formulate, come up with a solution, then put it out there,
gather feedback and literally come back to [...] what’s really your goal?”
— P4
OMDs balance simplification with
realism
“If [...] it’s more of a real-time application and we have millions of
variables [...] we’re going to have to go to a heuristic right away.” — P7
OMDs leverage literature and ex-
pert judgment
“I would build off of the ones that I found [in literature] but alter it based
on the type of work we were trying to do.” — P8
Model Implementa-
tion
Implementation advances through
iterative prototyping
“I go back and what I’ll do is like run something with no constraints
and then iteratively add constraints [...] does the solution behave like I
expect it would?” — P8
Initial implementations validate the
scalability of the proposed model
“We started out with a single model and the runtime would be like over
24 hours or so, which we, like, definitely could not do.” — P12
OMDs struggle to leverage off-the-
shelf optimization solvers
“So really the only thing to do is to kind of just tweak the, the parameters
and, and see what happens.” — P15
Model Validation
Models are validated through in-
spection and stakeholder critique
“[We] evaluate the solution using some common sense and then go back
to the customer, discuss the model, gather some feedback and go to step
one” — P4
OMDs satisfice with respect to com-
putation times
“We do have some requests from them to about how long this algorithm
should take” — P14
Model validation is concurrent with
data validation
“What combo data and constraints is causing a solution that’s non
intuitive.” — P7
Deployment
Documentation is key to successful
deployments
“I would say that writing the document itself takes, takes some time” —
P4
Portability and integration con-
straints shape deployment
“So we learned pretty late on that the third party didn’t support what we
needed them to support to be able to display the outputs of our model.”
— P1
the importance of focused engagement. Despite its importance, this process is time-intensive and often constitutes one
of the longest and most challenging part of a project (P3, P4, P6, P9).


--- Page 8 ---
8
Connor Lawless, Jakob Schoeffer, and Madeleine Udell
4.1.2
Problem understanding requires reconciling distributed expertise. Beyond the effort of sustained engagement,
OMDs also stressed the difficulty of integrating knowledge that is fragmented across diverse stakeholders. Optimization
projects typically involve business leaders, technical engineers, and end users, each holding partial and sometimes
conflicting perspectives on the problem (P4, P6, P9). As P9 noted “with this project in particular it was the sheer number
of stakeholders that we had to communicate with that I think was especially challenging.”
Practitioners emphasized that elicitation requires careful translation of domain expertise into actionable constraints,
often using non-technical language (P3, P6, P11). Access to the right people can be a bottleneck, particularly when
those with hands-on knowledge of the process are hard to reach (P15). P10 underscored the importance of “reading
between the lines” to surface implicit needs. Reconciling these distributed perspectives is not only time-consuming
but also essential for establishing a coherent problem definition and shared success criteria; without such alignment,
downstream modeling often requires rework or risks rejection.
4.1.3
OMDs critically assess when optimization is (and is not) appropriate. OMDs emphasized the importance of carefully
discerning whether optimization is the right tool for a given problem, or whether simpler interventions—such as business
process changes or dashboards—would be more appropriate. As P10 noted, “sometimes the problem they may think is
optimization, sometimes it’s just about business process change.” This perspective reflects a broader view of optimization
as one tool within a larger analytical toolbox, where the priority must be on understanding the problem thoroughly
before selecting a method. Determining whether optimization will truly add value is essential, ensuring that it is pursued
only when the problem is well-defined and the potential impact is clear. As P10 put it:
“It’s because people are taking hammers and trying to find the nails. The answer is you have to find a nail
and then pick out the right hammer.”
4.2
Data Processing
“In terms of the total number of lines in my code base, I would say it was 70% data engineering, 30% of
everything else. So data engineering is really the big pain point.”
— P1
After understanding the problem and before developing a model, OMDs must understand the data landscape. As
P7 emphasized, “we need to understand the data that’s going to be available before developing the model.” The data
processing stage of the optimization workflow is often a major bottleneck, as it involves gaining access to, cleaning, and
documenting messy, sparse, or poorly described datasets before they can be used effectively (P3, P10, P13, P14). OMDs
noted that a significant share of project time is devoted to cleaning and pre-processing (P1, P7, P10, P13, P14), clarifying
what data is static versus dynamic, and identifying which inputs are truly needed for the problem at hand (P10).
Understanding the client’s data flows and technology stack is essential, and early steps often include gathering sample
data, running simplified simulations, and creating documentation reports to ensure transparency and consistency (P10,
P11). Because available data can shape assumptions and even require updates to the problem formulation, there is
typically a feedback loop between model development and data acquisition (P3, P6). Since this work is distributed across
teams and requires extensive manipulation, it not only supports model building but also helps refine and concretize the
problem itself.
4.2.1
Data tends to be messy, sparse, or incomplete. OMDs consistently described data quality as one of the biggest
bottlenecks in optimization projects. Data is often messy, requiring extensive cleaning and pre-processing, with OMDs


--- Page 9 ---
“It Was a Magical Box”: Understanding Practitioner Workflows and Needs in Optimization
9
estimating that 70% of their time was spent on this step (P13, P14). As P13 explained, “the data typically takes a while
to clean because it’s messy and not all the pieces of information actually exist.” Beyond messiness, data is frequently
sparse or incomplete, which makes it difficult to support robust modeling (P10). Probabilities required for stochastic
programming, for example, are often unavailable, which forces developers to rely on simplified assumptions or to
consult domain experts to fill in gaps (P3, P10).
Even once data is collected and processed, challenges of accuracy and reliability persist. As P7 reflected, “I mean,
you’re modeling a real life situation and you know, the data aren’t always accurate and they’re, I mean, they’re never
completely accurate, so it’s, you know, how good do you have to be anyway?” In some cases, synthetic data provides a
practical workaround to accelerate development when real datasets are incomplete or unavailable (P6). While laborious,
these activities are essential to ensure that data aligns with optimization requirements and to move projects forward.
4.2.2
Accessing and interpreting data is slow and difficult. The challenges around messy, sparse, and incomplete data
are compounded by unclear or missing documentation (P14), which requires substantial effort to interpret the meaning
and flows of available datasets. OMDs noted that even producing simple sample data for testing can become a hurdle,
often constrained by access rights and availability (P3, P6, P9, P14). Accessing data is frequently slow and cumbersome,
which creates bottlenecks that delay both testing and progress more broadly. As P1 explained: “These projects where
you’re trying to make a non technologically adept company more technologically adept, is their data is not in a single
place, or it’s like, layered on top of a bunch of legacy systems. And so also separating that out was like a pretty big piece.”
Because acquisition is typically distributed across multiple teams and infrastructures, it demands extensive coordination
and manipulation, with OMDs often responsible for documenting structures, understanding client data flows, and
navigating technology stacks as a necessary precursor to modeling (P10, P14).
Model development and data acquisition often proceed in parallel, creating feedback loops in which gaps or delays in
data can stall modeling, while evolving modeling needs can generate new requirements (P3, P6). In some cases, synthetic
data is employed to temporarily unblock development (P6), but even with such workarounds, OMDs emphasized that
the difficulty of accessing and interpreting data remains one of the most persistent challenges in practice.
4.2.3
Data drives problem definition and modeling. A consistent finding is that the problem cannot be properly
formulated until the underlying data is fully understood. As P7 noted, “And so defining the data they have, you know, a lot
of times, and I guess I could have emphasized this more, a lot of times the data will drive the knowledge, the understanding
of the problem, frankly.” OMDs emphasized that optimization is fundamentally shaped by the data available, often
requiring a “data-first” rather than a “model-first” approach (P10). Identifying which information is static (e.g., fixed
geography) versus dynamic (e.g., sales patterns), gathering representative sample datasets, and initiating simulations
with simplified assumptions are common first steps (P10, P11). Mapping data flows, investing in pre-processing, and
collecting sample datasets are not only necessary to enable model development but also help concretize the problem
itself: they shape assumptions and occasionally require updates to the original problem framing (P3, P5, P10). Data
collection and modeling are tightly interwoven, often proceeding concurrently in iterative feedback loops: as data is
acquired and analyzed, assumptions may need to be revised, techniques adapted, and even the problem formulation
itself updated to reflect reality, as noted by P6. Because data directly influences how decisions are framed and evaluated,
OMDs stressed the importance of understanding its role in defining baselines, constraints, and potential solutions (P10).
Ultimately, data was described as both the most time-consuming part of the process and the foundation on which
optimization modeling rests.


--- Page 10 ---
10
Connor Lawless, Jakob Schoeffer, and Madeleine Udell
Fig. 2. Sample artifacts from along the optimization workflow. (Left) A sample document outlining the problem requirements in
natural language. (Center) A formal optimization model that translates the problem description into mathematics. (Right) A sample
implementation of the model in Python using GurobiPy [30].
4.3
Model Development
“99% of the time, something was not included in the model from the customer standpoint. They’ll say, well,
we have this, you can’t do that, they’ll say; and I said, well, you didn’t tell me that. And so then you have to
go back and update the model.”
— P7
Model development in optimization is a distinct phase that sets it apart from most ML workflows. Unlike ML,
where prediction tasks can often be operationalized without formal mathematical specification, OMDs must translate
natural-language descriptions into concrete mathematical models (P10, P12, P14, P15). This translation is demanding:
practitioners must balance tractability, realism, and theoretical soundness while working closely with both data and
stakeholders (P10, P13, P14). This is further complicated by the fact that there are often many valid ways to model the
same problem, and different formulations can have dramatically different computational properties [14]. As P11 put it,
“modeling is quite an art.” OMDs therefore draw on prior models and the literature, adapting and simplifying them to fit
context while making assumptions explicit and validating them with stakeholders (P4, P5, P8, P10, P11, P14). Early
efforts often take the form of minimal viable models, which are iteratively refined through testing, client feedback, and
deeper data engagement (P10, P12, P13, P14). This resource-intensive process—sometimes even exceeding customer
budgets (P7)—underscores a challenge unique to optimization and central to its success (P10, P12, P15).
4.3.1
Modeling is iterative and evolves with shifting requirements. Translating vague or high-level business requests
into precise mathematical formulations is one of the most challenging and time-consuming aspects of the workflow
(P14, P12, P15). As P12 explained, “it’s a lot of figuring out what he wants [...] and then we have to somehow translate
that into math.” Because requirements evolve, modeling is inherently iterative (P4, P13, P14), with algorithmic choices
adapting as understanding deepens (P4, P7, P13). Navigating this complex, evolving process is essential to producing
models that are both technically sound and aligned with organizational needs (P14, P15).
To manage this uncertainty, practitioners often employ agile strategies [81, 85] such as starting with minimal
viable models and refining them through successive iterations (P14, P10). Stakeholder feedback and data realities


--- Page 11 ---
“It Was a Magical Box”: Understanding Practitioner Workflows and Needs in Optimization
11
guide each revision, making ongoing collaboration essential (P4, P5, P10). As P5 described, stakeholder requests are
usually underspecified: “It’s never specific enough from a modeling perspective.” This iterative process requires careful
documentation of assumptions and continuous verification that models remain consistent with stakeholder expectations
and implementation requirements (P10). Yet the effort is substantial—sometimes exceeding budgets or stretching over
months of iteration (P7, P10, P12, P15).
4.3.2
OMDs balance simplification with realism. Modeling requires creativity and abstraction to simplify real-world
complexity without losing essential features. OMDs balance model fidelity to the decision-making problem with solver
runtime, stakeholder priorities, and data constraints, often blending simplification with realism to make models both
sound and useful. Rather than pursuing strict optimality, they frequently “satisfice”, delivering solutions that are good
enough within feasible runtimes (P4, P5). As P13 explained, “if you could push something out in a day, it’s better than a
week,” underscoring the trade-off between speed and perfection. This approach requires abstracting problems, deciding
which constraints can be simplified, and iteratively testing minimal viable models on available data (P3, P4, P15),
ultimately enabling OMDs to deliver actionable insights efficiently in complex, evolving contexts (P12, P13, P14).
4.3.3
OMDs leverage literature and expert judgment. OMDs frequently draw on both the literature and expert judgment
to guide model formulation, particularly when translating vague or high-level requests from non-optimization stake-
holders into formal mathematical models (P12, P14, P15). They often begin by identifying the most related model types
from prior work or existing frameworks (e.g., knapsack, scheduling) and then adapt these structures to the specific
context, a process sometimes described as “scaffolding” (P4, P5, P8, P14). Literature or graduate school course material
(P12) provides reusable components such as variable definitions, common constraints, and solution strategies, which can
accelerate development and ensure that models remain grounded in established practice (P4, P5, P11). Initial modeling
is frequently informed by practitioner experience or intuition to fill in gaps where theory is insufficient (P12, P14, P15).
As P8 described, “I like read a few different papers related to VRPs with pick up a drop off windows and [...] my model was
like an amalgamation,” illustrating how OMDs synthesize insights from multiple sources to construct tailored solutions.
Finally, scaffolding can also allow OMDs to leverage specialized solvers that are faster than general-purpose ones [66].
4.4
Model Implementation
“It was a mess. We had to open the documentation and try to understand how it works and how you could
do the callback and what you pass to the callback and everything.”
— P4
Once an initial optimization model has been formulated, OMDs turn to the task of implementation (see Figure
2). This stage is often time-consuming, involving both translating formulations into specialized code and managing
substantial data processing. OMDs’ experiences varied considerably depending on background: some framed it as
a relatively straightforward technical task that “doesn’t use a lot of brain power” (P4), while others described it as a
persistent bottleneck. As P12 reflected, “I feel like I spent a lot of time getting stuck in things saving properly or loading to
the wrong environment, like that kind of thing.” Despite this variation, OMDs agreed that implementation was a crucial
step in the pipeline, as it not only operationalized the model but also served to validate assumptions about the quality
of data and the ease of solving the problem at scale.
4.4.1
Implementation advances through iterative prototyping. OMDs described implementation as an incremental,
pragmatic process. Developers often began by prototyping on toy problems, testing “constraint by constraint” to confirm


--- Page 12 ---
12
Connor Lawless, Jakob Schoeffer, and Madeleine Udell
behavior before scaling up (P5, P8). Early implementations were treated as minimum viable products, prioritizing
correctness over speed or efficiency (P4, P12). To scaffold their work, many drew on existing resources such as pseudocode
or open-source implementations, adapting them to their solver environments. As P5 explained, “sometimes pseudocode
is the best method [...] starting from open source or papers with code like gets you the first leg up.” This scaffolding closely
paralleled the process of model development itself, where practitioners frequently mapped new problems onto known
formulations rather than starting from scratch.
4.4.2
Initial implementations validate the scalability of the proposed model. Once an initial implementation was correct
(i.e., correctly encoded the constraints), OMDs used early implementations to probe whether a formulation could
realistically scale. A common strategy was to run the model on off-the-shelf solvers, which quickly revealed whether the
problem was tractable in its initial form. These tests frequently surfaced limitations: P13 and P15 described models that
could not be solved on local machines, while P10 recalled a version that required 48 hours to solve, making iteration
“really hard” and motivating improvements in modeling to reduce runtime. P12 expressed a similar challenge, noting,
“we started out with a single model and the runtime would be like over 24 hours or so, which we, like, definitely could not do.”
In response to such bottlenecks, developers adopted a range of tactics. Some emphasized improving solution quality
through heuristics, while others focused on reformulation to bring down computational costs (P4, P14). For P12, the
24-hour runtime ultimately led to breaking the problem into smaller stages and incorporating heuristics to achieve
acceptable performance. Others leaned on relaxations as a practical workaround—P15 noted relying on a model relaxation,
an ‘easier’ version of the problem with fewer constraints, to generate feasible solutions when the full formulation
was computationally prohibitive. These adjustments illustrate how scalability testing through initial implementations
guided decisions about whether to invest in heuristics, decomposition, or more efficient formulations.
4.4.3
OMDs struggle to leverage off-the-shelf optimization solvers. Despite their ubiquity in the implementations
described by OMDs, many OMDs noted frustrations with interfacing with off-the-shelf optimization solvers such
as Gurobi [2]. Multiple OMDs (P2, P11, P12, P15) expressed difficulty understanding existing documentation and
limited information available on solver websites. Modern MILP solvers, such as Gurobi [2] and CPLEX [64], ship with a
staggering number of parameters that control key algorithmic components of the solver such as cutting plane separators,
branching rules, and heuristics. These parameters can have a dramatic impact on the runtime of a solver for a particular
problem but are notoriously difficult to tune by hand (P2, P11, P12, P15). P12 noted:
“One thing is we did play around with Gurobi. Like, they have their own, like, internal parameters that we
tried out and you could, like, set a time limit and things. So, yeah, we, like, try to understand how Gurobi
functioned a little bit, but it’s still like a black box.”
This frustration was echoed by P15 that remarked “the only thing to do is to kind of just tweak the, the parameters and,
and see what happens”. This is further exacerbated when using advanced solver functionality such as callbacks (P4, P15).
4.5
Model Validation
“Do you think it makes sense? Do you think this kind of managerial insights are really working for this
problem? Is it what you observe during your work?”
— P3
Once an optimization model has been implemented, the next stage is validation—a complex process aimed at
establishing confidence that the system is both correct and useful. Validation typically involves answering three


--- Page 13 ---
“It Was a Magical Box”: Understanding Practitioner Workflows and Needs in Optimization
13
questions: does the model faithfully capture the real-world decision-making problem; does the code correctly implement
the intended formulation; and can the approach scale to the size of instances required in practice. Addressing the first
question requires close collaboration with domain experts, who can evaluate whether proposed solutions align with
their expectations and constraints. The latter two tasks are typically carried out by practitioners themselves through a
mix of manual checks, ad hoc tests, and performance experiments.
Our findings highlight how validation unfolds in practice across several dimensions. Practitioners frequently engage
domain experts by presenting candidate solutions for critique, a process that surfaces missing requirements and clarifies
expectations (P1, P3, P6, P7, P9, P12, P15). They also rely on a variety of informal “sniff tests” to verify correctness
(P4, P5, P6, P8, P9), often supplemented with domain-specific visualizations that render results interpretable to both
developers and stakeholders (P9, P10, P15). Validation further extends to considerations of computational performance,
where OMDs describe satisficing behaviors in meeting runtime constraints rather than pursuing optimal speed (P8, P14,
P15). Finally, the validation process is closely intertwined with data quality, as mismatches between assumptions and
real-world inputs often necessitate iterative refinement of both the model and its data pipeline.
4.5.1
Models are validated through inspection and stakeholder critique. Validation in practice was highly interpretive
and collaborative, involving both model developers and domain experts. Developers often relied on informal “sniff tests,”
manually inspecting outputs and applying common sense to judge plausibility (P4, P5, P6, P8, P9). These ad hoc checks
ranged from validating solutions against small instances (P14) to visually spotting errors such as infeasible pickup and
drop-off times (P8). Some developers supplemented these checks with auxiliary metrics or sensitivity analysis (P4, P11),
while a few leveraged more formal processes such as independent “solution validators” (P10). Yet even these efforts
remained resource-intensive and contingent on manual interpretation.
Domain experts were brought into the process by being shown candidate solutions, which often surfaced missing
requirements or mismatches with expectations. As P9 noted, “showing somebody a solution and saying like, okay, this is
what we think we’re going to do is kind of the best way for them to look at it with a critical eye and think like, does this
actually work for me?” These critiques were iterative and ongoing, with teams frequently running scenarios and refining
the model in response to feedback (P7, P9). Because larger instances could feel like a “black box” (P13), developers and
stakeholders leaned on small examples and, importantly, on domain-specific visualizations to make outputs interpretable.
These ranged from CAD drawings for engineers (P15) to sample course schedules for administrators (P9), and were
equally valuable for developers themselves, who produced “lots of visualizations” (P10) to support debugging. In this
way, inspection, critique, and visualization collectively served as the primary mechanisms for verifying that models
behaved as intended and produced solutions stakeholders could trust.
4.5.2
OMDs satisfice with respect to computation times. Unlike much of the academic optimization literature, OMDs
described runtime not as something to minimize but as a practical constraint to be managed. They adopted a satisficing
approach: if a solution could be delivered within a stakeholder-defined budget—often negotiated in advance, such as
fifteen minutes for monthly planning runs—it was deemed acceptable (P14, P15). As P15 explained, solutions only
needed to be “fast enough to meet our business needs.” Once this threshold was reached, further speed improvements
were deprioritized, with developers instead balancing accuracy and runtime to achieve “good enough” performance (P4,
P8). Validation of computation times was therefore less about chasing theoretical efficiency and more about ensuring
models operated within the practical needs and tolerances of stakeholders.


--- Page 14 ---
14
Connor Lawless, Jakob Schoeffer, and Madeleine Udell
4.5.3
Model validation is concurrent with data validation. OMDs emphasized that validating optimization models was
inseparable from validating the data used to drive them. In practice, much of the debugging effort centered on ensuring
that model assumptions aligned with real-world data. P7 noted that this was often difficult in the absence of suitable test
cases, sometimes requiring developers to “write a data generator” to explore different scenarios. Others highlighted the
practical barriers to accessing representative data, with P15 describing challenges in obtaining test data directly from
clients. Several OMDs described building explicit mechanisms to check whether data assumptions were satisfied before
trusting model outputs. P10, for example, implemented a “data validator” that raised flags if input violated expected
conditions, explaining that this process only truly began once real data was incorporated. When such mismatches
occurred, the cycle of validation often required revisiting both the preprocessing pipeline and the model itself: “iterate
back if assumptions are not met and potentially modify the data prep or modeling” (P10).
4.6
Deployment
“When I talk about deployment, it’s not just standing up an application technically. It’s also about [...] what
change management do you have to do to be successful in the business? How are people going to use this
application? How are they going to change the way they’re working?”
— P10
OMDs described considerable variation in what constituted the final stage of an optimization project, depending on
the use case and organizational context. For some projects, the deliverable was simply the solution itself, often provided
as a schedule, routing plan, or allocation that stakeholders could directly act upon (P9, P12, P13). As P11 explained, “the
end product is what they looked like and were able to understand,” emphasizing that success was measured by whether
stakeholders could readily interpret and use the outputs.
In other cases, OMDs noted that an initial proof-of-concept could serve as the final artifact, with responsibility for
further productionization handed off to another team. P13 and P14 described situations where demonstrating feasibility
was sufficient, with subsequent integration and scaling carried out by dedicated production engineers. By contrast, the
most challenging scenarios arose when model developers themselves were expected to integrate the optimization into
existing systems. P4 described this as an often frustrating responsibility: “I’m not happy doing so much integration.”
Beyond technical handoff or integration, several OMDs emphasized that successful deployment also required
organizational coordination. P10 highlighted the importance of convening stakeholders to align on what adoption
would entail, from user training to ongoing support. Once deployed, lifecycle management became an ongoing concern,
encompassing monitoring, assessing whether the system delivered value, and making updates when the environment
changed. These activities were sometimes performed by the implementers themselves, but in other cases involved
knowledge transfer to clients or a hybrid arrangement.
4.6.1
Documentation is key to successful deployments. OMDs emphasized that documentation and communication were
essential for adoption, but also time-consuming. Deployment required materials tailored to diverse stakeholders, often
in the form of problem-specific interfaces with metrics or visualizations to make results actionable (P4, P9). As P4 noted,
“I would say that writing the document itself takes, takes some time,” underscoring the effort involved. Documentation
served both as training and as a way to build trust, especially since optimization was often perceived as a “black box”
(P13). Visualizations were especially important here, not only conveying outputs but also showing alignment with
stakeholder expectations, thereby fostering transparency and confidence (P13, P14).


--- Page 15 ---
“It Was a Magical Box”: Understanding Practitioner Workflows and Needs in Optimization
15
4.6.2
Portability and integration constraints shape deployment. OMDs described frequent challenges in deploying
models across different technical environments, as client restrictions or system limitations often dictated what tools
could be used. In some settings, commercial solvers were not permitted, requiring teams to re-implement or approximate
solutions with less capable tools. P3 recalled one such case: “They are not trained as experts for operations research. So
they don’t quite use Python, Gurobi, or any of it [...] so we decided to build a Microsoft Excel tool for them. So that kind of
brings some approximation for this problem because the Excel solver is really not that capable as Gurobi.” Portability issues
also extended to solvers themselves (P15) and to integration with third-party platforms, where missing features or
incompatible interfaces created unexpected barriers (P1, P11). These accounts show how deployment was often shaped
less by the model itself than by the constraints of the surrounding technical and organizational ecosystem, forcing
practitioners to adapt solutions to client-approved environments even at the cost of functionality.
5
Discussion
Our findings show that optimization model development unfolds as a highly iterative and socio-technical process.
Across the workflow, three themes consistently surfaced as central to optimization practice: data, decisions, and dialogue.
In the discussion that follows, we unpack each of these themes and highlight opportunities for future tooling that can
better support OMDs.
5.1
Data, Decisions, and Dialogue: Three Ds of Optimization Practice
Findings from our study suggest three broad themes that characterize successful optimization workflows: data, decisions,
and dialogue.
5.1.1
Data. Unlike ML, where data preparation is treated as a distinct stage [80], our study finds that in optimization,
data permeates every step of the workflow. Practitioners emphasized that meaningful formulation is often impossible
until the data landscape is understood, with data wrangling consuming up to 70% of effort and sometimes stalling
projects entirely when access is delayed (P1, P7, P10). Data also drives validation: beyond checking completeness and
edge cases, OMDs use it to test counterfactuals, stress-test extreme values, and assess robustness. Despite this centrality,
optimization education often downplays the importance of data. As P4 reflected, “that’s always the challenge between,
you know, school textbook work and real-world work [...] even just getting test data for a proof of concept is maybe not so
simple, let alone the full application” This mismatch highlights a gap between how optimization is taught—where data is
clean, provided, and secondary—and how it is practiced, where data wrangling and interpretation are critical parts of
the workflow.
Our findings echo observations from ML that data preparation is iterative and costly [63], but optimization introduces
unique challenges. First, data shapes the form and scope of the problem itself—for example, whether to optimize over a
daily, weekly, or monthly horizon—rather than directly driving model performance. Second, optimization often relies
on unlabeled data for one-off or periodic decisions, in contrast to the labeled datasets and continuous train/test loops of
ML Operations [80]. This makes tooling designed for ML pipelines—focused on labeling [74], monitoring drift [56],
or retraining [20]—poor fits for optimization. Finally, an important exception arises when data encodes uncertainty,
such as stochastic travel times or fluctuating demands. In these cases, richer or more accurate data can materially
improve decision quality, motivating lines of research on integrating predictions into optimization models (see [57] for
a discussion).


--- Page 16 ---
16
Connor Lawless, Jakob Schoeffer, and Madeleine Udell
The central role of data underscores the need for tools that treat it as a first-class concern, not just an input to
solvers. Promising directions include (i) interfaces for early exploration of messy datasets, (ii) support for documenting
assumptions and edge cases uncovered during validation, and (iii) mechanisms linking data characteristics to modeling
choices. Such data-centered environments would complement solver-focused tools by bridging the gap between raw
organizational data and robust, trustworthy models.
5.1.2
Decisions. Our interviews reinforce that the core goal of optimization is to support better decisions. While
research has long focused on algorithms for hard decision-making problems [93], practitioners emphasized that models
are only valuable if they yield decisions that are timely, interpretable, and actionable. This pragmatism pervaded the
workflow, as OMDs balanced model fidelity, computational efficiency, and data availability in ways that reflected
organizational realities.
A central strategy was satisficing—choosing solutions that are “good enough” rather than strictly optimal (P4, P5).
Computation time and model fidelity were treated as levers to be traded off: problems might be simplified into well-
known classes with specialized solvers [66], or heuristics substituted for formal methods when speed was paramount
(P3, P7). Tools that foreground such trade-offs and let users explore multiple modeling paths—from high-fidelity but
slow to lightweight and fast—are better aligned with the decision-making realities practitioners face.
At the same time, our findings highlight both the promise and the perils of automation, echoing parallels in data
science [88]. Automation can relieve bottlenecks in data cleaning, generating solver-ready formulations and code, or
checking feasibility of solutions, but practitioners worried that it might obscure critical judgment. As P10 warned, “I
just worry when we get to talking about building a math model, we have to know if it’s wrong. Someone has to figure it out.
Somebody could be using this technology and getting the wrong answer and not know it. I think there’s a lot[...] that really
scares me when it comes to non-optimization people building models” This concern echoes research showing that users
often over-rely on automated outputs, even when they are flawed [71, 78, 84]. Some design choices—such as surfacing
assumptions, exposing inconsistencies, or citing sources—have been shown to mitigate this over-reliance [38].
Together, these findings point to a crucial design imperative: optimization tools should augment rather than automate
away human expertise. By making trade-offs visible, supporting satisficing, and embedding features that encourage
critical evaluation, decision-support tools can scaffold better decisions—not just faster ones.
5.1.3
Dialogue. Across the optimization workflow, dialogue serves as a foundational mechanism for knowledge
transfer, coordination, and co-design between OMDs and stakeholders with distributed expertise. Inputs from engineers,
business experts, and end users are integrated into the modeling process, with iterative feedback shaping problem
definitions, constraints, and objectives. In practice, stakeholders help refine priorities, surface overlooked constraints,
and guide trade-offs, while OMDs adjust models to balance feasibility, realism, and optimization rigor. This iterative,
often non-linear process mirrors the “outer loop” described by Kross and Guo [42], in which data scientists engage
continuously with clients; similarly, OMDs interact with stakeholders throughout problem framing, model development,
and validation to ensure technical solutions align with organizational goals.
Yet, as our interviews highlight, dialogue is frequently slow and time-intensive, a major bottleneck in optimization
projects. As Delarue et al. [16] note, exchanging information across expertise levels further slows progress. Despite
these challenges, effective dialogue accelerates problem understanding, guides iterative model refinement, and improves
alignment with stakeholder objectives. Novel tooling approaches should therefore prioritize accelerating dialogue—
further details are provided in Section 5.2.


--- Page 17 ---
“It Was a Magical Box”: Understanding Practitioner Workflows and Needs in Optimization
17
Our interviews reveal that effective dialogue is often strengthened by visualizations, concrete examples, and simpli-
fied scenarios, which serve as extensions of the conversation and enable stakeholders to reason about intermediate
results without requiring deep technical expertise. Making these representations accessible to stakeholders with diverse
backgrounds is essential for eliciting meaningful feedback and sustaining engagement. Transparency is closely inter-
twined with this dialogue: by making assumptions, intermediate results, and potential limitations visible, OMDs help
stakeholders understand the model’s reasoning and build trust [24, 82]. As P1 emphasized, “[It is] super important that
you’re constantly communicating with [the stakeholders], because if you leave them behind, they have zero trust in the
model.” In practice, OMDs and stakeholders evaluate candidate solutions through both direct critique and informal
“sniff tests,” interpreting outputs for plausibility and relevance. Transparent communication reduces the perception
of optimization as a “black box” (or, as P13 described it, a “magical box”) and aligns with broader challenges in AI
transparency, including the need to tailor explanations to individual stakeholder backgrounds and needs [22, 23].
Proper interface design is crucial in this context to prevent unwarranted trust while enabling stakeholders to engage
meaningfully with the model [48, 77]. Ultimately, well-designed dialogue and interfaces are key to fostering both
understanding and (appropriate) trust in complex optimization systems.
5.2
Opportunities for Tooling and Support
This section outlines opportunities for tooling and support across the optimization workflow, highlighting where
practitioners experience friction and how well-designed systems could address challenges of translation, validation,
and appropriate reliance.
Problem elicitation. A major challenge during the problem elicitation phase is that communication between OMDs
and stakeholders is often slow. Novel tools that can rapidly elicit information, clarify vague problems, or shorten
feedback loops with OMDs could therefore have a significant impact. Large language models (LLMs) offer one promising
avenue. They could assist in problem elicitation by suggesting modifications to problem definitions or generating
alternative formulations. Generative AI could also support prototyping and ideation, expanding the space of possible
approaches. For example, if a stakeholder expresses a goal like “this to go up or this to go down” (P5), an LLM could
propose a range of concrete options, from which the stakeholder could select the one that best aligns with their business
objectives without needing to directly involve the OMD. At the same time, there are serious concerns around trust and
verifiability [47, 52]. LLM outputs can appear plausible while being incorrect, and non-experts often lack the means
to validate them [49]. This underscores the need for mechanisms that support creative exploration while providing
safeguards for error detection and correction. Ensuring that LLM suggestions can be translated into verifiable examples
or otherwise validated will be crucial for their effective use.
Data processing. Practitioners consistently described data handling as a major pain point. Formatting, pre-processing,
and connecting fragmented tools and systems were recurring barriers, and interfacing solvers with simulations or
databases was often difficult. Many current approaches are not wired to live data sources, limiting their usefulness
in practice. A data-centric AI perspective suggests that improving quality, integration, and accessibility may matter
more than refining models themselves [37, 97], yet current workflows rarely prioritize this principle. Waiting on data
also emerged as a significant blocker. As P5 explained, “if one could use generative AI to generate [data] samples, even
if they’re not quite right, that certainly would have made validation a lot simpler [...] while I’m waiting for [the data
programmer] to do that work, I could get simpler cut-down versions that look realistic enough [...] that person wouldn’t have
been a bottleneck for me to get at least some test data.” Tools that generate synthetic data or test scenarios could help


--- Page 18 ---
18
Connor Lawless, Jakob Schoeffer, and Madeleine Udell
practitioners validate toy examples and reduce data-related bottlenecks. Finally, practitioners highlighted the need for
models to adapt dynamically as data changes, reducing the manual effort currently required to reconfigure pipelines.
Model development. Translating natural language problem descriptions into formal optimization models (Figure 2)
remains a significant challenge, but also a major opportunity for tooling. LLM-based approaches show promise in
converting requirements into mathematical constraints (e.g., [46]), yet they rarely address pragmatic aspects such
as developing decomposition algorithms or heuristics. Transparency will be critical here, though optimization has
an advantage over ML: constraints are generally easier to verify, making models more interpretable than highly
parameterized predictors. Beyond translation, OMDs pointed to needs around exploration and refinement. Tools that
map problems to canonical formulations [66], scaffold new models using insights from the literature, or manage
simplifications to improve tractability could be especially valuable. Practitioners also envisioned environments that
support quick scenario analysis—e.g., “what happens to Y if I change X?”—and allow iterative adjustment of parameters
and objective weights. Such capabilities would create feedback loops that help practitioners refine models more efficiently
and with greater confidence.
Model implementation. Much effort during implementation is spent on boilerplate coding and manually translating
conceptual formulations into solver-ready inputs, pointing to the need for interfaces that streamline this translation
and reduce syntax burdens. Practitioners also described difficulty navigating solver functionality—knowing which
parameters to tune, or how to configure them for different problem classes—highlighting opportunities for tools
that scaffold parameter selection and make solver capabilities more transparent. Because implementation is iterative,
evolving with new data and stakeholder input, tools that support rapid refinement, lightweight experimentation, and
smoother integration with data pipelines would help modelers adapt as problems change. These opportunities parallel
the focus on versioning in ML operations [80]: just as ML engineers track changes in data and pipelines, optimization
practitioners could benefit from tools that log evolving problem formulations, constraint modifications, and solver
configurations to ensure reliability and reproducibility.
Model validation. Validation underscores the need for tools that make optimization models transparent, communicable,
and open to critique. Practitioners emphasized that validation extends beyond solver feasibility to assessing whether
results make sense in the domain and under realistic conditions. Problem-specific visualizations—such as routing
maps or sample schedules—can help stakeholders interpret outputs, while test scenarios probe robustness and expose
potential problems in the formulation or data. These opportunities align with the “visibility” dimension in ML operations
[80]: optimization tooling must surface assumptions, illustrate trade-offs, and make results legible across expertise
boundaries. Crucially, visibility should also foster appropriate reliance [76, 79]. OMDs warned against over-trusting
polished outputs built on flawed assumptions, highlighting the need for mechanisms that foreground uncertainty, link
decisions back to assumptions, and generate comprehensive test cases. Such features, together with efforts to make
optimization algorithms themselves more interpretable [15, 29, 40, 44], can encourage users across expertise levels to
critically validate models.
Deployment. Deployment revealed two persistent gaps in support. OMDs struggled to document model assumptions,
trade-offs, and dependencies, often relying on ad hoc notes or one-off presentations that left critical knowledge
vulnerable to loss as projects evolved. Structured documentation practices, akin to model cards or datasheets in ML
[27, 62], could help capture constraints, solver choices, and parameter settings in a reusable form. Portability was
another challenge: switching solvers or environments often required near-total re-implementation. While general


--- Page 19 ---
“It Was a Magical Box”: Understanding Practitioner Workflows and Needs in Optimization
19
abstractions could reduce this burden, they risk hiding solver-specific functionality that experts depend on. The difficulty
is compounded by reliance on commercial solvers like Gurobi [2] and CPLEX [64], in contrast to the open-source
portability of ML frameworks such as scikit-learn [70] and PyTorch [68].
5.3
Limitations
Our study offers an important first step toward understanding optimization practice, but it is limited in several ways.
We focused on expert optimization practitioners, rather than non-expert users who might engage with optimization
systems more peripherally, leaving questions of accessibility and usability for non-specialists outside the scope of
this work. Relatedly, our analysis emphasized high-level workflows across projects rather than going deeply into any
single stage or tool (e.g., data documentation). Our focus was also limited to model developers themselves, excluding
other stakeholders such as managers, domain experts, or end-users of optimization outputs. This choice allowed us to
surface the technical and organizational challenges specific to modeling, but it leaves open how other stakeholders
experience or influence the workflow. In addition, although the OMDs varied in company size, industry, and educational
background, we did not systematically analyze subgroup differences. Finally, our lens was centered on human-centered
workflows—how practitioners structure and navigate projects—rather than other approaches to eliciting user input,
such as interactive optimization.
6
Conclusion
Optimization holds tremendous promise for improving decision-making in domains such as healthcare, logistics, and
supply chains. From the outside, optimization software can appear to function as a “magical box,” but our study
shows that successful practice depends less on solvers alone and more on the expertise, trade-offs, and collaborations
that surround them. Through interviews with 15 optimization model developers, we surfaced the iterative nature of
optimization workflows and highlighted three themes—data, decisions, and dialogue—that cut across all stages. These
themes underscore that optimization is not only a technical exercise but also a socio-technical process grounded in
organizational realities.
By characterizing the unique challenges of optimization practice, our work extends HCI scholarship beyond prior
accounts of ML workflows. In optimization, data is not simply an input but a pervasive constraint that shapes formula-
tions; decisions are not about prediction accuracy but about satisficing within trade-offs; and dialogue is not incidental
but constitutive, enabling co-design, trust, and adoption. These distinctions carry important design implications: future
support tools should (i) foreground data exploration and assumption tracking, (ii) scaffold satisficing and make trade-offs
visible, and (iii) embed interactive, transparent mechanisms that sustain dialogue between developers and stakeholders.
Taken together, our findings point toward a new generation of human-centered optimization systems—ones that shift
focus from solver-centric pipelines to workflows that are interpretable, collaborative, and aligned with decision-making
in practice.
References
[1] Tobias Achterberg. 2009. SCIP: solving constraint integer programs. Mathematical Programming Computation 1, 1 (2009), 1–41.
[2] Tobias Achterberg. 2019. What’s new in gurobi 9.0. Webinar Talk url: https://www. gurobi. com/wp-content/uploads/2019/12/Gurobi-90-Overview-
Webinar-Slides-1. pdf 5, 9 (2019), 97–113.
[3] Ali AhmadiTeshnizi, Wenzhi Gao, Herman Brunborg, Shayan Talaei, Connor Lawless, and Madeleine Udell. 2024. OptiMUS-0.3: Using large
language models to model and solve optimization problems at scale. arXiv preprint arXiv:2407.19633 (2024).


--- Page 20 ---
20
Connor Lawless, Jakob Schoeffer, and Madeleine Udell
[4] Ali AhmadiTeshnizi, Wenzhi Gao, and Madeleine Udell. 2024. OptiMUS: Scalable Optimization Modeling with (MI) LP Solvers and Large Language
Models. In Forty-first International Conference on Machine Learning.
[5] Nicolás Astorga, Tennison Liu, Yuanzhang Xiao, and Mihaela van der Schaar. 2025. Autoformulation of Mathematical Optimization Models Using
LLMs. In Forty-second International Conference on Machine Learning.
[6] Aqeel Ahmed Bazmi and Gholamreza Zahedi. 2011. Sustainable energy systems: Role of optimization modeling techniques in power generation and
supply—A review. Renewable and sustainable energy reviews 15, 8 (2011), 3480–3500.
[7] Dimitris Bertsimas and Jean Pauphilet. 2024. Hospital-wide inpatient flow optimization. Management Science 70, 7 (2024), 4893–4911.
[8] Avinash Bhat, Austin Coursey, Grace Hu, Sixian Li, Nadia Nahar, Shurui Zhou, Christian Kästner, and Jin LC Guo. 2023. Aspirations and practice
of ml model documentation: Moving the needle with nudging and traceability. In Proceedings of the 2023 CHI Conference on Human Factors in
Computing Systems. 1–17.
[9] Julien Bramel and David Simchi-Levi. 1998. The logic of logistics: theory, algorithms and applications for logistics management. Springer Berlin.
[10] Edward Burnell, Nicole B Damen, and Warren Hoburg. 2020. GPkit: A human-centered approach to convex optimization in engineering design. In
Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1–13.
[11] Kathy Charmaz. 2006. Constructing grounded theory: A practical guide through qualitative analysis. sage.
[12] Hao Chen, Gonzalo Esteban Constante-Flores, Krishna Sri Ipsit Mantri, Sai Madhukiran Kompalli, Akshdeep Singh Ahluwalia, and Can Li. 2025.
OptiChat: Bridging Optimization Models and Practitioners with Large Language Models. arXiv preprint arXiv:2501.08406 (2025).
[13] Constantine Aaron Cois, Joseph Yankel, and Anne Connell. 2014. Modern DevOps: Optimizing software development through effective system
interactions. In 2014 IEEE International Professional Communication Conference (IPCC). IEEE, 1–7.
[14] Michele Conforti, Gerard Cornuejols, and Giacomo Zambelli. 2014. Integer programming. Springer.
[15] Kristijonas Čyras, Dimitrios Letsios, Ruth Misener, and Francesca Toni. 2019. Argumentation for explainable scheduling. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 33. 2752–2759.
[16] Arthur Delarue, Zhen Lian, and Sebastien Martin. 2024. Algorithmic precision and human decision: A study of interactive optimization for school
schedules. Available at SSRN 4324076 (2024).
[17] Wesley Hanwen Deng, Manish Nagireddy, Michelle Seng Ah Lee, Jatinder Singh, Zhiwei Steven Wu, Kenneth Holstein, and Haiyi Zhu. 2022.
Exploring how machine learning practitioners (try to) use fairness toolkits. In Proceedings of the 2022 ACM Conference on Fairness, Accountability,
and Transparency. 473–484.
[18] Guy Desaulniers, Jacques Desrosiers, and Marius M Solomon. 2006. Column generation. Vol. 5. Springer Science & Business Media.
[19] Steven Diamond and Stephen Boyd. 2016. CVXPY: A Python-embedded modeling language for convex optimization. Journal of Machine Learning
Research 17, 83 (2016), 1–5.
[20] Tom Diethe, Tom Borchert, Eno Thereska, Borja Balle, and Neil Lawrence. 2019. Continual learning in practice. arXiv preprint arXiv:1903.05202
(2019).
[21] Iain Dunning, Joey Huchette, and Miles Lubin. 2017. JuMP: A modeling language for mathematical optimization. SIAM review 59, 2 (2017), 295–320.
[22] Upol Ehsan, Samir Passi, Q Vera Liao, Larry Chan, I-Hsiang Lee, Michael Muller, and Mark O Riedl. 2024. The who in XAI: How AI background
shapes perceptions of AI explanations. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 1–32.
[23] Upol Ehsan, Philipp Wintersberger, Q Vera Liao, Martina Mara, Marc Streit, Sandra Wachter, Andreas Riener, and Mark O Riedl. 2021. Operationalizing
human-centered perspectives in explainable AI. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. 1–6.
[24] Andrea Ferrario and Michele Loi. 2022. How explainability contributes to trust in AI. In Proceedings of the 2022 ACM conference on fairness,
accountability, and transparency. 1457–1466.
[25] Matteo Fischetti, Andrea Lodi, et al. 2010. Heuristics in mixed integer programming. Wiley Encyclopedia of Operations Research and Management
Science. John Wiley & Sons, Inc (2010), 2–23.
[26] Robert Fourer, David M Gay, and Brian W Kernighan. 1990. AMPL: A mathematical programming language. Management Science 36, 5 (1990),
519–554.
[27] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. 2021.
Datasheets for datasets. Commun. ACM 64, 12 (2021), 86–92.
[28] Frederic Gmeiner, Nicolai Marquardt, Michael Bentley, Hugo Romat, Michel Pahud, David Brown, Asta Roseway, Nikolas Martelaro, Kenneth
Holstein, Ken Hinckley, et al. 2025. Intent tagging: Exploring micro-prompting interactions for supporting granular human-GenAI co-creation
workflows. arXiv preprint arXiv:2502.18737 (2025).
[29] Marc Goerigk and Michael Hartisch. 2023. A framework for inherently interpretable optimization models. European Journal of Operational Research
310, 3 (2023), 1312–1324.
[30] Gurobi Optimization. 2025. GurobiPy. https://www.gurobi.com/faqs/gurobipy/
[31] Raimo P Hämäläinen, Jukka Luoma, and Esa Saarinen. 2013. On the importance of behavioral operational research: The case of understanding and
communicating about dynamic systems. European Journal of Operational Research 228, 3 (2013), 623–634.
[32] Amy K Heger, Liz B Marquis, Mihaela Vorvoreanu, Hanna Wallach, and Jennifer Wortman Vaughan. 2022. Understanding machine learning
practitioners’ data documentation perceptions, needs, challenges, and desiderata. Proceedings of the ACM on Human-Computer Interaction 6, CSCW2
(2022), 1–29.


--- Page 21 ---
“It Was a Magical Box”: Understanding Practitioner Workflows and Needs in Optimization
21
[33] Kenneth Holstein, Jennifer Wortman Vaughan, Hal Daumé III, Miro Dudik, and Hanna Wallach. 2019. Improving fairness in machine learning
systems: What do industry practitioners need?. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1–16.
[34] Chenyu Huang, Zhengyang Tang, Shixi Hu, Ruoqing Jiang, Xin Zheng, Dongdong Ge, Benyou Wang, and Zizhuo Wang. 2025. ORLM: A customizable
framework in training large models for automated optimization modeling. Operations Research (2025).
[35] Chenyu Huang, Zhengyang Tang, Shixi Hu, Ruoqing Jiang, Xin Zheng, Dongdong Ge, Benyou Wang, and Zizhuo Wang. 2025. Orlm: A customizable
framework in training large models for automated optimization modeling. Operations Research (2025).
[36] INFORMS. 2025. INFORMS Analytics Framework. https://info.informs.org/analytics-framework/
[37] Johannes Jakubik, Michael Vössing, Niklas Kühl, Jannis Walk, and Gerhard Satzger. 2024. Data-centric artificial intelligence. Business & Information
Systems Engineering 66, 4 (2024), 507–515.
[38] Sunnie SY Kim, Jennifer Wortman Vaughan, Q Vera Liao, Tania Lombrozo, and Olga Russakovsky. 2025. Fostering appropriate reliance on large
language models: The role of explanations, sources, and inconsistencies. In Proceedings of the 2025 CHI Conference on Human Factors in Computing
Systems. 1–19.
[39] Gunnar W Klau, Neal Lesh, Joe Marks, and Michael Mitzenmacher. 2010. Human-guided search. Journal of Heuristics 16, 3 (2010), 289–310.
[40] Anton Korikov and J Christopher Beck. 2021. Counterfactual explanations via inverse constraint programming. In 27th International Conference on
Principles and Practice of Constraint Programming (CP 2021). Schloss Dagstuhl–Leibniz-Zentrum für Informatik, 35–1.
[41] Dominik Kreuzberger, Niklas Kühl, and Sebastian Hirschl. 2023. Machine learning operations (MLOps): Overview, definition, and architecture. IEEE
Access 11 (2023), 31866–31879.
[42] Sean Kross and Philip Guo. 2021. Orienting, framing, bridging, magic, and counseling: How data scientists navigate the outer loop of client
collaborations in industry and academia. Proceedings of the ACM on Human-Computer Interaction 5, CSCW2 (2021), 1–28.
[43] Martin Kunc, Jonathan Malpass, and Leroy White. 2016. Behavioral operational research: Theory, methodology and practice. Springer.
[44] Jannis Kurtz, Ş İlker Birbil, and Dick den Hertog. 2025. Counterfactual explanations for linear optimization. European Journal of Operational
Research (2025).
[45] Connor Lawless, Yingxi Li, Anders Wikum, Madeleine Udell, and Ellen Vitercik. 2025. Llms for cold-start cutting plane separator configuration. In
International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research. Springer, 51–69.
[46] Connor Lawless, Jakob Schoeffer, Lindy Le, Kael Rowan, Shilad Sen, Cristina St. Hill, Jina Suh, and Bahareh Sarrafzadeh. 2024. “I Want It That Way”:
Enabling Interactive Decision Support Using Large Language Models and Constraint Programming. ACM Transactions on Interactive Intelligent
Systems 14, 3 (2024), 1–33.
[47] Christine P Lee, David Porfirio, Xinyu Jessica Wang, Kevin Chenkai Zhao, and Bilge Mutlu. 2025. Veriplan: Integrating formal verification and llms
into end-user planning. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. 1–19.
[48] John D Lee and Katrina A See. 2004. Trust in automation: Designing for appropriate reliance. Human factors 46, 1 (2004), 50–80.
[49] Florian Leiser, Sven Eckhardt, Valentin Leuthe, Merlin Knaeble, Alexander Maedche, Gerhard Schwabe, and Ali Sunyaev. 2024. Hill: A hallucination
identifier for large language models. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 1–13.
[50] Beibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, and Ishai Menache. 2023. Large language models for supply chain optimization. arXiv
preprint arXiv:2307.03875 (2023).
[51] Q Vera Liao, Hariharan Subramonyam, Jennifer Wang, and Jennifer Wortman Vaughan. 2023. Designerly understanding: Information needs for
model transparency to support design ideation for AI-powered user experience. In Proceedings of the 2023 CHI Conference on Human Factors in
Computing Systems. 1–21.
[52] Q Vera Liao and Jennifer Wortman Vaughan. 2023. AI transparency in the age of llms: A human-centered research roadmap. arXiv preprint
arXiv:2306.01941 (2023).
[53] Jie Liu, Tim Dwyer, Guido Tack, Samuel Gratzl, and Kim Marriott. 2020. Supporting the problem-solving loop: Designing highly interactive
optimisation systems. IEEE Transactions on Visualization and Computer Graphics 27, 2 (2020), 1764–1774.
[54] Jie Liu, Kim Marriott, Tim Dwyer, and Guido Tack. 2023. Increasing user trust in optimisation through feedback and interaction. ACM Transactions
on Computer-Human Interaction 29, 5 (2023), 1–34.
[55] Michael Madaio, Lisa Egede, Hariharan Subramonyam, Jennifer Wortman Vaughan, and Hanna Wallach. 2022. Assessing the fairness of AI systems:
AI practitioners’ processes, challenges, and needs for support. Proceedings of the ACM on Human-Computer Interaction 6, CSCW1 (2022), 1–26.
[56] Ankur Mallick, Kevin Hsieh, Behnaz Arzani, and Gauri Joshi. 2022. Matchmaker: Data drift mitigation in machine learning for large-scale systems.
Proceedings of Machine Learning and Systems 4 (2022), 77–94.
[57] Jayanta Mandi, James Kotary, Senne Berden, Maxime Mulamba, Victor Bucarey, Tias Guns, and Ferdinando Fioretto. 2024. Decision-focused learning:
Foundations, state of the art, benchmark and future opportunities. Journal of Artificial Intelligence Research 80 (2024), 1623–1701.
[58] Beatriz MA Matsui and Denise H Goya. 2022. MLOps: Five steps to guide its effective implementation. In Proceedings of the 1st International
Conference on AI Engineering: Software Engineering for AI. 33–34.
[59] Nora McDonald, Sarita Schoenebeck, and Andrea Forte. 2019. Reliability and inter-rater reliability in qualitative research: Norms and guidelines for
CSCW and HCI practice. Proceedings of the ACM on Human-Computer Interaction 3, CSCW (2019), 1–23.
[60] David Meignan, Sigrid Knust, Jean-Marc Frayret, Gilles Pesant, and Nicolas Gaud. 2015. A review and taxonomy of interactive optimization methods
in operations research. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 3 (2015), 1–43.


--- Page 22 ---
22
Connor Lawless, Jakob Schoeffer, and Madeleine Udell
[61] Kaisa Miettinen, Francisco Ruiz, and Andrzej P Wierzbicki. 2008. Introduction to multiobjective optimization: interactive approaches. In Multiobjective
optimization: interactive and evolutionary approaches. Springer, 27–57.
[62] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and
Timnit Gebru. 2019. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency. 220–229.
[63] Michael Muller, Ingrid Lange, Dakuo Wang, David Piorkowski, Jason Tsay, Q Vera Liao, Casey Dugan, and Thomas Erickson. 2019. How data
science workers work with data: Discovery, capture, curation, design, creation. In Proceedings of the 2019 CHI Conference on Human Factors in
Computing Systems. 1–15.
[64] Stefan Nickel, Claudius Steinhardt, Hans Schlenker, and Wolfgang Burkart. 2022. Decision Optimization with IBM ILOG CPLEX Optimization
Studio. Angewandte Optimierung mit IBM ILOG CPLEX Optimization Studio; Springer: Berlin/Heidelberg, Germany (2022).
[65] Jorge Nocedal and Stephen J Wright. 2006. Numerical optimization. Springer.
[66] Axel Parmentier. 2022. Learning to approximate industrial problems by operations research classic problems. Operations Research 70, 1 (2022),
606–623.
[67] Samir Passi and Steven J Jackson. 2018. Trust in data science: Collaboration, translation, and accountability in corporate data science projects.
Proceedings of the ACM on human-computer interaction 2, CSCW (2018), 1–28.
[68] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32
(2019).
[69] Kayur Patel, James Fogarty, James A Landay, and Beverly Harrison. 2008. Investigating statistical machine learning as a tool for software development.
In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 667–676.
[70] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in Python. the Journal of machine Learning research 12 (2011), 2825–2830.
[71] Han Qiao, Jo Vermeulen, George Fitzmaurice, and Justin Matejka. 2025. To Use or Not to Use: Impatience and Overreliance When Using Generative
AI Productivity Support Tools. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. 1–18.
[72] Ragheb Rahmaniani, Teodor Gabriel Crainic, Michel Gendreau, and Walter Rei. 2017. The Benders decomposition algorithm: A literature review.
European Journal of Operational Research 259, 3 (2017), 801–817.
[73] Rindranirina Ramamonjison, Timothy Yu, Raymond Li, Haley Li, Giuseppe Carenini, Bissan Ghaddar, Shiqi He, Mahdi Mostajabdaveh, Amin
Banitalebi-Dehkordi, Zirui Zhou, et al. 2023. Nl4opt competition: Formulating optimization problems based on their natural language descriptions.
In NeurIPS 2022 competition track. PMLR, 189–203.
[74] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher Ré. 2017. Snorkel: Rapid training data creation with
weak supervision. In Proceedings of the VLDB endowment. International conference on very large data bases, Vol. 11. 269.
[75] Jeffrey S Saltz. 2021. CRISP-DM for data science: Strengths, weaknesses and potential next steps. In 2021 IEEE International Conference on Big Data
(Big Data). IEEE, 2337–2344.
[76] Max Schemmer, Niklas Kuehl, Carina Benz, Andrea Bartos, and Gerhard Satzger. 2023. Appropriate reliance on AI advice: Conceptualization and
the effect of explanations. In Proceedings of the 28th International Conference on Intelligent User Interfaces. 410–422.
[77] Nadine Schlicker, Kevin Baum, Alarith Uhde, Sarah Sterz, Martin C Hirsch, and Markus Langer. 2025. How do we assess the trustworthiness of AI?
Introducing the trustworthiness assessment model (TrAM). Computers in Human Behavior 170 (2025), 108671.
[78] Jakob Schoeffer, Maria De-Arteaga, and Niklas Kuehl. 2024. Explanations, fairness, and appropriate reliance in human-AI decision-making. In
Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 1–18.
[79] Jakob Schoeffer, Johannes Jakubik, Michael Vössing, Niklas Kühl, and Gerhard Satzger. 2025. AI reliance and decision quality: Fundamentals,
interdependence, and the effects of interventions. Journal of Artificial Intelligence Research 82 (2025), 471–501.
[80] Shreya Shankar, Rolando Garcia, Joseph M Hellerstein, and Aditya G Parameswaran. 2024. “We have no idea how models will behave in production
until production”: How engineers operationalize machine learning. Proceedings of the ACM on Human-Computer Interaction 8, CSCW1 (2024), 1–34.
[81] Helen Sharp, Robert Biddle, Phil Gray, Lynn Miller, and Jeff Patton. 2006. Agile development: opportunity or fad?. In CHI’06 Extended Abstracts on
Human Factors in Computing Systems. 32–35.
[82] Donghee Shin. 2021. The effects of explainability and causability on perception, trust, and acceptance: Implications for explainable AI. International
journal of human-computer studies 146 (2021), 102551.
[83] Stefan Studer, Thanh Binh Bui, Christian Drescher, Alexander Hanuschkin, Ludwig Winkler, Steven Peters, and Klaus-Robert Müller. 2021. Towards
CRISP-ML(Q): A machine learning process model with quality assurance methodology. Machine Learning and Knowledge Extraction 3, 2 (2021),
392–413.
[84] Siddharth Swaroop, Zana Buçinca, Krzysztof Z Gajos, and Finale Doshi-Velez. 2025. Personalising AI assistance based on overreliance rate in
AI-assisted decision making. In Proceedings of the 30th International Conference on Intelligent User Interfaces. 1107–1122.
[85] Desiree Sy and Lynn Miller. 2008. Optimizing agile user-centred design. In CHI’08 extended abstracts on Human factors in computing systems.
3897–3900.
[86] Madeleine Udell, Karanveer Mohan, David Zeng, Jenny Hong, Steven Diamond, and Stephen Boyd. 2014. Convex optimization in Julia. In 2014 first
workshop for high performance technical computing in dynamic languages. IEEE, 18–28.


--- Page 23 ---
“It Was a Magical Box”: Understanding Practitioner Workflows and Needs in Optimization
23
[87] François Vanderbeck and Martin WP Savelsbergh. 2006. A generic view of Dantzig–Wolfe decomposition in mixed integer programming. Operations
Research Letters 34, 3 (2006), 296–306.
[88] Dakuo Wang, Justin D Weisz, Michael Muller, Parikshit Ram, Werner Geyer, Casey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray.
2019. Human-AI collaboration in data science: Exploring data scientists’ perceptions of automated AI. Proceedings of the ACM on Human-Computer
Interaction 3, CSCW (2019), 1–24.
[89] Segev Wasserkrug, Leonard Boussioux, Dick den Hertog, Farzaneh Mirzazadeh, Ilker Birbil, Jannis Kurtz, and Donato Maragno. 2024. From large
language models and optimization to decision optimization copilot: A research manifesto. arXiv preprint arXiv:2402.16269 (2024).
[90] Amy Winecoff and Miranda Bogen. 2025. Improving governance outcomes through AI documentation: Bridging theory and practice. In Proceedings
of the 2025 CHI Conference on Human Factors in Computing Systems. 1–18.
[91] Wayne L Winston. 2004. Operations research: applications and algorithm. Thomson Learning, Inc.
[92] Rüdiger Wirth and Jochen Hipp. 2000. CRISP-DM: Towards a standard process model for data mining. In Proceedings of the 4th International
Conference on the Practical Applications of Knowledge Discovery and Data Mining, Vol. 1. Manchester, 29–39.
[93] Laurence A Wolsey. 2020. Integer programming. John Wiley & Sons.
[94] Ziyang Xiao, Dongxiang Zhang, Yangjun Wu, Lilin Xu, Yuan Jessica Wang, Xiongwei Han, Xiaojin Fu, Tao Zhong, Jia Zeng, Mingli Song, et al. 2023.
Chain-of-Experts: When LLMs Meet Complex Operations Research Problems. In The Twelfth International Conference on Learning Representations.
[95] Doris Xin, Eva Yiwei Wu, Doris Jung-Lin Lee, Niloufar Salehi, and Aditya Parameswaran. 2021. Whither AutoML? Understanding the role of
automation in machine learning workflows. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1–16.
[96] Qian Yang, Jina Suh, Nan-Chen Chen, and Gonzalo Ramos. 2018. Grounding interactive machine learning tool design in how non-experts actually
build models. In Proceedings of the 2018 Designing Interactive Systems Conference. 573–584.
[97] Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, and Xia Hu. 2025. Data-centric artificial intelligence:
A survey. Comput. Surveys 57, 5 (2025), 1–42.
[98] Haotian Zhai, Connor Lawless, Ellen Vitercik, and Liu Leqi. 2025. EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of Optimization
Formulations. In Forty-second International Conference on Machine Learning.
[99] Amy X Zhang, Michael Muller, and Dakuo Wang. 2020. How do data science workers collaborate? Roles, workflows, and tools. Proceedings of the
ACM on Human-Computer Interaction 4, CSCW1 (2020), 1–23.
A
Interview Protocol
A.1
Welcome
Introduction The goal of this study is to understand the lifecycle of creating and deploying a (mixed-integer) linear
programming optimization model to solve real-world problems. You are asked to participate in one 1-hour session for
this study.
During the study we’ll ask you to describe your background and walk us through some of your previous optimization
projects. The goal is to understand your process, how you interface with different stakeholders, and any challenges and
pain points. You do not have to disclose any sensitive or private details about the project including specific companies,
applications, or intellectual property. For example, instead of saying you built a production planning model to maximize
throughput of Coca Cola at an Atlanta production facility, you can just mention you worked on a model to maximize
production of a consumer good at a factory. Throughout the process we’ll be asking you to think-aloud to help us
understand your thought processes and pain points with your current process for building an optimization model. Do
you have any questions?
Consent to record Before we begin, do we have your permission to record the audio of this conversation for the
purposes indicated in the consent form that you signed?
A.2
Background and Experience (5 Minutes)
We are going to start by learning more about your background and experience with modeling and deploying (mixed-
integer) linear programming models in practice.
• What’s your background in optimization modeling?
• What do you and your team do?


--- Page 24 ---
24
Connor Lawless, Jakob Schoeffer, and Madeleine Udell
• Can you ballpark how many (mixed-integer) linear programming optimization models you’ve helped develop,
tweak, or implement during your career? What parts of the modeling process have you worked on?
• How long have you been an OR practitioner?
• Have you worked with other types of optimization models (e.g., queuing systems) or other analytics tools like
machine learning or data dashboards?
If the interview is conducted over Zoom, we will include a short overview of Google Slides and its functionality here.
Specifically, the interviewer will show the user how to add a shape, annotate it with text, and draw arrows between
shapes.
A.3
Retrospective Think-Aloud
For the next part of the interview, I want you to think about one project in which you had to interact with an optimization
model. Try to think about an end-to-end project where you worked on the optimization model from conception to
completion if possible.
(1) At a high-level, what is the pipeline or workflow you followed (starting from the problem to the final model)?
While you think about your process please use the whiteboard and markers in front of you to diagram the
different stages of your workflow. Feel free to annotate this diagram with any details about the stage that you
think are important to note.
• How did you learn about the business problem you were trying to solve? Was the problem ambiguous or
unclear? How did you resolve any ambiguity/translate the problem into a concrete optimization model?
Were there any surprises when you were modeling the program (e.g., business owners with different
definitions/constraints than you expected)?
• What tools∗did you use for the different stages of this workflow?
• Were there any team members or external teams you had to work with? What part of the process did you
have to liaise with them?
• How did the optimization model relate to other aspects of the broader project?
• Was the final optimization model used in practice (by the client or your business)? If not, why not?
(2) How did you evaluate that the model you built was correct? How did you go about debugging it?
• What metrics, if any, did you look at?
• Did you have to show the model to any other teams/stakeholders? How did you convince them this was
the correct model?
• What challenges did you face evaluating the model?
(3) Looking at the entire workflow, what are the biggest challenges you faced?
• How would you improve the workflow of this project?
• Approximately what fraction of the total time of the project did each stage take?
• Were there any major blockers that slowed down the project—what stage did they occur in?
• Is there anything else about the optimization modeling process you think is important for us to know?
(4) ∗For every tool mentioned during the life cycle:
• How familiar are you with the tool? How frequently do you use it?
• Are there any challenges in using the tool?
• Is there information or functionality you wish the tool had that it currently does not?


--- Page 25 ---
“It Was a Magical Box”: Understanding Practitioner Workflows and Needs in Optimization
25
• How are you using your time within the tool?
A.4
Comparison With Other Tasks
Now I want you to think about a different project.
If they indicated in the first section that they’ve done machine learning projects replace [PROJECT TYPE] with “machine
learning”. Otherwise, if they indicated in the first section that they’ve done projects with other modeling types (e.g., queuing
systems, simulation), replace [PROJECT TYPE] with “other optimization modeling paradigms”.
If additional project type: You mentioned earlier that you have also worked on projects that involved building a
[PROJECT TYPE] model.
• Can you think of one such project?
• Can you briefly describe the project including the problem you were trying to solve?
• Compare the workflow for that project versus the (mixed-integer) linear programming optimization project we
just discussed. How does it differ from the workflow you outlined for the last project?
• What are the biggest differences between working with a [PROJECT TYPE] model vs. a (mixed-integer) linear
programming model?
Otherwise: Now I want you to think about a different (mixed-integer) linear programming project. Let’s compare it
to the workflow we just outlined for your first project:
• Can you briefly describe the project including the problem you were trying to solve?
• Think about your workflow for this project—how does it differ from the workflow you outlined for the last
project?
Time permitting:
• Can you think of a project that started out as an optimization modeling project but ended up with a different
deliverable (e.g., dashboard) or vice versa?
• What caused the project to change form? What were the major challenges?
