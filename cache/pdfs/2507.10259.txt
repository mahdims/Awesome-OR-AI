--- Page 1 ---
JOURNAL OF LATEX CLASS FILES, 2025
1
Temporal-Aware GPU Resource Allocation for Distributed
LLM Inference via Reinforcement Learning
Chengze Du1, Zhiwei Yu1, Heng Xu1, Haojie Wang2, Bo Liu1, Jialong Li1,B
1 Computer Science and Control Engineering, Shenzhen University of Advanced Technology, Shenzhen, China
2 China Mobile Research Institute, Beijing, China
B: lijialong@suat-sz.edu.cn
Abstract—The rapid growth of large language model (LLM)
services imposes increasing demands on distributed GPU infer-
ence infrastructure. Most existing scheduling systems follow a
reactive paradigm, relying solely on the current system state to
make decisions, without considering how task demand and re-
source availability evolve over time. This lack of temporal aware-
ness in reactive approaches leads to inefficient GPU utilization,
high task migration overhead, and poor system responsiveness
under dynamic workloads. In this work, we identify the funda-
mental limitations of these instantaneous-state-only scheduling
approaches and propose Temporal Optimal Resource schedul-
ing via Two-layer Architecture (TORTA). TORTA introduces a
spatiotemporal scheduling framework that captures both long-
term workload patterns and short-term execution constraints.
It adopts a two-layer design: a macro-level scheduler leverages
reinforcement learning and optimal transport to coordinate inter-
region task distribution, while a micro-level allocator refines task-
to-server assignments within each region to reduce latency and
switching costs. Experimental results across multiple network
topologies show that TORTA reduces average inference response
time by up to 15%, improves load balance by approximately
4-5%, and cuts total operational cost by 10-20% compared to
state-of-the-art baseline methods.
Index Terms—Data Center Network, Reinforcement Learning,
LLM Requests Schedule, Optimal Transport
I. INTRODUCTION
T
HE rise of large language models (LLMs) has influ-
enced people’s lives, such as in text analysis and im-
age generation [1, 5, 30, 38]. Most LLM inference tasks
heavily rely on GPU resources. For example, models like
those developed by OpenAI and Anthropic (Claude) require
substantial GPU computing power for training and inference,
consuming significant energy and computational resources to
process complex tasks efficiently [29]. Due to geographical,
political, and other factors (e.g., economic disparities and
infrastructure limitations), the distribution of GPU resources
is often uneven compared to user demand [28, 39]. As shown
in Figure 1, global GPU resource distribution is polarized,
with most resources concentrated in a few countries and
regions (the numbers in the figure represent the number of
server clusters) [21]. This imbalance significantly affects the
quality of LLM product experiences. For instance, some LLM
providers frequently fail to deliver timely services to users
due to insufficient resources. This issue requires scheduling
algorithms to better match supply and demand, accelerate the
completion of GPU inference tasks, and thereby enhance user
satisfaction with LLM service providers.
GPU Clusters
Cluster Link
Fig. 1: Temporal-Aware GPU resource allocation.
Accelerating GPU inference can be divided into two cat-
egories. The first is local acceleration, which involves opti-
mizing the GPU’s operational mode. This includes techniques
such as optimizing checkpoint loading, introducing real-time
migration and intelligent scheduling strategies, or employing
model parallelism and statistical multiplexing to reduce un-
necessary overhead [8, 23, 24, 54]. The second category is
distributed resource scheduling, which aims to improve GPU
resource utilization and prevent GPU server idleness [49].
Some approaches focus on allocating suitable GPU resources
to different tasks to achieve the fastest inference, often with
additional optimization objectives such as minimizing com-
munication costs and power consumption [16, 40].
The complexity of this scheduling problem is often very
high. Finding an optimal scheduling solution typically requires
significant computational time and increases dramatically with
network scale expansion, severely impacting task completion
time. Existing approaches usually seek approximately opti-
mal solutions within acceptable time constraints, and these
methods have achieved good results to a certain extent. For
example, MLServerless [8] employs an offline-plus-online
strategy, where the offline phase invests substantial effort in
learning solution strategies, enabling the system to quickly
derive good solutions during the online phase to achieve
optimal inference speed. Other approaches [16, 40] model this
as a Mixed Integer Linear Programming (MILP) or Integer
Quadratic Programming (IQP) problem and accelerate solution
speed through solution space pruning. However, most of
these approaches are ”Reactive”, making immediate decisions
based on currently observed states (such as cluster interference
levels, resource utilization rates, and energy prices), essentially
finding optimal solutions within individual time slots. This
paradigm inherently ignores the temporal perspective and
arXiv:2507.10259v2  [cs.DC]  16 Sep 2025


--- Page 2 ---
JOURNAL OF LATEX CLASS FILES, 2025
2
continuity considerations in decision-making.
Beyond the geographical distribution considerations ad-
dressed by previous approaches, we also account for the
temporal dependencies in user GPU requests. The temporal
distribution provides us with more optimization opportunities.
On a smaller time scale, we can more effectively utilize
GPU warm-up effects (caching effects) [20] by allocating
similar tasks to models whenever possible, thereby reducing
inference time. On a larger time scale, we can focus on
long-term server utilization and continuous switching costs
across clusters, using optimization methods to achieve better
load balancing and avoid expensive switching costs [17, 37].
However, a critical consideration is that incorporating the
temporal dimension increases the solution space by an order
of magnitude compared to previously studied problems, un-
doubtedly increasing the difficulty of solving using traditional
methods (such as nested dynamic programming).
Given these challenges, our approach addresses GPU re-
source allocation by integrating reinforcement learning with
an optimal transport [41] perspective, particularly incorpo-
rating the temporal dimension. Unlike traditional methods,
our strategy—termed Temporal Optimal Resource scheduling
via Two-layer Architecture (TORTA)—divides the allocation
process into two hierarchical layers: macro and micro. In the
first layer (macro), TORTA assigns GPU inference tasks
to nodes in each region, primarily to mitigate the imbalance
between GPU resources and task distribution. By leveraging
optimal transport, inference costs are significantly reduced. In
the second layer (micro), within regional server clusters,
TORTA assigns specific GPU inference tasks to suitable
servers. The quality of server selection not only reduces task
completion time but also avoids oscillations and high overhead
caused by frequent server state transitions. Through offline
training with large-scale samples, the model can quickly solve
problems during the online phase.
Specifically, we model the GPU resource scheduling prob-
lem as a Markov Decision Process (MDP) with states encom-
passing GPU utilization, queue lengths, network latency, and
temporal features. The action space involves regional alloca-
tion decisions, while the reward function balances response
time, load variance, and operational cost. We employ Proximal
Policy Optimization (PPO) [35] with optimal transport deci-
sions as supervised signals, enabling the system to learn multi-
time-slot allocation strategies that achieve global optimization
rather than myopic single-slot decisions.
The major contributions of our work are summarized as
follows:
• Analysis. We identify the limitations of existing ”Re-
active” GPU scheduling approaches. These include a
lack of foresight and disregard for temporal dependencies,
which can significantly increase the completion time of
GPU inference tasks and the switching costs of GPU
servers. Additionally, we establish performance upper
bound for these methods and derive explicit conditions
under which temporal-aware optimization can provably
outperform this bound.
• Scheme. We propose a reinforcement learning scheme
combined with optimal transport called TORTA. It
integrates spatial and temporal dimensions to achieve
better GPU utilization and load balancing compared to
traditional approaches. By employing a two-layer re-
source allocation strategy, the scheme effectively reduces
the complexity of solving problems after incorporating
the temporal dimension.
• Evaluation. We demonstrate through extensive exper-
iments across various scenarios. They include four
different network topologies and multiple performance
metrics. We find that our approach achieves superior
response times (16.39s-19.31s vs. 18.72s-24.39s for base-
lines) and load balance coefficients (0.74-0.76 vs. 0.68-
0.73 for baselines) while maintaining lower operational
costs with power cost reductions of 10.7K-14.1K com-
pared to existing methods.
We organize the remainder of this paper as follows. In
Section II and III, we present our motivation, emphasizing
the importance of incorporating the temporal dimension in
the GPU scheduling problem. Section V provides a detailed
description of our methodology, illustrating how reinforcement
learning combined with optimal transport is employed to
address GPU resource allocation. In Section VI, we conduct
extensive experiments to demonstrate the effectiveness of
our approach. Sections VII and VIII cover related work,
and conclude the paper with conclusions and future direc-
tions, respectively. Detailed theoretical proofs for performance
guarantees, comprehensive network training procedures, and
additional experimental results are provided in the Appendix
as supplemental material.
II. MOTIVATION:
THE NEED FOR TEMPORAL-AWARE ALLOCATION
In this section, we primarily answer two questions.
1) What are the limitations of reactive approaches that do
not consider temporal dynamics?
2) How does introducing temporal awareness mitigate or
address these issues?
A. Reactive Scheduling and Its Limitations
Reactive scheduling strategies dominate current GPU re-
source allocation systems. These methods make decisions
based solely on the system’s state at the current moment,
lacking the ability to predict future load patterns. This short-
sighted decision-making approach is particularly inefficient
when handling predictable traffic patterns.
Figure 2 illustrates the performance difference between re-
active and predictive scheduling when handling periodic traffic
peaks. Reactive approach only respond passively after a traffic
surge (as seen in the red portion of Figure 2.a), leading to a
significant number of requests waiting in queues. Figure 2.b
shows the distribution of GPU inference task queuing times
during traffic surges, revealing a clear ”bimodal pattern” where
queuing times are predominantly either long or short, with
long queuing times significantly exceeding short ones. In real-
world scenarios, this greatly impacts the user experience of
LLM services. In contrast, an ideal predictive approach (the
blue dashed line in Figure 2.a) prepares resources 15 minutes


--- Page 3 ---
JOURNAL OF LATEX CLASS FILES, 2025
3
8:40
9:00
9:20
9:40
10:00
0.1
0.3
0.5
0.7
0.9
Request Load
9:00@Surge
(a) Request Load and GPU Power Changes 
Request Load
0
5
10
15
20
25
30
0.0
0.1
Density
Short Queue
Long Queue
(b) Queue Time Distribution (@Surge) 
0
10 20 30 40 50 60 70
Time
0
10
20
Time (sec)
9:00@Surge
(c) Queue Time Across Timeslots 
Avg.
P99
0.0
0.2
0.4
0.6
0.8
GPU Power
REACTIVE GPU Power
IDEAL GPU Power
Fig. 2: Visualization of the limitations of reactive scheduling.
in advance through forecasting, resulting in a smoother overall
power increase.
Figure 2.c displays the variation in average queuing time
throughout the process, showing a clear ”staircase effect”. The
system only begins scaling up after detecting a load increase,
and GPUs require 1-3 minutes to transition from cold start to
full readiness. During this period, accumulated requests result
in an average queuing time of 15.7 seconds. Queuing time
gradually decreases as servers scale up, eventually dropping
to within 1 second after the GPU request load stabilizes.
The absence of foresight affects not only immediate service
quality but also triggers cascading effects. Sudden queuing
can prompt client retries, further increasing system load [10,
13]. Additionally, passive scaling often overreacts, leading
to resource waste after traffic subsides [27]. These issues
highlight the necessity of incorporating temporal awareness
mechanisms.
B. Costs of Ignoring Decision Continuity
Single-time-slot optimization overlooks the temporal depen-
dencies between decisions. Each scheduling decision is treated
as an independent event, making it difficult to account for the
costs of state transitions and the lasting impact of decisions.
This fragmented optimization approach leads to drastic con-
figuration changes between adjacent time slots, resulting in
unnecessary task migrations and resource switching.
Figure 3.a illustrates the hidden costs of task migration and
model switching. For a typical large model (LLaMA-2-7B,
deployed on an V100), migrating a task from node A to node
B involves: (1) model state serialization (˜15.2 seconds), (2)
target node deserialization (˜4.8 seconds) and GPU memory
loading (˜5.6 seconds), and (3) inference engine warm-up
(˜5.1 seconds). Switching between different models on the
same server (e.g., from LLaMA-2-7B to Qwen-7B) also incurs
significant costs, including unloading the current model (˜3.5
seconds), memory cleanup (˜2.1 seconds), loading the new
model (˜6.8 seconds), state initialization (˜14.2 seconds), and
engine reconfiguration (˜3.4 seconds). We conducted tests on
various GPU types (shown in Figure 3.b). The figure indicates
that the V100 exhibits higher migration costs across all stages
compared to the H100, RTX 4090 and RTX 3090. These
experimental results demonstrate that even task migration and
model switching between small-scale language models incur
substantial system overhead.
GPU power consumption varies significantly across differ-
ent stages shown in Figure 3.c. During deserialization and
Task    
Migration
Model   
Switching
Model State
Serialization
15.2s
Target Node
Deserialization
4.8s
GPU Memory
Loading
5.6s
Inference
Engine Warmup
5.1s
Model
Unloading
3.5s
Memory
Cleanup
2.1s
New Model
Loading
6.8s
State
Initialization
14.2s
Engine
Reconfiguration
3.4s
(a) Breakdown of GPU Task Overhead (V100-32GB)
Seriali-
zation
Memory
Loading
Deseriali-
zation
Engine
Warmup
0.0
4.0
8.0
12.0
16.0
Time (seconds)
(b) Migration Time Across GPUs
V100
RTX 3090
RTX 4090
H100
0
10
20
30
40
50
60
Time (seconds)
10
60
110
160
210
260
Power (Watts)
Serialization
Network
Transmission
Deserialization
Memory
Loading
Engine
Warmup
Inference
Peak: 237W
(c) GPU Consumption Change
Fig. 3: Breakdown of GPU Task Overhead and Power Con-
sumption During Task Migration and Model Switching.
memory loading, power consumption increases notably and
exhibits significant fluctuations. For an V100 with a power
consumption of 250W, the peak power can reach 237W. Fre-
quent cold-to-hot transitions in GPUs may accelerate hardware
aging and adversely affect grid stability [52].
These observations reveal a fundamental limitation, reactive
methods face a performance upper bound determined by opti-
mal single-timeslot solutions plus unavoidable switching costs.
Our analysis in Appendix proves this upper bound exists and
provides conditions for surpassing it through temporal-aware
optimization. And we aim to design smoother state transition
strategies that reduce switching overhead while maintaining
flexibility.
C. Allocation Inefficiency Without Coordination
Reactive decision-making lacks foresight and coordination
across multiple time slots. This short-sightedness becomes
particularly evident when addressing sudden events. Figure 4.a
illustrates the geographical distribution of GPU resources and
their inter-regional connections, highlighting the limitations of
reactive strategies when a critical region experiences a failure
(as indicated by “CRITICAL FAILURE” in the figure) which
we stimulated in our system environment.
Figure 4.c visualizes the recovery process of traditional
single-time-slot methods in response to sudden GPU failures.
Upon detecting a failure, these methods, lacking foresight into
future load and resource conditions, tend to blindly migrate
affected tasks to the nearest available regions within the first
time slot (T1). This immediate but shortsighted decision causes
a sharp increase in task queuing latency in these neighboring
regions during T1, with a significant portion of tasks being
dropped due to resource overload. Recovery occurs gradually
over the subsequent three time slots (T2, T3, T4), but this
“delayed response” significantly impacts user experience and
system efficiency.
Figure 4.d demonstrates the recovery strategy of multi-time-
slot predictive methods under the same failure scenario. By
integrating temporal information and predictive capabilities,
this approach intelligently and evenly distributes tasks across
multiple future time slots and geographical regions, effectively
avoiding local overload. This smooth migration strategy not


--- Page 4 ---
JOURNAL OF LATEX CLASS FILES, 2025
4
Fig. 4: Comparison of Reactive and Predictive Scheduling
Strategies Inference Under Critical Failure Conditions.
only significantly improves overall task completion rates and
reduces queuing times but also shortens average completion
times. As shown in Figure 4.b, the reactive methods have
shorter completion time, primarily due to its task-dropping
mechanism that seriously affects user experience. The predic-
tive approach exhibits clear advantages in performance metrics
(completion rate, and queuing time) during failure recovery.
III. KEY CHALLENGES
Although introducing temporal-aware mechanisms appears
to address some limitations of reactive methods discussed in
§ II, their implementation still faces several challenges.
A. Multi-Objective Optimization Complexity
Optimizing GPU resource allocation requires balancing re-
sponse time, operational costs, and load balancing across dis-
tributed clusters, posing a complex combinatorial optimization
problem. When N tasks arrive in each time window and M
regional clusters each contain K servers, even single-time-slot
allocation yields O((M ×K)N) possible solutions. Real-world
scenarios often involve over 50 clusters and more than 100,000
tasks, making exhaustive search infeasible. Heterogeneous task
types (e.g., compute-intensive or memory-intensive) require
distinct resource-matching strategies, further increasing prob-
lem complexity.
Current methods model this as a mixed-integer linear
programming (MILP) problem, but the computational cost
becomes impractical in large-scale scenarios. As shown in
Figure 5.a, MILP solving time grows exponentially with
task volume—handling 5,000 tasks takes over 2 minutes
on our experimental hardware1, while production systems
require sub-second decisions [54]. Note that our experimental
scenario represents a significantly simplified version of real-
world deployments, which involve dozens of server types with
heterogeneous GPU architectures, diverse task categories with
complex resource requirements, dynamic pricing models, and
1Tested on an Intel i5-13490F (10 cores, 4.8GHz boost).
200
500
1K
2K
5K
Number of Tasks (N)
0.1
1
10
100
1k
Solving Time (seconds)
0.5
1.8
6.0
22.0
130.2
(a) MILP Solving Time (Log-Log Scale)
Measured Data
Linear Interpolation
(b) Configuration parameters
Component
Configuration
Scale
Infrastructure
5
regions,
10
servers each
50
total
servers
Task Prop-
erties
2
types
(com-
pute/memory)
Variable
arrival rates
Capacity
Model
Dynamic Server
limits
3-20 tasks per
server
Load
Balancing
Max 80% Per re-
gion
Prevents con-
centration
Variables
Binary matrix
N × 50 deci-
sions
Constraints
Assignment
+
capacity
N + 55 con-
straints
Fig. 5: MILP strategy solving time.
multi-tenant constraints. The computational complexity grows
exponentially with these additional dimensions, making tra-
ditional MILP approaches fundamentally unsuitable for real-
time multi-objective optimization.
B. How to Optimize for Long-term Benefits?
Optimizing GPU resources across multiple time slots re-
quires reasoning about future states and their impacts. The
system must learn complex temporal patterns: how current
allocation decisions affect future load distribution, when to
preheat servers for anticipated demand, and how to maintain
stability amid changes. Each decision triggers cascading ef-
fects—allocating tasks to a specific region not only impacts
immediate load but also alters migration patterns, queue accu-
mulation, and resource availability in subsequent time slots.
These temporal dependencies fundamentally create a long-
term credit assignment challenge, where:
The core difficulty manifests in several ways. First, deci-
sions with delayed consequences—like keeping servers pre-
heated—may initially appear suboptimal but prove critical
during demand surges. Conversely, aggressive task packing
for immediate efficiency often induces cascading migrations
that degrade overall performance. Second, the system must
infer these long-term causal relationships from historical data
while remaining adaptive to new patterns. Most critically, the
high-dimensional state space (encompassing load distribution,
server states, and queue lengths across regions) combined
with a continuous action space renders traditional dynamic
programming methods infeasible.
IV. TORTA OVERVIEW
We propose a scheme called Temporal Optimal Resource
scheduling via Two-layer Architecture (TORTA) to address
the limitations of reactive scheduling in §II and the chal-
lenges of multi-objective optimization complexity in §III. This
scheme systematically addresses the shortcomings of existing
methods in handling temporal dependencies, decision conti-
nuity, and long-term optimization through temporal dimension
modeling and a two-layer optimization architecture.
We remodel GPU resource allocation as a spatio-temporal
coupled optimization problem. This considers both spatial
heterogeneity (GPU resource distribution and performance
differences across geographical regions) and temporal dynam-
ics (load fluctuations, state transition costs, and long-term
dependencies). Through this unified spatiotemporal modeling,
the system can fully consider the impact of current allocation


--- Page 5 ---
JOURNAL OF LATEX CLASS FILES, 2025
5
decisions on future system states when making decisions,
thereby avoiding the ”short-sighted” problems caused by tra-
ditional reactive methods. The optimization objectives include
not only immediate task completion time and computational
costs, but also minimizing state transition overhead across time
slots.
For this optimization problem, we decomposes the origi-
nally exponential-complexity global optimization problem into
two relatively independent but coordinated subproblems by
designing a two-layer hierarchical architecture:
◦Macro Regional Allocation (§V-B): This layer handles
strategic resource scheduling tasks across regions, focusing
on solving geographical distribution imbalances and large-
scale load balancing problems. The layer first uses a demand
predictor (§V-B2) to analyze historical load patterns and tem-
poral features, predicting request distributions for future time
slots in each region to provide decision basis for subsequent
optimization. The core RL-OT optimizer combines the long-
term planning capability of reinforcement learning with the
supply-demand matching advantages of optimal transport the-
ory to generate inter-regional allocation strategies that consider
temporal continuity.
◦Micro Server Selection (§V-C): Under the regional allo-
cation constraints determined by the macro layer, this layer
focuses on fine-grained task-server matching problems within
clusters. The task-server optimization engine comprehensively
evaluates hardware compatibility, real-time load status, and
server warm-up conditions to compute optimal server alloca-
tion schemes for each task, and continuously optimizes initial
allocation results through local search algorithms. The state
manager coordinates the timing of server state transitions,
intelligently deciding the switching timing of each server
between different operating states based on load predictions,
significantly reducing the high switching costs analyzed in §3
through proactive state management.
V. DESIGN OF TORTA
We outline our problem modeling and optimization ob-
jectives and then divide our scheme into macro and micro
components for detailed explanation.
A. Problem Formulation
We formalize the temporal-aware GPU resource allocation
problem as a spatiotemporal optimization problem with mul-
tiple time slots. Consider a distributed system consisting of
R geographical regions, where each region r ∈{1, 2, . . . , R}
contains Sr GPU servers. The optimization is performed over
a time horizon of T discrete time slots, where each time slot
has a duration of ∆t.
Let Nt = {nt
1, nt
2, . . . , nt
Nt} denote the set of GPU infer-
ence tasks arriving at time slot t, where Nt represents the
total number of tasks in slot t. Each task nt
i is character-
ized by a tuple (ct
i, mt
i, dt
i), representing its computational
requirements, memory requirements, and deadline constraint,
respectively. The system state at time slot t is defined as
st = (Ut, Qt, Lt, Ht), where Ut ∈RR×Smax represents the
utilization matrix of all servers across regions, Qt ∈RR
Fig. 6: Overview of TORTA’s system architecture.
denotes the queue lengths in each region, Lt ∈RR×R is the
inter-region latency matrix, and Ht ∈RR×Smax captures the
historical load patterns and temporal features.
The allocation decision at time slot t is represented by the
assignment matrix Xt ∈{0, 1}Nt×R×Smax, where Xt
i,r,s =
1 indicates that task nt
i is assigned to server s in region r,
and 0 otherwise. The allocation must satisfy the following
constraints: each task is assigned to exactly one server, i.e.,
PR
r=1
PSr
s=1 Xt
i,r,s = 1 for all i ∈{1, . . . , Nt}.
The objective function consists of three components: re-
sponse time, power cost, and switching cost.
The response time for task nt
i assigned to server s in region
r is modeled as Tcompletion(i, r, s, t) = Tqueue(r, s, t) +
Tcompute(i, r, s) + Tnetwork(i, r)2, where Tqueue(r, s, t) rep-
resents the queuing delay, Tcompute(i, r, s) is the actual com-
putation time, and Tnetwork(i, r) accounts for network trans-
mission delays.
The power cost Ct
power captures the electricity consumption
costs across geographical regions, which vary significantly
due to differences in regional electricity pricing and infras-
tructure efficiency. This cost component accounts for both
the computational power required for task processing and the
regional cost variations that enable cost-aware geographical
optimization. Formally, Ct
power = PNt
i=1
PR
r=1
PSr
s=1 Xt
i,r,s ·
(wnetwork · Lorigin(i),r + wcompute · Pr,s), where origin(i)
denotes the origin region of task i, wnetwork and wcompute
are weight parameters, Lorigin(i),r represents the network
transmission cost, and Pr,s represents the computational power
cost at server s in region r.
The switching cost Ct
switch directly measures the alloca-
tion differences between consecutive timeslots, capturing the
overhead of changing resource allocation patterns over time.
This cost is crucial for capturing the temporal dependencies
identified in our motivation analysis. We define Ct
switch =
||Xt −Xt−1||2
F , where Xt and Xt−1 represent the allocation
matrices at timeslots t and t −1 respectively, and || · ||F de-
2Network latency is typically 1% of compute time. For reference, GPT-4
generates tokens at 13 tokens/s; a 250-word conversation takes ∼25s, while
intercontinental latency ranges between 10–100ms.


--- Page 6 ---
JOURNAL OF LATEX CLASS FILES, 2025
6
Fig. 7: Key component of TORTA.
notes the Frobenius norm. This formulation directly penalizes
dramatic allocation changes between consecutive timeslots,
encouraging temporal smoothness while allowing necessary
adaptations to changing demand patterns.
The complete optimization problem can be formulated as
finding an allocation policy π : S →A that minimizes the
expected cumulative cost over the entire time horizon:
min
π E
"
T
X
t=1
 Nt
X
i=1
Tcompletion(i, r, s, t)
+ α · Ct
switch + β · Ct
power
#
(1)
where α and β are weight parameters that serve dual
purposes: (1) they balance the relative importance of switching
costs and power costs against response time, and (2) they
normalize these heterogeneous metrics (time in seconds, power
in watts, switching overhead in computational cycles) into a
unified optimization objective with consistent units.
B. Macro-Level Allocation: OT with RL
The macro-level allocation layer addresses the strategic
distribution of GPU inference tasks across geographical re-
gions. This layer operates by first establishing an upper-bound
allocation strategy using optimal transport theory (as proved
in Appendix), which then serves as a supervised signal for
reinforcement learning optimization. The two-stage scheme
enables the system to leverage the theoretical optimality
guarantees of optimal transport while adapting to complex
temporal dynamics through reinforcement learning.
1) Optimal Transport for Regional Load Balancing: We
model the regional allocation problem as an optimal transport
problem between GPU request distributions and GPU resource
distributions. Let µt ∈RR represent the normalized GPU
request distribution at time slot t, where µ(r)
t
denotes the
proportion of total requests originating from region r. Sim-
ilarly, let νt ∈RR represent the normalized GPU resource
distribution, where ν(r)
t
indicates the proportion of total GPU
capacity available in region r.
In practice, the sum of raw request counts and re-
source capacities across regions are typically unequal, i.e.,
PR
r=1 RequestCountt
r ̸= PR
r=1 ResourceCapacityt
r. To ap-
ply optimal transport theory, we normalize both distribu-
tions to unit mass: µ(r)
t
=
RequestCountt
r
PR
i=1 RequestCountt
i and ν(r)
t
=
ResourceCapacityt
r
PR
i=1 ResourceCapacityt
i , ensuring PR
r=1 µ(r)
t
= PR
r=1 ν(r)
t
=
1.
The cost matrix C ∈RR×R captures the cost between
regions, where Ci,j represents the cost of serving a request
from region i using resources in region j. This cost primarily
incorporates regional power cost differences and minimal
network overhead: Ci,j = w1 · (PowerCostj) + w2 · (Li,j +
BandwidthCosti,j), where PowerCostj represents the elec-
tricity cost in region j, Li,j is the network latency between
regions i and j, BandwidthCosti,j accounts for data transfer
overhead, and w1 ≫w2 reflects the dominance of power costs
over network costs in the optimization objective.
The optimal transport problem seeks to find the allocation
matrix P ∗∈RR×R that minimizes the total transportation
cost while satisfying the marginal constraints:
P ∗= arg min
P ≥0
R
X
i=1
R
X
j=1
Ci,jPi,j
(2)
subject to the constraints PR
j=1 Pi,j = µ(i)
t
for all i (demand
satisfaction) and PR
i=1 Pi,j = ν(j)
t
for all j (capacity con-
straints).
The solution P ∗provides an upper-bound allocation strategy
where P ∗
i,j represents the optimal mass flow from region i
to region j. To convert this into actionable routing prob-
abilities, we normalize each row of the allocation matrix:
Probi→j =
P ∗
i,j
PR
k=1 P ∗
i,k . This normalization yields a row-
stochastic matrix where Probi→j represents the probability
that a task originating from region i should be routed to region
j for processing.
The
resulting
probability
distribution
Probi
=
[Probi→1, Probi→2, . . . , Probi→R] for each source region
i serves as a reference allocation strategy that balances
supply and demand optimally under the given cost structure.
However, this static optimal transport solution does not
account for temporal dependencies, future load predictions,
or the dynamic nature of system states. To address these
limitations,
we
integrate
the
optimal
transport
solution
as a supervised signal within our reinforcement learning
framework.
2) Reinforcement Learning with OT Supervision: The rein-
forcement learning component learns to generate temporally
smooth allocation matrices that respect the optimal transport
baseline while minimizing abrupt load transitions between
consecutive time slots. The primary objective is to produce
allocation decisions that maintain system stability and avoid
the cascading effects of sudden load redistribution across
regions.
We formulate the problem as a Markov Decision Process
(MDP) with state space S, action space A, transition function
T , and reward function R. The state at time slot t is defined
as st = (Ut, Qt, Lt, Ht, Ft, At−1), where Ut ∈RR represents
the current utilization levels across regions, Qt ∈RR denotes
the queue lengths in each region, Lt ∈RR×R is the inter-
region latency matrix, Ht captures historical load patterns
and temporal features, Ft ∈RR is the predicted task arrival
for the next time slot obtained from a pre-trained demand


--- Page 7 ---
JOURNAL OF LATEX CLASS FILES, 2025
7
predictor, and At−1 ∈[0, 1]R×R is the allocation matrix from
the previous time slot.
The demand predictor is trained offline using historical
data to forecast future task arrivals. Given historical features
{Ut−k, Qt−k, Ht−k}K
k=1 for the past K time slots, the predic-
tor estimates Ft = Predictor(Ut−K:t, Qt−K:t, Ht−K:t), where
F (r)
t
represents the predicted number of tasks arriving in
region r during time slot t + 1. And the demand predictor
is implemented as a neural network trained offline using
historical data.
The action space consists of allocation matrices At ∈
[0, 1]R×R where A(i,j)
t
represents the proportion of tasks from
region i that should be allocated to region j. The action must
satisfy the normalization constraint PR
j=1 A(i,j)
t
= 1 for all
source regions i ∈{1, 2, . . . , R}, ensuring that all tasks from
each region are allocated.
The reward function is designed to balance three objec-
tives: alignment with the optimal transport solution, temporal
smoothness, and load stability. The reward at time slot t is
formulated as:
rt = rOT (At, P ∗
t ) + λ1 · rsmooth(At, At−1) + λ2 · rcost(Qt)
(3)
where rOT (At, P ∗
t ) = −∥At −P ∗
t ∥2
F measures the alignment
between the RL allocation matrix and the optimal transport
solution, encouraging the agent to respect the theoretical
optimality while allowing adaptive deviations.
The smoothness term rsmooth(At, At−1) = −∥At−At−1∥2
F
penalizes large changes between consecutive allocation ma-
trices, promoting temporal continuity and preventing abrupt
load redistributions that can cause system instability. The cost
term rcost(Qt) = −∥Qt∥1
Qmax utilizes the aggregate backlog across
regional servers as a real-time proxy for system responsive-
ness, where Qt represents the vector of pending task counts
and Qmax denotes the maximum tolerable queue capacity. The
weights λ1 and λ2 are set to prioritize OT alignment while
ensuring sufficient temporal smoothness and load balance,
with the relative weighting empirically tuned to achieve stable
convergence during training.
The policy network πθ : S →A is parameterized by θ
and trained using Proximal Policy Optimization (PPO). The
network outputs the parameters of a Beta distribution for each
element of the allocation matrix, ensuring that the generated
probabilities lie within [0, 1] and can be appropriately normal-
ized. The policy gradient objective incorporates the temporal
smoothness directly into the learning process:
L(θ) = Est,at∼πθ
"
min
 πθ(at|st)
πθold(at|st)
ˆAt,
clip
 πθ(at|st)
πθold(at|st), 1 −ϵ, 1 + ϵ

ˆAt
#
(4)
where
ˆAt is the advantage estimate computed using the
temporal reward structure, and ϵ is the clipping parameter.
The advantage estimation incorporates the smoothness penalty
to encourage policies that maintain allocation stability across
time slots.
To achieve provable performance superiority over reactive
approaches (which focus on single-timeslot optimization) in
multi-timeslot joint optimization scenarios, we enhance the
standard PPO training objective with carefully designed con-
straint terms. Our modified loss function is formulated as:
Ltotal = LPPO + γ · Lϵ(s, ϵ) + δ · Ls(s, ϵ)
(5)
where Lϵ(s, ϵ)
=
max(0, ∥ARL
t −AOT
t ∥F −ϵmax
ϵ0
) controls the
maximum deviation from the optimal transport baseline, and
Ls(s, ϵ) = max(0, smin−scurrent
s0
) ensures sufficient improvement
in switching cost reduction.
This design ensures that TORTA operates within the theoret-
ical trade-off region where reduced switching costs outweigh
single-timeslot optimality losses. The constraint parameters
ϵ and s must satisfy a specific relationship to guarantee
theoretical performance advantages over any reactive method:
1−1/s
ϵ
> LR+βLP
K0
, where LR, LP are Lipschitz constants of
response time and power cost functions, and K0 is the baseline
switching cost of reactive methods. The rigorous mathematical
derivation of this relationship, detailed training procedures,
and the formal proof are provided in Appendix.
C. Micro-Level Allocation
The micro-level allocation layer operates within each re-
gional cluster to assign specific GPU inference tasks to ap-
propriate servers. This layer receives the regional allocation
decisions from the macro layer and focuses on optimizing
task-server matching based on hardware compatibility, current
load conditions, and anticipated future demands. The micro-
level optimization addresses two key decisions: determining
the optimal number of active servers and selecting the most
suitable server for each incoming task.
1) Dynamic Server Activation Strategy: The number of
active servers in each region is dynamically adjusted based on
current workload and predicted future demands. Let N (r,t)
active
denote the number of active servers in region r at time slot
t, where 1 ≤N (r,t)
active ≤Sr. The server activation decision
balances immediate processing capacity with operational costs
and transition overheads.
The activation strategy considers three factors: current queue
length Q(r)
t , predicted incoming tasks F (r)
t
from the demand
predictor, and the warm-up costs associated with transitioning
servers from idle to active states. The target number of active
servers is computed as:
N (r,t)
target = min

Sr,


Q(r)
t
+ F (r)
t
+ σ ·
q
F (r)
t
C(r)
avg




(6)
where C(r)
avg represents the average processing capacity per
server in region r, and σ is a safety factor that accounts
for load variability. The square root term provides a buffer
proportional to the standard deviation of the predicted load,
following queueing theory principles.
To minimize transition costs, servers are activated or de-
activated gradually. If N (r,t)
target > N (r,t−1)
active , additional servers
are selected for activation based on their warm-up time and


--- Page 8 ---
JOURNAL OF LATEX CLASS FILES, 2025
8
hardware compatibility with incoming tasks. Conversely, when
N (r,t)
target < N (r,t−1)
active , servers with the lowest current utilization
and longest idle periods are prioritized for deactivation to
minimize wasted warm-up investments.
2) Greedy Task-Server Matching: For the active servers,
we employ a greedy algorithm that iteratively assigns each
task to the server with the highest compatibility score. The
score Score(taski, servers) captures multiple dimensions of
task-server affinity:
Score(taski, servers) = w1 · Comphw(i, s)
+ w2 · Compload(i, s) + w3 · Complocality(i, s)
(7)
The hardware compatibility term Comphw(i, s) measures
how well the task’s computational and memory requirements
match the server’s capabilities. For compute-intensive tasks
requiring high throughput, this term favors servers with pow-
erful GPUs like A100 or H100. For memory-intensive tasks
involving large model inference, it prioritizes servers with
sufficient GPU memory. Formally:
Comphw(i, s) = min
 
1, GPU (s)
compute
task(i)
compute
!
· min
 
1, GPU (s)
memory
task(i)
memory
!
· Typematch(i, s)
(8)
where Typematch(i, s) ∈{0.5, 1.0} indicates whether the
server’s GPU architecture is optimal for the task type (e.g.,
A100 for training workloads, RTX series for inference tasks).
The load compatibility term Compload(i, s) considers the
server’s current utilization and queue length to promote load
balancing:
Compload(i, s) = exp
 
−
Util(s)
current + Queue(s)
length
Capacity(s)
!
(9)
This exponential decay function heavily penalizes over-
loaded servers while providing reasonable scores for moder-
ately loaded ones.
The locality term Complocality(i, s) accounts for data lo-
cality and network proximity. Tasks that require access to
cached model weights or intermediate results benefit from
being assigned to servers that have recently processed similar
tasks:
Complocality(i, s) =
X
j∈Recent(s)
tasks
Similarity(taski, taskj)
exp(λ · (t −timestampj))
(10)
where Recent(s)
tasks represents tasks recently processed by
server s, and Similarity(taski, taskj) = wm · I[modeli =
modelj] + wc · cos(embedi, embedj) measures task similarity
based on model type matching and input embedding cosine
similarity, with weights wm, wc.
The greedy assignment algorithm processes tasks in order of
urgency (deadline proximity) and computational requirements
(resource-intensive tasks first). For each task i, it computes
the compatibility scores with all available servers and assigns
the task to the server with the highest score, provided that
the server has sufficient remaining capacity. The algorithm
maintains running estimates of server utilization and queue
lengths, updating these estimates after each assignment to
ensure that subsequent decisions reflect the current allocation
state.
Algorithm 1: TORTA LOGIC
Input : System state st, Tasks Nt, Cost matrix C
Output: Task-server allocation Xt, System state st+1
// Phase 1: Macro-Level Allocation
// Demand and Supply Normalization
1 µt, νt ←Normalize request and resource distributions;
// Optimal Transport
2 Solve OT problem: P ∗←arg minP ≥0
P
i,j Ci,jPi,j;
3 Probi→j ←Row-normalize P ∗;
// Reinforcement Learning Optimization
4 Ft ←Predictor(Ut−K:t, Qt−K:t, Ht−K:t);
5 At ←πθ(st, Ft, At−1);
// Regional Task Distribution
6 for each task i ∈Nt do
7
regioni ←Sample from At[origin(i), :];
8
Add task i to regional queue Tregioni;
// Phase 2: Micro-Level Server Selection
9 for each region r = 1 to R do
// Dynamic Server Activation
10
N (r,t)
target ←Compute target active servers;
11
Update active server set Sactive
r
based on N (r,t)
target;
// Task-Server Matching
12
Tr ←Sort regional tasks by urgency (deadline
proximity);
13
for each task i ∈Tr do
14
for each active server s ∈Sactive
r
do
15
Score(i, s) ←w1 · Comphw(i, s) + w2 ·
Compload(i, s) + w3 · Complocality(i, s);
16
s∗←arg maxs Score(i, s);
17
Xt
i,r,s∗←1;
18
Utils∗←Utils∗+ ResourceReqi;
19
Queues∗←Queues∗+ 1;
// System State Update
20 Update utilization matrix Ut+1, queue lengths Qt+1;
21 At−1 ←At return Xt, st+1
To handle situations where no server has sufficient capacity
for a task, the algorithm implements a task buffering mecha-
nism that temporarily queues the task until capacity becomes
available. The buffering strategy prioritizes tasks based on
deadline urgency and can trigger additional server activations
if the buffer reaches capacity thresholds.
VI. EXPERIMENTS
We conduct comprehensive experiments to evaluate the
performance of our proposed TORTA scheme under realistic
distributed GPU inference scenarios. The experimental con-
figuration is designed to simulate real-world conditions while
maintaining controlled variables for fair comparison across
different methods.
A. Experimental Setup
System Configuration. The simulation runs for a total dura-
tion of 6 hours, divided into 480 discrete time slots with each
time slot lasting 45 seconds. This time granularity provides
sufficient resolution to capture dynamic load patterns while


--- Page 9 ---
JOURNAL OF LATEX CLASS FILES, 2025
9
maintaining computational tractability for the optimization
algorithms. Each GPU inference task has a processing time
requirement that follows a uniform distribution, reflecting the
heterogeneous nature of real-world inference workloads rang-
ing from lightweight image classification to complex natural
language processing tasks.
Network Topology and Infrastructure. We evaluate our
scheme across four distinct network topologies that repre-
sent different scales and geographical distributions of GPU
infrastructure. The topologies range from small-scale regional
deployments to large-scale global distributions (Table I.a), en-
abling us to assess scalability and performance across various
infrastructure configurations.
Each server cluster contains heterogeneous GPU resources
with different capabilities and capacities (see Table I.b). The
infrastructure setup is designed to reflect real-world deploy-
ment scenarios where different GPU types are optimized for
specific workload characteristics.
Cost Model. To provide realistic evaluation conditions,
we incorporate real-world electricity costs based on global
electricity pricing data. The power consumption and asso-
ciated costs [42] vary significantly across geographical re-
gions, reflecting actual operational expenses that cloud service
providers face. This cost model enables us to evaluate not only
performance metrics but also the economic efficiency of dif-
ferent allocation strategies under realistic pricing constraints.
Baselines. To comprehensively evaluate the effectiveness of
our proposed scheme, we compare it against several repre-
sentative baseline methods. SkyLB: This method deploys
local load balancers in each region that prioritize local request
processing to minimize inter-region communication overhead.
When a region reaches full capacity, requests are forwarded to
load balancers in other regions with available resources. The
system employs a prefix tree structure to route requests from
the same user to fixed replicas whenever possible, maintaining
session affinity and leveraging cache locality effects [45].
SDIB (Standard Deviation and Idle-time Balanced): This
approach optimizes for two primary objectives: minimizing
the standard deviation of load distribution across servers and
reducing the mean idle time of GPU resources. The implemen-
tation methodology follows the principles described in MERL-
LB [49], adapting the multi-objective optimization framework
to balance load variance and resource utilization efficiency. RR
(Round-Robin): We implement a round-robin scheduling algo-
rithm as a fundamental baseline that provides a simple yet fair
allocation strategy. This method applies round-robin selection
for both regional GPU resource allocation and intra-cluster
server assignment while maintaining necessary capacity and
compatibility constraints. The round-robin approach serves as
a performance lower bound and demonstrates the benefits of
more sophisticated optimization strategies.
It is important to note that the original system scenarios
addressed by SkyLB and MERL-LB differ from our distributed
GPU inference setting. We have carefully adapted and im-
plemented these algorithms to operate within our simulation
environment, preserving their core algorithmic principles while
adjusting them to handle the specific characteristics of GPU
workload allocation, temporal dependencies, and heteroge-
neous hardware configurations present in our problem domain.
B. Evaluation Metrics
We employ three primary metrics to comprehensively assess
the performance of our hierarchical allocation scheme, corre-
sponding directly to the three components of our optimization
objective:
Response Time: The average response time measures the
end-to-end latency experienced by each inference task, calcu-
lated from the moment a request is submitted until the pro-
cessed result is returned to the user. This metric encompasses
three components: (1) network delay time for request routing
and result transmission, (2) waiting time in queue before
processing begins, and (3) actual inference execution time
on the assigned GPU server. Response time directly reflects
user experience quality and serves as the primary performance
indicator for latency-sensitive GPU inference applications.
Load Balancing: We quantify load distribution across the
system using inverse of the coefficient of variation (1/CV)
of server utilization rates. This metric captures how evenly
computational workload is distributed across available GPU
resources, with larger values indicating better load balancing.
Effective load balancing prevents resource hotspots, ensures
efficient utilization of the entire infrastructure, and contributes
to overall system stability and predictable performance.
Total Cost: The aggregated cost encompassing both oper-
ational expenses and switching overhead incurred during the
experimental period. This includes electricity costs calculated
based on regional electricity pricing, as well as switching costs
measured as allocation differences between consecutive times-
lots. This comprehensive cost metric evaluates the economic
efficiency of different allocation strategies and their ability
to leverage both geographical cost differences and temporal
smoothness for overall optimization.
TABLE I: Experimental Infrastructure Configuration
(a) Topology Characteristics [31]
Topo.
Node
B/W.
(Gbps)
Lat.
(ms)
Abilene
12
10
25
Polska
12
10
45
Gabriel
25
15
80
Cost2
32
20
150
(b) GPU Types and Task Categories
GPU
Count
Task Type
A100
40-60
Compute-Int.
H100
20-40
Compute-Int.
4090
40-60
Lightweight
V100
60-80
Memory-Int.
T4
40-60
Lightweight
C. Experimental Analysis
1) Response Time Performance Analysis: The response
time distributions shown in Figure 8 demonstrate the effec-
tiveness of our TORTA scheme across four different network
topologies. TORTA consistently achieves the fastest average
task completion times, with mean response times of 16.39s,
19.31s, 17.58s, and 19.19s for Abilene, Cost2, Gabriel, and
Polska topologies respectively. This represents substantial im-
provements of 12.4% (vs. SkyLB: 18.72s), 10.5% (vs. SkyLB:
21.58s), 12.4% (vs. SkyLB: 20.07s), and 6.5% (vs. SkyLB:
20.53s) compared to the best-performing baseline method
across each topology.


--- Page 10 ---
JOURNAL OF LATEX CLASS FILES, 2025
10
10
20
30
0.00
0.02
0.04
0.06
0.08
0.10
Probability Density
Mean Values:
TORTA: 16.39
SkyLB: 18.72
SDIB: 19.66
RR: 21.50
Abilene
10
20
30
40
0.00
0.02
0.04
0.06
0.08
0.10
Mean Values:
TORTA: 19.31
SkyLB: 21.58
SDIB: 22.51
RR: 24.39
Cost2
10
20
30
40
0.00
0.02
0.04
0.06
0.08
0.10
Mean Values:
TORTA: 17.58
SkyLB: 20.07
SDIB: 21.10
RR: 22.92
Gabriel
10
20
30
40
0.00
0.02
0.04
0.06
0.08
0.10
Mean Values:
TORTA: 19.19
SkyLB: 20.53
SDIB: 21.53
RR: 23.18
Polska
Fig. 8: Response time probability distributions across different network topologies. The shaded regions represent the core
distribution range, with vertical dashed lines indicating mean values for each algorithm.
TORTA
SkyLB
SDIB
RR
10
12
14
16
Power Cost
12.5
14.3
14.3
14.7
Abilene
Power Cost
Operational Overhead
TORTA
SkyLB
SDIB
RR
10
11
12
13
14
15
11.1
13.2
13.2
13.3
Cost2
TORTA
SkyLB
SDIB
RR
10
11
12
13
14
10.7
12.8
12.8
12.9
Gabriel
TORTA
SkyLB
SDIB
RR
10
12
14
16
14.1
15.2
15.2
15.3
Polska
0
1
2
3
4
0.8
2.9
2.8
3.7
0
1
2
3
4
5
2.7
4.4
4.3
5.1
0
1
2
3
4
1.3
3.3
3.2
4.0
0
1
2
3
4
Switch Cost
2.3
3.4
3.4
4.0
Fig. 9: Power cost and operational overhead comparison across different network topologies. Power costs are measured in
thousands of dollars ($K), while operational overhead represent the relative overhead (normalized units).
A detailed examination of the distribution tails reveals
that TORTA performs particularly well for high-latency tasks,
effectively reducing task queuing times through intelligent
scheduling decisions. This improvement is especially pro-
nounced in the right tail of the distributions, where TORTA
shows substantially lower probability density for long response
times compared to other methods. The temporal-aware opti-
mization enables the system to proactively manage resource
allocation, preventing the accumulation of tasks in queues that
typically leads to extended completion times.
The performance gap between TORTA and baseline meth-
ods is notably smaller on the Polska topology compared
to other network configurations. This phenomenon can be
attributed to the superior connectivity characteristics of the
Polska topology, which provides more routing options and
reduces the impact of suboptimal scheduling decisions. When
the underlying network infrastructure offers better connec-
tivity, the advantages of sophisticated temporal optimization
become less pronounced, as even simpler algorithms can
achieve reasonable performance through the network’s inher-
ent redundancy.
2) Cost Optimization and Economic Efficiency: Figure 9
presents a comprehensive cost analysis comparing TORTA
with baseline methods across four network topologies. TORTA
consistently achieves the lowest power costs across all topolo-
gies: 12.5K (vs. SkyLB’s 14.3K, 12.6% reduction), 11.1K
(vs. SkyLB’s 13.2K, 15.9% reduction), 10.7K (vs. SkyLB’s
12.8K, 16.4% reduction), and 14.1K (vs. SkyLB’s 15.2K, 7.2%
reduction) for Abilene, Cost2, Gabriel, and Polska topologies
respectively. Compared to other baselines, TORTA achieves
similar improvements of 12-16% over SDIB and RR methods.
The optimal transport component enables TORTA to lever-
age geographical cost variations by routing tasks to regions
with lower electricity prices while maintaining network latency
constraints. This cost-aware allocation strategy is particularly
effective in Cost2 and Gabriel topologies, where TORTA
achieves the most substantial power cost reductions of 15.9%
and 16.4% respectively.
For operational overhead, TORTA demonstrates dramatic
improvements: 0.8 vs. 2.9 (72% reduction), 2.7 vs. 4.4 (39%
reduction), 1.3 vs. 3.3 (61% reduction), and 2.3 vs. 3.4 (32%
reduction) compared to SkyLB across the four topologies.
Compared to other baselines, TORTA achieves 71-79% re-
ductions vs. SDIB and 73-78% reductions vs. RR methods.
The operational overhead metric represents normalized system
transition costs (measured in planning units) that encompasses
actual model loading, memory management, and server state
changes, extending beyond the theoretical switching cost used
in our analysis (which measures allocation matrix differences
as a proxy). TORTA’s micro-level server selection employs
proactive preheating and locality-aware assignment, while
SkyLB and SDIB use cache-aware strategies and RR lacks
such optimizations, explaining the varied overhead levels
across methods.
3) Load Balance: Figure 10 demonstrates the superior load
balancing performance of TORTA across all tested network
topologies. TORTA consistently achieves the highest load bal-
ance coefficients with mean values of 0.765, 0.743, 0.755, and
0.745 for Abilene, Cost2, Gabriel, and Polska topologies re-
spectively. Compared to the best-performing baseline SkyLB,
this represents improvements of 4.4% (0.765 vs 0.733), 4.1%
(0.743 vs 0.714), 3.6% (0.755 vs 0.729), and 4.2% (0.745


--- Page 11 ---
JOURNAL OF LATEX CLASS FILES, 2025
11
0.65
0.70
0.75
0.80
0.85
0.0
0.2
0.4
0.6
0.8
1.0
Cumulative Probability
Mean Values:
TORTA: 0.765
SkyLB: 0.733
SDIB: 0.735
RR: 0.716
Abilene
0.65
0.70
0.75
0.80
0.85
0.0
0.2
0.4
0.6
0.8
1.0
Mean Values:
TORTA: 0.743
SkyLB: 0.714
SDIB: 0.709
RR: 0.688
Cost2
0.65
0.70
0.75
0.80
0.85
0.0
0.2
0.4
0.6
0.8
1.0
Mean Values:
TORTA: 0.755
SkyLB: 0.729
SDIB: 0.726
RR: 0.708
Gabriel
0.65
0.70
0.75
0.80
0.85
0.0
0.2
0.4
0.6
0.8
1.0
Mean Values:
TORTA: 0.745
SkyLB: 0.715
SDIB: 0.708
RR: 0.686
Polska
Fig. 10: Cumulative distribution function (CDF) of load balance coefficients across different network topologies. The load
balance coefficient is calculated as the inverse of one plus the coefficient of variation of server utilization rates. Higher values
indicate better load distribution. Vertical dashed lines represent mean values for each algorithm.
vs 0.715) across the four topologies. The improvements over
other baselines are more substantial: 4.1-6.9% vs SDIB and
6.8-11.5% vs RR methods. The load balance coefficient LB
is defined as:
LB =
1
1 + CVutil
=
1
1 +
√1
N
PN
i=1(Ui−¯U)2
1
N
PN
i=1 Ui
(11)
where Ui represents the utilization rate of server i, ¯U is the
mean utilization across all servers, N is the total number
of active servers, and CVutil is the coefficient of variation.
These results represent substantial improvements over baseline
methods, indicating more uniform resource utilization across
the distributed infrastructure.
The enhanced load balancing performance can be attributed
to TORTA’s smooth allocation strategy at the macro level,
which prevents dramatic load variations between consecutive
time slots within server clusters. The temporal smoothness
term in our reinforcement learning objective function explicitly
penalizes large allocation changes, ensuring gradual transitions
that avoid sudden spikes in server workloads. This temporal
consistency is particularly important for maintaining stable
system performance and preventing cascading effects that can
lead to load imbalances.
At the micro level, TORTA employs dynamic server ac-
tivation through Equation 6 and load-aware server selection
via Equation 9 to maintain balanced utilization across active
servers. The exponential decay function in the load compatibil-
ity term heavily penalizes assignments to already overloaded
servers, naturally distributing tasks toward underutilized re-
sources. Additionally, the dynamic activation strategy ensures
that the number of active servers adapts to current demand,
preventing situations where a small number of servers handle
disproportionate workloads while others remain idle.
4) Details about Performance Metrics: Figure 11 provides
a detailed breakdown of response time components, showing
that TORTA achieves superior performance in both waiting
times and inference times across all network topologies. The
waiting times for TORTA range from 0.3 to 1.1 seconds com-
pared to 1.2-2.4 seconds for baseline methods, representing
reductions of approximately 50-75%. This improvement stems
from TORTA’s proactive resource management and temporal-
aware allocation strategy that prevents task accumulation.
Figure 12 illustrates the relationship between demand pre-
Abilene
Cost2
Gabriel
Polska
0
1
2
Time (seconds)
Waiting Times
Abilene
Cost2
Gabriel
Polska
16
18
20
Time (seconds)
Inference Times
TORTA
SkyLB
SDIB
RR
Fig. 11: Breakdown of response time components across
different network topologies.
diction accuracy and system performance for TORTA com-
pared to baseline methods. Since the baseline methods (RR,
SkyLB, and SDIB) do not incorporate prediction modules,
their performance remains constant across different accuracy
levels, as shown by the horizontal lines. In contrast, TORTA
demonstrates clear performance improvements with increased
prediction accuracy, with response times decreasing from
approximately 20.5 seconds at 0.1 accuracy to 17.5 seconds
at 0.9 accuracy. The shaded region around TORTA’s curve
represents the confidence interval derived from multiple ex-
perimental runs.
A critical observation is that TORTA begins to outperform
all baseline methods when prediction accuracy reaches 0.5,
which represents a relatively achievable forecasting require-
ment. The prediction accuracy PA is defined as:
PA = exp
 
−1
T
T
X
t=1
|F pred
t
−F actual
t
|
F actual
t
+ ϵ
!
(12)
where F pred
t
represents the predicted traffic volume at time t,
F actual
t
is the actual observed traffic, T is the total number
of time slots, and ϵ is a small constant to prevent division
by zero. This metric ranges from 0 to 1, with higher values
indicating more accurate predictions.
This threshold (0.4) suggests that the prediction module
does not need to achieve exceptional accuracy to provide
meaningful benefits. The 0.4 accuracy level corresponds to
reasonable demand forecasting capabilities that can be ob-
tained using standard time series prediction techniques, mak-
ing TORTA practical for real-world deployment scenarios.
Even when prediction accuracy falls below the 0.4 thresh-
old, TORTA’s performance degradation is gradual rather than
catastrophic, indicating the robustness of the two-stage alloca-


--- Page 12 ---
JOURNAL OF LATEX CLASS FILES, 2025
12
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
18
20
22
24
Time (seconds)
Response Time
TORTA
RR
SkyLB
SDIB
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
16
17
18
19
Time (seconds)
Inference Time 
TORTA
RR
SkyLB
SDIB
Fig. 12: Impact of demand prediction accuracy on system
performance. Left panel shows response times and right panel
shows inference times as functions of prediction accuracy. The
prediction accuracy is defined as Equation 12.
tion scheme. The micro-level server selection component ef-
fectively compensates for macro-level prediction inaccuracies
through real-time adaptation and intelligent task-server match-
ing. This resilience ensures that TORTA remains competitive
even when demand forecasting is challenging, while provid-
ing substantial performance gains when reasonable prediction
accuracy is available.
VII. RELATED WORK
The optimization of large language model inference has
primarily focused on addressing the fundamental mismatch be-
tween the compute-intensive prefill phase, which processes all
input tokens in parallel, and the subsequent memory-intensive
decode phase, which generates tokens sequentially [7]. Several
approaches have emerged to tackle this challenge through
various architectural and algorithmic innovations. Methods
such as speculative decoding [12, 26, 44] attempt to predict
multiple tokens simultaneously to reduce the sequential bot-
tleneck, while attention optimization techniques [32, 34, 50]
focus on improving memory efficiency during the attention
computation. Model compression approaches [11, 47, 48]
reduce computational requirements through quantization and
pruning, though they may compromise model accuracy. Some
works target Mixture-of-Experts (MoE) models, aiming to
achieve expert load balancing [14, 15] and faster inter-
expert communication [22, 33]. These acceleration techniques
primarily operate at the algorithmic level within individual
inference engines, complementing but not replacing the need
for effective resource allocation across distributed systems.
LLM inference scheduling encompasses both local schedul-
ing within individual servers and distributed scheduling across
clusters. At the engine level, traditional approaches have
evolved from simple first-come-first-served strategies [46] to
sophisticated predictive methods. Non-preemptive scheduling
methods [51] pre-allocate maximum possible memory based
on context length limits, often leading to significant mem-
ory underutilization when requests complete early. Predictive
scheduling approaches [3, 9, 36, 53] leverage various predic-
tion mechanisms to estimate response lengths and optimize
task ordering, though these methods differ fundamentally from
our temporal demand forecasting as they focus on individual
request characteristics rather than system-wide load patterns
over time. For instance, systems like PiA [53] use the LLM
itself for length estimation, while TRAIL [36] implements
shortest predicted remaining processing time strategies. Dy-
namic scheduling frameworks [18, 19, 25, 43] emphasize real-
time adaptation and responsive resource management, with
systems like Bullet [25] addressing prefill/decode mismatches
through concurrent execution and InferMax [18] employing
cost models for batch time prediction. Systems like PETALS
[2, 4, 6] address fault-tolerant inference across distributed
commodity hardware, and provide global scheduling for large-
scale training across data centers. However, these approaches
remain fundamentally reactive, optimizing within individual
time windows without considering temporal dependencies
across scheduling decisions.
VIII. CONCLUSION
This paper presents TORTA, a novel scheme for distributed
GPU inference optimization that addresses the fundamental
limitations of existing reactive scheduling approaches. By
integrating reinforcement learning with optimal transport the-
ory and incorporating temporal dependencies into the allo-
cation process, TORTA achieves improvements in response
times, load balancing, and operational overhead across diverse
network topologies. The two-layer hierarchical architecture
effectively decomposes the complex spatiotemporal optimiza-
tion problem, enabling scalable solutions that consider both
geographical resource distribution and temporal dynamics.
Experimental results demonstrate that TORTA consistently
outperforms baseline methods across multiple metrics, achiev-
ing response time reductions of up to 15% and substantial
improvements in load balancing coefficients. The scheme
exhibits robustness to prediction inaccuracies and maintains
competitive performance even when demand forecasting accu-
racy is modest. However, several limitations remain: TORTA’s
performance gains depend on prediction quality with di-
minishing returns below 40% accuracy, and the two-layer
optimization introduces computational overhead for extremely
large-scale deployments. Future work will focus on extending
the framework to multi-tenant scenarios, investigating adaptive
prediction mechanisms for improved forecasting accuracy,
and developing lightweight variants for resource-constrained
environments.
APPENDIX A
THEORETICAL ANALYSIS OF TARTO
In this appendix, we provide a rigorous theoretical analy-
sis demonstrating that TORTA can provably outperform the
performance upper bound of single-timeslot reactive methods
in multi-timeslot scenarios. Our proof consists of three main
components: (1) establishing the performance upper bound
for single-timeslot methods, (2) analyzing the switching cost
convergence properties, and (3) deriving conditions under
which TORTA surpasses this upper bound.
A.
Upper Bound for Single-Timeslot Methods
We begin by establishing the theoretical upper bound for
any single-timeslot optimization method when applied across
multiple timeslots.
Definition 1 (Single-Timeslot Method). A single-timeslot
method M is a memoryless allocation strategy where the


--- Page 13 ---
JOURNAL OF LATEX CLASS FILES, 2025
13
decision at timeslot t depends only on the current state
(st, µt, νt), formally: AM
t
= fM(µt, νt, Ct).
Theorem 1 (Optimal Transport Achieves Single-Timeslot
Upper Bound). For any single timeslot t, the optimal transport
solution P ∗minimizes both response time and power cost
simultaneously, establishing the performance upper bound for
single-timeslot optimization.
Proof 1. Let wj = PR
i=1 P ∗
i,j denote the total workload
assigned to region j, and Tj = wj/cj be the response time
in region j with capacity cj. The maximum response time is
Tmax = maxj∈R Tj.
Suppose there exists an allocation P ′ such that T ′
max <
Tmax but P ′ is not the optimal transport solution. Since P ′ is
suboptimal for OT, there exist regions i, j such that w′
i/ci ̸=
w′
j/cj.
Without loss of generality, assume w′
i/ci > w′
j/cj. We
can construct a new allocation
˜P by redistributing tasks
from region i to region j such that ˜wi/ci = ˜wj/cj. This
redistribution satisfies:
X
j
˜wj =
X
j
w′
j
(workload conservation)
(13)
X
j
˜Pi,j = µ(i)
t
(demand constraint)
(14)
The new maximum response time satisfies:
˜Tmax = max
 ˜wi
ci
, ˜wj
cj

< max
w′
i
ci
, w′
j
cj

= T ′
max
This contradicts the assumption that P ∗is optimal. There-
fore, OT achieves the minimum possible response time.
For power cost optimality, since OT minimizes P
i,j Ci,jPi,j
where Ci,j incorporates regional electricity price differences,
it naturally achieves minimum power cost.
■
B. Switching Cost Convergence Analysis
Next, we analyze the switching costs incurred by single-
timeslot methods across multiple timeslots.
Assumption 1 (Temporal Independence). The resource distri-
butions {νt}T
t=1 and request distributions {µt}T
t=1 are tempo-
rally independent: νt ⊥νt′ and µt ⊥µt′ for all t ̸= t′.
Theorem 2 (Switching Cost Convergence). Under Assumption
1, for any single-timeslot method M, the expected switching
cost between consecutive timeslots converges to a method-
independent constant K0.
Proof 2. Let ∆M
t
= ||AM
t
−AM
t−1||2
F denote the switching
cost. We have:
E[∆M
t ] = E[||AM
t
−AM
t−1||2
F ]
(15)
= E[||AM
t ||2
F ] + E[||AM
t−1||2
F ] −2E[⟨AM
t , AM
t−1⟩]
(16)
By temporal independence and the memoryless property of
M:
E[⟨AM
t , AM
t−1⟩] = ⟨E[AM
t ], E[AM
t−1]⟩= ||E[AM]||2
F
Therefore:
E[∆M
t ] = 2E[||AM||2
F ] −2||E[AM]||2
F
(17)
= 2Var(AM) = K0
(18)
The key insight is that for methods satisfying the same
constraints (demand and capacity), the variance is primarily
determined by input distribution randomness rather than the
optimization algorithm itself, leading to method-independent
convergence.
■
Corollary 1 (Performance Upper Bound for Single-Timeslot
Methods). For any single-timeslot method M applied over T
timeslots, the total expected cost satisfies:
E[Costtotal
M ] ≥
T
X
t=1
(RTOT
t
+ β · PCOT
t ) + α · K0(T −1)
where RTOT
t
and PCOT
t
are the optimal response time and
power cost achieved by OT at timeslot t.
C. TORTA’s Performance Guarantee
Now we establish conditions under which TORTA can sur-
pass the performance upper bound of single-timeslot methods.
Definition 2 (TORTA Training Objective). TORTA is trained
with a constrained loss function:
Ltotal = LPPO + γ · Lϵ(s, ϵ) + δ · Ls(s, ϵ)
where:
Lϵ(s, ϵ) = max

0, ||Bt||F −ϵmax(s)
ϵ0

(19)
Ls(s, ϵ) = max

0, smin(ϵ) −s
s0

(20)
with Bt = ARL
t −AOT
t
and designed to ensure the performance
advantage condition.
Theorem 3 (TORTA Performance Guarantee). When trained
with the constrained loss function, TORTA satisfies:
1) Switching cost improvement: E[∆RL
t ] ≤K0/s for some
s > 1
2) Single-timeslot performance control: E[||Bt||F ] ≤ϵ
3) Performance advantage condition: When
1−1/s
ϵ
>
LR+βLP
α·K0
, TORTA strictly outperforms all single-timeslot
methods.
Proof 3. Part 1 (Switching cost improvement): The PPO
training objective includes an explicit smoothness reward:
rsmooth
t
= −||ARL
t −ARL
t−1||2
F
Through policy gradient optimization, the converged policy
satisfies:
E[∆RL
t ] = E[||ARL
t −ARL
t−1||2
F ] ≤K0
s
where s > 1 is the switching cost improvement factor
controlled by the smoothness weight.


--- Page 14 ---
JOURNAL OF LATEX CLASS FILES, 2025
14
Part 2 (Single-timeslot performance control): Let Bt =
ARL
t −AOT
t . Under the OT supervision constraint, the perfor-
mance degradation is bounded by:
δRt ≤LR||Bt||F
(21)
δPt ≤LP ||Bt||F
(22)
where
LR, LP
are
Lipschitz
constants.
The
constraint
E[||Bt||F ] ≤ϵ is enforced by Lϵ.
Part 3 (Performance advantage): The total cost difference
between TORTA and the single-timeslot upper bound is:
Diff = CostTORTA −CostUpper
(23)
=
T
X
t=1
(δRt + βδPt) + α
T −1
X
t=1
(∆RL
t −K0)
(24)
≤
T
X
t=1
[(LR + βLP )ϵ] + α
T
X
t=1
K0
s −K0

(25)
= T

(LR + βLP )ϵ −αK0

1 −1
s

(26)
When the performance advantage condition
1−1/s
ϵ
>
LR+βLP
α·K0
holds, we have:
1 −1
s > ϵ · LR + βLP
α · K0
Substituting this into the Diff expression:
Diff < T

(LR + βLP )ϵ −αK0 · ϵ · LR + βLP
α · K0

(27)
= T [(LR + βLP )ϵ −(LR + βLP )ϵ] = 0
(28)
Therefore, CostTORTA < CostUpper, proving TORTA’s strict
superiority.
■
Remark 1. The performance advantage condition 1−1/s
ϵ
>
LR+βLP
α·K0
provides explicit guidance for training TORTA. It
shows that superior performance can be achieved by either:
(1) increasing the switching cost improvement factor s, (2)
tightening the OT deviation bound ϵ, or (3) increasing the
switching cost weight α relative to response time and power
cost weights.
APPENDIX B
TRAINING IMPLEMENTATION DETAILS
A. Network Details
For Reinforcement learning, the policy network πθ is im-
plemented as a fully connected neural network with three
hidden layers (256, 512, 256 units) and ReLU activations. The
output layer uses a softmax activation to generate probability
distributions over regional allocations. The value network Vϕ
shares the same architecture but outputs scalar state values.
We use the Adam optimizer with an initial learning rate of
3 × 10−4 and exponential decay with factor 0.995 every 100
episodes.
The demand predictor Ft = Predictor(Ut−K:t, Qt−K:t,
Ht−K:t) employs a simple yet effective multi-layer percep-
tron architecture. The input consists of concatenated his-
torical features over K = 5 timeslots, yielding Input =
Algorithm 2: TORTA Training Procedure
Input : Historical data D, Environment parameters, Target
condition 1−1/s
ϵ
> LR+βLP
α·K0
Output: Trained policy πθ, predictor Pψ
// Initialization and Baseline Parameter
Estimation
1 Initialize policy network πθ, value network Vϕ, predictor Pψ;
2 Pre-train predictor Pψ on historical data D;
// Estimate Baseline Parameters for
Theoretical Conditions
3 K0 ←Compute baseline switching costs from reactive
methods;
4 LR, LP ←Estimate Lipschitz constants via finite
differences;
5 Set target parameters: ϵtarget = 0.15, starget = 2.5;
6 for epoch e = 1 to E do
// Collect Experience with Constraint
Monitoring
7
for environment step t = 1 to T do
8
Collect experience (st, at, rt, st+1) from
environment;
9
Compute OT baseline AOT
t
using current µt, νt;
10
Calculate deviation Bt = ARL
t
−AOT
t ;
11
Update switching cost estimate scurrent ←
K0
E[∆RL
t ];
12
Update constraint weights γt, δt based on current
violations;
// Policy Update with Theoretical
Constraints
13
Compute PPO loss LPPO from collected experience;
14
Compute constraint losses Lϵ = max(0,
||Bt||F −ϵtarget
ϵ0
);
15
Compute constraint losses Ls = max(0,
starget−scurrent
s0
);
// Gradient Update and Condition
Validation
16
θ ←θ −∇θ(LPPO + γtLϵ + δtLs);
17
Validate performance advantage condition:
1−1/scurrent
ϵcurrent
?> LR+βLP
α·K0
;
18
if condition violated then increase γt, δt by factor 1.5;
19 return πθ, Pψ
Concat([Ut−5:t, Qt−5:t, Ht−5:t]) ∈R5×3R. The network ar-
chitecture follows a standard 3-layer design with ReLU activa-
tions: input layer (15R dimensions) →hidden layer (512 units)
→hidden layer (256 units) →output layer (R dimensions with
softmax). The training objective minimizes mean squared error
with L2 regularization: Lpred =
1
N
PN
i=1 ||F pred
t+1 −F actual
t+1 ||2
2 +
λ||θ||2
2 where λ = 10−4.
B. Constrained Training Framework
To ensure the performance advantage condition
1−1/s
ϵ
>
LR+βLP
α·K0
is satisfied during training, we implement adaptive
constraint enforcement. The constraint weights γt and δt for
terms Lϵ and Ls are dynamically adjusted based on current
performance: γt = γ0 · exp(αγ · max(0, ||Bt||F −ϵtarget))
and δt = δ0 · exp(αδ · max(0, starget −scurrent)), where Bt =
ARL
t −AOT
t
represents the deviation from the optimal transport
solution.
The critical system parameters required for theoretical guar-
antees are estimated continuously during training. The baseline


--- Page 15 ---
JOURNAL OF LATEX CLASS FILES, 2025
15
switching cost K0 is computed as a moving average of switch-
ing costs from reactive baseline methods over past timeslots.
The Lipschitz constants LR and LP for response time and
power cost functions are estimated empirically using finite
differences over small perturbations in allocation matrices. The
current switching cost improvement factor scurrent is calculated
as the ratio of baseline switching costs to current RL switching
costs, updated every 10 training episodes.
REFERENCES
[1] Anthropic. Introducing the next generation of claude, 2024.
[2] Alexander Borzunov, Max Ryabinin, Artem Chumachenko, Dmitry
Baranchuk, Tim Dettmers, Younes Belkada, Pavel Samygin, and Colin
Raffel. Distributed inference and fine-tuning of large language models
over the internet, 2023.
[3] Seungbeom Choi, Jeonghoe Goo, Eunjoo Jeon, Mingyu Yang, and
Minsung Jang.
Elis: Efficient llm iterative scheduling system with
response length predictor, 2025.
[4] Arnab Choudhury, Yang Wang, Tuomas Pelkonen, Kutta Srinivasan,
Abha Jain, Shenghao Lin, Delia David, Siavash Soleimanifard, Michael
Chen, Abhishek Yadav, Ritesh Tijoriwala, Denis Samoylov, and Chun-
qiang Tang.
MAST: Global scheduling of ML training across Geo-
Distributed datacenters at hyperscale. In 18th USENIX Symposium on
Operating Systems Design and Implementation (OSDI 24), pages 563–
580, Santa Clara, CA, July 2024. USENIX Association.
[5] DeepSeek-AI, Aixin Liu, et al. Deepseek-v3 technical report, 2025.
[6] Haotian Dong, Jingyan Jiang, Rongwei Lu, Jiajun Luo, Jiajun Song,
Bowen Li, Ying Shen, and Zhi Wang. Beyond a single ai cluster: A
survey of decentralized llm training, 2025.
[7] Leo Donisch, Sigurd Schacht, and Carsten Lanquillon. Inference opti-
mizations for large language models: Effects, challenges, and practical
considerations, 2024.
[8] Yao Fu et al. ServerlessLLM: Low-Latency serverless inference for large
language models. In 18th USENIX Symposium on Operating Systems
Design and Implementation (OSDI 24), pages 135–153, Santa Clara,
CA, July 2024. USENIX Association.
[9] Yichao Fu, Siqi Zhu, Runlong Su, Aurick Qiao, Ion Stoica, and Hao
Zhang. Efficient llm scheduling by learning to rank, 2024.
[10] Farzad Habibi, Tania Lorido-Botran, Ahmad Showail, Daniel C. Stur-
man, and Faisal Nawab.
Msf-model: Queuing-based analysis and
prediction of metastable failures in replicated storage systems. In 2024
43rd International Symposium on Reliable Distributed Systems (SRDS),
pages 12–22, 2024.
[11] Jan Hansen-Palmus, Michael Truong Le, Oliver Hausd¨orfer, and Alok
Verma. Communication compression for tensor parallel llm inference,
2024.
[12] Yunhai Hu, Zining Liu, Zhenyuan Dong, Tianfan Peng, Bradley Mc-
Danel, and Sai Qian Zhang.
Speculative decoding and beyond: An
in-depth survey of techniques, 2025.
[13] Lexiang Huang, Matthew Magnusson, Abishek Bangalore Muralikr-
ishna, Salman Estyak, Rebecca Isaacs, Abutalib Aghayev, Timothy Zhu,
and Aleksey Charapko. Metastable failures in the wild. In 16th USENIX
Symposium on Operating Systems Design and Implementation (OSDI
22), pages 73–90, Carlsbad, CA, July 2022. USENIX Association.
[14] Changho Hwang et al.
Tutel: Adaptive mixture-of-experts at scale.
Proceedings of Machine Learning and Systems, 5:269–287, 2023.
[15] Ranggi Hwang et al. Pre-gated moe: An algorithm-system co-design
for fast and scalable mixture-of-expert inference. In 2024 ACM/IEEE
51st Annual International Symposium on Computer Architecture (ISCA),
pages 1018–1031. IEEE, 2024.
[16] Youhe Jiang et al.
Demystifying cost-efficiency in llm serving over
heterogeneous gpus, 2025.
[17] Jens Kehne et al. Gpuswap: Enabling oversubscription of gpu memory
through transparent swapping.
SIGPLAN Not., 50(7):65–77, March
2015.
[18] Kyoungmin Kim, Kijae Hong, Caglar Gulcehre, and Anastasia Ailamaki.
Optimizing llm inference for database systems: Cost-aware scheduling
for concurrent requests, 2025.
[19] Ferdi Kossmann, Bruce Fontaine, Daya Khudia, Michael Cafarella, and
Samuel Madden. Is the gpu half-empty or half-full? practical scheduling
techniques for llms, 2025.
[20] Fan Lai et al. ModelKeeper: Accelerating DNN training via automated
training warmup. In 20th USENIX Symposium on Networked Systems
Design and Implementation (NSDI 23), pages 769–785, Boston, MA,
April 2023. USENIX Association.
[21] Vili Lehdonvirta et al. Compute North vs. Compute South: The Uneven
Possibilities of Compute-Based AI Governance Around the Globe, page
828–838. AAAI Press, 2025.
[22] Jialong Li et al. Optimizing mixture-of-experts inference time combining
model deployment and communication scheduling.
arXiv preprint
arXiv:2410.17043, 2024.
[23] Jiamin Li et al. Accelerating distributed {MoE} training and inference
with lina. In 2023 USENIX Annual Technical Conference (USENIX ATC
23), pages 945–959, 2023.
[24] Zhuohan Li et al.
AlpaServe: Statistical multiplexing with model
parallelism for deep learning serving. In 17th USENIX Symposium on
Operating Systems Design and Implementation (OSDI 23), pages 663–
679, Boston, MA, July 2023. USENIX Association.
[25] Zejia Lin, Hongxin Xu, Guanyi Chen, Xianwei Zhang, and Yutong Lu.
Bullet: Boosting gpu utilization for llm serving via dynamic spatial-
temporal orchestration, 2025.
[26] Michele Marzollo, Jiawei Zhuang, Niklas Roemer, Lorenz K. M¨uller,
and Lukas Cavigelli. Sssd: Simply-scalable speculative decoding, 2024.
[27] Usama Naseer, Luca Niccolini, Udip Pant, Alan Frindell, Ranjeeth Dasi-
neni, and Theophilus A. Benson. Zero downtime release: Disruption-
free load balancing of a multi-billion user website. In SIGCOMM ’20,
page 529–541, New York, NY, USA, 2020. Association for Computing
Machinery.
[28] OpenAI. Openai status history. Documenting server load fluctuations
and regional service limitations.
[29] OpenAI.
Pre-training GPT-4.5.
Video interview discussing GPU
dependency in GPT-4’s reasoning process.
[30] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
[31] Sebastian
Orlowski,
Roland
Wess¨aly,
Michal
Pi´oro,
and
Artur
Tomaszewski. Sndlib 1.0—survivable network design library. Networks:
An International Journal, 55(3):276–286, 2010.
[32] Xiurui Pan, Endian Li, Qiao Li, Shengwen Liang, Yizhou Shan,
Ke Zhou, Yingwei Luo, Xiaolin Wang, and Jie Zhang. Instinfer: In-
storage attention offloading for cost-effective long-context llm inference,
2024.
[33] Samyam Rajbhandari et al.
Deepspeed-moe: Advancing mixture-of-
experts inference and training to power next-generation ai scale.
In
International conference on machine learning, pages 18332–18346.
PMLR, 2022.
[34] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake,
Carlo Luschi, and Douglas Orr.
Sparq attention: Bandwidth-efficient
llm inference, 2024.
[35] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and
Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint
arXiv:1707.06347, 2017.
[36] Rana Shahout, Eran Malach, Chunwei Liu, Weifan Jiang, Minlan Yu,
and Michael Mitzenmacher.
Don’t stop me now: Embedding based
scheduling for llms, 2024.
[37] Kun Suo et al.
Quantifying context switch overhead of artificial
intelligence workloads on the cloud and edges. In Proceedings of the
36th Annual ACM Symposium on Applied Computing, SAC ’21, page
1182–1189, New York, NY, USA, 2021. Association for Computing
Machinery.
[38] Gemini Team et al. Gemini: A family of highly capable multimodal
models, 2025.
[39] TechCrunch Staff. Openai’s GPT-4.5 delayed due to GPU shortages, pro
users get early access. TechCrunch, 2025. Reports CEO Sam Altman’s
confirmation of phased rollout due to GPU scarcity.
[40] Shahin Vakilinia. Energy efficient temporal load aware resource alloca-
tion in cloud computing datacenters. J. Cloud Comput., 7(1), December
2018.
[41] C´edric Villani et al.
Optimal transport: old and new, volume 338.
Springer, 2008.
[42] World Population Review. Cost of electricity by country 2025, 2025.
Accessed: [2025-05].
[43] Bingyang Wu, Yinmin Zhong, Zili Zhang, Shengyu Liu, Fangyue Liu,
Yuanhang Sun, Gang Huang, Xuanzhe Liu, and Xin Jin. Fast distributed
inference serving for large language models, 2024.
[44] Heming Xia, Cunxiao Du, Yongqi Li, Qian Liu, and Wenjie Li. Tutorial
proposal: Speculative decoding for efficient llm inference, 2025.
[45] Tian Xia, Ziming Mao, Jamison Kerney, Ethan J. Jackson, Zhifei Li,
Jiarong Xing, Scott Shenker, and Ion Stoica. Skylb: A locality-aware
cross-region load balancer for llm inference, 2025.
[46] Kaiyu
Xie
et
al.
Nvidia/TensorRT-LLM.
https://github.com/NVIDIA/TensorRT-LLM, jul 1 2025.


--- Page 16 ---
JOURNAL OF LATEX CLASS FILES, 2025
16
[47] Rui Xie, Asad Ul Haq, Linsen Ma, Yunhua Fang, Zirak Burzin Engineer,
Liu Liu, and Tong Zhang. Reimagining memory access for llm inference:
Compression-aware memory controller design, 2025.
[48] Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong
Zhou, Xia Hu, and Anshumali Shrivastava. Compress, then prompt: Im-
proving accuracy-efficiency trade-off of llm inference with transferable
prompt, 2023.
[49] Peng Yang, Laoming Zhang, Haifeng Liu, and Guiying Li. Reducing
idleness in financial cloud services via multi-objective evolutionary
reinforcement learning based load balancer. Science China Information
Sciences, 67(2), January 2024.
[50] Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang,
Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind
Krishnamurthy, and Luis Ceze. Flashinfer: Efficient and customizable
attention engine for llm inference serving, 2025.
[51] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and
Byung-Gon Chun. Orca: A distributed serving system for Transformer-
Based generative models. In 16th USENIX Symposium on Operating
Systems Design and Implementation (OSDI 22), pages 521–538, Carls-
bad, CA, July 2022. USENIX Association.
[52] Gulnara Zhabelova, Alireza Yavarian, and Valeriy Vyatkin. Data center
power dynamics within the settings of regional power grid. In 2015
IEEE 20th Conference on Emerging Technologies & Factory Automation
(ETFA), pages 1–5, 2015.
[53] Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, and
Yang You. Response length perception and sequence scheduling: An
llm-empowered llm inference pipeline, 2023.
[54] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu,
Xuanzhe Liu, Xin Jin, and Hao Zhang. Distserve: Disaggregating prefill
and decoding for goodput-optimized large language model serving,
2024.
