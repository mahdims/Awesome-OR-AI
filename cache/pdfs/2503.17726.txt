--- Page 1 ---
arXiv:2503.17726v1  [cs.AI]  22 Mar 2025
JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
1
A Survey on Mathematical Reasoning and
Optimization with Large Language Models
Ali Forootani∗, Senior Member, IEEE
Abstract—Mathematical reasoning and optimization
are fundamental to artiﬁcial intelligence and computa-
tional problem-solving. Recent advancements in large
language models have signiﬁcantly enhanced AI-driven
mathematical reasoning, theorem proving, and opti-
mization techniques. This survey explores the evolution
of mathematical problem-solving in AI, from early sta-
tistical learning approaches to the latest deep learning-
based and transformer-driven methodologies. We re-
view the capabilities of pre-trained language models
and Large Language Models in handling arithmetic
operations, complex mathematical reasoning, theorem
proving, and structured symbolic computation.
A key focus is on how Large Language Models
integrate with optimization and control frameworks,
including mixed-integer programming, linear quadratic
control, and multi-agent optimization strategies. We
discuss how Large Language Models assist in prob-
lem formulation, constraint generation, and heuristic
search, thereby bridging the gap between theoretical
mathematical reasoning and practical optimization ap-
plications. Additionally, we analyze various enhance-
ment techniques, including Chain-of-Thought reason-
ing, instruction tuning, and tool-augmented approaches
that improve Large Language Model’s problem-solving
capabilities.
Despite these advancements, Large Language Models
face challenges in numerical precision, logical consis-
tency, and veriﬁcation of mathematical proofs. We
highlight emerging trends in hybrid neural-symbolic
reasoning, structured prompt engineering, and multi-
step self-correction mechanisms aimed at improving
Large Language Model performance in mathematical
and optimization tasks. Future research should fo-
cus on reﬁning interpretability, integrating domain-
speciﬁc solvers, and ensuring the robustness of AI-
driven decision-making frameworks. This survey pro-
vides a comprehensive review of the current landscape
and future directions of mathematical reasoning and
optimization with Large Language Models, oﬀering
insights into their potential applications across engi-
neering, ﬁnance, and scientiﬁc research.
Index Terms—Mathematical Reasoning, Optimiza-
tion,
Large
Language
Models
(LLMs),
Chain-of-
Thought (CoT) Reasoning, Theorem Proving, Sym-
bolic Computation, Hybrid Neural-Symbolic Methods,
AI-Driven Decision-Making, Control and Optimiza-
tion, Linear Programming.
I. Introduction
Mathematics serves as a cornerstone of human cogni-
tion, underpinning a vast array of disciplines, including the
natural sciences, engineering, medicine, ﬁnance, computer
Ali Forootani is with Helmholtz Centre for Environmental Re-
search - UFZ, Permoserstraße 15, 04318 Leipzig, Germany, email:
ali.forootani@ufz.de, aliforootani@ieee.org.
science, and the social sciences. In the ﬁeld of natural
language processing (NLP), the development of compu-
tational models capable of autonomously solving math-
ematical word problems has been an area of signiﬁcant
research interest since the early 1960s [1]–[4]. This pur-
suit is not merely about solving arithmetic and algebraic
expressions but also about advancing general reasoning
mechanisms, which are fundamental to achieving artiﬁcial
general intelligence (AGI) [5]. By tackling the complexities
of mathematical reasoning, researchers aim to push the
boundaries of AI systems’ logical deduction capabilities
and their ability to understand and manipulate symbolic
knowledge.
Early approaches to solving mathematical word prob-
lems primarily relied on statistical learning techniques [6]–
[9]. Over time, more sophisticated methodologies emerged,
incorporating semantic parsing and structured representa-
tions [7], [10]–[13]. These innovations laid the foundation
for modern deep learning-based mathematical problem-
solving, where neural networks have become a powerful
tool for automating complex mathematical reasoning tasks
[5], [14].
In recent years, the ﬁeld has witnessed unprecedented
advancements, largely driven by the rise of powerful pre-
trained language models (PLMs) and large language mod-
els (LLMs) [15], [16]. These models, including BERT [17],
RoBERTa [18], BART [19], GPT-1 [20], and GPT-2 [21],
have demonstrated remarkable capabilities in learning
from large-scale textual corpora, thereby acquiring general
linguistic and numerical reasoning skills.
To enhance the mathematical reasoning abilities of
PLMs, researchers have explored various techniques, in-
cluding pre-training and ﬁne-tuning on specialized math-
ematical datasets [22]–[24]. Several models, such as Gen-
BERT [23], NF-NSM [24], MathBERT [25], and LISA [26],
integrate numerical data and mathematical expressions
into their training objectives, enhancing their ability to
perform symbolic and numerical reasoning. Moreover, ar-
chitectural innovations such as specialized loss functions
and task-speciﬁc modules have been proposed to further
reﬁne PLM-based mathematical comprehension [27]–[30].
The emergence of large-scale LLMs, exempliﬁed by
GPT-4 [31], has signiﬁcantly transformed mathematical
problem-solving. These models have demonstrated re-
markable generalization across various NLP tasks, bene-
ﬁting from key techniques such as in-context learning [32]–
[34] and instruction learning [35], [36]. Notably, research
by Wang et al. [37] suggests that LLMs with over 100
billion parameters—such as GPT-3 (175 billion) [33] and


--- Page 2 ---
2
JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
PaLM (540 billion) [38]—can eﬀectively solve complex
mathematical problems through chain-of-thought (CoT)
reasoning when provided with well-structured demonstra-
tions [15].
Advancements in CoT methodologies have further im-
proved the reasoning capabilities of LLMs, leading to
the development of structured reasoning techniques, pro-
grammatic reasoning frameworks, and dynamic CoT mod-
els [39]–[42]. Additionally, tool-augmented models that
integrate external symbolic manipulation systems have
been proposed to enhance mathematical problem-solving
performance [43]–[45]. Such approaches highlight the po-
tential of combining neural and symbolic reasoning to
achieve more robust AI systems capable of higher-order
mathematical cognition.
Several surveys have reviewed diﬀerent aspects of math-
ematical reasoning in LLMs. Notably:
•
[46] provides a comprehensive survey of deep
learning methodologies for mathematical reasoning
(DL4Math).
•
[47]
explores general reasoning mechanisms in
language
models,
covering
arithmetic,
common-
sense, logical, symbolic, and multi-modal reasoning
(LM4Reasoning).
•
[48] presents an analytical review of chain-of-thought
(CoT) reasoning methodologies.
•
[49] and [50] oﬀer surveys on pre-trained language
models (PLMs) and LLMs, respectively.
•
[51] reviews multi-modal LLMs for mathematical
reasoning (MMLM4Math).
Overall, the fusion of deep learning techniques, struc-
tured reasoning approaches, and tool-augmented models
continues to drive rapid progress in AI-driven mathe-
matical problem-solving. While existing methods have
demonstrated impressive performance, fundamental chal-
lenges remain, particularly in areas such as formal proof
generation, theorem proving, and real-world mathematical
applications. As research continues to push the bound-
aries of AI’s mathematical reasoning capabilities, future
advancements will likely be characterized by improved
interpretability, robustness, and the seamless integration
of symbolic and neural reasoning paradigms.
A. Bridging Mathematical Reasoning and Optimization
with LLMs
Mathematical reasoning forms the foundation of logical
inference, problem-solving, and numerical computation in
artiﬁcial intelligence [52]. Large language models (LLMs)
have demonstrated remarkable capabilities in handling
mathematical reasoning tasks, including theorem prov-
ing, algebraic manipulation, and structured numerical
problem-solving. Through advancements such as Chain-
of-Thought (CoT) prompting, programmatic reasoning,
and tool-augmented inference, LLMs have shown substan-
tial progress in solving complex mathematical problems
[53]. However, the transition from theoretical reasoning to
practical applications, such as optimization and control,
requires an additional layer of adaptability where models
can not only deduce but also generate and reﬁne solutions
iteratively.
Optimization and control problems often demand a
structured approach where solutions must satisfy strict
mathematical constraints while achieving predeﬁned ob-
jectives. In scenarios such as mixed-integer programming
(MIP) and linear programming (LP), traditional solvers
rely on well-established algorithms, including branch-and-
bound methods and gradient-based optimization. While
LLMs may not replace these solvers outright, they can
signiﬁcantly enhance the modeling process by formulat-
ing problem statements, generating initial constraints,
and suggesting heuristic improvements [54]. By leveraging
their extensive pretraining in symbolic mathematics and
structured problem-solving, LLMs can aid domain experts
in deﬁning well-posed optimization problems and verifying
computational solutions.
A crucial advantage of LLMs in this domain lies in
their ability to process natural language descriptions of
complex optimization scenarios and convert them into
structured mathematical formulations [55]. For instance,
in the context of battery energy storage system (BESS)
optimization, an LLM can help translate high-level policy
constraints—such as cost minimization, peak shaving, or
energy arbitrage—into a solvable mathematical model.
Similarly, in control applications like linear quadratic regu-
lation (LQR), LLMs can assist in deriving optimal control
laws by structuring the necessary state-space representa-
tions and computing feedback gains iteratively.
By integrating mathematical reasoning with optimiza-
tion and control strategies, LLMs present an opportunity
to bridge the gap between abstract theoretical models and
real-world engineering applications [56]. Their ability to
automate certain aspects of model formulation, constraint
generation, and heuristic search makes them valuable tools
for decision-making in complex systems. Future research
should focus on reﬁning their numerical precision, en-
hancing symbolic computation capabilities, and improving
their ability to interface seamlessly with existing optimiza-
tion solvers. This synergy between mathematical reasoning
and optimization will pave the way for more intelligent,
interpretable, and eﬃcient decision-support systems in
engineering and scientiﬁc domains.
This paper is organized as follows: In Section II, we
introduce the types of mathematical tasks that moti-
vate the use of language models. Section III presents
pre-trained language model (PLM)-based approaches for
mathematical reasoning, with
a
particular focus
on
Non-Autoregressive Language Models discussed in Sec-
tion III-B. Section IV explores large language model
(LLM)-based methods tailored for mathematical reason-
ing. In Section V, we compare various mathematical
LLMs, followed by a performance analysis in Section VI.
Section VIII investigates the application of LLMs to
time series analysis, outlining the associated challenges,
methodologies, and future directions. A technical compar-
ison of commonly used LLMs is provided in Section IX.


--- Page 3 ---
FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE3
Section X discusses the role of LLMs in optimization
and control. Section XI highlights open challenges and
outlines potential research directions. Finally, conclusions
are drawn in Section XII.
II. Mathematical Tasks
In this section, we categorize mathematical tasks into
two primary domains: mathematical calculation and math-
ematical reasoning.
A. Mathematical Calculation
The emergence of LMs has signiﬁcantly inﬂuenced
computational research, particularly in performing arith-
metic operations. Early LMs exhibited only rudimentary
arithmetic capabilities, encoding numbers as mere tex-
tual representations. However, as these models evolved,
they demonstrated the ability to develop arithmetic skills
through ﬁne-tuning on specialized datasets, even without
explicitly designed numerical representations [57].
1) Arithmetic Representation: Initially, numerical val-
ues were often omitted, oversimpliﬁed, or treated as or-
dinary text tokens, leading to suboptimal performance
on mathematical tasks. For instance, BERT performs ﬁve
times worse when the expected answer is a numerical value
rather than a textual span in the DROP benchmark [58].
This limitation highlighted the necessity of reﬁning how
LMs process and represent numbers.
Recent research has proposed several techniques to en-
hance numerical representation within LMs:
• GenBERT [23] tokenizes numbers at the digit level
and ﬁne-tunes on arithmetic word problems.
• In [59] a technique is introduced that converts
numbers
into
scientiﬁc
notation
(e.g.,
314.1
as
3141[EXP]2) to facilitate better numerical under-
standing.
• Studies such as [60] and [61] integrate digit embed-
dings into a single embedding that represents entire
numbers more eﬀectively.
• In [62] a method is proposed to leverage digit-RNN
and exponent embeddings, emphasizing the exponent
while disregarding the mantissa for improved numer-
ical processing.
• GOAT [36] introduced Consistent Tokenization to
enhance the numerical representation consistency
within LMs.
These techniques have collectively improved the way
LMs interpret, store, and manipulate numerical values,
signiﬁcantly boosting their eﬀectiveness in arithmetic rea-
soning.
2) Arithmetic Calculation: The ability of LMs to per-
form arithmetic computations has been a widely studied
topic. Early research primarily focused on fundamental
operations such as addition and subtraction [63], [64],
whereas more recent studies have expanded into complex
operations like multi-digit multiplication [65].
Key ﬁndings in arithmetic calculation include:
• In [66] arithmetic capabilities across various LMs,
including GPT-4 [31], Galactica [67], and LLaMA [68]
hase been evaluated.
• In [69]
has been
demonstrated that specialized
prompt engineering can signiﬁcantly improve addition
accuracy, though multiplication remains challenging
for numbers exceeding seven digits.
• In [70] length generalization has been investigated
in arithmetic using relative position embeddings and
training set priming.
• ScratchpadGPT
[71]
leveraged
Chain-of-Thought
(CoT) prompting to enhance addition accuracy.
• In GOAT [36] supervised instruction tuning has
been applied to improve performance on large-integer
arithmetic tasks.
• MathGLM [35] decomposed complex arithmetic ex-
pressions into simpler subproblems, enhancing model
precision.
These advancements illustrate the growing sophistica-
tion of LMs in arithmetic computations and their potential
applications in computational mathematics.
B. Mathematical Reasoning
Mathematical reasoning is a critical capability in artiﬁ-
cial intelligence, enabling models to understand, explain,
and solve complex mathematical problems. Research in
this domain has focused on two primary areas: math
problem-solving and theorem proving.
1) Math Problem Solving: Math problem-solving re-
quires LMs to apply logical reasoning and computational
models to solve mathematical problems at various levels
of complexity. Generally, mathematical problems fall into
two categories:
• Contextualized problems: These involve understand-
ing a given scenario, translating it into a mathemati-
cal problem, and generating a step-by-step solution.
• Direct mathematical problems: These require direct
computation of a solution without the need for addi-
tional contextual interpretation.
Several notable advancements in this domain include:
• MathPrompter [53] utilizes GPT-3 DaVinci for solv-
ing mathematical word problems (MWPs).
• In [72] Rejection Sampling Fine-Tuning (RFT) has
been proposed to enhance mathematical reasoning
capabilities in LMs.
• MetaMath [73] generates synthetic mathematical
problems to facilitate self-improvement.
• WizardMath [74] applies evolutionary instruction
tuning to reﬁne LM’s mathematical reasoning skills.
• MathAttack [75] evaluates the robustness of LMs
against
adversarial
examples
in
mathematical
problem-solving.
• LLEMMA [76] is an open-source LM designed specif-
ically for mathematical reasoning.
These models illustrate the ongoing eﬀort to reﬁne LM’s
ability to reason mathematically and solve increasingly
complex problems.


--- Page 4 ---
4
JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
2) Theorem Proving: Theorem proving (TP) is a chal-
lenging AI task that involves verifying mathematical state-
ments using formal logic and structured reasoning. Recent
research has investigated the ability of LMs to engage in
theorem proving by leveraging deep learning techniques
and formal proof veriﬁcation systems.
Key advancements in this area include:
• GPT-f [77] generates proof strategies using deep
learning-based methods.
• In [78] incorporating informal proofs into TP models
to enhance theorem reasoning has been proposed.
• NaturalProofs [79] compiled a comprehensive corpus
for evaluating theorem-proving models.
• DeepMath [80] and INT [81] assess the generalization
capabilities of LMs in theorem proving.
• CoqGym [82] provides a dataset of human-written
proofs to aid LM training in formal theorem proving.
• ASTactic [82] autonomously generates proof strate-
gies using deep learning.
• LLEMMA [76] explores auto-formalization and tactic
prediction for mathematical proof generation.
Despite promising advancements, a signiﬁcant challenge
remains: hallucination in neural text generation [83], [84].
Errors in logical reasoning can accumulate across proof
steps, leading to incorrect conclusions. This highlights the
urgent need for robust veriﬁcation mechanisms to ensure
mathematical rigor in AI-generated proofs.
Mathematical reasoning and arithmetic problem-solving
represent crucial steps in improving LMs’ structured rea-
soning capabilities. While current research continues to
push the boundaries of AI-driven mathematical under-
standing, future advancements will likely focus on inter-
pretability, error reduction, and the seamless integration
of symbolic and neural reasoning approaches.
III. Pre-trained Language Models
(PLMs)-Based Approaches for Mathematical
Reasoning
PLMs, built on the Transformer architecture [85],
have profoundly inﬂuenced Natural Language Processing
(NLP) through the pre-training and ﬁne-tuning paradigm.
PLMs have been extensively utilized in text generation
[86] and can be broadly categorized into two major types:
Autoregressive LMs (ALMs), such as GPT-1 [20] and T5
[87], and Non-Autoregressive LMs (NALMs), including
BERT [88] and RoBERTa [18]. These architectures have
laid the groundwork for advanced mathematical reasoning
models, oﬀering new capabilities in symbolic manipula-
tion, theorem proving, and arithmetic problem-solving.
A. Autoregressive Language Models
Autoregressive Language Models (ALMs) generate text
in a sequential manner, where each token is predicted
based on the preceding tokens. These models are primarily
categorized into two types: causal decoders, such as GPT-
1 [20] and GPT-2 [21], and encoder-decoder architec-
tures, exempliﬁed by T5 [87]. These architectures have
signiﬁcantly contributed to advancements in mathematical
reasoning within Pretrained Language Models (PLMs).
Several research eﬀorts have focused on adapting au-
toregressive models to improve mathematical reasoning
capabilities:
• GPT-f [77] explores automated theorem proving,
demonstrating that large-scale language models can
generate formal proofs, thereby showcasing their po-
tential for logical deduction.
• The
Expression-Pointer
Transformer
(EPT)
[89]
addresses expression fragmentation issues, signiﬁ-
cantly enhancing equation-solving abilities in sym-
bolic mathematical tasks.
• Generate & Rank [90] introduces a ranking-based
post-processing mechanism that reﬁnes the quality of
generated solutions for mathematical word problems
(MWPs).
• THOR [91] integrates theorem proving within lan-
guage models, leveraging logical reasoning to improve
deductive capabilities.
• HyperTree Proof Search (HTPS) [92] optimizes proof
search strategies by incorporating prior results, sur-
passing GPT-f in theorem-proving eﬃciency and ac-
curacy.
• Galactica [67] enhances performance on mathematical
reasoning benchmarks, outperforming conventional
PLMs on datasets such as MMLU [93] and MATH
[94] through improved pretraining and ﬁne-tuning
techniques.
The performance of autoregressive PLMs in mathemat-
ical reasoning has been further reﬁned through the devel-
opment of specialized datasets and ﬁne-tuning methodolo-
gies:
• MATH-PLM [94] underscores the challenge of scal-
ing PLMs for mathematical reasoning and provides
benchmark datasets that serve as rigorous evaluation
tools.
• LISA [26] constructs theorem-proving models by
leveraging formal proof structures, thereby strength-
ening structured logical reasoning within PLMs.
• PACT [95] introduces self-supervised learning tech-
niques to enhance proof-term generation, improving
the interpretability and accuracy of mathematical
proofs.
• Minerva [96] ﬁne-tunes PLMs on high-quality tech-
nical mathematical content, achieving state-of-the-
art (SOTA) performance on various mathematical
reasoning tasks.
• LIME [97] proposes a novel pretraining strategy
that induces mathematical reasoning biases, leading
to superior performance compared to conventional
transformer-based models.
The success of autoregressive LMs in solving mathemat-
ical problems can largely be attributed to their capacity to
model complex sequential dependencies. This makes them
particularly well-suited for theorem proving, symbolic ma-
nipulation, and structured logical inference tasks.


--- Page 5 ---
FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE5
B. Non-Autoregressive Language Models
Non-Autoregressive LMs (NALMs) diﬀer from their
autoregressive counterparts by generating multiple tokens
simultaneously rather than sequentially. This parallelism
signiﬁcantly reduces dependency on previously generated
tokens, leading to improved eﬃciency. Models such as
BERT [88] and RoBERTa [18] employ masked token pre-
diction, allowing them to learn contextual representations
more eﬀectively, making them well-suited for NLP tasks
that require robust comprehension rather than generative
capabilities.
In mathematical reasoning, ﬁne-tuned variants of BERT
have demonstrated remarkable performance:
• Aristo [22] adapts BERT for solving scientiﬁc exam
questions, demonstrating its ability to process struc-
tured reasoning problems.
• GenBERT [23] and NF-NSM [24] enhance numerical
reasoning by incorporating numerical data into pre-
training objectives.
• MWP-BERT [98] improves numerical representation
in mathematical word problems through numeracy-
augmented pretraining.
• MathBERT
[25]
jointly
trains
on
both
textual
and mathematical formula representations, improving
mathematical comprehension.
• TAGOP [27], MT2Net [28], and DeductReasoner [29]
leverage PLMs for advanced mathematical operations
and logical reasoning.
• BERT-TD [30] applies contrastive learning to improve
mathematical word problem pattern recognition.
Although NALMs are not inherently designed for gener-
ative tasks, they excel in mathematical comprehension due
to their bidirectional encoding capabilities. By leveraging
masked language modeling and contextual token embed-
dings, these models can capture intricate mathematical
structures and improve equation parsing accuracy.
C. Discussion and Future Directions
The adoption of PLM-based approaches in mathemati-
cal reasoning has yielded signiﬁcant progress, particularly
in theorem proving, arithmetic calculations, and mathe-
matical problem-solving. However, several challenges re-
main:
• Numerical Precision: PLMs often struggle with exact
arithmetic computations due to ﬂoating-point repre-
sentation errors.
• Long-Range Dependencies: Complex mathematical
proofs require deep reasoning chains, which current
models struggle to retain eﬀectively.
• Hallucination in Proof Generation: The tendency
of LMs to generate incorrect but plausible-looking
proofs remains a signiﬁcant limitation.
• Hybrid Neural-Symbolic Methods: Integrating PLMs
with symbolic solvers could enhance the rigor and
correctness of mathematical reasoning.
Future research directions should focus on improving
interpretability, reducing hallucinations, and integrating
structured reasoning modules to ensure mathematical
rigor. As PLMs continue to evolve, their role in automating
theorem proving and mathematical modeling is expected
to expand, pushing the boundaries of artiﬁcial intelligence
in formal reasoning.
IV. LLM-Based Approaches for Mathematical
Reasoning
LLMs have revolutionized text processing and gener-
ation, extending their capabilities to mathematical rea-
soning [31], [68]. Since mathematics follows structured
linguistic and symbolic rules, LLMs capable of under-
standing these rules serve as powerful tools for problem-
solving in various mathematical domains [74], [76]. Exist-
ing research in LLM-based mathematical reasoning can be
broadly categorized into four key areas: instruction learn-
ing, tool-based methods, fundamental Chain-of-Thought
(CoT) techniques, and advanced CoT methodologies.
A. Instruction Learning
Instruction learning improves mathematical reasoning
in LLMs through structured task formulation, supervised
ﬁne-tuning, and in-context learning.
Instruction Building involves generating structured ex-
planations and guidance to enhance model capabilities.
Auto-explanation [99] enables LLMs to construct self-
explanatory mathematical datasets, improving problem
comprehension. RefAug [100] leverages GPT-4o to aug-
ment mathematical reasoning data, enriching training
samples. Evol-Instruct [101] introduces an iterative reﬁne-
ment mechanism where LLMs generate and reﬁne their
own instructional prompts, further enhanced by Wizard-
Math’s [74] RLEIF optimization method. EURUS [102] in-
tegrates CoT [15] with external tools, optimizing reasoning
paths using Direct Preference Optimization (DPO) [103]
and Noise Contrastive Alignment (NCA) [104].
Instruction Tuning ﬁne-tunes LLMs to align outputs
with human intent [105]. MathGLM [35] and GOAT [36]
improve arithmetic operations through structured tok-
enization, while Calculon [65] employs digit decomposition
for numerical precision. PaLM 2-L-Math [106] demon-
strates that combining solution re-ranking with majority
voting enhances performance in mathematical reasoning.
LLEMMA [76] pre-trains on mathematical corpora and in-
tegrates a Python interpreter to execute symbolic compu-
tations. More recent LLMs such as Qwen2.5-MATH [107]
and InternLM-Math [108] employ iterative ﬁne-tuning and
reward modeling to reﬁne mathematical understanding
and improve accuracy.
In-Context Learning (ICL) enhances mathematical rea-
soning by presenting task examples during inference with-
out modifying model parameters [33]. ScratchpadGPT
[71] improves multi-step calculations by generating in-
termediate reasoning steps. Codex-math [44] ﬁne-tunes
LLMs on code-based mathematical tasks for automated
problem-solving. LAMBADA [109] introduces backward
reasoning, guiding LLMs in reverse problem-solving, while


--- Page 6 ---
6
JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
LogicSolver [110] embeds algebraic logic into in-context
prompts to enhance interpretability.
B. Tool-Based Methods
LLMs can augment their mathematical reasoning abili-
ties by integrating symbolic solvers and programming tools
[111], [112].
Single-tool methods focus on integrating LMs with a
speciﬁc symbolic reasoning framework. SymbLLM [45]
enhances mathematical word problem (MWP) solving by
leveraging symbolic solvers, while LeanReasoner [113] in-
corporates Lean theorem proving to ensure logical consis-
tency. PAL [43] improves problem-solving by generating
Python programs as intermediate reasoning steps.
Multi-tool approaches extend LLM’s capabilities by
enabling interactions with multiple external tools. Tool-
former [111] equips LLMs with APIs for calculators, search
engines, and symbolic computation systems. ART [114]
expands on this by integrating automated retrieval-based
tool selection, dynamically choosing the most appropriate
tool for each problem.
C. Fundamental Chain-of-Thought (CoT) Methods
Chain-of-Thought (CoT) reasoning enhances LLM’s
performance in mathematical tasks by decomposing prob-
lems into step-by-step logical sequences [15].
Auto-CoT [41] automatically clusters representative
problems to generate eﬀective reasoning examples, re-
ducing reliance on manual prompt engineering. Rejection
Sampling Fine-Tuning (RFT) [72] reﬁnes solution paths
by discarding incorrect reasoning chains through rejection
sampling. PromptPG-CoT [40] optimizes CoT prompts us-
ing reinforcement learning (RL), improving generalization
across diverse mathematical tasks.
D. Advanced Chain-of-Thought (CoT) Methods
Verify-based methods enhance mathematical reasoning
by incorporating self-veriﬁcation mechanisms. VerifyCoT
[115] ensures deductive consistency, while SSC-CoT [116]
validates intermediate steps to prevent cascading reason-
ing errors.
Ensemble-based approaches improve accuracy by ag-
gregating multiple reasoning trajectories. Self-Consistency
[37] selects the most frequent solution among multiple
sampled responses, while MCR [117] uses majority voting
across distinct reasoning paths.
Planning-based methods organize reasoning hierar-
chically for structured problem-solving. Tree-of-Thought
(ToT) [118] structures problem-solving as a tree-like pro-
cess, exploring multiple reasoning pathways. Graph-of-
Thought (GoT) [119] extends this concept to graph-based
reasoning, capturing complex interdependencies between
logical steps.
Self-correction methods introduce iterative feedback
loops to reﬁne responses. STaR [120] and CoRe [121]
employ Monte Carlo Tree Search (MCTS) to iteratively
improve generated solutions. OmegaPRM [122] applies
automated supervision to enhance Gemini’s mathematical
reasoning capabilities.
Socratic Teaching Methods [123] reﬁne mathematical
understanding through structured questioning. Socratic
Models (SMs) [124] integrate dialogue-based learning, en-
abling LLMs to explore mathematical concepts interac-
tively.
E. Future Directions
While LLM-based mathematical reasoning has made
remarkable progress, several challenges remain:
• Robust Veriﬁcation: Ensuring LLMs generate prov-
ably correct mathematical solutions remains an open
problem [125].
• Handling Complex Proofs: Current models struggle
with multi-step logical deductions in formal theorem
proving [126].
• Hybrid Symbolic-Neural Methods: Combining sym-
bolic solvers with LLMs could enhance mathematical
accuracy and reliability [127].
Future research should focus on improving reasoning
consistency [128], integrating interactive problem-solving
frameworks [129], and enhancing mathematical compre-
hension through hybrid neural-symbolic approaches [127].
V. Comparison of Mathematical Large
Language Models
Mathematical LLMs have evolved signiﬁcantly as seen
in benchmarks like MATH [213] and geometry reason-
ing [214], leveraging various architectural designs, train-
ing methodologies, reasoning strategies, and tool integra-
tions to enhance mathematical problem-solving capabili-
ties [215].
However, open-source models lag behind proprietary
ones like GPT-4 [216] and Gemini-Ultra [217]. To bridge
this gap, Deepseek [218] introduced a domain-speciﬁc
model (spmath) trained on the DeepSeekMath Corpus, a
high-quality dataset of 120B math tokens curated from
Common Crawl using a fastText-based classiﬁer [219].
This model, initialized with DeepSeek-Coder-Base-v1.5
7B [220], outperforms Minerva 540B [221] on MATH
[213] and exhibits strong multilingual performance on
CMATH [222] and AGIEval [223]. Pre-training on code
enhances mathematical reasoning, supporting the idea
that coding skills improve logic-based problem-solving.
spmath-Instruct 7B is ﬁne-tuned using chain-of-thought
[224], program-of-thought [225], [226], and tool-integrated
reasoning [227], achieving results comparable to 70B
open-source instruction-tuned models. Additionally, the
Deepseek introduced Group Relative Policy Optimization
(GRPO), a RL method that eliminates the critic model,
signiﬁcantly reducing training costs while improving per-
formance on GSM8K (82.9% →88.2%) and MATH (46.8%
→51.7%). A uniﬁed paradigm is presented in [218] to
connect various RL-based ﬁne-tuning methods, such as


--- Page 7 ---
FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE7
TABLE I
Comparison of mathematical Language Models. Pre-T: Pre-Training, FT: Finetuning, MM: Multimodal, Symbolic-S:
Symbolic solvers, F-CoT: Foundational CoT, A-CoT: Advanced CoT, RL: Reinforce Learning
Methods
Base Model
Training
Tool
CoT
RL
MM
Access
GPT-f [77]
-
Pre-T
Symbolic-S
✗
✗
✗
✗
LISA [26]
-
Pre-T
Symbolic-S
✗
✗
✗
✗
MATH-PLM [94]
GPT-2, GPT-3
Pre-T
✗
F-CoT
✗
✗
✗
Minerva [96]
PaLM
Pre-T
✗
✗
✗
✗
✗
MWP-BERT [98]
BERT
Pre-T
✗
✗
✗
✗
✗
ControlMath [130]
LLaMA-2, Mistral
FT
✗
F-CoT
✗
✗
✗
EURUS [102]
Mistral, CodeLLaMA
FT
✗
F-CoT
DPO, KTO, NCA
✗
✓
MathGLM [35]
GLM
FT
✗
✗
✗
✗
✓
WizardLM [101]
LLaMA
FT
✗
✗
✗
✗
✓
Wizardmath [74]
LLaMA-2
FT
✗
✗
PPO
✗
✓
PaLM 2-L-Math [106]
PaLM-2-L
FT
✗
F-CoT
✗
✗
✗
Codex-math [44]
Codex
Prompt
Program
✗
✗
✗
✗
Toolformer [111]
GPT-J
FT
Calculator
✗
✗
✗
✗
LeanReasoner [113]
CodeLLaMA
FT
Symbolic-S
✗
✗
✗
✓
LINC [131]
StarCoder+
Prompt
Symbolic-S
F-CoT
✗
✗
✓
MuMath-Code [132]
Llama-2
FT
Program
✗
✗
✗
✓
MAmmoTH [133]
-
FT
Program
F-CoT
✗
✗
✗
InternLM-Math [108]
InternLM2
Pre-T, FT
Program
F-CoT
PPO
✗
✓
Qwen-Math [107]
-
Pre-T, FT
Program
F-CoT
PPO
✗
✓
PAL [43]
-
Prompt
Program
F-CoT
✗
✗
✓
CoT [15]
-
Prompt
✗
F-CoT
✗
✗
✓
PromptPG-CoT [40]
-
Prompt
✗
✗
✗
✗
✓
BoostedPrompt [134]
-
Prompt
✗
F-CoT
✗
✗
✗
ToT [118]
-
Prompt
✗
A-CoT
✗
✗
✓
PoT [39]
-
Prompt
Program
F-CoT
✗
✗
✓
Self-check [135]
-
Prompt
✗
A-CoT
✗
✗
✓
Self-Consistency [37]
-
Prompt
✗
A-CoT
✗
✗
✓
Diversity-of-Thought [136]
-
Prompt
✗
A-CoT
✗
✗
✗
RAP [137]
-
Prompt
✗
A-CoT
✗
✗
✓
LATS [138]
-
Prompt
✗
A-CoT
✗
✗
✗
LLM+P [139]
-
Prompt
Symbolic-S
A-CoT
✗
✗
✓
LLM+DP [140]
-
Prompt
Symbolic-S
A-CoT
✗
✗
✓
ISR-LLM [141]
-
Prompt
✗
A-CoT
✗
✗
✓
MCR [117]
-
Prompt
✗
A-CoT
✗
✗
✓
Rank-veriﬁer [142]
-
Prompt
✗
A-CoT
✗
✗
✗
GRACE [143]
-
Prompt
✗
A-CoT
✗
✗
✓
Reﬂexion [144]
-
Prompt
✗
A-CoT
✗
✗
✓
CR [145]
-
Prompt
✗
A-CoT
✗
✗
✓
Step-Plan [146]
-
Prompt
✗
A-CoT
✗
✗
✓
STaR [120]
GPT-J
FT
✗
A-CoT
✗
✗
✓
V-STaR [147]
LLaMA2, CodeLLaMA
FT
✗
A-CoT
DPO
✗
✗
Quiet-STaR [148]
Mistral
Pre-T
✗
A-CoT
✗
✗
✗
REFT [149]
Galactica, CodeLLaMA
FT
✗
A-CoT
PPO
✗
✓
SCoRe [150]
Gemini
FT
✗
A-CoT
SCoRe
✗
✗
HGS-PRM [151]
LLaMA2, WizardMath
FT
✗
A-CoT
PPO
✗
✗
MCTSr [152]
LLaMa-3
FT
✗
A-CoT
✗
✗
✓
CoRe [121]
GPT-J, DeBERTa-large
FT
✗
A-CoT
✗
✗
✓
Marco-o1 [153]
Qwen2
FT
✗
A-CoT
✗
✗
✓
Step-DPO [154]
Qwen2
FT
✗
A-CoT
DPO
✗
✓
Flow-DPO [155]
LLaMA3, Phi-3
FT
✗
A-CoT
DPO
✗
✗
OmegaPRM [122]
Gemini Pro, Gemma2
FT
✗
A-CoT
PRM
✗
✗
SocraticLLM [156]
Qwen2
FT
✗
A-CoT
✗
✗
✓
MATHDIAL [157]
T5
FT
✗
A-CoT
✗
✗
✓
QwQ [158]
Qwen2.5
Pre-T
✗
Long CoT
✓
✗
✓
o1 [159]
-
Pre-T
✗
Long CoT
✓
✓
✗
GPT-4o [160]
-
Pre-T
✓
A-CoT
✓
✓
✗
GPT-4V [31]
-
Pre-T
✓
A-CoT
✓
✓
✗
Qwen2-VL [161]
Qwen2-VL
Pre-T
✗
F-CoT
✗
✓
✓
LLaVA-o1 [162]
LLaVA
FT
✗
Long CoT
✗
✓
✓
Gemini [163]
Gemini
Pre-T
✓
A-CoT
✓
✓
✗
GLM-4V [164]
ChatGLM
Pre-T
✓
A-CoT
✓
✓
✗
AtomThink [165]
LLaVA, EMOVA
FT
✗
Long CoT
PRM
✓
✓
Math-LLaVA [166]
LLaVA
FT
✗
F-CoT
✗
✓
✓
M-STAR [167]
MiniCPM-V
FT
✗
A-CoT
PRM
✓
✗
UnAC [168]
GPT-4V, Gemini, LLaVA
Prompt
✗
F-CoT
✗
✓
✗


--- Page 8 ---
8
JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
TABLE II
Performance of various mathematical LMs in terms of accuracy.
Text-based Methods
MATH
GSM8K
MMLU
ASDiv
AIME
MathQA
SVAMP
FOLIO
AQUA
MATH-PLM [94]
6.9
-
-
-
-
-
-
-
-
Minerva [96]
50.3
78.5
75.0
-
-
-
-
-
-
MWP-BERT [98]
-
-
-
-
-
76.6
-
-
-
ControlMath [130]
9.5
49.3
-
-
-
-
51.6
-
-
EURUS [102]
41.7
62.8
-
93.0
-
-
90.4
-
-
Wizardmath [74]
22.7
81.6
-
-
-
-
-
-
Codex-math [44]
81.8
-
-
-
-
-
-
-
-
Toolformer [111]
-
-
-
40.4
-
-
29.4
-
-
LeanReasoner [113]
-
-
-
-
-
-
-
82.6
-
LINC [131]
-
-
-
-
-
-
-
72.5
-
MuMath-Code [132]
55.1
90.7
-
-
-
-
-
-
-
MAmmoTH [133]
44.2
76.7
-
-
-
-
-
-
61.4
InternLM-Math [108]
37.7
82.6
-
-
-
-
-
-
-
Qwen-Math [107]
66.8
91.6
82.8
-
63.3
86.3
-
-
-
PAL [43]
-
72.0
-
79.6
-
-
79.4
-
-
BoostedPrompt [134]
-
87.1
-
-
-
-
-
-
63.8
PoT [39]
-
80.0
-
-
-
89.1
-
-
58.6
Self-check [135]
51.3
88.1
-
-
81.2
-
-
-
-
Self-Consistency [37]
-
78.0
-
87.8
-
-
86.8
-
52.0
Diversity-of-Thought [136]
-
96.3
-
-
-
-
-
-
81.7
GRACE [143]
-
36.3
-
-
-
84.4
68.6
-
-
CR [145]
72.2
-
-
-
-
-
-
98.0
-
REFT [149]
-
75.3
-
-
-
71.8
79.2
-
-
SCoRe [150]
64.4
-
-
-
-
-
-
-
-
HGS-PRM [151]
13.7
65.4
-
-
-
-
-
-
-
MCTSr [152]
58.2
96.6
-
-
11.8
-
-
-
-
CoRe [121]
-
63.2
-
90.5
-
-
-
-
-
Step-DPO [154]
70.8
94.0
-
-
-
-
-
-
-
Flow-DPO [155]
38.6
71.3
-
-
-
-
-
-
-
OmegapRM [122]
69.4
93.6
-
-
-
-
-
-
-
QwQ [158]
90.6
-
-
-
50.0
-
-
-
-
GLM-4-9B-Chat [164]
50.6
79.6
72.4
-
-
-
-
-
-
Multi-modal Methods
MATH
GSM8K
MMLU
ASDiv
AIME
MMMU
MathVista
CMM-Math
Math-V
o1 [159]
94.8
-
92.3
-
83.3
78.1
73.9
-
-
GPT-4o [160]
60.3
-
88.0
-
13.4
69.2
63.8
29.02
30.4
GPT-4V [31]
-
-
-
-
-
56.8
49.9
-
22.8
Qwen2-VL [161]
-
-
46.2
-
-
64.5
70.5
43.0
25.9
LLaVA-o1 [162]
64.0
-
-
-
-
-
54.8
-
-
Gemini [163]
67.7
-
85.9
-
-
62.2
63.9
41.9
17.7
GLM-4V [164]
-
-
-
-
-
47.2
-
-
-
AtomThink [165]
-
-
-
-
-
-
53.3
-
40.5
Math-LLaVA [166]
-
-
-
-
38.3
38.3
46.6
-
15.7
M-STAR [167]
-
-
-
-
-
-
59.5
-
-
UnAC [168]
-
-
-
-
-
59.7
56.6
-
-
Rejection Sampling Fine-Tuning (RFT) [72], Direct Pref-
erence Optimization (DPO) [228], and Proximal Policy
Optimization (PPO) [229], with extensive experiments
exploring online vs. oﬄine training, outcome vs. process
supervision, and single-turn vs. iterative RL. Deepseek
also constructs the DeepSeekMath Corpus using an itera-
tive fastText-based classiﬁer, ﬁltering 35.5M math-related
web pages while preventing contamination from evaluation
datasets like GSM8K [230] and MATH [213]. Evaluations
show spmath-Base 7B surpasses Mistral 7B [231] and
Llemma 34B [232] in mathematical reasoning, excelling
in theorem proving on miniF2F [233] and performing
well in general reasoning (MMLU [234], BBH [235]) and
coding (HumanEval [236], MBPP [237]). After supervised
ﬁne-tuning with 776K mathematical instructions, spmath-
Instruct 7B outperforms all 7B models and rivals 70B
instruction-tuned models such as InternLM2-Math 20B,
WizardMath 7B [238], MetaMath 70B [239], and MAm-
moTH 70B [240].
Table I provides a comparative overview of multiple
mathematical LLMs, highlighting key aspects such as
training methods, reasoning techniques, RL applications,
multi-modal capabilities, and accessibility. This section
explores the observed trends, major advancements, and
existing limitations in the domain of mathematical LLMs.
A. Chain-of-Thought (CoT) Reasoning
One of the most critical advancements in mathemat-
ical reasoning for LLMs is the incorporation of Chain-
of-Thought (CoT) prompting [15]. CoT enables models
to break down complex problems into intermediate steps,
improving their ability to solve mathematical and logical
problems. Foundational CoT (F-CoT) is a common feature
among models such as MATH-PLM [94], ControlMath
[130], and EURUS [102], which helps them generate struc-
tured multi-step solutions. More advanced forms of CoT
reasoning (A-CoT) are implemented in models like ToT
[118], Self-check [135], and Reﬂexion [144], allowing them
to reﬁne their solutions iteratively. Some recent models,
such as QwQ [158], o1 [159], and LLaVA-o1 [162], employ
Long CoT, which extends reasoning over a longer context
for more complex mathematical problems.
B. Integration of External Tools
Tool integration is another key diﬀerentiator among
mathematical LLMs. Some models, such as Toolformer


--- Page 9 ---
FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE9
TABLE III
The statistics information of mathematical datasets. Level: E = Elementary, M = Middle School, H = High School, U = University, C =
Competition, H = Hybrid, O = Others. Modality: T = Text, M = Multimodal
4 Dataset
#Train
#Val
#Test
#Total
Language
Task
Type
Solution
Level
Modality
VERBPHYSICS [169]
733
1,096
1,828
3,657
EN
Calculation
Training
Formula
-
T
Clinical [60], [170]
11,170
1,625
3,220
16,015
EN
Calculation
Training
Formula
O
T
Scientiﬁc [60]
14,694
2,037
4,231
20,962
EN
Calculation
Training
Formula
O
T
DoQ [171]
587
5,418
6,007
12,012
EN
Calculation
Training
Formula
-
T
DROP [58]
77,409
9,536
9,622
96,567
EN
Calculation
Training
Text
E
T
AddSub [6]
-
-
-
395
EN
MWP
Training
Formula
E
T
SingleOp [10]
265
107
159
531
EN
MWP
Training
Formula
E
T
SingleEq [13]
-
-
-
508
EN
MWP
Training
Formula
E
T
MultiArith [11]
420
-
180
600
EN
MWP
Training
Formula
-
T
Alg514 [7]
-
-
-
514
EN
MWP
Training
Formula
-
T
Math23k [5]
22,162
-
1000
23,162
CH
MWP
Training
Formula
E
T
AQuA [172]
97,467
254
254
97,975
EN
MWP
Training
Text
U
T
GSM8K [142]
7,473
1,319
8,792
EN
MWP
Training
Text
E
T
SVAMP [173]
700
300
1,000
EN
MWP
Training
Formula
E
T
DRAW [174]
-
-
-
1,000
EN
MWP
Training
Formula
O
T
Dolphin1878 [12]
-
374
1,504
1,878
EN
MWP
Training
Formula
-
T
HMWP [175]
-
-
-
5,470
CH
MWP
Training
Formula
-
T
ArMATH [176]
-
-
-
6,000
Arabic
MWP
Training
Formula
E
T
TabMWP [40]
-
-
-
38,431
EN
MWP
Training
Text
E
M
T
TAL-SCQ5K 1
3,000
-
2,000
5,000
CH/EN
MWP
Training
Text
C
T
REALFP [177]
185
185
558
928
EN
MWP
Training
Formula
C
T
SYNTHFP [177]
10,000
1,000
1,000
12,000
EN
MWP
Training
Formula
O
T
MultiHiertt [28]
7,830
1,044
1,566
10,440
EN
MWP
Training
Formula
O
M
MATHPILE [178]
-
-
-
903,180
EN
MWP
Training
Text
H
T
OpenWebMath [179]
-
-
-
-
EN
MWP
Training
Formula
H
T
MathQA [180]
29,837
4,475
28,985
37,297
EN
MQA
Training
Formula
-
T
FinQA [181]
6,251
883
1,147
8,281
EN
MQA
Training
Formula
O
T
TAT-QA [27]
-
-
-
16,552
EN
MQA
Training
Text
O
M
MML [182]
-
-
-
57,882
EN
TP
Training
Formula
-
T
HolStep [183]
2,013,046
-
196,030
2,209,076
EN
TP
Training
Formula
-
T
CoqGym [82]
-
-
-
71,000
EN
TP
Training
Formula
-
T
HOList [184]
-
-
-
29,462
EN
TP
Training
Formula
-
T
IsarStep [185]
820,000
5,000
5,000
830,000
EN
TP
Training
Formula
U
T
LISA [26]
-
-
-
183,000
EN
TP
Training
Formula
-
T
NaturalProofs [79]
32,000
EN
TP
Training
Text
O
T
LeanStep [95]
-
-
-
21,606,000
EN
TP
Training
Formula
-
T
NumGLUE [186]
-
-
-
101,835
EN
Calculation Benchmark
Text
H
T
Dophin18k [187]
-
-
-
18,460
EN
MWP
Benchmark
Text
E
T
MAWPS [188]
-
-
-
3,320
EN
MWP
Benchmark Formula
H
T
ASDiv [189]
-
-
-
2,305
EN
MWP
Benchmark Formula
E
T
MATH [94]
7,500
5,000
12,500
EN
MWP
Benchmark
Text
H
O
T
MGSM [190]
-
-
-
-
Multilingual
MWP
Benchmark
Text
E
T
Mathematics [191]
2,000,000
100,000
2,100,000
EN
MWP
Benchmark Formula
-
T
MMLU-Math [93]
-
-
-
906
EN
WMP
Benchmark Formula
E
H
C
T
AGIEval [192]
-
-
-
469/220
CH/EN
MWP
Benchmark Formula
H
C
O
T
AIME2
-
-
-
933
EN
MWP
Benchmark Formula
C
T
MATHTRAP [193]
-
-
-
105/155
EN
MWP
Benchmark Formula
H
T
MathVista [194]
-
-
-
6,141
EN
MWP
Benchmark Formula E
M
H
U
M
Math-V [195]
-
-
-
3,040
EN
MWP
Benchmark Formula
E
H
O
M
CMM-Math [196]
22,248
-
5,821
28,069
CH
MWP
Benchmark
Text
E
M
H
M
MathVerse [197]
-
-
-
2,612
EN
MWP
Benchmark Formula
H
M
GeoQA [198]
3,499
745
754
4,998
EN
MWP
Benchmark Formula
M
M
M3CoT [199]
-
-
-
11,459
EN
MWP
Benchmark Formula
O
M
MR-MATH [200]
-
-
-
-
EN
MWP
Benchmark Formula
E
T
MMMU-Math [201]
-
-
-
505
EN
MWP
Benchmark
Text
U
M
We-Math [202]
-
-
-
1700
EN
MWP
Benchmark Formula
E
H
U
M
U-MATH [203]
-
-
-
1,080
EN
MWP
Benchmark Formula
U
M
INT [81]
-
-
-
-
EN
TP
Benchmark Formula
-
T
miniF2F [204]
-
244
244
488
EN
TP
Benchmark Formula
E
H
O
T
FOLIO [205]
-
-
-
1,430
EN
TP
Benchmark Formula
H
T
Aggregate [206]
-
-
-
1,492
EN
MWP
Augmented Formula
H
T
MathQA-Python [207]
19,209
2,822
2,822
23,914
EN
MWP
Augmented
Code
-
T
Math50k [208]
-
-
-
50,000
EN
WMP
Augmented
Text
-
T
PRM800K [209]
-
-
-
2,868
EN
WMP
Augmented
Text
H
O
T
MetaMathQA [73]
-
-
-
395,000
EN
MWP
Augmented
Text
H
T
Lila [210]
-
-
-
134,000
EN
MWP
Augmented
Code
H
T
PEN [211]
-
-
-
3,581
EN
MWP
Augmented Formula
H
T
miniF2F+informal [78]
-
244
244
488
EN
TP
Augmented Formula
E
H
O
T
NaturalProofs-Gen [212]
12,500
1,000
1,000
14,500
EN
TP
Augmented
Text
O
T
4


--- Page 10 ---
10
JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
[111] and LeanReasoner [113], incorporate external sym-
bolic solvers and calculators to enhance their accuracy.
Other models, such as Codex-math [44] and MuMath-
Code [132], rely on program-based reasoning, allowing
them to execute mathematical scripts and verify calcula-
tions dynamically. These tool-enhanced approaches gener-
ally outperform models that rely purely on neural network
inference without external computation.
C. Training Paradigms: Pre-Training, Fine-Tuning, and
Prompt Engineering
Mathematical LLMs adopt diﬀerent training strategies,
primarily categorized into Pre-Training (Pre-T), Fine-
Tuning (FT), and Prompt Engineering. Pre-trained mod-
els, such as Minerva [96], GPT-4o [160], and Gemini
[163], are trained on large mathematical datasets before
being adapted for speciﬁc problem-solving tasks. Fine-
tuned models, such as WizardMath [74] and InternLM-
Math [108], undergo domain-speciﬁc adaptations to im-
prove their mathematical reasoning skills. Prompt-based
models, such as PAL [43] and PoT [39], rely on structured
prompt design to guide their reasoning, making them more
ﬂexible for zero-shot and few-shot learning.
D. RL for Mathematical Reasoning
RL techniques, particularly Proximal Policy Optimiza-
tion (PPO) and Direct Preference Optimization (DPO),
have been applied to mathematical LLMs to reﬁne their
solution generation process. Models like WizardMath [74]
and InternLM-Math [108] leverage PPO to enhance their
performance through iterative self-improvement. Addi-
tionally, DPO-based models, such as EURUS [102] and
V-STaR [147], optimize their outputs based on user pref-
erences and ranking mechanisms. Recent advances, in-
cluding Step-DPO [154] and Flow-DPO [155], incorporate
structured planning into reinforcement learning to reﬁne
problem-solving pathways.
E. Multi-Modal Capabilities
While most mathematical LLMs focus on text-based
problem-solving, a growing number of models support
multi-modal reasoning, integrating text and image inputs
to solve visual mathematics problems. Notable multi-
modal models include GPT-4V [31], Gemini [163], and
GLM-4V [164], which incorporate vision-language capabil-
ities for tasks involving diagrams, plots, and mathematical
notation recognition. Similarly, Math-LLaVA [166] and
AtomThink [165] extend multi-modal reasoning by ﬁne-
tuning on specialized mathematical datasets. However,
many models still lack this capability, indicating an area
for further improvement.
F. Open vs. Closed Access Models
The accessibility of mathematical LLMs varies signiﬁ-
cantly. Open-source models such as EURUS [102], Qwen-
Math [107], and LINC [131] provide transparency and
adaptability, allowing researchers to ﬁne-tune them for
speciﬁc applications. In contrast, proprietary models such
as GPT-4o [160], Gemini [163], and o1 [159] are closed-
source, limiting external modiﬁcations. The trade-oﬀbe-
tween openness and performance remains a key consider-
ation in model selection.
Mathematical LLMs continue to advance in reasoning
capabilities, tool integration, and training methodologies.
While Chain-of-Thought reasoning has become a stan-
dard technique for improving accuracy, models diﬀer in
their use of external tools, reinforcement learning, and
multi-modal support. Additionally, open-source models
provide ﬂexibility for customization, whereas closed-source
models typically oﬀer superior performance at the cost
of accessibility. Future advancements are likely to focus
on enhancing multi-modal mathematical reasoning, better
tool integrations, and more robust self-improving learning
mechanisms to achieve greater accuracy and eﬃciency in
solving complex mathematical problems.
VI. Performance Analysis of Mathematical
Language Models
Mathematical LMs have demonstrated varying degrees
of success across diﬀerent mathematical benchmarks. To
assess the arithmetic and mathematical capabilities of
language models, various datasets and evaluation methods
have been proposed. Commonly used benchmarks include
MATH [94], GSM8K [142], and AGIEval [192], which test
models across diﬀerent diﬃculty levels and problem types.
Traditional NLP evaluation metrics, such as accuracy and
perplexity, are insuﬃcient for mathematical reasoning.
Instead, numeric predictions are evaluated using regression
metrics such as Mean Absolute Error (MAE) and Root
Mean Squared Error (RMSE) [241]. Adjusted Perplexity
[60] has also been proposed to account for numeric token
distributions.
Some benchmarks adopt specialized evaluation strate-
gies. The MATH dataset [94] enforces strict answer for-
matting for consistency, while AGIEval [192] employs
Exact Match and F1 scores for ﬁll-in-the-blank questions.
Recent research explores automated assessment methods,
including large language model-based scoring [242], to
provide more nuanced evaluations of reasoning quality.
Given the complexity of mathematical problem-solving,
developing comprehensive evaluation metrics remains an
open challenge. Future directions should focus on reﬁning
benchmarks to assess multi-step reasoning, logical consis-
tency, and theorem-proving capabilities more eﬀectively.
Table II presents the performance of various text-based
and multi-modal LMs on multiple datasets, including
MATH, GSM8K, MMLU, ASDiv, AIME, and others.
Among the text-based methods, Codex-math achieves
the highest accuracy on the MATH dataset (81.8%), while
QwQ leads with a remarkable 90.6%. For GSM8K, MCTSr
outperforms other models with an accuracy of 96.6%, fol-
lowed closely by Diversity-of-Thought (96.3%) and Step-
DPO (94.0%). In the AIME dataset, Self-check achieves
the highest performance with 81.2%. Similarly, in ASDiv,


--- Page 11 ---
FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE11
EURUS attains the best result (93.0%), demonstrating its
eﬀectiveness in arithmetic reasoning tasks.
For dataset-speciﬁc performance, PoT leads in MathQA
(89.1%), and CR achieves the highest accuracy on the
FOLIO dataset (98.0%). These results highlight the
strengths of specialized models in certain tasks. Mean-
while, InternLM-Math and Qwen-Math show strong over-
all performance across multiple datasets, particularly with
Qwen-Math achieving 91.6% in GSM8K and 82.8% in
MMLU.
Multi-modal models, which leverage both textual and
visual input, generally perform better than their text-
based counterparts. o1 outperforms all models on the
MATH dataset with an accuracy of 94.8%, and it also
leads in MMLU (92.3%) and AIME (83.3%). GPT-4o also
exhibits strong performance in MMLU (88.0%), indicat-
ing the advantage of integrating vision and language for
mathematical reasoning. Qwen2-VL and AtomThink show
competitive results on multimodal datasets, with the latter
achieving the highest Math-V score (40.5%).
Overall, the results suggest that newer models incor-
porating diverse reasoning strategies, self-consistency, and
program-of-thought approaches achieve higher accuracy.
Future advancements in multi-modal LMs are likely to fur-
ther improve mathematical reasoning capabilities by inte-
grating structured symbolic processing with deep learning
architectures.
VII. Datasets
To train and evaluate the arithmetic and mathematical
reasoning abilities of language models, various math word
problem (MWP) datasets [7], [94], [142], [191] have been
developed. We categorize these datasets into training,
benchmark, and augmented datasets (Table III).
A. Training Datasets
Mathematical Calculation datasets extract numerical
information from text. Clinical Data [170] contains pa-
tient reports with structured attributes, while Scientiﬁc
Data [60] includes research papers. DoQ [171] captures
scalar attribute values for over 350K nouns and verbs.
VERBPHYSICS [169] encodes action-object relations, and
DROP [58] tests discrete reasoning over adversarial ques-
tions.
Math Word Problems datasets include AddSub [6],
focusing on addition and subtraction, and SingleOp [10],
covering multiplication and division. GSM8K [142] com-
prises high-quality grade school problems requiring multi-
step reasoning. Math23k [5] and MATHPILE [178] provide
structured equations. To improve semantic parsing, Dol-
phin1878 [12] and SingleEq [13] enable equation mapping.
Recent datasets like HMWP [175] and MathQA [180]
expand problem diversity with multilingual and multiple-
choice questions.
Theorem Proving datasets such as Feit-Thompson [243]
and IsarStep [185] provide structured theorem proofs.
CoqGym [82] and LeanStep [95] integrate proof tactics,
while NaturalProofs [79] covers formal and informal math-
ematics.
B. Benchmark Datasets
Benchmark datasets evaluate mathematical reasoning.
MAWPS [188] and Dolphin18K [187] contain diverse
MWPs. MATH [94] provides competition-level problems
with step-by-step solutions. MGSM [190] extends GSM8K
into 10 languages. miniF2F [204] benchmarks neural theo-
rem proving across Lean, Isabelle, and HOL Light. FOLIO
[205] oﬀers ﬁrst-order logic reasoning annotations.
Recent multimodal benchmarks assess vision-language
models. MathVista [194] evaluates diagram-based prob-
lems, while MMMU-Math [201] and U-Math [203] focus
on university-level problem solving.
C. Augmented Datasets
Augmented datasets enrich existing collections. Aggre-
gate [206] extends AllArith with additional perturbed
problems. MetaMathQA [73] reformulates mathematical
questions using GPT-4. Math5K [208] generates 50,000
new problems.
To enhance reasoning transparency, MathQA-Python
[207] converts text to code, and Lila [210] provides Python-
based solutions. PEN [211] annotates algebraic expla-
nations, while PRM800K [209] labels step-wise math-
ematical reasoning correctness. MiniF2F+informal [78]
and NaturalProofs-Gen [212] introduce informal theorem
statements to theorem-proving datasets.
VIII. LLMs for Time Series Analysis:
Challenges, Methods, and Future Directions
LLMs, such as GPT-4o and LLaMA [244], have achieved
remarkable success in natural language processing. In-
spired by this, researchers have started exploring their
application in time series analysis for tasks such as fore-
casting, classiﬁcation, and anomaly detection [245], [246].
By leveraging their strong pattern recognition capabilities,
LLMs have been integrated into various domains where
time series data play a crucial role. These applications
span across ﬁnance [247], [248], healthcare [249], [250],
traﬃc forecasting [251], and mobility prediction [252].
In ﬁnance, LLMs assist in stock market forecasting by
analyzing textual and numerical data. In healthcare, LLMs
are leveraged for Electrocardiogram classiﬁcation and elec-
tronic health record analysis [249].
However, applying LLMs to time series data presents
unique challenges: (1) diverse formats and domains make
model training complex, and (2) real-world time series
data often exhibit non-stationary properties, leading to
concept drift that impedes continuous adaptation [253].
To address these challenges, recent works exploit pre-
trained LLMs to analyze time series data without train-
ing new models from scratch. This section systematically
categorizes these eﬀorts into ﬁve key strategies: (1) di-
rect querying of LLMs, (2) tokenization techniques for
numerical data, (3) prompt engineering, (4) ﬁne-tuning


--- Page 12 ---
12
JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
methods, and (5) integration of LLMs with existing time
series models. Additionally, we discuss domain-speciﬁc
applications and propose future research directions.
A. Taxonomy of LLMs in Time Series Analysis
General Pipeline. LLM-based time series analysis
can be performed through three main approaches: direct
querying, ﬁne-tuning, or integrating LLMs into models.
Fine-tuning involves tokenizing time series data, enhanc-
ing representation with prompts, and adapting LLMs for
downstream tasks.
Direct Query. Works like PromptCast [254] and LLM-
Time [255] use zero-shot or few-shot learning by querying
LLMs with structured prompts.
Tokenization. Patching techniques [256], [257] and
STL decomposition [258] help convert numerical time
series data into a format interpretable by LLMs.
Prompt
Design.
Context-enhanced
and
soft
prompts [259], [260] improve the ability of LLMs to
process time series data eﬀectively.
Fine-tuning. Techniques such as LoRA [261] help op-
timize speciﬁc layers of LLMs to adapt them to time series
tasks [256], [257].
Model Integration. Some methods use LLMs as fea-
ture enhancement tools in multimodal frameworks [249],
[262], [263].
Open challenges include:
• Improved tokenization and prompts to better repre-
sent time series patterns [264].
• Interpretability
via
prototype-based
or
gradient-
based methods [265].
• Multi-modality fusion techniques for integrating time
series with text, images, and tabular data.
• Domain generalization techniques for robust perfor-
mance across diverse datasets.
• Scaling laws to understand how model size aﬀects
time series performance.
• Time series LLM agents for decision-making applica-
tions.
• Bias and safety mechanisms to ensure reliability in
critical domains.
IX. Comparison of commonly used LLMs based
on their technical specifications
LLMs have evolved signiﬁcantly, with various models
oﬀering diﬀerent strengths and weaknesses. In this section
we compare some famous LLMs including GPT-4 Turbo,
LLaMA3, DeepSeek-R1, Mixtral, Claude3, and Gemini1.5.
These models diﬀer in terms of architecture, training data,
computational eﬃciency, response accuracy, and suitabil-
ity for speciﬁc applications. Some models prioritize speed
and cost-eﬀectiveness, while others focus on reasoning
capabilities, multilingual support, or specialized domain
knowledge.
A. GPT-4 Turbo (OpenAI)
Pros:
• Best overall reasoning and language understanding.
• Supports multi-modal input (text and images).
• Handles up to 128K tokens3 [268].
• Strong API ecosystem and enterprise adoption.
Cons:
• Closed-source, making ﬁne-tuning diﬃcult.
• Expensive compared to open-source alternatives.
• Some guardrails limit responses to certain queries.
B. LLaMA 2 & 3 (Meta)
Pros:
• Open-source, allowing ﬁne-tuning and self-hosting.
• Eﬃcient performance and competitive results.
• Strong research community support.
Cons:
• Weaker reasoning than GPT-4 Turbo.
• Shorter context windows (4K–32K tokens) [269].
• Requires high-end GPUs for full deployment.
C. DeepSeek-R1 (DeepSeek)
Pros:
• Cost-Eﬀective Development: Achieved performance
comparable to GPT-4 at a fraction of the cost.
• Eﬃcient Resource Utilization: Trained using about
2,048 NVIDIA H800 GPUs, signiﬁcantly fewer than
competitors [270].
• Open-Source Commitment: Models are freely avail-
able under the MIT license, promoting transparency
and customization.
• Rapid Adoption: Quickly became the most down-
loaded application on the social media, surpassing
competitors like ChatGPT.
Cons:
• Limited Language Support: Primarily optimized for
English and Chinese, with less proﬁciency in other
languages.
• Less Established Ecosystem: Being relatively new, it
lacks the extensive integration and community sup-
port seen with models like GPT-4.
Best for: Users and developers seeking a cost-eﬀective,
open-source alternative with strong performance in En-
glish and Chinese.
D. Mixtral (Mistral AI)
Pros:
• Mixture of Experts (MoE) architecture for eﬃciency.
• Open-source and highly optimized.
• Compact and fast inference speed.
Cons:
• Shorter context length than GPT-4 (4K-32K tokens)
[271].
• Less adoption in enterprise applications.
3100 tokens are about 60-80 English words


--- Page 13 ---
FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE13
TABLE IV
Taxonomy of LLM-based time series methods. The data type TS denotes standard time series, STS represents
spatiotemporal time series, and the prefix MM- indicates multimodal inputs. The task entry Multiple includes forecasting,
classification, imputation, and anomaly detection. Query refers to direct LLM-based querying, Token indicates time series
tokenization design, Prompt denotes the use of textual or parameterized prompts, Tune represents fine-tuning of LLM
parameters, and Fusion specifies whether LLMs are integrated into external models for downstream tasks.
Method
Data Type
Domain
Task
Query
Token
Prompt
Tune
Fusion
LLM
Code
Time-LLM [260]
MM-TS
General
Forecasting
✗
✓
✓
✓
✗
LLaMA, GPT-2
Yes[1]
OFA [256]
TS
General
Multiple
✗
✓
✗
✓
✗
GPT-2
Yes[2]
TEMPO [258]
TS
General
Forecasting
✗
✓
✓
✓
✗
GPT-2
No
TEST [259]
MM-TS
General
Forecasting, Classiﬁcation
✗
✓
✓
✗
✓
BERT, GPT-2, ChatGLM, LLaMA2
Yes[3]
LLM4TS [257]
TS
General
Forecasting
✗
✓
✗
✓
✗
GPT-2
No
PromptCast [254]
TS
General
Forecasting
✓
✗
✓
✗
✗
Bart, BERT
Yes[4]
LAMP [262]
TS
General
Event Prediction
✓
✗
✓
✗
✓
GPT-3&3.5, LLaMA-2
Yes[6]
Yu et al. [247]
MM-TS
Finance
Forecasting
✓
✗
✓
✓
✗
GPT-4, Open-LLaMA
No
METS [249]
MM-TS
Healthcare
Classiﬁcation
✓
✗
✓
✗
✓
ClinicalBERT
No
Jiang et al. [250]
MM-TS
Healthcare
Classiﬁcation
✗
✗
✗
✓
✗
NYUTron (BERT)
Yes[8]
AuxMobLCast [266]
STS
Mobility
Forecasting
✗
✗
✓
✓
✓
BERT, RoBERTa, GPT-2, XLNet
Yes[9]
ST-LLM [251]
STS
Traﬃc
Forecasting
✗
✓
✗
✓
✗
LLaMA, GPT-2
No
GATGPT [267]
STS
Traﬃc
Imputation
✗
✓
✗
✓
✗
GPT-2
No
LA-GCN [263]
MM-STS
Vision
Classiﬁcation
✗
✓
✗
✗
✓
BERT
Yes[11]
[1] https://github.com/kimmeen/time-llm
[2] https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All
[3] https://openreview.net/forum?id=Tuh4nZVb0g
[4] https://github.com/HaoUNSW/PISA
[5] https://github.com/ngruver/llmtime
[6] https://github.com/iLampard/lamp
[7] https://github.com/ZihanChen1995/ChatGPT-GNN-Sto
[8]
https://github.com/nyuolab/NYUTron
[9]
https://github.com/cruiseresearchgroup/AuxMobLCast
[10]
https://github.com/xlwang233/LLM-Mob
[11]
https://github.com/damNull/LAGCN
E. Claude 3 (Anthropic)
Pros:
• Best alignment and ethical AI practices.
• Handles very long documents (up to 200K tokens
[272]).
• More human-like responses and high creativity.
Cons:
• Slower response times compared to GPT-4 Turbo.
• Closed-source, requiring API access.
F. Gemini 1.5 Pro (Google)
Pros:
• Strong multi-modal capabilities (text, images, and
audio).
• Deep integration with Google services.
• Very long context windows (up to 2M tokens [273]).
Cons:
• Still catching up to OpenAI in benchmarks.
• Limited open-source versions.
Table V summarizes the technical speciﬁcations of com-
monly used LLMs based on context length, strength, and
weaknesses.
X. Application of LLms on optimization and
control
LLMs in solving control and optimization problems is
essential for understanding their strengths, limitations,
and potential applications. As these models continue to
advance, their capabilities in multi-step reasoning, broad
knowledge retrieval, and intuitive code generation make
them promising tools for tackling complex computational
tasks. However, their limitations—such as logical inconsis-
tencies, sensitivity to prompt variations, and susceptibility
to errors—necessitate a rigorous assessment of their relia-
bility in control and optimization scenarios.
A systematic evaluation of LLMs in these domains
allows us to determine the extent to which they can
autonomously handle problem-solving, adapt to dynami-
cally changing parameters, and reﬁne solutions iteratively.
Moreover, this investigation provides critical insights into
how LLMs can complement traditional solvers, optimizing
human-AI workﬂows and enhancing the safety, robust-
ness, and eﬃciency of solutions in real-world applica-
tions. Additionally, by testing these models in structured
optimization problems, we may uncover novel solution
strategies that lead to improved computational eﬃciency
and innovation in control systems.
To address this objective, we systematically test LLMs
across a range of control and optimization tasks, including:
(i) Mixed-Integer Linear Programming (MILP) for
Battery Energy Storage Systems (BESS): Optimizing
charge/discharge scheduling while minimizing operational
costs and adhering to system constraints.
(ii) Linear Quadratic Control (LQC) for water level
regulation in interconnected tanks: Evaluating the ability
of LLMs to design and analyze optimal control strategies
for ﬂuid management.
(iii) Climate dataset analysis: Utilizing LLMs for pat-
tern recognition, forecasting, and decision-making based
on historical climate data.
(iv) Multi-Agent Linear Programming Optimization:
Exploring how LLMs can assist in coordinating multiple
decision-making agents to achieve optimal solutions.
By rigorously testing LLMs in these diverse applica-
tions, we aim to identify their strengths and weaknesses
in solving complex mathematical optimization and control
problems. This research will inform best practices for
integrating LLMs into existing computational workﬂows,
ultimately advancing the ﬁeld of AI-driven optimization
and control.
a) Code availability statement: The Python scripts
that we use for each example leverage multiple libraries
to perform data processing, optimization, and AI-driven
analysis. They utilize NumPy [274] and Pandas [275]
for numerical and tabular data manipulation, while Mat-
plotlib [276] is used for visualization. Scipy [275] pro-
vides numerical optimization functions, particularly for
linear programming, and Scikit-learn [277] is employed for
preprocessing tasks like scaling. For geospatial analysis,
Cartopy [278] is used to map wind data. The asyncio [279]


--- Page 14 ---
14
JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
TABLE V
Comparison of Commonly used LLMs based on their technical specifications
Model
Open-Source
Context Length
Strengths
Weaknesses
GPT-4 Turbo
✗
128K
Best reasoning, multi-modal
Expensive, closed-source
LLaMA 2/3
✓
4K–32K
Open-source, eﬃcient
Weaker reasoning
DeepSeek-R1
✓
12K
Good multilingual support
Less adoption
Mixtral
✓
4K–32K
Eﬃcient, compact
Shorter context
Claude 3
✗
200K
Ethical AI, document processing
Slower responses
Gemini 1.5 Pro
✗
2M
Multi-modal, long context
Not fully open-source
library enables asynchronous execution, ensuring smooth
parallel processing, especially in federated optimization
scenarios. The script also incorporates OpenAI’s API to
generate insights from wind and battery storage data,
assisting in decision-making. Furthermore, PuLP [280]
is used for mixed-integer linear programming (MILP)
to optimize battery charge/discharge schedules dynami-
cally. Finally, AutoGen [281] is used to create multi-agent
frameworks for executing linear programming problems
asynchronously. In Appendix A we provide the link to
the repository containing the codes that are used for the
simulation.
A. An example of the LLm application on the BESS
This section details an optimization problem for the
operation of a BESS with LLMs. Our objective is to min-
imize the net operational cost (or maximize net revenue)
by scheduling the charging and discharging of a battery
over a discrete 10-hour planning horizon. We incorpo-
rate fuzzy logic rules to capture realistic decision-making
heuristics concerning electricity prices, battery state of
charge (SOC), and operational switching considerations.
1) System Overview: We consider a BESS with the
following characteristics:
• Battery capacity: SOCmax = 100 kWh
• Initial SOC: SOC0 = 50 kWh
• Charging eﬃciency: η = 0.90
• Discharging eﬃciency: 1
η = 0.90
• Max charge/discharge power: P max
charge = P max
discharge =
20 kW
We operate over time steps t = 1, . . . , 10, each representing
a one-hour interval.
2) Decision Variables:
• Pcharge(t) [kW]: Charging power during hour t.
• Pdischarge(t) [kW]: Discharging power during hour t.
• SOC(t) [kWh]: State of charge of the battery at time
t.
3) Parameters:
Electricity Prices and Revenues: Let
C =

20, 25, 15, 10, 30, 35, 40, 20, 15, 10

($/kWh),
R =

30, 35, 25, 20, 40, 45, 50, 30, 25, 20

($/kWh).
Here, Ct is the cost of purchasing electricity in hour t, and
Rt is the revenue rate from selling electricity in hour t.
B. Constraints
a) Battery Capacity Bound:
0 ≤SOC(t) ≤SOCmax,
∀t.
b) Power Limits:
0 ≤Pcharge(t) ≤P max
charge = 20,
∀t,
0 ≤Pdischarge(t) ≤P max
discharge = 20,
∀t.
c) Energy Balance:
SOC(t + 1) = SOC(t) + η · Pcharge(t)
−1
η · Pdischarge(t),
t = 1, . . . , 9.
(1)
We take SOC(1) = SOC0 = 50 kWh as the initial
condition.
d) No Simultaneous Charge/Discharge:
Pcharge(t) · Pdischarge(t) = 0,
∀t.
This ensures the battery cannot charge and discharge in
the same hour.
C. Objective Function
We wish to minimize the total net cost Z over the 10-
hour horizon:
min Z =
10
X
t=1

Ct · Pcharge(t) −Rt · Pdischarge(t)

.
The term Ct Pcharge(t) represents the cost of charging,
while −Rt Pdischarge(t) represents revenue gained from
discharging.
1) Fuzzy Logic Integration: We enrich the classical op-
timization with fuzzy logic rules to guide or penalize
decisions that deviate from human-like strategies. The
fuzzy rules help shape how aggressively the system charges
or discharges based on electricity prices, battery SOC, and
the desire to avoid frequent switching.
1) If the electricity price Ct is high, prioritize discharg-
ing.
2) If the battery SOC is below 30% of SOCmax, limit
discharging and prioritize charging.
3) If Ct is low, delay or reduce charging unless SOC is
critically low.
4) Avoid frequent switching between charging and dis-
charging.


--- Page 15 ---
FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE15
In Table B (Appendix) we have shown an example of
how an hour-by-hour schedule might look based on the
output of the LLM. Actual solutions may vary depending
on speciﬁc fuzzy membership functions, penalty weights,
and solver nuances. In addition, the LLM model is also
plays an important role for the response to the prompt. In
this example we used GPT4O LLM model.
a) Cost Computation Example: The total cost Z is
computed by
Z =
10
X
t=1

Ct · Pcharge(t) −Rt · Pdischarge(t)

.
If, for instance, at hour 2 we have Pcharge(2) = 0 and
Pdischarge(2) = 10, then the hour-2 cost contribution is
Cost2 = 25 × 0 −35 × 10 = −350 $,
indicating a net proﬁt of $350 at hour 2 (negative cost).
D. LLM-Driven Fuzzy Logic Control for Water Level Reg-
ulation in Interconnected Tanks
Water level regulation in interconnected tank systems
is a fundamental problem in control engineering, fre-
quently addressed using classical and modern control
techniques such as Proportional-Integral-Derivative (PID)
controllers, Model Predictive Control (MPC), or fuzzy
logic controllers.
In this section, we integrate a LLM as a decision-
making component in a fuzzy logic-based control system
to optimize the regulation of water levels while minimizing
a quadratic cost function. This novel approach leverages
natural language processing (NLP)-based reasoning within
an LLM to generate real-time control decisions based on
predeﬁned fuzzy logic rules.
1) System Model: The dynamics of the interconnected
tanks are represented using a state-space formulation,
where the water levels in the two tanks form the system
state:
x(t) =

x1(t)
x2(t)

,
(2)
The system evolves according to the linear state-space
model:
˙x(t) = Ax(t) + Bu(t),
(3)
where:
A =
−0.1
0.05
0.05
−0.1

,
B =
0.1
0

,
(4)
The control input u(t) represents the pump ﬂow rate,
which is constrained within the range [−umax, umax], with
umax = 0.5. The cost function to be minimized is a
quadratic penalty function:
J = Q1(x1 −x1,desired)2 + Q2(x2 −x2,desired)2 + Ru2, (5)
where:
• Q1 = 10, Q2 = 8 are state penalty weights,
• R = 2 penalizes excessive control eﬀort,
• x1,desired = 2.0, x2,desired = 1.5 are the target levels.
2) Fuzzy Logic-Based Control Using LLM: In this sce-
nario the LLM is integrated as an inference engine that
follows predeﬁned fuzzy logic rules to determine the con-
trol action u(t) at each time step. The control strategy
involves:
1) Assessing Deviation: The LLM compares the actual
water levels x1, x2 with their desired levels.
2) Applying Fuzzy Rules:
• If x1 > x1,desired, decrease u.
• If x2 < x2,desired, increase u.
• Balance the adjustments to avoid oscillations.
3) Minimizing the Cost Function: The LLM aims to
select u(t) that optimally reduces deviations while
considering energy eﬃciency.
At each time step t, a prompt (see Algorithm X-D2) is
dynamically generated and sent to the LLM (see appendix
C). In Algorithm X-D2 we present a general procedure
that is used for tank level control.
The LLM generates a response, which is parsed to
extract the control input. If parsing fails, a fallback u = 0
is used.
Algorithm 1 LLM-Based Fuzzy Control for Intercon-
nected Tanks
1: Input: Initial state x0 = [x1, x2]⊤, desired state xd =
[x1,desired, x2,desired]⊤
2: Parameters:
3:
System matrices A, B
4:
Quadratic cost weights Q1, Q2, R
5:
Maximum pump ﬂow rate umax
6:
Time step ∆t, total time T
7: Initialize: Set t = 0, state x = x0, control input u = 0
8: while t < T do
9:
Construct prompt with current state and system
parameters
10:
Send prompt to LLM to obtain fuzzy logic-based
control input u(t)
11:
Parse the LLM response to extract u(t)
12:
Ensure control input is within bounds:
u(t) ←clip(u(t), −umax, umax)
13:
Compute state derivative:
˙x = Ax + Bu
14:
Update system state:
x ←x + ˙x · ∆t
15:
(t, x1, x2, u) for analysis
16:
Increment time: t ←t + ∆t
17: end while
18: Output: Time trajectory of states and control inputs
The model is simulated over a 10-second horizon, with
the LLM making decisions at each step. The summary of
the simulation results produced by LLM is shown in Table
C.


--- Page 16 ---
16
JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
E. Application of Large Language Models in Time Series
dataset analysis
In this section we consider the application of LLMs for
Wind and Pressure Data Analysis for renewable energies.
We describe the implementation of a climate data analysis
framework integrating machine learning techniques for
wind speed and pressure assessment in Germany [282]. The
system processes CMIP6 climate datasets, extracts and
interpolates meteorological variables, computes statistical
features, and formulates queries for a LLM to analyze pat-
terns and trends. Here we detail the algorithmic structure
and highlights how LLMs assist in interpreting climate
data for renewable energy applications. Wind energy fore-
casting and optimal wind turbine placement require an
extensive analysis of historical meteorological data. Tradi-
tional statistical methods provide limited insights due to
their inability to capture complex interactions among dif-
ferent environmental factors. The integration of advanced
machine learning techniques, speciﬁcally LLMs, oﬀers a
promising approach to identifying patterns, anomalies,
and correlations in meteorological datasets. The objective
here is to: (i) Extract and preprocess wind speed and pres-
sure data from climate datasets. (ii) Compute and analyze
statistical features across multiple geographic locations.
(iii) Utilize LLMs to generate domain-speciﬁc insights for
optimal wind turbine placement and energy forecasting.
(iv) Assess the eﬀectiveness of LLMs in climate data
interpretation and decision-making.
Algorithm 2 outlines the step-by-step procedure for
climate data processing and LLM-assisted analysis.
In this scenario the LLM serves as an intelligent assis-
tant, performing the following tasks: (i) Pattern Recog-
nition: Identiﬁes anomalies, trends, and correlations in
the data. (ii) Data Interpretation: Converts numerical
statistics into actionable insights. (iii) Domain-Speciﬁc
Suggestions: Oﬀers recommendations for wind farm place-
ment and data augmentation strategies. (iv) Exploratory
Analysis: Proposes hypotheses for further study based on
statistical distributions.
The advantages are: (i) Automates statistical analysis
and interpretation. (ii) Reduces manual eﬀort in data
exploration. (iii) Provides contextual recommendations for
renewable energy planning. Moreover, challenges are: (i)
LLM-generated insights must be validated against domain
expertise. (ii) Computational overhead due to large-scale
data processing. (iii) Dependency on high-quality input
prompts for eﬀective responses.
F. Application of Multi-Agent LLms to solve Linear Pro-
gramming
Optimization problems, particularly Linear Program-
ming (LP), play a crucial role in logistics, resource allo-
cation, and decision-making. Traditional LP solvers are
eﬀective but operate in a static manner without real-
time constraint validation or iterative execution based
on external feedback. This section presents an AutoGen
Multi-Agent LP Optimization framework, which utilizes
Algorithm 2 Climate dataset Analysis using LLMs
Input: Time series dataset, e.g. CMIP6 climate in-
cluding wind and pressure datasets, real wind power
production
Output: LLM-generated insights for energy forecasting
Step 1: Data Acquisition
Extract pressure data and wind speed from CMIP6
dataset.
Step 2: Data Preprocessing and Interpolation
Interpolate wind speed
Interpolate pressure
Normalize data using
Step 3: Statistical Feature Extraction
Compute location-wise statistics using such as mean,
variance, skewness, kurtosis, etc.
Step 4: LLM Integration for Data Interpretation
Formulate prompt incorporating computed statistics.
Send prompt to LLM model.
Retrieve and process LLM-generated insights.
Step 5: Interpretation and Decision Support
Analyze LLM response for wind energy site selection.
Identify temporal patterns for optimal energy capture.
Suggest additional data transformations for improved
accuracy.
Output: LLM response based on the Prompt
LLMs to autonomously verify constraints and execute
optimization procedures.
AutoGen is an open-source Python framework designed
to streamline the development of applications utilizing
LLMs through the collaboration of multiple agents. These
agents can communicate with each other, incorporate
human inputs, and employ external tools to collectively
tackle complex tasks [283]. Developed by Microsoft Re-
search, AutoGen oﬀers customizable agents capable of
conversing, executing code, and integrating various func-
tionalities, thereby simplifying the orchestration and au-
tomation of sophisticated LLM workﬂows. Its ﬂexible
architecture supports diverse applications, including code
generation, debugging, and problem-solving across various
domains [281].
The system consists of an EvaluatorAgent for constraint
validation and an LPExecutorAgent for executing the
solver iteratively. The proposed approach introduces an
intelligent layer of decision-making, improving the relia-
bility and adaptability of LP-based optimization.
G. Application to a Single Linear Programming Problem
1) Problem Statement: The optimization task is a trans-
portation cost minimization problem. Given two ware-
houses supplying goods to three stores, the objective is


--- Page 17 ---
FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE17
to determine the optimal transportation allocation that
minimizes costs while satisfying supply and demand con-
straints.
2) Mathematical Formulation: The LP model is deﬁned
as:
Minimize Z = 4x1 + 6x2 + 9x3 + 5x4 + 3x5 + 8x6,
(6)
Subject to:
x1 + x2 + x3 ≤50
(Supply limit at Warehouse 1), (7)
x4 + x5 + x6 ≤60
(Supply limit at Warehouse 2), (8)
x1 + x4 ≥30
(Demand at Store 1), (9)
x2 + x5 ≥40
(Demand at Store 2),
(10)
x3 + x6 ≥20
(Demand at Store 3),
(11)
xi ≥0
∀i ∈{1, 2, 3, 4, 5, 6}.
(12)
H. Application to a Multi-Stage LP Optimization Problem
1) Problem Statement: In a more complex scenario, the
optimization problem involves three separate but inter-
connected LP problems where solutions from the ﬁrst
stage inﬂuence constraints in the subsequent stages. This
is relevant for dynamic supply chain optimization, multi-
time-period scheduling, and adaptive cost minimization.
I. Mathematical Formulation
Each LP problem is formulated similarly to the single
problem but with additional constraints:
Minimize Z1 = 3x1 + 7x2 + 2x3 + 6x4 + 4x5 + 5x6, (13)
Subject to:
x1 + x2 + x3 ≤40,
(14)
x4 + x5 + x6 ≤70,
(15)
x1 + x4 ≥20,
(16)
x2 + x5 ≥30,
(17)
x3 + x6 ≥25,
(18)
xi ≥0,
(19)
For the second and third stage problems:
Minimize Z2 = 5x1 + 4x2 + 6x3 + 3x4 + 7x5 + 8x6, (20)
Subject to:
x1 + x2 + x3 ≤60,
(21)
x4 + x5 + x6 ≤80,
(22)
x1 + x4 ≥35,
(23)
x2 + x5 ≥25,
(24)
x3 + x6 ≥30,
(25)
xi ≥0,
(26)
Minimize Z3 = 6x1 + 8x2 + 7x3 + 5x4 + 9x5 + 4x6, (27)
Subject to:
x1 + x2 + x3 ≤50,
(28)
x4 + x5 + x6 ≤75,
(29)
x1 + x4 ≥28,
(30)
x2 + x5 ≥38,
(31)
x3 + x6 ≥22,
(32)
xi ≥0,
(33)
1) Role of LLMs: To integrate intelligent decision-
making in LP optimization, we employ LLM-based agents
using the AutoGen framework [281]. The system consists
of two key agents: (i) EvaluatorAgent, (ii) LPExecutorA-
gent.
J. EvaluatorAgent: Constraint Validation
Before executing the LP solver, an agent veriﬁes whether
constraints are correctly deﬁned. This prevents unneces-
sary computation and ensures the solver is executed under
valid conditions.
1) System Message for EvaluatorAgent:
$system_message$: "You verify if the
LP constraints are satisfied
before execution."
2) Prompt used by EvaluatorAgent:
"validate constraints."
The agent responds with:
• ✓Constraints are valid. Proceed with LP
execution.
• ✗Invalid constraints detected. Execution
halted.
3) LPExecutorAgent: Optimization Execution: Upon re-
ceiving conﬁrmation that constraints are valid, the execu-
tion agent runs the LP solver three times for consistency.
a) System Message for LPExecutorAgent:
$system_message$: "You execute the LP solver
only if constraints are valid."
b) Prompt used by LPExecutorAgent:
"Proceed with LP execution."
The agent iteratively solves the LP and returns solutions
for analysis.
4) Implementation and Execution: The process follows
these steps:
1) The EvaluatorAgent checks the validity of con-
straints.
2) If constraints are valid, the LPExecutorAgent exe-
cutes the LP solver three times.
3) The results are compiled and analyzed for consis-
tency.


--- Page 18 ---
18
JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
5) Results and Discussion: By leveraging LLMs for
constraint validation and execution, the system prevents
errors due to incorrect LP formulations and ensures a
structured execution process. The use of multiple runs
improves solution robustness and reliability. Future exten-
sions of this framework may include:
• Adaptive learning mechanisms to reﬁne constraints
dynamically.
• Multi-objective optimization for real-world applica-
tions.
• Integration with reinforcement learning for decision-
making in dynamic environments.
XI. Challenges and Further Directions
Despite the remarkable advancements in mathematical
language models, several critical challenges remain unre-
solved. These challenges include faithfulness, multi-modal
integration, uncertainty, evaluation, creative mathemati-
cal reasoning, application in education, and data scarcity.
a) Faithfulness: Mathematical LLMs suﬀer from hal-
lucinations, generating incorrect or misleading results that
undermine reliability [84], [284]. Several approaches at-
tempt to mitigate this issue by incorporating additional
knowledge [285], reinforcement learning from human feed-
back [209], tool usage [111], [112], and veriﬁcation mech-
anisms [115], [285]–[289]. However, these improvements
remain limited, necessitating further research to enhance
trustworthiness and practical applicability.
b) Multi-Modal Integration: Mathematical problems
often require multi-modal reasoning, involving not just
text but also diagrams, graphs, and equations [290],
[291]. While existing LLMs perform well in text-based
tasks, they struggle with processing and integrating multi-
modal information [28], [291]. Compared to general vision-
language tasks like VQA [292] and image captioning [293],
mathematical multi-modal reasoning is signiﬁcantly more
complex. Advancements in this area are essential for
improving problem-solving capabilities in geometry and
symbolic reasoning.
c) Uncertainty: LLMs inherently produce ambigu-
ous outputs, particularly in probabilistic reasoning and
problems with incomplete information [294], [295]. While
deterministic calculations are well-handled, tasks requir-
ing fuzzy logic, approximate solutions, or uncertain con-
straints remain challenging. Some studies address this
through controlled generation [296], but a more robust
approach is needed to ensure mathematical models accu-
rately navigate uncertainty.
d) Evaluation: Existing evaluation methods struggle
to comprehensively assess mathematical reasoning capa-
bilities. Traditional NLP metrics fail to capture the logi-
cal depth required for complex problem-solving. Several
benchmarks have been proposed [7], [94], [142], [191],
alongside evaluation metrics [66], [297]–[299]. However,
ensuring a fair assessment across arithmetic, theorem
proving, and applied mathematics remains an ongoing
challenge.
e) Creative Mathematical Reasoning: While LLMs ex-
cel at solving existing problems, their ability to formulate
and prove new mathematical theorems remains limited.
Mathematical creativity requires deep conceptual under-
standing beyond memorized patterns. Some studies have
explored machine-learning-driven theorem discovery [300],
and FunSearch [301] demonstrated the potential for LLMs
to contribute to open problems. However, bridging the gap
between automated reasoning and genuine mathematical
discovery remains a major research frontier.
f) Application in Education: Mathematical LLMs
have signiﬁcant potential as tutors and educational tools.
However, eﬀective deployment in classrooms requires ad-
dressing pedagogical challenges such as adaptive expla-
nations and interactive engagement. Socratic questioning
methods [124], [302] have been proposed to improve stu-
dent understanding. Future research should focus on cus-
tomizing LLMs to diﬀerent learning styles and integrating
them into structured educational frameworks.
g) Data Scarcity: Training data plays a crucial role
in the performance of mathematical LLMs [303]. While
instruction-building methods have explored dataset con-
struction using LLMs, current eﬀorts rely heavily on
limited datasets such as GSM8K [142] and MATH [94].
Expanding high-quality datasets and constructing multi-
modal mathematical training data will be key to further
improvements.
XII. Conclusion
This survey has explored the landscape of mathemat-
ical reasoning and optimization using Large Language
Models (LLMs), highlighting their rapid advancements,
core methodologies, and diverse applications. LLMs have
demonstrated signiﬁcant progress in solving mathematical
word problems, theorem proving, and symbolic compu-
tation, primarily due to techniques such as Chain-of-
Thought (CoT) reasoning, instruction tuning, reinforce-
ment learning, and tool-augmented inference. While au-
toregressive and non-autoregressive language models have
improved mathematical comprehension, the integration
of structured reasoning techniques and hybrid neural-
symbolic approaches remains a crucial research direction.
Beyond theoretical mathematical reasoning, we exam-
ined the role of LLMs in optimization and control, in-
cluding their applications in mixed-integer programming
(MIP), linear quadratic control (LQC), climate dataset
analysis, and multi-agent optimization frameworks. The
case studies on Battery Energy Storage Systems (BESS)
optimization and interconnected tank control illustrate
how LLMs can assist in formulating optimization prob-
lems, generating constraints, and enhancing decision-
making through fuzzy logic and reinforcement learning.
Similarly, multi-agent systems, such as AutoGen-based LP
solvers, provide a promising avenue for intelligent and
adaptive optimization workﬂows.
Despite these advancements, several challenges persist,
including faithfulness in reasoning, multi-modal integra-
tion, handling uncertainty, evaluation consistency, and


--- Page 19 ---
FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE19
fostering creative mathematical discovery. Additionally,
the gap between automated theorem proving and genuine
mathematical insight remains a major research frontier.
While LLMs have shown potential in assisting with opti-
mization tasks, their reliability and interpretability must
be further reﬁned to ensure robustness in real-world ap-
plications.
Future research should focus on improving the inter-
pretability of LLMs, enhancing their ability to perform
rigorous mathematical proofs, and developing seamless in-
tegrations with existing solvers and symbolic computation
frameworks. Moreover, expanding high-quality datasets,
reﬁning training paradigms, and adopting hybrid neuro-
symbolic architectures will be essential in pushing the
boundaries of AI-driven mathematical reasoning and op-
timization. As LLMs continue to evolve, their synergy
with formal mathematical methods, scientiﬁc computing,
and engineering applications will play a pivotal role in
advancing AI’s capabilities in structured problem-solving.


--- Page 20 ---
20
JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
Acknowledgment
To be written in the future.


--- Page 21 ---
FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE21
Appendix
A. Code Availability Statement
The
simulation
codes
are
publicly
available
in
the
following
links:
[Github
or
https://github.com/Ali-Forootani/A-Survey-on-Mathematical-Reasoning-and-Optimization-with-Large-Language-Models]
- [Gitlab or https://git.ufz.de/forootan/a-survey-on-mathematical-reasoning-and-optimization-with-large-language-models]
- [Zenodo or https://zenodo.org/records/15037375].
B. Prompt for BESS to GPT4O
prompt = """
We want to optimize the operation of a Battery Energy Storage System (BESS). The
goal is to minimize the operational cost by determining the charge/discharge
schedule for the battery over a 10-hour period.
### Problem Definition:
The problem involves scheduling the charging and discharging of a battery to
minimize the operational cost while adhering to constraints.
1. **Battery Parameters**:
- Maximum capacity (SOC_max): 100 kWh
- Initial state of charge (SOC_0): 50 kWh
- Charging efficiency (η): 90%
- Discharging efficiency (1/η): 90%
- Maximum charge/discharge power: 20 kW
2. **Electricity Prices and Revenue**:
- Electricity price (C_t, $/kWh): [20, 25, 15, 10, 30, 35, 40, 20, 15, 10]
- Revenue from discharging (R_t, $/kWh): [30, 35, 25, 20, 40, 45, 50, 30, 25, 20]
3. **Constraints**:
- Battery capacity: \(0 \leq SOC(t) \leq SOC_{max}\)
- Power limits: \(0 \leq P_{charge}(t) \leq P_{charge}^{max}\), \(0 \leq
P_{discharge}(t) \leq P_{discharge}^{max}\)
- Energy balance: \(SOC(t+1) = SOC(t) + η \cdot P_{charge}(t) - \frac{1}{η} \cdot
P_{discharge}(t)\)
- No simultaneous charging/discharging: \(P_{charge}(t) \cdot P_{discharge}(t) =
0\)
4. **Objective Function**:
- Minimize \(Z = \sum_{t=1}^{10} (C_t \cdot P_{charge}(t) - R_t \cdot
P_{discharge}(t))\)
### Fuzzy Logic Rules:
1. If the electricity price \(C_t\) is high, prioritize discharging the battery.
2. If the battery SOC is below 30%, limit discharging and prioritize charging.
3. If \(C_t\) is low, delay charging unless the SOC is critically low.
4. Avoid frequent switching between charge and discharge states.
### Requirements:
1. Use fuzzy logic rules to guide decisions dynamically during optimization.
2. Reformulate constraints or priorities based on the fuzzy logic rules.
3. Solve the optimization problem and provide:
- A step-by-step explanation of how fuzzy logic influenced the solution.
- The final optimal charge/discharge schedule (\(P_{charge}(t)\),
\(P_{discharge}(t)\)) and state of charge (\(SOC(t)\)) for each hour.
- The total minimized operational cost.
### Expected Output:
1. An explanation of how fuzzy logic rules were applied in each time step.
2. The optimal charge/discharge schedule over 10 hours.
3. The total minimized operational cost of the system.
"""


--- Page 22 ---
22
JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
TABLE VI
Illustrative Fuzzy-Guided Schedule with GPT4O
Hour
Ct
Rt
Fuzzy Action
Pcharge(t)
Pdischarge(t)
SOC(t)
Comment
1
20
30
Slight discharge
0
5
45
Moderate net revenue.
2
25
35
Discharge
0
10
35
Price is higher, good to discharge.
3
15
25
Idle/Wait
0
0
35
Price is moderate, fuzzy logic suggests wait.
4
10
20
Charge
15
0
48.5
Low cost, beneﬁcial to charge.
5
30
40
Discharge
0
10
39.5
Price is higher again, discharge.
6
35
45
Discharge
0
15
26.0
Very high price, discharge more.
7
40
50
Discharge
0
15
12.5
Highest price, maximize discharge.
8
20
30
Charge
10
0
21.5
Price moderate, partial recharge.
9
15
25
Charge
15
0
35.0
Low cost, continue charging.
10
10
20
Charge
10
0
44.0
Low cost, end at comfortable SOC.
C. Prompt for Tank Level Control
prompt = f"""
We aim to control the water levels in two interconnected tanks to achieve desired levels
while minimizing a quadratic cost function. The system is represented as a second-order
dynamical system using state-space representation.
The current time step is {t}. The system state is:
- Tank 1 water level: x1(t) = {x[0]:.2f}
- Tank 2 water level: x2(t) = {x[1]:.2f}
The desired water levels are:
- Tank 1: x1_desired = {x_desired[0]:.2f}
- Tank 2: x2_desired = {x_desired[1]:.2f}
System parameters:
- State transition matrix A:
{A}
- Control input matrix B:
{B}
- Quadratic cost function weights:
Q1 = {Q[0]}, Q2 = {Q[1]}, R = {R}
- Maximum pump flow rate: u_max = {u_max}
Based on the improved fuzzy logic rules:
1. If x1 > x1_desired, decrease the pump flow rate u(t) proportionally to the error (x1 -
x1_desired).
2. If x2 < x2_desired, increase the pump flow rate u(t) proportionally to the error
(x2_desired - x2).
3. If x1 and x2 are close to the desired levels, reduce the rate of change in u(t) to avoid
oscillations.
4. Balance the control action between x1 and x2 deviations to prioritize the larger deviation.
5. Minimize the quadratic cost function:
J = Q1 * (x1 - x1_desired)^2 + Q2 * (x2 - x2_desired)^2 + R * u(t)^2.
6. Ensure the control input u(t) stays within the range [-u_max, u_max].
Provide the control input u(t) for this time step. Ensure u(t) is within [-u_max, u_max].
"""


--- Page 23 ---
FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE23
TABLE VII
Simulation Results for LLM-Driven Fuzzy Control
Time (s)
x1 (Tank 1 Level)
x2 (Tank 2 Level)
u (Control Input)
0.00
0.93
0.50
0.00
1.00
0.86
0.50
0.00
2.00
0.80
0.49
0.00
3.00
0.79
0.48
0.50
4.00
0.74
0.47
0.00
5.00
0.69
0.46
0.00
6.00
0.64
0.45
0.00
7.00
0.60
0.44
0.00
8.00
0.56
0.42
0.00
9.00
0.53
0.41
0.00
D. Prompt for time series dataset and the LLM Response
prompt = f"""
Below are location-wise **extended** statistical features for wind speeds and pressure data
collected over a 1-year period (2928 time steps at 3-hour intervals, across 232 locations).
The following metrics are provided for each location:
• Mean • Median • Standard Deviation
• Variance • Min, Max • Range
• 25th Percentile • 75th Percentile • Interquartile Range (IQR)
• Skewness • Kurtosis • Coefficient of Variation
Overall (all locations, all time steps) aggregates include:
• Overall Mean Wind Speed: {overall_mean_ws:.3f}
• Overall Mean Pressure: {overall_mean_p:.3f}
Sample of per-location extended stats (first {num_locations_to_show} locations):
{stats_string}
Using these extended statistics, please address the following:
1. **Distribution & Patterns**
- Comment on any notable patterns in mean, standard deviation, range, or skewness for wind
speed and pressure.
- Identify whether certain locations exhibit heavy tails or extreme values (based on kurtosis)
and any implications for site selection or forecasting.
2. **Optimal Placement & Time Periods**
- Explain how the observed patterns could inform optimal turbine placement.
- Highlight which time periods or seasons might yield the highest energy output.
3. **Data Quality & Transformations**
- Suggest if any data transformations (e.g., log-transform, outlier removal, normalization)
might be warranted based on skewness/kurtosis.
- Propose additional data (e.g., temperature, humidity, topographic) that could refine
site-selection or energy forecasting models.
4. **Further Analysis**
- Discuss whether metrics like coefficient of variation or range reveal anomalies/outliers
needing investigation.
- If relevant, suggest how these statistics could guide more advanced modeling approaches.
"""
Location 231:
Wind Speed Stats:
Mean = 4.006
Median = 3.684
StdDev = 1.998
Variance= 3.993
Range = 15.428
25% = 2.510
75% = 5.173
IQR = 2.663


--- Page 24 ---
24
JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
Skewness= 1.004
Kurtosis= 1.643
CoefVar = 0.499
Pressure Stats:
Mean = 98710.059
Median = 98693.855
StdDev = 849.318
Variance= 721340.650
Range = 5267.575
25% = 98093.079
75% = 99315.328
IQR = 1222.249
Skewness= 0.218
Kurtosis= -0.021
CoefVar = 0.009
LLM Response:
1. **Distribution & Patterns**
* The mean wind speed of 3.526 m/s is relatively low, which indicates that the wind resource at
these locations is not very strong.
* The standard deviation values range from 1.412 to 2.059 m/s, with an overall standard deviation
of 1.690 m/s. This indicates that the wind speeds vary significantly across different
locations and time periods.
* The range of wind speeds varies from 2.507 m/s to 10.657 m/s, with an overall range of 8.162
m/s. This suggests that some locations experience stronger winds than others.
* The skewness values range from -0.219 to 0.938, with an overall skewness of 0.334. This
indicates that the wind speed data is slightly skewed towards positive values.
* The kurtosis values range from -0.110 to 0.837, with an overall kurtosis of 0.274. This
suggests that the wind speed data is slightly platykurtic, meaning that the distribution is
relatively flat and has a broader tail than a normal distribution.
* The mean pressure values range from 98838.522 hPa to 100667.027 hPa, with an overall mean of
99531.548 hPa. This indicates that the pressure values are relatively stable across different
locations and time periods.
* The standard deviation values range from 723.103 hPa to 1079.370 hPa, with an overall standard
deviation of 886.921 hPa. This suggests that the pressure values vary significantly across
different locations and time periods.
* The range of pressure values varies from 5102.595 hPa to 6082.317 hPa, with an overall range of
5299.912 hPa. This indicates that some locations experience larger pressure fluctuations than
others.
* The skewness values range from -0.118 to 0.207, with an overall skewness of 0.009. This
suggests that the pressure data is nearly symmetric.
* The kurtosis values range from -0.118 to 0.443, with an overall kurtosis of 0.108. This
indicates that the pressure data is slightly platykurtic, meaning that the distribution is
relatively flat and has a broader tail than a normal distribution.
Based on these observations, it appears that the wind speed data exhibits significant variability
across different locations and time periods, with some locations experiencing stronger winds
than others. The pressure data is relatively stable, but some locations experience larger
pressure fluctuations than others. The kurtosis values suggest that the wind speed and
pressure data are slightly platykurtic, which may indicate that the data is influenced by
extreme values or outliers.
2. **Optimal Placement & Time Periods**
* The observed patterns in wind speed and pressure can inform optimal turbine placement by
identifying locations with stronger wind speeds and more stable pressure conditions.
* The data suggests that certain seasons or time periods might yield higher energy output, as
wind speed and pressure conditions vary throughout the year. For example, locations with
stronger winds during the winter months might be more suitable for wind turbine installation.
3. **Data Quality & Transformations**
* Based on the skewness and kurtosis values, it appears that no data transformations are
necessary for wind speed data. However, for pressure data, a log-transform or normalization
might be useful to reduce the impact of extreme values and improve the distribution of the
data.
* Additional data such as temperature, humidity, and topographic information could refine
site-selection or energy forecasting models by providing more context about the local
environment and its impact on wind and pressure conditions.


--- Page 25 ---
FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE25
4. **Further Analysis**
* The coefficient of variation and range metrics can reveal anomalies or outliers in the data
that necessitate further investigation. For example, locations with high coefficient of
variation values may indicate unstable wind conditions or outliers in the data.
* These statistics could guide more advanced modeling approaches, such as using machine learning
algorithms to identify patterns and relationships in the data that are not apparent through
simple descriptive statistics.
E. Output of the LLM to Single LP problem
lp_evaluator (to lp_evaluator):
validate constraints.
--------------------------------------------------------------------------------
[DEBUG] EvaluatorAgent received message: validate constraints.
lp_evaluator (to lp_evaluator):
✓Constraints are valid. Proceed with LP execution.
--------------------------------------------------------------------------------
lp_executor (to lp_executor):
Proceed with LP execution.
--------------------------------------------------------------------------------
[DEBUG] LPExecutorAgent received message: Proceed with LP execution.
Execution 1: Running LP solver...
✓Result 1: {’status’: ’Success’, ’optimal_solution’: [30.0, 0.0, 0.0, 0.0, 40.0, 20.0],
’objective_value’: 400.0}
==== Final Results ====
Execution 1: {’status’: ’Success’, ’optimal_solution’: [30.0, 0.0, 0.0, 0.0, 40.0, 20.0],
’objective_value’: 400.0}
=======================
lp_executor (to lp_executor):
Execution 1: {’status’: ’Success’, ’optimal_solution’: [30.0, 0.0, 0.0, 0.0, 40.0, 20.0],
’objective_value’: 400.0}
--------------------------------------------------------------------------------
[INFO] LP Execution Finished.
F. Output of the LLM to multi-stage LP problem
lp_evaluator (to lp_evaluator):
validate constraints.
--------------------------------------------------------------------------------
[DEBUG] EvaluatorAgent received message: validate constraints.
lp_evaluator (to lp_evaluator):
✓Problem 0 constraints are valid.
✓Problem 1 constraints are valid.
✓Problem 2 constraints are valid.
--------------------------------------------------------------------------------
[DEBUG] LPExecutorAgent received message: Proceed with LP execution.


--- Page 26 ---
26
JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
==== Final Results ====
Problem 0 Execution: {’problem_id’: 0, ’status’: ’Success’, ’optimal_solution’: [30.0, 0.0, 0.0,
0.0, 40.0, 20.0], ’objective_value’: 400.0}
Problem 1 Execution: {’problem_id’: 1, ’status’: ’Success’, ’optimal_solution’: [0.0, 0.0, 20.0,
0.0], ’objective_value’: 40.0}
Problem 2 Execution: {’problem_id’: 2, ’status’: ’Success’, ’optimal_solution’: [0.0, -0.0,
15.0], ’objective_value’: 75.0}
=======================
[INFO] LP Execution Finished.


--- Page 27 ---
FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE27
References
[1] E. A. Feigenbaum, J. Feldman, et al., Computers and thought,
vol. 7. 1963.
[2] D. Bobrow et al., “Natural language input for a computer
problem solving system,” 1964.
[3] D. J. Briars and J. H. Larkin, “An integrated model of skill in
solving elementary word problems,” Cognition and instruction,
vol. 1, no. 3, pp. 245–296, 1984.
[4] C. R. Fletcher, “Understanding and solving arithmetic word
problems: A computer simulation,” Behavior Research Meth-
ods, Instruments, & Computers, vol. 17, no. 5, pp. 565–571,
1985.
[5] Y. Wang, X. Liu, and S. Shi, “Deep neural solver for math word
problems,” in EMNLP, pp. 845–854, 2017.
[6] M. J. Hosseini, H. Hajishirzi, O. Etzioni, and N. Kushman,
“Learning to solve arithmetic word problems with verb cate-
gorization,” in EMNLP, pp. 523–533, 2014.
[7] N. Kushman, Y. Artzi, L. Zettlemoyer, and R. Barzilay,
“Learning to automatically solve algebra word problems,” in
ACL, pp. 271–281, June 2014.
[8] L. Zhou, S. Dai, and L. Chen, “Learn to solve algebra word
problems using quadratic programming,” in EMNLP, pp. 817–
822, 2015.
[9] A. Mitra and C. Baral, “Learning to use formulas to solve
simple arithmetic problems,” in ACL, pp. 2144–2153, 2016.
[10] S. Roy, T. Vieira, and D. Roth, “Reasoning about quantities
in natural language,” TACL, vol. 3, pp. 1–13, 2015.
[11] S. Roy and D. Roth, “Solving general arithmetic word prob-
lems,” in EMNLP, pp. 1743–1752, 2015.
[12] S. Shi, Y. Wang, C.-Y. Lin, X. Liu, and Y. Rui, “Automati-
cally solving number word problems by semantic parsing and
reasoning,” in EMNLP, pp. 1132–1142, 2015.
[13] R. Koncel-Kedziorski, H. Hajishirzi, A. Sabharwal, O. Etzioni,
and S. D. Ang, “Parsing algebraic word problems into equa-
tions,” TACL, vol. 3, pp. 585–597, 2015.
[14] J. Couperus, “Large language models and mathematical un-
derstanding,” Master’s thesis, 2023.
[15] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi,
Q. V. Le, D. Zhou, et al., “Chain-of-thought prompting elic-
its reasoning in large language models,” NeurIPS, vol. 35,
pp. 24824–24837, 2022.
[16] N. Matzakos, S. Doukakis, and M. Moundridou, “Learning
mathematics with large language models: A comparative study
with computer algebra systems and other tools,” Interna-
tional Journal of Emerging Technologies in Learning (Online),
vol. 18, no. 20, p. 51, 2023.
[17] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT:
Pre-training of Deep Bidirectional Transformers for Language
Understanding,” 2019.
[18] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,
M. Lewis, L. Zettlemoyer, and V. Stoyanov, “RoBERTa: A
robustly optimized BERT pretraining approach,” 2019.
[19] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed,
O. Levy, V. Stoyanov, and L. Zettlemoyer, “Bart: Denoising
sequence-to-sequence pre-training for natural language genera-
tion, translation, and comprehension,” in ACL, pp. 7871–7880,
2020.
[20] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever,
“Improving
Language
Understanding
by
Generative
Pre-
Training,” p. 12, June 2018.
[21] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and
I. Sutskever, “Language Models are Unsupervised Multitask
Learners,” p. 24, Feb. 2019.
[22] P. Clark, O. Etzioni, D. Khashabi, T. Khot, B. D. Mishra,
K. Richardson, A. Sabharwal, C. Schoenick, O. Tafjord,
N. Tandon, S. Bhakthavatsalam, D. Groeneveld, M. Guerquin,
and M. Schmitz, “From ’F’ to ’a’ on the N.Y. regents science
exams: An overview of the aristo project,” 2021.
[23] M. Geva, A. Gupta, and J. Berant, “Injecting numerical rea-
soning skills into language models,” in ACL (D. Jurafsky,
J. Chai, N. Schluter, and J. Tetreault, eds.), pp. 946–958, July
2020.
[24] Y. Feng, J. Zhang, X. Zhang, L. Liu, C. Li, and H. Chen, “In-
jecting numerical reasoning skills into knowledge base question
answering models,” 2022.
[25] S. Peng, K. Yuan, L. Gao, and Z. Tang, “MathBERT: A pre-
trained model for mathematical formula understanding,” 2021.
[26] A. Q. Jiang, W. Li, J. M. Han, and Y. Wu, “Lisa: Language
models of isabelle proofs,” in AITP, pp. 378–392, 2021.
[27] F. Zhu, W. Lei, Y. Huang, C. Wang, S. Zhang, J. Lv, F. Feng,
and T.-S. Chua, “Tat-qa: A question answering benchmark on
a hybrid of tabular and textual content in ﬁnance,” in ACL,
pp. 3277–3287, 2021.
[28] Y. Zhao, Y. Li, C. Li, and R. Zhang, “Multihiertt: Numerical
reasoning over multi hierarchical tabular and textual data,” in
ACL, pp. 6588–6600, 2022.
[29] Z. Jie, J. Li, and W. Lu, “Learning to reason deductively: Math
word problem solving as complex relation extraction,” 2022.
[30] Z. Li, W. Zhang, C. Yan, Q. Zhou, C. Li, H. Liu, and
Y. Cao, “Seeking patterns, not just memorizing procedures:
Contrastive learning for solving math word problems,” 2022.
[31] OpenAI, “Gpt-4 technical report,” 2023.
[32] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi, “Metaicl:
Learning to learn in context,” in NAACL, pp. 2791–2809, 2022.
[33] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan,
P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,
et al., “Language models are few-shot learners,” NeurIPS,
vol. 33, pp. 1877–1901, 2020.
[34] Y. Chen, R. Zhong, S. Zha, G. Karypis, and H. He, “Meta-
learning via language model in-context tuning,” in ACL,
pp. 719–730, 2022.
[35] Z. Yang, M. Ding, Q. Lv, Z. Jiang, Z. He, Y. Guo, J. Bai,
and J. Tang, “Gpt can solve mathematical problems without
a calculator,” arXiv, 2023.
[36] T. Liu and B. K. H. Low, “Goat: Fine-tuned llama outperforms
gpt-4 on arithmetic tasks,” arXiv, 2023.
[37] X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi,
S. Narang, A. Chowdhery, and D. Zhou, “Self-consistency
improves chain of thought reasoning in language models,” in
ICLR, 2023.
[38] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,
A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann,
et al., “Palm: Scaling language modeling with pathways,”
arXiv, 2022.
[39] W. Chen, X. Ma, X. Wang, and W. W. Cohen, “Program of
thoughts prompting: Disentangling computation from reason-
ing for numerical reasoning tasks,” arXiv, 2022.
[40] P. Lu, L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu, T. Rajpuro-
hit, P. Clark, and A. Kalyan, “Dynamic prompt learning via
policy gradient for semi-structured mathematical reasoning,”
in ICLR, 2023.
[41] Z. Zhang, A. Zhang, M. Li, and A. Smola, “Automatic chain of
thought prompting in large language models,” in ICLR, 2023.
[42] Y. Fu, H. Peng, A. Sabharwal, P. Clark, and T. Khot,
“Complexity-based prompting for multi-step reasoning,” in
ICLR, 2023.
[43] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang,
J. Callan, and G. Neubig, “Pal: Program-aided language mod-
els,” in ICML, pp. 10764–10799, 2023.
[44] I. Drori, S. Zhang, R. Shuttleworth, L. Tang, A. Lu, E. Ke,
K. Liu, L. Chen, S. Tran, N. Cheng, et al., “A neural network
solves, explains, and generates university math problems by
program synthesis and few-shot learning at human level,”
PNAS, vol. 119, no. 32, p. e2123433119, 2022.
[45] J. He-Yueya, G. Poesia, R. E. Wang, and N. D. Goodman,
“Solving math word problems by combining language models
with symbolic solvers,” arXiv, 2023.
[46] P. Lu, L. Qiu, W. Yu, S. Welleck, and K.-W. Chang, “A
survey of deep learning for mathematical reasoning,” in ACL
(A. Rogers, J. Boyd-Graber, and N. Okazaki, eds.), pp. 14605–
14631, 2023.
[47] S. Qiao, Y. Ou, N. Zhang, X. Chen, Y. Yao, S. Deng, C. Tan,
F. Huang, and H. Chen, “Reasoning with language model
prompting: A survey,” in ACL, pp. 5368–5393, 2023.
[48] Z. Chu, J. Chen, Q. Chen, W. Yu, T. He, H. Wang, W. Peng,
M. Liu, B. Qin, and T. Liu, “A survey of chain of thought
reasoning: Advances, frontiers and future,” 2023.
[49] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, “Pre-
trained models for natural language processing: A survey,”
Science China Technological Sciences, vol. 63, no. 10, pp. 1872–
1897, 2020.


--- Page 28 ---
28
JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
[50] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,
B. Zhang, J. Zhang, Z. Dong, et al., “A survey of large language
models,” arXiv, 2023.
[51] Y. Yan, J. Su, J. He, F. Fu, X. Zheng, Y. Lyu, K. Wang,
S. Wang, Q. Wen, and X. Hu, “A survey of mathematical
reasoning in the era of multimodal large language model:
Benchmark, method & challenges,” arXiv, 2024.
[52] W. Liu, H. Hu, J. Zhou, Y. Ding, J. Li, J. Zeng, M. He,
Q. Chen, B. Jiang, A. Zhou, et al., “Mathematical language
models: A survey,” arXiv preprint arXiv:2312.07622, 2023.
[53] S. Imani, L. Du, and H. Shrivastava, “Mathprompter: Mathe-
matical reasoning using large language models,” arXiv, 2023.
[54] T. Wang, W.-Y. Yu, R. She, W. Yang, T. Chen, and J. Zhang,
“Leveraging large language models for solving rare MIP chal-
lenges,” arXiv preprint arXiv:2409.04464, 2024.
[55] T. Ahmed and S. Choudhury, “Lm4opt: Unveiling the po-
tential
of
large
language
models
in
formulating
mathe-
matical optimization problems, 2024,” URL: https://arxiv.
org/abs/2403.01342.
[56] X. Huang, Q. Shen, Y. Hu, A. Gao, and B. Wang, “Llms for
mathematical modeling: Towards bridging the gap between
natural and mathematical languages,” arXiv:2405.13144v3,
2025.
[57] J. Lai, J. Xu, Y. Yang, Y. Huang, C. Cao, and J. Xu, “Exe-
cuting arithmetic: Fine-tuning large language models as turing
machines,” arXiv preprint arXiv:2410.07896, 2024.
[58] D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and
M. Gardner, “Drop: A reading comprehension benchmark
requiring discrete reasoning over paragraphs,” in NAACL,
pp. 2368–2378, 2019.
[59] X. Zhang, D. Ramachandran, I. Tenney, Y. Elazar, and
D. Roth, “Do language embeddings capture scales?,” in Black-
boxNLP Workshop on Analyzing and Interpreting Neural Net-
works for NLP, pp. 292–299, 2020.
[60] G. Spithourakis and S. Riedel, “Numeracy for language models:
Evaluating and improving their ability to predict numbers,” in
ACL, vol. 56, pp. 2104–2115, 2018.
[61] E. Wallace, Y. Wang, S. Li, S. Singh, and M. Gardner, “Do nlp
models know numbers? probing numeracy in embeddings,” in
EMNLP, pp. 5307–5315, 2019.
[62] T. Berg-Kirkpatrick and D. Spokoyny, “An empirical inves-
tigation of contextualized number prediction,” in EMNLP,
p. 4754–4764, Nov. 2020.
[63] R. Nogueira, Z. Jiang, and J. Lin, “Investigating the limitations
of transformers with simple arithmetic tasks,” arXiv, 2021.
[64] C. Wang, B. Zheng, Y. Niu, and Y. Zhang, “Exploring gener-
alization ability of pretrained language models on arithmetic
and logical reasoning,” in NLPCC, pp. 758–769, 2021.
[65] M. Muﬀo, A. Cocco, and E. Bertino, “Evaluating Transformer
Language Models on Arithmetic Operations Using Number
Decomposition,”
[66] Z. Yuan, H. Yuan, C. Tan, W. Wang, and S. Huang, “How well
do large language models perform in arithmetic tasks?,” arXiv,
2023.
[67] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn,
E. Saravia, A. Poulton, V. Kerkez, and R. Stojnic, “Galactica:
A Large Language Model for Science,” 2022.
[68] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar,
et al., “Llama: Open and eﬃcient foundation language mod-
els,” arXiv, 2023.
[69] H. Zhou, A. Nova, H. Larochelle, A. Courville, B. Neyshabur,
and H. Sedghi, “Teaching algorithmic reasoning via in-context
learning,” 2022.
[70] S. Jelassi, S. d’Ascoli, C. Domingo-Enrich, Y. Wu, Y. Li, and
F. Charton, “Length generalization in arithmetic transform-
ers,” arXiv, 2023.
[71] M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski,
J. Austin, D. Bieber, D. Dohan, A. Lewkowycz, M. Bosma,
D. Luan, C. Sutton, and A. Odena, “Show Your Work: Scratch-
pads for Intermediate Computation with Language Models,”
2021.
[72] Z. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou,
“Scaling relationship on learning mathematical reasoning with
large language models,” arXiv, 2023.
[73] L. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok,
Z. Li, A. Weller, and W. Liu, “Metamath: Bootstrap your
own mathematical questions for large language models,” arXiv,
2023.
[74] H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng,
Q. Lin, S. Chen, and D. Zhang, “Wizardmath: Empowering
mathematical reasoning for large language models via rein-
forced evol-instruct,” arXiv, 2023.
[75] Z. Zhou, Q. Wang, M. Jin, J. Yao, J. Ye, W. Liu, W. Wang,
X. Huang, and K. Huang, “Mathattack: Attacking large lan-
guage models towards math solving ability,” 2023.
[76] Z. Azerbayev, H. Schoelkopf, K. Paster, M. Dos Santos,
S. McAleer, A. Q. Jiang, J. Deng, S. Biderman, and S. Welleck,
“Llemma: An open language model for mathematics,” Min-
erva, vol. 8, p. 164B.
[77] S. Polu and I. Sutskever, “Generative language modeling for
automated theorem proving,” 2020.
[78] A. Q. Jiang, S. Welleck, J. P. Zhou, T. Lacroix, J. Liu, W. Li,
M. Jamnik, G. Lample, and Y. Wu, “Draft, sketch, and prove:
Guiding formal theorem provers with informal proofs,” in
ICLR, 2023.
[79] S. Welleck, J. Liu, R. Le Bras, H. Hajishirzi, Y. Choi, and
K. Cho, “Naturalproofs: Mathematical theorem proving in nat-
ural language,” in NeurIPS Datasets and Benchmarks Track
(Round 1), 2021.
[80] G. Irving, C. Szegedy, A. A. Alemi, N. Eén, F. Chollet, and
J. Urban, “Deepmath-deep sequence models for premise selec-
tion,” NeurIPS, vol. 29, 2016.
[81] Y. Wu, A. Jiang, J. Ba, and R. B. Grosse, “Int: An inequality
benchmark for evaluating generalization in theorem proving,”
in ICLR, 2021.
[82] K. Yang and J. Deng, “Learning to prove theorems via inter-
acting with proof assistants,” in ICML, pp. 6984–6994, 2019.
[83] J. Maynez, S. Narayan, B. Bohnet, and R. McDonald, “On
faithfulness and factuality in abstractive summarization,” in
ACL, pp. 1906–1919, July 2020.
[84] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii,
Y. J. Bang, A. Madotto, and P. Fung, “Survey of hallucination
in natural language generation,” ACM Computing Surveys,
vol. 55, no. 12, pp. 1–38, 2023.
[85] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all
you need,” NeurIPS, vol. 30, 2017.
[86] K.
Noorbakhsh,
M. Sulaiman,
M. Shariﬁ, K. Roy,
and
P. Jamshidi, “Pretrained language models are symbolic math-
ematics solvers too!,” arXiv, 2021.
[87] C. Raﬀel, N. Shazeer,
A. Roberts,
K. Lee,
S. Narang,
M. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring the
Limits of Transfer Learning with a Uniﬁed Text-to-Text Trans-
former,” JMLR, vol. 21, no. 140, pp. 1–67, 2020.
[88] J. D. M.-W. C. Kenton and L. K. Toutanova, “Bert: Pre-
training of deep bidirectional transformers for language under-
standing,” in NAACL, pp. 4171–4186, 2019.
[89] B. Kim, K. S. Ki, D. Lee, and G. Gweon, “Point to the Expres-
sion: Solving Algebraic Word Problems using the Expression-
Pointer Transformer Model,” in EMNLP, (Online), pp. 3768–
3779, 2020.
[90] J. Shen, Y. Yin, L. Li, L. Shang, X. Jiang, M. Zhang, and
Q. Liu, “Generate & Rank: A Multi-task Framework for Math
Word Problems,” 2021.
[91] A.
Q.
Jiang,
W.
Li,
S.
Tworkowski,
K.
Czechowski,
T. Odrzygóźdź, P. Miłoś, Y. Wu, and M. Jamnik, “Thor:
Wielding hammers to integrate language models and auto-
mated theorem provers,” 2022.
[92] G. Lample, M.-A. Lachaux, T. Lavril, X. Martinet, A. Hayat,
G. Ebner, A. Rodriguez, and T. Lacroix, “HyperTree Proof
Search for Neural Theorem Proving,”
[93] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika,
D. Song, and J. Steinhardt, “Measuring massive multitask
language understanding,” ICLR, 2021.
[94] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart,
E. Tang, D. Song, and J. Steinhardt, “Measuring mathematical
problem solving with the math dataset,” in NeurIPS Datasets
and Benchmarks Track (Round 2), 2021.
[95] J. M. Han, J. Rute, Y. Wu, E. Ayers, and S. Polu, “Proof ar-
tifact co-training for theorem proving with language models,”
in ICLR, 2022.


--- Page 29 ---
FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE29
[96] A.
Lewkowycz,
A.
Andreassen,
D.
Dohan,
E.
Dyer,
H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag,
T.
Gutman-Solo,
et
al.,
“Solving
quantitative
reasoning
problems with language models,” NeurIPS, vol. 35, pp. 3843–
3857, 2022.
[97] Y. Wu, M. Rabe, and W. Li, “LIME: Learning Inductive Bias
for Primitives of Mathematical Reasoning,”
[98] Z. Liang, J. Zhang, L. Wang, W. Qin, Y. Lan, J. Shao, and
X. Zhang, “MWP-BERT: Numeracy-augmented pre-training
for math word problem solving,” 2022.
[99] R. NAKAMOTO, B. Flanagan, T. Yamauchi, D. Yilling,
K. Takami, and H. Ogata, “Enhancing automated scoring of
math self-explanation quality using llm-generated datasets: A
semi-supervised approach,” 2023.
[100] Z. Zhang, T. Ge, Z. Liang, W. Yu, D. Yu, M. Jia, D. Yu, and
M. Jiang, “Learn beyond the answer: Training language models
with reﬂection for mathematical reasoning,” arXiv, 2024.
[101] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao,
and D. Jiang, “Wizardlm: Empowering large language models
to follow complex instructions,” arXiv, 2023.
[102] L. Yuan, G. Cui, H. Wang, N. Ding, X. Wang, J. Deng, B. Shan,
H. Chen, R. Xie, Y. Lin, et al., “Advancing llm reasoning
generalists with preference trees,” arXiv, 2024.
[103] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon,
and C. Finn, “Direct preference optimization: Your language
model is secretly a reward model,” NeurIPS, vol. 36, 2024.
[104] H. Chen, G. He, L. Yuan, G. Cui, H. Su, and J. Zhu, “Noise con-
trastive alignment of language models with explicit rewards,”
arXiv, 2024.
[105] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li,
R. Hu, T. Zhang, F. Wu, et al., “Instruction tuning for large
language models: A survey,” arXiv, 2023.
[106] Y. Liu, A. Singh, C. D. Freeman, J. D. Co-Reyes, and P. J.
Liu, “Improving large language model ﬁne-tuning for solving
math problems,” arXiv, 2023.
[107] A. Yang, B. Zhang, B. Hui, B. Gao, B. Yu, C. Li, D. Liu,
J. Tu, J. Zhou, J. Lin, et al., “Qwen2. 5-math technical report:
Toward mathematical expert model via self-improvement,”
arXiv, 2024.
[108] H. Ying, S. Zhang, L. Li, Z. Zhou, Y. Shao, Z. Fei, Y. Ma,
J. Hong, K. Liu, Z. Wang, Y. Wang, Z. Wu, S. Li, F. Zhou,
H. Liu, S. Zhang, W. Zhang, H. Yan, X. Qiu, J. Wang, K. Chen,
and D. Lin, “Internlm-math: Open math large language models
toward veriﬁable reasoning,” 2024.
[109] M. Kazemi, N. Kim, D. Bhatia, X. Xu, and D. Ramachandran,
“Lambada: Backward chaining for automated reasoning in
natural language,” arXiv, 2022.
[110] Z. Yang, J. Qin, J. Chen, L. Lin, and X. Liang, “Logicsolver:
Towards interpretable math word problem solving with logical
prompt-enhanced learning,” arXiv, 2022.
[111] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli,
L. Zettlemoyer, N. Cancedda, and T. Scialom, “Toolformer:
Language models can teach themselves to use tools,” 2023.
[112] A. Parisi, Y. Zhao, and N. Fiedel, “Talm: Tool augmented
language models,” 2022.
[113] D. Jiang, M. Fonseca, and S. B. Cohen, “Leanreasoner: Boost-
ing complex logical reasoning with lean,” arXiv, 2024.
[114] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettle-
moyer, and M. T. Ribeiro, “Art: Automatic multi-step reason-
ing and tool-use for large language models,” 2023.
[115] Z. Ling, Y. Fang, X. Li, Z. Huang, M. Lee, R. Memisevic, and
H. Su, “Deductive veriﬁcation of chain-of-thought reasoning,”
arXiv, 2023.
[116] Z. Zhao, Y. Rong, D. Guo, E. Gözlüklü, E. Gülboy, and
E. Kasneci, “Stepwise self-consistent mathematical reasoning
with large language models,” arXiv, 2024.
[117] O. Yoran, T. Wolfson, B. Bogin, U. Katz, D. Deutch, and J. Be-
rant, “Answering questions by meta-reasoning over multiple
chains of thought,” arXiv, 2023.
[118] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griﬃths, Y. Cao, and
K. Narasimhan, “Tree of thoughts: Deliberate problem solving
with large language models,” arXiv, 2023.
[119] Y. Yao, Z. Li, and H. Zhao, “Beyond chain-of-thought, eﬀective
graph-of-thought reasoning in large language models,” arXiv,
2023.
[120] E. Zelikman, Y. Wu, J. Mu, and N. Goodman,
“Star:
Bootstrapping reasoning with reasoning,” NeurIPS, vol. 35,
pp. 15476–15488, 2022.
[121] X. Zhu, J. Wang, L. Zhang, Y. Zhang, R. Gan, J. Zhang,
and Y. Yang, “Solving math word problems via cooperative
reasoning induced language models,” arXiv, 2022.
[122] L. Luo, Y. Liu, R. Liu, S. Phatale, H. Lara, Y. Li, L. Shu,
Y. Zhu, L. Meng, J. Sun, et al., “Improve mathematical rea-
soning in language models by automated process supervision,”
arXiv, 2024.
[123] L. Nelson, “The socratic method,” vol. 2, pp. 34–38, 1980.
[124] A. Zeng, M. Attarian, B. Ichter, K. Choromanski, A. Wong,
S. Welker, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani,
et al., “Socratic models: Composing zero-shot multimodal rea-
soning with language,” arXiv, 2022.
[125] J. P. Zhou, C. Staats, W. Li, C. Szegedy, K. Q. Weinberger, and
Y. Wu, “Don’t trust: Verify–grounding llm quantitative reason-
ing with autoformalization,” arXiv preprint arXiv:2403.18120,
2024.
[126] K. Dong, A. Mahankali, and T. Ma, “Formal theorem proving
by rewarding llms to decompose proofs hierarchically,” arXiv
preprint arXiv:2411.01829, 2024.
[127] Z. Li, Z. Zhou, Y. Yao, X. Zhang, Y.-F. Li, C. Cao, F. Yang, and
X. Ma, “Neuro-symbolic data generation for math reasoning,”
Advances in Neural Information Processing Systems, vol. 37,
pp. 23488–23515, 2025.
[128] Z. Wu, Q. Zeng, Z. Zhang, Z. Tan, C. Shen, and M. Jiang,
“Enhancing mathematical reasoning in llms by stepwise cor-
rection,” arXiv preprint arXiv:2410.12934, 2024.
[129] K. M. Collins, A. Q. Jiang, S. Frieder, L. Wong, M. Zilka,
U. Bhatt, T. Lukasiewicz, Y. Wu, J. B. Tenenbaum, W. Hart,
et al., “Evaluating language models for mathematics through
interactions,” Proceedings of the National Academy of Sciences,
vol. 121, no. 24, p. e2318124121, 2024.
[130] N. Chen, N. Wu, J. Chang, and J. Li, “Controlmath: Con-
trollable data generation promotes math generalist models,”
arXiv, 2024.
[131] T. X. Olausson, A. Gu, B. Lipkin, C. E. Zhang, A. Solar-
Lezama, J. B. Tenenbaum, and R. Levy, “Linc: A neurosym-
bolic approach for logical reasoning by combining language
models with ﬁrst-order logic provers,” arXiv, 2023.
[132] S. Yin, W. You, Z. Ji, G. Zhong, and J. Bai, “Mumath-
code: Combining tool-use large language models with multi-
perspective data augmentation for mathematical reasoning,”
arXiv, 2024.
[133] X. Yue, X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su,
and W. Chen, “Mammoth: Building math generalist models
through hybrid instruction tuning,” arXiv, 2023.
[134] S. Pitis, M. R. Zhang, A. Wang, and J. Ba, “Boosted prompt
ensembles for large language models,” arXiv, 2023.
[135] N. Miao, Y. W. Teh, and T. Rainforth, “Selfcheck: Using llms
to zero-shot check their own step-by-step reasoning,” arXiv,
2023.
[136] R. Naik, V. Chandrasekaran, M. Yuksekgonul, H. Palangi, and
B. Nushi, “Diversity of thought improves reasoning abilities of
large language models,” arXiv, 2023.
[137] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and
Z. Hu, “Reasoning with language model is planning with world
model,” arXiv, 2023.
[138] A. Zhou, K. Yan, M. Shlapentokh-Rothman, H. Wang, and Y.-
X. Wang, “Language agent tree search uniﬁes reasoning acting
and planning in language models,” arXiv, 2023.
[139] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and
P. Stone, “Llm+ p: Empowering large language models with
optimal planning proﬁciency,” arXiv, 2023.
[140] G. Dagan, F. Keller, and A. Lascarides, “Dynamic planning
with a llm,” arXiv, 2023.
[141] Z. Zhou, J. Song, K. Yao, Z. Shu, and L. Ma, “Isr-llm: Iterative
self-reﬁned large language model for long-horizon sequential
task planning,” arXiv, 2023.
[142] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun,
L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano,
C. Hesse, and J. Schulman, “Training veriﬁers to solve math
word problems,” 2021.
[143] M. Khalifa, L. Logeswaran, M. Lee, H. Lee, and L. Wang,
“Discriminator-guided multi-step reasoning with language
models,” arXiv, 2023.


--- Page 30 ---
30
JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
[144] N. Shinn, F. Cassano, A. Gopinath, K. R. Narasimhan, and
S. Yao, “Reﬂexion: Language agents with verbal reinforcement
learning,” in NeurIPS, 2023.
[145] Y. Zhang, J. Yang, Y. Yuan, and A. C.-C. Yao, “Cumulative
reasoning with large language models,” arXiv, 2023.
[146] M. Zhang, Z. Wang, Z. Yang, W. Feng, and A. Lan, “Inter-
pretable math word problem solution generation via step-by-
step planning,” arXiv, 2023.
[147] A. Hosseini, X. Yuan, N. Malkin, A. Courville, A. Sordoni,
and R. Agarwal, “V-star: Training veriﬁers for self-taught
reasoners,” arXiv, 2024.
[148] E. Zelikman, G. Harik, Y. Shao, V. Jayasiri, N. Haber, and
N. D. Goodman, “Quiet-star: Language models can teach
themselves to think before speaking,” arXiv, 2024.
[149] T. Q. Luong, X. Zhang, Z. Jie, P. Sun, X. Jin, and H. Li, “Reft:
Reasoning with reinforced ﬁne-tuning,” arXiv, 2024.
[150] A. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes,
A. Singh, K. Baumli, S. Iqbal, C. Bishop, R. Roelofs, et al.,
“Training language models to self-correct via reinforcement
learning,” arXiv, 2024.
[151] Q. Ma, H. Zhou, T. Liu, J. Yuan, P. Liu, Y. You, and H. Yang,
“Let’s reward step by step: Step-level reward model as the
navigators for reasoning,” arXiv, 2023.
[152] D. Zhang, X. Huang, D. Zhou, Y. Li, and W. Ouyang, “Ac-
cessing gpt-4 level mathematical olympiad solutions via monte
carlo tree self-reﬁne with llama-3 8b,” arXiv, 2024.
[153] Y. Zhao, H. Yin, B. Zeng, H. Wang, T. Shi, C. Lyu, L. Wang,
W. Luo, and K. Zhang, “Marco-o1: Towards open reasoning
models for open-ended solutions,” arXiv, 2024.
[154] X. Lai, Z. Tian, Y. Chen, S. Yang, X. Peng, and J. Jia,
“Step-dpo: Step-wise preference optimization for long-chain
reasoning of llms,” arXiv, 2024.
[155] Y. Deng and P. Mineiro, “Flow-dpo: Improving llm mathe-
matical reasoning through online multi-agent learning,” arXiv,
2024.
[156] Y. Ding, H. Hu, J. Zhou, Q. Chen, B. Jiang, and L. He,
“Boosting large language models with socratic method for
conversational mathematics teaching,” in CIKM, pp. 3730–
3735, 2024.
[157] J. Macina, N. Daheim, S. P. Chowdhury, T. Sinha, M. Kapur,
I. Gurevych, and M. Sachan, “Mathdial: A dialogue tutoring
dataset with rich pedagogical properties grounded in math
reasoning problems,” arXiv, 2023.
[158] Q. Team, “Qwq: Reﬂect deeply on the boundaries of the
unknown,” 2024.
[159] OpenAI, “OpenAI O1 System Card,” 2024.
[160] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh,
A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford,
et al., “Gpt-4o system card,” arXiv, 2024.
[161] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen,
X. Liu, J. Wang, W. Ge, Y. Fan, K. Dang, M. Du, X. Ren,
R. Men, D. Liu, C. Zhou, J. Zhou, and J. Lin, “Qwen2-vl:
Enhancing vision-language model’s perception of the world at
any resolution,” arXiv, 2024.
[162] G. Xu, P. Jin, L. Hao, Y. Song, L. Sun, and L. Yuan, “Llava-o1:
Let vision language models reason step-by-step,” arXiv, 2024.
[163] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut,
J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, et al., “Gem-
ini: a family of highly capable multimodal models,” arXiv,
2023.
[164] T. GLM, A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin,
D. Zhang, D. Rojas, G. Feng, H. Zhao, et al., “Chatglm: A
family of large language models from glm-130b to glm-4 all
tools,” arXiv, 2024.
[165] K. Xiang, Z. Liu, Z. Jiang, Y. Nie, R. Huang, H. Fan, H. Li,
W. Huang, Y. Zeng, J. Han, et al., “Atomthink: A slow thinking
framework for multimodal mathematical reasoning,” arXiv,
2024.
[166] W. Shi, Z. Hu, Y. Bin, J. Liu, Y. Yang, S.-K. Ng, L. Bing,
and R. K.-W. Lee, “Math-llava: Bootstrapping mathematical
reasoning for multimodal large language models,” 2024.
[167] Anonymous, “Diving into self-evolve training for multimodal
reasoning,” in Submitted to The Thirteenth International Con-
ference on Learning Representations, 2024.
[168] Y. wang and Y. Fu, “Understanding, abstracting and checking:
Evoking complicated multimodal reasoning in LMMs,” 2024.
[169] M. Forbes and Y. Choi, “Verb physics: Relative physical knowl-
edge of actions and objects,” in ACL, pp. 266–276, 2017.
[170] G. Spithourakis, I. Augenstein, and S. Riedel, “Numerically
grounded language models for semantic error correction,” in
EMNLP, pp. 987–992, 2016.
[171] Y. Elazar, A. Mahabal, D. Ramachandran, T. Bedrax-Weiss,
and D. Roth, “How large are lions? inducing distributions over
quantitative attributes,” in ACL, pp. 3973–3983, 2019.
[172] W. Ling, D. Yogatama, C. Dyer, and P. Blunsom, “Program in-
duction by rationale generation: Learning to solve and explain
algebraic word problems,” in ACL, pp. 158–167, 2017.
[173] A. Patel, S. Bhattamishra, and N. Goyal, “Are nlp models
really able to solve simple math word problems?,” in NAACL,
pp. 2080–2094, 2021.
[174] S. Upadhyay and M.-W. Chang, “Draw: A challenging and
diverse algebra word problem set,” tech. rep., Citeseer, 2015.
[175] J. Qin, L. Lin, X. Liang, R. Zhang, and L. Lin, “Semantically-
aligned universal tree-structured solver for math word prob-
lems,” in EMNLP, pp. 3780–3789, 2020.
[176] R. Alghamdi, Z. Liang, and X. Zhang, “Armath: a dataset for
solving arabic math word problems,” in LREC, pp. 351–362,
2022.
[177] A. Kalyan, A. Kumar, A. Chandrasekaran, A. Sabharwal, and
P. Clark, “How much coﬀee was consumed during emnlp 2019?
fermi problems: A new reasoning challenge for ai,” in EMNLP,
pp. 7318–7328, 2021.
[178] Z. Wang, R. Xia, and P. Liu, “Generative ai for math: Part i–
mathpile: A billion-token-scale pretraining corpus for math,”
arXiv, 2023.
[179] K. Paster, M. D. Santos, Z. Azerbayev, and J. Ba, “Open-
webmath: An open dataset of high-quality mathematical web
text,” 2023.
[180] A. Amini, S. Gabriel, S. Lin, R. Koncel-Kedziorski, Y. Choi,
and H. Hajishirzi, “Mathqa: Towards interpretable math word
problem solving with operation-based formalisms,” in NAACL,
pp. 2357–2367, 2019.
[181] Z. Chen, W. Chen, C. Smiley, S. Shah, I. Borova, D. Langdon,
R. Moussa, M. Beane, T.-H. Huang, B. R. Routledge, et al.,
“Finqa: A dataset of numerical reasoning over ﬁnancial data,”
in EMNLP, pp. 3697–3711, 2021.
[182] A. Grabowski, A. Korniłowicz, and A. Naumowicz, “Four
decades of mizar: Foreword,” Journal of Automated Reasoning,
vol. 55, pp. 191–198, 2015.
[183] C. Kaliszyk, F. Chollet, and C. Szegedy, “Holstep: A machine
learning dataset for higher-order logic theorem proving,” in
ICLR, 2017.
[184] K. Bansal, S. Loos, M. Rabe, C. Szegedy, and S. Wilcox,
“Holist: An environment for machine learning of higher order
logic theorem proving,” in ICML, pp. 454–463, 2019.
[185] W. Li, L. Yu, Y. Wu, and L. C. Paulson, “Isarstep: a benchmark
for high-level mathematical reasoning,” in ICLR, 2021.
[186] S. Mishra, A. Mitra, N. Varshney, B. Sachdeva, P. Clark,
C. Baral, and A. Kalyan, “Numglue: A suite of fundamental yet
challenging mathematical reasoning tasks,” in ACL, pp. 3505–
3523, 2022.
[187] D. Huang, S. Shi, C.-Y. Lin, J. Yin, and W.-Y. Ma, “How well
do computers solve math word problems? large-scale dataset
construction and evaluation,” in ACL, pp. 887–896, 2016.
[188] R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman, and
H. Hajishirzi, “Mawps: A math word problem repository,” in
NAACL, pp. 1152–1157, 2016.
[189] S.-Y. Miao, C.-C. Liang, and K.-Y. Su, “A diverse corpus for
evaluating and developing english math word problem solvers,”
in ACL, pp. 975–984, 2020.
[190] F.
Shi,
M.
Suzgun,
M.
Freitag,
X.
Wang,
S.
Srivats,
S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder, D. Zhou, et al.,
“Language models are multilingual chain-of-thought reason-
ers,” in ICLR, 2022.
[191] D. Saxton, E. Grefenstette, F. Hill, and P. Kohli, “Analysing
mathematical reasoning abilities of neural models,” in ICLR,
2019.
[192] W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied,
W. Chen, and N. Duan, “Agieval: A human-centric benchmark
for evaluating foundation models,” 2023.
[193] J. Zhao, J. Tong, Y. Mou, M. Zhang, Q. Zhang, and X.-
J. Huang, “Exploring the compositional deﬁciency of large


--- Page 31 ---
FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE31
language models in mathematical reasoning through trap prob-
lems,” in EMNLP, pp. 16361–16376, 2024.
[194] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng,
K.-W. Chang, M. Galley, and J. Gao, “Mathvista: Evaluating
mathematical reasoning of foundation models in visual con-
texts,” arXiv, 2023.
[195] K. Wang, J. Pan, W. Shi, Z. Lu, M. Zhan, and H. Li, “Mea-
suring multimodal mathematical reasoning with math-vision
dataset,” arXiv, 2024.
[196] W. Liu, Q. Pan, Y. Zhang, Z. Liu, J. Wu, J. Zhou, A. Zhou,
Q. Chen, B. Jiang, and L. He, “Cmm-math: A chinese multi-
modal math dataset to evaluate and enhance the mathematics
reasoning of large multimodal models,” arXiv, 2024.
[197] R. Zhang, D. Jiang, Y. Zhang, H. Lin, Z. Guo, P. Qiu, A. Zhou,
P. Lu, K.-W. Chang, Y. Qiao, et al., “Mathverse: Does your
multi-modal llm truly see the diagrams in visual math prob-
lems?,” in ECCV, pp. 169–186, 2025.
[198] J. Chen, J. Tang, J. Qin, X. Liang, L. Liu, E. P. Xing, and
L. Lin, “Geoqa: A geometric question answering benchmark
towards multimodal numerical reasoning,” arXiv, 2021.
[199] Q. Chen, L. Qin, J. Zhang, Z. Chen, X. Xu, and W. Che,
“M3CoT: A novel benchmark for multi-domain multi-step
multi-modal chain-of-thought,” in ACL, pp. 8199–8221, 2024.
[200] S. Xia, X. Li, Y. Liu, T. Wu, and P. Liu, “Evaluating mathe-
matical reasoning beyond accuracy,” arXiv, 2024.
[201] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang,
S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan,
R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang,
H. Sun, Y. Su, and W. Chen, “Mmmu: A massive multi-
discipline multimodal understanding and reasoning benchmark
for expert agi,” 2024.
[202] R. Qiao, Q. Tan, G. Dong, M. Wu, C. Sun, X. Song,
Z. GongQue, S. Lei, Z. Wei, M. Zhang, R. Qiao, Y. Zhang,
X. Zong, Y. Xu, M. Diao, Z. Bao, C. Li, and H. Zhang, “We-
math: Does your large multimodal model achieve human-like
mathematical reasoning?,” 2024.
[203] K. Chernyshev, V. Polshkov, E. Artemova, A. Myasnikov,
V.
Stepanov,
A.
Miasnikov,
and
S.
Tilga,
“U-math:
A
university-level benchmark for evaluating mathematical skills
in llms,” 2024.
[204] K. Zheng, J. M. Han, and S. Polu, “minif2f: a cross-system
benchmark for formal olympiad-level mathematics,” in ICLR,
2023.
[205] S. Han, H. Schoelkopf, Y. Zhao, Z. Qi, M. Riddell, W. Zhou,
J. Coady, D. Peng, Y. Qiao, L. Benson, L. Sun, A. Wardle-
Solano, H. Szabó, E. Zubova, M. Burtell, J. Fan, Y. Liu,
B. Wong, M. Sailor, A. Ni, L. Nan, J. Kasai, T. Yu, R. Zhang,
A. R. Fabbri, W. Kryscinski, S. Yavuz, Y. Liu, X. V. Lin,
S. Joty, Y. Zhou, C. Xiong, R. Ying, A. Cohan, and D. Radev,
“FOLIO: natural language reasoning with ﬁrst-order logic,”
in Proceedings of the 2024 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2024, Miami, FL,
USA, November 12-16, 2024 (Y. Al-Onaizan, M. Bansal, and
Y. Chen, eds.), pp. 22017–22031, Association for Computa-
tional Linguistics, 2024.
[206] S. Roy and D. Roth, “Mapping to declarative knowledge for
word problem solving,” TACL, vol. 6, pp. 159–172, 2018.
[207] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski,
D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al., “Program
synthesis with large language models,” arXiv, 2021.
[208] G. Li, H. A. A. K. Hammoud, H. Itani, D. Khizbullin, and
B. Ghanem, “Camel: Communicative agents for" mind" explo-
ration of large language model society,” in NeurIPS, 2023.
[209] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker,
T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe,
“Let’s verify step by step,” in ICLR, 2024.
[210] S. Mishra, M. Finlayson, P. Lu, L. Tang, S. Welleck, C. Baral,
T. Rajpurohit, O. Tafjord, A. Sabharwal, P. Clark, et al., “Lila:
A uniﬁed benchmark for mathematical reasoning,” in EMNLP,
pp. 5807–5832, 2022.
[211] B. Kim, K. S. Ki, S. Rhim, and G. Gweon, “Ept-x: An
expression-pointer transformer model that generates explana-
tions for numbers,” in ACL, pp. 4442–4458, 2022.
[212] S. Welleck, J. Liu, X. Lu, H. Hajishirzi, and Y. Choi, “Natu-
ralprover: Grounded mathematical proof generation with lan-
guage models,” NeurIPS, vol. 35, pp. 4913–4927, 2022.
[213] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart,
E. Tang, D. Song, and J. Steinhardt, “Measuring mathemat-
ical problem solving with the math dataset,” arXiv preprint
arXiv:2103.03874, 2021.
[214] T. H. Trinh, Y. Wu, Q. V. Le, H. He, and T. Luong, “Solving
olympiad geometry without human demonstrations,” Nature,
vol. 625, no. 7995, pp. 476–482, 2024.
[215] A.
Patil,
“Advancing
reasoning
in
large
language
mod-
els: Promising methods and approaches,” arXiv preprint
arXiv:2502.03671, 2025.
[216] OpenAI,
“GPT4
technical
report,”
arXiv
preprint
arXiv:2303.08774, 2023.
[217] R. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut,
J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, D. Sil-
ver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser,
A. Glaese, J. Chen, E. Pitler, T. P. Lillicrap, A. Lazaridou,
O. Firat, J. Molloy, M. Isard, P. R. Barham, T. Hennigan,
B. Lee, F. Viola, M. Reynolds, Y. Xu, R. Doherty, E. Collins,
C. Meyer, E. Rutherford, E. Moreira, K. Ayoub, M. Goel,
G. Tucker, E. Piqueras, M. Krikun, I. Barr, N. Savinov, I. Dani-
helka, B. Roelofs, A. White, A. Andreassen, T. von Glehn,
L. Yagati, M. Kazemi, L. Gonzalez, M. Khalman, J. Sygnowski,
and et al., “Gemini: A family of highly capable multimodal
models,” CoRR, vol. abs/2312.11805, 2023.
[218] DeepSeek-AI, “Deepseek LLM: scaling open-source language
models with longtermism,” CoRR, vol. abs/2401.02954, 2024.
[219] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. Jégou,
and T. Mikolov, “Fasttext. zip: Compressing text classiﬁcation
models,” arXiv preprint arXiv:1612.03651, 2016.
[220] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen,
X. Bi, Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang,
“Deepseek-coder: When the large language model meets pro-
gramming – the rise of code intelligence,” 2024.
[221] A.
Lewkowycz,
A.
Andreassen,
D.
Dohan,
E.
Dyer,
H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag,
T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and
V. Misra, “Solving quantitative reasoning problems with
language models,” arXiv preprint arXiv:2206.14858, 2022.
[222] T. Wei, J. Luan, W. Liu, S. Dong, and B. Wang, “Cmath:
Can your language model pass chinese elementary school math
test?,” 2023.
[223] W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang,
A. Saied, W. Chen, and N. Duan, “AGIEval: A human-
centric benchmark for evaluating foundation models,” CoRR,
vol. abs/2304.06364, 2023.
[224] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia,
E. H. Chi, Q. V. Le, and D. Zhou, “Chain-of-thought prompt-
ing elicits reasoning in large language models,” in NeurIPS,
2022.
[225] W. Chen, X. Ma, X. Wang, and W. W. Cohen, “Program of
thoughts prompting: Disentangling computation from reason-
ing for numerical reasoning tasks,” CoRR, vol. abs/2211.12588,
2022.
[226] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang,
J. Callan, and G. Neubig, “PAL: program-aided language mod-
els,” in International Conference on Machine Learning, ICML
2023, 23-29 July 2023, Honolulu, Hawaii, USA (A. Krause,
E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett,
eds.), vol. 202 of Proceedings of Machine Learning Research,
pp. 10764–10799, PMLR, 2023.
[227] Z. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, M. Huang,
N.
Duan,
and
W.
Chen,
“Tora:
A
tool-integrated
rea-
soning agent
for mathematical
problem solving,” CoRR,
vol. abs/2309.17452, 2023.
[228] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning,
and C. Finn, “Direct preference optimization: Your language
model is secretly a reward model,” 2023.
[229] J.
Schulman,
F.
Wolski, P.
Dhariwal, A.
Radford, and
O. Klimov, “Proximal policy optimization algorithms,” arXiv,
2017.
[230] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun,
L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano,
et al., “Training veriﬁers to solve math word problems,” arXiv
preprint arXiv:2110.14168, 2021.
[231] A.
Q.
Jiang,
A.
Sablayrolles,
A.
Mensch,
C.
Bamford,
D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel,


--- Page 32 ---
32
JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
G. Lample, L. Saulnier, et al., “Mistral 7b,” arXiv preprint
arXiv:2310.06825, 2023.
[232] Z. Azerbayev,
H. Schoelkopf, K. Paster, M. D. Santos,
S. McAleer, A. Q. Jiang, J. Deng, S. Biderman, and S. Welleck,
“Llemma: An open language model for mathematics,” arXiv
preprint arXiv:2310.10631, 2023.
[233] K. Zheng, J. M. Han, and S. Polu, “Minif2f: a cross-system
benchmark for formal olympiad-level mathematics,” arXiv
preprint arXiv:2109.00110, 2021.
[234] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika,
D. Song, and J. Steinhardt, “Measuring massive multitask
language understanding,” arXiv preprint arXiv:2009.03300,
2020.
[235] M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W.
Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al.,
“Challenging big-bench tasks and whether chain-of-thought
can solve them,” arXiv preprint arXiv:2210.09261, 2022.
[236] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto,
J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman,
A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sas-
try, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov,
A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P.
Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes,
A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,
J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders,
C. Hesse, A. N. Carr, J. Leike,
J. Achiam,
V. Misra,
E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Mu-
rati,
K.
Mayer,
P.
Welinder,
B.
McGrew,
D.
Amodei,
S. McCandlish, I. Sutskever, and W. Zaremba, “Evaluat-
ing large language models trained on code,” arXiv preprint
arXiv:2107.03374, 2021.
[237] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski,
D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al., “Pro-
gram synthesis with large language models,” arXiv preprint
arXiv:2108.07732, 2021.
[238] H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng,
Q. Lin, S. Chen, and D. Zhang, “Wizardmath: Empowering
mathematical reasoning for large language models via rein-
forced evol-instruct,” arXiv preprint arXiv:2308.09583, 2023.
[239] L. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok,
Z. Li, A. Weller, and W. Liu, “Metamath: Bootstrap your own
mathematical questions for large language models,” CoRR,
vol. abs/2309.12284, 2023.
[240] X. Yue, X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun,
Y. Su,
and
W.
Chen,
“Mammoth:
Building math
gen-
eralist models through hybrid instruction tuning,” CoRR,
vol. abs/2309.05653, 2023.
[241] T. Berg-Kirkpatrick and D. Spokoyny, “An empirical inves-
tigation of contextualized number prediction,” in EMNLP,
pp. 4754–4764, Nov. 2020.
[242] H. He, H. Zhang, and D. Roth, “Socreval: Large language
models with the socratic method for reference-free reasoning
evaluation,” arXiv, 2023.
[243] D. Huang, P. Dhariwal, D. Song, and I. Sutskever, “Gamepad:
A learning environment for theorem proving,” in ICLR, 2019.
[244] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,
Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale,
et al., “Llama 2: Open foundation and ﬁne-tuned chat models,”
arXiv, 2023.
[245] H. Wu, T. Hu, Y. Liu, H. Zhou, J. Wang, and M. Long, “Times-
Net: Temporal 2D-Variation Modeling for General Time Series
Analysis,” in ICLR, 2022.
[246] A. Garza and M. Mergenthaler-Canseco, “TimeGPT-1,” arXiv
preprint arXiv:2310.03589, 2023.
[247] X. Yu, Z. Chen, Y. Ling, S. Dong, Z. Liu, and Y. Lu, “Temporal
Data Meets LLM–Explainable Financial Time Series Forecast-
ing,” arXiv preprint arXiv:2306.11025, 2023.
[248] A. Lopez-Lira and Y. Tang, “Can ChatGPT Forecast Stock
Price Movements? Return Predictability and Large Language
Models,” arXiv preprint arXiv:2304.07619, 2023.
[249] J. Li, C. Liu, S. Cheng, R. Arcucci, and S. Hong, “Frozen
Language Model Helps ECG Zero-Shot Learning,” in MIDL,
2023.
[250] L. Y. Jiang, X. C. Liu, N. P. Nejatian, M. Nasir-Moin, D. Wang,
A. Abidin, et al., “Health system-scale language models are all-
purpose prediction engines,” Nature, pp. 1–6, 2023.
[251] C. Liu, S. Yang, Q. Xu, Z. Li, C. Long, Z. Li, and R. Zhao,
“Spatial-Temporal Large Language Model for Traﬃc Predic-
tion,” 2024.
[252] X. Wang, M. Fang, Z. Zeng, and T. Cheng, “Where Would I Go
Next? Large Language Models as Human Mobility Predictors,”
2023.
[253] T. Kim, J. Kim, Y. Tae, C. Park, J.-H. Choi, and J. Choo,
“Reversible Instance Normalization for Accurate Time-Series
Forecasting against Distribution Shift,” in ICLR, 2021.
[254] H. Xue and F. D. Salim, “PromptCast: A New Prompt-based
Learning Paradigm for Time Series Forecasting,” IEEE TKDE,
2023.
[255] N. Gruver, M. Finzi, S. Qiu, and A. G. Wilson, “Large Lan-
guage Models Are Zero Shot Time Series Forecasters,” in
NeurIPS, 2023.
[256] T. Zhou, P. Niu, X. Wang, L. Sun, and R. Jin, “One Fits All:
Power General Time Series Analysis by Pretrained LM,” in
NeurIPS, 2023.
[257] C. Chang, W.-C. Peng, and T.-F. Chen, “LLM4TS: Aligning
Pre-Trained LLMs as Data-Eﬃcient Time-Series Forecasters,”
arXiv preprint arXiv:2308.08469, 2023.
[258] D. Cao, F. Jia, S. O. Arik, T. Pﬁster, Y. Zheng, W. Ye,
and Y. Liu, “TEMPO: Prompt-based Generative Pre-trained
Transformer for Time Series Forecasting,” in ICLR, 2024.
[259] C. Sun, Y. Li, H. Li, and S. Hong, “TEST: Text Prototype
Aligned Embedding to Activate LLM’s Ability for Time Se-
ries,” in ICLR, 2024.
[260] M. Jin, S. Wang, L. Ma, Z. Chu, J. Y. Zhang, X. Shi, P.-
Y. Chen, Y. Liang, Y.-F. Li, S. Pan, et al., “Time-LLM:
Time Series Forecasting by Reprogramming Large Language
Models,” in ICLR, 2024.
[261] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,
et al., “LoRA: Low-Rank Adaptation of Large Language Mod-
els,” in ICLR, 2021.
[262] X. Shi, S. Xue, K. Wang, F. Zhou, J. Y. Zhang, J. Zhou, C. Tan,
and H. Mei, “Language Models Can Improve Event Prediction
by Few-Shot Abductive Reasoning,” in NeurIPS, 2023.
[263] H. Xu, Y. Gao, Z. Hui, J. Li, and X. Gao, “Language
Knowledge-Assisted Representation Learning for Skeleton-
Based Action Recognition,” arXiv preprint arXiv:2305.12398,
2023.
[264] K.
Rasul,
A.
Ashok,
A.
R.
Williams,
A.
Khorasani,
G. Adamopoulos, R. Bhagwatkar, et al., “Lag-Llama: Towards
Foundation Models for Time Series Forecasting,” 2023.
[265] C. Mai, Y. Chang, C. Chen, and Z. Zheng, “Enhanced Scalable
Graph Neural Network via Knowledge Distillation,” IEEE
TNNLS, 2023.
[266] H. Xue, B. P. Voutharoja, and F. D. Salim, “Leveraging Lan-
guage Foundation Models for Human Mobility Forecasting,” in
SIGSPATIAL, 2022.
[267] Y. Chen, X. Wang, and G. Xu, “GATGPT: A Pre-trained
Large Language Model with Graph Attention Network for Spa-
tiotemporal Imputation,” arXiv preprint arXiv:2311.14332,
2023.
[268] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya,
F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman,
S. Anadkat, et al., “Gpt-4 technical report,” arXiv preprint
arXiv:2303.08774, 2023.
[269] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle,
A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan,
et
al.,
“The
llama
3
herd
of
models,”
arXiv
preprint
arXiv:2407.21783, 2024.
[270] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu,
S. Ma, P. Wang, X. Bi, et al., “Deepseek-r1: Incentivizing
reasoning capability in llms via reinforcement learning,” arXiv
preprint arXiv:2501.12948, 2025.
[271] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary,
C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna,
F. Bressand, et al., “Mixtral of experts,” arXiv preprint
arXiv:2401.04088, 2024.
[272] A. Anthropic, “Introducing the next generation of claude,”
2024.
[273] G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati,
G. Tanzer, D. Vincent, Z. Pan, S. Wang, et al., “Gemini 1.5:
Unlocking multimodal understanding across millions of tokens
of context,” arXiv preprint arXiv:2403.05530, 2024.


--- Page 33 ---
FIRST A. AUTHOR et al.: BARE DEMO OF IEEETAI.CLS FOR IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE33
[274] C. R. Harris, K. J. Millman, S. J. Van Der Walt, R. Gommers,
P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg,
N. J. Smith, et al., “Array programming with numpy,” Nature,
vol. 585, no. 7825, pp. 357–362, 2020.
[275] W. McKinney et al., “Data structures for statistical computing
in python.,” SciPy, vol. 445, no. 1, pp. 51–56, 2010.
[276] J. D. Hunter, “Matplotlib: A 2d graphics environment,” Com-
puting in science & engineering, vol. 9, no. 03, pp. 90–95, 2007.
[277] F.
Pedregosa,
G.
Varoquaux,
A.
Gramfort,
V.
Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, et al., “Scikit-learn: Machine learning in python,”
the Journal of machine Learning research, vol. 12, pp. 2825–
2830, 2011.
[278] U. Met Oﬃce, “Cartopy: A cartographic python library with
a matplotlib interface,” Exeter, Devon, 2010.
[279] S. Alemany Ibor, Diseño e implementación de un simulador
basado en agentes estilo JGOMAS en Python.
PhD thesis,
Universitat Politècnica de València, 2018.
[280] S. Mitchell, M. OSullivan, and I. Dunning, “Pulp: a linear
programming toolkit for python,” The University of Auckland,
Auckland, New Zealand, vol. 65, p. 25, 2011.
[281] Q. Wu, G. Bansal, J. Zhang, Y. Wu, S. Zhang, E. Zhu, B. Li,
L. Jiang, X. Zhang, and C. Wang, “Autogen: Enabling next-
gen llm applications via multi-agent conversation framework,”
arXiv preprint arXiv:2308.08155, 2023.
[282] A. Forootani, D. E. Aliabadi, and D. Thraen, “Climate aware
deep neural networks (cadnn) for wind power simulation,”
arXiv preprint arXiv:2412.12160, 2024.
[283] S. Zhang, J. Zhang, J. Liu, L. Song, C. Wang, R. Krishna,
and Q. Wu, “Oﬄine training of language model agents with
functions as learnable weights,” in Forty-ﬁrst International
Conference on Machine Learning.
[284] V. Rawte, A. Sheth, and A. Das, “A survey of hallucination in
large foundation models,” arXiv, 2023.
[285] H. He, H. Zhang, and D. Roth, “Rethinking with retrieval:
Faithful large language model inference,” arXiv, 2022.
[286] A. Zhou, K. Wang, Z. Lu, W. Shi, S. Luo, Z. Qin, S. Lu,
A. Jia, L. Song, M. Zhan, et al., “Solving challenging math
word problems using gpt-4 code interpreter with code-based
self-veriﬁcation,” arXiv, 2023.
[287] Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J.-G. Lou, and
W. Chen, “Making language models better reasoners with step-
aware veriﬁer,” in ACL, pp. 5315–5333, 2023.
[288] R. Zhao, X. Li, S. Joty, C. Qin, and L. Bing, “Verify-and-edit:
A knowledge-enhanced chain-of-thought framework,” arXiv,
2023.
[289] K. Shridhar, H. Jhamtani, H. Fang, B. Van Durme, J. Eisner,
and P. Xia, “Screws: A modular framework for reasoning with
revisions,” arXiv, 2023.
[290] H. Gelernter, J. R. Hansen, and D. W. Loveland, “Empiri-
cal explorations of the geometry theorem machine,” in west-
ern joint IRE-AIEE-ACM computer conference, pp. 143–149,
1960.
[291] P. Lu, L. Qiu, J. Chen, T. Xia, Y. Zhao, W. Zhang, Z. Yu,
X. Liang, and S.-C. Zhu, “Iconqa: A new benchmark for ab-
stract diagram understanding and visual language reasoning,”
in NeurIPS Datasets and Benchmarks Track, 2021.
[292] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L.
Zitnick, and D. Parikh, “Vqa: Visual question answering,” in
ICCV, pp. 2425–2433, 2015.
[293] M. Z. Hossain, F. Sohel, M. F. Shiratuddin, and H. Laga, “A
comprehensive survey of deep learning for image captioning,”
ACM Computing Surveys (CsUR), vol. 51, no. 6, pp. 1–36,
2019.
[294] J. Gawlikowski, C. R. N. Tassi, M. Ali, J. Lee, M. Humt,
J. Feng, A. Kruspe, R. Triebel, P. Jung, R. Roscher, et al.,
“A survey of uncertainty in deep neural networks,” Artiﬁcial
Intelligence Review, vol. 56, no. Suppl 1, pp. 1513–1589, 2023.
[295] J. Duan, H. Cheng, S. Wang, C. Wang, A. Zavalny, R. Xu,
B. Kailkhura, and K. Xu, “Shifting attention to relevance:
Towards the uncertainty estimation of large language models,”
arXiv, 2023.
[296] W. Zhou, Y. E. Jiang, E. Wilcox, R. Cotterell, and M. Sachan,
“Controlled text generation with natural language instruc-
tions,” in ICML, vol. 202, pp. 42602–42613, 2023.
[297] B. Wang and A. Komatsuzaki, “Gpt-j-6b: A 6 billion parameter
autoregressive language model,” 2021.
[298] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus,
Y. Li, X. Wang, M. Dehghani, S. Brahma, et al., “Scaling
instruction-ﬁnetuned language models,” arXiv, 2022.
[299] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kul-
shreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al.,
“Lamda: Language models for dialog applications,” arXiv,
2022.
[300] A. Davies, P. Veličković, L. Buesing, S. Blackwell, D. Zheng,
N. Tomašev, R. Tanburn, P. Battaglia, C. Blundell, A. Juhász,
et al., “Advancing mathematics by guiding human intuition
with ai,” Nature, vol. 600, no. 7887, pp. 70–74, 2021.
[301] A.
N.
Bernardino
Romera-Paredes,
Moham-
madamin
Barekatain
et
al.,
“Mathematical
discoveries
from program search with large language models,” Nature,
2023.
[302] K. Shridhar, J. Macina, M. El-Assady, T. Sinha, M. Kapur, and
M. Sachan, “Automatic generation of socratic subquestions for
teaching math word problems,” arXiv, 2022.
[303] N. Ding, Y. Chen, B. Xu, Y. Qin, S. Hu, Z. Liu, M. Sun, and
B. Zhou, “Enhancing chat language models by scaling high-
quality instructional conversations,” in EMNLP, pp. 3029–
3051, 2023.
