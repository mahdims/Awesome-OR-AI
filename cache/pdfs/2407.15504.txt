--- Page 1 ---
Fundamental Limits of Prompt Compression:
A Rate-Distortion Framework for Black-Box Language Models
Alliot Nagle∗
UT Austin
Adway Girish∗
EPFL
Marco Bondaschi
EPFL
Michael Gastpar
EPFL
Ashok Vardhan Makkuva†
EPFL
Hyeji Kim†
UT Austin
December 12, 2024
Abstract
We formalize the problem of prompt compression for large language models (LLMs) and
present a framework to unify token-level prompt compression methods which create hard prompts
for black-box models. We derive the distortion-rate function for this setup as a linear program,
and provide an efficient algorithm to compute this fundamental limit via the dual of the linear
program. Using the distortion-rate function as the baseline, we study the performance of existing
compression schemes on a synthetic dataset consisting of prompts generated from a Markov chain,
natural language queries, and their respective answers. Our empirical analysis demonstrates
the criticality of query-aware prompt compression, where the compressor has knowledge of the
downstream task/query for the black-box LLM. We show that there is a large gap between the
performance of current prompt compression methods and the optimal strategy, and propose
Adaptive QuerySelect, a query-aware, variable-rate adaptation of a prior work to close the gap.
We extend our experiments to a small natural language dataset to further confirm our findings
on our synthetic dataset.
∗Equal contribution. †Equal contribution.
1
arXiv:2407.15504v2  [cs.LG]  11 Dec 2024


--- Page 2 ---
Contents
1
Introduction
3
2
Background and related works
4
3
Distortion-rate function for prompt compression
6
3.1
A formal model for prompt compression . . . . . . . . . . . . . . . . . . . . . . . . .
6
3.2
Rate-distortion formulation for query-agnostic prompt compression . . . . . . . . . .
7
3.3
Linear program formulation of the distortion-rate function . . . . . . . . . . . . . . .
8
3.4
Extension to query-aware prompt compression . . . . . . . . . . . . . . . . . . . . . .
10
4
Experiments
13
4.1
Experimental setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
4.2
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
4.2.1
Synthetic dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
4.2.2
Small natural language dataset . . . . . . . . . . . . . . . . . . . . . . . . . .
17
4.2.3
Beam search for large-scale natural language dataset . . . . . . . . . . . . . .
19
5
Discussion and conclusion
20
A The dual linear program: proof and solution
25
A.1 Derivation of the dual linear program . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
A.2 Proof and illustration of Algorithm 1 . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
B Connections to information theory literature
29
C Experiment details
30
C.1 Synthetic data experiments
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
C.1.1
Synthetic dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
C.2 Natural language experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
C.2.1
Small natural language dataset . . . . . . . . . . . . . . . . . . . . . . . . . .
31
C.2.2
Choice of distortion metric
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
C.3 LLM fine-tuning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
C.3.1
Synthetic dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
C.3.2
Natural language dataset
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
D Additional experimental results
34
D.1 Small-scale datasets
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
D.2 NarrativeQA
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
2


--- Page 3 ---
1
Introduction
In spite of the recent success of transformer-based [VSP+17] large language models (LLMs) in
language modeling tasks, inference calls to a transformer can be costly in both time and memory
usage. Although significant progress has been made to improve the memory usage and runtime
efficiency via implementation-level optimizations [KLZ+23; DFERR22; Dao23] and architecture-
level optimizations and alternatives [WLKFM20; PAA+23; GD23], a third type of optimization
that compresses the input (an input-level optimization) has the benefit that it directly reduces the
resource usage of an LLM inference call, and it can be used in conjunction with the other two types
of optimizations for further efficiency gains. In this work, we offer a framework and analysis for a
recent body of literature in this direction, known as prompt compression [ABC+21; SKZ22; WSS22].
The goal of a prompt compression method is to transform a sequence of input tokens x into a
shorter sequence of tokens m such that the response generated by a target LLM will semantically
mean the same thing regardless of whether x or m is given as input. Using m as the input directly
decreases the memory and runtime requirements necessary for an LLM inference call. Moreover, the
additional benefits to this approach are: (1) redundant or superfluous tokens are removed, making
room to fit more pertinent information in the target LLM’s limited-size context window, (2) it can
be used in addition to implementation and architecture-level optimizations to get further efficiency
gains, and (3) it is the only technique available when seeking to lower costs for black-box API calls
to closed-source models. This third point is particularly important, since the associated cost for a
black-box API model inference call, from the perspective of the caller, is determined by the runtime
and the number of input tokens, both which can be reduced with prompt compression. In our
framework and analysis, we focus on the prompt compression for black-box models setting, where the
output of a prompt compression method is a set of tokens (“hard prompts”) [LDLG23; JWLYQ23;
JWL+23; PWJ+24], and exclude methods which output embedding vectors (“soft prompts”) [MLG23;
GJW+24; CWAC23] as those are not transferable to black-box models.
Despite the progress made in the prompt compression literature, there is a lack of proper
formalization of this problem and there is no clear framework to unify these works. Further, most
works propose methods that work well but offer no insight into key questions, such as “How far are we
from the theoretical limit of the rate-distortion trade-off?”, “How essential is the conditioning on the
query when compressing the prompt?”, and “How does tokenization impact the performance of prompt
compression methods?” We offer a unifying framework for the problem of prompt compression and
seek to answer these questions with theory and experimental results. Our main contributions can be
summarized as follows.
1. Theoretical analysis: We formalize the problem of prompt compression and formulate it as
a rate-distortion problem (Sec. 3.1). We characterize the optimal trade-off between the rate
of compression and the distortion incurred, i.e., the distortion-rate function, via a dual linear
program, and provide a geometric algorithm to compute this optimal trade-off (Sec. 3.2, Sec. 3.3).
2. Evaluation: We introduce a synthetic dataset with binary prompts and natural language queries,
for which we can compute the distortion-rate function (Sec. 4.1), and compare and obtain insights
on existing prompt compression algorithms as in Fig. 1 (Sec. 4.2). We further confirm our findings
by extending our experiments to a small natural language dataset and NarrativeQA [KSB+17].
3. Algorithm design: Our novel method, “Adaptive QuerySelect,” a query-aware, variable-rate
adaptation of LLMLingua-2 [PWJ+24], outperforms all prompt compression methods on our
datasets and has a rate-distortion curve that significantly reduces the gap with the theoretical
limit (Sec. 4).
3


--- Page 4 ---
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0
1
2
3
4
Average distortion
Log loss
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0.0
0.1
0.2
0.3
0.4
0.5
0/1 loss
Selective [LDLG23]
LLMLingua [JWLYQ23]
LLMLingua Query [JWL+23]
LLMLingua-2 [PWJ+24]
QuerySelect
Adaptive QuerySelect
Optimal (Query-agnostic)
Optimal (Query-aware)
No Compression
Figure 1: The distortion-rate trade-off of all prompt compression methods compared to the query-
aware and query-agnostic theoretical limits on a synthetic dataset with binary prompts. All distortions
are computed with the log loss (left) and 0/1 loss (right) distortion metrics formally defined
in (1). We observe that (1) most existing methods are far from the theoretical limit, suggesting
that there is still room for improvement in this field, (2) conditioning on the query allows for a
significant improvement, as seen by the performance of the query-aware method QuerySelect against
the query-agnostic LLMLingua-2 [PWJ+24], and (3) our proposed method Adaptive QuerySelect, a
query-aware and variable-rate adaptation of LLMLingua-2, achieves the best performance among all
methods considered, and is the only method to outperform the optimal query-agnostic strategy.
2
Background and related works
Long prompts slow the inference process due to the increase in the number of tokens that the LLM
has to process. It is also known that very long prompts can cause LLMs to ignore or “forget” parts
of the input and produce erroneous answers [LLH+23]. Therefore, studying how these prompts
can be compressed is essential. As illustrated in Fig. 2, we wish to design a compressor that, upon
receiving the prompt, produces a “compressed” version (in the sense that it should have fewer tokens
than the prompt) called the compressed prompt, such that a target LLM is able to give answers that
are “close enough,” in the sense of some appropriately chosen metric, to the ground truth. Though
similar in spirit to text summarization, prompt compression has the advantage that the compressed
prompt is not required to be human-readable.
All prompt compression methods belong to one of two groups: those that compress the prompt
into soft prompts and those that compress the prompt into hard prompts. In soft-prompt compression,
the compressor is trained to transform the input prompt into a set of embedding vectors (sometimes
referred to as “soft tokens”) that do not map back into the token space. These methods, including Gist
Tokens [MLG23], AutoCompressor [CWAC23], and In-Context Auto-Encoder [GJW+24] are trained
end-to-end and require specialized fine-tuning of the target LLM to interpret the soft prompt inputs.
In this work, we focus instead on methods that compress the prompt into hard prompts, where
the compressor’s output is a set of tokens. While it is technically feasible to fine-tune the target LLM
in this setting, it is unnecessary and often avoided because the utility of this setting is compressing
prompts for black-box models that are not fine-tuned. These methods often use either the target
LLM, or a smaller and faster LLM, to compress the prompt. The basic idea behind all these methods
is to identify the tokens that are “most relevant,” per an appropriate metric, and retain as many of
them in the compressed prompt as possible. These methods include Selective Context [LDLG23],
4


--- Page 5 ---
LLM
x
P ˆY = ϕLLM(x, q)
q
It was the best of times, it was the worst of
times, it was the age of wisdom, it was the age
of foolishness, it was the epoch of belief, it was
the epoch of incredulity, it was the season of
light, it was the season of darkness, it was the
spring of hope, it was the winter of despair.
Prompt
How were the times?
Query
Best and worst. (60%)
Contrasting.
(20%)
Mixed.
(10%)
Dualistic.
(5%)
. . .
Output
(a) Black-box LLM without prompt compression.
comp
LLM
x
m
P ˆY = ϕLLM(m, q)
q
best times worst, age wisdom
foolish, epoch belief incredul,
season light dark, hope despair.
Compressed prompt (query-agnostic)
(b) Query-agnostic prompt compression
comp
LLM
x
m
P ˆY = ϕLLM(m, q)
q
best worst.
Compressed prompt (query-aware)
(c) Query-aware prompt compression
Figure 2: Model for prompt compression in LLMs.
(a): Without prompt compression, the LLM
takes a long Prompt and Query as input, and produces an Output distribution.
(b) and (c): The
prompt is passed through a compressor to obtain a shorter Compressed prompt and the LLM takes
this compressed prompt and query as input instead.
(b) The compressor does not have access to
the query, and preserves all highlighted tokens.
(c) The compressor has access to the query, and
preserves only the tokens highlighted in orange.
LLMLingua [JWLYQ23], LLMLingua-2 [PWJ+24], and LongLLMLingua [JWL+23]. More details
on these works can be found in Sec. 4.2. Precursors to the prompt compression works include text
compression methods, which have the added constraint that the compressed text is human-readable
[NXS19; SMLVM20; GHI22]. Prompt compression methods are different from these in that the text
only needs to be interpretable by the target LLM, not by a human.
We offer a framework for hard-prompt compression methods where we assume that a query is
provided in addition to the compressed prompt during the target LLM inference call. Functionally,
this is the most useful interpretation of prompt compression since it clarifies that the goal is to
compress the prompt for a given query/task. This setting is also used in the LLMLingua and
LongLLMLingua works, and is slightly more general than the setting where no query is used (in
that case, the query can be empty).
Notation
General.
We use ≜to signify a definition. For a set X, with xi ∈X for i = 1, . . . , n, we
represent the sequence (x1, . . . , xn) by xn ∈X n, which is short for X × · · · × X. We use X ∗to
denote S
n≥1 X n, the set of all nonempty finite-length sequences on X. In general, for y ∈X n,
we denote the length of y by len(y) = n.
We denote the cardinality of a set X by |X|.
We
use R+ to denote the set of nonnegative real numbers. For a set X = {x1, . . . , xk}, we use the
boldface (Ax)x∈X to denote the vector (Ax1, . . . , Axk) indexed by elements of X. We also write
RX
+ = {(vx)x∈X : vx ∈R+ for each x ∈X}. We use 1 to denote the indicator function, which takes
the value 1 when its argument is true and 0 otherwise. The infimum and supremum of a set of
5


--- Page 6 ---
values is denoted using inf and sup respectively. We use 0 and 1 to denote the vector of appropriate
dimension with all elements equal to 0 and 1 respectively.
Probability.
We deal with discrete probability distributions on finite sets, for which we use
calligraphic letters to denote the set (e.g. X), uppercase letters to denote the random variable
(r.v., e.g. X) and lowercase letters to denote samples of the r.v. (e.g. x). The set of all probability
distributions on the set X is denoted by P(X). The probability distribution of the r.v. X on X
is denoted by PX ∈P(X), and we say X ∼PX. For a (measurable) function f, the expectation
of the r.v. f(X) is denoted by EX∼PX [f(X)], or E [f(X)], with the subscripts dropped when the
distribution and/or r.v.’s are clear from context. The degenerate probability distribution with mass
1 at x ∈X is represented by δx ∈P(X). Conditional probabilities from X to Y are denoted as PY |X,
and for each x ∈X, we denote the distribution on Y as PY |X(·|x).
Problem-setup-specific notation.
In our model, we use V to refer to the vocabulary of the
prompt. We use the uppercase letters X to refer to the prompt, M to refer to the compressed
prompt, Q to refer to the query and Y to refer to the answer as random variables (and corresponding
calligraphic and lowercase letters to denote the set and samples of the r.v. respectively). We use P ˆY to
refer to the output distribution of the LLM, which is modeled as the function ϕLLM. For a given prompt
x, we use Mx to refer to the set of possible compressed prompts, which is the set of all sequences of
length smaller than len(x). To denote the distortion measure, we use d, which can be the log loss
dlog, the 0/1 loss d0/1, or a semantic distortion measure. We denote the query-agnostic distortion-
rate function at rate R by D∗(R). The average query-aware distortion-rate function is denoted by
¯D∗(R), and the conditional query-aware distortion-rate function for query q ∈Q is given by D∗
q(R).
3
Distortion-rate function for prompt compression
We first formalize the problem of prompt compression, and then develop a rate-distortion framework
to study its fundamental limits. In particular, we define and characterize the distortion-rate function,
which describes the optimal trade-off between how much and how well the prompt is compressed.
3.1
A formal model for prompt compression
Black-box LLM.
As depicted in Fig. 2a, we assume that we have a pretrained LLM which takes a
pair of the prompt x ∈Vnx and the query q ∈Vnq, (x, q) ∈Vnx+nq as inputs, where V refers to the
vocabulary of the LLM (i.e., the set of all tokens), and nx and nq are the lengths of the prompt and
query respectively. The output of the LLM is given by P ˆY = ϕLLM(x, q) ∈P(V∗), where ϕLLM : V∗→
P(V∗) is a deterministic function which maps a sequence of tokens to a probability distribution on
sequences of tokens. We denote the set of all prompts x by X and the set of all queries q by Q. Clearly,
they are both equal to V∗, but this notation is useful in the subsequent discussion. We model prompt-
query pairs (X, Q) as random variables drawn according to the joint distribution PXQ ∈P(X × Q).
In cases where we have a correct answer y ∈Y = V∗corresponding to the pair (x, q), we
characterize the “closeness” of the LLM output P ˆY = ϕLLM(x, q) to the answer y using a distortion
measure d : Y × P(Y) →[0, ∞]. Two possible choices of d are the log loss dlog and the 0/1 loss d0/1,
given by
dlog(y, P ˆY ) = log
1
P ˆY (y)
and
d0/1(y, P ˆY ) = 1
n
y ̸= argmax
ˆy
P ˆY (ˆy)
o
.
(1)
6


--- Page 7 ---
These are respectively the cross-entropy loss between the distributions δy and P ˆY , and the prediction
error. When dealing with natural language queries, a semantic distortion metric such as RougeL
[Lin04] or BertScore [Ber71] is a more approrpiate choice. Additionally, there is no single answer
that is uniquely correct. To account for this variability in what qualifies as a correct answer, we
model the answer as a random variable Y drawn from the distribution PY |XQ(·|x, q), which depends
on the prompt x and the query q. This induces the joint distribution PXQY = PXQPY |XQ. We then
characterize the “closeness” between the correct answer and the LLM output by the average distortion,
given by EY ∼PY |XQ(·|x,q)

d
 Y, ϕLLM(x, q)

. With d = dlog, this is the cross-entropy loss between
the distributions PY |XQ(·|x, q) and P ˆY = ϕLLM(x, q), and with d = d0/1, this is the prediction error
probability.
Prompt compression.
As described in Sec. 2, we consider two types of prompt compression:
query-agnostic and query-aware. Fig. 2b depicts query-agnostic prompt compression, where the goal
is to design a compressor denoted by comp as a possibly random function from X to M, i.e., the set
of all compressed prompts. The compressor takes in the prompt X ∼PX and produces a compressed
prompt M = comp(X) with len(M) ≤len(X). Then, the user replaces the original prompt X with
the compressed prompt M and provides the LLM with the input (M, Q). The output distribution
produced by the LLM is P ˆY = ϕLLM(M, Q). To quantify the performance of this compressor comp,
two quantities are of interest:
(1) the rate E
h
len(M)
len(X)
i
, to measure how much the prompt is compressed, and
(2) the distortion E

d
 Y, ϕLLM(M, Q)

, to measure how well the prompt is compressed,
where both expectations are taken with respect to the joint distribution PMXQY . If we compress x
to a very low rate, it is likely that the compressed prompt m may not retain all of the information
in x that is necessary for the query q, leading to an output ϕLLM(m, q) that is very different from
PY |XQ(·|x, q) and consequently, a high distortion. Thus, there is a trade-off between these quantities,
and we formalize and study this trade-off via the distortion-rate function in Sec. 3.2.
We can also model query-aware prompt compression similarly, with the difference being that the
compressor also has access to the query q ∈Q, as shown in Fig. 2c. In addition to the average rate
and distortion computed over all queries, it is also interesting to consider the rate and distortion for
each query. To simplify the presentation, we first focus on the query-agnostic setting.
3.2
Rate-distortion formulation for query-agnostic prompt compression
Distortion-rate function D∗(R).
The distortion-rate function for any compression problem
characterizes the fundamental trade-off between the distortion and the rate [Sha59; Ber71; CT06;
EK11]. We say that the pair (R, D) is achievable if there exists a compressor with rate at most R
and distortion at most D. For a given rate R, the distortion-rate function D∗(R) is the smallest
distortion that can be achieved by a compressor with rate at most R. Formally, it is defined as
D∗(R) ≜inf{D ≥0 | (R, D) is achievable}
= inf{D ≥0 | there exists a compressor with rate ≤R and distortion ≤D}.
(2)
We are now ready to characterize the distortion-rate function for prompt compression.
7


--- Page 8 ---
D∗(R) for query-agnostic prompt compression.
Recall that our quantities of interest are the
rate E
h
len(M)
len(X)
i
, and the distortion E

d
 Y, ϕLLM(M, Q)

, with both expectations taken with respect
to PMXQY , where M = comp(X) for a random function comp. By the functional representation
lemma [EK11; HP79], a random function from X to M is equivalent to a conditional distribution
PM|X. Thus, we can equivalently model the compressor as a conditional distribution PM|X, and (2)
is explicitly written as
D∗(R) =
inf
PM|X
E

d
 Y, ϕLLM(M, Q)

s.t.
PM|X is a compressor, and
E
len(M)
len(X)

≤R,
(3)
with both expectations taken with respect to the joint distribution PMXQY = PM|XPXQY induced by
the compressor PM|X. The constraint “PM|X is a compressor” is short for the following requirements:
(1) it is a conditional distribution, i.e., for each x ∈X, P
m∈M PM|X(m|x) = 1, (2) if len(m) > len(x),
then PM|X(m|x) = 0, and (3) if len(m) = len(x), then PM|X(m|x) = 0 unless m = x. This means
that the compressor either strictly reduces the length of the prompt or does no compression and
retains the original prompt.
Note that all of the expressions in the objective and the constraints in (3) are linear in PM|X.
Hence, the optimization problem is simply a linear program, which is simple from an optimization
perspective [BV04; Dan02]. However, the dimensions of this problem are still large and solving
the linear program directly quickly becomes infeasible as the lengths of the prompts increase. In
Sec. 3.3, we deal with this optimization problem directly, and show that the dual of the linear
program provides an exact, practically realizable solution.
Connections to information-theoretic setups.
We provide a brief overview of rate-distortion
theory from the information theory literature in App. B and describe how our model compares. In
particular, we note that our model for prompt compression closely resembles the setup of compression
with side-information for function computation [WZ76; Wyn78; WE06; Yam82], where both the
encoder and the decoder are part of the system design. More recently, there has also been a growing
interest in computing the distortion-rate functions of these classical setups for real-world datasets
[LHS22; YENM23; KKV24]. However, in our model for prompt compression, only the encoder (which
is the compressor) can be designed, hence our model is one of compression for a fixed decoder. Such
a model has not been actively studied in the information theory literature before, but in the next
subsection, we show that the distortion-rate function can be written as an explicit linear program in
terms of this fixed decoder.
3.3
Linear program formulation of the distortion-rate function
Having expressed the distortion-rate function for prompt compression as a linear program, we now
focus on obtaining solutions to these linear programs. We first rewrite (3) as an explicit linear program
using optimization-theoretic notation, and hide the probabilistic notation involving expectations and
conditional probabilities in the parameters of the linear program, which are defined below.
Proposition 1 (Primal LP). The distortion-rate function for query-agnostic prompt compression
8


--- Page 9 ---
(3) is given by the solution to the linear program
D∗(R) =
inf
(zx∈RMx
+ )x∈X
X
x∈X
D⊤
x zx
s.t.
X
x∈X
R⊤
x zx ≤R,
1⊤zx = 1,
∀x ∈X,
(LP)
where for each x ∈X, Mx denotes the set of compressed prompts associated to x, i.e., the set of all
possible token sequences of length smaller than len(x), the vectors zx ∈RMx
+
are the optimization
variables and the constants Dx, Rx ∈RMx
+
with components indexed by m ∈Mx are given by
Dx,m ≜PX(x) E [d(Y, ϕLLM(m, Q))]
and
Rx,m ≜PX(x) len(m)
len(x) ,
m ∈Mx,
(4)
with the expectation taken with respect to PQY |MX(·, ·|m, x).
Proof. This follows immediately from (3) by defining the constants Dx, Rx ∈RMx
+
for each x ∈X
as given in (4), and taking zx to be PM|X(·|x).
We use the fact that PM|X(m|x) = 0 when
len(m) > len(x) to reduce the dimension of zx from M to Mx to obtain (LP).
For our experimental setup in Sec. 4.2, we see that dimension of the linear program is too large
to solve (LP) directly using off-the-shelf solvers. Fortunately, the dual of the linear program can be
written more concisely, and can also be solved using a relatively simple algorithm.
Theorem 1 (Dual LP). The distortion-rate function for query-agnostic prompt compression (3) is
given by the solution to the dual of the linear program (LP), i.e.,
D∗(R) = sup
λ≥0
(
−λR +
X
x∈X
min
m∈Mx [Dx,m + λRx,m]
)
.
(dual-LP)
Proof sketch. This follows by taking the dual [BV04] of the linear program (LP) and simplifying the
resulting expression. For a complete proof, refer to App. A.1.
Algorithm to solve (dual-LP).
While the optimization problem in (dual-LP) may seem difficult
to solve, with its max-min structure and the supremum over the continuous variable λ, it provides
a neat geometric interpretation which allows for a computationally simple algorithm, as given in
Algorithm 1. It takes as input R, (Dx)x∈X , (Rx)x∈X , and returns as output the distortion-rate
function at R, i.e., D∗(R). In App. A.2, we show that the output is indeed D∗(R), and also provide
a step-by-step illustration of the algorithm on an artificial example. A high-level description of the
algorithm is given below.
Before presenting the algorithm, it is useful to define the following geometric object. The lower-
left convex envelope of a set of points in R2
+ is the largest convex function that lies below and to
the left of the points, as shown in Fig. 3 for the points {(Rx,m, Dx,m)}m∈Mx for a fixed x ∈X. Let
kx be the number of points on this envelope, then these kx points are exactly the minimizers of
minm∈Mx [Dx,m + λRx,m] for some λ ≥0. Solving this inner minimization problem of (dual-LP)
is thus easy, and amounts to simply finding the points labelled as “m(x)” on the lower-left convex
envelope, ordered from left to right, as done in Lines 3–4 of Algorithm 1. Let the magnitudes of slopes
of the line segments on the envelope be given by the “λ(x)” terms in decreasing order (Lines 5–6).
Also letting λ(x)
0
= +∞and λ(x)
kx = 0, observe that for i = 1, . . . , kx, m(x)
i
minimizes Dx,m + λRx,m
9


--- Page 10 ---
over m for λ ∈

λ(x)
i
, λ(x)
i−1

. Importantly, it is enough to consider just these sequences “m(x)” and
“λ(x)” sequences computed for all x ∈X (Lines 2–6) to solve (dual-LP), instead of the entire set Mx
and the continuum of all positive real numbers λ respectively. This makes the problem tractable
even for large values of |Mx|.
Rx,m
Dx,m
0.1
0.1
0.2
0.2
0.3
0.3
0.4
0.4
−λ(x)
1
−λ(x)
2
m(x)
1
m(x)
2
m(x)
3
Figure 3: Geometric intuition for solving (dual-LP): lower-left convex envelope for an example
{(Rx,m, Dx,m)}m∈Mx for a fixed x with |Mx| = 11, kx = 3.
The rest of the algorithm computes the outer supremum. Lines 7–9 prepare for this by introducing
new notation “ em(x)” and “eλ” such that for λ ∈
eλj, eλj−1

, em(x)
j
minimizes Dx,m + λRx,m over
m ∈Mx. There is no calculation involved in this step; what we gain is that the range of λ on which
the minimizer is em(x)
j
no longer depends on x. This gives us everything we need to compute the
distortion-rate function, which is obtained by lines 10–13. Observe that the input R is only used for
lines 10–13, so for a given dataset, lines 1–9 can be run once and the results “ em(x)” and “eλ” stored,
with only lines 10–13 run for each value of R.
3.4
Extension to query-aware prompt compression
As mentioned in Sec. 3.2 and Sec. 3.3, analogous definitions and results can be obtained for the
query-aware setting as well. The difference is that the compressor has access to the query in addition
to the prompt. Thus, the compressor comp is a possibly random function from X × Q to M. For
the query Q, the compressor maps the prompt X to the compressed prompt M = comp(X, Q) with
len(M) ≤len(X). The user provides the input [M, Q] to the LLM, which produces the output
distribution P ˆY = ϕLLM(M, Q). Just as in the query-agnostic setting, two quantities of interest are
(1) the (average) rate E
h
len(M)
len(X)
i
,
and
(2) the distortion E [dlog(Y, ϕLLM(M, Q))],
with both expectations taken with respect to the joint distribution PMXQY . Since different queries
may require different amounts of information to be preserved during compression, it is also of
interest to define the (conditional) rate and distortion for the specific query q as E
h
len(M)
len(X)
i
and
E [dlog(Y, ϕLLM(M, q))] respectively, with both expectations taken with respect to the joint distribu-
tion PMXY |Q(·|q).
The rate-distortion problem for query-aware prompt compression can be also formulated similarly
to (3). We model comp as a random mapping PM|XQ from X × Q to M. Then, the (average)
distortion-rate function at rate R is the smallest distortion that can be achieved by a query-aware
10


--- Page 11 ---
Algorithm 1: To compute the distortion-rate function via the dual linear program (dual-LP)
1 Input: R, (Dx)x∈X , (Rx)x∈X ;
Output: D∗(R), the distortion-rate function at rate R;
2 for x ∈X do
3
Find M(x)
env ⊆Mx such that {(Rx,m, Dx,m)}m∈M(x)
env are on the lower-left convex boundary
of {(Rx,m, Dx,m)}m∈Mx ;
▷see Fig. 3 for an example
4
n
m(x)
1 , m(x)
2 , . . . , m(x)
kx
o
←M(x)
env ordered such that Rx,m(x)
kx
> · · · > Rx,m(x)
1 ;
5
for i = 1, . . . , kx −1 do λ(x)
i
←
D
x,m(x)
i
−D
x,m(x)
i+1
R
x,m(x)
i+1
−R
x,m(x)
i
;
6
λ(x)
0
←+∞; λ(x)
kx ←0; Λ(x) ←
n
λ(x)
0 , λ(x)
1 , . . . , λ(x)
kx−1, λ(x)
kx
o
;
▷λ(x)
0
> · · · > λ(x)
kx
7
n
eλ0, . . . , eλk
o
←S
x∈X Λ(x) with +∞= eλ0 > eλ1 > · · · > eλk−1 > eλk = 0 ;
▷k ≥kx ∀x ∈X
8 for x ∈X do
9
for j = 1, . . . , k do Find i ∈{1, . . . , kx} :

λ(x)
i
, λ(x)
i−1

⊇

eλj, eλj−1

;
set em(x)
j
←m(x)
i
;
10 for j = 1, . . . , k do
11
if P
x∈X Rx, em(x)
j
> R then λj ←eλj−1 else λj ←eλj ;
12
Dj ←−λjR + P
x∈X

Dx, em(x)
j
+ λjRx, em(x)
j

;
13 Return maxj=1,...,k Dj ;
▷= D∗(R)
compressor with rate at most R, given by
¯D∗(R) =
inf
PM|XQ
E

dlog
 Y, ϕLLM(M, Q)

s.t.
PM|XQ is a compressor, and
E
len(M)
len(X)

≤R,
(5)
with both expectations taken with respect to the joint distribution PMXQY = PM|XQPXQY induced
by the compressor. The condition “PM|XQ is a compressor” is short for (1) for each x ∈X and q ∈Q,
P
m∈M PM|XQ(m|x, q) = 1, (2) PM|XQ(m|x, q) = 0 if len(m) > len(x), and (3) if len(m) = len(x),
then PM|XQ(m|x, q) = 0 unless m = x. Similarly, the (conditional) distortion-rate function at rate
R is the smallest distortion that can be achieved by a query-aware compressor for query q at rate at
most R, given by
D∗
q(R) =
inf
PM|XQ(·|·,q)
E

dlog
 Y, ϕLLM(M, Q)

s.t.
PM|XQ(·|·, q) is a compressor, and
E
len(M)
len(X)

≤R,
(6)
with both expectations taken with respect to PMXY |Q(·, ·, ·|q).
Just like the query-agnostic setting, note that both (5) and (6) are linear programs, as the
objective and constraints are all linear in PM|XQ and PM|XQ(·|·, q) respectively. We obtain explicit
11


--- Page 12 ---
linear programs analogous to (LP) by defining constants ¯Dq
x and ¯Rq
x ∈RMx
+
for the average distortion-
rate function and Dq
x and Rq
x ∈RMx
+
for the conditional distortion-rate functions, for each x ∈X
and q ∈Q, similarly to (4).
Proposition 2 (Query-aware primal LPs). The (average) distortion-rate function for query-aware
prompt compression (5) is given by the solution to
¯D∗(R) =
inf
(zx,q∈RMx
+ )x∈X,q∈Q
X
x∈X,q∈Q
¯Dq
x
⊤zx,q
s.t.
X
x∈X,q∈Q
¯Rq
x
⊤zx,q ≤R,
1⊤zx,q = 1,
∀x ∈X, q ∈Q.
(avg-cond-LP)
The (conditional) distortion-rate function for query-aware prompt compression for query q (6) is
given by the solution to
D∗
q(R) =
inf
(zx∈RMx
+ )x∈X
X
x∈X
Dq
x
⊤zx
s.t.
X
x∈X
Rq
x
⊤zx ≤R,
1⊤zx = 1,
∀x ∈X.
(cond-LP)
For each x ∈X, Mx denotes the set of all possible compressed prompts associated to x, i.e., the set of
all possible token sequences of length at most len(x), the vectors zx,q ∈RMx
+
for q ∈Q and zx ∈RMx
+
are the optimization variables respectively and the constants ¯Dq
x, ¯Rq
x, Dq
x, Rq
x ∈RMx
+
are given by
¯Dq
x,m ≜PXQ(x, q) E [dlog(Y, ϕLLM(m, q))] and ¯Rq
x,m ≜PXQ(x, q) len(m)
len(x) ,
m ∈Mx,
(7)
Dq
x,m ≜PX|Q(x|q) E [dlog(Y, ϕLLM(m, q))] and Rq
x,m ≜PX|Q(x|q) len(m)
len(x) ,
m ∈Mx,
(8)
with the expectation taken with respect to PY |MXQ(·|m, x, q).
Proof. This follows immediately from (5) and (6) by defining the constants ¯Dq
x, ¯Rq
x, Dq
x, Rq
x ∈RMx
+
for each x ∈X and q ∈Q as given in (7) and (8) and taking zx,q and zx to be PM|XQ(·|x, q)
respectively,. We use the fact that PM|XQ(m|x, q) = 0 when len(m) > len(x) to reduce the dimension
of zx,q and zx from M to Mx.
Remark 1. An interesting phenomenon here that does not occur in the query-agnostic setting
is the comparison between the average and conditional distortion-rate functions, i.e., ¯D∗(R) and
D∗
q(R) for q ∈Q. One possible way to “average” the conditional distortion-rate functions would be
to simply compute EQ∼PQ
h
D∗
Q(R)
i
, but we always have ¯D∗(R) ≤EQ∼PQ
h
D∗
Q(R)
i
. This is because
the latter averages the distortion-rate functions over PQ at a fixed value of the rate, i.e., the prompt
for each query is forced to be compressed to the same rate R. For ¯D∗(R), on the other hand, only
the average rate over the queries is required to be R. This allows the compressor to set a higher
rate for “difficult queries” that have higher distortion values, and use a lower rate for queries that
have lower distortion values in general. This is exactly the phenomenon we exploit in designing the
variable-rate compression scheme Adaptive QuerySelect in Sec. 4.1, which outperforms other existing
schemes in our experiments.
12


--- Page 13 ---
Just as in the query-agnostic setting, it is useful to compute and solve the dual linear programs
instead of directly solving the linear programs above.
Theorem 2 (Query-aware dual LPs). The (average) distortion-rate function for query-aware prompt
compression (5) is given by the solution to the dual of the linear program (avg-cond-LP), i.e.,
¯D∗(R) = sup
λ≥0


−λR +
X
x∈X,q∈Q
min
m∈Mx
 ¯Dq
x,m + λ ¯Rq
x,m



.
(avg-cond-dual-LP)
The (conditional) distortion-rate function for query-aware prompt compression for query q (6) is
given by the solution to the dual of the linear program (cond-LP), i.e.,
D∗
q(R) = sup
λ≥0
(
−λR +
X
x∈X
min
m∈Mx

Dq
x,m + λRq
x,m

)
.
(cond-dual-LP)
Proof. (Conditional) This follows trivially by simply observing that the linear program in (cond-LP)
is identical to that in (LP), except that Dx and Rx are replaced by the (conditional) query-aware
versions Dq
x and Rq
x respectively. Henc, by Thm. 1 the solution to (cond-LP) is given by (dual-LP)
with Dx and Rx replaced by the (conditional) query-aware versions Dq
x and Rq
x respectively, which
gives (cond-dual-LP).
(Average) In addition to replacing Dx and Rx from (LP) by the (average) query-aware versions
¯Dq
x and ¯Rq
x respectively, we also have that the optimization variables are given by zx,q for each pair
(x, q) ∈X ×Q as opposed to simply zx for each x ∈X. Hence, by Thm. 1 the solution to (avg-cond-LP)
is given by (dual-LP) with Dx and Rx are replaced by the (average) query-aware versions ¯Dq
x and ¯Rq
x
respectively and X replaced by X × Q. This gives (avg-cond-dual-LP) exactly, and we are done.
Note that both (avg-cond-dual-LP) and (cond-dual-LP) are of the same form as (dual-LP), with
some minor differences. For a given q ∈Q, the conditional distortion-rate function D∗
q(R) is identical
to the query-unaware distortion-rate function D∗
q(R) with (Dx, Rx) replaced by (Dq
x, Rq
x), and hence
can be solved by running Algorithm 1 with the input

R, (Dq
x)x∈X , (Rq
x)x∈X
	
. For the average
distortion-rate function ¯D∗(R), in addition to replacing Dx and Rx by ¯Dq
x and ¯Rq
x, we also have
that X is replaced by X × Q, hence ¯D∗(R) is obtained by running Algorithm 1 with the input
n
R,
 Dq
x′

x′∈X ′ ,
 Rq
x′

x′∈X ′
o
, where X ′ ≜X × Q and x′ runs over all pairs (x, q).
4
Experiments
The distortion-rate function defined in Sec. 3 describes the best possible trade-off between the
achievable values of rate and distortion in the query-aware and query-agnostic cases. In this section, we
compare the performance of existing prompt compression methods (that are compatible with the black-
box model setting we consider in this work) with the optimal curve for synthetic and natural language
datasets. We observe that there is a sizeable gap between the performance of the existing methods and
the optimal curve. We propose Adaptive QuerySelect, a query-aware and variable-rate adaptation of
LLMLingua-2 [PWJ+24], that outperforms the existing methods on these datasets. We also consider
a query-aware version of LLMLingua-2 called QuerySelect and observe that it outperforms the query-
agnostic version, which highlights the importance of conditioning on the query. We report our results
with the log loss and 0/1 loss metrics (defined in (1)) for the synthetic dataset and semantic distortion
measures such as RougeL [Lin04] and BertScore [ZKWWA20] for natural language datasets.
13


--- Page 14 ---
We include an ablation study on the impact of tokenization of the prompt compression problem,
as tokenization is lossy since it groups together multiple symbols into a single symbol before passing
it to an LLM. We study the effect of tokenization on the prompt compression problem by forcing
the tokenizer on the encoder and decoder side to tokenize the bits of the binary string prompts in
our synthetic dataset individually, which we refer to as “forced tokenization.” We run experiments in
this setting and with the regular “standard tokenization.” Additional details on our experiments can
be found in App. C. Our code is made available for reproducibility purposes.1
4.1
Experimental setup
Dataset.
In order to run experiments that are computationally tractable but still meaningful
to the prompt compression problem, we construct a synthetic dataset {(xi, qi, yi)}N
i=1 with (1)
prompts xi being sequences from V = {0, 1}, i.e., binary prompts, (2) natural language queries qi,
such as “Count the number of 1s,” “Compute the parity,” and “Is the binary string a palindrome?”
and (3) their associated answers yi. In total, we construct a dataset of seven queries; a complete
specification of the dataset, including a few examples is available in App. C.1.1. The binary prompts
are generated from a first-order Markov chain on {0, 1} with a 0.1 probability of transitioning and a
0.9 probability of remaining in the same state, and the minimum and maximum possible lengths
for each prompt are four and ten, respectively. All methods are evaluated on a validation set of
1400 examples in total (7 queries, 200 examples per query). The optimal distortion-rate function
is computed using Algorithm 1, taking PXQY to be the empirical distribution on the dataset, i.e.,
PXQY = 1
N
PN
i=1 δ(xi,qi,yi). This is the natural choice when the true distribution is unknown, but
other choices may also be valid such as a parametric model with parameters learned from a dataset,
but it is unclear what an appropriate parametric model for LLM prompts is.
We also run experiments on a small natural language dataset curated with GPT-4 [OAA+24]
and NarrativeQA [KSB+17] for a large-scale experiment. The small dataset consists of ten prompts
with four queries each, and a few examples are provided in Table 3 in App. C.2.1. More details on
the considerations made in constructing our datasets are described in Sec. 4.2.
Baseline methods.
We compare the rate-distortion trade-off of the optimal strategy (both query-
aware and query-agnostic) with prompt compression methods that can be used to compress prompts
for a black-box target LLM: Selective Context [LDLG23], LLMLingua [JWLYQ23], LLMLingua
Query [JWL+23], LLMLingua-2 [PWJ+24]. As such, we do not consider methods like Gist Tokens
[MLG23], In-Context Autoencoder [GJW+24], and AutoCompressor [CWAC23] since they require
special training methods generally not compatible with black-box target LLMs. Selective Context
uses −log P(xi | x0, x1, . . . , xi−1) to score the i-th token, and retains the tokens whose score is larger
than the p-percentile, where p ∈[0, 1] is the ratio parameter. LLMLingua uses a similar method,
but they first partition the input prompt into segments and condition on previously compressed
segments to compress the current segment. They later extended their method to perform query-
aware compression, which is what we use for LLMLingua Query. While these methods use a decoder-
style (causal) transformer LLM to do prompt compression, this approach makes an independence
assumption on the influence of future tokens have on the i-th token. LLMLingua-2 instead uses an
encoder-style (bidirectional) LLM to perform a token classification task, where their model predicts
whether a given token should be kept or removed.
1Our code is available at https://github.com/acnagle/fundamental-limits
14


--- Page 15 ---
Our proposed methods.
We add two novel contributions over the LLMLingua-2 work: (1) we
adapt LLMLingua-2 to the query-aware setting, whereas the original work only proposed the query-
agnostic approach, which we call “QuerySelect,” and (2) we further adapt this query-aware approach
into a variable-rate approach we refer to as “Adaptive QuerySelect.” This approach which lets the
encoder model decide which tokens to keep based on the confidence over a specified threshold. In
other words, LLMLingua-2 and QuerySelect accept a rate parameter to determine the compression
ratio, but Adaptive QuerySelect replaces the rate parameter with a threshold parameter. As a result,
the encoder model predicts the probability of keeping a particular token, and the token is kept if
the predicted probability is above a given threshold, resulting in a variable-rate compression of the
prompt. This is an important aspect of the prompt compression problem, as some prompts are more
compressible than others, and vice versa.
Models.
We use Mistral 7B Instruct v0.2 [JSM+23] as our black-box target LLM (decoder side),
which is fine-tuned on the training set partition of our synthetic dataset. Critically, this model is
fixed after fine-tuning and no prompt compression methods have access to any part of it. All prompt
compression methods use an LLM as part of their compression algorithm (encoder side); we use
deduplicated Pythia 1B [BSA+23] for Selective Context, LLMLingua, and LLMLingua Query and
RoBERTa Base [LOG+20] for LLMLingua-2 methods. For each method, we finetune on the training
set partition to enable the best performance possible for that method. More information on how
we trained these methods and the data we used is in App. C. For all models, including the target
LLM, we fine-tune with LoRA [HSW+22] and conduct a hyperparameter grid search. We choose
the configuration with the best performance on a test set that is different from the validation set.
More details on the hyperparameter search are provided in App. C.3.
For the natural language dataset, no fine-tuning is necessary on the decoder side. On the encoder
side, Selective Context, LLMLingua, and LLMLingua Query use the same model as on the decoder side,
and LLMLingua-2 uses a specially fine-tuned version of XLM RoBERTa Large [RSV19; PWJ+24]. We
use a custom fine-tuned XLM RoBERTa Large model as the encoder for the QuerySelect and Adaptive
QuerySelect methods. The training dataset of (prompt, query, answer) tuples used to train this
custom model is filtered from the Databricks Dolly 15k [CHM+23] dataset to only include examples
with prompt lengths between a specified minimum and maximum length (see Sec. C.3.2 for details).
4.2
Results
Since computing the exact optimal curve is intractable for large vocabulary sizes, we first report
our results on the synthetic dataset consisting of binary prompts in Sec. 4.2.1. We then describe
how to overcome the vocabulary size barrier by extending our results to a small natural language
dataset in Sec. 4.2.2. Finally, we describe how to approximate the optimal curve for large-scale
natural language datasets in Sec. 4.2.3.
4.2.1
Synthetic dataset
Fig. 1 summarizes our experimental contributions on the synthetic dataset. We observe a large
gap between the optimal curve and existing prompt compression methods.
Thus, we propose
QuerySelect as a query-aware and Adaptive QuerySelect as a query-aware, variable-rate modification
of LLMLingua-2 to close this gap. Our results show that Adaptive QuerySelect achieves the best
performance and, in fact, is the only method to outperform the optimal query-agnostic strategy. We
also note that the optimal distortion-rate curves eventually fall below the baseline performance of
using the full prompt (no compression). This observation is especially interesting because it shows
15


--- Page 16 ---
that compressing prompts can improve performance on downstream tasks, as observed on natural
language datasets in previous prompt compression works [JWLYQ23; JWL+23; PWJ+24]. We
accredit the performance of Adaptive QuerySelect to variable-rate compression, where we allow
the compressor to choose how much it should compress based on the query and prompt as input
(see Sec. 3.4, Remark 1). Even though this approach relinquishes explicit control over the rate, our
experiments show that variable-rate compression is the closest to optimality.
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0.00
0.05
0.10
0.15
0.20
Average distortion (0/1 loss)
Is the binary string
a palindrome?
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0.0
0.1
0.2
0.3
0.4
0.5
Count the number of transitions
from 0 to 1 and 1 to 0.
Selective [LDLG23]
LLMLingua [JWLYQ23]
LLMLingua Query [JWL+23]
LLMLingua-2 [PWJ+24]
QuerySelect
Adaptive QuerySelect
Optimal (Query-aware)
No Compression
Figure 4: We highlight the distortion-rate curves for two of the seven queries in the validation partition
of our synthetic dataset. Our method, Adaptive QuerySelect, is able to match the performance of
the optimal query-aware strategy (left). Some queries naturally incur less distortion than others
with the target LLM, even with a query-agnostic approach, if the query is aligned well with the data
generation process for the prompt (right). Note that QuerySelect covers the line of LLMLingua-2
as their performance is identical for this query.
Gap from optimality depends on the query.
In Fig. 4, we highlight the distortion-rate curves
for two out of seven of the queries in our synthetic dataset. Despite the fact that Fig. 1 shows a gap
in average performance between the query-aware optimal strategy and Adaptive QuerySelect, Fig. 4
(left) shows that Adaptive QuerySelect can match the performance of the optimal query-aware
compression scheme. Comparing Fig. 4 (left) and (right), we see that the prompt compression
problem is easier (methods are closer to optimality) for certain tasks or queries depending on how the
prompts were generated. For our synthetic dataset, all prompts are generated from a Markov chain
with a transition probability of 0.1 and a probability of 0.9 for remaining in the same state. This
means the tokens with the highest entropy are those that are part of a transition, and those tokens
are the most important for answering this query. As a result, we see that methods that use the
negative log-likelihood as a means for compression (Selective Context, LLMLingua, and LLMLingua
Query) perform well, even without conditioning on the query. An exception here is the performance
of LLMLingua Query, which we find has mixed performance compared to vanilla LLMLingua for
token-level prompt compression on our dataset. LLMLingua Query performs markedly worse for
this query (refer to Fig. 12 in App. C for results on all queries).
Effect of tokenization.
Finally, the results of our ablation study on the effects of tokenization are
provided in Fig. 11 in App. C. Interestingly, the optimal curves are nearly identical, suggesting that
tokenization does not play a role in attaining the best possible trade-off. Furthermore, we see that, for
a fixed rate, the standard tokenization performance often matches or exceeds the performance of forced
16


--- Page 17 ---
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0.0
0.5
1.0
1.5
2.0
2.5
Average distortion
Log loss, All
Log loss, Pruned
0/1 loss, All
0/1 loss, Pruned
Figure 5: Query-agnostic distortion-rate curves plotted for log loss and 0/1 loss distortion measures.
The curves marked with a ‘diamond’ are computed using all possible shorter sequences, while those
marked with an ‘×’ are computed using only pruned versions of the original prompt. They are
nearly identical, which suggests that a good approximation to the optimal distortion-rate curve can
be obtained by considering pruned prompts only.
tokenization. However, the standard tokenization approach does not allow for average rates below 0.6
due to the limited size of the prompts in our synthetic dataset, so the comparison is somewhat limited.
In particular, standard tokenization allows for compression of at most four tokens (but usually only
two or three tokens), whereas forced tokenization allows for compression of at most ten tokens.
4.2.2
Small natural language dataset
Restriction to pruned prompts.
The decisive bottleneck in running Algorithm 1 turns out to
be obtaining Dx for each x ∈X, i.e., the input to the algorithm. We require one inference call for
each possible compressed prompt m ∈Mx to compute Dx for a particular prompt x and query.
Taking Mx to be all sequences of length smaller than len(x), we see that that the size of Mx is
Plen(x)−1
i=1
|V|i, where V is the vocabulary of the LLM. The model used in our experiments, Mistral
7B Instruct v0.2 [JSM+23], has |V| = 32,000. Clearly, it is then virtually impossible to consider
prompts with more than 2 tokens, and in fact, makes it difficult to consider even medium length
prompts (50 tokens) for a vocabulary of size more than 2.
A key first step towards extending our algorithm for natural language prompts is the observation
that all prompt compression methods in the literature work by pruning tokens, i.e., (1) they are non-
generative, i.e., work by removing tokens from the original prompt and therefore do not generate any
new tokens, and (2) they preserve the order of the tokens as they appear in the input sequence. Hence,
to compute the fundamental limit for the schemes that compress the prompt by pruning, it is enough
to consider Mx to be the sequences that are obtained from x by removing some number of (not
necessarily contiguous) tokens. Then, we have |Mx| = 2len(x), irrespective of the vocabulary size |V|.
In Fig. 5, we observe that the distortion-rate curves obtained by restricting Mx to be only those
sequences obtained from x from via pruning, are nearly identical to the original curves, where we
take Mx to be all shorter sequences. This suggests two things: (1) there is no fundamental drawback
17


--- Page 18 ---
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Average distortion
1 - RougeL
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0.02
0.04
0.06
0.08
0.10
0.12
1 - BertScore
Selective [LDLG23]
LLMLingua [JWLYQ23]
LLMLingua Query [JWL+23]
LLMLingua-2 [PWJ+24]
QuerySelect
Adaptive QuerySelect
Optimal (Query-agnostic)
Optimal (Query-aware)
No Compression
Figure 6: Comparison among all prompt compression methods on our natural language dataset. We
show the rate-distortion trade-off for RougeL [Lin04] (left) and BertScore [ZKWWA20] (right).
Since a higher RougeL and BertScore metric is better, we plot “1−the computed average distortion”
so that a higher rate should yield a lower loss. We discuss the choice of our metrics in App. C.2.2.
to considering compression schemes are not generative, i.e., work by pruning the original prompt,
and (2) we can approximate the optimal distortion-rate function reasonably well by considering only
pruned versions of the prompt as possible compressed prompts. Thus, in principle, we can replicate
experiments with natural language prompts of the same lengths (4 to 10) as the binary prompts in our
experiments above, with the same computational cost. However, it is difficult to identify sufficiently
rich natural language prompts of such short lengths for which compression is a reasonable problem, and
hence, use binary prompts (generated from a Markov chain, to model the dependence between tokens)
with natural language queries (since there is no computational restriction on the vocabulary of the
queries) to run experiments such as those in Sec. 4.2.1 at scale. Nonetheless, to illustrate that we can
get meaningful results by considering such “pruned” compressed prompts, we generate a small natural
language dataset using GPT-4 [OAA+24], consisting of ten prompts and four queries for each prompt;
a few examples of the (prompt, query, answer) tuples from this dataset are shown in App. C, Table 3.
Results for small natural language dataset.
The results of our extension to natural language
prompts, presented in Fig. 6, show that both of our proposed methods, QuerySelect and Adaptive
QuerySelect, achieve the lowest distortion among all other prompt compression algorithms for low
rates. However, the gap between all algorithms and the optimal strategies is significant. Surprisingly,
even the gap between the query-aware prompt compression methods and the query-agnostic optimal
strategy is quite large. We posit the quality of the training data for LLMLingua-2-based methods
accounts for the discrepancy in how far away the best method is from the optimal strategies between
binary (Fig. 1) and natural language (Fig. 6) prompts. More specifically, the labels used for the
binary synthetic dataset can be determined algorithmically and are optimal, but GPT-4 is used to
determine the labels for the natural language dataset, which generally does not have a set of optimal
“ground truth” labels. Remarkably, the gap between feeding the prompt directly to the black-box
LLM (no compression) and either optimal prompt compression strategy is also large, and Fig. 6
shows that much lower distortion can be achieved in roughly 70% and 40% fewer tokens for the
query-aware and query-agnostic cases, respectively. LLMLingua-2 methods are the only methods
that achieve lower distortion than the no compression result, albeit for higher rates. Finally, we
present a few histograms of the rates for QuerySelect and Adaptive QuerySelect in Fig. 16, which
18


--- Page 19 ---
shows the greater range of rates that Adaptive QuerySelect may choose from over QuerySelect.
4.2.3
Beam search for large-scale natural language dataset
Exactly computing the optimal rate-distortion curves for large-scale datasets, where the prompts
consist of hundreds or thousands of tokens, is intractable. Even for our curated small-scale dataset,
whose largest prompt consists of 15 tokens, it is not feasible to compute the “true” optimal curve
outright. Instead, as described in Sec. 4.2.2, we considered the set of compressed prompts constructed
from in-place token removal from the original prompt, which significantly reduced the number of
inference calls in constructing Dx from Plen(x)−1
i=1
|V|i to 2len(x), where |V| is the size of vocabulary of
the tokenizer and x ∈X is the prompt. While this approach does not yield the “true” optimal curve, it
does provide an optimal curve to all existing prompt compression methods since they all strategically
remove tokens in place. Thus, we can still establish the fundamental limit of existing methods!
However, when len(x) is a few hundred or thousand, as is the case for large-scale datasets, 2len(x)
is far too many inference calls to make Dx to be practical. Ideally, one can approximate the optimal
curve by finding an upper bound; to do this, we use beam search. Our search space is over all 2len(x)
binary masks (a binary mask, when applied to the prompt, forms a compressed prompt). Each mask
is assigned a distortion value by computing the distortion between the ground truth answer and the
generated answer when the black-box LLM is given the mask’s associated compressed prompt and
the query. Thus, we can construct the search tree over the binary masks and use beam search to
find masks with low distortion.
The search tree begins with the “all ones” mask at the root node (l = 0); each of the root node’s
children (l = 1) contains a bit flipped to 0 in precisely one of the len(x) positions. At the third level
(l = 2), each node inherits a 0 at the same position as its parent and then flips a 1 to a 0 so that each
mask at l = 2 has precisely two 0s. This pattern continues throughout the rest of the tree, resulting
in masks with l 0s at level l. Furthermore, the branching factor Fl of our beam search is the number
of children nodes a given node has at level l, is N −l. At level l, accounting for all nodes that are
shared among the parent nodes in level l −1, there are
 N
l

nodes, where N = len(x). As a sanity
check, this tree contains PN
k=0
 N
k

= 2len(x) nodes in this tree, so all binary masks of length N are in
the tree. We can search every node in the tree as long as the beam width B is large enough to contain
every node at the broadest level of the tree. To achieve this, we need to set B = maxl
 N
l

=
 N
⌊N/2⌋

.
Recall that our purpose for using beam search is to drastically reduce the number of LLM
inference calls required for constructing Dx while retaining a competitive upper bound to the optimal
curve. The cost C associated with the beam search, which is the number of nodes visited in the tree
(i.e., the number of LLM inference calls) is
C = B
N
X
l=0
Fl = B
N
X
l=0
(N −l) = B
N
X
k=0
k = BN(N + 1)
2
.
(9)
Note that, in this case, we search over C < |Mx| in (LP) compressed prompts, so beam search
will provide an upper bound. When N is large, checking O(BN2) nodes is a drastic reduction over
checking 2N nodes, but the growth rate is still too large. For example, running beam search on a
single prompt of just 100 tokens will require on the order of 106 LLM inference calls, or roughly 11.5
days if assuming a single inference call takes one second. To further reduce the number of checked
nodes, we can chunk the binary masks into spans of bits, and flip entire spans from 1 to 0 at each
level. With this approach, we can effectively control the length of the binary masks we search over,
which we call Neff, for a given budget/cost C. Solving (9) for N, we arrive at
19


--- Page 20 ---
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0.5
0.6
0.7
0.8
0.9
Average distortion
1 - RougeL
LLMLingua
Selective
LLMLingua Query
LLMLingua-2
QuerySelect
Adaptive QuerySelect
Beam Search (Query-agnostic)
Beam Search (Query-aware)
No Compression
Figure 7: Comparison between existing prompt compression methods and our approximation to the
optimal rate-distortion curves via beam search on NarrativeQA.
Neff =
−1 +
q
1 + 8C
B
2
This approach allows us to choose the total number of LLM inference calls C per prompt that
we will spend on beam search for a particular beam width B. Since we are chunking the binary
mask into spans and then masking the spans as we search, we have reduced the search space and the
resulting tree will be smaller than the Neff = N case.
Fig. 7 compares existing methods to our beam search upper bounds for C = 4000 and B = 5.
Although LLMLingua-2-based methods can outperform the upper bound in the query-agnostic case
for most rates, the query-agnostic beam search approach shows room for improvement from existing
methods. Impressively, the gap between existing methods and the query-aware case is quite large,
even for the methods that use the query to compress the prompt. More details on the performance
of compression methods on NarrativeQA are provided in App. D.2.
5
Discussion and conclusion
Building off previous works, we have proposed a framework for understanding the prompt compression
problem for black-box target LLMs. With this framework, we defined and formulated the optimal
distortion-rate trade-off as a linear program, and devised an algorithm to solve this efficiently via its
dual, for both query-agnostic and query-aware settings. We computed the optimal trade-off exactly
for synthetic datasets with binary prompts, and proposed methods to approximate the optimal trade-
off for large-scale natural language datasets such as NarrativeQA. We compared the optimal curves
with prompt compression methods in the existing literature and adapt one of them, LLMLingua-2,
to be query-aware and variable-rate; this modified method, which we call “Adaptive QuerySelect,”
exhibits superior performance, sometimes even matching the performance of the optimal query-
aware strategy, on our synthetic dataset. As future work, it is important to exhaustively study our
proposed method on natural language datasets.
20


--- Page 21 ---
Acknowledgements
This work was partly supported by ARO Award W911NF2310062, ONR Award N000142412542,
and the 6G@UT center within WNCG at UT Austin. The work was also supported in part by the
Swiss National Science Foundation under Grant 200364. The authors would like to thank Ananda
Theertha Suresh for introducing them to the problem of prompt compression. AG would like to
thank Emre Telatar for helpful discussions on the problem formulation.
References
[ABC+21]
Amanda Askell et al. A General Language Assistant as a Laboratory for Alignment.
2021 (cit. on p. 3).
[Ber71]
T. Berger. Rate Distortion Theory: A Mathematical Basis For Data Compression.
Englewood Cliffs, NJ: Prentice-Hall, Inc., 1971 (cit. on pp. 7, 29).
[BSA+23]
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien,
Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth,
Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar Van Der Wal. “Pythia: a
suite for analyzing large language models across training and scaling”. In: Proceedings
of the 40th International Conference on Machine Learning. ICML’23. Honolulu,
Hawaii, USA: JMLR.org, 2023 (cit. on pp. 15, 32).
[BV04]
Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge univer-
sity press, 2004 (cit. on pp. 8, 9).
[CHM+23]
Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali
Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free Dolly: Introducing
the World’s First Truly Open Instruction-Tuned LLM. 2023 (cit. on pp. 15, 33).
[CT06]
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series
in Telecommunications and Signal Processing). USA: Wiley-Interscience, 2006 (cit.
on pp. 7, 29).
[CWAC23]
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. “Adapting
Language Models to Compress Contexts”. In: Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing. Ed. by Houda Bouamor, Juan
Pino, and Kalika Bali. Singapore: Association for Computational Linguistics, Dec.
2023 (cit. on pp. 3, 4, 14).
[Dan02]
George B Dantzig. “Linear programming”. In: Operations research 50.1 (2002) (cit.
on p. 8).
[Dao23]
Tri Dao. “FlashAttention-2: Faster Attention with Better Parallelism and Work
Partitioning”. In: (2023) (cit. on p. 3).
[DFERR22]
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. “FlashAtten-
tion: Fast and Memory-Efficient Exact Attention with IO-Awareness”. In: Advances
in Neural Information Processing Systems. 2022 (cit. on p. 3).
[EK11]
Abbas El Gamal and Young-Han Kim. Network Information Theory. Cambridge
University Press, 2011 (cit. on pp. 7, 8, 29).
[GD23]
Albert Gu and Tri Dao. “Mamba: Linear-Time Sequence Modeling with Selective
State Spaces”. In: arXiv preprint arXiv:2312.00752 (2023) (cit. on p. 3).
21


--- Page 22 ---
[GHI22]
Demian Ghalandari, Chris Hokamp, and Georgiana Ifrim. “Efficient Unsupervised
Sentence Compression by Fine-tuning Transformers with Reinforcement Learning”.
In: Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Ed. by Smaranda Muresan, Preslav Nakov, and
Aline Villavicencio. Dublin, Ireland: Association for Computational Linguistics, May
2022 (cit. on p. 5).
[GJW+24]
Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. “In-context
Autoencoder for Context Compression in a Large Language Model”. In: The Twelfth
International Conference on Learning Representations. 2024 (cit. on pp. 3, 4, 14).
[Gra72]
R. Gray. Conditional rate-distortion theory. Tech. rep. Stanford University, 1972 (cit.
on pp. 29, 30).
[HP79]
Bruce Hajek and Michael B Pursley. “Evaluation of an achievable rate region for the
broadcast channel”. In: IEEE Transactions on Information Theory 25.1 (1979) (cit.
on p. 8).
[HSW+22]
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen. “LoRA: Low-Rank Adaptation of Large Language
Models”. In: International Conference on Learning Representations. 2022 (cit. on
p. 15).
[JSM+23]
Albert Q. Jiang et al. Mistral 7B. 2023 (cit. on pp. 15, 17, 32).
[JWL+23]
Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing
Yang, and Lili Qiu. LongLLMLingua: Accelerating and Enhancing LLMs in Long
Context Scenarios via Prompt Compression. 2023 (cit. on pp. 3, 5, 14, 16).
[JWLYQ23]
Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. “LLMLingua:
Compressing Prompts for Accelerated Inference of Large Language Models”. In:
Proceedings of the 2023 Conference on Empirical Methods in Natural Language
Processing. Association for Computational Linguistics, Dec. 2023 (cit. on pp. 3, 5, 14,
16).
[KKV24]
Heasung Kim, Hyeji Kim, and Gustavo De Veciana. “Estimation of Rate-Distortion
Function for Computing with Decoder Side Information”. In: First ’Learn to Compress’
Workshop @ ISIT 2024. 2024 (cit. on p. 8).
[KLZ+23]
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao
Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. “Efficient Memory Management
for Large Language Model Serving with PagedAttention”. In: Proceedings of the
29th Symposium on Operating Systems Principles. SOSP ’23. Koblenz, Germany:
Association for Computing Machinery, 2023 (cit. on p. 3).
[KSB+17]
Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann,
Gábor Melis, and Edward Grefenstette. The NarrativeQA Reading Comprehension
Challenge. 2017 (cit. on pp. 3, 14, 41).
[LDLG23]
Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. Compressing Context to
Enhance Inference Efficiency of Large Language Models. 2023 (cit. on pp. 3, 4, 14).
[LHS22]
Eric Lei, Hamed Hassani, and Shirin Saeedi Bidokhti. “Neural Estimation of the Rate-
Distortion Function With Applications to Operational Source Coding”. In: IEEE
Journal on Selected Areas in Information Theory 3.4 (2022) (cit. on p. 8).
22


--- Page 23 ---
[Lin04]
Chin-Yew Lin. “ROUGE: A Package for Automatic Evaluation of Summaries”. In:
Text Summarization Branches Out. Barcelona, Spain: Association for Computational
Linguistics, July 2004 (cit. on pp. 7, 13, 18, 32, 41).
[LLH+23]
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,
Fabio Petroni, and Percy Liang. Lost in the Middle: How Language Models Use Long
Contexts. arXiv:2307.03172. 2023 (cit. on p. 4).
[LOG+20]
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly
Optimized BERT Pretraining Approach. 2020 (cit. on pp. 15, 33).
[MLG23]
Jesse Mu, Xiang Lisa Li, and Noah Goodman. “Learning to Compress Prompts with
Gist Tokens”. In: (2023) (cit. on pp. 3, 4, 14).
[Neu28]
John von Neumann. “Zur Theorie der Gesellschaftsspiele”. In: Math. Ann. 100.1 (1928)
(cit. on p. 25).
[NXS19]
Tong Niu, Caiming Xiong, and Richard Socher. Deleter: Leveraging BERT to Perform
Unsupervised Successive Text Compression. 2019 (cit. on p. 5).
[OAA+24]
OpenAI et al. GPT-4 Technical Report. 2024 (cit. on pp. 14, 18, 31).
[PAA+23]
Bo Peng et al. “RWKV: Reinventing RNNs for the Transformer Era”. In: The 2023
Conference on Empirical Methods in Natural Language Processing. 2023 (cit. on p. 3).
[PWJ+24]
Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang,
Qingwei Lin, Victor Ruhle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu,
and Dongmei Zhang. “LLMLingua-2: Data Distillation for Efficient and Faithful Task-
Agnostic Prompt Compression”. In: ArXiv preprint abs/2403.12968 (2024) (cit. on
pp. 3–5, 13–16, 33, 42).
[RSM+23]
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano
Ermon, and Chelsea Finn. “Direct Preference Optimization: Your Language Model
is Secretly a Reward Model”. In: Thirty-seventh Conference on Neural Information
Processing Systems. 2023 (cit. on p. 32).
[RSV19]
Sebastian Ruder, Anders Sgaard, and Ivan Vuli. “Unsupervised Cross-Lingual Repre-
sentation Learning”. In: Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics: Tutorial Abstracts. Ed. by Preslav Nakov and Alexis
Palmer. Florence, Italy: Association for Computational Linguistics, July 2019 (cit.
on p. 15).
[Sha59]
Claude E Shannon. “Coding theorems for a discrete source with a fidelity criterion”.
In: IRE Nat. Conv. Rec 4.142-163 (1959) (cit. on pp. 7, 29, 30).
[Sim95]
Stephen Simons. “Minimax Theorems and Their Proofs”. In: Minimax and Applications.
Boston, MA: Springer US, 1995 (cit. on p. 25).
[SKZ22]
Charlie Snell, Dan Klein, and Ruiqi Zhong. Learning by Distilling Context. 2022 (cit.
on p. 3).
[SMLVM20]
Raphael Schumann, Lili Mou, Yao Lu, Olga Vechtomova, and Katja Markert. “Discrete
Optimization for Unsupervised Sentence Summarization with Word-Level Extraction”.
In: Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics. Ed. by Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault.
Online: Association for Computational Linguistics, July 2020 (cit. on p. 5).
23


--- Page 24 ---
[VSP+17]
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.
Gomez, Łukasz Kaiser, and Illia Polosukhin. “Attention is all you need”. In: Proceedings
of the 31st International Conference on Neural Information Processing Systems.
NIPS’17. Long Beach, California, USA: Curran Associates Inc., 2017 (cit. on p. 3).
[WE06]
Tsachy Weissman and Abbas El Gamal. “Source Coding With Limited-Look-Ahead
Side Information at the Decoder”. In: IEEE Transactions on Information Theory
52.12 (2006) (cit. on p. 8).
[WLKFM20]
Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer:
Self-Attention with Linear Complexity. 2020 (cit. on p. 3).
[WSS22]
David Wingate, Mohammad Shoeybi, and Taylor Sorensen. “Prompt Compression
and Contrastive Conditioning for Controllability and Toxicity Reduction in Language
Models”. In: Findings of the Association for Computational Linguistics: EMNLP
2022. Ed. by Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang. Abu Dhabi, United
Arab Emirates: Association for Computational Linguistics, Dec. 2022 (cit. on p. 3).
[Wyn78]
Aaron D Wyner. “The rate-distortion function for source coding with side information
at the decoder-II: General sources”. In: Information and Control 38.1 (1978) (cit. on
pp. 8, 29, 30).
[WZ76]
Aaron D Wyner and Jacob Ziv. “The rate-distortion function for source coding with
side information at the decoder”. In: IEEE Transactions on Information Theory 22.1
(1976) (cit. on pp. 8, 29, 30).
[Yam82]
Hirosuke Yamamoto. “Wyner-Ziv theory for a general function of the correlated
sources (Corresp.)” In: IEEE Transactions on Information Theory 28.5 (1982) (cit.
on pp. 8, 29, 30).
[YENM23]
Yibo Yang, Stephan Eckstein, Marcel Nutz, and Stephan Mandt. “Estimating the
Rate-Distortion Function by Wasserstein Gradient Descent”. In: ICML 2023 Workshop
Neural Compression: From Information Theory to Applications. 2023 (cit. on p. 8).
[ZKWWA20]
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi.
BERTScore: Evaluating Text Generation with BERT. 2020 (cit. on pp. 13, 18, 32, 41).
24


--- Page 25 ---
Appendix
The appendix is organized as follows:
1. In App. A, we prove the main result of the paper (Thm. 1) and give a detailed description of
the working of Algorithm 1.
2. In App. B, we provide an overview of the information theory literature on rate-distortion
theory, and explain how our model compares.
3. In App. C, we provide details on the setup used for the experiments in Sec. 4.
4. In App. D, we have additional experimental results.
A
The dual linear program: proof and solution
A.1
Derivation of the dual linear program
Proof of Thm. 1. We start from the linear program (LP) and construct its dual. Recall that (LP) is
given by
D∗(R) =
inf
(zx∈RMx
+ )x∈X
X
x∈X
D⊤
x zx
s.t.
X
x∈X
R⊤
x zx ≤R,
1⊤zx = 1,
∀x ∈X.
Introduce the Lagrange multipliers λ ≥0 to handle the inequality constraint and µx ∈R for each
x ∈X to handle the equality constraints. Then, the above equation is equivalent to
D∗(R) =
inf
(zx∈RMx
+ )x∈X
(X
x∈X
D⊤
x zx + sup
λ≥0
λ
 X
x∈X
R⊤
x zx −R
!
+
X
x∈X
"
sup
µx∈R
µx

1⊤zx −1
#)
.
To see why this equivalence holds, observe that the terms supλ≥0 λ
 P
x∈X R⊤
x zx −R

and
supµx∈R µx
 1⊤zx −1

are both 0 when (zx)x∈X is in the feasible set of (LP) and +∞otherwise.
Let µ ≜(µx)x∈X ∈RX , then we can simplify the above expression by rearranging terms, to obtain
D∗(R) =
inf
(zx∈RMx
+ )x∈X
sup
µ∈RX ,
λ≥0
(X
x∈X
(Dx + λRx + µx1)⊤zx −λR −
X
x∈X
µx
)
.
Note that the objective P
x∈X (Dx + λRx + µx1)⊤zx −λR −P
x∈X µx is linear in (zx)x∈X and in
(µ, λ), and the minimization and maximization are both over convex sets. Hence, by the minmax
theorem [Neu28; Sim95], we can switch their order without affecting the equality, i.e.,
D∗(R) = sup
µ∈RX ,
λ≥0
inf
(zx∈RMx
+ )x∈X
(X
x∈X
(Dx + λRx + µx1)⊤zx −λR −
X
x∈X
µx
)
.
If, for some x, there is a component of the vector Dx + λRx + µx1 ∈RMx that is negative, then
letting that component of zx go to infinity, we have that the inner infimum is −∞. On the other
25


--- Page 26 ---
hand, if every component of Dx + λRx + µx1 is nonnegative for every x, then the infimum is simply
0, attained by setting zx = 0. Hence, the above equation reduces to
D∗(R) =
sup
µ∈RX ,
λ≥0
−λR −
X
x∈X
µx
s.t.
Dx,m + λRx,m + µx ≥0
for every m ∈Mx and x ∈X.
For a given x, the constraint Dx,m + λRx,m + µx ≥0 for all m ∈Mx is equivalent to −µx ≤
minm∈Mx (Dx,m + λRx,m). Letting νx ≜minm∈Mx (Dx,m + λRx,m) + µx and ν ≜(νx)x∈X , the
constraint is simply that νx ≥0 for all x, or equivalently, ν ∈RX
+. Hence, the above equation can be
written as
D∗(R) = sup
ν∈RX
+,
λ≥0
−λR +
X
x∈X
min
m∈Mx (Dx,m + λRx,m) −
X
x∈X
νx.
Observe that only the first two terms depend on λ, and only the last term depends on ν. This lets
us optimize over λ and ν separately, to give
D∗(R) = sup
λ≥0
(
−λR +
X
x∈X
min
m∈Mx (Dx,m + λRx,m)
)
+ sup
ν∈RX
+
 
−
X
x∈X
νx
!
= sup
λ≥0
(
−λR +
X
x∈X
min
m∈Mx (Dx,m + λRx,m)
)
,
since supν∈RX
+
 −P
x∈X νx

= −infν∈RX
+
P
x∈X νx = −P
x∈X infνx≥0 νx = 0, and we are done.
A.2
Proof and illustration of Algorithm 1
In this section, we explain each step of Algorithm 1 in detail. In doing so, we prove that the algorithm
does indeed solve (dual-LP), i.e., computes
D∗(R) = sup
λ≥0
(
−λR +
X
x∈X
min
m∈Mx [Dx,m + λRx,m]
)
.
We also use an artificial example as described below to show the working of the algorithm, in
particular lines 1–9. For convenience, the algorithm is repeated verbatim below, without comments:
Consider the following artificial example with X = {α, β}. Let (Rα, Dα) and (Rβ, Dβ) be as
given by the blue points in the scatter plots over m ∈Mα and m ∈Mβ respectively in Fig. 8. In
our example, we have |Mα| = 11 and |Mβ| = 8. The following observation is crucial: recall the
definitions of Rx and Dx,
Dx,m ≜PX(x) E [dlog(Y, ϕLLM(m, Q))]
and
Rx,m ≜PX(x) len(m)
len(x) ,
m ∈Mx,
with the expectation computed with respect to PQY |MX(·, ·|m, x). For a fixed value of x, the positive
real numbers Dx,m can be arbitrary, but Rx,m must be an integral multiple of the constant PX(x)
len(x) .
Hence, for a given x, Rx,m takes at most len(x) possible values. This turns out to be extremely
beneficial in the first step, namely identifying the points on the lower-left convex boundary.
For x ∈X, lines 2–3 of the algorithm identify the kx points m(x)
1 , . . . , m(x)
kx that lie on the lower-
left convex boundary of {(Rx,m, Dx,m)}m∈Mx. The lower-left convex boundaries are given by the
26


--- Page 27 ---
Algorithm 1: To compute the distortion-rate function via the dual linear program (dual-LP)
1 Input: R, (Dx)x∈X , (Rx)x∈X ;
Output: D∗(R), the distortion-rate function at rate R;
2 for x ∈X do
3
Find M(x)
env ⊆Mx such that {(Rx,m, Dx,m)}m∈M(x)
env are on the lower-left convex boundary
of {(Rx,m, Dx,m)}m∈Mx;
4
n
m(x)
1 , m(x)
2 , . . . , m(x)
kx
o
←M(x)
env ordered such that Rx,m(x)
kx
> · · · > Rx,m(x)
1 ;
5
for i = 1, . . . , kx −1 do λ(x)
i
←
D
x,m(x)
i
−D
x,m(x)
i+1
R
x,m(x)
i+1
−R
x,m(x)
i
;
6
λ(x)
0
←+∞; λ(x)
kx ←0; Λ(x) ←
n
λ(x)
0 , λ(x)
1 , . . . , λ(x)
kx−1, λ(x)
kx
o
;
7
n
eλ0, . . . , eλk
o
←S
x∈X Λ(x) with +∞= eλ0 > eλ1 > · · · > eλk−1 > eλk = 0 ;
8 for x ∈X do
9
for j = 1, . . . , k do Find i ∈{1, . . . , kx} :

λ(x)
i
, λ(x)
i−1

⊇

eλj, eλj−1

;
set em(x)
j
←m(x)
i
;
10 for j = 1, . . . , k do
11
if P
x∈X Rx, em(x)
j
> R then λj ←eλj−1 else λj ←eλj ;
12
Dj ←−λjR + P
x∈X

Dx, em(x)
j
+ λjRx, em(x)
j

;
13 Return maxj=1,...,k Dj;
Rα,m
Dα,m
0.1
0.1
0.2
0.2
0.3
0.3
0.4
0.4
−λ(α)
1
−λ(α)
2
m(α)
1
m(α)
2
m(α)
3
Rβ,m
Dβ,m
0.2
0.2
0.4
0.4
0.6
0.6
−λ(β)
1
m(β)
1
m(β)
2
Figure 8: Scatter plots showing the points {(Rα,m, Dα,m)}m∈Mα and {(Rβ,m, Dβ,m)}m∈Mβ in blue.
The associated lower-left convex boundaries M(α)
bd = {m(α)
1 , m(α)
2 , m(α)
3 } and M(β)
bd = {m(β)
1 , m(β)
2 }
are in red; λ(α)
1 , λ(α)
2
and λ(β)
1
are the magnitudes of the slopes of the associated line segments.
red lines and the points lying on the boundary are outlined in red. Observe that kα = 3 and kβ = 2.
The quantities computed in line 5 are simply the magnitudes of the slopes of the line segments
on the boundary. A simple computation gives the result of line 6 of the algorithm in our example
to be Λ(α) = {+∞, 1.5, 0.5, 0} and Λ(β) = {+∞, 1, 0}. Clearly, for a given value of x ∈X and
λ ∈

λ(x)
j , λ(x)
j−1

, we have that m(x)
i
minimizes Dx,m + λRx,m over all m ∈Mx, by virtue of the fact
27


--- Page 28 ---
that these points come from the lower-left convex boundary. Hence, for λ ∈

λ(x)
j , λ(x)
j−1

, we have
X
x∈X
min
m∈Mx [Dx,m + λRx,m] =
X
x∈X

Dx,m(x)
j
+ λRx,m(x)
j

.
We cannot simplify this further in its current state since λ in the above expression depends on x.
Hence, we must remove the dependence of the range
h
λ(x)
j , λ(x)
j−1

on x. To do so, observe that each
Λ(x) is a partition of R+ on which m(x)
i
is the minimizer of Dx,m + λRx,m. Line 7 of the algorithm
simply constructs the union of all these partitions, with k elements, denoted by the eλ variables;
here we have k = 4 and the union is {+∞, 1.5, 1, 0.5, 0}. For each x, the minimizer on each interval
eλj, eλj−1

of the finer partition is known exactly to be one of the m(x)
i
’s; lines 8–9 associate to each
interval the corresponding minimizer, given by em(x)
j . There is no computation involved in these steps,
only notational rewriting. The corresponding values obtained for our example are given in the table
below (with eλ0 = +∞); observe that em(x)
j
minimizes Dx,m+λRx,m over m ∈Mx for λ ∈
eλj, eλj−1

.
Table 1: The outputs produced by lines 7–9 of Algorithm 1 with (Rα, Dα) and (Rβ, Dβ) as given
in Fig. 8.
j
eλj
em(α)
j
em(β)
j
1
1.5
m(α)
1
m(β)
1
2
1
m(α)
2
m(β)
1
3
0.5
m(α)
2
m(β)
2
4
0
m(α)
3
m(β)
2
At this point, we have for λ ∈
eλj, eλj−1

,
X
x∈X
min
m∈Mx [Dx,m + λRx,m] =
X
x∈X

Dx, em(x)
j
+ λRx, em(x)
j

=
P
x∈X Dx, em(x)
j

+ λ
P
x∈X Rx, em(x)
j

.
Hence, the right-hand side of (dual-LP) is simply
max
j=1,...,k
sup
λ∈[eλj,eλj−1)
P
x∈X Dx, em(x)
j

+ λ
P
x∈X Rx, em(x)
j
−R

= max
j=1,...,k
(P
x∈X Dx, em(x)
j

+
sup
λ∈[eλj,eλj−1)
λ
P
x∈X Rx, em(x)
j
−R
)
,
where
the
first
equality
follows
since
eλj
	k
j=0
is
a
partition
of
R+.
Consider
the
term supλ∈[eλj,eλj−1) λ
P
x∈X Rx, em(x)
j
−R

.
If Rx, em(x)
j
> R, this supremum occurs in the
limit as λ
→
eλj−1,
otherwise it is achieved at λ
=
eλj.
Hence,
defining λj
to
be eλj−1 or eλj accordingly as in line 11, we have that the above expression is simply
maxj=1,...,k
P
x∈X Dx, em(x)
j

+ λj
P
x∈X Rx, em(x)
j
−R

, which is exactly what line 13 returns.
Hence, we have that the algorithm correctly computes the distortion-rate function D∗(R).
28


--- Page 29 ---
B
Connections to information theory literature
Rate-distortion theory is an area of information theory introduced by Shannon [Sha59] to study the
fundamental limits of source compression. The simplest rate-distortion setup is shown in Fig. 9a:
We are given a source which generates samples X1, . . . , Xn independently and identically distributed
(i.i.d.) according to the distribution PX on the set X. We are also given a reconstruction alphabet
ˆ
X, which may or may not be equal to X. The goal is to compress Xn to a sequence of k elements
from an alphabet V, such that a reconstruction onto ˆ
X n is as “faithful” as possible, while keeping
k as small as possible (in the information theory literature, V = {0, 1} typically). The fidelity of
representation is quantified by a distortion function d : X × ˆ
X →[0, ∞]. For example, the problem
of compressing images with p real-valued pixels into bit sequences can be cast in this formulation by
taking X = ˆ
X = Rp, V = {0, 1}, and the squared-loss distortion function d(x, ˆx) = ∥x −ˆx∥2
2.
Formally, the goal is to construct an encoder enc : X n →Vk and a decoder dec : Vk →ˆ
X n such
that: (1) the rate k/n, and (2) the (average) distortion E [d(Xn, dec(enc(Xn))], are both as small
as possible. We say that the rate-distortion pair (R, D) is achievable for the source PX under the
distortion function d if there exists an (enc, dec) pair with rate at most R and average distortion at
most D. If the pair (R, D) is achievable, then clearly, for eR ≥R and eD ≥D, the pair ( eR, eD) is also
achievable. Thus, the quantity of interest to us is the lower boundary of the set of achievable (R, D)
pairs. This is given by the distortion-rate function D∗, which is defined as follows: the distortion-rate
function at rate R is the smallest distortion D such that the pair (R, D) is achievable, or equivalently,
D∗(R) ≜inf{D ≥0 | (R, D) is achievable}
(10)
= inf{D ≥0 | there exists (enc, dec) with rate ≤R and distortion ≤D}.
Note that the distortion-rate function depends on the source PX and the choice of distortion measure.
Closed form expressions are known in some cases, the reader is encouraged to refer to classical texts
on information theory [Ber71; CT06; EK11] for examples. It is important to note that the distortion-
rate function is a fundamental limit; no choice of encoder and decoder can give a lower rate and a
lower distortion. Thus, the distortion-rate function characterizes the Pareto-optimal front of the
trade-off between rate and distortion. It is more common in the information theory literature to
define the rate-distortion function R∗(D), which is the smallest rate R such that the pair (R, D) is
achievable. The two functions trace the same curve when plotted on the same two-dimensional plane.
Several variants of this problem can be defined by introducing the notion of side-information,
where we have i.i.d. samples (X1, Q1), . . . , (Xn, Qn) of a pair of correlated random variables (X, Q) ∼
PXQ ∈P(X × Q). A natural question to ask is what improvement is possible in terms of the
rate-distortion trade-off for Xn when either the encoder or decoder or both have access to this
side-information Qn, which is correlated with X. If only the encoder has access to Qn, then no
improvement can be obtained. If both the encoder and decoder have access to Qn as shown in
Fig. 9c and studied by Gray [Gra72], then, clearly, an improvement is possible. Surprisingly, we can
obtain nontrivial improvements when the decoder has access to Qn as shown in Fig. 9b and studied
by Wyner and Ziv [WZ76; Wyn78].
These models resemble our setups for query-aware and query-agnostic prompt compression
respectively, with a key difference being that the decoder in our problem is the pretrained LLM, which
is fixed. An rate-distortion setup that is closer to our problem in this sense is that of compression
for function computation, introduced by [Yam82]. Here, the goal is to recover an estimate ˆZn that
is close to Zn, with Zi = f(Xi, Qi) for some desired function f. At first glance, it might appear that
this setup is exactly our model for prompt compression, but this turns out to be false — the desired
output is an estimate of f(X, Q), but the decoder can be designed to compute any arbitrary function
29


--- Page 30 ---
enc
dec
Xn
M
ˆXn
(a) No side-information [Sha59]
enc
dec
Xn
M
ˆXn
Qn
(b) Side-information at only the decoder [WZ76;
Wyn78]
enc
dec
Xn
M
ˆXn
Qn
(c) Side-information at the encoder and the decoder
[Gra72]
enc
dec
Xn
M
ˆZn
Qn
(d) For function computation, Z = f(X, Q) [Yam82]
Figure 9: Rate-distortion models of compression.
of M and Q. In prompt compression, we have the constraint that the function computed by the
decoder is fixed to be ϕLLM, in addition to requiring that the output be close to some function of X
and Q. Thus, our model for prompt compression actually corresponds to a rate-distortion problem
for function computation with side-information with a fixed decoder, which has not been studied
before, to the best of our knowledge. The distortion-rate function D∗(R) for this setup is given by
(LP) and (dual-LP). A closed form expression for D∗(R) cannot be obtained without making further
assumptions on ϕLLM; nonetheless, D∗(R) can be computed for any ϕLLM by solving Algorithm 1.
C
Experiment details
C.1
Synthetic data experiments
We provide additional details regarding our synthetic dataset and how we fine-tuned all models.
Experiments were run on three different machines, two of which are identical machines with an
AMD Ryzen Threadripper PRO 5975WX CPU (32 cores), 256 GB of system RAM, and 2x Nvidia
RTX 4090 GPUs with 24 GB each. We also ran experiments on a DGX machine with an AMD
EPYC 7742 64-Core Processor, 512 GB of system RAM, and 4x 80GB SMX4 A100 GPUs. The
duration of LLM fine-tuning on the synthetic dataset varies, depending on the model being fine-
tuned. In general, it takes 10 to 30 minutes for a single fine-tuning run. Running the code necessary
to reproduce all plots takes several hours.
We use code from the LLMLingua and Selective Context GitHub repos, which are released under
the MIT license. In our experiments, we use the following models: Mistral 7B Instruct v0.2 (Apache-
2.0), RoBERTa Base (MIT), and Pythia 1B deduped (Apache-2.0).
Each
method
requires
a
rate
or
threshold
parameter
r,
for
which
we
use
r
∈
{0.04, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.96, 0.99, 1.0} in our experiments. However, the length
of the returned compressed prompt might not be faithful to this rate parameter, so for each r, we
report the average rate and average distortion on the examples in our validation dataset. LLMLin-
gua and LLMLingua Query have one additional parameter controlling the size of each segment the
prompt is broken into before compressing each segment. We found that using a segment size of 2
works best for our synthetic dataset.
30


--- Page 31 ---
C.1.1
Synthetic dataset
Solving for the optimal rate-distortion curve requires a substantial amount of compute as mentioned
in Sec. 4. To overcome this, we construct a synthetic dataset consisting of binary string prompts,
natural language queries, and numerical and yes/no answers. We show a few examples of the
validation partition of our synthetic dataset in Table 2.
Table 2: One example of each query from the validation set of our synthetic dataset
Prompt
Query
Answer
110011111
Count the number of 1s.
7
11111
Count the number of 0s.
0
00000111
Compute the parity.
1
11011111
What is the length of the
longest subsequence of 0s or
1s?
5
0110
Is the binary string a
palindrome?
Yes
1100111100
Count the number of
transitions from 0 to 1 and 1
to 0.
3
111111
Predict the next bit.
1
C.2
Natural language experiments
We train our models on the same DGX system used in the synthetic dataset experiments. Unlike
the synthetic dataset experiments, however, we train these XLM RoBERTa Large (MIT license)
models with the single precision (float32) data format and use full fine-tuning rather than LoRA.
We observed non-negligible performance improvements with this configuration over bfloat16 with
LoRA. As a result, training took one to two hours and 30 GB of VRAM on a single A100. We used
the same set or rate and threshold parameters as done in the synthetic data experiments.
C.2.1
Small natural language dataset
We curated a small natural language dataset to generate results shown in Fig. 6 by prompting GPT-
4 [OAA+24] to provide short natural language prompts of 15 tokens or less, provide four questions
about each prompt, and give the answer. Afterward, we modified some of the questions and prompts
slightly when the generated prompt by GPT-4 was too long or the questions and answers contained
too much overlap with each other for a given prompt. In total, our dataset consists of ten prompts
with four questions each. A few examples of our dataset are shown in Table 3.
C.2.2
Choice of distortion metric
The proper choice of distortion function is important to meaningfully measure the change in
performance (distortion) of the LLM as the rate is varied. Ideally, a distortion metric where two
texts with the “same meaning,” as would be determined by humans, achieve a “low distortion” is the
gold standard, but this metric is unknown. This is a crucial open problem not just for our work but
also for the fair benchmarking of LLMs in general.
31


--- Page 32 ---
Table 3: One example of each prompt from our natural language dataset.
Prompt
Query
Answer
After dinner, the cat chased a
mouse around the house.
What was the cat doing?
The cat was
chasing a mouse.
The dog barked loudly at the
passing mailman on a quiet
street.
Where did the barking
occur?
On a quiet street.
After school, the child played
with toys in the cozy living
room.
When was the child
playing?
After school.
At the art gallery, the artist
painted a colorful mural on
the wall.
Where was the painting
done?
On the wall at
the art gallery.
For our results on natural language, we report our results using the following distortion metrics
(or rather, “1 −” these, since these are similarity metrics and we want a low distortion to mean
high similarity). RougeL [Lin04] is a standard metric used to evaluate summaries, which does so
by computing the precision and recall between the tokens in the generated text and the reference
texts. In contrast, BertScore [ZKWWA20] computes contextualized embeddings for the generated
tokens and reference tokens and uses the pairwise cosine similarity among them to produce the final
score. The authors of the BertScore work highlight that their metric correlates better with human
judgements than all other metrics (rougeL included). Regardless, our results with these two metrics
are in agreement with each other, suggesting that a “good enough” metric may be sufficient. A popular
approach in current literature is to ask GPT4 to give a score on the similarity between generated and
reference texts. Although it has been shown that humans agree with GPT4’s evaluation more than the
evaluation of other humans [RSM+23], we are skeptical of this metric because it is not reproducible,
and GPT4 has biases that may result in unfair or inaccurate evaluations. Additionally, our theoretical
framework is general and does not assume any specific distortion function. In particular, it can also
be used with new distortion functions that better capture semantic notions when they are discovered.
C.3
LLM fine-tuning
C.3.1
Synthetic dataset
Given that our synthetic dataset of binary prompts is not naturally in the distribution of training
data of LLMs, we use Mistral 7B Instruct v0.2 [JSM+23] as our black-box model, and fine-tune
it on tuples of (prompt, query, answer). This is also known as “instruction fine-tuning;” we only
compute the loss over the answer.
Each prompt compression method requires an LLM as part of its compression algorithm; we
fine-tune Pythia 1B deduplicated [BSA+23] for Selective Context and LLMLingua-based methods.
Selective Context and LLMLingua only use negative log-likelihood scores over the prompt, so for
these methods we fine-tune with the next-word prediction over the prompts. For LLMLingua Query,
we place the (query, prompt, answer) tuple into context and then perform next token prediction over
the entire context. We place only the query and prompt into the context for the prompt compression
step (inference time).
LLMLingua-2 methods require an additional label set for every prompt as ground-truth answers
32


--- Page 33 ---
to teach the model to predict which tokens should be kept. For our dataset, gathering the labels
for each prompt is deterministic if the query is known, so it is easy to assemble the label set for
query-aware LLMLingua-2 methods. For example, for the query “Is the binary string a palindrome?”
we can easily choose the shortest sequence of tokens from the input that is also a palindrome (if the
answer is “yes”) as the ground-truth compressed prompt. For QuerySelect and Adaptive QuerySelect,
both of which are query-aware, we put the query and prompt into context and then train the LLM
to predict which tokens to keep using the constructed label set. This process is less straightforward
for query-agnostic LLMLingua-2 since it is not clear how to assign the labels without the query. In
this case, we choose the ground-truth compressed prompt to consist of the highest entropy tokens.
Given the Markov chain from which our prompts were generated, these tokens contain the transitions
between bits. For all LLMLingua-2 methods, we fine-tune RoBERTa Base [LOG+20].
We conduct a grid search over a set of hyperparameters before fine-tuning the final model used
for each method. Specifically, we use the training set to fine-tune a model with all combinations
of hyperparameters, evaluate the final performance on each model with a test set, and choose the
combination of hyperparameters leading to the best performance. We then merge the train and test
set and train with the chosen hyperparameters and do a final evaluation on the validation, which is
the dataset used in the results of this paper.
All models are searched over the same learning rate {5e−6, 1e−5, 5e−5, 1e−4}, batch size {16,
32}, LoRA rank {16, 32, 64, 128}, and LoRA alpha {16, 32, 64, 128} hyperparameters. For the
number of training epochs, we search over {1, 2, 4} for Mistral 7B Instruct v0.2 and Pythia 1B
deduplicated, and {8, 12} for RoBERTa Base.
We report our final set of hyperparameters used to fine-tune the LLM used for each prompt
compression method in Table 4.
C.3.2
Natural language dataset
We use fine-tuned models available on Hugging Face for the Selective Context, LLMLingua, LLM-
Lingua Query, and LLMLingua-2 prompt compression methods; only QuerySelect and Adaptive
QuerySelect, our novel methods, require specialized fine-tuning starting from a pretrained XLM
RoBERTa Large [LOG+20] model. To fine-tune these models, we modify the LLMLingua-2 training
code available in the LLMLingua GitHub repository to accept (prompt, query) pairs as input and
classify which tokens of the prompt should be kept. Since the query uses additional context, and
since the max context window of XLM RoBERTA Large is 512 tokens, we shrink the window size
dedicated to the prompt to 384 tokens. We used the same hyperparameters used in the LLMLingua-
2 paper [PWJ+24] for fine-tuning, i.e., learning rate 1e−5, batch size 10, epochs 10, and the Adam
optimizer with β1 = 0.9, β2 = 0.999, ϵ =1e−8, and weight decay 0.
The dataset used to train our models is a filtered version of Databricks Dolly 15k [CHM+23].
Our final filtered dataset contains samples that meet the following conditions: (1) the context is
non-empty, (2) the context is between 50 and 5000 characters in length, and (3) the instruction has
fewer than 360 characters. After filtering, the dataset contains 4.35k samples. The contexts of this
dataset are chunked into windows of 384 tokens, and we prompt GPT-4 to compress the context by
asking it only to keep the tokens necessary for responding to the provided instruction. This allows
us to construct a label set for the token classification problem of choosing which tokens to keep
or remove. Our dataset used for training consists of the instructions (queries), chunked contexts
(prompts), and the set of labels for the contexts. 95% of the samples in this dataset are used in
the training dataset, and the remaining 5% form the validation set. Table 5 shows the prompt we
used for GPT-4, which is a modified version of the prompt used in the LLMLingua-2 paper. All of
our code used for constructing the training dataset and training the models are modified from the
33


--- Page 34 ---
Table 4: Final set of hyperparameters used to train the LLM used in each prompt compression method.
Method
Tokenization
Epochs
Batch
Size
Learning
Rate
LoRA
Rank
LoRA
Alpha
Selective
Context
Standard
1
16
5e-5
32
32
Selective
Context
Forced
1
16
5e-5
128
64
LLMLingua
Standard
1
16
5e-5
32
32
LLMLingua
Forced
1
16
5e-5
128
64
LLMLingua
Query
Standard
4
32
1e-4
128
128
LLMLingua
Query
Forced
4
16
1e-4
64
128
LLMLingua-2
Standard
12
32
1e-4
128
128
LLMLingua-2
Forced
12
32
1e-4
64
128
QuerySelect
Standard
12
32
1e-4
128
128
QuerySelect
Forced
12
32
1e-4
64
128
Adaptive
QuerySelect
Standard
12
32
1e-4
128
128
Adaptive
QuerySelect
Forced
12
32
1e-4
64
128
Black-box
target LLM
Standard
4
16
5e-5
16
16
Black-box
target LLM
Forced
4
16
5e-6
16
64
LLMLingua GitHub repo. Our modified code is available in the code release.
D
Additional experimental results
D.1
Small-scale datasets
We provide the remainder of our empirical results below. Fig. 10 is similar to Fig. 1, but shows
the result when the prompt compression method uses standard tokenization rather than forced
tokenization. Fig. 11 shows a direct comparison between the trade-off for methods using standard
and forced tokenization. Fig. 12, Fig. 13, Fig. 14, and Fig. 15 show the rate-distortion trade-off
curves for each of the seven queries in our synthetic dataset. Fig. 12 shows forced tokenization with
0/1 loss, Fig. 13 shows forced tokenization with log loss, Fig. 14 shows standard tokenization with
0/1 loss, and Fig. 15 shows standard tokenization with log loss.
34


--- Page 35 ---
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0
1
2
3
4
5
6
Average distortion
Log loss
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0.0
0.1
0.2
0.3
0.4
0/1 loss
Selective
LLMLingua
LLMLingua Query
LLMLingua-2
QuerySelect
Adaptive QuerySelect
Optimal
Optimal (Query-aware)
No Compression
Figure 10: The distortion-rate curves of all prompt compression methods and the optimal strategy
attained by solving our dual LP formulation when standard tokenization is used for the prompt. All
methods are compared with the log loss (left) and 0/1 loss (right) metrics.
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0.0
0.1
0.2
0.3
0.4
0.5
Average distortion (0/1 loss)
Standard tokenization
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0.0
0.1
0.2
0.3
0.4
0.5
Forced tokenization
Selective
LLMLingua
LLMLingua Query
LLMLingua-2
QuerySelect
Adaptive QuerySelect
Optimal
Optimal (Query-aware)
No Compression
Figure 11: Performance comparison when standard tokenization (left) and forced tokenization
(right) is used on our synthetic dataset. Interestingly, the optimal performance is nearly equivalent
between the two, and, for a given rate, methods with standard tokenization match or improve upon
the performance of a method that forced separate tokenization of every bit. However, standard
tokenization results in compression of 1 to 4 tokens on our dataset, whereas forced tokenization
compresses up to 10 tokens, allowing for a greater range of rates.
35


--- Page 36 ---
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Average distortion (0/1 loss)
Count the number of 1s.
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Count the number of 0s.
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.1
0.2
0.3
0.4
0.5
Average distortion (0/1 loss)
Compute the parity.
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
What is the length of the longest subsequence of 0s or 1s?
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0.00
0.05
0.10
0.15
0.20
Average distortion (0/1 loss)
Is the binary string a palindrome?
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0.0
0.1
0.2
0.3
0.4
0.5Count the number of transitions from 0 to 1 and 1 to 0.
0.0
0.2
0.4
0.6
0.8
1.0
0.10
0.15
0.20
0.25
0.30
0.35
Average distortion (0/1 loss)
Predict the next bit.
Common Legend
Selective
LLMLingua
LLMLingua Query
LLMLingua-2
QuerySelect
Adaptive QuerySelect
Optimal (Query-aware)
No Compression
Figure 12: The rate-distortion trade-off of all methods on each individual query for forced tokenization
and 0/1 loss.
36


--- Page 37 ---
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
Average distortion (log loss)
Count the number of 1s.
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
10
Count the number of 0s.
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Average distortion (log loss)
Compute the parity.
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
10
What is the length of the longest subsequence of 0s or 1s?
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
Average distortion (log loss)
Is the binary string a palindrome?
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0Count the number of transitions from 0 to 1 and 1 to 0.
0.0
0.2
0.4
0.6
0.8
1.0
0.3
0.4
0.5
0.6
0.7
Average distortion (log loss)
Predict the next bit.
Common Legend
Selective
LLMLingua
LLMLingua Query
LLMLingua-2
QuerySelect
Adaptive QuerySelect
Optimal (Query-aware)
No Compression
Figure 13: The rate-distortion trade-off of all methods on each individual query for forced tokenization
and log loss.
37


--- Page 38 ---
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.1
0.2
0.3
0.4
0.5
Average distortion (0/1 loss)
Count the number of 1s.
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Count the number of 0s.
0.0
0.2
0.4
0.6
0.8
1.0
0.00
0.05
0.10
0.15
0.20
0.25
Average distortion (0/1 loss)
Compute the parity.
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
What is the length of the longest subsequence of 0s or 1s?
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0.00
0.05
0.10
0.15
0.20
Average distortion (0/1 loss)
Is the binary string a palindrome?
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0.0
0.1
0.2
0.3
0.4
Count the number of transitions from 0 to 1 and 1 to 0.
0.0
0.2
0.4
0.6
0.8
1.0
0.10
0.15
0.20
0.25
0.30
Average distortion (0/1 loss)
Predict the next bit.
Common Legend
Selective
LLMLingua
LLMLingua Query
LLMLingua-2
QuerySelect
Adaptive QuerySelect
Optimal (Query-aware)
No Compression
Figure 14: The rate-distortion trade-off of all methods on each individual query for standard
tokenization and 0/1 loss.
38


--- Page 39 ---
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
Average distortion (log loss)
Count the number of 1s.
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
Count the number of 0s.
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Average distortion (log loss)
Compute the parity.
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
10
12
What is the length of the longest subsequence of 0s or 1s?
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0.0
0.5
1.0
1.5
2.0
2.5
Average distortion (log loss)
Is the binary string a palindrome?
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0
1
2
3
4
5
Count the number of transitions from 0 to 1 and 1 to 0.
0.0
0.2
0.4
0.6
0.8
1.0
0.3
0.4
0.5
0.6
0.7
0.8
Average distortion (log loss)
Predict the next bit.
Common Legend
Selective
LLMLingua
LLMLingua Query
LLMLingua-2
QuerySelect
Adaptive QuerySelect
Optimal (Query-aware)
No Compression
Figure 15: The rate-distortion trade-off of all methods on each individual query for standard
tokenization and log loss.
39


--- Page 40 ---
0.0
0.2
0.4
0.6
0.8
1.0
Binned rates len(M)
len(X)
0
5
10
15
20
25
30
35
40
Frequency
Average ratio: 0.27
QuerySelect
Adaptive QuerySelect
0.0
0.2
0.4
0.6
0.8
1.0
Binned rates len(M)
len(X)
0
5
10
15
20
25
30
35
40
Frequency
Average ratio: 0.57
QuerySelect
Adaptive QuerySelect
0.0
0.2
0.4
0.6
0.8
1.0
Binned rates len(M)
len(X)
0
5
10
15
20
25
30
35
40
Frequency
Average ratio: 0.77
QuerySelect
Adaptive QuerySelect
0.0
0.2
0.4
0.6
0.8
1.0
Binned rates len(M)
len(X)
0
5
10
15
20
25
30
35
40
Frequency
Average ratio: 0.87
QuerySelect
Adaptive QuerySelect
Figure 16: Histograms of the rates for QuerySelect and Adaptive QuerySelect for a given average rate
from the left-side figure of Fig. 6. These figures show that Adaptive QuerySelect has a larger spread
of rates across the samples of the natural language dataset. In particular, Adaptive QuerySelect has
greater flexibility in choosing an appropriate rate for a given (prompt, query) pair.
40


--- Page 41 ---
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0.775
0.800
0.825
0.850
0.875
0.900
0.925
0.950
Average distortion
1 - RougeL
0.0
0.2
0.4
0.6
0.8
1.0
Average rate E
h
len(M)
len(X)
i
0.130
0.135
0.140
0.145
0.150
0.155
0.160
1 - BertScore
Selective
LLMLingua
LLMLingua Query
LLMLingua-2
QuerySelect
Adaptive QuerySelect
No Compression
Figure 17: Comparison among all prompt compression methods on the NarrativeQA [KSB+17]
summaries dataset. We show the rate-distortion trade-off for RougeL [Lin04] (left) and BertScore
[ZKWWA20] (right). Since a higher RougeL and BertScore metric is better, we plot “1−the
computed average distortion” so that a higher rate should yield a lower loss. (Left plot same as
Fig. 7, without the approximation to the optimal curve.)
D.2
NarrativeQA
We compare existing methods on the NarrativeQA [KSB+17] dataset. Specifically, we use the
summaries from the dataset as the prompt, and use the query and answer as given. Since the sizes of
the prompts are hundreds of tokens and approximately 3,500 samples, it is not feasible to compute
the optimal rate-distortion curve for this dataset. Instead, Fig. 17 showcases the performance of our
proposed methods over existing methods. In particular, Fig. 17 shows the same conclusion as Fig. 6:
our proposed methods are better for rates less than 0.5 and remain competitive for rates about 0.5.
Table 6 shows the average time required to compress a prompt on the NarrativeQA dataset and our
curated small-scale NLP dataset. Since our methods are adapted from LLMLingua-2, our methods
share the same timings. We compute approximations to the optimal rate-distortion trade-off curves
in Sec. 4.2.3.
41


--- Page 42 ---
Table 5: Our prompt for GPT-4 to determine which tokens to remove. This prompt was used to
construct the dataset for training QuerySelect and Adaptive QuerySelect on natural language. This
is a slight modification from the prompt used in the LLMLingua-2 work [PWJ+24].
System Prompt
You are an excellent linguist and very good at com-
pressing passages into short expressions by removing
unimportant words, while retaining as much informa-
tion as possible.
User Prompt
Compress some text to short expressions, such that you
(GPT-4) can answer the query based on the compressed
text. Unlike the usual text compression, I need you to
comply with the 5 conditions below:
1. You can ONLY remove unimportant words.
2. Do not change the order of words.
3. Do not change the original words, e.g., ‘asking’ →
‘ask’ is NOT OK; ‘current’ →‘now’ is NOT OK.
4. Do not use abbreviations or emojis, e.g., ‘without’
→‘w/o’ is NOT OK; ‘as soon as possible’ →‘ASAP’
is NOT OK.
5. Do not add new words or symbols, this is very im-
portant. For example, ‘dedicate 3 hours to each
chapter’ →‘3 hours/chapter’ is NOT OK because
you add new token ‘/’, just compress it into ‘3 hours
each chapter’. ’30 eggs plus 20 eggs equals 50 eggs’
→‘30+20=50’ is also NOT OK because you add
new symbols + and =; just compress it into ‘30 plus
20 equals 50’.
Compress the origin aggressively by removing words
only. Please output the compressed text directly. Com-
press the origin as short as you can, while retaining
ONLY the information needed to answer the following
query: {query}.
If you understand, please compress the following text:
{text_to_compress}
The compressed text is:
Table 6: Average time required to compress a single prompt (seconds).
Method
NarrativeQA
Small NLP Dataset
Selective
1.043
0.049
LLMLingua
0.510
0.273
LLMLingua Query
1.060
0.530
LLMLingua-2
0.113
0.044
QuerySelect
0.114
0.043
Adaptive QuerySelect
0.114
0.043
42
