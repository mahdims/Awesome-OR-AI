--- Page 1 ---
A Convex-optimization-based Layer-wise Post-training
Pruner for Large Language Models
Pengxiang Zhao∗
Department of Mathematics
The University of Hong Kong
pengxiangzhao@connect.hku.hk
Hanyu Hu∗
Department of Mathematics
The University of Hong Kong
hhy1224@connect.hku.hk
Ping Li
System AI Innovation Lab
Huawei Cloud
liping61@huawei.com
Yi Zheng
System AI Innovation Lab
Huawei Cloud
zhengyi29@huawei.com
Zhefeng Wang
System AI Innovation Lab
Huawei Cloud
wangzhefeng@huawei.com
Xiaoming Yuan†
Department of Mathematics
The University of Hong Kong
xmyuan@hku.hk
Abstract
Pruning is a critical strategy for compressing trained large language models (LLMs),
aiming at substantial memory conservation and computational acceleration without
compromising performance. However, existing pruning methods often necessitate
inefficient retraining for billion-scale LLMs or rely on heuristic methods such
as the optimal brain surgeon framework, which degrade performance. In this
paper, we introduce FISTAPruner, the first post-training pruner based on convex
optimization models and algorithms. Specifically, we propose a convex optimiza-
tion model incorporating ℓ1 norm to induce sparsity and utilize the FISTA solver
for optimization. FISTAPruner incorporates an intra-layer cumulative error cor-
rection mechanism and supports parallel pruning. We comprehensively evaluate
FISTAPruner on models such as OPT, LLaMA, LLaMA-2, and LLaMA-3 with
125M to 70B parameters under unstructured and 2:4 semi-structured sparsity,
demonstrating superior performance over existing state-of-the-art methods across
various language benchmarks.
1
Introduction
In recent years, large language models (LLMs) have revolutionized natural language processing
fields, achieving impressive results in tasks such as machine translation, sentiment analysis, question
answering, and text generation (Lyu et al., 2023; Yao et al., 2023; Zhang et al., 2023a,b; Wang et al.,
2023; Arefeen et al., 2024; Li et al., 2024). Advanced LLMs such as OpenAI’s GPT-4 (OpenAI,
2023), Meta’s LLaMA-3 (AI, 2023), and Google’s Gemini (Team et al., 2023) excel in generating
coherent text with extensive parameters. However, the growth in model sizes outpaces hardware
improvements, posing significant deployment and inference challenges (Steiner et al., 2023). For
example, operating OPT-175B (Zhang et al., 2022) requires over 320GB of memory and at least
∗Equal contribution
†Corresponding author
Preprint. Under review.
arXiv:2408.03728v1  [cs.LG]  7 Aug 2024


--- Page 2 ---
FISTAPruner
Input
Attention
FFN
+
+
weights
activations
Input
Attention
FFN
+
+
weights
pruned
Figure 1: Overview of the proposed FISTAPruner. Given a weight matrix W and its corresponding
input feature activation X, we employ the proposed convex optimization model, utilizing FISTA, to
derive the pruned weights.
five 80GB A100 GPUs for loading its parameters in FP16 precision. This challenge becomes more
pronounced in environments with limited resources, such as mobile devices, edge computing systems,
and real-time applications. Consequently, there has been considerable interest in compressing LLMs
to enhance their efficiency and practicality for deployment across various applications.
Pruning is a key method for compressing LLMs, aiming to eliminate redundant weights to reduce
model size and computational demands while striving to maintain performance. Methods such as those
in (Huang et al., 2020; Ma et al., 2023; Zhang et al., 2023c) require a retraining phase post-pruning,
which is inefficient for billion-scale LLMs. Recent developments, including SparseGPT (Frantar
and Alistarh, 2023) and Wanda (Sun et al., 2023), employ one-shot post-training pruning techniques
for LLMs. These methods, however, rely on the heuristic-based optimal brain surgeon (OBS)
framework (Hassibi and Stork, 1992) or utilize heuristic-based pruning metric to determine which
weights to prune, potentially compromising performance.
In this work, we introduce a novel convex optimization model for layer-wise post-training pruning of
LLMs. Figure 1 provides an overview of our method, which is applied to each linear operator, such as
WK, WQ, WV , and WO within the Transformer’s attention blocks (Vaswani et al., 2017). We employ
the Frobenius norm of the difference between the outputs obtained from the dense and pruned weights
to quantify the output error. Additionally, we integrate an ℓ1-norm regularization term, the optimal
convex approximation of the ℓ0-norm (Candès et al., 2006), into each row of weights to promote
sparsity. The solutions of the proposed optimization model demonstrate a balanced trade-off between
output error and sparsity, governed by our proposed adaptive tuning method that meticulously adjusts
the hyperparameter λ. To solve this optimization problem efficiently, we utilize the Fast Iterative
Shrinkage-Thresholding Algorithm (FISTA) (Beck and Teboulle, 2009), which ensures a convergence
rate of O(1/k2). Following this, we name our proposed method FISTAPruner.
In addition, our approach effectively addresses the cumulative error resulting from compression
within the decoder layers by incorporating an error correction mechanism. Specifically, there exists a
discrepancy between the outputs of the dense and the pruned weights. This error accumulates since
the output from one pruned weight serves as the input for the subsequent operator. To mitigate this
accumulated error, FISTAPruner sequentially prunes the weights of each linear operator within a
decoder layer. It utilizes the output from the pruned weights of the preceding operator as the input
activation for the ongoing pruning process, thereby minimizing the gap between the output weight
being pruned and its counterpart in the dense model. Moreover, FISTAPruner treats each decoder
layer as an independent pruning unit, supporting the simultaneous pruning of multiple decoder layers,
which significantly enhances efficiency.
We empirically evaluate FISTAPruner on the widely adopted OPT (Zhang et al., 2022), LLaMA (Tou-
vron et al., 2023a), and LLaMA-2 (Touvron et al., 2023b) model families, as well as the latest
LLaMA-3 (Touvron et al., 2023a) models. FISTAPruner’s layer-by-layer pruning implementation
allows for the pruning of these LLMs ranging from 125M to 70B parameters on a single NVIDIA
A100 GPU with 40GB of memory. Our results confirm that FISTAPruner can efficiently create sparse
networks from pretrained LLMs without retraining. Moreover, our approach exceeds the performance
of state-of-the-art methods such as SparseGPT and Wanda across various language benchmarks. We
2


--- Page 3 ---
believe our work sets a new direction and baseline for future research in this area and encourages
further exploration into understanding sparsity in LLMs with the tools of convex optimization.
2
Background and Related Work
Pruning of LLMs. Pruning is a widely used strategy to compress LLMs by generating sparse
weight matrices under unstructured, semi-structured, and structured sparsity based on calibration data.
Unstructured sparsity of rate s%, eliminates s% of the entries in a weight matrix. Semi-structured
sparsity with proportion n : m maintains a fixed overall sparsity level n/m, and allows at most
n non-zero entries in every group of m consecutive entries. Pruning weights into semi-structured
sparsity, especially with proportion 2:4, could yield up to 2× inference speedup using NVIDIA GPUs
with the Ampere architecture (Mishra et al., 2021) and hence is of particular interest. Structured
sparsity, which zeroes entire rows or columns, offers significant computational and memory benefits
but can lead to greater performance losses. Depending on whether to incorporate another round
of training after pruning to recover performance, pruning schemes could be further classified into
pruning with retraining and one-shot pruning.
Pruning with Retraining. Traditional pruning pipelines often include a retraining step to offset
performance losses (Huang et al., 2020; Ma et al., 2023; Zhang et al., 2023c). However, the sheer
scale of LLMs makes this additional retraining costly in terms of both time and computational
resources. (Dinh et al., 2020; Holmes et al., 2021; Xie et al., 2023) integrate retraining directly into
the pruning process by targeting the minimization of the highly non-convex loss function related
to the calibration dataset, using the alternating direction method of multipliers (ADMM) to derive
pruned weights. Nonetheless, this approach imposes significant computational demands and the use
of ADMM in non-convex optimization often results in unstable performance (He and Yuan, 2012).
One-Shot Pruning. One-shot pruning offers a straightforward alternative, eliminating the need
for post-pruning retraining. These methods prune LLMs in a single step, simplifying implemen-
tation and reducing both time and computational demands. Consequently, various one-shot prun-
ing algorithms have been developed under different sparsity frameworks. For structured pruning,
SliceGPT (Ashkboos et al., 2024) and Eigenpruning (Vergara-Browne et al., 2024) utilize singular
value decompositions to prune singular values of weight matrices and reduce model dimensions.
ZipLM (Kurti´c et al., 2024) adopts an OBS-based approach for structured pruning and updates
remaining weights to maintain performance. Our proposed FISTAPruner focuses on unstructured
and semi-structured pruning, and thus is orthogonal to these structured pruning methods, enabling
further model compression. For unstructured and semi-structured pruning, SparseGPT (Frantar and
Alistarh, 2023) and ISC (Shao et al., 2024) leverage the OBS framework to calculate saliency for each
entry using the inverse Hessian of the loss metric, based on which pruning masks are generated and
weights updated. Wanda (Sun et al., 2023) implements a heuristic approach, removing weights based
on the product of their magnitudes and activations without compensation. (Boža, 2024) employs
ADMM to optimize weight updates under iteratively refined pruning masks chosen through heuristic
methods. These strategies adopt a layer-wise pruning strategy, where errors between the pruned
output and the original output of each operator accumulates. Moreover, due to their heuristic nature,
the performances of the pruned models are unstable and compromised. In contrast, our proposed
FISTAPruner introduces a novel layer-wise one-shot pruning approach by formulating it as a convex
optimization problem with intra-layer error corrections. It employs the FISTA solver to efficiently
compute optimal pruned weights, providing theoretical guarantees for performance stability and
effectiveness.
3
Methodology
In this section, we present our proposed post-training pruning method, FISTAPruner, which comprises
three main components. First, we identify the error accumulation problem in layer-wise pruning
and propose an intra-layer error correction mechanism to address it. Based on this mechanism, we
establish a novel convex optimization model tailored for layer-wise pruning. We then detail the steps
for solving this optimization problem using the FISTA solver. Finally, we describe our adaptive
method that finely tunes the hyperparameter λ in our model, aiming to minimize the output error
between dense and pruned operators while achieving the desired sparsity level.
3


--- Page 4 ---
output error
output error
Figure 2: Illustration of the proposed intra-layer error correction mechanism. W1 and W2 represent
the weights of two sequential layers within the network architecture.
3.1
Post-Training Pruning Model with Intra-layer Error Corrections
Post-training compression is typically achieved by decomposing the full-model compression problem
into layer-wise subproblems (Frantar and Alistarh, 2023). For instance, a typical Transformer decoder
layer (Vaswani et al., 2017) comprises six crucial linear operators: WK, WQ, WV , WO, Wfc1, and
Wfc2. We individually prune each of these operators to eliminate redundant weights while striving
to preserve their intended functionality. Consider a linear operator with weight W ∈Rm×n from
the dense model and the corresponding input feature activation X ∈Rn×p. Its output is computed
by WX ∈Rm×p. Denoting the pruned counterpart by W ∗∈Rm×n, a straightforward approach to
quantify the output error is to use the Frobenius norm of the difference between the outputs from the
dense and pruned weights
∥W ∗X −WX∥F ,
(1)
which serves as a metric of the pruning quality at the target sparsity level and is widely adopted by
work such as (Frantar and Alistarh, 2023; Boža, 2024).
However, we observe that applying (1) can lead to an error accumulation issue across sequential
operators, as illustrated in Figure 2. In the figure, W1 and W2 represent the weights of two sequential
operators. Although (1) effectively quantifies the output error between W1 and its pruned counterpart
W ∗
1 since they are at the top of the layer and share the same inputs, issues arise when applying the
same metric to the outputs of W2 and W ∗
2 . Following (1), the deviation between the outputs of W2
and W ∗
2 is computed with the same input W1X. However, in a pruned network, the actual input for
W ∗
2 is W ∗
1 X, creating a discrepancy with W1X and thus leading to error propagation through the
operators. To address this, we propose a method to sequentially prune weights within each pruning
unit (e.g., a decoder layer of a Transformer), updating (1) to:
∥W ∗X∗−WX∥F ,
(2)
where X∗represents the input feature activation for W ∗from the sequentially pruned network.
Pruning essentially transforms dense weight matrices into sparse structures. The ℓ0-norm, which
directly counts the number of non-zero entries in a vector, is the most straightforward measure of
sparsity. However, because it leads to non-convex and NP-hard optimization problems, we turn to its
optimal convex approximation, the ℓ1-norm (Candès et al., 2006). Specifically, to effectively induce
sparsity while ensuring computational feasibility in the pruned weights W ∗, we apply the ℓ1-norm
to each row of W ∗, thereby promoting sparsity throughout the matrix (see Appendix A for detailed
explanations):
W ∗
i,:

1 , i = 1, 2, . . . , m,
(3)
where W ∗
i,: represents the i-th row of W ∗.
Then, we construct our optimization model by integrating (2) and (3)
min
W ∗∈Rm×n
1
2∥W ∗X∗−WX∥2
F + λ
m
X
i=1
∥W ∗
i,:∥1.
(4)
This model aims to simultaneously minimize both the output error and the sum of the ℓ1-norm values
while the hyperparameter λ > 0 balances these two terms.
Remark 3.1. The proposed optimization model (4) is convex. This is due to the fact that the square
of the Frobenius norm is a convex function, as is the ℓ1-norm. Consequently, the objective function,
being a sum of these two convex functions, is also convex. Since the problem is an unconstrained
optimization with a convex objective function, the overall optimization model is convex.
4


--- Page 5 ---
3.2
Optimization based on FISTA
We apply FISTA (Beck and Teboulle, 2009) to solve the proposed model (4) efficiently. Specifically,
starting with t0 = 1 and an initial W ∗
0 , the k-th iteration of FISTA reads:
FISTA for (4)























W ∗
k+ 1
3 = W ∗
k −1
L
 W ∗
k X(X∗)⊤−WX(X∗)⊤
,
W ∗
k+ 2
3 = SoftShrinkage λ
L

W ∗
k+ 1
3

,
tk+1 = 1
2

1 +
q
1 + 4t2
k

,
W ∗
k+1 = W ∗
k+ 2
3 + tk −1
tk+1

W ∗
k+ 2
3 −W ∗
k

,
(5a)
(5b)
(5c)
(5d)
where L = ∥X∗(X∗)⊤∥2 is the maximum eigenvalue of X∗(X∗)⊤and the SoftShrinkageρ(·) oper-
ator with parameter ρ ≥0 on a matrix X = (xij) ∈Rm×n performs elementwise transformations
defined by
SoftShrinkageρ(X) = X′, where x′
ij =



xij −ρ, if xij > ρ,
xij + ρ, if xij < −ρ,
xij = 0, otherwise.
Step (5a) executes a gradient descent update on the parameter W ∗
k , aiming to minimize the function
1/2∥W ∗
k X∗−WX∥2
F with a step size of 1/L. Step (5b) does a proximal update, defined as:
W ∗
k+ 2
3 = arg min
W ∗
(
L
2
W ∗−W ∗
k+ 1
3

2
F + λ
m
X
i=1
W ∗
i,:

1
)
.
(6)
Steps (5c) and (5d) calculate a linear combination of the previous two points,
n
W ∗
k+ 2
3 , W ∗
k
o
, to
facilitate accelerated convergence. Detailed derivations of these steps are provided in Appendix B.
The FISTA iteration terminates either when the maximum number of iterations, K, is reached or
when the following stopping criterion is satisfied:
W ∗
k −W ∗
k−1

F < 1 × 10−6.
(7)
Remark 3.2. FISTA is proven to achieve a convergence rate of O(1/k2) (Beck and Teboulle, 2009).
This indicates that the distance between the computed solution and the optimal solution decreases
proportionally to 1/k2 as the number of iterations, k, increases.
Due to the floating-point representation limitations in computers, values near zero computed by
FISTA may not be expressed precisely as zero. This imprecision can affect calculations of sparsity,
in which exact zeros are counted. To correct this numerical error, a rounding step is implemented
after the FISTA iterations to adjust values intended for pruning to exact zeros. Specifically, for the
final result W ∗
K at the K-th iteration and unstructured pruning at sparsity level s%, the rounding step
sets the s% elements with the smallest absolute values in W ∗
K to zero. For n : m semi-structured
sparsity, the rounding step targets the n elements with the least absolute values within every group of
m elements in a row for zeroing. We express the rounding step as:
W ∗
K+1 = round (W ∗
K, s% or n : m) ,
(8)
where round(·) denotes the operation for correcting numerical errors according to the designated
sparsity configuration.
3.3
Adaptive Hyperparameter Tuning
As mentioned, the hyperparameter λ regulates the balance between the two terms in the model (4). In
addition, the introduction of the rounding step (8) for numerical error corrections also implies that
the value of λ indirectly affects the precision loss in this step, especially when the sparsity of W ∗
K
achieved through FISTA falls significantly short of the target sparsity.
5


--- Page 6 ---
Algorithm 1 FISTAPruner
Inputs: original output WX, input activation X∗, λ, W ∗
0 , K, T, ϵ, s% or n : m
t ←0; W ∗
best ←W ∗
0 ; Ebest ←∥W ∗
0 X∗−WX∥F
repeat
W ∗
K ←FISTA (WX, X∗, λ, W ∗
best, K)
# FISTA iterations as in Section 3.2
W ∗
K+1 ←round (W ∗
K, s% or n : m)
# rounding step for numerical errors as in Section 3.2
Etotal ←
W ∗
K+1X∗−WX

F
# compute the total error
Eround ←Etotal −∥W ∗
KX∗−WX∥F
# compute the rounding error
if Etotal < Ebest then
W ∗
best ←W ∗
K+1
# preserve the best solution
Estop = (Ebest −Etotal)/Ebest
# compute the stop condition
Ebest ←Etotal
# update the best total error
else
t ←t + 1
# update the number of steps without improvement
end if
update λ by bisection based on Eround/Etotal as in Section 3.3
until t ≥T or Estop < ϵ
return W ∗
best
Specifically, increasing λ intensifies the focus on the ℓ1-norm within model (4), leading to higher
sparsity in W ∗
K but potentially increasing the output error. Conversely, decreasing λ shifts focus
towards minimizing the output error, which results in lower sparsity but enhances output accuracy.
However, sparsity levels in W ∗
K that are lower than the target can lead to increased rounding errors
in (8), potentially raising the total error.
To finely adjust λ for minimizing output discrepancy while achieving a target sparsity level, we
introduce an adaptive tuning method. We define the total error Etotal and the rounding error Eround as
Etotal :=
W ∗
K+1X∗−WX

F , Eround := Etotal −∥W ∗
KX∗−WX∥F .
(9)
Building on the previous analysis, a high ratio of Eround/Etotal suggests that a great portion of the error
originates from the rounding step (8), indicating that the sparsity of W ∗
K achieved through FISTA is
below the target. This implies that the current value of λ should be increased to enhance the emphasis
on the ℓ1-norm in the model (4). Conversely, a low ratio of Eround/Etotal suggests that the achieved
sparsity in W ∗
K is sufficient, indicating that a reduction in λ could shift the focus of model (4) towards
minimizing output error and thus reduce the total error.
Incorporating the above insights and applying a threshold ξ for Eround/Etotal, we adaptively tune λ
with the bisection method on [0, 106], where ξ is set at 0.3 in our experiments.
3.4
FISTAPruner Pseudocode
We treat each decoder layer as an independent pruning unit, enabling parallel pruning across multiple
decoder layers on different devices, which significantly enhances the efficiency. Within each decoder
layer, the proposed FISTAPruner sequentially prune weights to eliminate error accumulations, as
detailed in Section 3.1. Algorithm 1 presents FISTAPruner for the dense weight matrix W. It leverages
FISTA to generate candidate sparse weights based on the model (4), as detailed in Section 3.2. It
then rounds these weights to address numerical errors from floating-point representation and to meet
specified sparsity constraints (either s% unstructured or n : m semi-structured sparsity). Additionally,
the parameter λ is adaptively tuned, as detailed in Section 3.3, to optimize the trade-off between
output error and sparsity. The algorithm iteratively updates the weights, preserving the best solution
W ∗
best, based on the lowest total error Etotal. It terminates when the number of consecutive iterations
without an improvement in W ∗
best reaches T, or when the improvement ratio (Ebest −Etotal)/Ebest falls
below the threshold ϵ.
6


--- Page 7 ---
4
Experiments
In this section, we detail a comprehensive set of experiments designed to validate the efficacy of
FISTAPruner. We begin with an in-depth review of our experimental setup. Following this, we
explore the perplexity and zero-shot capabilities of the pruned LLMs through rigorous testing and a
series of ablation studies. Due to page length constraints, a portion of the results are presented in
Appendix C.1, C.2 and C.3.
4.1
Settings
Models. We utilize models from the OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023a),
LLaMA-2 (Touvron et al., 2023b), and LLaMA-3 (AI, 2023) families. Specifically, we assess our
method across OPT-125M/350M/1.3B/2.7B/6.7B/13B/30B, LLaMA-7B/13B/30B/65B, LLaMA-2-
7B/13B/70B, and LLaMA-3-8B/70B models.
Benchmarks. Our primary assessment focuses on evaluating the perplexity of pruned LLMs, a
metric renowned for its reliability in assessing LLM performance. Following methodologies from
previous studies (Frantar and Alistarh, 2023; Sun et al., 2023), we measure model perplexity using
the WikiText-2-raw (Merity et al., 2016) (hereafter shortened to WikiText), PTB (Marcus et al.,
1994), and C4 (Raffel et al., 2020) datasets. Additionally, we perform a comprehensive evaluation of
the zero-shot capabilities of pruned LLaMA-3-70B models using several standard common-sense
benchmark datasets. These include ARC Easy (Clark et al., 2018), ARC Challenge (Clark et al.,
2018), WinoGrande (Sakaguchi et al., 2021), BoolQ (Clark et al., 2019), RTE (Wang et al., 2018),
QNLI (Wang et al., 2018), and WNLI (Wang et al., 2018) tasks, facilitated by the LM Harness
library (Gao et al., 2021).
Baselines. We compare the FISTAPruner with two state-of-the-art pruning methods as baselines:
SparseGPT (Frantar and Alistarh, 2023) and Wanda (Sun et al., 2023). We evaluate two types of
sparsity configurations: unstructured and 2:4 semi-structured sparsity.
Setup. We implement FISTAPruner using PyTorch (Paszke et al., 2019) and leverage the HuggingFace
Transformers library (Wolf et al., 2019) for model and dataset management. All pruning experiments
are conducted on NVIDIA A100 GPUs, each equipped with 80GB of memory. We observe that
FISTAPruner efficiently prunes all LLMs using a single GPU and no more than 40GB of memory.
Conversely, SparseGPT can also prune all selected LLMs on a single GPU, while Wanda requires at
least four GPUs to prune large models, such as LLaMA-2-70B and LLaMA-3-70B, in their original
implementation. For calibration data, we adhere to the approach outlined in previous works (Frantar
and Alistarh, 2023; Sun et al., 2023), utilizing 128 sequences. Each sequence is composed of tokens
sampled from the first shard of the C4 dataset, with the number of tokens equal to the maximum
embedding length of the LLMs. For parameters of FISTAPruner, we set the initial value of λ to
1 × 10−5, K to 20, and T to 3. For the OPT model family, we use the result of SparseGPT as a warm
start for the FISTA iteration and set ϵ to 1 × 10−6. For the LLaMA model family, we use the result
of Wanda as a warm start and set ϵ to 1 × 10−3.
4.2
Perplexity Experiment Results
In Tables 1 and 2, we present the perplexity results for the pruned OPT, LLaMA, LLaMA-2, and
LLaMA-3 models of various sizes on WikiText. For results on PTB and C4, please refer to Ap-
pendix C.1 and C.2. We achieved a 50% unstructured or 2:4 semi-structured sparsity level by pruning
all linear operators, excluding embeddings and the model head. The data in Tables 1 and 2 illustrate
consistent improvements with FISTAPruner over existing methods.
To further investigate FISTAPruner’s performance under different unstructured sparsity levels, we
conducted experiments on the OPT-125M and LLaMA-3-8B models, with perplexity results visualized
in Figure 3 and measured using WikiText. The results indicate that FISTAPruner consistently
outperforms existing methods across different levels of unstructured sparsity. Notably, at 20%
unstructured sparsity on the OPT-125M model, FISTAPruner’s performance even surpasses that of
the dense network.
7


--- Page 8 ---
OPT
Method
Sparsity
125M
350M
1.3B
2.7B
6.7B
13B
30B
Dense
0%
27.66
22.00
14.63
12.47
10.86
10.13
9.56
SparseGPT
50%
37.01
31.53
17.55
13.46
11.60
11.15
9.77
Wanda
50%
38.96
36.22
18.41
14.22
11.98
11.93
10.03
FISTAPruner
50%
33.54
28.89
17.21
13.22
11.36
10.95
9.71
SparseGPT
2:4
60.02
50.15
23.83
17.20
14.13
12.94
10.92
Wanda
2:4
80.32
113.00
28.25
21.25
15.90
15.56
13.40
FISTAPruner
2:4
45.16
40.41
22.46
15.70
13.16
12.21
10.54
Table 1: WikiText perplexity (↓) of pruned OPT models under 50% unstructured and 2:4 semi-
structured sparsity. FISTAPruner outperforms state-of-the-art methods.
LLaMA
LLaMA-2
LLaMA-3
Method
Sparsity
7B
13B
30B
65B
7B
13B
70B
8B
70B
Dense
0%
5.68
5.09
4.10
3.53
5.12
4.57
3.12
5.54
2.59
SparseGPT
50%
7.24
6.22
5.33
4.60
6.54
5.63
3.99
8.64
5.30
Wanda
50%
7.26
6.15
5.25
4.60
6.46
5.58
3.97
9.06
5.33
FISTAPruner
50%
6.97
6.06
5.09
4.39
6.35
5.47
3.93
8.00
5.09
SparseGPT
2:4
11.32
9.11
7.21
6.24
10.37
8.29
5.38
14.65
8.63
Wanda
2:4
11.54
9.61
6.91
6.24
11.34
8.35
5.20
22.56
8.34
FISTAPruner
2:4
9.82
8.27
6.70
5.82
9.63
7.69
5.16
14.54
7.55
Table 2: WikiText perplexity (↓) of pruned LLaMA, LLaMA-2 and LLaMA-3 models under 50%
unstructured and 2:4 semi-structured sparsity. FISTAPruner outperforms state-of-the-art methods.
4.3
Zero-Shot Task Results
The results of zero-shot tasks on pruned LLaMA-3-70B models, with 50% unstructured and 2:4
semi-structured sparsity, are detailed in Table 3. These results indicate that FISTAPruner surpasses
existing methods on most tasks. Furthermore, when evaluating the average accuracy across the seven
tasks we examined, FISTAPruner consistently shows superior performance compared to existing
methods, particularly with 2:4 semi-structured sparsity.
Method
Sparsity
ARC-c
ARC-e
WinoGrande
RTE
BoolQ
QNLI
WNLI
Mean
Dense
0%
0.6024
0.8685
0.8035
0.6859
0.8560
0.5190
0.7183
0.7219
SparseGPT
50%
0.5401
0.8340
0.7979
0.7040
0.8480
0.5035
0.7042
0.7045
Wanda
50%
0.5427
0.8320
0.7814
0.7076
0.8480
0.5045
0.6338
0.6928
FISTAPruner
50%
0.5614
0.8410
0.8035
0.6895
0.8645
0.5055
0.7183
0.7120
SparseGPT
2:4
0.4590
0.7830
0.7609
0.6426
0.8165
0.4985
0.5493
0.6443
Wanda
2:4
0.4829
0.7860
0.7174
0.6354
0.7615
0.5390
0.6056
0.6468
FISTAPruner
2:4
0.4735
0.7985
0.7751
0.7004
0.8540
0.5675
0.6620
0.6901
Table 3: Zero-shot results (accuracy, ↑) of the pruned LLaMA-3-70B model under 50% unstructured
and 2:4 semi-structured sparsity. FISTAPruner outperforms state-of-the-art methods on most of the
tasks and yields much higher average accuracies especially under 2:4 semi-structured sparsity.
4.4
Ablation Studies
We conduct ablation studies to evaluate the intra-layer error correction mechanism of FISTAPruner,
the impact of varying the number of calibration samples, and the sensitivity to random seeds. To
ensure short iteration times, our experiments are limited to the OPT-125M model and take the result
of Wanda as a warm start of FISTA. Additionally, we consistently apply 50% unstructured sparsity in
these studies.
8


--- Page 9 ---
0
10
20
30
40
50
Sparsity (%)
28
30
32
34
36
38
40
Perplexity on WikiText-2-raw
SparseGPT
Wanda
FISTAPruner
Dense
(a) Sparsity-vs-perplexity on OPT-125M.
0
10
20
30
40
50
Sparsity (%)
6
7
8
9
Perplexity on WikiText-2-raw
SparseGPT
Wanda
FISTAPruner
Dense
(b) Sparsity-vs-perplexity on LLaMA-3-8B.
Figure 3: Comparative analysis of sparsity versus perplexity across different methods for OPT-125M
and LLaMA-3-8B models on WikiText dataset.
0
10
20
30
40
50
Sparsity (%)
28
30
32
34
36
38
40
Perplexity on WikiText-2-raw
SparseGPT
Wanda
FISTAPruner without Error Corrections
FISTAPruner
Dense
(a) Intra-layer error corrections ablation.
8
16
32
64
128
256
512
1024
Number of Calibration Samples
34
36
38
40
42
Perplexity on WikiText-2-raw
SparseGPT
Wanda
FISTAPruner
(b) Calibration samples ablation.
Figure 4: Ablation studies of FISTAPruner on the WikiText dataset, showcasing the effects of intra-
layer error correction and varying calibration sample sizes.
Intra-layer Error Corrections. We compare the performance of FISTAPruner with and without the
intra-layer error correction mechanism, with results on the WikiText dataset displayed in Figure 4(a)
(see results on PTB and C4 datasets in Appendix C.3). We observe that the perplexity of the
pruned model incorporating this mechanism consistently outperforms the version without it, thereby
confirming its effectiveness. Moreover, FISTAPruner, even without the intra-layer error correction
mechanism, outperforms existing methods such as SparseGPT and Wanda. This underscores the
effectiveness of applying convex optimization theory and algorithms to pruning problems.
Amount of Calibration Data. We investigate the performance of FISTAPruner and existing methods,
SparseGPT and Wanda, in relation to the number of calibration data samples, which we vary in
powers of two. The results on WikiText dataset are displayed in Figure 4(b) (see results on PTB and
C4 datasets in Appendix C.3). We observe that using more calibration samples significantly enhances
performance, but only up to a certain point as the improvement curve quickly flattens. This finding
aligns with observations in (Frantar and Alistarh, 2023; Sun et al., 2023). Given that using more
samples increases computational and memory costs, we consistently use 128 calibration samples in
all our experiments.
Sensitivity to Random Seeds. We assess the sensitivity of FISTAPruner’s results to randomness,
particularly in relation to the random sampling of calibration data. We conducted five repeated pruning
runs using different seeds for data sampling, yielding result of 33.22 ± 0.361 (mean ± standard
deviation), which suggests that FISTAPruner demonstrates considerable robustness to variations in
the calibration data used.
5
Discussion
Despite the rigorous theoretical foundation and impressive pruning performance of FISTAPruner, the
time required for pruning remains a limitation of our method compared to SparseGPT and Wanda.
This is primarily due to the iterative nature of FISTA and the process of tuning λ. Pruning time varies
with model size; for instance, it takes about 10 minutes for OPT-125M, while LLaMA-3-70B requires
9


--- Page 10 ---
approximately 12 hours on a single Nvidia A100 GPU with 40GB of memory. However, the parallel-
pruning capability of FISTAPruner, which allows for simultaneous pruning of multiple decoder layers
across various devices, can mitigate this issue to some extent. Furthermore, as post-training pruning is
typically an offline process, time sensitivity may not be a critical factor in real-world applications. In
addition, FISTAPruner represents an attempt to integrate convex optimization theory and algorithms
into LLM applications, potentially inspiring further advancements in this area.
6
Conclusion
In this paper, we introduce FISTAPruner, a layer-wise post-training pruning method for LLMs
based on convex optimization models and algorithms. Initially, we develop a convex optimization
model that employs the ℓ1-norm to induce sparsity in the weights, complemented by an intra-
layer error correction mechanism to eliminate cumulative errors across operators in the traditional
pruning process. Subsequently, we utilize the FISTA solver to efficiently solve the proposed model.
FISTAPruner supports both unstructured and n : m semi-structured pruning and facilitates parallel
pruning, which could reduce the total pruning time by utilizing various devices simultaneously.
Extensive experiments on the OPT, LLaMA, LLaMA-2, and LLaMA-3 model families demonstrate
FISTAPruner’s superior performance compared to existing methods. We hope this work enhances
understanding of sparsity in LLMs and inspires further integration of convex optimization within
LLM applications.
10


--- Page 11 ---
References
AI, M. (2023). Llama-3: Meta ai’s latest language model. https://ai.meta.com/blog/meta-llama-3/.
Arefeen, M. A., Debnath, B., and Chakradhar, S. (2024). Leancontext: Cost-efficient domain-specific question
answering using llms. Natural Language Processing Journal, 7:100065.
Ashkboos, S., Croci, M. L., Nascimento, M. G. d., Hoefler, T., and Hensman, J. (2024). Slicegpt: Compress
large language models by deleting rows and columns. arXiv preprint arXiv:2401.15024.
Beck, A. and Teboulle, M. (2009). A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAM journal on imaging sciences, 2(1):183–202.
Boža, V. (2024).
Fast and optimal weight update for pruned large language models.
arXiv preprint
arXiv:2401.02938.
Candès, E. J., Romberg, J., and Tao, T. (2006). Robust uncertainty principles: Exact signal reconstruction from
highly incomplete frequency information. IEEE Transactions on information theory, 52(2):489–509.
Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. (2019). Boolq: Exploring
the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044.
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. (2018). Think you
have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457.
Dinh, T., Wang, B., Bertozzi, A., Osher, S., and Xin, J. (2020). Sparsity meets robustness: Channel pruning for
the feynman-kac formalism principled robust deep neural nets. In Machine Learning, Optimization, and Data
Science: 6th International Conference, LOD 2020, Siena, Italy, July 19–23, 2020, Revised Selected Papers,
Part II 6, pages 362–381. Springer.
Frantar, E. and Alistarh, D. (2023). Sparsegpt: Massive language models can be accurately pruned in one-shot.
In International Conference on Machine Learning, pages 10323–10337. PMLR.
Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff,
N., et al. (2021). A framework for few-shot language model evaluation. Version v0. 0.1. Sept, page 8.
Hassibi, B. and Stork, D. (1992). Second order derivatives for network pruning: Optimal brain surgeon. Advances
in neural information processing systems, 5.
He, B. and Yuan, X. (2012). On the o(1/n) convergence rate of the douglas–rachford alternating direction method.
SIAM Journal on Numerical Analysis, 50(2):700–709.
Holmes, C., Zhang, M., He, Y., and Wu, B. (2021). Nxmtransformer: Semi-structured sparsification for natural
language understanding via admm. Advances in neural information processing systems, 34:1818–1830.
Huang, Z., Shao, W., Wang, X., Lin, L., and Luo, P. (2020). Convolution-weight-distribution assumption:
Rethinking the criteria of channel pruning. arXiv preprint arXiv:2004.11627.
Kurti´c, E., Frantar, E., and Alistarh, D. (2024). Ziplm: Inference-aware structured pruning of language models.
Advances in Neural Information Processing Systems, 36.
Li, J., Tang, T., Zhao, W. X., Nie, J.-Y., and Wen, J.-R. (2024). Pre-trained language models for text generation:
A survey. ACM Computing Surveys, 56(9):1–39.
Lyu, C., Xu, J., and Wang, L. (2023). New trends in machine translation using large language models: Case
examples with chatgpt. arXiv preprint arXiv:2305.01181.
Ma, X., Fang, G., and Wang, X. (2023). Llm-pruner: On the structural pruning of large language models. arXiv
preprint arXiv:2305.11627.
Marcus, M., Kim, G., Marcinkiewicz, M. A., MacIntyre, R., Bies, A., Ferguson, M., Katz, K., and Schasberger,
B. (1994). The penn treebank: Annotating predicate argument structure. In Human Language Technology:
Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994.
Merity, S., Xiong, C., Bradbury, J., and Socher, R. (2016). Pointer sentinel mixture models. arXiv preprint
arXiv:1609.07843.
Mishra, A., Latorre, J. A., Pool, J., Stosic, D., Stosic, D., Venkatesh, G., Yu, C., and Micikevicius, P. (2021).
Accelerating sparse deep neural networks. arXiv preprint arXiv:2104.08378.
11


--- Page 12 ---
OpenAI (2023). Gpt-4 technical report. arXiv, pages 2303–08774.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., et al. (2019). Pytorch: An imperative style, high-performance deep learning library. Advances in neural
information processing systems, 32.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020).
Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning
research, 21(140):1–67.
Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. (2021). Winogrande: An adversarial winograd schema
challenge at scale. Communications of the ACM, 64(9):99–106.
Shao, H., Liu, B., and Qian, Y. (2024). One-shot sensitivity-aware mixed sparsity pruning for large language
models. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pages 11296–11300. IEEE.
Steiner, B., Elhoushi, M., Kahn, J., and Hegarty, J. (2023). Model: memory optimizations for deep learning. In
International Conference on Machine Learning, pages 32618–32632. PMLR.
Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. (2023). A simple and effective pruning approach for large language
models. arXiv preprint arXiv:2306.11695.
Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth,
A., et al. (2023). Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.
Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society
Series B: Statistical Methodology, 58(1):267–288.
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro,
E., Azhar, F., et al. (2023a). Llama: Open and efficient foundation language models. arXiv preprint
arXiv:2302.13971.
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava,
P., Bhosale, S., et al. (2023b). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint
arXiv:2307.09288.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I.
(2017). Attention is all you need. Advances in neural information processing systems, 30.
Vergara-Browne, T., Soto, Á., and Aizawa, A. (2024). Eigenpruning. arXiv preprint arXiv:2404.03147.
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2018). Glue: A multi-task benchmark
and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461.
Wang, Y., Ma, X., and Chen, W. (2023). Augmenting black-box llms with medical textbooks for clinical question
answering. arXiv preprint arXiv:2309.02233.
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz,
M., et al. (2019). Huggingface’s transformers: State-of-the-art natural language processing. arXiv preprint
arXiv:1910.03771.
Xie, X., Gherardi, R., Pan, Z., and Huang, S. (2023). Hollownerf: Pruning hashgrid-based nerfs with trainable
collision mitigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
3480–3490.
Yao, B., Jiang, M., Yang, D., and Hu, J. (2023). Empowering llm-based machine translation with cultural
awareness. arXiv preprint arXiv:2305.14328.
Zhang, B., Haddow, B., and Birch, A. (2023a). Prompting large language model for machine translation: A case
study. In International Conference on Machine Learning, pages 41092–41110. PMLR.
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al.
(2022). Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.
Zhang, W., Deng, Y., Liu, B., Pan, S. J., and Bing, L. (2023b). Sentiment analysis in the era of large language
models: A reality check. arXiv preprint arXiv:2305.15005.
Zhang, Y., Lin, M., Zhong, Y., Chao, F., and Ji, R. (2023c). Lottery jackpots exist in pre-trained models. IEEE
Transactions on Pattern Analysis and Machine Intelligence.
12


--- Page 13 ---
Appendix
A
Derivations of the Proposed Optimization Model
We present detailed derivations of Model (4) in the following. Given X∗∈Rn×p and WX ∈Rm×p,
we want to find a sparse solution W ∗∈Rm×n that minimizes the pruning metric
∥W ∗X∗−WX∥F .
(10)
We observe its similarities to the well-known least absolute shrinkage and selection operator
(LASSO) (Tibshirani, 1996) problem and thus transform it into a standard LASSO model, which
could be efficiently solved by operator-splitting algorithms such as FISTA. To achieve such a trans-
formation, first, we leverage the following equality to write the decision variable W ∗in its vector
form:
∥W ∗X∗−WX∥2
F =
(X∗)⊤(W ∗)⊤−(WX)⊤2
F
=
m
X
i=1
(X∗)⊤(W ∗
i,:)⊤−(WX)⊤
i,:
2
2
=




(X∗)⊤
...
(X∗)⊤








(W ∗
1,:)⊤
(W ∗
2,:)⊤
...
(W ∗
m,:)⊤




−





(WX)⊤
1,:
(WX)⊤
2,:
...
(WX)⊤
m,:






2
2
Then we can rewrite the square of the pruning metric in its vector form,
∥Ax −b∥2
2 ,
(11)
where
A =



(X∗)⊤
...
(X∗)⊤


∈Rpm×nm, x =





(W ∗
1,:)⊤
(W ∗
2,:)⊤
...
(W ∗
m,:)⊤




∈Rnm, b =





(WX)⊤
1,:
(WX)⊤
2,:
...
(WX)⊤
m,:




∈Rpm.
Note that finding a sparse W ∗to minimize (10) is equivalent to finding a sparse x to minimize (11),
which could be modeled by the LASSO formulation
min
x
1
2 ∥Ax −b∥2
2 + λ∥x∥1.
Now, we have
1
2 ∥Ax −b∥2
2 + ∥x∥1 = 1
2∥W ∗X∗−WX∥2
F + λ






(W ∗
1,:)⊤
(W ∗
2,:)⊤
...
(W ∗
m,:)⊤






1
= 1
2∥W ∗X∗−WX∥2
F + λ
m
X
i=1
(W ∗
i,:)⊤
1 ,
and hence, we obtain the proposed optimization model (4).
B
Derivations of the FISTA Iterations
We derive here the FISTA Iterations for the optimization problem (4) in which one full iteration
includes a gradient descent step of the quadratic term 1
2∥W ∗X∗−WX∥2
F , a proximal step of
the regularization term λ Pm
i=1
(W ∗
i,:)⊤
1 and a Nestrov acceleration term that yields a improved
convergence rate of O(1/k2) (Beck and Teboulle, 2009).
13


--- Page 14 ---
Let f : Rm×n →R+ be a function defined by
f(Y ) := 1
2 ∥Y X∗−WX∥2
F .
The gradient of f at Y = W ∗
k is computed as
∇f(W ∗
k ) = (W ∗
k X∗−WX)(X∗)⊤
= W ∗
k X∗(X∗)⊤−WX(X∗)⊤.
Thus, given optimal step size 1/L where L is the maximum eigenvalue of X∗(X∗)⊤(Beck and
Teboulle, 2009), the gradient descent step (5a) of FISTA reads as
W ∗
k+ 1
3 = W ∗
k −1
L
 W ∗
k X(X∗)⊤−W ⊤X(X∗)⊤
.
In the second step (5b), we do a proximal update with respect to the regularization term by solving
min
W ∗∈Rm×n
L
2
W ∗−W ∗
k+ 1
3

2
F + λ
m
X
i=1
∥W ∗
i,:∥1.
(12)
Let h : R →R+ be a function defined by
h(y|z) := 1
2(y −z)2 + λ
L|y|.
Observe that
L
2
W ∗−W ∗
k+ 1
3

2
F + λ
m
X
i=1
∥W ∗
i,:∥1 = L
X
i,j
h

W ∗
ij
W ∗
k+ 1
3 ,ij

.
Hence problem (12) can be split into m × n independent subproblems of dimension 1 and we only
need to focus on solving each one of them. Note that h is convex but not smooth. It suffices to find a
point W ∗
k+ 2
3 ,ij such that
0 ∈∂h

W ∗
k+ 2
3 ,ij
W ∗
k+ 1
3 ,ij

,
where ∂denotes the sub-differential operator. Observe that
∂h(y|z) =



y −z + λ
L, if y > 0,
y −z −λ
L, if y < 0,
{y −z + u λ
L | u ∈[−1, 1]}, if y = 0.
We now solve for 0 ∈∂h(y|z) by considering the following cases:
• If y > 0, then we set y −z + λ
L = 0. This gives y = z −λ
L and requires z > λ
L.
• If y < 0, then we set y −z −λ
L = 0. This gives y = z + λ
L and requires z < −λ
L.
• If y = 0, then we want 0 ∈{y −z + u λ
L | u ∈[−1, 1]}. This requires −λ
L < z < λ
L.
Hence, 0 ∈∂h

W ∗
k+ 2
3 ,ij
W ∗
k+ 1
3 ,ij

yields
W ∗
k+ 2
3 ,ij =





W ∗
k+ 1
3 ,ij −λ
L, if W ∗
k+ 1
3 ,ij > λ
L,
W ∗
k+ 1
3 ,ij + λ
L, if W ∗
k+ 1
3 ,ij < −λ
L,
0, otherwise,
which is exactly the value given by SoftShrinkageλ/L

W ∗
k+ 1
3 ,ij

.
Finally, according to (Beck and Teboulle, 2009), we add a Nestrov acceleration step by setting t0 = 1
and computing
tk+1 = 1
2

1 +
q
1 + 4t2
k

,
(13)
W ∗
k+1 = W ∗
k+ 2
3 + tk −1
tk+1

W ∗
k+ 2
3 −W ∗
k

,
(14)
which gives steps (5c) and (5d).
The above illustrates the details of the FISTA iterations.
14


--- Page 15 ---
C
Additional Results
C.1
Perplexity Results on PTB
We present the PTB perplexity results of pruned OPT, LLaMA, LLaMA-2, and LLaMA-3 models
under 50% unstructured and 2:4 semi-structured sparsity in Tables 4 and 5. FISTAPruner outperforms
state-of-the-art methods on all OPT, LLaMA and LLaMA-3 models, as well as on most LLaMA-2
models on the PTB dataset. The sole exception is the pruning of the LLaMA-2-70B model under
50% unstructured sparsity, where FISTAPruner surpasses Wanda but falls short of SparseGPT. This
underperformance may be due to the generally poorer performance of LLaMA-2 models compared to
similarly sized models from other families. For instance, the dense LLaMA-2-13B model exhibits a
PTB perplexity of 56.52, even higher than the smaller LLaMA-2-7B model, which has a perplexity
of 50.19. Moreover, we observe that the PTB perplexity results for all dense LLaMA and LLaMA-2
models are consistently higher than those for similarly sized OPT models; for example, the LLaMA-2-
13B’s perplexity of 56.52 far exceeds the smallest OPT-125M model’s 38.99. In contrast, LLaMA-3
models show significantly better performance on the PTB dataset.
OPT
Method
Sparsity
125M
350M
1.3B
2.7B
6.7B
13B
30B
Dense
0%
38.99
31.07
20.29
17.97
15.77
14.52
14.04
SparseGPT
50%
55.38
43.58
25.64
20.52
17.38
15.98
14.97
Wanda
50%
57.60
55.47
27.98
21.85
17.92
17.45
15.47
FISTAPruner
50%
49.79
41.26
25.08
20.15
17.08
15.87
14.92
SparseGPT
2:4
94.21
72.82
37.30
26.87
21.65
18.69
16.56
Wanda
2:4
111.55
135.98
43.85
34.64
25.07
22.16
21.65
FISTAPruner
2:4
67.80
59.51
36.26
24.43
20.04
18.08
16.18
Table 4: PTB perplexity of pruned OPT models under 50% unstructured and 2:4 semi-structured
sparsity. FISTAPruner outperforms state-of-the-art methods.
LLaMA
LLaMA-2
LLaMA-3
Method
Sparsity
7B
13B
30B
65B
7B
13B
70B
8B
70B
Dense
0%
41.15
28.10
23.51
25.07
50.19
56.52
22.68
10.17
7.87
SparseGPT
50%
79.67
37.49
26.14
27.64
1020.01
95.41
24.87
14.00
9.24
Wanda
50%
80.48
36.43
26.64
25.77
97.58
86.79
26.07
15.54
9.44
FISTAPruner
50%
58.67
35.30
25.63
25.15
96.72
78.23
25.36
12.93
8.88
SparseGPT
2:4
154.62
71.68
32.44
32.91
1163.57
154.15
31.51
23.42
13.01
Wanda
2:4
211.40
74.29
35.56
33.39
587.54
224.55
33.97
48.96
14.17
FISTAPruner
2:4
91.84
64.04
30.86
30.78
361.16
136.84
31.49
22.60
11.11
Table 5: PTB perplexity (↓) of pruned LLaMA, LLaMA-2 and LLaMA-3 models under 50%
unstructured and 2:4 semi-structured sparsity.
C.2
Perplexity Results on C4
The C4 perplexity results of pruned OPT, LLaMA, LLaMA-2, and LLaMA-3 models under 50%
unstructured and 2:4 semi-structured sparsity are shown in Tables 4 and 5. FISTAPruner performs
consistently better than the state-of-the-art methods.
C.3
Additional Ablation Study Results
Intra-layer Error Corrections. We compare the performance of FISTAPruner with and without the
intra-layer error correction mechanism, with results on PTB and C4 datasets displayed in Figures 5(a)
and 6(a). The results indicate that the perplexity of the pruned model incorporating this mechanism
consistently outperforms the version without it, thereby confirming its effectiveness.
Amount of Calibration Data. The results of pruning performance in relation to the number of
calibration data samples on PTB and C4 datasets are displayed in Figures 5(b) and 6(b). The same
curve pattern as shown in Figure 4(b) is observed.
15


--- Page 16 ---
OPT
Method
Sparsity
125M
350M
1.3B
2.7B
6.7B
13B
30B
Dense
0%
26.56
22.59
16.07
14.34
12.71
12.06
11.45
SparseGPT
50%
33.52
29.14
19.23
15.77
13.73
12.98
11.96
Wanda
50%
34.89
34.46
20.63
16.44
14.25
13.57
12.32
FISTAPruner
50%
30.93
27.36
18.56
15.58
13.61
12.94
11.92
SparseGPT
2:4
52.11
46.36
25.77
19.35
16.44
14.85
13.18
Wanda
2:4
64.73
88.62
28.59
22.88
19.00
16.19
16.18
FISTAPruner
2:4
38.08
36.45
24.29
17.82
15.35
14.19
12.78
Table 6: C4 perplexity (↓) of pruned OPT models under 50% unstructured and 2:4 semi-structured
sparsity. FISTAPruner outperforms state-of-the-art methods.
LLaMA
LLaMA-2
LLaMA-3
Method
Sparsity
7B
13B
30B
65B
7B
13B
70B
8B
70B
Dense
0%
7.34
6.80
6.13
5.81
7.04
6.52
5.53
9.01
6.82
SparseGPT
50%
9.33
8.14
7.34
6.66
9.00
7.96
6.25
13.93
9.34
Wanda
50%
9.34
8.15
7.29
6.71
8.94
8.04
6.30
14.97
9.80
FISTAPruner
50%
8.90
7.96
7.05
6.49
8.62
7.73
6.22
13.12
8.94
SparseGPT
2:4
13.65
11.38
9.50
8.41
13.58
11.39
7.99
24.16
14.81
Wanda
2:4
14.47
12.11
9.46
8.78
15.07
12.13
7.89
36.70
14.47
FISTAPruner
2:4
11.95
10.27
8.81
7.82
12.41
10.34
7.59
23.15
12.18
Table 7: C4 perplexity (↓) of pruned LLaMA, LLaMA-2 and LLaMA-3 models under 50% unstruc-
tured and 2:4 semi-structured sparsity. FISTAPruner outperforms state-of-the-art methods.
0
10
20
30
40
50
Sparsity (%)
40
45
50
55
60
Perplexity on PTB
SparseGPT
Wanda
FISTAPruner without Error Corrections
FISTAPruner
Dense
(a) Intra-layer error corrections ablation.
8
16
32
64
128
256
512
1024
Number of Calibration Samples
48
50
52
54
56
58
Perplexity on PTB
SparseGPT
Wanda
FISTAPruner
(b) Calibration samples ablation.
Figure 5: Ablation studies of FISTAPruner on the PTB dataset, showcasing the effects of intra-layer
error correction and varying calibration sample sizes.
0
10
20
30
40
50
Sparsity (%)
26
28
30
32
34
36
Perplexity on C4
SparseGPT
Wanda
FISTAPruner without Error Corrections
FISTAPruner
Dense
(a) Intra-layer error corrections ablation.
8
16
32
64
128
256
512
1024
Number of Calibration Samples
32
34
36
Perplexity on C4
SparseGPT
Wanda
FISTAPruner
(b) Calibration samples ablation.
Figure 6: Ablation studies of FISTAPruner on the C4 dataset, showcasing the effects of intra-layer
error correction and varying calibration sample sizes.
16
