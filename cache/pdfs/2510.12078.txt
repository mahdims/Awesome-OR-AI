--- Page 1 ---
1
FedLoDrop: Federated LoRA with Dropout for
Generalized LLM Fine-tuning
Sijing Xie, Dingzhu Wen, Changsheng You, Qimei Chen, Mehdi Bennis, and Kaibin Huang
Abstractâ€”Fine-tuning (FT) large language models (LLMs) is
crucial for adapting general-purpose models to specific tasks, en-
hancing accuracy and relevance with minimal resources. To fur-
ther enhance generalization ability while reducing training costs,
this paper proposes Federated LoRA with Dropout (FedLoDrop),
a new framework that applies dropout to the rows and columns of
the trainable matrix in Federated LoRA. A generalization error
bound and convergence analysis under sparsity regularization
are obtained, which elucidate the fundamental trade-off between
underfitting and overfitting. The error bound reveals that a
higher dropout rate increases model sparsity, thereby lowering
the upper bound of pointwise hypothesis stability (PHS). While
this reduces the gap between empirical and generalization errors,
it also incurs a higher empirical error, which, together with the
gap, determines the overall generalization error. On the other
hand, though dropout reduces communication costs, deploying
FedLoDrop at the network edge still faces challenges due to
limited network resources. To address this issue, an optimization
problem is formulated to minimize the upper bound of the
generalization error, by jointly optimizing the dropout rate and
resource allocation subject to the latency and per-device energy
consumption constraints. To solve this problem, a branch-and-
bound (B&B)-based method is proposed to obtain its globally
optimal solution. Moreover, to reduce the high computational
complexity of the B&B-based method, a penalized successive
convex approximation (P-SCA)-based algorithm is proposed to
efficiently obtain its high-quality suboptimal solution. Finally,
numerical results demonstrate the effectiveness of the proposed
approach in mitigating overfitting and improving the generaliza-
tion capability.
Index Termsâ€”Large Language Models, Federated Learning,
Low-rank Adaptation, Dropout, Generalization Error
I. INTRODUCTION
Recent developments in large language models (LLMs),
such as ChatGPT, LLaMA, and Vision Transformers, have
driven significant progress in artificial general intelligence
(AGI) [1]. These models exhibit enhanced capabilities in gen-
eralization and inference. To adapt these models for specific
tasks and data distributions, fine-tuning (FT) has become a
key approach, which is crucial for establishing native artificial
S. Xie and D. Wen are with the School of Information Science and Technol-
ogy, ShanghaiTech University, Shanghai 201210, China (e-mail: {xiesj2023,
wendzh}@shanghaitech.edu.cn).
C. You is with the Department of Electronic and Electrical Engineering,
Southern University of Science and Technology, Shenzhen 518055, China
(e-mail: youcs@sustech.edu.cn).
Q. Chen is with the School of Electronic Information, Wuhan University,
Wuhan 430072, China (e-mail: chenqimei@whu.edu.cn).
M. Bennis is with the Centre for Wireless Communications, University of
Oulu, Oulu 90014, Finland (e-mail: mehdi.bennis@oulu.fi).
K. Huang is with the Department of Electrical and Electronic Engi-
neering, The University of Hong Kong, Hong Kong SAR, China (e-mail:
huangkb@eee.hku.hk).
intelligence (AI) in the era of 6G [2]â€“[4]. However, as model
sizes grow, full-model FT becomes unaffordable in practice
due to prohibitively high computational and memory costs.
To tackle these challenges, parameter-efficient fine-tuning
(PEFT) has emerged as a pivotal approach [5]â€“[7]. PEFT
updates only a small subset of model parameters, providing
a balanced trade-off between computational efficiency and
training performance. One notable method within PEFT is
Low-Rank Adaptation (LoRA) [8], which employs low-rank
matrices to approximate weight changes while keeping the
original model weights frozen. The flexibility of LoRA allows
it to be merged with the backbone, eliminating additional
costs. Compared to other PEFT methods based on pruning,
LoRA achieves superior efficiency-utility trade-offs [9]. The
authors in [10] applied LoRA to a hierarchical LLM, clas-
sifying the instruction type and then utilizing task-specific
networks to accomplish respective tasks. FT LLMs on a
single device often result in suboptimal performance due to
limited data and memory constraints [11]. Centralized FT faces
significant challenges, mainly because of privacy concerns and
regulatory restrictions on data access [12]. This is exacerbated
by the fact that massive amounts of fragmented data are
distributed across numerous devices at the network edge.
Federated learning (FL) enables privacy-preserving FT of
pre-trained LLMs on distributed clients by sharing model up-
dates between the server and clients, ensuring that distributed
data remains localized [13]â€“[17]. This makes FL an appealing
choice for aligning LLMs with specialized domains [18], [19].
Specifically, the authors in [20], [21] incorporated LoRA with
FedAvg, significantly reducing the number of parameters that
need to be synchronized across distributed devices. Several
works have focused on improving the efficiency of federated
LoRA. Due to the frequent exchange of LLM parameters
among distributed devices, [9] proposed integrating commu-
nication compression with federated LoRA to further reduce
the communication cost. In addition, a hierarchical FedLoRA
framework was proposed in [22], which dynamically assigned
diverse and suitable FT depths for each group, hence greatly
reducing the computation and communication cost. The au-
thors in [23] proposed a memory-efficient FT method, which
sets the A matrices to fixed after initialization and trains
only the B matrices. Other works focus on the deployment
and performance of federated LoRA. [24] proposed an im-
proved initialization strategy for LoRAâ€™s weights, resulting
in enhanced performance under the FedFM framework. Due
to limited storage and computational capabilities at the edge
devices, deploying the full model at edge devices is impracti-
cal. [25] presented a split federated LoRA framework, where
arXiv:2510.12078v2  [cs.IT]  31 Jan 2026


--- Page 2 ---
2
the computationally intensive encoder is deployed at the edge
server, while others remain on edge devices.
However, one significant challenge when FT models on
downstream tasks is overfitting. Simply reducing the rank
of LoRA could help alleviate overfitting, but fewer learnable
parameters indicate less expressive power, and might lead to
suboptimal performances. AdaLoRA [26] optimizes LoRA by
automatically pruning unimportant parameters with learned
importance scores during training to prevent overfitting. How-
ever, this parameter selection method heavily relies on gradi-
ents of parameters on the training data, making the models
less generalizable to unseen test data. To address this issue,
a federated dropout framework was proposed in [27], which
utilized the typical technique of dropout in deep learning.
[28] further provided the convergence analysis for federated
dropout, quantitatively showing the influence of dropout rate
on convergence. In addition, the authors in [29] combined
LoRA with dropout in an aggregation way. This, however,
has the centralized challenges as mentioned before.
Towards this end, we propose a practical Federated LoRA
with dropout (FedLoDrop) approach, which introduces ran-
dom noise to the learnable low-rank matrices. FedLoDrop
effectively mitigates overfitting while simultaneously reducing
communication costs and minimizing update sizes transmitted
from clients to the server 1. We provide the theoretical analysis
using pointwise hypothesis stability (PHS) and Taylor expan-
sion. Then, we aim to minimize the generalization error in
each round by jointly optimizing dropout rate and network
resource allocation. Finally, simulations are presented to val-
idate the effectiveness of our proposed approach. The main
contributions of this paper are listed below.
â€¢ FedLoDrop Framework with Reduced Communica-
tion Cost: We propose a practical FedLoDrop framework,
where dropout is applied to the rows and columns of
the tunable low-rank parameter matrices, e.g., A and B.
Specifically, for device k with a dropout rate of Î³k âˆˆ
[0, 1), dropout technique deactivates neurons on the input
and output sides of both trainable LoRA matrices [29].
After applying dropout, the communication overhead is
scaled down to (1 âˆ’Î³) times of the original one.
â€¢ Theoretical Analysis: We characterize the effect of
LoRA dropout on the sparsity and PHS upper bound. It is
shown that a lower dropout rate enhances model complex-
ity, increasing overfitting potential and widening the gap
between generalization and empirical errors (adaptation
function class). Conversely, a drastically high dropout
rate may incur underfitting, significantly impairing the
representation ability of the model and resulting in higher
empirical error. This provides a theoretical foundation to
balance the tradeoff between the adaptation function class
and empirical error. Moreover, a convergence analysis
1Mixture-of-Experts (MoE) has recently emerged as an effective paradigm
for reducing computational overhead and can serve as a complementary
approach to FedLoDrop. The adaptive dropout mechanisms in FedLoDrop
could be specifically tailored to promote more balanced expert utilization
across heterogeneous clients, thereby preventing any single expert from
dominating the learning process or remaining underutilized.
is also conducted, revealing that the convergence rate
becomes slower with increasing dropout rate.
â€¢ Joint Dropout Control and Resource Allocation: Based
on the theoretical analysis for FedLoDrop, we formulate
an optimization problem to minimize the generalization
error of each FT round under the network resource
constraint, which is shown to be dependent on the dropout
rate of each device. Particularly, a larger dropout rate
leads to a smaller gap between empirical and generaliza-
tion errors. As the exact form of the learning loss reduc-
tion in the generalization error bound is intractable, its
upper bound is minimized instead without loss of gener-
ality, under the constraints of limited system subcarriers,
completion latency, and per-device energy consumption.
A branch-and-bound (B&B)-based method is proposed
to find the globally optimal solution. Moreover, to ef-
ficiently solve the problem, a low-complexity penalized
successive convex approximation (P-SCA)-based solution
is proposed to find a high-quality suboptimal solution.
â€¢ Performance Evaluation: Extensive simulations based
on multi-language tasks are conducted to evaluate the
performance of the proposed schemes. We mainly FT
two LLMs, RoBERTa-large (355M) and LLaMA (7B),
on the GLUE and MMLU benchmarks, respectively. With
dropout, the generalization ability of the fine-tuned model
is significantly improved, thus enabling the capability to
effectively apply the knowledge from the FT dataset to
natural language response tasks. In both scenarios, more
network resources, i.e., a longer latency, allow lower
dropout rates of all devices, leading to improved testing
performance.
II. FRAMEWORK OF FEDERATED LORA DROPOUT
A. Preliminary-LoRA
LoRA fine-tunes LLMs efficiently by maintaining the orig-
inal model weights Î¸0 frozen and adding low-rank trainable
matrices [8]. Specifically, the loss function of an LLM in
the FT stage is L =
1
|D|
P
iâˆˆD â„“(âˆ†Î¸; Î¸0, xi) , where D is
the dataset, |D| is the size of dataset, xi is the i-th data
sample therein, and â„“(Â·; Â·) is the empirical loss function that
characterizes the difference between the output and real label.
Consider an arbitrary layer in an arbitrary training round
Wu,t applied with LoRA for FT, a LoRA update is thus
characterized by a set of low-rank trainable weights âˆ†Î¸t â‰œ
{âˆ†Wu,t}U â€²
u=1, a set of pre-trained weight Î¸0 â‰œ{Wu,0}U
u=1,
where U â€² is the number of weight matrices applying LoRA
and U is the number of all matrices. LoRA may not update
all matrices, in which case U â€² â‰¤U. The parameter matrix
Wu,t âˆˆRn1Ã—n2 can be regarded as the sum of a frozen
pre-trained matrix Wu,0 âˆˆRn1Ã—n2 and a trainable low-rank
decomposable matrix âˆ†Wu,t âˆˆRn1Ã—n2, i.e.,
Wu,t = Wu,0 + âˆ†Wu,t = Wu,0 + Bu,tAu,t,
(1)
where Au,t âˆˆRrÃ—n2 and Bu,t âˆˆRn1Ã—r are trainable low-
rank matrices, with r â‰ª{n1, n2}. For an arbitrary training
round t and data sample i, the training procedure of LoRA is
presented below.


--- Page 3 ---
3
Edge server
(Access point)
Local data
â‘¢ 
Local data
Local data
Global model
â‘  Sub-adapter generation
â‘¡ 
â‘£
â‘¤ Sub-adapter aggregration
â‘¡ 
â‘¡ 
â‘£
â‘£
â‘¢ Local tuning
â‘¢ 
ï¸™ Dropped
r
Broadband 
Channels
â‘¡ Adapter downloading
â‘£ Adapter uploading
 
 1Ë†B
1
Ë†A
2
Ë†B
K
BË†
2
Ë†A
K
AË†
1
Ë†A
K
AË†
1Ë†B
K
BË†
A
B
Fig. 1. The operations of FedLoDrop in a wireless system.
1) Forward Pass: The output of the u-th layer is given by
hu,i,t = Wu,tâˆ’1fuâˆ’1,i,t = (Wu,0 + âˆ†Wu,tâˆ’1)fuâˆ’1,i,t
= (Wu,0 + Bu,tâˆ’1Au,tâˆ’1)fuâˆ’1,i,t,
(2)
where fuâˆ’1,i,t is the output of the previous layer.
2) Backward Pass: During the backward propagation, the
stochastic gradients of the two low-rank matrices B and A are
calculated individually, given by
âˆ†Bu,i,tâˆ’1 =
âˆ‚â„“t
âˆ‚Bu,tâˆ’1
=
âˆ‚â„“t
âˆ‚hu,i,t
âˆ‚hu,i,t
âˆ‚Bu,tâˆ’1
=
âˆ‚â„“t
âˆ‚hu,i,t
Â· (Au,tâˆ’1fuâˆ’1,i,t)âŠ¤,
(3)
âˆ†Au,i,tâˆ’1 =
âˆ‚â„“t
âˆ‚Au,tâˆ’1
=
âˆ‚â„“t
âˆ‚hu,i,t
âˆ‚hu,i,t
âˆ‚Au,tâˆ’1
= BâŠ¤
u,tâˆ’1 Â·
âˆ‚â„“t
âˆ‚hu,i,t
Â· f âŠ¤
uâˆ’1,i,t.
(4)
3) Low-rank Matrices Updating: Bu,t and Au,t are up-
dated by the aggregated stochastic gradients of all data samples
[29]â€“[31], i.e.,
âˆ†Bu,tâˆ’1 =
âˆ‚L
âˆ‚Bu,tâˆ’1
=
1
|D|
X
iâˆˆD
âˆ‚â„“t
âˆ‚Bu,tâˆ’1
=
1
|D|
X
iâˆˆD
âˆ†Bu,i,tâˆ’1,
(5)
âˆ†Au,tâˆ’1 =
âˆ‚L
âˆ‚Au,tâˆ’1
=
1
|D|
X
iâˆˆD
âˆ‚â„“t
âˆ‚Au,tâˆ’1
=
1
|D|
X
iâˆˆD
âˆ†Au,i,tâˆ’1,
(6)
where âˆ†Bu,i,tâˆ’1 and âˆ†Au,i,tâˆ’1 are the sample-wise stochas-
tic gradient matrices of the u-th layer defined in (3), (4). Then
Bu,t and Au,t are updated through gradient descent, with Î±B
and Î±A being the learning rate:
Bu,t = Bu,tâˆ’1 âˆ’Î±Bâˆ†Bu,tâˆ’1,
(7)
Au,t = Au,tâˆ’1 âˆ’Î±Aâˆ†Au,tâˆ’1.
(8)
Next, the trainable matrix âˆ†Wu,t is updated by
âˆ†Wu,t = Bu,tAu,t,
(9)
and the parameter matrix Wu,t is updated by
Wu,t = Wu,0 + âˆ†Wu,t.
(10)
Remark 1. The low-rank matrices updating method presented
in (5) â€“ (9) is not equivalent to the sample average of the
updates of âˆ†Wu, i.e.,
âˆ†Wu,t =
 
Bu,tâˆ’1 âˆ’Î±B
|D|
X
iâˆˆD
âˆ†Bu,i,tâˆ’1
!
 
Au,tâˆ’1 âˆ’Î±A
|D|
X
iâˆˆD
Au,i,tâˆ’1
!
Ì¸=
1
|D|
X
iâˆˆD
(Bu,tâˆ’1 âˆ’Î±B,iâˆ†Bu,i,tâˆ’1)
(Au,tâˆ’1 âˆ’Î±A,iâˆ†Au,i,tâˆ’1).
However, this enjoys two benefits. On one hand, it simplifies
the initialization of B and A by utilizing the ones in the last
training round. On the other hand, this approach is well-suited
for distributed implementations [20], [24]. In each round,
devices only need to transmit Bu,tâˆ’1 and Au,tâˆ’1, which
have a lower dimensionality compared to âˆ†Wu,tâˆ’1, thereby
reducing the communication overhead.
B. LoRA Dropout
The original dropout technique was proposed in [32] to
avoid overfitting during training. In standard dropout, each
neuron in the network is dropped from the network with a
certain probability. Given that dropout techniques have proven
effective in controlling overfitting, we introduce a LoRA
dropout framework to enhance generalization when adapting
to downstream tasks. The loss function can be re-written as
L =
1
|D|
P
iâˆˆD â„“(âˆ†Î¸(mt); Î¸0, xi) , where âˆ†Î¸(mt) is the
LoRA parameters after the dropout, mt is the concatenation
of masks of all LoRA modules, which varies across rounds,
and Î¸0 is the original parameters of the pre-trained model.


--- Page 4 ---
4
As shown step
1âƒin Fig. 1, for a LoRA module, rows
and columns are randomly dropped from both tunable low-
rank parameter matrices with a probability of Î³t, which is
called dropout rate. In other words, LoRA dropout strategy
samples random neurons on the input and output sides of
LoRA matrices with a probability Î³t to mask them to zeros
[29]. The low-rank matrices after dropout are given by
( Ë†
Au,t = Au,t Â· diag(mA,t), mA,t âˆ¼Bern(1 âˆ’Î³t),
Ë†
Bu,t =
 BâŠ¤
u,t Â· diag(mB,t)
âŠ¤, mB,t âˆ¼Bern(1 âˆ’Î³t),
(11)
where mA,t âˆˆRn2, mB,t âˆˆRn1, Bern(1 âˆ’Î³t) means that
all elements of a matrix are independently distributed as the
identical Bernoulli distribution with parameter (1 âˆ’Î³t). With
dropout, the sizes of Ë†
Au,t and Ë†
Bu,t are further reduced. In
the t-th training round, for the i-th data sample, the training
procedure is presented below.
1) Forward Pass: After dropout, the output of the u-th layer
is given by Ë†hu,i,t = (Wu,0 + Ë†
Bu,tâˆ’1 Ë†
Au,tâˆ’1)fuâˆ’1,i,t.
2) Backward Pass and Low-rank Matrices Updating:
During backpropagation, the stochastic gradients of Ë†
Bu,t and
Ë†
Au,t are computed according to (5), (6) and updated following
(7), (8). Notably, the dropped parameters will not be updated,
thereby reducing the computational overhead. After obtaining
Ë†
Bu,t and Ë†
Au,t, the matrices âˆ†Ë†
Wu,t and Ë†
Wu,t are calculated
in the same manner as (9) and (10).
C. Federated LoRA with Dropout
1) Procedure and Algorithm: In this case, each device
k holds a local dataset, denoted by Dk
=
{xi|i
=
1, 2, . . . , |Dk|}, where xi is the i-th data sample and the
size of Dk is |Dk|. Denote âˆ†Î¸(mk,t) as the LoRA pa-
rameters after the dropout for the k-th device, mk,t as
the concatenation of all dropout masks of LoRA modules,
which varies across different training rounds and differ-
ent devices, and Î¸0 as the original parameters of the pre-
trained model, which has been stored in each device in
advance. The objective is to minimize the global loss func-
tion, given by L = PK
k=1
|Dk|
|D| â„“k (âˆ†Î¸(mk,t); Î¸0, Dk) , where
D = {Dk} is the global dataset, the local loss function of
the k-th device can be written as â„“k (âˆ†Î¸(mk,t); Î¸0, Dk) =
1
|Dk|
P
xiâˆˆDk â„“k,i (âˆ†Î¸(mk,t); Î¸0, xi) . Similar to (11), FedLo-
Drop can be written as
Ë†
Au,k,t = Au,t Â· diag(mk,A,t), mk,A,t âˆ¼Bern(1 âˆ’Î³k,t),
Ë†
Bu,k,t =
 BâŠ¤
u,t Â· diag(mk,B,t)
âŠ¤, mk,B,t âˆ¼Bern(1 âˆ’Î³k,t),
(12)
The forward and backward pass on each device has minor
changes to the aforementioned situations and thus are omitted.
As illustrated in Fig. 1 and Algorithm 1, the overall
framework involves two primary components: local tuning
operations on the client side, and dropout and aggregation
operations on the server side, which work together to ensure
efficient training. There are five steps to complete an arbitrary
training round t, as described below.
â€¢ LoRA Sub-adapter Generation: The server adopts the
LoRA dropout technique introduced in Section II-B to
generate a sub-adapter for each device. Crucially, the
dropout masks are statistically independent across rounds
and generated prior to any clientâ€™s local computation, thus
preventing privacy leaks.
â€¢ Adapter Downloading: Each device downloads its cor-
responding sub-adapter Ë†
Bk,t and Ë†
Ak,t from the server,
where Ë†
Bk,t â‰œ{ Ë†
Bu,k,t}U â€²
u=1, Ë†
Ak,t â‰œ{ Ë†
Au,k,t}U â€²
u=1.
â€¢ Client Local Tuning: Each device first calculates the
product of two low-rank adapters and updates them based
on pre-trained model and its own datasets, as shown in
(5) â€“ (8).
â€¢ Adapter Uploading: Each device uploads the updated
gradients of local sub-adapters to the server by the
allocated subcarriers.
â€¢ Server Aggregation: The server reconstructs the full-size
adapter update via zero-padding: it inserts zeros into the
dropped positions. This process is feasible because the
server has knowledge of the sparsity mask. Then, all
complete networks are aggregated for updating the global
network, expressed as:
Bt = Btâˆ’1 âˆ’Î±B
K
X
k=1
|Dk|
|D| âˆ†Ë†
Bk,tâˆ’1
= Btâˆ’1 âˆ’Î±Bâˆ†Ë†
Btâˆ’1,
(13)
At = Atâˆ’1 âˆ’Î±A
K
X
k=1
|Dk|
|D| âˆ†Ë†
Ak,tâˆ’1
= Atâˆ’1 âˆ’Î±Aâˆ†Ë†
Atâˆ’1.
(14)
Remark 2. Combined with LoRA dropout, the inference
caused by the cross-product of sub-adapters from different
clients can be diminished. Furthermore, FedLoDrop inherently
provides a degree of privacy due to the sparsification of
transmitted updates against potential privacy attacks such as
gradient inversion or model extraction. That said, the dropout-
based sparsification in FedLoDrop may offer incidental pro-
tection against gradient inversion attacks, as partially masked
updates reduce the amount of information available to an
adversary. Similarly, the local gradient tracking mechanism
further decouples client-specific information from the global
model, potentially mitigating model extraction risks.
2) Communication-computation-memory Overhead:
â€¢ Communication overhead: FedLoDrop provides signifi-
cant competitive savings at both uploading and down-
loading links. In round t, each device downloads and
uploads
Ë†
Bk,t and
Ë†
Ak,t to the server for aggregation.
The original number of transmitted parameters is M =
(n1 + n2)r. After applying dropout, with a dropout rate
of Î³k âˆˆ[0, 1) for device k, the communication overhead
becomes Ë†
Mk,t = (1 âˆ’Î³k,t)(n1 + n2)r = (1 âˆ’Î³k,t)M.
â€¢ Computation and memory overhead: FedLoDrop saves
local computation workloads and memory cost by main-
taining sparse adapters throughout the FL process. No
part of FedLoDrop requires dense training, and the
computation overhead is conducted sample-wise. As the
computation and memory costs of adapters are small
compared to the costs of the backbone and dropout only


--- Page 5 ---
5
Algorithm 1: The training process of FedLoDrop
Parameters: Communication round T; The pre-trained
model W0; The local trainable and efficient parameters
Bk, Ak and the local dataset Dk of the k-th device.
Before Training: Store W0 on each device, and initialize
B0, and A0 on the server.
Server executes:
for each communication round t = 1 to T do
Ë†
Bk,tâˆ’1, Ë†
Ak,tâˆ’1 â†(LoRA dropout)
Send Ë†
Bk,tâˆ’1, Ë†
Ak,tâˆ’1 to each device
for each device in parallel do
ClientLocalTuning (k, Ë†
Bk,tâˆ’1, Ë†
Ak,tâˆ’1)
end
Receive local updated parameters
Global Aggregation by zero padding and (13), (14)
end
ClientLocalTuning (k, Ë†
Bk,tâˆ’1, Ë†
Ak,tâˆ’1):
Ë†
Bk,t, Ë†
Ak,t â†(5) â€“ (8)
Send updated parameters to the server
reduces computation overhead in backward propagation,
the computation and memory benefits are omitted [9].
III. THEORETICAL ANALYSIS
In the FT phase, we first formulate an optimization problem
to minimize the loss function under the constraint of model
sparsity. Subsequently, we characterize a generalization error
bound within the framework of sparsity regularization, which
demonstrates a fundamental trade-off between underfitting and
overfitting in the context of LoRA dropout. Next, the upper
bounding loss descent is formulated. Finally, the convergence
of FedLoDrop is performed.
A. LoRA Dropout FT Through Sparse Regularization
Building upon the LoRA dropout mechanism presented on a
single device by [29], we extend this framework to accommo-
date FL environments. Assume dk,t âˆˆ{0, 1} as a dropout
instance applied to the production of LoRA matrices, i.e.,
dk,t âˆ¼Bern(1âˆ’(1âˆ’Î³k,t)2), namely, dk,t âˆ¼Bern(2Î³k,tâˆ’Î³2
k,t),
where dk,t equals to one when the corresponding entry is
dropped to zero and set as zero otherwise. The FT to minimize
the loss function can be formulated as:
min
{âˆ†Î¸k,t}
PK
k=1
|Dk|
|D| â„“k (âˆ†Î¸k,t; Î¸0, Dk) ,
subject to
Edk,tâˆ¼Bern(2Î³k,tâˆ’Î³2
k,t)âˆ¥dk,t âŠ™âˆ†Î¸k,tâˆ¥2
2 â‰¤c, âˆ€k,
(15)
where c is a constant, and the condition denotes the sparsity
of âˆ†Î¸k,t. The regularized optimization problem of the global
one is
Lâ€²
{Î½k,t} =
K
X
k=1
|Dk|
|D|

min
{âˆ†Î¸k,t} â„“k (âˆ†Î¸k,t; Î¸0, Dk) +
K
X
k=1
Î½k,tEdk,tâˆ¼Bern(2Î³k,tâˆ’Î³2
k,t)âˆ¥dk,t âŠ™âˆ†Î¸k,tâˆ¥2
2
!
=
K
X
k=1
|Dk|
|D| â„“k,Î»t,
(16)
where
â„“k,Î»t
=
minâˆ†Î¸k,t â„“k (âˆ†Î¸k,t; Î¸0, Dk)
+
Î»tEdk,tâˆ¼Bern(2Î³k,tâˆ’Î³2
k,t)âˆ¥dk,t âŠ™âˆ†Î¸k,tâˆ¥2
2 is the regularized
optimization problem for each device. Î½k,t, Î»t are arbitrary
hyper-parameter.
B. Generalization Error Analysis
The stability of the sparse-regularized algorithm is analyzed
to assess the generalization error bound of LoRA dropout FT
by optimizing â„“k,Î»t. Stability is a well-explored subject in
machine learning [33], [34] and we adopt a utilized analytical
framework, pointwise hypothesis stability (PHS)[35]. This
approach examines the perturbation of the optimal model when
a single training sample is removed. For each device, the entire
training dataset is Dk, and the dataset after removing a sample
xj as Dj
k = Dkâˆ’{xj}. It is assumed that j âˆ¼U(|Dk|), which
means the removal is sampled from a uniform distribution.
Î¸â„“(Dk) is defined as the optimal model parameters with
respect to (w.r.t.) loss function â„“and dataset Dk.
Definition 1. A learning algorithm parameterized by Î¸ w.r.t.
a loss function â„“has PHS Î², if:
EDk,jâˆ¼U(n)
â„“

Î¸â„“(Dj
k), xj

âˆ’â„“(Î¸â„“(Dk), xj)
 â‰¤Î².
(17)
In (17), â„“(Î¸, xj) represents the loss for sample xj given
model parameters Î¸. According to [29], if the following
requirements are met: â„“is Î·-Lipschitz, Î¸â„“k,Î»(Dk) is close to
Î¸â„“k,Î»t(Dj
k), Hessian matrix âˆ‡2â„“k,Î»t(Î¸â„“k,Î»t(Dk)) at Î¸â„“k,Î»t(Dk)
is positive-semidefinite with a singular value decomposition
Uk,tdiag(Î›k,t)U âˆ’1
k,t , and Î›k,t,min = min{Î›k,t,1, Â· Â· Â· , Î›k,t,m},
then the LoRA dropout algorithm optimizing â„“k,Î»t on |Dk|
has an upper bound of PHS of 2
EDk,jâˆ¼U(n)
â„“k,Î»t

Î¸â„“k,Î»t(Dj
k), xj

âˆ’â„“k,Î»t
 Î¸â„“k,Î»t(Dk), xj

â‰¤
2Î·2
(Î›k,t,min + 2Î»t(2Î³k,t âˆ’Î³2
k,t)) |Dk|.
(18)
2We only require the regularization coefficient to be sufficiently large
(i.e., Î»t â‰¥âˆ’1
2 Î›k,t,min), without invoking stronger assumptions about the
objective function. Since uniform stability is stricter and argument stability
focuses on parameters, we adopt PHS. Despite the non-convexity of LLM
training, which may introduce theoretical gaps, the resulting bound provides
meaningful qualitative insights into the client numbers and dropout rates.
Tighter bounds under weaker assumptions remain an exciting direction for
future work.


--- Page 6 ---
6
Theorem 1. If the conditions in Definition 1 are met (i.e.,
(17)), PHS of LoRA dropout algorithm on each device can be
upper-bounded as
EDk,jâˆ¼U(n)
â„“k,Î»t

Î¸â„“k,Î»t(Dj
k), xj

âˆ’â„“k,Î»t
 Î¸â„“k,Î»t(Dk), xj

â‰¤
2Î·2
(Î›k,t,min + 2Î»t(2Î³k,t âˆ’Î³2
k,t)) |Dk|.
(19)
Proof. Please refer to Appendix A.
Theorem 2. Based on the relation in (16), PHS of LoRA
dropout algorithm on the server can be upper-bounded as
ED,jâˆ¼U(n)
Lâ€²
{Î½k,t}

Î¸Lâ€²
{Î½k,t}(D{j}), x{j}

âˆ’Lâ€²
{Î½k,t}

Î¸Lâ€²
{Î½k,t}(D), x{j}

â‰¤
K
X
k=1
2Î·2
(Î›k,t,min + 2Î»t(2Î³k,t âˆ’Î³2
k,t))|D|.
(20)
It follows from Theorem 2 that increasing dropout rates
(enhancing sparsity) leads to a reduction in the upper bound,
indicating that sparser models exhibit greater stability. Conse-
quently, with established stability bounds, the generalization
error for the sparse fine-tuned model can be determined.
Lemma 1. For any learning algorithm M having parameter
W and bounded loss function â„“satisfying 0 â‰¤|â„“(x)âˆ’â„“(xâ€²)| â‰¤
C, âˆ€x, xâ€². If M has a PHS Î², with probability 1 âˆ’Î´, we have:
R(M, D) â‰¤Ë†R(M, D) +
s
C2 + 12C|D|Î²
2|D|Î´
,
(21)
where R(M, D) = P
k
|Dk|
|D| E[â„“k(Î¸, |Dk|)] and Ë†R(M, D) =
P
k
|Dk|
|D| â„“k(Î¸, |Dk|) denote the generalization risk and empir-
ical risk of algorithm M running on dataset D, respectively.
Theorem 3. In training round t, given a LoRA dropout
rate Î³k,t and sparsity regularization Î»t, if the conditions in
Definition 1 are met (i.e., (17)), then for some constant C,
with probability 1 âˆ’Î´,
R(M, D) â‰¤Ë†R(M, D)
+
v
u
u
tC2 + PK
k=1
24CÎ·2
Î›k,t,min+2Î»t(2Î³k,tâˆ’Î³2
k,t)
2|D|Î´
.
(22)
Theorem 3 elucidates the connection between generalization
error and dropout rate. It is shown that increasing the dropout
rate decreases the gap between empirical and generalization
errors but concurrently increases empirical error. Therefore,
an appropriate dropout rate would help balance a trade-off
between overfitting and underfitting. By enforcing sparsity
more stringently, a larger Î»t leads to more compact parameter
updates and a lower theoretical generalization error, thereby re-
ducing overfitting risk and improving expected out-of-sample
performance. While the theorem does not explicitly charac-
terize the relationship between generalization error and LoRA
rank, it suggests that decreasing the rank-enhancing parameter
sparsity-typically correlates with reduced generalization error.
This implies that a model with a lower effective rank tends to
have better generalization performance [35].
C. Upper Bounding Loss Descent
However, since Ë†R(M, D) lacks an analytical expression,
the problem is not directly solvable. To address this, upper
bounds for Ë†R(M, D) is derived. Without loss of generality,
the following assumptions are made [36], [37]. We assume a
constant batch size and learning rate in the following analysis.
Assumption 1 (Lipschitz Continuous Gradient). âˆ‡L(âˆ†Î¸) is
Lipschitz continuous with Î· > 0 such that, âˆ¥âˆ‡L(âˆ†Î¸2) âˆ’
âˆ‡L(âˆ†Î¸1)âˆ¥F â‰¤Î·âˆ¥âˆ†Î¸2 âˆ’âˆ†Î¸1âˆ¥F , where âˆ¥Â· âˆ¥F calculates
the Frobenius-norm (F-norm).
Assumption 2 (Bounded Gradient Matrix). The F-norm of
gradient matrix is upper bounded, i.e., E

âˆ¥H(Â·)âˆ¥2
F

â‰¤H2.
Assumption 3 (Small Dropout Rate). The dropout rate of each
device is small so that the higher-order terms can be ignored.
Assumption 4 (Bounded weight). The F-norm of the param-
eter vector is upper bounded, i.e., E

âˆ¥Atâˆ¥2
F

, E

âˆ¥Btâˆ¥2
F

â‰¤
G2, âˆ€k, t.
Assumption 5 (Polyak-Åojaciewicz inequality). Optimal loss
function value of L exists, denoted Lâˆ—, a constant Âµ â‰¥0 exists,
and Ï = L(âˆ†Î¸) âˆ’Lâˆ—that satisfies âˆ¥âˆ‡L(âˆ†Î¸)âˆ¥2
F â‰¥2ÂµÏ.
Specifically, it has been proven in [36], [37] that the
transformer-based LLM possesses a Lipschitz continuous gra-
dient, satisfying Assumption 1. The PL Assumption 5 is
weaker than strong convexity, and usually considered in the
analysis of non-convex cases.
Lemma 2. Similar to [37], the update of âˆ†Wu,t can be
expressed as
âˆ†Wu,t = Bu,tAu,t
= (Bu,tâˆ’1 âˆ’Î±Bâˆ†Ë†
Bu,tâˆ’1)(Au,tâˆ’1 âˆ’Î±Aâˆ†Ë†
Au,tâˆ’1)
(a)
â‰ˆBu,tâˆ’1Au,tâˆ’1 âˆ’Î±ABu,tâˆ’1âˆ†Ë†
Au,tâˆ’1
âˆ’Î±Bâˆ†Ë†
Bu,tâˆ’1Au,tâˆ’1
(b)
= âˆ†Wu,tâˆ’1 âˆ’Î±(Bu,tâˆ’1âˆ†Ë†
Au,tâˆ’1 + âˆ†Ë†
Bu,tâˆ’1Au,tâˆ’1)
(c)
= âˆ†Wu,tâˆ’1 âˆ’Î±(Gu,tâˆ’1 âˆ’Ju,tâˆ’1),
(23)
where (a) comes from the slight term of the gradient matrices
product caused by the Î±AÎ±B, (b) comes from letting Î±A =
Î±B = Î±. In (c), Gu,tâˆ’1 is the desired ideal global gradient
matrix, Ju,tâˆ’1 is the global gradient error matrix caused by
LoRA dropout, and can be written as Ju,tâˆ’1 = Gu,tâˆ’1 âˆ’
Ë†Gu,tâˆ’1, and Ë†Gu,tâˆ’1 = Bu,tâˆ’1âˆ†Ë†
Au,tâˆ’1 + âˆ†Ë†
Bu,tâˆ’1Au,tâˆ’1.
Lemma 3. The layer-wise LoRA dropout error E

âˆ¥Ju,tâˆ’1âˆ¥2
F

can be bounded as
E

âˆ¥Ju,tâˆ’1âˆ¥2
F

â‰¤2
K
X
k=1
|Dk|
|D| G2H2G2n2Î³k,t + 2
K
X
k=1
|Dk|
|D| G2H2G2n1Î³k,t
= 2(n1 + n2)H2G4
K
X
k=1
|Dk|
|D| Î³k,t.
(24)


--- Page 7 ---
7
Proof. Please refer to Appendix B.
By leveraging Assumptions 1-5 and Lemma 3, we derive
the expected upper bound of the optimality gap between
consecutive training rounds by elucidating how the gradient
error matrix Jtâˆ’1 affects the FT procedure.
Theorem 4. Setting the learning rate as Î± = 1
Î·, we get the
upper bounding loss descent by gradient norms,
E[L (âˆ†Î¸t)] âˆ’L (âˆ†Î¸tâˆ’1)
â‰¤âˆ’ÂµÏ
Î· +
U â€²(n1 + n2)H2G4 PK
k=1
|Dk|
|D| Î³k,t
Î·
.
(25)
Proof. Please refer to Appendix C.
D. Convergence Analysis
In this subsection, we investigate the convergence behavior
of FedLoDrop by capturing the learning error caused by the
LoRA dropout. Based on (64), we have
1
2Î· E

âˆ¥âˆ‡L(âˆ†Î¸tâˆ’1)âˆ¥2
F

â‰¤E[L (âˆ†Î¸tâˆ’1) âˆ’L (âˆ†Î¸t)] + 1
2Î· E

âˆ¥Jtâˆ’1âˆ¥2
F

.
(26)
Next, averaging both sides across communication rounds t =
2, 3, . . . , T + 1, and combining with Lemma 3, the average
F-norm of global gradient is bounded by
1
T
T +1
X
t=2
E

âˆ¥âˆ‡L(âˆ†Î¸tâˆ’1)âˆ¥2
F

â‰¤2Î·
T (L (âˆ†Î¸1) âˆ’L (âˆ†Î¸âˆ—))
+ 1
T
T +1
X
t=2
2(n1 + n2)U â€²H2G4
K
X
k=1
|Dk|
|D| Î³k,t,
(27)
where âˆ†Î¸1 and âˆ†Î¸âˆ—denote the initialized model and the
optimal model, respectively. It can be observed that the average
F-norm of global gradient matrix is bounded by two terms:
the first captures the discrepancy between âˆ†Î¸1 and âˆ†Î¸âˆ—,
while the second reflects the optimality gap introduced by
LoRA dropout. As the number of communication rounds
tends to infinity, the right-hand side of (27) converges to
zero. Furthermore, the convergence rate decreases with an
increasing dropout rate, since FedLoDrop reduces per-round
communication overhead at the cost of requiring more rounds
to achieve convergence.
IV. EDGE IMPLEMENTATION OF FEDLODROP
A. System Model
1) Network Model: A single-cell network implements the
FedLoDrop framework, as shown in Fig. 1. The system
includes one edge server with a single-antenna access point
(AP) and K single-antenna edge devices that cooperatively
perform FL via wireless links. Channels are frequency non-
selective, static within each round but vary across rounds.
Using orthogonal frequency division multiplexing (OFDM),
different users are allocated distinct subcarriers to avoid intra-
cell interference. Each device uses separate subcarriers for
downloading and uploading. The server obtains channel state
information (CSI) of channels of all devices by using effective
channel estimation methods (see, e.g., [38]).
2) Latency and Energy Consumption Models: Consider an
arbitrary communication round t and device k. We ignore
latency and energy consumption in the sub-adapter generation
and aggregation steps. The other three steps are modeled as
follows:
â€¢ Downloading Step: Denote the latency of device k on sub-
carrier s to download its assigned sub-adapter as T com,dl
k,t,s
.
If subcarrier s is not allocated to k, i.e., zk,t,s = 0,
T com,dl
k,t,s
= 0. Otherwise,
T com,dl
k,t,s
=
Ë†
Mk,t,sQ
Rdl
k,t,s
, âˆ€zk,t,s = 1,
(28)
where Q is the quantization bits used for one parameter,
Ë†
Mk,t,s is the number of parameters uploaded by device
k on subcarrier s. Rdl
k,t,s is the channel capacity, given as
Rdl
k,t,s = B log2
 
1 +
|hdl
k,t,s|2P com,dl
k,t,s
Ïƒ2
!
,
(29)
where Ïƒ2 is the power of additive white Gaussian noise,
hdl
k,t,s is the downlink channel gain, and P com,dl
k,t,s
is the
downlink transmit power of device k on subcarrier s,
respectively. Then, the overall uploading latency of device
k is decided by the slowest subcarrier:
T com,dl
k,t
= max
s
T com,dl
k,t,s
.
(30)
The energy consumption in this step is to receive the
model, which is included in the circuit energy consump-
tion Î¾k.
â€¢ Local Tuning Step: The latency of device k is given by
T cmp
k,t
= Ck,t|Dk|
fk,t
,
(31)
where fk,t (in cycle/s) is the CPU/GPU frequency of
device k, Ck,t is the number of processor operations to
update the subnet using one data sample. Then, following
[39], the energy consumption is given by
Ecmp
k,t
= â„¦kT cmp
k,t (fk,t)3,
(32)
where â„¦k is a constant characterizing the local computa-
tion performance of the processor on device k.
â€¢ Uploading Step: Denote the latency of device k on
subcarrier s to upload its assigned sub-adapter as T com,ul
k,t,s
.
If subcarrier s is not allocated to k, i.e., zk,t,s = 0,
T com,ul
k,t,s
= 0. Otherwise,
T com,ul
k,t,s
=
Ë†
Mk,t,sQ
Rul
k,t,s
, âˆ€zk,t,s = 1,
(33)
where Rul
k,t,s is the channel capacity, given by
Rul
k,t,s = B log2
 
1 +
|hul
k,t,s|2P com,ul
k,t,s
Ïƒ2
!
.
(34)


--- Page 8 ---
8
In (34), hul
k,t,s is the uplink channel gain, and other
notations follow that in (29). Based on (34), the uplink
transmit power is derived as
P com,ul
k,t,s
=

2
Rul
k,t,s
B
âˆ’1

Ïƒ2
|hul
k,t,s|2 .
(35)
Then, the overall uploading latency of device k is decided
by the slowest subcarrier:
T com,ul
k,t
= max
s
T com,ul
k,t,s
.
(36)
If subcarrier s is not allocated to k, i.e., zk,t,s = 0,
Ecom,ul
k,t,s
= 0. Otherwise, Ecom,ul
k,t,s
= zk,t,sP com,ul
k,t,s
T com,ul
k,t,s
.
The total uploading energy consumption of device k
is the sum of uploading energy consumption over all
subcarriers, given by
Ecom,ul
k,t
=
S
X
s=1
Ecom,ul
k,t,s
.
(37)
In summary, the overall latency of device k in this round
is derived as
Tk,t = T com,dl
k,t
+ T cmp
k,t
+ T com,ul
k,t
, âˆ€k,
(38)
where T com,dl
k,t
is the downlink communication latency
defined in (30), T cmp
k,t
is the computation latency defined
in (31), and T com,ul
k,t
is the uplink communication latency
defined in (36).
The total energy consumption of device k in this round
is
Ek,t = Ecom,ul
k,t
+ Ecmp
k,t
+ Î¾k, âˆ€k,
(39)
where Ecmp
k,t
is the computation energy consumption in
(32), Ecom,ul
k,t
is the communication energy consumption
in (37), and Î¾k is the circuit energy consumption for
global model reception.
B. Problem Formulation
Based on the error bound provided in (22), this subsection
formulates an optimization problem to jointly design the adap-
tive dropout rate and resource allocation, aiming to minimize
this bound. Since probability Î´ and other parameters, i.e.,
C and Î· are constants, minimizing the right side of (22) is
equivalent to
min
{Î³k,t,zk,t,s, Ë†
Mk,t,s}
U â€²(n1 + n2)H2G4 PK
k=1
|Dk|
|D| Î³k,t
Î·
+
v
u
u
t
PK
k=1
12CÎ·2
Î›k,t,min+2Î»t(2Î³k,tâˆ’Î³2
k,t)
|D|Î´
.
(40)
Due to the limited network resources, there are several
constraints on the latency, energy consumption, subcarrier al-
location, and LoRA dropout rate in each round t, as elaborated
below.
1) Per-Round Latency Constraint: The latency of each
device should not exceed the maximum permitted latency T0
to complete this round. Based on the devicesâ€™ latency {Tk,t}
derived in (38), the latency constraint is given by
T com,dl
k,t
+ T cmp
k,t
+ T com,ul
k,t
â‰¤T0, âˆ€k.
(41)
By substituting T com,dl
k,t
given in (30), T cmp
k,t
given in (31), and
T com,ul
k,t
given in (36) into the latency constraint, we have
C1 : max
s
T com,dl
k,t,s
+ T cmp
k,t
+ max
s
T com,ul
k,t,s
â‰¤T0, âˆ€zk,t,s = 1.
(42)
2) Energy Consumption Constraint: The total energy con-
sumption of each device should be no larger than its energy
budget Ek,0. Based on the energy consumption {Ek,t} of
devices given in (39), the energy consumption constraint is
given by
C2 : Ecom,ul
k,t
+ Ecmp
k,t
+ Î¾k â‰¤Ek,0, âˆ€k,
(43)
where Ecmp
k,t
is defined in (32) and Ecom,ul
k,t
is defined in (37).
3) Subcarrier Assignment Constraint: Each subcarrier can
be allocated to one worker:
C3 :
 zk,t,s âˆˆ{0, 1}, âˆ€k,
PK
k=1 zk,t,s = 1.
(44)
where PK
k=1 zk,t,s = 1 represents that the subcarrier s is
allocated to device k.
4) Parameter Constraint: For each device, the total up-
loaded number of parameters on all subcarriers should be no
smaller than its allocated subnet size:
C4 :
S
X
s=1
zk,t,s Ë†
Mk,t,s â‰¥Ë†
Mk,t, âˆ€k,
(45)
where Ë†
Mk,t = (1 âˆ’Î³k,t)M.
5) Dropout Rate Constraint: Based on the definition, the
dropout rate of each device in each communication round
should be limited between 0 and 1, namely,
C5 : 0 â‰¤Î³k,t < 1, âˆ€k.
(46)
Under the constraints above, the optimization problem is
formulated as (47). This is a mixed-integer non-linear problem
(MINLP) and thus NP-hard. In the sequel, an arbitrary round
is considered and the notation t is omitted for simplicity.
V. OPTIMIZED FEDLODROP IMPLEMENTATION
This section first presents an algorithm based on the B&B
framework to jointly optimize P1, which is a well-established
approach for handling mixed binary-continuous optimization
problems. To reduce the complexity, a P-SCA-based algorithm
is then proposed. In the context of the OFDM systems
considered in this work, the problem is further complicated
by the binary subcarrier allocation among devices, along
with the associated inter-subcarrier parameter allocation for
each device. The resource allocation is dynamically adjusted
during training, making it adaptive to changing conditions
and highly practical for real-world environments. To perform
this optimization, the system requires the CSI, data resource
profile, and computational capacity of the devices.


--- Page 9 ---
9
P1 :
min
{Î³k,t,zk,t,s, Ë†
Mk,t,s}
kâˆˆK
U â€²(n1+n2)H2G4 PK
k=1
|Dk|
|D| Î³k,t
Î·
+
s
PK
k=1
12CÎ·2
Î›k,t,min+2Î»t(2Î³k,tâˆ’Î³2
k,t)
|D|Î´
,
s.t.
C1 âˆ¼C5.
(47)
A. An Equivalent Problem Formulation
To handle the integer variables,
Ë†
Mk,s are relaxed to be
continuous for computational simplicity. These values are
rounded to integers for implementation, with negligible loss
due to the typically large magnitudes [38]. For the binary
variables zk,s, the B&B-based algorithm is employed. The
core idea of B&B is to iteratively partition the feasible region
and prune suboptimal subspaces through bound comparisons.
Initially, the root node (i.e., the original problem) is relaxed
by ignoring integrality constraints, in order to compute a
bounding value. To the end, the following variables are used
to transform P1 into a convex problem.
 ËœÎ³k = 1 âˆ’Î³k,
Ëœ
Mk,s = zk,s Ë†
Mk,s.
(48)
Thus, for a given zk,s, the primal problem of P1 can be
written as
P2 :
min
{ËœÎ³k}, { Ëœ
Mk,s},
{T com,ul
k
}, {T com,dl
k
}
I
Î·
K
X
k=1
|Dk|
|D| (1 âˆ’ËœÎ³k)
+
V Î·
p
|D|
v
u
u
t
K
X
k=1
1
âˆ’ËœÎ³2
k + ak
,
s.t.
T com,dl
k
+ Ck|Dk|
fk
+ T com,ul
k
â‰¤T0,
S
X
s=1
Ëœ
Mk,s

2
Rul
k,s
B
âˆ’1

Ïƒ2Q
|hul
k,s|2Rul
k,s
+ Ck|Dk|â„¦kf 2
k + Î¾k â‰¤Ek,0,
S
X
s=1
Ëœ
Mk,s â‰¥ËœÎ³kM,
âˆ€k,
0 â‰¤ËœÎ³k < 1,
âˆ€k,
T com,dl
k
â‰¥
Ëœ
Mk,sQ
Rdl
k,s
,
âˆ€(k, s),
T com,ul
k
â‰¥
Ëœ
Mk,sQ
Rul
k,s
,
âˆ€(k, s),
where I = U â€²(n1 + n2)H2G4, V
=
q
6C
Î´Î» , and ak =
2Î»+Î›k,min
2Î»
.
Lemma 4. Problem P2 is convex.
Proof. Please refer to Appendix D.
B. Global Optimal Solution
As sub-problem P2 is convex, its optimal solution is
obtained by the primal-dual method, which is summarized in
Algorithm 2.
Remark 3. Computational Complexity Analysis: In our B&B-
based method, the problem is solved iteratively. At each
iteration, the primal problem, which has a complexity of
O(K2S), is solved using the primal-dual method. Let Iiter be
the number of iterations for the master problem. Thus, the total
complexity is O(Iiter Â·K2S), reaching up to O(2KS Â·K2S) in
the worst case.
Algorithm 2: B&B-based Primal-dual Algorithm
Input: {Rdl
k }, {Rul
k } {P com,ul
k
}, {fk}, T0, {Ek,0}.
1 Initialize {Î¶(0)
s }, {Î¹(0)
k },{Ï‡(0)
k }, {Îº(0)
k }, {Ï•(0)
k },
{Ïˆ(0)
k }, the step sizes {Î·Î¶s}, {Î·Î¹k}, {Î·Ï‡k}, {Î·Îºk},
{Î·Ï•k}, {Î·Ïˆk}, and i = 0.
2 Loop.
3 Update the multipliers as
Î¶(i+1)
s
= max

Î¶(i)
s
+ Î·Î¶s
âˆ‚LP2
âˆ‚Î¶s
, 0

,
Î¹(i+1)
k
= max

Î¹(i)
k + Î·Î¹k
âˆ‚LP2
âˆ‚Î¹k
, 0

, âˆ€k,
Ï‡(i+1)
k
= max

Ï‡(i)
k + Î·Ï‡k
âˆ‚LP2
âˆ‚Ï‡k
, 0

, âˆ€k,
4 Calculate {ËœÎ³k} and { Ëœ
Mk,s}.
5 Get {Î³k} and { Ë†
Mk,s} using (48).
6 i = i + 1.
7 Until Convergence.
Output: {Î³k} and { Ë†
Mk,s}.
C. Low-complexity suboptimal solution
Although the proposed B&B-based algorithm obtains the
globally optimal solution, it may incur high computational
complexity due to the iterative partitioning and search for
the best-known feasible solution [40], [41]. To address this
limitation, a low-complexity P-SCA-based algorithm is pro-
posed as an alternative, aiming to strike a balance between
computational efficiency and system performance.
Specifically, the non-convex constraint (C3) is converted to
an equivalent form with continuous variables as follows:
C3 â†’
(
C6 : zk,s(1 âˆ’zk,s) â‰¤0,
C7 : 0 â‰¤zk,s â‰¤1.
(49)
By applying the penalty method, objective function of P2 is
transformed to I
Î·
PK
k=1
|Dk|
|D| (1âˆ’ËœÎ³k)+
V Î·
âˆš
|D|
qPK
k=1
1
âˆ’ËœÎ³2
k+ak +
Ï„ PK
k=1
PS
s=1(zk,sâˆ’z2
k,s). Approximating the objective func-
tion by using the first-order Taylor expansion. zk,s âˆ’z2
k,s is
rewritten as a difference-of-convex (DC) function shown as
zk,s âˆ’z2
k,s = Î¨1(zk,s) âˆ’Î¨2(zk,s), where Î¨1(zk,s) = zk,s,


--- Page 10 ---
10
TABLE I
GLUE BENCHMARK
Method
CoLa (Mcc)
RTE
MRPC
SST-2
QNLI
STS-B (Corr)
Avg
FedIT in [20]
62.08
77.26
90.44
95.99
94.86
91.54
85.36
FedLoDrop
(0.2)
63.56
78.70
89.46
96.67
94.95
91.34
85.78
FedLoDrop
(0.3)
62.84
81.6
88.48
96.56
95.17
90.84
85.92
FFA in [23]
58.32
70.76
87.49
95.59
93.85
90.24
82.71
FFA with Bernoulli Dropout
(0.2)
62.37
74.73
83.33
95.76
94.55
89.12
83.31
FFA with Bernoulli Dropout
(0.3)
61.83
69.31
78.18
96.22
94.29
88.84
81.45
FedIT with Gaussian Dropout
(0.2)
62.08
77.98
90.20
96.22
94.25
91.72
85.41
FedIT with Gaussian Dropout
(0.3)
61.82
77.98
89.22
95.41
93.85
91.47
84.96
Î¨2(zk,s) = z2
k,s. Thus, the convex lower bound of Î¨2(zk,s)
can be obtained by the first-order Taylor expansion, i.e.,
Â¯Î¨2(zk,s) = Î¨2(Â¯zk,s) + âˆ‡Î¨2(Â¯zk,s) (zk,s âˆ’Â¯zk,s)
= 2 Â¯zk,szk,s âˆ’(Â¯zk,s)2,
(50)
where Â¯zk,s is represents a feasible solution to Problem P2.
With Â¯Î¨2(zk,s), zk,sâˆ’z2
k,s is approximated by its convex upper
bound denoted as zk,sâˆ’z2
k,s â‰¤Î¨1(zk,s)âˆ’Â¯Î¨2(zk,s). An upper
bound of the problem can be found by solving
P3
min
{ËœÎ³k},{ Ëœ
Mk,s},{zk,s}
{T com,ul
k
},{T com,dl
k
}
I
Î·
K
X
k=1
|Dk|
|D| (1 âˆ’ËœÎ³k)
+
V Î·
p
|D|
v
u
u
t
K
X
k=1
1
âˆ’ËœÎ³2
k + ak
+ Ï„
K
X
k=1
S
X
s=1
(1 âˆ’2Â¯zk,s)zk,s + (Â¯zk,s)2.
Note that problem P3 is convex and can be solved by using
the primal-dual method as well.
Remark 4. Computational Complexity Analysis: solving
problem P3 is with the complexity order of O(Iâ€²
iter Â· K2S),
where Iâ€²
iter represents the iteration numbers.
VI. SIMULATION RESULTS
We evaluate the superiority of the proposed FedLoDrop
framework and compare it with other baseline methods on
two major NLP tasks: natural language understanding (NLU)
and question answering (QA). We first fine-tune two LLMs
including RoBERTa large (355M) and LLaMA (7B) on two
benchmarks including GLUE and MMLU. Following the con-
figurations in the original LoRA paper [8], the LoRA modules
are applied to the self-attention layers. For RoBERTa large,
LoRA rank is set as 4 in a three-client cross-silo federated
setting, and r = 16 with 10 devices for LLaMA in [23], [42].
The local batch size, epoch and learning rate are set to be 64,
1, and 3e-4, respectively. The AdamW optimizer is adopted
during the training. The downlink channel gains of devices
are assumed to be Rayleigh fading. The path loss is 10âˆ’3.
For data distribution among clients, a common method of
randomly sampling non-IID data for each client is employed,
as implemented in [20], [43]. Experiments are performed using
4 NVIDIA RTX 4090 (24 GB each) GPUs.
A. Effects of Dropout Rate
Fixed-rate experiments are conducted for ablation and
sensitivity analysis, which establish a baseline performance
landscape to compare how different levels of regularization
(dropout rates) affect final model accuracy and convergence.
Table I presents the results of RoBERTa-large on the GLUE
benchmark. It can be observed that models incorporating the
dropout method generally achieve superior performance, sug-
gesting that the proposed FedLoDrop framework helps LoRA-
based models control overfitting and enhance generalization
on downstream tasks. The framework is also applicable to
FFA [23]. In contrast to FedLoDrop, when the dropout rate
is set to 0.3, the average score begins to decline. This is
attributed to fewer parameters being updated under this setting,
which limits model adaptability and consequently reduces
performance.
To visualize the trend, we plot the effects of dropout rate
on the performance, as shown in Fig. 2. For LLaMA, we
selected 1444 samples from the dataset for a quick and
comprehensive evaluation, noted as Sampled. For MMLU,
among 57 disciplines, Humanities, Stem, and Average are
plotted. In both figures, as the dropout rate increases, the
accuracy first improves and then drops. This aligns with the
theoretical derivation that a proper dropout rate would help
balance the adaptation functionâ€™s empirical risk minimization
and complexity. A small dropout rate might fail to introduce
sufficient sparsity and lead to overfitting, while an excessively
large dropout rate would result in too few trainable parameters,
making the adapter lose its expressive power. This contrast
underscores the necessity of finding an optimal, and potentially
dynamic, dropout strategy.
To evaluate sensitivity to the dropout distribution, we
compare Bernoulli and Gaussian dropout. Gaussian dropout
multiplies activations by a continuous random variable drawn
from N(1, Ïƒ2
gaus) instead of setting them to zero. The results
on RoBERTa-large and LLaMA models are presented in Table
I and Fig. 2(b), respectively. The comparable performance
of both methods demonstrates the robustness to the specific
implementation of dropout. Bernoulli dropout provides more
stable results across a wider range of rates, while Gaussian
dropout yields slightly better performance at lower rates (e.g.,
0.2 for RoBERTa-large, 0.1 for LLaMA), likely because its
continuous noise injection prevents the complete silencing of


--- Page 11 ---
11
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Dropout Rate
77.5
80.0
82.5
85.0
87.5
90.0
92.5
95.0
97.5
Accuracy (%)
SST2
MRPC
RTE
(a) Accuracy v.s. dropout rate on RoBERTa large
0.0
0.1
0.2
0.3
0.4
0.5
Dropout Rate
26
27
28
29
30
31
32
Accuracy (%)
Humanities
Stem
Average
Sampled
Sampled Gaussian Drop
(b) Accuracy v.s. dropout rate on LLaMA
Fig. 2. Effects of dropout rate on the performance.
200
250
300
350
400
Per-round Latency (s)
72.5
75.0
77.5
80.0
82.5
85.0
87.5
90.0
Accuracy (%)
MRPC: Scheme Without Dropout
MRPC: B&B-based Scheme
MRPC: P-SCA-based Scheme
MRPC: Subcarrier-fixed Scheme
RTE: Scheme Without Dropout
RTE: B&B-based Scheme
RTE: P-SCA-based Scheme
RTE: Subcarrier-fixed Scheme
(a) Accuracy v.s. per-round latency on RoBERTa large
200
250
300
350
400
Per-round Latency (s)
26.4
26.6
26.8
27.0
27.2
27.4
27.6
Accuracy (%)
Scheme Without Dropout
B&B-based Scheme
P-SCA-based Scheme
Subcarrier-fixed Scheme
(b) Accuracy v.s. per-round latency on LLaMA
Fig. 3. Effects of per-round latency on the performance.
0.265
0.270
0.275
0.280
Accuracy
0.2679
0.2703
0.2730
0.2754
0.2775
Round 1
Round 2
Round 3
Round 4
Round 5
Communication Round
Device 1
Device 2
Device 3
Device 4
Device 5
Device 6
Device 7
Device 8
Device 9
Device 10
0.25
0.38
0.35
0.35
0.26
0.43
0.35
0.34
0.19
0.37
0.25
0.23
0.37
0.28
0.32
0.43
0.24
0.35
0.15
0.22
0.30
0.35
0.30
0.22
0.47
0.38
0.29
0.39
0.22
0.27
0.42
0.32
0.30
0.30
0.28
0.28
0.26
0.23
0.32
0.26
0.33
0.40
0.39
0.24
0.30
0.31
0.29
0.23
0.35
0.25
0.0
0.2
0.4
0.6
0.8
1.0
Dropout Rate
Fig. 4. The evolution of the optimized dropout rates under different rounds.
neurons. In contrast, higher dropout rates lead to exploding
variance, potentially destabilizing the training process.
B. Effects of Network Resources
To verify the superiority of the proposed two schemes, the
following benchmark schemes are compared under different
network resources. 1) Scheme Without Dropout: Sufficient
resource is allocated and dropout is not considered, which
is an ideal benchmark used to compare with the proposed
schemes. 2) Subcarrier-fixed Scheme: Subcarrier is fixed, and
then optimize the remaining variables the same as the proposed
ones. Fig. 3 illustrates the model accuracy v.s. different per-
round latencies. It can be observed that, under a given per-
round latency threshold, both the proposed B&B-based and
the P-SCA-based algorithms outperform the subcarrier-fixed
scheme when the constraint is satisfied. The P-SCA-based
algorithm is capable of obtaining a suboptimal solution with
performance close to that of the optimal method. Furthermore,
as the allowable per-round training delay increases, the model
performance tends to improve. This is attributed to a looser
constraint allowing for a lower dropout rate, which effectively
reduces the noise introduced during training and thereby
enhances overall performance.
The evolution of the optimized dropout rates for a subset
of clients over the training rounds can be shown in Fig. 4.
Fig. 4 is plotted under the per-round latency of 400s. This
visualization clearly shows how our algorithm dynamically


--- Page 12 ---
12
1
2
3
4
5
Communication Round
26.00
26.25
26.50
26.75
27.00
27.25
27.50
27.75
28.00
Accuracy (%)
Without Dropout
Latency = 200s
Latency = 250s
Latency = 300s
Latency = 350s
Latency = 400s
(a) Testing accuracy v.s. communication round
0.0
0.2
0.4
0.6
0.8
1.0
Epoch
1.0
1.2
1.4
1.6
1.8
2.0
2.2
2.4
Loss
Device 1
Device 5
Device 9
(b) Training loss v.s. epoch
Fig. 5. Convergence performance.
CoLa
MRPC
SST-2
STS-B
Task
0
20
40
60
80
100
Accuracy or Correlation
62.8459.34
88.4887.92
96.5695.76
90.84 89.5
K=3
K=5
(a) Effects of number of devices on RoBERTa large
Humanities
Stem
Average
Sampled
Task
0
5
10
15
20
25
30
Accuracy
32.6932.45
28.1 28.26
32.4532.28
28.9727.86
K=5
K=10
(b) Effects of number of devices on LLaMA (7B)
1
2
4
8
16
Rank
77.5
80.0
82.5
85.0
87.5
90.0
92.5
95.0
Accuracy (%)
QNLI ( = 0.3)
STS-B ( = 0.3)
RTE ( = 0.3)
RTE ( = 0.2)
RTE ( = 0.4)
(c) Effects of ranks
Fig. 6. Effects of parameters on the performance.
adjusts the dropout rate (regularization strength) per device
based on their real-time CSI and state. Simultaneously, the
testing accuracy improves as the number of communication
rounds increases, indicating effective learning despite adaptive
parameter reduction.
In addition to reporting the final accuracy to evaluate gener-
alization performance, we present the global testing accuracy
v.s. communication round and the device training loss v.s.
epoch to illustrate the convergence behavior, as shown in Fig.
5(a) and Fig. 5(b), respectively. For clarity, we plot the training
loss of three representative devices. The standard federated
LoRA scheme is denoted by â€™Without Dropoutâ€™. It can be
observed that the convergence rate improves under a looser
per-round latency constraint, which aligns with the theoretical
analysis in Section III-D.
C. Effects of Resource Utilization Efficiency
1) Per-round Latency: As shown in Fig. 3(a) and Fig.
3(b), given a fixed target accuracy, the proposed B&B-based
and P-SCA-based algorithms achieve the same performance
with shorter per-round latency compared to the subcarrier-
fixed scheme. This improvement is attributed to the reduced
number of transmitted parameters. Notably, when targeting a
higher accuracy (e.g., 27.2% on LLaMA), the subcarrier-fixed
scheme and the scheme without dropout fail to achieve the
desired performance (27.2% testing accuracy), highlighting the
limitations of fixed resource allocation under stringent latency
and accuracy requirements.
2) Communication Overhead: To quantify communication
overhead, we compare the communicated parameters of the
full model FT, FedIT, and FedLoDrop across 5 communication
rounds. The results show that FedLoDrop achieves the lowest
communication cost, transmitting only 7.6% of the parameters
required by full FT, while FedIT transmits 9.5%. By trans-
mitting only a subset of the model parameters, FedLoDrop
significantly reduces the volume of data exchanged per round
compared to full FT approaches.
D. Effects of Parameters and Models
Fig. VI-A presents the effects of parameters on the per-
formance. From Fig. 6(a) and Fig. 6(b), one can see that in
both models, a larger number of devices generally results in a
lower testing accuracy due to the difficulty in adapting to one
global model. We further evaluate the sampled setting with 50
devices on LLaMA, where the accuracy decreases modestly
to 26.57. As shown in Fig. 6(c), with Î³ = 0.3, performance
first improves with rank but slightly declines when the rank
becomes large due to overfitting. For RTE, we further include
results under dropout rates of 0.2 and 0.4. It can be observed
that insufficient dropout (0.2) leads to overfitting at high ranks,
while strong dropout (0.4) can initially cause underfitting but
enables higher ranks to be effectively utilized.


--- Page 13 ---
13
As a final stress test for FedLoDrop, we scale up to Yi-
34B. Due to the high training cost, we only report the results
for given dropout rates. The LoRA rank is set as 16, with
the learning rate of 3e âˆ’4. Experiments are performed on
2 NVIDIA A100 (40 GB each) GPUs. As shown in Table
II, FedLoDrop with 0.2 dropout rate achieves the best per-
formance, consistent with the findings in Section VI-A. For
brevity and due to space constraints, we omit the full results.
TABLE II
ACCURACY V.S. DROPOUT RATE ON YI-34B.
Dropout Rate
0
0.1
0.2
0.3
Accuracy
43.04
43.35
43.39
42.54
VII. CONCLUSION
In this paper, we propose a new framework, called Fed-
LoDrop, which effectively incorporates a parameter-efficient
finetuning technique, known as LoRA, to facilitate local train-
ing. This method reduces computational and communication
overheads for local edge devices that have limited resources.
We analytically demonstrate the trade-off between underfitting
and overfitting by deriving mechanisms that elucidate these
dynamics. The application of dropout alleviates the imbal-
anced update of the parameter matrix and mitigates parameter
overfitting in LoRA. Specifically, PHS-based analysis reveals
that increasing the dropout rate narrows the gap between
empirical and generalization errors but also increases empirical
error. Based on this insight, we propose two schemes, B&B-
based and P-SCA-based algorithms to minimize the general-
ization error bound by jointly optimizing the dropout rate and
network resource allocation, thereby enhancing the learning
performance. Frameworks for collaborative federated sensing
and communication systems will be left for future directions.
Besides, in future work, the proposed FedLoDrop can be
extended to multi-modal large language models (MLLMs) to
dynamically adjust local dropout strategies based on modality
importance or client-specific resource constraints.
APPENDIX A
PROOF OF THEOREM 1
A. Proof of Theorem 2
Lemma 5. Consider the learning algorithm M optimizing the
following loss function:
min
Î¸k,t â„“k,Î»t(Î¸k,t) = min
Î¸k,t â„“k,t(Î¸k,t) + Î»tâˆ¥Î¸k,t âˆ’Î¸0âˆ¥2
2.
(51)
If the requirements in Definition 1 are met, then it has PHS
Î² =
2Î·2
(Î›k,t,min+2Î»t)|Dk|, which is
EDk,jâˆ¼U(n)
â„“k,Î»t

Î¸â„“k,Î»t(Dj
k), xj

âˆ’â„“k,Î»t
 Î¸â„“k,Î»(Dk), xj

â‰¤
2Î·2
(Î›k,t,min + 2Î»t) |Dk|.
(52)
Proof. Denote Î¸â„“k,Î»t(Dk) as Ë†Î¸k,t, and âˆ†Ë†Î¸k,t = Ë†Î¸k,t âˆ’Î¸.
Consider the second-order Taylor expansion of â„“k,Î»t at local
optimal Ë†Î¸k,t, âˆ‡â„“k,Î»t(Ë†Î¸k,t) = 0. Thus, for âˆ€v close to Ë†Î¸k,t,
â„“k,Î»t(v) = â„“k,Î»t(Ë†Î¸k,t) + 1
2(v âˆ’Ë†Î¸k,t)âŠ¤âˆ‡2â„“k,Î»t(Ë†Î¸k,t)(v âˆ’Ë†Î¸k,t).
(53)
Then, expand âˆ‡2â„“k,Î»t(Ë†Î¸k) in (53),
âˆ‡2â„“k,Î»t(Ë†Î¸k) = âˆ‡2
Ë†Î¸k,t(â„“k(Ë†Î¸k,t) + Î»tâˆ¥Ë†Î¸k,t âˆ’Î¸0âˆ¥2
2)
= âˆ‡2â„“k(Ë†Î¸k,t) + 2Î»tI = Uk,tdiag(Î›k,t)U âˆ’1
k,t + 2Î»tI
= Uk,t(diag(Î›k,t) + 2Î»tI)U âˆ’1
k,t
= Uk,tdiag(
p
Î›k,t,1 + 2Î»t, Â· Â· Â· ,
p
Î›k,t,d + 2Î»t)U âˆ’1
k,t Uk,t
diag(
p
Î›k,t,1 + 2Î»t, Â· Â· Â· ,
p
Î›k,t,d + 2Î»t)U âˆ’1
k,t .
(54)
Take this back to (53),
â„“k,Î»t(v) âˆ’â„“k,Î»t(Ë†Î¸k,t)
= 1
2âˆ¥(Uk,tdiag(
p
Î›k,t,1 + 2Î»t, Â· Â· Â· ,
p
Î›k,t,d + 2Î»t)U âˆ’1
k,t )
(v âˆ’Ë†Î¸k,t)âˆ¥2
2
â‰¥1
2(Î›k,t,min + 2Î»t)âˆ¥v âˆ’Ë†Î¸k,tâˆ¥2
2.
(55)
According to the definition of â„“k,Î»t(Ë†Î¸k,t), for âˆ€u, v close to
Ë†Î¸k,t, (56) is got. Take u = Î¸â„“k,Î»t(Dj
k) and v = Î¸â„“k,Î»t(Dk),
â„“k,Î»t(Î¸â„“k,Î»t(Dj
k)) âˆ’â„“k,Î»(Î¸â„“k,Î»t(Dk))
â‰¤
â„“k,Î»t

Î¸â„“k,Î»t(Dj
k), xj

âˆ’â„“k,Î»t
 Î¸â„“k,Î»t(Dk), xj

|Dk|
,
(57)
take (55) into (57),
1
2(Î›k,t,min + 2Î»t)âˆ¥Î¸â„“k,Î»t(Dj
k) âˆ’Î¸â„“k,Î»t(Dk)âˆ¥2
2
â‰¤
â„“k,Î»t

Î¸â„“k,Î»t(Dj
k), xj

âˆ’â„“k,Î»t
 Î¸â„“k,Î»t(Dk), xj

|Dk|
.
(58)
Because the loss function is Î·-Lipschitz, thus,
â„“k,Î»t

Î¸â„“k,Î»t(Dj
k), xj

âˆ’â„“k,Î»t
 Î¸â„“k,Î»t(Dk), xj

|Dk|
â‰¤Î·âˆ¥Î¸â„“k,Î»t(Dj
k) âˆ’Î¸â„“k,Î»t(Dk)âˆ¥
|Dk|
.
(59)
Take (59) into (58), and basic calculation,
âˆ¥Î¸â„“k,Î»(Dj
k) âˆ’Î¸â„“k,Î»(Dk)âˆ¥â‰¤
2Î·
(Î›k,min + 2Î») |Dk|.
(60)
Plug (60) into (59), and as this holds for any j and Dk, the
proof of Lemma 5 is finished.
B. PHS upper bound of FedLoDrop
Consider loss function with sparsity regularization,
â„“k,Î»t(Î¸k,t)
= â„“k,t (Î¸k) + Î»tEdk,tâˆ¼Bern(2Î³k,tâˆ’Î³2
k,t)
X
j
d2
k,t,j(Î¸k,t,j âˆ’Î¸0,j)2
= â„“k,t (Î¸k) + Î»t
X
j
(Î¸k,t,j âˆ’Î¸0,j)2Edk,t,jâˆ¼Bern(2Î³k,tâˆ’Î³2
k,t)d2
k,j
= â„“k,t (Î¸k) + Î»t
X
j
(Î¸k,t,j âˆ’Î¸0,j)2(2Î³k,t âˆ’Î³2
k,t)
= â„“k,t (Î¸k) + Î»t(2Î³k,t âˆ’Î³2
k,t)âˆ¥Î¸k,t âˆ’Î¸0âˆ¥2.
(61)


--- Page 14 ---
14
â„“k,Î»t(u) âˆ’â„“k,Î»t(v) =
 
1
|Dk|
X
xiâˆˆDk
â„“k (u, xi) + Î»tâˆ¥u âˆ’Î¸0âˆ¥2
2
!
âˆ’
 
1
|Dk|
X
xiâˆˆDk
â„“k (v, xi) + Î»tâˆ¥v âˆ’Î¸0âˆ¥2
2
!
=

1 âˆ’
1
|Dk|
 ï£«
ï£­
1
|Dk| âˆ’1
X
iÌ¸=j
â„“k (u, xi) + Î»tâˆ¥u âˆ’Î¸0âˆ¥2
2
ï£¶
ï£¸âˆ’

1 âˆ’
1
|Dk|
 ï£«
ï£­
1
|Dk| âˆ’1
X
iÌ¸=j
â„“k (v, xi) + Î»tâˆ¥v âˆ’Î¸0âˆ¥2
2
ï£¶
ï£¸
+ Î»t(âˆ¥u âˆ’Î¸0âˆ¥2
2 âˆ’âˆ¥v âˆ’Î¸0âˆ¥2
2)
|Dk|
+ â„“k (u, xj) âˆ’â„“k (v, xj)
|Dk|
=

1 âˆ’
1
|Dk|
 ï£®
ï£°
ï£«
ï£­
1
|Dk| âˆ’1
X
iÌ¸=j
â„“k (u, xi) + Î»tâˆ¥u âˆ’Î¸0âˆ¥2
2
ï£¶
ï£¸âˆ’
ï£«
ï£­
1
|Dk| âˆ’1
X
iÌ¸=j
â„“k (v, xi) + Î»tâˆ¥v âˆ’Î¸0âˆ¥2
2
ï£¶
ï£¸
ï£¹
ï£»
+ â„“k,Î»t (u, xj) âˆ’â„“k,Î»t (v, xj)
|Dk|
.
(56)
Integrating the result above to Lemma 5 and substituting the
regularization coefficient finalizes the proof.
APPENDIX B
PROOF OF LEMMA 3
Denote
Ë†
Ju,k,A,tâˆ’1
=
âˆ†Au,k,tâˆ’1
âˆ’
âˆ†Ë†
Au,k,tâˆ’1,
Ë†
Ju,k,B,tâˆ’1 = âˆ†Bu,k,tâˆ’1 âˆ’âˆ†Ë†
Bu,k,tâˆ’1, thus
E

âˆ¥Ju,tâˆ’1âˆ¥2
F

= E
"
âˆ¥
K
X
k=1
|Dk|
|D| (Gu,k,tâˆ’1 âˆ’Ë†Gu,k,tâˆ’1)âˆ¥2
F
#
= E
"
âˆ¥
K
X
k=1
|Dk|
|D| (Bu,tâˆ’1 Ë†
Ju,k,A,tâˆ’1 + Ë†
Ju,k,B,tâˆ’1Au,tâˆ’1)âˆ¥2
F
#
(a)
â‰¤2
K
X
k=1
|Dk|
|D| E
h
âˆ¥Bu,tâˆ’1 Ë†
Ju,k,A,tâˆ’1âˆ¥2
F
i
+ 2
K
X
k=1
|Dk|
|D| E
h
Au,tâˆ’1 Ë†
Ju,k,B,tâˆ’1âˆ¥2
F
i
,
(62)
where
(a)
comes
from
the
convexity
of
F-
norm.
The
Taylor
expansion
is
adopted
to
approximate
the
weights
of
neural
networks,
i.e.,
âˆ†Ë†
Au,k,tâˆ’1
=
âˆ†Au,k,tâˆ’1 + O

Ë†
Au,k,tâˆ’1 âˆ’Au,k,tâˆ’1

+
H(Au,k,tâˆ’1)

Ë†
Au,k,tâˆ’1 âˆ’Au,k,tâˆ’1

. Under Assumption 3,
the last higher order on the right side can be ignored. Thus,
E
h
âˆ¥Ë†
Ju,k,A,tâˆ’1âˆ¥2
F
i
= E
h
âˆ¥âˆ’H(Au,k,tâˆ’1)

Ë†
Au,k,tâˆ’1 âˆ’Au,k,tâˆ’1

âˆ¥2
F
i
â‰¤H2 Â· E

âˆ¥Au,k,tâˆ’1 (diag(mA,k,tâˆ’1) âˆ’I) âˆ¥2
F

â‰¤H2G2E
" n2
X
q=1
(mA,k,tâˆ’1,q âˆ’1)2
#
= H2G2n2Î³k,t.
(63)
Take
(63)
back
into
(62),
and
minor
changes
to
E
h
âˆ¥Ë†
Ju,k,B,tâˆ’1âˆ¥2
F
i
, Lemma 3 has been proven.
APPENDIX C
PROOF OF THEOREM 4
According to Assumption 1, set Î± = 1
Î·, and take expectation
on both sides,
E[L (âˆ†Î¸t)] âˆ’L (âˆ†Î¸tâˆ’1)
â‰¤âˆ’1
2Î· âˆ¥âˆ‡L(âˆ†Î¸tâˆ’1)âˆ¥2
F + 1
2Î· E

âˆ¥Jtâˆ’1âˆ¥2
F

â‰¤âˆ’ÂµÏ
Î· + 1
2Î· E

âˆ¥Jtâˆ’1âˆ¥2
F

.
(64)
As Jtâˆ’1 â‰œ{Ju,tâˆ’1}U â€²
u=1, it can be further written as
E

âˆ¥Jtâˆ’1âˆ¥2
F

= E
ï£®
ï£¯ï£¯ï£¯ï£¯ï£°

ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
J1,tâˆ’1
Â· Â· Â·
Ju,tâˆ’1
Â· Â· Â·
JU â€²,tâˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸

2
F
ï£¹
ï£ºï£ºï£ºï£ºï£»
=
U â€²
X
u=1
E

âˆ¥Ju,tâˆ’1âˆ¥2
F

= U â€²E

âˆ¥Ju,tâˆ’1âˆ¥2
F

.
(65)
Taking this back to (64), and combining it with Lemma 3, the
proof is completed.
APPENDIX D
PROOF OF LEMMA 4
All the constraints and objective function can be derived
by substituting the variable transformation in (48). The first
term in the objective function is convex. For the second one,
considering two dimensions, e.g., Î³1, Î³2, its Hessian matrix is

o11
o12
o21
o22

where,
o11
=
4Î³2
1
(a1âˆ’Ëœ
Î³12)3q
1
a1âˆ’Ëœ
Î³12 +
1
a2âˆ’Ëœ
Î³22
âˆ’
Î³2
1
(a1âˆ’Ëœ
Î³12)4(
1
a1âˆ’Ëœ
Î³12 +
1
a2âˆ’Ëœ
Î³22 )3/2
+
1
(a1âˆ’Ëœ
Î³12)2q
1
a1âˆ’Ëœ
Î³12 +
1
a2âˆ’Ëœ
Î³22 ,
o12 = o21 = âˆ’
Î³1Î³2
(a1âˆ’Ëœ
Î³12)2(a2âˆ’Ëœ
Î³22)2(
1
a1âˆ’Ëœ
Î³12 +
1
a2âˆ’Ëœ
Î³22 )3/2 , o22 =
4Î³2
2
(a2âˆ’Ëœ
Î³22)3q
1
a1âˆ’Ëœ
Î³12 +
1
a2âˆ’Ëœ
Î³22 âˆ’
Î³2
2
(a2âˆ’Ëœ
Î³22)4(
1
a1âˆ’Ëœ
Î³12 +
1
a2âˆ’Ëœ
Î³22 )3/2 +
1
(a2âˆ’Ëœ
Î³22)2q
1
a1âˆ’Ëœ
Î³12 +
1
a2âˆ’Ëœ
Î³22 , and is non-negative. The higher
dimensions can be generalized with inductions and definitions.
Therefore, the overall objective function is convex. Besides,
all constraints are linear relations, and thus are convex.


--- Page 15 ---
15
REFERENCES
[1] Y. Shen, J. Shao, X. Zhang, Z. Lin, H. Pan, D. Li, J. Zhang, and K. B.
Letaief, â€œLarge language models empowered autonomous edge AI for
connected intelligence,â€ IEEE Commun. Mag., vol. 62, no. 10, pp. 140â€“
146, 2024.
[2] J. Du, T. Lin, C. Jiang, Q. Yang, C. F. Bader, and Z. Han, â€œDistributed
foundation models for multi-modal learning in 6G wireless networks,â€
IEEE Wireless Commun., vol. 31, no. 3, pp. 20â€“30, 2024.
[3] P. Jiang, C.-K. Wen, X. Yi, X. Li, S. Jin, and J. Zhang, â€œSemantic
communications using foundation models: Design approaches and open
issues,â€ IEEE Wireless Commun., vol. 31, no. 3, pp. 76â€“84, 2024.
[4] F. Jiang, C. Pan, L. Dong, K. Wang, M. Debbah, D. Niyato, and
Z. Han, â€œA comprehensive survey of large AI models for future com-
munications: Foundations, applications and challenges,â€ arXiv preprint
arXiv:2505.03556, 2025.
[5] F. Wu, J. Hu, G. Min, and S. Wang, â€œAdaptive rank allocation for
federated parameter-efficient fine-tuning of language models,â€ arXiv
preprint arXiv:2501.14406, 2025.
[6] B. Zhu, Y. Niu, Y. Han, Y. Wu, and H. Zhang, â€œPrompt-aligned gradient
for prompt tuning,â€ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV),
2023, pp. 15 659â€“15 669.
[7] J.
Bai,
Z.
Yan,
Z.
Yang,
J.
Yang,
X.
Liang,
H.
Guo,
and
Z. Li, â€œKnowprefix-tuning: A two-stage prefix-tuning framework for
knowledge-grounded dialogue generation,â€ in Proc. Euro. Conf. Mach.
Learn. (ECML), 2023, pp. 525â€“542.
[8] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,
and W. Chen, â€œLoRA: Low-rank adaptation of large language models,â€
in Proc. Int. Conf. Learn. Represent. (ICLR), 2022.
[9] K. Kuo, A. Raje, K. Rajesh, and V. Smith, â€œFederated Lora with sparse
communication,â€ arXiv preprint arXiv:2406.05233, 2024.
[10] X. Qiu, T. Hao, S. Shi, X. Tan, and Y.-J. Xiong, â€œChain-of-LoRA: En-
hancing the instruction fine-tuning performance of low-rank adaptation
on diverse instruction set,â€ IEEE Signal Process Lett., vol. 31, pp. 875â€“
879, 2024.
[11] H. Wu, X. Chen, and K. Huang, â€œResource management for low-latency
cooperative fine-tuning of foundation models at the network edge,â€ IEEE
Trans. Wireless Commun., vol. 24, no. 6, pp. 4839â€“4852, 2025.
[12] G. Qu, Q. Chen, W. Wei, Z. Lin, X. Chen, and K. Huang, â€œMobile edge
intelligence for large language models: A contemporary survey,â€ IEEE
Commun. Surv. Tutorials, pp. 1â€“1, 2025.
[13] D. Wen, S. Xie, X. Cao, Y. Cui, J. Xu, Y. Shi, and S. Cui, â€œIntegrated
sensing, communication, and computation for over-the-air federated
edge learning,â€ IEEE Trans. Wireless Commun., pp. 1â€“1, 2025.
[14] W. Ni, H. Ao, H. Tian, Y. C. Eldar, and D. Niyato, â€œFedsl: Federated split
learning for collaborative healthcare analytics on resource-constrained
wearable iomt devices,â€ IEEE Internet Things J., vol. 11, no. 10, pp.
18 934â€“18 935, 2024.
[15] X. Shang, Z. Liu, D. Gao, D. Yang, W. Zhang, C. H. Foh, and
H. Zhang, â€œComputing and network load balancing for decentralized
deep federated learning in industrial cyber-physical systems: A multi-
task approach,â€ IEEE J. Sel. Areas Commun., vol. 43, no. 9, pp. 2997â€“
3013, 2025.
[16] Y. Ai, Q. Chen, G. Zhu, D. Wen, H. Jiang, J. Zeng, and M. Li, â€œClustered
federated multi-task learning: A communication-and-computation effi-
cient sparse sharing approach,â€ IEEE Trans. Wireless Commun., vol. 24,
no. 6, pp. 4824â€“4838, 2025.
[17] Z. Wang, K. Huang, and Y. C. Eldar, â€œSpectrum breathing: Protecting
over-the-air federated learning against interference,â€ IEEE Trans. Wire-
less Commun., vol. 23, no. 8, pp. 10 058â€“10 071, 2024.
[18] A. Elbakary, C. B. Issaid, T. ElBatt, K. Seddik, and M. Bennis, â€œMira:
A method of federated multi-task learning for large language models,â€
IEEE Networking Lett., pp. 1â€“1, 2025.
[19] W. Ni, H. Sun, H. Ao, and H. Tian, â€œFederated intelligence: When large
AI models meet federated fine-tuning and collaborative reasoning at the
network edge,â€ IEEE Internet Things Mag., pp. 1â€“8, 2025.
[20] J. Zhang, S. Vahidian, M. Kuo, C. Li, R. Zhang, T. Yu, G. Wang,
and Y. Chen, â€œTowards building the federatedgpt: Federated instruction
tuning,â€ in Proc. IEEE Int. Conf. Acoust., Speech Signal Process.
(ICASSP), 2024, pp. 6915â€“6919.
[21] R. Ye, W. Wang, J. Chai, D. Li, Z. Li, Y. Xu, Y. Du, Y. Wang, and
S. Chen, â€œOpenfedllm: Training large language models on decentralized
private data via federated learning,â€ in Proc. ACM SIGKDD Conf.
Knowl. Discov. Data Min., 2024, pp. 6137â€“6147.
[22] J. Liu, Y. Liao, H. Xu, and Y. Xu, â€œResource-efficient federated fine-
tuning large language models for heterogeneous data,â€ arXiv preprint
arXiv:2503.21213, 2025.
[23] Y. Sun, Z. Li, Y. Li, and B. Ding, â€œImproving LoRA in privacy-
preserving federated learning,â€ in Proc. Int. Conf. Learn. Represent.
(ICLR), 2024.
[24] S. Babakniya, A. Elkordy, Y. Ezzeldin, Q. Liu, K.-B. Song, M. EL-
Khamy, and S. Avestimehr, â€œSLoRA: Federated parameter efficient fine-
tuning of language models,â€ in Proc. NeurIPS Workshop, 2023.
[25] Z. Wang, Y. Zhou, Y. Shi, and K. B. Letaief, â€œFederated fine-tuning
for pre-trained foundation models over wireless networks,â€ IEEE Trans.
Wireless Commun., vol. 24, no. 4, pp. 3450â€“3464, 2025.
[26] Q. Zhang, M. Chen, A. Bukharin, P. He, Y. Cheng, W. Chen, and
T. Zhao, â€œAdaptive budget allocation for parameter-efficient fine-tuning,â€
in Proc. Int. Conf. Learn. Represent. (ICLR), 2023.
[27] D. Wen, K.-J. Jeon, and K. Huang, â€œFederated dropoutâ€”a simple ap-
proach for enabling federated learning on resource constrained devices,â€
IEEE Wireless Commun. Lett., vol. 11, no. 5, pp. 923â€“927, 2022.
[28] S. Xie, D. Wen, X. Liu, C. You, T. Ratnarajah, and K. Huang, â€œFederated
dropout: Convergence analysis and resource allocation,â€ arXiv preprint
arXiv:2501.00379, 2024.
[29] Y. Lin, X. Ma, X. Chu, Y. Jin, Z. Yang, Y. Wang, and H. Mei, â€œLoRA
dropout as a sparsity regularizer for overfitting control,â€ arXiv preprint
arXiv:2404.09610, 2024.
[30] M. Zhu, A. Mao, J. Liu, and Y. Yuan, â€œDeer: Deviation eliminating and
noise regulating for privacy-preserving federated low-rank adaptation,â€
IEEE Trans. Med. Imaging, vol. 44, no. 4, pp. 1783â€“1795, 2025.
[31] Y. J. Cho, L. Liu, Z. Xu, A. Fahrezi, M. Barnes, and G. Joshi,
â€œHeterogeneous LoRA for federated fine-tuning of on-device foundation
models,â€ in Proc. NeurIPS Workshop, 2023.
[32] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-
dinov, â€œDropout: a simple way to prevent neural networks from overfit-
ting,â€ J. Mach. Learn. Res., vol. 15, no. 1, pp. 1929â€“1958, 2014.
[33] Z. Charles and D. Papailiopoulos, â€œStability and generalization of
learning algorithms that converge to global optima,â€ in Proc. Int. Conf.
Mach. Learn. (ICLR), 2018, pp. 745â€“754.
[34] O. Bousquet and A. Elisseeff, â€œStability and generalization,â€ J. Mach.
Learn. Res, vol. 2, pp. 499â€“526, 2002.
[35] Z. Fu, H. Yang, A. M.-C. So, W. Lam, L. Bing, and N. Collier, â€œOn
the effectiveness of parameter-efficient fine-tuning,â€ in Proc. AAAI Conf.
Artif. Intell. (AAAI), vol. 37, no. 11, 2023, pp. 12 799â€“12 807.
[36] Y. Yang, D. P. Wipf et al., â€œTransformers from an optimization perspec-
tive,â€ in Proc. Conf. Neural Inf. Process. Syst. (NeurIPS), vol. 35, 2022,
pp. 36 958â€“36 971.
[37] H. Sun, H. Tian, W. Ni, J. Zheng, D. Niyato, and P. Zhang, â€œFed-
erated low-rank adaptation for large models fine-tuning over wireless
networks,â€ IEEE Trans. Wireless Commun., vol. 24, no. 1, pp. 659â€“675,
2025.
[38] D. Wen, K.-J. Jeon, M. Bennis, and K. Huang, â€œAdaptive subcarrier,
parameter, and power allocation for partitioned edge learning over
broadband channels,â€ IEEE Trans. Wireless Commun., vol. 20, no. 12,
pp. 8348â€“8361, 2021.
[39] C. You, K. Huang, H. Chae, and B.-H. Kim, â€œEnergy-efficient resource
allocation for mobile-edge computation offloading,â€ IEEE Trans. Wire-
less Commun., vol. 16, no. 3, pp. 1397â€“1411, 2017.
[40] D. Li, X. Sun et al., Nonlinear integer programming.
Springer, 2006,
vol. 84.
[41] J. Liu, K. Xiong, D. W. K. Ng, P. Fan, Z. Zhong, and K. B. Letaief,
â€œMax-min energy balance in wireless-powered hierarchical fog-cloud
computing networks,â€ IEEE Trans. Wireless Commun., vol. 19, no. 11,
pp. 7064â€“7080, 2020.
[42] R. Singhal, K. Ponkshe, and P. Vepakomma, â€œExact aggregation for
federated and efficient fine-tuning of foundation models,â€ arXiv preprint
arXiv:2410.09432, 2024.
[43] F. Lai, Y. Dai, S. Singapuram, J. Liu, X. Zhu, H. Madhyastha, and
M. Chowdhury, â€œFedscale: Benchmarking model and system perfor-
mance of federated learning at scale,â€ in Proc. Int. Conf. Mach. Learn.
(ICML), 2022, pp. 11 814â€“11 827.
