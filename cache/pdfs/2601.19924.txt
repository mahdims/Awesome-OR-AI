--- Page 1 ---
OPT-Engine: Benchmarking the Limits of LLMs in
Optimization Modeling via Complexity Scaling
Yitian Chen1 ,
Cheng Cheng2,1,
Yinan Sun2,1,
Zi Ling3∗,
Dongdong Ge4
1 Cardinal Operations, Shanghai, China
2 Shanghai University of Finance and Economics
3 Booth School of Business, University of Chicago
4 Antai School of Economics and Management, Shanghai Jiao Tong University
chenyitian@shanshu.ai,
chengcheng@stu.sufe.edu.cn,
yinansun@stu.sufe.edu.cn, zling@chicagobooth.edu,
ddge@sjtu.edu.cn,
Abstract
Large Language Models (LLMs) have demonstrated impressive progress in optimization
modeling, fostering a rapid expansion of new methodologies and evaluation benchmarks.
However, the boundaries of their capabilities in automated formulation and problem solving
remain poorly understood, particularly when extending to complex, real-world tasks. To
bridge this gap, we propose OPT-ENGINE, an extensible benchmark framework designed to
evaluate LLMs on optimization modeling with controllable and scalable difficulty levels. OPT-
ENGINE spans 10 canonical tasks across operations research, with five Linear Programming
and five Mixed-Integer Programming. Utilizing OPT-ENGINE, we conduct an extensive
study of LLMs’ reasoning capabilities, addressing two critical questions: 1.) Do LLMs’
performance remain robust when generalizing to out-of-distribution optimization tasks
that scale in complexity beyond current benchmark levels? and 2.) At what stage, from
problem interpretation to solution generation, do current LLMs encounter the most significant
bottlenecks? Our empirical results yield two key insights: first, tool-integrated reasoning
with external solvers exhibits significantly higher robustness as task complexity escalates,
while pure-text reasoning reaches a ceiling; second, the automated formulation of constraints
constitutes the primary performance bottleneck. These findings provide actionable guidance
for developing next-generation LLMs for advanced optimization.
Our code is publicly
available at https://github.com/Cardinal-Operations/OPTEngine.
Keywords Artificial Intelligence, Optimization, Operations Research
1
Introduction
Recent advances in frontier large language models (LLMs) [1, 2, 3, 4, 5] have shown promising progress on
optimization problem modeling and solving. By automatically interpreting natural language descriptions into
precise mathematical models, and subsequently generating feasible solutions, current state-of-the-art LLMs
significantly lower the barrier to entry for complex optimization techniques, facilitating their accessibility to
a broader range of non-experts and accelerating the field’s democratization [6, 7].
In parallel, specialized benchmarks [8, 9, 10, 11] have emerged to systematically assess LLMs’ performances
in optimization modeling. Such benchmarks supply curated problem instances alongside canonical solutions,
including variable assignments and objective values derived from expert-collection [9] or semi-synthetic
pipeline [8]. While these benchmarks establish a gold standard for aggregate accuracy, they remain largely
confined to static evaluation on fixed-scale instances. Moreover, many remain disconnected from real-world
complexity and are dominated by college-level, textbook-style problems. For instance, network-flow tasks in
∗Corresponding authors.
arXiv:2601.19924v1  [cs.CL]  9 Jan 2026


--- Page 2 ---
the challenging OptMATH benchmark typically feature instances with around five nodes, which is insufficient
to stress high-dimensional constraints and combinatorial challenges inherent in real-world applications. In
contrast, the symbolic planning domain has pioneered extensible frameworks like PlanBench [12], which
establish a paradigm for scaling problem complexity via procedural world-state transitions. Such extensibility
is crucial for evaluation LLMs’ capability across optimization modeling domain. For example, to answer
whether LLM performance remains robust when generalizing to out-of-distribution complexity levels that far
exceed the dimensionality of current benchmarks?
The need for this shift is further underscored by a fundamental methodological bifurcation in how LLMs
approach optimization tasks. The classical tool-integrated reasoning paradigm focuses on automatic formula-
tion: LLMs first translate natural language problem statements into precise mathematical models (variables,
objectives, constraints) and then interface with external solvers like Gurobi [13] or COPT [14] to enhance
their computation capacity. This line of work includes multi-agent frameworks [15, 16] and tool-specialized
fine-tuning approaches [9, 17]. Conversely, the recent success of reasoning models, such as OpenAI’s o1 [18]
and DeepSeek-R1 [5], has revitalized purely text-based paradigms. By utilizing extended Chain-of-Thought
(CoT) processes [19], these models attempt to solve optimization problems through end-to-end sequential
deduction. While both reports promising results on existing benchmarks [10, 11], (a detailed comparison of
which can be viewed in Appendix C.2.2 Table 4), this divergence raises critical questions: Which paradigm is
more effective and robust for real-world Operations Research applications? What are the inherent mechanistic
differences between these two approaches? A clear, mechanistic understanding of their differences is essential
to advance the field beyond isolated demonstrations.
Furthermore, a stark performance gap persists. On one hand, LLMs have achieved super-human proficiency
in competitive mathematics, with even small-scale 4B models [20] surpassing the 95% threshold on the MATH
benchmark, which suites of college-level competition math problems; On the other hand, their application to
optimization remains brittle, achieving less than 50% accuracy on the most challenging benchmark [21, 17].
The IndustryOR benchmark [10] exemplifies this pattern, it curates realistic tasks with heterogeneous
constraints, and reported accuracy remains substantially low even though their resulting mathematical
formulations are relatively straightforward once auto-formulated correctly. This discrepancy poses another
critical question for the community: why does such a stark performance gap persist between a model’s
deductive reasoning capabilities for mathematical reasoning tasks and its capacity for robust optimization
modeling?
To address these challenges, We present OPT-ENGINE, an extensible benchmark framework designed to
move beyond static and anecdotal evaluations and to provide a rigorous assessment of LLMs’ capacity for
auto-formulation, reasoning and solving. OPT-ENGINE programmatically generates a vast repository of
optimization instances with controllable structure and difficulty, flexible natural language specifications, and
verifiable optimal solutions. This design enables a new paradigm of evaluation, facilitating the first scalable
and granular analysis of LLMs across the complexity landscape of optimization tasks, and providing the
empirical evidences necessary to move the field toward autonomous, industrial-scale optimization modeling.
In summary, our work provides the following three key contributions: 1.) We introduce OPT-Engine,
a novel framework that enables extensible benchmarking of LLMs on optimization auto-formulation and
problem-solving with fine grained, scalable control of mathematical structure and language. 2.) We conduct
a systematic study of LLMs’ reasoning paradigms. Our analysis characterizes their relative efficacy and
demonstrates that external tool integration is crucial for maintaining accuracy as problem complexity scales.
3.) Through controllable experiments with OPT-Engine, we quantify the primary performance bottlenecks of
current frontier LLMs in optimization modeling. Our results indicate that errors in grounding and formulating
constraints during formulation are the critical challenge compared with problem comprehension or modest
objective perturbations.
2
Related work
Benchmarks for Optimization Modeling.
The development of LLMs for optimization modeling has
been accelerated by the creation of specialized benchmarks. This line of work was notably advanced by the
LP word problem dataset and shared optimization tasks for the NeurIPS 2022 competition [22]. Subsequent
benchmarks like MAMO [23], IndustryOR [9], OptMATH [8], and OPTIBENCH [24] have expanded the scope
from linear to nonlinear and mixed-integer programming. The second stream, including ALE-Bench [25] and
NP-engine [26], focuses on heuristic solutions to combinatorial problems but does not provide exact solutions
for evaluations. Furthermore, while frameworks like OptMATH utilize programmatic pipelines for instance
2


--- Page 3 ---
Figure 1: An overview of the OPT-Engine taxonomy. The framework encompasses five Mixed-Integer
Programming (MIP) problem classes: traveling salesman problem, bin packing [roblem, job-shop Scheduling
minimum cost netflow problem, and knapsack problem and five linear programming (LP) problem classes:
inventory problem, portfolio allocation problem, production problem, transportation problem and pollution
control problem, covering prevalent optimization modeling scenarios in real-world Operations Research
applications.
generation, they prioritize training data synthesis over the high-fidelity solution accuracy necessary for robust
benchmarking.
Paradigms in LLM-driven Optimization.
Current research on LLMs for optimization broadly falls
into two categories. The first is tool-integrated reasoning (TIR), which follows an “Autoformulation-then-
Solve” [9, 17, 27] paradigm, utilizing LLMs as formulation and code generators to interface with external
optimization solvers such as Gurobi or COPT. These tool-augmented methods leverage code data to enhance
reasoning, effectively offloading heavy computation to external solvers. In contrast, pure-text reasoning
(PTR) approaches [28], exemplified by recent advanced reasoning models [18, 5], seek to solve optimization
problems within the textual domain. Such methods mainly rely on the model’s latent ability to navigate
complex search spaces through prompt-based method [28, 29] or supervised training [11]. Despite the promise
of these dual trajectories, a rigorous empirical analysis is still required to characterize their respective scaling
laws and failure modes.
Benchmarks with Scalable Complexity and Structural Perturbations.
Prior work in symbolic
logic and planning, such as PlanBench [12] and ACPBench [30], established the paradigm of scaling problem
complexity through world-state transitions. Based on these benchmarks, a series of systematic evaluations of
frontier LLMs have been conducted [31, 32, 33]. These studies reveal clear strengths and limitations in LLMs’
reasoning, particularly as problem complexity increases, and demonstrate phenomena such as “reasoning
illusions” where model performance degrades significantly as scale grows [34]. In the context of mathematical
reasoning, prior work [35, 36] has shown that changes to problem descriptions alone can induce distribution
shifts and subsequent accuracy drops. Further, [37] utilized perturbed mathematical problems, altering both
numeric values and logical structures, to evaluate LLM performance under dynamic conditions. Our work
extends these evaluative criteria to the optimization domain by introducing a novel decomposition of problem
complexity: linguistic variance, objective perturbations, and constraint disturbances.
3
OPT-Engine: Taxonomy and Pipeline Framework
3.1
OPT-Engine: Benchmark Taxonomy
As shown in fig. 1, the OPT-Engine framework encompasses ten canonical problem classes that cover the
breadth of real-world optimization categories essential to Operations Research (OR) applications. Specifically,
the framework is structured into two primary categories: 1.) Linear programming (LP). This family
comprises five optimization problem classes, including the inventory problem, the portfolio allocation problem,
the production problem, the transportation problem, and the pollution control problem. These classes
3


--- Page 4 ---
Figure 2: Overview of the problem instance generation workflow. The pipeline comprises four stages:
(1) Numeric Instance Generation, (2) Original Problem Construction, (3) Problem Augmentation, and (4)
Instance Validation. This end-to-end process yields comprehensive problem instances, including their specific
type, complexity metrics, natural language statements, and ground-truth verifiable solutions.
are especially chosen to evaluate the LLMs’ capacity to formulate and solve problems under continuous
resource and uncertainty constraints. 2.) Mixed-integer programming (MIP). This category includes five
combinatorial optimization classes with integer or binary constraints: the traveling salesman problem (TSP),
the knapsack problem, the bin packing problem, the job-shop scheduling problem and the minimum-cost
network flow problem. These problems are critical to investigate the LLMs’ proficiency in formulating and
solving the intricate logical, sequencing, and precedence constraints required for combinatorial decision-making.
Crucially, the framework is designed to generate problem instances with controllable and scalable complexity.
For each problem class, the difficulty of a generated problem instance is modulated by tuning specific structural
parameters. For example, TSP difficulty can scale with the number of cities Ncities, while portfolio allocation
problems can scale with the number of assets Nassets and corresponding constraint sets. This design enables
fine-grained and systematic evaluation across difficulty levels, thereby facilitating rigorous tests of LLMs’
ability to generalize across the full range of problem scale and constraint density.
We next detail the pipeline implementation, illustrating how OPT-Engine synergistically combines program-
matic instance generators, natural language problem construction, targeted augmentations, and solver-based
validation to achieve controlled generation and verified solutions.
3.2
OPT-Engine: Pipeline Framework
Given an optimization problem class d ∈D, where D = {d1, d2, . . . , dn} denotes the ten problem classes
introduced above, we then describe the pipeline that produces fully specified, verifiable instances from each
class.
As shown in Figure 2, the overall pipeline, which generates the final problem instance (including its type,
complexity, problem description, and verifiable solution), can be divided into four main stages: 1.) the
numeric instance generation stage, 2.) the original problem construction stage, 3.) the problem
augmentation stage, and 4.) the instance validation stage.
In the first stage, the numeric instance generator G takes the problem class d and a difficulty control parameter
vector θ as the input, and generates the core numeric data that represents a specific numeric instance. This
process can be formally viewed as a function G that maps the problem class D and control space Θ to the
numeric instance space I:
G : D × Θ →I.
Here, Θ ⊆Rk represents the k-dimensional space of controllable difficulty parameters (e.g., the number of
cities for TSP). Crucially, G is coupled with a solver template specific to class d. This code template is
essential for generating the exact optimal objective value and decision variables that serve as the ground
truth for subsequent validation stages. For example, a size parameter n induces a controllable n × n distance
matrix for TSP, after which the solver computes the optimal solution. Meanwhile, G is equipped with a
4


--- Page 5 ---
self-correction mechanism: if a draw is infeasible, the generator resamples until a valid numeric instance is
produced.
Once a valid numeric instance i ∈I is available, the pipeline proceeds to the “canonical problem construction”
stage. In this stage, we utilize a structured problem description template T to map the instance i into the
canonical problem statement sc ∈SC. This generated statement serves as the essential, formal original
optimization problem used for subsequent rephrasing and validation. This mapping M is defined as:
M : I × T →SC.
In the subsequent “problem augmentation” (or rephrasing) stage R, a dedicated LLM agent L, is employed to
diversify the problem descriptions. Specifically, L ingests the canonical problem statement sc and leverages
the LLM’s generative capability to produce a set of context-rich, domain-specific narratives sr ∈SR, which
render the original problem into multiple simulated Operations Research scenarios. This transformation is
defined as:
R : SC × L →SR.
The final phase of the pipeline is “problem instance validation”. This is carried out by a validation module
(V) that combines an LLM as a Judge [38] with a rule-based verifier. By simultaneously checking for
numerical correctness and structural preservation, this module guarantees the consistency of the constraints
and objectives across the original canonical problem sc and the generated rephrased statements sr.
If an augmented problem fails validation, the system repeats R until a valid problem instance is obtained.
This recursive loop ensures that the final output, comprising the problem type, complexity metrics, rephrased
narrative, and verifiable solution, maintains an analytically optimal value consistent with the ground truth
derived from the numeric generator G.
The full implementation details, including detailed class definitions with corresponding templates and required
prompt templates during the pipeline, are provided in Appendix B. With the pipeline established, we can
now analyze model behavior across complexity scales, reasoning paradigms and diversified descriptions in the
next sections.
4
Tool-Integrated vs. Pure-Text Reasoning
Leveraging the OPT-Engine framework, we first systematically compare two mainstream approaches for
solving optimization problems: Tool-Integrated Reasoning (TIR), which enhances LLMs’ capacity with
external tools, and Pure-Text Reasoning (PTR), a paradigm that treats LLMs as end-to-end optimizers using
purely textual reasoning.
4.1
Definitions and Experimental Setup.
We begin by formally defining these two approaches. Let X = {xi}N
i=1 denote the evaluation set of N
optimization problem instances generated by OPT-Engine. For each problem xi ∈X, we prompt the LLM to
generate a sequence of intermediate reasoning steps z(i) = (z(i)
1 , z(i)
2 , . . . , z(i)
m ) with terminal step z(i)
m . We
then run two approaches p ∈{PTR, TIR} that differ in how this terminal step is executed and how the
objective value ˆy(p)
i
is obtained. In PTR, the LLM is prompted to reason sequentially to derive the optimal
objective value ˆy(PTR)
i
, which is contained within the final reasoning step z(i)
m . In TIR, the terminal step z(i)
m
contains an executable code snippet, which we run in an external execution solver to obtain ˆy(TIR)
i
. The
specific prompt templates for both approaches are provided in the Appendix C.1.1.
To evaluate robustness, we generate ten distinct instances per problem class, varying difficulty by scaling
problem dimensionality. We report the avg-pass@1 metric, representing the mean accuracy across these
instances. An output is counted as correct when the relative error with respect to the ground-truth optimum
y∗
i satisfies:
ˆy(p)
i
−y∗
i

|y∗
i | + ϵ
< 10−3,
where ϵ is a small constant (e.g., 10−6) to ensure numerical stability.
5


--- Page 6 ---
Figure 3: Performance comparison between Tool-Integrated Reasoning (TIR) and Pure-Text Reasoning
(PTR) as problem size scales. The upper panel reports results for the DeepSeek-V3.2 model, and the lower
panel reports results for the GPT-5.1 model.
4.2
Comparative Analysis: TIR vs. PTR
Comparative Analysis with Top-Tier Models.
In the first phase of our comparative study, we utilized
two proprietary API-Accessed LLMs: DeepSeek-V3.2 [39] and GPT-5.1 [40]. By leveraging these top-tier
models, we aim to rigorously evaluate the performance of these two approaches, thereby establishing a strong
baseline that reflects these approaches’ intrinsic capabilities.
As illustrated in Figure 3, performance trend are consistent across all problem classes, including both MIP
and LP. As problem complexity increases, TIR sustains high accuracy or exhibits only minor degradation. In
contrast, PTR degrades substantially with scale.
Comparative Analysis with Weaker Models.
We then extended our analysis to a smaller-scale,
open-source model Qwen3-4B-Instruct [20]. This model was selected for its strong instruction-following
capability, a key criterion for evaluating these two reasoning approaches for small-scale models.
As illustrated in the top row of Figure 4, PTR consistently outperforms TIR at low-complexity settings. This
observation contrasts with the scaling trends in our analysis of frontier models, such as GPT-5.1 (Figure 3).
Nevertheless, this advantage is transient: the efficacy of PTR significantly degrades as the problem scale
expands.
We attribute this performance gap to limited domain-specific post-training and weaker code generation in
smaller open-source models. Without these specialized capabilities, models struggle to maintain the syntactic
and logical rigor required to interface with external optimization solvers. This limitation underscores the
necessity of targeted fine-tuning. As shown in Figure 4, Qwen3-4b-RL, initialized from Qwen3-4B-Instruct and
optimized via reinforcement learning from verifiable rewards (RLVR) [17], (the training and inference settings
of Qwen3-4b-RL are detailed in Appendix C.1.2), consistently outperforms the standard PTR approach
across all problem dimensions. Complete per-class results for all ten problem classes across both model tiers
are provided in Appendix C.2.1.
As evidenced by the execution metrics in Table 1, the transition from Qwen3-4b-Instruct to Qwen3-4b-RL
results in a marked increase in execution rates. We also evaluate the tool-use proficiency of frontier models,
including DeepSeek-V3.2 and GPT-5.1. These results highlight that generating syntactically and logically
sound code is a prerequisite for the TIR approach to successfully leverage external solvers.
Comparative Analysis: A Synthesis of Critical Findings.
Our experimental results support two
conclusions. First, when reasoning in PTR approach isolated from external optimization solvers, LLMs
reach a severe performance ceiling as complexity increases. This decline reflects a core mismatch between
LLMs’ intrinsic capabilities of textual reasoning and the exact algorithmic computations required in formal
optimization (e.g., the O(n3) operations of the Simplex method for Linear programming problems [7]).
6


--- Page 7 ---
Figure 4: Performance scaling of PTR (blue) vs. TIR (red) on the Qwen3-4B series. The upper
panel illustrates the reasoning performance of the base Qwen3-4B-Instruct model as problem complexity
increases. The lower panel incorporates results from Qwen3-4B-RL, indicating significantly improved accuracy
due to RLVR training in TIR modes.
Table 1: Execution Rate Comparison Across Models
Model
TSP
Knapsack
Inventory
Deepseek-V3.2
86.4%
100.0%
88.8%
GPT-5.1
83.1%
99.6%
92.5%
Qwen3-4B-Instruct
23.1%
23.1%
35.6%
Qwen3-4B-RL
38.1%
99.6%
38.1%
∆
+15.0%
+76.5%
+2.5%
Second, by generating executable solver code and invoking tool-calling interfaces, LLMs consistently out-
perform their tool-free counterparts. This delegation effectively offloads the exact computational burden to
deterministic engines, maintaining robustness where PTR fails. Consequently, our findings establish TIR as
the requisite framework for addressing high-dimensional, industrial-scale optimization challenges.
4.3
In-Depth Analysis of the PTR approach
Figure 5: TSP results with DeepSeek-V3.2: relationship between token length and accuracy across instance
sizes.
7


--- Page 8 ---
Figure 6: The first row is the accuracy across different perplexities. The second row is the accuracy across
different objective functions.
We have shown that PTR performance degrades as problem size increases under the defined strict evaluation
criterion, a finding that contrasts with reported successes of pure-text reasoning in prior work [11, 29]. To
gain deeper insight into the behavior of PTR, we examine token-level response characteristics and conduct
case studies of full reasoning traces.
Figure 5 shows the relationship between average token length and accuracy for the TSP problem under
DeepSeek-V3.2. At small scales, the model increases its apparent reasoning effort, measured by output
tokens, roughly in proportion to problem complexity. However, as the instance size approaches a critical
range, closely preceding the point of accuracy collapse, the model counterintuitively reduces its token budget
despite increasing task difficulty. Moreover, we provide a case study in Figure 17 which clarifies the strategies
behind this shift in Appendix C.2.1. In the initial phase with small problem sizes, the model predominantly
relies on explicit enumeration of Hamiltonian cycles to solve the problem exactly, because the search space
remains tractable (size (n −1)!). This yields high accuracy with growing token consumption in this phase.
However, beyond the critical threshold, the model shifts to employing lightweight heuristics such as nearest
neighbor and cheapest insertion to obtain approximate solutions, which holds token usage approximately flat
but consequently lowers solution quality.
The resulting PTR behavior is adaptive in a way that resembles expert practice, opting for fast heuristics
when exact solutions become computationally prohibitive. Furthermore, this dynamic provides a mechanistic
explanation for the efficacy of PTR-based end-to-end optimizers reported in prior studies [11, 29]. Their
efficacy in combinatorial optimization stems from an inherent ability to dynamically shift to heuristic methods
and evaluation metrics emphasizing near-optimal, practical solutions over strict optimality.
5
The Primary Bottleneck: Diagnosis and Evidence
In this section, we adopt a systematic diagnostic approach to further identify and analyze the primary
bottlenecks of current LLMs in optimization modeling using OPT-Engine. Inspired by recent diagnostic
frameworks in mathematical reasoning [35, 36, 37], we investigate how three critical dimensions affect solution
accuracy: 1.) linguistic complexity in problem descriptions, 2.) perturbations to objective functions, and 3.)
augmentation of constraint conditions. The first dimension probes the textual surface of the task, whereas
the latter two are intrinsically tied to the optimization problem’s mathematical structure that drives solving.
5.1
Linguistic Complexity and Objective Perturbations
Linguistic Complexity.
We test the causal role of linguistic complexity by constructing a controlled
template experiment. Starting from a base description, we generate two additional templates of systematically
higher complexity. Critically, all three templates describe the same numeric instance and constraint set,
isolating linguistic variation as the sole independent factor. To quantify this variation rigorously, we adopt
8


--- Page 9 ---
TSP
Augmented constraint description:
There is no direct road between city 0
and city 1. Exactly one of the following
two roads must be included in the tour:
the road between city 1 and city 2, the
road between city 2 and city 3.
Augmented constraint formulation:
x01 = 0
x12 + x23 = 1
Knapsack
Augmented constraint description:
Exactly one of item 1 and item 2 must
be selected. If item 3 is selected, the
effective backpack capacity is reduced by
2 kg.
Augmented constraint formulation:
x1 + x2 = 1
n
X
i=1
wixi ≤C −2x3
Inventory
Augmented constraint description:
On day t1, the maximum order quantity
is Qcap units. On day t2, the on-hand
inventory must be at least Imin units.
Augmented constraint formulation:
ot1 ≤Qcap
It2 ≥Imin
Figure 7: Augmented constraint descriptions and their corresponding mathematical formulations across
problem types.
perplexity [41] (PPL, see Appendix A.2 for details) as our primary complexity metric. Figure 9 illustrates
this template variation for the TSP problem.
The results in the top row of Figure 6 show that solution accuracy remains stable as PPL increases when the
underlying mathematical structure is held fixed 2.
Objective Perturbations. We next investigate whether perturbations to the objective function disrupt LLM-
based optimization modeling accuracy. Intuitively, modifying the objective function shifts the optimization
criterion and could potentially confuse models that rely on familiar signatures rather than explicitly reasoning
about objectives.
Formally, for an original objective f(x), we apply a linear perturbation by adding a constant:
min
x f(x) =⇒min
x f(x) + K,
where K is a randomly sampled constant that does not depend on the decision variables. This modification
preserves optimal solutions and the relative ordering of feasible solutions, while only shifting the objective
value.
As demonstrated in the bottom row of Figure 6, this perturbation has a negligible impact on accuracy across
all problem classes, which indicates that such objective shifts also do not function as a key bottleneck for
optimization modeling.
(a)
(b)
(c)
Figure 8: Accuracy with and without Extra Constraint
5.2
Constraint Augmentation
To evaluate the impact of augmented constraint descriptions, we construct variants for each problem class
by introducing a set of mathematically straightforward constraints. This setup reflects the multi-constraint
nature of real-world optimization while maintaining a controlled test environment. By construction, each
addition preserves the formal problem class and instance size: no new variables are introduced and only O(1)
2For subsequent experiments, we fix linguistic complexity by using the Easy template unless otherwise specified.
9


--- Page 10 ---
constraints are added, so the intrinsic mathematical difficulty and asymptotic complexity remain unchanged
even if the optimal solution may differ. The modeling burden therefore increases semantically rather than
computationally, because the model must correctly parse and integrate auxiliary conditions into the formal
formulation.
Figure 7 makes this concrete and illustrates representative augmentations and their exact formulations. For
TSP, we forbid one edge and require exactly one of two edges; for knapsack, we impose mutual exclusivity
and a simple capacity shift; and for inventory, we add a per-period order cap and a minimum stock level.
Figure 8 preports results for both PTR and TIR. A consistent trend emerges: solution accuracy degrades
significantly when augmented constraints are added compared with the canonical problems. This is counter-
intuitive because the math is not harder, and inspection of failure traces shows formulation errors such as
omitted constraints, which then propagate to solvers in TIR and yield incorrect objectives in PTR.
5.3
The Auto-Formulation Bottleneck: Initial Conclusions
Our results show that LLM-based solution accuracy remains largely robust to both increased linguistic
complexity and objective-function perturbations when the underlying problem structure is preserved. In
contrast, adding even a single simple constraint leads to a substantial accuracy drop across problem classes
under both TIR and PTR paradigms, highlighting augmented constraints as a primary bottleneck for
LLM-based optimization modeling.
This counterintuitive pattern indicates that current LLMs often do not fully internalize or reason robustly
about problem constraints, but instead rely on patterns tied to familiar, canonical formulations that are likely
to be abundant in their training data. The canonical versions of TSP, Knapsack, and related problems closely
resemble textbook examples, whereas the constrained variants introduce novel combinations of conditions
that the model is less likely to have studied. In practice, however, real-world optimization tasks rarely appear
as clean canonical problems and typically induce additional constraints tailored to complex operational or
regulatory requirements. Our findings, therefore, indicate that OPT-Engine provides a useful framework
for probing whether LLMs can move beyond solving stylized textbook instances and handle the richer,
constraint-heavy optimization problems that arise in real applications.
6
Conclusion
We present OPT-Engine, an extensible benchmark framework that bridges LLM linguistic reasoning with
formal mathematical optimization. Beyond a dataset, OPT-Engine defines a standardized, solver-verified
protocol with breadth across ten Operations Research classes, controllable structural scaling, and targeted
language variation. This benchmark enables comparisons of systematic scaling sweeps and problem description
edit diagnostics to localize failure modes. Our evaluation establishes TIR as the more reliable approach at
higher scales, for solving real-world optimization problems, showing that decoupling linguistic reasoning from
exact computation improves reliability when models can produce correct, executable code. We further expose
a critical “semantic sensitivity” bottleneck: even frontier LLMs struggle to maintain formulation integrity
when problem specifications deviate from canonical version. OPT-Engine thus provides the diagnostic
rigor and roadmap required to develop next-generation LLMs capable of addressing real-world optimization
challenges, with evaluations that quantify robustness across structure, language variation, and tool use.
7
Limitations
The conclusions of this study are derived from an exact optimal solution perspective. We prioritize the
accuracy of automated formulation for MIP and LP problem classes, but “exact modeling” is not always the
best choice in real-world optimization applications, such as large NP-hard problems with prohibitive scale
or other complicated scenarios, where high-performance heuristics are typically favored to balance solution
quality with execution time. Because our evaluation emphasizes optimality-checked objectives and verified
formulations, it may understate performance when near-optimal solutions are acceptable. Future tracks can
report optimality gaps, runtime, and robustness under time budgets. Evaluating LLMs’ ability to design or
select effective heuristics involves different training signals and evaluation criteria and is a natural extension
of the OPT Engine framework. In addition, our current benchmark focuses on linear structure in LP and
MIP and does not yet cover nonlinear, stochastic, or dynamic programs, which remain important extensions
that our framework is designed to accommodate.
10


--- Page 11 ---
References
[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774, 2023.
[2] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan
Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly capable
multimodal models. arXiv preprint arXiv:2312.11805, 2023.
[3] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer,
Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding
across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.
[4] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi
Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437,
2024.
[5] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong
Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement
learning. arXiv preprint arXiv:2501.12948, 2025.
[6] Andreas Antoniou and Wu-Sheng Lu. Practical optimization: algorithms and engineering applications.
Springer, 2007.
[7] David G Luenberger, Yinyu Ye, et al. Linear and nonlinear programming, volume 2. Springer, 1984.
[8] Hongliang Lu, Zhonglin Xie, Yaoyu Wu, Can Ren, Yuxuan Chen, and Zaiwen Wen. Optmath: A scalable
bidirectional data synthesis framework for optimization modeling. arXiv preprint arXiv:2502.11102,
2025.
[9] Chenyu Huang, Zhengyang Tang, Shixi Hu, Ruoqing Jiang, Xin Zheng, Dongdong Ge, Benyou Wang,
and Zizhuo Wang. Orlm: A customizable framework in training large models for automated optimization
modeling. Operations Research, 2025.
[10] Zhicheng Yang, Yinya Huang, Wei Shi, Liang Feng, Linqi Song, Yiwei Wang, Xiaodan Liang, and
Jing Tang. Benchmarking llms for optimization modeling and enhancing reasoning via reverse socratic
synthesis. arXiv e-prints, pages arXiv–2407, 2024.
[11] Xia Jiang, Yaoxin Wu, Minshuo Li, Zhiguang Cao, and Yingqian Zhang. Large language models as
end-to-end combinatorial optimization solvers. arXiv preprint arXiv:2509.16865, 2025.
[12] Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large language
models still can’t plan (a benchmark for llms on planning and reasoning about change). In NeurIPS
2022 Foundation Models for Decision Making Workshop, 2022.
[13] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2024.
[14] Dongdong Ge, Qi Huangfu, Zizhuo Wang, Jian Wu, and Yinyu Ye. Cardinal optimizer (copt) user guide.
arXiv preprint arXiv:2208.14314, 2022.
[15] Rindra Ramamonjison, Haley Li, Timothy Yu, Shiqi He, Vishnu Rengan, Amin Banitalebi-Dehkordi,
Zirui Zhou, and Yong Zhang. Augmenting operations research with auto-formulation of optimization
models from problem descriptions. In Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing: Industry Track, pages 29–62, 2022.
[16] Ali AhmadiTeshnizi, Wenzhi Gao, and Madeleine Udell. Optimus: Scalable optimization modeling with
(mi) lp solvers and large language models. arXiv preprint arXiv:2402.10172, 2024.
[17] Yitian Chen, Jingfan Xia, Siyu Shao, Dongdong Ge, and Yinyu Ye. Solver-informed rl: Grounding large
language models for authentic optimization modeling. arXiv preprint arXiv:2505.11792, 2025.
[18] OpenAI. Learning to reason with LLMs, September 2024. Accessed: 2026-01-07.
[19] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural
information processing systems, 35:24824–24837, 2022.
[20] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,
Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.
[21] Pei-Fu Guo, Ying-Hsuan Chen, Yun-Da Tsai, and Shou-De Lin. Towards optimizing with large language
models. arXiv preprint arXiv:2310.05204, 2023.
11


--- Page 12 ---
[22] Rindranirina Ramamonjison, Timothy Yu, Raymond Li, Haley Li, Giuseppe Carenini, Bissan Ghaddar,
Shiqi He, Mahdi Mostajabdaveh, Amin Banitalebi-Dehkordi, Zirui Zhou, et al. Nl4opt competition:
Formulating optimization problems based on their natural language descriptions. In NeurIPS 2022
competition track, pages 189–203. PMLR, 2023.
[23] Xuhan Huang, Qingning Shen, Yan Hu, Anningzhe Gao, and Benyou Wang. Mamo: a mathematical
modeling benchmark with solvers. arXiv e-prints, pages arXiv–2405, 2024.
[24] Zhicheng Yang, Yiwei Wang, Yinya Huang, Zhijiang Guo, Wei Shi, Xiongwei Han, Liang Feng, Linqi
Song, Xiaodan Liang, and Jing Tang. Optibench meets resocratic: Measure and improve llms for
optimization modeling. arXiv preprint arXiv:2407.09887, 2024.
[25] Yuki Imajuku, Kohki Horie, Yoichi Iwata, Kensho Aoki, Naohiro Takahashi, and Takuya Akiba. Ale-bench:
A benchmark for long-horizon objective-driven algorithm engineering. arXiv preprint arXiv:2506.09050,
2025.
[26] Xiaozhe Li, Xinyu Fang, Shengyuan Ding, Linyang Li, Haodong Duan, Qingwen Liu, and Kai Chen.
Np-engine: Empowering optimization reasoning in large language models with verifiable synthetic np
problems. arXiv preprint arXiv:2510.16476, 2025.
[27] Duc M Nguyen and Sungahn Ko. Technical report for icml 2024 automated math reasoning challenge:
Solving optimization problems with open source large language model. In AI for Math Workshop@
ICML 2024, 2024.
[28] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large
language models as optimizers. In The Twelfth International Conference on Learning Representations,
2023.
[29] Jianheng Tang, Qifan Zhang, Yuhan Li, Nuo Chen, and Jia Li. Grapharena: Evaluating and exploring
large language models on graph computation. arXiv preprint arXiv:2407.00379, 2024.
[30] Harsha Kokel, Michael Katz, Kavitha Srinivas, and Shirin Sohrabi. Acpbench: Reasoning about action,
change, and planning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39,
pages 26559–26568, 2025.
[31] Zhao Song, Song Yue, and Jiahao Zhang. Thinking isn’t an illusion: Overcoming the limitations of
reasoning models via tool augmentations. arXiv preprint arXiv:2507.17699, 2025.
[32] Karthik Valmeekam, Kaya Stechly, and Subbarao Kambhampati. Llms still can’t plan; can lrms? a
preliminary evaluation of openai’s o1 on planbench. arXiv preprint arXiv:2409.13373, 2024.
[33] Karthik Valmeekam, Kaya Stechly, Atharva Gundawar, and Subbarao Kambhampati. A systematic
evaluation of the planning and scheduling abilities of the reasoning model o1. Transactions on Machine
Learning Research, 2025.
[34] Parshin Shojaee, Iman Mirzadeh, Keivan Alizadeh, Maxwell Horton, Samy Bengio, and Mehrdad
Farajtabar. The illusion of thinking: Understanding the strengths and limitations of reasoning models
via the lens of problem complexity. arXiv preprint arXiv:2506.06941, 2025.
[35] Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Fara-
jtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language
models. arXiv preprint arXiv:2410.05229, 2024.
[36] Pengfei Hong, Deepanway Ghosal, Navonil Majumder, Somak Aditya, Rada Mihalcea, and Soujanya Poria.
Stuck in the quicksand of numeracy, far from agi summit: Evaluating llms’ mathematical competency
through ontology-guided perturbations. CoRR, 2024.
[37] Kaixuan Huang, Jiacheng Guo, Zihao Li, Xiang Ji, Jiawei Ge, Wenzhe Li, Yingqing Guo, Tianle Cai,
Hui Yuan, Runzhe Wang, et al. Math-perturb: Benchmarking llms’ math reasoning abilities against
hard perturbations. arXiv preprint arXiv:2502.06453, 2025.
[38] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena.
Advances in neural information processing systems, 36:46595–46623, 2023.
[39] Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei
Zhang, Chaofan Lin, Chen Dong, et al. Deepseek-v3. 2: Pushing the frontier of open large language
models. arXiv preprint arXiv:2512.02556, 2025.
12


--- Page 13 ---
[40] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon,
Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with
advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint
arXiv:2507.06261, 2025.
[41] Frederick Jelinek. Interpolated estimation of markov source parameters from sparse data. In Proc.
Workshop on Pattern Recognition in Practice, 1980, 1980.
[42] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin
Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256,
2024.
[43] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text
degeneration. arXiv preprint arXiv:1904.09751, 2019.
13


--- Page 14 ---
A
Technical Background
A.1
Auto-formulation of Optimization Problems
In this work, auto-formulation denotes the task of using an LLM-based agent to transform a human-readable
problem description into this formal representation and its executable counterpart. The formal target of
auto-formulation is the canonical optimization problem form [7]:
min
x
g(x),
subject to
ci(x) = 0,
i ∈E,
ci(x) ≥0,
i ∈I,
(1)
where x ∈Rn denotes the decision vector, and the objective function g : Rn →R assigns a scalar value to
each candidate solution x. The constraint functions ci : Rn →R define the feasible region through equality
constraints indexed by E and inequality constraints indexed by I.
This auto-formulation process involves three distinct representational components:
• Natural Language Problem.
A natural language problem is an unstructured textual description
of a decision-making scenario, often presented as a word problem or real-world query (e.g., How
should a factory allocate resources to maximize profit given limited labor and materials?). These
problems are intuitive for humans but ambiguous and non-executable for computers. It’s necessary
to translate into formal mathematical representations.
•
Mathematical Model.
This process yields a precise abstraction of the problem, necessitating
the explicit extraction and definition of decision variables, the objective function, and mathematical
constraints. The resulting model serves as a critical interpretable artifact, effectively bridging the
gap between natural language and computational execution
•
Solver Code.
The final executable output consists of code that instantiates the mathematical
model for a specific solver, such as Gurobi or COPT. This step translates the abstract formulation
into the syntactic format required by numerical optimization engines, effectively bridging the gap
between theoretical description and computational solution.
A.2
Perplexity: PPL
In NLP domain, PPL serves as an intrinsic measure of how well a probability distribution predicts a sample,
effectively capturing the "surprise" encountered by an LLM during parsing. For a given sequence X, the
formulation is given by:
PPL(X) = exp
(
−1
t
t
X
i
log pθ(xi|x<i)
)
(2)
where log pθ(xi|x<i) is the log-likelihood of the i-th token conditioned on the preceding tokens x<i according
to the model θ.
In the context of our benchmark, PPL provides a granular metric for problem statement complexity. A
low perplexity score implies that the optimization problem is described using standard, high-frequency
terminology and simple syntactic structures. Conversely, a high perplexity score identifies complex problem
descriptions—often characterized by specialized jargon, nested constraints, or unconventional phrasing—which
present a higher cognitive load for the LLM to parse and formulate.
A.2.1
Experimental Details of Bottleneck Diagnostics
We employed three templates exhibiting different levels of perplexity during the generation phase. These
templates were divided into three categories (easy, medium and high) and labeled with their distinct perplexity
scores. The details are presented below.
14


--- Page 15 ---
Easy (ppl=13.2)
Consider a delivery service that
needs to visit 4 cities. The
distances (km) are:
A–B: 184.2, A–C: 71.6, A–D: 94.6
B–C: 126.8, B–D: 94.5, C–D: 64.0
Find the shortest possible
route that visits each city once
and returns to the start.
Medium (ppl=18.2)
Consider a routing task where a
planner constructs a tour visiting 4
cities. Pairwise travel distances are
deterministic and symmetric:
A–B: 184.2, A–C: 71.6, A–D: 94.6
B–C: 126.8, B–D: 94.5, C–D: 64.0
Identify a minimum-length
cyclic route visiting each city
once and returning to departure.
Hard (ppl=22.6)
Consider a canonical TSP
instance involving 4 nodes in the
Euclidean plane. The symmetric
distance matrix (km) is encoded
as:
A–B: 184.2, A–C: 71.6, A–D: 94.6
B–C: 126.8, B–D: 94.5, C–D: 64.0
Compute a shortest
Hamiltonian tour, minimizing
aggregate travel distance over all
legs.
Figure 9: Comparison of Prompt Variation across Three Complexity Tiers. While the underlying TSP
problem structure remains invariant, the linguistic framing is scaled from simple delivery metaphors to formal
optimization terminology to evaluate model robustness to description perplexity.
B
Implementation Details of the OPT-Engine Framework
B.1
Rephrasing and Validation Prompt Templates
In this section, we present the specific prompt templates employed for the problem rephrasing and validation
processes. The Rephrase Prompt is designed to generate diverse narrative scenarios while strictly preserving
the underlying mathematical structure. Subsequently, the Rephrase-Verified Prompt serves as a quality
control mechanism to rigorously verify the structural equivalence between the original and the rephrased
problems.
Rephrase Prompt
You are an expert in operations research problem design and NLP data augmentation. Your task is to
take the following optimization problem and rewrite it according to the instructions.
Original Problem: {{Original problem}}
Instructions:
- Rewrite the problem in a different real-world scenario or application context, while preserving its
mathematical structure, optimization goal, and logical constraints.
- All numerical values, quantities, and parameter relationships must remain exactly the same.
- Use different terminology, phrasing, and narrative style to describe the problem, but ensure that the
underlying model and relationships are identical.
- Do not add or remove any mathematical constraints, variables, or objectives.
- The rewritten problem should read naturally and clearly as a self-contained description in the new
scenario.
- Do not include any explanations, reasoning, or headers.
- Output only the rewritten problem description, without commentary.
Output: [Start your output below. No headers, no comments.]
15


--- Page 16 ---
Rephrase-Verified Prompt
You are an expert in operations research and mathematical modeling.
Below are two optimization problem descriptions. Please check if the mathematical structure of the
rephrased problem is fully equivalent to the original one — meaning they have the same decision
variables, objective function type (min or max), and constraint relationships.
Original Problem: {{Original problem}}
Rephrased Problem: {{Rephrased problem}}
Answer only in the following JSON format:
“equivalent": true/false, “reason": “your short reasoning"
B.2
Classification and Design Templates for Optimization Problems
B.2.1
Traveling Salesman Problem (TSP)
Problem Description. The TSP problem is a classical combinatorial optimization problem where a salesman
must find the shortest possible routes that visits each city exactly once and return to the original city, given
a list of cities and the distances between each pair of cities. In formal terms, the TSP can be represented
on a complete weighted graph G = (V, E), where each vertex vi ∈V corresponds to a city, and each edge
(vi, vj) ∈E has an associated distance dij. A binary decision variable xij is defined such that xij = 1 if the
route directly travels from city i to city j, and xij = 0 otherwise. The objective is to find a Hamiltonian
cycle with the minimum total edge weight: min P
(i,j)∈E dijxij subject to constraints ensuring that each city
is entered and left exactly once, and subtour elimination constraints to guarantee a single connected tour.
This problem is a NP-hard optimization task whose complexity increases factorially with the number of
cities, that is, n cities correspond to n! possible routes. Even modest increases in n lead to a combinatorial
explosion in search space, making the problem an effective benchmark for evaluating optimization reasoning
under scaling difficulty.
Problem Design. We control the problem complexity by varying the number of cities n. {city_lines}
specifies the list of cities along with their corresponding coordinates, while {distance_text} describes the
pairwise distances between cities. {example_route} provides an illustrative example of a possible route,
formatted as A →B →D →C →A.
Traveling Salesman Problem(TSP) Template
Consider a delivery service that needs to visit {n} cities: {city_lines}. The distances between cities
are measured in kilometers:{distance_text}. The goal is to find the shortest possible route that visits
each city exactly once and returns to the starting city. This creates a ‘tour’ that minimizes the total
travel distance. For example, with just these n cities, starting from City A, some possible routes are:
{example_routes}. The challenge is to determine which of all possible routes has the minimum total
distance.
B.2.2
Bin Packing Problem
Problem Description. The Bin Packing Problem is a classical NP-hard combinatorial optimization problem
that seeks the most efficient way to pack a given set of items into the minimum number of identical bins, each
with a fixed capacity. Formally, let there be n items indexed by i = 1, . . . , n, each with weight wi, and bins of
identical capacity C. Define a binary variable xij such that xij = 1 if item i is assigned to bin j, and xij = 0
16


--- Page 17 ---
otherwise, and a binary variable yj indicating whether bin j is used. The problem can be formulated as:
min
X
j
yj
s.t.
X
j
xij = 1,
∀i = 1, . . . , n
(Each item in One Bin)
X
i
wixij ≤C yj,
∀j
(Bin Capacity Constraint)
xij ∈{0, 1},
yj ∈{0, 1},
∀i, j.
(3)
The objective is to minimize the number of bins used while ensuring that no bin exceeds its capacity and
every item is packed exactly once. The Bin Packing Problem is NP-hard, with computational complexity
growing exponentially with the number of items n. Exact algorithms such as branch-and-bound or integer
programming exhibit worst-case complexity of O(2n)
Problem Design. We control the problem complexity by varying the number of items n and the bin capacity
C. {item_lines} specifies each product’s weight, while {bin_capacity} denotes the maximum capacity of each
bin. The goal is to pack all items into the fewest possible bins without exceeding their capacity constraints.
Bin packing Problem Template
A warehouse manager needs to pack different products into identical shipping containers.
The
available items include: {item_lines}. Each shipping container has a maximum weight capacity of
{bin_capacity} kg. The manager’s goal is to use the minimum number of containers while ensuring all
products are packed. Each product must be assigned to exactly one container, and the total weight in
each container cannot exceed its capacity.
B.2.3
Job-Shop Scheduling Problem
Problem Description. The Job-Shop Scheduling Problem is a classical combinatorial optimization problem
that involves determining the most efficient way to process multiple jobs on multiple machines. Each job
consists of a specific sequence of operations, and each operation must be performed on a designated machine
for a fixed processing time. Once an operation starts, it must run continuously until completion, and each
machine can process only one operation at a time. The goal is to find a feasible schedule that minimizes the
makespan—the total time required to complete all jobs. In formal terms, the problem can be represented
by a set of jobs J and a set of machines M. Each job j ∈J is defined as a sequence of ordered operations
(Oj1, Oj2, . . . , OjKj), where each operation Ojk is associated with a machine Mjk ∈M and a processing time
pjk. The objective is to determine the start time of each operation on its assigned machine such that no two
operations overlap on the same machine, and the precedence order of operations within each job is respected.
The jobshop problem is an NP-hard optimization problem whose complexity grows combinatorially with
the number of jobs and machines. As both dimensions increase, the number of feasible schedules expands
exponentially, making it an effective benchmark for assessing reasoning and optimization performance under
increasing problem difficulty.
Problem Design. We control the problem complexity by varying the number of jobs n and machines
m. {job_text} specifies each job’s sequence of operations, where each operation is represented as a pair of
machine and processing time. The goal is to determine the optimal processing order on all machines that
minimizes the makespan, ensuring that each machine processes only one operation at a time and that job
precedence constraints are satisfied.
17


--- Page 18 ---
Job-shop Problem Template
Suppose there are {n_jobs} jobs that need to be processed on {n_machines} machines. Each job
consists of a sequence of operations represented as pairs (Machine, Processing time), where each pair
specifies the machine on which the operation must run and the amount of time it requires. The order
of pairs indicates the required sequence in which the operations must be performed. Job details:
{job_text} Each operation must run continuously once it starts and cannot be interrupted, and each
machine can only process one operation at a time. The objective is to determine the processing order
of all operations on the machines so that the makespan (i.e., the total completion time of all jobs) is
minimized.
B.2.4
Minimum Cost Netflow Problem
Problem Description. The minimum cost network flow problem in the transportation form seeks the
most cost-efficient way to ship goods from a set of warehouse nodes S (supply nodes) to a set of store
nodes D (demand nodes), while satisfying both supply and demand requirements and respecting capacity
limits on each transportation route. Each arc (i, j) from warehouse i ∈S to store j ∈D has an associated
unit transportation cost cij, capacity limit uij, and flow variable xij ≥0. Each warehouse i provides a
supply amount si, and each store j requires a demand amount dj, where total supply equals total demand:
P
i∈S si = P
j∈D dj. The objective is to minimize the total transportation cost while ensuring all supply and
demand constraints are satisfied:
min
{xij≥0}
X
i∈S
X
j∈D
cij xij
s.t.
X
j∈D
xij = si,
∀i ∈S
(Supply Constraints)
X
i∈S
xij = dj,
∀j ∈D
(Demand Constraints)
0 ≤xij ≤uij,
∀(i, j) ∈S×D
(Capacity Constraints).
(4)
The computational complexity grows rapidly with the number of nodes and arcs, since the total number of
decision variables scales with |S| × |D|. As the network expands, the solution space increases combinatorially,
making the optimization problem more challenging for larger instances.
Problem Design. We control the complexity of the problem by varying the total number of nodes n, which
determines the number of warehouses and stores in the network. {warehouse_lines} specifies the supply
capacity of each warehouse, {store_lines} specifies the demand of each store, and {arc_lines.strip()} describes
the transportation routes between them, including each route‘s capacity limit and per-unit shipping cost.
Netflow Problem Template
A logistics company needs to ship goods from {n} warehouses to {n} retail stores: Each warehouse
has a supply capacity: {warehouse_lines}. Each retail store has a fixed demand: {store_lines}. The
transportation routes between each warehouse and store have specific capacity limits and shipping
costs (cost per unit): {arc_lines.strip()}. The company wants to determine how many units of goods
to ship from each warehouse to each store in order to minimize the total shipping cost, while satisfying
all store demands, not exceeding any warehouse’s supply, and respecting the capacity limits of each
transportation route.
B.2.5
Knapsack Problem
Problem Description. The Knapsack Problem is a classical combinatorial optimization problem where,
given a set of items each with a weight and a value, the goal is to determine which items to include in a
collection so that the total weight does not exceed a given capacity limit, while maximizing the total value
obtained. In formal terms, let each item i ∈{1, 2, . . . , n} have a value vi and a weight wi, and let W denote
the maximum weight capacity of the knapsack. Define a binary decision variable xi ∈{0, 1}, where xi = 1 if
18


--- Page 19 ---
item i is included in the knapsack, and xi = 0 otherwise. The problem can then be formulated as:
max
n
X
i=1
vixi
s.t.
n
X
i=1
wixi ≤W,
xi ∈{0, 1}, i = 1, . . . , n.
(5)
The computational complexity of the Knapsack Problem grows exponentially with the number of items n
when solved by exhaustive enumeration, and pseudo-polynomially with the product of the number of items
and the capacity W when solved using dynamic programming (O(nW)). Consequently, increasing either n or
W significantly amplifies the computational burden.
Problem Design. We control the problem complexity by varying the number of items n, and define the
knapsack capacity as a fixed ratio of the total weight of all items. {item_list} specifies the list of items, each
associated with a weight and a value.
Knapsack Problem Template
A hiker is preparing for a outdoor hiking trip. They need to select the most valuable combination of
equipment and supplies from many available options within the limited backpack capacity. The items
include: {items_list}. Assuming the backpack has a maximum weight capacity of {capacity} kg, the
hiker’s goal is to select the combination of items with the highest total value while not exceeding the
weight limit. Each item must be either taken in its entirety or left behind.
B.2.6
Inventory Problem
Problem Description. The inventory Problem is an optimization-based decision-making problem based on
coordinating procurement, storage, and shortage management over multiple periods. The problem considers
a planning horizon of T discrete periods. In each period t, the decision-maker determines the quantity of
orders, subject to the daily quantity limit Qmin and Qmax, given the unit purchase cost p, the holding cost
h, and the shortage cost c. The initial inventory is I0, and the warehouse capacity is denoted by C. The
goal is to satisfy the time-varying demand Dt in each period t, accounting for a delivery lead time l, while
minimizing the total costs, including purchasing, holding, and shortage costs across the entire horizon.
In formal terms, let ot denote the order quantity placed at the beginning of period t, at represent the quantity
received at the beginning of period It denote the last of inventory amount at the end of period t, and
strepresent the shortage during period t. The problem can be formulated as:
min
X
t
(pot + hIt + cst)
s.t.
Qmin ≤ot ≤Qmax,
∀t ∈T
(Order Quantity Constraint)
It = It−1 + at + st −Dt,
∀t ∈T
(Production-Demand Balancing Constraint)
It−1 + at ≤C,
∀t ∈T
(Warehouse Capacity Constraint)
at = { ot−l
t > l
0
t ≤l ,
∀t ∈T
(Definition of the Receipt Quantity)
I0 = I0,
(Boundary Constraint).
(6)
The computational complexity of the Inventory Problem is primarily driven by the length of the planning
horizon T and the size of the discrete state and action spaces.
In dynamic programming sight,under the warehouse’s capacity C, there are O(C) states in each period,
and the DP recursion updates every state by evaluating all feasible order quantities. The resulting time
complexity is therefore O(T · C · |A|), (where |A| is the number of admissible order quantities per period),
which is pseudo-polynomial in the planning horizon and the sizes of the state and action spaces. Consequently,
holding the capacity and the admissible order range, increasing the horizon length T significantly amplifies
the computational burden of solving Inventory Problem.
Problem Design. We control the problem complexity by increasing the period T, and define the other
variables are fixed. {demand_list} specifies the list of time-varying demand.
19


--- Page 20 ---
Inventory Problem Template
A factory must develop an ordering and inventory plan for a key material over a planning horizon of
T days. The initial inventory at the beginning of the planning period is {I0} units. In each period
t = 1, ..., {T}, the supplier allows the factory to place an order whose quantity must lie between
{Qmin} and {Qmax} units. However, each order placed will take {lead} day(s) to arrive before it
can be used to satisfy demand or replenish inventory. The demand for the material in each period
is given as follows: {demand_lines} Shortages are permitted, but any unmet demand will not be
back-ordered. Throughout the planning horizon, material quantities are allowed to be fractional, and
the total amount of on-hand inventory at any time must not exceed the warehouse capacity of {C}
units. The total cost over the planning horizon consists of three components: the ordering cost, which
equals {p} per unit ordered; the holding cost, which equals {h} per unit of inventory carried from one
period to the next; and the shortage penalty, which equals {c} per unit of unmet demand. Please
determine the optimal order quantity for each period and track the resulting inventory and shortage
levels so as to minimize the total cost.
B.2.7
Portfolio Allocation Problem
Problem Description. The Portfolio Allocation Problem is a classical Linear Programming problem that
evaluates an investor’s ability to balance risk, return, and liquidity under multiple investment constraints. The
problem considers a set of I asset categories, each characterized by an expected return ri and an associated
risk level vi. The decision-maker determines the investment proportion xij ≥0 allocated to each asset i, which
must lie within a specified bound [li, ui]. A subset of assets L ⊆I represents liquid assets that contribute
to the portfolio’s liquidity requirement. The goal is to maximize the portfolio’s total expected return while
satisfying several constraints. The problem can be formulated as:
max
X
i∈I
rixi
s.t.
X
i∈I
xi = 1,
(Budget Constraint)
X
i∈I
vixi ≤Vmax,
(Risk Control Constraint)
X
i∈I
rixi ≥Rmin,
(Minimum Return Constraint)
X
i∈L
xi ≥Lmin,
(Liquidity Constraint)
li ≤xi ≤ui,
(Investment Proportion Constraint).
(7)
Problem Design. We control the complexity of the problem by increasing the number of the categories of
assets I. {asset_lines} includes the expected return ri, risk levelvi and proportion boundsli, ui of each asset
i. {L_assets} is a set of all liquid assets. {Lmin} is the minimum level of liquidity, {Rmin} is the required
minimum return, {Vmax} is the supreme risk level.
Portfolio Allocation Problem Template
An investor wishes to allocate capital among {I} asset classes with the goal of maximizing the
total expected return of the portfolio.The characteristics of each asset are summarized as follows:
{asset_lines}. Each asset must receive a proportion of the total investment that satisfies its individual
lower and upper bounds, and the total of all investment proportions must sum to one. To ensure
sufficient liquidity, the investor requires that the group of liquid assets, represented by the subset L
={L_assets}, collectively receive at least {L_min} of the total capital.At the same time, the overall
portfolio risk, measured by a specified risk index, must not exceed {V_max}, and the total expected
return of the portfolio must be no less than {R_min}. Please determine the optimal portfolio weights
that maximize total expected return subject to all constraints.
20


--- Page 21 ---
B.2.8
Production Problem
Problem Description. The Production Problem is a classical linear programming formulation that seeks
to maximize total profit subject to limited resource constraints. Consider a simplified setting with n types of
products to be manufactured, each requiring m distinct production processes. For convenience, We assume
that the types of productes and the number of processes are equal(that is m = n). The profit per kilogram of
product i is denoted by pi, and the time required for product i in process j is represented by tij. Each process
j has a maximum available processing time of Tj. The objective is to determine the optimal production
quantities that maximize the overall profit. Accordingly, the problem can be formulated as follows:
max
X
i
pixi
s.t.
X
i
tijxi ≤Tj,
∀j
(Time Constraint)
xi ≥0,
∀i
(Non-Negative Constraint).
(8)
Problem Design. We control the problem complexity by varying the number of product types and processing
operations. The set {I} represent the collections of products and production processes respectively, which
sizes are equal. The term {unit_label} denotes the unit of measurement for each product. The specification
{profit_lines} provides the profit values associated with each product. Both {time_lines} and {op_cap_lines}
describe the processing time required for each product across different operations, while {op_range} indicates
the maximum available time capacity for each operation.
Production Problem Template
A factory intends to produce {I} types of products, each of which requires {I} processing operations
to complete. The profit earned per {unit_label} of each product is given as follows: {profit_lines}.
Processing time required per for each operation is as followed: {time_lines}, {op_cap_lines}. On the
premise of guaranteeing for each operation {op_range}, total processing time must not exceed its
available time. Please schedule the production plan to maximize total profit.
B.2.9
Transportation Problem
Problem Description. The transportation problem is a classical linear optimization problem that focuses
on minimizing total shipping cost while satisfying supply and demand constraints across multiple locations. It
serves as a fundamental model in operations research and logistics optimization, widely applied in production
planning, distribution management, and resource allocation.
The problem involves two disjoint sets of nodes: a set of production sites A = {1, 2, · · · , n} and a set of
sales destinations B = {1, 2, · · · , m}. Each production site Ai has a limited supply capacity ei, while each
sales destination Bj has a fixed demand requirement dj. The cost of transporting one unit of goods from
production site Ai to destination Bj is denoted by cij. The decision variable xij represents the amount of
goods shipped from Ai to Bj.
The objective is to determine an optimal shipping plan that minimizes the total transportation cost. The
problem can be formulated as
min
X
i
X
j
cijxij
s.t.
X
j
xij ≤ei,
∀i ∈A
(Supply Constraint)
X
i
xij = dj,
∀j ∈B
(Demand Constraint)
xij ≥0,
∀i, j ∈A, B
(Non-Negative Constraint).
(9)
Each production site in set A is associated with a fixed supply capacity, while each sales destination in set
B has a specified demand that must be satisfied exactly. For convenience, we control both set’s size in A.
The unit shipping cost between each production site and destination is given by a cost matrix C = [cij]. By
adjusting n, we can scale the dimensionality of the decision space and the number of flow constraints.
21


--- Page 22 ---
Problem Design. We control the problem complexity by varying the number of production sites(which
equals to the number of sales destinations)n = |A|(|B|). The specification {supply_lines} presents the
available supply quantities for each production site in set A, while {demand_lines} provides the required
demand levels for each destination in set B. The term {cost_lines} denotes the unit transportation cost
matrix between all production–destination pairs. In particular, the cost information is organized and displayed
in a tabular form for clarity and ease of reference.
Transportation Problem Template
Consider a transportation problem that aims to minimize the total shipping cost from production
sites A to sales destinations B. The available supply at each production site (set A) is given as follows:
{supply_lines}. The demand that must be met at each sales destinations (set B) is specified below:
{demand_lines}. The unit shipping cost from each production site to each destination is as follows:
{cost_lines}. Please choose shipment to minimize the total cost.
B.2.10
Pollution Control Problem
Problem Description. The pollution control problem is a constrained optimization problem that focuses
on minimizing the total cost of emission control while achieving a predefined pollution reduction target.
The problem involves a set of emission sources T = {1, 2, · · · , T}, each representing a thermal power plant
or industrial facility that emits pollutants such as flue gas. Each source i has an emission rate wi and a
production output pi. A set of available abatement technologies K = {1, 2, · · · , K} is provided, where each
technology j is characterized by a removal efficiency sj and an associated unit cost cij when applied to source
i.
The decision variable xij represents the quantity of production at emission source i that adopts abatement
technology j. The goal is to minimize the total abatement cost. The problem can be formulated as,
min
T
X
i=1
k
X
j=1
cijxij
s.t.
k
X
j=1
xij = pi
∀j
(Production Constraint)
T
X
i=1
k
X
j=1
wixijsj ≥P ·
T
X
i=1
wipi
(Pollution Reduction Constraint)
xij ≥0
∀i, j
(Non-Negative Constraint)
(10)
Problem Design. We control the problem complexity by varying the number of emission sources T
and available control methods K.
For convenience, we assume these two numbers are equal(T = K).
{source_lines} specifies the characteristics of each emission source, including its emission rate and production
output. {method_lines} describes the set of available control methods, each associated with a distinct removal
efficiency. The cost structure for all source method combinations is summarized in {cost_lines}, which is
displayed in a tabular form.
Pollution Control Problem Template
A region seeks to design an air-pollution control plan to reduce total suspended particulate (TSP)
emissions from several industrial point sources. Initially, no control measures have been applied. The
characteristics of each emission source are as follows: {source_lines}. To mitigate emissions, several
control methods are available, each characterized by a specific removal efficiency: {method_lines}.
Applying a control method to a source incurs an additional cost per unit of production. The cost
structure for all source-method combinations is summarized below: {cost_lines}. Please choose how to
apply control methods to each source (sources may adopt multiple methods simultaneously), and note
that a source may also remain partially uncontrolled if necessary. The goal is to ensure that the total
TSP emissions are reduced by at least proportion P = {P} of E0, while minimizing the total cost.
22


--- Page 23 ---
C
Details of Experiments
C.1
Experiment Setup
C.1.1
Evaluation Prompt Template
To evaluate the solution accuracy of each optimization problem instance, we utilize the following two prompt
templates, corresponding to the TIR and PTR paradigms, respectively.
Tool Integrated Reasoning (TIR)
You are an operation research and Gurobi solver expert. Below is an operations research question:
{{Question}}
Carefully analyze the problem to identify the core elements such as Decision Variables, Objective
Function, and Constraints, determine whether the variable is an integer or a continuous variable; Build
a mathematical model and corresponding gurobi codes start with:
python
import gurobipy as gp
from gurobipy import GRB
- Make sure the model variable is named ‘model’. - Avoid using “<" and “>" in Gurobi constraints;
instead, use “<=" or “>=" as appropriate. Think step by step to ensure flawless translation from
math to code.
Pure Text Reasoning (PTR)
You are a Mathematical Modeling and Optimization Consultant specializing in analytical solutions.
Below is an operations research question:
{{Question}}
Your task is to rigorously formulate and solve the following optimization problem.
Follow this structured approach: Begin with understanding the problem →Extract the set and
parameters →Identify the variables →Provide the objective function →Analyze the constraints →
Develop the mathematical model →Solve it step-by-step, output the corresponding decision variables
Instruct: 1.) All equations and mathematical definitions must be presented clearly. 2.) Provide a
clear, step-by-step solution process. Absolutely do not use or reference any external OR solvers or
software (e.g., Gurobi, PuLP, Solver). The solution must be purely analytical. Output the final
optimal objective function value in markdown with the exact tag: <answer> optimal value here
< /answer>
C.1.2
Training and inference setting
Training setup.
The training of Qwen3-4B-RL involved a rigorous RL training phase initialized from
Qwen3-4B-Instruct [20]. Following the Solver-Informed RL approach [17], we adapted the Verl framework [42]
to incorporate optimization domain-rewards into the advantage estimation. Training was performed on a
node with eight 80GB NVIDIA H100 GPUs, requiring 192 GPU hours per stage. The key hyperparameters
for Qwen3-4B-RL training are detailed in Table 2:
Inference Setup.
We employ the top-P (nucleus) decoding strategy [43] for the training and inference
phases. The exact sampling hyperparameters used to generate our main results are specified in Table 3:
23


--- Page 24 ---
Table 2: Training Parameters
Type
Parameter
Value
Algorithm
Advantage Estimator
reinforce_plus_plus
Data
Batch size
64
Learning rate
1e-6
Max prompt length
2048
Max response length
16384
Truncation
left
Actor/Rollout
KL loss type
low_var_kl
KL loss coefficient
0.005
Rollout number
8
PPO mini batch size
8
PPO micro batch Size per GPU
4
Clip ratio low
0.20
Clip ratio high
0.28
Table 3: Sampling parameters used for text generation.
Parameter
Value
n
1
Temperature
0.5
Top-p
0.95
Max Response Length
16128
Repetition penalty
1.02
C.2
Experimental Details of TIR and PTR Comparison
C.2.1
Experimental Details for Ten Problems Classes
We evaluated overall ten classes of problems in OPT-ENGINE using DeepSeek V3.2, GPT 5.1, Qwen3-4B
Instruct and its fine-tuned variant Qwen3-4B-RL, applying both the TIR and PTR methods. While the
results for the Traveling Salesman Problem (TSP), Knapsack Problem, and Inventory Problem are presented
in the main text, the figures for the remaining seven problem classes are detailed below.
C.2.2
Experiments Details on Public Datasets
We also evaluated the TIR and PTR methods on three public available datasets: Mamo Complex, IndustryOR,
and OptMATH. The results are presented below.
24


--- Page 25 ---
Base Model
MamoComplex
IndustryOR
OptMATH
PTR
TIR
PTR
TIR
PTR
TIR
DeepSeek-V3.2
38.9%
56.2%
49.0%
61.0%
21.1%
36.1%
GPT-5.1
54.7%
61.1%
54.0%
65.0%
24.1%
35.5%
Qwen3-4B-Instruct
38.9%
17.7%
49.0%
18.0%
10.2%
10.2%
Qwen3-4B-RL
39.4%
49.8%
55.0%
51.0%
25.3%
30.7%
∆
+0.5%
+32.1%
+6.0%
+33.0%
+15.1%
+20.5%
Table 4: Comparison between TIR and PTR across established benchmarks. The top two rows feature
frontier API-based models (DeepSeek-V3.2 and GPT-5.1), while the bottom two rows display the compact
Qwen3-4B-Instruct and its fine-tuned variant, Qwen3-4B-RL.
25


--- Page 26 ---
C.2.3
Case Study: PTR Output for TSP problem
We examine DeepSeek-V3.2’s Pure-Text Reasoning (PTR) traces on the Traveling Salesperson Problem
(TSP). By contrasting two instance scales (N = 4 and N = 8), we demonstrate how increased dimensionality
shifts the model’s reasoning strategy: while it performs an explicit enumeration of Hamiltonian cycles for
small-scale instances, it reverts to a greedy nearest-neighbor heuristic as the search space expands.
TSP (N-cities=4)
With n = 4, possible Hamiltonian cycles:
(n−1)!
2
= 3!
2 = 3 distinct tours (undirected).
Let’s list them and compute total distances.
Tour 1: 1−2−3−4−1
(1−2) : 74.3
(2−3) : 26.0
(3−4) : 116.1
(4−1) : 168.4
Sum: 384.8 km
Tour 2: . . .
Tour 3: . . .
Minimum: 384.8 km.
TSP (N-cities=8)
We can attempt a Nearest Neighbor Heuristic for
an initial solution, then refine it manually.
Step 1: List distances. A–B: 147.1, A–C: 163.7, . . .
Step 2: Start at A. Nearest neighbor: D (44.3).
From D, closest unvisited is E (82.1). Tour: A–D–E
. . .
Step 3: Check improvements. Ending with F–A
(165.4) is suboptimal.
Best found: 502.7 km.
Figure 17: Comparative Analysis of DeepSeek-V3.2 Performance in Pure-Text Reasoning for TSP: N = 4 vs.
N = 8
26
