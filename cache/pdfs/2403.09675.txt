--- Page 1 ---
Open-Universe Indoor Scene Generation using
LLM Program Synthesis and Uncurated Object Databases
RIO AGUINA-KANG‚àó, UC San Diego, USA
MAXIM GUMIN‚àó, Brown University, USA
DO HEON HAN‚àó, Brown University, USA
STEWART MORRIS‚àó, Brown University, USA
SEUNG JEAN YOO‚àó, Brown University, USA
ADITYA GANESHAN, Brown University, USA
R. KENNY JONES, Brown University, USA
QIUHONG ANNA WEI, Brown University, USA
KAILIANG FU, Dymaxion, LLC, USA
DANIEL RITCHIE, Brown University, USA
‚ÄúA living room for watching TV‚Äù
‚ÄúA high-end mini restaurant‚Äù
‚ÄúA witch‚Äôs room with a cauldron‚Äô
‚ÄúA Japanese living room‚Äù
‚ÄúA living room‚Äù
‚ÄúA dining room for one‚Äù
‚ÄúA bedroom‚Äù
‚ÄúAn old-fashioned bedroom‚Äù
Fig. 1. Our method generates 3D indoor scenes from open-ended text prompts. Generated scenes are not limited to a fixed set of room types or object
categories; this ‚Äúopen-universe‚Äù capability is enabled by judicious use of pre-trained large language models (LLMs) and vision-language models (VLMs).
We present a system for generating indoor scenes in response to text prompts.
The prompts are not limited to a fixed vocabulary of scene descriptions, and
the objects in generated scenes are not restricted to a fixed set of object
categories‚Äîwe call this setting open-universe indoor scene generation. Un-
like most prior work on indoor scene generation, our system does not require
a large training dataset of existing 3D scenes. Instead, it leverages the world
knowledge encoded in pre-trained large language models (LLMs) to synthe-
size programs in a domain-specific layout language that describe objects and
spatial relations between them. Executing such a program produces a speci-
fication of a constraint satisfaction problem, which the system solves using
a gradient-based optimization scheme to produce object positions and orien-
tations. To produce object geometry, the system retrieves 3D meshes from a
‚àóThese authors contributed equally
Authors‚Äô addresses: Rio Aguina-Kang, raguinakang@ucsd.edu, UC San Diego, USA;
Maxim Gumin, maxgumin@gmail.com, Brown University, USA; Do Heon Han, do_
heon_han@brown.edu, Brown University, USA; Stewart Morris, stewart_morris@
brown.edu, Brown University, USA; Seung Jean Yoo, seung_jean_yoo@brown.edu,
Brown University, USA; Aditya Ganeshan, aditya_ganeshan@brown.edu, Brown Uni-
versity, USA; R. Kenny Jones, russell_jones@brown.edu, Brown University, USA; Qi-
uhong Anna Wei, qiuhong_wei@brown.edu, Brown University, USA; Kailiang Fu,
kailiang.fu@dymaxion.design, Dymaxion, LLC, USA; Daniel Ritchie, daniel_ritchie@
brown.edu, Brown University, USA.
database. Unlike prior work which uses databases of category-annotated,
mutually-aligned meshes, we develop a pipeline using vision-language mod-
els (VLMs) to retrieve meshes from massive databases of un-annotated,
inconsistently-aligned meshes. Experimental evaluations show that our sys-
tem outperforms generative models trained on 3D data for traditional, closed-
universe scene generation tasks; it also outperforms a recent LLM-based
layout generation method on open-universe scene generation.
CCS Concepts: ‚Ä¢ Computing methodologies ‚ÜíComputer graphics;
Neural networks; Natural language generation.
Additional Key Words and Phrases: indoor scene synthesis, program syn-
thesis, layout generation, large language models, vision language models,
foundation models
1
INTRODUCTION
Many people spend a significant portion of their lives indoors: in
their homes, workplaces, social gathering spaces, etc. Unsurpris-
ingly, indoor environments also feature heavily in virtual depic-
tions of the real world: in games, extended reality experiences, and
architectural visualizations. Such virtual scenes have real-world
uses, as well. For example, there are now a variety of free-to-use
arXiv:2403.09675v1  [cs.CV]  5 Feb 2024


--- Page 2 ---
2
‚Ä¢
Rio Aguina-Kang, Maxim Gumin, Do Heon Han, Stewart Morris, Seung Jean Yoo, Aditya Ganeshan, R. Kenny Jones, Qiuhong Anna Wei, Kailiang Fu, and Daniel Ritchie
interior design tools online which allow users to explore virtual
re-designs of their own real spaces [Planner5d 2024; RoomSketcher
2024; Target 2024]. In addition, furniture and home product retailers
are increasingly using renderings of virtual scenes to stage and
advertise their products, as the process of doing so is easier, less
expensive, and more adaptable to different regions of the world than
taking physical photographs [Hobbs 2024]. Finally, virtual indoor
scenes have become a critical data source for training autonomous
embodied agents to perceive and navigate within typical indoor
environments [Deitke et al. 2022b; Puig et al. 2023].
Given the importance of virtual indoor scenes to the above appli-
cations, computational design tools which ease their creation would
be valuable. Generative models, i.e. systems which can sample novel
scenes from a distribution of interest, are a particularly promising
technology for this purpose. Such models can be used to suggest
possible placements for new objects in a scene [Zhou et al. 2019],
suggest completions for partial scene designs [Ritchie et al. 2019],
or even synthesize entirely new scenes from whole cloth [Gao et al.
2023b; Paschalidou et al. 2021; Tang et al. 2023]. These capabilities
can be used to build tools for interactive design or for the automated
creation of large-scale virtual worlds.
The prevailing methodology for building generative models of
indoor scenes is to train machine learning models on datasets of
existing 3D room layouts. It is time-consuming and expensive to
produce such datasets at the scale required by modern machine
learning methods; as such, only a handful of these datasets exist [Fu
et al. 2021; Yadav et al. 2023]. These existing datasets contain room
labeled from a finite set of room types (e.g. bedrooms, living rooms,
offices), and each room is populated with objects from a curated
set of 3D object models belonging to a small finite set of object
categories (e.g. tables, chairs, beds). We can think of generative
models trained on these datasets as being closed-universe: they know
up-front the small, finite set of object and room types that they will
ever have to produce.
Could one create an open-universe generative model which can
synthesize any type of indoor scene containing whatever types of
objects are needed by that scene? It has only recently become possi-
ble to contemplate this question with the development of so-called
‚Äúfoundation models‚Äù: large machine learning models pre-trained on
enormous datasets of text and/or images [GPT-4 2023; Mizrahi et al.
2023; Radford et al. 2021; Rombach et al. 2021]. For example, prior
work has shown how to use pre-trained text-to-image generative
models to synthesize 3D content which satisfies an arbitrary text
prompt [H√∂llein et al. 2023; Jain et al. 2022; Poole et al. 2022; Yi et al.
2023]. While these systems are compelling, they produce output in
the form of a single unstructured mesh or density/radiance field;
these representations frequently exhibit artifacts and do not easily
support editing a scene by moving, swapping, or deleting objects.
In contrast, most prior methods for indoor scene synthesis produce
a layout of individual objects, each of which is represented by a
high-quality 3D mesh retrieved from a database.
In this paper, we present an open-universe generative model of
3D indoor scenes which produces such structured object layouts in
response to a text prompt (Fig. 1). Building such a system requires
solving several subproblems. First, the system must determine the
objects that should be in the scene and where they should be located.
To solve this problem, we leverage the commonsense world knowl-
edge encoded by a pre-trained large language model (LLM) [GPT-4
2023]. Empirically, we find that tasking an LLM with directly speci-
fying the locations of scene objects leads to poor performance, likely
due to the mismatch between metric location coordinates and the
vast majority of natural language text contained in its training cor-
pus. Instead, we task the LLM with producing a declarative program
in a domain-specific language that describes the objects in the scene
and a variety of spatial relation constraints between them. Executing
these programs produces a constraint satisfaction problem which
the system solves using a gradient-based optimizer to find one or
more object layouts which satisfy the specified constraints.
Once the layout of objects is determined, the system must next
insert a 3D mesh for each object in the layout. One could consider
using text-to-3D generative models to synthesize these meshes [Jun
and Nichol 2023; Nichol et al. 2022; Poole et al. 2022], but as men-
tioned above, these models can exhibit artifacts and do not (yet)
produce outputs with comparable quality to human-created 3D
meshes. Thus, like prior work on indoor scene synthesis, our system
retrieves 3D meshes from a 3D object database. However, unlike
prior work, our object retrieval system is designed for the open-
universe setting: retrieving from million-scale databases of unla-
beled, inconsistently-oriented 3D meshes [Deitke et al. 2023, 2022a].
We develop a ranking and filtering algorithm using a combination
of pre-trained models [GPT-4 2023; Zhai et al. 2023a] to retrieve a
3D mesh which matches the attributes of an object specified in the
object layout. We also leverage these models to automatically deter-
mine the front-facing direction of each retrieved object, allowing
the system to correctly orient each retrieved object as specified in
the layout.
We evaluate our system by using it to generate a large variety
of different types of rooms, ranging from common indoor spaces
(e.g. ‚Äúa bedroom‚Äù) to rooms designed for specific activities (e.g. ‚Äúa
musician‚Äôs practice room‚Äù) to outlandish/fantastical scenes (e.g. ‚Äúa
wizard‚Äôs lair‚Äù). For generating typical indoor rooms, we compare to
prior methods for closed-universe scene synthesis which are trained
on existing 3D scene datasets. Our system produces scenes which
are preferred to those generated by these prior methods in a forced-
choice perceptual study. For open-universe scene synthesis, we
compare to LayoutGPT, a recently-published method for generating
layouts using large language models. Our system also outperforms it
in forced choice perceptual study. We also conduct ablation studies
on each component of our system to validate our design decisions.
In summary, our contributions are:
(i) A DSL for specifying indoor scene layouts through declarative
constraints and a gradient-based executor for this DSL capable
of realizing a distribution of valid scenes from a single program
(ii) A robust prompting workflow that leverages LLMs to synthe-
size programs in our DSL from a high-level natural language
description of a scene
(iii) A pipeline using pretrained vision-language models for re-
trieving and orienting 3D meshes from a large, unannotated
database to fit object specifications from a scene program
(iv) Protocols for evaluating open-universe indoor synthesis sys-
tems, including a benchmark set of input descriptions covering
a wide variety of possible rooms.


--- Page 3 ---
Open-Universe Indoor Scene Generation
‚Ä¢
3
Our code will be made available as open source upon publication.
2
RELATED WORK
Indoor Scene Synthesis. Indoor scene synthesis has been a prob-
lem of interest in computer graphics for years. In an interesting
instance of history repeating itself, some of the earliest work in
this area formulated the problem as text-to-scene generation, albeit
via laboriously hand-crafted rules [Coyne and Sproat 2001]. Later,
researchers built systems for laying out objects to be consistent with
a set of manually-defined design principles [Merrell et al. 2011], sim-
ple statistical relationships between objects extracted from a small
set of examples [Yu et al. 2011], or with programmatically-specified
constraints [Yeh et al. 2012].
After this, indoor scene synthesis research focused on data-driven
methods, using a variety of machine learning methods: Bayesian
networks and Gaussian mixture models [Fisher et al. 2012], factor
graphs [Kermani et al. 2016], topic models [Liang et al. 2017], and
stochastic grammars [Qi et al. 2018]. Once deep neural networks
gained popularity, a wave of research applying them to indoor scene
synthesis followed: method were proposed using convolutional net-
works [Ritchie et al. 2019; Wang et al. 2019, 2018], tree and graph
neural networks [Li et al. 2018; Wang et al. 2019; Zhou et al. 2019],
generative adversarial networks [Zhang et al. 2018], transform-
ers [Paschalidou et al. 2021; Wang et al. 2020], and finally denoising
diffusion models [Tang et al. 2023]. All of these prior works present
closed-universe generative models, and all of them require (in some
cases quite large) datasets of 3D scenes for training.
Recently, the development of pre-trained large language models
(LLMs) has raised the possibility of a new generation of text-to-
scene generative models, more flexible and open-ended than the
early systems from decades ago. LayoutGPT [Feng et al. 2023] is an
LLM-based system designed for generating image layouts in an a
CSS-like format; the authors also show applications to indoor scene
synthesis, albeit for the closed-universe case. In work concurrent to
ours, the Holodeck system shows the ability to use LLMs to generate
environments for training embodied AI agents [Yang et al. 2023].
This system resembles ours in some aspects, including supporting
general text prompts instead of fixed room types and specifying
object locations implicitly via relations. It also differs from ours in
significant ways: using a simpler specification for object relations
(we use a DSL embedded in Python); lacking mechanisms for auto-
matically correcting errors in LLM output; solving for object layouts
on a grid, rather than continuously (so objects cannot be adjacent to
one another). Most importantly, it retrieves objects from a curated
set of annotated and aligned 3D models, so it cannot be considered
truly open-universe.
Open-vocabulary Text-to-3D. There has been a recent surge in
work leveraging pre-trained vision-language models [Radford et al.
2021; Rombach et al. 2021] to produce 3D content from arbitrary
text prompts without any training data. The most prevalent type of
such system works via ‚Äúoptimization-based inference,‚Äù optimizing
for a new 3D output in response to each new text prompt [Chen
et al. 2023; Jain et al. 2022; Lin et al. 2023; Poole et al. 2022; Wang
et al. 2023; Yi et al. 2023]. Another line of work seeks to amortize
this inference procedure by training feedforward neural networks
to produce 3D output from a distribution of text inputs [Jun and
Nichol 2023; Lorraine et al. 2023; Nichol et al. 2022; Sanghi et al.
2022, 2023]. The outputs of these methods are either point clouds,
unstructured meshes, or isosurfaces extracted from density fields,
which are (to date) lower-quality than human-created 3D models.
Additionally, since these systems leverage models trained on images
to synthesize 3D structures, they can also suffer from multiview in-
consistency artefacts, such as the infamous ‚ÄúJanus face‚Äù issue [Poole
et al. 2022]. Their output also cannot be easily modified, because it
is not decomposed into individual objects.
These systems are designed with single-object generation in mind
but have been extended to open-vocabulary scene synthesis. By
combining 2D generative image out-painting with a depth align-
ment module, Text2Room [H√∂llein et al. 2023] generates textured
meshes of 3D rooms for a given text prompt. Other works [Bai et al.
2023; Fang et al. 2023; Gao et al. 2023a; Schult et al. 2023] allow the
specification of a 3D semantic object layout, which is then used
along with text-to-image models for generating textured meshes
of scenes/rooms. These method suffer from the same mesh quality
drawbacks as their single-object counterparts They also assume
an object layout as input, whereas our method generates one. Our
approach could potentially be combined with methods in this space;
for example, generative re-texturing of retrieved 3D meshes to fit a
desired style or theme [Huang et al. 2023b].
3D Shape Analysis with Foundation Models. In addition to using
pre-trained vision language models (VLMs) for 3D content gen-
eration, researchers have also explored how to use these models
to analyze existing 3D content without requiring 3D supervision.
Methods have been proposed for captioning/annotating 3D ob-
jects [Haocheng et al. 2023; Luo et al. 2023], segmenting 3D shapes
into semantic parts or identifying regions of interest [Abdelreheem
et al. 2023b; Decatur et al. 2022; Liu et al. 2023; Zhou et al. 2023],
and even establishing correspondences between 3D shapes [Ab-
delreheem et al. 2023a] Our system uses VLMs to retrieve shapes
from a large database, determine their category, and determine their
front-facing orientation.
Program Synthesis with Large Language Models. One of the key
components of our system is using an LLM to generate a declarative
program which specifies the layout of objects in a scene. Other work
has also explored the use of LLMs to generate programs. Multiple
works [AlphaCode Team 2023; Li et al. 2022] explore the use of
LLMs for competitive programming, demonstrating the prowess
of LLMs (augmented with symbolic search) at synthesizing pro-
grams for a given natural language task description. LLMs‚Äô program
synthesis abilities have also been employed for solving complex
geometric reasoning problems [Trinh et al. 2024] and discovering
novel mathematical concepts [Romera-Paredes et al. 2023]. Beyond
generating programs, some recent works also explore the use of
LLMs to improve domain-specific languages automatically [Grand
et al. 2023].
Recently, systems have been proposed for visual question answer-
ing (VQA) by using LLMs to generate programs in a domain-specific
language (DSL) designed for image reasoning and then executing
that program to answer a given question [Gupta and Kembhavi 2023;
Sur√≠s et al. 2023]. LLMs have also been used for structured image


--- Page 4 ---
4
‚Ä¢
Rio Aguina-Kang, Maxim Gumin, Do Heon Han, Stewart Morris, Seung Jean Yoo, Aditya Ganeshan, R. Kenny Jones, Qiuhong Anna Wei, Kailiang Fu, and Daniel Ritchie
Inputs
Scene description
‚ÄúA chemical lab‚Äù
(Optional)
Room size, fullness
WxH, p% 
set_size(4.0, 4.0, 3.0)
room_fill_percentage = 48
dresser = Object(
  'dresser_cherry_wood‚Äô,
 1.0, 0.5, 0.8, SOUTH)
next_to_wall(dresser, NORTH)
...
for book in books:
 on(book, table)
...
Program 
Synthesizer
(Section 5)
Scene Description Program
(Section 4)
Scene Description
"A bedroom for the 
person who loves red."
(optional)
Room size, fullness
WxH, p%
Layout 
Optimizer
(Section 6)
Object 
Retrieval 
(Section 7)
Object 
Orientation 
(Section 8)
Scene Layout
Final Scene
Fig. 2. A schematic overview of our system. Given a high-level natural language description of a scene (plus optional constraints on the room size and object
density), an LLM-based program synthesizer produces a scene description program which specifies the objects in the scene and their spatial relations. Our
layout optimizer module then solves the constraint satisfaction problem implied by this program to produce a concrete layout of objects in the scene. For
each scene object, the object retrieval module finds an appropriate 3D mesh from a large, unannotated mesh database; the object orientation module then
identifies its front-facing direction so that it can be correctly inserted into the scene.
synthesis by equipping them with a DSL which helps specify 2D
object layouts [Cho et al. 2023]. Equipped with a library of Python
functions in Blender, LLMs have also been used to synthesize 3D
scenes, albeit for a closed set of scenes and objects supported by the
library [Sun et al. 2023].
3
OVERVIEW
We aim to solve the following problem: given a natural language
description of a desired 3D room-scale scene, produce a 3D scene
composed of positioned and oriented 3D meshes retrieved from a
database such that the output scene satisfies the input description.
The input description can be flexible: it could provide detailed in-
structions about the contents of the scene (e.g. ‚Äúan office with two
desks, a potted plant, and a sofa‚Äù) or be intentionally nebulous (‚Äúa
serious business office‚Äù). For additional control, we also support
optional inputs in the form of desired dimensions (in meters) for
the output room and how full the room should be (in terms of per-
centage of floor area occupied by objects). On the output side, we
assume that the generated room has four walls, and that each object
is oriented to face along one of the four cardinal directions (north,
south, east, west). These assumptions are reflective of many, but not
all, real-world rooms; in Section 10, we discuss ideas for removing
these assumptions.
Figure 2 shows a schematic overview of our system. Our sys-
tem specifies the objects which should be in the generated scene
and their spatial layout via a scene description program written in
a declarative domain-specific language embedded in Python (Sec-
tion 4). To produce this program, the system feeds the inputs to a
program synthesizer module (Section 5); internally, this module uses
a series of calls to an LLM to write the scene description program.
The complete scene description program is then passed to the layout
optimizer (Section 6), which converts the program into a constraint
satisfaction problem which it then solves using a gradient-based
optimization scheme, producing locations and orientations for all
objects. In addition, the object declarations in the scene description
program are sent to an object retrieval module (Section 7), which
retrieves from a large, unannotated database the 3D model which
best matches the description of each object. Finally, the object ori-
entation module (Section 8) determines the front-facing direction
of each retrieved object, allowing them to be inserted in the room
according to the optimized layout to produce the final output scene.
4
DESCRIBING SCENES WITH PROGRAMS
In this section we introduce our declarative domain-specific lan-
guage (DSL) for scene description. The simplest way to describe a
scene is to explicitly specify all object positions and orientations.
However, recent work has shown that even the state of the art LLMs
such as OpenAI‚Äôs GPT4 struggle with accurate placement of objects
and object parts [Makatura et al. 2023]. For example, when asked to
specify explicit coordinates, GPT4 often creates overlapping objects
and objects that are floating in the air. Thus, instead of specifying
explicit coordinates, we describe scenes in a declarative manner
using spatial relations. Our intuition is that it is easier for an LLM
to reason about sentences such as ‚Äúthe lamp is on the table‚Äù or ‚Äúthe
chair is adjacent to the table‚Äù than about precise numeric values.
Our scene description language is embedded in Python. Using
Python allows us to capitalize on the expressivity of modern pro-
gramming languages: features such as loops, conditionals, arith-
metic, list comprehensions, and many useful built-in functions. It
also takes advantage of GPT4‚Äôs strong Python programming abili-
ties, likely due to the large amount of Python code in its training
corpus. Our language adds several domain-specific functions to
Python. The new functions are either (1) object constructors, (2)
relation functions, or (3) parameter setting functions. Appendix A
contains a full listing of the new functions added by our language.
Figure 3 shows an example program written in our language
and the scene layout that it produces when run through our layout
optimizer module (Section 6). This program describes a small Italian
restaurant. Below, we walk through the functionality of each part
of this program:
‚Ä¢ Line 1 sets the size of the scene in meters.
‚Ä¢ Lines 3‚Äì8 create tables and chairs in a double loop. Most
objects are created with an Object(description, width,
depth, height, facing) constructor. The width of an


--- Page 5 ---
Open-Universe Indoor Scene Generation
‚Ä¢
5
1 set_size(6.5, 4.0, 3.0)
2
3 tables = [Object("Vintage Table", 1.2, 0.8, 0.6, EAST) for _ in range(3)]
4 for table in tables[0:-1]: 
5    for i in range(4):
6       chair = Object("Durable Chair", 0.45, 0.45, 0.8) 
7       adjacent(chair, table, EAST if i % 2 == 0 else WEST) 
8       facing(chair, table) 
9 
10 aligned(tables, WESTEAST)
11
12 bar = Object("Bar", 1.0, 0.5, 1.1, EAST)
13 next_to_wall(bar, NORTH)
14 counter = Object("Counter", 3.0, 0.4, 1.1, NORTH)
15 adjacent(bar, counter, NORTH, WEST)
16
17 menu = Object("Menu Board", 0.5, 0.05, 0.85)
18 mounted_on_wall(menu, NORTH, 1.5, above=bar)
19
20 register = Object("Cash Register", 0.3, 0.3, 0.2, NORTH)
21 on(register, counter)
22
23 shelf = Object("Wall-mounted Shelf", 1.6, 0.4, 0.05)
24 mounted_on_wall(shelf, EAST, 1.6)
25
26 smaller = None
27 for i in range(4): 
28    scale = 1.0 + i * 0.3 
29    cheese = Object("Cheese Wheel", 0.2 * scale, 0.2 * scale, 0.15 * scale) 
30    on(cheese, shelf) 
31    if smaller is not None: 
32      adjacent(cheese, smaller, NORTH) 
33   smaller = cheese 
34
35 kitchen_door = Door("Kitchen Door", 0.75, 2.0, EAST)
36 next_to_wall(kitchen_door, NORTH, 0.5)
37 entrance_door = Door("Restaurant Entrance Door", 0.85, 2.0, NORTH)
38 next_to_wall(entrance_door, WEST, 1.0)
39 window = Window("Window", 2.4, 1.5, WEST, 0.8)
40
41 for painting in unique_objects(3, "Still Life Painting", 0.6, 0.04, 0.5):
42    mounted_on_wall(painting, NORTH, 1.7) 
Fig. 3. An example program in our declarative scene description language (left) and the object layout produced by running this program through our layout
optimizer (right). This scene depicts a small, cozy Italian restaurant.
object is a dimension perpendicular to the object‚Äôs front-
facing direction; the depth is a dimension along this direc-
tion. Relation adjacent constrains chairs to be next to their
corresponding tables (two chairs on each side of the table).
Relation facing orients each chair to face its corresponding
table.
‚Ä¢ Line 10: relation aligned constrains all table centers to be
on the same line running west to east.
‚Ä¢ Lines 12‚Äì15 create an L-shaped configuration of a bar and
counter. Relation adjacent(bar, counter, NORTH, WEST)
constrains the bar to be adjacent to the counter from the
north and aligned with the west side of the counter.
‚Ä¢ Lines 17‚Äì18 create a menu board and constrain it to be
mounted on the north wall above the bar, to be accessible
both by customers and by staff.
‚Ä¢ Lines 23‚Äì33 create a shelf mounted on the east wall and
constrain a row of cheese wheels of various sizes to be on
top of this shelf.
‚Ä¢ Lines 35‚Äì39 create doors and a window. We constrain the
kitchen door to be no more than 0.5m from north wall, and
the entrance door to be no more than 1m from the west wall.
‚Ä¢ Lines 41‚Äì42 create 3 paintings mounted on the north wall.
However, unlike chairs, tables and cheese wheels, we don‚Äôt
want the paintings to be represented by the same 3d model.
That is why we use a unique_objects constructor.
5
GENERATING SCENE PROGRAMS
Given freeform text input, our system must synthesize scene pro-
grams like the one in Fig. 3. We use a large language model (LLM)
to perform this task. Going from a high-level, natural language de-
scription to a Python program is a challenging task. LLMs have a
remarkable ability to perform this task, but they are not perfect. In
our early experiments tasking an LLM to generate scene description
programs directly from text prompts, it tended to produce undesir-
ably short/sparse programs and make some errors (we taxonomize
the types of these errors in Section 6.3). To reduce the prevalence of
errors and produce richer scene description programs, we found it
helpful to split the program generation task into a series of smaller
sub-tasks (an instance of chain-of-thought prompting [Wei et al.
2023]):
(1) Generate a detailed natural language description of the com-
plete scene
(2) Generate the code to declare all objects in the scene (ensur-
ing there are enough objects to achieve the desired room
fullness)
(3) Generate the code to specify all object relations
Task (1) is better aligned with the LLM‚Äôs training data, so it is more re-
liable than directly generating code. Tasks (2) and (3) become easier
because they can refer to a concrete natural language description‚Äî
essentially, they are translation tasks, rather than synthesis tasks.
Figure 4 shows a schematic of this pipeline. The remainder of this
section describes each of these stages in more detail; the complete


--- Page 6 ---
6
‚Ä¢
Rio Aguina-Kang, Maxim Gumin, Do Heon Han, Stewart Morris, Seung Jean Yoo, Aditya Ganeshan, R. Kenny Jones, Qiuhong Anna Wei, Kailiang Fu, and Daniel Ritchie
Inputs
Scene description
‚ÄúA dining room.‚Äù
(Optional)
Room size, fullness
WxH, p% 
The room is 6 x 5 x 3 in
size.
Room fill percentage: 45%.
1. There is a varnished oak
dining table of size 2.0 x 
1.0
x 0.75. The dining table is
centered in the room,
providing ample space for
movement around it. The
table is facing north.
2...
Natural Language Description
LLM
set_size(6.0, 5.0, 3.0)
room_fill_percentage = 45
table =
Object('table_varnished_o
ak_dining', 2.0, 1.0, 0.75,
NORTH)
painting = 
Object('painting_abstract_l
arge_canvas', 2.0, 0.01, 
1.4, NORTH)
...
Object Declarations
LLM
...
# Using relation 5, 
surround.
surround(chairs, table)
for chair in chairs:
facing(chair, table)
# Using relation 2, 
mounted_on_wall.
mounted_on_wall(painting, 
SOUTH, 1.0)
...
Object Relations
LLM
Fig. 4. Our scene description program synthesizer proceeds in three steps, each of which uses a large language model. First, the LLM is asked to generate a
natural language description of all the objects in the scene, along with how and why they are spatially related to one another. Then, a sequence of two LLMs
translate this description into code which declares objects and relations, respectively.
prompt templates for each can be found in the supplemental mate-
rial. In Section 9, we conduct an ablation study to show the value of
this task decomposition.
5.1
Describing the Scene in Natural Language
The first stage of the program synthesizer tasks an LLM with de-
scribing the scene to be generated in natural language. First, if the
user has not provided an input room size or target object density,
the LLM is first asked to produce values for those quantities which
are appropriate for the input text prompt. The texture of the walls
and floors are described as well, to be retrieved later in the pipeline.
Then, the LLM is asked to list and thoroughly describe all the objects
in the scene: the type of object, its size, and any salient details about
its appearance. For each object, the LLM also outputs a description
of how that object is situated in the scene in relation to other objects.
Throughout, the LLM is asked to explain its reasoning.
To guide the LLM, our prompt template for this stage includes
two in-context examples of the type of output we expect in this
stage. The first in-context example describes an artist‚Äôs one room
apartment; the second describes a typical a dining room. These
two examples span a variety of object types and arrangements; our
results in Section 9 show that the system generalizes beyond these
two examples to synthesize an even wider variety of scenes.
5.2
Declaring Objects
The next stage of the program synthesizer tasks an LLM with produc-
ing Python code that declares all the objects in the scene. This stage
receives the natural language description output by the first stage as
its input. In addition to choosing the most appropriate constructor
for each object (i.e. Object, objects, or unique_objects) and the
relevant arguments for those constructors (description, dimensions,
and facing direction information), it also produces its estimate of
a ‚Äòcategory label‚Äô for the object (e.g. for ‚Äòa sleek dark wood dining
table,‚Äô the category label might be ‚Äòtable‚Äô). This category label is used
by the later object retrieval and orientation stages of the pipeline.
The prompt template for this stage of the program synthesizer
also includes two in-context examples to help the LLM produce
code in the correct output format. These in-context examples show
object declaration code for the same two scenes described in the first
stage‚Äôs in-context examples (the artist‚Äôs apartment and the dining
room).
Achieving target object density. Specifying the target occupied
room floor area in the input ishelpful for encouraging the LLM to
generate the right amount of objects, but it does not guarantee that
the LLM will produce output that satisfies this target. Thus, this
stage includes logic that checks if the target occupied area has been
achieved by the sizes of the generated object declarations; if not, it
invokes another LLM to generate more objects (both their natural
language descriptions and their object declaration code). This step
is iteratively repeated until the actual percentage of occupied room
floor area meets the set target object density.
5.3
Specifying Object Relations
The final stage of the program synthesis pipeline receives all previ-
ous pipeline outputs and completes the scene description program
by generating code describing object relations. This task boils down
to translating the free-text descriptions of object arrangement in
the Stage 1 output into calls to appropriate relation functions in our
DSL (which refer to the appropriate objects declared by Stage 2).
The prompt template for this stage also includes two in-context
examples. These in-context examples contain relation specification
code based on the same two scenes that the previous two stages used
as in-context examples. We designed these in-context examples to
demonstrate certain potentially non-obvious ways to use relations
to achieve layout goals (e.g. using two next_to_wall relations to
place an object in a corner).
6
SCENE LAYOUT OPTIMIZATION
In this section, we describe how a scene program is converted into
an object layout, i.e. a list of objects with locations and orientations.
This process consists of two stages. First, the Python interpreter con-
verts the program into a geometric constraint satisfaction problem,
where variables are object positions and object directions and con-
straints are derived from relation functions. Second, the constraint
problem is solved using an algorithm based on gradient descent.
Because LLMs are not perfect programmers, scene programs can
contain errors. In the last part of this section, we describe how the
system handles different types of errors.
6.1
Converting Scene Programs into Constraint Problems
First, the Python scene program is executed with a Python inter-
preter. As described in Section 4, the new functions in the scene DSL


--- Page 7 ---
Open-Universe Indoor Scene Generation
‚Ä¢
7
include object constructors and relation functions. Standard object
constructors define variables of the constraint problem. Relation
functions define constraints of the constraint problem. Door and
window constructors define both variables and constraints.
While we designed the scene DSL to be most convenient for
LLMs to use, we designed the constraint set to be most simple
mathematically. Each relation function within the DSL is translated
into one or more of ten defined constraints: ON, NEXTTOWALL, HEIGHT,
ADJACENT0, ADJACENT1, ADJACENT2, CEILING, ABOVE, ALIGNED and
FACING. This translation is mostly straightforward. For example,
mounted_on_wall(a, wall, height_above_ground, above) is
translated into
NEXTTOWALL(a, wall, 0.0),
HEIGHT(a, height_above_ground),
ABOVE(a, above, wall).
The non-obvious cases are relation functions that take list argu-
ments: aligned and surround. Relation function aligned(list,
direction) is desugared into a list of len(list)-1 constraints:
ALIGNED(list[0], list[1], direction)
ALIGNED(list[1], list[2], direction)
...
Relation functions surround(objects, centerobj) are handled
last. Surrounding objects in the objects list are processed one-by-
one. Each object is constrained to be adjacent to the centerobj
from the side of the centerobj that has the most free space available,
and facing the centerobj. This process respects other adjacencies
and walls.
In addition to the relational constraints, we also add default con-
straints: WITHINBOUNDS and NOOVERLAP. For every object a we add
WITHINBOUNDS(a) to ensure that the object stays within the bounds
of the room. For every unordered pair of distinct objects a, b we add
NOOVERLAP(a, b, distance_x, distance_z). Here distance_x
and distance_z are zeros if neither of objects a, b is a door or a
window, and some nonzero parameter if a or b is door or a window.
The meaning of these distance arguments is to create ‚Äôauras‚Äô for
doors and windows that no other objects can overlap. This is needed
to ensure that doors can be opened, and windows are not obstructed
by furniture. There is one exception to this rule: we allow wide flat
objects such as rugs to overlap with anything, to support the case
(common in real furniture arrangements) where a part of a furniture
object stands on a rug.
6.2
Solving the Constraint Problem
The purpose of the layout optimizer is to find a vector of object po-
sitions and object directions that satisfies all the constraints. Given
the geometric nature of our constraints, it is natural to formulate
our constraint solving problem as an optimization of a (mostly)
differentiable function. The only non-differentiable constraint that
we have is a FACING constraint (since orientations are restricted to
the four cardinal directions); we address this constraint separately.
We design differentiable loss functions for each constraint (ex-
cluding FACING), such that a constraint is satisfied if and only if
its loss is zero. For example, HEIGHT(a, height) is the squared
difference between object‚Äôs a bottom vertical coordinate and the
height parameter, and WITHINBOUNDS(a) is the sum of squares of
Without Repel Forces
With Repel Forces
Fig. 5. Adding repel forces to layout optimization allows objects to be ap-
propriately spaced without exhaustively specifying explicit relations.
object‚Äôs a linear extensions beyond the scene cuboid. We refer the
reader to Appendix B for the full list of constraint losses. Our system
finds a solution to a constraint problem by initializing objects in
random positions and minimizing the sum of constraint losses with
gradient descent. Since the initial configuration is random, different
runs of optimizer can produce different layouts (see Figure 10).
Custom gradients for non-overlap constraints. For constraints of
the form NOOVERLAP(a,b), the natural loss formulation is a mea-
sure of the overlap of the bounding boxes of a and b, and this is
indeed what we use. However, such functions are flat if one cuboid
is inside the other along each axis (for example, if two cuboids
overlap in a shape similar to the + sign, as in the inset figure).
Hence, the gradient can be zero when
the loss is not zero, and gradient de-
scent fails to minimize the loss. To solve
this issue, we define the gradient of the
NOOVERLAP(a,b) constraint to be pro-
portional to the vector connecting the
centroids of a and b, where the magni-
tude of this vector is equal to the mini-
mum side length of the cuboidal overlap
region between a and b.
Repel forces. Most scene description programs in our language are
underspecifications of scenes: there exist many object layouts which
can satisfy the specified constraints. Are there any priors we can
use to inform whether any of these layouts are better than others?
One reasonable assumption is that since object adjacencies have
such a strong perceptual impact on the scene (causing objects to be
perceived as part of some larger group), the optimized layout should
not have adjacencies that are not explicitly specified in the program.
We realize this assumption using repel forces, similar to repels in
force-directed graph drawing [Battista et al. 1998] (implementation
details in Appendix B). Repel forces make the distribution of objects
in a scene more balanced but do not push away objects that should
be together. This makes a notable difference in the plausibility of
optimized scene layouts; see Figure 5. However, gradient descent
with repel vectors added to the gradient does not necessarily con-
verge to a solution of the original constraint problem. We describe
the solution to this issue in the next paragraph.


--- Page 8 ---
8
‚Ä¢
Rio Aguina-Kang, Maxim Gumin, Do Heon Han, Stewart Morris, Seung Jean Yoo, Aditya Ganeshan, R. Kenny Jones, Qiuhong Anna Wei, Kailiang Fu, and Daniel Ritchie
Determine object orientations. In the scene description language,
an object can be specified to face either one of four cardinal direc-
tions or another object. If an object faces some cardinal direction, its
direction is known up-front and does not change. Situations where
one object faces another (for example, where a sofa faces a TV) are
trickier. To know the final direction, the system must know final
object positions. However, we cannot set directions after position
optimization: if we rotate an object, its bounding box would change,
which could make the object overlap with other objects or violate
other constraints. Thus, object directions should be optimized jointly
with object positions. This complicates our gradient descent scheme,
because direction is a discrete variable.
We developed a simple, multi-stage optimization approach that
solves this problem and also gets around the convergence issue with
repel forces mentioned at the end of the previous paragraph:
(1) Randomly initialize object positions and non-fixed object
directions
(2) Perform gradient descent with repel forces added to the
gradient
(3) Set non-fixed objects directions according to the current
object positions
(4) Perform gradient descent again without repel forces
Object positions do change in the second descent phase, so in the
end, some object directions may not agree with object positions.
However, the second descent phase usually moves objects only
slightly, and the mismatch between object positions and directions
happens very rarely in practice.
6.3
Error Correction
As mentioned in Section 5, LLMs tasked with synthesizing scene
description programs can sometimes produce code with errors. Our
decomposition of the program synthesis task into stages helps re-
duce these errors (as we show in Section 9), but it does not com-
pletely eliminate them. Since these errors typically affect only a
small part of an otherwise-valid program, the layout optimizer in-
cludes mechanisms for automatically fixing some of these errors to
avoid throwing out the entire LLM-generated program.
We taxonomize the types of errors the program synthesizer makes
into four classes:
‚Ä¢ Hallucination: calling functions which are not in our lan-
guage or referring to objects which do not exist in the scene
(e.g. below(footrest, desk), or on(lamp, table) when
lamp does not exist)
‚Ä¢ Misuse: incorrectly using a function in the language (e.g.
incorrect argument type, missing arguments)
‚Ä¢ Contradiction: creating relations which are provably in di-
rect conflict with one another (e.g. next_to_wall(statue,
NORTH) and next_to_wall(statue, SOUTH))
‚Ä¢ Unsatisfiability: creating a set of relations which do not
have any obvious conflicts but which cannot be jointly sat-
isfied (i.e. the layout optimizer converges to non-zero loss).
The layout optimizer catches the first two types of errors by
running the Python interpreter, catching any raised exceptions,
deleting the line which caused the exception, and trying to execute
the program again. We classify an exception as a hallucination if it
contains the string ‚Äúis not defined‚Äù and as a misuse otherwise.
After the program is successfully executed, the layout optimizer
identifies contradiction-type errors by searching for the following
patterns (identifiable as subgraphs in the overall constraint graph):
(1) An object is adjacent to, stands on top of, or faces itself.
(2) An object is next to two opposing walls and is not large
enough to span the room dimension.
(3) Object a is adjacent to object b from direction d, and also b
is adjacent to a from any direction other than the opposite
of d.
(4) Object a stands on top of object b, and also b is (horizontally)
adjacent to a.
(5) Object a is next to wall d, but some other object is adjacent
to a from direction d.
(6) The total length of objects adjacent to a from some direction
is more than the corresponding linear size of a.
The layout optimizer handles detected contradictory subgraphs by
deleting the constraint in the subgraph which is declared last in the
scene program.
If the system does not reach a near zero loss after 10 rounds of
layout optimization from random initial conditions, we consider
the constraint problem to be unsatisfiable (the fourth type of error).
In this case, the system backtracks to the third stage of the pro-
gram synthesizer and re-generates the relations part of the scene
description program.
7
RETRIEVING 3D OBJECTS
Given a generated scene description program, the system must re-
trieve relevant 3D object meshes for each object declared in the pro-
gram. In this section, we describe our object retrieval pipeline specif-
ically designed for retrieving objects from massive, un-annotated
3D asset datasets. Figure 6 shows a schematic overview of this stage.
Given an object‚Äôs natural language description ùëá(automatically-
generated by the LLM-based program synthesizer), our goal is to
retrieve a list of relevant 3D meshes from our 3D asset dataset D. As
we want to match a textual description ùëáto a dataset of 3D assets,
a natural approach is to leverage large pretrained Vision Language
Models (VLMs) such as CLIP [Radford et al. 2021]. VLMs enable
measuring similarity between the text ùëáand renderings of the 3D
meshes by generating their embeddings and measuring their cosine
similarities. Therefore, we can retrieve a list of ùëòrelevant 3D meshes
Dùëá
ùëòfor the given object description ùëáas follows:
Dùëá
ùëò= TOPùëò(ùëìùë°(ùëá), Dùê∏)
(1)
where ùëìùë°is the text encoder of the VLM and TOPùëò(ùë•,ùëå) returns
entries ùë¶‚ààùëåwith the top-k highest cosine similarities to ùë•. Dùê∏=
{ùê∏(ùë•)|ùë•‚ààD} is a database of VLM embeddings, one per object in
D. The per-object embedding function ùê∏(ùë•) is defined as rendering
12 views of the object ùë•, computing the VLM image embedding of
each, and averaging those embeddings. We create this embedding
database as a preprocess. Despite the massive scale of the database,
performing this retrieval step takes only 0.5 seconds, as we employ
approximate nearest neighbor search using FAISS [Douze et al. 2024].


--- Page 9 ---
Open-Universe Indoor Scene Generation
‚Ä¢
9
SigLIP
Text Encoder
Embedding Database
Object category
Preprocess
3D Object Database
...
...
Render
...
SigLIP Image Encoder
...
ùúá
KNN lookup
1.
.
2.
.
3.
.
4.
5.
.
...
Re-rank
1.
2.
.
3.
4.
.
5.
.
...
Filter
Wrong 
Category
1.
.
2.
3.
4.
.
5.
.
...
Final Result
‚Äúa tv stand‚Äù
Object description
"a tv stand that is sleek
  black and modern"
Objects
 on top
Fig. 6. Our pipeline for open-universe 3D object retrieval. As a preprocess, we compute embeddings for each object in our 3D mesh database using a vision
language model (VLM). Given a description and category of an object (both specified in the LLM-generated scene program), our system finds the ùëònearest
neighbors of the text description‚Äôs VLM embedding in our database. These initial retrieval results are then re-ranked to prioritize objects with the correct
category and further filtered to remove meshes which are the wrong category or which consist of multiple objects.
The memory requirements of building Dùê∏are also limited, as we
only need to store a single embedding vector for each 3D asset.
While this retrieval algorithm is a good starting point, we found
it to be insufficient in practice. Due to the large, unstructured, and
noisy nature of massive 3D asset datasets, we observe and correct for
some frequently-occurring failure cases, which we describe below.
7.1
Retrieving the Correct Category
We observe that often the retrieved 3D mesh might be visually be
similar to the text prompt but functionally be of a different category.
As the text prompt contains information about both the object cate-
gory (e.g. chair, table etc.) and the object style (e.g. wooden, modern
etc.), retrieved objects can sometimes have a style matching the
text description but a different category. To solve this problem, we
use the category attribute ùê∂that the program synthesizer produces
for each object and enhance the retrieval with a category-aware
reranking strategy. Specifically, we found the following recipe to
work well. First, we retrieve a large set (ùëò= 100) of 3D objects from
the embedding database Dùê∏, as in Equation 1. Then, we re-rank the
retrieved objects ùë•based on a category-aware embedding distance
given by
ùëë(ùëìùë°(ùëá), ùê∏(ùë•)) + ùúÜùëë(ùëìùë°( ÀÜùê∂), ùê∏(ùë•))
(2)
where ÀÜùê∂is the text prompt ‚ÄúA photo of a ùê∂‚Äù, where ùê∂is the category
attribute, and ùúÜis weighting coefficient (we use ùúÜ= 1).
For indoor scenes, we have found that retrieving an object of
the wrong category can often seriously disrupt the plausibility of
the scene. Therefore, we augment our category aware re-ranking
by leveraging a multimodal LLM [GPT-4 2023] to select an object
of the correct category. Specifically, we provide the LLM with an
image of an ùëõ√ó ùëõarray of renders of the top ùëõ√ó ùëõretrieved objects,
tasking it with identifying which images are/are not of the category
ùê∂. This step is applied sequentially on the next ùëõ√ó ùëõretrievals and
so on, until the system finds ùëöobjects judged to be of the correct
category (ùëö= 1, except in the case where the program dictates that
unique meshes be retrieved for a set of objects). Note that while one
could apply this category filtering directly on the initial retrieved
set of objects, applying the category-aware re-reranking first means
that fewer filter steps (and therefore fewer costly LLM API calls) are
required to find ùëöobjects of the correct category.
7.2
Retrieving Only Single Objects
Another mode of failure occurs when the retrieved mesh contains
additional secondary objects along with the desired object. Objects
such as tables and TV stands are often modeled alongside adjacent
or supported objects such as chairs and TVs, respectively. Compar-
ing VLM text embeddings to embeddings of images of such objects
often produces high similarities despite the presence of the addi-
tional secondary objects. To solve this problem, we again employ a
multimodal LLM to perform multi-object filtering. Similarly to our
category filtering step, we provide the LLM with a set of object
renders and task it with filtering out objects based on two criteria:
(i) if there are other objects on top of the main object of category
ùê∂(e.g. a TV stand with a TV on it) and (ii) if the object has other
objects around it (e.g. a table with chairs around it). The step is
also applied sequentially on the top ùëõ√ó ùëõretrievals until we have
retained ùëövalid object retrievals.
7.3
Matching the Specified Object Size
Not all 3D meshes which match the object category and description
specified in the program are good candidates, because they must
also reasonably match the object size specified in the program (e.g.
a long, thin mesh will not be a good candidate for an object which
is specified as being small and squarish). Thus, we also filter out
meshes whose bounding box aspect ratios are too dissimilar from
those specified by the object dimensions in the generated program.
Given the bounding boxes ùêµùëüof a candidate mesh and ùêµùëùof the ob-
ject as specified in the program, we compute the minimal Bounding
Box Distortion (BBD) as follows:
ùêµùêµùê∑(ùêµ, ùêµùëù) =
 
‚àëÔ∏Å
ùëñ‚àà{ùë•,ùë¶,ùëß}
 log ùêµùëñ
ùêµùëù
ùëñ

!
(3)
ùëöùêµùêµùê∑= min
ùëÖ‚ààR ùêµùêµùê∑(ùëÖ¬∑ ùêµ, ùêµùëù)
(4)


--- Page 10 ---
10
‚Ä¢
Rio Aguina-Kang, Maxim Gumin, Do Heon Han, Stewart Morris, Seung Jean Yoo, Aditya Ganeshan, R. Kenny Jones, Qiuhong Anna Wei, Kailiang Fu, and Daniel Ritchie
SigLIP
Text Encoder
SigLIP Image Encoder
BBox from 
scene program
Retrieved object
Match upright 
orientation
Render
‚ÄúThe front of a painting‚Äù
0.4
0.6
0.5
0.7
‚ÄúWhich image shows the 
front of the painting?‚Äù
LLM
‚ÄúLeft‚Äù
Fig. 7. Our pipeline for orienting retrieved 3D meshes. The system first tries a discrete set of rotations to match the upright orientation of the mesh‚Äôs bounding
box to that of the bounding box specified in the object layout. Then, a VLM is used to assess how similar each of four orthogonal views of the mesh are to the
phrase ‚Äúthe front of a ùê∂,‚Äù where ùê∂is the object‚Äôs estimated category. The two most similar views are then given to a multimodal LLM to decide which is the
best choice for the front face of the object.
where ùëÖis a rotation from the set of Euler angles R = { (0,0,0),
(0,90,0), (90,0,0), (90,0,90) }, and ùêµ‚àó
ùëñrefers to the volume normalized
side length along the axis ùëñ. That is, we measure the distortion on
whichever of these axis-aligned rotations best aligns ùêµùëüwith ùêµùëù.
This small set of candidate rotations is typically sufficient, because
artist-created 3D meshes are typically modeled with their up di-
rection aligned with one of the world coordinate axes. Candidate
objects with a minimal distortion larger than a threshold ùúè= 0.4 are
discarded (unless this would retain fewer than ùëöobjects, in which
case the lowest-loss ùëöobjects are kept).
8
ORIENTING RETRIEVED OBJECTS
The final problem we need to solve for end-to-end text to indoor
scene generation is correctly orientating the retrieved objects in the
scene. Specifically, the system must determine where the ‚Äòfront‚Äô of
an object faces, which can be critical for objects such as chairs and
bookshelves. Full ùëÜùëÇ(3) canonicalization of 3D objects is a challeng-
ing task, especially without assuming any supervised training data.
Some unsupervised canonicalization methods exist [Sajnani et al.
2022], but they are category-specific; in our case, there are no fixed
category labels for our retrieved meshes. Fortunately, as mentioned
in the previous section, most artist-created meshes are modeled
with their up direction and front direction aligned with world coor-
dinate axes, allowing us to reduce our problem from a continuous
search over ùëÜùëÇ(3) to a six-way classification problem: which of the
six faces of a mesh‚Äôs axis-aligned bounding box corresponds to its
‚Äòfront.‚Äô In this section, we present a simple, training-free approach
for solving this problem using pre-trained models. Figure 7 shows
an overview of our approach.
Our first goal is to ensure that the object is in upright position.
Once the object is in upright position, the problem further reduces to
detecting which of the four vertical faces of the object corresponds
to its ‚Äòfront‚Äô. To this end, we use the bounding box distortion (BBD)
metric from Equation 3. Empirically, we find that the majority of 3D
meshes are modeled in a y-up coordinate system, which our system
also uses. Thus, we only rotate the object if doing so would signifi-
cantly improve BDD loss w.r.t. the y-up bounding box specified in
the scene program. Specifically, we check if either of the rotations
(90, 0, 0) or (90, 0, 90) result in a BDD less than the minimum of
(0, 0, 0) and (0, 90, 0) by a margin of 1.0; if so, we re-orient the mesh.
At this point, we may be done: if the program synthesizer did not
declare the object with a facing argument in the generated scene
program, then we assume that the object does not have a unique
front-facing direction and does not need to be further oriented (e.g.
round tables).
If the object does have the facing argument, we next convert the
four-way classification problem into a two-way classification prob-
lem. For objects with a non-square footprint, two of the four options
can be directly rejected based on the BBD metric: for example, for
rectangular couches, the side faces can be rejected as mapping them
to the ‚Äòfront‚Äô leads to high BBD loss. Specifically, if the BBDs for the
rotations (0, 0, 0) and (0, 90, 0) differ by more than 0.4, we reject the
side faces. For objects with more square footprints, we use a VLM to
reject two of the four options. Specifically, we render the object from
the four cardinal directions, embed each of these renders with the
VLM‚Äôs image encoder, and measure the cosine similarity between
each of these embeddings and the embedding of the text ‚ÄòThe front
of a ùê∂‚Äô, where ùê∂stands for the object category (specified by the
LLM). We discard the directions with the lowest two similarities.
Finally, we solve the remaining two-way classification problem by
leveraging a multimodal LLM to perform visual question answering.
Specifically, we provide the LLM with the two remaining renders
and ask ‚ÄòWhich image shows the front of the a ùê∂?‚Äô.
In Section 9, we compare this multi-step orientation pipeline with
several simpler alternatives (including approaches using only a VLM
and only a multimodal LLM), showing that it performs best.


--- Page 11 ---
Open-Universe Indoor Scene Generation
‚Ä¢
11
‚ÄúA university dorm‚Äù
‚ÄúA boss‚Äôs office‚Äù
‚ÄúA vampire‚Äôs room‚Äù
‚ÄúA medieval knight‚Äôs room‚Äù
Fig. 8. Our method is capable of synthesizing a wide variety of indoor scenes that conform to input text prompts.
9
RESULTS AND EVALUATION
In this section, we evaluate our scene synthesis system by using it
to generate scenes in response to a variety of inputs and by com-
paring it to other scene generation methods, both closed- and open-
universe.
Implementation details. We use GPT4 and GPT4V for all language
generation and visual question answering tasks throughout the sys-
tem [GPT-4 2023]. For joint text-image embedding, we use the SigLIP
vision-language model [Zhai et al. 2023b]. For object retrieval, we
use the Objaverse dataset [Deitke et al. 2022a] as well as multi-view
renderings of Objaverse objects provided by the ULIP dataset [Xue
et al. 2023].
9.1
Qualitative Results
Figs. 1 and 8 show scenes generated by our system in response to
different text prompts. Our system is able to interpret prompts de-
scribing typical indoor scenes (e.g. ‚Äúa bedroom‚Äù), rooms for specific
purposes (e.g. ‚Äúa dining room for one‚Äù), different styles of interiors
(e.g. ‚Äúan old-fashioned bedroom‚Äù), and non-residential indoor spaces
(‚Äúa high-end mini restaurant‚Äù). It can also imagine scenes that don‚Äôt
exist in the real world (e.g. ‚Äúa vampire‚Äôs room‚Äù with a coffin and a
collection of occult tomes; ‚Äúa medieval knight‚Äôs room‚Äù). Our system
also supports user specification of the desired room size and object
density/fullness; in Fig. 9, we show examples of how the system
responds to these optional inputs.
Because we break the scene synthesis problem into declarative
program synthesis followed by layout optimization, it is possible to
optimize multiple layouts for the same program, producing multiple
design variations. Fig. 10 shows an example of this process. Objects
placed along walls (such as paintings and desks) are free to slide
along those walls, in some cases exchanging positions (e.g. the desk
can appear on both sides of the door).
9.2
Closed-Universe Scene Synthesis Comparison
Here we evaluate how well our system performs on a closed-universe
scene generation task when compared to prior methods for this
problem which learn from 3D scene data. Specifically, we compare
against the following methods:
‚Ä¢ ATISS [Paschalidou et al. 2021]: a recent autoregressive
Transformer-based generative model of indoor scenes.
‚Ä¢ DiffuScene [Tang et al. 2023]: a recent denoising diffusion-
based generative model of indoor scenes.
"A bedroom"
+ set room size to 3m x 6m
"A NYC apartment"
+ increase fullness by 15%
Fig. 9. When the user specifies the desired room size or target object den-
sity as part of the input, our system appropriately adjusts its output. Top:
controlling room size; Bottom: controlling room fullness.
Fig. 10. Given a single scene description program, we can run the layout
optimizer multiple times to produce stochastic variations on the same scene.
We evaluate each of these methods on generating bedrooms, living
rooms, and dining rooms (three commonly-occurring room types
in closed-universe scene generation work). We direct our method
to generate object layouts of these types by providing it with text
prompts of the form ‚ÄúA bedroom.‚Äù
To compare the object layouts generated by these different meth-
ods, we conducted a two-alternative forced-choice perceptual study.
We recruited 35 participants from a population of university stu-
dents. Each participant was shown a series of 45 comparisons, where
each comparison contained a room type label (bedroom, living room,
or dining room), images of two scenes, and a question asking them


--- Page 12 ---
12
‚Ä¢
Rio Aguina-Kang, Maxim Gumin, Do Heon Han, Stewart Morris, Seung Jean Yoo, Aditya Ganeshan, R. Kenny Jones, Qiuhong Anna Wei, Kailiang Fu, and Daniel Ritchie
Ours vs.
Bedroom
Living
Dining
Average
ATISS
76%
74%
86%
79%
DiffuScene
75%
79%
89%
81%
Table 1. Results of a two-alternative forced-choice perceptual study com-
paring scenes generate by our system to those generated by two existing
systems for closed-universe scene generation. The scenes our method gen-
erated were largely preferred over those from alternative approaches across
typical indoor room types.
Bedroom
Living Room
Dining Room
Ours
ATISS
DiffuScene
Fig. 11. Comparing closed-universe scene layouts generated by our system
to those generated by two existing closed-universe scene generative models
(zoom in to read object text labels)., Layouts generated by our method are
more detailed and devoid of object intersection artefacts.
to choose which scene they thought was a more realistic instance
of that type of room.
Table 1 shows the results of this study, and Fig. 11 shows some
of the object layouts generated by each method. Participants vastly
preferred the object layouts produced by our method compared with
ATISS (79% overall preference rate) and DiffuScene (81% overall
preference rate). While we find this trend is consistent across the
three room types we used, the gap between our approach and these
alternatives is most pronounced for dining rooms, where the objects
layout we produced were preferred at rates of 86% and 89% over
ATISS and DiffuScene respectively. As seen in the last column of
Fig. 11, our method captures relations important to scene fidelity
(e.g. surrounding a dining table with chairs), all the while avoiding
object overlaps and clutter that mar the scenes produced by the
other two approaches.
9.3
Open-Universe Scene Synthesis Comparison
We next evaluate our system‚Äôs ability to generate open-universe
scenes. To the best of our knowledge, there is no prior work which
solves this exact problem. Thus, we compare against the next best
thing: an existing method that uses LLMs for scene synthesis.
Specifically, we compare against LayoutGPT [Feng et al. 2023].
LayoutGPT was originally only evaluated in the closed-universe
setting; we adapt it to the open-universe setting by modifying its
prompt to remove references to fixed sets of room and object types
and by providing it the same in-context examples that our method
sees (converted into LayoutGPT‚Äôs scene representation format). To
convert LayoutGPT‚Äôs generated layouts into full 3D scenes, we use
the same object retrieval and orientation modules as in our system.
We test how well the two methods can generate scenes in response
to a range of different types of text prompts, ranging from simple
to more complex/subtle:
(1) Basic: basic room types such as ‚Äúa bedroom.‚Äù
(2) Completion: prompts that describe a subset of a basic scene
and ask the system to complete it, e.g. ‚Äúa living room with a
sofa, tv, and a coffee table.‚Äù
(3) Style: basic room types with style descriptors, e.g. ‚Äúa mini-
malist living room.‚Äù
(4) Activity: rooms that must accommodate some specific ac-
tivity, e.g. ‚Äúa musician‚Äôs practice room.‚Äù
(5) Fantastical: fantastical, outlandish, or whimsical rooms
that would not exist in reality, e.g. ‚Äúa wizard‚Äôs lair.‚Äù
(6) Emotion: rooms which should evoke specific emotions, e.g.
‚ÄúA lonely dark jail cell.‚Äù
We have created 59 prompts across these 6 types; a complete listing
of all prompts can be found in the supplemental material.
To compare how well the different methods fare on these prompts,
we conduct a two-alternative forced-choice perceptual study, pitting
our method against LayoutGPT. We recruited 24 participants from
a population of university students. Each participant was shown a
series of 50 comparisons, where each comparison contains a text
prompt, images of two scenes, and a question asking them to choose
which scene they thought was better (taking into account overall
scene plausibility and appropriateness for the prompt).
Table 2 shows the results of this experiment. In general, partici-
pants preferred our method‚Äôs scenes over those from LayoutGPT.
The largest margin between our system and LayoutGPT occurred for
prompts in the Style category. Since both systems used the same ob-
ject retrieval method, this difference is not attributable to the objects
in one condition having more stylistically-appropriate appearance;
rather, our system does a better job of interpreting which types
of objects should be in a scene and how they should be arranged
to satisfy stylistic goals. The Fantastical prompt category, with its
unusual prompts, proved to be challenging for both methods, with
no clear winner emerging.
Fig. 12 shows some of the scenes generated by each method in this
experiment. To demonstrate the value of our object retrieval mod-
ule, we also produce variants of scenes generated by our method
where the 3D meshes used are retrieved using a naive retrieval
method (only the initial VLM-based KNN retrieval step from our full
pipeline, without re-ranking or filtering). LayoutGPT, as it directly
generates numerical coordinates for object locations, suffers from
frequently interpenetrations between objects. Our method avoids
these errors by construction. Our full retrieval pipeline also helps
avoid some erroneous mesh retrievals (e.g. for the bookshelf in the
second column). The supplemental material contains more exam-
ples of objects from these scenes where our full retrieval pipeline
retrieves a mesh of the appropriate category but the naive approach
does not. Across all the scenes, for every three out of 100 objects,


--- Page 13 ---
Open-Universe Indoor Scene Generation
‚Ä¢
13
Basic
Completion
Style
Activity
Fantastical
Emotion
Average
Ours vs. LayoutGPT
66%
64%
76%
60%
51%
66%
65%
Table 2. How often open-universe scenes generated by our method are preferred to those generated by LayoutGPT in a two-alternative forced-choice
perceptual study (higher is better). We report results for the different types of prompts in our evaluation set as well as overall results. Our system is preferred
over LayoutGPT for all prompt types except Fantastical, where there is no clear preference.
LayoutGPT
Naive
Retrieval
"An american living room"
Ours
"A chemical lab"
"A modern bedroom"
Fig. 12. Comparing open-universe scenes generated by our system to those generated by LayoutGPT and to scenes generated by an ablation of our system
using a naive object retrieval method. LayoutGPT produces layouts with many overlapping objects; the naive retrieval baseline sometimes retrieves unusual
and undesirable meshes for some objects.
Synth. & Trans.
Lines‚Üë
H‚Üì
M‚Üì
C‚Üì
U‚Üì
Combined (No stage 1)
36.2
1.16
24.28
49.71
21.97
Ours (separated)
49.4
9.73
16.22
31.63
14.6
Table 3. Evaluating how separating synthesis and translation into different
LLM queries affects the complexity of the generated scene programs (Lines)
as well as the rates at which the types of errors described in Section 5 occur
(H = Hallucination, M = Misuse, C = Contradiction, U = Unsatisfiability).
Our full pipeline improves all metrics but one (Hallucination).
our method retrieved a correct-category mesh whereas the naive
method did not.
9.4
Ablation Studies & Other Evaluations
Here, we discuss several additional experiments we performed to
evaluate the performance of individual components of our system
in isolation.
Scene program synthesis. In Section 5, we discussed the benefits
of splitting the LLM-based program synthesizer into stages which
first generate a detailed natural language description of the scene
and then translate that description into code. Here, we empirically
demonstrate those benefits. For the prompts from the perceptual
study in the previous section, we generate scenes using our full
program synthesizer and a variant without the natural language
description stage, i.e. in this variant, the LLM must synthesize the
scene and translate it to code at the same time. To assess the com-
plexity of the generated scene programs, we measure the average
number of lines per scene. We also measure the frequency at which
the four types of errors described in Section 6.3 occur. Since scenes
have a different number of objects & relations (and thus a different
number of chances to make errors), rather than report the average
error rate per scene, we instead report the number of errors per
1000 objects.
Table 3 shows the results of this experiment. Using our full
pipeline results in more complex scene programs and leads to a
reduction in the rates of all error types except hallucinations. The


--- Page 14 ---
14
‚Ä¢
Rio Aguina-Kang, Maxim Gumin, Do Heon Han, Stewart Morris, Seung Jean Yoo, Aditya Ganeshan, R. Kenny Jones, Qiuhong Anna Wei, Kailiang Fu, and Daniel Ritchie
Fig. 13. Plotting the retrieval precision (left) and category accuracy (right)
of different ranking schemes for our open-universe object retrieval module
(x axis is the number of top ùëòobjects considered). Our weighted re-ranking
approach preserves high category accuracy while incurring only a small hit
to precision.
higher rate of hallucinations in our pipeline is not surprising: since
the first stage generates a free-form natural language description of
the scene, the latter stages may ‚Äúinvent‚Äù new relation functions that
correspond to parts of that description. By contrast, such errors are
less likely to happen when the LLM is instructed to directly generate
a program with a fixed vocabulary of functions. Nonetheless, the
other benefits offered by separating synthesis and translation make
this trade-off worth it.
Object retrieval. We first demonstrate the value of of our category-
aware re-ranking scheme for object retrieval. We compare this
scheme to two alternatives: no re-ranking (naive retrieval), and
re-ranking purely based on category (i.e. using only the second term
in Equation 2). We run each of these methods on text descriptions
from the Cap3D dataset [Luo et al. 2023], which contains paired (3D
mesh, text description) data with meshes sourced from Objaverse.
We compute the retrieval Precision@ùêæ(how often is the ground-
truth mesh associated with the text description contained in the top
ùêæretrieved objects) as well as a similar metric of category accuracy
(what percentage of the top ùêæretrieved objects are of the correct
category). Fig. 13 shows plots for both of these metrics over increas-
ing values of ùêæ. Naive retrieval performs well in terms of precision,
but suffers from a steep drop-off in category accuracy. By contrast,
re-ranking purely based on category leads to consistent category
accuracy across ùêæat the cost of a significant hit to precision. Our
weighted re-ranking scheme achieves the best of both worlds: mak-
ing sure that most of the top ùêæobjects are of the correct category
while retaining enough information about the overall description
of the object to suffer only a minor hit to retrieval precision.
We also evaluate the performance of our two retrieval filters
(category and multi-object). For each filter, we chose a handful of
common object categories and built benchmark datasets for each by
producing a set of two object text descriptions and running them
through the first part of our retrieval pipeline (initial retrieval and
re-ranking). For each set of retrieval results, we traverse the top ùëò
objects and create a set containing 10 meshes which should pass
the filter and 5 which should not (manually labeled by a human
observer). This results in a dataset with 802 annotated meshes. We
then run our filters on all of these sets of meshes three times (to
Filter
Category True Positive Rate ‚ÜëFalse Positive Rate ‚Üì
Category
Bookcase
0.77
0.27
Rug
0.88
0.22
Painting
0.97
0.1
Table
0.97
0.28
Average
0.90
0.21
Multi-object
Desk
0.75
0.15
TV Stand
0.95
0.17
Side Table
0.97
0.11
Table
0.92
0.2
Couch
0.89
0.31
Average
0.86
0.2
Table 4. Performance of our two object retrieval filters when used on a
benchmark set of of objects with manually-labeled ground truth labels. Our
filtration technique discards most unsuitable retrievals while retaining a
large fraction of suitable retrievals.
Method
Chair Couch Desk Wardrobe Painting Average
VLM (front)
65.3
54.0
67.3
87.3
84.3
71.6
VLM (front,back)
69.3
97.0
57.4
87.3
88.2
79.8
VLM (front,back,side) 96.0
97.0
57.4
86.3
88.2
85.0
LLM
75.0
97.4
92.4
94.1
94.8
90.7
Ours
87.8
97.0
93.7
95.8
94.8
93.8
Table 5. Classification accuracies for different orientation prediction ap-
proaches when evaluated on a benchmark set of objects with manually-
labeled ground truth orientations. Our approach combines a VLM with
a multimodal LLM to get the best of both worlds, achieving the highest
overall accuracy.
account for non-determinism in the LLM) and report their average
true positive and false positive rates in Table 4. Overall, both the
category and multi-object filters achieve around a 90% true positive
rate and 20% false positive rate. Given the size of our 3D object
dataset, this true positive rate is more than sufficient to ensure that
enough valid candidate meshes can be retrieved in almost all cases.
This false positive rate means that roughly one in five meshes which
passes a filter should actually have been rejected‚Äînot perfect, but a
notable improvement over not filtering at all.
Object orientation. Finally, we evaluate the performance of our
front-facing direction classifier. Similarly to the previous experiment,
we build a benchmark dataset (with 450 objects in total) containing
50-100 objects for each of several common categories, each of which
has a hand-labeled ground-truth front facing direction. We then
evaluate how well our method for orientation prediction perform
on this dataset, compared to the following alternatives we tried:
‚Ä¢ Ours: our full orientation prediction method as described
in Section 8.
‚Ä¢ VLM (front): choosing whichever orientation produces an
image whose VLM embedding has the highest cosine sim-
ilarity to that of the text ‚Äúthe front of a ùê∂,‚Äù where ùê∂is the
object category.


--- Page 15 ---
Open-Universe Indoor Scene Generation
‚Ä¢
15
‚Ä¢ VLM (front,back): like the previous method, but where
we also render the reverse face of the object and add the
similarity to ‚Äúthe back of a ùê∂‚Äù to the objective we minimize.
‚Ä¢ VLM (front,back,side): like the previous method, but where
we also render one of the side faces of the object and add the
similarity to ‚Äúthe side of a ùê∂‚Äù to the objective we minimize.
‚Ä¢ LLM: providing renders of all four faces of the object to an
LLM and asking it to choose which image best represents
the front of object.
Table 5 shows the results of this experiment. The method which
only uses a multimodal LLM performs better than the VLM-based
methods in general, but it does suffer from large performance drops
on certain types of objects (e.g. chairs). By using a VLM to filter the
set of views the LLM must consider down to two (a task likely more
prevalent in its training data than four-way image comparison),
our method improves over the LLM-only baseline on nearly all
categories.
9.5
Timing
On a MacBook Pro with an Apple M1 Max processor and 32GB RAM,
the median time to generate an object layout is about four minutes.
Almost all of this time is spent querying the LLM (the layout opti-
mizer stage takes under 10 seconds, typically). Converting the layout
into a full 3D scene is more computationally expensive, as our object
retrieval and orientation modules can invoke multiple LLM calls
for each object in the scene; for complex, densely-populated scenes,
this cost adds up. The median time to retrieve an object is slightly
under a minute (51 seconds); to orient the object, it is 16 seconds.
For a set of scenes we generated with an average of 17 objects per
scene, this led to a median total scene generation time of about 25
minutes. While slower than prior systems for closed-universe scene
synthesis (which often take only seconds), this is still faster than ex-
isting text-to-3D systems which optimize a VLM-based loss‚Äîthese
systems can take hours to produce a single scene. Our approach
could also be accelerated by caching information computed about
retrieved objects (e.g. their front-facing orientations, whether they
belong to a certain category) to avoid re-computing those quantities
if objects are encountered again.
10
DISCUSSION & FUTURE WORK
We presented a system for open-universe scene generation: taking
a text prompt as input, our system generates room-scale indoor
scenes of any requested type composed of whatever relevant objects
are needed for that room. Our system leverages LLMs to generate
scenes by tasking them with generating declarative object-relation
programs; these programs are then converted to constraint prob-
lems which are solved with gradient-based optimization to pro-
duce object layouts. Finally, our system uses multimodal LLMs and
vision-language models to retrieve appropriate meshes for each
object from a massive, unannotated dataset, as well as to estimate
the front-facing orientation of these retrieved meshes.
Open-universe scene generation is a complex, challenging task,
and our system for solving it is not perfect. In the remainder of the
paper, we discuss limitations and opportunities for improvement.
10.1
Viability for Interior Design
To get a sense for whether the outputs produced by our system
could currently be used in real-world interior design scenarios, we
conducted a small qualitative study. We recruited six individuals
(‚Äúclients‚Äù) seeking complimentary interior design services through
online ads, gathered their design needs through 30-minute inter-
views, and later presented to them designs created by both our
system and by professional designers. Both the clients and the de-
signers provided feedback on the generated scenes. More detail
about these interviews and about how the scenes were generated
can be found in the supplemental material.
The clients and the designers appreciated the system‚Äôs ability
to produce appropriate groupings of objects (e.g. dining tables and
chairs), correctly place rugs under furniture, and ensure adequate
space for door openings. They also appreciated the color and mate-
rial coordination in its generated furniture objects. However, they
found that the system could produce overly cluttered scenes in
which it seemed to lack an understanding of certain professional
interior design principles such as maintaining circulation (by e.g.
not clustering furniture in corners). Such design principles can be
expressed computationally [Merrell et al. 2011]; our system could
be improved by adding design principles as operations to our scene
modeling DSL and allowing the program synthesis LLMs to decide
which principles should be applied to which (parts of) scenes.
10.2
Other Limitations & Future Work
Our system currently only supports four-walled rooms. There are
many ways this limitation could be removed: non-rectangular rooms
could be subdivided into rectangular regions, or arbitrary arrange-
ments of walls could be specified parametrically in the system‚Äôs
input prompt (though reasoning about the resultant wall geometry
may prove difficult for an LLM; multimodal LLMs which can cor-
relate parametric wall objects with images of wall geometry may
help). In addition, objects in the current system are restricted to one
of four cardinal orientations. This discrete set could be expanded.
Orientations could also be represented as continuous values in the
layout optimizer (e.g. allowing small angular corrections to maintain
FACING constraints); this would necessitate a revision to our current
multi-step layout optimization scheme.
In very rare cases, objects in large databases such as Objaverse
are modeled with their up axis not aligned with one of the world
coordinate axes; this violates the assumptions of our orientation
prediction module and can result in ‚Äútilted‚Äù objects being inserted
into the scene. It may be possible to detect and correct (or filter out)
such objects using geometric heuristics [Fu et al. 2008] or carefully-
designed queries to multimodal language models.
We have introduced some mechanisms for fixing errors produced
by our LLM program synthesizer, but they are not foolproof. In the
future, we are interested in exploring LLM self-repair [Huang et al.
2023a] instead of / in addition to our existing heuristics: collecting
detected errors and tasking the LLM with correcting its own prior
output to eliminate them.
The open-ended capabilities of VLMs and LLMs could support
myriad approaches for open-universe scene generation. In this paper,
we have explored one small region of this design space; future work


--- Page 16 ---
16
‚Ä¢
Rio Aguina-Kang, Maxim Gumin, Do Heon Han, Stewart Morris, Seung Jean Yoo, Aditya Ganeshan, R. Kenny Jones, Qiuhong Anna Wei, Kailiang Fu, and Daniel Ritchie
is needed to map out its entirety. We hope that our work serves as
both a springboard and a strong baseline for a new line of research
on open-universe scene generation.
REFERENCES
Ahmed Abdelreheem, Abdelrahman Eldesokey, Maks Ovsjanikov, and Peter Wonka.
2023a. Zero-Shot 3D Shape Correspondence. In SIGGRAPH Asia.
Ahmed Abdelreheem, Ivan Skorokhodov, Maks Ovsjanikov, and Peter Wonka. 2023b.
SATR: Zero-Shot Semantic Segmentation of 3D Shapes. In Proceedings of the Inter-
national Conference on Computer Vision (ICCV).
Google DeepMind AlphaCode Team. 2023. AlphaCode 2 Technical Report. (2023).
Haotian Bai, Yuanhuiyi Lyu, Lutao Jiang, Sijia Li, Haonan Lu, Xiaodong Lin, and Lin
Wang. 2023. CompoNeRF: Text-guided Multi-object Compositional NeRF with
Editable 3D Scene Layout. arXiv:2303.13843 [cs.CV]
Giuseppe Di Battista, Peter Eades, Roberto Tamassia, and Ioannis G. Tollis. 1998. Graph
Drawing: Algorithms for the Visualization of Graphs (1st ed.). Prentice Hall PTR,
USA.
Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. 2023. Fantasia3D: Disentangling
Geometry and Appearance for High-quality Text-to-3D Content Creation. In Pro-
ceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).
Jaemin Cho, Abhay Zala, and Mohit Bansal. 2023. Visual Programming for Text-to-
Image Generation and Evaluation. In NeurIPS.
Bob Coyne and Richard Sproat. 2001. WordsEye: an automatic text-to-scene conversion
system. In Proceedings of the 28th Annual Conference on Computer Graphics and
Interactive Techniques (SIGGRAPH ‚Äô01). Association for Computing Machinery, New
York, NY, USA, 487‚Äì496. https://doi.org/10.1145/383259.383316
Dale Decatur, Itai Lang, and Rana Hanocka. 2022. 3D Highlighter: Localizing Regions
on 3D Shapes via Text Descriptions. CVPR.
Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya
Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli
VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani,
Ludwig Schmidt, and Ali Farhadi. 2023. Objaverse-XL: A Universe of 10M+ 3D
Objects. arXiv preprint arXiv:2307.05663 (2023).
Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli Vander-
Bilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. 2022a.
Objaverse: A Universe of Annotated 3D Objects. arXiv preprint arXiv:2212.08051
(2022).
Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi
Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mot-
taghi. 2022b.
ProcTHOR: Large-Scale Embodied AI Using Procedural Gener-
ation. In Advances in Neural Information Processing Systems, S. Koyejo, S. Mo-
hamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran As-
sociates, Inc., 5982‚Äì5994. https://proceedings.neurips.cc/paper_files/paper/2022/
file/27c546ab1e4f1d7d638e6a8dfbad9a07-Paper-Conference.pdf
Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-
Emmanuel Mazar√©, Maria Lomeli, Lucas Hosseini, and Herv√© J√©gou. 2024. The Faiss
library. (2024). arXiv:2401.08281 [cs.LG]
Chuan Fang, Xiaotao Hu, Kunming Luo, and Ping Tan. 2023. Ctrl-Room: Controllable
Text-to-3D Room Meshes Generation with Layout Constraints. arXiv preprint
arXiv:2310.03602 (2023).
Weixi Feng, Wanrong Zhu, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Xuehai He,
S Basu, Xin Eric Wang, and William Yang Wang. 2023. LayoutGPT: Compositional
Visual Planning and Generation with Large Language Models. In Thirty-seventh
Conference on Neural Information Processing Systems. https://openreview.net/forum?
id=Xu8aG5Q8M3
Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas Funkhouser, and Pat Hanrahan.
2012. Example-based synthesis of 3D object arrangements. ACM Transactions on
Graphics (TOG) 31, 6 (2012), 135:1‚Äì11.
Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng,
Chengyue Sun, Rongfei Jia, Binqiang Zhao, et al. 2021. 3d-front: 3d furnished rooms
with layouts and semantics. In Proceedings of the IEEE/CVF International Conference
on Computer Vision. 10933‚Äì10942.
Hongbo Fu, Daniel Cohen-Or, Gideon Dror, and Alla Sheffer. 2008. Upright orientation
of man-made objects. In ACM SIGGRAPH 2008 Papers (Los Angeles, California)
(SIGGRAPH ‚Äô08). Association for Computing Machinery, New York, NY, USA, Article
42, 7 pages. https://doi.org/10.1145/1399504.1360641
Gege Gao, Weiyang Liu, Anpei Chen, Andreas Geiger, and Bernhard Sch√∂lkopf. 2023a.
GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs. arXiv
2312.00093 (2023).
Lin Gao, Jia-Mu Sun, Kaichun Mo, Yu-Kun Lai, Leonidas J. Guibas, and Jie Yang. 2023b.
SceneHGN: Hierarchical Graph Networks for 3D Indoor Scene Generation with Fine-
Grained Geometry. IEEE Transactions on Pattern Analysis and Machine Intelligence
(2023), 1‚Äì18. https://doi.org/10.1109/TPAMI.2023.3237577
OpenAI GPT-4. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
Gabriel Grand, Lionel Wong, Matthew Bowers, Theo X. Olausson, Muxin Liu, Joshua B.
Tenenbaum, and Jacob Andreas. 2023. Learning Interpretable Libraries by Compress-
ing and Documenting Code. In Intrinsically-Motivated and Open-Ended Learning
Workshop @NeurIPS2023. https://openreview.net/forum?id=4gYLottfsf
Tanmay Gupta and Aniruddha Kembhavi. 2023. Visual Programming: Compositional
Visual Reasoning Without Training. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR). 14953‚Äì14962.
Yuan Haocheng, Xu Jing, Pan Hao, Bousseau Adrien, Mitra Niloy, and Li Changjian.
2023. CADTalk: An Algorithm and Benchmark for Semantic Commenting of CAD
Programs. arXiv preprint arXiv:2311.16703 (2023).
Jordan Hobbs. 2024. Why IKEA Uses 3D Renders vs. Photography for Their Fur-
niture Catalog. https://www.cadcrowd.com/blog/why-ikea-uses-3d-renders-vs-
photography-for-their-furniture-catalog/. Accessed: 2024-01-19.
Lukas H√∂llein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nie√üner. 2023.
Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models. In
Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).
7909‚Äì7920.
Ian Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas. 2023b. Aladdin:
Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions.
arXiv preprint arXiv:2306.06212 (2023).
Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu,
Xinying Song, and Denny Zhou. 2023a. Large Language Models Cannot Self-Correct
Reasoning Yet. arXiv:2310.01798 [cs.CL]
Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, and Ben Poole. 2022.
Zero-Shot Text-Guided Object Generation with Dream Fields. (2022).
Heewoo Jun and Alex Nichol. 2023. Shap-E: Generating Conditional 3D Implicit
Functions. arXiv:2305.02463 [cs.CV]
Z Sadeghipour Kermani, Zicheng Liao, Ping Tan, and H Zhang. 2016. Learning 3D Scene
Synthesis from Annotated RGB-D Images. In Computer Graphics Forum, Vol. 35.
197‚Äì206.
Manyi Li, Akshay Gadi Patil, Kai Xu, Siddhartha Chaudhuri, Owais Khan, Ariel Shamir,
Changhe Tu, Baoquan Chen, Daniel Cohen-Or, and Hao Zhang. 2018. GRAINS:
Generative Recursive Autoencoders for INdoor Scenes. CoRR arXiv:1807.09193
(2018).
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R√©mi
Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas
Hubert, Peter Choy, Cyprien de Masson d‚ÄôAutume, Igor Babuschkin, Xinyun Chen,
Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy,
Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas,
Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-level code generation
with AlphaCode. Science 378, 6624 (Dec. 2022), 1092‚Äì1097. https://doi.org/10.1126/
science.abq1158
Yuan Liang, Song-Hai Zhang, and Ralph Robert Martin. 2017. Automatic Data-Driven
Room Design Generation. In Next Generation Computer Animation Techniques, Jian
Chang, Jian Jun Zhang, Nadia Magnenat Thalmann, Shi-Min Hu, Ruofeng Tong,
and Wencheng Wang (Eds.). Springer International Publishing, Cham, 133‚Äì148.
Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang,
Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. 2023. Magic3D: High-
Resolution Text-to-3D Content Creation. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR).
Minghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling, Fatih Porikli, and Hao
Su. 2023. Partslip: Low-shot part segmentation for 3d point clouds via pretrained
image-language models. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 21736‚Äì21746.
Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan Lin, Towaki Takikawa,
Nicholas Sharp, Tsung-Yi Lin, Ming-Yu Liu, Sanja Fidler, and James Lucas. 2023.
ATT3D: Amortized Text-To-3D Object Synthesis. arXiv (2023).
Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. 2023. Scalable 3D
Captioning with Pretrained Models. arXiv preprint arXiv:2306.07279 (2023).
Liane Makatura, Michael Foshey, Bohan Wang, Felix H√§hnLein, Pingchuan Ma, Bolei
Deng, Megan Tjandrasuwita, Andrew Spielberg, Crystal Elaine Owens, Peter Yichen
Chen, Allan Zhao, Amy Zhu, Wil J Norton, Edward Gu, Joshua Jacob, Yifei Li,
Adriana Schulz, and Wojciech Matusik. 2023. How Can Large Language Models
Help Humans in Design and Manufacturing? arXiv:2307.14377 [cs.CL]
Paul Merrell, Eric Schkufza, Zeyang Li, Maneesh Agrawala, and Vladlen Koltun. 2011.
Interactive furniture layout using interior design guidelines. In ACM SIGGRAPH
2011 Papers (Vancouver, British Columbia, Canada) (SIGGRAPH ‚Äô11). Association for
Computing Machinery, New York, NY, USA, Article 87, 10 pages. https://doi.org/
10.1145/1964921.1964982
David Mizrahi, Roman Bachmann, Oguzhan Fatih Kar, Teresa Yeo, Mingfei Gao, Afshin
Dehghan, and Amir Zamir. 2023. 4M: Massively Multimodal Masked Modeling.
In Thirty-seventh Conference on Neural Information Processing Systems.
https:
//openreview.net/forum?id=TegmlsD8oQ
Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen.
2022. Point-E: A System for Generating 3D Point Clouds from Complex Prompts.
arXiv:2212.08751 [cs.CV]


--- Page 17 ---
Open-Universe Indoor Scene Generation
‚Ä¢
17
Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten Kreis, Andreas Geiger, and
Sanja Fidler. 2021. ATISS: Autoregressive Transformers for Indoor Scene Synthesis.
In Advances in Neural Information Processing Systems (NeurIPS).
Planner5d. 2024. Planner5d: House Design Software. https://planner5d.com. Accessed:
2024-01-19.
Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. 2022. DreamFusion:
Text-to-3D using 2D Diffusion. arXiv (2022).
Xavi Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Ruslan Partsey,
Jimmy Yang, Ruta Desai, Alexander William Clegg, Michal Hlavac, Tiffany Min,
Theo Gervet, Vladimir Vondrus, Vincent-Pierre Berges, John Turner, Oleksandr
Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra Singh Chap-
lot, Unnat Jain, Dhruv Batra, Akshara Rai, and Roozbeh Mottaghi. 2023. Habitat 3.0:
A Co-Habitat for Humans, Avatars and Robots.
Siyuan Qi, Yixin Zhu, Siyuan Huang, Chenfanfu Jiang, and Song-Chun Zhu. 2018.
Human-centric Indoor Scene Synthesis Using Stochastic Grammar. In Conference on
Computer Vision and Pattern Recognition (CVPR).
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini
Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From
Natural Language Supervision. In Proceedings of the 38th International Conference
on Machine Learning (Proceedings of Machine Learning Research, Vol. 139), Marina
Meila and Tong Zhang (Eds.). PMLR, 8748‚Äì8763.
Daniel Ritchie, Kai Wang, and Yu an Lin. 2019. Fast and Flexible Indoor Scene Synthesis
via Deep Convolutional Generative Models. In CVPR 2019.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn
Ommer. 2021. High-Resolution Image Synthesis with Latent Diffusion Models.
arXiv:2112.10752 [cs.CV]
Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej
Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan Ellenberg,
Pengming Wang, Omar Fawzi, Pushmeet Kohli, and Alhussein Fawzi. 2023. Mathe-
matical discoveries from program search with large language models. Nature (2023).
https://doi.org/10.1038/s41586-023-06924-6
RoomSketcher. 2024. Create Floor Plans and Home Designs Online. http://www.
roomsketcher.com. Accessed: 2024-01-19.
Rahul Sajnani, Adrien Poulenard, Jivitesh Jain, Radhika Dua, Leonidas J. Guibas, and
Srinath Sridhar. 2022. ConDor: Self-Supervised Canonicalization of 3D Pose for
Partial Shapes. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR).
Aditya Sanghi, Hang Chu, Joseph G. Lambourne, Ye Wang, Chin-Yi Cheng, Marco
Fumero, and Kamal Rahimi Malekshan. 2022. CLIP-Forge: Towards Zero-Shot Text-
To-Shape Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR). 18603‚Äì18613.
Aditya Sanghi, Rao Fu, Vivian Liu, Karl Willis, Hooman Shayani, Amir Hosein Khasah-
madi, Srinath Sridhar, and Daniel Ritchie. 2023. CLIP-Sculptor: Zero-Shot Generation
of High-Fidelity and Diverse Shapes from Natural Language. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR).
Jonas Schult, Sam Tsai, Lukas H√∂llein, Bichen Wu, Jialiang Wang, Chih-Yao Ma, Kun-
peng Li, Xiaofang Wang, Felix Wimbauer, Zijian He, Peizhao Zhang, Bastian Leibe,
Peter Vajda, and Ji Hou. 2023. ControlRoom3D: Room Generation using Semantic
Proxy Rooms. arXiv:2312.05208 (2023).
Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, and Stephen
Gould. 2023.
3D-GPT: Procedural 3D Modeling with Large Language Models.
arXiv:2310.12945 [cs.CV]
D√≠dac Sur√≠s, Sachit Menon, and Carl Vondrick. 2023. ViperGPT: Visual Inference via
Python Execution for Reasoning. Proceedings of IEEE International Conference on
Computer Vision (ICCV) (2023).
Jiapeng Tang, Nie Yinyu, Markhasin Lev, Dai Angela, Thies Justus, and Matthias Nie√üner.
2023. DiffuScene: Scene Graph Denoising Diffusion Probabilistic Model for Genera-
tive Indoor Scene Synthesis. In arxiv.
Target. 2024. Home Planner. https://www.target.com/room-planner/home. Accessed:
2024-01-19.
Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. 2024. Solving Olympiad
Geometry without Human Demonstrations. Nature (2024). https://doi.org/10.1038/
s41586-023-06747-5
Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, Angel X Chang, and Daniel
Ritchie. 2019. Planit: Planning and instantiating indoor scenes with relation graph
and spatial prior networks. ACM Transactions on Graphics (TOG) 38, 4 (2019), 132.
Kai Wang, Manolis Savva, Angel X. Chang, and Daniel Ritchie. 2018. Deep Convolu-
tional Priors for Indoor Scene Synthesis. In Annual Conference on Computer Graphics
and Interactive Techniques (SIGGRAPH).
Xinpeng Wang, Chandan Yeshwanth, and Matthias Nie√üner. 2020. SceneFormer: Indoor
Scene Generation with Transformers. arXiv preprint arXiv:2012.09793 (2020).
Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun
Zhu. 2023. ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with
Variational Score Distillation. In Advances in Neural Information Processing Systems
(NeurIPS).
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,
Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting Elicits
Reasoning in Large Language Models. In NeurIPS.
Le Xue, Mingfei Gao, Chen Xing, Roberto Mart√≠n-Mart√≠n, Jiajun Wu, Caiming Xiong,
Ran Xu, Juan Carlos Niebles, and Silvio Savarese. 2023. ULIP: Learning Unified
Representation of Language, Image and Point Cloud for 3D Understanding. In CVPR
2023.
Karmesh Yadav, Ram Ramrakhya, Santhosh Kumar Ramakrishnan, Theo Gervet, John
Turner, Aaron Gokaslan, Noah Maestre, Angel Xuan Chang, Dhruv Batra, Manolis
Savva, Alexander William Clegg, and Devendra Singh Chaplot. 2023. Habitat-
Matterport 3D Semantics Dataset. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR). 4927‚Äì4936.
Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun
Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Chris Callison-Burch, Mark Yatskar,
Aniruddha Kembhavi, and Christopher Clark. 2023. Holodeck: Language Guided
Generation of 3D Embodied AI Environments. arXiv preprint arXiv:2312.09067
(2023).
Yi-Ting Yeh, Lingfeng Yang, Matthew Watson, Noah D. Goodman, and Pat Hanrahan.
2012. Synthesizing open worlds with constraints using locally annealed reversible
jump MCMC. 31, 4, Article 56 (jul 2012), 11 pages. https://doi.org/10.1145/2185520.
2185552
Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu
Liu, Qi Tian, and Xinggang Wang. 2023. GaussianDreamer: Fast Generation from
Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models. arXiv preprint
arXiv:2310.08529 (2023).
Lap-Fai Yu, Sai Kit Yeung, Chi-Keung Tang, Demetri Terzopoulos, Tony F. Chan, and
Stanley Osher. 2011. Make it home: automatic optimization of furniture arrangement.
ACM Transactions on Graphics (TOG) 30, 4 (2011), 86:1‚Äì12.
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023a. Sigmoid
Loss for Language Image Pre-Training. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV). 11975‚Äì11986.
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023b. Sigmoid
Loss for Language Image Pre-Training. In ICLR 2023.
Zaiwei Zhang, Zhenpei Yang, Chongyang Ma, Linjie Luo, Alexander Huth, Etienne
Vouga, and Qixing Huang. 2018. Deep Generative Modeling for Scene Synthesis
via Hybrid Representations. CoRR abs/1808.02084 (2018). arXiv:1808.02084 http:
//arxiv.org/abs/1808.02084
Yuchen Zhou, Jiayuan Gu, Xuanlin Li, Minghua Liu, Yunhao Fang, and Hao Su. 2023.
PartSLIP++: Enhancing Low-Shot 3D Part Segmentation via Multi-View Instance
Segmentation and Maximum Likelihood Estimation. arXiv:2312.03015 [cs.CV]
Yang Zhou, Zachary While, and Evangelos Kalogerakis. 2019. SceneGraphNet: Neural
Message Passing for 3D Indoor Scene Augmentation. In IEEE Conference on Computer
Vision (ICCV).
A
SCENE DESCRIPTION LANGUAGE
As described in Section 4, the domain-specific functions we add to
Python are either (1) object constructors, (2) relation functions or
(3) parameter setting functions. The basic object constructor is
Object(description: str, width: float, depth: float,
height: float, facing: Object | int | None = None).
An object can face either one of the 4 cardinal directions (EAST=0,
NORTH=1, WEST=2, SOUTH=3), another object, or not face anything. A
programmer (a human or an LLM) may want not to specify a facing
direction, if this direction is not important (for example, for a tablet
lying on a sofa). In this case the object will appear in a scene in a
random orientation.
In addition to the default Object constructor, we have special
constructors for doors and windows:
Door(description:
str,
width:
float,
height:
float,
wall: int)
Window(description: str, width: float, height: float,
wall: int, height_above_ground: float, above: Cuboid =
None)


--- Page 18 ---
18
‚Ä¢
Rio Aguina-Kang, Maxim Gumin, Do Heon Han, Stewart Morris, Seung Jean Yoo, Aditya Ganeshan, R. Kenny Jones, Qiuhong Anna Wei, Kailiang Fu, and Daniel Ritchie
Doors and windows are treated as regular objects in our sys-
tem. The programmer can place additional relations on doors and
windows. However, doors and windows initialize with additional
constraints that ensure that (1) doors are always adjacent to walls,
and windows are always mounted on walls, and (2) there is enough
empty space in front of a door, so a door can be opened, windows
also require empty space in front of them.
For some types of objects such as paintings, books or statues, it
makes sense to retrieve different 3D meshes even for objects that
have the same description. For this purpose, we have a list-of-objects
constructor unique_objects(amount: int, description: str,
width: float, depth: float, height: float). For consis-
tency, we also have an objects(amount: int, description:
str, width: float, depth: float, height: float) construc-
tor, although it can be replaced with a list comprehension.
DSL relation functions are:
(1) on(top: Object, bottom: Object) means that the first
object stands on top of the bottom object. If the top object
is smaller, it should not extend beyond the bottom object.
(2) next_to_wall(a: Object, wall: int, distance: float
= 0.0) means that object a stands next to one of the 4
walls. If the optional distance argument is zero, the ob-
ject is touching the wall. Otherwise, it stands no more than
distance meters from the wall. The optional distance pa-
rameter allows to fit a chair between a table and a wall
when a table is standing next to a wall. Using a pair of
next_to_wall functions, the programmer can express that
some object stands in a corner.
(3) mounted_on_wall(a: Object, wall: int, height: float,
above: Object = None) means that that object a is mounted
on a wall height meters above the ground. This relation is
useful for paintings, mirrors, wall clocks or whiteboards.
When the optional above argument is used, object a is
mounted above some other object.
(4) mounted_on_ceiling(a: Object, above: Object = None)
means that object a is mounted on a ceiling, optionally above
another object. This relation is useful for describing fans,
projectors or chandeliers.
(5) adjacent(a: Object, b: Object, arg1: int | float
| None = None, arg2: int | float | None = None,
arg3: float = 0.0) relation can be used with 0, 1 or 2
direction arguments, and an optional distance argument:
(a) adjacent(chair, desk) means that the chair‚Äôs bound-
ing box touches the desk‚Äôs bounding box.
(b) adjacent(chair, desk, NORTH) is the most common
variant. It means that the chair is adjacent to the desk
from the NORTH.
(c) adjacent(chair, desk, NORTH, WEST) means that
chair is adjacent to the desk from the north side but is
aligned with the west side of the desk. This version of
the adjacency relation is useful for describing a night-
stand that is adjacent to a bed from the side, but is
aligned with the head of a bed.
If an optional distance argument is used, the touching re-
quirement is replaced with the distance requirement. For
example, adjacent(chair, desk, NORTH, 0.2) means
that the chair is located to the NORTH of the desk, no more
than 20cm away from the desk.
(6) aligned(cuboids: list[Object], axis: bool) means
that the centers of a list of objects should be aligned either
vertically or horizontally. The second argument can either
be WESTEAST=False or NORTHSOUTH=True. This relation is
useful for describing careful arrangements of furniture.
(7) facing(a: Object, direction: Object | int) means
that the forward vector of object a should face either one of
the cardinal directions or another object.
(8) surround(chairs: list[Object], table: Object) is a
syntax sugar relation that is implemented with adjacent
and facing relations. It adds surrounding objects (for exam-
ple, chairs) one-by-one, and picks the sides of the central
object (for a example, a table) that have the most free space
available. This relation respects other adjacencies and walls.
Finally, the DSL has parameter setting functions for setting the
size of the scene, the floor texture and the wall texture. These func-
tions are called only once per scene:
set_size(westeast: float, northsouth: float, height: float)
set_floor_texture(texture: str)
set_wall_texture(texture: str)
B
LAYOUT OPTIMIZER DETAILS
Here we provide more implementation details for our system‚Äôs
layout optimizer module.
B.1
Constraint Losses
Let ùëéùë†ùëñùëßùëí, ùëéùëêùëíùëõùë°ùëíùëü, ùëéùëöùëñùëõand ùëéùëöùëéùë•denote the size, the center, the
lower-left-bottom corner and the upper-right-top corner of cuboid ùëé,
and let ùë†be the scene cuboid. Let relu(ùë•) = max(ùë•, 0) and let relu be
defined for vectors component-wise. Let proj(ùë£) be the projection
of vector ùë£on the horizontal ùëãùëç-plane. Let ùëë(ùëé,ùëè) be Euclidean
distance between cuboids ùëéand ùëè. We define constraint losses in
the following way:
LWITHINBOUNDS(ùëé) = ‚à•relu(ùë†ùëöùëñùëõ‚àíùëéùëöùëñùëõ)‚à•2+‚à•relu(ùëéùëöùëéùë•‚àíùë†ùëöùëéùë•)‚à•2,
LON(ùëé,ùëè) = (ùëèùëöùëéùë•
ùë¶
‚àíùëéùëöùëñùëõ
ùë¶
)2 + ‚à•relu(proj(ùëèùëöùëñùëõ‚àíùëéùëöùëñùëõ))‚à•2+
+‚à•relu(proj(ùëéùëöùëéùë•‚àíùëèùëöùëéùë•))‚à•2 ‚àí1
2 ‚à•relu(proj(ùëéùë†ùëñùëßùëí‚àíùëèùë†ùëñùëßùëí))‚à•2,
LHEIGHT(ùëé,‚Ñéùëíùëñùëî‚Ñéùë°) = (ùëéùëöùëñùëõ
ùë¶
‚àí‚Ñéùëíùëñùëî‚Ñéùë°)2,
LNEXTTOWALL(ùëé, EAST,ùëëùëñùë†ùë°) = relu2(ùë†ùëöùëéùë•
ùë•
‚àíùëéùëöùëéùë•
ùë•
‚àíùëëùëñùë†ùë°),
LCEILING(ùëé) = (ùëéùëöùëéùë•
ùë¶
‚àíùë†ùëöùëéùë•
ùë¶
)2,
LALIGNED(ùëé,ùëè, WESTEAST) = (ùëéùëêùëíùëõùë°ùëíùëü
ùëß
‚àíùëèùëêùëíùëõùë°ùëíùëü
ùëß
)2,
LABOVE(ùëé,ùëè, EAST) = relu2(ùêµùëöùëñùëõ
ùëß
‚àíùê¥ùëöùëñùëõ
ùëß
) + relu2(ùê¥ùëöùëéùë•
ùëß
‚àíùêµùëöùëéùë•
ùëß
),


--- Page 19 ---
Open-Universe Indoor Scene Generation
‚Ä¢
19
where ùê¥, ùêµ= ùëé,ùëèif ùëéùë†ùëñùëßùëí
ùëß
< ùëèùë†ùëñùëßùëí
ùëß
and ùê¥, ùêµ= ùëè,ùëéotherwise.
LADJACENT0(ùëé,ùëè,ùëëùëñùë†ùë°) = (ùëë(ùëé,ùëè) ‚àíùëëùëñùë†ùë°)2,
LADJACENT1(ùëé,ùëè, EAST,ùëëùëñùë†ùë°) = LA(ùëé,ùëè, EAST,ùëëùëñùë†ùë°)+
+relu2(ùëèùëöùëñùëõ
ùëß
‚àíùëéùëöùëñùëõ
ùëß
),
LADJACENT2(ùëé,ùëè, EAST, NORTH,ùëëùëñùë†ùë°) = LA(ùëé,ùëè, EAST,ùëëùëñùë†ùë°)+
+(ùëèùëöùëñùëõ
ùëß
‚àíùëéùëöùëñùëõ
ùëß
)2,
where
LA(ùëé,ùëè, EAST,ùëëùëñùë†ùë°) = relu2(ùëéùëöùëñùëõ
ùë•
‚àíùëèùëöùëéùë•
ùë•
‚àíùëëùëñùë†ùë°)+
+relu2(ùëèùëöùëéùë•
ùë•
‚àíùëéùëöùëñùëõ
ùë•
) + relu2(ùëéùëöùëéùë•
ùëß
‚àíùëèùëöùëéùë•
ùëß
)‚àí
‚àí1
2relu2(ùëéùë†ùëñùëßùëí
ùëß
‚àíùëèùë†ùëñùëßùëí
ùëß
).
Generalization from EAST and NORTH to other cardinal directions
is straightforward.
B.2
Repel Force Implementation
To implement repel forces, we first define a binary connectivity
relation on the set of objects and walls in the scene: NEXTTOWALL(a,
wall) connects object a and wall wall, and ON(a,b) or ADJACENT(a,
b) connect objects a and b. This relation splits the set of objects
and walls into connected components. For every pair of objects that
belong to different connected components (and for pairs of objects
and walls from different connected components) we add a repelling
vector to the optimization gradient with magnitude proportional
to max(1 ‚àíùëë/ùëëmax, 0), where ùëëis the Euclidean distance between
the two objects (or between an object and a wall), and ùëëmax is the
maximum range of repels, set to be the 1/4 the minimum linear size
of the scene. We add a small random noise to repel forces to escape
local minima.
