--- Page 1 ---
IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. X, XX XXXX
1
Secure and Energy-Efficient Wireless Agentic AI
Networks
Yuanyan Song, Kezhi Wang, Senior Member, IEEE, Xinmian Xu
Abstractâ€”In this paper, we introduce a secure wireless agentic
AI network comprising one supervisor AI agent and multiple
other AI agents to provision quality of service (QoS) for usersâ€™
reasoning tasks while ensuring confidentiality of private knowl-
edge and reasoning outcomes. Specifically, the supervisor AI
agent can dynamically assign other AI agents to participate
in cooperative reasoning, while the unselected AI agents act
as friendly jammers to degrade the eavesdropperâ€™s interception
performance. To extend the service duration of AI agents, an
energy minimization problem is formulated that jointly optimizes
AI agent selection, base station (BS) beamforming, and AI agent
transmission power, subject to latency and reasoning accuracy
constraints. To address the formulated problem, we propose two
resource allocation schemes, ASC and LAW, which first decom-
pose it into three sub-problems. Specifically, ASC optimizes each
sub-problem iteratively using the proposed alternating direction
method of multipliers (ADMM)-based algorithm, semi-definite
relaxation (SDR), and successive convex approximation (SCA),
while LAW tackles each sub-problem using the proposed large
language model (LLM) optimizer within an agentic workflow.
The experimental results show that the proposed solutions can
reduce network energy consumption by up to 59.1% compared to
other benchmark schemes. Furthermore, the proposed schemes
are validated using a practical agentic AI system based on Qwen,
demonstrating satisfactory reasoning accuracy across various
public benchmarks.
Index Termsâ€”Agentic AI, quality of service, large language
model, cooperative reasoning, resource allocation.
I. INTRODUCTION
R
ECENTLY, AI agents and the emerging paradigm of
agentic AI, empowered by the remarkable capabilities
of large language models (LLMs), have gained significant
attention [1]. Leveraging the technical advantages of LLMs
in complex reasoning, multi-step planning, and in-context
learning, AI agents have evolved from passive responders to
pre-defined tasks into autonomous entities capable of active
environmental perception and automated tool invocation [2],
[3]. This technological leap significantly improves the automa-
tion of AI systems, enabling AI agents to dynamically execute
specific tasks by interacting with dynamic environments and
utilizing appropriate AI tools with minimal human intervention
[4]. To further address highly complex objectives, agentic AI
frameworks have been proposed to dynamically coordinate
multiple AI agents, forming a collaborative network rather
than relying on a monolithic agent [5]. Such architectures can
substantially improve quality of service (QoS) and robustness
for intricate tasks, as they allow the aggregation of diverse skill
Yuanyan Song, Kezhi Wang, and Xinmian Xu are with the Department of
Computer Science, Brunel University of London, UB8 3PH, UK. Xinmian
Xu is also with Nanjing University of Posts and Telecommunications.
sets and the verification of outputs from different AI agents
through cooperative mechanisms [6].
Agentic AI is increasingly recognized as a promising ar-
chitecture for QoS provisioning for the emerging reasoning
tasks [7]. Unlike traditional services, these reasoning tasks are
characterized by strong context-dependency, often requiring
up-to-date external information from dynamic environments
and private knowledge from users, making them difficult for
conventional wireless networks to handle [8]. Agentic AI
addresses these challenges by orchestrating multiple AI agents
following dynamic workflows. A typical workflow is shown
in Fig. 1. The agentic AI network first interprets the userâ€™s
request and subsequently dispatches it to multiple specialized
AI agents. Each AI agent independently collects environmental
data to derive its own reasoning conclusion. Finally, the system
aggregates these diverse reasoning outputs to obtain a final
result [9], [10]. This cooperative workflow offers a potential
pathway for mitigating the inaccuracies and incompleteness
caused by single-agent hallucinations or limited data [11].
Userâ€™s request:
I need to upload a large
file. BS I and BS II,
which one offers better
performance?
Reasoning
Supervisor AI agent
Prompt design:
We must identify the target
BS for the user. Gather CSI,
traffic
load,
and
energy
metrics of both BSs to
determine the target BS.
Environment
CSI
Buffer status
Distance
Evaluation&Voting:
Five AI agents chose A.
Only
one
AI
agent
selected B. I advise the
user connect to A.
AI agent 1:
1.Analyze the CSI of
both BSs.
2.Based on the SNR,
A offers a LOS link,
while B is blocked.
3.I select A.
AI agent 2:
1.Assess the network
traffic
load
of
both
BSs.
2.According to the BS
buffer status report, B
is congested because
of limited resources.
3.I select A.
Answerï¼š
A is better.
AI agent N:
1. Estimate Tx energy
consumption.
2. B is closer to the
user,
requiring
less
Tx power.
3.I select B.
â€¦
Prompt & private knowledge transmission
Environment information acquisition
Reasoning output transmission
â€¦
Userâ€™s request:
I need to upload a large
file.
Which
BS
offers
better performance: A or B?
Fig. 1: A typical workflow of agentic AI to execute a specific
reasoning task.
Thanks to the advancements in model lightweighting tech-
nologies, such as quantization and pruning, along with efficient
post-training methods, the computational overhead of LLMs
can be significantly reduced with negligible accuracy loss
arXiv:2602.15212v1  [cs.AI]  16 Feb 2026


--- Page 2 ---
IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. X, XX XXXX
2
[12]. These breakthroughs enable the deployment of LLMs on
mobile devices (MDs) while maintaining satisfactory inference
accuracy [13] and the realization of wireless agentic AI net-
works. This promising paradigm can serve as a complement to
traditional cloud-based agentic AI systems. By leveraging edge
resources and the QoS provisioning capabilities of agentic
AI, wireless agentic AI networks are expected to provide on-
demand reasoning services with low latency, high accuracy and
reliability for wireless networks. However, the implementation
of wireless agentic AI networks is facing several critical
technical challenges. First, MDs are typically constrained by
limited battery capacities. How to dynamically coordinate
network communication and computation resources to min-
imize the networkâ€™s energy consumption while satisfying QoS
constraints, such as reasoning accuracy and latency thresholds,
remains a pressing problem. Second, the open nature of
wireless channels renders the inter-AI agent communication
links vulnerable to eavesdropping. This exposure risks the
leakage of sensitive user information during the collaborative
workflow, making the realization of confidential communica-
tion a pivotal research focus in agentic AI networks.
Motivated by the aforementioned challenges, this paper
investigates secure communications for wireless agentic AI
networks and optimizes communication and computation re-
sources scheduling for energy minimization and reasoning
tasks QoS provisioning. The main contributions of this work
are summarized as follows:
1) This article proposes a novel secure wireless agentic AI
network, where one supervisor AI agent can dynami-
cally select other AI agents to participate in cooperative
reasoning, while other unselected AI agents perform as
friendly jammers to ensure the confidentiality of private
knowledge and reasoning outcomes. The energy mini-
mization problem is formulated by jointly considering AI
agent selection, BS beamforming, AI agent transmission
powers and QoS constraints. The formulated problem is
extremely challenging to tackle due to the coupling of
integer variables and non-convex constraints.
2) We first propose a resource allocation scheme called ASC
to solve the formulated problem. In particular, the original
problem is decoupled into three sub-problems, which are
iteratively tackled by the proposed alternating direction
method of multipliers (ADMM)-based algorithm, semi-
definite relaxation (SDR) and successive convex approxi-
mation (SCA), respectively. As such, the feasible solution
of the original problem can be obtained.
3) Another LLM-based resource allocation scheme called
LAW is proposed to decouple the original problem into
three sub-problems. Each sub-problem can be solved
by the proposed LLM optimizer following an agentic
workflow. LAW is an LLM-based scheme designed to
be directly implemented by the supervisor AI agent.
4) Simulation results verify the effectiveness of ASC and
LAW. The results demonstrate that the proposed resource
allocation schemes significantly reduce the energy con-
sumption of the proposed secure wireless agentic AI
network. Furthermore, the impact of key system pa-
rameters, such as different QoS requirements and BS
configurations, is investigated.
5) We develop a practical agentic AI system based on
the Qwen LLM series. Through extensive experiments
on multiple public benchmarks, we validate that the
proposed resource allocation schemes can achieve sat-
isfactory reasoning accuracy.
The rest of this article is organized as follows. The re-
lated works are demonstrated in Section II. The proposed
secure wireless agentic AI network and formulated energy
minimization problem are demonstrated in Section III. The
proposed resource allocation schemes are introduced in IV.
The performance evaluation regarding the proposed solutions
along with numerous selected benchmark schemes is shown
in Section V, followed by Section VI concluding this article.
II. RELATED WORKS
In this section, we report some related works regarding MD-
deployed LLMs, agentic AI-empowered reasoning and some
open challenges of wireless agentic AI networks.
A. MD-deployed LLMs
The deployment of LLMs in wireless networks is hindered
by the disparity between the colossal scale of these models and
the limited computation and memory resources of MDs. Some
state-of-the-art models like GPT-4 contain approximately 1.8
trillion parameters, with next-generation LLMs such as GPT-
5 expected to be higher [14]. To overcome this issue, model
quantization and pruning have been recognized as effective
methods to reduce the high dimensionality of LLMs. [12]
and [15] introduced advanced quantization methods, utilizing
approximate second-order information and activation smooth-
ing techniques, to significantly reduce model widths and
parameter quantities while maintaining satisfactory reasoning
capabilities. The authors of [16] proposed an efficient fine-
tuning and pruning strategy that effectively reduces the active
dimensions and memory usage of LLMs, enabling a 65B
parameter model to run on resource-limited hardware with
negligible performance degradation. The practical viability of
these optimizations was further investigated in [13], which
conducted extensive experiments across various MDs, verify-
ing that such lightweighted LLMs can be potentially deployed
in resource-constrained wireless networks.
Post-training is of great importance to promise the per-
formance of MD-deployed LLMs and enhance their accu-
racy when executing specific reasoning tasks. Scaling Law
stands as a fundamental theoretical framework for predicting
LLM training performance. The authors of [17] proposed
empirical scaling laws for LLMs performance on the cross-
entropy loss, which is one of the most important metrics of
LLMs training. They pointed out that the loss scales follow
a power-law with model size, dataset size, and the amount of
computation resources used for training. The authors of [18]
investigated the scaling behavior of LLMs in different learning
settings. They reported that downstream performance follows
a predictable log-law and providing sufficient distributional
alignment between the pretraining and downstream datasets


--- Page 3 ---
IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. X, XX XXXX
3
is important. The authors of [19] focused on scaling law of
reasoning capability enhancement in post-training and reported
that lower-dimensional LLMs can achieve better reasoning
accuracy by increasing the quantity of high-quality training
data. The authors of [20] studied the scaling behaviors of
LLMs during deep reinforcement learning (DRL)-based post-
training. Through extensive experiments across the Qwen2.5
models ranging from 0.5B to 72B parameters, they proposed
a scaling law for mathematical reasoning and validated its
accuracy. Scaling law provides essential guidance for MD-
deployed LLM systems formulation. How to realize a satis-
factory reasoning accuracy while enhancing computational and
energy efficiency needs further investigation.
B. Agentic AI-empowered reasoning
Reasoning tasks constitute a significant proportion of user
requests, requiring complex logical deduction rather than sim-
ple data computation. Agentic AI has been recognized as a
promising framework to provision QoS of these tasks. The
authors of [21] proposed a novel â€œchain-of-thoughtâ€ frame-
work. Experiments verified that the proposed framework could
significantly enhance the accuracy of single AI agent when
executing reasoning tasks. Based on this idea, the authors
of [9] proposed the ReAct framework, which enables AI
agents to invoke external tools to gather real-time environ-
ment information. Experiments demonstrated that the proposed
framework can effectively reduce hallucinations and further
improve the reliability of the reasoning process. The authors
of [22] proposed a novel self-consistency strategy to replace
the greedy decoding method employed in â€œchain-of-thoughtâ€
framework. By sampling diverse reasoning paths, this method
effectively exploits the redundancy in reasoning processes
to further enhance reasoning accuracy. The authors of [23]
proposed a novel agentic AI-based reasoning framework and
pointed out that the collaboration among multiple AI agents
during the reasoning process can significantly enhance the
accuracy and reliability of reasoning tasks.
Majority voting is a widely adopted aggregation scheme
in multi-agent reasoning, where the system selects the final
answer based on the highest support among independent AI
agents. The authors of [24] analyzed the multi-agent debate
process and revealed that majority voting contributes to the
primary performance gains. Similarly, the authors of [25] com-
pared various decision protocols and demonstrated that voting
schemes tend to yield larger improvements for reasoning tasks.
As such, majority voting is regarded as an effective scheme to
achieve satisfactory reasoning accuracy without significantly
increasing network complexity.
C. Open challenges of wireless agentic AI networks
Resource allocation schemes are important for QoS pro-
visioning in wireless agentic AI networks. Since agentic AI
requires continuous data exchange between AI agents and
intensive model inference, joint optimization of communi-
cation and computing resources becomes essential to satisfy
latency and accuracy requirements of complex reasoning tasks.
The authors of [26] proposed a DRL-based resource alloca-
tion scheme for edge-deployed LLM inference. Experiments
verified that the proposed scheme could reduce inference
latency while realizing satisfactory execution success rate.
The authors of [27] formulated a cooperative LLM inference
model. The throughput maximization problem considering
batch scheduling and resource allocation was formulated and
an efficient heuristic algorithm was proposed to tackle the
formulated problem. The simulation results verified that the
proposed algorithm can reduce time complexity significantly.
The authors of [28] proposed a wireless distributed mixture
of experts LLM network, and formulated inference latency
minimization problem subject to inference accuracy constraint.
A dynamic expert selection policy with bandwidth allocation
was proposed. Experiments verified that the proposed scheme
can reduce the inference latency without compromising LLM
performance. Although some recent works have investigated
resource allocation for LLM inference in wireless networks,
existing schemes cannot satisfy the unique requirements of
wireless agentic AI networks. First, the collaborative nature
of agentic AI involves significantly more complex data inter-
action between AI agents compared to standard LLM infer-
ence. Second, current approaches often overlook the energy
optimization of AI agents. How to minimize network energy
consumption while maintaining reasoning accuracy remains
a critical unresolved issue. Furthermore, security issue is a
major concern in wireless agentic AI networks. The frequent
data exchange among AI agents over open wireless channels
increases vulnerability to eavesdropping. How to protect the
confidentiality of sensitive user data and reasoning results dur-
ing inter-agent communication remains a significant challenge.
III. SYSTEM MODEL AND PROBLEM FORMULATION
As shown in Fig. 2, we propose a secure wireless agentic
AI network, where a supervisor AI agent As and a set of
N
= {A1, A2, Â· Â· Â· , AN} AI agents dynamically form an
agentic AI network for cooperative reasoning. As is deployed
on an edge server integrated with an L-antenna BS. Each An
runs on its respective single-antenna MD and can communicate
with As via wireless channels. An is equipped with a post-
trained LLM and AI tools, which can be utilized for specific
reasoning tasks and environment perception, respectively. The
knowledge database is hosted by As to provide up-to-date
private knowledge for Anâ€™s retrieval-augmented reasoning.
Moreover, there is a single-antenna eavesdropper (eve) in the
network attempting to obtain the reasoning task information,
private knowledge, and reasoning outputs.
Consider a reasoning task U â‰œ{s, D, o}, where s is the
length of input prompt, D represents the data size of required
private knowledge and o denotes the reasoning output. As
dispatches other AI agents to execute the reasoning task.
Let Î±n be the AI agent selection indicator, where Î±n = 1
represents As selecting An to execute U while Î±n = 0
otherwise. As transmits the input prompt and required private
knowledge to selected AI agents simultaneously. Each selected
AI agent can start task execution after receiving input prompt
and private knowledge and returns the reasoning output to


--- Page 4 ---
IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. X, XX XXXX
4
Reasoning 
request
AI agent 
assignment
Evaluation & Voting
ğ‘œğ‘œ= ğ’±ğ’±(ğ›¼ğ›¼1ğ‘œğ‘œ1, ğ›¼ğ›¼2ğ‘œğ‘œ2, â€¦ , ğ›¼ğ›¼ğ‘ğ‘ğ‘œğ‘œğ‘ğ‘)
â€¦
Wireless channels
Input prompt &
Private knowledge
Reasoning output
Cooperative reasoning
Supervisor AI agent
LLM
Post-trained LLM
BS
Knowledge database
Prompt
construction
â€¦
Friendly jamming
Eavesdropper
Edge sever
Downlink channel
Uplink channel
Jamming channel
Artificial nosie
Environment
â€¦
Weather condition
Traffic congestion
Network status
Environment perception
AI agent
AI tools
AI agent
AI tools
AI agent
AI tools
AI agent
AI tools
Fig. 2: The framework of a secure wireless agentic AI network.
As. For unselected AI agents, they transmit artificial noise
to function as friendly jammers in the downlink and uplink
transmission processes.
A. Secure communication model
As transmits the input prompt and private knowledge to
selected AI agents simultaneously, while other unselected
AI agents function as friendly jammers. Assume that An is
selected to execute reasoning task and Anâ€² injects artificial
noise to degrade eveâ€™s interception performance. Let hBS, n âˆˆ
CLÃ—1, hBS, eve âˆˆCLÃ—1, hnâ€™, n and hnâ€™, eve be the channel gain
between BS and An, BS and eve, An and Anâ€², Anâ€² and eve,
respectively1. Denote the downlink beamforming vector by w.
One has
||w||2 â‰¤ptr
BS,
(1)
where ptr
BS is the transmission power of BS. The received signal
at An can be given as
yn = hH
BS,nwxBS +
X
nâ€²âˆˆN
(1 âˆ’Î±nâ€²)hnâ€™, n
q
ptr
nâ€²vnâ€² + Ïƒ, n âˆˆN,
(2)
where xBS represents the transmitted data symbol of BS with
E{|xBS|2} = 1. vnâ€² is the artificial noise with vn âˆ¼CN(0, 1).
ptr
nâ€² denotes the transmission power of Anâ€². Ïƒ represents the
noise variance. The downlink data rate at An can be given as
rdl
n = log2(1 +
||hH
BS, nw||2
P
nâ€²âˆˆN (1 âˆ’Î±nâ€²)h2
nâ€™, nptr
nâ€² + Ïƒ2 ), n âˆˆN,
(3)
1In practical wireless scenarios, the eve typically remains passive and
traditional channel state information (CSI) acquisition methods are ineffective
for estimating the link between the transmitter and eve. Recently, integrated
sensing and communications (ISAC) has been recognized as a potential
framework to overcome this issue [29], [30]. By employing pre-sensing stages,
including beam scanning or environment probing, the transmitter can estimate
eveâ€™s angular and geometric parameters. This allows for the precise estimation,
or a reliable approximation, of the eveâ€™s CSI even without its cooperation.
Following this rationale, we assume that the CSI between the BS and eve,
each AI agent and eve, is available for the design of transmission strategies.
The achievable downlink data rate can be given as
rdl = min
âˆ€nâˆˆN
Î±n=1
{rdl
n }.
(4)
The downlink eavesdropping data rate at eve can be given as
rdl
eve = log2(1 +
||hH
BS, evew||2
P
nâ€²âˆˆN (1 âˆ’Î±nâ€²)h2
nâ€™, eveptr
nâ€² + Ïƒ2 ).
(5)
Based on (4) and (5), the downlink secrecy capacity is
cdl = [rdl âˆ’rdl
eve]+,
(6)
where [Â·]+ = max{Â·, 0}. Let B be the bandwidth resource
of BS. The latency for BS to transmit the input prompt and
private knowledge can be given as
tdl = s + D
B Â· cdl .
(7)
After all selected AI agents finish task execution, they can
transmit their reasoning outputs back to As simultaneously.
Let xn be the transmitted data symbol of An with E{|xn|2} =
1. The received signal at BS is
yBS =
X
nâˆˆN
Î±nhH
BS, n
p
ptr
n xn +
X
nâ€²âˆˆN
(1 âˆ’Î±nâ€²)hH
BS, nâ€™
q
ptr
nâ€²vnâ€² + ÏƒI,
(8)
where ptr
n represents the transmission power of An. I âˆˆCLÃ—1
is a unit vector. The uplink data rate of BS to decode reasoning
output from An is
rul
n = log2(1 +
||hH
BS,n
p
ptr
n ||2
P
nâ€²âˆˆN \{n} ||hH
BS,nâ€™
p
ptr
nâ€²||2 + Ïƒ2 ), n âˆˆN.
(9)
The uplink eavesdropping data rate at eve can be given as
rul
n, eve = log2(1 +
|hn, eve|2ptr
n
P
nâ€²âˆˆN \{n} h2
nâ€™, eveptr
nâ€² + Ïƒ2 ), n âˆˆN.
(10)
Based on (9) and (10), the uplink secrecy capacity is
cul
n = [rul
n âˆ’rul
n,eve]+.
(11)


--- Page 5 ---
IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. X, XX XXXX
5
As such, the latency for An to transmit reasoning output is
tul
n =
on
B Â· cul
n
, n âˆˆN,
(12)
where on represents the reasoning output of An.
B. Reasoning model
Consider the cooperative reasoning process. The number
of selected AI agents cannot be less than the predetermined
threshold Nmin. One has
X
nâˆˆN
Î±n â‰¥Nmin.
(13)
Let kn be the dimension of the LLM equipped by An. To
enhance the reasoning capabilities of specific tasks, AI agent
equipped LLMs need to undergo post-training. According to
the scaling law of LLM training, the accuracy of an AI agent in
completing one specific type of reasoning task can be predicted
by its LLM model scale and the computational resources
allocated during post-training [17], [18], [19], which is
Accn = 1 âˆ’exp(âˆ’M(kn) Â· log Cn + R(kn)), n âˆˆN.
(14)
In (14), M(kn) represents the learning efficiency, which is a
function of kn. Cn represents the training budget, including
training tokens and computation resources, used during train-
ing of An. R(kn) is a constant related to scale and structure
of the LLM equipped by An.
To guarantee the QoS of cooperative reasoning, the average
accuracy of selected AI agents cannot be less than a predefined
threshold Accmin. One has
P
nâˆˆN Î±n Â· Accn
P
nâˆˆN Î±n
â‰¥Accmin.
(15)
In the cooperative reasoning, An first employs AI tools
to collect environmental data, leveraging up-to-date sensory
data to ensure the accuracy of its reasoning. The tool calling
information is generated by its LLM after receiving the
input prompt and private knowledge. After obtaining required
environmental contexts, An can execute the assigned reasoning
task. The inference latency of An is
tinference
n
= (Î· + 1)kn + kn(qn + on) + kn(q2
n + o2
n)
fn
, n âˆˆN,
(16)
where qn denotes the length of the tool calling information.
Î· represents the floating point operations (FLOPs) required
by the network activation process of LLM [27]. fn represents
computing capability.
Then, An transmits reasoning output to As for evaluation.
After receiving reasoning outputs of assigned AI agents, As
can obtain the final output using a voting function V(Â·), which
can be given as o = V(Î±1o1, Î±2o2, Â· Â· Â· , Î±non) [24].
C. The overall process
As selects other AI agents to participate in cooperative
reasoning and other unselected AI agents perform as friendly
jammers. Then, As transmits input prompt and private knowl-
edge to selected AI agents simultaneously. After all selected
AI agents finish executing the assigned reasoning task, they
transmit reasoning output back to As for evaluation simulta-
neously. Let Ï„ be the maximum execution latency. One has
the latency constraint
tdl + max
âˆ€nâˆˆN
Î±n=1
{tinference
n
} + max
âˆ€nâˆˆN
Î±n=1
{tul
n } â‰¤Ï„.
(17)
The transmission energy consumption of An is formulated as
Etr
n = ptr
n (Î±ntul
n + (1 âˆ’Î±n)(tdl + max
âˆ€nâˆˆN
Î±n=1
{tul
n })), n âˆˆN.
(18)
Let Eexe
n
be the execution energy consumption of An, which
can be given as
Eexe
n
= Î±npexe
n tinference
n
, n âˆˆN,
(19)
where pexe
n
represents the computing power of An.
D. Problem formulation
In this paper, we aim to minimize the overall energy
consumption of AI agents by jointly optimizing AI agent
selection Î± â‰œ{Î±n, n âˆˆN}, BS beamforming vector w, and
AI agent transmission powers p â‰œ{ptr
n , n âˆˆN}, which can
be formulated as
min
Î±,w,p
X
nâˆˆN
Eexe
n + Etr
n,
(20a)
s. t.
(1), (13), (15), (17),
Î±n âˆˆ{0, 1}, n âˆˆN,
(20b)
0 â‰¤ptr
n â‰¤ptr,max
n
, n âˆˆN,
(20c)
where (20b) is the binary constraint of AI agent selection
indicator Î± and (20c) ensures the transmission power of each
An cannot exceed ptr,max
n
.
IV.
THE PROPOSED SOLUTIONS
The formulated optimization problem is a mixed-integer
non-linear optimization problem, which is extremely difficult
to tackle. To efficiently solve this problem, this section intro-
duces two resource allocation scheme, i.e., ASC, a ADMM-
SDR-SCA Coordinated sheme and LAW, a LLM-enabled
Agentic Workflow-based scheme. The detailed information
regarding the proposed solutions is given as follows.
A. ASC
In this subsection, we first introduce ASC to solve (20).
The original optimization problem is decoupled into three sub-
problems, i.e., AI agent selection, beamforming design and
transmission power optimization sub-problems. In particular,
the AI agent selection sub-problem is solved by the proposed
ADMM-based algorithm, while the beamforming design and
transmission power optimization sub-problems are tackled by
SDR and SCA, respectively. Each sub-problem is solved in an
iterative manner until convergence.


--- Page 6 ---
IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. X, XX XXXX
6
1) AI agent Selection: For any given feasible w and p, (20)
can be reduced as
min
Î±
X
nâˆˆN
Eexe
n + Etr
n,
s. t.
(13), (15), (17),
Î±n âˆˆ{0, 1}, n âˆˆN.
(21)
Problem (21) is extremely challenging to tackle due
to
the
complex
dependency
of
the
objective
function.
First, Î± is closely coupled within Etr
n
and in constraints
(13) and (15). The term rdl
=
min
âˆ€nâˆˆN,Î±n=1{log2(1 +
||hH
BS, nw||2
P
nâ€²âˆˆN (1âˆ’Î±nâ€²)h2
nâ€™, nptr
nâ€²+Ïƒ2 )} is non-smooth, which is difficult
to transformed into a closed-form algebraic function of Î±.
Second, even with fixed rdl, the binary nature of Î±n âˆˆ{0, 1}
classifies (21) as a combinatorial non-linear integer program-
ming problem, which is known to be NP-hard. Widely used ap-
proaches, such as branch-and-bound or genetic algorithms may
become inefficient to solve (21). To address these challenges,
an ADMM-based algorithm is proposed. First, an auxiliary
variable Ë†Î± â‰œ{Ë†Î±n, n âˆˆN} is introduced, which should satisfy
Ë†Î±n = Î±n, n âˆˆN.
(22)
One can transform Etr
n into Ë†Etr
n via substituting Î± in rdl, rdl
eve
and
max
âˆ€nâˆˆN,Î±n=1{
on
BÂ·[rul
n âˆ’rul
n,eve]+ } by Ë†Î±. Similarly, (17) can be
transformed as
Ë†tdl + max
âˆ€nâˆˆN
Ë†Î±n=1
{tinference
n
} + max
âˆ€nâˆˆN
Ë†Î±n=1
{tul
n } â‰¤Ï„,
(23)
where Ë†tdl can be obtained via substituting Î± by Ë†Î±. As such,
(21) can be transformed into
min
Î±, Ë†Î±
X
nâˆˆN
Ë†Eexe
n + Etr
n,
s. t.
(13), (15), (22), (23),
Î±n âˆˆ{0, 1}, n âˆˆN.
(24)
The corresponding augmented Lagrangian function of (24) can
be formulated as
L =
X
nâˆˆN
( Ë†Eexe
n + Etr
n) + Ï||Î± âˆ’Ë†Î±||2
2,
(25)
where Ï is the penalty coefficient of (22). One can observe
that L is separable along with Î± and Ë†Î±. (24) can be solved
by optimizing Î± and Ë†Î± in an iterative manner.
(a). The optimization of Î±: Let Ë†Î±rADMMâˆ’1 be the obtained
solution of Ë†Î± in the rADMM âˆ’1-th iteration. The optimization
problem of Î± in the the rADMM-th iteration can be reduced as
min
Î±
X
nâˆˆN
( Ë†Eexe
n + Etr
n) + Ï||Î± âˆ’Ë†Î±rADMMâˆ’1||2
2,
s. t.
(13), (15), Î±n âˆˆ{0, 1}, n âˆˆN.
(26)
In this paper, a LR-based algorithm is proposed to tackle
(26). Let Âµ and Î» be the Lagrangian multipliers of (13) and
(15), respectively. The Lagrangian function of (26) can be
formulated as (27). In this way, (26) can be reformulated as
max
Âµ,Î»
{min
Î±
LLR},
s. t. Î±n âˆˆ{0, 1}, n âˆˆN, Âµ â‰¥0, Î» â‰¥0.
(28)
Let Î±rLRâˆ’1(ÂµrLRâˆ’1, Î»rLR) â‰œ{Î±rLR
n , n âˆˆN} be the rLR-th
feasible solution to (28). For any given Lagrangian multipliers
ÂµrLRâˆ’1 and Î»rLRâˆ’1, the update mechanism of Î±rLR(ÂµrLR, Î»rLR)
can be given as
Î±rLR
n
=
(
1,
Ï + brLRâˆ’1
n
â‰¤0,
0,
otherwise,
(29)
where
brLRâˆ’1
n
=
ptr
n tul
n
+ pexe
n tinference
n
âˆ’
s+D
BÂ·Ë†cdl
âˆ’
max
âˆ€nâˆˆN
Ë†Î±
rADMMâˆ’1
n
=1
{
on
BÂ·cul
n } + ÂµrLRâˆ’1 + Î»rLRâˆ’1(Accn âˆ’Accmin) âˆ’
2ÏË†Î±rADMMâˆ’1
n
. One can utilize the subgradient method to update
the Lagrangian multipliers in each iteration. In particular, the
update mechanism of Âµ can be given as
ÂµrLR = max{ÂµrLRâˆ’1 + ÏˆrLR(Nmin âˆ’
X
nâˆˆN
Î±rLR
n ), 0},
(30)
where ÏˆrLR âˆˆ(0, 1) represents the proportionality coefficient
of ÂµrLR. The update mechanism of Î» can be given as
Î»rLR = max{Î»rLRâˆ’1 + Ï‰rLR(
X
nâˆˆN
Î±rLR
n (Accn âˆ’Accmin)), 0},
(31)
where Ï‰rLR âˆˆ(0, 1) is the proportionality coefficient of Î»rLR.
Let rmax
LR be the maximum number of iterations of the proposed
LR-based algorithm. The LR-based algorithm can be regarded
as convergence when rLR = rmax
LR .
Due to the relaxation of constraints, the obtained Î±rmax
LR may
not satisfy (13) and (15). To address this issue, a greedy
method is employed to reconstruct Î±rmax
LR . The modified AI
agent selection indicator Ë†Î±rmax
LR
can be utilized to obtain the
feasible solution Î±rADMM of (26).
(b). The optimization of Ë†Î±: For any given Î±rADMM, (28) can
be reduced as
min
Ë†Î±
X
nâˆˆN
( Ë†Eexe
n + Etr
n) + Ï|| Ë†Î± âˆ’Î±rADMMâˆ’1||2
2,
s. t.
(23), Ë†Î±n âˆˆ{0, 1}, n âˆˆN.
(32)
Problem (32) is difficult to tackle. First, Ë†rdl cannot be trans-
formed into a close-form analytical expression of Ë†Î±. Second,
the binary constraint leads to traditional convex optimization
algorithms hard to tackle (32). To settle this challenging
optimization problem, we first relax Ë†Î± to the continuous
domain. One has
Ë†Î±n âˆˆ[0, 1], n âˆˆN.
(33)
Then, a penalty-based method is utilized to transform Ë†rdl into
rdl. One has
rdl â‰œmin
âˆ€nâˆˆN{Ë†rdl
n + (1 âˆ’Ë†Î±n)â„¦},
(34)
where Ë†rdl
n can be obtained via substituting Î± by Ë†Î±. â„¦is a
sufficiently large penalty coefficient. This penalty term ensures
that if Ë†Î±n = 0, rdl
n + (1 âˆ’Ë†Î±n)â„¦becomes significantly larger
than the data rates of AI agents, thereby effectively excluding


--- Page 7 ---
IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. X, XX XXXX
7
LLR(Î±, Âµ, Î») =
X
nâˆˆN
( Ë†Eexe
n + Etr
n) + Ïâˆ¥Î± âˆ’Ë†Î±rADMMâˆ’1âˆ¥2
2 + Âµ(
X
nâˆˆN
Î±n âˆ’Nmin) + Î»(
X
nâˆˆN
Î±nAccn âˆ’Accmin
X
nâˆˆN
Î±n)
=
X
nâˆˆN
ÏÎ±2
n + Î±n(ptr
n tul
n + pexe
n tinference
n
âˆ’s + D
B Â· Ë†cdl âˆ’
max
âˆ€nâˆˆN
Ë†Î±
rADMMâˆ’1
n
=1
{
on
B Â· cul
n
} + Âµ + Î»(Accn âˆ’Accmin) âˆ’2ÏË†Î±rADMMâˆ’1
n
)
+ Ï(Ë†Î±rADMMâˆ’1
n
)2.
(27)
{âˆ’Ë†rdl
n }ub â‰œâˆ’log2
 
1 +
||hH
BS,nw||2
P
nâ€²âˆˆN (1 âˆ’Ë†Î±rADMMâˆ’1
nâ€²
)h2
nâ€²,nptr
nâ€² + Ïƒ2
!
âˆ’
X
nâ€²âˆˆN
||hH
BS,nw||2h2
nâ€²,nptr
nâ€²(Ë†Î±nâ€² âˆ’Ë†Î±rADMMâˆ’1
nâ€²
)
P
nâ€²âˆˆN (1 âˆ’Ë†Î±rADMMâˆ’1
nâ€²
)h2
nâ€²,nptr
nâ€² + Ïƒ2
 P
nâ€²âˆˆN (1 âˆ’Ë†Î±rADMMâˆ’1
nâ€²
)h2
nâ€²,nptr
nâ€² + Ïƒ2 + ||hH
BS,nw||2

(39)
An from the minimization operation and Ë†Î±n = 1 otherwise.
After removing irrelevant parameters, (32) can be rewritten as
min
Ë†Î±
X
nâˆˆN
(1 âˆ’Î±rADMM
n
)(
s + D
B Â· [rdl âˆ’rdl
eve]+
+ max
âˆ€nâˆˆN{
Ë†Î±non
B Â· [rul
n âˆ’rul
n,eve]+ }) + Ï|| Ë†Î± âˆ’Î±rADMMâˆ’1||2
2,
s. t.
(23), (33).
(35)
However, (35) is still difficult to tackle due to the non-
smooth objective function and non-convex constraint (23). An
auxiliary variable Ë†cdl is introduced. (23) can be transformed
into a convex constraint, which can be given as
s + D
Ë†cdl
+ max
âˆ€nâˆˆN{Ë†Î±ntinference
n
} + max
âˆ€nâˆˆN{Ë†Î±ntul
n } â‰¤Ï„.
(36)
As such, (35) can be transformed into
min
Ë†
Î±,Ë†cdl
X
nâˆˆN
(1 âˆ’Î±rADMM
n
)( s + D
B Â· Ë†cdl + max
âˆ€nâˆˆN{
Ë†Î±non
B Â· [rul
n âˆ’rul
n,eve]+ })
+ Ï|| Ë†Î± âˆ’Î±rADMMâˆ’1||2
2,
(37a)
s. t.
(33), (36),
Ë†cdl â‰¥0,
(37b)
Ë†cdl â‰¤rdl âˆ’rdl
eve.
(37c)
However, problem (37) is still non-convex due to the non-
convexity of (37c). To address this problem, SCA is utilized
to construct the convex approximation of (37c). (37c) can be
rewritten as
Ë†cdl + Ë†rdl
eve + max
âˆ€nâˆˆN{âˆ’Ë†rdl
n âˆ’(1 âˆ’Ë†Î±n)â„¦} â‰¤0.
(38)
One can observe that âˆ’Ë†rdl
n is concave with respect to Ë†Î±. The
upper bound of âˆ’Ë†rdl
n can be obtained via employing the first
order Taylor expansion, which can be formulated as (39).
Accordingly, (38) can be transformed into
Ë†cdl + Ë†rdl
eve + max
âˆ€nâˆˆN{{âˆ’Ë†rdl
n }ub âˆ’(1 âˆ’Ë†Î±n)â„¦} â‰¤0,
(40)
which is a convex constraint. Consequently, (37) can be
transformed into
min
Ë†Î±,Ë†cdl
X
nâˆˆN
(1 âˆ’Î±rADMM
n
)( s + D
B Â· Ë†cdl + max
âˆ€nâˆˆN{
Ë†Î±no
B Â· [rul
n âˆ’rul
n,eve]+ })
+ Ï|| Ë†Î± âˆ’Î±rADMMâˆ’1||2
2,
s. t.
(33), (36), (37b), (40),
(41)
which is a convex optimization problem and can be efficiently
solved. Let Ë†Î±âˆ—â‰œ{Ë†Î±âˆ—
n, n âˆˆN} be the obtained solution of
(41). We recover the binary values of Ë†Î± by setting Ë†Î±rADMM
n
= 1
if Ë†Î±âˆ—
n â‰¥0.5, and Ë†Î±rADMM
n
= 0 otherwise.
Let rmax
ADMM be the maximum number of iterations of the pro-
posed ADMM-based algorithm. The proposed ADMM-based
algorithm can be regarded as convergence when rADMM =
rmax
ADMM. As such, the feasible AI agent selection Î±r can be
obtained. The detailed information regarding the proposed
ADMM-based algorithm can be found in Algorithm 1.
2) Beamforming design:
For any given Î± and p and
according to (7) and (18), (20) can be reduced as
max
w
cdl,
s. t. (1).
(42)
cdl is non-convex with respect to w due to the existence of
min{Â·} function coupled in rdl. To address this problem, an
auxiliary variable, i.e., Ëœrdl is introduced linearize the objective
function. (42) can be transformed into
max
w,Ëœrdl Ëœrdl âˆ’rdl
eve,
(43a)
s. t.
(1),
Ëœrdl â‰¤rdl
n , n âˆˆN.
(43b)
In this paper, SDR is utilized to tackle (43). One has
||hH
BS, nw||2 = hH
BS, nwwHhBS, n = tr(hH
BS, nwwHhBS, n) =
tr(hH
BS, nhBS, nwwH). Let Hn = hH
BS, nhBS, n âˆˆCLÃ—L and
W
= wwH
âˆˆCLÃ—L, one can obtain ||hH
BS, nw||2
=
tr(HnW ). Similarly, let Heve = hH
BS, evehBS, eve âˆˆCLÃ—L. One
has ||hH
BS, evew||2 = tr(HeveW ). (1) can be rewritten as
tr(W ) â‰¤ptr
BS.
(44)


--- Page 8 ---
IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. X, XX XXXX
8
Algorithm 1: The framework of the proposed ADMM-
based algorithm
1 Initialize Î±0, Ë†Î±0, Î»0, Âµ0, rADMM = 0 and rLR = 0.
2 while rADMM â‰¤rmax
ADMM do
3
Fix Ë†Î±rADMM, construct Lagrangian function
according to (27).
4
while rLR â‰¤rmax
LR do
5
Update Î±rLR+1 according to (29).
6
Update Î»rLR+1 and ÂµrLR+1 according to (30)
and (31).
7
rLR â†rLR + 1.
8
end
9
if Î±rmax
LR cannot satisfy (13) or (15) then
10
Perform the greedy method to reconstruct
Î±rmax
LR .
11
end
12
Î±rADMM+1 â†Î±rmax
LR .
13
Fix Î±rADMM, solve (41) and obtain Ë†Î±âˆ—.
14
Obtain Ë†Î±rADMM+1 by rounding Ë†Î±âˆ—to the nearest
integers.
15
rADMM â†rADMM + 1.
16 end
17 Return Î±rmax
ADMM.
The objective function of (43) can be transformed into
Ëœrdl âˆ’rdl
eve = Ëœrdl âˆ’log2(1 +
tr(HeveW )
P
nâ€²âˆˆN (1 âˆ’Î±nâ€²)h2
nâ€™, eveptr
nâ€² + Ïƒ2 ).
(45)
However, (45) is non-convex with respect to W . One can
observe that âˆ’log2(1 +
tr(HeveW )
P
nâ€²âˆˆN (1âˆ’Î±nâ€²)h2
nâ€™, eveptr
nâ€²+Ïƒ2 ) is convex
with respect to tr(HeveW ). The lower bound of âˆ’rdl
eve can
be obtained via employing the first order Taylor expansion,
which can be formulated as
{âˆ’rdl
eve}lb â‰œâˆ’log2(1 +
tr(HeveW râˆ’1)
P
nâ€²âˆˆN (1 âˆ’Î±nâ€²)h2
nâ€™, eveptr
nâ€² + Ïƒ2 )
âˆ’
Heve(W âˆ’W râˆ’1)
ln2(P
nâ€²âˆˆN (1 âˆ’Î±nâ€²)h2
nâ€™, eveptr
nâ€² + Ïƒ2 + tr(HeveW râˆ’1)),
(46)
where W râˆ’1 represents the obtained feasible solution in the
(râˆ’1)-th iteration. According to (3), (43b) can be transformed
into
tr(HnW ) â‰¥(
X
nâ€²âˆˆN
(1 âˆ’Î±nâ€²)h2
nâ€™, nptr
nâ€² + Ïƒ2)(2Ëœrdl âˆ’1), n âˆˆN,
(47)
which is a convex constraint. Note that W should satisfy
Rank(W ) = 1.
(48)
As such, (43) can be transformed into
max
W ,Ëœrdl Ëœrdl + {âˆ’rdl
eve}lb,
s. t. (44), (47), (48).
(49)
One can observe that by relaxing the Rank-1 constraint (48),
(49) can be transformed into a convex optimization problem,
which can be solved efficiently. Let W âˆ—be the obtained
solution of (49). The optimized beamforming vector wr can
be obtained via Gaussian randomization based on W âˆ—[31].
3) Transmission power optimization: For any given feasible
Î± and w, (20) can be reduced as
min
p
X
nâˆˆN
Etr
n,
s. t. (17), (20c).
(50)
Problem (50) is difficult to tackle due to the non-smooth ob-
jective function and non-convex constraint (17). Two auxiliary
variables, i.e., Ë™cdl and Ë™cul â‰œ{Ë™cul
n , n âˆˆN} are introduced. (17)
can be transformed into
s + D
Ë™cdl
+
max
âˆ€nâˆˆN ,Î±n=1{tinference
n
} +
max
âˆ€nâˆˆN ,Î±n=1{ on
Ë™cul
n
} â‰¤Ï„.
(51)
Problem (50) can be reformulated as
min
p, Ë™cdl, Ë™cul
X
nâˆˆN
ptr
n (Î±n
on
Ë™cul
n
+ (1 âˆ’Î±n)(s + D
Ë™cdl
+ max
âˆ€nâˆˆN
Î±n=1
{ on
Ë™cul
n
})),
(52a)
s. t.
(20c), (51)
Ë™cdl â‰¥0,
(52b)
Ë™cdl â‰¤rdl âˆ’rdl
eve,
(52c)
Ë™cul
n â‰¥0, n âˆˆN,
(52d)
Ë™cul
n â‰¤rul
n âˆ’rul
n,eve, n âˆˆN.
(52e)
One can observe that (52c) and (52e) are non-convex con-
straints. First, we consider the convex approximation of (52c).
According to (3)-(4), rdl can be rewritten as
rdl =
min
âˆ€nâˆˆN ,Î±n=1{log2(1 +
||hH
BS, nw||2
P
nâ€²âˆˆN (1 âˆ’Î±nâ€²)h2
nâ€™, nptr
nâ€² + Ïƒ2 )}
=
min
âˆ€nâˆˆN ,Î±n=1{log2(||hH
BS, nw||2 +
X
nâ€²âˆˆN
(1 âˆ’Î±nâ€²)h2
nâ€™, nptr
nâ€² + Ïƒ2)
âˆ’log2(
X
nâ€²âˆˆN
(1 âˆ’Î±nâ€²)h2
nâ€™, nptr
nâ€² + Ïƒ2)}.
(53)
One
can
observe
that
log2(||hH
BS, nw||2 + P
nâ€²âˆˆN (1 âˆ’
Î±nâ€²)h2
nâ€™, nptr
nâ€² + Ïƒ2) and log2(P
nâ€²âˆˆN (1 âˆ’Î±nâ€²)h2
nâ€™, nptr
nâ€² + Ïƒ2)
are convex with respect to p. The lower bound of rdl can
be obtained via employing the first Taylor expansion of
log2(||hH
BS, nw||2 +P
nâ€²âˆˆN (1âˆ’Î±nâ€²)h2
nâ€™, nptr
nâ€² +Ïƒ2), which can
be given as (54), where prâˆ’1 â‰œ{ptr,râˆ’1
n
, n âˆˆN} represents
the obtained feasible solution in the râˆ’1-th iteration. Similarly,
rdl
eve can be rewritten as log2(P
nâ€²âˆˆN (1âˆ’Î±nâ€²)h2
nâ€™, eveptr
nâ€² +Ïƒ2+
||hH
BS, evew||2) âˆ’log2(P
nâ€²âˆˆN (1 âˆ’Î±nâ€²)h2
nâ€™, eveptr
nâ€² + Ïƒ2) and
the upper bound of rdl
eve, denoted by rdl,ub
eve
can be obtained
by employing the first Taylor expansion of log2(P
nâ€²âˆˆN (1 âˆ’
Î±nâ€²)h2
nâ€™, eveptr
nâ€² + Ïƒ2). (52c) can be transformed into
Ë™cdl â‰¤rdl,lb âˆ’rdl,ub
eve
,
(55)
which is a convex constraint. Then, the convex approximation
of (52e) is considered. The lower bound and upper bound of
rul
n and rul
n,eve, denoted by rul,lb
n
and rul,ub
n,eve respectively, can be
obtain in the similar fashion to rdl,lb and rdl,ub
eve
. As such, (52e)
can be transformed into
Ë™cul
n â‰¤rul,lb
n
âˆ’reve,ub
n
.
(56)


--- Page 9 ---
IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. X, XX XXXX
9
rdl,lb = min
âˆ€nâˆˆN
Î±n=1
{log2(||hH
BS, nw||2 +
X
nâ€²âˆˆN
(1 âˆ’Î±nâ€²)h2
nâ€™, nptr,râˆ’1
nâ€²
+ Ïƒ2) +
P
nâ€²âˆˆN (1 âˆ’Î±nâ€²)h2
nâ€™, n(ptr
nâ€² âˆ’ptr,râˆ’1
nâ€²
)
ln2(||hH
BS, nw||2 + P
nâ€²âˆˆN (1 âˆ’Î±nâ€²)h2
nâ€™, nptr,râˆ’1
nâ€²
+ Ïƒ2)
âˆ’log2(
X
nâ€²âˆˆN
(1 âˆ’Î±nâ€²)h2
nâ€™, nptr
nâ€² + Ïƒ2)},
(54)
Consequently, (52) can be transformed into
min
p, Ë™cdl, Ë™cul
X
nâˆˆN
ptr
n (Î±n
on
Ë™cul
n
+ (1 âˆ’Î±n)(s + D
Ë™cdl
+ max
âˆ€nâˆˆN
Î±n=1
{ on
Ë™cul
n
})),
s. t.
(20c), (51), (52b), (52d), (55), (56).
(57)
Problem (57) is a convex optimization problem, which can be
solved efficiently. The optimized transmission power pr for
the current iteration can be obtained.
4) The overall process of ASC: The detailed information
of ASC is summarized in Algorithm 2. Let rmax be the
maximum number of iterations of the proposed solution. The
feasible solution of (20) can be obtained when the proposed
solution reaches convergence with r = rmax. The complexity
of the proposed ADMM-based algorithm can be estimated as
O(rmax
ADMM(rmax
LR N + N 3.5)). The complexities of solving (49)
and (57) can be given as O(L3.5) and O(N 3.5), respectively.
As such, the complexity of ASC can be roughly given as
O(rmax(rmax
ADMM(rmax
LR N + N 3.5) + L3.5 + N 3.5)).
Algorithm 2: The framework of ASC
1 Initialize Î±0, w0, p0 and r = 0.
2 while r â‰¤rmax do
3
Fix wrâˆ’1 and prâˆ’1, utilize Algorithm 1 to obtain
Î±r.
4
Fix Î±r and prâˆ’1, solve (49) to obtain wr.
5
Fix Î±r and wr, solve (57) to obtain pr.
6
r â†r + 1.
7 end
8 Return Î±rmax, wrmax and prmax.
Agentic-based AI 
agent selection
Agentic-based 
beamforming 
design
Agentic-based 
transmission power 
optimization
ğœ¶ğœ¶ğ‘Ÿğ‘Ÿ& ğ’‘ğ’‘ğ‘Ÿğ‘Ÿâˆ’1
ğœ¶ğœ¶ğ‘Ÿğ‘Ÿ& ğ’˜ğ’˜ğ‘Ÿğ‘Ÿâˆ’1
ğ’˜ğ’˜ğ‘Ÿğ‘Ÿ& ğ’‘ğ’‘ğ‘Ÿğ‘Ÿ
Initialize ğœ¶ğœ¶0, ğ’‘ğ’‘0, ğ’˜ğ’˜0
Convergence?
No
Output ğœ¶ğœ¶ğ‘Ÿğ‘Ÿğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š, ğ’˜ğ’˜ğ‘Ÿğ‘Ÿğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š, ğ’‘ğ’‘ğ‘Ÿğ‘Ÿğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š
Yes
Agentic-based beamforming design
LLM generation module
Satisfy 
constraint (1)?
ğ‘Ÿğ‘Ÿğ¿ğ¿ğ¿ğ¿ğ¿ğ¿< ğ‘Ÿğ‘Ÿğ¿ğ¿ğ¿ğ¿ğ¿ğ¿
ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š?
Output 
ğ’˜ğ’˜âˆ—
Compare ğ‘“ğ‘“(ğ’˜ğ’˜ğ‘Ÿğ‘Ÿğ¿ğ¿ğ¿ğ¿ğ¿ğ¿)
with ğ‘“ğ‘“(ğ’˜ğ’˜âˆ—)
Reminder prompt:
*Constraint violation warning: The norm
of generated beamforming vector cannot
exceed maximum transmission power.*
Replace ğ’˜ğ’˜âˆ—by ğ’˜ğ’˜ğ‘Ÿğ‘Ÿğ¿ğ¿ğ¿ğ¿ğ¿ğ¿
Task description
Output format: ğ¿ğ¿Ã— 1 vector
Output example: ğ’˜ğ’˜âˆ—and ğ‘“ğ‘“(ğ’˜ğ’˜âˆ—)
ğ‘“ğ‘“(ğ’˜ğ’˜ğ‘Ÿğ‘Ÿğ¿ğ¿ğ¿ğ¿ğ¿ğ¿)
< ğ‘“ğ‘“(ğ’˜ğ’˜âˆ—)
Expert Searching:
Perform local search based on ğ’˜ğ’˜ğ‘Ÿğ‘Ÿğ¿ğ¿ğ¿ğ¿ğ¿ğ¿to generate ğ“ ğ“ ğ‘Ÿğ‘Ÿğ¿ğ¿ğ¿ğ¿ğ¿ğ¿
Select top ğ’’ğ’’feasible beamforming design in ğ“ ğ“ ğ‘Ÿğ‘Ÿğ¿ğ¿ğ¿ğ¿ğ¿ğ¿
Construct an expert solution example pool
+
ğ‘“ğ‘“(ğ’˜ğ’˜ğ‘Ÿğ‘Ÿğ¿ğ¿ğ¿ğ¿ğ¿ğ¿)
> ğ‘“ğ‘“(ğ’˜ğ’˜âˆ—)
ğ’˜ğ’˜ğ‘Ÿğ‘Ÿğ¿ğ¿ğ¿ğ¿ğ¿ğ¿
LAW
Evaluation module
No
Yes
Yes
No
ğ“ ğ“ ğ‘Ÿğ‘Ÿğ¿ğ¿ğ¿ğ¿ğ¿ğ¿= {à·¥ğ’˜ğ’˜1
ğ‘Ÿğ‘Ÿğ¿ğ¿ğ¿ğ¿ğ¿ğ¿, â€¦ , à·¥ğ’˜ğ’˜ğ‘„ğ‘„
ğ‘Ÿğ‘Ÿğ¿ğ¿ğ¿ğ¿ğ¿ğ¿}
Fig. 3: The framework of LAW.
B. LAW
In this subsection, we introduce LAW to tackle (20). The
framework of LAW is demonstrated in Fig. 3. Similar to the
introduced ASC, LAW decouples (20) into three sub-problems
and each sub-problem is solved by the formulated LLM
optimizer based on an adaptive reflection agentic workflow
in an iterative manner.
In particular, the LLM optimizer includes two parts, i.e.,
LLM generation module and evaluation module. The LLM
generation module is responsible for generating feasible solu-
tions to three sub-problems. Consider the beamforming design
sub-problem shown in Fig. 3. The input prompt of LLM gen-
eration module consists of task description, output formatting
constraints, and output example. In the initialization stage, task
description outlining the objective function, constraints and
key parameters, output format of (42), and the output example
are input to LLM generation module. The output example is
denoted by wâˆ—and f(wâˆ—), which can enhance the continuity
in each interaction with LLM [32]. One should notice that f(Â·)
is the objective function of (20) and f(wâˆ—) can be obtained
via fixing AI agent selection and transmission power.
The evaluation module is responsible for evaluating the
quality of LLM generated feasible solution and providing
reflection to dynamically refine the LLMâ€™s output and enhance
the solution quality through closed-loop feedback. Denote the
feasible beamforming vector generated by LLM generation
module in the rLLM-th iteration by wrLLM. First, the evalu-
ation module analyzes whether wrLLM satisfies (1). If (1) is
violated, the evaluation module generates a reminder prompt
to emphasize that the LLM-generated solution must adhere to
the constraint, and sets f(wrLLM) = +âˆ. Next, consider two
conditions, if f(wrLLM) < f(wâˆ—), One can set wâˆ—= wrLLM
and add updated wâˆ—to the input prompts in the next iteration.
If f(wrLLM) â‰¥f(wâˆ—), an expert searching mechanism is
triggered to further exploit the solution space. Specifically, a
local neighborhood search is performed based on wrLLM to
generate a candidate pool QrLLM = { ËœwrLLM
1
, Â· Â· Â· ËœwrLLM
Q
}. Each
candidate is analyzed by their fitness values based on f(Â·), and
the top q feasible solutions in the candidate pool are selected to
construct an expert solution example pool. By incorporating
these high-quality solutions into the next iterationâ€™s prompt,
the framework significantly enhances the search capability
of the LLM. The agentic-based beamforming design can be
regarded as convergence when rLLM = rmax
LLM, where rmax
LLM
represents the maximum number of iterations.
The overall frameworks of agentic-based AI agent selection
and transmission power optimization are identical to agentic-
based beamforming design. The proposed LAW successively
optimize AI agent selection, beamforming design and trans-
mission power until r = rmax, where rmax denotes the maxi-


--- Page 10 ---
IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. X, XX XXXX
10
mum number of iterations of the proposed LAW. As such, the
feasible solution of (20) can be obtained. One should notice
that LAW can be employed by the supervisor AI agent directly.
By utilizing its equipped LLM as an intelligent optimizer, the
supervisor AI agent can dynamically allocate communication
and computing resources to other AI agents.
V. PERFORMANCE EVALUATION
In this section, we investigate the effectiveness of the
proposed solutions for the proposed secure wireless agentic AI
network by comparing them with several selected benchmark
algorithms. All experiments are conducted in Python 3.13.9
with CVXPY toolbox on a PC with Intel Core i7-12700K and
16GB RAM. The proposed LAW employs GPT-4-turbo as an
optimizer via commercial API in the performance analysis.
The key simulation parameters are summarized as follows. The
coordinates of BS and eve are set as [0, 0] m and [150, 150]
m, respectively. All AI agents are randomly distributed in a
square area of 100 m Ã— 100 m. We assume that wireless
channels in the secure wireless agentic AI network follow
Rayleigh fading with a mean of 10âˆ’PL(d)
20 , where PL(d) =
32.4 + 20lg(fcarrier) + 20lg(d). d is the distance between
the transmitter and the receiver. fcarrier denotes the carrier
frequency and is set to 3.5 GHz. The lengths of input prompt
and reasoning output of a reasoning task are set as 2048 and
4096 tokens, respectively. Each token is represented by a 16-
bit integer index. Other important parameters are given in
TABLE I.
TABLE I: Simulation parameters [28], [33].
Notation
Value
Notation
Value
ptr
BS
10 W
B
5 MHz
ptr,max
n
2 W
Ïƒ2
-107 dBm
pexe
n
[10, 20] W
kn
[2560, 4096]
D
10 Mbits
Ï„
3000 s
A. Analysis of execution energy consumption
We first evaluate the execution energy consumption perfor-
mance by comparing the proposed ASC and LAW against two
benchmark schemes: the genetic algorithm (GA) and Random
AI agent selection (RandAS). These benchmarks utilize GA
and random selection for AI agent assignment, respectively,
while the optimization of BS beamforming and AI agent
transmission powers is identical to ASC.
Fig. 4 demonstrates the execution energy consumption
versus the number of AI agents. One can observe that the
execution energy consumption increases as the number of
AI agents increases. In particular, ASC achieves the lowest
execution energy consumption among the four schemes with
approximately 1.19 Ã—104 J and 1.87Ã—104 J when N = 30 and
50, respectively. LAW realizes slightly higher execution energy
consumption in comparison to ASC with corresponding values
of 1.37 Ã—104 J and 2.79Ã—104 J. RandAS realizes the worst
performance with 2.34Ã—104 J and 4.57Ã—104 J when N = 30
and 50. This is because ASC and LAW can dynamically select
10
15
20
25
30
35
40
45
50
The number of AI agents
0
1
2
3
4
5
The execution energy consumption (J)
Ã—10
4
RandAS
GA
LAW
ASC
Fig. 4: The execution energy consumption versus different
numbers of AI agents.
AI agents to participate in cooperative reasoning according to
their reasoning accuracy and computation resources. In this
way, the execution energy consumption can be reduced sig-
nificantly while satisfying the reasoning accuracy constraint.
TABLE II: The execution energy consumption (J) under dif-
ferent minimum numbers of reasoning AI agents (Nmin).
Schemes
Nmin = 10
Nmin = 15
Nmin = 20
RandAS
23417
41107
48999
GA
15019
29983
38402
LAW
13693
28436
36113
ASC
11868
26673
34996
TABLE III: The execution energy consumption (J) under
different average reasoning accuracy thresholds (Accmin).
Schemes
Accmin = 60%
Accmin = 70%
Accmin = 80%
RandAS
21222
23417
27520
GA
13120
15219
16189
LAW
12022
14242
14959
ASC
10423
11868
12966
The typical number of AI agents, i.e., N = 30 is employed
for further analysis of execution energy consumption. Since
the minimum number of reasoning AI agents plays a pivotal
role in guaranteeing the reliability and robustness of coopera-
tive reasoning, we investigate the impact of varying the min-
imum number of participating AI agents on the performance
of ASC and LAW. TABLE II shows the execution energy con-
sumption versus different minimum numbers of reasoning AI
agents. ASC realizes the best performance compared to other
three schemes. Specifically, ASC achieves 1.19 Ã— 104 J and
3.50Ã—104 J when Nmin = 10 and 20, respectively, followed by
LAW and GA with corresponding values of 1.37 Ã— 104 J and
3.61Ã—104 J, 1.50Ã—104 J and 3.84Ã—104 J. This phenomenon
can be explained by the fact that the computational energy
consumption for LLM inference is typically larger than the
communication energy overhead. As the number of AI agents
involved in cooperative reasoning increases, the total energy
consumption escalates significantly. Therefore, the number of
reasoning AI agents should be reasonably determined.


--- Page 11 ---
IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. X, XX XXXX
11
Furthermore, we evaluate the impact of the required average
reasoning accuracy on the performance of the proposed solu-
tions. The typical value Nmin = 10 is adopted in the following
study. TABLE III shows the execution energy consumption
under different average reasoning accuracy threshold. It can
be seen that a higher average reasoning accuracy threshold
leads to an increase in execution energy consumption. In
particular, ASC realizes 1.04 Ã— 104 J and 1.30 Ã— 104 J when
Accmin = 60% and 80%, respectively. LAW and GA achieve
slightly higher execution energy consumption in comparison
to ASC, with corresponding values of 1.20 Ã— 104 J and
1.49Ã—104 J, 1.31Ã—104 J and 1.62Ã—104 J. RandAS realizes the
highest execution energy consumption among four schemes
with 2.12 Ã— 104 J and 2.75 Ã— 104 J when Accmin = 60%
and 80%, respectively. This is because As prioritizes to select
AI agents equipped with larger-scale LLMs to satisfy the
stricter accuracy threshold, thereby increasing the networkâ€™s
computational cost and execution energy consumption.
B. Analysis of transmission energy consumption
In this subsection, we study the transmission energy con-
sumption performance of the proposed solutions in comparison
to three benchmark schemes, i.e., differential evolution scheme
(DE), fixed transmission power scheme (FixedTP) and fixed
beamforming scheme (FixedBF). In particular, DE shares the
identical AI agent selection scheme with ASC and utilizes DE
algorithm to jointly optimize BS beamforming and AI agent
transmission powers. FixedTP sets the transmission power of
each AI agent to 60% of the maximum transmission power and
the optimization of AI agent selection and BS beamforming
is identical to ASC. FixedBF allocates BS transmission power
equally to each antenna and adopts the same method as ASC
for AI agent selection and transmission power optimization.
10
15
20
25
30
35
40
45
50
The number of AI agents
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
The transmission energy consumption (J)
Ã—10
3
FixedTP
FixedBF
DE
LAW
ASC
Fig. 5: The transmission energy consumption versus different
numbers of AI agents.
Fig. 5 plots the transmission energy consumption under
different numbers of AI agents. ASC realizes the best per-
formance among five schemes with around 77 and 140 J
when N = 30 and 50, respectively. LAW achieves slightly
higher transmission energy consumption with corresponding
values of 105 J and 260 J. FixedTP exhibits 199 J and 1477
J when N = 30 and 50, respectively, which is the highest
among five schemes. This is because ASC and LAW can
dynamically optimize AI agent transmission powers according
to AI agent selection and channel states. As such, these
solutions can achieve the required uplink secrecy capacity with
minimal power overhead, thereby reducing the overall energy
consumption while satisfying the latency constraint.
TABLE IV: The transmission energy consumption (J) under
different numbers of BS antennas (L).
Schemes
L = 10
L = 20
L = 30
FixedTP
310.76
299.20
225.38
FixedBF
260.27
246.01
171.85
DE
155.95
139.21
54.967
LAW
146.01
105.21
53.785
ASC
109.87
77.219
49.872
A baseline configuration of N = 30 is adopted for the
evaluation of the impact of different numbers of BS antennas
on the transmission energy consumption. TABLE IV presents
the impact of the number of BS antennas on transmission
energy consumption. One can observe that the transmission
energy consumption decreases as the number of BS antennas
increases. In particular, ASC and LAW achieve 110 J and
50 J, 146 J and 54 J when L = 10 and 30, respectively,
followed by DE with corresponding values of 156 J and
55 J. FixedTP realizes the worst performance with 311 J
and 225 J when L = 10 and 30, respectively. One should
notice that the proposed solution can dynamically optimize
BS beamforming to achieve better downlink secrecy capacity.
The energy consumption of AI agents functioning as friendly
jammers in the downlink transmission process can be reduced
significantly. Moreover, with more antennas, BS can focus the
transmission beam more precisely on the selected AI agents,
which may reduce signal leakage to the eavesdropper and
improve the downlink secrecy capacity.
C. Analysis of reasoning performance on Public benchmarks
To evaluate the practical reasoning capabilities of the pro-
posed secure agentic AI network with ASC and LAW in real-
world scenarios, we construct an agentic AI system comprising
30 AI agents, where two sets of 12 AI agents are equipped with
Qwen-0.6B and Qwen-1.7B models. One set of 6 AI agents is
equipped with Qwen-4B models. Each AI agent is deployed on
an NVIDIA Tesla P100 GPU with 16G VRAM. The number
of AI agents involved in cooperative reasoning is set to 10.
The reasoning accuracy on three widely used datasets: ARC-
E, ARC-C, and BoolQ, which represent different levels of
reasoning difficulty, is investigated [28].
TABLE V: The reasoning accuracy under different public
benchmarks.
Schemes
ARC-E
ARC-C
BoolQ
ASC
85%
75%
83%
LAW
76%
65%
82%
GA
66%
56%
80%
RandAS
56%
41%
75%
TABLE V shows the reasoning accuracy of the proposed
secure agentic AI network under different public benchmarks.


--- Page 12 ---
IEEE TRANSACTIONS ON MOBILE COMPUTING, VOL. XX, NO. X, XX XXXX
12
In ARC-E and ARC-C, the proposed ASC and LAW achieve
significantly higher accuracy compared to that of RandAS.
In particular, ASC and LAW realize 85% and 75%, 76%
and 65%, respectively. This is because ASC and LAW can
dynamically select participating AI agents according to their
computational resources and the reasoning accuracy predicted
by the scaling law. As such, the proposed schemes can realize
satisfactory reasoning accuracy while minimizing energy con-
sumption. Additionally, one should note that the performance
gaps between four schemes on BoolQ are not as significant as
in the other datasets. This phenomenon may be attributed to the
fact that BoolQ questions are binary classification problems.
The majority voting mechanism remains robust in improving
reasoning accuracy even when hallucinations occur in some
participating AI agents. However, for the more challenging
ARC-C dataset with a larger solution space, RandAS performs
significantly worse than the other schemes because it cannot
dynamically select AI agents.
VI. CONCLUSION
We propose a novel secure wireless agentic AI network,
where one supervisor AI agent and other AI agents are jointly
deployed for QoS provisioning of usersâ€™ reasoning tasks.
In particular, the supervisor AI agent dynamically selects
multiple AI agents to engage in cooperative reasoning and the
unselected AI agents function as friendly jammers to mitigate
eavesdropping. To prolong the service duration of AI agents,
an energy minimization problem is formulated by jointly
considering AI agent selection, BS beamforming and AI agent
transmission powers, subject to a list of QoS constraints. To
solve the formulated challenging problem, two resource alloca-
tion methods, ASC and LAW, are introduced. These methods
first decouple the original problem into three sub-problems.
ASC optimizes three sub-problems by the proposed ADMM-
based algorithm, SDR and SCA in an alternating manner,
while LAW solves each sub-problem via the formulated LLM
optimizer following an agentic workflow. Experimental results
verify that the proposed schemes can significantly reduce AI
agents energy consumption while promising satisfactory QoS
performance in comparison to other benchmark algorithms.
REFERENCES
[1] R. Sapkota, K. I. Roumeliotis, and M. Karkee, â€œAI agents vs. agentic
AI: A conceptual taxonomy, applications and challenges,â€ Information
Fusion, vol. 126, p. 103599, 2026.
[2] W. X. Zhao et al., â€œA survey of large language models,â€ arXiv preprint
arXiv:2303.18223, 2023, v1.2.
[3] Y. Huang et al., â€œAI-generated network design: A diffusion model-based
learning approach,â€ IEEE Network, vol. 38, no. 3, pp. 202â€“209, 2024.
[4] D. B. Acharya, K. Kuppan, and B. Divya, â€œAgentic AI: Autonomous
intelligence for complex goalsâ€”a comprehensive survey,â€ IEEE Access,
vol. 13, pp. 18 912â€“18 936, 2025.
[5] R. Zhang et al., â€œToward edge general intelligence with agentic AI
and agentification: Concepts, technologies, and future directions,â€ arXiv
preprint arXiv:2508.18725, 2025.
[6] X. Bo et al., â€œReflective multi-agent collaboration based on large lan-
guage models,â€ in Advances in Neural Information Processing Systems
(NeurIPS), 2024.
[7] S. Hu, C. Lu, and J. Clune, â€œAutomated design of agentic systems,â€ in
International Conference on Learning Representations (ICLR), 2025.
[Online]. Available: https://openreview.net/forum?id=t9U3LW7JVX
[8] B. Li et al., â€œAgent-as-a-service: An ai-native edge computing frame-
work for 6g networks,â€ IEEE Network, vol. 39, no. 2, pp. 44â€“51, 2025.
[9] S. Yao et al., â€œReact: Synergizing reasoning and acting in language mod-
els,â€ in International Conference on Learning Representations (ICLR),
2023.
[10] T. Schick et al., â€œToolformer: Language models can teach themselves
to use tools,â€ in Advances in Neural Information Processing Systems
(NeurIPS), 2023.
[11] X. Wang et al., â€œSelf-consistency improves chain of thought reasoning
in language models,â€ in International Conference on Learning Repre-
sentations (ICLR), 2023.
[12] X. Wang, Y. Li, and Z. Zhang, â€œSmoothQuant: Accurate and efficient
post-training quantization for large language models,â€ in Proceedings of
the International Conference on Machine Learning (ICML), 2024.
[13] S. Laskaridis et al., â€œMobile and edge evaluation of large language
models,â€ in Proceedings of the International Conference on Machine
Learning (ICML), 2024.
[14] M. Schreiner, â€œGPT-4 architecture, datasets, costs and more leaked,â€
[Online]. Available: https://the-decoder.com/gpt-4-architecture-datasets-
costs-and-more-leaked/, 2023, accessed: Oct. 13, 2023.
[15] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, â€œGPTQ: Accurate
post-training quantization for generative pre-trained transformers,â€ in
Proceedings of the 12th International Conference on Learning Repre-
sentations (ICLR), 2024.
[16] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, â€œQlora: Ef-
ficient finetuning of quantized llms,â€ in Advances in neural information
processing systems (NeurIPS), 2023.
[17] J. Kaplan et al., â€œScaling laws for neural language models,â€ arXiv
preprint arXiv:2001.08361, 2020.
[18] B. Isik et al., â€œScaling laws for downstream task performance of
large language models,â€ in ICLR 2024 Workshop on Mathematical and
Empirical Understanding of Foundation Models, 2024.
[19] K. Kumar et al., â€œLLM post-training: A deep dive into reasoning large
language models,â€ arXiv preprint arXiv:2502.21321, 2025.
[20] Z. Tan et al., â€œScaling behaviors of llm reinforcement learning post-
training: An empirical study in mathematical reasoning,â€ arXiv preprint
arXiv:2509.25300, 2025.
[21] J. Wei et al., â€œChain-of-thought prompting elicits reasoning in large
language models,â€ in Advances in neural information processing systems
(NeurIPS), 2022.
[22] X. Wang et al., â€œSelf-consistency improves chain of thought reasoning
in language models,â€ arXiv preprint arXiv:2203.11171, 2022.
[23] Q. Wu et al., â€œAutogen: Enabling next-gen llm applications via multi-
agent conversations,â€ in First Conference on Language Modeling, 2024.
[24] H. K. Choi, X. Zhu, and S. Li, â€œDebate or vote: Which yields
better decisions in multi-agent large language models?â€ arXiv preprint
arXiv:2508.17536, 2025.
[25] L. B. Kaesberg et al., â€œVoting or consensus? decision-making in multi-
agent debate,â€ arXiv preprint arXiv:2502.19130, 2025.
[26] Y. He et al., â€œLarge language models (LLMs) inference offloading
and resource allocation in cloud-edge computing: An active inference
approach,â€ IEEE Transactions on Mobile Computing, vol. 23, no. 12,
pp. 11 253â€“11 264, 2024.
[27] X. Zhang et al., â€œBeyond the cloud: Edge inference for generative large
language models in wireless networks,â€ IEEE Transactions on Wireless
Communications, vol. 24, no. 1, pp. 643â€“658, 2025.
[28] N. Xue et al., â€œWdmoe: Wireless distributed mixture of experts for large
language models,â€ IEEE Transactions on Wireless Communications,
vol. 25, pp. 559â€“572, 2026.
[29] N. Su, F. Liu, and C. Masouros, â€œSensing-assisted eavesdropper estima-
tion: An isac breakthrough in physical layer security,â€ IEEE Transactions
on Wireless Communications, vol. 23, no. 4, pp. 3162â€“3174, 2024.
[30] Y. Cao, L. Duan, and R. Zhang, â€œSensing for secure communication
in isac: Protocol design and beamforming optimization,â€ IEEE Trans-
actions on Wireless Communications, vol. 24, no. 2, pp. 1207â€“1220,
2025.
[31] A. Azarbahram et al., â€œBeamforming and waveform optimization for rf
wireless power transfer with beyond diagonal reconfigurable intelligent
surfaces,â€ IEEE Transactions on Wireless Communications, vol. 25, pp.
8826â€“8842, 2026.
[32] M. Sun et al., â€œLLM-based task offloading and resource allocation in
satellite edge computing networks,â€ IEEE Transactions on Vehicular
Technology, 2025, doi: 10.1109/TVT.2025.3612207.
[33] Y. Sun et al., â€œAn empirical study of LLM reasoning ability under strict
output length constraint,â€ arXiv preprint arXiv:2504.14350, 2025.
