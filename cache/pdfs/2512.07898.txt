--- Page 1 ---
MARINE: Theoretical Optimization and Design for
Multi-Agent Recursive IN-context Enhancement
Hongwei Zhang 1 Ji Lu 1 Yongsheng Du 1 Yanqin Gao 1 Lingjun Huang 1 Baoli Wang 1 Fang Tan 1 Peng Zou 1
Abstract
Large Language Model (LLM)-based agents
demonstrate advanced reasoning capabilities, yet
practical constraints frequently limit outputs to
single responses, leaving significant performance
potential unrealized. This paper introduces MA-
RINE (Multi-Agent Recursive IN-context En-
hancement), a theoretically grounded framework
that reconceptualizes test-time reasoning as itera-
tive refinement of a persistent reference trajectory,
fundamentally departing from conventional one-
shot or multi-sample paradigms. The MARINE
refinement operator systematically converts a base
model’s pass@N capabilities into near-optimal
pass@1 performance. Rigorous theoretical analy-
sis establishes that minimal feasible batches max-
imize expected performance gains under fixed
invocation budgets, while logarithmically grow-
ing batch schedules ensure continuous improve-
ment without computational constraints. Com-
prehensive evaluation on the BrowserComp-ZH
benchmark demonstrates state-of-the-art results,
with a 685B-parameter implementation achiev-
ing 46.0% pass@1 accuracy. Meanwhile, MA-
RINE establishes a new paradigm for parameter-
efficient reasoning: an 80B-parameter model aug-
mented with MARINE matches the performance
of standalone 1000B-parameter agents, reducing
parameter requirements by over an order of mag-
nitude. Notably, within a fixed computational
budget, the proposed MARINE delivers higher-
quality samples to alignment and optimization
processes than traditional sampling-and-ranking
strategies. Consequently, it has great potential to
boost post-training efficiency.
1Zhongxing Telecom Equipment (ZTE), China. Correspon-
dence to: Ji Lu <AIM@zte.com.cn>.
The open-source code will be released soon.
1. Introduction
1.1. Background and Motivation
Large Language Model (LLM)-based agents achieve strong
performance on complex reasoning and searching tasks (Li
et al., 2025; Xu et al., 2025; Chen et al., 2025). In practice,
many systems are restricted to a single interaction round.
Under this constraint, the pass@1 performance (probability
of success with a single sample) can be substantially lower
than the pass@N performance (probability of success with
N samples) (Brown et al., 2024; Zhang et al., 2025).
Existing methods for enhancing reasoning performance can
be grouped into three paradigms. The first comprises multi-
sampling-and-selection schemes, such as Self-Consistency
(SC) (Wang et al., 2022) and Best-of-N (BoN) (Gui et al.,
2024). These methods generate multiple reasoning chains
and select the most consistent answer across samples,
thereby mitigating the brittleness of any single chain that
may drift away from the correct solution. Although such
techniques often yield substantial gains, their effectiveness
is highly sensitive to the sampling budget and lacks a the-
oretical guarantee that performance is strictly monotone
in N. Particularly, when all sampled trajectories are of
only mediocre quality, the pass@1 accuracy can be nearly
indistinguishable from standard single-sample decoding.
The second paradigm comprises self-correction and
reflection-based reasoning methods.
Representative ap-
proaches such as Self-Refine implement test-time iterative
loops of “generate–self-evaluate–self-revise” to iteratively
improve model outputs (Madaan et al., 2023). Tree-of-
Thoughts (ToT) further formulates reasoning as an explicit
tree search over intermediate “thoughts”, enabling branch-
ing and backtracking over partial solution sketches to induce
more structured problem-solving trajectories (Yao et al.,
2023). Despite their success, these methods remain largely
empirical. First, they lack a precise formalization and quan-
titative guarantees that each revision step actually improves
the quality of the trajectory. Second, prior work has shown
that single-agent reflection is vulnerable to Degeneration-
of-Thought (DoT): once a model develops overconfidence
in a candidate solution, subsequent reflection rarely escapes
1
arXiv:2512.07898v1  [cs.MA]  5 Dec 2025


--- Page 2 ---
MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement
the established reasoning path (Liang et al., 2024).
The third paradigm focuses on parameter optimization
through Reinforcement Learning (RL). RL from Human
Feedback (RLHF) methods (Bai et al., 2022), together with
algorithms in the style of Group Relative Policy Optimiza-
tion (GRPO) (Shao et al., 2024), update model parameters
using human preference signals or automatically constructed
reward functions. These approaches have achieved substan-
tial gains on various reasoning benchmarks (Shao et al.,
2024). However, their transferability to novel tasks remains
limited, as policies often require realignment when deployed
in new contexts, restricting their general applicability.
In summary, existing methods primarily optimize model
parameters rather than enhancing in-context reasoning pro-
cesses, or rely on multi-sample decoding and self-correction
strategies devoid of quantitative theoretical characteriza-
tion regarding reasoning evolution. These fundamental lim-
itations motivate our central research question: without
parameter updates, can a structured multi-agent refine-
ment framework systematically transform an LLM’s
pass@N capability into reliable pass@1 performance
while providing theoretical guarantees of monotonic im-
provement throughout the refinement process?
1.2. Contributions
MARINE (Multi-Agent Recursive IN-context Enhance-
ment) treats test-time reasoning as iterative refinement
around a persistent reference trajectory instead of one-
shot decoding or independent multi-sample selection. The
framework operates purely at inference time and is model-
agnostic. The main contributions are fourfold.
• Trajectory Refinement Paradigm. The framework
reconceptualizes test-time reasoning as an iterative op-
timization over reference trajectories, facilitated by a
theoretically-grounded refinement operator that aggre-
gates candidate trajectories from multiple agents.
• MARINE architecture. MARINE implements a lay-
ered architecture where each refinement layer employs
multiple heterogeneous agents operating on a shared
reference trajectory. The framework introduces struc-
tured trajectory representation, conflict-aware meta-
verification, and segment-level integration mechanisms
that collectively ensure trajectory improvement without
full regeneration. This architecture precisely isolates
local improvements while preserving global reasoning
coherence.
• Batch-size optimization. Theoretical analysis estab-
lishes complementary principles for agent orchestra-
tion under distinct computational constraints. Under
fixed invocation budgets, minimal feasible batches
maximize expected performance gains per agent call.
Conversely, when no invocation limit exists, logarith-
mically growing batch schedules guarantee monotonic
trajectory improvement with arbitrarily high proba-
bility, establishing rigorous worst-case performance
bounds critical for reliability-sensitive applications.
• Experimental Validation and Parameter Efficiency.
Comprehensive evaluation on the BrowserComp-ZH
benchmark demonstrates that MARINE achieves state-
of-the-art performance when implemented with a
685B-parameter LLM. Significantly, an 80B-parameter
LLM augmented with MARINE matches the perfor-
mance of standalone 1000B-parameter models, estab-
lishing a new paradigm for parameter-efficient reason-
ing. The framework consistently surpasses baseline
methods including Self-Refine and Best-of-N under
equivalent computational constraints, empirically vali-
dating the theoretical analysis regarding optimal batch
sizing and refinement depth. These results position
MARINE as a practical framework for enhancing test-
time reasoning capabilities while substantially reduc-
ing parameter requirements.
2. Related Work
2.1. Multi-Sample Decoding and Test-Time Scaling
Multi-sample decoding improves LLM reasoning at infer-
ence time by drawing multiple candidates and selecting
among them. Chain-of-Thought (CoT) prompting exposes
intermediate steps and improves performance on arithmetic
and symbolic tasks (Wei et al., 2022; Zhou et al., 2022;
Chen et al., 2022). SC samples many reasoning chains
and takes a majority vote over final answers, converting
some pass@N potential into higher pass@1 but without
formal scaling guarantees (Wang et al., 2022). Subsequent
work generalizes multi-sample decoding into BoN selec-
tion with reward models or self-certainty signals and shows
that inference-time selection can rival or complement post-
training alignment (Kang et al., 2025; Park et al., 2025; Sun
et al., 2024). Other work studies SC and BoN under cal-
ibrated confidence or budget-aware policies and proposes
strategies such as adaptive self-calibration and thought prun-
ing to reduce multi-sample costs (Zeng et al., 2025; Huang
et al., 2025; Hong et al., 2025).
These methods treat each sampled trajectory as an indepen-
dent solution and do not maintain a persistent reference tra-
jectory or analyze monotone trajectory improvement. MA-
RINE instead focuses on a single evolving trajectory and
provides a probabilistic analysis of its quality under repeated
multi-agent refinement and different batch-size schedules.
2.2. Self-Refinement and Revision Methods
Beyond pure sampling, self-refinement methods iteratively
critique and revise model outputs. Self-Refine establishes
2


--- Page 3 ---
MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement
a feedback–refine loop and reports gains without parame-
ter updates (Madaan et al., 2023). Multi-Aspect Feedback
(MAF) uses multiple specialized feedback modules target-
ing distinct error types and improves over Self-Refine on
several benchmarks (Nathani et al., 2023). Structured Rea-
soning with Revisions (SCREWS) formulates reasoning
with revisions via three stages, sampling, conditional resam-
pling and selection, that subsume a range of reflection and
tool-use strategies (Shridhar et al., 2023).
Revision is also integrated with preference optimization.
Process- and preference-based supervision is combined with
search over partial solutions, where reflective-guided ex-
ploration yields higher-quality preference data for policy
updates (Rafailov et al., 2023; Lightman et al., 2023; Hao
et al., 2023). Surveys consolidate diverse self-correction
strategies and empirical patterns (Pan et al., 2023).
These approaches are algorithmic and empirical: they typi-
cally lack an explicit trajectory metric, do not model how
improvement probability decays as solutions strengthen,
and do not study optimal batch size or depth under fixed
inference budgets. MARINE defines a trajectory-level error,
imposes assumptions on local superiority and comparative
evaluation, and derives monotone improvement and batch-
size trade-offs for structured in-context refinement.
2.3. Multi-Agent Reasoning
Multi-agent frameworks deploy multiple instances that col-
laborate or compete during problem solving. Recent surveys
report strong empirical performance of LLM-based multi-
agent systems across diverse tasks, highlight their promising
prospects, and discuss remaining coordination and reliabil-
ity challenges (Guo et al., 2024; Li et al., 2024b; Chen et al.,
2024; Tran et al., 2025).
Debate-style systems instantiate multi-agent interaction for
reasoning. Prior work shows that single-agent reflection can
suffer degeneration of thought and introduces Multi-Agent
Debate (MAD) to encourage divergent arguments overseen
by a judge (Liang et al., 2024). Follow-up work applies
debate to fake-news detection, requirements engineering
and stance detection, and uses debate traces as supervision
for fine-tuning (Subramaniam et al., 2025; Li et al., 2024a;
Chan et al., 2023).
Existing multi-agent systems generally operate over free-
form dialogue, without an explicit shared reference trajec-
tory or trajectory-level guarantees. MARINE instead orga-
nizes agents around a common reference trajectory, fuses
only locally superior segments under verification, and ana-
lyzes how the probability of global improvement scales with
the number of exploration agents and the refinement depth.
3. MARINE System
This section formalizes the trajectory refinement paradigm
and delineates the MARINE framework. Unlike one-shot
decoding or independent multi-sample selection approaches,
MARINE reconceptualizes test-time reasoning as an itera-
tive optimization process centered around a persistent refer-
ence trajectory. The framework establishes a theoretically
grounded refinement operator that aggregates candidate tra-
jectories from multiple heterogeneous exploration agents.
Implementation of this operator employs a trajectory repre-
sentation methodology and a recursive in-context enhance-
ment mechanism, collectively transforming a base model’s
pass@N capabilities into pass@1 performance.
3.1. Trajectory Refinement Paradigm
Let the original task be q ∈Q, and let τ ⋆denote the ideal
reasoning trajectory corresponding to the optimal solution.
The standard LLM reasoning process can be abstracted
as a fixed-parameter policy πθ that generates a complete
trajectory τ for task q as
τ ∼πθ(· | q),
y = Decode(τ),
(1)
where Decode(·) maps the trajectory to the final output y. In
this formulation, the model attempts to solve the entire task
in a single step by sampling directly in a high-dimensional
search space.
MARINE utilizes an agent-based trajectory refinement op-
erator. Denote the reference trajectories at the k-th round
by τ (k), and the set of trajectories generated in parallel by
Mk+1 agents by
τ (k+1)
i
∼πi(· | q, τ (k), C(k)),
i ∈(1, · · · , Mk+1) (2)
where C(k) encodes supplementary analyses of τ (k), such
as confidence scores and tool-usage logs. Heterogeneous
agents, instantiated via different prompts or reasoning pref-
erences, induce diverse and complementary reasoning paths.
The trajectory refinement operator is defined as
R

q, τ (k), C(k)
=

τ (k+1), C(k+1)
.
(3)
Under this paradigm, the objective of each reasoning round
shifts from “generating a fully correct trajectory from
scratch” to “performing local refinements around the current
reference trajectory.”
Trajectory quality is characterized through a continuous
distance metric. Each reasoning trajectory τ is mapped to a
J-dimensional evaluation vector
d(τ, τ ⋆) = (d1(τ, τ ⋆), . . . , dJ(τ, τ ⋆)) ∈[0, 1]J,
(4)
where each dimension j corresponds to semantic dimen-
sions such as “whether key facts are correct”, “whether
3


--- Page 4 ---
MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement
Question ݍ
…
ݍ, ߬1
(1), ܥ1
(1)
ݍ, ߬2
(1), ܥ2
(1)
ݍ, ߬ெ1
(1), ܥெ1
(1)
…
ݍ, ߬1
(2), ܥ1
(2)
ݍ, ߬2
(2), ܥ2
(2)
ݍ, ߬ெ2
(2), ܥெ2
(2)
…
…
…
…
ݍ, ߬1
(௄), ܥ1
(௄)
ݍ, ߬2
(௄), ܥ2
(௄)
ݍ, ߬ெ಼
(௄), ܥெ಼
(௄)
Response ݕ
Exploration stage
Recursive in-context enhancement stage
Answer 
stage
…
…
Comparison 
and merging
Reference trajectory
Comparative reflection:
Completeness
Logical coherence
Scoring and 
ranking
Trajectory 
integration
Verification:
Facts
Reasoning
Reasoning and 
summarization
Refinement:
Reasoning reuse
Hypothesis 
correction
Analysis
Confidence
Conclusion consistency
Reasoning consistency
Refinement operator ܴ
Figure 1. MARINE Framework: Multi-Agent Recursive IN-Context Enhancement for Trajectory Refinement. (Top) Layered
architecture comprising initial exploration with parallel agents, K recursive enhancement layers propagating reference trajectories
as persistent states, and final answer generation. Each layer employs Mk agents operating on structured context (q, τ (k), C(k)) with
controlled diversity mechanisms. (Bottom) Refinement operator R workflow: structured trajectory representation, multi-dimensional
conflict detection (factual and logical), meta-verification through authority assessment and cross-validation, and segment-level integration
of verified improvements. The operator ensures monotonic trajectory improvement via dimensional error minimization while preserving
reasoning coherence through hypothesis correction and comparative reflection.
intermediate equations hold”, “whether the logical chain is
coherent”, etc. The overall distance is defined as:
dist(τ, τ ⋆) = 1
J
J
X
j=1
dj(τ, τ ⋆) ∈[0, 1],
(5)
where dist = 0 indicates exact agreement with the ideal tra-
jectory across all dimensions, and dist = 1 indicates maximal
discrepancy with respect to the ideal trajectory.
In an idealized case, progressively improved reference tra-
jectories monotonically increase the distance score on the
primary task and yield higher-quality reasoning traces. At
the macro level, the refinement problem becomes increas-
ingly localized as τ (k) approaches the ideal trajectory τ ⋆. At
the micro level, the few-shot context becomes more closely
aligned with the ground-truth trajectory, which steers the
LLM’s autoregressive generation toward the correct solution
and consequently improves accuracy.
3.2. Workflow of MARINE
As illustrated in Figure 1, the MARINE framework can
be analogized to a feedforward neural network of sub-
agents: each layer comprises Mk parallel sub-agents, with
reference trajectories τ (k) propagating “hidden states” be-
tween layers.
At the exploration stage (k = 1), M1 agents independently
and in parallel generate {(τ (1)
1 , C(1)
1 ), · · · , (τ (1)
M1, C(1)
M1)}
based on the initial question q. The process then proceeds
Algorithm 1 MARINE Algorithm
Require: Query q; depth K; batch sizes {Mk}K
k=1
Ensure: Final answer y
1: Exploration stage
2: Generate initial trajectories {(τ (1)
i
, C(1)
i
)}M1
i=1 in paral-
lel from M1 agents given q
3: Select reference trajectory (τ (1), C(1)) from the initial
set
4: Recursive enhancement stage
5: for k = 1 to K do
6:
Generate {(ˆτ (k)
i
, ˆC(k)
i
)}Mk
i=1 in parallel conditioned
on (q, τ (k), C(k))
7:
(τ (k+1), C(k+1)) ←R(q, τ (k), C(k))
8: end for
9: Answer stage
10: y ←generation from an agent based on (q, τ (K), C(K))
11: return y
to the recursive in-context enhancement stage. At the k-th
round, Mk agents independently execute the function R in
(3) in parallel, producing new τ (k) and C(k). This stage is
recursively executed until the preset limit k = K is reached.
In the answer phase, one agent generates the ultimate re-
sponse to the initial question q based on τ (K) and C(K).
The workflow of MARINE is summarized in Algorithm 1.
The generation of diverse trajectories in the initial explo-
ration phase and the recursive enhancement of the context
are detailed in the following sections.
4


--- Page 5 ---
MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement
3.3. Diverse Trajectory Generation
At the k-th layer, given the tuple (q, τ (k), C(k)), the diver-
sity of the generated trajectories is encouraged through the
following mechanisms:
• Reasoning Path Diversity: Distinct problem-solving
preference prompts are applied to different agents.
• Intermediate Fact Diversity: Agents are permitted to
invoke different external tools or knowledge sources.
• Sampling Strategy Diversity: By configuring differ-
ent sampling temperatures, we establish a behavioral
spectrum where certain agents favor reliable outputs
while others encourage diverse explorations.
This design yields generated trajectories τ (k+1) exhibiting
strong local optimality diversity: even when no single tra-
jectory surpasses the reference τ (k) across all dimensions,
superior local-optimum fragments frequently emerge along
different evaluation dimensions.
3.4. Recursive In-Context Enhancement
To implement the trajectory refinement operator R, MA-
RINE enhances the conflict-aware meta-verification mech-
anism (Lu et al., 2025), which can identify and integrate
valuable information from diverse trajectories. As illustrated
in Figure 1, the mechanism comprises four stages:
1. Structured trajectory representation. All trajecto-
ries are first converted into a unified graph-structured
representation. This exposes intermediate reasoning
steps as aligned nodes, enabling direct comparison
across trajectories and avoiding spurious differences.
2. Conflict detection. The meta-verification module per-
forms a comparative analysis over these structured
graphs to merge consistent nodes and detect two types
of conflicts: factual conflicts and logical conflicts.
3. Conflict resolution. Conflicting results are evaluated
based on reliability and rationality: factual nodes are
ranked by source authority and external verifiability,
while reasoning nodes undergo stress testing via back-
ward substitution and boundary-condition analysis. For
unresolved conflicts, the system engages with external
verification sources, thereby minimizing the propaga-
tion of hallucinated or unsupported content.
4. Trajectory update. Verified information is recorded
in a facts module and used to update the reference
trajectory. The update operates at the segment level,
replacing erroneous spans with verified facts, inserting
missing but necessary intermediate steps, and repair-
ing logical gaps, so that improvements accumulate
monotonically across iterations without requiring full
regeneration of the trajectory.
4. Theoretical Analysis of Batch Size
This section investigates the optimal selection of batch size
Mk under different computational constraints, subject to
Assumption 4.1.
Assumption 4.1. Effectiveness of comparative evaluation.
When the number of agents in k-th round satisfies Mk ≥2,
the evaluation module can identify dimension-wise superi-
ority sources from the candidate set. Specifically, for each
dimension j ∈1, . . . , J, it can select from τ (k) ∪C(k) a
trajectory segment minimizing the local error dj(·, τ ⋆).
This assumption represents a minimal capability require-
ment: an evaluation mechanism unable to discern relative
trajectory quality would fundamentally lack the capacity to
solve the original problem q.
4.1. Comparison with Reinforcement Learning
A comparative analysis between reinforcement learning
(RL) and MARINE elucidates the corresponding decrease in
the successful enhancement probability pk with increasing
iteration count k.
Both MARINE and RL approaches (including GRPO and
GSPO variants) leverage groupwise relative comparisons
and multi-round iterative procedures. The fundamental dis-
tinction resides in their update mechanisms: RL methods
perform direct optimization of model parameters, while MA-
RINE operates exclusively within trajectory space through
iterative refinement of a reference trajectory via candidate
generation and correction.
Figure 2 provides an intuitive illustration of performance
evolution contrasting RL (left panel) and MARINE (right
panel). The solution score is modeled as a random variable,
with its expectation corresponding to the pass@1 metric.
In both panels, blue and green distributions represent score
distributions at iterations k = 1 and k = 2, respectively,
with corresponding dashed lines indicating their means (i.e.,
pass@1 scores). The purple dashed line denotes the score of
the reference trajectory selected at iteration k = 1, assumed
to exceed the initial pass@1 score.
In the RL paradigm (left panel), parameter updates shift
the entire score distribution rightward as model capabilities
improve, thereby increasing the pass@1 score from the blue
to green distribution. Under RL frameworks, the probability
that a newly sampled solution exceeds the current pass@1
score remains approximately 50%, as the pass@1 corre-
sponds to the distribution mean. Conversely, the MARINE
framework (right panel) demonstrates the effect of condi-
tioning on a fixed reference trajectory. This conditioning
induces a bias toward higher-scoring regions, positioning
the pass@1 score at k = 2 (green) between the reference
score (purple) and the original pass@1 at k = 1 (blue). The
5


--- Page 6 ---
MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement
pass@1
(k=1)
pass@1
(k=2)
Expected Sampling
(Exceed pass@1)
Gain from iteration
pass@1
(k=1)
pass@1
(k=2)
Expected  Sampling
(Exceed reference)
Gain from iteration
reference 
trajectory
influenced by 
pass@1 (k=1) and reference trajectory
Figure 2. Evolution of score distributions for generated solutions under RL (left) and MARINE trajectory refinement (right)
across iterations. The left panel illustrates how parameter updates in RL shift the entire score distribution to higher values, keeping
the probability of exceeding the current pass@1 roughly stable. The right panel shows how conditioning on a fixed, increasingly strong
reference trajectory in MARINE biases generation toward a high-score region, thereby enlarging the gap between pass@1 and the
reference and reducing the probability that newly sampled trajectories surpass the reference over iterations.
reference trajectory encodes only partial task information,
thereby biasing generation along specific dimensions while
model parameters maintain their inherent bias in others.
Consequently, the probability of sampling a trajectory that
strictly improves upon the reference falls below 50%. As re-
finement progresses and the reference trajectory strengthens,
the gap between its score and the corresponding pass@1
widens, further reducing the probability of sampling strictly
superior trajectories. This dynamic results in diminishing
improvement rates across iterations for MARINE, whereas
RL maintains a consistently high improvement probability.
Formally, at the k-th iteration, given the reference trajec-
tory τ (k) with score rk := S(τ (k)), consider Mk generated
trajectories {τ (k+1)
i
}Mk+1
i=1
with scores {S(k+1)
i
}Mk+1
i=1
. De-
fine the probability that a single candidate surpasses the
reference as
pk := P
 S(k+1)
i
> rk

,
i = 1, . . . , Mk+1.
(6)
It leads to the following theoretical characterization:
Proposition 4.2 (Progressively decreasing probability of
successful sampling in MARINE). In MARINE-based tra-
jectory optimization, iterative refinement of the reference
trajectory increases the scores of generated trajectories,
but the gap between the pass@1 score and the reference
trajectory score grows monotonically with the number of
iterations, which in turn causes the probability that a gener-
ated trajectory attains a higher score than the reference to
decrease steadily.
4.2. Optimal Batch Size with Fixed Budget
This section considers the setting where the total number
of agent invocations T is bounded. At the k-th round, the
focus is whether at least one success occurs among the Mk
parallel invocations. In this case, Theorem 4.3 holds.
Theorem 4.3 (Optimal exploring batch size Mk under con-
strained agent invocation budgets T). Suppose the total
number of agent invocations T is fixed and pk ∈(0, 0.5)
for all rounds k. Under this total invocation constraint, a
smaller batch size M yields higher efficiency in terms of
expected gain per invocation. In particular, if the MARINE
framework requires Mk ≥2 to ensure sufficient complemen-
tarity among agents and satisfy Assumption 4.1, the optimal
choice is
M ⋆
k = Mmin = 2.
(7)
Proof. The proof is provided in Appendix A.
4.3. Batch Size with Unlimited Budget
The previous subsection analyzed the optimal batch size
under a fixed T. This subsection considers a setting without
the invocation limit and studies how small the per-iteration
batch size can be while still guaranteeing monotone im-
provement of the reference trajectory with high probability.
The core question is whether there exists a schedule
{Mk}∞
k=1 such that, from some iteration onward, the ref-
erence trajectory improves at every step with probability
arbitrarily close to one. The following theorem shows that a
logarithmically increasing batch size is sufficient, provided
the per-sample improvement probability does not vanish.
Theorem 4.4 (Monotone improvement with growing batch
size). Suppose there exists a constant p > 0 and an index ˜k
such that pk ≥p for all k ≥˜k. If the batch sizes satisfy
Mk ≥
−2 ln k
|ln(1 −p)|,
k ≥˜k,
(8)
then for any δ > 0 there exists ˜k′ ≥˜k such that, with
probability at least 1 −δ, every iteration k ≥˜k′ produces
at least one trajectory that strictly improves over the current
reference trajectory. In other words, under a mild lower
bound on the per-sample improvement probability, a slowly
increasing batch size of order O(log k) suffices to make
“continuous” improvement overwhelmingly likely in the limit.
Proof. The proof is provided in Appendix B.
6


--- Page 7 ---
MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement
Theorems 4.3 and 4.4 establish complementary theoret-
ical foundations for inference-time resource allocation.
Theorem 4.3 provides the optimal batch-size configuration
under fixed invocation budgets, maximizing expected per-
formance gains through minimal feasible batches. This
principle is essential for practical deployments where com-
putational resources are strictly constrained yet average-case
performance remains the primary objective. Conversely,
Theorem 4.4 establishes rigorous worst-case guarantees by
prescribing logarithmically growing batch schedules that
bound performance degradation probability below δ while
minimizing computational overhead. This theoretical guar-
antee becomes critical in reliability-sensitive applications
where consistent monotonic improvement outweighs aver-
age performance considerations. Together, these theorems
delineate a complete optimization framework for test-time
reasoning systems across diverse operational constraints.
5. Experimental Evaluation and Ablations
5.1. Experimental Setup
In this section, the BrowserComp-ZH benchmark (Zhou
et al., 2025) is used as a representative multi-hop retrieval
task. It is a Chinese-focused evaluation dataset that mea-
sures model performance in complex information-seeking
scenarios, where each query requires several reasoning steps
over a large document collection to gather supporting evi-
dence and synthesize an answer, closely mirroring the infor-
mation search process in a browser environment.
To evaluate the generality of the proposed method, exper-
iments are conducted on two classes of language models
spanning large and small scales. The large-scale model is
DeepSeek-V3.2 (without thinking), with a total parameter
count of 685B and 37B active parameters. The small-scale
model is Qwen3-next-80B-A3B in thinking mode, with 30B
total parameters and 3B active parameters. All models are
open-source, which ensures strict reproducibility of the ex-
perimental results. Besides, all reported results are obtained
by averaging over three independent samples to mitigate
stochastic fluctuations.
5.2. Main Results on BrowserComp-ZH
Following Theorem 4.3, MARINE is instantiated with four
refinement rounds K = 4 and a constant batch size Mk = 2,
which maximizes the expected gain per agent invocation
under a fixed budget while preserving sufficient intra-batch
diversity for reliable comparative evaluation.
As shown in Figure 3, MARINE with the 685B LLM
achieves a 46.0% pass@1 score on the BrowserComp-
ZH task with a matched invocation budget, outperform-
ing strong baselines. This improvement arises from the
strategic allocation of the budget to a structured multi-agent
Tongyi
DeepResearch
MARINE
(685B)
OpenAI
DeepResearch
Kimi-K2
MARINE
(80B)
0
10
20
30
40
50
Score (Average@3, %)
46.7
46.0
42.9
28.8
28.0
Figure 3. Performance Comparison. MARINE achieves SOTA
with the 685B LLM and matches Kimi-K2 with the 80B LLM.
refinement process, centered around a persistent reference
trajectory. The iterative refinement progressively reduces
trajectory-level errors by targeting contested regions, ex-
celling in multi-hop retrieval, and mitigating issues where
intermediate evidence errors constrain single-shot decoding.
While MARINE’s result of 46.0% is slightly below Tongyi
DeepResearch’s 46.7%, it offers a generalizable framework
that avoids the training overhead inherent in Tongyi’s agen-
tic RL process. Notably, MARINE achieves a performance
with the 80B-A3B model, matching Kimi-K2’s results, de-
spite the significantly smaller model size.
5.3. Performance Comparison with Baseline Methods
In this section, MARINE’s performance is evaluated against
baseline methods, specifically single-sample CoT, Self-
Refine, and BoN under a fixed agent invocation budget.
The results highlight MARINE’s superior ability to leverage
iterative multi-agent trajectory refinement.
Table 1 presents the performance of MARINE across differ-
ent LLM sizes under a fixed total agent invocation budget
of T + 1 = 9 (average@3). MARINE achieves the highest
pass@1 accuracy compared to baseline methods. Specif-
ically, with the 685B model, MARINE reaches a pass@1
score of 46.0%, surpassing BoN at 35.3%, Self-Refine at
40.5%, and single-sample CoT at 26.0%. Similarly, with the
80B model, MARINE achieves 28.0%, outperforming BoN
(22.8%), Self-Refine (11.1%), and CoT (10.0%). These re-
sults highlight MARINE’s effectiveness in leveraging mul-
tiple agents for iterative reference trajectory refinement,
which enhances the reasoning process by isolating contexts
and iteratively improving the quality of candidate trajecto-
ries. Compared with Self-Refine, MARINE employs con-
trastive reflection to select promising partial trajectories
within each batch, ensuring more precise trajectory correc-
tions. Furthermore, MARINE integrates complementary
information from candidates across the batch, rather than
relying solely on intra-batch diversity, thereby consistently
outperforming BoN under the same computational budget.
7


--- Page 8 ---
MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement
Table 1. Score under a fixed total agent invocation budget T + 1 =
9 (average@3, %).
Method
MARINE
BoN
Self-Refine
CoT
685B
46.0↑
35.3
40.5
26.0
80B
28.0↑
22.8
11.1
10.0
Table 2. Effect of batch size Mk on MARINE pass@1 under a
fixed total agent invocation budget T + 1 = 9 (average@3, %).
Mk
1
2
4
8
685B
40.5
46.0↑
41.2
39.4
80B
11.1
28.0↑
24.9
22.8
5.4. Ablation Study on Batch Size
Table 2 presents the pass@1 scores for the MARINE
framework under a fixed total agent invocation budget of
T + 1 = 9, with varying batch sizes Mk ranging from 1 to
8, for both 685B and 80B language models. For the 685B
model, the highest pass@1 score, 46.0%, is achieved with
a batch size of Mk = 2, while smaller (1) and larger (4, 8)
batch sizes yield lower scores of 40.5%, 41.2%, and 39.4%,
respectively. For the 80B model, the best performance is
again obtained with Mk = 2, yielding a score of 28.0%,
with scores for Mk = 1 dropping to 11.1%, and larger batch
sizes showing diminishing returns (24.9% for Mk = 4,
22.8% for Mk = 8).
These results reinforce the importance of batch size selec-
tion in the MARINE framework. The optimal batch size of
Mk = 2 maximizes the pass@1 performance, aligning with
the theoretical prediction that, under a fixed invocation bud-
get, the expected improvement per invocation is maximized
by the smallest feasible batch size. Smaller batch sizes
are ineffective due to the lack of sufficient diversity in can-
didate trajectories, which prevents the refinement process
from reliably identifying improvements. On the other hand,
larger batch sizes (4 and 8) lead to diminishing returns as
the marginal gain per agent call decreases. This is consistent
with the theoretical trade-off between exploration width and
refinement depth, where larger batches increase the candi-
date pool and extend the context, making it harder to isolate
relevant improvements and amplifying the impact of noisy
or redundant content. Overall, the results empirically vali-
date the theoretical insights, highlighting that the optimal
batch size balances exploration and refinement efficiently,
thus maximizing performance within the given budget.
5.5. Ablation Study on Refinement Rounds
Table 3 presents the pass@1 scores for the MARINE frame-
work across varying refinement rounds K under a fixed
total agent invocation budget of T = 2K + 1. For the 685B
model, MARINE shows consistent improvement, reaching
46.0% at K = 4, outperforming Self-Refine, which reaches
40.5%. The pass@N baseline exceeds MARINE at 56.4%
Table 3. Effect of refinement rounds K under matched total agent
invocations 2K + 1 for MARINE, Self-Refine and oracle style
pass@N (average@3, %).
K
1
2
3
4
MARINE (685B)
35.6
41.2
44.3
46.0
Self-Refine (685B)
32.9
37.0
39.4
40.5
pass@N (685B)
44.3↑
49.5↑
54.0↑
56.4↑
MARINE (80B)
22.1↑
26.0↑
27.0
28.0
Self-Refine (80B)
11.1
10.0
11.1
11.1
pass@N (80B)
19.0
26.0
30.8↑
32.9↑
by K = 4. For the 80B model, MARINE demonstrates a
particularly strong advantage, surpassing pass@N at ( K =
1 ) with 22.1% compared to 19.0%. At K = 4, MARINE
achieves 28.0%, outperforming pass@N’s 32.9%. Self-
Refine lags behind, remaining at 11.1% across all rounds.
These results highlight the superior performance of MA-
RINE, particularly at smaller K values and for smaller mod-
els, where it not only outperforms Self-Refine but also ex-
ceeds pass@N. While pass@N eventually outperforms MA-
RINE, the framework excels in efficiency, showing compet-
itive performance in resource-constrained scenarios. These
findings emphasize the effectiveness of early refinement
rounds in improving pass@1, while also demonstrating the
diminishing returns as the number of rounds increases, es-
pecially for smaller models where the impact of each addi-
tional round is less pronounced.
6. Conclusion
MARINE establishes a framework reconceptualizing test-
time reasoning as iterative refinement of persistent ref-
erence trajectories via multi-agent collaboration, system-
atically transforming base models’ pass@N capabilities
into reliable pass@1 performance. Theoretical analysis
demonstrates minimal feasible batches maximize perfor-
mance under fixed invocation budgets, while logarithmi-
cally growing schedules guarantee continuous improvement.
Empirical validation reveals unprecedented parameter ef-
ficiency: an 80B-parameter model augmented with MA-
RINE matches standalone 1000B-parameter systems, reduc-
ing parameter requirements by over an order of magnitude.
The recursion-based architecture overcomes single-agent
limitations through structured trajectory representation and
conflict-aware integration. This paradigm fundamentally de-
couples reasoning capability from parameter scale, establish-
ing a new approach to parameter-efficient LLM deployment.
Crucially, under a fixed computational budget, MARINE
yields higher-quality samples for alignment and optimiza-
tion workflows than traditional sampling-and-ranking strate-
gies (i.e., the scenario where k = 1). Hence, it exhibits
significant potential for enhancing post-training efficiency.
8


--- Page 9 ---
MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement
Impact Statement
MARINE establishes a theoretically-grounded framework
for enhancing the reliability and efficiency of LLM-based
agents through recursive trajectory refinement rather than
single-pass decoding or unstructured multi-sampling. Rig-
orous analysis demonstrates that minimal feasible batches
maximize expected performance under fixed invocation bud-
gets, while logarithmically growing batch schedules guar-
antee monotonic trajectory improvement with arbitrarily
high probability—establishing critical worst-case perfor-
mance bounds for reliability-sensitive applications. Em-
pirical validation confirms unprecedented parameter effi-
ciency, with smaller models augmented by MARINE match-
ing or exceeding the performance of substantially larger
standalone systems. Notably, this methodology optimizes
reasoning trajectories under fixed computational budgets
and exhibits substantial potential to enhance post-training ef-
ficiency. Specifically, it generates samples of higher quality
than those produced by conventional sampling-and-ranking
strategies (i.e., k = 1) for alignment and optimization pro-
cedures. Consequently, this capability can reduce both data
volume and computational requirements during subsequent
fine-tuning phases. While these advances promise more
capable and efficient reasoning systems, they also introduce
risks, including potential misinformation propagation and
strategic manipulation. Therefore, responsible deployment
necessitates robust safeguards such as comprehensive au-
diting of external tools and evaluation signals, continuous
monitoring for failure modes, and explicit constraints on
high-stakes applications.
References
Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-
Sarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T.,
et al. Training a helpful and harmless assistant with rein-
forcement learning from human feedback. arXiv preprint
arXiv:2204.05862, 2022.
Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V.,
R´e, C., and Mirhoseini, A. Large language monkeys:
Scaling inference compute with repeated sampling. arXiv
preprint arXiv:2407.21787, 2024.
Chan, C.-M., Chen, W., Su, Y., Yu, J., Xue, W., Zhang, S.,
Fu, J., and Liu, Z. ChatEval: Towards better LLM-based
evaluators through multi-agent debate. arXiv preprint
arXiv:2308.07201, 2023.
Chen, Q., Qin, L., Liu, J., Peng, D., Guan, J., Wang, P., Hu,
M., Zhou, Y., Gao, T., and Che, W. Towards reasoning
era: A survey of long chain-of-thought for reasoning
large language models. arXiv preprint arXiv:2503.09567,
2025.
Chen, S., Liu, Y., Han, W., Zhang, W., and Liu, T. A
survey on LLM-based multi-agent system: Recent ad-
vances and new frontiers in application. arXiv preprint
arXiv:2412.17481, 2024.
Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program
of thoughts prompting: Disentangling computation from
reasoning for numerical reasoning tasks. arXiv preprint
arXiv:2211.12588, 2022.
Gui, L., Gˆarbacea, C., and Veitch, V. BoNBoN alignment
for large language models and the sweetness of best-of-n
sampling. Advances in Neural Information Processing
Systems, 37:2851–2885, 2024.
Guo, T., Chen, X., Wang, Y., Chang, R., Pei, S., Chawla,
N. V., Wiest, O., and Zhang, X. Large language model
based multi-agents: A survey of progress and challenges.
arXiv preprint arXiv:2402.01680, 2024.
Hao, S., Gu, Y., Ma, H., Hong, J., Wang, Z., Wang, D., and
Hu, Z. Reasoning with language model is planning with
world model. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing, pp.
8154–8173, 2023.
Hong, C., Guo, X., Singh, A. C., Choukse, E., and Ustiugov,
D. Slim-SC: Thought pruning for efficient scaling with
self-consistency. In Proceedings of the 2025 Conference
on Empirical Methods in Natural Language Processing,
pp. 34488–34505, 2025.
Huang, C., Huang, L., Leng, J., Liu, J., and Huang, J. Effi-
cient test-time scaling via self-calibration. arXiv preprint
arXiv:2503.00031, 2025.
Kang, Z., Zhao, X., and Song, D. Scalable best-of-N selec-
tion for large language models via self-certainty. arXiv
preprint arXiv:2502.18581, 2025.
Li, M., Chen, J., Chen, L., and Zhou, T. Can LLMs speak
for diverse people? tuning LLMs via debate to gener-
ate controllable controversial statements. arXiv preprint
arXiv:2402.10614, 2024a.
Li, X., Wang, S., Zeng, S., Wu, Y., and Yang, Y. A survey
on LLM-based multi-agent systems: workflow, infrastruc-
ture, and challenges. Vicinagearth, 1(1):9, 2024b.
Li, Z.-Z., Zhang, D., Zhang, M.-L., Zhang, J., Liu, Z., Yao,
Y., Xu, H., Zheng, J., Wang, P.-J., Chen, X., et al. From
system 1 to system 2: A survey of reasoning large lan-
guage models. arXiv preprint arXiv:2502.17419, 2025.
Liang, T., He, Z., Jiao, W., Wang, X., Wang, Y., Wang, R.,
Yang, Y., Shi, S., and Tu, Z. Encouraging divergent think-
ing in large language models through multi-agent debate.
9


--- Page 10 ---
MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement
In Proceedings of the 2024 conference on empirical meth-
ods in natural language processing, pp. 17889–17904,
2024.
Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker,
B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and
Cobbe, K. Let’s verify step by step. In The Twelfth
International Conference on Learning Representations,
2023.
Lu, J., Jiang, S., Zhang, H., Zhu, C., Xie, L., Zhong, C.,
Chen, H., Zhu, Y., Du, Y., Gao, Y., et al.
Co-sight:
Enhancing LLM-based agents via conflict-aware meta-
verification and trustworthy reasoning with structured
facts. arXiv preprint arXiv:2510.21557, 2025.
Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao,
L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S.,
Yang, Y., et al. Self-refine: Iterative refinement with self-
feedback. Advances in Neural Information Processing
Systems, 36:46534–46594, 2023.
Nathani, D., Wang, D., Pan, L., and Wang, W. MAF: Multi-
aspect feedback for improving reasoning in large lan-
guage models. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing, pp.
6591–6616, 2023.
Pan, L., Saxon, M., Xu, W., Nathani, D., Wang, X.,
and Wang, W. Y. Automatically correcting large lan-
guage models: Surveying the landscape of diverse self-
correction strategies. arXiv preprint arXiv:2308.03188,
2023.
Park, S., Liu, X., Gong, Y., and Choi, E. Ensembling large
language models with process reward-guided tree search
for better complex reasoning. In Proceedings of the 2025
Conference of the Nations of the Americas Chapter of
the Association for Computational Linguistics: Human
Language Technologies (Volume 1: Long Papers), pp.
10256–10277, 2025.
Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D.,
Ermon, S., and Finn, C. Direct preference optimiza-
tion: Your language model is secretly a reward model.
Advances in neural information processing systems, 36:
53728–53741, 2023.
Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang,
H., Zhang, M., Li, Y., Wu, Y., et al. DeepSeekMath: Push-
ing the limits of mathematical reasoning in open language
models. arXiv preprint arXiv:2402.03300, 2024.
Shridhar, K., Jhamtani, H., Fang, H., Van Durme, B., Eisner,
J., and Xia, P. SCREWS: A modular framework for rea-
soning with revisions. arXiv preprint arXiv:2309.13075,
2023.
Subramaniam, V., Du, Y., Tenenbaum, J. B., Torralba, A.,
Li, S., and Mordatch, I. Multiagent finetuning: Self im-
provement with diverse reasoning chains. arXiv preprint
arXiv:2501.05707, 2025.
Sun, H., Haider, M., Zhang, R., Yang, H., Qiu, J., Yin, M.,
Wang, M., Bartlett, P., and Zanette, A. Fast best-of-N
decoding via speculative rejection. Advances in Neural
Information Processing Systems, 37:32630–32652, 2024.
Tran, K.-T., Dao, D., Nguyen, M.-D., Pham, Q.-V.,
O’Sullivan, B., and Nguyen, H. D. Multi-agent collabo-
ration mechanisms: A survey of LLMs. arXiv preprint
arXiv:2501.06322, 2025.
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang,
S., Chowdhery, A., and Zhou, D. Self-consistency im-
proves chain of thought reasoning in language models.
arXiv preprint arXiv:2203.11171, 2022.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,
E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting
elicits reasoning in large language models. Advances in
neural information processing systems, 35:24824–24837,
2022.
Xu, F., Hao, Q., Zong, Z., Wang, J., Zhang, Y., Wang,
J., Lan, X., Gong, J., Ouyang, T., Meng, F., et al. To-
wards large reasoning models: A survey of reinforced
reasoning with large language models. arXiv preprint
arXiv:2501.09686, 2025.
Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao,
Y., and Narasimhan, K. Tree of thoughts: Deliberate
problem solving with large language models. Advances
in neural information processing systems (NeurIPS), 36:
11809–11822, 2023.
Zeng, Z., Cheng, Q., Yin, Z., Zhou, Y., and Qiu, X. Revisit-
ing the test-time scaling of o1-like models: Do they truly
possess test-time scaling capabilities?
arXiv preprint
arXiv:2502.12215, 2025.
Zhang, Q., Lyu, F., Sun, Z., Wang, L., Zhang, W., Guo,
Z., Wang, Y., King, I., Liu, X., and Ma, C. What, how,
where, and how well? a survey on test-time scaling in
large language models. CoRR, 2025.
Zhou, D., Sch¨arli, N., Hou, L., Wei, J., Scales, N., Wang,
X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q., et al.
Least-to-most prompting enables complex reasoning in
large language models. arXiv preprint arXiv:2205.10625,
2022.
Zhou, P., Leon, B., Ying, X., Zhang, C., Shao, Y.,
Ye, Q., Chong, D., Jin, Z., Xie, C., Cao, M., et al.
BrowseComp-ZH: Benchmarking web browsing ability
of large language models in Chinese. arXiv preprint
arXiv:2504.19314, 2025.
10


--- Page 11 ---
MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement
A. Proof of Theorem 4.3
Since the candidates are independently and identically distributed within the same round, the probability of obtaining at
least one improvement in the k-th round is
Psucc(Mk, k) := P(Ek)
(9)
= 1 −P

∀i, S(k)
i
≤rk

(10)
= 1 −(1 −pk)Mk,
(11)
where pk is defined in (6). The expected improvement in reference quality at the k-th round is
E[rk+1 −rk] = gk · Psucc(Mk, k)
(12)
= gk
 1 −(1 −pk)Mk
,
(13)
where gk denotes the expected gain conditional on success in round k.
To compare the cost effectiveness of different batch sizes Mk, define the expected gain per invocation as
hk(Mk) := E[rk+1 −rk]
Mk
(14)
= gk · 1 −(1 −pk)Mk
Mk
.
(15)
Since gk does not depend on Mk, it suffices to study
˜hk(Mk) := 1 −(1 −pk)Mk
Mk
.
(16)
The following simple calculus fact will be useful. For any fixed pk ∈(0, 0.5), consider
˜h(Mk) := 1 −(1 −pk)Mk
Mk
,
(17)
where Mk ∈Z+ and Mk ≥2. Increasing Mk raises the probability that at least one agent succeeds in a given round, but the
marginal benefit per invocation diminishes, which is exactly captured by the decrease of ˜h(Mk) in Mk.
Extending Mk to a real variable x > 0, set a := 1 −pk ∈(0.5, 1) and define f(x) := 1 −ax for x > 0. Then
˜h(x) = f(x)
x
,
x > 0.
(18)
The first derivative of ˜h is
˜h′(x) = ax(1 −x ln a) −1
x2
.
(19)
The sign of ˜h′(x) is determined by
N(x) := ax(1 −x ln a) −1.
(20)
Write b := −ln a > 0 and note that ax = e−bx. Then
N(x) = e−bx(1 + bx) −1.
(21)
Define g(t) := e−t(1 + t) with t := bx > 0. Its derivative satisfies
g′(t) = −te−t < 0
for all t > 0,
(22)
so g(t) is strictly decreasing on (0, ∞). Since limt→0+ g(t) = 1, it follows that g(t) < 1 for all t > 0, and therefore
N(x) = g(t) −1 < 0
for all x > 0.
(23)
Consequently ˜h′(x) < 0 for all x > 0, so ˜h(x) is strictly decreasing on (0, ∞). Under Assumption 4.1, it follows that ˜h(x)
is strictly decreasing at the integer points Mk = 2, 3, · · · , which implies
˜h(2) > ˜h(3) > ˜h(4) > . . .
(24)
for integer Mk ≥2, which proves Theorem 4.3.
11


--- Page 12 ---
MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement
B. Proof of Theorem 4.4
Let Fk denote the event that no corrected trajectory surpasses the reference trajectory in the k-th iteration, with probability
P(Fk). The core question is whether, for sufficiently large k, the probability that almost every iteration improves the
reference trajectory from some round onward approaches 1. To formalize this, consider iterations starting from a threshold
˜k:
k = ˜k, ˜k + 1, . . .
(25)
We aim for the probability of the event
F˜k ∪F˜k+1 ∪. . .
(26)
to be sufficiently small, i.e., the probability that failure occurs beyond round ˜k with negligible likelihood approaches 1. By
the union bound (Boole’s inequality):
P
 ∞
[
k=˜k
Fk

≤
∞
X
k=˜k
P(Fk) =
∞
X
k=˜k
(1 −pk)Mk.
(27)
Thus, if Mk can be chosen such that the tail sum P∞
k=˜k(1 −pk)Mk is sufficiently small, the probability of no failure after
round K converges to 1. Assume that pk ≥p > 0 for all k (or at least after a certain number of rounds), which implies:
(1 −pk)Mk ≤(1 −p)Mk.
(28)
Then, select a target sequence εk with a sufficiently fast decay rate, e.g.,
εk := 1
k2 ,
(29)
and require that for each k, Mk satisfies:
(1 −p)Mk ≤εk = 1
k2 .
(30)
This is equivalent to:
Mk ≥
ln εk
ln(1 −p) =
−2 ln k
| ln(1 −p)|.
(31)
Therefore, Mk must grow at least at the rate of O(log k). With this choice, we have:
∞
X
k=1
(1 −pk)Mk ≤
∞
X
k=1
(1 −p)Mk ≤
∞
X
k=1
1
k2 < ∞.
(32)
Hence, based on equation (27), we conclude that for any ˜k, the probability of failure occurring from round ˜k onward has a
controllable upper bound:
P
 ∞
[
k=˜k
Fk

≤
∞
X
k=˜k
1
k2 →π2
6 −
˜k−1
X
k=1
1
k2
(series sum of the Basel problem).
(33)
Next, define G˜k′ = T∞
k=˜k′ Fk (where Fk denotes the non-failure event at round k), so that
P(G˜k′) = 1 −P


∞
[
k=˜k′
Fk

.
(34)
12


--- Page 13 ---
MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement
Since P∞
k=1
1
k2 < ∞, by the tail property of convergent series, for any δ > 0, there exists ˜k′ ≥˜k such that P∞
k=˜k′
1
k2 < δ.
Combining Boole’s inequality with P(Fk) ≤
1
k2 , it obtain:
P


∞
[
k=˜k′
Fk

≤
∞
X
k=˜k′
1
k2 < δ.
(35)
Thus, P(G˜k′) ≥1 −δ. This establishes Theorem 4.4.
13
