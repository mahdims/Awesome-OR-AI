--- Page 1 ---
OptMATH: A Scalable Bidirectional Data Synthesis Framework
for Optimization Modeling
Hongliang Lu1,∗, Zhonglin Xie2,∗, Yaoyu Wu1, Can Ren3, Yuxuan Chen3, Zaiwen Wen2,†
1College of Engineering, Peking University
2Beijing International Center for Mathematical Research, Peking University
3School of Mathematics Science, Peking University
∗Equal contribution
†Corresponding author: wenzw@pku.edu.cn
Abstract
Despite the rapid development of large lan-
guage models (LLMs), a fundamental chal-
lenge persists: the lack of high-quality opti-
mization modeling datasets hampers LLMs’
robust modeling of practical optimization
problems from natural language descriptions
(NL). This data scarcity also contributes to
the generalization difficulties experienced by
learning-based methods.
To address these
challenges, we propose a scalable framework
for synthesizing a high-quality dataset, named
OptMATH. Starting from curated seed data
with mathematical formulations (MF), this
framework automatically generates problem
data (PD) with controllable complexity. Then,
a back-translation step is employed to obtain
NL. To verify the correspondence between the
NL and the PD, a forward modeling step fol-
lowed by rejection sampling is used. The ac-
cepted pairs constitute the training part of
OptMATH. Then a collection of rejected pairs
is identified and further filtered. This collec-
tion serves as a new benchmark for optimiza-
tion modeling, containing difficult instances
whose lengths are much longer than these of
NL4OPT and MAMO. Through extensive ex-
periments, we demonstrate that models of
various sizes (0.5B-32B parameters) trained
on OptMATH achieve superior results on mul-
tiple modeling benchmarks, thereby validat-
ing the effectiveness and scalability of our ap-
proach. Our dataset is publicly available at
https://github.com/AuroraLHL/OptMATH.
1
Introduction
Automatic translation of natural language de-
scriptions of optimization problems into solver-
ready formats is a critical step in democratiz-
ing access to optimization techniques. This
capability would enable individuals without
expertise in optimization to leverage the power
of optimization for solving real-world prob-
lems across various domains, including logis-
tics [32], finance [18], and engineering [64].
Known as optimization modeling, this task
has long been challenging due to the inher-
ent ambiguity of natural language and the
need for a deep understanding of optimization
modeling principles. The manual process of
formulating an optimization problem typically
involves iterative refinement and demands sig-
nificant mastery of the relevant techniques,
making it time-consuming and inaccessible to
many practitioners.
Recent advances in Large Language Mod-
els (LLMs), such as ChatGPT [14], GPT-
4 [59], and OpenAI’s o1 [58], have demon-
strated remarkable capabilities in understand-
ing natural language and performing complex
reasoning tasks.
Notably, the introduction
of o1 has significantly enhanced the perfor-
mance of LLMs in mathematical reasoning,
achieving state-of-the-art results on challeng-
1
arXiv:2502.11102v2  [cs.AI]  21 Feb 2025


--- Page 2 ---
ing datasets including AIME (2024), GPQA,
and CodeForces. However, optimization mod-
eling presents unique challenges. Unlike grade-
school mathematics, where problems typically
have a single correct solution, optimization
problems can often be approached using mul-
tiple valid models, making the task semi-open-
ended. Furthermore, optimization frequently
relies on a vast body of empirical knowledge
that is less formally structured than purely
mathematical concepts. As a result, research
has shown that directly applying LLMs to op-
timization modeling tasks yields suboptimal
outcomes [63].
LLMs for Optimization Modeling. To
address this issue, recent efforts have explored
various strategies.
Research on leveraging
LLMs for optimization modeling typically fol-
lows two main approaches.
The first uses
prompt engineering techniques to guide LLMs
in generating or refining optimization models,
without modifying the underlying model pa-
rameters. Examples include the NL4Opt com-
petition [63], which aims to extract optimiza-
tion formulations from natural language de-
scriptions, and OptiMUS [3], which proposes
an agent-based prompt engineering method.
More recently, Autoformulation [6] combines
Monte Carlo tree search with LLMs to address
optimization modeling. While these methods
rely heavily on the base capabilities of general-
purpose LLMs, they do not yield fundamental
advancements beyond refined prompting.
The second approach centers on fine-tuning,
wherein model parameters are adapted using
specially curated or synthesized optimization
datasets. ORLM [71] introduces OR-Instruct,
a data augmentation framework for optimiza-
tion problems, and demonstrates performance
gains via Supervised Fine-Tuning on a foun-
dation model. LLMOPT [44] likewise applies
a multi-instruction fine-tuning strategy and
self-correction mechanisms. However, most
of these methods generate small amounts of
synthetic training data—often of inconsistent
quality and insufficient complexity.
Conse-
quently, their ability to generalize to more
sophisticated optimization tasks is limited.
LLM-Based Data Synthesis for Op-
timization. Data synthesis methods have
become essential for addressing data scarcity
and enhancing the performance of LLMs.
According to [75], these methods can be
broadly categorized into two main approaches:
data augmentation and data synthesis. OR-
Instruct [71, 76] exemplifies a data augmen-
tation approach, following the self-instruct
framework to expand existing datasets. On
the other hand, MILP-Evolve [49] introduces
an evolutionary framework designed to gener-
ate diverse mixed-integer linear programming
(MILP) problems using LLMs. While MILP-
Evolve represents a significant step forward in
synthetic data generation, it does not address
the critical challenge of translating natural
language descriptions into mathematical opti-
mization models.
Instance Generation for Optimization.
Recent research in MILP instance genera-
tion has evolved along two primary axes:
learning-based structural synthesis and rule-
based distribution expansion. The learning-
based paradigm addresses data scarcity by
developing generative models that preserve
instance hardness and constraints. Examples
include the bipartite graph variational autoen-
coder framework proposed by [31], the block
decomposition operators for constraint ma-
trices introduced in [52], the duality-driven
feasibility guarantees established by [74], and
the adaptive constraint modification mecha-
nisms developed in [36]. The rule-based ap-
proach leverages instance space analysis tech-
niques [4, 70, 5] to guide the generation of
more diverse instance distributions [67, 9]. Al-
ternatively, it may rely on manually selected
features to control the characteristics of the
generated instances [10].
Our Contributions. We propose a scal-
able bidirectional synthesis framework that ad-
dresses the critical challenge of data scarcity in
optimization modeling through triplet-aligned
(NL, MF, PD) data generation and rigorous
validation. Our framework uniquely integrates
a closed-loop workflow with optimal value
matching, ensuring semantic equivalence be-
tween NL, MF, and PD. This approach demon-
strates exceptional scalability.
The frame-
2


--- Page 3 ---
work’s domain adaptability is evidenced by
coverage of 10+ real-world applications (e.g.,
logistics, energy, finance) through 53 seed
generators, with manual analysis confirming
99.6% equivalence accuracy across all triplets.
We introduce OptMATH-Train, a large scale
verified optimization modeling dataset con-
taining rigorously validated (NL, MF, PD)
triplets. Each triplet undergoes three-stage
quality control:
mathematical consistency
checks (MF-to-PD compilation), semantic fi-
delity validation (PD-to-NL backtranslation),
and solution equivalence verification through
solver-based rejection sampling.
From re-
jected instances, we curate OptMATH-Bench,
a challenging benchmark comprising “hard in-
stances” characterized by extended natural
language contexts (2.9× longer than MAMO
EasyLP) and complex constraints. We fur-
ther span it using various problems including
LP, MILP, IP, NLP, SOCP. This benchmark
provides the standardized evaluation for long-
context optimization modeling.
Finally, extensive experiments demonstrate
the efficacy of our framework. Models trained
on the OptMATH-Train dataset achieve state-
of-the-art performance on multiple established
modeling benchmarks, including NL4OPT
and MAMO. These results definitively vali-
date the effectiveness and scalability of our
framework for generating high-quality opti-
mization modeling datasets.
2
Backgrounds & Overview
In mathematical optimization theory, a canon-
ical optimization problem can be formulated
as:
min
x
g(x),
subject to
ci(x) = 0,
i ∈E,
ci(x) ≥0,
i ∈I,
(2.1)
where x ∈Rn denotes the decision vector.
The objective function g : Rn →R assigns a
scalar value to each candidate solution, which
we seek to minimize.
The constraint func-
tions ci : Rn →R define the feasible region
through equality constraints indexed by E and
inequality constraints indexed by I.
To formalize the optimization modeling
problem, we define several key concepts and
illustrate them using a concrete example in
Appendix D.1.
For a specific optimization
problem, we define NL as the natural lan-
guage description, which corresponds to the
“Input-Natural Language Description” in the
example. We represent the LP/MPS file, the
concrete mathematical expression (correspond-
ing to the “Output-Instance Formulation”),
and any other solver-ready formats, such as
executable Python code with Gurobi shown in
Appendix D.1, as problem data (PD). A com-
mon characteristic of these representations is
that they allow us to obtain the optimal value
of the problem by invoking a solver based
on the PD. Mathematical formulation (MF)
refers to formulation where concrete numbers
are not yet specified, corresponding to the
“Output-General Formulation” in the example.
We emphasize that, in subsequent sections, we
may use different forms of PD. However, these
forms essentially carry the same information
about the problem and can be inferred from
the context without ambiguity. We use differ-
ent forms of PD to facilitate their integration
into the workflow and to enhance clarity in
various contexts.
Modern solvers such as Gurobi and Mosek
[38, 56] can efficiently solve problems stored
with PDs using algorithms like interior-point
methods [45]. However, practical challenges
remain. In real-world applications, one of the
main difficulties lies in converting informal
NLs of problems into precise MFs. Moreover,
extracting the PDs from NLs poses an ad-
ditional significant challenge. Traditionally,
this process has required deep optimization
expertise [11], but recent advances in LLMs
offer promising opportunities to automate this
transformation.
Let Aθ represent an LLM parameterized by
θ. The formulation for increasing the modeling
capability of the LLM can be expressed as:
max
θ
E(NL,MF,PD)∼D[Q(NL,MF,PD)(MF′, PD′)],
(2.2)
s.t.
(MF′, PD′) = Aθ(promptM(NL)),
(2.3)
3


--- Page 4 ---
where promptM is a modeling prompt tem-
plate mapping NL to MF′ and PD′.
The
quality metric Q evaluates the generated
(MF′, PD′)
pairs
based
on
the
verified
(NL, MF, PD) triplet.
Constraint (2.3) for-
malizes the automated formulation process
(Autoformulation), as depicted in the exam-
ple in Appendix D.1. Optimizing θ relies on
having a large corpus of high-quality triplets
(NL, MF, PD). To address this requirement,
we propose a systematic framework for gener-
ating synthetic training data that maintains
mathematical rigor.
An Overview of Our Pipeline.
The
pipeline of our framework is presented in Fig-
ure 1. In the reverse data generation phase,
we collect optimization problems from two
sources: (1) LP/MPS files from challenging
benchmarks such as MIPLIB 2017 [34] and
netlib [57, 29], and (2) over 50 expert-curated
seed problem generators covering diverse op-
timization scenarios. Through our carefully
designed backtranslation pipeline, we leverage
both the LP files and their MFs to generate
high-quality NLs of optimization problems.
Notably, using an LLM-based feedback work-
flow with evaluation, our collected generators
can produce tremendous PDs with control-
lable varying difficulty levels. This enables us
to effectively address the data scarcity chal-
lenge in training learning-based optimization
methods.
In the forward modeling and evaluation pro-
cess, we utilize our trained AutoFormulator
to translate the generated NLs back into PDs.
Specifically, in this phase, all PDs are rep-
resented as solver code, which can then be
exported as LP files. We then implement a
rigorous rejection sampling strategy, where
only instances whose optimal objective values
match between the original and generated LP
files are retained. This equivalence-based fil-
tering mechanism ensures the high quality of
our OptMATH-Train dataset by guaranteeing
the semantic consistency of each instance.
Building upon the high-quality instances ob-
tained through rejection sampling, we employ
various data augmentation strategies to fur-
ther enhance the diversity and coverage of our
dataset. This enriched collection of training
pairs is then utilized to fine-tune a foundation
model, leading to AutoFormulator, a special-
ized model specifically designed for automated
mathematical optimization modeling.
3
Feedback-Driven PD Generation
The mature development of the optimization
community has provided us with access to
many high-quality optimization PDs. These
PDs are typically stored in standardized for-
mats like MPS or LP files. To effectively lever-
age these resources, we began by curating
over 50 seed problem classes sourced from a
variety of optimization journals and websites
(see Appendix A.2 for details). For each i-th
problem class, we developed a corresponding
instance generator Gi. This generator takes
a problem-specific configuration as input and
outputs a probability distribution over PDs.
The distribution is designed to produce PDs
with varying scales and complexities, which
are controllable through adjustable configura-
tions. Before delving into the controlled gen-
eration process for the PDs, we first explain
how the complexity of PDs can be measured.
Measuring the Modeling Complexity.
The complexity of formulating and solving a
MIP problem depends on modeling choices
such as the types of variables, the forms of
constraints, and auxiliary modeling techniques
employed. We introduce a scoring function S
defined as:
S(PD) = αbinNbin + αintNint + αcontNcont
+ βlinNlin + βindicNindic + βquadNquad
+ βgenNgen + γBigM fBigM + δexpr Lexpr,
(3.1)
where Nbin, Nint, Ncont are the number of bi-
nary, integer, and continuous variables, re-
spectively. Similarly, Nlin, Nindic, Nquad, Ngen
represent the number of linear, indicator,
quadratic, and general nonlinear constraints.
The term fBigM is a factor reflecting the fre-
quency of Big-M formulations, and Lexpr is
the average number of terms per constraint
and the objective function, which captures
the structural information of the expressions.
4


--- Page 5 ---
Benchmark
(Hard LP Files) 
Natural Language 
Description 
 Generated LP Files
Generator
AutoFormulator
min έΤπ
s.t. 푨π ≤ά
Math Formula
 LP Files
OptMATH-Train
Augmentation
Augmented Data
Fine Tuning
Base Model
Filter
Backtranslation
Pipeline
Rejection Samping
Reject
Step2: Forward Modeling and Evaluation
Step1: Reverse Data Generation
Step3: Fine-Tuning
LLM/Expert
Quality Filtering
Figure 1: An overview of our scalable, bidirectional data synthesizing pipeline.
Lastly, the weights α·, β·, γBigM, δexpr are tun-
able parameters reflecting the contribution
of each component to the overall complex-
ity. To illustrate it, we provide an example
in Appendix B.1 that considers a MIP prob-
lem incorporating multiple constraint types
to optimize production costs for two products
under resource constraints.
Selecting Parameters to Control the
Complexity. We now present the workflow
in Algorithm 1 for selecting parameter config-
urations for instance generator Gi to generate
PDs fitting the complexity, feasibility, and
solving time requirements. The prompt tem-
plates are illustrated in Appendix E.4. As
formalized in Algorithm 1, the process begins
by specifying target bounds. Then a template
promptIC for initializing the configuration are
incorporated. After obtaining the configura-
tion, we generate N PDs using it. We then
evaluate the generated PDs through the com-
plexity score, solving time, and feasibility satis-
factory. Then, a feedback promptRC is created
based on the statistics of these metrics over
the N generated PDs. The LLM iteratively
adjusts parameters based on feedback from
solved instances, ultimately converging to a
configuration that satisfy predefined criteria.
This ensures generated PDs remain both ex-
pressive and tractable by adhering to runtime
thresholds.
4
The Data Synthesis Framework
This section presents our bidirectional scalable
data synthesis framework. In this section, all
of the PDs are in solver code form (see subsec-
tion 4.2 for more details). Let L represents the
LLM employed on the reverse data generation
phase, and Aθ for our fine-tuned AutoFor-
mulator with weights θ. We define promptI,
promptC, promptR as the prompt templates
that accept certain inputs for the initial gener-
ation, self-critism, and self-refinement stages.
The algorithm is formalized in Algorithm 2,
with an illustrative example of the backtrans-
lation process shown in Figure 13.
The final OptMATH dataset D is con-
structed by collecting all valid quadruples
(NLi,j, MF′
i,j, PD′
i,j, OVi,j) that pass the val-
idation process, where MF′
i,j and PD′
i,j rep-
resent the generated mathematical formula-
tion and problem data using Aθ, OVi,j is the
optimal value obtained by solving the prob-
lem specified by PDi,j.
By leveraging our
instance generators and the iterative refine-
5


--- Page 6 ---
Algorithm 1 Feedback-Driven Problem Data
Generation
Require: Target
complexity
range
[Smin, Smax],
time
limits
[Tmin, Tmax],
instance generator G, feasibility threshold
Ftarget, max iterations T
Ensure: Configuration
Θ
such
that
for
PDi ∼G(Θ):
1: S(PDi) ∈[Smin, Smax] (complexity), τi ≤
Tmax (solving time),
2: Pr(fi = feasible) ≥Ftarget
3: Initialize parameters via LLM:
4: Θ0 ←L(promptIC(Smin, Smax, Tmin, Tmax))
5: for t = 1 to T do
6:
Generate
N
PDs:
{PDi}N
i=1
←
G(Θt−1)
7:
Compute metrics: S(PDi) (Eq. 3.1),
τi (solving time), fi (feasibility)
8:
Aggregate statistics:
9:
¯St = 1
N
P S(PDi)
10:
¯τt = 1
N
P τi
11:
Ft = 1
N
P I(fi = feasible)
12:
if ¯St ∈[Smin, Smax] and ¯τt ≤Tmax
and Ft ≥Ftarget then
13:
return Θt−1
14:
else
15:
Refine parameters via feed-
back:
16:
Θt
←
L(promptRC( ¯St, ¯τt, Ft; Θt−1))
17:
end if
18: end for
19: return ∅(no valid Θ found)
ment process, this algorithm enables scalable
generation of high-quality data pairs.
The
mathematical equivalence between the gener-
ated formulations and the original instances
is rigorously validated through rejection sam-
pling, ensuring the reliability of our dataset.
A comprehensive discussion of our quality con-
trol and rejection sampling can be found in
Section 4.3.
4.1
Backtranslation Pipeline
To generate high-quality NLs of optimization
problems at scale, we leverage a specific LLM
as the foundation of our pipeline. Recent re-
Algorithm 2 Bidirectional Data Synthesis
Algorithm
Require: Instance pair (MFi, PDi,j), Max It-
eration T
Ensure: (NLi,j, MF′
i,j, PD′
i,j, OVi,j)
1: Initial
generation:
NL
←
L(promptI(MFi, PDi,j))
2: Initialize: SC = SR = Null
3: for k = 1, . . . , T −1 do
4:
Self-Criticize:
5:
SC ←L(promptC(MFi, PDi,j, NL))
6:
Self-Refine:
7:
SR
←
L(promptR(MFi, PDi,j, NL, SC, SR))
8:
if SR is good enough then
9:
break
10:
end if
11: end for
12: NLi,j ←SR
13: AutoFormulation:
14: (MF′
i,j, PD′
i,j) ←Aθ(promptM(NLi,j))
15: OVi,j ←Solve PDi,j by Gurobi
16: OV′
i,j ←Solve PDi,j by Gurobi
17: if OVi,j = OV′
i,j then
18:
return (NLi,j, MF′
i,j, PD′
i,j, OVi,j)
19: else
20:
return Null
21: end if
search has demonstrated that complex tasks
often benefit from iterative refinement ap-
proaches rather than direct generation [53].
This observation aligns with human problem-
solving processes in mathematics, which typ-
ically requires multiple attempts and refine-
ments. Building upon this insight, we design a
three-phase backtranslation pipeline that sys-
tematically improves the quality of generated
descriptions through iterative refinement. All
prompt templates used in this pipeline can be
found in E.1.
Initial Generation. Given the mathemat-
ical formulation MFi and the corresponding
problem data PDi,j of a problem j in i-class,
the LLM generates an initial natural language
description NL using the prompt template
promptI.
This stage requires the model to
comprehend both the mathematical seman-
6


--- Page 7 ---
tics and the instance parameters to produce a
preliminary human-readable description.
Self-Criticism. Using prompt template
promptC, the LLM evaluates the current
description by examining the mathematical
equivalence with MFi, completeness of the
constraints and objective functions, clarity
and comprehensibility, and consistency of the
parameters with PDi,j. The criticism SC in
iteration k incorporates feedback from all pre-
vious iterations to guide improvements.
Self-Refinement. Based on the criticism,
the model generates refined descriptions SR
with the prompt template promptR. The re-
finement process focuses on improving the
mathematical accuracy, completeness of the
constraints, and clarity of the descriptions.
This process iterates for T rounds until a
satisfactory description NLi,j is obtained, with
each iteration potentially improving the qual-
ity of the generated description. Based on our
empirical analysis (see Appendix C.2), we set
T = 1 in the final implementation.
4.2
Forward modeling
Building upon the NLs generated in subsection
4.1, we leverage AutoFormulator to transform
them back into MFs and PDs in solver code
form, enabling rejection sampling for quality
validation. Given a NL as input, AutoFormu-
lator produces two key outputs: a MF and
corresponding PD in solver code form. While
previous works [71, 44] adopted fixed output
formats, our approach is not constrained to
any particular format, as our primary goal
is to obtain correct solver code, with the for-
mulation serving as an intermediate reason-
ing step.
To facilitate genuine mathemati-
cal modeling capabilities rather than superfi-
cial format mapping, we design diverse Chain-
of-Thought (CoT) prompting strategies [78].
This approach generates multiple valid rea-
soning paths and formulation variants for the
same problem, enriching our training data
with diverse modeling perspectives and en-
hancing the model’s mathematical reasoning
capabilities. Detailed implementation of these
CoT strategies is described in Appendix D.1.
4.3
Rejection Sampling
To ensure the quality and mathematical sound-
ness of our generated optimization problem
descriptions, we employ a rejection sampling
strategy [83, 51] to filter and select high-
quality samples from the generated candi-
dates.
As illustrated in Algorithm 2, our rejection
sampling mechanism relies on solution-based
comparison to validate the generated sam-
ples. Specifically, for each generated natural
language description NLi,j, we use AutoFor-
mulator to transform it into a mathematical
formulation MF′
i,j and solver code PD′
i,j, ob-
taining solution OV′
i,j. This solution is then
compared with OVi,j, obtained by directly
solving the original instance PDi,j. A sample
is accepted into our dataset D as a validated
quadruple (NLi,j, MF′
i,j, PD′
i,j, OVi,j) if and
only if OVi,j = OV′
i,j.
While this solution-based validation ap-
proach may not guarantee perfect equivalence
(as problems with identical optimal values may
represent different optimization problems),
our manual analysis of randomly sampled in-
stances (1% of the total dataset) reveals a
remarkable 99.6% accuracy rate. We acknowl-
edge that determining the exact equivalence
between two mathematical formulations re-
mains an open research question worthy of
further investigation. Nevertheless, our cur-
rent approach provides a practical and highly
effective mechanism for ensuring dataset qual-
ity.
5
Fine-Tuning
5.1
Data Augmentation
To improve the diversity of our dataset, we
use data augmentation to augment the train-
ing data. This method generates more non-
standard problems compared to a data gen-
erator, enhancing the model’s generalization
performance.
We create rules for problem
rewriting, semantic substitution, constraint
expansion, and numerical augmentation. For
each instance, a randomly selected rule is used
to prompt the LLMs to generate the corre-
7


--- Page 8 ---
sponding augmented data. The detailed aug-
mentation rules and prompt templates can be
found in Appendix E.7.
For quality control, we employ a specific
LLM L to sample each augmented description
twice independently, followed by the rejection
sampling strategy described in Section 4.3.
This process yields approximately 10 qualified
augmented datasets for each problem, and this
method was applied to augment 50 thousand
instances to complement our original dataset.
5.2
Training the AutoFormulator
We adopt a supervised fine-tuning (SFT)
approach to enhance the AutoFormulator’s
modeling capabilities.
Specifically, we em-
ploy the LoRA algorithm [42] for efficient
parameter-efficient fine-tuning, which signif-
icantly reduces memory requirements while
maintaining model performance by updat-
ing only a small set of adapter parameters.
Using the OptMATH-Train dataset DSFT =
{(NLi, MFi, PDi)}NTrain
i=1
, we train the model
to generate both mathematical formulations
and solver code given problem descriptions.
For each training sample, the input consists of
the problem description NLi, while the target
output is the concatenation of the formulation
and solver code: yi = [MFi; PDi], where [; ]
denotes sequence concatenation. The training
objective follows the standard sequence-to-
sequence loss:
LSFT(θ) = −E(p,y)∼DA
SFT


|y|
X
t=1
log Pθ(yt|y<t, p)


(5.1)
where yt represents the token at position t in
the target sequence, and y<t denotes all pre-
ceding tokens. This approach allows the model
to learn the mapping from natural language
problem descriptions to both mathematical
formulations and solver code within a unified
sequence-to-sequence framework.
6
Experiments
6.1
Statistics
of
the
OptMATH
Dataset
First, using our generators, we generated
a quality-filtered dataset containing over
600,000 LP files, which span 53 distinct prob-
lem types and are distributed across five hard-
ness levels. For more details on the seed data
class, please refer to Appendix A.2. To ensure
computational feasibility, we impose a solving
time threshold and employ a feedback pipeline
that leverages an LLM to regulate both the
complexity and feasibility of the generated in-
stances. Further details on this process are
provided in Appendix B. The distribution of
file lengths across these LP files is visualized in
Figure 2. As shown, the lengths range widely
from 1,000 to 25,000 characters, capturing a
rich variety of problem complexities. The pro-
portions of different lengths are well-balanced,
with a concentration on medium difficulty lev-
els (which are already quite challenging com-
pared to other benchmarks) and a gradual
decline as the problems become harder. Ad-
ditionally, the distribution confirms the effec-
tiveness of our complexity control mechanism.
5,000
10,000
15,000
20,000
25,000
Number of Characters
0
1
2
3
4
5
Percentage (%)
Easy
Medium Easy
Medium
Medium Hard
Hard
Figure 2: Distribution of LP file lengths.
We further conducted a comparative anal-
ysis of problem lengths between OptMATH
and other benchmark datasets, with their av-
erage lengths shown in Figure 3. The analysis
reveals that OptMATH presents significantly
more complex problem descriptions compared
to existing benchmarks. This increased com-
plexity, manifested through longer problem de-
scriptions, poses greater challenges for LLMs,
8


--- Page 9 ---
Table 1: Performance Comparison of Models on Different Benchmarks
Types
Models
Accuracy(pass@1)
Macro
AVG
Micro
AVG
NL4OPT
MAMO
EasyLP
MAMO
ComplexLP
OptMATH
Bench
Baseline
GPT-3.5-turbo
78.0%
79.3%
33.2%
15.0%
51.4%
61.0%
GPT-4
89.0%
87.3%
49.3%
16.6%
60.6%
70.9%
Deepseek-V3
95.9%
88.3%
51.1%
32.6%
67.0%
75.3%
Prompt-based
Chain-of-Experts
64.2%†
–
–
–
–
–
Optimus
78.8%†
–
–
–
–
–
Fine-tuning
ORLM-LLaMA-3-8B
85.7%†
82.3%†
37.4%†
0.0%
51.4%
64.8%
OptMATH-Qwen2.5-7B
94.7%
86.5%
51.2%
24.4%
64.2%
73.5%
OptMATH-Qwen2.5-32B
95.9%
89.9%
54.1%
34.7%
68.7%
76.5%
†: Results reported in their original papers.
as longer descriptions typically demand en-
hanced comprehension and reasoning capabil-
ities.
Average Question Length (character)
0
500
1000
1500
2000
2500
3000
3500
4000
3,315
2,974
1,724
1,045
541
OptMATH-Train
OptMATH-Bench
MAMO ComplexLP
MAMO EasyLP
NL4OPT
Figure 3: Question length analysis
As shown in Figure 4, OptMATH-Bench
has selected a number of representative mathe-
matical optimization problems covering a wide
range of application scenarios, including LP,
MILP, IP, NLP, SOCP and other optimiza-
tion problems.
For details, please refer to
Appendix A.1.
Figure 4: The proportion of problems in dif-
ferent datasets.
To visualize the distribution of differ-
ent benchmarks and OptMATH dataset, we
project their high-dimensional embeddings
onto a 2D space using t-SNE. As shown in
Figure 5, the instances from different sources
form distinct clusters, suggesting that Opt-
MATH effectively captures the diversity of
different problem families. It can be observed
that OptMATH surrounds the area of other
benchmarks. This explains the improvement
on various benchmarks obtained by training
on OptMATH-Train.
−40
−20
0
20
40
−40
−20
0
20
40
NL4OPT
MAMO EasyLP
MAMO ComplexLP
OptMATH-Bench
OptMATH-Train
Figure 5: Visualization of OptMATH and
other benchmarks.
6.2
Autoformulation
Evaluation Benchmarks and Metrics.
We evaluate our fine-tuned model on three
benchmarks: NL4OPT[63], MAMO[43], and
our newly constructed OptMATH-Bench. De-
tailed descriptions of these benchmarks can
be found in Appendix A.1. We use pass@1 ac-
curacy as the evaluation metric, which specif-
ically measures whether the optimal value
obtained by the generated code matches the
9


--- Page 10 ---
0.5B
1.5B
3B
7B
14B
32B
Model Size
0
20
40
60
80
100
Micro Accuracy (%)
0.1%
23.3%
23.2%
1.2%
49.3%
48.0%
48.0%
59.9%
11.8%
62.0%
73.6%
11.5%
68.0%
76.0%
8.0%
67.3%
76.9%
9.6%
Baseline Model
Finetuned Model
Figure 6: Scaling behavior of Qwen2.5 models
(0.5B-32B).
ground truth provided in the benchmark. The
detailed matching criteria are described in Ap-
pendix A.1.
Notably, since prompt design
can significantly impact model performance,
we maintain consistency by using the same
prompt template across all model evaluations
(see Appendix E.2 for details).Additionally,
comprehensive details about our fine-tuning
procedure are provided in Appendix D.3.
Main Results.
The primary results
are presented in Table 1.
First, our best-
performing model, OptMATH-Qwen2.5-32B,
achieves superior performance across all bench-
marks, surpassing proprietary large language
models such as GPT-3.5-Turbo[13], GPT4[59],
and Deepseek-V3[50], despite these models
having tens of times more parameters. Fur-
thermore, our OptMATH-Qwen2.5-7B outper-
forms ORLM-LLaMA-3-8B, a model of com-
parable size, on all benchmarks and demon-
strates performance only marginally inferior
to Deepseek-V3. Collectively, these results
demonstrate that training with OptMATH-
Train significantly enhances the model’s opti-
mization modeling capabilities.
Ablation Study on Model Size. To in-
vestigate the effectiveness of OptMATH train-
ing across different model scales, we conducted
experiments using Qwen2.5 models ranging
from 0.5B to 32B parameters. Due to com-
putational constraints, we used a randomly
sampled subset of 100,000 training examples.
As shown in Figure 6, all models exhibit
substantial performance improvements after
OptMATH-Train fine-tuning.
Notably, we
0.0
0.2
0.4
0.6
0.8
1.0
Proportion
0
10
20
30
40
50
60
70
80
Accuracy (%)
NL4OPT
MAMO EasyLP
MAMO ComplexLP
OptMATH-Bench
Micro Accuracy
Figure 7: Accuracy of Qwen2.5-1.5B within
one training epoch.
observe that while larger models generally
achieve better absolute performance, the rela-
tive performance gains from OptMATH-Train
training demonstrate diminishing returns as
model size increases.
Ablation Study on Data Size. Figure
7 presents our comprehensive analysis of how
varying amounts of training data influence
the performance of Qwen2.5-1.5B model on
OptMATH-Train. We observed significant im-
provements in the model’s optimization model-
ing capabilities even with only a small fraction
of the OptMATH-Train dataset. As we gradu-
ally increased the size of the training data, the
performance gains became less pronounced,
exhibiting a typical pattern of diminishing re-
turns. Larger models exhibit smoother learn-
ing curves, while smaller models demonstrate
greater sensitivity to additional training data,
indicating higher potential for improvement
through data scaling (detailed results across
model sizes can be found in the Appendix
D.4).
7
Conclusion
In this paper, we introduce a bidirectional data
synthesis framework for optimization model-
ing. It utilizes a two-step process: reverse data
generation, where LLMs refine themselves in
a loop to create diverse datasets, and autofor-
mulation, where a specialized model translates
natural language into mathematical represen-
tations. Our evaluation on NL4OPT, MAMO
10


--- Page 11 ---
and OptMATH-Benchmarks demonstrated
AutoFormulator’s superior performance in
generating accurate and well-formed optimiza-
tion models compared to baseline approaches.
Impact Statements
This study introduces OptMATH, a dataset
for optimization modeling, comprising a large-
scale training set (OptMATH-Train) and a
challenging benchmark (OptMATH-Bench).
OptMATH has the potential to democratize
optimization by enabling those without ex-
pertise to translate real-world problems into
mathematical formulations. The OptMATH-
Train dataset will significantly improve LLMs’
ability to understand and model optimiza-
tion problems.
Furthermore, OptMATH’s
structured data facilitates the integration of
optimization with advanced AI techniques
like reinforcement learning, Monte Carlo Tree
Search. Additionally, OptMATH-Bench pro-
vides a standardized benchmark for evaluat-
ing optimization modeling systems, pushing
the boundaries of LLM capabilities.
Ulti-
mately, OptMATH can improve efficiency and
decision-making across industries.
References
[1] Jeph Abara. Applying integer linear program-
ming to the fleet assignment problem. Inter-
faces, 19(4):20–28, 1989.
[2] Joseph Adams, Egon Balas, and Daniel
Zawack. The shifting bottleneck procedure
for job shop scheduling. Management Science,
34(3):391–401, 1988.
[3] Ali
AhmadiTeshnizi,
Wenzhi
Gao,
and
Madeleine Udell.
OptiMUS: Optimization
modeling using MIP solvers and large lan-
guage models, 2023.
[4] H. Alipour, M. A. Mu˜noz, and K. Smith-
Miles. Enhanced instance space analysis for
the maximum flow problem. European Jour-
nal of Operational Research, 2022.
[5] H. Alipour and K. Smith-Miles.
Instance
space analysis for 2d bin packing mathemati-
cal models. Discrete Optimization, 2023.
[6] Nicol´as Astorga, Tennison Liu, Yuanzhang
Xiao, and Mihaela van der Schaar. Autofor-
mulation of mathematical optimization mod-
els using LLMs, 2024.
[7] J. E. Beasley, M. Krishnamoorthy, Y. M.
Sharaiha, and D. Abramson. Scheduling air-
craft landings—the static case. Transporta-
tion Science, 34(2):180–197, 2000.
[8] Dimitri Bertsekas.
Network optimization:
continuous and discrete models, volume 8.
Athena Scientific, 1998.
[9] Simon Bowly. Stress testing mixed integer pro-
gramming solvers through new test instance
generation methods. PhD thesis, University
of Melbourne, Parkville, Victoria, Australia,
2019.
[10] Simon Bowly, Kate Smith-Miles, Davaatseren
Baatar, and Hans Mittelmann. Generation
techniques for linear programming instances
with controllable properties. Math. Program.
Comput., 12(3):389–415, 2020.
[11] Stephen Boyd. Convex optimization. Cam-
bridge UP, 2004.
[12] Gerald G. Brown, Robert F. Dell, and Alexan-
dra M. Newman. Optimizing military capital
planning. Interfaces, 34(6):415–425, 2004.
[13] Tom Brown, Benjamin Mann, Nick Ryder,
Melanie Subbiah, Jared D Kaplan, Pra-
fulla Dhariwal, Arvind Neelakantan, Pranav
Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. Ad-
vances in neural information processing sys-
tems, 33:1877–1901, 2020.
[14] Tom B. Brown, Benjamin Mann, Nick Ry-
der, Melanie Subbiah, Jared Kaplan, Pra-
fulla Dhariwal, Arvind Neelakantan, Pranav
Shyam, Girish Sastry, Amanda Askell, Sand-
hini Agarwal, Ariel Herbert-Voss, Gretchen
Krueger,
Tom Henighan,
Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey
Wu, Clemens Winter, Christopher Hesse,
Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei.
Language models are few-shot learners, 2020.
[15] Rainer Burkard, Mauro Dell’Amico, and Sil-
vano Martello. Assignment Problems. Soci-
ety for Industrial and Applied Mathematics,
2012.
11


--- Page 12 ---
[16] Alberto Caprara, Paolo Toth, and Matteo
Fischetti.
Algorithms for the set covering
problem.
Annals of Operations Research,
98(1):353–371, 2000. [Online; accessed 19-
January-2025].
[17] Guanglin Chen, Xin Li, and Yinyu Ye. An im-
proved analysis of lp-based control for revenue
management. arXiv preprint, 2022. [Online;
accessed 19-January-2025].
[18] Giorgio Consigli. Optimization methods in
finance. Quantitative Finance, 19:717 – 719,
2019.
[19] Lee G. Cooper.
Market-share models.
In
Handbooks in Operations Research and Man-
agement Science, volume 5, pages 259–314.
Elsevier Science Publishers, 1993. [Online;
accessed 19-January-2025].
[20] JF. Cordeau, F. Pasin, and M.M. Solomon.
An integrated model for logistics network de-
sign. Annals of Operations Research, 144:59–
82, 2006.
[21] G. Dantzig, R. Fulkerson, and S. Johnson.
Solution of a large-scale traveling-salesman
problem. Journal of the Operations Research
Society of America, 2(4):393–410, 1954.
[22] Mark S. Daskin. Network and Discrete Loca-
tion: Models, Algorithms, and Applications.
Wiley, 1995. [Online; accessed 19-January-
2025].
[23] A. Drexl and A. Kimms.
Lot sizing
and scheduling — survey and extensions.
European Journal of Operational Research,
99(2):221–235, 1997.
[24] Bernhard Fleischmann.
The discrete lot-
sizing and scheduling problem.
European
Journal of Operational Research, 44(3):337–
348, 1990.
[25] Michael Florian and Morton Klein. Deter-
ministic production planning with concave
costs and capacity constraints. Management
Science, 18(1):12–20, 1971.
[26] L. R. Ford and D. R. Fulkerson. Maximal
flow through a network. Canadian Journal
of Mathematics, 8:399–404, 1956.
[27] Michael R Garey and David S Johnson. Ap-
proximation algorithms for bin packing prob-
lems: A survey. In Analysis and design of al-
gorithms in combinatorial optimization, pages
147–172. Springer, 1981.
[28] Susan Garner Garille and Saul I. Gass.
Stigler’s diet problem revisited. Operations
Research, 49(1):1–13, 2001.
[29] David M Gay. Electronic mail distribution of
linear programming test problems. Mathemat-
ical Programming Society COAL Newsletter,
13:10–12, 1985.
[30] Bernard Gendron, Teodor Gabriel Crainic,
and Antonio Frangioni. Multicommodity ca-
pacitated network design. In Telecommunica-
tions network planning, pages 1–19. Springer,
1999.
[31] Zijie Geng, Xijun Li, Jie Wang, Xiao Li, Yong-
dong Zhang, and Feng Wu. A deep instance
generative framework for MILP solvers under
limited data availability. In Alice Oh, Tris-
tan Naumann, Amir Globerson, Kate Saenko,
Moritz Hardt, and Sergey Levine, editors,
Advances in Neural Information Processing
Systems 36: Annual Conference on Neural In-
formation Processing Systems 2023, NeurIPS
2023, New Orleans, LA, USA, December 10 -
16, 2023, 2023.
[32] Gianpaolo Ghiani, Gilbert Laporte, and
Roberto Musmanno. Introduction to Logistics
Systems Management: With Microsoft Excel
and Python Examples. John Wiley & Sons,
2022.
[33] P. C. Gilmore and R. E. Gomory. A linear
programming approach to the cutting-stock
problem. Operations Research, 9(6):849–859,
1961.
[34] Ambros
Gleixner,
Gregor
Hendel,
Ger-
ald Gamrath, Tobias Achterberg, Michael
Bastubbe,
Timo
Berthold,
Philipp
M.
Christophel, Kati Jarck, Thorsten Koch, Jeff
Linderoth, Marco L¨ubbecke, Hans D. Mit-
telmann, Derya Ozyurt, Ted K. Ralphs,
Domenico Salvagnin, and Yuji Shinano. MI-
PLIB 2017:
Data-Driven Compilation of
the 6th Mixed-Integer Programming Li-
brary. Mathematical Programming Computa-
tion, 2021.
[35] Bruce Golden, S. Raghavan, and Edward
Wasil, editors.
The Vehicle Routing Prob-
lem: Latest Advances and New Challenges.
Operations Research/Computer Science In-
terfaces Series. Springer New York, NY, 1
edition, 2008.
[36] Ziao Guo, Yang Li, Chang Liu, Wenli Ouyang,
and Junchi Yan. Acm-milp: Adaptive con-
straint modification via grouping and selec-
tion for hardness-preserving milp instance
12


--- Page 13 ---
generation. In International Conference on
Machine Learning (ICML), 2024.
[37] LLC Gurobi Optimization.
Optimization
modeling, 2025. [Online; accessed 19-January-
2025].
[38] Gurobi Optimization, LLC. Gurobi Optimizer
Reference Manual, 2024.
[39] Peter B. R. Hazell and Roger D. Norton.
Mathematical Programming for Economic
Analysis in Agriculture. Macmillan Publish-
ing Co., 1986. [Online; accessed 19-January-
2025].
[40] Jeffrey W. Herrmann, editor. Handbook of
Production Scheduling. International Series in
Operations Research & Management Science.
Springer New York, NY, 1 edition, 2006.
[41] Frederick S. Hillier and Gerald J. Lieber-
man. Introduction to Operations Research.
McGraw-Hill Education, 10th edition, 2014.
[Online; accessed 19-January-2025].
[42] Edward J Hu, Yelong Shen, Phillip Wallis,
Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. Lora: Low-rank
adaptation of large language models. arXiv
preprint arXiv:2106.09685, 2021.
[43] Xuhan Huang, Qingning Shen, Yan Hu, An-
ningzhe Gao, and Benyou Wang.
Mamo:
a mathematical modeling benchmark with
solvers. CoRR, abs/2405.13144, 2024.
[44] Caigao Jiang, Xiang Shu, Hong Qian, Xingyu
Lu, Jun Zhou, Aimin Zhou, and Yang Yu. LL-
MOPT: Learning to define and solve general
optimization problems from scratch, 2024.
[45] Narendra Karmarkar. A new polynomial-time
algorithm for linear programming. In Proceed-
ings of the sixteenth annual ACM symposium
on Theory of computing, pages 302–311, 1984.
[46] Morton Klein. A primal method for minimal
cost flows with applications to the assignment
and transportation problems. Management
Science, 14(3):205–220, 1967.
[47] Michael J. Kuby. Programming models for
facility dispersion: the p-dispersion and max-
isum dispersion problems. Mathematical and
Computer Modelling, 10(10):792, 1988.
[48] H. W. Kuhn. The hungarian method for the
assignment problem. Naval Research Logistics
Quarterly, 2(1-2):83–97, 1955.
[49] Sirui Li, Janardhan Kulkarni, Ishai Menache,
Cathy Wu, and Beibin Li.
Towards foun-
dation models for mixed integer linear pro-
gramming. arXiv preprint arXiv:2410.08288,
2024.
[50] Aixin Liu, Bei Feng, Bing Xue, Bingxuan
Wang, Bochao Wu, Chengda Lu, Chenggang
Zhao, Chengqi Deng, Chenyu Zhang, Chong
Ruan, et al. Deepseek-v3 technical report.
arXiv preprint arXiv:2412.19437, 2024.
[51] Haoxiong Liu, Yifan Zhang, Yifan Luo, and
Andrew Chi-Chih Yao. Augmenting math
word problems via iterative question compos-
ing. ArXiv, abs/2401.09003, 2024.
[52] Haoyang Liu, Jie Wang, Wanbo Zhang, Zi-
jie Geng, Yufei Kuang, Xijun Li, Bin Li,
Yongdong Zhang, and Feng Wu. Milp-studio:
MILP instance generation via block structure
decomposition. CoRR, abs/2410.22806, 2024.
[53] Aman Madaan,
Niket Tandon,
Prakhar
Gupta, Skyler Hallinan, Luyu Gao, Sarah
Wiegreffe, Uri Alon, Nouha Dziri, Shrimai
Prabhumoye, Yiming Yang, et al. Self-refine:
Iterative refinement with self-feedback. Ad-
vances in Neural Information Processing Sys-
tems, 36, 2024.
[54] Harry Markowitz. Portfolio selection. The
Journal of Finance, 7(1):77–91, 1952.
[55] Silvano Martello and Paolo Toth. Knapsack
problems: algorithms and computer imple-
mentations. John Wiley & Sons, Inc., USA,
1990.
[56] MOSEK ApS. MOSEK Optimization Soft-
ware, 2025. Version 11.0.3.
[57] Netlib. Netlib: A collection of mathemati-
cal software, papers, and databases. http:
//netlib.org, 1990.
Available at http:
//netlib.org.
[58] OpenAI. Learning to reason with llms: In-
troducing openai o1. https://openai.com/
index/learning-to-reason-with-llms/,
2024. Accessed: 2024-12-21.
[59] OpenAI, Josh Achiam, and Steven et al.
Adler. GPT-4 Technical Report, 2023.
[60] N.P. Padhy. Unit commitment-a bibliograph-
ical survey.
IEEE Transactions on Power
Systems, 19(2):1196–1205, 2004.
[61] Michael L. Pinedo. Scheduling: Theory, Al-
gorithms, and Systems. Springer Cham, 6
edition, 2022.
13


--- Page 14 ---
[62] Chandrasekharan Rajendran. Heuristics for
scheduling in flowshop with multiple objec-
tives. European Journal of Operational Re-
search, 82(3):540–555, 1995.
[63] Rindranirina Ramamonjison, Timothy T. L.
Yu,
Raymond
Li,
Haley
Li,
Giuseppe
Carenini, Bissan Ghaddar, Shiqi He, Mahdi
Mostajabdaveh, Amin Banitalebi-Dehkordi,
Zirui Zhou, and Yong Zhang. Nl4opt com-
petition: Formulating optimization problems
based on their natural language descriptions.
In Marco Ciccone, Gustavo Stolovitzky, and
Jacob Albrecht, editors, NeurIPS 2022 Com-
petition Track, November 28 - December 9,
2022, Online, volume 220 of Proceedings of
Machine Learning Research, pages 189–203.
PMLR, 2021.
[64] Singiresu S Rao. Engineering optimization:
theory and practice. John Wiley & Sons, 2019.
[65] A. Sch¨obel. Line planning in public trans-
portation: models and methods. OR Spec-
trum, 34:491–510, 2012.
[66] D. Smith. Network flows: Theory, algorithms,
and applications. J Oper Res Soc, 45:1340,
1994.
[67] Kate Smith-Miles and Simon Bowly. Generat-
ing new test instances by evolving in instance
space. Comput. Oper. Res., 63:102–113, 2015.
[68] Marius M. Solomon. Algorithms for the vehi-
cle routing and scheduling problems with time
window constraints. Oper. Res., 35:254–265,
1987.
[69] M. SteadieSeifi, N.P. Dellaert, W. Nuijten,
T. Van Woensel, and R. Raoufi. Multimodal
freight transportation planning: A literature
review. European Journal of Operational Re-
search, 233(1):1–15, 2014.
[70] S. Strassl and N. Musliu. Instance space anal-
ysis and algorithm selection for the job shop
scheduling problem. European Journal of Op-
erational Research, 2022.
[71] Zhengyang Tang, Chenyu Huang, Xin Zheng,
Shixi Hu, Zizhuo Wang, Dongdong Ge, and
Benyou Wang. ORLM: Training large lan-
guage models for optimization modeling,
2024.
[72] Constantine Toregas, Ralph Swain, Charles
ReVelle, and Lawrence Bergman. The loca-
tion of emergency service facilities. Operations
Research, 19(6):1363–1373, 1971.
[73] P Toth. The vehicle routing problem. SIAM
Monographs on Discrete Mathematics and Ap-
plications, 2002.
[74] Haoyu Wang, Jialin Liu, Xiaohan Chen,
Xinshang Wang, Pan Li, and Wotao Yin.
Dig-milp:
A deep instance generator for
mixed-integer linear programming with fea-
sibility guarantee.
arXiv preprint, 2023.
Code:
https://github.com/Graph-COM/
DIG_MILP.
[75] Ke Wang, Jiahui Zhu, Minjie Ren, Zeming
Liu, Shiwei Li, Zongye Zhang, Chenkai Zhang,
Xiaoyu Wu, Qiqi Zhan, Qingjie Liu, et al. A
survey on data synthesis and augmentation
for large language models.
arXiv preprint
arXiv:2410.12896, 2024.
[76] Yizhong Wang, Yeganeh Kordi, Swaroop
Mishra, Alisa Liu, Noah A Smith, Daniel
Khashabi, and Hannaneh Hajishirzi.
Self-
instruct:
Aligning language models with
self-generated instructions.
arXiv preprint
arXiv:2212.10560, 2022.
[77] Larry R. Weatherford and Samuel E. Bodily.
Forecasting and control of passenger book-
ings. Journal of Revenue and Pricing Man-
agement, 1(1):37–45, 1997. [Online; accessed
19-January-2025].
[78] Jason Wei, Xuezhi Wang, Dale Schuurmans,
Maarten Bosma, Ed H. Chi, F. Xia, Quoc Le,
and Denny Zhou. Chain of thought prompt-
ing elicits reasoning in large language models.
ArXiv, abs/2201.11903, 2022.
[79] Wikipedia.
Supply
chain
management
—
Wikipedia,
the
free
encyclopedia.
http://en.wikipedia.org/w/index.php?
title=Supply%20chain%20management&
oldid=1261250036, 2025. [Online; accessed
19-January-2025].
[80] Wayne L. Winston. Operations Research: Ap-
plications and Algorithms. Duxbury Press,
4th edition, 2004.
[Online; accessed 19-
January-2025].
[81] Ziyang Xiao, Dongxiang Zhang, Yangjun Wu,
Lilin Xu, Yuan Jessica Wang, Xiongwei Han,
Xiaojin Fu, Tao Zhong, Jia Zeng, Mingli Song,
and Gang Chen. Chain-of-experts: When llms
meet complex operations research problems.
In The Twelfth International Conference on
Learning Representations, ICLR 2024, Vi-
enna, Austria, May 7-11, 2024. OpenRe-
view.net, 2024.
14


--- Page 15 ---
[82] An Yang, Baosong Yang, Beichen Zhang,
Binyuan
Hui,
Bo
Zheng,
Bowen
Yu,
Chengyuan Li, Dayiheng Liu, Fei Huang, Hao-
ran Wei, et al. Qwen2. 5 technical report.
arXiv preprint arXiv:2412.15115, 2024.
[83] Zheng Yuan, Hongyi Yuan, Cheng Li, Guant-
ing Dong, Chuanqi Tan, and Chang Zhou.
Scaling relationship on learning mathemat-
ical reasoning with large language models.
ArXiv, abs/2308.01825, 2023.
[84] Yaowei Zheng, Richong Zhang, Junhao Zhang,
Yanhan Ye, Zheyan Luo, Zhangchi Feng, and
Yongqiang Ma. Llamafactory: Unified effi-
cient fine-tuning of 100+ language models.
In Proceedings of the 62nd Annual Meeting
of the Association for Computational Lin-
guistics (Volume 3: System Demonstrations),
Bangkok, Thailand, 2024. Association for
Computational Linguistics.
[85] Eric R. Zieyel. Operations research : applica-
tions and algorithms. Technometrics, 30:361–
362, 1988.
15


--- Page 16 ---
A
Dataset
A.1
An Introduction of Different Benchmarks
We evaluated the modeling capabilities of our trained model on NL4OPT, MAMO, and our
self-constructed dataset, OptMATH-Bench. Both MAMO and OptMATH-Bench have ground
truth annotations, while the original NL4OPT dataset lacks ground truth. To address this, we
utilized a LLM to generate initial ground truth for NL4OPT, followed by expert validation and
correction for each data. As a result, we obtained the ground truth for the NL4OPT dataset. In
addition, we have also analyzed these datasets in terms of problem scenarios and problem model
types, and the distribution of scenarios for each dataset is shown in Figure 8, the distribution of
problem types for each dataset is shown in Figure 4.
Figure 8: Scenarios distribution of the datasets.
NL4OPT[63] is a curated dataset derived from the NL4OPT Competition, where participants
were tasked with developing automated methods to convert natural language problem descriptions
into solver-ready code. This dataset primarily focuses on LP (Linear Programming) problems
across various contexts, though the underlying mathematical models are relatively uniform, with
more complex MIPS (Mixed Integer Programming and Scheduling) problems notably absent.
For our experiments, we selected the test set from this dataset, filtered out low-quality examples,
and retained a total of 245 high-quality instances.
MAMO[43] introduces a novel optimization dataset to assess the modeling capabilities of
LLMs. The dataset is divided into two main components, Easy LP and Complex LP, containing
652 and 211 instances, respectively. These components cover both LP and MILP problems,
capturing a wide range of real-life scenarios. However, the dataset does not include any nonlinear
programming (NLP) problems.
OptMATH-Bench. As shown in Table 1, while our fine-tuned model achieves remarkable
performance on NL4OPT and MAMO EasyLP, these datasets alone are insufficient to compre-
16


--- Page 17 ---
hensively evaluate the model’s optimization modeling capabilities. Moreover, both NL4OPT
and MAMO datasets are limited to linear programming problems, making them less repre-
sentative of the broader optimization landscape. To address this limitation, we constructed
a more challenging dataset for large models, while also expanding the diversity of problem
types—OptMATH-Bench. This dataset includes a carefully curated selection of representative
mathematical optimization problems that span a broad range of application scenarios, covering
LP, MILP, IP, NLP, SOCP, and other common optimization problems. Additionally, the prob-
lems in OptMATH-Bench are inherently challenging, making them effective in distinguishing the
modeling capabilities of the model.
During evaluation, we observed that certain ambiguities in problem statements could cause
the LLM to struggle in determining whether a variable is integer or continuous. To address this,
we applied a rule-based substitution approach: as long as the optimal solution derived under
either assumption (integer or continuous variable) matches the ground truth, we consider it a
pass. To determine whether the optimal values are equivalent, we use the following formula:
|ypred −ylabel|
|ylabel| + 1
< ϵ,
where ϵ is set to 1e-6.
A.2
Seed Classes
Our seed problem classes were curated by drawing from MIPLIB instances and integrating insights
from both Chain-of-Experts[81] and peer-reviewed literature. For each instance, we conducted
an in-depth analysis of its structure, starting from the problem description to identify its broader
optimization category and further refining it into specific subclasses. To ensure theoretical
accuracy, we consulted literature that provided detailed descriptions of these optimization
subclasses.
Based on these references, we formulated the mathematical representation of
each subclass, systematically outlining sets (where applicable), parameters, decision variables,
objective functions, and constraints. This step aimed to establish an abstract mathematical
framework rather than focusing on specific instances.
We organized comprehensive metadata for each problem class in a structured metadata.json
file, encompassing subclass names, references, reference links, and LaTeX-formulated mathe-
matical expressions. An example of this metadata structure is provided in Appendix E.5. This
systematic documentation not only ensures clarity but also facilitates dataset utilization and
future extensions.
Next, we focused on generating new problem instances. We implemented a custom Python class,
Generator(), in generator.py, which contained a step-by-step algorithm to create instances of
the identified subclasses (an example is provided in Appendix E.6). The input parameters and
outputs were explicitly defined, with detailed specifications for each parameter’s type and valid
range documented in README.txt. We validated the generator by running test generator()
with default parameters to ensure the produced instances were both mathematically valid and
practically meaningful.
Through this systematic and meticulous approach, we constructed a high-quality dataset
of Problem Description (PD) generators that lays a solid foundation for generating natural
language descriptions of optimization problems through Backtranslation Pipeline. This dataset
is designed to be versatile and scalable, making it suitable for a wide range of applications in
optimization research and practice.
17


--- Page 18 ---
Main Class
Problem Class
Num Reference
Assignment and Resource Alloca-
tion Optimization
Car Selection Problem
1
[15]
Contract Allocation Problem
1
[79]
Assignment Problem
2
[48]
Structure-Based Assignment
Problem
1
[15]
Team Formation Problem
1
[37]
Military
Personnel
Deploy-
ment Problem
1
[12]
Combinatorial Optimization
Knapsack Problem
1
[55]
Market Share Optimization
Problem
1
[19]
Set Multi-Cover Problem
1
[80]
Set Cover Problem
1
[16]
Cutting and Packing Optimiza-
tion
Bin Packing Problem
1
[27]
Blending Problem
1
[85]
Cutting Stock Problem
1
[33]
Domain-Specific Optimization
Diet Problem
3
[28]
Unit Commitment Problem
1
[60]
Farm Planning Problem
1
[39]
Facility Location Optimization
Facility Location Problem
2
[72]
Capacitated Facility Location
Problem
2
[22]
Transportation Problem, Air-
line Industry Resource Alloca-
tion
1
[73]
Facility Dispersion Problem
2
[47]
Financial and Revenue Optimiza-
tion
Portfolio Optimization Prob-
lem
1
[54]
Profit Maximization Problem
1
[41]
Revenue Management Prob-
lem
1
[17]
Revenue Maximization Prob-
lem
1
[77]
Network Flow Optimization
Multi-Commodity
Capac-
itated
Network
Design
Problem
1
[30]
Multi-Commodity Transporta-
tion Problem
1
[66]
Minimum Cost Flow Problem
1
[46]
Multi-Commodity
Network
Flow Problem
1
[66]
Network Flow Problem
1
[26]
Static Line Planning Problem
1
[65]
Supply Chain Optimization
1
[20]
Network Optimization
1
[8]
18


--- Page 19 ---
Main Class
Problem Class
Num Reference
Production
Planning
and
Scheduling Optimization
Capacitated Lot-Sizing Prob-
lem
1
[25]
Factory Planning Problem
1
[61]
Flow Shop Scheduling Prob-
lem
1
[62]
Job Shop Scheduling Problem
1
[2]
Discrete
Lot-Sizing
and
Scheduling Problem
1
[24]
Production Planning Problem
3
[40]
Lot-Sizing Problem
1
[23]
Transportation and Routing Op-
timization
Aircraft Assignment Problem
1
[1]
Aircraft Landing Problem
1
[7]
Transportation Problem
2
[69]
Traveling Salesman Problem
1
[21]
Operations Optimization
1
[35]
Capacitated Vehicle Routing
Problem with Time Windows
3
[68]
A.3
OptMATH-Train
The OptMATH-Train dataset consists of over 150k reverse-generated samples and 50k augmented
instances, forming a comprehensive collection of optimization problems. The dataset encompasses
a rich variety of real-world application scenarios. As illustrated in Figure 9, the dataset covers
over 10 major application domains spanning across both core business sectors and specialized
industries, demonstrating extensive coverage of real-world optimization scenarios. The substantial
proportions in logistics, supply chain, and manufacturing ensure robust representation of primary
industrial applications, while the balanced inclusion of sectors like transportation, energy, and
finance provides comprehensive coverage of specialized use cases. This thoughtful allocation
of problems across different domains not only prevents data concentration but also maintains
sufficient samples for each sector, enabling effective model training and evaluation.
The sequence length distribution of OptMATH-Train, as shown in Figure 10, exhibits a well-
balanced profile for both input and output sequences. The distribution approximates a normal
distribution with mild right-skewness, centered around 5,000 characters, demonstrating a natural
variation in problem complexity. This balanced distribution pattern is particularly advantageous
for model training, as it ensures sufficient context length for complex problem representation while
maintaining computational efficiency. Furthermore, the moderate right-skewness encompasses
challenging cases with extended sequences, which is essential for developing robust models
capable of handling sophisticated optimization problems that require comprehensive reasoning
and detailed solution steps.
B
Details of Instance Generation
B.1
An Example for Measuring the Complexity
Let binary variables y1, y2 ∈{0, 1} indicate whether products 1 and 2 are produced, integer
variables x1, x2 ∈Z+ represent production quantities, and a continuous variable z ≥0 denote
total cost.
The objective function minimizes total operational costs: min z + 10y1 + 8y2.
19


--- Page 20 ---
Figure 9: Distribution of Application Scenarios
across OptMATH-Train
0
5000
10000
Sequence Length
0
1000
2000
3000
4000
5000
Frequency
Input
Output
Figure 10: Sequence Length Distribution of
OptMATH-Train
The constraints span four categories: First, linear constraints include the resource limitation
2x1 + 3x2 ≤100 and market demand bounds x1 ≤50, x2 ≤30. Second, indicator constraints
using the Big-M method (with M = 100) enforce minimum production levels when activated:
y1 = 1 =⇒x1 ≥5 is reformulated as x1 ≥5−100(1−y1), and analogously for y2 = 1 =⇒x2 ≥
3. Third, a quadratic constraint z ≥0.5x2
1 + 0.3x2
2 integrates inventory costs into the objective.
Finally, a general nonlinear constraint x1ex2 ≤100 captures efficiency coupling between products.
To compute the complexity score S(PD), we identify: 2 binary variables, 2 integer variables,
and 1 continuous variable; 3 linear constraints, 2 indicator constraints, 1 quadratic constraint,
and 1 nonlinear constraint. The Big-M factor frequency is fBigM = 2, while the average number
of terms per expression Lexpr ≈2.71 is derived from structural analysis of constraints and the
objective. With all weights set to 1, the total complexity score becomes S = 16.71. This example
demonstrates how modeling choices (e.g., introducing nonlinear terms, Big-M parameterization)
directly influence the score, providing a quantitative framework for assessing model complexity.
B.2
An Overview of the Generated LP Files
The average lengths of LP files for different difficulty levels are illustrated in Figure 11. We
define the complexity thresholds for the five difficulty levels—easy, medium easy, medium,
medium hard, and hard—as [25, 75], [50, 100], [75, 125], [100, 150], and [125, 175], respectively.
The results demonstrate that our feedback-driven problem data generation approach is effective.
The average length of the generated LP files across the five difficulty levels is presented in Figure
12, categorized by seed data name. All generated LP files are feasible and possess optimal
solutions.
C
Details of Backtranslation
C.1
Backtranslation Pipeline
In our reverse generation pipeline, we employ Deepseek-V3[50] as our foundation model and
configure its temperature parameter to 0.8 to enhance the diversity of generated problems.
Furthermore, to achieve rich contextual diversity, we implement a random scenario assignment
mechanism during the Initial Generate phase. This mechanism directs the LLM to synthesize
problems that optimally integrate the mathematical characteristics with the designated scenario
20


--- Page 21 ---
Easy
Medium Easy
Medium
Medium Hard
Hard
Difficulty Level
0
5000
10000
15000
20000
25000
Average Number of Characters
4,092
8,430
11,919
16,098
24,329
Figure 11: Distribution of LP file lengths across generated instances by difficulty levels.
context. The detailed prompt is elaborated in Section E.1.
Through this backtranslation process, we initially generated approximately 120,000 easy
optimization problems. As shown in Figure 15, the length distribution of problem descriptions
exhibits a right-skewed pattern, with most problems containing 2,000 to 5,000 characters. After
applying rejection sampling, around 40% of the generated problems were filtered out while
maintaining a similar distribution pattern. This consistency in distribution before and after
filtering suggests that our quality control process effectively removes low-quality samples without
introducing length-related biases, ensuring the retained problems maintain natural and appropri-
ate descriptive lengths. After multi-stage refinement including semantic verification and difficulty
calibration, the pipeline ultimately produced 150,000 rigorously validated optimization problems.
Together with 50,000 augmented instances, this curated collection forms our OptMATH-Train
dataset, where each instance demonstrates: (1) Contextual alignment between mathematical
formulations and real-world scenarios, (2) Controllable complexity levels matching specified
difficulty tiers, and (3) Natural language expressions adhering to authentic problem-solving dis-
course patterns. The hierarchical quality assurance framework ensures the dataset’s applicability
for both educational interventions and benchmarking mathematical reasoning systems.
C.2
Ablation Study on the Impact of Self-Refine Iterations
To validate the effectiveness of each step in our backtranslation pipeline, we conducted com-
prehensive ablation studies. We first compared the accuracy between using only the Generate
step versus implementing the complete pipeline with Generate, Self-criticize, and Self-refine
steps. To investigate the impact of parameter T on the acceptance rate of rejection sampling,
we randomly selected 500 instances for evaluation, with results shown in Figure 14. The results
demonstrate that our Self-Refine loop (T ≥1) consistently outperforms direct generation (T = 0)
in terms of acceptance rate. While there are some fluctuations in performance across different
T values, possibly due to the inherent hallucination tendencies of large language models, we
observe that setting T = 1 achieves a satisfactory acceptance rate of 61.56%. Considering the
trade-off between performance and computational efficiency (token usage), we adopt T = 1 in
our final data synthesis process.
21


--- Page 22 ---
Figure 12: Distribution of LP file lengths across generated instances by problem types.
D
Details of AutoFormulation
D.1
CoT Instructions of AutoFormulation
To support comprehensive mathematical modeling capabilities, we developed a diverse set of CoT
instructions, which are detailed in Section E.3. These instructions vary in their decomposition
approaches, intermediate reasoning steps, and presentation formats, providing multiple pathways
for problem formulation.
In Figure 16, we present one representative formulation pattern
from our instruction set. This format includes three key components: a general mathematical
formulation with standard notation, a detailed instance-specific formulation with complete
parameter specifications, and the corresponding Python implementation using Gurobi. However,
this represents just one of many possible formulation styles. Other formats in our instruction set
may use different ordering of steps, alternative notation systems, or various levels of mathematical
abstraction. The diversity in formulation patterns ensures that our dataset captures a wide
range of valid mathematical modeling approaches while maintaining logical coherence and
mathematical correctness.
D.2
Ablation Study on Augmentation
As mentioned in the previous section, the purpose of data augmentation is to increase the
diversity of the dataset and generate more non-standard problems, which can help the fine-tuned
model to solve more difficult problems. We use 50k raw data, augmented data and mixed data
(50% raw data and 50% augmented data) for fine-tuning training on the Qwen2.5-7B model,
respectively, and the results show in Table 3 that the model fine-tuned with raw data performs
better in solving relatively simple problems, augmented data performs better in solving difficult
22


--- Page 23 ---
Output-Natural Language Description
A city is planning the layout of emergency medical stations. There are 6 candidate locations for building medical stations, each 
with different construction costs:
Location 1: Construction cost $80,000 ;    Location 2: Construction cost $40,000    ……
The city is divided into 10 districts, each requiring different numbers of medical stations for coverage due to population density and
emergency medical needs:
Districts 1 and 2: require coverage by at least 4 stations；    District 3: requires coverage by at least 2 stations      ……
Each candidate location can cover specific districts:
Location 1 covers districts: 1, 2, 6, 10；   Location 2 covers districts: 3, 5, 6, 9     ……
The objective is to decide which locations should be selected for building medical stations, minimizing the total construction cost
while meeting the coverage requirements for each district. Each location can only be selected or not selected (binary decision).
Input-LP File
Minimize
  80000 Selected[1] + 40000 Selected[2] + 20000
Selected[3] + 10000Selected[4]
   + 80000 Selected[5] + 90000 Selected[6]
Subject To
 MultiCover_e1: Selected[1] + Selected[3] + Selected[5] +
Selected[6] >= 4
…………
 MultiCover_e10: Selected[1] + Selected[4] + Selected[5] +
Selected[6]
   >= 4
Bounds
Binaries
 Selected[1] Selected[2] Selected[3] Selected[4] Selected[5]
Selected[6]
End
Backtranslation
An Example of Backtranslation
     Input-General Formulation
 represents the cost coefficient for each set
is a binary decision variable indicating whether set
i is selected
 represents the set of all sets containing element j
 represents the minimum number of times element j
needs to be covered
Generator
Figure 13: An example of backtranslation: transforming mathematical formulations and LP files
into natural language descriptions of optimization problems. The process transforms formal
mathematical notation and concrete data into human-readable problem descriptions.
problems, and mixed data combines the advantages of the above two very well, being the best in
terms of average accuracy across the four types of test sets.
D.3
SFT
We employ the LlamaFactory framework for fine-tuning [84]. We select the Qwen2.5 series
(0.5B∼32B) as our base models [82], and the hyperparameters are generally set as follows: initial
learning rate of 1e-4, 1∼3 epochs, LoRA rank of 32, LoRA alpha of 32, and LoRA dropout
of 0.1. While there are minor variations in hyperparameters across different experiments, the
overall settings remain similar and we omit these details for brevity. Notably, as illustrated in
our framework diagram 1, the entire AutoFormulator training process is an iterative cycle. The
Rejection Sampling in Step 2 relies on AutoFormulator’s modeling capabilities - stronger modeling
abilities lead to higher pass rates and better data quality. Similarly, the data augmentation
phase depends on AutoFormulator’s modeling competence. Higher quality data, in turn, results
in a more capable Formulator through training.Through this systematic model fine-tuning and
data augmentation approach, we have developed a dynamically evolving fine-tuning framework.
This framework not only accurately transforms natural language descriptions into mathematical
formulations and solver code but, more importantly, establishes a self-improving data flywheel
23


--- Page 24 ---
0
1
3
5
7
9
10
Max Iteration of the self-refine loop
60
61
62
63
64
65
66
Acceptance Rate (%)
60.86
61.56
64.37
61.67
63.06
64.26
65.77
Figure 14: Acceptance Rate vs. Maximum
Iteration of Self-Refine Loop. The bar chart
illustrates the acceptance rate achieved at dif-
ferent maximum iteration limits.
0
5000
10000
Problem Description Length
0
1000
2000
3000
4000
5000
Frequency
Original
After Rejection
Figure 15: Distribution of Natural Language
Description Lengths for Easy Problems in
OptMATH-Train Dataset. The histogram com-
pares the length distribution before and after
rejection sampling, showing the quality filter-
ing process.
Table 3: Comparison of Original Data and Augmentation Data in Training Models.
Types
NL4OPT
MAMO
EasyLP
MAMO
ComplexLP
OptMATH
Bench
Micro
Avg
Macro
Avg
Without Augmentation
86.9%
88.0%
44.5%
31.1%
72.1%
62.3%
Without Original
82.9%
85.5%
44.7%
23.6%
69.2%
59.2%
Mixture of Augmentation and Original
87.3%
87.7%
48.1%
33.3%
73.1%
64.1%
mechanism. This positive feedback loop enables the AutoFormulator system to continuously
enhance its capability in handling complex optimization problems through ongoing learning and
self-optimization, forming a virtuous growth cycle.
D.4
Detailed Ablation Studies on Model Size and Data Size
To investigate the impact of model capacity and training data volume on optimization modeling
performance, we conducted two sets of experiments using OptMATH-Train. For the model size
study in Figure 6 and Figure 17, we compare the performance of baseline and finetuned Qwen2.5
models ranging from 0.5B to 32B parameters. For the data scaling analysis in Figure 7 and
Figure 18, we track the accuracy progression within the first training epoch across different
model sizes, using varying proportions of the training data.
The model size experiments reveal distinct scaling patterns across benchmarks. On NL4OPT,
the performance improves from 12.7% (0.5B) to 96.7% (32B), showing particularly rapid gains in
the 0.5B-3B range. For MAMO EasyLP, we observe similar but more moderate improvements,
with accuracy increasing from 31.9% to 90.5%. However, on more challenging benchmarks like
MAMO ComplexLP and OptMATH-Bench, even the largest models achieve relatively modest
gains, reaching 52.6% and 30.6% respectively at 32B parameters.
The comparison between baseline and OptMATH-Train finetuned models reveals interesting
24


--- Page 25 ---
Input-Natural Language Description
A city is planning the layout of emergency medical stations. There are 6 candidate locations for building medical stations, each 
with different construction costs:
Location 1: Construction cost $80,000 ;    Location 2: Construction cost $40,000    ……
The city is divided into 10 districts, each requiring different numbers of medical stations for coverage due to population density and
emergency medical needs:
Districts 1 and 2: require coverage by at least 4 stations；    District 3: requires coverage by at least 2 stations      ……
Each candidate location can cover specific districts:
Location 1 covers districts: 1, 2, 6, 10；   Location 2 covers districts: 3, 5, 6, 9     ……
The objective is to decide which locations should be selected for building medical stations, minimizing the total construction cost
while meeting the coverage requirements for each district. Each location can only be selected or not selected (binary decision).
Output-Instance Formulation
1.Sets
L = {1,...,6}: set of potential locations
D = {1,...,10}: set of districts
2. Parameters
c[i]: construction cost for location i ∈ L
c[1] = 8, c[2] = 4, c[3] = 2, c[4] = 1, c[5] = 8, c[6] = 9
r[j]: coverage requirement for district j ∈ D
r[1] = 4, r[2] = 4, r[3] = 2, r[4] = 4, r[5] = 5
r[6] = 1, r[7] = 2, r[8] = 3, r[9] = 4, r[10] = 4
a[i,j]: coverage matrix, 1 if location i can cover district j, 0
otherwise.
3. Variables
x[i] ∈ {0,1}: binary decision variable
x[i] = 1 if location i is selected
x[i] = 0 otherwise
4. Objective Function
Minimize: ∑(i∈L) c[i]×x[i] (total construction cost)
5. Constraints
Coverage requirements: ∑(i∈L) a[i,j]×x[i] ≥ r[j], ∀j∈D
Binary constraints: x[i] ∈ {0,1}, ∀i∈L
 Output-Python Code with Gurobi
from gurobipy import *
# Create a new model
m = Model("Medical Station Location")
# Parameters
costs = {
1: 8, # $80,000
2: 4, 
…………
}
# Coverage requirements for each district
coverage_req = {
1: 4,
2: 4,
3: 2,
…………
}
# Coverage matrix
coverage = {
1: [1, 2, 6, 10],
2: [3, 5, 6, 9],
…………
}
# Decision Variables
selected = m.addVars(costs.keys(),
vtype=GRB.BINARY, name="Selected")
# Objective Function
m.setObjective(quicksum(costs[i] * selected[i] for
i in costs.keys()), GRB.MINIMIZE)
# Constraints
for district in coverage_req.keys():
# Sum up all locations
m.addConstr(quicksum(selected[loc] 
for loc in coverage.keys() if district in
coverage[loc])>=coverage_req[district],
f"MultiCover_e{district}")
# Optimize model
m.optimize()
# Print results
if m.status == GRB.OPTIMAL:
    print("\nOptimal solution found:")
else:
    print("No solution found")
AutoFormulation
An Example of AutoFormulation
     Output-General Formulation
 represents the cost coefficient for each set
is a binary decision variable indicating whether set i is
selected
 represents the set of all sets containing element j
 represents the minimum number of times element j
needs to be covered
Figure 16: An Example of AutoFormulation
25


--- Page 26 ---
Table 4: Performance comparison of Qwen2.5 models of varying sizes on mathematical optimiza-
tion tasks. The percentages in parentheses indicate improvements after fine-tuning.
Models
NL4OPT
MAMO EasyLP
MAMO ComplexLP
OptMATH-Bench
Micro AVG
Qwen2.5-0.5B
0.00%
0.15%
0.00%
0.00%
0.08%
Qwen2.5-0.5B(Finetuned)
12.65% (↑12.65%)
31.90% (↑31.75%)
16.59% (↑16.59%)
15.03% (↑15.03%)
23.29% (↑23.21%)
Qwen2.5-1.5B
0.00%
2.15%
0.95%
0.00%
1.23%
Qwen2.5-1.5B(Finetuned)
46.12% (↑46.12%)
68.10% (↑65.95%)
22.75% (↑21.80%)
18.65% (↑18.65%)
49.27% (↑48.04%)
Qwen2.5-3B
67.35%
65.18%
16.11%
0.52%
48.04%
Qwen2.5-3B(Finetuned)
68.57% (↑1.22%)
80.98% (↑15.80%)
25.59% (↑9.48%)
15.03% (↑14.51%)
59.88% (↑11.84%)
Qwen2.5-7B
86.94%
83.59%
21.80%
1.55%
62.03%
Qwen2.5-7B(Finetuned)
86.94%
89.42% (↑5.83%)
48.82% (↑27.02%)
30.05% (↑28.50%)
73.56% (↑11.53%)
Qwen2.5-14B
93.47%
82.52%
42.65%
14.51%
68.02%
Qwen2.5-14B(Finetuned)
95.51% (↑2.04%)
90.49% (↑7.97%)
51.18% (↑8.53%)
29.53% (↑15.02%)
76.02% (↑8.00%)
Qwen2.5-32B
92.65%
82.21%
44.55%
9.33%
67.26%
Qwen2.5-32B(Finetuned)
96.73% (↑4.08%)
88.04% (↑5.83%)
56.4% (↑11.85%)
36.27% (↑26.94%)
76.86% (↑9.60%)
patterns across different model scales. For simpler benchmarks like NL4OPT and MAMO
EasyLP, while the performance gap narrows with increased model size, OptMATH-Train fine-
tuning still provides consistent improvements even for the largest models. More notably, on
complex benchmarks such as MAMO ComplexLP and OptMATH-Bench, models finetuned on
OptMATH-Train demonstrate substantial performance gains across all model sizes, highlight-
ing the effectiveness of our training dataset in enhancing models’ capabilities for challenging
optimization problems.
The data scaling analysis reveals distinct learning dynamics across model sizes. Smaller models
(0.5B, 1.5B, 3B) exhibit higher initial performance variance during training, while larger models
(7B, 14B) demonstrate more stable learning curves from the outset. Notably, all model sizes
achieve relative performance stability after utilizing approximately 40% of the training data,
though the absolute performance levels differ significantly. The 3B model, for instance, maintains
consistently higher performance across all benchmarks while requiring a similar proportion of
training data to reach stability.
This efficient data utilization pattern holds true across all benchmarks, regardless of their
complexity levels. Whether for the relatively straightforward tasks in NL4OPT or the more chal-
lenging problems in OptMATH-Bench, models typically converge to their peak performance using
around 40% of the available training data. The remaining 60% of the data contributes primarily
to fine-tuning and minor performance adjustments rather than substantial improvements.
26


--- Page 27 ---
0.5B
1.5B
3B
7B
14B
32B
Model Size
0
20
40
60
80
100
Accuracy (%)
0.0%
12.7%
12.7%
0.0%
46.1%
46.1%
67.3%
68.6%
1.2%
86.9%
86.9%
93.5%
95.5%
2.0%
92.7%
96.7%
4.1%
Baseline Model
Finetuned Model
(a) NL4OPT
0.5B
1.5B
3B
7B
14B
32B
Model Size
0
20
40
60
80
100
Accuracy (%)
0.1%
31.9%
31.8%
2.1%
68.1%
65.9%
65.2%
81.0%
15.8%
83.6%
89.4%
5.8%
82.5%
90.5%
8.0%
82.2%
88.0%
5.8%
Baseline Model
Finetuned Model
(b) MAMO EasyLP
0.5B
1.5B
3B
7B
14B
32B
Model Size
0
20
40
60
80
100
Accuracy (%)
0.0%
16.6%
16.6%
0.9%
22.8%
21.8%
16.1%
25.6%
9.5%
21.8%
48.8%
27.0%
42.6%
51.2%
8.5%
44.5%
56.4%
11.9%
Baseline Model
Finetuned Model
(c) MAMO ComplexLP
0.5B
1.5B
3B
7B
14B
32B
Model Size
0
20
40
60
80
100
Accuracy (%)
0.0%
15.0%
15.0%
0.0%
18.6%
18.6%
0.5%
15.0%
14.5%
1.6%
30.1%
28.5%
14.5%
29.5%
15.0%
9.3%
36.3%
26.9%
Baseline Model
Finetuned Model
(d) OptMATH-Bench
Figure 17: Scaling behavior of Qwen2.5 models (0.5B-32B) on various benchmarks.
27


--- Page 28 ---
0.0
0.2
0.4
0.6
0.8
1.0
Proportion
0
5
10
15
20
25
30
35
40
Accuracy (%)
NL4OPT
MAMO EasyLP
MAMO ComplexLP
OptMATH-Bench
Micro Accuracy
(a) Qwen2.5-0.5B
0.0
0.2
0.4
0.6
0.8
1.0
Proportion
0
20
40
60
80
Accuracy (%)
NL4OPT
MAMO EasyLP
MAMO ComplexLP
OptMATH-Bench
Micro Accuracy
(b) Qwen2.5-3B
0.0
0.2
0.4
0.6
0.8
1.0
Proportion
0
20
40
60
80
Accuracy (%)
NL4OPT
MAMO EasyLP
MAMO ComplexLP
OptMATH-Bench
Micro Accuracy
(c) Qwen2.5-7B
0.0
0.2
0.4
0.6
0.8
1.0
Proportion
0
20
40
60
80
100
Accuracy (%)
NL4OPT
MAMO EasyLP
MAMO ComplexLP
OptMATH-Bench
Micro Accuracy
(d) Qwen2.5-14B
Figure 18: Scaling behavior of Qwen2.5-0.5B, Qwen2.5-3B, Qwen2.5-7B and Qwen2.5-14B
Accuracy Within One Training Epoch.
E
Prompt Templates
In this section, we present all the important prompt templates. Due to space constraints, certain
parts are omitted, and only the prompt frameworks are shown.
E.1
Reverse Data Generate Prompt
Generate Prompt
As an Operations Research Expert, analyze the given mathematical optimization expression
and LP data.
......
Input Mathematical Expression:
{{mathematical_expression}}
Input LP Data:
{{lp_data}}
Reference Examples:
{{examples}}
## Required Output:
Provide ONLY a clear, detailed natural language description of the optimization problem
that:
28


--- Page 29 ---
- Describes the complete scenario
- States all decisions to be made
- Specifies the objective clearly
- Incorporates all constraints and conditions naturally
- Includes all numerical parameters within narrative
- Uses appropriate domain terminology
- Maintains mathematical accuracy without showing formulation
Self-Criticism Prompt
As an Operations Research Expert, evaluate if the generated problem description matches
the mathematical optimization problem...
Input LP Data:
{{lp_data}}
Generated Problem Description:
{{problem_description}}
Analysis Steps...
## Required Output:
If perfect match:
"Complete Instance"
If inconsistencies exist:
"Incomplete Instance:
[List specific discrepancies...]"
29


--- Page 30 ---
Self-Refinement Prompt
As an Operations Research Expert, analyze the criticism and refine the problem
description if needed.
First, check the criticism result:
{{criticism}}
If the criticism shows "Complete Instance":
Output "Nothing need to refine"
Otherwise, follow these steps to generate an improved description:
1. Review Input Materials:
Mathematical Expression:
{{mathematical_expression}}
LP Data:
{{lp_data}}
Initial Description:
{{initial_description}}
2. Task:
Based on the criticism feedback, LP data information, and initial description, generate a
complete and accurate problem description.
Required Output:
If criticism is "Complete Instance":
Output "Nothing need to refine"
Otherwise:
[Direct natural language description of the optimization problem]
- No introductory phrases or meta-commentary
- No section headers or separators
- Just the complete problem description in clear natural language
- Ensure exact match with all LP data parameters
- Include all constraints and objectives naturally
- Avoid mathematical notation
Note: The output should be ONLY the complete natural language description itself, with no
additional text or formatting.
30


--- Page 31 ---
E.2
Baseline Prompt
Baseline Prompt Template for optimization modeling
Below is an operations research question. Build a mathematical model and corresponding
python code using ‘gurobipy‘ that appropriately addresses the question.
# Question:
{}
# Notes:
- Please output Python code starting with the following lines:‘‘‘python\n\nimport
gurobipy as gp\nfrom gurobipy import GRB\n‘‘‘
- Make sure the model variable is named ‘model‘.
- Avoid using "<" and ">" in Gurobi constraints; instead, use "<=" or ">=" as appropriate
.
- Carefully determine whether the variable is an integer or a continuous variable.
# Response:
(Provide your response here,keep the notes above in mind)
E.3
AutoFormulation Instructions
CoT Instructions
instructions = [
# Total instructions: 15 entries
# Showing 5 representative examples below...
"Below is an operations research question. Build a mathematical model and
corresponding Python code using ‘gurobipy‘ to solve it.",
"Create a complete solution that includes: 1) Mathematical formulation 2) Python code
using gurobipy 3) Results interpretation. Ensure all variables and constraints are
properly defined.",
"Transform this operations research problem into a mathematical model and implement
it in Python with gurobipy. Include clear variable definitions and constraint
explanations.",
"The following is an operations research problem. Let’s solve it step by step: 1)
Identify the decision variables, objective function, and constraints 2) Formulate the
mathematical model 3) Implement the solution using Gurobi in Python 4) Verify and
interpret the results.",
"This is an operations research problem. Follow this structured approach: Begin with
understanding the problem -> Identify the key variables -> Analyze the constraints ->
Develop the mathematical model -> Solve it programmatically using Gurobi in Python.
Provide clear reasoning and explanations at each stage."
]
31


--- Page 32 ---
E.4
Prompts for Configuration Selecting
Initializing the Parameters
You are an optimization expert helping to tune the parameters in:
\begin{verbatim}
{generator_code}
\end{verbatim}
CRITICAL REQUIREMENT:
Generated instances MUST be solvable to OPTIMALITY by Gurobi...
Model Complexity Score Calculation:
1. Variable Score:
- Binary variables: weight = {weights[’alpha\_bin’]}
- Integer variables: weight = {weights[’alpha\_int’]}
- Continuous variables: weight = {weights[’alpha\_cont’]}
2. Constraint Score...
3. Additional Complexity Factors...
Total Score = Variable Score + Constraint Score + Big-M Score + Expression Score
Secondary Requirements:
1. Model Complexity: {complexity\_score\_min} to {complexity\_score\_max}
2. Solve Time: {min\_solve\_time} to {max\_solve\_time} seconds
Return parameter values in JSON format matching ’default\_parameters’ structure:
- Keep exact same keys
- Preserve data types
- Use lists for tuples
- Format with proper indentation
Feedback Prompt
Based on testing {total_instances} instances with your suggested parameters:
{last_suggested_parameters}
Here are the detailed results:
1. Solution Status Analysis:
- Total Instances: {total_instances}
- Solvable Instances: {solvable_instances}
- OPTIMAL Solutions: {optimal_instances} ({optimal_rate:.1f}%)
- Solution Status Distribution:
...
- Status Percentages:
...
2. Overall Performance (Only OPTIMAL Solutions):
- Success Rate: {success_rate:.1f}% ({results["requirements_met"]["all_requirements"]}
out of {total_instances} instances met all requirements)
- Note: Only OPTIMAL solutions are considered successful
3. Requirements Satisfaction (Only OPTIMAL Solutions):
Complexity Score Distribution:{analyze_distribution("complexity")}
- Required range: {requirements}
- Success rate: {num_satisfying_requirements/total_instances}
32


--- Page 33 ---
Solve Time Distribution:{analyze_distribution("solve time")}
- Required range: {requirements}
- Success rate: {num_satisfying_requirements/total_instances}
4. Model Structure Analysis (Only OPTIMAL Solutions):
Variable Distributions:
Binary Variables: ...
Constraint Distributions:
Linear Constraints: ...
Indicator Constraints: ...
Quadratic Constraints: ...
General Constraints: ...
Complexity Score Components: ...
5. Distribution Analysis Insights:
{throughout analysis of the instances generated by the parameters by calling
statistics package in Python}
Based on these results and distribution analysis, please suggest parameter values that
would:
1. MAXIMIZE the proportion of instances that reach OPTIMAL status
2. Adjust the model complexity to meet the target score range (for OPTIMAL instances)
3. Maintain solve times within the required range (for OPTIMAL instances)
4. Reduce variability in key metrics where high variance was detected
5. Increase the overall success rate
Return your response in JSON format, strictly following the structure of the previous
suggestions dictionary. Ensure that:
1. All keys remain exactly the same as in the previous suggestions
2. The data types for each value are preserved
3. The JSON should be properly formatted with indentation for readability
4. Do not add any new keys or remove any existing keys
33


--- Page 34 ---
E.5
An Example of Metadata
Metadata
Subclass:
Bin Packing
Reference:
Garey, M. R. and Johnson, D. S. "Approximation Algorithms for
Bin Packing Problems:
A Survey." Analysis and Design of Algorithms in
Combinatorial Optimization (1981)
Reference URL: https://doi.org/10.1007/978-3-7091-2748-3 8
Mathematical Formula:
Consider n items, where each item i has:
• Weight si:
The weight of item i
The problem includes:
• Bin Capacity c:
The uniform capacity of each bin
• Bin Usage Variable yj:
A binary variable indicating whether bin j is
used
• Assignment Variable xi,j:
A binary variable indicating whether item i
is assigned to bin j
Minimize
n
X
j=1
yj
Subject to:
n
X
i=1
sixi,j ≤cyj
∀j = 1, . . . , n
n
X
j=1
xi,j = 1
∀i = 1, . . . , n
xi,j, yj ∈{0, 1}
∀i, j = 1, . . . , n
34


--- Page 35 ---
E.6
An Example of Generator
Python Code for Bin Packing Generator
1
import gurobipy as gp
2
from gurobipy import GRB
3
import random
4
5
class Generator:
6
def __init__(self, parameters=None, seed=None):
7
self.problem_type = "binpacking"
8
default_parameters = {
9
"n_items": (3, 10),
10
"weight_range": (1, 50),
11
"bin_capacity": 100
12
}
13
if parameters is None:
14
parameters = default_parameters
15
for key, value in parameters.items():
16
setattr(self, key, value)
17
self.seed = seed
18
if self.seed:
19
random.seed(seed)
20
21
def generate_instance(self):
22
self.n_items = random.randint(*self.n_items)
23
items = list(range(self.n_items))
24
item_weights = {i: random.randint(*self.weight_range) for i in items}
25
26
model = gp.Model("BinPacking")
27
model.Params.OutputFlag = 0
# Suppress Gurobi output
28
x = model.addVars(items, items, vtype=GRB.BINARY, name="x")
29
y = model.addVars(items, vtype=GRB.BINARY, name="y")
30
31
# Objective: Minimize the number of bins used
32
model.setObjective(gp.quicksum(y[j] for j in items), GRB.MINIMIZE)
33
for j in items:
34
model.addConstr(
35
gp.quicksum(item_weights[i] * x[i,j] for i in items) <= self.
bin_capacity * y[j],
36
name=f"Capacity_{j}"
37
)
38
for i in items:
39
model.addConstr(
40
gp.quicksum(x[i,j] for j in items) == 1,
41
name=f"Assignment_{i}"
42
)
43
return model
35


--- Page 36 ---
E.7
Augmentation Prompt
Generate Augmentation Problem Prompt
AUGMENTATION_RULES = [
# Semantic Enhancement
"Rephrase the problem description while maintaining the same mathematical structure
...",
"Rewrite the problem using different expressions and terminology...",
# Change the scenario
"Transform the problem into a different application scenario while preserving the
same structure...",
"Conceive a variant on another scenario for the mathematical model...",
# Numerical enhancement
"Change the numerical parameters while maintaining the same problem structure...",
"Scale up or down the problem size by adjusting parameters proportionally...",
# Problem variant generation
"Generate a variant by adding/removing/modifying constraints...",
"Create a variation by combining different types of constraints...",
# Complicating the problem
## Variable Expansion
"Increase the number of decision variables while maintaining similar structure...",
"Add bounds for adjustment variables...",
## Constraints Expansion
"Add realistic constraints like capacity limitations, budget restrictions...",
"Introduce cross-variable constraints between components...",
## Data Complexity
"Convert parameters into tabular form with more complex data structures...",
## Problem Types
"Generate non-linear problems by replacing linear relations...",
"Combine with other problem types to generate hybrid problems...",
# Multi-objective
"Add new objective functions to generate multi-objective problems...",
"Modify objective function to include additional terms...",
# Problem symmetry
"Generate variants by introducing symmetries...",
"Generate dual problems while modifying parameters and constraints..."
]
AUGMENTATION_TEMPLATE = """Below is an optimization problem, please generate a new
optimization problem by following the augmentation rule provided.
# Original Problem
The original optimization problem is as follows:
’’’
{original_problem}
’’’
# Augmentation Rule
{rule}
# Augmented Problem
Please construct a new optimization problem according to the above requirements and the
provided question in the following format:
[Write your new problem here]
Note: The generated problem should maintain mathematical validity and practical
feasibility.And just provide the problem description without any additional
information.
"""
36
