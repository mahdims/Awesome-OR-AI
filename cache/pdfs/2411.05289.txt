--- Page 1 ---
SpecHub: Provable Acceleration to Multi-Draft Speculative Decoding
Ryan Sun1,2, Tianyi Zhou3, Xun Chen2, Lichao Sun1,
1Lehigh University, 2Samsung Research America, 3University of Maryland, College park
Correspondence: lis221@lehigh.edu
Abstract
Large Language Models (LLMs) have become
essential in advancing natural language pro-
cessing (NLP) tasks, but their sequential token
generation limits inference speed. Multi-Draft
Speculative
Decoding
(MDSD)
offers
a
promising solution by using a smaller draft
model to generate multiple token sequences,
which the target LLM verifies in parallel.
However, current heuristic approaches, such
as Recursive Rejection Sampling (RRS), suffer
from low acceptance rates in subsequent drafts,
limiting the advantages of using multiple
drafts. Meanwhile, Optimal Transport with
Membership Cost (OTM) can theoretically
improve acceptance rates, but its computa-
tional cost is too high for real-time use. We
present SpecHub, a novel, efficient sampling-
verification method for MDSD that improves
acceptance rates with only linear computational
overhead. By simplifying the OTM problem
into a compact Linear Programming model,
SpecHub significantly reduces computational
complexity.
It further accelerates sampling
by leveraging a sparse joint distribution,
focusing computation on high-probability
token sequences.
In extensive experiments,
Spechub consistently generates 0.05-0.27 and
0.02-0.16 more tokens per step than RRS and
RRS without replacement. We attach our code
at
https://github.com/MasterGodzilla/
Speculative_decoding_OT.
1
Introduction
With the growing adoption of Large Language
Models (LLMs) in diverse applications, there is
a significant demand for faster inference and lower
latency in both local computing and online API ser-
vices. However, the sequential generation process
of autoregressive language models complicates par-
allel computation. This challenge is exacerbated
by the memory limitations of current hardware ar-
chitectures, where RAM and cache communication
latencies often constrain performance, resulting in
underutilized computing capacity.
Speculative decoding (Leviathan et al., 2023;
Chen et al., 2023a) accelerates LLM inference
while preserving the model‚Äôs output distribution.
By generating a sequence of draft tokens in ad-
vance using a smaller model, it leverages GPUs
to verify tokens simultaneously through rejection
sampling. Recent advancements (Chen et al., 2024;
Jeon et al., 2024; Sun et al., 2024; Miao et al., 2023)
have further enhanced this approach by introducing
tree-structured multi-drafts, where each path repre-
sents a draft. These tokens are verified in parallel
during a single forward pass of the LLM. Using a
token tree increases the number of accepted tokens
by providing multiple options for each token posi-
tion, thus increasing the overall acceptance rate of
the algorithm and generation efficiency.
Despite having various tree constructions, draft
model designs, and hardware optimizations, ex-
isting Multi-Draft Speculative Decoding (MDSD)
methods depend on recursive rejection sampling
(RRS) for acceptance, which is far from optimal.
While RRS greedily accepts the token from the first
draft, it does not consider the subsequent drafts and
misses the opportunity to dynamically adjust the
current token‚Äôs acceptance strategy to improve the
acceptance rates of the later drafts. RRS prioritizes
the first draft‚Äôs tokens but fails to dynamically ad-
just acceptance strategies for subsequent drafts. As
a result, later iterations use a residual distribution
modified by previous acceptances, leading to mis-
alignment with the original draft distribution and
lower acceptance rates (Chen et al., 2023b). Sun
et al. (2024) shows that the acceptance rule could
be optimized through an Optimal Transport prob-
lem with Membership Cost (OTM), which max-
imizes acceptance rates by better aligning draft
tokens with the accepted token. However, OTM
requires tremendous computation overhead and is
not practically feasible.
arXiv:2411.05289v1  [cs.CL]  8 Nov 2024


--- Page 2 ---
3
7
15
31
1.6
1.8
2
2.2
2.4
# nodes in a token tree
Batch Efficiency (Tokens/Step)
RRS
RRSw
SpecHub
(a) Llama2-7B with JF68m draft model on CNN dataset.
3
7
15
31
2
2.5
3
3.5
# nodes in a token tree
Batch Efficiency (Tokens/Step)
RRS
RRSw
SpecHub
(b) Vicuna-7B with EAGLE draft model on MT-Bench.
Figure 1: Batch efficiency of SpecHub, RRS, and RRSw with different numbers of nodes in a binary token tree with
temperature T = 1.0.
In this paper, we address the trade-off between
computational efficiency and sampling optimality
in Multi-Draft Speculative Decoding (MDSD). We
first reduce the OTM formulation to a much smaller
linear programming (LP) by focusing only on the
transport plan of scenarios where at least one draft
gets accepted. We then investigate the overlooked
design choice of draft sampling. While all previ-
ous methods used either sampling with or without
replacement, which makes finding the optimal so-
lution notoriously hard, we show that an optimal
acceptance rule can be trivially obtained if we in-
stead choose only certain drafts of tokens. As a
result, we can develop practical algorithms that bal-
ance acceptance rate with computation overhead.
Building on the new LP formulation and in-
sights, we introduce SpecHub, a faster sampling-
verification paradigm with only linear computa-
tional overhead. Instead of constructing a dense
distribution of k-draft and the accepted token,
SpecHub strategically selects drafts containing the
highest probability token sampled from the draft
model. The top draft token serves as a transport hub
for an oversampled token 1 to transfer its excessive
probability mass to an undersampled token. This
sparse structure simplifies and accelerates the un-
derlying linear programming. SpecHub performs
particularly well on LLMs whose output distribu-
tions are concentrated on the top token, resulting in
higher acceptance rates than RRS. It even provably
outperforms OTM under certain situations. The
algorithm is widely applicable and can seamlessly
integrate into various MDSD algorithms, enhanc-
ing their efficiency and overall decoding speed.
1Draft model probability exceeds that of the target model.
We empirically test SpecHub by implementing
it to various MDSD frameworks (Li et al., 2024;
Chen et al., 2024; Miao et al., 2023). We observe a
1‚àí5% increase in the second draft acceptance rate,
which yields a consistent 0.02‚àí0.16 improvement
in batch efficiency over current methods. More
impressively, SpecHub uses a tree with only half
the nodes of other methods to reach the same level
of batch efficiency. In our ablation study, SpecHub
brings consistent acceleration to LLM decoding
under different temperatures. Our toy experiments
further show that SpecHub sometimes outperforms
OTM in high-entropy regions.
2
Background and Related Work
Here, we review the sampling-verification schema
of speculative decoding. We discuss the theory
behind rejection sampling and explain why naively
extending it to Multi-Draft Speculative Decoding
(MDSD) becomes inefficient.
Speculative Sampling
Language model decod-
ing is intrinsically serial. Let V denote the vocab-
ulary, a discrete set of tokens that the language
model may generate. Let x1:t = (x1, . . . , xt) ‚àà
V‚äót denote a sequence of tokens. Then, the target
language model produces a conditional probabil-
ity p(¬∑|x1:t), from which we sample the next token
xt+1 ‚àºp(¬∑|x1:t). However, this process is slow for
its serial execution.
Speculative decoding (Chen et al., 2023a;
Leviathan et al., 2023) addresses the issue by par-
allelizing the decoding process with a draft and
verify phase. It first uses a smaller draft model
q(¬∑|x1:t) to generate a draft (xt+1, . . . , xt+d) se-
quentially. The depth of the draft, d, is usually


--- Page 3 ---
Figure 2: An example of a token tree of depth d = 4 for
MDSD. The tree is generated sequentially with the draft
model and evaluated concurrently with the target model.
Each path in the tree corresponds to a potential sequence
of tokens, with accepted tokens and rejected tokens high-
lighted. The black arrows indicate tokens that were not
visited. The dashed line represents a sample drawn from
the residual distribution after all drafts are rejected. Our
paper focuses on the evaluation of one step, how we
choose to sample the k = 2 tokens " dinner" and "
to" from the draft distribution q(¬∑|"I want") and decide
which of them to get accepted based on the target prob-
abilities p(" dinner"|"I want") and p(" to"|"I want").
around 5. This draft allows us to compute the tar-
get distributions p(xt+œÑ|x1:t+œÑ‚àí1) in parallel for
œÑ ‚â§d. Then, we iteratively accept each draft
token using rejection sampling with acceptance
probability min

1, p(xt+œÑ|x1:t+œÑ‚àí1)
q(xt+œÑ|x1:t+œÑ‚àí1)

. In this sin-
gle draft setting, speculative decoding equates to
sampling directly from the target distribution. After
rejection, we sample from the residual distribution
norm(max(0, p(¬∑|x1:t+œÑ‚àí1) ‚àíq(¬∑|x1:t+œÑ‚àí1))).
With only a single draft, the expected number of
tokens generated at each iteration is upper-bounded.
Assume the average acceptance rate for each to-
ken is Œ±, the maximum acceleration is 1/(1 ‚àíŒ±)
(Chen et al., 2024). Multi-Draft Speculative De-
coding solves this issue (Miao et al., 2023; Sun
et al., 2024). Instead of verifying one sequence per
time, MDSD generates a tree of tokens and calcu-
lates their target probability in parallel. Thus, when
the first draft gets rejected, the other drafts can be
picked up, and their offspring get verified in the
current step. By doing so, we trade more parallel
inference for more tokens generated in each step.
In the rest of the paper, we ignore any temporal
relationship and only focus on a single temporal
step in the decoding process.
In particular,
given q(¬∑|x1:t‚àí1) and p(¬∑|x1:t‚àí1), we discuss the
sampling and verification algorithm for generating
the offspring drafts and accepting one. We simplify
the notation and use p = p(¬∑|x1:t‚àí1) ‚àà‚àÜ|V|‚àí1
to denote the target model‚Äôs probability distri-
Figure 3: An illustration of rejection sampling. Sam-
pling from the draft distribution gives a point under the
blue distribution q. If the sample is also under the over-
lap with the target distributions p, we accept it. If not,
we reject the token and sample from the residual dis-
tribution, the remaining unexplored area max(p ‚àíq, 0)
normalized. The misalignment of the residual distribu-
tion and draft distribution makes Recursive Rejection
Sampling (RRS) inefficient in proceeding runs.
bution and q = q(¬∑|x1:t‚àí1) ‚àà‚àÜ|V|‚àí1 to denote
the draft model‚Äôs distribution. Here, ‚àÜ|V|‚àí1 =

p ‚ààR|V|

P
x‚ààV p(x) = 1, p(x) ‚â•0 ‚àÄx ‚ààV

is the probability simplex of dimension |V|. We
also notate the probability simplex of joint
distributions over a group of k drafts at a single
temporal step as x1:k = (x1, . . . , xk) as:
‚àÜ|V|k‚àí1 ={P ‚ààR|V|k 
X
X‚ààV‚äók
P(x1:k) = 1,
P(x1:k) ‚â•0 ‚àÄx1:k ‚ààV‚äók}
Rejection Sampling in Speculative Decoding
We here provide a geometric intuition behind rejec-
tion sampling. Given a target distribution p and a
sample token from the draft distribution x ‚àºq, we
seek to accept x as much as possible while ensur-
ing the outputted token from the process follows p.
We can visualize the process as sampling a point
under the probability mass function (PMF) of p.
The draft sample lies under the PMF of q. If the
token x is undersampled (q(x) < p(x)), we always
accept it. If it is oversampled (q(x) > p(x)), the
data point may or may not fall under p, in which
case we accept it with probability p(x)/q(x), the
height ratio between the two curves at this token.
Such methods fully utilize the overlap between the
two distributions and give the highest theoretical
acceptance rate. See Figure 3.


--- Page 4 ---
The residual distribution norm(max(0, p ‚àíq))
captures the remaining probability mass that was
not covered by q. Sampling from this residual dis-
tribution ensures that any rejections are accounted
for by exploring the regions where p exceeds q.
This approach aligns the accepted samples closely
with p, effectively achieving maximal coupling and
ensuring the samples represent the target distribu-
tion p.
Recursive Rejection Sampling
To facilitate
MDSD, previous methods use Recursive Rejection
Sampling, which naively applies rejection sampling
on the residual distributions. First, Recursive Re-
jection Sampling (RRS) samples k candidates inde-
pendently from the draft distribution. Then, it ac-
cepts each candidate with rejection sampling. If the
token is rejected, the target distribution is updated
to the residual distribution norm(max(p ‚àíq, 0)).
While the acceptance of the first candidate is high,
subsequent candidates suffer from the potential mis-
match between the residual distributions and draft
distribution q. Essentially, our residual distribution
deducts draft distribution, so we expect it to diverge
from the draft distribution q we used to generate
our samples, leading to small overlapping areas
and inefficiencies.
Algorithm 1 Token-level RRS
1: Input: Target model distribution p, draft model distribu-
tion q, number of candidates k
2: Output: A token x selected using RRS without replace-
ment.
3: Generate k samples x1, . . . , xk independently or without
replacement from q
4: for i = 1 ‚Üík do
5:
sample ri ‚àºUniform(0, 1)
6:
if ri < p(xi)
q(xi) then
7:
Return xi
8:
else
9:
p ‚Üênorm(max(p ‚àíq, 0))
10:
if without replacement then
11:
q(xi) ‚Üê0
12:
q ‚Üênorm(q)
13:
end if
14:
end if
15: end for
16: Return x ‚àºp
Recursive Rejection Sampling without Replace-
ment
In low-temperature settings, RRS may re-
peatedly sample the same token and fail to diver-
sify the tree. Furthermore, a rejected token will
continuously get rejected since the corresponding
entry of the residual probability is 0. Following
this intuition, several works(Chen et al., 2024; Jeon
et al., 2024; Li et al., 2024; Yang et al., 2024) pro-
posed Recursive Rejection Sampling without Re-
placement (RRSw). Instead of independently sam-
pling, it samples tokens without replacement. It
also modifies the draft distribution after each re-
jection to maintain a correct marginal distribution.
The differences are highlighted in Algorithm 1 in
red. While the method speeds up the decoding pro-
cess by avoiding repetition, it still falls short of
a theoretically optimal verification method as the
misalignment between residual distribution and the
draft distribution remains.
3
Mathematical Formulation of
Multi-Draft Speculative Decoding
In this section, we lay out the mathematical formu-
lation of the sampling and verification paradigm of
MDSD. We start by reviewing the Optimal Trans-
port with Membership Cost framework by Sun et al.
(2024) in Section 3.1. We show that it can simpli-
fied and propose an equivalent LP formulation that
greatly reduces computation complexity in Sec-
tion 3.2. Lastly, we point out that changing the
design of sampling can make the LP feasible for
real-world calculation in Section 3.3 while preserv-
ing the acceleration. We also discuss some consid-
erations for a real-world algorithm.
3.1
Optimal Transport with Membership Cost
We show how we can find the optimal sampling
and verification algorithm of MDSD that maxi-
mizes the acceptance rate as solving an Optimal
Transport problem with Membership Cost (Sun
et al., 2024). Let the target distribution be p and
the joint draft distribution Q = q‚äók ‚àà‚àÜ|V|k‚àí1
be the Cartesian product of the draft distributions
that gives the probability of sampling any par-
ticular series of draft tokens x1:k, so Q(x1:k) =
Qk
i=1 q(xi). Let y denote the accepted token. We
define the coupling between p and Q or equiva-
lently a transport plan from Q to p be a joint dis-
tribution œÄ(x1:k, y) ‚àà‚àÜ|V|k+1‚àí1 whose marginal
distributions satisfies P
y‚ààV œÄ(x1:k, y) = Q(x1:k)
and P
x1:k‚ààVk œÄ(x1:k, y) = p(y). We use the terms
coupling and transport plan interchangeably. The
Membership Cost is c(x1:k, y) = Qk
i=1 1yÃ∏=xi, an
indicator function of whether the accepted token
y equals any of the draft tokens xi. The transport
cost then calculates the expected rejection rate:
C(œÄ) = Ex1:k,y‚àºœÄ
" k
Y
i=1
1yÃ∏=xi
#
.


--- Page 5 ---
(a) OTM
(b) SpecHub
Figure 4: An illustration comparing the Optimal Trans-
port with Membership Cost (OTM) framework and
SpecHub. In both (a) and (b), the left side shows a two-
draft joint sampling distribution, while the right side
depicts the target distribution. The yellow bars highlight
the token of interest in the target. In (a), OTM requires
solving for the transport map œÄ of a dense sampling
distribution like Q = q‚äó2, which is computationally
expensive. In (b), SpecHub simplifies this process by
sparsifying the joint distribution, significantly reducing
the complexity of solving for œÄ.
It is well-known that Optimal Transport on discrete
space can be solved as a linear programming prob-
lem as
min
œÄ‚ààŒ†(p,q)
X
x1:k
X
y‚ààV
œÄ(x1:k, y)
k
Y
i=1
1yÃ∏=xi
(1)
where Œ†(p, q) is the set of all valid couplings be-
tween p and q‚äók. However, such a program con-
tains O(|V|k+1) variables, so even the fastest linear
programming algorithm struggles to calculate in
real-time.
3.2
A Simplified Linear Programming
Formulation
While the Optimal Transport formulation provides
a theoretical framework for understanding Multi-
Draft Speculative Decoding, its computational com-
plexity renders it impractical for real-time applica-
tions. To address this, we introduce a simplified
Linear Programming (LP) formulation that signifi-
cantly reduces the number of variables while pre-
serving the essence of the problem.
The key insight behind this simplification is that
the acceptance rate is primarily determined by how
the sampled draft tokens are handled. Once a token
is rejected, the subsequent actions, which involve
recalculating the residual distribution and resam-
pling, can be performed efficiently without explic-
itly considering the full coupling.
Instead of representing the entire coupling œÄ,
which has O(|V|k+1) variables, our simplified LP
formulation focuses on œÄ(x1:k, y = xi), i =
1, . . . , k, a smaller subset of transport plan which
denotes the probability of sampling the series of
drafts and accepting the i-th token xi. This effec-
tively reduces the number of variables to O(|V|k),
making the problem more tractable. The remaining
probabilities in the coupling, which correspond to
cases where the target token does not match any
of the draft tokens, are implicitly handled by the
residual distribution.
The simplified LP formulation is then:
minimizeœÄ 1 ‚àí
X
x1:k‚ààVk
k
X
i=1
œÄ(x1:k, xi)
subject to
œÄ(x1:k, xi) ‚â•0,
‚àÄx1:k ‚ààVk, i
k
X
i=1
œÄ(x1:k, xi) ‚â§Q(x1:k),
‚àÄx1:k ‚ààVk
k
X
i=1
X
x1:k‚ààVk,xi=y
œÄ(x1:k, y) ‚â§p(y),
‚àÄy ‚ààV
Given a solution to this simplified LP formula-
tion, we can reconstruct the complete transport plan
œÄ(x1:k, y). For any series of drafts x1:k and target
token y, if y does not equal one of the draft tokens
in x1:k, the entry is calculated as:
œÄ(x1:k, y)
# where y Ã∏= xi ‚àÄi = 1, . . . , k
=
p(y) ‚àíPk
i=1
P
x1:k‚ààVk,xi=y œÄ(x1:k, y)
P
y‚ààV p(y) ‚àíPk
i=1
P
x1:k‚ààVk,xi=y œÄ(x1:k, y)
¬∑ (Q(x1:k) ‚àí
k
X
i=1
œÄ(x1:k, xi))
The first term is the unallocated target probabil-
ity mass or the residual probability of y normalized .


--- Page 6 ---
The second term is the remaining probability mass
of the series of drafts x1:k after allocating probabil-
ities to cases where the target token matches a draft
token. This reconstruction process ensures that the
validity of the coupling. This simplified LP formu-
lation, while ignoring the explicit representation of
the full coupling, retains the essential information
needed to optimize the acceptance rate. It provides
a practical and computationally feasible approach
to solving the MDSD problem.
Theorem 1 (Equivalence of LP to OTM). For a
given joint draft distribution Q and target distri-
bution p, the optimal solution of the simplified
LP formulation achieves the same transport cost
as the maximal coupling in the Optimal Trans-
port with Membership Cost (OTM) problem, i.e.,
1 ‚àíP
x1:k‚ààVk
Pk
i=1 œÄ(x1:k, xi) = C(œÄ‚àó), where
œÄ‚àóis the optimal coupling for the OTM problem as
defined in Equation 1.
Proof. See Appendix A.
Examining Recursive Rejection Sampling (RRS)
How does an optimal solution to the Linear Pro-
gramming (LP) formulation differ from RRS? Con-
sider the simple case of k = 2. When a series of
drafts x1, x2 is sampled according to Q(x1:2), we
must decide whether to accept x1 or x2 based on
the target distribution p. If x1 is significantly over-
sampled, meaning p(x1) < q(x1). RRS makes
this decision independently for each draft token,
while the OTM solution considers the entire series.
Specifically, the OTM solution will tend to allo-
cate less probability mass to accepting x1 if x2 is
undersampled (p(x2) > q(x2)) and more probabil-
ity mass if x2 is also oversampled. This flexible
adaptation ensures a more targeted distribution in
subsequent drafts, leading to more efficient sam-
pling and verification.
Unbalanced Tree and Asymmetric Verification
When considering a single temporal step in the sam-
pling and verification process, the order in which
a pair of samples x1:k is selected appears inconse-
quential, as the branches are executed concurrently.
However, as suggested by Sequoia (Chen et al.,
2024), the most efficient tree structure is often un-
balanced. If the acceptance rate of the early draft is
higher than that of the second, designing a tree that
extends deeper along the first few branches while
keeping other branches shallower can enhance effi-
ciency. Optimal algorithms may decrease the first
few drafts‚Äô acceptance rate slightly to achieve a
(a) Optimal solution to LP
(b) RRSw solution to LP
Figure 5: A comparison of an optimal solution to an
RRSw solution under the LP formulation. Here, the
draft distribution q = [0.5, 0.3, 0.2] and the target dis-
tribution p = [0.1, 0.6, 0.3]. Each number on the top of
the cell is Q(x1, x2), and the numbers at the bottom of
the cell show œÄ(x1, x2, x1) and œÄ(x1, x2, x2), i.e. how
much of those draft probabilities are transferred to the
target probability. RRSw has a transport cost of 0.06
for not generating enough token ‚Äôb‚Äô.
higher overall acceptance rate, which we need to
carefully balance to leveraging the advantages of
unbalanced tree structures and significantly improv-
ing decoding speed and performance.
3.3
Design of Sampling
While the simplified LP formulation significantly
reduces the computational burden compared to the
OTM, it remains computationally expensive for
large vocabularies. Directly solving the LP prob-
lem is impractical, and previous research has pre-
dominantly focused on developing heuristics to
approximate the optimal solution. These heuristics,
such as Recursive Rejection Sampling (RRS) or
SpecTr(Sun et al., 2024), operate under a fixed joint
draft distribution, typically assuming independent
sampling with (Q = q‚äók) or without replacement
(Q(x1:k) =
Qk
i=1 q(xi)
Qk‚àí1
i=1 (1‚àíPi
j=1 q(xj))).
However, a crucial and often overlooked aspect


--- Page 7 ---
is the ability to modify the joint draft distribu-
tion Q, which unlocks a new dimension for op-
timization that has not been fully explored. The
key to designing a practical and efficient sampling
strategy is recognizing that Q does not need to be
a dense distribution over all possible drafts. In-
stead, we can strategically construct a sparse Q
that simplifies the LP formulation while capturing
the essential features of the target distribution. This
sparsity reduces the number of variables and con-
straints in the LP, making it significantly easier to
solve or approximate.
Ideally, the design of Q should satisfy two key
criteria: 1) Sparsity; Q should be sparse, concen-
trating on a small subset of highly probable draft
series to reduce computational complexity; and 2)
Efficiency; Q should effectively capture the es-
sential features of target distribution p, ensuring
that the sampled drafts are likely to contain the
target token. By carefully designing Q, we can bal-
ance computational efficiency and acceptance rate,
paving the way for practical and high-performance
MDSD algorithms.
4
SpecHub
Building on the aforementioned insights, we in-
troduce SpecHub, a faster sampling-and-verifying
paradigm with only linear computational overhead.
It effectively captures the transport features of
OTM solutions to enhance the acceptance rate and
can be applied to various multi-draft speculative
sampling algorithms. Since using more than two
drafts offers little gains in efficiency, SpecHub
uses two drafts (i.e., k = 2) to reduce complexity.
We thoroughly discuss expanding the algorithm to
more drafts in Appendix C.
First, we identify the token with the highest draft
probability, denoted as a, and sample it alongside
other tokens. We only populate the first column
and the first row in the joint draft distribution Q.
In particular, we define the joint draft distribution
Q(x1, x2) as follows:
Q(x1, x2) =
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
q(x1)
if x2 = a,
q(a)q(x2)
1‚àíq(a)
if x1 = a,
0
otherwise.
This specific design of Q makes the solution to the
simplified LP formulation straightforward. ‚àÄx ‚àà
Figure 6: SpecHub under the LP formulation. Here, the
draft distribution q = [0.5, 0.3, 0.2] and the target distri-
bution p = [0.1, 0.6, 0.3]. SpecHub focuses on the top
token "a", sampling pairs (x, a) and (a, x) with prob-
abilities q(x) and q(a)q(x)
1‚àíq(a) , respectively. This method
ensures efficient allocation of acceptance probabilities.
V, x Ã∏= a, we have
œÄ(x, a, x) = min(p(x), q(x))
œÄ(a, x, x) = min(p(x) ‚àíœÄ(x, a, x), Q(a, x))
After transporting draft probabilities to target prob-
abilities of non-top tokens, the remaining draft ac-
cepts the top token a evenly out of p(a) The remain-
ing entries in œÄ can be reconstructed as described
in the previous section. This solution effectively
allocates as much probability mass as possible to
the non-hub draft tokens while ensuring that the
hub token a is never undersampled. This strategy
maximizes the utilization of the draft distribution
and leads to a higher acceptance rate compared
to traditional methods like RRS. The pseudocode
implementation is in Appendix B.1.
Analysis
SpecHub offers several theoretical ad-
vantages.
First, since all drafts contain the
top token a, it is accepted with a probability
of p(a) and is never undersampled (see Corol-
lary 1). Additionally, let Œ± be the first draft ac-
ceptance rate of rejection sampling, defined as
Œ± = P
x max(p(x), q(x)). SpecHub achieves a
higher acceptance rate than Recursive Rejection
Sampling (RRS) if the top token a satisfies the
condition
q(a)
1‚àíq(a) > 1 ‚àíŒ±. We even guarantee
acceleration over OTM sampled with replacement
Q = q‚äó2 if
1
1‚àíq(a) > 2 or q(a) > 1
2. Detailed
proofs of these results are in Appendix B.3.
While SpecHub might theoretically decrease the
first draft acceptance rate for the top token a in
rare cases, our empirical results, detailed in Ap-
pendix B.4, show that this effect is negligible.


--- Page 8 ---
5
Experiments
In this section, we empirically show that SpecHub
improves batch efficiency in speculative multi-draft
decoding. We first show that SpecHub gives a sig-
nificantly higher acceptance rate for its better cou-
pling properties in the second draft acceptance rate.
We then illustrate how the improvement transfers
to higher batch efficiency.
5.1
Experiment Setup
Our experimental setup is based on the Llama and
Vicuna models. To mimic the setup of Chen et al.
(2024), we utilize the JackFram/Llama-68m and
JackFram/Llama-160m (JF68m, JF160m) (Miao
et al., 2023) models as our draft models and the
Llama2-7B (Touvron et al., 2023) models as our
target models. We evaluate our results on the Open-
WebText (Gokaslan and Cohen, 2019) and CNN
DailyMail (See et al., 2017) datasets. For each
run, we use 200 examples to measure the accep-
tance rate vector and sample another 200 examples
for evaluation. The prompt length and generation
length are both set to 128 tokens. We evaluate our
system on a single RTX A5000 GPU.
We also implement our algorithm on EAGLE (Li
et al., 2024). In short, EAGLE trains an autoregres-
sive decoding head that takes both the embedding
in the last layer of the target model and the draft
tokens to predict a draft. We test its performance
on Vicuna-7b (Zheng et al., 2024), a fine-tuned
LLaMA chatbot using ChatGPT (OpenAI et al.,
2024) to generate responses. We use the MT-Bench
dataset and temperatures T = 0.6, 1.0 with binary
trees and binary Sequoia trees.
5.2
Main Experiments
Second Draft Acceptance Rate
We evalu-
ate SpecHub at different temperatures T
=
0.3, 0.6, 1.0 using JF68m and JF160m as draft mod-
els. We observe that SpecHub consistently outper-
forms RRS and RRSw. In particular, at higher tem-
peratures, SpecHub achieves up to 5% improve-
ments in the second draft acceptance rate from
0.114‚àí0.118 to 0.166. At a lower temperature, the
improvement over RRSw becomes smaller since
the whole process assimilates greedy decoding. In
fact, SpecHub is equivalent to RRS without replace-
ment at zero temperature since both algorithms be-
come top-2 greedy decoding. Results are shown in
Table 1 and 2.
T
RRS
RRSw
SpecHub
0.3
0.0426
0.1114
0.1184
0.6
0.0740
0.1089
0.1379
1.0
0.1021
0.1140
0.1660
Table 1: Second Draft Acceptance Rate for JF68m
Model
T
RRS
RRSw
SpecHub
0.3
0.0399
0.1129
0.1221
0.6
0.0730
0.1212
0.1351
1.0
0.0910
0.1176
0.1660
Table 2: Second Draft Acceptance Rate for the JF160m
Model
Figure 7: Batch efficiency at different temperatures.
Batch Efficiency
We examine how the increased
second-draft acceptance rate translates to better
batch efficiency in different tree configurations. We
empirically test SpecHub and RRS without replace-
ment on binary trees of depth d with 2d ‚àí1 nodes
and report the batch efficiency in 1. We see that
with JF68M as the draft model, SpecHub consis-
tently outperforms RRS and RSSw by 0.02 ‚àí0.10
and 0.04 ‚àí0.20 in batch efficiency at temperatures
T = 0.6, 1.0. Meanwhile, using the EAGLE de-
coding head as the draft model, SpecHub generates
up to 3.53 and 3.33 tokens per iteration in the bi-
nary tree setting at T = 0.6, 1.0, an additional
0.08 tokens than RRS without replacement. We
also tested the batch efficiency on optimal binary
Sequoia trees(Chen et al., 2024). The full experi-
ment results are in Appendix F.
5.3
Ablations
Robustness to Different Temperatures
We an-
alyze the performance of SpecHub across differ-
ent temperatures (T) and compare it with Recur-


--- Page 9 ---
3
7
15
31
2
2.5
3
# nodes in a token tree
Batch Efficiency (Tokens/Step)
RRS
RRSw
SpecHub
Figure 8: Decoding efficiency of Vicuna-33B on the
MT-Bench dataset at temperature T = 1.0.
sive Rejection Sampling (RRS) and RRS with-
out replacement (RRSw).
We use a binary to-
ken tree of depth d = 5 with JF68m as the draft
model for Llama-2-7b.
As shown in Figure 7,
SpecHub consistently outperforms both RRS and
RRSw regarding batch efficiency across all temper-
ature settings. At lower temperatures (T < 0.4),
SpecHub assimilates RRSw in performance. At
medium (0.4 ‚â§T ‚â§0.6) and higher temperatures
(T > 0.6), SpecHub maintains superior perfor-
mance, demonstrating its robustness and adaptabil-
ity across varying entropy levels.
Scaling to Larger Models
We evaluate SpecHub
on larger models, specifically Llama 2-13B-Chat
and Vicuna-1.3-33B, to assess its scalability. For
Llama, we use JackFram/Llama-160m as the draft
model and test on the CNN DailyMail dataset. For
Vicuna, we evaluate using the MT-Bench dataset
with an EAGLE decoding head. Both experiments
are conducted at T = 1.0 with prompt and genera-
tion lengths set to 128 tokens. Results in Figure 8
show that SpecHub consistently outperforms RRS
and RRSw, generating up to 0.29 more tokens per
step on Llama and 0.19 tokens per step on Vicuna.
SpecHub extends to larger models well, maintain-
ing its efficiency gains as model size increases.
6
Related Work
Speculative Decoding
Speculative decoding
aims to execute multiple decoding steps in par-
allel. Early work (Stern et al., 2018) predicts fu-
ture tokens to accelerate greedy decoding. Spec-
ulative Sampling (Chen et al., 2023a; Leviathan
et al., 2023) extends to non-greedy decoding and
uses rejection sampling to recover target distribu-
tion optimally. Recent works focus on reducing
the running time of the draft model and increas-
ing the acceptance rate. OSD (Liu et al., 2023)
and DistillSpec (Zhou et al., 2023) train draft mod-
els on text generated by the target model. REST
(He et al., 2023) constructs drafts through retrieval.
Lookahead Decoding (Fu et al., 2024) breaks the
sequential dependency with Jacobi Iterations. Self-
Speculative Decoding (Zhang et al., 2023; Elhoushi
et al., 2024) avoids additional models and gener-
ates draft tokens by skipping intermediate layers.
Several works, such as MEDUSA (Cai et al., 2024)
and EAGLE (Li et al., 2024), reuse the feature em-
bedding of LLMs‚Äô last attention layer to predict
multiple future tokens in a non-causal or autore-
gressive manner.
Multi-Draft Speculative Decoding
Recent re-
search explores using tree attention to generate mul-
tiple drafts for speculative decoding (Miao et al.,
2023; Spector and Re, 2023; Li et al., 2024). Sun
et al. (2024) formulate the acceptance of multi-
ple drafts as a maximal coupling problem between
the drafts and the target distributions and propose
SpecTr with 1 ‚àí1
e optimality guarantee. CS Draft-
ing (Chen et al., 2023b) swaps in a lower-quality
model to generate drafts for less relevant branches.
MEDUSA (Cai et al., 2024) establishes candidates
according to the Cartesian product of the multi-
head predictions. Independently, Jeon et al. (2024)
and Yang et al. (2024) notice that a rejected to-
ken has zero probability in the residual distribu-
tion and use sampling-without-replacement in the
draft generation. Hu and Huang (2024) accelerates
MDSD Tree Monte Carlo methods, which treat lan-
guage model generation as a tree-space problem.
Sequoia (Chen et al., 2024) designed a dynamic
programming algorithm to search for the optimal
tree topology.
7
Conclusion
We presented SpecHub, a versatile and provably
faster sampling-verification paradigm for Multi-
Draft Speculative Decoding. Using an optimal
transport map between a sparse draft and target
distributions, SpecHub increases the acceptance
rate of the second draft by 1 ‚àí5%, which leads to
higher batch efficiency of LLM inference by up to
0.27 tokens per iteration. In addition to practical
speedups, SpecHub also provides insight into the
underlying mathematical structure in MDSD. We
hope to promotes future research in this area.


--- Page 10 ---
Acknowledgement
This work is done during an Internship at Samsung
Research America. We thank Zhengmian Hu, Yixin
Liu, Lichang Chen, and Qi Pang for their helpful
discussion.
Limitations
Our algorithm, SpecHub, is currently designed to
support only two drafts due to the computational
complexities associated with using more drafts.
This limitation may affect users who rely heavily
on large-scale parallel computations, particularly
when the number of nodes in the token tree exceeds
32. However, such extensive parallelism is rarely
utilized in practical applications, and most users
will not encounter this limitation.
Ethical Statement
This work focuses on accelerating LLM inferenc-
ing. There are no potential risks or negative effects
that the authors are aware of. Additionally, we
ensured that all datasets and benchmarks used in
the article comply with their intended purposes and
standards.
References
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng,
Jason D Lee, Deming Chen, and Tri Dao. 2024.
Medusa: Simple llm inference acceleration frame-
work with multiple decoding heads. arXiv preprint
arXiv:2401.10774.
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,
Jean-Baptiste Lespiau, Laurent Sifre, and John
Jumper. 2023a. Accelerating large language model
decoding with speculative sampling.
Preprint,
arXiv:2302.01318.
Zhuoming Chen, Avner May, Ruslan Svirschevski,
Yuhsun Huang, Max Ryabinin, Zhihao Jia, and
Beidi Chen. 2024. Sequoia: Scalable, robust, and
hardware-aware speculative decoding. arXiv preprint
arXiv:2402.12374.
Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun,
Jie Huang, and Kevin Chen-Chuan Chang. 2023b.
Cascade speculative drafting for even faster llm infer-
ence. arXiv preprint arXiv:2312.11462.
Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich,
Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas
Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed
Roman, et al. 2024.
Layer skip: Enabling early
exit inference and self-speculative decoding. arXiv
preprint arXiv:2404.16710.
Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang.
2024. Break the sequential dependency of llm in-
ference using lookahead decoding. arXiv preprint
arXiv:2402.02057.
Aaron Gokaslan and Vanya Cohen. 2019. Openwebtext
corpus.
Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee,
and Di He. 2023. Rest: Retrieval-based speculative
decoding. arXiv preprint arXiv:2311.08252.
Zhengmian Hu and Heng Huang. 2024. Accelerated
speculative sampling based on tree monte carlo.
In Forty-first International Conference on Machine
Learning.
Wonseok Jeon, Mukul Gagrani, Raghavv Goel, Juny-
oung Park, Mingu Lee, and Christopher Lott. 2024.
Recursive speculative decoding: Accelerating llm
inference via sampling without replacement. arXiv
preprint arXiv:2402.14160.
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
2023. Fast inference from transformers via spec-
ulative decoding. In International Conference on
Machine Learning, pages 19274‚Äì19286. PMLR.
Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang
Zhang. 2024.
Eagle:
Speculative sampling re-
quires rethinking feature uncertainty. arXiv preprint
arXiv:2401.15077.
Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Sto-
ica, Zhijie Deng, Alvin Cheung, and Hao Zhang.
2023. Online speculative decoding. arXiv preprint
arXiv:2310.07177.
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao
Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuom-
ing Chen, Daiyaan Arfeen, Reyna Abhyankar, and
Zhihao Jia. 2023. Specinfer: Accelerating generative
llm serving with speculative inference and token tree
verification. arXiv preprint arXiv:2305.09781.
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,
Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man, Diogo Almeida, Janko Altenschmidt, Sam Alt-
man, Shyamal Anadkat, Red Avila, Igor Babuschkin,
Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-
ing Bao, Mohammad Bavarian, Jeff Belgum, Ir-
wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,
Christopher Berner, Lenny Bogdonoff, Oleg Boiko,
Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-
man, Tim Brooks, Miles Brundage, Kevin Button,
Trevor Cai, Rosie Campbell, Andrew Cann, Brittany
Carey, Chelsea Carlson, Rory Carmichael, Brooke
Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully
Chen, Ruby Chen, Jason Chen, Mark Chen, Ben
Chess, Chester Cho, Casey Chu, Hyung Won Chung,
Dave Cummings, Jeremiah Currier, Yunxing Dai,
Cory Decareaux, Thomas Degry, Noah Deutsch,
Damien Deville, Arka Dhar, David Dohan, Steve
Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,
Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,


--- Page 11 ---
Sim√≥n Posada Fishman, Juston Forte, Isabella Ful-
ford, Leo Gao, Elie Georges, Christian Gibson, Vik
Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-
Lopes, Jonathan Gordon, Morgan Grafstein, Scott
Gray, Ryan Greene, Joshua Gross, Shixiang Shane
Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,
Yuchen He, Mike Heaton, Johannes Heidecke, Chris
Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,
Brandon Houghton, Kenny Hsu, Shengli Hu, Xin
Hu, Joost Huizinga, Shantanu Jain, Shawn Jain,
Joanne Jang, Angela Jiang, Roger Jiang, Haozhun
Jin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-
woo Jun, Tomer Kaftan, ≈Åukasz Kaiser, Ali Ka-
mali, Ingmar Kanitscheider, Nitish Shirish Keskar,
Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,
Christina Kim, Yongjik Kim, Jan Hendrik Kirch-
ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
≈Åukasz Kondraciuk, Andrew Kondrich, Aris Kon-
stantinidis, Kyle Kosic, Gretchen Krueger, Vishal
Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan
Leike, Jade Leung, Daniel Levy, Chak Ming Li,
Rachel Lim, Molly Lin, Stephanie Lin, Mateusz
Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,
Anna Makanju, Kim Malfacini, Sam Manning, Todor
Markov, Yaniv Markovski, Bianca Martin, Katie
Mayer, Andrew Mayne, Bob McGrew, Scott Mayer
McKinney, Christine McLeavey, Paul McMillan,
Jake McNeil, David Medina, Aalok Mehta, Jacob
Menick, Luke Metz, Andrey Mishchenko, Pamela
Mishkin, Vinnie Monaco, Evan Morikawa, Daniel
Mossing, Tong Mu, Mira Murati, Oleg Murk, David
M√©ly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,
Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,
Long Ouyang, Cullen O‚ÄôKeefe, Jakub Pachocki, Alex
Paino, Joe Palermo, Ashley Pantuliano, Giambat-
tista Parascandolo, Joel Parish, Emy Parparita, Alex
Passos, Mikhail Pavlov, Andrew Peng, Adam Perel-
man, Filipe de Avila Belbute Peres, Michael Petrov,
Henrique Ponde de Oliveira Pinto, Michael, Poko-
rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-
ell, Alethea Power, Boris Power, Elizabeth Proehl,
Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,
Cameron Raymond, Francis Real, Kendra Rimbach,
Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-
der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,
Girish Sastry, Heather Schmidt, David Schnurr, John
Schulman, Daniel Selsam, Kyla Sheppard, Toki
Sherbakov, Jessica Shieh, Sarah Shoker, Pranav
Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,
Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin
Sokolowsky, Yang Song, Natalie Staudacher, Fe-
lipe Petroski Such, Natalie Summers, Ilya Sutskever,
Jie Tang, Nikolas Tezak, Madeleine B. Thompson,
Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,
Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-
lipe Cer√≥n Uribe, Andrea Vallone, Arun Vijayvergiya,
Chelsea Voss, Carroll Wainwright, Justin Jay Wang,
Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,
CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-
ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,
Clemens Winter, Samuel Wolrich, Hannah Wong,
Lauren Workman, Sherwin Wu, Jeff Wu, Michael
Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-
ing Yuan, Wojciech Zaremba, Rowan Zellers, Chong
Zhang, Marvin Zhang, Shengjia Zhao, Tianhao
Zheng, Juntang Zhuang, William Zhuk, and Bar-
ret Zoph. 2024. Gpt-4 technical report. Preprint,
arXiv:2303.08774.
Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1073‚Äì
1083, Vancouver, Canada. Association for Computa-
tional Linguistics.
Benjamin Spector and Chris Re. 2023. Accelerating llm
inference with staged speculative decoding. arXiv
preprint arXiv:2308.04623.
Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit.
2018. Blockwise parallel decoding for deep autore-
gressive models. In Advances in Neural Information
Processing Systems, volume 31. Curran Associates,
Inc.
Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ah-
mad Beirami, Himanshu Jain, and Felix Yu. 2024.
Spectr: Fast speculative decoding via optimal trans-
port. Advances in Neural Information Processing
Systems, 36.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models. Preprint, arXiv:2307.09288.
Sen Yang, Shujian Huang, Xinyu Dai, and Jiajun Chen.
2024. Multi-candidate speculative decoding. arXiv
preprint arXiv:2401.06706.
Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen,
Gang Chen, and Sharad Mehrotra. 2023.
Draft
& verify:
Lossless large language model accel-
eration via self-speculative decoding.
Preprint,
arXiv:2309.08168.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,


--- Page 12 ---
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024.
Judging llm-as-a-judge with mt-bench and chatbot
arena. Advances in Neural Information Processing
Systems, 36.
Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat,
Aditya Krishna Menon, Afshin Rostamizadeh, San-
jiv Kumar, Jean-Fran√ßois Kagy, and Rishabh Agar-
wal. 2023. Distillspec: Improving speculative de-
coding via knowledge distillation. arXiv preprint
arXiv:2310.08461.
A
Correctness of the LP formulations
We prove Theorem 1 to show that the simplified LP
formulation is equivalent to the Optimal Transport
with Membership Cost (OTM) problem.
Proof. We first show that we can construct a valid
coupling from a valid solution to the simplified
LP formulation. Given a solution represented by
œÄ(x1:k, xi), we can derive a complete coupling
œÄ(x1:k, y), which represents the joint probability
distribution of the k draft tokens x1:k and the target
token y.
The construction process involves allocating
probabilities based on the LP solution. For each
possible combination of draft tokens and target to-
ken (x1:k, y), if y matches any of the draft tokens,
meaning y = xi for some i, then the corresponding
entry in the transport plan is given by the solution
to the LP:
œÄ(x1:k, y) = œÄ(x1:k, xi)
Let Œ±(y) = Pk
i=1
P
x1:k‚ààVk,xi=y œÄ(x1:k, y) be
the accepting probability of token y. If the target
token y is different from all draft tokens, the proba-
bility is calculated as the product of two terms:
œÄ(x1:k, y)
=
p(y) ‚àíŒ±(y)
P
y‚ààV(p(y) ‚àíŒ±(y))
¬∑ (Q(x1:k) ‚àí
k
X
i=1
œÄ(x1:k, xi))
The first term is the unallocated target probability
mass or the residual probability of y normalized.
The second term is the remaining probability mass
of the series of drafts x1:k after allocating probabil-
ities to cases where the target token matches a draft
token.
We now verify that the constructed œÄ is indeed
a valid coupling. First, we need to show that the
marginal distribution on the target token y is indeed
p(y):
X
x1:k
œÄ(x1:k, y)
=
k
X
i=1
X
x1:k,xi=y
œÄ(x1:k, y)
+ (p(y) ‚àí
k
X
i=1
X
x1:k,xi=y
œÄ(x1:k, y))
= p(y).
Then, we verify that the marginal distribution on
the series of drafts is the joint draft distribution:
X
y
œÄ(x1:k, y)
=
k
X
i=1
œÄ(x1:k, xi)
+
X
yÃ∏=xi‚àÄi
(
p(y) ‚àíŒ±(y)
P
y‚ààV(p(y) ‚àíŒ±(y))
¬∑ (Q(x1:k) ‚àí
k
X
i=1
œÄ(x1:k, xi)))
=Q(x1:k)
Now, we show that an optimal solution to the
simplified LP formulation is also optimal for the
OTM problem.
We prove this by contradiction. Assume there
exists a coupling œÄ‚Ä≤ that achieves a lower trans-
port cost than the optimal solution to the sim-
plified LP formulation. We can construct a so-
lution œÄ‚Ä≤‚Ä≤(x1:k, xi) to the LP from œÄ‚Ä≤ by setting
œÄ‚Ä≤‚Ä≤(x1:k, xi) = œÄ‚Ä≤(x1:k, xi). This œÄ‚Ä≤‚Ä≤ will have
the same objective value as the transport cost of
œÄ‚Ä≤, contradicting the optimality of the LP solution.
Therefore, an optimal solution to the simplified LP
formulation is also an optimal solution to the OTM
problem.
B
Properties of SpecHub
B.1
Pseudocode Implementation of SpecHub
The transport plan of top token a is:


--- Page 13 ---
œÄ(x, a, x) = min(p(x), q(x))
œÄ(a, x, x) = min(p(x) ‚àíœÄ(x, a, x), Q(a, x))
œÄ(a, x, a) = min(p(a),
X
x‚ààV
(Q(a, x) ‚àíœÄ(a, x, x))
¬∑
Q(a, x) ‚àíœÄ(a, x, x)
P
x‚ààV(Q(a, x) ‚àíœÄ(a, x, x))
œÄ(x, a, a) = min(p(a) ‚àí
X
x
œÄ(a, x, a),
X
x‚ààV
q(x) ‚àíœÄ(x, a, x))
¬∑
q(x) ‚àíœÄ(x, a, x)
P
x‚ààV q(x) ‚àíœÄ(x, a, x)
Here, we provide the pseudocode for using
SpecHub in real life. We follow a sequential proce-
dure and avoid explicitly writing out the underlying
coupling œÄ.
Algorithm 2 GetResidual
1: Inputs: target distribution p, draft distribution
q, highest probability token a
2: for all x in V, x Ã∏= a do
3:
p‚Ä≤(x) = max (p(x) ‚àíq(x), 0)
4:
q‚Ä≤(x) = max (q(x) ‚àíp(x), 0)
5: end for
6: p‚Ä≤(a) = p(a)
7: q‚Ä≤(a) = 0
8: return p‚Ä≤, q‚Ä≤
B.2
Correctness
Here, we proof that SpecHub does not sacrifice the
quality of generation.
Theorem 2. Given a target distribution p and a
draft distribution q, SpecHub generates tokens such
that for any token x ‚ààV, the probability of gener-
ating x under SpecHub, denoted as P(X = x), is
equal to p(x).
Proof. Given a target distribution p and a draft dis-
tribution q, we need to show that SpecHub gener-
ates tokens such that for any token x ‚ààV, the prob-
ability of generating x under SpecHub, denoted as
PSpecHub(x), is equal to p(x).
First, all draft pairs sampled by SpecHub involve
the top token a = arg maxx‚ààV q(x). For all x Ã∏= a,
pairs (x, a) and (a, x) are sampled with probabil-
ities Q(x, a) = q(x) and Q(a, x) = q(a)q(x)
1‚àíq(a) , re-
spectively.
Algorithm 3 Sampling and Verification with
SpecHub
Inputs: target distribution p, draft distribution q,
vocabulary V
Let a = arg maxx q(x) be the token with the
highest draft probability.
for all i ‚ààV, x Ã∏= a do
Q(x, a) = q(x), Q(a, x) = q(a)q(x)
1‚àíq(a)
end for
Sample draft tokens x(1), x(2) ‚àºQ
if x(2) = a then
Return
x(1)
with
probability
min

p(x(1))
Q(x(1),a), 1

end if
p‚Ä≤, Q‚Ä≤(‚àó, a) =GetResidual(p, Q(‚àó, a), a)
if x(1) = a then
Return
x(2)
with
probability
min

p‚Ä≤(x(2))
Q(a,x(2)), 1

end if
p‚Ä≤‚Ä≤, Q‚Ä≤(a, ‚àó) =GetResidual(p‚Ä≤, Q(a, ‚àó), a)
if x(1) = a then
Return
a
with
probability
min

p(a)
P
x Q‚Ä≤(a,x), 1

p‚Ä≤(a) = max(p(a) ‚àíP
x Q‚Ä≤(a, x), 0)
end if
if x(2) = a then
Return
a
with
probability
min

p‚Ä≤(a)
P
x Q‚Ä≤(x,a), 1

p‚Ä≤‚Ä≤(a) = max(p‚Ä≤(a) ‚àíP
x Q‚Ä≤(x, a), 0)
end if
Return a token sampled from the residual distri-
bution norm(p‚Ä≤‚Ä≤)
For a token x Ã∏= a, in the first draft, SpecHub
generates x with probability
P(x = x(1) and X = x)
= Q(x, a) min
 p(x)
Q(x, a), 1

= min (p(x), q(x)) .
In the second draft, given that x Ã∏= a, the residual
probability for token x after the first draft, denoted
as p‚Ä≤(x), is:
p‚Ä≤(x) = max(p(x) ‚àíq(x), 0)
= p(x) ‚àímin(p(x), q(x))
SpecHub generates x in the second draft with


--- Page 14 ---
probability
P(x = x(2) and X = x)
= Q(a, x) min
 p‚Ä≤(x)
Q(a, x), 1

= min (p(x) ‚àímin(p(x), q(x)), Q(a, x))
= min

p(x) ‚àímin(p(x), q(x)), q(a)q(x)
1 ‚àíq(a)

.
Now, let‚Äôs calculate the residual distribution after
both drafts for tokens x Ã∏= a. The residual proba-
bility p‚Ä≤‚Ä≤(x) for token x is calculated as follows:
p‚Ä≤‚Ä≤(x)
= max(p‚Ä≤(x) ‚àíQ(a, x), 0)
= max

p(x) ‚àíq(x) ‚àíq(a)q(x)
1 ‚àíq(a) , 0

Since p‚Ä≤‚Ä≤(x) represents the remaining probability
after both drafts, it ensures that:
P(X = x)
= P(x = x(1) and X = x)
+ P(x = x(2) and X = x)
+ p‚Ä≤‚Ä≤(x)
= min(p(x), q(x))
+ min

p(x) ‚àímin(p(x), q(x)), q(a)q(x)
1 ‚àíq(a)

+ max

p(x) ‚àíq(x) ‚àíq(a)q(x)
1 ‚àíq(a) , 0

= p(x)
Now for x = a:
In the first draft, SpecHub generates a with prob-
ability
P(a = x(1) and X = a)
=
X
x
Q‚Ä≤(a, x) min

p(a)
P
x Q‚Ä≤(a, x), 1

= min
 
p(a),
X
x
Q‚Ä≤(a, x)
!
.
In the second draft, given that a = x, the residual
probability for token a after the first draft, denoted
as p‚Ä≤(a), is:
p‚Ä≤(a) = max(p(a) ‚àí
X
x
Q‚Ä≤(a, x), 0).
SpecHub generates a with probability
P(a = x(2) and X = a)
=
X
x
Q‚Ä≤(x, a) min

p‚Ä≤(a)
P
x Q‚Ä≤(x, a), 1

= min
 
max(p(a) ‚àí
X
x
Q‚Ä≤(a, x), 0),
X
x
Q‚Ä≤(x, a)
!
The total probability for generating a is:
P(X = a)
= P(a = x(1) and X = a)
+ P(a = x(2) and X = a)
= min

p(a),
p(a)
P
x Q‚Ä≤(a, x)

+ min
 
max(p(a) ‚àí
X
x
Q‚Ä≤(a, x), 0),
p(a)
P
x Q‚Ä≤(x, a)

= min
 
p(a),
X
x
Q‚Ä≤(a, x) + Q‚Ä≤(x, a)
!
It can be shown that p(a) < P
x Q‚Ä≤(a, x) +
Q‚Ä≤(x, a). First, since Q(a, a) = 0), we have
X
x
Q(a, x) + Q(x, a)
=
X
x‚ààV\{a}
q(x) + q(a)q(x)
1 ‚àíq(a)
= 1
Also, we have p(a) = 1 ‚àíP
x‚ààV\{a} p(x). Thus,
X
x‚ààV\{a}
Q‚Ä≤(a, x) + Q‚Ä≤(x, a)
=
X
x‚ààV\{a}
(max(Q(a, x) ‚àíp(x), 0)
+ max(Q(x, a) ‚àíp‚Ä≤(x), 0))
=
X
x‚ààV\{a}
max(Q(a, x) + Q(x, a) ‚àíp(x), 0)
‚â•
X
x‚ààV\{a}
Q(a, x) + Q(x, a) ‚àíp(x)
=
X
x‚ààV\{a}
Q(a, x) + Q(x, a) ‚àí
X
x‚ààV\{a}
p(x)
= 1 ‚àí(1 ‚àíp(a)) = p(a)


--- Page 15 ---
Thus, for any token x ‚ààV, the probability of
generating x under SpecHub is equal to p(x), ensur-
ing that the output distribution matches the target
distribution p.
As a corrolary of the last part of the proof,
SpecHub accepts as much top token a as p(a).
Corollary 1 (Top Token Acceptance). Given a
draft distribution q and a target distribution p, let
a = arg maxx‚ààV q(x) denote the token with the
highest draft probability. Then, SpecHub generates
token a with probability p(a).
B.3
Acceptance Rate
We here prove a sufficient condition for SpecHub
to run faster than RRS.
Theorem 3 (Superiority over RRS). Let Œ± =
P
x‚ààV min(q(x), p(x)) be the acceptance rate of
the first draft. SpecHub has a higher acceptance
rate in the second draft if
q(a)
1‚àíq(a) > 1 ‚àíŒ±.
Proof. First, by Lemma 1, SpecHub generates the
top token a with probability p(a). This maximizes
the acceptance rate for a. Next, we calculate the
second draft acceptance rate for every other token
x ‚ààV \ {a}.
For RRS, the acceptance rate for token x in the
first draft is min(p(x), q(x)). The residual proba-
bility for token x after the first draft, denoted as
r(x), is:
p‚Ä≤(x) = p(x) ‚àímin(p(x), q(x))
1 ‚àíŒ±
where Œ± = P
x‚ààV min(p(x), q(x)) is the overall
acceptance rate in the first draft. The second draft
acceptance rate for token x under RRS is then:
(1 ‚àíŒ±) min
p(x) ‚àímin(p(x), q(x))
1 ‚àíŒ±
, q(x)

which simplifies to:
min (p(x) ‚àímin(p(x), q(x)), (1 ‚àíŒ±)q(x))
For SpecHub, the second draft acceptance rate
for token x is:
min

p(x) ‚àímin(p(x), q(x)),
q(a)
1 ‚àíq(a)q(x)

Comparing these rates shows that SpecHub has
a higher acceptance rate if
q(a)
1‚àíq(a) > 1 ‚àíŒ±.
In practice, this condition is usually satisfied. For
example, if Œ± = 0.5, then as long as the top token
has probability q(a) > 1
3 = 0.333, we guarantee
acceleration. Meanwhile, since SpecHub accepts
top tokens up to p(a), the above sufficient condi-
tions become necessary only in unusual cases when
p(a) = 0.
Using a similar proof strategy, we can show it
guarantees to outperform OTM with independent
sampling in rare cases.
Theorem 4 (Superiority over OTM). SpecHub
guarantees a higher total acceptance rate com-
pared to OTM with independent sampling if q(a) >
1/2.
Proof. Let Q = q‚äó2. Then, for a token x, the high-
est rate acceptance is upper bounded by the proba-
bility that it is contained in any draft pair with prob-
ability 1‚àí(1‚àíq(x))2 < 2q(x). Meanwhile, for the
first and second drafts, the acceptance rate when us-
ing SpecHub is
q(a)
1‚àíq(a)q(x)+q(x) =
q(x)
1‚àíq(a). Thus,
we can accept more tokens x if
q(x)
1‚àíq(a) > 2q(x), or
q(a) > 1
2.
Compared to the previous theorem, this bound
is nowhere near as tight since we are using a loose
upper bound on OTM‚Äôs performance. In reality we
expect OTM to perform worse.
B.4
First Draft Acceptance Rate
SpecHub is designed to optimize the acceptance
rate across multiple drafts, but in rare cases, it
might slightly decrease the acceptance rate of the
top token in the first draft. This situation occurs
when the probability of the top token in the target
distribution satisfies p(a) > q(a) and another token
x takes some of the probability mass Q(a, x). How-
ever, our empirical evaluations in Table 3 demon-
strate that this effect is not noticeable in practice, as
the acceptance rates of the first draft remain high.
Table 3: First Draft Acceptance Rates for SpecHub and
RRSw across different models and temperatures.
T
Draft
SpecHub
RRSw
0.3
JF68m
0.4921
0.4498
JF160m
0.5578
0.5465
0.6
JF68m
0.4842
0.4821
JF160m
0.5632
0.5587
1
JF68m
0.4248
0.4418
JF160m
0.5130
0.5257


--- Page 16 ---
C
A discussion on more drafts
C.1
Diminishing Returns of Increasing Drafts
While theoretically appealing, using more drafts
in practice offers diminishing returns. As we in-
crease the number of drafts, the probability mass
of the residual distribution decreases, leading to
lower acceptance rates for subsequent drafts. This
phenomenon is illustrated in Figure 9, where we
present the acceptance rates for up to 10 drafts us-
ing both RRSw and RRS with temperature T = 1.0.
As evident from the plots, the acceptance rate dras-
tically decreases after the first few drafts, suggest-
ing that the benefit of using more than 5 drafts is
negligible.
1
2
3
4
5
6
7
8
9
10
0
0.2
0.4
Draft Number
Acceptance Rate
RRSw
RRS
SpecHub
Figure 9: Acceptance rate decay for different drafts with
temperature T = 1.0.
C.2
Curse of Dimensionality
The computational complexity of finding the op-
timal coupling in Multi-Draft Speculative Decod-
ing grows exponentially with the number of drafts.
This is often referred to as the curse of dimensional-
ity. Specifically, the number of variables in the LP
formulation is on the order of O(|V|k+1), where
|V| is the vocabulary size and k is the number of
drafts. As k increases, solving the LP becomes
computationally intractable for even moderately
sized vocabularies.
C.3
Potential for Sparse Algorithms on more
drafts
The diminishing returns of additional drafts and
the curse of dimensionality suggest that a practi-
cal approach should focus on a small number of
drafts while ensuring an efficient probability of
mass transport. One promising direction is to ex-
plore sparse algorithms that leverage the structure
of the draft and target distributions. For instance,
instead of considering all possible combinations of
drafts, we can prioritize those with higher sampling
probabilities or those that exhibit significant over-
lap between the draft and target distributions. One
potential approach is to extend the "hub" concept
of SpecHub to multiple drafts. Instead of desig-
nating a single token as the hub, we can identify
a small set of high-probability tokens and create
a sparse flow network where probability mass is
primarily transported through these hubs. This
approach could potentially maintain high accep-
tance rates while significantly reducing the com-
putational complexity compared to solving the full
LP. Further research in this direction could lead to
more efficient and scalable algorithms for MDSD.
D
Comparing SpecHub to OTM in Toy
Settings
Here, we seek to compare OTM, RRS, and
SpecHub‚Äôs performance by measuring the accep-
tance rate of the three algorithms using a few toy
example drafts and target distributions with a small
vocab size |V| = 50 in Table 4. Given tempera-
ture T and a hyperparameter Œª that controls the
similarity between the two distributions, we gener-
ate two logits using uniform distributions such that
up ‚àºUnif(0, 1)‚äó|V| and uq ‚àºUnif(0, 1)‚äó|V|. The
corresponding target and draft distributions are p =
softmax(up
T ) and q = softmax(Œª up
T + (1 ‚àíŒª)uq
T ).
We calculate the acceptance rate for all methods
theoretically except for RRS without replacement,
where we perform a Monte-Carlo Simulation with
a thousand repetitions. We conduct the experi-
ment on a hundred pairs of toy distributions and
report the average. The results in Table 4 quantita-
tively illustrate the performance differences among
SpecHub, Recursive Rejection Sampling (RRS),
RRS without replacement, and Optimal Transport
(OTM) methodologies under varying conditions of
temperature T and similarity parameter Œª. In high
similarity scenarios (Œª = 0.7), SpecHub outper-
forms other methods significantly at lower temper-
atures (T = 0.1), achieving the best acceptance
rate of 0.7402, closely followed by OTM with-
out replacement at 0.7345. At higher temperatures
(T = 0.5), OTM methods, particularly OTM with-
out replacement, dominate, marking the best perfor-
mance with 0.9150 at T = 0.5 and Œª = 0.7. This
suggests that SpecHub is particularly effective in
tightly controlled environments with high similar-


--- Page 17 ---
Table 4: Acceptance Rates for Toy Experiments The acceptance rates for SpecHub, Recursive Rejection Sampling
(RRS), and Optimal Transport (OTM) algorithms using toy example drafts and target distributions. T represents
the temperature, and Œª controls the similarity between the draft and target distributions. We highlight the best,
second best, and third best entries.
T
Œª
RRS
RRSw
OTM
OTMw
SpecHub
0.1
0.7
0.6273
0.7120
0.6380
0.7345
0.7402
0.1
0.5
0.3323
0.4057
0.3346
0.4125
0.4123
0.25
0.7
0.7354
0.7653
0.7846
0.8321
0.8113
0.25
0.5
0.4564
0.4978
0.4743
0.5245
0.4968
0.5
0.7
0.8090
0.8122
0.9037
0.9150
0.8500
0.5
0.5
0.6456
0.6593
0.7052
0.7206
0.6403
ity between distributions and low entropy, whereas
OTM shines with increased distribution complexity.
SpecHub‚Äôs consistent performance across different
conditions emphasizes its robustness, particularly
when distribution similarity is moderate (Œª = 0.5),
where it maintains competitive acceptance rates,
closely trailing the best results.
E
Maximum Flow Problem Formulation
At k = 2, our Linear Programming (LP) formula-
tion describes an equivalent Maximum Flow Prob-
lem formulation. This formulation effectively mod-
els the Multi-Draft Speculative Decoding process
as the transportation of probability mass through a
network of pipes.
Given an LP formulation with vocabulary set V,
pair sampling distribution Q ‚àà‚àÜ|V|2‚àí1, and tar-
get distribution p ‚àà‚àÜ|V|‚àí1, we construct a graph
G = (V, E) where the vertex set V consists of the
vocabulary V, a source vertex s, and a sink vertex
t. The capacity function g : (u, v) ‚ààE ‚Üí[0, 1] is
defined for each edge as follows:
g(u, v) =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
P
x2 Qvx2,
if u = s and v ‚ààV,
p(v),
if u ‚ààV and v = t,
Quv,
if u, v ‚ààV and u Ã∏= v,
0,
otherwise.
In this formulation, the source vertex s distributes
the total probability mass to the vertices in the vo-
cabulary set V, while the sink vertex t collects the
transported probability mass from the vocabulary
vertices. The edges between the vocabulary ver-
tices represent the possible transitions dictated by
the pair sampling distribution Q. This network flow
model not only provides an intuitive visualization
of the probability mass transport process but also
allows us to leverage well-established algorithms
in network flow theory to solve the MDSD problem
efficiently.
F
More Experiment Details
JF68m on Full Binary Trees and Binary Sequoia
Unbalanced Trees
We conducted experiments to
measure the batch efficiency of the JF68m model
on both full binary trees and binary Sequoia unbal-
anced trees. For the full binary trees, we tested tree
depths ranging from d = 2 to d = 5, and for the
binary Sequoia trees, we used an unbalanced tree
structure with varying depths. The results demon-
strate that SpecHub consistently outperforms both
RRS and RRSw across all tree depths. In the full bi-
nary tree configuration, SpecHub achieves a batch
efficiency improvement of 0.02 ‚àí0.10 over RRSw
and 0.04‚àí0.20 over RRS at temperatures T = 0.6
and 1.0. For the binary Sequoia unbalanced trees,
SpecHub maintains a higher batch efficiency, con-
firming its robustness on the more efficient unbal-
anced tree structures.
JF160m on Binary and Ternary Trees
We also
evaluated the batch efficiency of RRS and RRSw
using the JF160m model on both binary and ternary
trees. For binary trees, we tested tree depths from
d = 2 to d = 6, and for ternary trees, we consid-
ered depths up to d = 4. The JF160m model shows
significant improvements in batch efficiency when
using SpecHub. At temperatures T = 0.6 and 1.0,
SpecHub outperforms RRS by 0.03 ‚àí0.12 and
RRSw by 0.05 ‚àí0.15 in binary tree configurations.
The performance of RRS and RRSw in the ternary
tree setting is worse than SpecHub on binary trees,
suggesting the benefit of using more drafts is less
significant.
EAGLE Decoding Head
To further explore the
efficiency of our proposed method, we imple-


--- Page 18 ---
mented the SpecHub algorithm using the EAGLE
decoding head. The batch efficiency was evaluated
on binary trees of depths d = 2 to d = 5. SpecHub
with the EAGLE decoding head shows a substan-
tial increase in efficiency, generating up to 3.53 and
3.33 tokens per iteration at temperatures T = 0.6
and 1.0, respectively. This represents an additional
0.08 tokens per iteration compared to RRS without
replacement. The experimental results reinforce
the benefits of integrating SpecHub with advanced
decoding heads like EAGLE, particularly in en-
hancing batch efficiency.
Larger Models
In addition to the results reported
in the main paper, we conducted further experi-
ments on larger models, specifically Llama 2-13B-
Chat and Vicuna-1.3-33B, to evaluate the scala-
bility of SpecHub. As shown in Tables 8 and 9,
SpecHub consistently outperforms both Recursive
Rejection Sampling (RRS) and RRS without re-
placement across all configurations of tree depth
and temperature (T = 0.6 and T = 1.0). For
Llama 2-13B-Chat, SpecHub achieves up to 2.77
tokens per step on the OpenWebText dataset, while
for Vicuna-33B, it generates up to 3.32 tokens per
step on the MT-Bench dataset. These results high-
light SpecHub‚Äôs ability to maintain high batch ef-
ficiency and token acceptance rates as model size
increases, demonstrating its robust scalability when
applied to larger language models.


--- Page 19 ---
Table 5: Batch Efficiency Results for JF68m Data Average accepted tokens and batch efficiency for different
configurations of target model and draft model pairs across various temperatures. SpecHub consistently outperforms
RRS and RRSw in both acceptance rate and batch efficiency. We also include binary Sequoia trees and show that
SpecHub performs well on unbalanced trees.
T
Data
Tree
RRS
RRSw
SpecHub
Tree
RRS
RRSw
SpecHub
0.6
CNN
22
1.5540
1.5997
1.6157
biSeq4
1.7938
1.8304
1.8498
0.6
OWT
22
1.5485
1.5895
1.6080
biSeq4
1.7971
1.8225
1.8424
0.6
CNN
23
1.8482
1.9685
1.9863
biSeq8
2.0361
2.1540
2.1542
0.6
OWT
23
1.8576
1.9241
1.9632
biSeq8
2.0247
2.1005
2.1285
0.6
CNN
24
2.0510
2.1694
2.2456
biSeq16
2.1354
2.2667
2.2839
0.6
OWT
24
2.0256
2.1299
2.2103
biSeq16
2.1378
2.2153
2.2064
0.6
CNN
25
2.1385
2.3149
2.4031
biSeq32
2.2452
2.4198
2.4353
0.6
OWT
25
2.0867
2.2295
2.3416
biSeq32
2.2007
2.3556
2.3868
1.0
CNN
22
1.5432
1.5521
1.5997
biSeq4
1.7401
1.7469
1.8057
1.0
OWT
22
1.5488
1.5509
1.5905
biSeq4
1.7355
1.7437
1.7879
1.0
CNN
23
1.8384
1.8790
1.9832
biSeq8
1.9522
2.0063
2.0667
1.0
OWT
23
1.8232
1.8585
1.9661
biSeq8
1.9304
2.0008
2.0720
1.0
CNN
24
1.9762
2.0441
2.2106
biSeq16
2.0529
2.1662
2.2843
1.0
OWT
24
1.9954
2.0493
2.1957
biSeq16
2.0330
2.1030
2.2619
1.0
CNN
25
2.0694
2.1383
2.3104
biSeq32
2.1197
2.1604
2.3445
1.0
OWT
25
2.0890
2.1574
2.3149
biSeq32
2.1008
2.1950
2.3571


--- Page 20 ---
Table 6: Batch Efficiency Results for JF160m Data Average accepted tokens and batch efficiency for different
configurations of target model and draft model pairs at T = 0.6 and T = 1.0. The results are presented for CNN
and OpenWebText datasets, comparing RRS, RRS without replacement, and TransportHub. We also contained
results on ternary trees to showcase that using k > 2 gives diminishing gain.
T
Data
Tree
RRS
RRS w/o
SpecHub
0.6
CNN
22
1.633994691
1.667634674
1.6861
0.6
OpenWebText
22
1.641550493
1.672971282
1.677
0.6
CNN
23
2.016376307
2.142804292
2.1758
0.6
OpenWebText
23
2.052868003
2.113952048
2.115
0.6
CNN
32
1.66262118
1.734944266
0.6
OpenWebText
32
1.669826224
1.70473377
0.6
CNN
24
2.282944241
2.369522017
2.4841
0.6
OpenWebText
24
2.28490566
2.411659014
2.4492
0.6
CNN
33
2.113219655
2.279599835
0.6
OpenWebText
33
2.111602497
2.212962963
0.6
CNN
25
2.378323523
2.604486152
2.7238
0.6
OpenWebText
25
2.449243411
2.642651616
2.6901
0.6
CNN
34
2.39760652
2.681949084
0.6
OpenWebText
34
2.433582166
2.667044296
1.0
CNN
22
1.608515798
1.633187465
1.6748
1.0
OpenWebText
22
1.633351663
1.635781207
1.6834
1.0
CNN
23
1.959878368
2.053886546
2.1362
1.0
OpenWebText
23
2.028797337
2.077786547
2.1584
1.0
CNN
32
1.663016602
1.689861121
1.0
OpenWebText
32
1.677094972
1.701585742
1.0
CNN
24
2.20357984
2.286009649
2.4204
1.0
OpenWebText
24
2.295532975
2.379759419
2.4922
1.0
CNN
33
2.105012354
2.165854573
1.0
OpenWebText
33
2.166307084
2.233691623
1.0
CNN
25
2.315296164
2.41812897
2.6624
1.0
OpenWebText
25
2.429887821
2.532017591
2.7334
1.0
CNN
34
2.382244389
2.474047719
1.0
OpenWebText
34
2.467950678
2.550284031


--- Page 21 ---
Table 7: Batch Efficiency Results for SpecHub and
RRS using EAGLE The batch efficiency of SpecHub
and Recursive Rejection Sampling (RRS) methods when
applied with EAGLE. The table reports average ac-
cepted tokens per step across different temperatures
and datasets, demonstrating that SpecHub consistently
outperforms RRS.
T
Tree
RRS
RRS-wo
SpecHub
0.6
22
1.8211
1.8687
1.8825
0.6
23
2.4325
2.5585
2.5939
0.6
24
2.9125
3.0899
3.1192
0.6
25
3.2501
3.4838
3.5380
1.0
22
1.8054
1.8327
1.8655
1.0
23
2.3961
2.4737
2.4850
1.0
24
2.8425
2.9019
3.0281
1.0
25
3.1451
3.2548
3.3318


--- Page 22 ---
Table 8: Batch Efficiency Results for Llama 2-13B-Chat Average accepted tokens and batch efficiency for
Llama 2-13B-Chat with different configurations at T = 0.6 and T = 1.0. The results compare RRS, RRS without
replacement, and SpecHub on the CNN and OpenWebText datasets.
T
Data
Tree
RRS
RRS w/o
SpecHub
0.6
CNN
22
1.5876
1.6420
1.6628
0.6
OpenWebText
22
1.6307
1.6724
1.6958
0.6
CNN
23
1.9981
2.0507
2.0888
0.6
OpenWebText
23
2.0151
2.1424
2.1798
0.6
CNN
24
2.1921
2.3305
2.3873
0.6
OpenWebText
24
2.3048
2.4331
2.5251
0.6
CNN
25
2.3181
2.4716
2.5689
0.6
OpenWebText
25
2.4882
2.6769
2.7774
1.0
CNN
22
1.5993
1.6251
1.6542
1.0
OpenWebText
22
1.6192
1.6415
1.6687
1.0
CNN
23
1.9477
1.9839
2.0878
1.0
OpenWebText
23
1.9994
2.0716
2.1538
1.0
CNN
24
2.1784
2.2354
2.3739
1.0
OpenWebText
24
2.2323
2.3355
2.4581
1.0
CNN
25
2.2778
2.3684
2.5540
1.0
OpenWebText
25
2.4226
2.5218
2.7190
Table 9: Batch Efficiency Results for Vicuna-1.3-33B Average accepted tokens and batch efficiency for Vicuna-
33B with different configurations at T = 0.6 and T = 1.0. The results compare RRS, RRS without replacement,
and SpecHub on the MT-Bench dataset.
T
Data
Tree
RRS
RRS w/o
SpecHub
0.6
MT-Bench
22
1.7878
1.8327
1.8490
0.6
MT-Bench
23
2.3469
2.4694
2.5098
0.6
MT-Bench
24
2.7273
2.9298
2.9762
0.6
MT-Bench
25
3.0102
3.2114
3.3245
1.0
MT-Bench
22
1.7792
1.8257
1.8454
1.0
MT-Bench
23
2.3399
2.4169
2.4650
1.0
MT-Bench
24
2.7252
2.8649
2.9230
1.0
MT-Bench
25
3.0190
3.1116
3.2079
