--- Page 1 ---
3D-Learning: Diffusion-Augmented Distributionally
Robust Decision-Focused Learning
Jiaqi Wen
University of Houston
Lei Fan
University of Houston
Jianyi Yang*
University of Houston
Abstractâ€”Predict-then-Optimize (PTO) pipelines are widely
employed in computing and networked systems, where Machine
Learning (ML) models are used to predict critical contextual
information for downstream decision-making tasks such as cloud
LLM serving, data center demand response, and edge workload
scheduling. However, these ML predictors are often vulnerable
to out-of-distribution (OOD) samples at test time, leading to
significant decision performance degradation due to large pre-
diction errors. To address the generalization challenges under
OOD conditions, we present the framework of Distributionally
Robust Decision-Focused Learning (DR-DFL), which trains ML
models to optimize decision performance under the worst-
case distribution. Instead of relying on classical Distributionally
Robust Optimization (DRO) techniques, we propose Diffusion-
Augmented Distributionally Robust Decision-Focused Learning
(3D-Learning), which searches for the worst-case distribution
within the parameterized space of a diffusion model. By lever-
aging the powerful distribution modeling capabilities of diffu-
sion models, 3D-Learning identifies worst-case distributions
that remain consistent with real data, achieving a favorable
balance between average and worst-case scenarios. Empirical
results on an LLM resource provisioning task demonstrate that
3D-Learning outperforms existing DRO and Data Augmenta-
tion methods in OOD generalization performance.1
Index
Termsâ€”Diffusion
Models,
Distributionally
Robust
Learning, Decision-Focused Learning.
I. INTRODUCTION
Many context-aware decision-making problems in comput-
ing and communication networks can be formulated within
the Predict-then-Optimize (PTO) framework, where effective
decision-making critically depends on the accurate prediction
of system context [29]. One key example is resource pro-
visioning for cloud (Large Language Model) LLM serving,
where the accurate token-level workload prediction is essen-
tial for efficiently allocating computing resources (e.g., GPU
cores or frequency) to mitigate over-provisioning or service
degradation [4], [45], [48]. Another critical application is
demand response in AI data centers, where accurate prediction
of AI workloads of different types and renewable energy
availability is essential for scheduling the AI computation
workload to reduce energy costs while maintaining service-
level agreements [6], [23], [50].
In many PTO applications, decision performance is highly
sensitive to specific types of prediction errors. For instance,
in cloud resource provisioning, underestimating the workload
* Correspondence to: Jianyi Yang (jyang66@uh.edu).
1Source Code Link: https://github.com/CIGLAB-Houston/3DLearning.git
can lead to sever service degradation, whereas overestimation
may simply incur additional cost. As a result, the decision
performance can be sub-optimal by training ML models solely
to minimize prediction error without considering its impact
on downstream decision objectives. Decision-Focused Learn-
ing (DFL) [10], [29] addresses the limitation of traditional
prediction-focused learning by training ML models in an end-
to-end manner to directly optimize the final decision objective.
By aligning the learning process with decision performance,
DFL can provide more effective and robust strategies.
Despite its advantages over prediction-focused learning,
DFL still struggles to generalize under Out-of-Distribution
(OOD) testing scenariosâ€”a common challenge in dynamic
ML-based systems. In cloud workload scheduling, for in-
stance, shifts in user demand, task types, or market dynamics
can lead to fluctuating workload patterns that differ signif-
icantly from those seen during training. When faced with
such distribution shifts, a DFL model trained solely on in-
distribution data may make decisions with poor performance.
This vulnerability arises because standard DFL optimizes
decision performance based solely on the empirical training
distribution, without accounting for potential distribution shifts
at test time. To address this limitation, we introduce the
Distributionally Robust Decision-Focused Learning (DR-DFL)
framework, which seeks to optimize decision performance
under the worst-case distribution within an ambiguity set based
on the training data. With a well-designed ambiguity set, DR-
DFL can enable more resilient decision-making under real-
world OOD deployment scenarios [13], [19], [32].
A central challenge in DR-DFL lies in the modeling of
the ambiguity set, which defines a distribution discrepancy
measure to capture meaningful variations around the training
distribution. However, Distributionally Robust Optimization
(DRO) methods with traditional ambiguity modeling often
lead to suboptimal DR-DFL performance. Ambiguity sets
based on Ï•-divergences (e.g., KL divergence) restrict the
distributions to be absolutely continuous with respect to the
training distribution, thereby excluding test distributions with
shifted support. As a result, DR-DFL with Ï•-divergence-based
ambiguity sets may yield non-robust solutions under support
shift, even when enhanced with data augmentation techniques.
In contrast, Wasserstein distance-based ambiguity sets allow
for support variation but often result in intractable optimization
problems, particularly when the decision objective is non-
convex. In such cases, solving the DRO problem typically
arXiv:2602.02943v1  [cs.LG]  3 Feb 2026


--- Page 2 ---
requires relaxations, which may result in overly conservative
training, ultimately degrading the average-case performance.
These limitations highlight the need for a more expressive and
computationally tractable ambiguity modeling in DR-DFL.
This paper focuses on addressing the challenges of ambigu-
ity set design and proposes a novel DR-DFL framework based
on diffusion models with the following main contributions:
â€¢ Diffusion-based Ambiguity Modeling. We introduce a
new ambiguity modeling based on the score matching
loss of diffusion models, offering several advantages.
First, it allows the ambiguity set to include distributions
with diverse and shifted support. Second, by constraining
the score matching loss, we can ensure that candidate
distributions remain consistent with the underlying data
distribution. Finally, it enables the tractable search for the
worst-case distribution within the parameterized space of
the diffusion model.
â€¢ Diffusion-Augmented
Algorithm
Design.
We
pro-
pose the Diffusion-Augmented Distributionally Robust
Decision-Focused Learning (3D-Learning) algorithm,
which integrates diffusion-based ambiguity modeling into
the DR-DFL pipeline. It addresses the challenges of the
constrained, non-convex inner maximization problem by
combining the dual learning techniques with diffusion
policy optimization methods. Furthermore, given the in-
ner maximization output, we design the min-max solver
for DR-DFL based on the diffusion sampling.
â€¢ Performance Evaluation on LLM Resource Provi-
sioning. We formulate the resource provisioning prob-
lem for LLM inference as a PTO pipeline and eval-
uate 3D-Learning against a range of DRO and
data augmentation baselines. Simulation results demon-
strate that 3D-Learning significantly outperforms
traditional DRO methods and data augmentation ap-
proaches in both average-case and worst-case perfor-
mance across test datasets exhibiting various distribution
shifts. Moreover, under various noisy perturbation sce-
narios, 3D-Learning demonstrates exceptional noise
robustness and stability.
II. FORMULATION AND APPICATIONS
A. Decision-Focused Learning
PTO problems have wide applications in computing and
communication networks [44], [46], [49]. In a PTO pipeline
as shown in 1, a ML predictor hw âˆˆH with weight w maps
an input v (e.g. historical context or side information) into a
context prediction Ë†c. Then a decision-making step is taken to
optimize the decision objective based on the predicted context
Ë†c. We consider a general decision-making objective as
yâˆ—(c) = arg min
yâˆˆY f(y, c),
(1)
where f is the objective functions and Y incorporates the
constraints on the decision variable y.
In a standard ML training loss, we usually define the
training loss by some discrepancy measure l(Ë†c, c) such as
Dataset
 ğ‘ºğœ½âˆ¼ğ‘·ğœ½
Task Loss
ğ‘­ğ’ğ’“ğ’˜ğ’‚ğ’“ğ’…
ğ‘¹ğ’†ğ’—ğ’†ğ’“ğ’”ğ’†
ğ‘¥0
ğ‘¥ğ‘‡
ğ‘¥ğ‘¡âˆ’1
ğ‘¥ğ‘¡
â‹… â‹… â‹…
â‹… â‹… â‹…
ğ‘(ğ‘¥ğ‘¡|ğ‘¥ğ‘¡âˆ’1)
ğ‘(ğ‘¥ğ‘¡âˆ’1|ğ‘¥ğ‘¡)
ğ‘“(ğ‘¦âˆ—( Æ¸ğ‘), ğ‘)
Decision Focused Training
ğ‘¦âˆ—( Æ¸ğ‘) = argmin
ğ‘¦âˆˆğ’´
ğ‘“(ğ‘¦, Æ¸ğ‘)
ML ğ’‰ğ’˜
Constrained Optimization
ğ‘¦âˆ—( Æ¸ğ‘)
Task Objective 
ğ‘©ğ’‚ğ’„ğ’Œğ’˜ğ’‚ğ’“ğ’…
Æ¸ğ‘
ğ‘©ğ’‚ğ’„ğ’Œğ’˜ğ’‚ğ’“ğ’…
Search for the distribution ğ‘·ğœ½ that maximizes 
the expected task loss ğ”¼ğ‘ƒğœƒ[ğ‘“ğ‘¦âˆ—
Æ¸ğ‘, ğ‘] within 
the diffusion-based ambiguity set 
Fig. 1. Framework of Decision Focused Learning
Mean Squared Error (MSE) or Cross-Entropy (CE) between
the predicted label Ë†c and the ground-truth label c. Denote
x = (v, c) as a labeled sample. We minimize the empiri-
cal loss ES0[l(hw(v), c)] based on a training dataset S0 =
{x1, Â· Â· Â· , x|S0|} with |S0| samples drawn from an underlin-
ing distribution P0. Existing works [29] have demonstrated
that such standard ML training may not achieve satisfactory
decision performance because the information of decision
objective (1) is not incorporated in the ML training. Therefore,
DFL is proposed to train the ML model in an end-to-end style
by directly minimizing the decision objective f(yâˆ—(Ë†c), c). The
training objective of DFL is expressed as
hw = arg min
hwâˆˆH ES0[f(yâˆ—(Ë†c), c)],
(2)
where yâˆ—(Ë†c) is the solution of (1) given a predicted context
Ë†c = hw(v). DFL is more general than standard ML training
because it reduces to a standard ML training by choosing the
decision objective as the label discrepancy measures.
B. Distributionally Robust Decision-Focused Learning
Although DFL directly optimizes the decision objective,
it suffers from significant performance drop in the OOD
testing environment. As illustrated by examples in Section
II-C, the distribution of testing context can shift a lot over
time, resulting in unreliable decision performance.
DRO is a widely-adopted framework to improve OOD
generalization performance. For the PTO pipeline with the
decision objective (1), we introduce DR-DFL which trains
predictor to minimize the worst-case decision objective by a
min-max optimization:
hw = arg min
hwâˆˆH
max
P âˆˆB(P0,Ïµ) EP [f(yâˆ—(Ë†c), c)],
(3)
where P0 is the underlining distribution of the training dataset
S0, and B(P0, Ïµ) is the ambiguity set which contains possible
testing distributions and is typically modeled as a distribution
ball B(P0, Ïµ) = {P | D(P, P0) â‰¤Ïµ} given a distribution
discrepancy measure D and a budget Ïµ. Based on the discrep-
ancy measure D in the ambiguity set, we can get different
DRO methods. The widely-adopted discrepancy measures in-
clude Wasserstein distance and the family of Ï•âˆ’divergence


--- Page 3 ---
which lead to Wasserstein DRO [5], [11], [32], [51] and
Ï•âˆ’divergence DRO [13]â€“[15], [17] respectively.
The choice of the ambiguity set has a large effect on the
performance of DRO. As the decision objective in the PTO
pipeline can be very sensitive to the distribution shifts, it
becomes critical to choose a proper ambiguity set in DR-DFL.
As the focus of this paper, we will discuss the challenges of
ambiguity modeling and present our diffusion-based ambiguity
modeling in Section III.
C. Applications in Computing and Communication Networks
The considered PTO problem has wide applications for
context-aware decision-making problems in computing and
communication networks.
1) Workload-Aware Resource Provisioning for LLM Infer-
ence: With the rapid deployment of AI, particularly large
language models (LLMs), the substantial energy costs of AI
workloads have become a critical concern. In AI data centers,
inference workloads often constitute a large fraction of total
computing demand [4], [45], [48]. As LLMs are increasingly
adopted, serving systems must process a large volume of
LLM inference requests. Given the limited computing capacity
and fluctuating demand, data center operators usually need
to provision resources in advance based on predicted work-
loads [7]. However, the distribution of inference demand can
evolve significantly, making it difficult to forecast workloads
accurately and to strike an effective balance between energy
efficiency and performance guarantees based on the prediction.
We develop a distributionally robust LLM workload predictor
focusing on the objectives of serving performance and energy
costs with a detailed case study in Section V.
2) Demand Response in AI Data Centers: The rapid ad-
vancement of AI technologies has driven an exponential
increase in the demand for high-performance AI data cen-
ters, which places a substantial burden on power grids and
contributes to high energy costs [8]. Despite their high power
consumption, the flexibility in AI training workloads opens up
opportunities for demand response (DR), where data centers
adapt their energy usage in response to grid signals such as
real-time electricity prices and carbon intensity [6], [23], [50].
The power usage can be controlled by workload shifting (e.g.,
deferring non-urgent jobs to off-peak hours) or power capping
(e.g., reducing server utilization or GPU frequency). In this
way, AI data centers can significantly lower energy costs while
satisfying Service Level Agreements (SLAs) for AI workloads.
However, the performance of DR strategies relies on the
accurate prediction of the workloads and the grid signals,
which is complicated by the high variability of AI workloads
and the integration of renewable sources. Given the context
uncertainty, it is essential to develop distributionally robust and
decision-focused ML models to provide accurate predictions
which directly support DR objectives under uncertainty.
3) Channel-Aware Edge Data Center Selection: Edge Data
Centers (EDCs) offer heterogeneous hardware and software
resourcesâ€”including CPUs, GPUs, memory, and pre-deployed
AI modelsâ€”to support low-latency Mobile Edge Computing
(MEC) services. In a typical MEC system, the incoming
computational requests must be assigned to suitable EDCs in
order to meet service quality objectives including latency and
inference accuracy [27], [30], [42]. Unlike traditional cloud
computing environments, where network latency is relatively
stable, edge computing environments are highly dynamic due
to user mobility, wireless conditions, and network congestion.
This makes it challenging to predict channel conditions which
are essential context to optimize the service quality of EDC
selection. Thus, to fight against uncertainty in real-world edge
environments, a robust and decision-focused channel quality
predictor is critically needed for optimizing the user-to-EDC
assignments with their service objectives.
III. DIFFUSION AMBIGUITY MODELING
A. Challenges in Ambiguity Modeling
The choice of ambiguity set in DR-DFL (3) has a large
effect on the robustness performance and the solution tractabil-
ity. The limitations of the commonly-used Wasserstein and
Ï•âˆ’divergence based ambiguity sets are introduced as below.
If we choose the discrepancy measure as the Wasser-
stein distance DW(P, P0), we get the Wasserstein DRO
(W-DRO). However, it is difficult to find a tractable solu-
tion for W-DRO. Some methods [5], [11], [32] reformulate
Wasserstein-constrained DRO into a finite optimization based
on the assumption of convex objectives which typically do
not hold in deep learning. Other methods convert W-DRO into
an adversarial training with norm constraints on samples [11],
[39], but these solutions can be overly conservative and cannot
fully exploit the benefits of probabilistic ambiguity modeling
to improve generalization.
Alternatively, the distribution discrepancy can be measured
by the family of Ï•-divergence DÏ•(Pâˆ¥P0) = EP0[Ï•( dP
dP0 )]
where Ï• is a convex function with Ï•(1) = 0 [13], [14], [17],
[18], [24]. If we choose Ï•(Ï„) = Ï„ ln(Ï„), we get KL-DRO with
a KL-divergence-based ambiguity set. A closed-form solution
to the inner maximization (3) of KL-DRO is provided in [13].
Nevertheless, the definition of Ï•-divergence requires any distri-
bution P in the ambiguity set to be absolutely continuous with
respect to the training distribution P0 (denoted as P << P0),
which means for any measurable set A, P0(A) = 0 implies
P(A) = 0. This implicit restriction narrows the ambiguity set
and consequently limits the robustness of DRO when facing
test scenarios with support shift.
The intrinsic difficulty of modeling ambiguity sets in DR-
DFL stems from the infinite dimension of the probability
space. This motivates us to model the ambiguity set in a
parameterized space. Therefore, unlike these traditional am-
biguity modeling, we leverage diffusion models to directly
learn the worst-case distribution in the context of DR-DFL
exploiting their strong distribution modeling capability [12],
[36]â€“[38] introduced as follows.
B. Distribution Modeling via Diffusion Models
The diffusion models learn the underlining distribution from
a finite dataset and can generate more samples from the


--- Page 4 ---
underlining distribution. They rely on forward and backward
stochastic processes introduced as follows.
Forward Process. The forward process incrementally in-
jects noise into the data, generating a sequence of perturbed
samples. It begins with an initial sample x0 âˆˆRd drawn from
the underlining distribution P0, and evolves according to a
stochastic process as:
dx = k(x, t)dt + g(t)dw,
(4)
where k(Â·, t) : Rd â†’Rd is a vector-valued function, g(t) âˆˆ
R, w is a standard Wiener process and dw is white Gaussian
noise. By the forward process, we get a collection of variables
{xt}tâˆˆ[0,T ]. We use Pt to represent the distribution of xt and
Pt|0 to denote the conditional distribution of xt given x0 âˆ¼P0.
With a sufficiently long time T, the marginal distribution
PT (xT ) approximates a tractable prior distribution Ï€(x) which
is typically chosen as a standard Gaussian distribution.
Reverse Process. A reverse diffusion process is associated
with the forward equation in (4) and is expressed as
dx =
 k(x, t) âˆ’g(t)2âˆ‡x log Pt(x)

dt + g(t)d Â¯w,
(5)
where Â¯w is a standard Wiener process in the reverse-time
direction, âˆ‡x log Pt(x) is the time-dependent score function.
Score Matching. In the reverse process, the score function
âˆ‡x log Pt(x) plays a critical role in directing the dynamics.
To estimate the score function âˆ‡x log Pt(x), we train a score-
based model sÎ¸(x, t) based on samples generated from the
forward diffusion process. The score-based model should
minimize the following score-matching loss:
JSM(Î¸) =
Z T
0
EPt(x)
h
Î»(t) âˆ¥âˆ‡x log Pt(x) âˆ’sÎ¸(x, t)âˆ¥2i
dt,
where Î»(t) > 0 is a positive weighting function. We usually
approximate the score-matching loss by a tractable denoising
score-matching loss up to a constant that does not rely on Î¸:
J(Î¸)=
Z T
0
EP0(x)Pt|0(xâ€²|x)
h
Î»(t)
âˆ‡xâ€²log Pt|0(xâ€²|x)âˆ’sÎ¸(x, t)
2i
dt,
(6)
Sampling. If we discretize the reverse process, initialize
xT âˆ¼Ï€ and replace âˆ‡x log Pt(x) with the score-based model
sÎ¸(x, t), we can generate samples with a Markov chain with
T steps:
xtâˆ’1 = xt +[k(xt, t)âˆ’g2(t)sÎ¸(xt, t)]âˆ†t+g(t)
p
|âˆ†t|zt, (7)
where âˆ†t is a small enough constant and zt âˆ¼N(0, I).
Most existing diffusion models generate samples following the
Markov chain [9], [12], [36], [38] and a common expression
for the joint distribution of the reverse outputs is
PÎ¸(x0:T ) = Ï€(xT )
T
Y
t=1
PÎ¸(xtâˆ’1 | xt),
(8)
where PÎ¸(xtâˆ’1 | xt) = N(xtâˆ’1; ÂµÎ¸(xt, t), Î£Î¸(xt, t)).
The following lemma shows that constraint on the denoising
score-matching loss (6) implies that the constraint on the KL-
divergence between P0 and PÎ¸ is satisfied.
Lemma 1 (Theorem 1 and Corollary 1 in [37]). Given the
assumptions listed in Appendix A of [37] 2, if the denoising
score-matching loss (6) satisfies J(Î¸) â‰¤Ïµ, the output distri-
bution of the diffusion model PÎ¸(x0) satisfies
DKL(P0||PÎ¸) â‰¤Ïµ + DKL(PT ||Ï€) + C1,
where PT is the final-step output distribution of the forward
process and PT â‰ˆÏ€ by the design of diffusion models, and
C1 is a constant that does not rely on Î¸.
Note that the KL-divergence DKL(P0||PÎ¸) in Lemma 1
is not the KL-divergence DKL(PÎ¸||P0) commonly used in
KL-DRO. The former KL-divergence allows PÎ¸ to have
broader support space than P0 (P0 << P).
C. Diffusion-Based Ambiguity Set
Lemma 1 implies that if we find a diffusion model dis-
tribution PÎ¸ whose parameters satisfy J(Î¸) â‰¤Ïµ, then PÎ¸
stays close to the training distribution P0 through a KL-
divergence DKL(P0||PÎ¸) up to a budget related to Ïµ. This
KL-divergence is the reversed KL-divergence and allows PÎ¸ to
have broader support space than P0. Therefore, we can define
a parameterized ambiguity set based on the diffusion models
without the support shift issue. The DR-DFL with diffusion-
based ambiguity set is expressed as below.
min
wâˆˆW max
Î¸âˆˆÎ˜ EPÎ¸(x)[f(yâˆ—(hw(v)), c)],
s.t. J(Î¸, S0) â‰¤Ïµ, (9)
where PÎ¸(x), x = (v, c) is the output distribution of the
diffusion model, J(Î¸, S0) is the denoising score-matching loss
of a diffusion model based on a training dataset S0.
The diffusion-based ambiguity set leverages the powerful
distribution modeling capabilities of diffusion models to en-
hance the generalization performance of DR-DFL. Specifi-
cally, diffusion models can generate diverse samples beyond
the support of the training distribution, enabling the discovery
of distributions with the worst decision-making performance,
thereby yielding robust solutions. By constraining on the
score-matching loss, the distributions in the ambiguity set
remains consistent with the training data, striking a balance
between average-case and worst-case performance. Moreover,
the inner maximization in (9) is conducted over a finite
parameterized space rather than an infinite probability space,
making the inner maximization tractable.
IV. 3D-LE A R N I N G ALGORITHM
Despite the advantages of diffusion-based ambiguity set, it
is challenging to solve DR-DFL in (9) due to the complexity
of diffusion models. We propose 3D-Learning algorithm
in this section to solve this challenge.


--- Page 5 ---
Algorithm 1 Inner Maximization of 3D-Learning (IMAX)
Input: Training dataset S0; ML model hw, Adversary budget
Ïµ > 0; Step size Î· > 0.
1: Initialization. Initialize the diffusion parameter Î¸ and
Lagrangian weight Î± > 0.
2: for k = 1, 2, Â· Â· Â· , K do
3:
Update diffusion model Î¸k by solving (10) given Î±.
4:
Update the Lagrangian parameter Î±: Î± â†max{Î± +
Î·(J(Î¸k, S0) âˆ’Ïµ), 0}.
5: end for
6: return Adversarial diffusion model Î¸K.
A. Inner Maximization of 3D-Learning
Solving the inner maximization of (9) presents two key
challenges. First, the problem is a constrained non-convex
optimization, making it difficult to maximize the objective
and minimize the constraint violation simultaneously. Second,
the objective function depends on the diffusion parameter Î¸
through the probability function inside the expectation, which
is computationally expensive to evaluate and differentiate.
We propose Algorithm 1 to tackle the challenges of inner
maximization. Observing that the constraint in (9) is a budget
constraint (J(Î¸) â‰¥0 and Ïµ > 0), we can solve the constrained
optimization by a dual learning method [1], [25]. Algorithm
1 adaptively learns a Lagrangian dual Î± > 0 to convert the
inner constrained maximization into multi-step unconstrained
optimizations below
max
Î¸
EPÎ¸[f(yâˆ—(hw(v)), c)] âˆ’Î±J(Î¸, S0).
(10)
We update Î± by dual gradient descent based on the denoising
score-matching loss J: Increase Î± to emphasize more on the
constraint satisfaction if J violates the budget and decrease Î±
otherwise. After enough iterations, the dual variable converges
to a near-optimal one that balances the objective maximization
and constraint violation.
Next, we transform the objective in (10) into a differentiable
term. By the Markov chain in (8), we can express the joint
probability of denoising outputs as
PÎ¸(x0:T ) = C Â· eâˆ’âˆ¥xT âˆ¥2
2
Â· e
âˆ’PT
t=1
âˆ¥xtâˆ’1âˆ’ÂµÎ¸(xt,t)âˆ¥2
2Ïƒ2
t
,
(11)
where C is a nomalizing constant, and Ïƒ2
t is the variance of the
reserve noise at step t. Then, we can exploit tricks in policy
optimization algorithms to transform the expected objective.
By the trick in vanilla policy gradient [41], we can derive
the gradient of the expected objective in (10) as
âˆ‡Î¸EPÎ¸(x)[f(yâˆ—(hw(v)), c)] =
EPÎ¸(x0:T )[âˆ‡Î¸ ln PÎ¸(x0:T ) Â· f(yâˆ—(hw(v)), c)],
(12)
2The
assumptions
require
that
the
expected
squared
norm
over
P0
and
Ï€
are
bounded
by
any
finite
value,
the
functions
k(Â·, t),
âˆ‡x log Pt(x),
and
sÎ¸(Â·, t)
are
Lipschitz
continuous
and
upper
bounded by a value related to âˆ¥xâˆ¥, g(t) is a non-zero function, and
R T
t=0
R
O âˆ¥Pt(x)âˆ¥2
2 + dg(t)2âˆ¥âˆ‡xPt(x)âˆ¥2
2dxdt for any open bounded set O
and E
h
exp( 1
2
R T
t=0 âˆ¥âˆ‡x log Pt(x) âˆ’sÎ¸(x, t)âˆ¥2
2dt)
i
are bounded by any
finite value.
where ln PÎ¸(x0:T ) = PT
t=1[xtâˆ’1 âˆ’ÂµÎ¸(xt, t)]2 + C2 where
C2 is a constant. We can empirically calculate the expected
gradient in (12) based on a dataset sampled from the reverse
process of the diffusion model PÎ¸.
Proximal Policy Optimization (PPO) [35] is believed to
have more stable performance than vanilla policy gradient.
By PPO, we can transform the expected objective in (10) into
a differentiable form as
EPÎ¸[f(yâˆ—(hw(v)), c)] = EPÎ¸0[min(rÎ¸f(yâˆ—(hw(v)), c)),
clip(rÎ¸(x0:T ), 1 âˆ’Îº, 1 + Îº) Â· f(yâˆ—(hw(v)), c))],
(13)
where clip is the clipping function in PPO [35] with the
clipping parameter Îº
âˆˆ
(0, 1), the probability ratio is
rÎ¸(x0:T ) =
PÎ¸(x0:T )
PÎ¸0(x0:T )
= exp{âˆ’PT
t=1( âˆ¥xtâˆ’1âˆ’ÂµÎ¸(xt,t)âˆ¥2
2Ïƒ2
t
âˆ’
âˆ¥xtâˆ’1âˆ’ÂµÎ¸0(xt,t)âˆ¥2
2Ïƒ2
t
)}, and the reference model PÎ¸0 can be a pre-
trained diffusion model on the training dataset S0. Similar as
(12), we can empirically calculate expected objective based on
a the dataset sampled from the reverse process of the diffusion
model PÎ¸0. To reduce the training complexity, we can fix the
parameters of the first T âˆ’T â€² steps and only optimize the
last T â€² steps of the reverse process by choosing rÎ¸(x0:T â€²) =
exp{âˆ’PT â€²
t=1( âˆ¥xtâˆ’1âˆ’ÂµÎ¸(xt,t)âˆ¥2
2Ïƒ2
t
âˆ’
âˆ¥xtâˆ’1âˆ’ÂµÎ¸0(xt,t)âˆ¥2
2Ïƒ2
t
)}.
B. Min-Max Solution of 3D-Learning
Now we are ready to solve the min-max problem in
(9). The min-max solution is extended from the algorithm
of Gradient Descent with Max-Oracle (GDMO) in [16].
For nonconvex-nonconcave min-max optimization problems,
GDMO is proved to guarantee an approximate stationary
solution with the approximation error depending on the error
of inner-maximization.
The algorithm flow of 3D-Learning is provided in Al-
gorithm 2. Following the GDMO framework, 3D-Learning
first runs IMAX in Algorithm 1 to search for the adversarial
diffusion model PÎ¸ that maximizes the expected loss of the
current ML model hw within the diffusion ambiguity set. Next,
given the updated diffusion model PÎ¸, we need to update the
ML parameter w to minimize the expected decision objective
EPÎ¸[f(yâˆ—(hw(v)), c)]. One choice is to perform the gradient
descent on the PPO transformation in (13). However, in order
to provide the ML model with more diverse samples, we
generate an adversarial dataset SÎ¸ by the diffusion model
PÎ¸ and directly approximate the expected decision objective
EPÎ¸[f(yâˆ—(hw(v)), c)] by SÎ¸. Next, we can perform a gradient
descent on the decision-focused objective based on the adver-
sarial dataset SÎ¸. The gradient of the objective can be obtained
by differentiable optimization layers [29]. Alternatively, we
can apply zero-order optimization methods [22] to estimate
the gradients.
V. CASE STUDY
In
this
section,
we
evaluate
the
performance
of
3D-Learning based on a simulation study on resource
management for LLM inference serving. We present the
problem statement and setups, followed by the empirical
comparison of 3D-Learning and baselines.


--- Page 6 ---
Algorithm 2 3D-Learning Algorithm
Input: Training dataset S0; Adversary budget Ïµ > 0.
1: Initialization.
Initialize the ML model parameter w.
2: for epoch = 1, 2, Â· Â· Â· , E do
3:
Run IMAX (hw,Ïµ) in Algorithm 1 to update the diffu-
sion model parameter Î¸.
4:
Generate adversarial dataset SÎ¸ based on the diffusion
model PÎ¸.
5:
Update the ML model parameter w based on SÎ¸.
6: end for
7: return ML model hw.
A. System Model
We give the system model for the application of Cloud Re-
source Provisioning [7] for LLM Inference. Our objective is to
develop a robust LLM workload predictor that achieves a good
trade-off between utility performance and energy costs across
diverse workload patterns. At each time step i âˆˆ[N], the
LLM inference workload is ci, measured as the total number
of input and output tokens assuming the best achievable LLM
performance. Since the exact workload ci is unknown at the
beginning of step i, the operator assigns an LLM instance with
capacity ai based on the predicted workload Ë†ci. While real-
world resource provisioning involves multiple dimensionsâ€”
including CPU, GPU cores, and memoryâ€”we abstract these
complexities by defining ai as the token-handling capacity
of the allocated LLM instance at time i. In other words, ai
represents the number of tokens the instance can process in
the slot i while maintaining optimal LLM performance (no
output length limit).
To jointly capture the inference performance and the cor-
responding energy costs, we define an objective function that
depends on the workload ci and the allocated capacity ai.
Specifically, the utility of assigning an LLM instance with
capacity ai to process a workload of size ci is quantified
by Utility(ai, ci), which reflects the service quality achieved
under the resource allocation ai. When the allocated capacity
fully satisfies the incoming workload (ai â‰¥ci), all requests
can be processed with the highest performance, resulting in the
maximum average utility s(1). In contrast, when the allocated
capacity is insufficient (ai < ci), the platform may either reject
non-critical requests or restrict the output lengths of LLMs,
leading to degraded service quality [21], [47]. In such cases,
the average utility is modeled as s

ai
ci

, where s(Â·) is an
increasing function of the ratio of the allocated capacity to
the demanded capacity. The overall Utility model is defined
as:
Utility(ai, ci) =
(
s(1) Â· ci,
if ai â‰¥ci,
s

ai
ci

Â· ci,
if ai < ci.
(14)
The function Cost(ai, ci) captures the total energy cost
associated with assigning an LLM instance with capacity
ai for a time slot i with an inference workload ci. The
instance processes the workload of min(ai, ci) using acti-
vated computing resources, incurring energy consumption of
Pact Â·min(ai, ci). If the allocated capacity exceeds the actually
demanded workload (ai > ci), the overly allocated capacity
(aiâˆ’ci)+ incurs additional energy consumption PidleÂ·(aiâˆ’ci)+
at a lower power rate , due to power-saving techniques such
as GPU frequency scaling. The total energy cost is scaled by
Power Usage Effectiveness (PUE) Ï‰, and is modeled as:
Cost(ai, ci) = Ï‰ Â·

Pact Â·min(ai, ci)+Pidle Â·(ai âˆ’ci)+
(15)
The overall objective of capacity provisioning is to maxi-
mize the net utility of LLM inference serving, accounting for
both service performance and energy cost. The optimization
problem is formulated as:
max
âˆ€i,aiâˆˆ[amin,amax]R(a1:N, c1:N) :=
N
X
i=1
h
Utility(ai, ci) âˆ’Î³ Â· Cost(ai, ci)
i
,
(16)
where [amin, amax] denote the range of allowable LLM pro-
cessing capacities, and Î³ > 0 is a scaling coefficient that
unifies the units of Utility and Cost.
B. Experiment Setups
1) System Setups: In the LLM inference serving system,
the capacity for an LLM instance is decided at the beginning
of each time slot and remains constant within the time slot to
avoid unstable service quality. A sequence example includes
consecutive N = 28 time slots. In the simulation, we set the
maximum processing speed of an LLM instance with multiple
GPUs as 4 Ã— 105 tokens per time slot. In addition, we set the
power consumption per token as Pact = 4 Ã— 10âˆ’6kWh based
on the estimation of GPT-3 [3] that GPT-3 consumes an order
of 0.4 kWh of electricity to generate 100 pages of content.
The idle power consumption is set as Pidle = 1.4Ã—10âˆ’6kWh
which is about one third of the activated power consumption.
The PUE is set as Ï‰ = 1.1.
For the utility model in (14), we adopt a logarithmic func-
tion s(X) = B log(AX + 1), which captures the diminishing
returns of resource allocation. Logarithmic utility functions are
widely used in network economics and resource allocation.
Similar logarithmic utility functions have also been employed
by Low et al. [26] to model the utility of network flows in
TCP congestion control. Moreover, Stephen et al. [2] high-
light that such utility functions possess desirable propertiesâ€”
monotonicity and strict concavityâ€”which make them well-
suited for modeling fair resource allocation problems. This
form can be readily substituted with alternative utility func-
tions that reflect revenue or service quality in practical LLM
serving scenarios. We choose the parameters in the utility
function as, A = 20, and B = 0.2. The cost coefficient Âµ
is set as 0.34
2) Baselines: The baselines which are compared with our
algorithms in our experiments are introduced as below.


--- Page 7 ---
Decision-Focused Learning (DFL): This method [29] trains
the ML to optimize the decision objective without considering
distributionally robustness.
Wasserstein-based DRO (W-DRO): This is a DRO algo-
rithm where the ambiguity set is defined by the Wasserstein
measure. In the experiments, we choose the FWDRO algo-
rithm in [39] which applies to general objectives, and replace
its loss function with our decision-focused objective.
KL-divergence-based DRO (KL-DRO): This is a DRO
algorithm where the ambiguity set is defined by the KL
divergence. We choose the commonly-used KL-DRO solution
derived in [13] and replace its loss function with our decision-
focused objective.
Data Augmentation (DA): Data augmentation techniques
are commonly used to improve the generalization performance
of ML by incorporating more diverse training samples [20]. In
the experiments, we inject new samples to the training datasets
by adding Gaussian, Perlin or Cutout noise.
3) Datasets: The experiments are conduct based on the
dataset of Azure LLM Inference Traces [33], [40]. The dataset
captures time series of input and output token counts for
each service request in the years 2023 and 2024 from two
production-grade LLM inference services deployed within
Azure, targeting code-related and conversational tasks, respec-
tively. To assess the generalization performance in different
testing distributions, we split the datasets into a training dataset
and several testing datasets with different distribution shifts.
All ML models are trained on the 2023-Conversations (23V
(Train)) dataset with 751 sequence samples and evaluated on
different testing datasets with dataset sizes ranging from 798
to 4320. The distributional discrepancy between each testing
set and the training set is quantified by Wasserstein distance
shown under the dataset names in Table I. The testing sets
are listed as below with their time, LLM task and acronym:
2023-Conversations (23V (Test)), 2023-Code (23D), 2024-
Code (24D), and 2024-Conversations (24V). To increase the
diversity of the testing sets, we merge instances from two
original datasets in an half-to-half way and get three addi-
tional testing sets: 2023-Code&2024-Code (23D24D), 2024-
Code&2024-Conversations (24D24V) and 2023-Code&2024-
Conversations (23D24V).
4) Training Setups: The experimental setup is divided into
the following parts:
Predictor: The workload predictors in 3D-Learning and
all the baselines share the same two-layer stacked LSTM
architecture with 128 and 64 hidden neurons.
Diffusion Model: The diffusion model in 3D-Learning
is DDPM [12] with U-Net backbone which has T = 500 steps
in a forward or a backward process.
Training: For 3D-Learning, we adopt the PPO-based
reformulation in (13) for inner maximization. We train the
reference DDPM Î¸0 in (13) based on the original training
dataset 23V (Train) and use it to generate an initial dataset
Z0 to calculate rÎ¸ in (13). The sampling variance of DDPM is
chosen from a range [0.05, 0.1]. To improve training efficiency,
only the last T â€² = 10 backward steps of the DDPM model
are fine-tuned by (13). We choose a slightly higher clipping
parameter Îº = 0.4 in (13) to encourage the maximization
while maintaining stability. We choose Ïµ = 0.03 as IMAXâ€™s
adversarial budget which gives the best average performance
over all validation datasets. We choose Î· = 0.01 as the rate
to update the Lagrangian parameter Î± in Algorithm 1. We use
the Adam optimizer with a learning rate of 10âˆ’6 for both the
diffusion training in the maximization and the predictor update
in minimization. The diffusion model is trained for 10 inner
epochs with a batch size of 64. The predictor is trained for 15
epochs with a batch size of 64.
For the baseline methods, we choose the same neural
network architecture as 3D-Learning. We carefully tuned
the hyperparameters of the baseline algorithms to achieve
optimal average performance over all validation datasets. For
W-DRO, we consider the Wasserstein distance with respect to
l2âˆ’norm and set the adversarial budget as Ïµ = 2. For KL-DRO,
we choose the adversarial budget Ïµ = 2. The predictors in both
baseline DRO methods are trained by Adam optimizer with a
learning rate of 2 Ã— 10âˆ’5. Both baselines are trained for 100
epochs with a batch size of 64.
C. Experiment Results
TABLE I
TEST REGRET ON DIFFERENT DATASETS.
Dataset
Algorithms
3D-Learning
KL-DRO
W-DRO
Cutout
Gaussian
DFL
23V(Test)
(0.0001)
0.0518
0.0797
0.0682
0.0967
0.0949
0.1298
24V
(0.0961)
0.0283
0.0604
0.0828
0.0707
0.0716
0.1003
23D
(0.1459)
0.2213
0.2967
0.3087
0.3241
0.3304
0.3993
24D
(0.1011)
0.2775
0.4558
0.6266
0.4648
0.5004
0.6103
23D24D
(0.1565)
0.3140
0.5071
0.6894
0.5215
0.5643
0.6828
23D24V
(0.1357)
0.0703
0.1214
0.1680
0.1317
0.1376
0.1791
24D24V
(0.0687)
0.1814
0.3072
0.3976
0.3259
0.3471
0.4360
Average
0.1635
0.2612
0.3345
0.2765
0.2923
0.3625
Worst
0.3140
0.5071
0.6894
0.5215
0.5643
0.6828
1) Default Setting: We give a comprehensive compari-
son between 3D-Learning and different decision-focused
baseline algorithms on various testing datasets with differ-
ent distribution shifts. We evaluate the performance by the
normalized Regret(alg) = ( Â¯Ropt âˆ’Â¯Ralg)/ Â¯Ropt, where Â¯Ralg
denotes the mean performance of the algorithm on a testing
dataset and Â¯Ropt represents the optimal mean performance
on the same testing dataset. The regrets and their average
and maximum values over all testing datasets are shown
in Table I, We can find that 3D-Learning outperforms
all DRO algorithms across all datasets. Specifically, while
KL-DRO and W-DRO also achieve notable improvements over
the standard DFL method, 3D-Learning attains an average
regret of 0.1635 over all datasets, exceeding the average
performance of KL-DRO and W-DRO by 37.4% and 51.1%,


--- Page 8 ---
respectively. Furthermore, on the 23D24D dataset with the
largest distribution shift, all baselines reach the maximum
regret, but 3D-Learning achieves a maximum regret of
0.3140, surpassing KL-DRO and W-DRO by 38.0% and 54.4%,
respectively. The advantages of 3D-Learning come from
the use of diffusion model to construct the ambiguity set.
Compared to the ambiguity sets with KL divergence, diffusion-
based ambiguity set allows 3D-Learning to generate di-
verse samples beyond the support of the training distribution,
discover distributions that lead to the worst-case decision-
making performance. Meanwhile, by restricting the denoising
score matching loss by a budget Ïµ, 3D-Learning ensures
that the distributions in the ambiguity set are consistent with
the underlining data distribution. This avoids the overly-broad
relaxation of ambiguity set in the W-DRO solution and achieve
performance improvements far exceeding W-DRO. The com-
parison with DRO baselines demonstrate that 3D-Learning
with the diffusion-based ambiguity set is superior in achieving
a favorable performance balance between average and worst-
case testing environments.
In addition, in Table I, we compare 3D-Learning with
two common data augmentation methods which inject new
samples by adding Cutout or Gaussian noise. For DA with
Cutout noise, each training data point at each time step is
masked to zero with a probability of 5%, and the cutout
dataset is added to original training dataset. For DA with
Gaussian noise, a standard Gaussian noise scaled by 5% of the
maximum value of the original data point is added to each data
point. at each time step. Then, the decision-focused training
is applied on the augmented training datasets. As shown in
Table I, both data augmentation methods provide obvious
performance improvements for both average and maximum
performance compared to DFL since DA makes the training
data more diverse and so enhances the generalization perfor-
mance. Cutout augmentation performs better than Gaussian
augmentation, achieving an average Regret of 0.2765, which
represents a 23.7% improvement over DFL. However, DA
methods have a limited performance improvement especially
for the maximum regret. This is because DA methods do
not optimize for the worst-case performance and they cannot
effectively inject samples that are important for the decision
objective. By contrast, 3D-Learning effectively optimizes
for the worst-case and decision aware objectives, thus signifi-
cantly outperforming DA methods on both average and worst-
case performance.
2) Evaluation on Corrupted Datasets: In Fig. 3, we ac-
cess the performance of distributionally robust algorithms on
corrupted testing environments. We create corrupted testing
datasets by injecting Cutout, Perlin, or Gaussian noise to
the original testing dataset (23D24D) which has the largest
Wasserstein distribution discrepancy compared to the training
dataset. For Cutout noise, each per-round data point is masked
to zero with a probability of 0.5%. The added Perlin noise has
a magnitude of 5% of the maximum value of the original data
point. Standard Gaussian noise scaled by 10% of the original
maximum value is added to the original dataset. The regret ra-
tios (Regret(alg)/Regret(DFL)Ã—100 %) with the regret of DFL
as the baseline are illustrated in Fig 2. Under various noisy
perturbations, 3D-Learning achieves an average Regret
Ratio of only 46.8% of DFL, while significantly outperforming
KL-DRO and W-DRO by 27.7% and 53.0%, respectively.
These results demonstrate that by diffusion augmented DRO,
3D-Learning exhibits outstanding robustness against noisy
corruptions compared to existing methods.
3) Effects of Training Objectives: Next, we investigate
the effects of training objectives (decision-focused or MSE
training) on 3D-Learning. The MSE training replaces the
decision objective in 3D-Learning with the mean squared
prediction error. The regret ratios of both training strategies are
illustrated in Fig 3. Decision-focused training achieves, on av-
erage, a 6.1% improvement over MSE training in decision per-
formance across various datasets. However, decision-focused
training does not win for all datasets: it performs slightly
worse on the datasets 23V (Test) 24V and 23D24D. This
is because distributionally robust learning optimizes for the
worst-case distribution but not for every distinct distribution.
Nevertheless, by the end-to-end training strategy, decision-
focused training demonstrates superior performance in most
cases compared to prediction-focused training.
4) Effects of DRO Budget: We investigate the effects of
the critical budget parameter Ïµ in (9) on the performance
of 3D-Learning. As shown in Fig 4, the regret-Ïµ curves
across all datasets exhibit a concave shape, achieving the
optimal average performance at around Ïµ = 0.03. When
Ïµ falls below this threshold and continues to decrease, the
diffusion-modeled distribution becomes overly constrained to
the training data, limiting the ability of 3D-Learning to
generalize to OOD datasets. Conversely, when Ïµ exceeds this
threshold and continues to increase, 3D-Learning with an
overly large ambiguity set can conservatively over-optimize
the decision objective on irrelevant distributions, which can
result in degraded performance on real OOD datasets. There-
fore, it is critical to select an appropriate budget Ïµ to generate
effective adversarial distributions, striking a desirable trade-off
between average and worst-case performance.
TABLE II
TEST REGRET OF 3D-LE A R N I N G WITH DIFFERENT DIFFUSION MODELS
AND FINETUNING BACKWARD STEPS T â€².
Regret
DFU-128
DFU-96
DFU-64
T â€²-15
T â€²-10
T â€²-5
Average
0.1635
0.1699
0.1721
0.1547
0.1635
0.1891
Worst
0.3140
0.3300
0.3304
0.3031
0.3140
0.3815
5) Effects of Diffusion Model Setups: The regret of dif-
fusion model setups of 3D-Learning is given in Table II.
DFU-M means the diffusion model with 4-level UNet back-
bone which has M channels in the first level and 2M channels
in the other levels. T â€² âˆ’N means the last N backward
steps of DFU-128 are finetuned for inner maximization. As
shown in Table II, evaluation across all test sets shows that
a larger diffusion model can result in lower regrets because


--- Page 9 ---
Cutout
Perlin
Gaussian
Corrupted 23D24D Dataset
0
20
40
60
80
100
Regret Ratio(%)
46.7
74.8
100.9100.0
46.8
75.4
101.6100.0
47.0
73.6
97.1 100.0
3D Learning Avg
KL-DRO Avg
W-DRO Avg
DFL Avg
3D-Learning
KL-DRO
W-DRO
DFL
Fig. 2.
Robustness evaluation under diverse noisy
corruptions on 23D24D dataset.
23V(Test)
24V
23D
24D
23D24D 23D24D 24D24V
Dataset
0
10
20
30
40
50
60
70
Regret Ratio(%)
39.937.5
28.2
22.2
55.4
68.2
45.5
52.8
46.0
55.0
39.237.4
41.6
49.8
DFL Avg
MSE Avg
DFL Training
MSE Training
Fig. 3. 3D-Learning with decision-focused train-
ing and MSE training.
0.010
0.015
0.020
0.025
0.030
0.035
0.040
0.1
0.2
0.3
Regret
24V
23V
24D
23D
23D24D
23D24V
24D24D
Avg
Fig. 4.
Effect of budget Ïµ on 3D-Learning
performance.
it has higher expressive capacity to represent the adversarial
distributions. In addition, fine-tuning more backward steps
improves the performance because it provides more flexibility
to solve the inner maximization.
6) Computational Overhead: We benchmark all methods
on the above time-series task, measuring training runtime and
GPU memory on a single NVIDIA RTX 6000 Ada GPU. Dur-
ing training, DFL and DA have similar computational overhead
(runtime â‰ˆ2100 s, GPU memory < 35 MB), whereas DRO
methods are substantially more expensive: 3D-Learning
requires a runtime of 4900 s. In terms of memory consumption,
W-DRO and KL-DRO use over 400 MB of GPU memory, while
3D-Learning requires significantly more (6.8 GB) due to
the usage of diffusion model. Importantly, this overhead is
confined to training: at test time, all methods exhibit similar
inference overhead (84 s, 418 MB), indicating no additional
inference overhead for 3D-Learning.
VI. RELATED WORKS
Our work is closely related to the literature on DRO. Most
existing DRO algorithms construct ambiguity sets using either
Wasserstein distances [5], [11], [32], [51] or Ï•-divergences
[13]â€“[15], [17]. However, these approaches often involve opti-
mization over an infinite-dimensional probability space, which
poses significant computational challenges and can result in
sub-optimal solutions. Recent studies [31], [34] represent the
adversarial distributions within traditional Wasserstein-based
or KL divergence-based ambiguity sets. However, due to the
reliance on these traditional ambiguity sets, the intractabil-
ity issue or the support shift issue still exist. In contrast,
3D-Learning constructs a novel ambiguity set based on
the score-matching loss of the diffusion model, which offers
better flexibility in searching adversarial distributions.
Robust DFL is also studied in recent literature. Ma et
al. [28] introduce a differentiable parameterized Second-Order
Cone (SOC) to define the ambiguity set and propose an end-
to-end framework that trains ML to predict the ambiguity
set for the downstream DRO task. In comparison, our DR-
DFL framework is fundamentally different, as it addresses
the distributionally robustness problem during the training of
the context predictor, rather than focusing on inference-time
DRO tasks as in [28]. Moreover, 3D-Learning constructs
ambiguity sets using diffusion models, which capture diverse
adversarial distributions, thereby offering greater expressive-
ness. Wang et al. [43] proposed a Generate-then-Optimize
framework that trains a diffusion model to generate data
for downstream statistical optimization, targeting the condi-
tional value-at-risk (CVaR) objective. While closely related,
it primarily addresses risk mitigation under in-distribution
scenarios, whereas 3D-Learning is designed to enhance
robustness in out-of-distribution (OOD) environments.
VII. CONCLUSION
Our work focuses on DR-DFL which models many critical
PTO applications in networking. We propose a diffusion aug-
mented algorithm 3D-Learning to improve the OOD gen-
eralization of DR-DFL. Specifically, by leveraging diffusion-
based ambiguity modeling, 3D-Learning enables the search
of worst-case distributions in the parameterized space of
diffusion models, which achieves a good balance between
average and worst-case performances. Extensive simulation
results on resource provisioning for LLM inference confirm
the effectiveness of 3D-Learning, demonstrating substan-
tial performance gains and enhanced robustness under per-
turbations. These findings highlight the potential of diffusion
models as a powerful tool for distributionally robust, decision-
driven learning in dynamic and uncertain environments. For
future work, the diffusion-based ambiguity set can be applied
to other DRO problems.
ACKNOWLEDGMENT
Jiaqi Wen and Jianyi Yang were supported in part by
University of Houston Start-up Funds 74825 [R0512039] and
74833 [R0512042]. Lei Fan was supported by UH Energy
Transition Institute.
REFERENCES
[1] Santiago Balseiro, Haihao Lu, and Vahab Mirrokni.
Dual mirror
descent for online allocation problems.
In Hal DaumÂ´e III and Aarti
Singh, editors, ICML, volume 119 of Proceedings of Machine Learning
Research, pages 613â€“628. PMLR, 13â€“18 Jul 2020.
[2] Stephen P Boyd and Lieven Vandenberghe.
Convex Optimization.
Cambridge University Press, 2004.
[3] Tom Brown, Benjamin Mann, et al.
Language models are few-shot
learners. 33:1877â€“1901, 2020.


--- Page 10 ---
[4] RenÂ´e Caspart, Sebastian Ziegler, Arvid Weyrauch, et al.
Precise en-
ergy consumption measurements of heterogeneous artificial intelligence
workloads. In HPCC, pages 108â€“121. Springer, 2022.
[5] Ruidi Chen and Ioannis Ch. Paschalidis. Distributionally robust learning.
Foundations and TrendsÂ® in Optimization, 4(1-2):1â€“243, 2020.
[6] Philip Colangelo, Ayse K Coskun, Jack Megrue, et al. Turning ai data
centers into grid-interactive assets: Results from a field demonstration
in phoenix, arizona. arXiv preprint arXiv:2507.00909, 2025.
[7] Joshua Comden, Sijie Yao, Niangjun Chen, Haipeng Xing, and Zhenhua
Liu. Online optimization in cloud resource provisioning: Predictions,
regrets, and algorithms. SIGMETRICS, 2019.
[8] Carly Davenport, CFA Singer, N Mehta, B Lee, and J Mackay. Ai data
centers and the coming us power demand surge. PDF). Goldman Sachs.
Archived from the original (PDF) on, 26, 2024.
[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans
on image synthesis. 34:8780â€“8794, 2021.
[10] Priya Donti, Brandon Amos, and J. Zico Kolter. Task-based end-to-end
model learning in stochastic optimization. 30, 2017.
[11] Rui Gao and Anton Kleywegt.
Distributionally robust stochastic
optimization with wasserstein distance.
Mathematics of Operations
Research, 48(2):603â€“655, 2023.
[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel.
Denoising diffusion
probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F.
Balcan, and H. Lin, editors, NeurIPS, volume 33, pages 6840â€“6851.
Curran Associates, Inc., 2020.
[13] Zhaolin Hu and L Jeff Hong. Kullback-leibler divergence constrained
distributionally robust optimization. Available at Optimization Online,
1(2):9, 2013.
[14] Hisham Husain, Vu Nguyen, and Anton van den Hengel. Distributionally
robust bayesian optimization with Ï•-divergences. 2023.
[15] Ruiwei Jiang and Yongpei Guan.
Data-driven chance constrained
stochastic program. Mathematical Programming, 158(1):291â€“327, 2016.
[16] Chi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimal-
ity in nonconvex-nonconcave minimax optimization? In Hal DaumÂ´e III
and Aarti Singh, editors, ICML, volume 119 of Proceedings of Machine
Learning Research, pages 4880â€“4889. PMLR, 13â€“18 Jul 2020.
[17] Johannes Kirschner, Ilija Bogunovic, Stefanie Jegelka, and Andreas
Krause. Distributionally robust bayesian optimization. In Silvia Chi-
appa and Roberto Calandra, editors, Proceedings of the Twenty Third
AISTATS, volume 108 of Proceedings of Machine Learning Research,
pages 2174â€“2184. PMLR, 26â€“28 Aug 2020.
[18] Burak Kocuk. Conic reformulations for kullback-leibler divergence con-
strained distributionally robust optimization and applications. IJOCTA,
11(2):139â€“151, April 2021.
[19] Daniel Kuhn, Peyman Mohajerin Esfahani, Shafieezadeh Nguyen, et al.
Wasserstein distributionally robust optimization: Theory and applications
in machine learning. pages 130â€“166. 2019.
[20] Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel,
and Aravind Srinivas.
Reinforcement learning with augmented data.
33:19884â€“19895, 2020.
[21] Baolin Li, Yankai Jiang, Vijay Gadepally, and Devesh Tiwari.
Llm
inference serving: Survey of recent advances and opportunities. In 2024
IEEE HPEC, pages 1â€“8, 2024.
[22] Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O
Hero III, and Pramod K Varshney. A primer on zeroth-order optimization
in signal processing and machine learning: Principals, recent advances,
and applications. IEEE Signal Processing Magazine, 37(5):43â€“54, 2020.
[23] Zhenhua Liu, Iris Liu, Steven Low, and Adam Wierman. Pricing data
center demand response. ACM SIGMETRICS Performance Evaluation
Review, 42(1):111â€“123, 2014.
[24] Zijian Liu, Qinxun Bai, Jose Blanchet, Perry Dong, Wei Xu, Zhengqing
Zhou, and Zhengyuan Zhou.
Distributionally robust q-learning.
In
ICML, pages 13623â€“13643. PMLR, 2022.
[25] Alfonso Lobos, Paul Grigas, and Zheng Wen. Joint online learning and
decision-making via dual mirror descent. In Marina Meila and Tong
Zhang, editors, ICML, volume 139 of Proceedings of Machine Learning
Research, pages 7080â€“7089. PMLR, 18â€“24 Jul 2021.
[26] S.H. Low. A duality model of tcp and queue management algorithms,
2003.
[27] Quyuan Luo, Shihong Hu, Changle Li, Guanghui Li, and Weisong Shi.
Resource scheduling in edge computing: A survey. IEEE Communica-
tions Surveys and Tutorials, 23(4):2131â€“2165, 2021.
[28] Xutao Ma, Chao Ning, and Wenli Du. Differentiable distributionally
robust optimization layers. 2024.
[29] Jayanta Mandi, James Kotary, et al. Decision-focused learning: Foun-
dations, state of the art, benchmark and future opportunities. Journal of
Artificial Intelligence Research, 80:1623â€“1701, August 2024.
[30] Erfan Meskar and Ben Liang.
Fair multi-resource allocation with
external resource for mobile edge computing. In INFOCOM WKSHPS,
pages 184â€“189, 2018.
[31] Paul Michel, Tatsunori Hashimoto, and Graham Neubig. Modeling the
second player in distributionally robust optimization. In ICLR, 2021.
[32] Peyman Mohajerin Esfahani and Daniel Kuhn.
Data-driven distribu-
tionally robust optimization using the wasserstein metric: Performance
guarantees and tractable reformulations. Mathematical Programming,
171(1):115â€“166, 2018.
[33] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Â´IËœnigo
Goiri, Saeed Maleki, and Ricardo Bianchini.
Splitwise: Efficient
generative llm inference using phase splitting. In 2024 ACM/IEEE 51st
Annual ISCA, pages 118â€“132, 2024.
[34] Allen Z. Ren and Anirudha Majumdar. Distributionally robust policy
learning via adversarial environment generation.
IEEE Robotics and
Automation Letters, 7(2):1379â€“1386, 2022.
[35] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
Klimov. Proximal policy optimization algorithms. 2017.
[36] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion
implicit models. 2022.
[37] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum
likelihood training of score-based diffusion models.
34:1415â€“1428,
2021.
[38] Yang Song and Stefano Ermon.
Generative modeling by estimating
gradients of the data distribution. 32, 2019.
[39] Matthew Staib and Stefanie Jegelka.
Distributionally robust deep
learning as a generalization of adversarial training. In NeurIPS workshop
on Machine Learning and Computer Security, volume 3, page 4, 2017.
[40] Jovan Stojkovic, Chaojie Zhang, Â´IËœnigo Goiri, Josep Torrellas, and Esha
Choukse. Dynamollm: Designing llm inference clusters for performance
and energy efficiency, 2024.
[41] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Man-
sour. Policy gradient methods for reinforcement learning with function
approximation. 12, 1999.
[42] Shreshth Tuli, Fatemeh Mirhakimi, Samodha Pallewatta, Giuliano Za-
wad, et al.
Ai augmented edge and fog computing: Trends and
challenges. Journal of Network and Computer Applications, 216:103648,
2023.
[43] Prince Zizhuang Wang, Jinhao Liang, Shuyi Chen, Ferdinando Fioretto,
and Shixiang Zhu. Gen-dfl: Decision-focused generative learning for
robust decision making. 2025.
[44] Xinyu Wang, Yiyang Peng, and Wei Ma. An end-to-end smart predict-
then-optimize framework for vehicle relocation problems in large-scale
vehicle crowd sensing. arXiv preprint arXiv:2411.18432, 2024.
[45] Grant Wilkins, Srinivasan Keshav, and Richard Mortier.
Hybrid het-
erogeneous clusters can lower the energy consumption of llm inference
workloads. In Proceedings of the 15th ACM e-Energy, pages 506â€“513,
2024.
[46] Jiajun Wu, Chengjian Sun, and Chenyang Yang. Proactive optimization
with machine learning: Femto-caching with future content popularity,
2020.
[47] Yuqing Yang, Lei Jiao, and Yuedong Xu.
A queueing theoretic
perspective on low-latency llm inference with variable token length. In
2024 22nd WiOpt, pages 273â€“280, 2024.
[48] Zhisheng Ye, Wei Gao, Qinghao Hu, Peng Sun, Xiaolin Wang, Yingwei
Luo, Tianwei Zhang, and Yonggang Wen.
Deep learning workload
scheduling in gpu datacenters: A survey. ACM Comput. Surv., 56(6),
January 2024.
[49] Jinlei Zhang, Ergang Shan, Lixia Wu, Jiateng Yin, Lixing Yang, and
Ziyou Gao.
An end-to-end predict-then-optimize clustering method
for stochastic assignment problems. IEEE Transactions on Intelligent
Transportation Systems, 25(9):12605â€“12620, 2024.
[50] Yijia Zhang, Daniel Curtis Wilson, Ioannis Ch. Paschalidis, and Ayse K.
Coskun. Hpc data center participation in demand response: An adaptive
policy with qos assurance. IEEE Transactions on Sustainable Comput-
ing, 7(1):157â€“171, 2022.
[51] Chaoyue Zhao and Yongpei Guan. Data-driven risk-averse stochastic
optimization with wasserstein metric.
Operations Research Letters,
46(2):262â€“267, 2018.
