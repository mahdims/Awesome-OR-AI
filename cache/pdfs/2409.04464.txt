--- Page 1 ---
Leveraging Large Language Models for Solving
Rare MIP Challenges
1st Teng Wang
Department of Mathematics
University of Hong Kong
Hong Kong, China
wt0318@connect.hku.hk
2nd Wing-Yin Yu
Noah’s Ark Lab
Huawei
Hong Kong, China
rocket.YuWingYin@huawei.com
3rd Ruifeng She
Noah’s Ark Lab
Huawei
Hong Kong, China
sheruifeng@huawei.com
4th Wenhan Yang
Department of Mathematics
University of Hong Kong
Hong Kong, China
u3621353@connect.hku.hk
5th Taijie Chen
Department of Civil Engineering
University of Hong Kong
Hong Kong, China
ctj21@connect.hku.hk
6th Jianping Zhang
Noah’s Ark Lab
Huawei
Hong Kong, China
zhang.jianping4@huawei.com
Abstract—Mixed Integer Programming (MIP) has been exten-
sively applied to areas requiring mathematical solvers to address
complex instances within tight time constraints. However, as the
problem scale increases, the complexity of model formulation and
finding feasible solutions escalates significantly. Beneficial from
outstanding text generation capacity of Large Language Models
(LLMs), building and solving industrial-level instances becomes
insensitive to problem scale. While LLMs, like GPT-4, can handle
some traditional medium-scale MIP problems, they struggle with
uncommon or highly specialized MIP scenarios. Fine-tuning
LLMs can yield some feasible solutions for medium-scale MIP
instances, but these models typically fail to explore diverse
solutions when constrained by a low and constant temperature.
In this paper, we propose and evaluate a recursively dynamic
temperature method integrated with a chain-of-thought approach
to exploit a large feasible region. Our findings show that starting
with a high temperature and gradually lowering it leads to
better feasible solutions compared to other dynamic temperature
strategies. Additionally, by comparing results generated by the
LLM with those from Gurobi, we demonstrate that the LLM
can produce solutions that complement traditional solvers by
accelerating the pruning process and improving overall efficiency.
Index Terms—Mixed integer programming, Chain-of-Thought
(CoT), dynamic temperature, ride pooling, bipartite matching.
I. INTRODUCTION
Mixed Integer Programming (MIP) is a fundamental tool in
many optimization domains, such as the Traveling Salesman
Problem (TSP) [1] and facility location planning [2]. MIP also
plays a particularly critical role in time-sensitive applications
like transportation and network scheduling [3], where finding
a feasible solution within a short time frame is essential to
maintaining system operability and avoiding downtime.
The traditional approach to solve MIP problems is the
branch-and-bound (B&B) algorithm [4]. While this method
guarantees to find the optimal solution for a given instance,
the efficiency of mathematical solvers that use such method
like Gurobi [5] diminishes as the problem scale increases
[6]. Moreover, the complexity of model formulation grows
significantly with the dimensionality of the problem. For
example, the growth rate of the complexity of a 3D bin-
packing problem [7] is considerably higher than that of a 2D
bin-packing problem [8].
To expedite the search for optimal solutions, mathematical
solvers implement various techniques such as heuristics, cut-
ting planes, parallelism, presolve [9]. However, despite these
advanced methods, solvers still face challenges in efficiently
handling large-scale MIP problems within tight time con-
straints. Large language models (LLMs), with their strong
pattern recognition capabilities, can achieve similar objectives
with only minimal data and modeling information. For in-
stance, Yang et al. [10] pioneered the application of Chain-of-
Thought (CoT) reasoning [11] in large language models such
as GPT-3.5 [12] and GPT-4 [13] to address problems like the
TSP using only the coordinates of cities, without explicitly
requiring distances between each pair of cities. This approach
reduces the time complexity from O(n2), typically required
for distance calculations in traditional mathematical solvers,
offering a more efficient solution.
However, the previous work by Yang et al. [10] has sev-
eral drawbacks. First, the instance data is generated from
randomly sampled integers, which may reduce its validity
as a demonstration of the LLMs capabilities in real-world
MIP applications. Second, the TSP is a well-known and
extensively studied problem, meaning LLMs have been trained
on similar data and the same TSP model numerous times. In
our experiments, we observed that LLMs often struggle with
complicated mathematic models and frequently fail to grasp
the MIP modeling process. These factors raise concerns about
the robustness of LLMs in real-world applications.
Our work focuses on how to integrate LLMs into real-
world applications. To demonstrate the generalizability of
LLMs in real-world scenarios, we developed a fine-grained
simulator and utilized the operational dataset provided by
DiDi in November 2016 [14] to simulate the passenger-
arXiv:2409.04464v2  [cs.CL]  18 Sep 2024


--- Page 2 ---
driver matching process in the ride-pooling market using MIP.
Our work is divided into three main components: (1) We
construct a carpooling MIP model based on real-world data
while capturing and storing vehicle locations, order locations,
MIP instances, and intermediate feasible solution statuses for
future training purposes. (2) Leveraging the pattern recognition
capabilities of LLMs and CoT reasoning, we generate prompts
using only abstract information from the carpooling dataset,
bypassing the need to compute MIP parameters, like the
distance between the vehicle and the user’s order, explicitly.
We then perform supervised fine-tuning on LLaMA 3.1 (8B)
[15] to discover better feasible solutions, comparing these
with the top three feasible solutions generated by traditional
mathematical solvers. The results from LLM can be used to
accelerate the pruning process in conventional mathematical
solvers. (3) We employ recursive dynamic temperature ad-
justments to refine the quality of feasible solutions generated
by the LLM. Through a strategy of starting at a higher
temperature and gradually reducing it, we observe significant
improvements in solution quality. By systematically evaluating
performance under various temperature schedules, we identify
the highly effective strategy for enhancing the effectiveness
and consistency of the solutions produced.
II. RELATED WORK
MIP plays a crucial role in combinatorial optimization, with
applications in planning [2], scheduling [16], and routing [17].
Traditional methods like B&B [4] have been widely used to
solve MIP problems. However, these methods can be compu-
tationally intensive, leading to growing interest in enhancing
MIP solvers with machine learning (ML) and LLMs.
Recent works integrating ML with MIP can be categorized
into two main approaches [18]: exact algorithms and heuristic
algorithms. For exact methods like B&B, ML models have
been used to optimize branching variable selection and node
selection, significantly improving solution efficiency [19] [20].
On the heuristic side, techniques like Large Neighborhood
Search and Feasibility Pump have benefited from ML inte-
gration, leading to higher solution quality and computational
efficiency [21] [22]. Additionally, Graph Neural Networks
have been leveraged to represent MIP instances, enhancing
decision-making processes like branching and node selection
[19]. Reinforcement learning is also increasingly applied in
both exact and heuristic methods to support adaptive decision-
making within the B&B framework [23].
With the advent of LLMs and the rise of AI agents,
more research has focused on translating natural language
into operations research problems [24]–[26]. Although zero-
shot learning typically performs poorly on complex problems,
LLMs have significant potential, and their performance can be
improved through techniques like the chain of thought [11],
tree of thought [27], and self-consistency [28]. Yang et al.’s
work [10] utilizes models like PaLM [29] and GPT-4 [13]
to tackle linear regression and the TSP with CoT reasoning,
demonstrating success on small-scale problems. Our work
fine-tunes the LLaMA 3.1 (8B) model [15] using both model
Fig. 1.
The time cost of building the model increases significantly as
the problem scale grows. This trend illustrates the growing computational
complexity associated with larger problem instances.
information and real MIP instance data capable of generating
feasible solutions, and proposes an adaptive temperature strat-
egy that iteratively enhances LLM performance, leading to the
generation of even more optimized feasible solutions.
III. METHOD
Given that LLMs have been trained on numerous traditional
MIP problems, such as the TSP, and considering the limitations
in the generalizability of previous work due to the use of
non-real-world data, we aim to assess the potential of LLMs
in MIP under real-world conditions. To achieve this, we
construct a carpooling MIP model and develop a simulator to
replicate the vehicle dispatching process. The DiDi operational
dataset, which consists of the location and time of orders from
November 2016, serves as the foundation for generating real-
world data in this study. The input is a long text containing
information about the locations of orders and the positions
of various categories of vehicles, while the output is feasible
solutions for dispatching these vehicles to different users.
A. Problem Statement
There are two types of vehicles: (1) empty vehicles and (2)
vehicles with one passenger. Additionally, we assume that each
order is associated with a single customer and that a vehicle
can accommodate at most two passengers at a time. Our
goal is to minimize the total distance traveled for picking up
customers, subject to the above constraints. Simplify the model
by calculating Manhattan distances when formulating the MIP.
However, in the simulation process, Dijkstra’s algorithm is
employed to simulate vehicle movement.
The notation is as follows:
- xij is a decision variable indicating whether empty car i is
assigned to user j.
- yijk is a decision variable indicating whether empty car i is
assigned to pick up user j and then user k.
- zij is a decision variable indicating whether car i with one
passenger willing to share is assigned to user j.
- dij is the distance between vehicle i and user j.


--- Page 3 ---
Fig. 2. Workflow of training and inference, showing the recursive approach
where the temperature starts high to explore a broad solution space and
gradually decreases to refine and improve solution quality.
- d′
jk is the distance between user j and user k.
- d′′
ij is the distance between vehicle i (with one order) and
user j.
- m is the number of empty vehicles.
- n is the number of vehicles with one order.
- p is the number of orders.
1. Objective Function
The objective is to minimize the total distance between
vehicles and passengers across all segments of the vehicle
paths:
min
n1
X
i=1
m
X
j=1
xij·dij+
n1
X
i=1
m
X
j=1
m
X
k=1,k̸=j
yijk·(dij+d′
jk)+
n2
X
i=1
m
X
j=1
zij·d′′
ij
2. Constraints
Order Coverage: Each user is assigned to exactly one
vehicle:
X
i
xij +
X
i
X
k,j̸=k
(yijk + yikj) +
X
i′
zi′,j = 1, ∀j
Vehicle Capacity for Empty Vehicles: Each empty vehicle
picks up at most two people:
X
j
xij +
X
j
X
k̸=j
yijk ≤1, ∀i
Vehicle Capacity for Shared Rides: Each vehicle with one
passenger picks up at most one more person:
X
j
zij ≤1, ∀i
Binary Variable Constraints:
xij, yijk, zi,j ∈{0, 1}, ∀i, j, k
3. Analysis of the Constraint Matrix
As discussed in the previous constraints part, Figure 1 illus-
trates the relationship between the model-building time and the
number of sets. The shape of the constraint matrix is given by:
(m + n + p, m · p + m · p2 + n · p)
The complexity of the constraint matrix increases signifi-
cantly as the size of the set grows, resulting in a very high
Fig. 3. The gap, defined as the difference between the feasible and optimal
solution, shows that a larger gap means worse performance. After fine-tuning,
LLaMA 3.1 8B generates feasible solutions with a smaller gap than solver’s
first three solutions, especially as MIP instance scale grows.
level of computational complexity. This makes it infeasible
to solve the problem instantly if the size of the set increases
significantly.
B. Data Collection and Prompt Generation
Prior to fine-tuning the LLMs, we first identify the positions
of the orders and the various types of vehicles. The next step
is to generate the labels, which consist of the intermediate fea-
sible solution and the optimal solution for this MIP instance.
1) Data Storage: When the simulator constructs and solves
the MIP instances using the Gurobi solver, we capture several
critical pieces of information that include (1) the location of
every vehicle and order, (2) the status of intermediate feasible
solutions, and (3) the optimal solution.
2) Prompt Generation: Following the collection of raw and
intermediate data, we compile the LaTeX code for the model-
building process, the details of each vehicle and order scenario,
and both feasible and optimal solutions into a text format. This
information is then used to generate the desired prompt for
subsequent training and inference processes.
C. Recursive CoT with dynamic temperature
Since LLMs, after fine-tuning, exhibit a strong ability to
grasp problem patterns, they often produce the same feasi-
ble solution at lower temperatures—even when the prompt
suggests that this solution isn’t optimal. Despite recursive
adjustments from lower temperatures to higher temperatures
intended to explore a broader solution space, LLMs may
still become trapped in the previous bad solutions. Thus, a
temperature strategy is needed to effectively explore diverse
possibilities and enhance solution quality.
To address this, we employ a recursive approach with
dynamic temperature to leverage CoT effectively. Our strategy
involves initially generating feasible solutions with a high
temperature to explore a broader solution space. We then
iteratively refine these solutions by gradually lowering the
temperature. This dynamic temperature adjustment begins with


--- Page 4 ---
a high temperature to facilitate exploration and progressively
decreases to a low temperature to focus on refinement. This
balanced approach helps us improve solution diversity and
quality, ultimately leading to better feasible solutions. Figure
2 illustrates the entire workflow of training and inference.
IV. EXPERIMENT
Using LLMs to retrieve the exact optimal solution for
medium and large-scale MIP instances is currently impractical.
However, due to their strong pattern recognition capabilities,
fine-tuned LLMs can provide satisfactory feasible solutions
that serve as upper bounds for minimization problems.
We aim to explore the potential of LLMs in this context.
If LLMs can provide satisfactory feasible solutions during the
model-building process, or if traditional solvers like Gurobi
face challenges in finding feasible solutions for large-scale
instances, these LLM-generated solutions could serve as valu-
able upper bounds to accelerate the pruning process. By
comparing the solutions generated by LLMs with the top
three feasible solutions produced by traditional mathematical
solvers, we can potentially leverage LLM-generated solutions
to enhance pruning strategies and improve overall solver
efficiency.
A. Comparision with Gurobi
We fine-tuned the LLaMA-3.1-Instruct-8B model for the
following experiment and split 10% of the total instances in
the generated dataset, which contains 12,500 MIP instances,
as our test dataset.
We compare the best feasible solution obtained from the
LLM using three recursive calls, where the temperature grad-
ually decreases from 1 to 0.1 to 0.01, against the first three
feasible solutions generated by GUROBI [5], CPLEX [30], and
COPT [31] to evaluate which approach achieves a smaller gap.
The gap is calculated using the formula:
gap = current objective value −optimal value
current objective value
Figure 3 illustrates the relationship between the scale of the
MIP instance and the gap observed from LLMs, GUROBI,
CPLEX, and COPT. The fine-tuned LLaMA 3.1 (8B) model
is able to generate feasible solutions that are much closer to the
optimal solution compared to the first three feasible solutions
generated by traditional mathematical solvers.
B. Ablation
To demonstrate the effectiveness of the temperature adjust-
ment strategy from higher to lower in enhancing the perfor-
mance of our fine-tuned LLaMA 3.1 8B model, we conduct
a comparison across several scenarios. These include cases
(1) where the temperature initially rises from 0.01 to 1 and
then falls back to 0.01 recursively, (2) where the temperature
remains constant at 0.01 during recursive calls, (3) where a
single temperature setting of 0.01 is used, and (4) where the
temperature progressively rises from 0.01 to 0.1 to 1 with each
TABLE I
AVERAGE SCORES FOR DIFFERENT TEMPERATURE STRATEGIES
Strategy
Average Score
Single Temperature Call
0.560
Constant Temperature
0.732
Temperature Rise Then Fall
0.756
Temperature Rise
0.813
Temperature Fall
0.840
Fig. 4.
The solution quality score represents the percentage of feasible
solutions generated by LLMs that have a smaller gap compared to the Gurobi
solver, which shows a larger score means better performance.
recursion. This comparative analysis highlights the impact
of each strategy on the model’s ability to generate optimal
feasible solutions.
The comparison results for medium-scale MIP instances are
shown in Figure 4. The solution quality score represents the
percentage of feasible solutions generated by LLMs that have a
smaller gap compared to the Gurobi solver. The average score
in the test dataset is illustrated in Table I. The optimal approach
involves using a high temperature to encourage LLMs to
explore a broader range of possibilities, which helps prevent
them from getting trapped in specific nodes. Subsequently,
lowering the temperature exploits and refines the better results,
allowing for improved solution quality.
V. CONCLUSION
Our study evaluates the potential of LLMs to address
unknown MIP models and uses the carpooling dispatch case
to demonstrate how LLMs can enhance efficiency in finding
better feasible solutions. This, in turn, can expedite tradi-
tional mathematical solvers’ processes by pruning unnecessary
nodes. We also examine the effectiveness of various temper-
ature management strategies for fine-tuned LLama-3.1-8B in
solving medium-scale MIP instances. The comparison results
reveal that different temperature management approaches sig-
nificantly influence the quality of feasible solutions obtained
by the model. Starting with a high temperature to explore
more nodes and then exploiting better solutions by gradually
lowering the temperature improves the quality of feasible
solutions generated by LLMs.


--- Page 5 ---
REFERENCES
[1] Gilbert Laporte,
“The traveling salesman problem: An overview of
exact and approximate algorithms,” European Journal of Operational
Research, vol. 59, no. 2, pp. 231–247, 1992.
[2] Andreas Klose and Andreas Drexl,
“Facility location models for
distribution system design,” European journal of operational research,
vol. 162, no. 1, pp. 4–29, 2005.
[3] Fang He, Jie Yang, and Meng Li, “Vehicle scheduling under stochastic
trip times: An approximate dynamic programming approach,” Trans-
portation Research Part C: Emerging Technologies, vol. 96, pp. 144–
159, 2018.
[4] Eugene L Lawler and David E Wood, “Branch-and-bound methods: A
survey,” Operations research, vol. 14, no. 4, pp. 699–719, 1966.
[5] Tobias Achterberg,
“What’s new in gurobi 9.0,”
Webinar Talk
url: https://www. gurobi. com/wp-content/uploads/2019/12/Gurobi-90-
Overview-Webinar-Slides-1. pdf, vol. 5, no. 9, pp. 97–113, 2019.
[6] Josef Jablonsk`y et al., “Benchmarks for current linear and mixed integer
optimization solvers,” Acta Universitatis Agriculturae et Silviculturae
Mendelianae Brunensis, vol. 63, no. 6, pp. 1923–1928, 2015.
[7] Silvano Martello, David Pisinger, and Daniele Vigo,
“The three-
dimensional bin packing problem,” Operations research, vol. 48, no.
2, pp. 256–267, 2000.
[8] David S Johnson,
“Fast algorithms for bin packing,”
Journal of
Computer and System Sciences, vol. 8, no. 3, pp. 272–314, 1974.
[9] Ralph E Gomory, Outline of an algorithm for integer solutions to linear
programs and an algorithm for the mixed integer problem, Springer,
2010.
[10] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le,
Denny Zhou, and Xinyun Chen, “Large language models as optimizers,”
arXiv preprint arXiv:2309.03409, 2023.
[11] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.,
“Chain-of-thought
prompting elicits reasoning in large language models,”
Advances in
neural information processing systems, vol. 35, pp. 24824–24837, 2022.
[12] Tom B Brown, “Language models are few-shot learners,” arXiv preprint
ArXiv:2005.14165, 2020.
[13] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
Sam Altman, Shyamal Anadkat, et al., “Gpt-4 technical report,” arXiv
preprint arXiv:2303.08774, 2023.
[14] Huaxiu Yao, Fei Wu, Jintao Ke, Xianfeng Tang, Yitian Jia, Siyu Lu,
Pinghua Gong, Jieping Ye, and Zhenhui Li, “Deep multi-view spatial-
temporal network for taxi demand prediction,” in Proceedings of the
AAAI conference on artificial intelligence, 2018, vol. 32.
[15] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,
Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy
Yang, Angela Fan, et al., “The llama 3 herd of models,” arXiv preprint
arXiv:2407.21783, 2024.
[16] Hegen Xiong, Shuangyuan Shi, Danni Ren, and Jinjin Hu, “A survey
of job shop scheduling problem: The types and models,” Computers &
Operations Research, vol. 142, pp. 105731, 2022.
[17] Kris Braekers, Katrien Ramaekers, and Inneke Van Nieuwenhuyse, “The
vehicle routing problem: State of the art classification and review,”
Computers & industrial engineering, vol. 99, pp. 300–313, 2016.
[18] Jiayi Zhang, Chang Liu, Xijun Li, Hui-Ling Zhen, Mingxuan Yuan,
Yawen Li, and Junchi Yan,
“A survey for solving mixed integer
programming via machine learning,”
Neurocomputing, vol. 519, pp.
205–217, 2023.
[19] Maxime Gasse, Didier Ch´etelat, Nicola Ferroni, Laurent Charlin, and
Andrea Lodi, “Exact combinatorial optimization with graph convolu-
tional neural networks,”
Advances in neural information processing
systems, vol. 32, 2019.
[20] Elias Khalil, Pierre Le Bodic, Le Song, George Nemhauser, and Bistra
Dilkina,
“Learning to branch in mixed integer programming,”
in
Proceedings of the AAAI conference on artificial intelligence, 2016,
vol. 30.
[21] Jialin Song, Yisong Yue, Bistra Dilkina, et al.,
“A general large
neighborhood search framework for solving integer linear programs,”
Advances in Neural Information Processing Systems, vol. 33, pp. 20012–
20023, 2020.
[22] Meng Qi, Mengxin Wang, and Zuo-Jun Shen,
“Smart feasibility
pump: Reinforcement learning for (mixed) integer programming,” arXiv
preprint arXiv:2102.09663, 2021.
[23] Yunhao Tang, Shipra Agrawal, and Yuri Faenza,
“Reinforcement
learning for integer programming: Learning to cut,” in International
conference on machine learning. PMLR, 2020, pp. 9367–9376.
[24] Ziyang Xiao, Dongxiang Zhang, Yangjun Wu, Lilin Xu, Yuan Jessica
Wang, Xiongwei Han, Xiaojin Fu, Tao Zhong, Jia Zeng, Mingli Song,
et al., “Chain-of-experts: When llms meet complex operations research
problems,”
in The Twelfth International Conference on Learning
Representations, 2023.
[25] Ali AhmadiTeshnizi, Wenzhi Gao, and Madeleine Udell,
“Optimus:
Optimization modeling using mip solvers and large language models,”
arXiv preprint arXiv:2310.06116, 2023.
[26] Teng Wang, Zhenqi He, Wing-Yin Yu, Xiaojin Fu, and Xiongwei Han,
“large language models are good multi-lingual learners : when llms meet
cross-lingual prompts,” arXiv preprint arXiv:2409.11056, 2024.
[27] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths,
Yuan Cao, and Karthik Narasimhan,
“Tree of Thoughts: Deliberate
problem solving with large language models,” 2023.
[28] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan
Narang, Aakanksha Chowdhery, and Denny Zhou,
“Self-consistency
improves chain of thought reasoning in language models,” arXiv preprint
arXiv:2203.11171, 2022.
[29] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,
Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,
Charles Sutton, Sebastian Gehrmann, et al., “Palm: Scaling language
modeling with pathways,” Journal of Machine Learning Research, vol.
24, no. 240, pp. 1–113, 2023.
[30] CPLEX User’s Manual, “Ibm ilog cplex optimization studio,” Version,
vol. 12, no. 1987-2018, pp. 1, 1987.
[31] Dongdong Ge, Qi Huangfu, Zizhuo Wang, Jian Wu, and Yinyu Ye, “Car-
dinal optimizer (copt) user guide,” arXiv preprint arXiv:2208.14314,
2022.
