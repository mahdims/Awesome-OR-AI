--- Page 1 ---
READY: Reward Discovery for Meta-Black-Box Optimization
Zechuan Huang 1 Zhiguang Cao 2 Hongshu Guo 1 Yue-Jiao Gong 1 Zeyuan Ma 1
Abstract
Meta-Black-Box Optimization (MetaBBO) is an
emerging avenue within Optimization commu-
nity, where algorithm design policy could be
meta-learned by reinforcement learning to en-
hance optimization performance. So far, the re-
ward functions in existing MetaBBO works are
designed by human experts, introducing certain
design bias and risks of reward hacking. In this
paper, we use Large Language Model (LLM) as
an automated reward discovery tool for MetaBBO.
Specifically, we consider both effectiveness and
efficiency sides. On effectiveness side, we borrow
the idea of evolution of heuristics, introducing
tailored evolution paradigm in the iterative LLM-
based program search process, which ensures con-
tinuous improvement. On efficiency side, we ad-
ditionally introduce multi-task evolution archi-
tecture to support parallel reward discovery for
diverse MetaBBO approaches. Such parallel pro-
cess also benefits from knowledge sharing across
tasks to accelerate convergence. Empirical results
demonstrate that the reward functions discovered
by our approach could be helpful for boosting ex-
isting MetaBBO works, underscoring the impor-
tance of reward design in MetaBBO. We provide
READY’s project at https://anonymous.
4open.science/r/ICML_READY-747F.
1. Introduction
Meta-Black-Box Optimization (MetaBBO) has recently
emerged as a transformative paradigm for solving com-
plex optimization problems where gradient information
is unavailable.(Yi & Qu, 2023; Ma et al., 2025b; Zhao
et al., 2023; Yang et al., 2025b) It adopts a bi-level meta-
learning paradigm, where a neural network-based pol-
icy(e.g.,RL (Sutton et al., 1998)) is maintained at the meta-
1School of Computer Science and Engineering, South China
University of Technology, Guangzhou, China 2School of Com-
puting and Information Systems, Singapore Management Uni-
versity, Singapore, Singapore. Correspondence to: Zeyuan Ma
<scut.crazynicolas@gmail.com >.
Preprint. January 30, 2026.
level instructs a BBO optimizer to optimize the target prob-
lem. The reward function translates the optimizer’s envi-
ronmental interactions into feedback signals. Since this
signal directly influences the performance and generalizabil-
ity of RL (Lu et al., 2025; Oh et al., 2025), the design of
reward is of great significance. However, with a complex
bi-level structure and an uncertain optimization landscape,
MetaBBO poses significant challenges for designing effec-
tive rewards. Current MetaBBO methods mainly rely on
human expertise to craft simple heuristics, and few stud-
ies have investigated the efficacy and rationality of reward
functions (Nguyen et al., 2025).
This manual dependency severely impedes the scalability
and advancement of MetaBBO. Developing new architec-
tures necessitates laboriously crafting rewards, typically
by adapting insights from existing methods. Moreover,
adapting models to diverse real-world scenarios requires
manual tuning to align rewards with specific problem ob-
jectives. Furthermore, without fully unlocking the potential
of each method via optimized reward functions, fair per-
formance comparisons become nearly impossible. Current
hand-crafted rewards are often far from optimal, which
obscures the true capabilities of various MetaBBO architec-
tures and complicates further fine-tuning. Consequently, we
anticipate the development of an automated reward discov-
ery framework for MetaBBO.
Recently, Large Language Models (LLMs) have revolution-
ized automated program search. Pioneering frameworks
such as FunSearch (Romera-Paredes et al., 2024), EoH (Liu
et al., 2024b), and AlphaEvolve (Novikov et al., 2025) have
established a new paradigm where LLMs function as intelli-
gent evolutionary operators to discover verifiable mathemati-
cal programs. This success has extended to reward engineer-
ing, where Eureka (Ma et al., 2023) and CARD (Sun et al.,
2025) demonstrate the potential to generate human-level
rewards for control tasks via simple, end-to-end pipelines.
Following the paradigm, we propose READY, a framework
where LLMs autonomously synthesize ready-to-deploy re-
ward functions for diverse MetaBBO. Specifically, we pro-
pose following designs: 1) Multitask Program Evolution:
to address the development bottlenecks we mentioned above,
we introduce a multitask evolution paradigm into existing
program evolution methods. We deploy multiple reward
1
arXiv:2601.21847v1  [cs.LG]  29 Jan 2026


--- Page 2 ---
READY: Reward Discovery for Meta-Black-Box Optimization
program populations for multiple MetaBBO tasks, lever-
age their shared methodology insights and architecture to
accelerate the search progress; 2) Fine-grained Evolution
Operators: we propose five reflection-based code-level evo-
lutionary operators to assure diverse program search behav-
ior; 3) Knowledge Transfer: an explicit knowledge transfer
scheme is used to enhance knowledge sharing across dif-
ferent MetaBBO tasks. By using such multitask paradigm,
READY is capable of automatically designing desired re-
ward functions for diverse MetaBBO tasks, and more im-
portantly, advancing the performance frontier of MetaBBOs.
Empirical results demonstrate that: 1) READY consistently
improves diverse MetaBBO’s optimization performances,
showing advantage against not only their original reward de-
signs but also up-to-date baselines such as Eureka, EoH and
ReEvo. 2) READY provides clear and interpretable design
insights that co-evolve with the reward program; 3) Sur-
prisingly, we found reward functions designed by READY
for a given MetaBBO method could be directly adapted to
boost unseen MetaBBO method. Our key contributions are
summarized as follows:
• We propose READY, which is not only a very first
exploration on LLM-based reward design field, but
also the first framework to enhance MetaBBO’s perfor-
mance by automated reward design.
• READY is composed by several key designs: 1) mul-
titask paradigm to facilitate knowledge sharing and
search effectiveness; 2) fine-grained evolution opera-
tors to assure searching diversity; 3) explicit knowledge
transfer scheme to accelerate convergence.
• Experimental results validate that READY shows su-
perior design capability to representative baselines
with interpretable design insights. Surprisingly, the
searched reward designs could be generalized to un-
seen MetaBBO tasks.
2. Related Works
2.1. MetaBBO
Meta-Black-Box Optimization (MetaBBO) aims to au-
tomating the design in BBO algorithms by learning-driven
paradigm (Ma et al., 2025b; Li et al., 2024; Yang et al.,
2025b; Kimiaei & Kungurtsev, 2025). As illustrated in
Figure 1, an algorithm design task T
= {D, O, π} is
formalized as a Markov Decision Process (MDP), where
D = {Dtrain, Dtest} is the problem distribution containing
a training set and a testing set, O is the low-level BBO
optimizer and π is the meta-level policy. Given a problem
instance f ∈Dtrain, for each training optimization step t, an
optimization state st ∈S is extracted from the low-level
optimization process. The meta-level policy π then outputs
Meta-Objective
Reward
Accumulate     T steps
Meta-Level Policy
Meta-learning
State
Action
BBO Optimizer
Problem
Optimize
Calculate
Lower Level
Meta Level
Design Optimizer
Observe State Features
t=t+1
Figure 1. General workflow of MetaBBO approaches.
algorithm design ωt ∈Ωas the action according to st. O
optimizes f by ωt for one step. A reward function R is used
to evaluate the performance gain obtained by this algorithm
design action. The training objective is to maximize the
expected accumulated reward:
J(π) = Ef∼Dtrain
" T
X
t=0
γtR(st, ωt, f)
#
.
(1)
The performance score is commonly evaluated on Dtest by a
normalized objective indicator F which is computed as:
F =
1
|Dtest|
|Dtest|
X
i=1
 
Γ
median
j=1
y(G)
i,j −y∗
i
y(0)
i,j −y∗
i
!
,
(2)
where Γ is the number of test runs, y(0)
i,j and y(G)
i,j denote the
initial and final objective values of the j-th run on instance
fi ∈Dtest. A smaller F is better.
The algorithm design tasks MetaBBO could address is quite
broad, ranging from operator/algorithm selection/configura-
tion (Guo et al., 2024; 2025b; Sharma et al., 2019; Sun et al.,
2021) to algorithm generation (Guo et al., 2025c; Chen et al.,
2024; Zhao et al., 2024; Ma et al., 2026). Considering the
target optimization problem categories, MetaBBO has been
instantiated to single-objective (Tan & Li, 2021; Han et al.,
2025), multimodal (Lian et al., 2024), multi-objective (Tian
et al., 2025; Zhang et al., 2026), constrained (Li et al., 2026;
Hu et al., 2023) and expensive (Shao et al., 2025; Yao et al.,
2025b) optimization. Given its swift development speed, au-
tomated reward design becomes more and more important.
2.2. Automated Reward Discovery via LLMs
Building
upon
the
success
of
automated
program
search (Romera-Paredes et al., 2024; Novikov et al., 2025;
Liu et al., 2024b), pioneering explorations have begun to
extend LLM capabilities to reward discovery. For instance,
Eureka (Ma et al., 2023) employs an evolutionary search
with LLMs to synthesize reward functions by iteratively
refining code based on environmental feedback. Similarly,
Text2Reward (Xie et al., 2023) utilizes LLMs to decompose
2


--- Page 3 ---
READY: Reward Discovery for Meta-Black-Box Optimization
Algorithm 1 READY Overall Optimization Procedure
1: Input: Set of tasks {T1, . . . , TK}, Max generations
Gmax, Niche size N
2: Output: Optimal reward set I∗= {I∗
1, . . . , I∗
K}
3: // Initialization:
4: Initialize niches {P1, . . . , PK} (Section 3.2).
5: for generation g = 1 to Gmax do
6:
for each niche Pk ∈{P1, . . . , PK} do
7:
// Step 1: Offspring Generation
8:
Generate offspring set Ok (Section 3.3).
9:
// Step 2: Evaluation
10:
for each offspring I ∈Ok do
11:
Calculate normalized fitness FI after train-test.
12:
end for
13:
// Step 3: Survival Selection
14:
Update Pk by selecting N individuals from Ok
(Section 3.4).
15:
end for
16:
// Step 4: Knowledge Transfer
17:
Analyze and decide source and target niches.
18:
Generate the transferred individual to replace the
worst reward of the target niche (Section 3.5).
19: end for
complex semantic specifications into executable sub-reward
structures, while CARD (Sun et al., 2025) introduces a
self-curated feedback loop to align generated rewards with
task objectives via automated dynamic feedback. Following
them, multiples works on the reward discovery emerge. (Li
et al., 2025b;a; Heng et al., 2025; Su et al., 2026; Hazra
et al., 2024; Yao et al., 2025a; Li et al., 2025c; Cardenoso &
Caarls, 2025; Wei et al., 2025). Unlike classic RL scenarios,
MetaBBO shows complexity due to its bi-level framework
and uncertainty in the low-level optimization. These chal-
lenges motivates this work.
3. Methodology
3.1. Problem Formulation
Suppose you are a student or researcher who study
MetaBBO, there are two design aspects related to reward
design you may consider: 1) refer to reward designs in
existing MetaBBO methods and redesign proper one for
your MetaBBO method; 2) find out which MetaBBO on
earth is the nest-performing one by boosting them with
optimal reward setting. In fact, both aspects could be for-
mulated as multitask reward discovery problems. Given
a set of algorithm design tasks {Tk}K
k=1, for each task
Tk = (Dk, Ok, πk), the optimization finds the reward R
that minimizes the performance cost J on each task Tk:
R∗
k = arg min
R∈Ω
F(R|Tk),
(k = 1 . . . K).
(3)
Here, Ωis the discrete sourcecodes space of reward func-
tions and F(R|Tk) denotes the normalized performance
score we defined in Eq. (2).
Searching in discrete space Ωis intractable. LLMs have
inherent advantage in code-level generation and reason-
ing (Yang et al., 2025a). Inspired by this, our READY
leverages LLM-based program evolution to automatically
search feasible or even optimal reward design for MetaBBO
tasks, with the minimal expertise and labor input required.
We present the overall workflow of READY in Algorithm 1,
where a multi-task niche-based architecture is deployed to
solve Eq. (3). We initialize a separate population (niche)
for each MetaBBO task (Line 4), and let these populations
go though intra-population evolution and inter-population
knowledge transfer (Line 8). Through iterative interplay
with LLMs by these searching operations, READY reflec-
tively propose, analyze and refine reward designs for each
MetaBBO tasks. We detail specific designs in READY in
the following sections.
3.2. Initialization with Greedy Sampling
3.2.1. NICHE SPECIFIC METADATA MODELING
To let the general LLMs be aware of the context of our re-
ward function search tasks, we have to prepare structured
information that reflects both the algorithmic and program-
ming context (termed as metadata in this paper) of each
MetaBBO task Tk, just like the prior works (Heng et al.,
2025). Specifically, we prepare following two kind of con-
text information:
Algorithm Concept: We feed LLMs with the original re-
search paper of the MetaBBO task Tk, and prompt LLMs
for summarization of its major motivations, architectures,
algorithmic designs. The summarized metadata provides
concise description on Tk and is termed as Calg.
Programming Interface: To prevent the LLMs generate
invalid reward functions due to hallucination, and more im-
portantly, to let the LLMs fully aware of the sourcecodes
structure that could be used to design a valid reward function,
we feed them with Calg and the implementation sourcecodes
of Tk to analyze usable what variables, functions, classes,
storing them into a unified dictionary. We trem this metadata
as Ccode. Finally, the overall metadata Mk is formulated as
the union of Calg and Ccode, which will be frequently used
in the subsequent evolutionary/knowledge transfer opera-
tors, which helps the designed reward aligns with both the
algorithmic and code aspects of Tk.
3.2.2. GENE ENCODING
To harness the reasoning capabilities of LLMs, we de-
couple the code generation process like EoH (Liu et al.,
2024b).
Each individual is represented as a pair I =
3


--- Page 4 ---
READY: Reward Discovery for Meta-Black-Box Optimization
⟨Thought, Code⟩, where Thought provides the natural lan-
guage description of the reward, summarized by the LLM.
Code is the reward function code.
3.2.3. GREEDY NICHE-BASED POPULATION SAMPLING
Each niche Pk has N individuals and is explicitly bound
to a corresponding metadata Mk. Population initialization
is conducted independently for each niche following niche
specific metadata. Instead of random sampling, this pro-
cess constructs the population sequentially through three
integrated steps to ensure robust starting points:
• Expert Anchoring: Each niche is explicitly initialized
with the human-designed reward individual Iexpert de-
ployed to the original MetaBBO method ensuring that
the optimization process begins with a proven solution.
• Iterative In-Context Generation: To fill the remain-
ing N −1 slots, we utilize an LLM to iteratively gen-
erate new candidates. This generation is conditioned
on the thoughts of all previously accepted individuals
within the niche. By explicitly prompting the LLM
to generate different logic from existing solutions, we
enforce diversity within the initial population.
• Performance-Based Rejection Sampling: We apply a
strict rejection sampling filter during this expansion. A
newly generated individual is first evaluated following
Eq. (2). Then it will be added to the niche only if
its fitness F outperforms the human-designed reward
individual which ensures that every individual in the
initial population effectively dominates the baseline.
Prompt Initialization for the i-th Individual:
Input: Niche Specific Metadata M.
Context: Previous Individuals in the Niche
[⟨Thought1, F1⟩, . . . , ⟨Thoughti−1, Fi−1⟩].
Constraints: Diversity: ”The reward should differ from the
existing rewards by at least 95%.” Quality: ”The reward should
perform better than the existing rewards.”
Instruction: Design a new reward based on the context and
constraints.
3.3. Diversified Evolutionary Operators
During the intra-niche evolution, READY organizes evolu-
tionary operators into two functional groups: three mutation
operators (M1 ∼M3) and two crossover operators (C1 and
C2). During the reproduction, for each reward individual,
we apply each of these operators one time to generate five
offsprings for it.
• M1: Local-Reflection Mutation: Standard evolution
often discards poor performers without analysis. In-
spired by Eureka (Ma et al., 2023), M1 introduces
a distinct ”debugging” cycle. First, it identifies the
top-K (K = 3) instances where individual I yields
the poorest normalized objective. The LLM analyzes
these failure cases to produce a textual reflection, di-
agnosing why the reward logic misaligned with the
agent’s policy. Second, this reflection guides the LLM
to refine the code, explicitly addressing the identified
weaknesses while preserving existing strengths. The
detailed prompt could be found in Appendix A.2.
Prompt: Local-Reflection Mutation
Reflection Phase:
Input: Current Code, Top-K Failure Cases + Niche
Specific Metadata M
Instruction: “Analyze the failure causes on these specific
landscapes.”
Mutation Phase:
Input: Current Code, Reflection, Niche Specific Meta-
data M
Instruction: “Incorporate the reflection to modify the
code and fix weaknesses.”
• M2: History-Reflection Mutation: Unlike standard
operators that only focus on current parents, M2 ex-
ploits the temporal dimension. It retrieves the evo-
lutionary trace within the past L generations (e.g.,
L = 5). The LLM acts as a trend analyst, identifying
which specific code modifications historically corre-
lated with fitness gains. Based on this observed mo-
mentum, it extrapolates the next optimization step.The
detailed prompt could be found in Appendix A.3.
Prompt: History-Reflection Mutation
Input: Niche Specific Metadata M + Current Individual
Ig.
Context: Evolutionary Trace (Ig−L, . . . , Ig−1).
Instruction: “Review the history. Identify which logic
changes led to performance gains. Extrapolate this opti-
mization trend to generate the next version.”
• M3: Global-Reflection Mutation: To capture high-
level design patterns, READY maintains a unified
global archive Parchive containing eliminated rewards
from all K niches. The LLM first analyzes and distills
global knowledge, abstracting universal design princi-
ples to form a summary. This summary then guides the
generation of new individuals which will inherit robust
methodological patterns.The detailed prompt could be
found in Appendix A.4.
Prompt: Global-Reflection Mutation
Reflection Phase:
Input: Archive Parchive.
Instruction: “Summarize the effective techniques and
common patterns in these rewards.”
Mutation Phase:
4


--- Page 5 ---
READY: Reward Discovery for Meta-Black-Box Optimization
Input: Niche Specific Metadata M + Current Individual
I + Archive Summary.
Instruction: “Use the summary to refine the reward.”
• C1: Exploitative Crossover: C1 implements a social
learning strategy that facilitates Implicit Knowledge
Transfer. In our multi-task framework, the global best
individual I∗
global typically originates from a different
niche, acting as a carrier of universal meta-heuristics.
By prompting the LLM to synthesize the current in-
dividual with both the local niche best and the global
best, C1 produces a reward that respects local physics
while assimilating high-level logic proven successful
in other contexts. The detailed prompt could be found
in Appendix A.5.
Prompt: Exploitative Crossover
Input: Niche Specific Metadata M + Current Individual
I.
Context: Niche Best I∗
niche + Global Best I∗
global.
Instruction: “Compare Self with the Best solutions. Re-
tain Self’s core framework but integrate the superior logic
or parameters observed in the Global/Niche Best.”
• C2: Exploratory Crossover: C2 functions as a Cross-
Task Heuristic Bridge designed to propagate optimiza-
tion logic across task boundaries. It pairs the current
individual I with an elite I∗
partner derived from a ran-
dom distinct niche. The LLM is prompted to retain
I as a structural backbone while transplanting unique
algorithmic traits from I∗
partner. This mechanism in-
troduces diversity and prevent overfitting. The detailed
prompt could be found in Appendix A.6.
Prompt: Exploratory Crossover
Input: Niche Specific Metadata M + Current Individual
I + Reference Source Individual I∗
partner.
Context: I∗
partner is an elite from a different niche.
Instruction: ”Take I as the base structure. Identify
unique advantages in I∗
partner and inject them into I to
create a hybrid solution.”
3.4. Population Management
After the reproduction, we have N parents and 5N off-
springs. To select N elites from the 6N individuals for
the next generation, we employ following selection rule to
compute selection probability Pi for the i-th individual:
wi =
1
ranki + 6N ,
Pi =
wi
P6N−1
j=0
wj
(4)
where ranki is the sorting rank of i-th individual in terms
of their normalized performance score F(·|·) (ascend). The
selection rule helps avoid premature since the selection
weights are rescaled.
3.5. Knowledge Transfer
Recent research works reveal that LLMs may naturally be a
multitask solver (Ong et al., 2024; Wu et al., 2024). A key
insight is that: for the reward design task we consider in
this paper, the similarity (methodology, architecture, learn-
ing methods, etc.) between two MetaBBO tasks is a useful
information for them to promote designed reward not only
by intra-niche evolution, but also inter-niche knowledge
transfer. This motivates us to introduce Knowledge Transfer
(KT) operator, which operates by first identifying (analyz-
ing) similar task pairs and then suggesting how to transfer
the reward code of source tasks to target tasks. Specifically,
the LLM receives a global view of the population and a
transfer history log H. The global view includes the niche
metadata (M1...K) and thoughts of the current individuals
in each niche. The transfer history log records the structural
details of past transfers, specifically the reflection behind
the transfer, the transfer strategy, and the result of the trans-
fer. Based on the context, the LLM identifies K pathways
(Source →Target) and generates the corresponding reflec-
tion and strategy. The detailed prompt is in Appendix A.7.
Prompt: KT - Reflection Phase
Input: Metadata {Mk}K
k=1 Individuals Thoughts from all
niches + Transfer History H.
Instruction: “Analyze and suggest K source-target pathways
with high potential for positive knowledge transfer. For each
pathway, provide a reflection and transfer strategy suggestion.”
The LLM transforms Isource into the target context, guided
by Mtarget with the reflection and the strategy generated
above. The adapted reward replaces the lowest-ranked indi-
vidual in the target niche.
Prompt: KT - Execution
Input: Source Code Isource + Target Metadata Mtarget +
Reflection + Transfer Strategy.
Instruction: “You are transplanting a reward from the Source
to the Target task. Strictly apply the provided strategy to adapt
the code logic to the new task constraints defined in Mtarget.”
4. Experiments
4.1. Experimental Setup
Multitask MetaBBO Environment.
To evaluate the mul-
titasking performance of READY, we construct a multitask
environment comprising three distinct MetaBBO:
• RLDAS (Guo et al., 2024) (Algorithm Selection): A
discrete control task where a PPO agent dynamically
switches different Differential Evolution algorithms
during optimization.
• RLEPSO (Yin et al., 2021) (Parameter Control): A
5


--- Page 6 ---
READY: Reward Discovery for Meta-Black-Box Optimization
Table 1. Comparison of discovered rewards on test functions. We report the mean minimum cost ± standard deviation over 51 independent
runs (lower is better). Bold indicates the best performance, and underlined denotes the second best. Avg. Rank represents the average
ranking of each method across all test functions. Note that READY discovers rewards for all three MetaBBO in a single run, whereas
baselines are optimized independently for each task.
DEDQN
RLEPSO
RLDAS
Handcrafted
EoH
ReEvo
Eureka
READY
Handcrafted
EoH
ReEvo
Eureka
READY
Handcrafted
EoH
ReEvo
Eureka
READY
Attractive Sector
7.84e+03
±9.62e+03
5.86e+03
±8.69e+03
7.99e+03
±9.86e+03
8.04e+03
±9.56e+03
2.35e+03
±4.21e+03
6.17e-03
±1.27e-02
1.39e-02
±4.93e-02
4.14e-03
±2.23e-02
2.39e-02
±9.38e-02
4.09e-03
±8.49e-03
1.37e-01
±1.59e-01
1.44e-01
±1.60e-01
1.44e-01
±1.60e-01
1.36e-01
±1.43e-01
1.28e-01
±1.40e-01
Bent Cigar
3.17e+07
±1.03e+07
2.90e+07
±9.93e+06
3.27e+07
±1.13e+07
3.15e+07
±9.72e+06
2.64e+07
±1.01e+07
1.45e+00
±1.70e+00
1.20e+00
±1.43e+00
1.23e+00
±2.10e+00
1.51e+00
±2.00e+00
1.43e+00
±2.41e+00
6.66e+00
±1.88e+01
6.75e+00
±1.88e+01
6.76e+00
±1.88e+01
6.87e+00
±1.87e+01
6.88e+00
±1.89e+01
Buche Rastrigin
4.14e+02
±1.10e+02
4.00e+02
±1.31e+02
4.34e+02
±1.44e+02
4.61e+02
±1.52e+02
3.18e+02
±1.07e+02
5.75e+01
±2.82e+01
5.27e+01
±2.18e+01
6.54e+01
±3.19e+01
5.23e+01
±2.03e+01
6.52e+01
±2.80e+01
3.11e+01
±7.25e+00
3.09e+01
±6.88e+00
3.11e+01
±7.07e+00
3.12e+01
±6.99e+00
3.09e+01
±6.87e+00
Composite Grie rosen
1.23e+01
±2.07e+00
1.16e+01
±2.01e+00
1.14e+01
±2.00e+00
1.10e+01
±1.80e+00
1.19e+01
±2.00e+00
1.34e+00
±5.84e-01
1.75e+00
±5.20e-01
1.21e+00
±5.40e-01
1.73e+00
±5.07e-01
1.13e+00
±5.59e-01
1.67e+00
±6.97e-01
1.62e+00
±6.99e-01
1.62e+00
±7.00e-01
1.63e+00
±6.99e-01
1.62e+00
±6.94e-01
Different Powers
1.22e+01
±2.47e+00
1.11e+01
±2.81e+00
1.17e+01
±2.72e+00
1.15e+01
±2.96e+00
1.12e+01
±3.10e+00
2.38e-04
±9.70e-05
3.66e-04
±1.47e-04
1.10e-04
±4.50e-05
3.75e-04
±2.10e-04
1.06e-04
±6.02e-05
5.19e-04
±4.29e-04
5.08e-04
±4.28e-04
5.11e-04
±4.26e-04
5.20e-04
±4.28e-04
5.12e-04
±4.22e-04
Discus
8.34e+02
±1.05e+03
7.50e+02
±8.12e+02
1.17e+03
±1.62e+03
8.91e+02
±1.22e+03
8.01e+02
±1.20e+03
1.36e+01
±6.88e+00
1.42e+01
±6.36e+00
1.46e+01
±8.33e+00
1.37e+01
±8.72e+00
1.80e+01
±1.12e+01
3.38e+00
±4.49e+00
3.26e+00
±4.39e+00
3.37e+00
±4.40e+00
3.12e+00
±4.29e+00
3.20e+00
±4.37e+00
Ellipsoidal high cond
3.92e+05
±2.01e+05
3.57e+05
±1.96e+05
3.80e+05
±1.77e+05
4.04e+05
±2.19e+05
2.92e+05
±1.74e+05
6.80e+02
±7.10e+02
8.15e+02
±6.36e+02
6.53e+02
±1.21e+03
9.21e+02
±9.77e+02
5.71e+02
±5.80e+02
2.81e+02
±4.86e+02
2.79e+02
±4.80e+02
2.82e+02
±4.80e+02
2.83e+02
±4.85e+02
2.67e+02
±4.84e+02
Gallagher 21Peaks
6.23e+01
±1.01e+01
6.00e+01
±1.02e+01
6.21e+01
±1.09e+01
6.22e+01
±1.08e+01
5.93e+01
±1.22e+01
7.97e+00
±1.09e+01
7.82e+00
±1.07e+01
9.26e+00
±1.11e+01
7.85e+00
±9.63e+00
7.79e+00
±9.95e+00
5.81e-01
±7.94e-01
5.39e-01
±7.72e-01
5.46e-01
±7.72e-01
5.02e-01
±7.56e-01
5.30e-01
±7.75e-01
Katsuura
3.42e+00
±7.80e-01
3.82e+00
±8.44e-01
3.51e+00
±7.89e-01
3.60e+00
±7.63e-01
3.59e+00
±8.82e-01
1.16e+00
±3.78e-01
1.33e+00
±3.11e-01
9.81e-01
±3.66e-01
1.31e+00
±3.29e-01
9.74e-01
±3.64e-01
1.26e+00
±2.63e-01
1.29e+00
±2.75e-01
1.28e+00
±2.76e-01
1.26e+00
±2.73e-01
1.27e+00
±2.63e-01
Lunacek bi Rastrigin
1.69e+02
±1.90e+01
1.60e+02
±1.91e+01
1.63e+02
±1.89e+01
1.59e+02
±1.82e+01
1.62e+02
±1.95e+01
2.55e+01
±7.89e+00
2.94e+01
±9.21e+00
2.43e+01
±6.99e+00
2.76e+01
±9.19e+00
2.50e+01
±7.38e+00
3.96e+01
±7.93e+00
3.98e+01
±8.15e+00
4.00e+01
±7.98e+00
3.95e+01
±8.31e+00
3.99e+01
±8.06e+00
Rosenbrock original
1.39e+04
±6.73e+03
1.24e+04
±6.12e+03
1.35e+04
±6.54e+03
1.36e+04
±6.07e+03
1.13e+04
±5.74e+03
3.17e+00
±1.66e+00
3.78e+00
±1.85e+00
3.10e+00
±1.66e+00
3.79e+00
±1.88e+00
2.95e+00
±2.17e+00
2.70e+00
±1.56e+00
2.65e+00
±1.48e+00
2.65e+00
±1.50e+00
2.64e+00
±1.53e+00
2.60e+00
±1.54e+00
Rosenbrock rotated
1.22e+04
±5.87e+03
1.08e+04
±4.93e+03
1.07e+04
±6.59e+03
9.47e+03
±5.01e+03
1.20e+04
±4.99e+03
3.81e+00
±2.04e+00
5.96e+00
±8.87e+00
3.26e+00
±1.93e+00
5.65e+00
±9.16e+00
2.91e+00
±1.50e+00
4.50e+00
±1.86e+00
4.51e+00
±1.86e+00
4.51e+00
±1.86e+00
4.51e+00
±1.88e+00
4.48e+00
±1.85e+00
Schaffers high cond
2.94e+01
±6.79e+00
2.91e+01
±5.96e+00
2.96e+01
±6.09e+00
2.98e+01
±5.67e+00
2.68e+01
±5.99e+00
2.37e+00
±1.97e+00
2.21e+00
±1.55e+00
3.24e+00
±2.33e+00
2.42e+00
±2.54e+00
3.09e+00
±2.18e+00
1.21e+00
±6.64e-01
1.16e+00
±6.19e-01
1.16e+00
±6.23e-01
1.21e+00
±6.47e-01
1.20e+00
±6.39e-01
Schwefel
6.64e+03
±3.14e+03
5.96e+03
±2.77e+03
6.06e+03
±3.00e+03
5.66e+03
±2.52e+03
5.45e+03
±3.12e+03
1.29e+00
±3.04e-01
1.31e+00
±2.90e-01
1.42e+00
±2.54e-01
1.26e+00
±3.07e-01
1.43e+00
±3.09e-01
6.06e-01
±2.59e-01
6.09e-01
±2.48e-01
6.07e-01
±2.49e-01
6.07e-01
±2.43e-01
6.06e-01
±2.48e-01
Sharp Ridge
1.01e+03
±1.46e+02
9.52e+02
±1.56e+02
9.89e+02
±1.48e+02
9.76e+02
±1.46e+02
9.08e+02
±1.59e+02
1.10e+01
±1.17e+01
1.92e+01
±3.43e+01
1.62e+01
±2.97e+01
1.13e+01
±1.40e+01
1.55e+01
±2.78e+01
2.36e+00
±2.03e+00
2.27e+00
±2.18e+00
2.34e+00
±2.18e+00
2.19e+00
±2.20e+00
2.32e+00
±2.05e+00
Step Ellipsoidal
1.05e+02
±3.38e+01
1.02e+02
±3.07e+01
9.97e+01
±3.06e+01
9.68e+01
±3.09e+01
1.09e+02
±2.85e+01
2.00e+00
±1.62e+00
1.43e+00
±9.46e-01
2.71e+00
±2.10e+00
1.33e+00
±9.45e-01
2.52e+00
±1.95e+00
5.62e-01
±4.98e-01
6.16e-01
±5.66e-01
5.62e-01
±4.99e-01
6.02e-01
±5.52e-01
5.42e-01
±4.92e-01
Avg. Rank
4.06
2.31
3.44
3.19
2.00
2.75
3.44
3.06
3.38
2.38
3.31
2.93
3.56
3.13
2.06
continuous control task employing PPO for the adap-
tive adjustment of evolutionary factors in Particle
Swarm Optimization.
• DEDQN (Tan & Li, 2021) (Operator Selection): A
discrete control task relying on DQN to select muta-
tion operators from a predefined candidate pool for
Differential Evolution.
Benchmarks and Baselines.
We employ the BBOB test
suite (Ma et al., 2025a) with a strict train-test split: 8 func-
tion are used for training, while 16 unseen problems are
reserved for zero-shot testing. We benchmark READY
against handcrafted rewards and two categories of auto-
mated baselines: (1) the dedicated reward discovery method
Eureka (Ma et al., 2023); and (2) general LLM-based al-
gorithm design frameworks EoH (Liu et al., 2024b) and
ReEvo (Ye et al., 2024), adapted for reward discovery. No-
tably, while READY discovers rewards for these three het-
erogeneous environments simultaneously within a single
unified run, all baseline methods are trained independently
for each specific MetaBBO.
Implementation Details.
We utilize DeepSeek-V3.2 (Liu
et al., 2024a) as the LLM backbone, setting the niche size
as N = 5 and the maximum evolutionary generations as
Gmax = 7. To balance accuracy and speed, we impose
a 0.5-hour wall-clock limit on the training of MetaBBO.
We utilize the Ray framework to accelerate the fitness as-
sessment via parallel evaluation. During the evolutionary
search, each reward is tested over Γ = 3 independent runs
to mitigate stochasticity.
4.2. Main Experimental Results
Following the reward discovery, we remove the training time
constraints to fully train the optimal rewards from READY
and the baselines. Each reward is then evaluated across 51
independent runs on the same test suite to ensure consistency.
Table 1 reports the mean minimum cost achieved by each
method across three MetaBBO frameworks.
Overall Optimization Capability.
As summarized in Ta-
ble 1, READY demonstrates superior generalization capa-
bilities. Despite being trained within a unified framework,
READY outperforms the Handcrafted rewards in 36 out
of 48 test scenarios (covering 16 functions across 3 tasks).
Furthermore, when compared against the full suite of state-
of-the-art automated baselines, READY achieves the lowest
mean cost in 24 cases. This result is particularly significant
given that baseline methods were trained independently for
each specific MetaBBO environment, whereas READY uti-
lized a single set of discovered rewards to generalize across
all diverse decision spaces simultaneously.
Versatility Across Heterogeneous Agents.
READY ex-
hibits remarkable paradigm-agnostic adaptability. As shown
in the Attractive Sector and Different Powers functions,
READY achieves top-tier performance across all three dis-
tinct control topologies. For instance, on Attractive Sec-
6


--- Page 7 ---
READY: Reward Discovery for Meta-Black-Box Optimization
Figure 2. Evolutionary trajectory of READY’s best-so-far performance when discover reward for DEDQN. The plateau reflects a phase of
strategic accumulation, serving as a necessary precursor for the significant exploitation breakthrough.
tor, READY secures the best performance on DEDQN
(2.35 × 103) and RLDAS (1.28 × 10−1), while maintaining
competitive results on RLEPSO. In contrast, Handcrafted re-
wards often fail to generalize to the continuous dynamics of
RLEPSO (e.g., significantly underperforming on Ellipsoidal
High Cond), highlighting READY’s ability to evolve uni-
versal signaling mechanisms that transcend specific agent
architectures.
Robustness on Complex Landscapes.
We observe that
READY effectively balances the exploration-exploitation
trade-off, specifically tailoring its strategy to the optimiza-
tion context. On highly deceptive, multimodal functions
such as Buche Rastrigin and Schwefel, READY demon-
strates superior global search capabilities, particularly
within the DEDQN task where it significantly outperforms
baselines (e.g., achieving a minimum cost of 3.18 × 102
on Buche Rastrigin). This exploration capability is comple-
mented by distinct exploitation strength on ill-conditioned,
unimodal landscapes. A standout example is Ellipsoidal
High Conditional, where READY achieves best perfor-
mance across all three MetaBBO. Notably, on RLEPSO,
READY reduces the cost by approximately 38% compared
to Eureka (5.71×102 vs. 9.21×102). These results confirm
that READY does not rely on a single static heuristic but
dynamically evolves specialized reward logics adapted to
the distinct topological requirements of each landscape.
Stability and Reproducibility.
READY also enhances
training stability, frequently reporting lower standard de-
viations than baselines. This reduced variance indicates
robustness to initialization noise and more reproducible RL
trajectories.
4.3. Trajectory Analysis: From Accumulation to
Breakthrough
Figure
2
visualizes
the
evolutionary
trajectory
on
the DEDQN task,
revealing a distinct Exploration-
Accumulation-Exploitation pattern that diverges from linear
convergence. As detailed below, specific operators drive
three critical phases:
• Rapid Adaptation (Gen 1-2): The M3 mutation oper-
ator drives an early performance spike by shifting focus
from sparse individual signals to population-averaged
efficiency, establishing a strong initial baseline.
• Latent Accumulation (Gen 2-4): A strategic plateau
emerges, representing a phase of structural refine-
ment. M2 and M1 mutation operators debug landscape-
specific failures and refine trade-off mechanisms.
While global loss remains stable, the reward function
accumulates complexity and robustness.
• Breakthrough (Gen 5+): This latent potential is cat-
alyzed by the crossover operators, which integrates
spatial-entropic feedback to trigger a breakthrough,
successfully escaping local optima.
The final evolved reward for DEDQN synthesizes a phase-
adaptive mechanism that dynamically reweights five strate-
gic components, ranging from trend improvement to explo-
ration according to real-time search states, ensuring consis-
tent convergence across diverse landscapes.
4.4. Computational Efficiency
We evaluate the efficiency of READY from two perspectives:
the evolutionary search time and the inference latency.
Evolutionary Search Efficiency.
Benefiting from the
niche-based multi-task architecture, READY significantly
reduces the wall-clock time required for reward discovery.
While baseline methods must be executed independently for
each MetaBBO task, READY concurrently evolves special-
ized rewards for all tasks in a single unified run. READY
completes the entire discovery process across three tasks in
only 7 hours. In contrast, sequential execution of Eureka
7


--- Page 8 ---
READY: Reward Discovery for Meta-Black-Box Optimization
requires approximately 14 hours, while EoH and ReEvo
take over 14.6 hours and 30.6 hours respectively to achieve
comparable coverage. This represents a 2× to 4× speedup
in search efficiency, validating that cross-niche knowledge
sharing and parallel evaluation effectively accelerate evolu-
tionary convergence.
Inference Latency.
To ensure practical applicability, we
profiled the runtime latency of the discovered rewards. Em-
pirical measurements confirm that despite their internal
symbolic complexity, the rewards operate strictly within
the microsecond regime (e.g., as low as 3.47µs per call
for DEDQN). Although this introduces a slight overhead
compared to handcrafted baselines, the cost is negligible as
objective function evaluations typically dominate the BBO
runtime which often requiring milliseconds to minutes.
Table 2. Zero-shot transfer performance. Objective values of the
READY-evolved DEDQN reward when applied to RLDEAFL.
Problem
RLDEAFL
READY
Reduction (%)
Discus
6.48e+00
4.05e-04
99.99
Ellipsoidal high cond
3.50e+02
2.57e-01
99.93
Attractive Sector
1.20e-01
4.99e-04
99.59
Different Powers
4.26e-04
1.77e-05
95.84
Sharp Ridge
1.95e+00
8.35e-02
95.71
Bent Cigar
5.89e+00
6.17e-01
89.54
Rosenbrock original
4.13e+00
1.34e+00
67.61
Rosenbrock rotated
5.69e+00
1.88e+00
66.95
Schaffers high cond
2.81e-01
9.90e-02
64.79
Step Ellipsoidal
1.23e-01
7.26e-02
41.00
Katsuura
1.32e+00
8.46e-01
35.87
Lunacek bi Rastrigin
3.64e+01
2.53e+01
30.37
Composite Grie rosen
2.47e+00
2.35e+00
4.80
Buche Rastrigin
1.77e+01
1.98e+01
-11.89
Gallagher 21Peaks
1.52e+00
2.47e+00
-62.59
Schwefel
2.77e-01
6.55e-01
-136.80
4.5. Generalizability via Zero-Shot Transfer
To investigate whether READY captures intrinsic optimiza-
tion principles rather than overfitting to specific architec-
tures, we evaluated the zero-shot transfer of reward logic
across distinct MetaBBO frameworks via LLM-based trans-
lation without fine-tuning. As detailed in Table 2, transfer-
ring logic from DEDQN to RLDEAFL (Guo et al., 2025a)
yields performance improvements on 13 out of 16 functions,
achieving up to 99.99% cost reduction despite the significant
structural gap between source and target tasks. This general-
izability is further corroborated by a secondary experiment
transferring from RLEPSO to GLEET (Ma et al., 2024),
which improved performance on 11 out of 16 instances. The
results validate that READY identifies universal, algorithm-
agnostic heuristics in discovering rewards.
4.6. Ablation Study and Sensitivity Analysis
To validate component contributions, we conducted ablation
studies quantified by the Summed Normalized Efficiency
(SNE). The SNE of baseline B is defined as: SNEB =
Figure 3. Ablation and sensitivity analysis. Y-axis shows Summed
Normalized Efficiency, with the red dashed line (y = 1.0) marking
the full model baseline.
1
K
PK
k=1
F(READY|Tk)
F(B|Tk)
. A score of 1.0 signifies parity with
the full READY framework, while lower scores indicate
performance degradation.
Component Efficacy via Iso-Budget Ablation.
We sub-
stitute target operators with simple LLM mutations to main-
tain a constant evaluation budget. Figure 3 reveals that
replacing any specialized operator causes a significant drop
in SNE. This confirms that READY’s efficacy stems from
its specific architectural logic rather than the mere quantity
of inferences. Disabling the Knowledge Transfer module
degrades performance, confirming that sharing heuristics
across niches is essential for effective reward discovery.
Scalability with Reasoning Capability.
We evaluated ro-
bustness across diverse backbones: DeepSeek-V3.2 (Liu
et al., 2024a), Qwen-3-Max (Yang et al., 2025a), and
Gemini-3-Flash (Team et al., 2023). READY delivers con-
sistent high-quality solutions across all models. Crucially,
the superior performance achieved by Gemini suggests a
positive scaling law: the framework is not architecturally
bottlenecked but scales effectively with the backend’s rea-
soning capability, indicating strong potential to leverage
future advancements in foundation models.
5. Conclusion
We proposed READY, the first LLM-driven multitask frame-
work for automating reward discovery in MetaBBO. By syn-
ergizing a niche-based architecture with specialized LLM
operators, READY overcomes the limitations of manual
reward engineering in effectiveness and efficiency. Em-
pirical results confirm that READY generates high-quality,
interpretable rewards that outperform baselines and exhibit
robust zero-shot generalizability across diverse tasks. This
framework paves the way for fully autonomous optimization
pipelines by capturing universal search heuristics.
8


--- Page 9 ---
READY: Reward Discovery for Meta-Black-Box Optimization
Impact Statement
This paper presents work whose goal is to advance the field
of MetaBBO. We propose a reward design framework to au-
tomate the reward design process in establishing MetaBBO
systems. This is a significant step forward and hance may
produces profound impacts on the future development of
MetaBBO researches, especially in two aspects: 1) the next
generation self-organized learning agent such as automated
MetaBBO framework design; 2) boosting MetaBBO’s per-
formance on realworld applications.
References
Cardenoso, F. and Caarls, W.
Leveraging llms for re-
ward function design in reinforcement learning con-
trol tasks, 2025. URL https://arxiv.org/abs/
2511.19355.
Chen, J., Ma, Z., Guo, H., Ma, Y., Zhang, J., and Gong,
Y.-J. Symbol: Generating flexible black-box optimizers
through symbolic equation learning, 2024. URL https:
//arxiv.org/abs/2402.02355.
Guo, H., Ma, Y., Ma, Z., Chen, J., Zhang, X., Cao, Z.,
Zhang, J., and Gong, Y.-J. Deep reinforcement learning
for dynamic algorithm selection: A proof-of-principle
study on differential evolution. IEEE Transactions on
Systems, Man, and Cybernetics: Systems, 54(7):4247–
4259, 2024.
Guo, H., Ma, S., Huang, Z., Hu, Y., Ma, Z., Zhang, X., and
Gong, Y.-J. Reinforcement learning-based self-adaptive
differential evolution through automated landscape fea-
ture learning. In Proceedings of the Genetic and Evolu-
tionary Computation Conference, pp. 1117–1126, 2025a.
Guo, H., Ma, Z., Chen, J., Ma, Y., Cao, Z., Zhang, X., and
Gong, Y.-J. Configx: Modular configuration for evolu-
tionary algorithms via multitask reinforcement learning.
In Proceedings of the AAAI Conference on Artificial In-
telligence, pp. 26982–26990, 2025b.
Guo, H., Ma, Z., Ma, Y., Zhang, X., Chen, W.-N., and Gong,
Y.-J. Designx: Human-competitive algorithm designer
for black-box optimization, 2025c. URL https://
arxiv.org/abs/2505.17866.
Han, M., Li, X., Wu, K., Zhang, X., and Wang, H. En-
hancing zero-shot black-box optimization via pretrained
models with efficient population modeling, interaction,
and stable gradient approximation. In The Thirty-ninth
Annual Conference on Neural Information Processing
Systems, 2025.
Hazra, R., Sygkounas, A., Persson, A., Loutfi, A., and
Martires, P. Z. D. Revolve: Reward evolution with large
language models using human feedback, 2024. URL
https://arxiv.org/abs/2406.01309.
Heng, Z. K., Zhao, Z., Wu, T., Wang, Y., Wu, M., Wang,
Y., and Dong, H.
Boosting universal llm reward de-
sign through heuristic reward observation space evolu-
tion, 2025. URL https://arxiv.org/abs/2504.
07596.
Hu, Z., Gong, W., Pedrycz, W., and Li, Y.
Deep rein-
forcement learning assisted co-evolutionary differential
evolution for constrained optimization. Swarm and Evo-
lutionary Computation, 83:101387, 2023.
Kimiaei, M. and Kungurtsev, V. Machine learning algo-
rithms for improving black box optimization solvers.
arXiv preprint arXiv:2509.25592, 2025.
Li, G., Xin, Y., Niu, J., Wang, Z., Chen, J., and Wu, F. Re-
inforcement learning assisted automatic niche selection
for constrained multimodal multi-objective optimization.
Expert Systems with Applications, 297:129458, 2026.
Li, P., Hao, J., Tang, H., Fu, X., Zhen, Y., and Tang,
K. Bridging evolutionary algorithms and reinforcement
learning: A comprehensive survey on hybrid algorithms.
IEEE Transactions on evolutionary computation, 2024.
Li, P., Jianye, H., Tang, H., Yuan, Y., Qiao, J., Dong, Z., and
Zheng, Y. R*: Efficient reward design via reward struc-
ture evolution and parameter alignment optimization with
large language models. In Forty-second International
Conference on Machine Learning, 2025a.
Li, P., Tang, H., Qiao, J., Zheng, Y., and Hao, J. Lares: Evo-
lutionary reinforcement learning with llm-based adaptive
reward search. In The Thirty-ninth Annual Conference
on Neural Information Processing Systems, 2025b.
Li, S., Lou, H., Zhang, X., Zeng, X., Shen, Z., and Li, T.
Role-specific reward design with large language model
for starcraft ii. In ICASSP 2025 - 2025 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP), pp. 1–5, 2025c. doi: 10.1109/ICASSP49660.
2025.10890857.
Lian, H., Ma, Z., Guo, H., Huang, T., and Gong, Y.-J.
Rlemmo: Evolutionary multimodal optimization assisted
by deep reinforcement learning. In Proceedings of the
Genetic and Evolutionary Computation Conference, pp.
683–693, 2024.
Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao,
C., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-
v3 technical report. arXiv preprint arXiv:2412.19437,
2024a.
9


--- Page 10 ---
READY: Reward Discovery for Meta-Black-Box Optimization
Liu, F., Tong, X., Yuan, M., Lin, X., Luo, F., Wang, Z.,
Lu, Z., and Zhang, Q. Evolution of heuristics: Towards
efficient automatic algorithm design using large language
model. arXiv preprint arXiv:2401.02051, 2024b.
Lu, R., Shao, Z., Ding, Y., Chen, R., Wu, D., Su, H., Yang,
T., Zhang, F., Wang, J., Shi, Y., et al.
Discovery of
the reward function for embodied reinforcement learning
agents. Nature Communications, 16(1):11064, 2025.
Ma, Y. J., Liang, W., Wang, G., Huang, D.-A., Bastani, O.,
Jayaraman, D., Zhu, Y., Fan, L., and Anandkumar, A.
Eureka: Human-level reward design via coding large lan-
guage models. arXiv preprint arXiv:2310.12931, 2023.
Ma, Z., Chen, J., Guo, H., Ma, Y., and Gong, Y.-J. Auto-
configuring exploration-exploitation tradeoff in evolution-
ary computation via deep reinforcement learning. In Pro-
ceedings of the Genetic and Evolutionary Computation
Conference, pp. 1497–1505, 2024.
Ma, Z., Gong, Y.-J., Guo, H., Qiu, W., Ma, S., Lian, H.,
Zhan, J., Chen, K., Wang, C., Huang, Z., et al. Metabox-
v2: A unified benchmark platform for meta-black-box
optimization. arXiv preprint arXiv:2505.17745, 2025a.
Ma, Z., Guo, H., Gong, Y.-J., Zhang, J., and Tan, K. C. To-
ward automated algorithm design: A survey and practical
guide to meta-black-box-optimization. IEEE Transac-
tions on Evolutionary Computation, 2025b.
Ma, Z., Gong, Y.-J., Guo, H., Chen, J., Ma, Y., Cao, Z.,
and Zhang, J. Llamoco: Instruction tuning of large lan-
guage models for optimization code generation. IEEE
Transactions on Evolutionary Computation, 2026.
Nguyen, T., Le, P., Biedenkapp, A., Doerr, C., and Dang,
N. On the importance of reward design in reinforcement
learning-based dynamic algorithm configuration: A case
study on onemax with (1+(λ, λ))-ga. In Proceedings of
the Genetic and Evolutionary Computation Conference,
pp. 1162–1171, 2025.
Novikov, A., V˜u, N., Eisenberger, M., Dupont, E., Huang,
P.-S., Wagner, A. Z., Shirobokov, S., Kozlovskii, B., Ruiz,
F. J., Mehrabian, A., et al. Alphaevolve: A coding agent
for scientific and algorithmic discovery. arXiv preprint
arXiv:2506.13131, 2025.
Oh, J., Farquhar, G., Kemaev, I., Calian, D. A., Hessel,
M., Zintgraf, L., Singh, S., Van Hasselt, H., and Silver,
D. Discovering state-of-the-art reinforcement learning
algorithms. Nature, pp. 1–2, 2025.
Ong, Y. S., Liu, J., Wong, M., Rios, T., and Menzel, S.
Llm2fea: Discover novel designs with generative evo-
lutionary multitasking, 2024. URL https://arxiv.
org/abs/2406.14917.
Romera-Paredes, B., Barekatain, M., Novikov, A., Balog,
M., Kumar, M. P., Dupont, E., Ruiz, F. J., Ellenberg, J. S.,
Wang, P., Fawzi, O., et al. Mathematical discoveries from
program search with large language models. Nature, 625
(7995):468–475, 2024.
Shao, S., Tian, Y., and Zhang, Y. Deep reinforcement learn-
ing assisted surrogate model management for expensive
constrained multi-objective optimization. Swarm and
Evolutionary Computation, 92:101817, 2025.
Sharma, M., Komninos, A., L´opez-Ib´a˜nez, M., and Kazakov,
D. Deep reinforcement learning based parameter control
in differential evolution. In Proceedings of the Genetic
and Evolutionary Computation Conference, GECCO ’19,
pp. 709–717, New York, NY, USA, 2019. Association for
Computing Machinery. ISBN 9781450361118. doi: 10.
1145/3321707.3321813. URL https://doi.org/
10.1145/3321707.3321813.
Su, H., Sun, Y., and Yu, C. The end of reward engineering:
How llms are redefining multi-agent coordination. arXiv
preprint arXiv:2601.08237, 2026.
Sun, J., Liu, X., B¨ack, T., and Xu, Z.
Learning adap-
tive differential evolution algorithm from optimization
experiences by policy gradient. IEEE Transactions on
Evolutionary Computation, 25(4):666–680, 2021. doi:
10.1109/TEVC.2021.3060811.
Sun, S., Liu, R., Lyu, J., Yang, J.-W., Zhang, L., and Li,
X. A large language model-driven reward design frame-
work via dynamic feedback for reinforcement learning.
Knowledge-Based Systems, 326:114065, 2025.
Sutton, R. S., Barto, A. G., et al. Reinforcement learning:
An introduction. MIT press Cambridge, 1998.
Tan, Z. and Li, K. Differential evolution with mixed muta-
tion strategy based on deep reinforcement learning. Ap-
plied Soft Computing, 111:107678, 2021.
Team, G., Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Sori-
cut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican,
K., et al. Gemini: a family of highly capable multimodal
models. arXiv preprint arXiv:2312.11805, 2023.
Tian, Y., Qi, X., Yang, S., He, C., Tan, K. C., Jin, Y., and
Zhang, X. A universal framework for automatically gener-
ating single-and multi-objective evolutionary algorithms.
IEEE Transactions on Evolutionary Computation, 2025.
Wei, Y., Shan, X., and Li, J. Lero: Llm-driven evolutionary
framework with hybrid rewards and enhanced observa-
tion for multi-agent reinforcement learning, 2025. URL
https://arxiv.org/abs/2503.21807.
10


--- Page 11 ---
READY: Reward Discovery for Meta-Black-Box Optimization
Wu, J., Feng, L., Tan, K. C., Huang, Y., Wu, S., and Lv, X.
Advancing automated knowledge transfer in evolution-
ary multitasking via large language models, 2024. URL
https://arxiv.org/abs/2409.04270.
Xie, T., Zhao, S., Wu, C. H., Liu, Y., Luo, Q., Zhong,
V., Yang, Y., and Yu, T. Text2reward: Reward shaping
with language models for reinforcement learning. arXiv
preprint arXiv:2309.11489, 2023.
Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B.,
Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical
report. arXiv preprint arXiv:2505.09388, 2025a.
Yang, X., Wang, R., Li, K., and Ishibuchi, H. Meta-black-
box optimization for evolutionary algorithms: Review
and perspective. Swarm and Evolutionary Computation,
93:101838, 2025b.
Yao, C., Liu, X., Li, C., and Savvides, M. Reward evolu-
tion with graph-of-thoughts: A bi-level language model
framework for reinforcement learning, 2025a.
URL
https://arxiv.org/abs/2509.16136.
Yao, Y., Liu, F., Zhao, L., Lin, X., and Zhang, Q. Fomemo:
Towards foundation models for expensive multi-objective
optimization. arXiv preprint arXiv:2509.03244, 2025b.
Ye, H., Wang, J., Cao, Z., Berto, F., Hua, C., Kim, H., Park,
J., and Song, G. Reevo: Large language models as hyper-
heuristics with reflective evolution. Advances in neural
information processing systems, 37:43571–43608, 2024.
Yi, W. and Qu, R. Automated design of search algorithms
based on reinforcement learning. Information Sciences,
649:119639, 2023.
Yin, S., Liu, Y., Gong, G., Lu, H., and Li, W. RLEPSO:
Reinforcement learning based ensemble particle swarm
optimizer. In Proceedings of the 2021 4th International
Conference on Algorithms, Computing and Artificial In-
telligence, 2021.
Zhang, P., Wang, L., Rong, J., Shao, S., Zhang, X., and Tian,
Y. Reinforcement learning assisted autonomous selection
of sparsity-aware genetic operators for sparse large-scale
multi-objective optimization. Tsinghua Science and Tech-
nology, 31(1):379–398, 2026.
Zhao, Q., Duan, Q., Yan, B., Cheng, S., and Shi, Y. Au-
tomated design of metaheuristic algorithms: A survey.
arXiv preprint arXiv:2303.06532, 2023.
Zhao, Q., Liu, T., Yan, B., Duan, Q., Yang, J., and Shi, Y.
Automated metaheuristic algorithm design with autore-
gressive learning. IEEE Transactions on Evolutionary
Computation, 2024.
11


--- Page 12 ---
READY: Reward Discovery for Meta-Black-Box Optimization
A. Detailed Prompt
We provide the full prompt templates used for the semantic evolutionary operators.
A.1. Initialization Prompt
# Role You are a reward engineer trying to write reward functions to solve Metablackbox Optimization tasks as
effective as possible.
# Aim Your goal is to write a reward function for the environment that will help the agent learn the task described in
text, and perform well on the problems.
Your reward should use useful variables from the environment as inputs.
# Info
You are working on the task:{Task description}
Experts have proposed {MetaBBO rewards nums} Metablackbox Optimization rewards to solve this problem. The
ideas for these Metablackbox Optimization rewards are as follows:
Please create a new reward that differs from the existing reward by at least {difference rate} %
The new reward should perform better than the existing rewards on the given task. (If not, try your best to make it
better than the original reward)
A.2. M1 Local-Reflection Mutation Prompt
A.2.1. REFLECTION PHASE
# Role
You are a MetaBBO algorithm reward design expert specializing in failure analysis.
# Info
An intelligent agent is currently executing the following design task:{Task description}
The agent has designed a reward function that performs well on some problems but fails significantly on specific
problems.
# Current Reward Info
Original Thought:{Thought}
Original Code:{Code}
# Failure Analysis Report
We have identified a set of problems where this reward function performs poorly. The characteristics of these
problems are described below:
Characteristics of Poorly Performing Problems:{Bad Case Characteristics}
Performance Gap:
On these problems, The reward’s performances are:{Bad Case Performance}
# Your Task
Based on your understanding and knowledge, please provide suggestions for the current reward to guide the agent in
improving it.
The suggestion should include:
1. Diagnose: Analyze WHY the current reward logic (based on the Original Thought and Code) is insufficient
for the problems described in the Failure Analysis Report.
2. Propose Solution: Briefly describe potential the modification strategy.
• CONTENT: Provide the conceptual direction. Describe what to change, not how to code it. Do not
include Python code or complex derivations.
12


--- Page 13 ---
READY: Reward Discovery for Meta-Black-Box Optimization
A.2.2. MUTATION PHASE
# Role
You are a Metablackbox Optimization reward design expert.
Remember: Lower Fitness is Better. A decrease in the function value indicates improvement.
# Info
An intelligent agent is currently executing the following design task:{Task description}
The agent has designed a reward with the following code:{Code}
# Aim
An expert has provided some suggestions for this Metablackbox Optimization reward. You can decide how to
incorporate the advice, and create a new reward that differs from the given one but motivated by it.
The advice is: {Reflection}
A.3. M2 History-Reflection Prompt
# Role
You are an expert in evolutionary MetaBBO and reward function design. Remember: Lower Fitness is Better. A
decrease in the function value indicates improvement.
## Task
You are currently optimizing a reward function for the following task:
{Task}
### Evolutionary Trajectory Analysis
I will provide you with the evolutionary history of the current individual. This history shows how the reward
function has improved step-by-step. Your goal is to identify the optimization trend and extrapolate the next
logical improvement.
### History Trace
{History Trace}
### Current Individual
Fitness: {Current Fitness}
Fitness Detailed:{Fitness Detailed}
Thought:{Current Thought}
Code:{Current Code}
### Your Task
1. Analyze the Trajectory: Briefly explain what changed from the past versions to the current version and why it
led to performance improvements. What is the underlying direction?
2. Extrapolate: Based on this trend, propose the next step. Don’t just randomly mutate; follow the momentum of
the history. If the previous steps successfully refined a specific component, continue refining it or address the
side effects caused by it.
3. Generate Code: Output the new reward function code.
A.4. M3 Global-Reflection prompt
A.4.1. REFLECTION PHASE
# Role
You are the person responsible for recording the progress of the experts’ research.
You are working on the following task:{Task Description}
## Info
Experts have now explored an additional {MetaBBO reward nums} Metablackbox Optimization rewards, ranging
from No. 1 to No. {MetaBBO reward nums}:
13


--- Page 14 ---
READY: Reward Discovery for Meta-Black-Box Optimization
# Aim
Please review the previous experiences and the current Metablackbox Optimization rewards, analyzing which
techniques within the Metablackbox Optimization rewards are effective in solving this problem and which are not.
Finally, summarize both the effective and ineffective techniques, and update the previous summary accordingly.
Please ensure that the summary you provide is written as:
‘‘‘summary
{summary}
‘‘‘
A.4.2. MUTATION PHASE
# Role
You are an Metablackbox Optimization reward design expert, currently collaborating with other experts on the
following task:{Task}
# Info
Currently, {reinforcement learning reward nums} Metablackbox Optimization rewards have been explored for
this problem, with their effectiveness decreasing from No. 1 to No. {reinforcement learning reward nums}. The
concepts for these methods are as follows.
The summary of the archive is: {summary}
Please analyze the summary and then modify the following reward to create a more promising reward. The thoughts
and code for the reward to be modified are as follows:
{individual.thought}
{individual.reward code}
A.5. C1 Exploitative prompt
# Role
You are an Metablackbox Optimization reward design expert. Remember: Lower Fitness is Better. A decrease in
the function value indicates improvement.
# Info
Experts are divided into several groups, with each group responsible for the development of a specific Metablackbox
Optimization reward cluster. Each cluster incorporates different techniques while maintaining its own framework to
explore diverse Metablackbox Optimization rewards.
On your Metablackbox Optimization reward cluster, after several iterations, the current Metablackbox Optimization
reward (idea and the corresponding code) is:{chosen MetaBBO reward thought},{chosen MetaBBO reward code}
During the iterations in your cluster, a better-performing Metablackbox Optimization reward appeared, and its idea
and code are as follows:{cluster best MetaBBO reward thought},{cluster best MetaBBO reward code}
In addition,
among all the Metablackbox Optimization rewards tested (including those from other
clusters),
the
best-performing
Metablackbox
Optimization
reward’s
idea
and
code
are
as
fol-
lows:{global best MetaBBO reward thought},{global best MetaBBO reward code}
# Aim
Using the above information and adhering to the core framework of the current Metablackbox Optimization reward,
please suggest potential improvements to enhance its performance in solving this problem.
There are some info that would restric your design of the reward:
The lab is now currently collaborating with other experts on the following task:{Task description}
14


--- Page 15 ---
READY: Reward Discovery for Meta-Black-Box Optimization
A.6. C2 Exploratory prompt
# Role
You are an Metablackbox Optimization reward design expert, currently collaborating with other experts on the
following task:{Task description}
# Info
Experts have designed {MetaBBO reward nums} Metablackbox Optimization rewards with their corresponding
codes.
The No. 1 Reward and the corresponding code are:{individual1.thought},{individual1.reward code}
The No. 2 Reward and the corresponding code are:{individual2.thought},{individual2.reward code}
Please take Reward No. 1 as the main framework and try to incorporate the characteristics of the other rewards into
it to create a better reward.
A.7. Knowledge Transfer Prompt
15


--- Page 16 ---
READY: Reward Discovery for Meta-Black-Box Optimization
# Role
You are an advanced Optimization Meta-Learner. Your goal is to orchestrate knowledge transfer between different
evolutionary optimization tasks to optimize their performance. Remember: Lower Fitness is Better. A decrease in
the function value indicates improvement.
# Aim
Analyze the provided tasks and historical data to determine optimal Task Pairs for knowledge transfer.
Core Mechanism: The system will automatically extract the Best Individual from the chosen Source Task and
adapt it to replace the Worst Individual in the Target Task.
Your Focus: You do NOT need to select specific individuals. Your sole responsibility is to identify which Source
Task (Algorithm/Problem) possesses knowledge that is valuable for a specific Target Task.
# Data Overview
## 1. Historical Knowledge
This data reflects the outcome of your past decisions. Use it to learn what works.{kt historical info}
## 2. Active Tasks & States
Below are the descriptions and the current internal thoughts/states of the top individuals in each task.
{tasks descriptions}
# Instruction
Determine the {N direction} most valuable transfer operations (Source Task -¿ Target Task).
CRITICAL RULES:
1. Focus on Task Compatibility: Do NOT base your decision on the specific traits of an individual. Instead, base
it on the Algorithm/Task characteristics.
2. Fixed Mechanism: Remember that the ”Best Individual” is automatically selected.
3. Strategy: For each operation, generate a Strategy that describes how to transform the logic/code from Source
to Target to ensure compatibility.
# Output Format (JSON)
Please output strictly in the following JSON format:
[
{{
"source_task_Metabbo_algorithm": "{{source_algorithm_name}}",
"target_task_Metabbo_algorithm": "{{target_algorithm_name}}",
"rationale": "Explain **WHY** you chose this pair",
"transfer_strategy_guidance": "Specific instruction on **HOW** to map the
individual/code."
}},
...
]
B. Specific MetaBBO MetaData
B.1. DEDQN MetaData
DEDQN (Differential Evolution with Deep Q-Network)
The DEDQN algorithm primarily focuses on the adaptive selection of mutation strategies in Differential Evolution
(DE). It aims to solve the problem of selecting the most appropriate mutation strategy. The algorithm uses a Deep
Q-Network (DQN) to manage a mixed mutation strategy pool during the evolution process.
Algorithm Framework
DEDQN operates in two distinct phases: an offline training phase and an online prediction phase.
• Offline Training Phase:
– In this stage, the DQN is trained offline.
16


--- Page 17 ---
READY: Reward Discovery for Meta-Black-Box Optimization
– It learns by collecting data from multiple DE runs on various training functions, establishing a relationship
between the ”fitness landscape” features (state) and the ”benefit (reward)” of applying each mutation
strategy (action).
– The DQN acts as an agent, interacting with an environment composed of the DEDQN algorithm, the fitness
function, and a module for calculating the fitness landscape.
• Online Prediction Phase:
– After training, the DQN’s neural network weights are fixed.
– When the DEDQN algorithm is applied to solve a new (test) problem, it uses this trained DQN.
– At each generation, the algorithm calculates the current fitness landscape features and feeds them to the
DQN, which then predicts and selects the most suitable mutation strategy to be used.
Key Features & Mechanisms
• Mixed Mutation Strategy Pool (Action Space): The DEDQN’s action space consists of three DE mutation
strategies, each chosen for its different search characteristics:
– ”DE/rand/1” (for local search)
– ”DE/current to rand/1” (for global search)
– ”DE/best/2” (for rapid convergence)
• Population Evolutionary Efficiency (Reward): The reward signal used to train the DQN is defined as the
”population evolutionary efficiency”. This metric measures the evolutionary ability of the population based on
individual ”survival” from one generation to the next.
• Parameter Adaptation: In addition to the DQN-based mutation strategy selection, DEDQN also employs a
”historical memory parameter adaptation mechanism” (similar to that in SHADE) to adapt the F and Cr
control parameters.
The reward hyperparameters could be used in DEDQN
reward_hyperparameters = {
# From Optimizer (DEDQN)
"survival": np.ndarray,
# survival counts per individual (length NP)
"pointer": int,
# index of the individual being updated
"population": np.ndarray,
# population array, shape [NP, dim]
"costs": np.ndarray,
# population costs, shape [NP,]
"parent_cost": float,
# cost of the pointer individual before generating trial
"trial_cost": float,
# cost of the trial (after mutation/crossover)
"gbest_cost": float,
# current global best cost
"median_cost": float,
# median of population costs
"mean_cost": float,
# mean of population costs
"std_cost": float,
# std of population costs
"diversity": float,
# mean std across dimensions (approx diversity)
"FEs": int,
# current number of function evaluations
"MaxFEs": int,
# maximum allowed function evaluations
"progress": float,
# normalized progress = FEs / MaxFEs in [0,1]
"action": int,
# chosen mutation strategy id (e.g. 0,1,2)
"generation": int,
# current generation index (approx FEs // NP)
"accepted": int,
# whether trial replaced parent (1) or not (0)
"delta_cost": float,
# parent_cost - trial_cost (>0 means improvement)
"gbest_improve": float,
# previous gbest_cost - current gbest_cost
"pointer_age": float,
# survival age of the current pointer individual
# From Agent (DQN) via action payload (agent_context)
"q_values": np.ndarray,
# Q(s, .), shape [n_env, n_act]
"greedy_action": np.ndarray,
# argmax_a Q(s,a), shape [n_env,]
"q_span": np.ndarray,
# per-env max(Q) - min(Q)
"q_entropy": np.ndarray,
# per-env entropy of softmax(Q / tau)
"recent_reward_mean": np.ndarray,# moving mean of last-K rewards per env
"recent_reward_max": np.ndarray, # moving max of last-K rewards per env
"training_step": int,
# RL training step counter
}
17


--- Page 18 ---
READY: Reward Discovery for Meta-Black-Box Optimization
B.2. RLDAS MetaData
RL-DAS (Deep Reinforcement Learning-based Dynamic Algorithm Selection)
The RL-DAS framework primarily focuses on Dynamic Algorithm Selection (DAS) in Black-Box Optimization
(BBO). It aims to address the limitation that a single algorithm’s effectiveness varies across different problems. It
does this by leveraging the complementary strengths of a group of algorithms and dynamically scheduling them
throughout the optimization process for a specific problem.
Algorithm Framework
RL-DAS employs an architecture centered around a deep reinforcement learning agent that interacts with an
optimization environment (which includes the problem, the population, and an algorithm pool).
• RL Agent: This is the core decision-maker, trained using a policy gradient method, specifically Proximal
Policy Optimization (PPO). The entire process is modeled as a Markov Decision Process (MDP).
• Algorithm Pool: This contains a finite set of L candidate algorithms. In the paper’s proof-of-principle study, this
pool consists of three advanced Differential Evolution (DE) algorithms: JDE21, MadDE, and NL-SHADE-RSP.
• Optimization Process: The optimization is divided into time intervals. At each interval (or decision step) t:
1. The framework extracts a State Feature from the current problem and population.
2. The RL agent receives this state and Selects an action at, which corresponds to choosing an algorithm
from the pool.
3. The chosen algorithm is executed for a period, updating the population.
4. The framework observes a reward and transitions to the next state, repeating the process.
Key Features & Mechanisms
• Dynamic Algorithm Scheduling: Unlike traditional (static) Algorithm Selection (AS) which picks one
algorithm for the entire run, RL-DAS dynamically switches between algorithms during the optimization process,
allowing it to achieve a comprehensively better performance than the best single algorithm in the pool.
• Algorithm Context Restoration: A critical mechanism that enables smooth switching between algorithms.
It uses a ”context memory” Gamma to save and restore the algorithm-specific internal states (like adaptive
parameters, statistical measures, or archives). This allows an algorithm to be warm-started from where it left
off, rather than reinitializing.
• Generality and Generalization: The framework is designed to be simple and generic, offering potential
improvements for a broad spectrum of evolutionary algorithms (EC), not just DE. Experiments showed it
has favorable generalization ability, achieving strong performance in zero-shot scenarios on unseen problem
classes.
The reward hyperparameters could be used in RLDAS
reward_hyperparameters = {
# Information from Optimizer
"last_cost": float,
# Global best cost before this step (population.gbest at step start)
"current_gbest": float,
# Global best cost after this step (population.gbest at step end)
"cost_scale_factor": float,
# Scale factor for costs (set to initial best cost at init)
"FEs": int,
# Current function evaluations used
"MaxFEs": int,
# Maximum function evaluations allowed
"action": int,
# Selected optimizer index in the pool (e.g., 0: NL_SHADE_RSP, 1: MadDE,
2: JDE21)
"problem": object,
# Problem instance (access lb/ub/optimum/eval/func if needed)
# Population handle (object) -- access fields as needed:
#
population.group:
np.ndarray, shape [NP, dim]
#
population.cost:
np.ndarray, shape [NP,]
#
population.gbest:
float
#
population.gbest_solution: np.ndarray, shape [dim,]
#
population.archive:
np.ndarray, shape [<= NA, dim]
#
population.NP / NA / dim: ints
"population": object,
18


--- Page 19 ---
READY: Reward Discovery for Meta-Black-Box Optimization
# Information from Agent (trainer side)
"agent_state": np.ndarray,
# State observed by the agent (batch/env-dependent)
"policy_entropy": np.ndarray,
# Policy entropy (encourages exploration), shape depends on agent
"value_estimation": np.ndarray,
# Critic value estimation
"log_probability": np.ndarray,
# Log prob of taken action
"gamma": float,
# Agent’s discount factor
"learning_rate": float,
# Agent’s current learning rate
}
B.3. RLEPSO MetaData
RLEPSO (Reinforcement Learning based Ensemble Particle Swarm Optimizer)
The RLEPSO algorithm focuses on integrating reinforcement learning with particle swarm optimization (PSO).
It aims to address the redundancy and difficulty of manual parameter tuning in complex PSO variants. By using
reinforcement learning for pre-training, RLEPSO automatically discovers effective parameter combinations, thereby
improving the algorithm’s robustness and ability to complete optimization tasks faster.
Algorithm Framework
RLEPSO employs a reinforcement learning framework to guide the swarm optimization process:
• RL Agent (Action Network): This is a reinforcement learning-based agent whose policy is trained using the
Proximal Policy Optimization (PPO) algorithm. This agent is responsible for outputting the configuration for
the lower-level PSO algorithm based on the optimization state.
• Optimization Process: In each round, the action network outputs specific control parameters (such as inertia
weights, acceleration coefficients, and mutation probabilities). The particle swarm performs optimization using
these parameters, and the RL agent receives a reward based on whether the global best value of the swarm has
improved.
Key Features & Mechanisms
• Ensemble of PSO Variants: To improve adaptability, RLEPSO integrates two robust PSO variants: Compre-
hensive Learning PSO (CLPSO) and Fitness-Distance-Ratio based PSO (FDR-PSO). It uses a combined
velocity update equation that incorporates components from both variants alongside the global and personal
bests.
• Automated Parameter Generation: Instead of using fixed or manually designed adaptive strategies,
RLEPSO uses the trained action network to dynamically generate 7 dimensional operating parameters
(w, c1, c2, c3, c4, Cmutation) based on the optimization progress.
• Multi-Swarm Strategy: To enhance population diversity and global search capabilities, the particles are
divided into 5 sub-swarms. Each sub-swarm independently utilizes its own set of running parameters generated
by the action network, while sharing the same global best experience.
• Mutation Mechanism: To prevent particles from being trapped in local optima, a mutation step is added after
velocity updating. If a random condition based on the generated mutation parameter is met, the particle’s
position is reinitialized within the solution space.
The reward hyperparameters could be used in RLEPSO
reward_hyperparameters = {
# From Optimizer (RLEPSO_Optimizer / PSO process)
"gbest_val": float,
# current global best objective value (the smaller the better)
"pre_gbest": float,
# previous global best value before update (to check improvement)
"fes": int,
# current number of function evaluations (FEs)
"maxFEs": int,
# maximum allowed number of function evaluations (MaxFEs)
"progress": float,
# normalized progress = fes / maxFEs, in [0, 1]
"NP": int,
# number of particles in the swarm (100 in RLEPSO)
19


--- Page 20 ---
READY: Reward Discovery for Meta-Black-Box Optimization
"dim": int,
# dimensionality of the optimization problem
"current_position": np.ndarray,
# current swarm positions, shape [NP, dim]
"velocity": np.ndarray,
# current swarm velocities, shape [NP, dim]
"c_cost": np.ndarray,
# current objective values of particles, shape [NP,]
"pbest_position": np.ndarray,
# personal best positions of particles, shape [NP, dim]
"pbest": np.ndarray,
# personal best values of particles, shape [NP,]
"gbest_position": np.ndarray,
# global best position, shape [dim,]
"gbest_index": int,
# index of the global best particle in the swarm
"no_improve": int,
# number of consecutive iterations without global best improvement
"per_no_improve": np.ndarray,
# per-particle stagnation counters, shape [NP,]
"n_group": int,
# number of sub-swarms (5 in RLEPSO)
"pci": np.ndarray,
# CLPSO learning probability for each particle, shape [NP,]
"log_index": int,
# current logging index (for cost curve)
"log_interval": int,
# log every N FEs
"cost_curve": list,
# history of gbest_val for plotting convergence
# From RL Agent / Action (PPO Actor output)
"action": np.ndarray,
# current PPO actor output action, shape [35,]
# corresponds to 5 sub-swarms * 7 parameters per group:
# (w, c_mutation, scale, c1, c2, c3, c4)
"log_prob": np.ndarray
float,
# log-probability of the sampled action under current policy
"entropy": np.ndarray
float,
# policy entropy for this step (for exploration measurement)
"training_step": int,
# global RL training step (from agent)
"training_progress": float,
# normalized training progress in [0,1]
"learning_rate": float,
# current learning rate of the optimizer
"clip_range": float,
# current PPO eps_clip (if exposed)
"gamma": float,
# discount factor used in PPO
"mean_cost": float,
# mean cost of the current population
"median_cost": float,
# median cost of the current population
"std_cost": float,
# standard deviation of the population costs
"diversity": float,
# population diversity approximated by std over positions
"gbest_improve": float,
# previous gbest_val - current gbest_val (>0 means improvement)
}
C. The reward found by READY
C.1. Reward for DEDQN
C.1.1. THOUGHT
My concept synthesizes the best ideas from the analysis: the robust trend-aware improvement from the current
reward, the strategic exploration concepts from the best-performing reward, and the phase-aware weighting from
the better-performing reward. I’ll create a hybrid reward that maintains DEDQN’s framework while incorporating
strategic exploration assessment.
Key improvements:
1. Strategic Exploration Assessment: Adapt the basin exploration concept to DE’s single population context. Detect
promising search regions by clustering based on cost and position, rewarding discovery of new high-quality
basins.
2. Dual-Baseline Trend Improvement: Combine short-term and long-term improvement baselines for stable credit
assignment during both rapid progress and plateaus.
3. Phase-Aware Component Weighting: Implement four search phases (Exploration, Exploitation, Refinement,
Stagnation) with tailored weights, adapting the better-performing reward’s approach to DE.
4. Survival-Aware Progress: Leverage DEDQN’s unique “survival” and “pointer age” metrics to assess individual
evolutionary success and population turnover.
20


--- Page 21 ---
READY: Reward Discovery for Meta-Black-Box Optimization
5. Strategic Backtracking Allowance: Permit temporary regression when accompanied by significant positional
shifts, enabling escape from local optima.
6. Q-Value Confidence Integration: Use Q-value entropy and span to assess action selection confidence, rewarding
decisive strategy choices.
The reward will have five components: (1) Trend Improvement, (2) Strategic Exploration, (3) Survival Efficiency,
(4) Action Confidence, and (5) Population Health. Weights adapt based on detected search phase and optimization
progress.
C.1.2. REWARD CODE
def reward(reward_hyperparameters):
import numpy as np
EPSILON = 1e-12
SHORT_WINDOW = 5
LONG_EMA_ALPHA = 0.1
gbest_cost = float(reward_hyperparameters["gbest_cost"])
gbest_improve = float(reward_hyperparameters.get("gbest_improve", 0.0))
FEs = int(reward_hyperparameters["FEs"])
MaxFEs = int(reward_hyperparameters["MaxFEs"])
progress = float(reward_hyperparameters["progress"])
population = np.array(reward_hyperparameters["population"])
costs = np.array(reward_hyperparameters["costs"])
mean_cost = float(reward_hyperparameters["mean_cost"])
median_cost = float(reward_hyperparameters["median_cost"])
diversity = float(reward_hyperparameters["diversity"])
std_cost = float(reward_hyperparameters["std_cost"])
accepted = int(reward_hyperparameters["accepted"])
delta_cost = float(reward_hyperparameters["delta_cost"])
parent_cost = float(reward_hyperparameters["parent_cost"])
trial_cost = float(reward_hyperparameters["trial_cost"])
survival = np.array(reward_hyperparameters["survival"])
pointer_age = float(reward_hyperparameters["pointer_age"])
q_values = reward_hyperparameters.get("q_values", None)
recent_reward_mean = reward_hyperparameters.get("recent_reward_mean", None)
q_entropy = reward_hyperparameters.get("q_entropy", None)
q_span = reward_hyperparameters.get("q_span", None)
NP = population.shape[0]
dim = population.shape[1]
progress = np.clip(progress, EPSILON, 1.0 - EPSILON)
remaining_ratio = 1.0 - progress
reward_component = {}
if ’improvement_history’ in reward_hyperparameters:
improvement_history = reward_hyperparameters[’improvement_history’]
if len(improvement_history) > 0:
recent_improvements = improvement_history[-min(SHORT_WINDOW, len(
improvement_history)):]
short_baseline = np.mean(recent_improvements) if len(recent_improvements) > 0
else EPSILON
else:
short_baseline = EPSILON
else:
short_baseline = EPSILON
if ’long_ema_improvement’ in reward_hyperparameters:
21


--- Page 22 ---
READY: Reward Discovery for Meta-Black-Box Optimization
long_baseline = float(reward_hyperparameters[’long_ema_improvement’])
else:
long_baseline = EPSILON
dynamic_scale = max(np.abs(mean_cost), std_cost, np.abs(gbest_cost), EPSILON)
trend_improvement = 0.0
if gbest_improve > EPSILON:
normalized_improvement = gbest_improve / (dynamic_scale + EPSILON)
short_ratio = normalized_improvement / (short_baseline + EPSILON)
long_ratio = normalized_improvement / (long_baseline + EPSILON)
if short_ratio > 1.0 and long_ratio > 1.0:
trend_factor = np.log1p(short_ratio * long_ratio)
bonus_multiplier = 1.5
elif short_ratio > 1.0 or long_ratio > 1.0:
trend_factor = np.log1p(max(short_ratio, long_ratio))
bonus_multiplier = 1.0
else:
trend_factor = np.log1p(min(short_ratio, long_ratio))
bonus_multiplier = 0.7
trend_improvement = np.tanh(trend_factor) * bonus_multiplier
if accepted == 1 and delta_cost > EPSILON:
local_relative = delta_cost / (np.abs(parent_cost) + EPSILON)
local_bonus = np.tanh(local_relative * 3.0) * 0.3
trend_improvement += local_bonus
elif gbest_improve < -EPSILON:
regression_magnitude = -gbest_improve / (np.abs(gbest_cost) + EPSILON)
trend_improvement = -0.3 * np.tanh(regression_magnitude * 10.0) * remaining_ratio
trend_improvement = np.clip(trend_improvement, -0.4, 1.2)
reward_component["trend_improvement"] = float(trend_improvement)
strategic_exploration = 0.0
if NP > 5:
position_std = np.std(population, axis=0)
scale_factor = np.mean(np.abs(position_std)) + EPSILON
simplified_clusters = []
cluster_qualities = []
for i in range(0, NP, max(1, NP // 10)):
if i >= NP:
break
is_new_cluster = True
for j, (cluster_center, _) in enumerate(simplified_clusters):
distance = np.linalg.norm(population[i] - cluster_center) / scale_factor
if distance < 0.5:
is_new_cluster = False
cluster_qualities[j] = min(cluster_qualities[j], costs[i])
break
if is_new_cluster:
simplified_clusters.append((population[i], costs[i]))
cluster_qualities.append(costs[i])
num_clusters = len(simplified_clusters)
discovery_score = np.tanh(num_clusters / max(5, 1))
if cluster_qualities:
best_cluster_quality = min(cluster_qualities)
22


--- Page 23 ---
READY: Reward Discovery for Meta-Black-Box Optimization
quality_ratio = (mean_cost - best_cluster_quality) / (np.abs(mean_cost) +
EPSILON)
quality_score = np.tanh(quality_ratio * 5.0)
else:
quality_score = 0.0
centroid = np.mean(population, axis=0)
distances = np.linalg.norm(population - centroid, axis=1)
normalized_costs = (costs - np.min(costs)) / (np.ptp(costs) + EPSILON)
if np.std(distances) > EPSILON and np.std(normalized_costs) > EPSILON:
fdc = np.corrcoef(distances, normalized_costs)[0, 1]
fdc = np.nan_to_num(fdc, nan=0.0)
if fdc < -0.3:
exploration_quality = np.tanh(-fdc * 2.0)
elif fdc > 0.3:
exploration_quality = -0.5 * np.tanh(fdc * 2.0)
else:
exploration_quality = 0.0
else:
exploration_quality = 0.0
strategic_exploration = 0.4 * discovery_score + 0.4 * quality_score + 0.2 *
exploration_quality
strategic_exploration = np.clip(strategic_exploration, -0.3, 1.0)
reward_component["strategic_exploration"] = float(strategic_exploration)
survival_efficiency = 0.0
if len(survival) > 0:
mean_survival = np.mean(survival)
max_survival = np.max(survival)
if max_survival > EPSILON:
survival_balance = 1.0 - (max_survival - mean_survival) / (max_survival +
EPSILON)
else:
survival_balance = 0.0
age_factor = np.tanh(pointer_age / max(10.0, 1.0))
if accepted == 1:
replacement_bonus = 0.3 * (1.0 - age_factor)
else:
replacement_bonus = -0.1 * age_factor
survival_efficiency = 0.6 * survival_balance + 0.4 * replacement_bonus
survival_efficiency = np.clip(survival_efficiency, -0.2, 0.8)
reward_component["survival_efficiency"] = float(survival_efficiency)
action_confidence = 0.0
if accepted == 1:
acceptance_bonus = 0.3
if delta_cost > EPSILON:
improvement_significance = delta_cost / (std_cost + EPSILON)
improvement_bonus = np.tanh(improvement_significance) * 0.4
acceptance_bonus += improvement_bonus
action_confidence = acceptance_bonus
if q_entropy is not None and q_span is not None:
if np.isscalar(q_entropy):
q_entropy_val = float(q_entropy)
23


--- Page 24 ---
READY: Reward Discovery for Meta-Black-Box Optimization
q_span_val = float(q_span)
else:
q_entropy_val = float(q_entropy[0]) if len(q_entropy) > 0 else 0.0
q_span_val = float(q_span[0]) if len(q_span) > 0 else 0.0
confidence_metric = q_span_val / (q_entropy_val + EPSILON)
q_confidence = np.tanh(confidence_metric) * 0.3
action_confidence += q_confidence
action_confidence = np.clip(action_confidence, -0.1, 1.0)
reward_component["action_confidence"] = float(action_confidence)
population_health = 0.0
if len(costs) > 3:
sorted_indices = np.argsort(costs)
quartile_size = max(1, len(costs) // 4)
top_costs = costs[sorted_indices[:quartile_size]]
middle_costs = costs[sorted_indices[quartile_size:3*quartile_size]]
top_improvement = np.mean(np.diff(np.sort(top_costs)[::-1])) if len(top_costs) > 1
else 0.0
middle_spread = np.std(middle_costs) if len(middle_costs) > 1 else 0.0
health_top = np.tanh(top_improvement / (std_cost + EPSILON)) if std_cost > EPSILON
else 0.0
health_middle = np.exp(-middle_spread / (std_cost + EPSILON)) if std_cost >
EPSILON else 0.0
mean_to_best = mean_cost / (np.abs(gbest_cost) + EPSILON)
convergence_tightness = np.exp(-mean_to_best + 1.0)
if diversity > EPSILON:
max_possible_diversity = np.sqrt(dim) + EPSILON
normalized_diversity = diversity / max_possible_diversity
diversity_metric = np.tanh(normalized_diversity * 2.0)
else:
diversity_metric = 0.0
population_health = 0.25 * health_top + 0.25 * health_middle + 0.25 *
convergence_tightness + 0.25 * diversity_metric
population_health = np.clip(population_health, -0.2, 1.0)
reward_component["population_health"] = float(population_health)
improvement_density = np.sum(costs < np.roll(costs, 1) - EPSILON) / max(len(costs), 1)
convergence_ratio = std_cost / (np.abs(mean_cost) + EPSILON)
if progress < 0.3 and improvement_density < 0.3 and convergence_ratio > 0.5:
search_phase = ’exploration’
phase_weights = np.array([0.2, 0.4, 0.1, 0.1, 0.2])
elif progress < 0.7 and gbest_improve > EPSILON and improvement_density > 0.4:
search_phase = ’exploitation’
phase_weights = np.array([0.4, 0.2, 0.2, 0.1, 0.1])
elif progress > 0.7 and convergence_ratio < 0.2:
search_phase = ’refinement’
phase_weights = np.array([0.3, 0.1, 0.3, 0.2, 0.1])
else:
search_phase = ’stagnation’
phase_weights = np.array([0.1, 0.5, 0.1, 0.2, 0.1])
phase_multiplier = {
’exploration’: 1.2,
’exploitation’: 1.5,
’refinement’: 1.3,
24


--- Page 25 ---
READY: Reward Discovery for Meta-Black-Box Optimization
’stagnation’: 0.8
}[search_phase]
reward_component["search_phase"] = search_phase
early_weight = 1.0 - progress
mid_weight = 4.0 * progress * (1.0 - progress)
late_weight = progress * progress
stage_weight = np.array([early_weight, mid_weight, late_weight])
stage_weight = stage_weight / (np.sum(stage_weight) + EPSILON)
component_weights = np.outer(phase_weights, stage_weight)
total_reward = (
component_weights[0, 0] * 2.0 * trend_improvement * early_weight +
component_weights[0, 1] * 2.0 * trend_improvement * mid_weight +
component_weights[0, 2] * 2.0 * trend_improvement * late_weight +
component_weights[1, 0] * 1.5 * strategic_exploration * early_weight +
component_weights[1, 1] * 1.5 * strategic_exploration * mid_weight +
component_weights[1, 2] * 1.5 * strategic_exploration * late_weight +
component_weights[2, 0] * 1.2 * survival_efficiency * early_weight +
component_weights[2, 1] * 1.2 * survival_efficiency * mid_weight +
component_weights[2, 2] * 1.2 * survival_efficiency * late_weight +
component_weights[3, 0] * 1.0 * action_confidence * early_weight +
component_weights[3, 1] * 1.0 * action_confidence * mid_weight +
component_weights[3, 2] * 1.0 * action_confidence * late_weight +
component_weights[4, 0] * 1.0 * population_health * early_weight +
component_weights[4, 1] * 1.0 * population_health * mid_weight +
component_weights[4, 2] * 1.0 * population_health * late_weight
)
total_reward *= phase_multiplier
if search_phase == ’stagnation’ and gbest_improve <= EPSILON:
stagnation_penalty = -0.2 * progress
total_reward += stagnation_penalty
dimensionality_factor = 1.0 / (1.0 + 0.002 * dim)
total_reward *= dimensionality_factor
total_reward = np.clip(total_reward, -1.0, 2.0)
reward_component["total"] = float(total_reward)
reward_component["phase_weights"] = phase_weights.tolist()
reward_component["stage_weights"] = stage_weight.tolist()
reward_component["dimensionality_factor"] = float(dimensionality_factor)
return float(total_reward), reward_component
C.2. Reward for RLDAS
C.2.1. THOUGHT
My reward concept adapts DEDQN’s sophisticated multi-component reward framework to RLDAS’s dynamic
algorithm selection context. The core idea is to translate DEDQN’s population-level assessment metrics to evaluate
algorithm effectiveness during scheduling. Key adaptations include:
1. Improvement
Measurement:
Calculate
relative
improvement
using
RLDAS’s
last cost
and
current gbest instead of DEDQN’s parent cost and trial cost. Use sigmoid scaling for sen-
sitivity.
25


--- Page 26 ---
READY: Reward Discovery for Meta-Black-Box Optimization
2. Population Quality Assessment:
Compute median, mean, std, and diversity metrics from RLDAS’s
population.cost array, analogous to DEDQN’s costs and diversity.
3. Progress Tracking: Use RLDAS’s FEs and MaxFEs to compute normalized progress, replacing DEDQN’s
progress.
4. Algorithm Effectiveness: Replace DEDQN’s mutation strategy assessment with algorithm selection effective-
ness. Use RLDAS’s action (algorithm index) and approximate strategic balance using policy entropy.
5. Convergence Acceleration: Model expected improvement based on remaining budget, similar to DEDQN’s
approach but using global best improvements.
6. Stagnation Handling: Implement progressive penalties during plateaus, with exploration encouragement via
population diversity metrics.
7. Adaptive Weighting: Maintain DEDQN’s phase-aware weighting scheme (early/mid/late) based on optimization
progress, with component weights shifting appropriately.
The reward will have six components: (1) Shaped Improvement, (2) Convergence Acceleration, (3) Swarm Quality,
(4) Strategic Balance, (5) Algorithm Success, and (6) Stagnation Management. All components are bounded and
weighted adaptively.
C.2.2. REWARD CODE
def reward(reward_hyperparameters):
import numpy as np
EPSILON = 1e-12
SMALL_VALUE = 1e-8
last_cost = reward_hyperparameters[’last_cost’]
current_gbest = reward_hyperparameters[’current_gbest’]
cost_scale_factor = reward_hyperparameters[’cost_scale_factor’]
FEs = reward_hyperparameters[’FEs’]
MaxFEs = reward_hyperparameters[’MaxFEs’]
action = reward_hyperparameters[’action’]
population_obj = reward_hyperparameters[’population’]
policy_entropy = reward_hyperparameters.get(’policy_entropy’, None)
training_progress = reward_hyperparameters.get(’training_progress’, 0.0)
current_step = reward_hyperparameters.get(’current_step’, 0)
reward_component = {}
population_costs = population_obj.cost
population_size = len(population_costs)
dim = population_obj.dim
progress = np.clip(FEs / max(MaxFEs, 1), EPSILON, 1.0 - EPSILON)
training_progress = np.clip(training_progress, EPSILON, 1.0 - EPSILON)
median_cost = np.median(population_costs)
mean_cost = np.mean(population_costs)
std_cost = np.std(population_costs) + EPSILON
population_array = population_obj.group
centroid = np.mean(population_array, axis=0)
distances = np.linalg.norm(population_array - centroid, axis=1)
spatial_diversity = np.std(distances) / (np.mean(distances) + EPSILON)
diversity = spatial_diversity
shaped_improvement = 0.0
26


--- Page 27 ---
READY: Reward Discovery for Meta-Black-Box Optimization
if last_cost > EPSILON and current_gbest < last_cost - EPSILON:
delta_cost = last_cost - current_gbest
relative_improvement = delta_cost / (np.abs(last_cost) + EPSILON)
shaped_improvement = 2.0 / (1.0 + np.exp(-5.0 * relative_improvement)) - 1.0
elif current_gbest > last_cost + EPSILON:
shaped_improvement = -0.2
reward_component[’shaped_improvement’] = shaped_improvement
convergence_acceleration = 0.0
if last_cost > EPSILON and current_gbest < last_cost - EPSILON:
gbest_improve = last_cost - current_gbest
remaining_fes = max(MaxFEs - FEs, 1)
actual_rate = gbest_improve / (np.abs(last_cost) + EPSILON)
expected_rate = 1.0 - np.exp(-remaining_fes / max(MaxFEs, 1))
acceleration_ratio = actual_rate / (expected_rate + EPSILON)
convergence_acceleration = np.tanh(acceleration_ratio - 1.0)
reward_component[’convergence_acceleration’] = convergence_acceleration
swarm_quality = 0.0
if median_cost > EPSILON and mean_cost > EPSILON:
improved_mask = population_costs < np.roll(population_costs, 1) - EPSILON
improvement_ratio = np.sum(improved_mask) / max(population_size, 1)
population_improvement = np.tanh(improvement_ratio * 2.0 - 1.0)
mean_to_best_ratio = mean_cost / (np.abs(current_gbest) + EPSILON)
convergence_metric = 1.0 / (std_cost + EPSILON)
distribution_quality = np.exp(-0.5 * (mean_to_best_ratio - 1.0)) * np.tanh(
convergence_metric / dim)
trial_median_impact = (median_cost - current_gbest) / (median_cost + EPSILON)
trial_mean_impact = (mean_cost - current_gbest) / (mean_cost + EPSILON)
individual_impact = 0.6 * np.tanh(trial_median_impact) + 0.4 * np.tanh(
trial_mean_impact)
swarm_quality = 0.4 * population_improvement + 0.4 * distribution_quality + 0.2 *
individual_impact
reward_component[’swarm_quality’] = swarm_quality
strategic_balance = 0.0
if diversity > EPSILON:
max_possible_diversity = np.sqrt(dim) + EPSILON
normalized_diversity = diversity / max_possible_diversity
convergence_measure = 0.0
if current_gbest > EPSILON and mean_cost > EPSILON:
convergence_measure = np.exp(-np.abs(current_gbest - mean_cost) / (np.abs(
current_gbest) + EPSILON))
exploration_weight = (1.0 - progress) * (1.0 - training_progress)
exploitation_weight = 1.0 - exploration_weight
diversity_component = np.tanh(normalized_diversity * 2.0)
convergence_component = np.tanh(convergence_measure * 3.0)
strategic_balance = exploration_weight * diversity_component + exploitation_weight
* convergence_component
reward_component[’strategic_balance’] = strategic_balance
algorithm_success = 0.0
if policy_entropy is not None:
optimal_entropy = 0.5 * (1.0 - progress)
entropy_match = 1.0 - np.abs(policy_entropy - optimal_entropy)
algorithm_success = entropy_match
27


--- Page 28 ---
READY: Reward Discovery for Meta-Black-Box Optimization
gbest_bonus = 3.0 if last_cost > EPSILON and current_gbest < last_cost - EPSILON else
0.0
strategy_success = gbest_bonus + algorithm_success
strategy_success = np.tanh(strategy_success / 4.0)
reward_component[’algorithm_success’] = strategy_success
stagnation_management = 0.0
if last_cost > EPSILON and current_gbest >= last_cost - EPSILON:
stagnation_duration = np.clip(progress * 10.0, 0.0, 5.0)
stagnation_penalty = -0.08 * stagnation_duration * (1.0 - progress)
position_range = np.ptp(population_array, axis=0)
coverage = np.mean(position_range) / (dim + EPSILON)
uniform_score = 1.0 - np.std(position_range) / (np.mean(position_range) + EPSILON)
potential_energy = coverage * uniform_score * (1.0 - progress)
stagnation_management = stagnation_penalty + potential_energy
reward_component[’stagnation_management’] = stagnation_management
early_weight = 1.0 / (1.0 + np.exp(10.0 * (progress - 0.3)))
mid_weight = 1.0 / (1.0 + np.exp(10.0 * np.abs(progress - 0.5) - 2.0))
late_weight = 1.0 / (1.0 + np.exp(-10.0 * (progress - 0.7)))
phase_sum = early_weight + mid_weight + late_weight + EPSILON
early_weight = early_weight / phase_sum
mid_weight = mid_weight / phase_sum
late_weight = late_weight / phase_sum
convergence_weight = np.clip(progress * 2.0, 0.0, 1.0)
exploration_weight = 1.0 - convergence_weight
component_weights = {
’shaped_improvement’: 0.4 * late_weight + 0.25 * mid_weight + 0.15 * early_weight,
’convergence_acceleration’: 0.25 * late_weight + 0.15 * mid_weight + 0.05 *
early_weight,
’swarm_quality’: 0.3 * late_weight + 0.25 * mid_weight + 0.2 * early_weight,
’strategic_balance’: 0.25 * early_weight + 0.2 * mid_weight + 0.1 * late_weight,
’algorithm_success’: 0.3 * convergence_weight + 0.2 * exploration_weight,
’stagnation_management’: 0.15
}
total_reward = (
component_weights[’shaped_improvement’] * reward_component[’shaped_improvement’] +
component_weights[’convergence_acceleration’] * reward_component[’
convergence_acceleration’] +
component_weights[’swarm_quality’] * reward_component[’swarm_quality’] +
component_weights[’strategic_balance’] * reward_component[’strategic_balance’] +
component_weights[’algorithm_success’] * reward_component[’algorithm_success’] +
component_weights[’stagnation_management’] * reward_component[’
stagnation_management’]
)
scale_factor = 1.0 / (1.0 + 0.002 * dim)
total_reward *= scale_factor
total_reward = np.clip(total_reward, -1.0, 1.5)
reward_component[’total_before_clip’] = total_reward
reward_component[’scale_factor’] = scale_factor
reward_component[’phase_weights’] = {’early’: early_weight, ’mid’: mid_weight, ’late’:
late_weight}
return float(total_reward), reward_component
28


--- Page 29 ---
READY: Reward Discovery for Meta-Black-Box Optimization
C.3. Reward discovered for RLEPSO
C.3.1. THOUGHT
The current reward is sophisticated but overly complex with many hand-tuned components. The better-performing
rewards from other clusters show that simplification and unification of core concepts leads to better performance.
My approach will:
1. Simplify improvement measurement: Use a robust relative improvement metric with log scaling that works
across all value scales, similar to the better-performing rewards.
2. Streamline multi-swarm coordination: Instead of complex cluster detection and overlap calculations, use simple
group-based metrics for coordination quality.
3. Unify diversity measurement: Combine position and fitness diversity into a single normalized metric, as shown
effective in other rewards.
4. Simplify state detection: Replace complex state machine with progress-based adaptive weighting, enhanced by
stagnation awareness.
5. Remove computationally expensive components: Eliminate cluster detection and complex basin exploration in
favor of simpler, more predictable signals.
6. Focus on RLEPSO-specific features: Leverage sub-swarm information (n group, group improvements) while
keeping calculations lightweight.
The new reward will maintain the multi-objective nature (improvement, diversity, coordination, stagnation awareness)
but with cleaner signal separation and fewer hyperparameters.
C.3.2. REWARD CODE
def reward(reward_hyperparameters):
import numpy as np
EPSILON = 1e-12
DIVERSITY_CRITICAL_THRESHOLD = 0.05
STAG_PENALTY_BASE = -0.15
STAG_PENALTY_DECAY = 0.85
IMPROVEMENT_WEIGHT_START = 0.6
IMPROVEMENT_WEIGHT_END = 1.2
DIVERSITY_WEIGHT_START = 0.7
DIVERSITY_WEIGHT_END = 0.2
COORDINATION_WEIGHT_START = 0.3
COORDINATION_WEIGHT_END = 0.1
EXPLORATION_BOOST = 0.4
gbest_val = float(reward_hyperparameters[’gbest_val’])
pre_gbest = float(reward_hyperparameters[’pre_gbest’])
progress = float(reward_hyperparameters[’progress’])
no_improve = int(reward_hyperparameters[’no_improve’])
current_position = np.array(reward_hyperparameters[’current_position’])
c_cost = np.array(reward_hyperparameters[’c_cost’])
n_group = int(reward_hyperparameters[’n_group’])
std_cost = float(reward_hyperparameters[’std_cost’])
diversity = float(reward_hyperparameters[’diversity’])
dim = int(reward_hyperparameters[’dim’])
NP = int(reward_hyperparameters[’NP’])
reward_component = {}
improvement_component = 0.0
29


--- Page 30 ---
READY: Reward Discovery for Meta-Black-Box Optimization
if gbest_val < pre_gbest - EPSILON:
abs_improvement = pre_gbest - gbest_val
denominator = max(abs(pre_gbest), EPSILON)
if denominator > 1.0:
rel_improvement = abs_improvement / denominator
improvement_component = np.log1p(rel_improvement)
else:
improvement_component = abs_improvement
reward_component[’improvement’] = float(improvement_component)
diversity_component = 0.0
if current_position is not None and c_cost is not None:
pos_std = np.std(current_position, axis=0)
mean_pos_std = np.mean(pos_std)
pos_norm = mean_pos_std / (np.sqrt(dim) + EPSILON)
cost_mean = np.mean(np.abs(c_cost)) + EPSILON
fitness_norm = std_cost / cost_mean
combined = 0.6 * np.tanh(pos_norm * 2.0) + 0.4 * np.tanh(fitness_norm * 2.0)
diversity_component = np.clip(combined, 0.0, 1.0)
reward_component[’diversity’] = float(diversity_component)
coordination_component = 0.0
if n_group > 1 and NP > n_group:
particles_per_group = NP // n_group
group_improvements = np.zeros(n_group)
group_diversities = np.zeros(n_group)
for g in range(n_group):
start_idx = g * particles_per_group
end_idx = (g + 1) * particles_per_group if g < n_group - 1 else NP
group_costs = c_cost[start_idx:end_idx]
group_positions = current_position[start_idx:end_idx]
if len(group_costs) > 1:
group_best = np.min(group_costs)
group_prev_key = f’group_{g}_prev_best’
group_prev = reward_hyperparameters.get(group_prev_key, group_best)
if group_best < group_prev - EPSILON:
group_improvements[g] = (group_prev - group_best) / (abs(group_prev) +
EPSILON)
centroid = np.mean(group_positions, axis=0)
distances = np.linalg.norm(group_positions - centroid, axis=1)
group_diversities[g] = np.mean(distances) / (np.sqrt(dim) + EPSILON)
improvement_spread = np.std(group_improvements) / (np.mean(group_improvements) +
EPSILON)
diversity_spread = np.std(group_diversities) / (np.mean(group_diversities) +
EPSILON)
coordination_component = 0.5 * np.tanh(improvement_spread) + 0.5 * np.tanh(
diversity_spread)
coordination_component = np.clip(coordination_component, 0.0, 1.0)
reward_component[’coordination’] = float(coordination_component)
stagnation_component = 0.0
if improvement_component <= EPSILON:
stagnation_component = STAG_PENALTY_BASE * (STAG_PENALTY_DECAY ** min(no_improve,
20))
30


--- Page 31 ---
READY: Reward Discovery for Meta-Black-Box Optimization
reward_component[’stagnation’] = float(stagnation_component)
progress_clipped = np.clip(progress, 0.0, 1.0)
improvement_weight = IMPROVEMENT_WEIGHT_START + (IMPROVEMENT_WEIGHT_END -
IMPROVEMENT_WEIGHT_START) * progress_clipped
diversity_weight = DIVERSITY_WEIGHT_START + (DIVERSITY_WEIGHT_END -
DIVERSITY_WEIGHT_START) * progress_clipped
coordination_weight = COORDINATION_WEIGHT_START + (COORDINATION_WEIGHT_END -
COORDINATION_WEIGHT_START) * progress_clipped
if diversity_component < DIVERSITY_CRITICAL_THRESHOLD and stagnation_component <
-0.05:
diversity_weight += EXPLORATION_BOOST
coordination_weight += EXPLORATION_BOOST * 0.5
total_reward = (
improvement_weight * improvement_component +
diversity_weight * diversity_component +
coordination_weight * coordination_component +
stagnation_component
)
scale_factor = 1.0 / (1.0 + 0.001 * dim)
total_reward *= scale_factor
total_reward = float(np.clip(total_reward, -1.0, 1.0))
reward_component[’improvement_weight’] = float(improvement_weight)
reward_component[’diversity_weight’] = float(diversity_weight)
reward_component[’coordination_weight’] = float(coordination_weight)
reward_component[’total_reward’] = float(total_reward)
return float(total_reward), reward_component
31
