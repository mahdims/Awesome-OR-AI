--- Page 1 ---
Leveraging Large Language Models to Develop
Heuristics for Emerging Optimization Problems
Thomas Bömer1[0000−0003−4979−7455], Nico Koltermann1[0009−0008−0359−0452],
Max Disselnmeyer2[0009−0008−5689−2235], Laura Dörr2[0000−0002−8007−1815], and
Anne Meyer2[0000−0001−6380−1348]
1 TU Dortmund University, Leonhard-Euler-Straße 5, 44227 Dortmund, Germany
{thomas.boemer,nico.koltermann}@tu-dortmund.de
2 Karlsruhe Institute of Technology, Kriegsstraße 77, 76133 Karlsruhe, Germany
{max.disselnmeyer,laura.doerr,anne.meyer}@kit.edu
Abstract. Combinatorial optimization problems often rely on heuristic
algorithms to generate efficient solutions. However, the manual design
of heuristics is resource-intensive and constrained by the designer’s
expertise. Recent advances in artificial intelligence, particularly large
language models (LLMs), have demonstrated the potential to automate
heuristic generation through evolutionary frameworks. Recent works
focus only on well-known combinatorial optimization problems like
the traveling
salesman
problem and
online bin
packing problem
when designing constructive heuristics. This study investigates whether
LLMs can effectively generate heuristics for niche, not yet broadly
researched optimization problems, using the unit-load pre-marshalling
problem as an example case. We propose the Contextual Evolution of
Heuristics (CEoH) framework, an extension of the Evolution of Heuristics
(EoH) framework, which incorporates problem-specific descriptions
to enhance in-context learning during heuristic generation. Through
computational experiments, we evaluate CEoH and EoH and compare
the results. Results indicate that CEoH enables smaller LLMs to generate
high-quality heuristics more consistently and even outperform larger
models. Larger models demonstrate robust performance with or without
contextualized prompts. The generated heuristics exhibit scalability to
diverse instance configurations.
Keywords: pre-marshalling · large language models · automated
heuristic design.
1
Introduction
Combinatorial optimization problems are pivotal in logistics, manufacturing,
and supply chain management. Heuristic algorithms are commonly employed for
such problems because they can provide good solutions quickly, making them
suitable for real-world applications. However, designing heuristics traditionally
requires significant human expertise and is time-consuming, making the process
financially expensive.
arXiv:2503.03350v1  [cs.AI]  5 Mar 2025


--- Page 2 ---
2
T. Bömer et al.
Automated heuristic design reduces reliance on specialized human expertise
[4]. Genetic programming is commonly used to evolve program code for solving
optimization problems [14]. These methods are effective but rely on predefined
search spaces. Human input determines permissible primitives and mutation
operations for generating heuristics. This dependency may limit the discovery of
novel approaches, especially in complex or less researched domains.
Recent advancements in artificial intelligence have introduced large language
models (LLMs) as a new resource for tackling optimization [28]. Though LLMs
are primarily recognized for their natural language generation capabilities,
studies show they can generate code and solve mathematical problems as
well [1,19]. This broader skill set may help address the challenge of heuristic
generation without relying on large, predefined search spaces.
In combinatorial optimization, LLMs generally play two roles [17]: optimizers
or designers. Optimizer-based approaches request the LLM to solve the
problem and return a solution [28,11], which shows moderate performance and
little scalability. Designer-based approaches request LLMs to create heuristics,
drawing on the models’ ability to generate and refine heuristics [26,16]. Although
promising, most designer-based studies focus on well-known problems like the
traveling salesperson problem (TSP) [16,29] and online bin packing problem
(oBPP) [26,16,5] when designing constructive heuristics. Their usage in niche
optimization problems remains unexplored.
This work investigates whether LLMs can generate effective constructive
heuristics for a specialized yet practically important niche optimization problem:
the unit-load pre-marshalling problem (UPMP) [23]. The UPMP arises in
block-stacking warehouses where unit loads must be rearranged to remove
blockages that block higher-priority loads. It requires a structured procedure
to select moves to reach a blockage-free state. We embed the generated heuristic
in a tree search framework. First, each possible reshuffling move is branched
from an initial warehouse state. Then, the LLM-generated heuristic rates each
resulting warehouse state. The highest-rated state is selected to proceed with
the next iteration. This cycle repeats until a blockage-free state is achieved.
We find the UPMP a strong candidate for testing LLM-based heuristic
design in niche optimization problems for the following reasons: The problem
description is most likely less prevalent in the training data of LLMs. Very few
or no heuristics code bases exist online on which an LLM may be trained. Related
problems - such as the container pre-marshalling problem (CPMP) [12] and block
relocation problem [13] - share similar constraints, but are not nearly as widely
studied as better-known problems like TSP or oBPP.
We introduce the Contextual Evolution of Heuristics (CEoH) framework,
an extension of the Evolution of Heuristics framework (EoH) [16] that
incorporates problem-specific details into the heuristics design process. By
adding a “contextual” component to each prompt, CEoH enables LLMs to
better understand the problem and its constraints. The result is a more
informed heuristic-generation process, where the model uses in-context learning
to generate heuristics tailored to the UPMP.


--- Page 3 ---
Leveraging LLMs to Develop Heuristics for Emerging Optimization Problems
3
The rest of the paper is organized as follows. Section 2 defines the UPMP.
Section 3 reviews relevant research on pre-marshalling and LLM-based heuristic
design. Section 4 introduces the CEoH framework. Section 5 presents the results
of computational experiments. Finally, Section 6 summarizes the findings and
outlines future research directions.
2
The Unit-load Pre-marshalling Problem
The UPMP, introduced by [23], is about sorting unit loads using autonomous
mobile robots (AMR) in a block-stacking warehouse according to predefined
priorities to remove blockages. A block-stacking warehouse is a storage system
in which unit loads such as pallets are stacked without additional infrastructure.
During pre-marshalling, no unit load enters or leaves the warehouse. A unit load
is deemed blocking if it hinders access to a unit load with a higher priority class.
Priority classes can, for example, be assigned based on the expected retrieval
time.
Row
Tier
1
2
3
1
2
3
2
1
Column
y
x
Bay
z
AMR
Fig. 1: Representation of a two-tier multibay block-stacking warehouse with three
rows and columns per bay and four bays in total [22]. This configuration displays
a 3x3 bay layout and 2x2 warehouse layout.
The warehouse consists of a set of interconnected bays B. Each bay b ∈
{1, ..., |B|} can be accessed from up to four access directions. Unit loads are
placed inside a bay in a shared storage policy. No empty gaps inside the bay
are allowed. Figure 1 illustrates the configuration of a block-stacking warehouse
consisting of four bays arranged in a square layout. Let C be the set of columns,
R the set of rows, and T the set of tiers. Each bay features a three-dimensional
grid of storage slots. Each storage slot is denoted by column c ∈{1, ..., |C|}, row
r ∈{1, ..., |R|}, and tier t ∈{1, ..., |T |}. Let P be the set of priority classes. The
grid is filled with unit loads categorized into priority classes. The priority class


--- Page 4 ---
4
T. Bömer et al.
in a grid position is denoted by pcrt ∈{1, ..., |P|}. Let A be the set of access
points. An access point a ∈{1, ..., |A|}, is located in the aisle space in front
of the bay, determining the slots that can be accessed from that point. Slots
that are accessed from the same access point are called lanes. Only the topmost,
outermost unit load of a lane is accessible.
In this study, we focus on the UPMP with only one-tier and one access
direction. [23] find the one access direction variant is the most challenging
because blockages cannot be avoided by selecting favorable access directions
for each unit load.
Figure 2 shows a one-tier single-bay warehouse problem instance from a
top-down perspective. The unit loads can only be accessed from the north. In
the initial state 0, three unit loads are marked as blocking. They block access to
higher-priority class unit loads or need to be removed to resolve a blockage. All
blockages are cleared after three moves (state 3).
State
0
1
2
3
2
4
5
3
2
2
2
1
2
3
4
5
3
3
2
4
5
3
2
1
2
3
4
5
3
2
2
2
4
5
2
2
1
2
3
4
5
2
3
2
2
4
5
3
2
3
2
4
5
3
2
1
1
Column
Row
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4
Fig. 2: Example sequence of moves to solve an instance. Top-down view on a
single-bay. Unit loads can be accessed from the north direction only.
3
Related Work
In this Section, we first present related work on pre-marshalling problems.
Following this, we present studies employing LLMs in an evolutionary framework
to develop heuristics for combinatorial optimization problems.
3.1
Related Work on Pre-marshalling
For the UPMP in single-bay block-stacking warehouses, [23] introduce a two-step
approach. First, the authors use a network flow formulation to decompose the
warehouse bays into virtual lanes. Afterward, an A*-algorithm-based tree search
solves the problem with minimizing the number of moves. The approach in [23]
for a single-bay warehouse is extended by the authors of [22] to a multibay
warehouse. The approach minimizes the loaded move distance for the minimal
number of reshuffling moves. [2] extend the approach of [22] by proposing a


--- Page 5 ---
Leveraging LLMs to Develop Heuristics for Emerging Optimization Problems
5
sequential approach that assigns the moves found by the tree search to robots
according to move dependencies using a mixed-integer formulation.
The pre-marshalling of unit loads in a block-stacking warehouse is closely
related to the CPMP in harbor logistics. The CPMP aims to rearrange stacks
of containers within a bay until no blockages persist. Solution approaches exist
in the form of greedy heuristics [8], tree search approaches [3,27,10], dynamic
programming [24], integer programming [15,21], and constraint programming
[25,12]. Since cranes in maritime harbors can often only operate within one bay,
no adaptation of the CPMP considers multiple blocks (here: bays).
For the UPMP and CPMP heuristic and exact approaches have been
developed to minimize the number of moves. However, the design required
extensive algorithmic and domain knowledge.
3.2
Application of LLMs in Combinatorial Optimization
In this section, we present studies that use LLMs to generate constructive
heuristics for combinatorial optimization problems in an evolutionary framework.
FunSearch, proposed in [26], is the first approach that pairs a pre-trained
LLM with an evolutionary procedure to explore algorithms. FunSearch starts
with a population of manually designed very basic, low-scoring heuristics. Each
heuristic is represented by program code. An LLM is requested to generate a new
heuristic based on parent heuristics. The generated heuristic is evaluated on test
instances and saved to a database. The heuristics are represented as program
code.
The authors of [16] introduce Evolution of Heuristics (EoH) as an extension
of Funsearch [26]. In EoH, heuristic ideas are represented as natural language
descriptions called ’thoughts’ and program code. This approach evolves thoughts
and program code simultaneously, leveraging the models to generate and
refine heuristics effectively. Further, EoH introduces two exploration and
three modification prompt strategies. The results show that EoH outperforms
handcrafted heuristics and FunSearch, while using significantly fewer LLM
queries (around one million queries in FunSearch vs. 2000 queries EoH for oBPP).
The authors of [29] propose the Reflective Evolution (ReEvo) framework.
ReEvo uses evolutionary search combined with model-generated reflections to
explore and modify heuristics more efficiently. The iterative process includes
the following five steps: selection, short-term reflection, crossover, long-term
reflection, and elitist mutation. Selection selects random parent heuristics; the
short-term reflection analyzes the selected heuristics and provides hints for
evolutionary search; the LLM performs a crossover mutation by generating
a new heuristic based on the parent heuristics, their relative performance,
and reflector output; the long-term reflection accumulates expertise among
short term reflections; finally in the elitist mutation an LLM samples multiple
heuristics based on the current best heuristic and the long-term reflection.
The latest approach we consider, HSEvo [5], aims to balance diversity and
optimization performance by leveraging harmony search alongside traditional
evolutionary operators. The framework introduces two diversity measurement


--- Page 6 ---
6
T. Bömer et al.
metrics to quantify heuristic exploration and ensure diversity in the search space.
HSEvo employs a multi-stage optimization pipeline, including flash reflection,
an efficient alternative to ReEvo’s reflection mechanism, elitist mutation to
refine top-performing heuristics, and harmony search to fine-tune heuristic
parameters. Experimental results demonstrate that HSEvo achieves superior
heuristic diversity and optimization performance, outperforming FunSearch,
EoH, and ReEvo in both solution quality and computational efficiency.
These studies demonstrate the potential of LLMs in generating constructive
heuristics for combinatorial optimization problems. However, the approaches
focus on well-established optimization problems as summarized in Table 1,
leaving niche problems under-researched. It is unclear whether LLMs can
generate good constructive heuristics for problems for which they have less
contextual knowledge. This study aims to bridge this gap by applying
LLM-generated constructive heuristics to the UPMP.
Table 1: Combinatorial optimization problems addressed with the design of a
constructive heuristic in related papers.
Paper Framework TSP oBPP UPMP
[26]
FunSearch
✓
[16]
EoH
✓
✓
[29]
ReEvo
✓
[5]
HSEvo
✓
CEoH
✓
4
Contextual Evolution of Heuristics Framework
This section describes the Contextual Evolution of Heuristics (CEoH). The core
idea of CEoH is to extend EoH [16] with an additional optimization problem
description in the prompt to leverage in-context learning. Figure 3 shows the
concept of CEoH.
Evolutionary Procedure.
CEoH is an iterative evolutionary framework that
generates and evolves heuristics. Each heuristic h ∈H = {1, 2, . . . , |H|} is
represented by program code and thoughts. The program code is a Python
function with a defined input and output. The thoughts describe the core ideas
the LLM had while generating the heuristic.
The approach iterates a set of generations G = {1, 2, . . . , |G|}. The population
of heuristics in generation g ∈G is denoted as Pg = {1, 2, . . . , |Pg|}. In each
generation g, a set of prompt strategies S = {1, 2, . . . , |S|} is iterated to generate
the population of heuristics Pg based on parent heuristics using a pre-trained
LLM. Each prompt strategy s ∈S is used ¯r times. Hence, |S| · ¯r heuristics


--- Page 7 ---
Leveraging LLMs to Develop Heuristics for Emerging Optimization Problems
7
Evaluation
Specifications
1. Exploration
2. Modification
Prompt Strategies
+ problem
description
Pre-trained LLM
Code
Thoughts
Programs Database
Code 1
Thoughts 1
Code 2
Thoughts 2
Code 3
Thoughts 3
Code n
Thoughts n
...
Best heuristic code
Fig. 3: The CEoH framework evolves code and thoughts while using an additional
problem description.
are generated each generation g for the population Pg. Each new heuristic h
is evaluated on a set of problem instances I = {1, 2, . . . , |I|} and assigned a
fitness value f I(h). Then, the heuristic h is added to the current population Pg
and the next heuristic is prompted. After a full generation (|S| · ¯r prompts), the
current population’s best ¯n heuristics are taken to the next generation. Before
the iterative procedure, an initialization prompt is called 2 · ¯n times to generate
P0.
Prompt Strategies For the prompt strategies, we adopt the initialization prompt
I0, the exploration strategy prompts E1 and E2, and the modification strategy
prompts M1 and M2 from [16]. We briefly describe the prompt strategies for
completeness.
I0: Generate a heuristic to solve the optimization problem.
E1: Generate a completely different heuristic based on ¯p parent heuristics
selected from the current population.
E2: Generate a new heuristic inspired by the same core idea as ¯p parent
heuristics selected from the current population.
M1: Modify a parent heuristic from the current population for better
performance.
M2: Modify the parametrization of a parent heuristic from the current
population for better performance.
In EoH [16] the authors only provide a very brief task description. This may
be sufficient for well-known problems like the oBPP and TSP. However, we argue
that additional problem description helps to design heuristics in less-known niche
optimization problems. Hence, in contrast to EoH we enrich the prompt with
the additional problem description.
Each prompt follows the same structure: (1) task description, (2) additional
problem description (new in CEoH), (3) parent heuristic(s) (not in I0), (4)
strategy-specific output instructions, and (5) additional instructions. We detail


--- Page 8 ---
8
T. Bömer et al.
each component in the following: The (1) task description informs the LLM about
the optimization problem and how the heuristic will be used in a bigger context
- for example, to score warehouse states in a tree search. The (2) additional
problem description enhances in-context learning by specifying the expected
input’s data structure and clarifying how it should be interpreted within the
problem domain. This section also includes examples of program input and
output to illustrate expected behavior and explicit instructions requiring the
model to thoroughly analyze the additional problem description. The inclusion
of (3) parent heuristic(s) in the form of program code and corresponding thoughts
facilitates few-shot learning, ensuring the LLM can generalize from prior
examples. The (4) strategy-specific output instructions guide the generation
process by incorporating a prompt-strategy-specific sentence, instructions to
formulate the thoughts behind a new heuristic, and a program code for that
heuristic with defined input and output. (5) Additional instructions impose key
implementation constraints. These include specifying input and output data
types while discouraging nested methods to minimize coding errors. To improve
solution robustness, the use of random components is restricted. Self-consistency
is reinforced to ensure that the generated Python function adheres to previously
formulated thoughts [18]. Finally, the LLM is instructed not to provide additional
explanations to optimize prompt efficiency by reducing generated tokens.
Fitness function. Each heuristic h has to be evaluated for its fitness. Let I be
the set of problem instances. Let mi be the number of moves the heuristic h
needs to solve problem instance i ∈I. Let mlb
i
be the lower bound of moves
to solve the problem instance i. We obtain the lower bound mlb
i according to
the approach of [23]. We assume a heuristic to be unable to solve an instance
i if a solution was not found after a maximum move number mmax. mmax is
selected significantly larger than mlb
i . We set mi = mmax, if mmax is reached
in the evaluation thereby effectively limiting the maximum fitness value of a
heuristic. This fitness function design promotes heuristics that are able to solve
a wide range of problem instances. Hence, we first search for a heuristic that
can solve all problem instances and then for a heuristic that solves all problem
instances efficiently with minimal moves. The Equation 1 calculates the fitness
as the average relative difference between the heuristic solution mi and the lower
bound mlb
i . We seek to minimize the fitness function value. A heuristic h with
f I(h) = 0.2 requires on average 20 % moves more on the problem instances I
than the lower bound. Please note that the lower bound mlb
i is not necessarily
equal to the optimal solution. Hence, a fitness of zero may not be achievable.
f I(h) = 1
|I|
X
i∈I
mi −mlb
i
mlb
i
(1)


--- Page 9 ---
Leveraging LLMs to Develop Heuristics for Emerging Optimization Problems
9
5
Computational Experiments
We conducted extensive experiments 3 to evaluate the effect of CEoH introduced
in Section 4. We apply the CEoH and EoH [16] framework to the UPMP and
compare the generated heuristics.
Parameters.
We adapted the same parameters for CEoH and EoH. As
pre-trained LLMs we consider the open-source models: Gemma2:27b [9],
Qwen2.5-Coder:32b [7], and DeepseekV3:685b [6] and the closed-source
models: GPT4o:2024-08-06 and GPT4o:2024-11-20 [20]. We select different
LLMs to observe the effect of context size and coding focus. Among
these, Qwen2.5-Coder:32b and DeepseekV3:685b are explicitly optimized
for code-related tasks, incorporating specialized pretraining on programming
languages
and
structured
reasoning.
In
contrast,
the
GPT4o
LLMs
are
general-purpose but have demonstrated strong coding capabilities. All LLM
sampling hyperparameters, such as temperature and top-p, are at their default
value to limit the design space of our study. We execute 10 experiment runs
(see Section 4 - Evolutionary Procedure) for CEoH and EoH with each LLM.
The number of generations is set to 20. The ¯n = 20 best heuristics are taken
to the next generation. We use the prompt strategy I0 for the initialization and
the four prompt strategies E1, E2, M1, and M2 to evolve the heuristics. The
number of parent heuristics for E1 and E2 is set to p = 5. The initialization
prompt I0 is called 40 times. The 20 best heuristics form the initial population.
The other prompt strategies (E1, E2, M1, and M2) are each called ¯r = 20 times
per generation. Hence, we perform 20·4·20 = 1600 prompts in one experimental
run to evolve the initial heuristics.
The evaluation instances have an average lower bound of 11.8 moves. Hence,
for the evaluation of heuristics, we set a sufficiently large maximum move limit
of mmax = 100 moves to penalize unsolved instances. The configuration of
the evaluation instances is: 5x5 bay layout, 1x1 warehouse layout, one access
direction (north), one-tier, five priority classes, and 60 % fill percentage. A 5x5
bay layout describes bays with five rows and five columns - 25 slots for the
one-tier case. 60 % of these slots are filled with priority classes of one (highest)
to five (lowest). A 1x1 warehouse layout denotes a warehouse that consists
of a single bay. One-tier means that no stacking is performed. We consider
10 instances (seeds 0-9) for the evaluation. All heuristics were evaluated on a
machine with an AMD EPYC 7401P processor and 64 gigabytes of RAM.
Fitness. Figure 4 shows the fitness of the best heuristics found up to each
generation and experiment run. The opaque lines represent the experiment runs
that yielded the best heuristics in the final population for each framework.
CEoH demonstrates superior performance for the models: Gemma2:27b,
Qwen2.5-Coder:32b, and DeepSeekV3:685b. The additional problem context
3 The source code to reproduce the experiments can be found in:
https://github.com/nico-koltermann/contextual-evolution-of-heuristics


--- Page 10 ---
10
T. Bömer et al.
10
20
Generation
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1e+01
Best Fitness (Log Scale)
Gemma2:27b
10
20
Generation
Qwen2.5-Coder:32b
10
20
Generation
DeepSeekV3:685b
10
20
Generation
GPT4o:2024-08-06
10
20
Generation
GPT4o:2024-11-20
Framework
EoH
CEoH
Fig. 4: Best found heuristic across generations for each model and experiment
run. The best run for CEoH and EoH is shown with opacity. Lower values indicate
better performance.
in CEoH enables smaller models, especially, to generate overall better heuristics
and to generate good heuristics more robustly. However, for the large models
GPT4o:2024-08-06 and GPT4o:2024-11-20 CEoH and EoH reliably found
heuristics with a fitness below 27 % except for two EoH outlier runs. For the
GPT4o:2024-08-06 CEoH generally generates worse heuristics than EoH. This
suggests that the added problem description in CEoH may have restricted the
model’s ability to leverage its pre-existing optimization knowledge effectively.
The
overall
best
heuristic
in
all
experiment
runs
is
generated
by
Qwen2.5-Coder:32b in the CEoH framework with a fitness of 12.5 %. The best
heuristic for the EoH framework is generated by GPT4o:2024-08-06 with a
fitness of 14.3 %.
Generated heuristic code. Figure 5 compares the best-scored heuristics generated
by (a) Qwen2.5-Coder:32b CEoH and (b) GPT4o:2024-08-06 EoH.
(a) The Qwen2.5-Coder:32b CEoH heuristic evaluates lanes based on
priority balance, blocking penalties, and density weighting. It rewards sequences
where unit loads follow a decreasing priority order and penalizes configurations


--- Page 11 ---
Leveraging LLMs to Develop Heuristics for Emerging Optimization Problems
11
that create blockages. The heuristic also incorporates a dynamic penalty scaling
mechanism, where higher-density lanes and misplaced units receive greater
penalties. Additionally, non-blocking lanes gain extra bonuses.
(b) The GPT4o:2024-08-06 EoH heuristic employs a logistic transformation
approach to dynamically adjust scores based on priority values and lane
positioning. It applies exponential penalties to blocking unit loads while
rewarding accessible, well-ordered stacks.
Notably, (b) GPT4o:2024-08-06 EoH refers to lanes as "stacks", a term
associated with container pre-marshalling. We do not use the word "stack"
in any prompt. We hypothesize, this suggests that GPT4o:2024-08-06 EoH
drew from related optimization problems when designing its heuristic, whereas
Qwen2.5-Coder:32b CEoH heuristic relied more on the explicit problem context
provided by CEoH. In all GPT4o:2024-08-06 experiment runs, the term "stack"
occurred 86,291 times within the EoH framework and 63 times within the
CEoH framework. For the same model, the term "lane" appeared 22 times in
heuristics generated within the EoH framework and 121,059 times in heuristics
generated within the CEoH framework. The other LLMs show a similar behavior
as GPT4o:2024-08-06.
Further Problem Instances. To assess the applicability of the generated heuristics
to various instance configurations, we evaluated the best-performing heuristic
from each framework (see Figure 5) on additional problem instances. The
performance was also compared with the state-of-the-art optimal A* approach
for the UPMP proposed by [23].
We vary the problem instance parameters bay layout, warehouse layout, and
fill percentage. Table 2 shows the mean move lower bound [23] for the evaluated
instances for 60 % and 80 % fill percentage. An increase in fill percentage, bay
layout size, and warehouse layout size causes additional moves. The largest
instances need on average at least 157.53 moves to be solved. Considering this, we
set the maximum move limit to mmax = 1000 moves to ensure feasibility for all
instance configurations. While the warehouse layout size especially contributes to
the number of moves, the fill percentage and bay layout size affect the complexity
of the problem instance. A higher fill percentage reduces the repositioning
options while maintaining the same number of slots. The larger bay layout size
causes deeper blockages which are harder to resolve.
Table 3 summarizes the performance of the best Qwen2.5:32b-Coder CEoH
heuristic and best GPT4o:2024-08-06 EoH heuristic across various problem
parameterizations. The top row displays the performance for the evaluation
instances used in the evolutionary process (seed 0–9). Both heuristics perform
very well on these evaluation instances. It is important to note that the
fitness for A* is not necessarily zero, since the fitness is computed relative
to the lower bound of moves. For example, the A*’s 8.15 % fitness value for
a 60 % fill percentage, 5x5 bay layout, and 1x1 warehouse layout (seed 0-9)
indicates that the optimal solution has on average 8.15 % more moves than the
lower bound, highlighting a 12.5 % −8.15 % = 4.35 % optimality gap for the


--- Page 12 ---
12
T. Bömer et al.
1
"""
2
The new
algorithm
scores
each
warehouse
state by
3
evaluating
the
accessibility
of high -priority
4
items
with
penalties
for
blocking
units
that
are
5
more
severe if they
block
higher
priority
items ,
6
rewards
for
lanes
with
minimal
blocking , and
7
incorporates a novel
scoring
mechanism
using
8
linear
decay
for
blocked
items
and
priority -based
9
bonuses
adjusted
by lane
density
with
modified
10
parameters.
11
"""
12
13
def
select_next_move ( warehouse_states ):
14
scores = []
15
for
state in
warehouse_states :
16
score = 0
17
total_units = sum(len ([ unit
for
unit
18
in lane if unit != 0])
19
for
lane in state)
20
num_lanes = len(state)
21
22
for i, lane in
enumerate(state ):
23
highest_priority_seen = float(’inf’)
24
blocking_occurred = False
25
non_zero_count = len ([ unit
for
unit
26
in lane if unit != 0])
27
density_weight = (( non_zero_count
28
/ total_units)
29
** 2 if
total_units
30
> 0 else 1)
31
block_penalty_factor = (4 + density_weight
32
* num_lanes * 1.2)
33
priority_balance = sum(unit * (5 - unit)
34
for
unit in lane)
35
36
for j, unit in
enumerate(reversed(lane )):
37
if unit != 0:
38
if unit > highest_priority_seen :
39
penalty = (( unit ** 2)
40
* block_penalty_factor
41
* (0.9 ** j))
42
score
-= penalty
43
blocking_occurred = True
44
else:
45
highest_priority_seen = unit
46
47
score += ( priority_balance * density_weight
48
* 1.8)
49
if not
blocking_occurred :
50
score += ( non_zero_count
** 2
51
* (1 + density_weight * 0.6))
52
53
scores.append(score)
54
return
scores
(a) Qwen2.5:32b-Coder CEoH
1
"""
2
This
heuristic
implements
an
exponential
function
3
to
further
penalize
blocked
high -priority
loads
4
and
further
rewards
states
where high -priority
5
loads
are
earlier
in the
stack to
prioritize
6
reshuffling
moves
that
release
high -priority
7
loads
more
efficiently.
8
"""
9
10
import
math
11
12
def
select_next_move ( warehouse_states ):
13
scores = []
14
for
state in
warehouse_states :
15
score = 0
16
for
stack in state:
17
bonus = 0
18
penalty = 0
19
can_access = True
20
for i in range(len(stack) - 1, -1,
-1):
21
priority_adjustment
=\
22
1 / (1 + math.exp ( -0.7 * stack[i]))
23
if
can_access:
24
bonus += (stack[i] *
25
(1 + math.exp ( -0.5 * i)))
26
if i > 0 and
stack[i] < stack[i - 1]:
27
penalty
+= ((1 - priority_adjustment ) *
28
(stack[i - 1] - stack[i])
29
* (1 / (1 + math.exp (0.5 * i))))
30
can_access = False
31
score += bonus - penalty
32
scores.append(score)
33
return
scores
(b) GPT4o:2024-08-06 EoH
Fig. 5: Heuristic thoughts and code with best fitness value for CEoH and EoH.
best Qwen2.5:32b-Coder CEoH heuristic and a 6.15 % optimality gap for the
GPT4o:2024-08-06 EoH heuristic.
A comparison of ten more instances (seed 10-19) with the same configurations
as the evaluation instances (60 % fill percentage, 5x5 bay layout, and
1x1 warehouse layout) shows that the Qwen2.5:32b-Coder CEoH heuristic
still outperforms the GPT4o:2024-08-06 EoH heuristic. This highlights the
superiority of the Qwen2.5:32b-Coder CEoH heuristic for the evaluation
configuration. However, the fitness for the new instances (seed 10-19) is
significantly higher than for the evaluation instances (seed 0-9). This indicates
an over-fitting to the training instances.
When applying the heuristics to other instance configurations, both heuristics
show good performances in solving bigger warehouse layout configurations.


--- Page 13 ---
Leveraging LLMs to Develop Heuristics for Emerging Optimization Problems
13
Table 2: Mean move number lower bound for different instance parameters (seed
10-19).
(a) 60 % fill percentage
warehouse
bay
1x1
2x2
3x3
4x4
3.85
7.05 12.11
5x5 12.17 26.28 42.94
6x6 27.61 57.44 97.24
(b) 80 % fill percentage
warehouse
bay
1x1
2x2
3x3
4x4
6.67 13.11
19.63
5x5 23.95 41.95
71.37
6x6 50.47 95.94 157.53
Table 3: Performance overview for the best heuristics and the state-of-the-art
optimal approach. The fitness [%] and mean evaluation runtime [s] (time) are
reported for the number of solved instances (sol.).
Qwen2.5-Coder:32b CEoH GPT4o:2024-08-06 EoH
A* [23]
fill
bay wh sol. fitness
time
sol. fitness
time
sol. fitness time
seed 0 - 9
60%
5x5 1x1 10
12.5
0.014
10
14.3
0.013
10
8.1
0.277
seed 10 - 19
60%
4x4 1x1
9
23.5
0.005
7
12.4
0.003
10
3.3
0.064
2x2 10
16.5
0.534
10
2.6
0.770
9
0
1.166
3x3 10
19.7
12.25
10
2.9
19.33
8
0
10.91
5x5 1x1 10
30.8
0.010
10
33.9
0.015
10
12.1
0.440
2x2 10
23.2
2.277
10
4.5
2.997
5
0
3.739
3x3 10
27
50.49
10
3.1
77.69
1
0
130.3
6x6 1x1 10
32.5
0.040
9
27.1
0.050
9
6.9
1.944
2x2 10
30.7
5.865
10
4.6
10.63
4
0
28.06
3x3 10
48.5
156.9
10
3.1
244.9
1
0
211.1
80%
4x4 1x1
1
0
0.001
1
50
0.003
5
50
0.250
2x2 10
39.8
0.631
10
18.6
0.916
4
1.2
51.27
3x3 10
37.8
12.55
10
6.3
22.64
1
0
170.1
5x5 1x1
0
-
-
0
-
-
3
19.9
10.30
2x2 10
47.3
2.661
10
19.4
4.046
1
0
12.51
3x3 10
42.6
50.83
10
10.1
87.37
-
-
-
6x6 1x1
0
-
-
0
-
-
1
29.4
581.4
2x2 10
50.5
8.949
10
23.1
15.33
0
-
-
3x3 10
53.7
162.2
10
18.9
258.9
0
-
-
Both heuristics solve 2x2 and 3x3 warehouse layout configurations, which
the A* can not solve within the 600-second runtime limit. The solution


--- Page 14 ---
14
T. Bömer et al.
quality of the GPT4o:2024-08-06 EoH heuristic is generally better than the
Qwen2.5:32b-Coder CEoH heuristic’s solution quality for such large warehouse
configurations.
The
GPT4o:2024-08-06
EoH
heuristic
shows
remarkable
performance with 2x2 and 3x3 warehouse configurations with 60 % fill percentage
with fitness values below 5 %. Both LLM-generated heuristics are unable to find
solutions for the most complex instances with 1x1 warehouse layout size and
80 % fill percentage.
6
Conclusion
This work investigated the potential of LLMs to generate heuristics for a
niche combinatorial optimization problem, the UPMP. We introduced CEoH, an
extension of EoH, which enhances heuristic design by incorporating in-context
learning through an additional problem-specific description.
Our
experiments
showed
that
CEoH
consistently
outperforms
EoH
when
using
the
open
source
models
Gemma2:27b,
Qwen2.5-Coder:32b,
and
DeepSeekV3:685b,
leading
to
better
heuristic
performance
and
greater
robustness
against
outliers.
In
contrast,
GPT4o:2024-08-06
and
GPT4o:2024-11-20 found good heuristics with or without additional problem
context. For the GPT4o:2024-08-06, heuristics generated with EoH performed
better than those with CEoH, suggesting that too much explicit problem
information may sometimes hinder heuristic exploration. However, the best
heuristic,
generated
by
Qwen2.5-Coder:32b
with
the
CEoH
framework,
achieved an optimality gap of only 4.35 % on evaluation instances.
Future research should investigate which types of LLMs benefit the most
from additional contextual problem descriptions in heuristic design. The design
of heuristics via LLMs for further niche optimization problems should be studied.
Acknowledgments.
This
research
was
funded
by
the
Deutsche
Forschungsgemeinschaft (DFG, German Research Foundation) - project number
276879186 and the European Union - NextGenerationEU - funding code 13IK032.
References
1. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E.,
Cai, C., Terry, M., Le, Q., et al.: Program synthesis with large language models.
arXiv preprint arXiv:2108.07732 (2021)
2. Bömer, T., Koltermann, N., Pfrommer, J., Meyer, A.: Sorting multibay block
stacking storage systems with multiple robots. In: International Conference on
Computational Logistics. pp. 34–48. Springer (2024), https://doi.org/10.1007/
978-3-031-71993-6_3
3. Bortfeldt, A., Forster, F.: A tree search procedure for the container pre-marshalling
problem. European Journal of Operational Research 217(3), 531–540 (2012).
https://doi.org/10.1016/j.ejor.2011.10.005
4. Burke, E.K., Gendreau, M., Hyde, M., Kendall, G., Ochoa, G., Özcan, E., Qu,
R.: Hyper-heuristics: A survey of the state of the art. Journal of the Operational
Research Society 64(12), 1695–1724 (2013)


--- Page 15 ---
Leveraging LLMs to Develop Heuristics for Emerging Optimization Problems
15
5. Dat, P.V.T., Doan, L., Binh, H.T.T.: Hsevo: Elevating automatic heuristic design
with diversity-driven harmony search and genetic algorithm using llms. arXiv
preprint arXiv:2412.14995 (2024)
6. DeepSeek: Deepseek v3. https://ollama.com/library/deepseek-v3
7. DeepSeek: Qwen2.5-coder. https://ollama.com/library/qwen2.5-coder
8. Expósito-Izquierdo, C., Melián-Batista, B., Moreno-Vega, M.: Pre-marshalling
problem: Heuristic solution method and instances generator. Expert Systems with
Applications 39(9), 8337–8349 (2012)
9. Google: Gemma 2. https://ollama.com/library/gemma2:27b
10. Hottung, A., Tanaka, S., Tierney, K.: Deep learning assisted heuristic tree search
for the container pre-marshalling problem. Computers & Operations Research 113,
104781 (2020)
11. Huang, Y., Zhang, W., Feng, L., Wu, X., Tan, K.C.: How multimodal integration
boost the performance of llm for optimization: Case study on capacitated vehicle
routing problems (2024), https://arxiv.org/abs/2403.01757
12. Jiménez-Piqueras, C., Ruiz, R., Parreño-Torres, C., Alvarez-Valdes, R.: A
constraint programming approach for the premarshalling problem. European
Journal of Operational Research 306(2), 668–678 (2023). https://doi.org/10.
1016/j.ejor.2022.07.042
13. Kang, J., Oh, M.S., Ahn, E.Y., Ryu, K.R., Kim, K.H.: Planning for intra-block
remarshalling
in
a
container
terminal.
In:
Advances
in
Applied
Artificial
Intelligence: 19th International Conference on Industrial, Engineering and Other
Applications of Applied Intelligent Systems, IEA/AIE 2006, Annecy, France, June
27-30, 2006. Proceedings 19. pp. 1211–1220. Springer (2006). https://doi.org/
10.1007/11779568_128
14. Langdon, W.B., Poli, R.: Foundations of genetic programming. Springer Science
& Business Media (2013)
15. Lee, Y., Hsu, N.Y.: An optimization model for the container pre-marshalling
problem. Computers & operations research 34(11), 3295–3313 (2007). https:
//doi.org/10.1016/j.cor.2005.12.006
16. Liu, F., Xialiang, T., Yuan, M., Lin, X., Luo, F., Wang, Z., Lu, Z., Zhang, Q.:
Evolution of heuristics: Towards efficient automatic algorithm design using large
language model. In: Forty-first International Conference on Machine Learning
(2024)
17. Liu, F., Yao, Y., Guo, P., Yang, Z., Lin, X., Tong, X., Yuan, M., Lu, Z., Wang,
Z., Zhang, Q.: A systematic survey on large language models for algorithm design.
arXiv preprint arXiv:2410.14716 (2024)
18. Min, M.J., Ding, Y., Buratti, L., Pujar, S., Kaiser, G., Jana, S., Ray, B.:
Beyond accuracy: Evaluating self-consistency of code large language models with
identitychain. arXiv preprint arXiv:2310.14053 (2023)
19. Naveed, H., Khan, A.U., Qiu, S., Saqib, M., Anwar, S., Usman, M., Akhtar, N.,
Barnes, N., Mian, A.: A comprehensive overview of large language models. arXiv
preprint arXiv:2307.06435 (2023)
20. OpenAI: Gpt-4o. https://openai.com/research/gpt-4o
21. Parreño-Torres, C., Alvarez-Valdes, R., Ruiz, R.: Integer programming models for
the pre-marshalling problem. European Journal of Operational Research 274(1),
142–154 (2019). https://doi.org/10.1016/j.ejor.2018.09.048
22. Pfrommer, J., Bömer, T., Akizhanov, D., Meyer, A.: Sorting multibay block
stacking storage systems. arXiv preprint arXiv:2405.04847 (2024). https://doi.
org/10.48550/arXiv.2405.04847


--- Page 16 ---
16
T. Bömer et al.
23. Pfrommer, J., Meyer, A., Tierney, K.: Solving the unit-load pre-marshalling
problem in block stacking storage systems with multiple access directions.
European Journal of Operational Research (2023). https://doi.org/10.1016/j.
ejor.2023.08.044
24. Prandtstetter, M.: A dynamic programming based branch-and-bound algorithm
for the container pre-marshalling problem. Technical repot, IT Austrian institute
of technology (2013)
25. Rendl, A., Prandtstetter, M.: Constraint models for the container pre-marshaling
problem. In: ModRef 2013: The Twelfth International Workshop on Constraint
Modelling
and
Reformulation
(2013),
https://api.semanticscholar.org/
CorpusID:43049743
26. Romera-Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar, M.P.,
Dupont, E., Ruiz, F.J., Ellenberg, J.S., Wang, P., Fawzi, O., et al.: Mathematical
discoveries from program search with large language models. Nature 625(7995),
468–475 (2024)
27. Wang, Z., Zhou, C., Che, A., Gao, J.: A policy-based monte carlo tree
search method for container pre-marshalling. International Journal of Production
Research 62(13), 4776–4792 (2024)
28. Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q.V., Zhou, D., Chen, X.: Large language
models as optimizers (2024), https://arxiv.org/abs/2309.03409
29. Ye, H., Wang, J., Cao, Z., Song, G.: Reevo: Large language models as
hyper-heuristics with reflective evolution. arXiv preprint arXiv:2402.01145 (2024)
