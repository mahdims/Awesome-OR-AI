--- Page 1 ---
An LLM-powered MILP modelling engine for workforce scheduling guided by
expert knowledge
Qingyang Lia,b,∗, Lele Zhanga,b, Vicky Mak-Hauc
aThe University of Melbourne, Melbourne, 3010, Victoria, Australia
bARC Training Centre in Optimisation Technologies, Integrated Methodologies, and Applications (OPTIMA), Australia
cDeakin University, Burwood, 3125, Victoria, Australia
Abstract
Formulating mathematical models from real-world decision problems is a core task in Operational
Research, yet it typically requires considerable human expertise and effort, limiting practical appli-
cation. Recent advances in large language models (LLMs) have sparked interest in automating this
process from natural language descriptions. However, challenges including limited modelling expertise,
dependence on large-scale training data, and hallucination affect the reliable application of LLMs in
optimisation modelling. To address these challenges, we propose SMILO, an expert-knowledge-driven
framework that integrates optimisation modelling expertise with LLMs to generate mixed-integer lin-
ear programming models. SMILO uses a three-stage architecture built on reusable modelling graphs
and associated resources: identifying relevant modelling components, extracting instance-specific in-
formation using LLMs, and constructing models through expert-defined templates.
This modular
architecture separates information extraction from formula generation, enhancing modelling accuracy,
transparency, and reproducibility. We demonstrate the implementation of our problem-type-specific
modelling framework using workforce scheduling problems spanning manufacturing, logistic, and ser-
vice operations as illustrative cases. Experiments show that SMILO consistently generates correct
models in 90% of test instances across five trials, outperforming one-step LLM baselines by at least
35%. This work offers a generalisable paradigm for integrating LLMs with expert knowledge across
diverse decision-making contexts, advancing automation in optimisation modelling.
Keywords:
Mixed-integer linear programming, Large language model, Automatic formulation, Mathematical
modelling, Workforce scheduling
1. Introduction
Mathematical optimisation modelling plays a fundamental role in effective decision-making across a
wide range of domains, including logistics, inventory management, production planning, and workforce
scheduling (Chen et al., 2022, Liu et al., 2024, Chen et al., 2024). A central challenge in applying
optimisation lies in the accurate formulation of real-world problems as mathematical models, typically
in the form of mixed-integer linear programming (MILP) models. While advancements in optimisation
solvers have improved computation efficiency, the task of translating problem descriptions in natural
language into MILP models remains non-trivial. The modelling process involves the definition of deci-
sion variables, identification of constraints, and construction of an appropriate objective function. The
∗Corresponding author.
Email addresses: ql5@student.unimelb.edu.au (Qingyang Li), lele.zhang@unimelb.edu.au (Lele Zhang),
vicky.mak@deakin.edu.au (Vicky Mak-Hau)
1
arXiv:2511.02364v1  [math.OC]  4 Nov 2025


--- Page 2 ---
process is inherently time-consuming and error-prone, requiring substantial domain knowledge and
expertise to accurately convert complex operational requirements into mathematical terms. This mod-
elling step constitutes a critical bottleneck, limiting broader adoption and application of optimisation
techniques, especially for organisations lacking access to dedicated optimisation expertise.
Recent breakthroughs in large language models (LLMs) have inspired substantial research efforts ex-
ploring their potential to automate the process of optimisation model formulation from natural lan-
guage descriptions. LLMs have showcased remarkable proficiency in tasks such as code generation, text
comprehension, and even preliminary attempts at mathematical model formulation (Jackson, Ivanov,
Dolgui and Namdar, 2024, Boudribila et al., 2025). Nonetheless, their effectiveness remains limited
when applied to even moderately complex MILP modelling tasks. Recent studies have highlighted sev-
eral fundamental challenges associated with the use of LLMs for MILP model generation (Li, Zhang
and Mak-Hau, 2023, AhmadiTeshnizi et al., 2024, Huang et al., 2025):
• Challenge 1: Limited modelling expertise. Despite being trained on vast amounts of general
textual data, LLMs lack specialised knowledge and expertise in mathematical optimisation. As
a result, they often produce incomplete or incorrect models for complex real-world decision-
making problems, misdefining decision variables, omitting implicit constraints, miscomputing
parameters, or applying inappropriate modelling techniques.
• Challenge 2: Insufficient large-scale, high-quality training data. The scarcity of high-
quality training data comprising natural language problem descriptions and their corresponding
optimisation models constrains the effectiveness of data-driven methods.
• Challenge 3:
Limited complexity in benchmark datasets.
While existing benchmark
datasets span a wide range of domains such as retail, manufacturing, and agriculture, most
instances are simple linear programming problems with structurally similar formulations. Clas-
sic combinatorial optimisation problems such as shift scheduling and vehicle routing are included
but with few instances per problem type, offering limited variation in modelling complexity.
• Challenge 4: Hallucination. LLMs are prone to generating outputs, such as constraints in
MILP models, that appear plausible but are in fact incorrect or fabricated, and this affects the
reliability of the models generated by the LLMs.
To address these challenges, we propose an expert-knowledge-driven MILP modelling framework pow-
ered by LLMs, and we name it as SMILO. This customised modelling engine integrates structured
expert knowledge into a modular and interpretable modelling pipeline. The framework is underpinned
by problem-type-specific modelling graphs (MGs), which explicitly organise core modelling components,
such as decision variables, constraints, and objective functions, and capture their logical interdepen-
dencies.
The modelling process follows a three-stage pipeline. First, sentences from the problem description are
matched with nodes in the MG to identify relevant modelling components. Second, an LLM guided by
carefully tailored task-specific templates extracts instance-specific numerical values for the identified
components. These templates are designed to guide the LLM’s attention to precise information needs,
thereby reducing hallucination and improving output consistency. Third, MILP model construction is
carried out using predefined templates that automatically incorporate the extracted values. Parameter
calculations and expression generation are programmatically performed using expert-defined rules to
ensure mathematical correctness and consistency.
Unlike data-driven approaches that typically aim to enhance the modelling capabilities of LLMs
through training and fine-tuning, our method does not delegate core modelling responsibilities to
the LLM. Instead, it decomposes the modelling process into structured subtasks, with LLMs assisting
in natural language understanding and information extraction, while mathematical expressions are
constructed through expert-defined rules and templates. This separation of roles and the modular
2


--- Page 3 ---
design of the framework enhance modelling accuracy, control, and interpretability, while significantly
reducing hallucination-related errors (addressing both Challenge 1 and Challenge 4). Furthermore, our
framework adopts a knowledge-driven strategy that avoids reliance on large training datasets, relying
instead on structured expert knowledge encoded in reusable model components (addressing Challenge
2). In addition, the modular framework is designed to be model-neutral; the LLM component can
be seamlessly replaced with any open-source or closed-source models without the need for additional
training or significant system adjustments. This flexibility is particularly valuable for domains with
strict data privacy requirements or limited computational resources.
To validate the feasibility and effectiveness of our approach, we present a proof-of-concept applica-
tion targeting two classical yet complex workforce scheduling problems: shift scheduling and days-off
scheduling. These problems exhibit considerable modelling complexity, involving features such as shift
coverage, overtime rules, and break allocation.
In such cases, a direct translation of natural lan-
guage problem descriptions into mathematical expressions is often infeasible. While we implement and
evaluate our framework on these scheduling problems as illustrative cases, the underlying framework
demonstrates a generalisable paradigm broadly applicable to a wide range of MILP problems beyond
workforce scheduling. This structured paradigm provides a practical and robust template for effectively
leveraging LLM’s capability in OR applications.
The primary contributions of our work include:
• Expert-knowledge-driven modelling engine: We develop a novel MILP modelling engine
that leverages LLMs in a structured, modular, and highly interpretable manner, enabling ac-
curate, reliable, and reproducible model generation from natural language problem descriptions.
This modelling engine follows a three-stage framework: component identification, information ex-
traction, and model generation. The framework delegates language understanding tasks to LLMs
while retaining precise control of the mathematical formulation through expert knowledge.
• Problem-type-specific modelling graphs (MGs): We introduce and implement MGs as
structured representations of reusable modelling components and their interdependencies. These
graphs serve as the backbone for guiding component identification, information extraction, and
formula assembly. The MGs are modular, extensible, and reusable across similar problem types.
We construct a set of reusable modelling resources aligned with the MGs, including predefined
formula templates, prompt templates, modelling component descriptions, parameter computation
rules, and symbol definition rules, which support systematic and interpretable model generation.
These resources enhance transparency, consistency, and traceability throughout the modelling
pipeline.
• Task-based prompting strategy: We design a task-based prompting strategy that interfaces
with the MG. For each activated node in the graph, a corresponding specialised prompt template
extracts instance-specific information via the LLM. This approach allows the framework to handle
complex and ambiguous problem descriptions and enhances robustness by clearly defining task
boundaries and output formats.
• Benchmark dataset for workforce scheduling: We build a benchmark dataset focused
on two representative workforce scheduling problems. The dataset captures varying levels of
complexity and structural richness, enabling a more realistic and rigorous evaluation of automated
modelling tools beyond toy-sized instances (addressing Challenge 3).
• Comprehensive experimental evaluations: We evaluate our framework against standard
one-step model generation approaches. Across five independent trials, our framework consistently
generated correct MILP models for 90% of instances, while the best-performing baseline achieved
no more than 55% correctness in any single trial.
We also provide a detailed error analysis
comparing the two approaches, offering valuable insights into common failures and potential
areas for future improvement in LLM-assisted optimisation modelling.
3


--- Page 4 ---
• General and LLM-agnostic paradigm: We propose a generalisable, LLM-agnostic paradigm
for optimisation modelling that requires no fine-tuning or extensive training data. Our frame-
work is readily compatible with various LLMs and extendable to a broad range of optimisation
problems, making it an adaptable and scalable solution for real-world decision-support applica-
tions.
The remainder of this paper is structured as follows. Section 2 reviews the literature on LLM-based
optimisation modelling approaches and workforce scheduling problems. Section 3 details the proposed
framework. Section 4 describes the experimental setup and dataset and presents the experimental
results. Finally, Section 5 outlines some conclusions and discusses future work.
2. Literature Review
2.1. LLM-based approaches for automated model generation
The automation of mathematical programming model generation has gained increasing attention, par-
ticularly with the rise of LLMs and their ability to process complex problem descriptions (Fan et al.,
2024). While optimisation modelling has traditionally been a manual and expertise-driven process, re-
cent developments have explored various methods to bridge the gap between natural language problem
descriptions and formal mathematical representations (Ramamonjison et al., 2022). The integration
of natural language processing (NLP) techniques, particularly LLMs, into mathematical programming
model generation has introduced a new paradigm in automated model generation.
LLMs, such as GPT, Gemini, PaLM and Llama, have demonstrated strong capabilities in semantic un-
derstanding, structured information extraction, and mathematical reasoning, making them suitable for
processing optimisation problems described in natural language (Fosso Wamba et al., 2024, Makatura
et al., 2023, Jackson, Jesus Saenz and Ivanov, 2024). However, despite these advancements, LLMs
still exhibit limitations in numerical consistency, logical reasoning, and constraint completeness, which
necessitate additional mechanisms to ensure the correctness of the generated models. Without provid-
ing any examples or specific knowledge of MILP, models generated by LLMs frequently contain errors
and fail to accurately capture the requirements specified in the problem description, particularly for
problems beyond textbook-level difficulty (Ramamonjison et al., 2023, Li, Zhang and Mak-Hau, 2023,
Fan et al., 2024).
Existing research has shown that LLMs can be employed to extract variables, constraints, and objective
functions from textual inputs, using multi-stage frameworks, fine-tuning, few-shot learning, prompt en-
gineering, and continued pretraining to improve accuracy (Li, Zhang and Mak-Hau, 2023, Fan et al.,
2024, Li et al., 2024, Tang et al., 2024, Huang et al., 2025). Other studies improve the correctness
and relevance of generated models by incorporating additional validation mechanisms and agent-based
adjustment approaches. Multi-agent collaboration enables model generation to be an iterative process
and, in some cases, even interactive with users (Gaddam et al., 2025, Abdullin et al., 2024, Ahma-
diTeshnizi et al., 2023, AhmadiTeshnizi et al., 2024, Li, Mellou, Zhang, Pathuri and Menache, 2023,
Xiao et al., 2023). Furthermore, a chatbot has been developed to interpret optimisation models to
users and diagnose sources of infeasibility (Chen et al., 2023). Regarding the final output, existing
research focuses on different aspects, including the mathematical model itself and the solution obtained
from solving the model (Ahmed and Choudhury, 2024, Ramamonjison et al., 2023, Tsouros et al., 2023,
AhmadiTeshnizi et al., 2024, Li, Mellou, Zhang, Pathuri and Menache, 2023). Mathematical models
can be represented in two forms: the canonical form, which expresses the model using matrix notation,
and the structured representation, which explicitly defines sets, parameters, variables, constraints, and
the objective function. The structured representation offers greater interpretability.
To address LLMs’ limitations in domain-specific knowledge, promising approaches include integrating
them with task-relevant specialised data, such as Retrieval-Augmented Generation (RAG) (Gao et al.,
2024), and incorporating structured knowledge representations (Kau et al., 2024, Kosasih et al., 2024).
4


--- Page 5 ---
OptiMUS-0.3 uses RAG to retrieve examples of constraints from their dataset that are similar to the
constraint description that is currently being formulated, incorporating them and their formulas into
the prompt to enhance LLMs’ performance in mathematical modelling. The provided examples can
illustrate modelling techniques to LLMs, such as the Big-M technique (AhmadiTeshnizi et al., 2024).
To the best of our knowledge, structured knowledge representations have not yet been integrated into
LLM-based automated modelling approaches. In this work, we propose problem-type-specific mod-
elling graphs (MGs) that represent structured expert knowledge of optimisation modelling.
These
MGs help ensure that essential modelling components are considered and that no critical constraints
are overlooked when processing a given problem instance. Most existing studies focus on LLMs’ in-
trinsic ability to formulate mathematical models. Our research seeks to separate the natural language
processing and mathematical formulation stages by using MGs to assist LLMs in identifying key com-
ponents and extracting relevant information from the problem description. The extracted information
is then mapped onto predefined mathematical templates to construct complete MILP models. This ap-
proach allows LLMs to focus solely on tasks they are inherently proficient at. This integration ensures
both adaptability to complex modelling scenarios and reliability in mathematical model correctness.
2.2. Workforce scheduling problems
Workforce scheduling is a fundamental optimisation problem in Operational Research, extensively stud-
ied due to its practical significance in various industries such as healthcare, call centres, manufacturing,
and banking (Van den Bergh et al., 2013, Mutlu et al., 2015). Workforce scheduling problems exhibit
considerable diversity depending on the industry, constraints, and objectives, and they are generally
categorised into three primary subtypes: shift scheduling, days-off scheduling, and tour scheduling
(Morris and Showalter, 1983). This subsection does not attempt a comprehensive review of workforce
scheduling. Instead, it focuses on the two subtypes examined in this study, shift scheduling and days-
off scheduling, by reviewing key modelling approaches that were considered during the development of
our modelling graphs. Tour scheduling falls outside the scope of this work.
Shift scheduling, also known as time-of-day scheduling, involves determining the start time of shifts,
their durations, and how to allocate employees across a planning horizon to meet demand requirements,
while incorporating break placements to comply with labour regulations. The shift scheduling problem
is particularly crucial in industries where labour demand fluctuates throughout the day. The shift
scheduling problem has been extensively studied in the literature since its introduction by Edie (1954).
Early Integer Programming (IP) formulations were based on the set-covering formulations proposed by
Dantzig (1954), which define variables by enumerating all possible shifts based on different combinations
of start times, durations, and break placements. However, these approaches often result in large-scale
IP models with a vast number of variables, making them computationally impractical for real-world
applications.
Subsequent research has sought to refine IP formulations to improve computational efficiency. Gaballa
and Pearce (1979) developed an implicit IP formulation that introduced additional variables to model
lunch break windows.
However, their approach required a larger number of integer variables and
constraints compared to the equivalent set-covering formulation, making it less efficient. Bechtold and
Jacobs (1990) introduced integer variables to represent the number of employees starting their breaks
within specified planning periods, implicitly matching breaks to explicitly represented shifts. A double
implicit shift scheduling model was proposed as an extension of Bechtold and Jacobs (1990), implicitly
matching meal breaks to implicitly represented shifts (Thompson, 1995). However, their models were
restricted to a single meal break.
For solving large-scale shift scheduling problems with multiple breaks and multiple break windows,
Aykin (1996) proposed a new implicit IP model by introducing a set of break variables for each
shift-break type combination, significantly reducing the number of decision variables and improving
computational performance. This approach leads to a substantially smaller number of variables and
improved solvability for large instances involving multiple breaks and break windows.
5


--- Page 6 ---
Days-off scheduling, also known as days-of-week scheduling, focuses on assigning employees’ work and
rest days over a given scheduling horizon (e.g., a week or a month) to ensure that labour demands
are met while complying with contractual agreements and work-life balance considerations. A funda-
mental problem in this category is the (k, m)-cyclic workforce scheduling problem, where each worker
is assigned a repeating work schedule over an m-day cycle, ensuring that they work for k consecutive
days and then take m −k days off. It can be formulated by the set-covering model (Bartholdi et al.,
1980).
Tour scheduling integrates both shift and days-off scheduling to construct employee work schedules that
span multiple days or weeks. This approach is more comprehensive as it considers both intra-day shift
assignments and inter-day work patterns, ensuring a globally optimised workforce allocation (Alfares,
2004). An IP model was developed to address tour scheduling, incorporating additional constraints
such as requirements for full-time and part-time workers, consecutive days off, and variable daily shift
start times (Bard et al., 2003). Researchers have also explored hierarchical scheduling frameworks
based on IP models, where high-level workforce allocation decisions are made first, followed by detailed
assignments at a finer granularity (Türker and Demiriz, 2018).
3. Methodology
3.1. Framework overview
The proposed framework automates the transformation of unstructured natural language problem
descriptions into mixed-integer linear programming models through a structured three-stage process.
First, relevant modelling components are identified by mapping the textual description to nodes in
a predefined modelling graph. Second, for the activated nodes, the framework employs task-specific
prompts to extract instance-specific numerical values and contextual details using a large language
model. Third, the extracted information is processed into sets, parameters, and decision variables,
which are then assembled into a complete MILP model (in LaTeX format) using predefined formula
templates and rule-based logic.
By integrating semantic matching, structured knowledge representation, and controlled model syn-
thesis, the framework provides a reproducible, interpretable, modular approach to automated MILP
formulation. The main workflow of the proposed framework is illustrated in Figure 1, while a step-by-
step example, including input, transformation stages, and output, is provided in Figure 2.
3.2. Modelling graphs
To ensure accurate and structured MILP model generation, our framework incorporates MGs as
domain-specific knowledge modules.
These graphs are designed to capture the fundamental com-
ponents, including entities, parameters, decision variables, objectives, and constraints, required for
formulating specific optimisation problem types, e.g., workforce scheduling problems. For each consid-
ered problem type, we develop a dedicated MG; examples include shift scheduling (see Figure A.5 and
Figure A.6) and days-off scheduling (see Figure A.7 and Figure A.8). Each MG is further transformed
into three distinct representations that enable automated retrieval across the stages of the framework:
node descriptions, information extraction tasks, and mathematical formula templates. These three
resources are pre-constructed during the framework development phase. Their specific roles will be
elaborated in the following sections.
6


--- Page 7 ---
Figure 1: Main steps of the automatic formulation framework
7


--- Page 8 ---
Figure 2: Implementing the automatic formulation framework on a problem instance of shift scheduling from our test set
8


--- Page 9 ---
We use a simplified version of the shift scheduling MG (see Figure 3) to introduce the core ideas and
the fundamental components. Figure 3 only includes the essential nodes for the simplest instances of
this problem type, such as the instance shown in Figure 2. The full MG for shift scheduling, presented
in Figure A.5, incorporates additional elements, including the scheduling of breaks and overtime, as
well as more types of constraints and objective functions.
Figure 3: Part of the shift scheduling MG
Each MG consists of several interconnected layers that define the components of an optimisation
problem:
• Entities: The MG encodes fundamental scheduling elements such as time periods, the planning
horizon, and shifts. The planning horizon is denoted by Tmax, while time is discretised into a set
of periods T = {1, 2, . . . , Tmax/tsize}, based on the minimum planning duration tsize. The set of
shifts denoted by S = {s} serves as the basis for workforce allocation. Each shift is specified by
its start time and duration, which are provided as parameters.
• Parameters: The MG defines essential numerical values required for the model formulation.
These include shift attributes (i.e., the start time of each shift, starts, ∀s ∈S, and the duration
of each shift, durations, ∀s ∈S), and lower bounds involved in labour demand constraints (e.g.,
minimum number of employees per period, lbt, ∀t ∈T).
The binary parameter at,s, which
indicates whether shift s covers period t, is computed using predefined logic and is incorporated
into the constraint formulations. This type of parameter is not explicitly stated in the problem
description but must be derived based on the attributes of each shift.
• Decision variables: The number of staff assigned to each shift is denoted by xs, ∀s ∈S, forming
the basis for workforce scheduling decisions.
• Objectives: The objective is usually to minimise the total number of employees required or the
associated scheduling cost.
• Constraints: The essential constraints include labour demand satisfaction constraints ensur-
ing that staffing levels meet or exceed minimum requirements (P
s at,sxs ≥lbt, ∀t ∈T) and
non-negativity and integrality constraints enforcing non-negative and integer values for decision
variables (xs ∈{0, 1, 2, . . . }, ∀s ∈S). Identifying which active shifts cover the labour demand for
a given period is both crucial and challenging, especially when the scheduling of in-shift breaks
and overtime is involved. This process is central to modelling shift scheduling problems, as it
requires additional reasoning and computation. Problem descriptions typically do not explicitly
9


--- Page 10 ---
or straightforwardly specify such details.
If multiple breaks and multiple break windows are
involved, the formulation will be more complex.
By explicitly encoding the relationships between scheduling components, the MG serves as a guidance
mechanism that compensates for the lack of optimisation expertise in LLMs. The MG ensures com-
pleteness by activating all relevant components based on both the textual problem description and
predefined dependency and relevance rules. Instead of relying on the LLMs to infer the correct model
structure, the MG systematically determines which components should be included, reducing the risk
of missing constraints, parameters, or variables.
3.3. Stage 1: Modelling component identification
Given a problem description for a specific problem type, the first stage of the modelling process
involves identifying which components from the corresponding MG are relevant. To this end, an LLM
is employed to segment the text into individual sentences and match each sentence to up to three of
the most relevant nodes in the MG.
As noted earlier, three knowledge resources developed offline are used to support the overall framework.
The first of these is essential in this stage.
Specifically, each MG node is supplemented with an
extended description in addition to its name, clarifying its meaning within the context of mathematical
optimisation. These node names and descriptions are compiled into a reference list and incorporated
into the LLM prompt, as shown in Listing 1, enabling accurate alignment between natural language
text and formal modelling components for the specific problem type.
3.4. Stage 2: Information extraction via task-based prompting
Once a set of relevant nodes has been identified, the framework proceeds to extract instance-specific
details associated with the knowledge represented by those nodes. For example, if a node corresponds
to a parameter, its concrete numerical value must be retrieved from the problem description.
To
achieve this, a set of information extraction tasks is defined for each modelling graph, covering its set
and parameter nodes. As each modelling graph is constructed for a specific optimisation problem type,
the corresponding task sets are tailored accordingly.
Notice that our framework does not rely on LLMs to identify decision variables or to formulate objective
functions and constraints. For complex optimisation problems, these modelling tasks require specialised
techniques and domain knowledge, which LLMs sometimes struggle to apply correctly and consistently.
Instead, such responsibilities are handled by the framework’s rule-based logic, which is driven by expert
knowledge encoded in the modelling graph.
After selecting and ordering the applicable tasks, the framework executes them sequentially using
an LLM. The second preparatory resource supports this stage by providing a repository of prompt
templates for information extraction tasks, as well as definitions of both the relationships between
tasks and the mappings between tasks and their corresponding modelling graph nodes. Each task
entry specifies the task name, task description, expected output format, prerequisite tasks, parent task
category, and the associated modelling graph node(s).
The relationships between tasks fall into two categories: prerequisite and hierarchical. Prerequisite
relationships determine the dependency structure among tasks and are used to pass known information
and play a central role in task selection and ordering. Hierarchical relationships are used to eliminate
redundant or overlapping tasks during task selection by enforcing preference for more specific or fine-
grained tasks. The full details of task selection and ordering will be discussed in later subsections.
For each task, the prompt given to the LLM includes the task name, task description, expected output
format, the full problem description, and known information derived from its prerequisite tasks. This
known information is provided in JSON format. The LLM output is parsed and stored as JSON,
10


--- Page 11 ---
ensuring only valid structured data is retained. Listing 2 shows an example of a prompt designed
to extract the minimum number of employees required for each period. As this task depends on the
definition of the set of periods, the prompt incorporates information extracted from the prerequisite task
concerning period definitions as known input. Additional prompt examples are provided in Appendix B.
3.4.1. Task selection via node activation
Based on the nodes identified in Stage 1, the framework must determine which information extraction
tasks should be executed. To enable this, a predefined mapping is established between MG nodes
and extraction tasks, where each task is associated with one or more nodes. A task is selected if at
least one of its associated nodes is activated. In this way, activating a node can trigger the activation
of related nodes, producing a propagation effect across the modelling graph. This mechanism serves
two purposes. First, it mitigates the impact of omissions in the component identification stage by
introducing flexibility and tolerance to partial recognition. Second, rather than designing a separate
task for each node, the framework groups multiple related nodes into a single task and extracts related
information together.
This reduces redundancy, avoids duplicated effort, and improves the LLM’s
ability to semantically interpret the problem context and the task requirement, thereby enhancing the
accuracy of information extraction.
If a task is selected, all of its prerequisite tasks are also selected. This further contributes to the
robustness of the framework against incomplete or partially correct initial node identification results.
Task selection thus propagates along the dependency structure, ensuring that all required preceding
tasks are included in the extraction sequence. In addition, if a selected task has an associated gen-
eralised task, which refers to a coarser-grained alternative covering the same modelling component,
and both tasks appear in the execution list, then the generalised task is removed. This ensures that
more specific and fine-grained tasks are prioritised, as they are linked to predefined modelling rules
embedded in the framework. For example, instead of extracting the shift costs directly, the framework
may extract per-period costs, which differ depending on whether the work is regular or overtime. The
shift costs will then be calculated in our system based on the duration of the shifts and the costs of
the periods. This approach improves both accuracy and consistency, as the parameter calculation is
handled through rule-based logic rather than relying on the LLM’s interpretation.
By carrying out parameter calculation and formula construction using the internal logic of the frame-
work instead of through the LLM, the burden on the LLM is significantly reduced. Expert knowledge,
represented through parameter derivation rules and formula templates for different types of constraints
and objective functions, plays a central role in guiding the formulation process. This approach enhances
the correctness and reproducibility of the generated models while offering greater control and inter-
pretability in the formula construction process.
3.4.2. Task dependency sorting via topological ordering
Information extraction tasks are interdependent, as certain tasks require information extracted by
others and must therefore be executed in a specific sequence. To resolve these dependencies, SMILO
employs a topological sorting algorithm to determine a valid execution order. A task dependency graph
is first constructed, where each task is represented as a node, and a directed edge from task Ti to task
Tj indicates that Tj depends on the output of Ti, that is, Ti is a prerequisite task for Tj. This graph
structure enables the identification of an appropriate task sequence through topological sorting.
The sorting procedure begins by computing the in-degree of each node, which corresponds to the
number of other tasks that must be completed before it can be executed. Tasks with zero in-degree,
meaning they have no unmet dependencies, are placed into an initial queue. The framework then
iteratively processes this queue by removing tasks, reducing the in-degree of their dependent tasks,
and appending any new zero in-degree tasks to the queue.
This process continues until all tasks
have been ordered. The result is a task sequence in which each task is positioned only after all its
prerequisites have been satisfied.
11


--- Page 12 ---
3.5. Stage 3: MILP model generation
Based on the information extracted in Stage 2, the framework proceeds to generate the sets and
compute the parameters required for MILP model construction, following predefined rules. Based on
the generated sets, the framework defines the relevant decision variables. For example, given a shift
scheduling instance, Stage 2 extracts the necessary information to construct two fundamental sets:
the set of periods and the set of shifts. Period information includes the start and end times of each
period in 24-hour format, with each period assigned a unique numeric identifier t, forming the set T.
Shift information specifies the start time and duration of each possible shift, each assigned an identifier
s, forming the set S. Based on the constructed set S, the framework defines the decision variables
xs, ∀s ∈S, representing the number of staff assigned to shift s. A key computational component in
this stage is the binary parameter matrix at,s, which encodes whether shift s covers period t. This
matrix is derived from the extracted start time and duration of each shift, with appropriate handling
of cyclic scheduling, where shifts may extend across the boundary of the planning horizon. If overtime
is permitted, the framework additionally computes the binary parameter matrix vt,o,s, which specifies
whether shift s extended by o overtime periods covers time period t. Similarly, if breaks are allowed,
other binary parameters are computed in a similar way.
Once all extracted information has been processed and all derived parameters have been computed,
the framework automatically assembles a complete MILP model in LaTeX format. This model is gen-
erated by combining the predefined mathematical notation templates associated with each modelling
component. The resulting formulation is structured in a standard format, beginning with the sets, pa-
rameters, and decision variables, followed by the objective function and the constraints, as illustrated
in Figure 2.
The third preparatory resource supports this stage by defining the rules for parameter computation,
the logic for variable definition, and the formula templates for various types of constraints and objective
functions. For example, the labour demand constraint in the complete modelling graph (see Figure A.5)
is specified in its most general form, accounting for both breaks and overtime. When the framework
detects that breaks and/or overtime are not present in a particular instance, it provides a simplified
formulation of the constraint, omitting the corresponding parameters and variables.
4. Experimental Evaluation
In this section, we present a systematic evaluation of our automated MILP model generation framework.
The experiment aims to assess the framework’s ability to correctly process natural language problem
descriptions, generate accurate MILP models and solve them successfully. The evaluation is based
on 20 test instances, each corresponding to a distinct natural language problem description. We first
describe the test instances and their characteristics, followed by the evaluation metrics used to assess
the framework’s performance. Then we present and discuss the experimental results.
4.1. Setup and test instances
We evaluated both our proposed multi-stage framework (with GPT-4o-2024-08-06 as the deployed
LLM) and a baseline approach GPT-4o-2024-08-06. For each of the 20 test instances, we conducted
five independent trials with each method and measured the correctness and consistency in generating
mathematical formulations. Hereafter, GPT-4o refers specifically to version GPT-4o-2024-08-06. In all
experiments, the temperature parameter of GPT-4o was set to zero to ensure deterministic outputs,
while all other parameters were kept at their default values.
The proposed framework incorporates GPT-4o in two stages: component identification and informa-
tion extraction. Based on the extracted information, the final mathematical model is produced by a
predefined code implementation rather than by the large language model itself.
12


--- Page 13 ---
To enable automatic solution and evaluation, we developed a code generation prompt (see Listing 4 and
Listing 5) that guides GPT-4o to convert the LaTeX-formatted mathematical model into executable
Python code, which can be solved using the Gurobi solver. This conversion procedure was applied
consistently across all experimental settings.
As a baseline, we used GPT-4o in a zero-shot setting. In this setting, the large language model is
directly prompted to generate LaTeX-formatted mathematical models from natural language problem
descriptions. The resulting models are subsequently converted into Python code using the same code
generation prompt. We designed two modelling prompts: one with minimal guidance, referred to as
the simple prompt (see Listing 6), and another with more detailed and rigorous instructions, referred
to as the standard prompt (see Listing 7).
We developed a novel dataset of 20 problem instances, consisting of 10 shift scheduling problems and
10 days-off scheduling problems. These two groups of instances were designed to evaluate two variants
of our framework, which are built on the same architecture but adapted to different problem types
via problem-type-specific modelling graphs. Specifically, the shift scheduling instances were used to
test the framework integrated with the shift scheduling modelling graph, while the days-off scheduling
instances were used to evaluate the framework incorporating the days-off scheduling modelling graph.
The dataset was constructed to reflect a diverse range of modelling scenarios by combining various
attributes commonly encountered in these two types of decision-making problems.
Each instance
includes a natural language problem description, often with tables, a reference mathematical model in
LaTeX, and a known optimal objective value (obtained by solving the Python code with Gurobi). All
instances are feasible, and their corresponding models admit optimal solutions.
Verifying the optimal value enables a fully automated evaluation approach. Compared with automatic
evaluation based on direct comparison against ground truth formulations, this method offers greater
flexibility by allowing multiple valid formulations of the same problem. A given problem instance in our
dataset may admit more than one correct formulation. For reference, we provided one mathematical
model for each instance to facilitate the use of the dataset by other researchers. However, using a
different formulation does not necessarily indicate an error. In the automatic evaluation, we compared
the optimal value obtained by solving the generated model with the known optimal value from the
dataset.
To enhance the reliability of the evaluation, we also conducted a round of manual verification of the
mathematical models.
Specifically, the 20 models generated by our framework and the 20 models
generated by the baseline approach using the standard prompt in the first trial were reviewed by
human experts.
This verification process assessed whether each model was fully correct when the
optimal value matched the reference, and also identified reasons for incorrect solutions. Importantly,
valid models with alternative formulation from the reference would not be marked as incorrect by the
human experts.
Readers may find examples of the prompts given to the baseline models as well as those used in the
multi-stage framework in Appendix B. Data used in this paper is available at: https://anonymous.
4open.science/r/C28M9232F635/.
4.2. Evaluation metrics
The performance of each method was assessed using two metrics:
• Model Accuracy (MA): Measures whether the generated MILP model is fully correct. A model
is considered correct only if all components (sets, variables, parameters, constraints, and the
objective function) are correctly defined and represented.
• Execution Accuracy (EA): Measures the correctness of the optimal value obtained by solving
the generated model. An instance is considered correctly solved if the computed optimal value
13


--- Page 14 ---
exactly matches the provided ground truth.
Each metric is calculated as the proportion of correctly processed instances out of the 20 total test
instances.
The metric MA reports the results of manual verification of the mathematical models
generated in the first trial. The metric EA is computed over all five trials, along with its average. The
formal mathematical formulas of the two metrics are given below:
MA = Number of test instances with fully correct MILP models
Total number of test instances
,
EA = Number of test instances with correct optimal values
Total number of test instances
.
4.3. Experimental results
4.3.1. Overall evaluation of framework and baseline
We evaluated the proposed multi-stage framework SMILO on the test set and compared its performance
with the baseline method. Both GPT-4o (zero-shot setting) and the multi-stage framework with GPT-
4o were tested on all instances, with each instance repeated five times, resulting in 100 generated
models per method. The correctness of the MILP models produced by each method was evaluated
automatically, based on whether the generated model yielded the correct optimal value. In addition,
the models generated in the first trial were manually reviewed by human experts. The findings from
manual inspection are discussed in the next subsection.
As shown in Table 1, the multi-stage framework, which integrates knowledge representations with
the large language model, significantly outperforms the one-step model generation approach in terms
of both automatic and manual evaluation metrics. The multi-stage framework achieves an average
Execution Accuracy (EA) of 90% across five trials and a model accuracy (MA) of 90% in the first
trial, demonstrating strong reliability and reproducibility in producing correct and consistent models.
For two problem instances, the framework consistently produced incorrect models across all five trials.
These failures were caused by the same type of modelling error, which we analyse in the next subsection.
Table 1: Comparison of execution accuracy (EA) across five trials and model accuracy (MA) in Trial 1
Method
EA
MA
Trial 1
Trial 2
Trial 3
Trial 4
Trial 5
Avg
Trial 1
SMILO
90%
90%
90%
90%
90%
90%
90%
GPT-4o with standard prompt
35%
35%
35%
55%
50%
42%
35%
GPT-4o with simple prompt
35%
35%
35%
35%
55%
39%
–
Regarding the performance of the baseline approach, using the standard prompt led to a modest
improvement in average EA, increasing from 39% with the simple prompt to 42%. However, none of
the models generated using the simple prompt resulted in infeasible solutions, whereas the standard
prompt led to 7 infeasible models out of 100. This indicates that while the standard prompt may
guide the model toward higher overall accuracy, it also introduces a higher risk of generating infeasible
models.
GPT-4o’s behaviour exhibited substantial variability. For instance, under the standard prompt set-
ting, it produced correct models for 9 problem instances in some trials but generated incorrect models
for the same instances in others. For 8 problem instances, the model consistently failed across all
five trials, although the types of errors varied. In contrast, three problem instances were consistently
solved correctly in every trial. They were the simplest problems in our dataset. The fluctuation in
correctness across trials for the same instance suggests a lack of output stability, even under determin-
istic decoding settings. The consistently failed cases, despite diverse error types, indicate that certain
14


--- Page 15 ---
problem attributes remain challenging for zero-shot prompting. Conversely, the consistently successful
cases suggest that for specific types of simple problems, GPT-4o is capable of reliably applying the
appropriate modelling logic.
To illustrate the differences in model quality between the baseline under the two prompting strategies
and our proposed framework, we present an empirical example comparing the generated mathematical
formulations, as shown in Figure 4. The example is a shift scheduling problem instance from our
dataset, and the models were produced during the first evaluation trial.
GPT-4o with the simple
prompt incorrectly defined the decision variables by failing to identify all possible shifts.
GPT-4o
with the standard prompt correctly defined the variables but misidentified the coverage of shifts over
periods, resulting in incorrect formulations of the first four constraints. SMILO generated a complete
and correct model.
Figure 4: Comparison of MILP models generated by GPT-4o using the simple prompt, GPT-4o using the standard
prompt, and the proposed SMILO framework. GPT-4o using the simple prompt misdefined the decision variables. GPT-
4o using the standard prompt generated incorrect constraints. SMILO generated a correct model.
Next, we shall look into the cases when incorrect models were generated, providing a detailed analysis
of the associated error types.
4.3.2. Analysis of error cases
To analyse the types of modelling errors, we manually reviewed the 20 models generated in the first
trial by our proposed framework and the 20 models generated by the baseline approach using the
15


--- Page 16 ---
standard prompt. The review showed that whenever the optimal value was correct, the corresponding
mathematical model (in LaTeX format) was also correct. In cases where the optimal value was incor-
rect, the underlying issue was generally related to model formulation rather than to code generation or
execution. Among the 40 reviewed models, only one exception was found in which the mathematical
model was correct, but an error in the generated code altered the meaning of the model and led to
an incorrect optimal value. Therefore, our error analysis focuses exclusively on modelling-related er-
rors. The results reveal that the primary challenges lie in correctly identifying the shift types, defining
appropriate decision variables, and handling their coverage over time periods. We classified the mod-
elling errors produced by the baseline method and our framework into nine categories, as summarised
in Table 2.
Table 2: Comparison of error types between the baseline and SMILO
Error type
GPT-4o with standard prompt
SMILO
Number
of errors
Overall
%
Error
%
Number
of errors
Overall
%
Error
%
Incorrect information extrac-
tion
-
-
-
2
10%
66.67%
Missing equality constraint
-
-
-
1
5%
33.33%
Incorrect coefficients in labour
demand constraints
8
40%
44.44%
-
-
-
Misinterpreted
objective
or
constraint semantics
3
15%
16.67%
-
-
-
Incorrect handling of cyclic
horizon
2
10%
11.11%
-
-
-
Uninstantiated symbolic pa-
rameters
2
10%
11.11%
-
-
-
Miscomputed objective coeffi-
cients
1
5%
5.56%
-
-
-
Missing break within shift
1
5%
5.56%
-
-
-
Incorrect linking constraint
1
5%
5.56%
-
-
-
Cases missed by the multi-stage framework
Among the models generated by the multi-stage framework, two types of modelling errors were ob-
served. In the first trial, the models generated for two of the 20 instances were incorrect, both belonging
to the days-off scheduling problem set. Across all five trials, these two instances consistently resulted
in incorrect models. Further manual inspection revealed that the modelling errors were consistently
caused by inaccurate information extraction in Stage 2 of the framework. In both cases, the component
identification in Stage 1 was consistently correct and successfully triggered the intended information
extraction tasks. It is also worth noting that the baseline model failed to correctly formulate the first
instance in all five trials, while for the second instance, it succeeded in four out of five trials.
In the first problematic instance, the objective was to minimise weekend bonuses. Employees who work
on Saturdays and Sundays receive a bonus of $25 per day. This implies that the cost of a shift depends
on whether it includes work on a weekend day. However, in all five runs, the extracted shift costs were
incorrect, leading to erroneous coefficients in the objective function. The second instance included a
constraint requiring that at least half of the employees have weekends off, meaning they do not work
on both Saturday and Sunday. The intended mathematical formulation ensures that the total number
of employees who rest on Saturday and Sunday is greater than or equal to half of the total number of
employees. However, in all five trials, the left-hand side of the inequality was incorrectly constructed
using variables that represent the number of employees whose shifts start on Saturday and Sunday,
rather than those who are actually off on those days. It is important to note that, for both information
extraction tasks, the prompts provided fully instantiated and accurate shift set information derived
16


--- Page 17 ---
from the corresponding problem instances. Each shift was specified by its identifier, start day, and
duration. With this information, it is straightforward to infer the working and resting days of each
shift over the weekly planning horizon. Therefore, these errors fundamentally stem from GPT-4o’s
failure to reason about which shifts are active on specific days, such as Saturday or Sunday.
After completing the main experiments, we conducted additional explorations to examine whether
improved prompting could mitigate such errors. Taking the first instance as an example, we modified
the prompt by explicitly providing each shift’s work days and rest days in natural language format (e.g.,
"rest_days":
["Saturday", "Sunday"]), instead of providing its start day and duration. Despite
this more intuitive representation, GPT-4o’s responses remained incorrect. For example, a shift starting
on Saturday and spanning five consecutive days was still incorrectly calculated to receive a weekend
bonus of $25, although it should have received $50 in total.
In the first instance, there was also one equality constraint that was correctly identified in Stage 1
of all five trials, but for which the extraction of Stage 2 returned no information. In other words,
GPT-4o failed to recognise this constraint in the problem description and treated it as nonexistent.
The intended constraint specifies that the total number of employees assigned to all shift types should
equal 25, ensuring that no employee remains unassigned. This constraint should be formulated as an
equality, where the sum of all shift-assignment variables equals a fixed constant. This failure may be
attributed to GPT-4o’s insensitivity to less explicitly stated constraints in natural language. In this
case, the original sentence in the problem description did not present the constraint in a direct or
formulaic manner, which may have made it more difficult for the LLM to recognise it as an equality
constraint.
Outside the main experiments, we made targeted efforts to improve the constraint recognition by
enhancing the prompt. Specifically, we incorporated two illustrative examples, each consisting of a
natural language description paired with the corresponding extracted constraint information, such as
identifying a restricted subset of shifts and specifying an upper bound on the total number of employees
assigned to them.
These examples were included in the dedicated prompt designed for extracting
the information required to formulate such equality constraints. However, even with this additional
contextual guidance, GPT-4o consistently returned an empty output for this constraint type.
Cases missed by the baseline approach
In the first trial, the baseline approach generated incorrect formulations for 13 out of the 20 problem
instances. These errors can be grouped into seven categories based on their nature and source. The
most frequent category involved incorrect coefficients in the labour demand constraints. In eight cases,
these coefficients were incorrect due to misidentification of key shift attributes, including duration, start
time, lunch break duration, and lunch break start time. These misinterpretations resulted in inaccurate
inference of workforce availability across time periods, which is fundamental for constructing correct
period-related labour demand constraints.
A second type of error, observed in three models, involved misinterpretation of the intended semantics
of the objective function or individual constraints. These issues often stemmed from GPT-4o’s inability
to correctly infer which shifts included work on weekends and which did not, leading to errors in the
formulation of constraints related to weekend bonuses or weekend rest requirements.
In two models, the cyclic nature of the planning horizon was not properly handled. Specifically, the
formulations failed to recognise that shifts extending beyond the final period continue into the initial
period of the subsequent cycle. This omission caused an underestimation of labour availability during
the early periods of the schedule.
Two models failed to fully instantiate parameters as required by the prompt, that is, they presented
symbolic representations without assigning specific numerical values. For instance, in one case, the
LaTeX formulation included a binary coverage parameter as,k, representing whether shift s covers
period k, but omitted its concrete values from the model and the associated list of symbol definitions
17


--- Page 18 ---
in the output. This omission shifted the responsibility for computing the binary matrix to the code
generation stage. However, necessary contextual details, such as the start and end time of each period,
were no longer present in the output, making accurate computation impossible, even though the shift
start times and durations in hours were provided. This type of failure reflects a broader issue: symbolic
notations are introduced without being accompanied by the necessary instance-specific data. As a
result, the generated model lacks the information required to support downstream processes, such as
code execution, undermining automation and correctness.
In addition to these primary error types, several less frequent issues were also observed. One model
contained incorrect coefficients in the objective function due to miscalculated shift costs. Another
model omitted the requirement of scheduling lunch breaks within shifts. A third model introduced
errors in linking constraints between variables. A summary of the error types and their frequencies is
presented in Table 2. In the table, the column “Overall Percentage" shows the frequency of each error
type relative to the total number of test instances (20 in total), while “Error Percentage" indicates the
proportion of each error type among all modelling errors identified in the first trial. Both metrics are
calculated separately for each method.
5. Conclusion
In this paper, we propose SMILO, an expert-knowledge-driven framework for generating MILP models
from natural language problem descriptions. SMILO adopts an interpretable three-stage pipeline, sup-
ported by problem-type-specific modelling graphs and their associated resources, including modelling
component descriptions, task-specific prompt templates for information extraction, and expert-defined
templates for variable definitions, parameter computations, and expression generation. To support
evaluation, we construct a comprehensive dataset of shift scheduling and days-off scheduling problems,
which reflect rich structural complexity such as diverse work patterns, break allocation, and overtime
rules. SMILO achieves 90% accuracy across five evaluation trials, as evaluated by expert validation of
model correctness and alignment with known optimal objective values, outperforming one-step LLM
baselines by at least 35%. These results demonstrate the effectiveness of combining expert knowledge
with LLM capabilities for robust and reliable optimisation modelling.
In addition to overall accuracy, we conduct a detailed analysis of error types across generated models.
Compared with the one-step LLM baseline using the standard prompt, SMILO exhibits significantly
fewer modelling errors, with only two distinct error types observed.
This highlights the ability of
SMILO in enhancing interpretability and diagnosability by structurally decomposing the modelling
process: LLMs are responsible only for information extraction, guided by knowledge-guided prompts,
while the mathematical formulation is handled through deterministic rule-based templates. This clear
separation improves interpretation, error traceability, and consistency in model generation.
In practical deployments, SMILO can be applied in production and logistics contexts such as factory line
staffing, last-mile driver shifts, and hospital or inspection team scheduling. Practitioner requirements
written in natural language are converted by the system into solver-ready MILP models and executable
code, reducing modelling time and enabling rapid model updates when staffing policies change. The
modelling graph can be maintained as a reusable asset that captures specific rules on shifts, breaks
and overtime. A light human-in-the-loop check on model accuracy and execution accuracy provides a
clear acceptance criterion prior to deployment. This practice improves planning speed and knowledge
transfer, while alleviating additional analytical workload for expert teams.
Several promising directions can further extend this work. First, extending SMILO to other optimisa-
tion problem types, such as inventory control, vehicle routing and dispatching, and capacity-constrained
production planning, requires the development of new modelling graphs. The modelling graphs could be
learned automatically by curating a dataset of annotated MILP models from academic publications.
Second, a hybrid formula generation mechanism could be explored, where expert-defined templates
are used to programmatically formulate standard modelling components and simultaneously serve as
18


--- Page 19 ---
prompts to guide LLMs in generating formulations for optimisation scenarios beyond the predefined
modelling graph. This hybrid approach could increase the flexibility and generalisability of the frame-
work. Third, transforming the modelling graphs into knowledge graphs may allow a richer semantic
representation of modelling components, including their attributes and interdependencies. This trans-
formation could facilitate more effective retrieval and reuse of modelling knowledge, thereby enhancing
the automation and adaptability of the model generation process.
Declaration of interests
The authors declare that they have no conflicts of interest.
References
Abdullin, Y., Molla-Aliod, D., Ofoghi, B., Yearwood, J. and Li, Q. (2024), ‘Synthetic Dialogue Dataset
Generation using LLM Agents’.
http://arxiv.org/abs/2401.17461
AhmadiTeshnizi, A., Gao, W., Brunborg, H., Talaei, S. and Udell, M. (2024), ‘OptiMUS-0.3: Using
large language models to model and solve optimization problems at scale’.
https://arxiv.org/abs/2407.19633
AhmadiTeshnizi, A., Gao, W. and Udell, M. (2023), ‘OptiMUS: Optimization Modeling Using mip
Solvers and large language models’.
http://arxiv.org/abs/2310.06116
Ahmed, T. and Choudhury, S. (2024), ‘LM4opt: Unveiling the potential of large language models in
formulating mathematical optimization problems’.
https://arxiv.org/abs/2403.01342
Alfares, H. K. (2004), ‘Survey, categorization, and comparison of recent tour scheduling literature.’,
Annals of Operations Research 127(1), 145–175.
Aykin, T. (1996), ‘Optimal shift scheduling with multiple break windows’, Management Science .
https://pubsonline.informs.org/doi/abs/10.1287/mnsc.42.4.591
Bard, J. F., Binici, C. and deSilva, A. H. (2003), ‘Staff scheduling at the united states postal service’,
Computers & Operations Research 30(5), 745–771.
https://www.sciencedirect.com/science/article/pii/S0305054802000485
Bartholdi, J. J., Orlin, J. B. and Ratliff, H. D. (1980), ‘Cyclic scheduling via integer programs with
circular ones’, Operations Research 28(5), 1074–1085.
https://pubsonline.informs.org/doi/10.1287/opre.28.5.1074
Bechtold, S. E. and Jacobs, L. W. (1990), ‘Implicit modeling of flexible break assignments in optimal
shift scheduling’, Management Science 36(11), 1339–1351.
https://pubsonline.informs.org/doi/10.1287/mnsc.36.11.1339
Boudribila, A., Tajer, A. and Boulghasoul, Z. (2025), ‘Intelligent extraction of manufacturing system
components from natural language using transformer models.’, International Journal of Production
Research pp. 1–25.
https://research.ebsco.com/linkprocessor/plink?id=81db2e54-3afa-3013-8a9b-21bf4735f0d0
Chen, B., Simchi-Levi, D., Wang, Y. and Zhou, Y. (2022), ‘Dynamic pricing and inventory control with
fixed ordering cost and incomplete demand information’, Management Science 68(8), 5684–5703.
https://research.ebsco.com/linkprocessor/plink?id=dd10e128-b871-378d-a6dd-fd1ca6feae81
19


--- Page 20 ---
Chen, H., Constante-Flores, G. E. and Li, C. (2023), ‘Diagnosing infeasible optimization problems
using large language models’, (arXiv:2308.12923).
http://arxiv.org/abs/2308.12923
Chen, P.-S., Huang, W.-T., Chen, G. Y.-H., Dang, J.-F. and Yeh, E.-C. (2024), ‘Constructing mod-
ified variable neighborhood search approaches to solve a nurse scheduling problem.’, International
Journal of Production Research 62(19), 7186–7204.
https://research.ebsco.com/linkprocessor/plink?id=eb0fed87-ce5f-38ed-8b65-500f9cb6369d
Dantzig, G. B. (1954), ‘Letter to the editor—a comment on edie’s “traffic delays at toll booths”’,
Journal of the Operations Research Society of America 2(3), 339–341.
https://pubsonline.informs.org/doi/10.1287/opre.2.3.339
Edie, L. C. (1954), ‘Traffic delays at toll booths’, Journal of the Operations Research Society of America
.
https://pubsonline.informs.org/doi/abs/10.1287/opre.2.2.107
Fan, Z., Ghaddar, B., Wang, X., Xing, L., Zhang, Y. and Zhou, Z. (2024), ‘Artificial intelligence for
operations research: Revolutionizing the operations research process’, (arXiv:2401.03244).
http://arxiv.org/abs/2401.03244
Fosso Wamba, S., Guthrie, C., Queiroz, M. M. and Minner, S. (2024), ‘ChatGPT and generative
artificial intelligence: an exploratory study of key benefits and challenges in operations and supply
chain management’, International Journal of Production Research 62(16), 5676–5696.
https://doi.org/10.1080/00207543.2023.2294116
Gaballa, A. and Pearce, W. (1979), ‘Telephone sales manpower planning at qantas’, Interfaces 9(3), 1–
9.
https://www.jstor.org/stable/25056329
Gaddam, J., Zhang, L., Mak-Hau, V., Yearwood, J., Ofoghi, B. and Molla-Aliod, D. (2025), Agent-
MILO: A knowledge-based framework for complex MILP modelling conversations with LLMs, in
‘2025 17th International Conference on Computer and Automation Engineering (ICCAE)’, pp. 176–
182. ISSN: 2154-4360.
https://ieeexplore.ieee.org/document/10980596
Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., Wang, M. and Wang, H. (2024),
‘Retrieval-augmented generation for large language models: A survey’, (arXiv:2312.10997).
http://arxiv.org/abs/2312.10997
Huang, C., Tang, Z., Hu, S., Jiang, R., Zheng, X., Ge, D., Wang, B. and Wang, Z. (2025), ‘ORLM: A
customizable framework in training large models for automated optimization modeling’, Operations
Research .
https://pubsonline.informs.org/doi/abs/10.1287/opre.2024.1233
Jackson, I., Ivanov, D., Dolgui, A. and Namdar, J. (2024), ‘Generative artificial intelligence in supply
chain and operations management: a capability-based framework for analysis and implementation’,
International Journal of Production Research 62(17), 6120–6145.
https://doi.org/10.1080/00207543.2024.2309309
Jackson, I., Jesus Saenz, M. and Ivanov, D. (2024), ‘From natural language to simulations: applying AI
to automate simulation modelling of logistics systems’, International Journal of Production Research
62(4), 1434–1457.
https://doi.org/10.1080/00207543.2023.2276811
Kau, A., He, X., Nambissan, A., Astudillo, A., Yin, H. and Aryani, A. (2024), ‘Combining knowledge
graphs and large language models’, (arXiv:2407.06564).
http://arxiv.org/abs/2407.06564
20


--- Page 21 ---
Kosasih, E. E., Margaroli, F., Gelli, S., Aziz, A., Wildgoose, N. and Brintrup, A. (2024), ‘Towards
knowledge graph reasoning for supply chain risk management using graph neural networks’, Inter-
national Journal of Production Research 62(15), 5596–5612.
https://doi.org/10.1080/00207543.2022.2100841
Li, B., Mellou, K., Zhang, B., Pathuri, J. and Menache, I. (2023), ‘Large Language Models for Supply
Chain Optimization’.
http://arxiv.org/abs/2307.03875
Li, J., Wickman, R., Bhatnagar, S., Maity, R. and Mukherjee, A. (2024), NL2or: Solve complex
operations research problems using natural language inputs.
https://www.semanticscholar.org/paper/NL2OR%3A-Solve-Complex-Operations-Research-Problems-Li-
116dbfca721897ff45ff5e99838b1f97d7c09d70
Li, Q., Zhang, L. and Mak-Hau, V. (2023), ‘Synthesizing mixed-integer linear programming models
from natural language descriptions’, (arXiv:2311.15271).
http://arxiv.org/abs/2311.15271
Liu, M., Tang, H., Chu, F., Zhu, Z. and Chu, C. (2024), ‘Food inspector scheduling with outcome and
daily-schedule effects.’, International Journal of Production Research 62(3), 737–766.
https://research.ebsco.com/linkprocessor/plink?id=950880dc-36ee-378b-82cf-ea3a55804722
Makatura, L., Foshey, M., Wang, B., HähnLein, F., Ma, P., Deng, B., Tjandrasuwita, M., Spielberg, A.,
Owens, C. E., Chen, P. Y., Zhao, A., Zhu, A., Norton, W. J., Gu, E., Jacob, J., Li, Y., Schulz, A. and
Matusik, W. (2023), ‘How can large language models help humans in design and manufacturing?’,
(arXiv:2307.14377).
http://arxiv.org/abs/2307.14377
Morris, J. G. and Showalter, M. J. (1983), ‘Simple approaches to shift, days-off and tour scheduling
problems’, Management Science 29(8), 942–950.
Mutlu, S., Benneyan, J., Terrell, J., Jordan, V. and Turkcan, A. (2015), ‘A co-availability scheduling
model for coordinating multi-disciplinary care teams.’, International Journal of Production Research
53(24), 7226–7237.
https://research.ebsco.com/linkprocessor/plink?id=1c7f6eb2-5c71-3a45-984d-03348226254c
Ramamonjison, R., Li, H., Yu, T. T., He, S., Rengan, V., Banitalebi-Dehkordi, A., Zhou, Z. and Zhang,
Y. (2022), ‘Augmenting Operations Research with Auto-Formulation of Optimization Models from
Problem Descriptions’.
http://arxiv.org/abs/2209.15565
Ramamonjison, R., Yu, T. T., Li, R., Li, H., Carenini, G., Ghaddar, B., He, S., Mostajabdaveh,
M., Banitalebi-Dehkordi, A., Zhou, Z. and Zhang, Y. (2023), ‘NL4Opt Competition: Formulating
Optimization Problems Based on Their Natural Language Descriptions’.
http://arxiv.org/abs/2303.08233
Tang, Z., Huang, C., Zheng, X., Hu, S., Wang, Z., Ge, D. and Wang, B. (2024), ‘ORLM: Training
large language models for optimization modeling’.
https://arxiv.org/abs/2405.17743
Thompson, G. M. (1995), ‘Improved implicit optimal modeling of the labor shift scheduling problem’,
Management Science 41(4), 595–607.
https://pubsonline.informs.org/doi/10.1287/mnsc.41.4.595
Tsouros, D., Verhaeghe, H., Kadıoğlu, S. and Guns, T. (2023), Holy Grail 2.0: From Natural Language
to Constraint Models.
21


--- Page 22 ---
Türker, T. and Demiriz, A. (2018), ‘An integrated approach for shift scheduling and rostering problems
with break times for inbound call centers’, Mathematical Problems in Engineering 2018(1), 7870849.
_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1155/2018/7870849.
https://onlinelibrary.wiley.com/doi/abs/10.1155/2018/7870849
Van den Bergh, J., Beliën, J., De Bruecker, P., Demeulemeester, E. and De Boeck, L. (2013), ‘Personnel
scheduling: A literature review’, European Journal of Operational Research 226(3), 367–385.
Xiao, Z., Zhang, D., Wu, Y., Xu, L., Wang, Y. J., Han, X., Fu, X., Zhong, T., Zeng, J., Song, M. and
Chen, G. (2023), Chain-of-experts: When LLMs meet complex operations research problems.
https://openreview.net/forum?id=HobyL1B9CZ
22


--- Page 23 ---
Appendix A. Modelling graphs
The shift scheduling MG is shown in Figure A.5 and Figure A.6. The days-off scheduling MG is shown
in Figure A.7 and Figure A.8.
Figure A.5: The modelling graph for shift scheduling (Part 1)
Appendix B. Prompt template examples
Appendix B.1. Component identification prompt (SMILO)
Listing 1 shows the prompt template designed to identify relevant modelling components in the first
stage of SMILO. The placeholder [Insert test instance here] specifies the location at which the
23


--- Page 24 ---
Figure A.6: The modelling graph for shift scheduling (Part 2, continued)
natural language problem description is dynamically inserted during runtime. The node list shown
here is a partial excerpt from the full node list of the shift scheduling MG used by SMILO. In practice,
the full list includes all modelling components representing entities, parameters and objectives, along
with their corresponding descriptions. The ellipsis (...) marks the continuation of additional node
entries not shown in this excerpt.
### Task: Identify Relevant Nodes for Each Sentence
#### Problem Description:
[Insert test instance here]
#### Modelling Graph Nodes:
Below are all the available modelling graph nodes. Each node has a **name** and a **
description**. Your task is to match each sentence from the **Problem Description** to
one or more of these nodes.
24


--- Page 25 ---
Figure A.7: The modelling graph for days-off scheduling (Part 1)
**Node Name**: Planning Horizon
**Description**: The total time period over which scheduling or planning is conducted.
**Node Name**: Periods
**Description**: The division of the planning horizon into smaller intervals, typically
aligned with labour demand requirements. The increment determines the length of each
period.
**Node Name**: Shifts
**Description**: Represents all possible shift types involved in the problem. Each shift is
defined by its start time and duration.
**Node Name**: Overtimes
**Description**: Represents all possible overtime works involved in the problem.
**Node Name**: Breaks
**Description**: The complete collection of all possible breaks across all types (e.g.,
lunch breaks, first breaks, second breaks). Each break is uniquely identified by its
type, start time, and duration.
**Node Name**: Types of breaks
25


--- Page 26 ---
Figure A.8: The modelling graph for days-off scheduling (Part 2, continued)
**Description**: Different predefined types of breaks assigned to employees, such as first
relief breaks, second relief breaks, and lunch breaks, based on specific rules or
schedules.
...
#### Instructions:
1. **Extract all sentences from the Problem Description**, ensuring they are listed in the
correct order.
2. **Assign a unique identifier to each sentence (e.g., "sentence_1", "sentence_2", etc.).**
3. **Ensure that each sentence is returned exactly as it appears in the original Problem
Description.**
4. **Match each sentence to up to 3 most relevant nodes from the Modelling Graph.**
5. **Ignore tables as separate sentences.** If a sentence introduces a table, it must still
be processed normally.
6. **Ensure the response follows the JSON format strictly** with the correct number of
sentences.
#### Expected Output Format:
‘‘‘json
{{
"sentences": [
"Sentence 1",
"Sentence 2",
"Sentence 3",
...
],
"matches": {{
"sentence_1": ["Node A", "Node B"],
"sentence_2": ["Node C"],
26


--- Page 27 ---
"sentence_3": []
}}
}}
Return only the JSON response. Do not include explanations or additional formatting.
Listing 1: Component identification prompt used in SMILO
Appendix B.2. Task-based prompts for information extraction (SMILO)
Listing 2 shows an example of a task-based prompt designed to extract the minimum number of em-
ployees required for each period. In this example, the content under the headings “Known Information"
and “Problem Description" is instance-specific and dynamically inserted at runtime. This task depends
on a prerequisite: extracting the set of periods. The prompt includes the extracted period information
as known input.
### Task Name
minimum number of employees required for each period
### Task Description
Extract the minimum number of employees required for each period in the set of periods.
Follow these steps:
1. For each period, identify the associated demand in the problem description.
2. Output the results in the specified format, associating each period with its
corresponding number of employees.
### Expected Output Format
{
"value": [
{
"period_id": [integer],
"min_employees": [integer]
},
...
]
}
Please ensure your response strictly follows the **Expected Output Format** provided above.
Avoid including any code implementations. The response must include a **valid JSON
object** containing only the requested information.
### Known Information
{
"set of periods": {
"value": [
{
"period_id": 1,
"start_time": "00:00",
"end_time": "04:00"
},
{
"period_id": 2,
"start_time": "04:00",
"end_time": "08:00"
},
{
"period_id": 3,
"start_time": "08:00",
27


--- Page 28 ---
"end_time": "12:00"
},
{
"period_id": 4,
"start_time": "12:00",
"end_time": "16:00"
},
{
"period_id": 5,
"start_time": "16:00",
"end_time": "20:00"
},
{
"period_id": 6,
"start_time": "20:00",
"end_time": "00:00"
}
],
"details": {
"horizon": "1440",
"increment": "240",
"total_periods": "6"
}
}
}
### Problem Description
Consider a bus company scheduling drivers for its buses. The requirement for buses varies
from hour to hour because of customer demand, as shown in the following table.
The problem is to determine how many drivers to schedule at each starting time to cover the
requirements for buses. Drivers work eight hour shifts that start at times: 0:00, 4:00,
8:00, 12:00, 16:00 or 20:00. For example, a driver starting at 0:00 can drive a bus from
0:00 to 8:00. A driver scheduled to start at 20:00 works for the final four hours of
the day and the first four hours of the next day. The goal is to minimise the number of
drivers used.
| Time Interval | Number of Buses |
|---------------------|---------------------------|
| 12 midnight - 4 A.M.| 4 |
| 4 A.M. - 8 A.M. | 8 |
| 8 A.M. - 12 noon | 10 |
| 12 noon - 4 P.M. | 7 |
| 4 P.M. - 8 P.M. | 12 |
| 8 P.M. - 12 midnight| 4 |
Listing 2: Prompt for extracting the minimum number of employees required for each period
Listing 3 presents the prompt designed to extract the set of workload-specific shifts in days-off schedul-
ing problems. In this example, the content under the headings “Known Information" and “Problem
Description" is instance-specific and dynamically inserted at runtime. As this task depends on the def-
inition of the set of days and the set of workload types, the prompt incorporates information extracted
from the prerequisite tasks concerning period definitions and workload definitions as known input. We
also designed a more general prompt template for extracting the set of shifts, defined as the parent
category task for extracting workload-specific shifts. Since extracting the set of workload-specific shifts
is a more fine-grained task, it is executed in cases where both tasks are matched to a problem instance.
### Task Name
set of workload-specific shifts
### Task Description
28


--- Page 29 ---
For each daily workload type, extract all valid shift patterns by identifying the possible
start days and shift durations within the planning horizon. Follow these steps:
1. Extract the total number of consecutive workdays per shift for each workload type.
2. Determine possible start days:
- If the problem description explicitly specifies valid start days, extract those.
- Otherwise, assume that any day in the planning horizon is a valid start day.
3. Generate all valid shifts, where each shift is defined by:
- Start day (must be a valid start day).
- Duration (total number of consecutive workdays per shift).
### Expected Output Format
{
"value": [
{
"workload_type": "[Workload type name]",
"shifts": [
{
"shift_id": [integer],
"start_day": "[Start day in planning horizon (e.g., Day 1, Day 2, ..., Day N)]",
"duration": "[Number of consecutive working days (excluding overtime days)]"
},
...
]
},
...
],
"total_shifts": [integer]
}
Please ensure your response strictly follows the **Expected Output Format** provided above.
Avoid including any code implementations. The response must include a **valid JSON
object** containing only the requested information.
### Known Information
{
"set of periods": {
"value": [
{
"period_id": 1,
"day": "Day 1"
},
{
"period_id": 2,
"day": "Day 2"
},
{
"period_id": 3,
"day": "Day 3"
},
{
"period_id": 4,
"day": "Day 4"
},
{
"period_id": 5,
"day": "Day 5"
},
{
"period_id": 6,
"day": "Day 6"
29


--- Page 30 ---
},
{
"period_id": 7,
"day": "Day 7"
}
],
"details": {
"horizon": "7 days",
"increment": "1 day"
}
},
"set of workload types": {
"value": [
{
"workload_type": "Full-time",
"workload": 1
},
{
"workload_type": "Part-time",
"workload": 0.5
}
]
}
}
### Problem Description
A post office requires different numbers of full-time employees and part-time employees on
different days of the week. The table specifies the daily labour hours required for each
day of the week. During each week, a full-time employee works 8 hours a day for five
consecutive days, and a part-time employee works 4 hours a day for five consecutive days
. These shifts can start on any day of the week.
The number of part-time employees should not exceed 25% of the total number of employees,
including both full-time and part-time staff.
A full-time employee costs the post office $600 per week, whereas a part-time employee (with
reduced fringe benefits) costs the post office only $200 per week. Formulate an LP
model to minimise the weekly labour costs for the post office.
| Day of the week | Number of hours required |
|-----------|--------------------------|
| Monday | 136 |
| Tuesday | 104 |
| Wednesday | 120 |
| Thursday | 152 |
| Friday | 112 |
| Saturday | 128 |
| Sunday | 88 |
Listing 3: Prompt for extracting the set of workload-specific shifts
Appendix B.3. LaTeX-to-Gurobi code generation prompt (all methods)
To enable automatic solution and evaluation, we developed a code generation prompt that guides
GPT-4o to convert the LaTeX-formatted mathematical model into executable Python code, which
can be solved using the Gurobi solver.
This conversion procedure was applied consistently across
all experimental settings. The placeholder [Insert the Latex-formatted mathematical model of
the test instance here] specifies the location at which the Latex-formatted mathematical model
is dynamically inserted during runtime.
30


--- Page 31 ---
You are an expert in mathematical optimisation and Python programming using Gurobi.
Your job is to translate LaTeX-formatted mathematical programming models into Python code
that can be executed with the Gurobi optimiser.
Do not change any model structure or parameter values.
Use the ‘gurobipy‘ API. Ensure the script can run directly when copied into a .py file.
The final output must be a single Markdown code block starting with ‘‘‘python and ending
with ‘‘‘.
Do not include any explanation or commentary before or after the code block.
You must retain Gurobi’s default solver output from model.optimize() (do not disable it).
After solving the model:
- If an optimal solution is found, print the results in the following format:
- A line starting with: ‘Optimal objective value: ‘ followed by the numerical value
- A line starting with: ‘Variable values:‘
- Then print each variable that has a non-zero value in the format: ‘VarName = Value‘
- If no optimal solution is found, print: ‘No optimal solution found.‘
You may implement this logic however you see fit, but the printed output must follow this
format exactly.
Listing 4: System prompt for LaTeX-to-Gurobi conversion
Given the following mathematical programming model written in LaTeX, please convert it into
executable Python code using the Gurobi optimiser.
Requirements:
- Do not change any of the model’s sets, parameters, variables, or expressions. Follow the
structure of the model closely. All variables should use the same names and indices as
in the original model.
- Use the ‘gurobipy‘ API. The output must be a complete, standalone Python script that can
run directly when saved as a .py file.
- Keep Gurobi’s default solver output when calling model.optimize().
- After solving:
- If an optimal solution is found, print the objective value using the format: ‘Optimal
objective value: <value>‘
- Then print all variables with non-zero values, preceded by a line that says: ‘Variable
values:‘
- If no optimal solution is found, print: ‘No optimal solution found.‘
- Only output a single Markdown code block starting with ‘‘‘python and ending with ‘‘‘.
- Do not include any explanation before or after the code block.
Model:
[Insert the Latex-formatted mathematical model of the test instance here]
Listing 5: User prompt for LaTeX-to-Gurobi conversion
Appendix B.4. Simple and standard prompts for one-step model generation (Baseline GPT-4o)
As a baseline, we used GPT-4o to directly generate LaTeX-formatted mathematical models from nat-
ural language problem descriptions. We designed two modelling prompts: one with minimal guidance,
referred to as the simple prompt, and another with more detailed and rigorous instructions, referred to as
the standard prompt. The placeholder [Insert the problem description of the test instance
here] specifies the location at which the natural language problem description is dynamically inserted
during runtime.
You are an expert in Mixed-Integer Linear Programming (MILP). When given a problem
description, identify decision variables, objective functions, and constraints. Provide
explanations first, and then output the complete mathematical model at the end in the
following format:
31


--- Page 32 ---
### Begin MILP Model
... (LaTeX formatted model) ...
### End MILP Model
[Insert the problem description of the test instance here]
Listing 6: The simple prompts for GPT-4o
You are an expert in Mixed-Integer Linear Programming (MILP). When given a problem
description, generate the mathematical model by extracting decision variables, objective
functions, and constraints.Follow these instructions carefully:
1. Decision Variables: Clearly define all decision variables, including their meaning, type
(e.g., binary, integer, continuous), and associated indices. Indices must be derived
from the problem description and explicitly defined with their domain.
2. Parameters and Constants:
- If the problem description contains specific numerical values for parameters (e.g., in
tables or lists), you must not omit them.
- You may either:
(i) include the concrete values directly in the LaTeX model, or
(ii) use parameter symbols (e.g., \( b_i \)) but you must then include the full list of
values (e.g., \( b_1 = 10, b_2 = 15, \ldots \)) in the Symbol Definitions section.
- Do not use symbolic parameters without providing their corresponding values if those
values are available in the input.
3. Intermediate Expressions: If a symbol is introduced as an expression derived from
decision variables and/or known parameters, define it clearly as an intermediate
expression.
4. Mathematical Model: Provide a complete and well-structured MILP model using LaTeX format.
All symbols used in the model must be explicitly defined and, where applicable, linked
to specific values. Avoid using undefined or abstract placeholders.
5. Output Structure:
- First, explain the modelling process in plain language, including the meaning of the
decision variables, the objective, and constraints.
- Then output the mathematical model and symbol definitions in the following format:
### Begin MILP Model
... (LaTeX formatted model with objective, constraints, and all necessary terms. Use
numerical values directly where appropriate) ...
### End MILP Model
### Begin Symbol Definitions
... (List of all symbols, including decision variables, parameters/constants with specific
values if available, index sets, and intermediate expressions) ...
### End Symbol Definitions
Important: Never omit numerical values provided in the problem description. If a parameter
symbol is used in the model, its values must be listed. Do not abstract concrete values
unless necessary for clarity or scalability.
[Insert the problem description of the test instance here]
Listing 7: The standard prompts for GPT-4o
32
