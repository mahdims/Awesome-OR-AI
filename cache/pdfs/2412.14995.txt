--- Page 1 ---
HSEvo: Elevating Automatic Heuristic Design with Diversity-Driven Harmony
Search and Genetic Algorithm Using LLMs
Pham Vu Tuan Dat1, Long Doan2, Huynh Thi Thanh Binh1
1Hanoi University of Science and Technology, Hanoi, Viet Nam
2George Mason University, Virginia, United States
dat.pvt210158@sis.hust.edu.vn, ldoan5@gmu.edu, binhht@soict.hust.edu.vn
Abstract
Automatic Heuristic Design (AHD) is an active research area
due to its utility in solving complex search and NP-hard com-
binatorial optimization problems in the real world. The recent
advancements in Large Language Models (LLMs) introduce
new possibilities by coupling LLMs with evolutionary com-
putation to automatically generate heuristics, known as LLM-
based Evolutionary Program Search (LLM-EPS). While pre-
vious LLM-EPS studies obtained great performance on var-
ious tasks, there is still a gap in understanding the proper-
ties of heuristic search spaces and achieving a balance be-
tween exploration and exploitation, which is a critical factor
in large heuristic search spaces. In this study, we address this
gap by proposing two diversity measurement metrics and per-
form an analysis on previous LLM-EPS approaches, includ-
ing FunSearch, EoH, and ReEvo. Results on black-box AHD
problems reveal that while EoH demonstrates higher diversity
than FunSearch and ReEvo, its objective score is unstable.
Conversely, ReEvo’s reflection mechanism yields good ob-
jective scores but fails to optimize diversity effectively. With
this finding in mind, we introduce HSEvo, an adaptive LLM-
EPS framework that maintains a balance between diversity
and convergence with a harmony search algorithm. Through
experimentation, we find that HSEvo achieved high diver-
sity indices and good objective scores while remaining cost-
effective. These results underscore the importance of balanc-
ing exploration and exploitation and understanding heuristic
search spaces in designing frameworks in LLM-EPS.
Code — https://github.com/datphamvn/HSEvo
1
Introduction
Heuristics are widely utilized to address complex search
and NP-hard combinatorial optimization problems (COPs)
in real-world systems. Over the past few decades, significant
efforts have been made to develop efficient heuristics, result-
ing in the creation of many meta-heuristic methods such as
simulated annealing, tabu search, and iterated local search.
These meticulously crafted methods have been effectively
applied across various practical systems.
Nevertheless, varied applications with distinct constraints
and goals may necessitate different algorithms or algorithm
Copyright © 2025, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
setups. The manual creation, adjustment, and configuration
of a heuristic for a specific problem can be extremely labo-
rious and requires substantial expert knowledge. This is a
bottleneck in many application domains. To address this is-
sue, Automatic Heuristic Design (AHD) has been proposed
aims to selects, tunes, or constructs effective heuristics for a
given problem class automatically (Choong, Wong, and Lim
2018). Various approaches have been used in AHD, includ-
ing Hyper-Heuristics (HHs) (Pillay and Qu 2018) and Neu-
ral Combinatorial Optimization (NCO) (Qu, Kendall, and
Pillay 2020). However, HHs are still constrained by heuristic
spaces that are predefined by human experts. Additionally,
NCO faces limitations related to the necessity for effective
inductive bias (Drakulic et al. 2024), and challenges regard-
ing interpretability and generalizability (Liu et al. 2023).
Recently, the rise of Large Language Models (LLMs)
has opened up new possibilities for AHD. It is believed
that LLMs (Nejjar et al. 2023; Austin et al. 2021) could
be a powerful tool for generating new ideas and heuris-
tics. However, standalone LLMs with prompt engineering
can be insufficient for producing novel and useful ideas be-
yond existing knowledge (Mahowald et al. 2024). Some at-
tempts have been made to coupling LLMs with evolutionary
computation to automatically generate heuristics, known as
LLM-based Evolutionary Program Search (LLM-EPS) (Liu
et al. 2024b; Meyerson et al. 2024; Chen, Dohan, and So
2024). Initial works such as FunSearch (Romera-Paredes
et al. 2024) and subsequent developments like Evolution
of Heuristic (EoH) (Liu et al. 2024a) and Reflective Evolu-
tion (ReEvo) (Ye et al. 2024b) have demonstrated significant
improvements over previous approaches, generating qual-
ity heuristics that often surpass current methods. Even so,
ReEvo yields state-of-the-art and competitive results com-
pared with evolutionary algorithms, neural-enhanced meta-
heuristics, and neural solvers.
A key difference between LLM-EPS and classic AHD lies
in the search spaces of heuristics. Classic AHD typically op-
erates within well-defined mathematical spaces such as Rn,
whereas LLM-EPS involves searching within the space of
functions, where each function represents a heuristic as a
program. LLM-EPS utilizes LLMs within an evolutionary
framework to enhance the quality of generated functions it-
eratively. Therefore, it is crucial to study and understand the
search spaces of heuristics to establish foundational theories
arXiv:2412.14995v1  [cs.NE]  19 Dec 2024


--- Page 2 ---
100K
200K
300K
400K
Total tokens
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Objective score
100K
200K
300K
400K
Total tokens
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Objective score
100K
200K
300K
400K
Total tokens
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Objective score
2.5
3.0
3.5
4.0
4.5
5.0
Diversity index
2.5
3.0
3.5
4.0
4.5
5.0
Diversity index
2.5
3.0
3.5
4.0
4.5
5.0
Diversity index
Objective score 
SWDI 
CDI 
Figure 1: Diversity indices and objective scores of ReEvo framework on BPO problem through different runs.
and principles for the automatic design of algorithms. This
aspect has been largely unconsidered in previous LLM-EPS
frameworks.
In this paper, we introduce two diversity metrics inspired
by the Shannon entropy to monitor population diversity.
We also explore relationships between our proposed di-
versity metrics with objective performance of LLM-EPS
frameworks on different optimization problems. Through
our analysis, we gain an understanding on the characteris-
tics and properties of heuristic search spaces in LLM-EPS
frameworks. Finally, building on these foundational princi-
ples, we propose a new framework called HSEvo, which
incorporates a new components based on harmony search
algorithm (Shi, Han, and Si 2012) and enhances other com-
ponents, including initialization, crossover, and mutation, to
optimize objective scores and diversity metrics.
In summary, our contributions are as follows:
• Two
diversity
measurement
metrics,
the
Shan-
non–Wiener
Diversity
Index
and
the
Cumulative
Diversity Index, are used to evaluate the evolutionary
progress of populations within the LLM-EPS framework.
• A novel framework, HSEvo, that aims to balance be-
tween the diversity and objective performance to improve
the optimization process.
2
Background and Related Works
2.1
LLM-based Evolutionary Program Search
Recent advances in LLM-EPS have shown promising results
in AHD. Evolutionary methods have been adopted in both
code generation (Nejjar et al. 2023; Ma et al. 2023; Hem-
berg, Moskal, and O’Reilly 2024) and text generation (Guo
et al. 2023; Yuksekgonul et al. 2024). Notable among these
methods are FunSearch, EoH, and ReEvo. FunSearch em-
ploys an island-based evolution strategy, leveraging LLMs
like Codey and StarCoder to evolve heuristics for mathemat-
ical and COPs, outperforming traditional methods on tasks
such as the cap set and admissible set problems. EoH uti-
lizes genetic algorithm with Chain of Thought prompt engi-
neering, consistently achieving superior performance in the
traveling salesman and online bin packing problems. ReEvo
introduces a reflective component to the evolution process,
employing two instances of GPT-3.5 to generate and refine
heuristics, which has demonstrated effectiveness across var-
ious optimization tasks. These methods highlight the poten-
tial of integrating LLMs with evolutionary strategies to en-
hance the efficiency and effectiveness of AHD solutions.
2.2
Diversity in Evolutionary Computation
Diversity plays a pivotal role in enhancing the efficiency
of algorithms in multi-objective optimization within the
domain of evolutionary computation (Solteiro Pires, Ten-
reiro Machado, and de Moura Oliveira 2014). Numerous
studies have investigated various methods to measure and
maintain diversity within populations, as it critically im-
pacts the convergence and overall performance of these
algorithms (Pires, Tenreiro Machado, and Moura Oliveira
2019; Wang and Chen 2012). Among these, the application
of Shannon entropy has been particularly prominent in quan-
tifying diversity and predicting the behavior of genetic algo-
rithms. However, to the best of our knowledge, no existing
studies have thoroughly explored diversity within the con-
text of LLM-EPS. This gap in the literature motivates our
in-depth exploration of diversity in LLM-EPS frameworks.
3
Diversity measurement metrics
In this section, we introduce a method to encode LLM-EPS
population and propose two diversity measurement metrics,
Shannon-Wiener diversity index (SWDI) and cummulative
diversity index (CDI). We also conduct a diversity analy-
sis on previous LLM-EPS frameworks, FunSearch, EoH and
ReEvo.
3.1
Population encoding
One particular problem of measuring diversity in LLM-EPS
is how to encode the population. While each individual in
traditional evolutionary algorithm is encoded as a vector, in
LLM-EPS they are usually presented as a string of code
snippet/program. This poses a challenge in applying pre-
vious diversity metrics on population of LLM-EPS frame-
works. To tackle this issue, we suggest an encoding ap-
proach consists of three steps: (i) removing comments and
docstrings using abstract-syntax tree, (ii) standardizing code


--- Page 3 ---
100K
200K
300K
400K
Total tokens
2.0
2.5
3.0
3.5
4.0
4.5
Objective score
BPO
100K
200K
300K
400K
Total tokens
0
2
4
6
8
10
Objective score
TSP
100K
200K
300K
400K
Total tokens
14.5
14.0
13.5
13.0
Objective score
OP
2.0
3.0
4.0
5.0
6.0
2.5
3.0
3.5
4.0
4.5
5.0
5.5
6.0
3.5
4.0
4.5
5.0
5.5
6.0
6.5
ReEvo
CDI 
FunSearch
Objective score 
EoH
Figure 2: CDI and objective scores of previous LLM-EPS on different AHD problems.
snippets into a common coding style (e.g., PEP81), (iii) con-
vert code snippets to vector representations using a code em-
bedding model.
3.2
Shannon–Wiener Diversity Index
Inspired by ecological studies, SWDI (Nolan and Callahan
2006) provides a quantitative measure of species diversity
within a community. In the context of search algorithms, this
index aims to quantify the diversity of the population at any
given time step based on clusters of individuals. To compute
the SWDI for a given set of individuals within a community,
or archive, first, we need to determine the probability dis-
tribution of individuals across clusters. This is represented
as:
pi = |Ci|
M
(1)
where Ci is a cluster of individuals and M represents the to-
tal number of individuals across all clusters {C1, . . . , CN}.
The SWDI, H(X), is then calculated using the Shannon en-
tropy:
H(X) = −
N
X
i=1
pi log(pi)
(2)
This index serves as a crucial metric in maintaining the
balance between exploration and exploitation within heuris-
tic search spaces. A higher index score suggests a more uni-
form distribution of individuals across the search space, thus
promoting exploration. Conversely, a lower index score indi-
cates concentration around specific regions, which may en-
hance exploitation but also increase the risk of premature
convergence.
To obtain SWDI score, at each time step t, we add all
individuals Ri = {r1, . . . , rn} generated at time step t to
an archive. We encode the archive using our proposed pop-
ulation encoding in Section 3.1 to obtain their vector rep-
resentations V = {v1, . . . , vn}. After that, we compute the
similarity between two embedding vectors vi and vj using
cosine similarity:
similarity(vi, vj) =
vi · vj
∥vi∥∥vj∥
1https://peps.python.org/pep-0008
To find clusters in the archive, we consider each embed-
ding vector vi. We assign vi to a cluster Ci if the similarity
between vi and all members in Ci is greater than a threshold
α. Mathematically, vi is assigned to Ci if
similarity(vi, vk) ≥α
∀k ∈{1, . . . , |Ci|}
If no cluster Ci ∈{C1, . . . , CN} can satisfy the above con-
dition, we create a new cluster CN+1 and assign vi to CN+1.
Finally, we compute SWDI score by computing Eq. (1), (2)
across found clusters.
3.3
Cumulative Diversity Index
While SWDI focuses on diversity of different groups of in-
dividuals, the Cumulative Diversity Index (CDI) plays a cru-
cial role in understanding the spread and distribution of the
whole population within the search space (Jost 2006). In the
context of heuristic search, the CDI measures how well a
system’s energy, or diversity, is distributed from a central-
ized state to a more dispersed configuration.
To calculate the CDI, we also consider all individu-
als within an archive, represented by their respective em-
beddings. We construct a minimum spanning tree (MST)
that connects all individuals within the archive A, where
each connection between individuals is calculated using Eu-
clidean distance. This MST provides a structure to assess the
diversity of the population. Let di represent the distance of
an edge within the MST, where i ∈{1, 2, . . . , #A−1}. The
probability distribution of these distances is given by:
pi =
di
P#A−1
j=1
dj
The cumulative diversity is then calculated using the
Shannon entropy:
H(X) = −
#A−1
X
i=1
pi log(pi)
This approach allows us to capture the overall diversity
of the search space, providing insights into the spread of so-
lutions. Higher CDI values indicate a more distributed and
diverse population, which is essential for maintaining a ro-
bust search process.


--- Page 4 ---
Initiation 
population 
of heuristic
xN
Selection
Crossover
Flash Reflection
Elitist Mutation
Harmony Search
x2
xN
+
xN
...
...
xM
xK
Harmony
Search
*
Try_HS 
is False
+
Heuristic
Deep analysis
LLM
Elitist Heuristic
Reflection
Flow direction
Stop
The Last
Generation?
Figure 3: Overview of the HSEvo framework.
An interesting point in Shannon diversity index (SDI) the-
ory is that normalizing the SDI with the natural log of rich-
ness is equivalent to the evenness value
H′
ln(S) where H′ rep-
resents the richness of SDI and S is the total number of
possible categories (Heip et al. 1998). The significance of
evenness is to demonstrate how the proportions of pi are dis-
tributed. Now, consider normalizing with the two current di-
versity indices. First, with SWID, if we have N clusters and
the number of individuals in each cluster from C1 to CN is
equal, then evenness will always be 1, regardless of the value
of N. This is not the desired behavior since we consider the
number of clusters to be a component of diversity. Similarly,
normalizing CDI will also ignore the impact of the number
of candidate heuristics, which correspond to the nodes in the
MST. This leads to the proposal not to normalize the SWID
and CDI metrics in order to achieve a [0, 1] bound.
3.4
Exploring the correlation between objective
score and diversity measurement metrics
To examine the correlation between the two diversity mea-
surement metrics and objective score, we conducted three
experimental runs using ReEvo on bin-packing online prob-
lem (Seiden 2002). The experiment details can be found in
the Appendix. The objective scores and the two diversity
metrics from these runs are presented in Fig. 1. Additional
results for other tasks are also available in the Appendix.
From the figure, there are a few observations that we can
drawn. First, the two diversity measurement metrics have a
noticeable correlation on the objective scores of the prob-
lem. When the objective score is converged into a local op-
tima, the framework either try to focus on the exploration
by increasing the diversity of the population, through the in-
crease of SWDI in first and second run, or try to focus on the
exploitation of current population, thus decrease the SWDI
in the third run. We can also see that focusing on explo-
ration can lead to significant improvement on the objective
score, while focusing on exploitation can make the popula-
tion stuck in local optima for a longer time. However, if the
whole population is not diverse enough, as can be seen with
the low CDI of the second run, the population might not be
able to escape the local optima.
3.5
Diversity analysis on previous LLM-EPS
framework
To analyse the overall diversity of previous LLM-EPS
frameworks, we conduct experiments on three differ-
ent LLM-EPS frameworks, including FunSearch (Romera-
Paredes et al. 2024), ReEvo (Ye et al. 2024b), and EoH (Liu
et al. 2024a), on three distinct AHD problems, bin-packing
online (BPO), traveling salesmen problem with guided local
search solver (TSP) (Voudouris and Tsang 1999), and orien-
teering problem with ACO solver (OP) (Ye et al. 2024a). The
details of each experiment are presented in the Appendix.
Note that, as highlighted in the previous section, SWDI fo-
cuses on understanding the diversity of the population dur-
ing a single run. As such, it may not be helpful for quanti-
fying the diversity across multiple experiment runs. Fig. 2
presents the experiment results.
In BPO and TSP, EoH obtain the highest CDI but got the
worst objective score. This implies that EoH does not fo-
cus enough on exploitation to optimize the population bet-
ter. In contrast, while ReEvo and FunSearch obtain lower
CDI than EoH on BPO and TSP, they achieve a better ob-
jective performance on all three problems. The experiments
on BPO and TSP problems highlight the inherent trade-off
between diversity and objective performance of LLM-EPS
frameworks. However, on OP problem, we can see that a
high CDI is required to obtain a better objective score, which
align with our findings in Section 3.4.


--- Page 5 ---
1
import numpy as np
2
3
def heuristics_v2(prize: np.ndarray,
4
distance: np.ndarray, maxlen: float) -> np.
ndarray:
5
reward_distance_ratio = prize / distance
6
cost_penalty = np.exp(-distance)
7
8
heuristics = (prize * prize[:, np.
newaxis])
9
heuristics[distance > maxlen] = 0
10
11
return heuristics
(a) Origin Code
1
import numpy as np
2
3
def heuristics_v2(prize: np.ndarray,
4
distance: np.ndarray, maxlen: float,
5
reward_threshold: float = 0,
6
distance_threshold: float = 0,
7
cost_penalty_weight: float = 1) -> np.ndarray:
8
reward_distance_ratio = prize / distance
9
cost_penalty = np.exp(-distance)
10
11
heuristics = (prize * prize[:, np.newaxis]) / (distance *
distance) * cost_penalty
12
heuristics[(distance > maxlen) | (reward_distance_ratio <
reward_threshold) | (distance < distance_threshold)] = 0
13
14
return heuristics
15
16
parameter_ranges = {
17
’reward_threshold’: (0, 1),
18
’distance_threshold’: (0, 100),
19
’cost_penalty_weight’: (0, 2)
20
}
(b) Modified Code using LLMs
Figure 4: An example of how Harmony search component works in HSEvo.
Description of Setting
Value
LLM (generator and reflector)
gpt-4o-mini-2024-07-18
LLM temperature (generator and reflector)
1
Maximum budget tokens
425K tokens
Population size (for EoH, ReEvo and HSEvo)
30 (initial stage), 10 (other stages)
Mutation rate (for ReEvo and HSEvo)
0.5
# of islands, # of samples per prompt (for FunSearch)
10, 4
Number of independent runs per experiment
3
HS size, HMCR, PAR, bandwidth, max iterations (for Harmony Search)
5, 0.7, 0.5, 0.2, 5
Maximum evaluation time for each heuristic
100 sec (TSP-GLS), 50 sec (Other)
Table 1: Summary of parameters settings.
4
Automatic Heuristic Design with HSEvo
In this section, we propose a novel LLM-EPS framework
called Harmony Search Evolution (HSEvo). HSEvo aim to
promote the diversity of the population while still achieve
better optimization and alleviate the trade-off between diver-
sity and optimization performance with a individual tuning
process based on harmony search. HSEvo also aim to reduce
the cost incurred from LLM with an efficient flash reflection
component. Figure 3 illustrates the pipeline of our HSEvo
framework. Examples of prompts used in each stage can be
found in the Appendix.
Individual encoding. Follow previous works in LLM-
EPS (EoH (Liu et al. 2024a), ReEvo (Ye et al. 2024b)),
HSEvo encodes each individual as a string of code snippet
generated by LLMs (Fig. 4a). This encoding method allow
the flexibility of each individual, i.e., not constrained by any
predefined encoding format, and also make it easier to eval-
uate on the AHD problem.
Initialization. HSEvo initializes a heuristic population by
prompting the generator LLM with task specifications that
describe the problem and detail the signature of the heuris-
tic function to be searched. Additionally, to leverage exist-
ing knowledge, the framework can be initialized with a seed
heuristic function and/or external domain knowledge. We
promote diversity by implementing in the prompts with var-
ious role instructions (e.g., ”You are an expert in the domain
of optimization heuristics...”, ”You are Albert Einstein, de-
veloper of relativity theory...”). This approach aims to en-
rich the heuristic generation process by incorporating di-
verse perspectives and expertise.
Selection. HSEvo randomly selects parent pairs from the
population, aiming to maintain the balance between explo-
ration and exploitation during our optimization process. The
random selection may also counteract premature conver-
gence, which is observed through the SWID trajectory out-
lined in Section 3.4.
Flash reflection. Reflections can provide LLMs with re-
inforcement learning reward signals in a verbal format for
code generation tasks, as discussed by (Shinn et al. 2024).
Later, (Ye et al. 2024b) also proposed integrating Reflections
into LLM-EPS in ReEvo as an approach analogous to pro-
viding “verbal gradient” information within heuristic spaces.


--- Page 6 ---
Method
BPO
TSP
OP
CDI (↑)
Obj. (↓)
CDI (↑)
Obj. (↓)
CDI (↑)
Obj. (↓)
FunSearch
4.97 ± 0.24
2.05 ± 2.01
5.24 ± 0.14
0.09 ± 0.06
-
-
EoH
5.86 ± 0.49
3.17 ± 2.97
5.81 ± 0.23
1.09 ± 3.11
6.17 ± 0.42
−14.62 ± 0.22
ReEvo
4.91 ± 0.53
2.48 ± 3.74
5.15 ± 0.19
0.05 ± 0.06
5.02 ± 0.13
−14.54 ± 0.21
HSEvo (ours)
5.68 ± 0.35
1.07 ± 1.11
5.41 ± 0.21
0.02 ± 0.03
5.67 ± 0.41
−14.62 ± 0.12
Table 2: Experiment results on different AHD problems.
Method
OP
CDI (↑)
Obj. (↓)
ReEvo
5.02 ± 0.13
−14.54 ± 0.21
ReEvo + HS
5.11 ± 0.27
−14.58 ± 0.38
HSEvo (ours)
5.67 ± 0.41
−14.62 ± 0.12
Table 3: Ablation results on harmony search.
Method
OP
CDI (↑)
Obj. (↓)
ReEvo
4.43 ± 0.23
−13.84 ± 1.13
ReEvo + F.R.
4.63 ± 0.37
−14.36 ± 0.19
HSEvo (ours)
4.77 ± 0.46
−14.07 ± 0.38
Table 4: Ablation results on flash reflection.
However, we argue that reflecting on each pair of heuristic
parents individually is not generalized and wastes resources.
To address these issues flash reflection proposed to alterna-
tive Reflection method for LLM-EPS. Flash reflection in-
cludes two steps as follows:
• At time step t, we perform grouping on all individuals
that are selected from the selection stage and remove all
duplication. After that, we use LLM to perform a deep
analysis on the ranking on parent pairs. This take inputs
as mixed ranking pairs (i.e., one good performance par-
ent and one worse performance), good ranking pairs (i.e.,
both good performance), and worse ranking pairs (i.e.,
both worse performance), then return a comprehensive
description on how to improve as a text string.
• From the current analysis at time step t, we use LLM to
compare with previous analysis at time step t−1 and out-
put a guide information, which can be used in subsequent
stages.
Crossover. In this stage, new offspring algorithms are
generated by combining elements of the parent algorithms.
The goal is to blend successful attributes from two parents
via guide result guide information part of flash reflections.
Through this step, HSEvo hopes to produce offspring that
inherit the strengths of both, which can potentially lead to
better-performing heuristics. The prompt includes task spec-
ifications, a pair of parent heuristics, guide information part
of flash reflection, and generation instructions.
Elitist mutation. HSEvo uses an elitist mutation strategy,
where the generator LLM is tasked with mutation an elite
individual—representing the best-performing heuristic—by
incorporating insights derived from LLM-analyzed flash re-
flections. Each mutation prompt includes detailed task spec-
ifications, the elite heuristic, deep analysis part of flash re-
flections, and specific generation instructions. This approach
leverages accumulated knowledge to enhance the heuristic,
ensuring continuous improvement in performance while pre-
serving the quality of the top solutions.
Harmony Search. From the analysis on Section 3.4 and
3.5, we hypothesize that if the population is too diverse,
each individuals inside will more likely to be not optimized,
which may cause harm to the optimization process. We em-
ploy the Harmony Search algorithm to alleviate this issue by
optimizing the parameters (e.g., thresholds, weights, etc.) of
best individuals in the population. The process is as follows:
• First, we use LLM to extract parameters from the best in-
dividual (i.e., code snippets, programs) of the population
and define a range for each parameters (Fig. 4).
• Following the work (Shi, Han, and Si 2012), we optimize
the above parameters with harmony search algorithm.
• After parameter optimization, we mark this individual
and add it back to the population. All marked individuals
will not be optimized again in the future time steps.
Fine-tuning these parameters makes the individual more op-
timized, therefore allowing us to encourage diversity during
the prompting while avoid the trade-off between objective
performance and diversity, as can be seen with EoH.
5
Experiments
5.1
Experimental Setup
Benchmarks: To assess the diversity and objective scores
of HSEvo in comparisons with previous LLM-EPS frame-
works, we adopt the same benchmarks as Section 3.4 and
conduct experiments on three different AHD problems, BPO
(Seiden 2002), TSP (Hoffman et al. 2013) and OP (Ye et al.
2024a).
• BPO: packing items into bins of fixed capacity in real-
time without prior knowledge of future items. In this
benchmark, LLM-EPS frameworks need to design deter-
ministic constructive heuristics to solve.
• TSP: find the shortest possible route that visits each city
exactly once and returns to the starting point. In this
setting, LLM-EPS frameworks need to design heuristics
to enhance the perturbation phase for the Guided Local
Search solver.
• OP: find the most efficient path through a set of locations,
maximizing the total score collected within a limited time
or distance. This setting requires LLM-EPS frameworks
to design herustics used by the ACO solver.


--- Page 7 ---
100K
200K
300K
400K
Total tokens
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Objective score
100K
200K
300K
400K
Total tokens
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Objective score
100K
200K
300K
400K
Total tokens
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Objective score
2.5
3.0
3.5
4.0
4.5
5.0
5.5
6.0
Diversity index
2.5
3.0
3.5
4.0
4.5
5.0
5.5
6.0
Diversity index
2.5
3.0
3.5
4.0
4.5
5.0
5.5
6.0
Diversity index
Objective score 
SWDI 
CDI 
Figure 5: Diversity indices and objective scores of HSEvo framework on BPO problem through different runs.
Experiment settings: All frameworks were executed un-
der identical environmental settings listed in Table 1. The
heuristic search processes across the LLM-EPS frameworks
and the evaluations of heuristics were conducted using a sin-
gle core of an Xeon Processors CPU.
5.2
Experiment results
Table 2 presents our experiment results on all three AHD
problems2. From the table, we observe that while our frame-
work still haven’t obtained better CDI than EoH, HSEvo is
able to achieve the best objective score on all tasks. On BPO,
HSEvo outperforms both FunSearch and ReEvo by a huge
margin. This highlights the important of our findings from
the analysis in Section 3.5, where it is crucial to improve
diversity in order to optimize the population better.
To investigate the impact of our framework on the diver-
sity of the population, we plot the diversity metrics and ob-
jective score of HSEvo through different runs on BPO prob-
lem in Fig. 5. We can draw a similar observation with find-
ings in Section 3.4, where with a high SWDI and CDI, the
objective score can be optimized significantly. One thing to
note here is that in HSEvo first run and ReEvo third run in
Fig. 1, both have the SWDI decreasing overtime. However,
in HSEvo, the SWDI is at around 3.0 when the objective
score improve significantly, then only decrease marginally
after that. In ReEvo, the objective score improves when
SWDI at around 3.0 and 2.7, and the magnitude of the im-
provement is not as large as HSEvo, which implies the im-
portant of diversity in the optimization of the problem and
also the impact of our proposed harmony search.
5.3
Ablation study
To gain a better understanding on our novel framework, we
conduct ablation studies on our proposed components, the
harmony search and flash reflection.
Harmony search analysis
As harmony search is a vital
component in our HSEvo framework, instead of removing it
from HSEvo, we conduct an experiment where we add har-
mony search into ReEvo framework. Table 3 presents our
2Extending FunSearch to solve the OP problem with an ACO
solver caused conflicts we could not resolve with reasonable effort.
experiment results on OP problem. From the results, we can
see that HSEvo outperforms both ReEvo and ReEvo with
harmony search variant on both objective score and CDI.
Here, notice that harmony search can only marginally im-
prove ReEvo. This can be explained that ReEvo does not
have any mechanism to promote diversity, therefore it is not
benefitted from the advantage of harmony search process.
Flash reflection analysis
We also conduct another experi-
ment where we replace the reflection used in ReEvo with our
flash reflection component. As our flash reflection is more
cost efficient than original reflection, we reduce the number
of tokens used for optimization to 150K. Table 4 presents
our experiment results on OP problem. The results show that
given a smaller number of timestep, ReEvo with flash reflec-
tion mechanism can outperform HSEvo in optimization per-
formance while obtain comparable results on CDI. However,
when running with a larger number of tokens, ReEvo with
flash reflection cannot improve on both objective score and
CDI, while HSEvo improve both metrics to 5.67 and -14.62,
respectively. This implies that without diversity-promoting
mechanism, flash reflection is not enough to improve the op-
timization process of LLM-EPS.
6
Conclusion
In this paper, we highlight the importance of population di-
versity in LLM-EPS for AHD problems. We propose two
diversity measure metrics, SWDI and CDI, and conduct an
analysis on the diversity of previous LLM-EPS approaches.
We find that previous approaches either lack focus on the di-
versity of the population or suffered heavily from the diver-
sity and optimization performance trade-off. We also intro-
duce HSEvo, a novel LLM-EPS framework with diversity-
driver harmony search and genetic algorithm for AHD prob-
lems. Our experiment results show that our framework can
maintain a good balance between diversity and optimization
performance trade-off. We also perform additional ablation
studies to verify the effectiveness of components of our pro-
posed framework. We hope our work can benefit future re-
search in LLM-EPS community.


--- Page 8 ---
Acknowledgments
This research was funded by Hanoi University of Science
and Technology under project code T2024-PC-038.
References
Arnold, F.; and S¨orensen, K. 2019. Knowledge-guided lo-
cal search for the vehicle routing problem. Computers &
Operations Research, 105: 32–46.
Austin, J.; Odena, A.; Nye, M.; Bosma, M.; Michalewski,
H.; Dohan, D.; Jiang, E.; Cai, C.; Terry, M.; Le, Q.; et al.
2021. Program synthesis with large language models. arXiv
preprint arXiv:2108.07732.
Chen, A.; Dohan, D.; and So, D. 2024. EvoPrompting: lan-
guage models for code-level neural architecture search. Ad-
vances in Neural Information Processing Systems, 36.
Choong, S. S.; Wong, L.-P.; and Lim, C. P. 2018. Automatic
design of hyper-heuristic based on reinforcement learning.
Information Sciences, 436: 89–107.
Drakulic, D.; Michel, S.; Mai, F.; Sors, A.; and Andreoli,
J.-M. 2024. Bq-nco: Bisimulation quotienting for efficient
neural combinatorial optimization. Advances in Neural In-
formation Processing Systems, 36.
Guo, Q.; Wang, R.; Guo, J.; Li, B.; Song, K.; Tan, X.; Liu,
G.; Bian, J.; and Yang, Y. 2023.
Connecting large lan-
guage models with evolutionary algorithms yields powerful
prompt optimizers. arXiv preprint arXiv:2309.08532.
Heip, C. H.; Herman, P. M.; Soetaert, K.; et al. 1998. Indices
of diversity and evenness. Oceanis, 24(4): 61–88.
Hemberg, E.; Moskal, S.; and O’Reilly, U.-M. 2024. Evolv-
ing code with a large language model. Genetic Program-
ming and Evolvable Machines, 25(2): 21.
Hoffman, K. L.; Padberg, M.; Rinaldi, G.; et al. 2013. Trav-
eling salesman problem.
Encyclopedia of operations re-
search and management science, 1: 1573–1578.
Jost, L. 2006. Entropy and diversity. Oikos, 113(2): 363–
375.
Kool, W.; Van Hoof, H.; and Welling, M. 2018.
At-
tention, learn to solve routing problems!
arXiv preprint
arXiv:1803.08475.
Liu, F.; Xialiang, T.; Yuan, M.; Lin, X.; Luo, F.; Wang, Z.;
Lu, Z.; and Zhang, Q. 2024a. Evolution of Heuristics: To-
wards Efficient Automatic Algorithm Design Using Large
Language Model. In Forty-first International Conference on
Machine Learning.
Liu, S.; Chen, C.; Qu, X.; Tang, K.; and Ong, Y.-S. 2024b.
Large language models as evolutionary optimizers. In 2024
IEEE Congress on Evolutionary Computation (CEC), 1–8.
IEEE.
Liu, S.; Zhang, Y.; Tang, K.; and Yao, X. 2023. How good is
neural combinatorial optimization? A systematic evaluation
on the traveling salesman problem.
IEEE Computational
Intelligence Magazine, 18(3): 14–28.
Ma, Y. J.; Liang, W.; Wang, G.; Huang, D.-A.; Bastani, O.;
Jayaraman, D.; Zhu, Y.; Fan, L.; and Anandkumar, A. 2023.
Eureka: Human-level reward design via coding large lan-
guage models. arXiv preprint arXiv:2310.12931.
Mahowald, K.; Ivanova, A. A.; Blank, I. A.; Kanwisher, N.;
Tenenbaum, J. B.; and Fedorenko, E. 2024.
Dissociating
language and thought in large language models. Trends in
Cognitive Sciences.
Martello, S.; and Toth, P. 1990. Lower bounds and reduction
procedures for the bin packing problem. Discrete applied
mathematics, 28(1): 59–70.
Meyerson, E.; Nelson, M. J.; Bradley, H.; Gaier, A.; Moradi,
A.; Hoover, A. K.; and Lehman, J. 2024. Language model
crossover: Variation through few-shot prompting.
ACM
Transactions on Evolutionary Learning, 4(4): 1–40.
Nejjar, M.; Zacharias, L.; Stiehle, F.; and Weber, I. 2023.
Llms for science: Usage for code generation and data analy-
sis. Journal of Software: Evolution and Process, e2723.
Nolan, K.; and Callahan, J. 2006. Beachcomber Biology:
The Shannon-Weiner Species Diversity Index. Proc. Work-
shop ABLE, 27.
Pillay, N.; and Qu, R. 2018. Hyper-heuristics: theory and
applications. Springer.
Pires, E.; Tenreiro Machado, J.; and Moura Oliveira, P. 2019.
Dynamic Shannon Performance in a Multiobjective Particle
Swarm Optimization. Entropy, 21: 827.
Qu, R.; Kendall, G.; and Pillay, N. 2020. The general com-
binatorial optimization problem: Towards automated algo-
rithm design. IEEE Computational Intelligence Magazine,
15(2): 14–23.
Romera-Paredes, B.; Barekatain, M.; Novikov, A.; Balog,
M.; Kumar, M. P.; Dupont, E.; Ruiz, F. J.; Ellenberg, J. S.;
Wang, P.; Fawzi, O.; et al. 2024. Mathematical discoveries
from program search with large language models. Nature,
625(7995): 468–475.
Seiden, S. S. 2002. On the online bin packing problem. Jour-
nal of the ACM (JACM), 49(5): 640–671.
Shanahan, M.; McDonell, K.; and Reynolds, L. 2023. Role
play with large language models. Nature, 623(7987): 493–
498.
Shi, W. W.; Han, W.; and Si, W. C. 2012. A Hybrid Ge-
netic Algorithm Based on Harmony Search and its Improv-
ing, 101–109. Springer London. ISBN 9781447148029.
Shinn, N.; Cassano, F.; Gopinath, A.; Narasimhan, K.; and
Yao, S. 2024. Reflexion: Language agents with verbal re-
inforcement learning. Advances in Neural Information Pro-
cessing Systems, 36.
Solteiro Pires, E. J.; Tenreiro Machado, J. A.; and
de Moura Oliveira, P. B. 2014. Diversity study of multi-
objective genetic algorithm based on Shannon entropy. In
2014 Sixth World Congress on Nature and Biologically In-
spired Computing (NaBIC 2014), 17–22.
T¨u˝u-Szab´o, B.; F¨oldesi, P.; and K´oczy, L. T. 2022. Analyz-
ing the performance of TSP solver methods. In Computa-
tional Intelligence and Mathematics for Tackling Complex
Problems 2, 65–71. Springer.
Voudouris, C.; and Tsang, E. 1999. Guided local search and
its application to the traveling salesman problem. European
journal of operational research, 113(2): 469–499.


--- Page 9 ---
Wang, L.; and Chen, Y. 2012. Diversity Based on Entropy: A
Novel Evaluation Criterion in Multi-objective Optimization
Algorithm. International Journal of Intelligent Systems and
Applications, 4(10): 113–124.
Wang, Y.; Le, H.; Gotmare, A. D.; Bui, N. D.; Li, J.; and
Hoi, S. C. 2023. Codet5+: Open code large language mod-
els for code understanding and generation. arXiv preprint
arXiv:2305.07922.
Ye, H.; Wang, J.; Cao, Z.; Liang, H.; and Li, Y. 2024a. Deep-
ACO: neural-enhanced ant systems for combinatorial opti-
mization. Advances in Neural Information Processing Sys-
tems, 36.
Ye, H.; Wang, J.; Cao, Z.; and Song, G. 2024b. Reevo: Large
language models as hyper-heuristics with reflective evolu-
tion. arXiv preprint arXiv:2402.01145.
Yuksekgonul, M.; Bianchi, F.; Boen, J.; Liu, S.; Huang, Z.;
Guestrin, C.; and Zou, J. 2024. TextGrad: Automatic” Dif-
ferentiation” via Text. arXiv preprint arXiv:2406.07496.


--- Page 10 ---
A
Benchmark dataset details
To demonstrate the effectiveness of HSEvo in particular and
LLM-EPS in general, we conduct experiments on three dif-
ferent optimization problems, BPO, TSP, and OP. The data
configurations and generation setups used in this paper are
regarded as challenging and have been used in recent related
studies (Romera-Paredes et al. 2024), (Liu et al. 2024a), (Ye
et al. 2024b).
A.1
Bin Packing Online (BPO)
Definition. The objective of this problem is to assign a col-
lection of items of varying sizes into the minimum number
of containers with a fixed capacity of C. Our focus is on
the online scenario, where items are packed as they arrive,
rather than the offline scenario where all items are known in
advance.
Dataset generation. We randomly generate five Weibull
instances of size 5k with a capacity of C = 100 (Romera-
Paredes et al. 2024). The objective score is set as the average
lb
n of five instances, where lb represents the lower bound of
the optimal number of bins computed (Martello and Toth
1990) and n is the number of bins used to pack all the items
by the evaluated heuristic.
Solver. LLM-EPS is used to design heuristic functions
that are able to solve this problem directly without any ex-
ternal solver.
A.2
Traveling Salesman Problem (TSP)
Definition. The Traveling Salesman Problem (TSP) is a
classic optimization challenge that seeks the shortest pos-
sible route for a salesman to visit each city in a list exactly
once and return to the origin city.
Dataset generation. We generate a set of 64 TSP in-
stances with 100 nodes (TSP100). The node locations
in these instances are randomly sampled from [0, 1]2
(Voudouris and Tsang 1999). This means that the nodes po-
sitioned within a square area bounded by (0, 0) and (1, 1).
The average gap from the optimal solution, generated by
Concorde (T¨u˝u-Szab´o, F¨oldesi, and K´oczy 2022), is used
as the objective score.
Solver. We use Guided Local Search (GLS) as solver
for this benchmark. GLS explores the solution space using
local search operations guided by heuristics. The idea be-
hind using this solver is to explore the potential of search
penalty heuristics for LLM-EPS. In our experiment, we
modified the traditional GLS algorithm by including per-
turbation phases (Arnold and S¨orensen 2019), where edges
with higher heuristic values are given priority for penaliza-
tion. Settings parameters are listed in Table 5, and the num-
ber of perturbation moves is 1.
A.3
Orienteering Problem (OP)
Definition. In the Orienteering Problem (OP), the objective
is to maximize the total score obtained by visiting nodes un-
der a maximum tour length constraint.
Dataset generation. We follow the process of DeepACO
(Ye et al. 2024a) during the generation of this synthetic
dataset. In each problem instance, we uniformly sample 50
Problem
Problem size
# of iterations
Other
BPO
5000
-
C=100
TSP
100
1000
-
OP
50
50
-
Table 5: Problems parameters used for heuristic evaluations
nodes (OP50), including the depot node, from the unit in-
terval [0, 1]2. This means that the nodes positioned within a
square area bounded by (0, 0) and (1, 1). We also adopt a
challenging prize distribution (Kool, Van Hoof, and Welling
2018):
pi =

1 +
99 ·
d0i
maxn
j=1 d0j
100

,
where d0i represents the distance between the depot and
node i, and the maximum tour length constraint is 3.
Solver. We use Ant Colony Optimization (ACO) as solver
for this benchmark. ACO is an evolutionary algorithm that
integrates solution sampling with pheromone trail updates.
It employs stochastic solution sampling, which is biased to-
wards more promising solution spaces based on heuristics.
We choose the population size of 20 for ACO to solve OP50.
B
Diversity analysis
B.1
Objective scores and diversity measurement
metrics
Analysis setting. The diversity evaluation process was con-
ducted with code embedding model used is CodeT5+3
(Wang et al. 2023), and similarity threshold α = 0.95 where
α ∈[0; 1]. The maximum budget for each run is 450K to-
kens.
Additional results. The results of the diversity analysis of
ReEvo on TSP and OP are shown in the Figure 6 and Figure
7, respectively.
B.2
Diversity analysis on previous LLM-EPS
frameworks
We conduct experiments with three LLM-EPS frameworks,
FunSearch, EoH and ReEvo, on all benchmark datasets,
BPO, TSP, and OP. The parameters and LLM information
for the corresponding frameworks are presented in Table 1.
We follow the same diversity evaluation process as Section
B.1.
C
HSEvo prompt examples
In this section, we synthesize the prompts used in the HSEvo
framework. Our prompt system includes four stages initial-
ization population, flash reflection, crossover, and elitist mu-
tation.
3https://huggingface.co/Salesforce/codet5p-110m-embedding


--- Page 11 ---
100K
200K
300K
400K
Total tokens
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Objective score
100K
200K
300K
400K
Total tokens
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Objective score
100K
200K
300K
400K
Total tokens
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Objective score
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
5.5
Diversity index
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
5.5
Diversity index
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
5.5
Diversity index
Objective score 
SWDI 
CDI 
Figure 6: Diversity indices and objective scores of ReEvo framework on TSP through different runs.
100K
200K
300K
400K
Total tokens
14.5
14.0
13.5
13.0
12.5
Objective score
100K
200K
300K
400K
Total tokens
14.5
14.0
13.5
13.0
12.5
Objective score
100K
200K
300K
400K
Total tokens
14.5
14.0
13.5
13.0
12.5
Objective score
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
Diversity index
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
Diversity index
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
Diversity index
Objective score 
SWDI 
CDI 
Figure 7: Diversity indices and objective scores of ReEvo framework on OP through different runs.
C.1
Task description prompts
Prompt 1: System prompt for generator LLM.
{role init} helping to design heuristics that can ef-
fectively solve optimization problems.
Your response outputs Python code and nothing
else. Format your code as a Python code string:
‘‘‘python ... ‘‘‘.
Prompt 2: Task description.
{role init} Your task is to write a {function name}
function for {problem description}
{function description}
Here, we assign different roles to the LLM via the vari-
able role init to create different personas (Shanahan,
McDonell, and Reynolds 2023), which can give LLM differ-
ent viewpoints and thus encourage the diversity of the pop-
ulation. The roles are selected in a round-robin order from
the list:
• You are an expert in the domain of optimization heuris-
tics,
• You are Albert Einstein, relativity theory developer,
• You are Isaac Newton, the father of physics,
• You are Marie Curie, pioneer in radioactivity,
• You are Nikola Tesla, master of electricity,
• You are Galileo Galilei, champion of heliocentrism,
• You are Stephen Hawking, black hole theorist,
• You are Richard Feynman, quantum mechanics genius,
• You are Rosalind Franklin, DNA structure revealer,
• You are Ada Lovelace, computer programming pioneer.
Variable function name, problem description
and function description correspond to the heuris-
tic function name, the specific problem description, and the
function description, respectively, which will be detailed in
the section C.7.


--- Page 12 ---
C.2
Population initialization prompt
Prompt 3: User prompt for population initialization.
{task description}
{seed function}
Refer to the format of a trivial design above. Be
very creative and give ‘{function name} v2‘. Out-
put code only and enclose your code with Python
code block: ‘‘‘python ... ‘‘‘.
Here, task description refer to Prompt 2, seed
function is a trivial heuristic function for the correspond-
ing problem.
C.3
Flash reflection prompts
Prompt 4: System prompt for reflector LLM.
You are an expert in the domain of optimization
heuristics. Your task is to provide useful advice
based on analysis to design better heuristics.
Prompt 5: User prompt for flash reflection pharse 1.
### List heuristics
Below is a list of design heuristics ranked from best
to worst.
{list ranked heuristics}
### Guide
- Keep in mind, list of design heuristics ranked from
best to worst. Meaning the first function in the list is
the best and the last function in the list is the worst.
- The response in Markdown style and nothing else
has the following structure:
‘**Analysis:**
**Experience:**’
In there:
+ Meticulously analyze comments, docstrings and
source code of several pairs (Better code - Worse
code) in List heuristics to fill values for **Analy-
sis:**.
Example: “Comparing (best) vs (worst), we see ...;
(second best) vs (second worst) ...; Comparing (1st)
vs (2nd), we see ...; (3rd) vs (4th) ...; Comparing
(worst) vs (second worst), we see ...; Overall:...”
+ Self-reflect to extract useful experience for design
better heuristics and fill to **Experience:** (< 60
words).
I’m going to tip $999K for a better heuristics! Let’s
think step by step.
list ranked heuristics is a list of heuristic func-
tions obtained by grouping parent pairs from selection,
removing duplicates, and ranking based on the objective
scores.
Prompt 6: User prompt for flash reflection pharse 2.
Your task is to redefine ‘Current self-reflection’ pay-
ing attention to avoid all things in ‘Ineffective self-
reflection’ in order to come up with ideas to design
better heuristics.
### Current self-reflection
{current reflection}
{good reflection}
### Ineffective self-reflection
{bad reflection}
Response (<100 words) should have 4 bullet points:
Keywords, Advice, Avoid, Explanation.
I’m going to tip $999K for a better heuristics! Let’s
think step by step.
current reflection
is
the
output
of
the
flash
reflection
step
1
from
the
current
generation.
good reflection is the output of the flash reflection
step 2 from previous generations where a new heuristic was
discovered. Conversely, bad reflection is the output
of the flash reflection step 2 from previous generations
where no new heuristic was discovered.
C.4
Crossover prompt
Prompt 7: User prompt for crossover.
{task description}
### Better code
{function signature better}
{code better}
### Worse code
{function signature worse}
{code worse}
### Analyze & experience
- {flash refection}
Your
task
is
to
write
an
improved
function
‘{func name} v2‘ by COMBINING elements of
two above heuristics base Analyze & experience.
Output the code within a Python code block:
“‘python ... “‘, has comment and docstring (<50
words) to description key idea of heuristics design.
I’m going to tip $999K for a better heuristics! Let’s
think step by step.
function signature better,
code better
and function signature worse, code worse are
the function signatures and code of the better and worse
parent individuals, respectively. flash refection is
the output of the flash reflection step 2 from the current
generation.


--- Page 13 ---
C.5
Elitist mutation prompt
Prompt 8: System prompt for elitist mutation.
{task description}
Current heuristics:
{function signature elitist}
{elitist code}
Now, think outside the box write a mutated function
‘{func name} v2‘ better than current version. You
can using some hints if need:
{flash reflection}
Output code only and enclose your code with Python
code block: ‘‘‘python ... ‘‘‘.
I’m going to tip $999K for a better solution!
function signature elitist, elitist code
are the function signatures and code of elitist individuals of
current generation.
C.6
Harmony Search prompts
Prompt 9: System prompt for Harmony Search.
You are an expert in code review. Your task ex-
tract all threshold, weight or hardcode variable of the
function make it become default parameters.
Prompt 10: User prompt for Harmony Search.
{elitist code}
Now extract all threshold, weight or hardcode vari-
able of the function make it become default param-
eters and give me a ’parameter ranges’ dictionary
representation. Key of dict is name of variable. Value
of key is a tuple in Python MUST include 2 float el-
ements, first element is begin value, second element
is end value corresponding with parameter.
- Output code only and enclose your code with
Python code block: ‘‘‘python ... ‘‘‘.
- Output ’parameter ranges’ dictionary only and
enclose your code with other Python code block:
‘‘‘python ... ‘‘‘.
elitist code is the snippet/string code of the elitist
individual in the current population that has not yet under-
gone harmony search.
C.7
Problem-specific prompts
Problem-specific prompts are given below:
• Table 6 presents problem description for all prob-
lems.
• Table 7 lists the function description for all
problem settings.
• Figure 8 shows the function signature, which
also includes the function name of each problem.
• Figure 9 shows the seed function used for each
problem.
Problem
Problem description
BPO
Solving
online
Bin
Packing
Problem
(BPP). BPP requires packing a set of items
of various sizes into the smallest number
of fixed-sized bins. Online BPP requires
packing an item as soon as it is received.
TSP
Solving
Traveling
Salesman
Problem
(TSP) via guided local search. TSP re-
quires finding the shortest path that visits
all given nodes and returns to the starting
node.
OP
Solving a black-box graph combinatorial
optimization problem via stochastic solu-
tion sampling following “heuristics”.
Table 6: Problem descriptions used in prompts
Problem
Function description
BPO
The priority function takes as input an item
and an array of bins remain cap (contain-
ing the remaining capacity of each bin) and
returns a priority score for each bin. The
bin with the highest priority score will be
selected for the item.
TSP
The ‘update edge distance’ function takes
as input a matrix of edge distances, a lo-
cally optimized tour, and a matrix indicat-
ing the number of times each edge has been
used. It returns an updated matrix of edge
distances that incorporates the effects of
the local optimization and edge usage. The
returned matrix has the same shape as the
input ‘edge distance’ matrix, with the dis-
tances adjusted based on the provided tour
and usage data.
OP
The ‘heuristics’ function takes as input a
vector of node attributes (shape: n), a ma-
trix of edge attributes (shape: n by n), and
a constraint imposed on the sum of edge
attributes. A special node is indexed by 0.
‘heuristics’ returns prior indicators of how
promising it is to include each edge in a
solution. The return is of the same shape as
the input matrix of edge attributes.
Table 7: Function descriptions used in prompts


--- Page 14 ---
1
# BPO
2
def priority_v{version}(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
3
4
# TSP
5
def update_edge_distance_v{version}(edge_distance: np.ndarray, local_opt_tour: np.ndarray, edge_n_used: np.ndarray)
-> np.ndarray:
6
7
# OP
8
def heuristics_v{version}(node_attr: np.ndarray, edge_attr: np.ndarray, node_constraint: float)->np.ndarray:
Figure 8: Function signatures used in HSEvo.
D
Generated heuristics
The best heuristics generated by HSEvo for all problem set-
tings are presented in Figure 10, 11, and 12.


--- Page 15 ---
1
# BPO
2
def priority_v1(item: float, bins_remain_cap: np.ndarray) -> np.ndarray:
3
"""Returns priority with which we want to add item to each bin.
4
5
Args:
6
item: Size of item to be added to the bin.
7
bins_remain_cap: Array of capacities for each bin.
8
9
Return:
10
Array of same size as bins_remain_cap with priority score of each bin.
11
"""
12
ratios = item / bins_remain_cap
13
log_ratios = np.log(ratios)
14
priorities = -log_ratios
15
return priorities
16
17
# TSP
18
def update_edge_distance(edge_distance: np.ndarray, local_opt_tour: np.ndarray, edge_n_used: np.ndarray) -> np.
ndarray:
19
"""
20
Args:
21
edge_distance (np.ndarray): Original edge distance matrix.
22
local_opt_tour (np.ndarray): Local optimal solution path.
23
edge_n_used (np.ndarray): Matrix representing the number of times each edge is used.
24
Return:
25
updated_edge_distance: updated score of each edge distance matrix.
26
"""
27
28
num_nodes = edge_distance.shape[0]
29
updated_edge_distance = np.copy(edge_distance)
30
31
for i in range(num_nodes - 1):
32
current_node = local_opt_tour[i]
33
next_node = local_opt_tour[i + 1]
34
updated_edge_distance[current_node, next_node] *= (1 + edge_n_used[current_node, next_node])
35
36
updated_edge_distance[local_opt_tour[-1], local_opt_tour[0]] *= (1 + edge_n_used[local_opt_tour[-1],
local_opt_tour[0]])
37
return updated_edge_distance
38
39
# OP
40
def heuristics_v1(node_attr: np.ndarray, edge_attr: np.ndarray, edge_constraint: float)->np.ndarray:
41
return np.ones_like(edge_attr)
Figure 9: Seed heuristics used in HSEvo.


--- Page 16 ---
1
import numpy as np
2
import random
3
import math
4
import scipy
5
import torch
6
def priority_v2(item: float, bins_remain_cap: np.ndarray, overflow_threshold: float = 1.0987600915713542,
mild_penalty: float = 0.5567025232550017, adaptability_lower: float = 0.7264590977149653, adaptability_higher:
float = 1.9441643982922379) -> np.ndarray:
7
"""Enhanced dynamic scoring function for optimal bin selection in online BPP with a more holistic approach."""
8
9
# Avoid division by zero by adjusting remaining capacities
10
adjusted_bins_remain_cap = np.maximum(bins_remain_cap, np.finfo(float).eps)
11
12
# Calculate effective capacities
13
effective_cap = np.clip(bins_remain_cap - item, 0, None)
14
valid_bins = effective_cap >= 0
15
16
# Calculate occupancy ratios with controlled overflow representation
17
occupancy_ratio = item / adjusted_bins_remain_cap
18
occupancy_scores = np.where(valid_bins, occupancy_ratio, 0)
19
20
# Enhanced overflow penalty: stronger influence for near-overflows
21
overflow_penalty = np.where(occupancy_ratio > overflow_threshold, mild_penalty * (occupancy_ratio -
overflow_threshold + 1), 1.0)
22
23
# Logarithmic penalty for remaining capacity to encourage SPACE utilization
24
log_penalty = np.where(bins_remain_cap > 0, np.log1p(adjusted_bins_remain_cap / (adjusted_bins_remain_cap - item)
), 0)
25
26
# Adaptability based on remaining mean capacity
27
remaining_mean = np.mean(bins_remain_cap[bins_remain_cap > 0])
28
adaptability_factor = np.where(bins_remain_cap < remaining_mean, adaptability_lower, adaptability_higher)
29
30
# Comprehensive scoring integrating all metrics for a robust approach
31
scores = np.where(valid_bins, occupancy_scores * overflow_penalty * log_penalty * adaptability_factor, -np.inf)
32
33
# Normalize scores for prioritization
34
max_score = np.max(scores)
35
prioritized_scores = (scores - np.min(scores)) / (max_score - np.min(scores)) if max_score > np.min(scores) else
scores
36
37
# Invert scores for selecting the highest priority bin
38
inverted_priorities = 1 - prioritized_scores
39
40
return inverted_priorities
Figure 10: The best heuristic for BPO found by HSEvo.


--- Page 17 ---
1
import numpy as np
2
3
def update_edge_distance_v2(edge_distance: np.ndarray, local_opt_tour: np.ndarray, edge_n_used: np.ndarray,
4
penalty_factor: float = 0.6713404008357979, bonus_factor: float = 1.343302294236627,
decay_factor: float = 0.3821795974433295,
5
scaling_factor: float = 1.116349420562543, min_distance: float = 7.965736386169868e-05,
6
penalty_threshold: float = 29.850922399224466, boost_threshold: float =
70.53785604399908) -> np.ndarray:
7
"""
8
Update edge distances using adaptive penalties and bonuses, considering edge usage dynamics
9
while ensuring nuanced performance in line with real-time data patterns.
10
"""
11
num_nodes = edge_distance.shape[0]
12
updated_edge_distance = np.copy(edge_distance)
13
14
# Calculate average usage for dynamic adjustments
15
avg_usage = np.mean(edge_n_used)
16
17
for i in range(num_nodes):
18
current_node = local_opt_tour[i]
19
next_node = local_opt_tour[(i + 1) % num_nodes]
20
usage_count = edge_n_used[current_node, next_node]
21
22
# Adaptive penalty for overused edges
23
if usage_count > penalty_threshold:
24
# Penalty increases exponentially with usage
25
penalty = penalty_factor * (usage_count - penalty_threshold) ** 2
26
updated_edge_distance[current_node, next_node] += penalty
27
updated_edge_distance[next_node, current_node] += penalty
# Ensure symmetry
28
29
elif usage_count < boost_threshold:
30
# Apply a bonus to underused edges
31
boost = bonus_factor * (1 + (boost_threshold - usage_count) * 0.1)
32
updated_edge_distance[current_node, next_node] *= boost
33
updated_edge_distance[next_node, current_node] *= boost
# Ensure symmetry
34
35
# Dynamic scaling based on average usage
36
if usage_count > avg_usage:
37
adjustment_factor = scaling_factor / (1 + decay_factor ** (usage_count - avg_usage))
38
else:
39
adjustment_factor = scaling_factor * (1 + decay_factor ** (avg_usage - usage_count))
40
41
# Update the distance with adjustment and ensure non-negative distances
42
updated_edge_distance[current_node, next_node] = max(min_distance,
43
updated_edge_distance[current_node, next_node] * adjustment_factor)
44
45
return updated_edge_distance
Figure 11: The best heuristic for TSP found by HSEvo.


--- Page 18 ---
1
import numpy as np
2
3
def heuristics_v2(node_attr: np.ndarray, edge_attr: np.ndarray, node_constraint: float) -> np.ndarray:
4
"""
5
Enhanced heuristics incorporating contextual adjustments, multi-dimensional scoring,
6
and adaptive responsiveness to edge conditions.
7
"""
8
normalization_epsilon = 1e-8
9
influence_threshold = 0.9
10
adaptability_factor = 2.0
11
non_linearity_base = 2.5
12
n = node_attr.shape[0]
13
score_matrix = np.zeros_like(edge_attr)
14
15
# Normalize node attributes (maintain zero division safety)
16
total_node_attr = np.sum(node_attr) + normalization_epsilon
17
normalized_node_attr = node_attr / total_node_attr
18
19
for i in range(n):
20
for j in range(n):
21
if i == j:
22
continue
# Skip self-loops
23
24
# Calculate contextual adjustment for edge attributes
25
dynamic_edge = edge_attr[i, j] ** adaptability_factor
26
adjusted_edge = dynamic_edge / (node_constraint + normalization_epsilon)
27
28
# Multi-dimensional scaling based on edge and node attributes
29
if adjusted_edge > influence_threshold:
30
scaling_factor = np.sqrt(adjusted_edge + normalization_epsilon)
31
else:
32
scaling_factor = influence_threshold / (adjusted_edge + normalization_epsilon)
33
34
# Layered logic for combined node influence using non-linear model
35
combined_node_influence = (
36
(normalized_node_attr[i] ** non_linearity_base) +
37
(normalized_node_attr[j] ** non_linearity_base)
38
) ** 1.5
# Further amplify the influence
39
40
# Score calculation with contextual penalties (non-linearity)
41
score = combined_node_influence * scaling_factor
42
43
if dynamic_edge > 0:
44
penalty_base = np.log1p(dynamic_edge)
# Non-linear penalty
45
penalty_factor = 1 / (1 + penalty_base ** 2)
# Stronger sensitivity with edges
46
score *= penalty_factor
47
48
# Normalization to retain meaningful scale
49
score_matrix[i, j] = score / (dynamic_edge + normalization_epsilon)
50
51
return score_matrix
Figure 12: The best heuristic for OP found by HSEvo.
