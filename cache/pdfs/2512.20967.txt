--- Page 1 ---
Deadline-Aware Online Scheduling for LLM
Fine-Tuning with Spot Market Predictions
Linggao Kong∗, Yuedong Xu∗, Lei Jiao†, Chuan Xu‡
∗Fudan University, China
†University of Oregon, USA
‡Inria, France
Abstract—As foundation models grow in size, fine-tuning them
becomes increasingly expensive. While GPU spot instances offer a
low-cost alternative to on-demand resources, their volatile prices
and availability make deadline-aware scheduling particularly
challenging. We tackle this difficulty by using a mix of spot and
on-demand instances. Distinctively, we show the predictability of
prices and availability in a spot instance market, the power of
prediction in enabling cost-efficient scheduling and its sensitivity
to estimation errors. An integer programming problem is formu-
lated to capture the use of mixed instances under both the price
and availability dynamics. We propose an online allocation algo-
rithm with prediction based on the committed horizon control
approach that leverages a commitment level to enforce the partial
sequence of decisions. When this prediction becomes inaccurate,
we further present a complementary online algorithm without
predictions. An online policy selection algorithm is developed
that learns the best policy from a pool constructed by varying
the parameters of both algorithms. We prove that the prediction-
based algorithm achieves tighter performance bounds as predic-
tion error decreases, while the policy selection algorithm possesses
a regret bound of O(
√
T). Experimental results demonstrate that
our online framework can adaptively select the best policy under
varying spot market dynamics and prediction quality, consistently
outperforming baselines and improving utility by up to 54.8%.
I. INTRODUCTION
Pretrained large language models (LLMs) such as GPT [1]
and Llama [2] have deeply penetrated into our daily lives.
Although these models demonstrate broad linguistic under-
standing, factual knowledge, and reasoning, they often lack
the precision and task-specific performance required in real-
world applications, making fine-tuning with domain-specific
data essential to adapt general-purpose LLMs. A variety of
fine-tuning methods have been developed, including full fine-
tuning, adapter-based fine-tuning, prompt tuning, instruction
tuning, and reinforcement learning from human feedback
(RLHF) [3]–[7]. Among these, LoRA (Low-Rank Adapta-
tion) [8] is notable for its efficiency and scalability. It updates
a small number of trainable parameters while keeping the base
model frozen, significantly reducing memory and computation
costs, which makes it particularly well-suited for rapid adap-
tation, cost-effective deployment, and efficient operation even
in resource-constrained environments.
Finetuning LLMs with LoRA typically requires high-end
cloud GPUs, incurring substantial costs. Recently, spot GPU
instances have emerged as a cost-effective alternative, offering
Yuedong Xu is with College of Computer Science and Artificial Intelli-
gence, and Artificial Intelligence Innovation and Incubation Institute, Fudan
University, Shanghai, China (e-mail: ydxu@fudan.edu.cn)
up to 91% discounts [9]. Third-party brokers further aggregate
GPUs from decentralized providers into federated clouds [10]–
[12], where the available GPUs are de facto spot instances.
However, spot instances exhibit high price volatility and
intermittent availability due to preemption and provider churn,
introducing a trade-off between cost savings and meeting
service-level objectives (SLOs).
CPU spot instance usage in cloud computing has been
widely studied over the past decade. For instance, Menache
et al. [13] proposed an online algorithm to determine both
the type (on-demand vs. spot) and quantity of instances to
use at any given time. Song et al. [14] developed a dynamic
bidding strategy from the perspective of a cloud broker, aiming
to maximize service profit by bidding for spot instances. In
contrast, the use of GPU spot instances in deep learning
systems has only recently gained attention. Yang et al. [15]
formulated an integer programming problem to assign CPU or
GPU spot instances to multi-tenant jobs, which they solved us-
ing a polynomial-time algorithm based on linear programming
rounding. Wu et al. [16] studied the strategy for switching
between spot and on-demand GPU instances to minimize
costs, considering the inherent variability of spot availability.
In this paper, we aim to minimize the cost of LoRA-based
LLM fine-tuning with deadline awareness in a hybrid market
consisting of both on-demand and spot GPU instances. Our
problem is characterized by three key features. First, we ex-
plore the impact of dynamically adjusting the number of GPU
instances on model convergence—an aspect that has not been
previously validated. Second, unlike prior work that assumes
the unpredictability of spot instance prices and availability, we
leverage their potential predictability to design more effective
scheduling strategies instead. Third, we significantly expand
the state space by incorporating dynamic spot instance prices
and GPU availability, in contrast to prior work that assumes
fixed price and binary availability indicators for spot instances.
The uncertainty in prediction quality and the enlarged
decision space from fluctuating spot prices and availabil-
ity complicate resource allocation and scheduling. We first
formulate scheduling as a mixed-integer optimization prob-
lem. To maximizng the objective under volatile spot market
conditions, we propose a prediction-based allocation algo-
rithm building on Committed Horizon Control algorithm and
a fallback heuristic leveraging real-time job progress and
market trends. Subsequently, we develop an online policy
selection algorithm that adaptively identifies the best policy
arXiv:2512.20967v1  [cs.DC]  24 Dec 2025


--- Page 2 ---
from a pool constructed by systematically varying the hy-
perparameters of these two algorithms. We rigorously prove
that smaller prediction errors yield a tighter upper bound
on the performance gap of our prediction-based allocation
algorithm relative to the optimal policy. Furthermore, we show
that the online policy selection achieves a regret bound of
O(
√
T). Extensive experiments demonstrate that our approach
consistently outperforms baselines—On-Demand Only Policy
(OD-Only), Maximal Spot Utilization Policy (MSU), and
Uniform Progress Policy (UP) [16]—and adapts effectively
across diverse prediction environments.
This work makes the following contributions:
1) Deadline-Aware Fine-Tuning Problem: We formulate a
deadline-aware resource allocation problem for fine-tuning
jobs on cloud platforms using mixed on-demand and spot
instances to minimize cost.
2) Hybrid Resource Allocation with Online Policy Selec-
tion: We design a prediction-based allocation algorithm with
performance guarantees that improve with prediction accuracy,
along with a non-predictive variant for robustness. To handle
dynamics, we propose an online policy selector that adaptively
identifies the best strategy from a hyperparameterized pool,
with O(
√
T) regret.
3) Experimental Evaluation: We evaluate our approach un-
der different prediction errors and dynamic conditions. Results
show that our algorithms adapt effectively and consistently
outperforms baselines. In typical configurations, the best-
selected policy by our adaptive algorithm improves utility by
49.0%, 54.8%, and 33.4% over the three baselines.
II. MOTIVATION
With the scaling of foundation models, their fine-tuning for
downstream tasks becomes costly on cloud platforms. This
section highlights the cost-time trade-off introduced by using
cheaper yet unreliable spot GPUs, where spot prediction can
greatly enhance scheduling decisions.
A. Multi-Instance Parallel Fine-tuning
LoRA is an important fine-tuning method for LLMs that
decomposes the incremental weight matrix ∆W into two low-
rank matrices B and A at each layer, i.e. W = W0 + ∆W ≃
W0 + B · A, where W0 is the original LLM model weight.
Therefore, the number of learnable parameters shirnks from
from d2 to 2d · r with d denoted as the dimension of matrix
W and r (r ≪d) denoted as the low dimensional rank. Even
though, finetuning LLMs is still compute intensive, which
usually demands multi-GPU data parallelism. We evaluate the
impact of the number of NVIDIA A100 GPUs on the training
throughput in Figure 1. Both ChatGLM3-6B and Llama2-7B
are considered, and the batch size is unanimously set to 32.
Here, x-coordinate denotes the number of GPUs used in a
machine and y-coordinate shows the fine-tuning throughput
in samples per second. One can observe that the training
throughput increases almost linearly with the number of GPUs,
despite the models used.
(a) ChatGLM3-6B
(b) Llama2-7B
Fig. 1: Training throughput vs number of GPUs (A100 GPU).
LoRA fine-tuning with spot instances is feasible on both
cloud platforms and federated edge clouds. For example,
consider LLaMA2-7B, which has a model dimension of 4096,
32 layers, and a LoRA rank of 16. When the low-rank
matrices are updated in each iteration, the total communication
overhead amounts to approximately 16.8MB per iteration for
half-precision gradients. If two spot instance GPUs are hosted
on different machines but connected via a 200Gbps RDMA
link, the communication time per GPU is negligible compared
to the computation time (e.g., 10 seconds). However, if they
are connected over standard Internet links with a bandwidth
of 100Mbps, the communication time per iteration is ap-
proximately 1.35 seconds. This overhead can be effectively
amortized by performing multiple rounds of local training
before communications.
Dynamically adjusting the degree of data parallelism can
introduce nontrivial overhead, particularly in spot instance
scenarios. The reconfiguration time includes the transmission
of the training checkpoint, the startup time of the Docker
container, and the initialization of the fine-tuning process. A
typical checkpoint (model + LoRA parameters + optimizer
state) takes 0.58s over 200Gbps RDMA, but up to 1152s over
100Mbps links. We define this as the switching cost during
spot preemption, which depends on network speed but not
GPU count, since only new instances require reconfiguration.
B. Volatility of Spot Instance Price and Availability
Service providers offer two pricing models for GPU re-
sources: on-demand instances and spot instances. On-demand
instances are available whenever needed and provide guaran-
teed reliability, following a pay-as-you-go model. In contrast,
spot instances are significantly more cost-effective but come
with the trade-off of intermittent availability. Chasing the
lowest spot instance prices may reduce costs but risks violating
the SLO, while relying solely on on-demand instances results
in high fine-tuning expenses.
Figure 2 illustrates the availability and price fluctuations of
NVIDIA A100 GPUs over a 10-day period on Vast.ai [10],
a cloud computing platform. We collect data at 30-minute
intervals, recording the number of available and rented GPU
instances, along with both the free and rented prices. As shown
in Figure 2(b), the median instance price is only about 60% of
the P90 price. Hence, using spot instances wisely will lead to
a significant cost saving in fine-tuning. In Figure 2(a), the
number of available GPUs on the platform fluctuates over
time. Since Vast.ai aggregates all the GPUs across its clients,


--- Page 3 ---
(a) Availability
(b) Price
Fig. 2: Fluctuations in A100 Spot Instances on Vast.ai over 10 Days.
(a) Availability
(b) Price
Fig. 3: Forecasting Spot Availability and Price Using ARIMA.
the number of GPUs available within a specific geographic
region, especially those suitable for LLM fine-tuning, is often
limited, making spot instance availability unreliable. Unlike
the existing study [16] that consider only the dynamics of spot
GPU instance availability, our work accounts for both price
and availability fluctuations. This motivates us to develop an
online learning algorithm for purchasing on-demand or spot
instances so as to minimize the cost with SLO guarantee.
C. Understanding the Power of Prediction
The price and availability dynamics are not completely
random, but is predictable to a certain extent. Under this
circumstance, an online algorithm is designed to minimize
regret, defined as the difference between its performance and
the best possible fixed strategy in hindsight. By anticipating
the future inputs, online learning with prediction is likely
to make more informed choices, thus reducing the expected
cumulative regret. Unfortunately, the power of prediction has
been overlooked in online resource allocation for networking
applications.
Preliminary analysis of Figure 2 indicates that spot instance
availability, such as that of A100 GPUs, tends to follow a
daily trend, with higher availability during the daytime than
at night. We employ an Auto-Regressive Integrated Moving
Average (ARIMA) model [17] to learn the temporal dynamics
of instance availability and pricing, using a 30-minute time
window for prediction. As illustrated in Figure 3, our predic-
tions closely match the actual fluctuations. When predictions
are accurate, this substantially reduces uncertainty and enables
more aggressive allocation of low-cost spot resources for
training jobs, thereby reducing overall costs.
However, due to platform-specific pricing and the unpre-
dictability of real-world events, predictions may be unreliable.
Different machine types and regions potentially exhibit varying
fluctuation patterns, leading to low accuracy or unpredictable
Fig. 4: Comparison of Workload and Cost under Different Resource
Allocation Strategies.
behavior. In such cases, relying on forecasts could increase
costs and risks, making the consequences of inaccurate pre-
dictions critical to consider.
Figure 4 presents a comparative analysis of resource allo-
cation strategies with and without prediction. We simulate a
fine-tuning job that must complete 20 units of workload within
5 time slots, ignoring reconfiguration overhead for simplicity.
Baseline strategies without prediction include: (1) using only
on-demand instances, (2) prioritizing spot instances, and (3)
a hybrid progress-tracking strategy that adjusts allocations
based on partial progress. Prediction-based strategies include
(i) perfect foresight and (ii) a constant forecast of 6 available
spot instances, both of which incorporate on-demand instances
as fallback resources to ensure deadline compliance. Results
show that using only on-demand instances guarantees deadline
compliance but incurs high costs; prioritizing spot instances
reduces costs but may violate deadlines; the progress-tracking
strategy ensures timely progress but fails to exploit cheaper
spot resources effectively. In contrast, when predictions are
accurate, the system can utilize low-cost spot instances while
maintaining progress guarantees, resulting in the lowest overall
cost. However, when predictions are inaccurate, the total cost
can even exceed that of non-predictive strategies.
In summary, while accurate forecasting can substantially
reduce costs by enabling better utilization of spot instances,
it is crucial to develop mechanisms that can gracefully handle
cases of poor or unpredictable forecast performance.
III. SYSTEM MODEL
In this section, we model the allocation of hybrid instances
to a fine-tuning job with a deadline constraint. Our objective
is to achieve an optimal balance between SLO and cost.
A. Modeling of Fine-tuning Job Processing
We consider the problem of fine-tuning large foundation
models using LoRA [8], where fine-tuning jobs arrive sequen-
tially in a stochastic manner. For modeling clarity, we focus
on a single job that arrives at the system, while our framework
can be readily extended to handle multiple jobs. A fine-
tuning job is characterized by a four-tuple {L, d, N min, N max},
where L represents the total computation workload required
for completion, d is the preset completion deadline, and N min
and N max represent the minimum and maximum number of
GPUs required to process the job. Here, L is computed by


--- Page 4 ---
L = D × nepoch, where D is the number of data samples in
fine-tuning and nepoch is the number of epochs.
The degree of GPU parallelism for the job is limited. We
define N min as the minimum number of GPUs required to
store all necessary components in their HBM for fine-tuning,
including the base model, the LoRA adapter parameters, any
associated data, and, if applicable, the checkpoint and opti-
mizer states. The maximum parallelism N max is the highest
number of GPUs that can be used without significantly dimin-
ishing parallel efficiency due to under-utilization or overhead.
B. Modeling of Computing Time
We consider using multiple instances for parallel computing
to improve computational efficiency, and a mixed use of on-
demand and spot instances to reduce costs. Without loss of
generality, we evenly divide time into discrete time slots.
At each time slot, the available quantity of spot instances is
denoted by navail
t
, and their corresponding price is ps
t, which
remain constant within each time slot and only change across
discrete time periods. On-demand instances are considered
continuously available with a fixed price po. Any spot and
on-demand instances are considered homogeneous in terms
of GPU computing capability. Based on the real-world ex-
perimental results illustrated in Figure 1, we represent the
relationship between throughput and the number of instances
as
H(n) =
(
α · n + β
(β ̸= 0),
n ∈Z+,
0,
n = 0,
(1)
where n is the number of computing instances, and α stands
for the improved efficiency with the increased scale of par-
allelism. To avoid affecting the model’s convergence due to
changes in the number of instances, we fix the global batch
size. While we adopt a linear formulation for clarity, the
framework remains applicable to nonlinear throughput models.
Additionally, when the total number of instances changes,
other overheads are introduced, such as launching new in-
stances, data transfer, and synchronization operations between
multiple instances. We denote the proportion of effective
computation time within the time slot as:
µt =



µ1, nt > nt−1,
µ2, nt < nt−1,
1, nt = nt−1,
(2)
where nt is the total number of instances at time slot t. We
have µ1 ≤µ2 ≤1, where µ1 accounts for the additional
overhead introduced by both instance initialization and system
reconfiguration, and µ2 reflects only the reconfiguration cost.
C. Modeling of Cost and Revenue
At each time slot t, we allocate no
t on-demand instances and
ns
t spot instances for the job, until the entire job is completed
at time T. Therefore, the total cost of completing the job is
C (no
t, ns
t) =
T
X
t=1
(no
t · po + ns
t · ps
t).
(3)
We denote the revenue as a function of the deadline and
the completion time [18]. Since the time requirement for fine-
tuning jobs is elastic, we denote d as a soft deadline, and we
also use another hard deadline γ × d (γ > 1) to appropriately
tolerate delays in job completion. We denote the value function
for completing the job as
V (T) =









v,
T ≤d,
v ·

1 −
T −d
(γ −1) · d

,
d < T < γd,
0,
T ≥γd.
(4)
In our framework, we do not impose constraints on the specific
form of the value function. As long as the value function
approximately follows the aforementioned pattern, it remains
effective in yielding desirable results.
D. Problem Formulation
We formulate our GPU resource allocation problem for a
fine-tuning job as follows:
max
V (T) −C (no
t, ns
t)
(5)
s.t.
T
X
t=1
µt · H (no
t + ns
t) ≥L,
(5a)
ns
t ≤navail
t
,
∀t,
(5b)
no
t + ns
t ≤δt · N max,
∀t,
(5c)
no
t + ns
t ≥δt · N min,
∀t,
(5d)
δt ∈{0, 1},
no
t, ns
t ∈N,
∀t.
(5e)
The objective is to maximize the profit of executing the
job, calculated as the completion-time revenue minus the
accumulated cost. Constraint (5a) ensures that the job is
completed at time T. Constraint (5b) ensures that at each time
slot t, the number of spot instances does not exceed the spot
availability. Constraints (5c) and (5d) define the range of the
total number of instances at each time slot t. When the job’s
state is pending, i.e., δt = 0, both the number of on-demand
and spot instances are 0. When the job’s state is executing,
i.e., δt = 1, the total number of on-demand and spot instances
should be within the range [N min, N max]. Constraint (5e)
specifies the domain of the variables.
E. Problem Reformulation
Our goal is to design a prediction-based online algorithm to
optimize the objective. However, the original problem cannot
be directly addressed using standard prediction-based methods.
These algorithms operate within a fixed prediction window
in each iteration, which is misaligned with the objective (5)
that depends on the uncertain and potentially unbounded job
completion time T, which is influenced by future decisions. To
address this, we first refine the problem formulation through
several key transformations.
1) Workload Slicing for Deadline Achievement: At each
time slot, we only have access to the current and historical
spot instance prices and availability, while future data remains
unknown. To ensure the job is completed before the deadline,
we predefine a reference progress trajectory by slicing the total


--- Page 5 ---
workload L over the deadline d. If the corresponding portion
of the workload can be completed in each time slot, the entire
job will be finished by the deadline. Here, we use uniform
slicing, where the expected progress of the job at each time t
is
Zexp
t
= L
d · t.
(6)
2) Transformation of the Value Function: The objective
function consists of two components with different decision
variables: the value function depends on the completion time
T, while the cost function depends on no
t and ns
t. However,
since T can only be determined after the job is completed,
it introduces uncertainty into the optimization process. To
address this, we express T as a function of the cumulative
allocated resources across time slots, allowing the entire
objective to be written in terms of no
t and ns
t only.
Given that the job’s revenue rapidly diminishes to zero when
the completion time exceeds the deadline, as described in (4),
we introduce a termination configuration that simply choose
on-demand GPU instances with the maximum parallelism to
complete the workload immediately, and different termina-
tion schemes can be adopted without changing the modeling
framework. Consequently, we only need to track the completed
workload by the deadline to deduce the total completion time
T. We denote the workload completed before the deadline as
Zddl =
d
X
t=1
µt · H (no
t + ns
t),
(7)
and the monetary cost before deadline as
Cddl (no
t, ns
t) =
d
X
t=1
(no
t · po + ns
t · ps
t).
(8)
By incorporating the relationship between T and Zddl into the
original value function (4), we obtain a new function eV
 Zddl
which absorbs the portion of the cost corresponding to the time
beyond the deadline d, with Zddl as the independent variable.
Consequently, the new optimization objective is formulated as
max eV
 Zddl
−Cddl (no
t, ns
t) .
(9)
IV. ONLINE GPU PROVISIONING ALGORITHMS
In this section, we propose two online GPU provisioning
algorithms, addressing both predictive and non-predictive sce-
narios.
A. Online Algorithm for Predictive Scenarios
We adopt the Committed Horizon Control (CHC) algo-
rithm [19] as our foundation and tailor it to the specific
characteristics of our problem, resulting in our algorithm:
Adaptive Hybrid Allocation with Prediction (AHAP). We
select CHC from existing prediction-based online optimization
methods for its effective balance between decision stability and
adaptability under imperfect forecasts. CHC optimizes over a
finite prediction horizon but commits only to the initial subset
of decisions, thereby reducing the risk of excessive reliance
on uncertain predictions. Unlike Receding Horizon Control
(RHC) [20], which is sensitive to prediction errors, and Aver-
aging Fixed Horizon Control (AFHC) [21], which suffers from
error accumulation, CHC provides a tunable trade-off between
responsiveness and robustness. Hence, CHC is especially well-
suited for our dynamic environment characterized by noisy and
volatile predictions.
CHC involves two key hyperparameters: the prediction win-
dow ω and the commitment level v. The prediction window ω
determines the forecasting horizon at each time slot, providing
a predicted sequence of length ω. This allows the algorithm
to generate a sequence of decisions by maximizing the cumu-
lative objective over the prediction window. The commitment
level v determines the portion of the decision sequence that is
actually implemented. Specifically, in each time slot, only the
first v decisions in the sequence are executed, meaning that
the actual decision depends on the current and previous v−1
decision sequences.
To further adapt CHC to our setting, we introduce an addi-
tional hyperparameter σ, representing the spot price threshold.
In our design, as long as the job has not reached its deadline,
all spot instances priced below σ are utilized. This approach
allows us to aggressively leverage low-cost spot instances
to minimize overall resource costs. The introduction of σ
constitutes a key theoretical contribution, as it introduces an
additional source of prediction error in our model, which
directly affects the performance bound. This distinction allows
our algorithm to better handle the volatility of spot markets
while remaining cost-efficient.
The AHAP algorithm, presented in Algorithm 1, operates
as follows. At each time slot t, we compute the future ω time
slots’ forecasts and expected job progress (Lines 3–4). The
current job progress then is compared against the expected
progress. If the current progress exceeds the expected progress
(Lines 5–11), job allocation within the prediction window
prioritizes spot instances with prices below the threshold σ. If
the current progress lags behind the expected progress (Lines
12–13), the CHC framework is applied to compensate for
the shortfall within the prediction window by solving the
following problem to obtain the optimal allocation sequence
{no,t
t , . . . , no,t
t+ω} and {ns,t
t , . . . , ns,t
t+ω}:
max
{no,t
τ
},{ns,t
τ }
τ∈[t,t+ω]
eV
 Zt
t+ω

−
t+ω
X
τ=t

no,t
τ
· po + ns,t
τ
· ps
τ|t

.
(10)
We store the optimal allocation sequences at each time slot.
For instance, at time slot t−1, the optimal allocation sequence
for the ω + 1 time slots is recorded as {no,t−1
t−1 , . . . , no,t−1
t+ω−1}
and {ns,t−1
t−1 , . . . , ns,t−1
t+ω−1}. The final allocation decision at
time slot t is determined by averaging the allocations over
the past v time slots (Lines 14–16). The job is then executed
based on the allocation results (Line 17). AHAP repeats this
process for each time slot t until the job reaches its deadline,
yielding the complete resource allocation plan and the final
job utility u. This framework ensures that AHAP effectively
adapts to dynamic cloud environments by leveraging historical
commitments, predictive results, and price-threshold-driven


--- Page 6 ---
Algorithm 1: Adaptive Hybrid Allocation for Predic-
tive Scenarios (AHAP)
Input: {L,d,N min,N max},{po, ps
t, navail
t
},ω, v, σ.
Output: u, no
t, ns
t, t ∈[1, d] .
1 Initialize Z0 = 0, n0 = 0;
2 for t = 1, 2, . . . , d do
3
Predict spot price ps
·|t and availability navail
·|t
;
4
Calculate expected progress Zexp
t+ω at current time
slot according to (6);
5
if Zt−1 ≥Zexp
t+ω then
6
for τ = 0, 1, . . . ω do
7
if ps
t+τ|τ ≤σ · poand navail
t+τ|t ≥N min then
8
ns,t
t+τ ←min
n
navail
t+τ|t , N maxo
;
9
else
10
ns,t
t+τ ←0;
11
no,t
t+τ ←0;
12
else
13
Solve the problem (10) and get solutions
{no,t
τ }, {ns,t
τ },τ ∈[t, t + ω];
14
no
t ←
v−1
P
k=0
no,t−k
t
;
15
ns
t ←min{
v−1
P
k=0
ns,t−k
t
, navail
t
};
16
Limit no
t + ns
t in range of

N min, N max
;
17
Process the job and update the job progress
Zt ←Zt−1 + ηt · H(ns
t + no
t);
18
if Zt ≥L then
19
break from line 2;
20 Calculate utility u according to (9);
21 return allocation decisions and job utility u;
optimizations, thereby achieving efficient and cost-effective
GPU resource allocation.
We analyze the performance of the AHAP algorithm and
establish an upper bound on the utility difference between
AHAP and the offline optimal strategy. In particular, we
consider the impact of prediction errors when forecasting.
Since multi-step predictions tend to accumulate errors over
time, and prediction errors generally increase as the prediction
window lengthens, we formally define the upper bound on the
utility function’s prediction error as follows.
Definition 1. The sequence of true data yω+1, . . . , yd and
their ω-step ahead predictions yω+1|1 , . . . , yd|d−ω in func-
tion u (·) satisfy a ω-step prediction budget Gω,d if
d
X
t=ω+1
sup
x∈∆
u (x, yt) −u
 x, yt|t−ω
 ≤Gω,d.
(11)
Furthermore, we investigate the relationship between pre-
diction error and the price threshold σ. We assume that at
prediction level ω, the predicted spot availability with price
below threshold σ is limited to the range [0, Dω,σ].
Theorem 1 establishes a performance guarantee for the
AHAP algorithm under the given assumptions on the utility
function’s prediction error. Smaller prediction errors yield a
tighter upper bound on the performance gap. The bound alse
captures two key trade-offs: a higher commitment level v
enhances stability but reduces responsiveness, while a lower
price threshold σ improves cost efficiency at the risk of spot
shortages. Notably, the scenario-specific hyperparameter σ
introduces a prediction error, widening the performance gap
from optimality compared to the original CHC algorithm.
This distinction forms the core contribution of our algorithm,
allowing it to better address the dynamic challenges of volatile
spot markets.
Theorem 1. Assuming that the prediction budget of the utility
function U follows (11), then for algorithm 1, we have:
sup {U (OPT) −U (AHAP)} ≤2
v
v
X
k=1
Gk,d + σpod
v
v
X
k=1
Dk,σ
(12)
Sketch of Proof. We sketch the main steps of the analysis and
defer the full proof in Appendix B. The decision horizon
d is segmented into fixed-length windows, within which lo-
cal scheduling decisions (xτv+1, . . . , x(τ+1)v) are computed
based on predicted workloads. For each window, we bound
the utility gap caused by prediction errors Gv,d and Dv,σ. To
facilitate analysis, we construct a sequence of hybrid policies
by gradually replacing segments of the offline optimal solution
(x∗
τv+1, . . . , x∗
(τ+1)v) with these local predicted solutions.
By summing the per-window gaps and averaging across all
segments, we derive an overall regret bound that tightens
with decreasing prediction error under a bounded prediction
budget.
B. Online Algorithm for Non-predictive Scenarios
To address the limitations of AHAP under large prediction
errors, as indicated by Theorem 1, we introduce the Adaptive
Hybrid Allocation for Non-Predictive Scenarios (AHANP) as a
fallback strategy. Rather than serving as a zero-horizon variant
of AHAP, AHANP is a distinct reactive algorithm tailored
for settings with poor or unavailable predictions. It bypasses
explicit optimization and instead relies on interpretable per-
slot metrics to guide decisions, enabling rapid adaptation to
external dynamics and progress deviations. Specifically, we
introduce three indicators: workload progress ˆz = zt−1/zexp
t−1,
spot price ratio ˆp = ps
t/(σ · po), and availability change
rate ˆn = navail
t
/navail
t−1. These guide adaptive decisions that
(1) ensure progress toward deadlines, (2) prefer low-cost
spot instances, and (3) reduce reconfiguration by promoting
allocation stability.
The AHANP algorithm is presented in Algorithm 3, with
full pseudocode available in Appendix A. Specifically, for each
time slot t, AHANP computes the expected progress and gets
ˆz, ˆn and ˆp based on the current spot price and availability
(Line 3). Then AHANP assigns the total number of instances
nt for time slot t according to following conditions (Line 4). If
progress is ahead of schedule and no spot instances are avail-


--- Page 7 ---
able, the job remains idle (case 1). If the availability of spot
instances decreases significantly, the total number of instances
is reduced (case 2). If spot availability is stable but price is
high, prior allocation is reused to avoid reconfiguration. (case
3&4). If the spot price is low, all available spot instances are
utilized (case 5). When the progress is behind schedule, the
instance count is doubled (case 6&7). Then AHANP allocates
as many instances as possible to spot instances, with the
remaining assigned to on-demand instances (Lines 6–7). Based
on this allocation, it then computes job execution (Line 8).
This process is repeated for each time slot t until the deadline
is reached, ultimately producing a complete resource allocation
plan and the final job utility u.
This heuristic approach optimizes resource allocation with-
out reliance on future predictions while effectively balancing
cost efficiency and workload performance.
V. ONLINE POLICY SELECTION
In this section, we construct a policy pool by varying
hyperparameters of the algorithms proposed in Section IV,
and develop Online Policy Selection Algorithm to identify the
optimal policy from the pool.
A. Policy Pool
The online GPU provisioning algorithms in Section IV
require hyperparameter tuning, as they may not generalize
across dynamic environments with volatile spot prices and
availability. Sudden data shifts (e.g., maintenance or holidays)
can also degrade prior policies.
To address this, we construct a policy pool that combines
multiple GPU provisioning algorithms with systematically
varied hyperparameter settings to accommodate diverse en-
vironments. Specifically, AHAP policies are parameterized
by prediction window length ω, commitment level v, and
price threshold σ, while AHANP strategies adjust the price
threshold σ. The pool remains extensible to support additional
algorithms and configurations, enabling robust adaptation to
changing workload dynamics.
B. Algorithm Overview
We present the process of our Online Policy Selection
Algorithm in Algorithm 2. We consider that there are K
various fine-tuning jobs and we leverage the candidate policies
in the policy pool to process them. The policy pool includes M
candidate policies. The goal of the algorithm is to search for
the best policy among the candidate policy pool under different
scenarios. After a job is finished with a certain allocation
policy, we can calculate the utility of this candidate policy
m for the particular job k, denoted as um
k . We represent
the utility vector as uk = [u1
k, u2
k, · · · , um
k , · · · , uM
k ]. Our
algorithm works by learning a weight vector wk. The weight
vector is normalized PM
i=1 wi
k = 1, and can also be viewed as
a distribution from another perspective. Each element wm
k of
wk indicates the weight of candidate policy m, and a higher
weight means that this candidate policy has generally better
performance in terms of the loss. We define that completing
Algorithm 2: Online Policy Selection Algorithm
Input: A set of M candidate policies.
Output: The policy of the best performance among
the candidate policy pool.
1 The weights of each policy make up a vector
wk ∈{w ∈RM : wi > 0, ∥w∥1 = 1};
2 Initialize the weight vector w1 = [ 1
M , 1
M , · · · , 1
M ];
3 Set the learning rate η =
q
2 ln(M)
K
;
4 for k = 1, 2, . . . , K do
5
Receive job k;
6
Select the resource allocation policy for job k
according to the current weight vector wk;
7
for m = 1, 2, . . . , M do
8
Calculate the utility um
k , assuming we apply
allocation policy m to finish the job k;
9
for m = 1, 2, . . . , M do
10
Update the weights wm
k+1 ←
wm
k exp(η·um
k )
PM
i=1 wi
k exp(η·ui
k);
11
The new weight vector
wk+1 = [w1
k+1, w2
k+1, · · · , wM
k+1] is learned;
12 return weight vector wK;
a job represents one iteration. After each iteration k, we
update the weight vector wk+1 based on the utility vector uk
we collect. The update rule follows Exponentiated Gradient
(EG) [22], [23]. The learning rate η controls the update step
size, and we can obtain strong theoretical guarantees by setting
a proper η. The online learning algorithm aims to minimize
the regret. After a certain learning iterations, the algorithm
should converge and the learned weight vector converges to
a sparse vector, where the candidate policy with the highest
weight stands out as the best policy.
C. Theoretical Analysis
We make a theoretical analysis for the regret bound of our
Online Policy Selection Algorithm. The average regret of our
algorithm scales with the number of candidate policies M and
decays with the number of iterations K. As K →∞, the
average regret will approach 0.
Theorem 2. Suppose the utility function u is normalized and
we pick the learning rate η =
q
2 ln(M)
K
. Then for our online
learning algorithm shown in Algorithm 2, we have
max
y
K
X
k=1
uk(y) −
K
X
k=1
Ewk[uk] ≤
p
2K ln(M).
(13)
Sketch of Proof. We outline the key steps of the regret analy-
sis and defer full details to Appendix C. Our algorithm follows
a multiplicative weights update. Tracking the KL divergence
Φk
= DKL(y∥wk), We bound the per-round change in
η (Ewk[uk] −⟨y, uk⟩)+η2. By summing over all K iterations,
and choosing the learning rate η =
q
2 ln M
K
, we obtain
the total regret bound of O(
√
K ln M), which guarantees
sublinear average regret as K grows.


--- Page 8 ---
VI. EXPERIMENTAL EVALUATION
This section evaluates the proposed algorithms via simula-
tions to validate their effectiveness, robustness, and adaptabil-
ity in dynamic environments.
A. Evaluation Settings
Cloud Service. We simulate fine-tuning jobs on NVIDIA
A100 GPUs in 30-minute time slots. Spot price traces are
normalized from Vast.ai data with the on-demand price set
to 1. Regional spot availability is approximated by uniformly
downscaling global Vast.ai availability and capped within
[0, 16]. To limit reconfiguration overhead, we pre-select in-
stances with both upload and download bandwidths above 800
Mbps. While bandwidth variations impact multi-GPU scaling
and reconfiguration costs, they do not alter the analytical
framework.
Fine-Tuning Jobs. We consider fine-tuning jobs based on
the LLaMA2-7B model using LoRA with a rank of 16 and
a training dataset of 20 million tokens for one epoch. Such
a job requires approximately 5 hours on 8 A100 GPUs [24],
corresponding to 10 time slots, , which we use as a reference
deadline. Assuming unit GPU compute power equals 1, the
total workload is simplified to 80. We use half-precision
storage, and the model and data fit within the 80GB memory of
a single A100 GPU; therefore, we set the minimum parallelism
to N min = 1. We set the maximum parallelism N max = 12
based on observations of communication overhead in multi-
GPU setups [25]. When reconfiguration is required, launching
a new instance takes approximately 3 minutes under an 800
Mbps bandwidth. Accordingly, the proportion of effective
computation time within the time slot is set to µ = 0.9.
Baselines. We compare the proposed AHAP and AHANP
algorithms against the following approaches: the On-Demand
Only Policy (OD-Only), the Maximal Spot Utilization Policy
(MSU) and Uniform Progress Policy (UP) [16]. OD-Only ex-
ecutes fine-tuning jobs using only on-demand instances. MSU
prioritizes using all available spot instances in the early stages
of job execution and switches to on-demand instances near
the deadline. UP, though designed for static spot prices and
synchronized availability changes, remains a relevant baseline
given its adaptability to dynamic environments. At each time
slot, UP compares the accumulated progress against the ex-
pected progress, which incorporates reconfiguration overhead.
It prioritizes spot usage when available, and resorts to on-
demand instances only when progress falls behind and spot
instances are unavailable.
Policy Pool. To construct the policy pool, we generate
105 AHAP and 7 AHANP policies by systematically vary-
ing algorithmic hyperparameters. Specifically, for AHAP, we
consider prediction window lengths ω ∈{1, 2, 3, 4, 5}. For
each ω, the commitment level v takes integer values in [1, ω],
yielding a total of 15 (ω, v) combinations. For each of these,
we vary the price threshold σ ∈{0.3, 0.4, . . . , 0.9}, resulting
in 15×7 = 105 distinct AHAP policies. Here, ω controls how
far to predict, v determines how many predicted decisions are
applied, and σ sets the threshold below which spot instances
are considered cheap and fully utilized. For AHANP, only the
price threshold σ is varied over the same 7 values, producing
7 policies.
Prediction Noise. To compare online algorithm conver-
gence under different prediction errors, we define four pre-
diction noise settings. Noise is either magnitude-dependent
(Mag-Dep.) or fixed-magnitude (Fixed-Mag.), and follows
either a uniform or heavy-tailed distribution. This yields four
scenarios: Mag-Dep. + Uniform, Fixed-Mag. + Uniform, Mag-
Dep. + Heavy-Tail, and Fixed-Mag. + Heavy-Tail.
B. Evaluation Results
Impact of Jobs and Cloud Resource Dynamics. Since
the Online Policy Selection Algorithm can identify the better-
performing policy between AHAP and AHANP, we compare
each of them individually against the baselines using normal-
ized utility, where the selected optimal policy is always the bet-
ter of the two. Figure 5 illustrates how varying deadlines affect
the utility of fine-tuning jobs. AHAP consistently outperforms
all baselines under both tight and relaxed deadlines. At a
representative setting with deadline = 10, AHAP improves
utility by 49.0%, 54.8%, 33.4%, and 23.2% over OD-Only,
MSU, UP, and AHANP, respectively. Figure 6 shows the
impact of reconfiguration overhead on utility, simulated by
varying network bandwidth from 100 Mbps to 800 Mbps.
AHAP and AHANP consistently outperform other baselines.
As reconfiguration overhead increases, all algorithms suffer
utility degradation, except AHANP, which maintains stable
performance. This robustness stems from its design principle
of maintaining a similar number of active instances across time
slots, thereby minimizing reconfiguration overhead. Figures 7
and 8 analyze the impact of average spot instance availabil-
ity and price fluctuation, respectively. AHAP and AHANP
remain among the top-performing algorithms across all set-
tings. Overall, AHAP delivers the most stable performance,
while AHANP exhibits advantages under limited bandwidth
by avoiding frequent reconfigurations.
Convergence under Prediction Noise. Figure 9 illustrates
the convergence of the online policy selection algorithm to the
optimal policy under different types and levels of prediction
noise. We also investigate the impact of fixing individual hy-
perparameters in the policy pool, which consists of 105 AHAP
policies (e.g., fixing v = 1 or σ = 0.9). We simulate 1000 fine-
tuning jobs with workloads uniformly distributed in [70, 120],
deadlines fixed at 10 time slots, and parallelism parameters
combining N min ∈[1, 4] and N max ∈[12, 16]. The results
show that both the type and magnitude of noise influence the
optimal policy, highlighting the necessity of online adaptation
in dynamic environments. Furthermore, constraining hyperpa-
rameter flexibility affects the convergence outcome, indicating
that each hyperparameter in AHAP contributes to policy ef-
fectiveness. Increasing the number of flexible hyperparameters
enables finer-grained policies, thereby raising the upper bound
of achievable utility.
Policy Evolution and Adaptation. Figure 10 presents a
heatmap illustrating the dynamic evolution of policy weights


--- Page 9 ---
Fig. 5: Impact of Deadline.
Fig. 6: Impact of Reconfiguration
Overhead.
Fig. 7: Impact of Mean Spot
Availability.
Fig. 8: Impact of Mean Spot
Price.
(a) Mag-Dep. + Uniform
(b) Fixed-Mag. + Uniform
(c) Mag-Dep. + Heavy-Tail
(d) Fixed-Mag. + Heavy-Tail
Fig. 9: Convergence of Online Policy Selection under Different Prediction Noises and Hyperparameter Settings.
Fig. 10: Policy Weight Dynamics in Online Policy Selection with
Changing Prediction Quality.
under changing prediction environments. The prediction set-
tings change across four phases: iterations 0–800 use Fixed-
Mag. + Uniform with 10% error; 800–1600 switch to Fixed-
Mag. + Heavy-Tail with 30% error; 1600–2400 revert to Fixed-
Mag. + Uniform with 50% error; and 2400–3600 increase the
noise to 200%. The policy pool includes 105 AHAP and 7
AHANP policies, indexed from 1 to 112. The algorithm con-
sistently adapts to different settings and promptly converges
to new optimal policies, ensuring robust and efficient resource
allocation under dynamic conditions.
VII. RELATED WORK
Spot Instance Scheduling under Preemptions. Spot instances
offer significant cost advantages for cloud computing but
pose challenges due to their inherent unreliability and risk of
preemption. A key focus of recent research lies in balancing
cost-efficiency with reliability under such uncertainty
[26]–
[29].Several works propose deadline-aware scheduling to mit-
igate the impact of preemptions [13], [16], [30]. To improve
robustness, redundancy and over-provisioning techniques are
frequently employed [15], [31]. However, these approaches
do not integrate predictive information on spot price and
availability for forecasting future resources.
Online Resource Scheduling in Cloud Platforms. Online
resource scheduling refers to the process of assigning com-
putational resources to tasks or virtual machines in real-time,
without prior knowledge of future workloads. Initial work
in cloud scheduling employed simple heuristic methods such
as First-Come-First-Serve (FCFS) and Round Robin [32],
[33]. Recent studies have focused on reinforcement learning
(RL) and other machine learning models to learn dynamic
scheduling policies [34]–[38]. These models adapt to workload
patterns and improve over time, with promising results in both
simulation and real systems. However, these methods often
assume stable resource availability, limiting their effectiveness
in volatile spot markets.
Prediction-Augmented Scheduling for Deadline Guarantees.
A foundational aspect of prediction-augmented scheduling
lies in accurately estimating task execution time [39]. Recent
approaches apply regression models, decision trees, and neu-
ral networks to learn non-linear relationships between input
features and task duration [40], [41]. To further enhance
deadline adherence, some works focus on forecasting system
load or future task arrivals [42], [43]. However, many existing
solutions rely heavily on prediction accuracy without robust
fallback mechanisms. In contrast, our approach combines
forecast-driven planning with fallback resource allocation to
meet deadlines cost-effectively in volatile spot markets.
VIII. CONCLUSIONS
To minimize fine-tuning costs under deadline constraints
in both predictable and unpredictable spot market conditions,
we propose an analytical framework. It features a prediction-
based allocation algorithm, a complementary non-predictive


--- Page 10 ---
algorithm and further an online policy selection algorithm
that adaptively chooses the best-performing policy from both.
We show that the prediction-based algorithm improves with
prediction accuracy, while the policy selector achieves a regret
bound of O(
√
T). Experiments demonstrate that our frame-
work adapts effectively to varying conditions and consistently
outperforms baselines, improving utility by up to 54.8%.
The framework is flexible and remains adaptable to future
prediction and allocation strategies.


--- Page 11 ---
REFERENCES
[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman,
D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4
technical report,” arXiv preprint arXiv:2303.08774, 2023.
[2] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al.,
“Llama: Open and efficient foundation language models,” arXiv preprint
arXiv:2302.13971, 2023.
[3] J. Howard and S. Ruder, “Universal language model fine-tuning for text
classification,” arXiv preprint arXiv:1801.06146, 2018.
[4] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,
A. Gesmundo, M. Attariyan, and S. Gelly, “Parameter-efficient transfer
learning for nlp,” in International conference on machine learning.
PMLR, 2019, pp. 2790–2799.
[5] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for
parameter-efficient prompt tuning,” arXiv preprint arXiv:2104.08691,
2021.
[6] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,
A. M. Dai, and Q. V. Le, “Finetuned language models are zero-shot
learners,” arXiv preprint arXiv:2109.01652, 2021.
[7] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei,
“Deep reinforcement learning from human preferences,” Advances in
neural information processing systems, vol. 30, 2017.
[8] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,
W. Chen et al., “Lora: Low-rank adaptation of large language models.”
ICLR, vol. 1, no. 2, p. 3, 2022.
[9] Google Cloud, “Spot vms documentation,” https://cloud.google.com/
compute/docs/instances/spot, 2025, accessed: 2025-06-30.
[10] Vast.ai, “More gpus. more control. less spend.” https://vastai/pricing,
2025, accessed: June 30, 2025.
[11] RunPod, “Runpod console,” 2025, accessed: 2025-07-28. [Online].
Available: https://console.runpod.io/
[12] Amazon Web Services, “Aws lambda,” 2025, accessed: 2025-07-28.
[Online]. Available: https://aws.amazon.com/cn/lambda/
[13] I. Menache, O. Shamir, and N. Jain, “On-demand, spot, or both:
Dynamic resource allocation for executing batch jobs in the cloud,”
in 11th International Conference on Autonomic Computing (ICAC 14),
2014, pp. 177–187.
[14] Y. Song, M. Zafer, and K.-W. Lee, “Optimal bidding in spot instance
market,” in 2012 Proceedings IEEE Infocom. IEEE, 2012, pp. 190–198.
[15] Z. Mao, T. Xia, Z. Wu, W.-L. Chiang, T. Griggs, R. Bhardwaj,
Z. Yang, S. Shenker, and I. Stoica, “Skyserve: Serving ai models across
regions and clouds with spot instances,” in Proceedings of the Twentieth
European Conference on Computer Systems, 2025, pp. 159–175.
[16] Z. Wu, W.-L. Chiang, Z. Mao, Z. Yang, E. Friedman, S. Shenker,
and I. Stoica, “Can’t be late: optimizing spot instance savings under
deadlines,” in 21st USENIX Symposium on Networked Systems Design
and Implementation (NSDI 24), 2024, pp. 185–203.
[17] G. E. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung, Time series
analysis: forecasting and control.
John Wiley & Sons, 2015.
[18] D. Cheng, X. Zhou, Y. Xu, L. Liu, and C. Jiang, “Deadline-aware
mapreduce job scheduling with dynamic resource availability,” IEEE
transactions on parallel and distributed systems, vol. 30, no. 4, pp. 814–
826, 2018.
[19] N. Chen, J. Comden, Z. Liu, A. Gandhi, and A. Wierman, “Using
predictions in online optimization: Looking forward with an eye on
the past,” ACM SIGMETRICS Performance Evaluation Review, vol. 44,
no. 1, pp. 193–206, 2016.
[20] D. Q. Mayne, J. B. Rawlings, C. V. Rao, and P. O. Scokaert, “Con-
strained model predictive control: Stability and optimality,” Automatica,
vol. 36, no. 6, pp. 789–814, 2000.
[21] M. Lin, Z. Liu, A. Wierman, and L. L. Andrew, “Online algorithms for
geographical load balancing,” in 2012 international green computing
conference (IGCC).
IEEE, 2012, pp. 1–10.
[22] E. Hazan et al., “Introduction to online convex optimization,” Founda-
tions and Trends® in Optimization, vol. 2, no. 3-4, pp. 157–325, 2016.
[23] F. Orabona, “A modern introduction to online learning,” arXiv preprint
arXiv:1912.13213, 2019.
[24] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora:
Efficient finetuning of quantized llms,” Advances in neural information
processing systems, vol. 36, pp. 10 088–10 115, 2023.
[25] NVIDIA Corporation, NVIDIA Collective Communication Library
(NCCL)
Documentation,
2020,
accessed:
2025-07-24.
[Online].
Available:
https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/
index.html
[26] J.-B. Monteil, G. Iosifidis, and I. Dusparic, “Reservation of virtualized
resources with optimistic online learning,” in ICC 2023-IEEE Interna-
tional Conference on Communications.
IEEE, 2023, pp. 5147–5153.
[27] J. Duan, Z. Song, X. Miao, X. Xi, D. Lin, H. Xu, M. Zhang, and Z. Jia,
“Parcae: Proactive,{Liveput-Optimized}{DNN} training on preemptible
instances,” in 21st USENIX Symposium on Networked Systems Design
and Implementation (NSDI 24), 2024, pp. 1121–1139.
[28] F. Yang, L. Wang, Z. Xu, J. Zhang, L. Li, B. Qiao, C. Couturier,
C. Bansal, S. Ram, S. Qin et al., “Snape: Reliable and low-cost
computing with mixture of spot and on-demand vms,” in Proceedings
of the 28th ACM International Conference on Architectural Support for
Programming Languages and Operating Systems, Volume 3, 2023, pp.
631–643.
[29] J. R. Gunasekaran, C. S. Mishra, P. Thinakaran, B. Sharma, M. T.
Kandemir, and C. R. Das, “Cocktail: A multidimensional optimization
for model serving in cloud,” in 19th USENIX Symposium on Networked
Systems Design and Implementation (NSDI 22), 2022, pp. 1041–1057.
[30] Q. Wu, J. Fang, J. Zeng, J. Wen, and F. Luo, “Monte carlo simulation-
based robust workflow scheduling for spot instances in cloud environ-
ments,” Tsinghua Science and Technology, vol. 29, no. 1, pp. 112–126,
2023.
[31] J. Thorpe, P. Zhao, J. Eyolfson, Y. Qiao, Z. Jia, M. Zhang, R. Netravali,
and G. H. Xu, “Bamboo: Making preemptible instances resilient for
affordable training of large {DNNs},” in 20th USENIX Symposium on
Networked Systems Design and Implementation (NSDI 23), 2023, pp.
497–513.
[32] R. Buyya, C. S. Yeo, S. Venugopal, J. Broberg, and I. Brandic, “Cloud
computing and emerging it platforms: Vision, hype, and reality for
delivering computing as the 5th utility,” Future Generation computer
systems, vol. 25, no. 6, pp. 599–616, 2009.
[33] S. Selvarani and G. S. Sadhasivam, “Improved cost-based algorithm
for task scheduling in cloud computing,” in 2010 IEEE International
Conference on Computational Intelligence and Computing Research.
IEEE, 2010, pp. 1–5.
[34] R. Liu, R. Piplani, and C. Toro, “Deep reinforcement learning for
dynamic scheduling of a flexible job shop,” International Journal of
Production Research, vol. 60, no. 13, pp. 4049–4069, 2022.
[35] T. Zhou, D. Tang, H. Zhu, and Z. Zhang, “Multi-agent reinforce-
ment learning for online scheduling in smart factories,” Robotics and
computer-integrated Manufacturing, vol. 72, p. 102202, 2021.
[36] Y. Zhang, T. Zhang, G. Zhang, and H.-A. Jacobsen, “Lifting the fog
of uncertainties: Dynamic resource orchestration for the containerized
cloud,” in Proceedings Of The 2023 ACM Symposium On Cloud Com-
puting, 2023, pp. 48–64.
[37] W. Gao, P. Sun, Y. Wen, and T. Zhang, “Titan: a scheduler for foundation
model fine-tuning workloads,” in Proceedings of the 13th Symposium on
Cloud Computing, 2022, pp. 348–354.
[38] Y. Bao, Y. Peng, C. Wu, and Z. Li, “Online job scheduling in distributed
machine learning clusters,” in IEEE INFOCOM 2018-IEEE Conference
on Computer Communications.
IEEE, 2018, pp. 495–503.
[39] Y. Zhang, L. Luo, G. Sun, H. Yu, and B. Li, “Deadline-aware online
job scheduling for distributed training in heterogeneous clusters,” IEEE
Transactions on Cloud Computing, 2025.
[40] S. Nabi, M. Ibrahim, and J. M. Jimenez, “Dralba: Dynamic and resource
aware load balanced scheduling approach for cloud computing,” Ieee
Access, vol. 9, pp. 61 283–61 297, 2021.
[41] M. Xu, C. Song, H. Wu, S. S. Gill, K. Ye, and C. Xu, “esdnn:
deep neural network based multivariate workload prediction in cloud
computing environments,” ACM Transactions on Internet Technology
(TOIT), vol. 22, no. 3, pp. 1–24, 2022.
[42] M. E. Karim, M. M. S. Maswood, S. Das, and A. G. Alharbi, “Bhyprec:
a novel bi-lstm based hybrid recurrent neural network model to predict
the cpu workload of cloud virtual machine,” IEEE Access, vol. 9, pp.
131 476–131 495, 2021.
[43] H. Mao, M. Schwarzkopf, S. B. Venkatakrishnan, Z. Meng, and M. Al-
izadeh, “Learning scheduling algorithms for data processing clusters,” in
Proceedings of the ACM special interest group on data communication,
2019, pp. 270–288.


--- Page 12 ---
APPENDIX
A. Pseudocode of AHANP Algorithm
Algorithm 3: Adaptive Hybrid Allocation Algorithm
for Non-Predictive Scenarios (AHANP)
Input: {L,d,N min,N max},{po, ps
t, navail
t
},σ.
Output: u, no
t, ns
t, t ∈[1, d] .
1 Initialize Z0 = 0, n0 = 0;
2 for t = 1, 2, . . . , d do
3
Calculate expected progress Zexp
t−1 at current time
slot according to (6) and get ˆz, ˆn, ˆp;
// Assign nt according to ˆz, ˆnand ˆp
4
nt ←



















0
if ˆz ≥1, ˆn = 0
max

0.5 × nt−1, N min	
if ˆz ≥1, 0 < ˆn ≤0.5
nt−1
if ˆz ≥1, 0.5 < ˆn ≤1
nt−1
if ˆz ≥1, ˆn > 1, ˆp > 1
max

nt−1, navail
t
	
if ˆz ≥1, ˆn > 1, ˆp ≤1
N min,
if ˆz < 1, ˆn = ∞
2 × nt−1,
if ˆz < 1, ˆn < ∞
5
Limit nt in range of

N min, N max
;
6
ns
t ←min{navail
t
, nt};
7
no
t ←nt −ns
t;
8
Process the job and update the job progress
Zt ←Zt−1 + ηt · H(ns
t + no
t);
9
if Zt ≥L then
10
break from line 2;
11 Calculate utility u according to (9);
12 return allocation decisions and job utility u;
B. Proof of Theorem 1
Proof. We first analyze the utility difference between the al-
gorithm 1 and the optimal result before averaging the decision
results. Let Nk denote the number of time steps t = 1, 2, . . . d
that satisfy the condition t mod v = k, and let these time
points be denoted by
 tk,1, . . . , tk,Nk
. Let xt be the decision
vector at time slot t. Let yt be the vector to be predicted and
yt1,...,t2|t be the prediction results from t1 to t2 at time t. Let
{x∗} = x∗
1, . . . , x∗
d denote the set of offline optimal solutions.
According to algorithm 1, at time slot τv, if current workload
is less than the expected workload, we solve the following
problem:
max Uτv+1,...,(τ+1)v
 x, y τv+1,...,(τ+1)v|τv

,
and get the locally optimal solutions

xk
τv+1, . . . , xk
(τ+1)v

.
The locally optimal solutions, prediction results, and the
locally utility function are denoted by xk
τ,v, y ·|τv, and
Uτ,v
 x, y ·|τv

, respectively.
We define Nk sequences

ξ1, . . . , ξNk	
, each of length d,
where ξ1 is the offline optimal solutions {x∗}, and ξ1
t =
x∗
t , for t ∈[1, d]. For τ ∈[1, Nk], sequence ξτ satisfies the
following update rule:
ξτ+1 =

ξτ
1, . . . , ξτ
τv, xk
τv+1, . . . , xk
(τ+1)v, ξτ
(τ+1)v+1, . . . , ξτ
d

.
By the definition of local optimality under the predicted
window, we have
Uτ,v
 x∗
τ,v, y · |τv

≤Uτ,v
 xk
τ,v, y · |τv

.
Hence the difference of the actual utility satisfies:
Uτ,v
 x∗
τ,v, yτ,v

−Uτ,v
 xk
τ,v, yτ,v

≤Uτ,v
 x∗
τ,v, yτ,v

−Uτ,v
 x∗
τ,v, y · |τv

+ Uτ,v
 xk
τ,v, y · |τv

−Uτ,v
 xk
τ,v, yτ,v

+ σpo
v
X
t=1
Dt,σ,
where the last term accounts for the prediction error when
considering actual progress exceeding the expected progress.
Summing these inequalities from τ = 1 to τ = Nk, we
have
Ud
 ξ1, y

−Ud
 ξNk, y

= U(OPT) −Ud
 ξNk, y

≤
Nk
X
τ=1
 Uτ,v
 x∗
τ,v, yτ,v

−Uτ,v
 x∗
τ,v, y · |τv

+
Nk
X
τ=1
 Uτ,v
 xk
τ,v, y · |τv

−Uτ,v
 xk
τ,v, yτ,v

+ σpo
Nk
X
τ=1
v
X
t=1
Dt,σ.
By summing the actual utility differences across all k =
1, . . . , v, taking the average, and applying the Jensen’s inequal-
ity, we have
U(OPT) −U(AHAP)
≤1
v
v
X
k=1
Nk
X
τ=1
 Uτ,v
 x∗
τ,v, yτ,v

−Uτ,v
 x∗
τ,v, y · |τv

+ 1
v
v
X
k=1
Nk
X
τ=1
 Uτ,v
 xk
τ,v, y · |τv

−Uτ,v
 xk
τ,v, yτ,v

+ σpo
v
v
X
k=1
Nk
X
τ=1
v
X
t=1
Dt,σ
≤1
v
v
X
k=1
Nk
X
τ=1
Uτ,v
 x∗
τ,v, yτ,v

−Uτ,v
 x∗
τ,v, y · |τv

+ 1
v
v
X
k=1
Nk
X
τ=1
Uτ,v
 xk
τ,v, y · |τv

−Uτ,v
 xk
τ,v, yτ,v

+ σpod
v
v
X
t=1
Dt,σ
≤2
v
v
X
k=1
Gk,d + σpod
v
v
X
k=1
Dk,σ.


--- Page 13 ---
C. Proof of Theorem 2
The online learning algorithm that we apply is an extension
of OMD algorithm. The update of parameters in traditional
OMD algorithm is defined as
wk+1 = arg min
w∈V (⟨gk, w⟩+ 1
η Bψ(w; wk)),
where gk ∈∂uk(wk), V is the search space, and Bψ is
the Bregman divergence. In the design of our online learning
algorithm, we set ψ(w) = PM
i=1 wi ln(wi). Then, we can get
the update rule of our online learning algorithm, as is shown
in Algorithm 2
wi
k+1 =
xi
k exp(η · ui
k)
PM
t=1 xt
k exp(η · ut
k)
.
First, we introduce the concept of Bregman divergence and
dual norm, which is prerequisite for proving Theorem 2.
Definition 2. Let ψ : X
→R be strictly convex and
differentiable on X. The Bregman Divergence with respect
to ψ is denoted by Bψ, defined as
Bψ(x; y) = ψ(x) −ψ(y) −⟨∇ψ(y), x −y⟩.
Bregman divergence is a similarity measure between x and y.
Note that the Bregman divergence is not symmetric.
Lemma 1. Let V
= {x ∈RM : xi > 0, ∥x∥1 = 1},
X = RM
≥0, and ψ(x) = PM
i=1 xi ln(xi), the negative entropy.
Then, according to the definition of the Bregman divergence
in Definition 2, for all x, y ∈X, we have
Bψ(x; y) = ψ(x) −ψ(y) −∇ψ(y)⊤(x −y)
=
M
X
i=1
(xi ln(xi) −yi ln(yi) −(ln(yi) + 1)(xi −yi))
=
M
X
i=1
(xi ln(xi
yi
) −xi + yi).
This is a special case of Bregman divergence, called as the
generalized Kullback-Leibler divergence (KL divergence).
Lemma 2. Let Bψ the Bregman divergence w.r.t. ψ : X →R.
Then, for any three points x, y and z ∈X, the following
identity holds
Bψ(z; x)+Bψ(x; y)−Bψ(z; y) = ⟨∇ψ(y)−∇ψ(x), z−x⟩.
Definition 3. The dual norm ∥· ∥⋆of a norm ∥· ∥is defined
as ∥θ∥⋆= maxx:∥x∥≤1⟨θ, x⟩.
Next, we present the one-step relationship.
Lemma 3. Let Bψ the Bregman divergence w.r.t. ψ : X →R,
and assume ψ to be proper, closed, and λ-strongly convex. Let
gk ∈∂uk(wk). ∀y ∈V , the following inequality holds
η(uk(y)−uk(wk)) ≤Bψ(wk; y)−Bψ(wk+1; y)+ η2
2λ∥gk∥2
⋆.
Proof. First of all, from the strong convexity of uk, we have
η(uk(y) −uk(wk)) ≤η⟨gk, y −wk⟩.
We further expand the equality as
η⟨gk, y −wk⟩= ⟨∇ψ(wk) −∇ψ(wk+1) −ηgk, wk+1 −y⟩
+ ⟨∇ψ(wk+1) −∇ψ(wk), wk+1 −y⟩+ ⟨ηgk, wk −wk+1⟩
≤⟨∇ψ(wk+1) −∇ψ(wk), wk+1 −y⟩+ ⟨ηgk, wk −wk+1⟩.
η⟨gk, y −wk⟩= ⟨∇ψ(wk) −∇ψ(wk+1) −ηgk, wk+1 −y⟩
+ ⟨∇ψ(wk+1) −∇ψ(wk), wk+1 −y⟩
+ ⟨ηgk, wk −wk+1⟩
≤⟨∇ψ(wk+1) −∇ψ(wk), wk+1 −y⟩
+ ⟨ηgk, wk −wk+1⟩.
Using Lemma 2, we can interpret the first term as
⟨∇ψ(wk+1) −∇ψ(wk), wk+1 −y⟩
=
Bψ(wk; y) −Bψ(wk+1; y) −Bψ(wk+1; wk).
As ψ(w) is λ-strongly convex, we have
Bψ(wk+1; wk) ≥λ
2 ∥wk+1 −wk∥2.
According to the definition of the dual norm, we have
⟨ηgk, wk −wk+1⟩≤∥ηgk∥⋆∥wk −wk+1∥.
Putting above two together, we have
⟨∇ψ(wk+1) −∇ψ(wk), wk+1 −y⟩+ ⟨ηgk, wk −wk+1⟩
≤Bψ(wk; y) −Bψ(wk+1; y) −λ
2 ∥wk+1 −wk∥2
+ η∥gk∥⋆∥wk −wk+1∥
≤Bψ(wk; y) −Bψ(wk+1; y) + η2
2λ∥gk∥2
⋆,
where in the last inequality, we use the fact that −b
2x2 +ax ≤
a2
2b holds true for x ∈R and a, b > 0.
Hence, we complete the proof that
η(uk(y) −uk(wk)) ≤η⟨gk, y −wk⟩
≤Bψ(wk; y) −Bψ(wk+1; y) −Bψ(wk+1; wk)
+ ⟨ηgk, wk −wk+1⟩
≤Bψ(wk; y) −Bψ(wk+1; y) + η2
2λ∥gk∥2
⋆.
Based on Lemma 3, we can prove the regret bound for OMD
algorithm.
Lemma 4. Set w1 ∈V such that ψ is differentiable in w1.
Assume ηk is constant, i.e., ηk = η, ∀k = 1, 2, · · · , K. Then,
under the assumptions of Lemma 3 and ∀y ∈V , we can prove
that
K
X
k=1
(uk(y) −uk(wk)) ≤Bψ(w1; y)
η
+ η
2λ
K
X
k=1
∥gk∥2
⋆.


--- Page 14 ---
Proof. Based on Lemma 3, we have
K
X
k=1
(uk(y) −uk(wk)) ≤1
η
K
X
k=1
(Bψ(wk; y) −Bψ(wk+1; y))
+ η
2λ
K
X
k=1
∥gk∥2
⋆= 1
η (Bψ(w1; y) −Bψ(wK+1; y))
+ η
2λ
K
X
k=1
∥gk∥2
⋆≤Bψ(y; w1)
η
+ η
2λ
K
X
k=1
∥gk∥2
⋆.
Then, we have to prove the strong convexity of ψ(w) =
PM
i=1 wi ln(wi) in our online learning algorithm, in order to
use the conclusion of Lemma 3.
Lemma 5. ψ(w) = PM
i=1 wi ln(wi) is 1-strongly convex with
respect to the L1 norm over the set J = {w ∈RM : wi >
0, ∥w∥1 = 1}.
Proof. We prove the strong convexity of ψ(w) directly
through the definition of the strong convex function, and we
have
⟨∇ψ(x) −∇ψ(y), x −y⟩=
M
X
i=1
(xi −yi) ln(xi
yi
)
=
M
X
i=1
(xi ln(xi
yi
) + yi ln( yi
xi
))
≥1
2
M
X
i=1
|xi −yi|2 + 1
2
M
X
i=1
|yi −xi|2
≥∥x −y∥2
1,
where we use Pinsker’s inequality to complete the proof.
Under the assumption of Lemma 1, when we set w1 =
[ 1
M , 1
M , · · · , 1
M ], we have
Bψ(w1; y) =
M
X
i=1
yi ln(yi) + ln(M) ≤ln(M).
We also have ∥gk∥⋆≤1 for the utility functions uk in our
resource allocation problem and we set η =
q
2 ln(M)
K
. Then,
from Lemma 4 and Lemma 5, we have
K
X
k=1
(uk(y) −uk(wk)) ≤Bψ(w1; y)
η
+ η
2λ
K
X
k=1
∥gk∥2
⋆
≤ln(M)
η
+ η · K
2
=
r
K · ln(M)
2
+
p
2K · ln(M)
2
=
p
2K ln(M),
which completes the theoretical proof of Theorem 2 for our
online learning algorithm.
Proof. Let Lk := PM
i=1 wi
kui
k denote the expected utility
under the current distribution, and let Uk(y) := PM
i=1 yiui
k be
the utility under the fixed distribution y. Define the cumulative
regret:
RegretK =
K
X
k=1
(Uk(y) −Lk) .
We define a potential function based on the Kullback-
Leibler (KL) divergence between a fixed target distribution
y ∈∆M and the algorithm’s current distribution wk:
Φk := DKL(y ∥wk) =
M
X
i=1
yi ln yi
wi
k
.
The multiplicative weights update is given by:
wi
k+1 = wi
k · exp(ηui
k)
Zk
,
where Zk =
M
X
j=1
wj
k · exp(ηuj
k).
We analyze the change in potential Φk+1 −Φk:
Φk+1 −Φk =
M
X
i=1
yi ln wi
k
wi
k+1
=
M
X
i=1
yi  ln Zk −ηui
k

= ln Zk −η
M
X
i=1
yiui
k
= ln Zk −ηUk(y).
Using the Taylor expansion inequality for the exponential
function exp(x) ≤1 + x + x2,
for x ∈[0, 1], and assuming
the learning rate η is sufficiently small, we have that
Zk ≤
M
X
i=1
wi
k
 1 + ηui
k + η2
= 1 + η
M
X
i=1
wi
kui
k + η2,
where we used the fact that PM
i=1 wi
k = 1.
Next, applying the inequality for the logarithm function
ln(1 + x) ≤x,
for x > −1, we obtain
ln Zk ≤η
M
X
i=1
wi
kui
k + η2 = ηLk + η2..
Combining the above inequalities, we obtain the per-round
change in KL divergence:
Φk+1 −Φk ≤η (Lk −Uk(y)) + η2,
which, after rearranging, yields:
Uk(y) −Lk ≤Φk −Φk+1
η
+ η.
This inequality bounds the instantaneous regret (i.e., the
performance gap between the fixed comparator distribution
y and the current decision wk) using the change in KL
divergence and a small additive term η.
Summing both sides over all rounds k = 1 to K, we get


--- Page 15 ---
the total regret:
K
X
k=1
(Uk(y) −Lk) ≤1
η
K
X
k=1
(Φk −Φk+1) + Kη
= 1
η (Φ1 −ΦK+1) + Kη
≤Φ1
η + Kη,
since ΦK+1 ≥0 by non-negativity of KL divergence.
Now we bound the initial divergence Φ1. Since the initial
weight vector is uniform, i.e., wi
1 =
1
M for all i, and since y
lies in the probability simplex ∆M, we have:
Φ1 = DKL(y∥w1) =
M
X
i=1
yi ln
 yi
1/M

=
M
X
i=1
yi ln(Myi) ≤ln M,
where the last inequality follows from Jensen’s inequality or
the fact that the KL divergence is maximized when y is a unit
vector.
Therefore, we obtain the following regret bound:
RegretK ≤ln M
η
+ Kη.
Optimizing the bound by choosing the learning rate η =
q
2 ln M
K
yields:
RegretK ≤
√
2K ln M,
which is sublinear in K, implying that the average regret
RegretK
K
→0 as K →∞.
