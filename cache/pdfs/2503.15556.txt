--- Page 1 ---
arXiv:2503.15556v2  [cs.SE]  4 Apr 2025
Fully Automated Generation of Combinatorial Optimisation
Systems Using Large Language Models
Daniel Karapetyana
aUniversity of Nottingham, School of Computer Science, United Kingdom
Abstract
Over the last few decades, researchers have made considerable eﬀorts to make decision
support more accessible for small and medium enterprises by reducing the cost of de-
signing, developing and maintaining automated decision support systems. However, due
to the diversity of the underlying combinatorial optimisation problems, reusability of
such systems has been limited; in most cases, expensive expertise has been required to
implement bespoke software components.
We explore the feasibility of fully automated generation of combinatorial optimisation
systems using large language models (LLMs). An LLM will be responsible for interpreting
the user-provided problem description in natural language and designing and implement-
ing problem-speciﬁc software components. We discuss the principles of fully automated
LLM-based optimisation system generation, and evaluate several proof-of-concept gen-
erators, comparing their performance on four optimisation problems.
Keywords:
automated generation of algorithms, large language models, combinatorial
optimisation, decision support systems
1. Introduction
Optimisation is one of the most ubiquitous mathematical tools, particularly crucial
in decision support. While some optimisation problems are mathematically trivial, oth-
ers have far too many choices for a human to handle. This gives rise to optimisation
algorithms that can eﬀectively search large spaces of solutions.
Over the last decades, the community of researchers and practitioners has rapidly
advanced optimisation algorithms and optimisation systems (OSs), i.e. decision support-
/making systems based on them.
While optimisation is still computationally expen-
sive, these OSs routinely achieve suﬃciently good results in most applications.
The
challenge, however, is that the development of OSs is often prohibitively expensive for
individuals and small and medium enterprises, as it requires unique expertise and is
time-consuming [1]. In other words, the bottleneck in many cases is not the algorithm
performance but the cost of the OS development and maintenance.
These costs could potentially be reduced by reusing existing OSs, but, unfortunately,
the reusability of OSs is limited due to the vast diversity of the optimisation problems [2].
This is particularly true for combinatorial optimisation problems, thus we focus on com-
binatorial optimisation in this paper.
Preprint submitted to Elsevier
March 2025


--- Page 2 ---
Nevertheless, signiﬁcant eﬀort has been put into reducing the cost of decision support.
We can identify three main approaches:
1. Reuse of an OS designed to tackle a narrow family of optimisation problems. Such
systems have user interfaces for customisation of the optimisation problem and
feeding in the instance data. In complex cases, such interfaces require the user to
write program code. This approach is practical only for relatively typical problems,
as it needs many users to be ﬁnancially viable. In this study, we focus on a generic
approach that works on a wide range of optimisation problems.
2. Partial automation of the algorithm design process, often referred to as hyper-
heuristics [3]. In this approach, a human expert develops a set of problem-speciﬁc
software components, and a problem-independent artiﬁcial intelligence (AI) frame-
work forms an algorithm based on these components.
3. Reduction to a standard problem. Instead of designing a new algorithm for the
problem in hand, one can model the problem using a formal language and then
apply an oﬀ-the-shelf solver for that language. A widely used example of such a
formal language is mixed-integer programming (MIP). The problem is modelled
using MIP, and then the search for an eﬃcient solution is delegated to an oﬀ-the-
shelf MIP solver.
This means, however, that an optimisation system based on
an MIP solver needs additional routines to translate the problem into the MIP
language and then translate the MIP solution back into the problem language.
These routines are problem-speciﬁc and often non-trivial.
Note that the second and third approaches still require a human expert to design
and develop problem-speciﬁc components, including data structures and input/output
routines.
Thus, the process involves the user describing the decision problem and a
human expert building the OS. The aim of this paper is to explore the feasibility of full
automation of OS generation to completely eliminate the need for a human expert. The
user will provide a description of the problem in a natural language. Additionally, the
user may be expected to provide a set of benchmark instances (training data). Based on
these inputs, a fully automated OS generator (FAOG) will produce an OS.
It should be noted that the design of an OS also requires mathematical modelling of
the problem, i.e. describing it with mathematical primitives. In this paper, we expect that
the user is capable of problem modelling; the problem description provided by the user
must be mathematically precise. Our rationale is that modelling is often straightforward
for simple optimisation problems; the number of people with the appropriate skills is
higher than the number of people capable of both modelling a problem and developing
an optimisation system.
However, we recognise that automation of modelling is an
interesting direction for future research.
The main novelty of this study is the use of large language models (LLMs) to achieve
full automation of OS design and development. Modern LLMs are capable of producing
computer code based on instructions in a natural language. By carefully engineering LLM
prompts with embeddings of the user-provided problem description and orchestrating the
code composition and testing processes, we can expect to fully automate the generation
of all the software components and, in turn, the entire OS.
Since the rise of LLMs, there have been several studies using LLMs for mathemat-
ical optimisation (most of them are not peer-reviewed yet). Researchers explored the
feasibility of using LLMs as the optimisation system [4, 5] or optimisation algorithm
2


--- Page 3 ---
component [6, 7]. In these studies, the instance data and, sometimes, the solution data
are fed to the LLM and the LLM uses its intelligence to produce new problem solutions.
The key strength of this approach is its ﬂexibility as LLMs can handle a wide range of
problems and data representations. However, this approach is limited due to the high
computation cost of LLMs, restrictions on the prompt size and hallucinations. There
were also several attempts to use LLMs to design new metaheuristics [8, 9]. These stud-
ies show the viability of using LLMs to generate algorithmic components based on textual
descriptions however they are focused on continuous optimisation. A recent study has
also explored the feasbility of automated generation of linear and mixed-integer linear
programs for combinatorial optimisation problems [10]. To the best of our knowledge,
there has been no research using LLMs to auto-generate heuristic combinatorial optimi-
sation optimisation systems, which is the focus of this paper.
The main contributions of the paper are as follows:
• The novel concept of a FAOG employing an LLM to develop problem-speciﬁc soft-
ware components.
• A discussion of FAOG principles and strategies for tackling associated issues.
• A combination of the LLM-based intelligence with the automated algorithm con-
ﬁguration.
• A library of problem descriptions for future research of automated OS generation.
• A computational study with several proof-of-concept FAOGs.
The paper is structured as follows.
In Section 2, we discuss possible approaches
to FAOG design. Section 3 gives details of our proof-of-concept FAOGs designed for
computational experiments. Section 4 introduces a library of problem descriptions, and
Section 5 reports on the results of our computational study. A discussion of our ﬁndings
and concluding remarks are given in Section 6.
2. Principles of fully automated OS generation
We begin with setting our objectives in designing a FAOG and then discuss the overall
architecture of FAOGs. Finally, we focus on speciﬁc modules of a FAOG.
2.1. FAOG design objectives
The aim of a FAOG is to generate an OS based on a natural language description
of the problem provided by the user. Interpretation of the problem description requires
natural language processing (NLP), making the OS generation inherently unreliable [11].
In other words, it is impossible to guarantee that the generator will always produce a
working and correct OS. Thus, our key objective is to maximise the success rate of the
FAOG, i.e. the probability that, given an adequate problem description, it will produce
a working and correct OS.
Observe that the design and development of high-performing optimisation algorithms
traditionally require unique expertise and signiﬁcant time investment. While there have
been many successful attempts to use artiﬁcial intelligence to develop optimisation algo-
rithms, to the best of our knowledge, none of them has been shown to consistently deliver
3


--- Page 4 ---
state-of-the-art performance across a wide range of optimisation algorithms without con-
siderable interventions from human experts. Thus, one may expect FAOGs to produce
relatively ineﬃcient algorithms; such algorithms may not be able to ﬁnd suﬃciently good
solutions or may take too long.
Finally, a FAOG is only useful if it can produce an OS within a reasonable time. As a
reference point, one can use the time required for a human expert to design and develop
an OS.
Considering the above observations, the key considerations when designing a FAOG
are as follows (in the order of priority):
1. Success rate of the generation process: a FAOG should be able to generate a
working OS for as many problems as possible.
2. Quality of solutions found by the generated OS.
3. Time budget required for the generated OS to ﬁnd reasonable solutions.
4. Time budget required for the OS generation.
Problem description
(natural language)
Training instances
(raw data, optional)
FAOG
NLP module
OS (computer program):
– Input routine
– Instance data structure
– Solution data structure
– Optimisation algorithm
– Output routine
(a) FAOG architecture.
OS
Instance data (UDF)
Input routine
Instance data (RAM)
Optimisation algorithm
Solution data (RAM)
Output routine
Solution data (UDF)
(b) OS architecture.
Figure 1: The architectures of a FAOG and the generated OS.
2.2. FAOG architecture
To implement an OS, one needs data structures to store the instance data and the
solution data in random access memory (RAM), input/output routines to input the
instance data in the user-deﬁned format (UDF) and output the solution data in UDF,
and an optimisation algorithm that takes the instance data and produces a solution,
see Figure 1b.
Figure 1a shows the architecture of a FAOG that takes the problem
description in a natural language and uses an NLP module to interpret it. Optionally,
such a FAOG may also take a set of training instances.
During the OS generation process, the computer is expected to interpret the problem
description in a natural language and produce computer code based on this description.
To the best of our knowledge, the only readily available tool capable of performing
4


--- Page 5 ---
such tasks is LLM. An LLM-based FAOG can produce prompts that include the user-
provided description of the problem and, for example, request the LLM to produce a
Python function that calculates the objective value of a given solution.
While FAOGs can, hypothetically, generate algorithms of any type, we focus on
heuristic algorithms in this study.
Heuristics are generally simpler than eﬀective ex-
act methods, and they can be composed of multiple simple algorithmic components. We
expect LLMs to work more reliably when the tasks are relatively simple. Another reason
for focusing on heuristic algorithms is that they are more likely to produce acceptable
solutions within the available time budget. For most business cases, a non-optimal so-
lution produced within a time budget is preferable to an optimal solution produced well
after it was needed.
To maximise the FAOG success rate, it is logical to use the divide and conquer
approach: split the generation of various OS modules into several processes. Each module
can be tested independently, with mistakes being corrected before the rest of the system
is generated. This approach increases the probability of successful OS generation within
a given number of attempts.
2.3. Data structures and input/output routines generation
An OS needs to store the instance data and the solution data, meaning that it needs
appropriate data structures. The speciﬁc data structures used by an OS depend on the
problem and, thus, have to be generated each time a new OS being developed.
Note that some optimisation solvers use uniﬁed problem representations during the
solution process (consider mixed-integer programming solvers). However, this approach
requires the original instance data to be translated into the uniﬁed representation, and
then the solution to be translated into the domain-speciﬁc solution data structure. Hence,
the system still needs custom data structures for the instance and solution data.
The choice of the data structures and their implementation require intelligence; we
rely on the LLM to supply this intelligence. We can, however, assist the LLM by providing
a clear and detailed set of instructions about the programming interfaces that must be
implemented.
Among other functions, the instance data structure needs to support
loading the instance data in UDF, and the solution data structure needs to support
outputting the solution data in UDF.
2.4. Optimisation algorithm generation
Once the data structures for storing instance data and solution data are composed,
the FAOG needs to generate the optimisation algorithm, i.e. a routine that takes the
instance data as the input and returns an eﬃcient solution to the problem. We propose
the following classiﬁcation of the optimisation algorithm generation approaches:
Monolithic approach: the LLM composes the entire optimisation algorithm. Specif-
ically, it produces a function that takes the problem instance as the input and
returns a solution to this problem.
Reduction-based approach: the LLM composes routines to reduce (translate) the
problem instance to another problem, run an external solver designed speciﬁcally
for that problem and translate the solution back to the original problem.
For
example, the LLM-composed routine encodes the problem in MIP, feeds it to an
5


--- Page 6 ---
oﬀ-the-shelf MIP solver, runs the MIP solver, and translates the MIP solution to
the original problem solution.
Component-based approach: the LLM composes simple algorithmic components that
are used by a problem-independent framework as building blocks. Each component
constructs or manipulates solutions. For example, a component may take a feasible
solution and modify it while preserving its feasibility.
The above approaches can also be hybridised. For example, some of the components in
the component-based approach could use the reduction-based approach, or the problem-
independent part of the component-based approach could also be produced by the LLM
thus hybridising it with the monolithic approach.
An algorithm performing well on one family of problem instances might be relatively
ineﬃcient on another family of problem instances.
This gives rise to algorithm con-
ﬁguration and parameter tuning commonly used in optimisation algorithm design [12].
The idea is to tweak the algorithm and/or its parameters so that it performs well on a
set of training instances. Then the expectation is that it will also perform well on the
previously unseen but similar instances.
We distinguish FAOGs with and without oﬄine training. FAOGs with oﬄine train-
ing require the user to provide a set of training instances in addition to the problem
description and use these instances to improve the performance of the algorithm on the-
se/similar instances. FAOGs without oﬄine training produce the algorithm solely based
on the problem description.
There is also an adjacent concept of an adaptive algorithm; an adaptive algorithm
performs learning during its run and adapts accordingly. An algorithm generator of any
type can potentially create an adaptive algorithm, however we do not explicitly explore
generation of adaptive algorithms in this paper.
3. FAOG design
To investigate the viability of fully automated OS generation, we built several proof-
of-concept FAOGs based on the ideas discussed in Section 2. Below, we provide the
details of the design of our FAOGs.
3.1. Problem description
The input of a FAOG is the problem description produced by the user. To help the
user in structuring the problem description, the input is divided into several sections as
detailed in Table 1. This also makes it easy for the FAOG to include only the relevant
information in each LLM prompt.
3.2. OS design
Here, we discuss the design of the OS to be generated by our FAOG.
We chose Python as the programming language for this study. While programs in
Python are known to be signiﬁcantly slower than programs in languages such as C++, it
has several important advantages. Most importantly, due to the vast amount of training
data, LLMs are particularly good in Python. Secondly, Python is a convenient language
6


--- Page 7 ---
Section
Section description
Example
Input data
Semantic description of the
instance data.
Integer n. Set V of n cities. Positive
integer cost c(u, v) for each u ∈V
and v ∈V .
Solution
Semantic description of the
solution data.
A sequence of cities (v1, v2, . . . , vn).
Constraints
List of constraints that have
to be satisﬁed for a solution
to be feasible.
The solution should include exactly
n cities. Cities cannot repeat.
Instance
ﬁle format
The description of the in-
stance ﬁle format.
Text ﬁle. The ﬁrst line contains num-
ber n.
The next n lines contain
the n × n matrix giving the weights
c(u, v), where u is the row index and
v is the column index. Numbers are
space-separated.
Solution
ﬁle format
The description of the solu-
tion ﬁle format.
Text ﬁle. One line with n numbers
giving the sequence of cities: v1, v2,
. . . , vn.
The numbers are space-
separated.
The ﬁrst city has index
1.
Example i
(for i =
1, 2, . . .)
An example of an instance
and a solution to it.
Instance: toy instance.txt
Solution: toy solution1.txt
Objective value: 8
Training
instances
A list of training instances.
instance1.txt
instance2.txt
Table 1: The sections of the problem description ﬁle.
for dynamic code handling. Thirdly, Python supports the object-oriented programming
paradigm which is convenient for splitting the code into several relatively independent
units. Finally, Python is frequently used by the research community for distributing
source codes as they are easy to understand and run on a wide range of hardware.
There are three main classes in our OS: instance class, solution class, and algorithm
class (which in turn may use several other classes), see Figure 2. The instance class is
responsible for reading the instance ﬁle and storing the instance data. The solution class
is responsible for creating a new random solution, storing solution data, calculating the
objective value, testing the solution feasibility and saving the solution to a ﬁle. The
algorithm class is responsible for producing an eﬃcient solution within a given time
budget.
7


--- Page 8 ---
MySolution
Attributes:
- problem instance: MyInstance
- 〈Solution data〉
Methods:
+
init (instance)
+ is feasible()
+ get objective()
+ load from ﬁle(input ﬁlename)
+ save to ﬁle(output ﬁlename)
MyAlgorithm
Methods:
+
init ()
+ solve(instance, time budget ms)
MyInstance
Attributes:
- 〈Instance data〉
Methods:
+
init (ﬁle path)
MyExtendedInstance
Methods:
+
init (ﬁle path)
+
deepcopy (memo)
Figure 2: UML diagram of the auto-generated OS.
3.3. General principles of LLM prompt engineering
LLMs are sensitive to prompts [13], meaning that good prompt engineering techniques
can contribute to the success rate of a FAOG. However, studies have shown that prompts
optimised for one model may not perform well on another model, and more research
is needed to develop our understanding of the eﬀects of prompt engineering [13, 14].
Considering that the FAOGs developed for this study are only proof-of-concept, we opt
for simple prompts leaving more advanced prompt engineering and its analysis to future
research.
Our FAOG divides the OS generation process into sub-tasks and executes multiple
prompts followed by response validation and error correction. This gives more opportu-
nities to correct mistakes.
Each of our prompts requests the LLM to create a single class. We aim to ensure
that these classes are compatible with the rest of the OS (have the correct programming
interfaces) and perform tasks as expected. To achieve compatibility, our prompts include
comprehensive lists of class methods and their signatures (lists of parameters with their
types and the return type). Additionally, some prompts ask to save data in instance
variables with speciﬁc names. The other implementation details are left to the ‘creativity’
of the LLM.
In our experience, LLMs may not follow every instruction precisely. Thus, to max-
imise the success rate of a FAOG, we reduce the reliance on the LLM where possible. Once
the problem-speciﬁc classes (MyInstance, MySolution and MyAlgorithm) are generated
by the LLM, the FAOG adds problem-independent code to the OS implementation. The
challenge is to add code that will work with any LLM-generated code. We found that the
most reliable way of doing it is to extend LLM-generated classes overriding methods as
appropriate. Where this is impossible, the FAOG adds snippets of problem-independent
code directly to the LLM-generated code.
Another lesson we learnt was to use identiﬁers that are clearly distinct from any
common words. For example, we use the class name ‘MyInstance’ instead of ‘Instance’
to avoid ambiguity.
8


--- Page 9 ---
3.4. Instance and solution class generation
Our OS generation process starts by creating a class for storing the instance data.
The instance class is also responsible for reading the instance data from a given ﬁle.
###
Problem
description
###
Consider
a combinatorial optimisation problem
with the
following
input
data
.
<Input data >
A solution
to the
problem
consists
of the
following .
<Solution >
The
constraints
are
as follows .
<Constraints >
The
objective
function
is as follows .
<Objective
function >
###
Instructions ###
Compose
a Python
class
MyInstance
with
exactly
one
method : ’__init__ (self ,
file_path ) ’.
The
__init__
method
should
open the
file
located
at file_path
, read the
instance
data from
the
file and
save it into
instance
variables .
The
file
format
is as
follows .
<Instance
file format >
Reply
only
with the
code of MyInstance .
Include
all
the
necessary
import
statements .
Do not
include
examples .
Figure 3: Instance class prompt.
The prompt for the code of the instance class is given in Figure 3. Once the LLM
composes the instance class, we test it and respond to the LLM with the problem de-
scription if the test fails (see Section 3.6 for details). We allow at most two attempts to
correct any mistakes.
The solution class is responsible for creating a new random solution, storing solution
data, calculating the objective value, and saving the solution to a ﬁle. For testing pur-
poses, it should also be able to verify that the solution is feasible and load a solution
from a ﬁle.
Our prompt for solution class is given in Figure 4. If the solution class fails its testing,
we allow at most two attempts to correct mistakes.
3.5. Algorithm generation
We designed one algorithm generator for each of the three approaches discussed in
Section 2.4.
Monolithic algorithm generator.. The LLM composes the entire algorithm class. The
prompt may instruct the LLM to use a speciﬁc algorithmic approach such as the simulated
annealing or iterated local search, see Figure 5. This generator does not support oﬄine
training.
If the composed algorithm class fails the testing, we allow up to four attempts to
correct the mistakes.
9


--- Page 10 ---
Produce
a Python
class
MySolution
with the
following
methods :
1.
__init__ (self , inst: MyInstance ) that
does the
following :
- Saves
the
parameter
’inst ’ into an instance
variable
’problem_instance ’.
- Composes
a random
solution
to the
problem
specified
by ’inst ’.
The
solution
has to
satisfy
all
the
problem
constraints .
- Saves
the
composed
solution
into
instance
variables .
2.
is_feasible (self) -> bool
that
returns
True if the
solution
satisfies
all
the
problem
constraints
and
False
otherwise .
If the
solution
breaks
some
constraint , the
method
should
also
print
an error
message
describing
how
exactly
a constraint
was
broken .
3.
get_objective(self) that
calculates
the
objective
value
of the
solution
and
returns
it.
Assume
that
the
solution
satisfies
all
the
constraints .
4.
save_to_file(self , output_filename: str) that
creates
a file ’
output_filename ’ and
saves
the
solution
to it.
The
output
file
format
is
as follows .
<Solution
file format >
5.
load_from_file(self , input_filename: str ) that
opens
the
file ’
input_filename ’ and
loads
the
solution
from it.
It should
save the
loaded
solution
in the
current
object .
The
file
format
is the
same as
described
in point
4.
Reply
only
with the
code of MySolution .
Include
all
the
necessary
import
statements .
Do not
include
examples .
Figure 4: Solution class prompt.
Reduction-based algorithm generator.. The LLM composes the entire algorithm class,
however it is instructed to use an external mixed integer programming solver to solve the
problem, see Figure 5. Speciﬁcally, we chose to use the Gurobi mixed integer program-
ming solver because it is widely used in the optimisation literature and has a convenient
Python API. This generator does not support oﬄine training, however it produces adap-
tive algorithms considering that Gurobi includes adaptive search strategies.
If the composed algorithm class fails the testing, we allow up to four attempts to
correct the mistakes.
Component-based algorithm generator.. The component-based approach requires a problem-
independent framework to assemble an optimisation algorithm from several components.
We chose the Conditional Markov Chain Search (CMCS) as a framework designed specif-
ically for the automation of metaheuristic generation [15]. The key strengths of CMCS
are its simplicity, low overheads (making it a good choice if the algorithmic components
take little time to run) and ﬂexibility (for example, it can model several standard meta-
heuristics).
CMCS works as follows. Let H = {H1, H2, . . . , Hk} be a set of algorithmic compo-
nents. Each component is seen by the framework as a black box that takes a solution
as the input and modiﬁes it according to the internal logic; the only requirement is that
the component maintains the feasibility of the solution. The search starts with a random
solution. At each iteration, CMCS selects the next component, applies it to the current
solution and replaces the current solution with the updated one. Additionally, CMCS
10


--- Page 11 ---
Compose
a Python
class
MyAlgorithm
with the
following
methods :
1.
__init__ (self).
The
method
should
not do
anything .
2. solve (self , instance : MyInstance , time_budget_ms: int) -> MySolution .
The
method
should
find and
return a heuristic
solution
to the
problem
instance
specified
in the
parameter
’instance ’.
The
solution
process
should
be
terminated
after
’time_budget_ms ’ milliseconds time.
Use
<
approach > approach .
Reply
only
with the
code of MyAlgorithm .
Include
all
the
necessary
import
statements .
Do not
include
examples .
Figure 5: Monolithic algorithm class prompt. The value of ‘approach’ deﬁnes the type of the algorithm
that the LLM is expected to generate.
Compose
a Python
class
MyAlgorithm
with the
following
methods :
1.
__init__ (self).
The
method
should
not do
anything .
2. solve (self , instance : MyInstance , time_budget_ms: int) -> MySolution .
The
method
should
encode
the
problem
as a mixed
integer
programming
program
and
solve
it using
the
Gurobi
solver .
It should
then
create
an
instance
of class
MySolution
and
populate
it with
the
solution
found
by Gurobi , even
if
Gurobi
did
not
find an
optimal
solution .
The
solution
process
should
be
terminated
after
’time_budget_ms ’ milliseconds time.
If no solution
is
found
within
the
time budget , return a random
solution .
Reply
only
with the
code of MyAlgorithm .
Include
all
the
necessary
import
statements .
Do not
include
examples .
Figure 6: MIP-based algorithm class prompt.
keeps track of the best solution found so far.
The intelligence of CMCS lies in its logic for selecting the next component at each
iteration. The decision is based on two pieces of information: the last executed com-
ponent and whether it improved the solution. Thus, the entire component logic can be
encoded with two transition matrices: one for the ‘success’ of the last component and
one for its ‘failure’. These two matrices along with the component set form the CMCS
conﬁguration.
The components for the CMCS conﬁguration are selected from a pool of algorithmic
components. To build the component pool, the FAOG requests the LLM to compose
several mutation components, i.e. algorithmic components that apply random modiﬁca-
tions to the given solution. Mutation components are particularly easy to implement,
hence our expectation is that an LLM will be capable of reliably composing a diverse set
of mutation components.
Our prompting algorithm repeats the following steps until the number of mutation
classes reaches 2, or the total number of attempts exceeds 10, whichever comes ﬁrst:
1. Prompt the mutation class code (see Figure 7). Unless this is a prompt for the
ﬁrst mutation class, include the statement requesting the logic of the class to be
diﬀerent to the logic of the previous mutation classes.
11


--- Page 12 ---
2. If the newly generated mutation class passes the testing, add it to the set of suc-
cessful mutation classes. Otherwise, request a new implementation specifying the
mistake. After two unsuccessful attempts to repair the current mutation class, move
on to the next mutation class (for example, if class MyMutation1 could not pass the
testing after two attempts to repair it, prompt a new mutation class MyMutation2).
Compose
Python
class
MyMutation <index > with
the
following
methods :
1.
__init__ (self).
The
method
should
not do
anything .
2. apply (self , cur_solution: MySolution ) -> None.
Assume
that ’
cur_solution ’ satisfies
all
the
problem
constraints .
The
method
should
apply a random
change
to the
’cur_solution ’ object
such
that ’cur_solution ’
still
satisfies
all
the
problem
constraints .
Do not
use
the
is_feasible ()
method .
The
logic of
MyMutation <index > should
be
different
to the
logic
of <list of
previously
generated
mutation
classes >.
Reply
only
with the
code of MyMutation <index >.
Include
all
the
necessary
import
statements .
Do not
include
examples .
Figure 7: Mutation class prompt. ‘Index’ is the index of the mutation class that is being generated.
The solution and mutation classes give us functionality to compose random solutions,
clone solutions and randomly modify them using the mutations. We use this functionality
to compose several additional components for the component pool:
Strong mutation(n):
apply a mutation component n times. For each mutation com-
ponent, our FAOG adds one strong mutation: n = 3.
Hill-climber(n):
apply a mutation component and keep the new solution if it is better
than the original one; otherwise backtrack to the original one. This process can
be repeated n times. For each mutation component, our FAOG adds three hill-
climbers: n = 10, n = 100, and n = 1000.
Ruin & Recreate:
replace the current solution with a new random solution.
Having a pool of algorithmic components, we apply oﬄine training to obtain an eﬃ-
cient CMCS conﬁguration. This involves selecting a subset of components and optimising
the transition matrices. We follow the training process proposed in [16]. Speciﬁcally, we
restrict our CMCS to 2-component deterministic conﬁgurations, i.e. conﬁgurations where
each row of each transition matrix includes exactly one non-zero element. We enumerate
all the ‘meaningful’ 2-component deterministic conﬁgurations and test each of them on
a set of training instances. For each training instance, we rank the conﬁgurations based
on the objective value, and then use the total rank of a conﬁguration as a measure of
its quality. The training process returns the conﬁguration with the best total rank. For
training, we randomly select ﬁve instances from the set of instances provided with the
problem description.
12


--- Page 13 ---
3.6. Validation and error correction
LLMs are infamous for the so-called hallucination – the phenomenon of returning of
incorrect results [17]. In our context, hallucinations can lead to issues of several types:
1. The LLM response has unexpected format (for example, it does not include any
code).
2. The LLM returns code that contains compile-time errors.
3. The LLM-generated code does not support the expected programming interface;
for example, a function signature is incorrect.
4. The LLM-generated code produces run-time errors.
5. The LLM-generated code produces infeasible solutions.
6. The LLM generates code that may never or practically never terminate (for ex-
ample, the code produces random solutions until a feasible one is found but the
probability of randomly creating a feasible solution is low, or the code includes a
user prompt), or it does not respect the speciﬁed time budget.
7. The LLM-generated code produces feasible solutions within the given time budget
but it is ‘unreasonable’. For example, the algorithm is restricted to exploring only
a small subset of the search space, or it always returns a random solution.
Issues 1–3 can often be avoided by careful prompt engineering and automated static
testing: following each prompt, we automatically compile the code and check that it
implements the expected classes and functions.
Issues 4–6 require dynamic testing. Assuming the sample instances and solutions are
provided by the user, we can run the following tests: reading instance ﬁles, reading solu-
tion ﬁles, correctly testing the feasibility of the provided solutions, correctly calculating
the objective values of the provided solutions, running the algorithm and ensuring that
it terminates within the speciﬁed time budget, testing feasibility of the newly produced
solutions, and saving new solutions to ﬁles and reading them back.
Each time new code is produced by the LLM, the FAOG runs all available tests. If
any test fails, the FAOG attempts to ﬁx the mistake instead of simply restarting the
generation process; this increases the probability of a successful OS generation within
a ﬁxed number of attempts. Speciﬁcally, it communicates the details of the mistakes
to the LLM and prompts it to re-generate the last piece of code. For example, if an
exception is caught, it includes the information about the test that was running (e.g.,
‘Failed to create an instance of MyInstance.’), the type of the exception, the text of the
exception and the line where it occurred (the actual content of the line so that the LLM
could relate to it). If several attempts to ﬁx a mistake fail, we exploit the stochasticity
of LLMs and restart the generation process from scratch. If three attempts to generate
the OS fail, we assume that the FAOG failed.
4. Library of problem descriptions
To evaluate the performance of our proof-of-concept FAOGs, we developed a library
of problem descriptions, each provided with a set of benchmark instances and example
solutions.
While the approach we chose for selecting optimisation problems and composing their
descriptions is not systematic, we followed several principles to make the conclusions of
this experimental research practically useful to a certain degree:
13


--- Page 14 ---
• Since a FAOG user is unlikely to have a computer science background, we avoid
mentions of the standard problem names.
• We aimed to include descriptions in various styles; some descriptions include math-
ematical notations while others are less formal.
• The problem descriptions are unambiguous.
• The descriptions of the ﬁle formats are technically precise.
We recognise that our problem descriptions might not fully resemble such descriptions
produced by users without expertise in optimisation and modelling. In future, we would
like to obtain problem descriptions from the target audience representatives to support
future research on automated OS generation.
Our library of problem descriptions consists of four combinatorial optimisation prob-
lems:
1. Travelling Salesman Problem (TSP) – widely used as a benchmark combinatorial
optimisation problem. Our description of the problem includes mathematical no-
tations and explicit formulas.
We developed a simple ﬁle format for the instance data; in our format, the cost
matrix is provided explicitly.
For the benchmark instances, we selected all the TSPLIB [18] instances of sizes up
to n = 500. For all those instances, the optimal solutions are known.
2. Generalised Travelling Salesman Problem (GTSP) – a famous generalisation of
TSP. Compared to TSP, GTSP is more complex in that it requires additional
input data, and the solutions have additional constraints.
We used the text-based ﬁle format from the GTSP Instance Library [19].
For the benchmark instances, we selected all GTSP Instance Library instances of
sizes up to n = 500. For all those instances, the optimal solutions are known [20].
3. Assignment Problem (AP) – a polynomially solvable problem with a wide range
of applications. While it may seem like a bad choice for testing a system that
generates heuristic algorithms, users may not be aware of AP being polynomially
solvable, hence we argue that this choice is valid. Additionally, the AP is supposed
to be solved eﬃciently by an MIP-based algorithm; our experiments will test if our
FAOG is capable of exploiting this problem property.
We developed a simple text-based ﬁle format for the instance data.
We generated 10 pseudo-random instances of sizes 10, 20, . . ., 100. Each weight is
independently drawn at random from {0, 1, . . ., 99} with a uniform distribution.
4. Exam Timetabling Problem (ETP) – a relatively complex problem with constraints
and objective function slightly diﬀerent to what is typically used in the literature.
Our aim is to test our system on a problem that LLMs have not seen before and that
has a relatively complex structure. Speciﬁcally, we ask for an assignment of exams
to time slots such that the minimum distance (in time slots) between exams for any
student is maximised. Since this is a maximisation problem and our system only
supports minimisation problems, our description of the objective function includes
multiplying the minimum distance by −1.
We generated 10 pseudo-random instances. To generate instance i ∈{1, 2, . . ., 10},
we randomly choose the number of exams n from {5, 6, . . ., 5i + 5}, the number of
14


--- Page 15 ---
Name
Description of the algorithm generator
Free
Monolithic algorithm generator; the choice of the algorithmic approach is left to the LLM.
SA
Monolithic algorithm generator based on the simulated annealing metaheuristic, i.e. the
algorithm generation prompt instructs the LLM to produce a simulated annealing algorithm.
TS
Monolithic algorithm generator based on the tabu search metaheuristic.
ILS
Monolithic algorithm generator based on the iterated local search metaheuristic.
MIP
Reduction-based algorithm generator; the algorithm generation prompt instructs the LLM
to use the Gurobi mixed integer programming solver to model and solve the problem.
CMCS
Component-based algorithm generator based on the Conditional Markov Chain Search.
Table 2: List of FAOGs tested in this study.
time slots from {n, n+1, . . ., 2n} and the number of students from {5, 6, . . ., 5i+5}.
Then, for each student, we choose the number of exams they take from {2, 3, 4}
and then select those exams randomly. All the distributions are uniform.
The detailed problem descriptions used as the FAOG inputs can be downloaded from
https://people.cs.nott.ac.uk/pszdk/problems_library.zip.
5. Experiments
To test our ideas, we conducted a series of experiments with our proof-of-concept
FAOGs, see Section 3. All our FAOGs share the parts that compose the data structures
and input/output routines but diﬀer in algorithm generation. The list of FAOGs we
experimented with is given in Table 2.
We used three LLMs in our experiments:
• ChatGPT 3.5 by OpenAI – perhaps the most widely used LLM at the moment of
writing the paper.
• Gemini 1.5 Pro by Google.
• Llama 3.1 (405B) by Meta – one of the most widely used free LLMs.
Each FAOG was tested with all three LLMs, thus 18 OSs were produced in total. We
refer to the OS generated by a FAOG x as ‘x OS’, for example, CMCS OS is the OS
generated by the CMCS FAOG.
Table 3 shows the performance of the auto-generated OSs. Each value is the gap to
the best-known solution averaged over all the test instances:
gap = 1
n
n
X
i=1
fi −bi
bi
· 100%,
(1)
where n is the number of test instances, fi is the objective value of the solution obtained
by the OS for instance i, and bi is the objective value of the best-known solution for
instance i. The optimal solutions for TSP, AP and GTSP are known; for ETP, we use
15


--- Page 16 ---
LLM
Problem
Free
SA
TS
ILR
MIP
CMCS
ChatGPT
TSP
1156.7%
102.0%
327.5%
451.6%
389.0%
16.9%
AP
131.3%
110.3%
68.6%
136.0%
0.0%
85.1%
GTSP
342.4%
327.1%
327.4%
340.5%
–
29.1%
ETP
93.8%
100.0%
84.0%
100.0%
96.0%
30.8%
Gemeni
TSP
–
(0%)
–
–
–
25.2%
AP
(0%)
1292.9%
1308.9%
152.2%
0.0%
63.1%
GTSP
–
60.3%
–
–
(50%)
–
ETP
100.0%
10.6%
100.0%
–
100.0%
32.8%
Llama
TSP
1189.7%
115.4%
735.9%
(86%)
(32%)
11.5%
AP
133.5%
110.6%
112.1%
62.0%
0.0%
75.2%
GTSP
60.7%
58.5%
54.0%
61.3%
–
11.6%
ETP
100.0%
19.9%
81.6%
58.0%
100.0%
33.6%
Table 3: Performance of the auto-generated OSs.
the best solution ever found by any of our algorithms during this study. The time budget
of each OS is 100 seconds per instance.
If an OS failed to solve some test instances within the given time budget, we report
the percentage of the instances that were solved in brackets; for example, ‘(86%)’ means
that the OS failed in 14% of the tests. If a FAOG failed to produce an OS, we use dash.
Based on the ChatGPT results in Table 3, we conclude that most of the FAOGs
reliably produce working OSs for every problem in the library. The only exception is the
MIP FAOG that failed to produce a working OS for GTSP.
The quality of solutions signiﬁcantly depends on the FAOG used. Letting the LLM
choose the solution approach yields relatively bad results; the Free OSs showed poor
performance across all the experiments. Other monolithic generators (SA, TS and ILS)
produced more eﬀective OSs but it is hard to identify the winning approach based on
our experiments.
The MIP OS predictably demonstrated excellent performance on AP; every LLM
correctly formulated the problem, and Gurobi quickly solved every instance of AP to
optimality. However, the MIP FAOG was less successful on the other problems. In many
cases, the MIP FAOG failed to produce a working OS; in other cases, the Gurobi-based
solver demonstrated poor results. Indeed, straightforward MIP formulations of many
problems including TSP are known to be ineﬃcient.
The CMCS OSs demonstrated the most consistent performance across all the exper-
iments, particularly on the NP-hard problems (TSP, GTSP and ETP).
To understand how the solution time budget aﬀects the quality of solutions, we con-
ducted four experiments for each OS and problem instance, with time budgets 0.1 sec,
1 sec, 10 sec and 100 sec, respectively. The LLM used in these experiments was Chat-
GPT. The results are reported in Figure 8. Note that most of the auto-generated OSs
are stochastic, meaning that the results of these experiments are noisy; an increased time
budget can, in rare cases, give an increased solution gap.
The CMCS OSs clearly dominate all the other OSs across all the time budgets on
the NP-hard problems. The CMCS OS is also competitive on AP compared to the other
metaheuristics. The MIP OS solved all the AP instances to optimality even within the
16


--- Page 17 ---
101
102
103
Gap, %
TSP
102
103
All MIP
solutions
were optimal
AP
10−1
100
101
102
102
MIP failed
OS generation
Time, sec
Gap, %
GTSP
10−1
100
101
102
101.6
101.8
102
Time, sec
ETP
Free
SA
TS
ILS
MIP
CMCS
Figure 8: Solution gap (1) vs time budget per instance. The FAOGs used in this experiment are based
on ChatGPT.
smallest time budget (the line is not visible because the y-axis is logarithmic). However,
MIP OS required a considerable time budget to produce any solutions for TSP, and the
MIP FAOG failed the OS generation on GTSP.
Note that the only OSs in this paper that are capable of utilising multiple CPU cores
are the MIP OSs; hypothetically, an LLM can produce parallelised code for any other
FAOG but we have not observed such behaviours and do not believe that current LLMs
are capable of reliably composing eﬀective parallelised optimisation algorithms (partly
because such algorithms are not common in the literature and hence the LLMs were not
trained to produce them). We also note that the multi-component approach makes it
relatively easy to use concurrency even if the code produced by the LLM is sequential.
We also analysed how the performance of the generated OSs depends on the LLM used
in the FAOG, see Figure 9. We chose CMCS as the highest-performing FAOG for this
experiment. It is diﬃcult to identify the winning LLM based on our limited results; with
the exception of GTSP (arguably, the most sophisticated problem in our library), all the
OSs performed similarly. Our hypothesis is that CMCS is adaptive enough to perform
well with a wide range of components; the oﬄine learning mechanism compensates for
the variability of the component quality.
An important observation is that the quality of solutions produced by the auto-
generated OSs signiﬁcantly improves with the time budget. We conclude that the per-
formance achieved by the CMCS FAOG might be suﬃcient for relatively small and simple
problems. We can also expect that further development of the generator and the use of
more advanced LLMs will increase the success rate of the process and the performance
17


--- Page 18 ---
101
102
103
Gap, %
TSP
102
102.5
AP
10−1
100
101
102
101
101.5
Time, sec
Gap, %
GTSP
10−1
100
101
102
101.6
101.8
Time, sec
ETP
ChatGPT
Gemini
LLama
Figure 9: How the performance of the auto-generated CMCS depends on the LLM used.
of the OSs.
It is also important to discuss the running time of the FAOG.
For most of the
approaches, OS generation consists of several LLM prompts and validation. As a rule of
thumb, the entire process takes in the order of one minute. However, the CMCS generator
also requires oﬄine training that can take considerable time. For our experiments, we
restricted each run of CMCS OS during training to 1 second, and the set of instances
used for training was limited to 5 instances. In case of a two-component deterministic
CMCS, there are around 300 ‘meaningful’ conﬁgurations, thus the training takes around
25 CPU minutes. Considering that CMCS training can be easily parallelised, this is a
modest increase in the OS generation time. However, training a less restricted CMCS
could take much longer (hours or days) which may become a limiting factor in certain
applications.
6. Conclusions
We explored the feasibility of full automation of combinatorial OS generation by
utilising LLMs. The input for such a generator is a description of a problem in a natural
language and, optionally, a set of training instances. The output is an OS, i.e. a software
system that takes a problem instance data as input and outputs an eﬃcient solution to
this problem. Such a generator will make decision support accessible to individuals and
organisations that cannot currently aﬀord it due to the cost of building OSs.
18


--- Page 19 ---
Within the study, we identiﬁed possible approaches to building FAOGs, developed
proof-of-concept generators of several types and conducted a set of computational exper-
iments to evaluate their performance. For the computational experiments, we composed
a library of problem descriptions and made it publicly available for future research.We
plan to maintain the library extending it with new problem descriptions, including de-
scriptions produced by practitioners without expertise in optimisation.
Our experiments demonstrated that a FAOG utilising a modern LLM is capable of
reliably producing OSs based on problem descriptions in English. Among our proof-
of-concept FAOGs, the most successful one uses the component-based approach with
oﬄine training.
In this approach, the LLM is responsible for interpreting the prob-
lem description and producing simple problem-speciﬁc algorithmic components whereas
the problem-independent framework is responsible for building an eﬃcient metaheuristic
from those components. Additionally, we found that the polynomially solvable problem
from our library can be eﬃciently tackled with the reduction-based FAOG that utilises
an MIP solver. However, the reduction-based FAOG was less reliable and/or eﬃcient on
the other problems often producing incorrect reduction procedures and causing the OS
to fail the automated testing.
Observe that the most successful FAOG is the only generator we tested that utilises
oﬄine training. We hypothesise that oﬄine training is crucial for generating eﬃcient OS.
Note that problem descriptions usually do not include information about the instances
that are expected to be solved in practice. Such information is usually implicitly con-
tained in the benchmark/training instances but may be unknown to the user. Examples
of such information are the number of isolated nodes in a graph, the size of the largest
clique, the distribution of node degrees, etc. An algorithm eﬀective on one set of in-
stances may not be eﬀective on another set, hence training is important. Traditionally,
this training is conducted by human experts during the design and testing of algorithms.
To replicate this process, a FAOG needs to implement formal oﬄine training.
Another argument for requesting the user to provide training instances along with
the problem description is the validation of the code produced by the LLM. Without
such tests, there is a high chance of producing a disfunctional OS that, either, termi-
nates without returning a solution or may return infeasible solutions. For most of the
applications, it is preferable to catch such issues before system deployment, i.e. during
the generation process.
We also observe that the reduction-based and component-based FAOGs delegate more
tasks to specialised AI compared to the monolithic generators; we can expect a specialised
AI to perform better, thus giving an edge to these approaches.
In our experiments, the performance of the auto-generated OSs for the NP-hard
problems was low compared to the state-of-the-art algorithms found in the literature;
modestly sized instances required considerable time budgets to ﬁnd solutions of accept-
able quality. However, we argue that even the OSs generated by our proof-of-concept
FAOGs might be suﬃcient for some applications; considering the motivation for this
study, it is more appropriate to compare the auto-generated OSs to human operators
solving the problem. We also believe that further development of the FAOGs will lead
to signiﬁcant performance improvements.
19


--- Page 20 ---
6.1. Future work
While our experiments demonstrated an acceptable success rate of the FAOGs, fur-
ther improvements will enable generation of OSs for more complex problems. Future
research will identify more eﬀective LLM prompting, code validation, and error cor-
rection.
For example, our implementation uses a single ‘conversation’ with an LLM;
every prompt includes the entire conversation history (unless a complete restart is per-
formed). A potentially more eﬃcient approach will break up this conversation into mul-
tiple conversations, with the relevant code fragments embedded into the prompt. Also,
some classes such as MySolution can be further divided. For example, the algorithm
constructing a random solution can be separated from the MySolution class. Finally,
prompt engineering techniques such as chain-of-thought may improve the quality of the
LLM responses [21, 22, 23].
No matter how advanced a FAOG is, it needs a precise description of the problem
to perform well. Since some users will not have expertise in optimisation, they might
need assistance in composing the problem description. The OS generation process can be
made interactive, with the FAOG providing feedback about the problem description (for
example, highlighting ambiguities) and requesting clariﬁcations. It can also summarise,
in a natural language, the algorithmic ideas it produced to let the user suggest new
algorithmic ideas or identify inaccuracies in the problem description.
With regards to the performance of the auto-generated OSs, the following are a few
areas where, we believe, it is possible to get signiﬁcant gains:
• A mutation component often takes O(1) time, however the CMCS framework re-
calculates the objective value each time a mutation (or any other component) is
applied. For a problem of size n, calculation of the objective value usually takes
O(n) time; as a result, solution evaluations could consume most of the time budget.
State-of-the-art algorithms often employ incremental evaluation; a component does
not only modify the solution but also evaluates how this modiﬁcation aﬀects the
objective value; this often takes only O(1) time. Incremental evaluation will require
an adjustment to current programming interfaces, more complex prompts, and
additional automated tests.
• At the moment, the CMCS FAOG prompts the LLM to compose random construc-
tion heuristic and mutation components whereas hill-climbers are later produced
based on the mutations. While the current approach is suﬃcient to build an OS,
the LLM might be able to compose more eﬃcient hill-climbers.
• The auto-generated OSs can utilise all the available CPU cores to further improve
the performance. This can be achieved by running one instance of the algorithm
on each CPU core, or a more sophisticated strategy such as a memetic algorithm
can be employed.
• We found that the monolithic algorithm generator often produced reasonable algo-
rithms which, however performed poorly due to the lack of parameter tuning. It is
possible to enhance the monolithic FAOG by introducing oﬄine parameter tuning.
Finally, a more advanced CMCS training procedure will allow ﬁnding more complex
and eﬀective CMCS conﬁgurations.
20


--- Page 21 ---
References
[1] M. Chouki, M. Talea, C. Okar, R. Chroqui, Barriers to information technology adoption within
small and medium enterprises: A systematic literature review, International Journal of Innovation
and Technology Management 17 (1) (2020).
[2] D.-Z. Du, P. Pardalos, X. Hu, W. Wu, Introduction to Combinatorial Optimization, Springer In-
ternational Publishing, Cham, 2022.
[3] E. Burke, M. Gendreau, M. Hyde, G. Kendall, G. Ocha, E. ¨Ozcan, R. Qu, Hyper-heuristics: a
survey of the state of the art, Journal of the Operational Research Society 64 (2013).
[4] P.-F. Guo, Y.-H. Chen, Y.-D. Tsai, S.-D. Lin, Towards optimizing with large language models
(2024). arXiv:2310.05204.
[5] C. Yang, X. Wang, Y. Lu, H. Liu, Q. V. Le, D. Zhou, X. Chen, Large language models as optimizers
(2024). arXiv:2309.03409.
[6] Y. Chen, Y. Li, B. Ding, J. Zhou, On the design and analysis of LLM-based algorithms (2024).
arXiv:2407.14788.
[7] Z. Wang, S. Liu, J. Chen, K. C. Tan, Large language model-aided evolutionary search for constrained
multiobjective optimization, in: Advanced Intelligent Computing Technology and Applications:
20th International Conference, ICIC 2024, Tianjin, China, August 5–8, 2024, Proceedings, Part II,
Springer-Verlag, Berlin, Heidelberg, 2024, p. 218–230.
[8] M. Pluhacek, A. Kazikova, T. Kadavy, A. Viktorin, R. Senkerik, Leveraging large language models
for the generation of novel metaheuristic optimization algorithms, in: Proceedings of the Companion
Conference on Genetic and Evolutionary Computation, GECCO ’23 Companion, Association for
Computing Machinery, New York, NY, USA, 2023, p. 1812–1820.
[9] R. Zhong, Y. Xu, C. Zhang, J. Yu, Leveraging large language model to generate a novel metaheuris-
tic algorithm with crispe framework, Cluster Computing 27 (10) (2024) 13835–13869.
[10] A. AhmadiTeshnizi, W. Gao, M. Udell, Optimus: Optimization modeling using mip solvers and
large language models (2023). arXiv:2310.06116.
[11] M. Brcic, R. V. Yampolskiy, Impossibility results in AI: A survey, ACM Comput. Surv. 56 (1) (Aug.
2023).
[12] Y. Hamadi, ´E. Monfroy, F. Saubion (Eds.), Autonomous Search, Springer, 2012.
[13] A. Leidinger, R. van Rooij, E. Shutova, The language of prompting: What linguistic properties
make a prompt successful? (2023). arXiv:2311.01967.
[14] M. Sclar, Y. Choi, Y. Tsvetkov, A. Suhr, Quantifying language models’ sensitivity to spurious
features in prompt design or: How i learned to start worrying about prompt formatting (2024).
arXiv:2310.11324.
[15] D. Karapetyan, A. P. Punnen, A. J. Parkes, Markov chain methods for the bipartite boolean
quadratic programming problem, European Journal of Operational Research 260 (2) (2017) 494–
506.
[16] D. Karapetyan, B. Goldengorin, Conditional markov chain search for the simple plant location
problem improves upper bounds on twelve korkel-ghosh instances, in: B. Goldengorin (Ed.), Opti-
mization Problems in Graph Theory, Springer, 2018, pp. 123–147.
[17] S. Farquhar, J. Kossen, L. Kuhn, Y. Gal, Detecting hallucinations in large language models using
semantic entropy, Nature 630 (8017) (2024) 625–630.
[18] G. Reinelt, TSPLIB – a traveling salesman problem library., INFORMS J. Comput. 3 (4) (1991)
376–384.
[19] G. Gutin, D. Karapetyan, A memetic algorithm for the generalized traveling salesman problem,
Natural Computing: An International Journal 9 (1) (2010) 47–60.
[20] M. Fischetti, J. J. S. Gonz´alez, P. Toth, A branch-and-cut algorithm for the symmetric generalized
traveling salesman problem, Operations Research 45 (3) (1997) 378–394.
[21] J. L. Espejel, M. S. Y. Alassan, M. Bouhandi, W. Dahhane, E. H. Ettifouri, Low-cost language
models: Survey and performance evaluation on python code generation, arXiv preprint (2024).
arXiv:2404.11160.
[22] L. Murr, M. Grainger, D. Gao, Testing llms on code generation with varying levels of prompt
speciﬁcity, arXiv preprint (2023). arXiv:2311.07599.
[23] P. Sahoo, A. K. Singh, S. Saha, V. Jain, S. Mondal, A. Chadha, A systematic survey of prompt
engineering in large language models: Techniques and applications (2024). arXiv:2402.07927.
21
