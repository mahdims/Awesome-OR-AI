--- Page 1 ---
Time-varying Mixing Matrix Design for
Energy-efﬁcient Decentralized Federated Learning
Xusheng Zhang∗, Tuan Nguyen†, and Ting He†
∗University of Oxford, Oxford, England, UK. Email: xusheng.zhang@cs.ox.ac.uk
† Pennsylvania State University, University Park, PA, USA. Email: {tmn5319,tinghe}@psu.edu
Abstract—We consider the design of mixing matrices to min-
imize the operation cost for decentralized federated learning
(DFL) in wireless networks, with focus on minimizing the maxi-
mum per-node energy consumption. As a critical hyperparameter
for DFL, the mixing matrix controls both the convergence
rate and the needs of agent-to-agent communications, and has
thus been studied extensively. However, existing designs mostly
focused on minimizing the communication time, leaving open the
minimization of per-node energy consumption that is critical for
energy-constrained devices. This work addresses this gap through
a theoretically-justiﬁed solution for mixing matrix design that
aims at minimizing the maximum per-node energy consumption
until convergence, while taking into account the broadcast nature
of wireless communications. Based on a novel convergence
theorem that allows arbitrarily time-varying mixing matrices,
we propose a multi-phase design framework that activates time-
varying communication topologies under optimized budgets to
trade off the per-iteration energy consumption and the conver-
gence rate while balancing the energy consumption across nodes.
Our evaluations based on real data have validated the efﬁcacy of
the proposed solution in combining the low energy consumption
of sparse mixing matrices and the fast convergence of dense
mixing matrices.
Index Terms—Decentralized federated learning, mixing matrix
design, energy consumption.
I. INTRODUCTION
Decentralized federated learning (DFL) [1] is an emerging
machine learning paradigm that allows distributed learning
agents to collaboratively learn a shared model from the union
of their local data without directly sharing the data. Instead
of coordinating through a parameter server as in centralized
federated learning (FL) [2], the agents participating in DFL
directly exchange model updates with their neighbors through
peer-to-peer communications, which are then aggregated lo-
cally [3]. DFL has attracted signiﬁcant attention since its
introduction, because compared to its centralized counterpart,
DFL has better robustness by avoiding a single point of failure
and better balances the communication loads across nodes
without increasing the computational complexity [1].
Meanwhile, DFL still faces signiﬁcant performance chal-
lenges due to the extensive data transfer between learning
agents. Such challenges are particularly prominent for deep
learning due to the large model size, where communication
cost can dominate the total cost of the learning task [4],
particularly in resource-constrained edge networks [5]. This
Research was partly supported by the National Science Foundation under award CNS-
2106294.
issue has attracted tremendous research interests in reducing
the communication cost of DFL, such as methods for reducing
the amount of data per communication through compression
(e.g., [6]) and methods for reducing the number of commu-
nications through hyperparameter optimization (e.g., [7]) or
adaptive communications (e.g., [8]).
In particular, the design of mixing matrix used for local
parameter aggregation plays a crucial role, as each non-
zero off-diagonal entry in the mixing matrix will trigger an
agent-to-agent communication. In this regard, most existing
works focused on accelerating learning by minimizing the
communication time, either measured in the maximum number
of neighbors an agent communicates with [9], [7], [10], [11]
or the number of time slots for scheduling all the communi-
cations [12]. However, in wireless networks such as HetNets
[5], device-to-device networks [13], and IoT networks [14],
an often more important cost measure is energy consumption,
which has received less attention. Only a few works have tried
to optimize energy consumption during DFL [7], [15], but
they either did not consider the balance of energy consumption
between nodes [7] or did not utilize the broadcast nature of
wireless communications [15].
In this work, we aim at ﬁlling this gap with the objective
of minimizing the maximum energy consumption per node for
DFL to reach a given level of convergence, while taking into
account the broadcast nature of wireless communications. Our
solution is built upon a novel convergence theorem under
time-varying mixing matrices, based on which we propose
a multi-phase mixing matrix design framework together with
concrete algorithms that designs randomized mixing matrices
under optimized per-node budgets to optimally balance the
convergence rate and the energy consumption at each node.
A. Related Works
Decentralized federated learning. First explored by [1]
through Decentralized Parallel Stochastic Gradient Descent
(D-PSGD), DFL removes the central server in [2] and enables
model training over peer-to-peer networks. A central research
question is how DFL performs compared to centralized FL,
particularly in convergence rate, communication cost, and
generalization. Subsequent studies including [16], [17], [18]
advance DFL in both algorithms and theories, though the
main focus remains on reducing the number of iterations
needed for convergence.
arXiv:2512.24069v1  [cs.LG]  30 Dec 2025


--- Page 2 ---
Communication cost reduction. As model sizes continue
to grow, communication overhead has become a key bottleneck
limiting the performance and reliability of DFL on wireless
edge networks [19]. Existing work for reducing communica-
tion cost can be broadly categorized into three approaches.
The ﬁrst is to lower the cost per communication round using
compression techniques [6], [20], [21], [22]. The second is to
reduce the total number of communication rounds [23], [24],
[25], [26]. A third line of work focuses on activating only
selected subsets of links rather than all links simultaneously. In
this direction, event-triggered mechanisms were introduced in
[8], [27], and optimized (possibly randomized) communication
patterns were proposed in [9], [7], [12].
In this work, we aim at designing the communication
patterns as in [9], [7], [12], which has the advantage of pro-
viding predictable performance compared to event-triggered
mechanisms, but we consider a different objective. While
most existing works on communication design focused on
minimizing the communication time, measured by the number
of matchings [9], [7], the maximum degree [10], [11], or
the number of collision-free transmission slots [12], we focus
on minimizing the maximum energy consumption per node,
which is critical for wireless networks [5], [13], [14]. Com-
pared to the communication time, fewer works have tackled
the optimization of energy consumption [7], [15], [28], and
the existing solutions either ignored the balance of energy
consumption across nodes [7] or ignored the broadcast nature
of wireless communications [15]. Some solution [28] even
ignored the energy consumption by communications. This
work aims at ﬁlling this gap by designing the communication
patterns to minimize the maximum energy consumption per
node, while considering broadcast communications. While
a few works considered broadcast-based DFL [29], [12],
they had different optimization objectives (e.g., maximizing
#successful links [29] or minimizing #transmission slots [12]).
Mixing matrix design in DFL. The mixing matrix, i.e., the
matrix containing the local aggregation weights, is an impor-
tant hyperparameter in DFL. The impact of the mixing matrix
on the convergence rate of DFL is usually captured through its
spectral gap [1], [30], [31] or an equivalent parameter that de-
notes the discrepancy between the designed mixing matrix and
the ideal mixing matrix under all-to-all communications [9].
Although recent studies have pointed out additional parameters
that can affect convergence, such as the effective number
of neighbors [32] and the neighborhood heterogeneity [11],
these results do not invalidate the importance of the spectral
gap. Based on the identiﬁed convergence parameters, several
mixing matrix designs have been proposed to balance the
convergence rate and the cost per iteration [9], [7], [12],
[10], [11]. In this regard, our mixing matrix design is also
based on a parameter related to the spectral gap, but we build
on a generalized convergence theorem allowing time-varying
mixing matrices (including random matrices with time-varying
distributions), which enables a multi-phase design.
DFL over time-varying topology. Most gossip-based dis-
tributed optimization algorithms have been analyzed under
ﬁxed network topologies, with only a few works establishing
convergence guarantees for time-varying topologies. [33] stud-
ies D-PSGD with multiple steps of local training and multiple
steps of gossip, which can be viewed as periodically alternat-
ing between the full base topology and an empty topology. [34]
provides the state-of-the-art convergence analysis for D-PSGD
with very general (randomized) mixing matrices, subject only
to a spectral condition in each period. Algorithm subgradient-
push [35] also handles time-varying topologies, but their
convergence analyses rely on convexity of the loss functions
and additional spectral assumptions on the mixing matrices.
Gradient tracking algorithms such as DIGing [36] and Acc-GT
[37] have likewise been studied over time-varying topologies
under convex loss functions. The convergence analysis in [38]
is the most general to date, applicable to a broad class of
algorithms, requiring neither convexity nor strong assumptions
on the topology class; however, it still assumes a uniform lower
bound on the spectral gap of the time-varying topologies, an
assumption not made in this paper. The convergence theorem
presented here focuses on D-PSGD and applies to topologies
with arbitrarily time-varying spectral gaps, covering both
convex and non-convex objectives.
Adaptive communications. Adapting communications dur-
ing training, e.g., via adapting the communicated content
(e.g., adaptive model compression [39], [40], adaptive model
pruning [41], adaptive self-distillation [42], [43], adaptive
parameter freezing [44]) or the act of communication (e.g.,
adapting communication period [45], adapting client selec-
tion [46], adapting topology construction [47], and adapting
both communication period and topology [48]), has been
shown to improve the performance of FL. Among these stud-
ies, only [47], [48] address decentralized federated learning
(DFL), the setting we study here. Their adaptive mechanisms,
however, come with non-trivial computational overhead or
system complexity: [47] integrates real-time deep reinforce-
ment learning at every iteration, while [48] relies on a cen-
tral coordinator to compute complicated adaptive decisions.
Moreover, the resulting adaptations depend on real-time deci-
sions, and thus the behavior would be difﬁcult to predict or
analyze. Our approach moves in a different direction. Based
on a generalized convergence theorem, we can analytically
characterize the impact of ﬁne-grained adaptations of the
act of communication, without requiring real-time algorithmic
decision-making (from a centralized orchestration), and our
method remains compatible with content-level adaptations for
further performance gains.
B. Summary of Contributions
We consider the mixing matrix design for broadcast-based
DFL, with the objective of minimizing the maximum per-node
energy consumption, with the following contributions:
1) Motivated by our initial experiments that suggest the
beneﬁt of varying the communication intensity during training,
we derive a new convergence theorem that allows arbitrarily
time-varying (random) mixing matrices, which generalizes
existing convergence theorems that require mixing matrices
2


--- Page 3 ---
to be ﬁxed [1], i.i.d. [9], B-connected [35], or periodic [34],
[33].
2) Based on the theorem, we propose a multi-phase design
framework as well as a corresponding trilevel optimization
algorithm, and derive explicit expressions of its objective
function in the cases of 1-phase and 2-phase design.
3) We develop a budgeted mixing matrix design algorithm
to solve the lower-level optimization under broadcast commu-
nications, and analytically characterize the convergence rate
under its design in the case of fully connected base topology.
4) We evaluate the proposed solution against baselines and
state-of-the-art benchmarks under realistic settings for learning
in wireless networks. Our results validate the efﬁcacy of the
proposed solution in combining the low energy consumption
of sparse mixing matrices and the fast convergence of dense
mixing matrices to improve the energy efﬁciency of DFL.
Roadmap. Section II states the problem formulation, Sec-
tion III presents our convergence theorem, based on which
Sections IV–V develop the proposed solution, Section VII
presents the performance evaluation, and Section VIII con-
cludes the paper. All proofs are deferred to the appendix.
II. BACKGROUND AND PROBLEM FORMULATION
A. Decentralized Learning Problem
Consider a network of m nodes linked via a base topology
G = (V, E), where V is the set of nodes (|V | = m) and E
indicates which node pairs can exchange information directly.
That is, each node i can only directly communicate with nodes
in its one-hop neighborhood (including itself), denoted by
Vi := {i} ∪{j ∈V : (i, j) ∈E}. Each node i ∈V is
associated with a local objective function Fi(x), which is a
function of the parameter x ∈Rd that depends on the local
dataset at node i. The objective of DFL is to collaboratively
minimize the global function
F(x) := 1
m
m
X
i=1
Fi(x),
(1)
which averages the local objectives across all nodes. We
assume that F attains its minimum value Finf.
B. Decentralized Learning Algorithm
We focus on a well-established decentralized learning
algorithm
known
as
Decentralized
Parallel
Stochastic
Gradient Descent (D-PSGD) [1]. Let x(t)
i
represent the
model parameter at node i at the start of iteration t, and let
g(x(t)
i ; ξ(t)
i ) denote the stochastic gradient computed at that
node using a minibatch ξ(t)
i
drawn from its local data and the
current parameter x(t)
i . At each iteration, D-PSGD performs
the following update in parallel at every node i:
x(t+1)
i
=
m
X
j=1
W (t)[i, j](x(t)
j
−ηg(x(t)
j ; ξ(t)
j )),
(2)
where W (t) = (W (t)[i, j])m
i,j=1 is the m × m mixing matrix
used at iteration t, and η > 0 denotes the learning rate.
Since node i requires communication from node j in iter-
ation t only when W (t)[i, j] ̸= 0, the communication pattern
can be controlled by appropriately designing the mixing matrix
W (t). According to [1], the mixing matrix should be topology-
compliant (W (t)[i, j] ̸= 0 only if (i, j) ∈E) and symmetric
with each row/column summing up to one1 in order to ensure
feasibility and convergence for D-PSGD.
In general, the mixing matrix W (t) can be a random matrix
drawn from a distribution W(t) as long as all its realizations
satisfy our desired properties, i.e., being topology-compliant
and symmetric with row/column sums of 1. In this case, by
IE[φ(W (t))] we denote the expectation of a function φ of
W (t), where W (t) is drawn from its distribution W(t).
Let x(t) :=
1
m
Pm
i=1 x(t)
i
denote the learned global model at
iteration t (which is a global average of the local models at this
iteration). Depending on the shape of the objective function
F, there are two widely adopted convergence criteria: for any
required level of convergence ǫ > 0, we say that D-PSGD has
achieved ǫ-convergence if
•
1
T
PT −1
t=0 (IE[F(x(t))]−Finf) ≤ǫ when the local objective
functions Fi are convex, or
•
1
T
PT −1
t=0 IE[∥∇F(x(t))∥2] ≤ǫ for general (possibly non-
convex) Fi2.
The convergence of D-PSGD can be guaranteed under the
following assumptions:
(1) Each local objective function Fi(x) is L-Lipschitz smooth,
i.e., for all x, x′ ∈Rd, ∥∇Fi(x) −∇Fi(x′)∥≤L∥x −
x′∥, ∀i ∈V .
(2) There
exist
constants
M1, ˆσ
such
that
1
m
P
i∈V IE[∥g(xi; ξi)
−
∇Fi(xi)∥2]
≤
ˆσ2
+
M1
m
P
i∈V ∥∇F(xi)∥2, ∀x1, . . . , xm ∈Rd.
(3) There
exist
constants
M2, ˆζ
such
that
1
m
P
i∈V ∥∇Fi(x)∥2 ≤ˆζ2 + M2∥∇F(x)∥2, ∀x ∈Rd.
While the convergence of D-PSGD has also been proved under
other assumptions [1], [9], the above assumptions, originally
from [34], are more general; see [34] for explanations.
C. Cost Models
We focus on energy consumption as the cost measure.
Speciﬁcally, we use ci(W (t)) to denote the energy consump-
tion at node i in an iteration when the mixing matrix is
W (t), which contains two parts: (i) computation energy ca
i
for computing the local stochastic gradient and the local
aggregation, and (ii) communication energy for sending the
parameter vector at node i to each of its activated neighbors.
Under broadcast communications where node i can send its
parameter vector to all its neighbors through a broadcast
transmission, we model the per-iteration energy consumption
at node i as
ci(W (t)) := ca
i + cb
i
1(∃(i, j) ∈E : W (t)[i, j] ̸= 0),
(3)
where cb
i is the energy consumption per transmission, and
1(·)
denotes the indicator function. We say that node i is activated
1In [1], the mixing matrix was assumed to be symmetric and doubly
stochastic with entries constrained to [0, 1], but we ﬁnd that all the con-
vergence proofs only required the mixing matrix to be symmetric with each
row/column summing up to one.
2In this work, we use ∥a∥to denote ℓ-2 norm if a is a vector, and spectral
norm if a is a matrix.
3


--- Page 4 ---
0
200
400
600
800
1000
1200
1400
Max-energy per node (mWh)
0.000001
0.00001
0.0001
0.001
 0.01
T
raining Losses
0
200
400
600
800
1000
1200
1400
Max-energy per node (mWh)
0.01
0.05
0.1
0.6
T
es ing Error
0
50
100
150
200
250
300
Epochs
0.000001
0.00001
0.0001
0.001
 0.01
T
raining Losses
0
50
100
150
200
250
300
Epochs
0.01
0.05
0.1
0.6
T
es ing Error
100% ac iva ion
0% ac iva ion
0%  o 100% ac iva ion
Fig. 1.
Motivating experiment based on MNIST over a 100-node clique
(x-axis truncated to show the same range for all curves).
in iteration t if
1(∃(i, j) ∈E : W (t)[i, j] ̸= 0) = 1. The
total energy consumption at node i over T iterations is thus
PT
t=1 ci(W (t)).
An alternative cost model under unicast communications has
been studied in [7], [15], where node i needs to separately
transmit its parameter vector over each activated link (i, j)
with energy consumption cb
ij. In this setting, the energy
consumption at node i in iteration t becomes
ci(W (t)) := ca
i +
X
j:(i,j)∈E
cb
ij
1(W (t)[i, j] ̸= 0).
(4)
Remark: The above models ignore random factors such as
link dynamics and retransmissions, which are left to future
work.
D. Design Objective
There is an inherent tradeoff between converging in fewer
iterations and spending less energy per iteration, which can be
controlled by designing possibly time-varying mixing matrices
(W (t))T
t=1. To maximize the lifetime of the learning system,
we aim at designing the mixing matrices to minimize the
maximum per-node energy consumption, deﬁned as
max
i∈V
T
X
t=1
ci(W (t)),
(5)
until the learning task reaches a required level of convergence.
E. Motivating Experiment
As a motivating example, we compare the tradeoff between
learning performance and energy consumption under three
example designs. Our experiment is based on the MNIST
dataset, randomly distributed across 100 nodes fully connected
with each other, each training a 4-layer CNN model with
1,663,370 parameters as in [2], with a batch size of 64
and a learning rate of 0.05. We set the energy consumption
parameters as in Section VII. We evaluate the training loss
and the testing error under three designs:
(1) W (t) ≡
1
m11⊤, i.e., all the nodes broadcast their local
models in all the iterations (‘100% activation’),
(2) W (t) ≡I, i.e., no communication between nodes (‘0%
activation’), and
(3) a 2-phase design that starts with I and switches to 1
m11⊤
after 100 epochs (‘0% to 100% activation’).
The trajectories showing training progress as energy is spent,
as shown in Fig. 1, suggest that while using a dense mixing
matrix yields a lower testing error under high energy budgets,
using a sparse mixing matrix is more efﬁcient under low
energy budgets due to the savings in communication energy.
The low training loss of ‘0% activation’ is due to overﬁtting,
which also explains the initial increase in training loss for the
2-phase design after switching. Suitably switching between
the two can achieve a better tradeoff between the quality of
learning and the energy consumption than each case alone.
III. CONVERGENCE ANALYSIS
The foundation of our solution is a set of convergence
theorems that characterize the number of iterations until
convergence as explicit functions of the mixing matrices. To
present these theorems, we will need a few notions: the ideal
mixing matrix J :=
1
m11⊤which maintains global consensus,
the divergence between a designed (possibly random) mixing
matrix W and the ideal mixing matrix deﬁned as
ρ(W ) := ∥IE[W ⊤W ] −J∥,
and the following parameters that capture the dependence of
convergence rate on time-varying mixing matrices.
We deﬁne
p(t) := 1 −ρ
 W (t)
,
and let p := {p(t)}∞
t=0. For each j ≥0, deﬁne
πj :=
X
i>j
i−1
Y
t=j+1

1 −p(t)
2

.
The quantity πj aggregates the contraction effects of time-
varying mixing matrices {W (t)}t≥j+1 starting from iteration
j + 1. Each factor 1 −p(t)/2 upper bounds the amount of
divergence at time t, while the product reﬂects mixing over
consecutive iterations accumulated over time. Consequently,
πj provides an upper bound on the cumulative divergence from
the averaging operator J that persists after the j-th iteration.
For a time horizon T > 0, we further deﬁne the time-
averaged quantities
Π1(T ) := 1
T
T −1
X
j=0
πj,
Π2(T ) := 1
T
T −1
X
j=0
πj
p(j) .
(6)
The quantity Π1(T ) represents the average cumulative diver-
gence over the ﬁrst T iterations, while Π2(T ) is a weighted
version that additionally accounts for the instantaneous mixing
strength p(j) at the starting time j. Both can be viewed as
ergodic measures of how effectively the sequence of time-
varying mixing matrices promotes consensus over time.
We collect a few useful facts about the parameters πj,
Π1(T ), and Π2(T ).
4


--- Page 5 ---
Lemma III.1. Assume there exists δ ∈(0, 1) such that p(t) ≥
δ for all t ≥0. Then the following statements hold:
1) For all j ≥0, πj < 2
δ .
2) For any T > 0, Π1(T ) < 2
δ and Π2(T ) <
2
δ2 .
3) In particular, if p(t) ≡p, then for any T > 0, Π1(T ) =
π0 = 2/p and Π2(T ) = 2/p2.
The following theorem describes how the convergence
rate depends on the mixing matrices through the parameters
Π1(T ), Π2(T ), π0, and pmin := minj p(j).
Theorem
III.2.
D-PSGD
under
assumptions
(1)–
(3)
in
Section
II-B
achieves
ǫ-convergence
(i.e.,
1
T
PT −1
t=0 IE[∥∇F(x(t))∥2]
≤
ǫ)
when
the
number
of
iterations T satisﬁes T ≥T1(Π1(T ), Π2(T ), π0, pmin, ǫ, x(0))
for
T1(Π1, Π2, π0, pmin, ǫ, x(0)) := O
 
f0L
p
(1 + M1)(1 + M2)
ǫpmin
!
+ O
f0L2[(1 + π0)Ξ0 + (1 + M1)]
ǫ

+ f0L · O

ˆσ2
mǫ2 +
q
(M1ˆζ2 + ˆσ2)Π1 + ˆζ2Π2
ǫ3/2

,
(7)
where f0 := IE[F(x(0))] −Finf (Finf denotes the minimum
value of F) and Ξ0 :=
1
m
Pm
i=1 ∥x(0)
i
−x(0)∥2.
The big-O notations used in the Theorem above and
throughout the manuscript hide absolute constants independent
of all the parameters.
Remark 1: From Theorem III.2, the required number of
iterations depends on p only through Π1(T ), Π2(T ), π0, and
pmin. When p(t) ≡p, we have Π1(T ) ≡2/p and Π2(T ) ≡
2/p2, in which case Theorem III.2 reduces to the state-of-
the-art convergence theorem in [34] in the case of τ = 1.
Furthermore, with a modiﬁed deﬁnition of p(t) to characterize
τ matrices in consecutive iterations, the proof of Theorem III.2
can be adapted to support the convergence conditions in [34]
for general τ ≥1.
Remark 2: We brieﬂy discuss the contributions of each term
in (7). The third term is expected to be the dominant one:
while the term O(ˆσ2/(mǫ2)) is asymptotically tight for any
stochastic methods [49], [17], the O(ǫ−3/2) part highlights the
main novelty in our bound, as it only involves the “average”
parameters Π1 and Π2 rather than worst-case parameters. The
second term scales as O(ǫ−1) and depends on π0. This
dependence is fairly innocuous, especially since one may force
Ξ0 = 0 by ﬁxing the same initialization point on all worker
nodes; we include such dependnece only for completeness.
Finally, the ﬁrst term depends on the worst-case parameter
pmin in the sequence p. While this dependence may appear
suboptimal, the term itself scales only as O(√M1M2/ǫ) and
is independent of ˆζ and ˆσ, so its overall impact is not expected
to be signiﬁcant.
Remark 3: Analysis of decentralized gradient-based opti-
mization algorithms with time-varying mixing matrices has
been studied in [35], [36], but their convergence guaran-
tees rely on convexity of Fi’s and other strong assumptions
on both the mixing matrices and the objective functions
{Fi}m
i=1. Our techniques also extend to the convex setting:
we establish a convergence theorem analogous to Theo-
rem III.2 showing that the algorithm achieves ǫ-convergence,
i.e.,
1
T
PT −1
t=0 (IE[F(x(t))] −Finf) ≤ǫ, under an additional
assumption that the local objectives are convex. Moreover, in
this case, our requirements (1)–(3) can be further relaxed; in
particular, we may assume M1 = M2 = 0. See Appendix B
for details.
The convergence condition in Theorem III.2 may be satis-
ﬁed by multiple choices of T . We thus deﬁne T2(p, ǫ, x(0))
as the smallest T that satisﬁes the convergence condition for
a given sequence p, error bound ǫ, and initial model x(0), i.e.,
T2(p, ǫ, x(0)) :=
min{T > 0 : T ≥T1(Π1(T ), Π2(T ), π0, pmin, ǫ, x(0))}. (8)
If the local objective functions are convex, we deﬁne T2
analogously as
T2(p, ǫ, x(0)) :=
min{T > 0 : T ≥T4(Π1(T ), Π2(T ), π0, pmin, ǫ, x(0))
(9)
where T4 denotes the convergence bound given in Theo-
rem A.3.
IV. OPTIMIZATION FRAMEWORK
At a high level, our design objective as stated in Section II-D
can be formulated as the following optimization problem:
min
T,{W (t)}T
t=1
max
i∈V
T
X
t=1
ci(W (t))
(10a)
s.t. T ≥T2(p, ǫ, x(0)),
(10b)
p(t) = 1 −ρ(W (t)),
∀t = 1, . . . , T.
(10c)
The design variables include both the number of iterations
T and the sequence of mixing matrices{W (t)}T
t=1. We treat ǫ
and the initial model x(0) as explicit inputs to the formulation.
A. Randomized Multi-phase Design Framework
We note that under broadcast communications, deterministic
mixing matrix design is not sufﬁcient. This is because if
any iteration t uses a deterministic mixing matrix W (t)
for which the set of communicating nodes U is not equal
to V , then W (t) is a reducible mixing matrix and thus
ρ(W (t)) = ∥W (t) −J∥2 = 1, which implies Π2(T ) = ∞
and thus the right-hand side of (10b) is inﬁnite. Therefore,
any deterministic mixing matrix design must activate com-
munications at all the nodes in all the iterations. Under this
constraint, the mixing matrix design problem reduces to the
simple problem of designing a single mixing matrix W with
the minimum ρ(W ) based on the entire base topology G,
whose solution can be rather suboptimal in energy efﬁciency
as shown in Section II-E (‘100% activation’). By employing a
randomized mixing matrix design, we can avoid this constraint
5


--- Page 6 ---
and signiﬁcantly enlarge the design space. Therefore, we
generalize the problem from a deterministic design problem
(10) to a randomized design problem, by replacing its objective
(10a) with
min
T,{W (t)}T
t=1
IE
"
max
i∈V
T
X
t=1
ci(W (t))
#
.
To simplify the computation, we divide the training process
into K phases, where K is a design variable. Each phase
s ∈{1, . . .K} contains τs consecutive iterations of using the
random mixing matrix Ws. For clarity, we use W (t)
s
to refer to
the mixing matrix used in the t-th iteration of phase s, which is
an instance of Ws drawn independently in each iteration. This
changes our design problem into the following optimization:
min
K,{Ws,τs}K
s=1
IE
"
max
i∈V
K
X
s=1
τs
X
t=1
ci(W (t)
s )
#
(11a)
s.t. τ1 + · · · + τK = T2(p, ǫ, x(0))
(11b)
p(t) = 1 −ρ(Ws), ∀t =
  s−1
X
l=1
τl, . . . ,
s
X
l=1
τl

,
(11c)
∀s = 1, . . . , K.
The decision variables are: the number of phases K, the
K random mixing matrices {Ws}K
s=1, and the number of
iterations {τs}K
s=1 for using each. For a given solution to (11),
the number of iterations required for convergence is denoted as
T3((p1, τ1), (p2, τ2), . . . , pK) := T2(p, ǫ, x(0)),
(12)
where ps := 1−ρ(Ws) is the convergence parameter for each
phase s = 1, . . . , K. Note that, given (p1, τ1), (p2, τ2) . . . , pK
and K, τK is no longer an independent variable due to (11b).
B. A Related Sub-problem
Since T3((p1, τ1), . . . , pK) is a decreasing function of each
ps = 1 −ρ(Ws), we want to design a random mixing
matrix for each phase s to minimize ρ(Ws) without triggering
too many communications. To this end, we introduce a sub-
problem of budgeted mixing matrix design. Let D > 0 denote
the budget for the per-iteration expected energy consumption
at each node in a given phase. We formulate the per-phase
mixing matrix design problem (omitting the phase index) as
min
Pr[·]

X
W ∈M
Pr[W ]W ⊤W −J

(13a)
X
W ∈M
Pr[W ] = 1, and Pr[W ] ≥0 ∀W ∈M,
(13b)
X
W ∈M
Pr[W ]ci(W ) ≤D, ∀i ∈V
(13c)
1[Pr[W ] · W [i, j] ̸= 0] ≤
1[E[i, j] ̸= 0],
(13d)
∀W ∈M, ∀i ̸= j ∈V,
where we use M to denote the set of symmetric matrices
whose row/column sums are 1, and Pr[W ] to denote the prob-
ability of choosing W ∈M in each iteration of this phase.
We will dive into a concrete solution to (13) in Section V-
VI. To present the overall solution, we assume the existence of
a hypothetical solution A to (13), treated as a black box for
the time being, with the following property: for any budget
D, A provides a random mixing matrix W that is feasible
for (13) with the guarantee that there exists a function ρ+
A :
R+ →R+ ∪{∞} such that ρ(W ) ≤ρ+
A(D).
Under a given budget D, one can bound the expected
maximum energy consumption per node for a phase of T
iterations by the following lemma.
Lemma IV.1. Let D > 0 and T > 0. Suppose a random
mixing matrix W satisﬁes IE[ci(W )] ≤D (∀i ∈V ). If
W (1), . . . , W (T ) are i.i.d. copies of W , then
IE
"
max
i
T
X
t=1
ci(W (t))
#
≤D ·
 
T + m
r
T π
8
!
=: q(T, D).
(14)
The proof of Lemma IV.1 is deferred to Section C.
C. Case Studies for K = 1 and K = 2
1) Computation of T3: Next we illustrate computation for
T3 through case studies of K = 1 and K = 2.
In the case of K = 1, as explained in the ﬁrst remark below
Theorem III.2, we have Π1(T ) ≡2/p1 and Π2(T ) ≡2/p2
1.
With this simpliﬁcation,
T3(p1) = T1(2/p1, 2/p2
1, 2/p1, p1, ǫ, x(0))
= O
 
f0L
p
(1 + M1)(1 + M2)
ǫp1
!
+ O
f0L2[Ξ0 + p1(1 + M1)]
ǫp1

+ f0L · O

ˆσ2
mǫ2 +
q
(M1ˆζ2 + ˆσ2)p1 + ˆζ2
ǫ3/2p1

. (15)
In the case of K = 2, Π1(T ) and Π2(T ) are given by the
following lemma.
Lemma IV.2. Let T ≥τ1 be a ﬁxed integer. If p(1) = · · · =
p(τ1) = p1 and p(τ1+1) = · · · = p(T ) = p2, then Π1(T ) and
Π2(T ) as deﬁned in (6) satisfy the followings:
Π1(T ) = 2(T −τ1)
T p2
+ 2τ1
T p1
−( 2
T p1
−
2
T p2
)
τ1
X
j=1
(1 −p1
2 )j,
Π2(T ) = 2(T −τ1)
T p2
2
+ 2τ1
T p2
1
−( 2
T p2
1
−
2
T p2p1
)
τ1
X
j=1
(1 −p1
2 )j.
The proof of Lemma IV.2 is elementary and will be
provided in Appendix C. From Lemma IV.2 we have
Π1(T ) →2/p2 and Π2(T ) →2/p2
2 as T grows. Therefore,
by a linear search over T
= τ1, τ1 + 1, . . . , the ﬁrst T
satisfying T
≥T1(Π1(T ), Π2(T ), π0, pmin, ǫ, x(0)) is the
value of T3((p1, τ1), p2).
6


--- Page 7 ---
Algorithm 1: Multi-phase Mixing Matrix Design
input : Maximum #phases K, subroutine A for the problem
(13) and an associated function ρ+
A.
output: Mixing matrices SK∗
s=1{W (t)
s }
τ∗
s
t=1.
1 for K ←1 to K do
2
Minimize QK = PK
s=1 q(τs, Ds) for
τK = T3((p1, τ1), . . . , (pK−1, τK−1), pK) −PK−1
s=1 τs
and ps = 1 −ρ+
A(Ds);
3 K∗←arg minK QK, with the corresponding solution
D∗
1, . . . , D∗
K∗, τ ∗
1 , . . . , τ ∗
K∗−1;
4 for s ←1 to K∗do
5
for t ←1 to τ ∗
s do
6
W (t)
s
←A(D∗
s);
7 return SK∗
s=1{W (t)
s }
τ∗
s
t=1;
2) Computation of Design Objective: Lemma IV.1 together
with the function ρ+
A(D) enables us to upper-bound the
objective function (11a). As an example, consider K = 1.
By Lemma IV.1, the random mixing matrix W designed by
A for budget D will achieve convergence with an expected
maximum energy consumption per node no more than
QK=1(D) := q
 T3(1 −ρ+
A(D)), D

.
(16)
Thus, by relaxing the objective function (11a) into its upper
bound (16), we can obtain a 1-phase (randomized) mixing
matrix design by minimizing QK=1(D) over D ∈R+ and
then feeding the resulting D into the given subroutine A to
obtain a random mixing matrix W .
Now consider the case of K = 2. This enlarges our design
space to include distinct positive budgets D1 > 0 and D2 > 0
for each phase, as well as the number of iterations τ1 for
phase 1. We ﬁrst rewrite the objective function (11a) speciﬁc
to K = 2:
min
{Ws,τs}K
s=1
IE
"
max
i∈V
K
X
s=1
τs
X
t=1
ci(W (t)
s )
#
=
min
W1,W2,τ1IE

max
i∈V
 τ1
X
t=1
ci(W (t)
1 ) +
T3((p1,τ1),p2)
X
t=τ1+1
ci(W (t−τ1)
2
)



(17)
Given a solution A to the sub-problem (13), we apply A to D1
and D2, respectively. Then, based on the performance bound
ρ+
A, the objective (17) of 2-phase design can be upper-bounded
by
QK=2(D1, D2, τ1) := q(τ1, D1)+q(T3((p1, τ1), p2)−τ1, D2),
(18)
where p1 = 1 −ρ+
A(D1) and p2 = 1 −ρ+
A(D2). Therefore,
optimizing QK=2(D1, D2, τ1) over the 3-tuple (D1, D2, τ1)
will yield an optimized 2-phase design.
D. Overall Solution
We propose to solve the multi-phase design problem (11)
through a trilevel optimization as shown in Algorithm 1:
• Upper-level optimization: Decide the number of phases
K to minimize the overall objective.
Algorithm 2: Budgeted Mixing Matrix Design for
Broadcast Communication
Input: A base topology G = (V, E) with per-computation
cost ca
i and per-transmission cost cb
i (∀i ∈V ) and
per-node budget D.
Output: A mixing matrix W with an expected per-node
cost no more than D.
1 Step 1: Sample a subset of nodes to activate
2
Let U ←∅denote the set of activated nodes;
3
foreach i ∈V do
4
Add node i to U independently with probability
min((D −ca
i )/cb
i, 1);
5 Step 2: Design entries for nodes not in U
6
foreach i ∈V \ U do
7
W [i, i] ←1;
8
W [i, j] ←0, W [j, i] ←0 for all j ̸= i;
9 Step 3: Design entries for nodes in U
10
foreach i ∈U do
11
foreach j ∈U \ Vi do
12
W [i, j] ←0;
13
foreach j ∈(Vi ∩U) \ {i} do
14
W [i, j] ←1/ max(|Vi ∩U|, |Vj ∩U|);
15
W [i, i] ←1 −P
j∈V \{i} W [i, j];
• Intermediate-level optimization: Given a number of
phases K, minimize the relaxed objective QK
:=
PK
s=1 q(τs, Ds) to determine the budget Ds and the
duration τs of each phase s = 1, . . . , K.
• Lower-level optimization: Given the budget Ds of each
phase, design a random instance W (t)
s
of the mixing
matrix for each iteration in this phase by applying the
given solution to the sub-problem (13).
The focus of Algorithm 1 is on the intermediate-level opti-
mization, which optimizes a function of 2K −1 variables (i.e.,
D1, . . . , DK, τ1, . . . , τK−1). Given the performance bound ρ+
A
of the lower-level optimization, this intermediate-level opti-
mization only has an O(K)-dimensional solution space, which
is much more tractable than directly optimizing K m × m
random matrices.
Remark: In the remainder of this work, we will solve the
upper/intermediate-level optimizations numerically for small
values of K to focus on solving the lower-level optimization
under broadcast communications. Our trilevel optimization
framework is generally applicable under any communication
model. We leave the development of more efﬁcient solutions
to the upper/intermediate-level optimizations to future work.
V. LOWER-LEVEL OPTIMIZATION UNDER BROADCAST
COMMUNICATION
In this section, we focus on developing efﬁcient solutions
to the budgeted problem deﬁned in (13) under the broadcast
communication cost model.
A. Algorithm Design
To pose a well-deﬁned and non-trivial problem, we assume
max
i
ca
i ≤D < max
i (ca
i + cb
i).
(19)
7


--- Page 8 ---
Indeed, if D ≥maxi(ca
i + cb
i), then we can deterministically
activate all the nodes, in which case the optimal mixing
matrix can be efﬁciently computed by solving a semi-deﬁnite
programming problem [7]. Meanwhile, D < maxi ca
i is not
feasible under the cost model in (3) as some node will exceed
the budget even if it does not communicate.
Under the cost model (3), the problem (13) becomes
min
Pr[·]

X
W ∈M
Pr[W ]W ⊤W −J

(20a)
X
W ∈M
Pr[W ] = 1, and Pr[W ] ≥0 ∀W ∈M,
(20b)
ca
i + Pr[∃j ̸= i : W [i, j] ̸= 0] · cb
i ≤D,
∀i ∈V
(20c)
Pr [1[W [i, j] ̸= 0] ≤
1[E[i, j] ̸= 0], ∀i ̸= j ∈V ] = 1.
(20d)
We now present Algorithm 2, a randomized algorithm
designed to solve (20) in 3 steps.
1) In Step 1, we activate a subset of nodes U ⊆V by
independently selecting each node with a probability
reﬂecting its residual budget after subtracting the com-
putation cost.
2) In Step 2, nodes in V \ U are assigned weights so that
they essentially perform only local updates.
3) In Step 3, nodes in U are assigned mixing weights be-
tween themselves according to the Metropolis–Hastings
rule [50].
Random client selection—as used in Step 1 of Algorithm
2—has been a standard technique in centralized FL to im-
prove communication efﬁciency [2], [51]. In the decentralized
setting, SwarmSGD [52] can be viewed as a special case of our
random client selection in which only two randomly chosen
adjacent nodes gossip. The method in [53] adopts a similar
strategy of keeping one random neighbor for each node, but
further reﬁnes it by optimizing the sampling distribution to
favor high-speed edges. Our random-selection, in contrast,
places no restrictions on the number of participating nodes or
the number of neighbors each node may have, yet it guarantees
that every realization yields a mixing-matrix design tailored
to our speciﬁc budgeted problem. In particular, in Lemma V.1
we show that the mixing matrix generated by Algorithm 2 is
always topology-compliant and a feasible solution to problem
(20), e.g., a valid randomized mixing matrix that satisﬁes the
budget D at every node.
Lemma V.1. Let G be an arbitrary base topology. Assume
(19) holds for the given (ca
i , cb
i)i∈V and D. The mixing matrix
W outputted by Algorithm 2 is a feasible solution to problem
(20).
B. Performance Analysis
1) Result for Fully-connected Base Topology: Consider the
case where the base topology is a clique, i.e., every node can
receive the broadcast of every other node3. In this case, we
can characterize the performance of Algorithm 2 analytically.
3We assume that proper transmission scheduling is in place to avoid
collision. The speciﬁc schedule is irrelevant under the cost model (3).
To present this result, we introduce some additional no-
tations. We use ∼m to relate two quantities A and B if
limm→∞A
B = 1, that is, A is asymptotically equivalent to
B. Also, for each i ∈V , we let
ωi := min
D −ca
i
cb
i
, 1

,
and we use ω to denote the vector [ω1, ω2, . . . , ωm]. Lastly,
given an arbitrary vector a, let diag(a) denote the diagonal
matrix with a on the main diagonal, and a2 denote the vector
[a2
1, . . . , a2
m]. Based on these notations, we characterize the
performance of Algorithm 2 as follows.
Theorem V.2. Assume G is a clique, and (19) holds for the
given cost vectors (ca
i , cb
i)i∈V and budget parameter D. The
mixing matrix W designed by Algorithm 2 satisﬁes
ρ(W ) ∼m
m⊥ωω⊤+ diag(1 −ω) −J
 ,
(21)
where m⊥denote the scalar IE[ 1
|U|
| U
̸= ∅] with the
expectation taken over the random generation of U.
Moreover, in
the special case
of homogeneous cost
parameters, i.e., ca
i
≡
ca, cb
i
≡
cb, we can explicitly
express the dependency of ρ(W ) on the cost parameters
(ca, cb) and the budget D. Speciﬁcally, for any D satisfying
ca ≤D < ca + cb, we have
ωi = D −ca
cb
=: ˜ω,
∀i ∈V.
Given the fact that m⊥∼m 1/(˜ωm), we obtain that
ρ(W ) ∼m

1
˜ωm · ˜ω211⊤+ (1 −˜ω)I −J

= ∥(1 −˜ω)(I −J)∥= 1 −˜ω = 1 −D −ca
cb
.
(22)
2) Discussion on General Base Topology: Using a similar
analysis as Theorem V.2, one could derive an analogous bound
on ρ(W ) for the W designed by Algorithm 2. However, the
bound will involve O(m2) conditional expectations analogous
to m⊥, which is challenging to compute numerically. In this
case, we evaluate ρ(W ) numerically through Monte Carlo
experiments, i.e., for a given budget D, we generate a large
number of matrices W1, . . . , WN from independent runs of
Algorithm 2 and use the empirical mean
ˆρ(W ) =

1
N
N
X
n=1
W ⊤
n Wn −J

to approximate ρ(W ).
VI. LOWER-LEVEL OPTIMIZATION UNDER UNICAST
COMMUNICATION
For the budgeted problem in (13) to be feasible and non-
trivial under the unicast cost model (4), we assume
max
i
ca
i ≤D < max
i

ca
i +
X
j:(i,j)∈E
cb
ij

.
(23)
8


--- Page 9 ---
This section presents a (meta-)algorithm for solving (13)
for any budget D that satisﬁes the condition in (23), on a
general base topology G = (V, E) with arbitrary cost vectors
(ca
i , cb
ij)(i,j)∈E.
A. Meta-algorithm Design
The algorithm starts by employing a random graph oracle to
sample subgraph of the base topology G as candidate topolo-
gies, with which we then compute an optimal distribution
over the candidates using semi-deﬁnite programming (SDP).
Random candidates are iteratively added until the mixing rate
objective reaches a convergence threshold. See Algorithm 3
for its pseudo-code.
Algorithm 3: Budgeted Mixing Matrix Design for
Unicast Communication
Input: A base topology G = (V, E) with
per-computation cost ca
i and per-transmission
cost cb
ij (∀i, j ∈V ) and per-node budget D; a
random graph oracle GG; a convergence
threshold parameter δ > 0.
Output: A list of mixing matrices with their
probabilities {(Ws, ps)}k
s=0.
1 Let ρ0 ←1;
2 Let k ←0 and W0 = I;
3 repeat
4
k ←k + 1;
5
Draw Gk = (Vk, Ek) from GG;
6
foreach (u, v) ∈Ek do
7
Let Ak[u, v] = 1/ max(degGk(u), degGk(v));
8
Set Dk[u, u] = P
v Ak[u, v];
9
Set Lk = Dk −Ak and Wk = I −Lk;
10
Given k candidate matrices (W0, W1, . . . , Wk),
solve (24) for the optimal probability vector
(p0, p1, . . . , pk) and denote its objective value by
ρk;
11 until |ρk −ρk−1| < δ and ρk < 1;
12 Return {(Ws, ps)}k
s=0;
It can be veriﬁed that the output of Algorithm 3 is a feasible
solution to the problem deﬁned in (13). Speciﬁcally, at Line 10
in each iteration k, we solve the optimization problem given
in (24). A straightforward calculation shows that the objective
(24a) is equivalent to (13a). Moreover, the budget constraint
in (24c) is a direct instantiation of (13c) under the unicast cost
model. Finally, the random graph oracle GG ensures that each
sampled graph Gk is a subgraph of G, thereby automatically
satisfying the constraint (13d).
min
p0,...,pk

k
X
h=0
phW ⊤
h Wh −J

(24a)
k
X
h=0
ph = 1
and
ph ≥0 ∀h = 0, . . . , k
(24b)
ca
i +
k
X
h=0
ph
X
j:(i,j)∈Eh
cb
ij ≤D,
∀i ∈V.
(24c)
Problem (24) is similar to problem (19) in [7], with the key
difference that constraint (24c) imposes a “per-node cost con-
straint”, whereas the latter enforces a total cost constraint. In
each iteration, we solve an instance of (24) with an increasing
value of k. Each instance is a linear SDP and can therefore
be solved in polynomial time with respect to #matrices k and
#nodes m. Since the optimal solution from iteration k −1
remains feasible for iteration k, the objective value ρk is non-
increasing. Assume that the random oracle guarantees ρk < 1
within at most O(m) iterations, a property that can be veriﬁed
for every oracle we consider for a sufﬁciently large D; see
Lemma VI.2. Once ρk < 1, there are two possibilities:
• If ρk decreases by at least δ in every iteration, then the
algorithm terminates after at most O(1/δ) iterations.
• Otherwise, the algorithm stops at some iteration k∗where
the improvement becomes negligible, i.e.,
|ρk∗−ρk∗−1| < δ.
Therefore, the total running time of Algorithm 3 is polynomial
in #nodes m and δ−1.
Algorithm 3 provides a uniﬁed framework that encompasses
the approaches proposed in several prior works. Each of these
algorithms can be implemented using Algorithm 3, equipped
with different random graph oracles and suitably modiﬁed
optimization objectives:
• [9] and [54] introduce Matching Decomposition Sam-
pling, which optimizes the distribution over sets of match-
ings.
• [7] proposes Laplacian Matrix Sampling, which computes
an optimal distribution over a set of (spectral) sparsiﬁers;
• [12] presents BASS, which designs a distribution over
collision-free subgraphs to accelerate communication.
For our unicast cost model, we propose to use the following
random graph generator: ﬁrst, in each iteration k draw a
random regular graph Hk with degree
d := min
i
D −ca
i
maxj cb
ij
;
then let Gk = (V, E ∩E(Hk)).
B. Performance Analysis
1) Complete Base Topology: When the base topology G
is a clique, the random graph oracle effectively produces
Ramanujan graphs [55]. The choice of degree ensures that
each Ramanujan graph satisﬁes the maximum per-node budget
9


--- Page 10 ---
constraint. In addition, the work in [15] establishes an upper
bound on ρ(W ) when W is a deterministic mixing matrix
deﬁned over a ﬁxed Ramanujan graph. Therefore, when Algo-
rithm 3 is equipped with this oracle, it constructs a randomized
mixing matrix by optimizing a distribution over multiple such
Ramanujan graphs — resulting in performance that is at
least as good as the individual graphs, if not better. As a
consequence, the upper bound established for the deterministic
case directly extends to our algorithm.
Corollary VI.1. Assume G is a clique, and (23) holds for the
given cost vectors (ca
i , cb
ij)(i,j)∈E and budget D. The mixing
matrix W designed by Algorithm 3 when equipped with a
Ramanujan graph generator as the oracle GG satisﬁes
ρ(W ) ≤4
d = max
i
4 maxj cb
ij
D −ca
i
,
regardless of #iterations k (i.e., valid even if k = 1).
2) General Base Topology: In the general setting with an
arbitrary cost vector and base topology G, Algorithm 3 is
expected to perform well because it addresses heterogene-
ity in two ways: the weight setting used in Line 7 helps
absorb structural imbalances in G, while the optimization
over a distribution of graphs allows the algorithm to adapt to
variations in the cost vector. These two components together
enable the construction of a randomized mixing matrix with
favorable spectral properties. In practice, since analytically
bounding ρ(W ) is difﬁcult, we use the empirical estimate of
ρ(W ) as a practical surrogate for ρ+
A in the intermediate-level
optimization.
Moreover, as observed in [15], ﬁnding a deterministic matrix
W for a general graph that satisﬁes the maximum per-node
budget constraint and achieves ρ(W ) < 1 is NP-hard under
the unicast cost model. In contrast, if the problem is relaxed to
allow W to be a randomized matrix as considered here, then
under the corresponding budget constraint (24c), the problem
becomes tractable. Speciﬁcally, Lemma VI.2 guarantees that
in Algorithm 3, ρk < 1 happens within O(m) iterations with
high probability.
Lemma VI.2. Suppose G = (V, E) is a connected graph, and
assume that there exists c > 0 such that for each e ∈E,
Gk ∼GG contains e with probability at least c. Let τ be the
ﬁrst iteration k when ρk < 1. With probability 1 −e−Ω(m),
we have τ = O(m/c).
VII. PERFORMANCE EVALUATION
We evaluate the proposed solution against benchmarks on
a real dataset under realistic settings.
A. Evaluation Setting
1) Problem Setting: We consider the standard task of image
classiﬁcation based on CIFAR-10, which consists of 60,000
color images in 10 classes. We train a lightweight version of
ResNet-50 with 1.5M parameters over its training dataset with
50,000 images, and then test the trained model over the testing
dataset with 10,000 images. We set the learning rate to 0.01,
0.2
0.4
0.6
0.8
1.0
1.2
1.4
D (mWh)
5000
10000
15000
20000
25000
30000
Q
K
=
1
 (mWh)
0.2
0.4
0.6
0.8
1.0
1.2
1.4
D
1
 (mWh)
5000
10000
15000
20000
25000
30000
35000
Q
K
=
2
 (mWh)
0.2
0.4
0.6
0.8
1.0
1.2
1.4
D
2
 (mWh)
5000
10000
15000
20000
25000
30000
Q
K
=
2
 (mWh)
0
50
100
150
200
250
300
τ
1
 (Epochs)
2000
4000
6000
8000
10000
Q
K
=
2
(mWh)
Fig. 2. Design objective vs. design parameters for clique.
0
500
1000
1500
2000
2500
3000
3500
4000
Max-energy per node (mW )
0.001
0.002
0.005
 0.01
T
raining Lo((e(
0
500
1000
1500
2000
2500
3000
3500
4000
Max-energy per node (mW )
0.2
0.3
0.4
0.6
0.9
T
e()ing Error
0
50
100
150
200
250
300
Epoc (
0.001
0.002
0.005
 0.01
T
raining Lo((e(
0
50
100
150
200
250
300
Epoc (
0.2
0.3
0.4
0.6
0.9
T
e()ing Error
Vanilla
AdaPC
Max (ucce((
BASS 55%
pr%p%sed 55%
BASS 16%/98%
pr%p%sed 16%/98%
Fig. 3. Training performance on clique (‘proposed/BASS x%’: 1-phase design
activating x% of nodes; ‘proposed/BASS x%/y%’: 2-phase design activating
x% of nodes in phase 1 and y% in phase 2).
the batch size to 64, the momentum to 0.9, and the weight
decay to 0.0005. We employ two base topologies: (i) a 33-node
clique, which represents a densely deployed wireless network
where every node can reach every other node in one hop; (ii)
the topology of Roofnet [56], which contains 33 nodes and
187 links. The data rate is set to 1 Mbps in both topologies
according to [56].
2) Cost Parameters: We consider two types of devices
commonly used for learning in edge networks: NVIDIA
Tegra X2 (TX2) with Broadcom BCM4354 and Jetson Xavier
NX with Intel 8265NGW, with a computation power of
4.7W/6.3W and a transmission power of 40mW/100mW,
respectively [57]. Based on these parameters, we set the com-
putation energy as ca
i = 0.086mWh for TX2 and 0.086mWh
for NX, and the communication energy as cb
i = 0.533mWh
for TX2 and 1.333mWh for NX4. We assign odd-numbered
nodes to TX2 and even-numbered nodes to NX.
3) Benchmarks: We compare the proposed solution with
the following benchmarks:
• ‘Vanilla D-PSGD’ [1], which is a baseline with all the
neighbors communicating in all the iterations;
4Our model size is S = 6MB at FP32, batch size is B = 64, and processing
speed is tc
i = 1.026ms per sample for TX2 and 0.769ms per sample for NX.
Under 1Mbps, we estimate the computation energy by ca
i = P c
i ∗B∗tc
i /3600
mWh, and the communication energy by cb
i = P t
i ∗S ∗8/1/3600 mWh (P c
i :
computation power of node i in W; P t
i : transmission power of node i in mW).
10


--- Page 11 ---
0.2
0.4
0.6
0.8
1.0
1.2
1.4
D (mWh)
10
4
10
5
10
6
10
7
10
8
Q
K
=
1
 (mWh)
0.2
0.4
0.6
0.8
1.0
1.2
1.4
D
1
 (mWh)
10
4
10
5
10
6
10
7
10
8
Q
K
=
2
 (mWh)
0.2
0.4
0.6
0.8
1.0
1.2
1.4
D
2
 (mWh)
10
4
10
5
10
6
10
7
10
8
Q
K
=
2
 (mWh)
0
50
100
150
200
250
300
τ
1
 (Epochs)
5000
7500
10000
12500
15000
17500
20000
Q
K
=
2
 (mWh)
Fig. 4. Design objective vs. design parameters for Roofnet.
0
1000
2000
3000
4000
5000
6000
7000
8000
Max-energy per node (mWh)
0.001
0.002
0.005
 0.01
T
)ainin  Losses
0
1000
2000
3000
4000
5000
6000
7000
8000
Max-energy per node (mWh)
0.2
0.3
0.4
0.6
0.9
T
esting Error
0
50
100
150
200
250
300
Epochs
0.001
0.002
0.005
 0.01
T
rainin  Losses
0
50
100
150
200
250
300
Epochs
0.2
0.3
0.4
0.6
0.9
T
estin  Error
Vanilla
AdaPC
Max success
SkipT
rain 4-2
BASS 59%
proposed 59%
BASS 28%/99%
proposed 28%/99%
Fig. 5. Training performance on Roofnet.
• ‘AdaPC’ [45], where all the neighbors communicate peri-
odically, with a period adapted according to [45, Alg. 2];
• ‘BASS’ [12], a state-of-the-art mixing matrix design for
broadcast communication with a different objective of
minimizing the communication time (by minimizing the
number of collision-free transmission slots);
• ‘Max Success’ [29], another mixing matrix design for
broadcast communication with a different objective of
maximizing the expected number of successful links;
• ‘SkipTrain’ [28], which is the opposite of ‘AdaPC’ that
periodically skips local updates5;
We note that under broadcast communication, the above
benchmarks subsume the performance of several other existing
solutions, e.g., [9] is subsumed by ‘BASS’ [12] and [7], [15]
are subsumed by ‘Vanilla D-PSGD’.
B. Evaluation Results
1) Results for Clique: We ﬁrst evaluate the design objective
QK with respect to the budget Ds and the duration τs for
each phase. Fig. 2 shows the results for K = 1 and K =
2, while ﬁxing the other design parameters at their optimal
values. The optimal value for each parameter is denoted by •.
The results not only suggest the beneﬁt of multi-phase design
as the optimal value of QK=2 is smaller than the optimal
5For a fully connected base topology, ‘SkipTrain’ reduces to ‘Vanilla
D-PSGD’, as there is no need for additional communications to achieve
synchronization after a training round.
value of QK=1, but also indicate the need of switching from
a lower level of activation to a higher level of activation (as
D1 < D2). However, τs from this optimization is often larger
than necessary as it is based on an upper bound on the total
number of iterations. We thus normalize it by τs·(T/T), where
T is the actual number of iterations to reach convergence and
T is an upper bound.
We then evaluate the training performance in terms of
training loss and testing error, omitting ‘SkipTrain’ as it
reduces to ‘Vanilla’ in this case. As ‘BASS’ also has a
conﬁgurable budget, we evaluate two versions of it with the
same percentage of activated nodes as our 1-phase/2-phase
designs. The results in Fig. 3 show that: (i) partial activation
can achieve a better energy efﬁciency than activating all the
nodes (‘Vanilla’); (ii) the proposed solution achieves a better
tradeoff between the testing error and the maximum per-node
energy consumption than the benchmarks, and 2-phase design
outperforms 1-phase design.
2) Results for Roofnet: We repeat the experiments on
Roofnet, where ‘SkipTrain’ is conﬁgured according to the
recommendation by [28] for a topology of similar average
degree. The results in Fig. 4–5 show similar observations
as Fig. 2–3, except that (i) the design objective in Fig. 4
is based on numerically estimated ρ values as explained in
Section V-B2 (from 500 matrices per budget), (ii) ‘BASS’
has a bigger performance gap with our solution due to its
negligence of balancing energy consumption across nodes, and
(iii) it takes more epochs and energy consumption to reach
convergence due to the limited connectivity between nodes.
VIII. CONCLUSION
We considered the mixing matrix design to minimize the
maximum per-node energy consumption in DFL while consid-
ering the broadcast nature of wireless communications. Based
on a novel convergence theorem that allows arbitrarily time-
varying mixing matrices, we proposed a multi-phase design
framework that designs randomized mixing matrices as well
as how long to use each matrix. Our evaluations on real data
demonstrated that the proposed design can achieve a superior
tradeoff between maximum per-node energy consumption and
accuracy even with two phases, thus improving the energy
efﬁciency of learning in wireless networks.
REFERENCES
[1] X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and J. Liu, “Can
decentralized algorithms outperform centralized algorithms? a case study
for decentralized parallel stochastic gradient descent,” in Proceedings
of the 31st International Conference on Neural Information Processing
Systems, 2017, p. 5336–5346.
[2] H. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efﬁcient learning of deep networks from decentralized
data,” in AISTATS, 2017.
[3] P. Kairouz et al., Advances and Open Problems in Federated Learning.
Now Foundations and Trends, 2021.
[4] L. Luo, P. West, J. Nelson, A. Krishnamurthy, and L. Ceze, “Plink:
Discovering and exploiting locality for accelerated distributed training
on the public cloud,” in Proceedings of Machine Learning and Systems,
vol. 2, 2020, pp. 82–97.
[5] X. Chen, G. Zhu, Y. Deng, and Y. Fang, “Federated learning over multi-
hop wireless networks with in-network aggregation,” IEEE Transactions
on Wireless Communications, vol. 21, no. 6, pp. 4622–4634, 2022.
11


--- Page 12 ---
[6] A. Koloskova, T. Lin, S. U. Stich, and M. Jagg, “Decentralized deep
learning with arbitrary communication compression,” in The Interna-
tional Conference on Learning Representations (ICLR), 2020.
[7] C.-C. Chiu, X. Zhang, T. He, S. Wang, and A. Swami, “Laplacian
matrix sampling for communication- efﬁcient decentralized learning,”
IEEE Journal on Selected Areas in Communications, vol. 41, no. 4, pp.
887–901, 2023.
[8] N. Singh, D. Data, J. George, and S. Diggavi, “SPARQ-SGD: Event-
triggered and compressed communication in decentralized optimization,”
in IEEE CDC, 2020.
[9] J. Wang, A. K. Sahu, G. Joshi, and S. Kar, “MATCHA: A matching-
based link scheduling strategy to speed up distributed optimization,”
IEEE Transactions on Signal Processing, vol. 70, pp. 5208–5221, 2022.
[10] Y. Hua, K. Miller, A. L. Bertozzi, C. Qian, and B. Wang, “Efﬁcient and
reliable overlay networks for decentralized federated learning,” SIAM
Journal on Applied Mathematics, vol. 82, no. 4, pp. 1558–1586, 2022.
[11] B. Le Bars, A. Bellet, M. Tommasi, E. Lavoie, and A.-M. Kermarrec,
“Reﬁned convergence and topology learning for decentralized SGD
with heterogeneous data,” in International Conference on Artiﬁcial
Intelligence and Statistics.
PMLR, 2023, pp. 1672–1702.
[12] D. P. Herrera, Z. Chen, and E. G. Larsson, “Faster convergence with less
communication: Broadcast-based subgraph sampling for decentralized
learning over wireless networks,” IEEE Open Journal of the Communi-
cations Society, pp. 1–1, 2025.
[13] H. Xing, O. Simeone, and S. Bi, “Federated learning over wireless
device-to-device networks: Algorithms and convergence analysis,” IEEE
Journal on Selected Areas in Communications, vol. 39, no. 12, pp. 3723–
3741, 2021.
[14] P. Pinyoanuntapong, W. H. Huff, M. Lee, C. Chen, and P. Wang, “Toward
scalable and robust AIoT via decentralized federated learning,” IEEE
Internet of Things Magazine, vol. 5, no. 1, pp. 30–35, 2022.
[15] X. Zhang, C.-C. Chiu, and T. He, “Energy-efﬁcient decentralized learn-
ing via graph sparsiﬁcation,” 2024, https://arxiv.org/abs/2401.03083.
[16] H. Tang, X. Lian, M. Yan, C. Zhang, and J. Liu, “d2: Decentralized
training over decentralized data,” in Proceedings of the 35th Interna-
tional Conference on Machine Learning, ICML, 2018.
[17] Y. Lu and C. D. Sa, “Optimal complexity in decentralized training,” in
International Conference on Machine Learning (ICML), 2021.
[18] R. Xin, U. Khan, and S. Kar, “A hybrid variance-reduced method
for decentralized stochastic non-convex optimization,” in Proceedings
of the 38th International Conference on Machine Learning, ser.
Proceedings of Machine Learning Research, M. Meila and T. Zhang,
Eds., vol. 139.
PMLR, 18–24 Jul 2021, pp. 11 459–11 469. [Online].
Available: https://proceedings.mlr.press/v139/xin21a.html
[19] L. Li, L. Yang, X. Guo, Y. Shi, H. Wang, W. Chen, and K. B. Letaief,
“Delay analysis of wireless federated learning based on saddle point
approximation and large deviation theory,” IEEE Journal on Selected
Areas in Communications, vol. 39, no. 12, pp. 3772–3789, 2021.
[20] Y. Lu and C. D. Sa, “Moniqua: Modulo quantized communication in
decentralized SGD,” in International Conference on Machine Learning
(ICML), 2020.
[21] H. Tang, S. Gan, C. Zhang, T. Zhang, and J. Liu, “Communication com-
pression for decentralized training,” in Advances in Neural Information
Processing Systems (NeurIPS), 2018.
[22] X. Zhang, J. Liu, Z. Zhu, and E. S. Bentley, “Communication-efﬁcient
network-distributed optimization with differential-coded compressors,”
in IEEE INFOCOM, 2020, p. 317–326.
[23] J. Wang and G. Joshi, “Adaptive communication strategies to achieve
the best error-runtime trade-off in local-update SGD,” in Systems for
ML, 2019.
[24] N. H. Tran, W. Bao, A. Zomaya, M. N. Nguyen, and C. S. Hong,
“Federated learning over wireless networks: Optimization model design
and analysis,” in IEEE INFOCOM, 2019.
[25] S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, and
K. Chan, “Adaptive federated learning in resource constrained edge com-
puting systems,” IEEE Journal on Selected Areas in Communications,
vol. 37, no. 6, pp. 1205–1221, 2019.
[26] J. Wang and G. Joshi, “Cooperative sgd: A uniﬁed framework for
the design and analysis of local-update sgd algorithms,” Journal of
Machine Learning Research, vol. 22, no. 213, pp. 1–50, 2021. [Online].
Available: http://jmlr.org/papers/v22/20-147.html
[27] N. Singh, D. Data, J. George, and S. Diggavi, “SQuARM-SGD:
Communication-efﬁcient momentum SGD for decentralized optimiza-
tion,” IEEE Journal on Selected Areas in Information Theory, vol. 2,
no. 3, pp. 954–969, 2021.
[28] M. De Vos, A. Dhasade, P. Dini, E. Guerra, A.-M. Kermarrec,
M. Miozzo, R. Pires, and R. Sharma, “Energy-aware decentralized
learning with intermittent model training,” in 2024 IEEE International
Parallel and Distributed Processing Symposium Workshops (IPDPSW),
2024, pp. 1172–1174.
[29] Z. Chen, M. Dahl, and E. G. Larsson, “Decentralized learning over
wireless networks: The effect of broadcast with random access,” in 2023
IEEE 24th International Workshop on Signal Processing Advances in
Wireless Communications (SPAWC), 2023, pp. 316–320.
[30] G. Neglia, C. Xu, D. Towsley, and G. Calbi, “Decentralized gradient
methods: does topology matter?” in International Conference on Artiﬁ-
cial Intelligence and Statistics.
PMLR, 2020, pp. 2348–2358.
[31] Z. Jiang, Y. Xu, H. Xu, L. Wang, C. Qiao, and L. Huang, “Joint
model pruning and topology construction for accelerating decentralized
machine learning,” IEEE Transactions on Parallel and Distributed
Systems, 2023.
[32] T. Vogels, H. Hendrikx, and M. Jaggi, “Beyond spectral gap: The role of
the topology in decentralized learning,” Advances in Neural Information
Processing Systems, vol. 35, pp. 15 039–15 050, 2022.
[33] W. Liu, L. Chen, and W. Zhang, “Decentralized federated learning:
Balancing communication and computing costs,” IEEE Transactions on
Signal and Information Processing over Networks, vol. 8, pp. 131–143,
2022.
[34] A. Koloskova, N. Loizou, S. Boreiri, M. Jaggi, and S. Stich, “A uniﬁed
theory of decentralized SGD with changing topology and local updates,”
in ICML, 2020.
[35] A. Nedi´c and A. Olshevsky, “Distributed optimization over time-varying
directed graphs,” IEEE Transactions on Automatic Control, vol. 60,
no. 3, pp. 601–615, 2015.
[36] A. Nedi´c, A. Olshevsky, and W. Shi, “Achieving geometric convergence
for distributed optimization over time-varying graphs,” SIAM Journal
on Optimization, vol. 27, no. 4, pp. 2597–2633, 2017. [Online].
Available: https://doi.org/10.1137/16M1084316
[37] H. Li and Z. Lin, “Accelerated gradient tracking over time-varying
graphs for decentralized optimization,” Journal of Machine Learning
Research, vol. 25, no. 274, pp. 1–52, 2024. [Online]. Available:
http://jmlr.org/papers/v25/21-0475.html
[38] X.
Huang
and
K.
Yuan,
“Optimal
complexity
in
non-convex
decentralized learning over time-varying networks,” 2022. [Online].
Available: https://arxiv.org/abs/2211.00533
[39] R. H¨onig, Y. Zhao, and R. Mullins, “DAdaQuant: Doubly-adaptive
quantization
for
communication-efﬁcient
federated
learning,”
in
Proceedings of the 39th International Conference on Machine Learning,
ser.
Proceedings
of
Machine
Learning
Research,
K.
Chaudhuri,
S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, Eds., vol.
162.
PMLR, 17–23 Jul 2022, pp. 8852–8866. [Online]. Available:
https://proceedings.mlr.press/v162/honig22a.html
[40] Y. Zhuansun, D. Li, X. Huang, and C. Sun, “Communication-
efﬁcient federated learning with adaptive compression under dynamic
bandwidth,” 2024. [Online]. Available: https://arxiv.org/abs/2405.03248
[41] L. Wang, X. Xu, and J. Pei, “Communication-efﬁcient federated learning
via dynamic sparsity: An adaptive pruning ratio based on weight
importance,” IEEE Transactions on Cognitive Communications and
Networking, pp. 1–1, 2025.
[42] Y. He, Y. Chen, X. Yang, Y. Zhang, and B. Zeng, “Class-wise
adaptive
self
distillation
for
federated
learning
on
non-iid
data
(student abstract),” Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, vol. 36, no. 11, pp. 12 967–12 968, Jun. 2022. [Online].
Available: https://ojs.aaai.org/index.php/AAAI/article/view/21620
[43] Y. Li, X. Wang, H. Li, P. K. Donta, M. Huang, and S. Dustdar,
“Communication-efﬁcient federated learning for heterogeneous clients,”
ACM Trans. Internet Technol., vol. 25, no. 2, Apr. 2025. [Online].
Available: https://doi.org/10.1145/3716870
[44] C.
Chen,
H.
Xu,
W.
Wang,
B.
Li,
B.
Li,
L.
Chen,
and
G. Zhang, “ Communication-Efﬁcient Federated Learning with Adaptive
Parameter Freezing ,” in 2021 IEEE 41st International Conference on
Distributed Computing Systems (ICDCS).
Los Alamitos, CA, USA:
IEEE Computer Society, Jul. 2021, pp. 1–11. [Online]. Available:
https://doi.ieeecomputersociety.org/10.1109/ICDCS51616.2021.00010
[45] J. Tchaye-Kondi, Y. Zhai, J. Shen, A. Telikani, and L. Zhu, “Adaptive
period control for communication efﬁcient and fast convergent federated
learning,” IEEE Transactions on Mobile Computing, vol. 23, no. 12, pp.
12 572–12 586, 2024.
[46] H. Zhang, Z. Xie, R. Zarei, T. Wu, and K. Chen, “Adaptive client
selection in resource constrained federated learning systems: A deep
12


--- Page 13 ---
reinforcement learning approach,” IEEE Access, vol. 9, pp. 98 423–
98 432, 2021.
[47] H. Xu, M. Chen, Z. Meng, Y. Xu, L. Wang, and C. Qiao, “Decentralized
machine learning through experience-driven method in edge networks,”
IEEE Journal on Selected Areas in Communications, vol. 40, no. 2, pp.
515–531, 2022.
[48] Y.
Liao,
Y.
Xu,
H.
Xu,
L.
Wang,
C.
Qian,
and
C.
Qiao,
“Decentralized
federated
learning
with adaptive
conﬁguration
for
heterogeneous participants,” IEEE Transactions on Mobile Computing,
vol.
23,
no.
6,
p.
7453–7469,
Jun.
2024.
[Online].
Available:
https://doi.org/10.1109/TMC.2023.3335403
[49] C. Blair, “Problem complexity and method efﬁciency in optimization
(a. s. nemirovsky and d. b. yudin),” SIAM Review, vol. 27, no. 2, pp.
264–265, 1985. [Online]. Available: https://doi.org/10.1137/1027074
[50] L. Xiao, S. P. Boyd, and S. Lall, “Distributed average consensus with
time-varying metropolis weights,” 2006, unpublished manuscript. [On-
line]. Available: https://api.semanticscholar.org/CorpusID:123313438
[51] Y. Wang, L. Lin, and J. Chen, “Communication-efﬁcient adaptive
federated learning,” in Proceedings of the 39th International Conference
on Machine Learning, ser. Proceedings of Machine Learning Research,
K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and
S. Sabato, Eds., vol. 162.
PMLR, 17–23 Jul 2022, pp. 22 802–22 838.
[Online]. Available: https://proceedings.mlr.press/v162/wang22o.html
[52] G. Nadiradze, A. Sabour, P. Davies, S. Li, and D. Alistarh, “Asyn-
chronous decentralized sgd with quantized and local updates,” in Pro-
ceedings of the 35th International Conference on Neural Information
Processing Systems, ser. NIPS ’21.
Red Hook, NY, USA: Curran
Associates Inc., 2021.
[53] P. Zhou, Q. Lin, D. Loghin, B. C. Ooi, Y. Wu, and H. Yu,
“Communication-efﬁcient decentralized machine learning over heteroge-
neous networks,” in 2021 IEEE 37th International Conference on Data
Engineering (ICDE), 2021, pp. 384–395.
[54] J. Wang, A. K. Sahu, G. Joshi, and S. Kar, “Exploring the error-
runtime trade-off in decentralized optimization,” in 2020 54th Asilomar
Conference on Signals, Systems, and Computers, 2020, pp. 910–914.
[55] S. Hoory, N. Linial, and A. Wigderson, “Expander graphs and their
applications,” Bull. Amer. Math. Soc., vol. 43, no. 04, p. 439–562, Aug.
2006.
[56] D. Aguayo, J. Bicket, S. Biswas, G. Judd, and R. Morris, “Link-level
measurements from an 802.11b mesh network,” in SIGCOMM, 2004.
[57] X. Qiu, T. Parcollet, J. Fernandez-Marques, P. P. B. Gusmao, D. J.
Beutel, T. Topal, A. Mathur, and N. D. Lane, “A ﬁrst look into
the carbon footprint of federated learning,” 2021. [Online]. Available:
https://arxiv.org/abs/2102.07627
[58] K.
Yuan,
Q.
Ling,
and
W.
Yin,
“On
the
convergence
of
decentralized
gradient
descent,”
SIAM
Journal
on
Optimization,
vol.
26,
no.
3,
pp.
1835–1854,
2016.
[Online].
Available:
https://doi.org/10.1137/130943170
APPENDIX
A. Proof of Theorem III.2
We use the following notation for the “consensus distance”
at iteration t:
Ξt := 1
mIE
m
X
i=1
∥x(t)
i
−x(0)∥2.
We use IEt+1[·] to denote the conditional expectation IE[· |
{x(t)
i }]. Also, we let x∗denote the optimal parameter vector
of F(·) and let Finf := F(x∗).
Lemma A.1 (Lemma 11, [34]). Under assumptions (1)-(3),
D-PSGD with stepsize η <
1
4L(M1+1) satisﬁes
IEt+1[F(x(t+1))] ≤F(x(t)) −η
4∥∇F(x(t))∥2
2 + ηL2Ξt + η2Lˆσ2
m
.
Lemma A.2. Under assumptions (1)-(3), D-PSGD with the
stepsize η ≤
p(t−1)
8L√
12+2p(t−1)M1 satisﬁes
Ξt ≤
 1 −p(t−1)
2

Ξt−1+
2η2
 6
p(t−1) + M1
 M2∥∇F(x(t−1))∥2 + ˆζ2
+ ˆσ2

.
Noting that the parameter p(t−1) in [34] is originally deﬁned
in a different form, but due to Lemma 3.1 in [15], it can be
treated the same, in the case of τ = 1. Then Lemma A.2
follows from Lemma 12 in [34] by setting τ
= 1 and
improving some constants in this special case.
Proof of Theorem III.2. We deﬁne the following notations for
ease of the proof:
• ft := IE[F(x(t))] −Finf.
• et := IE∥∇F(x(t))∥2.
• νt
j := Qt−1
i=j (1 −pi
2 ).
• κ :=
√
(1+M1)(1+M2)
pmin
.
• Υ := M2 maxj

6πj
p(j) + M1πj

.
In these notations, Lemma A.1 implies
ft+1 ≤ft −η
4et + Lˆσ2η2
m
+ ηL2Ξt,
(25)
and Lemma A.2 upper bounds Ξt by
Ξt ≤
 1 −p(t−1)
2

Ξt−1 + 2η2·
(26)
 6
p(t−1) + M1
 M2et−1 + ˆζ2
+ ˆσ2

.
We un-roll the recursion in (26) to obtain
Ξt ≤νt
0Ξ0 + 2η2
t−1
X
j=0
νt
j+1
  6
p(j) + M1
 M2ej + ˆζ2
+ ˆσ2

.
13


--- Page 14 ---
Noting that πj = P
t>j νt
j+1, we sum over t up to T and get
T
X
t=1
Ξt ≤
T
X
t=1
νt
0Ξ0 + 2η2
T
X
t=1
t−1
X
j=0
νt
j+1·
  6
p(j) + M1
 M2ej + ˆζ2
+ ˆσ2

≤
T
X
t=1
νt
0Ξ0 + 12η2ˆζ2
T −1
X
j=0
T
X
t=j+1
νt
j+1
p(j)
+ 2η2(ˆσ2 + M1ˆζ2)
T −1
X
j=0
T
X
t=j+1
νt
j+1
+ 2η2
T −1
X
j=0
M2
 6
p(j) + M1

ej
T
X
t=j+1
νt
j+1
≤(1 + π0)Ξ0 + 12η2ˆζ2
T −1
X
j=0
πj
pj
+ 2η2
T −1
X
j=0
(ˆσ2 + M1ˆζ2)πj
+ 2η2
T −1
X
j=0
M2
 6
p(j) + M1

ejπj
≤(1 + π0)Ξ0 + 2η2Υ
T −1
X
j=0
ej
+ 2η2T [(ˆσ2 + M1ˆζ2)Π1(T ) + 6ˆζ2Π2(T )].
Next we move ft in (25) to the right-hand side and average
it over time to get
1
8T
T −1
X
t=0
et ≤1
T
T −1
X
t=0
ft −ft+1
η
+ ηLˆσ2
m
+ L2
T
T −1
X
t=0
Ξt. (27)
Now we plug in the bound for PT
t=1 Ξt to (27) and obtain
T −1
X
t=0
et
8T ≤f0
T η + ηLˆσ2
m
+ L2(2 + π0)Ξ0
T
+ 2L2η2Υ
T
T −1
X
j=0
ej
(28)
+ 2η2L2[(ˆσ2 + M1ˆζ2)Π1(T ) + 6ˆζ2Π2(T )].
(29)
We let α(T ) := 2L2[(ˆσ2 + M1ˆζ2)Π1(T ) + 6ˆζ2Π2(T )]. We
need the learning rate to satisfy the conditions in Lemmas A.1-
A.2. By choosing a sufﬁciently small learning rate6
η = min

( mf0
ˆσ2LT )
1
2 , (
f0
T α(T ))
1
3 ,
1
20Lκ,
1
4L(M1 + 1)

,
6While the choice of η in [34] does not explicitly include the term
(4L(M1 +1))−1, we believe that, without this additional term, the condition
required by Lemma A.1 may not hold in general.
the learning rate satisﬁes the conditions in Lemma A.1 and
A.2. Moreover, we claim that η2 ≤
1
32L2Υ. Indeed,
η2 ≤
1
400L2κ2 ≤min
j
(p(j))2
32L2(M2 + 1)(12 + 2M1p(j))
≤
1
32L2 maxj{ 2M1M2
p(j)
+ 12M2
(p(j))2 }
Let
j∗
denote
the
index
of
j
that
maximizes
maxj

6πj
p(j) + M1πj

, that is, the index of j which yields Υ.
By Lemma III.1, πj∗≤maxj
2
p(j) , so
max
j
2M1M2
p(j)
+ 12M2
(p(j))2

≥M1M2πj∗+ max
j
6M2πj∗
p(j)
≥M1M2πj∗+ 6M2πj∗
p(j∗)
= Υ.
Then it follows that η2 ≤
1
32L2Υ and
2L2η2Υ
T
T −1
X
j=0
ej ≤
T −1
X
j=0
ej
16.
(30)
In addition, since η ≤min{( mf0
ˆσ2LT )
1
2 , (
f0
T α(T ))
1
3 }, we obtain
ηLˆσ2
m
≤( ˆσ2Lf0
mT )
1
2
(31)
and
η2α(T ) ≤
f 2
0 α(T )
T 2
 1
3
(32)
respectively. Finally,
f0
T η = f0
T · max

( ˆσ2LT
mf0
)
1
2 , (T α(T )
f0
)
1
3 , 20Lκ, 4L(M1 + 1)

(33)
= max

( ˆσ2Lf0
mT )
1
2 , (f 2
0 α(T )
T 2
)
1
3 , 20Lκf0
T
, 4f0L(M1 + 1)
T

.
(34)
Combining inequalities (28)–(34) implies that
T −1
X
t=0
et
16T ≤2( ˆσ2Lf0
mT
)
1
2 + 2(f 2
0α(T )
T 2
)
1
3 + L2(2 + π0)Ξ0
T
+ 20Lκf0
T
+ 4f0L(M1 + 1)
T
.
The theorem follows by setting the right-hand side to be at
most ǫ/16.
B. Convergence Theorem for Convex Objectives
The convergence of D-PSGD for convex objectives is es-
tablished under the following assumptions:
(4) Each local objective function Fi(x) is L-Lipschitz smooth
and convex, i.e., for x, x′ ∈Rd,
Fi(x) −Fi(x′) ≤⟨∇Fi(x), x −x′⟩.
(5) There exist a constant ˆσ > 0 such that
1
m
X
i∈V
IE[∥g(x∗; ξi) −∇Fi(x∗)∥2] ≤ˆσ2.
14


--- Page 15 ---
(6) There exist a constant ˆζ > 0 such that
1
m
X
i∈V
∥∇Fi(x∗)∥2 ≤ˆζ2.
While condition (4) is stronger than condition (1), the addi-
tional convexity assumption allows conditions (5)–(6) to be
signiﬁcantly weaker than conditions (2)–(3): not only can we
set M1 = M2 = 0, but it also sufﬁces to impose the two noise
bounds only at x = x∗.
Theorem A.3. The D-PSGD under assumptions (4)–(6)
can
achieve
ǫ-convergence (i.e.,
1
T
PT −1
t=0 (IE[F(x(t))] −
Finf) ≤ǫ) when the number of iterations T satisﬁes T ≥
T4(Π1(T ), Π2(T ), π0, pmin, ǫ, x(0)) for
T4(Π1, Π2, π0, pmin, ǫ, x(0)) := r0·
(35)
O

ˆσ2
mǫ2 +
q
(ˆσ2Π1 + ˆζ2Π2)L
ǫ3/2
+ (1 + π0)LΞ0
ǫ
+
L
ǫpmin

,
(36)
where r0 := ∥x(0) −x∗∥2 (x∗denotes the optimal parameter
vector) and Ξ0 := 1
m
Pm
i=1 ∥x(0)
i
−x(0)∥2.
Convergence analyses of gossip-based algorithms on static
communication graphs, such as Decentralized Gradient De-
scent (DGD) [58], are well established. For time-varying
topologies, D-PSGD [34] establishes convergence under peri-
odic communication patterns, while gradient-tracking variants
have been analyzed in [35] and [36] under stronger assump-
tions. Our approach is most closely related to [34], but it
removes the periodicity assumption and applies to arbitrary
time-varying graphs.
Lemma A.4 (Lemma 8, [34]). Under assumptions (4)-(6), D-
PSGD with the stepsize η ≤
1
12L satisﬁes
IEt+1[∥x(t+1) −x∗∥2] ≤∥x(t) −x∗∥2
−η(F(x(t)) −F ∗) + ˆσ2η2
m
+ 3ηLΞt,
when local functions F ′
i s are convex.
Lemma A.5. Under assumptions (4)-(6), D-PSGD with the
stepsize η ≤p(t−1)
400L satisﬁes
Ξt ≤
 1 −p(t−1)
2

Ξt−1 + η2·
"
72L
p(t−1) IE
 F(x(t−1)) −Finf

+ 8ˆσ2 + 18ˆζ2
p(t−1)
#
,
when local functions Fi’s are convex.
Analogous to Lemma A.2, Lemma A.5 follows directly
from Lemma 3.1 in [15] and Lemma 8 in [34] by setting
τ = 1 and appropriately adjusting the constants.
Proof of Theorem A.3. We introduce several notations to sim-
plify the expressions:
• rt := IE[∥x(t) −x∗∥2].
• ft := IEF(x(t)) −F ∗.
• Υ :=
p
maxj(πj/p(j)).
To control the error at each t, we ﬁrst apply the standard 1-step
descent contraction by Lemma A.4 and obtain
rt+1 ≤rt −ηft + ˆσ2η2
m
+ 3ηLΞt.
(37)
Next we apply the upper bound on the consensus distance by
Lemma A.5 for every t ≥1:
Ξt ≤(1 −p(t−1)
2
)Ξt−1 + η2
"
72Lft−1
p(t−1)
+ 8ˆσ2 + 18ˆζ2
p(t−1)
#
.
(38)
We un-roll the recursion in (38) to obtain
Ξt ≤


t−1
Y
j=0
(1 −p(j)
2 )

Ξ0 + 72η2L
t−1
X
j=0

fj
p(j)
t−1
Y
i=j+1
(1 −p(i)
2 )


+ η2
t−1
X
j=1

(8ˆσ2 + 18ˆζ2
p(j) ) ·
t−1
Y
i=j+1
(1 −p(i)
2 )

.
We sum up Ξt for t = 1, . . . , T .
T
X
t=1
Ξt ≤


T
X
t=1
t−1
Y
j=0
(1 −p(j)
2 )

Ξ0
+ 72η2L
T
X
t=1
t−1
X
j=0

fj
p(j)
t−1
Y
i=j+1
(1 −p(i)
2 )


+ 18η2
T
X
t=1
t−1
X
j=0

(ˆσ2 +
ˆζ2
p(j) )
t−1
Y
i=j+1
 1 −p(i)
2



≤


T
X
t=1
t−1
Y
j=1
(1 −p(j)
2 )

Ξ0
+ 72η2L


T −1
X
j=0
fj
p(j)

X
T ≥t>j
t−1
Y
i=j+1
 1 −p(i)
2





+ 18η2
T −1
X
j=0

(ˆσ2 +
ˆζ2
p(j) )
X
T ≥t>j
t−1
Y
i=j+1
(1 −p(i)
2 )


≤π0Ξ0 + 72η2L
T −1
X
j=0
fjπj
p(j) + 18η2
T −1
X
j=0
(ˆσ2 +
ˆζ2
p(j) )πj.
We average (37) over t followed by plugging in the inequality
15


--- Page 16 ---
above to obtain
1
T
T −1
X
t=0
ft ≤1
T
T −1
X
t=0
rt −rt+1
η
+ ηˆσ2
m + 3L
T
T −1
X
t=0
Ξt
≤r0
T η + ηˆσ2
m + 3L
T ·

(1 + π0)Ξ0 + 72η2L
T −2
X
j=0
fjπj
p(j) + 18η2
T −2
X
j=0
(ˆσ2 +
ˆζ2
p(j) )πj


≤r0
T η + ηˆσ2
m + 3L(1 + π0)Ξ0
T
+ 216L2η2Υ2
T
T −2
X
j=0
fj
+ 54Lη2 ˆσ2Π1(T ) + ˆζ2Π2(T )

.
We choose the learning rate to be
η := min
(
(mr0
ˆσ2T )
1
2 , min
t
p(t)
900L,
 r0
T L(Π1(T )ˆσ2 + ˆζ2Π2(T ))
 1
3
)
.
It is straightforward to see that with the current choice η <
1
12L. Thus, η satisﬁes conditions of both Lemma A.4 and A.5.
Also, observe that by Lemma III.1 πt ≤
2
minj p(j) for all t ≥0.
Let j∗denote the index that maximizes πj/p(j). It follows that
max
j
2
(p(j))2 ≥
1
p(j∗) ·
2
minj p(j) ≥πj∗
p(j∗) ,
so we have η2 ≤(450L2Υ2)−1 since η < mint
p(t)
900L.
Now we evaluate our upper bound for this choice of η. As
η2 ≤(450L2Υ2)−1, we always get
216L2η2Υ2
T
T −2
X
j=0
fj ≤1
2T
T −1
X
j=0
fj.
Also, since η ≤
 r0
T L(Π1(T )ˆσ2+ˆζ2Π2(T ))
 1
3 , and η2 ≤mr0
ˆσ2T , we
have
Lη2 ˆσ2Π1(T )+ˆζ2Π2(T )

≤r2/3
0
T 2/3
 L(Π1(T )ˆσ2+Π2(T )ˆζ2)
1/3,
and
ηˆσ2
m ≤
r
ˆσ2r0
mT .
Finally, when η = ( mr0
ˆσ2T )
1
2 , we have
r0
T η =
r
ˆσ2r0
mT .
When η = mint
p(t)
900L, we obtain
r0
T η =
900r0L
T mint p(t) ,
and when η =
 r0
T L(Π1(T )ˆσ2+ˆζ2Π2(T ))
 1
3 we have
r0
T η ≤r2/3
0
T 2/3
 L(Π1(T )ˆσ2 + Π2(T )ˆζ2)
1/3.
Therefore,
1
T
T −1
X
t=0
ft ≤4
r
ˆσ2r0
mT + 6L(1 + π0)Ξ0
T
+
1800r0L
T mint p(t)
+ 220r2/3
0
T 2/3
 L(Π1(T )ˆσ2 + Π2(T )ˆζ2)
1/3,
and the right-hand side can be upper bounded by ǫ when T ≥
T4(Π1(T ), Π2(T ), π0, pmin, ǫ, x(0)).
C. Deferred Proofs
We now present proofs for Lemma III.1, Lemma IV.1,
Lemma IV.2, and Theorem V.2.
Proof of Lemma III.1. By the deﬁnition of πj and our as-
sumption, we have:
πj ≤
X
i>j
i−1
Y
t=j+1
(1 −δ
2) ≤
∞
X
t=0
(1 −δ
2)t =
1
1 −(1 −δ
2) = 2
δ .
We have shown Item 1, and Item 2 follows from the deﬁnitions
of Π1(T ) and Π2(T ). For Item 3, in the special case of p(t) ≡
p, we have
π0 = π1 = π2 = · · · =
∞
X
t=0
tY
j=1
(1 −p(j)
2 ).
Thus,
Π1(T ) = π0 =
∞
X
t=0
tY
j=1
(1 −p(j)
2 ) =
∞
X
t=0
(1 −p
2)t = 2
p, (39)
and
Π2(T ) = π0
p = 2
p2 ,
(40)
for any T ≥1.
Proof of Lemma IV.1. For any ﬁxed i ∈V , by Hoeffding’s
concentration inequality, we have for any y > 0,
Pr
" T
X
t=1
ci(W (t)) ≥T D + y
#
≤exp

−2y2
T D2

.
It follows a union bound that
Pr
"
max
i
T
X
t=1
ci(W (t)) ≥T D + y
#
≤m · exp

−2y2
T D2

,
and by integration over y, we obtain that
IE
"
max
i
T
X
t=1
ci(W (t))
#
≤D ·
 
T + m
r
T π
8
!
=: q(T, D),
which complete the proof.
16


--- Page 17 ---
Proof of Lemma IV.2. If j ≥τ1, then Lemma III.1 implies
that πj =
2
p2 since (p(t+τ1))t≥0 can be treated as a constant
sequence where pt+τ1 ≡p2. If j < τ1 then
πj =
∞
X
i>j
i−1
Y
t=j+1
(1 −p(t)
2 )
=
τ1
X
i>j
i−1
Y
t=j+1
(1 −p1
2 ) + (1 −p1
2 )τ1−j
∞
X
i>τ1
i−1
Y
t=τ1+1
(1 −p2
2 )
=
τ1−j
X
t=0
(1 −p1
2 )t + (1 −p1
2 )τ1−j · πτ1
= 2[1 −(1 −p1
2 )τ1−j]
p1
+ 2(1 −p1
2 )τ1−j
p2
= 2
p1
−(1 −p1
2 )τ1−j( 2
p1
−2
p2
).
Plugging the above into the deﬁnitions (6) yields the desired
results.
Proof of Lemma V.1. We verify that the mixing matrix W
satisﬁes all the constraints in (20a)-(20d):
• Since we generate W from a randomized procedure,
P
W Pr[W ] = 1 is clearly satisﬁed.
• Symmetry of W also follows from the construction.
• The constraints on row sum and column sum also follows
from the construction.
• Since off-diagonal entries are only assigned non-zero
weights in Line 14, the construction respects the topology
constraint and thus W satisﬁes (20d).
• For (20c), let i ∈V be any node such that ca
i + cb
i > D
(otherwise (20c) holds trivially). Observe that due to the
aforementioned reason,
Pr[∃j ̸= i : W [i, j] ̸= 0] ≤Pr[i ∈U],
and the latter probability is set to be at most D−ca
i
cb
i
in the
step 1. Hence,
ca
i + Pr[∃j ̸= i : W [i, j] ̸= 0]cb
i ≤ca
i + (D −ca
i )
cb
i
cb
i = D.
Therefore, W is a feasible solution to (20).
Proof of Theorem V.2. Notice if the base topology is a clique,
then Vi ≡V for every i ∈V . Hence, if i ∈U, Algorithm 2
assigns W [i, j] = 1/|U| for all j ∈U in Lines 14-15. For
any i, j ∈V such that i ̸= j, we have
IE[(W ⊤W )[i, j]] = IE
" m
X
k=1
W [i, k]W [k, j]1[i, j, k ∈U]
#
= ωiωjIE
" m
X
k=1
W [i, k]W [k, j]1[k ∈U] | i, j ∈U
#
= ωiωjIE
"
|U|−2
m
X
k=1
1[k ∈U] | i, j ∈U
#
= ωiωjIE
 1
|U| | i, j ∈U

∼m ωiωjIE
 1
|U| | U ̸= ∅

= ωiωjm⊥.
Using the observation that m⊥tends to 0 as m grows, we
compute asymptotically the diagonal entries as
IE[(W ⊤W )[i, i]]
=
 1 −ωi

+ ωi · IE
"
|U|−2
m
X
k=1
1[k ∈U] | i ∈U
#
∼m 1 −ωi + ωim⊥∼m 1 −ωi.
By combining the steps above, we obtain that
ρ(W ) = ∥IE[W ⊤W ] −J∥
∼m
m⊥ωω⊤+ diag(1 −ω) −1
m11⊤
 ,
as desired.
Proof of Lemma VI.2. Let τ ′ be the ﬁrst iteration k when for
every e ∈E there exists an iteration j ≤k such that e ∈Ej.
We ﬁrst prove that τ ≤τ ′ by showing that ρτ ′ < 1. For any
ǫ ∈(0, 1), consider a probability vector
p′ := (p0 = 1 −ǫ, p1 = ǫ/τ ′, . . . , pτ ′ = ǫ/τ ′).
By taking ǫ sufﬁciently small, the constraint (24c) can be
satisﬁed by p′. As ρτ ′ is obtained by minimizing the problem
deﬁned in (24), we have
ρτ ′ ≤

(1 −ǫ)I + ǫ
τ ′
τ ′
X
s=1
W ⊤
s Ws −J

=: ρ∗.
Denote
1
τ ′
Pτ ′
s=1 Ws by W . By triangle inequality, we get
that
ρ∗≤(1 −ǫ) + ǫ

1
τ ′
τ ′
X
s=1
(W ⊤
s Ws −J)

According to the weight assignment in Lines 6-9, every Ws is
symmetric and double-stochastic. Thus W is also symmetric
17


--- Page 18 ---
and double-stochastic. For any double-stochastic matrix W
we have W ⊤J = J and ∥W ∥≤1, and it follows that

1
τ ′
τ ′
X
s=1
(W ⊤
s Ws −J)

=

1
τ ′
τ ′
X
s=1
(W ⊤
s Ws −W ⊤
s J)

=

1
τ ′
τ ′
X
s=1
W ⊤
s (Ws −J)

≤max
s
W ⊤
s (W −J)

≤max
s
W ⊤
s
 · ∥W −J∥≤∥W −J∥.
Moreover, by the choice of τ ′, W is an irreducible matrix,
and thus by Perron-Frobenius theorem, W has the following
eigenvalues:
1 = λ1(W ) > λ2(W ) > · · · > λm(W ) > −1,
and the eigenvector corresponding to the leading eigenvalue
is the all-one vector 1. On the other hand, J has only 1 non-
zero eigenvalue whose eigenvector is also the all-one vector
1. Hence,
∥W −J∥= max(|λ2(W )|, |λm(W )|) < 1,
and this implies that
ρτ ′ ≤ρ∗≤(1 −ǫ) + ǫ∥W −J∥< 1.
Next we prove that there exists C > 0 such that
Pr[τ′ > Cm] ≤e−Ω(m),
from which the lemma follows. By assumption, there is a
constant c > 0 such that for each e ∈E and every iteration
k, e /∈Ek occurs with probability at most 1 −c. Then e /∈Ej
for any of j = 1, . . . , t iterations occurs with probability at
most (1 −c)t. By a union bound over e ∈E,
Pr[τ′ > t] ≤m2 · (1 −c)t ≤m2 · exp (−ct).
By letting t = Cm/c with C sufﬁciently large, the right-hand
side is at most e−Ω(m).
18
