--- Page 1 ---
SCIENCE CHINA
Information Sciences
. RESEARCH PAPER .
Cross-reality Location Privacy Protection in 6G-enabled
Vehicular Metaverses: An LLM-enhanced Hybrid
Generative Diffusion Model-based Approach
Xiaofeng Luo1, Jiayi He1, Jiawen Kang1*, Ruichen Zhang2, Zhaoshui He1,
Ekram Hossain3 & Dong In Kim4
1School of Automation, Guangdong University of Technology, Guangzhou 510006, China
2School of Computer Science and Engineering, Nanyang Technological University, Singapore 639798, Singapore
3Department of Electrical and Computer Engineering, University of Manitoba, Winnipeg, MB R3T 2N2, Canada
4Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon 16419, South Korea
Abstract
The emergence of 6G-enabled vehicular metaverses enables Autonomous Vehicles (AVs) to operate across physical and virtual
spaces through spaceâ€“airâ€“groundâ€“sea integrated networks. The AVs can deploy Artificial Intelligence (AI) agents powered by large AI
models as personalized assistants, on edge servers to support intelligent driving decision making and enhanced on-board experiences.
However, such cross-reality interactions may cause serious location privacy risks, as adversaries can infer AV trajectories by correlating
the location reported when AVs request Location-Based Services (LBS) in reality with the location of the edge servers on which their
corresponding AI agents are deployed in virtuality.
To address this challenge, we design a cross-reality location privacy protection
framework based on hybrid actions, including continuous location perturbation in reality and discrete privacy-aware AI agent migration
in virtuality.
In this framework, a new privacy metric, termed cross-reality location entropy, is proposed to effectively quantify the
privacy levels of AVs. Based on this metric, we formulate an optimization problem to optimize the hybrid action, focusing on achieving
a balance between location protection, service latency reduction, and quality of service maintenance. To solve the complex mixed-integer
problem, we develop a novel LLM-enhanced Hybrid Diffusion Proximal Policy Optimization (LHDPPO) algorithm, which integrates
Large Language Model (LLM)-driven informative reward design to enhance environment understanding with double Generative Diffusion
Models (GDMs)-based policy exploration to handle high-dimensional action spaces, thereby enabling reliable determination of optimal
hybrid actions. Extensive experiments on real-world datasets demonstrate that the proposed framework effectively mitigates cross-reality
location privacy leakage for AVs while maintaining strong user immersion within 6G-enabled vehicular metaverse scenarios.
Keywords
Location privacy, vehicular metaverse, large AI model, generative diffusion model, 6G networks, agentic AI
Citation
Luo X F, He J Y, Kang J W, et al.
Cross-reality Location Privacy Protection in 6G-enabled Vehicular Metaverses: An
LLM-enhanced Hybrid Generative Diffusion Model-based Approach. Sci China Inf Sci, for review
1
Introduction
The vehicular metaverse is a mobility metaverse paradigm blending of reality and virtuality, which is tailored
to enhance intelligence and interactivity in vehicular networks [1, 2]. With the advancement of sixth-generation
(6G) cellular technology, 6G-enabled vehicular metaverses have emerged to provide ultra-reliable and low-latency
connectivity across spaceâ€“airâ€“groundâ€“sea integrated networks, enabling uninterrupted and high-quality services for
on-board users [3]. In this paradigm, due to the limited computing and storage resources of Autonomous Vehicles
(AVs), AVs typically deploy Artificial Intelligence (AI) agents powered by large AI models as their personalized
assistants on edge servers, where these edge servers can be terrestrial Roadside Units (RSUs) or Low-Earth Orbit
(LEO) satellite nodes [4, 5]. By hosting AI agents and executing computation-intensive tasks, these edge servers
enable resource-constrained AVs to access large AI model-based services, thereby enhancing driving safety and on-
board immersive experience. In this way, AVs exhibit agentic AI characteristics by leveraging perception, reasoning,
execution, and continuous learning capabilities to interact with physical and virtual spaces in an iterative and
context-aware manner [6].
Although the 6G-enabled vehicular metaverse empowered by agentic AI enables immersive services and intelligent
decision making across physical and virtual spaces, privacy risks become more prominent in such tightly coupled
cross-reality systems than traditional vehicular networks [7,8]. Specifically, the immersion of on-board users within
* Corresponding author (email: kavinkang@gdut.edu.cn)
arXiv:2601.12311v1  [cs.NI]  18 Jan 2026


--- Page 2 ---
Sci China Inf Sci
2
the vehicular metaverse is highly correlated to two key factors, i.e., reported location precision and service response
latency [9]. On the one hand, agentic AI-driven AVs in the physical space often request Location-Based Services
(LBS) frequently, such as Augmented Reality (AR) navigation and AR interactive games, where higher reported
location precision ensures better Quality of Service (QoS) but also incurs a higher risk of location privacy leakage [10].
On the other hand, due to the continuous mobility of AVs, their corresponding AI agents in the virtual space are
frequently migrated among edge servers in alignment with physical vehicle movements to maintain physicalâ€“virtual
synchronization and low-latency service provisioning, which further amplifies privacy risks [2, 11]. Consequently,
collusive adversaries can exploit their prior knowledge of AV routine mobility patterns, such as regular commuting
routes, to launch inference attacks that jointly reconstruct vehicle trajectories and AI agent migration patterns,
thereby accurately tracking target AVs [12]. This cross-reality linkage attack is more severe and harder to mitigate
than traditional single-domain privacy attacks, posing substantial threats to the location privacy of on-board users.
Existing research on cross-reality location privacy preservation in vehicular metaverses has primarily focused
on anonymity-based schemes [5, 9], which aim to obscure the linkage between vehicle identities and locations.
However, these approaches often suffer from significant performance degradation in low-traffic environments and
fail to meet the stringent privacy requirements of AVs [13]. These limitations highlight the need for a general
solution capable of protecting cross-reality location privacy in 6G-enabled vehicular metaverses. Moreover, most
existing location privacy metrics consider only a single dimension, typically confined to the physical space, and
therefore cannot accurately capture privacy leakage risks arising from cross-reality interactions. Consequently, a
more comprehensive metric is required to effectively evaluate location privacy under cross-reality inference attacks.
Furthermore, strengthening privacy protection at a large scale inevitably increases service response latency and
degrades QoS, thereby significantly diminishing user immersion in metaverse environments. As a result, achieving an
effective balance among cross-reality location privacy protection, latency reduction, and QoS maintenance remains
a challenging and open research problem.
To address the above challenges, this paper proposes a user-centric cross-reality location privacy protection
framework based on hybrid action decisions, including the continuous location perturbation in reality and discrete
privacy-aware AI agent migration in virtuality. The main contributions of this paper are summarized as follows.
â€¢ We present a cross-reality location privacy protection framework for 6G-enabled vehicular metaverses to
mitigate privacy leakage risk caused by spatiotemporal correlations between physical and virtual spaces.
The
framework integrates a hybrid action mechanism that combines continuous location perturbation in reality with
discrete privacy-aware AI agent migration in virtuality to preserve the location privacy of agentic AI-driven AVs.
â€¢ In the proposed framework, a new privacy metric named cross-reality location entropy is designed to quantify
the real-time location privacy levels of AVs under cross-reality inference attacks. The metric is derived from a
multi-condition Bayesian posterior distribution and captures the uncertainty of actual vehicle locations introduced
by hybrid action strategies. Based on this metric, a mixed-integer optimization problem that jointly considers cross-
reality location privacy, service response latency, and QoS loss to optimize hybrid action mechanisms is formulated.
â€¢ To solve the formulated complex problem in vehicular metaverse environments, we propose a novel LLM-
enhanced Hybrid Diffusion Proximal Policy Optimization (LHDPPO) algorithm, which incorporates Large Lan-
guage Model (LLM)-driven informative reward design for enhanced environment understanding and Generative
Diffusion Model (GDM)-based policy exploration for improved learning performance in high-dimensional action
spaces.
â€¢ Extensive experiments using real-world taxi mobility and base station datasets demonstrate that the proposed
framework achieves superior performance in terms of cross-reality location privacy preservation, immersive services
response latency reduction, and QoS maintenance when compared with existing baseline schemes.
2
Related Work
2.1
Location Privacy Protection in Cross-reality Systems
Location privacy protection has become an active research topic in cross-reality systems, where existing solu-
tions are generally classified into encryption-based, anonymization-based, and obfuscation-based schemes [14]. For
encryption-based approaches, Liu et al. [15] proposed BlockSC, a blockchain-empowered spatial crowdsourcing
framework that preserves location privacy in metaverses through spatial data encryption. Although effective in se-
curing sensitive information, the heavy computational overhead of cryptographic operations limits its suitability for
real-time services in 6G-enabled vehicular metaverses. Anonymization-based schemes aim to weaken the association
between users and their trajectories by concealing real identities. For instance, Luo et al. [12] developed a dual


--- Page 3 ---
Sci China Inf Sci
3
pseudonym change scheme to achieve identity anonymization in vehicular metaverses, but its effectiveness degrades
in sparse traffic areas, and it cannot fully realize location concealment. Obfuscation-based schemes protect privacy
by perturbing reported positions to reduce adversarial inference accuracy. For instance, Wang et al. [16] proposed
a perturbed sliding task queue algorithm based on differential privacy to protect sensitive user location information
prior to task offloading in a digital twin architecture, yet it considers only physical space location perturbation and
neglects interactions in the virtual space. Therefore, there is still a lack of practical solutions that are capable of
preserving cross-reality location privacy in 6G-enabled vehicular metaverses.
2.2
Location Privacy Metrics
Location privacy metrics provide quantitative tools for evaluating protection schemes in mobile networks. Emara
et al. [17] showed that widely used metrics in VANETs, such as anonymity set size, entropy, and adversary success
rate, each capture only a partial aspect of privacy and may yield inconsistent evaluations across scenarios. Li et
al. [18] proposed distance and visibility-based metrics for navigation services, but the metrics remain confined to
physical trajectories and ignore virtual space privacy leakage. To model inference from service migration, an entropy-
based metric proposed in [11] quantifies location uncertainty under MEC service placement, although it considers
only location exposure during service migration and neglects sensitive broadcast information in the physical space.
More recently, Kang et al. [5] introduced the Degree of Privacy Entropy (DoPE) to assess pseudonym changes in
vehicular edge metaverses, yet DoPE targets identity anonymity within pseudonym-based schemes and does not
apply to perturbation-based mechanisms.
Overall, existing metrics evaluate privacy within a single domain or
narrow application scope and cannot capture cross-reality linkability between broadcast messages and metaverse
activities, revealing the need for location privacy metrics tailored to 6G-enabled vehicular metaverses.
2.3
Diffusion Model-based Reinforcement Learning for Decision Optimization
Recent progress in GDMs has demonstrated strong potential in solving complex optimization problems. Du et
al. [19] surveyed GDM-based Deep Reinforcement Learning (DRL) and presented a power allocation example that
GDMs can capture high-dimensional structures and generate high-quality decisions. Building on this insight, Ning
et al. [20] proposed a two-level GDM-based soft actor critic framework for effective resource management, but their
method supports only a single action type and has limited applicability. Hence, Kang et al. [21] introduced a hybrid
GDM-based DRL algorithm for digital twin migration in vehicular metaverses, jointly generating migration and
offloading actions. However, their method relies on traditional RL relaxation techniques to handle hybrid actions,
which may destabilize learning and cause suboptimal convergence during training.
To further improve hybrid
action modeling, Wang et al. [22] embedded GDMs into a hybrid proximal policy optimization algorithm through
a parameterized action formulation to optimize spectral efficiency in LEO Satellite networks. Although effective
for parameterized decision tasks, this structure imposes mutual exclusivity among actions and limits exploration
of new hybrid action pairs. Moreover, existing GDM-based DRL algorithms for optimization still depend entirely
on manually crafted reward functions, which may neglect the potentially important components in intricate cross-
reality environments. These limitations highlight the need for a new GDM-based DRL algorithm that decouples
hybrid actions and leverages large AI models to design informative reward functions.
3
Cross-reality Location Privacy Protection Framework in 6G-enabled Vehicular
Metaverses
In this section, we propose a cross-reality location privacy framework based on hybrid action mechanisms in 6G-
enabled vehicular metaverses. As illustrated in Fig. 1, each AV in reality is associated with an AI agent in virtuality,
which is hosted on one of the edge servers. Firstly, the key components of the 6G-enabled vehicular metaverse
are described in the network model. Subsequently, the core hybrid action mechanism is presented, focusing on
safeguarding location privacy across both physical and virtual spaces.
3.1
Network Model
â€¢ Agentic AI-driven AVs: Agentic AI-driven AVs operating in the physical space act as primary participants
in 6G-enabled vehicular metaverses. Each AV can request customized LBS, such as AR navigation, to enhance
the on-board experience throughout the journey. Although these services are essential for user immersion, the
associated service requests inevitably disclose sensitive location information, which can be exploited by adversaries
to track AV movements [10].


--- Page 4 ---
Sci China Inf Sci
4
Drive out of communication 
range of the original edge server 
1
Run LHDPPO algorithm to 
output hybrid actions
2
a. Perturb location (continuous)
b. Migrate AI agent (discrete)
3
Eavesdrop LBS requests 
with perturbed location
4
Upload AI agent update and 
task offloading requests
5
Steal location information of 
the migrated AI agent
7
Coordinate with LBS providers 
to process AI agent tasks
6
Infer the location of target 
AV in the physical space
8
Download task results 
without privacy leakage
9
Actual 
Location
Inferred 
Location
4
7
9
8
8
3
5
2
1
3
Privacy
at risk
Privacy
protection
Initial 
Location
Connected
Server
Connected
Server
6G base station 
for remote areas
ğ’ğ’ğ’‚ğ’‚
ğ’ğ’ğ’ƒğ’ƒ
ğ’ğ’ğ’„ğ’„
(Agent Server)
(Agent Server)
6
a
b
RSU
Inference 
Attack
Share location
for LBS
Agentic AI-
driven AV
AI agent
Real 
trajectory
Fake 
trajectory
Wired 
link
Adversary
Wireless 
link
Information 
theft
LEO
satellite
Figure 1
Workflow of the proposed cross-reality location privacy protection framework based on hybrid actions in 6G-enabled vehicular
metaverses. The sequentially numbered arrows depict the main processes: (a) The target AV at privacy risks moves and executes hybrid actions,
i.e., the continuous location perturbation and discrete privacy-aware AI agent migration (Steps 1
â—‹to 3
â—‹). (b) The AV requests LBS and offloads
AI agent tasks to edge servers, while malicious adversaries exploit the illegally obtained information of the target AV to launch inference attacks
(Steps 4
â—‹to 8
â—‹). (c) The on-board user obtains immersive metaverse applications without compromising location privacy (Step 9
â—‹).
â€¢ AI agents: AI agents residing in the virtual space act as intelligent assistants for their corresponding AVs.
By directly interacting with other AI agents and LBS providers, AI agents empowered by large AI models can assist
AVs in processing computation-intensive tasks and providing decision-support functionalities. However, frequent
interactions and continuous migrations of AI agents may inadvertently leak location-related information [12]. To
tackle the privacy threats, the large AI model-based AI agents can support AVs with agentic AI capacities to
determine hybrid actions for cross-reality location privacy protection.
â€¢ Edge Servers: Edge servers function as the bridge between the physical and virtual spaces in 6G-enabled
vehicular metaverses.
Equipped with larger computing and storage resources than AVs, they manage the full
lifecycle of AI agents, including creation, synchronization, execution, and migration. In the 6G-enabled vehicular
metaverse, edge servers can be terrestrial RSUs or LEO satellites, where satellite nodes act as 6G base stations to
compensate for poor terrestrial connectivity in remote areas [23].
â€¢ Adversaries: Adversaries may exist in both physical and virtual spaces and are considered to possess prior
knowledge about target AVs. This consideration is reasonable, as the driving route and social interaction pattern of
an AV often exhibit temporal regularity [11]. In the physical space, adversaries can eavesdrop on broadcast sensitive
information of AVs when they request LBS. In the virtual space, adversaries may be malicious AI agents or metaverse
service providers aiming to infer user locations, particularly during AI agent migrations. Furthermore, adversaries
across physical and virtual spaces may share information to launch collusive inference attacks to associate an AV
with its AI agent, posing a significant threat to the location privacy of on-board users [12].
3.2
Hybrid Action-based Cross-reality Privacy Protection Mechanism
To conceal user location in 6G-enabled vehicular metaverses, the proposed cross-reality location privacy protection
framework adopts a hybrid action mechanism consisting of a continuous action for location perturbation in the
physical space and a discrete action for privacy-aware AI agent migration in virtual spaces. See the details below.


--- Page 5 ---
Sci China Inf Sci
5
3.2.1
Continuous actionâ€”location perturbation in reality
We consider there are multiple agentic AI-driven AVs, multiple RSUs, and an LEO satellite in an observation
area.
Specifically, we denote an AV set M = {1, . . . , m, . . . , M} with a total of M AVs and an RSU set N =
{1, . . . , n, . . . , N} with a total of N RSUs, while the index of the LEO satellite is denoted as S. In the physical
space, location perturbation (also called location obfuscation) can be applied to AV trajectories to reduce the
risk of direct location exposure [13].
Each AV introduces controlled perturbations to its reported coordinates
when requesting LBS. Notably, larger perturbations increase privacy by enlarging the uncertainty of an adversaryâ€™s
inference but also weaken QoS [10].
Hence, the perturbation intensity can be modeled as a continuous action
variable that is adjusted based on the privacy and QoS requirements of AVs.
The planar Laplacian perturbation mechanism proposed in [24] provides a principled approach to achieving
geo-indistinguishability based on differential privacy. This method samples random noise from a two-dimensional
Laplace distribution and perturbs the original location to satisfy Ïµ-geo-indistinguishability, where Ïµ denotes the
privacy budget. Inspired by this approach, the proposed framework adopts a similar mechanism to implement
continuous location perturbation in the physical space.
Let lt
m = (xt
m, yt
m) denote the true location of AV m at time slot t, where xt
m and yt
m represent the longitude
and latitude, respectively. In a polar coordinate system centered at lt
m, any relative location lx can be represented
as (rt
m, Î¸t
m), where rt
m denotes the perturbation radius and Î¸t
m denotes the perturbation angle with respect to the
horizontal axis. According to [24], the Probability Density Function (PDF) of the polar Laplacian distribution is
DÏµ(rt
m, Î¸t
m) = Î¸t
m
2
2Ï€ rt
meâˆ’Ïµrt
m.
(1)
Eq. (1) reflects that perturbation is jointly determined by the continuous random variables rt
m and Î¸t
m. Hence, to
perform location perturbation at time slot t, AV m only needs to select a continuous action policy ct
m = {rt
m, Î¸t
m}.
Given the selected rt
m and Î¸t
m âˆˆ[0, 2Ï€), the perturbed location Ëœlt
m is computed as
Ëœlt
m = (Ëœxt
m, Ëœyt
m) = (xt
m + rt
m cos Î¸t
m, yt
m + rt
m sin Î¸t
m).
(2)
The detailed procedure for determining ct
m with the LLM-enhanced learning approach is presented in Section 5.
3.2.2
Discrete actionâ€”privacy-aware AI agent migration in virtuality
In the virtual domain, AI agent migration serves not only to sustain continuous service provisioning under the
mobility of AVs, but also as an effective mechanism for enhancing location privacy [11]. By dynamically changing
the hosting edge server of an AI agent, the one-to-one correspondence between an AV and a specific server can be
disrupted, thereby introducing uncertainty into the inferred locations of AVs and reducing privacy risks. However,
if an AV always migrates its AI agent to the server offering the strongest signal strength, an adversary continuously
monitoring the target AI agent can still reconstruct the real trajectory of the target AV. Conversely, migrating the
AI agent to a distant edge server may increase service response latency and degrade user immersion in 6G-enabled
vehicular metaverses. To address this tradeoff, privacy-aware AI agent migration is modeled as a discrete action,
where an AV optimally selects whether and where to migrate its agent among edge server candidates, achieving a
dynamic balance between location privacy protection and service latency reduction.
Let ct
m denote the index of the edge server connected to AV m at time slot t. In a 6G-enabled vehicular metaverse,
each AV is connected to exactly one edge server in each time slot, which is expressed as 1(ct
m âˆˆN)+1(ct
m = S) = 1,
where 1(Â·) is the indicator function. Meanwhile, let et
m denote the index of the edge server that hosts the AI agent
of AV m, referred to as the agent server. As illustrated in Fig. 1, the glowing RSU na âˆˆN represents the edge server
providing the strongest signal to AV m at its initial position, serving both as the connected edge server and the
initial agent server. When AV m moves out of the coverage range of RSU na, it connects to another RSU nb âˆˆN
that offers the strongest signal at the new position. However, the connected server nb is not necessarily selected as
the agent server. After evaluating the current location privacy level, AV m may instead choose an alternative RSU
nc (nc âˆˆN) to balance privacy protection and service latency based on its discrete action decision policy dt
m = nc.
Therefore, to protect location privacy in the virtual space, the agentic AI-driven AV m can determine its discrete
action policy dt
m = {et
m | 1(et
m âˆˆN) + 1(et
m = S) = 1} for privacy-aware AI agent migration in time slot t. The
detailed procedure for determining dt
m with the LLM-enhanced learning approach is presented in Section 5.


--- Page 6 ---
Sci China Inf Sci
6
4
System Model
Based on the above analysis, the core of the proposed framework for achieving cross-reality location privacy pro-
tection is to determine the optimal hybrid action, striking a balance between location privacy protection and user
immersion. To achieve this, we design a new privacy metric named cross-reality location entropy to evaluate privacy
levels under the hybrid action mechanisms. Meanwhile, the service response latency and QoS loss are explicitly
modeled. By integrating these components, the overall optimization problem is formulated in Section 4.4.
4.1
New Privacy Metric: Cross-reality Location Entropy
Before determining the hybrid action for cross-reality location privacy protection, it is essential for AV m to evaluate
its current privacy level. When the assessed privacy level is high, AV m can prioritize its immersive experience
within the metaverse, meaning that it can request LBS with accurate locations and deploy its AI agent on a nearby
edge server.
In contrast, AV m can adopt a more aggressive protection strategy, requesting LBS with largely
perturbed positions and migrating its AI agent to a more distant edge server.
As discussed in Section 3.1, adversaries can obtain partial historical movement trajectories of target AVs. With
this side information, an adversary can analyze the migration pattern of the target AV and conduct a Bayesian
location inference attack, which is a type of knowledge-based attack [11]. Combining past movement traces and
migration patterns, adversaries can build a mapping relationship between AV location in reality and AI agent
location in virtuality, learning the probability distribution of the AVâ€™s real-time actual position. Specifically, let
Pr(Ë˜lt
m = pa), Pr(Ëœlt
m = pb), and Pr(Ë†lt
m = pc) represent the probability of the inferred AV location, perturbed AV
location, and corresponding AI agent location of AV m is located at the position pa, pb, and pc, respectively. Then,
the posterior distribution of AV mâ€™s location is obtained by observing the perturbed AV location in reality and the
AI agent location in virtuality based on the multi-condition Bayesâ€™ theorem, given by
Pr(Ë˜lt
m = pa | Ëœlt
m = pb, Ë†lt
m = pc) = Pr(Ë˜lt
m = pa, Ëœlt
m = pb, Ë†lt
m = pc)
Pr(Ëœltm = pb, Ë†ltm = pc)
=
Pr(Ëœlt
m = pb, Ë†lt
m = pc | Ë˜lt
m = pa) Pr(Ë˜lt
m = pa)
P
pxâˆˆPtm Pr(Ëœltm = pb, Ë†ltm = pc | ltm = px) Pr(ltm = px)
,
(3)
where Pt
m is the set of potential physical locations of AV m in time slot t.
Since the perturbed location and
AI agent location are each determined solely based on the hybrid action strategy of AV m, the two conditional
probabilities can be treated as mutually independent when the physical AV location is given. In this case, the
posterior distribution in Eq. (3) is transformed into
Pr(Ë˜lt
m = pa | Ëœlt
m = pb, Ë†lt
m = pc) =
Pr(Ëœlt
m = pb | Ë˜lt
m = pa) Pr(Ë†lt
m = pc | Ë˜lt
m = pa) Pr(Ë˜lt
m = pa)
P
pxâˆˆPtm Pr(Ëœltm = pb | ltm = px) Pr(Ë†ltm = pc | ltm = px) Pr(ltm = px)
.
(4)
To assess the privacy levels of AVs in 6G-enabled vehicular metaverses, we design a new physical-virtual location
privacy metric based on information entropy, referred to as cross-reality location entropy.
Definition 1.
(Cross-reality Location Entropy) In the 6G-enabled vehicular metaverse, the cross-reality
location entropy of an AV m at time slot t is defined as
Et
m = âˆ’
X
paâˆˆPtm
Pr(Ë˜lt
m = pa | Ëœlt
m = pb, Ë†lt
m = pc) log2

Pr(Ë˜lt
m = pa | Ëœlt
m = pb, Ë†lt
m = pc)

.
(5)
Notably, the cross-reality location entropy in Eq. (5) is established based on the concept of entropy in information
theory [25]. A larger entropy value implies a lower accuracy of the adversaryâ€™s inference regarding the actual location
of the target AV, indicating a lower risk of location privacy leakage. With this metric, AVs can evaluate their
location privacy levels across physical and virtual spaces, thereby guiding hybrid action mechanisms to maintain
an appropriate balance between location privacy and user immersion in 6G-enabled vehicular metaverses.
4.2
Service Response Latency Model
4.2.1
Communication model
The communication model describes the wireless transmission processes between AVs and edge servers. To maintain
accurate physicalâ€“virtual synchronization and receive high-quality metaverse services, AVs should upload real-time
sensing data to edge servers for updating their Large AI model-empowered AI agents and offloading computation


--- Page 7 ---
Sci China Inf Sci
7
tasks through uplink channels [9]. Consider the case where the AI agent of AV m is deployed on RSU n in the
virtual space during time slot t, i.e., et
m = n. Let Ë†lt
m = ln = {xn, yn} denote the position of agent server et
m, which
corresponds to the fixed geographical coordinates of RSU n, with xn and yn representing its longitude and latitude,
respectively. The longitudinal difference between AV m and RSU n is thus given by âˆ†xt
m,n =
Ï€
180 (xt
m âˆ’xn), while
the latitudinal difference is âˆ†yt
m,n =
Ï€
180(yt
m âˆ’yn). Therefore, the haversine of the central angle Î¸t
m,n (in radians) is
hav(Î¸t
m,n) = sin2(Î¸t
m,n
2
) = sin2(âˆ†xt
m,n) + cos( Ï€
180yt
m) cos( Ï€
180yn) sin2(âˆ†yt
m,n).
(6)
From Eq. (6), we know that Î¸t
m,n = 2 arcsin
q
hav(Î¸tm,n). Then, the actual distance between AV m and RSU n is
dt
m,n = RÎ¸t
m,n = 2R arcsin
q
hav(Î¸tm,n),
(7)
where R = 6371 km denotes the radius of the Earth.
For vehicular systems, the Rayleigh fading model is commonly used to characterize the wireless channel between
RSUs and AVs [2]. The channel gain is defined as ht
m,n = A
h
c
4Ï€fcdtm,n
iÎ¾
, where c = 3 Ã— 108 m/s is the speed of
light, A is the antenna gain, fc is the carrier frequency, and Î¾ is the path loss factor [2]. Therefore, the achievable
uplink data rate from AV m to the server ct
m is
Rup
m,n(t) = Bup
m,n log2

1 + pmht
m,n
N0

,
(8)
where Bup
m,n is the uplink bandwidth allocated to AV m when connected to RSU n, pm is the transmit power of AV
m, and N0 is the average noise power. Considering two types of edge servers, the uplink communication latency is
Lup
m (t) = 1(ct
m âˆˆN) Dup
m (t)
Rup
m,n(t) + 1(ct
m = S)Dup
m (t)
Rup
sat
,
(9)
where Dup
m (t) is the amount of data uploaded by AV m for AI agent updates and task offloading, and Rup
sat denotes
the average ground-to-satellite upload speed in Starlink1).
In addition to uplink transmission, AVs also receive customized AI agent task results from the RSU via the down-
link channel to obtain personalized metaverse services (e.g., AR navigation to a preferred destination). Considering
channel reciprocity, the uplink and downlink share identical path-loss characteristics [21]. Accordingly, the downlink
data rate is Rdown
m,n (t) = Bdown
m,n log2

1 + pnhm,n(t)
N0

, where Bdown
m,n
denotes the downlink bandwidth allocated to AV
m when connected to edge server ct
m, and pn is transmit power of RSU n. To receive AI agent task results of size
Ddown
m
(t), the downlink communication latency in time slot t is given by
Ldown
m
(t) = 1(ct
m âˆˆN)Ddown
m
(t)
Rdown
m,n (t) + 1(ct
m = S)Ddown
m
(t)
Rdown
sat
,
(10)
where Rdown
sat
denotes the average ground-to-satellite download speed in Starlink1).
Since the connected edge server may differ from the agent server, the communication model also accounts for the
backhaul latency incurred during data transmission between them. This latency only arises in ground-to-ground
RSU links, as satellite nodes typically possess sufficient onboard computing resources, eliminating the need for
AVs connected to satellites to offload AI agent tasks to terrestrial RSUs. The backhaul latency consists of two
components: the wired transmission latency and the relay latency, where the latter is influenced by processing,
forwarding, and queuing delays along the relay path. Therefore, the backhaul latency is expressed as
Lback
m
(t) = 1(et
m = n)

1(ct
m Ì¸= et
m)Dup
m (t) + Ddown
m
(t)
RN
+ 2Ï†back
t
dctmâ†’etm

.
(11)
Here, RN is the wired transmission rate between RSUs for AI agent migrations, Ï†back
t
is a positive coefficient of the
backhaul latency, and dct
mâ†’et
m is the hop count between the connected edge server ct
m and the agent server et
m.
Finally, the total communication latency is calculated by
Lcomm
m
(t) = Lup
m (t) + Lback
m
(t) + Ldown
m
(t).
(12)
1) https://www.starlink.com/technology


--- Page 8 ---
Sci China Inf Sci
8
4.2.2
Migration model
According to Section 3.2.2, a moving AV m executes a discrete action, namely privacy-aware AI agent migration,
to balance location privacy and service latency, which introduces migration latency. Let Sm denote the AI agent
model size of AV m. The incurred migration latency of AV m between the previous agent server and the current
one is then computed as
Lmig
m (t) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
0,
et
m = etâˆ’1
m ,
Sm
RN + Ï†migdetâˆ’1
m
â†’etm,
et
m = n, etâˆ’1
m
âˆˆ(N \ {n}),
Sm
Rup
sat ,
et
m = S, etâˆ’1
m
âˆˆN,
Sm
Rdown
sat
,
et
m âˆˆN, etâˆ’1
m
= S,
(13)
where Ï†mig denotes the RSU-to-RSU AI migration latency coefficient.
4.2.3
Computation model
In addition to wireless communication and migration operations, the process of delivering metaverse services to AVs
by AI agents also incurs computation latency at the agent server. Specifically, the agent server et
m is responsible
for AI agent updates and task processing for the agentic AI-driven AV m, resulting in the computation latency as
Lcomp
m
(t) = 1(et
m âˆˆN)cmDtask
m
(t)
fn
+ 1(et
m = S)cmDtask
m
(t)
fsat
.
(14)
Here, cm is the number of Central Processing Unit (CPU) cycles to process per unit data uploaded from AV m (in
CPU Cycles/bit), and Dtask
m
(t) = Î¹Dup
m (t) is the size of the requested AI agent task created based on the upload data
with Î¹ denoting the conversion factor. fn and fsat are the CPU frequency of RSU n and satellite S, respectively.
Finally, by integrating the communication latency, migration latency, and computation latency, the total service
response latency experienced by AV m in time slot t is expressed as
Lt
m = Lcomm
m
(t) + Lmig
m (t) + Lcomp
m
(t).
(15)
4.3
Penalties for QoS Loss
QoS is another critical requirement in 6G-enabled vehicular metaverses. When agentic AI-driven AVs travel and
request splendid LBS, injecting excessive noise into their status messages can decline the QoS, thus leading to
poor metaverse experiences, such as offset scenario rendering and improper site recommendation.
To prevent
such situations, a penalty term for QoS loss regarding the distance between the actual and perturbed locations is
introduced to constrain large-scale continuous actions. According to Eq. (2), the perturbed location of AV m after
applying the continuous action is Ëœlt
m = (Ëœxt
m, Ëœyt
m). Referring to the distance calculation formula in Eq. (7), we define
the penalty for QoS loss as
Qt
m = ln

1 + 2R arcsin
r
sin2  Ï€
180(xtm âˆ’Ëœxtm)

+ cos( Ï€
180ytm) cos( Ï€
180 Ëœytm) sin2 
( Ï€
180(ytm âˆ’Ëœytm)

.
(16)
4.4
Problem Formulation
Generally, the proposed framework requires each agentic AI-driven AV to determine how to perturb its physical
location and migrate its AI agent to protect cross-reality location privacy and ensure user immersion by maintaining
service response latency and QoS. Consequently, the utility function of AV m is defined as
ut
m = Ï‰EEt
m âˆ’Ï‰LLt
m âˆ’Ï‰QQt
m,
(17)
where Ï‰E, Ï‰L, and Ï‰Q represent the weighting factors for the cross-reality location entropy, service response latency,
and QoS loss, respectively. Finally, the overall optimization problem of the proposed hybrid action mechanisms in
the 6G-enabled vehicular metaverse is formulated as
P1 :
max
rtm,Î¸tm,etm
1
T
1
M
T
X
t=1
M
X
m=1
ut
m,
(18a)


--- Page 9 ---
Sci China Inf Sci
9
s.t. rt
m âˆˆ[0, rmax],
(18b)
Î¸t
m âˆˆ[0, 2Ï€),
(18c)
1(et
m âˆˆN) + 1(et
m = S) = 1.
(18d)
Here, rmax is the maximum perturbation radius of AVs. Constraints (18b), (18c) and (18d) limit the value ranges
of the optimization variables.
It is observed that the formulated problem P1 is a complex non-convex mixed-
integer programming problem. Since cross-reality location privacy preservation is persistent and varies over time,
traditional convex optimization methods that provide only single-round static solutions are often invalid. Moreover,
the long-term coupling among consecutive decisions in dynamic vehicular metaverse scenarios makes learning-based
methods more suitable than conventional optimization for achieving sustained privacy and immersion performance.
5
LLM-enhanced Hybrid GDM-based Reinforcement Learning Solution
Although it is difficult to solve the optimization problem directly, the problem P1 exhibits a memoryless sequential
decision-making structure that satisfies the Markov property. Based on this observation, we model the hybrid action
decision process as a Markov Decision Process (MDP) [11]. Moreover, large AI models exhibit a strong ability to
understand contextual information in highly dynamic systems, among which LLMs are particularly effective at
reasoning over abstract objectives and discovering latent performance indicators. Motivated by recent advances in
LLM-enhanced reinforcement learning [26], we propose an LLM-enhanced Hybrid Diffusion-based Proximal Policy
Optimization (LHDPPO) algorithm to solve the complex optimization problem P1.
5.1
Markov Decision Process Modeling
5.1.1
Environment state
Let Ë‡lt
m denote the position of the connected server ct
m. Thus, the state space of AV m in the current time slot
t âˆˆ{1, 2, . . . , T} is defined as a union of the real-time vehicle location lt
m, the location of the connected edge server
Ë‡lt
m, and the location of the edge server hosting the AI agent in the previous slot Ë†ltâˆ’1
m , which is represented by
st
m â‰œ{lt
m, Ë™Î¸t
m, Ë‡lt
m, Ë†ltâˆ’1
m }.
(19)
Therefore, the state space in the MDP is the aggregation of the states of all AVs, denoted as st = {st
m}M
m=1.
5.1.2
Action space
At each time slot t, each agentic AI-driven AV autonomously determines a hybrid action, consisting of a set of
continuous action at
m including the perturbation radius rt
m and perturbation angle Î¸t
m in reality, and a discrete
action dt
m, i.e., the AI agent migration decision et
m in virtuality. Therefore, the action of AV m is denoted as
at
m â‰œ{ct
m, dt
m} = {rt
m, Î¸t
m, et
m}. Consequently, the action space in the MDP is defined as at = {ct, dt} = {at
m}M
m=1.
5.1.3
Reward function
The reward function characterizes the immediate feedback received by AVs after executing the hybrid action at
under the environment state st, which plays a vital role in guiding policy updates in MDP. A well-designed reward
function should accurately reflect the system objective and provide informative gradients to facilitate stable and
reliable policy updates. Intuitively, assigning the reward to be directly equal to the utility of AVs encourages the
policy network to generate hybrid actions that jointly enhance location privacy, latency, and QoS performance.
Following this principle, the reward function is designed as
Rman(st, at) = 1
M
M
X
m=1
ut
m.
(20)
However, such manually crafted rewards are often limited to predefined objective terms and penalty components,
and may fail to capture implicit interactions among location privacy, service response latency, and QoS in the
complex cross-reality vehicular metaverse scenario, potentially leading to suboptimal convergence.


--- Page 10 ---
Sci China Inf Sci
10
Continuous
Action ğ’„ğ’„ğ‘¡ğ‘¡
Replay Buffer
Cross-reality LPP 
Environment
Mini-Batch 
Transition
ğ’”ğ’”ğ‘¡ğ‘¡, ğ’‚ğ’‚ğ‘¡ğ‘¡, ğ’“ğ’“ğ‘¡ğ‘¡, ğ’”ğ’”ğ‘¡ğ‘¡+1
Hybrid
action ğ’‚ğ’‚ğ‘¡ğ‘¡
Random
sampling
Auxiliary
Loss â„’diff
A mini-batch of trajectories ğ’”ğ’”ğ‘¡ğ‘¡, ğ’‚ğ’‚ğ‘¡ğ‘¡, ğ’“ğ’“ğ‘¡ğ‘¡, ğ’”ğ’”ğ‘¡ğ‘¡+1
ğ’”ğ’”ğ‘¡ğ‘¡
ğ’”ğ’”ğ‘¡ğ‘¡
LLM
Role: You are a professional reward 
function designer in reinforcement 
learning and understanding â€¦
Environment: This environment 
models cross-reality location privacy 
protectionâ€¦ The complete code is: 
class Metaverse_Env(): â€¦
Requirements: Please design five 
new reward function in JSON format 
with executable Python code. â€¦
Double-chain GDM-based Proximal Policy Optimization Training Loop
LLM-driven Reward Design Loop
Continuous actor
GDM-based Network
Gaussian Noise
Optimal Action
ğ’™ğ’™ğ‘¢ğ‘¢, ğ’”ğ’”, ğ‘¢ğ‘¢
MLP
ğ’™ğ’™ğ‘ˆğ‘ˆ, ğ’”ğ’”, ğ‘ˆğ‘ˆ
MLP
ğ’™ğ’™0
ğ’™ğ’™ğ‘¢ğ‘¢
ğ’™ğ’™ğ‘ˆğ‘ˆ
â€¦
â€¦
â€¦
â€¦
Discrete actor
GDM-based network
Gaussian Noise
Optimal Action
ğ’™ğ’™ğ‘¢ğ‘¢, ğ’”ğ’”, ğ‘¢ğ‘¢
MLP
ğ’™ğ’™ğ‘ˆğ‘ˆ, ğ’”ğ’”, ğ‘ˆğ‘ˆ
MLP
ğ’™ğ’™0
ğ’™ğ’™ğ‘¢ğ‘¢
ğ’™ğ’™ğ‘ˆğ‘ˆ
â€¦
â€¦
â€¦
â€¦
Generated Informative 
Reward Function
Here are five reward functions for 
the optimal continuous action for 
location perturbation and discrete 
action for privacy-aware AI agent 
migration in vehicular metaverses:
``` {"reward_functions": [{
â€œfunction1": "Privacy-Preserving 
Service Quality Optimizer", 
"formula": "reward = w1 *
location entropy - w2 * service 
response delay- w3 * QoS loss",
â€¦ }]}
```
Initial Reward 
Generation Prompt
Reward Function 
Refinement Prompt
Gradient-based optimizer
Hybrid Actor-Critic Optimizer
Input
Input
Output
Selective
reward 
function
Results to 
standardized 
prompt
The reward function 1 produced an 
average converged result of 1.639 
entropy and 122.274 latency â€¦
The reward function 2 produced an 
average converged result of 1.662 
entropy and 123.231 latency â€¦
Your reward function 5 produced a 
final result of 0.3433 entropy and 
150.026 latency.
As we can see, both the reward 
functions 1 and 2 performed well, 
and I like function 2 more. Please 
analyze the output results and 
refine these reward functions even 
more, and design 5 new, improved 
reward functions based on them.
â€¦
Policy update
Value update
Value-based Critic ğ‘‰ğ‘‰ğœ™ğœ™
ğ’”ğ’”ğ‘¡ğ‘¡+1
Gaussian 
Distributions
ğˆğˆğœ“ğœ“
Logits
Categorical 
Distributions
0
ğ‘ğ‘+ 1
â€¦
0
ğ‘ğ‘+ 1
1) Reparameterized
sampling
2) Action mapping
Discrete
Action ğ’…ğ’…ğ‘¡ğ‘¡
Estimated
Advantage Ì‚ğ´ğ´ğ‘¡ğ‘¡
Policy Gradient
Loss â„’PPO(ğœ“ğœ“)
TD 
Residual ğ›¿ğ›¿ğ‘¡ğ‘¡
Critic Loss â„’ğ‘‰ğ‘‰ğœ™ğœ™
Entropy
Loss â„’ent
1) Softmax
2) Sampling
ğğğœ“ğœ“(ğ’”ğ’”ğ‘¡ğ‘¡)
Ì‚ğ´ğ´ğ‘¡ğ‘¡> 0?
â„“diff ğ’™ğ’™0
ğ‘‘ğ‘‘ğ’”ğ’”ğ‘¡ğ‘¡
ğ‘ğ‘ğ‘šğ‘š,ğ‘—ğ‘—
ğ‘¡ğ‘¡
ğœŒğœŒğ‘¡ğ‘¡
ğ‘‰ğ‘‰ğœ™ğœ™(ğ’”ğ’”ğ‘¡ğ‘¡)
Ì‚ğ´ğ´ğ‘¡ğ‘¡
Human
preference
Figure 2
Architecture of the proposed LHDPPO algorithm for the cross-reality Location Privacy Protection (LPP) environment.
5.2
LHDPPO Algorithm Details
5.2.1
LLM-driven reward function design loop
Instead of relying solely on manual formulations that may neglect the important environmental components, the
proposed LHDPPO algorithm employs an LLM as an autonomous reward designer to generate more informative
reward functions. This design philosophy is consistent with the role of agentic AI, where the LLM is integrated into
the reinforcement learning loop to enhance policy learning through adaptive reward shaping.
As illustrated in Fig. 2, the LLM-driven reward design follows an iterative loop consisting of three main stages.
Stage 1: Initial reward generation: The LLM is first prompted with three categories of inputs to produce a
set of candidate reward functions. These inputs include
â€¢ Role assignment: The prompt instructs the LLM to act as an experienced and professional reward designer
with expertise in wireless communications, security and privacy, and deep reinforcement learning.
â€¢ Environment description: The prompt provides a detailed description of the 6G-enabled vehicular metaverse
scenario together with the complete environment code, including the MDP formulation and parameter configuration.
â€¢ Output specification: The prompt requires the LLM to generate reward functions in a structured JSON format,
accompanied by executable Python code that can be directly integrated into the simulation environment.
Stage 2: Selective reward evaluation: Based on the generated candidate reward functions, domain experts
selectively retain those that are suitable for training and discard unreasonable ones, such as functions that assign
excessive weight to a single term. Each retained reward function is then evaluated through interactions with the
environment, and key performance indicators such as achieved location entropy and service latency are collected.
Stage 3: Circular reward refinement: The evaluation results from Stage 2 are standardized and fed back
to the LLM as refinement prompts. Guided by explicit performance comparisons and optional human preferences,
the LLM iteratively updates and improves the reward formulations by emphasizing influential components that are
often overlooked in manual designs, such as the coupled effects of privacy, latency, and QoS. This closed reward
design loop continues until convergence to a high-performing reward function, as shown on the left of Fig. 2.
After multiple refinement rounds, the LLM outputs the most informative reward function for cross-reality location
privacy protection, which is finally adopted by the learning agent:
rt = RLLM(st, at) = 1
M
M
X
m=1

Ï‰1Et
m âˆ’Ï‰2 ln (1 + Lt
m) âˆ’Ï‰3 Ë†Qt
m âˆ’Ï‰4 ln (1 + dt
m,etm) + Ï‰5Î±t
m

.
(21)
Here, Ë†Qt
m =
Qt
m
Qmax is the normalized QoS loss, with Qmax representing the maximum QoS loss under the maximum
perturbation radius rmax. dt
m,etm is the distance between the AV m and the selected agent server et
m. Î±t
m is the
included angle between the vehicle-to-server direction and perturbation direction.
Ï‰1, Ï‰2, Ï‰3, Ï‰4, and Ï‰5 are


--- Page 11 ---
Sci China Inf Sci
11
the optimized weighting factors obtained through multiple rounds of LLM-guided reward refinement, which yield
superior learning performance. Compared with Rman(Â·), the LLM-generated reward incorporates latent yet critical
indicators and explicitly balances location privacy with user immersion, enabling the proposed LHDPPO algorithm
to better improve policy robustness and convergence performance in 6G-enabled vehicular metaverses.
5.2.2
Double-chain GDM-based PPO training loop
Beyond the LLM-driven reward design, the proposed LHDPPO algorithm integrates powerful GDMs that can
model complex, high-dimensional distributions through multi-step denoising processes, enabling structured action
exploration in the highly coupled decision space of 6G-enabled vehicular metaverses. Specifically, LHDPPO leverages
two diffusion policies to handle continuous and discrete actions, respectively, jointly forming the hybrid policy as
Ï€Ïˆ(at|st) = Ï€c
Ïˆc(c|s) Ï€d
Ïˆd(d|s),
(22)
where c = {Ïµm, Î¸m}M
m=1 denotes the continuous actions for location perturbation and d = {em}M
m=1 denotes the
discrete actions for privacy-aware AI agent migration. The key idea of GDM-based algorithms is to exploit the
reverse diffusion process to progressively refine actions from noise into feasible solutions, thereby enabling effective
exploration for near-optimal hybrid actions. This generative denoising process is conceptually described as
pÏˆ(x0:U|s) = N(xU; 0, I)
U
Y
u=1
pÏˆ(xuâˆ’1|xu, s),
(23)
where U is the number of denoising steps, I is the covariance matrix, and x0 is the final output of the reverse chain.
The reverse diffusion chain {pÏˆ(xuâˆ’1|xu, s)}U
u=1 is modeled as a sequence of conditional Gaussian transitions,
pÏˆ(xuâˆ’1|xu, s) = N(xuâˆ’1; ÂµÏˆ(xu, s, u), Î£Ïˆ(xu, s, u)) ,
(24)
where the covariance follows a predefined variance schedule Î£Ïˆ(xu, s, u) = Î²uI and the mean is parameterized via
the noise prediction network ÎµÏˆ as
ÂµÏˆ(xu, s, u) =
1
âˆšÎ±u

xu âˆ’
Î²u
âˆš1 âˆ’Â¯Î±u
ÎµÏˆ(xu, s, u)

,
(25)
with Î±u = 1 âˆ’Î²u and Â¯Î±u = Qu
k=1 Î±k.
Accordingly, the hybrid policy Ï€Ïˆ(at|st) is realized by executing the reverse diffusion process from xU âˆ¼N(0, I)
to x0, where the final denoised sample x0 corresponds to the executed action at. The same reverse diffusion principle
is applied to both continuous and discrete components, yielding c and d, respectively. Consequently, optimizing
the policy Ï€Ïˆ in the high-dimensional vehicular metaverse environment is equivalent to optimizing the denoising
networks Îµc
Ïˆc and Îµd
Ïˆd that govern the reverse diffusion dynamics of the continuous and discrete action spaces. These
denoising networks fully determine the action distribution and finally the behavior of the learning agent.
At time slot t, the hybrid log-probability of the executed action generated by the diffusion policy is computed as
log Ï€Ïˆ(at|st) = log Ï€c
Ïˆc(ct|st) + log Ï€d
Ïˆd(dt|st).
(26)
In LHDPPO, both the continuous and discrete policies are implemented by diffusion-based actors that execute
the reverse diffusion chain in a mean-only denoising mode. The stochasticity required for policy optimization is
solely introduced by the final distributions, which ensures tractable likelihood evaluation and stable PPO updates.
For the continuous component, the diffusion actor produces a mean ÂµÏˆc(st), and the continuous policy is then
modeled as a Gaussian distribution, given by
Ï€c
Ïˆc(ct|st) = N
 ct; ÂµÏˆc(st), diag(Ïƒ2
Ïˆc)

,
(27)
where the log-variance vector ÏƒÏˆc is learned jointly with the diffusion model parameters. This construction enables
the PPO policy gradient to propagate through the log-probability of the sampled action into the continuous denoising
network Îµc
Ïˆc, thereby shaping the reverse diffusion dynamics according to long-term system performance.
For the discrete component, a diffusion-based denoising network generates the categorical logits for each AV,
from which the migration action et
m is sampled. Then, the corresponding log-probability is computed by
log Ï€d
Ïˆd(dt|st) =
M
X
m=1
log Cat(et
m | logitsm(st)).
(28)


--- Page 12 ---
Sci China Inf Sci
12
The gradients of this log-probability propagate through the diffusion-generated logits and update the discrete denois-
ing network Îµd
Ïˆd, thereby completing the coupling between diffusion modeling and PPO-based policy optimization.
For PPO updates, we denote Ï€Ïˆold as the old hybrid behavior policy, and the importance ratio is computed as
Ït(Ïˆ) =
Ï€Ïˆ(at|st)
Ï€Ïˆold(at|st) = exp
 log Ï€Ïˆ(at|st) âˆ’log Ï€Ïˆold(at|st)

.
(29)
In LHDPPO, a learned value-based critic network VÏ•(s) is employed to approximate the expected value of the
current state, with parameters Ï• updated from sampled trajectories. Based on this approximation, the Generalized
Advantage Estimation (GAE) is constructed as Ë†At = Pâˆ
l=0(Î³Î»)lÎ´t+l, where Î» is the GAE parameter, Î³ is the
discount factor, and Î´t = rt + Î³VÏ•(st+1) âˆ’VÏ•(st) is the Temporal-Difference (TD) residual. This advantage signal
can drive the policy update by weighting the likelihood ratio in the PPO clipped surrogate objective, given by [27]
LPPO(Ïˆ) = âˆ’Et h
min

Ït(Ïˆ) Ë†At, clip(Ït(Ïˆ), 1 âˆ’Îº, 1 + Îº) Ë†Ati
,
(30)
where Îº âˆˆ[0, 1] is the clipping parameter.
The gradient of this objective is backpropagated through the log-
probability of the diffusion policy and ultimately updates the denoising networks Îµc
Ïˆc and Îµd
Ïˆd. In this manner,
LHDPPO forms another closed training loop for diffusion-based action generation, environment interaction, reward
evaluation, and PPO updates are tightly coupled, as shown on the right of Fig. 2.
Meanwhile, the value network is trained using the clipped value loss, calculated by
LVÏ• = 1
2 Et h
max

(VÏ•(st) âˆ’Ë†Rt)2, (Vclip(st) âˆ’Ë†Rt)2i
.
(31)
Here, Ë†Rt = Ë†At + VÏ•(st) is the return target for value learning. Vclip(st) = Vold(st) + clip(VÏ•(st) âˆ’Vold(st), âˆ’Îº, Îº)
is the clipped value prediction used to prevent excessively large value updates, with Vold(st) denoting the value
estimate stored when the trajectory was collected.
To prevent premature convergence of the discrete migration policy in the extremely large action space, LHDPPO
introduces an entropy regularization loss for the discrete categorical distributions, calculated by
Lent = âˆ’ct
ent
ï£®
ï£°1
M
M
X
m=1
ï£«
ï£­âˆ’
N+1
X
j=1
pt
m,j log pt
m,j
ï£¶
ï£¸
ï£¹
ï£».
(32)
Here, pt
m,j = Pr(et
m = j | st) denotes the probability assigned by the discrete diffusion policy to migrating the AI
agent of AV m to candidate destination edge server j at time slot t, with j âˆˆN âˆª{S}. ct
ent is an annealed entropy
coefficient to ensure stable exploration and efficient convergence during training.
Meanwhile, to stabilize learning of the discrete diffusion actor under the large and multi-dimensional discrete
action space, an auxiliary diffusion denoising loss is introduced. The executed migration decisions are converted
into a concatenated one-hot vector xd
0 âˆˆ{0, 1}M(N+1) and optimized through a diffusion reconstruction objective.
This auxiliary loss is applied only to samples with a positive estimated advantage to avoid reinforcing suboptimal
behaviors, which is calculated by
Ldiff = E
h
1( Ë†At > 0) â„“diff(xd
0, st)
i
,
(33)
where â„“diff(xd
0, st) measures the reconstruction error of the discrete diffusion model when denoising the executed
migration vector conditioned on the state st, and is instantiated as an L1 reconstruction loss in our implementation.
Eventually, the overall training objective of the proposed LHDPPO algorithm is given by
L = LPPO + cvLVÏ• + Lent + cauxLdiff,
(34)
where cv and ct
aux denote the value loss coefficient and the annealed auxiliary diffusion coefficient, respectively.
6
Experimental Results
6.1
Experimental Settings
As illustrated on the left of Fig. 3, we perform simulations on the 6G-enabled vehicular metaverse scenario with real
mobility traces of five representative taxis over a 24-hour period, selected from a public taxi dataset containing the


--- Page 13 ---
Sci China Inf Sci
13
Urban Area
Ground RSU
LEO satellite
Starting point
Arrival point
Trajectory
Vehicle 1
Vehicle 2
Vehicle 3
Vehicle 4
Vehicle 5
Remote area
Empower  communications in 
remote areas
Parameter
Value
Antenna gain ğ´ğ´
4.11
Carrier frequency ğ‘“ğ‘“ğ‘ğ‘
915 MHz
Path loss factor ğœ‰ğœ‰
2.8
Uplink / downlink bandwidth 
ğµğµğ‘šğ‘š,ğ‘›ğ‘›
up , ğµğµğ‘šğ‘š,ğ‘›ğ‘›
down
10 MHz
Transmit power of AVs  ğ‘ğ‘ğ‘šğ‘š
10 dBm
Transmit power of RSUs ğ‘ğ‘ğ‘›ğ‘›
43 dBm
Average noise power ğ‘ğ‘0
-174 dBm/Hz
Sizes of uploaded data and AI 
agent task results ğ·ğ·ğ‘šğ‘š
up, ğ·ğ·ğ‘šğ‘š
down
[10, 20] MB
Average ground-to-satellite 
upload speed ğ‘…ğ‘…sat
up
20 Mbps
Average ground-to-satellite 
download speed ğ‘…ğ‘…down
up
100 Mbps
Wired transmission rate ğ‘…ğ‘…ğ‘ğ‘
1 Gbps
Coefficient of the backhaul 
latency ğœ™ğœ™mig
[1, 3] s/hop
Coefficient of the RSU-to-RSU 
AI migration latency ğœ™ğœ™mig
0.02 s/hop
Parameter
Value
AI agent model size ğ‘†ğ‘†ğ‘šğ‘š
[5, 50] MB
Computation cost of AI 
agent tasks ğ‘ğ‘ğ‘šğ‘š
[100, 500] 
CPU cycles/bit
CPU frequencies of RSUs 
and the satellite ğ‘“ğ‘“ğ‘›ğ‘›, ğ‘“ğ‘“sat
[5, 20] GHz,
50 GHz
Conversion factor ğœ„ğœ„
0.1
Maximum perturbation 
radius of AVs ğ‘Ÿğ‘Ÿğ‘šğ‘šax
0.001
Utility weighting factors
ğœ”ğœ”ğ¸ğ¸, ğœ”ğœ”ğ¿ğ¿, ğœ”ğœ”ğ‘„ğ‘„
1, 0.02, 0.2
Hyperparameter
Value
Batch size
256
Discount factor
0.95
GAE parameter
0.95
Learning rates of diffusion actors
0.0001
Denoising steps
5
Value loss coefficient
0.25
Initial auxiliary coefficient
0.01
Initial entropy coefficient
0.1
Map
Figure 3
Left: Visualization of the AV trajectories and edge server locations. Right: Parameter and hyperparameter settings in this paper.
trajectories of 4,316 taxis in Shanghai on February 20, 2007 [28]. For the edge server deployment, RSUs are extracted
from a real-world dataset comprising approximately 840,000 base stations across China2) and filtered within the
target urban region, while a LEO satellite node is incorporated to provide ubiquitous 6G connectivity in remote
areas at the boundary of the map. Referring to [9, 11, 29], the environmental parameters and hyperparameters
adopted in the experiments are presented on the right of Fig. 3. In addition, the performance of the proposed
LHDPPO algorithm is compared with the following representative baseline methods:
â€¢ Geo-Indistinguishability (Geo-I) [24]: A non-learning method that perturbs location based on differential
privacy, and employs a greedy strategy to select the agent server with the best historical performance.
â€¢ Hybrid PPO (H-PPO) [30]: A classical hybrid reinforcement learning algorithm in which discrete action
selection and continuous parameter optimization are trained in parallel using PPO.
â€¢ GDMs-assisted Hybrid PPO (GHPPO) [22]: An extension of H-PPO that introduces a generative diffu-
sion model for discrete action generation while modeling continuous actions using parameterized policies.
â€¢ Hybrid-GDM (HGDM) [21]: A GDM-based Soft Actor-Critic (SAC) algorithm that expresses both discrete
and continuous decisions as continuous latent variables and maps them to executable hybrid actions during inference.
â€¢ PPO-relaxed [27]: A PPO-based algorithm employing clipped surrogate objectives and GAE for stable policy
updates, where discrete migration actions are similarly relaxed into continuous values for tractable training.
â€¢ HDPPO: A variant of the proposed LHDPPO that employs double diffusion chains for hybrid action decisions
but removes the LLM-driven reward design loop, in order to evaluate the contribution of reward refinement.
6.2
Numerical Results
Convergence analysis. Fig. 4(a) compares the convergence behaviors of LHDPPO and representative baselines.
Overall, our LHDPPO achieves a more stable and superior convergence performance. Specifically, several base-
lines show faster short-term reward gains but then saturate at lower performance levels or suffer from persistent
oscillations, indicating difficulty in reliably optimizing the hybrid action mechanism. Moreover, the gap between
LHDPPO and HDPPO verifies the effectiveness of the LLM-based reward design loop. By refining the reward to
better capture the cross-reality privacy-utility coupling, LHDPPO is guided toward more stable and higher-quality
policies. Fig. 4(b) further studies the impact of the learning rate on LHDPPO. When the learning rate becomes
larger (e.g., 10âˆ’3 or 10âˆ’2), the policy updates become more aggressive, resulting in larger variance and making the
training more likely to converge to suboptimal solutions. In contrast, an overly small learning rate (e.g., 5 Ã— 10âˆ’5)
makes parameter updates overly conservative, which slows down convergence and delays performance improvement.
Specifically, compared with a learning rate of 10âˆ’3, adopting a moderate learning rate of 10âˆ’4 improves the final
convergence reward by approximately 61.9%, motivating the use of a moderate learning rate (i.e., 10âˆ’4) to ensure
reliable policy refinement. Fig. 4(c) evaluates the influence of the diffusion denoising steps. Increasing the denoising
steps generally improves early-stage learning efficiency, as higher-step diffusion can generate higher-quality hybrid
action samples for policy optimization. However, excessively large denoising steps may introduce additional train-
ing variance. Quantitatively, compared with 7 denoising steps, the default setting of 5 steps improves the final
convergence reward by approximately 58.3%. Therefore, an intermediate denoising-step setting achieves the best
tradeoff between convergence stability and final reward.
2) https://gitcode.com/open-source-toolkit/abc64


--- Page 14 ---
Sci China Inf Sci
14
-270
-220
-170
-120
-70
0
10
20
30
40
50
Environment Steps
-750
-700
-650
-600
-550
Ã—105
Average episode reward
H-PPO
GHPPO
HGDM
PPO-relaxed
HDPPO
LHDPPO
(a) Test reward curves of LHDPPO and base-
line methods for hybrid action decisions.
0
10
20
30
40
50
Environment Steps
-300
-250
-200
-150
-100
Average Episode Reward
Ã—105
LHDPPO-LR: 0.00005
LHDPPO-LR: 0.0001
LHDPPO-LR: 0.0005
LHDPPO-LR: 0.001
(b) Test reward curves of LHDPPO under dif-
ferent learning rate settings.
0
10
20
30
40
50
Environment Steps
-300
-250
-200
-150
-100
Average Episode Reward
Ã—105
LHDPPO-DS: 3
LHDPPO-DS: 5
LHDPPO-DS: 7
LHDPPO-DS: 9
(c) Test reward curves of LHDPPO under dif-
ferent denoising step settings.
Figure 4
Convergence performance of the proposed LHDPPO algorithm and the baseline methods. LR: learning rate. DS: denoising step
Geo-I
H-PPO
GHPPO
HGDM
PPO-relaxed
HDPPO
LHDPPO
-4.00
-3.75
-3.50
-3.25
-3.00
-2.75
-2.50
-2.25
-2.00
Average utility ( )
-3.118
-3.201
-3.476
-3.670
-3.097
-2.655
-2.241
(a) AV utility.
Geo-I
H-PPO
GHPPO
HGDM
PPO-relaxed
HDPPO
LHDPPO
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Average cross-reality location entropy ( )
0.499
0.374
0.331
0.397
0.731
0.492
0.743
(b) Cross-reality location entropy.
Geo-I
H-PPO
GHPPO
HGDM
PPO-relaxed
HDPPO
LHDPPO
0
25
50
75
100
125
150
175
200
Average service response latency ( )
155.166159.690
181.958180.571
159.303
128.339
116.437
(c) Service response latency.
Geo-I
H-PPO
GHPPO
HGDM
PPO-relaxed
HDPPO
LHDPPO
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
Average QoS loss ( )
2.566
1.905
0.839
2.277
3.208
2.901
3.279
(d) QoS loss.
Figure 5
Performance comparison between the proposed LHDPPO algorithm and the baseline methods.
Performance analysis. Fig. 5 compares the averaged test performance of the proposed LHDPPO with rep-
resentative baseline schemes under the cross-reality location privacy protection framework. As shown in Fig. 5(a),
HDPPO, which integrates two diffusion chains without LLM assistance, already achieves strong performance with
a high utility, confirming the effectiveness of diffusion-based hybrid action exploration. LHDPPO further enhances
environmental understanding via LLM-driven reward design, leading to the highest AV utility, outperforming the
second-best method HDPPO by 15.6% and the strongest non-diffusion baseline PPO-relaxed by 27.6%. This demon-
strates that the proposed LHDPPO method strikes the most favorable balance between location privacy and user
immersion. Fig. 5(b) shows that LHDPPO attains the highest cross-reality location entropy of 0.743, surpassing the
classic privacy-preserving scheme Geo-I by 48.9%, demonstrating its effectiveness in resisting cross-reality inference
attacks. Fig. 5(c) shows that LHDPPO yields the lowest average service response latency, achieving a 9.3% reduction
compared with HDPPO and reductions exceeding 35% relative to most baselines, such as GHPPO and HGDM,
indicating that the privacy improvement does not rely on excessive AI agent migrations that degrade latency. Fig-
ure 5(d) illustrates the QoS loss performance of different methods. Although GHPPO achieves the smallest QoS
loss, it performs the worst in terms of privacy protection and latency performance. In contrast, LHDPPO delivers
the strongest privacy protection and the lowest service latency while maintaining an acceptable QoS loss, ensuring
both robust location privacy and high-quality immersive experiences for on-board users.
LLM influence analysis. To investigate why LHDPPO works, Fig. 6 illustrates how cross-reality location
entropy varies under the hybrid action mechanism. The surface plot shows that the perturbation radius in the
continuous action primarily dominates the entropy variation, while further privacy gains can be achieved by ad-
justing the included angle Î±t
m, which is jointly determined by the perturbation angle in the continuous action and
the selected agent server in the discrete migration action. This reveals a strong coupling between the physical
perturbed location and the virtual AI agent location: even with a fixed perturbation distance, different relative
orientations between the AV and the selected agent server lead to substantially different location privacy levels.
The reason is that the difference in direction between the disturbance location and the agent deployment location
can simultaneously increase the probabilities of multiple observation points, thereby increasing the uncertainty of
the posterior distribution. Through the LLM-driven reward design loop, LHDPPO identifies Î±t
m as a latent but
crucial indicator and introduces an additional reward shaping term to encourage a larger discrepancy between the
perturbation direction and the server direction, so that the learned policy is guided toward the high-entropy angular
peaks rather than only enlarging the perturbation distance.


--- Page 15 ---
Sci China Inf Sci
15
ğ’«ğ’«ğ‘šğ‘šğ‘¡ğ‘¡= ğ‘ğ‘ğ‘ğ‘=
ğ‘¥ğ‘¥ğ‘šğ‘š,ğ‘ğ‘
ğ‘¡ğ‘¡
, ğ‘¦ğ‘¦ğ‘šğ‘š,ğ‘ğ‘
ğ‘¡ğ‘¡
ğ‘ğ‘ğ‘ğ‘
ğ‘ğ‘ğ‘ğ‘
ğ‘ğ‘ğ‘ğ‘
ğ‘ğ‘ğ‘ğ‘= ğ‘’ğ‘’ğ‘šğ‘š
ğ‘¡ğ‘¡
ğ‘Ÿğ‘Ÿğ‘šğ‘šğ‘¡ğ‘¡
ğœƒğœƒğ‘šğ‘šğ‘¡ğ‘¡
Actual AV location
AI agent location
Expanded AV location
Perturbed AV location
ğ‘ƒğ‘ƒğ‘ƒğ‘ƒÌ‚ğ‘™ğ‘™ğ‘šğ‘š
ğ‘¡ğ‘¡= ğ‘ğ‘ğ‘ğ‘
Ì†ğ‘™ğ‘™ğ‘šğ‘š
ğ‘¡ğ‘¡= ğ‘ğ‘ğ‘ğ‘:
Probability of selecting ğ‘ğ‘ğ‘ğ‘
as agent server based on 
distance between ğ‘ğ‘ğ‘ğ‘and ğ‘ğ‘ğ‘ğ‘
ğ‘ƒğ‘ƒğ‘ƒğ‘ƒÌƒğ‘™ğ‘™ğ‘šğ‘š
ğ‘¡ğ‘¡= ğ‘ğ‘ğ‘ğ‘
Ì†ğ‘™ğ‘™ğ‘šğ‘š
ğ‘¡ğ‘¡= ğ‘ğ‘ğ‘ğ‘: 
Product of two PDF values 
of Gaussian distributions 
with mean ğ‘¥ğ‘¥ğ‘šğ‘š,ğ‘ğ‘
ğ‘¡ğ‘¡
and yğ‘šğ‘š,ğ‘ğ‘
ğ‘¡ğ‘¡
ğ›¼ğ›¼ğ‘šğ‘š
ğ‘¡ğ‘¡
Figure 6
Example of the joint influence of hybrid action mechanisms for cross-reality location privacy protection.
Evaluated privacy level
Attacker confidence
Hybrid action intensity
Figure 7
Comparison of the credibility
of location privacy metrics under differ-
ent hybrid action intensities.
Agent server
Perturbed AV 
trajectory
Actual AV 
trajectory
Agent server
Connected 
server
Figure 8
Applications of the proposed framework in real scenarios.
The figure visualizes the actual and perturbed trajectories of a target
AV and its agent server location.
Privacy metric analysis. Fig. 7 compares the evaluated privacy levels with the attacker confidence as the
hybrid action intensity increases. We consider two representative baselines about privacy metrics: Geo-I-based
entropy for physical location perturbation [24] and location entropy for virtual service migration [11]. A credible
privacy metric should show a clear inverse relationship with attacker confidence. However, the Geo-I-based entropy
and location entropy fail to satisfy this requirement. When the attacker confidence is high (e.g., action intensity
from 0 to 20, confidence close to 1.0), their evaluated privacy levels are not correspondingly low, and their highest
privacy values do not coincide with the lowest attacker confidence. This is because both metrics capture only a
single dimension of location uncertainty and fail to account for the cross-reality privacy risks jointly induced by
AV location disclosure in the physical space and AI agent location exposure in the virtual space. In contrast, the
proposed cross-reality location entropy explicitly models this characteristic, yielding privacy evaluations that are
consistently aligned with attacker confidence. This property is crucial for reinforcement learning, as it provides a
reliable and informative reward signal for stable and effective policy optimization.
Case study in real scenarios. Fig. 8 presents a real-map example to illustrate how the proposed framework
performs cross-reality location privacy protection in practice. The red polyline denotes the actual AV trajectory,
while the green polyline shows the perturbed trajectory generated by the learned privacy policy. We can observe that
the policy produces reasonable perturbations along the route, so that the released locations deviate from the true
path while avoiding excessive QoS degradation. Moreover, the framework jointly controls the virtual-space decision
by dynamically selecting agent servers at different time instants. As shown in Fig. 8, the selected agent servers
are distributed around the neighborhoods of both the actual and perturbed trajectories, rather than being fixed
to a single closest server. This coupled strategy maximally confuses the attacker by preventing a consistent cross-
reality alignment between the reported physical trajectory and the observed AI agent deployment, thereby reducing
attacker confidence in tracing the true route. The real-world experiments indicate that the proposed cross-reality
location privacy protection framework maintains privacy protection and user immersion across dynamic vehicular
environments, demonstrating a certain degree of generalizability.


--- Page 16 ---
Sci China Inf Sci
16
7
Conclusion
This paper has proposed a cross-reality location privacy protection framework to mitigate cross-reality location pri-
vacy leakage for agentic AI-driven AVs in 6G-enabled vehicular metaverses.By adopting a hybrid action mechanism
that jointly employs continuous location perturbation in reality and discrete privacy-aware AI agent migration in
virtuality, the proposed framework achieves unified cross-reality location privacy preservation while maintaining user
immersion. A cross-reality location entropy metric has been designed to quantify the uncertainty of AV positions
under hybrid actions and to support real-time privacy assessment. Combining location privacy, service latency,
and QoS loss, a non-convex mixed-integer programming problem has been established for optimizing the hybrid
action mechanism. To address the formulated problem, we have developed an LLM-enhanced Hybrid Diffusion
Proximal Policy Optimization (LHDPPO) algorithm, which combines LLM-driven informative reward design with
double GDMs-based policy exploration and PPO-based stable policy updates to improve learning performance and
scalability. Extensive experiments on real-world mobility and base station datasets have demonstrated that the
proposed framework effectively protects location privacy while ensuring immersion for on-board users, indicating its
practical applicability for large-scale 6G-enabled vehicular metaverse deployments. Future work will explore how
heterogeneous AV mobility patterns, from correlated commuting routes to random delivery trajectories, affect the
privacyâ€“immersion trade-off and how adaptive mechanisms can provide more balanced privacy protection.
References
1
Xie G, Xiong Z, Zhang X et al. Towards the vehicular metaverse: Exploring distributed inference with transformer-based diffusion model,
IEEE Trans Veh Technol, 2024, 73(12): 19931â€“19936.
2
Chen J, Kang J, Xu M et al. Efficient twin migration in vehicular metaverses: Multi-agent split deep reinforcement learning with spatio-
temporal trajectory generation, IEEE Trans Mobile Comput, 2025.
3
Kang J, Tong Y, Zhong Y et al. Diffusion-based auction mechanism for efficient resource management in 6G-enabled vehicular metaverses,
Sci China Inf Sci, 2025, 68: 1â€“16.
4
Seid A M, Abishu H N, Hevesli M et al. A multi-agent DRL-based dynamic resource allocation in O-RAN-enabled TN-NTN metaverse
services, IEEE Trans Commun, 2025, 73(12): 14243â€“14259.
5
Kang J, Luo X, Nie J et al.
Blockchain-based pseudonym management for vehicle twin migrations in vehicular edge metaverse, IEEE
Internet Things J, 2024.
6
Yu J. Agentic vehicles for human-centered mobility, arXiv preprint arXiv:2507.04996, 2025.
7
Auda J, Gruenefeld U, Faltaous S et al. A scoping survey on cross-reality systems, ACM Comput Surv, 2023, 56: 1â€“38.
8
Hiroi Y, Hatada Y, Hiraki T. Cross-reality lifestyle: Integrating physical and virtual lives through multi-platform metaverse, arXiv preprint
arXiv:2504.21337, 2025.
9
Luo X, He J, Fu Y et al.
Incentivizing pseudonym exchange with trajectory prediction for privacy-enhanced vehicular metaverses: A
diffusion-based auction approach, IEEE Trans Mobile Comput, 2025.
10
Min M, Zhu H, Ding J et al. Personalized 3D location privacy protection with differential and distortion geo-perturbation, IEEE Trans
Depend Secure Comput, 2023, 21: 3629â€“3643.
11
Wang W, Zhou X, Qiu T et al.
Location-privacy-aware service migration against inference attacks in multiuser MEC systems, IEEE
Internet Things J, 2023, 11: 1413â€“1426.
12
Luo X, Wen J, Kang J et al. Privacy attacks and defenses for digital twin migrations in vehicular metaverses, IEEE Netw, 2023, 37: 82â€“91.
13
Ullah I, Ali Shah M, Khan A et al. Location privacy schemes in vehicular networks: taxonomy, comparative analysis, design challenges,
and future opportunities, ACM Comput Surv, 2025, 57: 1â€“44.
14
Liu B, Zhou W, Zhu T et al. Location privacy and its applications: A systematic study, IEEE Access, 2018, 6: 17606â€“17624.
15
Liu Y, Zhang Y, Su S et al. BlockSC: A blockchain empowered spatial crowdsourcing service in metaverse while preserving user location
privacy, IEEE J Sel Areas Commun, 2023, 42: 880â€“892.
16
Wang L, Pang S, Gui H et al. Sustainable energy-efficient multi-objective task processing based on edge computing, IEEE Trans Netw
Serv Manag, 2025, 22(4): 3092â€“3105.
17
Emara K. Safety-aware location privacy in VANET: Evaluation and comparison, IEEE Trans Veh Technol, 2017, 66: 10718â€“10731.
18
Li M, Chen Y, Kumar N et al. Quantifying location privacy for navigation services in sustainable vehicular networks, IEEE Trans Green
Commun Netw, 2022, 6: 1267â€“1275.
19
Du H, Zhang R, Liu Y et al. Enhancing deep reinforcement learning: A tutorial on generative diffusion models in network optimization,
IEEE Commun Surv Tutor, 2024, 26: 2611â€“2646.
20
Ning P, Wang H, Tang T et al. Diffusion-based deep reinforcement learning for resource management in connected construction equipment
networks: A hierarchical framework, IEEE Trans Wireless Commun, 2025.
21
Kang Y, Wen J, Kang J et al. Hybrid-generative diffusion models for attack-oriented twin migration in vehicular metaverses, IEEE Trans
Veh Technol, 2025.
22
Wang K, Wang X, Zhao N et al. Uplink RSMA in LEO satellite communications: A perspective from generative artificial intelligence,
IEEE Trans Veh Technol, 2025.
23
Sheng M, Zhou D, Bai W et al. Coverage enhancement for 6G satellite-terrestrial integrated networks: performance metrics, constellation
configuration and resource allocation, Sci China Inf Sci, 2023, 66: 130303.
24
AndrÂ´es M E, Bordenabe N E, Chatzikokolakis K et al. Geo-indistinguishability: Differential privacy for location-based systems, In: Proc
ACM SIGSAC Conf Comput Commun Secur, 2013, 901â€“914.
25
RÂ´enyi A. On measures of entropy and information, In: Proc Fourth Berkeley Symp Math Stat Probab, Vol. 1: Contributions to the Theory
of Statistics, Univ California Press, 1961, 547â€“562.
26
Cao Y, Zhao H, Cheng Y et al. Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods, IEEE
Trans Neural Netw Learn Syst, 2025, 36: 9737â€“9757.
27
Zhang R, Zhao C, Du H et al. Embodied AI-enhanced vehicular networks: An integrated vision language models and reinforcement learning
method, IEEE Trans Mobile Comput, 2025, 24(11): 11494â€“11510.
28
Zhu H, Li M, Fu L et al. Impact of traffic influxes: revealing exponential intercontact time in urban VANETs, IEEE Trans Parallel Distrib
Syst, 2010, 22: 1258â€“1266.
29
Chen Z, Yi W, Nallanathan A et al. Distributed digital twin migration in multi-tier computing systems, IEEE J Sel Top Signal Process,
2024, 18: 109â€“123.
30
Fan Z, Su R, Zhang W et al. Hybrid actor-critic reinforcement learning in parameterized action space, In: Proc Int Joint Conf Artif Intell
(IJCAI), Macao, China, 2019, 2279â€“2285.
