--- Page 1 ---
arXiv:2505.03373v1  [cs.LG]  6 May 2025
SPAP: Structured Pruning via Alternating
Optimization and Penalty Methods
Hanyu Hu
Department of Mathematics
The University of Hong Kong
hhy1224@connect.hku.hk
Xiaoming Yuan∗
Department of Mathematics
The University of Hong Kong
xmyuan@hku.hk
Abstract
The deployment of large language models (LLMs) is often constrained by their
substantial computational and memory demands. While structured pruning presents
a viable approach by eliminating entire network components, existing methods
suffer from performance degradation, reliance on heuristic metrics, or expensive
finetuning. To address these challenges, we propose SPAP (Structured Pruning via
Alternating Optimization and Penalty Methods), a novel and efficient structured
pruning framework for LLMs grounded in optimization theory. SPAP formulates
the pruning problem through a mixed-integer optimization model, employs a
penalty method that effectively makes pruning decisions to minimize pruning errors,
and introduces an alternating minimization algorithm tailored to the splittable
problem structure for efficient weight updates and performance recovery. Extensive
experiments on OPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP’s
superiority over state-of-the-art methods, delivering linear inference speedups
(1.29× at 30% sparsity) and proportional memory reductions. Our work offers a
practical, optimization-driven solution for pruning LLMs while preserving model
performance.
1
Introduction
The rapid advancement of large language models (LLMs) has revolutionized natural language
processing (Zhang et al., 2022; OpenAI, 2023; Touvron et al., 2023a,b; Meta AI, 2023; Gemini Team
et al., 2023; Grattafiori et al., 2024; Apple Inc, 2024; DeepSeek-AI, 2025; Qwen Team, 2025), yet
their enormous size poses significant challenges for practical deployment. Pruning techniques have
emerged as a cornerstone for LLM compression (Frantar and Alistarh, 2023; Sun et al., 2023; Ma et al.,
2023; Shen et al., 2024; Fang et al., 2024), offering a promising solution to reduce model size and
computational requirements while preserving performance. While unstructured pruning (Frantar and
Alistarh, 2023; Sun et al., 2023; Dong et al., 2024; Zhao et al., 2024) provides flexibility by zeroing
out individual weights, it relies on specialized hardware for sparse computations and often yields
marginal speedups due to sparsity patterns. Similarly, semi-structured pruning methods (Holmes
et al., 2021; Meng et al., 2024; Fang et al., 2024) depend on specialized kernels and hardware
supports like NVIDIA’s 2:4 pattern (Mishra et al., 2021), making them highly hardware-dependent
and thus limiting their applicability. Empirical studies further show that semi-structured pruning
underperforms structured methods in inference acceleration at equivalent sparsity levels (Ashkboos
et al., 2024). In contrast, structured pruning distinguishes itself by eliminating entire network
components (e.g., channels, heads, or layers) rather than individual weight entries, enabling direct
computational speedups and broad hardware compatibility without relying on specialized hardware
∗Corresponding author
Work in progress.


--- Page 2 ---
Penalty
Method
Pruned
Weights
Alternating
Minimization
Updated
Weights
Figure 1: Overview of the SPAP method. Left: The layer-wise pruning problem of an MLP layer.
Middle: We propose a penalty method to decide which parts of the weight matrices should be pruned.
Right: We develop an alternating minimization algorithm to efficiently update the remaining weights
to restore model performance.
or sparse computations. These advantages make structured pruning a robust and hardware-agnostic
strategy for LLM compression.
Despite its compelling advantages, existing structured pruning methods face three fundamental
challenges when applied to modern LLM architectures. First, removing larger structural components
inherently leads to greater performance degradation, making preservation of model quality particularly
difficult (Men et al., 2024; Yang et al., 2024; Zhu et al., 2024). Second, modern architectures like
LLaMA-3 (Meta AI, 2023) and Qwen2.5(Qwen Team, 2025) employ grouped query attention
(GQA) (Ainslie et al., 2023), which reduces attention layer parameters to just 19.23% and 12.60%
of decoder layers respectively through weight sharing, while multi-layer perceptron (MLP) layers
dominate with 80.77% and 87.40% parameter shares. This architectural shift means approaches that
focus on attention heads yield diminishing returns and may cause significant performance drops (Hu
et al., 2025; Wang et al., 2024). Third, current methods exhibit notable limitations: LLM-Pruner (Ma
et al., 2023) requires extensive finetuning for recovery, while SliceGPT (Ashkboos et al., 2024) relies
on computationally intensive PCA operations and introduces additional parameters. Other methods
avoid finetuning but introduce new trade-offs: heuristic metrics may compromise performance (An
et al., 2024; Wang et al., 2024), discrete search spaces become prohibitively large (Shen et al., 2024),
or global loss minimization is needed for mask learning (Gao et al., 2024). These gaps highlight the
need for structured pruning methods that simultaneously achieve superior performance preservation,
hardware-agnostic acceleration, and computational efficiency.
To address these challenges, we propose SPAP (Structured Pruning via Alternating Optimization and
Penalty Methods), a novel structured pruning framework grounded in optimization and specifically
targeting MLP layers. Our approach tackles two core problems: (1) identifying optimal neurons
to prune with minimal performance impact, and (2) efficiently updating remaining parameters for
performance recovery. For the first challenge, we leverage a structural insight within MLP layers
to formulate a mixed-integer optimization problem, rigorously analyze its properties, and derive a
theoretical guarantee that relaxation preserves optimality, enabling efficient solutions via a penalty
method. For the second challenge, we introduce an alternating minimization algorithm that leverages
the splittable problem structure by exploiting closed-form updates for one variable and gradient
updates for the others, significantly outperforming the vanilla gradient descent approach. Figure 1
presents an overview of the SPAP method.
Extensive experiments demonstrate SPAP’s superior performance across multiple model families
(OPT, LLaMA-3/3.1/3.2, and Qwen2.5) on diverse language benchmarks. Our method achieves
this while maintaining computational efficiency: pruning the LLaMA-3.1-8B model requires only
one hour on a single NVIDIA RTX 4090 GPU (24GB memory) with just 128 calibration samples.
Comprehensive inference profiling reveals that SPAP delivers computational speedups proportional
to the achieved sparsity levels, and corresponding reductions in memory footprint. These results
collectively validate SPAP’s effectiveness and practical utility for real-world deployment scenarios.
2
Related Work
Traditional structured pruning approaches rely on retraining for recovery (Ma et al., 2023), but
the computational cost makes this impractical for LLMs, especially in scenarios requiring rapid
2


--- Page 3 ---
deployment. This motivates the development of structured pruning methods that preserve performance
without retraining.
Recent work explores two primary directions for structured pruning in LLMs. The first focuses
on pruning large model components. FinerCut (Zhang et al., 2024) employs a greedy algorithm to
remove entire attention or MLP blocks with minimal output impact, while ShortGPT (Men et al.,
2024) eliminates decoder layers based on importance metrics. Although effective for inference
acceleration, these coarse-grained approaches often incur significant performance drops, necessitating
computationally intensive finetuning.
The second direction involves pruning more fine-grained structured components such as rows,
columns, or heads in the weight matrices of linear operators, which could better preserve model perfor-
mance while still being able to provide direct speedups and memory reductions. SliceGPT (Ashkboos
et al., 2024) introduces rotation-based pruning using PCA-derived orthogonal matrices, but requires
storing these matrices for residual connection transformations during inference. Furthermore, its
activation-dependent strategy demands large calibration datasets and high-precision (64-bit) PCA
computations, making it memory-intensive. DISP-LLM (Gao et al., 2024) eliminates structural de-
pendencies by applying distinct selection matrices to weights, obviating residual projection matrices.
However, it requires training pruning masks via full-network loss minimization, consuming a large
amount computational resources (2.4 hours on 2×A100-80GB for 7B models). Other work (Shen
et al., 2024) employs architecture search with ADMM for weight updates, but its iterative search
process remains inefficient (5 hours for LLaMA-7B). SlimGPT (Ling et al., 2024) and ZipLM (Kurti´c
et al., 2024) extends the optimal brain surgeon (OBS) framework to structured pruning. However, the
greedy nature of OBS could lead to suboptimal solutions, potentially compromising performance.
FLAP (An et al., 2024) introduces channel stability metrics and bias compensation, but it does not
update the remaining weights and thus limits performance recovery. While CFSP (Wang et al.,
2024) improves upon FLAP by combining inter- and intra-block activation information, it still relies
on heuristic metrics and finetuning to recover performance. FASP (Hu et al., 2025), proposes a
pruning structure that interlinks rows and columns within a decoder layer and develops a prune-update
framework for structured pruning. Although FASP achieves relatively good performance with fast
pruning speed, the pruning decision is still largely dependent on heuristic metrics.
In summary, while structured pruning offers compelling advantages for LLM deployment, existing
methods face three key limitations: (1) dependence on heuristic pruning metrics, (2) inadequate
compensation for pruning-induced performance loss, and (3) high computational/memory overhead.
Our work addresses these challenges by formulating rigorous optimization models for structured
weight pruning, analyzing their mathematical properties, and developing scalable algorithms to enable
practical, accurate LLM pruning.
3
Methodology
3.1
Optimization Model for Structured Pruning
We formulate the structured pruning problem for MLP layers with gated linear unit (GLU) archi-
tecture (Shazeer, 2020), which is widely adopted in state-of-the-art models. Following FASP (Hu
et al., 2025), we exploit the structural correspondence between operators by jointly pruning corre-
sponding rows and columns. Consider an MLP layer with three linear operators: the up projection
Wup ∈Rn×m, gate projection Wgate ∈Rn×m, and down projection Wdown ∈Rm×n. Given input
X ∈Rn×p, the forward pass computes:
Xoutput = Wdown (WupX ⊙σ(WgateX)) ,
(1)
where σ(·) denotes the swish activation function and ⊙represents element-wise multiplication.
Through algebraic decomposition, we obtain:
Xoutput =
n
X
i=1
Wdown[:, i] (Wup[i, :]X ⊙σ(Wgate[i, :]X)) ,
(2)
revealing that pruning the i-th column of Wdown permits simultaneous pruning of the i-th rows of
both Wup and Wgate without additional pruning error incurred.
The layer-wise structured pruning problem requires determining which columns to prune in the down
projection while optimally updating remaining weights to minimize output distortion. We focus on
3


--- Page 4 ---
the down projection to determine which rows/columns should be pruned in the weight matrices as
this offers two key advantages: first, it reduces the objective to a tractable least squares formulation;
second, it implicitly incorporates the effects of up and gate projections through their activation outputs.
This leads to the following mixed-integer optimization problem with bilinear constraints:
min
W ,s
1
2∥W X −Y ∥2
F ,
(3a)
s.t. W diag(s) = 0,
(3b)
1⊤s = λ,
(3c)
s ∈{0, 1}n,
(3d)
where W ∈Rm×n represents the pruned weight matrix, s ∈{0, 1}n is a binary pruning indicator
variable, X ∈Rn×p denotes input activations, Y ∈Rm×p is the original output activations, and
λ ∈N specifies the number of columns to be pruned. The objective function (3a) minimizes the
Frobenius norm between pruned and original outputs, while constraint (3b) enforces zero columns
for pruned weights, (3c) controls sparsity level, and (3d) ensures binary pruning decisions.
The optimization model presents three significant challenges: its mixed-integer nature from binary
constraints, non-convexity introduced by bilinear constraints, and large-scale optimization require-
ments for LLM parameters. We develop solutions to address these challenges through optimization
techniques and algorithmic designs in the following.
We first address the mixed-integer challenge of model (3) by the following theorem that shows
relaxing the integer variables to the continuous space does not affect the optimality of the problem.
Theorem 1. Consider the following relaxed optimization problem:
min
W ,s
1
2∥W X −Y ∥2
F ,
(4a)
s.t. W diag(s) = 0,
(4b)
1⊤s = λ,
(4c)
s ∈[0, 1]n.
(4d)
For any optimal solution ( ˆ
W , ˆs) of (4), let J be an arbitrary λ-subset of the support of ˆs. Then, there
exists another optimal solution (W ′, s′) to (4) such that s′ ∈{0, 1}n and supp(s′) = J.
Due to page limitations, the proof is provided in Appendix A. Theorem 1 establishes an equivalence
relationship: every optimal solution to the relaxed problem (4) contains sufficient information to
reconstruct an optimal solution for the original model (3). This equivalence justifies our subsequent
focus on the relaxed formulation (4). The critical challenge emerges from the non-convex bilinear
constraint (4b). To address this obstacle, we develop a penalty method in Section 3.2 that strategically
transforms the constrained problem into a sequence of tractable subproblems.
3.2
A Penalty Method
To address the bilinear constraint, we introduce a penalty term, transforming the original problem
into:
min
W ,s
1
2∥W X −Y ∥2
F + ρ
2
n
X
i=1
si∥W [:, i]∥2
2,
(5)
s.t. 1⊤s = λ,
(6)
s ∈[0, 1]n,
(7)
where ρ > 0 is a penalty parameter controlling the strictness of constraint enforcement. As ρ increases, the
constraints si∥W [:, i]∥2
2 = 0 for all i ∈{1, . . . , n} are progressively enforced. This ensures that W [:, i] = 0
whenever si ̸= 0, thereby satisfying W diag(s) = 0. For sufficiently large ρ, the penalized model (5) yields the
same optimal solutions as the relaxed model (4).
We propose an alternating optimization approach to solve (5) approximately. For a given ρ(k), we update W
and s while fixing the other variable:











s(k+1) = argmin
s∈S
(
1
2∥W (k)X −Y ∥2
F + ρ(k)
2
n
X
i=1
si∥W (k)[:, i]∥2
2
)
,
(8)
W (k+1) = argmin
W
(
1
2∥W X −Y ∥2
F + ρ(k)
2
n
X
i=1
s(k+1)
i
∥W [:, i]∥2
2
)
,
(9)
4


--- Page 5 ---
where S :=

s : Pn
i=1 si = λ, s ∈[0, 1]n	
is the feasible set for s.
Let us first investigate the s-subproblem (8). Observe that with W (k) fixed, the reconstruction error term
1
2∥W X −Y ∥2
F becomes independent of s, reducing the problem to:
min
s∈S
ρ
2
n
X
i=1
si∥W (k)[:, i]∥2
2.
(10)
This could be naively solved by setting λ entries in s(k+1) corresponding to the λ columns with smallest norms
to 1 and the remaining entries to 0. However, this approach is myopic as it only considers the magnitude of
the current W (k) while neglecting our ultimate objective of minimizing the reconstruction error. Inspired by
Wanda (Sun et al., 2023), we incorporate the impact of reconstruction error by jointly considering both the
column norms of W (k) and the column-wise Wanda score. We define the following composite metric:
score(sj) := t
W (k)[:, j]

2
2 + (1 −t)
W (k)[:, j]

1 · ∥X[:, j]∥2 ,
(11)
where t ∈(0, 1) is a hyperparameter balancing these two terms. The λ entries of s corresponding to the smallest
scores are then set to 1, with the remaining entries set to 0. To promote smoother optimization while encouraging
exploration and leveraging the continuous relaxation of s, we employ a soft update mechanism:
s(k+1) = αs(k) + (1 −α)s(k+1)
new
,
(12)
where α ∈(0, 1) controls the update aggressiveness.
The W -subproblem (9) is a generalized ridge regression problem with column-specific regularization governed
by s. It admits the following closed-form solution (please see Appendix B for mathematical derivations):
W (k+1) = Y X⊤(XX⊤+ ρ(k)diag(s(k+1)))−1.
In case the matrix XX⊤+ ρ(k)diag(s(k+1)) is only positive semi-definite, a small diagonal perturbation δI
(δ > 0) may be added for numerical stability.
After obtaining the updated variables s(k+1) and W (k+1), we increase the penalty parameter ρ according to the
following scheme to progressively enforce the sparsity constraints:
ρ(k+1) = τρ(k),
where τ > 1 controls the rate of increase. This adaptive scheme ensures stronger constraint satisfaction as the
optimization progresses.
After a given K iterations, we do a final hard update step of s and update W accordingly to recover a feasible
solution of model (3). The described penalty method for solving model (4) is summarized in Algorithm 1.
3.3
Updating the Up and Gate Projections
Having established our penalty method for pruning and updating the down projections in MLP blocks, we
now address the remaining components: the up and gate projections. While these weight matrices are pruned
following the correspondence rules, they remain unupdated, missing opportunities to further reduce the pruning
error. To bridge this gap, we develop an efficient alternating minimization algorithm that jointly optimizes all
three projection matrices in the pruned architecture. This approach systematically reduces pruning error while
maintaining computational efficiency.
Let Wup ∈R(n−λ)×m, Wgate ∈R(n−λ)×m, and Wdown ∈Rm×(n−λ) denote the weight matrices of
the pruned up, gate, and down projections, respectively. Given input activations X ∈Rm×p and original
output activations Y ∈Rm×p of the MLP block, we formulate the weight update problem as the following
unconstrained optimization problem:
min
Wup,Wgate,Wdown ∥Wdown (WupX ⊙σ(WgateX)) −Y ∥2
2 .
(13)
Although unconstrained, Problem (13) presents computational challenges due to the complex interactions among
decision variables and the non-convexity introduced by the Swish activation function σ(·). While gradient
descent offers a straightforward approach for optimizing these variables, we observe that when Wup and Wgate
are fixed, the problem reduces to a least squares formulation with the closed-form solution:
Wdown = Y Z⊤
ZZ⊤−1
,
(14)
5


--- Page 6 ---
Algorithm 1 A penalty method for model (4)
Inputs:
original weight W ∈Rm×n,
input activation X ∈Rn×p,
output activation of the original network Y ∈Rm×p,
# of columns to be pruned λ ∈N,
# of iterations K, score parameter t, soft update parameter α, increase parameter τ
Output:
pruned and updated weight W ∗∈Rm×(n−λ)
Initialize s(0), W (0) and ρ(0)
for k ←0 to K-1 do
for j ←1 to n do
score

s(k)
j

←t
W (k)[:, j]

2
2 + (1 −t)
W (k)[:, j]

1 · ∥X[:, j]∥2,
# Compute scores
end for
for l ←1 to n do
if score

s(k)
j

≤Topn−λ

score

s(k−1)
j

then
s
(k+ 1
2 )
l
←1
# Compute the new s
end if
end for
s(k+1) = αs(k) + (1 −α)s(k+ 1
2 )
# Soft update s
W (k+1) = Y X⊤(XX⊤+ ρ(k)diag(s(k+1)))−1
# Update W
ρ(k+1) = τρ(k)
# Increase the penalty parameter ρ
end for
Compute score

s(K)
and conduct hard update to get s∗
Updated W ∗according to s∗
return W ∗
where Z = WupX ⊙σ(WgateX). This observation motivates our more efficient alternating minimization
approach, which combines gradient descent steps for Wup and Wgate with an optimal update for Wdown:
W (k+1)
up
= W (k)
up −η
∂f
∂Wup

W (k)
up , W (k)
gate, W (k)
down

,
(15a)
W (k+1)
gate
= W (k)
gate −η
∂f
∂Wgate

W (k)
up , W (k)
gate, W (k)
down

,
(15b)
Z(k+1) = W (k+1)
gate X ⊙σ

W (k+1)
up
X

,
(15c)
W (k+1)
down = Y

Z(k+1)⊤
Z(k+1) 
Z(k+1)⊤−1
.
(15d)
Notably, with Wdown and Wgate fixed, the problem reduces to a least squares formulation for Wup. However,
solving this optimally would require vectorization and Kronecker products, which leads to prohibitive memory
requirements when pruning LLMs. Consequently, we maintain memory-efficient gradient updates for Wup.
In our implementation, we employ the Adam optimizer (Kingma and Ba, 2014) to automatically adapt the
learning rate η during optimization. Section 4.3 presents a comprehensive comparison between our alternating
optimization approach and the vanilla gradient descent method, showcasing the consistently better performance
of the proposed method.
4
Experiments
4.1
Experimental Setup
Models and Baselines. We compare SPAP against four competitive pruning methods: CFSP (Wang et al., 2024),
FLAP (An et al., 2024), SliceGPT (Ashkboos et al., 2024), and FASP (Hu et al., 2025). Our evaluation spans
five prominent model families: LLaMA-3/3.1/3.2 series (Meta AI, 2023; Grattafiori et al., 2024), Qwen2.5
series (Qwen Team, 2025), and OPT series (Zhang et al., 2022) with sizes ranging from 125M to 8B. Due to lack
of support from the official implementation, SliceGPT comparisons are limited to OPT models, while FLAP
and CFSP evaluations exclude OPT models. All models were sourced from HuggingFace (Wolf et al., 2019).
The pruning via CFSP, FLAP, FASP and SPAP are only on the MLP blocks and we scale up the the actual
pruning ratio by the inverse of the proportion of parameters in the MLP blocks so that all the sparsity figures
6


--- Page 7 ---
0
10
20
30
40
50
Sparsity (%)
10
20
30
40
50
WikiText2 Perplexity
Dense
FASP
CFSP
FLAP
SPAP
(a) Perplexity-vs-sparsity on LLaMA-3.2-1B.
0
10
20
30
40
50
Sparsity (%)
10
20
30
40
50
WikiText2 Perplexity
Dense
FASP
CFSP
FLAP
SPAP
(b) Perplexity-vs-sparsity on Qwen2.5-0.5B.
Figure 2: Perplexity results of pruned LLaMA-3.2-1B and Qwen2.5-0.5B models under various
sparsity. SPAP achieves significantly lower ppl in all settings than the baseline methods.
represent overall sparsity of the whole model. The SliceGPT method is applied on both attention and MLP
blocks as the rotation matrices couples the pruning of the two blocks and we donot account for the additional
parameters introduced in the orthogonal matrices for modified residual connections so that the actual model
sparsity of SliceGPT is lower (e.g., 20% pruning via SliceGPT yields only 10.1% actual parameter reduction in
OPT-125M).
Datasets and benchmarks. Following prior works (Sun et al., 2023; Frantar and Alistarh, 2023), we employ
128 randomly selected calibration samples from the WikiText2 (Merity et al., 2016) dataset for SPAP and
the baseline methods. Full WikiText perplexity evaluation of the pruned models are then conducted to assess
the pruning performance. For reasoning tasks, we evaluate zero-shot accuracy on six benchmarks: ARC-
easy, ARC-challenge (Clark et al., 2018), OpenbookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020),
WinoGrande (Sakaguchi et al., 2021), and RTE (Wang et al., 2018), using the LM Harness library (Gao et al.,
2021).
Implementation Details. All experiments are conducted on a single NVIDIA RTX 4090 GPU with 24GB
memory. We implement SPAP using the PyTorch (Paszke et al., 2019) library. For baseline comparisons, we
implement FASP based on the specifications in their paper; employ the official implementations of CFSP and
SliceGPT; and derive FLAP results using CFSP’s implementation, as the original FLAP code lacks support
for LLaMA-3.1/3.2 and Qwen2.5 models. SPAP is able to prune LLaMA-3.1-8B on a single 4090 GPU in
approximately 1 hour.
4.2
Main Results
Method
Sparsity
OPT
125M
1.3B
2.7B
6.7B
Dense
0%
27.66
14.63
12.47
10.86
SliceGPT
10%
29.32
15.15
14.10
11.01
FASP
10%
28.35
14.85
12.51
10.86
SPAP
10%
28.09
14.77
12.41
10.83
SliceGPT
20%
34.53
16.59
16.81
11.62
FASP
20%
29.93
16.04
13.79
11.59
SPAP
20%
29.33
15.66
13.28
11.40
SliceGPT
30%
44.63
19.60
24.12
14.21
FASP
30%
34.06
18.65
17.01
13.31
SPAP
30%
32.55
17.90
15.39
12.90
Table 1: WikiText perplexity (↓) of pruned OPT
models under various sparsity.
Perplexity Results.
Tables 1 and 2 demon-
strate SPAP’s consistent superiority across all OPT,
LLaMA-3/3.1/3.2 and Qwen2.5 model families under
10%, 20% and 30% structured sparsity levels. SPAP
maintains lower perplexity than all baselines at every
sparsity level, with the advantage becoming more pro-
nounced for more challenging pruning tasks where
higher sparsity levels and smaller dense model are
considered. For Qwen2.5-1.5B and 7B models, CFSP
and FLAP fail to produce reasonable results, with
perplexity equals to NaN. Figure 2 illustrates the per-
plexity trend as sparsity increases from 0% to 50%
for LLaMA-3.2-1B and Qwen2.5-0.5B models. We
observe a clear advantage of SPAP compared with
baseline methods, especially against CFSP and FLAP,
across a wide range of sparsity levels.
Zero-shot Performance. As demonstrated in Table 3, SPAP maintains strong reasoning capabilities across
diverse tasks under 10% structured sparsity. On LLaMA-3.1-8B, SPAP achieves a mean accuracy of 64.89%,
outperforming CFSP (+1.49%), FLAP (+6.66%), and FASP (+2.59%). For Qwen2.5-3B, SPAP reaches 64.18%
mean accuracy, surpassing all baselines by 2.02%–4.76%. The consistent improvements across both model
families demonstrate SPAP’s robustness in maintaining model capabilities during compression.
Inference Profiling. We profile the CUDA execution time and peak memory usage during inference of pruned
LLaMA-3.1-8B and Qwen2.5-7B models on a single NVIDIA RTX 4090 GPU. For each test, 1024 tokens
7


--- Page 8 ---
Method
Sparsity
LLaMA-3.x
Qwen2.5
0-8B
1-8B
2-1B
2-3B
0.5B
1.5B
3B
7B
Dense
0%
6.14
6.24
9.76
7.81
13.08
9.28
8.03
6.85
CFSP
10%
7.17
7.20
11.58
9.08
14.44
-
8.96
-
FASP
10%
7.11
7.11
11.33
8.86
14.24
10.11
9.04
7.30
FLAP
10%
7.20
7.27
11.59
9.10
14.30
-
8.51
-
SPAP
10%
7.03
7.07
11.05
8.69
13.99
10.00
8.91
7.25
CFSP
20%
8.67
8.71
15.05
11.47
17.22
-
10.72
-
FASP
20%
8.34
8.33
13.39
10.69
15.89
11.41
10.16
7.96
FLAP
20%
8.91
8.89
15.42
11.77
16.84
-
9.91
-
SPAP
20%
8.17
8.12
12.58
10.27
15.22
11.07
9.85
7.81
CFSP
30%
11.16
11.12
20.69
15.62
22.20
-
13.59
-
FASP
30%
10.44
10.37
16.37
13.92
18.18
12.97
11.51
8.81
FLAP
30%
11.77
11.59
22.33
15.98
21.56
-
12.86
-
SPAP
30%
9.90
9.88
14.64
12.53
16.86
12.34
11.21
8.58
Table 2: WikiText perplexity (↓) of pruned LLaMA-3.x and Qwen2.5 models under various sparsity.
SPAP outperforms baseline methods.
Model
Method
ARC-e
ARC-c
PIQA
OBQA
WinoGrande
RTE
Mean
LLaMA-3.1-8B
Dense
81.14
53.24
81.12
44.80
73.80
71.12
67.54
CFSP
74.96
47.27
78.56
42.20
73.48
63.90
63.40
FLAP
67.76
40.61
76.77
41.40
69.77
53.07
58.23
FASP
75.00
47.18
79.11
42.40
71.27
58.84
62.30
SPAP
75.88
47.78
78.40
43.40
73.01
70.76
64.89
Qwen2.5-3B
Dense
73.02
47.10
78.40
42.60
67.96
75.09
64.02
CFSP
68.81
43.09
75.52
42.40
64.96
61.73
59.42
FLAP
73.69
45.22
74.81
42.00
67.56
68.59
61.98
FASP
70.33
44.11
75.46
42.40
65.19
75.45
62.16
SPAP
72.52
45.22
75.46
42.60
66.61
82.67
64.18
Table 3: Zero-shot results (accuracy, ↑) of the pruned models under 10% sparsity. SPAP outperforms
baseline methods for both LLaMA-3.1-8B and Qwen2.5-3B models.
are generated. As shown in Table 4, SPAP achieves linear speedups and memory reductios proportional to the
sparsity level. Specifically, at 30% sparsity, LLaMA-3.1-8B demonstrates a 1.28× speedup with 26% memory
reduction, while Qwen2.5-7B shows a 1.29× speedup with 23% memory savings. The improvements scale
consistently across sparsity levels, confirming SPAP’s hardware efficiency. Notably, even moderate sparsity
(10-20%) yields notable gains without significantly compromising model quality, as evidiented in the mentioned
perplexity and zero-shot results.
Model
Sparsity
CUDA
Speedup
Peak
Memory
Time (s)
Memory (GB)
Reduction
LLaMA-3.1-8B
Dense
19.03
1.00
15.09
1.00
10%
17.80
1.07
13.79
0.91
20%
16.29
1.17
12.49
0.83
30%
14.85
1.28
11.19
0.74
Qwen2.5-7B
Dense
17.75
1.00
16.04
1.00
10%
16.73
1.06
14.78
0.92
20%
15.26
1.16
13.58
0.85
30%
13.71
1.29
12.43
0.77
Table 4: Inference latency and memory footprint of pruned LLaMA-3.1-8B and Qwen2.5-7B models
at varying sparsity levels. Speedup and memory reduction are normalized to dense baselines.
8


--- Page 9 ---
Model
Method
Sparsity
10%
20%
30%
40%
50%
Qwen2.5-0.5B
FASP
14.24
15.89
18.18
21.69
27.44
CFSP
14.44
17.22
22.20
30.58
45.26
FLAP
14.30
16.84
21.56
30.20
107.10
SPAP w/o update
14.17
15.58
17.69
20.82
26.23
SPAP w. GD
14.15
15.53
17.50
20.23
24.92
SPAP
14.00
15.22
16.86
19.16
22.75
LLaMA-3.2-1B
FASP
11.33
13.39
16.37
21.08
28.52
CFSP
11.58
15.05
20.69
31.57
53.71
FLAP
11.59
15.42
22.33
33.31
53.41
SPAP w/o update
11.16
12.91
15.47
19.51
26.21
SPAP w. GD
11.15
12.89
15.40
19.47
26.08
SPAP
11.05
12.58
14.64
17.68
22.96
Table 5: Perplexity results (↓) of baseline methods and ablated varients on Qwen2.5-0.5B and
LLaMA-3.2-1B models across sparsity levels.
4.3
Ablation Study
We conduct comprehensive ablation studies to systematically evaluate the effectiveness of both the proposed
penalty method and the alternating minimization algorithm. Table 5 presents the perplexity results of pruned
LLaMA-3.2-1B and Qwen2.5-0.5B models, comparing our approach with three baseline methods (FASP, CFSP,
and FLAP) as well as two ablated variants of our method. The first variant, denoted as SPAP w/o update, applies
only the penalty method without executing the alternating minimization algorithm to update the operators. The
second variant, SPAP w. GD, implements the penalty method followed by gradient descent optimization using
the Adam optimizer to solve model (13). Both SPAP w. GD and our full SPAP method perform 20 iterations of
updates for fair comparison.
Several key observations emerge from the experimental results. First, even the basic SPAP w/o update variant
demonstrates superior performance compared to all baseline methods, achieving consistently lower perplexity
scores across all sparsity levels. This improvement is particularly notable when compared to FASP, as it validates
that our penalty method can indeed discover better structured pruning masks than those obtained from the
columnwise Wanda score approach.
Furthermore, the comparison between SPAP w. GD and our full SPAP method reveals a significant performance
gap, with SPAP achieving substantially better perplexity results. This empirical evidence strongly supports the
effectiveness of our proposed alternating minimization algorithm over conventional gradient descent optimization.
The progressive improvement from SPAP w/o update to SPAP w. GD and finally to our complete SPAP method
demonstrates the complementary benefits of both the penalty formulation and the specialized optimization
strategy.
5
Conclusion
We propose SPAP, an optimization-driven structured pruning framework for LLMs that formulates pruning as a
mixed-integer problem, solves it via penalty methods and alternating minimization, and achieves linear speedups
(1.29× at 30% sparsity) and memory reductions with minimal accuracy loss. Comprehensive experiments on
OPT, LLaMA-3/3.1/3.2, and Qwen2.5 model families demonstrate SPAP’s superiority over state-of-the-art
baselines in both perplexity and zero-shot task accuracy. Our work establishes a principled optimization approach
for scalable pruning, which opens new possibilities for structured pruning for LLMs.
9


--- Page 10 ---
References
Ainslie, J., Lee-Thorp, J., De Jong, M., Zemlyanskiy, Y., Lebrón, F., and Sanghai, S. (2023). Gqa: Training
generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245.
An, Y., Zhao, X., Yu, T., Tang, M., and Wang, J. (2024). Fluctuation-based adaptive structured pruning for
large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages
10865–10873.
Apple Inc (2024).
Apple intelligence:
Ai for the rest of us.
https://www.apple.com/
apple-intelligence/. Accessed: 2024-09-30.
Ashkboos, S., Croci, M. L., Nascimento, M. G. d., Hoefler, T., and Hensman, J. (2024). Slicegpt: Compress
large language models by deleting rows and columns. arXiv preprint arXiv:2401.15024.
Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. (2020). Piqa: Reasoning about physical commonsense in natural
language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432–7439.
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. (2018). Think you
have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457.
DeepSeek-AI (2025). Deepseek-v3 technical report.
Dong, P., Li, L., Tang, Z., Liu, X., Pan, X., Wang, Q., and Chu, X. (2024). Pruner-zero: Evolving symbolic
pruning metric from scratch for large language models. arXiv preprint arXiv:2406.02924.
Fang, G., Yin, H., Muralidharan, S., Heinrich, G., Pool, J., Kautz, J., Molchanov, P., and Wang, X. (2024).
Maskllm: Learnable semi-structured sparsity for large language models. arXiv preprint arXiv:2409.17481.
Frantar, E. and Alistarh, D. (2023). Sparsegpt: Massive language models can be accurately pruned in one-shot.
In International Conference on Machine Learning, pages 10323–10337. PMLR.
Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff,
N., et al. (2021). A framework for few-shot language model evaluation. Version v0. 0.1. Sept, page 8.
Gao, S., Lin, C.-H., Hua, T., Tang, Z., Shen, Y., Jin, H., and Hsu, Y.-C. (2024). Disp-llm: Dimension-
independent structural pruning for large language models. Advances in Neural Information Processing
Systems, 37:72219–72244.
Gemini Team, Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth,
A., et al. (2023). Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.
Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten,
A., Vaughan, A., et al. (2024). The llama 3 herd of models. arXiv preprint arXiv:2407.21783.
Holmes, C., Zhang, M., He, Y., and Wu, B. (2021). Nxmtransformer: Semi-structured sparsification for natural
language understanding via admm. Advances in neural information processing systems, 34:1818–1830.
Hu, H., Zhao, P., Li, P., Zheng, Y., Wang, Z., and Yuan, X. (2025). Fasp: Fast and accurate structured pruning of
large language models. arXiv preprint arXiv:2501.09412.
Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
Kurti´c, E., Frantar, E., and Alistarh, D. (2024). Ziplm: Inference-aware structured pruning of language models.
Advances in Neural Information Processing Systems, 36.
Ling, G., Wang, Z., and Liu, Q. (2024). Slimgpt: Layer-wise structured pruning for large language models.
Advances in Neural Information Processing Systems, 37:107112–107137.
Ma, X., Fang, G., and Wang, X. (2023). Llm-pruner: On the structural pruning of large language models. arXiv
preprint arXiv:2305.11627.
Men, X., Xu, M., Zhang, Q., Wang, B., Lin, H., Lu, Y., Han, X., and Chen, W. (2024). Shortgpt: Layers in large
language models are more redundant than you expect. arXiv preprint arXiv:2403.03853.
Meng, X., Behdin, K., Wang, H., and Mazumder, R. (2024). Alps: Improved optimization for highly sparse
one-shot pruning for large language models. arXiv preprint arXiv:2406.07831.
Merity, S., Xiong, C., Bradbury, J., and Socher, R. (2016). Pointer sentinel mixture models. arXiv preprint
arXiv:1609.07843.
10


--- Page 11 ---
Meta AI (2023). Llama-3: Meta ai’s latest language model. https://ai.meta.com/blog/meta-llama-3/.
Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. (2018). Can a suit of armor conduct electricity? a new
dataset for open book question answering. arXiv preprint arXiv:1809.02789.
Mishra, A., Latorre, J. A., Pool, J., Stosic, D., Stosic, D., Venkatesh, G., Yu, C., and Micikevicius, P. (2021).
Accelerating sparse deep neural networks. arXiv preprint arXiv:2104.08378.
OpenAI (2023). Gpt-4 technical report. arXiv, pages 2303–08774.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., et al. (2019). Pytorch: An imperative style, high-performance deep learning library. Advances in neural
information processing systems, 32.
Qwen Team (2025). Qwen2.5 technical report.
Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. (2021). Winogrande: An adversarial winograd schema
challenge at scale. Communications of the ACM, 64(9):99–106.
Shazeer, N. (2020). Glu variants improve transformer.
Shen, X., Zhao, P., Gong, Y., Kong, Z., Zhan, Z., Wu, Y., Lin, M., Wu, C., Lin, X., and Wang, Y. (2024). Search
for efficient large language models. arXiv preprint arXiv:2409.17372.
Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. (2023). A simple and effective pruning approach for large language
models. arXiv preprint arXiv:2306.11695.
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro,
E., Azhar, F., et al. (2023a). Llama: Open and efficient foundation language models. arXiv preprint
arXiv:2302.13971.
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava,
P., Bhosale, S., et al. (2023b). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint
arXiv:2307.09288.
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2018). Glue: A multi-task benchmark
and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461.
Wang, Y., Ma, M., Wang, Z., Chen, J., Fan, H., Shan, L., Yang, Q., Xu, D., Liu, M., and Qin, B. (2024). Cfsp:
An efficient structured pruning framework for llms with coarse-to-fine activation information. arXiv preprint
arXiv:2409.13199.
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz,
M., et al. (2019). Huggingface’s transformers: State-of-the-art natural language processing. arXiv preprint
arXiv:1910.03771.
Yang, Y., Cao, Z., and Zhao, H. (2024). Laco: Large language model pruning via layer collapse. arXiv preprint
arXiv:2402.11187.
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al.
(2022). Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.
Zhang, Y., Li, Y., Wang, X., Shen, Q., Plank, B., Bischl, B., Rezaei, M., and Kawaguchi, K. (2024). Finercut:
Finer-grained interpretable layer pruning for large language models. arXiv preprint arXiv:2405.18218.
Zhao, P., Hu, H., Li, P., Zheng, Y., Wang, Z., and Yuan, X. (2024). A convex-optimization-based layer-wise
post-training pruner for large language models. arXiv preprint arXiv:2408.03728.
Zhu, X., Li, J., Liu, Y., Ma, C., and Wang, W. (2024). A survey on model compression for large language models.
Transactions of the Association for Computational Linguistics, 12:1556–1577.
11


--- Page 12 ---
A
Proof of Theorem 1
We provide a constructive proof of Theorem 1, establishing that the original mixed-integer optimization
model (3) and its relaxed counterpart (4) share identical optimal solutions. Specifically, any optimal solution to
the continuous relaxation can be directly transformed into an optimal binary solution for the original integer-
constrained problem by construction. This equivalence demonstrates that relaxing the discrete variables in
model (3) preserves the global optimality while enabling more efficient numerical treatment.
Proof. Let ( ˆ
W , ˆs) be an optimal solution to the relaxed problem (4). We analyze the constraints and construct
the desired binary solution through the following steps:
Step 1: Analyzing the bilinear constraint. The constraint W diag(s) = 0 implies that for each index
i ∈{1, 2, ..., n}, we have:
ˆ
Wiˆsi = 0,
(16)
where ˆ
Wi denotes the i-th column of ˆ
W . This gives two complementary possibilities for each i:
• If ˆsi > 0, then ˆ
Wi must be the zero vector.
• If ˆ
Wi ̸= 0, then ˆsi must be zero.
Step 2: Constructing the binary solution. Let J ⊆supp(ˆs) be any λ-element subset of the support of ˆs. We
construct a binary vector s′ as:
s′
i =
(
1
if i ∈J,
0
otherwise.
(17)
By construction, s′ satisfies:
• 1⊤s′ = λ (sparsity constraint)
• s′ ∈{0, 1}n (binary constraint)
• supp(s′) = J
Step 3: Verifying feasibility. Let W ′ = ˆ
W . We verify that (W ′, s′) satisfies all constraints:
• For the bilinear constraint (4b):
W ′diag(s′) = ˆ
W diag(s′)
=
n
X
i=1
ˆ
Wis′
i
=
X
i∈J
ˆ
Wi · 1 +
X
i/∈J
ˆ
Wi · 0
=
X
i∈J
ˆ
Wi
Since J ⊆supp(ˆs), for each i ∈J we have ˆsi > 0, which by the original constraint implies ˆ
Wi = 0.
Thus W ′diag(s′) = 0.
• The sparsity constraint (4c) and binary constraint (4d) are satisfied by construction of s′.
Step 4: Optimality preservation. The objective value remains unchanged since:
1
2∥W ′X −Y ∥2
F = 1
2∥ˆ
W X −Y ∥2
F ,
(18)
and ˆ
W was optimal for the relaxed problem.
Therefore, (W ′, s′) is indeed an optimal solution to (4) with the desired binary property s′ ∈{0, 1}n and
support supp(s′) = J.
□
12


--- Page 13 ---
B
Derivations of the W-subproblem in the Penalty Method
To derive the mentioned closed-form solution to problem (9), we express the Frobenius norm and regularization
term using trace operations:
n
X
i=1
s(k+1)
i
∥W [:, i]∥2
2 = tr(W ⊤W diag(s(k+1))).
Thus, the objective function becomes:
f(W ) = 1
2∥W X −Y ∥2
F + ρ(k)
2 tr(W ⊤W diag(s(k+1))).
Taking the gradient yields:
df
dW = (W X −Y )X⊤+ ρ(k)W diag(s(k+1)).
Setting the gradient to zero gives the desired closed-form solution:
W (k+1) = Y X⊤(XX⊤+ ρ(k)diag(s(k+1)))−1.
13
