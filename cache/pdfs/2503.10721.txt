--- Page 1 ---
From Understanding to Excelling: Template-Free Algorithm
Design through Structural-Functional Co-Evolution
Zhe Zhao1,2, Haibin Wen5, Pengkun Wang1, Ye Wei2, Zaixi Zhang3, Xi Lin2, Fei
Liu2, Bo An4, Hui Xiong5, Yang Wang1, and Qingfu Zhang2
1University of Science and Technology of China, Hefei 230026, China
2City University of Hong Kong, Hong Kong 999077, China
3Princeton University, New Jersey 08544, USA
4Nanyang Technological University, Singapore 639798, Singapore
5The Hong Kong University of Science and Technology (Guangzhou), Guangzhou
511458, China
Abstract
Large Language Models (LLMs) have significantly accelerated the automation of algorithm
generation and optimization. However, existing methods (e.g., EoH and FunSearch) mainly rely
on predefined templates and manual calibration, focusing only on the local evolution of key
functions pre-identified by humans. As a result, these approaches struggle to fully exploit the
synergistic effects of the overall architecture and realize the full potential of global optimization.
In this paper, we propose an end-to-end algorithm generation and optimization framework
based on LLMs. The core idea is to leverage the deep semantic understanding and information
extraction capabilities of LLMs to transform natural language requirements into code solutions,
combined with a bi-dimensional co-evolution strategy that concurrently optimizes both the
functional and structural aspects of the algorithm. This closed-loop, automated design process
spans from problem analysis and code generation to global optimization. Our framework not
only overcomes the limitations imposed by fixed templates but also automatically identifies key
algorithmic modules to perform multi-level joint optimization, thereby continuously enhancing
the performance and innovative structural design of the generated code.
Extensive experiments conducted on various algorithm design scenarios, including the Trav-
eling Salesman Problem and large-scale optimization tasks, demonstrate that our approach
significantly outperforms traditional local optimization methods in terms of both performance
and innovation. Further studies reveal its high adaptability to unknown environments and its
breakthrough potential in structural design. Experimental results indicate that our method can
generate and optimize innovative algorithms that surpass human expert designs, thus broad-
ening the applicability of LLMs for algorithm design and offering a novel solution pathway for
automated algorithm development.
1
arXiv:2503.10721v1  [cs.SE]  13 Mar 2025


--- Page 2 ---
1
Introduction
Algorithm design is a core challenge in computer science, playing a critical role in driving scientific
discovery, engineering innovation, and solving computational problems [1, 2, 3, 4, 5, 6]. Algorithms
serve as the underlying engine of modern systems, powering optimization, decision-making, and
learning across diverse fields such as bioinformatics, robotics, and finance [7, 8, 9, 10, 11]. Nonethe-
less, designing efficient algorithms for complex problems remains a labor-intensive process that
demands extensive domain expertise, iterative manual refinement, and a deep understanding of al-
gorithmic paradigms [12, 13, 14, 15, 16]. These challenges severely limit the scalability of algorithm
development and impede the exploration of unconventional or truly innovative designs. Achiev-
ing end-to-end automation—from problem specification to generating optimized solutions—not only
promises to accelerate innovation and reduce human intervention but also holds the potential to
uncover designs beyond current human intuition [17, 18, 19, 20, 21].
Recent advances in evolutionary computation and metaheuristic optimization have partially au-
tomated parameter tuning and heuristic development. More recently, machine learning methods
have enabled systems to learn optimization strategies from data and synthesize code snippets tai-
lored to specific tasks [22, 23, 24]. In particular, large language models (LLMs) such as GPT have
emerged as transformative tools, capable of mapping natural language requirements directly into
executable code. Building on these breakthroughs, approaches such as EOH [1] and Reevo [4] have
integrated evolutionary computation and machine learning to automate algorithm design. Despite
these advancements, current methods primarily operate within fixed algorithmic templates and fo-
cus on local iterative optimizations. This leads to shortsighted improvements, as they overlook the
collaborative potential of global architectural innovation and heavily rely on manually constrained
search spaces [25, 26].
These inherent limitations expose a significant gap in achieving truly end-to-end automation in
algorithm design. While current systems can refine known strategies, they typically fall short in
exploring new paradigms or reimagining algorithm structures. To overcome these constraints, any
automated framework must transcend traditional function-level optimizations by adopting a holistic
design perspective—integrating structural innovation with global optimization.
To address these challenges, this paper introduces a novel framework that integrates the capa-
bilities of evolutionary computation, machine learning, and large language models into a unified
end-to-end system. Our method emphasizes global design exploration and structural innovation,
moving beyond local function-level fine-tuning to optimize entire algorithm architectures. The main
contributions of this work can be summarized as follows:
• Deep Understanding of Algorithm Requirements and Template-Free Generation:
Our system leverages information from input papers and knowledge bases to automatically
construct a complete workflow, thereby achieving a holistic understanding of algorithmic re-
quirements. On this foundation, the system autonomously explores innovative architectures
and uncovers non-intuitive design paradigms while completely discarding predefined algorith-
mic templates. This innovative paradigm breaks traditional design constraints, enabling the
generation of breakthrough and highly innovative algorithms.
2


--- Page 3 ---
• Holistic Architecture Optimization and Autonomous Search Space Expansion: By
globally optimizing the entire algorithm architecture, our approach precisely captures and
harnesses the intricate interactions among components, achieving significant breakthroughs
in overall performance and design. Simultaneously, the framework dynamically defines and
expands its own search space, greatly reducing reliance on manually specified constraints and
prior domain knowledge, thereby enhancing the system’s adaptability and innovation in novel
scenarios.
算法问
Crossover，Mutation，
Reflection，Rewriting …
Problem
Workflow
Paper
…
Two-dimensional 
Evolution
Sampling
Expert 
Designation
(b) Ours
(a) Algorithm Evolution - Funsearch/Eoh
Figure 1:
Schematic Comparison of Algorithm Evolution:
(a) Methods such as Fun-
Search/EOH primarily rely on predefined templates and iterate improvements at the level of in-
dividual functional modules through operations like crossover, mutation, reflection, and rewriting;
(b) The proposed method leverages the deep semantic parsing capabilities of large language models
to build a complete workflow, enabling multi-dimensional collaborative evolution of both algorithm
structure and function. This approach breaks the limitations of single-function or local operations,
achieving autonomous exploration and global optimization of the entire architecture, and providing
adaptive, innovative solutions for complex problems.
The proposed framework not only alleviates the limitations of existing technologies but also paves
the way for fully automated and innovative algorithm design. By significantly lowering the threshold
for high-performance algorithm development, this work has the potential to expedite solutions to
complex computational challenges and ignite new research directions in computational science. The
remainder of this paper is organized as follows: Section 2 reviews related work; Section 3 details
the proposed method; Section 4 presents extensive experimental validations; Section 5 provides a
comprehensive case study demonstrating end-to-end automation and superior performance relative
to human-designed methods; and finally, Sections 6 and 7 conclude the paper with discussions on
future research directions.
2
Related Work
2.1
Traditional Hyper-Heuristic Approaches (HH)
Traditional hyper-heuristic (HH) methods have long been considered a promising approach to achiev-
ing automation in algorithm design by abstracting the process of heuristic selection and combination
[27]. These methods typically operate by either selecting the most effective heuristics from a prede-
fined set or combining simpler components to create new heuristics [28]. This abstraction enables
3


--- Page 4 ---
HH to be applied to a wide range of optimization problems, offering greater generality compared to
problem-specific methods [29]. However, the reliance on predefined heuristic spaces—often manu-
ally crafted by domain experts [5]—represents a fundamental limitation, restricting their ability to
explore innovative algorithmic structures or paradigms.
Traditional HH methods primarily focus on parameter tuning and improving existing heuris-
tics, rather than designing entirely new algorithms [30]. This focus stems from their dependence
on manually defined search spaces, which limits their ability to discover new approaches that may
outperform traditional heuristics [31]. For example, HH frameworks based on genetic programming
[32] often require significant manual intervention to define reusable algorithm components, thereby
constraining their scalability and adaptability to complex, dynamic problems. Moreover, achieving
true end-to-end automation in algorithm design remains a major challenge for traditional HH meth-
ods. These approaches often lack the capability for autonomous exploration and innovation beyond
the constraints of manually defined templates. Consequently, their optimization processes are prone
to early convergence to local optima and frequently fail to uncover unconventional paradigms that
could lead to breakthroughs in algorithm design.
2.2
LLMs for Algorithm Design
The emergence of large language models (LLMs) has opened unprecedented opportunities for au-
tomating algorithm design.
Their code generation capabilities have been successfully applied in
various fields, including debugging, programming competitions, and optimization problem solving
[33, 34, 35]. Trained on extensive code datasets, LLMs not only generate syntactically correct code
snippets but also efficiently tackle complex programming tasks using templates or zero-/few-shot
prompts. However, current studies primarily focus on syntactic correctness and functional implemen-
tation—executing human instructions—thus falling significantly short of substituting expert-driven
algorithm innovation.
EoH [1] innovatively integrates LLMs with evolutionary algorithms to achieve, for the first time,
function-level innovation in algorithmic components. In contrast to the later-published FunSearch
[36], EoH distinguishes itself by evolving not only the code of functional modules but also the un-
derlying heuristic strategies. These approaches [34, 37, 38, 39, 40] mark significant advancements in
automating algorithm design by uncovering novel heuristics that outperform human-crafted methods
in specific domains. Nonetheless, these methods remain heavily dependent on predefined templates,
lack the capacity for autonomous exploration and innovation, and tend to focus on isolated compo-
nents rather than holistic optimization—limitations that constrain the overall potential of automated
algorithm design.
3
Method
This section presents in detail an end-to-end code generation and optimization framework based
on large language models.
The framework formalizes the process of automatically transforming
natural language problem descriptions into high-quality code solutions, and continuously improves
4


--- Page 5 ---
code performance by adopting an adaptive generation mechanism combined with a bi-dimensional
co-evolution strategy. The entire process is highly consistent with the modular workflow shown in
Figure 1: from problem analysis, code generation, and validation testing, to iterative optimization
based on a feedback loop, progressively realizing breakthroughs at the global architectural level.
3.1
Problem Definition and Formal Modeling
In order to automatically transform natural language requirements into code solutions, the sys-
tem first performs a rigorous formal modeling of the problem. Specifically, we abstract the code
generation and optimization problem as a triplet optimization model:
Q = (R, O, C),
(1)
where R represents the functional requirements, O denotes the set of target performance metrics
(such as execution efficiency and resource utilization), and C describes the constraints that must be
satisfied in the design.
A specific code implementation can be represented as:
W = (F, R),
(2)
where F = {f1, f2, . . . , fm} is the set of function modules constituting the code system, and R
characterizes the dependencies among these modules.
To quantitatively evaluate the candidate
solutions, we introduce a set of validation rules:
V = {v1, v2, . . . , vk}.
(3)
Based on the above model, the goal of our method is to select, from the search space S of all
candidate solutions, the code solution that both satisfies all validation rules and is optimal with
respect to the objective function:
W∗= arg min
W∈S O(W),
subject to ∀vi ∈V, W satisfies vi.
(4)
This formal model provides a rigorous quantitative basis for subsequent code generation, validation,
and evolutionary optimization, fundamentally transforming the entire automation process into a
mathematical problem that can be solved using optimization algorithms.
3.2
Adaptive Code Generation and Optimization Process
Based on the above formal modeling, we have designed an adaptive generation and optimization
process that fully leverages the deep semantic understanding and code generation capabilities of
large language models (LLMs) and refines the generated results through real-time feedback. The
entire process consists of three main stages, which correspond to the ”problem analysis,” ”code
generation,” ”validation,” and ”iterative optimization” modules shown in Figure 1.
5


--- Page 6 ---
3.2.1
Semantic Understanding Stage
In the first stage, the system employs a language model L to expand and structure the input problem
Q described in natural language:
S ←L(Q, Ps),
(5)
where Ps is a carefully designed prompt template. This stage aims to thoroughly extract and system-
atically organize the original functional requirements while identifying the optimization objectives O
and constraints C, thereby providing accurate semantic information for subsequent code generation
and optimization.
3.2.2
Code Generation Stage
Based on the structured semantic representation S, the system combines a prior knowledge base K
and a code generation prompt template Pw to automatically generate an initial code solution:
W, Pf ←(F, R),
where Pf ←L(S, K, Pw).
(6)
The code solution W generated at this stage includes a set of function modules F and the de-
pendencies R among them. The fusion strategy of multiple information sources ensures both the
diversity and the high potential quality of the solution, providing an initial population for subsequent
validation and evolutionary operations.
3.2.3
Validation and Feedback Optimization Stage
To ensure that the generated code solution meets practical functional and performance require-
ments, the system introduces a comprehensive validation mechanism. Specifically, the generated
code solution is evaluated against the predefined set of validation rules V:
Z ←(W, V).
(7)
The validation result Z quantifies the correctness and performance of the code solution. If the code
solution fails to pass validation, the system utilizes the feedback information to automatically adjust
and re-optimize the code, forming an adaptive process based on a validation-feedback loop until all
candidate solutions satisfy the predefined rules or an iteration limit is reached. Finally, the system
outputs an initial population composed of diverse code solutions:
I ←{F1, F2, . . . , Fn},
(8)
which lays the foundation for subsequent global optimization exploration.
3.3
Bi-dimensional Co-Evolution Optimization Framework
To achieve breakthroughs at the global architectural level, our method further introduces a bi-
dimensional co-evolution strategy based on the initial population by jointly optimizing both the
6


--- Page 7 ---
functional and structural dimensions. This is the key aspect that distinguishes our approach from
traditional methods (e.g., FunSearch/EOH) and corresponds closely with the process shown in Fig-
ure 1(b).
3.3.1
Functional Dimension Optimization
In the functional dimension, the function modules within each candidate are optimized indepen-
dently. Specific operations such as short-term reflection, crossover, and mutation are employed to
locally improve the function modules. Through the reflection module, which generates improvement
suggestions for local implementations, the system is able to automatically optimize the performance
of each individual function in iterative refinement, achieving gradual and meticulous enhancement.
3.3.2
Structural Dimension Optimization
In the structural dimension, the system is not limited to local function improvements but instead
explores new algorithmic architectures through cross-individual function combination and rewriting.
Specifically, by selecting function modules from different candidates in the population and construct-
ing new module combinations based on expert system prompts and fusion suggestions generated by
the large language model, the search space is continually expanded, thereby uncovering entirely new
design paradigms that surpass traditional templates.
3.3.3
Feedback Loop and Global Optimization
Through the co-evolution of the functional and structural dimensions, our method establishes a
global feedback loop system. The long-term reflection mechanism integrates information from each
iteration—taking into account both the current state and historical optimization experience—so that
the entire evolutionary process gradually converges toward the globally optimal code solution:
I(t+1) = Evaluate

I(t) ∪IF
new ∪IS
new, Z

.
(9)
Here, IF
new denotes the new candidate solutions generated from the functional dimension, while
IS
new represents those generated from the structural dimension. Through continuous iterations, the
system gradually approaches the code solution W∗that satisfies all validation rules and is optimal
in overall performance.
In summary, our method achieves full-chain automation from unstructured natural language to
optimal code implementation. Through template-free generation and a bi-dimensional co-evolution
strategy, it not only ensures meticulous optimization of local modules but also promotes innova-
tive breakthroughs in the overall architecture, providing a novel technical pathway for high-quality
automated algorithm design for complex problems.
7


--- Page 8 ---
Table 1: The performance of different heuristic algorithms was compared. We report the average
optimality gap for each instance, where the baseline results are drawn from [15], and all results are
averaged over 3 runs with different starting nodes each time.
Instance
GHPP
ReEvo
CAE
Instance
GHPP
ReEvo
CAE
Instance
GHPP
ReEvo
CAE
ts225
7.7
6.6
4.6
eil51
10.2
6.5
3.5
d657
16.3
16.0
14.8
rat99
14.1
12.4
11.7
d493
15.6
13.4
10.6
kroA150
15.6
11.6
10.1
rl1889
21.1
17.5
15.8
kroB100
14.1
12.2
7.0
fl1577
17.6
12.1
9.8
u1817
21.2
16.6
13.0
kroC100
16.2
15.9
6.8
u724
15.5
16.9
15.1
d1655
18.7
17.5
12.5
ch130
14.8
9.4
7.8
pr264
24.0
16.8
15.5
bier127
15.6
10.8
6.4
pr299
18.2
20.6
18.8
pr226
15.5
18.0
8.5
lin318
14.3
16.6
16.3
fl417
22.7
19.2
17.3
pr439
21.4
19.3
13.7
4
Comparative Analysis with Baseline Methods
In this section, we present a comprehensive evaluation of the proposed CAE framework against state-
of-the-art heuristic methods, including GHPP[41] and ReEvo, across multiple benchmark instances.
The evaluation focuses on the optimality gap (Gap), a key performance metric for quantifying the
quality of solutions. The Gap is computed as follows:
Gap = Base Obj −CAE Obj
Base Obj
× 100%,
(10)
where:
• Base Obj refers to the objective value of the baseline method (e.g., GHPP ).
• CAE Obj corresponds to the objective value obtained by the proposed CAE method.
The experimental setup includes evaluations on well-known instances of the Traveling Salesman
Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP). For each instance, the
results are averaged over three independent runs, ensuring robustness and consistency. Below, we
analyze the performance of CAE in comparison to the baseline methods.
4.1
Performance on Benchmark Instances
Table 1 highlights the average optimality gap for several benchmark instances, including small-scale
and large-scale problems. Across all instances, the CAE framework consistently outperforms the
baseline methods, demonstrating its superior optimization capabilities. For example, in the ts225
instance, CAE achieves an optimality gap of 4.6%, significantly lower than GHPP (7.7%) and ReEvo
(6.6%). Similarly, in the challenging rl1889 instance, CAE reduces the gap to 15.8%, compared
to 21.1% for GHPP and 17.5% for ReEvo. These results demonstrate the effectiveness of CAE in
handling both small and large problem scales.
Moreover, the framework exhibits strong adaptability in solving instances with varying charac-
teristics. In the eil51 instance, CAE achieves an impressive gap of 3.5%, compared to 10.2% for
GHPP and 6.5% for ReEvo. Even for highly complex instances, such as d1655 and u1817, CAE con-
8


--- Page 9 ---
Table 2: The performance of different heuristic methods on various algorithms for optimizing the
solution to the TSP.
Type
TSP20
TSP50
TSP100
Obj ↓
Gap(%) ↑
time ↓
Obj ↓
Gap(%) ↑
time ↓
Obj ↓
Gap(%) ↑
time ↓
GA
6.1
0.0
0.4
18.2
0.0
1.3
40.8
0.0
2.3
GA+EOH [1]
6.0
1.9
0.3
17.8
2.3
0.8
40.5
0.6
2.0
GA+ReEvo[4]
6.0
1.9
0.3
17.9
1.3
0.8
40.6
0.5
2.1
GA+CAE(ours)
5.7
6.6
0.2
16.3
10.3
0.6
36.6
10.2
1.3
ACO
3.8
0.0
2.1
5.9
0.0
7.6
8.5
0.0
17.9
ACO+EOH [1]
3.9
-0.7
3.5
5.9
0.8
9.1
8.5
0.3
17.4
ACO+ReEvo[4]
3.9
-0.2
2.5
5.9
0.3
7.6
8.4
0.7
12.2
ACO+CAE(ours)
3.8
0.5
2.5
5.8
1.8
6.4
8.4
1.6
13.7
KGLS[42]
4.4
0.0
4.1
6.7
0.0
10.3
9.3
0.0
26.8
KGLS+EOH
4.4
0.6
5.6
6.8
-0.2
14.0
9.2
0.4
28.8
KGLS+ReEvo
4.4
0.2
5.9
6.8
-1.0
14.9
9.3
-0.3
20.9
KGLS+CAE(ours)
3.9
11.2
3.5
5.9
11.7
9.0
8.5
8.2
28.0
sistently achieves lower optimality gaps, highlighting its robust performance under diverse problem
scenarios.
4.2
Analysis on TSP Instances
Further evaluation of CAE is conducted on TSP instances of varying sizes (TSP20, TSP50, and
TSP100), as summarized in Table 2. The comparison includes both standard heuristic methods
(e.g., GA, ACO, KGLS) and their enhanced variants (e.g., GA+EOH, GA+ReEvo).
Across all
instances, CAE demonstrates significant improvements in terms of objective value, optimality gap,
and computation time.
For small-scale problems such as TSP20, CAE achieves an objective value of 5.7 with a gap of
6.6%, outperforming all baseline methods, including GA (0.0%) and GA+ReEvo (1.9%). Similarly,
for medium-scale problems such as TSP50, CAE achieves a gap of 10.3%, compared to 2.3% for
GA+EOH and 1.3% for GA+ReEvo. For large-scale problems such as TSP100, CAE achieves a gap
of 10.2%, far exceeding the performance of GA+ReEvo (0.5%) and GA+EOH (0.6%).
Additionally, CAE’s integration with advanced heuristics such as ACO and KGLS further en-
hances its effectiveness. For instance, in TSP50, ACO+CAE achieves a gap of 1.8%, outperforming
ACO+ReEvo (0.3%) and ACO+EOH (0.8%). Similarly, in TSP100, KGLS+CAE achieves a gap of
8.2%, significantly improving upon KGLS+ReEvo (-0.3%) and KGLS+EOH (0.4%).
4.3
Insights and Observations
The experimental results reveal several key insights: 1. Compared to baseline methods, the CAE
framework consistently achieves lower optimality gaps, demonstrating its ability to both explore and
exploit superior solutions. 2. The two-dimensional optimization approach that integrates structural
and functional evolution enhances the framework’s ability to discover innovative algorithmic struc-
tures. 3. CAE exhibits strong scalability and adaptability, effectively addressing both small-scale
9


--- Page 10 ---
and large-scale problems with varying complexity.
These results validate the effectiveness of the proposed framework and underscore its potential
to push the frontier in the field of automated algorithm design.
The integration of LLMs with
evolutionary optimization has given rise to a novel paradigm for solving complex optimization prob-
lems, breaking through the limitations of traditional heuristic methods. In addition to these baseline
demonstrations, we will continue to provide analyses explaining how our approach expands the scope
of using LLMs for algorithm design.
5
Case Study: Automated Improvement of Large-scale Op-
timization Methods
This section demonstrates how our system autonomously optimizes the solution process for large-
scale quadratic optimization problems, with minimal human intervention.
We aim to showcase
how the proposed framework, through an end-to-end automated pipeline, can completely refine
and surpass human-designed algorithms. The case study highlights the ability of the framework
to discover non-intuitive solutions and adapt to highly ill-conditioned problems without relying on
traditional algorithmic templates or significant human input.
5.1
LLM-driven Automated Problem Analysis and Solution
For the quadratic minimization problem:
min
x∈Rd f(x) := 1
n
n
X
i=1
1
2⟨x, Aix⟩+ ⟨bi, x⟩

,
(11)
where Ai ∈Rd×d are positive definite matrices and bi ∈Rd are vectors, the framework begins
by analyzing the mathematical structure and properties of the objective function. It then generates
problem-specific algorithms tailored to the characteristics of the input data and constraints.
To simulate realistic optimization scenarios, we follow a setup adapted from existing literature.
Each Ai is constructed as a diagonal matrix, with elements in the first half sampled uniformly from
[1, 10ξ/2] and the second half from [10−ξ/2, 1], where ξ > 0 controls the condition number of the
matrices. The elements of vectors bi are sampled uniformly from [0, 103]. This configuration allows
us to test the robustness of the generated algorithms under varying levels of ill-conditioning.
The proposed framework employs LLMs to generate optimization algorithm code automatically.
The LLM analyzes the problem’s mathematical formulation and generates modular, interpretable
code that adapts dynamically to the problem’s structure. The iterative refinement process integrates
feedback from intermediate results to improve algorithmic performance progressively.
For example, a representative template for the LISR-k[43] method is shown below, illustrating
the modularity and flexibility of the generated code:
1
def
search_routine ( objective_function : callable , x0: np.ndarray ,
2
A_list: List[np.ndarray], b_list: List[np.ndarray],
10


--- Page 11 ---
3
max_iter: int = 100, tol: float = 1e-6) -> np.ndarray:
4
"""
5
Core
optimization
function
based on LISR -k method.
6
"""
7
for iter in range(max_iter):
8
# Update
Hessian
approximation
9
# Execute
optimization
iteration
10
pass
11
return
best_solution
Listing 1: Core optimization function.
The generated code is modular and extensible, allowing for straightforward incorporation of
advanced techniques such as adaptive learning rates, preconditioning, or gradient corrections, de-
pending on the problem’s requirements and complexity.
To further improve the initial LLM-generated algorithms, the framework incorporates an evo-
lutionary optimization process. This method systematically explores the algorithm design space to
enhance performance in terms of convergence speed, stability, and accuracy. The evolution process
is carried out as follows:
1. Initialize a population of algorithm variants generated by the LLM.
2. Evaluate the fitness of each variant based on performance metrics, such as convergence rate,
solution quality, and computational efficiency.
3. Apply mutation (code modifications) and crossover (combination of variants) to generate im-
proved algorithms.
4. Iterate until convergence criteria are met or performance saturates.
This approach ensures that the framework not only generates initial solutions but also system-
atically refines them through automated exploration and optimization.
We evaluated the framework’s performance on quadratic optimization problems with varying
levels of difficulty, characterized by condition numbers controlled by ξ ∈{12, 16, 20}, where higher
values of ξ correspond to more ill-conditioned problems.
The experiments were conducted with
n = 1000 and d = 50. We compared the convergence rates and optimization accuracy of several
methods, including IQN, SLIQN, LISR-1, LISR-k, and LISR-k(CAE).
For ξ = 12, IQN and SLIQN exhibited rapid initial convergence but plateaued early, failing to
achieve high accuracy. In contrast, LISR-k(CAE) achieved optimal convergence within 50 seconds.
As ξ increased to 16 and 20, the performance of IQN and SLIQN deteriorated significantly due
to increased sensitivity to ill-conditioning. However, LISR-k and LISR-k(CAE) maintained robust
optimization capabilities, demonstrating consistent convergence to high-quality solutions.
Notably, LISR-k(CAE) outperformed all other methods across all scenarios, particularly excelling
in high-dimensional and ill-conditioned problems. This highlights the effectiveness of the LLM-based
11


--- Page 12 ---
optimization framework, which combines algorithm generation, automated evolution, and adaptive
techniques to address challenging optimization tasks.
These experimental results validate the capability of the proposed framework to automatically
generate, optimize, and refine algorithms for complex optimization tasks. The LLM-driven approach
exhibits exceptional adaptability and robustness, achieving rapid convergence to optimal solutions
even under challenging conditions characterized by high condition numbers and high dimensionality.
The seamless integration of problem analysis, code generation, and evolutionary refinement under-
scores the potential of LLMs to revolutionize algorithm design for advanced optimization problems.
5.2
Model-driven Algorithmic Improvement and Analysis
0
10
20
30
40
50
60
Time (s)
10 3
10 1
101
103
105
107
Objective Value
= 12,
= 3.12 × 106
IQN
SLIQN
LISR-1
LISR-k
LISR-k(CAE)
(a)
0
10
20
30
40
50
60
Time (s)
10 6
10 3
100
103
106
109
Objective Value
= 16,
= 3.12 × 108
IQN
SLIQN
LISR-1
LISR-k
LISR-k(CAE)
(b)
0
10
20
30
40
50
60
Time (s)
10 6
10 3
100
103
106
109
Objective Value
= 20,
= 3.12 × 1010
IQN
SLIQN
LISR-1
LISR-k
LISR-k(CAE)
(c)
Figure 2: Performance comparison of five optimization algorithms—IQN, SLIQN, LISR-1, LISR-
k, and the proposed LISR-k (CAE)—on optimization problems with increasing difficulty (κ =
3.12 × 106, 3.12 × 108, 3.12 × 1010).
The x-axis represents time (s), and the y-axis shows the
objective value. The results highlight that LISR-k (CAE) achieves faster convergence and better
performance, particularly in highly challenging scenarios.
In this section, we investigate the ability of large language models (LLMs) to improve algo-
rithms automatically. We focus on analyzing the optimization process from the baseline algorithm
(Algorithm a, i.e., iter num 0.py) to the improved algorithm (Algorithm b, i.e., iter num 7.py)
generated by the model.
Through function-by-function comparisons and workflow analysis, we
demonstrate the contributions of LLMs in code generation, optimization strategy design, and func-
tionality enhancement.
5.2.1
Function-by-function Comparative Analysis
To reveal the improvement capabilities of LLMs at the function level, we compare the core imple-
mentations of Algorithm a and Algorithm b. The features and shortcomings of Algorithm a are
combined with the improvements in Algorithm b to highlight the enhancements and their effects.
Symmetric Rank-k Update (srk)
In Algorithm a, handling singularity issues is straightfor-
ward: the rank of temp is checked, and the original matrix G is returned in singular cases. The
12


--- Page 13 ---
update depends on direct matrix inversion (np.linalg.inv), which can lead to numerical instabil-
ity, especially in high-dimensional problems. In contrast, Algorithm b replaces direct inversion with
np.linalg.solve, significantly improving numerical stability. It also retains options for pseudoin-
verse handling in singular cases, offering greater flexibility for further extensions.
1
temp = U.T @ (G - A) @ U
2
if np.linalg.matrix_rank(temp) < U.shape [1]:
# Handle
singularity
3
return G
4
return G - (G - A) @ U @ np.linalg.inv(temp) @ U.T @ (G - A)
Listing 2: Algorithm a Implementation
1
temp = U.T @ (G - A) @ U
2
if np.linalg.matrix_rank(temp) < U.shape [1]:
3
return G
4
return G - (G - A) @ U @ np.linalg.solve(temp , np.eye(temp.shape [0])) @ U.T
@ (G - A)
Listing 3: Algorithm b Improvement
Greedy Matrix Selection (greedy matrix)
Algorithm a selects rows greedily based only on
the diagonal element differences, sorting and taking the top k largest differences. While simple,
this approach ignores the overall structure of the matrix, potentially leading to suboptimal choices.
Algorithm b improves upon this by using the Frobenius norm to compute row differences, taking
into account the global structure of the matrix. This results in more reasonable greedy selection and
better global performance for matrix updates.
1
diff = np.diag(G - A)
2
indices = np.argsort(diff)[:: -1][:k]
3
U = np.zeros ((G.shape [0], k))
4
U[indices , np.arange(k)] = 1
5
return U
Listing 4: Algorithm a Implementation
1
diff = G - A
2
row_norms = np.linalg.norm(diff , axis =1)
3
indices = np.argsort(row_norms)[:: -1][:k]
4
U = np.zeros ((G.shape [0], k))
5
U[indices , np.arange(k)] = 1
6
return U
Listing 5: Algorithm b Improvement
13


--- Page 14 ---
Sherman-Morrison Update (sherman morrison)
Algorithm a relies on the classical Sherman-
Morrison formula but uses direct matrix inversion (np.linalg.inv), which can be numerically unsta-
ble, especially in high-dimensional settings. Singular cases are handled by simply returning the origi-
nal inverse matrix A inv, lacking more sophisticated mechanisms. Algorithm b dynamically chooses
between pseudoinverse (np.linalg.pinv) and stable linear equation solving (np.linalg.solve),
greatly enhancing numerical stability and adapting better to singularity issues.
1
temp = W - U.T @ A_inv @ V
2
if np.linalg.matrix_rank(temp) < U.shape [1]:
# Handle
singularity
3
return
A_inv
4
return
A_inv + A_inv @ U @ np.linalg.inv(temp) @ V.T @ A_inv
Listing 6: Algorithm a Implementation
1
temp = W - U.T @ A_inv @ V
2
if np.linalg.matrix_rank(temp) < U.shape [1]:
3
temp_inv = np.linalg.pinv(temp)
4
else:
5
temp_inv = np.linalg.solve(temp , np.eye(temp.shape [0]))
6
update_term = A_inv @ U @ temp_inv @ V.T @ A_inv
7
return
A_inv + update_term
Listing 7: Algorithm b Improvement
Gradient Accumulation and Update
Algorithm a accumulates gradients and updates the solu-
tion x directly but does not adjust the gradient direction, potentially leading to slower convergence.
Its update process lacks dynamic adjustment mechanisms, making it more susceptible to noise.
Algorithm b introduces a gradient correction term to refine the direction, improving convergence
precision. Additionally, a dynamic scaling factor, computed based on the ratio of gradient norms,
enhances the algorithm’s robustness and adaptability.
1
grad_sum = np.sum([np.dot(A_i , z_i) + b_i for A_i , z_i , b_i in zip(A_list ,
z_list , b_list)], axis =0)
2
x_new = B_bar_inv @ grad_sum
Listing 8: Algorithm a Implementation
1
grad_correction = np.sum([np.dot(A_i , x_new) + b_i for A_i , b_i in zip(
A_list , b_list)], axis =0)
2
x_new
-= 0.1 * grad_correction / (t + 1)
3
scaling_factor = np.linalg.norm( grad_correction ) / np.linalg.norm(grad_sum)
4
x_new *= scaling_factor
Listing 9: Algorithm b Improvement
14


--- Page 15 ---
Through the above function-by-function comparisons, it is evident that large language models
can automatically identify shortcomings in baseline algorithms and propose improvements that en-
hance numerical stability, structural optimization, and dynamic adjustment strategies. The resulting
Algorithm b performs significantly better than Algorithm a, particularly in high-dimensional and
complex problems.
Workflow Comparison Analysis
This section presents a concise comparison of the workflows
across three algorithm generations: iter num 0.py (0th generation), iter num 3.py (3rd genera-
tion), and iter num 7.py (7th generation). The evolution highlights progressive enhancements in
numerical stability, gradient correction, and adaptive scaling, transforming a basic implementation
into a robust and efficient optimization framework. The 0th generation (iter num 0.py) provides
Figure
3:
Workflow
comparison
across
three
generations
of
algorithms:
iter num 0.py,
iter num 3.py, and iter num 7.py. The blue regions represent areas with minor changes, while the
red regions indicate significant modifications in the workflow. The figure illustrates the evolution
from a basic implementation to an enhanced framework, emphasizing improvements in numerical
stability, gradient correction, and adaptive scaling strategies.
a foundational, straightforward implementation. It initializes the target variable x0, computes a
simple greedy matrix selection based on diagonal differences, performs symmetric rank-k updates
using pseudo-inversion, and updates variables via the Sherman-Morrison formula. While numerically
stable for simple problems, its reliance on diagonal sorting and lack of gradient correction limit its
performance in handling complex or ill-conditioned problems.
By the 3rd generation (iter num 3.py), significant improvements are introduced. The symmetric
rank-k update now incorporates stability checks (np.allclose) to avoid unnecessary computations,
and the Sherman-Morrison update reduces redundant operations, improving both efficiency and nu-
15


--- Page 16 ---
merical robustness. While retaining the basic greedy selection strategy, this version better addresses
medium-complexity problems but still lacks advanced selection strategies and gradient correction,
which restrict its global optimization capabilities.
The 7th generation (iter num 7.py) achieves comprehensive advancements. It replaces diagonal-
based greedy selection with a Frobenius norm-based approach, capturing global matrix character-
istics more effectively.
Numerical stability is further enhanced by substituting pseudo-inversion
with np.linalg.solve in both symmetric rank-k and Sherman-Morrison updates. Additionally,
gradient correction is introduced to refine update directions, while adaptive scaling dynamically ad-
justs step sizes based on gradient norms, dramatically improving performance in high-dimensional,
ill-conditioned problems.
The evolution from iter num 0.py to iter num 7.py demonstrates a clear trajectory of in-
creasing sophistication and applicability. The 0th generation is suitable for simple problems with
modest stability requirements. The 3rd generation introduces numerical refinements and is effec-
tive for medium-complexity tasks. Finally, the 7th generation integrates advanced selection, cor-
rection, and scaling mechanisms, achieving exceptional adaptability and robustness for complex,
high-dimensional scenarios.
This progressive innovation in workflow design underscores the strengths of our framework. By
combining modular iterative optimization, dynamic greedy selection, and feedback-driven adaptive
mechanisms, the framework transcends traditional static workflows. It achieves a versatile, scalable,
and highly effective optimization process, capable of addressing a wide range of problem complexities
while maintaining structural clarity and functional synergy.
6
Future Research Directions
Despite the significant progress presented in this work, there remain numerous opportunities to
further enhance the capabilities of automated algorithm design systems. Below, we outline several
key directions for future research:
1. Enhanced Efficiency and Scalability
The current framework demonstrates scalability to large-scale problems, but computational
efficiency remains a challenge, particularly for problems requiring extensive LLM queries or
computationally intensive validation steps. Future work could focus on developing lightweight
LLM variants or hybrid methods that combine traditional heuristics with LLMs to reduce com-
putational overhead. Parallel and distributed optimization strategies could also be integrated
to accelerate the evolutionary process on large-scale problems.
2. Self-Learning and Continual Improvement
A promising direction is the development of self-learning systems where the framework con-
tinuously updates its knowledge base and optimization strategies based on prior runs. This
could involve leveraging reinforcement learning or meta-learning techniques to enable the sys-
tem to autonomously refine its problem-solving strategies over time, thereby improving its
performance on unseen problems.
16


--- Page 17 ---
3. Integration with Scientific Discovery Pipelines
Beyond algorithm design, this framework could be extended to automate broader scientific
discovery pipelines. For example, integrating it with experimental design, data analysis, and
hypothesis generation workflows could significantly accelerate the pace of scientific research.
Such systems could serve as collaborative tools for researchers, autonomously generating in-
sights and solutions that augment human creativity and expertise.
4. Exploration of Explainable AI in Automation
As automated systems increasingly influence algorithm design and scientific exploration, ensur-
ing transparency and interpretability becomes essential. Future work could focus on developing
explainable frameworks that not only generate optimal solutions but also provide clear, inter-
pretable insights into the reasoning behind those solutions. This would foster greater trust
and usability in scientific and industrial applications.
5. Ethical Considerations and Safe Automation
As the automation of algorithm design advances, ethical considerations must also be addressed.
These include ensuring that the generated algorithms are unbiased, fair, and safe for deploy-
ment across various applications. Future research could explore mechanisms for embedding
ethical principles directly into the optimization process to ensure responsible and sustainable
innovation.
7
Conclusion
This paper presents a novel framework for algorithm design that achieves fully automated and innova-
tive algorithm generation through global design exploration, holistic optimization, and autonomous
search space expansion. Extensive experiments and cross-domain case studies have thoroughly vali-
dated the framework’s outstanding adaptability, scalability, and superior performance in addressing
complex computational problems, further demonstrating its immense potential to disrupt traditional
algorithm design paradigms and reshape computing system architectures.
Our work represents a decisive stride toward fully automated algorithm design, significantly low-
ering the barrier for developing high-performance algorithms while offering fresh solutions to the
bottlenecks caused by manual design and local optimization in the past. By confronting the mul-
tifaceted challenges of scalability, adaptability, and structural innovation, this paper proves that
automated algorithm design can not only replicate the expertise of human specialists but also tran-
scend existing knowledge boundaries to discover unprecedented innovations.
These advancements inject new momentum into accelerating computational research and scien-
tific exploration, indicating that future automated systems will play an increasingly crucial role in
driving technological breakthroughs across interdisciplinary fields, optimizing complex engineering
systems, and exploring uncharted areas of computation. With this breakthrough, our framework
lays a solid foundation for the next generation of intelligent systems and heralds the dawn of a new
era in autonomous algorithm discovery.
17


--- Page 18 ---
References
[1] F. Liu, X. Tong, M. Yuan, X. Lin, F. Luo, Z. Wang, Z. Lu, and Q. Zhang, “Evolution
of heuristics:
Towards efficient automatic algorithm design using large language model,”
in
International
Conference
on
Machine
Learning
(ICML),
2024.
[Online].
Available:
https://arxiv.org/abs/2401.02051
[2] J. MENDLING, H. MEYERHENKE, and B. DEPAIRE, “Methodology of algorithm engineer-
ing,” arXiv preprint arXiv:2310.18979, 2023.
[3] B. Romera-Paredes, M. Barekatain, A. Novikov, M. Balog, M. P. Kumar, E. Dupont, F. J. R.
Ruiz, J. Ellenberg, P. Wang, O. Fawzi, P. Kohli, and A. Fawzi, “Mathematical discoveries from
program search with large language models,” Nature, 2023.
[4] H. Ye, J. Wang, Z. Cao, F. Berto, C. Hua, H. Kim, J. Park, and G. Song, “Reevo: Large lan-
guage models as hyper-heuristics with reflective evolution,” in Advances in Neural Information
Processing Systems, 2024, https://github.com/ai4co/reevo.
[5] P. Kerschke, H. H. Hoos, F. Neumann, and H. Trautmann, “Automated algorithm selection:
Survey and perspectives,” Evolutionary computation, vol. 27, no. 1, pp. 3–45, 2019.
[6] B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman, “Building machines that
learn and think like people,” Behavioral and brain sciences, vol. 40, 2017.
[7] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction to Algorithms.
MIT
press, 2009.
[8] J. Pearl, Heuristics: Intelligent Search Strategies for Computer Problem Solving.
Addison-
Wesley, 1984.
[9] S. S. Skiena, The algorithm design manual.
Springer, 2020.
[10] R. Sedgewick and K. Wayne, Algorithms.
Addison-wesley professional, 2011.
[11] D. E. Knuth, The art of computer programming.
Addison-Wesley, 1997, vol. 1.
[12] H. H. Hoos and T. St¨utzle, Stochastic Local Search: Foundations and Applications.
Elsevier/-
Morgan Kaufmann, 2004.
[13] M. R. Garey and D. S. Johnson, Computers and Intractability: A Guide to the Theory of
NP-Completeness.
W.H. Freeman, 1979.
[14] R. M. Karp, “Reducibility among combinatorial problems,” Complexity of computer computa-
tions, pp. 85–103, 1972.
[15] D. H. Wolpert and W. G. Macready, “No free lunch theorems for optimization,” IEEE trans-
actions on evolutionary computation, vol. 1, no. 1, pp. 67–82, 1997.
[16] M. Mitchell, An introduction to genetic algorithms.
MIT press, 1998.
18


--- Page 19 ---
[17] Q. Zhao, B. Yan, T. Hu, X. Chen, Q. Duan, J. Yang, and Y. Shi, “Autooptlib: Tailoring
metaheuristic optimizers via automated algorithm design,” arXiv preprint arXiv:2303.06536,
2023.
[18] H. Yu and J. Liu, “Deep insights into automated optimization with large language models and
evolutionary algorithms,” arXiv preprint arXiv:2410.20848, 2024.
[19] E. Real, C. Liang, D. R. So, and Q. V. Le, “Automl-zero: Evolving machine learning algorithms
from scratch,” International Conference on Machine Learning, 2020.
[20] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search: A survey,” Journal of
Machine Learning Research, vol. 20, no. 55, pp. 1–21, 2019.
[21] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum, and F. Hutter, “Efficient
and robust automated machine learning,” Advances in neural information processing systems,
vol. 28, 2015.
[22] N. van Stein, D. Vermetten, and T. B¨ack, “In-the-loop hyper-parameter optimization for
llm-based automated design of heuristics,” 2024. [Online]. Available: https://arxiv.org/abs/
2410.16309
[23] V. Stanovov, S. Akhmedova, and E. Semenkin, “Neuroevolution for parameter adaptation
in
differential
evolution,”
Algorithms,
vol.
15,
p.
122,
2022.
[Online].
Available:
https://api.semanticscholar.org/CorpusID:248044809
[24] S. Vladimir and E. Semenkin,
“Automatic design of mutation parameter adaptation
for differential evolution,” ITM Web of Conferences, 2024. [Online]. Available:
https:
//api.semanticscholar.org/CorpusID:267281673
[25] F. Liu, Y. Yao, P. Guo, Z. Yang, Z. Zhao, X. Lin, X. Tong, M. Yuan, Z. Lu, Z. Wang,
and Q. Zhang, “A systematic survey on large language models for algorithm design,” 2024.
[Online]. Available: https://arxiv.org/abs/2410.14716
[26] S. Musslick, L. K. Bartlett, S. H. Chandramouli, M. Dubova, F. Gobet, T. L. Griffiths,
J. Hullman, R. D. King, J. N. Kutz, C. G. Lucas, S. Mahesh, F. Pestilli, S. J. Sloman,
and W. R. Holmes, “Automating the practice of science – opportunities, challenges, and
implications,” 2024. [Online]. Available: https://arxiv.org/abs/2409.05890
[27] E. Burke, G. Kendall, J. Newall, E. Hart, P. Ross, and S. Schulenburg, “Hyper-heuristics: An
emerging direction in modern search technology,” Handbook of metaheuristics, pp. 457–474,
2003.
[28] K. Chakhlevitch and P. Cowling, “Hyperheuristics: recent developments,” Adaptive and multi-
level metaheuristics, pp. 3–29, 2008.
[29] E. ¨Ozcan, B. Bilgin, and E. E. Korkmaz, “A comprehensive analysis of hyper-heuristics,” In-
telligent data analysis, vol. 12, no. 1, pp. 3–23, 2008.
19


--- Page 20 ---
[30] C. Blum and A. Roli, “Metaheuristics in combinatorial optimization: Overview and conceptual
comparison,” ACM computing surveys (CSUR), vol. 35, no. 3, pp. 268–308, 2003.
[31] M. O’Neill, “Riccardo poli, william b. langdon, nicholas f. mcphee: A field guide to genetic
programming: Lulu. com, 2008, 250 pp, isbn 978-1-4092-0073-4,” 2009.
[32] V. Stanovov, S. Akhmedova, and E. Semenkin, “Neuroevolution for parameter adaptation in
differential evolution,” Algorithms, vol. 15, no. 4, p. 122, 2022.
[33] P.-F. Guo, Y.-H. Chen, Y.-D. Tsai, and S.-D. Lin, “Towards optimizing with large language
models,” arXiv preprint arXiv:2310.05204, 2023.
[34] F. Liu, X. Tong, M. Yuan, X. Lin, F. Luo, Z. Wang, Z. Lu, and Q. Zhang, “Evolution of
heuristics: Towards efficient automatic algorithm design using large language model,” arXiv
preprint arXiv:2401.02051, 2024.
[35] S. Brahmachary, S. M. Joshi, A. Panda, K. Koneripalli, A. K. Sagotra, H. Patel, A. Sharma,
A. D. Jagtap, and K. Kalyanaraman, “Large language model-based evolutionary optimizer:
Reasoning with elitism,” arXiv preprint arXiv:2403.02054, 2024.
[36] B. Romera-Paredes, M. Barekatain, A. Novikov, M. Balog, M. P. Kumar, E. Dupont, F. J.
Ruiz, J. S. Ellenberg, P. Wang, O. Fawzi et al., “Mathematical discoveries from program search
with large language models,” Nature, vol. 625, no. 7995, pp. 468–475, 2024.
[37] S. Yao, F. Liu, X. Lin, Z. Lu, Z. Wang, and Q. Zhang, “Multi-objective evolution of heuristic
using large language model,” arXiv preprint arXiv:2409.16867, 2024.
[38] Y. Yao, F. Liu, J. Cheng, and Q. Zhang, “Evolve cost-aware acquisition functions using large lan-
guage models,” in International Conference on Parallel Problem Solving from Nature. Springer,
2024, pp. 374–390.
[39] P. Wang, Z. Zhao, H. Wen, F. Wang, B. Wang, Q. Zhang, and Y. Wang, “Llm-autoda: Large
language model-driven automatic data augmentation for long-tailed problems,” Advances in
Neural Information Processing Systems, vol. 37, pp. 64 915–64 941, 2024.
[40] F. Liu, Y. Yao, P. Guo, Z. Yang, Z. Zhao, X. Lin, X. Tong, M. Yuan, Z. Lu, Z. Wang
et al., “A systematic survey on large language models for algorithm design,” arXiv preprint
arXiv:2410.14716, 2024.
[41] G. Duflo, E. Kieffer, M. R. Brust, G. Danoy, and P. Bouvry, “A gp hyper-heuristic approach
for generating tsp heuristics,” in 2019 IEEE International Parallel and Distributed Processing
Symposium Workshops (IPDPSW), 2019, pp. 521–529.
[42] F. Arnold and K. S¨orensen,
“Knowledge-guided local search for the vehicle routing
problem,” Computers & Operations Research, vol. 105, pp. 32–46, 2019. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S0305054819300024
20


--- Page 21 ---
[43] Z. Liu, L. Luo, and B. K. H. Low, “Incremental quasi-newton methods with faster superlinear
convergence rates,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38,
no. 13, 2024, pp. 14 097–14 105.
21
