--- Page 1 ---
ZU-TH 07/25
Explainable AI-assisted Optimization for Feynman Integral Reduction
Zhuo-Yang Song
,1, ∗Tong-Zhi Yang
,2, † Qing-Hong Cao,1, 3, ‡ Ming-xing Luo,4, § and Hua Xing Zhu1, 3, ¶
1School of Physics, Peking University, Beijing 100871, China
2Physik-Institut, Universit¨at Z¨urich, Winterthurerstrasse 190, 8057 Z¨urich, Switzerland
3Center for High Energy Physics, Peking University, Beijing 100871, China
4Beijing Computational Science Research Center, Beijing 100193, China
We present a novel approach to optimizing the reduction of Feynman integrals using integration-
by-parts identities. By developing a priority function through the FunSearch algorithm, which com-
bines large language models and genetic algorithms, we achieve significant improvements in memory
usage and computational efficiency compared to traditional methods. Our approach demonstrates
substantial reductions in the required seeding integrals, making previously intractable integrals more
manageable. Tested on a variety of Feynman integrals, including one-loop and multi-loop cases with
planar and non-planar configurations, our method demonstrates remarkable scalability and adapt-
ability. For reductions of certain Feynman integrals with many dots and numerators, we observed an
improvement by a factor of 3058 compared to traditional methods. This work provides a powerful
and interpretable framework for optimizing IBP reductions, paving the way for more efficient and
practical calculations in high-energy physics.
I.
INTRODUCTION
Precision theoretical predictions in particle physics are
crucial for rigorously testing the Standard Model and
interpreting experimental results, as even minor dis-
crepancies between theory and observation can reveal
new physics. Due to the asymptotic freedom of Quan-
tum Chromodynamics (QCD) and collinear factoriza-
tion, theoretical predictions for hadronic observables at
high-energy hadron colliders can be formulated within
the framework of perturbative theory.
In perturba-
tive quantum field theory (QFT), two primary meth-
ods are commonly used to compute the Feynman inte-
grals that contribute to scattering amplitudes or cross-
sections: integration-by-parts (IBP) reductions [1, 2] and
the differential equation (DE) method [3–5]. Notably, the
DE method itself relies heavily on IBP reductions, mak-
ing IBP reductions a major computational bottleneck in
high-precision calculations.
Traditionally, most of the IBP reductions were car-
ried out using the Laporta algorithm and Laporta seed-
ing [6], which, while effective, become increasingly in-
efficient as the complexity of the problem grows.
In
recent years, the finite field methods [7, 8], which
avoid intermediate expression swell, have been intro-
duced to enhance IBP reductions and improve com-
putational feasibility.
However, these approaches also
encounter challenges when applied to highly intricate
problems.
Additionally, alternative approaches such
as syzygy techniques [9–14], intersection number the-
ory [15, 16], and block-triangular form improved re-
duction [17–19] have emerged, offering new perspec-
∗zhuoyangsong@stu.pku.edu.cn
† tongzhi.yang@physik.uzh.ch
‡ qinghongcao@pku.edu.cn
§ mingxingluo@csrc.ac.cn
¶ zhuhx@pku.edu.cn
tives on IBP reductions. Building on these techniques,
several public packages have been developed to per-
form IBP reductions, including: Air [20], LiteRed [21,
22], FIRE6 [23, 24], Reduze [25, 26], Kira [27–29],
Forcer [30], FiniteFlow [31], NeatIBP [32], Blade [19],
AmpRed [33].
To further enhance the efficiency of IBP reductions,
we introduce a novel approach that optimizes the re-
duction of Feynman integrals through a priority function
developed via the FunSearch algorithm [34]. FunSearch
combines large language models (LLMs) with genetic al-
gorithms to evolve and refine priority functions that can
minimize IBP system size and improve computational ef-
ficiency. By leveraging the scalability and interpretabil-
ity of LLMs, this approach provides heuristic solutions
that can be extended to more complex problems, offer-
ing a new avenue for advancing precision calculations in
particle physics.
Our work focuses on optimizing the IBP reductions
process by identifying an optimal subset of seeding
integrals that are sufficient to solve the target inte-
grals. We demonstrate the effectiveness of our approach
through explicit examples involving one-loop and multi-
loop Feynman integrals, including both planar and non-
planar six-particle phase-space integral families. Our re-
sults show that the priority function method achieves
substantial improvements in memory usage and compu-
tational efficiency compared to traditional methods, en-
abling the reduction of previously intractable integrals.
In recent years, we have witnessed increasing applica-
tions of Artificial Intelligence (AI) in high-energy theo-
retical physics, despite the inherent tension between the
need for interpretability in theoretical physics and the
probabilistic, black-box nature of AI systems. Substan-
tial progress has been made, including applications of
machine learning to the string-theory landscape [35–38],
scattering amplitudes [39–46], jet physics [47–57], Par-
ton Distribution Functions [58–60], as well as advances
in understanding neural networks through the framework
arXiv:2502.09544v1  [hep-ph]  13 Feb 2025


--- Page 2 ---
2
of quantum field theories [61–69]. While most previous
studies have focused on learning underlying structures
from theoretical data, our work takes a different ap-
proach. We introduce the priority function method for
IBP reduction in Feynman integral calculations, where
rather than learning from data, we formulate a clearly
defined theoretical target for AI optimization in a fully
explainable way.
We have achieved substantial improvements in both
computational efficiency and memory usage through our
priority function method, clearly demonstrating the use-
fulness of AI in cutting-edge analytic theoretical research.
These enhancements are critical for making multi-loop
calculations more feasible and scalable, thereby enabling
researchers to explore more complex and higher-order
processes that were previously computationally pro-
hibitive. By integrating the scalability and interpretabil-
ity of LLMs with the optimization capabilities of genetic
algorithms, our FunSearch approach not only addresses
existing computational bottlenecks but also sets a new
benchmark for future research with AI assistance.
In the following, we present some necessary concepts
for Feynman integrals and FunSearch.
Subsequently,
we employ FunSearch algorithm to search for the best-
estimated priority function for one-loop bubble integrals,
which helps to greatly reduce the size of IBP system.
The obtained priority function is then directly general-
ized to the integrals with any number of loops and legs.
Finally, we show several explicit examples where the pri-
ority function is used to efficiently perform IBP reduc-
tions for six-particle phase-space integrals in both planar
and non-planar cases.
II.
OVERVIEW OF FEYNMAN INTEGRAL
REDUCTION
In this section, we provide an overview of the Feynman
integral and the traditional methods to perform IBP re-
ductions. This background will set the stage for intro-
ducing our approach in the subsequent sections.
A.
Feynman integral definition
A family of j-loop Feynman integrals is defined as fol-
lows (see for example [70]):
I(n1, n2, · · · , nm) =
Z  jY
i=1
dDli
iπd/2−1
!
i(n1, n2, · · · , nm)
=
Z  jY
i=1
ddli
iπd/2−1
!
1
Dn1
1 Dn2
2
· · · Dnm
m
,
(1)
where D is the spacetime dimension, the propagators Dm
depend on loop momentum lj and external momentum
p1, p2, · · · , pE.
The propagator’s powers nm are posi-
tive or negative integers.
The propagators which al-
low negative powers only are called irreducible scalar
products (ISPs).
The integrals are classified into sec-
tors (θ1, θ2, · · · , θm) with θi = Θ(ni −1/2). It’s conve-
nient to define several parameters: the number of dif-
ferent denominators t = P
i θi, the total power of de-
nominators r = P
i θini, the total power of numerators
s = P
i(θi −1)ni, as well as the dots d = r −t. The
integrals of total derivative are zero in dimensional reg-
ularization, this gives the IBP identities:
0 =
Z  jY
i=1
ddli
iπd/2−1
!
∂
∂lµ
i

qµ
k i(n1, n2, · · · , nm)

, (2)
where q = l1, · · · lj, p1, · · · pE. These identities are a set
of recurrence relations which relate a large amount of
Feynman integrals to a basis of integrals, called master
integrals. In the presence of cut propagators, the reverse
unitarity method [71] is introduced to derive IBP identi-
ties,
δ(Di) =
 1
Di

cut
=
1
2iπ

1
Di −i0 −
1
Di + i0

.
(3)
B.
Laporta seeding and improved seeding
A standard approach for solving IBP reduction prob-
lems is the Laporta algorithm [6]. This method works
by substituting specific integer values for the propagator
indices in the symbolic IBP identities in Eq. (2), then
solving the resulting large-scale linear system to express
generic Feynman integrals in terms of master integrals.
The specific integrals generated through this index as-
signment process are referred to as seeding integrals.
To systematically generate these seeding integrals, one
typically employs predefined patterns. The conventional
Laporta seeding scheme follows a bounding strategy s ≤
smax, d ≤dmax. However, this approach often produces
a large number of redundant equations that do not con-
tribute to new constraints, significantly increasing the
computational complexity of solving the large linear sys-
tem. Recent advances in the field have introduced more
sophisticated seeding strategies [19, 72, 73] that dramat-
ically reduce this redundancy while maintaining system
completeness.
These seeding strategies, referred to as
improved seeding, propose that, in addition to Laporta
seeding, s can be reduced by n + f for sectors missing n
propagators relative to the primary sector, where f is an
adjustable parameter.
Building on numerous past efforts to enhance the ef-
ficiency of Feynman integral reduction, including the
development of improved seeding methods, we propose
a novel advancement in IBP reduction. Our approach
leverages machine learning techniques, with a particular
focus on the FunSearch approach [34].


--- Page 3 ---
3
III.
OVERVIEW OF FUNSEARCH
With the background on Feynman integral reduction
established, we now turn to our approach for optimiz-
ing this process using the FunSearch algorithm [34]. We
propose a priority function that allows FunSearch to com-
bine LLMs with genetic algorithms to discover efficient
algorithms that can significantly reduce the number of
required seeding integrals.
FunSearch is an innovative approach that combines
LLMs with genetic algorithms to discover novel solutions
to complex mathematical problems. This method is par-
ticularly suitable for problems that are ”easy to evaluate
but difficult to solve” such as combinatorial optimization
and certain mathematical problems [34]. FunSearch op-
erates by evolving programs that describe how to solve
the problem, rather than directly searching for the so-
lution itself. This approach endows the discovered solu-
tions with scalability and interpretability.
The scalability allows FunSearch to repeatedly run on
simpler problems to obtain efficient and heuristic solu-
tions.
The interpretability enables humans to further
generalize and improve upon these heuristic solutions,
thus making a machine-expert collaborative workflow
possible.
On the other hand, extensive searches using
FunSearch can avoid the selective use of instances fa-
vorable to human experts’ solutions, thereby providing
more generalizable results. This leads to more objective
and comprehensive solutions in the field of high-energy
physics.
The objective of FunSearch in this work is to optimize
the seeding in the IBP reductions process by developing a
priority function that sorts seeding integrals, thereby ac-
celerating IBP reductions. The detailed implementation
is presented in Sec. IV.
FunSearch consists of two key components:
a pre-
trained large LLM and an evaluator that assesses the
quality of priority functions through scores. FunSearch
evolves an initially low-scoring priority function into a
higher-scoring one through iterative operations of these
components. The iteration flow chart is shown in Fig. 1.
During the iterative process, we employ an island-
based evolutionary approach to encourage exploration
and prevent the functions from falling into local op-
tima. The programs evolve independently on different
”islands,” and periodically, the worst-performing island
is reset with new programs from the best-performing is-
land [34].
Compared to traditional search methods, FunSearch
has several advantages:
Scalability: By searching in the space of programs,
FunSearch can handle problems with large search spaces
more efficiently than direct solution search methods. In
the context of IBP reductions, the number of required
seeding integrals is often much smaller than the total
number of initial seeding integrals. This suggests that
optimal seeding may depend on the specific target inte-
gral. However, designing a separate IBP reduction algo-
rithm for each target integral is impractical, so common
methods like Laporta seeding [6] are typically only mini-
mally dependent on the target integral. This results in a
large number of redundant seeding integrals. FunSearch,
with its scalability, has the potential to resolve this con-
tradiction.
Interpretability: The programs generated by Fun-
Search are typically interpretable, facilitating insights
and further refinement by domain experts. This means
that FunSearch can first be run on a relatively simple
IBP reductions, and then domain experts can extend the
heuristic functions generated by FunSearch to more com-
plex IBP reductions. This can significantly enhance the
efficiency of FunSearch, thereby expanding its applica-
tion scope and reducing deployment costs.
Robustness: Combining the creativity of LLMs and
evolutionary exploration enables FunSearch to avoid lo-
cal optima and discover novel solutions. In traditional
IBP reduction methods, human experts often select spe-
cific seedings based on their experience and preferences.
In contrast, FunSearch uses an automated approach that
eliminates such selectivity, ensuring objectivity and gen-
erality in the search process.
In summary, FunSearch provides a powerful framework
that leverages the strengths of LLMs and evolutionary
algorithms to solve complex and innovative problems. Its
ability to generate and refine programs offers a scalable
and interpretable problem-solving approach, making it
a valuable tool in high-energy physics, especially when
innovative solutions are required.
Recent work has demonstrated that FunSearch can be
further optimized in multiple aspects [74–78], enabling
it to tackle more complex problems while showcasing its
flexibility in solving problems across various domains [79,
80].
IV.
ENCODING IBP REDUCTIONS THROUGH
PRIORITY FUNCTION
With this background in mind, we will now discuss
how to apply FunSearch to IBP reductions.
In high-energy physics, the Laporta algorithm [6] is
widely used to solve the IBP reduction problem for Feyn-
man integrals. Previous optimization methods over La-
porta seeding, such as those in [19, 72, 73], have imposed
constraints on the search space that may not be optimal.
To optimize this process more effectively, we define a se-
ries of subsets Si from the entire set of initial seeding
integrals:
S1 ⊂S2 ⊂S3 ⊂· · · ⊂Sseeding .
(4)
Here, Sseeding represents the full set of initial seeding in-
tegrals, determined through Laporta seeding or improved
seeding, such that solving the IBP system generated from
the seeding integrals in Sseeding allows for the solution of
all target integrals. By properly defining these subsets,


--- Page 4 ---
4
there must exist a subset Sn that is the smallest subset
sufficient for solving the target integrals.
LLM
best shot prompt
Code Execution Successful?
have a priority 
function in output
Database
{priority: mean seeding integrals}
Evaluator:
Use the priority to do the IBP 
reduction
Y
Y
N
FIG. 1: This flowchart outlines the operational sequence of the FunSearch algorithm, which is designed to optimize
priority functions for IBP reduction tasks. The process begins with retrieving of one or two existing priority
functions from the database. These functions, coupled with a concise explanation of the IBP reduction process, are
formatted into a best-shot prompt. This prompt serves as input for a pre-trained LLM, which generates new
priority functions based on the provided information. FunSearch then extracts these newly generated priority
functions from the LLM’s output. The extracted priority functions undergo a validation process to determine if they
are executable. If successful, the evaluator utilizes these functions to perform IBP reductions, assessing their
effectiveness. Functions that demonstrate superior performance are subsequently integrated back into the database,
enriching it for future iterations of the algorithm. This cyclical process ensures continuous improvement and
refinement of the priority functions used in IBP reductions.
In general, the memory footprint for IBP reduction is
approximately proportional to the size of the IBP system,
which in turn is proportional to the number of required
seeding integrals. Therefore, our goal is to identify the
smallest subset Sn. Using seeding integrals from Sn in-
stead of Sseeding is then expected to reduce memory usage
and improve computational efficiency greatly. In other
words, Sn is an optimization of Sseeding.
To provide an effective way for finding Sn, we define
a priority function F (I(ni); Itarget(ti)), which calculates
the priority of a seeding integral I(ni) for solving a given
target integral Itarget(ti). Here, ni and ti denote the in-
dices of the seeding integral and the target integral, re-
spectively. This function is defined on the index space
of Sseeding and takes values in R. As an example, in the
case of seeding integrals with indices n1, n2, the priority
function for Laporta seeding is given by −n1 −n2, which
is irrelevant to the indices of the target integral. With


--- Page 5 ---
5
the definition of priority function in hand, the subset of
the seeding integrals can be expressed as
Si = {I(ni) | F (I(ni); Itarget(ti)) > fi} ,
(5)
where
fmax ≥f1 > f2 > f3 > · · · ≥fmin .
(6)
The upper and lower bounds of fi are:
fmax = F(Itarget; Itarget) ,
(7)
fmin = min
 F(Iany seeding; Itarget)

,
(8)
where Iany seeding represent all initial seeding integrals,
and Itarget represent the target integrals.
Provided that there exists a sufficiently accurate prior-
ity function F0 for estimating the priority of the seeding
integral I(ni), a small solvable subset can be defined as:
Sn = {I(ni) | F0 (I(ni); Itarget(ti)) > fc} ,
(9)
where fc, and consequently Sn, can be determined using
a binary search algorithm once the corresponding priority
function F0 has been identified. Therefore, our goal is to
determine the best-estimated priority function F0.
In practical operation, identifying the optimal priority
function is a critical challenge. While the optimal pri-
ority function may vary for different IBP problems, it is
impractical to run the complex IBP reduction thousands
of times as an evaluator to test and search for the best-
estimated priority function. Considering the scalability
and interpretability of FunSearch, we hope to obtain a
general best-estimated priority function under a simpler
IBP problem and naturally extend it to more complex
problems.
Specifically, we perform FunSearch for IBP reductions
of the one-loop massless bubble integral, where the cor-
responding index space of the Feynman integral is two-
dimensional. This problem is simple enough while still
possessing basic geometric properties.
The one-loop
massless bubble integral is defined as
I(n1, n2) =
Z
ddl
iπd/2−1
1
(l2)n1 (l −p)2n2 ,
(10)
where we set p2 = 1. The integrals satisfy the following
IBP identities:
0 = (d −2n1 −n2)I(n1, n2) −n2I(n1 −1, n2 + 1)
+ n2I(n1, n2 + 1) ,
(11)
0 = n1I(n1 + 1, n2 −1) −n1I(n1 + 1, n2)
−n2I(n1 −1, n2 + 1) + n2I(n1, n2 + 1)
+ (n2 −n1)I(n1, n2)
(12)
with zero sectors I(1, 0), I(0, 1) and a single master inte-
gral I(1, 1).
For this one-loop IBP reduction problem, the priority
function can be written as:
F(I(ni), Itarget(ti)) = priority(n1, n2; t1, t2) .
(13)
When evaluating the above priority function, we choose
Sseeding to be all non-zero integrals inside a square box
with a side length of 10+t1+t2. This box is large enough
to ensure that the final best-estimated priority function
is optimized based on the properties of the index space,
rather than the choice of the seeding integrals. This al-
lows us to confidently extend the results to more com-
plex IBP reduction problems. For the same reason, the
evaluation is performed for several different single-target
integrals, and the priority score is determined based on
the average size of Sn. Under this set of seeding inte-
grals, the Laporta seeding itself also manifests as such a
priority:
priorityLaporta(n1, n2; t1, t2) = −n1 −n2 ,
(14)
and
Sn =
{I(n1, n2) | priorityLaporta(n1, n2; t1, t2) ≥−t1 −t2} .
(15)
Through the steps outlined above, we can encode
the simple one-loop IBP reduction problem into Fun-
Search using a priority function priority(n1, n2; t1, t2).
This allows us to find an optimal priority function
priority0(n1, n2; t1, t2) via the iterative operation of Fun-
Search.
Subsequently,
by identifying the function
F0(I(ni); Itarget(ti)) from priority0(n1, n2; t1, t2), we can
naturally extend it to more complex IBP reduction prob-
lems, thereby optimizing the IBP reduction in a more
general sense. We anticipate that this approach will re-
duce memory requirements, enhance computational ef-
ficiency, and provide a more efficient and cost-effective
solution for IBP reduction problems.
V.
PRIORITY FUNCTION FOUND BY
FUNSEARCH AND GENERALIZED BY HUMAN
EXPERTS
Having introduced the concept of the priority function
and its application to IBP reductions via FunSearch, we
now present the results of optimizing Feynman integral
reductions using FunSearch.
Using our method, which combines LLMs, evolution-
ary algorithms, and a problem specific priority function,
we successfully discovered a highly effective priority func-
tion for solving one-loop IBP reductions. Compared to
the traditional Laporta approach, our method achieves
higher computational efficiency and reduced memory us-
age. Owing to its interpretability and scalability, domain
experts can establish an effective feedback loop with the
algorithm to easily extend this priority function to more
complex, multi-target IBP reductions.
Specifically, in one-loop reduction problem presented
above, this priority function reduces the solvable subset


--- Page 6 ---
6
Sn by approximately a factor of 5 compared to the La-
porta seeding, for the target integral I(15, 10). For more
complex target integrals, it achieves an even higher im-
provement factor. During the operation of FunSearch,
several similar best-estimated priority functions repeat-
edly emerged. An example is shown in Listing. 1.
Such best-estimated priority functions cluster around
target and master integrals in the index space. The evo-
lution process seems to encourage such a tendency. A
representative evolution process is shown in Fig. 2. De-
tailed analysis of the evolution process and algorithm ex-
ecution are presented in the appendix.
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
n_1
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
n_2
Target
Master
Laporta Priority
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
n_1
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
n_2
Target
Master
Box Priority
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
n_1
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
n_2
Target
Master
Ellipse-type Priority
40
35
30
25
20
15
10
5
0
6.6
6.3
6.0
5.7
5.4
5.1
4.8
4.5
4.2
3.9
20.0
17.5
15.0
12.5
10.0
7.5
5.0
2.5
0.0
FIG. 2: Visualization of the Laporta, Box, and Ellipse-like priority. The target integral I(15, 10) is marked as
’Target’, and the master Integral is marked as ’Master’.
Listing 1: Priority Function example (m = 2)
1
# Code
2
import
numpy as np
3
def
priority(node: tuple[int , int],
node_target: tuple[int , int]) -> float:
4
x, y = node
5
a, b = node_target
6
d = ((x - a) ** 2 + (y - b) ** 2) **
0.5
7
p = (x ** 2 + y ** 2) ** 0.5 - (a ** 2
+ b ** 2) ** 0.5
8
return - (d + p)
After running FunSearch 10 times, each with 5 000
epochs on one loop IBP reductions problem, we can
easily generalize the best-estimated priority function
priority0(n1, n2; t1, t2) from the best functions found by
FunSearch (for example, Listing 1) to:
F0(I(ni); Itarget(ti))
= −



"X
i
|ni|m
# 1
m
+
"X
i
(ni −ti)2
# 1
2 

,
(16)
where m is a number in set {1, 2, 4, ∞}, and the first term
is max (|ni|) for m = ∞. The priority function in above
equation constitutes one of the main results of this paper
and is applicable to Feynman integrals with any number
of loops and legs. The second term in the priority func-
tion favors seeding integrals without dots or numerators
when solving target integrals of the same type, aligning
with the objectives of syzygy techniques [9–14].
The efficiency of Eq. (16) is evidently scalable. In the
next section, we discuss its application to more complex
IBP reduction problems and further verify its effective-
ness and scalability in large-scale problems.
VI.
APPLICATIONS IN COMPLEX IBP
REDUCTIONS
Building on the success of AI-assistance in optimiz-
ing the one-loop IBP reduction problem, we explore its
application to more complex, multi-loop Feynman in-
tegrals in this section, demonstrating the scalability of
our approach. We employ the Ellipse-type priority func-
tion given in Eq. (16) with m = 2. The symbolic IBP
identities are generated using LiteRed [21, 22], while
Mathematica is employed to construct the initial seed-
ing integrals based on an improved seeding strategy and
to sort them according to the best-estimated priority
function. Finally, we solve the resulting system of lin-
ear equations with FiniteFlow [31], utilizing its sparse
solver capabilities.
In our implementation of improved seeding, we intro-
duce an adjustable vector (g0, g1, · · · gn, · · · ) to describe
the generation process: seeding integrals for sectors with
t = tp −n are produced only if they satisfy s ≤sp + gn,
starting from a primary sector with t = tp and s = sp for
the most complicated target integrals. A similar imple-
mentation is adopted in Ref. [19]. The minimal seeding
set which enables the solution of target integrals within
the improved seeding framework can be determined by
searching for the optimized vector (g0, g1, · · · gn, · · · ) and
the parameter dmax, starting from the configuration that
generates the fewest seeding integrals. As an example,
choosing gn = −n and dmax = 0 could yield a minimal
seeding set within this framework. Hereafter, ”improved


--- Page 7 ---
7
seeding” always refers to the minimal seeding within the
improved seeding framework.
The initial seeding integrals generated by the improved
seeding strategy can be classified into different categories,
which are then ordered based on the values of the priority
function. Typically, O(102) categories are produced for
O(105) seeding integrals. In most cases, the seeding in-
tegrals from only the first few categories are sufficient to
reduce the target integrals, significantly lowering memory
usage.
To determine the required number of categories, one
can directly apply the binary search algorithm to identify
the smallest subset Sn as defined in Eq. (9). However,
Sn does not always yield the optimal results in terms
of equation-solving time. This can be explained by the
fact that the priority function F0 is primarily designed
to minimize the number of required seeding integrals,
rather than to optimize the time for solving the linear sys-
tem. Designing a priority function to minimize equation-
solving time is left for future work. To provide greater
flexibility in selecting a suitable subset and thereby im-
prove both memory efficiency and computational time,
we propose the following search algorithm:
1. Count the total number of initial seeding integrals,
denoted as Ntot.
2. From left to right, identify the first M categories
in which the seeding integrals collectively span all
master integrals. Let the number of seeding inte-
grals in these first M categories be Nm.
3. Introduce an adjustable relaxation factor 0 ≤a ≤1
and define
Nr = N a
tot N 1−a
m
.
(17)
Identify the first G categories that collectively con-
tain at least Nr seeding integrals.
4. Generate IBP equations separately from the seed-
ing integrals in the first M and first G categories.
Solve the resulting linear equations using the sparse
solver provided in FiniteFlow. If both cases suc-
cessfully reduce the target integrals, we stop.
If
only the second case is sufficient, we use all seed-
ing integrals for reductions and also apply a binary
search to refine the selection within the range from
the first-(M +1) to the first-(G−1) categories. The
optimal choice is determined by the configuration
that solves the equations for a numerical sample in
the shortest time. If the seeding integrals from the
first G categories are still insufficient to reduce the
target integrals, we use all available seeding inte-
grals, equivalent to the improved seeding strategy.
5. The final selection of categories directly determines
the required seeding integrals.
The above algorithm also improves the search time com-
pared to a direct binary search approach.
In the following, we present several explicit examples to
show the effectiveness of Ellipse-type priority functions.
We will first show the examples with single target inte-
grals and then present an example with multiple target
integrals. In all examples, we set the relaxation factor
a in Eq. (17) to 1/3. All subsequent computations are
performed on the same machine, equipped with a Xeon
Gold 6148 CPU and 768 GB of available memory.
(a) The planar 5-loop
self-energy integral family,
where the propagators
crossed by dashed lines
represent cut propagators.
(b) A non-planar 5-loop
self-energy integral family,
where the propagators
crossed by dashed lines
represent cut propagators.
FIG. 3: Two six-particle phase-space integral families
A.
A planar six-particle phase-space integral family
We consider a planar six-particle phase-space integral
family, illustrated in Fig. 3a, which arises in the study
of multi-loop scattering amplitudes in quantum field the-
ory [30, 81–83]. The corresponding propagators are given
by
D1 = l2
1, D2 = l2
2, D3 = l2
3, D4 = l2
4, D5 = l2
5,
D6 = (l1 −p)2, D7 = (l2 −p)2, D8 = (l3 −p)2,
D9 = (l4 −p)2, D10 = (l5 −p)2, D11 = (l1 −l2)2,
D12 = (l2 −l3)2, D13 = (l3 −l4)2, D14 = (l4 −l5)2,
D15 = (l1 −l3)2, D16 = (l1 −l4)2, D17 = (l1 −l5)2,
D18 = (l2 −l4)2, D19 = (l2 −l5)2, D20 = (l3 −l5)2 ,
(18)
where p is the external momentum with p2 > 0, the
propagators D1, D10, D11, D12, D13, D14 are cut prop-
agators, while the last six propagators are ISPs. This
integral family contains 30 IBP identities and 25 master
integrals. We choose the following set of integrals to test
the effectiveness of priority function F0, the first seven
integrals are from the top sector with the first 14 indices


--- Page 8 ---
8
being 1:
s2 = (1, · · · , 1, −1, −1, 0, 0, 0, 0),
s3 = (1, · · · , 1, −1, −1, −1, 0, 0, 0),
s4 = (1, · · · , 1, −1, −1, −1, −1, 0, 0),
s5 = (1, · · · , 1, −1, −1, −1, −1, −1, 0),
s6 = (1, · · · , 1, −1, −1, −1, −1, −1, −1),
s8 = (1, · · · , 1, −2, −2, −1, −1, −1, −1),
s10 = (1, · · · , 1, −2, −2, −2, −2, −1, −1) ,
s12 = (1, · · · , 1, −2, −2, −2, −2, −2, −2) ,
(19)
and the last three are sub-sector integrals with dots and
numerators:
d1 = (1, 0, −1, 1, 0, 1, −1, 1, 0, 1, 2, 1, 1, 1, −2, 0, 0, 0, −1, 0) ,
d3 = (2, 0, −1, 2, 0, 1, −1, 1, 0, 1, 2, 1, 1, 1, −2, 0, 0, 0, −1, 0) ,
d5 = (2, 0, −1, 2, 0, 1, −1, 2, 0, 2, 2, 1, 1, 1, −2, 0, 0, 0, −1, 0) .
(20)
Tab. I presents a performance comparison of IBP re-
ductions using different seeding strategies: Laporta seed-
ing, improved seeding, and our best-estimated priority
function F0, as defined in Eq.(16).
For Laporta seed-
ing, we use the C++ package FIRE6 [23, 24] as a rep-
resentative package. For improved seeding and seeding
from priority function F0, we impose FiniteFlow [31]
to solve the sparse linear equation systems.
To en-
hance efficiency of reductions, FIRE6 is called with four
CPU cores to evaluate a single numerical sample point
over finite fields [7, 8].
The number of seeding inte-
grals listed in the second column of Tab. I is estimated
by dividing the total number of equations generated by
FIRE6 by the number of symbolic IBP identities.
For
the planar integral family, the improved seeding pattern
(g0, g1, g2, g3, · · · ) = (0, −1, −1, −2, · · · ), d ≤dmax = 0 is
sufficient and is adopted to reduce the selected integrals.
The results in the table indicate that while FIRE6 ef-
ficiently manages memory usage, it generates an exces-
sive number of seeding integrals, making it infeasible to
compute s8 and beyond within a reasonable time. The
improved seeding approach successfully handles s8 but
cannot extend beyond that, whereas the priority func-
tion F0 enables the computation of integrals up to s12
with relative ease, achieving an improvement factor of
24.8 in the number of required seeding integrals. This
highlights the crucial role of the priority function method
in drastically reducing the computational complexity of
IBP reductions, allowing previously intractable integrals
to be computed efficiently within practical memory lim-
its.
Integrals
FIRE6 (4 cores)
Improved seeding
Priority function F0
NI
NP
TI
TP
s2
1 299 923/286.7
3061/11.8/0.069
2985/12.3/0.066
1.03 1.05
s3
7 276 962/1147.21
4125/18.3/0.13
3708/15.4/0.11
1.11 1.18
s4
20 549 627 /2954.26
10 803/47.5/0.34
7144/32.9/0.34
1.51
1.0
s5
81 359 547/18779.5
35 065/165.7/1.5
19 052/89/1.25
1.84
1.2
s6
237 961 845/39245.8 108 215/551.9/5.02
46 390/226.8/4.0
2.33 1.26
s8
-
791 164/4271.4/36.2
101 549/624/20.9
7.79 1.73
s10
-
4 388 968/OOM
268 909/2198.8/132.7
16.32
-
s12
-
19 901 220/OOM
802 488/12508.9/1462.3 24.8
-
d1
523 596/249
154 700/686.39/0.07
1791/8.8/0.06
86.38 1.17
d3
2 940 855/1058.5
2 775 780/OOM
3466/15.8/0.35
800.9
-
d5
11 654 385/4523.17
21 740 796/OOM
7109/47.7/2.24
3058
-
TABLE I: Performance comparison of different IBP reduction methods for selected planar integrals. In column 2,
the notation a/b represents (number of seeding integrals) / (time in seconds to solve a single numerical sample over
finite fields using FIRE6 with four cores). And a dash ’-’ in this column indicates that the calculation was not
performed due to the potentially long runtime, as suggested by the scaling behavior of computation time from s2 to
s6. In columns 3–4, a/b/c denotes (number of seeding integrals) / (time in seconds for FiniteFlow to learn the
linear system) / (time in seconds for FiniteFlow to solve a single numerical sample over finite fields). The
abbreviation ’OOM’ indicates an out-of-memory error due to exceeding the 400GB RAM limit. The ratios NI
NP and
TI
TP measure the improvements in the number of seeding integrals and computational efficiency, respectively, when
comparing the F0-prioritized approach to the improved seeding method. The priority function F0 exhibits superior
scalability, particularly for high-complexity integrals such as s12 and d5.
For subsector integrals with many dots and numera-
tors, the advantage of using the priority function F0 be-


--- Page 9 ---
9
comes even more pronounced. For example, the integral
d5 initially required 21 740 796 seeding integrals, whereas
our method reduces this number to just 7109—an im-
provement by a factor of 3058. Previously, the IBP re-
duction process demanded excessive memory, exceeding
400GB RAM, making it impractical for improved seeding
strategy. With our approach, the reduction including the
reconstruction of D can now be performed on a standard
laptop in under two minutes, highlighting the remarkable
efficiency and scalability of the priority function F0.
B.
A non-planar six-particle phase-space integral
family
In this example, we consider a non-planar six-particle
phase-space integral family shown in Fig. 3b, the propa-
gators take the following explicit form:
D1 = l2
1, D2 = l2
2, D3 = l2
3, D4 = l2
4, D5 = l2
5,
D6 = (l1 −p)2, D7 = (l5 −p)2, D8 = (l2 −l3 + l5 −p)2,
D9 = (l1 −l3 + l5 −p)2, D10 = (l1 −l3 + l4 −p)2,
D11 = (l1 −l2)2, D12 = (l2 −l3)2, D13 = (l3 −l4)2,
D14 = (l4 −l5)2, D15 = (l1 −l3)2, D16 = (l1 −l4)2,
D17 = (l1 −l5)2, D18 = (l2 −l4)2, D19 = (l2 −l5)2,
D20 = (l3 −l5)2 .
(21)
where the propagators D3, D9, D11, D12, D13, D14 are cut
propagators, and the last six are ISPs. The integral fam-
ily contains 30 IBP identities and 46 master integrals. To
test the priority function, we take the top sector integrals
s2 to s6 which have the same form as shown in Eq. (19)
and the following integrals with dots and numerators
d2 = (0, 1, 1, −1, −1, 0, 1, 0, 2, 2, 1, 1, 1, 1, −1, −1, 0, 0, 0, 0) ,
d3 = (0, 1, 2, −1, −1, 0, 2, 0, 2, 2, 1, 1, 1, 1, −1, −1, 0, 0, 0, 0) ,
d5 = (0, 2, 2, −1, −1, 0, 2, 0, 2, 2, 1, 1, 1, 1, −1, −1, 0, 0, 0, 0) .
(22)
The non-planar integral family presents significantly
greater computational challenges compared to the pla-
nar case.
The improved seeding strategy used for the
planar family is no longer sufficient. Instead, a more re-
fined seeding pattern is required: (g0, g1, g2, g3, · · · ) =
(0, −1, −2, −3, · · · ), d ≤dmax = 1.
Tab. II presents a
performance comparison of IBP reductions using differ-
ent seeding strategies: improved seeding, and our best-
estimated priority function F0.
For integrals s2 to s5, the improved seeding and pri-
ority function F0 yield the same results when using the
searching algorithm introduced at the beginning of this
section. For the integral s6, the improved seeding strat-
egy is unable to solve the system within the 400GB mem-
ory constraint, whereas the priority function F0 success-
fully handles the reduction.
This shows the necessity
of using the priority function F0 method even though it
could be not quite efficient in time. For sub-sector inte-
grals with many dots and numerators, the priority func-
tion F0 demonstrates similar improvements as observed
in the planar case. For example, for the integral d5, it
results in an improvement factor of 1060, showcasing the
effectiveness of the approach in significantly reducing the
number of required seeding integrals.
Integrals
Improved seeding
Priority function F0
NI
NP
TI
TP
s2
34 755/336.28/6.2
34 755/322/6.4
1
0.97
s3
34 755/360.52/7.54
34 755/320.78/6.34
1
1.19
s4
59 157/1383/26.2
59 157/1383/25.5
1
1.03
s5
160 377/10361.7/151.7 160 377/10300.45/155.1
1
0.98
s6
492 401/OOM
93 297/15626.5/2539.3
5.3
-
d2
598 428/7956.6/5.7
1337/11.6/0.58
447.59 9.83
d4
4 779 738/OOM
4709/56.4/5.05
1015.02
-
d5
11 519 163/OOM
10 859/207.8/16.2
1060
-
TABLE II: Performance comparison of IBP reductions using different seeding patterns for selected non-planar
integrals. In this table, we do not include results for Laporta seeding. The format follows the same conventions as
those in Tab. I.
C.
An example with multiple target integrals
In the above two examples, the priority function is ap-
plied to the cases with only a single target integral. In
this example, we apply priority function to the case with
multiple target integrals.
For simplicity, we apply the
priority function recursively to the target integrals, each
time we only choose the most complicated unsolved tar-
get integral. Since we are able to solve a large fraction of
target integrals each time, applying the priority function


--- Page 10 ---
10
recursively a few times can give solutions for all target
integrals.
We take the integral family from the first example,
and derive dimensional recurrence relation [84] for the
top sector integral s1 = (1, · · · , 1, −1, 0, 0, 0, 0, 0) by ex-
pressing it in D + 2 dimension in terms of linear com-
binations of integrals in D dimension. This procedure
generate 955 target integrals, among them the most com-
plicated integrals have t = 14, s = 6, d = 0.
For
this example, we find that applying the priority func-
tion one time is able to derive the above recurrence re-
lation. The priority function method generates 129 333
seeding integrals, and takes FiniteFlow 948.4 seconds
to learn the linear system and 13.3 seconds to solve the
system on one numerical sample point. As a compari-
son, the improved seeding method with seeding pattern
(g0, g1, g2, g3, · · · ) = (0, −1, −1, −2, · · · ), d ≤dmax = 0
generate 255 929 seeding integrals, and takes FiniteFlow
2733.56 seconds and 14.1 seconds to learn and solve the
system on one numerical sample point, respectively. This
again demonstrates the improvement of the priority func-
tion method over the improved seeding method.
VII.
CONCLUSIONS
In this work, we introduced a novel approach to
optimizing the reduction of Feynman integrals using
integration-by-parts (IBP) identities.
By developing a
priority function method through the FunSearch algo-
rithm—a combination of large language models (LLMs)
and genetic algorithms—we have identified the best-
estimated priority function F0 as shown in Eq. (16). The
method has led to significant improvements in both mem-
ory usage and computational efficiency compared to tra-
ditional methods, such as the Laporta seeding and im-
proved seeding strategies.
Our best-estimated priority function F0 applies to, and
has been rigorously tested on, a diverse set of Feynman
integrals, including both single-loop and multi-loop cases
with planar and non-planar scenarios.
Specifically, we
have examined planar and non-planar six-particle phase-
space integral families in multi-loop scenarios. The re-
sults demonstrate that our method can substantially re-
duce required seeding integrals, leading to significant
memory savings and faster computation times. For in-
stance, for top sector integrals in the case of the planar
six-particle phase-space integral family, we observed a
reduction in the number of seeding integrals by up to a
factor of 24.8, whereas in the non-planar case, the re-
duction reached a factor of 5.3. For subsector integrals
with many dots and numerators, the advantage of using
the priority function F0 becomes even more pronounced,
achieving an improvement factor of 3058 in the planar
case and 1060 in the non-planar case. More importantly,
we observed that as the complexity of the Feynman in-
tegrals increases, the improvement factor achieved grows
larger. These findings underscore the effectiveness and
scalability of our approach, especially for integrals of high
complexity.
The success of our method is attributed to the unique
synergy between large language models (LLMs) and evo-
lutionary algorithms, which enables the generation of ef-
ficient and interpretable priority functions. This frame-
work provides a more optimized solution for IBP reduc-
tions and offers a scalable and generalizable approach
that can be extended to more complex problems. More-
over, the interpretability of the priority functions gen-
erated by FunSearch allows domain experts to further
refine and adapt these functions for specific applications,
thereby enhancing their practical utility.
In summary, our work contributes to applying AI
methods to high-energy theoretical physics by propos-
ing a new tool for optimizing the reduction of Feynman
integrals.
Combining the FunSearch algorithm with a
priority function offers a potentially more efficient and
scalable framework for addressing some of the computa-
tional challenges associated with multi-loop calculations.
This development could help facilitate more practical
evaluations of scattering amplitudes and cross-sections,
supporting researchers in studying higher-order processes
and complex topologies in quantum field theory. We hope
this work inspires future efforts to optimize priority func-
tions and explore further applications of explainable AI
methods in theoretical physics.
ACKNOWLEDGEMENTS
We would like to thank T. Peraro for the helpful in-
structions on the usage of FiniteFlow. T.-Z.Y. is sup-
ported by the European Research Council (ERC) under
the European Union’s Horizon 2020 research and innova-
tion programme grant agreement 101019620 (ERC Ad-
vanced Grant TOPUP). H.X.Z. is supported by the Na-
tional Science Foudantion of China under contract No.
12425505 and Asian Young Scientist Fellowship.
The
work of Q.-H. C. is partly supported by the National Sci-
ence Foundation of China under Grant Nos. 12235001.
M.-X.L. is supported by National Natural Science Foun-
dation of China under contract No. U2230402.
Note Added: While finalizing this paper, we became
aware of Ref. [85], where FunSearch was applied to IBP
from a different perspective. In Ref. [85], FunSearch is
used to search for conditional expressions, whereas in our
case, it is employed to search for a mathematical priority
function.
[1] K. G. Chetyrkin and F. V. Tkachov, Nucl. Phys. B 192,
159 (1981).
[2] F. V. Tkachov, Phys. Lett. B 100, 65 (1981).


--- Page 11 ---
11
[3] A. V. Kotikov, Phys. Lett. B 254, 158 (1991).
[4] T. Gehrmann and E. Remiddi, Nucl. Phys. B 580, 485
(2000), arXiv:hep-ph/9912329.
[5] J. M. Henn, Phys. Rev. Lett. 110, 251601 (2013),
arXiv:1304.1806 [hep-th].
[6] S. Laporta, Int. J. Mod. Phys. A 15, 5087 (2000),
arXiv:hep-ph/0102033.
[7] A. von Manteuffel and R. M. Schabinger, Phys. Lett. B
744, 101 (2015), arXiv:1406.4513 [hep-ph].
[8] T. Peraro, JHEP 12, 030 (2016), arXiv:1608.01902 [hep-
ph].
[9] J. Gluza, K. Kajda, and D. A. Kosower, Phys. Rev. D
83, 045012 (2011), arXiv:1009.0472 [hep-th].
[10] K. J. Larsen and Y. Zhang, Phys. Rev. D 93, 041701
(2016), arXiv:1511.01071 [hep-th].
[11] J. B¨ohm, A. Georgoudis, K. J. Larsen, M. Schulze,
and
Y.
Zhang,
Phys.
Rev.
D
98,
025023
(2018),
arXiv:1712.09737 [hep-th].
[12] R. N. Lee, in 49th Rencontres de Moriond on QCD
and
High
Energy
Interactions
(2014)
pp.
297–300,
arXiv:1405.5616 [hep-ph].
[13] T. Bitoun, C. Bogner, R. P. Klausen,
and E. Panzer,
Lett. Math. Phys. 109, 497 (2019), arXiv:1712.09215
[hep-th].
[14] B. Agarwal, S. P. Jones, and A. von Manteuffel, JHEP
05, 256 (2021), arXiv:2011.15113 [hep-ph].
[15] P. Mastrolia and S. Mizera, JHEP 02, 139 (2019),
arXiv:1810.03818 [hep-th].
[16] H. Frellesvig, F. Gasparotto, M. K. Mandal, P. Mastro-
lia, L. Mattiazzi, and S. Mizera, Phys. Rev. Lett. 123,
201602 (2019), arXiv:1907.02000 [hep-th].
[17] X. Liu and Y.-Q. Ma, Phys. Rev. D 99, 071501 (2019),
arXiv:1801.10523 [hep-ph].
[18] X. Guan, X. Liu,
and Y.-Q. Ma, Chin. Phys. C 44,
093106 (2020), arXiv:1912.09294 [hep-ph].
[19] X. Guan, X. Liu, Y.-Q. Ma,
and W.-H. Wu,
(2024),
arXiv:2405.14621 [hep-ph].
[20] C. Anastasiou and A. Lazopoulos, JHEP 07, 046 (2004),
arXiv:hep-ph/0404258.
[21] R. N. Lee, (2012), arXiv:1212.2685 [hep-ph].
[22] R. N. Lee, J. Phys. Conf. Ser. 523, 012059 (2014),
arXiv:1310.1145 [hep-ph].
[23] A. V. Smirnov and F. S. Chukharev, Comput. Phys.
Commun. 247, 106877 (2020), arXiv:1901.07808 [hep-
ph].
[24] A. V. Smirnov and M. Zeng, Comput. Phys. Commun.
302, 109261 (2024), arXiv:2311.02370 [hep-ph].
[25] A.
von
Manteuffel
and
C.
Studerus,
(2012),
arXiv:1201.4330 [hep-ph].
[26] C. Studerus, Comput. Phys. Commun. 181, 1293 (2010),
arXiv:0912.2546 [physics.comp-ph].
[27] P. Maierh¨ofer, J. Usovitsch, and P. Uwer, Comput. Phys.
Commun. 230, 99 (2018), arXiv:1705.05610 [hep-ph].
[28] J. Klappert and F. Lange, Comput. Phys. Commun. 247,
106951 (2020), arXiv:1904.00009 [cs.SC].
[29] J. Klappert, F. Lange, P. Maierh¨ofer,
and J. Uso-
vitsch, Comput. Phys. Commun. 266, 108024 (2021),
arXiv:2008.06494 [hep-ph].
[30] B. Ruijl, T. Ueda,
and J. A. M. Vermaseren, Comput.
Phys. Commun. 253, 107198 (2020), arXiv:1704.06650
[hep-ph].
[31] T. Peraro, JHEP 07, 031 (2019), arXiv:1905.08019 [hep-
ph].
[32] Z. Wu, J. Boehm, R. Ma, H. Xu, and Y. Zhang, Comput.
Phys. Commun. 295, 108999 (2024), arXiv:2305.08783
[hep-ph].
[33] W. Chen, (2024), arXiv:2408.06426 [hep-ph].
[34] B. Romera-Paredes, M. Barekatain, A. Novikov, M. Ba-
log, M. P. Kumar, E. Dupont, F. J. Ruiz, J. S. Ellenberg,
P. Wang, O. Fawzi, et al., Nature 625, 468 (2024).
[35] Y.-H.
He,
Nature
Rev.
Phys.
6,
546
(2024),
arXiv:2405.19973 [math.HO].
[36] Y.-H. He, ed., Machine Learning in Pure Mathematics
and Theoretical Physics (World Scientific, 2023).
[37] Y.-H. He and S.-J. Lee, Phys. Lett. B 798, 134889 (2019),
arXiv:1904.08530 [hep-th].
[38] Y.-H. He, Int. J. Mod. Phys. A 36, 2130017 (2021),
arXiv:2011.14442 [hep-th].
[39] T. Cai, G. W. Merz, F. Charton, N. Nolte, M. Wilhelm,
K. Cranmer,
and L. J. Dixon, Mach. Learn. Sci. Tech.
5, 035073 (2024), arXiv:2405.06107 [cs.LG].
[40] L. J. Dixon, O. Gurdogan, A. J. McLeod, and M. Wil-
helm, JHEP 07, 153 (2022), arXiv:2204.11901 [hep-th].
[41] M. D. Schwartz,
(2021), 10.1162/99608f92.beeb1183,
arXiv:2103.12226 [hep-ph].
[42] A. Dersy, M. D. Schwartz,
and X. Zhang, Int. J. Data
Sci. Math. Sci. 1, 135 (2024), arXiv:2206.04115 [cs.LG].
[43] A. Dersy, M. D. Schwartz, and A. Zhiboedov, JHEP 05,
200 (2024), arXiv:2308.09451 [hep-th].
[44] C. Cheung, A. Dersy, and M. D. Schwartz, SciPost Phys.
18, 040 (2025), arXiv:2408.04720 [hep-th].
[45] M. Demirtas, J. Halverson, A. Maiti, M. D. Schwartz,
and K. Stoner, Mach. Learn. Sci. Tech. 5, 015002 (2024),
arXiv:2307.03223 [hep-th].
[46] A. Bhattacharya, J. Cotler, A. Dersy,
and M. D.
Schwartz,
Phys.
Rev.
D
110,
116023
(2024),
arXiv:2402.18633 [hep-th].
[47] P. T. Komiske, E. M. Metodiev,
and M. D. Schwartz,
JHEP 01, 110 (2017), arXiv:1612.01551 [hep-ph].
[48] P. T. Komiske, E. M. Metodiev, B. Nachman, and M. D.
Schwartz, JHEP 12, 051 (2017), arXiv:1707.08600 [hep-
ph].
[49] T. Heimel, G. Kasieczka, T. Plehn, and J. M. Thompson,
SciPost Phys. 6, 030 (2019), arXiv:1808.08979 [hep-ph].
[50] H. Qu and L. Gouskos, Phys. Rev. D 101, 056019 (2020),
arXiv:1902.08570 [hep-ph].
[51] S. Gong, Q. Meng, J. Zhang, H. Qu, C. Li, S. Qian,
W. Du, Z.-M. Ma, and T.-Y. Liu, JHEP 07, 030 (2022),
arXiv:2201.08187 [hep-ph].
[52] S. Bright-Thonney, B. Nachman,
and J. Thaler, Phys.
Rev. D 110, 014029 (2024), arXiv:2311.07652 [hep-ph].
[53] E. M. Metodiev, J. Thaler, and R. Wynne, Phys. Rev.
D 110, 055012 (2024), arXiv:2312.00119 [hep-ph].
[54] K. Desai, B. Nachman, and J. Thaler, Phys. Rev. D 110,
116013 (2024), arXiv:2407.11284 [hep-ph].
[55] R. Gambhir, A. Osathapan, and J. Thaler, Phys. Rev.
D 110, 074020 (2024), arXiv:2403.08854 [hep-ph].
[56] T. Heimel, N. Huetsch, R. Winterhalder, T. Plehn,
and
A.
Butter,
SciPost
Phys.
17,
129
(2024),
arXiv:2310.07752 [hep-ph].
[57] S.
Badger
et
al.,
SciPost
Phys.
14,
079
(2023),
arXiv:2203.07460 [hep-ph].
[58] S. Forte, L. Garrido, J. I. Latorre,
and A. Piccione,
JHEP 05, 062 (2002), arXiv:hep-ph/0204232.
[59] R. D. Ball et al. (NNPDF), JHEP 04, 040 (2015),
arXiv:1410.8849 [hep-ph].
[60] D. Liu, C. Sun,
and J. Gao, JHEP 08, 088 (2022),
arXiv:2201.06586 [hep-ph].


--- Page 12 ---
12
[61] S. Badger and J. Bullock, JHEP 06, 114 (2020),
arXiv:2002.07516 [hep-ph].
[62] J. Aylett-Bullock, S. Badger, and R. Moodie, JHEP 08,
066 (2021), arXiv:2106.09474 [hep-ph].
[63] O. Fedkevych, C. K. Khosa, S. Marzani, and F. Sforza,
Phys. Rev. D 107, 034032 (2023), arXiv:2202.05082 [hep-
ph].
[64] F. Calisto, R. Moodie, and S. Zoia, JHEP 07, 124 (2024),
arXiv:2312.02067 [hep-ph].
[65] J. Halverson and F. Ruehle, Physical Review D 99,
046015 (2019).
[66] S. Gukov, J. Halverson, F. Ruehle,
and P. Su lkowski,
Machine Learning: Science and Technology 2, 025035
(2021).
[67] J. Halverson, A. Maiti, and K. Stoner, Machine Learn-
ing: Science and Technology 2, 035002 (2021).
[68] J. Halverson, B. Nelson, and F. Ruehle, Journal of High
Energy Physics 2019, 1 (2019).
[69] J. Carifio, J. Halverson, D. Krioukov, and B. D. Nelson,
Journal of High Energy Physics 2017, 1 (2017).
[70] V. A. Smirnov, Analytic Tools for Feynman Integrals,
Springer Tracts in Modern Physics, Vol. 250 (Springer,
2012).
[71] C. Anastasiou and K. Melnikov, Nucl. Phys. B 646, 220
(2002), arXiv:hep-ph/0207004.
[72] M. Driesse, G. U. Jakobsen, G. Mogull, J. Plefka,
B. Sauer, and J. Usovitsch, Phys. Rev. Lett. 132, 241402
(2024), arXiv:2403.07781 [hep-th].
[73] Z. Bern, E. Herrmann, R. Roiban, M. S. Ruf, A. V.
Smirnov, V. A. Smirnov,
and M. Zeng, JHEP 10, 023
(2024), arXiv:2406.01554 [hep-th].
[74] P. V. T. Dat, L. Doan, and H. T. T. Binh, “Hsevo: Ele-
vating automatic heuristic design with diversity-driven
harmony search and genetic algorithm using llms,”
(2024), arXiv:2412.14995 [cs.NE].
[75] F. Liu, X. Tong, M. Yuan, X. Lin, F. Luo, Z. Wang,
Z. Lu, and Q. Zhang, “Evolution of heuristics: Towards
efficient automatic algorithm design using large language
model,” (2024), arXiv:2401.02051 [cs.NE].
[76] H. Ye, J. Wang, Z. Cao, F. Berto, C. Hua, H. Kim,
J. Park,
and G. Song, “Reevo: Large language mod-
els as hyper-heuristics with reflective evolution,” (2024),
arXiv:2402.01145 [cs.NE].
[77] Z. Chen, Z. Zhou, Y. Lu, R. Xu, L. Pan,
and Z. Lan,
“Uber:
Uncertainty-based evolution with large lan-
guage models for automatic heuristic design,”
(2024),
arXiv:2412.20694 [cs.NE].
[78] Z. Zheng, Z. Xie, Z. Wang, and B. Hooi, “Monte carlo
tree search for comprehensive exploration in llm-based
automatic heuristic design,”
(2025), arXiv:2501.08603
[cs.AI].
[79] V. Aglietti, I. Ktena, J. Schrouff, E. Sgouritsa, F. J. R.
Ruiz, A. Malek, A. Bellot, and S. Chiappa, “Funbo: Dis-
covering acquisition functions for bayesian optimization
with funsearch,” (2024), arXiv:2406.04824 [cs.LG].
[80] R. Zhang, F. Liu, X. Lin, Z. Wang, Z. Lu, and Q. Zhang,
“Understanding the importance of evolutionary search in
automated heuristic design with large language models,”
(2024), arXiv:2407.10873 [cs.NE].
[81] O. Gituliar, V. Magerya, and A. Pikelner, JHEP 06, 099
(2018), arXiv:1803.09084 [hep-ph].
[82] V. Magerya and A. Pikelner, JHEP 12, 026 (2019),
arXiv:1910.07522 [hep-ph].
[83] V. Maheria, Semi- and Fully-Inclusive Phase-Space Inte-
grals at Four Loops, Ph.D. thesis, Hamburg U. (2022).
[84] O. V. Tarasov, Phys. Rev. D 54, 6479 (1996), arXiv:hep-
th/9606018.
[85] M. von Hippel and M. Wilhelm, “Refining integration-by-
parts reduction of feynman integrals with machine learn-
ing,” (2025), arXiv:2502.05121 [hep-th].
[86] S. Hu, Y. Tu, X. Han, C. He, G. Cui, X. Long, Z. Zheng,
Y. Fang, Y. Huang, W. Zhao, X. Zhang, Z. L. Thai,
K. Zhang, C. Wang, Y. Yao, C. Zhao, J. Zhou, J. Cai,
Z. Zhai, N. Ding, C. Jia, G. Zeng, D. Li, Z. Liu,
and
M. Sun, “Minicpm: Unveiling the potential of small lan-
guage models with scalable training strategies,” (2024),
arXiv:2404.06395 [cs.CL].
[87] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan,
W. Ge, Y. Han, F. Huang, B. Hui, L. Ji, M. Li, J. Lin,
R. Lin, D. Liu, G. Liu, C. Lu, K. Lu, J. Ma, R. Men,
X. Ren, X. Ren, C. Tan, S. Tan, J. Tu, P. Wang, S. Wang,
W. Wang, S. Wu, B. Xu, J. Xu, A. Yang, H. Yang,
J. Yang, S. Yang, Y. Yao, B. Yu, H. Yuan, Z. Yuan,
J. Zhang, X. Zhang, Y. Zhang, Z. Zhang, C. Zhou,
J. Zhou, X. Zhou, and T. Zhu, “Qwen technical report,”
(2023), arXiv:2309.16609 [cs.CL].
Appendix A: FunSearch Operation and
Hyperparameter Selection
To provide further insight into the implementation and
optimization of FunSearch, we include this appendix de-
tailing the selection of hyperparameters and an analysis
of the evolution process. This information complements
the main results presented in the preceding sections.
1.
Hyperparameter Selection
The selection of hyperparameters significantly impacts
the performance and outcomes of FunSearch. Below lists
some key hyperparameters and their selection criteria:
Model: The CPM-2B model [86] was selected due to
its superior performance in code generation tasks. It does
not require additional fine-tuning and has shorter invo-
cation times compared to other models such as Qwen-
14B [87], which showed similar performance but with
longer processing times.
Temperature: The temperature of the model was set
within the range of 0.9 to 1.1. Temperatures above 1.2
led to non-functional generated functions, while temper-
atures below 0.9 resulted in excessive repetition in model
outputs. At a temperature of 0.01, the output efficiency
decreased by nearly 10 times. As shown in Fig. 4
Islands:
The number of islands was set to 10.
This value was chosen based on literature recommen-
dations [34] and effectively balances computational re-
sources and exploration efficiency.
Island Temperature: The island temperature was
set within the range of 10 to 100. This parameter af-
fects the diversity of programs within an island. Higher
temperatures help prevent the algorithm from falling into
local optima.


--- Page 13 ---
13
Cleanup Frequency: The cleanup frequency was set
to 50 to 100 epoch per cleanup. This parameter ensures
that the worst-performing islands are regularly cleaned
up, maintaining population diversity. After each cleanup,
the best code on the best and second-best islands are
placed back into the cleaned islands. This ensures that
the population maintains a high level of quality.
Evolution Rounds: The number of evolution epochs
was set between 5000 and 100000. An inDistributionitial
value of 5000 rounds is reasonable and can be adjusted.
Output Length: The output length matters and the
output length was set within the range of about 1024
to 4096.
Short output lengths result in overly simple
programs that fail to effectively solve problems, while
long output lengths increase the risk of overfitting due to
excessive diversity. A range of 2048 to 4096 is considered
reasonable.
We allowed priority flipping so the efficiency of Fun-
Search can increase. That is, If priorities outside Laporta
seeding are generally higher than those inside, adding a
sign does not affect the function’s shape and ensures that
priorities expand from low to high complexity, maintain-
ing scalability.
The Laporta priority function in Eq. (14) was used for
initializing the islands. This function is the most widely
used and ensures that priority evolution does not start
from a specific local minimum.
Empirically, evolution
starting from Laporta priority often converges fastest.
Other designed functions tend to overfit or converge to
lower-scoring priorities.
These hyperparameter settings and initialization were
carefully chosen to optimize the performance of Fun-
Search while maintaining computational efficiency and
solution quality.
2.
Brief Analysis of the Evolution Process
In the experiments, we executed FunSearch for one-
loop IBP reductions a total of 10 times, with each run
comprising 5000 epochs.
The initial seeding employed
was the Laporta priority 2. During each epoch, the pri-
ority function was evaluated based on its average seeding
integrals for 30 distinct fixed target integrals in the top
sector of the one-loop IBP reductions. Specifically, the
scoring mechanism was defined as follows:
Score = 200 −
P30
j=1 Sj
30
−L
400,
(A1)
where Sj denotes the minimal number of seeding integrals
required to solve the j-th target, and L represents the
length of the text of the priority function.Here the second
term was introduced to penalize overly complex priority
functions, thereby encouraging more interpretable forms.
The best estimated priority function discovered is
given in Eq. (16). In single-loop reduction, the size of
the small set Sn corresponding to this priority is about 5
times smaller than that of the Laporta seeding. Out of 10
runs, 7 converged to the best-estimated priority function.
During the evolution, there are always 3 platforms,
namely Laporta priority (average seeding integral num-
ber is about 200), Box priority (average seeding integral
number is about 100), and Ellipse-type priority (average
seeding integral number is about 50). A set of typical
function examples is as follows:
- Laporta Priority: The seeding integral number is
approximately (Σni)D, where D is the dimension of the
indices in IBP reductions.
- Box Priority: The seeding integral number is ap-
proximately Πni.
- Ellipse-type Priority: The seeding integral number
is approximately b × Σni, where b is the width of the
ellipse corresponding to the small set Sn.
The changes in the average seeding integral number of
the best estimated priority during the evolution process
are shown in Fig. 4.
Listing 2: Laporta Priority
1
# Epoch 9: Score = 152.2
2
# Code
3
import
numpy as np
4
def
priority(node: tuple[int , int],
node_target: tuple[int , int]) ->
float:
5
return -np.sum(node)
Listing 3: Box Priority
1
# Epoch
400:
Score = 177.5
2
# Code
3
import
numpy as np
4
def
priority(node: tuple[int , int],
node_target: tuple[int , int]) ->
float:
5
n0 , n1 = node
6
n_target0 , n_target1 = node_target
7
d = 4
8
dist0 = max(abs(n0), abs(n_target0))
9
dist1 = max(abs(n1), abs(n_target1))
10
return - (dist0 + dist1) / (d + 2)
Listing 4: Ellipse-type Priority
1
# Epoch
4775:
Score = 188.4
2
# Code
3
import
numpy as np
4
def
priority(node: tuple[int , int],
node_target: tuple[int , int]) ->
float:
5
x, y = node
6
a, b = node_target
7
d = ((x - a) ** 2 + (y - b) ** 2) **
0.5
8
p = (x ** 2 + y ** 2) ** 0.5 - (a **
2 + b ** 2) ** 0.5
9
return - (d + p)


--- Page 14 ---
14
1000
2000
3000
4000
5000
epoch
0
25
50
75
100
125
150
175
200
the number of seeding integrals per target
maxlength = 2048, T = 0.99
maxlength = 1024, T = 0.99
maxlength = 4096, T = 0.99
maxlength = 4096, T=0.01
FIG. 4: Evolution of the average seeding integral
number with different hyperparameters. Each line
represents a different experiment. Red dot for each line
represents the best priority function achieved in this
experiment.
Figure 4 demonstrates how the average seeding inte-
gral number of the best estimated priority evolves over
the number of epochs under different hyperparameters.
As the maxlength increases, the priority function evolves
to allow more intermediate variables. On the other hand,
the penalty term ensures that we always strive to find
shorter and more interpretable priority functions. Con-
sequently, as the length increases from 1024 to 2048, the
final converged result transitions from the Box priority
to the Ellipse-type priority.
Moreover, the temperature parameter significantly
affects the diversity of FunSearch.
Lower tempera-
tures slow down the convergence, but as long as the
maxlength permits, the algorithm eventually converges
to the Ellipse-type priority. This highlights the impor-
tance of balancing exploration and exploitation in the
evolutionary process.
Initialization also plays a crucial role. In our exper-
iments, only the Laporta priority as the initial seeding
allowed the algorithm to escape local minima and pro-
vide effective heuristic priority functions. Other initial-
izations tended to get trapped in local minima, failing to
offer any meaningful improvements.
Appendix B: Executing FunSearch on Complex IBP
Reductions
We attempted to apply FunSearch to more complex
IBP reductions problems, specifically focusing on two-
loop IBP reductions. Despite running the algorithm for
approximately 2 days of CPU time and 2000 epoches, no
results surpassing the Box priority were observed. This
suggests that the complexity of the problem space signifi-
cantly increases with the addition of complexity, making
it more challenging for the algorithm to find improved
priority functions within a reasonable timeframe.
Additionally, we explored the performance of Fun-
Search in scenarios involving multiple target points. Over
30000 epochs (20 islands, CPU time 10879 mins), the
best estimated priority obtained is shown in Listing 5.
Listing 5: Best Approximation Priority for Multi-Target
1
# Epoch
18834:
Score = 153.4
2
# Code
3
import
numpy as np
4
def
priority(node: tuple[int , int], *
node_targets : tuple[int , int]) -> float
:
5
return -max((np.linalg.norm ([x - node
[0], y - node [1]]) ** 2 + node [0]
** 2 + node [1] ** 2 for x, y in
node_targets ))
Its expanded form is given by:
F0(I(ni);Itargetj(tji))
= −
"X
n2
i + max
j
 X
i
(ni −tji)2
!#
. (B1)
This result indicates that even with multiple target
points, the algorithm was able to identify a priority func-
tion that is at least on par with the Ellipse-type priority,
which is the best-estimated priority function we have ob-
served. Testing its full performance in all scenarios is left
for future work. This suggests that the algorithm has the
potential to find effective priority functions in more com-
plex IBP reductions scenarios, but further evaluation is
needed to confirm its superiority.
