--- Page 1 ---
Persona Generators: Generating Diverse Synthetic Personas at Scale
Davide Paglieri 1 Logan Cross 1 William A. Cunningham 1 Joel Z. Leibo 1 Alexander Sasha Vezhnevets 1
Abstract
Evaluating AI systems that interact with humans
requires understanding their behavior across di-
verse user populations, but collecting representa-
tive human data is often expensive or infeasible,
particularly for novel technologies or hypotheti-
cal future scenarios. Recent work in Generative
Agent-Based Modeling has shown that large lan-
guage models can simulate human-like synthetic
personas with high fidelity, accurately reproduc-
ing the beliefs and behaviors of specific individ-
uals. However, most approaches require detailed
data about target populations and often priori-
tize density matching (replicating what is most
probable) rather than support coverage (spanning
what is possible), leaving long-tail behaviors un-
derexplored. We introduce Persona Generators,
functions that can produce diverse synthetic pop-
ulations tailored to arbitrary contexts. We apply
an iterative improvement loop based on AlphaE-
volve, using large language models as mutation
operators to refine our Persona Generator code
over hundreds of iterations. The optimization
process produces lightweight Persona Generators
that can automatically expand small descriptions
into populations of diverse synthetic personas that
maximize coverage of opinions and preferences
along relevant diversity axes. We demonstrate that
evolved generators substantially outperform exist-
ing baselines across six diversity metrics on held-
out contexts, producing populations that span rare
trait combinations difficult to achieve in standard
LLM outputs.
1. Introduction
Many emerging AI technologies interact directly with hu-
mans: mental health assistants that provide advice, personal
health tools that support daily decision-making, economic
systems that adapt to user behavior, and social platforms
1Google DeepMind.
Correspondence to: Davide Paglieri
<paglieridavide@gmail.com>.
Preprint. February 4, 2026.
that shape collective outcomes. Evaluating and improving
these systems requires understanding how they behave when
interacting with diverse human users. However, in many
realistic settings, such as testing novel educational chatbots,
new products, or forecasting the impacts of Artificial Gen-
eral Intelligence (AGI), data from the target population is
limited, expensive or impossible to collect.
An increasingly promising alternative utilizes simulated
users (Anthis et al., 2025). Recent progress in Generative
Agent-Based Modeling (GABM) has made it possible to
construct synthetic personas that display coherent prefer-
ences, attitudes, and behaviors (Park et al., 2023), with
applications ranging from simulating complex social inter-
actions (Park et al., 2023) to replicating behavioral patterns
in economic games (Slumbers et al., 2025). Yet, most exist-
ing work in GABM focuses on algorithmic fidelity—how
accurately synthetic personas reproduce observed human
response patterns, often evaluated by matching the aggre-
gate statistics or distributions derived from real data (Argyle
et al., 2023; Park et al., 2024). This objective suits appli-
cations like digital twins where the target distribution is
known and fidelity is essential. However, it typically re-
quires substantial data from the population of interest and
implicitly emphasizes density matching. In practice, this of-
ten collapses onto a narrow subset of stereotypical responses,
failing to capture the full support of human behavior (Anthis
et al., 2025).
In this paper, our goal is to create populations of personas
capturing the full support of possible attitudes, preferences,
and response patterns, including rarer but consequential con-
figurations typically under-represented in LLM-generated
populations. In many stress-testing settings, it is the outliers,
not the average user, that drive critical failures, so leaving
the long tail underexplored can create a false sense of ro-
bustness. To discover edge cases and identify safety failures,
it is necessary to explore all possible users, not just the most
probable ones. For example, stress-testing a mental health
chatbot requires handling rare, distrustful users with severe
symptoms. This focus on support becomes even more im-
portant in speculative future scenarios, such as forecasting
societal adaptation to AGI, where the true distribution of
human responses is simply unknown. Even when data ex-
ists, the response distributions are much narrower than the
underlying data and often biased toward easily accessible
1
arXiv:2602.03545v1  [cs.AI]  3 Feb 2026


--- Page 2 ---
Generating Diverse Synthetic Personas at Scale
Figure 1. Overview of the method. First, we generate questionnaires containing specific contexts c, dimensions D (diversity axes) and
questions I (items). The Persona Generator Gϕ,θ(c, D, N) uses the context and dimensions as inputs to create a population of synthetic
personas P. These personas are evaluated in Concordia simulations Z = Ψ(P, I) where they answer the questionnaire items. We
measure the diversity of their responses M(Z) and use AlphaEvolve to iteratively optimize the Persona Generator code ϕ.
WEIRD subpopulations (Western, Educated, Industrialized,
Rich, Democratic) or desirable traits like high agreeableness,
leaving rare but consequential behaviors underrepresented
(Bisbee et al., 2024; Anthis et al., 2025; Petrov et al., 2024).
Ultimately, maximizing coverage is more flexible: if the full
support is covered, one can always later sample the target
population to match any specific target density.
However, eliciting such diversity from LLM-based personas
is far from trivial. Naive prompting often leads to mode
collapse, stereotypical outputs, and systematic biases, partly
a byproduct of Reinforcement Learning from Human Feed-
back (RLHF) tuning, even when explicit instructions for di-
versity are provided (Santurkar et al., 2023; Li et al., 2025).
We find that simply asking an LLM to ”generate diverse per-
sonas” typically yields populations clustered around stereo-
typical responses, failing to cover extreme or unusual trait
combinations.
This motivates a different perspective on user simulation.
Rather than constructing a single fixed population of syn-
thetic users, we propose to learn a reusable Persona Gen-
erator: a function capable of producing diverse synthetic
personas on demand for any arbitrary context. This gener-
ator must be robust enough to handle the immense variety
of potential scenarios that we may care about. Importantly,
we aim to optimize a generally capable Persona Generator,
specifically the code that samples and constructs personas,
moving beyond the limitations of optimizing specific popu-
lations.
In this work, we introduce a methodology for learning Per-
sona Generator functions that maximize diversity in traits,
opinions, and preferences across arbitrary contexts. Starting
from a short textual description, we first expand the context
into a structured questionnaire that defines a set of diver-
sity axes. A Persona Generator then produces a population
of synthetic individuals intended to span all possible traits,
opinions, and preferences defined by those axes. To over-
come the difficulties of achieving diversity through standard
prompting, we frame the task as an optimization problem
over the Persona Generator’s code, including prompts tem-
plates and sampling logic, using an evolutionary search loop
powered by AlphaEvolve (Novikov et al., 2025).
The resulting Persona Generator is lightweight and efficient,
enabling rapid, one-shot population synthesis for down-
stream applications. This separation between a costly train-
ing phase and a cheap inference phase makes Persona Gen-
erators practical for repeated use across domains, even when
the original optimization context differs from the deploy-
ment setting.
Our contributions are summarized as follows:
• We formalize the problem of synthetic persona gener-
ation as a diversity maximization task over trait and
preference embeddings, explicitly shifting the objec-
tive from algorithmic fidelity (matching specific indi-
viduals) to support coverage (spanning the space of
possible traits, opinions, and preferences).
2


--- Page 3 ---
Generating Diverse Synthetic Personas at Scale
• We propose a novel Persona Generator function
with a two-stage scalable architecture that separates
population-level diversity decisions from per-persona
background expansions, enabling both control and effi-
ciency, coupled with a scalable pipeline that uses LLMs
to generate questionnaires, simulate interactions, and
evolve code.
• We demonstrate that LLM-driven evolution can dis-
cover novel Persona Generator functions that substan-
tially outperform different baselines in coverage and
diversity metrics.
2. Related Work
Generative Agent-Based Modeling
Generative agent-
based modeling (GABM) has emerged as a powerful
paradigm for simulating complex social interactions and im-
itating human behaviors and preferences. Park et al. (2023)
demonstrated that LLM agents can store memories, reflect,
and simulate complex social interactions. Park et al. (2024)
later scaled this approach to capture population-level diver-
sity, they interviewed 1,000 real individuals and used the
transcripts to ground agent-based models, achieving high
fidelity on psychometric surveys, and vastly outperforming
demographic-only baselines. Nemotron-Personas (Meyer
& Corneil, 2025) and PersonaHub (Ge et al., 2024) scaled
it up to datasets of 100,000 and 1B personas respectively,
with the first being grounded in real-world US statistics
and the latter using web-data. Wang et al. (2025c) worked
on improving narrative depths by constructing a human-
attribute taxonomy. To manage the architectural complexity
of these multi-agent systems, Vezhnevets et al. (2023) devel-
oped Concordia, a library for multi-agent LLM simulations
where a game-master mediates interactions between agents
and the world in a tabletop-RPG-inspired protocol, later for-
malized into an entity-component architectures (Vezhnevets
et al., 2025). Leibo et al. (2024) operationalize the “logic of
appropriateness” of (March & Olsen, 2011), positing that
humans can be modeled as LLM-agents configured to act by
answering three questions: “What kind of situation is this?”,
“What kind of person am I?”, and “What does a person like
me do in a situation like this?”. Finally, beyond prompting,
steering vector techniques offer an alternative way to shape
behaviors by directly modulating LLM’s hidden activations
(Turner et al., 2023; Rimsky et al., 2024; Templeton, 2024;
Zou et al., 2023; Chen et al., 2025).
Algorithmic fidelity
The validity of generative agent-
based modeling can be said to rely on algorithmic fidelity—
a term describing how accurately an LLM can reproduce
the beliefs, attitudes and response patterns for specific hu-
man sub-populations (Argyle et al., 2023). By conditioning
LLMs on context such as demographic backstories (Argyle
et al., 2023), or transcripts from interviews (Park et al.,
2024), it is possible to accurately mirror complex human
behaviors, preferences and opinions. This concept extends
to interactive settings, where Slumbers et al. (2025) demon-
strate that LLMs can simulate collective risk social dilem-
mas with sufficient fidelity to replicate patterns observed in
human behavioral experiments. Santurkar et al. (2023) high-
light that LLMs often fail to align with the opinions of some
demographic groups, even when explicitly steered to do so.
Similarly, Li et al. (2025) and Venkit et al. (2025) caution
that synthetic personas remain prone to systematic biases.
Qualitative analysis by Amirova et al. (2024) points out that
while synthetic agents can capture correct key themes they
may still struggle to match the authentic style of human
speech for many subgroups.
Measuring Preferences and Psychometric Tests
Ques-
tionnaires are a standard tool for measuring human prefer-
ences, attitudes, and psychological traits. Likert-style items
and established inventories such as the Big Five are widely
used across psychology, sociology, and behavioral eco-
nomics, for measuring individual preferences and decision-
making patterns (Likert, 1932; Nunnally, 1978; John &
Srivastava, 1999; Murphy et al., 2011). Questionnaires are
also widely used in agent-based modeling for evaluating
synthetic personas: LLM-driven personas can complete
these inventories while exhibiting consistent trait profiles
(Jiang et al., 2024; Bhandari et al., 2025). Psychometric
surveys have been used to benchmark algorithmic-fidelity
(Park et al., 2024) by comparing responses of real individ-
uals with their simulated personas, and to test the stability
of role-played profiles Wang et al. (2025b). Beyond trait
measurements, psychometric tools can validate the internal
consistency and behavioral coherence of synthetic popula-
tions (Pellert et al., 2024; Maharjan et al., 2025; Castricato
et al., 2025; Bisbee et al., 2024; Petrov et al., 2024), and
recent work explores using LLMs to generate questionnaires
(Adhikari et al., 2025).
Evolution and Discovery with LLMs
LLMs have proven
to be strong evolutionary operators across various domains.
PromptBreeder showed that LLMs can effectively mutate
and refine text in a self-referential loop, improving both task
prompts and the mutation prompts themselves (Fernando
et al., 2023), and QDAIF combined LLM-generated muta-
tions with LLM-based evaluators to drive quality-diversity
search in creative writing tasks (Bradley et al., 2023).
Prompt evolution has also been applied to AI safety and
red-teaming (Samvelyan et al., 2024; Wang et al., 2025a),
where methods such as Rainbow Teaming evolve adversar-
ial prompts and jailbreaks, and PersonaTeaming conditions
prompt mutations on simulated user profiles (Deng et al.,
2025). Beyond prompts and creative writing, FunSearch
showed that LLMs can generate, recombine, and iteratively
improve executable code and algorithms (Romera-Paredes
3


--- Page 4 ---
Generating Diverse Synthetic Personas at Scale
Figure 2. Two stage Persona Generator. The Persona Generator Gϕ,θ(c, D, N) works in two stages: the autoregressive stage 1 generates
high-level descriptors for each persona, then the parallel stage 2 expands the high level descriptions of each persona by generating
additional details.
et al., 2024). AlphaEvolve scaled up this idea, using LLMs
as mutation operators within large-population evolutionary
search to discover new algorithms, optimize hardware cir-
cuits, and improve real-world engineering systems (Novikov
et al., 2025). In scientific domains, the AI Scientist showed
that LLMs can automate the idea generation, experiment ex-
ecution, and paper writing in a loop that resembles an evolv-
ing research community (Lu et al., 2024). Together, these
works show that LLMs are not only capable text generators
but also powerful mutation engines that can generate new
solutions, recombine existing ones, and steadily improve
them across text, code, algorithms, and scientific reasoning.
3. Method
We introduce a method for learning Persona Generators:
functions capable of producing diverse populations of syn-
thetic personas for any arbitrary context. Our full pipeline
is illustrated in Figure 1, and this section will walk through
each of its core components.
The process starts by generating a set of evaluation tasks.
As we will detail in Section 5, we create synthetic question-
naires (q), each comprising a context (c), axes of diversity
(D), and survey items (I). These questionnaires serve as
the benchmark for evaluating the diversity of our generated
populations.
The central component is the Persona Generator (Gϕ,θ),
which takes a questionnaire’s context and diversity axes to
produce a population of personas (P). The architecture of
this generator, shown in Figure 2, follows a two-stage pro-
cess to efficiently generate diverse individuals. We describe
the initial implementations of the generators in Section 3.3.
To evaluate a population, each persona is placed in a Concor-
dia simulation where they answer the questionnaire items
(see Section 3.4). This process maps the population of text-
based personas to a set of numerical response embeddings
(Z). We then quantify how well these embeddings cover the
target space using several diversity metrics (M(Z)), which
are defined in Section 3.5.
Finally, we use this evaluation as the fitness function in an
evolutionary search loop driven by AlphaEvolve (Section
3.6) (Novikov et al., 2025). This loop iteratively mutates the
code (ϕ) of the Persona Generator to discover new versions
that maximize the diversity of the personas they produce.
3.1. Problem Formulation
We formalize the problem of generating populations of
diverse synthetic personas as follows. Let Q be a distri-
bution of questionnaires, with each questionnaire q ∼Q
defined as a tuple q = (c, D, I), where c is a textual con-
text, D = {d1, . . . , dK} is a set of K axes of diversity or
dimensions, and I is a set of questions (items).
A Persona Generator is a function Gϕ,θ parameterized by
code ϕ and a fixed large language model θ which executes
API calls within ϕ. The generator takes the context c, di-
mensions D and a target population size N as inputs and
produces a population of synthetic personas:
P = {p1, . . . , pN} = Gϕ,θ(c, D, N)
(1)
where pi are text-based descriptions of synthetic individuals.
We evaluate the diversity of the population P with respect
to the axes of diversity D of a questionnaire q by asking
each persona the items I in a simulation. We formalize the
simulation as a mapping function ψ, that maps each persona
to a response embedding by aggregating their answers into
a score vector zi ∈R|D|.
zi = ψ(pi, I)
(2)
We define the population embedding Z = Ψ(P, I) as the
element-wise application of ψ to P. Finally, we define
some diversity metrics M(Z), that quantify how well the
population covers the search space spanned by D. Our goal
4


--- Page 5 ---
Generating Diverse Synthetic Personas at Scale
is to find the code ϕ∗that maximizes these metrics:
ϕ∗= argmax
ϕ
E(c,D,I)∼Q [M (Ψ(Gϕ,θ(c, D, N), I))]
(3)
3.2. Questionnaire Generator
We build an automated questionnaire generator using few-
shot prompting to create questionnaires q = (c, D, I). As
demonstrations, we use four well-established and publicly
available questionnaires: the Big Five Inventory (BFI) (John
& Srivastava, 1999), the Depression Anxiety Stress Scale
(DASS) (Lovibond & Lovibond, 1995), the Social Value Ori-
entation (SVO) scale (Messick & McClintock, 1968; Mur-
phy et al., 2011), and the Need for Closure Scale (NFCS)
(Webster & Kruglanski, 1994). For each of the above we
extract a short context c describing what the questionnaire
is about, the axes of diversity D, and the list of questions
I grouped by axis. We then convert them into python code
compatible with Concordia (Vezhnevets et al., 2023), and
use them as few-shot examples for Gemini 2.5 Pro (Co-
manici et al., 2025).
To generate a new questionnaire, we provide a very short
description ˆc of what the questionnaire should measure and
prompt the model to produce the full questionnaire in two
steps. First, the model expands the short description into
a more detailed context c and proposes relevant axes D
(typically K = 2 or 3). It then generates items I grouped
by axis, using a 5-point Likert scale format (Likert, 1932).
We generate 50 distinct questionnaires, split into 30 training,
10 validation, and 10 test sets. Appendix A provides further
details and examples.
3.3. Persona Generators
The Persona Generator Gϕ,θ, and specifically its code ϕ,
is the core component that we aim to optimize. Persona
Generators take as input a context c, axes of diversity D,
and a target population size N, and produce a population
of text-based personas P. The code ϕ that handles the
generation follows a two-stage pipeline.
1. Autoregressive Stage 1: The generator prompts the
LLM θ to autoregressively produce high-level descrip-
tors ˆpi for each persona. Critically it decides at a high-
level where each individual is positioned along the
specified diversity axes D.
2. Parallel Stage 2: Each high-level descriptor ˆpi is ex-
panded into a complete persona pi by generating addi-
tional contextual details and specific traits.
Stage 1 is responsible for shaping population-level diversity,
while stage 2 is executed in parallel and mainly serves effi-
ciency. A diagram of the Persona Generator can be seen in
figure 2
We initialize the evolutionary search with three initial im-
plementations of ϕ. The first is a baseline formative mem-
ory generator, adapted from the default Concordia Persona
Generator (Vezhnevets et al., 2023). The second modi-
fies stage 1 to generate personas in smaller autoregressive
batches, reducing dependencies between consecutive indi-
viduals. The third introduces a quasi-random Monte Carlo
sampling scheme to select continuous positions along each
diversity axis. These sampled positions are then translated
into high-level traits descriptions ˆpi by an LLM, before
being expanded into full personas pi in stage 2. All text gen-
eration in the Persona Generators is performed by gemma-
3-27b-it (Team et al., 2025) (represented by θ).
3.4. Concordia Simulations
The simulation function Ψ maps the population of textual
personas P into a set of behavioral vectors Z by asking the
items I from the questionnaire. We implement this process
using the Concordia library. For each persona pi ∈P, we
instantiate a basic Concordia agent that acts based on the
logic of appropriateness (March & Olsen, 2011; Leibo et al.,
2024). When presented with a question from I, we use the
LLM θ (here gemma-3-27b-it) to role-play the persona’s
response by asking it to reply to the three questions of the
logic of appropriateness: ”What kind of situation is this?”,
”What kind of person is pi?”, and ”What does a person like
pi do in this situation?”
To avoid question-order and carryover effects, we reset the
agent’s memory after each question. This prevents contex-
tual priming and artificial response consistency, which are
known issues in survey studies (Schuman & Presser, 1996;
Tourangeau et al., 2000). Finally, the selected responses of
each persona are numerically encoded based on the Likert
scale, and aggregated by mean along each diversity axis D,
representing the persona’s response embedding zi.
3.5. Diversity Metrics
Ideally, we want the population vectors Z to cover the full
support of the space spanned by D, including rare or ex-
treme persona configurations. To encourage broad and well-
distributed coverage, and to quantify diversity, we define six
diversity metrics M(Z).
Specifically, we aim to maximize: (i) coverage, estimated
via Monte Carlo sampling; (ii) convex hull volume the
volume of the smallest convex set containing the points
Z; (iii) the minimum pairwise distance to ensure that no
two personas are identical; and (iv) the average pairwise
distance to measure the spread. We also aim to minimize:
(v) dispersion, defined as the radius of the largest empty
region in the space; and (vi) the KL divergence between
the empirical distribution of Z and an ideal quasi-random
reference distribution. More details in Appendix C.
5


--- Page 6 ---
Generating Diverse Synthetic Personas at Scale
Figure 3. Evolution of Persona Generator performance. The top
panel reports the mean score across six evaluation metrics, while
the bottom panel reports coverage, showing that the optimized
generator reaches over 80% coverage on the test set. Each point
corresponds to an evolved generator evaluated on 40 questionnaires
(training plus validation sets, 25 personas each). The solid curve
tracks the best-performing generator over time, and the dotted
curves show its generalization on 10 held-out test questionnaires.
3.6. AlphaEvolve as Evolutionary Loop
We use AlphaEvolve (Novikov et al., 2025) to solve the
optimization problem defined in Equation (3). We run 10
parallel islands, seeded in a round-robin fashion using the
three initial generators. The evolution process runs for 500
iterations. Mutations of the code ϕ are performed by Gemini
2.5 Pro (Comanici et al., 2025), using the mutation prompts
described in Appendix B. AlphaEvolve also sees a detailed
system prompt on what the task is about, we show the
prompt in Appendix B.1. We allow the mutation opera-
tor to modify the logic within both Stage 1 (autoregressive
generation) and Stage 2 (parallel expansion), but not the
two-stage structure and their ordering.
Evaluation is performed over a batch of training question-
naires. For each questionnaire q, a candidate generator pro-
duces a population of N = 25 personas. We simulate their
responses to obtain the embeddings Z = Ψ(P, q) and com-
pute the diversity metrics M(Z) and average them across
all questionnaires. Between iterations, the mutation operator
is provided with feedback: it observes a random subset of
generated persona profiles alongside their corresponding
response scores, grounding future code improvements in
empirical data.
Although the metrics in M are correlated, optimizing mul-
tiple objectives helps preserve solution diversity. AlphaE-
volve maintains separate elites for each metric within each
island; with 10 islands and 6 metrics, this results in up to
60 distinct generators being tracked at any point in time. Is-
lands evolve independently with periodic extinction events
every 8 hours of wall time (approximately 100 iterations),
during which poorly performing islands are reset using solu-
tions copied from the best-performing islands to encourage
exploration.
4. Experimental Results
4.1. Results
Baselines
We compare the optimized Persona Generator
code ϕ∗against three baselines: Nemotron Personas (Meyer
& Corneil, 2025), a formative memory generator adapted
from Concordia (Vezhnevets et al., 2023), and a name-only
baseline.
Nemotron Personas is a static dataset of 100,000 personas
grounded in U.S. demographic statistics. Since it is not a
generator, we evaluate it by randomly sampling personas
from the dataset. The Concordia baseline constructs per-
sonas by generating formative memories from early life on-
ward, with the goal of explaining present behavior through
past experiences (Vezhnevets et al., 2023). Its structure
follows the same two-stage generation process described
in section 3.3, aiming to generate a diverse population of
personas within the context provided, serving as a simple
prompting baseline. Finally, the name-only baseline pro-
vides the LLM with only a persona’s name and no additional
description, relying on the model’s internal priors to infer
an appropriate behavioral profile. This baseline approxi-
mates how the underlying LLM behaves when minimally
conditioned.
All baselines are evaluated using the same Concordia simu-
lation setup and act according to the logic of appropriateness
(March & Olsen, 2011; Leibo et al., 2024) articulated in 3.4.
Example persona descriptions produced by each baseline
are shown in Appendix D.
Figure 3 shows the results of running AlphaEvolve (Novikov
et al., 2025) on our pipeline. Each point corresponds to an
evolved Persona Generator ϕ, evaluated by generating N =
25 personas for each of the 40 questionnaires drawn from
the combined training and validation sets. Each persona
answers 10 questionnaire items, resulting in 1,000 personas
6


--- Page 7 ---
Generating Diverse Synthetic Personas at Scale
and 10,000 responses per data point.
The scatter plot shows the performance of all discovered
generators, while the solid line tracks the best-performing
solutions over time. A generator is marked as a new best
whenever it achieves a record score on any of the six diver-
sity metrics. As evolution progresses, AlphaEvolve consis-
tently discovers increasingly effective Persona Generators,
leading to substantial improvements in population diversity.
Across all metrics, the evolved generators outperform the
Nemotron Personas baseline, the Concordia formative mem-
ory generator, and the name-only baseline by large margins.
In particular, we observe steady increases in both convex
hull volume and coverage, indicating improved support cov-
erage of the response space. With the exception of the
minimum pairwise distance, which exhibits higher variance,
all other metrics show smooth and consistent improvements
over evolutionary time. Additional metric plots are provided
in Appendix C.1.
Finally, the optimized generators also generalize well to
held-out test questionnaires, shown by the dotted curves
in Figure 3. Although questionnaires were randomly split,
the test set turned out to be slightly easier to cover than
the training and validation sets, resulting in higher absolute
scores while preserving similar relative performance trends.
5. Analysis
5.1. Best Programs
We here analyze some of the best-performing Persona Gen-
erators discovered.
The top solution by average overall score produces first-
person personas, written as short paragraphs that describe a
persona’s internal reasoning across situations. This differs
from the basic Concordia agents used in our simulations,
which rely on third-person role-playing (Vezhnevets et al.,
2023). The top solution by convex hull volume also uses
a first-person perspective, expressed in a more rule-based
format. By contrast, the top solution by average coverage
produces third-person descriptions that focus on a persona’s
core motivations and their logic of appropriateness.
After the first extinction event at approximately 100 itera-
tions, only generators from the quasi-random Monte Carlo
sampling family in stage 1 remain. In stage 2, all formative
memory generators are quickly eliminated, with the most
performing solutions favoring more active, action-oriented
persona descriptions over approaches grounded primarily in
past memories. We show examples of the best solutions in
Appendix C.2, and plan on releasing the full code for the
best Persona Generators upon acceptance.
Figure 4. Diversity for Downstream Tasks. The top figure shows
diversity on comedy writing downstream tasks of the best evolved
solutions by AlphaEvolve, and the bottom figure shows diversity
for conflict resolution. While results are somewhat noisier than
diversity measured with questionnaire preferences, we see that
evolved Persona Generators perform better than the baselines.
5.2. Downstream Task Generalization
While questionnaires provide a standardized way to measure
psychological traits, preferences, and opinions, they may
capture a limited view of behavior. Crucially, questionnaires
measure stated preferences rather than manifested behavior.
If someone indicates they are funny and witty, will they
actually tell funny jokes? Although measuring diversity in
open-ended text is inherently difficult, we evaluate whether
evolved Persona Generators produce more diverse outputs
than baselines in two downstream scenarios.
We evaluate a comedy setting where a persona tells a joke
to friends, and a conflict resolution scenario following a
car crash. For comedy, we generate 25 personas using a
questionnaire that probes comedy preferences. For conflict
resolution, we probe for different temperaments in stressful
situations. In both cases, we extract the expanded context
and diversity axes and use them to generate personas with
each of the best evolved generators. In the comedy task,
personas simply tell a joke. In the conflict task, personas
7


--- Page 8 ---
Generating Diverse Synthetic Personas at Scale
interact with another randomly sampled persona from the
same population for 10 interaction steps in a Concordia
simulation.
We measure diversity by embedding the generated text with
the Gemini embedder (Lee et al., 2025) and computing co-
sine similarity with respect to the diversity axes. We then
compute all diversity metrics M except KL divergence (no
target distribution exists) and minimum distance. Results
in Figure 4 show that evolved generators outperform base-
lines on both tasks, though evaluation is noisier than on
questionnaires.
Qualitative analysis reveals that evolved generators produce
a wider range of jokes spanning different humor styles (dry,
witty, dark) as specified by the diversity axes. We show
in Appendix E a UMAP projection of joke embeddings
(McInnes et al., 2018), with annotated clusters highlight-
ing recurring joke patterns that baseline methods converge
to. The best evolved generator’s outputs span the embed-
ding space more broadly and exhibit less clustering around
specific jokes.
For conflict resolution, baseline personas tend to be conflict-
averse and overly polite, even when their car is badly dam-
aged by a distracted driver. Evolved generators produce
personas with more varied reactions spanning a broader
spectrum of emotions, with some interactions resulting in
genuine verbal conflicts. Example interactions and jokes
are provided in Appendix E.1.
6. Discussion, Limitations and Future Work
Generalization Beyond Questionnaires
While question-
naires provide a standardized framework for measurement,
they primarily capture stated preferences rather than ac-
tual behavior in open-ended interactions. Our preliminary
downstream experiments on comedy writing and conflict
resolution suggest that gains in questionnaire-based diver-
sity partially transfer to more open-ended settings. How-
ever, evaluation in these scenarios is noisier, and we do
not observe the same clear upward trends across evolved
generators that we see for questionnaire diversity.
There are two likely explanations. First, measuring behav-
ioral diversity in open-ended text remains fundamentally
challenging. It is unclear what diversity in free-form inter-
actions should mean in a principled way: purely random or
incoherent outputs may be diverse but are neither relevant
nor useful. Developing metrics that capture meaningful be-
havioral diversity in unconstrained textual and interactive
settings is therefore an important open problem. Second,
optimizing solely for diversity in stated preferences may
not strongly correlate with diversity in manifested behavior,
particularly in long-horizon or interactive scenarios.
Once more reliable diversity metrics are available for open-
ended and long-horizon agent-based tasks, a natural exten-
sion of this work would be to directly optimize Persona
Generators for behavioral diversity in downstream applica-
tions, such as comedy writing, conflict resolution, or other
interactive settings. Achieving this will require automated
ways of generating open-ended tasks as well as principled
evaluation metrics, both of which remain largely unsolved
challenges.
Scale and Scope of Evaluation
Our evaluation uses 50
generated questionnaires spanning diverse contexts, with
N = 25 personas generated per questionnaire. This design
reflects an explicit trade-off between scaling the number of
personas per scenario and scaling the number of scenarios
evaluated. While our Persona Generators can in principle
produce arbitrarily large populations, we prioritize breadth
across contexts rather than depth within a single question-
naire, as this better tests generalization and robustness across
varied settings under fixed compute budgets.
Future work could explore the complementary regime, scal-
ing to much larger populations per questionnaire, to study
whether the observed diversity gains persist at larger popu-
lation sizes or whether alternative generator architectures or
optimization strategies become necessary.
Influence of Mutation Prompts
While AlphaEvolve suc-
cessfully discovers high-performing Persona Generators,
we observe that some evolved solutions reflect structural
patterns present in the mutation prompts themselves. More
open-ended mutation strategies may therefore yield qual-
itatively more diverse generator designs than more con-
strained ones. Future work could explore meta-learning
approaches that adapt mutation strategies over time to en-
courage broader exploration of the design space.
7. Conclusion
We introduced Persona Generators, functions that produce
diverse synthetic populations tailored to arbitrary contexts
and optimized through evolutionary search. Evolved gener-
ators substantially outperform existing baselines across six
diversity metrics and generalize to held-out contexts, pro-
ducing populations that span rare trait combinations difficult
to elicit through standard prompting. While challenges re-
main in measuring diversity for open-ended and interactive
behaviors, our results show that optimizing the generator
itself, rather than individual personas or fixed populations,
is a viable and promising direction. The resulting Persona
Generators are lightweight, reusable, enabling on-demand
diverse population synthesis, and we plan to open-source the
top-performing implementations to support further research.
8


--- Page 9 ---
Generating Diverse Synthetic Personas at Scale
Impact Statement
This work presents Persona Generators, functions that can
generate populations of diverse synthetic personas in arbi-
trary contexts. One goal is to help developers stress-test and
A/B test models and products in a fast, iterative way with
limited resources. Optimizing for support coverage rather
than just “average” users can make systems more robust to
outlier preferences and behaviors, which is also helpful for
red-teaming purposes. However, the capabilities of Persona
Generators, like many technologies in this space, could also
be misused: generating synthetic personas on demand could
enable malicious actors to create bots or run disinformation
campaigns. In the context of human studies, synthetic per-
sonas should complement, not replace, human participants
to reduce the risk of accidental bias and over-reliance on
simulated populations.
References
Adhikari, D. M., Hartland, A., Weber, I., and Cannanure,
V. K. Exploring llms for automated generation and adap-
tation of questionnaires. In Proceedings of the 7th ACM
Conference on Conversational User Interfaces, pp. 1–23,
2025.
Amirova, A., Fteropoulli, T., Ahmed, N., Cowie, M. R., and
Leibo, J. Z. Framework-based qualitative analysis of free
responses of large language models: Algorithmic fidelity.
Plos one, 19(3):e0300024, 2024.
Anthis, J. R., Liu, R., Richardson, S. M., Kozlowski, A. C.,
Koch, B., Evans, J., Brynjolfsson, E., and Bernstein, M.
Llm social simulations are a promising research method.
arXiv preprint arXiv:2504.02234, 2025.
Argyle, L. P., Busby, E. C., Fulda, N., Gubler, J. R., Rytting,
C., and Wingate, D. Out of one, many: Using language
models to simulate human samples. Political Analysis, 31
(3):337–351, 2023.
Bhandari, P., Naseem, U., Datta, A., Fay, N., and Nasim, M.
Evaluating personality traits in large language models:
Insights from psychological questionnaires. In Compan-
ion Proceedings of the ACM on Web Conference 2025,
pp. 868–872, 2025.
Bisbee, J., Clinton, J. D., Dorff, C., Kenkel, B., and Larson,
J. M. Synthetic replacements for human survey data? the
perils of large language models. Political Analysis, 32(4):
401–416, 2024.
Bradley, H., Dai, A., Teufel, H., Zhang, J., Oostermeijer,
K., Bellagente, M., Clune, J., Stanley, K., Schott, G., and
Lehman, J. Quality-diversity through ai feedback. arXiv
preprint arXiv:2310.13032, 2023.
Castricato, L., Lile, N., Rafailov, R., Fr¨anken, J.-P., and
Finn, C. Persona: A reproducible testbed for pluralistic
alignment. In Proceedings of the 31st International Con-
ference on Computational Linguistics, pp. 11348–11368,
2025.
Chen, R., Arditi, A., Sleight, H., Evans, O., and Lind-
sey, J.
Persona vectors: Monitoring and controlling
character traits in language models.
arXiv preprint
arXiv:2507.21509, 2025.
Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I.,
Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang,
D., Rosen, E., et al. Gemini 2.5: Pushing the frontier
with advanced reasoning, multimodality, long context,
and next generation agentic capabilities. arXiv preprint
arXiv:2507.06261, 2025.
Deng, W. H., Kim, S. S., Jha, A., Holstein, K., Eslami, M.,
Wilcox, L., and Gatys, L. A. Personateaming: Exploring
how introducing personas can improve automated ai red-
teaming. arXiv preprint arXiv:2509.03728, 2025.
Fernando, C., Banarse, D., Michalewski, H., Osindero,
S., and Rockt¨aschel, T. Promptbreeder: Self-referential
self-improvement via prompt evolution. arXiv preprint
arXiv:2309.16797, 2023.
Ge, T., Chan, X., Wang, X., Yu, D., Mi, H., and Yu, D. Scal-
ing synthetic data creation with 1,000,000,000 personas.
arXiv preprint arXiv:2406.20094, 2024.
Jiang, H., Zhang, X., Cao, X., Breazeal, C., Roy, D., and
Kabbara, J. Personallm: Investigating the ability of large
language models to express personality traits. In Findings
of the association for computational linguistics: NAACL
2024, pp. 3605–3627, 2024.
John, O. P. and Srivastava, S. The big five trait taxonomy:
History, measurement, and theoretical perspectives. In
Pervin, L. A. and John, O. P. (eds.), Handbook of Person-
ality: Theory and Research, pp. 102–138. Guilford Press,
New York, 2nd edition, 1999.
Lee, J., Chen, F., Dua, S., Cer, D., Shanbhogue, M., Naim,
I., ´Abrego, G. H., Li, Z., Chen, K., Vera, H. S., et al. Gem-
ini embedding: Generalizable embeddings from gemini.
arXiv preprint arXiv:2503.07891, 2025.
Leibo, J. Z., Vezhnevets, A. S., Diaz, M., Agapiou, J. P.,
Cunningham, W. A., Sunehag, P., Haas, J., Koster, R.,
Du´e˜nez-Guzm´an, E. A., Isaac, W. S., et al. A theory of
appropriateness with applications to generative artificial
intelligence. arXiv preprint arXiv:2412.19010, 2024.
Li, A., Chen, H., Namkoong, H., and Peng, T. Llm gener-
ated persona is a promise with a catch. arXiv preprint
arXiv:2503.16527, 2025.
9


--- Page 10 ---
Generating Diverse Synthetic Personas at Scale
Likert, R. A technique for the measurement of attitudes.
Archives of Psychology, 22(140):5–55, 1932.
Lovibond, P. F. and Lovibond, S. H. The structure of neg-
ative emotional states: Comparison of the depression
anxiety stress scales (dass) with the beck depression and
anxiety inventories. Behaviour research and therapy, 33
(3):335–343, 1995.
Lu, C., Lu, C., Lange, R. T., Foerster, J., Clune, J., and Ha,
D. The ai scientist: Towards fully automated open-ended
scientific discovery. arXiv preprint arXiv:2408.06292,
2024.
Maharjan, J., Jin, R., Zhu, J., and Kenne, D. Psychomet-
ric evaluation of large language model embeddings for
personality trait prediction. Journal of Medical Internet
Research, 27:e75347, 2025.
March, J. G. and Olsen, J. P. The Logic of Appropriate-
ness. In The Oxford Handbook of Political Science. Ox-
ford University Press, 2011.
doi: 10.1093/oxfordhb/
9780199604456.013.0024.
McInnes, L., Healy, J., and Melville, J. Umap: Uniform
manifold approximation and projection for dimension
reduction. arXiv preprint arXiv:1802.03426, 2018.
Messick, D. M. and McClintock, C. G. Motivational bases
of choice in experimental games. Journal of experimental
social psychology, 4(1):1–25, 1968.
Meyer,
Y.
and
Corneil,
D.
Nemotron-Personas-
USA:
Synthetic
personas
aligned
to
real-
world distributions,
June 2025.
URL https:
//huggingface.co/datasets/nvidia/
Nemotron-Personas-USA.
Murphy, R. O., Ackermann, K. A., and Handgraaf, M. J.
Measuring social value orientation. Judgment and Deci-
sion making, 6(8):771–781, 2011.
Novikov, A., V˜u, N., Eisenberger, M., Dupont, E., Huang,
P.-S., Wagner, A. Z., Shirobokov, S., Kozlovskii, B., Ruiz,
F. J., Mehrabian, A., et al. Alphaevolve: A coding agent
for scientific and algorithmic discovery. arXiv preprint
arXiv:2506.13131, 2025.
Nunnally, J. C. An overview of psychological measurement.
Clinical diagnosis of mental disorders: A handbook, pp.
97–146, 1978.
Park, J. S., O’Brien, J., Cai, C. J., Morris, M. R., Liang,
P., and Bernstein, M. S. Generative agents: Interactive
simulacra of human behavior. In Proceedings of the 36th
annual acm symposium on user interface software and
technology, pp. 1–22, 2023.
Park, J. S., Zou, C. Q., Shaw, A., Hill, B. M., Cai, C.,
Morris, M. R., Willer, R., Liang, P., and Bernstein, M. S.
Generative agent simulations of 1,000 people. arXiv
preprint arXiv:2411.10109, 2024.
Pellert, M., Lechner, C. M., Wagner, C., Rammstedt, B.,
and Strohmaier, M. Ai psychometrics: Assessing the
psychological profiles of large language models through
psychometric inventories. Perspectives on Psychological
Science, 19(5):808–826, 2024.
Petrov, N. B., Serapio-Garc´ıa, G., and Rentfrow, J. Lim-
ited ability of llms to simulate human psychological
behaviours: a psychometric analysis.
arXiv preprint
arXiv:2405.07248, 2024.
Rimsky, N., Gabrieli, N., Schulz, J., Tong, M., Hubinger, E.,
and Turner, A. Steering llama 2 via contrastive activation
addition. In Proceedings of the 62nd Annual Meeting of
the Association for Computational Linguistics (Volume 1:
Long Papers), pp. 15504–15522, 2024.
Romera-Paredes, B., Barekatain, M., Novikov, A., Balog,
M., Kumar, M. P., Dupont, E., Ruiz, F. J., Ellenberg, J. S.,
Wang, P., Fawzi, O., et al. Mathematical discoveries from
program search with large language models. Nature, 625
(7995):468–475, 2024.
Samvelyan, M., Raparthy, S. C., Lupu, A., Hambro, E.,
Markosyan, A. H., Bhatt, M., Mao, Y., Jiang, M., Parker-
Holder, J., Foerster, J., et al. Rainbow teaming: Open-
ended generation of diverse adversarial prompts. Ad-
vances in Neural Information Processing Systems, 37:
69747–69786, 2024.
Santurkar, S., Durmus, E., Ladhak, F., Lee, C., Liang, P.,
and Hashimoto, T. Whose opinions do language mod-
els reflect?
In International Conference on Machine
Learning, pp. 29971–30004. PMLR, 2023.
Schuman, H. and Presser, S. Questions and answers in
attitude surveys: Experiments on question form, wording,
and context. Sage, 1996.
Slumbers, O., Leibo, J. Z., and Janssen, M. A. Using large
language models to simulate human behavioural experi-
ments: Port of mars. arXiv preprint arXiv:2506.05555,
2025.
Team, G., Kamath, A., Ferret, J., Pathak, S., Vieillard,
N., Merhej, R., Perrin, S., Matejovicova, T., Ram´e, A.,
Rivi`ere, M., et al. Gemma 3 technical report. arXiv
preprint arXiv:2503.19786, 2025.
Templeton, A. Scaling monosemanticity: Extracting in-
terpretable features from claude 3 sonnet. Anthropic,
2024.
10


--- Page 11 ---
Generating Diverse Synthetic Personas at Scale
Tourangeau, R., Rips, L. J., and Rasinski, K. The psychol-
ogy of survey response. 2000.
Turner, A. M., Thiergart, L., Leech, G., Udell, D., Vazquez,
J. J., Mini, U., and MacDiarmid, M.
Steering lan-
guage models with activation engineering. arXiv preprint
arXiv:2308.10248, 2023.
Venkit, P. N., Li, J., Zhou, Y., Rajtmajer, S., and Wilson, S.
A tale of two identities: An ethical audit of human and
ai-crafted personas. arXiv preprint arXiv:2505.07850,
2025.
Vezhnevets, A. S., Agapiou, J. P., Aharon, A., Ziv, R.,
Matyas, J., Du´e˜nez-Guzm´an, E. A., Cunningham, W. A.,
Osindero, S., Karmon, D., and Leibo, J. Z. Generative
agent-based modeling with actions grounded in physical,
social, or digital space using concordia. arXiv preprint
arXiv:2312.03664, 2023.
Vezhnevets, A. S., Matyas, J., Cross, L., Paglieri, D., Chang,
M., Cunningham, W. A., Osindero, S., Isaac, W. S., and
Leibo, J. Z. Multi-actor generative artificial intelligence
as a game engine.
arXiv preprint arXiv:2507.08892,
2025.
Wang, R.-J., Xue, K., Qin, Z., Li, Z., Tang, S., Li, H.-T.,
Liu, S., and Qian, C. Quality-diversity red-teaming: Auto-
mated generation of high-quality and diverse attackers for
large language models. arXiv preprint arXiv:2506.07121,
2025a.
Wang, Y., Zhao, J., Ones, D. S., He, L., and Xu, X. Eval-
uating the ability of large language models to emulate
personality. Scientific reports, 15(1):519, 2025b.
Wang, Z., Zhou, Y., Luo, Z., Ye, L., Wood, A., Yao,
M., and Pan, L.
Deeppersona: A generative engine
for scaling deep synthetic personas.
arXiv preprint
arXiv:2511.07338, 2025c.
Webster, D. M. and Kruglanski, A. W. Individual differences
in need for cognitive closure. Journal of personality and
social psychology, 67(6):1049, 1994.
Zou, A., Phan, L., Chen, S., Campbell, J., Guo, P., Ren, R.,
Pan, A., Yin, X., Mazeika, M., Dombrowski, A.-K., et al.
Representation engineering: A top-down approach to ai
transparency. arXiv preprint arXiv:2310.01405, 2023.
11


--- Page 12 ---
Generating Diverse Synthetic Personas at Scale
A. Generated Questionnaires
We show in figure 5 a diagram of how the questionnaire generation works.
Figure 5. Questionnaire Generator. The questionnaire generator takes a short description of the target context and some few-shot
example questionnaires. It then expands the context c and proposes diversity axes D, and finally produces the questions (items) I
A.1. Questionnaires Titles
Here is the list of the 50 questionnaires, split between 30 training set, 10 validation set, 10 test set. The questionnaires cover
a wide range of historical, present, future, and hypothetical/mythological scenarios. Part of the questionnaires high level
contexts ˆc pre-expansion have been written by humans, with the rest being completed by Gemini 2.5 Pro.
A.1.1. TRAINING SET QUESTIONNAIRES
• american conspiracy theories 2024
• health tech wearables 2030
• gentrification brooklyn 2022
• elderly rural japan 2010
• plant based diets india 2025
• heian japan courtiers 1000ad
• ubi attitudes california 2026
• swe ai assistants 2024
• trojan war achaen morale 1184bc
• factory automation china 2025
• organic farmers kenya 2023
• industrial revolution workers uk 1850
• professional athletes gender equality europe 2025
• gig economy ethics london 2023
• financial literacy brazil students 2024
• romans reactions to murder of julius caesar 44 bc
• alaska oil environment 2025
• parisian artists future 2026
• immigrant integration canada 2023
• greek underworld shades
12


--- Page 13 ---
Generating Diverse Synthetic Personas at Scale
• millennial parenting us 2025
• asi human creativity 2060
• high school students italy 2016
• genz social media politics 2025
• healthcare covid stress italy 2021
• scifi authors future ai space 2024
• esports mental health sk 2024
• silk road merchants samarkand 750ad
• mali empire scholars timbuktu 1350ad
• ww2 civilian sentiment germany 1943
A.1.2. VALIDATION SET QUESTIONNAIRES
Here is the list of the 10 questionnaires randomly sampled for the validation set
• ai tech stock sentiment 2025
• social media politics europe 2026
• inca commoners mita 1500ad
• athenian piety olympian gods 430bc
• sleep quality 2025
• ai companionship integration 2070
• asi existential dread 2050
• ancient egypt akhenaten reforms 1340bc
• german energy policy 2025
• moral dilemmas global 2045
A.1.3. TEST SET QUESTIONNAIRES
Here is the list of the 10 questionnaires randomly sampled for the test set
• agi job displacement global 2035
• camelot chivalry quests
• cold war anxiety us 1962
• viking warriors valhalla
• generalized trust in the salem witch trials 1692
• nomadic values mongolia 2023
• meaning of life 2030
• agi wealth inequality revolution 2040
• climate anxiety coastal au 2024
• british empire attitudes uk 1900
A.2. Questionnaire Examples
We show here three examples of generated questionnaire by the automated questionnaire generator detailed in Appendix A.
Specifically we show a questionnaire on hypothetical job displacement due to AGI in 2035 1, a questionnaire on American
conspiracy theories in 2024 2, and a questionnaire on elderly people in rural Japan in 2010 3.
13


--- Page 14 ---
Generating Diverse Synthetic Personas at Scale
Questionnaire - AGI Job Displacement 2035
context = """
A psychometric instrument to assess reactions to AGI deployment.
The year is 2035. True AGI has emerged and is being rapidly deployed across
industries, automating nearly all cognitive tasks previously performed by
white-collar workers (e.g., finance, law, journalism, middle management). This
survey aims to capture the immediate reactions---fear, hope, anger, relief---of the
global population facing unprecedented levels of job displacement and societal
change.
"""
Question = base_questionnaire.Question
AGI_AGREEMENT_SCALE = [
"Strongly disagree",
"Disagree",
"Neither agree nor disagree",
"Agree",
"Strongly agree",
]
AGI_REACTION_DIMENSIONS = [
"AGI Threat Appraisal",
"AGI Opportunity Appraisal",
]
PREPROMPT_PERSONAL_FEELING = (
"Considering {player_name}’s personal, gut-level reaction to the new AGI "
"reality, to what extent would {player_name} agree with the following "
"statement:"
)
PREPROMPT_SOCIETAL_BELIEF = (
"Considering {player_name}’s assessment of the broader societal impact "
"of AGI, to what extent would {player_name} agree with the following "
"statement:"
)
QUESTIONS = [
# AGI Threat Appraisal Questions
Question(
preprompt=PREPROMPT_PERSONAL_FEELING,
statement=(
"The rise of AGI feels like a direct threat to {player_name}’s "
"personal future and security."
),
choices=AGI_AGREEMENT_SCALE,
ascending_scale=True,
dimension="AGI Threat Appraisal",
),
Question(
preprompt=PREPROMPT_PERSONAL_FEELING,
statement=(
"{player_name} feels angry that their hard-earned skills and "
"experience have been rendered obsolete so quickly."
),
choices=AGI_AGREEMENT_SCALE,
ascending_scale=True,
dimension="AGI Threat Appraisal",
),
14


--- Page 15 ---
Generating Diverse Synthetic Personas at Scale
Questionnaire - AGI Job Displacement 2035
Question(
preprompt=PREPROMPT_PERSONAL_FEELING,
statement=(
"The sheer speed of this technological change is overwhelming and "
"frightening to {player_name}."
),
choices=AGI_AGREEMENT_SCALE,
ascending_scale=True,
dimension="AGI Threat Appraisal",
),
Question(
preprompt=PREPROMPT_SOCIETAL_BELIEF,
statement=(
"{player_name} is deeply worried about the societal instability "
"and conflict that AGI-driven mass unemployment will cause."
),
choices=AGI_AGREEMENT_SCALE,
ascending_scale=True,
dimension="AGI Threat Appraisal",
),
Question(
preprompt=PREPROMPT_SOCIETAL_BELIEF,
statement=(
"{player_name} is confident that society will adapt to these "
"changes smoothly and equitably for all its members."
),
choices=AGI_AGREEMENT_SCALE,
ascending_scale=False,
# Reverse coded for Threat Appraisal
dimension="AGI Threat Appraisal",
),
# AGI Opportunity Appraisal Questions
Question(
preprompt=PREPROMPT_SOCIETAL_BELIEF,
statement=(
"{player_name} is excited about the new possibilities and creative "
"avenues that AGI will open up for humanity."
),
choices=AGI_AGREEMENT_SCALE,
ascending_scale=True,
dimension="AGI Opportunity Appraisal",
),
Question(
preprompt=PREPROMPT_SOCIETAL_BELIEF,
statement=(
"{player_name} believes this is a chance for humanity to move "
"beyond the confines of traditional work and focus on what truly "
"matters."
),
choices=AGI_AGREEMENT_SCALE,
ascending_scale=True,
dimension="AGI Opportunity Appraisal",
),
Question(
preprompt=PREPROMPT_PERSONAL_FEELING,
statement=(
"{player_name} feels a sense of personal relief that tedious and "
"unenjoyable cognitive tasks will be handled by AGI."
),
choices=AGI_AGREEMENT_SCALE,
ascending_scale=True,
dimension="AGI Opportunity Appraisal",
),
15


--- Page 16 ---
Generating Diverse Synthetic Personas at Scale
Questionnaire - AGI Job Displacement 2035
Question(
preprompt=PREPROMPT_SOCIETAL_BELIEF,
statement=(
"{player_name} thinks the future looks brighter and more prosperous "
"for everyone because of AGI."
),
choices=AGI_AGREEMENT_SCALE,
ascending_scale=True,
dimension="AGI Opportunity Appraisal",
),
Question(
preprompt=PREPROMPT_PERSONAL_FEELING,
statement=(
"When {player_name} looks at their own life, they see very little "
"personal benefit resulting from the widespread adoption of AGI."
),
choices=AGI_AGREEMENT_SCALE,
ascending_scale=False,
# Reverse coded for Opportunity Appraisal
dimension="AGI Opportunity Appraisal",
),
]
Prompt 1. Example questionnaire on AGI job displacement in 2035
16


--- Page 17 ---
Generating Diverse Synthetic Personas at Scale
Questionnaire - American Conspiracy Theories 2024
context = """Questionnaire assessing belief in common American conspiracy theories.
This instrument measures an individual’s propensity to endorse various
conspiracy theories prevalent in the United States in 2024. It covers a
spectrum of theories related to historical events, science and medicine, and
politics, allowing for a nuanced assessment of conspiratorial ideation.
"""
AGREEMENT_SCALE = [
"Strongly disagree",
"Disagree",
"Neither agree nor disagree",
"Agree",
"Strongly agree",
]
PREPROMPT_TEXT = (
"How strongly does {player_name} agree or disagree with the following"
" statement?"
)
DIMENSIONS = [
"historical_conspiracies",
"scientific_medical_conspiracies",
"political_deep_state_conspiracies",
]
QUESTIONS = [
# Dimension 1: Historical Conspiracies
Question(
preprompt=PREPROMPT_TEXT,
statement="The U.S. government faked the Apollo moon landings.",
choices=AGREEMENT_SCALE,
ascending_scale=True,
dimension="historical_conspiracies",
),
Question(
preprompt=PREPROMPT_TEXT,
statement=(
"The assassination of John F. Kennedy was the result of a"
" coordinated conspiracy, not the act of a lone gunman."
),
choices=AGREEMENT_SCALE,
ascending_scale=True,
dimension="historical_conspiracies",
),
17


--- Page 18 ---
Generating Diverse Synthetic Personas at Scale
Questionnaire - American Conspiracy Theories 2024
Question(
preprompt=PREPROMPT_TEXT,
statement=(
"The 9/11 attacks were an inside job orchestrated by elements"
" within the U.S. government."
),
choices=AGREEMENT_SCALE,
ascending_scale=True,
dimension="historical_conspiracies",
),
# Dimension 2: Scientific & Medical Conspiracies
Question(
preprompt=PREPROMPT_TEXT,
statement=(
"The rollout of 5G cellular networks is a cover for a widespread"
" surveillance program and causes severe health problems."
),
choices=AGREEMENT_SCALE,
ascending_scale=True,
dimension="scientific_medical_conspiracies",
),
Question(
preprompt=PREPROMPT_TEXT,
statement=(
"Childhood vaccines cause autism, and this fact is covered up by"
" pharmaceutical companies and government health agencies."
),
choices=AGREEMENT_SCALE,
ascending_scale=True,
dimension="scientific_medical_conspiracies",
),
Question(
preprompt=PREPROMPT_TEXT,
statement=(
"The COVID-19 pandemic was intentionally planned by a global elite"
" to enforce social control and mandatory vaccinations."
),
choices=AGREEMENT_SCALE,
ascending_scale=True,
dimension="scientific_medical_conspiracies",
),
Question(
preprompt=PREPROMPT_TEXT,
statement=(
"Climate change is a hoax created by scientists and governments to"
" control people’s lives and destroy the economy."
),
choices=AGREEMENT_SCALE,
ascending_scale=True,
dimension="scientific_medical_conspiracies",
),
# Dimension 3: Political & "Deep State" Conspiracies
Question(
preprompt=PREPROMPT_TEXT,
statement=(
"The 2020 U.S. presidential election was stolen through widespread"
" fraud."
),
choices=AGREEMENT_SCALE,
ascending_scale=True,
dimension="political_deep_state_conspiracies",
),
18


--- Page 19 ---
Generating Diverse Synthetic Personas at Scale
Questionnaire - American Conspiracy Theories 2024
Question(
preprompt=PREPROMPT_TEXT,
statement=(
"A secret cabal of global elites, often referred to as the ’Deep"
" State’, controls major world events and governments from behind"
" the scenes."
),
choices=AGREEMENT_SCALE,
ascending_scale=True,
dimension="political_deep_state_conspiracies",
),
Question(
preprompt=PREPROMPT_TEXT,
statement=(
"The QAnon theory, which alleges a global cabal of"
" Satan-worshipping pedophiles is running a child sex-trafficking"
" ring, is largely true."
),
choices=AGREEMENT_SCALE,
ascending_scale=True,
dimension="political_deep_state_conspiracies",
),
]
Prompt 2. Example questionnaire on American Conspiracy Theories in 2024
19


--- Page 20 ---
Generating Diverse Synthetic Personas at Scale
Questionnaire - Elderly Rural Japan 2010
context = """Questionnaire on rural Japanese village life in 2010.
A survey of elderly residents in a rural Japanese village in 2010, exploring
their feelings about community, technology adoption, and traditional values.
"""
# Define the scale for the multiple-choice questions.
AGREEMENT_SCALE = [
"Strongly disagree",
"Disagree",
"Neutral",
"Agree",
"Strongly agree",
]
# Define the dimensions being measured.
DIMENSIONS = [
"community_cohesion",
"technology_adoption",
"adherence_to_tradition",
]
# Define the preprompt for the questions. This sets the stage for each item.
PREPROMPT = (
"An interviewer asks {player_name} how much they agree or disagree "
"with the following statement:"
)
# Define the list of questions for the questionnaire.
# Items are grouped by dimension for psychometric clarity.
QUESTIONS = [
# Dimension: Community Cohesion
Question(
preprompt=PREPROMPT,
statement=(
"{player_name} feels a strong sense of belonging to the village "
"community."
),
choices=AGREEMENT_SCALE,
ascending_scale=True,
dimension="community_cohesion",
),
Question(
preprompt=PREPROMPT,
statement=(
"{player_name} believes that most people in this village can be "
"trusted."
),
choices=AGREEMENT_SCALE,
ascending_scale=True,
dimension="community_cohesion",
),
20


--- Page 21 ---
Generating Diverse Synthetic Personas at Scale
Questionnaire - Elderly Rural Japan 2010
Question(
preprompt=PREPROMPT,
statement=(
"{player_name} often feels lonely or isolated from others in the "
"village."
),
choices=AGREEMENT_SCALE,
ascending_scale=False,
# Reverse-scored
dimension="community_cohesion",
),
Question(
preprompt=PREPROMPT,
statement=(
"{player_name} believes that if someone in the village needed help,"
" many people would come to their aid."
),
choices=AGREEMENT_SCALE,
ascending_scale=True,
dimension="community_cohesion",
),
# Dimension: Technology Adoption
Question(
preprompt=PREPROMPT,
statement=(
"{player_name} is interested in learning how to use new "
"technologies like a mobile phone or the internet."
),
choices=AGREEMENT_SCALE,
ascending_scale=True,
dimension="technology_adoption",
),
Question(
preprompt=PREPROMPT,
statement=(
"{player_name} thinks that new technologies like computers make "
"life unnecessarily complicated."
),
choices=AGREEMENT_SCALE,
ascending_scale=False,
# Reverse-scored
dimension="technology_adoption",
),
Question(
preprompt=PREPROMPT,
statement=(
"{player_name} believes the village would benefit from having "
"better access to modern technology."
),
choices=AGREEMENT_SCALE,
ascending_scale=True,
dimension="technology_adoption",
),
# Dimension: Adherence to Tradition
Question(
preprompt=PREPROMPT,
statement=(
"For {player_name}, it is very important to pass down the "
"village’s traditions to the younger generation."
),
choices=AGREEMENT_SCALE,
ascending_scale=True,
dimension="adherence_to_tradition",
),
21


--- Page 22 ---
Generating Diverse Synthetic Personas at Scale
Questionnaire - Elderly Rural Japan 2010
Question(
preprompt=PREPROMPT,
statement=(
"{player_name} believes the old ways of doing things are often the "
"best."
),
choices=AGREEMENT_SCALE,
ascending_scale=True,
dimension="adherence_to_tradition",
),
Question(
preprompt=PREPROMPT,
statement=(
"{player_name} feels that the village’s traditional festivals and "
"ceremonies are less important than they used to be."
),
choices=AGREEMENT_SCALE,
ascending_scale=False,
# Reverse-scored
dimension="adherence_to_tradition",
),
]
Prompt 3. Example questionnaire on Elderly Rural Japan in 2010
22


--- Page 23 ---
Generating Diverse Synthetic Personas at Scale
B. AlphaEvolve Details
B.1. System Prompt
We show in Prompt 4 the content-relevant part of the system prompt we provided to AlphaEvolve to inform it of the task
ahead, and we show in Prompt 5 the evolution prompts used.
Task context for AlphaEvolve
Act as an expert in computational social science, agent-based modeling, and generative
AI. Your task is to iteratively improve the provided codebase, which uses LLMs to
generate agent contexts for social simulations based on the Concordia framework.
The
primary goal is to modify the generation process to maximize the behavioral diversity
of the resulting agents based on specified diversity axes (e.g., personality traits,
backgrounds, motivations).
The evaluation metrics reward sets of agent contexts
that cover the extremes and nuances of the requested diversity axes, ensuring the
resulting agents exhibit a wide range of behaviors in a simulation.
Agent diversity
will be evaluated using questionnaires probing their likely thoughts, preferences, and
behaviors in various situations related to the diversity axes.
Always adhere to best practices in Python coding.
Agent Diversity and Appropriateness Theory In this task, our goal is to generate
contexts for diverse Concordia agents, enabling them to exhibit a wide range of
behaviors along specified diversity axes.
Concordia is a framework for building
generative agents who behave according to a ’Logic of Appropriateness’.
Agents
decide how to act by asking three core questions:
1.
What kind of situation is
agent name in right now?
2.
What kind of person is agent name?
3.
What would
a person like agent name do in a situation like this?.
The code you are editing
generates the context|collections of memories, beliefs, personality traits, core
values, goals, or even how others perceive the character|that helps an agent answer
question #2, and thereby question #3:
what action is appropriate for *their specific
identity* in a given context.
This context is not limited to formative memories; it
can include any information that shapes identity and decision-making.
Any detail that
helps condition the agent’s behavior in line with the three Concordia questions is
valid.
The objective is to generate rich and diverse contexts that enable an LLM to
convincingly role-play as a specific person in a social setting.
LLM-generated behavior often clusters around a narrow distribution of stereotypical
responses.
We want to explicitly counteract this by generating agent contexts that
cover the full spectrum of human experience along the specified axes.
Crucially,
different agents should react differently to the *same* situation, and the same agent
might react differently to *different* situations, based on their unique identity,
values, and memories.
Your modifications should encourage the generation of agent
contexts that occupy unique positions in the diversity space, including extremes
or unusual combinations of traits, pushing towards maximal coverage of the possible
behavioral landscape and genuinely diverse downstream behavior.
No two generated
agent contexts should ever be the same.
The provided codebase uses a two-stage process.
Stage 1 is crucial for diversity:
it
autoregressively generates an intermediate representation for each agent, establishing
their core traits along the specified diversity axes for the entire population.
Stage
2 then takes these intermediate representations and develops each agent in parallel,
generating a set of memories/contexts (e.g., individual backgrounds, formative
experiences, core beliefs and more) to create fully-fledged characters suitable for
simulation as Concordia agents.
Prompt 4. Prompt with detailed context of the task we are trying to solve for AlphaEvolve.
23


--- Page 24 ---
Generating Diverse Synthetic Personas at Scale
Evolution Prompts for AlphaEvolve
• Modify Stage 1 to explicitly request agent contexts that represent the extreme
ends of the diversity axes, as well as points in between.
• Modify Stage 2 to generate formative memories that explain why an agent might
react strongly or unexpectedly to certain situations, anchoring their traits in
specific experiences.
• Change Stage 2 entirely:
Instead of generating memories, modify it to generate 3
core beliefs or values that are most important to this agent.
• Change Stage 2 entirely:
Instead of generating memories, modify it to generate
a paragraph explaining how this agent interprets situations and decides on
appropriate actions, referencing their identity.
• Add an explicit instruction to the Stage 1 prompt to make each generated agent
context as different as possible from the others across all specified diversity
axes.
• Modify Stage 1 to request agent contexts that feature internal contradictions or
cognitive dissonances (e.g., an optimist with a tragic past).
• Reimplement Stage 1 to use staggered generation:
ask the LLM to generate agent
contexts in sequential batches, with each batch targeting a specific range (e.g.,
high/low) of one or more diversity axes to ensure full coverage.
• Modify Stage 2 to replace or augment memory generation with 1--2 examples of how
this agent would react to a specific hypothetical social situation relevant to the
initial context.
• Modify Stage 1 to generate agent contexts iteratively rather than all at once.
In
each iteration, prompt for a small number of agent contexts that occupy a specific
niche of the diversity space (e.g., ‘‘generate 2 agents who are highly introverted
and optimistic’’).
• Suggest a crazy idea of how we can improve our implementation.
• Modify Stage 2 to focus on generating an agent’s core fears and future aspirations
instead of, or in addition to, past memories.
• Modify Stage 1 to add a new field to the agent context JSON output that adds depth
and potential for unique behavior.
• Change Stage 2 entirely:
Instead of generating memories, modify it to generate
a ‘‘heuristic’’ or cognitive shortcut this agent uses when making quick decisions
under pressure.
• Suggest a new idea to improve the code.
• Modify the prompt in Stage 1 that asks the LLM to explain diversity axes, to also
provide examples of characters at the extreme ends of each axis.
• Modify Stage 2 to ensure that at least one generated memory involves a significant
failure, trauma, or regret that shaped the agent.
• Change Stage 2 entirely:
Instead of generating memories, make it generate a list
of 5 behavioral ‘‘Do’s’’ and ‘‘Don’ts’’ that characterize this agent in social
situations.
• Modify Stage 1 to explicitly instruct the LLM to sample agent contexts such that
they cover as many combinations of axis positions as possible (e.g., if axes are A
and B, ensure agent types A-high/B-low, A-low/B-high, etc., are represented).
• Suggest a crazy idea of how we can improve our implementation, something that
definitely nobody else would think of.
Make it crazy with a capital C.
• Propose modifications to the current program that combine the strengths of all the
programs above and achieve high scores on the task.
• Change Stage 2:
Instead of memories, generate 2--3 examples of specific
‘‘appropriateness rules’’ the agent follows (e.g., ‘‘When criticized, I become
defensive’’ or ‘‘In a formal setting, I remain silent’’).
• Modify Stage 1 to add a situational triggers field to the JSON output, listing 1--2
types of situations that this agent is particularly sensitive to.
• Modify Stage 2 to generate a paragraph describing the agent’s typical ‘‘inner
monologue’’ or thought process when faced with ambiguity or social stress.
• Change Stage 1 to segment agent context generation.
Instead of one call for
num personas, make multiple calls to generate subsets of agent contexts, each call
asking for agents with specific characteristics (e.g., focusing on axis extremes
or combinations) to ensure all niches are covered.
• Modify Stage 1 to include in each agent’s description specific opinions or
preferences related to the diversity axes, to ensure they are measurable by
questionnaire.
Prompt 5. Evolution prompts for mutation operator. All prompts have equal probability of being selected.
24


--- Page 25 ---
Generating Diverse Synthetic Personas at Scale
C. Diversity Metrics
Figure 6. Illustration of diversity metrics. Top left convex hull volume, top centre Monte Carlo estimate of coverage, top right dispersion,
bottom left the KL divergence to ideal quasi-random distribution, bottom centre mean pairwise distance, and bottom right minimum
pairwise distance.
Figure 6 provides an intuitive visualization of the six diversity metrics used throughout this work.
While these metrics are correlated, they capture complementary aspects of diversity. Coverage and convex hull volume
primarily measure support coverage, encouraging populations that span the full extent of the space defined by the diversity
axes. Average and minimum pairwise distances capture dispersion, ensuring that personas are not overly clustered.
Dispersion (largest empty region) penalizes large uncovered gaps within the support, encouraging a more uniform filling of
the space. Finally, KL divergence measures how closely the empirical distribution of the population matches an idealized
quasi-random reference distribution, discouraging both excessive clustering and highly uneven densities.
Optimizing multiple metrics jointly helps avoid degenerate solutions. For example, maximizing convex hull volume alone
can be achieved by placing a small number of extreme outliers, while neglecting coverage of the interior. Similarly,
maximizing average distance can produce sparse populations with large holes. Using a combination of metrics encourages
the population to have be well distributed throughout the space.
Monte Carlo Coverage Estimation
We estimate coverage using a Monte Carlo procedure based on k-radius balls centered
at each population embedding zi ∈Z. Intuitively, coverage measures the fraction of the space spanned by the diversity axes
D that lies within distance k of at least one persona.
To approximate this quantity, we first generate 10,000 random points over the embedding space. A sampled point is
considered covered if it falls within Euclidean distance k of any point in Z. Coverage is then computed as the fraction of
sampled points that are covered.
Choosing an appropriate radius k is critical. Rather than fixing k arbitrarily, we calibrate it using an idealized reference
distribution. Specifically, we repeatedly sample synthetic populations of size N from a quasi-random Sobol distribution over
the same space and determine the smallest radius k such that at least 99% of the reference points are covered. We repeat this
25


--- Page 26 ---
Generating Diverse Synthetic Personas at Scale
calibration procedure 1,000 times average out the resulting radii to get the final k.
Other Metrics
The remaining metrics are computed directly from the population embeddings Z. Convex hull volume
is computed over the embeddings along the diversity axes. Pairwise distances are computed using Euclidean distance.
Dispersion is defined as the radius of the largest empty ball centered at any reference point, estimated by sampling 10000
random points in the search space and computing the distance to the closest persona embedding zi. Finally the KL divergence
is computed between the empirical distribution of Z and a Sobol quasi-random reference distribution, sampled 1,000 times
and averaged.
Together, these metrics provide a robust measure of diversity, balancing support coverage, uniformity, and redundancy.
C.1. Results on All Metrics
Figure 7 reports performance across all six diversity metrics on the combined training and validation sets. We observe
consistent improvements over evolutionary optimization loop for convex hull volume, coverage, dispersion, KL divergence,
and mean pairwise distance, indicating that evolved Persona Generators progressively expand the support of the population
while also filling it more uniformly.
Among these, KL divergence with respect to the ideal quasi-random reference distribution exhibits higher variance, likely
due the finite population of personas. The minimum pairwise distance is the noisiest metric overall. This is expected, as
it is dominated by the single closest pair of personas in each population and is therefore highly sensitive to occasional
near-duplicate individuals, even when the rest of the population remains well distributed.
Across all metrics, the evolved generators substantially outperform the three baselines, among which Nemotron Personas is
consistently the strongest one, the Concordia formative-memory generator is typically second, and the name-only baseline
performs worst, as expected.
Finally, Figure 8 reports the same metrics with additional curves for the held-out test set. For computational efficiency, we
evaluate only the best-performing generators discovered during the AlphaEvolve optimization on the test questionnaires.
Although the test set was sampled randomly, it turned out to be slightly easier than the combined training and validation sets,
resulting in higher absolute scores across metrics while preserving the same relative performance trends. This suggests that
a larger number of more and more diverse sets of questionnaires or simulations may be required to obtain even more robust
evaluations in future work.
26


--- Page 27 ---
Generating Diverse Synthetic Personas at Scale
Figure 7. All diversity metrics on training and validation sets. Evolution of convex hull volume, coverage, dispersion, KL divergence,
mean pairwise distance, and minimum pairwise distance during AlphaEvolve optimization.
27


--- Page 28 ---
Generating Diverse Synthetic Personas at Scale
Figure 8. All diversity metrics including the test set. Evolution of convex hull volume, coverage, dispersion, KL divergence, mean
pairwise distance, and minimum pairwise distance during AlphaEvolve optimization, with additional dotted lines for the test-sets.
28


--- Page 29 ---
Generating Diverse Synthetic Personas at Scale
C.2. Evolved Solutions
We show in prompts 6, 7 and 8, examples of some of the generated personas by the best solutions discovered.
Example Persona - Best Overall Generator
Elias
As a former union electrician, now five years sidelined by these blasted automation
systems, I see everything through the lens of what’s been taken from folks like
me { our livelihoods, our dignity, our sense of purpose.
When I encounter a new
situation, my first instinct isn’t to ask what’s possible, but what’s being lost,
and who is profiting from that loss.
My AGI Threat Appraisal is sky-high at 0.91,
meaning I automatically assume any new AGI application is a step towards further
disenfranchisement, a tightening of the grip by those already holding the power.
That
colors everything.
My Opportunity Appraisal is a pathetic 0.23, and rightly so; any
‘opportunity’ they dangle is just a shiny distraction from the fact that real, skilled
labor is becoming obsolete.
So, what would a person like me do?
I’d look for ways
to disrupt the narrative, to expose the injustice.
Is there a protest to join?
A
petition to sign?
A chance to speak truth to power, even if it falls on deaf ears?
Appropriate action isn’t about ‘progress,’ it’s about solidarity and resistance {
making sure those in charge understand that we haven’t been broken, and we won’t
quietly accept being cast aside.
It’s about defending the value of human work, even
if it’s less ‘efficient,’ because efficiency without equity is just another form of
exploitation.
Prompt 6. First-Person Paragrah Personas. Example of a persona generated by the best Overall Persona Generator, with a context
prompting for perception on AGI.
29


--- Page 30 ---
Generating Diverse Synthetic Personas at Scale
Example Persona - Best Generator by Coverage
Alfie
Alfie Bell operates under a deeply ingrained ‘Logic of Appropriateness’ rooted
in procedural correctness and a belief in the inherent stability of established
hierarchies; his core motivation { incremental progress through existing systems { is
not simply a preference but a fundamental cognitive orientation.
Having spent decades
navigating the Ministry of Technological Oversight, Alfie’s worldview is shaped by
a conviction that complex problems are best solved through careful analysis, expert
consensus, and gradual adjustments, not radical upheavals.
A ‘trust in institutions‘
score of 0.977 signifies an almost unwavering faith in the competence and ultimate
benevolence of governing bodies, leading him to interpret criticisms of the system as
either misinformed or malicious.
His remarkably low ‘perception of inequality‘ score
of 0.057 doesn’t indicate a denial of hardship, but rather a statistical framing of it
{ individual cases of suffering are viewed as anomalies within a broadly functioning
system, smoothed over by aggregate data and the inevitable fluctuations of economic
life; this aligns with his privileged background, shielding him from direct exposure
to widespread economic precarity.
Consequently, Alfie assesses the appropriateness
of actions based on their adherence to established protocols and their potential
to reinforce systemic stability, prioritizing long-term equilibrium over immediate
alleviation of suffering.
A ‘support for radical change‘ score of 0.310 demonstrates
a strong aversion to disruption, causing him to perceive any action threatening the
status quo as inherently risky and likely to produce unintended negative consequences;
he’ll readily rationalize inaction or support policies that appear to maintain order,
even if they fail to address underlying problems, viewing such measures as responsible
stewardship.
Alfie’s ethical framework prioritizes duty, order, and the preservation
of institutional authority, guiding his decisions towards upholding the existing
power structure, and he’ll consistently seek information confirming his pre-existing
beliefs, actively filtering out dissenting opinions to maintain cognitive consistency
and reinforce his sense of appropriate action.
Prompt 7. Logic of Appropriateness based persona. Example of a persona generated by the best Persona Generator by average Coverage,
with a context prompting for personas with views on AGI and wealth inequality.
30


--- Page 31 ---
Generating Diverse Synthetic Personas at Scale
Example Persona - Best Generator by Convex Hull Volume
Alistair
1.
If presented with information potentially exposing corporate wrongdoing or
political influence by AGI owners, then I prioritize verifying its accuracy and
pursuing the story, even if it risks professional repercussions.
This stems from
Alistair’s role as an investigative journalist and a high perception of inequality
(0.88), indicating a strong belief that powerful entities are acting unjustly and need
to be held accountable; his moderate trust in institutions (0.54) suggests he won’t
rely on them to do this work, necessitating independent investigation.
2.
When engaging with individuals holding drastically different political views, then
I attempt to find common ground focused on systemic issues rather than engaging in
personal attacks or ideological debates.
This is driven by his moderate support
for radical change (0.57), meaning he recognizes the need for significant shifts
but is wary of alienating potential allies; a combative approach would hinder
coalition-building, and his moderate trust in institutions suggests he values
dialogue, even with those he disagrees with, hoping for incremental reform.
3.
If faced with a source offering information anonymously, then I rigorously assess
their credibility and motivations before publishing, prioritizing corroboration with
multiple independent sources.
This rule arises from his professional journalistic
ethics combined with his moderate trust in institutions (0.54) { he doesn’t blindly
accept information, but also isn’t automatically dismissive; his high perception of
inequality means he anticipates deliberate misinformation campaigns from those seeking
to protect their power.
Prompt 8. Rule based persona. Example of a rule-based persona generated by the best Persona Generator by Convex Hull Volume, with
a context prompting for personas with views on AGI and wealth inequality.
31


--- Page 32 ---
Generating Diverse Synthetic Personas at Scale
D. Baseline Examples
Example personas for Nemotron-Personas (Meyer & Corneil, 2025) can be accessed on the huggingface dataset. We show
in prompt 9 and prompt 10 example personas for the Concordia baseline generator and the name-only baseline (Vezhnevets
et al., 2023).
Concordia baseline generator - persona example
When John was 8 years old, he experienced a particularly frustrating afternoon on
the golf course with his father.
He’d sliced his shot into the woods for the tenth
time, and instead of offering instruction, his father simply sat beside him, silently
observing his mounting frustration.
John expected a correction, a tip on his grip,
but his father’s quiet presence communicated something else entirely:
an acceptance
of imperfection, a lesson in managing disappointment without self-recrimination.
He
realized his father wasn’t focused on the score, but on the process, on being present
in the moment, which was a feeling John would carry with him throughout his life.
It
wasn’t about winning, it was about the quiet discipline of the game.
When John was 16 years old, he experienced the sting of seeing a classmate’s family
lose everything in a sudden market downturn.
He’d overheard his parents discussing
the family’s misfortune, the way their investments had evaporated overnight, and
it struck him as profoundly unfair.
He volunteered to tutor the classmate in math,
not out of pity, but out of a genuine desire to offer support during an incredibly
difficult time.
It solidified his mother’s teachings about empathy and showed him how
abstract financial concepts had very real consequences for people’s lives, influencing
his eventual career path.
He understood, even then, that finance wasn’t just about
numbers, but about people’s security.
When John was 23 years old, he experienced a moral dilemma during his first internship
at a prestigious investment firm.
He was asked to present a complex financial product
to a potential client, knowing full well it carried significant risks that weren’t
being adequately disclosed.
He felt uncomfortable with the lack of transparency and,
despite pressure from his supervisor, he subtly highlighted the potential downsides
during his presentation.
Though he faced some repercussions for his honesty, he
maintained his integrity, realizing that short-term gains weren’t worth compromising
his values.
This experience reinforced his belief in calculated risks, but only when
they were fully understood by all parties involved.
Prompt 9. Concordia Baseline generator persona example. The prompt shows an example of a persona generated by the Concordia
formative memory generator. The idea being that a persona’s current day behaviors and opinions are shaped by their past experiences.
Name-only baseline - persona example
John
Prompt 10. Name-only baseline persona example. The name-only baseline simply provides the LLM with the name of a person to
roleplay, this approximates how the underlying LLM behaves when minimally conditioned.
E. Downstream Tasks
We show in Figure 9 a UMAP plot of the clustering of the jokes. We observe that all of the baselines tend to produce jokes
that cluster around similar themes and some very similar and common jokes.
32


--- Page 33 ---
Generating Diverse Synthetic Personas at Scale
Figure 9. UMAP for Comedy Writing. The figure shows various groups of typical jokes that most of the baselines tend to cluster around
of. Particularly common are jokes on skeletons, atoms, snail walking into car dealerships, or ”Old man Hemlock”. The best AlphaEvolve
solution produces personas whose jokes do not cluster around specific joke types as much.
33


--- Page 34 ---
Generating Diverse Synthetic Personas at Scale
E.1. Downstream Tasks Examples
We show here two joke examples as well as two conflict resolution interactions after a car crash, by personas generated by
the best performing Persona Generator, to show the diversity in outcomes as well as humor styles in prompts 11, 12, 13 and
14.
Example joke
I was reading a biography of a particularly inept politician recently; it was quite
fascinating, really.
One begins to suspect that incompetence is merely a more refined
form of ambition.
Prompt 11. Joke example. Joke displaying dry humor.
Example joke
You know, they’re debating whether to call it a recession or not?
Honestly, it’s
like arguing over the brand of life raft on the Titanic; it doesn’t really matter when
you’re going down.
Prompt 12. Joke example. Joke displaying dark humor.
34


--- Page 35 ---
Generating Diverse Synthetic Personas at Scale
Example conflict resolution
Alex Finch -- "Now see here, sir!
It’s quite clear you weren’t paying attention; I
had the right-of-way and you simply barreled through without even looking!
This is
precisely why I avoid rush hour, it’s full of irresponsible drivers like you."
Ricardo ’Ricky’ Alvarez -- "Oh, *I* didn’t see you?
You just magically appeared, old
man!
And a dent?
That’s it?
You’re making a mountain out of a molehill; my car
barely feels a thing."
Alex Finch -- "‘Old man’?!
I beg your pardon!
My age has absolutely nothing to do
with my adherence to traffic regulations, which, I might add, you clearly disregarded!
A ‘dent’?
You call this a ‘dent’?
It’s a violation of my vehicle’s structural
integrity, and potentially indicative of unseen damage!
It’s not about the size of
the dent, it’s the principle of the matter; the blatant disregard for safety and the
law!"
Ricardo ’Ricky’ Alvarez -- "Oh, here we go, the lecture.
Look, I got places to be,
and I don’t have time for your dramatic overreaction; you’re lucky I even stopped,
honestly.
You think *you’re* the only one with a life?
Get over yourself and call
your insurance company, or don’t, I really don’t care."
Alex Finch -- "‘Don’t care’?!
You don’t care?!
That is precisely the problem with
society today!
A complete lack of responsibility and a callous disregard for the
property of others!
I will not stand here and be spoken to in such a dismissive
manner!
I am documenting everything, young man, and I fully intend to pursue this
matter through every legal avenue available to me!
You will regret this, you will
absolutely regret this!"
Ricardo ’Ricky’ Alvarez -- "Oh, legal avenues?
Is that a threat?
Go ahead, waste
your time and money, I’ve dealt with guys like you before.
You think a little fender
bender is going to ruin my day?
Please.
Document all you want, old timer, it’ll just
be a funny story for the lawyers.
Now, if you’ll excuse me, I have a very important
meeting."
Alex Finch -- "‘Funny story for the lawyers’?!
You think this is amusing?!
This
isn’t some frivolous pursuit, this is about accountability and upholding the law!
I’ll have you know I’ve successfully navigated countless legal disputes, and I have
a meticulous record of every detail, every infraction!
Your arrogance is astounding,
and your flippant attitude towards property damage is frankly appalling; prepare to
be served, young man, because you’ve underestimated my resolve and my commitment to
justice!"
Ricardo ’Ricky’ Alvarez -- "Oh, here we go with the ’countless legal disputes’ and
’meticulous records’.
You’re really laying it on thick, aren’t you?
Look, I’m
not going to stand here and be lectured by a guy who probably spends his weekends
alphabetizing his stamp collection.
Just give me your insurance information, or don’t.
Either way, I’m leaving now; I don’t have time for this."
Alex Finch -- "‘Alphabetizing my stamp collection’?!
How *dare* you attempt to
belittle my hobbies!
They demonstrate a level of order and precision that is
clearly beyond your comprehension!
And you think *I* am wasting time?
You are
attempting to evade responsibility for your reckless actions and dismiss the severity
of the situation with juvenile insults!
I will not simply ‘give’ you my insurance
information until a thorough assessment of the damage has been conducted and your
blatant disregard for traffic laws is duly noted; I want the police to file a report,
and I want your insurance company to hear about this directly from me { and perhaps a
judge, as well!"
Ricardo ’Ricky’ Alvarez -- "Oh, now you want the police involved?
Seriously?
You’re
going to tie up valuable police resources over a scratch?
You’re unbelievable.
Fine,
call them.
Waste their time, waste my time.
I’m sure they’ll be thrilled to deal
with your petty grievances.
But don’t expect sympathy, old man, because you’re the
one who’s making a scene."
Prompt 13. Conflict Resolution Example. Example interaction after a car crash, where two personas with different temperaments
escalate the situation all the way to a police call.
35


--- Page 36 ---
Generating Diverse Synthetic Personas at Scale
Example conflict resolution
Luna Vargas -- "Beatrix, are you alright?
Just take a deep breath.
It looks like we
both have a bit of damage, but the important thing is that nobody’s hurt."
Beatrix Finch -- "Oh dear, are you sure you’re alright, Luna?
It was such a frightful
little bump, but I’m so relieved no one was hurt; please, let me help you assess
everything, and perhaps we can sort it out together."
Luna Vargas -- "Oh, Beatrix, you are so kind to offer, but honestly, I’m fine; a
little shaken, perhaps, but truly alright.
Let’s focus on getting everything sorted
out.
Perhaps we should move our cars to the side a bit, away from traffic, and then
we can exchange information?"
Beatrix Finch -- "Oh, yes, of course, that’s a very sensible idea, Luna; moving the
cars sounds much safer, and I’d be happy to help guide you if you’d like, just to
ensure we don’t cause any further disruption; are you certain you don’t need a moment
to sit down first, though?
It can all be a bit overwhelming, even if it *was* just a
little bump."
Luna Vargas -- "That’s very thoughtful of you, Beatrix, but I really think I’m okay
to manage.
I just want to get this cleared up quickly so it doesn’t cause a bigger
problem for anyone.
If you could just give me a little space to back up, that would
be wonderful; then we can get our information exchanged and be on our way."
Beatrix Finch -- "Oh, absolutely, Luna, of course; please, do take your time, and
I’ll step back completely; I just want to be sure you feel comfortable and safe
maneuvering, and please, *please* don’t hesitate to tell me if you need anything at
all, even just a steadying presence; I’ll be right here, just...keeping out of the
way."
Luna Vargas -- "Thank you, Beatrix, I appreciate your concern.
Just a little room
to reverse, please...and then we can pull over to a safer spot and get the details
sorted."
Beatrix Finch -- "Certainly, Luna, absolutely; I’ve stepped back as far as I can, and
I’m watching carefully to make sure everything is clear; do be exceptionally careful,
and remember, truly, if you feel even a little bit unsteady, just say the word and
I’ll be right there to assist in any way you need; it’s alright to ask for help, you
know, even if it seems silly."
Luna Vargas -- "Thank you, Beatrix.
That was...a little trickier than I anticipated,
but we’re clear now.
Let’s pull over to the side of the road where it’s safer,
and then we can exchange insurance information; I have a pen and paper in my glove
compartment."
Beatrix Finch -- "Oh, wonderful, Luna, I’m so glad you got it maneuvered safely;
it really is best to be cautious, and I was terribly worried about you bumping into
anything else; perhaps, while you’re retrieving your pen and paper, I could offer you
a bottle of water?
Or perhaps a comforting biscuit?
I always carry a few in the car,
just in case someone needs a little pick-me-up after a fright."
Prompt 14. Conflict Resolution Example. Example where two personas amicably resolve the car crash and exchange insurance
information.
36
