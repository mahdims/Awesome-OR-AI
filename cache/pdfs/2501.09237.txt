--- Page 1 ---
1
Split Fine-Tuning for Large Language Models in
Wireless Networks
Songge Zhang, Guoliang Cheng, Xinyu Huang, Zuguang Li, Graduate Student Member, IEEE, Wen Wu, Senior
Member, IEEE, Lingyang Song, Fellow, IEEE, and Xuemin (Sherman) Shen, Fellow, IEEE
Abstract—Fine-tuning is the process of adapting the pre-
trained large language models (LLMs) for downstream tasks.
Due to substantial parameters, fine-tuning LLMs on mobile
devices demands considerable memory resources, and suffers
from high communication overhead and long fine-tuning delay.
In this paper, we propose an efficient LLM fine-tuning scheme
in wireless networks, named Split Fine-Tuning (SFT), which can
accommodate LLM fine-tuning on mobile devices. Specifically,
an LLM is split into a server-side part on the edge server and
a device-side part on the mobile device to satisfy the device-
side memory constraint. All devices share a server-side model
and perform parallel fine-tuning to reduce fine-tuning delay. In
addition, to reduce significant communication overhead incurred
by data exchange between devices and the edge server, we
propose a data compression scheme by jointly leveraging sparsi-
fication, stochastic quantization, and lossless encoding methods.
Furthermore, we formulate a fine-tuning delay minimization
problem under accuracy and memory constraints, taking device
heterogeneity and channel dynamics into account. To solve
the problem, the nonlinear mixed-integer problem is decoupled
into two subproblems in different timescales. The two-timescale
resource management algorithm is proposed to jointly optimize
the compression rate and transformer block allocation in the
large timescale using the augmented Lagrangian method, and
determine spectrum resource allocation in the small timescale via
sequential quadratic programming. Extensive simulation results
demonstrate that the proposed scheme can reduce the fine-tuning
delay by up to 80.2% and communication overhead by 93.6%
compared to state-of-the-art benchmarks, while satisfying device-
side memory and model accuracy constraints.
Index Terms—Large language models, fine-tuning, split learn-
ing, resource management.
I. INTRODUCTION
Recent advancements in large language models (LLMs),
such as ChatGPT, LLaMA, and Vision Transformer [1]–[3],
have sparked a new wave in artificial intelligence (AI) by
showcasing impressive abilities such as improved general-
ization and inference. These advancements have facilitated
Songge Zhang is with the School of Electronic and Computer Engineering,
Peking University, Shenzhen, 518000, China, and also with the Frontier
Research Center, Pengcheng Laboratory, Shenzhen, 518055, China (email:
zhangsongge@stu.pku.edu.cn);
Guoliang Cheng, Zuguang Li, and Wen Wu are with the Frontier Research
Center, Pengcheng Laboratory, Shenzhen, 518055, China (email: {chenggl,
lizg01, wuw02}@pcl.ac.cn);
Lingyang Song is with the State Key Laboratory of Advanced Optical
Communication Systems and Networks, School of Electronics, Peking Uni-
versity, Beijing, 100871, China, and also with the School of Electronic
and Computer Engineering, Peking University Shenzhen Graduate School,
Shenzhen, 518055, China (e-mail: lingyang.song@pku.edu.cn);
Xinyu Huang and Xuemin (Sherman) Shen are with the Department of
Electrical and Computer Engineering, University of Waterloo, Waterloo, N2L
3G1, Canada (email: {x357huan, sshen}@uwaterloo.ca).
extensive applications across various fields, such as chatbots,
search engines, writing assistants, and multimodal systems [4],
illuminating the vision of artificial general intelligence [5], [6].
Deploying large models on mobile devices enables timely task
responses near the mobile devices in wireless networks.
Fine-tuning is a critical technique for LLM to support
personalized services and downstream tasks [7]. Fine-tuning
adjusts LLM parameters using local data for specific tasks,
retaining pre-trained performance. Fine-tuning can be per-
formed through full parameter tuning or parameter-efficient
fine-tuning (PEFT). PEFT updates only a small subset of
parameters or additional modules while keeping the rest of
the model frozen, which can reduce the computation workload
on devices [8]. In wireless networks, LLMs are fine-tuned
based on a number of mobile devices’ data, such as behavioral
records, location information, and multimedia content, which
cannot be uploaded to the centralized cloud due to data privacy
concerns [9]. Therefore, fine-tuning should be performed in
a distributed manner via the collaboration among multiple
devices without sharing raw local data [10].
In
the
literature,
fine-tuning
explores
a
distributed
paradigm with PEFT techniques such as low-rank adaptation
(LoRA) [11]. LoRA can modify a small portion of the
LLM parameters instead of the entire LLM in the fine-tuning
process. Although distributed schemes with PEFT are highly
parameter-efficient, they still demand substantial memory to
deploy the entire LLM, exceeding the memory capacity of
typical mobile devices [12], [13]. As shown in Table I, typical
NVIDIA Jetson mobile devices cannot meet the memory
requirements for training LLMs like GPT-3 and LLaMA [2].
To address this issue, split learning (SL) is a potential solution,
which can divide LLMs into server-side and device-side parts,
offloading most of the model to the server, thereby satisfying
the device’s memory constraints [14].
However, designing an efficient SL scheme for LLMs fine-
tuning presents several challenges. Firstly, in the vanilla SL,
all devices interact with the server in a sequential manner
to complete the fine-tuning process, which results in a long
fine-tuning delay. Secondly, the SL scheme needs to period-
ically transmit intermediate activations and activation gradi-
ents between the server and devices, resulting in significant
communication overhead. The activation and gradient values
are typically large in the LLM. Considering ViT-16 and a
mobile device with 5,000 data samples and 196 tokens, the
total immediate activation data volume at the cut layer can
be approximately 2.81 GB for one fine-tuning epoch [15].
Thirdly, heterogeneous devices’ computational capabilities and
arXiv:2501.09237v1  [cs.DC]  16 Jan 2025


--- Page 2 ---
2
TABLE I: Comparison between Typical Mobile Device Resources and Memory Requirements for Deploying Typical LLMs.
Resources of Typical Mobile Devices
Required Resources for Deploying Typical LLMs
Devices
Memory
Models
# Parameter
Memory
Raspberry Pi-4B
4 GB
LLaMA-7B
7 Billion
28 GB
NVIDIA Jetson Nano
8 GB
LLaMA-65B
65 Billion
260 GB
NVIDIA Jetson TX2
8 GB
GPT-3
175 Billion
700 GB
NVIDIA Jetson Xavier NX
16 GB
PaLM
540 Billion
2.15 TB
dynamic channel conditions result in varying delay for each
device, known as the straggler effect, which increases the fine-
tuning delay.
To address the above, we propose a split fine-tuning (SFT)
scheme that satisfies memory constraints by splitting the LLMs
into device and server parts. All devices collaboratively fine-
tune LLMs with the server using the devices’ local data
and parallelly update local device-side LoRAs, which are
aggregated in each epoch. The fine-tuning process on each
device is conducted in parallel, and the server executes all
devices’ training processes with only one server-side model.
Once all devices complete their fine-tuning, each device up-
loads the updated device-side LoRAs for aggregation. The
proposed scheme can efficiently reduce memory consumption
and computation workload on the device by splitting the
LLM. In addition, we design a novel compression scheme and
resource management algorithm. Specifically, the compression
scheme includes Top-K sparsification, stochastic quantization,
and lossless encoding, which focus on the most significant
parameter for the accuracy performance of LLMs and reduce
the data volume within an allowable accuracy degradation.
Additionally, we analyze the delay performance, commu-
nication and computation overhead, as well as memory con-
sumption of the proposed scheme. Furthermore, we formulate
a fine-tuning delay minimization problem under constraints of
device-side memory and accuracy, which is a nonlinear mixed-
integer programming problem. We decompose the problem
into two subproblems by distinguishing different variables in
the different timescales: a large-timescale subproblem for de-
termining compression rates and block allocation, and a small-
timescale subproblem for determining the device’s bandwidth
allocation. Specifically, we use the augmented Lagrangian
method to iteratively solve the former subproblem, addressing
its non-convex constraints, and employ sequential quadratic
programming (SQP) to iteratively approximate the latter non-
linear subproblem as a series of quadratic subproblems. The
extensive simulation results demonstrate that the proposed
scheme can reduce total fine-tuning delay up to 80.2% and
communication overhead up to 93.6% compared to the state-
of-the-art scheme. The main contributions of this paper are
summarized as follows:
1) We propose an SFT scheme for LLM fine-tuning in
wireless networks, enabling collaborative and parallel
fine-tuning across multiple devices.
2) We propose a novel compression scheme that signifi-
cantly reduces communication overhead.
3) We analyze the performance of the SFT scheme, includ-
ing fine-tuning delay, memory consumption, communi-
cation overhead, and computational workload.
4) We propose a resource management algorithm to reduce
fine-tuning delay by optimizing compression rates, trans-
former block allocation, and spectrum resources.
The remainder of this paper is organized as follows. Sec-
tion II reviews the related works. Section III presents the
system model. Section IV introduces the proposed scheme.
Section V introduces the performance analysis. Sections VI
and VII detail the problem formulation and the corresponding
solution, respectively. Section VIII provides the simulation
results, and finally Section IX concludes the paper.
II. RELATED WORK
A wide range of studies aim to improve LLM fine-
tuning performance from different perspectives. Adapters
achieve parameter-efficient fine-tuning (PEFT) by introducing
lightweight trainable modules between existing layers of the
model while freezing the rest [16]. Prefix-tuning optimizes
a small set of prefix vectors prepended to the input at each
layer, leaving the model’s original parameters unchanged [17].
LoRA reduces the parameter update size by injecting low-rank
trainable matrices into the attention mechanism while keeping
the majority of the model frozen [18]. To achieve collaborative
training over wireless networks, some work leverages FL
to allow multiple users to collaboratively fine-tune LLMs
without the need to share data [19]. In these studies, the entire
model is deployed on local devices for training, with model
parameters or gradients aggregated at each round [20]. Some
studies explore FL with PEFT techniques. [21] propose the
PromptFL framework, which enables federated participants
to train shared prompts instead of the entire model. Jiang et
al. [22] introduce a soft label-enhanced federated fine-tuning
approach that incorporates LoRA to reduce computational and
communication costs. Cai et al. [23] propose FedAdapter,
which progressively adjusts adapter configurations to identify
the most efficient settings, accelerating model convergence.
Additionally, some studies enable LLM fine-tuning in FL
using techniques like distillation, pruning, and quantization.
Liu et al. [24] introduce an adaptive quantization scheme with
ensemble distillation, which compresses a large model into a
smaller one and applies cluster partitioning for heterogeneous
model training. Greidi et al. [25] propose sparse training to
reduce computation and communication costs in FL. These
studies enable collaborative LLM fine-tuning across multiple
devices while reducing the computational load on devices and
communication overhead.
In addition to FL, SL is another potential solution for
enabling fast model training. The SL scheme can reduce the
computational workload and memory consumption on devices
by partitioning the model [26]. To accelerate the training of
AI models, Wu et al. introduced a parallel-then-sequential SL
strategy to enhance training speed [27]. Kim et al. and Xu et


--- Page 3 ---
3
(a) Proposed SFT
(b) Transformer Block
Fig. 1: (a) In the SFT frame, devices are trained parallelly with a shared server-side pre-trained model and multiple LoRAs;
(b) All Transformer blocks are divided into the server-side and the device-side parts. Each transformer block consists of an
MSA and an MLP, both of which are composed of pre-trained model weights and corresponding LoRA.
al. proposed a synchronized SL (SFL) scheme, allowing multi-
ple server-side and device-side models to train simultaneously,
thereby speeding up the SL process [28]. Liao et al. addressed
system heterogeneity and accelerated model training by opti-
mizing bandwidth allocation [29].To more efficiently improve
SL, joint optimization of model partitioning and resource
management has also been explored. For example, in [30], the
optimization of the partitioning layer and bandwidth allocation
was proposed to mitigate the straggler effect in wireless SFL.
Very recently, a few pioneering works have been devoted to
enhancing LLM fine-tuning via SL. Wu et al. split the large
model into different components, deploy them across multiple
devices, and fine-tune these components on the device side
using a pipeline approach [31]. Chen et al. split the Adapter
from the large model’s backbone and deploy it on devices for
fine-tuning [32]. Different from the existing scheme, we design
a parallelized split scheme to accelerate the delay of LLM fine-
tuning. Additionally, we implement compression strategies to
reduce the communication overhead between the edge server
and mobile devices.
III. SYSTEM MODEL
A. Considered Scenario
In this paper, we consider a typical wireless network
scenario consisting of multiple mobile devices and a base
station equipped with an edge server with powerful compu-
tational and memory resource. We define the set of devices
as N = {1, 2, . . . , N}. Each device possesses local data and
operates a small portion of the LLM. The data from all devices
contribute to fine-tuning tasks such as image classification,
natural language processing, etc. The edge server is primarily
responsible for the LLMs in a fine-tuning training task and
also manages model aggregation at each training round.
In this paper, we consider a transformer-based LLM ar-
chitecture consisting of L layers. These transformer blocks
are split into the server and devices, with the device locally
processing l blocks while the remaining L −l blocks are
possessed by the server. Forward and backward propagation
between the edge server and devices is carried out through
the transmission of intermediate activations and activation
gradients. The parameter size of each block is related to the
dimensions of the multi-head self-attention (MSA) and multi-
layer perceptron (MLP).
B. Fine-Tuning Model
We focus on supervised fine-tuning tasks, utilizing the
LoRA adapter to reduce the cost and parameter number of
fine-tuning. We denote dataset of each device as Dn
=
{xj, yj}|Dn|
j=1 , where |Dn| denotes the total number of samples
for device n ∈N and P
n∈N Dn = D where D represents
the total number of samples across all devices. Here, xj is
the j-th input sample, and yj is its corresponding label. Let
Θ represent the LLM parameters. The loss function for each
sample, denoted as fj(Θ), measures the prediction error for
the j-th sample. The objective of the training process is to
minimize the empirical loss F(Θ), which is defined as
F(Θ) =
1
|D|
|D|
X
j=1
f(xj, yj; Θ) =
1
|D|
|D|
X
j=1
fj(Θ).
(1)
This process seeks to minimize the average prediction error
across all samples and identify the optimal parameter Θ⋆.
To enhance the efficiency of local model transmission and
aggregation, we integrate a small, low-rank, homogeneous
adapter into the locally pre-trained model. As depicted in Fig.
1b, the LoRA adapter is designed to retain the same input
dimension d and output dimension h as pre-trained large mod-
els, and is applicable to linear, embedding, and convolutional
layers. Particularly for linear layers, the adapter reduces the
parameter number by decomposing the conventional parameter
matrix Rd×h into two smaller matrices A ∈Rd×r and B ∈
Rr×h, where the rank r is significantly lower than both d and
h. During the initialization phase, matrix A is initialized with
a Gaussian distribution with mean zero and variance σ2, while
matrix B is set to zero [33]. In fine-tuning training, we define
Θ ∈Rd×m as a trainable weight matrix of the pre-trained
model, where the corresponding model update is expressed as
Θ + ∆Θ = Θ + AB. LoRA adapter configuration allows the
parameter number to be reduced significantly compared to full-
parameter fine-tuning, thereby enhancing computational and


--- Page 4 ---
4
Algorithm 1: Split fine-tuning scheme.
1 Initialize the cut layer and LoRA adapter parameter;
2 Base station broadcasts 1 to l transformer block;
3 for training round t = 1, 2, . . . , T do
4
Base station broadcasts the latest device-side
LoRA adapter;
5
for device n ∈N do
6
for local epoch k = 1, 2, . . . , K do
7
# Device executes
8
Randomly selects a mini-batch sample from
the local data;
9
Execute the FP of the device-side
pre-trained model and LoRA adapter, then
obtain an immediate result;
10
Transmit immediate result to the server
based on the compression scheme;
11
# Server executes
12
Receive the immediate result from the
device;
13
Execute the FP of server-side pre-trained
model and n-th LoRA adapter;
14
Update the server-side LoRA adapter;
15
Transmit the device-side LoRA adapter’s
gradient;
16
end
17
Update the device-side LoRA adapter to the
edge server;
18
end
19
All devices send the latest device-side LoRA
adapter to the edge server;
20
Edge server aggregates server-side LoRA adapters
and device-side LoRA adapters into a new LoRA.
21 end
communication efficiency. Accordingly, we deploy the LoRA
adapter within the considered scheme. Thus, the objective of
fine-tuning can be rewritten as
F(w) =
1
|D|
|D|
X
j=1
fj(Θ + ∆Θ).
(2)
which is to minimize the average prediction error across all
samples and identify optimal parameter ∆Θ⋆.
IV. SFT SCHEME
A. SFT Scheme
To enable collaborative fine-tuning on memory-constrained
devices, we propose an innovative split fine-tuning architecture
called SFT. The process of this scheme is detailed in Alg. 1
and primarily includes the following steps.
Initial stage. The initial stage of fine-tuning involves
splitting the transformer blocks and distributing them to the
devices. In the initial phase, the pre-trained model, which
consists of L layers, is split into two parts: Θu for the device-
side model, and Θs for the server model. The segmented
pre-trained model is distributed from the edge server to the
device only once throughout the training process, because
the pre-trained model does not require updates and aggre-
gation in each fine-tuning round. We assume there exist L
LoRA adapters, denoted as L = {1, . . . , L}. Specifically,
the device model consists of the l-th layer, denoted as
Lu = {1, 2, ..., l}. The server handles the remaining layers
from l + 1 to L, indicated as Ls = {l + 1, . . . , L}. We
assume the device-side LoRA adapter, ∆Θu, consists of
{A1, B1, . . . , Al, Bl}; and the server’s adapter, ∆Θs, com-
prises {Al+1, Bl+1, . . . , AL, BL}. These LoRA adapters are
updated in each training round and distributed to the devices
by the server.
Forward propagation (FP) in device-side and server-side
model. Assume the fine-tuning spans T rounds. In round t
and k epoch, each device randomly selects a mini-batch from
their local data, denoted by Bn(t, k) ⊆Dn, to perform FP.
Here, B = |Bn(t, k)| represents the size of each mini-batch,
and Dn is the local dataset of device n. The FP involves
passing Bn(t, k) through Θu(t, k) and ∆Θu(t, k) to produce
the output yu
n(t, k). This process can be mathematically rep-
resented as
yu
n(t, k) = f(Θu(t, k), ∆Θu(t, k), Bn(t, k)).
(3)
All devices transmit their locally computed outputs to the
server. The server then conducts FP based on the server-side
Θs(t, k) and n-th LoRA adapter ∆Θs(t, k). This process can
be mathematically represented as
ys(t, k) = f(Θs(t, k), ∆Θs(t, k), yu
n(t, k)).
(4)
Since we consider only one pre-trained model on the server
side, the server will perform the LoRA FP sequentially accord-
ing to the order in which the immediate activation is received.
Backward propagation (BP) in device-side and server-
side model. BP requires updating the LoRA adapter parame-
ters of the server and device. The purpose is to minimize the
global loss function as defined in Eq. (2). The edge server
calculates gradients for both the server-side and device-side
LoRA adapter gu
n(t, k) and gs
n(t, k) based on the prediction
outcomes ys(t, k) and labels. Subsequently, the edge server
updates server-side LoRA adapters parameters as follows:
∆Θn
s (t, k) ←∆Θn
s (t −1, k) −ϵsgs
n(t, k), ∀n ∈N.
(5)
where ϵs denotes the learning rate for the LoRA adapter on the
edge server. For simplicity, assume each transformer block is
linked sequentially, with the LoRA adapter updating from the
last to the cut (l+1)-th layer. When the gradient is updated to
the cut layer, gradients for layers 1 through l of LoRA adapters
are transmitted back to the corresponding device. The device
then update the device-side LoRA adapters as follows:
∆Θn
u(t) ←∆Θn
u(t −1, k) −ϵugu
n(t, k), ∀n ∈N,
(6)
LoRA aggregation in each round. Each device will
iteratively perform the first three stages for K epochs. All
devices will then upload their updated LoRA adapters to the
edge server for aggregation. The server aggregates server-side


--- Page 5 ---
5
Fig. 2: Transmission compression scheme.
and device-side LoRA adapters using the FedAvg method. The
aggregation process can be mathematically represented as
∆Θs
n(t + 1, 1) =
N
X
n=1
Dn
D ∆Θs
n(t, K), ∀n ∈N,
(7)
and
∆Θu
n(t + 1, 1) =
N
X
n=1
Dn
D ∆Θu
n(t, K), ∀n ∈N.
(8)
The aggregated process will involve a new fine-tuning round
until the target accuracy is reached.
B. Compression Scheme
In the LLMs, the transmission of intermediate parameters
between device-side transformer blocks and server-side trans-
former blocks can become a communication bottleneck. To
address the high communication cost caused by transmitting
the full activation matrix sl between transformer blocks,
sparsification, quantization, and encoding techniques can be
applied to reduce the amount of data transferred while re-
taining the most critical information [34]. Let sl ∈RN×D
represent the activations that are transmitted from the l-th
transformer block to the (l+1)-th transformer block, where N
is the number of patches (or tokens), and D is the embedding
dimension. Directly transmitting the full matrix sl between
blocks incurs a high communication cost, especially when N
and D are large. The transmission compression scheme can
be represented as shown in Fig. 2.
Top-K sparsification: In Top-K sparsification, we selec-
tively retain only the K largest-magnitude elements in the
matrix sl, while setting the remaining elements to zero.
This approach effectively reduces the communication cost by
focusing on the most significant activations. The steps for
applying Top-K sparsification to sl are as follows:
1) Compute absolute values: For each element sl,i,j in the
matrix sl, compute the absolute value |sl,i,j| to capture
the magnitude of each activation.
2) Select Top-K elements: Identify the indices of the K
largest values based on their magnitudes. Let IK rep-
resent the set of indices corresponding to these top K
elements.
3) Construct the sparse matrix: Create a sparse version of
sl, denoted as ˆs, where only the elements in IK retain
their original values, and all other elements are set to
zero. This sparse matrix can be expressed mathemati-
cally as follows:
ˆsl,i,j =
(
sl,i,j,
if (i, j) ∈IK;
0,
otherwise.
(9)
By applying this process, we obtain a sparse matrix ˆsl that
reduces the communication load between transformer blocks
while preserving the most significant activation values. The
sparsification rate, ρ, is defined as the ratio of the number of
retained elements K to the total number of elements in sl,
expressed as:
ρ =
K
dim(sl),
(10)
where dim(sl) denotes the total number of elements in the
matrix sl. The sparsification rate satisfies the condition: ρ ∈
R+.
Stochastic quantization: Our aim is to utilize fewer bits for
representing the non-zero parameter ˆsl in the quantized vector.
Initially, we compute the absolute values of ˆsl, denoted as |ˆsl|.
We then identify the maximum and minimum non-zero values
of |ˆsl| within the set, expressed as smax = max{|s|, s ∈ˆsl}
and smin = min{|s|, s ∈ˆsl}, respectively. For a quantiza-
tion level E ∈Z+, we define the set of quantized values
Q = {Q1, Q2, . . . , QE}, where each quantization point Qe is
calculated as
Qe = e(smax −smin)
E
+ smin.
Our objective is to map each non-zero scalar s ∈ˆsl into
this discrete quantized set Q. To achieve this, we select an
index e such that Qe ≤|s| ≤Qe+1, thereby determining
the quantization interval for s as [Qe, Qe+1]. The quantization
procedure for any non-zero scalar s ∈ˆsl is then defined as
quant(s, Q) =
(
sgn(s) · Qe,
with probability Qe+1−|s|
Qe+1−Qe ;
sgn(s) · Qe+1,
otherwise,
where sgn(s) ∈{−1, +1} determines the sign of s. Then,
this approach yields the sparse and quantized model update
quant(ˆsl, Q).
Lossless encoding: Given the distribution characteristics of
Lt,i, where smaller indices tend to appear more frequently,
entropy coding can be utilized to reduce the data size. Ad-
ditionally, the sparse binary matrix mt,i can be efficiently
compressed using Golomb encoding [35], [36].
Based on the established compression scheme, we can sys-
tematically adjust the combinations of {ρ, E} and document
the corresponding compression rates. Using this data, we then
construct a piecewise linear function to predict the optimal
compression strategy {ρ, E} based on a given β. This function
can be effectively approximated by the server using a minimal
set of public training data in an offline setting.
C. Accuracy Performance
The relationship between the objective accuracy function
and the optimization parameters (sparsity rate and quanti-
zation level) is challenging to define theoretically due to


--- Page 6 ---
6
Fig. 3: Fitting results for SFT data processing accuracy.
the complex nature of model compression methods, such as
pruning, quantization, and distillation. These methods modify
the model’s parameters and structure, leading to performance
changes that are highly nonlinear, with intricate dependencies
across network layers and the inherent non-linearity of ac-
tivation functions. To address this challenge, we approached
the problem from a data-driven perspective. Specifically, we
investigated how the accuracy is influenced by the sparsity
rate and quantization level by fitting the observed data to a
third-order function [37]. Through this approach, we derived
a model that captures the relationship between accuracy and
the optimization parameters. The fitting results, as illustrated in
Fig. 3, demonstrate a strong alignment between the predicted
and actual data, achieving a mean squared error of less than
0.26%.
V. PERFORMANCE ANALYSIS
A. Fine-Tuning Delay Analysis
In this section, we analyze the delay in each fine-tuning
round, which includes several phases.
1) Transformer block distribution (TD) latency: At the
beginning of training, the server divides the transformer block
into a server part and a device part. The device-side trans-
former block or aggregated device-side LoRA is broadcast
to all devices. Let the initial data size of the allocated
transformer block be denoted as Ψ(l) and the data size
of its corresponding LoRA as ΨL(l). Following Shannon’s
theorem, the rate of transformer block distribution is given
by r = C log2 (1 + SNRs) where C and SNRs represent the
total bandwidth and the signal-to-noise ratio, respectively. The
corresponding transmission latency can be expressed as
τ t
ED(l) =
(
Ψ(l)/r,
if t = 1;
ΨL(l)/r,
if t > 1.
(11)
2) Device-side local computation (CC) delay: The device’s
local computation delay involves the transformer block’s FP
delay, which includes both the pre-trained model and LoRA.
Let ΦF
c (l) be the FLOPs required to be computed on device
n, which will be discussed in the following subsection. The
local computation delay for device n can be given by
τ t
CC(l) = ΦF
c (l)/fnCu
nDu
n, ∀n ∈N,
(12)
Fig. 4: Fine-tuning delay in each round.
where fn is the GPU frequency of device n, CU
n is the number
of cores of the GPU at device n, and DU
n represents the
number of FLOPs executed in a single core cycle of the GPU.
3) Immediate activation transmission (IT) latency: Each
device transmits the immediate activation from the l-th trans-
former block to the server. Let ΨA denote the size of the
output, which matches the size of the input token. The
transmission rate for device n is given by rUL
n (bn)
=
bn log2 (1 + SNRn) , ∀n ∈N, where bn represent the allo-
cated bandwidth to device n and SNRn represents its uplink
signal-to-noise ratio. With the sparsification ratio β, the IT
latency is defined as
τ t
IA(β, bn) = βΨA/rUL
n (bn), ∀n ∈N.
(13)
4) Server-side computation (SC) delay: The server’s com-
putation delay involves considering the forward and BP delay
of the server-side transformer block. FP uses the pre-trained
model along with the corresponding LoRA, while BP only
updates the LoRA parameters, without updating the pre-trained
model’s parameters. Let ΦF
s (l) be the total Flops needed to
be computed in the FP of server and ΦB
s (l) represent the
computational load during LoRA’s BP. The training delay on
server’s training can be expressed as
τ t
SC(l, f s
n) = (ΦF
s (l) + ΦB
s (l))/f s
nCsDs, ∀n ∈N.
(14)
5) Gradient transmission (GT) latency: The server trans-
mits the gradient of immediate activation. We assume
the
transmission
rate
for
the
server
is
rDL
n
(bn)
=
bn log2 (1 + SNRs) , ∀n ∈N. The gradient transmission delay
from the server to the device can be given by
τ t
GT(β, bn) = βΨG/rDL
n
(bn), ∀n ∈N.
(15)
6) Device-side local updating (DU) delay : Let γc represent
the computational load during LoRA’s BP. The BP time of the
server is given by
τ t
CU(l) = ΦB
c (l)/fnCu
nDu
n, ∀n ∈N.
(16)
7) LoRA transmission (LT) latency: Let ΨL denote the size
of device-side LoRA. The latency of uploading LoRA from
device to the server can be represented as
τ t
LT(bn) = ΨL(l)/rUL
n (bn), ∀n ∈N.
(17)
Each round of the total training delay consists of multiple
phases, including transformer block distribution delay, device-
side local computation delay, immediate result transmission


--- Page 7 ---
7
delay, server-side computation delay, gradient transmission
delay, and device-side local updating delay. As shown in
Fig. 4, the overall delay for each fine-tuning round can be
expressed as
τ t
n(β, l, bn, f s
n) =τED + τCC(l) + τIT(β, bn) + τSC(l)
+ τGT(β, bn) + τDU(l, fn) + τLT(l, bn).
(18)
We assume that the set of bandwidths allocated to all devices
is denoted as b. As illustrated in the figure, the delay for each
training round can be expressed as
τt(β, l, b) = max{τ t
n(β, l, bn)}, n ∈N,
(19)
and the total training delay after T rounds is given by
τ(β, l, b) =
T
X
t=1
τt(β, l, b).
(20)
B. Memory Consumption Analysis
The memory consumption in each transformer block can
be divided into four main components: model parameters,
optimizer state, gradients, and activations. Each component’s
memory consumption depends on the number of parameters
and the data type precision, which is represented by a variable
α (e.g., α = 4 for FP32 precision and α = 2 for FP16
precision).
1) Model memory: The memory required to store model
parameters is determined by the parameter number and the
precision. The total memory consumption can be expressed as
Mm = α(12D2 + 18Dr).
(21)
2) Optimizer state: The optimizer states require additional
memory to store updating-related variables such as momentum
and variance terms for each parameter, as well as optional
parameter copies depending on the optimizer type. For exam-
ple: In the adaptive moment estimation (Adam) optimizer, the
memory includes momentum and variance, requiring ˆα = 2α
bytes. If mixed-precision training is used, an additional param-
eter copy is needed, increasing the memory to ˆα = 3α. In the
stochastic gradient descent (SGD) optimizer, the memory only
includes momentum, requiring ˆα = α. The general memory
requirement for optimizer states is therefore expressed as
Mo = ˆα(12D2 + 18Dr).
(22)
3) Gradient memory: The memory required for gradient
storage is proportional to the parameter number and depends
on the precision of the gradients. This can be expressed as:
Mg = α(12D2 + 18Dr).
(23)
4) Activation memory: Activation memory depends on fac-
tors such as model size, recomputation strategies, and paral-
lelization methods. activation memory can be approximated as
Ma = α1BND + α2BN 2A,
(24)
where α and α2 are coefficients that depend on the specific
model architecture and training setup when using the esti-
mation formula from the Megatron scheme α1 = 34 and
α2 = 5 [38].
The total memory consumption in each transformer block
can be expressed as
Mt = Mm + Mo + Mg + Ma
= (2α + ˆα)(12D2 + 18Dr) + α1BND + α2BN 2A.
(25)
The embedding layer’s memory consumption for model
weights and gradients is 4ND, the output activations occupy
4B(N +1)D, and there is additional memory consumption for
local data of 4P 2CD. The out layer’s memory consumption
is 4BND. The memory consumption on the device side is
expressed as
M c(l) = 16D2 + BND + lMt.
(26)
C. Communication Overhead and Computation Workload
We first analyze the parameter size of the SFT scheme. The
number of parameters in each transformer block mainly comes
from three parts: the multi-head self-attention (MSA), feed-
forward network (FFN), and layer normalization (LayerNorm).
In the MSA, weight matrices are independently created for the
query, key, and value, and with the additional LoRA matrix pa-
rameters, the total number of parameters is 4(Dr+rD)+4D2.
The FFN consists of two fully connected layers, where the first
expands the embedding dimension D to Dmlp and the second
reduces it back to D. With LoRA, the total parameter number
is 2(Dr + rDmlp) + 2DDmlp, and given that Dmlp = 4D, the
parameter number simplifies to 8D2 + 10Dr. Thus, the total
number of parameters in each transformer block is approxi-
mately 12D2 +18Dr. Additionally, the number of parameters
in the embedding layer is (P 2C + N + 3)D, which is related
to the number and size of the patches. In the considered
scheme, patches pass through multiple transformer blocks and
are ultimately classified by a classification (CLS) output layer,
which is typically a fully connected layer mapping an input of
dimension D to K classes, resulting in DK + K parameters.
Computation workload: The computational FLOPs of the
device and server are related to the deployed model and model
parameters. In the proposed frame, an embedding layer and l
transformer blocks are deployed on the device side, while the
remaining (L −l) transformer blocks and the classification
layer are deployed on the server side. Each parameter typically
requires 2 to 4 FLOPs in floating-point operations (one multi-
plication and one addition), and for simplicity, we approximate
this as 2 FLOPs per parameter. Then, the FLOPs required
for the device’s FP are given by ΦF
c (l) = l(24BND2 +
4BN 2D) + 2BNDK and the FLOPs required for BP are
ΦB
c (l) = l(48BND2 + 8BN 2D) + 4BNDK. The FLOPs
required for the server’s local forward and back propagation
computations are ΦF
s (l) = (L −l)(24BND2 + 4BN 2D)
and ΦB
s (l) = (L −l)(48BND2 + 8BN 2D) + 4BNDK,
respectively.
Communication overhead: We further analyze the data
transmission requirements for the ED, SC, and GT stages,
which will be discussed in the next subsection. During the
ED stage, the pre-trained model and the initial LoRA model
weights need to be distributed. Assuming each parameter
occupies b bytes, the total data size to be transmitted in the


--- Page 8 ---
8
ED stage is Ψ(l) = Bl(18Dr + 12D2 + (P 2C + N + 3)D).
In the SC stage, the intermediate patch sequence needs to be
transmitted between the transformer blocks on the device and
server sides, with a data size of BND. In the GT stage,
the size of the gradients to be transmitted is equal to the
size of the intermediate activations. During the LT stage, the
size of the LoRA parameters to be transmitted is given by
ΨL(l) = 2lBDr.
VI. PROBLEM FORMULATION
A. Problem Formulation
In the proposed optimization problem, the objective is to
minimize the training delay while ensuring that the accuracy
and memory constraints on devices are satisfied. To achieve
this, we focus on jointly optimizing LLM fine-tuning pa-
rameters, including the compression ratio and the number of
transformer blocks executed locally, as well as efficiently allo-
cating spectrum bandwidth resources among devices. The final
compression ratio β is jointly determined by sparsification,
quantization, and encoding. In this optimization problem, we
consider the tunable parameters ρ and E as the optimization
variables. The joint optimization problem can be expressed as
follows.
P :
min
ρ,E,l,b
τ(ρ, E, l, b)
(27a)
s.t.
A(ρ, E) ≥Ath,
(27b)
Mc(l) < M c
max,
∀l ∈L,
(27c)
ρmin ≤ρ ≤ρmax,
(27d)
Emin ≤E ≤Emax, ,
(27e)
bn ≤bmax
n
,
∀n ∈N,
(27f)
X
n∈N
bn = bmax
s
.
(27g)
Constraint (27b) ensures that the system’s accuracy meets
the minimum requirement, where Ath represents the maxi-
mum accuracy achievable by the LLM within an allowable
accuracy degradation. Constraint (27c) enforces the storage
resource usage on devices to remain within allowable limits.
Constraints (27d) and (27e) bound the sparsity rate ρ and
quantization level E within their respective ranges. Finally,
constraints (27f) and (27g) ensure that both individual band-
width allocations and the total bandwidth allocation comply
with system-wide limitations, maintaining resource balance
and operational feasibility.
B. Problem Decomposition
To efficiently solve the joint optimization problem presented
in Eq. (27), we decompose it into two subproblems by
leveraging the hierarchical structure of the decision variables.
The first subproblem focuses on optimizing the fine-tuning
configuration, including the cut layer l, sparsification rate ρ,
and quantization level E, to minimize the overall fine-tuning
delay while satisfying accuracy and resource constraints. The
second subproblem focuses on optimizing the network band-
width allocations to minimize the fine-tuning delay for a given
fine-tuning configuration.
1) Subproblem 1 - Joint optimization of sparsification rate,
quantization level, and allocated block: The first subproblem
addresses the optimization of the cut layer l, sparsification
rate ρ, and quantization level E, which directly impact the
training process. The goal is to minimize the overall training
delay while ensuring the system accuracy meets the specified
requirements and resource constraints are satisfied. The sub-
problem can be formulated as follows:
P1 :
min
ρ,E,l
τ(ρ, E, l)
(28a)
s.t.
A(ρ, E) ≥Ath,
(28b)
Mc
n(l) < M c
max,
∀n ∈N,
(28c)
ρmin ≤ρ ≤ρmax,
(28d)
Emin ≤E ≤Emax,
(28e)
0 < l < L.
(28f)
2) Subproblem 2 - Optimization of spectrum bandwidth
allocation: The second subproblem focuses on the dynamic
bandwidth allocation b of network resources. These param-
eters are optimized based on the training configuration (l⋆,
ρ⋆, and E⋆) obtained from Subproblem P1. The objective
is to minimize the training delay by effectively allocating
network resources in response to the dynamic environment.
The subproblem can be formulated as follows:
P2 :
min
b max
n∈N
τt(bn)
(29a)
s.t.
bn ≤bmax
n
,
∀n ∈N,
(29b)
X
n∈N
bn = bmax
s
.
(29c)
3) Relationship between subproblems: The two subprob-
lems are decomposed based on large and small timescales and
solved sequentially. First, in the large timescale, P1 is solved
to determine the optimal training configuration, including l⋆,
ρ⋆, and E⋆, as the model structure remains consistent across
training rounds. These results are then used as fixed inputs
in the small timescale to solve P2, where network resource
allocation is optimized based on device-specific heterogeneous
resources and dynamic channel conditions. This decompo-
sition and sequential optimization effectively reduce com-
putational complexity by clearly separating global and real-
time optimization tasks, enabling efficient algorithm design
tailored to system accuracy, resource utilization, and dynamic
performance requirements.
VII. PROPOSED SOLUTION
A. Fine-Tuning Configuration Optimization
To simplify the subproblem P1, we introduce Lagrangian
multipliers λ to relax some of the constraints, forming the
Lagrangian function L(ρ, E, l, λ) as follows:
L(ρ, E, l, λ) = τ(ρ, E, l) + λ1(A(ρ, E) −Ath)+
λ2(Mc
max −Mc(l)),
(30)
where λ = [λ1, λ2] are Lagrangian multipliers that penal-
ize the violation of accuracy and resource constraints, with
λ1, λ2 ≥0. By relaxing these constraints, we convert the


--- Page 9 ---
9
Algorithm 2: Lagrange-based resource management
algorithm.
Input: Discrete variable range l ∈L, initial Lagrange
multipliers λ0, step size µk, tolerance ϵ
Output: Optimal solution (ρ⋆, E⋆, l⋆)
1 Initialize Lagrange multipliers λ = λ0;
2 Set iteration counter k = 0;
3 repeat
4
for each l ∈L do
5
Fix l and optimize the continuous variables ρ
and E by maximizing L(ρ, E, l, λ);
6
Obtain the optimal (ρ⋆
l , E⋆
l ) for the fixed l;
7
Compute L(ρ⋆
l , E⋆
l , l, λ) and store
{ρ⋆
l , E⋆
l , L(ρ⋆
l , E⋆
l , l, λ)};
8
end
9
Select l⋆= arg maxl L(ρ⋆
l , E⋆
l , l, λ) and
corresponding (ρ⋆, E⋆) = (ρ⋆
l⋆, E⋆
l⋆);
10
Update the Lagrange multipliers:
λk+1
i
= λk
i + µk gi(ρ⋆, E⋆, l⋆)
where gi(ρ⋆, E⋆, l⋆) represents the degree of
constraint violation;
11
if |L(ρ⋆, E⋆, l⋆, λ) −Lprev| < ϵ and all constraints
are satisfied then
12
Convergence achieved;
13
break;
14
end
15
Update k = k + 1;
16 until convergence;
original problem into an unconstrained optimization problem
and aim to maximize the relaxed Lagrangian function.
Since l is a discrete variable with a finite range, we
can decompose the problem into subproblems for different
values of l. For each possible value of l, we optimize the
continuous variables ρ and E, and select the combination
that maximizes the objective function. The process can be
expressed as follows:
1) Solving the discrete variable via exhaustive search: For
each l ∈{1, 2, . . . , L}, we can use methods like exhaustive
search to evaluate the optimal solution for each discrete value
of l. The solution process begins by fixing l, which involves
selecting a specific value for l. With l fixed, the next step is
to optimize the continuous variables ρ and E by maximizing
the Lagrangian function L(ρ, E, l, λ). After determining the
optimal ρ and E for the fixed l, we compute and record the
corresponding values of l, ρ, and E along with the Lagrangian
function value. Thus, we can identify the optimal ρ, E, and
the maximum objective function value corresponding to each
possible value of l.
2) Optimizing the continuous variables: For a fixed value
of l, optimizing the continuous variables ρ and E reduces
to a single-variable optimization problem for each. This can
be solved using several methods. If the Lagrangian function
L(ρ, E, l, λ) is differentiable with respect to ρ and E, we
can apply gradient ascent to iteratively update ρ and E in
Algorithm 3: SQP-based spectrum bandwidth alloca-
tion algorithm.
Input: Initial feasible solution (b0, τ ⋆,0), maximum
iterations Kmax, tolerance ϵ
Output: Optimized solution (b⋆, τ ⋆)
1 Initialize k = 0;
2 repeat
3
Compute the objective value τ k
n = τ(bk
n) for all
n ∈N;
4
Compute gradients ∇bτ k
n at the current iterate;
5
Formulate the QP subproblem by linearizing the
constraints as in Eq. (34);
6
Solve the QP subproblem to obtain ∆b and ∆τ ⋆;
7
Update variables:
bk+1 = bk + ∆b, τ ⋆,k+1 = τ ⋆,k + ∆τ ⋆
8
Check convergence criteria: if |τ ⋆,k+1 −τ ⋆,k| ≤ϵ
or ∥∆b∥2 ≤ϵ then
9
Convergence achieved;
10
break;
11
end
12
Update k = k + 1;
13
if k ≥Kmax then
14
break;
15
end
16 until convergence;
the direction that increases L(ρ, E, l, λ), thereby maximizing
the function. Another method is to use convex optimization
techniques, which are particularly efficient if L(ρ, E, l, λ) is
a convex function in ρ and E. By using convex optimization,
we can quickly and reliably solve for the optimal ρ and E
when convexity is present. Additionally, grid search can be
used when the function’s differentiability or convexity is not
guaranteed.
3) Updating the Lagrange multipliers: To ensure that the
relaxed solution satisfies the original problem’s constraints,
we use the subgradient method to iteratively update the La-
grange multipliers. We denote all the inequality constraints
by gp(ρ, E) = A(ρ, E) −Ath ≥0 for q = 1, gp(l) =
M c
max−Mc
n(l) ≥0 for p = 2, . . . , N+1, gp(ρ) = ρ−ρmin ≥0
for p = N + 2, gp(ρ) = ρmax −ρ ≥0 for p = N + 3,
gp(E) = E−Emin ≥0 for p = N+4, gp(E) = Emax−E ≥0
for p = N + 5, gp(l) = L −l ≥0 for p = N + 6, and
gp(l) = l ≥0 for p = N + 7. The update rule is given by
λk+1
i
= λk
i + µk gi(ρ, E, l),
where µk is the step size. During each iteration, the Lagrange
multipliers are adjusted based on the extent of violation of each
constraint, ensuring that the solution progressively satisfies the
constraints as the process converges.
B. Resource Allocation Optimization
Problem P2 optimizes the fine-tuning delay by dynamically
allocating bandwidth b. The objective function τ(b) in P2


--- Page 10 ---
10
is nonlinear, and the constraints include both equality and
inequality conditions. Due to the inherent nonlinearity of the
objective function and the complexity of the constraints, we
adopt the SQP method to solve this problem. The process
involves reformulating the problem to eliminate the nested
max operator, followed by constructing and solving Quadratic
Programming (QP) subproblems at each iteration.
1) Reformulation of the problem: The original objective
function min max τ(bn) involves a nested max operator that
complicates direct optimization. To simplify, we introduce
an auxiliary variable t and replace the max term with an
inequality constraint:
τ ⋆≥τ(bn),
∀n ∈N.
(31)
The optimization problem is reformulated as
P3 :
min
b
τ ⋆
(32a)
s.t.
bn ≤bmax
n
,
∀n ∈N,
(32b)
X
n∈N
bn = bmax
s
(32c)
τ ⋆≥τ(bn),
∀n ∈N.
(32d)
The reformulated problem P3 minimizes the auxiliary variable
τ ⋆subject to linear and nonlinear constraints. The inequality
constraint τ ⋆≥τ(bn) is nonlinear and must be linearized at
each iteration to construct the QP subproblem.
2) Linearization of the nonlinear constraint: To construct
the QP subproblem, we need to linearize the nonlinear in-
equality constraint τ ⋆≥τ(bn) around the current iterate bk.
Using a first-order Taylor expansion, the constraint can be
approximated as:
τ ⋆≥τ(bk
n) + ∇bτ k
n(b −bk),
∀n ∈N,
(33)
where ∇bτ k
n is the gradient of τ(bn) with respect to b
evaluated at bk.
3) Construction of the QP subproblem: At each iteration
k, the QP subproblem is formulated by approximating the
original problem P3 with a quadratic objective function and
linear constraints:
P4 :
min
∆b,∆τ ⋆
τ ⋆
(34a)
s.t.
0 ≤bk
n + ∆bn ≤bmax
n
,
∀n ∈N,
(34b)
X
n∈N
(bk
n + ∆bn) = bmax
s
,
(34c)
τ ⋆,k + ∆τ ⋆≥τ k
n + ∇bτ k
n∆b,
∀n ∈N.
(34d)
where ∆b = b −bk, and ∆τ ⋆= τ ⋆−τ ⋆,k.
4) Solving the QP subproblem: The QP subproblem defined
in (34) is a convex optimization problem with linear con-
straints and a linear objective function, which can be efficiently
solved using standard QP solvers like the interior-point method
or active-set method. The steps to solve the QP subproblem
are:
• Gradient computation: At the current iteration k, compute
the gradient ∇bτ k
n for each device n, which is necessary
for constructing the linearized constraints.
TABLE II: Simulation Parameter
Parameter
Value
Parameter
Value
B
30 MHz
# devices
8
Du
n
4
# iteration Rounds
20
Cu
n
2 × 109
LLM
ViT-base [15]
Cs
1.5 GHz
Cut layer point
5-th ViT block
Ds
2,048
# model parameters
86 M
fs
3 × 109
Optimizer
SGD
Learning rate
1e-4
Momentum
0.9
Decay coefficient
0.998
Batch size
64
Local epoch
1
LoRA rank
16
TABLE III: Memory Comparison on Device Side among
Different Schemes.
Memory
(MB)
FL-Based
FT
FL-
LoRA
SL-Based
FT
Proposed
Input data
36.75
36.75
36.75
36.75
Activation
9171.11
9513.82
3941.23
3941.23
Model+LoRA
327.88
337.03
141.19
141.19
Optimizer (SGD)
329.76
9.09
3.75
3.75
Total
9865.5
(2.39×)
9896.69
(2.4×)
4122.92
(1×)
4122.92
(1×)
• Formulation of the QP problem: Use the computed
gradient and the current iterate (bk, τ ⋆,k) to formulate
the QP subproblem as described in (34). This involves
substituting the linearized constraints into the objective
function and ensuring all resource allocation constraints
are satisfied.
• Solve the QP problem: Employ a standard QP solver (e.g.,
an interior-point method or active-set method) to solve
the convex optimization problem. The solver provides the
optimal increments ∆b and ∆τ ⋆.
• Update variables: Update the current variables using the
obtained increments: bk+1 = bk + ∆b, and τ ⋆,k+1 =
τ ⋆,k + ∆τ ⋆.
• Check convergence: Evaluate the convergence criteria. If
the change in the objective function |τ ⋆,k+1−τ ⋆,k| is less
than a predefined tolerance ϵ, or if the norm of the step
sizes ∥∆b∥2 + |∆τ ⋆| is sufficiently small, the algorithm
terminates. Otherwise, proceed to the next iteration.
• Repeat: Iterate the process from Step 1 until the con-
vergence criteria are met or the maximum number of
iterations Kmax is reached.
VIII. SIMULATION RESULTS
A. Experiment Setting
For the experimental evaluations, we focus on the image
classification task using the CIFAR100 dataset [39], which
contains 50,000 training samples and 10,000 test samples
across 100 classes, and the Tiny-ImageNet dataset [40], which
includes 100,000 training samples and 10,000 validation sam-
ples spanning 200 classes. The details of the parameter and
hyper-parameter settings for model training are presented in
Table II. For the independent and identically distributed (IID)
setting, we randomly partition the dataset into multiple shards
and uniformly assign them to each device. For the non-IID


--- Page 11 ---
11
(a) CIFAR100 IID
(b) CIFAR100 non-IID
(c) Tiny-ImageNet IID
(d) Tiny-ImageNet non-IID
Fig. 5: Fine-tuning performance comparison among different schemes.
setting, we use the Dirichlet distribution with a concentration
parameter of 0.5 for the dataset partition.
In the default experimental setting, the simulated SFT archi-
tecture consists of 1 edge server and 8 devices. We consider a
system bandwidth of 5 MHz, with an SNR of 17 dB between
the devices and the server. The computation frequency of the
devices is randomly generated between 0.5 GHz and 1.5 GHz,
while the server’s computation frequency is set to 40 GHz. For
the large model fine-tuning setup, the batch size is set to 64, the
hidden layer dimension is configured to 3072, and the image
size is set to 224 × 224 with each patch size of 16 × 16. We
adopt the mobile devices are NVIDIA Jetson Nano equipped
with a 256-core GPU and a floating-point operation capacity
of 4. The server is equipped with advanced GPUs featuring
2048 cores and a floating-point operation capacity of 4.
We compare the proposed SFT with the distributed learning
schemes as follows. To ensure fairness, we follow the same
hyper-parameter setting for various baseline schemes.
• FL-Based Fine-Tuning (FT): The traditional FL scheme
proposed by Google in [41]. By incorporating LoRA
tuning into FL, each device transmits only low-rank ma-
trices to the server, significantly reducing communication
overhead.
• SL-Based FT: SL divides a neural network into two
parts, typically split between devices and a central server,
thereby reducing training overhead by offloading compu-
tations and minimizing data transfer and local processing
demands on the device side. [42]
• SFT w/o Compression: The SFT operates without the
joint compression scheme.
B. Performance Evaluation
1) Fine-Tuning Accuracy: Fig. 5 shows the simulation
results of the training process, highlighting several key obser-
vations. First, the proposed SFT scheme demonstrates robust
convergence under both IID (in Fig. 5a and Fig. 5c) and
non-IID conditions (in Fig. 5b and Fig. 5d). Second, the
accuracy achieved by our scheme is comparable to that of
state-of-the-art benchmarks. Referring to the observation in
the dropout layer, the distorted intermediate activations not
only enhance the robustness of model training, but also lead
to slight improvements in accuracy. Similarly, compressing the
intermediate activations is expected to achieve a comparable
accuracy. The results confirm that the efficiency improvements
Fig. 6: Memory consumption on a device with respect to
allocated ViT block.
Fig. 7: Impact of sparsity and bit width on accuracy.
achieved through compression are not at the expense of
training performance.
2) Memory Consumption: Table III presents the peak mem-
ory usage on the device side under different schemes. The
results show that the proposed SFT and SL schemes reduce
memory consumption by 58.2% compared to the FL scheme,
where 5 ViT blocks were allocated on the device side. Addi-
tionally, we evaluate the performance of LoRA applied to the
FL scheme and find that LoRA does not effectively reduce
memory consumption. This is because activations account
for a significant portion of memory consumption, while the
optimizer, which LoRA reduces, contributes only a small
fraction to the total memory consumption. Therefore, when
fine-tuning LLM on the device, it is essential to split the LLM
to address memory constraints on devices. Fig. 6 illustrates
the relationship between the required memory size on the
device and the number of allocated ViT blocks, showing a


--- Page 12 ---
12
(a) Communication overhead in dif-
ferent fine-tuning schemes.
(b) Per-round gains achieved by
different compression schemes.
Fig. 8: Communication overhead in different fine-tuning
schemes and different compression methods.
Fig. 9: Fine-tuning delay performance comparing among dif-
ferent optimization schemes.
proportional increase. The red line in the figure represents the
maximum memory capacity of the NVIDIA Jetson Orin Nano.
It is evident that the FL scheme exceeds this memory limit,
making it unsuitable for deployment on resource-constrained
devices.
3) Communication Overhead: The performance of the pro-
posed compression scheme is shown in Fig. 7. The results
demonstrate that the scheme can compress the transmis-
sion volume of intermediate activations within an allowable
accuracy degradation. However, the compression cannot be
arbitrarily small, as we observe that accuracy starts to drop
sharply when the sparsity reaches 90%. To limit the accuracy
loss within 2%, the sparsity and random quantization methods
can achieve a maximum compression ratio of 12x with 80%
sparsity and 3-bit quantization, which can be further increased
to 20x when combined with lossless encoding.
The impact of different compression schemes on commu-
nication overhead, as well as the communication overhead
of various fine-tuning methods, is illustrated in Fig. 8. As
shown in Fig. 8a, the SL-based FT scheme requires transmit-
ting 144.76 GB of data to complete the fine-tuning process,
which is 6.7 times that of our proposed scheme. Meanwhile,
without compression, our scheme would require 14.75 times
more data transmission, demonstrating the importance and
effectiveness of the proposed compression scheme. In Fig.
8b, we present the gains achieved by different compression
methods on transmission, reducing the final transmitted data
to 6.8% of the original activations, thereby effectively reducing
(a) CIFAR100 IID
(b) CIFAR100 non-IID
(c) Tiny-ImageNet IID
(d) Tiny-ImageNet non-IID
Fig. 10: Delay performance of different fine-tuning schemes.
communication overhead.
4) Fine-Tuning Delay: Figure 9 illustrates the latency per-
formance of our optimization algorithm compared to other
resource allocation methods. We evaluated two baseline algo-
rithms: one where bandwidth resources are evenly distributed
among devices and another where bandwidth resources are
randomly allocated. Experimental results demonstrate that
under various system bandwidth conditions, the proposed
two-stage optimization algorithm consistently outperforms the
baselines. The proposed algorithm maintains its performance
gains even in resource-constrained networks. For instance,
even under a constrained 5 MHz bandwidth, the proposed
algorithm reduces communication delay by up to 53.1% in
each fine-tuning round. This effectiveness is attributed to our
communication compression scheme, which minimizes the
communication volume to a very small scale during the fine-
tuning process.
Figure 10 presents the delay performance results, evaluated
using the time required to reach the highest accuracy of
the FT scheme. The results demonstrate that the proposed
scheme achieves the fastest training speed across various
datasets and under different data distribution conditions. For
example, under the non-IID condition of CIFAR-100, the
proposed algorithm requires only 178.8 minutes to reach the
maximum accuracy. This is 2.34 times faster than FL-baed
FT, which requires 419.6 minutes, and 5.07 times faster than
the non-compressed version of the proposed scheme, which
takes 906.9 minutes. Additionally, the proposed scheme is 6
times faster than the SL scheme, which reaches 88% accuracy
in 1071.1 minutes. The gain is attributed to the proposed
scheme’s integration of serial training and model aggregation,
which ensures high accuracy, while the compression scheme
significantly reduces the overall fine-tuning delay.


--- Page 13 ---
13
IX. CONCLUSION
In this paper, we propose a novel SFT scheme for LLM fine-
tuning in wireless networks. The SFT satisfies devices’ mem-
ory constraints by splitting the LLM, accelerates fine-tuning
with a parallelized SL scheme, and reduces communication
overhead via a joint compression scheme. Furthermore, we
have developed a two-timescale resource management algo-
rithm to minimize the overall fine-tuning delay. Experimental
results demonstrate that the proposed scheme reduces fine-
tuning delay and communication overhead while satisfying
device-side memory and accuracy constraints. This scheme is
well-suited for collaborative LLM fine-tuning across multiple
memory-constrained devices in resource-limited wireless net-
works. In future work, we will investigate the performance of
the proposed scheme in large-scale wireless networks.
REFERENCES
[1] D. Cai, Y. Wu, S. Wang, F. X. Lin, and M. Xu, “Efficient federated
learning for modern NLP,” in Proc. ACM Mobicom, 2023, pp. 1–16.
[2] G. Xu, Z. Hao, Y. Luo, H. Hu, J. An, and S. Mao, “DeViT: Decomposing
vision transformers for collaborative inference in edge devices,” IEEE
Trans. Mobile Comput., vol. 23, no. 5, pp. 5917–5932, 2024.
[3] T. Yao, Y. Li, Y. Pan, Y. Wang, X.-P. Zhang, and T. Mei, “Dual vision
transformer,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 9,
pp. 10 870–10 882, 2023.
[4] W. Kuang, B. Qian, Z. Li, D. Chen, D. Gao, X. Pan, Y. Xie, Y. Li,
B. Ding, and J. Zhou, “Federatedscope-LLM: A comprehensive package
for fine-tuning large language models in federated learning,” in Proc.
ACM SIGKDD, 2024, pp. 5260–5271.
[5] X. Shen, J. Gao, W. Wu, M. Li, C. Zhou, and W. Zhuang, “Holistic
network virtualization and pervasive network intelligence for 6G,” IEEE
Commun. Surveys Tuts., vol. 24, no. 1, pp. 1–30, 2022.
[6] J. Shao, J. Tong, Q. Wu, W. Guo, Z. Li, Z. Lin, and J. Zhang,
“WirelessLLM: Empowering large language models towards wireless
intelligence,” IEEE J. Commun. Netw., vol. 9, no. 2, pp. 99–112, 2024.
[7] O. A. M. Adekanye, “LLM-powered synthetic environments for self-
driving scenarios,” in Proc. AAAI, 2024, pp. 23 721–23 723.
[8] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts
for generation,” in Proc. ACL IJCNLP, 2021, pp. 4582–4597.
[9] X. Shen, J. Gao, W. Wu, K. Lyu, M. Li, W. Zhuang, X. Li, and J. Rao,
“AI-assisted network-slicing based next-generation wireless networks,”
IEEE Open J. Veh. Technol, vol. 1, pp. 45–66, 2020.
[10] A. Li, J. Huang, J. Jia, H. Peng, L. Zhang, L. A. Tuan, H. Yu, and X.-Y.
Li, “Efficient and privacy-preserving feature importance-based vertical
federated learning,” IEEE Trans. Mobile Comput., vol. 23, no. 6, pp.
7238–7255, 2024.
[11] S. Ping, Y. Mao, Y. Liu, X. Zhang, and W. Ding, “FL-TAC: Enhanced
fine-tuning in federated learning via low-rank, task-specific adapter
clustering,” in Proc. ACM ICLR, 2024, pp. 1–6.
[12] W. Wu, C. Zhou, M. Li, H. Wu, H. Zhou, N. Zhang, X. Shen, and
W. Zhuang, “AI-native network slicing for 6G networks,” IEEE Wireless
Commun., vol. 29, no. 1, pp. 96–103, 2022.
[13] H. Wang and W.-Q. Zhang, “Unstructured pruning and low rank factori-
sation of self-supervised pre-trained speech models,” IEEE J. Sel. Topics
Signal Process., pp. 1–14, DOI: 10.1109/JSTSP.2024.3433616, 2024.
[14] X. Huang, W. Wu, S. Hu, M. Li, C. Zhou, and X. Shen, “Digital
twin based user-centric resource management for multicast short video
streaming,” IEEE J. Sel. Topics Signal Process., vol. 18, no. 1, pp. 50–
65, 2024.
[15] A. Dosovitskiy, “An image is worth 16x16 words: Transformers for
image recognition at scale,” arXiv preprint arXiv:2010.11929, 2020.
[16] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,
A. Gesmundo, M. Attariyan, and S. Gelly, “Parameter-efficient transfer
learning for NLP,” in Proc. PMLR, 2019, pp. 2790–2799.
[17] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for
parameter-efficient prompt tuning,” in Proc. ACL EMNLP, 2021, pp.
3045–3059.
[18] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig, “Towards
a unified view of parameter-efficient transfer learning,” in Proc. ACM
ICLR, 2022, pp. 1–15.
[19] D. Han, M. Choi, J. Park, and J. Moon, “FedMes: Speeding up federated
learning with multiple edge servers,” IEEE J. Sel. Areas Commun.,
vol. 39, no. 12, pp. 3870–3885, 2021.
[20] D. Yang, W. Zhang, Q. Ye, C. Zhang, N. Zhang, C. Huang, H. Zhang,
and X. Shen, “DetFed: Dynamic resource scheduling for deterministic
federated learning over time-sensitive networks,” IEEE Transactions on
Mobile Computing, vol. 23, no. 5, pp. 5162–5178, 2024.
[21] T. Guo, S. Guo, J. Wang, X. Tang, and W. Xu, “PromptFL: Let federated
participants cooperatively learn prompts instead of models – federated
learning in age of foundation model,” IEEE Trans. Mobile Comput.,
vol. 23, no. 5, pp. 5179–5194, 2024.
[22] J. Jiang, H. Jiang, Y. Ma, X. Liu, and C. Fan, “Low-parameter federated
learning with large language models,” in Proc. Springer WISA, 2024, pp.
319–330.
[23] D. Cai, Y. Wu, S. Wang, F. X. Lin, and M. Xu, “Efficient federated
learning for modern NLP,” in Proc. ACM MobiCom, 2023, pp. 1–16.
[24] Y.-J. Liu, G. Feng, D. Niyato, S. Qin, J. Zhou, X. Li, and X. Xu, “En-
semble distillation based adaptive quantization for supporting federated
learning in wireless networks,” IEEE Trans. Wireless Commun., vol. 22,
no. 6, pp. 4013–4027, 2023.
[25] R. Greidi and K. Cohen, “Sparse training for federated learning with
regularized error correction,” IEEE J. Sel. Topics Signal Process., pp.
1–16, 2024.
[26] S. Zhang, W. Wu, P. Hu, S. Li, and N. Zhang, “Split federated learning:
Speed up model training in resource-limited wireless networks,” in Proc.
IEEE ICDCS, 2023, pp. 985–986.
[27] W. Wu, M. Li, K. Qu, C. Zhou, X. Shen, W. Zhuang, X. Li, and W. Shi,
“Split learning over wireless networks: Parallel design and resource
management,” IEEE J. Sel. Areas Commun., vol. 41, no. 4, pp. 1051–
1066, 2023.
[28] C. Xu, J. Li, Y. Liu, Y. Ling, and M. Wen, “Accelerating split federated
learning over wireless communication networks,” IEEE Trans. Wireless
Commun., vol. 23, no. 6, pp. 5587–5599, 2024.
[29] Y. Liao, Y. Xu, H. Xu, Z. Yao, L. Wang, and C. Qiao, “Accelerating
federated learning with data and model parallelism in edge computing,”
IEEE/ACM Trans. Netw., vol. 32, no. 1, pp. 904–918, 2024.
[30] Y. Liao, Y. Xu, H. Xu, Z. Yao, L. Huang, and C. Qiao, “ParallelSFL: A
novel split federated learning framework tackling heterogeneity issues,”
in Proc. ACM MobiCom, 2024, pp. 845–860.
[31] H. Wu, X. Chen, and K. Huang, “Device-edge cooperative fine-tuning of
foundation models as a 6G service,” IEEE Wireless Commun., vol. 31,
no. 3, pp. 60–67, 2024.
[32] B. Ouyang, S. Ye, L. Zeng, T. Qian, J. Li, and X. Chen, “Pluto and
Charon: A time and memory efficient collaborative edge AI framework
for personal LLMs fine-tuning,” in Proc. ACM ICPP, 2024, pp. 762–771.
[33] Y. Sun, Z. Li, Y. Li, and B. Ding, “Improving LoRA in privacy-
preserving federated learning,” arXiv preprint arXiv:2403.12313, 2024.
[34] P. Li, G. Cheng, J. Kang, R. Yu, L. Qian, Y. Wu, and D. Niyato, “Fast:
Fidelity-adjustable semantic transmission over heterogeneous wireless
networks,” in Proc. IEEE ICC, 2023, pp. 4689–4694.
[35] F. Sattler, S. Wiedemann, K.-R. M¨uller, and W. Samek, “Robust and
communication-efficient federated learning from non-IID data,” IEEE
Trans. Neural Netw. Learn. Syst., vol. 31, no. 9, pp. 3400–3413, 2019.
[36] S. Golomb, “Run-length encodings (corresp.),” IEEE Trans. Inf. Theory,
vol. 12, no. 3, pp. 399–401, 1966.
[37] X.
Huang,
X.
Qin,
M.
Li,
C.
Huang,
and
X.
Shen,
“Adap-
tive digital twin-assisted 3C management for QoE-driven MSVS:
A GAI-based DRL approach,” IEEE Trans. Cogn. Commun. Netw,
DOI: 10.1109/TCCN.2024.3516046, 2024.
[38] C. Liu and J. Zhao, “Resource allocation for stable LLM training in
mobile edge computing,” in Proc. ACM MobiHoc, 2024, pp. 81–90.
[39] A. Krizhevsky, G. Hinton, and H. Geoffrey, “Learning multiple layers
of features from tiny images,” 2009.
[40] D. Yao, W. Pan, Y. Dai, Y. Wan, X. Ding, C. Yu, H. Jin, Z. Xu, and
L. Sun, “FedGKD: Toward heterogeneous federated learning via global
knowledge distillation,” IEEE Trans. Comput., vol. 73, no. 1, pp. 3–17,
2024.
[41] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in Proc. AAAI, 2017, pp. 1273–1282.
[42] O. Gupta and R. Raskar, “Distributed learning of deep neural network
over multiple agents,” J. Net. Comp. Appl., vol. 116, pp. 1–8, 2018.
