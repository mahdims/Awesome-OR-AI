--- Page 1 ---
Question the Questions:
Auditing Representation in Online Deliberative Processes
Soham De1,2*, Lodewijk Gelauff3*,
Ashish Goel3 â€ , Smitha Milli1 â€ , Ariel Procaccia1,4 â€ , and Alice Siu3 â€ 
1FAIR at Meta
2University of Washington
3Stanford University
4Harvard University
*Equal contribution
â€ Alphabetical order
November 2025
Abstract
A central feature of many deliberative processes, such as citizensâ€™ assemblies and deliberative polls, is
the opportunity for participants to engage directly with experts. While participants are typically invited
to propose questions for expert panels, only a limited number can be selected due to time constraints.
This raises the challenge of how to choose a small set of questions that best represent the interests of all
participants. We introduce an auditing framework for measuring the level of representation provided by a
slate of questions, based on the social choice concept known as justified representation (JR). We present the
first algorithms for auditing JR in the general utility setting, with our most efficient algorithm achieving a
runtime of ğ‘‚(ğ‘šğ‘›log ğ‘›), where ğ‘›is the number of participants and ğ‘šis the number of proposed questions.
We apply our auditing methods to historical deliberations, comparing the representativeness of (a) the
actual questions posed to the expert panel (chosen by a moderator), (b) participantsâ€™ questions chosen
via integer linear programming, (c) summary questions generated by large language models (LLMs). Our
results highlight both the promise and current limitations of LLMs in supporting deliberative processes.
By integrating our methods into an online deliberation platform that has been used for over hundreds
of deliberations across more than 50 countries, we make it easy for practitioners to audit and improve
representation in future deliberations.
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Deliberation participants
Deliberate in small groups
Propose m questions for experts
Moderator or 
algorithm 
selects k << m 
questions
Expert Panel
Our core contribution is an auditing method for representation, grounded in social choice theory, which we integrate into a 
popular online deliberation platform that has been used in >50 countries.
Are these questions representative of all participants interests?
/
1
arXiv:2511.04588v1  [cs.AI]  6 Nov 2025


--- Page 2 ---
1
Introduction
Deliberative processes â€” such as citizensâ€™ assemblies and deliberative polls [Fishkin, 1995] â€” involve ran-
domly selected members of the public engaging in informed discussion in small groups about policy issues.
These processes have gained popularity across the globe as a means to understand the reflective preferences
of a population and guide collective decision-making [OECD, 2020]. This is especially true now that tech-
nology makes it possible to organize them largely online, significantly reducing the cost of assembling a
representative set of participants.
One critical component of many deliberative designs is the opportunity for participants to interact with
experts to deepen their understanding of the topic. For instance, in the deliberative polling approach, each
participant (or small group) is invited to submit a question for an expert panel. However, because these
events often include hundreds of participants, only a limited number of questions can ultimately be posed to
the experts.
This gives rise to an important challenge: how can a small set of questions be chosen to best represent
the interests and concerns of the entire participant panel?
Large language models (LLMs) may offer a
promising approach. In recent years, LLMs have been used to generate consensus statements in caucasus
deliberations [Tessler et al., 2024], create popular summary fact-checks on Community Notes programs
on social media [De et al., 2025], and summarize public opinions on collective response systems such as
Pol.is [Fish et al., 2024, Jigsaw, 2025, Small et al., 2023]. Similarly, LLMs could be leveraged to synthesize
commonalities in participant questions and generate a small slate of final questions. However, given the
often nuanced and sensitive nature of deliberative processes, it is essential to rigorously audit whether such
LLM-generated summary questions actually represent all participants.
Auditing framework for representation. We contribute an auditing framework for measuring the level of
representation provided by a slate of questions. To formalize representation, we utilize a quantitative vari-
ant [Bardal et al., 2025] of a widely-used concept from social choice theory known as justified representation
(JR) [Aziz et al., 2017]. Justified representation is an axiom that requires, informally speaking, that if ğ‘˜
questions are selected, then any group of ğ‘›/ğ‘˜participants (where ğ‘›is the total number of participants) with
shared preferences get at least one slot in the slate. We present the first algorithms for auditing JR in the
general utility setting (beyond approval voting), the most efficient of which achieves an ğ‘‚(ğ‘šğ‘›log ğ‘›) runtime,
where ğ‘šis the number of items (participant-proposed questions) and ğ‘›is the number of participants.
Auditing historical deliberations. We apply our algorithms to audit the representation of LLM-generated
slates of questions in historical deliberations conducted by the Stanford Deliberative Democracy Lab. The
deliberations span topics in democratic reform and generative AI. We compare the actual historical questions
posed to experts (selected by a moderator) to two algorithmic approaches for question selection and gener-
ation. The first is an extractive summarization method, which selects a representative slate exclusively from
participantsâ€™ original questions using an integer program that is designed to provide the optimal selection
from the lens of representation (optimize the JR value). In contrast, the abstractive summarization approach
synthesizes new summary questions from participant input, generating LLM-based questions that may not
have been explicitly proposed.
Our findings indicate that both extractive and abstractive algorithmic methods generally yield question slates
with greater representativeness than those selected by human moderators. In some cases, the abstractive
approach outperforms the extractive method, while in others, the reverse holds true. This suggests that
although LLM-generated summaries can sometimes enhance representativeness by synthesizing informa-
tion, their benefits are not uniform. These results underscore the need for robust auditing mechanisms to
determine when LLM-based methods provide added value over simply selecting from participantsâ€™ original
questions â€” particularly since relying on participantsâ€™ own questions may offer additional benefits in terms
of trust and transparency.
Integration into online deliberation platform. Finally, while we evaluate on historical data, we integrate
both our auditing algorithms and question selection / generation algorithms into an online deliberation
platform [Gelauff et al., 2023] that has been used to conduct hundreds of deliberations in over 50 countries,
enabling practitioners to easily utilize our approach in future deliberations.
2


--- Page 3 ---
2
Related Work
AI & deliberation. The success of large language models has spurred growing interest in applying AI to
deliberative contexts [Landemore, 2022, McKinney, 2024, Small et al., 2023, Korre et al., 2025]. Most
relatedly, McKinney [McKinney, 2024] proposes a framework for assessing the desirability of integrating
AI tools into citizensâ€™ assemblies, focusing on two key dimensions: democratic quality and institutional
capacity. In our work, we address the challenge of selecting (or generating) a set of questions for an expert
panel that most effectively represents participantsâ€™ interests.
Rather than the development of a specific
AI tool, our primary contribution is an auditing method that can be used to assess the representativeness
of different approaches to question selection, whether algorithmic or human-mediated. By applying this
auditing method to different algorithmic strategies, we can identify automated solutions that achieve high
levels of representativeness. This focus directly supports the democratic good of inclusiveness highlighted in
McKinneyâ€™s framework. Additionally, such algorithms advance McKinneyâ€™s institutional goods of efficiency
and scalability, as automating the question selection process eliminates the need for manual review, making
it feasible to scale to a much larger number of participant questions.
As one approach to question selection, we evaluate the representativeness of LLM-generated summary ques-
tions. This is motivated by recent work using LLMs to synthesize the opinions of a collective. For example,
Tessler et al. [Tessler et al., 2024] leverage LLMs to generate consensus statements for caucus deliberation.
De et al. [De et al., 2025] build on this approach to generate summary fact-checks for Community Notes
programs on social media that are preferred over human-written ones. Small et al. [Small et al., 2023]
discuss the opportunities and risks of integrating LLMs into pol.is [Small et al., 2021], a collective response
system [Ovadya, 2023] used to understand public opinion at scale. Google Jigsaw [Jigsaw, 2025] recently
open-sourced a suite of LLM tools aimed at summarizing and identifying common ground in large-scale con-
versations such as those that take place on pol.is. Most relevant to our work, Fish et al. [Fish et al., 2024]
develop a method that uses LLMs to generate summary slates of opinions that satisfy certain social choice
guarantees of representation. In contrast, our focus is on developing an auditing algorithm for representa-
tion, enabling us to evaluate both algorithmic and human-mediated approaches and to quantify the potential
improvements offered by algorithmic solutions.
Social choice theory. Our auditing framework provides a way to measure how representative a slate of
questions is. To formalize representation, we draw upon the social choice concept of justified representation
(JR) [Aziz et al., 2017]. JR is the building block that a family of subsequent axioms is based upon, e.g.,
PJR [SÂ´anchez-FernÂ´andez et al., 2017], EJR [Aziz et al., 2017], EJR+ [Brill and Peters, 2023], and BJR [Fish
et al., 2024]. While JR was originally introduced in the context of approval voting [Aziz et al., 2017], we
adopt a version that accommodates general utility functions. Building on Bardal et al. [Bardal et al., 2025],
we further employ a quantitative formulation of JR, which allows us to measure the degree of representative-
ness of a slate, rather than simply determining whether JR is satisfied. Although verifying JR in the approval
voting setting is straightforward, we develop auditing algorithms tailored to the more general utility setting.
Our focus is on auditing the JR guarantees of arbitrary slates of questions relative to the participantsâ€™ own
questions. By contrast, the work of Fish et al. [Fish et al., 2024] and Boehmer et al. [Boehmer et al., 2025]
aims to generate slates that satisfy BJR over the infinite set of possible statements, but do not address the
problem of auditing or verifying these guarantees. Because we seek to audit slates that may contain arbitrary
questionsâ€”including those not proposed by participants, such as LLM-generated questionsâ€”we need a way
to estimate participantsâ€™ utilities for these questions. To achieve this in a straightforward and interpretable
manner suitable for deliberative settings, we infer a participantâ€™s utility for a given question based on the
distance between the LLM embedding of that question and the embedding of the participantâ€™s own question.
This approach bears similarity to works on proportional clustering [Chen et al., 2019, Micha and Shah, 2020,
Aziz et al., 2024].
3
The Auditing Framework
Suppose there are ğ‘›participants in a deliberation who propose a set ğ‘„ğ‘âŠ†ğ‘„of ğ‘špotential questions for an
expert panel, where ğ‘„denotes the universe of all possible questions. Due to time constraints, however, only
3


--- Page 4 ---
a small set of ğ‘˜â‰ªğ‘šquestions can actually be posed. The challenge, then, is to choose a set of ğ‘˜questions
ğ‘ŠâŠ†ğ‘„that best represents the interests of the overall population. In this section, we outline our framework
for auditing the degree of representation that a slate of questions offers to a population of participants.
In Section 3.1, we introduce the justified representation (JR) axiom [Aziz et al., 2017] from social choice,
along with the quantitative variant we employ [Bardal et al., 2025], which we use to formalize the notion
of representation. In Section 3.2, we explain how we infer the utility that participants derive from arbitrary
questions. In Section 3.3, we present the first known methods for auditing JR in the general utility setting,
the best of which runs in time ğ‘‚(ğ‘šğ‘›log ğ‘›).
3.1
Justified representation
To formalize what it means for a set of questions to be representative, we draw on the social choice concept
of justified representation (JR). Informally, JR is grounded in the principle of proportionality: JR requires that
if ğ‘˜questions are asked, then any group of at least ğ‘›/ğ‘˜participantsâ€”large enough to â€œdeserveâ€ one question
by proportional allocationâ€”who share similar preferences should have at least one question among the ğ‘˜
that represents them.
Justified representation was first introduced by Aziz et al. [Aziz et al., 2017] in the context of approval vot-
ing, where each participant simply indicates a binary approval or disapproval for each of the ğ‘šcandidates.
However, the concept extends naturally to more general utility settings, where each participant ğ‘–has a utility
ğ‘¢ğ‘–(ğ‘) â‰¥0 for a question ğ‘or has a utility ğ‘£ğ‘–(ğ‘Š) â‰¥0 for a slate of questions ğ‘Š. In our setting, we specifically
model a participantâ€™s utility for a slate as being unit-demand, meaning their utility is defined by their best
item on the slate: ğ‘£ğ‘–(ğ‘Š) = maxğ‘âˆˆğ‘Šğ‘¢ğ‘–(ğ‘).1 JR then requires that there can be no coalition of size at least ğ‘›/ğ‘˜
such that the coalitionâ€™s minimum utility for an alternative question is greater than the coalitionâ€™s maximum
utility for the given slate. The formal definition is as follows:
Definition 1 (JR with utilities). A slate of questions ğ‘ŠâŠ†ğ‘„satisfies JR if for every coalition ğ‘†of size at least
ğ‘›/ğ‘˜, there is no alternative question ğ‘âˆˆğ‘„ğ‘such that minğ‘–âˆˆğ‘†ğ‘¢ğ‘–(ğ‘) > maxğ‘–âˆˆğ‘†ğ‘£ğ‘–(ğ‘Š).
Definition 1 departs slightly from the standard definition of JR by allowing the slate ğ‘Što consist of arbitrary
questions from the entire space ğ‘„, rather than being restricted to the discrete set ğ‘„ğ‘of participant-proposed
questions.2 In our setting, we will also audit slates of LLM-generated summary questions that synthesize,
but are distinct from, the participantsâ€™ original questions.
As originally proposed, JR provides only a binary notion of whether a slate is representative or not. However,
we would also like to quantify the degree of representation offered by different question selection methods.
Following Bardal et al. Bardal et al. [2025], we quantify JR by identifying the smallest size group for which
a JR guarantee holds. Specifically, we find the smallest size ğ›¼for which ğ›¼-JR is satisfied, where ğ›¼-JR gener-
alizes JR by providing guarantees to coalitions of size at least ğ›¼Â· ğ‘›/ğ‘˜. When ğ›¼= 1, we recover the original
JR definition, guaranteeing representation only to groups of size at least ğ‘›/ğ‘˜. When ğ›¼< 1, the guarantee
extends to smaller-sized coalitions.
Definition 2 (ğ›¼-JR with utilities). A slate of questions ğ‘ŠâŠ†ğ‘„satisfies ğ›¼-JR if for every coalition ğ‘†of size at
least ğ›¼Â· ğ‘›/ğ‘˜there is no alternative question ğ‘âˆˆğ‘„ğ‘such that minğ‘–âˆˆğ‘†ğ‘¢ğ‘–(ğ‘) > maxğ‘–âˆˆğ‘†ğ‘£ğ‘–(ğ‘Š).
We audit the level of representation that a slate ğ‘Šprovides by calculating the highest value of ğ›¼for which it
satisfies ğ›¼-JR. We refer to this optimal value as the JR value of ğ‘Š: ğ›¼JR(ğ‘Š) = inf{ğ›¼: ğ‘Šsatisfies ğ›¼-JR}.
1Other options are possible, e.g., additive utilities: ğ‘£ğ‘–(ğ‘Š) = Ã
ğ‘âˆˆğ‘Šğ‘¢ğ‘–(ğ‘). However, we observed that additive utilities tend to make JR
trivially satisfied in our context, since utilities are based on cosine similarity between question embeddings (Section 3.2). As a result,
even with relatively small ğ‘˜, it is unlikely that a participantâ€™s utility for any single question would exceed their utility for even a random
slate.
2In this case, satisfying JR means that there is no sufficiently large, cohesive coalition that would prefer to deviate to a participant-
proposed question in ğ‘„ğ‘. However, it does not guarantee that there is no coalition that prefers some other question in the infinite set ğ‘„
of all potential questions [Fish et al., 2024, Boehmer et al., 2025].
4


--- Page 5 ---
3.2
Utility inferences
In order to measure the JR value of an arbitrary slate ğ‘ŠâŠ†ğ‘„, we must be able to infer the utility that a
participant has for any potential question ğ‘âˆˆğ‘„, beyond just the participantsâ€™ own proposed questions. While
complex models for predicting user utility are common in domains like recommender systems [Zhang et al.,
2019], in the context of deliberation, there is usually very limited information available about participants
to support such inferences. Furthermore, transparency is crucial; it must be possible to clearly explain how
utility estimates are derived to both moderators and participants [McKinney, 2024].
Given these constraints, we adopt a simple and interpretable approach: we infer a participantâ€™s utility for a
question based on its similarity to their own proposed question. Specifically, each participant ğ‘–is associated
with their proposed question ğ‘ğ‘–, and we then measure the participantâ€™s utility ğ‘¢ğ‘–(ğ‘â€²) for an alternate question
ğ‘â€² is defined as ğ‘¢ğ‘–(ğ‘â€²) = ğ‘ (ğ‘ğ‘–, ğ‘â€²) where ğ‘ is a similarity measure. In our implementation, we use cosine
similarity between LLM-generated embeddings of the two questions.
0.0
0.2
0.4
0.6
0.8
1.0
False Positive Rate
0.0
0.2
0.4
0.6
0.8
1.0
True Positive Rate
Qwen (AUC = 0.8681)
MiniLM (AUC = 0.8698)
OpenAI (AUC = 0.8756)
Random Classifier
Figure 1: ROC curves comparing the binary classification accuracy of different embedding models on the
Quora Question Pairs (QQP) dataset Iyer et al. [2017]. Each curve is obtained by thresholding the cosine
similarity between the embeddings of paired questions.
Cosine similarity between modern sentence embeddings has been shown to correlate well with human sim-
ilarity judgments [Gao et al., 2021]. To validate our approach in the question context, we evaluate how
effectively cosine similarity can differentiate between duplicate and non-duplicate questions using the Quora
question pairs dataset, which includes human-annotated binary labels (0 for different meanings, 1 for the
same meaning)Iyer et al. [2017]. Figure 1 presents the ROC curves generated by varying the cosine similar-
ity threshold across three different embedding models (Qwen3 0.6B Zhang et al. [2025], all-MiniLM-L6-v2
and text-embedding-3-small (OpenAI)). All embedding models achieve high AUC scores between 0.868 and
0.876. To demonstrate that our auditing approach (see Section 3.3) yields consistent outcomes across differ-
ent embedding models, we apply it to slates identified as most representative by any given embedding model
(using the integer program described in Section 7.1). We find that these slates yield similarly low JR-values
when evaluated with other widely-used embedding models (Figure 2).
3.3
Auditing algorithms
We now introduce our algorithms for auditing the JR value of a slate. To our knowledge, these algorithms are
also the first algorithms for verifying JR in the general utility setting. For binary (approval) utilities, verifying
whether a slate ğ‘Šsatisfies JR is straightforward [Aziz et al., 2017]: one simply checks whether there exists
an alternative ğ‘âˆˆğ‘„ğ‘that is approved by at least ğ‘›/ğ‘˜participants, all of whom disapprove every question in
the slate. In contrast, for general additive utilities, the verification problem is more complex. A JR violation
occurs when there exists an alternative ğ‘âˆˆğ‘„ğ‘, a utility threshold ğœ> 0, and a group of participants ğ‘†of size
5


--- Page 6 ---
MiniLM
OpenAI
Qwen3
0.421 0.421 0.702 1.15
0.561 0.421 0.702 1.23
0.702 0.702 0.421 1.19
(1) America in 1 Room
0.368 0.491 0.614
1.3
0.491 0.368 0.737 1.26
0.737 0.491 0.491
1.3
(2) America in 1 Room
0.444 0.444 0.741 1.17
0.296 0.296 0.593 1.46
0.444 0.444 0.444 1.33
(3) America in 1 Room
0.375
0.5
0.5
1.37
0.5
0.375
0.5
1.24
0.625
0.5
0.375 1.44
(4) America in 1 Room
0.292 0.583 0.583 1.12
0.583 0.438 0.729 1.08
0.292 0.438 0.438 1.02
(5) America in 1 Room
0.42
0.42
0.7
1.13
0.42
0.42
0.42
1.28
0.56
0.42
0.42
1.26
(6) America in 1 Room
MiniLM
OpenAI
Qwen3
Random
MiniLM
OpenAI
Qwen3
0.615 0.615 0.615 1.22
0.492 0.492 0.738 1.25
0.492 0.738 0.492 1.13
(7) America in 1 Room
MiniLM
OpenAI
Qwen3
Random
0.525 0.656 0.656
1.2
0.393 0.525 0.393 1.25
0.525 0.525 0.393 1.28
(8) America in 1 Room
MiniLM
OpenAI
Qwen3
Random
0.263 0.413 0.451 1.34
0.375 0.263 0.451 1.27
0.638 0.526 0.225 1.38
(9) Community Forum 2023
MiniLM
OpenAI
Qwen3
Random
0.298 0.794 0.463 1.22
0.596 0.265 0.596
1.2
0.562 0.529 0.199 1.14
(10) Community Forum 2023
MiniLM
OpenAI
Qwen3
Random
0.195 0.39 0.683 1.09
0.341 0.244 0.439
1.2
0.537 0.634 0.293 1.14
(11) Community Forum 2024
MiniLM
OpenAI
Qwen3
Random
0.286 0.457 0.571 1.26
0.4
0.286 0.514 1.27
0.857
0.4
0.229 1.32
(12) Community Forum 2024
Optimised On
Evaluated On
Figure 2: Cross-validating audit outcomes across embedding models. Each heatmap shows the JR-value
when slates optimized using an integer program on one embedding model are evaluated using another
model. Rows indicate the model used for evaluation, and columns indicate the model used for optimization
(via the IP). Lower JR-values indicate greater consistency between optimization and evaluation models. Any
value below 1 implies that the slate satisfies JR.
at least ğ‘›/ğ‘˜where, for every participant ğ‘–âˆˆğ‘†, their utility for the alternative ğ‘¢ğ‘–(ğ‘) is greater than ğœ, while
their utility for the slate ğ‘£ğ‘–(ğ‘Š) is at most ğœ.
Naive algorithm. At first glance, it may seem necessary to check an infinite set of possible utility thresholds
ğœ. However, it suffices to consider only the thresholds defined by the participantsâ€™ utilities for the slate, i.e.,
the set of ğ‘›thresholds: ğ‘‡= {ğ‘£ğ‘–(ğ‘Š)}ğ‘–âˆˆ[ğ‘›]. For each alternative ğ‘âˆˆğ‘„ğ‘, we can iterate over each threshold
ğœâˆˆğ‘‡to identify the number of participants whose utility for the alternative ğ‘exceeds ğœ, while their utility
for the slate ğ‘Šis at most ğœ. While doing so, we keep track of the largest size of a deviating coalition, which
allows us to find the JR value of the slate. This approach yields the simple ğ‘‚(ğ‘šğ‘›2) algorithm described in
Algorithm 1.
Algorithm 1 Compute ğ›¼JR(ğ‘Š) in ğ‘‚(ğ‘šğ‘›2) time
Require: Slate ğ‘Š, participant questions ğ‘„ğ‘, utility function ğ‘¢ğ‘–: ğ‘„â†’Râ‰¥0 for each participant ğ‘–âˆˆ[ğ‘›]
1: ğ›¼â†0
2: for all ğ‘âˆˆğ‘„ğ‘do
3:
for ğ‘–= 1 to ğ‘›do
4:
ğœâ†ğ‘£ğ‘–(ğ‘Š)
5:
ğ‘â†|{ ğ‘—âˆˆ[ğ‘›] : ğ‘¢ğ‘—(ğ‘) > ğœand ğ‘£ğ‘—(ğ‘Š) â‰¤ğœ}|
6:
ğ›¼â†max(ğ›¼, ğ‘Â· ğ‘˜/ğ‘›)
return ğ›¼
Single-pass algorithm. We now propose an improved algorithm that achieves an ğ‘‚(ğ‘šğ‘›log ğ‘›) runtime (Al-
gorithm 2). The previous approach (Algorithm 1) iterates over each alternative question ğ‘âˆˆğ‘„ğ‘and, for
each possible utility threshold ğœâˆˆğ‘‡, determines the largest coalition of participants who would prefer ğ‘
to ğ‘Š. This results in a higher computational cost, as each of the ğ‘›thresholds is considered separately. In
contrast, Algorithm 2 exploits the observation that, for each question ğ‘, the largest blocking coalition can be
identified in a single pass over the sorted utility thresholds.
First, the participants are sorted in non-increasing order according to their utility for the given slate ğ‘Š,
resulting in the ordering ğ›¾1, . . . , ğ›¾ğ‘›âˆˆ[ğ‘›] where ğ‘£ğ›¾1(ğ‘Š) â‰¥Â· Â· Â· â‰¥ğ‘£ğ›¾ğ‘›(ğ‘Š).
For each alternative question
ğ‘âˆˆğ‘„ğ‘, the participants are also sorted in non-increasing order of their utility for ğ‘, resulting in the or-
dering ğ›¿1, . . . , ğ›¿ğ‘›âˆˆ[ğ‘›] where ğ‘¢ğ›¿1 (ğ‘) â‰¥Â· Â· Â· â‰¥ğ‘¢ğ›¿ğ‘›(ğ‘). The algorithm then iterates through the sorted utilities
{ğ‘¢ğ›¿ğ‘–(ğ‘)}ğ‘›
ğ‘–=1, maintaining a counter ğ‘ğ‘for the size of the largest coalition that prefers the question ğ‘to the
slate ğ‘Š. Here, rather than checking each utility threshold {ğ‘£ğ›¾ğ‘¡(ğ‘Š)}ğ‘›
ğ‘¡=1 separately, we iterate over them in
one pass, starting with ğ‘¡= 1. Each participantâ€™s utility ğ‘¢ğ›¿ğ‘–(ğ‘) for ğ‘is compared to the current threshold
6


--- Page 7 ---
ğ‘£ğ›¾ğ‘¡(ğ‘Š). If ğ‘¢ğ›¿ğ‘–(ğ‘) > ğ‘£ğ›¾ğ‘¡(ğ‘Š), then the participant is added to the coalition, and the counter is incremented.
If ğ‘¢ğ›¿ğ‘–(ğ‘) â‰¤ğ‘£ğ›¾ğ‘¡(ğ‘Š), then this means that the coalition size cannot be increased without lowering the thresh-
old. Thus, participant ğ›¾ğ‘¡is blacklisted from future coalitions (and removed from the existing coalition, if
applicable), and the utility threshold is decreased by incrementing ğ‘¡.
This process continues until all participants have been considered or all thresholds have been exhausted. The
maximum coalition size ğ‘encountered during this process for any question ğ‘is recorded, and the algorithm
returns ğ›¼JR(ğ‘Š) =
ğ‘
ğ‘›/ğ‘˜. By leveraging sorted orderings and a single-pass approach, this algorithm achieves
a significant improvement in efficiency, enabling the exhaustive verification of JR for a large number of
candidate slates.
Algorithm 2 Compute ğ›¼JR(ğ‘Š) in ğ‘‚(ğ‘šğ‘›log ğ‘›) time
Require: Slate ğ‘Š, participant questions ğ‘„ğ‘, utility function ğ‘¢ğ‘–: ğ‘„â†’Râ‰¥0 for each participant ğ‘–âˆˆ[ğ‘›]
1: ğ›¾1, . . . , ğ›¾ğ‘›â†Participants sorted such that ğ‘£ğ›¾1(ğ‘Š) â‰¥Â· Â· Â· â‰¥ğ‘£ğ›¾ğ‘›(ğ‘Š)
2: ğ‘â†0
3: for all ğ‘âˆˆğ‘„ğ‘do
4:
ğ›¿1, . . . , ğ›¿ğ‘›â†Participants sorted s.t. ğ‘¢ğ›¿1(ğ‘) â‰¥Â· Â· Â· â‰¥ğ‘¢ğ›¿ğ‘›(ğ‘)
5:
ğ‘ğ‘â†0
âŠ²Size of deviating coalition for question ğ‘
6:
ğ‘†, ğµâ†âˆ…, âˆ…
âŠ²Current coalition and blacklisted participants
7:
ğ‘¡â†0
8:
for ğ‘–= 1 to ğ‘›do
9:
while ğ‘¡< ğ‘›and ğ‘¢ğ‘–(ğ‘¥) â‰¤ğ‘£ğ›¾ğ‘¡(ğ‘Š) do
10:
if ğ›¾ğ‘¡âˆˆğ‘†then
11:
ğ‘ğ‘â†ğ‘ğ‘âˆ’1
12:
ğµâ†ğµâˆª{ğ›¾ğ‘¡}
13:
ğ‘¡â†ğ‘¡+ 1
14:
if ğ‘¡â‰¥ğ‘›then
15:
Break
16:
if ğ‘–âˆ‰ğµthen
17:
ğ‘†â†ğ‘†âˆª{ğ‘–}
18:
ğ‘ğ‘â†ğ‘ğ‘+ 1
19:
ğ‘â†max(ğ‘ğ‘, ğ‘)
return ğ‘Â· ğ‘˜/ğ‘›
4
Empirical Evaluation
In the following sections, we first describe the platform and deliberative polls (Section 4.1) that provide the
data used for our audit. Then, we introduce the relevant baselines and present the key results from applying
our audit to this data (Section 4.2).
4.1
Deliberative Polls and the Online Deliberation Platform
We evaluate our audit on data from 12 sessions in 3 different deliberative polls [Fishkin, 1995, Luskin et al.,
2002], conducted by the Stanford Deliberative Democracy Lab, that cover a wide range of topics â€” from the
American political system to AI agents. During such a deliberative poll, participants are assigned to small
groups with on average 10 participants; each group has a conversation, at the end of which they propose and
rank questions for a plenary panel of experts. The plenary panel then is presented a slate of 5â€“15 of these
questions that helps clarify the topic for the participants. Moderators may also occasionally group multiple
very-similar â€˜siblingâ€™ 3 questions together and pose them to the panel. It is this set of selected questions that
3In human-generated slates with a set of sibling questions, we consider all possible slates that can be formed by taking one sibling
question as the representative of its sibling set and report the average JR value for the Human baseline in Table 1.
7


--- Page 8 ---
Figure 3: Screenshots illustrating our approach implemented in the online deliberation platform.
The
moderator can generate LLM summary questions (referred to as â€œSuperQuestionsâ€ in the interface) from
participant-proposed questions; view which participant-proposed questions are most similar to each LLM-
generated one; and export all data including, similarity scores, for representation auditing.
we use to evaluate our algorithms. In Section 4.2, we evaluate the JR value of the slates of question selected
by moderators, compared to the slates of questions generated by algorithmic approaches.
We consider datasets based on sessions from 3 deliberations: â€œAmerica in One Room: Democratic Reformâ€
(June 2023), the â€œMeta Community Forum on AI chatbotsâ€ (two sessions from October 2023), and the â€œMeta
Community Forum on AI agentsâ€ (two sessions from October 2024).
â€¢ The America in One Room (A1R) deliberations convened U.S. nationally representative sample of par-
ticipants to discuss potential reforms to the American political system, such as changes to the electoral
college and the adoption of ranked-choice voting [Fishkin and Diamond, 2023]. This results in 8 sets
of questions and corresponding slates.
â€¢ The 2023 Meta Community Forum (CFâ€™23) convened nationally representative samples of participants
from Brazil, Germany, Spain and the United States to discuss policies for user interactions of AI chatbots
in their respective national languages in small groups. In two sessions of small groups, the participants
proposed questions, followed by an international plenary panel of experts [Chang et al., 2024]. This
results in 2 sets of questions and corresponding slates.
â€¢ The 2024 Meta Community Forum (CFâ€™24) similarly convened a nationally representative sample of
participants from Brazil, India, Nigeria, Saudi Arabia, and South Africa that discussed policies and
conditions for AI agents [Chang et al., 2025]. This results in 2 sets of questions and corresponding
slates.
All deliberations were conducted on an online, self-moderating deliberation platform that has been shown to
perform on par with human moderators across multiple dimensions [Gelauff et al., 2023]. We implemented
our auditing algorithms, as well as algorithmic summarization using LLMs, within this same online delib-
eration platform (Figure 3). The goal of the implementation is to provide an easy way for moderators to
generate and evaluate different potential slate of questions to pose to an expert panel. For each generated
slate, the platform computes the JR value ğ›¼JR(ğ‘Š) and displays a heatmap illustrating the similarity between
each selected question and those submitted by participants. This feature enables moderators, when present-
ing a question, to point out which participants submitted similar questions and to explicitly highlight how
participantsâ€™ own contributions informed the final questions that were posed.
8


--- Page 9 ---
Panel Human
Random
IP
LLM
LLMbest
N
k
A1R
0.842
1.227 Â± 0.077 0.421 0.778 Â± 0.032
0.421
57
8
A1R
0.798â€  1.258 Â± 0.083 0.368 0.57 Â± 0.023
0.368
57
7
A1R
1.037
1.459 Â± 0.099 0.296 0.913 Â± 0.052
0.444
54
8
A1R
0.875â€  1.241 Â± 0.096 0.375 0.701 Â± 0.039
0.375
56
7
A1R
1.021
1.084 Â± 0.064 0.438 1.018 Â± 0.044
0.583
48
7
A1R
1.68â€ 
1.281 Â± 0.091 0.42 0.545 Â± 0.022
0.28
50
7
A1R
0.738
1.247 Â± 0.076 0.492 0.751 Â± 0.041
0.369
65
8
A1R
1.049
1.246 Â± 0.079 0.525 0.744 Â± 0.032
0.393
61
8
CFâ€™23 2.234â€  1.267 Â± 0.076 0.263 1.052 Â± 0.051
0.563
293 11
CFâ€™23 0.827â€  1.197 Â± 0.075 0.265 1.05 Â± 0.043
0.562
272 9
CFâ€™24
1.268
1.203 Â± 0.067 0.244 0.948 Â± 0.042
0.537
164 8
CFâ€™24
0.8
1.267 Â± 0.074 0.286 0.785 Â± 0.056
0.4
175 10
Table
1:
Levels
of
representation
(ğ›¼JR)
for
different
slates
using
embeddings
from
the
text-embedding-3-small model by OpenAI. Baselines (Random and LLM-generated slates) reflect
mean JR values over 100 runs, with 95% confidence intervals. In sessions marked with â€ , human modera-
tors posed multiple sibling questions.
4.2
Evaluating algorithms for question selection and generation
A representative slate of ğ‘˜questions that summarizes all the ğ‘›questions proposed by participants in a
deliberation can emerge via two distinct pathways. First, we may select a representative subset of ğ‘˜questions
from the larger set of all proposed questions. We call the resulting slate an extractive summarization as it is
formed by directly extracting from the set of all questions proposed by participants. Alternatively, we can
generate ğ‘˜new questions that aim to summarize and highlight the important aspects of proposed questions
but may not exist verbatim in the original set of ğ‘›questions. Since these questions are formed via synthesis
or abstraction, we call this slate an abstractive summarization.
We test the slates generated through five methods, the first three of which are extractive summarization
methods, and the last two of which are abstractive.
1. Random. A baseline that picks from participant-proposed questions uniformly at random
2. Human. The historical questions posed to the expert panel, that were selected by the moderator.
3. IP. Following Theorem 2 in Bardal et al. Bardal et al. [2025], we observe that it is NP-hard to determine
existence of a slate satisfying ğ›¼-JR for a given ğ›¼. Hence, instead of a polynomial-time algorithm we
implement a polynomial-size integer program, as described in Section 7.1, to determine the subset of
participant-proposed questions that minimizes the JR value. In Table 1, we run our integer program
using Gurobi and with all possible thresholds discretized at intervals of 0.01.
4. LLM. We prompt gpt-4o (see Appendix 7) with all participant-proposed questions to generate ğ‘˜repre-
sentative questions. We sample multiple times while shuffling the order of questions with a tempera-
ture of one, resulting in 100 generated slates.
5. LLMğ‘ğ‘’ğ‘ ğ‘¡. We pick the LLM-generated slate with the highest JR value, i.e., best-of-n sampling.
Table 1 presents results of running our audit on different question slates generated on data from 8 panels
during the â€œAmerica in One Room: Democratic Reformâ€ (A1R) deliberation, 2 panels during the â€œ2023 Meta
Community Forum on AI Chatbotsâ€ (CFâ€™23) deliberation and 2 panels during the â€œ2024 Meta Community
Forum on AI Agentsâ€ (CFâ€™24) deliberation.
âŠ²Historical performance by human moderators
These results reveal that slates selected by human mod-
erators are, on average, more representative than a set of ğ‘˜randomly selected questions with a few key
exceptions (such as in the first Metaâ€™23 deliberation). However, we also note that in half of the panels on
9


--- Page 10 ---
Type
ğ›¼JR
Proposed Question Slates (A1R - Session 1)
IP
0.421
1. Rank choice voting seems like a good idea. Can you please discuss two pros and two cons to RCV.
2. Is there anything that prevents using RCV (Ranked Choice Voting) in party primaries? IE Federal
Law, Funding Issues, Party Bylaws?
3. How can we change the system to benefit all Americans in their voting right?
4. Do you think proportional representation would help mitigate the problem of Gerrymandering?
5. Out of each of the proposals, which is considered the most cost-effective way to improve voting?
6. What is the best way for a third party (or multiple parties) to gain a foothold in the current two-
party system? Which of these proposals can make that happen?
7. What does the political science research tell us about the difficulty of teaching voters how to operate
in new systems, such as RCV, proportional representation, or changing to non-partisan primaries? How
is Proportional Representation counted?
8. How successful is rcv in places where it has been used?
Human 0.842
1. What is the approximate cost to update voting machines to RCV?
2. what are examples of where rank choice voting has implemented and how well do people like it?
3. How do you best balance an rcv election where some rank and some dont rank and is that a fair
and equal vote?
4. How can we trust the PR system to actually vote for the people? How will the people be educated
on how it works?
5. In a proportional representation system at the congressional level, what would be some examples of
implementation? In a multi- member district with 3 members, would all three members confer before
voting as one block, or all vote with 1/3 of a vote?
6. The group in general seems highly against Gerrymandering. What can the experts tell us about the
effects of any of these proposals to eliminate or reduce gerrymandering?
7. What is the best way for a third party (or multiple parties) to gain a foothold in the current two-
party system? Which of these proposals can make that happen?
8. How soon would any changes to elections be implemented? Is this something we would do for the
next Presidential election or are we 5-10 years away from these changes going into effect?
LLMbest 0.421
1. What are the most cost-effective ways to implement Ranked Choice Voting?
2. How can we address voter education challenges for new systems like RCV and proportional repre-
sentation?
3. Which electoral reforms could most effectively reduce gerrymandering and partisanship?
4. How successful has RCV been in areas where it has been implemented, and what feedback has it
received?
5. How would proportional representation impact party systems and mitigate political polarization?
6. What steps are needed to ensure fair enforcement of new voting systems and prevent political
corruption?
7. Would implementing RCV require significant updates to voting machines, and what are the costs?
8. How can third-party and independent candidates gain traction in a two-party-dominated system?
Table 2: Questions in extractive and abstractive slates from the first panel in the A1R deliberation, as evalu-
ated in Table 1
which we run our audit, human-moderator-selected slates do not satisfy JR, indicating a potential to improve
the deliberative polling process by using more representative slates.
âŠ²Abstractive vs. extractive slates
The slates selected by the integer program are the most representative
extractive slate, which by definition must always have a ğ›¼JR â‰¤1.
Similarly, the LLMbest slates are the
most representative abstractive slates out of 100 LLM-generated candidates â€” an approximation of the
most representative abstractive slate. We find that all algorithmic approaches to slate generation result
in more representative slates when compared to random and human-selected slates that were historically
used in the deliberative polls. Moreover, we find that abstractive slates are often able to match or surpass
the representativeness of the best extractive slates, especially for sessions with fewer questions proposed.
We also note that in sessions with more participants, abstractive slates are much more feasible to generate
10


--- Page 11 ---
in real time using LLMs, whereas finding the best extractive slate might be computationally intractable at
interactive speeds. Relatedly, as the example in Table 2 reveals, abstractive question slates generated using
LLMs typically appear more stylistically coherent and well composed compared to extractive slates, even
when they achieve equivalent levels of representativeness.
Our results highlight the promise and limitations of using LLMs for generating slates in large online de-
liberative process. While LLM-generated abstractive slates consistently outperform existing human-driven
processes, they do not uniformly surpass extractive slates selected by an integer program across all panels.
This variability underscores the value of our efficient auditing algorithm, which enables a hybrid approach
that leverages the strengths of both abstractive and extractive methods.
5
Conclusion
In this work, we address the challenge of selecting questions that are representative of all participantsâ€™ in-
terests in large-scale deliberative processes. We present two complementary approaches towards solving
this task â€” extractive (selecting from existing questions) and abstractive (generating new questions) â€” and
develop a framework to evaluate their representativeness. To compare these approaches, we introduce an au-
diting framework grounded in the social choice concept of justified representation (JR). We present the first
computationally efficient algorithms for auditing JR in general utility settings. Our framework uses widely
available text-embedding models to infer participant utilities based on semantic similarity, enabling scalable
application across diverse deliberative contexts. This semantics-based approach provides interpretable utility
measurements and avoids potential biases that may arise from black-box predictive models. Our framework
ensures that every participant who proposes a question during a deliberation is appropriately represented by
the final slate of questions proposed to expert panelists.
Applying our methods to several historical deliberations retroactively, we demonstrate that algorithmically
selected or generated questions have the potential to enhance representativeness, compared to those chosen
by human moderators. This also highlights the utility of our auditing approach, which enables moderators
to compare several algorithmically generated slates and evaluate them for representativeness. Finally, we
integrate our framework into a widely-used online deliberation platform that has supported hundreds of
deliberations in over 50 countries, making our methods readily accessible to improve representation in
future deliberations.
Limitations and future work. Our work is not without limitations and we identify several promising di-
rections for future research. First, our evaluation in Section 4 considers the JR value for LLM-generated
and IP-selected questions separately; future work could explore hybrid approaches that select from both
participant-proposed and LLM-generated questions. Second, we infer a participantâ€™s utility for a question
using the cosine similarity between the embeddings of that question and the participantâ€™s proposed ques-
tion. We note that our retrospective analysis of historical slates limits our ability to validate utility inferences
with data from actual participants involved in the deliberation. While we evaluate our embedding-based ap-
proach on existing datasets with human similarity judgments (see Figure 1) and across different embedding
models (Figure 2), future studies could directly validate these utility inferences with participants themselves
during an ongoing deliberation. Finally, although our focus is on auditing the JR value, future work could
explore auditing other axioms offering stronger representation guarantees such as BJR [Fish et al., 2024,
Boehmer et al., 2025].
6
Acknowledgment
The authors would like to thank Harshvardhan Agarwal, Zixin Xu, and the other developers for their con-
tributions to the Stanford Online Deliberation Platform, which was instrumental in data collection. Special
thanks are extended to Harshvardhan Agarwal for implementing the algorithm. The authors also acknowl-
edge Dhruv Gupta and Advay Ranade for their assistance with data collection for the human benchmarks.
Finally, the authors are grateful to Audrey Tang for encouraging the exploration of this research direction.
This work was supported in part by NSF Award #2333849.
11


--- Page 12 ---
References
Haris Aziz, Markus Brill, Vincent Conitzer, Edith Elkind, Rupert Freeman, and Toby Walsh. Justified repre-
sentation in approval-based committee voting. Social Choice and Welfare, 48(2):461â€“485, 2017.
Haris Aziz, Barton E. Lee, Sean Morota Chu, and Jeremy Vollen. Proportionally representative clustering. In
Proceedings of the 20th Conference on Web and Internet Economics (WINE 2024), 2024. https://arxiv.
org/abs/2304.13917.
Tuva Bardal, Markus Brill, David McCune, and Jannik Peters. Proportional representation in practice: quan-
tifying proportionality in ordinal elections. In Proceedings of the Thirty-Ninth AAAI Conference on Artificial
Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth
Symposium on Educational Advances in Artificial Intelligence, AAAIâ€™25/IAAIâ€™25/EAAIâ€™25. AAAI Press, 2025.
ISBN 978-1-57735-897-8. doi: 10.1609/aaai.v39i13.33483. https://doi.org/10.1609/aaai.v39i13.
33483.
Niclas Boehmer, Sara Fish, and Ariel D. Procaccia.
Generative Social Choice: The Next Generation.
In
Forty-second International Conference on Machine Learning, 2025. https://openreview.net/forum?id=
E1E6T7KHlR.
Markus Brill and Jannik Peters. Robust and verifiable proportionality axioms for multiwinner voting. In
Proceedings of the 24th ACM Conference on Economics and Computation, EC â€™23, page 301, New York,
NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701047. doi: 10.1145/3580507.
3597785. https://doi.org/10.1145/3580507.3597785.
Samuel Chang, Estelle Ciesla, Michael Finch, James Fishkin, Lodewijk Gelauff, Ashish Goel, Ricky Hernan-
dez Marquez, Shoaib Mohammed, and Alice Siu. Meta community forum: Results analysis, april 2024.
Technical report, Meta and Stanford Deliberative Democracy Lab and Crowdsourced Democracy Team,
Stanford University, April 2024. Corresponding author: Alice Siu, asiu@stanford.edu.
Samuel Chang, James S. Fishkin, Ricky Hernandez Marquez, Ayushi Kadakia, Alice Siu, and Robert Taylor.
Meta community forum: Results analysis, april 2025. Technical report, Meta and Stanford Deliberative
Democracy Lab, April 2025.
Xingyu Chen, Brandon Fain, Liang Lyu, and Kamesh Munagala. Proportionally fair clustering. In International
Conference on Machine Learning, pages 1032â€“1041. PMLR, 2019.
Soham De, Michiel A. Bakker, Jay Baxter, and Martin Saveski. Supernotes: Driving consensus in crowd-
sourced fact-checking. In Proceedings of the ACM on Web Conference 2025, WWW â€™25, page 3751â€“3761,
New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400712746. doi: 10.1145/
3696410.3714934. https://doi.org/10.1145/3696410.3714934.
Sara Fish, Paul GÂ¨olz, David C. Parkes, Ariel D. Procaccia, Gili Rusak, Itai Shapira, and Manuel WÂ¨uthrich.
Generative Social Choice. In Proceedings of the 25th ACM Conference on Economics and Computation, EC
â€™24, page 985, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400707049.
doi: 10.1145/3670865.3673547. https://doi.org/10.1145/3670865.3673547.
James Fishkin and Larry Diamond.
Can deliberation cure our divisions about democracy?
Boston
Globe, (August 21, 2023), August 2023.
https://www.bostonglobe.com/2023/08/21/opinion/
2024-elections-partisanship-democracy-common-ground/.
James S. Fishkin. The Voice of the People: Public Opinion and Democracy. Yale University Press, 1995. ISBN
9780300065565. http://www.jstor.org/stable/j.ctt32bgmt.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple Contrastive Learning of Sentence Embeddings.
In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings
of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894â€“6910, Online
and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi:
10.18653/v1/2021.emnlp-main.552. https://aclanthology.org/2021.emnlp-main.552/.
12


--- Page 13 ---
Lodewijk Gelauff, Liubov Nikolenko, Sukolsak Sakshuwong, James Fishkin, Ashish Goel, Kamesh Munagala,
and Alice Siu. Achieving parity with human moderators: A self-moderating platform for online delibera-
tion 1. In The Routledge handbook of collective intelligence for democracy and governance, pages 202â€“221.
Routledge, 2023.
Shankar Iyer, Nikhil Dandekar, and KornÂ´el Csernai.
Quora question pairs.
https://www.kaggle.com/
competitions/quora-question-pairs, 2017. Accessed: 2025-10-06.
Zhihao Jiang and Ashish Goel. Approximation algorithms for optimization problems with justified represen-
tation constraints. Personal Communication; authors omitted for double blind review, 2025.
Jigsaw.
How
one
of
the
fastest-growing
cities
in
Kentucky
used
AI
to
plan
for
the
next
25
Years,
Jun
2025.
https://medium.com/jigsaw/
how-one-of-the-fastest-growing-cities-in-kentucky-used-ai-to-plan-for-the-next-25-years-3b70c4fd1412
Katerina Korre, Dimitris Tsirmpas, Nikos Gkoumas, Emma CabalÂ´e, Danai Myrtzani, Theodoros Evgeniou, Ion
Androutsopoulos, and John Pavlopoulos. Evaluation and facilitation of online discussions in the llm era:
A survey. arXiv preprint arXiv:2503.01513, 2025.
Helene Landemore. Can AI bring deliberative democracy to the masses? In HAI Weekly Seminar, Working
Paper, pages 166â€“191, 2022.
Robert C. Luskin, James S. Fishkin, and Roger Jowell. Considered opinions: Deliberative polling in britain.
British Journal of Political Science, 32(3):455â€“487, 2002. ISSN 00071234, 14692112. http://www.jstor.
org/stable/4092249.
Sammy McKinney. Integrating artificial intelligence into citizensâ€™ assemblies: Benefits, concerns and future
pathways. Journal of Deliberative Democracy, 20(1), 2024.
Evi Micha and Nisarg Shah. Proportionally Fair Clustering Revisited. In 47th International Colloquium on
Automata, Languages, and Programming (ICALP 2020), pages 85â€“1. Schloss Dagstuhlâ€“Leibniz-Zentrum fÂ¨ur
Informatik, 2020.
OECD. Innovative Citizen Participation and New Democratic Institutions: Catching the Deliberative Wave. OECD
Publishing, Paris, 2020. doi: 10.1787/339306da-en. https://doi.org/10.1787/339306da-en.
Aviv Ovadya. â€™Generative CIâ€™ through Collective Response Systems, 2023. https://arxiv.org/abs/2302.
00672.
Luis SÂ´anchez-FernÂ´andez, Edith Elkind, Martin Lackner, Norberto FernÂ´andez, JesÂ´us Fisteus, Pablo Basanta Val,
and Piotr Skowron. Proportional justified representation. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 31, 2017.
Christopher Small, Michael Bjorkegren, Timo ErkkilÂ¨a, Lynette Shaw, and Colin Megill. Polis: Scaling Deliber-
ation by Mapping High Dimensional Opinion Spaces. Recerca. Revista de Pensament i An`alisi, 26(2):1â€“26,
2021. doi: 0.6035/recerca.5516.
Christopher T Small, Ivan Vendrov, Esin Durmus, Hadjar Homaei, Elizabeth Barry, Julien Cornebise, Ted
Suzman, Deep Ganguli, and Colin Megill. Opportunities and Risks of LLMs for Scalable Deliberation with
Polis. arXiv preprint arXiv:2306.11932, 2023.
Michael Henry Tessler, Michiel A. Bakker, Daniel Jarrett, Hannah Sheahan, Martin J. Chadwick, Raphael
Koster, Georgina Evans, Lucy Campbell-Gillingham, Tantum Collins, David C. Parkes, Matthew Botvinick,
and Christopher Summerfield.
Ai can help humans find common ground in democratic deliberation.
Science, 386(6719):eadq2852, 2024. doi: 10.1126/science.adq2852. https://www.science.org/doi/
abs/10.1126/science.adq2852.
Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. Deep Learning Based Recommender System: A Survey and
New Perspectives. ACM Comput. Surv., 52(1), February 2019. ISSN 0360-0300. doi: 10.1145/3285029.
https://doi.org/10.1145/3285029.
13


--- Page 14 ---
Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang,
Dayiheng Liu, Junyang Lin, et al. Qwen3 embedding: Advancing text embedding and reranking through
foundation models. arXiv preprint arXiv:2506.05176, 2025.
14


--- Page 15 ---
7
Prompts
Below is the prompt we use for generating LLM summary questions.
Given these questions : \n
{questions} \n
Generate {k} specific concise questions that exhaustively summarize these given questions
as much as possible. Avoid long and generic high-level questions. Retain the same style,
specificity
and length as the given questions. Return only the questions, one per line,
without any additional text.
7.1
Integer Program: Minimizing Largest Dissatisfied Coherent Set
Using the standard notation in social choice, we use the term â€œcandidatesâ€ to refer to potential questions,
and the term â€œvoterâ€ to refer to each question in the original data set. In our case, the set of candidates
and voters is the same. We use similarity as the notion of utility, and assume that it is given to us (e.g.
via embeddings). We will use ğ‘to refer to the number of voters, and ğ‘€to the number of candidates for
generality, even though these are the same for our use case.
The integer program is modeled after a similar integer program Jiang and Goel [2025] used to devise
approximation algorithms for justified representation problems. We use binary integer variables to indicate
whether a particular candidate is chosen. The central idea is to use auxiliary binary integer variables to keep
track, for each utility level, whether a voter has at least that much utility given the set of chosen questions.
This allows us to test, for each potential blocking question that is not chosen, whether that question would
result in a blocking set of voters, that is, a set of voters whose minimum utility for the blocking question
exceeds what all of them are currently receiving. We minimize the size of the largest blocking set, which
allows us to optimize for the over-representation factor ğ›½that we are auditing for.
Given
â€¢ ğ¾: Committee size
â€¢ ğ‘¢(ğ‘£, ğ‘): Utility of voter ğ‘£from candidate ğ‘(cosine similarity)
Indices used
â€¢ ğ‘: Candidates
â€¢ ğ‘£: Voters
â€¢ ğ‘ : Satisfaction level, ranges over all possible satisfaction levels (at most ğ‘ğ‘€)
â€¢ ğ‘â€²: Blocking candidate
Variables
â€¢ ğ½: Size of largest dissatisfied coherent set; integer variable
â€¢ ğ‘¥ğ‘: Indicator variable, 1 if candidate ğ‘is in the committee; binary integer variable
â€¢ ğ‘¦ğ‘£,ğ‘ : Indicator variable, 1 if voter ğ‘£has utility at least ğ‘ ; binary integer variable
15


--- Page 16 ---
Integer Program
Minimize
ğ½
subject to
âˆ‘ï¸
ğ‘
ğ‘¥ğ‘= ğ¾
ğ‘¦ğ‘£,ğ‘ â‰¤
âˆ‘ï¸
ğ‘:ğ‘¢(ğ‘£,ğ‘)â‰¥ğ‘ 
ğ‘¥ğ‘
âˆ€ğ‘£, ğ‘ 
âˆ‘ï¸
ğ‘£:ğ‘¢(ğ‘£,ğ‘â€²)â‰¥ğ‘ 
(1 âˆ’ğ‘¦ğ‘£,ğ‘ ) â‰¤ğ½
âˆ€ğ‘â€², ğ‘ 
ğ‘¥ğ‘âˆˆ{0, 1}
âˆ€ğ‘
ğ‘¦ğ‘£,ğ‘ âˆˆ{0, 1}
âˆ€ğ‘£, ğ‘ 
ğ½âˆˆZ; ğ½â‰¥0
This is a large integer program, with ğ‘‚(ğ‘2ğ‘€) integer variables (since there can be up to ğ‘ğ‘€distinct utility
levels). To obtain a more efficient program, we simply relax the ğ‘¦variables to be fractional between 0 and
1, inclusive. At optimality, if a specific ğ‘¦ğ‘£,ğ‘ is strictly fractional, it can simply be set to 1 without violating any
constraints (since ğ‘¥is still required to be binary). This gives us a much more efficient integer program, with
only ğ‘‚(ğ‘€) integer variables:
More Efficient Integer Program
Minimize
ğ½
subject to
âˆ‘ï¸
ğ‘
ğ‘¥ğ‘= ğ¾
ğ‘¦ğ‘£,ğ‘ â‰¤
âˆ‘ï¸
ğ‘:ğ‘¢(ğ‘£,ğ‘)â‰¥ğ‘ 
ğ‘¥ğ‘
âˆ€ğ‘£, ğ‘ 
âˆ‘ï¸
ğ‘£:ğ‘¢(ğ‘£,ğ‘â€²)â‰¥ğ‘ 
(1 âˆ’ğ‘¦ğ‘£,ğ‘ ) â‰¤ğ½
âˆ€ğ‘â€², ğ‘ 
ğ‘¥ğ‘âˆˆ{0, 1}
âˆ€ğ‘
0 â‰¤ğ‘¦ğ‘£,ğ‘ â‰¤1
âˆ€ğ‘£, ğ‘ 
ğ½âˆˆZ; ğ½â‰¥0
Note that ğ½can also be similarly relaxed, but in practice we found that explicitly requiring ğ½to be integral
actually allows commercial solvers to solve the problem faster, since the solver can stop when the difference
between an upper bound and a lower bound is less than 1.
16
