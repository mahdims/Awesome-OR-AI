--- Page 1 ---
arXiv:2505.07956v1  [cs.LG]  12 May 2025
Symbolic Regression with
Multimodal Large Language Models
and Kolmogorov–Arnold Networks
Thomas R. Harveya,1, Fabian Ruehleb,c,a,2, Kit Fraser-Taliented,3,
James Halversonb,a,4
a NSF AI Institute for Fundamental Interactions, MIT, Cambridge, MA 02139, USA
b Department of Physics, Northeastern University, Boston, MA 02115, USA
c Department of Mathematics, Northeastern University, Boston, MA 02115, USA
d Rudolf Peierls Centre for Theoretical Physics, University of Oxford, Oxford OX1
2JD, UK
Abstract
We present a novel approach to symbolic regression using vision-capable large
language models (LLMs) and the ideas behind Google DeepMind’s Funsearch.
The LLM is given a plot of a univariate function and tasked with proposing
an ansatz for that function. The free parameters of the ansatz are fitted using
standard numerical optimisers, and a collection of such ans¨atze make up the
population of a genetic algorithm. Unlike other symbolic regression techniques,
our method does not require the specification of a set of functions to be used
in regression, but with appropriate prompt engineering, we can arbitrarily con-
dition the generative step. By using Kolmogorov–Arnold Networks (KANs), we
demonstrate that “univariate is all you need” for symbolic regression, and extend
this method to multivariate functions by learning the univariate function on each
edge of a trained KAN. The combined expression is then simplified by further
processing with a language model.
1trharvey@mit.edu
2f.ruehle@northeastern.edu
3cristofero.fraser-taliente@physics.ox.ac.uk
4j.halverson@northeastern.edu
1


--- Page 2 ---
Contents
1
Introduction
2
2
Symbolic Regression with LLMs
3
2.1
A simple example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
2.2
Extending to a genetic algorithm and Funsearch
. . . . . . . . . . . .
5
2.3
Comparison to traditional methods and benchmarking
. . . . . . . . .
7
2.4
Using open models . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.5
Adding noise
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.6
Special functions and prompt engineering . . . . . . . . . . . . . . . . .
16
3
Combining with Kolmogorov-Arnold Networks
17
3.1
Univariate is all you need . . . . . . . . . . . . . . . . . . . . . . . . . .
17
3.2
Multivariate Examples . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
3.3
Improving Univariate Examples . . . . . . . . . . . . . . . . . . . . . .
22
4
Conclusion
22
1
Introduction
Symbolic regression is a long-standing and inherently challenging problem in the fields
of machine learning and applied mathematics. The task involves searching for mathe-
matical expressions that best describe a given dataset, yet the space of possible func-
tions is unyielding. To navigate this expansive search space effectively, modern sym-
bolic regression algorithms often rely on explicit or implicit complexity measures to
mitigate expression “bloat” and encourage simpler expressions [1–3].
Humans exhibit an impressive intuitive ability to infer plausible functional forms from
visual representations of data, typically guided by a natural simplicity bias. When
presented with a graph of a function, expert human observers tend to prefer and
propose simpler ans¨atze that often outperform those generated by automated symbolic
regression methods.
2


--- Page 3 ---
This observation naturally raises the question: can large multimodal language models
(LLMs) emulate this human-like intuition? In this work, we show that they can—at
least in the context of univariate functions—especially when integrated with Deep-
Mind’s FunSearch [4]. To that end, we introduce LLM-LEx (Large Language Models
Learning Expressions), a package available at our GitHub repository [5], which lever-
ages commercially available LLMs for guiding symbolic regression.
By invoking the Kolmogorov–Arnold representation theorem, which tells us that multi-
variate functions can be represented as sums and compositions of univariate functions,
we moreover argue that focussing on univariate cases is sufficient for a broad class of
problems. We train Kolmogorov–Arnold Networks (KANs) on the data for which we
aim to find a symbolic expression. Then, we employ LLM-LEx to propose symbolic
expressions for the edge functions within these networks. The combined expression
is then simplified using an additional step of generative modelling using LLMs. We
refer to this combined method as KAN-LEx, and it is likewise publicly available at our
repository.
We emphasise that we do not expect beyond state-of-the-art performance from this
method of symbolic regression. For one, many traditional and freely available pack-
ages have been highly optimised through years of development. Our aim is simply to
demonstrate that this new approach to symbolic regression is viable and surprisingly
successful given the simplicity of its implementation. Our initial implementation com-
prised approximately 100 lines of code. The combination of our method with KANs
allows application to multivariate functions, although in principle any symbolic regres-
sion method can be used for this step.
Unless stated otherwise, we use gpt-4o via OpenRouter throuhout.
2
Symbolic Regression with LLMs
In this section, we begin with a simple example of using a vision-capable language
model for symbolic regression with a one-shot prompt. We then extend this approach
by incorporating a genetic algorithm. We compare to traditional techniques and sub-
sequently benchmark against a set of randomly generated functions. We also consider
the use of open-source language models as an alternative to proprietary ones.
We
then consider the effects of noisy data, before concluding with a discussion on prompt
engineering and learning special functions.
3


--- Page 4 ---
Figure 1: An example function to learn
2.1
A simple example
Consider the graph in Figure 1, generated by interpolating 500 evenly spaced points
between −1.2 and 1.2. It is not too difficult for a human to make a reasonable ansatz
for the underlying function. In particular, the oscillatory behaviour and intersection
at zero suggest the function may take the form y(x) = g(x) sin(ax), where g(x) is a
smooth function and a is a constant. The way the function flattens out toward zero
then suggests the presence of exponential decay. Putting these together, a suitable,
and correct, ansatz for such a function would be
y(x) = ce−bx2 sin(ax),
(1)
which after fitting to the data yields a = 2, b = 10 and c = 1. Despite the apparent ease
with which a human can make an educated guess for the function, developing software
that replicates this kind of intuition is highly challenging. This, in essence, captures
the core difficulty of symbolic regression. For example, when we input the 100 data
points into a traditional symbolic regression algorithm—specifically, Mathematica’s
4


--- Page 5 ---
FindFormula1 function [6]—we find
f(x) = −13.7072x19. + 104.411x17. −344.585x15. + 647.044x13. −763.814x11.
+ 591.433x9. −303.867x7. + 101.756x5. −20.6594x3. + 1.98844x.
(2)
While (2) gives a good fit to the data, for the given range, the resulting expression is
difficult to interpret and offers little additional insight into the underlying structure of
the problem: we might just as well have fitted the data with a high-degree polynomial
from the outset.
We turn instead to a multimodal language model (in this example, gpt-4o): we will
see language models display some of the intuitive symbolic regression ability exhibited
by humans, most likely acquired from their extensive pretraining data. We present the
LLM with the image in Figure 1, accompanied by the following prompt:
“An initial ansatz for this function is curve = lambda x, params: params[0]*x +
params[1]. Give an improved ansatz for the image. params can be any length”
The response from the LLM will, sometimes2, contain the Python lambda function:
curve = lambda x ,
params :
np . sin ( params [ 0 ] ∗x)∗np . exp(−params [ 1 ] ∗x∗∗2)
Remarkably, this is exactly the functional form we were aiming for. We propose adopt-
ing this as the foundation of a new methodology for symbolic regression. As the range
is increased, and the exponential decay becomes clearer, the LLM predicts the correct
function more frequently, whilst Mathematica resorts to suggesting the function is zero.
2.2
Extending to a genetic algorithm and Funsearch
Despite the success demonstrated in the example above, language models struggle to
produce satisfactory results when presented with more complex functions. To address
this limitation, we propose enhancing the methodology by incorporating a genetic
algorithm [7]. The idea of using a genetic algorithm to generate prompts for a language
1As it is proprietary, very little public information is available about how FindFormula operates.
However, it is believed to involve a heuristic search over symbolic expressions, guided by both the
complexity and accuracy of the functions. Such methodology is typical of other approaches to symbolic
regression.
2As the response from the LLM is probabilistic, it does not always return exactly the correct
function. We address this in the next subsection by incorporating a genetic algorithm, where this
actually becomes a strength. The probabilistic nature of the LLM’s response can be thought of as
mutation; when given access to LLM sampling parameters, we gain some parametric control over this
mutation.
5


--- Page 6 ---
LLM
Prompt
Ansatz
Fit Params
Score
Figure 2: The structure of LLM-LEx.
model has been previously explored in the context of Funsearch [4,8,9]. We suggest
integrating this approach into the framework introduced in the previous section. The
general structure of our approach is indicated in Fig. 2.
We begin with a population of proposed functions, all initially defined as the constant
function.
lambda x ,∗params :
params [ 0 ] .
Each function in the population is evaluated using the following scoring metric:
Score(fθ) = 1
N
N
X
i=1
|fθ(xi) −yi|2
max(α|yi|, global-scale)2,
(3)
where (xi, yi) are the data points, α is a hyperparameter (default value: 0.01), and
f is the candidate function. The parameters θ are optimised using SciPy’s optimisa-
tion routines. The value of global-scale is a non-vanishing characteristic scale for the
function. It is defined as:
global-scale = max({MADy, α · mean({|yi|}, ϵ)}),
MADy = median({yi −median({yj})})
(4)
where ϵ ≪1 is a small hyperparameter which we introduce to avoid division by zero.
The intuition behind this scoring metric is that, unlike mean squared error (MSE)
(which favours fitting regions with large values) this score emphasises capturing the
overall shape of the function.
The resulting scores are then normalised so that the maximum score in the population
is unity; these normalised scores are denoted as {si | i = 1, . . . , N}.
To construct the prompts for generating the next population of functions, two examples
are randomly selected from the previous generation at least N times (some may fail
to parse, and so are redrawn). The selection is based on a probability distribution
6


--- Page 7 ---
derived from the normalised scores, which are passed through a softmax function with
temperature T. Specifically, the probability of choosing the i-th function is given by:
Pi =
esi/T
P
j esj/T ,
(5)
where, unless stated otherwise, T = 1.
The two chosen functions are then used to construct the user prompt:
import numpy as np
curve 0 = lambda x ,∗params:< First Random Function>
curve 1 = lambda x ,∗params:<Second Random Function>
curve 2 = lambda x ,∗params :
along with the image to which we wish to fit. The system prompt is set as follows:
“You are a symbolic regression expert. Analyze the data in the image and provide an
improved mathematical ansatz. Respond with ONLY the ansatz formula, without any
explanation or commentary. Ensure it is in valid Python. You may use numpy
functions. params is a list of parameters that can be of any length or complexity.”
The function for the new population can then be extracted from the LLM’s response.
If the response fails to parse correctly, a new pair of functions is selected from the
previous population. The genetic algorithm terminates early if any individual in the
population exceeds a specified score threshold (by default, 10−5).
Whilst not discussed here, standard genetic algorithm techniques, such as ‘island mod-
els’ and ‘elitism’, can be straightforwardly incorporated into the process described
above. This algorithm is implemented in a GitHub repository [5]. From this point for-
ward, we will refer to this algorithm as LLM-LEx (Large Language Models Learning
Expressions).
2.3
Comparison to traditional methods and benchmarking
Many traditional methods for symbolic regression exist, typically involving the breeding
and mutation of expression trees via genetic algorithms [1,2,10,11]. The specificities—
functions considered, as well as the techniques used to suppress the dominance of
overly complex functions (a problem often referred to as “bloat”)—vary depending on
the implementation. A notable exception to this is “exhaustive symbolic regression,”
where an exhaustive search is conducted over expressions up to a predefined maximal
complexity [3]. As an example, we compare Mathematica’s FindFormula function to
our method [6].
7


--- Page 8 ---
Whilst the use of LLMs for symbolic regression is not entirely new [12–15], existing
methods either rely solely on raw data or use the LLM as an assistant (with both data
and visual input) within a more traditional symbolic regression process. In contrast,
our method exclusively uses the image as input, without any raw data. The model
then infers relationships and patterns purely from visual information.
One key distinction between LLM-LEx (along with some of the other approaches using
language models) and traditional methods is that LLM-LEx does not require the user
to specify a list of basis functions. Instead, the language model selects appropriate
functions based on those it has encountered during training. This approach mirrors how
humans perform symbolic regression, with the added advantage that it is particularly
easy to condition the generative steps by simply providing additional context to the
prompted model (in, for example, the system prompt).
A comparison of the two methods can be found in Tables 1 and 2 for a set of randomly
generated functions. Not only does LLM-LEx find the exact expression more frequently
than Mathematica, but in all but one instance where Mathematica achieves a higher
score, it returned a polynomial, which provides little additional insight into the nature
of the function. Plots of some of the functions found by LLM-LEx are shown in Fig. 3.
Interestingly, even when the functions score poorly, they often appear aesthetically
correct when plotted. The algorithm also seemed unaffected by the aliasing visible in
the function cos (ex) + 4.67315. The functions used for benchmarking were generated
from random symbolic trees in Mathematica. The code to generate these functions is
available on the GitHub repository, in the file named generate functions.nb.
The primary downside of LLM-LEx over traditional methods is the cost of inference,
and the latency of LLM calls. We hope that both of these factors will improve as the
technology evolves. Currently, a population of 25 individuals over 10 generations takes
about 10 minutes (although this can be muched improved using asynchronous API calls,
a feature of LLM-LEx) and costs approximately $0.50 when using the latest version
of gpt-4o. The algorithm can terminate early if it reaches the required exit condition
on its score; we naturally observe this in some cases. Additionally, the method can be
made more cost-effective, typically at the expensive of some quality, by running one
of the many available open-source models locally. We explore this option in the next
section.
8


--- Page 9 ---
(a) Target Function:
p
| sin(x)|
(b) Target Function: ecos(x) −0.0126997
(c) Target Function: sin
 log
  4.1746
x

(d) Target Function: cos (ex) + 4.67315
Figure 3: Four example functions found by LLM-LEx with various scores. The func-
tions returned by the algorithm and their scores are given in Table 1 and Table 2,
respectively. The number of points used to produce the plots are the same that we
used to provide the graph of the function to LLM-LEx, which leads to the aliasing
effect in the highly oscillatory part of the last function.
9


--- Page 10 ---
Expression
Mathematica Result
LLM-LEx Result
# LLM-LEx runs
p
| sin(x)|
0.786005
0.752| sin(x)| + 0.302
2
e1.83169−3.35509
x
P(x9)
−2.586 tanh(1.159x) + 3.73 log(1.096 + x)
−0.046x1.568 −0.371
2
x3
✓
✓
1
(√x + 1.44439) (log(x) + π)
−0.253251 −2.97827 × 2.1889−9.73907x
+7.92679√x + 33.2638 × 3.24104−14.192xx
0.757√x + 2.010 + 4.027x0.631
+1.252 log(x −0.0017) + 2.341
2
3.09529x3
✓
✓
1
(x3 + π)2
✓
✓
2
51.2288 cos(1.18219x)
P(x12)
✓
1
−55.0512 (√x + 1.)
✓
✓
1
x
✓
✓
1
ecos(x) −0.0126997
P(x11)
−0.115 sin(2.221x) + 0.307 cos(1.809x)
+0.486x2 −3.307x + 5.885
−3.505 exp(−1.075x)
2
1.54251 −x
✓
✓
1
e2x
✓
✓
1
4.01209 + ex
P(x12)
✓
1
0.729202√x −π
P(x10)
✓
1
−3x3 + x + 1.99594
✓
✓
1
log(x + 1)
P(x7)
✓
1
sin
 log
  4.1746
x

126.517x4.00974 −178.8x3.68092
−0.00171611x9. + 0.0512601x8. −0.583991x7.
+3.1446x6. −11.7513x5. + 527.55x
+0.232186 log(x) −516.318 sin(x)
+64.7972 cos(x) −65.6805
1.299e−0.182x sin(0.391x + 1.518)
−2.812e−4.401x
2
cos (ex) + 4.67315
4.54681
✓
1
2e−3x + e−x
P(x11)
5.796
(x+4.673)9.427 −4.611e−1.809x −7.187 × 10−4
2
x+4.11509
x3
4.22225/x3
0.778
(x+0.009)2.143 −0.489e−0.282x +
0.422
x−0.056
2
Table 1: P(xn) indicates a polynomial of degree n, where the target function was not a polynomial, and ✓indicates the
exact expression (to within numerical error of the coefficients). All runs with LLM-LEx had a population size of 25 and
ran for 10 generations. The initial run had a terminal threshold of 10−7 with elitism, while the second had 10−10 without
elitism. These runs can be found in the Jupyter notebook fit functions.ipynb in the examples folder of the GitHub
repository [5]. Little improvement is achieved with repeated iterations with Mathematica. In all cases the data were given
by 100 evenly spaced points with x between 0.1 and 5. The scores from both methods are given in Table 2. Plots of four
examples can be found in Figure 3.
10


--- Page 11 ---
Expression
Mathematica Score
LLM-LEx Score
p
| sin(x)|
−0.87*
−0.091
e1.83169−3.35509
x
−0.36*
−10−7
x3
0
0
(√x + 1.44439) (log(x) + π)
−10−6
−10−11
3.09529x3
0
0
(x3 + π)2
0
0
51.2288 cos(1.18219x)
−10−13*
0
−55.0512 (√x + 1.)
0
0
x
0
0
ecos(x) −0.0126997
−10−8*
−10−5
1.54251 −x
0
0
e2x
0
0
4.01209 + ex
−10−16*
0
0.729202√x −π
−10−8*
0
−3x3 + x + 1.99594
0
0
log(x + 1)
−10−7*
0
sin
 log
  4.1746
x

−10−5
−10−4
cos (ex) + 4.67315
−0.02*
0
2e−3x + e−x
−10−7*
−10−7
x+4.11509
x3
−0.1
−10−4
Table 2: Score comparison (less negative is better) for the functions in Table 1. Scores
with a star indicate the cases where the traditional method returned a constant or
high order polynomial (while the target expression was not), rather than a more inter-
pretable representation. Plots of four examples can be found in Figure 3.
2.4
Using open models
In this section, we aim to demonstrate that LLM-LEx can be used locally, even with
modest resources, by employing open-source language models.
Specifically, we benchmark several open-source models against gpt-4o, by fitting the
20 functions listed in Table 1. All models are executed locally using ollama 0.4.7,
with the following model configurations:
Model Name
# Params
Size
llama3.3
70B
43 GB
mistral
7B
4.1 GB
codestral
22B
13 GB
11


--- Page 12 ---
More specifically, we use llama3.3:70b-instruct-q4 K M, the original codestral,
and mistral v0.3. All experiments were conducted on a 2024 M4 Max MacBook Pro,
using a population size of 25 and evolving for 10 generations. The complete suite of 20
functions required approximately 4 hours (codestral, mistral), 7 hours (llama3.3),
and 22 minutes (gpt-4o). In each LLM-LEx example, a single run was performed
in asynchronous mode. The average scores and runtimes across all functions were as
follows:
Model
Avg Scores
Avg Time (s)
gpt-4o
−5.81 × 10−3 ± 2.51 × 10−2
29.27 ± 65.42
llama3.3
−2.05 × 10−2 ± 8.95 × 10−2
536.25 ± 2922.17
codestral
−2.63 × 10−2 ± 1.13 × 10−1
202.46 ± 768.27
mistral
−2.79 × 10−2 ± 1.20 × 10−1
187.10 ± 702.53
As expected, gpt-4o is the fastest, benefiting from execution on OpenAI hardware,
while llama (the largest local model) is the slowest; codestral and mistral exhibit
comparable performance.
However, the data is influenced by difficult outliers that
introduce high variance, and all median scores are approximately −10−17.
Function-by-function computation times and scores are shown in Figure 5. These plots
highlight the substantial speed advantage of gpt-4o; nonetheless, in many cases, the
open-source models are sufficiently fast — for example, codestral completes many
tasks in under 100 seconds.
Table 3 presents the results indicating whether each model produced the correct expres-
sion. For this table, we first required a score greater than −10−15 for an expression to be
considered for correctness evaluation. The remaining expressions were then manually
inspected, allowing the application of trigonometric identities in verifying correctness.
From the table, it is evident that codestral performs very well, achieving results
comparable to gpt-4o.
Higher resolution for harder examples. In the previous analysis of open-source models,
one of the more challenging cases was the function f(x) = 4.67315 + cos(exp(x)),
where the corresponding image was generated using 100 evenly spaced x-values in the
interval(0.1, 5). For larger x, the function oscillates rapidly, and the sparse sampling
results in interpolation artefacts due to aliasing errors. The impact of this aliasing is
illustrated in Figure 4 and is clearly substantial. Given the severity of the aliasing, it
is remarkable that LLM-LEx with gpt-4o was able to fit the function.
To assess the significance of the aliasing effect, we refitted LLM-LEx using a denser
sampling of points, as shown on the left-hand side of the figure. In this case, mistral
achieved a perfect fit, whereas the other models failed. As gpt-4o executes considerably
faster, we repeated the same experiment four additional times; it succeeded in three
12


--- Page 13 ---
0
1
2
3
4
5
3.75
4.00
4.25
4.50
4.75
5.00
5.25
5.50
5.75
Dense Sampling (10000 points)
0
1
2
3
4
5
3.75
4.00
4.25
4.50
4.75
5.00
5.25
5.50
5.75
Sparse Sampling (100 points)
Figure 4: Dense vs. Sparse sampling of x values for f(x) = 4.67315 + cos(exp(x)).
out of four runs, yielding a 4/5 total success rate. Given the probabilistic nature of
language models at non-zero temperature, this variability is unsurprising.
2.5
Adding noise
Thus far, LLM-LEx has been tested exclusively on clean data, free from noise. How-
ever, such conditions are rarely encountered in scientific domains. We therefore briefly
explore how LLM-LEx responds to the introduction of noise.
As examples, we consider four of the functions that LLM-LEx successfully identified
in the absence of noise in the previous section, namely:
x3,
51.2288 cos(1.18219x),
x,
4.01209 + ex
We add noise ξ, where
y = ytrue + ξ,
(6)
ξ ∼N(0, ϵ |ymax|),
(7)
and N(µ, σ) is a normal distribution with mean µ and standard deviation σ.
When introducing noise into our curve fitting algorithm, we must naturally increase the
acceptable error tolerance (the exit condition), as the model should not be expected to
match the ground truth as closely as with noise-free data. Without this adjustment, we
observed that LLM-LEx tends to identify functions that, while visually approximating
the ground truth well, on closer inspection instead overfit the noise.
By contrast,
the single-generation (single-shot) ansatz, which involves fewer parameters, offers a
reasonably accurate approximation of the true function. The success of the single-shot
13


--- Page 14 ---
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
Function Index
101
102
103
104
Compute Time (s)
Compute Times by Model and Function
Model
codestral
gpt_4o
llama3.3:70b-instruct-q4_K_M
mistral
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
Function Index
10
29
10
25
10
21
10
17
10
13
10
9
10
5
10
1
Abs(Score)
Best Scores by Model and Function
Model
codestral
gpt_4o
llama3.3:70b-instruct-q4_K_M
mistral
Figure 5: Compute times and scores of LLM-LEx across different LLMs. Below the
dashed red line, the genetic algorithm is stopped early due to high performance.
approach may indicate that the LLM, in a sense, recognises it is working with noisy
data and prioritises fitting the underlying ground truth rather than the noise. Indeed,
we can simply request that it attempt to do this.
Nevertheless, as the noise level
increases, the probability of recovering the exact ground truth function diminishes.
Let us now present two illustrative examples. In both cases, the x-values consist of
100 points uniformly spaced between 0.1 and 5.0.
We considered noise levels ϵ ∈
{0.01, 0.03} and set the exit condition to 0.04.
In the first example, we fit the function f(x) = 51.2288 cos(1.18219x).
LLM-LEx
returned the correct expression for ϵ = 0 and 0.01, but for ϵ = 0.03, it fit the data
to an exponential function, indicating that the noise had become too large for reliable
recovery of the true form.
14


--- Page 15 ---
Expression
Mathematica
gpt-4o
llama3.3
mistral
codestral
p
| sin(x)|
e1.83169−3.35509
x
✓
x3
✓
✓
✓
✓
✓
(√x + 1.44439) (log(x) + π)
3.09529x3
✓
✓
✓
✓
✓
(x3 + π)2
✓
51.2288 cos(1.18219x)
✓
✓
✓
−55.0512 (√x + 1.)
✓
✓
✓
✓
x
✓
✓
✓
✓
✓
ecos(x) −0.0126997
1.54251 −x
✓
✓
✓
✓
✓
e2x
✓
✓
✓
✓
✓
4.01209 + ex
✓
✓
✓
0.729202√x −π
✓
✓
−3x3 + x + 1.99594
✓
✓
✓
log(x + 1)
✓
✓
✓
sin
 log
  4.1746
x

cos (ex) + 4.67315
✓
2e−3x + e−x
✓
✓
x+4.11509
x3
Total Correct
8
12
8
8
12
Table 3: Comparison of Mathematica results with four different LLMs utilised by LLM-
LEx. The best results utilised LLM-LEx with gpt-4o or codestral. All runs with
LLM-LEx were single shot.
In the second example, we fit the simple linear function f(x) = x. Again, LLM-LEx
returned the correct result for ϵ = 0 and 0.01. However, for ϵ = 0.03, it produced the
expression
2.17 x −11.2 sin(0.109x) + 0.053.
(8)
While this initially appears incorrect, a Taylor expansion of the low-frequency sine
term yields
0.053 + 0.952x + 0.00243x3 −1.45 × 10−6x5 + O(x6).
(9)
Examining these terms over the domain, it is evident that the dominant contribution
beyond the constant is 0.952x, with the higher-order terms negligible and on the order
of the noise. In effect, despite the seemingly more complex ansatz, the model is still
approximating a linear function.
This suggests that LLM-LEx, in the presence of
moderate noise, tends to prioritise capturing the underlying trend of the true function,
even if the explicit form deviates from the exact ground truth.
15


--- Page 16 ---
2.6
Special functions and prompt engineering
One potential advantage of LLM-LEx over traditional symbolic regression is that it does
not require the user to specify a predefined basis of functions. When provided with
access to appropriate Python packages, LLM-LEx can even propose special functions
within its suggested ans¨atze.
As an example, we can modify the prompt (the change to the default prompt is un-
derlined here for emphasis) to:
Prompt 1: “You are a symbolic regression expert. Analyze the data in the image and
provide an improved mathematical ansatz. Respond with ONLY the ansatz formula,
without any explanation or commentary. Ensure it is in valid Python. You may use
numpy functions, and scipy.special. params is a list of parameters that can be of any
length or complexity.”
Given this prompt, we found that LLM-LEx could readily identify certain special func-
tions, such as the error function, with relative ease. However, for more complex com-
binations of special functions, LLM-LEx typically produced accurate approximations
using functions from numpy, unless the prompt was adjusted to explicitly emphasise
the use of scipy.special. Specifically, we consider the following prompt:
Prompt 2: “You are a symbolic regression expert. Analyze the data in the image and
provide an improved mathematical ansatz. Respond with ONLY the ansatz formula,
without any explanation or commentary. Ensure it is in valid Python. You may use
numpy functions, and scipy.special. Give preference to scipy.special over numpy.
params is a list of parameters that can be of any length or complexity.”
In addition to varying the prompts, we also modified the x-values used to generate the
image, aiming to push beyond the domains where the numpy approximate expressions
are valid. Specifically, we define xVals1 to be 100 uniformly spaced points between -3
and 3, while xVals2 as 500 uniformly spaced points between -10 and 10.
In the second example, we study the Bessel function J0. For this case, Prompt 1 yields
a score of order 10−9 for xVals1 and 10−4 for xVals2, with the plot of the LLM-LEx
function and the ground truth being indistinguishable to the naked eye. The returned
expressions, however, are approximations of the Bessel function on this interval, consist-
ing of numpy trigonometric functions and polynomials. A similar outcome is observed
for Prompt 2 with xVals1. However, for Prompt 2 and xVals2, the score improves to
10−15, with a perfect visual match, and LLM-LEx suggests scipy.special.jn(0, x),
which is precisely the desired Bessel function.
16


--- Page 17 ---
We will briefly explore further possibilities for prompt engineering when discussing
extensions to this work in Section 4.
3
Combining with Kolmogorov-Arnold Networks
3.1
Univariate is all you need
Fit KAN
Prune Suggested?
Yes - update architecture
Data
No
Fit edges with LLM-LEx
Simplify Expression
Figure 6: The general structure of KAN-LEx.
The visual symbolic regression technique we have introduced is developed for univari-
ate functions f : R →R and utilises 2D images. However, it is not applicable to
multivariate functions, since these cannot be plotted in 2D.
Luckily, all we ever need from a mathematical point of view are (sums of) univariate
functions. This is due to the Kolmogorov-Arnold representation theorem, which postu-
lates that (under mild assumptions) any function can be written as sums of univariate
functions3. This fact was used in Kolmogorov-Arnold Networks (KANs) [16]. The
general structure of our approach is indicated in Fig. 6, and is called KAN-LEx.
A KAN is a directed computational graph with edges between nodes that are arranged
in layers, much like a standard MLP. Unlike an MLP, however, the edges of the graph
3One should keep in mind, however, that there is no theoretical guarantee the required univariate
functions are simple or remotely well behaved. In practice however, this does not appear to be an
obstacle—particularly when we generalise to ‘deep’ KANs.
17


--- Page 18 ---
that connect nodes i of layer ℓto node j of layer (ℓ+1) are instead univariate functions
of the value of node i in layer ℓ. In KANs, these functions are approximated by (cubic)
splines. The operation on nodes is just the sum over the values from the incoming
edges. Deep KANs are obtained by stacking multiple KAN layers, i.e., by interweaving
sum operations and applying univariate functions to the sums. Thus, for a KAN with
L layers where each layer has only a single node, the sum at the nodes is just the
identity operation, and the KAN represents a concatenation of univariate functions.
We can thus combine symbolic regression on univariate functions with KANs by ap-
plying the technique outlined in Section 2 to the univariate function on each edge of
the KAN. In more detail, we proceed as follows.
Firstly, we define a KAN architecture of maximal desired complexity, meaning whose
number of layers equals or exceeds the number of nested functions we expect or want
to map to, and whose number of nodes exceeds the number of summands expected in
the result. We should err on the over-parametrised side, since this can be more easily
remedied. We can detect if we have too many layers if some of the edge functions are
just linear (or affine) functions; we can then reduce the number of layers. The number
of nodes per layer can be pruned efficiently using KANs built-in pruning routines which
rely on edge scores, see [16] for details.
Once we have settled on an architecture, we fit each univariate edge function as in
Section 2. Armed with an expression for each edge, we can build the nested function
expressed by the KAN by constructing the expression from the computational graph.
We simplify the expression with sympy [17] and refit the coefficients using scipy’s
optimisers.
Next, we run the simplified expression through an LLM, asking it to further simplify
using the following strategies:
• Taylor expand terms that are small in this interval
• Remove negligible terms
• Recognise polynomial patterns as Taylor series terms
• Combine similar terms and factor them when possible
In order for the LLM to be able to judge which terms are small, negligible, or could
be involved in an (inverse) Taylor expansion, we also provide the interval from which
the input values were sampled. We then refit the free-parameters of the simplified
expression.
The reason for this simplification step (sympy →scipy →llm →scipy) is that
numerical fitting and function composition do not necessarily commute, and we can
18


--- Page 19 ---
improve the quality of the fit by fitting the combination. At the very least, the constant
terms bℓ
i appearing in the fit of each individual edge function is arbitrary, only their
sum P
i bℓ
i is meaningful. Moreover, since KANs have to express everything in terms
of sums of functions, some elementary operations are expressed in a cumbersome way
by the KAN. For example, since KANs have no multiplication built in, they would
express (for positive x, y)
xy = exp[log(x) + log(y)] ,
(10)
meaning they would learn a log on the edges of the first layer, then sum the logs on
the node, and then learn an exponential at the second layer. The simplification step
(sympy, scipy, llm, scipy) can be repeated N times, but we only used N = 1 in our
examples.
3.2
Multivariate Examples
We now demonstrate the efficacy of KAN-LEx in a number of examples.
Example 1
To illustrate the procedure of fitting multivariate functions, we choose the target
f :
R2
→R
(x, y)
7→exp[sin(πx) + y2]
(11)
The first (and arguably most important) step is to find a good KAN architecture. We
generate 10k points drawn random uniformly from the interval −1 ≤x, y ≤1. As a
first guess for the KAN architecture, we choose a [2,4,4,1] KAN and prune it. The
pruned model suggests an architecture choice of [2,4,1,1], see Figure 7a. We retrain a
[2,4,1,1] KAN and prune it. Pruning did not simplify the KAN but it has several edges
with almost linear activation functions (see Figure 7b), which motivates stripping a
layer and retraining a [2,4,1] KAN. After pruning, we see that the architecture can
be further simplified to a [2,3,1] KAN, cf. Figure 7c. Retraining and pruning a [2,3,1]
KAN then gives the [2,1,1] KAN in Figure 7d.
In the next step, we fit a function to each edge.
We use a population of 3 and 2
generations. The best-fitting LLM suggestions for the three edges e(l)
ij that connect
node i in layer ℓto node j in layer (ℓ+ 1) are
e(1)
11 (x, y)
= a0 sin(a1x) + a2 cos(a3x) ,
⃗a ≈(0.373, 3.142, 0.170, 10−6)
e(1)
21 (x, y)
= b0y2 + b1y + b2 ,
⃗b ≈(0.373, 10−5, −0.166)
e(2)
11 (z)
= c0 + c1z + c2 ∗exp(c3z) ,
⃗c ≈(−0.031, −0.050, 1.018, 2.651)
(12)
19


--- Page 20 ---
(a) [2, 4, 4, 1] (pruned) (b) [2, 4, 1, 1] (pruned)
(c) [2, 4, 1] (pruned)
(d) [2, 3, 1] (pruned)
Figure 7: Selection stages for the KAN architecture. The final model is a [2, 1, 1] KAN.
Although the functions are close to the correct expressions, they show some of the
subtleties discussed in Section 3.1. Firstly, the argument of the cosine is (almost) zero,
meaning that the edge function is e(1)
11 ≈a0 sin(a1x) + a2. The constant a2 ≈−b2,
meaning that the two constants will cancel out upon summing the contributions at the
node. Second, the coefficient a0 in front of the sine term is equal to the coefficient b0 in
front of the quadratic term, and both are roughly equal to 1/c3 ≈a0 ≈b0. The linear
term in the second edge function is close to zero, and the linear term in the last edge
function
c1z ≈c1(a0 sin(a1x) + b0y2)
(13)
is small with c1a0 ≈c1b0 ≈10−2.
This discussion illustrates the necessity of rounding coefficients to get rid of terms that
are close to zero, simplifying the expression with sympy and an LLM, and refitting
the parameters with scipy. This pipeline automatises the discussion in the previous
paragraph. The sympy and LLM optimiser approximate the cosine term with just
a2 and combine it with b2.
Moreover, they combine c1a0 and c1b0.
Refitting the
resulting expression with scipy and neglecting terms that are numerically zero, we get
the function (rounded to six significant digits)
1.0 exp[1.0y2 + sin(3.141593x)] .
(14)
Example 2
As a second example, we use the multiplication example in (10) and study the range
1 ≤x, y ≤2. We start again with a [2,4,4,1] KAN, which after pruning suggests a
20


--- Page 21 ---
(a) [2, 4, 4, 1] (pruned)
(b) [2, 1, 1] (pruned)
Figure 8: Selection stages for the KAN architecture. The final model is a [2, 1, 1] KAN.
[2,1,1] architecture. Running edge fitting on this architecture gives
e(1)
11 (x, y)
= a0 log(x) + a1 ,
⃗a ≈(1.182, 0.261)
e(1)
21 (x, y)
= b0 log(y) + b1 ,
⃗b ≈(1.182, 0.339)
e(2)
11 (z)
= c0 exp(c1z) ,
⃗c ≈(0.602, 0.846)
(15)
We find again that a0 ≈b0 ≈1/c0. There is also a discrepancy in the constant terms,
a1 ̸= −b1, but also c0 ̸= 1. In fact, c0 ≈1/ec1(a1+b1), such that the final functional form
is
f(x, y) = c0ec1(a0 log(x)+b0 log(y))+c1(a0+b0)
≈c0ec1(a0+b0)elog(x)+log(y)
≈elog(x)+log(y) .
(16)
Of course these simplifications are discovered automatically in our pipeline. The scipy
refitting does not change much, since the KAN is already very accurate. The sympy
simplification step turns elog(x)+log(y) into xy, and the subsequent fitting step does not
change anything. It is interesting to see what the subsequent LLM call suggests given
that it is already presented with a solution. We ask for 9 proposals to improve the
function, and it returns
[‘x * y’, ‘y * x’, ‘x * y + 0’, ‘x*y’, ‘y*x’
’x**1 * y**1’,’x*y’, ‘x**1 * y**1’, ‘x*y + 0’] ,
(17)
21


--- Page 22 ---
so it returns 9 equivalent expressions (out of which only 7 are distinct strings). In any
case, we now have the correct function, and the final scipy fitting step does of course
not change anything.
Example 3 - Newton’s Law
As a third, and final, example we consider Newton’s law of gravitation, given by V (r) =
GMm
r
. In dimensionless form, this reduces to the expression xy/z. We examine the
range of variables 0.5 < x, y, z < 3, and begin by training a KAN with architecture
[3, 4, 4, 1]. Several rounds of pruning yield the sequence:
[3, 4, 4, 1] →[3, 2, 2, 1] →[3, 1, 1] →[3, 1],
at which point we fit each edge using LLM-LEx, and find
e(1)
11 (x)
= a0 log(x) + a1 ,
⃗a ≈(2.026, −0.605)
e(1)
21 (y)
= b0 log(y) + b1 ,
⃗b ≈(2.029, −6.302)
e(1)
31 (z)
= c0 log(z) + c1 ,
⃗c ≈(−2.029, −1.547)
e(2)
11 (w)
= d0 exp(d1w) + d2 ,
⃗d ≈(64.608, 0.493, −0.001).
(18)
Simplifying and refitting automatically returns the correct function xy/z.
3.3
Improving Univariate Examples
The methodology outlined above can also be applied to enhance our univariate results.
Starting with data for a univariate function, we follow the approach described in the
previous subsection: pruning the architecture, fitting a function to each edge of the
pruned KAN, and subsequently simplifying the result.
Although this approach is clearly feasible and intuitively appealing—particularly for
“deep” functions (i.e., functions that are naturally expressed as compositions of many
simpler functions)—we found it to be largely ineffective in practice. We applied it to
those functions in Table 1 for which LLM-LEx failed to identify the exact expression,
but only one case, 2e−3x + e−x, yielded a successful outcome.
4
Conclusion
In conclusion, we have presented a new method for univariate symbolic regression based
on large language models and funsearch. This approach has demonstrated remarkable
22


--- Page 23 ---
success across a wide range of functions, despite the simplicity of the underlying genetic
algorithm. Notably, we do not enforce a simplicity score, which is often employed to
reduce bloat, suggesting that the LLM inherently exhibits a bias towards simplicity.
The results are summarised in Tables 1 and 2. Furthermore, as discussed in Section 2.5,
we observed that the method exhibits a degree of resilience to moderate noise, albeit
with a tendency to overfit if the noise is too large.
In Section 3, we extend our univariate method to multivariate functions by first train-
ing a KAN on the dataset. This process decomposes the multivariate problem into
several univariate subproblems. We then apply our previous method to each of these
subproblems individually, before simplifying the combined expression (using the assis-
tance of LLMs as well as standard symbolic simplification techniques as implemented
in sympy).
We have developed a Python package that facilitates applications to both univariate
and multivariate cases, which is available on GitHub [5]. The GitHub repository also
includes the Jupyter Notebooks containing the benchmarks and examples discussed in
this paper.
There are several natural extensions to this work.
One obvious direction is to ex-
plore more sophisticated genetic algorithms as an enhancement to LLM-LEx.
An-
other promising avenue involves training purpose-built vision transformers on images
of known functions. One might consider training higher-dimensional analogues of vi-
sion transformers to handle multivariate functions directly, thereby bypassing the need
for KANs. However, the latter two approaches would require constraining the sym-
bolic regression task to a specific class of functions. In contrast, a commercial LLM is
likely to have encountered a far broader range of functional forms during pretraining;
it follows that an effective route might be to fine-tune models on symbolic regression
data. In domain-specific contexts, where the types of functions expected are more
predictable, this limitation could potentially be mitigated by training the transformer
exclusively on relevant function classes.
Finally, another avenue worth exploring is prompt engineering—in particular, tailoring
the LLM’s prompt to incorporate domain-specific knowledge. For instance, if the target
function represents a particular physical quantity, the LLM might be able to leverage
insights from scientific domains in suggesting appropriate functions.
Acknowledgements
KFT is supported by the Gould-Watson Scholarship. JH, TRH, and FR are supported
by the National Science Foundation under Cooperative Agreement PHY-2019786 (The
NSF AI Institute for Artificial Intelligence and Fundamental Interactions, http://iaifi.org/).
23


--- Page 24 ---
FR is also supported by the NSF grant PHY-2210333 and startup funding from North-
eastern University. JH is supported by NSF CAREER grant PHY-1848089.
References
[1] M. Cranmer, “Interpretable Machine Learning for Science with PySR and
SymbolicRegression.jl,” arXiv e-prints (May, 2023) arXiv:2305.01582,
arXiv:2305.01582 [astro-ph.IM].
[2] B. Burlacu, G. Kronberger, and M. Kommenda, “Operon c++: An efficient
genetic programming framework for symbolic regression,” in Proceedings of the
2020 Genetic and Evolutionary Computation Conference Companion, GECCO
’20, p. 1562–1570. Association for Computing Machinery, New York, NY, USA,
2020. https://doi.org/10.1145/3377929.3398099.
[3] D. J. Bartlett, H. Desmond, and P. G. Ferreira, “Exhaustive Symbolic
Regression,” IEEE Trans. Evol. Comput. 28 no. 4, (2024) 964,
arXiv:2211.11461 [astro-ph.CO].
[4] B. Romera-Paredes, M. Barekatain, A. Novikov, M. Balog, M. P. Kumar,
E. Dupont, F. J. Ruiz, J. S. Ellenberg, P. Wang, O. Fawzi, et al., “Mathematical
discoveries from program search with large language models,” Nature 625
no. 7995, (2024) 468–475.
[5] “Llm-lex.” https://github.com/harveyThomas4692/llmlex.
[6] W. R. Inc., “Mathematica, Version 14.2.”
https://www.wolfram.com/mathematica. Champaign, IL, 2024.
[7] E. Cant´u-Paz et al., “A survey of parallel genetic algorithms,” Calculateurs
paralleles, reseaux et systems repartis 10 no. 2, (1998) 141–171.
[8] M. von Hippel and M. Wilhelm, “Refining Integration-by-Parts Reduction of
Feynman Integrals with Machine Learning,” arXiv:2502.05121 [hep-th].
[9] J. S. Ellenberg, K. Fraser-Taliente, T. R. Harvey, K. Srivastava, and A. V.
Sutherland, “Generative modeling for mathematical discovery,” 2025.
https://arxiv.org/abs/2503.11061.
[10] W. G. L. Cava, P. Orzechowski, B. Burlacu, F. O. de Fran¸ca, M. Virgolin, Y. Jin,
M. Kommenda, and J. H. Moore, “Contemporary symbolic regression methods
and their relative performance,” CoRR abs/2107.14351 (2021) , 2107.14351.
[11] R. Poli, W. B. Langdon, N. F. McPhee, and J. R. Koza, A Field Guide to
Genetic Programming. lulu.com, 2008.
24


--- Page 25 ---
[12] P. Shojaee, K. Meidani, S. Gupta, A. B. Farimani, and C. K. Reddy, “Llm-sr:
Scientific equation discovery via programming with large language models,”
arXiv preprint arXiv:2404.18400 (2024) .
[13] M. Merler, K. Haitsiukevich, N. Dainese, and P. Marttinen, “In-context symbolic
regression: Leveraging large language models for function discovery,” arXiv
preprint arXiv:2404.19094 (2024) .
[14] A. Grayeli, A. Sehgal, O. Costilla Reyes, M. Cranmer, and S. Chaudhuri,
“Symbolic regression with a learned concept library,” Advances in Neural
Information Processing Systems 37 (2024) 44678–44709.
[15] D. Li, J. Yin, J. Xu, X. Li, and J. Zhang, “Visymre: Vision-guided multimodal
symbolic regression,” arXiv preprint arXiv:2412.11139 (2024) .
[16] Z. Liu, Y. Wang, S. Vaidya, F. Ruehle, J. Halverson, M. Soljaˇci´c, T. Y. Hou, and
M. Tegmark, “KAN: Kolmogorov-Arnold Networks,” arXiv:2404.19756
[cs.LG].
[17] A. Meurer, C. P. Smith, M. Paprocki, O. ˇCert´ık, S. B. Kirpichev, M. Rocklin,
A. Kumar, S. Ivanov, J. K. Moore, S. Singh, T. Rathnayake, S. Vig, B. E.
Granger, R. P. Muller, F. Bonazzi, H. Gupta, S. Vats, F. Johansson,
F. Pedregosa, M. J. Curry, A. R. Terrel, v. Rouˇcka, A. Saboo, I. Fernando,
S. Kulal, R. Cimrman, and A. Scopatz, “Sympy: symbolic computing in
python,” PeerJ Computer Science 3 (Jan., 2017) e103.
25
