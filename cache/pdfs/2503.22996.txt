--- Page 1 ---
Unified Sparse Mixture of Experts
Giang Do
Hung Le
Truyen Tran
Applied Artificial Intelligence Institute (A2I2), Deakin University
{truong.do,thai.le,truyen.tran}@deakin.edu.au
Abstract
Sparse Mixture of Experts (SMoEs) models scale the capacity of models while
maintaining constant computational overhead. Early designs typically relied on a
fixed value of k, where k represents either the number of experts selected per token
or the number of tokens assigned per expert. However, these approaches encounter
three key limitations: they may fail to route to important experts or tokens, may
assign irrelevant ones, and often suffer from representation collapse among experts.
This paper reexamines SMoEs through the lens of Linear Programming, and
proposes a Unified Sparse Mixture of Experts (USMoE) framework that addresses
these limitations. Specifically, our approach introduces a unified mechanism
that integrates information from both the expert and token dimensions, and a
unified scoring function that linearly combines similarity scores between experts
and tokens. We provide both theoretical justification and empirical evidence
demonstrating USMoE’s effectiveness in overcoming the limitations of traditional
routing methods. Through comprehensive evaluations on both clean and corrupted
settings for large language models and vision tasks, under both training-free and
training scenarios, USMoE achieves up to a 10% performance improvement over
standard approaches or reduces inference costs by up to 14%, while maintaining
competitive accuracy.
1
Introduction
Sparse Mixture of Experts (SMoE) models have achieved notable success in natural language
processing (NLP) and visual representation learning tasks [20, 21, 44, 48]. These advancements
build on the Transformer architecture [53] and its variants [11, 15], which leverage large datasets
and significant compute resources. However, training large Transformer models can be prohibitively
expensive, requiring extensive compute hours [31]. To overcome this issue, SMoE models activate
only a subset of experts for each input, reducing inference time compared to dense models [46, 61,
2, 32]. The SMoE architecture can be categorized into two variants: Token Choice, which assigns
experts to each token [12, 52, 37, 26], and Expert Choice, which assigns tokens to each expert [59].
The advantage of Token Choice lies in its ability to dynamically select experts for each token, while
Expert Choice ensures a more balanced token distribution across experts.
Despite their promising results, SMoE models have several limitations. The Expert Choice approach
suffers from token dropping [59], while the Token Choice approach struggles with unbalanced expert
loading [47]. Additionally, both approaches are prone to representation collapse, where either a few
experts dominate the routing or all experts learn similar representations [9, 8]. Recent research has
explored improving router policies [10, 6, 17] to mitigate these issues. However, existing methods
face three key challenges: (1) The reliance on auxiliary losses requires careful balancing between the
router loss and the task loss, which introduces trade-offs; (2) Token Choice (TC) struggles to handle
noisy tokens effectively; and (3) Expert Choice (EC) suffers from information leakage issues, which
significantly degrade performance on autoregressive models [58, 54, 43]. As a result, the question of
how to optimally select experts or tokens remains open.
Preprint. Under review.
arXiv:2503.22996v2  [cs.CL]  27 Oct 2025


--- Page 2 ---
Input
USMoE
TC
EC
Input
USMoE
TC
EC
Figure 1: We compare token routing performance in vision tasks using 7×7 images, where each token is
color-coded based on its assigned expert under setting c = t, where c is a sparsity constraint and t is number of
image patches. Left: When the object is easy to distinguish, Expert Choice (EC) fails to assign different experts
appropriately. Token Choice (TC) performs better but still does not align perfectly with the actual object, while
USMoE correctly separates the object. Right: In more challenging images, both Expert Choice (EC) and Token
Choice (TC) fail to distinguish between object and background. In contrast, USMoE successfully differentiates
the object from the background, demonstrating greater efficiency in vision tasks compared to EC and TC, as
further shown in Section 4.
.
Research Question
What are the optimal criteria for selecting experts or tokens in SMoE models under fixed
computational budgets?
Loading Balance
Robustness
Token Dropping
Information Leak
Expert Collapses
Token Choice
⌣
⌢
⌣
⌣
⌢
Expert Choice
⌣
⌣
⌢
⌢
⌢
USMoE (ours)
⌣
⌣
⌣
⌣
⌣
Table 1: Benefits of the proposed USMoEs’ model form over existing methods.
In this paper, we revisit SMoEs through the lens of Linear Programming [3, 1] to better understand
the criteria for expert and token selection. From this perspective, Token Choice selects experts along
the expert dimension by assigning each token to its most similar expert, while Expert Choice selects
tokens along the token dimension by allowing each expert to choose the most similar tokens.
This perspective highlights a key trade-off: Expert Choice risks dropping important tokens, whereas
Token Choice struggles to handle noisy or irrelevant tokens. Furthermore, both approaches suffer
from the problem of representation collapse [10, 17, 41].
Building on this analysis, we propose the Unified Sparse Mixture of Experts (USMoE), a robust
and efficient framework consisting of two key components: (1) the Unified Score and (2) the
Unified Mechanism. The Unified Score is defined as a linear combination of two mapping functions
that transform the dot product similarity between token and expert embeddings into a probability
distribution - using functions such as softmax or sigmoid, as shown in Algorithm 1. In parallel,
the Unified Mechanism incorporates information from both the token and expert dimensions, as
illustrated in Figure 2. These components enable the SMoE model to dynamically prioritize tokens or
experts while ensuring the selection of the most similar token-expert pair, enhancing both robustness
and effectiveness. We summarize the benefits of USMoEs’ model form over existing methods in
Table 1. To demonstrate the effectiveness of our approach, we evaluate USMoE across a variety
of scenarios, including training-free, pretraining, and fine-tuning settings, spanning both language
and vision domains under both clean and corrupted datasets. USMoE consistently outperforms
baseline methods across these scenarios, with particularly strong gains in tasks that require deep
input understanding, such as semantic textual similarity, classification, and clustering. Extensive
experiments across various benchmarks show that USMoE achieves up to a 10% improvement over
traditional approaches or reduces inference computational costs by 14%, all while maintaining high
performance.
In summary, this paper makes the following key contributions:
2


--- Page 3 ---
• We introduce a Linear Programming perspective on SMoE, highlighting the weaknesses
of existing approaches.
• We propose USMoE, a robust and efficient framework comprising two modules: (1) the
Unified Score and (2) the Unified Mechanism, which together address the limitations of
conventional approaches, as summarized in Table 1.
• We theoretically demonstrate that USMoE effectively mitigates representation collapse,
outperforming baseline methods.
• We conduct extensive experiments on both large language models and vision domains,
covering pretraining and both fine-tuning and training-free settings, providing a detailed
analysis of USMoE’s performance and effectiveness.
2
Related Work
Sparse Mixture of Experts (SMoE). Sparse Mixture of Experts (SMoE), an extension of the
Mixture of Experts framework [25, 30], has gained traction with large language models and has since
been applied in various domains, including computer vision and speech recognition [60, 45]. The
SMoE architecture consists of two main variants: Token Choice, where experts are assigned to each
token [47, 21, 27, 19], and Expert Choice, where tokens are assigned to specific experts [59].
Token Choice treats all tokens equally, which has raised concerns among researchers [55, 24, 35],
while Expert Choice suffers from token-dropping issues. Additionally, SMoE faces the challenge
of representation collapse, where experts produce similar outputs. Various solutions have been
proposed, such as XMoE, which employs low-dimensional routing scores [10], and SMoE-dropout,
which gradually activates more experts [6]. Other approaches, including HyperRouter [17] and
StableMoE [13], focus on enhancing router stability and robustness. Although these advancements
have improved SMoE models, representation collapse remains a persistent issue [41, 16]. Our
approach addresses this by optimizing the alignment between tokens and the most suitable experts,
expanding expert specialization and mitigating collapse. Moreover, we also discuss related work on
Linear Programming and other studies relevant to our approach in Section A.1.
3
Methodology
We introduce Unified Sparse Mixture of Experts (USMoE), a novel and efficient Sparse Mixture
of Experts framework designed to address the limitations of both Token Choice and Expert Choice
through a Unified Score, a scoring function that balances expert and token important and Unified
Mechanism, a structured routing strategy that consider information from both token dimension and
expert dimension.
3.1
Preliminaries
The Sparse Mixture of Experts (SMoE) architecture replaces the MLP layers in standard transformers
with Mixture of Experts (MoE) layers [46]. Let h ∈RT ×d denote the token representations from
the attention layer, where T is the number of tokens and d is the hidden dimension. Given N expert
functions {FFNj}N
j=1, a gating projection matrix W ∈Rd×N maps tokens to expert affinity scores.
Denotes S = f(hW) ∈RT ×N is a compatibility score, where f maps the scores to a routing
distribution (e.g., softmax or sigmoid). Given X ∈{0, 1}T ×N be a binary routing matrix, where
xij = 1 if token i is routed to expert j, the output of the SMoE layer is:
fSMoE(h) =
N
X
j=1
S·j ⊙X·j ⊙FFNj(h ⊙X·j),
(1)
where FFNj(·) denotes the computation performed by expert jth, and ⊙represents element-wise
multiplication, and FFN denotes for the Feed-forward neural networks.
3.2
Experts Selection as Linear Programming
Viewing expert selection as a linear programming problem allows for globally optimizing expert
assignments by maximizing similarity scores under a strict sparsity (computational) constraint.
3


--- Page 4 ---
E 1
Token 1
E 2
 E 3
E 4
Token #
Token 3
Token 4
Expert dim
Token Choice (TC)
E 1
Token 1
E 2
E 3
E 4
Token #
Token 3
Token 4
Expert dim
Token dim
USMoE (ours)
E 1
Token 1
E 2
E 3
E 4
Token #
Token 3
Token 4
Token dim
Expert Choice (EC)
Figure 2: An illustration of our USMoE selection mechanism (middle) is shown, which incorporates information
from both the token and expert dimensions. In contrast, Token Choice (TC, left) considers only the expert
dimension, while Expert Choice (EC, right) focuses solely on the token dimension. Tokens marked with #
represent noisy or irrelevant tokens. TC struggles to handle these # tokens effectively, and EC is prone to
missing important tokens. USMoE addresses both issues, making our method more robust than traditional MoE
approaches. This robustness is demonstrated in both theoretical analysis and experimental results. ’E’ denotes
experts. Best viewed in color.
Expert Selection Optimization.
Given a computational budget of c experts per token, the objective
of Sparse Mixture-of-Experts (MoE) routing is to determine the optimal expert assignment X∗that
maximizes the total compatibility score, subject to a sparsity constraint c:
maximize
T
X
i=1
N
X
j=1
Sijxij
subject to
T
X
i=1
N
X
j=1
xij ≤c
xij ∈{0, 1},
∀i, j.
(2)
Proposition 3.1. Let S ∈RT ×N be the compatibility score matrix and c ∈N a global routing
budget. Consider the objective:
M = ⟨S, X⟩=
T
X
i=1
N
X
j=1
Sijxij,
subject to the constraint P
i,j xij ≤c, with xij ∈{0, 1}.
Let XUSMoE = TopK(S, c) be the binary mask produced by selecting the top-c entries of S globally.
Then for any other feasible binary routing matrix XT ∈{0, 1}T ×N satisfying the same budget
constraint P
i,j(XT )ij ≤c, we have:
⟨S, XT ⟩≤⟨S, XUSMoE⟩.
The Proposition 3.2 indicate that the Unified Mechanism (USMoE) yields the optimal solution that
maximizes the total similarity under the global budget constraint. The proof is provided in Appendix
A.
Definition 3.2 (Unified Score Function). Let S ∈RT ×N be the compatibility score matrix between
T tokens and N experts. Define two score mapping functions:
• ft(S) ∈RT ×N: a row-wise scoring function used in Token Choice (e.g., softmax applied
across each row).
• fe(S) ∈RT ×N: a column-wise scoring function used in Expert Choice (e.g., softmax
applied across each column).
The Unified Score Function combines both perspectives via a linear combination:
fUSMoE(S) = α · fe(S) + β · ft(S),
where α, β ∈R are non-negative coefficients such that α + β = 1.
The Unified Score as Definition 3.2 integrates both token-centric and expert-centric preferences to
inform more balanced routing decisions.
4


--- Page 5 ---
Lemma 3.3. The Unified Score Function fUSMoE(S) = α · fe(S) + β · ft(S), where α, β ∈R are
non-negative coefficients such that α + β = 1, is more robust to representation collapse [10] than
using ft(S) or fe(S) alone.
Lemma 3.2 establishes that USMoE outperforms conventional SMoEs by mitigating the representation
collapse issue, as discussed in [10]. A detailed proof is presented in Appendix A.
3.3
Unified Sparse Mixture of Experts (USMoE)
Revising Expert Choice. The Expert Choice (EC) approach faces scalability challenges in autore-
gressive models due to information leakage [54, 43] introduced by the softmax operator. To address
this, we replace softmax with a sigmoid function for Token Choice, which eliminates inter-token
leakage and, as shown in Figure 6, leads to improved bits-per-character (BPC) performance and a
reduced token dropping ratio.
Finding 1: The Sigmoid function outperforms Softmax in Expert Choice MoE, as it handles
information leakage and token dropping more effectively.
Unified Score. We empirically observe that Token Choice effectively captures in-context learning
behavior, while Expert Choice conveys rich semantic alignment as Figure 3. To harness the comple-
mentary strengths of both, we propose the Unified Score, defined in Algorithm 1, as a weighted sum
of the Token Choice and Expert Choice scores. This unified formulation integrates both token-centric
and expert-centric signals to enable more balanced and informed routing decisions. Empirically,
we find that setting the combination weight to α ≈0.5 yields a robust trade-off between the two
strategies.
Figure 3: The performance of Unified Score (USMoE), Softmax (TC), and Sigmoid (EC) across the MTEB
benchmark. Sigmoid outperforms Softmax on tasks without prompting, indicating stronger semantic representa-
tions.
Finding 2: Sigmoid (EC) excels at capturing semantic information, while Softmax (TC) is
better suited for leveraging in-context cues.
Unified Mechanism. We propose the Unified Mechanism, which formulates expert-token assignment
as a joint selection problem. By flattening the similarity matrix and selecting the top-N expert-token
pairs based on the highest unified scores, as illustrated in Figure 3.2, this mechanism enables efficient,
context-aware routing within sparse MoE architectures.
4
Experiments
In this section, we evaluate our method across both Large Language Models (LLMs) and Vision
tasks, under clean and adversarial (attack) settings, and in both training-free and fine-tuned scenar-
ios. We empirically demonstrate the advantages of USMoE over Token Choice (TC) and Expert
Choice (EC) across advanced Sparse Mixture of Experts (SMoE) models, including QwenMoE [52],
OLMoE [37], and DeepSeekMoE [12]. Through extensive experiments, we show that: (1) USMoE
outperforms baseline methods even without additional computational cost; (2) USMoE provides
5


--- Page 6 ---
Algorithm 1: USMoE Layer
Require: X ∈RB×L×D, router weights R ∈RD×N, experts, controlling factor α
Ensure: Output Y
1: logits ←X · R
▷Dot product similarity
2: tc_score ←softmax(logits),
ex_score ←sigmoid(logits)
3: U ←(1 −α) · tc_score + α · ex_score,
U ←reshape(U, B, −1)
4: topn_val, topn_idx ←TopK(U, topn, dim = 1)
5: Y ←SMoE(X, experts, topn_val, topn_idx)
6: return Y
significant improvements across both language and vision domains; (3) Our method demonstrates
robustness in both LLMs and vision tasks; and (4) USMoE supports flexible Top-k expert selection,
which is valuable for scenarios with limited computational resources while maintaining competitive
performance.
4.1
Large Language Models (LLMs)
We evaluate our method against two standard SMoE baselines across the following three settings:
Training-Free. We evaluate our approach on three state-of-the-art Sparse Mixture of Experts (MoE)
models: (1) OLMoE-1B-7B [37], which has 7B parameters, 16 layers, and 64 experts per layer; (2)
Qwen1.5-MoE-A2.7B [52], comprising 7B parameters, 24 layers, and 60 experts per layer; and
DeepSeekMoE-16B which consists of 16B parameters, 28 layers, and 64 experts per layer. We
evaluate these models on the Massive Text Embedding Benchmark (MTEB) [38] without additional
fine-tuning.
Supervised Fine-Tuning. We demonstrate the effectiveness of USMoE on supervised fine-tuning
using the Alpaca dataset [51], adopting the memory-efficient full-parameter training approach
proposed by [57], which outperforms common low-rank adaptation methods in terms of memory
efficiency. To evaluate the robustness of our method, we follow the procedure in [39], applying a
Text Attack strategy where words are randomly replaced with a generic token “AAA”.
Training from scratch.
To assess the effectiveness of our method, we compare USMoE with
the Token Choice approaches, including SMoE [27], SMoE-Dropout (abbreviated as "SMoE-DR"),
XMoE [10], and StableMoE [13], as well as the Expert Choice approach [58] for pre-training and fine-
tuning tasks. We follow the approach of [7] and use a base Transformer-XL [14] with four decoder
layers. We train both base and large-scale versions of Transformer-XL on four datasets (Enwik8,
Text8, Wikitext-103, and One Billion Words) for 100k iterations, following the implementation in [7].
Then we fine-tune the pre-trained weights for text classification tasks, including SST-2 [49], SST-5
[49], IMDB [36], and BANKING77 [4]. More implementation details and additional results are provided
in the Appendix A.
4.1.1
Training-Free
In this section, inspired by [34], we test our method as a plug-in framework on well-trained SMoE
models, including OLMoE-1B-7B [37]:, DeepSeekMoE-16B [12], Qwen1.5-MoE-A2.7B [52]. We
evaluate performance on a subset of tasks from the Massive Text Embedding Benchmark (MTEB)
[38], which covers key downstream applications for sentence embeddings, including Classification,
Clustering, Pair Classification, Re-ranking, Retrieval, Semantic Textual Similarity (STS), and Summa-
rization. Following the MTEB evaluation framework, we use Accuracy for Classification, V-Measure
for Clustering, Average Precision for Pair Classification, Mean Average Precision for Re-ranking,
nDCG for Retrieval, and Spearman’s correlation for STS and Summarization. Our method consis-
tently demonstrates performance improvements across a range of MTEB tasks . Detailed results for
datasets under each task type are provided in Appendix A. USMoE outperforms both the Expert
Choice and Token Choice approaches in most cases, underscoring the complementary nature of these
two methods.
For tasks evaluated in Figure 4, USMoE proves even more effective at enhancing the Token
Choice approach, delivering notable gains of 14%, 12%, and 13% for OLMoE-1B-7B, Qwen1.5-
MoE-A2.7B, and DeepSeekMoE-16B, respectively, across MTEB tasks without additional training.
Specifically, Qwen1.5-MoE-A2.7B achieves a remarkable improvement from 13.4% (Token Choice)
to 40.0% (USMoE) in the Summarization task, representing a 198% gain. This trend persists across
6


--- Page 7 ---
Classif
Clustering
PairClassif
Reranking
STS
Summ
20
40
60
50.3
27.4
46.9
45.3
38.0
13.4
50.3
52.2
29.9
56.2
53.1
60.7
40.0
52.2
Qwen1.5-MoE-A2.7B
Classif
Clustering
PairClassif
Reranking
STS
Summ
20
40
60
43.4
14.7
39.1
37.4
24.1
20.9
43.4
51.3
21.0
61.7
46.5
59.6
31.8
51.3
OLMoE-1B-7B
Classif
Clustering
PairClassif
Reranking
STS
Summ
20
40
60
46.6
18.1
40.9
38.9
26.3
22.0
46.6
49.4
21.0
53.5
45.7
51.2
29.9
49.4
DeepSeekMoE-16B
TC
EC
USMoE
USMoE 
 vs TC
Figure 4: Performance comparison of USMoE, Token Choice (TC), Expert Choice (EC), and MoEE across
MTEB Tasks and advance SMoE models. The best result for each row is highlighted in bold. Best viewed in
color.
DeepSeekMoE-16B and OLMoE-1B-7B, where USMoE consistently outperforms both the Token
Choice and Expert Choice approaches. Overall, our approach outperforms the baselines in terms of
performance while exhibiting lower variance across multiple tasks and different runs.
4.1.2
Supervised Fine-Tuning
Supervised Fine-Tuning. We demonstrate the efficiency and robustness of USMoE in supervised
fine-tuning across two advanced SMoE models: QwenMoE (7B)[52] and OLMoE (7B)[37]. For
training, we adopt Supervised Fine-Tuning (SFT) using GaLore[57], which enables full-parameter
optimization with improved memory efficiency compared to typical low-rank adaptation methods
such as LoRA. The fine-tuning is conducted on the Alpaca dataset[51] for 2,000 steps, with eval-
uation performed every 10 steps. Furthermore, we assess our method on both clean (referred to
as QwenMoE and OLMoE) and adversarially corrupted versions (denoted QwenMoE-Corrupt and
OLMoE-Corrupt), where sequences of random “AAA” tokens are injected to simulate noise.
Robustness. Figure 5 presents the training results for both QwenMoE and OLMoE models. The
results show that USMoE consistently outperforms the baselines across both QwenMoE and OLMoE
models. Our method surpasses both Token Choice (TC) and Expert Choice (EC), particularly on
QwenMoE, where it achieves significantly lower training and validation perplexity, indicating faster
convergence and better generalization. Notably, the performance gap becomes even more pronounced
under the corrupted setting, highlighting the superior robustness of USMoE compared to traditional
SMoE approaches.
4.1.3
Training from Scratch
Robustness. Table 2 presents pre-training results on four datasets (enwik8, text8, WikiText-103,
and One Billion Words) under both clean and corrupted settings. USMoE consistently outperforms
both Expert Choice and Token Choice routing methods across all datasets. Similar to the SFT task,
the performance gap widens under corruption, especially on One Billion Words—highlighting the
robustness advantage of USMoE over traditional SMoEs. For additional experiments on training from
scratch, such as scaling models up to 400M parameters or fine-tuning on downstream tasks—please
refer to Section A.
Flexibility. One of USMoE’s key strengths is its flexibility in computational resource usage. Unlike
traditional SMoEs, which require the number of selected experts (top-k) to be an integer, USMoE
supports fractional expert selection. This allows the model to maintain competitive performance
while leveraging fewer experts. For instance, on text8, USMoE outperforms SMoE using only
1.5 experts. It also surpasses Token Choice, which uses two experts, on enwik8, text8, and One
Billion Words, while averaging just 1.5 experts per token. Moreover, using only 1.5 experts enables
USMoE to reduce FLOPs by 14% compared to SMoE, with minimal performance trade-off. This
makes USMoE particularly well-suited for resource-constrained environments such as IoT and edge
devices.
7


--- Page 8 ---
Figure 5: Illustration of comparing the performance of USMoE, Token Choice (TC), Expert Choice (EC) using
QwenMoE and OLMoE for the Supervised Fine-Tuning task on Apaca dataset for 2K steps under both clean
and corrupted settings. Training and validation perplexity over training steps are reported, and lower values are
better.
Setting
Original
Corrupt
Transformer-XL(20M) Topk Enwik8 Text8 WikiText-103 lm1b Enwik8 Text8 WikiText-103 lm1b
USMoE
2
1.18
1.20
29.20
56.90
1.75
1.83
38.45
68.43
1.5
1.19
1.28
30.67
57.55
1.78
1.95
40.39
70.47
TC
2
1.20
1.29
30.16
58.00
1.77
1.86
39.31
79.73
EC
2
1.18
1.24
29.83
58.60
1.76
1.89
39.28
71.75
Table 2: Performance comparison of USMoE, Token Choice (TC), and Expert Choice (EC) across multiple
datasets, with BPC on the Enwik8 and Text8 test sets, and perplexity on the WikiText-103 and One Billion Word
test sets. Lower values are better, with the best results highlighted in bold.
4.2
Vision
Effiency. To ensure a thorough and meaningful performance comparison, we evaluate against two
categories of baselines that are closely aligned with USMoE, including Token Choice, Expert Choice,
and SoftMoE [42], currently among the most advanced MoE models in the vision domain. We explore
two Mixture of Experts configurations: (1) a small model with 10 million parameters (10M), and
(2) a large model with 110 million parameters (110M). As shown in Table 3, USMoE consistently
outperforms Vision Transformer variants with Token Choice, Expert Choice, and SoftMoE across all
eight tasks and four image classification datasets. Our experiments are conducted three times on four
datasets (CIFAR-10, CIFAR-100, STL-10, SVHN, and ImageNet1K), using different random seeds.
We report average results along with standard deviations. USMoE not only achieves superior average
performance but also demonstrates greater stability, as evidenced by lower standard deviation values.
ViT-MoE
10M
110M
Avg.
Dataset
Cifar10 Cifar100 STL-10
SVHN
ImageNet-1K Cifar10 Cifar100 STL-10
SVHN
ImageNet-1K
USMoE
89.6±0.3
66.6±0.5
66.7±0.4 95.6±0.1
60.2±0.1
91.5±0.5
67.3±0.5
66.2±0.1 96.1±0.1
73.5±0.4
77.3±0.3
TC
88.7±0.2
65.4±0.5
66.4±0.1 95.1±0.1
56.6±0.5
85.7±0.8
55.5±2.8
64.4±0.2 94.5±0.4
72.0±0.4
74.4±1.5
EC
88.9±0.3
65.7±0.4
66.1±0.4 95.0±0.1
56.2±0.4
87.4±0.7
66.2±0.9
65.5±0.4 93.2±0.2
70.9±0.5
75.5±1.2
SoftMoE 85.6±0.3
61.4±0.3
65.4±0.2 94.8±0.1
46.8±0.6
80.3±1.0
42.9±1.4
63.9±1.2 93.5±0.1
71.2±0.3
70.6±1.5
Table 3: Accuracy of models evaluated on vision datasets. Each method is evaluated 3 times, reporting the mean
and standard deviation. Higher is better, the best results are in bold.
Robustness. Table 4 presents a comparative analysis of ViT-MoE variants, USMoE, Token Choice
(TC), Expert Choice (EC), and SoftMoE, under three common adversarial attacks: Projected Gradient
Descent(PGD)[5], Fast Gradient Sign Method(FGSM) [23], and Simultaneous Perturbation Stochastic
8


--- Page 9 ---
Approximation (SPSA) [50], across five standard image classification datasets. USMoE consistently
achieves strong performance, outperforming or matching the best baselines in most scenarios. Notably,
it achieves the highest average robustness across all three attack types, particularly excelling under
PGD (46.2%), FGSM (43.3%), and SPSA (63.9%). While Token Choice (TC) performs poorly
across all three attack types, USMoE consistently delivers stronger and more reliable performance on
average. These results highlight USMoE’s robustness and generalization capability under adversarial
conditions, making it a more stable and effective choice for secure image classification tasks.
Finding 3: Token Choice (TC) struggles under adversarial attacks.
Dataset
PGD
FGSM
SPSA
ViT-MoE (10M) USMoE
TC
EC
SoftMoE USMoE
TC
EC
SoftMoE USMoE
TC
EC
SoftMoE
CIFAR-10
57.4
27.7 57.5
55.6
49.5
26.4 48.5
45.8
83.6
33.4 87.0
69.1
CIFAR-100
27.5
11.9 27.0
28.6
21.5
11.1 19.3
20.6
65.8
15.8 59.1
39.3
STL-10
41.0
37.7 39.5
39.8
36.5
35.8 34.8
35.0
64.3
44.1 61.3
49.6
SVHN
81.0
38.4 80.5
91.3
76.0
36.1 75.4
69.2
92.5
36.3 91.8
84.4
ImageNet-1K
24.0
1.3
22.4
13.3
12.2
1.4
11.2
9.9
13.4
1.5
12.3
10.9
Avg.
46.2
23.4 45.4
45.7
43.3
22.2 37.8
36.1
63.9
26.2 62.3
50.7
Table 4: Robustness evaluation of different ViT-MoE models under adversarial attacks: PGD, FGSM, and SPSA
across five datasets. Bold indicates the best performance for each task and attack.
5
Conclusion
In this work, we reinterpret Token Choice and Expert Choice Sparse Mixture of Experts (SMoE)
through a linear programming lens, revealing their inherent limitations. Based on this analysis,
we propose Unified Sparse Mixture of Experts (USMoE) - a novel and efficient framework that
advances SMoE by introducing a unified mechanism and unified score learning. We theoretically
prove that USMoE achieves superior expert selection compared to traditional methods, effectively
improving expert learning capacity while mitigating expert collapse. As a result, USMoE learns more
robust expert representations and overcomes the representation collapse issues commonly observed
in conventional SMoE training. Extensive experiments across diverse domains, including vision and
large language models (LLMs), under both clean and adversarial settings (covering training-free and
fine-tuning scenarios), demonstrate that USMoE enables more efficient and effective training and
inference compared to state-of-the-art routing strategies.
References
[1] David Applegate, Mateo Diaz, Oliver Hinder, Haihao Lu, Miles Lubin, Brendan O' Donoghue,
and Warren Schudy. Practical large-scale linear programming using primal-dual hybrid gradient.
In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,
Advances in Neural Information Processing Systems, volume 34, pages 20243–20257. Curran
Associates, Inc., 2021.
[2] Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Vic-
toria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li,
Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura,
Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Ves Stoyanov.
Efficient large scale language modeling with mixtures of experts, 2022.
[3] Carlo Baldassi, Alessandro Ingrosso, Carlo Lucibello, Luca Saglietti, and Riccardo Zecchina.
Neural sparse coding via l0 gradient-based optimization. In Advances in Neural Information
Processing Systems, volume 28, 2015.
[4] Iñigo Casanueva, Tadas Temˇcinas, Daniela Gerz, Matthew Henderson, and Ivan Vuli´c. Efficient
intent detection with dual sentence encoders. In Proceedings of the 2nd Workshop on Natural
Language Processing for Conversational AI, pages 38–45, Online, July 2020. Association for
Computational Linguistics.
9


--- Page 10 ---
[5] Ting-Jui Chang, Yukun He, and Peng Li. Efficient two-step adversarial defense for deep neural
networks, 2018.
[6] Tianlong Chen, Zhenyu Zhang, Ajay Jaiswal, Shiwei Liu, and Zhangyang Wang. Sparse moe as
the new dropout: Scaling dense and self-slimmable transformers, 2023.
[7] Tianlong Chen, Zhenyu Zhang, AJAY KUMAR JAISWAL, Shiwei Liu, and Zhangyang Wang.
Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers. In The
Eleventh International Conference on Learning Representations, 2023.
[8] Zixiang Chen, Yihe Deng, Yue Wu, Quanquan Gu, and Yuanzhi Li. Towards Understanding the
Mixture-of-Experts Layer in Deep Learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,
and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.
[9] Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal,
Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On the Representation
Collapse of Sparse Mixture of Experts. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and
Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.
[10] Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal,
Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On the representation
collapse of sparse mixture of experts, 2022.
[11] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with
sparse transformers, 2019.
[12] Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li,
Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong
Ruan, Zhifang Sui, and Wenfeng Liang. Deepseekmoe: Towards ultimate expert specialization
in mixture-of-experts language models, 2024.
[13] Damai Dai, Li Dong, Shuming Ma, Bo Zheng, Zhifang Sui, Baobao Chang, and Furu Wei.
Stablemoe: Stable routing strategy for mixture of experts, 2022.
[14] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.
Transformer-XL: Attentive Language Models beyond a Fixed-Length Context. In Proceedings
of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978–2988,
Florence, Italy, July 2019. Association for Computational Linguistics.
[15] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov.
Transformer-xl: Attentive language models beyond a fixed-length context, 2019.
[16] Giang Do, Hung Le, and Truyen Tran. Simsmoe: Solving representational collapse via similarity
measure, 2024.
[17] Giang Do, Khiem Le, Quang Pham, TrungTin Nguyen, Thanh-Nam Doan, Bint T. Nguyen,
Chenghao Liu, Savitha Ramasamy, Xiaoli Li, and Steven Hoi. Hyperrouter: Towards efficient
training and inference of sparse mixture of experts, 2023.
[18] Giang Do, Khiem Le, Quang Pham, TrungTin Nguyen, Thanh-Nam Doan, Bint T. Nguyen,
Chenghao Liu, Savitha Ramasamy, Xiaoli Li, and Steven Hoi. Hyperrouter: Towards efficient
training and inference of sparse mixture of experts, 2023.
[19] Giang Do, Kha Pham, Hung Le, and Truyen Tran. On the effectiveness of discrete representa-
tions in sparse mixture of experts, 2024.
[20] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,
Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P
Bosma, Zongwei Zhou, Tao Wang, Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson,
Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc Le, Yonghui Wu, Zhifeng
Chen, and Claire Cui. GLaM: Efficient Scaling of Language Models with Mixture-of-Experts.
In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan
Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume
162 of Proceedings of Machine Learning Research, pages 5547–5569. PMLR, July 2022.
10


--- Page 11 ---
[21] William Fedus, Barret Zoph, and Noam Shazeer. Switch Transformers: Scaling to Trillion
Parameter Models with Simple and Efficient Sparsity. Journal of Machine Learning Research,
23(120):1–39, 2022.
[22] Seokjin Go and Divya Mahajan. Moetuner: Optimized mixture of expert serving with balanced
expert placement and token routing, 2025.
[23] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adver-
sarial examples, 2015.
[24] Le Hou, Richard Yuanzhe Pang, Tianyi Zhou, Yuexin Wu, Xinying Song, Xiaodan Song, and
Denny Zhou. Token dropping for efficient bert pretraining, 2022.
[25] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive
mixtures of local experts. Neural Computation, 3(1):79–87, 1991.
[26] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris
Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand,
Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier,
Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak,
Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
William El Sayed. Mixtral of experts, 2024.
[27] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris
Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand,
Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier,
Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak,
Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
William El Sayed. Mixtral of experts, 2024.
[28] Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang, and Fuzhen Zhuang. Scaling
sentence embeddings with large language models, 2023.
[29] Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang, and Fuzhen Zhuang. Scaling
sentence embeddings with large language models. In Yaser Al-Onaizan, Mohit Bansal, and
Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP
2024, pages 3182–3196, Miami, Florida, USA, November 2024. Association for Computational
Linguistics.
[30] Michael Jordan and Robert Jacobs. Hierarchical mixtures of experts and the. Neural computa-
tion, 6:181–, 01 1994.
[31] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert
McHardy. Challenges and applications of large language models, 2023.
[32] Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pióro, Michał Krutul, Szymon
Antoniak, Kamil Ciebiera, Krystian Król, Tomasz Odrzygó´zd´z, Piotr Sankowski, Marek Cygan,
and Sebastian Jaszczur. Scaling laws for fine-grained mixture of experts, 2024.
[33] Sirui Li, Janardhan Kulkarni, Ishai Menache, Cathy Wu, and Beibin Li. Towards foundation
models for mixed integer linear programming, 2025.
[34] Ziyue Li and Tianyi Zhou. Your mixture-of-experts llm is secretly an embedding model for free,
2024.
[35] Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu
Yang, Jian Jiao, Nan Duan, and Weizhu Chen. Rho-1: Not all tokens are what you need, 2025.
[36] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning Word Vectors for Sentiment Analysis. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguistics: Human Language Technologies,
pages 142–150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics.
11


--- Page 12 ---
[37] Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min,
Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita
Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers,
Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, and Hannaneh
Hajishirzi. Olmoe: Open mixture-of-experts language models, 2024.
[38] Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. Mteb: Massive text
embedding benchmark, 2023.
[39] Stefan K. Nielsen, Rachel S. Y. Teo, Laziz U. Abdullaev, and Tan M. Nguyen. Tight clusters
make specialized experts, 2025.
[40] Kazuma Obata, Tatsuya Aoki, Takato Horii, Tadahiro Taniguchi, and Takayuki Nagai. Lip-
llm: Integrating linear programming and dependency graph with large language models for
multi-robot task planning, 2024.
[41] Quang Pham, Giang Do, Huy Nguyen, TrungTin Nguyen, Chenghao Liu, Mina Sartipi, Binh T.
Nguyen, Savitha Ramasamy, Xiaoli Li, Steven Hoi, and Nhat Ho. Competesmoe – effective
training of sparse mixture of experts via competition, 2024.
[42] Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby. From sparse to soft
mixtures of experts, 2024.
[43] David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys,
and Adam Santoro. Mixture-of-depths: Dynamically allocating compute in transformer-based
language models, 2024.
[44] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, An-
dré Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of
experts, 2021.
[45] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André
Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts.
In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,
Advances in Neural Information Processing Systems, volume 34, pages 8583–8595. Curran
Associates, Inc., 2021.
[46] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts
layer, 2017.
[47] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts
layer, 2017.
[48] Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, and Yuxiong He. Scaling
vision-language models with sparse mixture of experts. In Houda Bouamor, Juan Pino, and
Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023,
pages 11329–11344, Singapore, December 2023. Association for Computational Linguistics.
[49] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew
Ng, and Christopher Potts. Recursive Deep Models for Semantic Compositionality Over a
Sentiment Treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural
Language Processing, pages 1631–1642, Seattle, Washington, USA, October 2013. Association
for Computational Linguistics.
[50] James C. Spall. A one-measurement form of simultaneous perturbation stochastic approximation.
Automatica, 33(1):109–112, 1997.
[51] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.
https://github.com/tatsu-lab/stanford_alpaca, 2023.
12


--- Page 13 ---
[52] Qwen Team. Qwen1.5-moe: Matching 7b model performance with 1/3 activated parameters",
February 2024.
[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural
Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
[54] Lean Wang, Huazuo Gao, Chenggang Zhao, Xu Sun, and Damai Dai. Auxiliary-loss-free load
balancing strategy for mixture-of-experts, 2024.
[55] Hongqiu Wu, Hai Zhao, and Min Zhang. Not all attention is all you need, 2021.
[56] Yike Yuan, Ziyu Wang, Zihao Huang, Defa Zhu, Xun Zhou, Jingyi Yu, and Qiyang Min. Expert
race: A flexible routing strategy for scaling diffusion transformer with mixture of experts, 2025.
[57] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong
Tian. Galore: Memory-efficient llm training by gradient low-rank projection, 2024.
[58] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai,
Zhifeng Chen, Quoc Le, and James Laudon. Mixture-of-experts with expert choice routing,
2022.
[59] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai,
zhifeng Chen, Quoc V Le, and James Laudon. Mixture-of-Experts with Expert Choice Routing.
In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in
Neural Information Processing Systems, volume 35, pages 7103–7114. Curran Associates, Inc.,
2022.
[60] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai,
zhifeng Chen, Quoc V Le, and James Laudon. Mixture-of-experts with expert choice routing.
In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in
Neural Information Processing Systems, volume 35, pages 7103–7114. Curran Associates, Inc.,
2022.
[61] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer,
and William Fedus. St-moe: Designing stable and transferable sparse expert models, 2022.
13


--- Page 14 ---
A
Appendix
Supplementary Material for “Unified Sparse Mixture of Experts”
This document is organized as follows: Appendix A.1 discuss related work on Linear Programming
and other studies relevant to our approach. We also discuss more about the Ablation Studies at
Appendix A.2. Appendix A.3 provides the theoretical proof supporting the discussion in Section 3.
Following this, Appendix A.4 additional experiment results that show the advantage of our method.
Appendix A.5 offers a detailed analysis of the Token Choice and Expert Choice routers and explains
why our method outperforms the baseline approaches. Appendix A.6 describes the implementation
details in full.
A.1
Related Work
Linear Programming. Linear programming, a foundational method in mathematical optimization,
has gained relevance in the domain of Large Language Models (LLMs), where task allocation
and dependency management among skills can be effectively modeled as constrained optimization
problems [40, 33]. Recent work such as [22] introduces an Integer Linear Programming (ILP)
formulation to jointly optimize expert placement by accounting for token load, communication
overhead, and computation cost. Framing SMoEs from the perspective of linear programming not
only offers deeper theoretical understanding but also opens promising directions for developing more
efficient and principled routing strategies.
The most relevant work to ours is Expert Race[56], which proposes a flexible routing strategy
for diffusion transformers. However, similar to the traditional Expert Choice (EC) approach, it
suffers from a critical limitation, information leakage - where a model unintentionally accesses
future tokens [43, 54], which becomes especially problematic in autoregressive language modeling
settings[43, 54]. In contrast, we propose a modified version of Expert Choice [58] by replacing the
softmax mapping function with a sigmoid function and limiting comparisons to within the token
dimension. This design effectively mitigates information leakage, making Expert Choice more
suitable for large language models.
Additionally, we introduce two novel modules: (1) the Unified Score, a linear combination of the
mapping functions from Token Choice and Expert Choice, and (2) the Unified Mechanism, which
jointly considers both expert and token dimensions when selecting experts. Both theoretical analysis
and empirical results demonstrate that our method outperforms traditional SMoE approaches in terms
of both performance and robustness.
A.2
Ablation Studies
We investigate the effectiveness and robustness of USMoE to the different hyper-parameter settings.
A.2.1
Unified Learning Strategy Comparison
The Unified Mechanism is implemented using two approaches: (1) a sequence-based method that
compares all tokens within a sequence (referred to as "USMoE-Sequence") and (2) a batch-based
method that compares all tokens within a batch or mini-batch (referred to as "USMoE-Batch").
We evaluate both approaches on the Classification task, with results presented in Table 10. The
findings indicate that both methods outperform the Expert Choice and Token Choice approaches,
demonstrating the effectiveness of our method. Notably, the sequence-based approach achieves
superior performance in the Classification task, as it ensures that no important tokens are missed
within a sequence - an assurance that the batch/mini-batch implementation may not always provide.
A.2.2
Robustness to the controlling factor
The Unified Score (α) enables the model to adjust its scoring mechanism, either favoring a diverse set
of experts per sequence or distributing experts more evenly across tokens. We evaluate the robustness
of the controlling factor α on the classification task using the DeepSeekMoE-16B model, with results
presented in Table 11. When α = 0.0, the scoring mechanism aligns with Token Choice, while at
α = 1.0, it follows Expert Choice.
Overall, USMoE demonstrates strong effectiveness within the range of α ∈(0.3, 0.7), striking a
balance between expert diversity and token importance. This range provides an optimal trade-off
between enforcing SMoE’s unified policy and enhancing traditional approaches for the task, leading
14


--- Page 15 ---
to superior overall performance. Notably, all tested α configurations outperform the Expert Choice
approach (α = 1.0).
A.3
Theoretical Proof for Section 3
Definition A.1 (General Routing Function R). Let S ∈RT ×N be the compatibility score matrix
between T tokens and N experts, and let c ∈N denote a global routing budget that constrains
the total number of token-to-expert assignments. Let TopKR, TopKC, and TopK denote the TopK
selection operators applied row-wise, column-wise, and globally, respectively. We define a general
routing function R(S, c, mode) that constructs a sparse binary routing matrix X ∈{0, 1}T ×N, where
xij = 1 indicates that token i is routed to expert j, as follows:
X = R(S, c, mode) =







TopK(S, c),
mode = USMoE,
TopKR(S,
 c
T

),
mode = Token Choice,
TopKC(S,
 c
N

),
mode = Expert Choice,
Lemma A.2. The Unified Mechanism (USMoE), as defined in Definition A.3, yields a superior
solution to the optimization problem in Equation 2 compared to both Token Choice and Expert Choice
mechanisms.
The proof is given in Appendix A, where Lemma A.3 establishes the superiority of USMoE over
Token Choice and Expert Choice.
A.3.1
Proof of Lemma A.3
Let the ordered arrays (St
1, St
2, . . . , St
T ), (Se
1, Se
2, . . . , Se
T ), and (Su
1 , Su
2 , . . . , Su
T ) represent the top-T
compatibility scores selected by the Token Choice, Expert Choice, and Unified Score mechanisms,
respectively. These scores are sorted in ascending order such that
St
1 ≤St
2 ≤· · · ≤St
T ,
Se
1 ≤Se
2 ≤· · · ≤Se
T ,
Su
1 ≤Su
2 ≤· · · ≤Su
T .
We claim the following inequality holds:
St
i ≤Su
i ,
∀i ∈[1, T],
Se
i ≤Su
i ,
∀i ∈[1, T].
(3)
Proof. Let x ∈RT ×d denote a matrix of token embeddings and E ∈Rd×N the matrix of expert
embeddings, where d is the embedding dimension, T is the number of tokens, and N is the number of
experts. The similarity matrix S ∈RT ×N between tokens and experts is computed via a dot product:
S = x · E.
(4)
Since the softmax function is monotonic, the top-k selection remains invariant under softmax
transformation:
TopK(softmax(S), k) = TopK(S, k).
(5)
For the Token Choice approach, the top similarity score for each token is defined as:
St
j = max
k∈[1,N] Sjk,
(6)
and across all tokens, the top-T scores are aggregated into {St
i}T
i=1.
In contrast, the Unified Mechanism selects the top-T scores globally across all token-expert pairs:
Su
j =
max
(q,k)∈[1,T ]×[1,N] Sqk.
(7)
By construction, it follows that:
St
i ≤Su
i ,
∀i ∈[1, T],
(8)
since Token Choice selects top experts per token independently, while Unified Score considers all
token-expert pairs jointly.
15


--- Page 16 ---
Similarly, in the Expert Choice approach, each expert selects its most compatible token:
Se
i = max
q∈[1,T ] Sqi,
∀i ∈[1, N],
(9)
and again, the top-T scores across experts are extracted as {Se
i }T
i=1. As the Unified approach
considers the global maximums, we have:
Se
i ≤Su
i ,
∀i ∈[1, T].
(10)
Combining (8) and (10), we conclude:
St
i ≤Su
i ,
Se
i ≤Su
i ,
∀i ∈[1, T],
(11)
which completes the proof.
A.3.2
Proof of Proposition 3.2
Proof. Following [9] and [18], we illustrate the representation collapse issue using the Jacobian
matrix approach. Specifically, the Jacobian matrix of the SMoE with respect to x ∈Rb×d is given as:
JSMoE = S(x)kJFFN +
n
X
j=1
S(x)k (δkj −Sj) E(x)ie⊤
j
=⇒JSMoE = S(x)kJFFN +
n
X
j=1
cje⊤
j ,
(12)
where cj = S(x)k (δkj −Sj) E(x)i. The first part of Equation 12, S(x)kJFFN, represents the
contribution from the input token and experts to the final output. The second part, (2) Pn
j=1 cje⊤
j
relates to learning an improved gating function to minimize task loss. Furthermore, Equation 12
should be updated as a linear combination of expert embeddings. Due to n << d in practice, the
above equation illustrates the representation.
Given S(x) = α × Se(x) + (1 −α) × St(x), where Se(x) and St(x) represent the similarity score
functions for expert choice and token choice, respectively, the Jacobian matrix of USMoE with
respect to x ∈Rb×d is expressed as:
JU = S(x)kJFFN +
n
X
j=1
Se(x)k (δkj −Se(x)j) E(x)ke⊤
j
+
n
X
j=1
St(x)k (δkj −St(x)j) E(x)ke⊤
j .
=⇒JU = J1 +
n
X
j=1
cje⊤
j +
n
X
j=1
dje⊤
j
(13)
where
J1 = S(x)kJFFN,
(14)
cj = Se(x)k (δkj −Se(x)j) E(x)ke⊤
j ,
(15)
dj = St(x)k (δkj −St(x)j) E(x)k.
(16)
Similar to the Jacobian matrix of SMoE as Equation 12, the Jacobian matrix of USMoE also consists
of two terms: (1) J1, which depends on the input token and experts for the final output; and (2)
P2n
j=1 oje⊤
j indicates to learn better gating function to minimize the task loss. Since 2n >> n,
USMoE is more effective than SMoE in addressing the representation collapse issue.
16


--- Page 17 ---
A.3.3
Proof of Lemma 3.2
Proof. Let S ∈Rt×n be the similarity matrix, and let c ∈N be a global routing budget. The goal is
to find a binary matrix X ∈{0, 1}t×n such that
t
X
i=1
n
X
j=1
xij ≤c
and the total score
M = ⟨S, X⟩=
t
X
i=1
n
X
j=1
Sijxij
is maximized.
Let XUSMoE = TopK(S, c) denote the binary matrix that marks the positions of the top-c largest
entries in S. By construction, XUSMoE sets exactly c entries of S with the highest values to 1 and all
others to 0.
Suppose there exists another feasible solution XT ∈{0, 1}t×n with
X
i,j
(XT )ij ≤c
and assume for contradiction that
⟨S, XT ⟩> ⟨S, XUSMoE⟩.
Since XT selects at most c entries from S, and the values of the selected entries are summed to
compute ⟨S, XT ⟩, the only way for ⟨S, XT ⟩to exceed ⟨S, XUSMoE⟩is if some entries selected by
XT are larger than those selected by XUSMoE. However, this contradicts the definition of XUSMoE as
containing the top-c largest values in S.
Therefore, for any feasible XT , we must have
⟨S, XT ⟩≤⟨S, XUSMoE⟩,
which proves that XUSMoE is the optimal solution.
A.4
Experimental details and additional results
A.4.1
Which Mapping Function for Expert Choice?
The Expert Choice (EC) approach faces scalability challenges for large language models due to the
issues of information leakage [54, 43] and token dropping [42]. We revisit this problem and identify
that these issues stem from the use of the softmax operator, which introduces information leakage
across tokens, and the top-k selection applied along the batch dimension.
To address this, we propose replacing the softmax mapping function with a sigmoid function to
eliminate inter-token information leakage. Additionally, we mitigate batch-wise leakage by applying
the top-k selection only along the sequence dimension.
Figure 6 compares the performance of EC using softmax versus sigmoid. The results show that the
sigmoid-based EC not only achieves better bits-per-character (BPC) performance but also reduces
the token dropping ratio. This improvement suggests that using the sigmoid mapping function has
strong potential for scaling up the EC approach in large language models.
A.4.2
Training-free additional results
In this section, we compare methods both with and without prompts, including PromptEOL [29].
Inspired by [34], we also compare our method with an approach that uses the similarity score
between router and expert embeddings as the hidden representation, which we refer to as "Router
Embedding" or simply "Router" Additionally, we evaluate against MoEE [34], which leverages both
Router Embedding and the hidden representation of the SMoE model as embeddings.
In this section, inspired by [34], we test our method as a plug-in framework on well-trained SMoE
models, including OLMoE-1B-7B [37]:, DeepSeekMoE-16B [12], Qwen1.5-MoE-A2.7B [52]. We
17


--- Page 18 ---
Figure 6: Performance comparison of the Expert Choice approach using two mapping functions: sigmoid and
softmax, evaluated on training performance and token dropping ratio (lower is better).
evaluate performance on a subset of tasks from the Massive Text Embedding Benchmark (MTEB)
[38], which covers key downstream applications for sentence embeddings, including Classifica-
tion, Clustering, Pair Classification, Re-ranking, Retrieval, Semantic Textual Similarity (STS), and
Summarization. Following the MTEB evaluation framework, we use Accuracy for Classification,
V-Measure for Clustering, Average Precision for Pair Classification, Mean Average Precision for
Re-ranking, nDCG for Retrieval, and Spearman’s correlation for STS and Summarization. Our
method consistently demonstrates performance improvements across a range of MTEB tasks in two
scenarios: (1) with PromptEOL [28] (denoted as "with prompts" for brevity), as shown in Table 5,
and (2) without prompts, as shown in Table 6. Detailed results for datasets under each task type are
provided in Appendix A. USMoE outperforms both the Expert Choice and Token Choice approaches
in most cases, underscoring the complementary nature of these two methods.
Interestingly, Router Embedding is less affected by prompting on the Classification dataset, as
shown in Figure 8a, while Token Choice, Expert Choice, and USMoE (ours) achieve significant
performance improvements in the prompting setting. Figure 8a also demonstrates that our method
is not only more effective but also more stable than the baselines, as indicated by a lower standard
deviation. Additionally, Figure 8b illustrates the distribution of our method across MTEB tasks in
both prompted and non-prompted scenarios. Overall, our approach outperforms the baselines in
terms of performance while exhibiting lower variance across multiple tasks and different runs.
We provide a detailed evaluation of three state-of-the-art SMoE models: OLMoE-1B-7B (Table 7),
Qwen1.5-MoE-A2.7B (Table 8), and DeepSeekMoE-16B (Table 9). Our results demonstrate the
effectiveness of our method across various models and prompts, comparing its performance against
baseline approaches such as Token Choice (TC) and Expert Choice (EC).
A.4.3
Training from scratch
Large models training. USMoE not only delivers strong performance in base model training but
also remains highly competitive at a large scale. Table 12 presents perplexity (PPL) results on the
18


--- Page 19 ---
Model
Task
Router
TC
EC
MoEE
USMoE
OLMoE-1B-7B
Classification
43.1
57.7
56.2
51.7
61.4
Clustering
16.2
24.8
26.9
23.2
32.1
PairClassification
53.5
62.0
58.9
66.0
68.9
Reranking
41.7
51.3
51.0
53.2
55.1
STS
49.4
63.5
44.2
67.8
71.1
Summarization
25.6
28.9
29.7
30.4
30.5
Average
38.3
48.0
44.5
48.7
53.2
Qwen1.5-MoE-A2.7B
Classification
48.8
58.0
35.2
54.0
59.7
Clustering
14.3
34.2
29.2
30.1
37.5
PairClassification
51.9
60.5
56.0
60.3
66.6
Reranking
41.0
46.6
45.0
51.1
56.8
STS
48.3
50.1
50.0
64.3
69.0
Summarization
27.0
23.0
21.9
27.3
31.0
Average
38.6
45.4
39.6
47.9
53.4
DeepSeekMoE-16B
Classification
48.6
56.4
55.4
53.0
60.4
Clustering
17.8
29.0
20.3
28.5
32.8
PairClassification
57.4
59.8
53.8
63.3
67.9
Reranking
43.8
45.7
40.9
50.6
52.4
STS
52.8
49.0
37.1
63.4
68.1
Summarization
29.1
24.4
25.7
29.2
30.7
Average
41.6
44.0
38.9
48.0
52.0
Table 5: Performance comparison of USMoE, Token Choice (TC), Expert Choice (EC), and MoEE across across
MTEB Tasks with PromptEOL [28]. The best result for each row is highlighted in bold.
Classif
Clustering
PairClassif
Reranking
STS
Summ
20
40
60
58.0
34.2
60.5
46.6
50.1
23.0
58.0
59.7
37.5
66.6
56.8
69.0
31.0
59.7
Qwen1.5-MoE-A2.7B
Classif
Clustering
PairClassif
Reranking
STS
Summ
20
40
60
57.7
24.8
62.0
51.3
63.5
28.9
57.7
61.4
32.1
68.9
55.1
71.1
30.5
61.4
OLMoE-1B-7B
Classif
Clustering
PairClassif
Reranking
STS
Summ
20
40
60
56.5
29.0
59.8
45.7
49.0
24.4
56.5
60.4
32.8
67.9
52.4
68.1
30.7
60.4
DeepSeekMoE-16B
TC
EC
USMoE
USMoE 
 vs TC
Figure 7: Performance comparison of USMoE, Token Choice (TC), Expert Choice (EC), and MoEE across
across MTEB Tasks with PromptEOL [28]. The best result for each row is highlighted in bold.
WikiText-103 and One Billion Words datasets using a large Transformer-XL model with 15 SMoE
layers, 16 experts, and 420M parameters. The performance gap between USMoE and the baselines
becomes even more pronounced at this scale, highlighting its strong scalability with increasing model
complexity. Regardless of backbone size or the number of activated experts, USMoE consistently
outperforms all baselines, demonstrating its effectiveness in scaling up large language models.
Training from scratch.
To assess the effectiveness of our method, we compare USMoE with
the Token Choice approaches, including SMoE [27], SMoE-Dropout (abbreviated as "SMoE-DR"),
XMoE [10], and StableMoE [13], as well as the Expert Choice approach [58] for pre-training and fine-
tuning tasks. We follow the approach of [7] and use a base Transformer-XL [14] with four decoder
layers. We train both base and large-scale versions of Transformer-XL on four datasets (Enwik8,
Text8, Wikitext-103, and One Billion Words) for 100k iterations, following the implementation in [7].
Then we fine-tune the pre-trained weights for text classification tasks, including SST-2 [49], SST-5
[49], IMDB [36], and BANKING77 [4]. More implementation details and additional results are provided
in the Appendix A.
19


--- Page 20 ---
Model
Task
Router
TC
EC
MoEE
USMoE
OLMoE-1B-7B
Classification
41.2
43.4
44.9
41.8
51.3
Clustering
13.7
14.7
12.0
14.5
21.0
PairClassification
45.3
39.1
35.5
45.7
61.7
Reranking
37.5
37.4
35.3
39.5
46.5
STS
39.9
24.1
18.2
39.9
59.6
Summarization
28.4
20.9
21.1
29.8
31.8
Average
34.3
29.9
27.8
35.2
45.3
Qwen1.5-MoE-A2.7B
Classification
43.8
50.3
25.5
47.7
52.2
Clustering
13.6
27.4
23.2
25.2
29.9
PairClassification
45.9
46.9
43.4
51.5
56.2
Reranking
39.6
45.3
41.6
48.5
53.1
STS
38.8
38.0
35.6
51.8
60.7
Summarization
28.3
13.4
15.1
31.2
40.0
Average
35.0
36.9
30.7
42.6
48.7
DeepSeekMoE-16B
Classification
43.4
46.6
44.7
44.4
49.4
Clustering
13.4
18.1
13.5
17.8
21.0
PairClassification
45.5
40.9
37.1
46.1
53.5
Reranking
38.5
38.9
35.1
42.2
45.7
STS
37.7
26.3
23.3
40.2
51.2
Summarization
24.9
22.0
18.5
24.4
29.9
Average
33.9
32.1
28.7
35.9
41.8
Table 6: Performance comparison of USMoE, Token Choice (TC), Expert Choice (EC), and MoEE across
MTEB Tasks without prompts and models. The best result for each row is highlighted in bold.
(a) Performance comparison of USMoE, Token
Choice (TC), and Expert Choice (EC) on Classifi-
cation Task.
(b) Results distribution of USMoE, Token
Choice (TC), Expert Choice (EC), and MoEE
across MTEB Tasks
Figure 8: Illustration of comparing the performance of USMoE, Token Choice (TC), Expert Choice (EC), and
MoEE across MTEB tasks and three SMoE models. Each benchmark is run 10 times, reporting both the mean
and standard deviation to highlight the performance and stability of our method compared to the baselines.
Fine-tuning. We report the results of the fine-tuning experiment on the SST-2, SST-5, IMDB, and
BANKING77 datasets in Table 13, using Transformer-XL pre-trained on enwik8. Overall, USMoE
consistently achieves higher accuracy compared to other baselines across all datasets. The results
demonstrate that our method is not only effective for pre-training tasks but also performs effectively
on existing pre-trained models.
A.5
In-depth Analysis
We visualize the router behavior of USMoE in Figure 11 and contrast it with the router behaviors
of the Token Choice approach (Figure 9) and the Expert Choice approach (Figure 10). Notably, the
router in the OLMoE-1B-7B model exhibits a strong preference for specific experts. For instance, in
the Emotion Classification task, Experts 8, 30, and 58 are consistently prioritized in both the Token
Choice and Expert Choice approaches. This bias limits the model’s adaptability and effectiveness
for downstream tasks. USMoE tackles this challenge by introducing the Unified Mechanism, which
promotes more balanced and diverse expert selections, as illustrated in Figure 11. This enhancement
enables USMoE to outperform the baselines on the Emotion Classification task.
We track the number of unique experts utilized by the OLMoE-1B-7B model for each sequence
in the Emotion Classification task as Figure 12a. Our analysis reveals that the Expert Choice
20


--- Page 21 ---
Category
Model
Dataset
Setting
Router
TC
EC
MoEE
USMoE
Classification
OLMoE
Emotion
None
24.1
24.5
26.3
25.1
35.8
Prompt
27.6
49.9
49.0
44.5
54.8
Toxic
None
51.9
58.9
59.9
51.9
62.8
Prompt
52.3
65.2
61.3
53.4
67.6
Tweet
None
47.7
46.8
48.3
48.4
55.2
Prompt
49.5
58.0
58.4
57.2
61.7
Clustering
OLMoE
Medrxiv
None
15.0
17.6
14.8
17.4
20.6
Prompt
15.8
23.9
27.7
22.0
28.2
20Groups
None
12.4
11.8
9.2
11.5
21.3
Prompt
16.7
25.7
26.2
24.4
36.0
Pair Classification
OLMoE
SemEval
None
43.6
35.8
31.3
43.6
50.2
Prompt
45.7
46.7
40.9
53.8
55.4
URLCorpus
None
47.0
42.4
39.7
47.8
73.2
Prompt
61.4
77.4
76.9
78.2
82.3
Reranking
OLMoE
Ask
None
41.3
41.0
39.0
41.4
47.1
Prompt
43.4
51.9
49.9
50.2
51.6
SciDocs
None
45.5
46.3
46.9
50.8
59.1
Prompt
53.6
69.6
73.1
75.1
77.2
StackOver
None
25.8
24.8
20.1
26.4
33.2
Prompt
28.1
32.5
30.0
34.3
36.6
STS
OLMoE
Biosses
None
39.3
13.6
7.7
29.7
64.0
Prompt
51.2
61.8
67.6
70.2
75.2
SickR
None
50.3
46.3
26.4
53.0
58.9
Prompt
51.9
65.7
37.6
66.1
66.5
STS12
None
40.1
8.6
11.1
37.8
58.2
Prompt
51.3
53.8
37.5
63.6
66.4
STS13
None
40.5
21.1
18.2
43.4
61.5
Prompt
52.5
66.5
40.4
72.7
76.4
STS14
None
29.5
13.4
13.3
31.7
52.9
Prompt
41.1
56.8
33.9
64.2
68.2
STS15
None
30.8
27.8
22.5
33.3
63.2
Prompt
46.4
69.3
38.4
66.4
72.5
STS16
None
46.5
38.9
28.9
45.8
60.7
Prompt
52.4
70.1
49.4
68.3
71.8
STSBen
None
42.2
23.4
17.5
44.5
57.4
Prompt
48.6
63.6
48.9
70.7
72.1
Summarization
OLMoE
Medrxiv
None
28.4
20.9
21.1
29.8
31.8
Prompt
25.6
28.9
29.7
30.4
30.5
Table 7: Performance comparison of USMoE, Token Choice (TC), Expert Choice (EC), and MoEE across
MTEB tasks using the OLMoE model. The best result for each row is highlighted in bold.
approach employs 11 out of 16 experts, indicating a lower level of specialization among experts.
In contrast, both USMoE and the Token Choice approach use an average of 0.9 to 1 expert per
sequence, demonstrating superior expert specialization. Furthermore, we analyze the token dropping
behavior of the Expert Choice approach and observe a significant increase in dropping rates when
scaling to larger datasets or models, such as pre-training the Transformer-XL Large model on the
One Billion Word dataset, as shown in Figure 12b. This increase in dropping rates may negatively
impact model performance. In contrast, our method maintains a consistently low dropping rate
(<0.1), demonstrating its superiority over the Expert Choice approach for scalability. Additionally,
our method proves more robust than the Token Choice approach, as it effectively drops irrelevant
tokens without compromising performance.
A.6
Implementation Details
For the Without Training experiments, we implement our method based on the publicly available
MoEE implementation [34]1. Due to resource constraints, we validate our method and the baselines
1https://github.com/tianyi-lab/MoE-Embedding
21


--- Page 22 ---
Category
Model
Dataset
Setting
Router
TC
EC
MoEE
USMoE
Classification
Qwen1.5-MoE-A2.7B
Emotion
None
27.2
33.9
30.1
34.3
37.1
Prompt
37.0
48.5
47.4
47.2
51.3
Toxic
None
53.0
61.1
21.3
52.9
61.4
Prompt
53.4
64.5
19.8
54.1
65.5
Tweet
None
51.1
55.9
25.1
55.9
58.2
Prompt
56.1
61.1
38.6
60.7
62.3
Clustering
Qwen1.5-MoE-A2.7B
Medrxiv
None
15.3
23.3
21.3
23.0
25.3
Prompt
14.2
24.6
19.8
21.8
27.9
20Groups
None
12.0
31.5
25.1
27.4
34.4
Prompt
14.4
43.8
38.6
38.4
47.0
Pair Classification
Qwen1.5-MoE-A2.7B
SemEval
None
42.0
38.8
34.7
42.5
47.5
Prompt
47.0
52.4
46.3
52.4
57.7
URLCorpus
None
49.8
54.9
52.1
60.6
64.8
Prompt
56.7
68.7
65.8
68.2
75.5
Reranking
Qwen1.5-MoE-A2.7B
Ask
None
43.1
45.8
43.4
47.3
49.1
Prompt
43.3
48.3
49.1
49.5
52.8
SciDocs
None
49.6
60.6
55.3
67.0
71.6
Prompt
50.9
60.1
55.8
68.7
73.0
StackOver
None
26.2
29.5
26.2
31.1
38.5
Prompt
28.8
31.3
30.2
35.2
44.5
STS
Qwen1.5-MoE-A2.7B
Biosses
None
33.8
32.5
34.7
49.6
66.6
Prompt
55.1
55.8
48.5
68.4
74.5
SickR
None
51.0
55.5
40.4
61.0
63.5
Prompt
50.2
59.7
51.1
64.3
69.1
STS12
None
40.2
16.9
18.6
46.3
52.3
Prompt
49.3
25.0
31.8
59.2
62.5
STS13
None
38.1
42.9
44.2
56.7
67.3
Prompt
53.3
57.5
54.6
73.4
75.6
STS14
None
28.1
26.5
25.6
45.4
53.7
Prompt
40.4
38.8
40.7
60.0
64.8
STS15
None
34.8
40.5
38.4
46.1
56.1
Prompt
40.7
52.3
54.2
58.8
66.8
STS16
None
47.6
51.0
48.1
58.1
64.4
Prompt
51.6
64.2
65.1
65.7
68.7
STSBen
None
37.0
37.7
34.7
50.9
61.6
Prompt
45.6
47.8
54.5
64.5
70.0
Summarization
Qwen1.5-MoE-A2.7B
Medrxiv
None
28.3
13.4
15.1
31.2
40.0
Prompt
27.0
23.0
21.9
27.3
31.0
Table 8: Performance comparison of USMoE, Token Choice (TC), Expert Choice (EC), and MoEE across
MTEB Tasks with Qwen1.5-MoE-A2.7B models. The best result for each row is highlighted in bold.
using 4-bit quantization with a batch size of 128. For the OLMoE-1B-7B model, we conduct
experiments on a single H100 GPU, while for the Qwen1.5-MoE-A2.7B and DeepSeekMoE-16B
models, we utilize two H100 GPUs.
The base Transformer-XL variant [7] comprises four Transformer decoder layers, each with an
input dimension of 256. Each layer includes a self-attention mechanism with eight attention heads,
followed by a Feed-forward Neural Network (FFN) that has an inner dimension of 512. The dropout
ratio is set at 0.1. We divide the FFN into 16 experts, each with the same dimensions. For the larger
variants, we scale the model up to twelve layers.
Our experiments are based on the publicly available SMoE-Dropout implementation [7]2. The
pre-training experiments were conducted using a single H100 GPU, while the fine-tuning experiments
were performed on a single A100 GPU. It is important to note that parallel training on multiple GPUs
may produce different results.
A.6.1
Pre-training Experiments
We provide the USMoE implementation details for pre-training our Transformer-XL base and large
on enwik8, text8, WikiText-103, and One Billion Word in Table 15.
2https://github.com/VITA-Group/Random-MoE-as-Dropout
22


--- Page 23 ---
Category
Model
Dataset
Setting
Router
TC
EC
MoEE
USMoE
Classification
DeepSeekMoE-16B
Emotion
None
26.1
27.4
26.5
27.6
31.3
Prompt
37.9
48.3
46.1
46.4
52.6
Toxic
None
53.3
60.4
58.1
53.1
61.7
Prompt
53.1
62.4
62.5
53.6
67.5
Tweet
None
51.0
51.9
49.5
52.6
55.2
Prompt
54.9
58.4
57.5
58.9
61.0
Clustering
DeepSeekMoE-16B
Medrxiv
None
15.1
23.0
17.3
22.0
25.7
Prompt
17.0
25.7
20.9
24.0
27.9
20Groups
None
11.7
13.2
9.7
13.7
16.2
Prompt
18.6
32.3
19.8
33.0
37.6
Pair Classification
DeepSeekMoE-16B
SemEval
None
44.6
40.2
32.6
43.5
44.6
Prompt
48.4
47.2
46.6
51.3
55.7
URLCorpus
None
46.4
41.7
41.6
48.6
62.4
Prompt
66.5
72.4
61.1
75.4
80.0
Reranking
DeepSeekMoE-16B
Ask
None
41.7
41.1
40.1
42.3
44.9
Prompt
43.5
43.8
44.7
46.9
50.6
SciDocs
None
48.2
50.6
44.7
57.1
61.9
Prompt
58.3
65.6
55.3
72.6
72.9
StackOver
None
25.7
24.9
20.4
27.3
30.2
Prompt
29.7
27.6
22.6
32.3
33.6
STS
DeepSeekMoE-16B
Biosses
None
29.5
31.7
27.7
26.8
56.6
Prompt
47.0
40.1
41.5
57.6
67.2
SickR
None
50.4
47.4
29.4
53.1
60.1
Prompt
56.0
61.9
38.7
65.8
68.0
STS12
None
44.0
4.3
13.9
45.0
48.6
Prompt
57.8
31.0
28.4
64.0
65.9
STS13
None
36.0
28.4
27.5
41.1
50.7
Prompt
55.3
56.0
41.2
70.9
76.1
STS14
None
25.4
12.0
13.0
28.2
41.2
Prompt
44.9
41.0
31.1
58.6
65.1
STS15
None
34.8
33.9
25.6
38.7
45.0
Prompt
49.7
46.5
33.0
58.5
64.1
STS16
None
44.9
34.4
33.1
46.9
55.2
Prompt
56.7
58.0
44.2
64.5
67.4
STSBen
None
36.6
18.3
15.8
42.1
51.9
Prompt
54.9
57.7
39.0
67.8
71.0
Summarization
DeepSeekMoE-16B
Medrxiv
None
24.9
22.0
18.5
24.4
29.9
Prompt
29.1
24.4
25.7
29.2
30.7
Table 9: Performance comparison of USMoE, Token Choice (TC), Expert Choice (EC), and MoEE across
MTEB Tasks with DeepSeekMoE-16B models. The best result for each row is highlighted in bold.
Model
Dataset
TC
EC
USMoE-Sequence
USMoE-Batch
DeepSeekMoE-16B
Emotion
27.4
26.5
27.8
27.4
Toxic
60.4
58.1
60.1
59.2
Tweet
51.9
49.5
52.5
51.7
Table 10: Comparison of USMoE, Token Choice (TC), and Expert Choice (EC) on the classification task. Higher
values are better, with the best results highlighted in bold.
Model
Dataset
α
0.0
0.3
0.5
0.7
0.9
1.0
DeepSeekMoE-16B
Emotion
27.4
27.1
27.8
27.6
27.7
26.5
Toxic
60.4
60.0
60.1
56.8
57.3
58.1
Tweet
51.9
53.2
52.5
53.3
52.9
49.5
Table 11: Performance comparison of DeepSeekMoE-16B across different classification datasets with varying α
values. Higher is better; best results are in bold.
A.6.2
Fine-tuning Experiments
To perform the fine-tuning experiments, we utilize the same model architecture as in the pre-training
phase. Table 16 presents the implementation details for the fine-tuning experiments conducted across
four different datasets.
23


--- Page 24 ---
Transformer-XL(420M)
WikiText-103
lm1b
Topk
TC
EC
USMoE
TC
EC
USMoE
1
31.70
35.52
25.48
58.65
65.43
56.90
2
22.42
23.30
22.06
44.56
43.39
40.53
4
23.57
23.60
22.65
45.52
43.70
40.90
8
24.20
24.37
22.88
46.36
44.22
43.24
Table 12: Large Scale performance comparison of USMoE, Token Choice (TC), and Expert Choice (EC) across
multiple datasets, with perplexity on the WikiText-103 and One Billion Word test sets. Lower values are better,
with the best results highlighted in bold.
Transformer-XL(20M)
FLOPs(x1010)
SST-2
SST-5
IMDB
BANKING77
USMoE (Topk=2)
7.7620
81.5
40.1
88.5
87.8
(Topk=1.5)
6.6753
83.8
39.6
88.3
83.0
TC (Topk=2)
SMoE
7.7620
77.1
35.1
84.4
69.2
SMoE-DR
78.6
34.4
83.5
66.7
XMoE
76.7
35.3
83.3
67.4
StableMoE
77.7
34.3
83.9
60.8
EC (Topk=2)
7.7620
81.5
39.3
88.0
75.6
Table 13: Accuracy performance comparison of USMoE, Token Choice (TC), and Expert Choice (EC) after
fine-tuned on various datasets. Higher is better, best results are in bold.
Transformer-XL(20M)
Enwik8
Text8
WikiText-103
lm1b
USMoE (Topk=2)
1.18
1.20
29.20
56.90
(Topk=1.5)
1.19
1.28
30.67
57.55
TC (Topk=2)
SMoE
1.20
1.29
30.16
58.00
SMoE-DR
1.56
1.56
58.37
93.17
XMoE
1.21
1.28
30.34
58.33
StableMoE
1.20
1.28
29.97
58.25
EC (Topk=2)
1.18
1.24
29.83
58.60
Table 14: Performance comparison of USMoE, Token Choice (TC), and Expert Choice (EC) across multiple
datasets, with BPC on the Enwik8 and Text8 test sets, and perplexity on the WikiText-103 and One Billion Word
test sets. Lower values are better, with the best results highlighted in bold.
Table 15: Implementation details for pre-training experiments on enwik8, text8, WikiText-103, and One
Billion Word datasets.
Dataset
Input length
Batch size
Optimizer
Lr
# Iterations
enwik8
512
48
Adam
2.5e-4
100k
text8
512
48
Adam
2.5e-4
100k
WikiText-103
512
22
Adam
2.5e-4
100k
One Billion Word
512
11
Adam
2.5e-4
100k
Table 16: Implementation for fine-tuning experiments on downstream tasks.
Dataset
Input length
Batch size
Optimizer
Lr
# Epochs
SST-2
512
16
Adam
1e-4
15
SST-5
512
16
Adam
1e-4
15
IMDB
512
4
Adam
1e-4
15
BANKING77
512
16
Adam
1e-4
15
Limitations
Our study focuses on enhancing the efficiency and effectiveness of Large Language Models (LLMs)
through SMoE. Our approach proves effective in both training and non-training settings. While the
results are promising, our pre-training experiments were constrained by computational resources,
limiting us to medium-scale datasets and a base Transformer-XL model. Consequently, further
24


--- Page 25 ---
Figure 9: Token Choice Router visualization for the OLMoE-1B-7B model on the Emotion Classification task.
The scores of selected experts are replaced with -10.0 (lower than the minimum score) to enhance visualization.
Best viewed in color.
Figure 10: Expert Choice Router visualization for the OLMoE-1B-7B model on the Emotion Classification
task. The scores of selected experts are replaced with -10.0 (lower than the minimum score) to enhance
visualization. Best viewed in color.
empirical evaluation is required to assess the scalability of USMoE beyond 100B parameters and
compare it with other SMoE strategies in modern LLMs.
Ethics Statement
Despite encouraging results, training large-scale LLMs remains highly resource-intensive, requiring
careful management of computational costs. Additionally, our study relies on web-sourced data,
which is known to contain gender and racial biases, highlighting the need for further efforts to
mitigate these issues. Lastly, while our work represents a significant step toward advancing LLMs
development, it also emphasizes the importance of robust regularization to prevent potential misuse
in harmful applications.
25


--- Page 26 ---
Figure 11: USMoE Router visualization for the OLMoE-1B-7B model on the Emotion Classification task. The
scores of selected experts are replaced with -10.0 (lower than the minimum score) to enhance visualization. Best
viewed in color.
(a) Number Experts per Sequence of US-
MoE, TC, and EC on Emotion dataset.
(b) Token Dropping of USMoE, Token Choice (TC), Ex-
pert Choice (EC) for Pre-training on One Billion Word
dataset.
Figure 12: Comparison of the number of experts per sequence for USMoE, Token Choice (TC), and Expert
Choice (EC) on the Emotion dataset using the OLMoE-1B-7B model, along with a comparison of token dropping
rates for USMoE, TC, and EC during pre-training on the One Billion Word dataset.
26
