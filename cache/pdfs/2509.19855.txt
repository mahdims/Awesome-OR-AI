--- Page 1 ---
1
CollaPipe: Adaptive Segment-Optimized Pipeline
Parallelism for Collaborative LLM Training in
Heterogeneous Edge Networks
Jiewei Chen, Xiumei Deng, Zehui Xiong, Shaoyong Guo, Xuesong Qiu, Ping Wang, Dusit Niyato
Abstract‚ÄîThe increasing demand for intelligent mobile appli-
cations has made multi-agent collaboration with Transformer-
based large language models (LLMs) essential in mobile edge
computing (MEC) networks. However, training LLMs in such
environments remains challenging due to heavy computation,
high end-to-end latency, and limited model generalization. We
introduce CollaPipe, a hybrid distributed learning framework
that integrates collaborative pipeline parallelism with federated
aggregation to support self-evolving intelligent networks. In Col-
laPipe, the encoder part is adaptively partitioned into variable-
sized segments and deployed across mobile devices for pipeline-
parallel training, while the decoder is deployed on edge servers to
handle generative tasks. Then we perform global model update
via federated aggregation. To enhance training efficiency, we
formulate a joint optimization problem that adaptively allocates
model segments, micro-batches, bandwidth, and transmission
power. We derive and use a closed-form convergence bound
to design an Dynamic Segment Scheduling and Resource Al-
location (DSSDA) algorithm based on Lyapunov optimization,
ensuring system stability under long-term constraints. Extensive
experiments on downstream tasks with Transformer and BERT
models show that CollaPipe improves computation efficiency by
up to 15.09%, reduces end-to-end latency by at least 48.98%, and
cuts single device memory usage by more than half, enabling
online learning in heterogeneous and dynamic communication
environments.
Index Terms‚ÄîFederated Learning, Pipeline Parallelism, Trans-
former, Large Language Models, Resource Allocation, Mobile
Edge Computing.
I. INTRODUCTION
With the rapid development of artificial intelligence gener-
ated content (AIGC) technologies in mobile Internet of Things
(IoT), AI agent systems powered by large language models
(LLMs) are emerging as a critical enabler for next-generation
intelligent applications in mobile edge computing (MEC)
networks [1]‚Äì[3]. By leveraging the strong generalization and
content generation capabilities of LLMs, such systems can
facilitate autonomous learning and decision-making across
distributed IoT devices, supporting diverse scenarios such as
Jiewei Chen, Shaoyong Guo, and Xuesong Qiu are with the State Key
Laboratory of Networking and Switching Technology, Beijing University of
Posts and Telecommunications, Beijing, China (e-mail: {chenjiewei, syguo,
xsqiu}@bupt.edu.cn). Xiumei Deng is with the Singapore University of
Technology and Design, Singapore (e-mail: xiumei_deng@sutd.edu.sg). Ze-
hui Xiong is with the School of Electronics, Electrical Engineering and
Computer Science, Queen‚Äôs University Belfast, United Kingdom (e-mail:
z.xiong@qub.ac.uk). Ping Wang is with Dept. of Electrical Engineering &
Computer Science, Lassonde School of Engineering, York University, Canada
(e-mail: ping.wang@lassonde.yorku.ca). Dusit Niyato is with College of
Computing and Data Science, Nanyang Technological University, Singapore
(e-mail: dniyato@ntu.edu.sg).
UAV-based inspection in substations [4] and intelligent main-
tenance with industrial robots [5]. In the future, MEC networks
are expected to accelerate the deployment of distributed LLM
services by providing massive data, idle computing resource,
and native support for autonomous collaboration and continual
learning across heterogeneous networks [6], [7].
However, optimizing LLMs typically requires access to
sensitive user data, raising privacy concerns and limiting
adoption in many domains. To address this, recent research
has explored the use of LLM-based federated learning (FL) to
enable collaborative training and inference without centralized
data sharing [8]‚Äì[10]. By integrating distributed knowledge
through local model training, FL paradigm mitigates the risk of
user data leakage while enhancing the generalization of edge
models by embedding global knowledge [11]‚Äì[13]. Despite
these benefits, the FL framework faces significant challenges
in practical deployment for LLM-driven IoT applications.
First, the substantial computational and memory demands of
LLMs often exceed the capacity of individual devices. Second,
collaborative distributed training of LLMs entails frequent
communication, where the FL paradigm suffers from excessive
communication overhead and inefficient resource utilization
[14].
Recent efforts have aimed to improve the computational
and communication efficiency of LLM training through split
learning frameworks. For instance, Wang et al. [15] proposed
a split federated learning approach for fine-tuning pre-trained
models, where the computationally intensive encoder is placed
on the edge server, while the embedding and task-specific
modules are deployed on edge devices. While this approach
reduces the computational burden on devices, it shifts most
of the workload to the edge server and lacks a collaboration
mechanism for constructing a global model across distributed
nodes. As a result, it fails to support cross-domain collabora-
tive learning, particularly in IoT scenarios where data remains
siloed across diverse domains.
Existing distributed intelligence paradigms also face lim-
itations in accommodating the architectural complexity of
Transformer-based LLMs. Traditional split learning and par-
titioning approaches typically focus on identifying an opti-
mal cut layer between device-side and server-side models
[16]. However, this coarse-grained partitioning often fails to
fully utilize available network and computational resources,
resulting in limited learning performance and system ef-
ficiency [17]. Conversely, overly fine-grained segmentation
of LLMs increases the frequency of inter-device commu-
arXiv:2509.19855v1  [eess.SY]  24 Sep 2025


--- Page 2 ---
2
nication, introducing significant overhead and potential bot-
tlenecks. Implementing adaptive LLM training in heteroge-
neous networks presents a critical challenge. Zheng et al.
[18] demonstrated a tailored 6G architecture and a resource
management scheme to support split edge learning for LLM
training on mobile devices. However, key factors, such as
the interplay between communication overhead and resource
allocation strategy, as well as the impact of device participation
scale on overall learning performance for the LLM, remain
insufficiently explored [19]. Most existing wireless learning
frameworks treat communication and training as separate
processes, without considering how the collaborative learn-
ing paradigm influences both model performance and overall
system efficiency [20]‚Äì[23]. In addition, current collaborative
intelligence paradigms primarily concentrate on split learning
for deep neural networks (DNNs) [24]‚Äì[26], while offering
limited exploration of quantitative evaluations for training
Transformer-based LLMs and enabling generative AI tasks.
Therefore, integrating learning performance and communica-
tion efficiency into a bidirectional optimization framework,
achieving a dynamic balance through joint scheduling of re-
sources and learning strategies, has become a critical direction
for distributed collaborative intelligence in IoT.
To address these challenges, we propose CollaPipe, a hybrid
distributed learning framework integrates pipeline parallelism
and federated aggregation. CollaPipe incorporates key sys-
tem parameters, including the number of LLM segments,
participating devices, and transmission power, into a uni-
fied optimization framework to support collaborative LLM
training. Furthermore, we design a long-term dynamic device
scheduling and resource allocation strategy to minimize end-
to-end system latency while ensuring convergence of hybrid
parallel training. To the best of our knowledge, this work is
the first to introduce an adaptive collaboration framework that
unifies pipeline parallelism and FL within edge-device net-
work architectures, supporting the autonomous and continuous
evolution of mobile AI agents at the network edge. Our key
contributions are summarized as follows:
1) We develop a hybrid distributed learning framework
(CollaPipe) over heterogeneous edge networks by de-
composing the LLM into embedding, encoder, and de-
coder (for generative tasks) modules. Specifically, the
embedding module is deployed on the control units
(CUs) within each cluster, the decoder is hosted on
the edge server, and the computationally intensive en-
coder is partitioned into variable-sized segments, which
are adaptively deployed across heterogeneous devices
within each cluster. These modules are sequentially
connected via wireless links, enabling efficient forward
and backward propagation across the network. By fa-
cilitating fine-grained coordination of data, model com-
ponents, and computational resources while preserving
data privacy, CollaPipe supports efficient cross-domain
collaboration. Compared to existing distributed learning
paradigms, it offers a more effective balance between
generalization performance and resource efficiency un-
der constrained wireless edge environments.
2) We propose an optimized pipeline parallelism mecha-
nism that adaptively determines the number of micro-
batches (m) and model segments (S), reduce train-
ing runtime in clustered edge environments. A closed-
form expression is derived to quantify both the latency
and energy consumption when K devices cooperatively
process S segments using pipeline parallelism. Fur-
thermore, we provide the first convergence analysis of
CollaPipe. Our analysis highlights the necessity of con-
sistent communication and computation times per seg-
ment and reveals how the number of encoder segments
and wireless uplink interference affect overall system
performance. Based on these insights, we formulate a
convergence-aware online system latency minimization
problem, which jointly optimizes encoder partitioning,
intra-cluster device scheduling, inter-cluster bandwidth
allocation, and power control, under a T-round conver-
gence constraint.
3) To address the complexity of jointly optimizing device
scheduling and resource allocation across communica-
tion rounds, we employ Lyapunov theory to decouple the
problem into two alternating subproblems, each corre-
sponding to a single round. System stability is evaluated
through a convergence gap metric. Building on this for-
mulation, we design a Dynamic Segment Scheduling and
Resource Allocation (DSSDA) algorithm and provide a
structural analysis of the subproblems to establish the
theoretical optimality of the proposed online algorithm.
4) We conduct extensive experiments to validate the ef-
fectiveness of the proposed framework and optimization
algorithm. Using both the Transformer and BERT mod-
els, we evaluate performance on the machine translation,
named entity recognition and sentence classification
tasks. Experimental results demonstrate that our scheme
not only achieves competitive training and inference per-
formance but also consistently outperforms conventional
pipeline-parallel and FL approaches, with improvements
of up to 2.76% in inference accuracy, and 48.98% in
training efficiency.
The rest of the paper is organized as follows. In Section
II, we introduce the related work on Transformer-based LLM
partition, pipeline parallelism and edge computing. Then, Sec-
tion III provides the CollaPipe framework for the distributed
training. In Section IV, we analyze the convergence behavior
and provide the problem formulation. Section V proposes
problem transformation method and the DSSRA algorithm.
Experiments and results are provided in Section VI. Finally,
Section VII concludes this paper. Table I lists the main
notations used in this paper.
II. PRELIMINARIES AND RELATED WORK
A. Model Parallelism and Edge Computing
In the field of parallel computing for large-scale models,
most existing works focus on co-training a complete model
across multiple accelerators (i.e. GPU) in the cloud data center
[27], [28]. The parallelism technologies aim to enable terminal
devices to train large-scale models efficiently and accelerate


--- Page 3 ---
3
the learning process. In data parallelism (DP), each worker
holds a complete model replica, and input data is partitioned
across workers [29]. However, this approach is unsuitable
for large-scale models due to memory constraints. Hence,
the model parallelism (MP) was introduced to address this
limitation by partitioning the model itself and distributing
its segments across different workers [30], [31]. Pipeline
parallelism (PP) is a specific instance of MP in which the
model is partitioned into sequential stages to reduce memory
usage on individual workers. This strategy minimizes idle time
across workers and achieves efficient training of large models
[32]‚Äì[34].
Current training clusters commonly adopt hierarchical net-
work architectures, which improve communication efficiency
by fully utilizing the bandwidth of each network layer [35].
However, implementing a cross-device hybrid parallel strategy
in heterogeneous wireless edge networks presents greater
challenges due to the resource-constrained nature of terminal
devices and the unstable communication environment [36],
[37]. Deng et al. developed a cross-device collaborative fash-
ion design system that enables distributed inference across
different devices with optimized computation offloading [38].
However, the system does not take into account the efficiency
and quality of model training tasks. Liao et al. also presented
an efficient split federated learning (SFL) method for edge
computing, which controls the local updating frequency and
batch size to accelerate model training [39]. Nevertheless,
this method fails to consider network resource constraints,
limiting its applicability and scalability for SFL computation in
edge networks. Chen et al. introduced a fault-tolerant pipeline-
parallel distributed training approach for heterogeneous edge
devices [40]. This method can periodically evaluate devices‚Äô
computing power and resource status, allowing for dynamic
adjustment of the deep neural network (DNN) partitioning
strategy. However, it does not address issues on device energy
consumption and power control in heterogeneous networks.
Moreover, since LLM encoder training involves repeatedly ap-
plying the same Transformer block for feature representation,
designing a dynamic cut layer has limited impact on improving
learning performance.
Therefore, in this paper, we adopt pipeline parallelism
exclusively for LLM encoder training [41], where each worker
adaptively executes a variable number of Transformer layers
as an encoder segment. At edge server, data parallelism is
applied across different clusters to support full LLM training.
This hybrid-parallel approach enables efficient utilization of
heterogeneous devices while maintaining scalability. For the
communication protocol of partitioned segments in production
environments, we follow the design principles established in
related works [42], [43].
B. Resource Management for Training Services
From the perspective of network resource management, the
rapid growth of mobile devices and applications has led to
edge servers nearing critical resource shortages [44]. As a
result, efficient resource allocation strategies are essential to
reduce network latency in LLM training and enhance user
satisfaction.
TABLE I: Notation declarations
Symbols
Description
BU
n
Bandwidth allocated for the uplink transmission
Bdd
Bandwidth allocated for the D2D transmission
ÀÜb
Micro-batch size
Dn
Dataset of the n-th cluster
Ecom
n
Energy consumption of the n-th CU for
transmitting the encoder parameter
Esch
k
Energy consumption of the k-th device for D2D
transmission
Epipe
n
Energy consumption for n-th encoder training
Fn
Loss function of the n-th local model
gs
Smash data‚Äôs gradients calculated by the s-th
segment during backward propagation
hn
Channel gain between the CU and BS
hdd
Channel gain between devices
In,j ‚àà{0, 1}
Channel assignment matrix
Ii, Idd
Interference over channels
J
Index set of available channels
K
Set of IoT devices within a cluster
L
Number of TEBs of a LLM encoder
m
Number of micro-batches for pipeline parallelism
N
Set of clusters (FL participants)
ol, o‚Ä≤
l
FLOPs of the forward and backward propagation in
the one TEB for each pipeline, respectively
pn
Transmit power of the n-th CU
pk
Transmit power of the k-th device within a cluster
S
Number of encoder segments (S ‚â§K)
t =
{1, . . . , T}
Communication rounds between the server and
CUs (i.e., epochs of FL)
zs
Smash data from the s-th segment during forward
propagation
Œ¥k
Number of encoder blocks assigned on device k
Œ∏s
n
Parameters of the s-th segment in cluster n
Œ∏enc
n
Parameters of the n-th encoder
eŒ∏enc
Parameters of the global encoder
œÑ U
n
Uplink transmission delay of the CU n
œÑ cop
n
Computation delay for pipeline training within n-th
cluster
œÑ dd
k
D2D transmission delay of device k
œÑ pipe
k
Cross-device pipeline training delay
œÑ(t)
Total latency of the t-th communication round
œïk
FLOPs per clock cycle of the k-th device.
Liu et al. [45] demonstrated that when mobile devices,
edge servers, and cloud servers collaborate effectively through
improved situational awareness, the overall task processing
performance can be significantly enhanced. The authors also
provided closed-form expressions for task partitioning ratios
and resource allocation strategies [20]. Although this work
considers the parallel and sequential structures of computa-
tional subtasks [46], the characterization of training tasks,
particularly LLM training, is inherently more complex in
practice.
Similarly, as we propose a Transformer Encoder Block
(TEB)-wise model partitioning strategy, which differs from
convolutional layer-based DNN model training, the scheduling
of LLM training tasks and the corresponding resource alloca-
tion in edge networks require further investigation, which is
the focus of this paper.
III. HYBRID PARALLEL COMPUTING FRAMEWORK IN
HETEROGENEOUS EDGE NETWORKS
In this section, we introduce the novel CollaPipe framework
for collaborative LLM training, aiming to implement the


--- Page 4 ---
4
Fig. 1: Illustration of LLM module deployment over a distributed mobile edge IoT architecture. (a) A distributed mobile edge
IoT scenario with device‚Äìdevice and device‚Äìedge collaboration; (b) Modularized LLM components mapped onto heterogeneous
devices in (a).
efficient LLM services for mobile AI applications.
A. System Model
We consider a two-tier hierarchical network architecture
comprising the edge server and multiple clusters [47], as
illustrated in Fig. 1 (a). The edge servers consists of N base
stations (BSs), collectively forming an edge-cloud network.
Within the network, devices are organized into clusters, each
containing K devices. A designated Control Unit (CU) serves
as the cluster head, managing data storage and coordination
within the cluster [24], [48], [49].
The Transformer-based LLM is modularized and deployed
as shown in Fig. 1 (b). The edge server is responsible for
initializing the complete global model, training the decoder
module, and aggregating local models. Within each cluster,
pipeline parallelism is employed to pre-train the encoder
module. The CU stores local training data and executes
the embedding layer, while encoder segments are adaptively
assigned to devices based on their capabilities. These devices
then collaborate to train the encoder module in a distributed,
parallel manner.
To enable efficient collaboration in heterogeneous networks,
we propose CollaPipe, which organizes the learning process
into two levels: device-to-device (D2D) collaboration within
clusters and device-to-edge (D2E) collaboration across the
network.
1) D2D Collaboration: As shown in Fig. 1 (a), each cluster
consists of K devices and one CU, interconnected through
D2D communication [49]. The LLM encoder is divided into
S sequential segments, with each segment comprising a set
of Transformer Encoder Blocks (TEBs). The number of TEBs
assigned to device k is denoted by Œ¥k, and the device set
is represented by K = 1, 2, . . . , K. During training, devices
collaboratively execute pipeline-parallel learning by exchang-
ing intermediate activations, labels, and gradients, enabling
efficient distributed training across heterogeneous devices.
2) D2E Collaboration: The CUs from N clusters are
responsible for transmitting the local encoder parameters to
the BS for FL, determining segment granularity and match-
ing strategy for pipeline parallelism. In each communication
round, the CU collects and concatenates the segments‚Äô pa-
rameters to assemble the complete encoder, and subsequently
uploads the parameters to the BS for further integration and
federated aggregation.
Let J = {1, 2, . . . , J} denote the index set of the avail-
able channels. Orthogonal frequency division multiplexing
(OFDM) is utilized to transmit the local encoder parameters
from large-scale factories to the BS in parallel. In each
communication round, the BS determines a cluster-selection
strategy, selects J CUs and allocates available channels to
them. After receiving the updated encoder parameters from
different clusters, the BS conducts subsequent training of the
decoder module and performs federated aggregation to update
the parameters of the global LLM.
B. Learning Process
We consider a LLM based on encoder-decoder architecture,
with the Transformer backbone. The entire LLM is regard as
consisting of two main modules, namely encoder and decoder.
Inspired by [50], we partition the model into S segments in
a TEB-wise manner for collaborative deployment and training
[51]. The parameters of the i-th segment of cluster n are
parametrized by Œ∏i
n ‚ààRp, i ‚ààS, where p represents the
dimension of segment parameter vectors. The encoder of the
n-th cluster is parametrized by Œ∏enc
n
‚ààRe, where e is the
dimension of the parameter vector. Let Dn denote the initial
local dataset of CU n. Each CU executes the embedding layer
on Dn and it is responsible for selecting S devices to execute
the encoder sequentially.
In practical mobile edge scenarios, the computing power
and memory space of terminal devices vary significantly.


--- Page 5 ---
5
Fig. 2: Workflow of CollaPipe with one iteration in heterogeneous networks.
1‚ÉùData preprocessing; 2‚ÉùExecute embedding layer and ouput the embedded vectors; 3‚ÉùExecute partitioned segments and
output the intermediate activations;
4‚ÉùExecute LLM decoder and compute the intermediate activations;
5‚ÉùOutput the
sequence/[CLS]sign, compute the loss between target and predicted values; 6‚ÉùCompute and output the gradients of loss with
respect to the weight of partitioned segment or decoder, transmit the partial gradients to previous device, then update its own
partitioned segment or decoder‚Äôs weights via the gradients. 7‚ÉùTransmit the updated segment weights to the CU; 8‚ÉùTransmit
the updated encoder weights to the server.
Therefore, it is necessary to design efficient TEB-wise alloca-
tion, system power control, and wireless resource scheduling
strategies to ensure the efficiency of the edge learning system
and the quality of delivered models. We assume that all devices
within each cluster have different computational capabilities,
while different clusters have one CU interacting with the
BS. The BS typically possesses significant computing and
communication resources. Inspired by the GPipe algorithm
[32], we design a hybrid parallel strategy and propose a
learning algorithm of CollaPipe.
The objective of CollaPipe is to identify the optimal LLM
model parameters eŒ∏‚àóthat minimize the training loss. Let Fn(¬∑)
represent the local loss function for cluster n ‚ààN, then we
have the following learning objective:
eŒ∏‚àó= arg
min
{Œ∏enc
n
},{Œ∏sn} G(eŒ∏F M; Œ∏enc
n
, (x, y) ‚ààDn)
= 1
N
N
X
n=1
S
X
i=1
Fn
 Œ∏F M
n
; Œ∏i
n, (x, y) ‚ààDn

,
(1)
where
Œ∏F M
n
(t) = eŒ∏F M(t),
(2)
Œ∏F M
n
= Œ∏enc
n
‚äïŒ∏dec, ‚àÄn ‚ààN,
(3)
Œ∏enc
n
= Œ∏1
n ‚äïŒ∏2
n ‚äï¬∑ ¬∑ ¬∑ ‚äïŒ∏S
n, ‚àÄn ‚ààN.
(4)
where Œ∏F M represents the parameters of the full model, Œ∏dec
is the decoder parameters. In Eqs. (3) and (4), the symbol ‚äï
denotes the concatenation operation.
Specifically, in each learning epoch in one cluster, the
following steps are sequentially performed.
1) Key Hyperparameters Determining: First, each cluster‚Äôs
CU executes embedding layer using its local data. The output
from the embedding layer is then sent to the next terminal
device. Consequently, the LLM encoder needs to be partitioned
into several segments. These segments are executed sequen-
tially from the CU to the last selected device K using a micro-
batch-based pipeline parallel algorithm. Therefore, the number
of micro-batches, denoted as m, must also be determined. Let b
denote the overall batch size. The micro-batch size in pipeline
parallelism is denoted by ÀÜb and defined as:
bb = b
m.
(5)
Since the pipeline-parallel learning process involves cross-
device communication and scheduling, we require that each
micro-batch‚Äôs D2D transmission delay plus the computation
delay be consistent, so that the working time of each stage will
not overlap to ensure the effective operation of the algorithm.
We will discuss the details in Section III-D.
2) Segment Scheduling: Since the computing, memory and
radio resources of each terminal device are limited and their
available idle resources fluctuate over time, the CUs are
required to determine the number of TEBs to train on each
device, ensuring efficient operation of the split-pipline-parallel
learning.
3) Local Forward Propagation of LLM Encoder: Within
each cluster n, forward propagation of the LLM encoder is
executed during the t-th FL communication round using a
pipeline scheduling approach. Each device k assigned to the
task receives intermediate activations zs from its preceding
device k ‚àí1, performs local computations through multiple


--- Page 6 ---
6
Transformer layers1, generates new activations zs+1, and
passes them to the next device k + 1.2 The last device in the
sequence uploads the activation of the encoder‚Äôs final segment
to the server, where the forward propagation of the decoder is
performed.
4) Global Training of LLM Decoder: After receiving acti-
vations from N clusters, the edge server at the base station
directly feeds them into the decoder module. It computes
the loss value based on the predicted output, calculates the
gradient of the loss function with respect to the decoder‚Äôs
weights, and updates the model parameters of the decoder
accordingly.
5) Local Backward Propagation of LLM Encoder: The
last device in each cluster receives the decoder activation‚Äôs
gradients from the server. Subsequently, cluster n performs
backpropagation following the reverse order of the devices
used during forward propagation, as Step 6 illustrated in Fig.
2.
6) Global Model Aggregation And Updating: After com-
pleting backpropagation, each segment and the decoder up-
date their parameters using stochastic gradient descent (SGD)
method. Each device sends its updated model parameters Œ∏s
n to
its cluster‚Äôs CU n, where the parameters are concatenated. The
CU then uploads the encoder parameters Œ∏enc
n
to the server,
then FL aggregation is performed to update the global full
model eŒ∏F M as follows:
eŒ∏F M(t + 1) = Œ∏F M(t) ‚àíŒ∑gF M(t),
(6)
where gF M(t) represents the aggregation gradients at the t-th
round.
Following the chain rule, the sever calculates the averaged
gradient of N local full models as follows:
gF M(t) = 1
N
N
X
n=1
‚àáŒ∏nFn(t),
(7)
where
gF M
n
= genc
n
‚äïgdec, ‚àÄn ‚ààN,
(8)
genc
n
= g1
n ‚äïg2
n ‚äï¬∑ ¬∑ ¬∑ ‚äïgS
n, ‚àÄn ‚ààN.
(9)
The process of CollaPipe is detailed in Algorithm 1. Ini-
tially, pipeline parallelism is implemented within each cluster
(Lines 5-23). Subsequently, FL is applied to facilitate cooper-
ative training across clusters (Line 4 and Lines 24-28).
C. Communication Model
The CollaPipe framework operates within a heterogeneous
network, where we consider a wireless edge system and a
D2D communication scenario utilizing Orthogonal Frequency
Division Multiple Access (OFDMA) technology.
1We balance end-to-end latency through TEB-wise partition and segment
scheduling, requiring only that forward and backward propagation follow the
same path‚Äîwithout ordering segments or devices.
2In our designed strategy, not all devices are assigned to training tasks. k
is used to represent the task dependency between devices.
Algorithm 1 Hybrid Parallel Learning of CollaPipe
1: Input: Number of clusters N; number of encoder seg-
ments S; number of TEBs L; Communication rounds
(learning epochs) T; batch size b.
2: Output: The optimal global LLM model eŒ∏‚àó.
3: Initialize the full model Œ∏F M;
4: for each communication round t = 1, 2, . . . , T do
5:
Forward Propagation of Encoder in Clusters:
6:
for each cluster n = 1, 2, . . . , N in parallel do
7:
for each device from 1 to s within cluster n in
sequence do
8:
Perform forward propagation using pipeline
parallelism with m micro batches;
9:
Send the intermediate activations of each seg-
ment zs to the CU;
10:
end for
11:
Send the intermediate activations of the encoder
zenc to the server;
12:
end for
13:
Decoder Traning on The Server:
14:
Perform forward and backward propagation of the
decoder from N clusters;
15:
Calculate the gradients and send the intermediate
gradients gdec to CUs;
16:
Backward Propagation of Encoder in Clusters:
17:
for each cluster n = 1, 2, . . . , N in parallel do
18:
for device from s to 1 in sequence do
19:
Perform backward propagation of the encoder
on each device through pipeline parallelism;
20:
Update the parameters of all segments Œ∏s
n;
21:
end for
22:
end for
23:
Send segments‚Äô tunable parameters bŒ∏s
n to the server
for concatenation and aggregation;
24:
Global Aggregation on The Server:
25:
Concatenate segments of the encoder and decoder, and
perform global aggregation of the full LLM;
26:
Broadcast the updated encoder Œ∏enc to all clusters for
next round training;
27: end for
28: Return The updated full model eŒ∏F M(T)
1) Device-to-Edge (D2E) Communication: As for the D2E
collaboration, the uplink rate of the CU from the n-th cluster
is given by
rUp
n = BU
n Ehn

log2

1 +
pnhn
Ii + BU
n N0

,
(10)
where BU
n represents the allocated bandwidth for CU n and
pn is the transmit power of CU n. hn denotes the channel
gain between CU n and the BS. Ehn is the expectation with
respect to hn [52]. N0 is the noise power spectral density, Ii
represents the interference caused by the CUs located far away
from the service area [53].
In this paper, we consider that the BS allocates sufficient
bandwidth, so the downlink delay is ignored in the communi-
cation model construction.


--- Page 7 ---
7
Fig. 3: Entire on-device training model with pipelines.
Based on the transmission rate, in the t-th communication
round, the transmission delays between the BS and each CU
are derived as [25]
œÑ Up
n (t) =
J
X
j=1
In,j
zenc
n
+ Œ∏enc
n
BU
n (t)Ehn

log2

1 +
pn(t)hn(t)
Ii+BU
n (t)N0
,
(11)
where zenc
n
and Œ∏enc
n
are the size of smash data and encoder
model, respectively.
The energy consumption of the n-th CU for transmitting the
concatenated encoder parameters in the t-th communication
round is [54]
Ecom
n
(t) = pn(t)
PJ
j=1 In,jŒ∏enc
n
BU
n (t)Ehn

log2

1 +
pn(t)hn(t)
Ii+BU
n (t)N0
. (12)
2) Device-to-Device (D2D) Communication: As for the
D2D collaboration, our goal is to identify a sequence of
devices eK(t) to instantiate the encoder segments. In the
heterogeneous wireless communication system, devices com-
municate with each other using a D2D protocol within a
cluster. For the sake of simplicity, we assume that the speed
of D2D communication is steady, which is denoted as rdd.
The data transmit delay from device i to the subsequent
device j within a cluster can be calculated by the following
formula:
œÑ dd
k (t) =
zs + gs+1
Bdd log2

1 + pk(t)hdd(t)
Idd+BddN0
,
(13)
where zs represents the size of intermediate activations of
the s-th segment during forward propagation, while gs+1 is
the smash data‚Äôs gradients of the (s + 1)-th segment during
backward propagation.
Accordingly, the energy consumption of the scheduled de-
vice k in the t-th communication round is represented by
Esch
k (t) = pk(t) ¬∑ œÑ dd
k (t).
(14)
D. Pipeline Parallelism Model
Considering the heterogeneity in devices‚Äô computational
capabilities, each device is characterized by a relative com-
putation factor, denoted as fk, ‚àÄk. In this paper, we apply the
traditional pipeline parallel concept to achieve cross-device
parallel training and reduce the communication overhead
caused by multi-segment segmentation learning. As shown
in Fig. 3, we consider the computation and communication
overhead of a micro-batch as one forward chunk and one back-
ward chunk, so that each micro-batch can perform pipeline
calculations independently without clock overlap [32]. The
entire training process under CollaPipe framework contains
a total of (S + m ‚àí1) chunks, where a chunk is the unit size
of a micro-batch. Thus, the cross-device pipeline latency for
the all scheduled devices K in cluster n during the encoder
training at communication round t can be expressed as
œÑ pipe
n,Œ¥k(t) = (S + m ‚àí1) max
k‚ààK
 
Œ¥k(ÀÜb ¬∑ ol + o‚Ä≤
l)
œïkfk
+ œÑ dd
k
!
‚àíœÑ dd
k ,
(15)
where ol and o‚Ä≤
l represent the FLOPs of the forward and
backward propagation for each pipeline in the l-th TEB,
respectively; œïD and œïG represent the FLOPs per clock for
forward and backward propagation for each pipeline in the
l-th TEB, respectively [25].
In Eq. (15), Œ¥k(ÀÜb¬∑ol+o‚Ä≤
l)
œïkfk
represents the unit computation delay
for training on the device, and œÑ dd
k
represents the unit delay
for D2D transmission scheduling. Since the communication
between the s-th device and BS is not in the pipeline delay
model, the device scheduling time in one chunk is subtracted,
i.e., ‚àíœÑ dd
k .
Within a cluster, the devices are responsible for executing
the forward and backward operations of the matched segments.
Therefore, the energy consumption of K devices within the
cluster n for encoder training is given by
Epipe
n
= 2m
 
Œ¥k(ÀÜb ¬∑ ol + o‚Ä≤
l)
œïk
(fk)2 + Esch
k
!
.
(16)
IV. CONVERGENCE ANALYSIS AND PROBLEM
FORMULATION
In this section, we first derive a bound on model divergence
to quantify the effects of model partitioning and signal trans-
mission in heterogeneous network environments. Based on this
analysis, we then formulate the corresponding optimization
problem.
A. Impact of Heterogeneous Network Environment and Model
Splitting
The segment number S and micro-batch size ÀÜb for collabo-
rative LLM training can be determined based on the derived
divergence bound in advance. Furthermore, D2E collaboration
involves long-distance communication. Therefore, the signal
interference in the wireless environment needs to be consid-
ered. Therefore, our analysis of the model divergence bound
focuses on three parts, i.e., the interference error, the number
of segments S and clusters N.
To facilitate the analysis, we consider several assumptions
regarding the loss function, model weights, and gradients, as
outlined below [15], [16], [55].


--- Page 8 ---
8
Assumption 1. The loss function Fn(Œ∏) is non-convex,
differentiable and Œ≤-smooth, i.e.,
‚à•‚àáFn(Œ∏1) ‚àí‚àáFn(Œ∏2)‚à•‚â§Œ≤‚à•Œ∏1 ‚àíŒ∏2‚à•, ‚àÄŒ∏1, Œ∏2.
(17)
Assumption 2. (Bounded gradient). The variance of all
entries of local gradient gs
n is upper bounded by a constant
œï, i.e.,
E‚à•gs
n‚à•2 ‚â§œï2.
(18)
Assumption 3. There exists a constant Œæ ‚â•0 such that the
Polyak-Lojasiewicz inequality holds for F(Œ∏), i.e.,
1
2‚à•‚àáF(Œ∏)‚à•2
2 ‚â•Œæ (F(Œ∏) ‚àíF(Œ∏‚àó)) .
(19)
The first three assumptions are commonly adopted in
stochastic optimization and federated learning literature to fa-
cilitate convergence analysis under non-convex settings. Next,
we provide the assumption for pipeline parallelism.
Assumption 4. The sum of computation and communication
delays of all devices in pipeline parallel training tends to be
consistent.
Assumption 4 can be achieved by adjusting the model
partitioning strategy (the number of TEBs Œ¥k in each segment
and the size of a micro-batch ÀÜb, etc.) to avoid the overall
pipeline being blocked due to some devices with excessive
latency, which affects the convergence speed and performance
of the model.
Assumption 5. (Stationary backpropagation). Expectation
terms involving Œ∏s
n remain constant during training.
Assumption 5 holds true for our TEB-wise split training: we
can optimize the weights Œ∏s
n for a partitioned segment of the
LLM‚Äôs encoder while keeping the weights of other partitions
fixed, thereby ensuring that the backpropagated statistics gs
n
remain stationary.
Lemma 1. Given the objective in (1), the convergence
behavior of F
 Œ∏F M
n
; {Dn}

can be separately optimized with
respect to the LLM‚Äôs encoder {Œ∏enc
n
} and decoder (task
module), i.e.,
‚à•‚àáF(Œ∏F M)‚à•2 ‚â§
S2
N 2L
X
n‚ààN
L/S
X
l=1
S
X
s=1
‚àáŒ∏s,l
n Fn

2
|
{z
}
partitioned encoder module
+
1
N
X
n‚ààN
‚à•‚àáŒ∏decFn‚à•2
|
{z
}
decoder module
,
(20)
where L/S represents the average number of TEBs in each
segment.
Proof: See Appendix A.
We next propose a new lemma to characterize the impact of
the communication environment on the convergence behavior.
Lemma 2. (Interference impact). Let Œ∏k, ÀÜŒ∏k ‚ààRE be the
true and corrupted word embedding and sk,ÀÜsk ‚ààC be the
corresponding true and corrupted transmit symbols (signals).
According to [53], it holds that
E
h
‚à•F(Œ∏enc
n
) ‚àíF(ÀÜŒ∏enc
n
)‚à•2i
‚â§Œ≤E

‚à•sn ‚àíÀÜsn‚à•2
.
(21)
In Lemma 2, each CU n transmits the signal xn which is
composite of the beamforming vector w and word embedding
symbols sn. Then, the mean square error of signal transmit
depends on the transmitted power pn of sender (CU) n, we
have [56]
sn = PL ¬∑ SF ¬∑
Ô£´
Ô£≠|llos|2 +
N
X
n=1
Zray
X
z=1
|lnlos|2
Ô£∂
Ô£∏¬∑ pn,
(22)
where PL and SF represent the path loss and shadow fading
between the CU and BS, respectively. llos is the line-of-sight
(LOS) path contribution, and lnlos is non-line-of-sight (NLOS)
paths for each ray z and cluster n, where Zray is the number
of rays [56], [57].
According to Lemmas 1, 2 and Assumptions 1 to 5, we
have the following theorem.
Theorem 1. Given the arbitrary bandwidth allocation and
device segment matching policy, when Œ∑ <
4ŒæNL
Œ≤(S2+L), the op-
timality gap after T communication rounds is upper bounded
by
F
 Œ∏F M(T)

‚àíF
 Œ∏F M‚àó
‚â§
 T ‚àí1
Y
t=0
(1 ‚àí2œÉ(t))
!
F(Œ∏F M(0)) ‚àíF(Œ∏F M‚àó)
|
{z
}
initial gap
+
T ‚àí1
X
t=0
T ‚àí1
Y
j=t+1
(1 ‚àíœÉ(j)) Œ≤Œ∑2œï2
N
S2
L + 1

|
{z
}
task related gap
+
T ‚àí1
X
t=0
T ‚àí1
Y
j=t+1
(1 ‚àíœÉ(j)) Œ∑
N œµ(pn)
|
{z
}
interference related gap
,
(23)
where
œÉ(t) = Œ∑Œæ ‚àíŒ≤Œ∑2
2

1 + S2
NL

,
(24)
œµ(pn) =
C
pnhn + Ii
.
(25)
Proof: See Appendix B.
Remark 1: The granularity of parallelism has a dual ef-
fect on convergence behavior, and increasing the number of
segments does not necessarily lead to faster convergence.
The number of model segments S reflects the granularity
of pipeline parallelism and directly affects both the training
latency and convergence behavior. As shown in Theorem 1, S
appears quadratically in the task-related term:
Œ≤Œ∑2œï2
N
S2
L + 1

,
which indicates that excessively large S amplifies the local
model divergence due to partial gradient computation in dis-
tributed clusters. Moreover, S indirectly affects the per-round
delay and thus limits the number of global updates T within
a time budget. While increasing S enhances parallelism and
improves device utilization, it may also introduce diminishing


--- Page 9 ---
9
returns in convergence due to elevated communication over-
head and inconsistencies among sub-models. These findings
suggest that S should not be optimized independently. Instead,
it should be jointly tuned with other system-level parameters,
such as the number of micro-batches, to achieve a balanced
trade-off between parallel efficiency and convergence perfor-
mance under the CollaPipe framework.
Remark 2: Communication interference causes cumulative
delays in convergence. In the interference-related term, the
transmission power pn of each cluster‚Äôs CU directly affects the
signal distortion during the encoder parameter upload process.
Specifically, the main gap component
Œ∑
N ¬∑ œµ(pn) = Œ∑
N ¬∑
C
pnhn + Ii
reveals that increasing pn leads to a lower transmission-
induced error œµ(pn), thereby improving the accuracy of feder-
ated aggregation and accelerating global convergence. More-
over, the function œµ(pn) is monotonically decreasing and
convex with respect to pn, indicating diminishing returns of
increasing pn under high-power regimes. This observation
guides the design of the CU‚Äôs power control module when
designing the dynamic optimization algorithm. We expect
that in each round of communication, higher power levels
should be allocated to clusters with better channel gain hn
or greater influence on the global model, so as to transmit
local encoder parameters more accurately. At the same time,
the energy constraint En(t) = pn(t) ¬∑ œÑn(t) ensures that the
improvement of convergence performance will not come at
the cost of excessive energy consumption, so as to balance
the convergence performance and resource efficiency in the
wireless network.
B. Problem Formulation
According to the analysis above, the delay of each commu-
nication round is divided into two stages: encoder training
via pipeline parallelism in each cluster; model parameter
transmission under the edge FL framework. Thus, the total
delay of the t-th communication round for LLM training is
given by
œÑ(t) = max
n‚ààN {œÑ pipe
n,Œ¥k(t) + œÑ Up
n (t)}.
(26)
According to Theorem 1, the convergence of the Col-
laPipe is influenced by model partitioning, pipeline parallelism
and power control strategies. To obtain the communication-
computation efficient LLM collaborative training framework,
we develop a dynamic segment scheduling and resource
allocation protocol. This protocol, guided by Theorem 1,
explicitly incorporates model partitioning, pipeline parallelism
and power control to minimize the average training delay while
adhering to constraints on energy consumption and network
resource usage. Let the scheduling protocol be X(t) =
[m, S(t), Œ¥k(t), In,j(t), pn(t)], we formulate a stochastic op-
timization problem as
P0 : min
X(t)
1
T
T
X
t=1
œÑ(t),
s.t.
C1 :
K
X
k=1
Œ¥k = L, Œ¥k, m ‚ààZ+,
C2 : S(t) =
K
X
k=1
1{Œ¥k>0}, 1 ‚â§S ‚â§K, S ‚ààZ+,
C3 : In,j(t) ‚àà{0, 1}, ‚àÄn ‚ààN, j ‚ààJ , t ‚ààT ,
C4 :
X
j‚ààJ
In,j(t) = 1, ‚àÄn ‚ààN, t ‚ààT ,
C5 :
X
n‚ààN
In,j(t) ‚â§1, ‚àÄj ‚ààJ , t ‚ààT ,
C6 : 0 ‚â§pn(t) ‚â§P max
n
, ‚àÄn ‚ààN, t ‚ààT ,
C7 : 0 ‚â§Œ¥k(t)Œ≥0 ‚â§Œ≥max
k
, ‚àÄk ‚ààK, t ‚ààT ,
C8 : 0 ‚â§Ecom
n
(t) ‚â§Emax
n
(t), ‚àÄn ‚ààN, t ‚ààT ,
C9 : 0 ‚â§Epipe
k
(t) ‚â§Emax
k
(t), ‚àÄk ‚ààeK, t ‚ààT ,
C10 : 1
T
T
X
t=1
Œì ‚â§Œìmax.
(27)
where Œ≥k
max denotes the memory budget on device k, while
Œ≥0 represents the memory required for one TEB training. Œì
is the balance bound of one-round training, according to the
proof of Theorem 1, we have
Œì = Œ≤Œ∑2
2N
œï2
L S2 +
C
pnhn + Ii
+ œï2

.
(28)
The ranges of the variables m, Œ¥k are constrained by C1,
ensuring that all the TEBs can be trained by K devices within
one cluster; C2 denotes that the number of encoder segments
can not exceed the number of devices in a cluster; C3, C4
and C5 indicate that all the CUs cooperatively train the LLM
via FL, and one local encoder will be transmitted to the
server at each communication round; pn is constrained by C6;
C7 specifies that each device where a segment is deployed
must ensure sufficient memory to fine-tune its corresponding
segment; C8 and C9 are the energy consumptions for devices
and CUs in each communication round, respectively; the
long-term constraint C10 is adopted to optimize the learning
performance by balancing the number of segments and the
CUs‚Äô power.
V. DYNAMIC SEGMENT SCHEDULING AND RESOURCE
ALLOCATION ALGORITHM
In this section, we leverage Lyapunov optimization theory
to transform the original problem P0 into a per-round delay
minimization problem subject to system stability constraints.
To address the inherent non-convexity, we propose a dynamic
optimization algorithm based on problem decoupling and an
block coordinate descent method.


--- Page 10 ---
10
A. Problem Transformation via Lyapunov Analysis
Since the goal of our problem formulation is to optimize the
average time, we transform the P0 problem (27) into a virtual
stable queue Yn(t) through Lyapunov analysis. The following
formula is used to express the virtual queue Yn(t) of each CU
n related the delay and convergence constraint in a long-term
state:
Yn(t + 1) ‚âúmax{Yn(t) + Œìt ‚àíŒìmax, 0},
(29)
where Yn(t) is the virtual delay queue, denoting the cu-
mulative degree of failure to meet convergence and delay
requirements. Œìt represents the convergence bound at t-th
communication round. Œìmax is the maximum bound con-
straint.
In order to ensure the optimality of the system in a long-term
perspective, we add a stability constraint to the P0 problem
and rewrite it into the following optimization problem:
P1 : min
X(t)
1
T
T
X
t=1
œÑ(t),
s.t.
C1 ‚àºC9,
C10‚Äô :
lim
t‚Üí‚àû
E|Yn(t)|
t
= 0, ‚àÄn ‚ààN.
(30)
To solve P1, we next transform the long-term stochastic
problem P1 into a static problem P2 in each communication
round by characterizing the Lyapunov drift-penalty function.
Definition 1 [58]: Given V > 0, the Lyapunov drift-penalty
function is defined as
‚àÜV (t) ‚âú‚àÜ‚Ñ¶(t) + V œÑ(t),
(31)
where ‚àÜ‚Ñ¶(t) ‚âúE{‚Ñ¶(t + 1) ‚àí‚Ñ¶(t)|Yn(t)} is the conditional
Lyapunov drift, and ‚Ñ¶(t) ‚âú
1
2
P
n‚ààN Yn(t)2 defines the
Lyapunov function.
Minimizing the drift-penalty function helps stabilize the vir-
tual queue Yn(t), ensuring that it satisfies the average conver-
gence rate stability constraint C10‚Äô. This, in turn, minimizes
the delay in the FL system while also adhering to the segment
scheduling and resource allocation constraints C1 ‚àºC9 in
a long-term perspective. The parameter V is a control factor
that balances the trade-off between delay minimization and
the satisfaction of long-term convergence constraints. There-
fore, we propose a dynamic segment scheduling and resource
allocation algorithm to minimize the Lyapunov drift-penalty
function. Using the conclusion of Lemma 1 in [25], P1 can
be rewritten as
P2 : min
X(t) V œÑ(t) +
X
n‚ààN
Yn(t) (S(t) + pn(t)) ,
s.t.
C1 ‚àºC9.
(32)
We summarize œÑ(t) as the following explicit expression:
œÑ(t) = (S + m ‚àí1)√ó
Ô£´
Ô£≠Œ¥k(Sol + mo‚Ä≤
l)
mœïkfk
+
zs + gs
Bdd log2

1 +
pk(t)hdd(t)
Ik+Bdd(t)N0

Ô£∂
Ô£∏
+
PJ
j=1 In,j(zenc
n
+ Œ∏enc
n
)
BU
n log2

1 +
pn(t)hn(t)
Ii+BU
n (t)N0
.
(33)
B. Optimal Solution of P2
To solve P2, we introduce two auxiliary variables, Œõ(t) and
Œ•(t). The first is to represent model partition and segment
scheduling for each cluster performing TEB-wise pipeline
parallel procedure:
Œõ(t) = V max
k‚ààK {œÑ pipe
single(t)} +
X
n‚ààN
Yn(t)S(t)
= V (S + m ‚àí1) max
k‚ààK

Œ¥k
Sol + mo‚Ä≤
l
mœïkfk
+ rdd
k (t)

‚àíV rdd
k (t)
+
X
n‚ààN
Yn(t)S(t).
(34)
The variable Œõk(t) represents the total delay during D2D
collaboration within one cluster if it is assigned by the model
partition and segment scheduling strategy {Œ¥k(t), S, m} in the
t-th communication round.
In addition, we have Œ•(t) of auxiliary variables for the FL
procedure with D2E collaboration:
Œ•(t) = V max
n‚ààN {œÑ Up
n (t)} +
X
n‚ààN
Yn(t)pn(t)
= V max
n‚ààN
Ô£±
Ô£≤
Ô£≥
zenc
n
+ Œ∏enc
n
BU
n (t) log2

1 +
pn(t)hn(t)
Ii+BU
n (t)N0

Ô£º
Ô£Ω
Ô£æ
+
X
n‚ààN
Yn(t)pn(t).
(35)
As such, P2 can be rewritten as
P3 : min
X(t) Œõ(t) + Œ•(t),
s.t.
C1 ‚àºC9.
(36)
By exploiting the independence between S(t), Œõ(t) and
pn(t), œÑ Up
n (t) in the objective function of P3, we decouple the
joint optimization problem into the following sub-problems.
1) Optimal Auxiliary Variable Œõ(t): We minimize Œõ(t) by
optimizing the segment number S, mini-batch size m, number
of TEBs on each device Œ¥k for end-end collaboration.
The first sub-problem formulation to minimize Œõk(t) is
denoted as
Sub-1 : min
Œ¥k,S,m Œõk(t),
s.t.
C1, C2, C7,
C9‚Äô : Œ¥k(Sol + mo‚Ä≤
l)
mœïk
(fk(t))2 + pk(t)(zs + gs)
rdd
k (t)
‚â§Emax
k
,
‚àÄn ‚ààN, ‚àÄk ‚ààK.
(37)


--- Page 11 ---
11
Algorithm 2 Dynamic Segment Scheduling Algorithm
1: Input: Control factor: V , Number of the TEBs: L.
2: Initialize S(0) = 1, m(0), Œ¥(0)
k , Œìmax;
3: Optimize Œ¥k, S, m with block coordinate decent method;
4: repeat
5:
Optimize Œ¥k and S by solving (39) with integer
programming;
6:
Optimize m by solving (38);
7:
Update Yn(t) according to (29);
8: until Yn(t) stabilizes
9: Compute the auxiliary variable Œõ(t) according to (34);
10: Return S‚àó, m‚àó, Œ¥‚àó
k and Œõ(t)
Problem Sub-1 involves integer and continuous variables,
and both the objective function and constraints are non-convex.
Thus, we use the alternating optimization (AO) method to
reduce the difficulty of solving the problem. Since S and Œ¥k are
strongly coupled and both are integers, we determine group 1:
m, group 2: Œ¥ and S. First we fix S and Œ¥k, and then optimize
m, next the problem is transformed into:
min
m V (S + m ‚àí1) max
k‚ààK

Œ¥k(t)Sol + mo‚Ä≤
l
mœïkfk
+ rdd
k (t)

,
s.t.
C7, C9‚Äô,
(38)
and the optimal solution is obtained by taking the derivative
of m.
Then we fix m, optimize Œ¥k and S. The problem is trans-
formed into:
min
Œ¥k,S V (S + m ‚àí1) max
k‚ààK

Œ¥k(t)Sol + mo‚Ä≤
l
mœïkfk
+ rdd
k (t)

+ S
X
n‚ààN
Yn(t),
s.t.
C1, C7, C9‚Äô,
C2‚Äô : zk ‚â§Œ¥k ‚â§Mzk, S =
X
k
zk, zk = 1{Œ¥k>0},
C11 : Œì ‚â§Œìmax,
(39)
where the C2 is transformed into a linear constraint C2‚Äô
through integer programming to obtain the optimal solution.
The AO method allows us to optimize discrete variable Œ¥k
and integer variables S, m separately. The solution steps are
summarized in Algorithm 2.
2) Optimal Auxiliary Variable Œ•(t):
The second sub-
problem formulation for D2E communication is denoted as
Sub-2 : min
In,j,pn V
PJ
j=1 In,j(t)(zenc
n
+ Œ∏enc
n
)
BU
n log2

1 +
pn(t)hn
Ii+BU
n N0

+
X
n‚ààN
Yn(t)pn(t),
s.t.
C3 ‚àºC6,
C8‚Äô :
pnŒ∏enc
n
BU
n

log2

1 +
pnhn
Ii+BU
n N0
 ‚â§Emax
n
, ‚àÄn ‚ààN.
(40)
Algorithm 3 Dynamic Resource Allocation Algorithm
1: Input: Control factor V .
2: Initialize Yn(t) = 0, p(0)
n , Œìmax;
3: Require: Channel state at the beginning of the t-th
communication round;
4: Optimize In,j, pn with block coordinate decent method;
5: repeat
6:
Optimize In,j(t) by solving (41) with improved Hun-
garian method;
7:
Linearize Constraint C8‚Äô and C11 and optimize pn(t)
by solving (42) with SCA method;
8:
Update Yn(t) according to (29);
9: until Yn(t) stabilizes
10: Compute the auxiliary variable Œ•(t) according to (35);
11: Return I‚àó(t), p‚àó
n(t) and Œ•(t)
We also use the AO method to solve Problem Sub-2, where
the group 1 is pn, the group 2 is In,j.
Firstly, we solve the channel matching problem. Fixing
pn(t), Problem Sub-2 is transformed as a Weighted matching
problem:
min
In,j
X
j‚ààJ ,n‚ààN
In,j

V (zenc
n
+ Œ∏enc
n
)
BU
n log2(SINRn) + Yn(t)pn

.
(41)
In Problem (41), some CUs cannot be allocated channels
due to resource limitations, so we use the improved Hungarian
algorithm to construct a virtual channel (In,j = 0) to solve the
bipartite graph matching problem.
Next, we solve the power control problem. Fixing In,j,
the objective involves fractions and logarithmic functions.
We adopt Dinkelbach Transformation and Successive Convex
Approximation (SCA) method [59], then Problem Sub-2 is
transformed into:
min
pn V
X
j‚ààJ
In,j(zenc
n
+ Œ∏enc
n
)
‚àíŒªBU log2

1 +
pnhn
Ii + BU
n N0

+
X
n‚ààN
Ynpn
s.t.
C3 ‚àºC6, C8‚Äô,
C11 : Œì ‚â§Œìmax.
(42)
We adopt the first-order Taylor approximation for convex
approximation of C8‚Äô, and the gradient of the function with
respect to pn is calculated to construct a convex approximate
constraint. At the i-th iteration, given the current power pn,
the approximate constraint is
C8‚Äù : pn ‚â§Emax
n
BU
n [f(pn) ‚àípn‚àáf(pn)]
Œ∏enc
n
‚àíEmax
n
BU
n ‚àáf(pn)
,
(43)
where
f(pn) = Ehn

log2

1 +
pnhn
Ii + BU
n N + 0

,
‚àáf(pn) = Ehn

hn
(Ii + BU
n N0 + pnhn) ln 2

.


--- Page 12 ---
12
Algorithm 4 Dynamic Segment Scheduling and Resource
Allocation (DSSRA)
1: Input: Control factor V .
2: Initialize t = 0;
3: for each communication t = 1, 2, ..., T do
4:
repeat
5:
Compute Œõ(t) and Œ•(t) according to (34) and (35)
with the current optimal X‚àó(t);
6:
Given the optimized auxiliary variables Œõ(t) and
Œ•(t), find the optimal segment number S and power pn
with block coordinate descent methd;
7:
Update Yn(t) according to (29);
8:
until Yn(t) stabilizes
9: end for
10: Return X‚àó(t)
Similarly, we linearize constraint C11, Taylor expansion
C
pnhn+Ii at point p(e)
n , then the linear approximation constraint
of Œì is:
C11‚Äô : Œ≤Œ∑2
2N
œï2
L S2 + [g1(pn) + g2(pn)] + œï2

‚â§Œìmax,
(44)
where g1 and g2 are the first and second terms of Œì, respec-
tively, after Taylor expansion at p(e)
n , i.e.,
g1(pn) =
C
p(e)
n hn + Ii
,
g2(pn) = ‚àí
Chn
(p(e)
n hn + Ii)2 (pn ‚àíp(e)
n ).
As such, we use the convex optimization tool such as
CVXPY to solve the problem (42).
According to the current solution p‚àóand S‚àó, we calculate
the value of Œìt and update the queue Yn(t) in Eq. (29). The
solution steps are summarized in Algorithm 3.
Given the optimized Œõ(t) and Œ•(t), we obtain the optimal
scheduling protocol X‚àó(t). Algorithm 4 shows the Dynamic
online algorithm for Segment Scheduling and Resource Allo-
cation (DSSRA).
C. Complexity Analysis
We analyze the complexity of the proposed DSSRA algo-
rithm. In Algorithm 2, optimizing m requires a closed-form
solution with a complexity of O(K). Optimizing S requires
integer programming with constraints, leading to a worst-
case complexity of O(2K); however, this is acceptable when
K ‚â§10. Therefore, the alternating AO solution has an overall
per-iteration complexity of approximately O(K3).
In Algorithm 3, the Hungarian algorithm for channel match-
ing has a complexity of O(J3), while the SCA method
for power allocation requires gradient calculations in each
iteration, with a complexity of O(N).
Algorithm 4 calls Algorithms 2 and 3 once per round and
updates the virtual queue. Since the virtual queue update has
a complexity of O(N), the total time complexity per round
is O(K3 + J3 + N). The overall space complexity, which
includes device allocation, the channel allocation matrix, the
virtual queue, temporary gradients, and latency, is O(NJ+K).
VI. EXPERIMENTS
The experiments were conducted on different tasks: machine
translation, named entity recognition and sentence classifica-
tion. To ensure a fair comparison, we keep the data distribution
and training settings as consistent as possible with other
learning framework.
A. Experimental Setup
1) Models: To assess the performance of CollaPipe for the
Transformer-based LLM training, we conduct experiments us-
ing both the classic Transformer model and the BERT model,
which are widely applied in natural language understanding
tasks.
‚Ä¢ Classic Transformer [60]: The classic Transformer model
comprises two main components: the Encoder and the
Decoder, each consisting of 6 Transformer blocks. The
decoder and encoder have similar structures, but their
sublayers are different, and the interactions between de-
coder blocks are more complex.
‚Ä¢ BERT [61]: BERT consists of 12 Transformer encoder
blocks and is a large language model with an encoder-
only architecture.
2) Benchmark: To evaluate the performance of CollaPipe,
we conduct three experiments: (i) training the classic Trans-
former model on the Multi30K dataset for a machine trans-
lation task, (ii) pre-training the BERT model on a Chinese
named entity recognition (NER) dataset, and (iii) fine-tuning
the BERT model on a 15-class Chinese dataset to assess
accuracy in a sentence classification task.3
For comparison, we adopt three baseline methods:
‚Ä¢ VanillaFL: A standard FL approach that uses simple
averaging for model aggregation, without any model
partitioning or parallel optimization.
‚Ä¢ PipeLine: A batch-based pipeline parallelism method that
does not perform model segmentation optimization or
apply FL aggregation.
‚Ä¢ TITANIC [43]: A model auto-splitting framework that
incorporates device selection strategies and uses a P2P
communication protocol.
To further evaluate the effectiveness of our dynamic online
scheduling algorithm, we compare it with the following addi-
tional strategies:
‚Ä¢ Random Scheduling: In each round of communication,
BS randomly selects J CUs for communication, and
each CU randomly determines the training order of its
associated devices.
‚Ä¢ Loss-only Driven Scheduling: The BS selects J CUs
based on the training loss of local model in each cluster.
‚Ä¢ Delay-only Driven Scheduling: The BS selects J CUs
based on the local training delay reported by each cluster.
3All
datasets
are
available
at:
https://github.com/moon-
hotel/BertWithPretrained.


--- Page 13 ---
13
TABLE II: Experimental parameter setting for our scheme
Symbols
Value
Symbols
Value
Emax
n
10 J
Emax
k
5 J
Œ≥max
n
3 G
BU
n
[0.4, 0.6] MHz
Œ≥max
k
1.5 G
Bdd
0.5 MHz
fk
[0.1, 0.8] GHz
pk
[0.07, 0.1] W
P max
n
0.5 W
N0
-174 dBm/Hz
P max
k
0.18 W
V
[0.01, 10, 100]
hdd
-30
Idd
5 √ó 10‚àí10 W
hn
[‚àí0.12, ‚àí0.08]
Ii
[0.06, 0.08] W
b
64
œïk
[10, 24] MFLOPs
ol
2 √ó 106 FLOPs
o‚Ä≤
l
2 √ó 106 FLOPs
3) Configurations: We consider a default setting with J =
4 channels, N = 3 clusters, each comprising K = 6 devices.
To estimate the theoretical computation and scheduling time
for each segment, we use a predefined formula to calculate
FLOPs. To mitigate hardware-level variations in computation
time, we leverage the Transformers and fvcore libraries
to precompute the FLOPs for each segment. More settings for
CollaPipe and DSSRA algorithms are shown in Table II.
B. Evaluation within one cluster
To verify the impact of the number of segments S and
micro-batches m, we first test the device latency under uniform
partitioning, i.e., S =
L
Œ¥k . Taking Transformer model as an
example, we evaluate different configurations. As can be seen
from Fig. 4, if the model is evenly split and S devices
are selected to execute the pipeline parallel strategy, there is
always an optimal S and m value. However, in the mobile edge
scenario, the resource conditions of the devices are different.
We simulate a heterogeneous edge network environment to
evaluate the training delay of the BERT model. As shown
in Fig. 5, the DSSRA algorithm dynamically determines
the optimal number of model segments and their corre-
sponding assignments to mobile devices, effectively balancing
computation and communication delays across devices with
varying capabilities. For clusters with imbalanced computa-
tional resources, the optimal scheduling strategy yields the
configuration S = {5, 4, 3}, m = {14, 11, 9}. Fig. 5 also
presents a comparative scenario in which one cluster (cluster
4) deviates from the proposed scheduling strategy and instead
adopts a uniform TEB allocation policy. In this case, training
latency increases significantly, highlighting the effectiveness
of the proposed adaptive scheduling mechanism. These results
provide empirical support for the validity of Assumption 4 of
our system model.
C. Evaluation of the CollaPipe Framework
We first pre-trained the original Transformer model under
our proposed scheme and compared it with the baselines.
To ensure fairness in the comparison, all distributed learning
frameworks use the same configuration.
The learning performance under different methods is illus-
trated in Fig. 6. As shown in Fig. 6a, under identical batch size
and channel conditions, no significant differences are observed
in the training loss across the compared methods. However,
we can see from Fig. 6d that our optimized framework can
Fig. 4: Training time with the even distribution of Œ¥.
Fig. 5: Training delay of one cluster with heterogeneous
devices.
better adapt to machine translation tasks, with the BLEU score
improved by 19.5% compared to baselines.
The BERT model is pre-trained on the named entity recogni-
tion (NER) task, and the corresponding learning performance
is illustrated in Figs. 6b and 6e. The proposed framework
achieves a faster reduction in training loss during the early
stages of training, indicating improved convergence speed and
learning efficiency in the initial phase.
Based on the pre-trained model, we fine-tune the BERT
model for a single-sentence classification task. The training
process converges more rapidly‚Äîtypically within 15 training
rounds, as shown in Fig. 6c. Compared with the existing
resource scheduling strategies in federated learning systems,
the proposed method does not show a significant advantage
in learning performance. However, it consistently outperforms
the PipeLine and TITANIC mechanisms by about 1.82%
and 0.15%, respectively. These results demonstrate that the
proposed learning framework, under independently and identi-
cally distributed (IID) data conditions, effectively compensates
for the training loss caused by encoder partitioning through
adaptive device scheduling and resource management. As
shown in Figs. 6a, 6b, and 6c, this approach maintains a
relatively stable training loss. Moreover, we observe that Col-
laPipe achieves more significant performance gains in machine
translation and sentence classification tasks. We conjecture that
these improvements may stem from differences in block-level
or token-level behavior introduced by model partitioning.
Fig. 7 presents the accumulated computation time during
Transformer training. CollaPipe consistently achieves the low-
est computational latency across all benchmark methods. Fig.
8 shows that VanillaFL achieves a 29.98% reduction in training


--- Page 14 ---
14
(a) Training loss of Machine Translation
(b) Training loss of Named Entity Recognition
(c) Training loss of Sentence Classification
(d) BLEU score of Machine Translation
(e) F1 score of Named Entity Recognition
(f) Accuracy of Sentence Classification
Fig. 6: Learning performance of various tasks with the Transformer and BERT.
Fig. 7: On-device accumulated computation time of machine
translation with the Transformer.
Fig. 8: On-device accumulated computation time of sentence
classification with the BERT.
delay compared to the PipeLine method. In contrast, other
approaches introduce additional scheduling latency due to
TABLE III: Scalability of different training frameworks (i.e.,
the BERT model)
Method
Memory
usage of
single
devices
Number of
data users
Segment
Schedul-
ing
Resource
Alloca-
tion
Vanilla FL
‚àº5-6 G
Large
N
N
PipeLine
‚àº5-6 G
Small
N
N
TITANIC
Adapted
Large
N
Y
CollaPipe
Adapted
Small
Y
Y
the need for inter-device coordination. Among all evaluated
schemes, CollaPipe achieves the best performance, reducing
training delay by 18.94% compared to TITANIC, 15.09%
compared to VanillaFL, and 40.55% compared to PipeLine.
However, when the segment scheduling module is removed
from CollaPipe, latency increases by approximately 22.45%,
indicating a significant drop in efficiency. The light blue
bars in the figures represent CollaPipe without the segment
scheduling module (CollaPipe w/o SS). Compared to the fully
optimized framework, the cumulative computation latency in-
creases significantly, demonstrating that the proposed DSSRA
algorithm effectively reduces intra-cluster computation delay.
These results highlight the effectiveness of CollaPipe‚Äôs coordi-
nated scheduling mechanism in minimizing system delay and
improving training efficiency in distributed learning environ-
ments.
Although FL does not incur additional D2D communication
delay, it imposes high demands on the memory capacity of
mobile devices and the availability of high-quality user data.
Table III compares memory usage and data requirements
across different methods. The CollaPipe framework offers
greater flexibility by dynamically adjusting per-device mem-


--- Page 15 ---
15
Fig. 9: Learning performance of machine translation with
different number of segment and micro-batch.
Fig. 10: Learning performance of named entity recognition
with different number of segment and micro-batch.
Fig. 11: Total delay with different optimization schemes.
ory usage based on the number of encoder blocks assigned
during model partitioning, making it well-suited for resource-
constrained edge environments. Moreover, as training data is
stored centrally in the control unit (CU), participating devices
contribute only computational resources without needing to
share local data. This significantly reduces the number of data-
providing users and alleviates device management overhead in
mobile edge networks.
D. Sensitivity Analysis
We conduct sensitivity analysis on the proposed DSSRA
algorithm to illustrate the impact of joint optimization on
system performance.
Fig. 9 shows the inference performance with different num-
bers of segments S and micro-batches m. For the Transformer
model, the optimal configuration determined by the DSSRA
algorithm is S = 2 and m = 2, under which the best learning
performance is achieved. As shown in Fig. 10, for the BERT
model, disabling the segment scheduling module results in
a slight reduction in inference performance. More notably,
reducing the value of m improves convergence performance,
indicating a trade-off between parallel granularity and learning
efficiency.
Fig. 11 presents that the proposed DSSRA algorithm
achieves lower system latency compared to the baseline
schemes, and its advantage becomes increasingly pronounced
as the number of communication rounds increases. Specifi-
cally, compared to the loss-only driven scheduling strategy,
DDSRA with V = 0.01 reduces the BERT training latency
on the Toutiao dataset by 46.44%. When compared to the
delay-only driven scheduling, the latency reduction is 15.48%,
and against random scheduling, the DSSRA achieves a 7.12%
reduction. This improvement is attributed to our proposed
dynamic optimization algorithm, which comprehensively ac-
counts for the additional communication and computation
overhead caused by frequent interactions at the Transformer
block level. By effectively balancing intra-cluster and inter-
cluster coordination, the algorithm enables efficient collabora-
tion across heterogeneous networks.
VII. CONCLUSION
This paper has presented CollaPipe, a novel hybrid parallel
learning framework for collaborative LLM training. In the
proposed approach, the encoder of the LLM is partitioned
into S segments in a TEB-wise manner and distributed across
mobile devices, where the segments are trained using pipeline
parallelism. After encoder training, each cluster‚Äôs CU commu-
nicates with the edge server to perform federated aggregation.
To address challenges posed by heterogeneous and dynamic
edge environments, we have formulated a bidirectional opti-
mization framework that achieves a dynamic balance between
learning performance and communication efficiency through
a selective scheduling and resource allocation strategy. We
have derived the closed-form convergence bound, establishing
the relationship between system performance, the number
of segments, and transmission power. To ensure long-term
system stability, we have further integrated Lyapunov opti-
mization with convergence analysis of the learning process.
Extensive experiments have validated the effectiveness of the
CollaPipe framework in enhancing intra-cluster collaboration
and improving training efficiency. The DSSRA algorithm has
demonstrated its ability to achieve near-optimal system latency
under resource-constrained and heterogeneous conditions.
Our findings also highlight the critical role of the number of
micro-batches in collaborative learning, particularly for models
like BERT. Future work will explore batch-level or token-
level scheduling and optimization strategies in conjunction
with model segmentation, aiming to further enhancing self-
collaboration and self-intelligence within distributed intelligent
IoT systems.


--- Page 16 ---
16
APPENDIX A
PROOF OF LEMMA 1
Proof: According to (1), we can expand the formula as
‚àáF(Œ∏F M)
2
=

1
N
N
X
n=1
‚àáFn(Œ∏F M
n
)

2
=

1
N
N
X
n=1
 
‚àÇFn(ÀúŒ∏dec)
‚àÇŒ∏enc
n
, ‚àÇFn(Œ∏enc
n
)
‚àÇŒ∏dec
!
2
.
(45)
Then we derive the derivatives for {Œ∏enc
n
}n‚ààN and Œ∏dec
separately to obtain
‚àáF(Œ∏F M)
2
=
1
N 2
N
X
n=1

‚àÇFn(ÀúŒ∏dec)
‚àÇŒ∏enc
n

2
+ 1
N 2

N
X
n=1
‚àÇFn(Œ∏enc
n
)
‚àÇŒ∏dec

2
=
1
N 2
 X
n‚ààN
‚àáŒ∏enc
n Fn(Œ∏enc
n
; ÀúŒ∏dec)

2
+

X
n‚ààN
‚àáŒ∏decFn(Œ∏dec; Œ∏enc
n
)

2Ô£∂
Ô£∏
(a)
‚â§
1
N 2
X
n‚ààN
‚àáŒ∏enc
n Fn(Œ∏enc
n
; ÀúŒ∏dec)

2
+
1
N
X
n‚ààN
‚àáŒ∏decFn(Œ∏dec; Œ∏enc
n
)
2 ,
(46)
where (a) follows by using the inequality ‚à•Pn
i=1 zi‚à•2 ‚â§
n Pn
i=1 ‚à•zi‚à•2 for any vectors zi and any positive integer n
(using n = N in (a)).
The encoder is further partitioned into segments, and the
parameters of each segment are split and substituted into
‚àáŒ∏enc
n Fn
2 =
S
X
s=1
‚àáŒ∏snFn
2 ,
(47)
and we gain
‚àáŒ∏enc
n Fn
2
(a)
=

‚àÇFn(Œ∏enc
n
)
‚àÇŒ∏1n

2
+

‚àÇFn(Œ∏enc
n
)
‚àÇŒ∏2n

2
+ ¬∑ ¬∑ ¬∑ +

‚àÇFn(Œ∏enc
n
)
‚àÇŒ∏Sn

2
‚âú

1
Œ¥1
Œ¥1
X
l=1
g1,l
n

2
+

1
Œ¥2
Œ¥2
X
l=1
g2,l
n

2
+ ¬∑ ¬∑ ¬∑ +

1
Œ¥S
Œ¥S
X
l=1
gS,l
n

2
(b)
‚â§S
L
L/S
X
l=1
g1,l
n
2 +
g2,l
n
2 + ¬∑ ¬∑ ¬∑ +
gS,l
n
2
,
(48)
where (a) follows from Eqs. (7) and (9); (b) follows by using
the inequality ‚à•Pn
i=1 zi‚à•2 ‚â§n Pn
i=1 ‚à•zi‚à•2 (using n = Œ¥i for
(b)), Œ¥i indicates the average number of TEBs in a segment 4,
so we remove Œ¥i by Œ¥i = L/S.
4To facilitate the proof, the same number of TEBs per segment is assumed
only in the proof. The segment scheduling module in the CollaPipe framework
allocates different numbers of TEBs according to different device capacities.
Combing (46) and (48), (45) can be upper bounded by
‚à•‚àáF(Œ∏F M)‚à•2
‚â§
1
N 2
X
n‚ààN

1
Œ¥i
Œ¥i
X
l=1
S
X
s=1
‚à•gs,l
n ‚à•2

2
+ 1
N
X
n‚ààN
‚à•‚àáŒ∏decFn‚à•2
(a)
‚â§
S2
N 2L
X
n‚ààN
L/S
X
l=1
S
X
s=1
gs,l
n
2 + 1
N
X
n‚ààN
‚à•‚àáŒ∏decFn‚à•2,
(49)
where (a) follows by using the inequality ‚à•Pn
i=1 zi‚à•2 ‚â§
n Pn
i=1 ‚à•zi‚à•2 (using n = S for (a)) and Œ¥i = L/S.
The proof is completed.
APPENDIX B
PROOF OF THEOREM 1
Before proceeding with the proof, recall that ÀÜŒ∏ represents the
gradient after transmission through the interference channel,
and eŒ∏ model parameters after global update.
According to Lemma 1, given the fixed decoder parameters,
we can derive the upper bound on convergence related to the
encoder‚Äôs segment. In each communication round t for FL, the
global LLM update follows:
eŒ∏(t + 1) = ÀÜŒ∏(t) ‚àíŒ∑
N
N
X
n=1
ÀÜgn(t),
(50)
where
ÀÜgn(t) =
S
X
s=1
gs
n(t) + ‚àÜgn(t).
(51)
According to Lemma 2, the channel interference results in
the gradient error, we have
E‚à•‚àÜgn(t)‚à•2 ‚â§Œ≤œµ(pn(t)).
(52)
To assist the derivation, we define a virtual model v(t +
1) that assumes no interference in the transmission signals
between the mobile device and BS, that is, there is almost no
error before and after data transmission, which is
vn(t + 1) = eŒ∏(t) ‚àíŒ∑
N
N
X
n=1
gn(t).
(53)
Utilizing Œ≤-Smoothness, we can recursively obtain
F(v(t + 1)) ‚â§F(Œ∏(t)) ‚àíŒ∑‚à•‚àáF(Œ∏(t))‚à•2
+ Œ≤Œ∑2
2N 2
N
X
n=1
‚à•gn(t)‚à•2.
(54)
The convergence upper bound of a full LLM can be de-
coupled into two modules: the encoder and the decoder. Next,
we analyze the convergence with split learning of the encoder
part. According to Lemma 1, the encoder gradient variance is
split into S segments:
E‚à•genc
n
(t)‚à•2 ‚â§S
Œ¥i
S
X
s=1
E‚à•gs
n‚à•2 ‚â§S
Œ¥i
œï2.
(55)


--- Page 17 ---
17
The overall gradient variance is:
E‚à•‚àáF(Œ∏)‚à•2 ‚â§S
Œ¥i
œï2 + 1
N œï2.
(56)
We rewrite (54) in the form of F ‚àíF ‚àóand take its
expectation:
E[F(v(t + 1)) ‚àíF ‚àó]
‚â§E[F(Œ∏(t + 1)) ‚àíF ‚àó] ‚àíŒ∑‚à•‚àáF(Œ∏(t))‚à•2
+ Œ≤Œ∑2
2N 2
N
X
n=1
‚à•gn(t)‚à•2
(a)
‚â§E[F(Œ∏(t + 1)) ‚àíF ‚àó] ‚àí2Œ∑ŒæE[F(Œ∏(t + 1)) ‚àíF ‚àó]
+ Œ≤Œ∑2
2N 2
N
X
n=1
‚à•gn(t)‚à•2
(b)
‚â§(1 ‚àí2Œ∑Œæ)E[F(Œ∏(t + 1)) ‚àíF ‚àó] + Œ≤Œ∑2
2N
 S
Œ¥i
+ 1

œï2,
(57)
where (a) follows Assumption 5, substituting the PL condition
into the second term; (b) follows (56) and combines like terms.
Subtracting (50) from (53), we obtain the gap between the
actual parameter update and the virtual queue:
Œ∏(t + 1) ‚àív(t + 1) = ‚àíŒ∑
N
N
X
n=1
‚àÜgn(t).
(58)
According to Assumption 1, taking the expectation of the
above function, the second-order term of Œ≤-smoothness is
transformed into:
E‚à•Œ∏(t + 1) ‚àív(t + 1)‚à•2 = Œ∑2
N 2 E‚à•
N
X
n=1
‚àÜgn(t)‚à•2
(a)
‚â§Œ∑2Œ≤
N œµ(pn(t)),
(59)
where (a) follows Assumption 1 and Lemma 2.
Similarly, we rewrite (59) in the form of F ‚àíF ‚àóand take
its expectation:
E[F(Œ∏(t + 1)) ‚àíF ‚àó] ‚â§E[F(v(t + 1)) ‚àíF ‚àó] + Œ≤Œ∑2
2N œµ(pn(t)).
(60)
Combing (57) and (60), we have
E[F(Œ∏(t + 1)) ‚àíF ‚àó]
‚â§(1 ‚àí2Œ∑Œæ)E[F(Œ∏(t + 1)) ‚àíF ‚àó] + Œ≤Œ∑2
2N
 S
Œ¥i
+ 1

œï2
+ Œ≤Œ∑2
2N œµ(pn(t)).
(61)
Let œÉ(t) = 2Œ∑Œæ ‚àíŒ≤Œ∑2
2

S
NŒ¥i + 1
N

, œµ(pn) =
C
pnhn+Ii , then
we expand (61) from t = 0 to T ‚àí1:
F (Œ∏(T)) ‚àíF (Œ∏‚àó)
‚â§
 T ‚àí1
Y
t=0
(1 ‚àí2œÉ(t))
!
F(Œ∏(0)) ‚àíF(Œ∏‚àó)
+
T ‚àí1
X
t=0
T ‚àí1
Y
j=t+1
(1 ‚àíœÉ(j))
Œ≤Œ∑2œï2
N
S2
L + 1

+ Œ∑
N œµ(pn(t))

.
(62)
According to (62), œÉ(t) controls the degree of convergence
in each round. As long as œÉ(t) > 0, the system can be
guaranteed to converge. Let œÉ(t) > 0 and solve the system
convergence condition:
Œ∑ <
4ŒæNL
Œ≤(S2 + L).
The proof is completed.
REFERENCES
[1] H. Wang, H. Li, M. Sheng, and J. Li, ‚ÄúCollaborative fine-tuning of
mobile aigc models with wireless channel conditions,‚Äù IEEE Wireless
Communications, vol. 31, no. 4, pp. 32‚Äì38, 2024.
[2] Q. Guan, J. Ouyang, D. Wu, and W. Yu, ‚ÄúCitygpt: Towards urban
iot learning, analysis and interaction with multi-agent system,‚Äù 2024.
[Online]. Available: https://arxiv.org/abs/2405.14691
[3] ÀôI. K√∂k, O. Demirci, and S. √ñzdemir, ‚ÄúWhen iot meet llms: Applications
and challenges,‚Äù in 2024 IEEE International Conference on Big Data
(BigData), 2024, pp. 7075‚Äì7084.
[4] J. Liu, H. Li, C. Chai, K. Chen, and D. Wang, ‚ÄúA llm-informed multi-
agent ai system for drone-based visual inspection for infrastructure,‚Äù
Advanced Engineering Informatics, vol. 68, p. 103643, 2025.
[5] P. Wang, M. Zhu, X. Zheng, H. Lu, H. Zhong, X. Chen, S. Shen,
X. Wang, Y. Wang, and F.-Y. Wang, ‚ÄúBevgpt: Generative pre-trained
foundation model for autonomous driving prediction, decision-making,
and planning,‚Äù IEEE Transactions on Intelligent Vehicles, pp. 1‚Äì13,
2024.
[6] W. Xu, Z. Yang, D. W. K. Ng, R. Schober, H. V. Poor, Z. Zhang,
and X. You, ‚ÄúA new pathway to integrated learning and communication
(ilac): Large ai model and hyperdimensional computing for communi-
cation,‚Äù 2025.
[7] X. Li, S. Wang, S. Zeng, Y. Wu, and Y. Yang, ‚ÄúA survey on llm-
based multi-agent systems: workflow, infrastructure, and challenges,‚Äù
Vicinagearth, vol. 1, no. 1, p. 9, 2024.
[8] S. Yu, J. P. Munoz, and A. Jannesari, ‚ÄúFederated foundation models:
Privacy-preserving and collaborative learning for large models,‚Äù in
Proceedings of the 2024 Joint International Conference on Computa-
tional Linguistics, Language Resources and Evaluation (LREC-COLING
2024), 2024, pp. 7174‚Äì7184.
[9] F. Wu, Z. Li, Y. Li, B. Ding, and J. Gao, ‚ÄúFedbiot: Llm local fine-
tuning in federated learning without full model,‚Äù in Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining, ser. KDD ‚Äô24. New York, NY, USA: Association for Computing
Machinery, 2024, p. 3345‚Äì3355.
[10] H. Wang, Z. Yin, B. Chen, Y. Zeng, X. Yan, C. Zhou, and A. Li, ‚ÄúRofed-
llm: Robust federated learning for large language models in adversarial
wireless environments,‚Äù IEEE Transactions on Network Science and
Engineering, pp. 1‚Äì13, 2025.
[11] J. C. Zhao, S. Bagchi, S. Avestimehr, K. S. Chan, S. Chaterji, D. Dim-
itriadis, J. Li, N. Li, A. Nourian, and H. R. Roth, ‚ÄúFederated learning
privacy: Attacks, defenses, applications, and policy landscape - a survey,‚Äù
CoRR, vol. abs/2405.03636, 2024.
[12] G. Lu, Z. Xiong, R. Li, N. Mohammad, Y. Li, and W. Li, ‚ÄúDefeat:
A decentralized federated learning against gradient attacks,‚Äù High-
Confidence Computing, vol. 3, no. 3, p. 100128, 2023.
[13] L. Meng, Z. Qi, L. Wu, X. Du, Z. Li, L. Cui, and X. Meng, ‚ÄúImproving
global generalization and local personalization for federated learning,‚Äù
IEEE Transactions on Neural Networks and Learning Systems, vol. 36,
no. 1, pp. 76‚Äì87, 2025.


--- Page 18 ---
18
[14] J. Hu, D. Wang, Z. Wang, X. Pang, H. Xu, J. Ren, and K. Ren,
‚ÄúFederated large language model: Solutions, challenges and future
directions,‚Äù IEEE Wireless Communications, vol. 32, no. 4, pp. 82‚Äì89,
2025.
[15] Z. Wang, Y. Zhou, Y. Shi, and K. B. Letaief, ‚ÄúFederated fine-tuning
for pre-trained foundation models over wireless networks,‚Äù IEEE Trans-
actions on Wireless Communications, vol. 24, no. 4, pp. 3450‚Äì3464,
2025.
[16] J. Zhang, W. Ni, and D. Wang, ‚ÄúFederated split learning with model
pruning and gradient quantization in wireless networks,‚Äù IEEE Transac-
tions on Vehicular Technology, vol. 74, no. 4, pp. 6850‚Äì6855, 2025.
[17] Z. Zhang, C. Chang, H. Lin, Y. Wang, R. Arora, and X. Jin, ‚ÄúIs network
the bottleneck of distributed training?‚Äù in Proceedings of the Workshop
on Network Meets AI & ML, ser. NetAI ‚Äô20.
New York, NY, USA:
Association for Computing Machinery, 2020, p. 8‚Äì13.
[18] Z. Lin, G. Qu, X. Chen, and K. Huang, ‚ÄúSplit learning in 6g edge
networks,‚Äù IEEE Wireless Communications, vol. 31, no. 4, pp. 170‚Äì176,
2024.
[19] J. Liu, Y. Du, K. Yang, Y. Wang, X. Hu, Z. Wang, Y. Liu, P. Sun,
A. Boukerche, and V. C. M. Leung, ‚ÄúEdge-cloud collaborative com-
puting on distributed intelligence and model optimization: A survey,‚Äù
2025.
[20] C. T. Dinh, N. H. Tran, M. N. H. Nguyen, C. S. Hong, W. Bao, A. Y.
Zomaya, and V. Gramoli, ‚ÄúFederated learning over wireless networks:
Convergence analysis and resource allocation,‚Äù IEEE/ACM Transactions
on Networking, vol. 29, no. 1, pp. 398‚Äì409, 2021.
[21] W. Shi, S. Zhou, Z. Niu, M. Jiang, and L. Geng, ‚ÄúJoint device scheduling
and resource allocation for latency constrained wireless federated learn-
ing,‚Äù IEEE Transactions on Wireless Communications, vol. 20, no. 1,
pp. 453‚Äì467, 2021.
[22] Y. Shen, Y. Qu, C. Dong, F. Zhou, and Q. Wu, ‚ÄúJoint training and
resource allocation optimization for federated learning in uav swarm,‚Äù
IEEE Internet of Things Journal, vol. 10, no. 3, pp. 2272‚Äì2284, 2023.
[23] H. Kim, J. S. Choi, J. Kim, and J. H. Ko, ‚ÄúA dnn partitioning framework
with controlled lossy mechanisms for edge-cloud collaborative intelli-
gence,‚Äù Future Generation Computer Systems, vol. 154, pp. 426‚Äì439,
2024.
[24] L. Wang, C. Zhao, S. Yang, X. Yang, and J. Mccann, ‚ÄúAce: Toward
application-centric, edge-cloud, collaborative intelligence,‚Äù Commun.
ACM, vol. 66, no. 1, p. 62‚Äì73, Dec. 2022.
[25] X. Deng, J. Li, C. Ma, K. Wei, L. Shi, M. Ding, and W. Chen, ‚ÄúLow-
latency federated learning with dnn partition in distributed industrial iot
networks,‚Äù IEEE Journal on Selected Areas in Communications, vol. 41,
no. 3, pp. 755‚Äì775, 2023.
[26] S. Hu, J. Lin, Z. Lu, X. Du, Q. Duan, and S.-C. Huang, ‚ÄúCollars : A
cloud‚Äìedge‚Äìterminal collaborative lifelong learning framework for aiot,‚Äù
Future Generation Computer Systems, vol. 158, pp. 447‚Äì456, 2024.
[27] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary,
V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro,
A. Phanishayee, and M. Zaharia, ‚ÄúEfficient large-scale language model
training on gpu clusters using megatron-lm,‚Äù in Proceedings of the
International Conference for High Performance Computing, Networking,
Storage and Analysis, ser. SC ‚Äô21.
New York, NY, USA: Association
for Computing Machinery, 2021.
[28] J. H. Park, G. Yun, C. M. Yi, N. T. Nguyen, S. Lee, J. Choi, S. H. Noh,
and Y. ri Choi, ‚ÄúHetPipe: Enabling large DNN training on (whimpy)
heterogeneous GPU clusters through integration of pipelined model
parallelism and data parallelism,‚Äù in 2020 USENIX Annual Technical
Conference (USENIX ATC 20).
USENIX Association, Jul. 2020, pp.
307‚Äì321.
[29] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y. Arcas,
‚ÄúCommunication-Efficient Learning of Deep Networks from Decen-
tralized Data,‚Äù in Proceedings of the 20th International Conference
on Artificial Intelligence and Statistics, ser. Proceedings of Machine
Learning Research, A. Singh and J. Zhu, Eds., vol. 54.
PMLR, 20‚Äì22
Apr 2017, pp. 1273‚Äì1282.
[30] E. P. Xing, Q. Ho, W. Dai, J.-K. Kim, J. Wei, S. Lee, X. Zheng,
P. Xie, A. Kumar, and Y. Yu, ‚ÄúPetuum: A new platform for distributed
machine learning on big data,‚Äù in Proceedings of the 21th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining,
ser. KDD ‚Äô15.
New York, NY, USA: Association for Computing
Machinery, 2015, p. 1335‚Äì1344.
[31] Y. Zhuang, L. Zheng, Z. Li, E. Xing, Q. Ho, J. Gonzalez, I. Stoica,
H. Zhang, and H. Zhao, ‚ÄúOn optimizing the communication of model
parallelism,‚Äù in Proceedings of Machine Learning and Systems, D. Song,
M. Carbin, and T. Chen, Eds., vol. 5.
Curan, 2023, pp. 526‚Äì540.
[32] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee,
J. Ngiam, Q. V. Le, Y. Wu, and z. Chen, ‚ÄúGpipe: Efficient training
of giant neural networks using pipeline parallelism,‚Äù in Advances in
Neural Information Processing Systems, H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alch√©-Buc, E. Fox, and R. Garnett, Eds., vol. 32.
Curran Associates, Inc., 2019.
[33] W. Zhang, B. Zhou, X. Tang, Z. Wang, and S. Hu, ‚ÄúMixpipe: Efficient
bidirectional pipeline parallelism for training large-scale models,‚Äù in
2023 60th ACM/IEEE Design Automation Conference (DAC), 2023, pp.
1‚Äì6.
[34] X. Miao, Y. Shi, Z. Yang, B. Cui, and Z. Jia, ‚ÄúSdpipe: A semi-
decentralized framework for heterogeneity-aware pipeline-parallel train-
ing,‚Äù Proc. VLDB Endow., vol. 16, no. 9, p. 2354‚Äì2363, May 2023.
[35] S. Li, K. Lu, Z. Lai, W. Liu, K. Ge, and D. Li, ‚ÄúA multidimensional
communication scheduling method for hybrid parallel dnn training,‚Äù
IEEE Transactions on Parallel and Distributed Systems, vol. 35, no. 8,
pp. 1415‚Äì1428, 2024.
[36] J. Yoon, Y. Byeon, J. Kim, and H. Lee, ‚ÄúEdgepipe: Tailoring pipeline
parallelism with deep neural networks for volatile wireless edge de-
vices,‚Äù IEEE Internet of Things Journal, vol. 9, no. 14, pp. 11 633‚Äì
11 647, 2022.
[37] H. Shi, R. Ma, D. Li, and H. Guan, ‚ÄúHierarchical adaptive collaborative
learning: A distributed learning framework for customized cloud services
in 6g mobile systems,‚Äù IEEE Network, vol. 37, no. 2, pp. 44‚Äì53, 2023.
[38] H. Deng, J. Jiang, Z. Yu, J. Ouyang, and D. Wu, ‚ÄúCrossgai: A cross-
device generative ai framework for collaborative fashion design,‚Äù Proc.
ACM Interact. Mob. Wearable Ubiquitous Technol., vol. 8, no. 1, Mar.
2024.
[39] Y. Liao, Y. Xu, H. Xu, Z. Yao, L. Wang, and C. Qiao, ‚ÄúAccelerating
federated learning with data and model parallelism in edge computing,‚Äù
IEEE/ACM Transactions on Networking, vol. 32, no. 1, pp. 904‚Äì918,
2024.
[40] Y. Chen, Q. Yang, S. He, Z. Shi, J. Chen, and M. Guizani, ‚ÄúFtpipehd:
A fault-tolerant pipeline-parallel distributed training approach for het-
erogeneous edge devices,‚Äù IEEE Transactions on Mobile Computing,
vol. 23, no. 4, pp. 3200‚Äì3212, 2024.
[41] L. Guan, D.-S. Li, J.-Y. Liang, W.-J. Wang, K.-S. Ge, and X.-C. Lu,
‚ÄúAdvances of pipeline model parallelism for deep learning training: An
overview,‚Äù JOURNAL OF COMPUTER SCIENCE AND TECHNOL-
OGY, vol. 39, no. 3, pp. 567‚Äì584, MAY 2024.
[42] J. M. Tarnawski, A. Phanishayee, N. Devanur, D. Mahajan, and
F. Nina Paravecino, ‚ÄúEfficient algorithms for device placement of dnn
graph operators,‚Äù in Advances in Neural Information Processing Systems,
H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds.,
vol. 33.
Curran Associates, Inc., 2020, pp. 15 451‚Äì15 463.
[43] N. Su, C. Hu, B. Li, and B. Li, ‚ÄúTitanic: Towards production federated
learning with large language models,‚Äù in IEEE INFOCOM, 2024.
[44] A. Ndikumana, K. K. Nguyen, and M. Cheriet, ‚ÄúFederated learning
assisted deep q-learning for joint task offloading and fronthaul segment
routing in open ran,‚Äù IEEE Transactions on Network and Service
Management, vol. 20, no. 3, pp. 3261‚Äì3273, 2023.
[45] F. Liu, J. Huang, and X. Wang, ‚ÄúJoint task offloading and resource allo-
cation for device-edge-cloud collaboration with subtask dependencies,‚Äù
IEEE Transactions on Cloud Computing, vol. 11, no. 3, pp. 3027‚Äì3039,
2023.
[46] S. E. Mahmoodi, R. N. Uma, and K. P. Subbalakshmi, ‚ÄúOptimal
joint scheduling and cloud offloading for mobile applications,‚Äù IEEE
Transactions on Cloud Computing, vol. 7, no. 2, pp. 301‚Äì313, 2019.
[47] X. Xu, B. Xu, S. Han, C. Dong, H. Xiong, R. Meng, and P. Zhang,
‚ÄúTask-oriented and semantic-aware heterogeneous networks for artificial
intelligence of things: Performance analysis and optimization,‚Äù IEEE
Internet of Things Journal, vol. 11, no. 1, pp. 228‚Äì242, 2024.
[48] D. Wei, X. Xu, S. Mao, and M. Chen, ‚ÄúOptimizing communication
and device clustering for clustered federated learning with differential
privacy,‚Äù IEEE Transactions on Mobile Computing, pp. 1‚Äì14, 2025.
[49] S. Liu, G. Yu, D. Wen, X. Chen, M. Bennis, and H. Chen, ‚ÄúCommuni-
cation and energy efficient decentralized learning over d2d networks,‚Äù
IEEE Transactions on Wireless Communications, vol. 22, no. 12, pp.
9549‚Äì9563, 2023.
[50] J. Du, T. Lin, C. Jiang, Q. Yang, C. F. Bader, and Z. Han, ‚ÄúDistributed
foundation models for multi-modal learning in 6g wireless networks,‚Äù
IEEE Wireless Communications, vol. 31, no. 3, pp. 20‚Äì30, 2024.
[51] J. Chen, J. Wang, S. Guo, J. Hao, X. Qiu, and Z. Xiong, ‚ÄúA transformer-
block-wise collaborative training mechanism with hybrid parallelism
over heterogeneous networks,‚Äù in 2025 IEEE Wireless Communications
and Networking Conference (WCNC), 2025, pp. 01‚Äì06.


--- Page 19 ---
19
[52] M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor, and S. Cui, ‚ÄúA joint
learning and communications framework for federated learning over
wireless networks,‚Äù IEEE Transactions on Wireless Communications,
vol. 20, no. 1, pp. 269‚Äì283, 2021.
[53] A. Djuhera, V. C. Andrei, X. Li, U. J. M√∂nich, H. Boche, and W. Saad,
‚ÄúR-sfllm: Jamming resilient framework for split federated learning with
large language models,‚Äù IEEE Transactions on Information Forensics
and Security, pp. 1‚Äì1, 2025.
[54] Z. Yang, M. Chen, Z. Zhang, and C. Huang, ‚ÄúEnergy efficient semantic
communication over wireless networks with rate splitting,‚Äù IEEE Journal
on Selected Areas in Communications, vol. 41, no. 5, pp. 1484‚Äì1495,
2023.
[55] M. M. Amiri, D. G√ºnd√ºz, S. R. Kulkarni, and H. V. Poor, ‚ÄúConvergence
of update aware device scheduling for federated learning at the wireless
edge,‚Äù IEEE Transactions on Wireless Communications, vol. 20, no. 6,
pp. 3643‚Äì3658, 2021.
[56] C. V. Nahum, V. H. L. Lopes, R. M. Dreifuerst, P. Batista, I. Correa,
K. V. Cardoso, A. Klautau, and R. W. Heath, ‚ÄúIntent-aware radio re-
source scheduling in a ran slicing scenario using reinforcement learning,‚Äù
IEEE Transactions on Wireless Communications, vol. 23, no. 3, pp.
2253‚Äì2267, 2024.
[57] 3GPP, ‚ÄúStudy on 3d channel model for lte,‚Äù 3rd Generation Partnership
Project (3GPP), Tech. Rep. TR 36.873, Jun. 2017.
[58] M. Neely, Stochastic Network Optimization with Application to Com-
munication and Queueing Systems, ser. Synthesis Lectures on Commu-
nication Networks.
Morgan & Claypool Publishers, 2010.
[59] M. Razaviyayn, M. Hong, Z.-Q. Luo, and J.-S. Pang, ‚ÄúParallel succes-
sive convex approximation for nonsmooth nonconvex optimization,‚Äù in
Advances in Neural Information Processing Systems, Z. Ghahramani,
M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, Eds., vol. 27.
Curran Associates, Inc., 2014.
[60] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. u. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù in Advances
in Neural Information Processing Systems, vol. 30.
Curran Associates,
Inc., 2017.
[61] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-training
of deep bidirectional transformers for language understanding,‚Äù in
Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers).
Minneapolis,
Minnesota: Association for Computational Linguistics, jun 2019, pp.
4171‚Äì4186.
