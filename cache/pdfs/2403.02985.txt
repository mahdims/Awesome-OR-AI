--- Page 1 ---
Evolution Transformer: In-Context Evolutionary Optimization
Robert Tjarko Lange
TU Berlin, Google DeepMind
Germany, Japan
robert.t.lange@tu-berlin.de
Yingtao Tian
Google DeepMind
Japan
alantian@google.com
Yujin Tang
Google DeepMind
Japan
yujintang@google.com
Figure 1: Left. Evolution Transformer overview. The model encodes three different types of information (solution space, fitness
& search distribution). The resulting per-search dimension embedding is processed by an update network using parameter
sharing. The Evolution Transformer is trained by generating optimization trajectories from teacher BBO algorithms and
minimizing the KL divergence between student and teacher search distribution updates. Right. Aggregated results on 8 Brax
tasks and 5 individual runs (see SI, Figure 10). We report the interquartile mean and optimality gap after normalizing by the
task-specific OpenAI-ES [32] performance. Evolution Transformer robustly outperforms Evolutionary Optimization baselines.
ABSTRACT
Evolutionary optimization algorithms are often derived from loose
biological analogies and struggle to leverage information obtained
during the sequential course of optimization. An alternative promis-
ing approach is to leverage data and directly discover powerful
optimization principles via meta-optimization. In this work, we
follow such a paradigm and introduce Evolution Transformer, a
causal Transformer architecture, which can flexibly characterize
a family of Evolution Strategies. Given a trajectory of evaluations
and search distribution statistics, Evolution Transformer outputs
a performance-improving update to the search distribution. The
architecture imposes a set of suitable inductive biases, i.e. the invari-
ance of the distribution update to the order of population members
within a generation and equivariance to the order of the search di-
mensions. We train the model weights using Evolutionary Algorithm
Distillation, a technique for supervised optimization of sequence
ArXiv, March 2024, Preprint
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM.
https://doi.org/10.1145/nnnnnnn.nnnnnnn
models using teacher algorithm trajectories. The resulting model ex-
hibits strong in-context optimization performance and shows strong
generalization capabilities to otherwise challenging neuroevolution
tasks. We analyze the resulting properties of the Evolution Trans-
former and propose a technique to fully self-referentially train the
Evolution Transformer, starting from a random initialization and
bootstrapping its own learning progress. We provide an open source
implementation under https://github.com/RobertTLange/evosax.
CCS CONCEPTS
‚Ä¢ Computing methodologies ‚ÜíEvolutionary Robotics.
KEYWORDS
evolution strategies, machine learning
ACM Reference Format:
Robert Tjarko Lange, Yingtao Tian, and Yujin Tang. 2024. Evolution Trans-
former: In-Context Evolutionary Optimization. https://doi.org/10.1145/nnnnnnn.
nnnnnnn
arXiv:2403.02985v1  [cs.AI]  5 Mar 2024


--- Page 2 ---
ArXiv, March 2024, Preprint
Lange et al.
1
INTRODUCTION
Evolutionary optimization [EO; e.g. 32, 35] provides a scalable and
simple-to-use tool for black-box optimization (BBO). Instead of
having to rely on access to well-behaved gradients, such methods
only require the evaluation of the function (e.g. neural network loss,
training run, or physics simulator). This makes them especially well
suited for otherwise challenging applications such as meta-learning
or neural architecture search. Most of the specific algorithms are
derived from loose biological metaphors and are oftentimes not very
adaptive to the problem at hand. Therefore, recent work has tried
to address these limitations by aiming to meta-optimize attention-
based modules of EO algorithms [16, 17].
Here, we extend this line of work and introduce the first fully
Transformer-based Evolution Strategy (ES), which we call Evolu-
tion Transformer (EvoTF; Figure 1, Left Top). It leverages both self-
and cross-attention to implement two crucial inductive biases de-
sirable for BBO algorithm: Invariance of the distribution update
with respect to the within-generation ordering of the solutions and
equivariance with respect to the ordering of the individual search
dimensions. We show that this Transformer can be successfully
trained via supervised learning using optimization trajectories ob-
tained by teacher algorithms. We name this method Evolutionary
Algorithm Distillation (EAD; Figure 1, Left Bottom). After training,
the resulting model can imitate various teacher BBO algorithms
and performs in-context evolutionary optimization (Figure 1, Right).
Furthermore, the resulting Evolution Strategy generalizes to pre-
viously unseen optimization problems, the number of search di-
mensions, and population members. We propose two avenues to
improve upon the teacher‚Äôs performance by introducing additions
to the supervised algorithm distillation training setup: Finally, we
introduce Self-Referential Evolutionary Algorithm Distillation (SR-
EAD), which can be used to self-train Evolution Transformer mod-
els: Given a parametrization, we can generate multiple perturbed
trajectories, filter such, and use them to bootstrap the observed per-
formance improvements. Hence, it provides a promising direction
for the open-ended discovery of in-context evolutionary optimizers.
Our contributions are summarized as follows:
(1) We introduce Evolution Transformer, an architecture induc-
ing a population-order invariant and dimension-order equi-
variant search update (Section 3). After supervised training
with Evolutionary Algorithm Distillation, the Transformer
has distilled various evolutionary optimizers and performs
in-context learning on unseen tasks (Section 4).
(2) We analyze the Transformer BBO algorithm and show em-
pirically that it captures desirable properties such as scale-
invariance and perturbation strength adaptation (Section 5).
(3) We compare supervised EAD with the meta-evolution of Evo-
lution Transformer network parameters. We find that meta-
evolution is feasible but tends to overfit the meta-training
tasks and requires more accelerator memory (Section 6).
(4) We introduce Self-Referential Algorithm Distillation, which
alleviates the need for teacher algorithm specification. By
perturbing the parameters of the Evolution Transformer, we
construct a set of diverse self-generated trajectories. After
performance filtering, the sequences can be used to self-
referentially train the model from scratch (Section 7).
2
RELATED WORK & BACKGROUND
Black-Box Optimization. Our work develops efficient BBO algo-
rithms with flexible Transformer-based parametrization. Given a
function ùëì(x) : Rùê∑‚ÜíR with unknown functional form, i.e. we
cannot compute its derivative (or it is not well behaved), BBO seeks
to find its global optimum leveraging only function evaluations:
min
x
ùëì(x), s.t. xùëë‚àà[ùëôùëë,ùë¢ùëë] ‚äÇ[‚àí‚àû, ‚àû], ‚àÄùëë= 1, ..., ùê∑.
Evolutionary Optimization Methods. EO provides a set of
BBO algorithms inspired by the principles of biological evolu-
tion. Roughly speaking, they can be grouped into Evolution Strate-
gies [30] and Genetic Algorithms (GA), which differ in the way
they perform the selection and mutation of solution candidates.
Here, we focus on diagonal Gaussian ES. Given a population size
ùëÅand the search distribution summary statistics ùùÅ‚ààRùê∑, Œ£ =
ùùà1ùê∑√óùê∑‚ààRùê∑√óùê∑, ES sample a population of candidate solutions
ùëã= [ùë•1, ...,ùë•ùëÅ] ‚ààRùëÅ√óùê∑at each generation. Afterwards, the per-
formance (or fitness) of each candidate is evaluated on the task of
interest and one obtains fitness scores ùêπ= [ùëì1, ..., ùëìùëÅ] ‚ààRùëÅ. The
search distribution is then updated to increase the likelihood of sam-
pling well-performing solutions, ùùÅ‚Ä≤, ùùà‚Ä≤ ‚ÜêUPDATE(ùùÅ, ùùà,ùëã, ùêπ, ùêª),
where ùêª‚ààRùê∑√óùê∑ùêªdenotes a set of ES summary statistics (e.g.
momentum-like terms). There exist various types of ES including
estimation-of-distribution [CMA-ES, 6], natural [SNES, 33, 41] and
finite-difference-based ES [OpenAI-ES, 32]. Evolution Transformer
provides a flexible parametrization of the ES-update UPDATEùúÉ(ùùÅ, ùùà,ùëã, ùêπ, ùêª)
and optimizes the set of Transformer weights ùúÉto distill teacher
BBO algorithm updates using gradient-descent.
Meta-Learned Evolutionary Optimizers. Formally, ES-update
rules are inherently set of operations, i.e. the order of the population
members within a generation should not affect the performed distri-
bution change. Self-attention provides a natural inductive bias for
such an operation. Previously, Lange et al. [16, 17] constructed ES
and GA algorithms, which used self- and cross-attention to process
the information within a single generation. The associated parame-
ters were then meta-evolved on a small task distribution of BBO
problems. Unlike Evolution Transformer, the meta-evolved EO was
not a sequence model, i.e. does not leverage a causal Transformer
with positional encoding.
Self-Referential Higher-Order Evolution. Schmidhuber [34]
first articulated the vision of self-refining Genetic Algorithms. Later
on Metz et al. [27] showed that randomly initialized black-box
gradient-descent optimizers can optimize themselves using large-
scale population-based training [10]. Kirsch and Schmidhuber [13]
further formulated a rule to automatically allocate resources for
self-referential meta-learning. Finally, [26] provides theoretical re-
sults for infinite-order meta-evolution.
Autoregressive Models for BBO. Chen et al. [2] proposed to learn
RNN-based BBO algorithms using privileged access to gradient com-
putations of the fitness (Gaussian Process sampled) functions at
training time. Furthermore, Chen et al. [3], Dery et al. [4] used large
pre-collected datasets to train a T5 Encoder-Decoder architecture
[29] for BBO, called OptFormer. Krishnamoorthy et al. [15] later on
investigated the usage of generating offline data for training autore-
gressive BBO models. Lange et al. [22] show that language models
can be used as recombination operators for ES. In contrast, our work


--- Page 3 ---
Evolution Transformer: In-Context Evolutionary Optimization
ArXiv, March 2024, Preprint
Figure 2: Evolution Transformer. We construct features resembling information from solution evaluations and the search
distribution. They are processed by self-attention and Perceiver modules to obtain four separate embeddings. The stacked
per-dimension embeddings are processed by standard Transformer encoder blocks. An MLP outputs the distribution update
predictions. The model is invariant to the order of the population members and equivariant to the search dimension order.
introduces an architecture specialized to flexibly represent ES algo-
rithms and proposes several complementary enhancements to the
supervised training pipeline to improve upon teacher algorithms.
Finally, OptFormer is intended to be applied to hyperparameter
optimization problems and not to train large numbers of neural
network weights, unlike Evolution Transformer.
Meta-Learned In-Context Learning & AD for RL. Kirsch et al.
[12], Lu et al. [25], Team [38] trained large sequence models using
supervised and reinforcement learning without access to teacher
algorithms. Instead, they only leveraged large programmatically-
generated or augmented task spaces to induce in-context learning
across long timescales. Laskin et al. [23], on the other hand, in-
troduced the concept of Transformer-based AD in the context of
Reinforcement Learning. They showed that it is possible to per-
form AD for actor-critic-based teacher algorithms. Afterwards, a
Transformer policy shows in-context learning capabilities when
evaluated on unseen environments. Our work extends these ideas
to zero-order optimization and introduces a tailored architecture
and AD enhancements in order to increase performance.
Self-Attention & Perceiver Cross-Attention. The Transformer
[39] stacks blocks of multi-head self-attention (MHSA) operations,
feedforward MLP transformation, dropout and layer normalization.
At its core self-attention is a set operation which projects an input
matrix ùëã‚ààRùëÅ√óùê∑onto ùê∑ùêæ-dimensional vectors ùëÑ, ùêæ,ùëâ‚ààRùëÅ√óùê∑ùêæ
called queries, keys and values, respectively:
Attention(ùëã) = softmax(ùëÑùêæùëá/
‚àöÔ∏Å
ùê∑ùêæ)ùëâ
= softmax(ùëãùëäùëÑ(ùëãùëäùêæ)ùëá/
‚àöÔ∏Å
ùê∑ùêæ)ùëãùëäùëâ.
Permuting the rows of ùëãwill apply the same permutation to
Attention(ùëã) [e.g., 14, 24, 36]. MHSA has quadratic complexity
with respect to the sequence length ùëÅ. The Perceiver module [11]
was introduced to partially alleviate this problem. Instead, it per-
forms a cross-attention operation:
Perceiver(ùëã,ùëç) = softmax(ùëçùëäùëÑ(ùëãùëäùêæ)ùëá/
‚àöÔ∏Å
ùê∑ùêæ)ùëãùëäùëâ,
where ùëç‚ààRùëÅùëç√óùê∑ùëçdenotes a set of learned latent vectors.
Hence, the dimensionality of Perceiver(ùëã,ùëç) ‚ààRùëÅùëç√óùê∑ùêæis fixed
and independent of the sequence length.
3
EVOLUTION TRANSFORMER:
POPULATION-ORDER IN- & DIMENSION-
ORDER EQUIVARIANT SEARCH UPDATES
We design a Transformer-based architecture that can flexibly rep-
resent an ES for different population sizes and the number of
search space dimensions. To enable this endeavor, we leverage
self-attention and Perceiver cross-attention to induce population-
order invariance and batch over the individual search dimensions
to obtain a dimension-order equivariant neural network ES. At a
high level the architecture (Figure 2) constructs a set of embedding
features from evaluated solution candidates, their fitness scores,
and search distribution statistics per generation. Such features con-
sist of common Evolutionary Optimization ingredients such as
finite difference gradients, normalized fitness scores, and evolu-
tion paths/momentum statistics (see SI for detailed input feature
specification). After combining these features, we perform causal
self-attention over the different generations. Finally, we output the
search distribution updates per search dimension.
Solution Perceiver. Given solution candidates ùëã‚ààRùëÅ√óùê∑sam-
pled within a generation, we pre-compute the set of normalized
features for each dimension, Àúùëã‚ààRùëÅ√óùê∑√óùê∑ùëã. The information is
processed by a Perceiver module applied per individual search di-
mension ùëó. The weights are shared across the dimensions. Hence,
we perform the following operations:
ùíâùëÜ
ùëë= softmax
 
ùëçùëäùëÑ( Àúùëã[:,ùëë]ùëäùêæ)ùëá
‚àöùê∑ùêæ
!
Àúùëã[:,ùëë]ùëäùëâ‚ààRùëÅùëç√óùê∑ùêæ,
for ùëë= 1, . . . , ùê∑. Note that this information compresses the
population information into a set of ùê∑tensors with a fixed shape.
Thereby, the dimensionality of the resulting representation is inde-
pendent of the number of population members. This together with
per-dimension updates allows for general applicability to different
BBO settings.
Fitness Perceiver. The fitness scores ùêπ‚ààRùëÅare similarly first
processed into a set of features Àúùêπ‚ààRùëÅ√óùê∑ùêπand afterwards com-
pressed into a set of Perceiver latent vectors, ùíâùêπ= RùëÅùëç√óùê∑ùêæ.


--- Page 4 ---
ArXiv, March 2024, Preprint
Lange et al.
Distribution Attention & Cross-Dimension Perceiver. We fur-
ther process a set of features capturing the search distribution
dynamics (e.g. momentum and evolution paths with different time
scales for both ùùÅand ùùà). We perform both MHSA within a single
dimension and Perceiver compression across dimensions.
MHSA Transformer Blocks Across Time. The four embedding
steps can be computed in parallel over time, making the implemen-
tation efficient on hardware accelerators. Afterwards, we repeat and
concatenate the different embeddings to construct a single stacked
per-dimension representation, ùíâ. The aggregated representation is
further processed by a set of standard MHSA Transformer blocks
with positional encoding over the timesteps/generations.
Distribution Update Module. The final representation is pro-
jected per dimension to output the proposed change in the search
distribution and we perform a multiplicative update inspired by
SNES [33]:
ùùÅ‚Ä≤
EvoTF = ùùÅ+ ùúÇùúá√ó ùùà√ó EvoTransformerùúá(ùëã, ùêπ, ùíâ)
ùùà‚Ä≤
EvoTF = ùùà√ó exp(ùúÇùúé√ó EvoTransformerùúé(ùëã, ùêπ, ùíâ)),
with tune-able learning rates ùúÇùúá,ùúÇùúé. During supervised algo-
rithm distillation training both of them are set to 1.
Evolutionary Algorithm Distillation. We train Evolution Trans-
former to distill the search procedure of various teacher algo-
rithms. More specifically, we collect batches of teacher trajectories
{ùëãùëî, ùêπùëî, ùúáùëî, ùúéùëî}ùê∫
ùëî=1 on a set of synthetic BBO tasks. We minimize
the Kullback-Leibler (KL) divergence between the teacher‚Äôs and
proposed updated distribution at each generation with ùëÜ= ùùà2Iùê∑:
KL((ùùÅEvoTF, ùë∫EvoTF)||(ùùÅT, ùë∫T)) = 1/2(tr(ùë∫‚àí1
T ùë∫EvoTF)+
log
|ùë∫ùëá|
|ùë∫EvoTF| + (ùùÅT ‚àíùùÅEvoTF)ùëáùë∫‚àí1
T (ùùÅT ‚àíùùÅEvoTF) ‚àíùê∑).
Deploying EvoTF-based Evolution Strategies. After successful
teacher distillation, we can use the trained Transformer in ‚Äôinfer-
ence mode‚Äô and use it for optimization on different BBO tasks, e.g.
with previously unseen numbers of search dimensions or popula-
tion members. Note that during training the Evolution Transformer
is never actively acting on the problems. Instead, it only has to
predict the on-policy behavior of the teacher algorithm. Hence,
on new evaluation problems, it has to effectively act ‚Äôoff-policy‚Äô
since the previous ùùÅ, ùùàupdates were generated by the Evolution
Transformer strategy itself and not by a teacher algorithm.
General Implementation Details. We use JAX [1] to implement
the Evolution Transformer architecture and dimension-batched
search updating. We further leverage evosax [20] to parallelize the
generation of teacher ES algorithm sequences on hardware acceler-
ators. The architecture uses standard Transformer encoder blocks
and during training, we leverage a causal (lower diagonal) mask
for autoregressively training Evolution Transformer [28] to predict
the teacher algorithm‚Äôs search distribution updates. Training is
conducted using a single to four A100 NVIDIA GPU. For inference,
we use a sliding window context of the most recent ùêægenerations.
This is largely due to memory constraints arising for ùê∑> 1000
search dimensions. All Evolution Transformer ES evaluation exper-
iments are conducted using a single V100S NVIDIA GPU. The code
is published under https://github.com/<anonymized_repository>.
4
SUPERVISED EVOLUTIONARY ALGORITHM
DISTILLATION CLONES VARIOUS TEACHER
BBO ALGORITHMS
Experiment Outline. We investigate whether the proposed Evolu-
tion Transformer architecture is capable of distilling different indi-
vidual BBO and ES teacher algorithms. More specifically, we focus
on 4 common classes of teacher BBO algorithms: Finite-difference
ES including OpenAI-ES [32], estimation-of-distribution ES [Sep-
CMA-ES, 31], Gaussian Hill Climbing and Natural Evolution Strate-
gies [33, 41]. For each individual algorithm, we generate Evolution
Transformer AD training trajectories by executing it on a set of syn-
thetic tasks (see below) with different search settings. Afterwards,
we train the Evolution Transformer to auto-regressively predict the
search distribution updates of the teacher algorithm.
Implementation Details. The teacher optimization trajectories
are sampled from a subset of BBOB [7] benchmark functions con-
taining different fitness landscape characteristics. For all trajecto-
ries, we use a fixed set of search dimensions (ùê∑= 5) and population
size (ùëÅ= 10). We randomize the initial mean of the search algo-
rithm as well as its search variance. Each trajectory consists of 32
generations and we collect the solution, fitness, and distribution
features. At each update step, we sample a rollout batch of 32 trajec-
tories online before constructing the KL loss. We use pre-attention
layer normalization, no dropout, and the Adam optimizer with a
cosine warmup learning rate schedule. The Evolution Transformer
modules use a single attention/Perceiver block and in total contain
approximately 300k parameters. Throughout supervised learning,
we evaluate the performance of the current network weight check-
point with a fixed context window of ùêæ= 5 on a set of holdout
tasks including both unseen BBOB and small neuroevolution tasks.
Results Interpretation. In the first row of Figure 3 we show that
this procedure successfully distills the considered source teacher al-
gorithms. We note that the teacher algorithms vastly differ in their
types of search update equations. Hence, the Evolution Transformer
architecture design is flexible enough to represent all of these. Fur-
thermore, the successful distillation of a teacher BBO algorithm
requires only a handful BBOB tasks (Figure 11). The performance in-
crease does not significantly increase with the number of considered
BBOB problems. Throughout the supervised AD training procedure,
we evaluate the Evolution Transformer ES on various downstream
tasks. In this case, the trained Evolution Transformer model is used
to perform BBO without any explicit teacher guidance. Instead,
it has to perform optimization using frozen network weights and
only make use of the in-context information provided by the on-
going BBO trajectory. We find that the Evolution Transformer ES
is capable of generalizing to completely unseen neuroevolution
tasks such as the ant control task (Figure 1). This indicates that the
supervised AD procedure did not lead to overfitting, but instead
has led to successful in-context evolutionary optimization. Finally,
we perform ablation studies on the modules used in the Evolution
Transformer architecture (Figure 3). We find that adding both the
fitness and distribution information helps the model in distilling
the different algorithms. The cross-dimensional Perceiver is not
required. Note, that especially the distribution features are required
to distill SNES, Sep-CMA-ES, and OpenAI-ES.


--- Page 5 ---
Evolution Transformer: In-Context Evolutionary Optimization
ArXiv, March 2024, Preprint
Figure 3: Evolutionary Algorithm Distillation allows EvoTF to distill teacher algorithms. Top. KL distillation loss with different
Transformer modules. Middle. Evaluation on a 14x14 MNIST CNN Classification task throughout distillation. Bottom. ‚ÄôS+F‚Äô
uses only the Solution and Fitness Perceiver. ‚ÄôS+F+D‚Äô also uses the Distribution Attention and ‚ÄôS+F+D+CD‚Äô uses all network
modules. Evaluation on a Pendulum MLP Control task throughout distillation. Results are averaged across 3 independent runs.
5
ANALYSIS: EVOLUTION TRANSFORMER
CAPTURES DESIRABLE EVOLUTION
STRATEGY PROPERTIES
EvoTF Properties on Characteristic Problems. After having
demonstrated that EAD can successfully distill teacher BBO into
the Evolution Transformer, we next analyze the properties of the
resulting Evolution Strategy. We focus on three core properties
desirable for an Evolution Strategy [6]:
(1) Unbiasedness/Stationarity: Given a random fitness func-
tion, e.g. ùëì(ùíô‚àºN (0, 1), we do not want to observe a drift
in the mean statistic, i.e. E(ùùÅ‚Ä≤|ùùÅ) = ùùÅ.
(2) Translation Invariance: Given an offset to the fitness func-
tion, the performance of the ES is not going to change, e.g.
ùëìùëè(ùíô) = ùëì(ùíô‚àíùëè) we desire ùíô‚òÖ
ùëè= ùíô‚òÖ‚àíùëè.
(3) Scale Self-Adaptation: Given a linear fitness function, ùëì(ùíô) =
√çùê∑
ùëë=1 ùíôùëë, we desire that the perturbation strength ùùàincreases
over the course of the evolutionary optimization trajectory.
We test these properties for an Evolution Transformer trained to
distill the SNES teacher algorithm. After supervised EAD training,
we roll out the resulting EvoTF Evolution Strategy and track the
relevant statistics. In Figure 5 we show that the model correctly
incorporates all the desired characteristics. The mean on a random
fitness function does not show any clear bias and remains at its
initialization. Furthermore, it correctly finds the optimal ùíô‚òÖ
ùëèfor
different translation offset levels. Finally, we find that the scale
quickly increases for the linear fitness function, which allows for
quick adaptation and fitness improvements.
Evolution Transformer Attention Maps. Next, we visualize
the attention maps for an Evolution Transformer trained to distill
the SNES teacher [33]. We consider two different 3-dimensional
BBO tasks: The separable Sphere task and the Rosenbrock func-
tion moderate condition number. For both cases, we consider 3
search dimensions and 5 population members. For both problems,
the fitness feature-based attention scores across the population
members are highest for the best-performing members within the
considered generation. Interestingly, we observe that the distribu-
tion self-attention maps correctly identify the separability of the
fitness landscape. For the Sphere task, it attributes the majority of
its attention to its diagonal, indicating that each dimension-specific
update mostly considers the dimension-specific evaluation informa-
tion. For the non-separable Rosenbrock task, on the other hand, the
attention is split across the different dimensions, hinting at a trans-
fer of evaluation information. The multi-head self-attention over
the generation sequence, on the other hand, attributes attention
across all previous generations. This indicates that the Transformer
integrates information accumulated through multiple generations,
providing evidence for in-context evolutionary optimization. This
observation is consistent for both considered BBO problems. Finally,
the solution attention does not appear visually interpretable. This is


--- Page 6 ---
ArXiv, March 2024, Preprint
Lange et al.
Figure 4: Self-Attention and Perceiver maps for Evolution Transformer (EAD-trained on SNES) with a single attention block at a
single generation. Top. Separable Sphere problem. Bottom. Non-separable Rosenbrock problem. All problems are 3-dimensional
and use 5 population members. The fitness attention assigns higher credit to the best-performing population members. The
distribution attention indicates that the EvoTF correctly infers whether the fitness landscape is separable.
not too surprising, given the non-linear transformations performed
by the Perceiver module with the solution features.
Figure 5: Evolution Transformer (SNES) properties on char-
acteristic problems. EvoTF correctly implements the core
properties of unbiasedness on the random fitness (top row),
translation invariance on a Sphere Task (middle row), and
scale self-adaptation on the linear fitness (bottom row). All
tasks consider 3 search dimensions and 5 population mem-
bers The search mean is initialized between [‚àí3, 3].
6
META-EVOLUTION OF EVOLUTION
TRANSFORMER WEIGHTS CAN OVERFIT
THE META-TRAINING TASK DISTRIBUTION
Experiment Outline. Is supervised EAD more efficient than meta-
learning the weights of the Evolution Transformer directly via evo-
lutionary optimization? Can one potentially obtain performance
improvements by fine-tuning a previously distilled Evolution Trans-
former checkpoint via meta-evolution? To answer these questions
we turn to the meta-black-box optimization (MetaBBO, Figure 7)
paradigm introduced by Lange et al. [16, 17]. More specifically, we
use a standard diagonal ES to evolve the EvoTF weights on the
same set of BBOB tasks previously used for EAD. We again evalu-
ate the performance of the resulting EvoTF checkpoint throughout
meta-evolution training.
Implementation Details. We use Sep-CMA-ES [31] as the meta-
ES and sample a population of 256 different EvoTF parametrizations,
which we evaluate on a set of 64 BBOB tasks. Afterwards, we con-
struct normalized meta-fitness scores for all meta-population mem-
bers and update the meta-ES. This iterate meta-evolution procedure
is executed for 1000 meta-generations. We consider two settings: 1)
Meta-evolving EvoTF parameters starting from a random initializa-
tion and 2) starting from a previously distilled SNES teacher. Again,
the entire meta-evaluation procedure is conducted using hardware
acceleration and the vectorization and parallelization capabilities of
JAX [1, 20]. The training is done using 4 A100 NVIDIA GPUs. Due
to memory constraints, we consider a smaller EvoTF model with
ca. 200k parameters and do not use the cross-dimension Perceiver
module.
Results Interpretation. In Figure 6 we show that meta-evolving
an EvoTF parametrization from a randomly initialized meta-search
mean is indeed feasible and results in an ES capable of generaliz-
ing to unseen BBOB tasks and partially to neuroevolution tasks
(e.g. CartPole). Only 500 meta-generations are required to obtain a


--- Page 7 ---
Evolution Transformer: In-Context Evolutionary Optimization
ArXiv, March 2024, Preprint
Figure 6: Meta-Evolution of Evolution Transformer weights. We train the neural network weights using meta-black-box
optimization [16, 17] on a set of 5 BBOB problems. We compare evolving a EvoTF parametrization from scratch with fine-tuning
an EAD-pretrained SNES-EvoTF initialization. While meta-evolution quickly improves performance on BBOB tasks, it tends to
not generalize to neuroevolution tasks. Results are averaged across 3 independent runs.
Figure 7: Evolution Transformer Meta-Black-Box Optimiza-
tion (MetaBBO) procedure. We sample neural network pa-
rameters for EvoTF from a meta-optimizer. Afterwards, all
EvoTransformer candidates are evaluated by running BBO
on a set of tasks. We compute an aggregated meta-fitness
score across the tasks and update the meta-optimizer.
competitive EvoTF-based ES. We also investigated whether meta-
evolution can act as a fine-tuning strategy after performing super-
vised Evolutionary Algorithm Distillation. To do so we pre-train
an EvoTF to distill SNES, afterwards we initialize the meta-search
mean to the previously obtained checkpoint and continue training
via meta-evolution. While such a fine-tuning exercise quickly im-
proves performance on BBOB tasks, it dramatically decreases the
performance on the neural network weight evolution evaluation
tasks. This indicates a degree of overfitting to the meta-train task
distribution and highlights the need for diverse meta-training cover-
age. This opens up question with regards to generating a curriculum
of tasks, that automatically increase the difficulty of meta-training
tasks according to the capabilities of the current EvoTF.
7
SELF-REFERENTIAL EVOLUTIONARY
ALGORITHM DISTILLATION IS FEASIBLE
BUT CAN BE UNSTABLE
Experiment Outline. We wondered whether it is possible to com-
pletely alleviate the need for a teacher algorithm (in EAD) or a
meta-black-box optimization algorithm (e.g. Sep-CMA-ES). More
specifically, we propose to leverage random perturbations of an
EvoTF checkpoint to generate different so-called ‚Äôself-referential
offspring‚Äô (Figure 9). Afterwards, these offspring generate diverse
optimization trajectories on a fixed set of tasks. We then filter the
collected trajectories by their performance. Only the best trajecto-
ries are used to train the EvoTF with algorithm distillation. This
procedure iterates between perturbation, trajectory generation, per-
formance filtering and distillation. Thereby, at each iteration of the
procedure, the EvoTF from the previous iteration can bootstrap its
behavior based on previously observed performance improvements
of its perturbed versions.
Implementation Details. As a proof of concept, we use sim-
ple scalar Gaussian perturbations to the current EvoTF weight
checkpoint to generate ùëÅ= 64 EvoTF offspring. We compare
two different perturbation strengths used to generate offspring,
ùúé0 ‚àà{0.004, 0.005}. We very slowly (exponentially) decay the
strength throughout training. At each iteration, we collect the per-
formance of the offspring on the previously used subset of 5 BBOB


--- Page 8 ---
ArXiv, March 2024, Preprint
Lange et al.
Figure 8: Self-Referential Evolutionary Algorithm Distillation of Evolution Transformer weights. By iterating the generation
of offspring EvoTF parametrizations, the generation and filtering of BBO trajectories, and self-distillation of such filtered
trajectories, we obtain a self-referentially trained evolution strategy. Results are averaged across 3 independent runs.
tasks. All of the offspring are evaluated on the same randomly
sampled tasks and we select the best BBO trajectory observed by
all offspring to be added to the distillation batch. We proceed by
performing a single gradient descent distillation update and iterat-
ing the procedure. We conduct training on 4 A100 NVIDIA GPUs
leveraging JAX-parallelization of the trajectory generation step.
Results Interpretation. In Figure 8 we show that is indeed fea-
Figure 9: Self-Referential Evolutionary Algorithm Distilla-
tion Procedure for Evolution Transformer without the need
for a teacher algorithm or meta-optimizer.
sible to perform self-referential learning without the need for an
explicit improvement algorithm in the form of a meta-optimizer or
teacher algorithm. The performance on the BBOB tasks quickly im-
proves above random. Furthermore, on a subset of neuroevolution
tasks such as CartPole and MNIST classification, the SR-EAD-based
EvoTF can perform the optimization of MLP weights. On the Pen-
dulum control task, on the other hand, self-referential training
appears unstable. After an initial improvement, the performance
drops again. This observation appears robust to the choice of per-
turbation strength.
8
CONCLUSION
Summary. We introduced the Evolution Transformer, a sequence
model fully based on the Transformer architecture that character-
izes a family of ES. Using Evolutionary Algorithm Distillation, we
cloned various BBO algorithms, which can afterwards be deployed
on unseen optimization tasks. It provides a novel point of view for
data-driven evolutionary optimization. While using meta-evolution
to directly optimize the Transformer parameters is feasible, it results
in overfitting to the considered meta-training problems. Finally, we
introduced Self-Referential Evolutionary Algorithm Distillation,
which alleviates the need for a teacher or meta-optimization algo-
rithm. Instead, it uses random parameter perturbations to bootstrap
observed self-improvements. We hypothesize that this procedure
can facilitate the open-ended discovery of novel in-context evolu-
tionary optimization algorithms.
Limitations. While our Transformer-based model can be trained
efficiently on modern hardware accelerators, the deployment comes
with large memory requirements, especially for a large number of
search dimensions or population members. Therefore, we had to
employ a sliding context window. This ultimately limits the power
of in-context learning. While we provide empirical insights into the
behavior of the Evolution Transformer, we still lack a full mechanis-
tic understanding of such a ‚Äôblack-box BBO‚Äô. Finally, self-referential
training can be unstable and appears to jump between local optima.
Stabilization will require a better theoretical understanding of the
learning dynamics induced by mixing evolutionary perturbation/-
data generation and gradient descent-based self-distillation.
Future Work. We are interested in the open-ended discovery of
novel algorithms leveraging SR-EAD. While we experimented solely
with evolutionary optimization in the inner loop, the framework is
more general and could be applied to any sequential algorithm with
a suitable filtering mechanism. Furthermore, recent developments
in state space models may provide an opportunity to address the
context length limitations. Finally, further benchmarking on a more
diverse task sets will be required [18].


--- Page 9 ---
Evolution Transformer: In-Context Evolutionary Optimization
ArXiv, March 2024, Preprint
REFERENCES
[1] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George
Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transfor-
mations of Python+NumPy programs. (2018). http://github.com/google/jax
[2]
Yutian Chen, Matthew W Hoffman, Sergio G√≥mez Colmenarejo, Misha Denil, Timothy P Lillicrap, Matt Botvinick,
and Nando Freitas. 2017. Learning to learn without gradient descent by gradient descent. In International Conference
on Machine Learning. PMLR, 748‚Äì756.
[3] Yutian Chen, Xingyou Song, Chansoo Lee, Zi Wang, Richard Zhang, David Dohan, Kazuya Kawakami, Greg
Kochanski, Arnaud Doucet, Marc‚Äôaurelio Ranzato, et al. 2022.
Towards learning universal hyperparameter
optimizers with transformers. Advances in Neural Information Processing Systems 35 (2022), 32053‚Äì32068.
[4] Lucio M Dery, Abram L Friesen, Nando De Freitas, Marc‚ÄôAurelio Ranzato, and Yutian Chen. 2022. Multi-step
Planning for Automated Hyperparameter Optimization with OptFormer. arXiv preprint arXiv:2210.04971 (2022).
[5] C Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem. 2021. Brax‚ÄìA
Differentiable Physics Engine for Large Scale Rigid Body Simulation. arXiv preprint arXiv:2106.13281 (2021).
[6]
Nikolaus Hansen. 2006. The CMA evolution strategy: a comparing review. Towards a new evolutionary computation:
Advances in the estimation of distribution algorithms (2006), 75‚Äì102.
[7] Nikolaus Hansen, Anne Auger, Steffen Finck, and Raymond Ros. 2010. Real-parameter black-box optimization
benchmarking 2010: Experimental setup. Ph.D. Dissertation. INRIA.
[8]
Charles R Harris, K Jarrod Millman, St√©fan J Van Der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric
Wieser, Julian Taylor, Sebastian Berg, Nathaniel J Smith, et al. 2020. Array programming with NumPy. Nature 585,
7825 (2020), 357‚Äì362.
[9]
John D Hunter. 2007. Matplotlib: A 2D graphics environment. IEEE Annals of the History of Computing 9, 03 (2007),
90‚Äì95.
[10]
Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals,
Tim Green, Iain Dunning, Karen Simonyan, et al. 2017. Population based training of neural networks. arXiv
preprint arXiv:1711.09846 (2017).
[11]
Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. 2021. Perceiver:
General perception with iterative attention. In International conference on machine learning. PMLR, 4651‚Äì4664.
[12]
Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. 2022. General-purpose in-context learning by
meta-learning transformers. arXiv preprint arXiv:2212.04458 (2022).
[13]
Louis Kirsch and J√ºrgen Schmidhuber. 2022. Eliminating meta optimization through self-referential meta learning.
arXiv preprint arXiv:2212.14392 (2022).
[14] Jannik Kossen, Neil Band, Clare Lyle, Aidan N Gomez, Thomas Rainforth, and Yarin Gal. 2021. Self-attention
between datapoints: Going beyond individual input-output pairs in deep learning. Advances in Neural Information
Processing Systems 34 (2021), 28742‚Äì28756.
[15]
Siddarth Krishnamoorthy, Satvik Mehul Mashkaria, and Aditya Grover. 2022. Generative pretraining for black-box
optimization. arXiv preprint arXiv:2206.10786 (2022).
[16]
Robert Lange, Tom Schaul, Yutian Chen, Chris Lu, Tom Zahavy, Valentin Dalibard, and Sebastian Flennerhag. 2023.
Discovering Attention-Based Genetic Algorithms via Meta-Black-Box Optimization. In Proceedings of the Genetic
and Evolutionary Computation Conference. 929‚Äì937.
[17]
Robert Lange, Tom Schaul, Yutian Chen, Tom Zahavy, Valentin Dalibard, Chris Lu, Satinder Singh, and Sebastian
Flennerhag. 2023. Discovering evolution strategies via meta-black-box optimization. In Proceedings of the Companion
Conference on Genetic and Evolutionary Computation. 29‚Äì30.
[18] Robert Lange, Yujin Tang, and Yingtao Tian. 2024. NeuroEvoBench: Benchmarking Evolutionary Optimizers for
Deep Learning Applications. Advances in Neural Information Processing Systems 36 (2024).
[19] Robert Tjarko Lange. 2021. MLE-Infrastructure: A Set of Lightweight Tools for Distributed Machine Learning
Experimentation. (2021). http://github.com/mle-infrastructure
[20] Robert Tjarko Lange. 2022. evosax: JAX-based Evolution Strategies. arXiv preprint arXiv:2212.04180 (2022).
[21] Robert Tjarko Lange. 2022. gymnax: A JAX-based Reinforcement Learning Environment Library. (2022). http:
//github.com/RobertTLange/gymnax
[22]
Robert Tjarko Lange, Yingtao Tian, and Yujin Tang. 2024. Large Language Models As Evolution Strategies. arXiv
preprint arXiv:2402.18381 (2024).
[23]
Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven
Hansen, Angelos Filos, Ethan Brooks, et al. 2022. In-context reinforcement learning with algorithm distillation.
arXiv preprint arXiv:2210.14215 (2022).
[24]
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. 2019. Set transformer: A
framework for attention-based permutation-invariant neural networks. In International conference on machine
learning. PMLR, 3744‚Äì3753.
[25] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani.
2023. Structured state space models for in-context reinforcement learning. arXiv preprint arXiv:2303.03982 (2023).
[26]
Chris Lu, Sebastian Towers, and Jakob Foerster. 2023. Arbitrary Order Meta-Learning with Simple Population-Based
Evolution. arXiv preprint arXiv:2303.09478 (2023).
[27]
Luke Metz, C Daniel Freeman, Niru Maheswaranathan, and Jascha Sohl-Dickstein. 2021. Training learned optimizers
with randomly initialized learned optimizers. arXiv preprint arXiv:2101.07367 (2021).
[28]
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding
by generative pre-training. (2018).
[29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,
and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal
of Machine Learning Research 21, 1 (2020), 5485‚Äì5551.
[30]
Ingo Rechenberg. 1978. Evolutionsstrategien. In Simulationsmethoden in der Medizin und Biologie. Springer, 83‚Äì114.
[31] Raymond Ros and Nikolaus Hansen. 2008. A simple modification in CMA-ES achieving linear time and space
complexity. In International conference on parallel problem solving from nature. Springer, 296‚Äì305.
[32] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. 2017. Evolution strategies as a scalable
alternative to reinforcement learning. arXiv preprint arXiv:1703.03864 (2017).
[33] Tom Schaul, Tobias Glasmachers, and J√ºrgen Schmidhuber. 2011. High dimensions and heavy tails for natural
evolution strategies. In Proceedings of the 13th annual conference on Genetic and evolutionary computation. 845‚Äì852.
[34] J√ºrgen Schmidhuber. 1987. Evolutionary principles in self-referential learning, or on learning how to learn: the
meta-meta-... hook. Ph.D. Dissertation. Technische Universit√§t M√ºnchen.
[35] Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O Stanley, and Jeff Clune. 2017.
Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for
reinforcement learning. arXiv preprint (2017).
[36]
Yujin Tang and David Ha. 2021. The sensory neuron as a transformer: Permutation-invariant neural networks for
reinforcement learning. Advances in Neural Information Processing Systems 34 (2021), 22574‚Äì22587.
[37] Yujin Tang, Yingtao Tian, and David Ha. 2022. EvoJAX: Hardware-Accelerated Neuroevolution. arXiv preprint
arXiv:2202.05008 (2022).
[38] Adaptive Agents Team. 2023.
Human-timescale adaptation in an open-ended task space.
arXiv preprint
arXiv:2301.07608 (2023).
[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and
Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).
[40]
Michael L Waskom. 2021. Seaborn: statistical data visualization. Journal of Open Source Software 6, 60 (2021), 3021.
[41] Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and J√ºrgen Schmidhuber. 2014. Natural
evolution strategies. The Journal of Machine Learning Research 15, 1 (2014).
.


--- Page 10 ---
ArXiv, March 2024, Preprint
Lange et al.
Figure 10: Full Learning Curve on Brax Tasks using a population size of ùëÅ= 128 and a Tanh Control Network. A SNES-distilled
EvoTF performs very competitively. Results are averaged across 5 independent runs.
A
ADDITIONAL RESULTS
A.1
Brax Learning Curves
We use the Brax [5] set of continuous control tasks and the Evo-
JAX [37] evaluation wrapper. We use the standard 4-layer tanh
activation network to control various robots. The population size
is ùëÅ= 128 and we tune all baselines and EvoTF hyperparame-
ters using a small grid search over ùúé0 and the learning rates. See
Figure 10.
A.2
Impact of Task Distribution for EAD
We compare the impact of the task distribution used to generate
distillation trajectories. See Figure 11.
Function
Reference
Property
Tasks
Sphere
Hansen et al. [p. 5, 7]
Separable (Indep.)
Small
Rosenbrock
Hansen et al. [p. 40, 7]
Moderate Condition
Medium
Discus
Hansen et al. [p. 55, 7]
High Condition
Medium
Rastrigin
Hansen et al. [p. 75, 7]
Multi-Modal (Local)
Medium
Schwefel
Hansen et al. [p. 100, 7]
Multi-Modal (Global)
Medium
BuecheRastrigin
Hansen et al. [p. 20, 7]
Separable (Indep.)
Large
AttractiveSector
Hansen et al. [p. 30, 7]
Moderate Condition
Large
Weierstrass
Hansen et al. [p. 80, 7]
Multi-Modal (Global)
Large
SchaffersF7
Hansen et al. [p. 85, 7]
Multi-Modal (Global)
Large
GriewankRosen
Hansen et al. [p. 95, 7]
Multi-Modal (Global)
Large
Table 1: Meta-BBO BBOB-Based Task Families.
Throughout all main experiments, we use the ‚ÄôMedium‚Äô set of
BBOB tasks.
B
EVOLUTION TRANSFORMER FEATURES &
MODEL HYPERPARAMETERS
Parameter
Value
Parameter
Value
Number Transformer Blocks
1
Number of MHSA Heads
2
Number of Perceiver Latents
4
Perceiver Latent Dim
32
Embedding Dimension
64
Batchsize
32
Sequence Length
32
Learning rate
0.0015
Gradient Clip Norm
1
Train steps
5000
Table 2: Evolution Transformer Hyperparameters.
The features used as inputs include:
(1) Solution: Normalized difference from mean, squared normal-
ized difference from mean, difference from best seen solution,
normalized difference from best seen solution, etc.
(2) Fitness: Improvement boolean, z-score, centered rank, nor-
malized in range, SNES weights, etc.
(3) Distribution: Finite-difference gradient, SNES gradient, evo-
lution paths, momentum paths
C
META-EVOLUTION HYPERPARAMETERS
Parameter
Value
Parameter
Value
Population Size
256
# Generations
1000
MetaBBO
Sep-CMA-ES
Perturbation Strength
0.005
Task Batch Size
64
Table 3: Meta-evolution hyperparameters.


--- Page 11 ---
Evolution Transformer: In-Context Evolutionary Optimization
ArXiv, March 2024, Preprint
Figure 11: Impact of task distribution for EAD performance. We compare using 1 (small), 5 (medium) and 10 (large) BBOB
functions with random offsets and mean intialization. Successful distillation does not require many optimization tasks. Results
are averaged across 5 independent runs.
D
SELF-REFERENTIAL EVOLUTIONARY
ALGORITHM DISTILLATION (SR-EAD)
HYPERPARAMETERS
Parameter
Value
Parameter
Value
Population Size
64
Perturbation Strength
0.004
Exponential Decay
0.99999
Filtering
Best
Table 4: SR-EAD hyperparameters.
E
SOFTWARE DEPENDENCIES
The codebase will be open-sourced under Apache 2.0 license and
publicly available under https://github.com/<anonymous>. All train-
ing loops and ES are implemented in JAX [1]. All visualizations
were done using Matplotlib [9] and Seaborn [40, BSD-3-Clause Li-
cense]. Finally, the numerical analysis was supported by NumPy
[8, BSD-3-Clause License]. Furthermore, we used the following
libraries: Evosax: Lange [20], Gymnax: Lange [21], Evojax: Tang
et al. [37], Brax: Freeman et al. [5].
F
COMPUTE REQUIREMENTS &
EXPERIMENT ORGANIZATION
The experiments were organized using the MLE-Infrastructure
[19, MIT license] training management system.
Simulations were conducted on a high-performance cluster using
between 3 and 5 independent runs (random seeds). We mainly
rely on individual V100S and up to 4 A100 NVIDIA GPUs. The
experiments take between 2 and 5 hours wall clock time.
