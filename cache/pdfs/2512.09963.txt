--- Page 1 ---
GoodSpeed: Optimizing Fair Goodput with Adaptive
Speculative Decoding in Distributed Edge Inference
Phuong Tran1, Tzu-Hao Liu1, Long Tan Le1, Tung-Anh Nguyen1, Van Quan La1, Eason Yu1, Han Shu1
Choong Seon Hong2, Nguyen H. Tran1
1School of Computer Science, The University of Sydney, Darlington, NSW 2006, Australia
2Department of Computer Science and Engineering, Kyung Hee University, Yongin-si, Gyeonggi-do 17104, Korea
Abstractâ€”Large language models (LLMs) have revolutionized
natural language processing, yet their high computational de-
mands pose significant challenges for real-time inference, es-
pecially in multi-user and resource-constrained environments.
Speculative decoding has emerged as a promising technique to
accelerate LLM inference by using lightweight draft models to
generate candidate tokens, which are subsequently verified by
a larger, more accurate model. However, ensuring both high
goodputâ€”the effective rate of accepted tokensâ€”and fairness
across multiple draft servers cooperating with a central veri-
fication server remains an open challenge. This paper introduces
GoodSpeed, a novel distributed inference framework that opti-
mizes goodput through adaptive speculative decoding. GoodSpeed
employs a central verification server that coordinates a set of
heterogeneous draft servers, each running a small language model
to generate speculative tokens. To manage resource allocation effec-
tively, GoodSpeed incorporates a gradient scheduling algorithm
that dynamically assigns token verification tasks, maximizing a
logarithmic utility function to ensure proportional fairness across
servers. By processing speculative outputs from all draft servers in
parallel, the framework enables efficient collaboration between the
verification server and distributed draft generators, streamlining
both latency and throughput. Through rigorous fluid sample
path analysis, we show that GoodSpeed converges to the optimal
goodput allocation in steady-state conditions and maintains near-
optimal performance with provably bounded error under dynamic
workloads. These results demonstrate that GoodSpeed provides
a scalable, fair, and efficient solution for multi-server speculative
decoding in distributed LLM inference systems.
Index Termsâ€”Speculative Decoding, Gradient Scheduling Algo-
rithm, Resource Allocation, Large Language Models, Distributed
Edge Inference
I. Introduction
The emergence of agentic AI systems, autonomous agents
powered by large language models (LLMs), e.g., Llama3-
70B, capable of performing complex reasoning, planning, and
communication, has introduced transformative capabilities for
applications at the network edge. These agents facilitate tasks
such as protocol adaptation, semantic data processing, and
distributed decision-making.However, deployingsuchfunctions
on resource-constrained edge hardware remains challenging,
as multi-step inference and contextual reasoning typically
exceed the computational and memory limitations of most edge
devices. Small language models (SLMs), e.g., Qwen3-0.6B,
while suitable for local deployment, often fall short in delivering
the required accuracy for complex tasks. As a result, existing
architectures rely heavily on offloading to either high-capacity
servers orcentralized cloud-basedLLM services(e.g., OpenAIâ€™s
ChatGPT, Anthropicâ€™s Claude, Microsoftâ€™s AutoGen), where
edge devices forward input data for remote inference.
The reliance on communication between edge devices and
centralized cloud systems introduces several critical challenges.
First, the performance of large language models (LLMs) is
hindered by inference latency, which arises from both round-
trip communication delays and the computationally intensive
nature of LLM inference. These factors collectively increase
response times and energy consumption, posing limitations
for latency-sensitive interactive applications. Second, as the
number of edge devices scales, centralized servers face growing
computational and bandwidth demands, leading to scalability
bottlenecks and elevated operational costs. These constraints
complicate the deployment of LLMs that require low latency
and high throughput responses.
Recent developments in LLM serving systems offer potential
solutions. DistServe improves resource efficiency by separating
prefill and decoding phases across GPUs [1], while Sarathi-
Serve addresses throughput-latency tradeoffs through chunked-
prefills and stall-free scheduling [2]. The notion of smooth
goodput has also been introduced as a unified metric to
balance service-level objectives (SLOs) and performance [3].
Among these, speculative decoding emerges as a promising
approach, using smaller draft models to generate candidate token
sequences verified in parallel by a more capable model, thus
reducing latency. However, implementing speculative decoding
in distributed edge systems is underexplored.
To address the aforementioned challenges of distributed LLM
inference, we propose GoodSpeed, a framework for opti-
mizing Goodput with Adaptive Speculative Decoding for
Distributed Edge Inference. GoodSpeed equips edge servers
with lightweight small language models (SLMs) that locally
generate speculative tokens, which are subsequently verified by a
central server running a full-scale large language model (LLM).
Designed for deployment in resource-constrained and latency-
sensitive environments, GoodSpeed operates as a coordinated
serving framework in which the central server dynamically
manages verification and feedback across a heterogeneous pool
of draft servers. This architecture enables scalable inference
by parallelizing speculative token generation at the edge and
leveraging centralized verification to minimize latency, while
arXiv:2512.09963v2  [cs.DC]  14 Dec 2025


--- Page 2 ---
jointly optimizing goodput and fairness across the distributed
system. The key contributions are summarized as follows.
â€¢ We propose GoodSpeed, a distributed edge inference sys-
tem that leverages speculative decoding to reduce latency.
In GoodSpeed, multiple draft servers use lightweight
SLMs to autoregressively generate speculative token se-
quences. These sequences are then sent to a central
verification server equipped with a large target model for
validation. While each draft server generates tokens se-
quentially, GoodSpeed enables parallel generation across
these servers. Meanwhile, the verification server efficiently
verifies batches of speculative tokens in parallel using GPU
acceleration, significantly reducing the overall response
time.
â€¢ For efficiency and fairness, GoodSpeed employs a
gradient-based scheduling algorithm that dynamically de-
termines the optimal number of speculative tokens to be
generated by each draft server. The key innovation of this
scheduling approach lies in its use of estimated token
acceptance rates for the SLM at each draft server, which
serve as critical inputs for estimating the expected goodput
required by the scheduling process.
â€¢ We provide a rigorous theoretical analysis of GoodSpeed.
Using fluid model and sample path techniques, we prove
that the sequence of token allocations generated by
GoodSpeed converges to a state that maximizes the
systemâ€™s goodput.
â€¢ We conduct extensive experiments using state-of-the-
art LLMs, including Qwen3-14B and Llama-3-70B, to
validate our frameworkâ€™s performance. The results demon-
strate that our dynamic scheduling algorithm achieves
substantial gains in system-wide goodput and reduces end-
to-end latency compared to baseline methods, confirming
its effectiveness.
II. Background and Related Works
A. Speculative Decoding in LLM Inference
1) LLM Inference: The standard method for LLM inference is
autoregressive decoding, where tokens are generated sequen-
tially, with each new token conditioned on the full sequence of
its predecessors [4]. Mathematically, given a sequence of tokens
ğ‘ 1, ğ‘ 2, . . . , ğ‘ ğ‘—âˆ’1, an LLM model ğ‘€computes the conditional
probability distribution for the next token ğ‘ ğ‘—as:
ğ‘ƒğ‘€(ğ‘ ğ‘—| ğ‘ 1, ğ‘ 2, . . . , ğ‘ ğ‘—âˆ’1) = softmax(ğ‘ŠÂ· â„ğ‘—âˆ’1)
where â„ğ‘—âˆ’1 is the hidden state of the Transformer-based model
ğ‘€at step ğ‘—âˆ’1, and ğ‘Šis the modelâ€™s weight matrix mapping the
hidden state to the vocabulary logits. The model ğ‘€then selects
the next token (e.g., via greedy sampling, ğ‘ ğ‘—= arg maxğ‘ ğ‘ƒğ‘€(ğ‘ |
ğ‘ 1, . . . , ğ‘ ğ‘—âˆ’1)) and appends it to the sequence.
In practice, this process involves two distinct computational
phases: (1) a compute-bound prefill phase, where the clientâ€™s
prompt is processed in a single, parallel forward pass across
all tokens, dominated by large matrix multiplications and atten-
tion operations; and (2) a memory-bandwidth-bound decoding
phase, which generates the response one token at a time by
repeatedly accessing a cached set of intermediate states. The
sequential nature of the decoding phase represents a significant
latency bottleneck in client-server applications [5].
2) Speculative Decoding: Traditional decoding approaches,
such as greedy or beam search, which generate tokens sequen-
tially, requiring each one to be produced and verified before
advancing to the next. This sequential dependency imposes
significant latency, particularly in real-time or interactive sce-
narios. Speculative decoding (SD) mitigates this bottleneck
by performing fast drafting and verification in parallel. It
works as an efficient method for faster decoding without
compromising quality. A lightweight but weaker draft model ğ‘€ğ‘
autoregressively generates ğ‘†candidate tokens by its sampling
distribution ğ‘ğ‘—(ğ‘ ) := ğ‘ƒğ‘€ğ‘(Â· |, ğ‘ 1, . . . , ğ‘ ğ‘—âˆ’1)
ğ‘ ğ‘—âˆ¼ğ‘ğ‘—(ğ‘ ),
ğ‘—= 1, . . . , ğ‘†.
A larger target model ğ‘€ğ‘then computes the probabilities
of candidate tokens using its sampling distribution ğ‘ğ‘—(ğ‘ ) :=
ğ‘ƒğ‘€ğ‘(Â· | ğ‘ 1, . . . , ğ‘ ğ‘—âˆ’1), ğ‘—= 1, . . . , ğ‘†, in parallel. For each
candidate ğ‘ ğ‘—, draw a uniform random variable ğ‘Ÿğ‘—âˆ¼ğ‘ˆ(0, 1), ğ‘—=
1, . . . , ğ‘†. The token ğ‘ ğ‘—is accepted if ğ‘Ÿğ‘—â‰¤ğ‘ğ‘—(ğ‘ ğ‘—)
ğ‘ğ‘—(ğ‘ ğ‘—) , ensuring the
draft modelâ€™s token is retained only if its probability aligns
sufficiently with the target modelâ€™s distribution; otherwise, it
is rejected. Then, the number of accepted tokens is defined
ğ‘š:= min({ ğ‘—âˆ’1 | 1 â‰¤ğ‘—â‰¤ğ‘†, ğ‘Ÿğ‘—> ğ‘ğ‘—(ğ‘ ğ‘—)/ğ‘ğ‘—(ğ‘ ğ‘—)} âˆª{ğ‘†}). If
ğ‘š< ğ‘†, we sample the last token ğ‘ â€² using the normalized dis-
tribution of max(0, ğ‘ğ‘š+1(ğ‘ ) âˆ’ğ‘ğ‘š+1(ğ‘ )); otherwise, we sample
ğ‘ â€² from ğ‘ğ‘†+1(ğ‘ ). The accepted token output using SD will be
ğ‘ prefix, ğ‘ 1, . . . , ğ‘ ğ‘š, ğ‘ â€².
From [6], the token acceptance probability is
ğ›¼:= Eğ‘ âˆ¼ğ‘(Â·) [min (1, ğ‘(ğ‘ )/ğ‘(ğ‘ ))] .
Since the large model verifies candidate tokens in parallel,
verification typically completes within the time of a single token
generation, enabling a 2â€“3Ã— speedup on T5-XXL compared
with autoregressively decoding [6]. The effectiveness of SD
depends on the acceptance rate of drafted tokens, which is
influenced by factors such as draft model quality and behavioral
alignment between the draft and verification models [7]. A
higher acceptance rate leads to greater speedup.
Recent advances in SD focus on improving the quality of
speculative drafts, enabling self-speculative generation, and
optimizing system-level efficiency. To enhance the quality and
diversity of speculative drafts, SpecInfer [8] use multiple small
models to generate diverse token trees at the cost of higher
management overhead. The authors in [9] reformulate decoding
as a Jacobi iteration to enable parallel ğ‘›-gram generation, but
struggle with error propagation, where a single incorrect token
can invalidate an entire draft. Other recent proposals focus on
further improving the efficiency and verification process of these
internal drafts [10], [11]. However, these self-speculation ap-


--- Page 3 ---
proaches require careful tuning of their architectural parameters
to balance performance and accuracy.
Despite these advances, existing SD approaches assume ide-
alized execution environments, often ignoring constraints such
as inter-model communication costs or hardware locality. Our
work addresses this gap by modelling an SD-based multi-
agent networked system, where draft and target models are
distributed across physical nodes and interact under various
input load datasets.
B. Distributed Edge Inference
1) Server-Side Inference Optimization: A substantial body of
work has addressed the server-side bottlenecks in LLM serving.
Continuous batching has emerged as a central mechanism
for improving throughput by processing inference requests
as they arrive, rather than relying on static batching. This
approach was introduced in Orca [12] and further advanced in
vLLM [13], which proposed PagedAttention to reduce memory
fragmentation and improve memory reuse. Subsequent frame-
works such as Sarathi-Serve [2] and DeepSpeed-FastGen [14]
incorporate techniques like dynamic input/output chunking to
further increase system efficiency.
Systems including ShuffleInfer [15], Splitwise [16], and Dist-
Serve [1] implement phase-specific scheduling to enhance
parallelism and adapt to network conditions. At scale, model-
parallel architectures such as HeteGen [17], ExeGPT [18], and
Helix [19] distribute model computation across GPUs or nodes
to increase aggregate throughput, though often with a trade-off
in per-request latency.
2) Client-Side and On-Device Optimization: Improving client-
side performance is essential for edge deployments. Lightweight
draft models can be made more efficient through post-training
quantization and distillation techniques. Approaches such as
GPTQ [20] and AWQ [21] significantly reduce memory and
compute requirements, making them well-suited for mobile
or embedded devices. Emerging work also explores self-
speculative architectures that eliminate the need for server-
side verification. Medusa [22] incorporates parallel decoding
heads to perform both drafting and verification internally, while
EAGLE [23] applies early-exit strategies to reduce inference
latency without external assistance.
C. Motivation
Works like Ring Attention [24] and TetriInfer [15] optimize
system-level metrics (memory, latency) but do not ensure
equitable goodput distribution across clients. Solutions such
as Prompt Cache [25] and ExeGPT [18] assume static or pre-
dictable inputs, ignoring dynamic prompt variations. Methods
like Helix [19] and MÂ´elange [26] use intricate partitioning or
allocation strategies, increasing computational overhead. Cloud
deployments [27] focus on cost efficiency, often leaving server
resources underutilized. Moreover, traditional SD systems re-
quire a uniform draft length across batched requests, leading
to inefficient decoding and limited scalability when handling
prefixes with varying acceptance rates.
To address these gaps, we introduce GoodSpeed, a novel frame-
work whose key contribution is a new edge-based distributed
architecture for SD. This architecture leverages lightweight
draft models on heterogeneous edge devices for local token
generation, coupled with a central server for efficient verification
and adaptive feedback, overcoming the latency and scalability
limitations of centralized SD systems. GoodSpeed further
enhances this design with gradient scheduling to ensure fair
resource allocation that facilitates efficient computation, and
provides asymptotic optimality via fluid sample path analysis,
validated by finite-interval experiments.
III. Problem Formulation
A. GoodSpeed System Architecture
Recent studies have extended SD to distributed settings. A com-
mon paradigm is edgeâ€“server collaboration, where lightweight
draft models run on edge devices and a central server verifies
proposed tokens. This structure supports heterogeneous devices
while enabling the server to batch verification requests across
clients, thereby improving utilization [28], [29]. To reduce
communication overhead, network-efficient inference protocols
have been proposed. For instance, DSSD [30] introduces
a split verification mechanism that transmits only minimal
accept/reject signals and required resample tokens, reducing
bandwidth usage while preserving output fidelity.
In this work, we consider a distributed edge inference system
comprising ğ‘edge-deployed draft servers, each equipped with
a lightweight draft model ğ‘€(ğ‘–)
ğ‘, ğ‘–= 1, . . . , ğ‘. These servers
interact with a single verification server hosting a target (large)
model ğ‘€ğ‘for verification, as illustrated in Figure 1. The
draft servers receive prompts from their respective end-users
and engage in diverse real-time applications, including natural
language translation, chatbot services, and code generation.
This distributed edge inference setup allows for low-latency
drafting close to end-users, minimizing data transfer overheads
and enabling scalable real-time processing across geographi-
cally dispersed clients. The verification server, endowed with
high-performance GPUs, serves as the central computational
resource, validating the speculative tokens drafted by the small
servers to ensure accuracy and efficiency. This validation
employs a draft-then-verify process inspired by SD. Lightweight
models on small servers first generate candidate token se-
quences, proposing potential continuations (steps 1âƒand 2âƒ).
The verification server then verifies these drafts in parallel,
accepting accurate prefixes that align with the target modelâ€™s
distribution and rejecting mismatches. Corrections are sampled
from an adjusted distribution, enabling faster inference while
maintaining fidelity to the original output (steps 3âƒ, 4âƒand
5âƒ). Each small server maintains a prefix (e.g., a prompt) for
its ongoing inference task, which is updated in every round
by appending the accepted tokens and the correction token (if
any) returned from the verification server after verification. This
prefix serves as the conditioning context for generating the next
set of draft tokens, ensuring that the inference process remains
coherent and aligned with the target modelâ€™s distribution across


--- Page 4 ---
Fig. 1: An overview of GoodSpeed framework: 1âƒDraft servers prepare draft tokens for current prompts; 2âƒDraft servers send
their draft tokens to the verification server; 3âƒDraft sequences are batched; 4âƒRejection-verification process and 5âƒGoodSpeedâ€™s
gradient scheduling for the allocation in the next round ğ‘¡+ 1; and 6âƒThe verification server sends optimal allocation to the draft
servers.
multiple rounds. The system operates in discrete time slots
ğ‘¡= 0, 1, 2, . . ., and the interaction between the small servers and
the verification server follows a structured flow: small servers
submit draft tokens to the verification server, which manages a
FIFO queue to process requests in the order of arrival, performs
verification and rejection, and provides feedback to the small
servers with updated allocations and accepted tokens (step 6âƒ).
This distributed architecture underpins GoodSpeed, designed to
optimize inference performance across draft servers considering
the verifying serverâ€™s limited resources, particularly under non-
stationary prompt conditions prevalent in dynamic operational
environments.
B. GoodSpeed Optimization Problem
GoodSpeed is formulated by the following utility maximization
problem:
max
ğ‘¥
ğ‘ˆ(ğ‘¥) :=
âˆ‘ï¸ğ‘
ğ‘–=1 ğ‘ˆğ‘–(ğ‘¥ğ‘–)
subject to
ğ‘¥âˆˆX,
(1)
where ğ‘¥ğ‘–denotes the goodput (i.e., expected number of accepted
tokens per unit time) of draft serverğ‘–, ğ‘¥= (ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘) âˆˆRğ‘
+
is the long-term average goodput vector, and X denotes the
achievable goodput region. This metric is computed as the limit
of the empirical average Â¯ğ‘¥ğ‘–(ğ‘‡) =
1
ğ‘‡
Ãğ‘‡
ğ‘¡=1 ğ‘¥ğ‘–(ğ‘¡) as the number
of time steps ğ‘‡approaches infinity, where ğ‘¥ğ‘–(ğ‘¡) represents
the realized goodput (accepted tokens) at time step ğ‘¡. The
long-term average goodput captures the steady-state throughput
achieved by each client under the frameworkâ€™s scheduling policy,
accounting for variations in acceptance rates and resource
allocation, and serves as the key performance metric optimized
by the utility function ğ‘ˆ(ğ‘¥) over the achievable region X. In
our paper, the utility function ğ‘ˆğ‘–: R+ â†’R is continuously
differentiable, strictly increasing, and strictly concave in each
component (e.g., ğ‘ˆğ‘–(ğ‘¥ğ‘–) = log ğ‘¥ğ‘–for proportional fairness [31]).
To define the achievable goodput region X, consider a
scheduling decision ğ‘˜= (ğ‘†1, ğ‘†2, . . . , ğ‘†ğ‘) âˆˆZğ‘
+ , where ğ‘†ğ‘–
is the proposed length of draft tokens allocated to client ğ‘–.
The expected goodput vector associated with ğ‘˜is ğœ‡(ğ‘˜) =
(ğœ‡1(ğ‘˜), ğœ‡2(ğ‘˜), . . . , ğœ‡ğ‘(ğ‘˜)) âˆˆRğ‘
+
ğœ‡ğ‘–(ğ‘˜) = 1 âˆ’ğ›¼ğ‘†ğ‘–+1
ğ‘–
1 âˆ’ğ›¼ğ‘–
,
representing the expected number of tokens generated for clientğ‘–
under speculative decoding [6], assuming a fixed acceptance rate
0 < ğ›¼ğ‘–< 1. This is the expected value of a geometric random
variable with success probability 1 âˆ’ğ›¼ğ‘–, capped at ğ‘†ğ‘–+ 1. We
assume non-degeneracy bounds: there exist constants 0 < ğœ‡â‰¤
Â¯ğœ‡< âˆsuch that ğœ‡â‰¤ğœ‡ğ‘–(ğ‘˜) â‰¤Â¯ğœ‡for all ğ‘–= 1, . . . , ğ‘and all
feasible ğ‘˜, ensuring X is bounded and non-degenerate.
The feasible set of GoodSpeedâ€™s scheduling decisions is
K = {ğ‘˜= (ğ‘†1, ğ‘†2, . . . , ğ‘†ğ‘) âˆˆZğ‘
+ :
âˆ‘ï¸ğ‘
ğ‘–=1 ğ‘†ğ‘–â‰¤ğ¶},
where the constant ğ¶denotes the hardware budget of the
verification server, representing the ideal number of tokens
per forward pass to fully utilize both compute and memory
bandwidth on modern GPUs [32]. Sample ğ¶values of H100
GPUs are given in Table I. It is selected through systematic
hardware profiling to balance memory usage, latency, and
throughput, while avoiding out-of-memory (OOM) errors at
the verification server. Let ğœ™(ğ‘˜) â‰¥0 denote the scheduling
probability of choosing decision ğ‘˜âˆˆK, with Ã
ğ‘˜âˆˆK ğœ™(ğ‘˜) = 1.


--- Page 5 ---
The goodput region X is the convex hull of all achievable long-
term average goodput vectors:
X = {Â¯ğ‘¥| Â¯ğ‘¥=
âˆ‘ï¸
ğ‘˜âˆˆK ğœ™(ğ‘˜)ğœ‡(ğ‘˜),
âˆ‘ï¸
ğ‘˜âˆˆK ğœ™(ğ‘˜) = 1}.
(2)
The optimization problem (1) admits a unique optimal solution
ğ‘¥âˆ—âˆˆX because ğ‘ˆğ‘–(ğ‘¥) is strictly concave and X is convex and
compact (guaranteeing that the maximum is attained). However,
the dynamic evolution of client prompts, which may transition
abruptly between domains (e.g., casual dialogue to technical
queries), introduces variability in the acceptance rate ğ›¼ğ‘–(ğ‘¡), the
probability that a drafted token is accepted. Consequently, the
problem (1) involves dynamically allocating ğ‘†ğ‘–(ğ‘¡) to optimize
ğ‘¥ğ‘–(ğ‘¡) while adhering to the verification serverâ€™s capacity ğ¶.
Therefore, directly solving this requires global knowledge of
ğ‘ˆ(ğ‘¥), which depends on all possible ğ›¼ğ‘–(ğ‘¡) and ğ¶, making it
computationally infeasible in real-time, especially with non-
stationary prompts.
C. GoodSpeedâ€™s Gradient Scheduling Algorithm
The optimization problem (1) defines a static benchmark for
the optimal long-term goodput allocation ğ‘¥âˆ—, under the assump-
tion of fixed acceptance rates and averaged system behavior.
However, distributed edge inference operates in a dynamic
environment where acceptance rates ğ›¼ğ‘–(ğ‘¡) fluctuate due to non-
stationary prompts and task variations, and system capacity may
vary over time. To adapt to these dynamics, GoodSpeed employs
a gradient-based scheduling algorithm that performs online
allocation at each time step t. This algorithm adjusts the number
of proposed tokens for each draft server for the next time slot
ğ‘¡+ 1, using real-time system estimates. By doing so, it ensures
adaptive resource allocation and asymptotically converges to the
optimal allocation ğ‘¥âˆ—, maintaining both efficiency and fairness
in dynamic conditions.
Central to this is the use of smoothed estimates for both
acceptance rates and goodput, which provide stable repre-
sentations of each clientâ€™s performance while incorporating
recent observations. As described in 1, at each time step ğ‘¡,
the framework maintains a vector of smoothed acceptance
rate estimates Ë†ğ›¼(ğ‘¡) = ( Ë†ğ›¼1(ğ‘¡), Ë†ğ›¼2(ğ‘¡), . . . , Ë†ğ›¼ğ‘(ğ‘¡)) âˆˆ(0, 1)ğ‘,
where Ë†ğ›¼ğ‘–(ğ‘¡) updates the probability that a drafted token of
server ğ‘–is accepted by the target model, after the verification
process (line 14). These estimates are updated using exponential
smoothing by the verification server in parallel as follows:
Ë†ğ›¼ğ‘–(ğ‘¡) = (1 âˆ’ğœ‚) Ë†ğ›¼ğ‘–(ğ‘¡âˆ’1) + ğœ‚Â·
1
ğ‘†ğ‘–(ğ‘¡)
ğ‘†ğ‘–(ğ‘¡)
âˆ‘ï¸
ğ‘—=1
min

1, ğ‘ğ‘—(ğ‘ ğ‘—)
ğ‘ğ‘–, ğ‘—(ğ‘ ğ‘—)

(3)
where the sum computes the empirical acceptance rate from
the verification outcomes at time ğ‘¡, with ğ‘ğ‘—and ğ‘ğ‘–, ğ‘—being
the target and draft serverâ€™s model ğ‘–probabilities on token
ğ‘ ğ‘—, and ğœ‚
âˆˆ
(0, 1) is a fixed smoothing parameter that
controls the adaptation to recent observations. We proposed
this smoothing estimate Ë†ğ›¼ğ‘–(ğ‘¡) to mitigate short-term noise from
prompt variations, ensuring that Ë†ğ›¼(ğ‘¡) converges to a stable
value reflective of the systemâ€™s ergodic behaviour under some
assumptions (outlined in section III-D).
Building on the acceptance estimates, the framework also
maintains a vector of smoothed goodput estimates ğ‘‹ğ›½(ğ‘¡) =
(ğ‘‹ğ›½
1 (ğ‘¡), ğ‘‹ğ›½
2 (ğ‘¡), . . . , ğ‘‹ğ›½
ğ‘(ğ‘¡)) âˆˆRğ‘
+ , where ğ‘‹ğ›½
ğ‘–(ğ‘¡) approximates
the expected number of accepted tokens per unit time for client
ğ‘–based on historical data (line 14). These estimates are updated
using exponential smoothing by the verification server:
ğ‘‹ğ›½
ğ‘–(ğ‘¡) = (1 âˆ’ğ›½)ğ‘‹ğ›½
ğ‘–(ğ‘¡âˆ’1) + ğ›½ğ‘¥ğ‘–(ğ‘¡),
(4)
where ğ‘¥ğ‘–(ğ‘¡) is the realized goodput for client ğ‘–at time ğ‘¡
(the number of accepted tokens plus one correction from
verification [33]), and ğ›½âˆˆ(0, 1) is a fixed smoothing pa-
rameter. The smoothing for ğ‘‹(ğ‘¡) further stabilizes the system
against fluctuations. Building on these estimates, the gradient
scheduling problem optimizes the allocation of draft token slots
ğ‘†(ğ‘¡+1) = (ğ‘†1(ğ‘¡+1), ğ‘†2(ğ‘¡+1), . . . , ğ‘†ğ‘(ğ‘¡+1)) âˆˆZğ‘
+ at each time
step ğ‘¡by maximizing the projection of the estimated goodput
onto the gradient of the utility function (line 15). This approach
ensures that each allocation step favors draft servers with the
highest marginal utility gain, promoting both efficiency and
fairness while adapting to the current state encoded in ğ‘‹(ğ‘¡)
[31]. Mathematically, the gradient scheduling problem solved
by the verification server at each time ğ‘¡is formulated as follows.
GoodSpeed-sched:
max
ğ‘†(ğ‘¡+1)
âˆ‘ï¸ğ‘
ğ‘–=1 âˆ‡ğ‘ˆğ‘–
 ğ‘‹ğ›½
ğ‘–(ğ‘¡) Ë†ğ‘¥ğ‘–(ğ‘¡+ 1)
s.t.
âˆ‘ï¸ğ‘
ğ‘–=1 ğ‘†ğ‘–(ğ‘¡+ 1) â‰¤ğ¶,
ğ‘†ğ‘–(ğ‘¡+ 1) âˆˆZ+, âˆ€ğ‘–.
(5)
Here, Ë†ğ‘¥ğ‘–(ğ‘¡+ 1) is the estimated goodput of the draft server ğ‘–at
timestep ğ‘¡+ 1, computed by the following:
Ë†ğ‘¥ğ‘–(ğ‘¡+ 1) = 1 âˆ’Ë†ğ›¼ğ‘–(ğ‘¡)ğ‘†ğ‘–(ğ‘¡+1)+1
1 âˆ’Ë†ğ›¼ğ‘–(ğ‘¡)
,
where Ë†ğ›¼ğ‘–(ğ‘¡) is the estimated acceptance rate obtained by (3).
D. Convergence Analysis
The purpose of this section is to establish the asymptotic
convergence of the GoodSpeedâ€™s gradient scheduling to the
optimal goodput ğ‘¥âˆ—of problem (1). Given the non-stationary
nature of acceptance rates Ë†ğ›¼ğ‘–(ğ‘¡) due to varying prompts, we
adapt fluid sample path techniques to analyze the long-term
behavior of the smoothed estimates ğ‘‹(ğ‘¡) and Ë†ğ›¼(ğ‘¡). Under
suitable assumptions, we show that the framework achieves
optimality in the fluid limit as the smoothing parameter ğ›½â†’0.
Assumption 1 (Ergodicity with Converging Time Average).
For each client ğ‘–âˆˆ{1, 2, . . . , ğ‘}, the sequence of acceptance
rates { Ë†ğ›¼ğ‘–(ğ‘¡), ğ‘¡= 0, 1, 2, . . . } is ergodic, such that the time
average
Â¯ğ›¼ğ‘–(ğ‘‡) = 1
ğ‘‡
ğ‘‡âˆ’1
âˆ‘ï¸
ğ‘¡=0
Ë†ğ›¼ğ‘–(ğ‘¡)
converges almost surely to a limit Â¯ğ›¼ğ‘–âˆˆ[0, 1] as ğ‘‡â†’âˆ.


--- Page 6 ---
Algorithm 1 GoodSpeed Scheduling Algorithm
1: Initialize: ğ›½, ğœ‚, ğ‘†ğ‘–(0), ğ‘‹ğ‘–(0), Ë†ğ›¼ğ‘–(0), âˆ€ğ‘–= 1, . . . , ğ‘
2: for each timestep ğ‘¡do
3:
for each draft server, ğ‘–= 1, . . . , ğ‘parallelly do
4:
Sample ğ‘†ğ‘–(ğ‘¡) draft tokens from ğ‘€(ğ‘–)
ğ‘
(step 1âƒ):
5:
for ğ‘—= 1 to ğ‘†ğ‘–(ğ‘¡) do
6:
ğ‘ğ‘–, ğ‘—(ğ‘ ) â†ğ‘ƒğ‘€(ğ‘–)
ğ‘(Â· | prefixğ‘–, ğ‘ ğ‘–,1, ..., ğ‘ ğ‘–, ğ‘—âˆ’1)
7:
ğ‘ ğ‘–, ğ‘—(ğ‘¡) âˆ¼ğ‘ğ‘–, ğ‘—(ğ‘ )
8:
end for
9:
ğ‘ ğ‘–(ğ‘¡) â†(prefixğ‘–(ğ‘¡), ğ‘ ğ‘–,1(ğ‘¡), ...ğ‘ ğ‘–,ğ‘†ğ‘–(ğ‘¡) (ğ‘¡))
10:
Send ğ‘ ğ‘–(ğ‘¡), ğ‘ğ‘–, ğ‘—to the verification server (step 2âƒ)
11:
end for
12:
Verification server:
13:
Does Batching (step 3âƒ) and Verification (step 4âƒ)
14:
Computes ğ‘¥ğ‘–(ğ‘¡), updates Ë†ğ›¼ğ‘–(ğ‘¡) using (3), and updates
ğ‘‹ğ‘–(ğ‘¡) using (4).
15:
Solves the GoodSpeed-sched problem (5) to obtain ğ‘†(ğ‘¡+
1) (step 5âƒ)
16:
Sends ğ‘†(ğ‘¡+ 1) to draft servers (step 6âƒ).
17: end for
This assumption is realistic in distributed edge-server setups,
and the ergodicity requirement is met if prompt variations (e.g.,
topic shifts in real-time translation) are sufficiently mixing,
which is typical in conversational AI where user inputs follow
predictable patterns over time. The smoothing parameter ğœ‚
can be dynamically adjusted based on observed variance in
Ë†ğ›¼ğ‘–(ğ‘¡) (e.g., reducing ğœ‚if variance exceeds a threshold). The
convergence of Â¯ğ›¼ğ‘–(ğ‘‡) is empirically achievable within 700
rounds, as detailed in Section IV-B3 and Figure 4.
Assumption 2 (Uniform Boundedness and Lipschitz Con-
tinuity). For all ğ‘¡â‰¥0 and each client ğ‘–âˆˆ{1, 2, . . . , ğ‘}, the
acceptance rate Ë†ğ›¼ğ‘–(ğ‘¡) is uniformly bounded by a constant Ë†ğ›¼max,
i.e., Ë†ğ›¼ğ‘–(ğ‘¡) â‰¤Ë†ğ›¼max < 1, âˆ€ğ‘–, ğ‘¡. Additionally, Ë†ğ›¼ğ‘–(ğ‘¡) is Lipschitz
continuous with a constant ğ¿> 0, such that
| Ë†ğ›¼ğ‘–(ğ‘¡+ 1) âˆ’Ë†ğ›¼ğ‘–(ğ‘¡)| â‰¤ğ¿Â· ğœ‚
where ğ¿depends on the maximum variation in the empirical
acceptance indicators min

1, ğ‘ğ‘—(ğ‘ ğ‘—)
ğ‘ğ‘–, ğ‘—(ğ‘ ğ‘—)

, and ğœ‚âˆˆ(0, 1) is the
smoothing parameter for Ë†ğ›¼ğ‘–(ğ‘¡).
The Lipschitz constant ğ¿captures the maximum rate of change,
influenced by the variability of min

1, ğ‘ğ‘—
ğ‘ğ‘–, ğ‘—

. With ğœ‚= ğ‘‚(1/ğ‘¡Ë†ğ›¼)
( Ë†ğ›¼> 0.5), the step size shrinks, ensuring smooth transitions.
Stochastic approximation theory [34] guarantees Lipschitz
continuity for bounded noise terms, with ğ¿â‰¤1 derived as
the maximum gradient, making this assumption theoretically
plausible.
Assumption 3 (Sufficient Smoothing and Step Size). The
smoothing parameter for acceptance rates is chosen as ğœ‚=
ğ‘‚(1/ğ‘¡Ë†ğ›¼) with Ë†ğ›¼âˆˆ(0.5, 1], and the step size for goodput
estimates satisfies ğ›½= ğ‘‚(1/ğ‘¡ğ›½) with ğ›½âˆˆ(0.5, 1], ensuring
that the ratio ğœ‚/ğ›½â†’0 as ğ‘¡â†’âˆ.
Theorem 1 (Uniform Convergence in Probability). Let
{ğ‘‹ğ›½(ğ‘¡)}ğ‘¡â‰¥0 be the scaled stochastic process governed by the
proposed algorithm with step-size parameter ğ›½> 0. Suppose
the initial condition ğ‘‹ğ›½(0) lies within a bounded set ğ´âŠ‚Rğ‘
+ .
Under Assumptions 1â€“3, for any ğœ–> 0, there exists a time
threshold ğ‘‡> 0 such that
lim
ğ›½â†’0
sup
ğ‘‹ğ›½(0)âˆˆğ´, ğ‘¡>ğ‘‡/ğ›½
Pr

âˆ¥ğ‘‹ğ›½(ğ‘¡) âˆ’ğ‘¥âˆ—âˆ¥> ğœ–

= 0,
where ğ‘¥âˆ—âˆˆRğ‘
+ is the unique globally attractive fixed point of
the fluid dynamics.
Proof. The proof of this theorem leverages the following
foundational theorems, which are formally established in Ap-
pendix VI. First, Theorem 2 shows that any weak limit of the
scaled process ğ‘‹ğ›½(ğ‘¡) is almost surely a Fluid Sample Path (FSP).
Then, Theorem 3 ensures that all FSPs starting from bounded
initial conditions converge uniformly to the globally attractive
fixed point ğ‘¥âˆ—, due to a Lyapunov argument and boundary
drift. Finally, Theorem 4 guarantees that the time-averaged
performance also converges in expectation to ğ‘¥âˆ—. Together, these
imply that the stochastic process concentrates near ğ‘¥âˆ—with
high probability for sufficiently small ğ›½and large enough time,
yielding the stated uniform convergence in probability.
â–¡
IV. Performance Evaluation
A. Experimental Setting
1) Testbed: To evaluate GoodSpeed, we conducted experi-
ments in a distributed inference environment designed to reflect
practical deployment scenarios, where a large verification server
collaborates with multiple lightweight draft servers to handle
heterogeneousworkloads.The verificationserver,equippedwith
an NVIDIA H100 GPU, hosted two target modelsâ€”Qwen3-14B
and Llama3.1-70B-Instruct-AWQ-INT4â€”representing distinct
verification configurations. Meanwhile, each draft server, run-
ning on an NVIDIA L4 GPU, performed initial generation using
lightweight models from the Llama3 and Qwen3 families, with
sizes ranging from 0.6B to 3B parameters. This heterogeneous
setup enabled us to evaluate the frameworkâ€™s ability to opti-
mize resource usage across diverse model sizes and inference
demands. In each time slot, the verification server processed
batched draft outputs collected from the draft servers. Table I
summarizes the experimental configurations, including model
choices, hardware specifications, the number of draft servers
(clients), and token budget constraints.
2) Datasets: The experiments leveraged eight public datasets
to simulate real-world applications. Alpaca and Awesome-
ChatGPT-Prompts focus on instruction tuning and conversa-
tional Q/A; CNN/DailyMail targets long-context summariza-
tion; OpenOrca and Chatbot Arena involve reasoning and
open-domain Q/A; GSM8K addresses mathematical problem
solving; SPIDER covers text-to-SQL generation; and HLE
includes high-difficulty, long-tail queries. These datasets were
distributed across four to eight draft servers to create a mix of
short, interactive prompts and longer, compute-intensive tasks.


--- Page 7 ---
Maximum token lengths were set to 50 or 150 tokens depending
on the setup, reflecting typical constraints in latency-sensitive
environments. To increase task diversity among clients, each
draft server was assigned a distinct dataset from the set above.
3) Hyperparameters Choice: A critical experimental param-
eter in our system is the capacity ğ¶. We select ğ¶based
on two primary factors: (1) H100 HBM3 memory usage:
Considering the memory footprint of the verification server
models (Qwen3-14B, Llama3.1-70B-Instruct-AWQ-INT4), in-
cluding model weights, intermediate buffers, and output logits,
and assuming the maximum prefix length observed in our
datasets, we constrain memory usage to remain below 75% of
the HBM3â€™s total 80GB capacity to prevent fragmentation and
ensure stability. (2) Latency tolerance: A larger ğ¶increases the
speculative draft length, but also leads to longer transmission
time between verification and draft server due to the need to
carry the full probability distributions for more tokens. This can
delay response time in latency-sensitive applications.
To balance available memory headroom and latency constraints,
we set the SD budget to ğ¶= {6, 20} for 150-token generations
and ğ¶= {24, 28} for 50-token generations, which helps achieve
a stable trade-off between efficiency and system responsiveness.
B. Main Results
1) Goodput Estimation: We evaluate the effectiveness of our
smoothed goodput estimation by comparing it with the actual
system-level goodput over time. Figure 2 shows results for
two representative scenariosâ€”Qwen3 and Llama3â€”each with
8 clients operating under dynamic and heterogeneous prompt
conditions. To reduce transient fluctuations, we apply a moving
average (MA) filter with a window size of 10 to both the
estimated and measured goodput curves. The plots demonstrate
a strong alignment between our estimated and the ground-truth
goodput, indicating that the proposed exponential smoothing
method captures system dynamics despite the stochastic nature
of SD and clientsâ€™ prompt variability. We visualize the empirical
standard deviation (square root of MA variance) as shaded
confidence bands around both curves.
These regions encompass most observed goodput peaks, further
confirming the stability and predictive fidelity of our estimation
process. This supports our design choice to use smoothed
estimates in the gradient scheduling algorithm, allowing for
robust and adaptive token allocation across non-stationary
environments and diverse LLMs.
2) Time Distribution: We evaluate the end-to-end wall time of
the GoodSpeed framework and compare it against two common
SD baselines: (1) Fixed-S: each draft server consistently gener-
ates a fixed number of tokens per iteration, with ğ‘†ğ‘–= ğ¶/ğ‘;
and (2) Random-S: each draft server randomly samples ğ‘†ğ‘–per
iteration, constrained such that the total across clients does
not exceed ğ¶. In Fig. 3, we see that GoodSpeed achieves
comparable total wall time to the Fixed-S baseline across both
Qwen3 and Llama3 settings. In contrast, Random-S exhibits a
5â€“25% increase in wall time due to scheduling inefficiencies.
To better understand this behavior, we decompose wall time
into three components: (1) Receiving time, the duration the
verification server waits for draft servers to complete token
generation and transmit their draft distributions, until the batch
is fully assembled; (2) Verification time, the time taken by the
verification process;
and (3) Sending time, the duration for communicating the
accepted tokens and next-round scheduling instructions back to
draft servers.
We see that the receiving and verification dominate total wall
time, while sending time contributes less than 0.1%. The
increased receiving time in GoodSpeed and Random-S results
from variable drafting lengths, requiring the verification server
to wait for the slowest client (largest ğ‘†ğ‘–), unlike Fixed-S,
which enables faster batch formation without this bottleneck.
In contrast, GoodSpeed achieves 5% lower verification time
than Fixed-S, reflecting more efficient workload balancing under
dynamic client behavior.
3) Convergence of Utility Function: Figure 4 illustrates the
utility functionğ‘ˆ(Â·) computed on the empirical average goodput
Â¯ğ‘¥(ğ‘‡) =
1
ğ‘‡
Ãğ‘‡
ğ‘¡=1 ğ‘¥(ğ‘¡) over the first 600 iterations, showing that
the GoodSpeed curve starts lower due to initial exploration but
rises steadily, stabilizing by approximately iteration 400, and
consistently surpasses the utilities of the baselines (â€œFixed-ğ‘†â€ for
uniform allocation and â€œRandom-ğ‘†â€ for stochastic assignment).
The observed stabilization of ğ‘ˆ( Â¯ğ‘¥(ğ‘‡)) within 600 iterations
shows that the system is converging, which can be explained
by looking at the utility function and how the system works.
The utility function ğ‘ˆ(ğ‘¥) = Ãğ‘
ğ‘–=1 log ğ‘¥ğ‘–is smooth and achieves
the highest value at the optimal ğ‘¥âˆ—. As the smoothed goodput
estimate ğ‘‹ğ›½(ğ‘¡) gets closer to ğ‘¥âˆ—over time (c.f. Theorem 3),
the value of ğ‘ˆ(ğ‘‹ğ›½(ğ‘¡)) rises and levels off toward ğ‘ˆ(ğ‘¥âˆ—). This
smoothed estimate ğ‘‹ğ›½(ğ‘¡) helps decide how many speculative
tokens ğ‘†ğ‘–(ğ‘¡) each client gets, using a simple adjustment
method, which moves the system toward the optimal allocation.
Therefore, the observed stabilization of ğ‘ˆ( Â¯ğ‘¥(ğ‘‡)) within 600
iterations suggests that the utility reaching a steady value close
toğ‘ˆ(ğ‘¥âˆ—) after the initial adjustments, matching Theorem 1. With
ğ›½= 0.5, the threshold ğ‘‡/ğ›½â‰ˆ400âˆ’600 iterations (derived from
ğ‘‡â‰ˆ200 âˆ’300 rounds) aligns well with the stabilization range,
indicating the system surpasses the point where the probability
of significant deviation âˆ¥ğ‘‹ğ›½(ğ‘¡) âˆ’ğ‘¥âˆ—âˆ¥> ğœ–becomes negligible, a
trend supported by the lack of oscillations after 400 iterations.
V. Conclusion
We propose GoodSpeed, a distributed LLM inference frame-
work that combines speculative decoding with gradient-based
scheduling for efficient and adaptive resource allocation between
a verification server and multiple draft servers. By formulating
inference as a utility maximization problem over the achievable
goodput region, we define a clear performance benchmark. Our
scheduling algorithm approximates the optimal allocation in real
time, adapting to non-stationary acceptance rates. Experiments
demonstrate fast convergence within 600 iterations and superior


--- Page 8 ---
TABLE I: Experimental configurations
Verification Model
Draft Model
Verification Server GPU
Draft Server GPU
ğ¶(tokens)
Number of Draft Servers
Max Token Length
Qwen3-14B
Qwen3-0.6B
H100
L4
24, 28
4
50
Qwen3-14B
Qwen3-0.6B/1.7B
H100
L4
16, 20
8
150
Llama-3.1-70B-Instruct
Llama 3.2-1B-Instruct/
H100
L4
16, 20
8
150
-AWQ-INT4
Llama 3.2-3B-Instruct
0
500
1000
1500
2000
2500
3000
3500
Iterations
0
5
10
15
GoodPut
C=16
real x
estimate x
0
500
1000
1500
2000
2500
3000
Iterations
0
5
10
15
GoodPut
C=20
real x
estimate x
(a) Qwen3 â€“ 8 clients
0
500
1000
1500
2000
2500
Iterations
0
5
10
15
GoodPut
C=16
real x
estimate x
0
500
1000
1500
2000
Iterations
0
5
10
15
GoodPut
C=20
real x
estimate x
(b) Llama3 â€“ 8 clients
Fig. 2: Comparison of estimated goodput vs real goodput across different LLMs with 8 clients.
16
20
C
0
5000
10000
Time(s)
Total Wall Time
Fixed-S
Random-S
GOODSPEED
16
20
C
0
2000
4000
Time(s)
Receiving Time
Fixed-S
Random-S
GOODSPEED
16
20
C
0
2500
5000
Time(s)
Verification Time
Fixed-S
Random-S
GOODSPEED
16
20
C
0
2
4
Time(s)
Sending Time
Fixed-S
Random-S
GOODSPEED
(a) Qwen3 â€“ 8 clients
16
20
C
0
2000
4000
Time(s)
Total Wall Time
Fixed-S
Random-S
GOODSPEED
16
20
C
0
500
1000
Time(s)
Receiving Time
Fixed-S
Random-S
GOODSPEED
16
20
C
0
2000
Time(s)
Verification Time
Fixed-S
Random-S
GOODSPEED
16
20
C
0
2
Time(s)
Sending Time
Fixed-S
Random-S
GOODSPEED
(b) Llama3 â€“ 8 clients
Fig. 3: Comparison of time distribution across different LLMs with 8 clients.
efficiency and fairness compared to baselines across diverse
datasets and model configurations.
VI. Appendix
In this section, we establish a sketch of proof for the asymptotic
convergence of the GoodSpeed framework to the optimal
goodput ğ‘¥âˆ—from the problem (1). We begin by producing key
lemmas on fluid sample path (inspired from [31]) properties, and
finally prove the main theorem on process convergence, uniform
attraction, and asymptotic optimality. We consider a sequence of
systems indexed by ğ›½â†’0 along B = {ğ›½ğ‘—, ğ‘—= 1, 2, . . . } with
ğ›½ğ‘—> 0. Processes are superscripted by ğ›½, e.g., ğ‘‹ğ›½(ğ‘¡), ğ›¼ğ›½(ğ‘¡),
ğ‘†ğ›½(ğ‘¡). The fluid-scaled processes are defined as follows:
â€¢ Scaled goodput estimates: ğ‘¥ğ›½(ğ‘¡) = ğ‘‹ğ›½(ğ‘¡/ğ›½), ğ‘¡âˆˆR+
â€¢ Scaled acceptance rates: Ëœğ›¼ğ›½(ğ‘¡) = ğ›¼ğ›½(ğ‘¡/ğ›½)
â€¢ Scaled realized goodputs: Ë†ğ‘“ğ›½(ğ‘¡) = ğ›½ÃâŒŠğ‘¡/ğ›½âŒ‹
ğ‘™=1
ğ‘¥ğ›½(ğ‘™), where
ğ‘¥ğ›½(ğ‘™) is the realized goodput vector at step ğ‘™
â€¢ Scaled allocation counts: Ë†ğ‘”ğ›½
ğ‘˜(ğ‘¡) = ğ›½Ë†ğºğ›½
ğ‘˜(ğ‘¡/ğ›½), where Ë†ğºğ›½
ğ‘˜(ğœ)
counts steps up to ğœusing decision ğ‘˜âˆˆKâ€˜
The composite scaled process is ğ‘§ğ›½= (ğ‘¥ğ›½, Ëœğ›¼ğ›½, Ë†ğ‘“ğ›½, Ë†ğ‘”ğ›½). A fixed
set of functions ğ‘§= (ğ‘¥, Ëœğ›¼, Ë†ğ‘“, Ë†ğ‘”) is defined as a fluid sample path
(FSP) if there exists a subsequence B0 âŠ†B such that ğ‘§ğ›½â†’ğ‘§
uniformly on compact sets (u.o.c.) as ğ›½â†’0 along B0, with the
initial condition satisfying âˆ¥ğ‘¥(0)âˆ¥< âˆ.
Lemma 1. (Basic Properties of FSPs) For any FSP ğ‘§, all
component functions are Lipschitz continuous on [0, âˆ) with


--- Page 9 ---
0
200
400
600
Iterations
2
4
U(x)
C=24
Fixed-S
GOODSPEED
0
200
400
600
Iterations
2
4
U(x)
C=28
Fixed-S
GOODSPEED
(a) Qwen3 â€“ 4 clients
0
200
400
600
Iterations
0
5
U(x)
C=16
Fixed-S
Random-S
GOODSPEED
0
200
400
600
Iterations
0
5
U(x)
C=20
Fixed-S
Random-S
GOODSPEED
(b) Qwen3 â€“ 8 clients
0
200
400
600
Iterations
2.5
5.0
7.5
U(x)
C=16
Fixed-S
Random-S
GOODSPEED
0
200
400
600
Iterations
5
10
U(x)
C=20
Fixed-S
Random-S
GOODSPEED
(c) Llama3 â€“ 8 clients
Fig. 4: Convergence of utility function ğ‘ˆover iterations for different models and client settings.
Lipschitz constant ğ¶+ âˆ¥ğ‘¥(0)âˆ¥, where ğ¶> 0 is a constant
depending only on the system parameters (including bounds
ğœ‡and Â¯ğœ‡). Moreover, the functions Ë†ğ‘“(ğ‘¡) and Ë†ğ‘”ğ‘˜(ğ‘¡) are nonde-
creasing and satisfy the relations
Ë†ğ‘“(ğ‘¡) =
âˆ‘ï¸
ğ‘˜âˆˆK ğœ‡(ğ‘˜; Ëœğ›¼(ğ‘¡)) Ë†ğ‘”ğ‘˜(ğ‘¡),
ğ‘¡â‰¥0
where ğœ‡(ğ‘˜; Ëœğ›¼(ğ‘¡)) is the goodput vector evaluated at the scaled
acceptance rates Ëœğ›¼(ğ‘¡), and the total time measure is normal-
ized as Ã
ğ‘˜âˆˆK Ë†ğ‘”ğ‘˜(ğ‘¡) = ğ‘¡for ğ‘¡â‰¥0.
Proof. The discrete update for goodput estimates is given by
ğ‘‹ğ›½
ğ‘›(ğ‘™) = (1 âˆ’ğ›½)ğ‘‹ğ›½
ğ‘›(ğ‘™âˆ’1) + ğ›½ğ‘¥ğ›½
ğ‘›(ğ‘™), where ğ‘¥ğ›½
ğ‘›(ğ‘™) is the realized
goodput at step ğ‘™, bounded by Â¯ğœ‡due to the verification constraint.
The difference between consecutive updates is
|ğ‘‹ğ›½
ğ‘›(ğ‘™) âˆ’ğ‘‹ğ›½
ğ‘›(ğ‘™âˆ’1)| â‰¤ğ›½(2 Â¯ğœ‡+ ğ‘‹ğ›½
ğ‘›(0)),
since ğ‘¥ğ›½
ğ‘›(ğ‘™) â‰¤Â¯ğœ‡and the initial condition ğ‘‹ğ›½
ğ‘›(0) is finite. For
acceptance rates, the update rule 3 results in a change bounded
by ğœ‚times the maximum variation of the acceptance indicator,
which is at most 1. Scaling these updates by ğ›½and taking the
uniform on compact sets (u.o.c.) limit as ğ›½â†’0 along B0,
the Lipschitz constant of the limit functions is determined as
ğ¶= 2 Â¯ğœ‡+ ğ¿, where ğ¿arises from the Lipschitz continuity
of ğ›¼ğ‘–(ğ‘¡) as per Assumption 2. The nondecreasing property
of Ë†ğ‘“ğ›½(ğ‘¡) and Ë†ğ‘”ğ›½
ğ‘˜(ğ‘¡) stems from their definition as cumulative
sums over discrete steps. In the limit, Ë†ğ‘“(ğ‘¡) = limğ›½â†’0 Ë†ğ‘“ğ›½(ğ‘¡) and
Ë†ğ‘”ğ‘˜(ğ‘¡) = limğ›½â†’0 Ë†ğ‘”ğ›½
ğ‘˜(ğ‘¡) retain this monotonicity. The relation
Ë†ğ‘“(ğ‘¡)
=
Ã
ğ‘˜âˆˆK ğœ‡(ğ‘˜; Ëœğ›¼(ğ‘¡)) Ë†ğ‘”ğ‘˜(ğ‘¡) holds because the realized
goodput is the accepted portion of allocated tokens, with
ğœ‡(ğ‘˜; Ëœğ›¼(ğ‘¡)) representing the expected goodput for allocation ğ‘˜
given Ëœğ›¼(ğ‘¡).
â–¡
Lemma 2. (Differential Inclusion and Boundary Conditions)
For almost all ğ‘¡â‰¥0, an FSP ğ‘§satisfies
ğ‘¥â€²(ğ‘¡) = ğ‘£(ğ‘¡) âˆ’ğ‘¥(ğ‘¡),
ğ‘£(ğ‘¡) =
âˆ‘ï¸
ğ‘˜âˆˆK Ë†ğ‘”â€²
ğ‘˜(ğ‘¡)ğœ‡(ğ‘˜; Ëœğ›¼(ğ‘¡)) âˆˆarg max
ğ‘£âˆˆX(ğ‘¡)
âˆ‘ï¸ğ‘
ğ‘–=1
1
ğ‘¥ğ‘–(ğ‘¡) ğ‘£ğ‘–,
and X(ğ‘¡) uses ğœ‡(ğ‘˜; Ëœğ›¼(ğ‘¡)). If ğ‘¥ğµ(ğ‘¡) = 0 for ğµâŠ†{1, . . . , ğ‘} and
ğ‘¥ğ‘–(ğ‘¡) > 0 for ğ‘–âˆ‰ğµ, then with ğ‘â‰¥ğœ‡, we have ğ‘‘+
ğ‘‘ğ‘¡
Ã
ğ‘–âˆˆğµğ‘¥ğ‘–(ğ‘¡) â‰¥
ğ‘> 0.
Proof. The differential equation is derived from the integral
form of the FSP. The scaled goodput update leads to the
relation ğ‘¥ğ‘›(ğ‘¡) âˆ’ğ‘¥ğ‘›(0) =
Ë†ğ‘“ğ‘›(ğ‘¡) âˆ’
âˆ«ğ‘¡
0 ğ‘¥ğ‘›(ğœ‰)ğ‘‘ğœ‰, where Ë†ğ‘“ğ‘›(ğ‘¡) is
the cumulative realized goodput. Differentiating with respect to
ğ‘¡at points of continuity (almost everywhere due to Lipschitz
continuity from Lemma 1), we obtain ğ‘¥â€²(ğ‘¡) =
ğ‘‘
ğ‘‘ğ‘¡Ë†ğ‘“(ğ‘¡) âˆ’ğ‘¥(ğ‘¡).
Since Ë†ğ‘“(ğ‘¡) = Ã
ğ‘˜âˆˆK ğœ‡(ğ‘˜; Ëœğ›¼(ğ‘¡)) Ë†ğ‘”ğ‘˜(ğ‘¡) and Ë†ğ‘”ğ‘˜(ğ‘¡) is absolutely
continuous as a limit of step functions, the derivative is
ğ‘‘
ğ‘‘ğ‘¡Ë†ğ‘“(ğ‘¡) = Ã
ğ‘˜âˆˆK Ë†ğ‘”â€²
ğ‘˜(ğ‘¡)ğœ‡(ğ‘˜; Ëœğ›¼(ğ‘¡)). The scheduling algorithm
selects Ë†ğ‘”â€²
ğ‘˜(ğ‘¡) to maximize âˆ‡ğ‘ˆ(ğ‘¥(ğ‘¡)) Â· ğ‘£, where âˆ‡ğ‘ˆğ‘–(ğ‘¥ğ‘–) =
1
ğ‘¥ğ‘–
for ğ‘ˆ(ğ‘¥) = Ã
ğ‘–log ğ‘¥ğ‘–. Thus,
ğ‘£(ğ‘¡) =
âˆ‘ï¸
ğ‘˜âˆˆK Ë†ğ‘”â€²
ğ‘˜(ğ‘¡)ğœ‡(ğ‘˜; Ëœğ›¼(ğ‘¡)) âˆˆarg max
ğ‘£âˆˆX(ğ‘¡)
âˆ‘ï¸ğ‘
ğ‘–=1
1
ğ‘¥ğ‘–(ğ‘¡) ğ‘£ğ‘–,
reflecting the logarithmic utilityâ€™s emphasis on fairness by
prioritizing clients with lower ğ‘¥ğ‘–(ğ‘¡). For the boundary condition,
if ğ‘¥ğµ(ğ‘¡) = 0 for some ğµ, the gradient
1
ğ‘¥ğ‘–(ğ‘¡) â†’+âˆas ğ‘¥ğ‘–(ğ‘¡) â†’0+,
compelling the maximization to allocate tokens to ğµ. Since X(ğ‘¡)
includes points ğœ‡(ğ‘˜; Ëœğ›¼(ğ‘¡)) with Ã
ğ‘–âˆˆğµğœ‡ğ‘–(ğ‘˜; Ëœğ›¼(ğ‘¡)) â‰¥ğœ‡for some
ğ‘˜(due to ğœ‡> 0), and Ã
ğ‘˜Ë†ğ‘”â€²
ğ‘˜(ğ‘¡) = 1 with Ë†ğ‘”â€²
ğ‘˜(ğ‘¡) â‰¥0, we have
ğ‘‘+
ğ‘‘ğ‘¡
Ã
ğ‘–âˆˆğµğ‘¥ğ‘–(ğ‘¡)
=
âˆ‘ï¸
ğ‘–âˆˆğµ
âˆ‘ï¸
ğ‘˜âˆˆK
Ë†ğ‘”â€²
ğ‘˜(ğ‘¡)ğœ‡ğ‘–(ğ‘˜; Ëœğ›¼(ğ‘¡)) â‰¥ğœ‡
âˆ‘ï¸
ğ‘˜:ğœ‡ğµ(ğ‘˜)â‰¥ğœ‡
Ë†ğ‘”â€²
ğ‘˜(ğ‘¡) â‰¥ğœ‡> 0
where ğœ‡ğµ(ğ‘˜) = Ã
ğ‘–âˆˆğµğœ‡ğ‘–(ğ‘˜; Ëœğ›¼(ğ‘¡)), ensuring a positive right-
hand derivative.
â–¡
Theorem 2. (Process-Level Convergence): The sequence {ğ‘§ğ›½}
is relatively compact, and any weak limit is concentrated on
FSPs with probability 1.
Proof. Relative compactness holds as |ğ‘§ğ›½(ğ‘¡2) âˆ’ğ‘§ğ›½(ğ‘¡1)|
â‰¤
ğ¶âˆ—(ğ‘¡2 âˆ’ğ‘¡1) + ğœ–in probability, from Lipschitz bounds and Â¯ğœ‡,
ğ›¼max. Using Skorohod representation, converging subsequences
are almost surely FSPs, as scaled updates and Assumption 1
ensure consistency.
â–¡
Theorem 3. (Uniform Attraction of Fluid Sample Paths) For
any bounded set ğ´âŠ‚Rğ‘
+ , FSPs satisfy ğ‘¥(ğ‘¡) â†’ğ‘¥âˆ—as ğ‘¡â†’âˆ,
uniformly on ğ‘¥(0) âˆˆğ´.
Proof. The distance ğœŒ(ğ‘¥(ğ‘¡), X(ğ‘¡)) â‰¤ğœŒ(ğ‘¥(0), X(0))ğ‘’âˆ’ğ‘¡decays,
as ğ‘¥â€²(ğ‘¡) = ğ‘£(ğ‘¡) âˆ’ğ‘¥(ğ‘¡) with ğ‘£(ğ‘¡) âˆˆX(ğ‘¡). Therefore, if ğ‘¥ğµ(ğ‘¡) = 0,
the inequality ğ‘‘+
ğ‘‘ğ‘¡
Ã
ğ‘–âˆˆğµğ‘¥ğ‘–(ğ‘¡) â‰¥ğœ‡> 0 ensures ğ‘¥ğ‘–(ğ‘¡) > 0 after ğ‘‡1,
uniformly on ğ´. And outside ğ‘‚ğ›¿(ğ‘¥âˆ—), Ãğ‘
ğ‘–=1
1
ğ‘¥ğ‘–(ğ‘¡) (ğ‘¥âˆ—
ğ‘–âˆ’ğ‘¥ğ‘–(ğ‘¡)) > 0,


--- Page 10 ---
so
ğ‘‘
ğ‘‘ğ‘¡ğ‘ˆ(ğ‘¥(ğ‘¡))
> 0, driving ğ‘ˆ(ğ‘¥(ğ‘¡))
â†’ğ‘ˆ(ğ‘¥âˆ—). Uniform
convergence holds by finite ğ‘‡2.
â–¡
Theorem 4. (Asymptotic Optimality): For bounded ğ´âŠ‚Rğ‘
+
and ğœ–> 0, there exist ğ‘‡,ğ‘‡âˆ—> 0 such that
lim
ğ›½â†’0
sup
ğ‘‹ğ›½(0)âˆˆğ´,ğ‘™1>ğ‘‡/ğ›½,ğ‘™2âˆ’ğ‘™1>ğ‘‡âˆ—/ğ›½
Eğ‘ˆğ›½(ğ‘™1, ğ‘™2) âˆ’ğ‘¥âˆ— < ğœ–,
where ğ‘ˆğ›½(ğ‘™1, ğ‘™2) is the average realized goodput over [ğ‘™1, ğ‘™2].
Proof. Theorems 2 and 3 ensure ğ‘¥ğ›½(ğ‘¡) â†’ğ‘¥âˆ—. Assumption 3â€™s
ğœ‚
ğ›½â†’0 stabilizes X(ğ‘¡) â†’X, so interval averages converge
uniformly in expectation.
â–¡
Corollary 1: In the stationary regime, limğ›½â†’0 Eğ‘¥ğ›½(1) = ğ‘¥âˆ—.
References
[1] Y. Zhong, S. Liu, J. Chen, J. Hu, Y. Zhu, X. Liu, X. Jin, and H. Zhang,
â€œDistserve: disaggregating prefill and decoding for goodput-optimized
large language model serving,â€ in Proceedings of the 18th USENIX
Conference on Operating Systems Design and Implementation, ser.
OSDIâ€™24.
USA: USENIX Association, 2024.
[2] A. Agrawal, N. Kedia, A. Panwar, J. Mohan, N. Kwatra, B.S. Gulavani,
A. Tumanov, and R. Ramjee, â€œTaming throughput-latency tradeoff in
llm inference with sarathi-serve,â€ in Proceedings of the 18th USENIX
Conference on Operating Systems Design and Implementation, ser.
OSDIâ€™24.
USA: USENIX Association, 2024.
[3] Z. Wang, S. Li, Y. Zhou, X. Li, R. Gu, N. Cam-Tu, C. Tian, and S. Zhong,
â€œRevisiting slo and goodput metrics in llm serving,â€ 2024. [Online].
Available: https://arxiv.org/abs/2410.14257
[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez,
L. Kaiser, and I. Polosukhin, â€œAttention is all you need,â€ in Proceedings
of the 31st International Conference on Neural Information Processing
Systems, ser. NIPSâ€™17.
Red Hook, NY, USA: Curran Associates Inc.,
2017, p. 6000â€“6010.
[5] S. Yun, S. Park, H. Nam, Y. Lee, G. Lee, K. Kyung, S. Kim, N.S. Kim,
J. Kim, H. Kim, J. Cho, S. Baek, and J.H. Ahn, â€œThe new llm bottleneck:
A systems perspective on latent attention and mixture-of-experts,â€ 2025.
[Online]. Available: https://arxiv.org/abs/2507.15465
[6] Y. Leviathan, M. Kalman, and Y. Matias, â€œFast inference from transformers
via speculative decoding,â€ in Proceedings of the 40th International
Conference on Machine Learning, ser. ICMLâ€™23.
JMLR.org, 2023.
[7] H. Xia, Z. Yang, Q. Dong, P. Wang, Y. Li, T. Ge, T. Liu, W. Li,
and Z. Sui, â€œUnlocking efficiency in large language model inference:
A comprehensive survey of speculative decoding,â€ 2024. [Online].
Available: https://arxiv.org/abs/2401.07851
[8] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, Z. Zhang, R.Y.Y.
Wong, A. Zhu, L. Yang, X. Shi, C. Shi, Z. Chen, D. Arfeen, R. Abhyankar,
and Z. Jia, â€œSpecinfer: Accelerating large language model serving with
tree-based speculative inference and verification,â€ in Proceedings of
the 29th ACM International Conference on Architectural Support for
Programming Languages and Operating Systems (ASPLOS â€™24), ser.
ASPLOS â€™24.
New York, NY, USA: Association for Computing
Machinery, 2024, p. 932â€“949.
[9] Y. Fu, P. Bailis, I. Stoica, and H. Zhang, â€œBreak the sequential dependency
of llm inference using lookahead decoding,â€ in Proceedings of the
41st International Conference on Machine Learning, ser. ICMLâ€™24.
JMLR.org, 2024.
[10] J. Zhang, J. Wang, H. Li, L. Shou, K. Chen, G. Chen, and S. Mehrotra,
â€œDraft & verify: Lossless large language model acceleration via self-
speculative decoding,â€ in Proceedings of the 62nd Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers),
L.W. Ku, A. Martins, and V. Srikumar, Eds.
Bangkok, Thailand:
Association for Computational Linguistics, Aug. 2024, pp. 11 263â€“11 282.
[11] H. Xia, Y. Li, J. Zhang, C. Du, and W. Li, â€œSWIFT: On-the-fly self-
speculative decoding for LLM inference acceleration,â€ in The Thirteenth
International Conference on Learning Representations, 2025.
[12] G.I. Yu and J.S. Jeong, â€œOrca: A distributed serving system for
transformer-based generative models,â€ in USENIX Symposium on Op-
erating Systems Design and Implementation, 2022.
[13] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C.H. Yu, J.E. Gonzalez,
H. Zhang, and I. Stoica, â€œEfficient memory management for large language
model serving with pagedattention,â€ 2023.
[14] C. Holmes, M. Tanaka, M. Wyatt, A.A. Awan, J. Rasley, S. Rajbhandari,
R.Y. Aminabadi, H. Qin, A. Bakhtiari, L. Kurilenko, and Y. He,
â€œDeepspeed-fastgen: High-throughput text generation for llms via mii and
deepspeed-inference,â€ ArXiv, vol. abs/2401.08671, 2024.
[15] C. Hu, H. Huang, L. Xu, X. Chen, J. Xu, S. Chen, H. Feng, C. Wang,
S. Wang, Y. Bao, N. Sun, and Y. Shan, â€œInference without interference:
Disaggregate llm inference for mixed downstream workloads,â€ ArXiv, vol.
abs/2401.11181, 2024.
[16] P. Patel, E. Choukse, C. Zhang, A. Shah, I. Goiri, S. Maleki, and
R. Bianchini, â€œSplitwise: Efficient generative llm inference using phase
splitting,â€ in 2024 ACM/IEEE 51st Annual International Symposium on
Computer Architecture (ISCA), 2024, pp. 118â€“132.
[17] X. Zhao, B. Jia, H. Zhou, Z. Liu, S. Cheng, and Y. You, â€œHetegen: Efficient
heterogeneous parallel inference for large language models on resource-
constrained devices,â€ in MLSys, 2024.
[18] H. Oh, K. Kim, J. Kim, S. Kim, J. Lee, D.s. Chang, and J. Seo,
â€œExegpt: Constraint-aware resource scheduling for llm inference,â€ in
Proceedings of the 29th ACM International Conference on Architectural
Support for Programming Languages and Operating Systems, Volume
2, ser. ASPLOS â€™24.
New York, NY, USA: Association for Computing
Machinery, 2024, p. 369â€“384.
[19] Y. Mei, Y. Zhuang, X. Miao, J. Yang, Z. Jia, and R. Vinayak, â€œHelix:
Serving large language models over heterogeneous gpus and network via
max-flow,â€ in Proceedings of the 30th ACM International Conference
on Architectural Support for Programming Languages and Operating
Systems, Volume 1, ser. ASPLOS â€™25, 2025, p. 586â€“602.
[20] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, â€œOPTQ: Accurate
quantization for generative pre-trained transformers,â€ in The Eleventh
International Conference on Learning Representations, 2023.
[21] J. Lin, J. Tang, H. Tang, S. Yang, W.M. Chen, W.C. Wang, G. Xiao,
X. Dang, C. Gan, and S. Han, â€œAwq: Activation-aware weight quantization
for llm compression and acceleration,â€ in MLSys, 2024.
[22] T. Cai, Y. Li, Z. Geng, H. Peng, J.D. Lee, D. Chen, and T. Dao, â€œMedusa:
Simple llm inference acceleration framework with multiple decoding
heads,â€ in Proceedings of the 41st International Conference on Machine
Learning, ser. ICMLâ€™24.
JMLR.org, 2024.
[23] Y. Li, F. Wei, C. Zhang, and H. Zhang, â€œEagle: speculative sampling
requires rethinking feature uncertainty,â€ in Proceedings of the 41st Inter-
national Conference on Machine Learning, ser. ICMLâ€™24.
JMLR.org,
2024.
[24] H. Liu, M. Zaharia, and P. Abbeel, â€œRingattention with blockwise
transformers for near-infinite context,â€ in The Twelfth International
Conference on Learning Representations, 2024.
[25] I. Gim, G. Chen, S.S. Lee, N. Sarda, A. Khandelwal, and L. Zhong,
â€œPrompt cache: Modular attention reuse for low-latency inference,â€ in
MLSys, 2024.


--- Page 11 ---
[26] T. Griggs, X. Liu, J. Yu, D. Kim, W.L. Chiang, A. Cheung,
and
I.
Stoica,
â€œMÂ´elange:
Cost
efficient
large
language
model
serving by exploiting gpu heterogeneity,â€ 2024. [Online]. Available:
https://arxiv.org/abs/2404.14527
[27] Y. Fu, L. Xue, Y. Huang, A.O. Brabete, D. Ustiugov, Y. Patel, and L. Mai,
â€œServerlessLLM: Low-Latency serverless inference for large language
models,â€ in 18th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 24), 2024.
[28] X. Li, D. Spatharakis, S. Ghafouri, J. Fan, H. Vandierendonck, D. John,
B. Ji, and D. Nikolopoulos, â€œSLED: A Speculative LLM Decoding
Framework for Efficient Edge Serving,â€ 2025. [Online]. Available:
http://arxiv.org/abs/2506.09397
[29] F. Chen, P. Li, T.H. Luan, Z. Su, and J. Deng, â€œSpin: Accelerating large
language model inference with heterogeneous speculative models,â€ in
IEEE INFOCOM, 2025, pp. 1â€“10.
[30] J. NING, C. ZHENG, and T. Yang, â€œDSSD: Efficient edge-device
deployment and collaborative inference via distributed split speculative
decoding,â€ in ICML 2025 Workshop on Machine Learning for Wireless
Communication and Networks (ML4Wireless), 2025.
[31] A.L. Stolyar, â€œOn the asymptotic optimality of the gradient scheduling
algorithm for multiuser throughput allocation,â€ Oper. Res., vol. 53, no. 1,
p. 12â€“25, Jan. 2005.
[32] Z. Li, Z. Chen, R. Delacourt, G. Oliaro, Z. Wang, Q. Chen, S. Lin, A. Yang,
Z. Zhang, Z. Chen, S. Lai, X. Cheng, X. Miao, and Z. Jia, â€œAdaServe:
Accelerating Multi-SLO LLM Serving with SLO-Customized Speculative
Decoding,â€ May 2025, arXiv:2501.12162 [cs].
[33] Y. Leviathan, M. Kalman, and Y. Matias, â€œFast inference from transformers
via speculative decoding,â€ in International Conference on Machine
Learning.
PMLR, 2023, pp. 19 274â€“19 286.
[34] H. Kushner and G. Yin, Stochastic Approximation Algorithms and
Recursive Algorithms and Applications, 01 2003.
