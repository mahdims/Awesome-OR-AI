--- Page 1 ---
B4: Towards Optimal Assessment of Plausible Code Solutions
with Plausible Tests
Mouxiang Chen
The State Key Laboratory of
Blockchain and Data Security,
Zhejiang University
Hangzhou, China
chenmx@zju.edu.cn
Zhongxin Liuâˆ—â€ 
The State Key Laboratory of
Blockchain and Data Security,
Zhejiang University
Hangzhou, China
liu_zx@zju.edu.cn
He Tao
Zhejiang University
Hangzhou, China
tao_he@zju.edu.cn
Yusu Hong
Zhejiang University
Hangzhou, China
yusuhong@zju.edu.cn
David Lo
Singapore Management University
Singapore, Singapore
davidlo@smu.edu.sg
Xin Xia
Zhejiang University
Hangzhou, China
xin.xia@acm.org
Jianling Sun
The State Key Laboratory of
Blockchain and Data Security,
Zhejiang University
Hangzhou, China
sunjl@zju.edu.cn
ABSTRACT
Selecting the best code solution from multiple generated ones
is an essential task in code generation, which can be achieved by
using some reliable validators (e.g., developer-written test cases) for
assistance. Since reliable test cases are not always available and can
be expensive to build in practice, researchers propose to automati-
cally generate test cases to assess code solutions. However, when
both code solutions and test cases are plausible and not reliable,
selecting the best solution becomes challenging. Although some
heuristic strategies have been proposed to tackle this problem, they
lack a strong theoretical guarantee and it is still an open question
whether an optimal selection strategy exists. Our work contributes
in two ways. First, we show that within a Bayesian framework, the
optimal selection strategy can be defined based on the posterior
probability of the observed passing states between solutions and
tests. The problem of identifying the best solution is then framed as
an integer programming problem. Second, we propose an efficient
approach for approximating this optimal (yet uncomputable) strat-
egy, where the approximation error is bounded by the correctness
of prior knowledge. We then incorporate effective prior knowledge
âˆ—Corresponding Author
â€ Also with Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data
Security
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ASE â€™24, October 27-November 1, 2024, Sacramento, CA, USA
Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-1248-7/24/10...$15.00
https://doi.org/10.1145/3691620.3695536
to tailor code generation tasks. Both theoretical and empirical stud-
ies confirm that existing heuristics are limited in selecting the best
solutions with plausible test cases. Our proposed approximated
optimal strategy B4 significantly surpasses existing heuristics in se-
lecting code solutions generated by large language models (LLMs)
with LLM-generated tests, achieving a relative performance im-
provement by up to 50% over the strongest heuristic and 246% over
the random selection in the most challenging scenarios. Our code
is publicly available at https://github.com/ZJU-CTAG/B4.
CCS CONCEPTS
â€¢ Computing methodologies â†’Artificial intelligence; â€¢ Soft-
ware and its engineering â†’Software design engineering.
KEYWORDS
Code Generation, Software Engineering, Large Language Models
ACM Reference Format:
Mouxiang Chen, Zhongxin Liu, He Tao, Yusu Hong, David Lo, Xin Xia,
and Jianling Sun. 2024. B4: Towards Optimal Assessment of Plausible Code
Solutions with Plausible Tests. In 39th IEEE/ACM International Conference
on Automated Software Engineering (ASE â€™24), October 27-November 1, 2024,
Sacramento, CA, USA. ACM, New York, NY, USA, 13 pages. https://doi.org/
10.1145/3691620.3695536
1
INTRODUCTION
Code generation is an important task in the field of software
engineering [23], aiming to generate code solutions that satisfy
the given requirement. In practice, we often face the problem of
selecting the best code solution from multiple generated alterna-
tives [7, 22]. A common practice is using some validators (e.g., test
cases) to assess the validity of each solution and choose the best
one [5, 33, 36, 46]. However, in real-world scenarios, reliable test
arXiv:2409.08692v1  [cs.SE]  13 Sep 2024


--- Page 2 ---
ASE â€™24, October 27-November 1, 2024, Sacramento, CA, USA
Mouxiang Chen, Zhongxin Liu, He Tao, Yusu Hong, David Lo, Xin Xia, and Jianling Sun
return a + b
return b + a
return a * b
return a * 2
assert add_two_numbers(4, 5) == 9
assert add_two_numbers(2, 2) == 4
assert add_two_numbers(1, 1) == 2
assert add_two_numbers(3, 2) == 6
assert add_two_numbers(3, 1) == 6
Code Solutions
Test Cases
Figure 1: A simple example showing the problem "return the
sum of ğ‘andğ‘". A link between a generated code solution and
a generated test case indicates that the solution passes the
test. How can we select the best code solution solely based
on these links?
cases are not always available. Developing and maintaining reli-
able test cases can also be resource-intensive and laborious. With
advancements in deep learning and large language models (LLMs),
using auto-generated test cases has gained popularity among re-
searchers and practitioners [20, 26, 27, 35]. Unfortunately, selecting
code solutions based on these potentially unreliable tests poses
significant challenges, since incorrect test cases can disrupt our
decision-making. Fig. 1 provides an example, where selecting the
best code solution becomes difficult since the fourth and fifth test
cases are incorrect.
Few studies systematically explore how to assess plausible code
solutions and select the best using plausible test cases. Under the
assumption that the generated test cases are (mostly) correct, some
existing research favors the solutions that pass the most test cases
[18, 19, 22, 33]. However, this strategy is ineffective when test cases
are merely plausible, indicated by our theoretical analysis (see
Section 4). Other research addresses this challenge by designing
clustering-based heuristic rules. For instance, Shi et al. [36] and Li
et al. [22] clustered code solutions based on test outputs, and se-
lected the solutions from the largest cluster. Chen et al. [5] similarly
clustered code solutions based on the passed test cases, and selected
the best cluster according to the count of solutions and passed test
cases in each. However, these heuristics rely on human-designed
rules and lack strong theoretical foundations, leading to potentially
suboptimal performance. To the best of our knowledge, the optimal
selection strategy for this problem is still an open question.
In this work, we aim to develop a general framework to define
and compute the optimal selection strategy. We first show that
under a Bayesian framework, the optimal strategy can be defined
based on the posterior probability of the observed passing states
between solutions and tests. The problem of identifying the optimal
strategy is then framed as an integer programming problem. Under
a few assumptions, this posterior probability can be further ex-
panded into four integrals, which cannot be directly computed due
to four unknown prior distributions. We then leverage Bayesian sta-
tistics techniques to deduce a computable form for approximating
this posterior probability and optimize the integer programming
from exponential to polynomial complexity. The approximation
error is bounded by the correctness of prior knowledge. Based on
this bound, we investigate two effective priors and incorporate
them into our framework to enhance code generation performance.
Given that the approximated optimal strategy involves scoring code
solutions with four Beta functions [10], we refer to it as B4.
Based on our developed framework, we further provide a theo-
retical analysis to compare B4 with existing heuristics. We observe
that some heuristics require sufficient correct test cases, while oth-
ers necessitate a higher probability of correct code solutions, as
confirmed by subsequent simulated experiments. In real-world ap-
plications involving selecting LLM-generated code solutions with
LLM-generated test cases, B4 significantly outperforms existing
heuristics across five LLMs and three benchmarks.
In summary, our paper makes the following contributions:
â€¢ Optimal Strategy. We systematically address the challenging
problem of selecting plausible code solutions with plausible tests
and establish an optimal yet uncomputable strategy.
â€¢ Technique. We derive an efficiently computable approach to
approximate the uncomputable optimal strategy with an error
bound. While our framework is broadly applicable, we adapt it
to code generation by incorporating two effective priors.
â€¢ Theoretical Study. Using our framework, we explore the condi-
tions under which existing heuristics are effective or ineffective
and compare them to the approximated optimal strategy B4.
â€¢ Empirical Study. We empirically evaluate our selection strategy
with five code LLMs on three benchmarks. Experimental results
show that our strategy demonstrates up to a 12% average relative
improvement over the strongest heuristic and a 50% improvement
in the most challenging situations where there are few correct
solutions.
2
PRELIMINARIES
Notations. We use bold lowercase letters to denote vectors (e.g.,
x and y), bold uppercase letters to denote matrices (e.g., E), and thin
letters to denote scalars (e.g., ğ‘¥and ğ‘¦). We also use thin uppercase
letters to denote random variables (e.g., ğ‘‹, ğ‘Œ, and ğ¸). ğ’†ğ‘–denotes
the ğ‘–-th row in matrix E. The index set [ğ‘] denotes {1, 2, Â· Â· Â· , ğ‘}.
{0, 1}ğ‘denotes a length-ğ‘binary vector, and {0, 1}ğ‘Ã—ğ‘€denotes
an ğ‘Ã— ğ‘€binary matrix.
2.1
Problem Definition
Code generation is a crucial task in software engineering, which
aims at automatically generating a code solution ğ‘¥from a given
context ğ‘. We explore the selection of the best code solution from
ğ‘code solutions generated based on ğ‘, with ğ‘€test cases (also
generated based onğ‘) to aid this selection. It is worth noting that the
correctness of both code solutions and test cases is plausible; they
might be either correct or incorrect, which is unobserved however.
All we can observe is a matrix E = {ğ‘’ğ‘–ğ‘—}ğ‘Ã—ğ‘€âˆˆ{0, 1}ğ‘Ã—ğ‘€where
ğ‘’ğ‘–ğ‘—= 1 indicates the ğ‘–-th code solution passes the ğ‘—-th test case,
and 0 indicates failure. We term E as passing matrix, and ğ‘’ğ‘–ğ‘—as
passing state.
Let x = {ğ‘¥1, Â· Â· Â· ,ğ‘¥ğ‘} âˆˆ{0, 1}ğ‘denote the ground-truth correct-
ness of each code solution (unknown to us), in which 1 denotes
correct and 0 denotes incorrect. We assume at least one code solution
is correct since designing a selection strategy would be meaningless
without any correct code. Similarly, the correctness of each test
case is denoted by y = {ğ‘¦1, ...,ğ‘¦ğ‘€} âˆˆ{0, 1}ğ‘€. We assume that all
correct code solutions share identical functionality and all tests


--- Page 3 ---
B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests
ASE â€™24, October 27-November 1, 2024, Sacramento, CA, USA
are not flaky, meaning that all solutions pass the same test cases
on the same context ğ‘. This can be formulated as the following
assumption.
Assumption 1 (Consistency). For all ğ‘–, ğ‘—âˆˆ[ğ‘], if ğ‘¥ğ‘–= 1 and
ğ‘¥ğ‘—= 1, then E should satisfy:
ğ’†ğ‘–= ğ’†ğ‘—
(i.e.,ğ‘’ğ‘–ğ‘˜= ğ‘’ğ‘—ğ‘˜,
âˆ€ğ‘˜âˆˆ[ğ‘€]).
Furthermore, the correctness of test cases y corresponds to the passing
states of the correct code solutions. Formally, if ğ‘¥ğ‘–= 1,ğ‘–âˆˆ[ğ‘], then:
y = ğ’†ğ‘–
(i.e.,ğ‘¦ğ‘˜= ğ‘’ğ‘–ğ‘˜,
âˆ€ğ‘˜âˆˆ[ğ‘€]).
This assumption indicates that E and y should be consistent with
x. Intuitively, E should satisfy that the rows corresponding to the
correct code solutions are the same. y is defined based on these rows.
For example, in Fig. 1, we have x = {1, 1, 0, 0}, y = {1, 1, 1, 0, 0}, and
E =
Â©Â­Â­Â­
Â«
1
1
1
0
0
1
1
1
0
0
0
1
1
1
1
0
0
1
1
0
ÂªÂ®Â®Â®
Â¬
.
(1)
In this paper, our goal is to use E to assess the correctness of
code solutions and select the best one by recovering x and y from
E. Following Chen et al. [5], we do not rely on any specific details
of the code solutions or test cases in this paper.
2.2
Existing Heuristics
In this section, we briefly review two representative heuristic
methods for addressing this problem. The first family of methods
MaxPass [18, 19, 22, 33] always rewards passing test cases. The
best code solution can be selected by counting the passed cases, i.e.,
Select code solution ğ‘–, where ğ‘–= arg max
ğ‘–âˆˆ[ğ‘]
ğ‘€
âˆ‘ï¸
ğ‘—=1
ğ‘’ğ‘–ğ‘—.
The other family of methods examines the consensus between
code solutions and test cases, and clusters the code solutions with
the same functionality [5, 22, 36]. One of the most representative
methods is CodeT [5]. It divides the code solutions into ğ¾disjoint
subsets based on functionality: ğ‘†ğ‘¥= {ğ‘†ğ‘¥
1 , Â· Â· Â· ,ğ‘†ğ‘¥
ğ¾}, where each set
ğ‘†ğ‘¥
ğ‘–(ğ‘–âˆˆ[ğ¾]) consists of code solutions that pass the same set of
test cases, denoted by ğ‘†ğ‘¦
ğ‘–. The tuple (ğ‘†ğ‘¥
ğ‘–, ğ‘†ğ‘¦
ğ‘–) is termed a consensus
set. Taking Fig. 1 as an example, there are three consensus sets:
({ğ‘¥1,ğ‘¥2}, {ğ‘¦1,ğ‘¦2,ğ‘¦3}), ({ğ‘¥3}, {ğ‘¦2,ğ‘¦3,ğ‘¦4,ğ‘¦5}) and ({ğ‘¥4}, {ğ‘¦3,ğ‘¦4}).
CodeT proposes that a consensus set containing more code
solutions and test cases indicates a higher level of consensus, and
thus the more likely they are correct. Therefore, CodeT scores each
consensus set based on the capacity and selects the code solutions
associated with the highest-scoring set, i.e.,
Select code solutions ğ‘–âˆˆğ‘†ğ‘˜, where ğ‘˜= arg max
ğ‘˜âˆˆ[ğ¾]
|ğ‘†ğ‘¥
ğ‘˜| Â· |ğ‘†ğ‘¦
ğ‘˜|.
Similarly, other clustering methods, such as MBR-exec [36] and
AlphaCode-C [22], also cluster the code solutions based on test
cases, but only score each set by the number of code solutions |ğ‘†ğ‘¥
ğ‘˜|.
We focus our analysis on CodeT as it was verified to outperform
other existing scoring strategies [5].
In this study, we develop a systematic analysis framework, to
evaluate the effectiveness of these heuristics and address the fol-
lowing research questions (RQs):
â€¢ RQ1: Given a passing matrix E, what constitutes the optimal selec-
tion strategy?
â€¢ RQ2: Is this optimal strategy computable?
â€¢ RQ3: Can a practical algorithm be developed to compute (or ap-
proximate) this optimal strategy efficiently?
â€¢ RQ4: Under what conditions do existing heuristics not work, based
on our developed analysis framework?
â€¢ RQ5: If the answer to RQ3 is true, how does the computable (or
approximated) optimal strategy compare to these heuristics?
3
METHODOLOGY
In this section, we outline our proposed methodology to address
this problem.
3.1
Optimal Strategy
We use ğ‘‹= {ğ‘‹1, Â· Â· Â· ,ğ‘‹ğ‘} âˆˆ{0, 1}ğ‘, ğ‘Œ= {ğ‘Œ1, Â· Â· Â· ,ğ‘Œğ‘€} âˆˆ
{0, 1}ğ‘€, and ğ¸= {ğ¸ğ‘–ğ‘—}ğ‘Ã—ğ‘€âˆˆ{0, 1}ğ‘Ã—ğ‘€to denote random vari-
ables of code solutionsâ€™ and testsâ€™ correctness, and the passing ma-
trix, respectively. Note that all ğ‘‹, ğ‘Œ, and ğ¸depend on the same con-
text ğ¶, which we omit for ease of notation. A strategyâ€™s estimation
for ğ‘‹and ğ‘Œis denoted by Ë†x = { Ë†ğ‘¥1, Â· Â· Â· , Ë†ğ‘¥ğ‘} and Ë†y = { Ë†ğ‘¦1, Â· Â· Â· , Ë†ğ‘¦ğ‘€}.
To answer RQ1, our goal is to find the most probable Ë†x and Ë†y given an
observation ğ¸= E. This motivates us to design the optimal strategy
by modeling ğ‘ƒ(ğ‘‹,ğ‘Œ| ğ¸). Based on Bayesâ€™ theorem, we have:
ğ‘ƒ(ğ‘‹,ğ‘Œ| ğ¸)
|       {z       }
posterior
= ğ‘ƒ(ğ¸| ğ‘‹,ğ‘Œ)
ğ‘ƒ(ğ¸)
ğ‘ƒ(ğ‘‹,ğ‘Œ) âˆğ‘ƒ(ğ¸| ğ‘‹,ğ‘Œ)
|       {z       }
likelihood
ğ‘ƒ(ğ‘‹,ğ‘Œ)
|  {z  }
prior
.
Therefore, we propose to use maximum a posteriori (MAP)
estimator to obtain the best solution [11]:
Ë†xâˆ—, Ë†yâˆ—=
arg max
Ë†xâˆˆ{0,1}ğ‘,Ë†yâˆˆ{0,1}ğ‘€
ğ‘ƒ(ğ¸= E | ğ‘‹= Ë†x,ğ‘Œ= Ë†y)
|                          {z                          }
likelihood
ğ‘ƒ(ğ‘‹= Ë†x,ğ‘Œ= Ë†y)
|               {z               }
prior
. (2)
That is to say, we exhaustively explore all 2ğ‘possible configu-
rations of Ë†x and 2ğ‘€configurations of Ë†y, computing the likelihood
and prior for each pair. We then find the Ë†xâˆ—and Ë†yâˆ—that yield the
highest posterior and select the correct code solutions and test cases
indicated by Ë†xâˆ—and Ë†yâˆ—. This optimization problem is a 0/1 integer
programming problem, in which all variables are restricted to 0 or
1. The following then answers RQ1.
Answer to RQ1: Given a passing matrix E, the optimal
selection strategy can be framed as a 0/1 integer programming
problem, by finding the one Ë†x âˆˆ{0, 1}ğ‘and Ë†y âˆˆ{0, 1}ğ‘€that
maximizes the posterior probability ğ‘ƒ(ğ‘‹= Ë†x,ğ‘Œ= Ë†y | ğ¸= E).
Before calculating Eq.(2), we first introduce the following two
assumptions which are necessary for our subsequent computation.
Assumption 2. The code solutions ğ‘‹and the test cases ğ‘Œare
independent and randomly sampled.
Assumption 3. Each ğ¸ğ‘–ğ‘—is only dependent by the ğ‘‹ğ‘–and ğ‘Œğ‘—,
âˆ€ğ‘–âˆˆ[ğ‘], ğ‘—âˆˆ[ğ‘€].


--- Page 4 ---
ASE â€™24, October 27-November 1, 2024, Sacramento, CA, USA
Mouxiang Chen, Zhongxin Liu, He Tao, Yusu Hong, David Lo, Xin Xia, and Jianling Sun
Correct code
Incorrect code
Correct test case
Incorrect test case
Figure 2: Illustration of the generation process. The correct-
ness of code ğ‘‹ğ‘–and test case ğ‘Œğ‘—is sampled using parameters
ğœƒğ‘¥and ğœƒğ‘¦respectively. ğ¸ğ‘–ğ‘—is generated based on ğ‘‹ğ‘–and ğ‘Œğ‘—,
using the corresponding parameters (1, 0, ğœƒ1 or ğœƒ0).
Remark 1. Assumption 2 is also used by Chen et al. [5]. Assump-
tion 3 assumes that a passing state ğ¸ğ‘–ğ‘—is independent of any other
variables except for the corresponding code ğ‘‹ğ‘–and test case ğ‘Œğ‘—, which
means that ğ¸ğ‘–ğ‘—(ğ‘–âˆˆ[ğ‘], ğ‘—âˆˆ[ğ‘€]) are conditional independent when
given ğ‘‹and ğ‘Œ. We will further discuss these assumptions in Section 6.
Based on Assumption 3, we can explicitly formulate ğ‘ƒ(ğ¸ğ‘–ğ‘—|
ğ‘‹ğ‘–,ğ‘Œğ‘—) as follows,
ğ‘ƒ(ğ¸ğ‘–ğ‘—= 1 | ğ‘‹ğ‘–= 1,ğ‘Œğ‘—= 1) = 1,
ğ‘ƒ(ğ¸ğ‘–ğ‘—= 1 | ğ‘‹ğ‘–= 1,ğ‘Œğ‘—= 0) = 0,
ğ‘ƒ(ğ¸ğ‘–ğ‘—= 1 | ğ‘‹ğ‘–= 0,ğ‘Œğ‘—= 1) = ğœƒ1,
ğ‘ƒ(ğ¸ğ‘–ğ‘—= 1 | ğ‘‹ğ‘–= 0,ğ‘Œğ‘—= 0) = ğœƒ0, (3)
where ğœƒ1 and ğœƒ0 are unknown parameters, indicating the probabil-
ities of an incorrect solution passing a correct test case (ğœƒ1) and
passing an incorrect test case (ğœƒ0). Eq.(3) suggests that if a solution
is correct (ğ‘‹ğ‘–= 1), ğ¸ğ‘–ğ‘—is deterministic by ğ‘Œğ‘—to fulfill the consis-
tency (Assumption 1). When a solution is incorrect (ğ‘‹ğ‘–= 0), ğ¸ğ‘–ğ‘—is
a Bernoulli random variable, i.e., a random variable that can only
take 0 or 1, where the probability depends on ğ‘Œğ‘—.
Based on Assumption 2, the correctness of code solutions ğ‘‹
and test cases ğ‘Œare independent and therefore follow Bernoulli
distributions as well. Suppose that:
ğ‘ƒ(ğ‘‹ğ‘–= 1) = ğœƒğ‘¥,
ğ‘ƒ(ğ‘Œğ‘—= 1) = ğœƒğ‘¦,
âˆ€ğ‘–âˆˆ[ğ‘], ğ‘—âˆˆ[ğ‘€],
where ğœƒğ‘¥and ğœƒğ‘¦are two unknown parameters. To summarize,
Fig. 2 illustrates the generation process of ğ¸based on four unknown
parameters ğœƒ1, ğœƒ0, ğœƒğ‘¥and ğœƒğ‘¦for a clear presentation.
For ease of notation, we omit the random variables in the prob-
ability expressions in subsequent sections, e.g., using ğ‘ƒ(Ë†x, Ë†y) to
replace ğ‘ƒ(ğ‘‹= Ë†x,ğ‘Œ= Ë†y). In the following sections, we provide a
detailed explanation of how to derive the likelihood and prior in
Eq.(2) based on the generation process proposed in Fig. 2.
Computing the likelihood. Based on Assumption 3 and Remark 1,
we can expand the likelihood ğ‘ƒ(E | Ë†x, Ë†y) into the following form:
ğ‘ƒ(E | Ë†x, Ë†y) =
Ã–
ğ‘–
Ã–
ğ‘—
ğ‘ƒ(ğ‘’ğ‘–ğ‘—| Ë†x, Ë†y)
=
Ã–
Ë†ğ‘¥ğ‘–=1
Ã–
ğ‘—
ğ‘ƒ(ğ‘’ğ‘–ğ‘—| Ë†x, Ë†y)
|                    {z                    }
ğ‘ƒ1
Ã–
Ë†ğ‘¥ğ‘–=0
Ã–
ğ‘—
ğ‘ƒ(ğ‘’ğ‘–ğ‘—| Ë†x, Ë†y)
|                    {z                    }
ğ‘ƒ0
,
(4)
where ğ‘–âˆˆ[ğ‘] and ğ‘—âˆˆ[ğ‘€]. The first equality is based on the
independence of ğ¸ğ‘–ğ‘—. The second equality splits ğ‘’ğ‘–ğ‘—into two parts,
i.e., ğ‘ƒ1 and ğ‘ƒ0, based on Ë†ğ‘¥ğ‘–.
According to Eq.(3), ğ‘ƒ1 is either 1 or 0. If Ë†y and E are consistent
with Ë†x (i.e., satisfy Assumption 1), then ğ‘ƒ1 is 1; otherwise ğ‘ƒ1 is
0. Here we only focus on consistent configurations that satisfy
Assumption 1. Under this condition, ğ‘ƒ(E | Ë†x, Ë†y) = ğ‘ƒ0, so we only
need to compute ğ‘ƒ0. Suppose:
E1 = {ğ‘’ğ‘–ğ‘—| Ë†ğ‘¥ğ‘–= 0, Ë†ğ‘¦ğ‘—= 1,ğ‘–âˆˆ[ğ‘], ğ‘—âˆˆ[ğ‘€]},
E0 = {ğ‘’ğ‘–ğ‘—| Ë†ğ‘¥ğ‘–= 0, Ë†ğ‘¦ğ‘—= 0,ğ‘–âˆˆ[ğ‘], ğ‘—âˆˆ[ğ‘€]}.
(5)
Based on Eq.(3), E1 (or E0) contains a set of independent Bernoulli
variables related to ğœƒ1 (or ğœƒ0). Therefore:
ğ‘ƒ0 =
Ã–
Ë†ğ‘¥ğ‘–=0
Ã–
Ë†ğ‘¦ğ‘—=1
ğ‘ƒ(ğ‘’ğ‘–ğ‘—| Ë†x, Ë†y) Â·
Ã–
Ë†ğ‘¥ğ‘–=0
Ã–
Ë†ğ‘¦ğ‘—=0
ğ‘ƒ(ğ‘’ğ‘–ğ‘—| Ë†x, Ë†y)
= ğ‘ƒ(E1 | Ë†x, Ë†y) Â· ğ‘ƒ(E0 | Ë†x, Ë†y)
=
âˆ«1
0
ğ‘ƒ(E1 | ğœƒ1)ğ‘ƒ(ğœƒ1)dğœƒ1
âˆ«1
0
ğ‘ƒ(E0 | ğœƒ0)ğ‘ƒ(ğœƒ0)dğœƒ0
=
âˆ«1
0
ğœƒğ‘›1
1 (1 âˆ’ğœƒ1)|E1|âˆ’ğ‘›1ğ‘ƒ(ğœƒ1)dğœƒ1
âˆ«1
0
ğœƒğ‘›0
0 (1 âˆ’ğœƒ0)|E0|âˆ’ğ‘›0ğ‘ƒ(ğœƒ0)dğœƒ0,
(6)
where the third equality uses the fact that E1 only depends on ğœƒ1
and E0 only depends on ğœƒ0, which follows Bernoulli distributions
based on Eq.(3). We leverage the law of total probability, where
ğ‘ƒ(ğœƒ1) and ğ‘ƒ(ğœƒ0) are prior distributions for the two unknown param-
eters. The fourth equality leverages the formulation of the Bernoulli
distribution, where ğ‘›1 = Ã
ğ‘’ğ‘–ğ‘—âˆˆE1 ğ‘’ğ‘–ğ‘—and ğ‘›0 = Ã
ğ‘’ğ‘–ğ‘—âˆˆE0 ğ‘’ğ‘–ğ‘—are the
element sums of E1 and E0 respectively.
Computing the prior. To compute the prior ğ‘ƒ(Ë†x, Ë†y), following the
similar derivation as above, we have:
ğ‘ƒ(Ë†x, Ë†y) = ğ‘ƒ(Ë†x)ğ‘ƒ(Ë†y)
=
âˆ«1
0
ğ‘ƒ(Ë†x | ğœƒğ‘¥)ğ‘ƒ(ğœƒğ‘¥)dğœƒğ‘¥
âˆ«1
0
ğ‘ƒ(Ë†y | ğœƒğ‘¦)ğ‘ƒ(ğœƒğ‘¦)dğœƒğ‘¦
=
âˆ«1
0
ğœƒğ‘›ğ‘¥
ğ‘¥(1 âˆ’ğœƒğ‘¥)ğ‘âˆ’ğ‘›ğ‘¥ğ‘ƒ(ğœƒğ‘¥)dğœƒğ‘¥
âˆ«1
0
ğœƒ
ğ‘›ğ‘¦
ğ‘¦(1 âˆ’ğœƒğ‘¦)ğ‘€âˆ’ğ‘›ğ‘¦ğ‘ƒ(ğœƒğ‘¦)dğœƒğ‘¦,
(7)
where ğ‘ƒ(ğœƒğ‘¥) and ğ‘ƒ(ğœƒğ‘¦) are prior distributions. ğ‘›ğ‘¥= Ã
Ë†ğ‘¥ğ‘–âˆˆË†x Ë†ğ‘¥ğ‘–and
ğ‘›ğ‘¦= Ã
Ë†ğ‘¦ğ‘—âˆˆË†y Ë†ğ‘¦ğ‘—are the element sums of Ë†x and Ë†y, respectively.
Answer to RQ2: Under Assumptions 2 and 3, the posterior
of the optimal strategy can be expanded into four integrals
(Eq.(6) and Eq.(7)) related to some observed events (ğ‘›1, ğ‘›0, ğ‘›ğ‘¥,
andğ‘›ğ‘¦) and prior distributions on four unobserved parameters
(ğœƒ1, ğœƒ0, ğœƒğ‘¥, and ğœƒğ‘¦), which is not computable.
3.2
Practical Implementation
Recall that to compute the optimal strategy, we need to compute
likelihood (Eq.(6)) and prior (Eq.(7)), which is not computable how-
ever due to complicated integrals and unknown prior distributions.
In this section, we describe how to design an efficient approach to
approximate the optimal strategy.
Computing integrals. In Bayesian statistics, employing conju-
gate distributions for prior distributions is a standard technique to
simplify integrals in posterior computation [31]. In our case, all
the variables ğ‘‹, ğ‘Œ, and ğ¸follow the Bernoulli distributions, whose
conjugate prior is the Beta distribution [4]. Thus, we assume the
four parameters follow Beta distributions, formally,
ğ‘ƒ(ğœƒ0) âˆğœƒğ›¼0âˆ’1
0
(1 âˆ’ğœƒ0)ğ›½0âˆ’1,
ğ‘ƒ(ğœƒ1) âˆğœƒğ›¼1âˆ’1
1
(1 âˆ’ğœƒ1)ğ›½1âˆ’1,
ğ‘ƒ(ğœƒğ‘¥) âˆğœƒğ›¼ğ‘¥âˆ’1
ğ‘¥
(1 âˆ’ğœƒğ‘¥)ğ›½ğ‘¥âˆ’1,
ğ‘ƒ(ğœƒğ‘¦) âˆğœƒğ›¼ğ‘¦âˆ’1
ğ‘¦
(1 âˆ’ğœƒğ‘¦)ğ›½ğ‘¦âˆ’1,
(8)


--- Page 5 ---
B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests
ASE â€™24, October 27-November 1, 2024, Sacramento, CA, USA
where ğ›¼and ğ›½are eight hyperparameters that reflect our existing
belief or prior knowledge. We ignore all probability normalizing
constants for ease of notation since they will not change the selec-
tion decision. These hyperparameters allow us to integrate some
effective prior knowledge, which will be elaborated in Section 3.3.
To illustrate how Beta distributions simplify computation, we
take ğœƒğ‘¥as an example. Combining the integral about ğœƒğ‘¥in Eq.(7)
with ğ‘ƒ(ğœƒğ‘¥) in Eq.(8), we obtain:
âˆ«1
0
ğœƒğ‘›ğ‘¥
ğ‘¥(1 âˆ’ğœƒğ‘¥)ğ‘âˆ’ğ‘›ğ‘¥ğ‘ƒ(ğœƒğ‘¥)dğœƒğ‘¥
âˆ
âˆ«1
0
ğœƒğ‘›ğ‘¥
ğ‘¥(1 âˆ’ğœƒğ‘¥)ğ‘âˆ’ğ‘›ğ‘¥ğœƒğ›¼ğ‘¥âˆ’1
ğ‘¥
(1 âˆ’ğœƒğ‘¥)ğ›½ğ‘¥âˆ’1dğœƒğ‘¥
=
âˆ«1
0
ğœƒğ‘›ğ‘¥+ğ›¼ğ‘¥âˆ’1
ğ‘¥
(1 âˆ’ğœƒğ‘¥)ğ‘âˆ’ğ‘›ğ‘¥+ğ›½ğ‘¥âˆ’1dğœƒğ‘¥
= B (ğ‘›ğ‘¥+ ğ›¼ğ‘¥, ğ‘âˆ’ğ‘›ğ‘¥+ ğ›½ğ‘¥) ,
where B (Â·) is known as the Beta function [10], which can be effi-
ciently computed by modern scientific libraries like SciPy [41]. This
deduction is applicable to ğœƒ1, ğœƒ0, and ğœƒğ‘¦as well. Combining Eq.(2),
Eq.(4), Eq.(6), and Eq.(7), and applying the similar transformation
to integrals yields the formula for the computable posterior:
ğ‘ƒ(E | Ë†x, Ë†y)ğ‘ƒ(Ë†x, Ë†y) = ğ‘ƒ1 Â· ğ‘ƒ0 Â· ğ‘ƒ(Ë†x, Ë†y)
âˆğ‘ƒ1Â· [B (ğ‘›1 + ğ›¼1, |E1| âˆ’ğ‘›1 + ğ›½1) B (ğ‘›0 + ğ›¼0, |E0| âˆ’ğ‘›0 + ğ›½0)]
Â·

B (ğ‘›ğ‘¥+ ğ›¼ğ‘¥, ğ‘âˆ’ğ‘›ğ‘¥+ ğ›½ğ‘¥) B  ğ‘›ğ‘¦+ ğ›¼ğ‘¦, ğ‘€âˆ’ğ‘›ğ‘¦+ ğ›½ğ‘¦

(9)
This formula implies that the posterior probability can be approx-
imated by multiplying four Beta functions, multiplied by a term
ğ‘ƒ1 indicating whether Ë†x, Ë†y, and E are consistent. We next present
an error bound for this approximation (Proof can be found in the
online Appendix [6]).
Theorem 1 (Approximation error bound). Let Î” denote the
absolute error between the true posterior (i.e., ğ‘ƒ(Ë†x, Ë†y | E)) and the es-
timated posterior probability (i.e., multiplying the four Beta functions
with the probability normalizing constants in Eq.(8)). Then:
Î” â‰¤
2
ğ‘ƒ(E)

ğ‘1Î”ğœƒ1 + ğ‘0Î”ğœƒ0 + ğ‘ğ‘¥Î”ğœƒğ‘¥+ ğ‘ğ‘¦Î”ğœƒğ‘¦

,
where Î”ğœƒ1 is the total variance distance [38] between ğ‘ƒ(ğœƒ1) and our
assumed Beta prior distribution for ğœƒ1. Î”ğœƒ0, Î”ğœƒğ‘¥, and Î”ğœƒğ‘¦are defined
similarly. ğ‘1, ğ‘0, ğ‘ğ‘¥, and ğ‘ğ‘¦are some positive constants less than 1.
Theorem 1 shows that the difference of scores given by the
approximated approach and the optimal strategy (i.e., the true pos-
terior probability) is bounded by the approximation errors in the
prior distributions of the four parameters. If we can accurately give
the prior distributions for each parameter ğœƒ, then Î”ğœƒ1 = Î”ğœƒ0 =
Î”ğœƒğ‘¥= Î”ğœƒğ‘¦= 0 and this approach can reduce to the optimal strat-
egy. This highlights the importance of incorporating appropriate
prior knowledge for different contexts.
Reducing computation complexity. Recall that the MAP strategy
in Eq.(2) requires enumerating all 2ğ‘+ğ‘€combinations. Although
the posterior probability is computable in Eq.(9), the enumeration
cost still constrains the efficient identification of the optimal solu-
tion. Fortunately, given the role of the indicator ğ‘ƒ1, only consistent
combinations where ğ‘ƒ1 = 1 need consideration. To be specific, for
any Ë†x âˆˆ{0, 1}ğ‘and Ë†y âˆˆ{0, 1}ğ‘€combination:
â€¢ Ë†x must conform to the consistency assumption (Assumption 1).
Thus, any correct solution ğ‘–with Ë†ğ‘¥ğ‘–= 1 must pass the same test
cases, i.e., they should be within the same consensus set.
â€¢ Ë†y must match the test cases passable by any correct solution,
meaning all correct test cases ğ‘—with Ë†ğ‘¦ğ‘—= 1 should also reside in
the corresponding consensus set of the correct solutions.
Therefore, we claim that valid combinations must ensure that
all correct solutions and test cases should be in the same consensus
set. To reduce computations further, we consider any two solutions
within the same consensus set. As these solutions pass identical
test cases, they are completely symmetric and indistinguishable
in E. Therefore, it is illogical to differentiate between them. Thus,
we assume that solutions within the same consensus set should have
identical predicted correctness.
Based on these insights, we propose an enumeration method
based on consensus sets. Similar to CodeT, we initially divide solu-
tions and test cases into ğ¾consensus sets (ğ‘†ğ‘¥
ğ‘–,ğ‘†ğ‘¦
ğ‘–)ğ¾
ğ‘–=1. Within each
set (ğ‘†ğ‘¥
ğ‘–,ğ‘†ğ‘¦
ğ‘–), we predict all solutions in ğ‘†ğ‘¥
ğ‘–as 1 and all test cases
in ğ‘†ğ‘¦
ğ‘–as 1, while others are predicted as 0. This forms a consistent
configuration (Ë†x, Ë†y). We then calculate the posterior of (Ë†x, Ë†y) with
Eq.(10), where ğ‘ƒ1 = 1 is always satisfied. This significantly reduces
the number of explored configurations from 2ğ‘+ğ‘€to ğ¾.
3.3
Incorporating Prior Knowledge
We have derived a general explicit expression for the posterior
probability in Eq.(9), which includes eight hyperparameters corre-
sponding to the Beta distribution for four ğœƒ. According to Theo-
rem 1, we should incorporate proper prior knowledge to effectively
approximate the optimal strategy. In this section, we investigate
how to achieve this in the context of code generation.
Priors for ğœƒ0 and ğœƒ1. In practical scenarios, a test suite, not to
mention a test case, is often incomplete. Therefore, a correct test
case can fail to identify an incorrect solution, causing incorrect
solutions to have a moderate probability of passing correct test
cases (i.e., ğœƒ1). Conversely, to pass incorrect test cases that validate
flawed functionalities, incorrect solutions must "accidentally" match
this specific flaw to pass, making such occurrences (ğœƒ0) relatively
rare. This suggests that in practice, ğœƒ0 may be very small, but ğœƒ1
may not have a clear pattern.
To validate this conjecture, we analyzed code and test case gen-
eration tasks with five different models on HumanEval (See Sec-
tion 5.2.1 for details of models) and computed the actual values of
ğœƒ1 and ğœƒ0 for each problem in HumanEval using ground-truth solu-
tions. Fig. 3(a) displays the true distributions of these parameters,
showing that most ğœƒ0 values are concentrated near zero, while ğœƒ1
tends to follow a uniform distribution.
Based on this finding, we propose adopting a prior distribution
approaching zero for ğœƒ0 and a uniform prior distribution for ğœƒ1.
Therefore, we choose a beta prior distribution parameterized by
(ğ›¼0 = 1, ğ›½0 â‰«1) for ğœƒ0, and choose (ğ›¼1 = ğ›½1 = 1) for ğœƒ1. As
demonstrated in Fig. 3(b), such choice aligns with the findings in
Fig. 3(a). In practice, ğ›½0 serves as a tunable hyperparameter.
Priors for ğœƒğ‘¥and ğœƒğ‘¦. As discussed previously, each consistent
(Ë†x, Ë†y) corresponds to a consensus set. Chen et al. [5] identified a
heuristic rule that the consensus set with the largest capacity (i.e.,


--- Page 6 ---
ASE â€™24, October 27-November 1, 2024, Sacramento, CA, USA
Mouxiang Chen, Zhongxin Liu, He Tao, Yusu Hong, David Lo, Xin Xia, and Jianling Sun
0.0
0.2
0.4
0.6
0.8
1,
0
Distribution Density
1
0
(a) Distributions for ğœƒ0 and ğœƒ1
0.0
0.2
0.4
0.6
0.8
1.0
0.0
2.5
5.0
7.5
10.0
P( )
1(1
)
1
= 1,
= 10
=
= 1
= 10,
= 1
(b) Beta distributions
Figure 3: (a) Real distributions for two parameters ğœƒ0 and ğœƒ1.
(b) Three Beta distributions with different hyperparameters.
ğ‘›ğ‘¥ğ‘›ğ‘¦) is most likely correct. We will validate this rule theoretically
in Section 4. Accordingly, we want the prior distribution ğ‘(Ë†x, Ë†y)
to favor configurations containing more ones and reward larger
consensus sets. This can be implemented by setting the hyperpa-
rameters for ğœƒğ‘¥as (ğ›¼ğ‘¥â‰«1, ğ›½ğ‘¥= 1), and for ğœƒğ‘¦as (ğ›¼ğ‘¦â‰«1, ğ›½ğ‘¦= 1),
as illustrated in Fig. 3(b). Moreover, we find it sufficient to combine
ğ›¼ğ‘¥and ğ›¼ğ‘¦into a single hyperparameter ğ›¼ğ‘¥ğ‘¦, further reducing the
parameter tuning space (see Section 5.2.4 for details).
Answer to RQ3: A practical strategy to approximate un-
computable optimal strategy is to score ğ¾consensus sets and
select solutions within the highest-score set. The score is
determined by multiplying 4 Beta functions, i.e.,
B (ğ‘›1 + 1, |E1| âˆ’ğ‘›1 + 1) Â· B (ğ‘›0 + 1, |E0| âˆ’ğ‘›0 + ğ›½0)
Â·B  ğ‘›ğ‘¥+ ğ›¼ğ‘¥ğ‘¦, ğ‘âˆ’ğ‘›ğ‘¥+ 1 Â· B  ğ‘›ğ‘¦+ ğ›¼ğ‘¥ğ‘¦, ğ‘€âˆ’ğ‘›ğ‘¦+ 1 , (10)
where ğ›½0 and ğ›¼ğ‘¥ğ‘¦are tunable hyperparameters.
3.4
Further Analysis of Algorithm B4
Given that the score in Eq.(10) is multiplied by four Beta func-
tions, we name this practical strategy B4. In this section, we provide
a detailed analysis of the proposed B4 to deepen the understanding.
Full algorithm. Algorithm 1 outlines the workflow. Line 1 starts by
collecting the set of test cases each code ğ‘–passes (denoted as ğ’†ğ‘–, i.e.,
{ğ‘’ğ‘–1, Â· Â· Â· ,ğ‘’ğ‘–ğ‘€}) and removes duplicates. In Line 3, we iterate over
all unique test case sets. For each Ë†y processed, we identify solutions
whose passed test cases precisely match Ë†y as Ë†x in Line 4. Note that
Ë†x and Ë†y define a consensus set together. Lines 5-9 compute the
score of this consensus set (i.e., the posterior) by Eq.(10). Ultimately,
Lines 10-11 identify the consensus set with the highest score as the
prediction. For numerical stability, we often store the logarithm of
the scores in practice, by summing the logarithms of the four Beta
functions.
A running example. We reuse Fig. 1 to illustrate how B4 works,
using the hyperparameters ğ›½0 = ğ›¼ğ‘¥ğ‘¦= 10. Firstly, we deduplicate
the rows in Eq.(1) and obtain ğ‘†ğ‘¦= {[1, 1, 1, 0, 0], [0, 1, 1, 1, 1], [0, 0, 1,
1, 0]}, indicating there are three distinct sets of passed test cases cor-
responding to three consensus sets. We need to iterate all three sets
and score for each one. For the first iteration, Ë†y = [1, 1, 1, 0, 0] and
Ë†x = [1, 1, 0, 0]. It indicates the first consensus set is ({ğ‘¥1,ğ‘¥2}, {ğ‘¦1,ğ‘¦2,
ğ‘¦3}). Using Eq.(5), we obtain:
Algorithm 1: Algorithm for B4
Input: Passing matrix E = {ğ‘’ğ‘–ğ‘—} âˆˆ{0, 1}ğ‘Ã—ğ‘€,
hyperparameters ğ›½0 > 1, ğ›¼ğ‘¥ğ‘¦> 1
Output: Ë†xâˆ—âˆˆ{0, 1}ğ‘and Ë†yâˆ—âˆˆ{0, 1}ğ‘€indicating the
predicted correct solutions and test cases
1 ğ‘†ğ‘¦â†Deduplicate({ğ’†ğ‘–| ğ‘–âˆˆ[ğ‘]});
2 ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’âˆ—â†âˆ’âˆ;
3 for Ë†y âˆˆğ‘†ğ‘¦do
4
Ë†x â†

1ğ’†ğ‘–=Ë†y | ğ‘–âˆˆ[ğ‘]
	
;
5
E1 â†{ğ‘’ğ‘–ğ‘—| Ë†ğ‘¥ğ‘–= 0, Ë†ğ‘¦ğ‘—= 1,ğ‘–âˆˆ[ğ‘], ğ‘—âˆˆ[ğ‘€]};
6
E0 â†{ğ‘’ğ‘–ğ‘—| Ë†ğ‘¥ğ‘–= 0, Ë†ğ‘¦ğ‘—= 0,ğ‘–âˆˆ[ğ‘], ğ‘—âˆˆ[ğ‘€]};
7
ğ‘›1 â†Ã
ğ‘’âˆˆE1 ğ‘’,
ğ‘›0 â†Ã
ğ‘’âˆˆE0 ğ‘’;
8
ğ‘›ğ‘¥â†Ã
ğ‘–âˆˆ[ğ‘] Ë†ğ‘¥ğ‘–,
ğ‘›ğ‘¦â†Ã
ğ‘—âˆˆ[ğ‘€] Ë†ğ‘¦ğ‘—;
9
ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’â†
B (ğ‘›1 + 1, |E1| âˆ’ğ‘›1 + 1) Â· B (ğ‘›0 + 1, |E0| âˆ’ğ‘›0 + ğ›½0) Â·
B  ğ‘›ğ‘¥+ ğ›¼ğ‘¥ğ‘¦, ğ‘âˆ’ğ‘›ğ‘¥+ 1 Â· B  ğ‘›ğ‘¦+ ğ›¼ğ‘¥ğ‘¦, ğ‘€âˆ’ğ‘›ğ‘¦+ 1;
10
if ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’> ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’âˆ—then
11
(ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’âˆ—, Ë†xâˆ—, Ë†yâˆ—) â†(ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’, Ë†x, Ë†y);
12 return Ë†xâˆ—, Ë†yâˆ—;
0
2000
4000
n0
0.0
0.2
0.4
0.6
0.8
1.0
0
Ã—106
3.15
2.70
2.25
1.80
1.35
0.90
0.45
0.00
Ã—104
(a) log B (ğ‘›0 + 1, |E0| âˆ’ğ‘›0 + ğ›½0)
0
25
50
75
100
nx
0
1
2
3
4
5
xy
Ã—103
4.8
4.2
3.6
3.0
2.4
1.8
1.2
0.6
0.0
Ã—102
(b) log B  ğ‘›ğ‘¥+ ğ›¼ğ‘¥ğ‘¦, ğ‘âˆ’ğ‘›ğ‘¥+ 1
Figure 4: Visualization of two Beta functions used in our
scoring strategy. We set |E0| = 5000 and ğ‘= 100.
E1 ={ğ‘’ğ‘–ğ‘—| Ë†ğ‘¥ğ‘–= 0, Ë†ğ‘¦ğ‘—= 1} = {ğ‘’31,ğ‘’32,ğ‘’33,ğ‘’41,ğ‘’42,ğ‘’43},
E0 ={ğ‘’ğ‘–ğ‘—| Ë†ğ‘¥ğ‘–= 0, Ë†ğ‘¦ğ‘—= 0} = {ğ‘’34,ğ‘’35,ğ‘’44,ğ‘’45},
where E1 (or E0) represents the events that an incorrect solution
passes a correct (or an incorrect) test case, under the prediction Ë†x
and Ë†y. We count these events: ğ‘›1 = Ã E1 = 3, ğ‘›0 = Ã E0 = 3,ğ‘›ğ‘¥=
Ã Ë†x = 2, and ğ‘›ğ‘¦= Ã Ë†y = 3. Following this, the score is:
B (3 + 1, 6 âˆ’3 + 1) Ã— B (3 + 1, 4 âˆ’3 + 10)
Ã— B (2 + 10, 4 âˆ’2 + 1) Ã— B (3 + 10, 5 âˆ’3 + 1) = 1.20 Ã— 10âˆ’12.
For the second iteration, we have Ë†y = [0, 1, 1, 1, 1] and Ë†x =
[0, 0, 1, 0], resulting the score 1.15 Ã— 10âˆ’13. For the third iteration,
we have Ë†y = [0, 0, 1, 1, 0] and Ë†x = [0, 0, 0, 1], resulting the score
1.24 Ã— 10âˆ’15. One can find that the first consensus set has the
largest score 1.20 Ã— 10âˆ’12, leading to the selection of {ğ‘¥1,ğ‘¥2} as the
optimal solution.
Understanding Beta functions. To further explore the role of
two hyperparameters used in the B4 and our scoring strategy, we
visualize two Beta functions related to two hyperparameters ğ›½0 and
ğ›¼ğ‘¥ğ‘¦in Fig. 4. Fig. 4(a) reveals that the function value is insensitive


--- Page 7 ---
B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests
ASE â€™24, October 27-November 1, 2024, Sacramento, CA, USA
to ğ‘›0 when ğ›½0 is very small. As ğ›½0 increases, the Beta function has
little change for small ğ‘›0 but has a particularly small value for large
ğ‘›0. This suggests that a larger ğ›½0 leads the algorithm to reward
predictions with smaller ğ‘›0. Recall that ğ‘›0 represents the number of
incorrect solutions passing incorrect test cases, which is generally
small in the real world (as discussed in Section 3.3). This indicates
that our B4, which uses a ğ›½0 â‰«1, aligns with practical conditions
well. Similarly, Fig. 4(b) shows a large ğ›¼ğ‘¥ğ‘¦leads the algorithm to
predict more correct solutions or tests (i.e., larger ğ‘›ğ‘¥or ğ‘›ğ‘¦), which
rewards a larger consensus set as we expected in Section 3.3.
4
THEORETICAL ANALYSIS
In this section, we address RQ4 by a theoretical accuracy anal-
ysis of the two representative heuristics, MaxPass and CodeT, to
investigate under what conditions they can and cannot work. Max-
Pass is a widely-used heuristic [18, 19, 22, 33] and CodeT is the
state-of-the-art heuristic for code generation. Furthermore, these
theoretical analyses further explain why the priors for ğ‘ƒ(Ë†x, Ë†y) intro-
duced in Section 3.3 are chosen. We assume that Assumptions 1-3
are satisfied, and the data follows the generation process in Fig. 2.
All proofs can be found in the online Appendix [6].
We begin with a theorem which assesses MaxPassâ€™s accuracy
when there is a large number of test cases:
Lemma 4.1. Suppose there exist ğ‘›ğ‘¦correct test cases and ğ‘›ğ‘¦incor-
rect test cases (ğ‘›ğ‘¦+ğ‘›ğ‘¦= ğ‘€). When both ğ‘›ğ‘¦and ğ‘›ğ‘¦are large enough,
the probability of any incorrect code passing ğ‘Œ(ğ‘Œâ‰¥ğ‘›ğ‘¦) test cases is:
ğ‘ƒ(ğ‘Œâ‰¥ğ‘›ğ‘¦) âˆ¼Î¦
 
ğ‘›ğ‘¦ğœƒ0 âˆ’ğ‘›ğ‘¦(1 âˆ’ğœƒ1)
âˆšï¸ğ‘›ğ‘¦ğœƒ1(1 âˆ’ğœƒ1) + ğ‘›ğ‘¦ğœƒ0(1 âˆ’ğœƒ0)
!
,
where Î¦ is the cumulative distribution function (CDF) of the standard
normal distribution. ğœƒ0 and ğœƒ1 are defined in Eq.(3).
Theorem 2 (Impact of correct test cases for MaxPass). If
ğœƒ1 < 1, the accuracy of MaxPass (i.e., the probability of all incorrect
solutions passing less than ğ‘›ğ‘¦test cases) can exponentially converge
to 1 as ğ‘›ğ‘¦â†’âˆ.
Theorem 3 (Impact of incorrect solutions for MaxPass). If
there are ğ‘›ğ‘¥incorrect solutions, the accuracy of MaxPass can expo-
nentially converge to 0 as ğ‘›ğ‘¥â†’âˆ.
Theorem 2 demonstrates the working condition for MaxPass: it
requires a large amount of correct test casesğ‘›ğ‘¦to make the accuracy
converge to 1. However, Theorem 3 also underscores a limitation
of MaxPass: it lacks scalability to the number of code solutions
ğ‘. As ğ‘increases, ğ‘›ğ‘¥increases and the accuracy of MaxPass will
exponentially converge to zero.
Following this, we analyze the error of CodeT. Considering the
problemâ€™s complexity, we fix the ğ‘€test cases and explore how the
error evolves as the number of generated code solutions ğ‘grows,
as shown in the following theorem.
Lemma 4.2. Suppose the correctness of code solutions and test cases
are x and y. Let ğ‘›ğ‘¥= Ã x and ğ‘›ğ‘¦= Ã y denote the number of correct
code solutions and test cases, respectively. For any incorrect consensus
set that corresponds to a prediction Ë†x and Ë†y, similarly let ğ‘›Ë†ğ‘¥= Ã Ë†x
and ğ‘›Ë†ğ‘¦= Ã Ë†y. For arbitrary y and Ë†y, if ğ‘is sufficiently large, the
probability of this consensus set being scored higher than the correct
one by CodeT (i.e., ğ‘›Ë†ğ‘¥ğ‘›Ë†ğ‘¦> ğ‘›ğ‘¥ğ‘›ğ‘¦) follows:
ğ‘ƒ(ğ‘›Ë†ğ‘¥ğ‘›Ë†ğ‘¦> ğ‘›ğ‘¥ğ‘›ğ‘¦) âˆ¼Î¦
Â©Â­Â­
Â«
âˆš
ğ‘(ğœƒâ€²ğ‘›Ë†ğ‘¦âˆ’ğœƒğ‘¥ğ‘›ğ‘¦)
âˆšï¸ƒ
ğ‘›2
Ë†ğ‘¦ğœƒâ€²(1 âˆ’ğœƒâ€²) + ğ‘›2ğ‘¦ğœƒğ‘¥(1 âˆ’ğœƒğ‘¥) âˆ’2ğ‘›Ë†ğ‘¦ğ‘›ğ‘¦ğœƒâ€²ğœƒğ‘¥
ÂªÂ®Â®
Â¬
,
where ğœƒâ€² is a constant, defined as:
ğœƒâ€² = (1 âˆ’ğœƒğ‘¥)ğœƒË†yâŠ¤y
1
(1 âˆ’ğœƒ1)(1âˆ’Ë†y)âŠ¤yğœƒ0 Ë†yâŠ¤(1âˆ’y) (1 âˆ’ğœƒ0)(1âˆ’Ë†y)âŠ¤(1âˆ’y).
Theorem 4 (Impact of ğœƒğ‘¥and ğ‘for CodeT). If ğœƒğ‘¥is large
enough such that ğœƒâ€²ğ‘›Ë†ğ‘¦< ğœƒğ‘¥ğ‘›ğ‘¦, then the error probability ğ‘ƒ(ğ‘›Ë†ğ‘¥ğ‘›Ë†ğ‘¦>
ğ‘›ğ‘¥ğ‘›ğ‘¦) can exponentially converge to 0 as ğ‘â†’âˆ. Otherwise, if ğœƒğ‘¥
is low enough such that ğœƒâ€²ğ‘›Ë†ğ‘¦> ğœƒğ‘¥ğ‘›ğ‘¦, the error probability converges
to 1 as ğ‘â†’âˆ.
Theorem 4 elucidates the working condition for CodeT: it re-
quires a sufficient high correct probability of code solutions (high
ğœƒğ‘¥). If the generated solutions contain excessive incorrect solutions,
CodeT may not work well. An important insight is that under the
condition of high ğœƒğ‘¥, CodeT offers better scalability compared to
MaxPass: as the number of solutions ğ‘increases, CodeTâ€™s selec-
tion accuracy can exponentially converge towards 1 (Theorem 4),
whereas MaxPassâ€™s accuracy will converge towards 0 (Theorem 3).
Answer to RQ4: Existing heuristics work under specific con-
ditions. MaxPass requires sufficient correct test cases, while
CodeT requires a high correct probability of solutions. When
both of their requirements are satisfied, CodeT has better
scalability with the number of solutions ğ‘than MaxPass.
Considering the analyzing complexity, whether a similar error
probability analysis can be directly provided for B4 is an open
question.1 Fortunately, these theoretical analyses still indirectly
support the effectiveness of B4. For example, Theorem 4 validates
the effectiveness of the priors forğœƒğ‘¥andğœƒğ‘¦of our B4. Recall that our
introduced priors for ğ‘ƒ(Ë†x, Ë†y) are similar to CodeTâ€™s assumptions
(Section 3.3), which offers similar scalability benefits under the
condition that ğœƒğ‘¥is relatively large. However, it is crucial to note
that these priors are just part of our methods. Besides the priors for
ğœƒğ‘¥andğœƒğ‘¦, we also incorporate priors forğœƒ0 andğœƒ1, which effectively
compensates for the limitations of CodeTâ€™s priors, particularly in
scenarios where ğœƒğ‘¥is low. As our subsequent experiments confirm,
B4 significantly outperforms CodeT in such challenging scenarios.
5
EXPERIMENT
In this section, we conduct experiments to further answer RQ4
and RQ5. We start with exploring the conditions under which exist-
ing heuristics can work efficiently through simulation experiments
in different controlled environments, to validate the theoretical
insights discussed in Section 4. Subsequently, we compare the per-
formance of B4 with existing heuristics on real-world datasets.
1To show the complexity, note that computing the distribution for B4â€™s score is
necessary for estimating error probability. The score can be represented as the product
of ğ‘›ğ‘¥, ğ‘›1, and ğ‘›0 after nonlinear transformations (here we assume ğ‘›ğ‘¦is given, as
Lemma 4.2). However, despite oversimplification, i.e., treating three variables as normal,
linearizing the transformations, and assuming their independence, the computation is
still a challenge in the literature [37].


--- Page 8 ---
ASE â€™24, October 27-November 1, 2024, Sacramento, CA, USA
Mouxiang Chen, Zhongxin Liu, He Tao, Yusu Hong, David Lo, Xin Xia, and Jianling Sun
5
10
15
20
Number of codes N
0.5
0.6
0.7
0.8
0.9
1.0
Pass@1
B4 (ours)
CodeT
MaxPass
(a) Varying ğ‘
5
10
15
20
25
30
Number of tests M
0.2
0.4
0.6
0.8
1.0
Pass@1
B4 (ours)
CodeT
MaxPass
(b) Varying ğ‘€
0.0
0.2
0.4
0.6
0.8
Probability of correct codes 
x
0.5
0.6
0.7
0.8
0.9
1.0
Pass@1
B4 (ours)
CodeT
MaxPass
(c) Varying ğœƒğ‘¥
0.0
0.2
0.4
0.6
0.8
Probability of correct tests 
y
0.0
0.2
0.4
0.6
0.8
1.0
Pass@1
B4 (ours)
CodeT
MaxPass
(d) Varying ğœƒğ‘¦
Figure 5: Pass@1 results of the three methods under different conditions in the simulated experiments. By default, we set
ğ‘= 10, ğ‘€= 30, ğœƒğ‘¥= 0.2, and ğœƒğ‘¦= 0.3 except for the varied one.
5.1
Simulated Experiments
In our simulated experiments, we sampled ğ‘= 10 solutions
and ğ‘€= 30 test cases, and set four parameters ğœƒğ‘¥= 0.2, ğœƒğ‘¦= 0.3,
ğœƒ1 = 0.4 and ğœƒ0 = 0.1 by default. These default values are based on
our measurement of the real data generated by CodeGen [29] on
HumanEval [7]. Based on these parameters, we randomly sampled
a data point (x, y, E) following the process shown in Fig. 2. Subse-
quently, we used MaxPass, CodeT, and B4 to select the solutions
Ë†x using E, and computed the proportion of correct solutions within
Ë†x (i.e., Pass@1) using the ground-truth x. We repeated this process
20,000 times and averaged the results to ensure stability for each
experiment. Following Section 3.3, the hyperparameters ğ›½0 and ğ›¼ğ‘¥ğ‘¦
should be larger than 1, and we preliminarily chose ğ›½0 = ğ›¼ğ‘¥ğ‘¦= 10.
Figs. 5(a) and 5(b) display the results as the scale of data ğ‘and
ğ‘€change. One can observe in Fig. 5(a) that CodeTâ€™s performance
gradually improves with an increase in the number of code solutions
ğ‘, whereas MaxPass shows a decline as ğ‘increases. This confirms
our theoretical results in Section 4: CodeT has better scalability
with ğ‘than MaxPass. Fig. 5(b) shows that unlike with ğ‘, MaxPass
tends to improve as ğ‘€increases. Regardless of the values of ğ‘and
ğ‘€, B4 consistently outperforms the two baselines, proving that
existing heuristic algorithms are not optimal. Specifically, B4 tends
to provide greater performance enhancements relative to CodeT
when ğ‘is small. This could be because CodeT does not perform
as well when ğ‘is low, which is also validated in Theorem 4.
Figs. 5(c) and 5(d) display the results as the probability of correct
solutions ğœƒğ‘¥and test cases ğœƒğ‘¦change. All three methods gradu-
ally improve as the accuracy increases. Specifically, both B4 and
CodeTâ€™s accuracies can converge to 1 as ğœƒğ‘¥increases, while all
three methods converge to 1 as ğœƒğ‘¦increases. This indicates that
MaxPass is less sensitive to ğœƒğ‘¥but more responsive to ğœƒğ‘¦, con-
firming the findings of Lemma 4.1 that the number of correct test
cases matters for MaxPass. B4 consistently outperforms all the
two heuristics under all conditions. Notably, when ğœƒğ‘¥is low, it
significantly outperforms CodeT with a large improvement. This
suggests that CodeT struggles under the condition of few correct
solutions and affirms the findings of Theorem 4.
5.2
Real-world Experiments
5.2.1
Experiment setup. We conducted experiments on three public
code generation benchmarks, HumanEval [7], MBPP [2] (sanitized
version), and APPS [16] with three difficulty levels. These bench-
marks have been widely used by LLM-based code generation studies
[7, 13, 21, 29, 34]. Specifically, each benchmark contains some cod-
ing tasks, and each task consists of a natural language requirement,
a function signature, and a golden test suite for evaluating the cor-
rectness of generated solutions. Notably, these golden test suites
and the generated test cases are not the same; the generated test
cases are used by each selection strategy to select the generated
code, while the golden test suites are solely used to evaluate the
performance of selection strategies.
We used the same zero-shot prompt format as CodeT [5] for
both code and test case generation. Following CodeT, the numbers
of generated solutions and test cases are 100 for HumanEval and
MBPP and 50 for APPS. Both solutions and tests are generated by
the same model.
For models, our experiments are based on Codex [7] (code-
davinci-002 version), CodeGen [29] (16B Python mono-lingual
version), and three recent open-source models, StarCoder [21],
CodeLlama [34] (7B Python version) and Deepseek-Coder [13]
(6.7B Instruct version). The generation hyperparameters such as
temperature, top ğ‘, and max generation length are the same as [5].
Additionally, as APPS has significantly more problems (5,000) com-
pared to HumanEval (164) and MBPP (427), testing all models on it
is prohibitively expensive. Given that Codex outperforms the other
models on HumanEval and MBPP in most of our experiments (using
CodeT strategy), we followed Chen et al. [5] by only evaluating
Codexâ€™s outputs on the APPS dataset.
For baselines, in addition to MaxPass [18, 19] and CodeT [5],
we also used MBR-exec [22, 36], which is similar to CodeT but
scores each consensus set with the number of solutions, and a
naive Random, which picks a code from the generated solutions
randomly. We reported the average Pass@1 of the selected solutions.
Our method is presented in the format of B4(log10 ğ›½0, log10 ğ›¼ğ‘¥ğ‘¦).
For example, B4(4,3) represents ğ›½0 = 104 and ğ›¼ğ‘¥ğ‘¦= 103. For a fair
comparison, all the methods operate on the same passing matrices
E. We reported three variants of methods: B4(4,3), B4(5,3), and
B4(6,3), and compared each of them with CodeT using Wilcoxon
signed-rank significance test [43].
To comprehensively evaluate different selection methods, we
filtered the problems based on the proportion of correct solutions
among all generated solutions (i.e., ğœƒğ‘¥). We first filtered out prob-
lems with ğœƒğ‘¥= 1 and ğœƒğ‘¥= 0, as the solutions for these problems are
either entirely correct or incorrect, which can not differentiate se-
lection strategies. We name this setting discriminative problems.
To provide a more challenging environment for selecting correct


--- Page 9 ---
B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests
ASE â€™24, October 27-November 1, 2024, Sacramento, CA, USA
Table 1: Pass@1 (%) of the code solutions selected by different strategies across various datasets and models with two settings
(RD=Random, MP=MaxPass, MBR=MBR-exec, CT=CodeT). We also reported the average relative improvement of the three
B4 variants over the strongest heuristic CodeT and the p-values derived from the Wilcoxon signed-rank test.
Dataset
Model
Discriminative Problems (0 < ğœ½ğ’™< 1)
Hard Problems (0 < ğœ½ğ’™< 0.5)
RD
MP
MBR
CT
ours
RD
MP
MBR
CT
ours
B4(4,3)
B4(5,3)
B4(6,3)
B4(4,3)
B4(5,3)
B4(6,3)
HumanEval
CodeGen
32.5
28.6
44.8
51.5
56.8
58.0
56.9
13.0
11.1
21.0
31.4
38.2
40.0
40.8
Codex
39.2
57.8
55.0
71.7
70.6
73.1
73.1
19.2
43.2
27.6
54.6
52.9
56.9
56.9
StarCoder
29.8
32.2
47.9
55.0
59.0
59.3
57.8
15.0
16.3
29.3
38.9
44.4
44.8
42.8
CodeLlama
34.1
40.6
52.6
61.7
63.5
64.8
64.0
15.8
24.5
30.8
44.1
46.7
48.6
47.4
Deepseek-Coder
65.3
58.2
80.4
79.2
80.5
78.5
78.5
24.7
33.7
35.0
30.6
35.5
31.3
31.3
MBPP
CodeGen
42.4
48.1
56.4
64.9
66.7
64.9
64.7
21.8
30.8
28.4
43.5
45.6
42.5
42.3
Codex
55.1
70.5
71.9
80.0
80.8
81.3
81.9
23.9
46.4
32.5
53.9
55.1
56.6
58.0
StarCoder
46.1
55.6
65.6
69.6
70.6
70.6
70.6
21.5
39.5
37.8
45.6
47.5
47.9
47.9
CodeLlama
47.2
60.0
65.4
72.4
72.6
73.4
73.8
19.8
39.0
30.7
44.7
45.0
46.7
47.5
Deepseek-Coder
56.5
71.4
66.9
75.2
75.9
75.9
75.9
22.3
45.6
25.7
45.9
46.7
47.6
47.6
APPS introductory
Codex
36.2
46.4
41.6
59.5
63.7
63.7
64.4
17.6
29.5
15.9
41.6
46.6
47.6
48.3
APPS interview
15.6
26.0
14.7
36.0
40.4
40.8
41.1
11.2
22.4
8.0
30.6
35.1
35.5
35.9
APPS competition
7.9
16.8
3.1
17.3
23.1
25.2
25.2
7.0
16.2
2.5
16.0
21.9
24.0
24.0
Avg. relative improvement over the strongest heuristic CodeT
+6.1%
+7.5%
+7.2%
+10.1%
+12.0%
+12.0%
p-value
0.001
0.0003
0.0006
0.0009
0.0004
0.0004
solutions, we propose a new setting on a subset of discriminative
problems where 0 < ğœƒğ‘¥< 0.5, named hard problems.
5.2.2
Main results. Table 1 presents the main results, showing that
all three B4 variants consistently and significantly outperform ex-
isting heuristics. Specifically, each single variant of B4 outperforms
all baselines in most cases. On average, each variant surpasses the
strongest heuristic baseline, CodeT, by 6-12% with statistically sig-
nificant differences (proven by significance tests). This highlights a
substantial gap between existing heuristics and the optimal strategy
and suggests our method effectively approximates the optimal.
Additionally, B4 shows a greater performance improvement over
CodeT in more challenging scenarios (i.e., smaller ğœƒğ‘¥). It achieves
a 6.1%-7.5% relative improvement in discriminative problems and
a 10.1%-12.0% improvement in hard problems. In the most chal-
lenging scenario (APPS competition on hard problems), B4 can
even deliver up to a 50% enhancement over CodeT and 246% over
random selection. These findings align with the conclusions of
Lemma 4.2 and the simulated experiments depicted in Fig. 5(c), con-
firming that existing heuristics struggle with more difficult tasks.
We also observed that the gains from B4 on the MBPP dataset are
less significant than on HumanEval and APPS, likely because the
MBPP problems are inherently simpler, as indicated by Random.
For hyperparameters, the optimal hyperparameter ğ›½0 for B4 varies
across different scenarios, suggesting that the prior distribution
of ğœƒ0 may differ depending on the context. This makes sense as
different models might generate incorrect solutions and test cases
with different patterns. For example, when models more easily mis-
interpret the problem, leading solutions and test cases to follow
the same incorrect patterns, the probability of incorrect solutions
passing incorrect test cases ğœƒ0 can increase, thus necessitating a
larger ğ›½0 to reflect this change. We will further discuss the impact
of hyperparameters in the next section.
Answer to RQ5: The proposed B4 significantly outperforms
existing heuristics, achieving a 6.1%-7.5% relative improve-
ment in discriminative problems and a 10.1%-12.0% improve-
ment in hard problems over the strongest CodeT.
5.2.3
Ablation studies on two hyperparameters. Figs. 6(a) and 6(b)
show the average performance on two datasets as influenced by two
hyperparameters ğ›½0 and ğ›¼ğ‘¥ğ‘¦. Recall that ğ›½0 controls the likelihood
ğ‘ƒ(E | Ë†x, Ë†y) and ğ›¼ğ‘¥ğ‘¦controls the prior ğ‘ƒ(Ë†x, Ë†y). For ğ›½0, performance
on both datasets initially increases and decreases as ğ›½0 increases,
with the optimal value around 104-106. This pattern suggests that
an appropriate ğ›½0 can better align with the prior distribution of ğœƒ0,
resulting in more accurate likelihood estimates.
For ğ›¼ğ‘¥ğ‘¦, we found that performance improves with an increase
in ğ›¼ğ‘¥ğ‘¦on HumanEval and MBPP, whereas the opposite is true for
APPS. Recall that a larger ğ›¼ğ‘¥ğ‘¦makes the strategy closer to CodeT.
One possible reason is that the tasks in HumanEval and MBPP are
relatively simpler, so CodeT performs better on these two datasets,
as shown in Theorem 4.
5.2.4
Ablation studies on splitting ğ›¼ğ‘¥ğ‘¦into two individual hyper-
parameters ğ›¼ğ‘¥and ğ›¼ğ‘¦. As discussed in Section 3.3, we combined
ğ›¼ğ‘¥and ğ›¼ğ‘¦into a single ğ›¼ğ‘¥ğ‘¦in Eq.(8). This section examines the
effects of tuning ğ›¼ğ‘¥and ğ›¼ğ‘¦independently. Section 5.2.3 shows the
trend of average performance across all datasets as ğ›¼ğ‘¥and ğ›¼ğ‘¦vary,
with ğ›½0 set at 106. We observe that performance declines signif-
icantly when ğ›¼ğ‘¦âˆ’ğ›¼ğ‘¥has a large value (i.e., in the bottom right
area of Section 5.2.3). As ğ›¼ğ‘¦âˆ’ğ›¼ğ‘¥gradually decreases (moving from
the bottom right towards the top left), performance can be gradu-
ally improved. The method achieves optimal performance when
ğ›¼ğ‘¥and ğ›¼ğ‘¦are closed (ğ›¼ğ‘¥= 103 and ğ›¼ğ‘¦= 102). Considering that
the modelâ€™s performance is not sensitive to ğ›¼ğ‘¥ğ‘¦when ğ›½0 is within
an appropriate range, we argue that merging ğ›¼ğ‘¥and ğ›¼ğ‘¦into one
hyperparameter simplifies tuning without substantially affecting
performance. Therefore, we adopted ğ›¼ğ‘¥= ğ›¼ğ‘¦= 103 in our previous
main experiment.


--- Page 10 ---
ASE â€™24, October 27-November 1, 2024, Sacramento, CA, USA
Mouxiang Chen, Zhongxin Liu, He Tao, Yusu Hong, David Lo, Xin Xia, and Jianling Sun
100 101 102 103 104 105 106 107
xy
109
108
107
106
105
104
103
102
101
100
0
66.7 67.1 66.9 67.1 67.1 67.2 67.4 67.4
67.6 67.6 67.6 67.8 68.0 67.8 67.8 67.6
67.8 67.8 68.0 67.9 67.8 67.8 67.8 67.8
68.1 68.1 68.1 68.3 68.3 68.3 68.3 68.1
68.9 68.9 68.9 68.9 69.1 68.9 68.8 68.8
68.2 68.2 68.6 68.4 68.6 68.8 68.9 69.3
66.6 66.6 66.8 66.9 67.1 67.2 67.1 67.5
65.2 65.5 65.7 66.3 66.5 66.7 67.0 67.3
64.4 64.6 65.1 66.3 66.5 66.7 67.0 67.3
64.2 64.4 65.1 66.3 66.5 66.7 67.0 67.3
65
66
67
68
69
(a) HumanEval
100 101 102 103 104 105 106 107
xy
109
108
107
106
105
104
103
102
101
100
0
74.4 74.4 74.4 74.4 74.4 74.5 74.4 74.5
74.5 74.5 74.6 74.5 74.7 74.7 74.9 74.9
74.7 74.7 74.7 74.8 74.9 74.9 74.9 74.8
75.2 75.2 75.3 75.5 75.4 75.4 75.4 75.4
75.2 75.1 75.2 75.3 75.2 75.2 75.3 75.3
74.9 74.9 75.0 75.0 74.9 75.1 75.0 75.0
73.6 73.6 73.8 73.8 73.8 74.0 74.2 74.4
73.0 73.1 73.3 73.4 73.8 73.8 73.8 74.0
72.9 73.0 73.1 73.4 73.7 73.7 73.8 74.0
72.9 73.0 73.1 73.4 73.7 73.6 73.8 74.0
73
74
75
(b) MBPP
100 101 102 103 104 105 106 107
xy
109
108
107
106
105
104
103
102
101
100
0
31.1 31.2 31.4 31.6 31.8 31.9 31.5 30.4
31.7 32.3 32.0 31.7 31.7 31.9 30.8 30.6
32.6 32.7 32.9 32.8 32.1 31.3 31.5 30.8
32.8 32.9 33.4 33.1 31.9 31.6 30.8 29.8
33.8 33.8 33.6 33.0 31.4 31.0 30.2 30.2
34.0 34.0 33.1 31.7 30.8 30.4 30.2 29.5
30.1 30.0 30.1 29.9 28.4 29.0 28.7 28.2
28.9 28.3 28.4 28.4 27.8 28.1 27.6 27.1
28.9 28.2 28.3 27.9 27.8 28.1 27.6 27.1
28.9 28.2 28.3 27.9 27.8 28.1 27.6 27.1
28
29
30
31
32
33
34
(c) APPS
Figure 6: Pass@1 (%) results of varying ğ›¼ğ‘¥ğ‘¦and ğ›½0 on HumanEvalâ€™s, MBPPâ€™s, and APPSâ€™ discriminative problems.
100
101
102
103
104
105
106
y
106
105
104
103
102
101
100
x
63.5
63.6
63.6
63.7
63.5
63.4
63.3
63.6
63.6
63.6
63.6
63.3
63.3
63.2
63.5
63.5
63.6
63.5
63.4
63.3
63.1
63.6
63.7
63.8
63.7
63.4
63.3
63.2
63.6
63.6
63.4
63.4
63.0
62.9
62.8
63.3
63.2
63.2
63.0
62.8
62.7
62.6
63.1
63.0
63.0
62.8
62.7
62.6
62.4
62.6
62.8
63.0
63.2
63.4
63.6
Figure 7: Pass@1 (%) results of splitting ğ›¼ğ‘¥ğ‘¦into two hy-
perparameters ğ›¼ğ‘¥and ğ›¼ğ‘¦on the discriminative problems of
HumanEval and MBPP when ğ›½0 = 106.
5.2.5
Computational Cost. Table 2 shows the running time of the
B4 algorithm and CodeT, where B4 is slightly slower than CodeT
due to the relatively higher overhead of beta functions in B4 com-
pared to simple counting in CodeT. Notably, the computational
complexity of both is the same, as both first partition the consensus
sets and then score them. We can observe that even for large ğ‘€and
ğ‘(e.g., ğ‘€= ğ‘= 400), the running time is less than one second,
which is much less than the time to generate 400 solutions and tests
with LLMs. Therefore, we believe that the efficiency of B4 will not
become a bottleneck for practical systems.
6
DISCUSSION
In this section, we discuss the limitations and threats to the
validity of this study.
6.1
Limitations
Assumption 2 and 3. These assumptions are related to indepen-
dence. Assumption 2 considers the correctness of code solutions and
test cases are independent, which can be violated if there is a causal
relationship in their generation, such as using a generated test case
as input to an LLM for further generation. Assumption 3 states
that passing probability is solely determined by the correctness of
Table 2: Computation cost with the increases of the number
of code solutions ğ‘and test cases ğ‘€.
ğ‘and ğ‘€
100
200
300
400
CodeT
10 ms
65 ms
202 ms
455 ms
B4
15 ms
79 ms
243 ms
588 ms
the associated code and test case. However, the independence of
passing states may be broken by other unobserved factors hidden
in the code. For example, if two incorrect solutions exhibit similar
structures and similar error types, their passing states might be
positively correlated. Considering the significant complexity in-
troduced by the lack of independence, further exploration of the
dependence case is deferred to future research.
Prior for ğœƒ0. This prior assumes that ğœƒ0 (i.e., the probability of
incorrect solutions passing incorrect test cases) is typically low.
However, when LLMs misinterpret a problem, incorrect test cases
may coincidently specify the functionality of incorrect solutions
and potentially increase ğœƒ0. Considering that this prior can bring
considerable benefits (as shown in Section 5.2.3), we argue that its
advantages significantly outweigh the limitations.
Priors for ğœƒğ‘¥and ğœƒğ‘¦. These priors, similar to the heuristic rule
of CodeT, suggest that larger consensus sets are more likely to be
correct. We have validated its theoretical effectiveness under the
conditions of large ğ‘and high ğœƒğ‘¥, as detailed in Section 4. Even
though its efficacy may diminish when these conditions are not
met, the prior for ğœƒ0 effectively compensates for this situation as
demonstrated in Section 5.2.3.
Hyperparameters. Our method includes two hyperparameters,
ğ›¼ğ‘¥ğ‘¦and ğ›½0, which may pose challenges in tuning across different
usage scenarios. Fortunately, we have found that using consistent
hyperparameters across all benchmarks can still yield significant
improvements in our experimental scenarios. The tuning of hyper-
parameters for specific applications, potentially using a validation
set to optimize them, remains an area for future research.
Theoretical results. To derive a closed form of the probabilities,
we used the Law of Large Numbers to examine the scenarios where
ğ‘and ğ‘€are sufficiently large. Besides, in Lemma 4.2, we focus on a


--- Page 11 ---
B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests
ASE â€™24, October 27-November 1, 2024, Sacramento, CA, USA
single incorrect consensus set and neglect the complex interactions
of multiple incorrect sets for computational convenience. Despite
these simplifications, the key insights from these theorems are
empirically validated in Section 5, thus we believe these theoretical
analyses remain valuable. Finally, whether an error probability of
B4 can be explicitly provided, similar to those of existing heuristics
provided in Section 4, is an interesting open question.
6.2
Threats to Validity
The used benchmarks, i.e., HumanEval, MBPP, and APPS, con-
sist of small-scale function-level tasks and may not capture the
nuances of more complex scenarios in practice. Additionally, some
ground-truth test suites used to evaluate the solutionâ€™s correctness
in the benchmarks are just an approximation to the specification
and can be incomplete. This leads to a few correct solutions (i.e.,
the solutions passing the ground truth test suite) not exhibiting
identical functionality and violating Assumption 1. Considering
that such cases are relatively rare and most related work is centered
on these benchmarks [5, 7, 13, 21, 34], we believe this threat will
not significantly influence our conclusions.
Our experiments focus on Python code generation tasks, which
may not reflect the effectiveness of our method on other program-
ming languages and other software engineering (SE) generation
tasks. However, Python is one of the most popular programming
languages and code generation is a challenging and important SE
generation task. In addition, our method is language-agnostic and
our theoretical framework can be easily adapted to other SE gen-
eration tasks, such as Automated Program Repair (APR) and code
translation. Therefore, we believe this threat is limited.
7
RELATED WORK
Reranking and selection for plausible solutions. Using external
validators (e.g., test cases) to assess, rerank, or select the generated
solutions is widely used in various software engineering tasks. In
code generation, Lahiri et al. [18] incorporated user feedback to
choose test cases for code selection. In APR, Yang et al. [46] used test
cases generated by fuzz testing to validate automatically generated
patches. In code translation, Roziere et al. [33] leveraged EvoSuite
[12] to automatically generate test cases for filtering out invalid
translations. These methods are developed by assuming that the
validators are reliable and can be reduced to the MaxPass strategy
in our work. However, it may be ineffective when the validators
are plausible, as evidenced in Section 4. In code generation, several
cluster-based strategies are proposed to leverage incomplete or
plausible test cases to rerank LLM-generated code solutions [5, 22,
36]. Li et al. [22], Shi et al. [36] and Chen et al. [5] clustered code
solutions based on their test results and scored each with the cluster
capacity. These cluster-based heuristics, particularly CodeT [5], can
work well when the test cases are plausible but are susceptible to
the incorrectness of solutions as in Section 4.
Some research uses deep learning techniques for ranking LLM-
generated code snippets without executable test cases. Inala et
al. [17] introduced a neural ranker for predicting the validity of a
sampled program. Chen et al. [7] and Zhang et al. [49] leveraged
the LLM likelihood of the generated program for selecting the
most probable code snippets. These strategies fall beyond the scope
of this work since the problem we tackle does not assume the
existence of additional training data or the ranking scores produced
by the generation techniques. However, it is an interesting question
whether these strategies have a theoretical guarantee.
Code generation. Code generation is an important task in software
engineering, aimed at automating the production of code from
defined software requirements [23]. Traditional techniques rely on
predefined rules, templates, or configuration data to automate the
process [14, 42], and often struggle with flexibility across different
projects. Due to the impressive success of large language models
(LLMs), recent studies focus on training LLMs on extensive code
corpora to tackle complex code generation challenges [48]. Many
code LLMs have shown remarkable capabilities in this domain, such
as Codex [7], CodeGen [29], StarCoder [21], CodeLlama [34] and
DeepSeek-Coder [13]. This paper focuses on assessing the code
solutions generated by a code generation approach with plausible
test cases, and is thus orthogonal to these techniques.
Test case generation. Developing and maintaining human-crafted
test cases can be expensive. Many techniques have been proposed
to automatically generate test cases. Traditional approaches include
search-based [15, 20, 24], constrained-based [44], and probability-
based techniques [30]. Although most of these approaches achieve
satisfactory correctness, they are constrained by inadequate cov-
erage and poor readability, and are typically limited to generating
only regression oracles [45] or implicit oracles [3]. Recently, ap-
plying deep learning models (e.g., LLMs) to generate test cases has
become popular [1, 8, 9, 25â€“28, 32, 35, 39, 40, 47]. However, ensuring
the correctness and reliability of these generated test cases remains
difficult. This paper explores the challenging problem of employing
such plausible test cases for selecting plausible code solutions.
8
CONCLUSION AND FUTURE WORK
In this study, we introduce a systematic framework to derive an
optimal strategy for assessing and selecting plausible code solu-
tions using plausible test cases. We then develop a novel approach
that approximates this optimal strategy with an error bound and
tailors it for code generation tasks. By theoretical analysis, we show
that existing heuristics are suboptimal. Our strategy substantially
outperforms existing heuristics in several real-world benchmarks.
Future work could explore adapting our framework to other gen-
eration tasks in software engineering, such as automatic program
repair and code translation. Also, the effectiveness of our proposed
priors in these contexts, as well as the potential for alternative
priors, remains an open question.
Our online appendix is available on Zenodo [6].
ACKNOWLEDGMENTS
This research is supported by the National Natural Science Foun-
dation of China (No. 62202420) and the Software Engineering Appli-
cation Technology Lab at Huawei under the Contract TC20231108060.
Zhongxin Liu gratefully acknowledges the support of Zhejiang
University Education Foundation Qizhen Scholar Foundation. We
would also like to thank Yihua Sun for inspiring the incorporation
of prior knowledge and for proofreading the manuscript, as well as
Zinan Zhao and Junlin Chen for their discussions on the theory.


--- Page 12 ---
ASE â€™24, October 27-November 1, 2024, Sacramento, CA, USA
Mouxiang Chen, Zhongxin Liu, He Tao, Yusu Hong, David Lo, Xin Xia, and Jianling Sun
REFERENCES
[1] Saranya
Alagarsamy,
Chakkrit
Tantithamthavorn,
and
Aldeida
Aleti.
2023.
A3Test: Assertion-Augmented Automated Test Case Generation.
arXiv:2302.10352 [cs.SE]
[2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk
Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le,
and Charles Sutton. 2021. Program Synthesis with Large Language Models.
arXiv:2108.07732 [cs.PL]
[3] Earl T Barr, Mark Harman, Phil McMinn, Muzammil Shahbaz, and Shin Yoo. 2014.
The oracle problem in software testing: A survey. IEEE transactions on software
engineering 41, 5 (2014), 507â€“525.
[4] Thomas Bayes. 1763. LII. An essay towards solving a problem in the doctrine of
chances. By the late Rev. Mr. Bayes, FRS communicated by Mr. Price, in a letter
to John Canton, AMFR S. Philosophical transactions of the Royal Society of London
53 (1763), 370â€“418.
[5] Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang
Lou, and Weizhu Chen. 2023. CodeT: Code Generation with Generated Tests.
In The Eleventh International Conference on Learning Representations, ICLR 2023,
Kigali, Rwanda, May 1-5, 2023. OpenReview.net. https://openreview.net/pdf?id=
ktrw68Cmu9c
[6] Mouxiang Chen, Zhongxin Liu, He Tao, Yusu Hong, David Lo, Xin Xia, and
Jianling Sun. 2024. B4: Towards Optimal Assessment of Plausible Code Solutions
with Plausible Tests. https://doi.org/10.5281/zenodo.13737381
[7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de
Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail
Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo-
tios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex
Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shan-
tanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh
Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei,
Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large
Language Models Trained on Code. arXiv:2107.03374 [cs.LG]
[8] Yinghao Chen, Zehao Hu, Chen Zhi, Junxiao Han, Shuiguang Deng, and Jian-
wei Yin. 2024. ChatUniTest: A Framework for LLM-Based Test Generation.
arXiv:2305.04764 [cs.SE]
[9] Arghavan Moradi Dakhel, Amin Nikanjam, Vahid Majdinasab, Foutse Khomh,
and Michel C. Desmarais. 2023. Effective Test Generation Using Pre-trained
Large Language Models and Mutation Testing. arXiv:2308.16557 [cs.SE]
[10] Philip J Davis. 1972. Gamma function and related functions. Handbook of
mathematical functions 256 (1972).
[11] Morris H DeGroot. 2005. Optimal statistical decisions. John Wiley & Sons.
[12] Gordon Fraser and Andrea Arcuri. 2011. Evosuite: automatic test suite generation
for object-oriented software. In Proceedings of the 19th ACM SIGSOFT symposium
and the 13th European conference on Foundations of software engineering. 416â€“419.
[13] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang,
Guanting Chen, Xiao Bi, Y Wu, YK Li, et al. 2024. DeepSeek-Coder: When the
Large Language Model Meets Programmingâ€“The Rise of Code Intelligence. arXiv
preprint arXiv:2401.14196 (2024).
[14] Nicolas Halbwachs, Pascal Raymond, and Christophe Ratel. 1991. Generating ef-
ficient code from data-flow programs. In Programming Language Implementation
and Logic Programming: 3rd International Symposium, PLILPâ€™91 Passau, Germany,
August 26â€“28, 1991 Proceedings 3. Springer, 207â€“218.
[15] Mark Harman and Phil McMinn. 2010. A Theoretical and Empirical Study of
Search-Based Testing: Local, Global, and Hybrid Search. IEEE Transactions on
Software Engineering 36, 2 (2010), 226â€“247. https://doi.org/10.1109/TSE.2009.71
[16] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora,
Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob
Steinhardt. 2021. Measuring Coding Challenge Competence With APPS. In
Thirty-fifth Conference on Neural Information Processing Systems Datasets and
Benchmarks Track (Round 2). https://openreview.net/forum?id=sD93GOzH3i5
[17] Jeevana Priya Inala, Chenglong Wang, Mei Yang, Andres Codas, Mark
EncarnaciÃ³n, Shuvendu Lahiri, Madanlal Musuvathi, and Jianfeng Gao.
2022.
Fault-Aware Neural Code Rankers. In Advances in Neural Infor-
mation Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Bel-
grave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc.,
13419â€“13432.
https://proceedings.neurips.cc/paper_files/paper/2022/file/
5762c579d09811b7639be2389b3d07be-Paper-Conference.pdf
[18] Shuvendu K. Lahiri, Sarah Fakhoury, Aaditya Naik, Georgios Sakkas, Saikat
Chakraborty, Madanlal Musuvathi, Piali Choudhury, Curtis von Veh, Jee-
vana Priya Inala, Chenglong Wang, and Jianfeng Gao. 2023. Interactive Code
Generation via Test-Driven User-Intent Formalization. arXiv:2208.05950 [cs.SE]
[19] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven
Chu Hong Hoi. 2022. Coderl: Mastering code generation through pretrained
models and deep reinforcement learning. Advances in Neural Information Pro-
cessing Systems 35 (2022), 21314â€“21328.
[20] Caroline Lemieux, Jeevana Priya Inala, Shuvendu K. Lahiri, and Siddhartha Sen.
2023. CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-trained
Large Language Models. In 2023 IEEE/ACM 45th International Conference on Soft-
ware Engineering (ICSE). 919â€“931. https://doi.org/10.1109/ICSE48619.2023.00085
[21] Raymond Li, Loubna Ben allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,
Chenghao Mou, Marc Marone, Christopher Akiki, Jia LI, Jenny Chim, Qian
Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene,
Joel Lamy-Poirier, Joao Monteiro, Nicolas Gontier, Ming-Ho Yee, Logesh Kumar
Umapathi, Jian Zhu, Ben Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra
Murthy, Jason T Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca,
Manan Dey, Zhihan Zhang, Urvashi Bhattacharyya, Wenhao Yu, Sasha Luccioni,
Paulo Villegas, Fedor Zhdanov, Tony Lee, Nadav Timor, Jennifer Ding, Claire S
Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu,
Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy,
Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos MuÃ±oz Ferrandis, Sean
Hughes, Thomas Wolf, Arjun Guha, Leandro Von Werra, and Harm de Vries.
2023. StarCoder: may the source be with you! Transactions on Machine Learning
Research (2023). https://openreview.net/forum?id=KoFOg41haE Reproducibility
Certification.
[22] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, RÃ©mi
Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas
Hubert, Peter Choy, Cyprien de Masson dâ€™Autume, Igor Babuschkin, Xinyun Chen,
Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy,
Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas,
Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-level code generation
with AlphaCode. Science 378, 6624 (2022), 1092â€“1097. https://doi.org/10.1126/
science.abq1158 arXiv:https://www.science.org/doi/pdf/10.1126/science.abq1158
[23] Hui Liu, Mingzhu Shen, Jiaqi Zhu, Nan Niu, Ge Li, and Lu Zhang. 2022. Deep
Learning Based Program Generation From Requirements Text: Are We There
Yet? IEEE Transactions on Software Engineering 48, 4 (2022), 1268â€“1289. https:
//doi.org/10.1109/TSE.2020.3018481
[24] Stephan Lukasczyk and Gordon Fraser. 2022. Pynguin: Automated unit test gen-
eration for python. In Proceedings of the ACM/IEEE 44th International Conference
on Software Engineering: Companion Proceedings. 168â€“172.
[25] Antonio Mastropaolo, Nathan Cooper, David Nader Palacio, Simone Scalabrino,
Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2023. Using Transfer
Learning for Code-Related Tasks. IEEE Transactions on Software Engineering 49,
4 (2023), 1580â€“1598. https://doi.org/10.1109/TSE.2022.3183297
[26] Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader Palacio,
Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Studying the
Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks.
In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE).
336â€“347. https://doi.org/10.1109/ICSE43902.2021.00041
[27] Noor Nashid, Mifta Sintaha, and Ali Mesbah. 2023. Retrieval-Based Prompt Selec-
tion for Code-Related Few-Shot Learning. In 2023 IEEE/ACM 45th International
Conference on Software Engineering (ICSE). 2450â€“2462. https://doi.org/10.1109/
ICSE48619.2023.00205
[28] Pengyu Nie, Rahul Banerjee, Junyi Jessy Li, Raymond J Mooney, and Milos
Gligoric. 2023. Learning deep semantics for test completion. In 2023 IEEE/ACM
45th International Conference on Software Engineering (ICSE). IEEE, 2111â€“2123.
[29] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou,
Silvio Savarese, and Caiming Xiong. 2023. CodeGen: An Open Large Language
Model for Code with Multi-Turn Program Synthesis. In The Eleventh Interna-
tional Conference on Learning Representations. https://openreview.net/forum?id=
iaYcJKpY2B_
[30] Carlos Pacheco, Shuvendu K. Lahiri, Michael D. Ernst, and Thomas Ball. 2007.
Feedback-Directed Random Test Generation. In 29th International Conference on
Software Engineering (ICSEâ€™07). 75â€“84. https://doi.org/10.1109/ICSE.2007.37
[31] Howard Raiffa and Robert Schlaifer. 2000. Applied statistical decision theory.
Vol. 78. John Wiley & Sons.
[32] Nikitha Rao, Kush Jain, Uri Alon, Claire Le Goues, and Vincent J. Hellendoorn.
2023. CAT-LM Training Language Models on Aligned Code And Tests. In 2023
38th IEEE/ACM International Conference on Automated Software Engineering (ASE).
409â€“420. https://doi.org/10.1109/ASE56229.2023.00193
[33] Baptiste Roziere, Jie Zhang, Francois Charton, Mark Harman, Gabriel Synnaeve,
and Guillaume Lample. 2022. Leveraging Automated Unit Tests for Unsupervised
Code Translation. In International Conference on Learning Representations. https:
//openreview.net/forum?id=cmt-6KtR4c4
[34] Baptiste RoziÃ¨re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiao-
qing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, JÃ©rÃ©my
Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cris-
tian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre DÃ©fossez, Jade
Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas
Scialom, and Gabriel Synnaeve. 2024. Code Llama: Open Foundation Models for
Code. arXiv:2308.12950 [cs.CL]
[35] Max SchÃ¤fer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. 2024. An Empirical
Evaluation of Using Large Language Models for Automated Unit Test Generation.


--- Page 13 ---
B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests
ASE â€™24, October 27-November 1, 2024, Sacramento, CA, USA
IEEE Transactions on Software Engineering 50, 1 (2024), 85â€“105. https://doi.org/
10.1109/TSE.2023.3334955
[36] Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I Wang.
2022. Natural Language to Code Translation with Execution. In Proceedings of the
2022 Conference on Empirical Methods in Natural Language Processing. 3533â€“3546.
[37] Å½eljka Stojanac, Daniel Suess, and Martin Kliesch. 2017. On products of Gaussian
random variables. arXiv preprint arXiv:1711.10516 (2017).
[38] A.B. Tsybakov. 2008. Introduction to Nonparametric Estimation. Springer New
York. https://books.google.com.hk/books?id=mwB8rUBsbqoC
[39] Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel
Sundaresan. 2021. Unit Test Case Generation with Transformers and Focal
Context. arXiv:2009.05617 [cs.SE]
[40] Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, and Neel Sundaresan. 2022.
Generating accurate assert statements for unit test cases using pretrained trans-
formers. In Proceedings of the 3rd ACM/IEEE International Conference on Automa-
tion of Software Test (AST â€™22). ACM. https://doi.org/10.1145/3524481.3527220
[41] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler
Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser,
Jonathan Bright, StÃ©fan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jar-
rod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern,
Eric Larson, C J Carey, Ä°lhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas,
Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,
Charles R. Harris, Anne M. Archibald, AntÃ´nio H. Ribeiro, Fabian Pedregosa,
Paul van Mulbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental Al-
gorithms for Scientific Computing in Python. Nature Methods 17 (2020), 261â€“272.
https://doi.org/10.1038/s41592-019-0686-2
[42] Michael W Whalen. 2000. High-integrity code generation for state-based for-
malisms. In Proceedings of the 22nd international conference on Software engineer-
ing. 725â€“727.
[43] Frank Wilcoxon. 1992. Individual comparisons by ranking methods. In Break-
throughs in statistics: Methodology and distribution. Springer, 196â€“202.
[44] Xusheng Xiao, Sihan Li, Tao Xie, and Nikolai Tillmann. 2013. Characteristic
studies of loop problems for structural test generation via symbolic execution. In
2013 28th IEEE/ACM International Conference on Automated Software Engineering
(ASE). 246â€“256. https://doi.org/10.1109/ASE.2013.6693084
[45] Tao Xie. 2006. Augmenting automatically generated unit-test suites with regres-
sion oracle checking. In European Conference on Object-Oriented Programming.
Springer, 380â€“403.
[46] Jinqiu Yang, Alexey Zhikhartsev, Yuefei Liu, and Lin Tan. 2017. Better test cases
for better automated program repair. In Proceedings of the 2017 11th joint meeting
on foundations of software engineering. 831â€“841.
[47] Zhiqiang Yuan, Yiling Lou, Mingwei Liu, Shiji Ding, Kaixin Wang, Yixuan Chen,
and Xin Peng. 2024. No More Manual Tests? Evaluating and Improving ChatGPT
for Unit Test Generation. arXiv:2305.04207 [cs.SE]
[48] Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan,
Wang Yongji, and Jian-Guang Lou. 2023. Large Language Models Meet NL2Code:
A Survey. In Proceedings of the 61st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan Boyd-Graber,
and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto,
Canada, 7443â€“7464. https://doi.org/10.18653/v1/2023.acl-long.411
[49] Tianyi Zhang, Tao Yu, Tatsunori Hashimoto, Mike Lewis, Wen-Tau Yih, Daniel
Fried, and Sida Wang. 2023. Coder Reviewer Reranking for Code Generation. In
Proceedings of the 40th International Conference on Machine Learning (Proceed-
ings of Machine Learning Research, Vol. 202), Andreas Krause, Emma Brunskill,
Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.).
PMLR, 41832â€“41846. https://proceedings.mlr.press/v202/zhang23av.html
