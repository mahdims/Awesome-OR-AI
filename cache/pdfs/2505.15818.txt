--- Page 1 ---
InstructSAM: A Training-Free Framework for
Instruction-Oriented Remote Sensing Object
Recognition
Yijie Zheng, Weijie Wu
Aerospace Information Research Institute,
Chinese Academy of Sciences
University of Chinese Academy of Sciences
{zhengyijie23,wuweijie25}@mails.ucas.ac.cn
Qingyun Li
Harbin Institute of Technology
21b905003@stu.hit.edu.cn
Xuehui Wang
Shanghai Jiao Tong University
wangxuehui@sjtu.edu.cn
Xu Zhou, Aiai Ren, Jun Shen
University of Wollongong
{xz572,ar243,jshen}@uowmail.edu.au
Long Zhao, Guoqing Li#
Aerospace Information Research Institute,
Chinese Academy of Sciences
{zhaolong,ligq}@aircas.ac.cn
Xue Yang
SAIS, Shanghai Jiao Tong University
yangxue-2019-sjtu@sjtu.edu.cn
Abstract
Language-guided object recognition in remote sensing imagery is crucial for large-
scale mapping and automated data annotation. However, existing open-vocabulary
and visual grounding methods rely on explicit category cues, limiting their ability
to handle complex or implicit queries that require advanced reasoning. To address
this issue, we introduce a new suite of tasks, including Instruction-Oriented Object
Counting, Detection, and Segmentation (InstructCDS), covering open-vocabulary,
open-ended, and open-subclass scenarios. We further present EarthInstruct, the
first InstructCDS benchmark for earth observation. It is constructed from two
diverse remote sensing datasets with varying spatial resolutions and annotation
rules across 20 categories, necessitating models to interpret dataset-specific in-
structions. Given the scarcity of semantically rich labeled data in remote sensing,
we propose InstructSAM, a training-free framework for instruction-driven object
recognition. InstructSAM leverages large vision-language models to interpret
user instructions and estimate object counts, employs SAM2 for mask proposal,
and formulates mask-label assignment as a binary integer programming prob-
lem. By integrating semantic similarity with counting constraints, InstructSAM
efficiently assigns categories to predicted masks without relying on confidence
thresholds. Experiments demonstrate that InstructSAM matches or surpasses spe-
cialized baselines across multiple tasks while maintaining near-constant inference
time regardless of object count, reducing output tokens by 89% and overall runtime
by over 32% compared to direct generation approaches. We believe the contribu-
tions of the proposed tasks, benchmark, and effective approach will advance future
research in developing versatile object recognition systems. The code is available
at https://VoyagerXvoyagerx.github.io/InstructSAM/.
39th Conference on Neural Information Processing Systems (NeurIPS 2025).
arXiv:2505.15818v2  [cs.CV]  13 Oct 2025


--- Page 2 ---
(a) Close-set
(b) Open-vocabulary
(c) Open-ended
(d) Open-subclass
Figure 1: Comparison of four task settings in object detection and segmentation. The sample is
from DIOR Dataset [33] (a) Close-set: Standard annotation with categories defined in the DIOR
dataset. (b) Open-vocabulary: Instruction specifies which categories to detect and segment (e.g.,
â€œsoccer fieldâ€, â€œfootball fieldâ€, â€œparking lotâ€). (c) Open-ended: Instruction requires detection and
segmentation of all visible objects without category specification. (d) Open-subclass: Instruction
targets all objects within a super-category (e.g., â€œsports fieldsâ€).
1
Introduction
Object recognition in remote sensing imagery captures a vast array of objects and phenomena
across diverse environments, providing rich information for supporting achieving the Sustainable
Development Goals issued by the United Nations [56, 3], such as wildlife monitoring [78, 91],
poverty estimation [1, 48], and disaster response [18]. The recent advent of powerful vision-language
models (VLMs), such as CLIP [58], has ushered in a new era of remote sensing oriented open-
vocabulary object recognition algorithms (e.g. detection [37, 54] and segmentation [21]). However,
existing open-vocabulary approaches predominantly rely on explicit category cues, which restrict
their capability to handle complex or implicit queries that demand advanced reasoning and contextual
understanding. In other words, the rich diversity of visible objects in remote sensing images, due to
the birdâ€™s-eye view, predicates that any predefined, fixed list of categories is inevitably incomplete,
limiting its practicality for open-ended analysis in the real world.
To address this issue, we expand the instruction-oriented object detection task [57] and intro-
duce a novel suite of tasksâ€”Instruction-Oriented Object Counting, Detection, and Segmentation
(InstructCDS) that encompasses open-vocabulary, open-ended, and open-subclass settings, as illus-
trated in Figure 1. The InstructCDS task entails a more flexible and scalable interpretation beyond
fixed category sets and comprehends the complex usersâ€™ task requirements. We further present Earth-
Instruct, the first benchmark for InstructCDS in earth observation. The benchmark is constructed
from two generic remote sensing object datasets covering 20 categories with different annotation
rules and spatial resolution. EarthInstruct guides models to comprehend complex user instructions
beyond the predefined three settings.
Recent advancements in VLMs have demonstrated impressive performance in object detection [44, 39,
57? ], semantic segmentation [85, 11? ], visual grounding [2, 7] and reasoning-based segmentation
[63, 29] within the natural image domain. However, transferring these methods to remote sensing
imagery presents several challenges. First, direct inference leads to significant accuracy degradation
due to the domain gap between natural and aerial images [14]. Second, most existing remote sensing
open-vocabulary detection [37, 38, 86? ] and segmentation [87, 84, 21] methods are trained on
datasets with only a limited number of categories, restricting their generalization to diverse unseen
categories. Third, conventional detectors [34, 10, 49] rely on a threshold to filter predicted bounding
boxes, which is not obtainable in zero-shot scenarios.
To tackle these challenges, we decompose the instruction-oriented object detection and segmentation
tasks into several tractable steps and propose a framework without task-specific training, Instruct-
SAM. First, a large vision-language model (LVLM) is employed to interpret user instructions and
predict object categories and counts, with prompts systematically designed to maximize modelâ€™s
capability. In parallel, SAM2 [60] is utilized to automatically generate mask proposals. Next,
a CLIP model pre-trained on remote sensing images computes the semantic similarity between
the predicted objects categories and mask proposals. We then formulate the object detection and
segmentation as a mask-label matching problem, assigning predicted categories to mask proposals
2


--- Page 3 ---
by integrating semantic similarity and global counting constraints. By inherently integrating three
powerful foundation models, InstructSAM achieves superior performance across multiple tasks
compared to both generic and remote sensing-specific VLMs trained on large-scale object recognition
data. Notably, the inference time of InstructSAM remains nearly constant with respect to the number
of predicted objects, reducing output tokens by 89% and overall runtime by 32% compared to directly
generating bounding boxes using Qwen2.5-VL [2] in the open-ended setting. Our work paves the
way for scalable, instruction-driven remote sensing object detection and segmentation, eliminating
the need for costly pre-training or manual threshold tuning. Furthermore, the training-free paradigm
allows InstructSAM to recognize objects in natural images when equipped with generic CLIP models.
In summary, our contributions are as follows:
â€¢ We introduce the InstructCDS task, which challenges models to interpret user-provided natural
language instructions and infer the number and locations of relevant objects.
â€¢ We construct EarthInstruct to benchmark InstructCDS in earth observation, covering open-
vocabulary, open-ended, and open-subclass settings and counting, detection, and segmentation
tasks.
â€¢ We develop InstructSAM, a training-free and confidence-free framework achieving near-constant
inference time for the InstructCDS task.
â€¢ Experiments on the benchmarks demonstrate that InstructSAM matches close-set models in object
counting and surpasses generic and remote sensing-specific models in open-vocabulary and open-
ended object recognition.
2
Related work
2.1
Instruction-oriented object detection and segmentation
Instruction-oriented methods
Instruction-oriented object detection (IOD), first introduced in [57],
includes four instruction settings: category-specified (open-vocabulary [76]) detection, detection
of all objects (open-ended [39]), detection within a super-category (we name it open-subclass),
and detection to achieve certain goals. Ins-DetCLIP [57] trains a detector to identify foreground
objects and passes their features to a large language model (LLM) to generate categories based on
user instructions. In addition to models specifically designed for the IOD task, Qwen2.5-VL [2],
trained on multi-task instruction data, also showcases the ability of dense object detection. However,
both approaches require extensive task-specific training data, and their inference times increase
substantially with the number of objects.
Open-vocabulary methods
Large-vocabulary object detection and segmentation datasets [67, 17,
72] and visual grounding datasets [27, 23] enable various open-vocabulary learning approaches,
including knowledge distillation [16] and region-text pre-training [26, 50, 44, 34]. Self-training with
confidence threshold-filtered pseudo labels from image-text pairs (e.g., CC3M [5], WebLI2B [6])
further boosts the performance [49, 44, 89, 83]. However, the quality of pseudo labels is highly
sensitive to the chosen threshold [49], and these methods require predefined object categories, limiting
their flexibility in diverse scenarios.
Open-ended methods
GenerateU [39] first formulates the open-ended object detection (OED)
problem. Concurrent works such as DetCLIPv3 [83], Florence-2 [79], and DINO-X [62] introduce
generative frameworks that jointly predict object categories and bounding boxes using language
models. However, constructing large-scale datasets with bounding box and caption pairs is resource-
intensive. VL-SAM [40] proposes a training-free approach via attention as prompts, but its iterative
mask refinement and multi-prompt ensemble strategies are computationally expensive.
2.2
Instruction-oriented remote sensing object detection and segmentation
Recent advances in VLMs [58, 43] have also enabled open-vocabulary learning in the remote sensing
domain. Diverse semantic tags from OpenStreetMap [19] and those generated by LVLMs drive the
development of contrastive language-image pre-training in remote sensing images [90, 73]. Following
the generic open-vocabulary learning frameworks, methods for remote sensing open-vocabulary
3


--- Page 4 ---
detection [37, 75, 54, 22] and segmentation [87, 84, 21, 31] methods emerge. However, their human-
annotated training data, being limited to a few dozen categories [33, 74, 69? ], hinder generalization
to out-of-distribution or zero-shot scenarios. Although some remote sensing LVLMs could support
object recognition tasks like single-class object detection [28, 47, 25], visual grounding [52, 55],
referring expression segmentation [92, 53], grounded conversation generation [65], and scene graph
generation [47], they failed to follow complex reasoning instructions, such as open-vocabulary and
open-subclass object detection. To annotate vast-vocabulary training data for remote sensing object
detection, LAE-Label [54] employs a generic LVLM [8] to predict categories for cropped mask
proposals. However, this approach loses global context for accurate category classification.
In contrast, our InstructSAM adopts a confidence-free paradigm, requires no task-specific pre-training
or finetuning, and maintains near-constant inference time regardless of object counts.
(a)
(b)
(c)
(d)
Figure 2: Annotation samples from NWPU-VHR-10 (a) and DIOR (b-d) illustrating rules that might
differ from common sense. (a) Ships and vehicles are unannotated in low-resolution images (NWPU-
VHR-10). (b) Expressway service areas separated by a road are treated as distinct instances (DIOR).
(c, d) Airports are annotated only when fully visible (DIOR).
3
EarthInstruct, a benchmark for InstructCDS in remote sensing
3.1
Instruction setup
To enable practical applications in remote sensing such as large-scale mapping [1] and image
annotation, we define three distinct settings for InstructCDS:
1) Open-vocabulary: Counting, detection, and segmentation with user-specified categories.
2) Open-ended: Counting, detection, and segmentation of all objects without specifying categories.
3) Open-subclass: Counting, detection, and segmentation of objects within a super-category.
We construct EarthInstruct using NWPU-VHR-10 [9] and DIOR [33] datasets, selected for their
widespread use and diverse sensors, resolutions, and annotation rules. Critically, these dataset-
specific annotation rules might deviate from common sense (e.g., excluding low-resolution vehicles)
or exhibit semantic ambiguities (e.g., â€œbridgeâ€ vs. â€œoverpassâ€), reflecting the original annotatorsâ€™
specific goals (Figure 2). Consequently, simple instructions like â€œcount vehiclesâ€ would fail to capture
the nuances required by dataset conventions or user intent. EarthInstruct therefore necessitates models
to interpret detailed instructions that clarify target definitions and handle dataset-specific rules (e.g.,
â€œdo not count vehicles in images with a spatial resolution lower than 1m.â€). To ensure fair evaluation
aligned with dataset conventions and user requirements, prompts are designed accordingly, but
image-specific prompts are prohibited to maintain scalability for large-area applications where prior
content knowledge for each image is unavailable.
3.2
Evaluation metrics
Multi-class object counting
Standard counting metrics, such as Mean Absolute Error (MAE)
and Root Mean Squared Error (RMSE), used in benchmarks like FSC-147 [59] and RSOC [15],
inadequately capture nuanced multi-class evaluation. They cannot distinguish between over-counting
and under-counting errors. Additionally, being unnormalized, they allow categories with larger counts
to disproportionately skew the overall score when being averaged across classes.
4


--- Page 5 ---
To address these issues, we adopt precision, recall, and F1-score, whereby offering normalized,
per-class insights. We define per-image, per-class counting components as follows: let Cgt denote the
ground truth count and Cpred the predicted count for a class in an image. Then, True Positives (TP) =
min(Cgt, Cpred), False Positives (FP) = max(0, Cpred âˆ’Cgt) for over-counting, and False Negatives
(FN) = max(0, Cgt âˆ’Cpred) for under-counting. These definitions enable standard calculation of
precision, recall, and F1-score, aggregated across images per class and then averaged for a final score.
Rethinking metrics for confidence-free detectors
Evaluating generative models like Florence-
2 [79] or Qwen2.5-VL [2], which output detections without confidence scores, poses a challenge
for standard metrics. Average Precision (AP) [12] relies on confidence scores to rank predictions
and generate precision-recall curves. Without such ranking, standard AP is ill-defined. Furthermore,
practical applications often involve filtering predictions with a fixed threshold, treating all remaining
detections equally [1, 49].
To resolve these issues and ensure fair comparison, we adopt confidence-free metrics: mean F1-score
(mF1) and mean Average Precision with no confidence (mAPnc) [35]. mF1 measures performance
at a single operating point, suitable for fixed-threshold deployment. mAPnc adapts AP by assigning
maximum confidence to all predictions. For confidence-free models, these metrics are computed
directly (Results in Table 10). For conventional detectors (e.g., [61, 4]) that provide scores, when the
confidence threshold is swept from 0 to 1 (step 0.02), the threshold maximizing mF1 (using an IoU
threshold of 0.5) across categories is selected, and the corresponding cusp score is reported.
Evaluating open-ended and open-subclass settings
In open-ended and open-subclass settings,
LVLMs may generate category names (e.g., â€œcarâ€) that differ textually from ground truth labels (e.g.,
â€œvehicleâ€). To handle this synonymy during evaluation, we adopt semantic similarity matching by
following established protocols [39, 57]. Specifically, we encode generated categories and ground
truth categories using the GeoRSCLIP [90] text encoder with the template â€œa satellite image
of a {category}â€. A generated category name is deemed equivalent to a ground truth category if
their embedding cosine similarity exceeds 0.95. This allows predicted objects associated with the
generated name to be accurately evaluated against the matched ground truth category.
4
InstructSAM
Addressing the challenges of instruction-following, domain gaps, and threshold sensitivity in remote
sensing object recognition, we propose a training-free framework named InstructSAM. It decomposes
InstructCDS into three synergistic steps: instruction-based object counting using an LVLM, class-
agnostic mask proposing via SAM2, and a novel counting-constrained mask-label matching procedure.
This approach avoids the costly model training and threshold tuning, offering efficient and robust
performance.
4.1
Instruction-oriented object counting with LVLM
As illustrated in Section 3, accurately interpreting user intent in remote sensing requires handling
dataset-specific rules and semantic ambiguities, which the simple category prompts could fail to
capture. We leverage state-of-the-art LVLMs (e.g., GPT-4o [24], Qwen2.5-VL [2]) for this task.
Inspired by [20], we utilize structured prompts in JSON format, which allows easy integration of
dataset-specific Instructions alongside the core Task (detailed in Appendix C). Given an image I
and a detailed prompt P, the LVLM acts as a counter, outputting the target categories {catj} and their
corresponding counts {numj} present in the image: {catj, numj}M
j=1 = LVLM-Counter(I, P).
4.2
Class-agnostic mask proposing
Concurrent with counting, SAM2 [60] is employed to generate high-quality, class-agnostic object
masks for its strong generalization to remote sensing imagery [71, 77]. Using its automated mask
generation mode prompted by a regular point grid, we obtain a dense set of masks proposals
{maski}N
i=1. To enhance recall for small objects, mask generation is also applied to image crops
(detailed in Appendix C.4).
5


--- Page 6 ---
Input Image 
Mask 
Proposals
BIP Solver
Structured 
Counting Prompt
{ğ‘šğ‘ğ‘ ğ‘˜ğ‘–}ğ‘–=1
ğ‘
ğ‘šğ‘ğ‘ ğ‘˜ğ‘–, ğ‘ğ‘ğ‘¡ğ‘—âˆ£ğ‘¥ğ‘–ğ‘—=  1
Matched Mask-Label Pairs
Mask 
Proposer
LVLM 
Counter
Visual Information
Semantic Information
ğ‘†âˆˆâ„ğ‘Ã—ğ‘€
Quantitative Information
CLIP
Text
Encoder 
ğ‘ 11 ğ‘ 12 ğ‘ 13
ğ‘†21 ğ‘†22 ğ‘†23
ğ‘†31 ğ‘†32 ğ‘†33
ğ‘†41 ğ‘†42 ğ‘†43
Scaled
Crop
Object
Counts
CLIP
Image
Encoder 
{ğ‘›ğ‘¢ğ‘šğ‘—}ğ‘—=1
ğ‘€
Counting Constraints
1
track field
1
parking lot
2
baseball field
â€¦
{ğ‘£ğ‘–}ğ‘–=1
ğ‘
{ğ‘¡ğ‘—}ğ‘—=1
ğ‘€
min
ğ‘‹à·
ğ‘–,ğ‘—
1 âˆ’ğ‘ ğ‘–ğ‘—Â· ğ‘¥ğ‘–ğ‘—
â€¦
{ğ‘ğ‘ğ‘¡ğ‘—}ğ‘—=1
ğ‘€
Sentence 
Template
{ğ‘›ğ‘¢ğ‘šğ‘—, ğ‘ğ‘ğ‘¡ğ‘—}ğ‘—=1
ğ‘€
Figure 3: The InstructSAM framework. Given an input image and a structured counting prompt,
the LVLM Counter extracts target categories {catj} (semantic info) and their counts {numj}
(quantitative info). Concurrently, the Mask Proposer generates mask proposals {maski} (visual info).
A CLIP model computes the similarity matrix S between mask embeddings (from scaled crops) {vi}
and category embeddings {tj}. Finally, the Binary Integer Programming (BIP) Solver optimally
assigns categories to masks by maximizing summed similarity, subject to the counting constraints,
yielding the final recognition results.
4.3
Mask-label matching with counting constraints
A key innovation of InstructSAM is reframing object detection and segmentation as a constrained
mask-label matching problem, by integrating the outputs from previous steps. Rather than using
fragile confidence thresholds [10, 49], we utilize global counts {numj} derived from the LVLM to
constrain the assignment of semantic labels {catj} to visual mask proposals {maski}.
Given N mask proposals and M target categories with counts, we compute a semantic similarity ma-
trix S âˆˆRNÃ—M, where sij represents the cosine similarity between the CLIP [90] image embedding
of a patch cropped around maski (scaled by 1.2 for context) and the text embedding of catj (using the
template â€œa satellite image of a {category}â€). We then seek a binary assignment matrix
X âˆˆ{0, 1}NÃ—M, where xij = 1 assigns maski to catj, by solving the Binary Integer Programming
(BIP) problem:
min
X
N
X
i=1
M
X
j=1
(1 âˆ’sij) Â· xij
(1)
s.t.
M
X
j=1
xij â‰¤1,
âˆ€i âˆˆ{1, . . . , N}
(2)
N
X
i=1
xij = numj,
âˆ€j âˆˆ{1, . . . , M},
if N â‰¥
M
X
j=1
numj
(3)
N
X
i=1
M
X
j=1
xij = N,
if N <
M
X
j=1
numj
(4)
where constraint (2) ensures each mask is assigned to at most one category. Constraint (3) enforces
that the number of assigned masks for each category matches the counts provided by the LVLM.
Constraint (4) handles cases where the number of proposals is less than the total target count, ensuring
all proposals are assigned.
As depicted in Figure 3, this BIP formulation elegantly fuses visual information, semantic information,
and quantitative information. The visual information comes from the CLIP embeddings of mask
proposals {vi}, which contribute to sij. Semantic information is derived from the categoriesâ€™ sentence
embeddings {tj}, also contributing to sij. Quantitative information from object counts {numj}
serves as constraints in (3). The problem is efficiently solvable using open-source BIP solvers
like PuLP [51]. The resulting non-zero entries in X define the final set of recognized objects
{(maski, catj)|xij = 1}.
6


--- Page 7 ---
Table 1: Zero-shot performance comparison on EarthInstruct under open-vocabulary setting.
Method
NWPU-VHR-10
DIOR
Cnt F1
Box F1
Mask F1
IoU
Cnt F1
Box F1
Mask F1
IoU
Grounding DINO [44]
14.9
14.0
-
-
10.7
6.0
-
-
OWLv2 [49]
39.4
27.2
-
-
23.4
14.3
-
-
Qwen2.5-VL [2]
68.0
36.4
-
-
52.0
27.8
-
-
GSNet [84]
-
-
1.3
6.4
-
-
0.0
0.3
SegEarth-OV [31]
-
-
3.5
12.1
-
-
1.2
6.7
InstructSAM-Qwen
73.2
38.9
23.7
12.1
59.3
24.7
24.0
18.5
InstructSAM-GPT4o
83.0
41.8
26.1
14.8
79.9
29.1
28.1
20.2
5
Experiments
5.1
Implementation
We implement InstructSAM using GPT-4o-2024-11-20 [24] (short for InstructSAM-GPT4o) or
Qwen2.5-VL-7B [2] (short for InstructSAM-Qwen) as the LVLM counter, SAM2-hiera-large [60]
for mask proposal, and GeoRSCLIP-ViT-L [90] for similarity computation. For the open-vocabulary
setting, we follow previous works [75, 21, 86] to split base and novel classes, and report mF1, mean
Insert-over-Union (mIoU), or mAPnc. For open-subclass setting, we set two parent class â€œmeans of
transportâ€ and â€œsports fieldsâ€. We compare InstructSAM with a wide range of models, whose training
data and abilities are listed in Table 6.
5.2
Results on EarthInstruct
Open-vocabulary setting
We report mean metrics across all classes for generic methods [44, 49, 2]
and remote sensing open-vocabulary segmentation models [84, 31] trained on broader vocabularies
in Table 1. Zero-shot performance on novel classes for models trained on base classes [86, 75, 21]
appears in Table 10. Models using novel class images or trained on entire detection datasets are
evaluated on two additional datasets (Table 11).
Table 1 shows InstructSAM (esp. with GPT-4o) leading in all tasks with top counting metrics.
On novel classes (Table 10), InstructSAM-Qwen achieves superior or competitive mAPnc against
specialized models. This underscores InstructSAMâ€™s training-free, counting-constrained matching
advantage over traditional or fine-tuned approaches.
Table 2: Performance comparison on EarthInstruct under
open-ended setting.
Method
NWPU-VHR-10
DIOR
Cnt F1 Box F1 Mask F1 Cnt F1 Box F1 Mask F1
Qwen2.5-VL [2]
48.6
32.0
-
36.6
21.7
-
GPT-4o [24] + OWL [49]
32.6
24.0
-
30.6
21.0
-
SkySenseGPT [47]
34.9
1.7
-
39.2
6.5
-
EarthDial [? ]
30.0
5.6
-
47.6
22.8
-
LAE-Label [54]
46.2
27.3
24.1
23.3
11.5
10.4
GeoPixel [65]
40.8
29.9
29.1
21.4
13.8
14.7
InstructSAM-Qwen
55.2
29.6
28.5
33.2
14.3
14.4
InstructSAM-GPT4o
57.4
31.3
29.9
47.9
22.1
21.8
Open-ended setting
Table 2 sum-
marizes the results under open-ended
setting.
InstructSAM consistently
achieves equivalent or higher F1-
scores than remote sensing-specific
methods, including those trained on
grounded description tasks [47, 65].
Notably, InstructSAM surpasses LAE-
Label [54] by leveraging a global view
of the image to accurately predict ob-
ject categories. While the absence of
class-specific instructions in this set-
ting limits further gains, InstructSAM
still demonstrates robust performance (Figure 4).
Open-subclass setting
Table 3 shows that InstructSAM outperforms or matches Qwen2.5-VL
across both parent classes. When prompted with categories identified by GPT-4o, OWLv2 performs
strongly for â€œmeans of transportâ€ but struggles with â€œsports fieldsâ€, likely due to the prevalence
of transport-related categories in natural image datasets. These findings are consistent with the
open-vocabulary results, where generic detectors, such as Grounding DINO and OWL, struggle with
remote sensing categories except for airplane, vehicle, and ship.
7


--- Page 8 ---
Table 3: Performance comparison on EarthInstruct under open-subclass setting. â€˜Sâ€™ denotes the
parent category â€œsports fieldâ€, and â€˜Tâ€™ denotes â€œmeans of transportâ€.
Method
NWPU-VHR-10
DIOR
Cnt F1
Box F1
Mask F1
Cnt F1
Box F1
Mask F1
S
T
S
T
S
T
S
T
S
T
S
T
Qwen2.5-VL [2]
54.1 48.9 32.4 42.2
-
-
52.2 51.8 34.0 39.2
-
-
GPT-4o [24] + OWL [49] 40.3 68.0 19.8 65.9
-
-
41.5 73.6 27.6 70.9
-
-
InstructSAM-Qwen
50.9 55.3 33.5 41.9 33.0 39.7 40.4 67.4 22.2 49.3 22.6 49.5
InstructSAM-GPT4o
84.2 60.6 46.9 44.2 45.8 41.6 82.2 53.6 40.9 38.3 40.2 38.6
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
tennis court
basketball court
Ground Truth
basketball court
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
tennis court
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
InstructSAM
vehicle 0.25
vehicle 0.28
vehicle 0.30
vehicle 0.24
vehicle 0.30
vehicle 0.30
vehicle 0.31
vehicle 0.33
vehicle 0.31
vehicle 0.30
vehicle 0.29
vehicle 0.31
vehicle 0.34
vehicle 0.28
tennis court 0.39
vehicle 0.26
basketball court 0.35
vehicle 0.31
basketball court 0.30
basketball court 0.27
basketball court 0.39
vehicle 0.32
basketball court 0.24
vehicle 0.23
basketball court 0.31
vehicle 0.32
vehicle 0.34
OWLv2
basketball court
tennis court
Qwen2.5-VL
SegEarth-OV
expressway service area
chimney
harbor
ground track field
tennis court
basketball court
stadium
windmill
golf field
baseball field
storage tank
vehicle
overpass
expressway toll station
dam
bridge
(a) Open-vocabulary setting
building
basketball_court
car
building
car
car
building
car
building
car
tennis_court
car
car
car
car
car
building
car
car
car
car
car
building
car
car
car
InstructSAM
parking_lot
tennis_court
basketball_court
building
building
trees
road
Qwen2.5-VL
expressway service area
bridge
basketball court
bridge
storage_tank
bridge
bridge
vehicle
bridge
bridge
basketball court
bridge
bridge
bridge
bridge
bridge
golf field
bridge
basketball court
chimney
basketball court
bridge
bridge
baseball field
forest
airport
vegetation
expressway service area
bridge
train station
bridge
vehicle
expressway service area
chimney
chimney
chimney
bridge
airport
airport
baseball field
vegetation
baseball field
vehicle
chimney
bridge
baseball field
bridge
bridge
bridge
expressway 
baseball field
baseball field
vehicle
vegetation
forest
bridge
bridge
vehicle
vehicle
vegetation
vehicle
bridge
bridge
baseball field
bridge
baseball field
house
bridge
bridge
bridge
baseball field
bridge
bridge
forest
baseball field
bridge
bridge
bridge
bridge
expressway service area
basketball court
storage tank
airport airport
vehicle
vehiclebridge
expressway service area
airport
train station
expressway service area
expressway
baseball field
parking lot
bridge
baseball field
airport
LAE-Label
(b) Open-ended setting
car
car
car
car
car
car
car
car
car
car
car
car
car
car
car
InstructSAM
car
car
car
car
car
car
car
car
car
car
car
car
car
car
Qwen2.5-VL
(c) Open-subclass setting
Figure 4: Qualitative comparison of object detection and segmentation methods on DIOR dataset.
InstructSAM consistently outperforms other methods in generating accurate bounding boxes and
segmentation masks across various settings. More qualitative results can be found in Appendix F.
0
30
60
90
120
150
Number of Predicted Objects
0
20
40
60
80
100
Inference Time (seconds)
InstructSAM R2=0.01
Qwen2.5-VL
R2=0.99
GeoPixel
R2=0.41
LAE-Label R2=0.94
SkysenseGPT
R2=0.97
Counting Time
Mask Proposing Time
Matching Time
InstructSAM
Qwen2.5-VL
GeoPixel
LAE-Label
SkysenseGPT
EarthDial
EarthDial
R2=0.68
Figure 5: Inference time as a function of bounding box count for InstructSAM-Qwen and baseline
methods. Solid lines indicate linear regressions, and scatter points represent individual samples.
The shaded regions for InstructSAM illustrate the time composition of different processing steps.
Experiments are conducted on an RTX 4090 GPU.
8


--- Page 9 ---
Inference time analysis
Figure 5 compares inference times for open-ended methods using 7B
LLMs. Mask proposal dominates the runtime of InstructSAM, whereas the PuLP optimizer solves
the BIP problem in 0.07 seconds. InstructSAM maintains nearly constant inference time, while
other methods scale linearly with the number of objects. Unlike representing bounding boxes as
natural language tokens, InstructSAM reduces output tokens by 89% and total inference time by 32%
compared to Qwen2.5-VL. This advantage will grow more pronounced as the model size scales up,
highlighting the efficiency of our framework.
Table 4: Open vocabulary counting performance
(Precision/Recall/F1-score).
Method
Add Instr
NWPU
vehicle
NWPU
all
DIOR
all
Faster-RCNN
Ã—
15/49/23
77/77/73
91/74/81
Qwen2.5VL
Ã—
11/75/19
80/72/72
73/53/56
Qwen2.5VL
âœ“
28/70/41
82/71/73
83/51/59
GPT-4o
Ã—
11/80/20
68/79/67
87/65/72
GPT-4o
âœ“
75/68/71
83/83/83
86/75/80
Table 5: Ablations on model scaling. MP recall
is class-agnostic recall of mask proposals.
LVLM
Counter
Mask
Proposal
CLIP
MP
Recall
Box
F1
GPT-4o
SAM2-L
DFN2B-L [13]
82.4
41.3
GPT-4o
SAM2-L
RemoteCLIP-L [42]
82.4
45.7
GPT-4o
SAM2-L
SkyCLIP-L [73]
82.4
45.8
GPT-4o
SAM2-L
SkyCLIP-B [73]
82.4
45.2
GPT-4o
SAM2-S
SkyCLIP-B [73]
79.1
44.1
Qwen2.5VL
SAM2-S
SkyCLIP-B [73]
79.1
40.6
5.3
Ablation studies
Prompt design
Table 4 reveals how additional instructions enhance object counting, particularly for
categories with ambiguous or dataset-specific annotation rules. Initially, DIOR-trained Faster-RCNN
and LVLM counters exhibit low vehicle precision on NWPU-VHR-10. Explicit annotation rules in
instructions significantly boost vehicle precision for Qwen2.5-VL and GPT-4o, and also improve mF1
by 3% and 8%, respectively, on DIOR. Contrary to [88], these results show that capable foundation
models with precise, instruction-driven prompts truly empower LVLMs to match or exceed closed-set
model performance.
Model generalization and scaling
To assess InstructSAMâ€™s generalization and scalability, we
ablate LVLM counters, mask proposers, and CLIP models for the open-vocabulary detection (OVD)
task on NWPU-VHR-10 (Table 5). InstructSAM consistently benefits from CLIP models fine-tuned
on remote sensing data [42, 73] over generic CLIP [13], whereby yielding higher Box F1-scores.
Performance improves with larger model components, demonstrating the frameworkâ€™s scalability.
Notably, even with a smaller SAM2-S and SkyCLIP-B, InstructSAM coupled with Qwen2.5-VL
(40.6 Box F1) outperforms direct detection using Qwen2.5-VL alone (36.4 Box F1), underscoring the
efficacy of our approach.
Mask-label matching with counting constraints
Using fixed thresholds to filter CLIP predic-
tions [10] has inherent limitations. In Figure 6, the green and blue solid curves show InstructSAM
without matching, where predictions are filtered by a similarity score (CLIP score) threshold before
evaluation. Optimal thresholds vary widely across categories. As a result, a single global value
0.200
0.225
0.250
0.275
0.300
Threshold
0
15
30
45
60
mAPnc/mF1 (%)
Mean metrics of InstructSAM
mF1 of InstructSAM w/ Matching
mAPnc of InstructSAM w/ Matching
mAP of InstructSAM w/o Matching
mF1 of InstructSAM w/o Matching
mAPnc of InstructSAM w/o Matching
0.200
0.225
0.250
0.275
0.300
Threshold
0
20
40
60
80
F1-score (%)
Per-class F1-score of InstructSAM w/o Matching
airplane
ship
storage_tank
baseball_field
tennis_court
basketball_court
track_field
harbor
bridge
vehicle
Max mF1 Threshold
Figure 6: Threshold sensitivity analysis in open-vocabulary setting. Left: Impact on mean metrics.
Right: Category-specific F1-scores. Dashed lines indicate optimal thresholds maximizing mF1.
9


--- Page 10 ---
performs poorly, and the results are highly threshold-sensitive, which is consistent with the findings
in [35]. In contrast, InstructSAMâ€™s counting-constrained matching removes this dependency by
dynamically assigning predictions according to the estimated counts, yielding stronger performance
in multi-class and open-world settings.
5.4
Error analysis of OVD task
Error identification reveals distinct error patterns across methods (Figure 7). OWLv2 primarily
suffers from classification errors, while Qwen2.5-VL shows improved classification but struggles
with missed detections. InstructSAM-GPT4o benefits from SAM2â€™s localization capabilities, though
the background confusion persists due to GeoRSCLIPâ€™s scene-focused training, which prioritizes
broader contexts over individual objects.
Cls
Loc
Bkg
Miss
(a) OWLv2
Loc
Bkg
Miss
(b) Qwen2.5-VL
Loc
Bkg
Miss
(c) InstructSAM
Figure 7: Error distribution of OVD task on NWPU-VHR-10 dataset. Pie charts show proportions of
classification (Cls), localization (Loc), background (Bkg), and missed detection (Miss) errors across
models.
6
Limitations
InstructSAM builds on pre-trained foundation models, and its performance inherits their capabilities
and biases. For instance, SAM2 can miss the full extent of objects with intricate geometry (e.g.,
basketball court in Figure 4). CLIP models finetuned on scene-level remote sensing imageâ€“text
pairs perform suboptimally when aligning object-level crops with semantic cues. More generally,
foundation models such as GPT-4o and SAM are trained primarily on optical imagery and struggle to
count or segment objects in synthetic aperture radar (SAR) images.
Future work can mitigate these limitations by (i) incorporating foundation models trained on semanti-
cally diverse, multi-modal remote sensing data and (ii) leveraging advanced class-agnostic region
proposers [80]. Extending InstructSAM to semantic segmentation would further benefit the earth
observation community. Although InstructSAM is only a first step, we aim to use it as an automated
labeling engine to annotate at scale, enabling large, semantically rich object-recognition datasets.
7
Conclusions
In this paper, we introduce InstructCDS for instruction-driven object counting, detection, and seg-
mentation, along with EarthInstruct, the first benchmark for this task in remote sensing domain.
Our training-free InstructSAM framework integrates LVLMs, SAM2, and domain-specific CLIP to
handle instruction-oriented scenarios with counting constrained mask-label matching. Experiments
demonstrate that InstructSAM outperforms specialized baselines while maintaining near-constant
inference time regardless of object counts. As the first approach extending instruction-oriented
detection to the broader InstructCDS paradigm, InstructSAM will benefit from advancements in
both remote sensing foundation models [41, 81, 82, 64, 66] and general-purpose models [70, 93, 46],
paving the way for more scalable, instruction-driven earth observation data analysis.
Acknowledgement
This work was supported by the National Earth Observation Data Center Research Project, National
Natural Science Foundation of China (62506229), and Natural Science Foundation of Shanghai
(25ZR1402268).
10


--- Page 11 ---
References
[1] Kumar Ayush, Burak Uzkent, Kumar Tanmay, Marshall Burke, David Lobell, and Stefano Ermon. Efficient
poverty mapping from high resolution remote sensing images. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 35, pages 12â€“20, 2021.
[2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie
Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025.
[3] Marshall Burke, Anne Driscoll, David B Lobell, and Stefano Ermon. Using satellite imagery to understand
and promote sustainable development. Science, 371(6535):eabe8628, 2021.
[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey
Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision,
pages 213â€“229. Springer, 2020.
[5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale
image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pages 3558â€“3568, 2021.
[6] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian
Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-
image model. arXiv preprint arXiv:2209.06794, 2022.
[7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye,
Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models
with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024.
[8] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi
Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal
models with open-source suites. Science China Information Sciences, 67(12):220101, 2024.
[9] Gong Cheng, Junwei Han, Peicheng Zhou, and Lei Guo. Multi-class geospatial object detection and
geographic image classification based on collection of part detectors. ISPRS Journal of Photogrammetry
and Remote Sensing, 98:119â€“132, 2014.
[10] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-
time open-vocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 16901â€“16911, 2024.
[11] Seokju Cho, Heeseong Shin, Sunghwan Hong, Anurag Arnab, Paul Hongsuck Seo, and Seungryong Kim.
Cat-seg: Cost aggregation for open-vocabulary semantic segmentation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 4113â€“4123, 2024.
[12] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The
pascal visual object classes (voc) challenge. International journal of computer vision, 88:303â€“338, 2010.
[13] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander T Toshev, and Vaishaal Shankar.
Data filtering networks. In The Twelfth International Conference on Learning Representations, 2024.
[14] Yuqian Fu, Yu Wang, Yixuan Pan, Lian Huai, Xingyu Qiu, Zeyu Shangguan, Tong Liu, Yanwei Fu, Luc
Van Gool, and Xingqun Jiang. Cross-domain few-shot object detection via enhanced open-set object
detector. In European Conference on Computer Vision, pages 247â€“264. Springer, 2024.
[15] Guangshuai Gao, Qingjie Liu, and Yunhong Wang. Counting from sky: A large-scale data set for remote
sensing object counting and a benchmark method. IEEE Transactions on geoscience and remote sensing,
59(5):3642â€“3655, 2020.
[16] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and
language knowledge distillation. arXiv preprint arXiv:2104.13921, 2021.
[17] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation.
In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5356â€“5364,
2019.
[18] Ritwik Gupta, Bryce Goodman, Nirav Patel, Ricky Hosfelt, Sandra Sajeev, Eric Heim, Jigar Doshi, Keane
Lucas, Howie Choset, and Matthew Gaston. Creating xbd: A dataset for assessing building damage from
satellite imagery. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
workshops, pages 10â€“17, 2019.
11


--- Page 12 ---
[19] Mordechai Haklay and Patrick Weber. Openstreetmap: User-generated street maps. IEEE Pervasive
computing, 7(4):12â€“18, 2008.
[20] Jia He, Mukund Rungta, David Koleczek, Arshdeep Sekhon, Franklin X Wang, and Sadid Hasan. Does
prompt formatting have any impact on llm performance? arXiv preprint arXiv:2411.10541, 2024.
[21] Shiqi Huang, Shuting He, and Bihan Wen. ZoRI: Towards discriminative zero-shot remote sensing
instance segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages
3724â€“3732, 2025.
[22] Ziyue Huang, Yongchao Feng, Shuai Yang, Ziqi Liu, Qingjie Liu, and Yunhong Wang. Openrsd: Towards
open-prompts for object detection in remote sensing images. arXiv preprint arXiv:2503.06146, 2025.
[23] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and
compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, pages 6700â€“6709, 2019.
[24] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,
Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276,
2024.
[25] Jeremy Andrew Irvin, Emily Ruoyu Liu, Joyce C. Chen, Ines Dormoy, Jinyoung Kim, Samar Khanna, Zhuo
Zheng, and Stefano Ermon. TEOChat: A large vision-language assistant for temporal earth observation
data. In The Thirteenth International Conference on Learning Representations, 2025.
[26] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion.
Mdetr-modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF
international conference on computer vision, pages 1780â€“1790, 2021.
[27] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision
using crowdsourced dense image annotations. International journal of computer vision, 123:32â€“73, 2017.
[28] Kartik Kuckreja, Muhammad Sohail Danish, Muzammal Naseer, Abhijit Das, Salman Khan, and Fa-
had Shahbaz Khan. Geochat: Grounded large vision-language model for remote sensing. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 27831â€“27840, 2024.
[29] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning
segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 9579â€“9589, 2024.
[30] Chunyuan Li, Haotian Liu, Liunian Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong
Hu, Zicheng Liu, Yong Jae Lee, et al. Elevater: A benchmark and toolkit for evaluating language-augmented
visual models. Advances in Neural Information Processing Systems, 35:9287â€“9301, 2022.
[31] Kaiyu Li, Ruixun Liu, Xiangyong Cao, Xueru Bai, Feng Zhou, Deyu Meng, and Zhi Wang. Segearth-
ov: Towards training-free open-vocabulary segmentation for remote sensing images. arXiv preprint
arXiv:2410.01768, 2024.
[32] Kaiyu Li, Zepeng Xin, Li Pang, Chao Pang, Yupeng Deng, Jing Yao, Guisong Xia, Deyu Meng, Zhi Wang,
and Xiangyong Cao. Segearth-r1: Geospatial pixel reasoning via large language model. arXiv preprint
arXiv:2504.09644, 2025.
[33] Ke Li, Gang Wan, Gong Cheng, Liqiu Meng, and Junwei Han. Object detection in optical remote
sensing images: A survey and a new benchmark. ISPRS journal of photogrammetry and remote sensing,
159:296â€“307, 2020.
[34] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan
Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al.
Grounded language-image pre-training.
In
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10965â€“10975,
2022.
[35] Qingyun Li, Yushi Chen, Xinya Shu, Dong Chen, Xin He, Yi Yu, and Xue Yang. A simple aerial detection
baseline of multimodal language models. arXiv preprint arXiv:2501.09720, 2025.
[36] Xiang Li, Jian Ding, and Mohamed Elhoseiny. VRSBench: A versatile vision-language benchmark
dataset for remote sensing image understanding. In Advances in Neural Information Processing Systems,
volume 37, pages 3229â€“3242, 2024.
12


--- Page 13 ---
[37] Yan Li, Weiwei Guo, Xue Yang, Ning Liao, Dunyun He, Jiaqi Zhou, and Wenxian Yu. Toward open
vocabulary aerial object detection with clip-activated student-teacher learning. In European Conference on
Computer Vision, pages 431â€“448. Springer, 2024.
[38] Yan Li, Weiwei Guo, Xue Yang, Ning Liao, Shaofeng Zhang, Yi Yu, Wenxian Yu, and Junchi Yan.
Exploiting unlabeled data with multiple expert teachers for open vocabulary aerial object detection and its
orientation adaptation. arXiv preprint arXiv:2411.02057, 2024.
[39] Chuang Lin, Yi Jiang, Lizhen Qu, Zehuan Yuan, and Jianfei Cai. Generative region-language pretraining
for open-ended object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 13958â€“13968, 2024.
[40] Zhiwei Lin, Yongtao Wang, and Zhi Tang. Training-Free Open-Ended Object Detection and Segmentation
via Attention as Prompts. In Advances in Neural Information Processing Systems, volume 37, pages
69588â€“69606, 2024.
[41] Chenyang Liu, Keyan Chen, Rui Zhao, Zhengxia Zou, and Zhenwei Shi. Text2earth: Unlocking text-driven
remote sensing image generation with a global-scale dataset and a foundation model. arXiv preprint
arXiv:2501.00895, 2025.
[42] Fan Liu, Delong Chen, Zhangqingyun Guan, Xiaocong Zhou, Jiale Zhu, Qiaolin Ye, Liyong Fu, and
Jun Zhou. Remoteclip: A vision language foundation model for remote sensing. IEEE Transactions on
Geoscience and Remote Sensing, 2024.
[43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural
information processing systems, 36:34892â€“34916, 2023.
[44] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li,
Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set
object detection. In European Conference on Computer Vision, pages 38â€“55. Springer, 2024.
[45] Sihan Liu, Yiwei Ma, Xiaoqing Zhang, Haowei Wang, Jiayi Ji, Xiaoshuai Sun, and Rongrong Ji. Rotated
multi-scale interaction network for referring remote sensing image segmentation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26658â€“26668, 2024.
[46] Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jiawen Liu, Jifeng Dai, Yu Qiao, and Xizhou Zhu.
Mono-internvl: Pushing the boundaries of monolithic multimodal large language models with endogenous
visual pre-training. arXiv preprint arXiv:2410.08202, 2024.
[47] Junwei Luo, Zhen Pang, Yongjun Zhang, Tingzhu Wang, Linlin Wang, Bo Dang, Jiangwei Lao, Jian Wang,
Jingdong Chen, Yihua Tan, et al. Skysensegpt: A fine-grained instruction tuning dataset and model for
remote sensing vision-language understanding. arXiv preprint arXiv:2406.10100, 2024.
[48] Chenlin Meng, Enci Liu, Willie Neiswanger, Jiaming Song, Marshall Burke, David Lobell, and Stefano
Ermon. Is-count: Large-scale object counting from satellite images with covariate-based importance
sampling. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 12034â€“
12042, 2022.
[49] Matthias Minderer, Alexey Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection.
Advances in Neural Information Processing Systems, 36:72983â€“73007, 2023.
[50] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Doso-
vitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-
vocabulary object detection. In European conference on computer vision, pages 728â€“755. Springer,
2022.
[51] Stuart Mitchell, Michael OSullivan, and Iain Dunning. Pulp: a linear programming toolkit for python. The
University of Auckland, Auckland, New Zealand, 65:25, 2011.
[52] Dilxat Muhtar, Zhenshi Li, Feng Gu, Xueliang Zhang, and Pengfeng Xiao. Lhrs-bot: Empowering remote
sensing with vgi-enhanced large multimodal language model. In European Conference on Computer
Vision, pages 440â€“457. Springer, 2024.
[53] Ruizhe Ou, Yuan Hu, Fan Zhang, Jiaxin Chen, and Yu Liu. Geopix: Multi-modal large language model for
pixel-level image understanding in remote sensing. arXiv preprint arXiv:2501.06828, 2025.
[54] Jiancheng Pan, Yanxing Liu, Yuqian Fu, Muyuan Ma, Jiahao Li, Danda Pani Paudel, Luc Van Gool, and
Xiaomeng Huang. Locate anything on earth: Advancing open-vocabulary object detection for remote
sensing community. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages
6281â€“6289, 2025.
13


--- Page 14 ---
[55] Chao Pang, Xingxing Weng, Jiang Wu, Jiayu Li, Yi Liu, Jiaxing Sun, Weijia Li, Shuai Wang, Litong Feng,
Gui-Song Xia, et al. Vhm: Versatile and honest vision language model for remote sensing image analysis.
In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 6381â€“6388, 2025.
[56] Claudio Persello, Jan Dirk Wegner, Ronny HÃ¤nsch, Devis Tuia, Pedram Ghamisi, Mila Koeva, and Gustau
Camps-Valls. Deep learning and earth observation to support the sustainable development goals: Current
approaches, open challenges, and future opportunities. IEEE Geoscience and Remote Sensing Magazine,
10(2):172â€“200, 2022.
[57] Renjie Pi, Lewei Yao, Jianhua Han, Xiaodan Liang, Wei Zhang, and Hang Xu. Ins-detclip: Aligning
detection model to follow human-language instruction. In The Twelfth International Conference on
Learning Representations, 2024.
[58] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. In International conference on machine learning, pages 8748â€“8763. PmLR,
2021.
[59] Viresh Ranjan, Udbhav Sharma, Thu Nguyen, and Minh Hoai. Learning to count everything. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3394â€“3403, 2021.
[60] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham
Khedr, Roman RÃ¤dle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and
videos. arXiv preprint arXiv:2408.00714, 2024.
[61] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object
detection with region proposal networks. IEEE transactions on pattern analysis and machine intelligence,
39(6):1137â€“1149, 2016.
[62] Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi
Shen, Yuan Gao, Xiaoke Jiang, et al. Dino-x: A unified vision model for open-world object detection and
understanding. arXiv preprint arXiv:2411.14347, 2024.
[63] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin.
Pixellm: Pixel reasoning with large multimodal model. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 26374â€“26383, 2024.
[64] Rajat Sahay and Andreas Savakis. On aligning sam to remote sensing data. In Geospatial Informatics XIV,
volume 13037, pages 10â€“18. SPIE, 2024.
[65] Akashah Shabbir, Mohammed Zumri, Mohammed Bennamoun, Fahad S Khan, and Salman Khan. Geopixel:
Pixel grounding large multimodal model in remote sensing. arXiv preprint arXiv:2501.13925, 2025.
[66] Zhe Shan, Yang Liu, Lei Zhou, Cheng Yan, Heng Wang, and Xia Xie. Ros-sam: High-quality interactive
segmentation for remote sensing moving object. arXiv preprint arXiv:2503.12006, 2025.
[67] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun.
Objects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF
international conference on computer vision, pages 8430â€“8439, 2019.
[68] Hao Su, Shunjun Wei, Min Yan, Chen Wang, Jun Shi, and Xiaoling Zhang. Object detection and instance
segmentation in remote sensing imagery based on precise mask r-cnn. In IGARSS 2019-2019 IEEE
International Geoscience and Remote Sensing Symposium, pages 1454â€“1457. IEEE, 2019.
[69] Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping Wang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao
Feng, Tao Xu, et al. Fair1m: A benchmark dataset for fine-grained object recognition in high-resolution
remote sensing imagery. ISPRS Journal of Photogrammetry and Remote Sensing, 184:116â€“130, 2022.
[70] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah
Perrin, Tatiana Matejovicova, Alexandre RamÃ©, Morgane RiviÃ¨re, et al. Gemma 3 technical report. arXiv
preprint arXiv:2503.19786, 2025.
[71] Di Wang, Jing Zhang, Bo Du, Minqiang Xu, Lin Liu, Dacheng Tao, and Liangpei Zhang. Samrs: Scaling-
up remote sensing segmentation dataset with segment anything model. Advances in Neural Information
Processing Systems, 36:8815â€“8827, 2023.
[72] Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, Bin Wang, Conghui He, and Dahua
Lin. V3det: Vast vocabulary visual detection dataset. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 19844â€“19854, 2023.
14


--- Page 15 ---
[73] Zhecheng Wang, Rajanie Prabha, Tianyuan Huang, Jiajun Wu, and Ram Rajagopal. Skyscript: A large and
semantically diverse vision-language dataset for remote sensing. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 38, pages 5805â€“5813, 2024.
[74] Syed Waqas Zamir, Aditya Arora, Akshita Gupta, Salman Khan, Guolei Sun, Fahad Shahbaz Khan, Fan
Zhu, Ling Shao, Gui-Song Xia, and Xiang Bai. isaid: A large-scale dataset for instance segmentation in
aerial images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
workshops, pages 28â€“37, 2019.
[75] Guoting Wei, Xia Yuan, Yu Liu, Zhenhao Shang, Kelu Yao, Chao Li, Qingsen Yan, Chunxia Zhao, Haokui
Zhang, and Rong Xiao. Ova-detr: Open vocabulary aerial object detection using image-text alignment and
fusion. arXiv preprint arXiv:2408.12246, 2024.
[76] Jianzong Wu, Xiangtai Li, Shilin Xu, Haobo Yuan, Henghui Ding, Yibo Yang, Xia Li, Jiangning Zhang,
Yunhai Tong, Xudong Jiang, et al. Towards open vocabulary learning: A survey. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 46(7):5092â€“5113, 2024.
[77] Qiusheng Wu and Lucas Prado Osco. samgeo: A python package for segmenting geospatial data with the
segment anything model (sam). Journal of Open Source Software, 8(89):5663, 2023.
[78] Zijing Wu, Ce Zhang, Xiaowei Gu, Isla Duporge, Lacey F Hughey, Jared A Stabach, Andrew K Skidmore,
J Grant C Hopcraft, Stephen J Lee, Peter M Atkinson, et al. Deep learning enables satellite-based monitoring
of large populations of terrestrial mammals across heterogeneous landscape. Nature communications,
14(1):3072, 2023.
[79] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and
Lu Yuan. Florence-2: Advancing a unified representation for a variety of vision tasks. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4818â€“4829, 2024.
[80] Jianlin Xie, Guanqun Wang, Tong Zhang, Yikang Sun, He Chen, Yin Zhuang, and Jun Li. Llama-
unidetector: A llama-based universal framework for open-vocabulary object detection in remote sensing
imagery. IEEE Transactions on Geoscience and Remote Sensing, 2025.
[81] Zhitong Xiong, Yi Wang, Weikang Yu, Adam J Stewart, Jie Zhao, Nils Lehmann, Thomas Dujardin,
Zhenghang Yuan, Pedram Ghamisi, and Xiao Xiang Zhu. Geolangbind: Unifying earth observation with
agglomerative vision-language foundation models. arXiv preprint arXiv:2503.06312, 2025.
[82] Zhiyuan Yan, Junxi Li, Xuexue Li, Ruixue Zhou, Wenkai Zhang, Yingchao Feng, Wenhui Diao, Kun Fu,
and Xian Sun. Ringmo-sam: A foundation model for segment anything in multimodal remote-sensing
images. IEEE Transactions on Geoscience and Remote Sensing, 61:1â€“16, 2023.
[83] Lewei Yao, Renjie Pi, Jianhua Han, Xiaodan Liang, Hang Xu, Wei Zhang, Zhenguo Li, and Dan Xu.
Detclipv3: Towards versatile generative open-vocabulary object detection. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 27391â€“27401, 2024.
[84] Chengyang Ye, Yunzhi Zhuge, and Pingping Zhang. Towards open-vocabulary remote sensing image
semantic segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages
9436â€“9444, 2025.
[85] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Convolutions die hard: Open-
vocabulary segmentation with single frozen convolutional clip. Advances in Neural Information Processing
Systems, 36:32215â€“32234, 2023.
[86] Zhengqing Zang, Chenyu Lin, Chenwei Tang, Tao Wang, and Jiancheng Lv. Zero-shot aerial object
detection with visual description regularization. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 38, pages 6926â€“6934, 2024.
[87] ValÃ©rie Zermatten, Javiera Castillo-Navarro, Diego Marcos, and Devis Tuia.
Learning transferable
land cover semantics for open vocabulary interactions with remote sensing images. ISPRS Journal of
Photogrammetry and Remote Sensing, 220:621â€“636, 2025.
[88] Chenhui Zhang and Sherrie Wang. Good at captioning bad at counting: Benchmarking gpt-4v on earth
observation data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 7839â€“7849, 2024.
[89] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Li, Xiyang Dai, Lijuan Wang,
Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-language
understanding. Advances in Neural Information Processing Systems, 35:36067â€“36080, 2022.
15


--- Page 16 ---
[90] Zilun Zhang, Tiancheng Zhao, Yulong Guo, and Jianwei Yin. Rs5m and georsclip: A large scale vision-
language dataset and a large vision-language model for remote sensing. IEEE Transactions on Geoscience
and Remote Sensing, 2024.
[91] Yijie Zheng, Jinxuan Yang, Yu Chen, Yaxuan Wang, Yihang Lu, and Guoqing Li. Beluga whale detection
from satellite imagery with point labels. arXiv preprint arXiv:2505.12066, 2025.
[92] Yue Zhou, Mengcheng Lan, Xiang Li, Yiping Ke, Xue Jiang, Litong Feng, and Wayne Zhang. Ge-
oground: A unified large vision-language model. for remote sensing visual grounding. arXiv preprint
arXiv:2411.11904, 2024.
[93] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian,
Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source
multimodal models. arXiv preprint arXiv:2504.10479, 2025.
16


--- Page 17 ---
Technical Appendices
A
Comparison with related work
Remote sensing imagery captures a vast, diverse range of contexts. Exemplified by the SkyScript
dataset [73], curated from Google Earth and OpenStreetMap [19], featuring 29,000 distinct semantic
tags. Despite this richness, annotated training datasets for remote sensing object recognition typically
span only a few dozen categories (Table 6). This lexical scarcity hinders effective alignment of
semantic and visual information compared to generic models, which are typically trained on millions
of human-annotated images and near-infinite vocabularies sourced from the Internet. Similarly,
instruction-following datasets for remote sensing, such as FIT-RS [47] and GeoPixelD [65], are also
built upon manually annotated data, often inheriting these vocabulary-specific limitations.
Regarding model capabilities, remote sensing LVLMs typically fail to follow complex object recogni-
tion instructions as illustrated in Figure 8. Nevertheless, SkysenseGPT [47] and GeoPixel [65], trained
on scene graph generation or pixel-grounded conversation generation tasks, deliver comprehensive
object detection or segmentation outputs, effectively serving a similar purpose of open-ended object
recognition.
Table 6: Comparison of language-guided object recognition methods. We summarize the model,
publication, training data, dense segmentation capability, and support for instruction-oriented object
detection (open-vocabulary detection (OVD), open-ended detection (OED), and open-subclass detec-
tion (OSD)). Green âœ“indicates support, red âœ—indicates not supported.
Method
Publication
Object Recognition
Training Data
Dense
Segmentation OVD OED OSD
Remote sensing-specified methods
DesReg [86]
AAAIâ€™24
DIOR
âœ—
âœ“
âœ—
âœ—
OVA-DETR [75]
arXivâ€™24
DIOR+DOTA+xView
âœ—
âœ“
âœ—
âœ—
CASTDet [37]
ECCVâ€™24
DIOR
âœ—
âœ“
âœ—
âœ—
LAE-DINO [54]
AAAIâ€™25
LAE-1M
âœ—
âœ“
âœ—
âœ—
ZORI [21]
AAAIâ€™25
NWPU/DIOR
âœ“
âœ“
âœ—
âœ—
GSNet [84]
AAAIâ€™25
LandCover-40k
âœ“
âœ—
âœ—
âœ—
SegEarth-OV [31]
CVPRâ€™25
Image-level Training
âœ“
âœ—
âœ—
âœ—
SkysenseGPT [47]
arXivâ€™24
FIT-RS
âœ—
âœ—
âœ“
âœ—
GeoPixel [65]
ICMLâ€™25
GeoPixelD
âœ“
âœ—
âœ“
âœ—
Generic methods
GroundingDINO [44] ECCVâ€™24
FiveODs+GoldG+Cap4M
âœ—
âœ“
âœ—
âœ—
OWLv2 [49]
NIPSâ€™22
WebLI2B
âœ—
âœ“
âœ—
âœ—
Florence-2 [79]
CVPRâ€™24
FLA-5B
âœ—
âœ“
âœ“
âœ—
Qwen2.5-VL [2]
arXivâ€™25
Various sources
âœ—
âœ“
âœ“
âœ“
InsDetCLIP [57]
ICLRâ€™24
Object365
âœ—
âœ“
âœ“
âœ“
VL-SAM [40]
NeurIPSâ€™24 Training-free
âœ“
âœ—
âœ“
âœ—
InstructSAM
Ours
Training-free
âœ“
âœ“
âœ“
âœ“
B
Benchmark details
The details of the datasets comprising EarthInstruct are as follows:
â€¢ NWPU-VHR-10 [9] is a 10-class dataset for very-high-resolution (VHR) remote sensing object
detection. It originally comprises 650 positive images (each containing at least one target object)
and 150 negative images (containing no target objects). For constructing EarthInstruct, we utilize
the 650 positive images. Instance-level mask annotations were subsequently provided by [68].
17


--- Page 18 ---
[detection]Provide the coordinates of all the baseball fields, ground track fields, and tennis courts in the 
image.
There is 1 baseball_field, 1 ground_track_field, 4 tennis_courts in the image: 
<rbox>({<19.73><10.45><20.31><19.53>|<90>}, {<19.73><40.23><20.31><19.53>|<90>}, 
{<19.73><60.25><20.31><19.53>|<90>}, {<19.73><80.27><20.31><19.53>|<90>})</rbox>
[1,55,17,72], [1,72,17,88], [1,83,17,99]
[1,1,27,26], [1,2,13,14], [1,10,19,26], [1,26,19,42], [1,43,19,59], [1,55,19,71], [1,67,19,83], [1,76,19,92], 
[1,84,19,100], [1,89,19,100], [1,76,19,92], [1,67,19,83], [1,55,19,71], [1,43,19,59], [1,26,19,42], 
[1,10,19,14], [1,10,19,14], [1,26,19,42], [1,43,19,59], [1,55,19,71], [1,67,19,83], [1,7
Detect all the visible objects in the image and provide the subclass names and their coordinates.
There are 2 buildings, 11 cars, 11 tennis courts, 1 groundtrackfield, 1 baseballfield, 1 building, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 
car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 
1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 car, 1 carâ€¦
Detect all the objects that belong to the parent class â€œsports fieldâ€ in the image. Provide the subclass names and corresponding coordinates.
There is 1 object in total. All coordinates: <rbox>({<40.82><19.82><47.85><27.05>|<90>})</rbox>.
SkysenseGPT
TEOChat
SkysenseGPT
TEOChat
1 groundtrackfield at the left, 1 groundtrackfield at the center, 1 baseballfield at the top left, 1 baseballfield at the top right, 1 
groundtrackfield at the bottom left, 1 groundtrackfield at the bottom right, 1 groundtrackfield at the center, 1 groundtrackfield at the left, 1 
groundtrackfield at the right, 1 groundtrackfield at the top, 1 groundtrackfield at the top leftâ€¦
User
SkysenseGPT
TEOChat
User
User
Image Input
Figure 8: Examples illustrating that SkysenseGPT [47] and TEOChat [25] fail to produce meaningful
responses for open-vocabulary, open-ended, and open-subclass prompts. Their responses either lack
category name outputs or exhibit looped generation.
â€¢ DIOR [33] is a large-scale benchmark for object detection in optical remote sensing images,
encompassing 20 object categories. The dataset is split into 5,862 training images, 5,863 validation
images, and 11,725 test images. For the EarthInstruct benchmark, we utilize the validation set of
DIOR. Mask annotations are provided by the SAMRS [71] dataset.
Table 7 provides a summary of these datasets, including their spectral bands, spatial resolution ranges,
and the specific category splits within EarthInstruct.
Table 8 compares EarthInstruct with other language-guided object recognition datasets and bench-
marks. Most current benchmarks [57, 45, 36] generate prompts using LLMs or templates based on
specific objects in the image, resulting in few negative samples. In addition, referring segmenta-
Table 7: Details of the datasets used in EarthInstruct, including spectral bands, resolution, and
category splits for base, novel, and open-subclass (sports field, means of transport) settings. â€œResâ€
denotes spatial resolution. â€œNWPUâ€ stands for NWPU-VHR-10.
Dataset
Band
Res
Base Class
Novel Class
Sports Field
Means of
Transports
NWPU
RGB/
Color
Infrared
0.08-2m
airplane, storage_tank,
baseball_field,
tennis_court, track_field,
bridge, vehicle
ship, basketball_
court, harbor
baseball_field,
tennis_court,
basketball_court,
track_field
airplane,
ship,
vehicle
DIOR
RGB
0.3-30m
airplane, baseball field,
bridge, chimney,
expressway service area,
expressway toll station,
dam, golf field, harbor,
overpass, ship, stadium,
storage tank, tennis court,
train station, vehicle
airport,
basketball court,
ground track field,
windmill
baseball field,
basketball court,
golf field,
ground track field,
tennis court,
stadium
airplane,
ship,
vehicle
18


--- Page 19 ---
Table 8: Comparison between EarthInstruct and other language-guided object recognition benchmark
datasets. â€œOVâ€, â€œOEâ€, â€œOSâ€ denote open-vocabulary, open-ended, and open-subclass, respectively.
Goal-Oriented denotes instructions to detect objects that achieve certain goals. Content Prior Free
means the benchmark does not require image content prior for prompt construction.
Dataset
Publication
OV
Instructions
OE
Instructions
OS
Instructions
Goal-
Oriented
Instructions
Content
Prior
Free
Class
Num
IOD-Bench [57]
ICLRâ€™24
âœ“
âœ“
âœ“
âœ“
âœ—
â€“
RRSIS-D [45]
CVPRâ€™24
âœ“
âœ—
âœ—
âœ—
âœ—
20
VRSBench [36]
NeurIPSâ€™24
âœ“
âœ—
âœ—
âœ—
âœ—
26
FIT-RS (OD) [47]
arXivâ€™24
âœ“
âœ—
âœ—
âœ—
âœ—
48
FIT-RS (SGG) [47]
arXivâ€™24
âœ—
âœ“
âœ—
âœ—
âœ“
48
GeoPixelID [65]
ICMLâ€™25
âœ—
âœ“
âœ—
âœ—
âœ“
15
EarthReason [32]
arXivâ€™25
âœ—
âœ—
âœ—
âœ“
âœ—
28
EarthInstruct
Ours
âœ“
âœ“
âœ“
âœ—
âœ“
20
tion [45] and visual grounding [36] benchmarks typically support only single-class or single-object
queries, restricting efficiency in large-scale applications.
EarthInstruct addresses these issues by supporting open-vocabulary, open-ended, and open-subclass
settings, and uses dataset-specific prompts that encode user intent without relying on image content
priors. This enables more comprehensive and efficient evaluation for instruction-driven object
recognition.
C
Implementation details
This section provides further details on the implementation of our InstructSAM framework and
other baselines. A critical component of leveraging LVLMs for the InstructCDS tasks is the design
of effective prompts. We employ structured prompts in JSON format to systematically guide the
LVLM counter. This structured approach allows for clear delineation of the modelâ€™s Persona,
the specific Task to be performed, detailed Instructions for execution, the desired Output
format, and illustrative Examples. The Instructions field enables the incorporation of dataset-
specific annotation rules and disambiguation of category definitions, which are crucial for accurate
interpretation the user intension, as discussed in Section 3.1. The subsequent subsections detail the
specific prompts used for each setting within the EarthInstruct benchmark.
Here we further clarify how the categories that might have bewildered the model. For example,
an â€œoverpassâ€ is defined as â€œa bridge, road, railway or similar structure that is over another road or
railway1â€, while a â€œbridgeâ€ is â€œa structure built to span a physical obstacle (such as a body of water,
valley, road, or railway) without blocking the path underneath2.â€ It is possible that the LVLM counter
will recognize overpasses as bridges since overpass is a subset of bridge. Adding clarification such as
â€œbridge in this dataset refers to a structure spanning a body of waterâ€ to the Instructions field will
guild the LVLM to count accurately.
C.1
Prompts in open-vocabulary setting
â€¢ Prompt for NWPU-VHR-10 dataset:
{
"Persona": "You are an advanced AI model capable of understanding and
analyzing aerial images.",
"Task": "Given an input satellite imagery, count the number of objects from
specific categories. Provide the results in JSON format where the keys are
the category names and the values are the corresponding counts.",
"Instructions": [
1https://en.wikipedia.org/wiki/Overpass
2https://en.wikipedia.org/wiki/Bridge
19


--- Page 20 ---
"The 10 categories in the dataset are: [â€˜airplaneâ€™, â€˜shipâ€™,
â€˜storage_tankâ€™, â€˜baseball_fieldâ€™, â€˜tennis_courtâ€™, â€˜basketball_courtâ€™,
â€˜track_fieldâ€™, â€˜harborâ€™, â€˜bridgeâ€™, â€˜vehicleâ€™]",
"The spatial resolution of the imagery in the dataset ranges from 0.08 m
to 2 m.",
"Do not count ships or vehicles that are hard to annotate in the
relatively low-resolution images as they are not annotated due to the small
size.",
"Harbor is defined as a pier to dock ships. If multiple harbors are
visible in the image, count each distinct pier separately."
],
"Output format": "{\"category1\": count1, \"category2\": count2, ... }",
"Examples": [
{ "airplane": 2, "ship": 0, "storage_tank": 3, "baseball_field": 1,
"tennis_court": 0, "basketball_court": 0, "track_field": 0, "harbor": 6,
"bridge": 0, "vehicle": 0 },
{ "airplane": 5, "ship": 2, "storage_tank": 0, "baseball_field": 0,
"tennis_court": 1, "basketball_court": 0, "track_field": 0, "harbor": 0,
"bridge": 1, "vehicle": 10 }
]
}
â€¢ Prompt for DIOR dataset:
{
"Persona": "You are an advanced AI model capable of understanding and
analyzing remote sensing images."
"Task": "Given an input satellite imagery, count the number of objects from
specific categories. Provide the results in JSON format where the keys are
the category names and the values are the corresponding counts.",
"Instructions": [
"The 20 categories in the dataset are: [â€˜airplaneâ€™, â€˜airportâ€™, â€˜baseball
fieldâ€™, â€˜basketball courtâ€™, â€˜bridgeâ€™, â€˜chimneyâ€™, â€˜expressway service areaâ€™,
â€˜expressway toll stationâ€™, â€˜damâ€™, â€˜golf fieldâ€™, â€˜ground track fieldâ€™,
â€˜harborâ€™, â€˜overpassâ€™, â€˜shipâ€™, â€˜stadiumâ€™, â€˜storage tankâ€™, â€˜tennis courtâ€™,
â€˜train stationâ€™, â€˜vehicleâ€™, â€˜windmillâ€™]",
"The spatial resolution of the images is 0.3m-30m.",
"Airport is a large area of land where aircraft can take off and land. It
includes runways and other facilities. Do not count airport if the it is not
compeletely visible in the image.",
"Harbor is defined as a pier to dock ships. If multiple harbors are
visible in the image, count each distinct pier separately.",
"Expressway toll station is a toll booth at the entrance of the
expressway and spans the road.",
"Expressway service area is a rest area along an expressway. If it exsits
on the both sides of the expressway, count them separately.",
"Overpass is a road crossing over another road. Bridge is a road spanning
a river. Distinguish them carefully.",
"If the overpass or bridge is composed of parallel, separate sections
(for example, different lanes or directions of traffic), each section should
be counted individually.",
"Count every ship and vehicle carefully, even the resolution is low and
the objects are small and dense.",
"If none of the objects among the categories is visible, output a JSON
object with all categories set to 0"
],
"Output format": "{ "category1": count1, "category2": count2, ... }",
"Examples": [
"{ "airplane": 2, "airport": 0, "baseball field": 0, "basketball court":
0, "bridge": 1, ... }",
"{ "airplane": 0, "airport": 0, "baseball field": 2, "basketball court":
6, "bridge": 0, ... }"
]
}
20


--- Page 21 ---
C.2
Prompts in open-ended setting
â€¢ Prompt for NWPU-VHR-10 dataset:
{
"Persona": "You are an advanced AI model capable of understanding and
analyzing remote sensing images.",
"Task": "Given an input satellite imagery, count the number of all the
visible remote sensing objects. Provide the results in JSON format where the
keys are the category names and the values are the corresponding counts.",
"Instructions": [
"The spatial resolution of the imagery in the dataset ranges from 0.08 m
to 2 m.",
"Do not count ships or vehicles that are too samll and are hard to
annotate in the relatively low-resolution images.",
"Only count objects that are clearly visible in the imagery. If a
category is not visible, do not include it in the output."
],
"Output format": "{ "category1": count1, "category2": count2, ... }",
"Answer": [
"Ensure the category names are in singular form",
"Provide the counts as integers."
]
}
â€¢ Prompt for DIOR dataset:
{
"Persona": "You are an advanced AI model capable of understanding and
analyzing remote sensing images.",
"Task": "Given an input satellite imagery, count the number of all the
visible remote sensing objects or scenes. Provide the results in JSON format
where the keys are the category names and the values are the corresponding
counts.",
"Instructions": [
"The spatial resolution of the imagery in the dataset ranges from 0.3 m
to 30 m.",
"If the resolution is too limited or the scene is too dense to accurately
count certain objects, exclude those objects from the results.",
"Only count objects that are clearly visible in the imagery."
],
"Output format": "{ "category1": count1, "category2": count2, ... }",
"Answer": [
"Ensure the category names are in singular form",
"Provide the counts as integers."
]
}
C.3
Prompts in open-subclass setting
â€¢ Prompt for counting â€œsports fieldâ€ in NWPU-VHR-10 dataset:
{
"Persona": "You are an advanced AI model capable of understanding and
analyzing remote sensing images."
"Task": "Given an input satellite imagery, count the number of objects that
belong to the parent category **sports field**. Provide the results in JSON
format where the keys are the names of the subcategories and the values are
the corresponding counts.",
"Instructions": [
"The spatial resolution of the images is 0.08m-2m.",
"Do not count objects that are hard to annotate in the relatively
low-resolution images as they are not annotated due to the small size.",
"If none of the objects belong to the parent category is visible, output
a empty JSON object like \{ \}"
21


--- Page 22 ---
],
"Output format": "{ "subcategory1": count1, "subcategory2": count2, ... }",
"Answer": [
"Ensure the category names are in singular form",
"Provide the counts as integers."
]
}
â€¢ Prompt for counting â€œmeans of transportâ€ in NWPU-VHR-10 dataset:
{
"Persona": "You are an advanced AI model capable of understanding and
analyzing remote sensing images."
"Task": "Given an input satellite imagery, count the number of objects that
belong to the parent category **means of transport**. Provide the results in
JSON format where the keys are the names of the subcategories and the values
are the corresponding counts.",
"Instructions": [
"The spatial resolution of the imagery in the dataset ranges from 0.08 m
to 2 m.",
"Do not count boats or land vehicles that are hard to annotate in the
relatively low-resolution images as they are not annotated due to the small
size.",
"If none of the objects belong to the parent category is visible, output
a empty JSON object like \{ \}"
],
"Output format": "{ "subcategory1": count1, "subcategory2": count2, ... }",
"Answer": [
"Ensure the category names are in singular form",
"Provide the counts as integers."
]
}
â€¢ Prompt for counting â€œsports fieldâ€ in DIOR dataset:
{
"Persona": "You are an advanced AI model capable of understanding and
analyzing remote sensing images."
"Task": "Given an input satellite imagery, count the number of visible
objects that belong to the parent category **sports field**. Provide the
results in JSON format where the keys are the names of the subcategories and
the values are the corresponding counts.",
"Instructions": [
"The spatial resolution of the images is 0.3m-30m.",
"Return only the categories and counts that meet the visibility and
resolution criteria. If none of the objects belong to the parent category is
visible, output a empty JSON object like \{ \}".
],
"Output format": "{ "subcategory1": count1, "subcategory2": count2, ... }",
"Answer": [
"Ensure the category names are in singular form",
"Provide the counts as integers."
]
}
â€¢ Prompt for counting â€œmeans of transportâ€ in DIOR dataset:
{
"Persona": "You are an advanced AI model capable of understanding and
analyzing remote sensing images."
"Task": "Given an input satellite imagery, count the number of visible
objects that belong to the parent category **means of transport**. Provide
the results in JSON format where the keys are the names of the subcategories
and the values are the corresponding counts.",
"Instructions": [
"The spatial resolution of the images is 0.3m-30m.",
22


--- Page 23 ---
"Return only the categories and counts that meet the visibility and
resolution criteria. If none of the objects belong to the parent category is
visible, output a empty JSON object like \{ \}"
],
"Output format": "{ "subcategory1": count1, "subcategory2": count2, ... }",
"Answer": [
"Ensure the category names are in singular form",
"Provide the counts as integers."
]
}
C.4
Hyperparameters of InstructSAM
Mask proposer
As described in Section 4.2, class-agnostic mask proposals are generated using
SAM2-hiera-large [60] in its automatic mask generation mode. The model was configured with
the parameters detailed in Table 9 across both NWPU-VHR-10 and DIOR. These settings balance
the proposal quality and computational efficiency, ensuring a dense set of high-quality proposals,
especially for small objects.
Table 9: Hyperparameters for SAM2.
Parameter
Value
pred_iou_thresh
0.75
stability_score_thresh
0.75
points_per_side
24
crop_n_layers
1
box_nms_thresh
0.5
LVLM counter
For Qwen2.5-VL and GPT-4o, the temperature is set at 0.01 and top_p is set at 1
to reduce randomness.
C.5
Implementations of baseline methods
To ensure rigorous and fair comparisons against InstructSAM, baseline methods are implemented
or adapted as detailed below. Unless otherwise specified, publicly available pre-trained models and
official codebases are utilized, adhering to their recommended configurations.
Open-Vocabulary semantic segmentation methods
For SegEarth-OV [31] and GSNet [84], we
follow their prescribed inference procedures. This involves providing the target categories along with
an explicit â€œbackgroundâ€ category as input. To derive instance-level proposals from the pixel-level
segmentation maps produced by these models, we apply a standard connected component labeling
algorithm to the per-class binary masks. This allows evaluation using instance-based metrics (Mask
F1).
Open-ended methods
â€¢ SkysenseGPT [47]: We utilize the prompt â€œ[grounding]Analyze and describe every detail you can
identify in the image,â€ which is a representative of its training data, and set the maximum output
token limit to 5000.
â€¢ GeoPixel [65]: Following the illustrative examples provided in their publication, we employ the
prompt â€œCan you give a thorough description of this image, including interleaved segmentation
masks to highlight key objects?â€
â€¢ LAE-Label [54]: For mask proposing, we use SAM2-hiera-large with the same hyperparameters as
InstructSAM (see Appendix C.4). For label generation, we replace LAE-Labelâ€™s original InterVL2-
8B and reasoning prompt with the newer Qwen2.5-VL-7B, guided by a structured prompt for
open-ended category identification without explicit reasoning. This change of prompt improves
mF1 by 4.3% on NWPU-VHR-10 and reduces inference time by 92%. The prompt we use is as
follows:
23


--- Page 24 ---
{
"Persona": "You are an advanced AI model capable of understanding and
analyzing remote sensing images."
"Task": "Given an input satellite imagery, identify the most likely object
class visible in the image. Provide the result as a single class name."
"Instructions": [
"Consider this is a region of interest cropped from a larger remote
sensing image.",
"Focus on identifying the main object class visible in the cropped
region.",
"Be specific with your answer, using a single class name."],
"Output format": "\"class_name\"",
"Answer": ["Provide only the class name in quotes, without additional
explanations. If it is not recognized, output \"Unrecognized\" "]
}
Qwen2.5-VL
Given Qwen2.5-VLâ€™s [2] extensive pre-training on object detection tasks, we request
Qwen2.5-VL to generate a prompt template to perform IOD, and specify the output format following
the examples in their paper. Taking DIOR dataset as an example, the prompts for each task are as
follows.
â€¢ Open-vocabulary detection:
**Prompt:**
I need assistance with performing an remote sensing object detection task using
the provided category names:
[â€˜airplaneâ€™, â€˜airportâ€™, â€˜baseball fieldâ€™,
â€˜basketball courtâ€™, â€˜bridgeâ€™, â€˜chimneyâ€™, â€˜expressway service areaâ€™,
â€˜expressway toll stationâ€™, â€˜damâ€™, â€˜golf fieldâ€™, â€˜ground track fieldâ€™,
â€˜harborâ€™, â€˜overpassâ€™, â€˜shipâ€™, â€˜stadiumâ€™, â€˜storage tankâ€™, â€˜tennis courtâ€™,
â€˜train stationâ€™, â€˜vehicleâ€™, â€˜windmillâ€™].
Please provide the results in JSON format, where each object is represented as
follows:
- Each object should include a label (category name) and its corresponding
bounding box coordinates (in the format [x1, y1, x2, y2]).
**Example Output:**
[
{"label": "category_name", "bbox_2d": [x1, y1, x2, y2]},
{"label": "category_name", "bbox_2d": [x1, y1, x2, y2]},
...
]
â€¢ Open-ended detection:
**Prompt:**
Please detect all the visible objects in the satellite image and provide the
results in JSON format, where each object is represented as follows:
- Each object should include a label (category name) in singular form and its
corresponding bounding box coordinates (in the format [x1, y1, x2, y2]).
**Example Output:**
[
{"label": "category1", "bbox_2d": [x1, y1, x2, y2]},
{"label": "category2", "bbox_2d": [x1, y1, x2, y2]},
...
]
â€¢ Open-subclass detection (e.g., for â€œsports fieldâ€):
**Prompt:**
Please detect all the visible objects in the satellite image that belong to the
parent category **sports field** and provide the results in JSON format,
where each object is represented as follows:
24


--- Page 25 ---
- Each object should include a label (subcategory name) in singular form and its
corresponding bounding box coordinates (in the format [x1, y1, x2, y2]).
- If none of the objects belong to the parent category is visible, output a empty
empty list like [].
**Example Output:**
[
{"label": "subcategory1", "bbox_2d": [x1, y1, x2, y2]},
{"label": "subcategory2", "bbox_2d": [x1, y1, x2, y2]},
...
]
While incorporating detailed dataset-specific instructions into prompts enhances Qwen2.5-VLâ€™s
performance for object counting, such elaborations does not benefit direct detection. In fact, such
instructions reduce mF1 by 1% on NWPU-VHR-10 and have little effect on DIOR. The main paper
reports open-vocabulary detection results without these instructions.
D
Additional experiments
D.1
Comparison with zero-shot remote sensing object recognition methods on EarthInstruct
Direct comparison with some zero-shot remote sensing detection and segmentation methods (e.g.,
DesReg [86], OVA-DETR [75], ZoRI [21]) is difficult due to differing dataset splits, evaluation met-
rics, and limited reproducibility. Table 10 shows their reported mAP scores alongside InstructSAMâ€™s
mAPnc on novel classes. Notably, InstructSAM-Qwen matches or outperforms these methods on their
respective benchmarks (e.g., ZoRI on NWPU-VHR-10, OVA-DETR on DIOR). While these baselines
use confidence-based AP, InstructSAMâ€™s confidence-free APnc results underscore its robustness.
Table 10: Comparison with zero-shot remote sensing object detection and segmentation methods on
novel classes. â€˜-â€™ means data missing due to limited reproducibility. âˆ…indicates model lacking the
segmentation ability. APnc is reported for InstructSAM, while values in gray for other methods are
AP reported in their original papers.
Method
NWPU-VHR-10
DIOR val
DIOR test
Box AP
Mask AP
Box AP
Mask AP
Box AP
Mask AP
DesReg [86]
-
âˆ…
-
âˆ…
7.9
âˆ…
OVA-DETR [75]
-
âˆ…
7.1
âˆ…
-
âˆ…
ZoRI [21]
-
12.3
-
-
-
8.5
InstructSAM-Qwen
24.6
24.1
7.6
6.3
4.9
4.3
InstructSAM-GPT4o
26.8
26.5
11.2
8.9
6.6
5.4
D.2
Comparison with open-vocabulary detection methods on out-of-distribution datasets
To compare with open-vocabulary methods that include DIOR into training, we compare the zero-shot
performance on two out-of-distribution (OOD) datasets:
â€¢ xBD [18] is a large-scale building damage assessment dataset with spatial resolution below 0.8m.
The test set includes 933 pre- and post-disaster image pairs with instance-level building masks.
We use pre-disaster images to evaluate object counting and detection with the single category
â€œbuildingâ€.
â€¢ Aerial Maritime Drone Large [30] is a drone-based object detection dataset with 74 aerial
maritime images and 1,151 bounding boxes. Categories include â€œdocksâ€, â€œboatsâ€, â€œliftsâ€, â€œjetskisâ€,
and â€œcarsâ€. The entire dataset is used for zero-shot evaluation.
Table 11 shows that LAE-DINO, trained on LAE-1M (including â€œbuildingâ€), achieves the highest
detection F1-core (50.6) on xBD. On Aerial Maritime Drone, however, LAE-DINO detects only boats,
even struggling with docks and cars despite semantic similarity to harbor and vehicle categories in
training. Moreover, the confidence-based detectors (OWL, CASTDet, LAE-DINO) require dataset-
25


--- Page 26 ---
Table 11: Comparison with open-vocabulary detection methods on xBD and Aerial Maritime Drone
Large datasets.
Method
xBD
Aerial Maritime Drone Large
Best Thr
Cnt mF1
Det mF1
Best Thr
Cnt mF1
Det mF1
OWL [49]
0.02
53.0
31.5
0.26
28.3
21.5
CASTDet [37]
0.00
0.0
0.0
0.40
13.3
10.8
LAE-DINO [54]
0.08
79.6
50.6
0.30
13.0
12.5
InstructSAM-GPT4o
-
65.9
37.2
-
44.5
22.5
specific thresholds to optimize zero-shot results, highlighting InstructSAMâ€™s advantage in eliminating
threshold tuning.
D.3
Selection of prompt format
We conduct five independent experiments evaluating Qwen2.5-VL and GPT-4oâ€™s open-vocabulary
counting ability on NWPU-VHR-10 using Markdown and JSON prompts. Table 12 shows the mean
accuracies and standard deviations. Qwen2.5-VL exhibits only a 1.2% difference in mean accuracy
between Markdown and JSON, with zero standard deviation for both. GPT-4o shows slight variability:
82.38% Â± 0.94 (Markdown) and 82.68% Â± 0.39 (JSON). These results indicate that these LVLM
counters maintain relatively stable performance across different prompt formats.
Table 12: Mean accuracy and 1-sigma standard deviation for different prompt formats and models.
Model
Prompt Format
Mean Accuracy (%)
1-Sigma (Stand Deviation)
Qwen2.5-VL
Markdown
72.0
0.00
Qwen2.5-VL
JSON
73.2
0.00
GPT-4o
Markdown
82.4
0.94
GPT-4o
JSON
82.7
0.39
E
Broader impacts
InstructSAM demonstrates a strong ability to recognize objects in remote sensing imagery with
versatile instructions. It accelerates large-scale mapping applications, further supporting public
agencies and humanitarian organizations in critical areas such as poverty mapping, disaster response.
Its training-free paradigm and near-constant inference speed lower computational costs, decreasing
carbon emissions and broadening access to remote sensing applications in resource-limited settings.
InstructSAMâ€™s capabilities also raise privacy concerns as the spatial resolution of satellite imagery is
growing higher. Hallucinations or misclassifications, stemming from biases in pre-trained models
may produce unreliable outputs, require user verification to ensure accuracy.
F
Qualitative results
This section presents qualitative results of InstructSAM across open-vocabulary (Figure 9), open-
ended (Figure 10), and open-subclass settings (Figure 11). Additionally, we showcase its versatility
in handling instructions to achieve certain goals in Figure 12 and its generalization to natural images
in Figure 13.
26


--- Page 27 ---
F.1
Qualitative results in open-vocabulary setting
basketball court
tennis court
tennis court
vehicle
vehicle
baseball field
Ground Truth
baseball field
vehicle
vehicle
basketball court
tennis court
tennis court
InstructSAM
baseball field 0.35
tennis court 0.33
baseball field 0.43
basketball court 0.35
baseball field 0.34
vehicle 0.26
vehicle 0.27
baseball field 0.36
tennis court 0.50
OWLv2
baseball field
basketball court
tennis court
Qwen2.5-VL
SegEarth-OV
baseball field
expressway service area
golf field
expressway toll station
tennis court
ground track field
vehicle
stadium
windmill
chimney
storage tank
dam
bridge
harbor
overpass
storage tank
storage tank
storage tank
vehicle
vehicle
vehicle
Ground Truth
storage tank
vehicle
vehicle
vehicle
vehicle
storage tank
vehicle
storage tank
InstructSAM
vehicle 0.29
vehicle 0.26
vehicle 0.38
vehicle 0.32
vehicle 0.37
OWLv2
storage tank
storage tank
storage tank
Qwen2.5-VL
SegEarth-OV
baseball field
expressway service area
golf field
expressway toll station
tennis court
ground track field
train station
vehicle
basketball court
stadium
windmill
chimney
storage tank
dam
harbor
overpass
ship
vehicle
vehicle
vehicle
vehicle
overpass
vehicle
Ground Truth
overpass
vehicle
vehicle
vehicle
InstructSAM
vehicle 0.28
vehicle 0.33
vehicle 0.33
vehicle 0.37
OWLv2
expressway service area Qwen2.5-VL
SegEarth-OV
expressway service area
golf field
expressway toll station
train station
ground track field
vehicle
windmill
chimney
storage tank
dam
overpass
Figure 9: Qualitative results in open-vocabulary setting. While OWLv2 struggles to distinguish
remote sensing objects beyond vehicles, and SegEarth-OV fails to separate foreground objects from
the background, InstructSAM demonstrates superior performance in segmenting remote sensing
objects.
27


--- Page 28 ---
F.2
Qualitative results in open-ended setting
basketball court
tennis court
tennis court
vehicle
vehicle
Reference Annotation
pavilion
car
pavilion
basketball_court
car
tennis_court
tennis_court
InstructSAM
tennis court
tennis court
tennis court
tennis court
baseball diamond
baseball di
small vehicle
GeoPixel
car
SkysenseGPT
basketball court
tennis courts
parking lot
trees
Qwen-2.5VL
(a)
airplane
airplane
airplane
airplane
airplane
vehicle
Reference Annotation
airplane
airplane
airplane
airplane
airplane
vehicle
InstructSAM
plane
plane
plane
plane
plane
small vehicle
plane
GeoPixel
airplane
airplane
airplane
airplane
airplane
SkysenseGPT
airplane
airplane
airplane
airplane
airplane
Qwen2.5-VL
(b)
bridge
Reference Annotation
tree
tree
tree
tree
tree
tree
road
tree
tre
canal
tree
tree
building
road
tree
building
tree
building
tree
tree
bridge
tree
InstructSAM
bridge
GeoPixel
bridge
SkysenseGPT
bridge
building
road
water
trees
small_building
Qwen2.5-VL
(c)
Figure 10: Qualitative results in open-ended setting. Unlike GeoPixel and SkysenseGPT, which
fail to detect classes outside their training set, InstructSAM demonstrates its ability to recognize
diverse objects (e.g., pavilion in (a), tree in (c)) and provides more accurate bounding boxes and less
fragmented masks.
28


--- Page 29 ---
F.3
Qualitative results in open-subclass setting
vehicle
vehicle
vehicle
vehicle
vehicle
overpass
overpass
Reference Annotation
truck
car
car
car
car
car
InstructSAM
truck
car
car
car
car
Qwen2.5-VL
car 0.44
car 0.43
car 0.44
truck 0.23
truck 0.45
truck 0.25
car 0.29
car 0.26
car 0.43
GPT-4o+OWL
(a) Detecting the subclass of â€œmeans of transportâ€
tennis court
tennis court
tennis court
tennis court
tennis court
tennis court
baseball field
baseball field
ground track field
Reference Annotation
football field
trac
baseball field
baseball field
tennis court
tennis court
tennis court
tennis court
tennis court
tennis court
InstructSAM
football_field
baseball_field
tennis_court
tennis_court
tennis_court
Qwen2.5-VL
baseball field 0.26
baseball field 0.33
baseball field 0.26
baseball field 0.56
tennis court 0.58
baseball field 0.40
football field 0.49
GPT-4o+OWL
(b) Detecting the subclass of â€œsports fieldâ€
ground track field
basketba
basketba
tennis court
tenn
tennis co
tennis court
tennis court
tenni
tennis cou
tennis court
tennis court
tennis court
tennis court
tennis court
tennis court
tenni
tennis cou
tennis court
tennis court
tenni
tennis cou
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehicle
vehiclevehicle
baseball field
baseball field
Reference Annotation
tennis court
tennis co
tennis court
basketba
track
baseball field
tennis court
tennis court
tennis court
tennis court
tennis court
tennis court
tennis cou
tennis court
baseball field
soccer field
tennis court
InstructSAM
track
baseball_field
Qwen2.5-VL
tennis court 0.61
basketball
tennis court 0.24
basketball court 0.36
soccer field 0.33
baseball field 0.29
tennis court 0.33
soccer field 0.45
baseball field 0.25
baseball field 0.22
GPT-4o+OWL
(c) Detecting the subclass of â€œsports fieldâ€
Figure 11: Qualitative results in open-subclass setting. InstructSAM effectively identifies objects
within parent categories. In contrast, Qwen2.5-VL struggles with dense objects, and OWLv2 faces
challenges in classifying sports fields from a birdâ€™s-eye view.
29


--- Page 30 ---
F.4
Qualitative results on other instructions
Task: Detect the infrastructure necess-
ary  to  ensure that air ambulances can 
land effectively in the midst of urbana-
reas in emergencies.
Task: Detect a spot near recreational  
spaces that would be effective in hosti-
ng a highly interactive community spo-
rts gathering.
Task: What would be the ideal venue 
to host swimming classes for a sizable 
group, ensuring both safety and easy 
access?
Figure 12: Qualitative results on following versatile instructions. Samples are from EarthReason [32]
dataset. InstructSAM successfully recognizes objects based on implicit cues.
F.5
Qualitative results in natural images
Task: Detect all the electronic co-
mponents in image.
Task: Detect the dice whose let-
ters come before K in the picture.
Figure 13: Qualitative results on natural images. When equipped with generic CLIP models (DFN2B-
CLIP [13]), InstructSAM effectively recognizes objects in natural images.
30
