--- Page 1 ---
NC2C: Automated Convexification of Generic Non-Convex Optimization
Problems
Xinyue Peng1 Yanming Liu2 Yihan Cang1 Yuwei Zhang
Xinyi Wang Songhang Deng Jiannan Cao3
1Southeast University 2Zhejiang University
3Massachusetts Institute of Technology
xinyuepeng@seu.edu.cn, oceann24@zju.edu.cn
jiannan@mit.edu
Abstract
Non-convex optimization problems are perva-
sive across mathematical programming, engi-
neering design, and scientific computing, of-
ten posing intractable challenges for traditional
solvers due to their complex objective functions
and constrained landscapes. To address the in-
efficiency of manual convexification and the
over-reliance on expert knowledge, we propose
NC2C, an LLM-based end-to-end automated
framework designed to transform generic non-
convex optimization problems into solvable
convex forms using large language models.
NC2C leverages LLMs’ mathematical reason-
ing capabilities to autonomously detect non-
convex components, select optimal convexifi-
cation strategies, and generate rigorous convex
equivalents. The framework integrates sym-
bolic reasoning, adaptive transformation tech-
niques, and iterative validation, equipped with
error correction loops and feasibility domain
correction mechanisms to ensure the robust-
ness and validity of transformed problems. Ex-
perimental results on a diverse dataset of 100
generic non-convex problems demonstrate that
NC2C achieves an 89.3% execution rate and a
76% success rate in producing feasible, high-
quality convex transformations. This outper-
forms baseline methods by a significant margin,
highlighting NC2C’s ability to leverage LLMs
for automated non-convex to convex transfor-
mation, reduce expert dependency, and enable
efficient deployment of convex solvers for pre-
viously intractable optimization tasks.
1
Introduction
Non-convex optimization problems are ubiquitous
across various domains, including mathematical
programming, engineering design, machine learn-
ing, and scientific computing (Sui et al., 2024;
Mikhalevich et al., 2024). Convex optimization of-
fers stable and efficient solutions, especially when
closed-form solutions are available (Garatti and
Campi, 2025). However, many real-world opti-
mization problems are inherently non-convex, char-
acterized by complex objective functions, non-
linear constraints, and coupled variables that cre-
ate multiple local optima (Liu et al., 2024). Ad-
dressing these challenges often requires sophis-
ticated modeling, transformation, and relaxation
techniques to convert non-convex problems into
convex ones, typically depending heavily on expert
knowledge in advanced mathematical optimization,
problem-specific domain expertise, and transforma-
tion strategies (Jiang et al., 2024).
To reduce reliance on expert knowledge, re-
cent research shows that large language models
(LLMs) show significant potential in tackling com-
plex mathematical problems. For instance, LLMs
achieve 97.1% accuracy in solving mathemati-
cal challenges with zero-shot prompting on the
GSM8K dataset (Zhong et al., 2024).
In a se-
ries of reasoning and derivations involving com-
plex mathematical proof lemmas, a wide range of
large language model–based approaches, such as
Seed-Prover (Chen et al., 2025b,a), have demon-
strated the strong capability of LLMs in relational
reasoning and lemma-proof inference. They can
also address classical optimization problems, such
as linear regression and the traveling salesman
problem, through iterative methods (Yang et al.,
2024).
Moreover, LLMs can leverage existing
solvers like Gurobi and CVXPY for optimization
tasks (Diamond and Boyd, 2016). For example,
the gurobi.abs function enables direct modeling of
ℓ1-norm objectives, allowing LLMs to avoid redun-
dant auxiliary constraints and variables, thus accel-
erating the solving process (AhmadiTeshnizi et al.,
2023). However, these solvers primarily handle
convex problems, limiting their ability to solve non-
convex challenges directly, so LLMs must draw on
their own capabilities to transform and solve non-
convex challenges.
To address the existing gap in applying LLMs
to automated non-convex to convex transformation,
arXiv:2601.04789v1  [cs.CL]  8 Jan 2026


--- Page 2 ---
we introduce NC2C, an LLM-based framework that
leverages advancements in mathematical reasoning,
prompt engineering, and few-shot learning. The
main contributions of this paper are summarized as
follows.
• We propose the NC2C framework, an LLM-
based approach to address generic non-convex
optimization problems. The framework au-
tomatically detects non-convex components
and efficiently transforms them into convex
problems, significantly reducing reliance on
expert knowledge and enabling automated
non-convex problem solving across diverse
domains.
• We introduce Error Correction Loops (ECL)
and Feasibility Domain Correction (FDC) as
refinement stages within the NC2C frame-
work. These enhancements not only boost the
reliability and performance of NC2C’s solu-
tions but also improve other existing advanced
methods, demonstrating the framework’s ro-
bustness in handling complex optimization
transformations.
• Experimental
results
show
that
NC2C
achieves an execution rate of 89.3% and a
success rate of 76% on the GPT-4 model,
significantly outperforming baseline meth-
ods. These results highlight NC2C’s effective-
ness and robustness in solving generic non-
convex optimization problems through auto-
mated convexification.
2
Related Work
2.1
LLMs for Optimization Problems.
Recent studies have shown that large language mod-
els (LLMs) have significant potential in address-
ing complex optimization problems, both convex
and non-convex. LLMs have demonstrated suc-
cess in solving classical optimization problems like
linear regression and the traveling salesman prob-
lem using iterative methods (Yang et al., 2024).
Moreover, LLMs can leverage existing solvers like
Gurobi and CVXPY to handle convex optimiza-
tion tasks, accelerating the solving process (Ahma-
diTeshnizi et al., 2023). LLMOPT (JIANG et al.,
2025) and OptimAI (Thind et al., 2025) focus on
end-to-end frameworks that define and solve prob-
lems from scratch using autonomous agents. To
enhance reliability, OR-LLM-Agent (Zhang and
Luo, 2025) leverages reasoning-heavy models to
automate complex operations research modeling.
Meanwhile, OptiBench (Yang et al.) introduces
standardized benchmarks to systematically mea-
sure and improve LLMs’ modeling accuracy. How-
ever, these solvers primarily focus on convex prob-
lems, and LLMs must utilize their inherent capabil-
ities to transform and solve non-convex problems,
as direct solutions remain a challenge (Zhong et al.,
2024).
2.2
Tool Learning and Reasoning for LLMs.
The integration of LLMs with external tools rep-
resents a paradigm shift from closed-loop gen-
eration to open-environment interaction.
Be-
yond basic function calling (Ouyang et al., 2022),
frameworks like ReAct(Yao et al., 2022) and
ToolChain*(Zhuang et al., 2024) formalize the in-
terleaving of reasoning and acting. To enhance
long-horizon consistency, Tool-Planner (Qin et al.,
2023; Liu et al., 2025) introduces explicit global
planning, transitioning from reactive execution to
proactive strategy optimization.
To overcome the limitations of linear decision-
making, LATS (Zhou et al., 2024) incorporates
tree-search heuristics to traverse the strategy space,
while Reflexion enables iterative self-correction
through feedback loops, establishing a closed-loop
refinement mechanism for tool invocation (Schick
et al., 2023). Consequently, benchmarks (Ye et al.,
2025; Patil et al., 2024) have evolved from evaluat-
ing API selection accuracy to assessing high-order
dimensions . These paradigms are now permeating
multimodal GUI Agents (Zhang et al., 2025) and
specialized domains, with Reinforcement Learn-
ing (Qian et al., 2025) further aligning dynamic
reasoning policies.
3
Methodology
As illustrated in Figure 1, we propose the NC2C
framework, which leverages LLMs to automati-
cally transform generic non-convex optimization
problems into convex forms.
3.1
Problem Formulation and Mathematical
Modeling
Given a natural language problem description Dp
provided by the user, NC2C first constructs a for-
mal mathematical representation. The framework
employs Named Entity Recognition (NER) meth-
ods (Wang et al., 2023) to extract optimization-


--- Page 3 ---
Figure 1: Overview of the NC2C framework, which leverages LLMs to automatically transform generic non-convex
optimization problems into convex forms.
related entities from Dp, identifying decision vari-
ables, parameters, and their relationships.
Let V = {v1, v2, . . . , vn} denote the set of ex-
tracted variables, where each variable vi is associ-
ated with a type τi ∈{continuous, integer, binary}.
The variable extraction process is formalized as:
V = ExtractVariables(Dp)
(1)
τi = InferType(vi, Dp).
(2)
The objective function is identified by analyzing
optimization goals expressed in Dp. The frame-
work constructs the objective function f : Rn →R
by mapping extracted variables to their mathemat-
ical relationships. Constraint extraction identifies
inequality constraints gi(x) ≤0 and equality con-
straints hj(x) = 0 from the problem description.
The complete optimization problem P is formu-
lated as:
optimize
x∈X
f(x)
subject to
gi(x) ≤0,
i ∈I = {1, 2, . . . , m},
hj(x) = 0,
j ∈J = {1, 2, . . . , p},
x ∈X,
(3)
where x = [x1, x2, . . . , xn]T is the decision vector,
X denotes the variable domain constraints, m and
p represent the number of inequality and equality
constraints, respectively, and the optimization di-
rection (minimize or maximize) is determined from
Dp.
3.2
Non-Convex Detection and
Convexification
NC2C systematically identifies non-convex compo-
nents in P and applies appropriate convexification
strategies. The non-convex detection process an-
alyzes the structure of f(x), gi(x), and hj(x) to
identify problematic terms such as bilinear prod-
ucts, fractional expressions, logarithmic functions
in maximization contexts, or integer constraints.
Let N = {n1, n2, . . . , nk} denote the set of
identified non-convex components. For each com-
ponent nl ∈N, the framework selects a con-
vexification strategy σl from a strategy set Σ =
{SCA, SDR, Lagrangian, Substitution, . . .}.
The
strategy selection is formalized as:
σl = SelectStrategy(nl, P),
∀nl ∈N.
(4)
For non-convex objective functions, NC2C com-
monly employs Successive Convex Approximation


--- Page 4 ---
(SCA). Given a reference point x(0) ∈X, SCA
constructs a convex approximation ˜f(x; x(0)) us-
ing first-order Taylor expansion:
˜f(x; x(0)) = f(x(0))+∇f(x(0))T (x−x(0)), (5)
where ∇f(x(0)) denotes the gradient of f evalu-
ated at x(0). The convexified objective function is
then:
fc(x) = Convexify(f(x), {σl}k
l=1, x(0)).
(6)
Non-convex constraints are similarly trans-
formed. For inequality constraints gi(x) ≤0, the
convexified version becomes:
˜gi(x; x(0)) = gi(x(0))+∇gi(x(0))T (x−x(0)) ≤0,
(7)
resulting in the convexified problem:
Pc :
optimize
x∈X
fc(x)
subject to
˜gi(x; x(0)) ≤0,
i ∈I,
˜hj(x) = 0,
j ∈J ,
x ∈Xc,
(8)
where Xc represents the convexified variable do-
main.
3.3
Code Generation and Execution
Based on the convexified problem Pc, NC2C gen-
erates executable Python code C(Pc) that im-
plements the optimization model using appro-
priate solvers.
The framework selects a solver
S ∈{CVXPY, Gurobi, SCIPY} based on prob-
lem characteristics: CVXPY(Diamond and Boyd,
2016) for standard convex optimization, Gurobi
for large-scale linear and mixed-integer problems,
and SCIPY for general nonlinear convex problems.
The code generation process is formalized as:
C(Pc) = GenerateCode(Pc, S).
(9)
Upon execution, the code either succeeds and
returns a solution x∗, or fails and generates an error
report E. The execution status is captured by an
indicator function:
Ie(C(Pc)) =
(
1
if execution succeeds,
0
if execution fails.
(10)
When Ie = 1, the solver returns the optimal
solution:
x∗= arg min
x∈Xc
fc(x)
s.t. ˜gi(x) ≤0, ˜hj(x) = 0.
(11)
3.4
Error Correction Loop (ECL)
When code execution fails (Ie = 0), NC2C acti-
vates the Error Correction Loop (ECL) to iteratively
refine the generated code. In the k-th iteration
(k = 1, 2, . . . , K), ECL analyzes the error report
E(k) to identify failure causes, such as syntax errors,
dimension mismatches, or solver incompatibilities.
The error analysis and code correction process
is formalized as:
C(k+1)(Pc) = CorrectCode(C(k)(Pc), E(k), S, Pc),
(12)
where C(k) denotes the code at iteration k, and
CorrectCode(·) represents the LLM-based correc-
tion function that modifies the code based on error
analysis.
The ECL process terminates when either execu-
tion succeeds or the maximum iteration count K is
reached:
x∗=
(
Solve(C(k)(Pc), S)
if Ie(C(k)(Pc)),
ECL(C(1)(Pc), E, S, K)
otherwise,
(13)
where ECL(·) represents the iterative correction
process that attempts up to K iterations to produce
executable code.
3.5
Solution Validation
After obtaining a solution x∗from the convexified
problem Pc, NC2C validates its feasibility with
respect to the original problem P. This validation
is critical because convexification may relax con-
straints, potentially yielding solutions that violate
the original constraints.
The feasibility validation checks ∀i, j ∈I, J
whether x∗satisfies all original constraints:
If(x∗) =
(
1
if gi(x∗) ≤0 and hj(x∗) = 0 ,
0
otherwise.
(14)
Additionally, NC2C performs theoretical con-
sistency validation during the mathematical mod-
eling stage to ensure the constructed problem P
accurately reflects the original problem description
Dp. This validation checks four criteria: (1) align-
ment between formulas and problem description,
(2) completeness of variables and constraints, (3)
correctness of variable types, and (4) accuracy of
numerical values. The consistency metric is:
Ic(P, Dp) =
4
Y
i=1
H(ξi),
(15)


--- Page 5 ---
where ξi represents the i-th validation criterion and
H(·) is an indicator function returning 1 if the cri-
terion is satisfied and 0 otherwise.
3.6
Feasibility Domain Correction (FDC)
When the solution x∗fails feasibility validation
(If(x∗) = 0), NC2C activates the Feasibility Do-
main Correction (FDC) mechanism to iteratively
refine the solution. FDC operates in two stages
over a maximum of L iterations.
• Stage 1: Initial Value Adjustment (iterations
l = 1, 2, . . . , ⌊L/2⌋): FDC adjusts the initial
point x0 using a correction function that grad-
ually moves toward the feasible region:
x(l+1)
0
= x(l)
0 + α(l) · ∆x(l),
(16)
where α(l) ∈(0, 1] is the step size at itera-
tion l, and ∆x(l) is the correction direction
computed based on constraint violations. The
updated initial point is used to re-solve Pc,
yielding a new candidate solution x∗(l+1).
• Stage 2: Problem Re-convexification (iter-
ations l = ⌊L/2⌋+ 1, . . . , L): If Stage 1
fails to find a feasible solution, FDC reana-
lyzes the original problem P and applies al-
ternative convexification strategies. The re-
convexification process generates a new con-
vexified problem:
P(l)
c
= ReConvexify(P, {σ′
l}k′
l=1, x∗(l−1)),
(17)
where {σ′
l} represents alternative convexifica-
tion strategies. The new problem is solved to
obtain x∗(l).
The FDC process terminates when either a fea-
sible solution is found or the maximum iteration
count is reached:
x(final) =
(
x∗(l)
if If(x∗(l)) = 1 for some l ≤L,
x∗(L)
otherwise.
(18)
4
Experimental Setup
4.1
Dataset
To comprehensively evaluate the performance of
NC2C in automated non-convex to convex trans-
formation, we conduct experiments on four diverse
datasets that cover different aspects of optimization
problem solving. We choose NL4Opt (Ramamon-
jison et al., 2023), NLP4LP (AhmadiTeshnizi et al.,
2023), ComplexOR (Xiao et al., 2023), WireOpt
(Peng et al., 2025) as our experiment datasets. De-
tailed of the datasets are list in the Appendix C.
4.2
Baseline Methods
To comprehensively evaluate NC2C’s performance,
we compare it against state-of-the-art baseline
methods that represent different approaches to op-
timization problem solving with LLMs, includ-
ing Reflexion (Shinn et al., 2023), Chain-of-
Experts (Xiao et al., 2023), OptiMUS (Ahma-
diTeshnizi et al., 2023), Vanilla LLMs. More de-
tailed are shown in the Appendix B.
4.3
Evaluation Metrics
The methods are evaluated based on two pri-
mary metrics:
Success Rate and Execution
Rate(AhmadiTeshnizi et al., 2023). For each opti-
mization problem P ∈D, a total of N = 10 evalu-
ations are conducted using both models (GPT-5.1
and Qwen3-235B-A22B), and the rates are com-
puted as the average of these evaluations.
• Success Rate is defined as the proportion of
outputs that yield an optimal solution based
on code execution and lie within the feasible
domain while satisfying all constraints, ex-
pressed as
Success Rate =
1
|D|
X
P∈D
 
1
N
N
X
i=1
VP
!
,
(19)
where VP = 1 indicates the solution of prob-
lem P is feasible, and VP = 0 otherwise.
• Execution Rate measures the proportion of
generated code that successfully executes and
produces outputs, expressed as
Execution Rate =
1
|D|
X
P∈D
 
1
N
N
X
i=1
QP
!
,
(20)
where QP = 1 represents the code of prob-
lem P is successful executed, and QP = 0
otherwise.
5
Experiment Results
5.1
Main Experiments
We evaluate NC2C and all baseline methods across
the four datasets using GPT-5.1 and Qwen3-235B-


--- Page 6 ---
Table 1: Main experimental results on NL4Opt, NLP4LP, ComplexOR, and NC2C datasets. We report Success
Rate (SR, %) and Execution Rate (ER, %) for each dataset. Results are averaged over 10 independent runs for each
problem.
Model
Method
NL4Opt
NLP4LP
ComplexOR
WireOpt
SR
ER
SR
ER
SR
ER
SR
ER
GPT-5.1
Vanilla
85.0
90.2
78.5
85.3
52.8
60.0
73.0
80.0
Chain-of-Experts
88.3
92.5
82.0
88.2
58.3
66.7
77.0
84.0
OptiMUS
90.2
94.0
85.4
90.5
63.9
72.2
80.0
87.0
Reflexion
87.5
92.8
81.2
88.0
55.6
64.4
75.0
83.0
NC2C
94.5
96.7
90.5
94.2
72.2
80.6
87.0
93.0
Qwen3-235B-A22B
Vanilla
78.5
85.2
72.3
80.1
45.0
52.8
68.0
75.0
Chain-of-Experts
82.3
88.5
76.5
83.2
50.0
58.3
72.0
79.0
OptiMUS
85.0
90.5
80.2
87.0
55.6
63.9
75.0
82.0
Reflexion
80.8
88.0
74.5
82.5
48.3
56.1
70.0
78.0
NC2C
91.2
94.4
86.5
91.0
66.7
75.0
82.0
89.0
Figure 2: Impact of omitting key components of the
NC2C framework on success and execution rates across
GPT-5.1 and Qwen3-235B-A22B.
A22B as the underlying LLMs. For each optimiza-
tion problem P ∈D, we conduct N = 10 inde-
pendent evaluations, and all reported metrics are
averaged over these evaluations. Table 1 presents
the comprehensive experimental results.
NC2C achieves superior performance across
all datasets and models. As shown in Table 1,
NC2C consistently outperforms all baseline meth-
ods on both GPT-5.1 and Qwen3-235B-A22B. On
the NL4Opt dataset with GPT-5.1, NC2C achieves
a success rate of 94.5% and execution rate of
96.7%, significantly surpassing OptiMUS (90.2%
SR, 94.0% ER), Chain-of-Experts (88.3% SR,
92.5% ER), Reflexion (85.0% SR, 90.2% ER), and
Vanilla (87.5% SR, 92.8% ER). On Qwen3-235B-
A22B, NC2C achieves 91.2% success rate and
94.4% execution rate on NL4Opt, outperforming
all baselines. The performance gap is even more
pronounced on the more challenging ComplexOR
dataset, where NC2C with GPT-5.1 achieves 72.2%
success rate compared to OptiMUS’s 63.9%, and
with Qwen3-235B-A22B achieves 66.7% com-
pared to OptiMUS’s 55.6%, demonstrating NC2C’s
effectiveness in handling complex non-convex op-
timization scenarios.
NC2C shows robust performance across di-
verse problem domains. On the NLP4LP dataset,
which contains verbose problem descriptions and
complex constraints, NC2C achieves 90.5% suc-
cess rate with GPT-5.1 and 86.5% with Qwen3-
235B-A22B, outperforming OptiMUS by 5.1% and
6.3%, respectively. On our curated NC2C dataset,
which spans multiple domains including mathemat-
ical programming, engineering design, and scien-
tific computing, NC2C achieves 87.0% success rate
with GPT-5.1 and 82.0% with Qwen3-235B-A22B,
demonstrating its generalizability across different
optimization problem types and model architec-
tures.
5.2
Ablation Studies
The NC2C framework consists of multiple key pro-
cesses, assessing the impact of removing these com-
ponents on model performance. We will analyze
the following scenarios:
• NC2C: The complete framework proposed in
this paper.
• o - convex: The convexification process is
removed, and the model directly handles non -
convex mathematical problems.


--- Page 7 ---
Figure 3: Success and execution rates for NC2C under different maximum iteration counts for ECL and FDC.
• o - ECL: ECL is omitted, and no error correc-
tion is performed on erroneous code.
• o - FDC: FDC is omitted, and no re - solving
is performed for solutions that do not satisfy
feasibility constraints.
As shown in the Fig.2, o-convex significantly
reduces both the success and execution rates,
highlighting the importance of convexification
within the NC2C framework. Specifically, GPT-
5.1’s success rate drops from 0.945 to 0.15, re-
flecting an 84.1% decline, while the execution rate
decreases from 0.967 to 0.616. With Qwen3-235B-
A22B, the success rate drops from 0.912 to 0.15,
reflecting an 83.6% decline. This highlights the
critical need for convexification to transform non-
convex problems into solvable forms.
o-ECL negatively impacts the execution rate,
which subsequently lowers the success rate. For
GPT-5.1, the execution rate drops from 0.967 to
0.70, a 27.6% decrease, and the success rate de-
clines from 0.945 to 0.75, reflecting a 20.6% re-
duction. With Qwen3-235B-A22B, the execution
rate drops from 0.944 to 0.65, a 31.1% decrease,
and the success rate declines from 0.912 to 0.60,
reflecting a 34.2% reduction. This decrease in exe-
cution rates leads to an increase in failed attempts
to generate correct solutions, ultimately harming
overall success rate.
o-FDC primarily decreases the success rate,
emphasizing its role in ensuring solution feasi-
bility. Experiments show that without FDC, GPT-
5.1’s success rate drops to 0.20, a 78.8% reduction,
while Qwen3-235B-A22B’s falls to 0.15, an 83.6%
decrease. Although GPT-5.1’s execution rate re-
mains at 0.80, the lack of FDC means that many
solutions may not be within feasible regions, ren-
dering them ineffective.
5.3
Maximum Iteration Count Analysis of
ECL and FDC
The previous section highlights the significant roles
that ECL and FDC play in the performance of the
NC2C framework. Therefore, this section delves
deeper into their maximum iteration counts K and
L, respectively, examining their effects on the suc-
cess and execution rates, as illustrated in Fig.3.
ECL effectively increases success and execu-
tion rates in early iterations. In terms of success
rate, ECL quickly identifies and corrects key er-
rors in the early iterations, significantly enhancing


--- Page 8 ---
Figure 4: Average Running Time of NC2C and Baseline Methods on GPT-5.1 and Qwen3-235B.
overall performance. On the NL4Opt dataset with
GPT-5.1, the success rate increases from 70% to
94.5%; with Qwen3-235B-A22B, it increases from
60% to 91.2%, demonstrating ECL’s effectiveness
across different model capabilities.
FDC demonstrates notable improvements in
success rate and maintains steady execution rate
enhancements with increasing iterations. For
the success rate on ComplexOR dataset, GPT-5.1
improves from 45% to 72.2%, and Qwen3-235B-
A22B rises from 35% to 66.7%, demonstrating
FDC’s strong effectiveness in correcting infeasible
solutions. This improvement results from FDC’s
ability to adjust based on the number of iterations
L; it provides corrections during initial optimiza-
tion attempts and reanalyzes the problem after
reaching
 L
2

. In terms of execution rates, GPT-
5.1’s execution rate increases from 75% to 80.6%
on ComplexOR, while Qwen3-235B-A22B rises
from 70% to 75.0%.
5.4
Running Time Analysis of NC2C
To explore whether the running time of NC2C is
within an acceptable range while maintaining high
success and execution rates, we conduct a compar-
ative experiment on program running times across
all baseline methods.
As shown in Fig.4, NC2C demonstrates ex-
cellent efficiency across different models. With
GPT-5.1, NC2C achieves an average running time
of 22.5s per problem, which is faster than Op-
tiMUS (28.5s), Reflexion (26.8s), and Chain-of-
Experts (27.2s), while only being slightly slower
than Vanilla (20.5s). This represents a 21.1% re-
duction compared to OptiMUS, a 16.0% reduc-
tion compared to Reflexion, and a 17.3% reduction
compared to Chain-of-Experts. On Qwen3-235B,
NC2C also demonstrates superior efficiency.
Compared with baseline methods,
NC2C
achieves superior performance with faster running
times. While Vanilla requires the least computa-
tional time due to its direct approach without addi-
tional processing, NC2C’s slightly longer running
time is justified by its significantly higher success
and execution rates. More importantly, NC2C out-
performs all other methods in both performance
metrics and computational efficiency, demonstrat-
ing that its integrated convexification, error correc-
tion, and feasibility verification mechanisms are
not only effective but also efficiently implemented.
6
Conclusion
In this paper, we propose NC2C, a groundbreaking
LLM-based framework that automatically trans-
forms generic non-convex optimization problems
into convex forms.
Our experimental analysis
shows that NC2C achieves an execution rate of
89.3% and a success rate of 76% on the GPT-4
model, significantly outperforming baseline meth-
ods. The framework’s success hinges on its inte-
grated components as convexification, error cor-
rection, and feasibility checking, each of which
plays a critical role in enhancing solution quality
and overall robustness. As Language models that
combine real-time feedback could achieve more ef-
ficient optimization, NC2C sets a new standard for
automated non-convex to convex transformation,
demonstrating its capabilities in handling diverse
optimization problems across various domains.


--- Page 9 ---
Limitations
While NC2C demonstrates significant improve-
ments in automated non-convex to convex trans-
formation, several limitations should be acknowl-
edged. The framework’s performance is inherently
dependent on the underlying LLM’s mathematical
reasoning capabilities, and its effectiveness may
vary with different model architectures or when ap-
plied to domains with highly specialized terminol-
ogy. Although NC2C achieves competitive running
times compared to sophisticated baseline methods,
the multi-stage processing pipeline introduces com-
putational overhead that may limit its suitability
for resource-constrained environments or real-time
applications. The convexification process involves
relaxation and approximation techniques that may
introduce suboptimality, and while NC2C ensures
feasibility through the Feasibility Domain Correc-
tion mechanism, the gap between the optimal so-
lution of the original non-convex problem and the
convexified problem is not always quantifiable. Ad-
ditionally, the Error Correction Loop may strug-
gle with fundamental mathematical errors, and the
framework’s decision-making process for selecting
convexification strategies may lack transparency,
which could affect trust in critical applications.
Ethical Considerations
The development and deployment of automated
optimization frameworks like NC2C raise several
ethical considerations. Users should understand
the framework’s limitations and not rely solely on
automated solutions for critical decisions without
appropriate human oversight, particularly in high-
stakes scenarios involving resource allocation or
decision-making systems. While NC2C operates
on mathematical problem formulations, potential
biases may arise from the training data used to de-
velop underlying LLMs or from problem selection
in evaluation datasets. The framework’s reliance
on advanced LLMs and computational resources
may create barriers to access for researchers with
limited resources, and the computational require-
ments contribute to energy consumption and carbon
emissions. When applied to problems involving
sensitive data, users must ensure appropriate data
handling practices, as the framework’s interaction
with LLM APIs may involve transmitting problem
descriptions to external services. We encourage the
research community to develop evaluation metrics
that assess not only technical performance but also
fairness, transparency, and ethical implications of
automated optimization systems.
References
Ali AhmadiTeshnizi, Wenzhi Gao, and Madeleine Udell.
2023. Optimus: Optimization modeling using mip
solvers and large language models. arXiv preprint
arXiv:2310.06116.
Jiangjie Chen, Wenxiang Chen, Jiacheng Du, Jinyi Hu,
Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin,
Chenggang Li, Wenlei Shi, et al. 2025a. Seed-prover
1.5: Mastering undergraduate-level theorem prov-
ing via learning from experience. arXiv preprint
arXiv:2512.17260.
Luoxin Chen, Jinming Gu, Liankai Huang, Wenhao
Huang, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing
Jin, Chenggang Li, Kaijing Ma, et al. 2025b. Seed-
prover: Deep and broad reasoning for automated
theorem proving. arXiv preprint arXiv:2507.23726.
Steven Diamond and Stephen Boyd. 2016. Cvxpy: A
python-embedded modeling language for convex op-
timization. Journal of Machine Learning Research,
17(83):1–5.
Simone Garatti and Marco C Campi. 2025. Non-convex
scenario optimization. Mathematical Programming,
209(1):557–608.
Y. Thomas Hou, Yi Shi, and Hanif D. Sherali. 2014.
Convex programming and applications, page 38–60.
Cambridge University Press.
The MathWorks Inc. 2022.
Matlab version: 9.13.0
(r2022b).
Caigao JIANG, Xiang Shu, Hong Qian, Xingyu Lu,
JUN ZHOU, Aimin Zhou, and Yang Yu. 2025. LL-
MOPT: Learning to define and solve general opti-
mization problems from scratch. In The Thirteenth
International Conference on Learning Representa-
tions.
Yibo Jiang, Goutham Rajendran, Pradeep Kumar
Ravikumar, Bryon Aragam, and Victor Veitch. 2024.
On the origins of linear representations in large lan-
guage models. In International Conference on Ma-
chine Learning, pages 21879–21911. PMLR.
Chengchang Liu, Chaowen Guan, Jianhao He, and John
Lui. 2024. Quantum algorithms for non-smooth non-
convex optimization. Advances in Neural Informa-
tion Processing Systems, 37:35288–35312.
Yanming Liu, Xinyue Peng, Jiannan Cao, Shi Bo, Yuwei
Zhang, Xuhong Zhang, Sheng Cheng, Xun Wang,
Jianwei Yin, and Tianyu Du. 2025. Tool-planner:
Task planning with clusters across multiple tools. In
The Thirteenth International Conference on Learning
Representations.


--- Page 10 ---
VS Mikhalevich, AM Gupal, and VI Norkin. 2024.
Methods of nonconvex optimization. arXiv preprint
arXiv:2406.10406.
OpenAI. 2025. Gpt-5.1: A smarter, more conversa-
tional chatgpt. https://openai.com/index/
gpt-5-1/. Accessed: 2025-11-12.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex
Ray, John Schulman, Jacob Hilton, Fraser Kelton,
Luke E. Miller, Maddie Simens, Amanda Askell, Pe-
ter Welinder, Paul Francis Christiano, Jan Leike, and
Ryan J. Lowe. 2022. Training language models to
follow instructions with human feedback.
ArXiv,
abs/2203.02155.
Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E
Gonzalez. 2024.
Gorilla: Large language model
connected with massive apis. Advances in Neural
Information Processing Systems, 37:126544–126565.
Xinyue Peng, Yanming Liu, Yihan Cang, Chaoqun Cao,
and Ming Chen. 2025. Llm-optira: Llm-driven opti-
mization of resource allocation for non-convex prob-
lems in wireless communications. arXiv preprint
arXiv:2505.02091.
Cheng Qian, Emre Can Acikgoz, Qi He, Hongru
WANG, Xiusi Chen, Dilek Hakkani-Tür, Gokhan
Tur, and Heng Ji. 2025. ToolRL: Reward is all tool
learning needs. In The Thirty-ninth Annual Confer-
ence on Neural Information Processing Systems.
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan
Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,
Bill Qian, et al. 2023. Toolllm: Facilitating large
language models to master 16000+ real-world apis.
In The Twelfth International Conference on Learning
Representations.
Rindranirina Ramamonjison, Timothy Yu, Raymond
Li, Haley Li, Giuseppe Carenini, Bissan Ghaddar,
Shiqi He, Mahdi Mostajabdaveh, Amin Banitalebi-
Dehkordi, Zirui Zhou, et al. 2023. Nl4opt competi-
tion: Formulating optimization problems based on
their natural language descriptions. In NeurIPS 2022
competition track, pages 189–203. PMLR.
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta
Raileanu, Maria Lomeli, Eric Hambro, Luke Zettle-
moyer, Nicola Cancedda, and Thomas Scialom. 2023.
Toolformer: Language models can teach themselves
to use tools. Advances in Neural Information Pro-
cessing Systems, 36:68539–68551.
Kaiming Shen and Wei Yu. 2018. Fractional program-
ming for communication systems—part i: Power con-
trol and beamforming. IEEE Transactions on Signal
Processing, 66(10):2616–2630.
Noah Shinn, Federico Cassano, Ashwin Gopinath,
Karthik Narasimhan, and Shunyu Yao. 2023. Re-
flexion: Language agents with verbal reinforcement
learning. Advances in Neural Information Process-
ing Systems, 36:8634–8652.
Xiaomeng Sui, Zehao He, Daping Chu, and Liangcai
Cao. 2024.
Non-convex optimization for inverse
problem solving in computer-generated holography.
Light: Science & Applications, 13(1):158.
Raghav Thind, Youran Sun, Ling Liang, and Haizhao
Yang. 2025. Optimai: Optimization from natural
language using llm-powered ai agents. arXiv preprint
arXiv:2504.16918.
Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang,
Fei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang.
2023. Gpt-ner: Named entity recognition via large
language models. arXiv preprint arXiv:2304.10428.
Ziyang Xiao, Dongxiang Zhang, Yangjun Wu, Lilin Xu,
Yuan Jessica Wang, Xiongwei Han, Xiaojin Fu, Tao
Zhong, Jia Zeng, Mingli Song, et al. 2023. Chain-
of-experts: When llms meet complex operations re-
search problems. In The twelfth international confer-
ence on learning representations.
An Yang, Anfeng Li, Baosong Yang, Beichen Zhang,
Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,
Chengen Huang, Chenxu Lv, et al. 2025. Qwen3
technical report. arXiv preprint arXiv:2505.09388.
Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao
Liu, Quoc V Le, Denny Zhou, and Xinyun Chen.
2024.
Large language models as optimizers.
In
The Twelfth International Conference on Learning
Representations.
Zhicheng Yang, Yiwei Wang, Yinya Huang, Zhijiang
Guo, Wei Shi, Xiongwei Han, Liang Feng, Linqi
Song, Xiaodan Liang, and Jing Tang. Optibench
meets resocratic: Measure and improve llms for opti-
mization modeling. In The Thirteenth International
Conference on Learning Representations.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik R Narasimhan, and Yuan Cao. 2022.
React: Synergizing reasoning and acting in language
models. In The eleventh international conference on
learning representations.
Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang,
Yilong Wu, Sixian Li, Xiaoran Fan, Shihan Dou, Tao
Ji, Qi Zhang, et al. 2025. Tooleyes: Fine-grained eval-
uation for tool learning capabilities of large language
models in real-world scenarios. In Proceedings of
the 31st international conference on computational
linguistics, pages 156–187.
Bowen Zhang and Pengcheng Luo. 2025. Or-llm-agent:
Automating modeling and solving of operations re-
search optimization problem with reasoning large
language model. arXiv preprint arXiv:2503.10009.
Chaoyun Zhang, Shilin He, Liqun Li, Si Qin, Yu Kang,
Qingwei Lin, Saravan Rajmohan, and Dongmei
Zhang. 2025. API agents vs. GUI agents: Diver-
gence and convergence. In ICML 2025 Workshop on
Computer Use Agents.


--- Page 11 ---
Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu,
Liang Ding, Bo Du, and Dacheng Tao. 2024. Achiev-
ing> 97% on gsm8k: Deeply understanding the prob-
lems makes llms perfect reasoners. arXiv preprint
arXiv:2404.14963.
Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman,
Haohan Wang, and Yu-Xiong Wang. 2024.
Lan-
guage agent tree search unifies reasoning, acting, and
planning in language models. In International Con-
ference on Machine Learning, pages 62138–62160.
PMLR.
Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra,
Victor Bursztyn, Ryan A Rossi, Somdeb Sarkhel,
and Chao Zhang. 2024. Toolchain*: Efficient action
space navigation in large language models with a*
search. In The Twelfth International Conference on
Learning Representations.
A
Broader Impact And Limitations
Broader Impact.The proposed NC2C method sig-
nificantly improves success and execution rates in
generic non-convex optimization by introducing
automated non-convex to convex transformations
and multi-stage solution strategies. We believe
that by integrating more optimization techniques,
such as reinforcement learning and online learning,
large-scale models can further enhance their solv-
ing capabilities in complex scenarios. In particular
in dynamic environments, models that combine
real-time feedback could achieve more efficient op-
timization. Furthermore, the current model can be
further integrated with information retrieval and
incremental learning techniques to handle more
complex tasks with a large amount of noise or un-
certainty in practical applications. We believe that
the fusion of these technologies will provide high-
quality solutions for the broader optimization field,
advancing the development and application of au-
tomated convexification techniques.
B
Baselines
Reflexion (Shinn et al., 2023) leverages LLMs’
self-reflection capabilities to improve reasoning
performance. The method enables the model to
critique its own outputs, identify errors, and re-
generate corrected formulations through iterative
refinement. Reflexion has demonstrated significant
improvements in optimization tasks, achieving ac-
curacy improvements from 78.8% to 92.3% on the
NL4Opt benchmark. We adapt Reflexion for non-
convex to convex transformation by incorporating
reflection steps that evaluate the correctness of con-
vexification strategies.
Chain-of-Experts (Xiao et al., 2023) employs
a multi-expert architecture where different expert
models are chained together to handle complex rea-
soning tasks. Each expert specializes in different
aspects of the problem-solving process, enabling
collaborative problem decomposition and solution
generation. This approach has shown effectiveness
in complex operations research scenarios, achiev-
ing 68.8% success rate on NL4Opt and 40.5%
on ComplexOR datasets. We configure Chain-of-
Experts with experts specialized in mathematical
modeling, convexification, and code generation.
OptiMUS (AhmadiTeshnizi et al., 2023) is an
LLM-based agent designed to automatically formu-
late and solve Mixed Integer Linear Programming
(MILP) problems from natural language descrip-


--- Page 12 ---
tions. The framework can develop mathematical
models, write and debug solver code, and eval-
uate generated solutions. OptiMUS has demon-
strated superior performance, achieving over 12%
improvement on easy datasets and over 8% on chal-
lenging datasets including NLP4LP compared to
existing state-of-the-art methods. We extend Opti-
MUS to handle non-convex problems by incorpo-
rating convexification steps.
Vanilla represents the direct application of the
underlying the base LLM models (GPT-5.1 (Ope-
nAI, 2025) or Qwen3-235B-A22B (Yang et al.,
2025)) to solve optimization problems without
additional frameworks or specialized prompting
strategies. This baseline evaluates the raw capa-
bility of advanced LLMs in handling non-convex
to convex transformation tasks, providing a strong
comparison point for our framework.
C
Dataset
NL4Opt (Ramamonjison et al., 2023) is a bench-
mark dataset from NL4Opt competition, designed
to evaluate the ability to automatically convert natu-
ral language descriptions of optimization problems
into solver-ready code. The original dataset con-
tains 289 instances, primarily consisting of Linear
Programming (LP) and Mixed Integer Linear Pro-
gramming (MILP) problems. After data cleaning
and validation, we use 214 instances that contain
well-formed non-convex optimization problems re-
quiring convexification.
NLP4LP (AhmadiTeshnizi et al., 2023) is pro-
vided by the OptiMUS benchmark, containing 344
linear and integer programming problems with ver-
bose descriptions and multi-dimensional parame-
ters. The dataset is particularly challenging due
to its lengthy problem descriptions and complex
constraint structures. After filtering for non-convex
problems that require transformation, we retain 178
instances for evaluation.
ComplexOR (Xiao et al., 2023) initially con-
tains 37 challenging optimization problems in-
volving complex operations research scenarios, in-
cluding combinatorial optimization tasks. This
dataset focuses on evaluating LLMs’ reasoning
and problem-solving capabilities in complex op-
timization contexts. After data validation, we use
18 instances that involve non-convex components
requiring convexification.
WireOpt (Peng et al., 2025) is specifically
designed for non-convex to convex transforma-
tion tasks. We extract 50 non-convex optimiza-
tion problems from authoritative sources includ-
ing textbooks, research papers, and optimization
libraries (Shen and Yu, 2018; Hou et al., 2014; Inc.,
2022). These problems span various domains in-
cluding mathematical programming, engineering
design, and scientific computing in a total of 100
instances. This dataset serves as a complementary
evaluation set to assess NC2C’s performance on
diverse non-convex optimization challenges.
D
Running time of each component
within the NC2C framework
Figure 5: Pie chart of the time proportion of each part.
Building on the running time analysis in last
subsection, we continue to analyze the time distri-
bution and contributions of the internal components
within the NC2C framework.
When the NC2C framework runs, convex prob-
lem transformation, mathematical model genera-
tion, and feasibility verification each take up more
than 23% of the total time. The convex problem
transformation part, accounting for about 30.4% of
the time, involves in - depth analysis and conver-
sion of complex non - convex problems into convex
ones, entailing numerous complex mathematical
operations and logical derivations when handling
complex function structures and constraints. The
mathematical model generation part, consuming
around 26.4% of the time, demands precise under-
standing of input problems and strict transforma-
tion of natural language descriptions into mathe-
matical models. The pursuit of accuracy in identify-
ing entities, relationships, and semantics in natural
language for constructing accurate models leads to
significant time investment. The feasibility verifica-


--- Page 13 ---
tion part, taking approximately 23.8% of the time,
has a strict verification mechanism that requires
multiple solution checks and possible adjustments.
The detailed evaluation of various aspects of the so-
lutions to ensure they meet constraints and practical
needs results in increased time consumption.
As can also be seen from Figure 3 in Section
4 of the main text, in the absence of convex prob-
lem transformation (o - convex), the success rate of
GPT - 4 plummets from 0.76 to 0.15, a significant
drop of 80.3%, and the execution rate decreases
from 0.893 to 0.616. This vividly demonstrates
the crucial role of convex problem transformation
in converting non - convex problems into solvable
forms, and also explains why, despite the signifi-
cant time consumption, this part is indispensable
in the NC2C framework.
Similarly, the high requirements of the mathe-
matical model generation part for understanding
and transforming input problems, as well as the
strict verification mechanism of the feasibility veri-
fication part, result in both of these parts consuming
a relatively large amount of time within the frame-
work. However, these components work together.
Although they increase the running time, they bring
about a performance improvement that far exceeds
the time cost for the framework, ensuring the effi-
ciency and reliability of NC2C in solving complex
problems.
In summary, different in time - consumption,
these parts collaborate. They achieve performance
gains far beyond the time cost, showing the frame-
work’s effectiveness and superiority in solving com-
plex non - convex optimization problems.
E
Pseudocode and Explanation of NC2C
Optimization Process
In the domain of optimization problem - solving,
the NC2C optimization process offers an efficient
approach. The following pseudocode outlines the
core steps of this process, followed by a brief ex-
planation of each significant step.


--- Page 14 ---
Algorithm 1 NC2C Optimization Process
1: Input: Optimization problem description
2: Output: Success flag, Execute flag, Solution result
3: success_flag ←0
4: execute_flag ←0
5: math_formulation ←GenerateMathFormulation(problem)
6: convex_formulation ←ConvertToConvex(math_formulation)
7: code ←GenerateCode(convex_formulation)
8: solution, execute_status ←ExecuteCode(code)
9: if execute_status = Success then
10:
execute_flag ←1
11:
feasibility ←CheckFeasibility(math_formulation, solution)
12:
if feasibility = Feasible then
13:
success_flag ←1
14:
else
15:
for attempt ←1 to MaxAttempts do
16:
new_code ←RefineCode(math_formulation, code, attempt, convex_formulation)
17:
new_solution, new_execute_status ←ExecuteCode(new_code)
18:
if new_execute_status = Success then
19:
new_feasibility ←CheckFeasibility(math_formulation, new_solution)
20:
if new_feasibility = Feasible then
21:
solution ←new_solution
22:
success_flag ←1
23:
break
24:
end if
25:
end if
26:
end for
27:
end if
28: end if
29: Output: success_flag, execute_flag, solution
E.1
Explanation of the Pseudocode
The entire process aims to solve the optimization problem effectively and obtain a feasible solution, with
the status flags indicating the success of different stages of the process.
1. Initialization:
• success_flag and execute_flag are initialized to 0. These flags will be updated later
to indicate the success of the overall process and the code execution respectively.
2. Problem Formulation:
• GenerateMathFormulation(problem): Transforms the input natural - language opti-
mization problem into a mathematical formula.
• ConvertToConvex(math_formulation): Converts the obtained mathematical formula
into a convex optimization form, which is easier to solve.
• GenerateCode(convex_formulation): Generates executable code based on the con-
vex optimization formula.
3. Code Execution and Solution Checking:
• ExecuteCode(code): Runs the generated code and returns the solution and the execution
status.


--- Page 15 ---
• CheckFeasibility(math_formulation, solution): Checks if the obtained so-
lution is feasible according to the original mathematical formulation.
4. Iterative Refinement:
• If the initial solution is not feasible, RefineCode is called up to MaxAttempts times to
improve the code and try to find a feasible solution.
The effectiveness of these functions mainly relies on well - designed prompts, which will be detailed
in the next section of the appendix.
F
Details of Key Prompt Sections
In the process of solving optimization problems using LLMs, prompts play a crucial role. They serve
as the interface between user problems and model outputs. Well-designed prompts can guide the model
to generate expected results. This appendix will elaborate on the key prompts used in each stage from
problem input to convex optimization and then to solution. By presenting and analyzing the original text
of the prompts, we will reveal their design ideas and working mechanisms.
F.1
Overall Process Overview
The entire optimization problem-solving process mainly includes the following key steps:
1. Problem Input: Users provide descriptions of actual optimization problems, which will be used as
the basic input for subsequent stages.
2. Mathematical Formula Generation: Utilize carefully designed prompts to guide the LLM to
transform the problem described in natural language into a mathematical formula and clarify the
optimization objective (maximization or minimization).
3. Convex Optimization Conversion: Through specific prompts, prompt the LLM to convert the
generated non-convex mathematical formula into a convex form for subsequent solution.
4. Code Generation: Based on the convex optimization formula, use corresponding prompts to let the
LLM generate executable code.
5. Code Execution and Solution: Execute the generated code to obtain the solution of the optimization
problem.
F.2
Details of Prompts in Each Stage
F.2.1
Mathematical Formula Generation Stage
1. Prompt File Overview
The main prompt file used in this stage is math_query.txt. Its core function is to guide the
large language model (LLM) to understand and analyze the input optimization problem, generate
the corresponding mathematical formula, and clarify the optimization objective. Additionally, it
requests the LLM to provide formulas for other variables mentioned in the main formula and their
corresponding values.
2. Original Text and Analysis of math_query.txt
The original text content of math_query.txt is as follows:
math_query.txt
Based on this optimization problem, construct a complete mathematical formula, including
the objective function and constraints. Please also provide formulas for some of the other
variables mentioned in the formula, such as channel condition h, and the values corresponding
to these variables.


--- Page 16 ---
Input: $input$
Detailed analysis of the original text:
• Problem Description Placeholder: The $input$ in the original text is a placeholder, which
will be replaced by the specific optimization problem description in actual use. This design
makes the prompt highly versatile, enabling it to handle different optimization problems. For
instance, if the optimization problem is about resource allocation in a network system, the
$input$ will be replaced with the detailed description of this resource - allocation problem,
including parameters like available resources, entity demands, etc.
• Comprehensive Formula Requirement: The prompt asks the LLM to construct a complete
mathematical formula, covering the objective function and constraints. This ensures that the
generated mathematical representation fully captures the essence of the optimization problem.
Moreover, it specifically requests formulas for other relevant variables (e.g., channel condition
h) and their corresponding values. This detailed requirement helps to build a more accurate and
practical mathematical model.
• Optimization Goal Specification in Code:
Although the original prompt text in
math_query.txt does not explicitly mention the optimization goal specification format,
in the Python code ‘generate_math‘, an additional instruction is added to the user prompt.
The line ‘user_prompt += "Please specify whether to maximize or minimize, using the format
’[Optimization Flag: 1]’ for maximize and ’[Optimization Flag: 0]’ for minimize."‘ forces the
LLM to clearly indicate the optimization objective in the generated mathematical formula. This
clear requirement helps the model output results that meet the specifications and facilitates
subsequent processing and analysis.
3. Code - based Process Explanation
The ‘generate_math‘ function implements the process of generating a mathematical formula based
on the optimization problem. Here is a step - by - step explanation:
• Initialization: The function initializes an empty string ‘math‘ to store the generated mathemati-
cal formula and a variable ‘optimization_flag‘ to record the optimization objective.
• Prompt Preparation: It reads the content of math_query.txt as the base user prompt.
Then, it replaces the $input$ placeholder with the actual optimization problem description.
• Optimization Goal Instruction Addition: An instruction about specifying the optimization
goal is appended to the user prompt.
• Iterative Generation and Validation: The function enters a loop where it calls the GPT model
using the prepared user prompt to generate a mathematical formula. Then, it constructs a
validation prompt to check if the generated formula is consistent with the problem description.
If the validation result indicates consistency, the loop is terminated.
• Optimization Flag Extraction: After obtaining a valid mathematical formula, the function
extracts the optimization flag from the formula. If ‘[Optimization Flag: 1]‘ is found, it sets ‘op-
timization_flag‘ to 1 (maximize); if ‘[Optimization Flag: 0]‘ is found, it sets ‘optimization_flag‘
to 0 (minimize).
• Return Result: Finally, the function returns the generated mathematical formula and the
optimization flag.
4. Example of Generating a Mathematical Formula
Suppose the optimization problem is: "In a resource allocation problem, we have N entities. The
goal is to maximize the total utility of all entities. The utility of each entity i is given by Ri =
log2(1 + pihi
σ2 ), where pi is the resource allocated to entity i, hi is the utility gain parameter of entity
i, and σ2 is a base constraint parameter. The total available resources is Ptotal, so PN
i=1 pi ≤Ptotal,


--- Page 17 ---
and pi ≥0 for all i." After replacing the $input$ in the prompt and adding the optimization goal
instruction, the GPT model might generate the following mathematical formula:
max
p1,p2,··· ,pN
N
X
i=1
log2

1 + pihi
σ2

subject to
(PN
i=1 pi ≤Ptotal
pi ≥0,
i = 1, 2, · · · , N
and the formula might also contain ‘[Optimization Flag: 1]‘ to indicate maximization.
F.2.2
Convex Optimization Conversion Stage
1. Prompt File Overview
In this stage, two prompt files convex_example.txt and convex_query.txt are used.
convex_example.txt provides comprehensive examples of converting non - convex problems
into convex problems, serving as references and learning templates for the large language model
(LLM). These examples include detailed analyses of non - convex parts, convexification approaches,
formula derivations, and the final convex optimization problems. convex_query.txt contains
the input information of the specific non - convex mathematical formula to be convexly converted,
guiding the model to perform the conversion operation by specifying clear requirements for the
output.
2. Original Text and Analysis of convex_example.txt and convex_query.txt
• Original Text and Analysis of convex_example.txt
The original text content of convex_example.txt is as follows:
convex_example.txt
Remember your job: 1. Following the input and output format, aligning with the length
and context.
Example:
1: Input:
(a) Mathematical Formulation: Utility Maximization Problem
(b) Objective Function The goal is to maximize the total utility across all entities:
max
p,α
X
i
log
 
1 +
|hii|2pi
P
j̸=i |hij|2pj + σ2
!
where:
– pi: Resource allocated to entity i (continuous).
– hii: Utility gain parameter for entity i.
– hij: Interaction parameter from entity j to entity i (continuous).
– σ2: Base constraint parameter.
(c) Constraints
i. Resource Constraints:
A. Individual entity resource limits:
0 ≤pi ≤Pmax,
∀i


--- Page 18 ---
B. Total resource budget:
X
i
pi ≤Ptotal
ii. Minimum Requirement Constraints: Each entity must satisfy the minimum
requirement:
|hii|2pi
P
j̸=i |hij|2pj + σ2 ≥γmin,
∀i
iii. Allocation Constraints: Each entity can only be assigned to one category:
αik ∈{0, 1},
∀i,
X
k
αik = 1
(d) Additional Variable Definitions
i. Utility Gain Parameter Definitions:
A. Scaling Function: L(d), which can be expressed as:
L(d) = L0 + 10n log10
 d
d0

where L0 is the reference value at distance d0, n is a scaling exponent, and
d is a distance parameter.
B. Gain Model: The utility gain parameters can be modeled as:
hii = √giiL(dii)
hij = √gijL(dij)
where gii and gij are random variables representing variability, typically
modeled as complex Gaussian variables.
Output:
(a) Identification of Non - Convex Parts The optimization problem presented contains
several non - convex components due to the following:
i. Logarithmic Function: The objective function includes a logarithmic term,
which is concave, but the overall maximization creates a non - convex problem.
ii. Ratio Constraints: The ratio constraints can also introduce non - convexity
because they involve ratios of quadratic forms.
iii. Binary Variables: The use of binary variables αik creates a mixed - integer
programming (MIP) problem, which is generally non - convex.
(b) Convexification Approach To address the non - convexities, the following strate-
gies can be employed:
i. Change of Variables: Introduce auxiliary variables to reformulate the ratio
constraints into a convex form.
ii. Logarithm Reformulation: Convert the logarithmic objective to a form
suitable for optimization.
iii. Relaxation of Binary Variables: Use continuous relaxation for binary vari-
ables where appropriate.
(c) Derivation for Convexification


--- Page 19 ---
i. Logarithmic Objective Reformulation: Let zi =
|hii|2pi
P
j̸=i |hij|2pj+σ2 . Instead
of maximizing log(1 + zi), we can maximize zi directly under certain trans-
formations.
ii. Ratio Constraints: To make the ratio constraints convex, we can rewrite:
|hii|2pi ≥γmin

X
j̸=i
|hij|2pj + σ2


This can be treated as a convex constraint by rearranging terms to isolate pi.
iii. Relaxation of Binary Variables: Instead of using binary variables αik, intro-
duce continuous variables 0 ≤αik ≤1 such that:
X
k
αik = 1
∀i
(d) Convex Optimization Problem After applying these transformations, the new
optimization problem can be expressed as follows:
i. Objective Function Maximize:
max
p,α
X
i
zi
where zi is defined as:
zi =
|hii|2pi
P
j̸=i |hij|2pj + σ2
ii. Constraints
A. Resource Constraints:
0 ≤pi ≤Pmax,
∀i
X
i
pi ≤Ptotal
B. Convex Ratio Constraints:
|hii|2pi ≥γmin

X
j̸=i
|hij|2pj + σ2

,
∀i
C. Relaxed Allocation Constraints:
0 ≤αik ≤1,
∀i, k
X
k
αik = 1,
∀i
iii. Supplementary Variables
A. Utility Gain Parameters: The utility gain parameters can still be defined
as:
hii = √giiL(dii),
hij = √gijL(dij)
This reformulated optimization problem is now in a convex form and can be directly
addressed using standard convex optimization solvers, allowing for efficient computation
and resource allocation.


--- Page 20 ---
The examples in this file provide ideas and methods for convex conversion for the LLM.
By demonstrating different types of non - convex problems and their convex forms after
conversion, the model can learn common convex conversion techniques such as change of
variables, logarithm reformulation, and relaxation of binary variables.
• Original Text and Analysis of convex_query.txt
The original text content of convex_query.txt is as follows:
convex_query.txt
Please identify if there are any non - convex parts based on the mathematical formula of
the optimization problem you built in the previous step and the type of corresponding
variables, and if so, choose the algorithm that you think is most appropriate to make all
the non - convex components convex (including mixed integer programming problems)
until you can solve them directly with the solver. And tell me the derivation of the
formula to make it convex, and finally, give me the optimization problem after the
convexity, including some supplementary variable formulas. Input: $input$
Output:
The $input$ in it is also a placeholder, which will be replaced by the actual non - convex
mathematical formula. This prompt directly requires the model to perform a comprehensive
convex conversion operation on the input non - convex formula, including identifying non -
convex parts, choosing appropriate convexification algorithms, providing formula derivations,
and presenting the final convex optimization problem.
3. Code Explanation: transfer_convex Function The transfer_convex function is de-
signed to convert a non - convex mathematical formula into a convex one using the LLM. Here is a
step - by - step breakdown of what the function does:
(a) Initialization: The function initializes an empty string convex to store the converted convex
mathematical formula.
(b) Prompt Loading: It reads the system prompt from convex_example.txt and the user
prompt from convex_query.txt using the pure_set_prompt function. These prompts
serve as guidelines for the LLM to understand the task and learn from the provided examples.
(c) Prompt Modification: The function replaces the $input$ placeholder in the user prompt
with the actual non - convex mathematical formula (math) passed as an argument.
(d) Model Call: It calls the LLM using the call_gpt_from_sys function, passing the system
prompt and the modified user prompt. The LLM then processes the input and generates the
convex version of the mathematical formula.
(e) Result Return: Finally, the function returns the converted convex mathematical formula.
F.2.3
Convex Optimization Conversion Stage
1. Prompt File Overview
In this stage, two prompt files convex_example.txt and convex_query.txt are used.
convex_example.txt provides some examples of converting non-convex problems into convex
problems, providing references and learning templates for the LLM; convex_query.txt contains
the input information of the specific non-convex mathematical formula to be convexly converted,
guiding the model to perform the conversion operation.
2. Original Text and Analysis of convex_example.txt and convex_query.txt
• Original Text and Analysis of convex_example.txt
The original text content of convex_example.txt is as follows:


--- Page 21 ---
convex_example.txt
Example 1: Non-convex problem: minx x2 −2x + 1 (Non-convex part: x2 may not be
convex in some intervals) Convex conversion: minx(x −1)2 (Converted to a convex
function by completing the square)
Example 2: Non-convex problem: minx,y x2y + 3xy −2 (Non-convex part: x2y is
non-convex) Convex conversion: (List the specific conversion method and result here)
The examples in this file provide ideas and methods for convex conversion for the LLM. By
demonstrating different types of non-convex problems and their convex forms after conversion,
the model can learn common convex conversion techniques such as completing the square and
variable substitution.
• Original Text and Analysis of convex_query.txt
The original text content of convex_query.txt is as follows:
convex_query.txt
Please convert the following non-convex mathematical formula into a convex form:
$input$
The $input$ in it is also a placeholder, which will be replaced by the actual non-convex
mathematical formula. This prompt directly requires the model to perform convex conversion
on the input non-convex formula.
F.2.4
Code Generation Stage
1. Prompt File Overview
In the code generation stage, the prompt files code_example.txt and code_query.txt
are used. code_example.txt contains some examples of generating code from mathematical
formulas, demonstrating the structure and implementation of the code. code_query.txt provides
the convex optimization formula for which code needs to be generated, guiding the large language
model to generate the corresponding executable code.
2. Original Text and Analysis of code_example.txt and code_query.txt
• Original Text of code_example.txt
The original text content of code_example.txt is as follows:
code_example.txt
Suppose we have a simple convex optimization problem with the objective of minimizing
the objective function f(x) = x2 + 2x + 1, where x is a real - valued variable. Here
is a code example using Python and the ‘scipy.optimize‘ library to solve this problem:
“‘python from scipy.optimize import minimize
Define the objective function def objective(x): return x**2 + 2*x + 1
Initial guess x0 = 0
Call the optimizer result = minimize(objective, x0)
Output the result if result.success: print(f"Optimization succeeded. The optimal solution
is: result.x[0], and the optimal value is: result.fun") else: print("Optimization failed")
• Analysis of codeexample.txt The examples in codeexample.txt provide the large
language model with a specific paradigm for translating mathematical formulas into code
implementations. It shows how to use a specific Python library (such as scipy.optimize) to solve
convex optimization problems. The example details the definition of the objective function, the


--- Page 22 ---
setting of the initial guess, and the invocation of the optimizer. This helps the model understand
the general process of code generation for convex optimization problems, including how to
represent mathematical concepts in code, how to choose appropriate libraries and functions, and
how to handle the results of the optimization process.
• Original Text of codequery.txt The original text content of codequery.txt is as
follows:
codequery.txt
Please generate Python code to solve the following convex optimization problem. The
convex optimization formula is: input. Use appropriate optimization libraries (e.g.,
scipy.optimize). Clearly define the objective function, constraints (if any), and initial
guesses. Provide clear comments in the code to explain the key steps.
• Analysis of codequery.txt codequery.txt guides the large language model in code
generation in multiple ways. Firstly, it directly specifies the input, which is the convex optimiza-
tion formula, enabling the model to focus on the specific problem. Secondly, it recommends
using appropriate optimization libraries, giving the model a clear direction on where to find
the necessary tools for solving the problem. Thirdly, by requiring the clear definition of the
objective function, constraints, and initial guesses, it ensures that the generated code is complete
and accurate. The requirement for clear comments in the code also helps to make the generated
code more understandable and maintainable.
F.2.5
Code Execution Stage
1. Prompt File Overview In the code execution stage, the prompt file executecodequery.txt
is used. Its main purpose is to guide the large language model to analyze and handle the execution
results of the generated code, including error handling and result extraction.
2. Original Text and Analysis of executecodequery.txt
• Original
Text
of
executecodequery.txt
The
original
text
content
of
executecodequery.txt is as follows:
execute_code_query.txt
The following is the code generated to solve the convex optimization problem: code.
Execute this code. If the execution fails, analyze the error message and suggest possible
solutions. If the execution is successful, extract the optimal solution and the optimal
value from the output, and present them in a clear format.
• Analysis of execute_code_query.txt This prompt guides the large language model to
perform two main tasks. When the code execution fails, it requires the model to analyze the
error message, which helps the model understand common error types in code execution for
convex optimization problems, such as incorrect function calls, improper parameter settings, or
library import issues. Based on the analysis, the model is expected to suggest possible solutions,
which promotes the model’s ability to troubleshoot and optimize code. When the code execution
is successful, the model is required to extract the optimal solution and the optimal value, which
trains the model’s ability to understand and process the output of the optimization process.
F.2.6
Feasibility Check Stage
1. Prompt
File
Overview
In
the
feasibility
check
stage,
the
prompt
file
feasibilitycheckquery.txt is used.
Its function is to guide the large language
model to determine whether the obtained solution of the optimization problem is within the feasible
region based on the original problem description and the mathematical formula.


--- Page 23 ---
2. Original Text and Analysis of feasibilitycheckquery.txt
• Original Text of feasibilitycheckquery.txt The original text content of
feasibilitycheckquery.txt is as follows:
feasibility_check_query.txt
The original convex optimization problem description is: problemdescription. The
corresponding mathematical formula is: mathformula. The obtained solution of the
optimization problem is: solution. Determine whether the solution is within the feasible
region. If it is, return 1; if not, return 0.
• Analysis of feasibility_check_query.txt This prompt provides the large language
model with all the necessary information for a feasibility check, including the problem descrip-
tion, the mathematical formula, and the obtained solution. By asking the model to determine
whether the solution is within the feasible region and return a binary result, it forces the model
to understand the constraints and conditions in the convex optimization problem and use logical
reasoning to make a judgment. This helps to ensure the quality and validity of the final solution.
G
Case Study
G.1
Problem
G.1.1
Problem Description
Consider a non-convex optimization problem where the objective is to maximize a utility function subject
to resource constraints. The problem involves allocating resources to multiple entities to maximize the
total utility while ensuring that each entity meets a minimum requirement.
This optimization problem aims to optimize resource allocation to enhance overall performance while
maintaining feasibility for all entities.
G.1.2
Variable Parameters
1. Number of Entities: 5
2. Maximum Resource per Entity: 30 units
3. Minimum Requirement per Entity: 10 units
4. Utility Function Parameters: The utility gain is modeled as:
Gij =
1
(dij)γ
(γ = 2.2, dij is a distance parameter between entity i and j)
5. Base Constraint Parameter: N0 = 1 × 10−9
6. Total Available Resources: 120 units
H
Answer
H.1
Mathematical Formulation
We start by formulating the problem mathematically. Given an optimization problem with the goal of
optimizing resource allocation to maximize utility subject to certain constraints.
H.1.1
Objective Function
The objective is to maximize the overall system utility, denoted as R:
maximize R =
N
X
i=1
log2

1 + Pi|hi|2
N0

where:


--- Page 24 ---
• N is the number of entities.
• Pi is the resource allocated to entity i.
• hi is the utility gain parameter for entity i.
• N0 is a base constraint parameter.
H.1.2
Constraints
1. Total resource constraint:
N
X
i=1
Pi ≤Pmax
where Pmax is the maximum total available resources.
2. Non - negativity constraints for resources:
Pi ≥0,
∀i = 1, 2, . . . , N
3. Minimum requirement for each entity (optional):
Ri ≥Rmin,
∀i = 1, 2, . . . , N
where Ri = log2

1 + Pi|hi|2
N0

is the utility for entity i and Rmin is the minimum required utility.
H.1.3
Variables and Formulas
• Utility Gain Parameter h is typically modeled as:
hi =
p
Gi · exp(jθi)
where:
– Gi represents the gain parameter for entity i (can vary based on problem context).
– θi is a phase parameter (uniformly distributed in [0, 2π]).
• Base Constraint Parameter N0 can be calculated as:
N0 = kT · B
where:
– k is a constant parameter.
– T is a scaling factor.
– B is a bandwidth or capacity parameter.
H.1.4
Optimization Flag
Since the objective is to maximize utility:
[Optimization Flag: 1]
This formulation sets up an optimization problem that can be solved using various optimization
techniques and algorithms to find the optimal resource allocation Pi for each entity in the system. The
parameters specified in the problem can be adjusted and implemented according to the model being
studied.


--- Page 25 ---
H.2
Identification of Non - Convex Parts
The optimization problem described contains several non - convex components:
1. Logarithmic Function: Although the logarithmic function is concave, maximizing the sum of indi-
vidual utility terms Ri = log2

1 + Pi|hi|2
N0

creates a non - convex problem due to the interactions
between different Pi variables.
2. Minimum Requirement Constraints: The constraints Ri ≥Rmin involve logarithmic expressions,
which can lead to non - convexities, especially when the resource allocation is in the denominator.
3. Resource Constraints: The total resource constraint coupled with individual non - negativity
constraints does not inherently introduce non - convexity but can complicate the optimization
landscape.
H.3
Proposed Algorithm for Convexification
To address these non - convexities, we can use the following strategies:
• Change of Variables: Reformulate the problem using auxiliary variables.
• Convex Relaxation: Transform existing logarithmic terms into convex equivalent expressions.
• Use of Sequential Convex Programming (SCP): An iterative approach that optimizes a convex
approximation of the original problem at each step.
A specific algorithm suitable for this type of problem is the Alternating Direction Method of Multi-
pliers (ADMM), which can be effective for dealing with non - convex optimization problems involving
separable structures.
H.4
Derivation for Convexification
1. Logarithmic Objective Reformulation: We can rewrite Ri as:
Ri = log2

1 + Pi|hi|2
N0

Using a change of variable, let zi = Pi|hi|2. Then we have:
Ri = log2

1 + zi
N0

2. Handling Minimum Requirement Constraints: Instead of using the logarithmic form for minimum
requirements directly, we can reformulate it to express resources:
Pi|hi|2
N0
≥2Rmin −1
Rearranging gives:
Pi|hi|2 ≥N0(2Rmin −1)
This can be made into a convex constraint by separating Pi from the non - linear aspects.
3. Relaxation: The resource constraints remain linear since they are in the form of:
N
X
i=1
Pi ≤Pmax


--- Page 26 ---
H.5
Convex Optimization Problem
After applying these transformations and simplifications, the new optimization problem can be expressed
as follows:
H.5.1
Objective Function
maximize R =
N
X
i=1
zi,
where zi = log2

1 + Pi|hi|2
N0

H.5.2
Constraints
1. Total Resource Constraint:
N
X
i=1
Pi ≤Pmax
2. Non - negativity Constraints:
Pi ≥0,
∀i = 1, 2, . . . , N
3. Convex Minimum Requirement Constraints:
Pi|hi|2 ≥N0(2Rmin −1),
∀i = 1, 2, . . . , N
H.5.3
Supplementary Variable Formulas
• Utility Gain Parameter remains defined as:
hi =
p
Gi · exp(jθi)
• Base Constraint Parameter can be expressed as:
N0 = kT · B
This reformulated optimization problem is now in a convex form and can be addressed using standard
convex optimization solvers, enabling efficient computation and resource allocation.
H.6
Code Generation
The following is a Python code example using ‘scipy.optimize.minimize‘ to solve the optimization
problem:
code
import numpy as np
from scipy.optimize import minimize
# Parameters
N = 5 # Number of entities
P_max = 10 # Maximum total resources
R_min = 5 # Minimum requirement
G = np.array([1, 1, 1, 1, 1]) # Utility gain parameters
N0 = 0.001 # Base constraint parameter
# Objective function
def objective(P):
z = np.log2(1 + (P * G) / N0) # Calculate z_i


--- Page 27 ---
return -np.sum(z) # Minimize negative sum for maximization
# Constraints
def constraint_total_power(P):
return P_max - np.sum(P)
def constraint_qos(P):
return np.array([P[i] * G[i] - N0 * (2**(R_min) - 1) for i in
range(N)])
# Initial guess
P0 = np.ones(N)
# Constraints dictionary
constr = [{’type’: ’ineq’, ’fun’: constraint_total_power},
{’type’: ’ineq’, ’fun’: constraint_qos}]
# Bounds
bounds = [(0, P_max)] * N # Each P_i must be between 0 and P_max
# Solve the optimization problem
result = minimize(objective, P0, bounds=bounds, constraints=
constr)
# Output results
print("Objective Function Value:", -result.fun)
print("Optimized Resource Allocations (P_i):", result.x) #
Optimized resource allocations
H.7
Optimization Results
The output of the script shows:
Output
- The value of the objective function is 54.8325272595287.
- Optimized Resource Allocations (Pi) are [2.2.2.2.2.].
The initial solution lies within the feasible region.
