--- Page 1 ---
arXiv:2507.03605v1  [cs.NE]  4 Jul 2025
Behaviour Space Analysis
of LLM-driven Meta-heuristic Discovery
Niki van Stein1[0000−0002−0013−7969], Haoran Yin1, Anna V.
Kononova1[0000−0002−4138−7024] Thomas Bäck1[0000−0001−6768−1478], and
Gabriela Ochoa2[0000−0001−7649−5669]
1 Leiden University, Leiden, The Netherlands, n.van.stein@liacs.leidenuniv.nl
2 University of Stirling, Scotland, UK
Abstract. We investigate the behaviour space of meta-heuristic opti-
misation algorithms automatically generated by Large Language Model
driven algorithm discovery methods. Using the Large Language Evo-
lutionary Algorithm (LLaMEA) framework with a GPT o4-mini LLM,
we iteratively evolve black-box optimisation heuristics, evaluated on 10
functions from the BBOB benchmark suite. Six LLaMEA variants, fea-
turing different mutation prompt strategies, are compared and analysed.
We log dynamic behavioural metrics including exploration, exploitation,
convergence and stagnation measures, for each run, and analyse these via
visual projections and network-based representations. Our analysis com-
bines behaviour-based projections, Code Evolution Graphs built from
static code features, performance convergence curves, and behaviour-
based Search Trajectory Networks. The results reveal clear differences
in search dynamics and algorithm structures across LLaMEA configu-
rations. Notably, the variant that employs both a code simplification
prompt and a random perturbation prompt in a 1+1 elitist evolution
strategy, achieved the best performance, with the highest Area Over
the Convergence Curve. Behaviour-space visualisations show that higher-
performing algorithms exhibit more intensive exploitation behaviour and
faster convergence with less stagnation. Our findings demonstrate how
behaviour-space analysis can explain why certain LLM-designed heuris-
tics outperform others and how LLM-driven algorithm discovery nav-
igates the open-ended and complex search space of algorithms. These
findings provide insights to guide the future design of adaptive LLM-
driven algorithm generators.
Keywords: Automated algorithm design · Large Language Models ·
Evolutionary Computation · Meta-heuristics
1
Introduction
The automated design of optimisation algorithms, often termed Automated Al-
gorithm Design (AAD) [30], has been a longstanding challenge in evolutionary
computation and the field of AI. Early approaches like Programming by optimi-
sation (PbO) [8] advocated maintaining rich sets of algorithm components to be


--- Page 2 ---
2
N. van Stein et al.
tuned by meta-optimisation methods. Subsequent research introduced dynamic
algorithm configuration and modular algorithm frameworks such as modular
CMA-ES [15] and modular DE [34], evolving algorithm structures for better per-
formance. With the advent of powerful Large Language Models (LLMs), a new
paradigm has emerged: using LLMs to generate and evolve functions or entire
algorithms in natural language and code [2, 11]. Recent work has shown LLMs
can automatically produce novel meta-heuristics that rival human-designed ones
[20, 25]. For example, the LLaMEA framework by van Stein et al. [25] uses an
LLM in-the-loop to iteratively generate and improve black-box optimizers, and
other studies have explored evolving heuristics with LLMs in various contexts
[4, 23, 35, 28].
However, a key challenge remains: understanding the behaviour of these
LLM-generated algorithms. While performance comparisons (e.g., final accu-
racy or convergence speed) tell us which method works best, they do not explain
why. Traditional algorithm analysis tools, such as fitness landscape analysis and
exploratory landscape analysis [12] for problem instances, are not directly appli-
cable when the “individuals” being evolved are algorithms themselves. Currently,
we have limited insight into how an LLM-driven algorithm search navigates the
space of possible programs, how algorithm structure and strategy evolve over
time, and what behavioural characteristics distinguish successful runs from fail-
ures. Gaining such insights is crucial for explainability and for improving the
automated design process [29].
In this paper, we address this gap by conducting a comprehensive behaviour
space analysis of LLM-generated meta-heuristics. We focus on the LLaMEA
framework using an OpenAI GPT-based model (o4-mini-2025-04-16) to evolve
algorithms for continuous optimisation. We benchmark six different LLaMEA
configurations on a set of 10 noiseless BBOB functions [6], and record entire
traces from each run using IOH experimenter [16]. By computing behavioural
metrics from these traces and leveraging advanced visualisation techniques, we
reveal patterns in exploration, exploitation, convergence, and stagnation exhib-
ited by the generated algorithms.
Our contributions are as follows:
– We define a set of quantitative behaviour metrics for LLM-generated opti-
mizers, capturing aspects such as search space coverage, intensification near
optima, convergence speed, and stagnation periods.
– We use Parallel coordinate plots, code evolution graphs and behaviour-based
search trajectory networks for visual comparison of algorithm “footprints” in
behaviour space.
– We compare 6 different variants of mutation and evolutionary search strate-
gies and identify which configuration yields the best performance (via AOCC
metric) and use our behaviour analysis to explain its success. In particular,
we find that a 1+1 elitist strategy combining simplification and random per-
turbation prompts consistently outperforms others, likely due to a balanced
exploration-exploitation and sustained improvement without disruption.


--- Page 3 ---
Behaviour Space Analysis of LLM-driven Meta-heuristic Discovery
3
2
Related Work
2.1
Automated Algorithm Design
The concept of automatically designing or configuring algorithms has evolved
over the past decade. The Programming by optimisation paradigm [8] encour-
aged specifying algorithm components as parameters to be tuned rather than
fixed, enabling automated search in algorithm space. A related approach, Ge-
netic Improvement [10], attempts to automatically improve the behaviour of
a software system using evolutionary algorithms. Starting from human-written
code, genetic improvement tries to evolve it so that it is better with respect to
given criteria, typically non-functional properties, such as execution time and
power consumption, though others are possible.
This idea was expanded to numerical optimisation through modular algo-
rithm frameworks that assemble heuristics from predefined components. Notably,
van Rijn et al. [22] demonstrated evolving the structure of CMA-ES variants
via evolutionary strategies, effectively treating algorithm blueprint as a geno-
type. These earlier approaches required a human-defined modular design space,
whereas the rise of LLMs enables generating algorithms free-form, without a
fixed module library. The works of Liu et al. [4] and Zhang et al. [37] exemplify
this new direction: by leveraging an LLM (GPT-style) to propose pseudocode
or code for heuristics, they achieved competitive optimisation results and under-
scored the importance of evolutionary search in guiding LLMs. The LLaMEA
framework and related efforts have further formalized LLM-driven algorithm
evolution as a novel class of evolutionary computation technique.
2.2
Analysing Meta-heuristic Behaviour
Understanding why an optimisation algorithm succeeds or fails on a problem
has long been of interest. Traditionally, researchers have analysed algorithm
trajectories through frameworks like fitness landscape analysis and performance
footprints.
Search Trajectory Networks (STNs) [17] are a data-driven, graph-based tool
to visualize, quantify and contrast the dynamics of iterative optimisation algo-
rithms. In an STN, nodes are locations of representative solutions in the search
space, and edges connect successive locations in the search trajectory. Edges are
weighted with the sampling frequency of transitions between pairs of nodes dur-
ing the STN construction process, which is based on logs from a number of runs
of the studied algorithms on the considered problems. A representative solution
is a solution to the optimisation problem at a given time step that represents the
status of the search algorithm, for example, the best in the population, although
all solutions could be logged for small populations. In the initial STN model
[17], representative solutions belong to the genotype search space. For complex
search spaces (i.e neural networks, graphs, programs), solutions can be logged in
the phenotype or behaviour spaces [9, 13, 24], which are generally much smaller.


--- Page 4 ---
4
N. van Stein et al.
Since any search space for realistic optimisation problems is very large, a process
of partitioning the space into locations is required to have manageable models. A
partition strategy groups subset of related solutions into locations. For example,
if solutions are vectors of real numbers, locations can be defined as hypercubes
with a prefixed lower numerical precision [17]. Each solution is an element of
one and only one location, and each location is assigned a representative objec-
tive/fitness value.
Attractor Networks (ANs) [33] were later developed by Thomson et al. to
highlight regions where an optimizer stagnates. These represent the “stalling”
behaviour by grouping contiguous non-improving iterations into nodes (attrac-
tors). Another related concept is Local Optima Networks (LONs) [18], which
map the connectivity of local optima in combinatorial landscapes. LONs and
ANs provide coarse views of search dynamics but have mainly been applied
to fixed algorithms. Here, we extend the idea to algorithm-evolution processes,
where the “search” happens over algorithm designs.
2.3
Explainable Benchmarking
While performance traces reveal how an algorithm searches, static code features
reveal what the algorithm is. Recent work has explored using code analysis for
understanding algorithm performance. Pulatov et al. [21] showed that static
metrics like cyclomatic complexity and Abstract Syntax Tree (AST) structure
can improve algorithm selection by characterising algorithm behaviour beyond
external performance alone. In the context of AAD, static analysis can help
quantify how algorithms generated by LLMs differ structurally. For example, van
Stein et al. [26] introduced Code Evolution Graphs (CEGs) as a method to trace
the lineage of evolving code and its properties. CEGs integrate graph metrics
(from ASTs) and code complexity measures to visualise algorithm evolution over
generations. This approach is inspired by earlier software visualisation techniques
like Code Flow graphs [32], which depict how code changes across versions. To
our knowledge, no prior work has systematically combined dynamic behaviour
analysis (e.g. STNs, attractors) with static code analysis in the AAD domain. By
doing so, our study provides a holistic view of LLM-generated meta-heuristics,
linking how they search to what they consist of and how they perform.
Finally, our work is related to efforts in explainable AI and benchmarking
for optimisation heuristics. The BLADE benchmark suite [27] was recently pro-
posed to evaluate LLM-driven AAD methods in a standardized way, including
logging of all runs for transparency. In particular, metrics like the Area Over
the Convergence Curve (AOCC) [5] have been promoted as a more informative
performance measure than final results alone, as they capture anytime perfor-
mance. We adopt AOCC in our comparisons to evaluate not just whether an
algorithm eventually finds a good optimum, but how quickly and consistently
it does so over the evaluation budget. Our analysis framework can be seen as
contributing to explainable benchmarking: by augmenting performance metrics
with behavioural and structural analysis, we can better interpret the outcomes
of an algorithm design experiment.


--- Page 5 ---
Behaviour Space Analysis of LLM-driven Meta-heuristic Discovery
5
Putting it together. To our knowledge no prior work combines (i) dynamic search-
trace analysis (STNs/ANs), (ii) static code metrics (CEGs), and (iii) standard-
ised anytime performance measures such as AOCC within a single benchmark.
Our methodology closes that triangle, providing the first holistic view of LLM-
driven automated algorithm design.
3
Methodology
3.1
LLaMEA Framework and Configurations
We build our study on LLaMEA [25] – a framework where an LLM acts as
a generative engine to produce and iteratively refine algorithms. In LLaMEA,
the individuals in the evolutionary process are complete algorithms (expressed
as code). Each algorithm individual is evaluated by running it on a user-defined
evaluation function, in this case a set of optimisation problems, to obtain a
performance score (e.g., best function value found, in our case the any-time per-
formance metric AOCC averaged over multiple instances and runs). Selection
then chooses algorithms for the next iteration, and variation is implemented
by prompting the LLM to generate modified algorithms (analogous to muta-
tion/crossover in genetic algorithms). In our experiments, we use an OpenAI
GPT-derived model named o4-mini-2025-04-16 as the generative LLM. This is a
relatively compact LLM specialized for code generation and LLM-reasoning [19],
allowing multiple calls within our computational budget. We integrate LLaMEA
with the BLADE benchmarking framework [27] to ensure rigorous evaluation
and logging. The target problems are 10 noiseless BBOB functions [6], chosen to
cover a range of landscapes (unimodal, multimodal, separable, ill-conditioned,
etc.). Each function is 5-dimensional and uses a search domain of [−5, 5]5. For
each function, we designate 5 instances as training (instances 1 to 5) and 10 as
testing (instances 6 to 15) which are unseen during evolution, following a train-
test scheme to evaluate generalisation of the evolved algorithms. The LLaMEA
evolution is given a budget of 100 algorithms (evaluations) per run, and we
repeat each configuration for 5 independent runs (with different random seeds
(seeds 1–5)).
LLaMEA Configurations: We consider six distinct configurations of LLaMEA,
denoted LLaMEA-1 through LLaMEA-6. These configurations differ in their mu-
tation prompt strategy and evolutionary parameters, as summarized in Table 1.
The specific mutations prompts are:
Refine and Simplify "Refine and simplify the selected algorithm to improve
it."
Random-new "Generate a new algorithm that is different from those tried be-
fore."
Adaptive-mutation Here the LLM has to follow a refine prompt with a dy-
namically specified percentage (sampled from a long tail distribution) on
how much to mutate the code [36].


--- Page 6 ---
6
N. van Stein et al.
Table 1: Overview of the six LLaMEA configurations evaluated in this study.
Population size µ, offspring size λ, and elitism refer to the evolutionary param-
eters, while the right-most column sketches the search paradigm.
Variant
Mutation prompt(s)
µ/λ
Elitism Search paradigm
LLaMEA-1 Refine & simplify
4 / 12
✗
Population, incremental sim-
plification
LLaMEA-2 Generate new algorithm
4 / 12
✗
Population, pure exploration
LLaMEA-3 Both above prompts
4 / 12
✗
Population,
mixed
ex-
plore–exploit
LLaMEA-4 Both above prompts
1 / 1
✓
Elitist (1 + 1) hill-climber
LLaMEA-5 Adaptive-mutation prompt 4 / 12
✗
Population, self-adaptive mu-
tation
LLaMEA-6 Adaptive-mutation prompt 1 / 1
✓
Elitist
(1 + 1)
with
self-
adaptation
All methods start from scratch with the LLM generating initial algorithms
(e.g., a random algorithm from the prompt describing the optimisation task).
The fitness of an algorithm is evaluated as the mean performance across the 5
training instances of each BBOB function (aggregated across functions). Specif-
ically, we use the BLADE experiment setup with training instances to evolve
algorithms, then report performance on test instances for final evaluation. Key
settings like function evaluation budget per algorithm (we use 2000d evaluations
as typical in BBOB, so 10 000 for d = 5) and logging frequency are consistent
across methods for fairness.
3.2
Behaviour Metrics
To analyse how each generated algorithm performs its search (not just how well
it performs), we record the complete optimisation trace of each algorithm on each
function instance. A trace is the sequence of objective values (fitness evaluations)
and search-space locations over time as the algorithm runs. From these traces,
we compute a set of scalar behavioural metrics that capture different aspects
of the search dynamics. Our metric set is inspired by previous works on search
behaviour analysis, but here adapted to continuous optimisation traces.
For the metrics below, let us define the search domain and optimisation trace
as follows: Let D = Qd
k=1[lk, uk] ⊂Rd be the search domain with lower bounds
lk and upper bounds uk and X = {x1, . . . , xN} ⊂D the set of evaluated points
(trace).
We group the metrics into four categories:
Exploration & Diversity Measure how broadly the algorithm searches the
space.
– Average Nearest-Neighbors Distance (NN-dist): Mean distance between each
evaluated solution and its nearest neighbour in the same trace, indicating the


--- Page 7 ---
Behaviour Space Analysis of LLM-driven Meta-heuristic Discovery
7
spread of search points. Higher values mean the algorithm tends to sample
points far apart (diverse exploration). Proposed by [14] as Mean Distance
from Prior Evaluations (MDPE) to measure the novelty of newly sampled
points versus the already evaluated ones, here we use it only to measure the
average distance for the full trace.
– Coverage Dispersion (Disp): The dispersion of points relative to the search
domain. We approximate the volume of space covered by the visited solu-
tions.
The coverage–dispersion metric is defined as
disp(X) = sup
y∈D
min
x∈X
 y −x

2.
It equals the radius of the largest empty hypersphere that can be placed
inside D without containing any sample from X; smaller values imply better
space-filling coverage.
– Average Exploration Percentage (Expl %): The fraction of iterations classi-
fied as exploratory moves. The metric was proposed in [31]. We have adapted
the metric to calculate it per batch instead of per iteration, to significantly
speed up the calculation without loosing too much information.
For any finite set S ⊂Rd define
D(S) =
2
|S|(|S| −1)
X
p<q
xp,xq∈S
∥xp −xq∥2,
the average pairwise Euclidean distance. With widths wk = uk−lk and mean
width ¯w = 1
d
Pd
k=1 wk, the expected pairwise distance of two i.i.d. uniform
samples in D is approximated by
Drand = ¯w
r
d
6,
(cf. the closed-form expression for the unit hyper-cube).
Partition the trace X into C = ⌈n/K⌉consecutive chunks S0, . . . , SC−1 of
size K. With the diversity measure D(·) introduced earlier, the exploration
score of chunk c is
Ec = min

100 D(Sc)
Drand , 100
	
 0 ≤Ec ≤100

.
The average exploration percentage is
E =
1
C
C−1
X
c=0
Ec.
and the complementary exploitation percentage is 100 −E.
Exploitation & Intensification: Measure the focus on local search around
the best found solutions.


--- Page 8 ---
8
N. van Stein et al.
– Average Distance to Best (Dist→best): The mean distance of each evalu-
ated solution to the best solution found so far [3]. A lower average distance
indicates the algorithm spends more time exploiting near the current best
(intensification).
– Intensification Ratio (Inten-ratio): The proportion of iterations where a
solution is within a small radius of the best-so-far solution . We set the radius
to 10% of the search range by default. A higher intensification ratio means
the search frequently samples near the known best (intensive exploitation).
– Average Exploitation Percentage (Eplt %): Complementary to exploration
percentage (see above).
Convergence Progress: Metrics indicating the speed and magnitude of im-
provement over time.
– Average Convergence Rate (Conv-rate): The geometric mean of successive
error reductions, following definition from He et al. [7]. Average convergence
rate < 1 indicates convergent behaviour (fitness error decreases on average
each step); smaller average convergence rate means faster convergence.
– Average Improvement (∆fitness): The mean improvement in (normalized)
objective value on iterations that yielded an improvement. This captures
how large the typical improvement step is when progress is made.
– Success Rate (Success %): The proportion of iterations that resulted in
any improvement over the current best. A higher success rate means the
algorithm frequently finds better solutions (which could indicate either an
easy problem or effective search steps).
Stagnation & Reliability: Metrics diagnosing lack of improvement and search
stability.
– Longest No-Improvement Streak (No-imp streak): The longest sequence of
iterations with no improvement. This reveals whether the algorithm can get
stuck for long periods.
– Last Improvement Fraction (Last-imp frac): The fraction of the total evalu-
ations that have elapsed since the last improvement. A value close to 1 means
the algorithm’s final phase was stagnant, whereas a lower value indicates it
kept improving until near the end.
These metrics are computed directly from each algorithm’s trace data. They
are fast to compute and together they span a multi-dimensional behaviour space
that lets us position any LLM-generated meta heuristic run as a single point 3.
For multi-instance evaluation, we aggregate the metrics computed per instance.
Implementations of the metrics are provided in our repository [1].
3 We purposely restrict the study to single–objective, box-constrained problems where
the entire evaluation trace is available.


--- Page 9 ---
Behaviour Space Analysis of LLM-driven Meta-heuristic Discovery
9
3.3
Visualisation and Analysis Techniques
To interpret the high-dimensional behavioural data and the evolution of the
algorithms, we apply several complementary visualisation techniques:
Performance Convergence Plots: For each LLaMEA variant, we evaluate
the AOCC [5] anytime performance of all the generated algorithms. We aggregate
the results of the 5 runs per LLaMEA variant by computing the mean best
AOCC at each evaluation step. A higher AOCC means the generated algorithm
finds good optima faster on average. We plot the mean best-so-far curves with
confidence intervals from the 5 runs. These curves allow visualising not just the
final outcome but the trajectory of progress: e.g., whether a method quickly
gains moderate performance then plateaus, or improves steadily.
Parallel Coordinates Plot for Metrics: One way to view behaviour met-
rics is via a parallel coordinates plot, where each metric is an axis and each
algorithm is represented by a poly-line across axes. We use this to compare
the metric profiles of different runs, especially to contrast high-performing ver-
sus low-performing cases. By coloring runs according to their final performance
quartile, we identify which metrics correlate with success. For instance, we will
see if the top-performing runs tend to have consistently higher exploitation or
lower stagnation than poorer runs.
Code Evolution Graphs: To visualise how each algorithm’s code evolved
during LLaMEA runs, we construct CEGs. In a CEG, each node represents a
generated algorithm (with its code features and performance), and directed edges
link a parent algorithm to its mutated offspring. We extract a rich set of static
code features from each algorithm’s source code via AST analysis, including
counts of AST nodes and edges, graph connectivity measures (degree distribu-
tion, clustering coefficients), code complexity metrics (cyclomatic complexity,
code token count, parameter counts), etc. We plot the total token count of the
algorithm’s code over generations as a curve to indicate code growth or simpli-
fication. The CEG figure shows how algorithms structurally evolve as LLaMEA
progresses. Node sizes in the CEG can be scaled by parent frequency; we size
each node by how many times it was selected as a parent, to highlight influential
ancestors. We generate CEGs for each LLaMEA variant for each independent
run, arranging them in a grid for visual comparison.
Search Trajectory Networks: To visualise the search dynamics of the 6
LLaMEA variants, we constructed STNs aggregating nodes and edges from the 5
runs of each variant. Nodes represent locations in the behaviour space, which we
defined as a vectors of 5 real numbers representing the least correlated behaviour
metrics: Expl. %, Conv-rate, ∆fitness, Success %, No-imp streak. The behaviour
space is partitioned into hypercubes of a prefixed dimension. We explored two
hypercube dimensions {0.01, 0.1}. All sampled behaviour vectors within the same
hypercube are grouped into a single node. Counts are kept for nodes and edges
indicating their sampling frequency during the construction process.
By combining these analyses, metric projections, code evolution visualisa-
tion, performance curves, and trajectory networks, we obtain a multifaceted
understanding of each method’s behaviour.


--- Page 10 ---
10
N. van Stein et al.
4
Results
4.1
Any-time Performance Comparison
In Figure 1, we can observe the performance of each LLaMEA variant aggregated
over 5 independent runs. It is interesting to observe that most variants show a
relatively large variation between runs except for LLaMEA-4, which outperforms
the other configurations. LLaMEA-4 uses both the ‘simplify’ and ‘new random’
mutation prompts using a (1 + 1) strategy. Also from the validation results on
the 10 test instances (1 right plot), it seems that the LLaMEA-4 variant was
more stable and produced overall better algorithms.
4.2
Code Evolution and Search Dynamics
From Figure 2, we can observe that LLaMEA-1 runs show relatively stable or
even decreasing token count as the LLM simplifies code. LLaMEA-2 (new ran-
dom) shows high variance in code complexity between successive algorithms,
indicating inconsistency in the generated algorithms. LLaMEA-5 and 6 (adap-
tive) show relative smaller updated in behaviour, sometimes simplifying, some-
times complexifying, but with small steps. Overall, CEGs reveal that the best-
performing configuration (LLaMEA-4) managed to avoid code bloat while con-
sistently improving algorithm performance (yellow colour), whereas others often
grew in complexity without proportional gains.
Fig. 1: Aggregated results of the 5 runs per LLaMEA variant by computing the
mean best AOCC at each evaluation step (left subplot). A higher AOCC means
the generated algorithm finds good optima faster on average. 95% confidence
intervals are visualized as shaded area. On the right the AOCC distribution of
the 5 final generated algorithms per LLaMEA configuration, evaluated on the
10 test instances.


--- Page 11 ---
Behaviour Space Analysis of LLM-driven Meta-heuristic Discovery
11
Fig. 2: CEGs for the BBOB runs under different LLaMEA configurations
(LLaMEA 1–6). Each sub panel corresponds to one independent run (columns) of
a given configuration (rows). Within each panel, the y-axis shows the algorithm’s
code length (total code token count) over the sequence of generated algorithms
(x-axis), and the nodes denote the generated algorithms. Directed edges con-
nect each algorithm to its offspring. Elitist 1+1 methods (LLaMEA-4 and 6)
produce linear chains, while population-based methods yield branching graphs.
Node colour denotes normalized AOCC (performance), with bright yellow as the
best AOCC and dark blue as the worst.


--- Page 12 ---
12
N. van Stein et al.
4.3
Behaviour Analysis
To verify that the defined behaviour metrics are complementary to our analy-
sis, we started with a correlation analysis. In Figure 3, the Pearson r correla-
tion is given between all considered metrics and including the normalized fitness
(AOCC) score. It can be observed that some features clearly correlate. The most
obvious pair is Exploitation % and Exploration %, this was expected as it is by
design. Other pairs include Expl.% with Distance to best, Exploitation % with
Intensification ratio and Last no-improvement fraction with No improvement
streak. For further analysis we have selected only one of these highly correlated
feature pairs. In Figure 4 parallel coordinate plots of the normalized behaviour
metrics for each generated algorithm are provided. On the left side we can see
a clear “profile” for good algorithms (dark red), with an average dispersion, low
nearest neighbour distance and relatively low exploration percentage. On the
right side of the figure we can see the top performing algorithms per individ-
ual BBOB function. From this figure it is clear that some objective functions
benefit from a different behaviour profile than others, while for some metrics
such as exploration percentage this is relatively condensed, meaning that similar
behaviour works well on all problems.
Figure 5 shows the STNs for the 6 LLaMEA variants with a partition hyper-
cube of dimension 0.01. For the STN analysis we have selected the 5 least corre-
Fig. 3: Pearson r correlation between the different behaviour metrics and nor-
malized fitness.


--- Page 13 ---
Behaviour Space Analysis of LLM-driven Meta-heuristic Discovery
13
Fig. 4: Behaviour profile over all generated algorithms (left) and the best 100
algorithms per BBOB function (right).
lated metrics: Expl. %, Conv-rate, ∆fitness, Success %, No-imp streak. At this
level of granularity, there is little convergence across runs and the networks fea-
ture several disconnected components. Nodes indicating the start of runs (green
squares) appear in separated components for all variants except the first two,
where two start nodes can be seen in the same connected component. This visu-
alisation captures the different EAs selection dynamics. The population variants
(LLaMEA-1, 2, 3 and 5) show “bushy" graphs with wider branches of improving
edges, as well as small components indicating solutions in the population that
were not further explored in the evolutionary process. On the other and, the
1+1 variants (LLaMEA 4 and 6) show elongated components (one for each run)
following a single path of improving edges with side clusters of deteriorating
edges indicating unsuccessful mutation attempts.
With the coarser partition factor, that is, larger hypercubes of dimension
0.1 for grouping solutions into nodes (Figure 6) we observe convergence across
runs for most variants: a single large component is now visible. The exception
is LLaMEA-6 where a second component of smaller size can be seen. The most
explorative variant is LLaMEA-2 featuring the largest network (larger number
of nodes and edges), while the most compact and efficient search is captured by
the elitist variant LLaMEA-4, featuring short paths to the best solution.
5
Discussion
Our findings highlight several important points for the design of LLM-driven
algorithm generators and for the analysis of their behaviour:
(1) The importance of exploitation-exploration balance in AAD: Through
LLaMEA-4’s performance, we see that giving the LLM two contrasting mu-
tation prompts, one to exploit (refine/simplify the current best algorithm) and
one to explore (generate something entirely new), yields a highly effective search.
This mirrors classic evolutionary computation principles: too much exploration


--- Page 14 ---
14
N. van Stein et al.
Fig. 5: Search trajectory networks for the 6 LLaMEA variants with partition
hypercubes of size 0.01. Node and edge types are as indicated in the legend.
Node sizes and edge widths are proportional to sampling frequency.
(LLaMEA-2) is inefficient, while too little (LLaMEA-1) risks premature con-
vergence. The LLM’s creative capabilities might tempt one to always generate
fresh algorithms, but our results demonstrate the value of refining what you
have. Notably, the simplify prompt not only improved performance but often
reduced code complexity, suggesting that simpler algorithms often generalized
better and were easier for the LLM to tune.
(2) The role of Elitism and selection pressure: The stark difference between
LLaMEA-3 (both prompts, no elitism) and LLaMEA-4 (both prompts, elitist
1+1) indicates that how we select and carry forward algorithms in each genera-
tion has a significant effect. LLaMEA-4 effectively conducts a (1 + 1)-ES in the
space of algorithms, which guarantees never losing the best found. This appears
important in the AAD context because evaluating each algorithm is expensive;
one cannot afford to discard a good strategy hoping it might re-evolve later. The
population-based methods occasionally lost their best solutions when offspring
didn’t include similar quality algorithms. That said, maintaining diversity is still
important; an interesting future direction is to incorporate niching or maintain-
ing a portfolio of top algorithms.
(3) Interpretability through behaviour metrics: By using explainable behaviour
metrics, we were able to diagnose why certain methods underperformed, some-
thing that would be difficult from performance data alone. For instance, we
could attribute LLaMEA-2’s poor performance to its low success rate and high
stagnation, pointing to an overly exploratory behaviour. We also learned that in


--- Page 15 ---
Behaviour Space Analysis of LLM-driven Meta-heuristic Discovery
15
Fig. 6: Search trajectory networks for the 6 LLaMEA variants with partition
hypercubes of size 0.1. Node and edge types are as indicated in the legend. Node
sizes and edge widths are proportional to sampling frequency.
general, algorithms that obey to a certain behaviour profile, generally perform
better on BBOB problems than others. This includes a relatively low Nearest
Neighbour distance (small step size), a exploitation focused strategy and a rel-
atively low success chance (0.1). We also could observe that for different BBOB
functions, different behaviour patterns are beneficial, though many of the BBOB
functions seem to share at least some portion of the behaviour profiles in their
top-performers.
(4) Limitations: It should be noted that the study is currently limited to
relatively low-dimensional problems (5D) and one type of LLM; behaviour on
higher dimensions or with different LLMs (e.g., coding styles of different models
might differ. For example, a larger LLM might generate more complex code
by default or conversely, handle instructions to simplify even better. The still
limited number of 5 runs also means that the conclusions of these work are made
with caution. Due to the API and computational expenses we could not afford
additional runs for now.
6
Conclusions and Future Directions
We presented an in-depth behaviour space analysis of meta-heuristics generated
by an LLM-driven evolutionary algorithm (LLaMEA) on black-box optimisation
tasks. By defining a rich set of behavioural metrics and visual representations,


--- Page 16 ---
16
N. van Stein et al.
we were able to explain performance differences between various LLaMEA con-
figurations that use different prompt-based mutation strategies. Our analysis
identified a particular configuration, using dual prompts for simplification and
diversification in a 1+1 elitist framework (LLaMEA-4), as the most successful.
By analysing the behaviour space of these algorithms we identified an effective
exploration-exploitation balance. From a methodological perspective, this work
demonstrates the value of combining trace analysis (exploration/exploitation
metrics, STNs) with static code analysis (CEGs) to get a full picture of algo-
rithm evolution. We showed that LLM-generated algorithms can be analysed
with similar tools developed for traditional meta-heuristics, and these tools can
yield actionable insights. In practical terms, an engineer using LLaMEA or a
similar system could use our approach to diagnose why an automated design
run failed (e.g., metrics show it stagnated early) or succeeded (e.g., code com-
plexity dropped, indicating a cleaner strategy was found), and adjust the system
accordingly (change prompts, selection, etc.).
There are several avenues for future work. First, applying this behaviour anal-
ysis to other problem domains (e.g., combinatorial problems or mixed-integer
optimisation) would test the generality of the findings. The STN and attractor
concepts, for example, could reveal different patterns if the search space has dis-
crete states. Second, integrating these analysis techniques into the evolutionary
loop is a promising direction, for instance, one could create a feedback where if
the behaviour metrics indicate too much exploitation, the system automatically
increases exploration pressure. This would effectively close the loop between
analysis and design, leading to self-correcting LLM-driven optimizers. Third,
scaling to other LLMs and more complex problems will be important. While
the BBOB suite gave a meaningful test-bed, real-world optimisation often in-
volves constraints, high dimensions, or multiple objectives. Understanding how
an LLM designs algorithms under those conditions, and whether our identified
best practices hold, is an open question.
Finally, from an explainable AI standpoint, one could explore user-friendly
visualisations (perhaps interactive) based on our approach, to allow human ex-
perts to inspect and verify automatically designed algorithms before trusting
them on critical tasks. In conclusion, the synergy of LLMs with evolutionary
search opens exciting possibilities for automatic algorithm discovery. By map-
ping the behaviour space of these evolved algorithms, we gain not only perfor-
mance improvements but also understanding, a crucial step toward reliable and
accountable AI-driven algorithm design. We hope this study serves as a tem-
plate for analysing complex adaptive systems that include AI components, and
that the insights gained will guide both theorists and practitioners in harnessing
LLMs for optimisation in a principled way.
Disclosure of Interests. The authors have no competing interests to declare
that are relevant to the content of this article.


--- Page 17 ---
Behaviour Space Analysis of LLM-driven Meta-heuristic Discovery
17
References
1. Anonymous:
Behaviour
metrics
for
llm-driven
automated
algo-
rithm
design
(Jun
2025).
https://doi.org/10.5281/zenodo.15675581,
https://doi.org/10.5281/zenodo.15675581
2. Chauhan, D., Dutta, B., Bala, I., van Stein, N., Bäck, T., Yadav, A.: Evolution-
ary computation and large language models: A survey of methods, synergies, and
applications. arXiv preprint arXiv:2505.15741 (2025)
3. Črepinšek, M., Liu, S.H., Mernik, M.: Exploration and exploitation in evolutionary
algorithms: A survey. ACM computing surveys (CSUR) 45(3), 1–33 (2013)
4. Fei, L., Tong, X., Yuan, M., Lin, X., Luo, F., Wang, Z., Lu, Z., Zhang, Q.: Evo-
lution of heuristics: Towards efficient automatic algorithm design using large lan-
guage model. In: International Conference on Machine Learning (ICML) (2024),
https://arxiv.org/abs/2401.02051
5. Hansen, N., Auger, A., Brockhoff, D., Tušar, T.: Anytime performance assessment
in blackbox optimization benchmarking. IEEE Transactions on Evolutionary Com-
putation 26(6), 1293–1305 (2022)
6. Hansen, N., Finck, S., Ros, R., Auger, A.: Real-Parameter Black-Box Optimization
Benchmarking 2009: Noiseless Functions Definitions. Research Report RR-6829,
INRIA (2009), https://hal.inria.fr/inria-00362633
7. He,
J.,
Lin,
G.:
Average
convergence
rate
of
evolutionary
algorithms.
IEEE
Transactions
on
Evolutionary
Computation
20(2),
316–321
(2016).
https://doi.org/10.1109/TEVC.2015.2444793
8. Hoos, H.H.: Programming by optimization. Communications of the ACM 55(2),
70–80 (2012)
9. Hu, T., Ochoa, G., Banzhaf, W.: Phenotype search trajectory networks for linear
genetic programming. In: European Conference on Genetic Programming (Part of
EvoStar). pp. 52–67. Springer (2023)
10. Langdon, W.B., Harman, M.: Optimizing existing software with genetic program-
ming. IEEE Transactions on Evolutionary Computation 19(1), 118–135 (2015).
https://doi.org/10.1109/TEVC.2013.2281544
11. Liu, F., Yao, Y., Guo, P., Yang, Z., Zhao, Z., Lin, X., Tong, X., Yuan, M., Lu,
Z., Wang, Z., et al.: A systematic survey on large language models for algorithm
design. arXiv preprint arXiv:2410.14716 (2024)
12. Mersmann, O., Bischl, B., Trautmann, H., Preuss, M., Weihs, C., Rudolph, G.:
Exploratory landscape analysis. In: Proceedings of the 13th annual conference on
Genetic and evolutionary computation. pp. 829–836 (2011)
13. Nadizar, G., Rusin, F., Medvet, E., Ochoa, G.: The role of stepping stones in
map-elites: Insights from search trajectory networks. In: Genetic Programming.
pp. 224–239. Springer Nature Switzerland, Cham (2025)
14. Nezami, N., Anahideh, H.: Building trust in black-box optimization: A compre-
hensive framework for explainability. arXiv preprint arXiv:2410.14573 (2024)
15. de
Nobel,
J.,
Vermetten,
D.,
Wang,
H.,
Doerr,
C.,
Bäck,
T.:
Tun-
ing
as
a
means
of
assessing
the
benefits
of
new
ideas
in
interplay
with
existing
algorithmic
modules.
In:
Proc.
of
Genetic
and
Evolu-
tionary
Computation
Conference
(GECCO’21,
Companion
material).
pp.
1375–1384.
ACM
(2021).
https://doi.org/10.1145/3449726.3463167,
https://doi.org/10.1145/3449726.3463167
16. de Nobel, J., Ye, F., Vermetten, D., Wang, H., Doerr, C., Bäck, T.: Iohex-
perimenter: Benchmarking platform for iterative optimization heuristics. Evol.


--- Page 18 ---
18
N. van Stein et al.
Comput. 32(3), 205–210 (2024). https://doi.org/10.1162/EVCO\_A\_00342,
https://doi.org/10.1162/evco_a_00342
17. Ochoa, G., Malan, K.M., Blum, C.: Search trajectory networks: A tool for
analysing and visualising the behaviour of metaheuristics. Applied Soft Computing
109, 107492 (2021). https://doi.org/https://doi.org/10.1016/j.asoc.2021.107492,
https://www.sciencedirect.com/science/article/pii/S1568494621004154
18. Ochoa, G., Verel, S., Daolio, F., Tomassini, M.: Local optima networks: A new
model of combinatorial fitness landscapes. Recent advances in the theory and ap-
plication of fitness landscapes pp. 233–262 (2014)
19. Plaat, A., Wong, A., Verberne, S., Broekens, J., van Stein, N., Back, T.: Reasoning
with large language models, a survey. arXiv preprint arXiv:2407.11511 (2024)
20. Pluhacek, M., Kazikova, A., Kadavy, T., Viktorin, A., Senkerik, R.: Leveraging
large language models for the generation of novel metaheuristic optimization algo-
rithms. In: Proceedings of the Companion Conference on Genetic and Evolutionary
Computation. pp. 1812–1820 (2023)
21. Pulatov, D., Anastacio, M., Kotthoff, L., Hoos, H.: Opening the black box: Au-
tomated software analysis for algorithm selection. In: International Conference on
Automated Machine Learning. pp. 6–1. PMLR (2022)
22. van Rijn, S., Wang, H., van Leeuwen, M., Bäck, T.: Evolving the structure of evo-
lution strategies. In: 2016 IEEE Symposium Series on Computational Intelligence
(SSCI). pp. 1–8 (2016). https://doi.org/10.1109/SSCI.2016.7850138
23. Romera-Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar, M.P.,
Dupont, E., Ruiz, F.J., Ellenberg, J.S., Wang, P., Fawzi, O., Kohli, P., Fawzi,
A.: Mathematical discoveries from program search with large language models.
Nature 625, 468–475 (01 2024)
24. Sarti, S., Adair, J., Ochoa, G.: Neuroevolution trajectory networks of the behaviour
space. In: International Conference on the Applications of Evolutionary Computa-
tion (Part of EvoStar). pp. 685–703. Springer (2022)
25. van Stein, N., Bäck, T.: Llamea: A large language model evolutionary algorithm
for automatically generating metaheuristics. IEEE Transactions on Evolutionary
Computation pp. 1–1 (2024). https://doi.org/10.1109/TEVC.2024.3497793
26. van Stein, N., Kononova, A.V., Kotthoff, L., Bäck, T.: Code evolution graphs:
Understanding large language model driven design of algorithms. arXiv preprint
arXiv:2503.16668 (2025)
27. van Stein, N., Kononova, A.V., Yin, H., Bäck, T.: Blade: Benchmark suite for llm-
driven automated design and evolution of iterative optimisation heuristics. arXiv
preprint arXiv:2504.20183 (2025)
28. van Stein, N., Vermetten, D., Bäck, T.: In-the-loop hyper-parameter optimization
for llm-based automated design of heuristics. ACM Transactions on Evolutionary
Learning (2024)
29. van Stein, N., Vermetten, D., Kononova, A.V., Bäck, T.: Explainable benchmarking
for iterative optimization heuristics (2024), arXiv:2401.17842
30. Stützle, T., López-Ibáñez, M.: Automated design of metaheuristic algorithms. In:
Handbook of metaheuristics, pp. 541–579. Springer (2018)
31. Subburaj, B., Maheswari, J.U., Ibrahim, S.S., Kavitha, M.S.: Population diversity
control based differential evolution algorithm using fuzzy system for noisy multi-
objective optimization problems. Scientific Reports 14(1), 17863 (2024)
32. Telea, A., Auber, D.: Code flows: Visualizing structural evolution of source code.
In: Computer Graphics Forum. vol. 27, pp. 831–838. Wiley Online Library (2008)


--- Page 19 ---
Behaviour Space Analysis of LLM-driven Meta-heuristic Discovery
19
33. Thomson, S.L., Renau, Q., Vermetten, D., Hart, E., Stein, N.v., Kononova, A.V.:
Stalling in space: Attractor analysis for any algorithm. In: International Conference
on the Applications of Evolutionary Computation (Part of EvoStar). pp. 510–526.
Springer (2025)
34. Vermetten, D., Caraffini, F., Kononova, A.V., Bäck, T.: Modular differen-
tial evolution. In: Proceedings of the Genetic and Evolutionary Computa-
tion Conference. p. 864–872. GECCO ’23, Association for Computing Ma-
chinery, New York, NY, USA (2023). https://doi.org/10.1145/3583131.3590417,
https://doi.org/10.1145/3583131.3590417
35. Ye, H., Wang, J., Cao, Z., Berto, F., Hua, C., Kim, H., Park, J., Song, G.: Reevo:
Large language models as hyper-heuristics with reflective evolution. arXiv preprint
arXiv:2402.01145 (2024)
36. Yin, H., Kononova, A.V., Bäck, T., van Stein, N.: Controlling the mutation in
large language models for the efficient evolution of algorithms. In: International
Conference on the Applications of Evolutionary Computation (Part of EvoStar).
pp. 403–417. Springer (2025)
37. Zhang, R., Liu, F., Lin, X., Wang, Z., Lu, Z., Zhang, Q.: Understanding the im-
portance of evolutionary search in automated heuristic design with large language
models. In: International Conference on Parallel Problem Solving from Nature. pp.
185–202. Springer (2024)
