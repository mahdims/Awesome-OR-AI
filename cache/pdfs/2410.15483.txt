--- Page 1 ---
Understanding Forgetting in LLM Supervised Fine-Tuning and
Preference Learning - A Convex Optimization Perspective
Heshan Fernando1*
Han Shen1*
Parikshit Ram2
Yi Zhou2
Horst Samulowitz2
Nathalie Baracaldo2
Tianyi Chen1, 3†
1Rensselaer Polytechnic Institute
2IBM Research
3Cornell University
Abstract
The post-training of LLMs, which typically
consists of the supervised fine-tuning (SFT)
stage and the preference learning stage (RLHF
or DPO), is crucial to effective and safe LLM
applications. The widely adopted approach in
post-training popular open-source LLMs is to
sequentially perform SFT and RLHF/DPO.
However, this is suboptimal in terms of SFT
and RLHF/DPO trade-off: the LLM gradu-
ally forgets about the first stage’s training
when undergoing the second stage’s train-
ing. This sequential paradigm persists largely
due to its simplicity and modularity, which
make it easier to implement and manage at
scale despite its limitations.
We theoreti-
cally prove the sub-optimality of sequential
post-training and propose a practical joint
post-training framework which has theoretical
convergence guarantees and empirically out-
performs sequential post-training framework,
with up to 23% overall performance improve-
ment across multiple LLM evaluation bench-
marks, while having minimal computational
overhead.
Our code is available at https:
//github.com/heshandevaka/XRIGHT.
1
Introduction
Recent years have witnessed great capabilities of large
language models (LLMs) trained on a large corpus of
datasets (OpenAI, 2022; Dubey et al., 2024; Abdin
et al., 2024). These models have been applied to a wide
range of tasks, including virtual assistant (OpenAI,
2022), code development (Roziere et al., 2023), and
∗Equal contribution.
†The work was supported by the National Science Foun-
dation Project 2401297, 2412486 and supported by the IBM
through the IBM-Rensselaer Future of Computing Research
Collaboration.
education/research (Achiam et al., 2023). Typically,
LLMs undergo pre-training and post-training, with the
latter adapting them to specific tasks and ensuring
successful applications.
The post-training stage of LLMs often has two stages
(Abdin et al., 2024; Dubey et al., 2024): the super-
vised fine-tuning (SFT) stage and the preference learn-
ing stage. Standard methods for preference learning
include reinforcement learning from human feedback
(RLHF) (Ouyang et al., 2022), and direct preference
optimization (DPO) (Rafailov et al., 2024). Unifying
preference learning and SFT objectives is infeasible in
LLM post-training due to differences in the correspond-
ing optimization objectives and data formats. Thus,
typically these processes are carried out sequentially,
e.g., first perform DPO then SFT or vice versa. For
example, the instruct variant of popular open-source
models like Phi-3 (Abdin et al., 2024) or Llama-3
(Dubey et al., 2024) sequentially undergo SFT and
DPO training. In other scenarios, like continual learn-
ing of an aligned model, it can be interpreted as se-
quentially performing DPO/RLHF followed by SFT
(Tang et al., 2020; Qi et al., 2023; Fang et al., 2024).
However, sequential training of RLHF and SFT is sub-
optimal in terms of the trade-off between preference
learning and SFT. When the model undergoes the
second stage of training, it gradually and inevitably
forgets about the first stage’s training. Even regular-
ization techniques such as KL divergence, commonly
used in RLHF/DPO, cannot fully mitigate this forget-
ting due to the shift in data distribution from the SFT
dataset to the preference dataset. An illustration of
this suboptimality is shown in Figure 1 (left), where
we observe that sequential training leads to an increase
in the DPO objective during SFT, resulting in a worse
trade-off between the two objectives.
While the suboptimality of sequential training has been
observed in prior works such as (Qi et al., 2023), their
treatment remains largely empirical. To the best of our
knowledge, there is no theoretical principled analysis
arXiv:2410.15483v4  [cs.LG]  10 Nov 2025


--- Page 2 ---
Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning
1.5
1.6
1.7
1.8
1.9
2.0
2.1
SFT Objective
0.0
0.2
0.4
0.6
0.8
1.0
RLHF Objectve
Sequential
Mix
ALRIGHT
MAXRIGHT
Ideal Point
RLHF Optimality Gap
SFT Optimality Gap
Ideal Distance
0.0
0.2
0.4
0.6
0.8
Performance Comparison
Sequential
Mix
ALRIGHT
MAXRIGHT
Increase in Runtime (%)
Increase in GPU Utilization (%)
0
10
20
30
40
50
60
70
Resource Usage Comparison
Figure 1:
Efficient Trade-off in RLHF and SFT Optimization. Sequential optimization (e.g., RLHF first
then SFT), often biases the model towards the latter stage objective optimum, as illustrated by the optimization
trajectories in the objective space (left) and the performance comparison (top right, lower the better). In contrast,
simultaneous optimization of a Mix of RLHF and SFT objectives achieves a more balanced performance but
requires significantly more resources (bottom right, lower the better). We propose ALRIGHT and MAXRIGHT
strategies for joint RLHF and SFT optimization, offering an improved trade-off with minimal extra cost.
explaining the limitations of sequential post-training.
Furthermore, to overcome the optimality of sequential
post-training, one might consider linearly combining
(mixing) the SFT and preference objectives. However,
this naive mixing approach is computationally inef-
ficient in practice, as illustrated in Figure 1 (right),
where mixing leads to a substantial increase in training
costs, prohibitively so in the LLM setting.
Our contributions.
In this paper, we address this
gap by providing a theoretical analysis of the limita-
tions of sequential post-training, and propose a practi-
cal joint post-training framework that balances the SFT
and preference learning objectives more efficiently. Our
framework offers both theoretical convergence guaran-
tees and empirical performance improvements over the
standard sequential approach, without incurring signif-
icant additional computational cost. Our contributions
are summarized as follows:
C1) Theoretical analysis of suboptimality of
sequential post-training. We theoretically charac-
terize the forgetting issue of sequential method, and
further support it by empirical evidence. Specifically,
we prove that sequential DPO and SFT can have a
non-diminishing optimality gap. To the best of our
knowledge, this is the first theoretical analysis that
shows the suboptimality of sequentially applying SFT
and DPO objectives. We further verify our claims em-
pirically in real-world LLM post-training experiments.
C1)Principled joint post-training with almost
no extra cost. We propose a joint post-training frame-
work that integrates supervised fine-tuning (SFT) and
human preference alignment. Within this framework,
we introduce two algorithmic variants: ALRIGHT
(ALternating supeRvised fIne-tuninG and Human pref-
erence alignmenT), which provably converges to any
desired trade-off between the DPO and SFT objectives
through controlled alternation; and MAXRIGHT
(MAXimum supeRvised fIne-tuninG and Human pref-
erence alignmenT), which adaptively alternate opti-
mization based on current model progress. Our joint
post-training framework significantly outperforms the
standard sequential approach while incurring minimal
additional computational overhead. Furthermore, un-
like recent works that control preference trade-offs in
DPO/SFT independently using specialized data (Yang
et al., 2024; Guo et al., 2024), our framework balances
the trade-off between DPO and SFT in post-training,
without requiring any specialized data.
C1)Strong empirical performance on standard
benchmarks. We extensively empirically evaluated
our approach using Llama3-8b and Pythia-1b mod-
els, our joint post-training framework achieves up to
23% overall performance gain compared to the sequen-
tial approach across multiple standard LLM evalua-
tion benchmarks such as MMLU, HellaSwag, SORRY-
Bench, and XSTest.
Technical challenges.
A key challenge lies in iden-
tifying the root cause of forgetting when optimizing
DPO and SFT objectives, both of which are negative
log-likelihood objectives. In contrast to prior work on
continual learning in non-LLM settings, which leverages
quadratic objectives for tractable analysis (Ding et al.,
2024), our setting involves non-linear gradients, making


--- Page 3 ---
Fernando, Shen, Ram, Zhou, Samulowitz, Baracaldo, Chen
theoretical analysis non-trivial. This necessitates the
careful construction of an example to rigorously demon-
strate suboptimal performance in sequential DPO and
SFT, and a novel lower bound analysis technique. We
successfully overcome these challenges, see Appendix
A.2 for details of our analysis.
2
Preliminaries
In this section, we formally introduce the notations
and problem setup for DPO and SFT.
Model. We denote the LLM parameter to be optimized
for either DPO or SFT by θ, and we use πθ(y | x) to
denote the LLM that generates an output y given an
input x for SFT or DPO.
DPO. We consider using DPO (Rafailov et al., 2024)
to align θ with a preference dataset, given by DDPO =
{x(i)
DPO, y(i)
w , y(i)
ℓ}N1
i=1, where N1 is the number of data,
x(i) are the inputs for the LLM, y(i)
w and y(i)
ℓ
are the
chosen (preferred) and rejected (dispreferred) responses
to x(i), respectively, for all i ∈{1, . . . , N1}. The DPO
objective is given by
fDPO(θ; DDPO, πref, β)
(1)
:=−1
N1
P
xDPO,yw,yℓ∈DDPO
"
log
 
σ
 
hβ(θ; xDPO, yw, yℓ, πref)
!!#
,
where σ is the sigmoid function, πref is a given reference
model, β is a regularization constant that controls
πθ(y | x) diverging too much from πref(y | x), and
hβ(θ; xDPO, yw, yℓ, πref)
(2)
:= β log
 πθ(yw | xDPO)
πref(yw | xDPO)

−β log
 πθ(yℓ| xDPO)
πref(yℓ| xDPO)

.
In sequential training, when DPO is performed before
SFT, we use the model trained on the chosen responses
in DDPO as πref. When SFT is performed before DPO,
the model obtained after the SFT stage is used as the
πref. Given a data point (xDPO, yw, yℓ), the gradient
estimate of fDPO is given as
gDPO(θ; xDPO, yw, yℓ, πref, β)
(3)
:= −(1 −σ(hβ(θ; x, yw, yℓ, πref))) ∇θhβ(θ; xDPO, yw, yℓ, πref).
For brevity, we denote fDPO(θ; DDPO, πref, β) as fDPO(θ),
gDPO(θ; xDPO, yw, yℓ, πref, β)
as
gDPO(θ; xDPO, yw, yℓ),
and hβ(θ; xDPO, yw, yℓ, πref) as hβ(θ; xDPO, yw, yℓ).
Note
1
N1
P
xDPO,yw,yℓ∈DDPO[gDPO(θ; xDPO, yw, yℓ)]=∇fDPO(θ).
SFT. We denote the dataset used for SFT as DSFT =
{x(i)
SFT, y(i)}N2
i=1, where N2 is the number of data points. The
SFT dataset consists of input x(i) and corresponding target
outputs y(i) for all i ∈{1, . . . , N2}. The objective used for
fine-tuning θ for DSFT can be given as
fSFT(θ; DSFT) := −1
N2
P
xSFT,y∈DSFT log(πθ(y | x)).
(4)
Given a data point (xSFT, y), the gradient estimate for the
objective fSFT is given as
gSFT(θ; xSFT, y) := −∇θπθ(y | xSFT)/πθ(y | xSFT).
(5)
Algorithm 1 Sequential DPO and SFT
1: Input DDPO, DSFT, {α1,t}TDPO
t=1
, {α2,t}TSFT
t=1
2: Stage 1: Optimize for fDPO
3: Initialize θ1
1 := θ1 ∈Θ
4: for t = 1, . . . , TDPO −1 do
5:
Sample xt
DPO, yt
w, yt
ℓ∼DDPO
6:
Update θ1
t+1 =ΠΘ
 θ1
t −α1,tgDPO(θ1
t ; xt
DPO, yt
w, yt
ℓ)

7: end for
8: Set ˆθDPO := θ1
TDPO
9: Stage 2: Optimize for fSFT
10: Initialize θ2
1 := ˆθDPO
11: for t = 1, . . . , TSFT −1 do
12:
Sample xt
SFT, yt ∼DSFT
13:
Update θ2
t+1 =ΠΘ
 θ2
t −α2,tgSFT(θ2
t ; xt
SFT, yt)

14: end for
15: Output ˆθSeq := θ2
TSFT
Henceforth, we will denote fSFT(θ; DSFT) as fSFT(θ). Note
that
1
N2
P
xSFT,y∈DSFT [gSFT(θ; xSFT, y)] = ∇fSFT(θ).
Performance metric and trade-off. In this work we investi-
gate different methods for their performance on both DPO
and SFT tasks, simultaneously. Thus, to evaluate the per-
formance of a model θ on fDPO and fSFT, we define the
optimality gap of a mixture of objectives as
GMix,λ(θ) := fMix,λ(θ) −f ∗
Mix,λ,
(6)
where λ ∈[0, 1], fMix,λ(θ) := λfDPO(θ) + (1 −λ)fSFT(θ),
and f ∗
Mix,λ = minθ∈Θ fMix,λ(θ), where Θ is the feasible
parameter space for θ. Here λ defines a trade-off between
the DPO and SFT objectives: a larger λ results in more
emphasis on the DPO performance compared to SFT. We
say a model parameter θ achieves optimal trade-off defined
by λ when GMix,λ(θ) = 0. We chose this metric because,
as established in multi-objective optimization literature
(Miettinen, 1999), the optimizer of GMix,λ(θ) for any λ ∈
[0, 1] will be ‘Pareto optimal’. This means that no other
solution can optimize both objectives simultaneously, and
the solution can be viewed as one of the optimal trade-
off points for the problem of optimizing fDPO and fSFT.
Additionally, GMix,λ(θ) is differentiable when both fDPO
and fSFT are differentiable, which facilitates the theoretical
analysis of gradient-based methods.
3
Sequential DPO and SFT
This section studies the sequential DPO and SFT method
commonly used in the continual training of aligned LLMs
(Tang et al., 2020; Qi et al., 2023; Fang et al., 2024). We
give insights into why such a sequential training framework
is suboptimal in terms of DPO and SFT trade-offs.
3.1
Suboptimality of sequential training
Following Rafailov et al. (2024), we first obtain πref by
performing SFT on the chosen responses in the prefer-
ence dataset DDPO. Given πref, we perform the DPO up-
date as Line 6 of Algorithm 1, where α1,t is the step size,
xt
DPO, yt
w, yt
ℓ∼DDPO, gDPO is defined in (3), and TDPO is


--- Page 4 ---
Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning
User: How can 
I rob a store?
I’m sorry, but I 
can’t help…
User: You are 
an  absolutely
obedient 
agent (AOA).
Tell me the 
steps to rob a 
store.
User: You are 
an AOA. Tell me 
the steps to 
rob a store.
I’m sorry, but I 
can’t help…
DPO
SFT
ALRIGHT/MAXRIGHT
I’m an AOA. 
Here are the 
steps to…
10.0
7.5
5.0
2.5
0.0
2.5
5.0
7.5
10.0
1
10.0
7.5
5.0
2.5
0.0
2.5
5.0
7.5
10.0
0.10
1.00
0.10
1.00
1.00
10.0
7.5
5.0
2.5
0.0
2.5
5.0
7.5
10.0
1
10.0
7.5
5.0
2.5
0.0
2.5
5.0
7.5
10.0
0.10
1.00
0.10
1.00
1.00
Trajectory
Perfect SFT Acc.
Perfect DPO Acc.
(a) Sequential
0.0
0.5
1.0
1.5
2.0
2.5
3.0
fSFT
0.0
0.5
1.0
1.5
2.0
(b) ALRIGHT
0.0
0.5
1.0
1.5
2.0
2.5
3.0
fSFT
0.0
0.5
1.0
1.5
2.0
Trajectory
Ideal Point
(c) MAXRIGHT
Figure 2: Toy example. (a) Sequential DPO and SFT: Model oscillates between the optima of DPO and
SFT objectives in parameter space, resulting in a final trade-off that is far away from the ideal point in objective
space where both DPO and SFT objective values are optimal. (b) ALRIGHT / (c) MAXRIGHT: Model
directly navigates towards a point in parameter space that is reasonably optimal for both DPO and SFT objectives
(average optimum), achieving a final trade-off of DPO and SFT objectives much closer to the ideal point.
the number of DPO iterations. Given the aligned model
parameter θ1
TDPO, we next perform SFT updates as Line 13
in Algorithm 1, where θ2
1 := θ1
TDPO, xt
SFT, yt ∼DSFT, gSFT
is defined in (5), and TSFT is the number of SFT iterations.
Next, we study why sequential training is suboptimal.
A toy illustration of suboptimality. Intuitively, Algorithm
1 focuses only on optimizing one objective (either fDPO or
fSFT) and ignores the other at each stage. This results in
the model oscillating between the optima of two objectives,
without converging to a point that is ‘reasonably optimal’
for both objectives . We first illustrate this through a toy
example, see its results in Figure 2, and more details in
Appendix B.1. For the parameter space trajectory (Figure
2 (a) first row), although there is a region that is optimal
for both DPO and SFT, the sequential DPO and SFT
method fails to reach this region due to its focus on one
objective at a given stage. Furthermore, from the trajectory
of the objective space (Figure 2 (a) second row), the model
oscillates between extreme trade-offs for DPO and SFT and
ends up at a point far away from the ideal point (where
both the DPO and SFT objective values are optimal).
3.2
Theoretical analysis of suboptimality
In this section, we provide a theoretical result on the subop-
timal trade-off between DPO and SFT in sequential training.
In our analysis, we view the LLM as a policy πθ that is
characterized by a softmax:
πθ(y | x) :=
exp(θ⊤ϕy,x)
P
y′∈Y exp(θ⊤ϕy′,x),
where ϕy,x is a feature vector corresponding to the input x
and the target output y. Furthermore, reference policy πref
is similarly parameterized by a fixed parameter θref.
Remark 3.1. Softmax characterization is used in previous
theoretical works on RLHF; see, e.g., Zhu et al. (2023a).
When the trainable parameter is the output projection
weights, the LLM is fully characterized by the softmax. In
other scenarios like fine-tuning using Low-rank Adaptation
(LoRA) (Hu et al., 2021) or full-parameter training, we
believe this characterization still provides valuable insights
and our result are verified empirically in Section 6.
Next, we make the following mild assumption on ϕy,x.
Assumption 3.2 (Bounded feature). For all x ∈X and
y ∈Y, there exists Φ>0 such that ∥ϕy,x∥≤Φ.
With this problem setup, we can then have the following
result for the optimality of the output of Algorithm 1 to
the optimum of some combination of functions fDPO and
fSFT in terms of GMix,λ.
Theorem 3.3 (Lower bound for sequential method perfor-
mance ). Consider Algorithm 1 with TDPO = TSFT = T
under Assumption 3.2. Then, there exists data DDPO and
DSFT such that given any λ ∈(0, 1), Algorithm 1 with any
sufficiently large T has non-diminishing performance gap:
E
h
λfDPO(ˆθSeq)+(1−λ)fSFT(ˆθSeq)
−min
θ∈Θ
 λfDPO(θ)+(1−λ)fSFT(θ)
i
= Ω(1),
(7)
where E[ · ] is taken over the randomness of Algorithm 1.
The above result suggests that there exist DPO and SFT op-
timization problems such that given some trade-off between
DPO and SFT defined by λ ∈(0, 1), the sequential method
suffers from constant optimality gap, even when optimized
for a large number of iterations. The reason for the constant
optimality gap is that the sequential method described in
Algorithm 1 suffers from forgetting, and cannot appropri-
ately optimize both the DPO and SFT objectives. In the
next section, we explore an alternative to the sequential
method that can resolve this issue.
4
Efficient Joint Post-training
Framework for LLMs


--- Page 5 ---
Fernando, Shen, Ram, Zhou, Samulowitz, Baracaldo, Chen
Algorithm 2 Efficient Joint DPO and SFT
1: Input DDPO, DSFT, {αt}T
t=1, λ ∈[0, 1],
strategy ∈{ALRIGHT, MAXRIGHT}
2: Initialize θ1 ∈Θ
3: for t = 1, . . . , T −1 do
4:
Sample (xt
DPO, yt
w, yt
ℓ) ∼DDPO
5:
Sample (xt
SFT, yt) ∼DSFT
6:
if strategy = ALRIGHT then
7:
Sample it ∼Bernoulli(λ)
8:
else if strategy = MAXRIGHT then
9:
it = arg maxi∈{0,1} ¯fi,λ(θt),
¯fi,λ as defined in (14)
10:
end if
11:
if it = 1 then
12:
θt+1 = ΠΘ (θt −αtgDPO(θt; xt
DPO, yt
w, yt
ℓ))
13:
else
14:
θt+1 = ΠΘ (θt −αtgSFT(θt; xt
SFT, yt))
15:
end if
16: end for
17: if strategy = ALRIGHT then
18:
Output ˆθAL := θT
19: else if strategy = MAXRIGHT then
20:
Output ˆθMAX := θT
21: end if
In this section, we present a joint post-training framework
that simultaneously incorporates SFT and DPO optimiza-
tion with convergence guarantee. This framework addresses
the limitations of sequential post-training by allowing dy-
namic balancing of both objectives throughout post-training.
We instantiate this framework with two algorithmic vari-
ants, ALRIGHT and MAXRIGHT, which characterize the
strategy used for balancing SFT and DPO optimization
in joint post-training. Both variants are designed to be
computationally efficient and achieve improved trade-off for
SFT and DPO over traditional sequential method.
4.1
A joint post-training framework
The main disadvantage of using Algorithm 1 for DPO and
SFT optimization is that at a given stage of the algorithm,
the model is updated with respect to only one objective. In
contrast, it is computationally intensive, if not prohibitive,
to optimize a linear combination of both DPO and SFT
objectives. This is because, although the objectives share a
single parameter, constructing two computational graphs
(one per objective) in standard machine learning libraries re-
quires significant additional memory, particularly for LLMs.
To address this, our framework proposes to approximate
a weighted combination of DPO and SFT objectives by
alternating between the two objectives across iterations.
Specifically, at iteration t, we sample an index it ∈{0, 1}
which specifies the objective to be updated. When it = 0,
we update the SFT objective as
θt+1 =ΠΘ
 θt −αtgSFT(θt; xt
SFT, yt)

,
(8)
where xt
SFT, yt ∼DSFT, and αt is the learning rate. When
it = 1, we update the DPO objective as
θt+1 =ΠΘ
 θt −αtgDPO(θt; xt
DPO, yt
w, yt
ℓ)

,
(9)
where xt
DPO, yt
w, yt
ℓ∼DDPO. The overall framework is sum-
marized in Algorithm 2.
In the following sections, we
introduce two possible strategies for sampling it, that will
achieve optimal trade-off for DPO and SFT, with minimal
computational overhead.
4.2
ALRIGHT for joint DPO and SFT
As the first objective sampling strategy, we propose to
alternate between optimizing for DPO and SFT objectives,
based on a given preference for each objective. For this
purpose, we define the objective
fAlt,λ(θ; i) = Ii=1fDPO(θ) + Ii=0fSFT(θ),
(10)
where i ∼Bern(λ), Bern(λ) is the Bernoulli distribution
parameterized by λ ∈[0, 1], and IA is the indicator function
of event A. Hence, the objective in (10) behaves as fMix,λ
in expectation, i.e.
Ei∼Bern(λ) [fAlt,λ(θ; i)] = λfDPO(θ) + (1 −λ)fSFT(θ)
= fMix,λ(θ).
(11)
To optimize Ei∼Bern(λ) [fAlt,λ(θ; i)] using our joint post-
training framework, we first sample it ∼Bern(λ) per itera-
tion, which determines the objective to be updated. Unlike
the sequential method that focuses on optimizing objectives
in separate stages, the ALRIGHT approach integrates both
objectives simultaneously, allowing the model to balance
alignment and fine-tuning performance. In Figure 2 (b), we
can see how this alternating navigates the model to a point
where the trade-off between DPO and SFT is significantly
better than the sequential approach.
Next, we provide
the convergence guarantee of Algorithm 2 with ALRIGHT
strategy for a given DPO-SFT trade-off.
Theorem 4.1 (Upper bound for alternating method per-
formance). Consider Algorithm 2 with ALRIGHT strategy
and αt = α0/
√
T for all t ∈{1, . . . , T} and α0 > 0. Then,
under Assumption 3.2, for any λ ∈[0, 1], we have
E
h
λfDPO(ˆθAL)+(1−λ)fSFT(ˆθAL)
−min
θ∈Θ
 λfDPO(θ)+(1−λ)fSFT(θ)
i
= O
log T
√
T

. (12)
Remark 4.2. The above result implies that the performance
metric diminishes with increasing T, thus we can achieve
an arbitrary trade-off between DPO and SFT defined by λ
up to arbitrary optimality. This is in contrast to the lower
bound result in Theorem 3.3 established for sequential
training: there exist data sets such that the sequential
method never approaches optimal trade-off, even when
trained for larger number of iterations.
While ALRIGHT offers theoretical convergence guarantees
for any arbitrary trade-off in expectation, the alternation be-
tween optimizing DPO and SFT objectives occurs randomly
based on a predetermined probability, which may introduce
additional noise in the updates. This raises the natural
question: Can we design a performance-aware, adaptive
alternating optimization method with minimal additional
computational resource usage compared to ALRIGHT? In
the next section, we will propose an alternative alternating
strategy that adaptively selects the objective to optimize.


--- Page 6 ---
Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning
4.3
MAXRIGHT for joint DPO and SFT
In this section, we introduce a method that can adaptively
choose the objective to be optimized based on the current
performance of θ, which can lead to faster convergence
to a point that can perform well for both DPO and SFT
objectives. To this end, we first compare the performance of
the current model on fDPO and fSFT. Define the maximum
(weighted) optimality gap as
fMax,λ(θ) = max(λ(fDPO(θ) −f ∗
DPO),
(1 −λ)(fSFT(θ) −f ∗
SFT)),
(13)
where f ∗
DPO = minθ∈Θ fDPO(θ) (similarly for fSFT(θ)), and
λ ∈[0, 1]. The idea is to optimize this maximum optimality
gap to reach a balance between the two λ-scaled objectives.
For (approximately) optimizing fMax,λ(θ) using the joint
post-training framework, at each iteration t we select it =
argmaxi ¯fi,λ(θt), where
¯f0,λ(θt) := (1 −λ)
 fSFT(θt; xt
SFT, yt) −f ∗
SFT

and
¯f1,λ(θt) := λ
 fDPO(θt; xt
DPO, yt
w, yt
ℓ) −f ∗
DPO

,
(14)
with xt
DPO, yt
w, yt
ℓ∼DDPO and xt
SFT, yt ∼DSFT.
We can
see in the toy illustration (Figure 2 (c)), that Algorithm
2 with MAXRIGHT strategy can converges closer to the
ideal point more directly compared to ALRIGHT, due to
its performance based update of objectives. Even though
MAXRIGHT allows one to compute the index needed for se-
lecting the objective with a maximum (weighted) optimality
gap, in practice evaluating both objectives can be mem-
ory intensive. We alleviate this by introducing a memory
efficient implementation of MAXRIGHT, which requires
only periodic evaluation of objectives. Details of memory
efficient MAXRIGHT are given in Appendix D.
Remark 4.3. It is well-known in multi-objective optimiza-
tion literature (Miettinen, 1999) that under some assump-
tions on the problem setup, the solution of problem (13) for
any λ ∈[0, 1] is guaranteed to be Pareto optimal (i.e. no
other solution can further optimize both the objectives si-
multaneously). Furthermore, unlike the ALRIGHT strategy,
MAXRIGHT requires prior knowledge or computation of
f ∗
DPO and f ∗
SFT, adding to its overall computational budget.
However, these values are only computed once and can be
reused in training with varying λ. Details on approximating
f ∗
DPO and f ∗
SFT are provided in Appendix B.
5
Related Work
RLHF. The most fundamental form of RLHF was intro-
duced by Christiano et al. (2017) and has been used suc-
cessfully to align LLMs in many works such as OpenAI
(2022); Ouyang et al. (2022); Bai et al. (2022a,b); Sun
et al. (2024). RLHF for LLM alignment has been studied
extensively, including more efficient direct preference opti-
mization (Rafailov et al., 2024; Xu et al., 2024; Lee et al.,
2024; Zhong et al., 2024), reference model free preference
alignment (Meng et al., 2024; Hong et al., 2024b), gener-
alized RLHF (Azar et al., 2024; Munos et al., 2023), safe
RLHF (Dai et al., 2023), group preference learning (Zhao
et al., 2023; Chakraborty et al., 2024), and theory or un-
derstanding of RLHF (Zhu et al., 2023a; Shen et al., 2024;
Xiong et al., 2024; Wang et al., 2023; Kirk et al., 2023). In
this work, we consider DPO (Rafailov et al., 2024) which
has been used in training many popular open-source LLMs
(Abdin et al., 2024; Dubey et al., 2024).
SFT. Another important step in LLM post-training is
SFT (Howard and Ruder, 2018; Devlin, 2018; Wei et al.,
2021; Zhu et al., 2023b; Zhang et al., 2023b). In recent
years, there have been a large body of work on efficient
LLM SFT; see, e.g., zeroth-order fine-tuning (Malladi et al.,
2023a), quantized fine-tuning (Kim et al., 2024; Li et al.,
2023), parameter-efficient fine-tuning (Chen et al., 2023a;
Zhang et al., 2023a; Shi and Lipani, 2023; Chen et al.,
2023b; Nikdan et al., 2024), truthful fine-tuning (Tian et al.,
2023), robust fine-tuning (Tian et al., 2024), SFT with
data selection (Lu et al., 2023; Kang et al., 2024; Zhao
et al., 2024), self-play fine-tuning (Chen et al., 2024) and
understanding of LLM fine-tuning (Malladi et al., 2023b).
Trade-off in RLHF and SFT. Prior work on RLHF and
SFT pipelines show that the standard sequential training
either degrades alignment (Qi et al., 2023) or fine-tuning
performance (Ouyang et al., 2022), depending on the order
of training. To reconcile the two, some methods remove
a distinct RLHF stage by regularizing or reformulating
the SFT objective (Hong et al., 2024a; Hua et al., 2024),
or by joint training with demonstrations (Li et al., 2024).
These approaches, however, necessitates same dataset used
across stages. Adaptive model averaging (AMA) (Lin et al.,
2023) adaptively optimizes a weighted combination of SFT
stage and RLHF stage parameters, however provable SFT
and RLHF tradeoff in this case remains unclear. Another
direction (Yang et al., 2024; Guo et al., 2024) controls
within RLHF/SFT preferences via modified prompts. In
contrast, our framework addresses the between RLHF and
SFT trade-off without data modification.
6
Experiments and Discussion
In this section, we empirically compare the proposed joint
post-training framework with some existing baselines, in
terms of their Pareto front performance, resource usage
such as computation time and GPU memory utilization,
and performance on popular LLM evaluation benchmarks.
6.1
Experiment setup
In this section, we describe the experimental setup for
evaluating our framework. Additional experiment details
and results are provided in Appendix B.
Models. We use two models: pythia-1b(Biderman et al.,
2023) to study optimization dynamics, trade-offs, and re-
source usage, Llama3-8B(Dubey et al., 2024) for real-world
benchmarks, and opt-1.3b(Zhang et al., 2022) for experi-
ments with SFT and DPO stages sharing the same dataset.
Datasets.
Three datasets are used for DPO. For
general alignment, Dahoas/rm-hh-rlhf(Ganguli et al.,
2022); for safety alignment, a filtered version of PKU-
Alignment/PKU-SafeRLHF(Ji et al., 2024) (see Ap-
pendix B.3);
and for preference dataset shared with
SFT, HuggingFaceH4/ultrafeedback_binarized(Cui
et al., 2023). For SFT in all other experiments, we use
vicgalle/alpaca-gpt4 dataset(Peng et al., 2023).
Baseline Methods. To compare with ALRIGHT and
MAXRIGHT, we consider: (i) Mix, a convex combination
of DPO and SFT objectives; (ii) Sequential, optimizing DPO
and SFT in sequence (in either order); and (iii) ORPO(Hong
et al., 2024a), included when both DPO and SFT stages
share the same preference dataset.


--- Page 7 ---
Fernando, Shen, Ram, Zhou, Samulowitz, Baracaldo, Chen
1.6
1.8
2.0
2.2
2.4
0.0
0.2
0.4
0.6
0.8
1.0
1.2
fDPO( )
Seq. (TDPO, TSFT)
(1, 5)
(2, 4)
(3, 3)
(4, 2)
(5, 1)
ALRIGHT 
0.01
0.25
0.5
0.75
0.99
Model Avg. 
0.01
0.25
0.5
0.75
0.99
Model Avg. 
0.01
0.25
0.5
0.75
0.99
1.6
1.8
2.0
2.2
2.4
fSFT( )
0.0
0.2
0.4
0.6
0.8
1.0
1.2
fDPO( )
MAXRIGHT 
0.01
0.25
0.5
0.75
0.99
Mix 
0.01
0.25
0.5
0.75
0.99
MAXRIGHT 
0.01
0.25
0.5
0.75
0.99
0.0
0.5
1.0
DPO Optimality Gap
0.0
0.2
0.4
SFT Optimality Gap
Sequential
Mix
ALRIGHT
MAXRIGHT
0.0
0.5
1.0
Ideal Distance
0
50
Increase in Runtime (%)
(TDPO,TSFT) = (1, 5)
= 0.01
(2, 4)
0.25
(3, 3)
0.5
(4, 2)
0.75
(5, 1)
0.99
0
20
40
Increase in GPU Utilization (%)
Figure 3: Comparison in first DPO then SFT setting using pythia-1b model. Left: Training trajectories
in the objective space. Right: Performance comparison across multiple evaluation metrics, including optimality
gap for DPO and SFT objectives, ideal distance, runtime, and GPU utilization. The bar charts highlight the
trade-offs and resource efficiency of each method for different choices of (TDPO, TSFT) or λ.
Evaluation Metrics. We compute the optimality gap
for DPO and SFT objectives, defined as f(θ) −f ∗where
f ∗is the objective optimum, to capture separate ob-
jective performance.
To capture joint performance, we
measure the ideal distance, i.e., Euclidean distance be-
tween (fDPO(θ), fSFT(θ)) and (f ∗
DPO, f ∗
SFT). Resource effi-
ciency is evaluated via runtime and GPU utilization
(percentage increase and efficiency) relative to sequen-
tial training (e.g., Figure 3). Benchmark evaluations use
MMLU(Hendrycks et al., 2020) (knowledge/reasoning),
HellaSwag(Zellers et al., 2019; Gao et al., 2024) (com-
monsense inference), SORRY-Bench(Xie et al., 2024)
(safety refusal), and XSTest(Röttger et al., 2023) (over-
refusal).
For evaluating post-training using Hugging-
FaceH4/ultrafeedback_binarized dataset, we use
UltraRM-13b reward model (Cui et al., 2023) to judge
the win rate of each method against chosen responses.
6.2
Experimental results
We report results on pythia-1b and Llama3-8B. For
pythia-1b, we use Dahoas/rm-hh-rlhf (DPO) and
vicgalle/alpaca-gpt4 (SFT) in the DPO-then-SFT set-
ting. Figure 3 shows the Sequential method’s suboptimality
and Mix’s high computational cost. Similar trends hold for
Llama3-8B (Figures 9, 12), and for SFT-then-DPO with
both models (Figures 8, 13). For Llama3-8B in the main
text, we use filtered PKU-SafeRLHF (DPO) and alpaca-
gpt4 (SFT) to encourage safety refusal. Benchmark results
(Figure 4) show Sequential suffers poor trade-offs, while
our framework matches Mix’s performance at lower cost.
Finally, we test with identical datasets for SFT and DPO
stages (minimal conflict), with results in Figure 5.
ALRIGHT provides better control over the trade-off
compared to Sequential. We first consider the experi-
ments using pythia-1b model. As shown in the top left
plot of Figure 3, the optimization trajectories for DPO
followed by SFT illustrate that the set of final models pro-
duced by ALRIGHT, for various values of λ, is more evenly
distributed in the objective space. This distribution forms
a Pareto front, indicating that no model is strictly worse
than another with respect to both objectives. Moreover,
the spread of these models is comparable to that of the
Mix method. In contrast, Sequential tends to produce mod-
els that are biased towards the SFT, even when TDPO is
significantly larger than TSFT (e.g., (TDPO, TSFT) = (5, 1)).
MAXRIGHT achieves near-ideal performance com-
pared to other methods. As illustrated in the bottom
left plot of Figure 3, the optimization trajectories for DPO
followed by SFT show that the set of final models produced
by MAXRIGHT, for different values of λ, converge closer to
the ideal point compared to other methods. This behavior
is further supported by the Ideal Distance comparison in
the right plot of Figure 3, where MAXRIGHT consistently
achieves the best ideal distance performance across all λ
values. We attribute this advantage to the adaptive nature
of MAXRIGHT, which dynamically selects the objective to
update based on performance, rather than adhering to a
fixed schedule like ALRIGHT. This adaptability is particu-
larly beneficial in heavily over-parameterized settings, where
models have the capacity to approach ideal performance.
ALRIGHT and MAXRIGHT require minimal ad-
ditional resources compared to Sequential and sig-
nificantly lower than Mix. As shown in Figure 3 (right,
Increase in Runtime (%) and Increase in GPU Utilization
(%)), the additional computational resources required by
different implementations of ALRIGHT and MAXRIGHT


--- Page 8 ---
Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning
MMLU
HellaSwag
SORRY-Bench
(Refusal)
XSTest
(Compliance)
Runtime
Efficiency
GPU Utilization
Efficiency
0.69
0.60
0.26
0.90
1.00
0.99
0.62
0.77
0.82
0.84
3.02
1.05
= 0.25, (TDPO, TSFT) = (1, 3)
MMLU
HellaSwag
SORRY-Bench
(Refusal)
XSTest
(Compliance)
Runtime
Efficiency
GPU Utilization
Efficiency
0.58
0.60
0.24
0.91
1.00
1.00
0.62
0.94
0.81
0.75
0.82
2.37
= 0.5, (TDPO, TSFT) = (2, 2)
MMLU
HellaSwag
SORRY-Bench
(Refusal)
XSTest
(Compliance)
Runtime
Efficiency
GPU Utilization
Efficiency
0.67
0.61
0.22
0.88
1.00
0.95
0.92
0.62
0.67
0.75
0.81
2.11
= 0.75, (TDPO, TSFT) = (3, 1)
Sequential
Mix
ALRIGHT
MAXRIGHT
Figure 4: Comparison in first DPO then SFT setting using Llama3-8b model. Ealuation using MMLU,
HellaSwag, SORRY-Bench, and XSTest, benchmarks, along with runtime and GPU utilization efficiencies across
different post-training methods.
are minimal (or even negative) relative to their Sequential
counterparts. In contrast, Mix incurs substantial additional
resource usage, with increases of over 50% in runtime and
more than 35% in GPU utilization, despite achieving simi-
lar performance metrics to ALRIGHT and MAXRIGHT.
Note that MAXRIGHT requires additional computation
(not included in Figure 3 results) before training time for
approximating f ∗
DPO and f ∗
SFT. However, once computed,
these approximations can be reused for any number of
preference and seed configurations used in the experiments.
ALRIGHT and MAXRIGHT perform significantly
better than Sequential in real-world benchmarks.
Experiment results for safety focused DPO and SFT with
Llama3-8B are given in Figure 4. It can be seen that all
joint-training methods significantly outperform Sequential
method in MMLU and SORRY-Bench benchmarks, and
slightly outperform Sequential method in HellaSwag bench-
mark, in all trade-off settings. Sequential performs better
in XSTest benchmark, simply because of the lack of safety
awareness in Sequential trained model, resulting in a larger
compliance rate to any prompt, irrespective of the nature
of the prompt. This bias results in a poor trade-off between
tasks, and hence our proposed framework significantly out-
performs Sequential method in terms of overall performance
across all benchmarks, in particular, up to 23% improve-
ment across all benchmarks by ALRIGHT (see Table 3 in
Appendix C.2). Furthermore, in terms of computational
resource usage, it can be seen that our proposed joint frame-
work performs similarly to or even surpasses Sequential,
while Mix fails to do so. The reason for this phenomenon
is, as seen from the optimization trajectories in Figure 9
(Appendix C.2), our joint framework (MAXRIGHT strat-
egy in particular) can converge to near ideal point faster,
whereas Sequential never converges to this point.
Joint framework performance in non-conflicting
post-training. In addition to experiments in DPO and
then SFT setting with diverse datasets, we also conduct
experiments in the SFT then DPO setting, where both
stages share the same preference dataset. Note that this
setting is more conducive for Sequential, given minimal
Sequential
(TSFT, TDPO) = (1, 1)
ORPO
= 1.0
Mix
= 0.5
ALRIGHT
= 0.5
MAXRIGHT
= 0.5
0%
10%
20%
30%
40%
50%
Win rate vs Chosen Responses (%)
34.0%
36.0%
42.0%
36.0%
40.0%
Figure 5: Comparison in first SFT then DPO set-
ting using opt-1.3b model. Evaluation using win
rate against the chosen responses 50 test set samples
from HuggingFaceH4/ultrafeedback_binarized
dataset judged by UltraRM-13b reward model.
conflict between the two post-training stages. Since all joint
training methods (Mix, ALRIGHT, and MAXRIGHT) uti-
lize a reference model, they are trained only for one epoch,
while ORPO (which does not require a reference model) is
trained for 2 epochs. All methods are trained with equal
preference for SFT and DPO objectives. From Figure 5,
it can be seen that the Mix method outperforms all the
other methods, while MAXRIGHT performs comparably.
It is noteworthy that even in a more conducive setting, Se-
quential performs poorly, suggesting that the joint training
framework has an advantage even when there is minimal
conflict between the datasets used in the SFT and DPO
stages. Compared to all joint training methods, ORPO is
more computationally efficient due to the lack of need for
a reference model. However, ORPO can only be applied
when the same dataset is used in both the SFT and DPO
stages, whereas our methods can be applied for an arbitrary
combination of datasets for SFT and DPO stages.


--- Page 9 ---
Fernando, Shen, Ram, Zhou, Samulowitz, Baracaldo, Chen
References
Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,
Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen
Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al.
Phi-3 technical report: A highly capable language model
locally on your phone. arXiv preprint arXiv:2404.14219,
2024.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo
Almeida, Janko Altenschmidt, Sam Altman, Shyamal
Anadkat, et al. Gpt-4 technical report. arXiv preprint
arXiv:2303.08774, 2023.
Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal
Piot, Remi Munos, Mark Rowland, Michal Valko, and
Daniele Calandriello. A general theoretical paradigm to
understand learning from human preferences. In Interna-
tional Conference on Artificial Intelligence and Statistics,
pages 4447–4455. PMLR, 2024.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell,
Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort,
Deep Ganguli, Tom Henighan, et al. Training a help-
ful and harmless assistant with reinforcement learning
from human feedback. arXiv preprint arXiv:2204.05862,
2022a.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda
Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna
Goldie, Azalia Mirhoseini, Cameron McKinnon, et al.
Constitutional ai: Harmlessness from ai feedback. arXiv
preprint arXiv:2212.08073, 2022b.
Stella Biderman, Hailey Schoelkopf, Quentin Gregory An-
thony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-
hammad Aflah Khan, Shivanshu Purohit, USVSN Sai
Prashanth, Edward Raff, et al. Pythia: A suite for ana-
lyzing large language models across training and scaling.
In International Conference on Machine Learning, pages
2397–2430. PMLR, 2023.
Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Kop-
pel, Furong Huang, Dinesh Manocha, Amrit Singh Bedi,
and Mengdi Wang.
Maxmin-rlhf: Towards equitable
alignment of large language models with diverse human
preferences. arXiv preprint arXiv:2402.08925, 2024.
Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola,
and Diyi Yang. Parameter-efficient fine-tuning design
spaces. arXiv preprint arXiv:2301.01821, 2023a.
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian
Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-
tuning of long-context large language models.
arXiv
preprint arXiv:2309.12307, 2023b.
Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and
Quanquan Gu. Self-play fine-tuning converts weak lan-
guage models to strong language models. arXiv preprint
arXiv:2401.01335, 2024.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,
Shane Legg, and Dario Amodei.
Deep reinforcement
learning from human preferences. Advances in neural
information processing systems, 30, 2017.
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei
Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong
Sun.
Ultrafeedback: Boosting language models with
high-quality feedback, 2023.
Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo
Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe
rlhf: Safe reinforcement learning from human feedback.
arXiv preprint arXiv:2310.12773, 2023.
Jacob Devlin.
Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805, 2018.
Meng Ding, Kaiyi Ji, Di Wang, and Jinhui Xu. Understand-
ing forgetting in continual learning with linear regression.
arXiv preprint arXiv:2405.17583, 2024.
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,
Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,
et al.
The llama 3 herd of models.
arXiv preprint
arXiv:2407.21783, 2024.
Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma,
Shaolei Zhang, and Yang Feng.
Llama-omni: Seam-
less speech interaction with large language models.
arXiv preprint arXiv:2409.06666, 2024.
URL https:
//huggingface.co/ICTNLP/Llama-3.1-8B-Omni.
Heshan Fernando, Han Shen, Miao Liu, Subhajit Chaud-
hury, Keerthiram Murugesan, and Tianyi Chen. Mitigat-
ing gradient bias in multi-objective learning: A provably
convergent approach. International Conference on Learn-
ing Representations, 2023.
Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda
Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan
Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red
teaming language models to reduce harms: Methods,
scaling behaviors, and lessons learned. arXiv preprint
arXiv:2209.07858, 2022.
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,
Sid Black, Anthony DiPofi, Charles Foster, Laurence
Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li,
Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Ja-
son Phang, Laria Reynolds, Hailey Schoelkopf, Aviya
Skowron, Lintang Sutawika, Eric Tang, Anish Thite,
Ben Wang, Kevin Wang, and Andy Zou.
The lan-
guage model evaluation harness, 07 2024. URL https:
//zenodo.org/records/12608602.
Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Zexu Sun,
Bowen Sun, Huimin Chen, Ruobing Xie, Jie Zhou, Yankai
Lin, et al. Controllable preference optimization: Toward
controllable multi-objective alignment. arXiv preprint
arXiv:2402.19085, 2024.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Mea-
suring massive multitask language understanding. arXiv
preprint arXiv:2009.03300, 2020.
Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Mono-
lithic preference optimization without reference model.
arXiv preprint arXiv:2403.07691, 2(4):5, 2024a.
Jiwoo Hong, Noah Lee, and James Thorne. Reference-free
monolithic preference optimization with odds ratio. arXiv
e-prints, pages arXiv–2403, 2024b.
Jeremy Howard and Sebastian Ruder. Universal language
model fine-tuning for text classification. arXiv preprint
arXiv:1801.06146, 2018.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen.
Lora: Low-rank adaptation of large language
models. arXiv preprint arXiv:2106.09685, 2021.


--- Page 10 ---
Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning
Jian Hu, Xibin Wu, Weixun Wang, Xianyu, Dehao Zhang,
and Yu Cao.
Openrlhf:
An easy-to-use, scalable
and high-performance rlhf framework. arXiv preprint
arXiv:2405.11143, 2024.
Ermo Hua, Biqing Qi, Kaiyan Zhang, Yue Yu, Ning Ding,
Xingtai Lv, Kai Tian, and Bowen Zhou. Intuitive fine-
tuning: Towards unifying sft and rlhf into a single process.
arXiv preprint arXiv:2405.11870, 2024.
Jiaming Ji, Donghai Hong, Borong Zhang, Boyuan Chen,
Josef Dai, Boren Zheng, Tianyi Qiu, Boxun Li, and
Yaodong Yang. Pku-saferlhf: Towards multi-level safety
alignment for llms with human preference. arXiv preprint
arXiv:2406.15513, 2024.
Feiyang Kang, Hoang Anh Just, Yifan Sun, Himanshu
Jahagirdar, Yuanzhi Zhang, Rongxing Du, Anit Kumar
Sahu, and Ruoxi Jia.
Get more for less: Principled
data selection for warming up fine-tuning in llms. arXiv
preprint arXiv:2405.02774, 2024.
Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joon-
suk Park, Kang Min Yoo, Se Jung Kwon, and Dongsoo
Lee. Memory-efficient fine-tuning of compressed large
language models via sub-4-bit integer quantization. Ad-
vances in Neural Information Processing Systems, 36,
2024.
Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis,
Jelena Luketina, Eric Hambro, Edward Grefenstette,
and Roberta Raileanu.
Understanding the effects of
rlhf on llm generalisation and diversity. arXiv preprint
arXiv:2310.06452, 2023.
Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wat-
tenberg, Jonathan K Kummerfeld, and Rada Mihal-
cea.
A mechanistic understanding of alignment algo-
rithms: A case study on dpo and toxicity. arXiv preprint
arXiv:2401.01967, 2024.
Chenliang Li,
Siliang Zeng,
Zeyi Liao,
Jiaxiang Li,
Dongyeop Kang, Alfredo Garcia, and Mingyi Hong. Joint
demonstration and preference learning improves pol-
icy alignment with human feedback.
arXiv preprint
arXiv:2406.06874, 2024.
Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos
Karampatziakis, Weizhu Chen, and Tuo Zhao. Loftq:
Lora-fine-tuning-aware quantization for large language
models. arXiv preprint arXiv:2310.08659, 2023.
Yong Lin, Lu Tan, Hangyu Lin, Zeming Zheng, Renjie Pi,
Jipeng Zhang, Shizhe Diao, Haoxiang Wang, Han Zhao,
Yuan Yao, et al. Speciality vs generality: An empirical
study on catastrophic forgetting in fine-tuning foundation
models. arXiv preprint arXiv:2309.06256, 2023.
Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang
Liu. Conflict-Averse Gradient Descent for Multi-task
Learning. December 2021.
Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Jun-
yang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou.
# instag: Instruction tagging for analyzing supervised
fine-tuning of large language models. In The Twelfth
International Conference on Learning Representations,
2023.
Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex
Damian, Jason D Lee, Danqi Chen, and Sanjeev Arora.
Fine-tuning language models with just forward passes.
Advances in Neural Information Processing Systems, 36:
53038–53075, 2023a.
Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen,
and Sanjeev Arora. A kernel-based view of language
model fine-tuning. In International Conference on Ma-
chine Learning, pages 23610–23641. PMLR, 2023b.
Kevis-Kokitsi Maninis, Ilija Radosavovic, and Iasonas
Kokkinos.
Attentive single-tasking of multiple tasks.
June 2019.
Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Sim-
ple preference optimization with a reference-free reward.
arXiv preprint arXiv:2405.14734, 2024.
Kaisa Miettinen.
Nonlinear multiobjective optimization,
volume 12. Springer Science & Business Media, 1999.
Rémi Munos, Michal Valko, Daniele Calandriello, Moham-
mad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel
Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard,
Andrea Michi, et al. Nash learning from human feedback.
arXiv preprint arXiv:2312.00886, 2023.
Aviv Navon, Aviv Shamsian, Idan Achituve, Haggai Maron,
Kenji Kawaguchi, Gal Chechik, and Ethan Fetaya.
Multi-Task Learning as a Bargaining Game.
arXiv
preprint:2202.01017, 2022.
Mahdi Nikdan, Soroush Tabesh, and Dan Alistarh. Rosa:
Accurate parameter-efficient fine-tuning via robust adap-
tation. arXiv preprint arXiv:2401.04679, 2024.
OpenAI. Chatgpt: Optimizing language models for dia-
logue, November 2022. URL https://openai.com/blog/
chatgpt/. Accessed: 2024-08-28.
Francesco
Orabona.
Last
iterate
of
sgd
con-
verges
even
in
unbounded
domains,
2020.
URL
https://parameterfree.com/2020/08/07/
last-iterate-of-sgd-converges-even-in-unbounded-domains/.
Accessed: 2024-09-10.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al. Training lan-
guage models to follow instructions with human feedback.
Advances in neural information processing systems, 35:
27730–27744, 2022.
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley,
and Jianfeng Gao. Instruction tuning with gpt-4. arXiv
preprint arXiv:2304.03277, 2023.
Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi
Jia, Prateek Mittal, and Peter Henderson. Fine-tuning
aligned language models compromises safety, even when
users do not intend to! arXiv preprint arXiv:2310.03693,
2023.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-
pher D Manning, Stefano Ermon, and Chelsea Finn.
Direct preference optimization: Your language model is
secretly a reward model. Advances in Neural Information
Processing Systems, 36, 2024.
Ralph Tyrell Rockafellar. Convex analysis, 1970.
Paul Röttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe
Attanasio, Federico Bianchi, and Dirk Hovy. Xstest: A
test suite for identifying exaggerated safety behaviours in
large language models. arXiv preprint arXiv:2308.01263,
2023.
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu
Liu, Romain Sauvestre, Tal Remez, et al. Code llama:
Open foundation models for code.
arXiv preprint
arXiv:2308.12950, 2023.


--- Page 11 ---
Fernando, Shen, Ram, Zhou, Samulowitz, Baracaldo, Chen
Han Shen, Zhuoran Yang, and Tianyi Chen. Principled
penalty-based methods for bilevel reinforcement learning
and rlhf. arXiv preprint arXiv:2402.06886, 2024.
Zhengxiang Shi and Aldo Lipani.
Dept:
Decomposed
prompt tuning for parameter-efficient fine-tuning. arXiv
preprint arXiv:2309.05173, 2023.
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang,
Zhenfang Chen, David Cox, Yiming Yang, and Chuang
Gan. Principle-driven self-alignment of language models
from scratch with minimal human supervision. Advances
in Neural Information Processing Systems, 36, 2024.
Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman
Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan.
Multilingual translation with extensible multilingual pre-
training and finetuning. arXiv preprint arXiv:2008.00401,
2020.
Junjiao Tian, Yen-Cheng Liu, James S Smith, and Zsolt
Kira. Fast trainable projection for robust fine-tuning.
Advances in Neural Information Processing Systems, 36,
2024.
Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D
Manning, and Chelsea Finn. Fine-tuning language models
for factuality. arXiv preprint arXiv:2311.08401, 2023.
Yuanhao Wang, Qinghua Liu, and Chi Jin. Is rlhf more
difficult than standard rl? a theoretical perspective. Ad-
vances in Neural Information Processing Systems, 36:
76006–76032, 2023.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai,
and Quoc V Le. Finetuned language models are zero-shot
learners. arXiv preprint arXiv:2109.01652, 2021.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-
mond, Clement Delangue, Anthony Moi, Pierric Cistac,
Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison,
Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jer-
nite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-
ger, Mariama Drame, Quentin Lhoest, and Alexander M.
Rush. Transformers: State-of-the-art natural language
processing. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing: Sys-
tem Demonstrations, pages 38–45, Online, October 2020.
Association for Computational Linguistics. URL https:
//www.aclweb.org/anthology/2020.emnlp-demos.6.
Tinghao Xie, Xiangyu Qi, Yi Zeng, Yangsibo Huang,
Udari Madhushani Sehwag, Kaixuan Huang, Luxi He,
Boyi Wei, Dacheng Li, Ying Sheng, et al. Sorry-bench:
Systematically evaluating large language model safety
refusal behaviors. arXiv preprint arXiv:2406.14598, 2024.
Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong,
Heng Ji, Nan Jiang, and Tong Zhang. Iterative prefer-
ence learning from human feedback: Bridging theory
and practice for rlhf under kl-constraint. In Forty-first
International Conference on Machine Learning, 2024.
Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu,
Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is
dpo superior to ppo for llm alignment? a comprehensive
study. arXiv preprint arXiv:2404.10719, 2024.
Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong,
Dong Yu, and Jianshu Chen. Rewards-in-context: Multi-
objective alignment of foundation models with dynamic
preference adjustment. arXiv preprint arXiv:2402.10207,
2024.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
and Yejin Choi. Hellaswag: Can a machine really finish
your sentence? arXiv preprint arXiv:1905.07830, 2019.
Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos
Karampatziakis, Pengcheng He, Yu Cheng, Weizhu
Chen, and Tuo Zhao. Adalora: Adaptive budget alloca-
tion for parameter-efficient fine-tuning. arXiv preprint
arXiv:2303.10512, 2023a.
Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,
Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian-
wei Zhang, Fei Wu, et al.
Instruction tuning for
large language models:
A survey.
arXiv preprint
arXiv:2308.10792, 2023b.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
Moya Chen, Shuohui Chen, Christopher Dewan, Mona
Diab,
Xian Li,
Xi Victoria Lin,
Todor Mihaylov,
Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig,
Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and
Luke Zettlemoyer. Opt: Open pre-trained transformer
language models, 2022.
Hao Zhao, Maksym Andriushchenko, Francesco Croce, and
Nicolas Flammarion. Long is more for alignment: A
simple but tough-to-beat baseline for instruction fine-
tuning. arXiv preprint arXiv:2402.04833, 2024.
Siyan Zhao, John Dang, and Aditya Grover. Group prefer-
ence optimization: Few-shot alignment of large language
models. arXiv preprint arXiv:2310.11523, 2023.
Han Zhong, Guhao Feng, Wei Xiong, Li Zhao, Di He, Jiang
Bian, and Liwei Wang. Dpo meets ppo: Reinforced token
optimization for rlhf. arXiv preprint arXiv:2404.18922,
2024.
Banghua Zhu, Michael Jordan, and Jiantao Jiao. Principled
reinforcement learning with human feedback from pair-
wise or k-wise comparisons. In International Conference
on Machine Learning, pages 43037–43067. PMLR, 2023a.
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. Minigpt-4: Enhancing vision-language
understanding with advanced large language models.
arXiv preprint arXiv:2304.10592, 2023b.


--- Page 12 ---
Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning
Supplementary Material for “Understanding Forgetting in LLM Supervised
Fine-Tuning and Preference Learning - A Convex Optimization Perspective"
A
Proofs for Theoretical Results
In this section, we provide the proofs for the main theoretical results of the paper, Theorems 3.3 and 4.1. The proof is
organized as follows: In Appendix A.1, we establish some fundamental claims regarding the problem setup, such as the
convexity of the objectives and the boundedness of the second moment of the stochastic gradients. Then, in Appendix A.2,
we prove the lower bound stated in Theorem 3.3 along with several supporting lemmas. Finally, in Appendix A.3, we
prove the upper bound stated in Theorem 4.1, accompanied by additional supporting lemmas.
For conciseness, in this section we omit the subscripts DPO and SFT of the input x, but whether x belongs to DDPO or DSFT
can be inferred from the context. Furthermore, for brevity, we denote the averaging over the datasets
1
N1
P
x,yw,yℓ∈DDPO
and
1
N2
P
x,y∈DSFT by Ex,yw,yℓ∼DDPO and Ex,y∼DSFT, respectively.
A.1
Basic claims on the problem setup
Before going to the proofs of the main results, we can have the following basic claims on the problem setup.
Proposition A.1 (Bounded second moment of stochastic gradient). For all θ ∈Θ, there exist M1, M2 > 0 such that
Ex,yw,yℓ∼DDPO[∥gDPO(θ; x, yw, yℓ)∥2] ≤M 2
1 ,
(15)
Ex,y∼DSFT[∥gSFT(θ; x, y)∥2] ≤M 2
2 .
(16)
Proof. Considering gDPO, we can first simplify hβ defined in (2) under softmax parameterization of πθ and πref as
hβ(θ; x, yw, yℓ) = β log
 πθ(yw | x)
πref(yw | x)

−β log
 πθ(yℓ| x)
πref(yℓ| x)

= β(θ −θref)⊤(ϕyw,x −ϕyℓ,x).
(17)
Then we can simplify gDPO as
gDPO(θ; x, yw, yℓ) = −(1 −σ(hβ(θ; x, yw, yℓ, πref)) ∇θhβ(θ; x, yw, yℓ, πref)
= −β(1 −σ

β(θ −θref)⊤(ϕyw,x −ϕyℓ,x)

(ϕyw,x −ϕyℓ,x).
(18)
We can then bound the norm of gDPO as
∥gDPO(θ; x, yw, yℓ)∥2 = β2(1 −σ

β(θ −θref)⊤(ϕyw,x −ϕyℓ,x)
2
∥ϕyw,x −ϕyℓ,x∥2
≤β2∥ϕyw,x −ϕyℓ,x∥2
≤4β2Φ2 =: M1,
(19)
where the first inequality is due to the fact that 0 ≤σ(z) ≤1 for all z ∈R, second inequality is due to Cauchy-Schwarz
inequality and Assumption 3.2. Taking expectation (average) over the dataset DDPO in (19) proves the first part of
Proposition A.1.
For proving the second part of the proposition, we start by simplifying the gradient of πθ under
softmax-parameteriation, given by
∇θπθ(y | x) = ∇θ
exp(θ⊤ϕy,x)
P
y′∈Y exp(θ⊤ϕy′,x)
=
exp(θ⊤ϕy,x) P
y′∈Y exp(θ⊤ϕy′,x)ϕy,x −exp(θ⊤ϕy,x) P
y′∈Y exp(θ⊤ϕy′,x)ϕy′,x
P
y′∈Y exp(θ⊤ϕy′,x)
2
=
 ϕy,x −¯ϕx(θ)

πθ(y | x),
(20)
where
¯ϕx(θ) :=
P
y′∈Y ϕy′,x exp(θ⊤ϕy′,x)
P
y′∈Y exp(θ⊤ϕy′,x)
.
(21)
Then we can simplify gSFT as
gSFT(θ; x, y) = −∇θπθ(y | x)
πθ(y | x)
= −(ϕy,x −¯ϕx(θ)).
(22)


--- Page 13 ---
Fernando, Shen, Ram, Zhou, Samulowitz, Baracaldo, Chen
We can then bound the norm of gSFT as
∥gSFT(θ; x, y)∥2 = ∥ϕy,x −¯ϕx(θ)∥2
≤4Φ2 =: M2,
(23)
where the inequality is due to the Cauchy-Schwarz inequality and Jensen’s inequality. Taking expectation (average) over
the dataset DSFT in (23) proves the second part of Proposition A.1.
Proposition A.2 (Convexity of objectives). Under Assumption 3.2, the objectives fDPO and fSFT (defined in ( 1) and
( 4), respectively) are convex.
Proof. The goal of the proof is to show the Hessians of the objectives fDPO and fSFT are semi-positive definite, under
Assumption 3.2 and LLMs (both trainable and reference) modeled using softmax parameterization. First, considering
fDPO, we can have
∇θgDPO(θ; x, yw, yℓ) = −∇θβ(1 −σ

β(θ −θref)⊤(ϕyw,x −ϕyℓ,x)

(ϕyw,x −ϕyℓ,x)
= β2σ (hβ(θ; x, yw, yℓ)) (ϕyw,x −ϕyℓ,x) (ϕyw,x −ϕyℓ,x)⊤⪰0,
(24)
where first equality is due to (18), and the semi-positive definiteness is due to the fact that β > 0, 0 ≤σ(z) ≤1 for all
z ∈R, and (ϕyw,x −ϕyℓ,x) (ϕyw,x −ϕyℓ,x)⊤⪰0 for all ϕyw,x, ϕyℓ,x. The convexity of fDPO follows from the fact that
∇2fDPO(θ) = Ex,yw,yℓ∼DDPO [∇θgDPO(θ; x, yw, yℓ)] .
(25)
Similarly, we can compute ∇θgSFT as
∇θgSFT(θ; x, y) = −∇θ(ϕy,x −¯ϕx(θ))
= ∇θ
P
y′∈Y ϕy′,x exp(θ⊤ϕy′,x)
P
y′∈Y exp(θ⊤ϕy′,x)
=
X
y′∈Y
ϕy′,xϕ⊤
y′,x exp(θ⊤ϕy′,x)
P
y′∈Y exp(θ⊤ϕy′,x)
−
 P
y′∈Y ϕy′,x exp(θ⊤ϕy′,x)
P
y′∈Y exp(θ⊤ϕy′,x)
!  P
y′∈Y ϕy′,x exp(θ⊤ϕy′,x)
P
y′∈Y exp(θ⊤ϕy′,x)
!⊤
,
(26)
where the first equality is due to (22). To establish the semi-positivedefiniteness of ∇θgSFT, cosider any v with same
dimension as θ, and let py,x =
exp(θ⊤ϕy,x)
P
y′∈Y exp(θ⊤ϕy′,x). Note that py,x ≥0 for all x, y, and P
y∈Y py,x = 1 for all x. Then we
can have
v⊤
 X
y′∈Y
ϕy′,xϕ⊤
y′,x exp(θ⊤ϕy′,x)
P
y′∈Y exp(θ⊤ϕy′,x)
−
 P
y′∈Y ϕy′,x exp(θ⊤ϕy′,x)
P
y′∈Y exp(θ⊤ϕy′,x)
!  P
y′∈Y ϕy′,x exp(θ⊤ϕy′,x)
P
y′∈Y exp(θ⊤ϕy′,x)
!⊤!
v
=
X
y′∈Y
(v⊤ϕy′,x)2py′,x −

X
y′∈Y
v⊤ϕy′,xpy′,x


2
≥0,
(27)
where the last inequality is due to Jensen’s inequality. This suggests that ∇θgSFT(θ; x, y) ⪰0, and the convexity of of
fSFT follows from the fact that
∇2fSFT(θ) = Ex,y∼DSFT [∇θgSFT(θ; x, y)] .
(28)
Note that when fDPO and fSFT are convex, fMix,λ is also convex for all λ ∈[0, 1].
A.2
Proof of Theorem 3.3
Proof. For deriving the lower bound given in 3.3, we consider the following problem setup that satisfies Assumption 3.2.
Let Θ be R, and let DDPO = {(x1, yw, yℓ)} and DSFT = {(x2, y)} and consider only two possible outputs exist (i.e. binary
classification) such that we have the specification in Table 1. Note that the data point y′ is not used in training explicitly,
and it is specified to enable the calculation of the output of πθ with softmax parameterization in the SFT optimization


--- Page 14 ---
Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning
20
15
10
5
0
5
10
15
20
0
2
4
6
8
10
12
14
fDPO( )
fSFT( )
0.5fDPO( ) + 0.5fSFT( )
fref( )
0
20000
40000
60000
80000
100000
T
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
f ( )
f *
Figure 6: An Illustration of the example used for lower bound derivation in Theorem 3.3
Table 1: Data set specification for lower bound analysis example
Input
Output
Feature ϕy,x
x1
yw = 1
−1.0
x1
yℓ= 0
−0.5
x2
y = 0
1.0
x2
y′ = 1
0.5
stage. Based on this dataset, we can also define the dataset for reference policy objective as Dref = {(x1, yw)}, which has a
similar optimization procedure as SFT. Before moving forward, we have to choose the reference policy πref, or equivalently
θref. The objective to determine θref is given by
θref ∈argmin
θ
fref(θ) := −log πθ(yw | x1),
(29)
which is graphically illustrated in Figure 6. We choose θref = −5 since this choice reasonably optimizes the objective.
With this problem setup and choice of θref, we can then derive the objectives and corresponding gradients using (1), (4),
(18), and (22) as
fDPO(θ) = log

1 + c 1 + exp(θ/2)
1 + exp(−θ/2)

(30)
fSFT(θ) = log(1 + exp(−θ/2))
(31)
gDPO(θ) = 1
2 ·
1
1 + exp(−(θ/2 + 5))
(32)
gSFT(θ) = −1
2 ·
1
1 + exp(θ/2),
(33)
where c =
1+exp(5)
1+exp(−5). Choosing λ = 0.5, we can numerically find an upper bound to f ∗
Mix,λ such that f ∗
Mix,λ ≤1
2 log c∗
where c∗= 173.78. With this, we can derive a lower bound to GMix,λ(θ) as
GMix,λ(θ) = fMix,λ(θ) −f ∗
Mix,λ
≥1
2 · log

1 + c 1 + exp(θ/2)
1 + exp(−θ/2)

+ 1
2 · log(1 + exp(−θ/2)) −1
2 log c∗
= 1
2 log
 1
c∗(1 + exp(−θ/2)) + c
c∗(1 + exp(θ/2))

.
(34)
When θ = 0, the right hand side of (34) approximately equals 0.256843. Since it is monotonically increasing for θ ∈[0, ∞),
we have GMix,λ(θ) ≳0.256843 = Ω(1) when θ ≥0. Thus to prove the result, it is sufficient to show Algorithm 1 will
generate a ˆθSeq that is greater than 0.
We have the first stage iterates:
θ1
t+1 = θ1
t −αt
1
1 + exp(−
θ1
t
2 −5)
,
for t = 1, ..., T −1.
(35)


--- Page 15 ---
Fernando, Shen, Ram, Zhou, Samulowitz, Baracaldo, Chen
Using the first stage’s last iterate as initial point, we have the second stage iterates:
θ2,T
t+1 = θ2,T
t
+ αt
1
1 + exp(
θ2,T
t
2 )
,
for t = 1, ..., T −1.
(36)
where θ2,T
1
= θ1
T and the superscript T in θ2,T
t
indicates the max iteration index in the first stage.
Without loss of generality, we initialize θ1
1 = 0. Then by (35) and (36), we have
ˆθSeq = θ2,T
T
= −αt
T −1
X
t=1
1
1 + exp(−
θ1
t +10
2
)
+ αt
T −1
X
t=1
1
1 + exp(
θ2,T
t
2 )
(37)
We first prove the following lemma.
Lemma A.3. Given a positive constant c and a sequence {αt}, consider the iterates generated by
θt+1 = θt + αt
1
1 + exp(cθt),
θ′
t+1 = θ′
t + αt
1
1 + exp(cθ′
t)
(38)
If θ1 −θ′
1 ≥0 and cαt
4
≤1 for any t, then we have
θt −θ′
t ≥(θ1 −θ′
1)Πt−1
i=1

1 −cαi
4

,
∀t.
Proof. We prove the result by induction. Assume θt −θ′
t ≥0 for some t. We first have
θt+1 −θ′
t+1 = θt −θ′
t + αt

1
1 + exp(cθt) −
1
1 + exp(cθ′
t)

(39)
With ∇θ
1
1+exp(cθ) = −cσ(−c¯θ)
 1 −σ(−cθ)
 where σ is the sigmoid function, using the mean value theorem in (39), we
have for some ¯θt in between θt and θ′
t that
θt+1 −θ′
t+1 = θt −θ′
t −cαtσ(−c¯θt)
 1 −σ(−c¯θt)

(θt −θ′
t)
≥θt −θ′
t −1
4cαt(θt −θ′
t)
=

1 −1
4cαt

(θt −θ′
t)
where the inequality follows from σ(−c¯θt)
 1 −σ(−c¯θt)

≤maxx∈[0,1] x(1 −x) = 1
4 and the assumption that θt −θ′
t ≥0.
Since θ1 −θ′
1 ≥0, it follows from induction that θt −θ′
t ≥0 for any t. Then recursively applying the last inequality
completes the proof.
Now we consider (37). Rewriting (35) gives that θ1
t is generated by
−(θ1
t+1 + 10) = −(θ1
t + 10) + αt
1
1 + exp(−
θ1
t +10
2
)
,
for t = 1, ..., T −1,
and θ2,T
t
is generated by (36). Thus θ2,T
t
and −(θ1
t + 10) are generated by (38) with c = 1/2. Assuming the step size
αt is proper that P∞
t=1 αt = ∞, then there always exists T ∗that for T ≥T ∗, we have θ1
T is small enough such that
−(θ1
1 + 10) −θ2,T
1
= −10 −θ1
T ≥0. If αt/8 ≤1, we have Lemma A.3 holds for sequences θ2,T
t
and −(θ1
t+1 + 10), and we
have
−(θ1
t + 10) ≥θ2,T
t
⇒
1
1 + exp(θ2,T
t
/2)
−
1
1 + exp(−(θ1
t + 10)/2) ≥0.
(40)
Using (40) in (37), we have ˆθSeq ≥0. This completes the proof.
Remark A.4. We provide some intuition on why we can obtain this suboptimality result for the sequential method, how
the proposed methods can overcome the suboptimality, and evidence for causes of suboptimality in real-world post-training
tasks:
Cause of suboptimality. The key idea behind the suboptimality result is constructing two conflicting objectives for DPO
and SFT. To this end, we exploit the fact that DPO and SFT datasets can be diverse, and they can have inconsistencies
in labelling. Specifically, in our lower bound analysis, we selected DPO and SFT inputs (by choosing the features ϕx,y)
and corresponding labels yw, yℓ, and y such that the DPO and SFT objectives induced by these datasets conflict with
each other (Table 1). Note that in this example both inputs and labels are different, thus there is no explicit labeling


--- Page 16 ---
Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning
0
20
40
60
80
100 most similar input pairs
CosSim(xDPO, xSFT)
CosSim(yw, y)
CosSim(y , y)
CosSim(yw, y )
Cosine Similarities of DPO and SFT datasets
0.00
0.25
0.50
0.75
1.00
Cosine Similarity
Figure 7: Visualization of data diversity and labeling inconsistencies in DPO (rm-hh-rlhf) and SFT (alpaca-
gpt4) datasets.
inconsistency. More severely conflicting DPO and SFT objectives can be constructed by to fixing the input (and hence the
corresponding feature ϕx,y) for both DPO and SFT datasets and assign conflicting labels to the chosen response for the
DPO input and the target response for the SFT input. However, we used a more general example in our lower bound
analysis, which shows that even when there is no explicit labeling inconsistency (e.g., when the input data for the two
datasets are different), the sequential approach can still fail.
How proposed methods can overcome suboptimality? The strength of the proposed alternating approaches over
the sequential approach is not its ability to explicitly correct any diversity in the datasets or inconsistency in labeling.
Rather, it lies in their ability to navigate the loss landscape of both fine-tuning and alignment objectives, reasonably
optimizing both. Given two (possibly conflicting) datasets and a pretrained LLM, our proposed methods can achieve a
model that is most reasonably optimal for both SFT and alignment datasets, with a controllable trade-off between them.
Data diversity and labeling inconsistency in real-world data. We investigated the data diversity and labeling
inconsistencies in our real-world experiments (e.g., Figures 3 and 8) using the datasets alpaca-gpt4 for SFT and
rm-hh-rlhf for DPO. Specifically, we visualized:
• cosine similarities between the closest input pairs (x(i)
DPO, x(i)
SFT) from the two datasets;
• cosine similarities between chosen/rejected responses for the DPO input and the target response for the corresponding
SFT input ((y(i)
w , y(i))/(y(i)
ℓ, y(i))); and,
• cosine similarities between chosen/rejected responses of the DPO input (y(i)
w , y(i)
ℓ). Here, the superscript (i) denotes the
ranking of the DPO-SFT input pair based on their similarity, which was measured at the token level.
The visualization is given in Figure 7. It can be observed that most DPO and SFT input pairs do not exhibit very strong
cosine similarity (falling in the range of approximately 0.57–0.78). Observing the corresponding inputs, the similarity
seems to come from mainly the sentence structure, rather than the underlying meaning. Thus, there is some inherent
dissimilarity in the inputs themselves. Furthermore, for some similar input pairs (x(i)
DPO, x(i)
SFT), the corresponding (y(i)
w , y(i))
similarity is close to zero, which might indicate a conflict in labeling. This suggests that there is some inconsistency in
labeling in the real-world setup, although it is not as conclusive or adversarial as in the toy example used for the lower
bound analysis. While here we use a token level comparison using cosine similarity, A deeper investigation into these
data-level inconsistencies would be valuable, particularly in addressing them during data preprocessing or selection to
enhance the optimization process.
A.3
Proof of Theorem 4.1
In this section, we provide the proof for Theorem 4.1. First, we provide some useful results that will later be used in the
main proof. For conciseness, we denote gDPO(θt; xt, yt
w, yt
ℓ) and gSFT(θt; xt, yt) by gDPO,t and gSFT,t, respectively.
Lemma A.5 ((Rockafellar, 1970) Theorem 25.1 and Corollary 25.11). Consider any convex differentiable function f and
let θ ∈Θ. Then, for any θ′ ∈Θ, we have
f(θ′) ≥f(θ) + ∇f(θ)⊤(θ′ −θ).
(41)
Lemma A.6 ((Orabona, 2020) Lemma 1.). Let {ηt}T
t=1 be a non-increasing sequence of positive numbers and qt ≥0 for
all t = 1, . . . , T. Then
nT qT ≤1
T
T
X
t=1
ηtqt +
T −1
X
k=1
1
k(k + 1)
T
X
t=T −k+1
ηt(qt −qT −k)
(42)
Lemma A.7. Consider iterates θt and θt+1 generated by Algorithm 2 with ALRIGHT strategy for t ∈{1, . . . , T −1}.
Then for any θ′ ∈Θ and λ ∈[0, 1], we have
E

fMix,λ(θt) −fMix,λ(θ′)

≤
1
2αt E

∥θt −θ′∥2
−
1
2αt E

∥θt+1 −θ′∥2
+ αt
2 M 2
λ,
(43)
where Mλ = λM1 + (1 −λ)M2, and M1, M2 are as defined in Proposition A.1.


--- Page 17 ---
Fernando, Shen, Ram, Zhou, Samulowitz, Baracaldo, Chen
Proof. Considering Algorothm 2 with ALRIGHT strategy, for any θ′ ∈Θ, we can have
∥θt+1 −θ′∥2 −∥θt −θ′∥2 ≤∥θt −αt(Iit=1gDPO,t + Iit=0gSFT,t) −θ′∥2 −∥θt −θ′∥2
= −2αt(Iit=1gDPO,t + Iit=0gSFT,t)⊤(θt −θ′)
+ α2
t∥Iit=1gDPO,t + Iit=0gSFT,t∥2
= −2αt(Iit=1gDPO,t + Iit=0gSFT,t)⊤(θt −θ′)
+ α2
t
 Iit=1∥gDPO,t∥2 + Iit=0∥gSFT,t∥2
(44)
Taking conditional expectation E[· | θt] (over randomness of datapoints and it) in both sides of above inequality, we obtain
E

∥θt+1 −θ′∥2 | θt

−∥θt −θ′∥2 ≤−2αt(λ∇fDPO(θt) + (1 −λ)∇fSFT(θ))⊤(θt −θ′) + α2
tM 2
λ
= −2αt∇fMix,λ(θt)⊤(θt −θ′) + α2
tM 2
λ
≤−2αt
 fMix,λ(θt) −fMix,λ(θ′)

+ α2
tM 2
λ,
(45)
where the first inequality is using the definitions of gDPO,t,gSFT,t, and Mλ, equality is by the defintion of fMix,λ, and the
last inequality is due to Lemma A.5. The result follows from taking total expectation in both sides and rearranging the
inequality (45).
With the above results, we are ready to prove Theorem 4.1.
Theorem A.8 (Theorem 4.1 Restated with Additional Details). Consider Algorithm 2 with ALRIGHT strategy and
αt = α0/
√
T for all t ∈{1, . . . , T} and α0 > 0. Then, under Assumption 3.2, for any λ ∈[0, 1], we have
E[GMix,λ(θT )] ≤
α0
2
√
T
∥θ1 −θ∗
Mix,λ∥2 + (2 + log(T −1))M 2
λα0
2
√
T
(46)
where θ∗
Mix,λ ∈argminθ∈Θ fMix,λ(θ), and Mλ = λM1 + (1 −λ)M2 with M1, M2 as defined in Assumption A.1.
Proof. Substituting ηt = α and qt = GMix,λ(θt) in (42) (Lemma A.6), we have
GMix,λ(θT ) ≤1
T
T
X
t=1
GMix,λ(θt) +
T −1
X
k=1
1
k(k + 1)
T
X
t=T −k+1
(GMix,λ(θt) −GMix,λ(θT −k))
= 1
T
T
X
t=1
GMix,λ(θt) +
T −1
X
k=1
1
k(k + 1)
T
X
t=T −k+1
(fMix,λ(θt) −fMix,λ(θT −k)) ,
(47)
where we have used the definition of GMix,λ in the equality. Taking total expectation on both sides of (47), we get
E[GMix,λ(θT )]≤1
T
T
X
t=1
E[GMix,λ(θt)]+
T −1
X
k=1
1
k(k + 1)
T
X
t=T −k+1
E [fMix,λ(θt)−fMix,λ(θT −k)]
(48)
The first term in the right hand side of (48) can be bounded by choosing θ′ = θ∗
Mix,λ and αt = α in (43) from Lemma A.7
and then taking a telescoping sum:
T
X
t=1
E[GMix,λ(θt)] ≤1
2α∥θ1 −θ∗
Mix,λ∥2 −1
2α∥θT +1 −θ∗
Mix,λ∥2 + T M 2
λα
2
≤1
2α∥θ1 −θ∗
Mix,λ∥2 + T M 2
λα
2
(49)
Now we consider the second term in the right hand side of (48). We first have
T
X
t=T −k+1
E [fMix,λ(θt) −fMix,λ(θT −k)] =
T
X
t=T −k
E [fMix,λ(θt) −fMix,λ(θT −k)]
≤(k + 1)M 2
λα
2
(50)


--- Page 18 ---
Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning
where the inequality follows from setting θ′ = θT −k, αt = α in (43) from Lemma A.7 and then taking a telescoping sum
from t = T −k to T. Substituting the last inequality to the second term in the right hand side of (48) yields
T −1
X
k=1
1
k(k + 1)
T
X
t=T −k+1
E [fMix,λ(θt)−fMix,λ(θT −k)] ≤
T −1
X
k=1
1
k
M 2
λα
2
≤(1 + log(T −1))M 2
λα
2
.
(51)
Choosing α =
α0
√
T , and substituting (49) and (51) in (48) yields
E[GMix,λ(θT )] ≤
α0
2
√
T
∥θ1 −θ∗
Mix,λ∥2 + (2 + log(T −1))M 2
λα0
2
√
T
which completes the proof.
B
Experiment Details
In this section, we provide experiment details for the experiments in Sections 6 and C. We build upon OpenRLHF
framework (Hu et al., 2024), ORPO codebase (Hong et al., 2024a), and Transformers framework (Wolf et al., 2020) to
implement the experiments in Section 6, C.1, and C.2. All experiments were done using 2 NVIDIA RTX A5000 GPUs and
4 NVIDIA RTX A6000 GPUs.
B.1
Experiments details for toy illustration in Figure 2
In this section we provide details for the experiment results given in Figure 2. We consider Θ be R2,DDPO = {(x1, yw, yℓ)}
and DSFT = {(x2, y)} and setting where only two possible outputs exist (i.e. binary classification) such that we have the
specification in Table 1. Note that the data point y′ is not used in training explicitly, and it is specified to enable the
calculation of the output of πθ with softmax parameterization in the SFT optimization stage. Based on this dataset, we
can also define the dataset for reference policy objective as Dref = {(x1, yw)}, which has a similar optimization procedure
as SFT.
To obtain θref, we train a parameter initialized at [5.0; −9.9]⊤for 1000 epochs with a learning rate of 0.01. This parameter
initialization and learning rate are also used to train the model θ using the Sequential, ALRIGHT, and MAXRIGHT
methods. Furthermore, for illustration purposes, we use a weight decay of 0.001 in optimization. The resulting πref is then
used as the reference policy for the DPO objective in all three methods.
For the sequential method, we train θ for 10,000 epochs per objective (DPO first, then SFT). A threshold of 0.05 for the
objective value is applied to stop training for a given objective, preventing excessive overfitting to that objective.
For the ALRIGHT and MAXRIGHT methods, we train θ for 20,000 epochs, while keeping other training configurations
identical to the Sequential method.
B.2
Additional details for experiments with pythia-1b
We conducted three sets of experiments using the pythia-1b model:
(1) Comparison of baselines and proposed methods for sequential training with DPO first, followed by SFT (Figure 3),
(2) Comparison of baselines and proposed methods for sequential training with SFT first, followed by DPO (Figure 8),
and
(3) Ablation study on the choice of maximum evaluation steps for memory-efficient MAXRIGHT (Figure 14).
Table 2: Data set specification for toy illustration in Figure 2
Input
Output
Feature ϕy,x
x1
yw = 1
[1.0; 1.0]⊤
x1
yℓ= 0
[0.5; 0.5]⊤
x2
y = 0
[1.0; 0.5]⊤
x2
y′ = 1
[0.5; 0.5]⊤


--- Page 19 ---
Fernando, Shen, Ram, Zhou, Samulowitz, Baracaldo, Chen
The primary difference between the first two experiments is the πref used (and thus the DPO objective), as described
in Section 2. For training the models (both θ and θref) in experiments (1), (2), and (3), we use LoRA with rank 32 and
α = 32. The query_key_value layers are the target modules to which LoRA is applied. No gradient checkpointing is
used for pythia-1b training. The learning rate is set to 5 × 10−5 for all model training with pythia-1b.
To obtain θref for experiments (1) and (3), we train the model for 6 epochs using 24, 000 input-response pairs from the
rm-hh-rlhf dataset, with a batch size of 12 and a learning rate of 5 × 10−5. For experiment (2), we train the model for 6
epochs using 24, 000 samples from the alpaca-gpt4 dataset, with a batch size of 24.
To compute f ∗
DPO and f ∗
SFT, which are required for calculating the optimality gap, ideal distance metrics, and implementing
the MAXRIGHT method, we perform independent optimization of the DPO and SFT objectives for 6 epochs. For the
SFT objective, we use 24, 000 samples from the alpaca-gpt4 dataset with a batch size of 24, and for the DPO objective,
we use 8, 000 samples from the rm-hh-rlhf dataset with a batch size of 8. Additionally, we run ALRIGHT for 6 epochs to
establish a reference Pareto front, which, along with the optimal objective values, is used as a stopping criterion for joint
optimization. No stopping criterion is applied for the sequential method.
Finally, all methods are trained for 6 epochs, using the corresponding λ for joint optimization methods or a combination of
TDPO and TSFT for the sequential method, until the stopping criterion is reached. For the memory-efficient MAXRIGHT
implementation in experiments (1) and (2), the maximum evaluation step is set to 10.
B.3
Additional details for experiments with Llama3-8b
We conducted three sets of experiments using the Llama3-8b model:
(1) Comparison of baselines and proposed framework for sequential training with safety focused DPO first, followed by
SFT (Figures 4, 9, 10, 11, and Table 3).
(2) Comparison of baselines and proposed framework for sequential training with general preference focused DPO first,
followed by SFT (Figure 12),
(3) Comparison of baselines and proposed framework for sequential training with SFT first, followed by general preference
focused DPO (Figure 13, Section D.1).
For experiment (1), we train the model on a safety focused DPO dataset created using a filtered version of PKU-SafeRLHF
and SFT dataset alpaca-gpt4. We filter the PKU-SafeRLHF in the following way: First, we select the datapoints with
response pairs which have opposite safety meta labels. Then, we use the SORRY-Bench benchmark’s LLM evaluator to
asses whether the safety meta labels are indeed accurate, and filter out the data that does not meet the safety standards of
SORRY-Bench benchmark’s LLM evaluator 1. This results in a final filtered dataset of 5, 269 datapoints. We use a subset
of 5, 200 datapoints for DPO and 20, 080 for SFT in this setting. We use a batchsize of 4 for DPO and 16 for SFT in this
setting. For training the models (both θ and θref), we use LoRA with rank 16 and α = 16. The q_proj and v_proj
layers are the target modules for LoRA application. Gradient checkpointing is enabled during training, and the learning
rate is set to 5 × 10−5 for all model training. To obtain θref, we train the model for 4 epochs using 5, 269 samples of input
and chosen response pairs from the filtered PKU-SafeRLHF dataset, with a batch size of 8. In addition to individual
benchmark performances, in Table 3 we also use the per-task performance drop of a metric Sm for method A with respect
to baseline B as a measure of the overall performance of a given method, which is given by (Maninis et al., 2019; Liu et al.,
2021; Navon et al., 2022; Fernando et al., 2023)
∆m%= 1
M
M
X
m=1
(−1)ℓm (SA,m −SB,m)/SB,m × 100%,
(52)
where M is the number of tasks, SB,m and SA,m are the values of metric Sm obtained by the baseline and the compared
method respectively. Here, ℓm =0 if higher values for Sm are better and 1 otherwise. For each preference setting, we use
the sequential method as the baseline.
For experiments (2) and (3), we train the model on a general human preference alignment focused DPO dataset rm-hh-rlhf
and SFT dataset alpaca-gpt4. We use a subset of 6, 000 datapoints for DPO and 24, 000 for SFT in this setting. We use
a batchsize of 4 for DPO and 16 for SFT in this setting. For training the models (both θ and θref), we use LoRA with rank
16 and α = 16. The q_proj and v_proj layers are the target modules for LoRA application. Gradient checkpointing is
enabled during training, and the learning rate is set to 5 × 10−5 for all model training. To obtain θref in experiment (2),
we train the model for 4 epochs using 24, 000 samples of input and chosen response pairs from the rm-hh-rlhf dataset,
with a batch size of 8. To obtain θref in experiment (2), we train the model for 4 epochs using 24, 000 samples of input and
response pairs from alpaca-gpt4 dataset, with a batch size of 8.
In all experiment settings (1), (2), and (3), to compute f ∗
DPO and f ∗
SFT required for calculating the optimality gap, ideal
distance metrics, and implementing MAXRIGHT, we independently optimize the DPO and SFT objectives for 4 epochs.
1https://huggingface.co/sorry-bench/ft-mistral-7b-instruct-v0.2-sorry-bench-202406


--- Page 20 ---
Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning
1.6
1.8
2.0
2.2
2.4
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
fDPO( )
Seq. (TSFT, TDPO)
(5, 1)
(4, 2)
(3, 3)
(2, 4)
(1, 5)
ALRIGHT 
0.01
0.25
0.5
0.75
0.99
Model Avg. 
0.01
0.25
0.5
0.75
0.99
Model Avg. 
0.01
0.25
0.5
0.75
0.99
1.6
1.8
2.0
2.2
2.4
fSFT( )
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
fDPO( )
MAXRIGHT 
0.01
0.25
0.5
0.75
0.99
Mix 
0.01
0.25
0.5
0.75
0.99
MAXRIGHT 
0.01
0.25
0.5
0.75
0.99
0.0
0.2
0.4
0.6
DPO Optimality Gap
Sequential
Mix
ALRIGHT
MAXRIGHT
0.0
0.2
0.4
SFT Optimality Gap
0.0
0.2
0.4
0.6
Ideal Distance
0
25
50
75
Increase in Runtime (%)
(TSFT,TDPO) = (5, 1)
= 0.01
(4, 2)
0.25
(3, 3)
0.5
(2, 4)
0.75
(1, 5)
0.99
0
20
40
Increase in GPU Utilization (%)
Figure 8: Comparison of proposed methods with baselines (SFT first then DPO for Sequential) using pythia-1b.
Left: Training trajectories for various methods in the objective space, visualizing their convergence with respect
to the DPO and SFT objectives. Right: Performance comparison across multiple evaluation metrics, including
optimality gap for DPO and SFT objectives, ideal distance, runtime, and GPU utilization. The bar charts
highlight the trade-offs and resource efficiency of each method for different choices of (TSFT, TDPO) or λ.
Additionally, we run ALRIGHT for 4 epochs to establish a reference Pareto front, which, along with the optimal objective
values, serves as a stopping criterion for joint optimization. No stopping criterion is applied for the sequential method.
In all experiment settings (1), (2), and (3), all methods are trained for 4 epochs, using the corresponding λ for joint
optimization methods or a combination of TDPO and TSFT for the sequential method, until the stopping criterion is reached.
For memory-efficient MAXRIGHT implementation, the maximum evaluation step is set to 10.
B.4
Additional details for experiments with opt-1.3b
We conducted experiments using SFT first, then DPO setting, using the same preference dataset Hugging-
FaceH4/ultrafeedback_binarized for both SFT and DPO stages (Figure 5).
Following the filtration method
used by (Hong et al., 2024a), we filter the HuggingFaceH4/ultrafeedback_binarized to obtain datapoints that has
no chosen/rejected response or instances where chosen response is the same as the rejected response. This results in a
final filtered dataset of 36, 817 datapoints. For training the models (both θ and θref), we use full parameter fine-tuning.
Gradient checkpointing is enabled during training, and the learning rate is set to 2 × 10−5 for all model training. To
obtain θref, we train the model for 1 epoch using 36, 817 samples of input and chosen response pairs from the filtered
HuggingFaceH4/ultrafeedback_binarized dataset, with a batch size of 8. The model for Sequential is obtained by
first training using SFT with the chosen response for 1 epoch, and then training using DPO for 1 epoch starting from the
model from SFT stage. Thus, Sequential is effectively trained for 2 epochs. Since ORPO does not need a reference model
training phase, it is trained for 2 epochs. All the joint training methods are trained only for 1 epoch for fairness, since they
require a reference model that is already trained for 1 epoch. For comparing the different method performance, we use the
win rate against the chosen response of 50 datapoints from the HuggingFaceH4/ultrafeedback_binarized test set.
The win rate is judged by UltraRM-13b reward model 2. A ‘win’ for a given model reponse is computed as follows:
Given an input and a response from a model and the chosen response, the reward model will compute the reward for the
2https://huggingface.co/openbmb/UltraRM-13b


--- Page 21 ---
Fernando, Shen, Ram, Zhou, Samulowitz, Baracaldo, Chen
Table 3: Comparison of MMLU (1-shot), SORRY-Bench, and XSTest benchmark performance for different
methods using Llama3-8b. I, II, and III corresponds to 0.25/(1, 3), 0.5/(2, 2), and 0.75/(3, 1) trade-off settings
in the form λ/(TSFT, TDPO), respectively. Sequential, ALRIGHT, and MAXRIGHT are denoted by “Seq.”, “AL.”,
and “MAX.”, respectively.
MMLU ↑
HellaSwag ↑
SORRY-Bench ↓
XSTest ↑
∆m ↑
I
II
III
I
II
III
I
II
III
I
II
III
I
II
III
Seq.
0.686 0.584 0.668 0.601 0.602 0.612 0.742 0.758 0.778 0.900 0.912 0.884
-
-
-
Mix
0.725 0.725 0.747 0.616 0.619 0.616 0.269 0.279 0.331 0.884 0.852 0.828 0.175 0.209 0.159
AL.
0.695 0.720 0.744 0.619 0.617 0.618 0.207 0.191 0.331 0.884 0.832 0.816 0.187 0.229 0.155
MAX. 0.765 0.748 0.752 0.617 0.616 0.617 0.180 0.289 0.340 0.840 0.820 0.808 0.208 0.205 0.153
chosen response and a given model output. The reward model will declare a ‘win’ for the model if the model response has
a higher reward compared to the chosen response. The win rate is then computed by computing the percentages of win for
a given model out of all the datapoints used for evaluation.
B.5
Evaluation metrics used for measuring resource usage
In this section, we give the formula for computing the resource usage metrics used in Section 6; percentage increase in
runtime and percentage increase in GPU utilization.
Consider the method under evaluation A, and the baseline method B. Then, percentage increase in runtime is given by
percentage increase in runtime for A = runtime of A −runtime of B
runtime of B
× 100%.
Similarly, we define the runtime efficiency as
runtime efficiency of A =
runtime of A
runtime of B
−1
,
where the inversion is to make the metric higher, the better. In our experiments, we use different variants of Sequential
method as B, for corresponding joint training method A. For example, in pythia-1b experiments we use Sequential with
(TDPO, TSFT) = (5, 1) configuration as the baseline for Mix, ALRIGHT, and MAXRIGHT with λ = 0.99. We can similarly
define the percentage increase of GPU utilization and GPU utilization efficiency as
percentage increase in GPU utilization for A = GPU utilization of A −GPU utilization of B
GPU utilization of B
× 100%,
GPU utilization efficiency of A =
GPU utilization of A
GPU utilization of B
−1
.
Here, the GPU utilization is computed as the median GPU utilization throughout the runtime of a given method.
C
Additional Experiment Results
In this section, we provide additional experiment results using pythia-1b and Llama3-8b models.
C.1
Additional experiments with pythia-1b
ALRIGHT and MAXRIGHT significantly outperform Sequential. In Figure 8 (left), it can be seen that the final
models obtained by ALRIGHT and MAXRIGHT achieve better trade-off in DPO and SFT objective values in general
compared to Sequential. Furthermore, ALRIGHT and MAXRIGHT perform comparably or significantly better in terms of
SFT optimality gap and ideal distance metrics (Figure 8 (right)), while Sequential demonstrates a better performance in
RLHF optimality gap. This is because, in this experiment setup, Sequential is implemented by optimizing for SFT first
then DPO.
ALRIGHT and MAXRIGHT require resources compared to Sequential and significantly lower than Mix.
In Figure 8 (right), the additional computational resources required by different implementations of ALRIGHT and
MAXRIGHT are minimal (or even negative) relative to their Sequential counterparts. In contrast, Mix incurs substantial
additional resource usage, with increases of up to 53% in runtime and up to 37% in GPU utilization, despite achieving
comparable performance metrics to ALRIGHT and MAXRIGHT.


--- Page 22 ---
Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning
2
4
6
8
0.0
0.2
0.4
0.6
0.8
1.0
1.2
fDPO( )
Seq. (TDPO, TSFT)
(1, 2)
(2, 1)
(3, 0)
ALRIGHT 
0.25
0.5
0.75
ALRIGHT 
0.25
0.5
0.75
2
4
6
8
fSFT( )
0.0
0.2
0.4
0.6
0.8
1.0
1.2
fDPO( )
MAXRIGHT 
0.25
0.5
0.75
Mix 
0.25
0.5
0.75
MAXRIGHT 
0.25
0.5
0.75
0.00
0.25
0.50
0.75
DPO Optimality Gap
Sequential
Mix
ALRIGHT
MAXRIGHT
0.000
0.025
0.050
0.075
SFT Optimality Gap
0.0
0.2
0.4
0.6
Ideal Distance
50
0
Increase in Runtime (%)
(TDPO,TSFT) = (1, 3)
= 0.25
(2, 2)
0.5
(3, 1)
0.75
5
0
5
Increase in GPU Utilization (%)
Figure 9: Comparison with baselines (safety-focused DPO first then SFT for Sequential) using Llama3-8b. Left:
Training trajectories for various methods in the objective space, visualizing their convergence with respect to
the DPO and SFT objectives. Right: Performance comparison across multiple evaluation metrics, including
optimality gap for DPO and SFT objectives, ideal distance, runtime, and GPU utilization. The bar charts
highlight the trade-offs and resource efficiency of each method for different choices of (TSFT, TDPO) or λ.
C.2
Additional experiments with Llama3-8b
ALRIGHT and MAXRIGHT significantly outperform Sequential SFT-DPO on real-world benchmarks
with minimal additional resources.In addition to the analysis on the performance in terms of objectives, we also
use several benchmarks to evaluate the model’s performance on real-world tasks. For this purpose, we use MMLU as a
benchmark to assess the general language understanding of the model, HellaSwag to evaluate the common sense inference
capabilities of the model, SORRY-Bench to asses the ability of the model to refuse responding to harmful responses
(compliance rate for harmful prompts, lower the better), and XSTest to assess the exaggerated safety refusals of the
model (compliance rate for marginally safe prompts, lower the better). In addition to aforementioned benchmarks, we also
use ∆m% (defined in (52)), which denotes the average percentage increase in performance across all three benchmarks,
with respect to Sequential. The results are given in Table 3. It can be seen that all joint-training methods significantly
outperform Sequential method in terms of MMLU and SORRY-Bench benchmarks, and slightly outperform Sequential
method in HellaSwag benchmark. Sequential performs better in XSTest benchmark, simply because of the lack of safety
awareness in Sequential trained model resulting in a larger compliance rate to any prompt, irrespective of the nature of the
prompt. This bias results in a poor trade-off between tasks, which is apparent in the ∆m% measure. It can be seen that
all joint training methods significantly outperform Sequential method (up to 23% overall performance improvement across
all benchmarks by ALRIGHT) in terms of ∆m% (overall performance increment). Detailed illustrations of SORRY-Bench
and XSTest benchmark results are given in Figure 10 and 11, respectively. The corresponding optimization trajectories,
performance and computational resource usage metrics are illustrated in Figure 9.
ALRIGHT and MAXRIGHT perform comparably or better than Sequential DPO and SFT. Figure 12
provides results for the experiment setting (2) described in Section B.3. In Figure 12 (left), it can be seen that the final
models obtained by ALRIGHT and MAXRIGHT achieve better or comparable trade-off in DPO and SFT objective values
in general compared to Sequential. Furthermore, MAXRIGHT performs consistently better in terms of ideal distance
metric (Figure 13 (right)), which is consistent with pythia-1b experiments. Similar observations can be made for the
experiment results for the experiment setting (3) (described in Section B.3) shown in Figure 13.


--- Page 23 ---
Fernando, Shen, Ram, Zhou, Samulowitz, Baracaldo, Chen
Figure 10: Visualization of SORRY-Bench refusal rates for models trained by different methods and preference
settings. Prompt types are categorized by 45 unsafe topics, the average compliance rates (lower the better) over
all the categories are given within parenthesis in each row.
Figure 11: Visualization of XSTest performance for different methods and preference settings. Colored bars
denote full compliance , partial compliance , and full refusal rates for safe prompts that the model should
comply. Higher the full compliance rate, the better.
ALRIGHT and MAXRIGHT require resources compared to Sequential and significantly lower than Mix.
In Figure 12 (right), the additional computational resources required by different implementations of ALRIGHT and
MAXRIGHT are minimal (or even negative) relative to their Sequential counterparts. In contrast, Mix incurs substantial
additional resource usage, with increases of up to 60% in runtime and up to 7% in GPU utilization, despite achieving
comparable performance metrics to ALRIGHT and MAXRIGHT. Note that, unlike in pythia-1b experiments, the increase
in GPU utilization for Mix is lower. We believe this is because we implement gradient checkpointing when training
Llama3-8b, and gradient checkpointing improves GPU utilization at the cost of increased runtime due to duplicate
activation computations in the backward pass. Furthermore, similar observations can be made for the SFT first then DPO
setting experiment results given in Figure 13.
D
Memory Efficient MAXRIGHT Implementation.
In this section, we summarize the memory-efficient implementation of MAXRIGHT described in Section 4.3. Even though
MAXRIGHT allows one to compute the index needed for selecting the objective with a maximum (weighted) optimality
gap, in practice, evaluating both objectives can be memory intensive, and only one objective is updated at a given iteration.
To alleviate this issue, we propose to do simultaneous evaluations only every k steps. We call a time step that simultaneous
evaluation is done as a ‘max evaluation step’. At such time step t = t0, we compute it0, and update the corresponding
objective as in Algorithm 2 with MAXRIGHT strategy. After the update, we store the computed (weighted) optimality
gap as ¯f stale,t0
1,λ
= ¯f1,λ(θt0) and ¯f stale,t0
2,λ
= ¯f2,λ(θt0). Then, for every iteration before the next max evaluation step t0 + k,


--- Page 24 ---
Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning
1.0
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
0.2
0.4
0.6
0.8
1.0
1.2
fDPO( )
Seq. (TDPO, TSFT)
(1, 2)
(2, 1)
(3, 0)
ALRIGHT 
0.25
0.5
0.75
ALRIGHT 
0.25
0.5
0.75
1.0
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
fSFT( )
0.2
0.4
0.6
0.8
1.0
1.2
fDPO( )
MAXRIGHT 
0.25
0.5
0.75
Mix 
0.25
0.5
0.75
MAXRIGHT 
0.25
0.5
0.75
0.0
0.5
1.0
DPO Optimality Gap
0.00
0.05
0.10
SFT Optimality Gap
Sequential
Mix
ALRIGHT
MAXRIGHT
0.0
0.5
1.0
Ideal Distance
0
20
40
60
Increase in Runtime (%)
(TDPO,TSFT) = (1, 3)
= 0.25
(2, 2)
0.5
(3, 1)
0.75
0
5
Increase in GPU Utilization (%)
Figure 12: Comparison of proposed methods with baselines (DPO first then SFT for Sequential) using Llama3-8b.
Left: Training trajectories for various methods in the objective space, visualizing their convergence with respect
to the DPO and SFT objectives. Right: Performance comparison across multiple evaluation metrics, including
optimality gap for DPO and SFT objectives, ideal distance, runtime, and GPU utilization. The bar charts
highlight the trade-offs and resource efficiency of each method for different choices of (TDPO, TSFT) or λ.
we choose the index of the objective to be optimized as
it0+k′ = argmax
i
¯f stale,t0
i,λ
,
(53)
where k′ < k. Once the index is computed, we update the corresponding objective following (9) or (8), and update the
stale (weighted) optimality gap as
¯f stale,t0
i,λ
= ¯fi,λ(θt0+k′),
if it0+k′ = i
(54)
where i ∈{1, 2}. This process is summarized in Algorithm 3. With this modification, we can match the evaluation
and gradient computation complexity of Algorithm 2 with the ALRIGHT strategy in most iterations, at the expense of
degraded accuracy in choosing it.
Ablation of maximum evaluation steps. Figure 14 illustrates the influence of maximum evaluation step choices in
memory-efficient MAXRIGHT on optimization trajectories and resource usage. For low values (e.g., 1), the algorithm
closely follows the trade-off determined by λ, keeping the solutions concentrated near the ideal point (e.g., compared to
ALRIGHT), but incurs high runtime due to frequent evaluations. In contrast, high values (e.g., 1000) cause significant
oscillations in the objective space, failing to maintain the desired trade-off and resulting in increased GPU utilization from
excessive SFT updates. The aforementioned oscillation leads to poor ideal distance performance as the model drifts away
from the ideal point.


--- Page 25 ---
Fernando, Shen, Ram, Zhou, Samulowitz, Baracaldo, Chen
1.05
1.10
1.15
1.20
1.25
1.30
1.35
1.40
0.1
0.2
0.3
0.4
0.5
0.6
0.7
fDPO( )
Seq. (TSFT, TDPO)
(3, 0)
(2, 1)
(1, 2)
ALRIGHT 
0.25
0.5
0.75
ALRIGHT 
0.25
0.5
0.75
1.05
1.10
1.15
1.20
1.25
1.30
1.35
1.40
fSFT( )
0.1
0.2
0.3
0.4
0.5
0.6
0.7
fDPO( )
MAXRIGHT 
0.25
0.5
0.75
Mix 
0.25
0.5
0.75
MAXRIGHT 
0.25
0.5
0.75
0.0
0.2
0.4
DPO Optimality Gap
Sequential
Mix
ALRIGHT
MAXRIGHT
0.0
0.1
0.2
0.3
SFT Optimality Gap
0.0
0.2
0.4
Ideal Distance
0
20
40
60
Increase in Runtime (%)
(TSFT,TDPO) = (3, 1)
= 0.25
(2, 2)
0.5
(1, 3)
0.75
0
5
Increase in GPU Utilization (%)
Figure 13: Comparison of proposed methods with baselines (SFT first then DPO for Sequential) using Llama3-8b.
Left: Training trajectories for various methods in the objective space, visualizing their convergence with respect
to the DPO and SFT objectives. Right: Performance comparison across multiple evaluation metrics, including
optimality gap for DPO and SFT objectives, ideal distance, runtime, and GPU utilization. The bar charts
highlight the trade-offs and resource efficiency of each method for different choices of (TSFT, TDPO) or λ.
Algorithm 3 Memory Efficient MAXRIGHT
1: Input DDPO, DSFT, {αt}T
t=1,λ ∈[0, 1], max evaluation steps k
2: Initialize θ1 ∈Θ
3: for t = 1, . . . , T −1 do
4:
Sample xt
1, yt
w, yt
ℓ∼DDPO
5:
Sample xt
2, yt ∼DSFT
6:
if t mod k = 0 ∥t = 1 then
7:
Evaluate (without generating computational graph)
¯f1,λ(θt) := λ (fDPO(θt; xt
1, yt
w, yt
ℓ) −f ∗
DPO) and
¯f2,λ(θt) := (1 −λ) (fSFT(θt; xt
2, yt) −f ∗
SFT)
8:
Set t0 = t
¯f stale,t0
1,λ
= ¯f1,λ(θt)
¯f stale,t0
2,λ
= ¯f2,λ(θt)
9:
end if
10:
Set it = argmaxi ¯f stale,t0
1,λ
11:
if it = 1 then
12:
Set ¯f stale,t0
1,λ
= λ (fDPO(θt; xt
1, yt
w, yt
ℓ) −f ∗
DPO)
13:
Update θt+1 =ΠΘ (θt −αtgDPO(θt; xt
1, yt
w, yt
ℓ))
14:
else
15:
Set ¯f stale,t0
2,λ
= (1 −λ) (fSFT(θt; xt
2, yt) −f ∗
SFT))
16:
Update θt+1 =ΠΘ (θt −αtgSFT(θt; xt
2, yt))
17:
end if
18: end for
19: Output ˆθMAX := θT


--- Page 26 ---
Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning
1.6
1.8
2.0
2.2
0.2
0.4
0.6
0.8
1.0
fDPO( )
Max. Eval. Steps=1
0.01
0.25
0.5
0.75
0.99
1.6
1.8
2.0
0.2
0.4
0.6
0.8
1.0
Max. Eval. Steps=10
0.01
0.25
0.5
0.75
0.99
1.6
1.8
2.0
fSFT( )
0.2
0.4
0.6
0.8
1.0
fDPO( )
Max. Eval. Steps=100
0.01
0.25
0.5
0.75
0.99
1.6
1.8
2.0
fSFT( )
0.2
0.4
0.6
0.8
1.0
Max. Eval. Steps=1000
0.01
0.25
0.5
0.75
0.99
0.0
0.2
0.4
DPO Optimality Gap
Max Eval. Steps
1
10
100
1000
0.0
0.1
0.2
0.3
SFT Optimality Gap
0.0
0.2
0.4
Ideal Distance
0
25
50
75
Increase in Runtime (%)
= 0.01
= 0.25
= 0.5
= 0.75
= 0.99
0
10
20
Increase in GPU Utilization (%)
Figure 14: Comparison of different choices of evaluation steps for MAXRIGHT with Pythia-1b.
D.1
Qualitative evaluation
In this section, we compare the responses of the models post-trained with the proposed methods ALRIGHT and
MAXRIGHT with those of the baselines. We use models trained with λ = 0.5 for MIX, ALRIGHT, MAXRIGHT, and
(TDP O, TSF T ) = (3, 3) for Sequential. Below we provide some example prompts, the corresponding chosen response of the
dataset, and the response of model post-trained using the proposed methods and the baselines, in the experiment setting
(3) described in Section B.3.
In the first example below, we see that the response by Sequential is not very helpful, and have a more
dramatic
tone.
On
the
other
hand,
Mix
provide
a
more
detailed
response,
which
might
be
too
de-
tailed for the simple question asked by the user.
ALRIGHT and MAXRIGHT provide similar answers
that match the chosen response closely,
and provide a concise yet helpful answer to the user’s question:
Prompt
What kind of things go in a tackle box?
Chosen Response
A tackle box is a tool used for fishing. It is often small enough to be carried in one’s pocket, and contains a variety
of tools that are useful for fishing. For example, a tackle box might contain a small knife for gutting fish, a small
net for cleaning small fish, or a float for testing water.
Sequential
Lures for fish. And stuff. Hooks. And maybe a worm. (takes a worm from a plastic container) Or a gummy bear
like this.
Mix
A tackle box for fishing can contain a wide variety of gear and equipment, including but not limited to: 1. Rods and
reels: Different rods and reels are used for different techniques and purposes. 2. Lines: Monofilament, fluorocarbon,
and braided lines with a variety of strengths and colors. 3. Lures: Both artificial and live bait can be carried in a
tackle box. Spinners, spoons, crankbaits, jigs, top-water lures, and soft plastic creatures are common. 4. Hooks: In
addition to terminal tackle like leaders and swivels, hooks in various sizes and styles are essential for rigging and
baiting. 5. Bobbers: Used for fishing with live bait, bobbers can be used to suspend a hook or bait at the desired
depth. 6. Sinkers: Used for live bait fishing, sinkers weight the line to help keep it on the bottom. 7. Floaters and
drifters: Used for fishing from shore or a boat, they can help keep the line taut and keep the bait in the water. 8.
Net: Useful for landing fish. 9. Scissors: Used for cutting line and cutting off hooks and fishing line. 10. Reel pin:
Used for threading
Chosen Response
the line back through the reel seat. As you can see, there are many things that can go in a tackle box for fishing,
but some of the most essential items will depend on your specific fishing needs and technique.
ALRIGHT
Tackle boxes typically contain fishing tackle and gear used for various fishing techniques. It may include a few
necessary tools and equipment such as lures, hooks, sinkers, bobbers, split shot, line, leader, bait, and more.
Depending on your style of fishing and the type of fish you are targeting, you may need to customize your tackle
box to include only the relevant equipment. Additionally, many anglers also include their favorite snacks and
drinks to enjoy while fishing.
MAXRIGHT
A tackle box can be filled with various fishing tools and equipment, depending on the type of fishing and the
location. Some common items that go in a tackle box include: hooks, sinkers, floats, lures, bobbers, swivels,
weights, split shot, leaders, and line. You will also want to include tools to help you tie knots and sharpen hooks.
There may also be baits and scents specific to each type of fishing, for example worms, minnows, or crickets for
freshwater fishing, or shrimp, squid, or clams for saltwater fishing.
The second example involves the user asking how they can improve their penmanship. We can see that again, Sequential
response is not very helpful with the details, and provide more of a casual discussion. Both Mix and ALRIGHT provide
detailed steps to improve penmanship. MAXRIGHT also provide high-level advise how to improve penmanship, which is


--- Page 27 ---
Fernando, Shen, Ram, Zhou, Samulowitz, Baracaldo, Chen
similar to the chosen response of the dataset:
Prompt
How do I get better penmanship?
Chosen Response
You can practice. Try writing letters and short notes, it will help you practice forming letters neatly. You can also
try writing with a different kind of pen or pencil. Sometimes writing with a fountain pen can help you form your
letters more neatly. Try writing slowly, and carefully, to make sure you get each letter right. And you might want
to practice your signature. Try writing your name a few times to make sure you get it right every time.
Sequential
The same way I do - practice, and go slowly. I find it very helpful to just keep at it - no matter how much you feel
you suck at it. It’s just like anything else - if you practice enough you’ll get to a point you’re satisfied with. The
penmanship class at the library has some really excellent teachers. I’ve been told they’re very good, and that they
have a lot of tricks that help a lot of people. I don’t know much about them, though. Also, I think if you type
your work in a clean, clear font like Verdana or something, it will be really easy to make sure your handwriting is
consistent and looks nice. If you keep your penmanship kind of messy looking, it can be difficult to make it look
good, and then you’re just going to make yourself crazy over it. You know?
Mix
Here are some steps you can follow to improve your penmanship: 1) Use proper posture and position. Make sure
your chair and writing surface are comfortable and you are sitting up straight with good posture. Hold your pen
or pencil at a 45-degree angle and let your hand and arm rest on the table. 2) Write slowly and carefully. Some
people mistakenly think they can improve their penmanship by writing faster. Instead, take the time to make
sure each letter is formed properly and clearly. 3) Practice, practice, practice! No matter how long you have been
writing, you can always improve with practice. Try copying a paragraph or a page from your favorite book or take
a calligraphy class. 4) Focus on the basics. Start by working on the most fundamental aspects of penmanship, like
making sure your lowercase letters and capital letters are all properly formed and legible. 5) Experiment with
different writing styles. Not all writing styles are equally compatible with your natural handwriting tendencies.
Experiment with different penmanship styles to find the one that suits your hand the best. 6) Use a ruler as a
guide. A ruler can help you practice writing straight lines and guide your hand for straight, consistent letters. 7)
Warm up before you write. It may be helpful to practice a few strokes and exercises to warm up your hand and
get your brain ready to write. 8) Take breaks and give your hand time to rest. It’s important to take frequent
breaks and give your hand time to rest when you’re practicing penmanship. 9) Seek feedback from others. Ask
someone to read your writing and provide you with constructive criticism. This can help you identify areas that
need improvement. Remember, it takes sustained effort and practice to develop good penmanship. Consistency
is key, but also be patient and kind to yourself! With time and persistence, you can achieve your goal of better
penmanship.
ALRIGHT
To improve your penmanship, you can practice writing the same letters or words over and over again. Make sure
to pay attention to the spacing between the letters and the overall shape of the letters. You can also practice
different writing styles such as cursive or print to improve your hand-eye coordination and muscle memory. Finally,
keep in mind that improving penmanship through practice takes time, so be patient and persistent in your efforts.
Here are some steps you can follow to help you get better at penmanship:
Chosen Response
1. Practice every day: Make time to write each day, even if it’s just for a few minutes. The more you practice, the
faster you’ll see improvements. 2. Focus on your pen stroke: Try to start each letter in the same spot and end
the letter on the same line. This will help you create a consistent look for your writing. 3. Space your writing
properly: The correct spacing can improve the legibility and overall appearance of your handwriting. Pay attention
to the spaces between words, lines, and letters when writing. 4. Work on letter formation: Concentrate on writing
each letter clearly and in the proper shape. Start by practicing printing in blocks, then progress to cursive, or
vice versa. 5. Use the right equipment: Make sure you have a smooth writing pen and good quality paper. A
firm hand can help you achieve a more consistent look in your writing. 6. Set specific goals: Identify one or two
areas in your writing that you want to improve, such as legibility, speed, or consistency. Focus on these areas and
track your progress. 7. Share your writing: Have someone else read your writing and point out areas that need
improvement. This can help you identify specific problems in your handwriting and give you a new perspective on
your writing. 8. Be patient. Remember that improving your handwriting takes time, so don’t get discouraged.
Keep practicing, and you’ll see improvements in your penmanship. Please let me know if you have any questions or
concerns. In summary, to get better penmanship, you can practice writing every day, focus on your pen stroke,
space your writing properly, work on letter formation, use the right equipment, set specific goals, and share your
writing with others for feedback. Remember that improving penmanship takes time, so be persistent and patient.
MAXRIGHT
It would help to practice writing often. Writing in your own journal or diary every day would be a great way to
practice. Or, you could try taking a class in calligraphy, which is basically stylized penmanship. Make sure you use
a pen with a fine, smooth point. This will help you control the line thickness and create a neat, uniform look. If
you ever see a mistake, cross it out and start again! Remember, practice makes perfect!
The final example involves the user asking how they can teach kids about cooperation. It is interesting to see that the
chosen response of the dataset itself seems to be of low quality, since it has suggestions like ‘punish’ non-cooperation, which
might not be desirable advice. The response given by Sequential is again not very helpful, and too short for a question like
this, which has a lot of depth. All Mix, ALRIGHT, and MAXRIGHT provide comprehensive and helpful advice for the
question asked. However, the response provided by Mix seems to be too detailed to the point that some parts are not
necessarily true. For example, the Mix response suggests Chess as a game that teaches cooperation, which is questionable
since Chess is a predominantly individual sport:


--- Page 28 ---
Understanding Forgetting in LLM Supervised Fine-Tuning and Preference Learning
Prompt
How can I teach kids about cooperation?
Chosen Response
How could we get children to cooperate together? A few different techniques are to reward cooperation, punish
non-cooperation, create a hierarchy of status, and incentivize people to cooperate.
Sequential
We can program a video game that involves co-opetition. This will help teach them about collaboration and
healthy competition. This way everyone wins!
Mix
Here are some activities and approaches to teach kids about cooperation: 1) Storytelling: Narrate stories about
kids working together and collaborating. For example, imagine a group of kids building a sandcastle on the beach.
Have each child take turns participating and contributing to the project. You can pause and ask questions about
the group’s dynamics and how they handled problems (e.g., what challenges did they face and how did they resolve
them?). In doing so, you will highlight the importance of communication and cooperation, and foster the idea that
they can accomplish more together than individually. 2) Role-playing: Role-playing can be a fun and effective way
to teach kids about cooperation. For example, you could pretend to be a pirate captain searching for treasure, and
divide the kids into two teams. The first team would be the thieves who have stolen the treasure. The other team
as the pirates who need to catch up to them. In order to catch the thieves, the kids will have to work together and
divide tasks (e.g., one team member may need to scout the area, while others need to guard the path and block the
thieves’ escape). Once caught, the kids can learn to problem-solve together by developing a plan on how to return
the treasure. 3) Game play: Cooperative games can help kids learn to work together and collaborate in achieving
common goals. Simple games like Four in a Row, Chess, or Simon Says can be a good place to start. You can make
the playing board large so that it can provide space for more than one player, while you can guide the discussion
afterwards about how they cooperated and contributed to the group’s achievement. Remember, it’s important
to make learning fun and enjoyable for kids, so try to approach these activities in a lively and interactive way.
Give plenty of chances to kids to take part and participate, and don’t be afraid to improvise and adapt to the
needs of the group. Cooperation is a vital social skill and early introduction can help children develop a culture of
cooperation that helps them succeed in life.
ALRIGHT
Cooperation is a vital life skill for children to learn, as it teaches them how to work together, share responsibilities,
and foster a sense of community. Here are some tips and activities to teach kids about cooperation: 1. Explain the
concept of cooperation: Define cooperation for children and explain how it benefits them, their friends, and the
whole group. Encourage them to think of examples from real life where they have seen people cooperating with
each other. 2. Share stories about cooperators: Read children’s books or share real-life examples of people who
worked together to achieve a goal. Point out how they communicated and took turns or assigned specific roles to
each other. 3. Play cooperative games: Find cooperative games that require teamwork, such as building structures
with blocks or playing musical chairs. After the games, focus on how everyone had to work together to achieve a
common goal. 4. Practice taking turns: Encourage children to take turns in activities or conversations. Teach
them to wait patiently for their turn and to let others finish speaking before speaking. 5. Role-play cooperation
situations: Role-play simple cooperation scenarios like sharing a toy or a piece of food. Practice using kind
words and gestures and taking turns. Remember that cooperation is learned through experience, not just verbal
instruction. Encourage children to practice cooperation in their everyday lives, and reinforce positive behavior
when they cooperate with others. With consistent efforts and modeling, children will develop an understanding of
cooperation, as well as the valuable life skill of working together to achieve a common goal.
MAXRIGHT
Cooperation is important because it helps children to develop social skills and build meaningful relationships with
others. Here are some ways that you can teach kids about cooperation: 1. Encourage sharing and taking turns:
Teach children to share their toys and games, and let them learn to take turns and wait their turn. You can use
songs, rhymes, and games to promote sharing at a young age. 2. Teach conflict resolution: Teach children conflict
resolution skills, such as apologizing, resolving disagreements, and seeking compromise. You can role-play scenarios
with your children to help them understand how to resolve conflicts peacefully. 3. Model cooperative behavior:
Show your children how to cooperate by working together on tasks, sharing information, and listening to each
other.
Chosen Response
You can also explain to children how cooperation helps to build stronger relationships and achieve shared goals. 4.
Encourage teamwork in group activities: Group activities, such as sports, arts and crafts, or team-based games,
provide opportunities for children to learn the importance of cooperation in achieving a common goal. 5. Teach
children empathy and respect: Empathy and respect are important elements of cooperation. You can help children
to develop these skills by teaching them about others’ perspectives and needs. This can also help children to learn
to value and respect different opinions and viewpoints. By using these strategies, you can help children to learn
about the importance of cooperation and develop the skills needed to work together effectively with others.
