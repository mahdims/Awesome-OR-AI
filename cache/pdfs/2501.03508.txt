--- Page 1 ---
A Sequential Optimal Learning Approach
to Automated Prompt Engineering in Large Language Models
Shuyang Wang1
Somayeh Moazeni2
Diego Klabjan3
1Department of Engineering Sciences and Applied Mathematics, Northwestern University
2School of Business, Stevens Institute of Technology
3Department of Industrial Engineering and Management Sciences, Northwestern University
Abstract
Designing effective prompts is essential to guiding large lan-
guage models (LLMs) toward desired responses. Automated
prompt engineering aims to reduce reliance on manual ef-
fort by streamlining the design, refinement, and optimiza-
tion of natural language prompts. This paper proposes an
optimal learning framework for automated prompt engineer-
ing, designed to sequentially identify effective prompt fea-
tures while efficiently allocating a limited evaluation bud-
get. We introduce a feature-based method to express prompts,
which significantly broadens the search space. Bayesian re-
gression is employed to utilize correlations among similar
prompts, accelerating the learning process. To efficiently ex-
plore the large space of prompt features for a high quality
prompt, we adopt the forward-looking Knowledge-Gradient
(KG) policy for sequential optimal learning. The KG policy is
computed efficiently by solving mixed-integer second-order
cone optimization problems, making it scalable and capable
of accommodating prompts characterized only through con-
straints. We demonstrate that our method significantly out-
performs a set of benchmark strategies assessed on instruction
induction tasks. The results highlight the advantages of using
the KG policy for prompt learning given a limited evalua-
tion budget. Our framework provides a solution to deploying
automated prompt engineering in a wider range applications
where prompt evaluation is costly.
Key Words:
automated prompt engineering, optimal
learning, Knowledge-Gradient, Bayesian regression, feature-
based prompts
1
Introduction
Large Language Models (LLMs) demonstrate exceptional ca-
pabilities in following instructions, making them a power-
ful tool to various downstream tasks [31, 20, 2, 30].
A
well-designed prompt steers an LLM to generate desired re-
sponses, enabling effective adaptation to downstream appli-
cations without incurring the high cost of fine-tuning the
model weights. Nevertheless, creating effective prompts can
be challenging due to the sensitivity of LLM outputs to
prompt variations [32, 21, 38]. In addition, manually iden-
tifying ideal prompts is often time-consuming and lacks sys-
tematic guidance. Automated approaches to designing, opti-
mizing, and refining LLM prompts mitigate this challenge by
minimizing the need for manual intervention.
Recent efforts in automated prompt engineering primar-
ily have focused on iterative evaluation and refinement in or-
der to converge to ideal prompts [12, 39, 28, 27, 35]. The
methods by these studies often assume the availability of nu-
merous iterations. However, in many real-world scenarios,
opportunities to evaluate prompts are limited, as each prompt
evaluation is costly or time-consuming. For example, in med-
ical research, each prompt evaluation could involve extensive
time and resources from medical professionals, making it im-
practical to test a large number of variations before selecting
the final one.
Moreover, many existing approaches search over a set of
precrafted candidate prompts for ideal prompts [33, 39, 22]
These methods require identifying and enumerating a set of
prompts, which restricts the scalability as the candidate set
expands. Furthermore, it fails to utilize the correlation among
similar prompt to expedite the learning.
To fully unlock the potential of LLMs across diverse
scenarios, it is crucial to develop an automated prompting
framework that is capable of capturing dependencies among
prompts and efficiently identifying high-performing prompts
within few evaluations.
This paper presents a principled
forward-looking iterative process for automated prompt engi-
neering through the optimal design of a sequence of prompts.
We propose an interpretable feature-based approach to
prompt representation. Various categorical or numerical fea-
tures can be considered to characterize detailed aspects of a
prompt, such as the selection and ordering of demonstrative
examples. Previous works identify factors within a prompt
that influence the LLM outputs but often treat these aspects in
isolation. We allow for capturing various interactions among
prompt attributes and can accommodate potential constraints
on the features. This feature-based prompt representation en-
ables the inclusion and exploration of a vast and diverse set
1
arXiv:2501.03508v1  [cs.CL]  7 Jan 2025


--- Page 2 ---
of prompts, which, unlike previous approaches, do not require
manual prompt provision. In addition, in contrast to prompt
descriptions based on embedding vectors, our representation
is inherently interpretable.
We adopt a Bayesian approach to refine beliefs about the
influence of prompt features on the LLM response. This ap-
proach supports the integration of prior knowledge and user
opinions as well as enabling to capture feature correlations.
To operationalize this, we define a probabilistic model to link
prompt features to a response quality of interest. In this paper,
we demonstrate our approach using LLM response accuracy
as the primary quality metric.
Next, we formalize the iterative process of automated
prompt engineering in the presence of limitations on the num-
ber of prompt evaluations as a sequential decision-making
problem. Given the limited opportunities for prompt eval-
uation, this problem falls into the category of finite-horizon
discrete-time Markov decision processes; see [29]. An op-
timal learning policy sequentially selects a feasible prompt
representation for evaluation, aiming to maximize the ex-
pected outcome of the final prompt. Due to the potentially
large prompt feature space, the curse of dimensionality [25]
hinders the exact computation of the optimal prompt selec-
tion. We adopt an approximate policy for the optimal learn-
ing problems, known as the expected improvement policy in
[4, 5] or Knowledge-Gradient (KG) policy in [9, 26]. This
is a forward-looking policy that maximizes the expected im-
provement in the value of information in each learning phase.
The KG policy often excels in practical scenarios with lim-
ited evaluation budgets, frequently outperforming common
static data acquisition strategies and dynamic test-and-learn
policies. For the consistency of the KG policy, see [10] for
correlated features and [14] for constrained search space.
The large space of prompt candidates, defined by con-
strained features, makes it impractical to enumerate all fea-
sible alternatives for determining KG decisions. To address
this challenge, we leverage recent advancements in scalable
optimal learning and KG computation. In contrast to ear-
lier results to compute KG decisions [9] based on enumera-
tion of all feasible alternatives, recent computational methods
[24, 14, 6] build on optimal quantization of the response prob-
ability followed by mixed-integer conic optimization refor-
mulations to leverage efficient optimization solution methods.
For optimal learning problems with larger feature spaces, an
iterative process involving solving mixed-integer linear opti-
mization problems is employed to achieve even greater com-
putational efficiency and scalability.
Our framework allows for different prompt representa-
tions and selection policies, hence encompassing various ex-
isting methods for automated prompt engineering. For exam-
ple, when a small, finite set of precrafted prompt templates
is provided and a point-wise utility model is used, our setting
simplifies to the setup in [22, 39, 33].
We assess the performance of sequential prompt learn-
ing with adaptive prompt selection policies on a dataset of
instruction induction tasks [15].
This benchmark dataset
contains 24 instances of instruction induction, designed for
LLMs to deduce implicit tasks or instructions from language
prompts including answers or contextual information. For the
instruction induction tasks, we first propose a feature-based
prompt template and use the accuracy of responses collected
from GPT-3.5 on the validation data as the primary perfor-
mance metric. For the prompt selection, we evaluate the KG
policy, the adaptive myopic policy, the increasingly popular
Thompson sampling policy, and a number of other automated
prompt engineering methods such as EvoPrompt [12] using
an evolutionary algorithm to refine prompts and TRIPLE [33]
using a multi-armed bandit approach to select prompts.
Our analyses show the effectiveness of our approach par-
ticularly with the KG prompt selection policy, which is ca-
pable to converge to high-quality prompts within 30 or fewer
prompt evaluations. These prompts significantly outperform
those generated by the benchmarks on the test data using the
same number of LLM interactions. Further analysis reveals
that the KG policy is particularly favorable for challenging
tasks with high uncertainty in LLM responses and signifi-
cant sensitivity to prompts, achieving a substantial margin
over baseline policies. Our findings highlight the advantages
of using the KG policy in prompt engineering to selectively
evaluate prompts, expanding the potential for deploying auto-
mated prompt engineering in applications with large prompt
evaluation costs.
Our contributions can be summarized as follows.
• We introduce a sequential optimal learning framework
for automated prompt engineering to guide through the
process of designing a sequence of prompts that effec-
tively elicit accurate responses from an LLM. The ap-
proach is particularly effective for applications where
prompt evaluation is resource-intensive.
• We propose a feature-based approach to represent lan-
guage prompts, which greatly expands the prompt
search space.
A link function maps the features to
LLM response accuracy through Bayesian model pa-
rameters to leverage correlations among prompts with
shared characteristics. Our method enables simultane-
ous optimization of multiple features to generate im-
proved prompts.
• We leverage the KG policy within our sequential
prompt learning to efficiently identify high-performing
prompts in large prompt spaces. The KG policy outper-
forms various benchmark policies, especially for chal-
lenging tasks with high uncertainty in LLM response.
The remainder of the paper is organized as follows: Sec-
tion 2 reviews the related literature. Section 3 discusses the
generic iterative process for automated prompt learning, for-
malizing the problem as a sequential decision making prob-
2


--- Page 3 ---
lem. Section 4 discusses the forward-looking KG policy for
the prompt selection in iterative automated prompting. Illus-
trative examples and computational results are provided in
Sections 5 and 6. Finally, conclusions and potential exten-
sions are discussed in Section 7.
2
Related Work
Generation-then-selection Methods
[39] present a two-
phase pipeline of instruction generation and selection. This
method generates a set of candidate instructions, which are
evaluated and filtered based on their performance on the
downstream tasks until the best one from the candidate set is
found. [22] uses an optimal control paradigm to systematize
the process of iteratively updating and selecting from a set of
candidate prompts. However, these approaches are limited to
search spaces represented by individual prompt candidates.
Our proposed framework encompasses different prompt rep-
resentations and exploration policies. Our feature-based ap-
proach to represent prompts captures their dependencies and
enables a diverse search space. [33] follows the two-phase
pipeline with a focus on prompt selection subject to a fixed
evaluation budget. They formulate the selection as a multi-
armed bandit (MAB) problem and utilize the continuously
reject method to select from the candidate set. However, their
solution is not scalable with the size of the prompt candidate
set. In contrast, our framework with the KG prompt selection
policy can accommodate larger spaces of prompts.
Edit-based Methods
An approach to automated prompting
focuses on generating refined prompts by iteratively editing
base prompts. [27] propose GrIPS, which iteratively applies
text-based edit operations, such as word substitutions and
deletions, to a base prompt. The best candidate is then se-
lected as the new base prompt. Alternatively, [12] apply evo-
lutionary algorithms and generate new candidate prompts by
performing mutation and crossover operations using an LLM,
retaining high-quality prompts for the next generation. [8]
also use an LLM to perform mutation operations on a popu-
lation of prompts but employ a self-referential way of improv-
ing both the prompts and the mutation operations. [16] pro-
pose an iterative prompt refinement scheme specially crafted
for relevance ranking in information retrieval. While these at-
tempts show the potential of edit-based approaches for gener-
ating high-quality prompts, they mainly rely on local search
by modifying existing prompts. Our method, however, ex-
plores the prompt space in a forward-looking principled man-
ner using Bayesian optimal learning, and utilizes knowledge
from prior observations to inform future selections of prompts
to evaluate.
Prompt Learning Methods
Recent works [3, 19] experi-
ment using Bayesian Optimization to search in the embed-
ding space of prompts, but white-box LLMs are required to
facilitate the optimization steps. Our approach also builds on
Bayesian learning, but we directly search in the space of dis-
crete prompts and eliminate the need for white-box LLMs.
The concept of prompt learning is also used in [23], which
trains a reinforcement learning model to select tokens as ac-
tions to form prompts. The training stage requires sufficient
evaluation budget, while our framework is capable of learning
from only a limited number of prompt evaluation.
Gradient-based Methods
Gradient-based algorithms have
been used to solve the problem of optimizing the prompt per-
formance over the prompt space. [35, 34] model prompts as
sequences of trigger tokens, and compute the gradients of the
log-likelihood of the language model generating the target
outputs with respect to the embeddings of candidate tokens
to guide the search for optimal tokens. [36] compute the gra-
dients of a similarity metric between the generated and tar-
get outputs with respect to the prompt embeddings to guide
prompt optimization, and project the optimized embeddings
to discrete prompts using nearest neighbors. These methods
require access to the internal parameters of the LLM, making
them incompatible with black-box LLMs. Moreover, these
methods require computationally intensive gradient compu-
tations. In contrast, our approach generates human-readable
prompts without accessing internal parameters.
3
Sequential Optimal Prompt Learning
We introduce a sequential optimal prompt learning frame-
work, called SOPL, for automated prompt engineering that
is compatible with black-box LLMs and generates human-
readable prompts, while addressing the challenge of limited
number of iterations for prompt evaluation. Our framework,
depicted in Figure 1, follows the iterative process outlined
in Algorithm 1. The components of the framework are ex-
plained in the subsequent subsections.
3.1
Feature-Based Prompt Representation
To identify high-quality prompts, it is essential to explore a
diverse set of prompts tailored to the specific downstream
task. We adopt a feature-based representation for prompts,
expressing a language meta prompt through various features
that capture its content and structure. Prompt features are
denoted by the vector x that specifies a textual prompt. Ex-
amples of such features include the structural template, tone,
role, context, demonstrations, specificity, complexity, embed-
dings, task type, question framing, constraints, and temporal
references. These features are generally either manually en-
gineered by the user or derived from established prompt tem-
plates in the literature corresponding to the task. Refer to
Section 5.1 for the specific features and categories utilized in
3


--- Page 4 ---
Figure 1: SOPL: Sequential optimal prompt learning for automated prompt engineering
Algorithm 1 Sequential optimal prompt learning
Require Maximum iteration N, score function Eval : X →
(0, 1), prompt representation selection policy π : S →X,
Bayesian update function Update : S × R →S that imple-
ments (4)-(7).
1: Initialize knowledge state S.
2: Initialize best prompt so far x∗.
3: Initialize best score so far u∗←−1.
4: for step n = 1, ..., N do
5:
Get prompt representation x ←π(S).
6:
Evaluate prompt and get a score ux ←Eval(x).
7:
if u > u∗then
8:
Record best score so far u∗←ux.
9:
Record best prompt so far x∗←x.
10:
end if
11:
Update knowledge state S ←Update(S, logit(ux)).
12: end for
13: return x∗.
the instruction induction task for our experiments, developed
based on the template proposed in [15].
Previous studies have primarily examined each feature in
isolation, whereas we integrate these features in the prompt
representation to leverage the potential synergies that emerge
from their combination. By enriching the meta prompt with
multiple features known to influence LLM responses, we can
expand the search space.
In general, the feature space may contain variables of dif-
ferent types: continuous, categorical, and ordinal. In addi-
tion, various requirements may be imposed on the features
either by definition or by the user’s preferences to account
for mutually exclusive features, conditional features, com-
bined effects of multiple features, disjunctive features, and
multiple-choice decisions. The set of feasible feature com-
binations forms a diverse and potentially large search space,
denoted by X. Our framework does not require explicitly
identifying and enumerating all feasible prompts. Instead,
the feasible prompt space X, which encompasses the feasi-
ble values of the prompt features, is specified solely by linear
inequality or equality constraints.
Our framework encompasses existing approaches as spe-
cial cases. For example, the methods [39, 33, 22] that follow
the pipeline of generating and selecting from a set of candi-
dates can be thought of as using the candidate set as X.
3.2
Prompt Evaluation
The prompt evaluation phase involves querying a black-box
LLM with the prompt constructed from x and collecting the
LLM’s response. When evaluating the prompt on the down-
stream task, we measure the quality of the observed LLM re-
sponse to the prompt associated with x using a numeric score
ux, which is computed from the score function Eval defined
on X; see Algorithm 2 for details on Eval(x). Given labeled
data, the score can be computed by comparing the LLM re-
sponses to the ground truth labels and calculating the percent-
age of accurate response. For downstream tasks that require
human evaluation, the score is based on human feedback.
For common metrics such as accuracy, F1-score, and
point-wise mutual information [1], the score lies in the in-
terval between 0 and 1. We assume that
ηx := logit(ux) = Θ⊤x + ϵ,
(1)
where Θ ∼N(µΘ, ΣΘ/ρ) is the D-dimensional model pa-
rameter and ϵ ∼N(0, 1/ρ) with variance 1/ρ is the measure-
ment noise. The quantity ηx represents the utility of x. The
mean µΘ and the precision ρ are the unknown parameters to
estimate. For general score functions that return values be-
4


--- Page 5 ---
yond the interval between 0 and 1, alternative link functions
can be employed in (1) in place of the logit function.
3.3
Knowledge State Update
An approach to addressing uncertainty in the effectiveness of
prompt features is Bayesian learning. The Bayesian approach
to inference can account for multiple levels of randomness
and correlation by using prior distributions for model param-
eters. Additionally, existing knowledge and user input can
be incorporated into these prior probability distributions. We
adopt the Bayesian framework, letting a multivariate normal
prior for the coefficients, µΘ, and a Gamma prior for the pre-
cision, ρ, i.e.,
µΘ|ρ ∼N (θ, Σ/ρ)
(2)
ρ ∼Gamma(a, b).
(3)
The belief represented by S = (θ, σ, a, b) is referred to
as the knowledge state. The knowledge state encodes prior
observations of prompt performance and serves as the basis
to inform future decisions. The multivariate distribution (2)
captures dependencies among unknown parameters, implying
that learning about one prompt can provide insights into the
effectiveness of several other prompts, thereby enhancing the
learning speed.
We iteratively update the knowledge state based on ob-
served responses to queried prompts.
At iteration n, the
knowledge state is denoted by Sn = (θn, Σn, an, bn), and
the selected prompt representation is denoted by xn. After
the n-th iteration of prompt evaluation, we observe the score
un := uxn and update the knowledge state as follows.
θn+1 = θn +
logit(un) −θ⊤
n xn
(1 + x⊤
t (Σn + ΣΘ) xn)Σnxn
(4)
Σn+1 = Σn −
Σnxnx⊤
n Σn
1 + x⊤
n (Σn + ΣΘ) xn
(5)
an+1 = an + 1
2
(6)
bn+1 = bn +
(logit(un) −θ⊤
n xn)2
2(1 + x⊤
n (Σn + ΣΘ) xn)
(7)
Equations
(4)-(7)
collectively
define
the
mapping
Update(S, logit(x)) appeared in line 11 of Algorithm 1.
3.4
Prompt Representation Selection Policy
For each iteration, a prompt representation xn is selected ac-
cording to a policy π. The goal is to find a prompt that maxi-
mizes the score on the downstream task after N iterations of
prompt evaluation.
Our framework allows for different prompt representation
selection policies to explore the feasible prompt space X and
update the knowledge state. Heuristic policies, such as adap-
tive myopic (Greedy) and Thompson sampling (TS), can be
adopted. The Greedy policy selects the best prompt represen-
tatio x based on the current knowledge state by
πGreedy(Sn) := argmax
x∈X
E[ηx|Sn] = argmax
x∈X
θ⊤
n x.
(8)
The TS policy samples from the posteriors of the parameters
by
ˆρ ∼Gamma(an, bn), ˆθ ∼N (θn, Σn/ˆρ) ,
(9)
and then selects the prompt representation x by
πT S(Sn) := argmax
x∈X
ˆθ⊤x.
(10)
The heuristic policies such as Greedy and TS policies are
adaptive, but they are not forward-looking, in the sense that
they do not explicitly take into account the effect of selected
prompts on the subsequent prompt selections and the overall
learning process.
The prompt representation selection policy π is a key
building block influencing the performance of the SOPL
framework. When the number of iterations is limited to N,
the process of sequential optimal prompt learning can be for-
mulated as a finite-horizon Markov decision process, where
the action space X consists of prompt representations, and the
state space S consists of knowledge states. In the next sec-
tion, we discuss an approximate policy for the optimal prompt
representation selection policy.
4
KG Prompt Selection Policy
We consider a forward-looking optimal learning policy de-
signed to maximize the expected improvement in an approx-
imated value of information during each iteration. This ap-
proach, known as the Knowledge-Gradient (KG) policy, of-
fers an approximate solution to the MDP for prompt se-
lection.
For additional discussion and analysis, refer to
[13, 5, 11, 26]. The value of information is measured by
the expected single-period improvement, i.e., the difference
between the values of the knowledge states Sn+1 and Sn if
the prompt representation xn+1 = x is selected. Hence, at
iteration n, the following KG quantity is maximized:
νn
x := E [VN(Sn+1)|Sn, x] −VN(Sn).
(11)
where VN(S) is the value of the optimal policy at iteration N
for any knowledge state S, i.e., VN(S) = maxπ∈Π V π
N(S) =
maxx∈X E[ηx|S], where ηx is as in equation (1). Recall that
Sn+1 is the transition from state Sn induced by the updat-
ing procedure in equations (4)-(7). The quantity νn
x is the
marginal value of one more prompt representation x being
queried. Its value is always nonnegative.
5


--- Page 6 ---
The decision of the KG policy selects the prompt repre-
sentation that maximizes the KG quantity in equation (11):
πKG(Sn) := argmax
x∈X
νn
x .
(12)
For discussion on the related concepts of asymptotic optimal-
ity and statistical consistency of the KG policy, the reader is
referred to [11, 10] when X is specified in the enumerative
form, and see [14] when X is represented in a constraint-
based form.
For any x ∈X, the KG quanitity corresponding to model
(1) is given by:
νn
x = E[max
y∈X (pn
y + qn
y (x)T2an)|Sn] −max
y∈X pn
y
(13)
where T2an follows a student’s t distribution with 2an degrees
of freedom, and
pn
y = θ⊤y
(14)
qn
y (x) =
s
bn
an(1 + x⊤(Σn + ΣΘ)x)x⊤Σny (15)
The expectation in equation (13) is with respect to the one-
dimensional random variable T2an.
The first term in the KG quantity in equation (13) can be
approximated by
J
X
j=1
wj max
y∈X (pn
y + qn
y (x)tj)
(16)
where t1, ..., tJ ∈R is the sequence of points that minimizes
the quadratic quantization error of the Voronoi quantizer for
T2an, t0 = −∞, and tJ+1 = ∞. The weights are defined as
wj = FT2an

tj+tj+1
2

−FT2an

tj−1+tj
2

for j = 1, · · · , J.
Here, FT2an is the cumulative distribution function of T2an.
Therefore, the selected prompt representation based on the
KG policy at state Sn is computed by solving the following
mixed-integer optimization problem:
max
(ˆx,τ)∈X +,τ≤M,x,y1,...,yJ∈X
J
X
j=1
wjθ⊤
n yj + τ
(17)
s.t. ∥P 1/2
n
ˆx∥2 ≤
J
X
j=1
wjtjx⊤Σnyj
(18)
τ · 1m −M(1m −x) ≤ˆx ≤Mx
(19)
Here, m is the dimensionality of the prompt representa-
tion features, and Pn :=
an
bn ( A⊤A
h⊤h + Σn + ΣΘ), where
A and h form the equality constraints of the feasible set
X = {x|Ax = h, Bx ≤g}. In this problem, X +, con-
sisting of elements (x, τ) ∈Rm, represents the homogenized
version of the set X. In the last constraint, M denotes a large
constant. For further details and a computationally efficient
iterative algorithm only involving solving mixed-integer pro-
gramming problems to solve this problem, see Propositions 6
and 7 in [24].
5
Computational Experiments
We demonstrate the performance of the proposed SOPL
framework on the instruction induction tasks [15].
The
dataset consists of 24 individual tasks, covering various
aspects of text comprehension.
Each data point com-
prises a pair of input and output.
For example, for the
task larger animal, one data point consists of an input
“cougar, flea” and an output “cougar”. The objective is to
find an instruction such that when the LLM is queried with
the instruction and an input, its response matches the correct
output. A possible instruction for this task can be “choose
the larger animal”. Similar to [39], we generate possible in-
structions by prompting an LLM using a meta prompt, which
consists of demonstrative examples of input-output pairs and
asks for a possible instruction. For each task, we partition the
dataset into three sets: a demonstration dataset, a validation
dataset, and a held-out test dataset.
Feature-Based Prompt Representation.
We focus on five
aspects of prompts that have been shown to impact LLM re-
sponses. [39] note that the template of the meta prompt im-
pacts the effectiveness of the induced prompts; [21, 38] find
that both the selection and the order of demonstration exam-
ples influence generated texts; [37, 17] show that specifying
different roles elicits diverse text generations from LLMs;
[7, 39] discover that prompts paraphrased by LLMs yield im-
proved performance on downstream tasks; [18] observe that
LLMs respond differently to different descriptions of tones.
We create a set of choices for each feature, summarized
in Table 1. A feature vector x specifies one choice for each
feature. By applying one-hot encoding to represent each cat-
egorical feature, the prompt representation feature x becomes
a binary vector. All combinations of the prompt features, sub-
ject to the constraint that exactly one choice is selected for
each feature, form the search space X.
Prompt Evaluation.
We evaluate the selected feature vec-
tor x on the validation data and obtain the validation score
ux by Algorithm 2.
We first convert the feature vector x
to a textual instruction in line 1 and 2. For example, if the
feature vector specifies meta prompt template 1, 5 demon-
strative examples, roles of I and friend, no paraphrasing,
and description of clear, then we create a meta prompt by
MetaPrompt(x), which inserts the 5 demonstrative exam-
ples in meta prompt template 1 in Figure 2, and replaces
[ROLE1] by “I”, [ROLE2] by “friend”, and [DESCRIP-
TION] by “clear”. We query an LLM with the meta prompt to
generate an instruction Ix that reflects the selected features.
For each input pi in the validation data, we create an eval-
uation prompt by EvalPrompt(Ix, pi), which combines the
instruction and the input using the template in Figure 4. The
prompt is then used to query the LLM. A task-specific met-
ric, such as exact match or F1-score defined in [15], is used
6


--- Page 7 ---
Feature
Choices
Meta prompt template
4 meta prompt templates shown in Figure 2.
Demonstrative examples
20 choices, each consists of 5 examples sampled from the demonstration dataset.
Roles
(Scientist, research assistant), (Professor, PhD student), (Mom, kid),
(Programmer, AI system), (Manager, employee), (I, friend),
(Director, actor), (Coach, athlete), (Chef, sous chef).
Paraphrasing
Binary: if paraphrasing is used, we prompt the LLM again to generate
a variant of the instruction using the template in Figure 3.
Description
(empty), clear, detailed, simple, complex, precise, ambiguous,
technical, expository, conceptual, authoritative, friendly, formal,
informal, encouraging, stern, rude, assertive, humorous.
Table 1: Prompt features used for instruction induction tasks
(a) Meta Prompt Template 1
(b) Meta Prompt Template 2
(c) Meta Prompt Template 3
(d) Meta Prompt Template 4
Figure 2: Meta Prompt Templates
Figure 3: Paraphrasing Template
Figure 4: Evaluation Template
to compute a score by comparing the LLM response with the
correct output qi. The average score across all validation ex-
amples is used as the score ux.
5.1
Benchmark Methods
We compare our method with two benchmarks EvoPrompt
[12] and TRIPLE [33]. Both methods provide solutions com-
patible with black-box access to LLMs and generate human-
readable prompts.
EvoPrompt uses the differential evolu-
tion algorithm to iteratively refine a population of prompts.
TRIPLE employs the continuously reject algorithm to iden-
tify an effective prompt from a candidate pool under a fixed
budget. In addition, we use Greedy and TS presented in (8)
and (10) as the baseline policies.
7


--- Page 8 ---
Algorithm 2 Score function Eval(x)
Require An LLM, validation data {(pi, qi)}V
i=1, meta
prompt
construction
function
MetaPrompt,
eval-
uation
prompt
construction
function
EvalPrompt,
metric
function
Metric
to
evaluate
LLM
re-
sponse.
1: Create meta prompt Mx ←MetaPrompt(x).
2: Generate instruction Ix ←LLM(Mx).
3: for i = 1, ..., V do
4:
Get evaluation prompt Ei ←EvalPrompt(Ix, pi).
5:
Receive LLM response Ri ←LLM(Ei).
6:
Evaluate LLM response Ui ←Metric(Ri, qi).
7: end for
8: Compute the average score ux = 1
V
PV
i=1 Ui.
9: return ux.
5.2
Implementation Details
We record the instruction with the highest validation score
during the process. When the maximum number of prompt
evaluation is reached, the instruction with the highest valida-
tion score is used as the final instruction. The test score is ob-
tained by evaluating the final instruction on the held-out test
data, using a similar procedure in Algorithm 2. We report the
average test score across 20 replications with different ran-
dom seeds.
The held-out test dataset for each task consists of 100
examples unless specified otherwise in [15], and is kept the
same for all replications. For each replication, we randomly
select 10 examples from the rest of the data as the demon-
stration dataset, and then randomly select 100 examples or all
remaining examples if fewer are available as the validation
dataset.
We use OpenAI GPT-3.5 as the LLM for both generating
and evaluating instructions. We allow N = 30 opportunities
to evaluate on the entire validation dataset. We set the popu-
lation size to be 10 for EvoPrompt as in [12], and set the size
of the candidate pool to be 30 for TRIPLE as in [33]. We
ensure that the same number of API calls to the LLM is used
for evaluating on the validation data across all methods.
6
Results and Sensitivity Analysis
We focus on the 13 challenging tasks where the validation
score are below 80% with relatively large variance using the
default meta prompt template in [15]. Table 2 presents the
average test performance across 13 tasks for SOPL and the
benchmark approaches. For each task, we illustrate the mean
and standard deviation of the test score across 20 replications
in Figure 5. The results indicate that SOPL-KG outperforms
the benchmarks. It exhibits the highest average test score of
0.6281 among all methods, with a 6.47% improvement in av-
erage test score and a 17.92% average improvement per task
relative to EvoPrompt. For each task, we rank the five meth-
ods from the highest to the lowest test score, and calculate
the average ranking across 13 tasks. The SOPL-KG achieves
the highest average ranking of 1.85, and the SOPL-TS has the
second best ranking of 2.69, outperforming both EvoPrompt
and TRIPLE. We compute the standard deviation of the test
scores across 20 replications for each task, and report the
average across 13 tasks in Table 2. The SOPL-KG exhibits
the lowest standard deviation, demonstrating its robustness to
variations in random seeds.
The findings highlight the effectiveness of our proposed
framework with feature-based prompt representations and the
KG prompt selection policy in comparison to other bench-
marks.
Although the space of all feature combinations is
prohibitively large for exhaustive exploration, the SOPL-KG
is capable of discovering high-quality prompts within given
prompt evaluations.
6.1
Sensitivity to the Number of Iterations
We consider more challenging scenarios with fewer iterations
of N = 20 and N = 10. Table 3 presents the average test
performance across 13 tasks. The SOPL-KG outperforms all
other methods within fewer iterations, with a slightly lower
average test score of 0.6174 when N = 20 compared to N =
30. The SOPL-KG also has the lowest standard deviation
when N = 20, while the SOPL-TS is slightly more robust to
variations in random seeds when N = 10.
We implement an early stopping mechanism in our frame-
work to further reduce the cost of prompt evaluation. We
terminate the process early if the best validation score does
not improve for τ consecutive steps, or when the maximum
number N of steps is reached. We conduct experiments with
τ = 5 and τ = 10 for 20 replications and report the results
in Table 4. Within 17 realized iterations, all three prompt
selection policies yield close to but slightly worse test score
compared to N = 30. TS has a small advantage in the av-
erage number of steps used, but KG dominates the test score
relative to baseline policies. It shows that the KG policy has
the potential of reducing the number of evaluations required
without significantly compromising performance.
6.2
Sensitivity to the Prompt Selection Policy
As Figure 5 illustrates, the advantage of the KG policy is
more evident for some tasks while more subtle for others.
For each task, we randomly sample 100 feature vectors and
evaluate their validation scores. We compute the mean µ100
and the standard deviation σ100 of the 100 scores, and cal-
culate the coefficient of variation by CV100 := σ100/µ100.
This metric aims to represent the uncertainty in the LLM re-
sponse performance as the prompt feature values vary. Fig-
ure 6 depicts the relationship between the relative improve-
8


--- Page 9 ---
Metric
SOPL-KG
EvoPrompt
TRIPLE
SOPL-TS
SOPL-Greedy
Test score
0.6281
0.5900
0.5609
0.5948
0.5750
Standard deviation
0.0668
0.0881
0.0966
0.0880
0.0959
Improvement of SOPL-KG
0.00%
6.47%
11.99%
5.60%
9.23%
Improvement of SOPL-KG per task
0.00%
17.92%
17.19%
9.13%
14.35%
Ranking
1.85
2.92
3.77
2.69
3.69
Table 2: Average test performance across 13 tasks for different methods
Figure 5: Test performance on 13 tasks for different methods. The height of each bar represents the average test score and the
error bar represents the standard deviation across 20 replications with different random seeds
Method
N = 20
N = 10
Mean
STD
Mean
STD
EvoPrompt
0.5776
0.0956
0.5625
0.0920
TRIPLE
0.5561
0.0996
0.5333
0.1087
SOPL-KG
0.6174
0.0771
0.5800
0.0935
SOPL-TS
0.5926
0.0845
0.5696
0.0893
SOPL-Greedy
0.5757
0.0941
0.5490
0.1012
Table 3: Average test score after fewer iterations
Policy
τ = 10
τ = 5
Test score
Steps
Test score
Steps
SOPL-KG
0.6060
16.88
0.5711
8.45
SOPL-TS
0.5813
16.10
0.5488
8.20
SOPL-Greedy
0.5653
16.60
0.5389
8.37
Table 4: Average number of realized iterations and average
test score for different policies with early stopping
ment of the KG prompt representation selection policy over
the TS and Greedy policies in the average test score, and the
coefficient of variation CV100 for different tasks. The correla-
tion coefficient between CV100 and the relative improvement
of KG over TS and Greedy across 13 tasks are ρ1 = 0.8901
and ρ2 = 0.7789. The positive correlations suggest that for
tasks where the LLM response is highly sensitive to prompt
features, the KG policy can prominently outperform TS and
Greedy in the SOPL framework.
We also observed that for easier tasks when the LLM re-
sponse is relatively insensitive to the prompts and the perfor-
mance function is relatively flat with respect to prompt fea-
tures, full exploitation policies such as Greedy are adequate
to identify an effective prompt. In particular, we observe that
Greedy outperforms KG by a very small margin for two eas-
ier tasks, antonyms and informal to formal. How-
ever, for challenging tasks with high uncertainty, KG consis-
tently yields over a 10% improvement relative to both base-
lines when other parts of the framework remain the same. Our
analysis reveals the importance of the choice of the policy de-
pending on the problem context.
9


--- Page 10 ---
Figure 6: Upper plot: Improvement over SOPL-TS versus
the coefficient of variation. Lower plot: Improvement over
SOPL-Greedy versus the coefficient of variation
6.3
Sensitivity to Feature Selection
We assess the effectiveness of enriching the prompt rep-
resentation by multiple features on the two tasks with the
largest improvement of SOPL-KG compared to the second
best performance, i.e., orthography starts with and
rhymes. We use the default meta prompt from [15], which
only includes the feature for the required demonstration ex-
amples, and use SOPL-KG to select from 20 predefined con-
figurations of demonstration examples. We allow N = 30
evaluations and repeat the experiments for 20 replications.
Figure 7 shows that the performance deteriorates as the fea-
tures that enhance the meta prompt are excluded.
While
searching in a single dimension is easier than optimizing mul-
tiple features simultaneously, it results in finding suboptimal
prompts. The result suggests that enriching the meta prompt
with different features effectively expands the search space
and leads to improved performance of the generated prompts.
Figure 7: Comparison of the average test score between our
proposed method using all five features and the method using
only one feature for the demonstrative examples
7
Conclusion and Future Work
This paper introduces SOPL, a sequential optimal prompt
learning framework for automated prompt engineering fo-
cused on efficient prompt learning in practical scenarios
where exhaustive evaluation is costly or impossible. Specifi-
cally, we develop a feature-based approach to model prompts,
enabling a constraint-based and expansive prompt search
space. The forward-looking KG policy with correlated be-
liefs facilitates efficient and scalable prompt learning. We
demonstrate that our proposed method achieves superior per-
formance on instruction induction tasks with only 30 or fewer
opportunities of prompt evaluation. We find that the KG pol-
icy yields substantial performance gains compared to base-
line policies, especially for challenging tasks with high uncer-
tainty. Moreover, our framework allows for future investiga-
tion of continuous representations of prompts by embedding
vectors. Our work shows a promising direction of leverag-
ing optimal learning methods for efficient prompt learning,
paving the way for future research on scalable prompt engi-
neering.
Acknowledgment
This research was supported in part through the computa-
tional resources and staff contributions provided for the Quest
high performance computing facility at Northwestern Univer-
sity which is jointly supported by the Office of the Provost,
the Office for Research, and Northwestern University Infor-
mation Technology.
10


--- Page 11 ---
References
[1] Gerlof Bouma. Normalized (pointwise) mutual infor-
mation in collocation extraction. Proceedings of GSCL,
30:31–40, 2009.
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell,
Sandhini
Agarwal,
Ariel
Herbert-Voss,
Gretchen Krueger,
Tom Henighan,
Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. Language models
are few-shot learners. In H. Larochelle, M. Ranzato,
R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances
in Neural Information Processing Systems, volume 33,
pages 1877–1901, 2020.
[3] Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng
Huang, and Tianyi Zhou. Instructzero: Efficient instruc-
tion optimization for black-box large language mod-
els. In International Conference on Machine Learning,
2024.
[4] Stephen E Chick. Bayesian ideas and discrete event sim-
ulation: why, what and how. In Proceedings of the 2006
Winter Simulation Conference, pages 96–106, 2006.
[5] Stephen E Chick,
J¨urgen Branke,
and Christian
Schmidt. Sequential sampling to myopically maximize
the expected value of information. INFORMS Journal
on Computing, 22(1):71–80, 2010.
[6] Boris Defourny, Ilya O Ryzhov, and Warren B Pow-
ell. Optimal information blending with measurements
in the l 2 sphere. Mathematics of Operations Research,
40(4):1060–1088, 2015.
[7] Yihe Deng, Weitong Zhang, Zixiang Chen, and Quan-
quan Gu. Rephrase and respond: Let large language
models ask better questions for themselves.
arXiv
preprint arXiv:2311.04205, 2023.
[8] Chrisantha Fernando, Dylan Sunil Banarse, Henryk
Michalewski, Simon Osindero, and Tim Rockt¨aschel.
Promptbreeder: Self-referential self-improvement via
prompt evolution. In International Conference on Ma-
chine Learning, 2024.
[9] Peter Frazier, Warren Powell, and Savas Dayanik. The
knowledge-gradient policy for correlated normal be-
liefs. INFORMS journal on Computing, 21(4):599–613,
2009.
[10] Peter I Frazier and Warren B Powell. Consistency of
sequential bayesian sampling policies. SIAM Journal
on Control and Optimization, 49(2):712–731, 2011.
[11] Peter I Frazier, Warren B Powell, and Savas Dayanik.
A knowledge-gradient policy for sequential information
collection. SIAM Journal on Control and Optimization,
47(5):2410–2439, 2008.
[12] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao
Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu
Yang. Connecting large language models with evolu-
tionary algorithms yields powerful prompt optimizers.
In International Conference on Learning Representa-
tions, 2024.
[13] Shanti S Gupta and Klaus J Miescke. Bayesian look
ahead one-stage sampling allocations for selection of
the best population. Journal of statistical planning and
inference, 54(2):229–244, 1996.
[14] Bin Han, Ilya O Ryzhov, and Boris Defourny.
Op-
timal learning in linear regression with combinatorial
feature selection.
INFORMS Journal on Computing,
28(4):721–735, 2016.
[15] Or Honovich, Uri Shaham, Samuel R Bowman, and
Omer Levy. Instruction induction: From few examples
to natural language task descriptions.
arXiv preprint
arXiv:2205.10782, 2022.
[16] Can
Jin,
Hongwu
Peng,
Shiyu
Zhao,
Zhenting
Wang, Wujiang Xu, Ligong Han, Jiahui Zhao, Kai
Zhong,
Sanguthevar Rajasekaran,
and Dimitris N
Metaxas.
Apeer: Automatic prompt engineering en-
hances large language model reranking. arXiv preprint
arXiv:2406.14449, 2024.
[17] Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li,
Yong Qin, Ruiqi Sun, Xin Zhou, Enzhi Wang, and Xi-
aohang Dong.
Better zero-shot reasoning with role-
play prompting.
In Proceedings of the 2024 Confer-
ence of the North American Chapter of the Association
for Computational Linguistics: Human Language Tech-
nologies (Volume 1: Long Papers), pages 4099–4113,
2024.
[18] Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu,
Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang,
and Xing Xie. Large language models understand and
can be enhanced by emotional stimuli. arXiv preprint
arXiv:2307.11760, 2023.
[19] Xiaoqiang Lin,
Zhaoxuan Wu,
Zhongxiang Dai,
Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jaillet,
and Bryan Kian Hsiang Low. Use your INSTINCT: IN-
STruction optimization for LLMs using neural bandits
11


--- Page 12 ---
coupled with transformers. In International Conference
on Machine Learning, 2024.
[20] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig.
Pre-train,
prompt, and predict: A systematic survey of prompting
methods in natural language processing. ACM Comput-
ing Surveys, 55(9):1–35, 2023.
[21] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,
and Pontus Stenetorp.
Fantastically ordered prompts
and where to find them: Overcoming few-shot prompt
order sensitivity.
In Proceedings of the 60th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 8086–8098, 2022.
[22] Yifan Luo, Yiming Tang, Chengfeng Shen, Zhen-
nan Zhou,
and Bin Dong.
Prompt engineering
through the lens of optimal control.
arXiv preprint
arXiv:2310.14201, 2023.
[23] Deng Mingkai and Wang Jianyu. Rlprompt: Optimiz-
ing discrete text prompts with reinforcement learning.
In Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing, 2022.
[24] Somayeh Moazeni, Boris Defourny, and Monika J
Wilczak.
Sequential learning in designing marketing
campaigns for market entry.
Management Science,
66(9):4226–4245, 2020.
[25] Warren B Powell. Reinforcement Learning and Stochas-
tic Optimization: A Unified Framework for Sequential
Decisions. John Wiley & Sons, 2022.
[26] Warren B Powell and Ilya O Ryzhov. Optimal learning,
volume 841. John Wiley & Sons, 2012.
[27] Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit
Bansal.
Grips: Gradient-free, edit-based instruction
search for prompting large language models.
arXiv
preprint arXiv:2203.07281, 2022.
[28] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-
guang Zhu, and Michael Zeng. Automatic prompt op-
timization with ”gradient descent” and beam search. In
The 2023 Conference on Empirical Methods in Natural
Language Processing, 2023.
[29] Martin L Puterman. Markov decision processes: dis-
crete stochastic dynamic programming. John Wiley &
Sons, 2014.
[30] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. Language mod-
els are unsupervised multitask learners. OpenAI Blog,
2019.
[31] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha,
Vinija Jain, Samrat Mondal, and Aman Chadha. A sys-
tematic survey of prompt engineering in large language
models: Techniques and applications. arXiv preprint
arXiv:2402.07927, 2024.
[32] Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane
Suhr. Quantifying language models’ sensitivity to spu-
rious features in prompt design or: How i learned to
start worrying about prompt formatting.
In Interna-
tional Conference on Learning Representations, 2024.
[33] Chengshuai Shi, Kun Yang, Jing Yang, and Cong Shen.
Best arm identification for prompt learning under a lim-
ited budget. arXiv preprint arXiv:2402.09723, 2024.
[34] Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtz-
man, Yulia Tsvetkov, and Luke Zettlemoyer. Toward
human readable prompt tuning: Kubrick’s the shining
is a good movie, and a good prompt too? In The 2023
Conference on Empirical Methods in Natural Language
Processing, 2023.
[35] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric
Wallace, and Sameer Singh.
Autoprompt: Eliciting
knowledge from language models with automatically
generated prompts. arXiv preprint arXiv:2010.15980,
2020.
[36] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Gold-
blum, Jonas Geiping, and Tom Goldstein. Hard prompts
made easy: Gradient-based discrete optimization for
prompt tuning and discovery. Advances in Neural In-
formation Processing Systems, 36, 2024.
[37] Ning Wu, Ming Gong, Linjun Shou, Shining Liang, and
Daxin Jiang. Large language models are diverse role-
players for summarization evaluation. In International
Conference on Natural Language Processing and Chi-
nese Computing, pages 695–707, 2023.
[38] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. Calibrate before use: Improving few-
shot performance of language models. In International
Conference on Machine Learning, pages 12697–12706,
2021.
[39] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,
Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba.
Large language models are human-level prompt engi-
neers. In International Conference on Learning Repre-
sentations, 2023.
12
