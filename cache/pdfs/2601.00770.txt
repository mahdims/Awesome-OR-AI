--- Page 1 ---
LLM Agents for Combinatorial Efficient Frontiers:
Investment Portfolio Optimization
Simon Paquette-Greenbaum
Jiangbo Yu∗
Department of Civil Engineering, McGill University
Montreal, Quebec, Canada
Abstract
Investment portfolio optimization is a task conducted in all major financial institu-
tions. The Cardinality Constrained Mean-Variance Portfolio Optimization (CCPO)
problem formulation is ubiquitous for portfolio optimization. The challenge of this
type of portfolio optimization, a mixed-integer quadratic programming (MIQP)
problem, arises from the intractability of solutions from exact solvers, where heuris-
tic algorithms are used to find approximate portfolio solutions. CCPO entails many
laborious and complex workflows and also requires extensive effort pertaining
to heuristic algorithm development, where the combination of pooled heuristic
solutions results in improved efficient frontiers. Hence, common approaches are to
develop many heuristic algorithms. Agentic frameworks emerge as a promising
candidate for many problems within combinatorial optimization, as they have been
shown to be equally efficient with regard to automating large workflows and have
been shown to be excellent in terms of algorithm development, sometimes surpass-
ing human-level performance. This study implements a novel agentic framework
for the CCPO and explores several concrete architectures. In benchmark problems,
the implemented agentic framework matches state-of-the-art algorithms. Further-
more, complex workflows and algorithm development efforts are alleviated, while
in the worst case, lower but acceptable error is reported.
1
Introduction
1.1
Context
Agentic Large Language Models (LLM) are emerging as critical elements in automating large work-
flows and decision support systems in many fields, such as logistics [Xu et al., 2021], management
[Chen et al., 2025a], healthcare [Liu et al., 2025], urban planning [Yu and Hyland, 2025], and
transportation [Yu, 2025]. Hence, LLM agents have consequently been cast as tools for algorithm
development in relevant combinatorial optimization problems such as scheduling [Romera-Paredes
et al., 2024]. They have been studied extensively in natural language processing for optimization
[Ramamonjison et al., 2022], where natural-language problem descriptions have been effectively
translated into valuable mathematical formulations. Language model agents are equally proficient
at generating algorithmic solutions to optimization problems when presented with natural-language
descriptions and mathematical formulations [Xiao et al., 2024], and their performance has also been
shown to exceed that of human experts in time-constrained scenarios [Sun et al., 2025].
Researchers and industry alike have been very interested in these frameworks, as they serve as
valuable benchmarks for some of the foremost applications of LLMs. These applications stimulate
two crucial facets of LLM performance: Natural-Language Processing (NLP) and coding [Xi et al.,
2025]. However, application of language model agents to combinatorial optimization problems has
∗Corresponding author: jiangbo.yu@mcgill.ca
Preprint.
arXiv:2601.00770v1  [cs.CE]  2 Jan 2026


--- Page 2 ---
been limited to academic cases, where problem framing and descriptions are taken directly or inspired
by textbooks used in human education [Mostajabdaveh et al., 2025]. Although these studies present
structured benchmarks for LLM agents, they are limited to problems with single objectives and are
tractable to exact solution approaches.
Limited work has been conducted on combinatorial optimization that reflects real life. Problems
that reflect real life are rarely transcribed into textbooks and can seldom be solved exactly, either
due to computational resource constraints or problem uncertainty. Furthermore, real-life problems
rarely come without trade-offs [Deb, 2014], and real-life decision makers often require knowledge of
those trade-offs’ implications, i.e., the Pareto fronts resulting from multi-objective optimization tasks.
Studies have applied LLM agents to develop heuristic algorithms for problems intractable to exact
solvers [˙Ibrahim O˘guz Çetinkaya et al., 2026]. Still, limited work has been conducted on LLM agent
solutions for multi-objective optimization problems.
1.2
Rationale
This study’s focus is on the development of a language model framework capable of generating
algorithm solutions to combinatorial optimization problems, reflecting real-life worst-case scenarios
(opposite to several benchmark studies, where problem framing is a best-case scenario). Combi-
natorial optimization problems are frequently intractable due to their NP-hardness and due to the
combinatorial explosion of subsets [Hoffman, 2000], rendering exact solutions exploring all subsets
intractable. Furthermore, real-life problems are often multi-objective, where decision makers are
faced with competing business needs, etc.
An agentic framework capable of heuristic algorithm development presents itself as a valuable tool
in such instances. Problems intractable to exact solvers often require approximate solutions from
metaheuristics [Peres and Castelli, 2021]. Furthermore, multi-objective optimization problems have
been shown to sometimes greatly benefit from the pooling of heuristic solutions, where pooled
heuristic solutions form non-dominated frontiers with greater convergence and coverage [Calderín
et al., 2015]. For example, the Cardinality-Constrained Mean-Variance Portfolio Optimization
(CCPO) [Chang et al., 2000] fits this description very well. Unlike standard Markowitz mean-
variance portfolio optimization [Markowitz, 1952], which can be solved trivially with exact dynamic
programming, CCPO is an NP-hard problem [Moral-Escudero et al., 2006] with a non-convex and
discontinuous efficient frontier. The subject of metaheuristic CCPO solutions has been studied
extensively in the literature [Kalayci et al., 2019], and the performance of pooled heuristics in this
case has been shown to greatly improve performance beyond singular heuristics [Woodside-Oriakhi
et al., 2011].
1.3
Contributions
Following this rationale, this study contributes several findings:
• This study presents an agentic language model framework that serves in the construction
of algorithm portfolios for multi-objective combinatorial optimization problems with NP-
hardness. The agent framework not only trivializes the immense developmental burden
associated with the construction of algorithm portfolios but also has the added benefit of the
potential discovery of novel algorithms.
• This study validates the agentic framework, along with its produced algorithm portfolio, in a
series of challenging multi-objective investment portfolio optimization benchmark problems
studied extensively throughout the literature. The agent framework, measured against CCPO
cases taken from OR-Library [Beasley, 1990], is shown to produce algorithms on par with
the state of the art. Furthermore, the pooling of algorithms from its derived algorithm
portfolio is shown to greatly enhance solution performance.
2
Preliminary
2.1
LLM Agents
Coding agents have received a lot of attention within the broader computer science and machine
learning communities. They have been shown to be extremely promising in terms of coding ability
2


--- Page 3 ---
and simplifying complex workflows. They have also received considerable attention in the domain of
combinatorial optimization, given the large algorithm design burden in terms of required effort in
solving intractable or NP-hard problems [Chen et al., 2025b]. Several coding agent frameworks have
been employed successfully in solving combinatorial optimization problems, namely Self-Refine
[Madaan et al., 2023], FunSearch [Romera-Paredes et al., 2024], and ReEvo [Ye et al., 2024].
2.1.1
Agent Framework
The coding agent frameworks employed in this study serve as the central element in the development
of its algorithm portfolio. Given the successful usage of iterative agents in algorithm generation
and discovery, this study will base its framework around their architectures. One-time generation
using LLMs has been shown to most often lead to algorithms that frequently exhibit suboptimal
performance or runtime/execution errors [Madaan et al., 2023]. Hence, agent frameworks like the
one used in this study rely on iterative refinement with external environments, formally referred to as
reasoning-action iterations [Yao et al., 2023].
The language model, herein referred to as M, will be framed in this study as a coding agent.
Equation 1 describes the overview of one generation instance of algorithm A via coding agent
M, where p is the vector of engineered prompt templates, and a, f, and s are the algorithm(s),
feedback(s), and the score(s) from the previous iteration(s).
M(p, a, f, s) 7→A
(1)
This study’s agent framework is based on a greedy refinement agent framework as described in [Sun
et al., 2025], which itself is based on Self-Refine [Madaan et al., 2023], where greedy refinements
are iteratively used to generate algorithms. Equation 2 describes the reasoning-action iterations of
this study’s agent framework. Here, reasoning requires the vector of engineered prompt templates
p = {p, pPF, pRA, pI/O}, including p for general iteration instructions, pPF for the problem formulation,
pRA for role assignment, and pI/O for formatting instructions. Furthermore, a∗
t , f ∗
t , and s∗
t are
respectively the algorithm, feedback, and scores of the best-scoring previous iteration in ASCII
format at an iteration t, injected in the prompt templates where appropriate. Action requires problem
parameters and inputs x (constant over iterations), as well as the generated algorithm A.
At ∼M(p, a∗
t−1, f ∗
t−1, s∗
t−1)
&
{ft, st} ∼At(x)
(2)
2.1.2
Scoring
In this study, algorithm solutions are scored externally to the language model agent M, eliminating
the possibility of hallucinated and biased self-assessments. Generated algorithm A is tasked with
producing metaheuristic solutions to a multi-objective combinatorial optimization problem. Hence,
the action portion of reasoning-action iterations requires the implementation of metrics beyond simple
objective fitness scores. Here, the decision makers studying multi-objective optimization problems
rely on information pertaining to the efficient frontier of solutions, i.e., the set of non-dominated
solutions nearest optimality. For example, in investment portfolio optimization, no rational decision
maker would select a portfolio of assets over another with greater risk and for the same return.
Inversely, they would neither select a portfolio over another with a lesser return with the same risk.
This study defines the efficient frontier, or more formally, the Pareto front, as the subset H ⊆Y of
strictly non-dominated solutions, taken from the set Y of all feasible solutions. Equation 3 defines
how a point y′ ∈Y strictly dominates another point y ∈Y across objective dimensions n, and
Equation 4 defines the subset H of strictly non-dominated solutions.
y′ ≺y ⇐⇒y′
i < yi
∀i ∈{1, . . . , m}
(3)
H = {y ∈Y | ∀y′ ∈Y, y′ ̸≺y}
(4)
Several performance metrics can be used to quantify the performance of the generated solutions
contained in the set H [Kalayci et al., 2019]. Convergence and coverage/diversity are the main metrics
through which efficient frontier performance is measured. Convergence represents the closeness
of the approximate frontier to that of a theoretical Pareto optimal frontier. Coverage represents the
uniformity of distribution of approximate solutions obtained along the efficient frontier. Furthermore,
domain-specific metrics can be used to measure Pareto front performance. For example, Percentage-
deviation Error (PE) has been used commonly in benchmarking CCPO convergence performance
3


--- Page 4 ---
[Chang et al., 2000]. However, this study relies on hybrid metrics that measure both convergence
and coverage. The hypervolume indicator [Guerreiro et al., 2022] and Inverted Generation Distance
(IGD) [Ishibuchi et al., 2015] are commonly used to that effect. In this study, IGD is relied upon as
the standard Markowitz portfolio optimization provides an Unconstrained Efficient Frontier (UEF) as
a reference for scoring. Equation 5 describes the computation of the IGD metric, where P, taken
as the UEF in this study, represents the set of theoretical optimal Pareto solutions y∗, H is the set
of non-dominated approximate Pareto solutions y, and ∥y∗−y∥is the Euclidean distance between
approximate solution y and nearest optimal solution y∗.
IGD(P, H) =
1
card(P)
X
y∗∈P
min
y∈H ∥y∗−y∥
(5)
2.2
Problem Formulation
This study employs the CCPO problem formulation described in [Chang et al., 2000] as it has been
studied extensively in the literature, where state-of-the-art algorithm performance can be mapped as a
reference. The CCPO is a constrained version of the standard Markowitz mean-variance portfolio
optimization. Like the standard Markowitz approach, variance is assumed to be an adequate measure
of the risk associated with the investment portfolio.
2.2.1
Objective
CCPO can be cast as a multi-objective optimization problem, where the objectives are to minimize
portfolio risk and maximize portfolio return (equivalently, minimize the negative of return) across
potential portfolios composed from a universe of N assets. Like the standard Markowitz approach,
Equation 6 defines risk and is taken as the variance of portfolio return, where wi is the proportion held
of an asset i (∀i ∈{1, . . . , N}) and σij is the covariance between assets i and j (∀i ∈{1, . . . , N}
and ∀j ∈{1, . . . , N}). Return, described in Equation 7, is simply the weighted sum of expected
return µi of assets i (∀i ∈{1, . . . , N}).
min
N
X
i=1
N
X
j=1
wiwjσij
(6)
min
−
N
X
i=1
wiµi
(7)
Given the computational complexity of the aforementioned CCPO problem, a single objective
formulation, as opposed to a multi-objective formulation, is desirable, as it permits the use of single
objective metaheuristics, where multi-objective metaheuristics can add immense computational
burden to an already challenging problem. Hence, techniques are used to convert multi-objectives
into single objectives. Here, the Weighted Sum (WS) method [Bazgan et al., 2022] for objective
linear scalarization is taken in favor of the ε-constraint objective formulation [Mavrotas, 2009], for
the same reason, the computational complexity of the aforementioned CCPO problem. Unlike the
ε-constraint objective formulation, non-convex regions of the efficient frontier are not covered with
the WS objective formulation, where, given its linear nature, only solution points on the convex hull
of the objective set can be found. However, decision makers concerned with CCPO efficient frontiers
do not typically concern themselves with the exact frontier shape but are satisfied with the trade-off
information it provides. In this study, the objective makes use of WS for computational efficiency,
where non-convex regions are not considered. Optimization problems requiring complete frontier
diversity and coverage should instead opt for ε-constraint objective formulation.
In this instance, there is no known a priori preference between risk and return objectives. Here, the
interest lies in the efficient frontier of investment portfolio solutions. It is necessary to sweep the
linear weights of the WS method to find an efficient frontier, as it is unreasonable to assume that
exploratory capacity from metaheuristics alone will cover the efficient frontier without guidance
towards specific objective trade-off ratios. In the general case for n objectives, the weight vector λ
can be found by exploring a unit simplex [Boyd and Vandenberghe, 2004] of weights. Exploring
4


--- Page 5 ---
trade-off ratios is then simply a matter of sweeping the weight simplex Λn−1 defined in Equation 8.
Λn−1 =
(
λ ∈Rn
 λi ≥0,
n
X
i=1
λi = 1
)
(8)
Using the risk and return objectives (n = 2) defined in Equation 6 and Equation 7, a single objective
can be defined formally, where λ = {λ1, λ2}. Furthermore, the objective can be simplified given the
linear equality constraint of the unit simplex, where the weight vector becomes λ = {λ, 1 −λ} as
described in Equation 9. Exploring trade-off ratios between risk and return becomes simply sweeping
between λ ∈[0, 1], where λ = 0 represents the scenario where return is maximized irrespective of
risk. Conversely, λ = 1 represents the scenario where risk is minimized irrespective of return.
min
λ


N
X
i=1
N
X
j=1
wiwjσij

−(1 −λ)
" N
X
i=1
wiµi
#
(9)
2.2.2
Constraints
This formulation includes cardinality constraints for the number of assets included in the portfolio,
where the number of selected assets is fixed to K as described in Equation 11. Accordingly, zero-one
decision z variables are introduced for assets as described in Equation 13. Boundary constraints
are included for the minimum (buy-in threshold) and maximum proportions of assets held, if they
are held, where ε and δ are respectively the minimum and maximum proportions as described in
Equation 12. Like standard Markowitz portfolio optimization, budget constraints are also enforced as
described in Equation 10. Furthermore, transaction costs and round lot constraints are not considered
in order to benefit from the plethora of studies relying on the benchmark problems of CCPO.
N
X
i=1
wi = 1
(10)
N
X
i=1
zi = K
(11)
εizi ≤wi ≤δizi,
∀i ∈{1, . . . , N}
(12)
zi ∈{0, 1},
∀i ∈{1, . . . , N}
(13)
3
Methods
3.1
Approach
In this study, an LLM is framed algorithmically as a coding agent, where the Multi-objective
Combinatorial-optimization Agent (MOCO–AGENT) algorithm, designed in this study, employs
reasoning-action iterations [Yao et al., 2023] in the generation of Python metaheuristic algorithms.
Figure 1 presents the employed MOCO–AGENT algorithm in its generalized form, abstracted for any
multi-objective optimization problem. The algorithm employs external functions and templates in
the generation of prompts that serve as inputs to an internal language model. The MOCO–AGENT
defined in this study is designed to operate greedily in its reasoning-action iterations, where the
performance, represented by feedback f ∗
t and score s∗
t , of the best-scoring generated Python algorithm
A∗
t , is injected into model prompt inputs at every iteration beyond instantiation. In this instance, a
greedy approach to reasoning-action iterations is elected, as its simplicity and low token usage have
been shown to outperform more complex agentic frameworks [Sun et al., 2025] when employed in
algorithm generation tasks for combinatorial optimization.
An initial algorithm A0 is generated at the initial iteration t = 0, where model prompts are free from
previous algorithm a, feedback f, and scoring s information. However, engineered prompts for the
CCPO problem formulation pPF, role assignment pRA, and Python algorithm formatting instructions
pI/O are still included. CCPO objective and constraints are written in ASCII format in pPF. The
MOCO–AGENT’s role as a coding agent is defined in pRA, including the instructions pertaining to
the type of metaheuristic algorithm to be developed, as well as instructions pertaining to algorithm
5


--- Page 6 ---
hybridization. Formatting instructions are defined in pI/O, where strict constraints are imposed on
Python code generation such that generated code adheres to external MOCO–AGENT algorithm
functionalities, such as calling on libraries for loading asset universes.
Algorithm 1 MOCO–AGENT
Require: input x, model M, prompts p, optimal front P, iterations T, dimensions n, step size ∆λ
1: A0 = M(p, ·)
▷Instantiate algorithm (Equation 1)
2: for all objective vectors λ ∈{Λn−1 discretized by ∆λ} do
▷Simplex (Equation 8)
3:
Yλ = A0(x, λ)
4: end for
5: Y0 = AGGREGATE(Yλ∈Λ)
6: if FEASIBLE(Y0) then
▷Feasibility (e.g., Equation 10-13)
7:
H0 ⊆Y0
▷Non-dominated set (Equation 3-4)
8:
{f0, s0} = IGD(P, H0)
▷IGD metric (Equation 5)
9: else
10:
{f0, s0} = ∞
11: end if
12: {a∗
0, f ∗
0 , s∗
0} = TEXTIFY(A0, f0, s0)
▷Initial "best" iteration
13: for iterations t ∈1, ..., T do
14:
At = M(p, a∗
t−1, f ∗
t−1, s∗
t−1)
▷Refine algorithm (Equation 2)
15:
for all objective vectors λ ∈{Λn−1 discretized by ∆λ} do
▷Simplex (Equation 8)
16:
Yλ = At(x, λ)
17:
end for
18:
Yt = AGGREGATE(Yλ∈Λ)
19:
if FEASIBLE(Yt) then
▷Feasibility (e.g., Equation 10-13)
20:
Ht ⊆Yt
▷Non-dominated set (Equation 3-4)
21:
{ft, st} = IGD(P, Ht)
▷IGD metric (Equation 5)
22:
else
23:
{ft, st} = ∞
24:
end if
25:
if st ≤s∗
t−1 then
▷Current best iteration
26:
{a∗
t , f ∗
t , s∗
t } = TEXTIFY(At, ft, st)
27:
else
28:
{a∗
t , f ∗
t , s∗
t } = TEXTIFY(a∗
t−1, f ∗
t−1, s∗
t−1)
29:
end if
30: end for
31: return a∗
T
Figure 1: Agent framework algorithm.
Similarly, regular reasoning-action MOCO–AGENT iterations, from the first regular iteration t = 1
to the final iteration t = T, employ the pPF, pRA, and pI/O engineered prompts, but incorporate
information pertaining to best-scoring previous algorithm a∗
t , feedback f ∗
t , and scoring s∗
t using
the prompt template p for general iterating instructions. Here, a∗
t is the "textified" version of the
best-scoring Python algorithm. Furthermore, its corresponding feedback f ∗
t contains the feasibility
and number of non-dominated CCPO solutions, as well as any execution errors. The IGD score s∗
t is
equally included. The resulting engineered prompts are inputs to the language model M to generate
a new algorithm At (Equation 2).
Approximate solutions to the CCPO can then be generated using the Python metaheuristic algorithm
At across the weight simplex (Equation 8) using problem-specific inputs x, e.g., the cardinality
constraint parameter K and the asset universe in the CCPO. For simplicity, the unit weight simplex
6


--- Page 7 ---
Λn−1 is discretized uniformly by the step size ∆λ across all n-objective directions. Hence, in n = 2
dimensions, the resulting weight vector λ = {λ, 1 −λ} is swept between λ ∈[0, 1]. Approximate
solutions of the CCPO are aggregated across all sampled weight vectors λ into a solution set Yt.
The subset Ht ⊆Yt of non-dominated solutions can be extracted (Equation 3-4), after checking
solution feasibility with respect to CCPO constraints (Equation 10-13). Finally, the efficient frontier
performance score st can be measured with the IGD metric (Equation 5) using the standard Markowitz
portfolio optimization UEF as a theoretically optimal reference. Here, a∗
t , f ∗
t , and s∗
t are updated
if the current iteration’s algorithm At score st is found to outperform the historical best s∗
t . In the
general case where reference frontiers are not easily attainable, the hypervolume indicator [Guerreiro
et al., 2022] can be used as a suitable alternative metric.
3.2
Study Design
The OpenAI o4-mini LLM is cast as a coding agent in this study using the MOCO–AGENT framework.
This study employs the MOCO–AGENT in the generation of an algorithm portfolio for the CCPO.
Here, multiple algorithms are generated by modifying the role assignment prompt pRA across this
study’s multiple instances of the MOCO–AGENT algorithm. However, given the innumerable quantity
of hitherto existing metaheuristic algorithms [Hussain et al., 2019], as well as the criticisms that
recent metaphor-based metaheuristics have faced [Sörensen, 2015], this study limits itself to the
generation of a small number of algorithms based on classical and early metaheuristics methods.
This study explores the generation of 10 algorithms, each based on a unique metaheuristic method.
Evolutionary-based methods such as Genetic Algorithms (GA) [Holland, 1992], Population-Based
Incremental Learning (PBIL) [Baluja, 1994], and Differential Evolution (DE) [Storn and Price,
1997] are included. Early nature-inspired methods such as Artificial Bee Colony (ABC) [Karaboga,
2005] and Firefly Algorithm (FA) [Yang, 2009] are included. Swarm-based algorithms such as
Particle Swarm Optimization (PSO) [Kennedy and Eberhart, 1995] are included. Finally, trajectory-
based methods such as Hill Climbing (HC) [Minsky, 1961], Simulated Annealing (SA) [Kirkpatrick
et al., 1983], Tabu Search (TS) [Glover, 1989], and Greedy Randomized Adaptive Search Procedure
(GRASP) [Feo and Resende, 1995] are included. Furthermore, beyond specifying the metaheuristic
method to be adapted by the MOCO–AGENT in its reasoning step, the role assignment prompt pRA
equally indicates that the coding agent should aim to hybridize the generated algorithm however it
sees fit, but should not employ exact solution methods.
In the action step of the MOCO–AGENT, the performance of the generated algorithms is measured
on an asset universe taken from OR-Library [Beasley, 1990]. This study utilizes the same Hang Seng,
DAX, FTSE, S&P, and Nikkei asset universes used in foundational works for the CCPO [Chang
et al., 2000]. Hence, for ease of comparison with the swath of following literature on CCPO, the
same K = 10, ε = 0.01, and δ = 1 constraint parameters are taken, as well as ∆λ = 0.02 objective
step size. The Hang Seng asset universe data, composed of 31 assets, is the smallest of the series
taken from OR-Library, and is consequently selected as the training set used in algorithm generation.
Each generated algorithm is given T = 32 reasoning-action iterations, where an execution time
limit of 10 minutes is given at each action iteration for each iteration’s generated Python algorithm.
The remaining DAX, FTSE, S&P, and Nikkei asset universes are reserved as testing sets for the
best-scoring algorithms a∗
T generated using the Hang Seng training set.
4
Results
4.1
Generated Algorithms
The performance summary of the 10 metaheuristic algorithms generated with the MOCO–AGENT
framework is presented in Table 1, when measured on the Hang Seng training set. The generated
algorithms are scored on two performance metrics. The first metric, being the Mean Percentage-
deviation Error (MPE) [Chang et al., 2000], is an indicator of efficient frontier convergence with
respect to the UEF, and is measured on the solution set V (λ). This set represents the solutions
associated with the best objective function value for each sampled weight vector λ discretized by
∆λ steps. Hence, the set V (λ) is comprised of 51 investment portfolio solutions as λ ∈[0, 1] is
sampled at every ∆λ = 0.02 step. Furthermore, dominated solutions are not eliminated. The second
performance metric is the IGD [Ishibuchi et al., 2015], which is an indicator of both efficient frontier
convergence and coverage.
7


--- Page 8 ---
Table 1: Metaheuristic algorithm performance on the Hang Seng (N = 31) training set.
ABC
DE
FA
GA
GRASP
HC
PSO
PBIL
SA
TS
MPE
1.0983
1.0965
3.4104
1.9444
1.2319
1.0965
1.1165
4.5653
1.4103
20.4934
Rank
3
2
8
7
5
1
4
9
6
10
IGD
0.53E-4
0.69E-4
1.61E-4
1.17E-4
0.88E-4
1.09E-4
0.70E-4
3.31E-4
1.43E-4
18.10E-4
Rank
1
2
8
6
4
5
3
9
7
10
Score
2
2
8
6.5
4.5
3
3.5
9
6.5
10
Retain
Yes
Yes
No
No
Yes
Yes
Yes
No
No
No
For both performance indicators, the generated algorithms based on the FA, PBIL, and TS methods
performed significantly worse than the others, while GA and SA also lagged behind slightly when
measured on the Hang Seng training set. Hence, the ABC, DE, GRASP, HC, and PSO are retained for
evaluation against the testing set, while the remainder are pruned. The generated algorithm portfolio
is equally comprised of the four present metaheuristic method categories: evolutionary-based, nature-
inspired, swarm-based, and trajectory-based. Furthermore, self-assessed descriptions of the generated
algorithms are shown in Table 2, a byproduct of the engineered prompts, which require the language
model produce a self-assessment of the generated model at each reasoning iteration.
Table 2: Self-assessed descriptions of generated algorithms.
Algorithm
Description
ABC
"Enhanced ABC with biased initialization, adaptive neighbor strategies,
and asset-frequency guidance".
DE
"Hybrid adaptive differential evolution with memetic local search includ-
ing gradient-based weight adjustment and greedy asset swap moves".
GRASP
"Enhanced GRASP with cluster-diverse RCL, adaptive large neighbor-
hood search operators, and operator selection learning".
HC
"Hybrid memetic hill-climbing with adaptive multi-swap moves, greedy
swaps, crossover diversification, and tabu tenure".
PSO
"Hybrid PSO with ring topology, Levy-flight mutation, enhanced DE and
adaptive local swap search".
4.2
Algorithm Portfolio Performance
The generated algorithm portfolio is evaluated on the DAX, FTSE, S&P, and Nikkei asset universes,
which compose this study’s testing sets. These datasets, taken from OR-Library [Beasley, 1990],
serve adequately as test sets as they differ substantially from the Hang Seng training set, both in terms
of asset content and universe size. For example, the Hang Seng universe has N = 31 assets, whereas
the Nikkei universe has N = 225 assets. The performance against the testing set is compared equally
against the State Of The Art (SOTA). Table 3 presents the performance of the generated metaheuristic
algorithms on the training and testing sets. Furthermore, the performance of the SOTA is included as
a reference. Performance is measured using the Percentage-deviation Error (PE) indicator [Chang
et al., 2000] using the asset universes’ respective UEFs as reference theoretical optimal frontiers,
where mean, median, min, and max PE for sets V (λ) are reported across all included algorithms.
The score(s) of the best-performing algorithm(s) are underlined in Table 3, whereas the score(s) of
the best-performing algorithm(s) generated via the MOCO–AGENT framework are emboldened.
Metrics for GRASP are left empty for the S&P test set, as the Python algorithm code execution time
limit of 30 minutes was reached before portfolios were found across all sampled ∆λ steps. Table 3
indicates that the generated algorithm portfolio frequently matches SOTA performance across several
performance indicators, and can also infrequently surpass SOTA performance. Here, the SOTA is
taken as the High-Population ABC (ABC-HP) [Cura, 2021] algorithm as measured in [Alcazar et al.,
2024]. However, the generated algorithm portfolio greatly outperforms historical SOTA algorithms
measured on V (λ) using PE indicators such as the GA, TS, and SA from [Chang et al., 2000], PSO
from [Deng et al., 2012], and PBIL-DE from [Lwin and Qu, 2013].
8


--- Page 9 ---
Table 3: Comparison with SOTA for V (λ).
ABC
DE
GRASP
HC
PSO
SOTA†
Hang Seng (N = 31)
Mean
1.0983
1.0965
1.2319
1.0965
1.1165
1.0873
Median
1.2149
1.2155
1.2155
1.2154
1.2211
1.2154
Min
0.0000
0.0000
0.0000
0.0000
0.0224
0.0000
Max
1.5538
1.5538
8.4637
1.5537
1.6241
1.5538
DAX (N = 85)
Mean
2.5920
2.3232
2.3778
2.3398
2.5802
2.2898
Median
2.6050
2.5470
2.5630
2.5630
2.6069
2.5629
Min
0.0059
0.0059
0.0059
0.0059
0.0164
0.0059
Max
10.3078
5.7669
6.7775
4.3210
5.8620
4.0275
FTSE (N = 89)
Mean
1.0042
0.9443
0.9728
0.8799
1.1566
0.8406
Median
1.0841
1.0841
1.0841
1.0841
1.2485
1.0841
Min
0.0045
0.0016
0.0016
0.0327
0.0327
0.0016
Max
4.9484
5.9721
7.5422
2.6819
2.8589
2.0670
S&P (N = 98)
Mean
1.7416
1.8478
–
1.4351
1.6552
1.3464
Median
1.1810
1.2144
–
1.1420
1.1755
1.1515
Min
0.0046
0.2655
–
0.0009
0.0061
0.0009
Max
9.8889
8.9263
–
7.7908
10.5647
5.4520
Nikkei (N = 225)
Mean
0.6787
2.7516
0.7184
0.5782
1.2403
0.5665
Median
0.6229
1.8011
0.6118
0.5858
1.1422
0.5858
Min
0.0000
0.5863
0.0238
0.0003
0.1675
0.0000
Max
2.7825
7.9270
5.0087
1.1606
3.1126
1.1606
† ABC-HP algorithm [Cura, 2021] measured as the best performing in the SOTA [Alcazar et al., 2024].
4.3
Pooled Metaheuristics
Beyond individual algorithm performance, the solutions of the generated algorithm portfolio can
be pooled to improve efficient frontier performance. Figure 2 illustrates the efficient V (λ) frontier
of the pooled investment portfolio solutions from the generated metaheuristic algorithms across the
Hang Seng, DAX, FTSE, S&P, and Nikkei asset universes taken from OR-Library [Beasley, 1990].
The black lines in Figure 2 represent the datasets’ respective UEFs. The best algorithm portfolio
solution is taken for each sampled λ step. However, dominated solutions across different sampled
λ values are not eliminated. Figure 2, along with Table 4, indicates the variety across the extent to
which generated metaheuristic algorithms contribute to the efficient frontiers, where algorithms differ
significantly in terms of their respective contributions across asset universes. For example, the DE
and PSO algorithms contribute greatly to the Hang Seng frontier, whereas the HC algorithm makes
no contribution. Conversely, the HC algorithm contributes greatly to the Nikkei frontier, whereas
the DE and PSO algorithms make no contribution. Notably, the GRASP algorithm makes important
contributions to the frontiers of each evaluated asset universe.
Table 4: Metaheuristic algorithm contributions to pooled V (λ) frontiers.
ABC
DE
GRASP
HC
PSO
Hang Seng (N = 31)
–
19/51
8/51
–
24/51
DAX (N = 85)
–
34/51
10/51
1/51
6/51
FTSE (N = 89)
3/51
27/51
19/51
1/51
1/51
S&P (N = 98)
–
3/51
23/51
16/51
9/51
Nikkei (N = 225)
1/51
–
17/51
33/51
–
The contribution of individual algorithms within the generated algorithm portfolio can also be
measured by their contributions towards improving the IGD indicator. Table 5 indicates the relative
improvement of the pooled IGD when specific algorithm solutions are incorporated. For example,
the pooled IGD indicator is improved by 48.52% on the Hang Seng asset universe when the ABC
solutions are added to the pooled DE, GRASP, HC, and PSO solutions. Table 5 also indicates the
9


--- Page 10 ---
0
1
2
3
4
5
Variance
×10
3
0.2
0.4
0.6
0.8
1.0
1.2
Return
×10
2
Hang Seng (N = 31)
DE
GRASP
PSO
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Variance
×10
3
0.2
0.4
0.6
0.8
1.0
Return
×10
2
DAX (N = 85)
DE
GRASP
HC
PSO
0.0
0.5
1.0
1.5
2.0
Variance
×10
3
2
3
4
5
6
7
8
9
Return
×10
3
FTSE (N = 89)
ABC
DE
GRASP
HC
PSO
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Variance
×10
3
0.0
0.2
0.4
0.6
0.8
1.0
Return
×10
2
S&P (N = 98)
DE
GRASP
HC
PSO
0.0
0.5
1.0
1.5
2.0
Variance
×10
3
0
1
2
3
4
Return
×10
3
Nikkei (N = 225)
ABC
GRASP
HC
Figure 2: Pooled metaheuristic V (λ) frontiers.
variety across the extent to which algorithms contribute to the improvement of the IGD indicator.
Unlike the PE metrics, the IGD indicator not only measures convergence but also measures coverage.
For example, the ABC algorithm, which made almost no contribution to the evaluated V (λ) frontiers,
contributed significantly to improving IGD across all evaluated asset universes. This indicates that
while its per λ convergence performance might be relatively poor, the coverage of its set H of
non-dominated solutions is very good. Conversely, algorithms such as DE or GRASP, which made
important contributions to the V (λ) frontiers, had little impact on the IGD metric across several of
the evaluated asset universes. This illustrates the importance of the diversity of algorithms contained
within the generated algorithm portfolio, where different algorithms respectively have varying levels
of contribution to efficient frontier convergence and coverage.
5
Conclusion
In conclusion, this study successfully implements an agentic framework for the generation of algo-
rithm portfolios for multi-objective combinatorial optimization. The framework is applied to the
NP-hard CCPO problem, where a portfolio of metaheuristic algorithms is successfully generated
without extensive human developmental effort. The resulting algorithm portfolio is validated across
benchmark asset universes and compared with the SOTA. The findings show that algorithms generated
10


--- Page 11 ---
Table 5: Metaheuristic algorithm contributions to improving pooled IGD.
ABC
DE
GRASP
HC
PSO
Hang Seng (N = 31)
48.52%
0.12%
0.03%
4.42%
0.04%
DAX (N = 85)
33.21%
10.38%
16.80%
2.09%
35.05%
FTSE (N = 89)
30.32%
11.08%
34.53%
2.16%
3.32%
S&P (N = 98)
61.04%
8.89%
0.58%
118.28%
7.62%
Nikkei (N = 225)
215.08%
0%
5.63%
32.54%
18.22%
from the agentic framework frequently match, and in some cases surpass, the performance of the
SOTA. Furthermore, the study also investigates the impact of pooling solutions from the generated
algorithms, revealing that each algorithm contributes uniquely to different regions of efficient frontier
convergence and coverage. The study is limited to the CCPO problem and to one agent framework
based on greedy refinement. Future research should explore alternative agentic framework designs
and extend the evaluation to other multi-objective combinatorial optimization problems to further
validate performance. Furthermore, future research should consider the interaction between differ-
ent generated algorithms, where the efficiency of efficient frontier generation can be improved via
hyper-heuristics. Nevertheless, the results demonstrate the effectiveness of the proposed agentic
framework in generating algorithms for the CCPO and demonstrate the overall developmental utility
of the implemented agentic framework, where the diversity of derived algorithms in pooling can
improve the global performance of the studied efficient frontiers.
Acknowledgments
This work was supported by the Natural Sciences and Engineering Research Council of Canada
(NSERC) Discovery Grant, and the McGill Engineering Doctoral Award (MEDA). Compute resources
were provided by the Digital Research Alliance of Canada and Calcul Québec.
References
Liming Xu, Stephen Mak, and Alexandra Brintrup. Will bots take over the supply chain? revisiting
agent-based supply chain automation. International Journal of Production Economics, 241:108279,
2021. doi: 10.1016/j.ijpe.2021.108279.
Yang Chen, Samuel N. Kirshner, Anton Ovchinnikov, Meena Andiappan, and Tracy Jenkin. A
manager and an AI walk into a bar: Does chatgpt make biased decisions like we do? Manufacturing
& Service Operations Management, 27(2):354–368, 2025a. doi: 10.1287/msom.2023.0279.
Fei Liu, Yue Niu, Qihua Zhang, Kai Wang, Zheyi Dong, Io Nam Wong, Linling Cheng, Ting Li, Lian
Duan, Kun Li, Gen Li, Tai Wa Hou, Manson Fok, Huiyan Luo, Xiangmei Chen, Kang Zhang, and
Yun Yin. A foundational architecture for ai agents in healthcare. Cell Reports Medicine, 6(10):
102374, 2025. doi: 10.1016/j.xcrm.2025.102374.
Jiangbo Yu and Michael F. Hyland. Interpretable state-space model of urban dynamics for human-
machine collaborative transportation planning. Transportation Research Part B: Methodological,
192:103134, 2025. doi: 10.1016/j.trb.2024.103134.
Jiangbo Yu. Preparing for an agentic era of human-machine transportation systems: Opportunities,
challenges, and policy recommendations. Transport Policy, 171:78–97, 2025. doi: 10.1016/j.
tranpol.2025.05.030.
Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog,
M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming
Wang, Omar Fawzi, Pushmeet Kohli, and Alhussein Fawzi. Mathematical discoveries from
program search with large language models. Nature, 625(7995):468–475, January 2024. doi:
10.1038/s41586-023-06924-6.
11


--- Page 12 ---
Rindranirina Ramamonjison, Timothy Yu, Raymond Li, Haley Li, Giuseppe Carenini, Bissan Ghaddar,
Shiqi He, Mahdi Mostajabdaveh, Amin Banitalebi-Dehkordi, Zirui Zhou, and Yong Zhang. NL4Opt
competition: Formulating optimization problems based on their natural language descriptions. In
Proceedings of the NeurIPS 2022 Competitions Track, volume 220 of Proceedings of Machine
Learning Research, pages 189–203. PMLR, 28 Nov–09 Dec 2022.
Ziyang Xiao, Dongxiang Zhang, Yangjun Wu, Lilin Xu, Yuan Jessica Wang, Xiongwei Han, Xiaojin
Fu, Tao Zhong, Jia Zeng, Mingli Song, and Gang Chen. Chain-of-experts: When LLMs meet
complex operations research problems. In The Twelfth International Conference on Learning
Representations, 2024.
Weiwei Sun, Shengyu Feng, Shanda Li, and Yiming Yang. CO-Bench: Benchmarking Language
Model Agents in Algorithm Search for Combinatorial Optimization. arXiv, 2025. doi: 10.48550/
ARXIV.2504.04310.
Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe
Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents:
A survey. Science China Information Sciences, 68(2):121101, 2025.
Mahdi Mostajabdaveh, Timothy Tin Long Yu, Samarendra Chandan Bindu Dash, Rindra Rama-
monjison, Jabo Serge Byusa, Giuseppe Carenini, Zirui Zhou, and Yong Zhang. Evaluating LLM
reasoning in the operations research domain with orqa. Proceedings of the AAAI Conference on
Artificial Intelligence, 39(23):24902–24910, Apr. 2025. doi: 10.1609/aaai.v39i23.34673.
Kalyanmoy Deb. Multi-objective Optimization, pages 403–449. Springer US, Boston, MA, 2014.
doi: 10.1007/978-1-4614-6940-7_15.
˙Ibrahim O˘guz Çetinkaya, ˙I. Esra Büyüktahtakın, Parshin Shojaee, and Chandan K. Reddy. Discover-
ing heuristics with large language models (LLMs) for mixed-integer programs: Single-machine
scheduling. Computers & Operations Research, 186:107325, 2026. doi: 10.1016/j.cor.2025.
107325.
Karla L. Hoffman.
Combinatorial optimization: Current successes and directions for the fu-
ture.
Journal of Computational and Applied Mathematics, 124(1):341–360, 2000.
doi:
10.1016/S0377-0427(00)00430-1. Numerical Analysis 2000. Vol. IV: Optimization and Non-
linear Equations.
Fernando Peres and Mauro Castelli. Combinatorial optimization problems and metaheuristics: Review,
challenges, design, and development. Applied Sciences, 11(14), 2021. doi: 10.3390/app11146449.
Jenny Fajardo Calderín, Antonio D. Masegosa, and David A. Pelta. Algorithm portfolio based scheme
for dynamic optimization problems. International Journal of Computational Intelligence Systems,
8(4):667, 2015. doi: 10.1080/18756891.2015.1046327.
T.-J. Chang, N. Meade, J.E. Beasley, and Y.M. Sharaiha. Heuristics for cardinality constrained
portfolio optimisation. Computers & Operations Research, 27(13):1271–1302, 2000. doi: 10.
1016/S0305-0548(99)00074-X.
Harry Markowitz. Portfolio selection. The Journal of Finance, 7(1):77–91, 1952.
R. Moral-Escudero, R. Ruiz-Torrubiano, and A. Suarez. Selection of optimal investment portfolios
with cardinality constraints. In 2006 IEEE International Conference on Evolutionary Computation,
pages 2382–2388, 2006. doi: 10.1109/CEC.2006.1688603.
Can B. Kalayci, Okkes Ertenlice, and Mehmet Anil Akbay. A comprehensive review of determin-
istic models and applications for mean-variance portfolio optimization. Expert Systems with
Applications, 125:345–368, 2019. doi: 10.1016/j.eswa.2019.02.011.
M. Woodside-Oriakhi, C. Lucas, and J.E. Beasley. Heuristic algorithms for the cardinality constrained
efficient frontier. European Journal of Operational Research, 213(3):538–550, 2011. doi: 10.
1016/j.ejor.2011.03.030.
J. E. Beasley. OR-Library: Distributing test problems by electronic mail. Journal of the Operational
Research Society, 41(11):1069–1072, 1990. doi: 10.1057/jors.1990.166.
12


--- Page 13 ---
Hongzheng Chen, Yingheng Wang, Yaohui Cai, Hins Hu, Jiajie Li, Shirley Huang, Chenhui Deng,
Rongjian Liang, Shufeng Kong, Haoxing Ren, Samitha Samaranayake, Carla P. Gomes, and
Zhiru Zhang. HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial
Optimization. arXiv, 2025b. doi: 10.48550/ARXIV.2506.07972.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder,
Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-Refine: Iterative
refinement with self-feedback. In Advances in Neural Information Processing Systems, volume 36,
pages 46534–46594. Curran Associates, Inc., 2023.
Haoran Ye, Jiarui Wang, Zhiguang Cao, Federico Berto, Chuanbo Hua, Haeyeon Kim, Jinkyoo Park,
and Guojie Song. ReEvo: Large language models as hyper-heuristics with reflective evolution.
In Advances in Neural Information Processing Systems, volume 37, pages 43571–43608. Curran
Associates, Inc., 2024. doi: 10.52202/079017-1381.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao.
ReAct: Synergizing reasoning and acting in language models. In The Eleventh International
Conference on Learning Representations, 2023.
Andreia P. Guerreiro, Carlos M. Fonseca, and Luís Paquete. The Hypervolume Indicator: Com-
putational Problems and Algorithms. ACM Computing Surveys, 54(6):1–42, July 2022. doi:
10.1145/3453474.
Hisao Ishibuchi, Hiroyuki Masuda, Yuki Tanigaki, and Yusuke Nojima. Modified distance calculation
in generational distance and inverted generational distance. In Evolutionary Multi-Criterion
Optimization, pages 110–125. Springer International Publishing, 2015.
Cristina Bazgan, Stefan Ruzika, Clemens Thielen, and Daniel Vanderpooten. The Power of the
Weighted Sum Scalarization for Approximating Multiobjective Optimization Problems. Theory of
Computing Systems, 66(1):395–415, February 2022. doi: 10.1007/s00224-021-10066-5.
George Mavrotas. Effective implementation of the ε-constraint method in multi-objective mathemati-
cal programming problems. Applied Mathematics and Computation, 213(2):455–465, 2009. doi:
10.1016/j.amc.2009.03.037.
S.P. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
Kashif Hussain, Mohd Najib Mohd Salleh, Shi Cheng, and Yuhui Shi. Metaheuristic research: a
comprehensive survey. Artificial Intelligence Review, 52(4):2191–2233, December 2019. doi:
10.1007/s10462-017-9605-z.
Kenneth Sörensen. Metaheuristics—the metaphor exposed. International Transactions in Operational
Research, 22(1):3–18, 2015. doi: 10.1111/itor.12001.
John H. Holland. Adaptation in Natural and Artificial Systems: An Introductory Analysis with
Applications to Biology, Control and Artificial Intelligence. MIT Press, Cambridge, MA, USA,
1992. ISBN 0262082136.
Shummet Baluja. Population-based incremental learning: A method for integrating genetic search
based function optimization and competitive learning. Technical report, Carnegie Mellon Univer-
sity, USA, 1994.
Rainer Storn and Kenneth Price. Differential Evolution – A Simple and Efficient Heuristic for global
Optimization over Continuous Spaces. Journal of Global Optimization, 11(4):341–359, December
1997. doi: 10.1023/A:1008202821328.
Dervis Karaboga. An idea based on honey bee swarm for numerical optimization, technical report -
tr06. Technical Report, Erciyes University, 01 2005.
Xin-She Yang. Firefly algorithms for multimodal optimization. In Stochastic Algorithms: Foundations
and Applications, pages 169–178, Berlin, Heidelberg, 2009. Springer Berlin Heidelberg.
13


--- Page 14 ---
J. Kennedy and R. Eberhart. Particle swarm optimization. In Proceedings of ICNN’95 - International
Conference on Neural Networks, volume 4, pages 1942–1948 vol.4, 1995. doi: 10.1109/ICNN.
1995.488968.
Marvin Minsky. Steps toward artificial intelligence. Proceedings of the IRE, 49(1):8–30, 1961. doi:
10.1109/JRPROC.1961.287775.
S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by simulated annealing. Science, 220
(4598):671–680, 1983. doi: 10.1126/science.220.4598.671.
Fred Glover. Tabu search—Part I. ORSA Journal on Computing, 1(3):190–206, 1989. doi: 10.1287/
ijoc.1.3.190.
Thomas A. Feo and Mauricio G. C. Resende. Greedy Randomized Adaptive Search Procedures.
Journal of Global Optimization, 6(2):109–133, March 1995. doi: 10.1007/BF01096763.
Tunchan Cura. A rapidly converging artificial bee colony algorithm for portfolio optimization.
Knowledge-Based Systems, 233:107505, 2021. doi: 10.1016/j.knosys.2021.107505.
Javier Alcazar, Mohammad Ghazi Vakili, Can B. Kalayci, and Alejandro Perdomo-Ortiz. Enhancing
combinatorial optimization with classical and quantum generative models. Nature Communications,
15(1):2761, March 2024. doi: 10.1038/s41467-024-46959-5.
Guang-Feng Deng, Woo-Tsong Lin, and Chih-Chung Lo. Markowitz-based portfolio selection
with cardinality constraints using improved particle swarm optimization. Expert Systems with
Applications, 39(4):4558–4566, 2012. doi: 10.1016/j.eswa.2011.09.129.
Khin Lwin and Rong Qu. A hybrid algorithm for constrained portfolio selection problems. Applied
Intelligence, 39(2):251–266, September 2013. doi: 10.1007/s10489-012-0411-7.
14
