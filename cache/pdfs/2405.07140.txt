--- Page 1 ---
Edge Intelligence Optimization for Large Language
Model Inference with Batching and Quantization
Xinyuan Zhang∗, Jiang Liu∗†, Zehui Xiong‡, Yudong Huang∗, Gaochang Xie∗, Ran Zhang∗†
∗State Key Laboratory of Networking and Switching Technology, BUPT, China
†Purple Mountain Laboratories, Nanjing, China
‡Information Systems Technology and Design Pillar, SUTD, Singapore
Abstract—Generative Artificial Intelligence (GAI) is taking the
world by storm with its unparalleled content creation ability.
Large Language Models (LLMs) are at the forefront of this
movement. However, the significant resource demands of LLMs
often require cloud hosting, which raises issues regarding privacy,
latency, and usage limitations. Although edge intelligence has long
been utilized to solve these challenges by enabling real-time AI
computation on ubiquitous edge resources close to data sources,
most research has focused on traditional AI models and has left
a gap in addressing the unique characteristics of LLM inference,
such as considerable model size, auto-regressive processes, and
self-attention mechanisms. In this paper, we present an edge
intelligence optimization problem tailored for LLM inference.
Specifically, with the deployment of the batching technique and
model quantization on resource-limited edge devices, we formu-
late an inference model for transformer decoder-based LLMs.
Furthermore, our approach aims to maximize the inference
throughput via batch scheduling and joint allocation of commu-
nication and computation resources, while also considering edge
resource constraints and varying user requirements of latency
and accuracy. To address this NP-hard problem, we develop an
optimal Depth-First Tree-Searching algorithm with online tree-
Pruning (DFTSP) that operates within a feasible time complexity.
Simulation results indicate that DFTSP surpasses other batching
benchmarks in throughput across diverse user settings and
quantization techniques, and it reduces time complexity by over
45% compared to the brute-force searching method.
Index Terms—Generative AI, large language model, edge
intelligence, wireless networks
I. INTRODUCTION
We are living in an era of rapid Artificial Intelligence
(AI) advancements. In the past year, Generative AI (GAI)
has revolutionized the AI field, automating content creation
and liberating creators from time-consuming manual efforts
[1]. A standout within GAI is the Large Language Models
(LLMs), such as GPT and Claude. Their applications extend to
code generation, customer service chatbots, novel writing, and
beyond [2]. At its core, such impressive generative potential
of LLMs arises from their complex architecture with billions
of neurons, which leads to resource-intensive training and
inference. Hence, LLMs are primarily hosted in the cloud.
However, this centralized approach brings forth challenges like
privacy concerns, usage limitations, and latency [3], impeding
This work was supported by the National Natural Science Foundation of
China (Grant No. 62171064).
The corresponding author is Jiang Liu (Email: liujiang@bupt.edu.cn).
GAI’s broader acceptance and underscoring the need for
innovative solutions.
Edge intelligence presents a viable solution to these chal-
lenges faced by LLMs, building on its proven success in tradi-
tional AI services. By leveraging ubiquitous edge computing
resources near data sources, edge intelligence mitigates issues
of prolonged propagation latency and resource limitations.
With user data distributed across various edge nodes, rather
than being solely stored on cloud servers, the risk of data leak-
age diminishes. The authors in [4] delineated the advantages
of edge intelligence, while also highlighting the challenges
of executing efficient inference on resource-constrained edge
nodes. The authors in [5] proposed partitioning deep neural
network (DNN) inference between local devices and edge
servers, leveraging multi-threading to enhance DNN inference
throughput. In [6], the authors jointly optimized the early-
exit selection, model partitioning, and computation resource
allocation to minimize the device-edge collaborate inference
latency. While these studies provide valuable insights into edge
intelligence optimization, they mainly focus on traditional
DNN models, overlooking the unique challenges posed by
LLMs to edge inference.
Optimizing edge intelligence tailored for transformer-based
LLMs is challenging due to LLM’s distinct attributes com-
pared to traditional DNN models. The reasons are threefold.
(1) Transformer-based LLMs are significantly larger in size.
For instance, GPT-4 has around 1.8 trillion parameters [7],
while AlexNet only has 61 million [8]. This surge in size
notably increases both processing time and memory consump-
tion. (2) Transformer decoder-based LLMs generate outputs
sequentially in an autoregressive manner [9]. Given an input
sequence, LLM traverses the entire layer stack to produce only
one output token. This procedure is repeated for each output
token, intensifying the computational demands. (3) LLMs
employ self-attention modules, which means each output token
is computed based on all preceding tokens. Consequently, the
memory cache for all prior tokens must be maintained until
the output sequence concludes, further increasing memory
demands [9]. In essence, the distinctive attributes of LLMs
make their inference on resource-constrained edge nodes chal-
lenging, which is not tackled in existing research.
In this paper, we introduce a novel edge intelligence
optimization problem tailored for transformer decoder-based
LLM inference within wireless edge networks. We utilize
arXiv:2405.07140v1  [cs.LG]  12 May 2024


--- Page 2 ---
model quantization to store model weights and activations in
decreased bit precisions, reducing the memory footprint on
edge servers [10]. Furthermore, we employ batching to process
requests from multiple users in parallel [11], thereby boosting
the inference throughput. We also account for the varied user
requirements of latency and text generation accuracy. For
instance, while dialogue-based interactions in the Metaverse
demand strict timelines, online medical prescriptions prioritize
text accuracy. We introduce the perplexity differential [10] as
a metric to strike a balance between the latency reduction and
accuracy loss due to quantization.
This paper’s contributions can be summarized as follows:
• We propose an edge intelligence optimization problem
for LLM inference in wireless networks, aiming to max-
imize inference throughput via batching scheduling and
joint allocation of communication and computation re-
sources. Constraints include communication and memory
resources on edge devices and user-specific latency and
accuracy demands. The problem is a variant of multi-
dimensional knapsack problem, which is NP-hard.
• We design an optimal Depth-First Tree-Searching algo-
rithm with tree-Pruning (DFTSP) to address the problem
in a polynomial time. The solution space is constructed
as a search tree. To expedite the solution process, search-
ing prioritizes branches formed by requests with higher
latency tolerance, shorter output length, and lower com-
munication bandwidth demands. Tree-pruning eliminates
searching redundant tree branches, streamlining the algo-
rithm’s complexity.
• In the simulation, we demonstrate the superior throughput
performance of DFTSP against other batching schemes
across various user settings and quantization methods.
Moreover, the advantage of DFTSP algorithm is con-
firmed compared to the brute-force searching approach.
The rest of the paper is structured as follows: Section
II presents the system model and problem formulation. The
solution is proposed in Section III. Simulation results are given
in Section IV, followed by the conclusions in Section V.
II. SYSTEM MODEL AND PROBLEM FORMULATION
Consider a wireless edge intelligence network as shown in
Fig. 1. An edge node (EN) serves multiple users. The EN com-
bines a base station and a nearby edge server connected via
wired links. The EN hosts an LLM using model quantization.
Notice that while Fig. 1 focuses on one LLM, our approach
is adaptable for multiple LLMs. The EN’s total uplink and
downlink bandwidths are BU and BD, with C and M indicat-
ing EN’s computing speed and memory capacity, respectively.
User requests are indexed by i ∈I = {1, 2, ..., I}. As shown
in Fig. 1, the inference request information ⟨si, ni, τi, ai⟩is
sent to EN via application APIs, just like ChatGPT playground
[12]. Here, si is i’s input prompt length, ni is i’s desired
maximum text output length, categorized into multiple levels
of {N1, N2, ..., N}, depending on the input length and the type
Fig. 1: The wireless edge intelligence network and workflows for
LLM inference. An LLM is deployed with model quantization.
of LLM service1. We assume the maximum output length is
set by users or the API. τi is the latency requirement, and ai
is the required text output accuracy.
The following protocol for LLM inference on the EN is
assumed. As shown in Fig. 2, time is divided into epochs, each
of which is further divided into an uplink communication slot
TU, a computation slot TC, and a downlink communication
slot TD. For resource efficiency, each TC starts immediately
after the prior, overlapping with the preceding epoch’s TD and
the subsequent epoch’s TU. Slot durations are periodically up-
dated based on long-term observation. EN aggregates requests
that arrive in the previous epoch, and schedules LLM inference
for the next epoch. Once scheduled, EN allocates resources for
the assigned requests and users get notified.
A. Communication Model
The data sizes for inference request and decision signaling
in Fig. 1 are negligible. The communication model focuses
on prompt transmission and output feedback. The system uses
orthogonal frequency-division multiple access (OFDMA) for
broadband spectrum allocation to users. Given the vast number
of sub-carriers (e.g., thousands in 5G), bandwidth splitting
can be viewed as continuous [11]. Let ρU
i
and ρD
i
denote
the bandwidth fractions for i’s allocated uplink and downlink,
respectively, both between 0 and 1. For any scheduled user
request i, the channel is frequency non-selective [11], with
channel gain hi being constant within an epoch and can get
known by the EN via measurement techniques like CSI-RS.
The uplink and downlink transmission power are pU
i and pD,
respectively. The transmission rate are given by:
rU
i =ρU
i BUlog2

1+ pU
i hi
2
N0

, rD
i =ρD
i BDlog2

1+ pDhi
2
N0

,
where N0 is the white Gaussian channel noise power. Consid-
ering the prompt must be uploaded within TU, represented
mathematically as rU
i TU
≥si, the minimum fraction of
allocated uplink bandwidth ρU
i,min is:
ρU
i,min ≜
si
TUBU log2

1 + pU
i hi2
N0
,
where ρU
i ≥ρU
i,min if i is scheduled. Similary, ρD
i,min is defined.
1For example, in translation, the output length matches the input. For text
summary, the output is much shorter than the input, while in article writing,
the output is much longer.


--- Page 3 ---
Fig. 2: Timeline for LLM edge inference.
B. Inference Model
In this part, we analyze the memory footprint and latency of
the batched LLM inference based on the transformer decoder-
based computation precedures.
LLMs, built with stacked transformer layers, operate in
an autoregressive manner. Fig. 3 depicts a three-layer GPT-
3’s simplified inference. We use GPT-3 for example as it
exemplifies transformer decoder-based LLMs, with GPT-4
details unavailable. As shown, a sequence of prompt tokens
undergoes all layers to generate an output token, which is then
fed back and processed again until an end-of-sequence token
⟨EOS⟩is generated. Each full run through all layers is an
iteration. The first iteration, termed the Initial Stage, processes
all prompt tokens to produce the first output token. Subsequent
iterations, termed the Auto-regressive Stage, use the previously
generated tokens to produce subsequent tokens [13].
LLM inference’s memory footprint mainly comes from
weight storage and key-value cache (KV cache). The KV
cache, essential for the Attention mechanism, facilitates inter-
token awareness by storing the matrices of token keys and
values for weighted token averaging [14]. In the following,
we formulate the memory footprint for weight storage and
KV cache, as well as the latency during Initial Stage and Auto-
regressive Stage.
(1) Memory footprint for weight storage: Let dm repre-
sent the transformer’s hidden dimension and df the hidden
dimension of Feed-Forward Network (FFN). Assuming each
parameter is stored using a 2-byte floating point format, a
matrix A ∈Rm×n requires 2mn bytes. Focusing on the l-th
transformer decoder layer, the weight parameters are specified
by wl
Q,wl
K,wl
V ∈Rdm×dm, wl
O ∈Rdm×dm, wl
1∈Rdm×df , and
wl
2 ∈Rdf×dm. Hence, the memory footprint for weight storage
on GPUs during LLM inference is:
m1 = L (8dmdhnh + 4dmdf) ,
where L is the number of model layers.
(2) Memory footprint and latency during Initial Stage:
Before this stage, all input prompts must be extended to the
maximum token length for parallel execution. We set the max-
imum length as s′. Given one input prompt of Xl ∈Rs′×dm,
the computation operations of the Initial Stage in Fig. 3 are:
Xl
V =Xl·wl
V , Xl
K =Xl·wl
K, Xl
Q =Xl·wl
Q,
Xl
Out = fsoftmax(
Xl
QXl
K
T
√dh
)·Xl
V ·wl
O+Xl,
Xl+1 = frelu(Xl
Out · wl
1) · wl
2 + Xl
Out.
Fig. 3: LLM inference procedure.
Consider the batched inference within any TC, we define
a binary variable xi to represent whether user i’s request is
scheduled: xi = 1 if scheduled, and xi = 0 otherwise. The
batch size is then P
i∈I xi. During the Initial Stage, the size
of KV cache (Xl
K, Xl
V ) for all input prompts across layers is:
mI
2 = 4Ls′dm
X
i∈I xi.
Floating point operations (FLOPs) quantify computational
work. For a matrix-vector multiplication with A ∈Rm×n, b ∈
Rn, the computation requires 2mn FLOPs. The matrix-matrix
multiplication involving A ∈Rm×n, B ∈Rn×p needs 2mnp
FLOPs. Hence, the batched inference latency for all input
prompts across layers during the Initial Stage is:
tI= LP
i∈Ixi
C
 6s′d2
m+
 4s′2dm+ 2s′d2
m

+ 4s′dmdf

,
where 6s′d2
m denotes the computation to get Xl
Q,Xl
K,Xl
V ,
4s′2dm+ 2s′d2
m denotes the calculation of Xl
Out, and 4s′dmdf
is related to Xl+1.
(3) Memory footprint and latency during Autoregressive
Stage: Given the input of gl ∈R1×dm, the computation
operations in the l-th layer are:
Xl
V ←Concat(Xl
V , gl·wl
V ), Xl
K ←Concat(Xl
K, gl·wl
K),
gl
Q =gl·wl
Q, gl
Out =fsoftmax(gl
QXl
K
T/
p
dh)·Xl
V ·wl
O+ gl,
gl+1 =frelu(gl
Out·wl
1)·wl
2 + gl
Out.
During this stage, the KV cache of gl
K, gl
V updates with every
new token until the sequence completes. Hence, the memory
footprint of KV cache is:
mA
2 = 4Lnidm
X
i∈I xi.
The batched inference latency of all output tokens across
layers during Auto-regression Stage is:
tA = L
C
X
i∈Ixi(ni−1)
 6d2
m+
 4(s′+ni
2 )dm+2d2
m

+4dmdf

,
where the latency increases with output iteration rounds.
3) Quantization Model
Due to the resource demands of LLMs, the post-training
quantization (PTQ) method is mainly adopted, as it streamlines
computation without retraining. PTQ encodes LLM weights
and activation tensors at precision below 16-bit, typically
using 8-bit, 4-bit, or a combination [2]. While some PTQ
methods only adjust weights, others quantize both weights and
activations. To quantify these effects, we define α to represent
memory savings and β to denote computational time reduction.
Both metrics are measured via offline exhaustive evaluations
on diverse datasets [10].


--- Page 4 ---
Model quantization reduces memory usage and latency but
can degrade accuracy. To address this, we use the perplexity
(PPL) differential, denoted as∆PPL, to measure the change in
LLM accuracy between pre- and post-quantization [10]. Based
on various datasets, ∆PPL is predetermined and known. A
larger ∆PPL indicates a greater accuracy loss. We introduce
f as a function linking PPL differential to accuracy, which
is monotonically decreasing. For users seeking high-accuracy
LLM services, if xi =1, it is imperative that ai ≤f(∆PPL),
ensuring generated text meets accuracy requirements.
C. Problem Formulation
We formulate the edge intelligence optimization problem
for LLM inference in wireless networks, aimed at maximizing
throughput while adhering to resource allocation constraints
and accommodating heterogeneous user QoS requirements.
Let X ={xi|∀i ∈I} be the decision variable, the optimization
problem is formulated as follows:
P1: max
X
X
i∈I xi
(1)
s.t.
X
i∈I ρU
i,minxi ≤1,
X
i∈I ρD
i,minxi ≤1
(1a, 1b)
α(m1 + mI
2 + mA
2 ) ≤M,
(1c)
tw,i + TU + β(tI + tA) + TD ≤τi, ∀xi = 1,
(1d)
ai ≤f(∆PPL),
∀i ∈I,
(1e)
xi ∈{0, 1} ,
∀i ∈I.
(1f)
where the objective is the throughput of edge inference, mea-
sured by the number of user prompts processed successfully.
(1a)-(1c) represent the constraints for uplink communication,
downlink communication, and memory footprint, respectively.
(1d) and (1e) ensure that each scheduled request receives
satisfactory generation output within its deadline. By removing
constraints (1d) and (1e), Problem P1 is simplified to a
standard Multi-dimensional Knapsack Problem (MKP), which
is NP-hard [15]. The complexity of P1 is aggravated by
introducing (1d) and (1e). Hence, P1 is proved to be NP-hard.
III. PROBLEM SOLUTION
In this section, we first reformulate the problem into a series
of sub-problems aimed at finding feasible solutions. Then, we
design an optimal DFTSP algorithm for resolution. We delve
into the tree construction, searching, and pruning processes.
Lastly, we analyze the algorithm’s complexity.
A. Problem Reformulation
We observe that the optimization object is only concerned
with the number of scheduled requests, whereas the con-
straints emphasize which requests are chosen. Let ˜I be the
set of requests who are satisfied with the edge node’s output
accuracy, S =
n
i|i∈˜I, xi =1
o
denotes the set of scheduled
requests, we have |S|=P
i∈˜I xi. Problem P1 then translates to
identifying such an S that possesses the maximum cardinality
while satisfying the constraints. Given |S| = z, Problem P1 is
reformulated as the subsequent question:
Algorithm 1 Optimal DFTSP algorithm.
Input: Available request set ˜I.
Output: Optimal solution S to Problem P1.
1: Initialize z = 0, d = 0, Fd = ∅, S′ = ∅;
2: for z = |˜I|, |˜I| −1, ..., 1 do
3:
Sort ˜I according to ˜τi’s descending order;
4:
for d = z, z + 1, ..., |˜I| do
5:
τmin ←˜τd, Fd ←the first d requests in ˜I;
6:
Construct root node v0;
7:
Call DFS(v0, d);
8:
if DFS(v0, d) returns solution S′ then
9:
S ←S′, return S;
return no solution
10: function DFS(vN(v), d)
11:
N(v) ←depth of vN(v);
12:
Visit the path to vN(v) along unvisited nodes;
13:
if PN(v)
k=1 vk = d then
14:
Recover the subset S′ from vN(v);
15:
if S′ meets constraints (2b)-(2e) then
16:
return S′
17:
else
18:
Mark vN(v) visited;
19:
v′←vN(v)’s sibling node with the largest index;
20:
Call DFS(v′, d);
21:
else if PN(v)
k=1 vk < d and N(v) = N then
22:
Mark vN(v), its parent node, and its sibling nodes
with lower index visited;
23:
v′ ←vN(v)’s parent node;
24:
Call DFS(v′, d);
25:
else if PN(v)
k=1 vk < d and N(v) < N then
26:
if all child nodes are visited then
27:
if vN(v) is the root node then
28:
return no solution
29:
else
30:
v′ ←vN(v)’s parent node;
31:
Call DFS(v′, d);
32:
vN(v)+1 ←min

z′,
FN(v)+1
	
;
33:
Call DFS(vN(v)+1, d);
P2 :
find S ⊆˜I
(2)
s.t.
|S| = z,
(2a)
X
i∈S kisi ≤1,
X
i∈S k1ni ≤1,
(2b,2c)
X
i∈S ni ≤˜
M,
(2d)
X
i∈S k4ni + k5n2
i ≤˜τi.
(2e)
Here, we suppose that the users are geographically concen-
trated, such as in the same building. In this case, hi is same for
all users. Besides, TU, BU, TD, and BD are known aforehand.
Hence, ki is a variable associated with TU, BU, pU
i , and hi,
and it varies with pU
i . k1 is a constant related to TD, BD, pD,
and hi. ˜
M=k2−s′z, ˜τi = (τi−tw,i−TU−TD)C
β
−k3z, k2 to k5 are
constants related to LLM parameters. The feasible solution
to P2 with the maximum z is exactly the optimal solution
to P1. We propose an efficient algorithm to search for the
optimal solution to P1, presented as Algorithm 1. Observing
that (2c), (2d) and (2e) increase with ni, we prioritize requests
with lower ni and higher ˜τi to minimize constraint violation.
Starting with the largest z (Line 2), we rank requests by ˜τi


--- Page 5 ---
Fig. 4: An example of Algorithm 1. z = 6, d = 10, N = 3,
|FN1| = |FN2| = 4, |FN3| = 2. All paths meet the memory and
latency constraints. Inside each dotted circle, the number represents
cumulative uplink bandwidth of requests in Sk.
and assess P2’s feasibility among the top d requests, reorded
by ni (Line 3-5). The approach expedites the solution through
sequential prioritization.
B. Tree Construction
Given the z and d fixed, the top d candidate requests are
denoted as Fd. We categorize Fd based on output length: N1
represents the shortest. Fd = FN1 ∪FN2 ∪... ∪FN, where
FNk = {i|i ∈Fd, ni = Nk} , k ∈{1, 2, ..., N}. Let S′ define
the set of selected z requests, S′ = S′
1 ∪S′
2 ∪... ∪S′
N, where
S′
k = {i|i ∈FNk} , k ∈{1, 2, ..., N}. Each S′
k is a subset
of FNk. With Fd established, we aim to identify a potential
solution S′ efficiently by constructing a search tree. As shown
in Fig. 4, the methodology for this construction is as below.
(1) Root, parent, and child nodes: The root node, v0, has
child nodes representing possible choices for S1. While S1 can
be any subset of FN1, priority is given to requests with lower
uplink bandwidth. Each S1 acts as a parent for S2, spawning
|FN2| + 1 child nodes, building the search tree layer by layer.
(2) Path: Each node vN(v) at depth N(v) uniquely indicates
the size of SN(v). The path from the root to vN(v) can be
traced by the vector p =

v0, v1, ..., vN(v)

, representing a
partial solution S where
SN(k)
 = vk for k = 1, 2, ..., N(v).
(3) Leaf node: A node vN(v) is a leaf if PN(v)
k=1 vk = z,
representing a solution, or if PN(v)
k=1 vk < z and N(v) = N,
indicating maximum depth without solution. Otherwise, vN(v)
has undiscovered child nodes.
C. Tree Searching and Tree Pruning
Consider a search over the tree constructed for given z
and τmin. When dealing with large values of z and d, it is
impractical to store the entire tree in memory and check the
feasibility of all paths. We addresses this by optimizing node
exploration order and reducing unnecessary search complexity.
(1) Tree-searching: When exploring all child nodes of any
node, we prioritize nodes with the highest index, favoring
requests with smaller ni values. This ensures constraints (2c)-
(2e) are met efficiently. Additionally, we emphasize depth over
breadth, quickly accessing leaf nodes. Upon exploring a node,
we proceed to its largest unvisited child, deepening the search.
If a node is terminal or fully explored, we backtrack.
TABLE I: LLM settings in the simulation [2]
Model
Layer
number
Dimension
Head
number
Head
dimension
BLOOM-3B
30
2560
32
80
BLOOM-7.1B
30
4096
32
128
OPT-13B
40
5120
40
128
TABLE II: PPL degration under different quantization methods [10]
Precision
Quantization method
BLOOM-3B
BLOOM-7.1B
OPT-13B
W4A16
GPTQ
0.75
0.54
0.2
ZQ-Local
0.92
0.59
0.42
(2) Tree-pruning: For any node vN(v), if PN
k=N(v) |FNk| <
z−PN(v)
k=1 vk, we skip vN(v), its child nodes, its sibling nodes
with smaller indices and their child nodes.
Complexity. Sorting nodes by uplink bandwidth has a com-
plexity ofO(I2). Each path’s complexity isO(I2 max {I, N}).
With the search tree having O(( I
N )N) nodes, the DFS com-
plexity is O(I2 max {I, N} ( I
N )N). The overall complexity
of Algorithm 1 is O(I4 max {I, N} ( I
N )N). Since N is a
constant much smaller than I, the complexity is polynomial
in the number of I, even without considering the complexity
ruduction by pruning.
IV. SIMULATIONS
The arrival of user requests are modeled by a Poisson
process with rates between 5 to 250 requests per second. The
input and output length randomly vary among {128, 256, 512}
tokens. We use Byte-Pair Encoding (BPE) tokenization, with
each token as a 2-byte index. Latency requirements are uni-
formly distributed among [0.5, 2] seconds, and the required
accuracy range from [0, 1] PPL degradation. In simulations,
our edge server uses 20 NVIDIA JETSON TX2 GPUs, each
with 1.33TFLOPs and 32GB memory. Bandwidth is 20MHz.
Transmit powers are 20dBm (user-to-EN) and 43dBm (EN-
to-user) [16]. N0 = -174dBm/Hz. The channel gain follows
Rayleigh fading at 10−3 pass loss [11]. The duration of
an epoch is set as 2 seconds, with TU = TD =250ms.
Two benchmarking schemes of batched inference are adopted.
(1) Static batching (StB): The edge node has a set batch
size based on epoch duration and LLM parameters to avoid
GPU overflow. (2) No batching (NoB): Each GPU accepts a
request once idle. Three open-source LLMs are adopted in
this simulation, as detailed in Table 1. The FFN’s dimension
is four times the model’s. Default quantization is 8-bit weight,
16-bit activation (W8A16). Other methods are in Table 2.
Fig. 5 contrasts the batching schemes between BLOOM-3B
and BLOOM-7.1B inference. In Fig.5(a), throughput initially
increases with the arrival rate but later stabilizes due to edge
node constraints. The DFTSP scheme surpasses both StB and
NoB. This is because DFTSP dynamically adjusts batch sizes,
while StB maintains a consistent batch to prevent overflow and
NoB handles one request per GPU. Overall, BLOOM-7.1B
has a reduced throughput compared to BLOOM-3B across all
schemes, attributed to its larger parameter count. In Fig. 5(b),
throughput varies based on user latency requirements. As these
requirements become more lenient, more requests meet their
deadlines. BLOOM-3B consistently surpasses BLOOM-7.1B
due to its optimized dimensions and reduced matrix operations.


--- Page 6 ---
Fig. 5: Throughput under different batching schemes.
Fig. 6: Throughput under different quantization methods.
NoB, without the advantage of parallel processing, struggles
to consistently meet tight deadlines for larger BLOOM-7.1B
inferences.
Fig. 6 delves into the throughput effects of various quan-
tization methods. In Fig. 6(a), overlooking user accuracy
requirements, we observe that larger models using the same
quantization precision handle fewer requests per epoch, con-
strained by computational resources. However, diminishing
quantization precision minimizes memory usage, boosting
throughput. Still, there are trade-offs: Fig. 6(b) highlights that
broader quantization can expand the PPL differential, thus
affecting accuracy and subsequent throughput. Dotted lines
denote W8A16 quantization throughput. Intriguingly, even
at the same precision, GPTQ and ZQ-Local quantizations
vary due to their distinct tensor operations. This suggests the
importance of choosing the right quantization method tailored
to specific LLMs. As accuracy constraints are relaxed, more
inferences are processed.
Lastly, we contrast the computational complexity of the
DFTSP algorithm, optimized for tree-search, with a bench-
mark that lacks tree-pruning. Table 3 underscores that brute-
force search complexity surges exponentially with higher
arrival rates. The merits of DFTSP become even more apparent
with rising rates, achieving a notable 97.92% complexity
reduction at an arrival rate of 200.
V. CONCLUSION
In this paper, we delved into the potential of edge intel-
ligence in the context of LLM inference. We investigated
the optimization of edge intelligence tailored for LLM infer-
ence in wireless networks, focusing on the joint optimization
of batching scheduling and resource allocation to maximize
TABLE III: Algorithm Time Reduction with Tree-Pruning
Arrival rate
(requests per second)
10
50
100
200
Complexity Reduction
45.52%
71.18%
79.07%
97.92%
throughput. To balance the trade-off between quantization and
LLM service accuracy, we introduced the PPL differential
metric. We developed the DFTSP algorithm, which efficiently
achieves the optimal solution in polynomial time using tree-
search and tree-pruning techniques. Simulation results under-
scored DFTSP’s superior throughput enhancement compared
to other batching approaches. These findings also shed light on
selecting appropriate quantization schemes for specific LLMs
in resource-constrained wireless edge nodes. Notably, DFTSP
significantly outperforms the brute-force tree-search algorithm
on time complexity.
REFERENCES
[1] P. Hacker, A. Engel, and M. Mauer, “Regulating ChatGPT and other
large generative AI models,” in FAccT 23.
New York, NY, USA:
Association for Computing Machinery, 2023, pp. 1112–1123.
[2] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang,
J. Zhang, Z. Dong et al., “A survey of large language models,” arXiv
preprint arXiv:2303.18223, 2023.
[3] M. Xu, H. Du, D. Niyato, J. Kang, Z. Xiong, S. Mao, Z. Han,
A. Jamalipour, D. I. Kim, V. Leung et al., “Unleashing the power
of edge-cloud generative AI in mobile networks: A survey of AIGC
services,” arXiv preprint arXiv:2303.16129, 2023.
[4] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, “Edge
intelligence: Paving the last mile of artificial intelligence with edge
computing,” Proc. IEEE, vol. 107, no. 8, pp. 1738–1762, 2019.
[5] J. Li, W. Liang, Y. Li, Z. Xu, X. Jia, and S. Guo, “Throughput maxi-
mization of delay-aware dnn inference in edge computing by exploring
dnn model partitioning and inference parallelism,” IEEE. Trans. Mob.
Comput., vol. 22, no. 5, pp. 3017–3030, 2023.
[6] F. Dong, H. Wang, D. Shen, Z. Huang, Q. He, J. Zhang, L. Wen,
and T. Zhang, “Multi-exit dnn inference acceleration based on multi-
dimensional optimization for edge intelligence,” IEEE. Trans. Mob.
Comput., vol. 22, no. 9, pp. 5389–5405, 2023.
[7] M. Schreiner, “GPT-4 architecture, datasets, costs and more leaked.”
[Online]. Available:
https://the-decoder.com/gpt-4-architecture-dataset
s-costs-and-more-leaked/
[8] E. Li, L. Zeng, Z. Zhou, and X. Chen, “Edge AI: On-demand acceler-
ating deep neural network inference via edge computing,” IEEE Trans.
Wirel. Commun., vol. 19, no. 1, pp. 447–457, 2020.
[9] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in NeurIPS
17, vol. 30.
Long Beach, CA, USA: Curran Associates, Inc., 2017.
[10] Z. Yao, C. Li, X. Wu, S. Youn, and Y. He, “A comprehensive study
on post-training quantization for large language models,” arXiv preprint
arXiv:2303.08302, 2023.
[11] Z. Liu, Q. Lan, and K. Huang, “Resource allocation for multiuser edge
inference with batching and early exiting,” IEEE J. Sel. Areas Commun.,
vol. 41, no. 4, pp. 1186–1200, 2023.
[12] [Online]. Available: https://platform.openai.com/playground
[13] G.-I. Yu, J. S. Jeong, G.-W. Kim, S. Kim, and B.-G. Chun, “Orca:
A distributed serving system for transformer-based generative models,”
in OSDI 22.
Carlsbad, CA, USA: Advanced Computing Systems
Association, 2022, pp. 521–538.
[14] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, D. Y. Fu, Z. Xie,
B. Chen, C. Barrett, J. E. Gonzalez et al., “High-throughput generative
inference of large language models with a single GPU,” arXiv preprint
arXiv:2303.06865, 2023.
[15] V. Cacchiani, M. Iori, A. Locatelli, and S. Martello, “Knapsack prob-
lems—an overview of recent advances. part ii: Multiple, multidimen-
sional, and quadratic knapsack problems,” Comput. Oper. Res., vol. 143,
p. 105693, 2022.
[16] C. W. Zaw, N. H. Tran, Z. Han, and C. S. Hong, “Radio and computing
resource allocation in co-located edge computing: A generalized nash
equilibrium model,” IEEE. Trans. Mob. Comput., vol. 22, no. 4, pp.
2340–2352, 2023.
