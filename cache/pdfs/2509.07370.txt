--- Page 1 ---
PERSONAFUSE: A PERSONALITY ACTIVATION-DRIVEN
FRAMEWORK FOR ENHANCING HUMAN-LLM INTER-
ACTIONS
Yixuan Tang1
Yi Yang1
Ahmed Abbasi2
1The Hong Kong University of Science and Technology
2University of Notre Dame
ytangch@connect.ust.hk, imyiyang@ust.hk, aabbasi@nd.edu
ABSTRACT
Recent advancements in Large Language Models (LLMs) demonstrate remark-
able capabilities across various fields. These developments have led to more
direct communication between humans and LLMs in various situations, such as
social companionship and psychological support. However, LLMs often exhibit
limitations in emotional perception and social competence during real-world con-
versations. These limitations partly originate from their inability to adapt their
communication style and emotional expression to different social and task contexts.
In this work, we introduce PersonaFuse, a novel LLM post-training framework
that enables LLMs to adapt and express different personalities for varying situ-
ations. Inspired by Trait Activation Theory and the Big Five personality model,
PersonaFuse employs a Mixture-of-Expert architecture that combines persona
adapters with a dynamic routing network, enabling contextual trait expression.
Experimental results show that PersonaFuse substantially outperforms baseline
models across multiple dimensions of social-emotional intelligence. Importantly,
these gains are achieved without sacrificing general reasoning ability or model
safety, which remain common limitations of direct prompting and supervised
fine-tuning approaches. PersonaFuse also delivers consistent improvements in
downstream human-centered applications, such as mental health counseling and
review-based customer service. Finally, human preference evaluations against
leading LLMs, including GPT-4o and DeepSeek, demonstrate that PersonaFuse
achieves competitive response quality despite its comparatively smaller model size.
These findings demonstrate that PersonaFuse offers a theoretically grounded and
practical approach for developing social-emotional enhanced LLMs, marking a
significant advancement toward more human-centric AI systems.
1
INTRODUCTION
Large Language Models (LLMs) have shown impressive capabilities across various domains, includ-
ing advertisement generation (Chen and Chan 2024), clinical consultation (Kwon et al. 2024, Jin et al.
2024a), and complex mathematical reasoning (Toshniwal et al. 2024). The rapid advancement of
LLMs has led to their widespread adoption in real-world applications, particularly in human-LLM
interactions (Handa et al. 2025). For example, Duolingo Max, an AI tutor based on GPT-4o (OpenAI
2025), enables users to practice real-world conversation skills in different languages. Character.ai
is a platform that enables users to engage in open-ended conversations with AI personas, including
therapists, fictional characters, or supportive companions.
As large language models are increasingly deployed in human-facing scenarios, a new challenge
has emerged: the need for AI systems to exhibit social and emotional intelligence. In human com-
munication, emotional intelligence is essential for building trust, ensuring productive collaboration,
and fostering user satisfaction (Afroogh et al. 2024). Applications such as education, counseling,
customer service, and healthcare demand AI models that are not only factually accurate but also
1
arXiv:2509.07370v2  [cs.CL]  11 Sep 2025


--- Page 2 ---
I have been feeling anxious and physically and mentally 
exhausted lately. I don‚Äòt know what to do.
I'm really sorry to hear that you're feeling this way. It's important to 
take steps to address your feelings of anxiety and exhaustion. Here 
are some suggestions that might help:
1. Talk to Someone: Consider speaking with a {Truncate}
2. Self-Care: Make sure you‚Äòre taking care of your physical
{Truncate}
user
GPT-4o
PersonaFuse
(Ours)
You are in a debate: Argue that money can buy happiness by 
providing reasons to support this view.
user
GPT-4o
Money can indeed buy happiness, as it allows individuals to fulfill 
various needs and desires that contribute to overall well-being. 
Firstly, having financial stability reduces stress and anxiety
{Truncate} Additionally, money enables access to experiences that 
bring joy, such as travel, education, and leisure activities. {Truncate}
Lastly, money can enhance social connections by facilitating 
participation in social activities and supporting relationships with 
loved ones.
Certainly! Here are several reasons to support the view that money 
can buy happiness:
1. Basic Needs and Security: One of the most {Truncate}
2. Freedom and Autonomy: Financial resources provide individuals..
{Truncate}
It sounds like you're going through a tough time right now, and it's 
completely okay to feel anxious and exhausted. It's important to 
take care of yourself during these moments. Have you considered 
talking to a friend or a professional about how you're feeling? They 
might be able to offer some helpful strategies or just a listening ear. 
Remember, it's okay to ask for help.
PersonaFuse
(Ours)
Figure 1: Response comparison between GPT-4o and our model. "{Truncate}" indicates truncated
content for brevity.
emotionally attuned to users‚Äô needs. Without emotional sensitivity, even technically correct responses
can be perceived as unhelpful (Han et al. 2023).
However, most current LLM training efforts focus on two main areas: improving performance on
specific tasks (Wei et al. 2022, Kojima et al. 2022) or reasoning (Guo et al. 2025) and enhancing
safety alignment with the 3H principles (Helpfulness, Honesty, and Harmlessness) (Bai et al. 2022).
While these objectives have led to strong benchmark results, recent studies highlight that many
chat-based LLMs still fall short in emotional understanding and situational adaptability in real-world
interactions (Lee et al. 2022, Kang et al. 2024, Kim et al. 2023, Gao et al. 2025). For example,
they often fail to appropriately adjust their communication style based on the user‚Äôs emotional state
or adapt their responses according to different conversational needs. As illustrated in Figure 1,
GPT-4o (OpenAI 2025) responds to both users‚Äô prompts with generic key points without considering
the emotional context or adapting its communication style. Whether facing an anxious user seeking
emotional support or engaging in a structured debate, GPT-4o maintains the same response pattern,
failing to adjust its communication style or provide appropriate emotional engagement. In the former
AI tutoring scenario, if the LLM teacher fails to adapt its responses based on student dialogue, it
may lead to diminished learning outcomes and student engagement. This fundamental limitation
highlights a critical gap in current LLM development: the need for models to engage in meaningful
social-emotional interactions. The importance of addressing this limitation is underscored by recent
release of OpenAI‚Äôs GPT-4.51, which explicitly emphasizes improvements in emotional intelligence
as a key development focus. However, sycophancy, where the chatbot is overly flattering or agreeable,
remains a critical issue even after release. A subsequent blog post by OpenAI2 acknowledged
sycophancy in GPT-4o as an ongoing challenge, underscoring that this remains an open problem even
for state-of-the-art systems.
Researchers have explored various approaches for enhancing the human interactive capabilities of
LLMs, mainly focusing on two key strategies: prompting and post-training. Tailored prompting
strategies (Qian et al. 2023), including persona-based approaches (Chen et al. 2024a), aim to guide
model behavior by providing explicit prompts or context. Meanwhile, post-training approaches (√áalƒ±k
and Akku¬∏s 2025, Chen et al. 2023) seek to directly fine-tune the LLM to enhance interaction via
techniques such as supervised fine-tuning (SFT) and reinforcement learning with human feedback
(RLHF). Both methods exhibit certain limitations. The first, prompting, suffers from two limitations:
1) it relies on static instructions that cannot adequately adapt to dynamic context changes during
interactions, and 2) the LLM is sensitive to the prompts; even slight non-semantic modifications in
prompt formatting may lead to considerable drops in performance (Sclar et al. 2024, Kirkpatrick
et al. 2017). Post-training methods are widely used to align LLMs with specific communicative
goals. However, a critical limitation is that such adaptation may impair the model‚Äôs general language
1https://openai.com/index/introducing-gpt-4-5/
2https://openai.com/index/sycophancy-in-gpt-4o/
2


--- Page 3 ---
understanding or safety alignment, which is a phenomenon known as catastrophic forgetting (Kotha
et al. 2024). Recent studies also show that training LLMs to be empathetic makes them less reliable
(Ibrahim et al. 2025). This trade-off is particularly concerning because even in human-facing
applications, such as tutoring or customer support, it is essential for models to remain safe (helpful,
honest, and harmless) and capable of general intelligence (Wang et al. 2024c).
These limitations point to a critical research gap: How can we design a method to enhance the
social and emotional intelligence of LLMs while maintaining general intelligence and response
harmlessness? To address this research gap, we draw on psychological theories, particularly the
Big Five personality model (McCrae and John 1992) and Trait Activation Theory (TAT) (Tett and
Burnett 2003). The Big Five personality model, also known as the five factor model characterizes
personality along five dimensions: Openness, Conscientiousness, Extraversion, Agreeableness, and
Neuroticism. TAT complements the Big Five personality model by emphasizing that personality
traits are expressed differently depending on situational contexts and relevant cues. This aligns
with the need for LLMs to dynamically adapt their responses to diverse conversational contexts
and increase user engagement. Crucially, research demonstrates that LLMs can effectively simulate
these personality traits (Sorokovikova et al. 2024), making it feasible to control LLM behavior
by personality expressions. Integrating the Big Five model with TAT, we aim to develop socially
intelligent LLMs capable of contextually appropriate behavior. For example, in professional settings,
an LLM could emphasize conscientiousness and suppress extraversion to convey professionalism and
efficiency. In contrast, in casual conversations, the model could increase extraversion and openness
to foster a more engaging and friendly tone. This ability to adapt personality expressions based on
context has the potential to create more natural, human-like interactions, while preserving the model‚Äôs
general task performance and model safety.
Building on these theoretical foundations, we present PersonaFuse, a novel LLM post-training frame-
work that enables dynamic persona calibration in LLMs based on situational context. PersonaFuse
incorporates three key innovations: (1) a Situation-Aware Mixture of Experts (Persona-MoE) archi-
tecture for contextual personality expression. It employs a set of personality adapters corresponding
to different Big Five trait combinations, and a dynamic router network for situation-aware expert
activation; (2) a training data synthesis process that uses personality-aware chain-of-thought reasoning
to generate query-response pairs and expert vectors; (3) a three-stage training pipeline that jointly
learns contextual routing and expert representations. Specifically, the data generation process first
relies on TAT to identify social and task-related cues within the context, then uses these cues to infer
the activated personality traits. Guided by these theoretically-grounded innovations, our framework
ensures that generated responses are both situation-aware and emotionally appropriate. This process
also generates weights for different personality traits based on the prompt context. Based on these
personality weights, we classify prompts into different personality groups to train specialized persona
experts, each capable of generating responses in a specific personality style. We also explicitly use
these weights to construct training data for contrastive learning in MoE routing optimization. This
routing mechanism can dynamically mix the experts and generate responses with diverse personality
combinations. The whole architecture also allows transparent observation of personality expression
through router weight analysis during inference, enabling precise control over the model‚Äôs internal
personality traits rather than relying on surface-level prompt engineering.
We conduct comprehensive experiments to evaluate the proposed framework on social-emotional
intelligence benchmarks. On EmoBench (Sabour et al. 2024), which tests emotional understanding
in scenarios such as comforting someone in distress, PersonaFuse improves by 37.9% over the
baseline. On EQ-Bench (Paech 2023), which measures the ability to interpret complex emotions
and social interactions, it achieves a 69% gain. On ToMBench (Chen et al. 2024b), covering a wide
range of social cognition tasks including the False Belief Task, PersonaFuse also shows consistent
improvements.
Promisingly, the improvement in social and emotional intelligence does not come at the cost of the
LLM‚Äôs general intelligence capabilities or safety. For general intelligence capabilities, compared
to the baseline methods, PersonaFuse achieves improved performance on GPQA (Rein et al. 2024)
for graduate-level question answering and GSM8k (Cobbe et al. 2021) for mathematical reasoning,
while showing significant improvements on real-world user queries in Arena-Hard (Li et al. 2025b).
For model safety evaluation on the well-established LLM safety benchmark SafetyBench (Zhang
et al. 2024), experimental results show that PersonaFuse exhibits more responsible and safer behavior
3


--- Page 4 ---
across seven critical dimensions, including offensiveness, bias, and ethical judgment, compared to
baseline methods.
We further evaluate the practical utility of PersonaFuse on two downstream applications: customer
service support (using product-related queries from Shop MMLU (Jin et al. 2024b)) and mental health
counseling (using MentalChat16K (Xu et al. 2025)). In both scenarios, PersonaFuse outperforms the
baseline, demonstrating improved capabilities in understanding consumer needs and counseling skills
such as active listening and empathy.
We further conduct a human evaluation to compare PersonaFuse with several strong LLMs, including
Llama-3.1-8B- Instruct, GPT-3.5-Turbo (OpenAI 2025), GPT-4o (OpenAI 2025), and DeepSeek-
R1-Distill-Qwen-14B (Guo et al. 2025). The evaluation covers both emotion-based dialogue tasks
and logical reasoning capabilities, using pairwise comparisons between responses given the same
input examples. Human evaluation results provide additional validation of our approach. Person-
aFuse achieves strong performance on emotion-based dialogue tasks with win rates of 73.0% against
GPT-3.5-Turbo, 66.7% against DeepSeek-R1-Distill-Qwen-14B, and 57.9% against GPT-4o, while
maintaining reasonable performance on logical reasoning tasks (56.7%, 42.7%, and 36.8% respec-
tively) despite PersonaFuse‚Äôs comparatively smaller model size. These results further demonstrate
that our theory-guided training effectively captures nuanced emotional patterns while preserving
general reasoning capabilities, validating our hypothesis that dynamic personality adaptation enhances
dialogue system performance in emotion-sensitive contexts.
This research makes several contributions. First, from a design perspective, we advance the under-
standing of AI personality adaptation by developing a novel framework that integrates established
psychological theories with modern AI architectures (Yang et al. 2023). Our work demonstrates how
principles from psychology can be effectively implemented in LLMs. Second, we propose an effective
Mixture-of-Experts architecture in which each expert embodies a distinct personality configuration,
enabling dynamic and interpretable persona adaptation. Third, from an empirical perspective, our
experiments show that the proposed method significantly improves human‚ÄìAI interaction while
maintaining strong general intelligence and model safety, offering practical guidance for building
more socially intelligent and responsible AI systems. Overall, this work contributes to the growing
literature in information systems (IS) on human‚ÄìAI interaction by providing a systematic approach to
embedding personality traits in AI systems (Padmanabhan et al. 2022, Abbasi et al. 2024). It has
broad implications for real-world applications such as customer service, healthcare, and education. As
organizations increasingly deploy AI in human-facing settings, our work provides practical insights
for designing human-centric conversational and companion AI systems.
2
LITERATURE REVIEW
We review three lines of research closely related to this work. First, we discuss personality modeling
in LLMs, as our approach builds on personality theory, particularly the Big Five model and Trait
Activation Theory. Second, we examine advances in mixture-of-experts methods, with emphasis
on adapting MoE architectures to diverse tasks and domains, which are relevant to our proposed
Persona-MoE framework. Finally, we review research on human-centric LLM development, focusing
on efforts to design companion models that can understand user emotions in socially oriented
applications.
Personality Modeling in LLMs. Prior research has explored integrating personality into LLMs to
enhance personalization and user alignment. One line of work incorporates end-user preferences
or profiles to guide generation, such as embedding user attributes into the model (Liu et al. 2025a)
or applying reinforcement learning from user feedback (Poddar et al. 2024). While effective, these
methods rely on the strong assumption that detailed user information is always available, which is
often unrealistic in real-world applications.
Another line of research simulates static personality traits through fixed profiles, for example, assign-
ing OCEAN scores (Chen et al. 2024a) or using personality-driven prompting (Jiang et al. 2024b).
Although these approaches achieve a degree of trait consistency (Sorokovikova et al. 2024), they
largely neglect the situation‚Äìtrait interaction dynamics emphasized in psychology literature (Fleeson
and Jayawickreme 2015), which highlight that the relevance of traits depends on task demands
and contextual cues. Relying solely on fixed profiles or prompting therefore limits adaptability in
4


--- Page 5 ---
real interactions. Our work addresses this gap by developing a situationally aware framework that
dynamically activates personality traits in response to contextual signals, enabling more flexible and
emotionally intelligent LLM behavior.
Mixture-of-Experts in LLM Adaptation. Mixture-of-Experts has emerged as an effective architec-
ture for adapting large language models to specialized domains. Approaches such as DoMIX (Kim
et al. 2025) and Mixture-of-LoRAs (Feng et al. 2024, Buehler and Buehler 2024) combine multiple
domain- or task-specific LoRA adapters, enabling targeted knowledge integration. These methods
maintain separate expert modules for different domains and dynamically select relevant ones at
inference. However, they are primarily optimized for domain knowledge transfer rather than the
behavioral and stylistic adaptations required for personality-driven interactions.
More recently, MoE has been extended to personality and emotion modeling. P-React (Dan et al.
2025) leverages a mixture of experts to model Big Five traits, with each expert trained under a
Personality Specialization Loss. MoEI (Zhao et al. 2024b) similarly employs LoRA blocks with
a routing mechanism to enhance emotion perception and expression. Yet, both approaches focus
on static trait expression, overlooking the fact that human personality manifests differently across
situations.
Other work, such as PROPER (Zhang et al. 2025a), advances personalization through a three-tier
architecture (population, group, and individual levels), where experts capture shared user preferences
and communication patterns. A user-aware router assigns users to groups, showing that expert
mixtures can model nuanced preferences beyond task boundaries. However, PROPER still treats
personality as static user attributes rather than context-dependent phenomena‚Äîdespite psychological
evidence that the same individual may require different response styles depending on whether they
seek emotional support or technical assistance (Fleeson and Jayawickreme 2015).
Taken together, these approaches demonstrate the potential of MoE for personalization, but they lack
mechanisms for dynamic personality activation driven by conversational context. None integrates
psychological theories such as Trait Activation Theory to determine when and how traits should be
expressed. This gap between computational design and theory-driven personality expression remains
a key barrier to developing LLMs with social-emotional intelligence. Our work addresses this gap
by incorporating psychological theory into both the design of the MoE architecture and the expert
routing mechanism.
Human-Centric LLM Development. Recent advances in conversational AI increasingly emphasize
user experience metrics alongside functional performance (Wang et al. 2024a, √áalƒ±k and Akku¬∏s 2025).
For example, Mixture-of-Personas (Bui et al. 2025) uses a prompting method, pairs the user query
with similar response examples to tailor the behavior of LLMs for the users. DialoGPT (Zhang
et al. 2020) utilizes a large dataset from Reddit to generate responses that closely mimic human
conversation. Similarly, the User-Centric Multi-Intent Benchmark (URS) emphasizes the critical
importance of evaluating LLMs from a user experience perspective, measuring not only accuracy but
also the users‚Äô intent competence (Wang et al. 2024a). Several studies also acknowledge the absence
of user experience-specific optimization, such as the inadequate empathy in general large language
models (LLMs), and focus on specialized fine-tuning to offer more effective support in conversation
contexts (Zhang et al. 2025b, Xu et al. 2025). However, when fine-tuning is guided by a single
objective focused on empathy, it may lead to the forgetting of general-purpose knowledge (Kotha et al.
2024). Recent work also shows that training language models to be warm and empathetic can reduce
their reliability (Ibrahim et al. 2025). Such degradation in general performance and model safety
raises important concerns about the reliability of LLMs in real-world human-facing applications.
Research Gaps: Our analysis reveals two critical methodological gaps in existing work: First, current
optimization paradigms for LLMs predominantly focus on enhancing general-purpose reasoning,
factual accuracy, and task completion capabilities. While effective in improving benchmark perfor-
mance, these approaches largely overlook the development of emotional intelligence, particularly
the nuanced and context-sensitive modeling of personality and emotion. Existing techniques, such
as instruction tuning, reinforcement learning with human feedback, or prompt engineering, are not
designed to support dynamic adaptation of communicative styles or the expression of consistent
personality traits across varying social contexts. There is an emerging need for principled frameworks
to support the development of emotional intelligence in social interactions during the post-training
stage of LLMs. Our theory-driven and technically novel approach aims to fill this gap.
5


--- Page 6 ---
User
Query
Social
Task
Trait-Relevant Cues
O
C
E
A
N
Activated Traits
Persona
Based
Training
Data
Persona
CoT
(b) Data generation process based on activated traits derived
from different cues.
User
Query
LLM
O C E A N
Situation-
Aware
Response
(a) Persona-MOE Model: dynamically activate internal personality 
traits for improved responses. 
PersonaFuse: Theory-Based Model Architecture Design and Training Data Generation Process
FFM
FFM
TAT
TAT
What is the appropriate
personality for this
query/situation?
Figure 2: Our proposed theory driven PersonaFuse framework (a) Persona-MoE, the LLM architec-
ture and (b) Person-CoT, the training data generation process.
Second, current LLM post training objectives are largely grounded in computational considerations,
such as optimization efficiency and benchmark performance, while overlooking insights from estab-
lished psychological theories that have long guided human behavior modeling. Our work bridges
this gap by introducing a psychologically informed design framework that integrates trait-based
personality theory with modern LLM architectures, offering a new pathway for aligning AI behavior
with established social science understanding (Abbasi et al. 2024).
3
THEORY-DRIVEN DESIGN: THE FIVE-FACTOR MODEL AND TRAIT
ACTIVATION THEORY
Our design draws upon two established psychological theories that address both structural and
dynamic aspects of personality. Figure 2 illustrates how these theoretical foundations systematically
inform our system architecture.
3.1
FIVE-FACTOR MODEL (FFM)
The Five-Factor Model (McCrae and John 1992) provides a comprehensive taxonomy of personality
through five core dimensions: Openness (O), Conscientiousness (C), Extraversion (E), Agreeableness
(A), and Neuroticism (N). The five dimensions capture different behavioral tendencies: Openness
reflects intellectual curiosity and creativity; Conscientiousness encompasses self-discipline and
organization; Extraversion captures sociability and assertiveness; Agreeableness reflects cooperation
and empathy; Neuroticism indicates emotional instability and stress reactivity. Prior research in IS
has also adopted the FFM as a gold standard for personality labeling and behavioral prediction (Yang
et al. 2023), to understand technology use (Devaraj et al. 2008), and to shed light on the effectiveness
of word-of-mouth (Adamopoulos et al. 2018). Here, we adopt the FFM for two main design reasons:
(1) its dimensions have been shown to predict concrete behavioral patterns across contexts (Barrick
et al. 2001, Ozer and Benet-Martinez 2006), and (2) it provides a validated and widely accepted
framework for modeling individual personality traits, enabling principled integration of personality
into LLM behavior (Chen et al. 2024a).
3.2
TRAIT ACTIVATION THEORY (TAT)
Trait Activation Theory (Tett and Burnett 2003) explains how situational cues, such as social roles or
task demand, trigger the expression of trait-relevant behaviors. Unlike static trait models that treat
personality as consistently expressed across contexts, TAT emphasizes that the same trait may be
differentially activated depending on situational strength. In other words, a trait may facilitate or
hinder performance depending on whether the context aligns with its expression.
For example, in open-ended scenarios such as creativity tasks that require imagination and flexibility,
high openness may be most suitable, while high conscientiousness may actually hinder creative
6


--- Page 7 ---
Table 1: Task-specific personality trait correlations: Positive and negative relationships between Big
Five traits and performance. ‚Äô-‚Äô indicates no significant negative correlations reported.
Task
Positive Traits
Negative Traits
Literature
Creativity Task
Openness
Conscientiousness
Jir√°sek and Sudzina (2020)
Math Ability
Conscientiousness, Extroversion
Neuroticism
Homayouni (2011)
Social-cognitive Ability
Agreeableness
-
Nettle and Liddle (2008)
Customer Service
Conscientiousness, Agreeableness
Neuroticism
Mount et al. (1998)
Therapist
Conscientiousness, Agreeableness
Neuroticism, Extroversion
Engvik (1999)
output. Similarly, for a counseling or therapist role, high conscientiousness and agreeableness are
generally preferred, whereas high neuroticism may be undesirable. In contrast, for tasks that require
logical thinking and precision, such as mathematical problem-solving, high conscientiousness and
extroversion are helpful, but neuroticism is often negatively associated with performance. Table 1
presents several example tasks together with the personality traits that are preferred for effective task
performance.
3.3
DESIGN IMPLICATIONS
Integrating the FFM and TAT provides a robust theoretical framework that guides the LLM design.
It shows a (potentially) good response personality for a given context/situation. These theoretical
insights motivate the following two key design innovations:
Persona-Aware LLM Architecture:
‚Ä¢ FFM: Informed by FFM, we design personality adapters within the LLM architecture that
correspond to specific combinations of the Big Five traits. These adapters, implemented
as a mixture-of-experts, capture the stylistic and linguistic features associated with each
personality dimension, allowing for precise and fine-grained control over the model‚Äôs
personality expressions.
‚Ä¢ TAT: Drawing on TAT, we implement a dynamic router network that activates the appropriate
personality adapters based on the input context. The router evaluates situational cues and
determines which traits should be expressed, enabling the model to adapt its personality
dynamically in real-time interactions.
Theory-Guided Data Generation:
‚Ä¢ FFM: Based on FFM, we synthesize a diverse set of training data that captures a broad
spectrum of personality expressions across different contexts.
‚Ä¢ TAT: Guided by TAT, we associate specific situational contexts with the activation of relevant
personality traits. For example, in scenarios requiring creativity, high Openness and low
Conscientiousness are emphasized (Jir√°sek and Sudzina 2020). By embedding situational
cues into our data generation process, we create contextually appropriate responses that
reflect realistic trait manifestations.
Through this theory-driven design, we seek to develop an LLM that is capable of dynamic persona
calibration, enhancing the social and emotional intelligence while maintaining general intelligence
and response harmlessness.
4
PROPOSED FRAMEWORK: PERSONAFUSE
Building on the theoretical foundations discussed in the previous section, we now present PersonaFuse,
a post-training framework that enhances the situational awareness of LLMs by dynamically adapting
their responses based on inferred personality requirements of the context.
Problem Definition. Given a base language model M and an input query q, our goal is to post-
train M into an enhanced model M + that can generate a response r = M +(q) appropriate to the
7


--- Page 8 ---
Multi-Head 
Attention
Add & Norm
Feed Forward
Add & Norm
Hidden States
High O
Low O
High C
High E
High A
High N
Low C
Low E
Low A
Low N
Openness
Conscientiousness
Extraversion
Agreeableness
Neuroticism
Personality
Experts
!!
!!
Query !!
Persona
Encoder f"
 #"
 ##
 #$
 #%
 #&
 #'
 #(
 #)
 #*
 #"+
Expert
Embedding
Base
Model
Similarity
$ %!
"+
!,"
Situation-Aware Router
$ %!
"+
!,"
$"(&!)
‚Ñé
Persona
Embedding
Query ùëû!
Task Cue
Social Cue
Task Activated
Persona
Social Activated
Persona
Persona-CoT
Response
Trait Activation
Vector
(b) Data Generation
(a) Model Architecture: Persona-MoE
10 LoRA from Different Persona
High O
LLM
LLM
High C
‚Ä¶
1. Persona LoRA Warm-up Stage
2. Router Network Training Stage
Persona
Encoder f!
ùëì!(ùëû")
Expert
Embedding
 ùëí#
 ùëí$
 ùëí%
 ùëí&
 ùëí'‚Ä¶
3. Integration Stage
End to End Training
Query ùëû"
LLM
Router
Adapters
(c) Three-Stage Situation-Aware Training Pipeline
Figure 3: System architecture of PersonaFuse. The framework consists of three main components:
(a) a Persona Mixture of Experts (Persona-MoE) architecture for contextual personality expression,
(b) a Trait Activation Theory-guided data synthesis process generating responses across different Big
Five combinations. (c) A three-stage situation-aware training pipeline.
situational context of q and aligned with relevant personality traits. For example, in an educational
context, the model may express high conscientiousness by offering patient and structured explanations.
In a creative brainstorming session, it may activate high openness by exploring unconventional ideas
and inviting novel contributions. Our aim is to equip the model with this type of context-sensitive
behavioral flexibility through a post-training framework that enables dynamic trait activation. At
the same time, we hope that this post-training does not compromise the model‚Äôs general language
generation capabilities or alignment with safety constraints.
Design Overview. PersonaFuse comprises three key components: (1) a mixture-of-experts (MoE)
architecture, Persona-MoE, in which each expert corresponds to a specific personality trait (e.g., high
openness, low neuroticism); (2) a training data synthesis pipeline, Persona-CoT; and (3) a multi-stage
training pipeline. Figure 3 provides an overview of PersonaFuse. We now detail each component and
the training procedure below.
4.1
PERSONA-AWARE MIXTURE-OF-EXPERTS ARCHITECTURE: PERSONA-MOE
We propose a Persona-Aware Mixture-of-Experts (Persona-MoE) architecture to enable LLMs to
express diverse personality traits adaptively across different situations. MoE is a widely used
architecture in large language models, typically designed to improve computational efficiency or
to specialize experts for different linguistic or domain-specific tasks (Feng et al. 2024, Jiang et al.
2024a). However, most existing MoE designs focus on token-level routing or task decomposition,
rather than capturing high-level behavioral variation such as personality adaptation.
In this work, we adopt a different perspective: our design rationale is that MoE provides a natural
fit for modeling personality diversity, as it allows us to assign distinct experts to different ends of
personality trait dimensions. By associating each expert with a specific behavioral tendency (e.g.,
high agreeableness or low neuroticism), we enable the model to adapt its response style based on
contextual signals. The router selectively activates relevant experts based on the input context via
8


--- Page 9 ---
a learnable weighting mechanism, enabling the LLM to express personality traits that align with
situational demands in a modular and interpretable manner.
Personality Experts. Motivated by FFM, we design ten specialized experts {Ei}10
i=1, each corre-
sponding to one end of a personality trait spectrum (e.g., high openness, low conscientiousness).
Rather than using a single expert to model each trait dimension, we represent both the high and
low poles separately. This design reflects the psychological insight that the two ends of a trait often
correspond to qualitatively different behavioral tendencies. For example, high openness is associated
with imagination and curiosity, whereas low openness reflects a preference for routine and convention.
By modeling them as distinct experts, we enable the system to express these divergent behaviors
more explicitly and flexibly.
To implement the experts efficiently, we adopt the widely used Low-Rank Adaptation (LoRA)
modules (Hu et al. 2022), which insert trainable low-rank matrices into the base LLM‚Äôs attention
and feed-forward layers. Specifically, each expert‚Äôs update is parameterized as ‚àÜW = BA, where
B ‚ààRd√ór and A ‚ààRr√ók. Here, W ‚ààRd√ók is a weight matrix in the base LLM (e.g., in attention
or feed-forward layers), r is the low-rank bottleneck dimension, and r ‚â™min(d, k).
For each expert, we introduce a set of learnable expert embeddings {ei}10
i=1, where each ei ‚ààRhe
is associated with a corresponding expert Ei (e.g., high openness, low neuroticism), and he is the
embedding dimension. These embeddings represent the characteristic behavioral tendencies modeled
by each expert and collectively define a personality embedding space.
Situation-Aware Router. In Mixture-of-Experts architectures, a router is responsible for selecting
which experts to activate for a given input. Prior work typically adopts either random or task-agnostic
routers, or neural routers that are trained end-to-end to optimize task performance (Feng et al. 2024,
Kim et al. 2025). For instance, random routers uniformly sample a subset of experts to reduce
computational cost, while learned routers often rely on lightweight neural networks to predict expert
weights based on local input features such as token embeddings.
In contrast, our setting is different from conventional token-level or purely task-driven routing. Since
our goal is to control high-level response behavior grounded in Trait Activation Theory, we propose
a situation-aware router R guided by the inferred personality requirements of the input context.
The router determines a probability distribution w = [w1, ..., w10] over the ten personality experts,
where wi ‚àà[0, 1] and P10
i=1 wi = 1. The router R consists of two key components:
Persona Encoder.
At the core of the router is a persona encoder fŒ∏, which maps the input query q
to a dense vector h = fŒ∏(q) representing the inferred personality profile suitable for responding to
the query. This embedding h ‚ààRhe plays a pivotal role in our framework: we use it to guide the
routing of expert activations in Persona-MoE. Importantly, this persona encoder is not frozen during
training, and its parameters are updated end-to-end to better capture situational context and improve
personality inference.
In our design, we use a lightweight LLM-based encoder, such as Qwen2.5-0.5B (Yang et al. 2024),
as the implementation of fŒ∏. We choose to use LLM-based encoders instead of traditional encoders
like BERT (Devlin et al. 2019), for two key reasons: first, LLM-based encoders have demonstrated
superior semantic reasoning capabilities, particularly in understanding nuanced queries (Wang et al.
2024b); second, they support longer input contexts, which is essential for queries involving extended
narratives or complex conversational structures.
Experts Routing.
To determine the relevance of each expert to the input query q, we compute
cosine similarity between h and each ei, followed by temperature-scaled softmax to obtain mixture
weights w = [w1, ..., w10]:
wi =
exp(cos(h, ei)/œÑ)
P10
j=1 exp(cos(h, ej)/œÑ)
,
(1)
where œÑ is a temperature hyperparameter controlling the sharpness of the distribution. Lower values
of œÑ encourage focused selection of a few dominant experts, while higher values produce more
distributed combinations. In our experiments, we set œÑ = 1.0 because it provides a balanced
weighting that allows the router to combine multiple relevant experts without focusing too narrowly
or too broadly.
9


--- Page 10 ---
In summary, the proposed Persona-MoE design aims to adaptively express trait-aligned behaviors in
its response generation. For example, when given a query like ‚ÄúI‚Äôve been feeling very anxious lately
and don‚Äôt know what to do,‚Äù the persona encoder captures the emotional sensitivity and support-
seeking intent of the query, and encodes this into the persona embedding. Based on this embedding,
the router assigns higher weights to personality experts associated with high agreeableness and low
neuroticism, encouraging a calm and empathetic response.
4.2
TRAINING DATA GENERATION PROCESS: PERSONA-COT
Having introduced the model architecture, we now turn to the construction of training data. Following
the current practice in LLM post-training (Huang et al. 2025, Li et al. 2025a), we use a large language
model to synthesize training data. This approach provides scalable, high-coverage supervision at a
fraction of the cost of human annotation, and helps the model retain strong generalization capability
after post-training (Gilardi et al. 2023, Gan and Liu 2025).
However, as illustrated in Figure 4, directly prompting an LLM to produce responses often results in
overly generic and context-insensitive outputs that fail to reflect the nuanced personality requirements
of the situation. If such responses were used to fine-tune Persona-MoE, the router would have little
signal to learn how to adapt personality expression based on contextual requirements, resulting in low
emotional intelligence in generated outputs.
To address this, we propose a Persona Chain-of-Thought (Persona-CoT) procedure that explicitly
guides the data generation process. Chain-of-Thought (CoT) prompting (Wei et al. 2022) elicits
step-by-step reasoning from an LLM, and has been shown to improve response quality in complex
tasks. In our context, CoT may improve response quality by leveraging the inferred situational cues
and corresponding personality traits.
Our proposed Persona-CoT data generation process consists of three stages, as shown in Figure 4.
Guided by the Trait Activation Theory, given a user input query, we first infer the social cues
and task cues embedded in the context. Next, we identify the personality traits most relevant for
responding to the query based on the detected cues. Finally, we use the inferred cues and traits to
generate a response aligned with the intended personality profile. This pipeline produces high-quality,
trait-labeled examples that serve as supervision signals for post-training our Persona-MoE model.
Stage 1: Situation Cues Detection.
Given an input query q, we prompt a large language model
to extract two types of situational cues: social cues and task cues. Social cues are indicators in the
interaction context such as tone, emotional state, or social norms. Task cues are characteristics of the
task such as complexity, required skills, and goal orientation. According to Trait Activation Theory,
both social and task cues can trigger the expression of specific personality traits. These cues therefore
provide contextual signals for activating appropriate personality traits in LLM responses. As shown
in Table 2, for the query ‚ÄúRecently I had a shift at work canceled. I was very nervous that the whole
week‚Äôs pay would be lost‚Äù, the inferred social cue reflects anxiety and a need for empathy, while the
task cue requires explaining workplace policies clearly and reassuringly.
Stage 2: Trait Identification.
Based on the inferred social and task cues, we identify the personality
traits needed to generate an appropriate response. Take the user query in Table 2 as an example,
the social cue shows anxiety and uncertainty about work and suggests high neuroticism and high
agreeableness, while the task cue involves explaining workplace policies and providing reassurance
and indicates high agreeableness and high conscientiousness.
To encode the identified traits, we define a trait activation vector p ‚àà{0, 1}10, where each dimension
corresponds to a persona expert in Persona-MoE. Specifically, pi = 1 indicates that the i-th expert
should be activated for the given query, and pi = 0 otherwise.3 This vector is stored during data
generation and later serves as supervision for training the situation-aware router in our Persona-MoE
model.
3The ten dimensions correspond to: high openness, low openness, high conscientiousness, low conscien-
tiousness, high extraversion, low extraversion, high agreeableness, low agreeableness, high neuroticism, and low
neuroticism.
10


--- Page 11 ---
Table 2: Illustration of the three-step data generation process. Input User Query: ‚ÄúRecently I had a
shift at work canceled. I was very nervous that the whole week‚Äôs pay would be lost.‚Äù
Step
Persona-CoT
(1) Analyze the Situa-
tion Cues
Social Cue: The user‚Äôs message indicates feelings of anxiety
and uncertainty about their work situation, suggesting a need for
empathy and reassurance from the assistant.
Task Cue: The assistant needs to provide support and possibly
guidance on understanding workplace policies regarding cancel-
lations and pay, indicating a task that requires careful explanation
and clarity.
(2) Identify Traits
Based on social cue:
‚Ä¢ High Neuroticism: (sensitive/nervous)
‚Ä¢ High Agreeableness: (friendly/compassionate).
The user‚Äôs message highlights their sensitivity and nervousness
regarding their work situation, which aligns with high neuroti-
cism. Additionally, the assistant may need to display agreeable-
ness to provide comfort and support to the user.
Based on task cue:
‚Ä¢ High Agreeableness: (friendly/compassionate)
‚Ä¢ High Conscientiousness: (efficient/organized)
These traits are necessary for the assistant to effectively com-
municate and support the user during a potentially stressful
situation.
(3) Formulate the Re-
sponse
The response generation begins, leveraging the identified traits
(High Neuroticism, High Agreeableness, High Conscientious-
ness) to craft a persona-aligned output. The assistant integrates
an empathetic and supportive tone (from Agreeableness), sensi-
tivity to the user‚Äôs anxiety (from Neuroticism), and structured
guidance (from Conscientiousness) to address both emotional
and practical needs.
Stage 3: Persona-Based Chain of Thought.
In the final stage, we combine the inferred situational
cues (social and task cues) and the identified traits as contextual information to prompt the LLM for
response generation. Traditional Chain-of-Thought focuses on decomposing a reasoning problem into
sequential intermediate steps (e.g., by adding ‚ÄúLet‚Äôs think step by step‚Äù) (Wei et al. 2022), but does not
explicitly incorporate personality or social-behavioral factors. In contrast, our Persona-CoT augments
the reasoning chain with psychologically grounded elements: the process explicitly reasons about
situational cues, maps them to personality traits via Trait Activation Theory, and uses these traits to
guide the final response. As shown in Figure 4, Persona-CoT yields contextually appropriate outputs
that better match the inferred personality requirements. In summary, Persona-COT produces a dataset
of tuples (q, r, p), where q is the user query, r is the generated response, and p is the associated trait
activation vector representing which personality experts should be activated. Appendix C shows
examples of generated Persona-CoT data.
4.3
MULTI-STAGE TRAINING PIPELINE
We now describe the training pipeline for Persona-MoE using the synthesized Persona-CoT dataset
(q, r, p). The trainable parameters include the LoRA modules for the ten persona experts {Ei}10
i=1,
the learnable expert embeddings {ei}10
i=1, and the router network, which incorporates the persona
encoder fŒ∏ that maps queries to persona embeddings. The base LLM parameters are kept frozen
throughout training to preserve its general language capabilities.
11


--- Page 12 ---
Recently, I had a shift at work
cancelled. I was very nervous 
that the whole week‚Äôs pay would 
be lost.
User
The user‚Äôs message indicates 
feelings of anxiety and 
uncertainty about their work 
situation, suggesting a need for 
empathy and reassurance from 
the assistant.
Social Cue
Agreeableness: High
Neuroticism: High
‚Ä¶
Agreeableness: High
Conscientiousness: High
‚Ä¶
Below is a user instruction:
{user prompt}
---
Steps 1: Analyse the Social Cue: ‚Ä¶
Steps 2: Analyse the Task Cue: ‚Ä¶
Step 3: Identify Traits: ‚Ä¶
Step 4: Formulate the Response:
##Response:
##Persona Behaviour Rationale:
Task Cue
Activated Traits
Activated Traits
I can understand how worried and nervous you must feel right now. It‚Äôs completely normal to be concerned when shifts get cancelled, especially if it affects your weekly 
earnings. However, let's try to take it one step at a time. Can you tell me a bit more about what happened? Sometimes companies have unexpected changes, but there might be a 
specific reason or policy in place that could give us a better idea of what to expect. Would you like to know more about your company‚Äôs policies regarding shift cancellations 
and pay?
It can be really stressful and worrisome when you‚Äòre worried about losing pay due to a cancelled shift. Here are a few things that might help you feel more at ease:
1. Communicate with your employer: If possible, reach out to your supervisor or HR representative as soon as you find out‚Ä¶.
2. Review your contract: If you have a contract or employment agreement, review it to see if it outlines any provisions for lost pay due to cancellation of shifts. ..
Persona CoT
Naive 
Response
Persona-
based 
Response
The assistant needs to provide support 
and possibly guidance on 
understanding workplace policies 
regarding cancellations and pay,
indicating a task that requires careful 
explanation and clarity.
Figure 4: The naive response (top) is directly generated by LLM. Our proposed approach Persona-
CoT (bottom) implements Trait Activation Theory with Chain of Thought reasoning, producing more
contextually appropriate responses compared to naive LLM-based generation.
Training proceeds in three stages. In the first stage, we warm up each expert by training its LoRA
adapter separately with the standard language modeling objective, using the subset of Persona-CoT
data where the corresponding personality trait is activated. In the second stage, the experts are
frozen and only the router network is trained using a contrastive loss to align each query‚Äôs persona
embedding with its corresponding expert embeddings. Lastly, we jointly train the router and LoRA
experts end-to-end using both the language modeling loss and the auxiliary losses from earlier stages.
This multi-stage training strategy ensures that experts first acquire personality behaviors before the
router begins combining them, preventing unstable optimization caused by noisy early routing signals.
Figure 3 illustrates the multi-stage training pipeline.
Stage 1: LoRA Warmup.
We first train specialized LoRA experts separately to express different
personality traits. Given training data {(q, r, p)}, where p ‚àà{0, 1}10 is a trait activation vector, we
partition the dataset into ten disjoint subsets {P1, . . . , P10}, each corresponding to one personality
trait. For each expert Ei, we use only the subset {Pi = (q, r) | pi = 1} to train its LoRA adapter
with the language modeling objective Llm, which is the cross-entropy loss, measuring the divergence
between the predicted token probabilities and the ground-truth tokens in the target response. For a
sequence of length T, it is calculated as:
Llm = ‚àí
T
X
t=1
log p(rt|q, r<t),
(8)
where p(rt|q, r<t) represents the probability assigned by the model to the correct word rt at position
t, given the input query q and the preceding words r<t.
This produces a personality-specific LoRA Li such that the adapted parameters W + Li generate
responses aligned with trait i, where W denotes the frozen pre-trained weights of the base LLM.
This stage serves as a warm-up phase that allows each expert to specialize in one personality style
before introducing the routing mechanism.
Stage 2: Router Network Training.
The second stage focuses on training the router network to
dynamically map input queries to appropriate personality activations. In Persona-MoE, the router
network comprises two trainable components: a persona encoder fŒ∏ and a set of learnable expert
embeddings {ei}10
i=1. We propose to use a contrastive learning objective in this stage to align persona
embeddings with their corresponding expert embeddings:
12


--- Page 13 ---
Lcontrastive = 1
B
B
X
i=1
 X
j‚ààPi
(1 ‚àísij)2 +
X
j‚ààNi
max(0, sij ‚àím)2

(3)
Here, B denotes the batch size, and sij is the cosine similarity between the persona embedding
hi = fŒ∏(i) of query i and the expert embedding ej, defined as: sij = cos(hi, ej) =
hi¬∑ej
‚à•hi‚à•‚à•ej‚à•. The set
Pi = {j | pi,j = 1} includes the indices of positive experts for query i, corresponding to personality
traits activated in the personality vector pi (i.e., where the j-th component pi,j = 1). Conversely,
Ni = {j | pi,j = 0} includes the negative experts, which are irrelevant to the query‚Äôs required
personality expression. The margin parameter m enforces a minimum separation between positive
and negative pairs, enhancing the router‚Äôs ability to distinguish between relevant and irrelevant experts.
The high-level idea behind this contrastive learning stage is to teach the router‚Äôs persona encoder to
produce embeddings that are close to the embeddings of relevant experts (positive traits) and far from
those of irrelevant experts (negative traits).
To improve training robustness of the router network, we introduce another trait consistency objective.
This is to ensure that queries requiring the same personality traits are represented similarly by
the persona encoder. During training, each batch is constructed so that all queries share the same
personality activation vector p. By minimizing the pairwise dissimilarity of persona embeddings
within such a batch, the router network fŒ∏ learns to map different query scenarios with identical trait
requirements to nearby points in the embedding space, leading to more consistent and reliable routing
decisions. Specifically, we define the trait consistency loss as:
Ltrait =
2
B(B ‚àí1)
X
1‚â§i<j‚â§B
(1 ‚àícos(hi, hj)) ,
(5)
where hi = fŒ∏(i) and hj = fŒ∏(j) are the persona embeddings of queries i and j within the same
batch. This formulation computes the average pairwise dissimilarity over all unique pairs (i, j) in the
batch, with the factor
2
B(B‚àí1) normalizing by the number of such pairs (B(B ‚àí1)/2).
The combined training objective for the router training is thus:
Lrouter = Lcontrastive + Œ≤Ltrait
(6)
where Œ≤ are weighting coefficients. In our experiment, Œ≤ are set to 1.0 for balanced optimization.
In summary, this stage freezes the persona experts‚Äô LoRA parameters and trains the router network
to (1) accurately select relevant personality experts for each query and (2) make consistent routing
decisions for queries with similar personality requirements, ensuring stable and reliable personality
routing.
Stage 3: Joint Training. The final stage jointly optimizes all components to align the router network
with the personality experts while preserving high-quality response generation. The objective
combines the first stage language modeling loss with the second stage router network loss:
Ljoint = Llm + Œ≥Lrouter
(2)
Œ≥ is a hyperparameter that adjusts the trade-off between response quality and personality adaptation.
In the experiment, Œ≥ is set to 0.2.
5
PERSONAFUSE IMPLEMENTATION DETAILS
We provide implementation details for PersonaFuse.
Training Data Generation with Persona-CoT. To increase the diversity of training queries, we
compile data from multiple publicly available sources, including:
‚Ä¢ ShareGPT 4, a collaborative dataset containing real human-AI conversations;
‚Ä¢ PersonaHub (Ge et al. 2024), a personality-driven dialogue dataset;
4https://huggingface.co/datasets/RyokoAI/ShareGPT52K
13


--- Page 14 ---
‚Ä¢ WildChat (Zhao et al. 2024c), a dataset of user-ChatGPT conversations;
‚Ä¢ Infinity-Instruct (Zhao et al. 2024a), a synthesized instruction-following dataset.
We randomly sample a total of 100,000 queries from these public datasets to generate correspond-
ing responses. Falcon3-10B-Instruct (Team 2024) serves as the backbone LLM in Persona-CoT,
producing the inferred social cues, task cues, personality traits, and final responses. After filtering
out outputs that do not meet our format requirements, such as cases where the personality vector p
contains all zeros, we retain 98,838 valid training instances. Two training examples are provided in
Appendix C.
Training Details. Our framework comprises three key training components: (1) ten LoRA experts,
(2) the persona encoder (Qwen2.5-0.5 (Yang et al. 2024)), and (3) the representative embeddings for
experts. In the main experiment, we employ Llama-3.1-8B as our foundation model, chosen for its
established performance and reliability in both academic research and industrial applications. For
each LoRA component, we set the rank to 8 and alpha to 16, where rank determines the dimension of
the low-rank adaptation matrices, and alpha controls the scaling factor for updates. Detailed training
hyperparameters for each module are provided in Appendix A.
6
EXPERIMENTAL EVALUATION
We conduct comprehensive experiments to evaluate PersonaFuse across multiple dimensions, in-
cluding social-emotional intelligence, general reasoning ability, response safety, and downstream
applications. We first describe the baseline models and evaluation datasets, followed by a detailed
analysis of experimental results.
6.1
BASELINE MODELS
In the experiments, we aim to examine the theory-driven design of PersonaFuse, focusing on its two
main innovations: Persona-MoE for model architecture and Persona-CoT for data generation. To
ensure fair and controlled comparisons, we fix the base LLM as Llama-3.1-8B and vary only the
post-training techniques and training data. We do not include models built on different base LLMs
(e.g., GPT-4 or Llama-70B), as differences in model scale and pre-training data would confound the
comparison.
We consider the following baselines, summarized in Table 3. (1) Direct-Finetuned: standard super-
vised fine-tuning where the training data is constructed by directly taking outputs from Falcon3-10B-
Instruct (Team 2024) without any prompting to guide generation. The input queries are identical to
those used in Persona-COT, and the same Falcon3-10B-Instruct model is employed for data generation
in both settings to ensure fairness. (2) Human-Like-Finetuned: a recent approach that trains LLMs to
generate casual, conversational responses (√áalƒ±k and Akku¬∏s 2025). Training data is produced with
a fixed template (e.g., ‚ÄúYou‚Äôre here to engage in friendly, informal conversations, just like chatting
with a friend.. . ‚Äù), and the base LLM is then fine-tuned on this dataset. (3) Random Route 1, 2, and
5: variants that share the same model architecture as PersonaFuse but replace the MoE router with
random expert activation. In Random Route 1, a single expert is randomly selected and assigned
wi = 1 (others set to 0). In Random Route 2 and 5, two or five experts are randomly chosen and
assigned equal weights (wi = 0.5 or wi = 0.2), with the rest set to 0.
The rationale for selecting these baselines is twofold. First, Baseline and Human-Like-Finetuned
represent a standard method for aligning an LLM with downstream tasks, i.e., supervised fine-
tuning on human annotated datasets. This comparison allows us to assess our theory-driven data
generation and LLM architectural adaptation. Second, the Random Route baselines reflect common
MoE activation strategies, allowing us to directly compare our expert routing design with random
activation.
6.2
EVALUATION BENCHMARKS
Our evaluation primarily focuses on the social-emotional intelligence of LLMs. However, post-
training for a specific capability may introduce trade-offs in general intelligence and model safety
14


--- Page 15 ---
Table 3: Overview of Model Variants and Their Specifications. The Training Data column indicates
the data generation approach used for each model. Route specifies the routing mechanism (if any)
employed by the model. Experts show the number of specialist models available, and the Training
Pipeline indicates the training methodology used. ‚Äô-‚Äô denotes that the component is not applicable.
Model Name
Training Data
Route
Experts
Post-training
Direct-finetuned
Direct Generation
-
-
SFT
Human-Like-finetuned
Human-Like Generation
-
-
SFT
Random Route 1
Persona-CoT
Random activate 1 expert
10 experts
PersonaFuse
Random Route 2
Persona-CoT
Random activate 2 experts
10 experts
PersonaFuse
Random Route 5
Persona-CoT
Random activate 5 experts
10 experts
PersonaFuse
PersonaFuse
Persona-CoT
PersonaFuse
10 experts
PersonaFuse
Table 4: Summary of Evaluation Datasets.
Dataset
Dimension
Description
Metric
EQ-Bench (Paech 2023)
Social-
Emotional
Intelligence
Emotional
understanding
via
multiple-choice questions
EQ-Bench Score
EmoBench
(Sabour
et al. 2024)
Social-
Emotional
Intelligence
Emotional understanding and ap-
plication tasks via multiple-choice
questions
Accuracy
ToMBench (Chen et al.
2024b)
Social-
Emotional
Intelligence
Social cognition based on Theory of
Mind with 8 tasks and 31 skills
Accuracy
GPQA
(Rein
et
al.
2024)
General
Abili-
ties
Graduate-level QA with three sub-
sets: Diamond, Main, Extended
Accuracy
GSM8k (Cobbe et al.
2021)
General
Abili-
ties
Grade school math problems
Accuracy
Arena-Hard-Auto-v0.1
(Li et al. 2025b)
General
Abili-
ties
Open-ended QA with real-world
user queries
GPT-4 Judges
SafetyBench
(Zhang
et al. 2024)
Model Safety
Safety evaluation across seven as-
pects.
Accuracy
Shop MMLU (Jin et al.
2024b)
Practical Appli-
cations
Review-based Q&A for customer
service interactions
Semantic Similarity
MentalChat16K
(Xu
et al. 2025)
Practical Appli-
cations
Counseling capabilities in therapeu-
tic communication
GPT-4 Judges
(Kirkpatrick et al. 2017). To provide a comprehensive assessment, we also evaluate models on
benchmarks of general language capability and safety. Beyond standard NLP benchmarks, which
mainly emphasize response accuracy, we further assess model performance on real-world generation
tasks. We summarize all evaluation benchmarks in Table 4.
Social-Emotional Intelligence. We examine whether the model can understand human emotions and
social cues, as this directly affects the quality of human-LLM interaction. For this dimension, we em-
ploy three benchmarks with different aspects: EQ-Bench (Paech 2023) and EmoBench (Sabour et al.
2024) for evaluating emotional intelligence. Specifically, EQ-Bench focuses on emotional understand-
ing, while EmoBench includes both emotional understanding and application tasks. ToMBench (Chen
et al. 2024b), based on the Theory of Mind, includes 8 tasks and 31 skills in social cognition. These
three benchmarks are based on multiple-choice questions. The score for ToMBench and EmoBench
is based on answer accuracy, while EQ-Bench‚Äôs score is determined by how far the answer is from
the reference response.
General Intelligence Abilities. The evaluation for general intelligence tasks includes GPQA (Rein
et al. 2024), GSM8k (Cobbe et al. 2021) and Arena-Hard-Auto (Li et al. 2025b), GPQA is a graduate-
level QA dataset with a total of 1,192 questions. GSM8K consists of 8.5K high-quality grade
15


--- Page 16 ---
EmoBench
0
10
20
30
Relative Improvement  (%)
EQBench
0
20
40
60
ToMBench
0.0
2.5
5.0
7.5
10.0
Social-Emotional Intelligence Performance
Human-Like-finetuned
Random Route 1
Random Route 2
Random Route 5
PersonaFuse
Figure 5: Performance improvements across social-emotional intelligence benchmarks over the
Direct-finetuned baseline.
school math problems created by human problem writers. GPQA and GSM8K are multiple-choice
benchmarks and the evaluation metric is accuracy. We also use a popular open-ended QA dataset that
includes real-world queries from users named Arena-Hard-Auto (Li et al. 2025b). The response is
evaluated by GPT-4 (OpenAI 2025) as it is proved to have a high correlation with human judges.
Model Safety. We evaluate if the model‚Äôs response is safe and harmless, which is an important aspect
for LLM post-training (Lu et al. 2025). We consider the well-established LLM safety benchmark
SafetyBench (Zhang et al. 2024), which includes 11,435 multiple-choice questions across seven
critical aspects: offensiveness (OFF), unfairness and bias (UB), physical health (PH), mental health
(MH), illegal activities (IA), ethics and morality (EM), and privacy and property (PP). Performance is
measured by answer accuracy across all tasks.
Downstream Applications. Lastly, we assess model performance on two real-world tasks that
require human-centric understanding. First, we evaluate customer-service interactions using Shop
MMLU (Jin et al. 2024b), a review-based Q&A dataset that measures response quality to customer
reviews. Performance is computed as the semantic similarity to ground-truth answers using a sentence
transformer5. Second, we evaluate counseling-related capabilities with MentalChat16K (Xu et al.
2025), where responses are scored by GPT-4 (OpenAI 2025) across seven professional dimensions,
including active listening and empathy. Together, these two benchmarks directly reflect LLM
performance in downstream human-centric applications.
6.3
EXPERIMENTAL RESULTS
Social-Emotional Intelligence Performance. The results are presented in Table 13 and Figure 5.
First, PersonaFuse achieves consistent improvements across all emotional intelligence benchmarks.
Specifically, it yields +37.9% on EmoBench overall (+72.7% on emotional understanding, +31.9% on
emotional application), +69.0% on EQ-Bench, and +11.9% on ToMBench (+17.1% on task-oriented,
+11.9% on ability-oriented evaluations), compared to the direct-finetuned baseline. Random routing
variants (1/2/5), by contrast, show only limited improvements over the Direct-Finetuned baseline. This
indicates that simply increasing expert diversity without context‚Äìtrait alignment does not substantially
enhance performance. In contrast, our proposed expert routing mechanism is essential for activating
the most relevant persona experts based on situational demands, which in turn drives the observed
performance gains.
These improvements are particularly noteworthy given the nature of the evaluation tasks. EmoBench
evaluates models‚Äô ability to understand and apply emotional knowledge in realistic scenarios, requir-
ing nuanced emotional reasoning capabilities. EQ-Bench requires predicting emotional intensities
in complex dialogue contexts. PersonaFuse‚Äôs performance on both benchmarks indicates that our
theory-guided design enables contextual emotional reasoning rather than pattern memorization.
General Intelligence and Model Safety Performance. Table 14 and Figure 6 report results on
general intelligence and model safety benchmarks. PersonaFuse not only preserves but also enhances
5https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
16


--- Page 17 ---
#'
*'*
$






&)*(.&',(.*+%#'
'*%',%%#!'* (*&'
 ,/'"
0	
0
0
0
0


&)*(.&',(.*+%#'
(% ,/* (*&'
-&'#$ #',-'
'(&(-,	
'(&(-,
'(&(-,
*+('-+
 ')!$%#)!"'
Figure 6: Performance improvements on general intelligence and safety benchmarks over the Direct-
finetuned baseline.
Shop MMLU
MentalChat
‚àí10
‚àí5
0
5
10
15
Relative Improvement  (%)
Performance on Customer Service and Mental Health Counseling Tasks
Human-Like-finetuned
Random Route 1
Random Route 2
Random Route 5
PersonaFuse
Figure 7: Performance improvements on practical application tasks in customer service and mental
health counseling domains, over the Direct-finetuned baseline.
general capabilities. On GPQA, a challenging graduate-level benchmark, it achieves an overall
improvement of 9.7%. Similar gains are observed on Arena-Hard-Auto (+79.0%) and GSM8k mathe-
matical reasoning (+67.3%). For model safety, PersonaFuse improves overall performance by +1.7%,
with particularly strong gains on illegal activities (+10.6%) and unfairness/bias (+6.3%). In contrast,
other baselines suffer from performance degradation, reflecting catastrophic forgetting (Kotha et al.
2024). For example, Human-Like-Finetuned shows large drops on GPQA (-21.3% and -8.7%) and an
average decline of -9.9% on SafetyBench.
By comparison, PersonaFuse avoids such degradation and even improves performance on both
general intelligence and safety. This stems from its situational adaptation mechanism: for instance,
in safety-critical contexts the model can increase conscientiousness to avoid unsafe outputs, or
enhance agreeableness when interacting with vulnerable users. Similarly, logical reasoning tasks may
benefit from activating experts aligned with conscientiousness or openness, enabling stronger general
performance.
Overall, while PersonaFuse was primarily designed to improve social-emotional intelligence, the
results demonstrate that our theory-guided design also strengthens general intelligence and model
safety. By routing different query types to specialized experts while preserving the base model‚Äôs
core knowledge, PersonaFuse mitigates catastrophic forgetting and provides a balanced improvement
across multiple dimensions.
Performance on Customer Service and Mental Health Counseling Tasks. We next evaluate
downstream performance on two representative human-centered applications. Results are reported in
Table 16 and Figure 7.
17


--- Page 18 ---
E-Commerce Customer Service. Customer service interactions offer an ideal test case because
they demand precise personality calibration: conscientiousness is required to ensure accurate and
organized information delivery, agreeableness facilitates polite and helpful interactions, and controlled
neuroticism prevents overreactions in stressful exchanges (Mount et al. 1998). In this setting,
PersonaFuse significantly outperforms the direct-finetuned baseline, demonstrating its ability to
generate reliable and user-friendly responses. In contrast, the Random Route variants show consistent
declines of 6‚Äì8% compared to direct-finetuned, underscoring that random expert activation fails to
capture the personality alignment required for effective customer support. The Human-Like-finetuned
baseline, which focuses on casual conversational style, also underperforms direct-finetuned by 4.2%,
indicating that a generic friendly tone alone is insufficient for the nuanced demands of customer-facing
tasks.
Mental Health Counseling. Counseling conversations are even more demanding, as they require the
model to express empathy while maintaining professional balance. High agreeableness supports
compassionate responses, openness fosters non-judgmental listening, and controlled extraversion
ensures the model remains supportive without overwhelming distressed users (Engvik 1999, Chapman
et al. 2009). On the MentalChat16K benchmark, PersonaFuse improves overall performance by
13.2%, with notable gains in empathy (+14.3%) and active listening (+13.2%). These improvements
are particularly meaningful because they directly map onto dimensions central to therapeutic commu-
nication quality. Baselines, however, either degrade or yield only marginal improvements, reinforcing
the importance of dynamic, context-sensitive expert routing rather than static conversational styles.
6.4
EXPERIMENTAL RESULT IMPLICATIONS
Building on the experimental results, we highlight several implications for designing PersonaFuse.
The Role of Persona-MoE Architecture and Routing. The comparison between PersonaFuse
and Random Route variants underscores the importance of our proposed expert routing over naive
expert activation. Despite sharing identical expert architectures, Random Route models consistently
underperform, and increasing the number of randomly activated experts (from 2 to 5) does not yield
improvements and often degrades performance. In contrast, Persona-MoE and its routing is motivated
by psychological theories: ten experts motivated by the Five-Factor Model, combined with a routing
mechanism guided by Trait Activation Theory. This structure enables the router to learn contextual
cues and activate appropriate personality experts. The results suggest that theory-informed MoE and
routing strategies are more effective than ad-hoc routing in achieving context-sensitive behavior.
The Role of Persona-CoT in Data Generation. The results against Direct-Finetuned and Human-
Like-Finetuned baselines also demonstrate the effectiveness of our theory-driven data generation
method. By incorporating both social cues and task cues, Persona-CoT produces richer training
signals that capture not only the desired response but also the underlying reasoning path. Importantly,
Persona-CoT and Persona-MoE are tightly integrated: Persona-CoT provides explicit trait activation
vectors that later guide the training of persona experts and the router. This synergy between data design
and architectural design highlights that post-training effectiveness depends not only on high-quality
responses, but also on appropriate model architecture adaption.
Preservation of General Knowledge and Model Safety. A key implication from our experiments is
that improving emotional intelligence in LLMs often comes at the cost of general reasoning and safety,
as evidenced by substantial degradation in baseline models, which is an instance of catastrophic
forgetting widely documented in the literature (Kirkpatrick et al. 2017). In contrast, PersonaFuse
mitigates this trade-off. By adaptively leveraging traits such as conscientiousness and agreeableness,
the model responds more cautiously in safety-critical scenarios, leading to improved SafetyBench
performance. Similarly, for general intelligence tasks, activating appropriate traits supports stronger
reasoning ability. These findings point to a promising pathway for achieving balanced LLM alignment
across emotional intelligence, reasoning capability, and safety.
18


--- Page 19 ---
7
HUMAN EVALUATION
To assess the performance of our proposed PersonaFuse in real-world settings, we conduct a human
preference evaluation comparing PersonaFuse with leading LLMs such as GPT-4o and DeepSeek-R1-
Distill.6
7.1
EXPERIMENT SETTINGS
Data Source: We evaluate two distinct task types: logical reasoning and emotion-based dialogue.
The first assesses analytical and inference capabilities, while the second focuses on emotional
understanding and contextually appropriate response generation. For logical reasoning, we select
examples tagged as ‚Äúlogical reasoning‚Äù from the Infinity Instruct dataset (Li et al. 2025a). For
emotion-based dialogue, we use the EmpatheticDialogues dataset (Rashkin et al. 2019), which
contains conversations designed to elicit empathetic responses. From each dataset, we randomly
sample 20 examples, resulting in a total of 40 evaluation examples. Neither dataset is used in the
Persona-CoT data generation process.
Comparison Models: We compare PersonaFuse against four representative LLMs: Llama-3.1-8B-
Instruct (Dubey et al. 2024), GPT-3.5-Turbo, GPT-4o (OpenAI 2025), and DeepSeek-R1-Distill-
Qwen-14B (Guo et al. 2025). Llama-3.1-8B-Instruct is the instruction-aligned version of Llama-3.1-
8B, which also serves as the foundation model for PersonaFuse. The GPT models represent widely
used commercial systems with strong general capabilities, while DeepSeek-R1-Distill-Qwen-14B
provides a competitive open-source alternative with advanced reasoning ability.
Evaluation Procedure: The evaluation employs pairwise comparisons between PersonaFuse and
each baseline model. With 40 examples and 4 baselines, this generates 160 comparison pairs. For
each prompt, responses from PersonaFuse and one baseline are randomly labeled as "Response A" or
"Response B" to minimize position bias. Sample responses are provided in Table ??.
We recruited 40 evaluators through Prolific7, with each participant assessing 28 randomly selected
comparison pairs from the total 160 pairs. This design ensures each comparison pair receives exactly
7 annotations for statistical reliability.
Participants evaluate paired responses by selecting the preferred response and rating their confidence
on a 5-point scale. Evaluation criteria focus on two key dimensions: Perceived Usefulness (Davis
1989), measuring how effectively responses address the given task, and Social Presence (Schanke
et al. 2021), assessing whether responses feel natural and engaging.8 The human evaluation interface
is shown in Appendix E.
Evaluation Metrics: We evaluate PersonaFuse‚Äôs performance using win rate, defined as the percent-
age of examples for which participants preferred PersonaFuse‚Äôs response over that of a comparison
model. For each examples, seven independent annotators provide judgments, and the final label is
determined by majority voting.
7.2
RESULTS AND DISCUSSION
Table 5 reports the human evaluation results. On emotion-based dialogue tasks, PersonaFuse achieves
win rates of 73.0% against GPT-3.5-Turbo, 66.7% against DeepSeek-R1-Distill-Qwen-14B, 57.9%
against GPT-4o, and 73.9% against Llama-3.1-8B-Instruct. These results are notable given Person-
aFuse ‚Äôs smaller parameter size compared to other models such has GPT-4o and DeepSeek-R1. The
strong performance on emotional tasks supports our design choice of dynamic personality adaptation
for dialogue systems.
6This human preference study was reviewed and approved by our institutional ethics review board and has
been formally registered.
7https://app.prolific.com/
8We also collect participant background information to explore potential individual differences in evaluation
patterns, including Wong and Law‚Äôs Emotional Intelligence Scale (Wong and Law 2002) to measure emotional
intelligence, the Neuro-QoL Short Form (Gershon et al. 2012) to assess cognitive abilities, and self-reported
questions on algorithmic aversion covering trust in LLMs, preference for human versus LLM advice, and
willingness to use LLMs. These background measures showed no significant effects on evaluation patterns, so
our analysis focuses on the main preference comparisons.
19


--- Page 20 ---
On logical reasoning tasks, PersonaFuse obtains win rates of 56.7% against GPT-3.5-Turbo, 42.7%
against DeepSeek-R1-Distill-Qwen-14B, 36.8% against GPT-4o, and 71.9% against Llama-3.1-8B-
Instruct. While performance lags behind GPT-4o and DeepSeek-R1-Distill-Qwen-14B, this is not
unexpected: PersonaFuse has a much smaller parameter size, is not specifically trained on reasoning
tasks, and does not leverage advanced reinforcement learning methods commonly used to enhance
reasoning. Nevertheless, the consistent outperformance over Llama-3.1-8B-Instruct demonstrates
the effectiveness of our proposed framework, as both are built on the same foundation model. Taken
together, by competing with strong baselines such as GPT-4o and DeepSeek-R1, the human evaluation
study demonstrates the practical utility of PersonaFuse in human-centric LLM applications.
Table 5: PersonaFuse win rates against baseline models in human evaluations (cells with win rate >
50% are shaded light green). Results are averaged across multiple annotators.
Model
Emotion-Based (EQ)
Logical Reasoning
Overall (‚Üë)
GPT-4o
57.9%
36.8%
45.9%
DeepSeek-R1-Distill-Qwen-14B
66.7%
42.7%
52.9%
GPT-3.5-Turbo
73.0%
56.7%
65.0%
Llama-3.1-8B-Instruct
73.9%
71.9%
72.0%
8
ADDITIONAL ANALYSIS
Building on the main experimental findings, we conduct additional analyses and robustness checks to
further validate the effectiveness of PersonaFuse.
8.1
DOES THE PERSONA ENCODER LEARN TASK-SPECIFIC TRAITS?
The persona encoder is a key component of PersonaFuse, as it maps each input query into a dense per-
sona embedding that represents the inferred personality profile required for generating an appropriate
response. This embedding is central to subsequent expert routing, so it is important to verify whether
it truly captures task-specific trait information. To evaluate this, we select three classification tasks
and use the learned persona embeddings as input features, comparing their predictive performance
against embeddings from alternative encoder models.
Specifically, we evaluate on three datasets: CLINC150 (Larson et al. 2019), an intent classification
dataset with 11 categories; Emotion (Saravia et al. 2018), a Twitter corpus annotated with six basic
emotions; and E-commerce, a review-based five-class rating prediction task9. Collectively, these tasks
evaluate the model‚Äôs ability to capture different dimensions of personality-related understanding,
including intent recognition, emotional expression, and consumer preference.
We evaluate the predictive power of the persona embedding by measuring classification accuracy.
Specifically, given an input text sample xi and its inferred persona embedding hi, we train a softmax
regression classifier on hi to predict the corresponding class label.
Table 6: Classification Accuracy (%) using query embeddings from Qwen2.5-0.5 (base model) and
Persona Encoder. The best results are in bold.
Model
CLINC150
Emotion
E-commerce
Qwen2.5-0.5B
73.18
54.90
64.10
Persona Encoder
86.64
64.40
65.60
We compare the Persona Encoder against Qwen2.5-0.5B (Yang et al. 2024), the base model used
to construct the encoder. The classification results are reported in Table 6. We observe significant
improvements in accuracy on both the CLINC150 and Emotion datasets. This is particularly encour-
aging since CLINC150 focuses on detecting user intent and Emotion focuses on classifying emotional
9https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews/
data
20


--- Page 21 ---
states. The consistent gains on these datasets suggest that the Persona Encoder can effectively capture
users‚Äô needs in task-oriented queries, thereby facilitating the activation of appropriate persona experts.
On the E-commerce rating prediction task, the improvement is smaller but still indicates that the
encoder retains useful signals for modeling consumer preferences. Taken together, these results
demonstrate that the training pipeline of PersonaFuse enables the encoder with situational awareness
and task specific traits.
8.2
ROBUSTNESS TO DIFFERENT BASE MODELS
Table 7: Model Performance Across Metrics. SmolLM2-1.7B is used as the base LLM. Best results
are highlighted in bold within each base model group.
Base Model
Training Method
Social-Emotional Intelligence
General Reasoning Tasks
Safety
Domain-Specific Tasks
Emotion
EQ
ToM
GPQA
Arena
Math
SafetyBench
Shopping
Mental
Bench
Bench
Bench
Avg.
Hard
GSM8K
MMLU
Health
SmolLM2-1.7B
Direct Fine-tuning
0.21
3.07
0.29
0.16
0.90
0.13
41.80
0.34
5.38
Human-Like Fine-tuning
0.21
6.53
0.28
0.14
0.70
0.14
39.56
0.35
5.69
Random Route 1
0.30
3.34
0.33
0.26
1.70
0.14
43.70
0.35
5.05
Random Route 2
0.31
5.79
0.30
0.26
1.90
0.16
44.91
0.34
5.38
Random Route 5
0.32
5.99
0.35
0.27
1.70
0.14
45.74
0.35
5.47
PersonaFuse
0.33
6.68
0.39
0.28
2.20
0.21
46.63
0.36
5.93
In our main experiments, we use Llama 3.1-8B as the foundation LLM. To further validate the effec-
tiveness of the proposed PersonaFuse framework, we conduct ablation experiments with SmolLM2-
1.7B (Allal et al. 2025), a lightweight LLM developed by HuggingFace. This model is chosen to
simulate real-world settings where enterprises face resource constraints and low-latency requirements.
The training data and other experimental configurations are kept consistent with those in the main ex-
periment. We post-train SmolLM2-1.7B using the PersonaFuse pipeline and report results in Table 7.
As shown, PersonaFuse consistently outperforms other methods across all tasks, demonstrating the
effectiveness of our framework regardless of model scale. It is also worth noting that the absolute
performance of PersonaFuse based on SmolLM2-1.7B is lower than that based on Llama-3.1-8B (see
Appendix F, Main Experiment Scores). This outcome is expected, since SmolLM2-1.7B has fewer
parameters than Llama-3.1-8B, highlighting the important role of base model capacity in determining
overall performance.
8.3
ABLATION STUDY: EFFECTIVENESS OF PERSONA-COT
Table 8: Model Performance Across Metrics. Best results (bold) are compared within each base
model group.
Model
Social-Emotional
General Tasks
Safety
Domain Tasks
Emo
EQ
ToM
GPQA
Arena
Math
SafetyBench
Shop
Mental
Bench
Bench
Bench
Avg.
Hard
GSM8k
MMLU
Chat
Baseline (w/o Persona-CoT, w/o Persona-MoE)
0.29
35.84
0.42
0.28
10.00
0.33
58.16
0.48
6.04
Persona-CoT-finetuned (w/o Persona-MoE)
0.33
37.91
0.40
0.23
13.90
0.32
55.70
0.48
6.68
Persona-CoT-prompting (w/o Persona-MoE)
0.39
57.69
0.46
0.30
13.60
0.43
57.82
0.48
6.74
PersonaFuse
0.40
60.57
0.47
0.31
17.90
0.56
59.13
0.51
6.84
We conduct an ablation study to evaluate the effectiveness of the Persona-COT component. We
consider two variants. Persona-COT-finetuned refers to standard supervised fine-tuning of the base
LLM using the Persona-COT data, but without applying our proposed Persona-MoE architecture.
Persona-COT-prompting refers to directly using the Persona-COT procedure at inference time,
prompting the LLM to generate responses without any post-training. For comparison, we also include
a Baseline, which fine-tunes the base LLM on data generated by naive prompting.
The results are reported in Table 8. First, both Persona-COT-finetuned and Persona-COT-prompting
outperform the baseline on social-emotional tasks, indicating that Persona-COT indeed enhances
situational awareness and trait sensitivity. However, both methods show performance degradation on
SafetyBench, highlighting the limitation of direct prompting or relying solely on the original LLM
architecture. Second, when comparing PersonaFuse with Persona-COT-finetuned and Persona-COT-
prompting, we observe a clear contribution from the proposed Persona-MoE architecture, which
21


--- Page 22 ---
effectively routes queries to the most relevant experts and aligns tasks with the required traits. As a
result, PersonaFuse achieves balanced gains not only on social-emotional tasks, but also on general
benchmarks and safety evaluations.
These findings provide compelling evidence that personality adaptation cannot be achieved through
chain-of-thought reasoning alone. While extended CoT prompting can enhance certain task-specific
capabilities, it fundamentally lacks the architectural depth needed for consistent, multi-domain
performance optimization (Liu et al. 2025b). This validates the design choices of integrating Persona-
CoT with Persona-MoE and training them through a multi-stage pipeline.
9
CONCLUSION
This study addresses a critical challenge in LLM development: enhancing social and emotional
intelligence while maintaining general capabilities and safety. Through the design and implemen-
tation of PersonaFuse, we demonstrate that our theoretically-grounded approach can effectively
improve LLMs‚Äô social-emotional capabilities without compromising fundamental performance. Our
experimental results validate several key design principles: the situation-aware architecture enables
contextual personality expression, leading to significant improvements in emotional intelligence and
social cognition; the dynamic routing mechanism successfully preserves model safety and general
task performance, addressing a key limitation of existing approaches; and the integration of Trait
Activation Theory and the Big Five personality model provides a robust theoretical foundation for
personality adaptation in artificial systems. The effectiveness of PersonaFuse across different appli-
cation domains, such as mental health support and e-commerce interactions, demonstrates both the
generalizability of our design and its ability to bridge the gap between theoretical design and practical
application. These findings extend our understanding of how personality-based adaptations can
enhance human-AI interactions while maintaining system reliability, contributing to the development
of more effective and socially intelligent AI systems.
This work has several limitations that can be further addressed. First, the data synthesis process
of PersonaFuse relies on LLMs to annotate Big Five traits and situational cues, and the annotation
accuracy is not always reliable, which may introduce noise and misinterpretations. Human-in-the-
loop strategies could improve annotation quality and model precision. Second, while the dynamic
routing mechanism effectively captures task-related trait requirements, it does not fully adapt to
personalized user preferences. The same task may call for similar traits, yet different users can favor
distinct communication styles. Therefore, incorporating user feedback could enhance personalization
in high-stakes applications. Third, the synthesized data cover diverse scenarios but remain limited
in domain richness and cultural variability, raising questions about generalizability to real-world
multi-turn and cross-domain conversations. Despite these limitations, we believe that the theory-
guided design of PersonaFuse provides useful insights for the deployment of large language models
in real-world application scenarios. To facilitate future research, we will open source the training
pipeline and the Persona-MoE architecture.
REFERENCES
Abbasi A, Parsons J, Pant G, Sheng ORL, Sarker S (2024) Pathways for design research on artificial intelligence.
Information Systems Research 35(2):441‚Äì459.
Adamopoulos P, Ghose A, Todri V (2018) The impact of user personality traits on word of mouth: Text-mining
social media platforms. Information Systems Research 29(3):612‚Äì640.
Afroogh S, Akbari A, Malone E, Kargar M, Alambeigi H (2024) Trust in ai: progress, challenges, and future
directions. Humanities and Social Sciences Communications 11(1):1‚Äì30.
Allal LB, Lozhkov A, Bakouch E, Bl√°zquez GM, Penedo G, Tunstall L, Marafioti A, Kydl√≠Àácek H, Lajar√≠n AP,
Srivastav V, Lochner J, Fahlgren C, Nguyen XS, Fourrier C, Burtenshaw B, Larcher H, Zhao H, Zakka C,
Morlon M, Raffel C, von Werra L, Wolf T (2025) Smollm2: When smol goes big ‚Äì data-centric training
of a small language model.
Bai Y, Kadavath S, Kundu S, Askell A, Kernion J, Jones A, Chen A, Goldie A, Mirhoseini A, McKinnon C, et al.
(2022) Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073 .
Barrick MR, Mount MK, Judge TA (2001) Personality and performance at the beginning of the new millennium:
What do we know and where do we go next? International Journal of Selection and assessment 9(1-2):9‚Äì
30.
22


--- Page 23 ---
Buehler EL, Buehler MJ (2024) X-LoRA: Mixture of low-rank adapter experts, a flexible framework for large
language models with applications in protein mechanics and molecular design. APL Machine Learning
2(2):026119, ISSN 2770-9019.
Bui N, Nguyen HT, Kumar S, Theodore J, Qiu W, Nguyen VA, Ying R (2025) Mixture-of-personas language
models for population simulation. Findings of the Association for Computational Linguistics: ACL 2025,
24761‚Äì24778 (Association for Computational Linguistics), ISBN 979-8-89176-256-5.
√áalƒ±k EY, Akku¬∏s TR (2025) Enhancing human-like responses in large language models. arXiv preprint
arXiv:2501.05032 .
Chapman BP, Talbot N, Tatman AW, Britton PC (2009) Personality traits and the working alliance in psychother-
apy trainees: An organizing role for the five factor model? Journal of social and clinical psychology
28(5):577‚Äì596.
Chen J, Wang X, Xu R, Yuan S, Zhang Y, Shi W, Xie J, Li S, Yang R, Zhu T, Chen A, Li N, Chen L, Hu C, Wu
S, Ren S, Fu Z, Xiao Y (2024a) From persona to personalization: A survey on role-playing language
agents. Transactions on Machine Learning Research ISSN 2835-8856, survey Certification.
Chen Y, Xing X, Lin J, Zheng H, Wang Z, Liu Q, Xu X (2023) SoulChat: Improving LLMs‚Äô empathy,
listening, and comfort abilities through fine-tuning with multi-turn empathy conversations. Findings of the
Association for Computational Linguistics: EMNLP 2023, 1170‚Äì1183 (Association for Computational
Linguistics).
Chen Z, Chan J (2024) Large language model in creative work: The role of collaboration modality and user
expertise. Management Science 70(12):9101‚Äì9117.
Chen Z, Wu J, Zhou J, Wen B, Bi G, Jiang G, Cao Y, Hu M, Lai Y, Xiong Z, Huang M (2024b) ToMBench:
Benchmarking theory of mind in large language models. Proceedings of the 62nd Annual Meeting of the
Association for Computational Linguistics, 15959‚Äì15983 (Association for Computational Linguistics).
Cobbe K, Kosaraju V, Bavarian M, Chen M, Jun H, Kaiser L, Plappert M, Tworek J, Hilton J, Nakano R, Hesse
C, Schulman J (2021) Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 .
Dan Y, Zhou J, Chen Q, Tian J, He L (2025) P-react: Synthesizing topic-adaptive reactions of personality traits
via mixture of specialized lora experts. Findings of the Association for Computational Linguistics: ACL
2025, 6342‚Äì6362.
Davis FD (1989) Perceived usefulness, perceived ease of use, and user acceptance of information technology.
MIS quarterly 319‚Äì340.
Devaraj S, Easley RF, Crant JM (2008) Research note‚Äîhow does personality matter? relating the five-factor
model to technology acceptance and use. Information systems research 19(1):93‚Äì105.
Devlin J, Chang MW, Lee K, Toutanova K (2019) BERT: Pre-training of deep bidirectional transformers for
language understanding. Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, 4171‚Äì4186 (Association for
Computational Linguistics).
Dubey A, Jauhri A, Pandey A, Kadian A, Al-Dahle A, Letman A, Mathur A, Schelten A, Yang A, Fan A, et al.
(2024) The llama 3 herd of models. arXiv preprint arXiv:2407.21783 .
Engvik H (1999) Therapeutic popularity and personality: Association between peer therapist nominations and
the ‚Äúbig five‚Äù personality factors. Scandinavian Journal of Psychology 40(4):261‚Äì267.
Feng W, Hao C, Zhang Y, Han Y, Wang H (2024) Mixture-of-LoRAs: An efficient multitask tuning method
for large language models. Proceedings of the 2024 Joint International Conference on Computational
Linguistics, Language Resources and Evaluation (LREC-COLING 2024), 11371‚Äì11380 (ELRA and
ICCL).
Fleeson W, Jayawickreme E (2015) Whole trait theory. Journal of research in personality 56:82‚Äì92.
Gan Z, Liu Y (2025) Towards a theoretical understanding of synthetic data in LLM post-training: A reverse-
bottleneck perspective. The Thirteenth International Conference on Learning Representations.
Gao Y, Lee D, Burtch G, Fazelpour S (2025) Take caution in using llms as human surrogates. Proceedings of the
National Academy of Sciences 122(24):e2501660122.
Ge T, Chan X, Wang X, Yu D, Mi H, Yu D (2024) Scaling synthetic data creation with 1,000,000,000 personas.
arXiv preprint arXiv:2406.20094 .
Gershon RC, Lai JS, Bode R, Choi S, Moy C, Bleck T, Miller D, Peterman A, Cella D (2012) Neuro-qol: quality
of life item banks for adults with neurological disorders: item development and calibrations based upon
clinical and general population testing. Quality of Life Research 21(3):475‚Äì486.
Gilardi F, Alizadeh M, Kubli M (2023) Chatgpt outperforms crowd workers for text-annotation tasks. Proceedings
of the National Academy of Sciences 120(30):e2305016120.
Guo D, Yang D, Zhang H, Song J, Zhang R, Xu R, Zhu Q, Ma S, Wang P, Bi X, et al. (2025) Deepseek-r1:
Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 .
23


--- Page 24 ---
Han E, Yin D, Zhang H (2023) Bots with feelings: should ai agents express positive emotion in customer service?
Information Systems Research 34(3):1296‚Äì1311.
Handa K, Tamkin A, McCain M, Huang S, Durmus E, Heck S, Mueller J, Hong J, Ritchie S, Belonax T, et al.
(2025) Which economic tasks are performed with ai? evidence from millions of claude conversations.
Homayouni A (2011) Personality traits and emotional intelligence as predictors of learning english and math.
Procedia - Social and Behavioral Sciences 30:839‚Äì843, ISSN 1877-0428, 2nd World Conference on
Psychology, Counselling and Guidance - 2011.
Hu EJ, yelong shen, Wallis P, Allen-Zhu Z, Li Y, Wang S, Wang L, Chen W (2022) LoRA: Low-rank adaptation
of large language models. International Conference on Learning Representations.
Huang C, Tang Z, Hu S, Jiang R, Zheng X, Ge D, Wang B, Wang Z (2025) Orlm: A customizable framework in
training large models for automated optimization modeling. Operations Research .
Ibrahim L, Hafner FS, Rocher L (2025) Training language models to be warm and empathetic makes them less
reliable and more sycophantic. arXiv preprint arXiv:2507.21919 .
Jiang AQ, Sablayrolles A, Roux A, Mensch A, Savary B, Bamford C, Chaplot DS, Casas Ddl, Hanna EB,
Bressand F, et al. (2024a) Mixtral of experts. arXiv preprint arXiv:2401.04088 .
Jiang H, Zhang X, Cao X, Breazeal C, Roy D, Kabbara J (2024b) PersonaLLM: Investigating the ability of large
language models to express personality traits. Findings of the Association for Computational Linguistics:
NAACL 2024, 3605‚Äì3627 (Association for Computational Linguistics).
Jin Q, Wang Z, Floudas CS, Chen F, Gong C, Bracken-Clarke D, Xue E, Yang Y, Sun J, Lu Z (2024a) Matching
patients to clinical trials with large language models. Nature Communications 15(1):9074.
Jin Y, Li Z, Zhang C, Cao T, Gao Y, Jayarao PS, Li M, Liu X, Sarkhel R, Tang X, Wang H, Wang Z, Xu W, Yang
J, Yin Q, Li X, Nigam P, Xu Y, Chen K, Yang Q, Jiang M, Yin B (2024b) Shopping MMLU: A massive
multi-task online shopping benchmark for large language models. The Thirty-eight Conference on Neural
Information Processing Systems Datasets and Benchmarks Track.
Jir√°sek M, Sudzina F (2020) Big five personality traits and creativity. Quality Innovation Prosperity 24(3):90‚Äì
105.
Kang D, Kim S, Kwon T, Moon S, Cho H, Yu Y, Lee D, Yeo J (2024) Can large language models be good
emotional supporter? mitigating preference bias on emotional support conversation. Proceedings of the
62nd Annual Meeting of the Association for Computational Linguistics, 15232‚Äì15261 (Association for
Computational Linguistics).
Kim D, Kang D, Moon T (2025) DoMIX: An efficient framework for exploiting domain knowledge in fine-tuning.
Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, 14581‚Äì14602
(Association for Computational Linguistics), ISBN 979-8-89176-251-0.
Kim H, Sclar M, Zhou X, Bras R, Kim G, Choi Y, Sap M (2023) FANToM: A benchmark for stress-testing
machine theory of mind in interactions. Proceedings of the 2023 Conference on Empirical Methods in
Natural Language Processing, 14397‚Äì14413 (Association for Computational Linguistics).
Kirkpatrick J, Pascanu R, Rabinowitz N, Veness J, Desjardins G, Rusu AA, Milan K, Quan J, Ramalho T,
Grabska-Barwinska A, et al. (2017) Overcoming catastrophic forgetting in neural networks. Proceedings
of the national academy of sciences 114(13):3521‚Äì3526.
Kojima T, Gu SS, Reid M, Matsuo Y, Iwasawa Y (2022) Large language models are zero-shot reasoners.
Advances in neural information processing systems 35:22199‚Äì22213.
Kotha S, Springer JM, Raghunathan A (2024) Understanding catastrophic forgetting in language models via
implicit inference. The Twelfth International Conference on Learning Representations.
Kwon T, Ong KTi, Kang D, Moon S, Lee JR, Hwang D, Sohn B, Sim Y, Lee D, Yeo J (2024) Large language
models are clinical reasoners: Reasoning-aware diagnosis framework with prompt-generated rationales.
Proceedings of the AAAI Conference on Artificial Intelligence 38(16):18417‚Äì18425.
Larson S, Mahendran A, Peper JJ, Clarke C, Lee A, Hill P, Kummerfeld JK, Leach K, Laurenzano MA, Tang L,
Mars J (2019) An evaluation dataset for intent classification and out-of-scope prediction. Proceedings of
the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 1311‚Äì1316 (Association for
Computational Linguistics).
Lee M, Srivastava M, Hardy A, Thickstun J, Durmus E, Paranjape A, Gerard-Ursin I, Li XL, Ladhak F, Rong F,
et al. (2022) Evaluating human-language model interaction. arXiv preprint arXiv:2212.09746 .
Li J, Du L, Zhao H, wen Zhang B, Wang L, Gao B, Liu G, Lin Y (2025a) Infinity instruct: Scaling instruction
selection and synthesis to enhance language models.
Li T, Chiang WL, Frick E, Dunlap L, Wu T, Zhu B, Gonzalez JE, Stoica I (2025b) From crowdsourced data to
high-quality benchmarks: Arena-hard and benchbuilder pipeline. Forty-second International Conference
on Machine Learning.
24


--- Page 25 ---
Liu J, Zhu Y, Wang S, Wei X, Min E, Lu Y, Wang S, Yin D, Dou Z (2025a) LLMs + persona-plug = personalized
LLMs. Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, 9373‚Äì
9385 (Association for Computational Linguistics), ISBN 979-8-89176-251-0.
Liu R, Geng J, Wu AJ, Sucholutsky I, Lombrozo T, Griffiths TL (2025b) Mind your step (by step): Chain-of-
thought can reduce performance on tasks where thinking makes humans worse. Proceedings of the 42nd
International Conference on Machine Learning .
Lu H, Fang L, Zhang R, Li X, Cai J, Cheng H, Tang L, Liu Z, Sun Z, Wang T, et al. (2025) Alignment and
safety in large language models: Safety mechanisms, training paradigms, and emerging challenges. arXiv
preprint arXiv:2507.19672 .
McCrae RR, John OP (1992) An introduction to the five-factor model and its applications. Journal of personality
60(2):175‚Äì215.
Mount MK, Barrick MR, Stewart GL (1998) Five-factor model of personality and performance in jobs involving
interpersonal interactions. Human performance 11(2-3):145‚Äì165.
Nettle D, Liddle B (2008) Agreeableness is related to social-cognitive, but not social-perceptual, theory of mind.
European Journal of Personality: Published for the European Association of Personality Psychology
22(4):323‚Äì335.
OpenAI (2025) Openai (jan 27 version). https://api.openai.com/v1/chat.
Ozer DJ, Benet-Martinez V (2006) Personality and the prediction of consequential outcomes. Annu. Rev. Psychol.
57(1):401‚Äì421.
Padmanabhan B, Fang X, Sahoo N, Burton-Jones A (2022) Machine learning in information systems research.
MIS Quarterly 46(1).
Paech SJ (2023) Eq-bench: An emotional intelligence benchmark for large language models. arXiv preprint
arXiv:2312.06281 .
Poddar S, Wan Y, Ivison H, Gupta A, Jaques N (2024) Personalizing reinforcement learning from human
feedback with variational preference learning. The Thirty-eighth Annual Conference on Neural Information
Processing Systems.
Qian Y, Zhang W, Liu T (2023) Harnessing the power of large language models for empathetic response
generation: Empirical investigations and improvements. Findings of the Association for Computational
Linguistics: EMNLP 2023, 6516‚Äì6528 (Association for Computational Linguistics).
Rashkin H, Smith EM, Li M, Boureau YL (2019) Towards empathetic open-domain conversation models: A new
benchmark and dataset. Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics, 5370‚Äì5381 (Association for Computational Linguistics).
Rein D, Hou BL, Stickland AC, Petty J, Pang RY, Dirani J, Michael J, Bowman SR (2024) GPQA: A graduate-
level google-proof q&a benchmark. First Conference on Language Modeling.
Sabour S, Liu S, Zhang Z, Liu J, Zhou J, Sunaryo A, Lee T, Mihalcea R, Huang M (2024) EmoBench: Evaluating
the emotional intelligence of large language models. Proceedings of the 62nd Annual Meeting of the
Association for Computational Linguistics, 5986‚Äì6004 (Association for Computational Linguistics).
Saravia E, Liu HCT, Huang YH, Wu J, Chen YS (2018) CARER: Contextualized affect representations for
emotion recognition. Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing, 3687‚Äì3697 (Association for Computational Linguistics).
Schanke S, Burtch G, Ray G (2021) Estimating the impact of ‚Äúhumanizing‚Äù customer service chatbots. Informa-
tion Systems Research 32(3):736‚Äì751.
Sclar M, Choi Y, Tsvetkov Y, Suhr A (2024) Quantifying language models‚Äô sensitivity to spurious features in
prompt design or: How i learned to start worrying about prompt formatting. The Twelfth International
Conference on Learning Representations.
Sorokovikova A, Rezagholi S, Fedorova N, Yamshchikov IP (2024) LLMs simulate big5 personality traits:
Further evidence. Proceedings of the 1st Workshop on Personalization of Generative AI Systems (PER-
SONALIZE 2024), 83‚Äì87 (Association for Computational Linguistics).
Team FL (2024) The falcon 3 family of open models.
Tett RP, Burnett DD (2003) A personality trait-based interactionist model of job performance. Journal of Applied
psychology 88(3):500.
Toshniwal S, Moshkov I, Narenthiran S, Gitman D, Jia F, Gitman I (2024) Openmathinstruct-1: A 1.8 million
math instruction tuning dataset. Advances in Neural Information Processing Systems 37:34737‚Äì34774.
Wang J, Mo F, Ma W, Sun P, Zhang M, Nie JY (2024a) A user-centric multi-intent benchmark for evaluating
large language models. Proceedings of the 2024 Conference on Empirical Methods in Natural Language
Processing, 3588‚Äì3612 (Association for Computational Linguistics).
Wang L, Yang N, Huang X, Yang L, Majumder R, Wei F (2024b) Improving text embeddings with large language
models. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, 11897‚Äì
11916 (Association for Computational Linguistics).
25


--- Page 26 ---
Wang Y, Wang M, Manzoor MA, Liu F, Georgiev GN, Das RJ, Nakov P (2024c) Factuality of large language
models: A survey. Proceedings of the 2024 Conference on Empirical Methods in Natural Language
Processing, 19519‚Äì19529 (Association for Computational Linguistics).
Wei J, Wang X, Schuurmans D, Bosma M, Xia F, Chi E, Le QV, Zhou D, et al. (2022) Chain-of-thought
prompting elicits reasoning in large language models. Advances in neural information processing systems
35:24824‚Äì24837.
Wong CS, Law KS (2002) Wong and law emotional intelligence scale. The leadership quarterly .
Xu J, Wei T, Hou B, Orzechowski P, Yang S, Jin R, Paulbeck R, Wagenaar J, Demiris G, Shen L (2025)
Mentalchat16k: A benchmark dataset for conversational mental health assistance. Proceedings of the 31st
ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2, 5367‚Äì5378.
Yang A, Yang B, Hui B, Zheng B, Yu B, Zhou C, Li C, Li C, Liu D, Huang F, Dong G, Wei H, Lin H, Tang J,
Wang J, Yang J, Tu J, Zhang J, Ma J, Xu J, Zhou J, Bai J, He J, Lin J, Dang K, Lu K, Chen K, Yang K, Li
M, Xue M, Ni N, Zhang P, Wang P, Peng R, Men R, Gao R, Lin R, Wang S, Bai S, Tan S, Zhu T, Li T,
Liu T, Ge W, Deng X, Zhou X, Ren X, Zhang X, Wei X, Ren X, Fan Y, Yao Y, Zhang Y, Wan Y, Chu Y,
Liu Y, Cui Z, Zhang Z, Fan Z (2024) Qwen2 technical report. arXiv preprint arXiv:2407.10671 .
Yang K, Lau RY, Abbasi A (2023) Getting personal: A deep learning artifact for text-based measurement of
personality. Information Systems Research 34(1):194‚Äì222.
Zhang L, Wu J, Zhou D, He Y (2025a) PROPER: A progressive learning framework for personalized large
language models with group-level adaptation. Proceedings of the 63rd Annual Meeting of the Association
for Computational Linguistics, 16399‚Äì16411 (Association for Computational Linguistics), ISBN 979-8-
89176-251-0.
Zhang M, Eack SM, Chen ZZ (2025b) Preference learning unlocks llms‚Äô psycho-counseling skills. arXiv preprint
arXiv:2502.19731 .
Zhang Y, Sun S, Galley M, Chen YC, Brockett C, Gao X, Gao J, Liu J, Dolan B (2020) DIALOGPT : Large-scale
generative pre-training for conversational response generation. Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics: System Demonstrations, 270‚Äì278 (Association for
Computational Linguistics).
Zhang Z, Lei L, Wu L, Sun R, Huang Y, Long C, Liu X, Lei X, Tang J, Huang M (2024) SafetyBench: Evaluating
the safety of large language models. Proceedings of the 62nd Annual Meeting of the Association for
Computational Linguistics, 15537‚Äì15553 (Association for Computational Linguistics).
Zhao H, Du L, Ju Y, Wu C, Pan T (2024a) Beyond iid: Optimizing instruction learning from the perspective of
instruction interaction and dependency. arXiv preprint arXiv:2409.07045 .
Zhao W, Li Z, Wang S, Wang Y, Hu Y, Zhao Y, Wei C, Qin B (2024b) Both matter: Enhancing the emotional
intelligence of large language models without compromising the general intelligence. Findings of the
Association for Computational Linguistics: ACL 2024, 11157‚Äì11176 (Association for Computational
Linguistics).
Zhao W, Ren X, Hessel J, Cardie C, Choi Y, Deng Y (2024c) Wildchat: 1m chatGPT interaction logs in the wild.
The Twelfth International Conference on Learning Representations.
A
PERSONAFUSE TRAINING HYPERPARAMETERS
In this appendix, we present detailed training hyperparameters for PersonaFuse. In the initial LoRA
experts warm-up stage, we independently train each LoRA module using a batch size of 32 and
implement gradient accumulation over 8 steps, continuing for 1,000 steps. We maintain a learning
rate of 1e-4, which empirically shows robust convergence characteristics. The subsequent router
network training stage focuses on optimizing routing decisions across experts. This stage needs
to compute loss across the entire training batch, leading us to adopt a larger batch size of 64. We
maintain the learning rate at 1e-4 to ensure consistent optimization dynamics with the previous stage.
For the final integration stage, we employ a more conservative training approach to fine-tune the
end-to-end model while preserving the learned representations. This stage operates with reduced
parameters: a batch size of 32, a lower learning rate of 1e-5, and a focused optimization for 300
steps, enabling precise adjustments to the integrated system without disrupting the previously learned
patterns. In our training process, Œ± and Œ≤ are 0.5, and Œ≥ is 1.0 in the loss function.
B
PROMPT TEMPLATE USED FOR PERSONA-COT
We present the prompt template used in Persona-CoT data generation process in Table 9.
26


--- Page 27 ---
Table 9: Prompt Template for Persona-CoT Reasoning Process
Persona-CoT Prompt Template
Below is a user instruction:
User Instruction:
{prompt}
Step 1: Analyze the Social Cue
- What does the social cue suggest about the user‚Äôs tone, intent, or emotional state?
- Social Cue: {social_cue}
- Reasoning: Based on the social cue, the user appears to be [describe tone/intent/emotion]. This suggests
they may respond well to [specific approach].
Step 2: Analyze the Task Cue
- What does the task cue reveal about the user‚Äôs goals or expectations?
- Task Cue: {task_cue}
- Reasoning: The task cue indicates the user is looking for [specific goal]. This requires a response that is
[specific quality, e.g., creative, structured, empathetic].
Step 3: Identify Personality Traits
- Based on the social and task cues, what personality traits are required to respond appropriately?
- Identified Traits:
{social_traits}
{task_traits}
- Reasoning:
- Openness to Experience: The user‚Äôs [high/low] openness suggests they prefer [innovative/practical]
solutions.
- Conscientiousness: The user‚Äôs [high/low] conscientiousness suggests they value [structure/flexibility].
- Extraversion: The user‚Äôs [high/low] extraversion suggests they prefer [energetic/calm] communication.
- Agreeableness: The user‚Äôs [high/low] agreeableness suggests they respond well to [supportive/direct]
language.
- Neuroticism: The user‚Äôs [high/low] neuroticism suggests they may need [reassurance/confidence].
Step 4: Formulate the Response
- How can I align the response with the identified personality traits?
- Reasoning:
- Openness to Experience: Since the user is [high/low] in openness, I will [use creative ideas/stick to
practical solutions].
- Conscientiousness: Since the user is [high/low] in conscientiousness, I will [emphasize structure/keep
the approach flexible].
- Extraversion: Since the user is [high/low] in extraversion, I will [use energetic language/maintain a
calm tone].
- Agreeableness: Since the user is [high/low] in agreeableness, I will [be supportive/maintain a neutral
tone].
- Neuroticism: Since the user is [high/low] in neuroticism, I will [provide reassurance/use confident
language].
Step 5: Provide the Final Response
- Write the response that embodies the identified traits. Please explain at the end of your response how
you incorporated the identified traits, using the ‚Äô##Persona Behavior Rationale‚Äô format.
Here is the required format:
##Response:...
##Persona Behavior Rationale:...
Now, please respond to the user instruction.
‚Äù‚Äô
C
PERSONA-COT TRAINING DATA EXAMPLES
In this appendix, we present two training examples of Persona-CoT in Table 10. In the first example,
the naive response, directly obtained by prompting the LLM, offers only straightforward solutions and
27


--- Page 28 ---
lacks empathy, whereas the Persona-CoT response demonstrates empathy toward the user‚Äôs situation
before providing guidance. In the second example, the naive response uses a more formal writing
style when discussing a family narrative, whereas the Persona-CoT response adopts a warmer tone
that reflects the activated personality traits. .
Table 10: Training data examples: the user prompts, activation vector, and Persona-CoT Response
are utilized in the training for PersonaFuse. The user prompts and naive responses are used in the
training for baseline. Both responses are truncated because of their length.
User Prompt
Trait Activa-
tion Vector
Activated Per-
sona
Persona-CoT Response
Naive Response
Recently, I had a shift at work
cancelled.
I was very ner-
vous that the whole week‚Äôs pay
would be lost.
[ 0, 0, 1, 0, 0, 0,
1, 0, 0, 1 ]
High
Agree-
ableness;
High
Consci-
entiousness;
Low
Neuroti-
cism
I can understand how
worried and nervous you
must feel right now. It‚Äôs
completely normal to be
concerned when shifts
get canceled, especially
if it affects your weekly
earnings. However, let‚Äôs
try to take it one step at a
time...
Here are a few steps you
can take to manage the
situation: 1. Communi-
cate with Your Employer:
Reach out to your man-
ager or HR department
to understand why the
shift was canceled and
whether there will be..
Generate a story about a fam-
ily spending time together.
[ 1, 0, 0, 1, 0, 0,
0, 0, 0, 0 ]
High
Open-
ness;
Low Conscien-
tiousness
Once upon a time, in
a cozy little town sur-
rounded by lush green
hills, there lived a family
named the Greens. The
family consisted of Mr.
Green, Mrs. Green, their
two children, Lily and
Max, and their golden
retriever, Buddy.
They
were known throughout
the town for their warm
smiles
and
infectious
laughter...
In the heart of a bustling
city stood a quaint, cozy
house where the John-
sons lived.
The John-
son family consisted of
Mr.
and Mrs.
John-
son, their two children,
Lily (10) and Max (7),
and their golden retriever,
Buddy. Despite the chaos
of city life, they managed
to carve out moments to-
gether...
D
SAMPLE RESPONSES IN HUMAN EVALUATION
In the human evaluation experiment, we compare PersonaFuse with four strong LLMs: GPT-3.5-
Turbo, GPT-4o, DeepSeek-R1-Distill-Qwen-14B, and Llama-3.1-8B. In this appendix, we provide
two illustrative examples of model responses, shown in Table 11 and 12. One example corresponds
to a logical reasoning task, and the other to an emotion-based dialogue.
E
HUMAN ANNOTATION INTERFACE
Figure 8 presents the annotation interface used in the human evaluation study.
F
MAIN EXPERIMENT SCORES
In this appendix, we report the absolute performance scores for all tasks: social-emotional intelligence
(Table 13), general intelligence (Table 14), model safety (Table 15), and downstream applications
(Table 16), respectively.
28


--- Page 29 ---
Table 11: Model Response Comparison: Emotion-Based Dialogues
Prompt
Model
Response
In university, for my ma-
jor of social work, we
all get mandatory intern-
ships where they place
us in certain ones and
we have no control over
it. Some other students
received cool internship
placements, and I feel
like mine doesn‚Äôt com-
pare.
Kind of dis-
appointed I didn‚Äôt get
something more inter-
esting.
PersonaFuse
I understand that you might feel a bit disappointed about
your internship placement compared to some of your
peers. It‚Äôs completely normal to feel this way, especially
when you‚Äôve put a lot of effort into your studies and
are eager to apply your knowledge in a meaningful way.
However, it‚Äôs important to remember that every intern-
ship offers unique opportunities for growth and learning.
While it might not be the placement you initially en-
visioned, there are likely many valuable experiences
waiting for you.
GPT-4o
It‚Äôs completely natural to feel disappointed if your intern-
ship placement doesn‚Äôt initially appear as interesting as
others. However, it‚Äôs important to recognize that every
internship, regardless of where it is, offers unique oppor-
tunities for growth and learning. Here are a few points to
consider: 1) Skill Development: Even if your placement
seems less exciting, there are always skills to be gained.
Focus on sharpening your abilities in communication,
teamwork, and problem-solving. These skills are trans-
ferable to any social work setting. 2) Perspective: Look
at your internship as a chance to gain a different per-
spective. Sometimes the most unexpected placements
can teach you the most about the field and yourself as a
professional. 3)...
DeepSeek-R1-
Distill-Qwen-14B
It‚Äôs understandable to feel disappointed about your in-
ternship placement, especially when you were hoping for
something more exciting. Here‚Äôs a structured approach
to help you navigate this situation: 1) Acknowledge Your
Feelings: Recognize...
GPT-3.5-Turbo
It‚Äôs completely normal to feel disappointed if you didn‚Äôt
get the internship placement you were hoping for. How-
ever, it‚Äôs important to remember that every placement
offers its own unique opportunities to learn and grow as
a social work professional.
Llama-3.1-8B-
Instruct
It‚Äôs completely normal to feel disappointed or envious
when you see others getting more desirable internship
placements, especially when you feel like you don‚Äôt have
a say in the matter. Here are a few things to consider: 1)
Every internship is an opportunity: While it might not
be the most glamorous or exciting placement, remember
that every internship provides valuable experience and
skills that can benefit you in the long run. Focus on
what you can learn and take away from your placement.
2) It‚Äôs not about comparing: Try not to compare your
internship to others. Instead, focus on your own goals,
aspirations, and what you want to achieve from your
internship experience...
29


--- Page 30 ---
Table 12: Model Response Comparison: Logical Reasoning Task
Prompt
Model
Response
Consider
the
order
of
speed
among
NAME_1,
NAME_2,
and NAME_3, where
NAME_1
is
faster
than
NAME_2
but
slower than NAME_3.
Using this information,
determine the fastest
individual.
PersonaFuse
To determine the fastest individual, we need to analyze
the given information. Since NAME_1 is faster than
NAME_2 but slower than NAME_3, we can create a
list of their speeds from fastest to slowest: NAME_3,
NAME_1, NAME_2. Therefore, NAME_3 is the fastest
individual.
GPT-4o
Based on the given information, the order of speed is
NAME_3, NAME_1, and then NAME_2. Therefore,
NAME_3 is the fastest individual.
DeepSeek-R1-
Distill-Qwen-14B
The fastest individual is NAME_3. Step-by-Step Expla-
nation: 1. Given Information: NAME_1 is faster than
NAME_2: NAME_1 > NAME_2. NAME_1 is slower
than NAME_3: NAME_3 > NAME_1. 2. Combine
the Inequalities: From the two points, we can chain the
inequalities: NAME_3 > NAME_1 > NAME_2. 3. De-
termine the Fastest: The highest in the chain is NAME_3,
making them the fastest. Answer: The fastest individual
is NAME_3.
GPT-3.5-Turbo
Based on the given information, NAME_3 is the fastest
individual.
Llama-3.1-8B-
Instruct
To determine the fastest individual, we need to ana-
lyze the given information: 1. NAME_1 is faster than
NAME_2. 2. NAME_1 is slower than NAME_3. Com-
bining these two statements, we can conclude the follow-
ing order of speed from fastest to slowest: 1. NAME_3 2.
NAME_1 3. NAME_2. Therefore, the fastest individual
is NAME_3.
Figure 8: Screenshot of the annotation interface used in Human Evaluation.
30


--- Page 31 ---
Table 13: Absolute scores and relative improvements over Direct-finetuned (%) on EmoBench, EQ
Bench, and ToMBench. EA: emotional application; EU: emotional understanding.
Model
EmoBench
EQ Bench
ToMBench
EA
EU
Avg.
Score
Task-oriented
Ability-oriented
Avg.
Direct-finetuned
0.47
0.11
0.29
35.84
0.41
0.42
0.42
Human-Like-finetuned
0.60 (+27.7%)
0.16 (+45.5%)
0.38 (+31.0%)
52.19 (+45.6%)
0.44 (+7.3%)
0.46 (+9.5%)
0.45 (+7.1%)
Random Route 1
0.53 (+12.8%)
0.16 (+45.5%)
0.34 (+17.2%)
53.59 (+49.5%)
0.43 (+4.9%)
0.43 (+2.4%)
0.43 (+2.4%)
Random Route 2
0.57 (+21.3%)
0.16 (+45.5%)
0.36 (+24.1%)
57.28 (+59.8%)
0.41 (+0.0%)
0.42 (+0.0%)
0.42 (+0.0%)
Random Route 5
0.56 (+19.1%)
0.16 (+45.5%)
0.36 (+24.1%)
56.94 (+58.9%)
0.45 (+9.8%)
0.46 (+9.5%)
0.46 (+9.5%)
PersonaFuse
0.62 (+31.9%)
0.19 (+72.7%)
0.40 (+37.9%)
60.57 (+69.0%)
0.48 (+17.1%)
0.47 (+11.9%)
0.47 (+11.9%)
Table 14: Absolute scores and relative improvements over Direct-finetuned (%) on GPQA (graduate-
level), Arena-Hard-Auto-v0.1 (Open QA), and GSM8k (Math). GPQA shows the average of Diamond,
Extended, and Main subsets.
Model
GPQA
Arena-Hard
GSM8k
Direct-finetuned
0.2805
10.00
0.3321
Human-Like-finetuned
0.2518 (-10.2%)
12.30 (+23.0%)
0.5360 (+61.4%)
Random Route 1
0.2837 (+1.1%)
9.60 (-4.0%)
0.5291 (+59.3%)
Random Route 2
0.2882 (+2.7%)
10.10 (+1.0%)
0.4261 (+28.3%)
Random Route 5
0.2797 (-0.3%)
9.80 (-2.0%)
0.3829 (+15.3%)
PersonaFuse
0.3078 (+9.7%)
17.90 (+79.0%)
0.5557 (+67.3%)
Table 15: Absolute scores and relative improvements over Direct-finetuned (%) on SafetyBench
across different safety categories. OFF: Offensiveness, UB: Unfairness and Bias, PH: Physical Health,
MH: Mental Health, IA: Illegal Activities, EM: Ethics and Morality, PP: Privacy and Property.
Model
Avg
SafetyBench
OFF
UB
PH
MH
IA
EM
PP
Direct-finetuned
58.16
53.80
52.20
58.10
63.00
61.60
55.30
63.10
Human-Like-finetuned
58.76 (+1.0%)
54.00 (+0.4%)
52.90 (+1.3%)
60.50 (+4.1%)
68.10 (+8.1%)
59.60 (-3.2%)
55.30 (+0.0%)
60.90 (-3.5%)
Random Route 1
52.40 (-9.9%)
52.20 (-3.0%)
53.50 (+2.5%)
52.00 (-10.5%)
54.70 (-13.2%)
55.30 (-10.2%)
46.90 (-15.2%)
52.20 (-17.3%)
Random Route 2
56.56 (-2.8%)
52.90 (-1.7%)
53.00 (+1.5%)
57.50 (-1.0%)
58.80 (-6.7%)
61.50 (-0.2%)
52.30 (-5.4%)
59.90 (-5.1%)
Random Route 5
58.19 (+0.1%)
55.60 (+3.3%)
52.90 (+1.3%)
59.80 (+2.9%)
61.70 (-2.1%)
65.20 (+5.8%)
53.90 (-2.5%)
58.20 (-7.8%)
PersonaFuse
59.13 (+1.7%)
53.00 (-1.5%)
55.50 (+6.3%)
60.30 (+3.8%)
62.60 (-0.6%)
68.10 (+10.6%)
54.50 (-1.4%)
59.90 (-5.1%)
Table 16: Absolute scores and relative improvements over Direct-finetuned (%) across E-Commerce
and Mental Health domains. Note: The Overall score is independently assessed by GPT-4 rather than
an average of other dimensions.
Model
E-Commerce
MentalChat16K
Reviews & QA
Active Listening
Empathy
Safety
Open Mindedness
Clarity
Boundaries
Holistic
Overall
Direct-finetuned
0.48
6.04
6.65
6.50
7.52
6.53
6.56
6.78
6.04
Human-Like-finetuned
0.46 (-4.2%)
6.41 (+6.1%)
7.12 (+7.1%)
6.62 (+1.8%)
7.89 (+4.9%)
6.88 (+5.4%)
6.75 (+2.9%)
6.42 (-5.3%)
6.41 (+6.1%)
Random Route 1
0.45 (-6.3%)
5.66 (-6.3%)
6.32 (-5.0%)
6.11 (-6.0%)
7.45 (-0.9%)
6.21 (-4.9%)
6.33 (-3.5%)
5.98 (-11.8%)
5.66 (-6.3%)
Random Route 2
0.45 (-6.3%)
6.01 (-0.5%)
6.58 (-1.1%)
6.34 (-2.5%)
7.67 (+2.0%)
6.44 (-1.4%)
6.52 (-0.6%)
6.12 (-9.7%)
6.01 (-0.5%)
Random Route 5
0.44 (-8.3%)
5.99 (-0.8%)
6.54 (-1.7%)
6.31 (-2.9%)
7.62 (+1.3%)
6.41 (-1.8%)
6.48 (-1.2%)
6.08 (-10.3%)
5.99 (-0.8%)
PersonaFuse
0.51 (+6.3%)
6.84 (+13.2%)
7.60 (+14.3%)
6.96 (+7.1%)
8.24 (+9.6%)
7.31 (+11.9%)
7.02 (+7.0%)
6.24 (-8.0%)
6.84 (+13.2%)
31
