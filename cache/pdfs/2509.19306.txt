--- Page 1 ---
A Federated Fine-Tuning Paradigm of Foundation
Models in Heterogenous Wireless Networks
Jingyi Wang∗, Zhongyuan Zhao†, Qingtian Wang∗, Zexu Li∗, Yue Wang∗, Tony Q. S. Quek‡
∗Wireless AI System Research Team, China Telecom Research Institute, Beijing, 102209, China
†The State Key Laboratory of Networking and Switching Technology
Beijing University of Posts and Telecommunications, Beijing, 100876, China
‡Singapore University of Technology and Design, 487372, Singapore
E-mail: wangjy74@chinatelecom.cn, zyzhao@bupt.edu.cn, {wangqt08, lizx28, yue.wang}@chinatelecom.cn,
tonyquek@sutd.edu.sg
Abstract—Edge intelligence has emerged as a promising
strategy to deliver low-latency and ubiquitous services for
mobile devices. Recent advances in fine-tuning mechanisms
of foundation models have enabled edge intelligence by inte-
grating low-rank adaptation (LoRA) with federated learning.
However, in wireless networks, the device heterogeneity and
resource constraints on edge devices pose great threats to the
performance of federated fine-tuning. To tackle these issues,
we propose to optimize federated fine-tuning in heterogenous
wireless networks via online learning. First, the framework of
switching-based federated fine-tuning in wireless networks is
provided. The edge devices switches to LoRA modules dy-
namically for federated fine-tuning with base station to jointly
mitigate the impact of device heterogeneity and transmission
unreliability. Second, a tractable upper bound on the inference
risk gap is derived based on theoretical analysis. To improve the
generalization capability, we formulate a non-convex mixed-
integer programming problem with long-term constraints, and
decouple it into model switching, transmit power control, and
bandwidth allocation subproblems. An online optimization
algorithm is developed to solve the problems with polynomial
computational complexity. Finally, the simulation results on
the SST-2 and QNLI data sets demonstrate the performance
gains in test accuracy and energy eﬀiciency.
Index Terms—Federated fine-tuning, online learning, device
heterogeneity, foundation model, model switching.
I. Introduction
Benefiting from the potentiality of massive data and
enhancement of computing capability on edge devices,
network intelligentization is extending from the remote
cloud server to the network edge [1]. Edge intelligence,
which is envisioned as a key enabler to the sixth gener-
ation (6G) networks, has facilitated real-time processing
for applications such as autonomous vehicles, embodied
intelligence and augmented reality. Recent advances in
fine-tuning on foundation model have provided an eﬀicient
mechanism to realize edge intelligence [2], in which pre-
trained foundation models adapt to diverse downstream
tasks and generate personalized solutions for edge devices.
As the computing resources are decentralized on the
edge devices, federated fine-tuning (FedFT) is proposed
to improve fine-tuning eﬀiciency and protect user privacy
[3]. The edge server periodically collects the fine-tuned
foundation models from edge devices and executes global
aggregation until convergence [4]. However, full-parameter
FedFT may lead to excessive communication and com-
putation overheads for resource-constrained edge devices.
To tackle this issue, the low-rank adaptation (LoRA)
technique [5] is proposed as a parameter-eﬀicient fine-
tuning strategy, which only updates the LoRA matrix
components while freezing the foundation models to
reduce the amount of trainable parameters.
Nonetheless, deploying FedFT for edge devices should
address the challenge of device heterogeneity with non-
independent and identical distributed (non-IID) data
characteristics, and diverse computation and communi-
cation resource constraints [6]. The recent works mainly
focused on adjusting LoRA rank and enhancing aggre-
gation strategy. In [6], the local fine-tuning matrices are
aggregated in the same dimension by zero-padding and
truncated towards a personalized LoRA rank. In [7], a
dynamic LoRA rank is employed for device fine-tuning
by adjusting the parameter allocation strategy for weight
redesign. In [8], devices are divided into near-IID groups
for FedFT, and the intra-group aggregation frequency and
fine-tuning depth are jointly optimized.
Although the recent approaches are effective in miti-
gating device heterogeneity, few works are engaged in the
context of wireless networks, and the quantitative analysis
of FedFT on device heterogeneity and resource constraints
has not been studied in the literature. Meanwhile, the
LoRA configurations are assumed to be uniform, which
may degrade the performance under time-varying wireless
circumstances. The dynamic FedFT schemes, including
model switching-based FedFT, show great potentials for
practical application. Motivated by these challenges, we
propose a model switching-based federated fine-tuning
paradigm in heterogenous wireless networks. Our main
contributions are summarized as follows:
• First, the framework of federated fine-tuning with
model switching is proposed. The foundation models
are customized with diverse LoRA modules, and the
edge devices implement switching-based federated
fine-tuning to mitigate the impact of device hetero-
geneity.
arXiv:2509.19306v1  [eess.SP]  5 Sep 2025


--- Page 2 ---
• Second, a tractable upper bound on the generaliza-
tion performance is derived with regard to device
heterogeneity, and an online optimization algorithm
is designed on model switching and wireless resource
management.
• Finally, the simulation results prove the validity of
theoretical analysis and demonstrate the performance
gains.
II. System Model of Federated Fine-tuning
In Fig. 1, we focus on the scenario of federated fine-
tuning based on LoRA in the wireless edge network. The
gNodeB (gNB) is located at the center of its Voronoi
cell with K associated user equipments (UEs) U
=
{U1, · · · , UK}, and equipped with a computing server to
enable data processing and interaction with devices. Each
UE Uk ∈U is equipped with a computing unit for model
fine-tuning based on its local data set Dk, which follows a
non-IID probability density function. The gNB customizes
N LoRA modules, denoted by ∆W = {∆w1, · · · , ∆wN},
based on foundation model w0.
Foundation 
Model
FM
Global LoRA 
Parameters
Global
Local
Updated Local
LoRA Parameters
Round t
Uplink & Downlink
Transmission
…
Device K
…
Local K
FM
Device K
…
Local K
FM
Edge Network
…
Global 1
FM
Global N
…
…
Global 2
Edge Network
…
Global 1
FM
Global N
…
…
Global 2
Edge Network
…
Global 1
FM
Global N
…
…
Global 2
Device 1
…
Local 1
FM
Device 1
…
Local 1
FM
Device K
…
Local K
FM
Device K
…
Local K
FM
…
Round t+1
Local 
Data Set
Online 
Learning
Device 1
…
Local 1
FM
Device 1
…
Local 1
FM
Fig. 1. The framework of federated fine-tuning scheme with model
switching.
In order to fine-tune foundation models in a dynamic
manner, the model switching mechanism is deployed. In
each round, the UEs individually subscribe LoRA modules
from gNB to implement local fine-tuning by employing
gradient-descent methods, and then transmit the feedback
gradients to the gNB for federated learning. In specific,
the model parameters of the n-th LoRA module in the
t-th round, denoted by ∆wt
n, is updated at gNB based
on the aggregation of feedback gradients from UEs in
subscription, i.e.,
∆wt+1
n
= ∆wt
n−η
∑K
k=1 Dt
kβt
n,kγt
n,k∇Fk(∆wt
n; Dt
k)
∑K
k=1 Dt
kβt
n,kγt
n,k
, (1)
where ∇Fk(∆wt
n; Dt
k) =
1
Dt
k
∑Dt
k
d=1 ∇l(∆wt
n; xd, yd) de-
notes the gradient of empirical risk of Uk for local model
update on the n-th LoRA module in the t-th round,
Dt
k denotes the volume of fine-tuning data set randomly
sampled from Dk in the t-th round, l(∆w; x, y) is the
risk function of ∆w on a specific data sample (x, y), βt
n,k
is a binary indicator, in which βt
n,k = 1 when Uk has
subscribed the n-th LoRA module in the t-th round and
βt
n,k = 0 otherwise, and η is the step size.
To characterize the uplink channel quality for transmit-
ting ∇Fk(wt
n; Dt
k) to gNB, γt
n,k is defined as
γt
n,k = 1{SINR =
|ht
k|2P t
kd−α
k
∑
Ul∈˜Ξo,k |ht
l|2P t
l d−α
l
+ σ2 ≥θ}, (2)
where ht
k captures the small-scale flat Rayleigh channel
fading, P t
k is the transmit power of Uk in the t-th round,
d−α
k
denotes the path loss with exponent α, ˜Ξo,k is the
location set of out-of-cell UEs that interfere with Uk, θ
is the threshold of signal-to-interference-plus-noise ratio
(SINR) at receiver, σ2 = Wδt
n,kN0 is the power of noise,
W denotes the total bandwidth resource, δt
n,k denotes the
bandwidth allocation coeﬀicient of Uk for transmission of
∇Fk(∆wt
n; Dt
k), 0 ≤δt
n,k ≤1, and N0 is the noise power
density.
As ∇Fk(∆wt
n; Dt
k) is transmitted via orthogonal chan-
nels, the intra-cell interference can be eliminated. The
gNB first broadcasts back the intact w0 for UEs updating
LoRA modules in each round, and iteratively implements
federated learning until ∆wt
n converge. In the inference
phase, by summing the LoRA module parameters with
the foundation model parameters [5], the inference risk of
Uk on the test data set Dt
k,te via model ensemble can be
modelled as1
¯Fk(wt; Dt
k,te) = 1
N
N
∑
n=1
Fk(∆wt+1
n
+ w0; Dt
k,te).
(3)
The energy consumption of Uk in the t-th round
of federated learning consists of both computation and
communication costs, which can be formulated as
Cn,k(t) = τDt
kGw
n,kf 2
kϱkMk + P t
k
G∆w
n,k
Wδt
n,k log2(1 + SINR),
(4)
where τ is the number of local iterations, fk is the
CPU/GPU frequency of Uk, Gw
n,k and G∆w
n,k denote the
data size of w0 and ∇Fk(∆wt
n; Dt
k) measured in bits,
respectively, Mk is the number of CPU/GPU cycles
required to process one bit at Uk, and ϱk is the energy
consumption coeﬀicient of the chip.
III. Generalization Performance Analysis
To analyze the generalization capability of the proposed
federated fine-tuning paradigm, we study the expected
risk gap between the inference risk in (3) and the minimum
risk over the downstream tasks in the t-th round, i.e.,
Φt =
K
∑
k=1
ρt
kEh{ ¯Fk(wt; Dt
k,te) −Fk(w∗
k,t; Dt
k,te)},
(5)
1For simplicity of analysis, we employ a uniform weight for model
ensemble. The sophisticated weight design for harmonizing non-IID
data distribution divergence can follow the work in [9].


--- Page 3 ---
Φt ≤1
N
K
X
k=1
ρt
k
N
X
n=1
(1 −At
n)[Fk(∆wt
n) −Fk(w∗
k,t)]
|
{z
}
Convergence of local ﬁne−tuning
+
t
X
q=0
t
Y
p=q+1
N
X
n=1
(1 −Ap
n)Bq
n
|
{z
}
Long−term impact of transmission unreliability
+
ϵ
N
K
X
k=1
ρt
k
N
X
n=1
At
n
s
4ζ1
ξ2Dt
k,te
|
{z
}
Data heterogenity of UE−speciﬁc tasks
+
ϵ + 2
4
K
X
k=1
ρt
k∥w0∥2
|
{z
}
Intrinsic impact of foundation model
,
(6)
where
ρt
k
=
Dt
k
∑K
k=1 Dt
k
denotes
the
weight
of
data
volume in the t-th round, and w∗
k,t denote the op-
timal model parameters for inference on Dt
k,te, i.e.,
w∗
k,t = arg minw Fk(w; Dt
k,te). For brevity of notation,
Fk(w; Dt
k,te) is rewritten as Fk(w). The following assump-
tions on l(w; x, y) are made [9], [10].
Assumption 1. l(w; x, y) is ϵ-Lipchitz continuous and ξ-
strongly convex with respect to w.
Assumption 2. l(w; x, y) is twice-continuously differen-
tiable, i.e., ξI ⪯∇2l(w; x, y) ⪯ϵI.
Assumption 3. There exists ζ1, ζ2 ≥0 that satisfies
∥∇l(w; x, y)∥2 ≤ζ1 + ζ2∥∇F(w)∥2.
Theorem 1 gives an upper bound of Φt as follows.
Theorem 1. Given the step size as η = 1
ϵ , there exists an
upper bound on the expected risk gap Φt as (6), where
λn,k = Eh{γt
n,k} denotes the successful transmission prob-
ability given in (15), and the convergence rate is derived
as At
n = 2ξ
ϵ [1−8ζ2
∑K
k=1 ρt
k(1−λn,k)(1−βt
n,k)−4ζ2K−ϵ
2],
Bt
n = 2ζ1
ϵ [K + 2 ∑K
k=1 ρt
k(1 −λn,k)(1 −βt
n,k)].
Proof. Please refer to Appendix A.
Theorem 1 illustrates that there exists a tractable
upper bound on the generalization risk gap between the
proposed federated fine-tuning scheme and the optimal
scheme over downstream tasks, which is correlated with
the impact of model switching in terms of βt
n,k, transmis-
sion unreliability in terms of λn,k, data heterogeneity of
UE-specific tasks, and the impact of foundation models.
Moreover, the convergence rate of generalization risk gap
is ensured, which depends on channel quality and model
switching algorithms.
IV. Online Optimization on Model Switching, Power
Control and Bandwidth Allocation
In order to improve the generalization capability per-
formance, and reduce the energy consumption of the
federated fine-tuning system, the objective function is
formulated by optimizing on model switching βt
=
{βt
1, · · · , βt
K}, βt
k = {βt
1,k, · · · , βt
N,k}, transmit power
control Pt
=
{P t
1, · · · , P t
K}, and resource allocation
δt = {δt
1, · · · , δt
K}, δt
k = {δt
1,k, · · · , δt
N,k} as follows
min
Pt,βt,δt Φt + µ
∑
t
N
∑
n=1
K
∑
k=1
βt
n,kCn,k(t)
(7a)
s.t.
βt
n,k ∈{0, 1}, ∀n, ∀t,
(7b)
N
∑
n=1
βt
n,k ≤st, ∀k, ∀t,
(7c)
1
T
∑
t
βt
n,k ≥vn, ∀n,
(7d)
0 < P t
k ≤Pmax, ∀k, ∀t,
(7e)
N
∑
n=1
K
∑
k=1
δt
n,k ≤1, δt
n,k ∈[0, 1],
(7f)
where µ is the energy eﬀiciency coeﬀicient to balance
generalization capability and resource overhead, st de-
notes the maximum number of LoRA modules in sub-
scription, and vn is the participation rate constraint.
Since the optimization problem is a non-convex, mixed-
integer programming problem with long-term constraints,
we decouple (7) into three subproblems, and develop an
online algorithm to solve them iteratively.
A. Model Switching from An Energy-Eﬀicient Perspective
Recalling the impact of βt
n,k in (6), the dynamic
model switching method can benefit UEs with enhanced
generalization capability and energy eﬀiciency. Therefore,
in order to minimize the long-term cost in (7), the
subproblem of model switching for an individual UE Uk
is organized as
min
βt
k
t
∑
q=0
N
∑
n=1
t∏
p=q+1
(1 −Ap
n,k)Bq
n,k + µ
t
∑
q=0
N
∑
n=1
βt
n,kCn,k(t)
(8a)
s.t. (7b), (7c) and (7d),
(8b)
where At
n,k = 2ξ
ϵ [1 −8ζ2(1 −λn,k)(1 −βt
n,k) −4ζ2K −ϵ
2],
Bt
n,k = 2ζ1
ϵ [K + 2(1 −λn,k)(1 −βt
n,k)].
The integer linear programming problem in (8) can
be transformed into a linear programming problem by
relaxing βt
n,k to the real domain, i.e., βt
n,k ∈[0, 1], ∀n, ∀t.
The cost function to be minimized in the t-th round is
expressed as
Qt
k =
N
∑
n=1
[(1−At
n,k)Bt−1
n,k +Bt
n,k]+µ
N
∑
n=1
βt
n,kCn,k(t). (9)


--- Page 4 ---
ˆQt
k = 16ξζ2
ϵ
N
X
n=1
Bt−1
n,k (1 −λn,k)(1 −βt
n,k) + 4ζ1
ϵ
N
X
n=1
(1 −λn,k)(1 −βt
n,k) + µ
N
X
n=1
βt
n,kP t
kEt
(14)
Denote by yt(βt
k) = ∑N
n=1 βt
n,k −st, and zt
n = vn−βt
n,k,
which satisfies zt(βt
k) = [zt
1, · · · , zt
N] ⪯0. Thus, the
problem in (8) can be transformed into its min-max
version as
min
βt
k
max
λt
∑
t
Ft(βt
k, λt) =
∑
t
Qt
k(βt
k) +
∑
t
λtzt(βt
k)
(10a)
s.t.
yt(βt
k) ≤0, βt
n,k ∈[0, 1], ∀n, ∀t,
(10b)
where
λt
is
the
Lagrange
multiplier,
Qt
k(βt
k)
=
16ξζ2
ϵ
∑N
n=1{Bt−1
n,k (1 −λn,k)(1 −βt
n,k) + 4ζ1
ϵ (1 −λn,k)(1 −
βt
n,k)} + µ ∑N
n=1 βt
n,kCn,k(t), in which Bt−1
n,k is obtained
in the (t −1)-th round. To solve (10), we can alternately
update λt+1 and βt+1
k
on the fly based on the knowledge
before the (t + 1)-th round. The dual variable λt+1 is
updated via standard dual ascent step as
λt+1 = [λt + ς∇λFt( ˆβt
k, λt)]+,
(11)
where ∇λFt( ˆβt
k, λt) is the gradient of Ft( ˆβt
k, λ) given
λ = λt, ς is step size, and ˆβt
k is the solution obtained
at t. βt+1
k
is updated via a decent step by approximating
Ft(β, λt+1) as
min
β ∇Qt
k( ˆβt
k)(β −ˆβt
k) + λt+1zt(β), s.t. yt(β) ≤0. (12)
The problem in (12) can be solved by employing the
interior point method to approach κ-accurate optimal so-
lution with polynomial time complexity O(N 2 log(1/κ)).
The fractional solution
ˆβt
k from (12) can be further
converted into integers through randomized rounding
algorithms.
B. Wireless Resource Management for Reliable Transmis-
sion
Due to the sophisticated characteristics of SINR per
UE, we first relax ˆCn,k(t) = τDt
kGw
n,kf 2
kϱkMk + P t
kEt in
(4), where Et denotes the minimum transmission delay
for the specific LoRA module of UE, i.e.,
Et = min max
n,k
G∆w
n,k
Wδt
n,k log2(1 + SINR).
(13)
Thus, Qt
k in (9) can be reformulated as (14). λn,k
takes the following form by conditioning on Rayleigh
distribution as
λn,k = exp[
−Wδt
n,kN0θd2
i
P t
k(2ϕ)
α
2 −1
−ϕπd2
i θ
2
α
∫+∞
0
1 −e−12
5π θ
2
α x
1 + x
α
2
dx],
(15)
where ϕ denotes the spatial density of gNBs following a
homogeneous Poisson point process [11].
1) Bandwidth allocation: To balance the tradeoff be-
tween transmission reliability and energy consumption,
we first optimize Et with regard to bandwidth resource
allocation, and then an adaptive transmit power control
scheme is proposed to minimize ˆQt
k based on Et. Thus, the
subproblem of bandwidth allocation can be reformulated
as
min
δt
N
∑
n=1
βt
n,kP t
kEt, s.t. (7f) and (13).
(16)
Theorem 2. Denote the optimal Et of (16) as Et∗=
G∆w
n,k
W δt
n,k log2(1+SINR), ∀k. The optimal bandwidth allocation
solution to (16) satisfies
K
∑
k=1
N
∑
n=1
βt
n,kδt∗
n,k = 1.
(17)
Proof. Denote
by
S
=
|ht
k|2P t
kd−α
k ,
and
I
=
∑
Ul∈˜Ξo,k |ht
l|2P t
l d−α
l
, the inequality exists
d
dδ (Wδ log2(1 +
S
I + WN0δ ))
= W
ln2[ln(1 +
S
I + WN0δ ) −
SWN0δ
(S + I + WN0δ)(I + WN0δ)]
(a)
> 0,
(18)
where (a) is based on ln(1+x) >
x
1+x, ∀x > 0. For 0 ≤δ ≤
1, it holds dE
dδ ≤0, and thus Et∗monotonically decreases
with δt
n,k. The optimal solution to (16) is achieved if and
only if all bandwidth is allocated [12], which corroborates
(17).
Based on Theorem 2, δt∗
n,k of (16) can be derived in the
form of Lambert-W function LW(·) as
δt∗
n,k =
|ht
k|2P t
kd−α
k
WN0(e−LW(φ) −1) −
∑
Ul∈˜Ξo,k |ht
l|2P t
l d−α
l
WN0
, (19)
where φ can be approximated by the following implicit
equation through numerical algorithms such as Newton’s
method
φ = [
G∆w
n,k N0ln2
Et∗|ht
k|2P t
kd−α
k
−
P
Ul∈˜Ξo,k |ht
l|2P t
l d−α
l
|ht
k|2P t
kd−α
k
LW(φ)](
φ
LW(φ)−1).
(20)
As the analytical solution for δt
n,k cannot be obtained
with Lambert-W function, a two-tier binary search-based
algorithm is designed to approach the optimal solution
numerically in Algorithm 1. On the first tier, the LoRA
module-specific bandwidth δt
n,k is derived, and then aggre-
gated based on model switching results βt
n,k on the second
tier. If the overall bandwidth δA exceeds the threshold,
the searching region for Et is halved by retaining the
larger half, and vice versa until reaching the convergence
requirement.


--- Page 5 ---
∇P ˆQt
k(P t
k) = −
N
X
n=1
(1 −βt
n,k)(16ξζ2
ϵ
Bt−1
n,k + 4ζ1
ϵ ) Wδt
n,kN0θd2
i
(P t
k)2(2ϕ)
α
2 −1 e
−
W δt
n,kN0θd2
i
P t
k(2ϕ)
α
2 −1 + µ
N
X
n=1
βt
n,kEt
(22)
Algorithm 1 A two-tier binary search algorithm for band-
width allocation
1: Initialization: Et
up = Einput, Et
down = Emin, Et =
1
2(Et
up + Et
down), precision coeﬀicient ω, and the max-
imum number of iterations jmax ;
2: Repeat: For the j-th iteration, 1 ≤j ≤jmax
• For the device Uk, k = 1, · · · , K;
(Tier 1. Solve LoRA module-specific bandwidth)
– For the n-th LoRA module;
∗Derive φ by solving (20) based on P t
k and
Et.
∗Derive δt
n,k by solving (19) based on φ.
(Tier 2. Solve UE-specific bandwidth)
• Update δA = ∑K
k=1
∑N
n=1 βt
n,kδt
n,k.
– If δA > 1, Et
down = Et, Et = 1
2(Et + Et
up).
– If δA < 1 −ω, Et
up = Et, Et = 1
2(Et + Et
down).
3: Termination: When 1 −ω ≤δA ≤1 or j > jmax.
4: Return: δt
n,k, ∀n, ∀k, and Et.
Algorithm 2 A joint online optimization algorithm of (7)
1: Initialization: β1
n,k = 1, δ1
n,k, E1, P 1
k , ∀n, ∀k, λ0 = 0,
maximum iteration emax, and convergence threshold
qth.
2: For the t-th round, t = 1, · · · , T,
(Step 1. Wireless resource management)
• Repeat: For the e-th iteration, 1 ≤e ≤emax;
– Initialize δt,1
n,k = δt
n,k, Et,1 = Et, and P t,1
k
= P t
k.
– Update δt,e+1
n,k
and Et,e+1 by invoking Algo-
rithm 1 based on βt
n,k and Et,e.
– Update P t,e+1
k
by solving (21) based on βt
n,k,
δt,e+1
n,k
and Et,e+1.
– Calculate the value of ˆQt,e+1
k
in (14)
• Termination: When | ˆQt,e+1
k
−ˆQt,e
k | ≤qth, ∀k, or
e > emax.
(Step 2. Model switching optimization)
• Update βt+1
n,k and λt+1 by solving (12) based on
δt+1
n,k and P t+1
k
, and derive the integer solutions.
2) Transmit power control: The optimization subprob-
lem for P t
k can be expressed as
min
Pt
ˆQt
k(P t
k) in (14), s.t. (7e).
(21)
The transmit power P t
k is first relaxed as a continuous
real number, and thus (21) can be converted into a single-
variate optimization problem. The first-order derivative
of ˆQt
k(P t
k) with respect to P t
k is obtained as (22). Since
directly solving ∇P ˆQt
k(P t
k) = 0 has no tractable closed-
form solutions, the stationary point cannot be acquired.
Therefore, the numerical algorithms such as interior point
method are employed to improve computational eﬀiciency.
In Algorithm 2, we design an online optimization
algorithm of (7) by jointly solving the aforementioned
three subproblems. In the t-th round, the variables of
transmit power and bandwidth allocation are updated
iteratively until the objective Qt
k converged, and then the
model switching solutions are updated to minimize the
cumulative cost.
V. Simulation Results
In this section, our proposed scheme is evaluated in
a federated fine-tuning system to implement language
sentiment analysis and language inference tasks based
on SST-2 and QNLI data set, respectively. We configure
N = 4 LoRA modules based on RoBERTa model [7]. The
system parameters are set as fk = 1.5 GHz, α = 3.8,
st = 1, τ = 4, vn = 0.1, W = 1 GHz, ϱk = 1 × 10−27,
Mk = 737.5 cycles/bit, N0 = −162 dBm/Hz, Pmax = 200
mW, and Emin = 5 ms. To characterize the features of
heterogenous settings, the data of 100 UEs is drawn from
a Dirichlet distribution [8].
In Fig. 2, the convergence performance versus the
number of UEs K is evaluated with four benchmarks,
including vanilla FedLoRA [4], full-parameter FedLoRA,
HetLoRA [6] and FlexLoRA [7]. The number of rounds
to converge first declines in the lower K region and
then increases, which showcases the tradeoff between
scheduling more UEs and allocating more bandwidth re-
sources to enhance transmission reliability. By employing
multiple LoRA modules for adapting to the non-IID data
characteristics, Algorithm 2 can speed up convergence by
42.7% on average, compared with FedLoRA which fine-
tunes a common LoRA module. In addition, our proposed
scheme outperforms HetLoRA and FlexLoRA by provid-
ing each UE a tailored LoRA module for federated fine-
tuning with device heterogeneity, and the LoRA module
configuration is dynamic via model switching to overcome
the impact of time-varying wireless circumstances. Thus,
the performance gains can be further enlarged by 34.1%
and 5.0%, respectively.
In Fig. 3, the results show that Algorithm 1 can always
attain a stable bandwidth allocation solution. As K rises,
the iteration to converge also increases, which manifests
the time complexity of Algorithm 1 follows the order of
O(Klog2(Einput/ω)). Moreover, the transmission latency
is adaptive to K, which corroborates the scalability of
Algorithm 1 in diverse wireless circumstances to jointly
enhance bandwidth usage while reducing communication
overheads.
Fig. 4 demonstrates the test accuracy on QNLI data
set with model switching schemes, in which the greedy


--- Page 6 ---
10
20
30
40
50
60
70
80
90
100
40
50
60
70
80
90
100
110
Fig. 2. Convergence performance
of Algorithm 2 (SST-2 data set).
0
5
10
15
20
25
30
35
40
45
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
0
50
100
150
200
250
300
350
400
Fig. 3. Stability performance of
Algorithm 1 (SST-2 data set).
0
20
40
60
80
100
0.4
0.5
0.6
0.7
0.8
0.9
1
Fig. 4. Test accuracy performance
with model switching schemes
(QNLI data set).
-15
-10
-5
0
5
10
15
0
0.5
1
1.5
2
2.5
Fig. 5. Energy eﬀiciency perform-
ance of Algorithm 2 with coeﬀicient
µ (QNLI data set).
scheme optimizes short-term slice of (8a) in each round,
and the one-shot scheme implements model switching once
and follows vanilla FedLoRA thereafter. By minimizing
the long-term generalization risk gap with device het-
erogeneity, the convergence performance of our proposed
scheme is guaranteed, and the test accuracy improves
by 1.9% and 1.3% compared with one-shot and greedy
schemes, respectively. Moreover, the tradeoff between
generalization capability and energy eﬀiciency is achieved
via energy consumption-adaptive switching (w/ energy
constraints) with a 0.9% reduction in test accuracy.
In Fig. 5, the communication overheads of Algorithm 2
are evaluated with respect to the transmit power control
scheme and energy eﬀiciency coeﬀicient µ. Unlike the
maximum transmit power scheme with P t
k = Pmax, the
transmit power in (21) can be optimized adaptively based
on wireless channel condition θ. Therefore, the power
consumption can be saved by 16.8% when θ = −5 dB
and µ = 1 × 105, and the performance gap is mitigated
when the channel quality deteriorates. As the impact of
energy consumption increases in the larger µ region, the
communication overheads have been reduced by 11.9%
when µ = 1 × 106, and the energy eﬀiciency requirement
can be guaranteed.
VI. Conclusion
In this paper, a federated fine-tuning paradigm via
online optimization over wireless networks is proposed,
in which the UEs employ a model-switching scheme
and subscribe LoRA modules dynamically to implement
federated fine-tuning. The theoretical analysis has proven
an tractable upper bound on the generalization perfor-
mance, and an online algorithm to mitigate the impact of
device heterogeneity is designed. Finally, the simulation
results verify the effectiveness of the proposed scheme in
enhancing learning performance.
Appendix A
Proof of Theorem 1
Denote
the
global
risk
function
as
F(w)
=
∑K
k=1 ρt
kFk(w). Recalling the definition in (3), Φt is first
rewritten as
Φt = 1
N
K
X
k=1
ρt
k
N
X
n=1
E[Fk(∆wt+1
n
+ w0) −Fk(w∗)]
+ 1
N
K
X
k=1
ρt
k
N
X
n=1
E[Fk(w∗) −Fk(w∗
k,t)],
(23)
where w∗= arg minw F(w). The second-order Taylor
expansion of Fk(∆wt+1
n
+ wn) can be derived as
Fk(∆wt+1
n
+ w0) ≤Fk(∆wt+1
n
) + w⊤
n ∇Fk(∆wt+1
n
) + ϵ
2∥wn∥2,
(24)
where the inequality follows Assumption 2. Denote by
et
n = ∇F(∆wt
n) −
∑K
k=1 Dt
kβt
n,kγt
n,k∇Fk(∆wt
n)
∑K
k=1 Dt
kβt
n,kγt
n,k
, which satis-
fies ∆wt+1
n
= ∆wt
n −η(∇F(∆wt
n) −et
n) in (1).
To study the relationship between Fk(∆wt+1
n
) and
Fk(∆wt
n), Fk(∆wt+1
n
) can be second-order expanded as
Fk(∆wt+1
n
) ≤Fk(∆wt
n) −η(∇F(∆wt
n) −et
n)⊤∇Fk(∆wt
n)
+ η2ϵ
2 ∥∇F(∆wt
n)∥2 −η2ϵ(et
n)⊤∇F(∆wt
n) + η2ϵ
2 ∥et
n∥2.
(25)
By taking the summation of (25) over K UEs weighted
by ρt
k, and subtracting ∑K
k=1 ρt
kFk(w∗) in both sides with
step size η = 1
ϵ , it can be derived that
K
X
k=1
ρt
k[(Fk(∆wt+1
n
) −Fk(w∗)) −(Fk(∆wt
n) −Fk(w∗))]
≤−1
ϵ ∇F(∆wt
n)⊤
K
X
k=1
ρt
k∇Fk(∆wt
n) + 1
2ϵ∥∇F(∆wt
n)∥2
+ 1
ϵ
K
X
k=1
ρt
k(et
n)⊤(∇Fk(∆wt
n) −∇F(∆wt
n)) + ∥et
n∥2
2ϵ
,
(26)
where ∥et
n∥2 can be bounded based on Assumption 3 as
[10]
E∥et
n∥2 ≤4
K
X
k=1
ρt
k(1 −λn,k)(1 −βt
n,k)(ζ1 + ζ2∥∇F(∆wt
n)∥2).
(27)


--- Page 7 ---
The expectation of (et
n)⊤(∇Fk(∆wt
n)−∇F(∆wt
n)) can
be extended by invoking E(∥et
n∥2) in (27) as
E[
K
X
k=1
ρt
k(et
n)⊤(∇Fk(∆wt
n) −∇¯F(∆wt
n))]
(a)
≤1
2
K
X
k=1
ρt
k{E∥et
n∥2
+ E[
PK
j=1(Dt
j,te)2 PK
j=1 ∥∇Fk(∆wt
n) −∇Fj(∆wt
n)∥2
(PK
j=1 Dt
j,te)2
]},
(28)
where (a) stems from Cauchy-Schwarz inequality. Based
on (1), we expand quadratic terms and combine like terms
as
∥∇Fk(∆wt
n) −∇Fj(∆wt
n)∥2
≤[PDt
k,te
d=1
∇l(wt
n; xd)]2
(Dt
k,te)2
+ [PDt
j,te
d=1 ∇l(wt
n; xd)]2
(Dt
j,te)2
+ 2Dt
j,teDt
k,te
PDt
k,te
d=1
∇l(wt
n; xd) PDt
j,te
d=1 ∇l(wt
n; xd)
(Dt
k,te)2(Dt
j,te)2
(b)
≤4(ζ1 + ζ2∥∇F(∆wt
n)∥2),
(29)
where (b) follows Assumption 3. As Dt
j,te ≥1, ∀j, we have
∑K
j=1(Dt
j,te)2 ≤(∑K
j=1 Dt
j,te)2. By invoking Assumption
2, the Polyak-Łojasiewicz inequality holds for F(w) as
[10]
∥∇F(∆wt
n)∥2 ≥2ξ(F(∆wt
n) −F(w∗)).
(30)
Recalling the second term in (23), as Fk(w) is ϵ-Lipschitz
continuous in Assumption 1, it holds that
E[Fk(w∗) −Fk(w∗
k,t)] ≤ϵ
q
E∥w∗−w∗
k,t∥2
(c)
≤ϵ
s
4ζ1
ξ2Dt
k,te
,
(31)
where (c) follows Lemma 1 in [13]. By substituting (27),
(29), (30) and (31) into (26), and applying it recursively
for t rounds, (6) can be derived. The proof has finished.
References
[1] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo and J. Zhang, “Edge
intelligence: Paving the last mile of artificial intelligence with
edge computing,” Proc. IEEE, vol. 107, no. 8, pp. 1738-1762,
Aug. 2019.
[2] H. Wu, X. Chen and K. Huang, “Device-edge cooperative fine-
tuning of foundation models as a 6G service,” IEEE Wirel.
Commun., vol. 31, no. 3, pp. 60-67, Jun. 2024.
[3] Z. Wang, Y. Zhou, Y. Shi, and K. B. Letaief, “Federated
low-rank adaptation for large language model fine-tuning over
wireless networks,” in Proc. IEEE Global Commun. Conf.
(GLOBECOM), Cape Town, South Africa, pp. 1–6, Dec. 2024.
[4] Z. Chen, H. H. Yang, Y. C. Tay, K. F. E. Chong and T. Q. S.
Quek, “The role of federated learning in a wireless world with
foundation models,” IEEE Wireless Commun., vol. 31, no. 3,
pp. 42–49, Jun. 2024.
[5] E. J. Hu et al., “LoRA: Low-rank adaptation of large language
models,” in Proc. Int. Conf. Learn. Represent. (ICLR), Apr.
2022, pp. 1–17.
[6] Y. J. Cho, L. Liu, Z. Xu, et al., “Heterogeneous low-rank
approximation for federated fine-tuning of on-device foundation
models,” in Proc. 2024 Conference on Empirical Methods in
Natural Language Processing (EMNLP), Miami, Florida, USA,
pp. 12903–12913, 2024.
[7] Bai J, Chen D, Qian B, et al., “Federated fine-tuning of large
language models under heterogeneous language tasks and client
resource,” 2024. [Online]. Available: arXiv:2402.11505.
[8] Liu J, Liao Y, Xu H, et al., “Resource-eﬀicient federated fine-
tuning large language models for heterogeneous data,” 2025.
[Online]. Available: arXiv:2503.21213.
[9] Z. Zhao, C. Feng, W. Hong, J. Jiang, C. Jia, T. Q. S. Quek and
M. Peng, “Federated learning with non-IID data in wireless
networks,” IEEE Trans. Wirel. Commun., vol. 21, no. 3, pp.
1927-1942, Mar. 2022.
[10] M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor and S. Cui,
“A joint learning and communications framework for federated
learning over wireless networks,” IEEE Trans. Wirel. Commun.,
vol. 20, no. 1, pp. 269-283, Jan. 2021.
[11] H. H. Yang, Z. Liu, T. Q. S. Quek and H. V. Poor, “Scheduling
policies for federated learning in wireless networks,” IEEE
Trans. Wirel. Commun., vol. 68, no. 1, pp. 317-333, Jan. 2020.
[12] W. Shi, S. Zhou, Z. Niu, et al., “Joint device scheduling and
resource allocation for latency constrained wireless federated
learning,” IEEE Trans. Wirel. Commun., vol. 20, no. 1, pp.
453-467, Jan. 2021.
[13] A. Rakhlin, O. Shamir and K. Sridharan, “Making gradient
descent optimal for strongly convex stochastic optimization,” in
Proc. Int. Conf. Machin. Learn. (ICML), Edinburgh, Scotland,
pp. 1571-1578, Jun. 2012.
