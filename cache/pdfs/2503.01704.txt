--- Page 1 ---
DILEMMA: Joint LLM Quantization and Distributed LLM Inference Over Edge
Computing Systems
Minoo Hosseinzadeh, 1 Hana Khamfroush, 1
1 University of Kentucky
mho357@uky.com, khamfroush@uky.com
Abstract
With a recent trend of using Large Language Models (LLMs)
for different applications within smart cities, there is a need
for pushing these models toward the edge of network while
still preserving their performance. Edge Computing (EC)
as a physically closer computing resource to the end users
can help to reduce the communication delay for serving
end users’ tasks for LLM-dependent services. However, EC
servers have limited capacity in terms of communication,
computation, and storage capacity. This paper introduces
DILEMMA, a novel framework addressing the challenges of
deploying LLMs in EC systems by jointly optimizing layer
placement and layer quantization in EC systems. DILEMMA
formulates an Integer Linear Programming problem to min-
imize total inference delay while ensuring acceptable LLM
performance levels, leveraging layer-wise quantization and
knowledge distillation for LLM performance control. Exper-
imental evaluations on OPT-350 model using the SQuAD
dataset demonstrate that DILEMMA achieves a quantization
ratio of up to 12.75% while preserving model loss, highlight-
ing its effectiveness in resource-constrained environments.
Introduction
Smart City applications (Gaur et al. 2015) heavily rely
on Machine Learning (ML) models (Hosseinzadeh et al.
2021b,a) for their various services. Typically, these appli-
cations leverage Edge Computing (EC) (ETSI 2014) as
their underlying computational infrastructure (Hosseinzadeh
et al. 2021b,a). The recent progress in Large Language Mod-
els (LLM) (Vaswani 2017; Radford 2018) impacts numer-
ous applications within smart cities (Krystek et al. 2024).
Notably, several applications have seen significant improve-
ments in performance, particularly in areas such as and
smart virtual assistants (Sezgin 2024). However, the com-
putational demands of LLM models pose challenges, partic-
ularly when it comes to running them on end-user devices or
even EC devices (Bhardwaj, Singh, and Pandit 2024).
EC devices have limited communication, computation,
and storage capacity. Several smart city applications are
computationally intensive to run on limited capacity devices
such as EC devices (Hosseinzadeh et al. 2021b,a). Research
has shown that EC systems provide a great opportunity for
Copyright © 2025, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
distributed service running in the smart cities (Karjee et al.
2022). This option can be used for collaborative inference
systems for ML-dependent services (Li et al. 2024) and
more specifically LLM-dependent services (Cai et al.; Zhao
et al. 2024b).
On the other hand, ML model quantization tech-
niques (Gholami et al. 2022) have been widely used to re-
duce the ML models size. Additionally, this size reduction
helps to reduce the memory cost and computational cost as-
sociated with the inference process, making it feasible to run
these models across a variety of devices with varying com-
putational capacities (Gholami et al. 2022). Model quantiza-
tion transforms the model parameters from float32 or float64
to lower-bit float numbers or even integer numbers. The
fewer the bits, the less memory is used. It also helps with fas-
tening the inference time (Cai et al.). LLM quantization can
be done layer-wise, channel-wise, row/column-wise, token-
wise, and group-wise (Xiao et al. 2023).
Research shows the benefit of either quantization (Xiao
et al. 2023; Bai et al. 2022; Li et al. 2023) or distributed in-
ference (Wu et al. 2023; Borzunov et al. 2024; Zhao et al.
2024a; Zhang et al. 2024) to improve LLM inference time.
There are small number of papers that investigated the joint
quantization and distributed LLM inference problem (Cai
et al.; Zhao et al. 2024b). Cai et al. (Cai et al.) proposed
a client-server-based LLM inference using EC servers while
quantizing the LLM. The difference between our paper and
(Cai et al.) is that we consider a fully distributed system
model. Zhao et al. (Zhao et al. 2024b) proposed a joint dis-
tributed inference and LLM quantization method to reduce
the total delay. The authors distribute the LLM layers over a
set of GPU servers while quantizing each layer of the LLM.
The main difference between our work and
(Zhao et al.
2024b) is first the objective functions and the system mod-
els are different. Second, the authors in paper (Zhao et al.
2024b) uses the variance of output of a layer due to weight
quantization to estimate the performance of the LLM after
quantization while we use a different approach.
In this paper, we consider a two-tier EC system con-
sisting of a heterogeneous set of edge servers and a cloud
server where EC servers are connected to each other us-
ing Device-to-Device (D2D) communication as represented
in Fig. 1. EC servers have limited communication, com-
putation, and storage capacity. We assume that there are
arXiv:2503.01704v1  [cs.LG]  3 Mar 2025


--- Page 2 ---
Cloud Server
Edge Servers
l1
b1
l2
b2
l3
b3
li
bi
...
...
D2D
D2D
Figure 1: Two Layers System Architecture
LLM-dependent tasks offloaded to the EC servers. And EC
servers do not have enough capacity to run the entire LLM
by themselves. Additionally, tasks are time-sensitive. There-
fore, it is preferred to run the tasks on the edge devices
compared to offloading them to the cloud server. We pro-
pose a DIstributed LLM placEMent and layer-wised LLM
quAntization scheme (DILEMMA) in EC systems while si-
multaneously making decision on layer-wise quantization of
LLM models to reduce the total cost of the system. To solve
this problem, we propose an optimization model to jointly
decide which edge server host adn run which layer of the
LLM while making decision on each layer quantization in-
dependent of other layers. We summarize the contributions
of this work as following:
• We investigate the problem of joint layer placement on
the edge computing servers and layer quantization to
minimize the total completion time–called DILEMMA.
• We formulate the DILEMMA problem as an Integer
Linear Programming (ILP) model and prove that the
DILEMMA model is NP-hard when the number of LLMs
in the system is greater than one.
• We solve the DILEMMA model for the case when we
only have one LLM model in the system using off-the-
shelf optimization solver and show that using our pro-
posed method to control the LLM performance we can
compress the model by 12.75% while still preserving a
good level of LLM performance.
Problem Definition
System Definition. We consider a two layers system as rep-
resented in Fig. 1. The first layer consists of the cloud server
cl. The cloud server has the full knowledge of the system
and the DILEMMA decision making happens on the cloud
server. The second layer is the edge layer consisting of a
set of EC servers, M. Edge servers are heterogeneous in
terms of hardware capacity (e.g. communication, computa-
tion, and storage capacity). Each edge server i ∈M has lim-
ited communication capacity, ηi, computation capacity, γi,
and storage capacity, κi. EC servers are connected to their
neighborhood edge servers using Device-to-Device (D2D)
communication from one side and they are connected to the
cloud server from another side.
The LLM consists of a set of layers represented by the
set of L. For each layer l ∈L, it is possible to quan-
tize its weights with b bit size. We show the possible bit
sizes with the set B. To limit the range of set B, we define
two input variables, bmin and bmax, representing the mini-
mum number of bits that each layer weights can have and
the maximum number of bits that each layer weights can
have, respectively. Therefore, B = {bmin, bmin + 1, bmin +
2, ..., bmax}. Please note that the bmax is equal to the number
of bits of the weights that the LLM is trained with. The goal
of DILEMMA problem is to make two important decisions:
1) each layer of LLM should be placed on which edge server,
and 2) how much should we quantize each layer which de-
pends on the capacity of that edge server and the desired
performance of LLM.
Problem Formulation
The objective of the DILEMMA problem is to minimize the
total delay incurred by inference time of each layer placed
on a specific edge server, and sending the output of each
layer from one edge server to the edge server hosting the
next layer while still preserving a satisfying level of LLM
performance. We define two decision variables to jointly de-
cide both the placement of the LLM layers on edge servers
and the degree of quantization for each placed layer. The first
decision variable is xilb ≜1 when the layer l ∈L is placed
on the edge server i ∈M with layers quantized with b bits.
The second decision variable is ys
ij which is a binary vari-
able representing the output of edge server i will be sent to
the edge server j assuming that xilb = 1 and xj(l+1)b = 1.
When the parameters of each layer l ∈L are quantized
using l bits, the edge server i ∈M hosting layer l needs a
minimum computational capacity to process this layer. We
show this parameter using cpilp which is dependent on the
number of operations in layer l, the number of bits b for each
weight (i.e. precision), and the FLOPS of edge server i. We
explain how to calculate cpilp in Sec. . Now, we define the
DILEMMA optimization problem as following.
T =
X
i∈M
X
l∈L
xilb cpilb + yij cmilj
(1)
where the objective is to minimize the total completion time
T considering the following constraints.
min
xilb,yij T
(2a)
s.t.:
(2b)
X
i∈M
X
l∈L
xilb = 1
∀l ∈L, ∀b ∈B
(2c)
X
i∈M
X
l∈L
xilb ≤1
∀i ∈M
(2d)
strilb xilb pl ≤κi
∀i ∈M, l ∈L, ∀b ∈B
(2e)
xilb + xj(l+1)b −1 ≤yij
∀i&j ∈M, l ∈L, ∀b ∈B
(2f)
|P −Plb| ≤ϵ,
∀i ∈M, l ∈L, ∀b ∈B
(2g)
xilb&yij ∈{0, 1},
∀i ∈M, l ∈L, ∀b ∈B
(2h)
where constraint (2c) guarantees that each layer l of LLM is
placed only at one edge server i and is quantized with b bits.


--- Page 3 ---
Constraint (2d) guarantees that each EC server only hosts
one layer l ∈L or does not host any layer. Eq. (2e) guaran-
tees that edge server i has enough storage capacity to host
the layer l quantized with b bits. The constraint (2f) ensures
that if layer l is placed on edge server i and the layer l + 1
is placed at edge server j, then the decision variable yij is
equal to one (i.e. to consider the dependency between lay-
ers). The constraint (2g) guarantees that the difference be-
tween performance of LLM model after quantizing all lay-
ers and performance of LLM model before quantization is
less than an error rate, ϵ. The last constraint emphasizes on
that both xilb and yij are binary decision variables.
Estimating Completion Time. We define the total com-
pletion time considering the processing delay and commu-
nication delay for each layer. Note that most LLMs use au-
toregressive inference. It means that the model processes one
token at a time, passes it to the entire model, and then gener-
ates the next token and repeats the process. Therefore, with
a number of tokens of n, there will be n communication
rounds and computation rounds. Additionally, we assume
that every edge server stores its past attention cache, so that
in every round each server only transfers activation for a sin-
gle token.
Estimating cpilp. To estimate the computation delay,
cpilp, of running the inference task on edge server i, we
use the FLoating-point Operations per Second (FLOPS). We
model the cpilp as following:
cpilp = n × FLOPSl
CCSi
×
b pl
Original Bit Precisionl
(3)
where FLOPSl represents the number of floating-point oper-
ations required for the inference task of layer l. We assume
FLOPS as a function based on the type of the layer, the num-
ber of input features, the number of output features, kernel
size (if applicable), and the hidden state size (if applicable).
CCSi is the CPU clock speed of edge server i. b is the num-
ber of bits of layer l after quantization. pl is the total number
of output tensor of layer l. And, Original Bit Precisionl rep-
resents the bit precision of the original parameters of layer
l (i.e. before quantization). n is the number of tokens. We
multiply it by n to consider the autoregressive inference be-
havior which requires n pass through the LLM.
Estimating cmilb. We estimate the communication delay
due to send the output of layer l placed on the edge server i
represented with b bits to the next server considering the up-
load delay. We ignore the download delay on the next server
because it usually takes less amount of communication ca-
pacity to download compared to upload (Hosseinzadeh et al.
2021b). We consider the transmission delay and propagation
delay to model the communication delay to transmit data
from edge server i to the edge server j as:
cmilp = n × pl bl
Ωij
(4)
where Ωij is the link capacity between EC server i ∈M and
EC server j ∈M. pl is the total number of output tensor of
layer l and bl is the precision (number of bits) of layer l.
The total number of output tensor of layer l depends on the
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
Weight Value
0
1
2
3
4
5
6
Frequency
1e7
Weight Distribution of OPT-350m
Figure 2: Distribution of the OPT-350m Weights
number of tokens n, batch size, and the embedding size. We
represent the batch size by β, and embedding size by e. And,
n is the number of tokens.
Estimating
LLM
Performance
After
Layer-wise
Quantization.
The
main
challenge
of
solving
the
DILEMMA problem is the constraint (2g). The au-
thors of recent paper (Lin et al. 2024) have used loss value
as the evaluation metric of the LLM performance after
quantizing each layer. However, the value of loss is not
the only metric to evaluate the LLMs (Chang et al. 2024).
A combination of the metrics are used to evaluate the
performance of the LLMs such as BLEU, accuracy, and
perplexity (Zhou et al. 2023). It is not possible to estimate
the effect of quantizing each layer of the LLM on all these
metrics unless using a brute force approach which is not
scalable considering the large scale of the LLMs. Therefore,
we first focus on estimating the performance of LLM after
quantizing each layer. To do that, we borrow the Knowledge
Distillation (KD) concept. We use the main LLM model
as the teacher to supervise the quantization in the student
model which is the quantized model. The important factor
affecting the final performance of the model are the weights
and biases of the model. Because we only quantize the
weights in this paper, and we do not touch the biases,
therefore, they remain unchanged. Therefore, we define
the difference between weights of the teacher model and
the student model as the performance metric instead of
constraint (2g) as:
max
w∈l |(wo
l −ws
lb)| ≤δ
∀l ∈L
(5)
where w is the value of each element in feature map of layer
l. wo
l and ws
lb refer to the value of each element in feature
map of layer l of teacher (original) model and student model
where the student one is quantized with b bits, respectively.
δ is the error rate. ws
lb is a function of quantization rate b
of layer l. The max function and the || function makes the
constrain (5) non-convex and the optimization problem non-
linear.
Constrain (5) Linearization We know that any |N| ≤I
is equal to −I ≤N ≤I where N and I are just example
numbers. Additionally, the max function which traverse the


--- Page 4 ---
weights of layer l can be converted to ∀weights in layer l.
Therefore, the constraint (5) can be re-written as:
−δ ≤(wo
l −ws
lb) ≤δ
∀w ∈l, l ∈L
And finally, we break this constraint to two constraints to
incorporate the two inequalities:
wo
l −ws
lb ≤δ
∀w ∈l, l ∈L
(6)
wo
l −ws
lb ≥−δ
∀w ∈l, l ∈L
(7)
Quantization Function Quantization can be symmetric
or asymmetric. Choosing a quantization function depends
on the distribution of the data in each layer. For example,
for one-tailed data distributions, a symmetric function per-
forms better while signed symmetric quantization might be
suitable for distributions that are roughly symmetric about
zero (Wang et al. 2024). The data distribution of the weights
affects model’s performance and behavior. Therefore, know-
ing the distribution of weights in each layer of the LLM is
of fundamental importance.
Theorem 0.1. The optimization problem (2) is NP-hard if
and only if we have more than one LLM in the system.
Proof. We prove the Theorem 0.1 by a reduction from the
NP-hard Job-shop Scheduling Without Preemption (JSP)
problem (Mastrolilli and Svensson 2011) to our problem
when we have more than LLM. we are given n jobs J =
j1, j2, ..., jn with different processing times, which need to
be scheduled on M = m1, m2, ..., mm machines with vary-
ing processing power pm. Each job consists of a set of oper-
ations O1, O2, ..., On which need to be processed in a spe-
cific order (known as precedence constraints). The objective
is to minimize the makespan – the total length of the sched-
ule (i.e. when all the jobs have finished processing) while
serving all jobs. The decision variable xij is defined if and
only if the job j ∈J is assigned to machine m ∈M. And,
the decision variable yoo′ where o′ > o is defined to guar-
antee that operation o′ can start if and only if operation o is
finished. We will show that a simple instance of our problem
would be as hard as JSP problem. We now construct an in-
stance of our problem. We construct m edge servers with m
machines where each has a total communication, computa-
tion, and storage capacity of pm (i.e. pm = ηm + γm + κm.
For each job j ∈J construct a LLM query with opera-
tions defined as layer loinL where l can be processed if and
only if l1, l2, ..., lo−1 are all processed. We assume that the
compression ratio is 1 meaning that the layer lo is not com-
pressed when assigning to machine m. The objective is to
minimize the total length of schedule which is total com-
putation time of processing each job on each machine plus
total communication time of sending the output of the job
to the next machine. We claim that the optimal solution to
the constructed instance of our problem with more than one
LLM gives the optimal solution to the JSP problem. This is
because an algorithm that solves our problem can solve the
JSP. Since JSP is NP-hard, this concludes our proof.
1e-07
1e-06
1e-05
0.0001
0.001
0.01
0.1
1.0
delta
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
22.5
Objective
Figure 3: Objective value based on different δ values
In this paper, we focus on the case that we only have one
LLM. The JSP problem with one job is not NP-hard (Mas-
trolilli and Svensson 2011). Therefore, we leverage one of
the off-the-shelf solvers to solve the DILEMMA problem.
Results
Environment Setup. We use the Python Pulp package to
solve the DILEMMA problem. The DILEMMA problem is
a large size problem with several parameters. Therefore, to
find the effect of each important parameters, we relax other
parameters to make sure that the problem is still feasible. We
compare the results of LLM performance compared to the
original LLM model (not quantized and centrally processed
model). We use OPT-350m model (Zhang et al. 2022) and
SQuAD dataset (Rajpurkar 2016) for the evaluation. Unless
otherwise stated, we set the maximum length of token equal
to 128 and batch size equal to 128. We consider the mini-
mum bit precision to 4 and set the bit steps to 2. To have
a fair comparison, we divide the value of wo
l −ws
lb by 2b
to have a normalized value. To choose the best quantization
method for the OPT-350m, we present the the weight distri-
bution of OPT-350m model (Zhang et al. 2022) in the Fig. 2.
It implies that weights are mostly distributed in the range of
-1 and 1. Therefore, using any symmetric/asymmetric quan-
tization with a round function only increases error. We use
truncation method for the quantization technique.
Effect of δ. We relax the storage constraint (2e) to find
the effect of δ on the DILEMMA problem. Fig. 3 represents
the objective value of the DILEMMA problem for different
δ value. As the δ decreases, the objective increases which
means an increase in the total completion time of the in-
ference task. Although the completion time increases, the
performance of the quantized model is better compared to
the case where δ is equal to 1 as presented in the Table. 1.


--- Page 5 ---
Table 1: The value of loss and perplexity of the OPT-350m
model using DILEMMA scheme based on different δ values.
The loss value of the original model is 0.0591, its BLEU is
0.2951, and the its perplexity is 1.0609.
LLM Performance Metric
δ Values
Loss
Perplexity
BLEU
Quant. Ratio
1e-7
0.0591
1.0609
0.2949
37.50%
1e-6
0.0591
1.0609
0.2945
31.25%
1e-5
0.0591
1.0609
0.2942
25.00%
0.0001
0.0591
1.0609
0.2942
25.00%
0.001
0.0591
1.0609
0.3050
18.75%
0.01
0.0605
1.0623
0.2939
12.50%
0.1
0.0605
1.0623
0.2939
12.50%
1.0
0.0605
1.0623
0.2939
12.50%
0
2
4
6
8
10
Average Score
original
1
0.1
0.01
0.001
0.0001
1e-5
1e-6
1e-7
Model Quantized Based on Different  Value
6.40
5.70
6.00
5.80
5.90
6.10
5.90
6.20
6.30
Average Scores of Different Models
Figure 4: Models scored by GPT-4 based on their answers to
SQuAD questions dataset
The Table. 1 implies that a good level of the LLM perfor-
mance can be achieved even with not a very strict delta value
such as 1e −5. Additionally, when the delta decrease, the
performance increase. We define the quantization ratio as
the
P
l pl bl
P
l pl prl × 100 where pl is the total number of weights
of layer l, bl is the precision of the weights of layer l after
quantization, and the prl is the precision of layer l weights in
the original form (i.e. before quantization). The results states
that the higher the δ value is, the more the quantization rate
is, and consequently, less total completion time but with a
low performance degradation.
Next, we randomly chose 1000 questions from SQuAD
dataset (Rajpurkar 2016) and used all the quantized
OPT350m models represnted in Table.1 along with the origi-
nal model to answer these questions. Then, we asked GPT-4
to score each questions and answers by each model based
how relevant, correct, clearness, and fluent they are. The
range of the scores are from 0 to ten where zero is the worst
and ten is the best. Fig. 4 presents the scores provided by
GPT-4. It implies that quantizing the model with strict δ
value provides models with more similar answers compared
to the original LLM.
Effect of Communication Speed Between the Edge
Servers on the Objective Value. We run the next tests
for 100 Monte Carlo runs. We generate the communication
1 Mbps
10 Mbps
100 Mbps
1 Gbps
10 Gbps
Communication Speed Between the Edge Servers
0
500
1000
1500
2000
2500
3000
3500
4000
4500
Objective
4172.94
315.76
37.39
3.56
0.55
Figure 5: The impact of Ωij on the objective value
2 GHz
2.5 GHz
3 GHz
3.5 GHz
4 GHz
CPU Clock Between the Edge Servers
0
2
4
6
8
10
12
14
Objective
12.96
12.95
12.94
12.93
12.92
Figure 6: The impact of CCSi on the objective value
speed between the edge servers using the random number
between from the range of [minΩij, 10∗minΩij]. We change
the minΩij, from 1 Mbps to 10 Gbps to find the impact of
communication speed between the edge layers on the objec-
tive value of the DILEMMA problem. The results are plotted
in the Fig. 5 implying that with increasing the communica-
tion speed between the edge servers, the objective decreases,
meaning an improvement in the total completion time of the
autoregressive inference task.
Effect of Computation Capacity of the Edge Servers
on the Objective Value. Next, we investigate the effect of
CCSi, the CPU clock of each edge server i ∈M, on the
objective value. Fig .6 represents the impact of CCSi on the
objective value implying the objective value decreases when
the CCS increases; i.e. improving in completion time.
Conclusion
This paper presents DILEMMA, a framework for the joint
optimization of LLM layer placement and quantization in


--- Page 6 ---
edge computing systems. By leveraging layer-wise quantiza-
tion and knowledge distillation, the framework addresses the
challenges posed by limited computational, communication,
and storage resources in edge servers. Experimental results
validate the efficiency of the proposed approach, showing
that it achieves a balance between LLM performance and
resource utilization while reducing model size by 87.5%.
In future work, we will explore extending this approach to
multiple LLMs and dynamic edge network conditions while
considering multimodal LLMs.
Acknowledgment
This work is funded by research grants provided by the Na-
tional Science Foundation (NSF) under the grant number
2340075.
References
Bai, H.; Hou, L.; Shang, L.; Jiang, X.; King, I.; and Lyu,
M. R. 2022. Towards efficient post-training quantization of
pre-trained language models. Advances in neural informa-
tion processing systems, 35: 1405–1418.
Bhardwaj, S.; Singh, P.; and Pandit, M. K. 2024. A survey on
the integration and optimization of large language models in
edge computing environments. In 2024 16th ICCAE. IEEE.
Borzunov, A.; Ryabinin, M.; Chumachenko, A.; Baranchuk,
D.; Dettmers, T.; Belkada, Y.; Samygin, P.; and Raffel, C. A.
2024.
Distributed inference and fine-tuning of large lan-
guage models over the internet. Advances in Neural Infor-
mation Processing Systems, 36.
Cai, F.; Yuan, D.; Yang, Z.; and Cui, L. ???? Edge-LLM: A
Collaborative Framework for Large Language Model Serv-
ing in Edge Computing. In 2024 IEEE ICWS.
Chang, Y.; Wang, X.; Wang, J.; Wu, Y.; Yang, L.; Zhu, K.;
Chen, H.; Yi, X.; Wang, C.; Wang, Y.; et al. 2024. A survey
on evaluation of large language models. ACM Transactions
on Intelligent Systems and Technology, 15(3): 1–45.
ETSI. 2014. Mobile Edge Computing - Introductory Tech-
nical White Paper.
Gaur, A.; Scotney, B.; Parr, G.; and McClean, S. 2015. Smart
city architecture and its applications based on IoT. Procedia
computer science, 52: 1089–1094.
Gholami, A.; Kim, S.; Dong, Z.; Yao, Z.; Mahoney, M. W.;
and Keutzer, K. 2022. A survey of quantization methods for
efficient neural network inference. In Low-Power Computer
Vision, 291–326. Chapman and Hall/CRC.
Hosseinzadeh, M.; Hudson, N.; Zhao, X.; Khamfroush, H.;
and Lucani, D. E. 2021a. Joint compression and offloading
decisions for deep learning services in 3-tier edge systems.
In 2021 IEEE DySPAN. IEEE.
Hosseinzadeh, M.; Wachal, A.; Khamfroush, H.; and Lu-
cani, D. E. 2021b. Optimal Accuracy-Time Trade-off for
Deep Learning Services in Edge Computing Systems. In
IEEE ICC.
Karjee, J.; Naik, P.; Anand, K.; and Bhargav, V. N. 2022.
Split computing: DNN inference partition with load balanc-
ing in IoT-edge platform for beyond 5G. Measurement: Sen-
sors, 23: 100409.
Krystek, M.; Basi´nski, M.; Morzy, M.; and Mazurek, C.
2024.
Managing Data Platforms for Smart Cities Using
Large Language Models.
Li, H.; Li, X.; Fan, Q.; He, Q.; Wang, X.; and Leung, V. C.
2024. Distributed DNN inference with fine-grained model
partitioning in mobile edge computing networks.
IEEE
Transactions on Mobile Computing.
Li, Q.; Zhang, Y.; Li, L.; Yao, P.; Zhang, B.; Chu, X.;
Sun, Y.; Du, L.; and Xie, Y. 2023.
Fptq: Fine-grained
post-training quantization for large language models. arXiv
preprint arXiv:2308.15987.
Lin, J.; Tang, J.; Tang, H.; Yang, S.; Chen, W.-M.; Wang, W.-
C.; Xiao, G.; Dang, X.; Gan, C.; and Han, S. 2024. AWQ:
Activation-aware Weight Quantization for On-Device LLM
Compression and Acceleration.
Proceedings of Machine
Learning and Systems, 6: 87–100.
Mastrolilli, M.; and Svensson, O. 2011. Hardness of approx-
imating flow and job shop scheduling problems. Journal of
the ACM (JACM), 58(5): 1–32.
Radford, A. 2018. Improving language understanding by
generative pre-training.
Rajpurkar, P. 2016. Squad: 100,000+ questions for machine
comprehension of text. arXiv preprint arXiv:1606.05250.
Sezgin, E. 2024.
Redefining Virtual Assistants in Health
Care: The Future With Large Language Models.
Vaswani, A. 2017. Attention is all you need. Advances in
Neural Information Processing Systems.
Wang, W.; Chen, W.; Luo, Y.; Long, Y.; Lin, Z.; Zhang,
L.; Lin, B.; Cai, D.; and He, X. 2024. Model compression
and efficient inference for large language models: A survey.
arXiv preprint arXiv:2402.09748.
Wu, B.; Zhong, Y.; Zhang, Z.; Liu, S.; Liu, F.; Sun, Y.;
Huang, G.; Liu, X.; and Jin, X. 2023. Fast distributed in-
ference serving for large language models. arXiv preprint
arXiv:2305.05920.
Xiao, G.; Lin, J.; Seznec, M.; Wu, H.; Demouth, J.; and Han,
S. 2023. Smoothquant: Accurate and efficient post-training
quantization for large language models.
In International
Conference on Machine Learning, 38087–38099. PMLR.
Zhang, M.; Cao, J.; Shen, X.; and Cui, Z. 2024. EdgeShard:
Efficient LLM Inference via Collaborative Edge Computing.
arXiv preprint arXiv:2405.14371.
Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;
Chen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V.; et al. 2022.
Opt: Open pre-trained transformer language models. arXiv
preprint arXiv:2205.01068.
Zhao, J.; Song, Y.; Harris, I.; Jyothi, S. A.; et al. 2024a. Lin-
guaLinked: Distributed Large Language Model Inference on
Mobile Devices. In Proceedings of the 62nd Annual Meeting
of the Association for Computational Linguistics, 160–171.
Zhao, J.; Wan, B.; Peng, Y.; Lin, H.; and Wu, C. 2024b.
LLM-PQ: Serving LLM on Heterogeneous Clusters with
Phase-Aware Partition and Adaptive Quantization.
arXiv
preprint arXiv:2403.01136.


--- Page 7 ---
Zhou, K.; Zhu, Y.; Chen, Z.; Chen, W.; Zhao, W. X.; Chen,
X.; Lin, Y.; Wen, J.-R.; and Han, J. 2023.
Don’t make
your llm an evaluation benchmark cheater. arXiv preprint
arXiv:2311.01964.
