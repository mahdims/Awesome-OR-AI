--- Page 1 ---
Extending QAOA-GPT to Higher-Order Quantum Optimization Problems
Leanto Sunny,1, ∗Abhinav Rijal,1, † and George Siopsis1, ‡
1Department of Physics and Astronomy, The University of Tennessee, Knoxville, TN 37996-1200, USA
(Dated: November 11, 2025)
The recently proposed QAOA-GPT framework [1] demonstrated that generative pre-trained trans-
formers can learn mappings between problem graphs and optimized quantum circuits for the Quan-
tum Approximate Optimization Algorithm (QAOA). In this work, we extend QAOA-GPT to Higher-
Order Unconstrained Binary Optimization (HUBO) problems, focusing on spin-glass Hamiltonians
that include cubic interaction terms. Using FEATHER graph embeddings to encode topological
information, we train the model on graph–circuit pairs generated via ADAPT-QAOA and evalu-
ate its performance on 8- and 16-qubit instances embedded on heavy-hex lattices. The generative
model produces adaptive QAOA-like circuits and corresponding variational parameters in a single
forward pass, bypassing the iterative classical optimization loop. The generated circuits achieve av-
erage approximation ratios exceeding 0.95, closely matching classically optimized ADAPT-QAOA
results, while maintaining consistent parameter distributions across circuit depths. These results
demonstrate that QAOA-GPT generalizes effectively to higher-order cost Hamiltonians and com-
plex energy landscapes, establishing generative modeling as a scalable pathway toward autonomous
variational circuit design and quantum algorithm discovery in the NISQ era.
I.
INTRODUCTION
Quantum computing promises to transform the solu-
tion of complex problems in optimization, chemistry, and
materials science by exploiting the superposition and en-
tanglement of quantum states to explore exponentially
large configuration spaces. In the near term, variational
quantum algorithms (VQAs) have emerged as a practi-
cal strategy for leveraging noisy intermediate-scale quan-
tum (NISQ) devices [2]. By combining a parameterized
quantum circuit [3] with a classical optimizer in a hybrid
feedback loop, VQAs prepare quantum states that ap-
proximately minimize an objective function derived from
a problem Hamiltonian. Prominent examples include the
Variational Quantum Eigensolver (VQE) for molecular
energy estimation and the Quantum Approximate Opti-
mization Algorithm (QAOA) for combinatorial optimiza-
tion problems [4].
QAOA constructs a variational ansatz through alter-
nating applications of a cost Hamiltonian, encoding the
problem instance, and a mixer Hamiltonian, which drives
transitions between computational basis states. The vari-
ational parameters governing these layers are optimized
to minimize the expectation value of the cost Hamilto-
nian.
Despite its conceptual simplicity and hardware
compatibility, QAOA remains limited by the difficulty of
identifying optimal parameters, the exponential growth
of the search space with circuit depth, and the hardware
noise sensitivity of long circuits [5]. Adaptive approaches
such as ADAPT-QAOA [6] have sought to address these
limitations by building circuits iteratively, adding oper-
ators that yield the steepest local gradient descent in
∗lsunny@vols.utk.edu
† arijal@vols.utk.edu
‡ siopsis@tennessee.edu
energy. While this adaptive strategy produces more ex-
pressive and compact ans¨atze, it incurs significant com-
putational cost due to repeated gradient evaluations and
classical optimization.
This has motivated data-driven strategies to learn
these complex optimization patterns. Initial approaches
have used Graph Neural Networks to predict QAOA pa-
rameters [7], but these often lack the generative capac-
ity for full circuit synthesis. Recently, Tyagin et al. in-
troduced the QAOA-GPT framework [1], which reimag-
ines QAOA circuit construction as a sequence-generation
problem.
Using a generative pre-trained transformer
(GPT) trained on graph–circuit pairs,
QAOA-GPT
learns correlations between problem structure and op-
timized variational parameters. The trained model can
then generate efficient QAOA circuits for unseen graph
instances in a single forward pass, eliminating the need
for iterative classical optimization and dramatically re-
ducing computation time. The framework demonstrated
strong performance for quadratic unconstrained binary
optimization (QUBO) problems such as MaxCut, pro-
ducing circuits with accuracy comparable to ADAPT-
QAOA while maintaining consistent scalability.
In this work, we extend the QAOA-GPT paradigm
to Higher-Order Unconstrained Binary Optimization
(HUBO) problems, focusing on spin-glass Hamiltonians
that include cubic interaction terms.
These higher-
order systems possess rugged and frustrated energy
landscapes, making them an ideal benchmark for test-
ing the generalization capacity of generative quantum-
circuit models.
We employ FEATHER graph embed-
dings [8] to encode topological and higher-order con-
nectivity information into the model input and generate
datasets using ADAPT-QAOA-optimized circuits for 8-
and 16-qubit spin-glass instances embedded on heavy-
hex graphs. The resulting model, trained end-to-end on
these graph–circuit pairs, autonomously produces adap-
tive QAOA-like circuits and corresponding variational
arXiv:2511.07391v1  [quant-ph]  10 Nov 2025


--- Page 2 ---
2
parameters without any classical post-optimization.
We evaluate the performance of the extended QAOA-
GPT framework on unseen HUBO instances and analyze
its learned parameter distributions. The model achieves
average approximation ratios exceeding 0.95 for 16-qubit
systems, matching ADAPT-QAOA benchmarks while of-
fering orders-of-magnitude faster inference. The smooth
scaling of parameters with circuit depth further confirms
that the generative model internalizes the structure of
effective QAOA ans¨atze.
This work demonstrates that generative modeling can
serve as a scalable and data-driven mechanism for quan-
tum circuit synthesis, extending beyond quadratic op-
timization to higher-order and more complex Hamilto-
nians.
Beyond its immediate application to spin-glass
models, the present study establishes a foundation for
autonomous discovery of problem-specific quantum algo-
rithms—an essential step toward realizing practical quan-
tum advantage in the NISQ regime.
This paper is organized as follows.
Section II pro-
vides a detailed overview of the theoretical foundations
of QAOA and graph embedding via FEATHER [8], along
with the inner workings of the QAOA-GPT model and
the experimental setup, including the simulation im-
plementations used in this study.
Section III presents
our experimental results, comparing the performance of
our GPT-based approach against classical methods and
ADAPT-QAOA. Finally, Section IV discusses the impli-
cations of our findings and outlines promising future re-
search directions for QAOA-GPT frameworks.
II.
BACKGROUND & METHODOLOGY
This section outlines the theoretical and computational
foundations underlying our study. We begin by briefly
reviewing QAOA and its adaptive variant (ADAPT-
QAOA), which provide the variational framework upon
which our generative model is built. We then describe
the FEATHER graph-embedding method used to encode
structural information about problem instances, followed
by the architecture and training procedure of the QAOA-
GPT model employed in this work. Finally, we define
the higher-order spin-glass Hamiltonians used as bench-
marks for evaluating the model’s performance. Together,
these subsections establish the methodological basis for
extending QAOA-GPT to HUBO problems.
A.
QAOA
QAOA [4] is a hybrid quantum–classical framework de-
signed to find approximate solutions to discrete combi-
natorial optimization problems, especially those that can
be mapped to Ising Hamiltonians. Given a problem rep-
resented on a graph G(V, E), the cost Hamiltonian HC
encodes the objective function whose ground state corre-
sponds to the optimal solution, while a mixer Hamilto-
nian HM drives transitions between computational basis
states to explore the solution space. For a graph with
weighted edges wij, these operators typically take the
form
HC =
X
(i,j)∈E
wijZiZj, HM =
X
i
Xi,
(1)
where Zi and Xi denote Pauli operators acting on qubit
i.
Starting from the uniform superposition |+⟩⊗n, QAOA
alternates between cost and mixer unitaries for p layers:
|ψ(γ, β)⟩=
p
Y
k=1
e−iβkHM e−iγkHC |+⟩⊗n ,
(2)
where γ = (γ1, . . . , γp) and β = (β1, . . . , βp) are sets of
variational parameters.
The objective function is defined as
C(γ, β) = ⟨ψ(γ, β)| HC |ψ(γ, β)⟩,
(3)
which is minimized by a classical optimizer to approxi-
mate the ground-state energy of HC.
QAOA is compatible with near-term quantum hard-
ware due to its shallow circuit structure, but its efficiency
critically depends on the choice of parameters γ, β. Ex-
haustive or gradient-based searches over these parame-
ters scale poorly with system size, motivating data-driven
or adaptive alternatives.
B.
ADAPT-QAOA
The ADAPT-QAOA approach [6] enhances expressiv-
ity by adaptively constructing the variational circuit. In-
stead of fixing the ansatz structure, the algorithm selects,
at each iteration, the operator from a predefined pool
A = {A1, A2, . . . , AN} that produces the steepest energy
descent. The gradient of the cost function with respect
to each candidate operator Aj is computed as
gj = −i ⟨ψ(k−1)| eiγ0HC[HC, Aj]e−iγ0HC |ψ(k−1)⟩,
(4)
where |ψ(k−1)⟩is the current variational state.
The operator A(k) corresponding to the largest |gj| is
appended to the circuit, yielding the update
|ψ(k)⟩= e−iβkA(k)e−iγkHC |ψ(k−1)⟩.
(5)
This iterative procedure continues until convergence or a
predefined circuit depth is reached.
Although
ADAPT-QAOA
generates
compact,
problem-tailored
circuits
with
high
approximation
quality, it remains computationally demanding because
each iteration requires multiple gradient evaluations and
re-optimizations over all accumulated parameters.


--- Page 3 ---
3
C.
Graph Embedding via FEATHER
To integrate structural information from problem
graphs into the generative model,
we employ the
FEATHER embedding algorithm [8].
FEATHER en-
codes each node u ∈V through a characteristic function
based on random-walk transition probabilities:
ϕu(θ, r) =
X
w∈V
ˆAr
u,weiθxw ,
(6)
where ˆA = D−1A is the normalized adjacency matrix, r
is the walk length, and xw represents node attributes.
Averaging ϕu across sampled frequencies θ yields a
compact, complex-valued representation of the node’s
higher-order connectivity pattern.
The resulting node-level embeddings are mean-pooled
to obtain an isomorphism-invariant graph descriptor.
FEATHER embeddings thus provide the generative
model with continuous structural features that capture
graph topology and local correlation structure, enabling
it to generalize across diverse problem instances.
D.
Generative Model: QAOA-GPT
We utilize a modified version of nanoGPT [9], an im-
plementation of the decoder-only Transformer architec-
ture [10], following the QAOA-GPT framework of Tya-
gin et al. [1], but extend it to handle HUBO Hamiltoni-
ans that include linear, quadratic, and cubic interaction
terms. The model is trained on paired datasets of graphs
and corresponding ADAPT-QAOA circuits, treating cir-
cuit synthesis as a sequence modeling task. Each training
sample encodes a spin-glass instance and its optimized
circuit as a token sequence comprising graph descriptors
and layerwise circuit parameters.
Each instance is represented by two token streams:
• Graph
tokens,
which
encode
the
coefficients
dv, dij, dijk ∈{−1, +1} of the cost Hamiltonian,
• Circuit tokens, which specify the operator indices
and optimized parameters (ok, βk, γk) for each cir-
cuit layer.
Numerical parameters are rounded to two decimal places
and clipped to [−10, 10]. Graph embeddings computed
via FEATHER are broadcast across all tokens, providing
global context.
The model is trained autoregressively to minimize
next-token prediction loss, enabling it to generate full cir-
cuits conditioned on graph embeddings. Early stopping
is guided by two metrics: (i) the average approximation
ratio on a validation set, and (ii) the structural validity
of generated circuits.
E.
Higher-Order Spin-Glass Hamiltonians
The HUBO problem instances used for training and
evaluation are derived from spin-glass Hamiltonians of
the form
HC =
X
v∈V
dvZv +
X
(i,j)∈E
dijZiZj +
X
(i,j,k)∈W
dijkZiZjZk
(7)
where dv, dij, dijk are random coupling strengths chosen
from {−1, +1}.
The inclusion of cubic terms introduces higher-order
frustration, producing rugged energy landscapes that
make these models suitable benchmarks for quantum op-
timization algorithms.
The training dataset comprises 8- and 16-qubit in-
stances embedded on heavy-hex topologies (depicted in
Figure 1) to reflect realistic hardware connectivity con-
straints. Circuits achieving a target approximation ratio
α ≥0.87 are retained for training to ensure high-quality
supervision.
This methodological framework allows the model to
learn mappings from graph-encoded Hamiltonians to op-
timized circuit representations. All quantum circuit sim-
ulations for generating training data and evaluating gen-
erated circuits were performed using the CUDA-Q plat-
form [11], leveraging its high-performance simulator on
classical HPC resources. Once trained, QAOA-GPT can
generate adaptive QAOA-like circuits for unseen HUBO
instances in a single forward pass, effectively bypassing
classical parameter optimization.
F.
Performance Metric: Approximation Ratio
To quantify the quality of solutions produced by either
ADAPT-QAOA or the generative model, we employ the
approximation ratio
α = ⟨ψ(γ, β)|HC|ψ(γ, β)⟩
EOPT(G)
(8)
where HC is the cost Hamiltonian associated with the
problem instance and EOPT(G) denotes its exact ground-
state energy obtained from a classical solver.
Because both ⟨HC⟩and EOPT(G) are negative for min-
imization problems such as spin-glass Hamiltonians, we
take their absolute values when reporting ratios so that
0 < α ≤1, with α = 1 corresponding to the exact ground
state. This normalization ensures a consistent compari-
son across instances of different sizes and coupling distri-
butions.
Throughout this work, we report both the best and
average approximation ratios over test-set instances to
evaluate the model’s overall accuracy and generalization.
The methodological framework described above en-
ables QAOA-GPT to map higher-order spin-glass Hamil-
tonians directly to optimized variational circuits.


--- Page 4 ---
4
In the following Section, we present numerical results
demonstrating the model’s performance on 8- and 16-
qubit HUBO instances, analyze the learned parameter
distributions, and compare the generated circuits with
those obtained from classical ADAPT-QAOA optimiza-
tion.
0
3
1
5
6
2
4
7
(a) 8-qubit graph
0
1
4
7
10
12
15
2
13
3
5
8
11
14
6
9
(b) 16-qubit graph
FIG. 1: Heavy-hex graph topologies used for spin glass
problem generation. (a) 8-qubit graph. (b) 16-qubit
graph. Nodes represent qubits (linear terms Zv on
nodes in V ), edges represent quadratic interactions
(ZiZj) along edges in E, and hyperedges in W
encircling three neighboring nodes represent cubic
interaction terms (ZiZjZk) of the HUBO Hamiltonian.
All Ising coefficients dv, dij, dijk were uniformly sampled
from {−1, +1}.
III.
RESULTS AND DISCUSSION
We evaluate the performance of the extended QAOA-
GPT model on ensembles of 8- and 16-qubit higher-
order spin-glass instances defined on heavy-hex graphs.
Each instance contains linear, quadratic, and cubic cou-
pling terms with coefficients dv, dij, dijk ∈{−1, +1}. For
both system sizes, the dataset was partitioned into non-
overlapping training and test subsets, and only ADAPT-
QAOA circuits achieving a minimum approximation ra-
tio α ≥0.87 were used for model training. The trained
model was then evaluated on unseen instances to assess
its capacity for generalization and autonomous circuit
generation.
A.
Overall Performance
Figures 2 and 3 summarize the approximation ratios
obtained for 16-qubit test instances for pmax = 15 and
pmax = 5, respectively. The generative model achieves
average and best-case approximation ratios of 0.9496 ±
0.0305 and 0.9614±0.0281, respectively, closely matching
the results obtained from classically optimized ADAPT-
QAOA circuits used in training.
For a summary, see
Table I.
TABLE I: Approximation Ratios for 16-Qubit
QAOA-GPT Circuits Across Depth Configurations
Circuit Depth (pmax)
Mean AR
Best AR
15
0.950 ± 0.031
0.961 ± 0.028
5
0.855 ± 0.053
0.862 ± 0.052
For 8-qubit systems, the corresponding mean ratio is
approximately 0.93. Details are depicted in Figure 4 for
pmax = 15 and Figure 5 for pmax = 5. For a summary,
see Table II.
TABLE II: Approximation Ratios for 8-Qubit
QAOA-GPT Circuits Across Depth Configurations
Circuit Depth (pmax)
Mean AR
Best AR
15
0.840 ± 0.051
0.940 ± 0.036
5
0.815 ± 0.064
0.843 ± 0.062
These values confirm that QAOA-GPT maintains high
solution quality while completely bypassing classical vari-
ational optimization.
The agreement between generated and reference cir-
cuits demonstrates that the model has learned an effec-
tive internal representation of the mapping from graph
structure to optimized circuit parameters. In particular,
the consistency of α across system sizes indicates that the
generative approach scales favorably with the number of
qubits and the order of interaction terms, a non-trivial
result given the combinatorial growth of the underlying
configuration space.


--- Page 5 ---
5
Graph ID
0.84
0.86
0.88
0.90
0.92
0.94
0.96
0.98
Approximation Ratio
Avg. Best AR: 0.9614 (±0.0281)
Avg. Mean AR: 0.9496 (±0.0305)
Best QAOA-GPT Approx. Ratio
Average QAOA-GPT Approx. Ratio
Approx. Ratio (0.97)
FIG. 2: Performance distribution of QAOA-GPT on 200 16-qubit spin glass instances at depth pmax = 15. For each
instance, the model generated 10 circuits. The ‘Best QAOA-GPT Approx. Ratio‘ (blue) shows the highest
approximation ratio (α) achieved per instance, averaging 0.9614 ± 0.0281. The ‘Average QAOA-GPT Approx.
Ratio‘ (orange) shows the mean performance across all 10 generated circuits per instance, averaging 0.9496 ± 0.0305.
The dashed line indicates the target approximation ratio of 0.97.
B.
Scaling Behavior and Model Generalization
The increase in qubit count from 8 to 16 provides
an opportunity to examine how well the model gener-
alizes to larger graphs and more complex energy land-
scapes. Interestingly, the 16-qubit results exhibit both
a higher mean approximation ratio and smaller vari-
ance than the 8-qubit case. This behavior suggests that
the transformer’s contextual representation benefits from
richer graph embeddings, allowing it to capture higher-
order connectivity patterns more effectively. Moreover,
as the training set for 16-qubit instances contains more
structurally diverse graphs, the model likely develops
a smoother latent representation of the problem–circuit
mapping, leading to more stable performance.
The absence of any noticeable degradation in α with
increasing system size implies that the generative ap-
proach can scale to larger HUBO problems without the
exponential runtime penalties characteristic of classical
parameter-optimization loops. In practice, circuit gen-
eration requires only a single forward pass through the
network, reducing computation time from hours to mil-
liseconds.
C.
Learned Variational Parameters
To gain insight into the model’s internal representation
of optimal circuits, we analyze the distributions of the
learned β and γ parameters as functions of circuit depth
p. Figures 6, 7, 8, and 9 display the average values and
one-standard-deviation intervals for each layer, as well as
corresponding heatmaps across all test graphs.
The β parameters exhibit a smooth oscillatory behav-
ior centered near 3.0 rad, with decreasing variance as cir-
cuit depth increases, indicating that the model converges
toward stable rotation patterns in deeper layers. The γ
parameters, in contrast, cluster near 0.3 rad, consistent
with the short-time evolution limit often observed in op-
timized QAOA circuits. The narrow spread of both pa-
rameters suggests that the model has internalized a con-
sistent structural pattern that generalizes across different
problem instances.
Such behavior parallels trends ob-
served in classically optimized ADAPT-QAOA circuits,
reinforcing that QAOA-GPT does not merely memorize
token sequences but learns physically meaningful param-
eter relationships.
D.
Implications for Circuit Expressivity
The generative model’s ability to reproduce high-
quality circuits without explicit optimization implies an
implicit learning of circuit expressivity conditioned on
graph topology. Because the FEATHER embeddings en-
code global connectivity and higher-order correlations,
the transformer effectively learns a compressed represen-
tation of the operator pool selection and parameteriza-
tion process. This enables the model to generate ans¨atze
that respect the structural constraints of ADAPT-QAOA


--- Page 6 ---
6
Graph ID
0.70
0.75
0.80
0.85
0.90
0.95
Approximation Ratio
Avg. Best AR: 0.8623 (±0.0520)
Avg. Mean AR: 0.8552 (±0.0530)
Best QAOA-GPT Approx. Ratio
Average QAOA-GPT Approx. Ratio
Approx. Ratio (0.97)
FIG. 3: Performance distribution of QAOA-GPT on 200 16-qubit spin glass instances at constrained depth
pmax = 5. For each instance, the model generated 10 circuits. The ‘Best QAOA-GPT Approx. Ratio‘ (blue) shows
the highest approximation ratio (α) achieved per instance, averaging 0.8623 ± 0.0520. The ‘Average QAOA-GPT
Approx. Ratio‘ (orange) shows the mean performance across all 10 generated circuits per instance, averaging
0.8552 ± 0.0530. The dashed line indicates the target approximation ratio of 0.97.
while avoiding irregular circuit topologies that can limit
hardware execution efficiency.
The resulting circuits display smooth scaling of depth
with system size and consistent layer-wise parameter
structure, both of which are desirable for implementa-
tion on NISQ hardware. In practical terms, this indicates
that generative circuit synthesis can deliver expressive,
hardware-compatible ans¨atze without iterative search,
paving the way for automated quantum-algorithm design
pipelines.
E.
Comparison with Classical Optimization
While the present study focuses primarily on genera-
tive performance, the practical advantage of QAOA-GPT
is its computational efficiency. In conventional ADAPT-
QAOA, each circuit layer requires multiple evaluations
of gradient operators and repeated classical optimization
steps, leading to scaling costs that grow rapidly with
both system size and circuit depth. In contrast, QAOA-
GPT generates a complete circuit—including operator
sequence and optimized variational parameters—in a sin-
gle forward pass, reducing computational cost by several
orders of magnitude.
Although the generated circuits
may not always achieve the absolute best energy found
by ADAPT-QAOA, the negligible inference cost makes
the generative approach particularly attractive for rapid
exploration of large problem spaces or as an initialization
scheme for further fine-tuning.
The results presented here demonstrate that QAOA-
GPT
generalizes
effectively
to
higher-order
uncon-
strained binary optimization problems, achieving near-
optimal energies for spin-glass Hamiltonians with cubic
terms. The learned parameter distributions remain sta-
ble across circuit depths and system sizes, and the ap-
proximation ratios remain consistently above 0.93 for all
test cases. Together, these outcomes validate the hypoth-
esis that generative sequence models can learn the struc-
ture of efficient variational quantum circuits directly from
data, providing a scalable and data-driven alternative to
classical optimization-based variational algorithms.
In the following section, we summarize the implica-
tions of these findings for scalable quantum algorithm
design and discuss potential extensions of the QAOA-
GPT framework to broader classes of optimization and
simulation problems.
IV.
CONCLUSIONS
In this work, we have extended the QAOA-GPT frame-
work of Tyagin et al. [1] to address HUBO problems,
focusing on spin-glass Hamiltonians with cubic inter-
action terms. By leveraging FEATHER graph embed-
dings to represent higher-order connectivity and train-
ing on datasets of ADAPT-QAOA circuits, the genera-
tive model learns a direct mapping from graph-encoded
Hamiltonians to optimized variational circuits. This ap-
proach enables autonomous synthesis of adaptive QAOA-


--- Page 7 ---
7
Graph ID
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
Approximation Ratio
Avg. Best AR: 0.9391 (±0.0363)
Avg. Mean AR: 0.8403 (±0.0508)
Best QAOA-GPT Approx. Ratio
Average QAOA-GPT Approx. Ratio
Approx. Ratio (0.97)
FIG. 4: Performance distribution of QAOA-GPT on 200 8-qubit spin glass instances at depth pmax = 15. For each
instance, the model generated 10 circuits. The ‘Best QAOA-GPT Approx. Ratio‘ (blue) shows the highest
approximation ratio (α) achieved per instance, averaging 0.9391 ± 0.0363. The ‘Average QAOA-GPT Approx.
Ratio‘ (orange) shows the mean performance across all 10 generated circuits per instance, averaging 0.8403 ± 0.0508.
The dashed line indicates the target approximation ratio of 0.97.
like circuits without the need for iterative classical opti-
mization.
Our results demonstrate that QAOA-GPT maintains
high approximation ratios, typically exceeding 0.95 for
16-qubit HUBO instances, while reproducing β, γ param-
eter distributions consistent with classically optimized
circuits. The model exhibits robust generalization across
system sizes and interaction orders, indicating that it
effectively captures the structural correlations between
problem topology and circuit design. Because circuit gen-
eration requires only a single forward pass through the
network, the computational cost is reduced by several
orders of magnitude compared to conventional ADAPT-
QAOA workflows, offering a scalable alternative for near-
term quantum optimization.
Beyond confirming the adaptability of QAOA-GPT
to higher-order cost Hamiltonians, this study under-
scores the broader promise of generative modeling as
a paradigm for quantum algorithm discovery.
Future
directions include incorporating reinforcement learning
to refine circuit quality through closed-loop feedback,
extending the approach to constrained or continuous-
variable
Hamiltonians,
and
validating
performance
on real superconducting and trapped-ion hardware.
More
broadly,
integrating
generative
models
with
physics-informed representations offers a path toward
autonomous and data-driven quantum algorithm design,
bridging artificial intelligence and quantum optimization
in the NISQ era.
ACKNOWLEDGMENTS
We acknowledge support by NSF award DGE-2152168
and DOE award DE-SC0024328. A portion of the com-
putation for this work was performed on the University of
Tennessee Infrastructure for Scientific Applications and
Advanced Computing (ISAAC) computational resources.
This research also used resources of the Oak Ridge Lead-
ership Computing Facility, which is a DOE Office of Sci-
ence User Facility supported under Contract DE-AC05-
00OR22725.
[1] I. Tyagin, M. H. Farag, K. Sherbert, K. Shirali, Y. Alex-
eev, and I. Safro, Qaoa-gpt: Efficient generation of adap-
tive and regular quantum approximate optimization algo-
rithm circuits, arXiv preprint arXiv:2504.16350 (2025).
[2] M. Cerezo, A. Arrasmith, R. Babbush, S. C. Benjamin,
S. Endo, K. Fujii, J. R. McClean, K. Mitarai, X. Yuan,
L. Cincio, et al., Variational quantum algorithms, Nature
Reviews Physics 3, 625 (2021).
[3] A.
Choquette,
A.
Di
Paolo,
P.
K.
Barkoutsos,
D. S´en´echal, I. Tavernelli, and A. Blais, Quantum-
optimal-control-inspired ansatz for variational quantum
algorithms, Phys. Rev. Res. 3, 023092 (2021).


--- Page 8 ---
8
Graph ID
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
Approximation Ratio
Avg. Best AR: 0.8427 (±0.0620)
Avg. Mean AR: 0.8147 (±0.0636)
Best QAOA-GPT Approx. Ratio
Average QAOA-GPT Approx. Ratio
Approx. Ratio (0.97)
FIG. 5: Performance distribution of QAOA-GPT on 200 8-qubit spin glass instances at constrained depth pmax = 5.
For each instance, the model generated 10 circuits. The ‘Best QAOA-GPT Approx. Ratio‘ (blue) shows the highest
approximation ratio (α) achieved per instance, averaging 0.8427 ± 0.0620. The ‘Average QAOA-GPT Approx.
Ratio‘ (orange) shows the mean performance across all 10 generated circuits per instance, averaging 0.8147 ± 0.0636.
The dashed line indicates the target approximation ratio of 0.97.
[4] E. Farhi, J. Goldstone, and S. Gutmann, A quan-
tum
approximate
optimization
algorithm
(2014),
arXiv:1411.4028 [quant-ph].
[5] K. Blekos, D. Brand, A. Ceschini, C.-H. Chou, R.-H.
Li, K. Pandya, and A. Summer, A review on quan-
tum approximate optimization algorithm and its vari-
ants, Physics Reports 1068, 1 (2024).
[6] L. Zhu et al., Adaptive quantum approximate optimiza-
tion algorithm for solving combinatorial problems on a
quantum computer, Physical Review Research 4, 043178
(2022).
[7] N. Jain, B. Coyle, E. Kashefi, and N. Kumar, Graph neu-
ral network initialisation of quantum approximate opti-
misation, Quantum 6, 861 (2022).
[8] B. Rozemberczki and R. Sarkar, Characteristic functions
on graphs: Birds of a feather, from statistical descriptors
to parametric models, in Proceedings of the 29th ACM in-
ternational conference on information & knowledge man-
agement (2020) pp. 1325–1334.
[9] Karpathy, nanoGPT: The simplest, fastest repository for
training/finetuning medium-sized gpts.
[10] A. Vaswani,
N. Shazeer,
N. Parmar,
J. Uszkoreit,
L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin,
Attention is all you need, in Advances in Neural Infor-
mation Processing Systems, Vol. 30, edited by I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett (Curran Associates, Inc.,
2017).
[11] NVIDIA Corporation, NVIDIA CUDA-Q framework,
https://github.com/NVIDIA/cuda-quantum.


--- Page 9 ---
9
2
4
6
8
10
12
14
Circuit Depth
0.1
0.2
0.3
0.4
0.5
0.6
Gamma Value
Mean Gamma
±1 STD
(a) Mean γ parameters with standard deviation across
circuit depths
2
4
6
8
10
12
14
Circuit Depth
2.5
3.0
3.5
4.0
4.5
5.0
5.5
6.0
Beta Value
Mean Beta
±1 STD
(b) Mean β parameters with standard deviation across
circuit depths
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Circuit Depth
Graphs
Range: [0.040, 0.626]
0.1
0.2
0.3
0.4
0.5
0.6
Gamma Value
(c) Heatmap of γ values across instances and depths
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Circuit Depth
Graphs
Range: [2.528, 6.214]
3.0
3.5
4.0
4.5
5.0
5.5
6.0
Beta Value
(d) Heatmap of β values across instances and depths
FIG. 6: Variational parameter distributions for QAOA-GPT generated circuits on 8-qubit spin glass instances at
depth pmax = 15. (a) Mean γ parameters with standard deviation, showing a pronounced decrease across layers. (b)
Mean β parameters with standard deviation, maintaining a nearly flat profile. (c) Heatmap of γ values across all
graphs and circuit depths, showing concentration in the range [0.040, 0.626]. (d) Heatmap of β values across all
graphs and circuit depths, showing values in the range [2.528, 6.214].


--- Page 10 ---
10
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
Circuit Depth
0.2
0.3
0.4
0.5
0.6
Gamma Value
Mean Gamma
±1 STD
(a) Mean γ parameters with standard deviation across
circuit depths
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
Circuit Depth
3
4
5
6
7
8
9
Beta Value
Mean Beta
±1 STD
(b) Mean β parameters with standard deviation across
circuit depths
1
2
3
4
5
Circuit Depth
Graphs
Range: [0.156, 0.636]
0.2
0.3
0.4
0.5
0.6
Gamma Value
(c) Heatmap of γ values across instances and depths
1
2
3
4
5
Circuit Depth
Graphs
Range: [2.814, 9.325]
3
4
5
6
7
8
9
Beta Value
(d) Heatmap of β values across instances and depths
FIG. 7: Variational parameter distributions for QAOA-GPT generated circuits on 8-qubit spin glass instances at
depth pmax = 5. (a) Mean γ parameters with standard deviation, showing a gentle decrease across layers. (b) Mean
β parameters with standard deviation, maintaining a nearly flat profile. (c) Heatmap of γ values across all graphs
and circuit depths, showing concentration in the range [0.156, 0.636]. (d) Heatmap of β values across all graphs and
circuit depths, showing values in the range [2.814, 9.325].


--- Page 11 ---
11
2
4
6
8
10
12
14
Circuit Depth
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Gamma Value
Mean Gamma
±1 STD
(a) Mean γ parameters with standard deviation across
circuit depths
2
4
6
8
10
12
14
Circuit Depth
2.8
2.9
3.0
3.1
3.2
3.3
Beta Value
Mean Beta
±1 STD
(b) Mean β parameters with standard deviation across
circuit depths
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Circuit Depth
Graphs
Range: [0.010, 0.623]
0.1
0.2
0.3
0.4
0.5
0.6
Gamma Value
(c) Heatmap of γ values across instances and depths
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Circuit Depth
Graphs
Range: [2.545, 3.640]
2.6
2.8
3.0
3.2
3.4
3.6
Beta Value
(d) Heatmap of β values across instances and depths
FIG. 8: Variational parameter distributions for QAOA-GPT generated circuits on 16-qubit spin glass instances at
depth pmax = 15. (a) Mean γ parameters with standard deviation, showing a smooth monotonic decrease across
layers. (b) Mean β parameters with standard deviation, exhibiting complex behavior with an initial decrease
followed by an upturn at later depths. (c) Heatmap of γ values across all graphs and circuit depths, showing
concentration in the range [0.010, 0.623]. (d) Heatmap of β values across all graphs and circuit depths, showing
values in the range [2.545, 3.640].


--- Page 12 ---
12
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
Circuit Depth
0.2
0.3
0.4
0.5
0.6
Gamma Value
Mean Gamma
±1 STD
(a) Mean γ parameters with standard deviation across
circuit depths
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
Circuit Depth
2.00
2.25
2.50
2.75
3.00
3.25
3.50
3.75
4.00
Beta Value
Mean Beta
±1 STD
(b) Mean β parameters with standard deviation across
circuit depths
1
2
3
4
5
Circuit Depth
Graphs
Range: [0.150, 0.610]
0.2
0.3
0.4
0.5
0.6
Gamma Value
(c) Heatmap of γ values across instances and depths
1
2
3
4
5
Circuit Depth
Graphs
Range: [2.775, 4.917]
3.00
3.25
3.50
3.75
4.00
4.25
4.50
4.75
Beta Value
(d) Heatmap of β values across instances and depths
FIG. 9: Variational parameter distributions for QAOA-GPT generated circuits on 16-qubit spin glass instances at
depth pmax = 5. (a) Mean γ parameters with standard deviation, showing a gentle decrease across layers. (b) Mean
β parameters with standard deviation, maintaining a nearly flat profile. (c) Heatmap of γ values across all graphs
and circuit depths, showing concentration in the range [0.150, 0.610]. (d) Heatmap of β values across all graphs and
circuit depths, showing values in the range [2.775, 4.917].
