--- Page 1 ---
Leveraging LLM Agents for Automated
Optimization Modeling for SASP Problems: A
Graph-RAG based Approach
Tianpeng Pan∗, Wenqiang Pu∗, Licheng Zhao∗, Rui Zhou∗
∗Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong, Shenzhen, Guangdong, China
Email: 224010189@link.cuhk.edu.cn, {wpu,zhaolicheng,rui.zhou}@sribd.cn
Abstract—Automated
optimization
modeling
(AOM)
has
evoked considerable interest with the rapid evolution of large
language models (LLMs). Existing approaches predominantly
rely on prompt engineering, utilizing meticulously designed
expert response chains or structured guidance. However, prompt-
based techniques have failed to perform well in the sensor array
signal processing (SASP) area due the lack of specific domain
knowledge. To address this issue, we propose an automated
modeling approach based on retrieval-augmented generation
(RAG) technique, which consists of two principal components:
a multi-agent (MA) structure and a graph-based RAG (Graph-
RAG) process. The MA structure is tailored for the architectural
AOM process, with each agent being designed based on principles
of human modeling procedure. The Graph-RAG process serves
to match user query with specific SASP modeling knowledge,
thereby enhancing the modeling result. Results on ten classi-
cal signal processing problems demonstrate that the proposed
approach (termed as MAG-RAG) outperforms several AOM
benchmarks.
Index Terms—Automated optimization modeling, large lan-
guage
models,
sensor
array
signal
processing,
retrieval-
augmented generation.
I. INTRODUCTION
Sensor Array Signal Processing (SASP) has experienced
remarkable advancements over the past few decades [1], which
finds utility in a spectrum of applications, including telecom-
munications, radar, sonar, etc. Research within this field has
encompassed areas such as beamforming, direction-of-arrival
(DOA) estimation, primal user detection, source localization,
etc. Over time, SASP has witnessed a paradigm shift from
a predominantly parametric approach [2] to optimization
methodologies [1], [3], leading to substantial advances in
various application domains. Typically, SASP problems can
be formulated as optimization problems, where mathematical
formulations (objective functions and constraints) are estab-
lished from the prior knowledge of the sensor system models
and the final processing goal.
Traditionally, solving SASP problems necessitates the man-
ual formulation and development of algorithms by human
experts. However, the recent invention of Large Language
Models (LLMs) demonstrates the potential to revolutionize
SASP problem-solving. In particular, LLMs are capable of
comprehending natural language inputs and generating logical
sequences as responses, allowing users to describe the SASP
problem and requirement in an intuitive way. Moreover, LLM
has shown talent towards comprehension on mathematical
equations [4]–[6]. This enables the automation of optimization
model and algorithm suggestions, streamlining the process of
finding effective solutions for diverse SASP problems. This
approach is termed automated optimization modeling (AOM),
which has great potential for immediate but reasonable solu-
tions for a wide range of SASP applications.
Currently, AOM methods [4], [6] predominantly utilize
prompt engineering, including guiding LLMs to perform step-
by-step reasoning [6]–[12] and deploying multi-agent systems
to generate manually crafted response chains [13]–[16]. This
approach engages LLMs in constructing logical sequences for
problem solving, emulating human cognitive processes [7].
However, the inherent knowledge deficiencies present within
LLMs has not yet been resolved. To clarify the knowledge
referenced, the retrieval-augmented generation (RAG) [17],
[18] method has recently been proposed. Notable performance
improvements have been realized through the optimization
of dataset structure [19] and the enhanced training of the
retriever model [20]–[22]. However, despite these efforts, the
SASP domain involves substantial domain-specific knowledge,
limiting the success achieved by current AOM strategies.
To realize the potential of LLM-assisted AOM for SASP
problem-solving, we introduce an automated modeling ap-
proach, which combines a multi-agent (MA) structure with
a specific graph-based RAG (Graph-RAG) process. The MA
structure is specifically tailored for the architectural complex-
ities of AOM processes, following on principles of human
expert’s problem-solving logic. Each agent in the system
is designed to tackle a segment of the problem, thereby
decomposing a challenging mission into manageable sub-
tasks [13]. The Graph-RAG component enhances this setup by
matching user inputs with detailed domain modeling knowl-
edge. This process mitigates the complexity of the AOM task,
and further improves the performance by ensuring that only
pertinent information is retrieved and utilized in modeling
generation. Unlike traditional RAG, Graph-RAG organizes
prior knowledge using a graph structure, making the retrieval
process precise, crucial for fields like SASP where specific
knowledge is needed [18]. The proposed approach is termed
as MAG-RAG. To evaluate it, we build a testing dataset, which
includes 10 classical SASP problems along with recommended
solutions. The experimental results indicate that MAG-RAG
arXiv:2501.18320v1  [cs.AI]  30 Jan 2025


--- Page 2 ---
Fig. 1. The overall workflow of the proposed method.
approach outperforms several AOM benchmarks. Meanwhile,
several challenging issues are identified and discussed for
further research.
II. THE PROPOSED AOM APPROACH
A. The MAG-RAG Pipeline
The pipeline of the developed approach consists of two
workflows as illustrated in Fig. 1. The blue workflow illustrates
the utilization of the Graph-RAG technique for constructing
a knowledge database from domain-specific documents. This
knowledge database can provide professional optimization
modeling examples tailored to the user’s query input (see
example in Fig. 2). The other workflow in brown is the
automated optimization modeling procedure, requiring the
involvement of several agents. Before we formally introduce
the pipeline structure1, we specify three agents for AOM in the
SASP domain, namely, Extraction Agent AET, Terminology
Agent ATG, and Optimization Modeling Agent AOM. The role
of each agent is explained as follows:
Extraction Agent AET. This agent receives raw documents
{I1, I2, . . . , IN} in the SASP field as input, and strictly
follows instructions from prompt PET to extract knowledge
{O1, O2, . . . , ON} critical for optimization modeling. Take
Ii, i ∈[1, . . . , N] as an example, the key knowledge extrac-
tion process can be formulated as follows:
Oi = AET (PET, Ii) .
(1)
Terminology Agent ATG. This agent transforms original
user input Q into terminological description P. Given a spe-
cially designed prompt instruction PTG, the extraction process
is as follows:
P = ATG (PTG, Q) .
(2)
1Agent in this work refers to one LLM, which takes specific prompt and
other context as input. To make LLM agents being aware of their concrete
responsibilities, each agent is provided a specially designed prompt (available
at https://github.com/advantages/MAG-RAG-for-SASP) that outlines specific
tasks, guidelines, and structured outputs.
Optimization Modeling Agent AOM. This agent provides
a complete modeling result M for P with reference to the
extracted prior knowledge K. Prior knowledge K is obtained
from a Graph-RAG searching process (to be explained in
Sec. II-C) with the query embedding of P. The generation
process of AOM can be formulated as follows:
M = AOM (POM, K, P) .
(3)
The pipeline of AOM is summarized as follows. Firstly,
with the user-input query Q including the scene description, a
Terminology Agent ATG converts unspecified user input query
into a terminological problem description P. Secondly, we
retrieve top-k most relevant documents as reference knowledge
K based on description P. Finally, combining the terminology
description P with reference knowledge K, Agent AOM pro-
vides an answer M as the output. The retrieval technique in
the second step is Graph-RAG which will be explained in the
subsequent section.
B. Graph-RAG Dataset Construction
To assist LLMs with sophisticated SASP modeling, we
construct a graph-based data base, where domain knowledge
is extracted and represented as nodes, then weighted edges are
formed among correlated nodes before knowledge searching.
Modeling Information Requirements: Initially, N domain
knowledge documents {I1, I2, . . . , IN} are collected as raw
property to construct the database. Though we anticipate the
original documents to possess more information, they may also
introduce information redundancy. Since not all the provided
information contributes positively to the AOM task, excessive
information imposes an additional burden on the modeling
agent, which must first extract the essential information from
the documents before proceeding with the subsequent mod-
eling process. Besides, context limitation of LLMs [13], [23]
should also be seriously treated.
Thus, an Extraction Agent AET is developed to distill
the original documents into content pertinent to optimiza-
tion modeling {O1, O2, . . . , ON}. In order to conquer the
uncontrollability of the LLM responses, we make it strictly
follow the modeling mindset of human expert (as illustrated
in Fig.2) with a target-definite prompt, where LLM response is
instructed to consist of five parts: terminological description,
example information, system model, optimization formulation
and optimization algorithm. An example of the results is
illustrated in Fig. 3.
Dataset Construction of Graph-RAG: The extracted con-
tent is formulated as a graph structure [19], [24]:
G = (V, E) ,
(4)
where G denotes the graph dataset, V and E represent nodes
and edges, respectively. Such a structure is chosen due to the
fact that the graph structure is inherently endowed with a hier-
archical process, allowing for a natural division of graphs into
sub-graphs to capture the optimization modeling procedure.
Additionally, graphs present superior flexibility for community


--- Page 3 ---
Fig. 2. Human optimization modeling procedure for SASP problems.
Fig. 3. An example of the output generated by Example Extraction Agent
clustering and efficient RAG searching. The intrinsic logic
and hierarchical relationships inherent in graphs closely align
with the optimization modeling procedure employed by human
experts.
Concretely, a four-layer graph consisting of “System Model
(SM)” layer, “Optimization Formulation (OF)” layer, “Opti-
mization Algorithm (OA)” layer and “Problem Type (PT)”
layer is constructed. For each Oi, i ∈[1, . . . , N], related con-
tent is traced and represented as nodes, which are subsequently
allocated to the corresponding layers. Besides, nodes inherit
the type of their respective layers, and are further assigned a
“keyword” attribute generated using all-MiniLM-L6-v2 [25]
to serve as the searching query.
Two types of edges are developed according to the nodes’
belonging entity: Nodes extracted from the same document
are connected with “single document (SD)” edges weigh-
ing 1.0, following the unoriented chain of “PT-SM-OF-OA”.
Nodes extracted from different documents are linked with
“different documents (DD)” edges, with the edge weights
representing their similarity. Specifically, for nodes ni and
nj, the embedding of each node’s key words is generated as:
vi = fe (fk (ni)), vj = fe (fk (nj)), where fe denotes the
transformation from natural language to feature space using
text-embedding-3-small [26], and fk is the value extraction
process from the node attribute “keyword”. Then, cosine
similarity sij is applied to calculate the relevance:
sij =
vi · vj
||vi||×||vi||.
(5)
If sij is greater than ε, a relationship between ni and nj
is established, with sij assigned as the value of “similarity”
attribute.
C. Knowledge Searching Using Graph-RAG
Knowledge searching process plays a vital role in Graph-
RAG process. Due to the tendency of LLMs to utilize few-
shot learning [27], they prefer to draw on the given examples
for responses. Furthermore, ATG produces content consistent
with SASP terminologies, thus we employ “PT” layer for
knowledge searching. For instance, considering a node np, p ∈
[1, . . . , N], the relevance of it to P can be determined as:
sep =
fe (P) · fe (fk (np))
||fe (P)||×||fe (fk (np))||.
(6)
Then we manually produce a set L that preserves the similarity
of np to P:
L ←L ∪
n
np : sep
o
.
(7)
Finally, the nodes corresponding to the top-k similarities
in L are selected. We build the knowledge K for AOM by
concatenating the node content connected by “SD” edges from
these selected nodes. In this paper, we set k = 3, by taking
into account the context limitation and knowledge richness.
III. EXPERIMENTS
A. Experimental Setup
Dataset: We select ten classical SASP problems to evalu-
ate the AOM performance, including transmitted beam pat-
tern matching (Q1), cooperative sensing under ideal com-
munication conditions (Q2), sensor placement (Q3), MIMO
radar waveform design (Q4), direct positioning determination
(Q5), DOA estimation (Q6), interference signal suppression
(Q7), bearing-based localization (Q8), TOA-based localization
(Q9) and TDOA-FDOA-based localization (Q10). For each
issue, we finely select a number of documents containing
standard modeling approaches to construct dataset SPAMR.
The selected documents contain heuristic modeling strategies
and optimization algorithms that can help researchers and
LLMs improve the modeling process. The implementation
code and evaluation dataset are available at https://github.com/
advantages/MAG-RAG-for-SASP.
Comparison Methods: To fully evaluate MAG-RAG, we
employ two external benchmarks. Pure MA refers to a pure
agent logic chain for AOM, that the knowledge retrieval
process of Graph-RAG is replaced by Knowledge Generation


--- Page 4 ---
Agent AKG. Pure LLM outputs the overall solutions for the
input signal processing issue Q without any reference or prior
knowledge.
Metrics: Five metrics granted different scores in overall
100'are adopted to evaluate the modeling results: Complete-
ness (30'), Standardization (20'), Correctness (30'), Relevance
(10'), Readability (10').
B. Performance Evaluation
With the assistance of three human scientists specializing
in SASP domain, we evaluate all the generated AOM results.
The overall performances are shown in Table I.
TABLE I
OVERALL PERFORMANCE ON DIFFERENT BASE LLMS.
H: HAIKU-3 [28], S: SONNET-3, G3.5: GPT-3.5, G4: GPT-4.
D: PURE LLM, G: MAG-RAG, T: PURE MA
Q1
Q2
Q3
Q4
Q5
Q6
Q7
Q8
Q9
Q10
HD
82
62
40
62
73
72
89
90
85
72
HG
70
60
70
75
64
81
61
63
42
46
HT
87
65
0
70
58
61
48
88
92
76
SD
75
21
42
71
68
75
91
91
90
84
SG
62
15
85
82
80
82
85
96
92
91
ST
70
40
35
79
71
80
80
92
87
81
G3.5D
10
40
0
41
53
51
54
30
61
40
G3.5G
75
40
44
49
64
73
46
38
43
46
G3.5T
35
20
45
28
53
28
15
52
61
43
G4D
60
65
60
70
63
75
74
75
72
59
G4G
92
45
60
52
66
80
64
83
70
71
G4T
65
78
60
55
58
81
52
76
80
68
Three human scientists are responsible for evaluating Q1-
Q3, Q4-Q6 and Q7-Q10, respectively. From Table. I, we dis-
cover that there remains a strong tendency in the given scores,
where LLMs with MAG-RAG tend to achieve higher scores,
despite the varied scoring preferences of the human scientists.
Moreover, Sonnet-3 typically achieves better performance on
modeling tasks across ten selected SASP problems. We statisti-
cally calculate the distributional properties of different metrics,
and the results are shown in Fig. 4.
From Fig. 4(A), we observe that MAG-RAG achieves better
results, occupying 67 percent on items getting the highest
score, while pure MA and pure LLM approaches achieve
only 25 percent and 8 percent, respectively. And in terms
of completeness and correctness, MAG-RAG similarly far
outperforms the comparison benchmarks, demonstrating that
the knowledge extracted using a specially designed graph
database has a positive effect on modeling.
In viewing of standardization, relevance and readability,
three methodologies perform similarly. This phenomenon is
caused by the fact that LLM itself performs well in content
formation, and the insertion of prior knowledge primarily aims
to promote the optimization modeling.
From Fig. 4 (B), we conclude that the utilization of prior
knowledge for specific problems can indeed have a positive
impact on modeling results, with the percentage of improved
scores significantly exceeding that of declined scores. And in
the cases where scores decreased, we statistically discover that
Fig. 4. Statistical results on overall scores. (A) Frequency of different methods
achieving the highest scores across various metrics. (B) Frequency of scoring
gains (positive, negative and no gain) obtained with prior knowledge (both
pure MA and MAG-RAG) compared to pure LLM. (C) Frequency of scoring
gains obtained with MAG-RAG compared to pure LLM. (D) Frequency of
scoring gains obtained with pure MA compared to pure LLM.
four out of the eight samples had reduced scores originating
from Q7. By referencing scientists’ advice on Q7, directly
invoking LLMs usually achieves higher results (89, 91, 54,
74), aligning with the expectations of different base LLMs.
However, with the insertion of extracted prior knowledge,
LLMs may additionally introduce incorrect constraints or miss
key steps in algorithms.
In Fig. 4 (C) and (D), we find that LLMs augmented with
prior knowledge generally yield lower performance in terms
of readability and contextual relevance compared to directly
employing LLMs. Considering the auto-regression process in
LLMs, this phenomenon may be attributed to the follow-
ing factors: During the process of knowledge integration,
the attention mechanism in Transformers often struggles to
allocate attention weights appropriately, leading to biases in
comprehension.
IV. CONCLUSION
In this paper, we propose MAG-RAG approach for AOM,
targeting SASP problems. We transform the AOM process
into consecutive but separate parts, and based on this, a
MA architecture is utilized to assign different sub-tasks into
different LLMs. To enhance the efficiency, a graph-based RAG
is adopted, where prior knowledge can be structurally stored
and searched with more performance improvement.
Finally, we note that the proposed MAG-RAG mainly
explores AOM for different SASP problems, while implicit
relation between similar optimization algorithms is not suf-
ficiently explored. Moreover, the inherent clustering strategy
towards correlative SASP issues that may potentially enhance
the knowledge searching efficiency is still unexplored.


--- Page 5 ---
REFERENCES
[1] M. Pesavento, M. Trinh-Hoang, and M. Viberg, “Three more decades
in array signal processing research: An optimization and structure
exploitation perspective,” IEEE Signal Processing Magazine, vol. 40,
no. 4, pp. 92–106, 2023.
[2] H. Krim and M. Viberg, “Two decades of array signal processing
research: the parametric approach,” IEEE signal processing magazine,
vol. 13, no. 4, pp. 67–94, 1996.
[3] W. Liu, M. Haardt, M. S. Greco, C. F. Mecklenbr¨auker, and P. Willett,
“Twenty-five years of sensor array and multichannel signal processing:
A review of progress to date and potential research directions,” IEEE
Signal Processing Magazine, vol. 40, no. 4, pp. 80–91, 2023.
[4] Z. Tang, C. Huang, X. Zheng, S. Hu, Z. Wang, D. Ge, and B. Wang,
“Orlm: Training large language models for optimization modeling,”
arXiv preprint arXiv:2405.17743, 2024.
[5] P. Song, K. Yang, and A. Anandkumar, “Towards large language
models as copilots for theorem proving in lean,” arXiv preprint
arXiv:2404.12534, 2024.
[6] A. AhmadiTeshnizi, W. Gao, and M. Udell, “Optimus: Optimization
modeling using mip solvers and large language models,” arXiv preprint
arXiv:2310.06116, 2023.
[7] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,
D. Zhou et al., “Chain-of-thought prompting elicits reasoning in large
language models,” Advances in neural information processing systems,
vol. 35, pp. 24 824–24 837, 2022.
[8] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and
K. Narasimhan, “Tree of thoughts: Deliberate problem solving with large
language models,” Advances in Neural Information Processing Systems,
vol. 36, 2024.
[9] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski,
L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk
et al., “Graph of thoughts: Solving elaborate problems with large
language models,” in Proceedings of the AAAI Conference on Artificial
Intelligence, vol. 38, no. 16, 2024, pp. 17 682–17 690.
[10] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large lan-
guage models are zero-shot reasoners,” Advances in neural information
processing systems, vol. 35, pp. 22 199–22 213, 2022.
[11] B. Wang, X. Deng, and H. Sun, “Iteratively prompt pre-trained language
models for chain of thought,” arXiv preprint arXiv:2203.08383, 2022.
[12] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan,
and G. Neubig, “Pal: Program-aided language models,” in International
Conference on Machine Learning.
PMLR, 2023, pp. 10 764–10 799.
[13] Z. Xiao, D. Zhang, Y. Wu, L. Xu, Y. J. Wang, X. Han, X. Fu, T. Zhong,
J. Zeng, M. Song et al., “Chain-of-experts: When llms meet complex
operations research problems,” in The Twelfth International Conference
on Learning Representations, 2023.
[14] C.-M. Chan, W. Chen, Y. Su, J. Yu, W. Xue, S. Zhang, J. Fu, and
Z. Liu, “Chateval: Towards better llm-based evaluators through multi-
agent debate,” arXiv preprint arXiv:2308.07201, 2023.
[15] Y. Talebirad and A. Nadiri, “Multi-agent collaboration: Harnessing the
power of intelligent llm agents,” arXiv preprint arXiv:2306.03314, 2023.
[16] Q. Wu, G. Bansal, J. Zhang, Y. Wu, S. Zhang, E. Zhu, B. Li,
L. Jiang, X. Zhang, and C. Wang, “Autogen: Enabling next-gen llm
applications via multi-agent conversation framework,” arXiv preprint
arXiv:2308.08155, 2023.
[17] G. Mialon, R. Dess`ı, M. Lomeli, C. Nalmpantis, R. Pasunuru,
R. Raileanu, B. Rozi`ere, T. Schick, J. Dwivedi-Yu, A. Celikyil-
maz et al., “Augmented language models: a survey,” arXiv preprint
arXiv:2302.07842, 2023.
[18] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,
H. K¨uttler, M. Lewis, W.-t. Yih, T. Rockt¨aschel et al., “Retrieval-
augmented generation for knowledge-intensive nlp tasks,” Advances in
Neural Information Processing Systems, vol. 33, pp. 9459–9474, 2020.
[19] D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt,
and J. Larson, “From local to global: A graph rag approach to query-
focused summarization,” arXiv preprint arXiv:2404.16130, 2024.
[20] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang, “Retrieval
augmented language model pre-training,” in International conference
on machine learning.
PMLR, 2020, pp. 3929–3938.
[21] O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-
Brown, and Y. Shoham, “In-context retrieval-augmented language mod-
els,” Transactions of the Association for Computational Linguistics,
vol. 11, pp. 1316–1331, 2023.
[22] S. Weijia, M. Sewon, Y. Michihiro, S. Minjoon, J. Rich, L. Mike, and
Y. Wen-tau, “Replug: Retrieval-augmented black-box language models,”
ArXiv: 2301.12652, 2023.
[23] Y. Ding, L. L. Zhang, C. Zhang, Y. Xu, N. Shang, J. Xu, F. Yang, and
M. Yang, “Longrope: Extending llm context window beyond 2 million
tokens,” arXiv preprint arXiv:2402.13753, 2024.
[24] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, Y. Bengio
et al., “Graph attention networks,” stat, vol. 1050, no. 20, pp. 10–48 550,
2017.
[25] Y. He, Z. Yuan, J. Chen, and I. Horrocks, “Language models as hierarchy
encoders,” arXiv preprint arXiv:2401.11374, 2024.
[26] T. Abdullahi, R. Singh, and C. Eickhoff, “Retrieval augmented zero-shot
text classification,” in Proceedings of the 2024 ACM SIGIR International
Conference on Theory of Information Retrieval, 2024, pp. 195–203.
[27] S. Cahyawijaya, H. Lovenia, and P. Fung, “Llms are few-shot in-context
low-resource language learners,” arXiv preprint arXiv:2403.16512,
2024.
[28] A. Anthropic, “The claude 3 model family: Opus, sonnet, haiku,”
Claude-3 Model Card, vol. 1, 2024.
