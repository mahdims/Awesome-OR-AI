--- Page 1 ---
MENAGE: Mixed-Signal Event-Driven Neuromorphic
Accelerator for Edge Applications
Armin Abdollahi, Mehdi Kamal, and Massoud Pedram
Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, USA
{arminabd, mehdi.kamal, pedram}@usc.edu
Abstract—This paper presents a mixed-signal neuromorphic
accelerator architecture designed for accelerating inference with
event-based neural network models. This fully CMOS-compatible
accelerator utilizes analog computing to emulate synapse and
neuron operations. A C2C ladder structure implements synapses,
while operational amplifiers (op-amps) are used to realize neuron
functions. To enhance hardware resource utilization and power
efficiency, we introduce the concept of a virtual neuron, where a
single neuron engine emulates a set of model neurons, leverag-
ing the sparsity inherent in event-based neuromorphic systems.
Additionally, we propose a memory-based control technique to
manage events in each layer, which improves performance while
maintaining the flexibility to support various layer types. We also
introduce an integer linear programming (ILP)-based mapping
approach for efficiently allocating the model onto the proposed
accelerator. The accelerator is a general-purpose neuromorphic
platform capable of executing linear and convolutional neural
models. The effectiveness of the proposed architecture is evaluated
using two specially designed neuromorphic accelerators and two
event-based datasets. The results show that the proposed architec-
ture achieves 12.1 TOPS/W energy efficiency when accelerating a
model trained on CIFAR10-DVS.
Index Terms—Neuromorphic Accelerator, Mixed-signal design,
Fully CMOS compatible, Event-Driven, Energy Efficiency.
I. INTRODUCTION
Neuromorphic computing, inspired by the architecture and
functionality of the human brain, offers a highly efficient
alternative to traditional computing paradigms [1]. In the brain,
neurons communicate through electrical impulses or spikes,
which allow for parallel processing and event-driven compu-
tation [2]. Spiking Neural Networks (SNNs) are a key compo-
nent of neuromorphic computing, utilizing discrete spikes for
communication between artificial neurons [3]. This spike-based
approach contrasts with traditional artificial neural networks
(ANNs) that rely on continuous values, leading to significant
reductions in power consumption and enabling the natural
processing of temporal information [4]. Moreover, SNNs have
shown potential in various applications, including sensory pro-
cessing, autonomous systems, and robotics, where low latency
and real-time responses are critical. SNN also could be used
on medical diagnostics [5], and hardware optimization [6], [7]
to enhance the efficiency. Additionally, The use of spiking
neural networks (SNNs) could complement traditional deep
learning models in mortality prediction [8], [9] by leveraging
the event-driven processing of temporal health data to improve
efficiency. By leveraging the sparse nature of spike-based
communication, SNNs excel in environments where data is
dynamic and computational efficiency is essential, making them
ideal for edge computing and other low-power scenarios [10].
The Leaky Integrate-and-Fire (LIF) neuron model, com-
monly used in SNNs, emulates the behavior of biological
neurons by accumulating input signals and firing spikes when a
threshold is exceeded, making SNNs suitable for tasks involv-
ing time-dependent data [11]. This model not only provides a
biologically plausible representation of neural activity but also
introduces inherent noise robustness and adaptability to varying
input patterns, enhancing the system’s ability to handle com-
plex, time-varying stimuli. The LIF model’s reliance on simple
arithmetic operations further contributes to the computational
efficiency of SNNs, making them a promising alternative for
neuromorphic processors.
Despite these advantages, scaling neuromorphic systems
remains a significant challenge due to hardware constraints and
energy inefficiencies. One critical limitation of existing neuro-
morphic chips is their high energy consumption and hardware
complexity when scaling to large datasets and networks [12].
Traditional SNN implementations are often computationally
expensive due to dense synaptic connections and the need for
high precision in neural computations. One promising solution
for improving energy efficiency is analog and mixed-signal
computing. They can significantly enhance energy efficiency by
directly processing in analog form, which reduces the need for
power-intensive digital conversions, making them particularly
suitable for handling the sparse, event-driven nature of neural
models in neuromorphic applications.
Memristor-based neuromorphic systems, which have gained
attention for their potential to enable efficient in-memory
computing, also face significant challenges such as device
variability, limited endurance, and resistance drift, which can
severely impact the stability and accuracy of neural compu-
tations [13], [14]. These issues make it difficult to maintain
consistent performance, particularly in large-scale deployments
where variability can lead to unpredictable behavior. As neu-
romorphic systems become increasingly deployed in real-time
and resource-constrained environments, reducing power con-
sumption while maintaining accuracy is paramount.
To address the aforesaid challenges, this paper describes a
fully CMOS-compatible mixed-signal neuromorphic accelera-
tor architecture, which leverages analog computing to emulate
the synaptic connections and neuron behavior. The C2C ladder
structure, which can act as a high-precision analog multiplier, is
used to scale the spikes based on the model parameters, and op-
arXiv:2410.08403v1  [cs.AR]  10 Oct 2024


--- Page 2 ---
amp-based neurons have been used to emulate the LIF behavior.
To improve the energy efficiency and because of the sparsity
in the events, we suggest modeling more than one neuron in
each physically designed neuron engine. Moreover, to fully
utilize the accelerator components, we suggest an integer linear
programming (ILP) formulation to mathematically formulate
the mapping problem and solve it efficiently. The effectiveness
of the designed accelerator is assessed on two well-known
neuromorphic datasets.
The remainder of this paper is organized as follows. Section
II reviews the prior work. The details of the proposed accel-
erator architecture and the ILP formulation for the mapping
are discussed in Section III. Section IV presents the evaluation
results, and finally, the paper is concluded in Section V.
II. RELATED WORK
Neuromorphic hardware, such as IBM’s TrueNorth and In-
tel’s Loihi, is designed to support the unique requirements of
SNNs, with architectures optimized for sparse, event-driven
computation [12], [15]. These chips integrate multiple cores
to simulate vast networks of neurons and synapses efficiently,
enabling real-time processing while maintaining low power
consumption. [16] demonstrates the use of memristor-based
neural networks for efficient in-situ learning with adaptive
capabilities to hardware imperfections, achieving competitive
classification accuracy on standard machine learning datasets.
In [17], fully memristive neural networks are implemented
using memristors for both neurons and synapses, facilitating
pattern classification via unsupervised synaptic weight updates.
[18] explores the cooperative development of memristor-based
spiking neural networks (SNNs), emphasizing the integration
of neural network architectures and memristor technology for
efficient energy-saving systems. [19] discusses the BrainScaleS-
2 accelerated analog neuromorphic computing architecture,
which integrates both analog and digital components for ef-
ficient spike-based processing. In [20], the authors present
advancements in CMOS-integrated memristive arrays, em-
phasizing their use in high-performance computing systems
based on brain-inspired architectures. The paper discusses
how memristors, combined with crossbar architectures, enable
efficient in-memory computing for vector-matrix multiplication.
In [21], large-scale memristor crossbar arrays are implemented
for neuromorphic computing, enabling efficient parallel in-
memory processing for neural networks. [22] introduces a
sparsity-aware neuromorphic computing unit that combines
spiking and artificial neural networks using leaky integrated
neurons. [23] proposes a stochastic-bits enabled binary spiking
neural network (sBSNN) that uses probabilistic computing for
energy-efficient neuromorphic systems. The network leverages
binary synapses and stochastic neuron. In [24], the authors
present NeuRRAM, a compute-in-memory chip utilizing re-
sistive memory for energy-efficient edge AI applications. [25]
presents a 4-Mbyte compute-in-memory (CIM) macro based on
resistive random-access memory (ReRAM) for AI edge devices.
[26] present a 4M-synapse integrated neural network processor
based on analog ReRAM, with cell current-controlled writing to
achieve high power efficiency. [27] introduces a fully integrated
analog ReRAM-based compute-in-memory chip for efficient
neural network processing with parallel MAC operations. [28]
presents a mixed-signal spiking neural network processor with
8-bit synaptic weights and on-chip learning that operates in
the continuous-time domain. [29] propose a clock-free spiking
neural network for AIoT applications, using a multilevel event-
driven architecture to reduce power consumption and inference
latency. [30] shows a fully integrated spiking neural net-
work combining analog neurons and resistive RAM (RRAM)
synapses, tailored for the N-MNIST classification task.
III. MENAGE ARCHITECTURE
The general structure of the proposed neuromorphic acceler-
ator architecture (called MENAGE) is depicted in Figure 1. The
architecture contains a chain of mixed-signal Neuromorphic
(MX-NEURACORE) engines, each used for executing one layer
of the given neural model. Each MX-NEURACORE consists of
a memory-based controller to manage the received events to
the layer as well as scheduling and sending them to the analog
synapse (A-SYN) engines. The output of A-SYN is connected
to the Analog Neuron (A-NEURON) engine. The generated
pulse by an A-NEURON of a MX-NEURACORE is passed to
the next MX-NEURACORE. In the proposed architecture, the
weights are mapped to the SRAM memories of A-SYNs, while
control signals generated by a distiller are stored on the mem-
ories of the proposed memory-based controller. The control
signals are generated based on the proposed ILP-based mapping
approach, which aims to improve the utilization of the A-SYN
and A-NEURON engines inside each MX-NEURACORE. The
following subsections will discuss the details of the engines
and units of the MX-NEURACORE.
The proposed accelerator supports rate-based spike en-
coding where spikes are pulses passed between the MX-
NEURACOREs. The weights are in the 8-bit digital format.
Hence, the parameters of the given model should be quantized
before mapping them to the accelerator’s memory. In addition,
since our proposed accelerator supports pruned neural models,
we suggest pruning the network before mapping the model onto
the accelerator. The general flow for mapping the given model
on the proposed accelerator is illustrated in Algorithm 1.
A system clock is used to synchronize the digital compo-
nents. When the clock’s rising edge occurs, any new event
received by a MX-NEURACORE is stored in the Event Memory
(MEME). Each received event contains the index of the source
neuron. The MX-NEURACORE includes a controller, which
utilizes a polling-based approach to check the MEME in each
clock cycle. In the case of an event, the controller extracts the
initial address and the number of the rows inside the Synapse
and Neuron Assignment memory (MEMS&N), which contains
the synapse connections. This information is obtained via an
Event-to-Address memory (MEME2A). It may take more than
one clock cycle to dispatch the received event from a neuron
to the destination neurons. In this case, the controller does not
fetch any new event from the MEME.
In MEME2A, the initial bits specify the number of con-
nections or rows that need to be read from MEMS&N for a
specific element in MEME2A. In contrast, the subsequent bits


--- Page 3 ---
Fig. 1. The proposed MENAGE architecture
provide the starting address of these connections in MEMS&N.
This sequential storage scheme facilitates efficient access and
processing of synaptic connections. The MEMS&N utilizes an
assignment derived from the Integer Linear Programming (ILP)
solution, which allocates each connection to the appropriate
MX-NEURACORE hardware component, optimizing resource
utilization and ensuring adherence to hardware constraints.
A. A-NEURON Structure
The A-NEURON emulates the leaky integrate-and-fire (LIF)
neuron, whose structure is illustrated in Figure 2. In the LIF
neuron, the membrane potential V (t) of the neuron evolves
according to the differential equation as shown in (1).
τm
dV (t)
dt
= −V (t) + RmI(t)
(1)
In the above equation, τm denotes the membrane time con-
stant, V (t) is the membrane potential at time t, Rm represents
the membrane resistance, I(t) is the input current to the neuron.
A neuron fires a spike when V (t) exceeds a threshold Vth, and
the membrane potential is reset to Vreset. The implemented
neuron emulates the LIF neuron model using discrete time
steps (clock edges), updating the membrane potential at each
step based on incoming spikes. However, using operational
amplifiers (op-amps) results in significant area and power
overheads, especially given the large number of neurons in
neural models. Additionally, high sparsity is a characteristic
of event-based neural models. To address these challenges,
we propose utilizing a pair of storage cells (i.e., capacitance)
within each neuron engine (A-NEURON), each referred to as a
virtual neuron. This approach allows each A-NEURON to model
multiple neurons using a single circuit for integration and firing,
leading to substantial reductions in energy consumption and
area requirements. It is important to note that neurons from
a given model should be mapped onto an A-NEURON with
lower overhead. We employ an ILP formulation to facilitate
this mapping process, taking into account the overlap between
Algorithm 1 Training, Pruning, Quantization, and Mapping
Process
1: Input: Dataset, Neural model, and Accelerator specifica-
tions
2: Step 1: Train Network
3: Train the network on a dataset.
4: Step 2: Pruning and Quantization
5: Apply pruning to reduce the number of synaptic connec-
tions in the network.
6: Apply quantization to reduce the precision of synaptic
weights.
7: Ensure the network fits within the constraints of the hard-
ware architecture.
8: Step 3: Extract Weights and Spikes
9: Extract the pruned and quantized weights from the trained
network.
10: Step 4: Generate the ILP Formulation
11: Formulate the Integer Linear Programming (ILP) problem
based on the extracted weights for each layer in different
time steps.
12: Define the objective function and constraints to mimic the
Python-level spiking neural network behavior.
13: Step 5: Mapping and Executing on the Hardware
14: Store the ILP solutions as the config bits on the internal
memories.
15: Store the quantized parameters on the weight memory of
A-SYNs.
16: Using LIF Neurons to generate the output spikes for
different layers.
17: Determining the output class based on the output spikes.
18: End
neuron operations based on profiles generated through simula-
tions conducted with SNNTorch [31].
In this structure, for each set of synaptic connections, the
previously determined voltage of a neuron (which is stored in
a dedicated capacitor within the A-NEURON) is restored and
applied to the output of the op-amp. Following the integration
phase, the resulting output voltage is temporarily stored in
the capacitor when the input is accumulated into a voltage.
This approach ensures that the stored voltage is preserved for
subsequent processing cycles. To emulate the leaky behavior
of the neuron, a portion of the stored voltage in the capacitors
is discharged at each time step. The controller generates the
command for this discharge process.
B. A-SYN Engine
C2C ladders can act as an analog multiplier, where one of its
inputs is analog voltage (Vref) while the other one is a multi-
bit digital input (W) [32]. The output of this multiplier is an
analog signal (Vout) which its value is obtained by
Vout = Vref ×
 n−1
X
i=0
 Wi × 2i−n
!
(2)
The digital input of the C2C can be sourced directly from the
memory array, allowing it to function as a computing engine


--- Page 4 ---
Fig. 2. A-NEURON architecture.
located close to the memory array. This arrangement is advan-
tageous for implementing charge-based, CMOS-compatible in-
memory computing. In the proposed A-SYN engine, depicted
in Figure 3, a C2C ladder is utilized to model the synaptic
connections, with the weights of these connections stored in
SRAM memory. The bitlines of the SRAM are connected
to the digital input of the C2C ladder. Additionally, Metal-
Oxide-Metal (MOM) capacitors can implement the C2C ladder,
significantly reducing the area overhead typically associated
with C2C capacitance [33].
Fig. 3. The proposed A-SYN structure
C. Memory Construction
The memory structures for distributing the pulses of the
received events to the proper A-SYN and A-NEURON engines
in the MX-NEURACORE are shown in Figure 4. In MEME,
Ni represents the index of a source neuron in the previous
layer. This index is also the address for memory access to
MEME2A. Note that the number of rows of MEME2A is higher
than the number of neurons in the previous layer. Each row of
MEME2A has two columns, i.e., Bi and Ai. Bi indicates the
number of rows in MEMS&N that are linked to Ni, starting
from the address pointed by Ai. MEMS&N contains information
about the synaptic connections and the destination neurons of
the Ni. This memory has three column groups (which are
appropriately color-coded). The first column group contains
M columns where M denotes the number of the A-NEURON
engines. Each of these columns is a binary value (NIi) that
indicates to which A-NEURON the received spike should be
sent. The second column group contains the virtual neuron
indices and identifies the virtual neuron in each A-NEURON
to which the destination neuron is mapped. Thus the width
of each column of this column type is log(N). Each column
of the third and final column group shows the address of the
synaptic weight in the weight memory of the A-SYN. Since
a source neuron may be connected to more than M available
A-NEURONs, its connections may be defined in a couple of
rows, which, as mentioned before, is indicated by the B⋆.
D. Proposed ILP-based Mapping
The objective of the ILP formulation is to allocate each
neuron in the destination layer to a designated capacitor in a A-
NEURON for each time step and layer. Once all the connections
for a given neuron in the destination layer are processed, the
capacitor tied to that neuron must be reassigned to another.
This problem is NP-complete and can be modeled as an ILP
problem. In the following paragraphs, we provide details of the
ILP formulation for assigning synaptic connections and neurons
to MX-NEURACORE and A-NEURON in a spiking neural
network, ensuring compliance with several imposed constraints.
Note that this ILP must be solved for each layer individually,
requiring multiple ILPs to be solved at each time step. Below,
we present the ILP formulation for a single layer.
ILP is particularly valuable in this context because it allows
us to define the objective function and constraints clearly and
comprehensively. By employing the ILP formulation, we can
minimize the number of unassigned neurons in the spiking
network while ensuring efficient hardware utilization. The
constraints guarantee that the capacity of each A-NEURON, as
defined by the number of capacitors assigned to it, is not ex-
ceeded. Additionally, assigning neurons in the spiking network
to capacitors within the NE is done in a manner that adheres to
fan-out limitations and minimizes the communication overhead.
The binary decision variable xi,j,k, as defined in (3), is set
to 1 if neuron i in the destination layer is assigned to the kth
capacitor of jth A-NEURON and 0 otherwise. This variable is
crucial to the optimization problem, as it reflects the assignment
of neurons to capacitors in A-NEURONs.
xi,j,k =



1,
if neuron i is assigned to the kth
capacitor of the jth A-NEURON
0,
otherwise
(3)
The objective of the ILP is to minimize the (4), which is
the total number of unassigned neurons. This is formulated by
minimizing the sum of (1 −xi,j,k) over all possible combi-
nations. By doing so, the ILP ensures that as many neurons
as possible are assigned to each A-NEURON, reducing the
number of unassigned neurons. Here N1 shows the number of
neurons in the destination layer, M denotes the number of A-
NEURON inside MX-NEURACORE, and N shows the number
of capacitors inside each A-NEURON.
Minimize
N1
X
i=1
M
X
j=1
N
X
k=1
(1 −xi,j,k)
(4)


--- Page 5 ---
Fig. 4. The memory generated by the ILP problem to input to the system verilog
The first constraint stated in (5), called the Engine Capacity
Constraint, ensures that the total number of neurons assigned
to any A-NEURON does not exceed the number of capacitors
in that A-NEURON, denoted by N. For each A-NEURON,
the sum of xi,j,k over all neurons i and capacitors k must
be less than or equal to N. This constraint guarantees that
the capacity limitations of each A-NEURON are respected,
preventing overloading. This is shown in (5).
N1
X
i=1
N
X
k=1
xi,j,k ≤N,
∀j ∈{1, 2, . . . , M}
(5)
The second constraint given in (6), i.e., the Unique Engine
Assignment Constraint, requires that each neuron in the des-
tination layer is assigned to exactly one A-NEURON. This
is achieved by ensuring that the sum of xi,j,k over all A-
NEURONs and capacitors equals 1 for each neuron. This
constraint ensures that each neuron has a unique assignment,
preventing conflicts and ensuring that data routing is straight-
forward and efficient.
M
X
j=1
N
X
k=1
xi,j,k = 1,
∀i ∈{1, 2, . . . , N1}
(6)
The third constraint provided in (7), i.e., the Connection
Constraint in (7), applies to neurons in the source layer. It
states that the sum of connections from each neuron in the
source layer to neurons in the destination layer, across all A-
NEURONs in a MX-NEURACORE, must not exceed the fan-out
limit for that neuron. For each neuron m in the source layer, the
sum of xi,j,k for all connected neurons i and all A-NEURONs
and capacitors must be less than or equal to the predefined
fan-out limit for that neuron. This constraint helps maintain the
structural integrity of the network, ensuring that the connections
do not exceed the maximum allowable fan-out. In this equation,
N2 denotes the number of neurons in the source layer, and Sm
is the set of connections starting from the source layer to ith
neuron in the destination layer.
X
i∈Sm
M
X
j=1
N
X
k=1
xi,j,k ≤fanoutm,
∀m ∈{1, 2, . . . , N2}
(7)
Together, these constraints and the objective function create
a robust framework for optimizing the assignment of neurons
to A-NEURONs and capacitors in them in a way that is both
efficient and adheres to hardware limitations.
IV. RESULTS AND DISCUSSION
A. Experimental Setup
In this study, we employed the SNNTorch framework, a
Python-based library for building and training spiking neural
networks, to execute our models on two neuromorphic datasets,
N-MNIST [34] and CIFAR10-DVS [35]. These datasets repre-
sent spike-based versions of the MNIST and CIFAR10 datasets,
respectively, and are frequently used in neuromorphic comput-
ing research due to their suitability for testing SNN models.
In this work, we have considered a multi-layer perception
(MLP) neural model for both datasets. The network architec-
tures were 200/100/40/10 and 1000/500/200/100/10, in the case
of the N-MNIST and CIFAR10-DVS, respectively. Also, we
designed two proposed accelerator models with 4 (Accel1) and
5 (Accel2) MX-NEURACOREs for executing the N-MNIST
and CIFAR10-DVS. The total weight memory in each MX-
NEURACORE of the Accel1 (Accel2) was 400 KB (20 MB),
and we have considered 10 (20) A-NEURON inside each MX-
NEURACORE. Also, each A-NEURON consisted of 16 (32)
virtual neuron.
For both datasets, pruning and post-training quantization
techniques were applied to optimize the networks while pre-
serving accuracy. For the N-MNIST (CIFAR10-DVS) dataset,
the network achieved an accuracy of 94.75% (65.38%) before
pruning. After applying the unstructured L1 pruning and 8-bit
quantization, the accuracy slightly dropped to 94.1% (65.03%).
Table I summarizes the details of the models and their training
parameters.
TABLE I
DETAILS OF THE MODELS AND THEIR TRAINING PARAMETERS
Attribute
N-MNIST
CIFAR10-DVS
Number of Parameters
0.49 M
33.4 M
Number of Hidden Layers
3 (200/100/40)
4 (1000/500/200/100)
Output Neurons
10
10
Learning Rate
1e-3
1e-3
Epochs
50
100
Pruning Technique
L1 pruning
L1 pruning
Quantization
8-bit post-training
quantization
8-bit post-training
quantization
For the mapping process, the ILP was implemented in
Python, and the PuLP package was utilized to solve the


--- Page 6 ---
TABLE II
COMPARISON OF OUR PROPOSED ACCELERATOR WITH PRIOR WORK BASED ON VARIOUS FEATURES
Author
Neural Operations
TOPS/Watt
Bit Width
Technology
Evaluated Dataset
# Neurons
MENAGE (Accel1)
Analog LIF
3.4
8
90nm
N-MNIST
40
MENAGE (Accel2)
Analog LIF
12.1
8
90nm
CIFAR10-DVS
100
Liu et al. 2023 [29]
Mixed Signal LIF
1.88
4
180nm
MIT-BIH Arrhythmia
102
Qi et al. 2024 [36]
Mixed Signal LIF
0.67–5.4
8
55nm
N/A
128-256
Zhang et al. 2024
[37]
Digital LIF
0.66
8-10
28nm
N-MNIST, DVS-Gesture, N-TIDIGIT, SeNic
522
Liu et al. 2024 [38]
Digital LIF
0.26
N/A
22nm
N-MNIST, DVS-Gesture
N/A
ILP problem. Also, the digital part of the accelerator was
developed by SystemVerilog HDL, and its design parameters
were extracted using the Synopsys Design Compiler tool. On
the other hand, its analog parts have been defined by Spice
scripts and simulated by Synopsys HSpice tool.
B. Results
The functionality of the A-NEURON is shown in Figure 5.
The output spike of the A-NEURON, which is the output of
the comparator (second op-amp), and the output of the first
op-amp in the neuron circuit, along with the input signal, is
shown in this figure. The power consumption and delay for
each A-NEURON are 97nW and 6.72 ns, respectively. Also, the
whole MX-NEURACORE simulation shows that the operating
frequency of the system is 103.2MHz, and Accel1 (Accel2)
could provide 3.4 (12.1) TOPS/W energy efficiency.
Fig. 5. Spice simulation of the designed A-NEURON circuit with input, output,
and integration voltage
Figure 6 and Figure 7 illustrates the average memory usage
of MEMS&N, the highest memory-consuming component in
this study, on N-MNIST and CIFAR10-DVS datasets using the
proposed Accel1 and Accel2, respectively, at various time steps
during the processing of a single input image. The figures
highlight that, due to the high sparsity of spikes in these
datasets, average memory usage remains relatively low most
of the time. However, there are instances where memory usage
rises at specific time steps or in certain layers, particularly
when a large number of spikes occur simultaneously, which
the architecture efficiently manages. Notably, CIFAR10-DVS
exhibits higher spike activity, leading to increased memory
usage compared to N-MNIST. Finally, Table II compares the
features of our proposed neuromorphic accelerator with those of
some fully programmable prior work. This table shows that our
architecture is more power efficient compared to the previous
work. Additionally, our approach employs substantially fewer
neurons than other methods, even when handling a more
complex dataset. This reduction in neuron count contributes to a
significant decrease in the overall area, enhancing the efficiency
of the design.
Fig. 6. The memory utilization of running N-MNIST on Accel1 in different
time steps.
Fig. 7.
The memory utilization of running CIFAR10-DVS on Accel2 in
different time steps.
V. CONCLUSION
This work introduced a mixed-signal neuromorphic accelera-
tor architecture that enhances the efficiency of event-based neu-
ral models using analog computing with C2C ladders and LIF
for synapse and neuron emulation, respectively. By leveraging
virtual neurons, the design improves resource utilization and
power efficiency. The spike management inside the proposed
MX-NEURACORE has been done through soft codes, which


--- Page 7 ---
have been extracted during the model compilation. For efficient
mapping of the neural model on the proposed accelerator, we
have suggested an ILP formulation. The energy efficiency of
the designed accelerator for executing the CIFAR10-DVS was
12.1 TOPS/W.
REFERENCES
[1] Carver Mead.
Neuromorphic electronic systems.
Proceedings of the
IEEE, 78(10):1629–1636, 1990.
[2] Wulfram Gerstner and Werner M Kistler. Spiking neuron models: Single
neurons, populations, plasticity. Cambridge university press, 2002.
[3] Wolfgang Maass. Networks of spiking neurons: the third generation of
neural network models. Neural networks, 10(9):1659–1671, 1997.
[4] Michael Pfeiffer and Thomas Pfeil. Deep learning with spiking neurons:
opportunities and challenges. Frontiers in neuroscience, 12:409662, 2018.
[5] Parsa Razmara, Tina Khezresmaeilzadeh, and B Keith Jenkins. Fever de-
tection with infrared thermography: Enhancing accuracy through machine
learning techniques. arXiv preprint arXiv:2407.15302, 2024.
[6] Mohammad Erfan Sadeghi, Arash Fayyazi, Seyedarmin Azizi, and Mas-
soud Pedram. Peano-vit: Power-efficient approximations of non-linearities
in vision transformers. In Proceedings of the 29th ACM/IEEE Interna-
tional Symposium on Low Power Electronics and Design, pages 1–6,
2024.
[7] Arya Fayyazi, Mehdi Kamal, and Massoud Pedram.
Arco: Adap-
tive multi-agent reinforcement learning-based hardware/software co-
optimization compiler for improved performance in dnn accelerator
design. arXiv preprint arXiv:2407.08192, 2024.
[8] Negin Ashrafi, Armin Abdollahi, Greg Placencia, and Maryam Pishgar.
Effect of a process mining based pre-processing step in prediction of the
critical health outcomes. arXiv preprint arXiv:2407.02821, 2024.
[9] Negin Ashrafi, Yiming Liu, Xin Xu, Yingqi Wang, Zhiyuan Zhao, and
Maryam Pishgar. Deep learning model utilization for mortality prediction
in mechanically ventilated icu patients. Informatics in Medicine Unlocked,
49:101562, 2024.
[10] Kaushik Roy, Akhilesh Jaiswal, and Priyadarshini Panda.
Towards
spike-based machine intelligence with neuromorphic computing. Nature,
575(7784):607–617, 2019.
[11] Anthony N Burkitt. A review of the integrate-and-fire neuron model: I.
homogeneous synaptic input. Biological cybernetics, 95:1–19, 2006.
[12] Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya,
Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, Prasad Joshi, Nabil
Imam, Shweta Jain, et al. Loihi: A neuromorphic manycore processor
with on-chip learning. Ieee Micro, 38(1):82–99, 2018.
[13] Giulia Pacchioni.
Improving memristors’ reliability.
Nature Reviews
Materials, 7:594, 2022.
[14] Huajun Liu, Badri Narayanan, Hua Zhou, and Dillon Fong. Kinetic monte
carlo simulation analysis of the conductance drift in multilevel hfo2-based
rram devices. Nanoscale, 2023.
[15] Filipp Akopyan, Jun Sawada, Andrew Cassidy, Rodrigo Alvarez-Icaza,
John Arthur, Paul Merolla, Nabil Imam, Yutaka Nakamura, Pallab Datta,
Gi-Joon Nam, et al.
Truenorth: Design and tool flow of a 65 mw 1
million neuron programmable neurosynaptic chip. IEEE transactions on
computer-aided design of integrated circuits and systems, 34(10):1537–
1557, 2015.
[16] Can Li, Daniel Belkin, Yunning Li, Peng Yan, Miao Hu, Ning Ge, Hao
Jiang, Eric Montgomery, Peng Lin, Zhongrui Wang, et al. Efficient and
self-adaptive in-situ learning in multilayer memristor neural networks.
Nature communications, 9(1):2385, 2018.
[17] Zhongrui Wang, Saumil Joshi, Sergey Savel’Ev, Wenhao Song, Rivu
Midya, Yunning Li, Mingyi Rao, Peng Yan, Shiva Asapu, Ye Zhuo,
et al. Fully memristive neural networks for pattern classification with
unsupervised learning. Nature Electronics, 1(2):137–145, 2018.
[18] Huihui Peng, Lin Gan, Xin Guo, HH Peng, L Gan, and X Guo. Memristor
based spiking neural networks: Cooperative development of neural net-
work architecture/algorithms and memristors. Chip, page 100093, 2024.
[19] Johannes Schemmel, Sebastian Billaudelle, Philipp Dauer, and Johannes
Weis. Accelerated analog neuromorphic computing. In Analog Circuits
for Machine Learning, Current/Voltage/Temperature Sensors, and High-
speed Communication: Advances in Analog Circuit Design 2021, pages
83–102. Springer, 2021.
[20] Alexey N Mikhaylov, Evgeny G Gryaznov, Maria N Koryazhkina, Ilya A
Bordanov, Sergey A Shchanikov, Oleg A Telminov, and Victor B Kazant-
sev.
Neuromorphic computing based on cmos-integrated memristive
arrays: current state and perspectives.
Supercomputing Frontiers and
Innovations, 10(2):77–103, 2023.
[21] Yesheng Li and Kah-Wee Ang. Hardware implementation of neuromor-
phic computing using large-scale memristor crossbar arrays. Advanced
Intelligent Systems, 3(1):2000137, 2021.
[22] Ying Liu, Zhiyuan Chen, Wentao Zhao, Tianhao Zhao, Tianyu Jia,
Zhixuan Wang, Ru Huang, Le Ye, and Yufei Ma.
Sparsity-aware in-
memory neuromorphic computing unit with configurable topology of
hybrid spiking and artificial neural network.
IEEE Transactions on
Circuits and Systems I: Regular Papers, 2024.
[23] Minsuk Koo, Gopalakrishnan Srinivasan, Yong Shim, and Kaushik Roy.
Sbsnn: Stochastic-bits enabled binary spiking neural network with on-chip
learning for energy efficient neuromorphic computing at the edge. IEEE
Transactions on Circuits and Systems I: Regular Papers, 67(8):2546–
2555, 2020.
[24] Weier Wan, Rajkumar Kubendran, Clemens Schaefer, Sukru Burc Ery-
ilmaz, Wenqiang Zhang, Dabin Wu, Stephen Deiss, Priyanka Raina,
He Qian, Bin Gao, et al. A compute-in-memory chip based on resistive
random-access memory. Nature, 608(7923):504–512, 2022.
[25] Je-Min Hung, Cheng-Xin Xue, Hui-Yao Kao, Yen-Hsiang Huang, Fu-
Chun Chang, Sheng-Po Huang, Ta-Wei Liu, Chuan-Jia Jhang, Chin-I Su,
Win-San Khwa, et al. A four-megabit compute-in-memory macro with
eight-bit precision based on cmos and resistive random-access memory
for ai edge devices. Nature Electronics, 4(12):921–930, 2021.
[26] Reiji Mochida, Kazuyuki Kouno, Yuriko Hayata, Masayoshi Nakayama,
Takashi Ono, Hitoshi Suwa, Ryutaro Yasuhara, Koji Katayama, Takumi
Mikawa, and Yasushi Gohou. A 4m synapses integrated analog reram
based 66.5 tops/w neural-network processor with cell current controlled
writing and flexible network architecture. In 2018 IEEE Symposium on
VLSI Technology, pages 175–176. IEEE, 2018.
[27] Qi Liu, Bin Gao, Peng Yao, Dong Wu, Junren Chen, Yachuan Pang,
Wenqiang Zhang, Yan Liao, Cheng-Xin Xue, Wei-Hao Chen, et al. 33.2
a fully integrated analog reram based 78.4 tops/w compute-in-memory
chip with fully parallel mac computing.
In 2020 IEEE International
Solid-State Circuits Conference-(ISSCC), pages 500–502. IEEE, 2020.
[28] Seiji Uenohara and Kazuyuki Aihara. A 18.7 tops/w mixed-signal spiking
neural network processor with 8-bit synaptic weight on-chip learning that
operates in the continuous-time domain. IEEE Access, 10:48338–48348,
2022.
[29] Ying Liu, Yufei Ma, Wei He, Zhixuan Wang, Linxiao Shen, Jiayoon
Ru, Ru Huang, and Le Ye.
An 82-nw 0.53-pj/sop clock-free spiking
neural network with 40-µs latency for aiot wake-up functions using
a multilevel-event-driven bionic architecture and computing-in-memory
technique. IEEE Transactions on Circuits and Systems I: Regular Papers,
70(8):3075–3088, 2023.
[30] A Valentian, F Rummens, E Vianello, T Mesquida, C Lecat-Mathieu
de Boissac, O Bichler, and C Reita. Fully integrated spiking neural net-
work with analog neurons and rram synapses. In 2019 IEEE International
Electron Devices Meeting (IEDM), pages 14–3. IEEE, 2019.
[31] Jason K Eshraghian, Benjamin R Ward, Emre O Neftci, Leo Wang,
Yaocheng Chua, Changhao Wen, Xunzhao Xiang, Daniel Abbott, Ben-
jamin Benz, Raymond Quek, and Tara J Hamilton.
Training spiking
neural networks with surrogate gradients: Theory, methods, and appli-
cations. IEEE Transactions on Neural Networks and Learning Systems,
33(9):3059–3071, 2021.
[32] Behzad Razavi. The r-2r and c-2c ladders [a circuit for all seasons]. IEEE
Solid-State Circuits Magazine, 11(3):10–15, 2019.
[33] Hechen Wang, Renzhi Liu, Richard Dorrance, Deepak Dasalukunte, Dan
Lake, and Brent Carlton. A charge domain sram compute-in-memory
macro with c-2c ladder-based 8-bit mac unit in 22-nm finfet process for
edge inference. IEEE Journal of Solid-State Circuits, pages 1–14, 2023.
[34] Garrick Orchard, Ajinkya Jayawant, Gregory K Cohen, and Nitish Thakor.
Converting static image datasets to spiking neuromorphic datasets using
saccades. Frontiers in neuroscience, 9:437, 2015.
[35] Hongmin Li, Hanchao Liu, Xiangyang Ji, Guoqi Li, and Luping Shi.
Cifar10-dvs: an event-stream dataset for object classification. Frontiers
in neuroscience, 11:309, 2017.
[36] Xiang’ao Qi, Xiangting Li, Yuqing Lou, Yongfu Li, Guoxing Wang, Kea-
Tiong Tang, and Jian Zhao. A 0.67-to-5.4 tsops/w spiking neural network
accelerator with 128/256 reconfigurable neurons and asynchronous fully
connected synapses. IEEE Journal of Solid-State Circuits, 2024.


--- Page 8 ---
[37] Jilin Zhang, Dexuan Huo, Jian Zhang, Chunqi Qian, Qi Liu, Liyang
Pan, Zhihua Wang, Ning Qiao, Kea-Tiong Tang, and Hong Chen. Anp-
i: A 28-nm 1.5-pj/sop asynchronous spiking neural network processor
enabling sub-0.1-µj/sample on-chip learning for edge-ai applications.
IEEE Journal of Solid-State Circuits, 2024.
[38] Ying Liu, Yufei Ma, Ninghui Shang, Tianhao Zhao, Peiyu Chen, Meng
Wu, Jiayoon Ru, Tianyu Jia, Le Ye, Zhixuan Wang, et al. 30.2 a 22nm
0.26 nw/synapse spike-driven spiking neural network processing unit us-
ing time-step-first dataflow and sparsity-adaptive in-memory computing.
In 2024 IEEE International Solid-State Circuits Conference (ISSCC),
volume 67, pages 484–486. IEEE, 2024.
