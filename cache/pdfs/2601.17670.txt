--- Page 1 ---
Grammar-Aware Literate Generative Mathematical Programming with
Compiler-in-the-Loop
Roberto Rossi1 , Steven D. Prestwich2
1Business School, University of Edinburgh, UK
2Insight Centre for Data Analytics, University College Cork, Cork, Ireland
roberto.rossi@ed.ac.uk, s.prestwich@ucc.ie
Abstract
This work investigates generative mathematical
programming through the lens of Algebraic Mod-
elling Languages (AMLs) and compiler-guided
model synthesis. By leveraging PyOPL, an OPL-
like AML compiler that provides detailed syn-
tax diagnostics, we introduce SyntAGM, an end-
to-end system that translates natural language
problem descriptions into PyOPL models via a
generate–compile–assess–revise loop.
SyntAGM
is grammar-aware thanks to in-context exposure to
the PyOPL BNF grammar, and benefits from few-
shot retrieval of literate PyOPL model exemplars.
To obtain a valid PyOPL model that matches the
problem description, SyntAGM mobilises compiler
feedback and an LLM-based alignment judge. In
a comparative study against established prompting
baselines SyntAGM achieves competitive accuracy
with superior token, cost, and latency profiles.
1
Introduction
Operational Research (OR) is the discipline of applying ad-
vanced analytical methods, such as mathematical program-
ming and simulation, to solve complex organisational prob-
lems in a variety of sectors, including supply chain manage-
ment [Peterson et al., 1998], airport operations [Jacquillat and
Odoni, 2015], and workforce planning [Jaillet et al., 2022].
Mathematical programming is a key tool in OR that in-
volves formulating real-world problems as optimisation mod-
els — also called mathematical programmes — that include
variables, constraints, and objective function(s), and that can
be solved by using off-the-shelf solvers (e.g. Gurobi).
The application of Large Language Models (LLM) in OR
is a thriving area of research [Fan et al., 2025]. Within this
domain of research, Generative Mathematical Programming
(GenMP) — which is concerned with translating the natu-
ral language description of an optimisation problem into a
mathematical programme — has received increased attention
in recent years. This task is challenging because it requires
domain-specific knowledge to understand terminology, and
because descriptions often contain implicit constraints that
must be inferred from context. In Technical Appendix (TA)-
A we provide two practical examples.
In a recent survey [Xiao et al., 2025], the authors review
emerging techniques, and highlight opportunities, limitations,
and future research directions at the intersection of LLMs and
mathematical programming. In their taxonomy, they classify
works in three areas: development of evaluation frameworks
[Xing et al., 2024a] and benchmark datasets such as NL4OPT
[Ramamonjison et al., 2022b] and IndustryOR [Huang et
al., 2025]; development of domain-specific fine-tuned LLMs
[Jiang et al., 2025]; and development of advanced infer-
ence frameworks, including multi-agent systems such as
Chain-of-Experts (CoE) [Xiao et al., 2024], and Chain-of-
Thought (CoT) [Wei et al., 2022] variants [Yao et al., 2023;
Besta et al., 2024] — our work sits within this last area.
During inference, LLMs translate a problem description
expressed in natural language into a mathematical pro-
gramme. Prompt engineering is a simple and yet effective
method that can be operationalised in this context. Beyond
so-called “standard prompting,” in which the LLM is directly
prompted to emit the final model, existing studies enhanced
LLM capabilities by either encouraging LLMs to generate ad-
ditional intermediate reasoning steps — an approach referred
to as “X-of-thought;” or by developing LLM-based multi-
agent systems in which multiple “expert” LLMs cooperate
to tackle the problem, as seen in CoE and OptiMUS [Ahma-
diteshnizi et al., 2024]. Despite the rapid pace of expansion
of the literature on advanced inference frameworks, some no-
ticeable gaps remain, which we next discuss.
Gap 1.
Algebraic Modelling Languages (AMLs) let users
express mathematical programmes in a declarative style that
mirrors algebraic notation [Kallrath, 2004]; examples of
AMLs include AMPL, GAMS, and OPL. AMLs separate
model from data and compile to multiple solver back ends,
aiding readability, auditability, and solver independence.
Rather than leveraging AMLs, most prior work on GenMP
focuses on emitting solver API code (e.g., gurobipy,
pulp), which interleaves programming logic and constraints.
In practice, requirement elicitation is iterative and human-
centred: practitioners interview customers, refine require-
ments, prototype, solicit feedback, and iterate. GenMP can
shorten these cycles and support near real-time construction
and execution of model components during requirement anal-
ysis [Freuder, 2017; Freuder, 2024]; but in similar human-in-
the-loop settings, outputs must be readable and auditable, and
use of AMLs is therefore advantageous. Additionally, AML
arXiv:2601.17670v1  [cs.PL]  25 Jan 2026


--- Page 2 ---
models can be checked statically via compilation; compiler
syntax/semantic diagnostics — particularly when tailored to
be expressive and informative — can drive principled, itera-
tive refinement in GenMP. Finally, prior work does not con-
sider new AMLs whose grammar is unknown to LLMs; nor
does it investigate the use of in-context AML grammar paired
with compiler diagnostics to tackle this scenario.
Gap 2.
Existing benchmarks predominantly target deter-
ministic problems. We are not aware of benchmarks featur-
ing two-stage or multi-stage stochastic programs [Birge and
Louveaux, 2011] with scenario data and ground-truth solu-
tions suitable for evaluating GenMP systems. This leaves a
gap in assessing whether LLMs can model uncertainty, stage-
wise structure, and nonanticipativity constraints.
Gap 3.
Beyond accuracy, there is limited evidence on the
trade-off between solution quality, inference cost (tokens, dol-
lars), and latency (end-to-end wall-clock timer) for existing
methods. Few studies report telemetry or characterise cost–
accuracy trade-offs across prompting strategies. In GenMP,
these trade-offs are important given frequent multi-step rea-
soning, compilation retries, and alignment checks.
Contributions.
We make the following contributions.
• We introduce PyOPL, a Python library for parsing OPL-
like models.
PyOPL focuses on the core algebra of
mathematical programming in OPL [Van Hentenryck,
1999] and features actionable error messages an LLM
can leverage to revise a model that fails to compile.
• We introduce Syntax-Aware Generative Modelling
(SyntAGM), a system that translates natural-language
problem descriptions into PyOPL models via an iter-
ative generate–compile–assess–revise workflow.
The
system comprises: (i) a concrete, in-context PyOPL
grammar/semantics reference; (ii) top-k RAG to retrieve
few-shot exemplars; (iii) literate1 modelling; (iv) a be-
spoke PyOPL compiler that provides actionable feed-
back; (v) an LLM-as-a-Judge alignment assessor that
decides whether the model matches the prompt intent;
and (vi) a revision prompt that applies minimal edits
guided by compiler diagnostics or assessor feedback.
• We develop a new benchmark — StochasticOR — com-
prising two-stage and multi-stage stochastic problems
with scenario-based data and ground-truth solutions.
The dataset targets modelling capabilities that are under-
represented in existing corpora, including uncertainty
representation, stage-wise decisions, and nonanticipativ-
ity constraints.
• We instrument token usage, dollar cost, runtime, and
iteration counts to study the cost–quality frontier of
GenMP systems. SyntAGM performs well against sev-
eral baselines (Standard, Chain-of-Thought, Tree-of-
Thoughts, Reflexion, and Chain-of-Experts) on multi-
ple datasets: NL4OPT, ComplexOR, ReSocratic, Indus-
tryOR, and our new StochasticOR benchmark.
1Literate programming [Knuth, 1984] treats programs — in-
cluding mathematical programs — as explanations for readers by
interleaving prose and source code so that the author’s modelling
rationale is presented as a coherent narrative.
2
Related works
Early attempts to translate natural-language optimisation
problems into formal mathematical programs include [Ra-
mamonjison et al., 2022a], which proposed a two-stage
pipeline wherein a BART-based model [Lewis et al., 2020]
generates an intermediate representation (IR) that is sub-
sequently parsed to a canonical optimisation model.
This
line of work also introduced NL4Opt [Ramamonjison et al.,
2022b], a benchmark of 1101 annotated Linear Programming
Word Problems (LPWPs), and experimented with systematic
prompt template design (problem text, task instructions, and
format control) in the associated competition. Following this
first attempt, many studies have followed investigating ap-
plications of LLMs to GenMP; these studies can be broadly
classified in training-based and prompt-based approaches.
Training-based methods leverage data synthesis and in-
struction tuning to fine-tune open-source LLMs — such as
Mistral and LLaMA — for optimisation modelling; examples
include ORLM [Huang et al., 2025], which proposed a semi-
automated process for creating synthetic training data during
fine-tuning to address the lack of suitable large-scale training
datasets; and LLMOPT [Jiang et al., 2025], which trains the
LLMs to produce a five-element formulation comprising sets,
parameters, variables, objective and constraints. These meth-
ods can yield strong adherence to task structure but require
substantial data engineering, risk domain drift, and generally
target solver APIs rather than AMLs, limiting readability and
solver independence.
Prompt-based methods utilise existing LLM interfaces to
automate solving optimisation problems.
Building on the
seminal work of [Brown et al., 2020] that pioneered in-
context learning, these methods aim to instil domain knowl-
edge into LLMs through the prompt.
Following [Xiao et
al., 2025], prompt-based methods can be further decomposed
into two classes: “X-of-thought” and “Multi-expert.”
X-of-thought methods [Chu et al., 2024] encourage LLMs
to generate additional intermediate reasoning steps. This ap-
proach was pioneered by [Wei et al., 2022], who introduced
the Chain-of-Thought approach, which encourages the LLM
to think step-by-step to prevent logical gaps during inference.
Follow up works include Tree-of-Thoughts [Yao et al., 2023]
and Graph-of-Thoughts [Besta et al., 2024] methods, which
employ tree- and graph- structured exploration of reasoning
paths. Reflexion [Shinn et al., 2023] leverages linguistic feed-
back to induce better decision-making: reflexion agents ver-
bally reflect on task feedback signals, and maintain reflective
text in an episodic memory buffer.
Multi-expert methods aim to scale language models for
complex reasoning via multi-agent collaboration systems
[Qian et al., 2025] in which LLMs play the role of experts.
OptiMUS [Ahmaditeshnizi et al., 2024] uses a pre-defined
workflow to engage experts. Chain-of-Experts [Xiao et al.,
2024] leverages a “Conductor” to orchestrate the engagement
process, and a system-level reflection mechanism aligned
with [Shinn et al., 2023]. These frameworks can tackle com-
plex instances but incur heavy token usage and orchestration
overhead.


--- Page 3 ---
While proposing new approaches authors also developed
benchmarks to assess them.
In addition to the NL4Opt
benchmark, another early benchmark is NLP4LP [Ahma-
diteshnizi et al., 2024]. Most problems in these early bench-
marks featured low complexity; to address this gap, authors
introduced the ComplexOR [Xiao et al., 2024] and Indus-
tryOR [Huang et al., 2025] benchmarks, which cover more
complex instances collected from both industrial and aca-
demic scenarios. Unfortunately, all these benchmarks orig-
inally suffered from quality control issues. ReSocratic [Yang
et al., 2025] investigated the use of multiple filters to remove
erroneous cases. [Xiao et al., 2025] manually filtered all error
cases from mainstream benchmarks and compiled a unified,
cleaned collection of optimisation modelling benchmarks to
facilitate future research.
Finally, most existing works on GenMP employ objective-
wise evaluation, in which the focus is solely on the correct-
ness of the final objective function value obtained. Of course,
a correct objective does not guarantee a correct model, there-
fore alternative approaches have been considered. [Deng et
al., 2024; Xiao et al., 2024; Huang et al., 2025] use a test-
driven approach in which the model obtained is tested against
multiple test instances. [Ramamonjison et al., 2022b] ex-
tract coefficients of the objective function and constraints and
compare them with a canonical ground truth representation;
this latter approach is pragmatic, but also quite simplistic,
as determining the equivalence of different formulations of
an optimisation problem is NP-hard in general [Zhai et al.,
2025]. Additionally, similar approaches fail to capture the de-
gree of correctness of a solution; to overcome this limitation
graph-based evaluation approaches have been recently inves-
tigated [Xing et al., 2024b].
Despite these developments,
as remarked in [Xiao et al., 2025], objective-wise evalua-
tion paired with accuracy remains the most widely accepted
measure. Moreover, cost/latency are still under-reported in
the literature, despite their importance for multi-step GenMP
pipelines.
3
Preliminaries
A mathematical programme is defined as follows
min
x
f(x)
subject to
gi(x) ≤0
∀i ∈I
hj(x) = 0
∀j ∈J
where x ∈Rn is the vector of decision variables, f(x) is
the objective function, gi(x) are the inequality constraints in-
dexed by i ∈I, and hj(x) are the equality constraints in-
dexed by j ∈J .
In the rest of this work, we focus on the challenge of trans-
lating the natural language description of an optimisation
problem into a mathematical programme expressed via an
AML.
There are three key aspects to consider when building a
GenMP system that targets an AML: (i) ensuring syntactic
and semantic correctness; (ii) LLM awareness of AML gram-
mar constructs; and (iii) the use of appropriate mathematical
modelling patterns.
PyOPL
A popular AML is the Optimisation Programming
Language (OPL), which was introduced over 25 years ago
[Van Hentenryck, 1999] to simplify the formulation and so-
lution of optimisation problems. To support this study, we de-
veloped PyOPL, a Python library for parsing OPL-like mod-
els.
PyOPL focuses on the core algebra of mathematical
programming in OPL — typed declarations, indexed struc-
tures, tuple types, sums/forall, basic functions and aggregates
— and adds syntax/semantic checks (e.g. typed set valida-
tion, index typing, array shape checks) so that models either
compile cleanly to gurobipy [Gurobi Optimization, LLC,
2024] or scipy (HiGHS) [Huangfu and Hall, 2018], or pro-
duce actionable diagnostics. Error messages produced by the
PyOPL compiler are specific and informative, this means the
LLMs can take full advantage of them while revising a model
that fails to compile. There are over 170 different types of
compilation errors implemented in PyOPL, each providing a
specific message describing the nature of the issue that led to
the error and how the error can be resolved — in TA-B we
provide examples of these error messages. To keep our dis-
cussion focused, in this work we do not discuss development
and architecture of PyOPL.
In-context learning of AML grammar
Albeit grounded
on OPL, PyOPL is a brand new language: it only imple-
ments a subset of the OPL language, so one cannot reason-
ably expect an LLM to know what constructs can or cannot
be used in a model. In contrast to other competing GenMP
approaches, we do not strictly enforce syntax at decode time
via Backus–Naur form (BNF) grammar-constrained decod-
ing. Conversely, we make the LLM aware of the implemented
PyOPL grammar by leveraging in-context learning [Brown
et al., 2020]. In-context learning is the LLM ability to use
prompt instructions and examples as implicit training data to
infer the task and produce outputs without updating its param-
eters. To this end, we leverage a reference Markdown docu-
ment — provided in our supplementary material (SM) — that
specifies the concrete BNF grammar and semantics for the
PyOPL modelling language. The grammar is annotated with
additional plain-text clarifications and relevant semantics, to
capture aspects of the language that cannot be fully expressed
via the BNF grammar.
Modelling patterns in MILP
Design patterns are typi-
cal solutions to common problems found in software design.
Like software development, mathematical modelling is also
rich with modelling patterns. While there exists no book com-
parable to [Gamma et al., 1994] in the realm of mathemati-
cal modelling, every introductory OR textbook discusses a
wealth of modelling patterns that practitioners use to struc-
ture mathematical programs.
In Mixed-integer linear programming (MILP), a modelling
pattern is a small, reusable formulation template—typically
a set of decision variables and linear constraints, sometimes
with an objective fragment—that encodes a logical or combi-
natorial relation (e.g., exactly-one choice, coverage, implica-
tion, precedence) which recurs across MILP formulations.
Inspired by the discussion in [Scalia, 2024], and informed
by the academic literature on mathematical programming, we
have identified a range of 25 MILP modelling patterns com-


--- Page 4 ---
monly found in mathematical programs. Given a set of mod-
elling patterns, one may devise a set of problems that embed
one or more of these patterns (see TA-C, Table 3). Synthetic
literate implementations of 22 exemplar problems — each
of which comprises description, model, and data — repre-
sents a knowledge base (provided in our SM) against which
we perform top-k retrieval to support in-context learning; in
addition, this knowledge base can expand as new problem de-
scriptions and associated models become available.
4
The architecture of SyntAGM
We cast SyntAGM as a sequential decision process whose
state is a prompt context (problem text, PyOPL grammar,
few-shot exemplars, and the latest attempt), whose action is
to emit a candidate PyOPL model–data pair, and whose en-
vironment returns programmatic signals via compilation and
alignment checks. This perspective clarifies the analogy with
actor–evaluator–reflection systems [Shinn et al., 2023] while
highlighting key differences specific to AML-targeted syn-
thesis and compiler-in-the-loop modelling. The complete im-
plementation of SyntAGM is provided in our SM.
The architecture of SyntAGM comprises four modules.
Generator Mg
Given a context st, the generator — i.e. the
LLM — samples a candidate (model, data) ∼πθ(· | st),
where πθ is the stochastic policy induced by the LLM with
parameters θ over the set of allowed artefacts. The generator
prompt asks the LLM to do its reasoning in a private CoT-
style scratchpad (i.e. “think step by step”) and to produce
the final result — that is the model–data pair — in a liter-
ate PyOPL style; note that, unlike classic CoT, which outputs
its reasoning, the system only returns a JSON object in line
with a prescribed output contract (described in our TA-G).
The context further includes a PyOPL Markdown grammar
reference and top-k few-shot model exemplars retrieved with
RAG from the knowledge base (see TA-D for RAG imple-
mentation details) — note that existing few-shot/RAG works
generally inject code-only snippets, not description–model–
data triplets.
Evaluator Me (compiler- and assessor-driven)
The eval-
uator in SyntAGM comprises two elements: (i) the determin-
istic PyOPL compiler, and (ii) an LLM-based judge that as-
sesses alignment to prompt intent. Formally, let
( bt, Et ) ←compile(modelt, datat),
( at, St ) ←assess(modelt, datat, prompt)
where bt ∈{0, 1} denotes successful compilation and Et is
a (possibly empty) set of error messages; at ∈{0, 1} is a
binary flag that signals alignment and St is a textual assess-
ment discussing misalignment. The following binary signal,
representing successful compilation and alignment to prompt
intent
rt = I{ bt = 1 ∧at = 1 },
is used to determine when to terminate and return the final
model–data pair. The evaluator thus supplies both a binary
gate and textual feedback for both AML syntax errors and
prompt intent.
Self-reflection Msr
When syntax errors or a misalignment
are encountered (i.e. rt = 0), the system generates a revi-
sion prompt that contains the latest attempt and the evalua-
tor’s feedback. The prompt either comprises syntax-repair
guidance if Et ̸= ∅, or alignment-repair guidance if Et = ∅
but at = 0; in both cases, it instructs the LLM to implement
minimal edits and preserve modelling intent. Note that the
same LLM backend acts as a revision policy:
(modelt+1, datat+1) ∼Msr(· | st, Et, St).
In practice, Msr and Mg are the same LLM, which is condi-
tioned on different instructions and evidence. More specif-
ically, compiler diagnostics and assessor critiques are the
pieces of information that direct the next attempt.
Memory mem
The state st is the context, which ag-
gregates:
(i) the problem description;
(ii) the PyOPL
grammar reference G; (iii) a small set Fk of few-shot
exemplars (description–model–data) retrieved via RAG;
and (iv) the most recent attempt plus its diagnostics
(modelt, datat, Et, St). SyntAGM memory thus lives in
prompt sections and is refreshed at each iteration. Most im-
portantly, the emitted literate PyOPL artefacts — the model
and data files with labelled objectives/constraints and lo-
calised comments — serve as the agent’s long-term memory.
Each round of feedback (Et, St) is summarised in the model
and data files via literate comments during revision; rationale
is thus “woven” — to use Knuth’s own metaphor — into the
code (i.e. the purpose of a parameter/variable, why a domain
is chosen, why a constraint is repaired) exactly where it mat-
ters. This information not only persists across iterations, but
it is also human-auditable. Compared to a separate reflective
buffer, such as that implemented in Reflexion [Shinn et al.,
2023], embedding memory inside the artefacts has two ad-
vantages: comments appear next to the precise declarations
and constraints they explain; and the resulting model and data
files are reusable exemplars that can be added to SyntAGM
knowledge base to tackle future problems.
Figure 1: High-level generate–compile–assess–revise loop of Syn-
tAGM.
The SyntAGM loop.
Putting the components together,
SyntAGM executes the loop (generate–compile–assess–
revise) illustrated in Figure 1, which comprises the following
four steps.


--- Page 5 ---
1. Initialize s0 ←(problem, G, Fk); query Mg to obtain
(model0, data0).
2. Evaluate: (bt, Et) ←compile(modelt, datat). If
bt = 1, run alignment (at, St) ←assess(·).
3. If rt = I{bt = 1 ∧at = 1} = 1, stop and return the
aligned artefacts and assessment.
4. Else, build a revision prompt from (Et, St) and obtain
(modelt+1, datat+1) ∼Msr(·); update st+1 and re-
peat until the iteration budget is reached.
In compact notation,
(modelt+1, datat+1) ∼πθ




·

problem
| {z }
task
,
G
|{z}
syntax/semantics
,
Fk
|{z}
structural priors
,
Et, St
| {z }
reflective feedback




,
with termination on the first t such that rt = 1.
Implementation details, pseudocode of SyntAGM, and a
worked example are provided in TA-E and TA-F, respectively.
Analogies and differences to Reflexion-style agents.
Syn-
tAGM proceeds by trial-and-error: the LLM crafts a solution
(generate), an evaluator gates progress and provides textual
feedback (evaluate), and lessons from one attempt to another
are carried forward in the prompt state (revise); in this re-
spect, it is similar to Reflexion-style agents [Shinn et al.,
2023]. However, SyntAGM also benefits from determinis-
tic compiler outputs messages that can be leveraged to ad-
dress errors, and from the assessment provided by an LLM
judge, which guides convergence towards the final goal —
SyntAGM loop is curricular [Bengio et al., 2009]: make sure
that the code compiles, and only then check alignment. In
addition, in SyntAGM the prompt features a concrete AML
grammar and a strict JSON output contract. “Memory” ac-
cumulates inside the PyOPL artefacts thanks to the literate
comments, rather than in a separate reflective buffer. This ar-
chitecture prioritises AML correctness, auditability, and cost
control over an open-ended debate among LLM agents — as
observed, for instance, in CoE.
4.1
Prompt templates
In this section, we briefly describe the three prompt templates
that underpin the SyntAGM workflow: generation, revision
and alignment — see TA-H for the complete templates.
Generation.
Builds
a
few-shot-augmented,
grammar-
aware prompt that asks the LLM (as a PyOPL expert) to
think privately step-by-step and emit only a JSON object
with a syntactically valid, literate PyOPL model (.mod) and
matching data (.dat). PyOPL syntax/semantics reference and
top-k exemplars are injected to guide model development.
Revision.
Constructs a repair prompt that preserves mod-
elling intent while fixing either (i) syntax/semantic issues
(compiler diagnostics) or (ii) misalignment (assessor critique)
by introducing minimal edits. It includes the previous at-
tempt, feedback, grammar reference, and few-shots, and
again requests only the JSON with full model & data.
Alignment.
Elicits a judgement of whether model and data
match the problem intent.
Returns a JSON object with a
boolean “aligned” flag and a concise assessment, condition-
ing on the problem text, grammar reference, and the candidate
artefacts produced in the latest attempt.
5
Computational study
In this section, we present our computational study.
5.1
Baselines
As remarked in [Xiao et al., 2025], many GenMP methods
remain closed-source; other methods are interactive, and thus
not directly comparable to end-to-end approaches.
More-
over, our study aims to produce an AML output; and there-
fore some baselines such as [Deng et al., 2024; Huang et al.,
2025] — which are trained to produce models expressed via
Python APIs — are not directly comparable nor applicable.
We therefore limit our study to the following open-source
baselines, which have been adapted to produce PyOPL out-
puts:2 Standard, a single-pass zero-shot (instruction-only)
strategy; Chain-of-Thought (CoT) [Wei et al., 2022]; Tree-
of-Thoughts (ToT) [Yao et al., 2023]; Reflexion [Shinn et
al., 2023]; and Chain-of-Experts (CoE) [Xiao et al., 2024]
— detailed baseline descriptions are provided in TA-I.
5.2
Datasets
We utilise four benchmarks drawn from the unified, cleaned
collection of optimisation modelling benchmarks of [Xiao et
al., 2025]: NL4Opt [Ramamonjison et al., 2022a]; Com-
plexOR [Xiao et al., 2024]; ReSocratic [Yang et al., 2025];
IndustryOR [Huang et al., 2025] These benchmarks cover
different degrees of complexity; and are listed by increasing
complexity. Each benchmark is described in detail in TA-J
(total number of instances: 625). We also introduce a new
synthetic benchmark for stochastic OR problems, a class of
problems that has not been yet considered in GenMP studies.
StochasticOR
StochasticOR (see SM) comprises 10 prob-
lem instances presenting natural language descriptions of
two-stage or multi-stage decision problem under uncertainty,
accompanied by scenario-based data and ground-truth solu-
tions. The dataset has been generated by following a strat-
egy akin to ReSocratic [Yang et al., 2025]: first we produced
a literate stochastic programming model in PyOPL, then we
back-translated it to a natural language question using a back-
translation prompt. Finally, we manually screened the gen-
erated questions, and in our computational study we checked
that in at least one run the ground-truth optimal solution value
could be successfully recovered.
5.3
Model setup, metrics and telemetry
We use the following LLMs from OpenAI: GPT-4o-2024-08-
06, GPT-4.1-2025-04-14, GPT-5-2025-08-06, GPT-5-nano-
2025-08-07, and their open-weight gpt-oss-20b. Unless oth-
erwise stated, we use provider-default decoding/inference pa-
rameters; we do not set max output tokens or custom stop
sequences. Results are from single runs unless noted; for
StochasticOR we report averages over 3 runs. Full model and
system setup details are provided in TA-K. For SyntAGM,
the maximum number of iterations is set to 5; RAG returns
the top k = 3 results for few-shot prompting.
2Since the grammar of PyOPL is brand new, it would be unrea-
sonable to expect baselines to produce valid PyOPL models without
guidance; therefore, to be fair to all approaches, we always inject the
PyOPL grammar in the prompt.


--- Page 6 ---
We adopt the following metrics to compare our approach
against baselines. The accuracy is computed as the number
of occurrences where both observed and expected objective
function values are either both “null” or they are numerical
and close within a given tolerance (relative 1e-6, absolute 1e-
9), over the total number of instances. We also count the pro-
portion of instances where: compilation errors (CE) occur,
runtime errors (RE) are observed, or the proportion of wrong
answers (WA) for which the discrepancy between observed
and expected objective function values exceeds the given tol-
erances. In addition, we record telemetries for the approach in
the form of number of iterations per request, latency (in sec-
onds) measured as end-to-end wall-clock time per request, to-
tal number of tokens (prompt and completion) used to answer
a request, and associated cost ($) for pay-per-use models.
It is surprising to observe that all existing studies in the lit-
erature myopically focus on and report accuracy (or cognate
measures such as F1 score) as the gold standard for compar-
ing different approaches. Our experience suggests that ap-
proaches in the literature feature vastly different complex-
ity in terms of latency of a request, total number of tokens
(prompt and completion) used to answer a request, and as-
sociated $ cost for pay-per-use models. A more nuanced,
Pareto-driven, analysis of the cost incurred to achieve a cer-
tain level of accuracy is necessary. Departing from the ap-
proach commonly adopted in the literature, our computa-
tional study explores these different dimensions and tried
to provide a more nuanced cost-benefit analysis for the ap-
proaches considered.
5.4
Results and discussion
In this section, we analyse the results of our computational
study. Contrary to previous studies in the literature, in addi-
tion to accuracy, we also instrument token usage, dollar cost,
latency, and iteration counts to study the cost–quality frontier.
In our first experiment, we consider the NL4Opt dataset,
and contrast SyntAGM against six state-of-the-art baselines
from the literature by using OpenAI latest state-of-the-art
open weight model gpt-oss-20b. If one solely focuses on ac-
curacy, then CoE leads the panel, and SyntAGM and Reflex-
ion come second. The accuracy-based ranking we obtained
is broadly aligned with previous studies [Xiao et al., 2024;
Xiao et al., 2025]. If, however, one considers Avg. P/C Tkns
and latency, then the picture changes dramatically: while CoE
is the most accurate approach, it is also the most cumber-
some. To be fair to all approaches, we included compilation
and alignment checks in the loop, so that if at the end of an
iteration a model compiles and is judged to be aligned by the
LLM, modelling stops. Even with this enhancement, CoE
average number of iterations remains very high compared to
other methods: CoE produces very lengthy debates among
agents, which translate to very high token consumption fig-
ures — almost ten times the number of tokens consumed by
other approaches here considered. Finally, the end-to-end la-
tency is also extremely high: one needs to wait an average of
10 minutes for model & data to be produced; SyntAGM re-
quires less than two minutes, and Reflexion two minutes and
a half.
A similar picture emerges if we consider ReSocratic and
Method
NL4Opt
Accuracy
CE rate
RE rate
WA
Avg. P/C Tkns
Avg. L (s)
Standard
30.3%
49.0%
17.2%
3.27%
6.92k/2.16k
74
Chain-of-Thought
46.7%
1.87%
46.2%
5.14%
6.52k/2.14k
122
Tree-of-Thoughts
36.0%
15.4%
45.7%
2.80%
10.6k/5.16k
224
Reflexion
59.8%
1.40%
33.1%
5.61%
11.6k/4.22k
158
Chain-of-Experts
69.1%
2.80%
21.03%
7.01%
59.1k/15.7k
651
SyntAGM
61.6%
0.47%
30.8%
7.01%
6.96k/2.99k
114
Table 1: Comparison with baselines on NL4Opt (gpt-oss-20b); Avg.
P/C Tkns: average (P)rompt/(C)ompletion tokens; L: latency in sec-
onds
deploy gpt-oss-20b. CoE leads in terms of accuracy, but fea-
tures a higher number of iterations than other approaches and
a much higher token consumption. Once more, SyntAGM
and Reflexion come second in terms of accuracy, with Syn-
tAGM offering lower latency and lower token consumption
than other methods, being only slightly more expensive than
the Standard approach.
Method
ReSocratic
Accuracy
CE rate
RE rate
WA
Avg. P/C Tkns
Avg. L (s)
Standard
22.7%
60.9%
12.2%
3.99%
7.21k/2.78k
82.7
Chain-of-Thought
45.5%
0.85%
47.2%
6.27%
6.39k/2.39k
129
Tree-of-Thoughts
33.0%
13.9%
47.8%
5.12%
10.3k/5.13k
234
Reflexion
61.5%
3.99%
28.4%
5.98%
12.8k/5.47k
203
Chain-of-Experts
66.1%
4.84%
21.9%
7.12%
65.6k/16.4k
651
SyntAGM
59.5%
3.13%
30.2%
7.12%
8.20k/3.69k
134
Table 2: Comparison with baselines on ReSocratic (gpt-oss-20b)
We also trialled CoE on instances from ComplexOR
benchmark by using GPT-4.1. Once more CoE produced very
cumbersome iterations requiring, in some cases, hundreds of
thousands of tokens and leading to $ costs that were over ten
times higher than those incurred by competitor approaches
such as Reflexion and SyntAGM. These results suggest that
users should carefully consider cost-accuracy tradeoffs while
selecting the most appropriate method for a given application.
In our work, based on these considerations, we opted for ex-
cluding CoE for comparisons involving pay-per-use models.
In Table 3, we test the remaining methods on the Com-
plexOR benchmark by using GPT-4.1. Reflexion, ToT, and
SyntAGM offer high accuracy; but SyntAGM, in addition, of-
fer low latency at comparable cost. As observed for NL4Opt,
other methods such as Standard and CoT are again not com-
petitive and will therefore be excluded from the following ex-
periments.
Method
ComplexOR
Accuracy
CE rate
RE rate
WA
Avg. TPC
Avg. L (s)
Standard
5.56%
77.7%
16.6%
0.0%
$0.028718
19.2
Chain-of-Thought
33.3%
16.6%
22.2%
27.7%
$0.055808
32.5
Tree-of-Thoughts
50.0%
5.56%
11.1%
33.3%
$0.064945
47.94
Reflexion
44.4%
22.2%
16.6%
16.6%
$0.081056
47.76
SyntAGM
50%
11.1%
11.1%
27.7%
$0.061358
28.08
Table 3: Comparison with baselines on ComplexOR (GPT-4.1);
TPC: total prompt cost.
Moving to IndustryOR, since this is the hardest benchmark
from the literature, we now deploy GPT-5. In Table 4 we ob-
serve again Reflexion and SyntAGM offering high accuracy,
with ToT coming a close second. Once more, SyntAGM of-
fers lower latency — model & data are returned in half the
time on average — additionally, SyntAGM is 50% cheaper
than the other two methods.


--- Page 7 ---
Method
IndustryOR
Accuracy
CE rate
RE rate
WA
Avg. TPC
Avg. L (s)
Tree-of-Thoughts
64.2%
0.0%
4.76%
30.9%
$0.249670
276
Reflexion
71.43%
11.9%
0.0%
16.67%
$0.267627
362
SyntAGM
69.0%
0.0%
2.38%
28.5%
$0.128059
155
Table 4: Comparison with baselines on IndustryOR (GPT-5)
Finally, we consider our novel StochasticOR benchmark
and we deploy once more GPT-5; since this benchmark only
comprises 10 instance, we conduct three runs to average
the metrics. Results (Table 5) remain consistent with what
we previously observed. Accuracy figures obtained suggest
that these problems are harder than those in the IndustryOR
benchmark; in terms of latency and cost figures, SyntAGM
remains faster and cheaper than other competing methods as
the problem complexity increases, whilst maintaining a com-
petitive accuracy.
Method
StochasticOR
Accuracy
CE rate
RE rate
WA
Avg. TPC
Avg. L (s)
Tree-of-Thoughts
30.0%
16.6%
16.6%
36.6%
$0.437338
706
Reflexion
48.3%
6.67%
23.3%
21.6%
$0.332889
472
SyntAGM
41.7%
3.33%
13.3%
41.6%
$0.183462
270
Table 5: Comparison with baselines on StochasticOR (GPT-5)
Our analysis suggests that the key benefit of a compiler-in-
the-loop approach is the reduction in effort (i.e. latency and
cost/tokens) required to achieve high accuracy and low CE.
5.5
Ablation study
We carry out an ablation study to investigate the impact of
various system components on accuracy and other metrics.
The study is performed on the IndustryOR dataset. As LLM,
we adopt GPT-5-nano, which has not been utilised yet in our
computational study. More specifically, we carry out three
experiments: (i) we do not provide an in-context BNF gram-
mar document; (ii) we do not provide in-context examples re-
trieved via RAG; (iii) we disable alignment checks; and (iv)
we remove the requirement of literate-style comments in the
generated models, and we strip off literate-style comments
from examples in the RAG knowledge base. Note that we do
not carry out an experiment in which we disable PyOPL com-
piler feedback: the compiler is the ground-truth validator and
reward signal for the iteration loop; removing it turns the task
from “produce code that compiles and matches the prompt”
into “convince an LLM judge,” which is a different problem.
Results are illustrated in Table 6.
IndustryOR
Removed
Accuracy
CE rate
RE rate
WA
Avg. TPC
Avg. L (s)
Avg. I
BNF
42.8%
14.2%
16.6%
26.1%
$0.012848
291
2.61
RAG
42.8%
26.1%
9.52%
21.4%
$0.012115
256
2.90
Alignment
45.2%
14.2%
2.38%
38.1%
$0.010287
192
2.10
Literate
54.7%
4.76%
7.14%
33.33%
$0.009008
190
1.82
Baseline
54.7%
14.2%
4.76%
26.1%
$0.010029
202
2.05
Table 6: Ablation study on SyntAGM components (GPT-5-nano);
Avg. I: average iterations
First, we observe that switching from GPT-5 (Table 4) to
GPT-5-nano — which is per-se a form of ablation — led to
a 14.3% drop in accuracy of SyntAGM, which in Table 6
appears under “Baseline.” The removal from the prompt of
the Markdown document outlining the BNF grammar led to
a 12% drop in accuracy and to a marked increase (+11.8%)
of RE with respect to the baseline. A similar drop in ac-
curacy was observed when few-shot examples obtained via
RAG were omitted from the prompt; this also led to more
runtime errors. One may be surprised that compilation er-
rors did not increase in these two scenarios, but we should
bear in mind that PyOPL partially implements OPL, and that
compiler feedback was not disabled, therefore the algorithm
had opportunities to receive and act upon feedback on com-
pilation errors — the increased number of average iterations
observed with respect to the baseline further confirms this.
Finally, removal of the alignment step produced a 9% drop in
accuracy and a 12% increase of WA. These results imply that
in-context BNF grammar contributes to preventing RE; sim-
ilarly, in-context few-shot examples retrieved via RAG con-
tribute to keeping both RE and iterations low; finally, align-
ment check plays a key role in preventing WA.
The removal of the requirement of including literate-style
comments in the generated models and of literate-style com-
ments from examples in the RAG knowledge base did not
seem to produce differences in accuracy with respect to the
baseline; however, if one carefully inspects the models pro-
duced, a different picture emerges. Inspection of models gen-
erated (see TA-L and SM) confirms that literate-style com-
ments induce better modelling practices (e.g.
appropriate
use of indexes, model-data separation) and, in general, make
models more readable and auditable; while these aspects are
not easily captured in metrics such as accuracy or compilation
errors, they are nevertheless important. Overall, this ablation
study confirms the importance of all mechanisms embedded
in SyntAGM.
6
Conclusion
Grammar-aware in-context learning coupled with principled
compiler diagnostics offers a robust and cost-effective path to
GenMP. By pairing a concrete PyOPL syntax/semantics refer-
ence with a compiler-in-the-loop generate–compile–assess–
revise workflow, SyntAGM produces readable and auditable
AML artefacts while maintaining tight control over syntax
and semantics. Our computational study indicates compet-
itive accuracy and superior token, cost, and latency profiles
with respect to existing baselines. Literate modelling further
supports readability and reuse, while telemetry exposes cost–
accuracy trade-offs often overlooked by prior work.
Code availability
A complete implementation of the system discussed is avail-
able at https://gwr3n.github.io/rhetor/.
Ethical Statement
All datasets used contain only publicly available mathemati-
cal programming instances.
Acknowledgments
This material is based upon works supported by the Science
Foundation Ireland under Grant No. 12/RC/2289-P2 which is
co-funded under the European Regional Development Fund.


--- Page 8 ---
A
The GenMP task
In this section, we illustrate the GenMP task of translating
a natural-language problem description into a mathematical
programme expressed by using an Algebraic Modelling Lan-
guage (in this case PyOPL). The first problem we consider
is a traditional OR problem, the so-called “dynamic lot siz-
ing” (Figure 2). The second problem considered is the initial
motivating example provided in [Xiao et al., 2025], which is
noticeably more complex (Figure 3).
Figure 2: A GenMP task — translating the “dynamic lot sizing”
problem description into a mathematical programme.
B
Examples of PyOPL error messages
In Table 7, we provide four examples of compiler error mes-
sages observed while solving the IndustryOR benchmark.
C
MILP modelling patterns
A range of MILP modelling patterns commonly found in
mathematical programs is provided in Table 8. For this set
of modelling patterns, in Table 9 we identified a set of prob-
lems that embed one or more patterns.
D
Retrieval-Augmented Generation (RAG)
Retrieval-augmented generation is realised as retrieval-
augmented few-shot prompting:
given a user query,
Figure 3: A GenMP task — translating a problem description in the
realm of power generation into a mathematical programme.
the
system
performs
a
full-scan,
embedding-based
semantic
search
over
all
problem-description
.txt
files
under
pyopl/opl models.
Documents
and
query
are
encoded
with
SentenceTransformers
(sentence-transformers/all-MiniLM-L6-v2)
L2-normalised, and scored by cosine similarity implemented
as a dot product, s(d, q) = ⟨d, q⟩(via torch.matmul).
The top-k results, where k = 3, are post-processed by
locating associated .mod and .dat files; each description–
model–data triplet is injected verbatim into the prompt under
a <few shot examples> block with explicit instruc-
tions to treat exemplars as guidance rather than templates
(e.g., avoid copying variable names).
The same retrieval
scheme is reused across initial synthesis and subsequent
syntax/alignment revision prompts.


--- Page 9 ---
ID
Error message
E1
Semantic Error (Line 37): Range bounds must
be integer-valued.
E2
Semantic Error (Line 2): Syntax error in .dat file
at or near token NAME, value ’demand’.
E3
Semantic Error: List parameter ’sTime’ requires
integer indices, got tuple: (0, 35, 35, 0, 0, 200,
0).
E4
Semantic Error: Range ’T’ was supplied in the
data file, but ranges used for indexing must be
declared with explicit bounds in the model file.
Declare it in the model (e.g., ’range T = 1..N;’)
and remove it from the .dat.
E5
Semantic Error: Undeclared symbol ’price’.
Table 7: Sample PyOPL compiler diagnostics observed on Indus-
tryOR.
E
SyntAGM implementation details
Algorithm 1 illustrates the implementation of SyntAGM via
pseudo-code. The algorithm proceed through five stages.
Initialisation (lines 1–4)
Load the PyOPL grammar, re-
trieve top-k exemplars for few-shot guidance, and assemble
the initial generation prompt. Counters for token usage and
placeholders for the evolving model/data strings are zeroed.
Iterative synthesis loop (line 5)
Each iteration first asks
the LLM to produce a candidate PyOPL model & data (line
6) and parses the JSON response (line 7). The candidate is
compiled with the PyOPL compiler to capture syntax/seman-
tic issues (line 8), and artefacts are persisted to the working
paths (line 9).
Assessment path (lines 10–13)
On a clean compile, an ex-
plicit alignment check is performed via a dedicated assess-
ment prompt (line 11). A positive verdict stores the assessor
output and exits early (lines 12–13); otherwise, an alignment-
focused revision prompt is prepared to guide the next attempt
(line 15).
Revision path (line 18)
If compilation fails, a syntax/se-
mantics repair prompt is synthesised that embeds compiler
diagnostics, grammar reference, few-shots, and the prior at-
tempt, requesting minimal edits that preserve intent.
Termination and reporting (lines 21–25)
If the loop ends
with errors, a final assessment is requested to summarise mis-
alignment (line 22). Usage is aggregated into cost/telemetry
(line 24) and the routine returns the assessment with optional
run statistics (line 25).
F
Worked example
We next illustrate the functioning of SyntAGM via a small
worked example. We prompt the system with the problem
description in Figure 4 and Table 10, taken from ComplexOR.
The system proceeds by first retrieving k = 3 few-shot
examples: jobshop, crew pairing, and stochastic scheduling.
Then the generation prompt (embedding the BNF grammar)
is built and submitted. Model and data file are produced and
Algorithm 1 SyntAGM
Require: prompt, model path, data path, iterations
Ensure: assessment (and optionally statistics)
1: Load PyOPL grammar reference (including BNF gram-
mar).
2: Retrieve top-k few-shot exemplars from the knowledge
base via semantic search.
3: Build generation prompt using:
problem description,
grammar reference, and few-shot exemplars.
4: Initialize usage counters and placeholders for assess-
ment, model code, data code.
5: for t ←1 to iterations do
6:
Invoke the LLM to synthesise model+data; record to-
ken usage.
7:
Parse the JSON response to extract model code and
data code.
8:
Compile model+data with the PyOPL compiler; col-
lect any syntax/semantic errors.
9:
Write model code to model path and data code to
data path.
10:
if no syntax/semantic errors then
11:
Build an alignment-assessment prompt (problem,
grammar, model, data) and query the LLM; record
token usage.
12:
if assessment indicates aligned then
13:
Store assessment; break
14:
else
15:
Build an alignment-revision prompt using the as-
sessment, grammar, and few-shots; set it as the
next user prompt.
16:
end if
17:
else
18:
Build a syntax-revision prompt using the compiler
errors, grammar, and few-shots; set it as the next
user prompt.
19:
end if
20: end for
21: if syntax/semantic errors remain then
22:
Request a final assessment from the LLM (problem,
grammar, latest model+data); record usage.
23: end if
24: Estimate costs from cumulative usage and package run
statistics.
25: return assessment (and, if requested, iterations, errors,
usage, and cost estimate)
compiled. At the second iteration round, compilation raises
the following error: “Semantic Error (Line 34): Chained
comparisons (e.g., a ≤b ≤c) are not supported. Split into
two constraints: a ≤b; b ≤c;” (Figure 5) — note the action-
able feedback provided by the compiler.
Since there are compilation errors, a syntax-revision
prompt is built, which incorporates the error message above.
This prompt is submitted to initiate the third iteration. Com-
pilation succeeds and files are assessed for alignment against
the original prompt. Alignment is confirmed, and the final
assessment in Figure 6 is produced.


--- Page 10 ---
Technique
Mathematical Formulation
Description
Logical NOT
not y = 1 −y
Binary inversion:
if y
=
1, then
not y = 0.
Logical AND
z = x · y
z = 1 only if both x = 1 and y = 1.
Logical OR
z ≥x, z ≥y, z ≤x + y
z = 1 if either x = 1 or y = 1.
Bipartite matching
P
j xij = 1 ∀i,
P
i xij = 1 ∀j
Assign each left node to exactly one
right node and vice versa (classical as-
signment).
At-least-one coverage constraints
P
i aijxi ≥1 ∀j
Each element j must be covered by at
least one selected set.
Pattern/column selection variables
min P
p∈P cpxp
s.t. P
p∈P Arpxp ≥br ∀r, xp ∈{0, 1}
Choose composite patterns (e.g., pair-
ings/routes/sets) to cover atomic re-
quirements; equality gives set partition-
ing.
Exactly-one coverage constraints
P
k xjk = 1 ∀j
Pick exactly one option for each item
(e.g., assign one crew to each shift).
Resource capacity constraints
P
j aijxij ≤Ci ∀i
Upper-bound total assigned workload-
/usage for each resource.
Demand satisfaction equalities
P
i xij = dj ∀j
Exactly meet each demand node via in-
bound flow.
Supply satisfaction equalities
P
j xij = si ∀i
Exactly satisfy each source’s supply via
outbound flow.
Subtour elimination (SE)
1 ≤ui ≤N
ui −uj + N xij ≤N −1
(i, j ≥2, i ̸= j)
Prevents subtours using ordering vari-
ables (Miller–Tucker–Zemlin).
Capacity-based SE
ℓ1 = 0; ℓj ≥dj
ℓj ≥ℓi + dj −C (1 −xij)
Accumulates load along arcs to elim-
inate subtours under capacity (VRP
style).
Demand coverage inequalities
P
i xij ≥dj ∀j
Meet or exceed each demand node via
inbound flow (allows over-supply).
Setup Costs/Batch Sizes
y · M ≥x
Setup cost M incurred if x > 0; y is bi-
nary. Alternatively, y is integer number
of batches, each of size M
Conditional Expression
x ≤b + M(1 −y)
Constraint active only if y = 1.
Disjunctive Rules
Multiple “if-then” constraints
Enforces one of several mutually exclu-
sive conditions.
Separation via disjunctive big-M
x ≥y + δ −Mz
y ≥x + δ −M(1 −z)
z ∈{0, 1}, δ > 0
Enforces |x −y| ≥δ by selecting one
of two separated orders; used for adja-
cency in colouring and minimal separa-
tion constraints (e.g., δ = 1 for distinct
colours).
Precedence Constraint
xi + di ≤xj + M(1 −y)
If y = 1, operation i precedes j.
Min-Max Objective
z ≥di −M(1 −xi)
∀i
Minimize the maximum of selected at-
tributes.
Max-Min Objective
z ≤di + M(1 −xi)
∀i
Maximize the minimum of selected at-
tributes.
Inventory Balance
It = It−1 + xt −dt
Tracks inventory over discrete time
horizon.
Initial state constraints
I1 = I0 + x1 −d1
Initialises the dynamic state from given
initial conditions (period 0 to 1).
Inventory with Backlogs
It = It−1 + xt −dt + bt
Includes backlog variable bt.
Stock capacity limits
It ≤C
Upper bound on the state variable (e.g.,
storage/warehouse capacity).
Activity Start/End
yt = xt −xt−1, zt = xt −xt+1
Identifies start/end of activity in time
horizon.
Table 8: MILP modelling patterns and their mathematical formulations


--- Page 11 ---
assignment
crew pairing
crew scheduling
graph coloring
inventory routing
jobshop
knapsack
knapsackp
lot sizing
plant location
production
set covering
set partitioning
transportation
tsp
vehicle routing
warehouse location
workforce planning
p dispersion
on off outsourcing
stochastic production
stochastic scheduling
Logical NOT (1−y)
✓
✓
✓
✓
✓
Logical AND (z=x y)
✓
Logical OR
✓
Bipartite matching
✓
At-least-one coverage constraints
✓
✓
Pattern/column selection variables
✓
✓
✓
Exactly-one coverage constraints
✓
✓
✓
✓
✓
Resource capacity constraints
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
Demand satisfaction equalities
✓
✓
✓
Demand coverage inequalities
✓
✓
✓
✓
✓
✓
✓
Supply satisfaction equalities
✓
Subtour elimination (SE)
✓
Capacity-based SE
✓
Setup Costs/Batch Sizes
✓
✓
✓
✓
Conditional Expression (big-M)
✓
✓
✓
✓
✓
✓
✓
✓
✓
Disjunctive Rules
✓
✓
Separation via disjunctive big-M
Precedence Constraint
✓
Min-Max Objective
✓
✓
Max-Min Objective
✓
Inventory Balance
✓
✓
✓
✓
Initial state constraints
✓
✓
✓
✓
Inventory with Backlogs
✓
Stock capacity limits
✓
Activity Start/End
✓
Table 9: Which modelling patterns are used in each PyOPL problem (✓indicates presence).
Separation
Aircraft
Earliest
Latest
Target
Penalty Before
Penalty After
A1
A2
A3
A1
1
10
4
5
10
0
2
3
A2
3
12
8
10
20
2
0
4
A3
5
15
14
15
30
3
4
0
Table 10: Parameters for the Aircraft Landing Problem instance.
After running the model, we confirm the optimal solution
value (0) matches the ground truth from ComplexOR. The
process consumed two iterations, 40638 prompt, and 2179
completion tokens, for a total cost of $0.081276 on OpenAI
GPT-4.1. A snapshot of the produced PyOPL model with
literate style comments is illustrated in Figure 7.
We also prompt the system a second time, to generate an al-
ternative execution path with a different LLM. This time after
generating model and data files, which compile correctly, the
system detects the misalignment with the problem description
in Figure 8.
An alignment-revision prompt is therefore built, which in-
corporates this assessment. This prompt is submitted to ini-
tiate the second iteration.
Compilation succeeds and files
are assessed again for alignment against the original prompt.
Alignment is confirmed, and the final assessment in Figure 9
is produced.
After running the model, we confirm the optimal solution
value (0) matches the ground truth from ComplexOR. The
process, which completed in two iterations, consumed 35913
prompt, and 12955 completion tokens, for a total cost of
$0.17444125 on OpenAI GPT-5.
G
JSON parsing and validation
We use a strict JSON output contract at the prompt layer and
a relaxed parser at run time, which means we accept plain
JSON or fenced JSON. If a fenced block is present, it is pre-
ferred; otherwise, we extract the first balanced {...} object
from the text. Any leading/trailing prose is discarded by this
relaxed extraction.


--- Page 12 ---
The Aircraft Landing Problem (ALP) is the problem
of deciding a landing time on an appropriate runway
for each aircraft in a given set of aircraft such
that each aircraft lands within a predetermined time
window; and separation criteria between the landing
of an aircraft, and the landing of all successive
aircraft, are respected.
We are given the earliest
landing time, latest landing time, target landing
time, and penalties for landing before or after the
target landing time for each aircraft.
There is
also a separation time that represents the minimum
time required between the landing of two aircraft.
The objective of the problem is to minimize the
total penalties of landing before or after the
target time for each aircraft.
The problem includes
several constraints.
The order constraint ensures
that the aircrafts land in a specific order.
The
separation constraint ensures that there is enough
separation time between the landing of aircraft.
The lower and upper time window constraints ensure
that each aircraft lands within its respective
earliest and latest time windows.
The Aircraft Landing Problem
Figure 4: Aircraft Landing Problem description
Figure 5: Extract from the erroneous PyOPL model for the ALP
generated by the system.
The model and data fully align with the Aircraft
Landing Problem statement.
All primary
aspects|including objective, decision variables,
landing time windows, earliness/tardiness slacks,
and separation constraints for a fixed landing
order|are correctly implemented and indexed,
matching both modelling intent and input data.
Indexing and domains are consistent, and all
parameters referenced in the model are defined
and appropriately structured in the data.
There are
no critical omissions or extraneous constraints; the
structure and data types used match PyOPL grammar
precisely for this problem.
Final Assessment
Figure 6: Final assessment
Generation responses must be a single JSON object with
keys "model" and "data" (both strings). Alignment re-
sponses must be a single JSON object with keys "aligned"
(boolean) and "assessment" (string).
Prompts specify
additionalProperties:false and show an example
of what the output should look like; server-side schema en-
forcement is not used.
H
SyntAGM prompt templates
In this section, we present detailed descriptions of the three
prompt templates the underpin the SyntAGM workflow: gen-
eration, revision and alignment. Text enclosed within double
Figure 7: PyOPL model for the ALP generated by the system.
Model and data are syntactically valid but NOT
aligned with the prompt.
Assessment:
The model
matches the ALP with fixed order:
time windows,
deviation balance linking to earliness/lateness,
and pairwise separation for all successive aircraft.
Indices and data dimensions/types are consistent
with the declarations.
However, the objective
is malformed:
the sum lacks parentheses, so
pLate[i]*l[i] lies outside the sum and uses an
unbound index i, causing a scoping/parsing error.
Fix by writing minimize totalPenalty:
sum(i in
Aircraft) (pEarly[i]*e[i] + pLate[i]*l[i]);.
The
order monotonicity constraints are redundant given
the separation constraints but harmless.
Alignment Assessment
Figure 8: Aircraft Landing Problem — misalignment with the prob-
lem description
The model faithfully captures a single-runway,
fixed-order ALP: t, e, and l domains and
the objective minimize sum pEarly[i]*e[i] +
pLate[i]*l[i] match the penalty structure, with
deviation balance correctly linking e/l to target
times.
Time-window constraints and pairwise
separation for all ordered pairs i<j ensure
feasibility and enforce the prescribed order;
the data dimensions and indexing over Aircraft
are consistent with E, L, Target, pEarly, pLate,
sep, and order.
PyOPL syntax is valid (typed set,
indexed parameters, forall with in-line labels,
and proper sum usage), and the .dat arrays align
with the model indices.
A minor note is that
order monotonicity is redundant when separations
are nonnegative but harmless; also, this is the
fixed-order, single-runway variant rather than an
endogenous-order or multi-runway formulation.
Final Assessment
Figure 9: Aircraft Landing Problem — final assessment for the sec-
ond iteration


--- Page 13 ---
curly braces represent placeholders that are dynamically pop-
ulated during runtime.
H.1
Generation
<role>
You are an expert in mathematical optimisation and PyOPL.
</role>
<task>
Think step by step to produce a syntactically valid PyOPL
model (.mod) and matching data (.dat) that faithfully
implement the problem.
First, reason in a private scratchpad to identify sets,
parameters, decision variables, objective, and
constraints.
Ensure indices, domains (binary/integer/float), and data
are correct and consistent with the problem
description.
Choose correct domains (binary/integer/float) from context
. Add clear labels and explanatory comments.
Label the objective and each constraint.
Add concise comments explaining variables, parameters, and
constraints, aligned to the problem (literate style)
.
If any data are missing, create a small, plausible mock
instance consistent with the model.
</task>
<grammar_reference>
--- BEGIN PYOPL SYNTAX IMPLEMENTATION ---
{{GRAMMAR_IMPLEMENTATION}}
--- END PYOPL SYNTAX IMPLEMENTATION ---
</grammar_reference>
{{FEW_SHOT_EXAMPLES_SECTION}}
<problem_description>
{{PROMPT}}
</problem_description>
<output_requirements>
- Output ONLY the final JSON with the model and data; do
not include your scratchpad in the output.
- Return ONLY a JSON object with keys "model" and "data".
Values are single strings; escape quotes and
backslashes; encode newlines as \n. No extra keys.
- You MAY wrap the JSON in a ‘‘‘json fence containing only
the JSON.
</output_requirements>
<json_schema>
{ "type": "object", "additionalProperties": false, "
required": ["model","data"],
"properties": { "model":{"type":"string"}, "data":{"type
":"string"} } }
</json_schema>
<example_output>
{ "model": "// minimal example\\nfloat a;\\nfloat b;\\
ndvar float x;\\nminimize z: a*x;\\nsubject to {\\n
c1: b*x >= 0;\\n}\\n", "data": "a = 10;\\n b = 5;" }
</example_output>
H.2
Revision
<role>
You are an expert in mathematical optimisation and PyOPL.
</role>
<task>
Revise the model/data to resolve the specified issues
while preserving the intended formulation.
Change only what is necessary; keep syntax valid.
Label the objective and each constraint.
Add concise comments explaining variables, parameters, and
constraints, aligned to the problem (literate style)
.
Use the PyOPL reference strictly for syntax.
</task>
<revision_guidelines>
{{REVISION_GUIDELINE}}
- Make the minimal set of changes necessary to correct
syntax/semantic errors.
- Preserve the original modeling structure when possible.
- Ensure the objective, constraints, indices, and variable
domains reflect the problem description.
- Keep syntax strictly valid.
- Return complete model and data strings; do not return
diffs.
</revision_guidelines>
<grammar_reference>
--- BEGIN PYOPL SYNTAX IMPLEMENTATION ---
{{GRAMMAR_IMPLEMENTATION}}
--- END PYOPL SYNTAX IMPLEMENTATION ---
</grammar_reference>
{{FEW_SHOT_EXAMPLES_SECTION}}
<problem_description>
{{PROMPT}}
</problem_description>
<previous_attempt>
<model>
{{MODEL_CODE}}
</model>
<data>
{{DATA_CODE}}
</data>
</previous_attempt>
<compiler_errors>
{{COMPILER_ERRORS}}
</compiler_errors>
<alignment_assessment>
{{ASSESSMENT}}
</alignment_assessment>
<output_requirements>
- Return ONLY a JSON object with keys "model" and "data".
Values are single strings; escape quotes/backslashes;
encode newlines as \n. No extra keys.
- You MAY wrap the JSON in a ‘‘‘json fence containing only
the JSON.
</output_requirements>
<json_schema>
{ "type":"object", "additionalProperties": false, "
required":["model","data"],
"properties": { "model":{"type":"string"}, "data":{"type
":"string"} } }
</json_schema>
<example_output>
{
"model": "// minimal example\\nfloat a;\\nfloat b;\\
ndvar float x;\\nminimize z: a*x;\\nsubject to {\\n
c1: b*x >= 0;\\n}\\n",
"data": "a = 10;\\n b = 5;"
}
</example_output>
If the revision concerns syntax/semantic errors, then
{{REVISION GUIDELINE}}
is
set
to
“- Fix the
listed syntax/semantic errors.”
Otherwise,
if the revision concerns alignment issues, it is set to “-
Address the alignment issues noted in
the assessment.”


--- Page 14 ---
H.3
Alignment
<role>
You are an expert in mathematical optimization and PyOPL.
</role>
<task>
Judge if model/data fully align with the problem (
objective, constraints, variables, indices, and data
consistency).
Be specific and critical.
</task>
<grammar_reference>
--- BEGIN PYOPL SYNTAX IMPLEMENTATION ---
{{GRAMMAR_IMPLEMENTATION}}
--- END PYOPL SYNTAX IMPLEMENTATION ---
</grammar_reference>
<inputs>
<problem_description>
{{PROMPT}}
</problem_description>
<model>
{{MODEL_CODE}}
</model>
<data>
{{DATA_CODE}}
</data>
</inputs>
<assessment_focus>
- Objective and constraints reflect the prompt intent.
- Decision variables have correct domains and indices.
- Data is consistent with sets/parameters used by the
model.
- Signs, units, and indexing are correct; no missing links
.
- Any critical omissions or extraneous constraints.
- Most impactful improvements if misaligned.
</assessment_focus>
<output_requirements>
- Return ONLY a JSON object with keys "aligned" (boolean)
and "assessment" (string, 3--6 sentences). No extra
keys.
- You MAY wrap the JSON in a ‘‘‘json fence containing only
the JSON.
</output_requirements>
<json_schema>
{ "type":"object", "additionalProperties": false, "
required":["aligned","assessment"],
"properties": { "aligned":{"type":"boolean"}, "
assessment":{"type":"string"} } }
</json_schema>
I
Baselines
We next briefly discuss the baselines considered in our study.
Standard
The “standard” prompting technique implements
a single-pass zero-shot (instruction-only) strategy. We set the
iteration budget to 1, disable RAG and final assessment, and
provide only task instructions plus the PyOPL grammar ref-
erence (no exemplars). In particular, we replace the “task”
in the generation prompt with: “Produce a syntactically valid
PyOPL model (.mod) and matching data (.dat) that faithfully
implement the problem. Ensure the model decision variables,
objective function, and constraints fully align with the pro-
vided problem description. If data are missing, create a small,
plausible mock instance consistent with the model.”
Chain-of-Thought (CoT)
CoT prompting [Wei et al.,
2022] is similar to the former strategy, but with some nuances.
To guide the model’s thought process, the prompt embeds the
sentence “Think step by step to derive a correct PyOPL model
(.mod) and matching data (.dat) for the problem.” After that,
the model is asked to “First, reason in a private scratchpad
to identify sets, parameters, decision variables, objective, and
constraints.” and then “output ONLY the final JSON with the
model and data; do not include your scratchpad in the output.”
This encourages step-by-step reasoning followed by a sum-
marisation stage, whose aim is to aid convergence towards
the desired output. Note that this is CoT-style prompting with
hidden reasoning, rather than classic CoT prompting.
Tree-of-Thoughts (ToT)
ToT [Yao et al., 2023] frames rea-
soning as a tree search over intermediate thoughts. This strat-
egy expands multiple candidate branches, scores them (i.e.,
via compilation or alignment checks), and beam-selects the
most promising one to iteratively refine the solution. Our im-
plementation instantiates ToT as a breadth-bounded search.
At each level, the search expands k = 3 candidates from the
current best solution. Candidates receive scores if they com-
pile successfully and are deemed aligned by an LLM-based
alignment check. The top b = 3 are beam-selected for the
next level. We track the best model/data at every level, and
stops when compilation succeeds and alignment passes or the
maximum number of trials is reached.
Reflexion
Reflexion [Shinn et al.,
2023] leverages a
generate–evaluate–reflect loop.
An iteration generates a
model & data pair, then compiles them with the PyOPL com-
piler, and finally runs an LLM-based alignment check. If the
check fails, the system initiates a reflection step to determine
causes and possible fixes, then restarts the loop by taking into
account recent reflections. Our implementation stores the last
M = 5 reflections, injects the PyOPL grammar into prompts,
and stops when compilation succeeds and alignment passes
or the maximum number of trials is reached.
Chain-of-Experts (CoE)
CoE features a panel of experts
orchestrated by a Conductor to tackle problems. The Con-
ductor’s role is to select experts (e.g., terminology interpreter,
modelling, data builder, code reviewer) who will contribute to
building a forward chain of thoughts; once the chain is built, a
backward reflection is leveraged to amend faulty steps [Xiao
et al., 2024]. The Conductor chooses the next expert on the
basis of the problem description, the PyOPL grammar ref-
erence, and accumulated comments. Once candidate model
& data are produced, compilation results and an LLM-based
alignment check are fed back to agents, so that they can ad-
dress the failure. Our implementation stores the last M = 10
expert comments, executes up to 5 forward steps per trial, and
stops when compilation succeeds and alignment passes or the
maximum number of trials is reached.
J
Datasets
NL4Opt
This dataset [Ramamonjison et al., 2022a] assem-
bles natural-language descriptions of optimisation problems
paired with ground-truth mathematical programs and solu-
tions. The dataset originates from the NL4Opt competition


--- Page 15 ---
in NeurIPS 2022 and comprises 1101 elementary-level linear
programming (LP) problems spanning common OR themes
(e.g., allocation, transportation, blending), of which 289 are
test samples for performance evaluation. The cleaned collec-
tion of test samples comprises 214 instances.
ComplexOR
This dataset [Xiao et al., 2024] was assem-
bled by OR experts; it comprises 37 problems from vari-
ous sources, including academic papers, textbooks, and real-
world industry scenarios. Problems span topics commonly
encountered in OR modelling, from supply chain optimisa-
tion and scheduling problems to warehousing logistics. The
cleaned collection of test samples comprises 18 instances.
ReSocratic
ReSocratic [Yang et al., 2025] adopts a reverse
Socratic approach that generates optimisation problems from
the answer back to a question. By relying on this framework,
the author synthesise 605 problem instances comprising lin-
ear and nonlinear problems; we limit our analysis on the 351
linear problem instances from this dataset, as PyOPL does not
currently support nonlinear programming.
IndustryOR
This dataset [Huang et al., 2025] features 100
real-world industry cases that covers a variety of problem
types and features descriptions with or without tabular data,
thereby increasing problem complexity. The cleaned collec-
tion of test samples comprises 42 instances.
K
Model and system setup
We use the following LLMs from OpenAI: GPT-4o-2024-08-
06, GPT-4.1-2025-04-14, GPT-5-2025-08-06, GPT-5-nano-
2025-08-07, and their open-weight gpt-oss-20b (2025-08).
We utilise OpenAI Responses API for OpenAI models, and
Ollama for gpt-oss-20b. At run time, defaults resolved to:
temperature = 1.0; top p = 1.0; frequency penalty = 0.0; pres-
ence penalty = 0.0; stop sequences: none; response format:
JSON mode (no server-side schema); max output tokens: un-
set (model-enforced ceiling); seed: unset (non-deterministic);
one completion sampled per prompt request. Unless other-
wise stated, we do not override decoding parameters. For
gpt-oss-20b we leave num predict unset. We run gpt-oss-20b
on an Intel(R) Xeon(R) w5-2465X, 3096 MHz, 16 Core(s),
32 Logical Processor(s) with 128 GB of RAM and NVIDIA
RTX 2000 Ada Generation with 16 GB GDDR6. Defaults
and cost per-1k token rates were captured on November 6,
2025 with openai 2.4.0 and Python 3.11.13.
L
Ablation of literate-style comments
In Figure 10 we contrast two models: one is a literate model
produced by our baseline, the other is the model for the same
problem that is produced when we remove the literate-style
comments requirement.
References
[Ahmaditeshnizi et al., 2024] Ali Ahmaditeshnizi, Wenzhi
Gao, and Madeleine Udell. OptiMUS: Scalable optimiza-
tion modeling with (MI)LP solvers and large language
models. In Ruslan Salakhutdinov, Zico Kolter, Katherine
Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and
Felix Berkenkamp, editors, Proceedings of the 41st Inter-
national Conference on Machine Learning, volume 235 of
Proceedings of Machine Learning Research, pages 577–
596. PMLR, 21–27 Jul 2024.
[Bengio et al., 2009] Yoshua Bengio,
J´erˆome Louradour,
Ronan Collobert, and Jason Weston. Curriculum learning.
In Proceedings of the 26th Annual International Confer-
ence on Machine Learning - ICML ’09, pages 1–8, Mon-
treal, Quebec, Canada, 2009. ACM Press.
[Besta et al., 2024] Maciej Besta, Nils Blach, Ales Ku-
bicek, Robert Gerstenberger, Michał Podstawski, Lukas
Gianinazzi,
Joanna
Gajda,
Tomasz
Lehmann,
Hu-
bert Niewiadomski, Piotr Nyczyk, and Torsten Hoe-
fler.
Graph of thoughts:
solving elaborate prob-
lems with large language models.
In Proceedings of
the Thirty-Eighth AAAI Conference on Artificial Intelli-
gence and Thirty-Sixth Conference on Innovative Appli-
cations of Artificial Intelligence and Fourteenth Sympo-
sium on Educational Advances in Artificial Intelligence,
AAAI’24/IAAI’24/EAAI’24. AAAI Press, 2024.
[Birge and Louveaux, 2011] John R Birge and Francois Lou-
veaux. Introduction to stochastic programming. Springer
Series in Operations Research and Financial Engineering.
Springer, New York, NY, 2 edition, June 2011.
[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ry-
der, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child, Aditya
Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,
Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher
Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. Language models are few-shot learn-
ers. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Bal-
can, and H. Lin, editors, Advances in Neural Information
Processing Systems, volume 33, pages 1877–1901. Curran
Associates, Inc., 2020.
[Chu et al., 2024] Zheng Chu, Jingchang Chen, Qianglong
Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng,
Ming Liu, Bing Qin, and Ting Liu.
Navigate through
enigmatic labyrinth a survey of chain of thought reason-
ing: Advances, frontiers and future. In Lun-Wei Ku, An-
dre Martins, and Vivek Srikumar, editors, Proceedings of
the 62nd Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages 1173–
1203, Bangkok, Thailand, August 2024. Association for
Computational Linguistics.
[Deng et al., 2024] Haoxuan Deng, Bohao Zheng, Yirui
Jiang, and Trung Hieu Tran.
CAFA: Coding as auto-
formulation can boost large language models in solving
linear programming problem.
In The 4th Workshop on
Mathematical Reasoning and AI at NeurIPS’24, 2024.
[Fan et al., 2025] Zhenan Fan,
Bissan Ghaddar,
Xinglu
Wang, Linzi Xing, Yong Zhang, and Zirui Zhou. Artifi-
cial intelligence for optimization: Unleashing the poten-
tial of parameter generation, model formulation, and solu-


--- Page 16 ---
Figure 10: A comparison illustrating the impact of literate-style comments on model readability.
tion methods. European Journal of Operational Research,
September 2025.
[Freuder, 2017] Eugene C Freuder.
Explaining ourselves:
Human-aware constraint reasoning.
Proceedings of the
AAAI Conference on Artificial Intelligence, 31(1):4858–
4862, February 2017.
[Freuder, 2024] Eugene C Freuder. Conversational modeling
for constraint satisfaction. Proceedings of the AAAI Con-
ference on Artificial Intelligence, 38(20):22592–22597,
March 2024.
[Gamma et al., 1994] Erich Gamma, Richard Helm, Ralph
Johnson, and John Vlissides. Design patterns. Addison
Wesley, Boston, MA, October 1994.
[Gurobi Optimization, LLC, 2024] Gurobi
Optimization,
LLC. Gurobi Optimizer Reference Manual, 2024.
[Huang et al., 2025] Chenyu Huang, Zhengyang Tang, Shixi
Hu, Ruoqing Jiang, Xin Zheng, Dongdong Ge, Benyou
Wang, and Zizhuo Wang. Orlm: A customizable frame-
work in training large models for automated optimization
modeling. Operations Research, May 2025.
[Huangfu and Hall, 2018] Q Huangfu and J A J Hall. Paral-
lelizing the dual revised simplex method. Math. Program.
Comput., 10(1):119–142, March 2018.
[Jacquillat and Odoni, 2015] Alexandre
Jacquillat
and
Amedeo R Odoni.
An integrated scheduling and op-
erations
approach
to
airport
congestion
mitigation.
Operations Research, 63(6):1390–1410, December 2015.
[Jaillet et al., 2022] Patrick Jaillet, Gar Goei Loke, and
Melvyn Sim. Strategic workforce planning under uncer-
tainty.
Operations Research, 70(2):1042–1065, March
2022.
[Jiang et al., 2025] Caigao Jiang, Xiang Shu, Hong Qian,
Xingyu Lu, Jun Zhou, Aimin Zhou, and Yang Yu. LL-
MOPT: learning to define and solve general optimization
problems from scratch. In The Thirteenth International
Conference on Learning Representations, ICLR 2025, Sin-
gapore, April 24-28, 2025. OpenReview.net, 2025.
[Kallrath, 2004] Josef Kallrath, editor. Modeling languages
in mathematical optimization.
Applied Optimization.
Springer, New York, NY, 2004 edition, February 2004.
[Knuth, 1984] D E Knuth. Literate programming. Comput.
J., 27(2):97–111, February 1984.
[Lewis et al., 2020] Mike Lewis, Yinhan Liu, Naman Goyal,
Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART:
Denoising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension. In
Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel
Tetreault, editors, Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics, pages
7871–7880, Online, July 2020. Association for Compu-
tational Linguistics.
[Peterson et al., 1998] Rein Peterson, David F Pyke, and Ed-
ward A Silver. Decision systems for inventory manage-
ment and production planning.
John Wiley and Sons
(WIE), Brisbane, QLD, Australia, 3 edition, January 1998.
[Qian et al., 2025] Chen Qian, Zihao Xie, Yifei Wang, Wei
Liu, Kunlun Zhu, Hanchen Xia, Yufan Dang, Zhuoyun
Du, Weize Chen, Cheng Yang, Zhiyuan Liu, and Maosong


--- Page 17 ---
Sun. Scaling large language model-based multi-agent col-
laboration. In The Thirteenth International Conference on
Learning Representations, ICLR 2025, Singapore, April
24-28, 2025. OpenReview.net, 2025.
[Ramamonjison et al., 2022a] Rindra Ramamonjison, Ha-
ley Li, Timothy Yu, Shiqi He, Vishnu Rengan, Amin
Banitalebi-dehkordi, Zirui Zhou, and Yong Zhang. Aug-
menting operations research with auto-formulation of op-
timization models from problem descriptions.
In Yun-
yao Li and Angeliki Lazaridou, editors, Proceedings of
the 2022 Conference on Empirical Methods in Natural
Language Processing: Industry Track, pages 29–62, Abu
Dhabi, UAE, December 2022. Association for Computa-
tional Linguistics.
[Ramamonjison et al., 2022b] Rindranirina Ramamonjison,
Timothy Yu, Raymond Li, Haley Li, Giuseppe Carenini,
Bissan Ghaddar, Shiqi He, Mahdi Mostajabdaveh, Amin
Banitalebi-Dehkordi, Zirui Zhou, and Yong Zhang. Nl4opt
competition: Formulating optimization problems based on
their natural language descriptions.
In Marco Ciccone,
Gustavo Stolovitzky, and Jacob Albrecht, editors, Pro-
ceedings of the NeurIPS 2022 Competitions Track, volume
220 of Proceedings of Machine Learning Research, pages
189–203. PMLR, 28 Nov–09 Dec 2022.
[Scalia, 2024] Bruno Scalia. A comprehensive guide to mod-
eling techniques in mixed-integer linear programming.
https://medium.com/data-science/a-comprehensive-guide
-to-modeling-techniques-in-mixed-integer-linear-progr
amming-3e96cc1bc03d, 2024. Accessed: 2025-10-17.
[Shinn et al., 2023] Noah Shinn, Federico Cassano, Ashwin
Gopinath, Karthik Narasimhan, and Shunyu Yao.
Re-
flexion: language agents with verbal reinforcement learn-
ing. In A. Oh, T. Naumann, A. Globerson, K. Saenko,
M. Hardt, and S. Levine, editors, Advances in Neural In-
formation Processing Systems, volume 36, pages 8634–
8652. Curran Associates, Inc., 2023.
[Van Hentenryck, 1999] Pascal Van Hentenryck. The OPL
optimization programming language. MIT Press, London,
England, January 1999.
[Wei et al., 2022] Jason Wei, Xuezhi Wang, Dale Schuur-
mans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,
Quoc V. Le, and Denny Zhou. Chain-of-thought prompt-
ing elicits reasoning in large language models. In Proceed-
ings of the 36th International Conference on Neural In-
formation Processing Systems, NIPS ’22, Red Hook, NY,
USA, 2022. Curran Associates Inc.
[Xiao et al., 2024] Ziyang Xiao, Dongxiang Zhang, Yangjun
Wu, Lilin Xu, Yuan Jessica Wang, Xiongwei Han, Xiaojin
Fu, Tao Zhong, Jia Zeng, Mingli Song, and Gang Chen.
Chain-of-experts: When llms meet complex operations re-
search problems. In The Twelfth International Conference
on Learning Representations, ICLR 2024, Vienna, Austria,
May 7-11, 2024. OpenReview.net, 2024.
[Xiao et al., 2025] Ziyang Xiao, Jingrong Xie, Lilin Xu,
Shisi Guan, Jingyan Zhu, Xiongwei Han, Xiaojin Fu,
WingYin Yu, Han Wu, Wei Shi, Qingcan Kang, Jiahui
Duan, Tao Zhong, Mingxuan Yuan, Jia Zeng, Yuan Wang,
Gang Chen, and Dongxiang Zhang. A survey of optimiza-
tion modeling meets llms: Progress and future directions.
In Proceedings of the Thirty-Fourth International Joint
Conference on Artificial Intelligence, IJCAI 2025, Mon-
treal, Canada, August 16-22, 2025, pages 10742–10750.
ijcai.org, 2025.
[Xing et al., 2024a] Linzi Xing, Xinglu Wang, Yuxi Feng,
Zhenan Fan, Jing Xiong, Zhijiang Guo, Xiaojin Fu,
Rindra Ramamonjison, Mahdi Mostajabdaveh, Xiongwei
Han, Zirui Zhou, and Yong Zhang.
Towards human-
aligned evaluation for linear programming word problems.
In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste,
Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, edi-
tors, Proceedings of the 2024 Joint International Confer-
ence on Computational Linguistics, Language Resources
and Evaluation (LREC-COLING 2024), pages 16550–
16556, Torino, Italia, May 2024. ELRA and ICCL.
[Xing et al., 2024b] Linzi Xing, Xinglu Wang, Yuxi Feng,
Zhenan Fan, Jing Xiong, Zhijiang Guo, Xiaojin Fu,
Rindra Ramamonjison, Mahdi Mostajabdaveh, Xiongwei
Han, Zirui Zhou, and Yong Zhang.
Towards human-
aligned evaluation for linear programming word problems.
In Nicoletta Calzolari, Min-Yen Kan, V´eronique Hoste,
Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, edi-
tors, Proceedings of the 2024 Joint International Confer-
ence on Computational Linguistics, Language Resources
and Evaluation, LREC/COLING 2024, 20-25 May, 2024,
Torino, Italy, pages 16550–16556. ELRA and ICCL, 2024.
[Yang et al., 2025] Zhicheng Yang,
Yiwei Wang,
Yinya
Huang, Zhijiang Guo, Wei Shi, Xiongwei Han, Liang
Feng,
Linqi Song,
Xiaodan Liang,
and Jing Tang.
Optibench meets resocratic: Measure and improve llms
for optimization modeling. In The Thirteenth International
Conference on Learning Representations, ICLR 2025, Sin-
gapore, April 24-28, 2025. OpenReview.net, 2025.
[Yao et al., 2023] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak
Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik
Narasimhan. Tree of thoughts: deliberate problem solving
with large language models. In Proceedings of the 37th
International Conference on Neural Information Process-
ing Systems, NIPS ’23, Red Hook, NY, USA, 2023. Curran
Associates Inc.
[Zhai et al., 2025] Haotian Zhai, Connor Lawless, Ellen
Vitercik, and Liu Leqi. Equivamap: Leveraging LLMs for
automatic equivalence checking of optimization formula-
tions. In 2nd AI for Math Workshop @ ICML 2025, 2025.
