--- Page 1 ---
arXiv:2506.00490v2  [cs.NE]  3 Jun 2025
LLM-Driven Instance-Specific Heuristic Generation and Selectionâ‹†
Shaofeng Zhanga,b, Shengcai Liua,b, Ning Lua,c, Jiahao Wua,d, Ji Liue, Yew-Soon Ongf,g and
Ke Tanga,b
aSouthern University of Science and Technology, Shenzhen, 518055, China
bZhongguancun Academy, Beijing, 100094, China
cThe Hong Kong University of Science and Technology, Hong Kong, 999077, China
dThe Hong Kong Polytechnic University, Hong Kong, 999077, China
eAdvanced Micro Devices Inc., Beijing, China
fCFAR, A*STAR, Singapore
fNanyang Technological University, Singapore
A R T I C L E I N F O
Keywords:
Automatic Algorithm Design
Code Generation
Large Language Models
Combinatorial Optimization
A B S T R A C T
Combinatorial optimization problems are widely encountered in real-world applications. Designing
high-quality heuristic algorithms that efficiently approximate optimal solutions within reasonable
time is a critical research challenge. In recent years, many works have explored integrating Large
Language Models (LLMs) with Evolutionary Algorithms to automate heuristic algorithm design
through prompt engineering. However, these approaches generally adopt a problem-specific paradigm,
applying a single algorithm across all problem instances, failing to account for the heterogeneity across
instances. In this paper, we propose InstSpecHH, a novel framework that introduces the concept
of instance-specific heuristic generation. InstSpecHH partitions the overall problem class into sub-
classes based on instance features and performs differentiated, automated heuristic design for each
problem subclass. By tailoring heuristics to the unique features of different sub-classes, InstSpecHH
achieves better performance at the problem class level while avoiding redundant heuristic generation
for similar instances, thus reducing computational overhead. This approach effectively balances the
trade-off between the cost of automatic heuristic design and the quality of the obtained solutions.
To evaluate the performance of InstSpecHH, we conduct experiments on 4,500 subclasses of the
Online Bin Packing Problem (OBPP) and 365 subclasses of the Capacitated Vehicle Routing Problem
(CVRP). Experimental results show that InstSpecHH demonstrates strong intra-subclass and inter-
subclass generalization capabilities. Compared to previous problem-specific methods, InstSpecHH
reduces the average optimality gap by more than 5.6% for OBPP and 0.9% for CVRP. These results
highlight the potential of instance-aware automatic heuristic design to further enhance solution quality.
1. Introduction
Efficiently finding the solutions of NP-hard combinato-
rial optimization problems has long been a central research
focus for both academia and industry [2, 4, 21]. Unlike
exact algorithms, which require substantial computational
resources as the problem scale increases [18], heuristic
algorithms can generate high-quality solutions within rea-
sonable time, demonstrating considerable practical value in
engineering applications [3, 44, 46]. However, traditional
heuristic algorithm designs exhibit two critical limitations.
Firstly, their development heavily depends on human exper-
tise, leading to prohibitively high design costs [6]. Secondly,
manual design paradigms often inadequately explore the
heuristic algorithm space, making it challenging to discover
more effective heuristic strategy combinations [48].
The emergence of Large Language Models (LLMs) of-
fers a viable approach to significantly reduce the cost of gen-
erating heuristics [42, 30], thereby enabling the exploration
â‹†This document is the results of the research project funded by the
National Key Research and Development Program of China under Grant
2022YFA1004102, the Guangdong Major Project of Basic and Applied
Basic Research under Grant 2023B0303000010, and the Zhongguancun
Academy Project No.20240303.
12445025@mail.sustech.edu.cn (S. Zhang); liusc3@sustech.edu.cn
(S. Liu); asysong@ntu.edu.sg (Y. Ong); tangk3@sustech.edu.cn (K. Tang)
ORCID(s): 0009-0008-7560-5596 (S. Zhang)
of a broader heuristic algorithm space. Recently, several
studies have attempted to leverage LLMs to automatically
generate heuristics, aiming to utilize the strong represen-
tational and generalization capabilities of these models to
assist in generating heuristic algorithms for solving NP-
hard optimization problems [36, 23, 48, 24]. Specifically,
these studies integrate LLMs with Evolutionary Algorithms
(EAs), employing iterative querying and evaluation mech-
anisms to progressively generate the Python code of the
heuristic algorithm tailored to a specific problem class. How-
ever, such approaches often attempt to generate a single al-
gorithm to solve all problem instances belonging to the class,
without adequately accounting for the heterogeneity across
different instances. In this paper, we refer to this paradigm
as LLM-driven problem-specific heuristic generation. As
a result, it often performs poorly over the entire problem
class. EoH [23] is one representative work in this line. In
its original paper, it demonstrates that when provided with 5
test instances of the Online Bin Packing Problem (OBPP),
EoH can generate a heuristic algorithm that outperforms
Best Fit, a traditional heuristic for OBPP. However, when
we consider 94,500 instances with diverse problem features
as test instances, the single heuristic generated by EoH
achieves almost the same solution quality as Best Fit, as
S. Zhang et al.: Preprint submitted to Elsevier
Page 1 of 13


--- Page 2 ---
Leveraging social media news
Settings
Case1
Case2
# Generation
20
16
# Population
20
10
# LLM Queries
2000
800
Initial Quality (%)
96.08
96.08
Best Achieved Quality (%)
97.23
96.96
Time / hour
6.86
2.85
Table 1
Single-run statistics of EoH under different settings: Algo-
rithm quality is evaluated by the ratio between the average
number of bins required by the generated heuristic algorithm
and the problem-specific lower bound, computed over 5 in-
stances. Each algorithm is assessed using 10 parallel samples
and evaluations. The Code generation LLM is DeepSeek-V3.
shown in Table 2. In such cases, it is difficult to obtain further
improvements with a single heuristic algorithm.
To address the aforementioned limitation and achieve
better performance, a straightforward approach is to leverage
existing methods [23, 48] to generate a single heuristic for
each individual instance within the problem class. While
this can improve performance, the cost is prohibitively high.
As illustrated in Table 1, even generating a single heuristic
can require considerable computational effort. For instance,
executing 800 queries with DeepSeek-V3 requires approxi-
mately 3 hours of computation time, making this approach
impractical when faced with a large number of problem
instances. This naturally leads to a fundamental research
question: How can we account for the heterogeneity of
problem instances as much as possible under the limited
computational budget?
In this work, we propose an instance-specific heuristic
generation and selection framework, named InstSpecHH,
to address the above question. This framework operates
under the assumption that problem instances with similar
features can be effectively solved using the same heuristic
algorithm. The core process of InstSpecHH is as follows.
First, the original instance space is partitioned into differ-
ent subclasses characterized by problem features. Instances
within each subclass share common characteristics, allowing
them to be solved using the same heuristic algorithm. Then,
by integrating EAs with LLM, a specific heuristic algorithm
is generated for each problem subclass. Finally, InstSpecHH
leverages the LLM to select the most appropriate heuristic
algorithm from the available problem subclasses to solve
the target instance. This approach enables adaptation to
instance-level variations (i.e., instance-specific) while avoid-
ing the inefficiency of generating a separate heuristic for
every individual instance.
The key contributions of this work are outlined below:
â€¢ Limitation of Problem-Specific Heuristic Gener-
ation: This work identifies the limitation of using
a single heuristic algorithm to solve an entire prob-
lem class, the failure to account for the heterogeneity
among individual instances, which often leads to sub-
optimal solution quality.
â€¢ Instance-Specific Framework: This work proposes
InstSpecHH, the first LLM-driven instance-specific
framework that partitions the problem class into sub-
classes to assist in heuristic algorithm generation and
selection.
â€¢ Comprehensive Evaluation: Extensive experiments
in both intra-subclass and inter-subclass generaliza-
tion settings show that InstSpecHH achieves an op-
timality gap reduction of over 5.6% on OBPP and
0.9% on CVRP, relative to previous problem-specific
methods.
2. Related Work
2.1. Traditional Hyper-Heuristics
Traditional heuristic algorithm design typically entails
substantial manual effort, involving problem-specific mod-
eling, feature analysis, and the development of customized
solution strategies
[3]. In response to these limitations,
Hyper-Heuristics have garnered increasing attention. Hyper-
Heuristics operate by searching the space of heuristics rather
than the problemâ€™s solution space, with the objective of
solving optimization problems through the selection or com-
bination of existing heuristics [11, 6, 5, 50].
Existing hyper-heuristic approaches can be broadly cate-
gorized into selection-based and generation-based methods.
Selection-based hyper-heuristics rely on a predefined set
of low-level heuristics and employ either online or offline
learning mechanisms to dynamically select the most suit-
able heuristic based on the current problem state [39]. In
recent years, there has been increasing interest in leverag-
ing machine learning techniques to enhance the decision-
making process for heuristic selection [17, 33]. In con-
trast, generation-based hyper-heuristics operate directly on
the structure or parameter space of algorithms. Techniques
such as genetic programming and evolution strategies are
commonly used to evolve new heuristic rules or algorithmic
structures [19, 13]. These approaches enable the exploration
of more complex and diverse solution strategies.
Despite their advantages in terms of transferability and
problem independence, the effectiveness of hyper-heuristics
largely depends on the quality of the heuristic search space.
Most existing approaches require domain expertise to man-
ually define the set of low-level heuristics or the structure of
the search space, leading to high development costs and lim-
ited expressiveness. Such reliance on human-designed priors
may constrain the scalability and performance of hyper-
heuristics, especially when applied to high-dimensional or
complex problem classes.
2.2. Neural Combinatorial Optimization
Traditional heuristic and exact algorithms often demand
significant manual effort for tasks like problem modeling
and rule design. To address this, Neural Combinatorial Opti-
mization Solvers have emerged as a data-driven approach to
S. Zhang et al.: Preprint submitted to Elsevier
Page 2 of 13


--- Page 3 ---
Leveraging social media news
Problem Instance Representation
Task 
Description
Problem 
Domain
Feature
Demision
Problem 
Subclass
Heuristic Algorithm Selection
Heuristic Algorithm Design
Instance
ï¿½ï¿½ï¿½ï¿½
Prompt
Code LLM
(SOTA)
Heuristic
ï¿½
Evaluation
Target
Instance
Distance 
Metric
Problem 
Subclass
Candidate 
Algorithms
Chat LLM
Target Heuristic
ï¿½âˆ—
Chat LLM
Feature
Value
Check
Score
Heuristic 
Population 
Prompt
Describe
Figure 1: InstSpecHH Diagram. First, construct problem subclasses and their representations (top). Next, generate instance-
specific heuristic algorithms (middle). Finally, select the best-matching heuristic algorithm for the target instance (bottom).
solving NP-hard problems [2, 32, 26]. From the perspective
of heuristic algorithm design, the network architecture de-
fines the heuristic algorithm space, and the training process
then explores this space to optimize the network param-
eters. Neural solvers can be categorized into autoregres-
sive and non-autoregressive construction-heuristic solvers.
The former generate solutions sequentially, adding one el-
ement at a time conditioned on the current partial solu-
tion. Representative work includes Pointer Networks [41],
which can emit variable-length outputs; methods that em-
ploy reinforcement learning to avoid the high cost of ac-
quiring high-quality labels [1]; and attention-based models
built on the Transformer architecture [18]. In contrast, non-
autoregressive solvers output an entire solution in a single
pass. Recent studies have exploited diffusion models to learn
a probability distribution over the solution space, enabling
the direct sampling of high-quality solutions and thereby
eliminating the sequential dependencies inherent in autore-
gressive approaches [40, 22, 37].
2.3. Heuristic Generation with LLMs
Recent advances in LLMs have achieved remarkable
success across a wide range of tasks [34, 25, 45, 27]. Among
them, code generation has emerged as a critical area of
research [49, 12, 16]. Thanks to their exceptional ability to
understand and follow instructions [8], LLMs can effectively
assist programmers in generating code for the specific task,
significantly reducing the cost of manual coding. Represen-
tative models in this line of work include PaLM-Coder [7]
and Code LLaMA [15]. However, directly generating syn-
tactically and semantically correct code that satisfies the task
description is only the first step in code generation [12]. A
more challenging and ongoing problem lies in generating
high-quality code tailored to the target task.
Compared to generating the source code of the heuristic
algorithm only once, leveraging prompt engineering to feed
back model outputs into the LLM for iterative refinement
can yield significantly higher-quality results [35, 14, 31].
A mainstream approach combines EAs [20, 47, 38] with
LLMs: a population of high-quality heuristic algorithms is
maintained, and new candidates are generated by prompting
the LLM with evolutionary operations such as crossover
and mutation. Representative work in this direction includes
FunSearch [36], where the generated programs are scored
by an automatic evaluator, and the top-performing ones are
retained and stored in a database. These programs are then
used for the next round of best-shot prompting, enabling
the LLM to generate improved variants. EoH [23] extends
FunSearch by incorporating explicit algorithmic descrip-
tions into prompts, and combining these with crossover
and mutation operators from evolutionary computation. This
hybridization enhances the generation of novel and efficient
heuristic algorithms. ReEvo [48], on the other hand, in-
tegrates genetic programming techniques with LLMs and
introduces long/short feedback mechanisms. By incorporat-
ing compilation feedback from generated programs, ReEvo
provides additional signal to guide and refine future code
generation, further improving the quality and correctness of
the synthesized algorithms.
3. InstSpecHH Framework
In contrast to problem-specific paradigms that rely on a
single heuristic to solve all the instances belonging to the
problem class, this work takes into account the variability
among individual instances and proposes an automatic algo-
rithm design and selection framework named InstSpecHH
S. Zhang et al.: Preprint submitted to Elsevier
Page 3 of 13


--- Page 4 ---
Leveraging social media news
(Instance-Specific Hyper-Heuristic), as illustrated in Fig-
ure 1. The framework consists of the following three key
components:
â€¢ Problem Instance Representation: The entire prob-
lem class is partitioned into multiple subclasses. Each
subclass is described using natural language to cap-
ture its key characteristics, providing a foundation for
instance-specific algorithm design and selection.
â€¢ Heuristic Algorithm Design: For each identified
problem subclass, InstSpecHH automatically gener-
ates instance-specific heuristic algorithms tailored to
the corresponding subclass.
â€¢ Heuristic Algorithm Selection: Based on the features
of a given problem instance, InstSpecHH matches it
with the most relevant problem subclass and selects
the most suitable existing algorithm accordingly.
3.1. Problem Instance Representation
A problem instance refers to a specific input (data) for
a given problem, containing all the necessary information to
solve it. Since problem instances often share similar features,
repeatedly designing heuristic algorithms would result in
significant redundancy and unnecessary costs. Thus, it is
essential to identify these problem instances into groups
to reduce unnecessary overhead in the design of heuristic
algorithms and provide practical information for the selec-
tion of heuristic algorithms. Each group of instances with
same features is referred to as a problem subclass, and for
each subclass, a corresponding representation is constructed,
including feature-based and natural language-based repre-
sentations.
To group problem instances, we begin by generating
key features through iterative collaboration between domain
experts and LLMs. These features are designed to capture
factors that may influence the performance of heuristic al-
gorithms. Each problem feature consists of multiple discrete
feature values, allowing problem instances to be distinctly
identified and grouped accordingly. Let ğ‘–represent a specific
problem instance characterized by ğ‘‘distinct features. The
value of the ğ‘š-th feature for instance ğ‘–is denoted as ğ‘“ğ‘š(ğ‘–).
When feature values are categorical rather than numeric,
they are encoded as integer IDs. Consequently, the feature
information of problem instance ğ‘–can be represented by a
feature vector (i.e., feature-based representations):
ğŸ(ğ‘–) = (ğ‘“1(ğ‘–), ğ‘“2(ğ‘–), â€¦ , ğ‘“ğ‘‘(ğ‘–)) âˆˆâ„ğ‘‘.
(1)
When different instances share an identical feature vector
ğŸ(ğ‘–), they are categorized into the same subclass. Further-
more, through interaction with LLMs, we generate natural
language descriptions corresponding to each feature value
ğ‘“ğ‘š(ğ‘–) to aid in the characterization of problem subclasses.
These descriptions serve as language-based representations
of the problem instance, as illustrated in Figure 3. Addi-
tionally, Section 3.4 demonstrates two example optimization
problems to illustrate the proposed problem representation.
After establishing the feature values for each feature, we
can partition the entire problem class accordingly. Suppose
the ğ‘š-th feature has |ğ‘“ğ‘š| distinct feature values; conse-
quently, a total of |ğ‘“1| Ã— |ğ‘“2| Ã— â‹¯Ã— |ğ‘“ğ‘‘| problem subclasses
can be constructed. To ensure a uniform distribution of
samples within each subclass, we design instance generators
tailored specifically to each feature value. Test instances
for each problem subclass are generated using dedicated
instance generators, enabling precise evaluation of heuristic
algorithms within the subclass.
3.2. Heuristic Algorithm Design
Algorithm 1 Heuristic Algorithm Design
Input: Problem pool îˆµ, evolutionary operator pool îˆ», eval-
uate function ğ‘“(â‹…), population size ğ‘, maximum gener-
ation ğºmax, problem distance function ğ‘‘(â‹…), LM ğ¿ğ‘€ğœƒ,
neighbor search number ğ‘˜ğ‘›
Output: Heuristic algorithm pool îˆ´
1: // Generate the heuristic algorithm for each feature-
homogeneous instance
2: for all problem subclass ğ‘ âˆˆproblem pool îˆµdo
3:
ğ‘¡ğ‘ â†ğ¿ğ‘€ğœƒ(ğ‘ , Desc)
âŠ³instance-specific prompt
4:
ğ‘ƒ0 â†{ â„1, â„2, â€¦ , â„ğ‘}
âŠ³Initialize population
5:
Evaluate fitness ğ‘“(â„, ğ‘ ) for every individual heuris-
tic in ğ‘ƒ0
6:
for ğ‘”â†1 to ğºmax do
7:
// Repeat Line 8-12 with different operator ğ‘œ
8:
Get evolutionary prompt ğ‘¡ğ‘’with operator ğ‘œâˆˆîˆ»
9:
Choose parents ğ‘€ğ‘”from ğ‘ƒğ‘”âˆ’1
10:
Generate offspring ğ¶ğ‘”â†ğ¿ğ‘€ğœƒ(ğ‘¡ğ‘, ğ‘¡ğ‘’, ğ‘€ğ‘”)
11:
Evaluate the fitness of offspring ğ¶ğ‘”
12:
Update ğ‘ƒğ‘”âˆ’1 with ğ¶ğ‘”, yielding the ğ‘ƒğ‘”
13:
// Select the algorithm with the highest score
14:
îˆ´[ğ‘ ] â†arg
max
ğ‘ âˆˆâ‹ƒğºmax
ğ‘”=0
ğ‘ƒğ‘”
ğ‘“(â„, ğ‘ )
15: // Neighbor search
16: for all ğ‘ âˆˆîˆµdo
17:
Compute distance ğ‘‘(ğ‘ , ğ‘ â€²) to ğ‘ for every ğ‘ â€² âˆˆîˆµ
18:
ğ·ğ‘˜
ğ‘ â†the ğ‘˜ğ‘›closest subsets to ğ‘ 
19:
îˆ´[ğ‘ ] â†arg max
ğ‘ â€²âˆˆğ·ğ‘˜ğ‘ 
ğ‘“(ğ»[ğ‘ â€²], ğ‘ )
20: return îˆ´
Unlike previous work [36, 23, 48], which generates a
single heuristic algorithm for the entire problem class, this
section aims to develop differentiated heuristic solutions
tailored to each problem subclass. Building on the subclass
definitions introduced in Section 3.1, and inspired by the
approach in [23], we generate instance-specific heuristic
algorithms by combining LLMs with EAs, as shown in
Algorithm 1.
For each problem subclass, the population ğ‘ƒis initially
established (Line 4), where each individual â„corresponds
to a candidate heuristic algorithm. The performance of each
S. Zhang et al.: Preprint submitted to Elsevier
Page 4 of 13


--- Page 5 ---
Leveraging social media news
Now I have <num of candidates> algorithm(s), please select the most appropriate algorithm for the target <Problem Class> with specific features. 
Consider the problem characteristics that may impact algorithm performance, and choose the algorithm you believe will achieve the best results.
Target Instance-Specific <Problem Class>: 
<problem subclass description>
Candidate <Problem Class> Algorithms:
<candidate algorithm description> # repeat for different candidate algorithm(s)
Objective: Choose an algorithm that minimize the number of used bins with the target instance.
The output format is a JSON structure as follows:
{
        reason: Explanation for your choice
        result:  The algorithm id you chose. Just give me a number
}
Where 'reason' justifies your algorithm selection, and 'result' is the chosen algorithm id.
Figure 2: Heuristic Algorithm Selection Prompt Template.
candidate heuristic is then evaluated using test instances rep-
resentative of the respective problem subclass (Line 5). Var-
ious genetic operators, such as crossover and mutation, are
applied in sequence during each evolutionary iteration. For
each operator, the corresponding prompt is generated. Next,
parent heuristic algorithms ğ‘€ğ‘”are chosen via roulette wheel
selection, and the LLM is used to produce the operation-
specific offspring ğ¶. The newly generated algorithm ğ¶ğ‘”is
subsequently evaluated using test instances from the targeted
subclass. Its performance quality is quantified as a fitness
score, which guides the updating of the heuristic population
ğ‘ƒğ‘”âˆ’1. Candidates demonstrating superior fitness scores are
retained, while lower-performing heuristics are systemati-
cally phased out from the population (Line 12). This iterative
evolutionary refinement optimizes the heuristic algorithm
population, leading to progressively more robust and cus-
tomized heuristic solvers tailored to each problem subclass.
In our experiments, we observed a tendency for the
population-based evolutionary process to become constrained
by local optima. To mitigate this limitation and further
advance the quality of automatically designed algorithms,
we introduced a neighbor search (NS) strategy. This strat-
egy assumes that closely related problem subclasses exhibit
similar instance distributions and thus possess potentially
transferable solution strategies. Specifically, for each target
problem subclass, we identify and retrieve the ğ‘˜ğ‘›closest
subclasses and incorporate their existing heuristic algo-
rithms as additional candidate solutions (Line 18). These
candidate algorithms undergo evaluation with the target
subclass, with the top-performing heuristic selected as the
final solver for the target subclass (Line 19). Incorporating
neighbor search effectively broadens the search space and
substantially enhances the overall solution quality produced
by the automated design framework.
By designing instance-specific heuristic algorithms for
various problem subclasses, a diverse heuristic algorithm
library can be constructed, which provides richer candi-
date solutions for different problem instances and enhances
the robustness and generalization capability of optimization
problem solving.
3.3. Heuristic Algorithm Selection
With the impressive reasoning and expressive capabil-
ities demonstrated by LLMs in various natural language
processing tasks [43, 29, 28], we hope to leverage LLMs to
assist in selecting the most suitable heuristic algorithms for
specific problem instances. However, due to the vast number
of problem subclasses, it is impractical for LLMs to select
from all available heuristic algorithms directly. Therefore,
introducing a pre-selection mechanism to narrow down the
candidate algorithms before invoking the LLM becomes
essential, effectively controlling the input size and enhancing
the quality of algorithm selection.
For a target problem instance ğ‘–, we first construct a
feature vector ğŸ(ğ‘–) along the key features defined in Section
3.1. These features effectively distinguish among problem
subclasses and capture the core attributes that influence the
performance of the heuristic algorithm. Next, we use ğŸ(ğ‘–) to
perform a similarity search over the existing set of problem
subclasses. Because the features in ğŸ(ğ‘–) have different scales,
we normalize each feature value to obtain the standardized
vector Ì‚ğŸ(ğ‘–). We then compute the distance between instance
ğ‘–and each stored subclass in InstSpecHH with the Euclidean
metric:
ğ‘‘(ğ‘–, ğ‘–â€²) = â€– Ì‚ğŸ(ğ‘–) âˆ’Ì‚ğŸ(ğ‘–â€²)â€–2,
(2)
where â€–â‹…â€–2 denotes the L2-norm. The ğ‘˜ğ‘ subclasses with
the smallest Euclidean distance to the target instance are
selected, and the associated heuristics are aggregated to form
the candidate algorithm set. This pre-selection step preserves
only the most relevant heuristics while sharply reducing
the search space, thereby enhancing both the efficiency and
accuracy of the subsequent selection process.
To fully leverage the language-understanding and rea-
soning capabilities of LLM, we first translate the feature
S. Zhang et al.: Preprint submitted to Elsevier
Page 5 of 13


--- Page 6 ---
Leveraging social media news
The following is the description of the online bin packing problem: 
â€¢
Items Num = 500: With small number of items, a greedy approach quickly allocates crates, 
allowing complex strategies to minimize bin usage.
â€¢
Capacity = 300: Generous capacity supports more robust greedy allocations. Fewer 
rejections occur, but vigilance against oversized arrivals remains.
â€¢
Uniform Distribution: Evenly spread weights yield stable greedy scheduling. Minimal 
skewness reduces overhead, but tie-breaking remains crucial for similar-size arrivals
â€¢
Random Sequence: Items arrive in an unpredictable order, making greedy decisions highly 
dependent on early selections.
â€¢
Capacity Ratio = 0.4:  Larger item sizes relative to capacity raise partial-bin overhead. The 
greedy scheduler must track leftover fragments to maintain throughput and avoid performance 
bottlenecks.
The following is the description of the capacitated vehicle routing problem: 
â€¢
Num. of Locations = 1000: Large-scale greedy routing must address compound 
inefficiencies.
â€¢
Capacity = 125: Higher capacity favors long, consolidated routes.
â€¢
Capacity Ratio = 0.3:  Low average demand relative to vehicle capacity enables vehicles to 
serve many customers per route.
â€¢
Location Distribution = Normal Distribution: Customer locations follow a normal 
distribution and are concentrated near the center.
â€¢
Demand Distribution = Weibull: Heavy-tailed demand introduces rare but extreme 
customer loads.
Figure 3: Example of natural language description for OBPP (left) and CVRP (right) problem features.
vector of the target problem instance into a concise natural-
language description. This description, combined with tex-
tual profiles of the candidate algorithms, is supplied to the
LLM as contextual input, as shown in Figure 2. Guided by
this prompt, the LLM could assess the fit between the target
instance and each candidate algorithm and then identify the
heuristic that is most likely to deliver near-optimal perfor-
mance for the given instance. By integrating feature-vector
similarity retrieval with the semantic reasoning capabilities
of LLM, the proposed approach enables scalable and gen-
eralized heuristic algorithm selection across complex and
extensive problem classes.
3.4. Instantiation for OBPP and CVRP
To illustrate our approach, this section focuses on two
representative optimization problems: the Online Bin Pack-
ing Problem (OBPP) and the Capacitated Vehicle Rout-
ing Problem (CVRP). Each problem class is partitioned
into subclasses, and natural language representations are
constructed for each subclass to support instance-specific
algorithm selection.
In the OBPP, a sequence of items with varying sizes
arrives one by one, and each item must be irrevocably
assigned to a bin upon arrival. All bins have a fixed capacity,
and the objective is to minimize the total number of bins
used. The sum of item sizes in any bin must not exceed
its capacity. To characterize the OBPP domain, we define
five key features. By enumerating combinations of these
features, we could systematically span the entire problem
space. These features include:
â€¢ Number of Items: Specifies the instance scale, rang-
ing from 500 to 5000 items.
â€¢ Item Weight Distribution: Captures the statistical
characteristics of item weights, modeled using Uni-
form, Gaussian, or Weibull distributions.
â€¢ Sequence Type: Describes the arrival pattern of item
weights in the online setting. Items may arrive in a
random sequence or be arranged in non-decreasing or
non-increasing order by weight.
â€¢ Capacity: Determines the size constraint for each bin,
with values ranging from 50 to 500.
â€¢ Capacity Ratio: Represents the overall packing tight-
ness of the items, defined as the average item weight
divided by the bin capacity, with values ranging from
0.3 to 0.7.
We further consider the CVRP, a fundamental combi-
natorial optimization problem with extensive applications
in logistics and transportation systems. The objective of
CVRP is to determine a set of routes for a fleet of vehicles
with uniform capacity, such that a set of geographically
distributed customers, each with a specific demand, is served
exactly once, and the total travel distance is minimized. Each
route must start and end at a central depot, and the total
demand on any route must not exceed the capacity limit
of the assigned vehicle. Similarly, we construct five key
features for the CVRP domain to systematically partition the
entire problem space.
â€¢ Number of Customers: Specifies the instance scale,
with values ranging from 200 to 1000.
â€¢ Location Distribution: Models the spatial distribu-
tion of customer locations, using Uniform, Gaussian,
or Grid-based layouts.
â€¢ Demand Distribution: Captures the statistical vari-
ability in customer demands, represented by Uniform,
Gaussian, or Weibull distributions.
â€¢ Vehicle Capacity: Determines the load limit for each
vehicle, selected from the range [50, 75, 100, 125, 150].
â€¢ Capacity Ratio: Reflects the overall instance tight-
ness, defined as the ratio of average customer demand
to vehicle capacity, with values in [0.3, 0.5, 0.7].
Each problem instance is characterized by a feature
vector, where each dimension corresponds to a specific prob-
lem attribute. By enumerating combinations of predefined
feature values, we can systematically span the entire problem
space. Naturally, some instances may fall outside the pre-
defined feature configuration; in such cases, the algorithm
S. Zhang et al.: Preprint submitted to Elsevier
Page 6 of 13


--- Page 7 ---
Leveraging social media news
Problem Class
Method
Obj. (â†“)
Opt. gap % (â†“)
OBPP
Best Fit
1383.78
5.83
First Fit
1386.58
6.05
EOH
1383.68
5.82
ReEvo
1383.78
5.83
InstSpecHH w/o NS
1334.12
2.17
InstSpecHH w/ NS
1304.47
0.00
CVRP
Closest Priority
450.22
13.74
EoH
405.12
0.93
ReEvo
407.63
1.89
InstSpecHH w/o NS
413.85
3.41
InstSpecHH w/ NS
400.95
0.00
Table 2
Intra-Subclass Generalization: The problem features of the test instances are aligned with those of the training instances used to
construct the heuristic algorithm. In OBPP, the objective value is the number of bins used, whereas in CVRP, the objective value
refers to the total vehicle travel distance. The optimality gap (Opt. gap) is calculated with respect to the current best-known
solution.
selection strategy introduced in Section 3.3 can be applied
to handle out-of-distribution instances.
4. Experiments
As introduced in Section 3.4, this work focuses on two
optimization problems: the OBPP and the CVRP. By enu-
merating combinations of key features, we construct 4,500
problem subclasses for the OBPP and 675 for the CVRP.
These subclasses are then split into training and testing sets
with a ratio of 7:3. The training problem subclasses are
used for algorithm evaluation during the heuristic generation
process. Two types of generalization are considered: intra-
subclass generalization, where the test instances share the
same problem features as the training subclasses, and inter-
subclass generalization, where the test instances differ in
problem features from the training subclasses. Each prob-
lem subclass is associated with 30 test instances, and each
training subclass additionally includes 30 training instances.
To demonstrate the effectiveness of our instance-specific
approach in improving heuristic performance across the
problem class, we compare it against two representative
baselines: EoH[23] and ReEvo[48]. Both methods leverage
LLMs in conjunction with EAs to automatically generate
heuristic algorithms. However, unlike our method, they do
not account for instance-specific differences. As a result,
their frameworks generate a single, general-purpose heuris-
tic for the entire problem class. We also compared against
manually designed heuristic algorithms. For OBPP, we use
Best Fit and First Fit as the handcrafted heuristics, while for
CVRP, we choose Closest Priority.
All algorithms are configured with a population size of
10, and the number of LLM queries is capped at 800. For
EoH and ReEvo, heuristic generation is conducted using
DeepSeek-V3[10]. In contrast, our method, InstSpecHH, in-
volves generating a large number of instance-specific heuris-
tics. To manage computational cost, we employ a more ef-
ficient model, DeepSeek-R1-Distill-Qwen-14B[9], for both
heuristic generation and selection. We repeat the experiment
5 times during algorithm selection.
We use the objective value (Obj.) and the optimality gap
(Opt. gap) as the primary evaluation metrics. Lower values
for both indicate higher solution quality achieved by the
algorithm. In OBPP, the objective value is the number of
bins used, and in CVRP, the objective value refers to the total
vehicle travel distance. The optimality gap is computed with
respect to the best-known solution:
Optimality Gap =
Objalg âˆ’Objbest
|Objbest|
Ã— 100%
(3)
where Objalg is the objective value obtained by the algo-
rithm, and Objbest represents the best known objective value.
4.1. Performance Comparison
InstSpecHH demonstrates strong intra-subclass general-
ization capabilities, where the problem features of the test
instances are aligned with those of the training instances
used to construct the heuristic algorithm. To assess the
intra-subclass generalization capability of the proposed Inst-
SpecHH framework, we constructed 3,150 OBPP subclasses
and 472 CVRP subclasses, each consisting of 30 training
instances and 30 testing instances. Table 2 summarizes the
comparative performance in terms of optimality gap across
the different methods. The results clearly demonstrate that
InstSpecHH achieves superior performance over both EoH
and ReEvo. Specifically, InstSpecHH reduced the average
optimality gap by 5.82% for OBPP and 0.93% for CVRP
when compared to the other two methods. These findings
suggest that compared to using a single heuristic algorithm
for the entire problem class, the instance-specific paradigm
is better suited for solving instances with diverse problem
features. Besides, we find that for OBPP, both EoH and
ReEvo tend to evolve heuristics that follow a logic consistent
with the Best Fit algorithm. This observation show that Best
Fit is a robust heuristic for OBPP with strong generalization
S. Zhang et al.: Preprint submitted to Elsevier
Page 7 of 13


--- Page 8 ---
Leveraging social media news
Problem Class
Method
Obj. (â†“)
Opt. gap % (â†“)
# Retry (â†“)
OBPP
Best Fit
1345.94
6.23
-
First Fit
1343.15
6.01
-
EOH
1343.05
6.01
-
ReEvo
1343.15
6.01
-
InstSpecHH w/o NS
1290.66 Â± 0.67
2.43 Â± 0.07
1.00 Â± 0.068
InstSpecHH w/ NS+Random
1266.26 Â± 0.49
0.51 Â± 0.08
-
InstSpecHH w/ NS+Closest
1265.54
0.44
-
InstSpecHH w/ NS+LLM
1264.78 Â± 0.41
0.34 Â± 0.08
1.02 Â± 0.15
CVRP
Closest Priority
431.57
13.89
-
EOH
388.70
1.01
-
ReEvo
391.11
2.11
-
InstSpecHH w/o NS
398.41 Â± 0.62
4.35 Â± 0.21
1.01 Â± 0.10
InstSpecHH w/ NS+Random
384.40 Â± 0.09
0.07 Â± 0.02
-
InstSpecHH w/ NS+Closest
384.46
0.08
-
InstSpecHH w/ NS+LLM
384.34Â± 0.12
0.06 Â± 0.02
1.01 Â± 0.10
Table 3
Inter-Subclass Generalization: The problem features of the test instances differ from those of the training instances used to
construct the heuristic algorithm. #Retry indicates the average number of LLM queries. The number of candidate heuristics for
algorithm selection is ğ‘˜ğ‘ = 3 for OBPP and ğ‘˜ğ‘ = 2 for CVRP.
capability. As for the CVRP, both EoH and ReEvo evolve
heuristics that outperform Closest Priority. InstSpecHH fur-
ther improves overall performance by accounting for the
heterogeneity across instances. Furthermore, we observe
that the integration of a neighborhood search strategy in
the InstSpecHH module contributes notably to performance
gains. It leads to an additional 2.17% reduction in optimality
gap for OBPP and 3.41% for CVRP, highlighting the im-
portance of escaping local optima during the evolutionary
search process. This result underscores that hybridizing evo-
lutionary strategies with the neighborhood search strategy is
an effective approach for heuristic construction in complex
optimization scenarios.
InstSpecHH maintains high performance even when ap-
plied to inter-subclass subclasses, where the problem fea-
tures of the test instances differ from those of the training in-
stances used to construct the heuristic algorithm. To evaluate
the inter-subclass generalization capability of the proposed
algorithm, we tested the heuristics constructed from intra-
subclass training on 1,350 unseen OBPP subclasses and 203
unseen CVRP subclasses. These experiments compared the
performance of automatically generated heuristics against
handcrafted baselines. Due to occasional invalid outputs
from the LLM, we report the average number of LLM
calls per instance as #Retry, which reflects the stability and
efficiency of the algorithm selection process. As shown in
Table 2, InstSpecHH consistently outperforms the single-
heuristic strategies generated by EoH and ReEvo, achieving
over 5.67% improvement in optimality gap on OBPP and
0.95% improvement on CVRP. The average number of LLM
queries remains around 1.01, indicating that failure rates are
within an acceptable range and do not significantly impact
efficiency.
Leveraging LLMs for heuristic algorithm selection could
enhance the performance of InstSpecHH. For a given tar-
get instance, InstSpecHH first filters the top-k most rel-
evant candidate heuristics (with ğ‘˜ğ‘ = 3 for OBPP and
ğ‘˜ğ‘ = 2 for CVRP), and compares different algorithm se-
lection strategies. InstSpecHH w/ NS+Random denotes ran-
domly selecting from the candidate heuristics; InstSpecHH
w/ NS+Closest represents selecting the heuristic algorithm
corresponding to the problem subclass that is closest to
the target instance, where distance is computed based on
instance feature vectors as defined in 2; InstSpecHH w/
NS+LLM uses LLM to perform the heuristic selection. For
the OBPP task, the LLM-based selection yields approxi-
mately a 0.1% improvement over the Closest strategy. As
for CVRP, since InstSpecHH contains fewer heuristics and
problem subclasses, the performance gain of LLM over
Closest is marginalâ€”around 0.01%. This limited improve-
ment is attributed to the low diversity among candidate
heuristics.
4.2. Sensitivity Analysis
As the number of candidate algorithms increases, Inst-
SpecHH exhibits a performance trend of first improving and
then declining when using the LLM for heuristic selection.
To investigate the impact of the number of candidate algo-
rithms on the performance of LLM-based algorithm selec-
tion, we analyze InstSpecHH w/ NS under varying values
of ğ‘˜ğ‘ , the number of candidate algorithms. By allowing
the LLM to perform algorithm selection across different
candidate sets, we compare the performance outcomes. The
experimental results are shown in Figure 4.
The Lower Bound line represents the objective value
corresponding to the best-performing heuristic algorithm
among the candidates; as the number of candidates in-
creases, the Lower Bound solution quality steadily improves.
S. Zhang et al.: Preprint submitted to Elsevier
Page 8 of 13


--- Page 9 ---
Leveraging social media news
Obj.
# Candidates (k)
OBPP
Lower Bound
Closest
LLM
Obj.
# Candidates (k)
CVRP
Lower Bound
Closest
LLM
Figure 4: InstSpecHH Performance vs Number of Candidates: Inter-subclass generalization performance comparison of different
algorithm selection strategies used by InstSpecHH under varying numbers of candidate heuristics (ğ‘˜ğ‘ ). Lower objective value
(Obj.) indicates a higher quality of the selected heuristic.
Obj.
Retention Ratio (%)
OBPP
Lower Bound
Closest
LLM
Obj.
Retention Ratio (%)
CVRP
Lower Bound
Closest
LLM
Figure 5: InstSpecHH Performance vs Number of Heuristic Algorithm: Inter-subclass generalization performance under different
heuristic retention ratios in InstSpecHH. Lower objective values indicate a higher quality of the selected heuristics.
The Closest line selects the algorithm whose feature vector
is closest to that of the target problem subclass, and thus its
performance remains constant regardless of ğ‘˜ğ‘ . The LLM
line refers to InstSpecHH using an LLM for algorithm selec-
tion. As the number of candidate algorithms increases, the
LLM is initially able to select better-performing heuristics.
However, with a further increase in the number of candidate
heuristics, the relevance of the algorithms to the target
problem subclass gradually decreases. When the number of
candidate heuristics is too large, it lowers the overall quality
of the candidate set and reduces the likelihood that the LLM
will select the optimal heuristic, thereby making it more
difficult to identify high-quality heuristics. Additionally, as
the number of candidate algorithms grows, the variance in
solution quality also increases.
Furthermore, we hope to explore the relationship be-
tween the number of heuristics in InstSpecHH and its inter-
subclass generalization capability on unseen problem sub-
classes. Specifically, we randomly retain 80%, 60%, 40%,
and 20% of the heuristics used in InstSpecHH w/ NS, and
evaluate the resulting performance on test subclasses of two
problem classes: OBPP and CVRP. The performance is mea-
sured in terms of the objective value, where a lower value
indicates better solution quality. As shown in the Figure
5, the generalization performance of InstSpecHH degrades
gradually with the initial reduction in the number of heuris-
tics. However, when the proportion of retained heuristics
drops further, both the LLM-based heuristic selection strat-
egy (LLM line) and the Closest heuristic selection strategy
(Closest line) exhibit a marked decline in performance. This
indicates that while a certain degree of redundancy exists
among the heuristics within InstSpecHH, removing a small
subset does not significantly impair its overall effectiveness.
Moreover, we observe that the performance variance of the
LLM-based strategy increases as the number of available
heuristics decreases. This may be due to significant differ-
ences between the problem subclass features of the candidate
heuristics and the problem features of the target instance.
As a result, the LLM struggles to effectively distinguish
between heuristics based solely on the high-level features of
the target instance, leading to unreliable selection decisions.
It is also noteworthy that the Lower Bound curve increases
at a relatively slow rate, suggesting that despite substantial
differences across problem subclasses, there still exist high-
quality heuristics capable of effectively addressing the target
subclasses. This observation highlights the robustness and
adaptability of certain heuristics, even in scenarios with
limited heuristic availability.
S. Zhang et al.: Preprint submitted to Elsevier
Page 9 of 13


--- Page 10 ---
Leveraging social media news
InstSpecHH w/ NS
InstSpecHH w/o NS
Best Fit
(d) Capacity
(e) Capacity Ratio
(c) Sequence Type
(b) Item Weight Distribution
(a) Num of Items
500
5000
1000
1500
4500
4000
3500
3000
2500
2000
50
500
100
150
450
400
350
300
250
200
0.3
0.7
0.6
0.5
0.4
Inc.
Rand.
Dec.
Unif.
Weib.
Gauss.
Figure 6: OBPP Feature Analysis: Performance Comparison of InstSpecHH and individual heuristics on OBPP across varying
problem features. The evaluation metric is the 1-Opt. gap, where values closer to 1 indicate higher algorithm quality.
4.3. Feature Analysis
InstSpecHH exhibits consistent performance advantages
over baseline algorithms under various problem features.
To further investigate the performance differences of In-
stSpecHH across various problem feature dimensions, we
compare the performance of InstSpecHH with NS, Inst-
SpecHH without NS, and the problem-specific algorithm on
OBPP and CVRP. We use 1 âˆ’Opt. gap as the evaluation
metric and visualize the results using the radar chart. The
experimental results are shown in Figure 6 and 7.
For the OBPP problem, InstSpecHH demonstrates a con-
siderable performance improvement when the sequence type
is Increasing, the capacity ratio is 0.7, and the item weight
distribution is Uniform. Specifically, when the sequence type
is increasing, it achieves a notable 16.61% improvement in
the 1-Opt. gap compared to the Best Fit algorithm. The
largest gain with respect to item characteristics is observed
under a uniform item weight distribution, with a 7.66% im-
provement. Regarding the capacity ratio, InstSpecHH with
NS performs best when the ratio is 0.7, improving by 6.53%,
and least when the ratio is 0.3, improving by 5.20%. In
contrast, without the NS component, the gains drop to 3.24%
at 0.3 and merely 1.10% at 0.7, suggesting that lower ca-
pacity ratios increase the risk of local optima. For the
number of items, InstSpecHH shows steady performance at
1500 items, achieving a 2.17% improvement over its NS-
disabled variant and 5.83% over Best Fit. However, without
NS, the improvement over Best Fit decreases as the number
of items growsâ€”from 4.26% at 1000 items to just 2.19%
at 5000. This indicates a higher likelihood of suboptimal
behavior with larger instances. Finally, with respect to bin
capacity, performance gains remain relatively uniform, with
the largest (6.24%) observed at a capacity of 150 and the
smallest (5.46%) at 400.
In the context of the CVRP, InstSpecHH demonstrates
relatively higher performance gains over EoH when the
capacity ratio is 0.5 and the customer demand distribu-
tion follows a Weibull distribution. To further investigate
the performance differences of InstSpecHH across CVRP
instances with varying characteristics, we conduct exper-
iments on specific problem subclasses and compare the
performance of InstSpecHH with that of EoH. Across key
instance featuresâ€”including the number of customers, cus-
tomer location distribution, customer demand distribution,
and vehicle capacityâ€”InstSpecHH achieves relatively con-
sistent improvements over EoH, with an average perfor-
mance gain of 0.93%. Notably, when the capacity ratio is
fixed at 0.5, the performance improvement reaches 2.1%, and
for instances with a Weibull demand distribution, the gain
is 1.14%. However, it is worth noting that in the absence
of a neighborhood search strategy, the InstSpecHH variant
exhibits approximately 2.5% lower performance compared
to EoH on average. This suggests that, in CVRP scenarios,
InstSpecHH is more prone to getting trapped in local
optima during the generation of heuristics for specific
problem subclasses.
5. Conclusion and Discussion
In this paper, we propose InstSpecHH, a heuristic gen-
eration framework that automatically generates differenti-
ated heuristics for distinct problem subclasses within the
given problem class. By leveraging LLMs, InstSpecHH en-
ables semi-automated construction of problem subclasses,
as well as automated generate and selection of heuristic
algorithms. Through the instance-specific heuristic genera-
tion paradigm, InstSpecHH significantly improves solution
quality. We construct 4,500 OBPP subclasses and 675 CVRP
subclasses and conduct extensive, computationally expen-
sive experiments. The key findings are as follows:
(1) InstSpecHH demonstrates strong intra-subclass gen-
eralization capabilities. For 3,150 OBPP subclasses and 472
CVRP subclasses, InstSpecHH generates heuristics tailored
to each subclass. On test instances from these subclasses,
InstSpecHH reduces the average optimality gap by more
than 5.82% for OBPP and 0.93% for CVRP, compared to
state-of-the-art methods such as EoH and ReEvo.
(2) InstSpecHH exhibits robust inter-subclass general-
ization. When evaluated on 1,350 previously unseen OBPP
subclasses and 203 CVRP subclasses, InstSpecHH consis-
tently outperforms single-heuristic strategies generated by
EoH and ReEvo, achieving over 5.67% improvement in
optimality gap for OBPP and 0.95% for CVRP.
S. Zhang et al.: Preprint submitted to Elsevier
Page 10 of 13


--- Page 11 ---
Leveraging social media news
(d) Capacity
(e) Capacity Ratio
(c) Demand Distribution
(b) Location Distribution
(a) Number of Customers
200
5000
400
1000
800
600
50
75
150
400
125
100
0.3
0.7
0.5
Unif.
Weib.
Gauss.
Gauss.
Unif.
Grid
InstSpecHH w/ NS
InstSpecHH w/o NS
EoH
Figure 7: CVRP Feature Analysis: Performance Comparison of InstSpecHH and individual heuristics on CVRP across varying
problem features. The evaluation metric is the 1-Opt. gap, where values closer to 1 indicate higher algorithm quality.
(3) Leveraging LLMs for heuristic selection further en-
hances InstSpecHH performance. Compared to simply se-
lecting the heuristic of the most similar subclass based
on feature values, using LLMs to select heuristics from a
carefully limited candidate pool leads to an additional 0.1%
improvement for OBPP and 0.02% for CVRP.
Although InstSpecHH has achieved promising perfor-
mance, there is still room for improvement in several aspects.
One limitation is its relatively high offline construction cost,
as generating high-quality heuristic programs for each prob-
lem subclass typically requires multiple iterations of LLM
queries. A promising research direction is to leverage ex-
isting heuristic constructions for known subclasses to assist
in the efficient generation of heuristics for new subclasses,
thereby reducing the overall LLM query overhead. In ad-
dition, redundancy exists among the heuristics constructed
by InstSpecHH. Effectively reducing such redundancy while
promoting code diversity can help minimize storage over-
head. Another challenge arises when the number of available
heuristics in InstSpecHH is limited. Although some candi-
date heuristics may still perform well on specific instances,
the LLM often struggles to select the most appropriate one.
In such cases, guiding the LLM to make more informed
selections can significantly reduce evaluation expenses and
improve selection efficiency.
Overall, InstSpecHH showcases the effectiveness of the
instance-specific heuristic generation and presents a new
paradigm for automated heuristic design. It also presents a
promising direction for future research.
A. Candidate Algorithm Description
Template
Figure 8 illustrates the prompt template used to describe
candidate algorithms. This template is designed to help the
LLM assess the relationship between each candidate algo-
rithm and the target instance, thereby facilitating heuristic
algorithm selection.
B. Heuristic Algorithm Generation Prompt
Figure 9 presents the prompt used for heuristic function
generation, while Figures 10 and 11 show example Python
Algorithm ID <algorithm idx>:
â€¢
Candidate Instance-Specific <Problem Class>  Description:
          <problem subclass description>
â€¢
High Discrepancy (critical mismatch): 
         <mismatch features>
â€¢
Low Discrepancy (compatible):
         <match features>
â€¢
Code:
         <algorithm python code>
â€¢
Explain:
         <algorithm description>
Figure 8: Candidate Algorithm Description Template.
Problem Description:
<problem subclass description>
Task Description: 
Implement a python function that returns the priority with 
which we want to add an item to each bin. In each step, the 
item will be assigned to the bin with the maximum priority 
score. If the rest capacity of a bin equals the maximum 
capacity, it will not be used. The final goal is to minimize the 
number of used bins.
1. First, describe your new algorithm and main steps
2. Next, implement the following Python function
<Python function template>
Figure 9: Heuristic Algorithm Design Prompt Template for
OBPP.
implementations of heuristic functions for OBPP and CVRP,
respectively.
References
[1] Bello, I., Pham, H., Le, Q.V., Norouzi, M., Bengio, S., 2016. Neural
Combinatorial Optimization with Reinforcement Learning. arXiv e-
prints arXiv:1611.09940.
[2] Bengio, Y., Lodi, A., Prouvost, A., 2021. Machine learning for com-
binatorial optimization: A methodological tour dâ€™horizon. European
Journal of Operational Research 290, 405â€“421.
S. Zhang et al.: Preprint submitted to Elsevier
Page 11 of 13


--- Page 12 ---
Leveraging social media news
def priority(item: float, remaining_capacities: np.ndarray, init_capacity: int, item_id: int, num_items: int) -> np.ndarray:
    """Returns priority with which we want to add item to each bin.
    Args:
        item: Size of item to be added to the bin.
        remaining_capacities: Array of remaining capacities for each bin.
        init_capacity: Init capacity of each bin.
        item_id: Greedy step of the current item.
        num_items: Total number of items. Also the total greedy steps.
    Return:
        Array of same size as bins with priority score of each bin.
    """
    remaining_ratio = remaining_capacities / init_capacity
    fits = remaining_capacities >= item
    space_ratio = np.zeros_like(remaining_capacities)
    space_ratio[fits] = (init_capacity - item) / remaining_capacities[fits]
    
    fit_factor = 15  # Scaling coefficient for fit_factor with higher priority
    priority_scores = remaining_ratio * (1 - (item / (init_capacity + 1e-9))) * (space_ratio ** fit_factor)
    return priority_scores
def priority(item: float, remaining_capacities: np.ndarray, init_capacity: int, item_id: int, num_items: int) -> np.ndarray:
    remaining_capacities = remaining_capacities.copy().astype(float)
    # Calculate the remaining capacity ratio, which represents the fraction of bin capacity still available
    cap_ratio = remaining_capacities / init_capacity
    # Calculate the occupancy of each bin
    occupancy = (init_capacity - remaining_capacities) / init_capacity
    # Penalize bins that are already heavily occupied
    occupancy_penalty = occupancy ** 2
    # Calculate the priority as a weighted sum of the remaining capacity ratio and a spreading term,
    # while penalizing heavily occupied bins
    priorities = (cap_ratio * (1 - 0.005 * item_id)) - occupancy_penalty * 2
    # Ensure that if the remaining capacity is less than the item size, the priority is very low
    priorities[remaining_capacities < item] = -1
    return priorities
Figure 10: Python Function Example for OBPP Heuristic Algorithm.
def priority(current_node: int, depot: int, unvisited_nodes: np.ndarray, rest_capacity: float, demands: np.ndarray, 
distance_matrix: np.ndarray) -> np.ndarray:
    """Compute a priority score for every unvisited node. Pick the next node with the highest score.
    Args:
        current_node: ID of the current node.
        depot: ID of the depot.
        unvisited_nodes: Array of IDs of unvisited nodes.
        rest_capacity: rest capacity of vehicle
        demands: demands of nodes
        distance_matrix: Distance matrix of nodes.
    Return:
        Array of same size as unvisited_nodes with priority score of each bin. The higher the score, the higher the 
probability that the node will be selected.
    """
    priorities = []
    for j in unvisited_nodes:
        if demands[j] > rest_capacity:
            priorities.append(0.0)
            continue
        demand_factor = (demands[j] / rest_capacity) * 0.3  # Lower weight on demand
        if current_node == depot:
            saving = distance_matrix[depot, j]
        else:
            saving = distance_matrix[current_node, depot] + distance_matrix[depot, j] - distance_matrix[current_node, j]
        
        remaining_capacity = (rest_capacity - demands[j]) / rest_capacity if rest_capacity > 0 else 0.0
        adjusted_saving = saving * (1 - remaining_capacity)
        total_priority = (demand_factor) + (adjusted_saving * 0.7)  # Higher weight on saving
        priorities.append(total_priority)
    return np.array(priorities)
def priority(current_node: int, depot: int, unvisited_nodes: np.ndarray, rest_capacity: float, demands: np.ndarray, 
distance_matrix: np.ndarray) -> np.ndarray:
    n = len(unvisited_nodes)
    priorities = np.zeros(n, dtype=np.float64)
    
    for i, node in enumerate(unvisited_nodes):
        distance_depot = distance_matrix[node, depot]
        if current_node == depot:
            distance_current = 0.0
        else:
            distance_current = distance_matrix[current_node, node]
        saving = distance_depot - distance_current
       # Normalize saving to ensure high values have higher priority
        max_saving = np.max([distance_matrix[n, depot] for n in unvisited_nodes])
        saving_score = (saving / max_saving) if max_saving != 0 else 0.0
        
        max_distance = np.max(distance_matrix[unvisited_nodes, depot])
        distance_score = 1.0 - (distance_matrix[node, depot] / max_distance) if max_distance != 0 else 1.0
        # Adjust weight based on remaining capacity
        capacity_usage = 1.0 - (rest_capacity / (np.max(demands) if np.any(demands) else 1))
        weight_saving = 0.5 + 0.5 * (1.0 - capacity_usage)
        weight_proximity = 0.5 + 0.5 * capacity_usage
        # Combine saving and proximity scores
        combined_score = (saving_score * weight_saving) + (distance_score * weight_proximity)
        # Apply heavy penalty if node demand exceeds remaining capacity
        if demands[node] > rest_capacity:
            combined_score *= 0.1  # Lower priority for exceeding nodes
        priorities[i] = combined_score
    
    return priorities
Figure 11: Python Function Example for CVRP Heuristic Algorithm.
[3] Blackstone, J.H., Phillips, D.T., and, G.L.H., 1982. A state-of-the-art
survey of dispatching rules for manufacturing job shop operations.
International Journal of Production Research 20, 27â€“45.
[4] Blum, C., Roli, A., 2003.
Metaheuristics in combinatorial opti-
mization: Overview and conceptual comparison. ACM Computing
Surverys 35, 268â€“308.
[5] Branke, J., Nguyen, S., Pickardt, C.W., Zhang, M., 2016. Automated
design of production scheduling heuristics: A review. IEEE Transac-
tions on Evolutionary Computation 20, 110â€“124.
[6] Burke, E.K., Gendreau, M., Hyde, M., Kendall, G., Ochoa, G., Ã–zcan,
E., Qu, R., 2013. Hyper-heuristics: a survey of the state of the art.
Journal of the Operational Research Society 64, 1695â€“1724.
[7] Chowdhery, A., Narang, S., Devlin, J., et al., 2023. Palm: scaling
language modeling with pathways.
Journal of Machine Learning
Research 24.
[8] Chung, H.W., Hou, L., Longpre, S., et al., 2024. Scaling instruction-
finetuned language models. Journal of Machine Learning Research
25.
[9] DeepSeek-AI, Guo, D., Yang, D., Zhang, H., et al., 2025. DeepSeek-
R1: Incentivizing Reasoning Capability in LLMs via Reinforcement
Learning. arXiv e-prints arXiv:2501.12948.
[10] DeepSeek-AI, Liu, A., Feng, B., Xue, B., et al., 2024. DeepSeek-V3
Technical Report. arXiv e-prints arXiv:2412.19437.
[11] Drake, J.H., Kheiri, A., Ã–zcan, E., Burke, E.K., 2020. Recent ad-
vances in selection hyper-heuristics. European Journal of Operational
Research 285, 405â€“428.
[12] Du, X., Liu, M., Wang, K., Wang, H., Liu, J., Chen, Y., Feng,
J., Sha, C., Peng, X., Lou, Y., 2024.
Evaluating large language
models in class-level code generation, in: Proceedings of IEEE/ACM
ICSEâ€™2024.
[13] Duflo, G., Kieffer, E., Brust, M.R., Danoy, G., Bouvry, P., 2019. A gp
hyper-heuristic approach for generating tsp heuristics, in: Proceedings
of IEEE IPDPSWâ€™2019, pp. 521â€“529.
[14] Grishina, A., Liventsev, V., HÃ¤rmÃ¤, A., Moonen, L., 2025.
Fully
autonomous programming using iterative multi-agent debugging with
large language models. ACM Transactions on Evolutionary Learning
S. Zhang et al.: Preprint submitted to Elsevier
Page 12 of 13


--- Page 13 ---
Leveraging social media news
and Optimization 5.
[15] Guo, D., Zhu, Q., Yang, D., et al., 2024.
Deepseek-coder: When
the large language model meets programming - the rise of code
intelligence. arXiv e-prints arXiv:2401.14196.
[16] Jiang, J., Wang, F., Shen, J., Kim, S., Kim, S., 2024.
A Survey
on Large Language Models for Code Generation.
arXiv e-prints
arXiv:2406.00515.
[17] Kallestad, J., Hasibi, R., Hemmati, A., SÃ¶rensen, K., 2023.
A
general deep reinforcement learning hyperheuristic framework for
solving combinatorial optimization problems. European Journal of
Operational Research 309, 446â€“468.
[18] Kool, W., van Hoof, H., Welling, M., 2019. Attention, learn to solve
routing problems!, in: Proceedings of ICLRâ€™2019.
[19] Kumar, R., Joshi, A.H., Banka, K.K., Rockett, P.I., 2008. Evolution
of hyperheuristics for the biobjective 0/1 knapsack problem by multi-
objective genetic programming, in: Proceedings of GECCOâ€™2008, p.
1227â€“1234.
[20] Li, B., Li, J., Tang, K., Yao, X., 2015. Many-objective evolutionary
algorithms: A survey. ACM Computing Surveys 48.
[21] Li, X., Liu, S., Wang, J., Chen, X., Ong, Y.S., Tang, K., 2024a.
Chance-constrained multiple-choice knapsack problem: Model, al-
gorithms, and applications. IEEE Transactions on Cybernetics 54,
7969â€“7980.
[22] Li, Y., Guo, J., Wang, R., Zha, H., Yan, J., 2024b. Fast t2t: Optimiza-
tion consistency speeds up diffusion-based training-to-testing solving
for combinatorial optimization, in: Proceedings of NeurIPSâ€™2024, pp.
30179â€“30206.
[23] Liu, F., Tong, X., Yuan, M., Lin, X., Luo, F., Wang, Z., Lu, Z.,
Zhang, Q., 2024. Evolution of heuristics: towards efficient automatic
algorithm design using large language model, in: Proceedings of
ICMLâ€™2024.
[24] Liu, F., Zhang, R., Xie, Z., Sun, R., Li, K., Lin, X., Wang, Z., Lu, Z.,
Zhang, Q., 2024. LLM4AD: A Platform for Algorithm Design with
Large Language Model. arXiv e-prints arXiv:2412.17287.
[25] Liu, S., Chen, C., Qu, X., Tang, K., Ong, Y., 2024. Large language
models as evolutionary optimizers, in: Proceedings of CECâ€™2024, pp.
1â€“8.
[26] Liu, S., Zhang, Y., Tang, K., Yao, X., 2023.
How good is neural
combinatorial optimization? a systematic evaluation on the traveling
salesman problem. IEEE Computational Intelligence Magazine 18,
14â€“28.
[27] Liu, Y., Li, X., Luo, Y., Du, J., Zhang, Y., Lv, T., Yin, H., Tang,
X., Liu, H., 2025a. Toward a large language model-driven medical
knowledge retrieval and qa system: Framework design and evaluation.
Engineering .
[28] Liu, Y., Zhou, Y., Liu, Y., Xu, Z., He, Y., 2025b. Intelligent fault
diagnosis for cnc through the integration of large language models
and domain knowledge graphs. Engineering .
[29] Lu, N., Liu, S., He, R., Ong, Y.S., Wang, Q., Tang, K., 2024. Large
language models can be guided to evade AI-generated text detection.
Transactions on Machine Learning Research .
[30] Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma,
J., Lin, Q., Jiang, D., 2024. Wizardcoder: Empowering code large
language models with evol-instruct, in: Proceedings of ICLRâ€™2024.
[31] Ma, Y.J., Liang, W., Wang, G., et al., 2024. Eureka: Human-level
reward design via coding large language models, in: Proceedings of
ICLRâ€™2024.
[32] Mazyavkina, N., Sviridov, S., Ivanov, S., Burnaev, E., 2021. Rein-
forcement learning for combinatorial optimization: A survey. Com-
puters & Operations Research 134, 105400.
[33] Mouelhi-Chibani, W., Pierreval, H., 2010. Training a neural network
to select dispatching rules in real time.
Computers & Industrial
Engineering 58, 249â€“256.
[34] Peng, Y., Han, J., Zhang, Z., Fan, L., Liu, T., Qi, S., Feng, X., Ma,
Y., Wang, Y., Zhu, S.C., 2024. The tong test: Evaluating artificial
general intelligence through dynamic embodied physical and social
interactions. Engineering 34, 12â€“22.
[35] Qu, Y., Zhang, T., Garg, N., Kumar, A., 2024.
Recursive intro-
spection: Teaching language model agents how to self-improve, in:
Proceedings of NeurIPSâ€™2024, pp. 55249â€“55285.
[36] Romera-Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar,
M.P., Dupont, E., Ruiz, F.J.R., Ellenberg, J.S., Wang, P., Fawzi, O.,
Kohli, P., Fawzi, A., 2024. Mathematical discoveries from program
search with large language models. Nature 625, 468â€“475.
[37] Sanokowski, S., Hochreiter, S., Lehner, S., 2024. A diffusion model
framework for unsupervised neural combinatorial optimization, in:
Proceedings of ICMLâ€™2024.
[38] Slowik, A., Kwasnicka, H., 2020.
Evolutionary algorithms and
their applications to engineering problems. Neural Computing and
Applications 32, 12363â€“12379.
[39] Storer, R.H., Wu, S.D., Vaccari, R., 1992.
New search spaces
for sequencing problems with application to job shop scheduling.
Management Science 38, 1495â€“1509.
[40] Sun, Z., Yang, Y., 2023.
Difusco: Graph-based diffusion solvers
for combinatorial optimization, in: Proceedings of NeurIPSâ€™2023, pp.
3706â€“3731.
[41] Vinyals, O., Fortunato, M., Jaitly, N., 2015.
Pointer networks, in:
Proceedings of NeurIPSâ€™2015, pp. 2692â€“2700.
[42] Wang, Y., Wang, W., Joty, S., Hoi, S.C., 2021. Codet5: Identifier-
aware unified pre-trained encoder-decoder models for code under-
standing and generation, in: Proceedings of EMNLPâ€™2021, pp. 8696â€“
8708.
[43] Wei, J., Tay, Y., Bommasani, R., et al., 2022. Emergent abilities of
large language models. Transactions on Machine Learning Research
2022.
[44] Wei, L., Cui, Y., Chen, M., Wan, Q., Xing, L., 2025. Multi-objective
neural policy approach for agile earth satellite scheduling problem
considering image quality. Swarm and Evolutionary Computation 94,
101857.
[45] Wu, J., Liu, Q., Hu, H., Fan, W., Liu, S., Li, Q., Wu, X., Tang, K.,
2025. Leveraging chatgpt to empower training-free dataset condensa-
tion for content-based recommendation, in: Companion Proceedings
of WWWâ€™2025, pp. 1402â€“1406.
[46] Xing, L., Chen, Y., Wang, P., Zhao, Q., Xiong, J., 2010. A knowledge-
based ant colony optimization for flexible job shop scheduling prob-
lems. Applied Soft Computing 10, 888â€“896.
[47] Yao, X., 1999. Evolutionary computation: Theory and applications.
World scientific.
[48] Ye, H., Wang, J., Cao, Z., Berto, F., Hua, C., Kim, H., Park, J., Song,
G., 2024.
Reevo: Large language models as hyper-heuristics with
reflective evolution, in: Proceedings of NeurIPSâ€™2024, pp. 43571â€“
43608.
[49] Zan, D., Chen, B., Zhang, F., Lu, D., Wu, B., Guan, B., Wang, Y.,
Lou, J., 2023. Large language models meet nl2code: A survey, in:
Proceedings of ACLâ€™2023, pp. 7443â€“7464.
[50] Zhang, F., Mei, Y., Nguyen, S., Tan, K.C., Zhang, M., 2022. Multitask
genetic programming-based generative hyperheuristics: A case study
in dynamic scheduling. IEEE Transactions on Cybernetics 52, 10515â€“
10528.
S. Zhang et al.: Preprint submitted to Elsevier
Page 13 of 13
