--- Page 1 ---
1
Communication-and-Computation Efficient Split
Federated Learning: Gradient Aggregation and
Resource Management
Yipeng Liang, Graduate Student Member, IEEE, Qimei Chen, Member, IEEE, Rongpeng Li, Member, IEEE,
Guangxu Zhu, Member, IEEE, Muhammad Kaleem Awan, and Hao Jiang, Member, IEEE
Abstract‚ÄîWith the prevalence of Large Learning Models
(LLM), Split Federated Learning (SFL), which divides a learning
model into server-side and client-side models, has emerged as
an appealing technology to deal with the heavy computational
burden for network edge clients. However, existing SFL frame-
works would frequently upload smashed data and download
gradients between the server and each client, leading to severe
communication overheads. To address this issue, this work
proposes a novel communication-and-computation efficient SFL
framework, which allows dynamic model splitting (server- and
client-side model cutting point selection) and broadcasting of
aggregated smashed data gradients. We theoretically analyze the
impact of the cutting point selection on the convergence rate
of the proposed framework, revealing that model splitting with
a smaller client-side model size leads to a better convergence
performance and vise versa. Based on the above insights, we
formulate an optimization problem to minimize the model
convergence rate and latency under the consideration of data
privacy via a joint Cutting point selection, Communication and
Computation resource allocation (CCC) strategy. To deal with
the proposed mixed integer nonlinear programming optimization
problem, we develop an algorithm by integrating the Double
Deep Q-learning Network (DDQN) with convex optimization
methods. Extensive experiments validate our theoretical analyses
across various datasets, and the numerical results demonstrate
the effectiveness and superiority of the proposed communication-
efficient SFL compared with existing schemes, including parallel
split learning and traditional SFL mechanisms.
Index
Terms‚ÄîCommunication-and-computation
efficient,
LLM, distributed training, edge AI, federated split learning,
resource allocation.
I. INTRODUCTION
With significant advancements in Artificial Intelligence
(AI), future 6G networks are envisioned to transition from
‚Äúconnected things‚Äù to ‚Äúconnected intelligence‚Äù by decentral-
izing AI from the central cloud to edge networks [1]. This
evolution aims to support edge AI vision in the 6G networks,
enabling pervasive intelligence to support emerging intelligent
Yipeng Liang, Qimei Chen, and Hao Jiang are with the School of
Electronic Information, Wuhan University, Wuhan 430072, China (e-mail:
liangyipeng@whu.edu.cn, chenqimei@whu.edu.cn, jh@whu.edu.cn).
Guangxu Zhu is with Shenzhen Research Institute of Big Data, Shenzhen,
518172, China (e-mail: gxzhu@sribd.cn).
Rongpeng Li is with the College of Information Science and Electronic
Engineering, Zhejiang University, Hangzhou 310027, China (e-mail: lirong-
peng@zju.edu.cn).
Muhammad Kaleem Awan is with Oman Telecommunications Company
Ltd., PC 112, Ruwi, Oman (e-mail: Muhammad.Awan@omantel.om).
This manuscript is a preliminary version of the work and may be subject
to further revisions.
applications such as, eXtended Reality (XR), intelligent trans-
portation systems, and Internet of Things (IoT) [2]‚Äì[4].
In this context, Distributed Collaborative Machine Learning
(DCML) has emerged as a pivotal technology, particularly due
to its inherent data privacy advantages. Among various DCML
approaches, Federated Learning (FL) and Split Learning (SL)
have become particularly attractive in recent years. FL facili-
tates the training of a complete Machine Learning (ML) model
through collaboration between a central server and distributed
clients, without requiring clients to share their local data
[5]‚Äì[7]. However, while FL supports parallel model training
across multiple clients, resource-constrained devices in edge
networks, such as those in IoT environments, often struggle
to handle the computational demands of training complete
ML models [8], especially Large Language Models (LLMs).
To deal with this issue, SL offers a promising solution by
offloading a portion of the computational burden to the server.
Specifically, SL divides a complete ML model into smaller
network portions, deploying one portion on the client and the
other on the server. The vanilla SL conducts model training
sequentially, with the server interacting with clients one by one
to update the model [9]. However, this sequential approach
introduces significant latency, particularly when managing a
large number of clients. Moreover, it can lead to catastrophic
forgetting, which severely impacts learning performance [10].
To overcome these limitations, Split Federated Learning (SFL)
combines the strengths of FL and SL, enabling parallel model
training while alleviating the computational burden on clients
[11]. With the recent prevalence of LLMs, SFL presents con-
siderable potential for facilitating their training and inference
at the edge of 6G networks.
Despite the advantageous integration of SL and FL, SFL ne-
cessitates frequent exchanges of information, such as smashed
data and corresponding gradients, between the server and
clients to update both server-side and client-side models.
Additionally, the synchronous aggregation of client-side mod-
els at the server introduces additional communication over-
head. Consequently, communication overhead has become a
significant challenge in SFL. Although SFL has garnered
increasing attention in recent years, efforts to mitigate this
communication overhead remain limited. To address this gap,
this work proposes a co-design of Cutting point selection,
Communication and Computation resource allocation (CCC)
strategy for SFL.
arXiv:2501.01078v1  [cs.DC]  2 Jan 2025


--- Page 2 ---
2
Joint CCC strategy
Communication 
allocation
(Power ùëùùëùt
ùëõùëõ/ bandwidth ùêµùêµùë°ùë°
ùëõùëõ)
Cutting point selection
(Choice of cutting point ùë£ùë£)
Computation allocation
(Server frequency ùëìùëìùë°ùë°
ùë†ùë†,ùëõùëõ/ 
Client frequency ùëìùëìùë°ùë°
ùëõùëõ)
Communication 
overhead
Convergence rate
Privacy
Computation burden
Communication rate
Computation speed
Communication 
rounds
Communication 
latency per round
Computation latency 
per round
‚àë
‚àë
‚àë
‚àë
Total Communication 
Latency
Total Computation 
Latency
‚àë
Communication-and-
Computation efficient SFL
Fig. 1. Illustration of the proposed joint CCC strategy.
A. Related Work
Different from existing DCML approach such as FL, the
research on SFL is still in its early stages. The SFL frame-
work was proposed in [11], enabling parallel training and
synchronous aggregation of both client-side and server-side
models, which has recently garnered significant interest in
various fields, including medical image segmentation [12],
wireless networks [13], and emotion detection [14].
Several existing works aim to reduce training latency arising
from the sequential training process and client heterogeneity
in SFL. For example, the authors in [15] introduced a cluster-
based approach that partitions clients into multiple clusters.
Within each cluster, client-side models are trained and aggre-
gated in parallel, followed by sequential training of server-side
and client-side models across clusters. A resource management
algorithm was proposed in [16] to minimize training latency
of SFL by jointly optimizing the selection of the cutting point
and the allocation of computational resources. In [17], an Fed-
Pairing scheme was proposed to enhance training efficiency
by pairing clients with varying computational resources. A
communication-and-storage efficient SFL framework was ex-
plored in [18], where an auxiliary network was integrated into
the client-side model to facilitate local updates. This approach
maintained a single server-side model at the server, thereby
eliminating the need to transmit gradients from the server.
Besides, data privacy and security in SFL have received
increasing attention. The authors in [19] investigated a privacy-
aware SFL, where a client-based privacy approach was intro-
duced to enhance resilience against attacks. In [20], the authors
analyzed the tradeoff between privacy and energy consumption
in SFL, with a particular focus on the impact of the cutting
point selection.
Additionally, some works focus primarily on enhancing SFL
performance considering the non-independent and identically
distributed (Non-IID) data across clients. For instance, the
MergeSFL framework was proposed in [21] addressed Non-
IID challenges by employing feature merging and batch size
regulation across different clients.
However, these works suffer from communication overhead
due to the synchronous client-side model aggregation. To
address this issue, a Parallel Split Learning (PSL) framework
was introduced to enhance communication efficiency by elim-
inating synchronous aggregation. For instant, the authors in
[22] presented a PSL method to prevent overfitting through
minibatch size selection and client layer synchronization at
each client. In [23], a last-layer gradient aggregation scheme
was proposed for PSL to reduce training and communication
latency at the server. A joint subchannel allocation, power con-
trol, and cutting point selection strategy was further proposed,
considering heterogeneous channel conditions and computing
capabilities among clients. In [24], the authors explored a
personalized PSL framework to address Non-IID issues, em-
ploying a bisection method with a feasibility test to optimize
the tradeoff between energy consumption for computation and
wireless transmission, training time, and data privacy. A local-
loss-based training method was proposed in [25] to expedite
the PSL training process by incorporating an auxiliary network
into the client-side model, serving as the local loss function
for model updates.
Despite these efforts, previous approaches for SFL still
suffer from communication overhead. Specifically, clients are
still required to individually upload smashed data to the server
and download the corresponding gradients to update both
client-side and server-side models. This process leads to sub-
stantial communication burden, particularly in environments
with limited communication resources.
B. Motivation and Contribution
Motivated by the above critical issue, we explore a novel
communication-and-computation efficient SFL with Gradient
Aggregation (SFL-GA) framework in this work. Specifically,
the SFL-GA framework enables dynamic model cutting point
selection based on the wireless communication environment,
privacy requirements, and computation abilities of edge de-
vices. According to the model splitting, gradients of the
smashed data at the server are aggregated, and then broad-
casting to all the devices to effectively reduce communication
overhead. For further communication-and-computation effi-
ciency enhancement, we introduce a joint CCC strategy as
shown in Fig. 1. In detail, the communication and computation
resource allocation schemes influence the communication rate
and computation speed, respectively. Meanwhile, the model
cutting point affects communication overhead, computation
burden, convergence rate, and privacy leakage. These el-
ements collectively influence both the communication and
computation latency per round, as well as the number of
communication rounds required for model convergence. Ulti-


--- Page 3 ---
3
mately, the above elements collectively determine the overall
communication and computation latency.
Overall, the main contributions of this work are summarized
as follows.
‚Ä¢ Communication-and-computation efficient SFL-GA
framework: We propose a novel SFL-GA framework,
which enables dynamic model cutting point selection and
aggregated gradient broadcasting. Specifically, the cutting
point is selected to improve the communication and
computation efficiency. On the other hand, we aggregate
all the smashed data gradients before broadcasting instead
of traditional individual gradients feedback to each client,
thus alleviates the communication overhead.
‚Ä¢ Theoretical convergence analysis and problem formu-
lation: A theoretical convergence analysis of our pro-
posed framework is conducted, which reveals that cutting
a smaller client-side model leads to better convergence
performance. As a cost, it increases the risk of privacy
leakage. Based on these insights, we formulate a conver-
gence rate and latency optimization problem under the
limitation of communication and computation resources
as well as the privacy leakage requirement, which is a
Mixed-Integer Non-Linear Programming (MINLP).
‚Ä¢ Joint CCC strategy: To address the MINLP issues, a
joint CCC strategy that integrates double-deep Q-learning
(DDQN) algorithm and convex optimization techniques is
developed. Specifically, the problem is decomposed into
two subproblems: resource allocation and cutting point
selection. The resource allocation subproblem is resolved
using existing convex optimization methods, while the
cutting point selection subproblem is tackled with the
DDQN algorithm.
‚Ä¢ Performance evaluation: Numerical results are con-
ducted to validate the theoretical analyses, and evaluate
the superior performance of the proposed SFL-GA mech-
anism compared with benchmarks, including SFL, PSL,
and FL.
The rest of this paper is organized as follows. Section II
introduces the SFL-GA framework and corresponding system
models. In Section III, we theoretically analyze the conver-
gence performance for the proposed framework. In Section IV,
we formulate the optimization problem and design the resource
allocation strategy. Numerical results are presented in Section
V followed by a conclusion in Section VI.
Throughout the paper, we use the following notation: We
use a to denote a scalar, a is a column vector, A is a matrix,
and | ¬∑ | represents the modulus operator. The Euclidean norm
is written as ‚à•¬∑‚à•, ‚ü®a, a‚Ä≤‚ü©is the inner product of a and a‚Ä≤, and
E represents mathematical expectation.
II. SYSTEM MODEL
In this section, we first introduce a novel SFL-GA frame-
work, and then discuss the related system model.
A. SFL-GA Framework
As shown in Fig.2, we consider an SFL wireless network
with one server and a set of clients denoted by N
‚âú
{1, 2, ..., N}. All of the clients are collaboratively training a
shared Machine Learning (ML) model w ‚ààRq of size q and
layer V for a specific data analysis task, such as classification
and recognition. In the SFL framework, each client splits/cuts
its learning model at the v ‚ààV ‚âú{1, 2, 3, ..., V ‚àí1}-th
layer. Thus, the entire model divided into the client-side model
wc ‚ààRœï(v) and the server-side model ws ‚ààRq‚àíœï(v), de-
ployed on the client and server for model training, respectively.
Here, œï (v) denotes the client-side model size, which depends
on the cutting point v.
Therefore, the desired ML model is collaboratively trained
by the server and clients over T communication rounds.
During each communication round t ‚ààT ‚âú{1, 2, ..., T}, the
training process unfolds as follows:
1) Smashed data generation: All clients conduct forward
propagation (FP) based on their local datasets and gen-
erate smashed data.
2) Server-side model update: All clients transmit the
smashed data, along with the corresponding labels, to
the server. Thereafter, the server performs FP and cal-
culates the loss function in parallel based on the received
smashed data and their corresponding labels. Then, the
server executes back propagation (BP) to obtain the up-
dated server-side models and the smashed data gradients.
3) Server-side gradients aggregation: The server aggre-
gates these updated server-side models into a new global
server-side model, as well as aggregates the gradients of
the smashed data.
4) Gradient broadcast: The server broadcasts the aggre-
gated gradients of the smashed data to all the clients. Un-
like the traditional SFL, where the gradients of smashed
data are transmitted to corresponding clients separately,
our proposed framework effectively reduces communica-
tion overhead.
5) Client-side model BP: The clients perform client-side
model BP and update their client-side models based on
the received gradients. Since the client-side models have
identical weight parameters and are updated based on the
same gradients, synchronous client-side model aggrega-
tion is eliminated, leading to a further communication
overhead reduction.
The core concept of the proposed framework is to aggregate
the gradients of smashed data from all the clients and then
broadcast the aggregated gradients to these clients. Compared
with the traditional SFL mechanism, it can significantly re-
duced the communication overhead.
B. SFL-GA Model
Denote that the complete ML model w is partitioned into
the server-side model ws and the client-side model wc,
denoted as w = [ws; wc]. Moreover, each client n is assumed
to possess a local dataset Dn with size of Dn.
For communication round t, all clients perform client-side
model FP in parallel based on their own client-side models
wc
t‚àí1. Specifically, each client n takes a mini-batch of samples


--- Page 4 ---
4
Smashed data
Loss
Gradients of 
Smashed data
Client 1
Client-side model
Dataset 1
Smashed data
Loss
Gradients of 
Smashed data
Client N
Client-side model
Dataset N
Server-side model
Server-side model
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
Edge server
Fed server
(a) Traditional Split Federated Learning
Smashed data
Loss
Client 1
Client-side model
Smashed data
Loss
Client N
Client-side model
Server-side model
Server-side model
‚Ä¶
‚Ä¶
Aggregated gradients of 
Smashed data
Broadcast
Dataset 1
Dataset N
Edge server
(b) Split Federated Learning with Gradient Aggregation framework
Fig. 2. Proposed SFL-GA framework and Traditional SFL.
Œæn, randomly chosen from Dn, as the input for wc
t‚àí1 to obtain
the smashed data, which can be expressed as
Sn
t = ‚Ñì
 wc
t‚àí1; Œæn
,
(1)
where ‚Ñìis the the client-side function mapping from the input
data to the smashed data. Then, the server collects the smashed
data along with their corresponding labels from all the clients
for the server-side model update and aggregation. Specifically,
the server first performs FP to calculate the loss for each device
by inputting the corresponding smashed data into the server-
side model ws
t‚àí1. Therefore, the local loss with respect to the
complete model of device n can be expressed as
F
 wn
t‚àí1

= F
 ws,n
t‚àí1, wc
t‚àí1; Œæn
=
1
Dn
X
(xj,yj)‚ààŒæn
f
 ws,n
t‚àí1, wc
t‚àí1; (xj, yj)

,
(2)
where f is the loss function. (xj, yj) is the j-th sample of
Œæn with data xj and label yj. Subsequently, after server-side
model BP, the server obtains the gradients of the loss function
for client n‚Äôs model update, which is given by
gs,n
t‚àí1 = ‚àáwsF
 ws,n
t‚àí1, wc
t‚àí1; Œæn
.
(3)
Meanwhile, the gradients of the smashed data of each device
n are also computed as
sn
t = ‚àáSn
t .
(4)
Different from the existing SFL in [11] where the server
sends sn
t to the corresponding client n, the server in this work
broadcasts the aggregated gradients of smashed data to all
clients, which is given by
st =
N
X
n=1
œÅnsn
t ,
(5)
where œÅn = Dn
D with D = PN
n=1 Dn being the total size of
datasets across clients.
After receiving st from the server, both the server and
the clients can update the server- and client-side models,
respectively. Specifically, each client first computes the gra-
dient of its client-side model, denoted by gc
t, based on st.
The client-side model is then updated via gradient descent.
Consequently, the complete ML model wn
t of the client n in
the t-th communication round can be updated through multiple
epochs, described as follows
wn
t =
ws,n
t
wc
t

=
ws
t‚àí1
wc
t‚àí1

‚àíŒ∑
œÑ
X
i=1
gs,n
t‚àí1,i
gc
t‚àí1,i

,
(6)
where Œ∑ is the learning rate. œÑ is the number of local epochs.
Since the model wc
t at each client is updated based on the
same gradient gc
t‚àí1 and the same parameters wc
t‚àí1, each client
attains the same model parameters after updating. Therefore,
the proposed SFL-GA eliminates the necessity for client-side
model aggregation as [11]. Consequently, we only need to
aggregate the server-side model at the server, which is given
by
ws
t =
N
X
n=1
œÅnws,n
t
.
(7)


--- Page 5 ---
5
As a result, with the complete global model wt = [ws
t, wc
t]
at the t-th communication round, the global loss function can
be represented as
F (wt) =
N
X
n=1
œÅnF (wn
t ) .
(8)
Assuming the global model converges after T communi-
cation rounds, the training objective of Eq. (8) is to find a
minimal global model w‚àó= [ws‚àó; wc‚àó] that satisfies
w‚àó= arg min
w F (wT ) .
(9)
C. Communication Model
In this subsection, we introduce the communication model
in this work. We assume that the channel remains constant
during a given communication round but may vary across
different rounds. For an arbitrary communication round t,
the communication process of each client includes an uplink
phase for uploading smashed data and a downlink phase for
broadcasting gradients.
Regarding the uplink communication, the total available
bandwidth B is divided into multiple orthogonal subchannels
to transmit the smashed data and labels from each client to
the server. Therefore, the achievable data rate of device n can
be expressed as
rn,U
t
= Bn
t log2

1 + pn
t gn
t
Bn
t N0

,
(10)
where Bn
t is the bandwidth allocated to client n. gn
t is the
channel gain between the server and device n. pn
t and N0
denote the transmit power of device n and the thermal noise
spectrum density, respectively.
For the downlink communication, the server broadcasts
the aggregated gradients (5), occupying the entire bandwidth.
Thus, the achievable rate for device n can be represented as
rn,D
t
= B log2

1 + Pgn
t
BN0

,
(11)
where P denotes the transmit power of server.
Note that the size of smashed data and corresponding gra-
dients depend on the cutting point v. Therefore, the latency of
uplink and downlink transmission can be respectively written
as
ln,U
t
= Xt (v)
rn,U
t
,
(12)
ln,D
t
= Xt (v)
rn,D
t
,
(13)
where Xt (v) is the communication bit size of smashed data
(and its gradient) related to cutting point v.
D. Computation Model
In this subsection, we present the computation model of
the proposed SFL-GA. During communication round t, SFL
initiates with client-side model FP. Let Œ≥n
F (v) denote the
computation workload (in FLOPs) of client n for performing
FP with one data sample [26], [27]. Therefore, the latency for
client-side model FP is given by
ln,F
t
= DnŒ≥n
F (v)
f n
t
,
(14)
where f n
t denotes the central processing unit (CPU) resource
of device n.
Subsequently, SFL performs FP and BP to update the server-
side model at the server. Let Œ≥s
F (v) and Œ≥s
B (v) denote the
computation workload of the server for performing FP and
BP with one data sample, respectively. Therefore, the latency
for both server-side model FP and BP is given by
ln,s
t
= Dn (Œ≥s
F (v) + Œ≥s
B (v))
f s,n
t
,
(15)
where f s,n
t
is the CPU computation resource of server that
allocated to server-side model of client n.
Finally, SFL-GA conducts client-side model BP to update
the model at each client. Let Œ≥n
B (v) denote the computation
workload of client n for performing BP with one data sample.
Then, we have
ln,B
t
= DnŒ≥n
B (v)
f n
t
.
(16)
E. Privacy Model
In the proposed SFL framework, the privacy concerns arise
from the transmission of smashed data between clients and
the server. Existing research has demonstrated the significant
impact of cutting point on privacy leakage [20], [28], [29].
Generally, a greater number of layers in the client-side model
leads to poorer reconstruction results, making it more difficult
to infer the original input data. Consequently, as the number of
layers increases, the potential for privacy leakage diminishes.
This insight underscores the importance of carefully selecting
the cutting point in SFL to balance computational efficiency
with privacy preservation.
In this work, to quantify privacy leakage, we adopt the
privacy model introduced in [24], which is formulated as
log(1 + œït (v)
q
) ‚â•œµ, ‚àÄt,
(17)
where œµ represents a desired threshold for privacy protection.
III. THEORETICAL ANALYSIS AND PERFORMANCE
EVALUATION
In this section, we present a theoretical analysis of the
proposed SFL-GA framework. We start by analyzing the
convergence of SFL-GA, followed by a discussion of its
complexity and scalability.
A. Convergence Analysis
To facilitate analysis, we denote the mini-batch (stochastic)
gradient and full-batch gradient of the loss function, respec-
tively, as
gn
t = [gs,n
t
; gc
t] ,
(18)
and
hn
t =
h
‚àáwsF (wn
t ) ;
Àú
‚àáwcF (wt)
i
.
(19)


--- Page 6 ---
6
Recall that in traditional SFL, each client n generates the
gradient of the client-side model ‚àáwcF (wn
t ) based on its
own gradient of smashed data sn
t , instead of the aggregated
one in Eq. (6), to update wc
t. This discrepancy may affect the
convergence performance. Therefore, we denote the full-batch
gradients as
‚àáF(wn
t ) = [‚àáwsF (wn
t ) ; ‚àáwcF (wn
t )] ,
(20)
Accordingly, we further define the the SFL‚Äôs global gradient
as
‚àáF(wt) =
N
X
n=1
‚àáF(wn
t )
=
" N
X
n=1
‚àáwsF (wn
t ) ;
N
X
n=1
‚àáwcF (wn
t )
#
.
(21)
To begin with, we introduce the following assumptions,
which are commonly adopted in existing works, such as [13],
[25].
Assumption 1 (L-smoothness). For any w, v, the loss
function is either continuously differentiable or Lipschitz
continuous with a non-negative Lipschitz constant L ‚â•0,
which can be formulated as
F(v) ‚àíF(w) ‚â§(v ‚àíw)‚ä§‚àáF(w) + L
2 ‚à•v ‚àíw‚à•2.
(22)
Assumption 2 (Unbiased Gradient and Bounded Vari-
ance ). For each client, the stochastic gradient is unbiased,
i.e., E (gs,n
t
) = ‚àáwsF (wn
t ) and E (gc
t) =
Àú
‚àáwcF (wn
t ).
Moreover, the variance of stochastic gradients of each client
is bounded by
E
 ‚à•gn
t ‚àí‚àáF(wn
t )‚à•2
‚â§œÉ2.
(23)
Note that in both the SFL-GA framework and the tra-
ditional SFL framework, the server-side model is updated
using the same smashed data. The divergence between the
two frameworks arises only in the updating of the client-side
model: SFL-GA employs aggregated gradients of the smashed
data, while the traditional SFL framework uses the gradients
from each client‚Äôs individual smashed data. This divergence
influences the convergence behavior, and accurately charac-
terizing this discrepancy is challenging. Nevertheless, we can
observe that this discrepancy is significantly related to the
size of the client-side model. Specifically, as the size of the
client-side model increases, the differences between the client-
side models in the SFL-GA framework and the traditional
SFL framework become more pronounced, thereby exerting
a greater impact on the convergence of both frameworks.
Considering the convergence discrepancy related to the size of
the client-side model, we introduce the following assumption.
Assumption 4. (Bounding the Difference Between SFL
Gradient and SFL-AG Gradient). When the client-side
model holds a size of œït (v) in t-th round, the expected
gradient variance between SFL and SFLAG is bounded by
E

‚à•hn
t ‚àí‚àáF(wn
t )‚à•2
= E

‚à•‚àáwsF (wn
t ) ‚àí‚àáwsF (wn
t )‚à•2
+

Àú
‚àáwsF (wt) ‚àí‚àáwcF (wn
t )

2
= E

Àú
‚àáwcF (wt) ‚àí‚àáwcF (wn
t )

2
‚â§Œì (œït (v)) ,
(24)
where Œì (¬∑) is a monotone non-decreasing function with
respect to the client-side model size œït (v). Assumption 4
indicates that the gradient difference between vanilla SFL and
SFL-GA is related to the client-side model size œït (v) and is
bounded. Specifically, a smaller client-side model size œït (v)
results in a smaller gradient difference.
With above assumptions, we introduce Lemma 1 to demon-
strate the upper bound of the improvement of the global loss
function in each round.
Lemma
1.
When
the
learning
rate
Œ∑
satisfies
0
‚â§
2L2Œ∑2œÑ (œÑ ‚àí1) ‚â§
1
5 in the t-th communication round, the
improvement of the global loss function is bounded by
E (F (wt+1) ‚àíF (wt)) ‚â§‚àíŒ∑œÑ
4 ‚à•‚àáF(wt)‚à•2 + Œ∑œÑŒì (œït (v))
+LŒ∑2œÑœÉ2
N
X
n=1
(œÅn)2 + 5L2Œ∑3œÉ2œÑ (œÑ ‚àí1)
4
.
(25)
Proof. Please refer to Appendix A.
Based on Lemma 1, we further introduce the following
Theorem to show the upper bound of the average squared
gradient norm, which illustrates the convergence performance.
Theorem 2. Under the condition of 0 ‚â§2L2Œ∑2œÑ (œÑ ‚àí1) ‚â§
1
5, the average squared gradient norm after T communication
rounds is bounded by
1
T
T
X
t=1
‚à•‚àáF(wt)‚à•2 ‚â§4 (F (wT ) ‚àíF ‚àó)
Œ∑œÑT
|
{z
}
Effects of initialization
+ 4
T
T
X
t=1
Œì (œït (v))
|
{z
}
Effects of cutting point
+ 4LŒ∑œÉ2
N
X
n=1
(œÅn)2 + 5L2Œ∑2œÉ2 (œÑ ‚àí1)
|
{z
}
Effects of gradient variance
.
(26)
Remark 1. It is observed that the (26) is influenced by the
initialization, gradient variance, as well as the cutting point.
In particular, reducing the impact of the cutting point during
the training process can lower the bound of (26), suggesting
that a smaller client-side model size enhances convergence
performance. At the same time, the cutting point also affects
communication overhead, computational burden, and privacy
leakage. This observation motivates the co-design of CCC for
achieving a communication-and-computation efficient SFL.


--- Page 7 ---
7
B. Computational Complexity and Scalability
To facilitate the analysis of computational complexity, we
let œÅn = 1
N . Therefore, (26) can be reformulated as
1
T
T
X
t=1
‚à•‚àáF(wt)‚à•2 ‚â§4 (F (wT ) ‚àíF ‚àó)
Œ∑œÑT
+ 4LŒ∑œÉ2
N
+5L2Œ∑2œÉ2 (œÑ ‚àí1) + 4
T
T
X
t=1
Œì (œït (v)) .
(27)
If the learning rate satisfies Œ∑ =
q
N
œÑT [6], the computational
complexity of SFL-GA is given by
E
 
1
T
T
X
t=1
‚à•‚àáF(wt)‚à•2
!
‚â§O

1
‚àö
œÑNT
+
œÉ2
‚àö
œÑNT
+ N (œÑ ‚àí1) œÉ2
œÑT
+ M

,
(28)
where M =
4
T
PT
t=1 Œì (œït (v)). If M is bounded, the com-
putational complexities of proposed SGL-GA is given by
O

1
‚àö
œÑNT

+ O

œÉ2N(œÑ‚àí1)
œÑT

.
To evaluate the scalability of our proposed SFL-GA, we
focus on the convergence behavior in (28) with respect to the
number of clients N. From (28), the first two terms decrease
with the increment of N. It indicates that the convergence rate
benefits from a larger number of clients through improving
the average updates performance. On the other aspect, the
third term of (28) increases linearly with N, which suggests
a potential risk in computational complexity and variance.
Consequently, the convergence behavior initially improves but
eventually deteriorates as N continues to grow.
IV. PROBLEM FORMULATION AND RESOURCE
OPTIMIZATION
In this section, we formulate a convergence rate and latency
optimization problem based on the system models and conver-
gence results. Subsequently, a joint CCC strategy that solved
by the DDQN algorithm and convex optimization is designed.
A. Problem Formulation
According to the latency analysis in Section II, the gradients
of smashed data are aggregated before broadcasting to all
clients. Therefore, the total latency for communication round
t is derived by
lt = max
n {ln,U
t
+ ln,F
t
+ ln,s
t
} + max
n {ln,D
t
+ ln,B
t
}.
(29)
Apparently, the wireless channel conditions and heteroge-
neous computing capabilities of clients may significantly affect
the latency of SFL, while the cutting point influences the con-
vergence rate. In this work, we aim to realize communication-
and-computation efficient SFL. To this end, we formulate
a convergence rate and latency optimization problem while
considering resource and privacy constraints.
Let f s =
h
f 1,s
1 , ..., f N,s
T
i
, f c =
h
f 1,c
1 , ..., f N,c
T
i
, p =

p1
1, ..., pN
T

and B =

B1
1, ..., BN
T

. The communication-and-
computation efficient problem can be formulated as
P1 :
min
{v,f s,f c,p,B}
T
X
t=1
(wŒì (œït (v)) + lt (v)) ,
(30)
s.t.
v ‚ààV,
(30a)
0 ‚â§f n
t ‚â§f n,c
max, ‚àÄn, t,
(30b)
0 ‚â§pn
t ‚â§pn
max, ‚àÄn, t,
(30c)
N
X
n=1
f n,s
t
‚â§f s
max, ‚àÄt,
(30d)
log(1 + œït (v)
q
) ‚â•œµ, ‚àÄt,
(30e)
N
X
n=1
Bn
t ‚â§B, ‚àÄt,
(30f)
where w is a weighted factor to balance the convergence rate
and latency. (30a) is the layer constraint of ML model, f n,c
max
and ps
max in (30b) and (30c) are the the maximum computa-
tion resource and transmit power of each client, respectively,
f n,s
max in (30d) denotes the maximum computation resource
constraint for model update at the server. (30e) is the constraint
for privacy protection, (30f) ensures that the total bandwidth
for all clients doesn‚Äôt exceed the available bandwidth B.
P1 is a min-max MINLP problem, which is coupling of
cutting point selection and resource allocation. The major
difficulty of solving P1 lies in the efficient determination
of the cutting point selection under dynamic fading channels
and heterogeneous capabilities of clients. Existing optimiza-
tion algorithms require iteratively adjusting or exhaustively
enumerate the cutting point variables, which is inefficient.
Therefore, a joint CCC strategy is developed based on DDQN
algorithm.
B. Joint CCC Strategy
To address the min-max issue of P1, we introduce auxiliary
variables œát and œàt. Then, P1 can be equivalently reformu-
lated as
P2 :
min
{v,f s,f c,p,B}
T
X
t=1
(wŒì (œït (v)) + œát + œàt) ,
(31)
s.t.
(30a), (30b), (30c), (30d), (30e), (30f),
(31a)
ln,U
t
+ ln,F
t
+ ln,s
t
‚â§œát,
(31b)
ln,D
t
+ ln,B
t
‚â§œàt ‚àÄt.
(31c)
Nonetheless, P2 remains an NP-hard problem. Intuitively,
it can be divided into two subproblems: resource allocation
and cutting point selection. Therefore, we can address P2
by jointly solving these subproblems. In what follows, we
present the optimization methods for the resource allocation
and cutting point selection subproblems, respectively.


--- Page 8 ---
8
1) Resource Allocation: Given the optimal cutting point
selection variables v‚àó, the resource allocation subproblem is
independent to the communication rounds. Therefore, it can
be decomposed into T separate subproblems, each addressed
independently. Without loss of generality, the resource alloca-
tion subproblem for communication roud t is formulated as
P2.1 :
min
{f s,f c,p,B} œát + œàt,
(32)
subject to (30b), (30c), (30d), (30f), (31b), (31c).
It can be easily shown that P2.1 is a convex optimization
problem. Therefore, it can be resolved by existing optimization
technique (e.g., CVX).
2) Cutting Point Selection: Given the optimal resource allo-
cation f ‚àó, p‚àóand B‚àó. The cutting point selection subproblem
can be expressed as
P2.2 : min
{v}
T
X
t=1
(wŒì (œït (v)) + œát + œàt) ,
(33)
subject to (30a), (30e),(31b), (31c).
Due to the integer variables, P2.2 is an integer program-
ming. The DDQN algorithm has been identified as an effective
method to tackle such decision issues with integer variables
[30]‚Äì[32]. Therefore, we employ the DDQN algorithm to deal
with P2.2 in this work. Before using the DDQN algorithm,
the subproblem P2.2 needs to be transformed into a Markov
Decision Process (MDP) problem with a tuple ‚ü®S, A, P, R‚ü©,
where S, A, P and R are the state space, action space, state
transition probability, and reward, respectively. Specifically,
the corresponding elements in the tuple are presented as
follows.
‚Ä¢ State space S. It is observed the channel gain at the
beginning of communication round t Therefore, we define
the state space in communication round t as
st = {hn,c
t
,
t‚àí1
X
i=1
(Œì (œïi (v)) + œái + œài)}‚àÄn.
(34)
‚Ä¢ Action space A. Since the cutting point v is selected from
V that composed of V ‚àí1 layers in each communication
round, we define the state space in communication round
t as at = {1, 2, ..., V ‚àí1}.
‚Ä¢ State transition probability P. Let P (st‚àí1|st, at) be the
probability of transitioning from state st‚àí1 to state st
under action at.
‚Ä¢ Reward R. Reward rt is designed to evaluate the quality
of a learning policy under state-action pair (st, at), which
is defined as
rt (st, at) =
(
Œì (œït (v)) + œát + œàt,
log(1 + œït(v)
q
) ‚â•œµ,
C,
otherwise.
(35)
where C is a sufficiently large value used as a penalty.
Based on the tuple above, we further define the cumulative
discounted reward for t-th communication round as
Ut =
lim
T ‚Üí+‚àû
T
X
i=t
Œ≥i‚àítri (si, ai) ,
(36)
where Œ≥ ‚àà(0, 1] is the discount factor for weighting future
rewards. Then the MDP problem is formulated, aiming to find
an optimal policy œÄ‚àóthat maximizes the expected long-term
discounted rewards, i.e.,
œÄ‚àó= arg max
œÄ
EœÄ [Ut] .
(37)
To tackle the MDP problem, the DDQN algorithm defines
an agent that interacts with the environment to choose better
actions based on a certain policy œÄ for maximizing long-
term discounted rewards. To this end, the DDQN introduces
a state-action function QœÄ (st, at; Œ∏) for a certain policy œÄ as
the expected future long-term reward for a state-action pair
(st, at), which is presented by
QœÄ (st, at; Œ∏) = EœÄ [Ut|st, at] ,
(38)
where Œ∏ is the parameter vector of the Q-network.
To find the optimal policy œÄ‚àó, it is equivalent to obtaining
the optimal action-value function Q‚àó(st, at; Œ∏), which can be
achieved through the Bellman equation as
Q
‚àó(st, at; Œ∏) = rt + Œ≥ max
at+1 Q
‚àó(st+1, at+1; Œ∏) .
(39)
The optimal action-value function Q
‚àócan be obtained by
optimizing the parameter vector Œ∏ of the Q-network. To this
end, the DDQN algorithm optimizes the parameter Œ∏ by
minimizing the following loss function
L(Œ∏) =

rt + Œ≥ max
at+1 Q

st+1, arg max
at+1 Q (st+1, at+1; Œ∏) ; ÀÜŒ∏

‚àíQ(st, at; Œ∏))2 .
(40)
Then, a gradient descent method is employed to minimize
the loss function L(Œ∏). As a result, the optimal policy is
achieved by obtaining the optimal parameter vector Œ∏‚àó.
Following the proposed optimization methods above, we
now introduce a joint cutting point control and resource
allocation strategy for P1. Specifically, we employ the DDQN
to optimize the cutting point selection subproblem by reformu-
lating P2.2 as a MDP. During each exploration in the DDQN
algorithm, the agent take an action to acquire rewards as
defined in equation (35) where œát and œàt are obtained through
resolving P2.1 by convex optimization technique.
The detailed procedure is shown in Algorithm 1.
C. Complexity Analysis of Algorithm 1
As described previously, the proposed Algorithm 1 inte-
grates convex optimization method and DDQN algorithm,
where the agent needs to resolve P2.1 before obtaining a
reward. Therefore, we first analyses the complexity of solving
P2.1. Then, the complexity of algorithm 1 is further presented.
Note that P2.1 is a convex optimization problem, which can
be resolved with a polynomial complexity, e.g., O(N 3.5) [34].
The DDQN network is represented by an fully-connected
NN (FCNN) in this work. Generally, the computational com-
plexity of an FCNN is O (P
k (2Ik ‚àí1) Ik+1) [35], where
k ‚àà[0, K] denotes the layer index and Ik represents the
neuron number of hidden layers. Let M be the total number


--- Page 9 ---
9
0
20
40
60
80
100
Communication Rounds
0.0
0.2
0.4
0.6
0.8
1.0
Test accuracy
FL
SFL-GA with cutting point v = 1
SFL-GA with cutting pointv = 2
SFL-GA with cutting pointv = 3
SFL-GA with cutting pointv = 4
(a) MINIST
0
20
40
60
80
100
Communication Rounds
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Test accuracy
FL
SFL-GA with cutting point v = 1
SFL-GA with cutting point v = 2
SFL-GA with cutting point v = 3
SFL-GA with cutting point v = 4
(b) FMINIST
0
200
400
600
800
1000
Communication Rounds
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Test accuracy
FL
SFL-GA with cutting point v = 1
SFL-GA with cutting point v = 2
SFL-GA with cutting point v = 3
SFL-GA with cutting point v = 4
(c) CIFAR-10
Fig. 3. Convergence performance evaluation over different cutting layer.
0
1000
2000
3000
4000
5000
6000
7000
8000
Communication Overhead (Mb)
0.0
0.2
0.4
0.6
0.8
1.0
Test accuracy
SFL-GA
SFL
PSL
2000
2500
3000
3500
4000
4500
0.90
0.92
0.94
0.96
0.98
(a) MNIST
0
1000
2000
3000
4000
5000
6000
7000
8000
Communication Overhead (Mb)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Test accuracy
SFL-GA
SFL
PSL
2000
2500
3000
3500
4000
4500
0.700
0.725
0.750
0.775
0.800
(b) FMNIST
0
10000
20000
30000
40000
50000
60000
70000
80000
Communication Overhead (Mb)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Test accuracy
SFL-GA
SFL
PSL
15000 20000 25000 30000 35000
0.70
0.72
0.74
0.76
0.78
(c) CIFAR-10
Fig. 4. Communication overheads over different schemes.
Algorithm 1: The joint CCC strategy for P1.
Input: Initialize parameter vector of Q-networks Œ∏1;
Initialize the experience buffer; Maximum
episode number Lmax.
1 for episode ‚Ñì= 1 to Lmax do
2
Reset the initial state s1;
3
for communication round t = 1 to T do
4
DQN agent selects discrete action at based on
the observed state st;
5
Obtain the optimal f n,c‚àó
t
, f n,s‚àó
t
, pn‚àó
t
and Bn‚àó
t
by resolving P2.1;
6
Calculate the reward rt with f n,c‚àó
t
, f n,s‚àó
t
, pn‚àó
t
and Bn‚àó
t ;
7
Observe the next st+1;
8
Add transition (st, at, rt, st+1) to the replay
buffer;
9
Sample a minibatch from the replay buffer;
10
Update DQN network by the gradient descent
method: Œ∏t+1 ‚ÜêŒ∏t;
11
end
12 end
of episodes and T be the number of steps per episode.
Then, the overall computational complexity of Algorithm 1
is O
 TM
 P
k (2Ik ‚àí1) Ik+1 + N 3.5
.
V. SIMULATION RESULTS
In this section, we provide simulation results to validate
the effectiveness of proposed SFL-GA and the efficiency of
developed algorithm design.
A. Experiment Setup
1) Proposed training setting: The experiments are con-
ducted on a environment with a server and N = 10 devices.
The learning task is to train a Convolutional Neural Networks
(CNN) model for different classification tasks. To evaluate
our proposed scheme, we conduct the experiments over three
different datasets: MNIST, fashion MNIST and CIFAR-10
datasets. We use similar model architectures as adopted in [33]
for model training. The max CPU-cycle frequency f n,max
t
for
each client is 0.1 GHz, and the total CPU-cycle frequency for
server is 100 GHz. We assume the computation workloads for
each client and server are set to Œ≥n
F = Œ≥n
B = 5.6 MFlops and
Œ≥n
F = Œ≥n
B = 86.01 MFlops, respectively [13].
2) Wireless communication setting: We assume that the
path loss of wireless channels between devices and the edge
server is given by 128.1 + 37.6log10(d) (in dB) , where d
represents the distance in kilometer (km) [36]. We assume
that the thermal noise spectrum density N0 = ‚àí174 dBm. The
maximum transmit power budgets for each client and server
are pn
max = 25 dBm and P = 33 dBm, respectively. The total
bandwidth B = 20 MHz.


--- Page 10 ---
10
0
200
400
600
800
1000
1200
1400
Latency (s)
0.0
0.2
0.4
0.6
0.8
1.0
Test accuracy
SFL-GA
SFL
PSL
FL
150
200
250
300
0.90
0.92
0.94
0.96
0.98
(a) MNIST
0
200
400
600
800
1000
1200
1400
Latency (s)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Test accuracy
SFL-GA
SFL
PSL
FL
150
200
250
300
0.72
0.74
0.76
0.78
0.80
0.82
(b) FMNIST
0
2000
4000
6000
8000
10000
12000
14000
Latency (s)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Test accuracy
SFL-GA
SFL
PSL
FL
3000
3200
3400
0.75
0.76
0.77
0.78
(c) CIFAR-10
Fig. 5. Accuracy under latency of different schemes.
0
100
200
300
400
500
600
Latency (s)
0.0
0.2
0.4
0.6
0.8
1.0
Test accuracy
Propsoed SFL-GA
SFL-GA with fixed resource
random layer with optimal resource
random layer with fixed resource
Fixed layer with optimal resource
Fixed layer with fixed resource
(a) MNIST
0
100
200
300
400
500
600
Latency (s)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Test accuracy
Propsoed SFL-GA
SFL-GA with fixed resource
random layer with optimal resource
random layer with fixed resource
Fixed layer with optimal resource
Fixed layer with fixed resource
(b) FMNIST
0
1000
2000
3000
4000
5000
6000
Latency (s)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Test accuracy
Propsoed SFL-GA
SFL-GA with fixed resource
random layer with optimal resource
random layer with fixed resource
Fixed layer with optimal resource
Fixed layer with fixed resource
(c) CIFAR-10
Fig. 6. Accuracy vs latency over different resource strategies.
B. Theoretical Analyses
Fig. 3 illustrates the convergence behavior across different
cutting points under various datasets. Here, the SFL scheme
serves as a benchmark for validating our theoretical analyses.
It is evident that SFL outperforms the proposed SFL-GA in
terms of communication rounds. Moreover, the convergence
performance of SFL-GA deteriorates with an increasing cut-
ting point. For example, while SFL-GA with cutting point
v = 1 achieves approximately 95% test accuracy over the
MINIST dataset, SFL-GA with cutting point v = 4 only
achieves about 82% after 100 communication rounds. This
observation aligns with our theoretical analysis, indicating that
a smaller client-side model size leads to better convergence
performance for SFL-GA.
Fig. 4 presents communication overheads versus conver-
gence performance across various existing schemes, including
SFL-GA, traditional SFL, and PSL. It is observed that our
proposed SFL-GA demonstrates greater communication effi-
ciency compared to traditional SFL and PSL, as it achieves
comparable test accuracy with significantly lower communi-
cation overhead. For example, the communication overhead
for SFL-GA to achieve approximately 94% test accuracy is
below 20 MB on the MNIST dataset, whereas it exceeds 40
MB for traditional SFL. This underscores the effectiveness of
our proposed SFL-GA in terms of communication efficiency.
Additionally, we note that the communication overhead for
PSL is marginally lower than that of traditional SFL for
achieving the same test accuracy. This discrepancy arises
from the fact that PSL does not necessitate client-side model
aggregation, unlike SFL.
Fig. 5 illustrates the accuracy versus latency across differ-
ent datasets for different schemes. As shown in Fig. 5, FL
exhibits the highest latency to achieve convergence, thereby
demonstrating the poorest performance. This is due to the
fact that FL updates the entire model on clients with limited
computational resources. In contrast, SFL-GA, SFL, and PSL
offload portions of the ML model to a server with greater
computational power for model training, thus reducing the
latency required for convergence. In particular, our proposed
SFL-GA achieves similar test accuracy compared to SFL and
PSL. However, as shown in Fig. 4, SFL-GA significantly
reduces communication overhead, highlighting the superiority
of our framework.
C. Effectiveness of the Proposed Algorithms
Fig. 6 evaluates accuracy against latency under various
resource allocation strategies. We consider both fixed cutting
layer and random cutting layer strategies, each assessed using
optimal and fixed computation and communication resource
allocations as benchmarks. It is seen that that our proposed
Algorithm 1 for SFL-GA achieves the shortest latency for con-
vergence among the benchmarks. Additionally, the selection
of the cutting layer significantly impacts latency. For instance,
with identical computation and communication resource allo-
cations, Algorithm 1 substantially reduces latency compared
to the random strategy across different datasets.


--- Page 11 ---
11
0
100
200
300
400
500
Episodes
200
180
160
140
120
100
80
60
40
Rewards
 = 0.01
 = 0.001
 = 0.0001
0
100
200
300
400
500
400
375
350
325
300
275
250
 = 0.1
Fig. 7. Convergence performance of Algorithm 1
10
20
30
40
50
60
Bandwidth (MHz)
250
300
350
400
450
500
550
Latency (s)
Propsoed SFLGA
SFL
PSF
FL
10
20
30
40
50
60
1436
1438
1440
1442
1444
Fig. 8. Latency under different bandwidth allocation.
Fig. 7 illustrates the convergence of the proposed Algorithm
1 under various privacy constraints. The results clearly show
that the rewards converge within 500 episodes across differ-
ent constraints, highlighting the effectiveness of Algorithm
1. Meanwhile, the convergence points of the rewards vary
depending on the œµ value, underscoring the effectiveness of our
designed algorithm. For instance, with œµ = 0.001, the reward
converges to approximately ‚àí45, whereas with œµ = 0.0001, it
converges to about ‚àí250, demonstrating the significant impact
of privacy constraints on model training.
Fig. 8 presents the latency under different bandwidth alloca-
tions in MNIST datasets. It is evident that latency decreases for
all schemes as the available bandwidth increases. This is rea-
sonable since more bandwidth leads to a higher transmission
rate, thereby reducing communication latency. Additionally,
the proposed SFL-GA achieves the lowest latency for given
bandwidth budgets compared to the benchmarks, including
FL, traditional SFL, and PSL. In particular, the latency of
our framework is significantly lower than that of both SFL
and PSL. This reduction results from the proposed gradient
aggregation scheme, in which the aggregated gradients of the
smashed data are broadcast to all clients. Moreover, it is worth
noting that the latency of SFL is slightly higher than that of
PSL. This is due to the additional communication overhead in
SFL, as it requires client-side model aggregation updates in
each communication round.
VI. CONCLUSION
In this work, we proposed an SFL-GA framework to im-
prove both communication and computation efficiency for
traditional SFL. Specifically, the framework enabled dynamic
model cutting point selection by considering wireless network
environment, privacy constraints, and computational burden.
Given the cutting point, the gradients of smashed data were
aggregated before broadcasting, effectively reducing commu-
nication overhead. We theoretically analyzed the impact of
the cutting point selection on convergence performance of the
proposed SFL-GA framework. Furthermore, we formulated an
optimization problem to further enhance the communication-
and-computation efficiency of the framework, considering the
model convergence rate, latency, as well as privacy leakage.
To deal with the problem, an efficient joint CCC strategy was
designed by integrating the DDQN algorithm and optimization
method. Extensive simulation results were provided to verify
the effectiveness of the proposed framework and demonstrate
the efficiency of the proposed algorithm.
APPENDIX A
PROOF OF LEMMA 1
To prove Lemma 1, we first derive the following auxiliary
variables Averaged Mini-batch Gradient Of SFL-GA, as
¬Øgn
t =
¬Øgs,n
t
¬Øgc
t

=
 1
œÑ
PœÑ
i=1 gs,n
t,i
1
œÑ
PœÑ
i=1 gc
t,i

.
(41)
Meanwhile, we can expressed the Averaged Full-batch
Gradient of SFL-GA as
¬Øhn
t =
¬Øhs,n
t¬Øhc
t

=
 1
œÑ
PœÑ
i=1 ‚àáwsF
 wn
t,i

1
œÑ
PœÑ
i=1 ‚àáwc ÀúF
 wt,i


.
(42)
Moreover, the Averaged Full-batch Gradient of SFL can be
written as
‚àá¬ØF(wn
t ) =
‚àá¬ØF(ws,n
t
)
‚àá¬ØF(wc
t)

=
 1
œÑ
PœÑ
i=1 ‚àáwsF
 wn
t,i

1
œÑ
PœÑ
i=1 ‚àáwcF
 wn
t,i


. (43)
Then, the update of the global model between two consec-
utive adjacent rounds is formulated as
wt+1 ‚àíwt =
N
X
n=1
œÅn 
wn
t+1 ‚àíwn
t

= ‚àíŒ∑œÑ
N
X
n=1
œÅn¬Øgn
t . (44)
According to the assumption of L-smoothness, the im-


--- Page 12 ---
12
provement on the global loss can be expressed as
E (F (wt+1) ‚àíF (wt))
(a)
‚â§‚àíŒ∑œÑ
*
‚àáF(wt),
N
X
n=1
œÅn¬Øhn
t
+
+ LŒ∑2œÑ 2E
Ô£Æ
Ô£∞

N
X
n=1
œÅn  ¬Øgn
t ‚àí¬Øhn
t


2
+

N
X
n=1
œÅn¬Øhn
t

2Ô£π
Ô£ª
(b)
= ‚àíŒ∑œÑ
2 ‚à•‚àáF(wt)‚à•2 + LŒ∑2œÑ 2E
Ô£Æ
Ô£∞

N
X
n=1
œÅn  ¬Øgn
t ‚àí¬Øhn
t


2Ô£π
Ô£ª
+Œ∑œÑ
2
‚àáF(wt)‚àí
N
X
n=1
œÅn¬Øhn
t

2
+Œ∑œÑ (2LŒ∑œÑ ‚àí1)
2

N
X
n=1
œÅn¬Øhn
t

2
(c)
‚â§‚àíŒ∑œÑ
2 ‚à•‚àáF(wt)‚à•2 + Œ∑œÑ
2 E
Ô£Æ
Ô£∞
‚àáF(wt) ‚àí
N
X
n=1
œÅn¬Øhn
t

2Ô£π
Ô£ª
+ LŒ∑2œÑ 2E
Ô£Æ
Ô£∞

N
X
n=1
œÅn  ¬Øgn
t ‚àí¬Øhn
t


2Ô£π
Ô£ª
(d)
= ‚àíŒ∑œÑ
2 ‚à•‚àáF(wt)‚à•2 + Œ∑œÑ
2 E
Ô£Æ
Ô£∞
‚àáF(wt) ‚àí
N
X
n=1
œÅn¬Øhn
t

2Ô£π
Ô£ª
+ LŒ∑2œÑ 2
N
X
n=1
(œÅn)2 1
œÑ 2
œÑ
X
i=1
E
h
‚à•(gn
t ‚àíhn
t )‚à•2i
(e)
‚â§‚àíŒ∑œÑ
2 ‚à•‚àáF(wt)‚à•2 + LŒ∑2œÑœÉ2
N
X
n=1
(œÅn)2
+ Œ∑œÑ
2
N
X
n=1
œÅn E

‚à•‚àáF(wt) ‚àí¬Øhn
t ‚à•2
|
{z
}
A1
,
(45)
where (a) results from the facts that E
¬Øgn
t ‚àí¬Øhn
t

=
1
œÑ
PœÑ
i=1 E [gn
t ‚àíhn
t ] = 0 and ‚à•a + b‚à•2 ‚â§2‚à•a‚à•2 + 2‚à•b‚à•2. (b)
comes from the fact that 2 ‚ü®a, b‚ü©= ‚à•a‚à•2 + ‚à•b‚à•2 ‚àí‚à•a ‚àíb‚à•2.
(c) is hold when Œ∑ ‚â§
1
2LœÑ . (d) comes form the fact that
E
hD
¬Øgi
t ‚àí¬Øhi
t, ¬Øgj
t ‚àí¬Øhj
t
Ei
= 0, ‚àÄi Ã∏= j. (e) is achieved due to
the Jensen Inequality.
To find the upper bound of (45), we applied the inequality
of ‚à•a + b‚à•2 ‚â§2‚à•a‚à•2 + 2‚à•b‚à•2 on A1 that
A1 = E
h ‚àáF(wt) ‚àí‚àá¬ØF(wn
t )

+
 ‚àá¬ØF(wn
t ) ‚àí¬Øhn
t
2i
‚â§2E
h ‚àáF(wt) ‚àí‚àá¬ØF(wn
t )
2 +
 ‚àá¬ØF(wn
t ) ‚àí¬Øhn
t
2i
(e)
‚â§2
œÑ
œÑ
X
i=1
E
‚àáF(wt)‚àí‚àáF(wn
t,i)
2+
‚àáF(wn
t,i)‚àíhn
t,i
2
(f)
‚â§2
œÑ
œÑ
X
i=1
E
 ‚àáF(wt) ‚àí‚àáF(wn
t,i)
2 + 2Œì (œït (v))
(g)
‚â§2L2
œÑ
œÑ
X
i=1
E
wt ‚àí¬Øwn
t,i
2
|
{z
}
A2
+2Œì (œït (v)) ,
(46)
where (e) comes form the Jensen‚Äôs inequality, and (f) is
achieved due to Assumption 4. (g) follows the Lipschitz-
smooth property. ¬Øwn
t,i is the model of client n obtained after
the i-th local update based on the SFL gradient ‚àáF(wn
t,i; Œæ).
Similar with (46), we further find the upper bound of A2 in
(46) as
A2 = Œ∑2E

i
X
j=1
‚àáF
 wn
t,i; Œæ


2
‚â§2Œ∑2E

i
X
j=1

‚àáF
 wn
0,i; Œæ

‚àí‚àáF
 wn
t,i


2
+ 2Œ∑2E

i
X
j=1
‚àáF
 wn
t,i


2
(h)
‚â§2iŒ∑2E
Ô£Æ
Ô£∞
i
X
j=1
‚àáF
 wn
t,j; Œæ

‚àí‚àáF
 wn
t,j
2+
‚àáF
 wn
t,j
2
Ô£π
Ô£ª
(i)
‚â§2iŒ∑2
i
X
j=1
œÉ2 + 2Œ∑2i
œÑ
X
j=1
E
‚àáF
 wn
t,j
2 ,
(47)
where (h) results from Cauchy‚ÄìSchwarz inequality. (i) is hold
from Assumption 2. Applying PœÑ
i=1 i =
œÑ(œÑ‚àí1)
2
, we can
obtain
œÑ
X
i=1
E
wn
t ‚àíwn
t,i
2 ‚â§Œ∑2œÑ(œÑ ‚àí1)
Ô£´
Ô£≠œÉ2+
œÑ
X
j=1
E
‚àáF
 wn
t,j
2
Ô£∂
Ô£∏
‚â§Œ∑2œÑ (œÑ ‚àí1)
Ô£´
Ô£≠œÉ2+ 2L2
œÑ
X
j=1
E
 ¬Øwn
t,j‚àíwt
2+2
œÑ
X
j=1
‚à•‚àáF(wt)‚à•2
Ô£∂
Ô£∏
= Œ∑2œÑ (œÑ ‚àí1) œÉ2 + 2Œ∑2œÑ (2) (œÑ ‚àí1) ‚à•‚àáF(wt)‚à•2
+ 2L2Œ∑2œÑ (œÑ ‚àí1)
œÑ
X
j=1
E
 ¬Øwn
t,j ‚àíwt
2 .
(48)
By rearranging (48), we have
œÑ
X
i=1
‚à•wt ‚àí¬Øwt,i‚à•2 ‚â§
Œ∑2œÉ2œÑ (œÑ ‚àí1)
1 ‚àí2L2Œ∑2œÑ (œÑ ‚àí1)
+
2Œ∑2œÑ 2 (œÑ ‚àí1)
1 ‚àí2L2Œ∑2œÑ (œÑ ‚àí1) ‚à•‚àáF(wt)‚à•2
= Œ∑2œÉ2œÑ (œÑ ‚àí1)
1 ‚àíA
+
A
1 ‚àíA ‚à•‚àáF(wt)‚à•2
(49)
where A = 2L2Œ∑2œÑ (œÑ ‚àí1).
As a result, A1 in (46) is bounded by
A1 ‚â§2L2Œ∑2œÉ2 (œÑ ‚àí1)
1 ‚àíA
+
2A
1 ‚àíA ‚à•‚àáF(wt)‚à•2 + 2Œì (œït (v))
(j)
‚â§5
2L2Œ∑2œÉ2 (œÑ ‚àí1) + 1
2 ‚à•‚àáF(wt)‚à•2 + 2Œì (œït (v)) ,
(50)
where (j) results from the fact that A ‚â§1
5.


--- Page 13 ---
13
Plug (50) back into (45), we have
E (F (wt+1) ‚àíF (wt)) ‚â§‚àíŒ∑œÑ
2 ‚à•‚àáF(wt)‚à•2+LŒ∑2œÑœÉ2
N
X
n=1
(œÅn)2
+ Œ∑œÑ
2
N
X
n=1
œÅn
5
2L2Œ∑2œÉ2 (œÑ ‚àí1)+ 1
2 ‚à•‚àáF(wt)‚à•2+ 2Œì (œït (v))

= ‚àíŒ∑œÑ
4 ‚à•‚àáF(wt)‚à•2 + 5L2Œ∑3œÉ2œÑ (œÑ ‚àí1)
4
+ LŒ∑2œÑœÉ2
N
X
n=1
(œÅn)2 + Œ∑œÑŒì (œït (v)) .
(51)
This completes the proof.
REFERENCES
[1] K. B. Letaief, Y. Shi, J. Lu and J. Lu, ‚ÄúEdge artificial intelligence for
6G: Vision, enabling technologies, and applications,‚Äù IEEE J. Sel. Areas
Commun., vol. 40, no. 1, pp. 5-36, Jan. 2022.
[2] K. B. Letaief, W. Chen, Y. Shi, J. Zhang and Y. -J. A. Zhang, ‚ÄúThe
roadmap to 6G: AI empowered wireless networks,‚Äù IEEE Commun. Mag.,
vol. 57, no. 8, pp. 84-90, Aug. 2019.
[3] W. Saad, M. Bennis and M. Chen, ‚ÄúA vision of 6G wireless systems:
Applications, trends, technologies, and open research problems,‚Äù IEEE
Network, vol. 34, no. 3, pp. 134-142, May/Jun. 2020.
[4] Y. Shi, K. Yang, T. Jiang, J. Zhang and K. B. Letaief, ‚ÄúCommunication-
efficient edge AI: Algorithms and systems,‚Äù IEEE Commun. Surv. Tut.,
vol. 22, no. 4, pp. 2167-2191, Fourthquarter 2020.
[5] T. Li, A. K. Sahu, A. Talwalkar and V. Smith, ‚ÄúFederated learning:
challenges, methods, and future directions,‚Äù IEEE Signal Processing
Mag., vol. 37, no. 3, pp. 50-60, May 2020.
[6] Y.
Liang,
Q.
Chen,
G.
Zhu,
H.
Jiang,
Y.
C.
Eldar
and
S.
Cui,
‚ÄúCommunication-and-energy
efficient
over-the-air
federated
learning,‚Äù
IEEE
Trans.
Wireless
Commun.,
early
access,
doi:
10.1109/TWC.2024.3501297.
[7] Q. Chen, X. Xu, Z. You, H. Jiang, J. Zhang and F. -Y. Wang,
‚ÄúCommunication-efficient federated edge learning for NR-U-based IIoT
networks,‚Äù IEEE Internet Things J., vol. 9, no. 14, pp. 12450-12459, Jul.
2022.
[8] Q. Chen, H. Cheng, Y. Liang, G. Zhu, M. Li and H. Jiang, ‚ÄúTinyFEL:
Communication, Computation, and Memory Efficient Tiny Federated
Edge Learning via Model Sparse Update,‚Äù IEEE Internet Things J., early
access, doi: 10.1109/JIOT.2024.3499375.
[9] Vepakomma, P., Gupta, O., Swedish, T., Raskar, R., ‚ÄúSplit learning
for health: Distributed deep learning without sharing raw patient data,‚Äù
arXiv:1812.00564, 2018. Available: http://arxiv.org/abs/1812.00564.
[10] H. Hafi, B. Brik, P. A. Frangoudis, A. Ksentini and M. Bagaa, ‚ÄúSplit
Federated Learning for 6G Enabled-Networks: Requirements, Challenges,
and Future Directions,‚Äù IEEE Access, vol. 12, pp. 9890-9930, 2024.
[11] C. Thapa, M.A.P. Chamikara, S. Camtepe, and L. Sun, ‚ÄúSplitFed: When
federated learning meets split learning,‚Äù in
Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 36 No. 8, 2022, pp: 8485-8493.
[12] Z. H. Kafshgari, C. Shiranthika, P. Saeedi and I. V. Baji¬¥c, ‚ÄúQuality-
adaptive split-federated learning for segmenting medical images with
inaccurate annotations,‚Äù IEEE Intern. Sympos. Biomedical Imaging (ISBI),
Cartagena, Colombia, 2023, pp. 1-5.
[13] C. Xu, J. Li, Y. Liu, Y. Ling and M. Wen, ‚ÄúAccelerating split federated
learning over wireless communication networks,‚Äù IEEE Trans. Wireless
Commun., vol. 23, no. 6, pp. 5587-5599, Jun. 2024.
[14] D. Waref and M. Salem, ‚ÄúSplit federated learning for emotion detection,‚Äù
2022 4th Novel Intell. Leading Emerging Sci. Conf. (NILES), Giza, Egypt,
2022, pp. 112-115.
[15] W. Wu, M. Li, K. Q, C. Zhou, X. Shen, W. Zhuang, X. Li, and W.
Shi, ‚ÄúSplit learning over wireless networks: Parallel design and resource
management,‚Äù IEEE J. Sel. Areas Commun., vol. 41, no. 4, pp. 1051-1066,
Apr. 2023.
[16] J. Guo, C. Xu, Y. Ling, Y. Liu and Q. Yu, ‚ÄúLatency Minimization for
Split Federated Learning,‚Äù IEEE Veh. Techn. Conf. (VTC2023-Fall), Hong
Kong, 2023, pp. 1-6.
[17] J. Shen, X. Wang, N. Cheng, L. Ma, C. Zhou and Y. Zhang, ‚ÄúEffectively
heterogeneous federated learning: A pairing and split learning based
approach,‚Äù IEEE Global Commun. Conf. (GLOBECOM), Kuala Lumpur,
Malaysia, 2023, pp. 5847-5852.
[18] Y. Mu and C. Shen, ‚ÄúCommunication and storage efficient federated
split learning,‚Äù IEEE Int. Conf. Commun. (ICC), Rome, Italy, 2023, pp.
2976-2981.
[19] Z. Zhang, A. Pinto, V. Turina, F. Esposito and I. Matta, ‚ÄúPrivacy and
efficiency of communications in federated split learning,‚Äù IEEE Trans.
Big Data, vol. 9, no. 5, pp. 1380-1391, Oct. 2023.
[20] J. Lee, M. Seif, J.n Cho and H. V. Poor, ‚ÄúExploring the privacy-energy
consumption tradeoff for split federated learning,‚Äù IEEE Network, IEEE
Network, vol. 38, no. 6, pp. 388-395, Nov. 2024.
[21] Y. Liao, Y. Xu, H. Xu, L. Wang, Z. Yao and C. Qiao, ‚ÄúMergeSFL: Split
Federated Learning with Feature Merging and Batch Size Regulation,‚Äù
IEEE Int. Conf. Data Engineer. (ICDE), Utrecht, Netherlands, 2024, pp.
2054-2067.
[22] J. Jeon and J. Kim, ‚ÄúPrivacy-sensitive parallel split learning,‚Äù Int. Conf.
Inform. Networking (ICOIN), Barcelona, Spain, 2020, pp. 7-9.
[23] Z. Lin, G. Zhu, Y. Deng, X. Chen, Y. Gao, K. Huang and Y. Fang,
‚ÄúEfficient parallel split learning over resource-constrained wireless edge
networks,‚Äù IEEE Trans. Mobile Comput., vol. 23, no. 10, pp. 9224-9239,
Oct. 2024.
[24] M. Kim, A. DeRieux and W. Saad, ‚ÄúA bargaining game for personalized,
energy efficient split learning over wireless networks,‚Äù IEEE Wireless
Commun. Network. Conf. (WCNC), Glasgow, United Kingdom, 2023, pp.
1-6.
[25] D. Han, H. Bhatti, J. Lee and J. Moon, ‚ÄúAccelerating federated learning
with split learning on locally generated losses,‚Äù Int. Conf. Machine Learn.
workshop, virtual, 2021, pp. 1-12.
[26] X. Zhang, X. Zhou, M. Lin, and J. Sun, ‚ÄúShuffleNet: An extremely
efficient convolutional neural network for mobile devices,‚Äù in Proc.
IEEE/CVF Conf. Comput. Vis. Pattern Recognit.(CVPR), Salt Lake City,
UT, USA, Jun. 2018, pp. 6848‚Äì6856.
[27] Q. Zeng, Y. Du, K. Huang, and K. K. Leung, ‚ÄúEnergy-efficient re-
source management for federated edge learning with CPU-GPU hetero-
geneous computing,‚Äù IEEE Trans. Wireless Commun., vol. 20, no. 12, pp.
7947‚Äì7962, Dec. 2021.
[28] M. Fredrikson, S. Jha, and T. Ristenpart, ‚ÄúModel inversion attacks that
exploit confidence information and basic countermeasures,‚Äù in Proc. ACM
Conf. Comput. Commun. Security, New York, USA, Oct. 2015, pp:1322-
1333.
[29] J. Kim, S. Shin, Y. Yu, J. Lee, and K. Lee. ‚ÄúMultiple classification with
split learning,‚Äù Int. Conf. Smart Media Applications, New York, USA,
2020, pp:358‚Äì363.
[30] J. Pei, P. Hong, M. Pan, J. Liu and J. Zhou, ‚ÄúOptimal VNF placement
via deep reinforcement learning in SDN/NFV-enabled networks,‚Äù IEEE
J. Sel. Areas Commun., vol. 38, no. 2, pp. 263-278, Feb. 2020.
[31] L. Huang, S. Bi and Y. -J. A. Zhang, ‚ÄúDeep reinforcement learning
for online computation offloading in wireless powered mobile-edge
computing networks,‚Äù IEEE Trans. Mobile Comput., vol. 19, no. 11, pp.
2581-2593, 1 Nov. 2020.
[32] R. Zhang, K. Xiong, Y. Lu, B. Gao, P. Fan and K. B. Letaief, ‚ÄúJoint
coordinated beamforming and power splitting ratio optimization in MU-
MISO SWIPT-enabled hetNets: A multi-agent DDQN-based approach,‚Äù
IEEE J. Sel. Areas Commun., vol. 40, no. 2, pp. 677-693, Feb. 2022.
[33] B.
McMahan,
E.Moore,
D.
Ramage,
S.
Hampson,
and
B.
A.
yArcas,‚ÄúCommunication-efficient learning of deep networks from decen-
tralized data,‚Äù in Proc. Int. Conf. Artif. Intell. Stat. (AISTATS), Lauderdale,
FL, USA, 2017, pp.1273‚Äì1282.
[34] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge, U.K.:
Cambridge Univ. Press, 2014.
[35] W. Zhang, D. Yang, W. Wu, H. Peng, N. Zhang, H. Zhang, and X. Shen,
‚ÄúOptimizing federated learning in distributed industrial IoT: A multi-agent
approach,‚Äù IEEE J. Sel. Areas Commun., vol. 39, no. 12, pp. 3688-3703,
Dec. 2021.
[36] H. Xing, Y. Liu, A. Nallanathan, Z. Ding and H. V. Poor, ‚ÄúOptimal
throughput fairness tradeoffs for downlink Non-orthogonal multiple ac-
cess over fading channels,‚Äù IEEE Trans. Wireless Commun., vol. 17, no.
6, pp. 3556-3571, Jun. 2018.
