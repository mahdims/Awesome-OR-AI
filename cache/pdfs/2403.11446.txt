--- Page 1 ---
LLM Guided Evolution - The Automation of Models Advancing
Models
Clint Morris, Michael Jurado, and Jason Zutty
clintmaxmorris@gmail.com,michael.jurado@gtri.gatech.edu,jason.zutty@gtri.gatech.edu
Georgia Tech Research Institute
Atlanta, Georgia, USA
ABSTRACT
In the realm of machine learning, traditional model development
and automated approaches like AutoML typically rely on layers of
abstraction, such as tree-based or Cartesian genetic programming.
Our study introduces "Guided Evolution" (GE), a novel framework
that diverges from these methods by utilizing Large Language Mod-
els (LLMs) to directly modify code. GE leverages LLMs for a more
intelligent, supervised evolutionary process, guiding mutations and
crossovers. Our unique "Evolution of Thought" (EoT) technique
further enhances GE by enabling LLMs to reflect on and learn from
the outcomes of previous mutations. This results in a self-sustaining
feedback loop that augments decision-making in model evolution.
GE maintains genetic diversity, crucial for evolutionary algorithms,
by leveraging LLMsâ€™ capability to generate diverse responses from
expertly crafted prompts and modulate model temperature. This
not only accelerates the evolution process but also injects expert
like creativity and insight into the process. Our application of GE in
evolving the ExquisiteNetV2 model demonstrates its efficacy: the
LLM-driven GE autonomously produced variants with improved
accuracy, increasing from 92.52% to 93.34%, without compromising
model compactness. This underscores the potential of LLMs to ac-
celerate the traditional model design pipeline, enabling models to
autonomously evolve and enhance their own designs.
CCS CONCEPTS
â€¢ Computing methodologies â†’Search methodologies; Learn-
ing from critiques; Natural language generation.
KEYWORDS
Large Language Models, Automated Machine Learning, Evolution-
ary Algorithms
ACM Reference Format:
Clint Morris, Michael Jurado, and Jason Zutty. 2024. LLM Guided Evolution
- The Automation of Models Advancing Models. In Proceedings of Genetic
Evolutionary Computing Conference (GECCO â€™24). ACM, New York, NY, USA,
9 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
GECCO â€™24, July 14â€“18, 2018, Melbourne, Australia
Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM
https://doi.org/10.1145/nnnnnnn.nnnnnnn
1
INTRODUCTION
In the ever-evolving domain of machine learning, the convergence
of human cognitive skills and automated algorithms is entering a
pivotal junction. This paper introduces â€œGuided Evolutionâ€ (GE), a
novel framework that combines the human-like expertise of Large
Language Models (LLMs) with the robust capabilities of Neural
Architecture Search (NAS) through genetic algorithms. This inno-
vative fusion advances automated machine learning, elevating tra-
ditional NAS by integrating a more insightful, intelligently guided
evolutionary process.
Central to this framework is our â€œEvolution of Thoughtâ€ (EoT)
technique, which extends and refines concepts like Zero-Shot Chain-
of-Thought, Automated Chain-of-Thought, and Tree-of-Thought
[7, 17, 18]. These methodologies aim to improve the reasoning capa-
bilities of LLMs. EoT takes a unique step forward by enabling LLMs
to receive result-driven feedback, empowering them to make in-
formed improvements based on the performance of their prior code
augmentations, a significant advancement in intelligent automated
machine learning.
EoT catalyzes LLMs to introspect and fine-tune suggestions based
on past iterations, creating a self-enhancing feedback loop that
fine-tunes architectural evolution. At the same, GE maintains es-
sential genetic diversity for evolutionary algorithms while injecting
human-like expertise and creativity into the evolutionary frame-
work. Building from the insights of Ma et al. [11], our Guided Evo-
lutionary framework is further enhanced by a Character Role Play
(CRP) technique, to markedly increase the feasibility, usefulness
and creativity of ideas engendered by the LLM.
The effectiveness of the Guided Evolution (GE) framework is
showcased in the evolution of the ExquisiteNetV2 model. This evo-
lution, initiated with a State-Of-The-Art (SOTA) seed model, not
only demonstrates the capacity of LLMs to build upon and enhance
SOTA models in collaboration with human expertise but also under-
scores their autonomous model design. This case study illustrates
the frameworkâ€™s self-sufficient ability to generate improved model
variants, emphasizing the burgeoning impact of LLMs in redefin-
ing traditional model design pipelines, a step towards models that
independently evolve and refine their architectures.
2
NEURAL ARCHITECTURE SEARCH
Neural Architecture Search (NAS) stands at the forefront of ma-
chine learning innovation, focusing on the automated discovery of
optimal neural network architectures. Within this dynamic field, a
variety of methodologies have emerged, each contributing unique
approaches and benefits. These include Reinforcement Learning
(RL), Evolutionary Algorithms (EAs), Surrogate Model-Based Opti-
mization, and One-Shot Architecture Search.
arXiv:2403.11446v1  [cs.NE]  18 Mar 2024


--- Page 2 ---
GECCO â€™24, July 14â€“18, 2018, Melbourne, Australia
Clint Morris, Michael Jurado, and Jason Zutty
Focusing first on Reinforcement Learning, important contribu-
tions by Zoph and Le [21] and Baker et al. [1] have showcased how
an RL agent can be used to iteratively optimize network architec-
tures, with an emphasis on maximizing key performance metrics
like validation accuracy. Building on this foundation, Ramachan-
dran, Zoph, and Le expanded the RL [14] application to include the
search for activation functions, furthering the versatility of this
approach.
In a distinct but equally innovative vein, Evolutionary Algo-
rithms draw inspiration from the principles of biological evolution.
They utilize mutation and crossover processes to evolve network
architectures. The potential of neuroevolution in NAS was aptly
demonstrated by Real et al. [15], creating the first evolved model to
surpass hand-designs on ImageNet. Complementing this, CoDeep-
NEAT, developed by Miikkulainen et al. [12], extends the NEAT
algorithm to deep learning, co-evolving modules and blueprints for
constructing deep neural networks, thus highlighting the adapt-
ability of evolutionary approaches in NAS. Furthermore, Lu et al.
[10] introduced NSGA-NET, a multi-objective genetic algorithm,
signifying a substantial leap forward in this domain.
Surrogate Model-Based Optimization diverges from RLâ€™s trial-
and-error approach by employing predictive models to estimate
the performance of various architectures to better navigate the
architecture space Cai et al. [2] .
Lastly, the realm of One-Shot Architecture Search represents
a paradigm shift in NAS. It involves training an expansive super
network that encompasses all potential architectures within the
search space. Notable contributions in this area include the Efficient
Neural Architecture Search (ENAS) by Pham et al. [13], and the
Differentiable Architecture Search (DARTS) by Liu, Simonyan, and
Yang [9]. By leveraging shared weights across architectures, these
methods significantly reduce computational requirements.
In the rapidly evolving field of machine learning, our research
introduces novel methodologies that synergize evolutionary al-
gorithms (EA) with the reasoning and generative capabilities of
LLMs. We present "Guided Evolution" and "Evolution of Thought"
(EoT), innovative approaches designed to address the inherent inef-
ficiencies in traditional EA, a challenge highlighted in the works by
Guariso et al. [4] and Yang et al. [16]. These inefficiencies become
particularly pronounced with the increasing size and training costs
of modern machine learning models, necessitating a novel approach
to enhance efficiency and adaptability.
Our methodologies integrate LLMs within the genetic algorithm
framework, thereby introducing a robust intrinsic feedback mecha-
nism (EoT) and an intelligently supervised evolution, where muta-
tions and genetic crossovers are chosen by the LLMs framework.
This integration not only alleviates the inefficiency issues but also
significantly reduces the manual intervention often required in
EA, such as parameter tuning and range setting. By leveraging the
domain expertise of LLMs, we achieve a dynamic, efficient, and
adaptable framework for exploring complex solution spaces in ma-
chine learning. Moreover, genetic diversity is preserved through
the LLMâ€™s capability to produce an unlimited number of responses
from a single prompt. This is achieved by adjusting the temperature
parameter and utilizing a range of hand crafted prompts, thereby
facilitating a thorough exploration of alternative model designs.
Traditional EA methods are often limited by their rigid muta-
tion and crossover structures, a constraint partially addressed by
developments such as those by Zutty et al. [22], who introduced
human-derived primitives. However, the utilization of predefined
building blocks and tree structures in traditional genetic program-
ming necessitates a prolonged set-up time. Rather, by generating
individuals directly as interpretable code, LLMs save time by au-
tomatically enriching the evolutionary process with contextual
understanding and nuanced insights which normally would come
from human-derived primitives. Moreover, they are not constrained
by a rigid structure; their development is limited only by the LLMâ€™s
creativity.
The prevalent issue with LLMs stems from their design to emu-
late, rather than authentically generate, human-like responses. This
often results in the production of erroneous or fabricated informa-
tion, a phenomenon known as "hallucination" [5]. Addressing this,
prompt engineering emerges as a method to navigate LLMs towards
structured, logical reasoning via carefully formulated prompts. Our
research significantly extends this nascent field. Additionally, an
advantage of evolution is in the natural experimentation that arises
during the process: a "hallucinated" piece of code will be evaluated
against supervised data, its objectives and fitness scores accurately
reflecting its success. Not every individual need be a winner as long
as the evolution continues to make steady progress.
Building on the foundational work of Kojima et al. [7] in Chain-
of-Thought (CoT) reasoning and the advancements by Zhang et al.
[18] in Auto-CoT, our study introduces the EoT approach. This
method incorporates genetic algorithms to enable LLMs to au-
tonomously curate and refine prompts. Detailed in Section 3.2.2,
EoT leverages selective evolutionary strategies to not only augment
the quality of LLM outputs but also to introduce a mechanism for
self-optimization. This approach marks a step forward in devel-
oping self-improving, adaptive language models, showcasing the
potential for more autonomous and efficient evolution in the field
of machine learning.
3
METHODOLOGY
In our methodology, we introduce the Guided Evolution (GE) frame-
work, initiating with ExquisiteNetV2 as the seed model. This model
is first dissected into discrete code blocks, each corresponding to a
distinct Python class. These blocks function analogously to genetic
segments in a genome, providing the foundational elements for our
LLM driven evolutionary process.
For this study, we utilized Mixtral [6], Mistral AIâ€™s 8x7B Mixture
of Experts Open Source Model, noted for its balance of efficiency
and high performance in code generation. Mixtral employs a Sparse
Mixture of Experts (SMoE) architecture, selectively utilizing 13B
out of 47B parameters across eight feedforward blocks for each
token, optimizing for inference efficiency. This model demonstrates
superior performance in code generation compared to competitors
like Llama 2 70B, making it particularly effective for the precise and
complex code modifications required in our Guided Evolutionary
framework.
In a departure from traditional evolutionary algorithms (EA),
our approach replaces conventional mutation and mating opera-
tions with a series of prompts directed at a LLM. This introduces


--- Page 3 ---
LLM Guided Evolution - The Automation of Models Advancing Models
GECCO â€™24, July 14â€“18, 2018, Melbourne, Australia
Gene Segment
Description
get_optimizer
Optimization Process
SE
Squeeze-and-Excitation
SE_LN
Squeeze-and-Excitation Layer-
Normalization
DFSEBV2
Feature-Processor
FCT
Feature-Concentrator
EVE
Extreme-Value-Expansion
ME
Max-Min Expansion
DW
Depthwise Convolution
ExquisiteNetV2
Aggregator
Table 1: Block Descriptions
a more dynamic and exploratory dimension to the EA framework.
To further encourage innovative architectural evolution, we uti-
lize "Character Role Play" (CRP), as outlined in Section 3.2.1. This
strategy enhances the creativity and unconventionality of guided
mutations.
Another key feature of our methodology is the EoT approach
(detailed in Section 3.2.2). This induces an intrinsic feedback mech-
anism within the EA framework, allowing it to adapt and respond
to effective changes in the model architecture. Through this mecha-
nism, the LLM becomes attuned to successful adaptations, guiding
the evolution of the model architecture in a more informed and
targeted manner.
This combination of LLM repeated prompting and the EoT method-
ology creates a dynamic environment for the guided evolution of
the seed model. It allows for both exploratory diversification and
the exploitation of effective architectural changes, providing a novel
approach to evolving neural network architectures.
By breaking down the ExquisiteNetV2 model into modular seg-
ments, such as the feature concentrator, we can direct the LLMâ€™s
capabilities more precisely. This focused approach ensures that
each segment is optimized in isolation, allowing for a detailed and
specific enhancement of the modelâ€™s overall architecture. Table
1 delineates the decomposition of the state-of-the-art model into
distinct code segments, providing a structured overview of its com-
ponents.
3.1
Mating
To enhance genome mating efficiency, the Guided Evolution (GE)
framework employs a strategic selection process. Figure 1 shows the
process of randomly selecting two distinct code segments from the
available genomes, ensuring they are not identical to avoid ineffec-
tual mating attempts. These selected segments are then processed
through a LLM. The LLMâ€™s task is to intelligently amalgamate these
segments, aiming to either heighten accuracy or boost efficiency in
the resultant genome.
Figure 1: LLM Driven Code Block Mating
3.2
Mutation
Incorporating a LLM into our mutation process has introduced a
higher degree of flexibility within the Guided Evolutionary frame-
work. Mutation here involves the random selection of code seg-
ments, which are then augmented through LLM prompts. To en-
hance the diversity of these mutations, we varied the model temper-
ature for each prompt between 0.05 and 0.4, and set the maximum
token length to a random value within the range of 600 to 1400
tokens.
Distinguishing features of our approach include the integration
of "Character Role Play" and "Evolution of Thought" (EoT), the
latter being akin to the AutoCoT methodology and introduced in
Section 3.2.2. "Character Role Play" broadens the exploration within
the potent areas of the LLMâ€™s latent space, fostering innovative
suggestions. EoT, on the other hand, introduces a feedback loop,
refining the LLMâ€™s output over successive iterations for progressive
improvement.
This methodology positions the GE framework not merely as
a tool for random mutation but as a strategic guide for the LLM.
It directs the exploration and enhancement of solutions in a more
focused and effective manner, as detailed in our discussion of EoT
in Section 3.2.2.
3.2.1
Character Role Play: In the Guided Evolution framework,
we approach the mutation process distinctively, diverging from
conventional methods. Our initial experiments utilizing a LLM for
code mutation identified a tendency towards producing standard
solutions, exemplified by the frequent selection of a 0.2 dropout
rate. This trend likely mirrors the commonality of such values in
the modelâ€™s training dataset, limiting the exploration of a more
diverse solution space.
To address this, we implemented a method that incorporates
some unconventional prompt templates, specifically designed to
prompt the LLM away from generating these typical solutions. This
method employs a range of randomly selected prompt templates,
some of which guide the LLM to generate more atypical or novel
code modifications. Descriptions of these prompt categories can be
found in Table 2
This approach increased the diversity and scope of exploration
in our preliminary tests, boosting the likelihood of developing
solutions that surpass current state-of-the-art models. To further
encourage diversity and enhance the quality of generated code, we
introduced an element of â€œexpertâ€ roleplay in our prompts. This
feature was intended to encourage the LLM to access its latent


--- Page 4 ---
GECCO â€™24, July 14â€“18, 2018, Melbourne, Australia
Clint Morris, Michael Jurado, and Jason Zutty
Category
Summary
Hyperparam.
Modify existing parameter values to alter the
modelâ€™s behavior.
Hyperparam.
Uncommon
Change parameter values to less conventional
ones to explore diverse outcomes.
Complex
Introduce advanced functionality that could
potentially enhance the modelâ€™s accuracy.
Reduce Model
Size
Streamline the code by reducing parameters
with minimal impact on the modelâ€™s perfor-
mance.
Uncommon
Implement a distinctive or rarely used en-
hancement to the model.
Significant
Execute substantial modifications to the code,
incorporating auxiliary functions for im-
proved structure and capability.
Table 2: Prompt Type Categories
Name
Summary Description
Expert
A top expert in machine learning with a deep
understanding of advanced AI methods.
Dr. MaGoo
An AI innovator with a serendipitous approach,
surprising peers with unorthodox model en-
hancements.
Innovative
Scientist
A globally famous AI researcher known for cre-
ative and unconventional techniques.
Table 3: Character Role Play
knowledge of â€œexpert-level codeâ€ by casting it as a skilled AI re-
searcher when queried. Namely, we incorporated a range of charac-
ter personas for the LLM to adopt, each designed to induce different
types and qualities of mutations.
Specifically, we choose three unique character roles which act as
compatible extensions to the previously mentioned 6 foundational
prompts. These character types were chosen after manual analy-
sis monitoring the quality of the produced models in response to
both the standard prompts and the character-augmented prompts.
Details regarding the three character roles, including their descrip-
tions, are delineated in Table 3.
Our evolutionary algorithm utilizes six fundamental prompt tem-
plates, compatible with three distinct "expert" character roles. By
combining these baseline prompts with the character role variations
that modify the base template, we achieve a total of twenty-four
diverse prompts. This variety in prompts fosters a broader and more
enriched exploration of potential solutions by the LLM.
In future studies, prompt templates and character implementa-
tion are something that could be co-evolved as opposed to arbitrarily
chosen.
3.2.2
Evolution of Thought: In 2022, Zhang et al [? ]. pioneered
the Auto Chain of Thought prompting (Auto-CoT), an innovative
approach that automates the construction of demonstrations for
LLMs. This method employs question clustering, where a dataset is
divided into several clusters of similar questions. For each cluster,
a representative question is selected, and its reasoning chain is
Figure 2: EoT prompt template
generated using Zero-Shot-CoT with simple heuristics. Thus Auto-
CoT addresses the limitations of manual and zero-shot CoT (Chain
of Thought) prompting by leveraging the diversity of questions
to mitigate the impact of errors in reasoning chains generated by
LLMs. The key innovation lies in utilizing clustering algorithms
to group similar questions, thus ensuring a variety of reasoning
types and improving the robustness of the modelâ€™s problem-solving
capabilities.
Our study introduces the "Evolution of Thought" (EoT) method-
ology, which extends the reasoning principles of both Auto-CoT
and Manual-CoT. What sets EoT apart is its incorporation of feed-
back from evolutionary processes within the Guided Evolution (GE)
framework. In this framework, each mutation subsequent to the
first generation can conduct a basic LLM mutation, â€œCharacter Role
Playâ€, or the EoT mutation. EoT uniquely adopts the SPEA-2 elite se-
lection algorithm to identify high-performing individuals from the
previous generation. These individuals are then used as exemplary
models for the LLM in subsequent mutations. This approach res-
onates with Zero-Shot-CoT through its â€œLet us think step by stepâ€
reasoning, and with Manual-CoT in its use of demonstrative exam-
ples. Leveraging the comparison of a selected elite block against
its un-evolved seed serves as a thought guide for the LLM and
encourages it to reflect about why mutations caused performance
gains. The LLM is then asked to apply these derived insights to
new code blocks, thus forming a performance enhancing feedback
mechanism across generations. The EoT template is detailed in
Figure 2.
Through analyzing empirical feedback on preceding concepts,
EoT is designed to mimic and accelerate human like software de-
velopment and invention.
3.3
Evolutionary Loop
This study introduces a novel approach in machine learning by
integrating LLM adaptations into the three key components of
evolutionary algorithms: genome creation, mutation, and mating.
Detailed discussions on the adaptations of mating and mutation
processes are presented in Section 3.1 and 3.2. The following section
section provides an overview of the entire evolutionary process,
which also covers how our method addresses gene creation.
In our evolutionary framework, genes are generated through
LLM-driven mutations applied to copies of the state-of-the-art seed
code. This gene creation method involves the seed code undergoing


--- Page 5 ---
LLM Guided Evolution - The Automation of Models Advancing Models
GECCO â€™24, July 14â€“18, 2018, Melbourne, Australia
our unique LLM mutation procedure, intentionally excluding EoT
mutations due to their reliance on previous change evaluations.
The evaluation process within this study is multi-objective, si-
multaneously considering accuracy on a holdout set and the modelâ€™s
parameter count. This dual-objective strategy enables the evolu-
tionary algorithm to effectively balance accuracy and model size,
optimizing both in tandem.
Elitism plays a crucial role in our approach due to the low likeli-
hood of recovering or reconverging to a lost individual. We maintain
elitism in the evolutionary loop through the Strength Pareto Evolu-
tionary Algorithm 2 (SPEA-2) developed by Zitzler et al [20]. SPEA-2
was selected for its ability to preserve top-performing solutions
while ensuring diversity. It achieves this balance via â€œfine-grained
fitness assignmentâ€ and â€œk-th nearest neighbor density estimationâ€
methods that enhance exploration and prevent premature conver-
gence by evaluating solutions based on dominance relationships
and density in the objective space.
In the context of selection for mating and mutation processes,
the study leverages the NSGA-II algorithm, as delineated by Deb et
al. [3]. This algorithm is characterized by its â€™fast non-dominated
sortingâ€™ technique coupled with â€™crowding distanceâ€™ assessments,
pivotal for curating a diverse yet superior pool of solutions. The
fast non-dominated sorting algorithm stratifies solutions by their
dominance levels, while the crowding distance metric evaluates the
populationâ€™s spatial density, preferentially guiding the selection
process towards sparser regions. Such a methodology ensures an
optimal equilibrium between the caliber and the variety of solutions,
a crucial factor for effective exploration within the multiobjective
optimization domain.
The integration of prompt variations significantly broadens the
effective model search space, limited only by the LLMâ€™s vast out-
put capabilities. This expansion greatly benefits the discovery of
unique and effective solutions, but it also necessitates the retention
of efficient individuals. To address this, before mutation and mating,
both SPEA-2 and NSGA-II selections are conducted on populations
of the same size. Post-augmentation and replacement through mat-
ing and mutation, the new individuals are concatenated with the
SPEA-2 selected genes. Elite genes are only replaced if the new
generation surpasses their performance. This strategy is employed
to mitigate issues arising from the considerably expansive search
space , ensuring the capture and retention of optimal solutions.
In our mutation process, a probabilistic method, directed by
"prob_eot" is used to alternate randomly between fixed prompt
mutation and "Evolution of Thought" (EoT). When fixed prompt
mutation is selected, the system chooses a single prompt template
from either traditional direction prompts or Character Role Play
prompts, as outlined in Section 3.2.1.
The developed evolutionary loop aims to optimally utilize the
explorative capabilities of LLMs while ensuring to exploit effective
adaptations.
3.4
Example Problem
In the pursuit of advancing neural architecture search methodolo-
gies, our investigation employed the CIFAR10 dataset, a seminal
dataset developed by the Canadian Institute for Advanced Research,
as a primary benchmark [8]. The dataset encompasses 60,000 32x32
Algorithm 1: LLM-Guided Evolution Algorithm
Function LLMGuidedEvolution(ğ‘†ğ‘’ğ‘’ğ‘‘ğ¶ğ‘œğ‘‘ğ‘’):
ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›â†InitializePopulation(ğ‘†ğ‘’ğ‘’ğ‘‘ğ¶ğ‘œğ‘‘ğ‘’)
ğ¸ğ‘™ğ‘–ğ‘¡ğ‘’ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›â†âˆ…
ğ»ğ‘ğ‘™ğ‘™ğ‘‚ğ‘“ğ¹ğ‘ğ‘šğ‘’â†âˆ…
while not ConvergenceCondition() do
ğ¸ğ‘£ğ‘ğ‘™ğ‘¢ğ‘ğ‘¡ğ‘’ğ‘‘ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›â†Evaluate(ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›)
ğ¸ğ‘™ğ‘–ğ‘¡ğ‘’ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ â†SPEA2(ğ¸ğ‘£ğ‘ğ‘™ğ‘¢ğ‘ğ‘¡ğ‘’ğ‘‘ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›)
ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ¹ğ‘œğ‘Ÿğ‘€ğ‘ğ‘¡ğ‘–ğ‘›ğ‘”â†
NSGA2(ğ¸ğ‘£ğ‘ğ‘™ğ‘¢ğ‘ğ‘¡ğ‘’ğ‘‘ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›)
ğ‘€ğ‘ğ‘¡ğ‘’ğ‘‘ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›â†
LLMMate(ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ¹ğ‘œğ‘Ÿğ‘€ğ‘ğ‘¡ğ‘–ğ‘›ğ‘”)
ğ‘€ğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘‘ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›â†
LLMMutate(ğ‘€ğ‘ğ‘¡ğ‘’ğ‘‘ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›)
ğ»ğ‘ğ‘™ğ‘™ğ‘‚ğ‘“ğ¹ğ‘ğ‘šğ‘’â†UpateHoF(ğ¸ğ‘£ğ‘ğ‘™ğ‘¢ğ‘ğ‘¡ğ‘’ğ‘‘ğ‘‚ğ‘“ğ‘“ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘”)
ğ¶ğ‘œğ‘šğ‘ğ‘–ğ‘›ğ‘’ğ‘‘ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›â†
Concatenate(ğ¸ğ‘™ğ‘–ğ‘¡ğ‘’ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ , ğ¸ğ‘£ğ‘ğ‘™ğ‘¢ğ‘ğ‘¡ğ‘’ğ‘‘ğ‘‚ğ‘“ğ‘“ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘›ğ‘”)
ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›â†ğ¶ğ‘œğ‘šğ‘ğ‘–ğ‘›ğ‘’ğ‘‘ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›
return ğ»ğ‘ğ‘™ğ‘™ğ‘‚ğ‘“ğ¹ğ‘ğ‘šğ‘’
Algorithm 2: Detailed LLM Mutation Process
Function LLMMutate(ğºğ‘’ğ‘›ğ‘’):
// Select a random code block Y from the
geneâ€™s alternative blocks
ğµğ‘™ğ‘œğ‘ğ‘˜ğ‘Œâ†SelectRandomBlock(ğºğ‘’ğ‘›ğ‘’)
// Probabilistic method to choose mutation
type
if Random() < prob_eot then
// Perform Fixed Prompt Mutation
ğ‘ƒğ‘Ÿğ‘œğ‘šğ‘ğ‘¡â†SelectRandomPrompt(FixedPrompts âˆª
RolePlayPrompts)
ğ‘€ğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘‘ğµğ‘™ğ‘œğ‘ğ‘˜ğ‘Œâ†
FixedPromptMutation(ğµğ‘™ğ‘œğ‘ğ‘˜ğ‘Œ, ğ‘ƒğ‘Ÿğ‘œğ‘šğ‘ğ‘¡)
else
// Perform Evolution of Thought (EoT)
ğ¸ğ‘™ğ‘–ğ‘¡ğ‘’â†SPEA2Selection(ğ¿ğ‘ğ‘ ğ‘¡ğºğ‘’ğ‘›ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘›,ğ‘˜)
ğ¸ğ‘™ğ‘–ğ‘¡ğ‘’ğ¼ğ‘›ğ‘‘ğ‘–ğ‘£ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™â†SelectRandomIndividual(ğ¸ğ‘™ğ‘–ğ‘¡ğ‘’)
(ğµğ‘™ğ‘œğ‘ğ‘˜ğ‘‹ğ¸ğ‘™ğ‘–ğ‘¡ğ‘’, ğµğ‘™ğ‘œğ‘ğ‘˜ğ‘‹ğ‘†ğ‘’ğ‘’ğ‘‘) â†
SelectRandomBlock(ğ¸ğ‘™ğ‘–ğ‘¡ğ‘’ğ¼ğ‘›ğ‘‘ğ‘–ğ‘£ğ‘–ğ‘‘ğ‘¢ğ‘ğ‘™âˆªğ‘†ğ‘’ğ‘’ğ‘‘ğ¶ğ‘œğ‘‘ğ‘’)
ğ‘€ğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘‘ğµğ‘™ğ‘œğ‘ğ‘˜ğ‘Œâ†
EoTMutation(ğµğ‘™ğ‘œğ‘ğ‘˜ğ‘Œ, ğµğ‘™ğ‘œğ‘ğ‘˜ğ‘‹ğ¸ğ‘™ğ‘–ğ‘¡ğ‘’, ğµğ‘™ğ‘œğ‘ğ‘˜ğ‘‹ğ‘†ğ‘’ğ‘’ğ‘‘)
// Replace the original block Y with the
mutated block Y
ğºğ‘’ğ‘›ğ‘’â†ReplaceBlock(ğºğ‘’ğ‘›ğ‘’, ğµğ‘™ğ‘œğ‘ğ‘˜ğ‘Œ, ğ‘€ğ‘¢ğ‘¡ğ‘ğ‘¡ğ‘’ğ‘‘ğµğ‘™ğ‘œğ‘ğ‘˜ğ‘Œ)
return ğºğ‘’ğ‘›ğ‘’
pixel color images across 10 classes, with a standard division of
50,000 images for training and 10,000 for testing.
Our rationale for selecting CIFAR10 was twofold: firstly, the chal-
lenge presented by its low-resolution images, which necessitates
sophisticated object recognition algorithms to address limited detail


--- Page 6 ---
GECCO â€™24, July 14â€“18, 2018, Melbourne, Australia
Clint Morris, Michael Jurado, and Jason Zutty
availability; and secondly, its extensive usage in the machine learn-
ing domain, which has resulted in a proliferation of highly effective
SOTA models to benchmark our auto-enhancing GE against.
CIFAR10â€™s balance of complexity and computational feasibility
makes it an ideal candidate for evaluating new neural architectures.
It demands intricate learning algorithms for accurate classification
while remaining manageable in terms of data handling and compu-
tational requirements, a crucial consideration for developing from
scratch a novel NAS methodology such as Guided Evolution.
Conclusively, the CIFAR10 datasetâ€™s historical significance in
machine learning, combined with its unique challenges and practi-
cal usability, positions it as an optimal choice for our research. It
not only serves as a fundamental benchmark but also as a catalyst
for designing a new and effective NAS paradigm in a demanding
and well-established domain.
3.5
State of the Art
Our study aimed to assess our frameworkâ€™s capacity to outper-
form human-engineered designs in well-researched domains. We
used a state-of-the-art CIFAR-10 classifier as our seed model for
autonomous evolution, chosen for its complex block and layer di-
versity, presenting a challenging environment for evolution.
A pivotal aspect of our methodology was the deliberate choice
of models with fewer parameters. This decision was twofold: firstly,
models with a lower parameter count emphasize the critical role of
architectural ingenuity. Secondly, such models are more conducive
to exhaustive experimentation within limited time frames, ensuring
thorough evaluation.
We opted for "ExquisiteNetV2", a model innovatively crafted by
Zhou and Su in 2022 [19], as our foundational model. ExquisiteNetV2
is not only compact, ranking ninth in terms of size on the CIFAR-10
benchmark, but its architecture also comprises a variety of effective
blocks. This aligns seamlessly with our goal of advancing sophisti-
cated architectures through our evolutionary framework.
ExquisiteNetV2 is characterized by several innovative compo-
nents. It incorporates Extreme-Value-Expansion (EVE) Blocks, which
concatenate min and max pooling layers to rapidly discard unimpor-
tant features while retaining critical ones. The Feature-Concentrator
Block integrates a 4x4 depthwise convolution with the EVE block
to focus on preserving raw image features. In an effort to reduce
parameter count, SE-Layer-Normalization (SE-LN) Blocks replace
fully-connected layers with Layer Normalization in a Squeeze Ex-
citation blocks to further reduce parameter count. The DFSEBV2
Block, a cornerstone of the ExquisiteNetV2 architecture, integrates
pointwise convolutions, batch normalization, and depthwise convo-
lutions for efficient feature processing. It uses SiLU and Hardswish
activation functions for advanced pattern learning, and includes
either an SE-LN or a standard SE block for improved channel-wise
feature recalibration. This blockâ€™s design is further enhanced by
residual connections, ensuring optimal learning efficiency and com-
putational economy. These elements make ExquisiteNetV2 a fitting
candidate to demonstrate the capabilities of our guided evolutionary
framework.
Figure 3: Metrics of evolved ExquisiteNetV2 models.
4
RESULTS
In this investigation, our primary objective was to leverage LLM
Guided Evolution for the generation of ExquisiteNetV2 variants,
with the dual goals of maintaining high test accuracy while con-
straining model size. Furthermore, this research endeavored to
evaluate the contributions of two novel methodologies: Evolution
of Thought (EoT) and Character Role Play (CRP), within the context
of this evolutionary framework.
The ensuing sections will delve into the specifics of our findings.
Section 4.0.1 will detail the performance metrics of the most effec-
tive models derived from this study. Subsequent to this, Section
4.0.2 will present an ablation study that explores the individual and
combined influences of the EoT and CRP methodologies. This latter
section aims to offer observations regarding their respective impact
on the GE framework.
4.0.1
Evolved Seeds: The experimental results from our Guided
Evolution approach are promising. The autonomous framework
has successfully evolved ExquisiteNetV2 variants, surpassing the
original modelâ€™s benchmarks of 92.52% accuracy and a parameter
count of 0.518230M.
A notable example is the "GE-Evolved-L" variant, achieving an
accuracy of 93.34% with a parameter count of 0.518230M, aligning
exactly with the original ExquisiteNetV2â€™s size. Figure 3 provides
a comparative illustration of this variant against the seed model,
illustrating the capacity of our GE framework to autonomously
enhance neural architectures with human-like expertise.
Moreover, significant progress was made in enhancing parame-
ter efficiency. For instance, the "GE-Evolved-M" variant achieved
a remarkable 43.1% reduction in parameter count compared to
ExquisiteNetV2, alongside an improved test accuracy of 93.16%.
This efficiency and performance are visually represented in Figure
3. Additionally, the "GE-Evolved-S" and "GE-Evolved-T" variants,
while slightly less accurate than the seed model at 88.83% and 87.45%
respectively, showcased a dramatic improvement in model size to
0.119254M and 0.039190M, illustrating a substantial advancement
in model compactness without significant accuracy trade-offs.
To illustrate the performance improvements achieved through
Guided Evolution, we compare ExquisiteNetV2â€™s enhanced perfor-
mance with other leading low-parameter CIFAR-10 classifiers in
Figure 4. This comparison provides a contextual understanding of
the advancements made by our autonomous framework. Looking


--- Page 7 ---
LLM Guided Evolution - The Automation of Models Advancing Models
GECCO â€™24, July 14â€“18, 2018, Melbourne, Australia
Figure 4: Evolved ExquisiteNetV2 models compared to State
of the Art.
Figure 5: Example of an augmented code block.
ahead, we plan to expand the application of the GE methodology
to a broader range of seed models in future research endeavors.
To illustrate the types of augmentations produced by the Genetic
Engineering (GE) process, Figure 5 showcases an example of an
augmented code block from an effective individual (GE-Evolved-M).
In addition to the notable performance improvements observed
in this study, the GE framework also demonstrated potential for
further advancement over longer evolutionary periods. This ex-
tended potential can likely be attributed to the vast array of design
pathways that the LLM-guided framework is capable of exploring.
It is important to highlight that the cutoff point of the evolu-
tionary process in this study was due to time constraints and not
because a significant convergence point was reached.
Figure 6: Pareto Frontier of Guided Evolution variants: Gen-
eration 9
4.0.2
Ablation Study: A pivotal aspect of our study was assessing
the impact of Evolution of Thought (EoT) and Character Role Play
on the Guided Evolution process. To this end, we conducted variant
experiments: one excluding EoT and another excluding both EoT
and Character Role Play. The results demonstrated that the incor-
poration of EoT and Character Role Play substantially influenced
the evolution trajectory. Notably, the inclusion of EoT accelerated
the convergence towards optimal solutions. This enhancement is
graphically represented through the comparison of Pareto frontiers
at generation 9, as illustrated in Figure 6.
To further elucidate the development trajectory of individuals
that conformed to our primary objectiveâ€”enhancing accuracy with-
out increasing the parameter count beyond that of ExquisiteNetV2â€”we
tracked the progression of the most accurate model at each genera-
tion within the three distinct Guided Evolution runs that maintained
the same or fewer parameters than ExquisiteNetV2. The models
that ultimately achieved the highest accuracy, while adhering to the
parameter constraints of ExquisiteNetV2, are highlighted in Figure
7. This figure depicts the influence of the EoT and Character Role
Play methodologies on the trajectory and efficiency of the Guided
Evolution process.


--- Page 8 ---
GECCO â€™24, July 14â€“18, 2018, Melbourne, Australia
Clint Morris, Michael Jurado, and Jason Zutty
Figure 7: Guided Evolution exclusionary study.
Our findings show that the GE framework quickly identified
model overfitting. Implementing various regularization methods
early on led to significant improvements. While crossover facili-
tated horizontal transfer of updates across gene blocks, the EoT
feedback mechanism enabled vertical propagation of these changes.
Thus, if adding regularization to an elite exampleâ€™s code block X
improved performance, EoT could extend these updates to another
genomeâ€™s block Y. Consequently, EoT ensures intrinsic feedback
and synchronicity across a genomeâ€™s disparate code blocks.
5
CONCLUSION
In this study, we introduced Guided Evolution (GE), a novel method-
ology for harnessing the power of Large Language Models (LLMs)
in advancing neural architectures. Central to GE are two innova-
tive concepts: Evolution of Thought (EoT) and Character Role Play
(CRP), which significantly contribute to the intelligent evolution
of model architectures and infuse diversity and creativity into the
evolutionary process, respectively.
Our findings, especially with models such as "GE-Evolved-L",
"GE-Evolved-M", "GE-Evolved-S" and "GE-Evolved-T" demonstrate
GEâ€™s potential in autonomously enhancing state-of-the-art neural
architectures. These outcomes underline the efficacy of EoT and
CRP within the GE framework.
Itâ€™s important to note that our ablation studies, conducted with
a single trial for each variant due to time constraints, serve as an
initial exploration. Future work is needed to fully assess the impact
of EoT and CRP within this algorithm framework.
Future research will aim to expand GE across various datasets
and domains, fully harnessing its capabilities in machine learning.
We will explore the impact of GE on the efficiency of Neural Archi-
tecture Search (NAS), the role of instructive commenting in code
gene sequences, and the effects of different LLM models on GEâ€™s
parameter space exploration.
In conclusion, our study lays a foundational framework for fu-
ture exploration in GE, with the anticipation that advancements
in NLP and LLM development will significantly enhance Guided
Evolutionâ€™s performance. This work opens new horizons for the
exploitation of LLMs in genetic algorithms and multi-objective opti-
mization, promising an exciting future for models that advance mod-
els. For access to our code, datasets, and supplementary materials,
visit our GitHub repository: https://github.com/clint-kristopher-
morris/llm-guided-evolution. We invite the community to explore,
replicate, and contribute to GEâ€™s development.
6
ACKNOWLEDGEMENTS
We thank Aaron McDaniel for his expert guidance on genetic algo-
rithms, which was pivotal to our researchâ€™s success. His insights
significantly enriched our approach, demonstrating both depth and
practical applicability.
REFERENCES
[1] Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. 2017. Acceler-
ating neural architecture search using performance prediction. arXiv preprint
arXiv:1705.10823 (2017).
[2] Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, and Yong Yu. 2018. Path-
level network transformation for efficient architecture search. In International
Conference on Machine Learning. PMLR, 678â€“687.
[3] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan. 2002. A fast and elitist multiobjec-
tive genetic algorithm: NSGA-II. IEEE Transactions on Evolutionary Computation
6, 2 (2002), 182â€“197. https://doi.org/10.1109/4235.996017
[4] Giorgio Guariso and Matteo Sangiorgio. 2020. Improving the performance of
multiobjective genetic algorithms: An elitism-based approach. Information 11,
12 (2020), 587.
[5] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian
Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting
Liu. 2023. A Survey on Hallucination in Large Language Models: Principles,
Taxonomy, Challenges, and Open Questions. arXiv:2311.05232 [cs.CL]
[6] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou
Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample,
LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep
Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet,
Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, and William El Sayed. 2024.
Mixtral of Experts. arXiv:2401.04088 [cs.LG]
[7] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and
Yusuke Iwasawa. 2023.
Large Language Models are Zero-Shot Reasoners.
arXiv:2205.11916 [cs.CL]
[8] Alex Krizhevsky. 2009. Learning Multiple Layers of Features from Tiny Images.
(2009), 32â€“33. https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf
[9] Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. Darts: Differentiable
architecture search. arXiv preprint arXiv:1806.09055 (2018).
[10] Zhichao Lu, Ian Whalen, Vishnu Boddeti, Yashesh Dhebar, Kalyanmoy Deb,
Erik Goodman, and Wolfgang Banzhaf. 2019. Nsga-net: neural architecture
search using multi-objective genetic algorithm. In Proceedings of the genetic and
evolutionary computation conference. 419â€“427.
[11] Kevin Ma, Daniele Grandi, Christopher McComb, and Kosa Goucher-Lambert.
2023.
Conceptual Design Generation Using Large Language Models.
arXiv:2306.01779 [cs.CL]
[12] Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Dan Fink, Olivier
Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy, and Babak
Hodjat. 2017. Evolving Deep Neural Networks. arXiv:1703.00548 [cs.NE]
[13] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. 2018. Efficient
neural architecture search via parameters sharing. In International conference on
machine learning. PMLR, 4095â€“4104.
[14] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. 2017. Searching for Activation
Functions. arXiv:1710.05941 [cs.NE]
[15] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. 2019. Regularized
evolution for image classifier architecture search. In Proceedings of the aaai
conference on artificial intelligence, Vol. 33. 4780â€“4789.
[16] Dingming Yang, Zeyu Yu, Hongqiang Yuan, and Yanrong Cui. 2022. An improved
genetic algorithm and its application in neural network adversarial attack. Plos
one 17, 5 (2022), e0267970.
[17] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao,
and Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving
with Large Language Models. arXiv:2305.10601 [cs.CL]
[18] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023. Automatic
Chain of Thought Prompting in Large Language Models. In The Eleventh Interna-
tional Conference on Learning Representations. https://openreview.net/forum?id=
5NTt8GFjUHkr


--- Page 9 ---
LLM Guided Evolution - The Automation of Models Advancing Models
GECCO â€™24, July 14â€“18, 2018, Melbourne, Australia
[19] Shi-Yao Zhou and Chung-Yen Su. 2022. A Novel lightweight Convolutional
Neural Network, ExquisiteNetV2. arXiv:2105.09008 [cs.CV]
[20] E. Zitzler, M. Laumanns, and L. Thiele. 2001. SPEA2: Improving the strength
pareto evolutionary algorithm for multiobjective optimization. In Evolutionary
Methods for Design Optimization and Control with Applications to Industrial Prob-
lems (19â€“21 September 2001), K. C. Giannakoglou, D. T. Tsahalis, J. PÃ©riaux, K. D.
Papailiou, and T. Fogarty (Eds.). International Center for Numerical Methods in
Engineering, Athens, Greece, 95â€“100.
[21] Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement
learning. arXiv preprint arXiv:1611.01578 (2016).
[22] Jason Zutty, Daniel Long, Heyward Adams, Gisele Bennett, and Christina Baxter.
2015. Multiple objective vector-based genetic programming using human-derived
primitives. In Proceedings of the 2015 Annual Conference on Genetic and Evolu-
tionary Computation. 1127â€“1134.
