--- Page 1 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
AlphaResearch : ACCELERATING NEW ALGORITHM
DISCOVERY WITH LANGUAGE MODELS
Zhaojian Yu1∗, Kaiyue Feng2∗, Yilun Zhao3, Shilin He4, Xiao-Ping Zhang1, Arman Cohan3
1Tsinghua University 2New York University 3Yale University 4ByteDance
ABSTRACT
Large language models have made significant progress in complex but easy-to-
verify problems, yet they still struggle with discovering the unknown. In this
paper, we present AlphaResearch, an autonomous research agent designed to dis-
cover new algorithms on open-ended problems. To synergize the feasibility and
innovation of the discovery process, we construct a novel dual research environ-
ment by combining the execution-based verify and simulated real-world peer re-
view environment. AlphaResearch discovers new algorithm by iteratively running
the following steps: (1) propose new ideas (2) verify the ideas in the dual research
environment (3) optimize the research proposals for better performance. To pro-
mote a transparent evaluation process, we construct AlphaResearchComp, a new
evaluation benchmark that includes an eight open-ended algorithmic problems
competition, with each problem carefully curated and verified through executable
pipelines, objective metrics, and reproducibility checks. AlphaResearch gets a
2/8 win rate in head-to-head comparison with human researchers, demonstrate
the possibility of accelerating algorithm discovery with LLMs. Notably, the al-
gorithm discovered by AlphaResearch on the “packing circles” problem achieves
the best-of-known performance, surpassing the results of human researchers and
strong baselines from recent work (e.g., AlphaEvolve). Additionally, we conduct
a comprehensive analysis of the remaining challenges of the 6/8 failure cases,
providing valuable insights for future research.1
0
100
200
300
400
500
Steps
0.75
1.00
1.25
1.50
1.75
2.00
2.25
Program Score
Packing Circles (n=26)
ShinkaEvolve
OpenEvolve
AlphaResearch
Figure 1: Comparison of OpenEvolve (with program-based reward), ShinkaEvolve (with program-
based reward) and AlphaResearch (with program-based and peer-review reward). We run three
agents on Packing Circles (n=26) problems. AlphaResearch achieves better performance than others.
∗Equal contribution.
1Code, data and models are available at https://github.com/answers111/alpha-research.
1
arXiv:2511.08522v1  [cs.CL]  11 Nov 2025


--- Page 2 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
Reward Model
Collect real-world research papers 
and train a reward model. 
step 1
Input: research idea
Output: idea score
Extract ideas or 
proposals from 
real-world research 
papers and their review 
scores.
Train language models 
with the research idea 
and review score.
Collect peer-reviewed 
papers from online 
platforms.
Peer-reviewed Papers
step 2 
Research ideas 
(Proposals)
Human reviews
Launch AlphaReseach with original 
ideas and programs 
Reward Model
Initial idea 
(Proposal)
Score: \boxed{6.0} 
(>=threshold,accept)
LLM Ensemble
Task: Propose new 
ideas based on the 
MetaData
Initial idea 
(Proposals)
Initial 
Program
Automatic
Evaluation
A proposal to describe 
the research problem.
Evaluate proposal as 
positive idea or negative 
ones using reward 
models.
Construct initial program 
and automatic evaluation 
(metadata) for the idea.
step 3 (Iteration 1~n)
LLM Ensemble
Task: Programming 
based on the ideas
Iterate the autonomous research process 
MetaData
Reward Model
Generate new idea based 
on the MetaData 
(proposals, program, 
evaluation results ) at 
Round n-1
Evaluate proposal as 
positive idea or negative 
ones using reward 
models.
Evaluate
MetaData at Round n
Update the program to 
match the new idea. And 
evaluate it with 
execution.
Obtain new MetaData 
and enter the next 
iteration
Figure 2: The launch of AlphaResearch contains two steps. (1) Train reward models with real-
world peer-reviewed records. (2) Prepare initial research proposals, initial programs and evalution
program. AlphaResearch will refine the research proposals and programs autonomously.
1
INTRODUCTION
Recent progress has shown that frontier LLMs like GPT-5 (OpenAI, 2025) and Gemini 2.5 (Co-
manici et al., 2025) could achieve expert-level performance in complex tasks such as mathemat-
ics (Trinh et al., 2024; Lin et al., 2025) and programming (Jimenez et al., 2024; Jain et al., 2025).
While LLMs excel at processing and reasoning on problems that are within the boundary of ex-
isting human knowledge (Wang et al., 2024b; Phan et al., 2025), their capacity for independent
discovery that pushes the boundaries of human knowledge still remains a question of paramount
importance (Novikov et al., 2025). Can these models create advanced knowledge or algorithms that
surpass human researchers?
Previous studies demonstrate that LLMs can generate novel ideas at a human expert level (Si et al.,
2024; Wang et al., 2024a). However, the outcome evaluation of LLM-generated research ideas still
struggles with biased verification methods (Ye et al., 2024) that constrain the exploration of novel
machine knowledge, such as LLM-as-a-judge (Lu et al., 2024), where misaligned LLMs are used
to evaluate fresh ideas and inevitably favor solutions within existing knowledge boundaries. Fur-
thermore, the ideation–execution gap (Si et al., 2025) between generating and executing on new
ideas also hinders models from producing advanced research outcomes. Moreover, prior attempts at
autonomous algorithm discovery face a fundamental tension. Execution-based verification systems
like AlphaEvolve Novikov et al. (2025) can rigorously validate whether code runs and meets con-
straints, but this verification alone might not be completely sufficient for discovery. For example,
these systems could converge on technically correct but scientifically uninteresting or less impactful
solutions—code that executes successfully yet offers no advancement over existing methods. Con-
versely, idea-generation systems evaluated purely by LLM judges can propose innovative concepts
that prove computationally infeasible or violate problem constraints when implemented. The ab-
sence of real-world research environment rewards in execution-based agents and execution-based
reward in idea-generation systems renders the discovery of new knowledge and algorithms chal-
lenging for current autonomous research agents (Tian et al., 2024). and algorithms challenging for
current autonomous research agents (Tian et al., 2024).
To combine the feasibility and innovation of the algorithm discovery process, we introduce Al-
phaResearch, an autonomous research agent that could discover new advanced algorithms with a
suite of research skills including idea generation and code implementation that could interact with
the environment. To synergize these research skills during the discovery process, we construct a
2


--- Page 3 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
novel dual research-based environment (Tian et al., 2024), where novel insights are forged by the
simulated real-world peer-reviewed environment and execution-based verification. We use this dual
environments to accelerate the discovery process because many research ideas can be evaluated be-
fore even implementing and executing on the idea. based on factors such as novelty, literature and
the knowledge used. Specifically, we (1) train a reward model AlphaResearch-RM-7B with real-
world peer-reviewed records, addressing the limitation of prior coding-only approaches that lack
real-world research feedback, and use it to score the fresh ideas generated by LLMs; (2) construct
an automatic program-based verifiable environment that executes these ideas with an interpreter.
This dual environment facilitates a rigorous algorithm discovery process for autonomous research
agents. As illustrated in Figure 2, AlphaResearch discovers new algorithms by iteratively running
the following steps: (i) proposing new research ideas, (ii) verify the ideas in the dual research-
based environment, and (iii) optimizing the proposals for higher reward from the environment. The
synergy between an iterative real-world peer review environment and program-based verification
empowers AlphaResearch to continuously accept novel research ideas and verify them via program
execution. Once the generated optimal program surpasses current human-best achievements, these
validated novel ideas could form feasible algorithms, thereby pushing the boundaries of human re-
search forward.
Comparable systems such as AlphaEvolve (Novikov et al., 2025) have not publicly disclosed all
the test problems so far. To compare our AlphaResearch with human researchers for novel algo-
rithm discovery, we construct AlphaResearchComp, a simulated discovery Competition between
research agents and human researchers, by collecting 8 open-ended research problems and their best-
of-human records (shown in Appendix D). Our results demonstrate that AlphaResearch surpasses
human researchers on two problems but fails on the other six. The novel algorithms discovered by
AlphaResearch not only surpass best-of-human performance but also significantly outperform the
state-of-the-art results achieved by AlphaEvolve. Specifically, AlphaResearch optimize the result
of “Packing Circles (n=32)” problem to 2.939, where the goal is to pack n disjoint circles inside
a unit square so as to maximize the sum of their radii. This result surpasses both the best human-
designed solutions and the previous state-of-the-art performance obtained by AlphaEvolve. These
entirely novel ideas and algorithms constitute the most advanced solutions currently present in the
human knowledge base, demonstrating the feasibility of employing LLMs to advance the frontiers
of human knowledge. The six failure modes in AlphaResearchComp demonstrate the challenges for
the autonomous algorithm discovery with research agents. We analyze the benefits and remaining
challenges of autonomous research agents for knowledge discovery, providing valuable insights for
future work.
2
ALPHARESEARCH
2.1
OVERVIEW
AlphaResearch discovers novel algorithms by continuously optimizing the research outcome from
a reward model that synergizes rigorous program verification and simulated real-world peer review
environment. As shown in Figure 2, given initial idea i0 and program p0, AlphaResearch runs the
program p0 with execution, producing r0, which represents the initial overall rating. The triplet
(i0, p0, r0) will be fed to AlphaResearch for subsequent processing, including newer idea gen-
eration, idea verification (rejected by AlphaResearch-RM), and program-based execution. When
reaching a point where execution output rn surpasses the previous rating, AlphaResearch will save
the triplet (ibest, pbest, rbest) as the best record. We repeat the process until rbest surpasses the
best-of-human score, or the maximum round is reached. The resulting trajectory is denoted as
τ = i0p0r0...in−1pn−1rn−1inpnrn, where n is the total rounds.
2.2
ACTIONS
New Idea Generation.
For each step k, AlphaResearch start with generating a new idea ik based
on a sampled previous step (it, pt, rt) from previous trajectory τk−1 = i0p0r0...ik−1pk−1rk−1. This
process can be denoted as:
ik ∼PA(·|it ⊕pt ⊕rt)
(1)
3


--- Page 4 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
Algorithm 1 AlphaResearch
Require: initial idea i0, initial program p0, initial result r0, model A, evaluation program E(·), maximum
iteration rounds n,
1: τ0 ←(i0, p0, r0), rbest = 0
▷Initialization
2: for k = 1 to n do do
3:
(it, pt, rt) ∼P(·|τk−1)
▷States Sampling
4:
ik ∼PA(·|it ⊕pt ⊕rt)
▷New Idea Generation (Eq. 1)
5:
if RM(ik) < threshold then
6:
continue
▷Reward Model for New Idea
7:
end if
8:
pk ∼PA(·|pt ⊕ik)
▷Program Generation (Eq. 2)
9:
rk ←E(pk)
▷Program-based Execution
10:
if rk > rbest then
11:
(ibest, pbest, rbest) = (ik, pk, rk)
12:
end if
13:
τk ←τk−1 ⊕ik ⊕pk ⊕rk
▷Trajectory Update (Eq. 3)
14: end for
15: return (ibest, pbest, rbest)
where ⊕means concatenation, t is the sampled step from trajectory τi−1. We use a reward model
to filter out high-quality ideas overall. If RM(in) outputs a negative score, we reject the idea and
skip subsequent actions in this round.
Program-based Verification.
After obtaining the fresh idea, AlphaResearch generates new pro-
gram pk based on the previous implementation pt and new idea ik next:
pk ∼PA(·|pt ⊕ik)
(2)
and yield the evaluation result rk by verifying pk with code executor rk ←E(pk). Then, we update
the trajectory τk with the newly generated idea ik, program pk and result rk:
τk ←τk−1 ⊕ik ⊕pk ⊕rk
(3)
We repeat the above interaction process until k reaches the maximum rounds n and get the best
result (ibest, pbest, rbest) as final output.
2.3
ENVIRONMENT
2.3.1
REWARD FROM REAL-WORLD RESEARCH RECORDS
Existing autonomous idea generation process suffers from a trade-off where highly novel research
ideas may lack feasibility (Guo et al., 2025). To address this gap and ensure the feasibility of idea
candidates, we train a reward model with ideas from real-world peer-review information to simulate
the real-world peer-review environment.
Dataset For Reward Model.
To train our reward model (RM) to identify good ideas, we collect
all ICLR peer review records from 2017 to 2024 as our training set. We sample a subset of ICLR
2025 records as a test set, where the dates of train and test are disjoint, which prevents knowledge
contamination between the train and test split. We select Qwen2.5-7B-Instruct as our base model,
whose release date (Sept, 2024) is earlier than the ICLR 2025 author-reviewer rebuttal period (Oct,
2024). For each record in the training dataset, we extract the abstract part as RM input and wrap
the average peer-review overall ratings with \boxed{} as RM output. We fine-tune Qwen2.5-7B-
Instruct with the RM pairs, yielding the AlphaResearch-RM-7B model.
Can LLMs Identify Good Ideas?
To simplify the RM evaluation, we binarize the RM out-
put score according to the ICLR 2025 Reviewer Guide: records with an average overall rating
> 5.5 are treated as positive, while those with ratings ≤5.5 are treated as negative. We compute
the binary classification accuracy and evaluate three models (GPT-5, Qwen2.5-Coder-Instruct, and
AlphaResearch-RM-7B) on the AlphaResearch-RM test set. To benchmark human performance,
three of the authors independently assessed 100 randomly sampled examples, and we report their
4


--- Page 5 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
Table 1: Dataset for reward model training.
We use the end of author-reviewer rebuttal
period as the latest knowledge date.
Split
Train
Test
Records
ICLR
ICLR
Range
2017∼2024
2025
Num
24,445
100
Start Date
2016-11
2024-10
End Date
2023-12
2024-12
Table 2: Evaluation results of RM. We use the
more recent date between the model release
date and the dataset cutoff as the latest date.
Reward Model
Cutoff
Acc
Random (theoretical)
-
50.0%
Human Annotator
-
65.0%
GPT-5 (medium)
2025-08
53.0%
Qwen2.5-7B-Instruct
2024-09
37.0%
AlphaResearch-RM-7B
2024-09
72.0%
Table 3: Problem overview in AlphaResearchComp. Detailed definitions, baseline values, and ref-
erences for each problem are provided in the Appendix D.
Problem
Human Best
Human Researcher
packing circles (n=26)
2.634
David Cantrell (2011)
packing circles (n=32)
2.936
Eckard Specht (2012)
minimizing max-min distance raio (d=2, n=16)
12.89
David Cantrell (2009)
third autocorrelation inequality
1.4581
Carlos Vinuesa (2009)
spherical code (n=30)
0.67365
Hardin & Sloane (1996, 2002)
autoconvolution peak minimization (upper bound)
0.755
Matolcsi–Vinuesa (2010)
littlewood polynomials (n=512)
32
Rudin–Shapiro (1959/1952)
MSTD (n=30)
1.04
Hegarty (2006/2007)
average scores. Table 2 presents the evaluation results that eliminate the knowledge contamina-
tion, highlighting the following observations: (1) Both GPT-5 and Qwen2.5-7B-Instruct have lower
than 50% accuracy when identifying the good ideas from ICLR 2025 records. (2) After fine-tuned
with ideas from previous ICLR peer-review information, AlphaResearch-RM-7B demonstrates 72%
classification accuracy on unseen ICLR 2025 ideas, significantly outperforming baseline models and
human experts. Based on these observations, we use the fine-tuned AlphaResearch-RM-7B as the
final RM to simulate a real-world peer-review environment and filter out good ideas generated by
AlphaResearch.
2.3.2
REWARD FROM PROGRAM-BASED EXECUTION
Inspired by AlphaEvolve (Novikov et al., 2025), we construct automatic evaluation process with
code executor where each new program pk generated by AlphaResearch will be captured and eval-
uated. The evaluation program E(·) includes two modules: (i) Verification module that validates
whether pk conforms to the problem constraints. (ii) Measurement module that output the score
rk of program performance. The program output rk will be injected to the idea generation prompt
(if sampled), thereby participating in the optimization process for fresh ideas. These programs and
results are stored in a candidates pool, where the primary goal is to optimally resurface previously
explored ideas in future generations. The verifiable reward by code executor significantly simplify
the action spaces of AlphaResearch, thereby enhancing the efficiency of the discovery process.
3
ALPHARESEARCHCOMP
Problems collection.
AlphaEvolve has not publicly disclosed all the test problems so far. To pro-
vide a more transparent evaluation, we curate and open source a set of 8 frontier program-based
research tasks spanning geometry, number theory, harmonic analysis, and combinatorial optimiza-
tion. These problems were selected based on the following principles:
• Well-defined Objectives. Each task has a precise mathematical formulation with an objective
function that admits rigorous automatic evaluation.
5


--- Page 6 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
Table 4: Results on AlphaResearchComp. ↑inidicates that higher score is better and ↓for lower.
Problem
Human
AlphaResearch
Excel@best
init
best
packing circles (n=26) ↑
2.634
0
2.636
0.32%
packing circles (n=32) ↑
2.936
0
2.939
0.10%
minimizing max-min distance ratio ↓
12.89
15.55
12.92
-0.23%
third autocorrelation inequality ↓
1.458
35.746
1.546
-6.03%
spherical code (d=3, n=30) ↑
0.6736
0.5130
0.6735
-0.01%
autoconvolution peak minimization ↓
0.755
1.512
0.756
-0.13%
littlewood polynomials (n=512) ↓
32
32
32
0
MSTD (n=30) ↑
1.04
1.04
1.04
0
• Known Human-best Baselines. For every problem, we provide the best-known human result
from the literature. These represent conjectured best-known values rather than proven optima,
ensuring ample room for further improvement.
Table 3 presents the overview of the curated problems. They are either refined from prior work (e.g.,
AlphaEvolve) or collected from online repositories and domain experts. Each problem’s baseline
is supported by verifiable resources in the corresponding field. This design enables AlphaResearch
to demonstrate both the reproducibility of established mathematical results and the potential for
discovery beyond current human-best achievements.
Initialization Strategy.
After obtaining the research problems of AlphaResearchComp, we con-
struct diverse initial states for each problem with the following strategies: (1) For the “Packing
Circles” (n=26) and “Packing Circles” (n=32) problems, we initialize them with null programs (r0
= 0) to simulate researches starting from scratch.(2) For the “Littlewood Polynomials” and “MSTD
(n=30)” problems, we directly adopt the best-known solutions (r0 = rhuman) from human re-
searchers to emulate improvements upon established methods. (3) For the remaining problems, we
employ a moderate initialization strategy (0 < r0 < rhuman) to ensure sufficient room for the re-
search agent to explore. This initialization strategy simulates a variety of real-world scenarios for
the research agent, thereby facilitating a thorough evaluation process.
Metrics.
For benchmarks like code generation with good verification techniques (e.g., unit tests),
pass@k (Chen et al., 2021) is a metric denoting that at least one out of k i.i.d. task trials is successful,
which captures the ability of LLMs to solve easy-to-verified problems. For open-ended real-world
algorithm discovery tasks, we propose a new metric - excel@best (excel at best), defined as the
percentage excess on baseline (best of human level) results:
excel@best =
E
Problems
|rbest −rhuman| · Id
rhuman

(4)
where rhuman indicates the results of human best level. Id indicates the optimization direction
where Id = 1 represents that higher score is better and Id = −1 represents lower.
4
EXPERIMENTS
4.1
SETUP
We select o4-mini, a strong but cost-efficient LLM as our research agent and run AlphaResearch
on each problem to get the best algorithm. We perform supervised finetuning on Qwen-2.5-7B-
Instruct (Yang et al., 2025) with the collected ICLR papers, yielding AlphaResearch-RM-7B. We
do not compute loss on paper information, only on the predicted rating scores within the context.
For fine-tuning, we adopt a learning rate of 1e-5 with a linear warm-up for 100 steps. All models
are trained for 2 epochs with a batch size of 128, using bfloat16 precision under FSDP. All other
unspecified hyperparameters follow the default settings of the Hugging Face Trainer 2.
2https://huggingface.co/docs/transformers/main_classes/trainer
6


--- Page 7 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
0
1000
2000
3000
4000
Iteration
0.0
0.5
1.0
1.5
2.0
2.5
Execution-based Reward
Packing Circles in Unit Square (n=26)
rbest
rk
0
200
400
600
800
1000
1200
1400
Iteration
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Execution-based Reward
Third Autocorrelation Inequality
rbest
rk
Figure 3: Execution-based reward of AlphaResearch on packing circles (n=26) problem (left) and
third autocorrelation inequality problem (right).
4.2
RESULTS
3
4
5
6
7
8
Score by AlphaResearch-RM-7B
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Frequency
AlphaResearch vs. Execution-based Agent
Execution-based Agent
AlphaResearch
Figure 4:
The idea comparison between
execution-only research agent and AlphaRe-
search where AlphaResearch-RM-7B are used.
LLMs could sometimes discover new algo-
rithms themselves. Table 4 presents the results
of AlphaResearchComp on 8 algorithms discovery
problems. AlphaResearch achieved a 2/8 win rate
(excel@best > 0) against human researchers, with
one notable success:
the algorithm discovered
by AlphaResearch for “Packing Circles” problem
reaches the best-of-known performance (2.636 for
n=26, 2.939 for n=32), outperforming human re-
searchers (2.634 for n=26, 2.936 for n=32) and Al-
phaEvolve (2.635 for n=26, 2.937 for n=32).
LLMs can refine their research ideas au-
tonomously. AlphaResearch discovers advanced
algorithm by iteratively propose and verify new re-
search ideas. As shown in Table 4, 6/8 problems
demonstrate consistent improvement throughout
the discovery process.
Figure 3 presents two
examples of the reward trend in AlphaResearch,
where the execution-based reward initially grows
rapidly, then slowly plateaus for optimal perfor-
mance seeking. This improvement trend emphasizes the autonomous discovery ability of research
agents.
The discovery of superhuman algorithms remains challenging for LLMs. Despite exhibiting
continuous reward growth, AlphaResearch’s performance still underperforms human researchers in
6 out of 8 problems. We initialize AlphaResearch with the best-of-known solution from human re-
searchers on “Littlewood polynomials“ and “MSTD(n=30)“ problems, where AlphaResearch didn’t
show an increase in execution-based rewards. This indicates that current LLMs still struggle to con-
sistently find better algorithms than human researchers.
4.3
ABLATIONS AND ANALYSIS
AlphaResearch vs.
OpenEvolve.
As shown in Figure 1, we compare AlphaResearch,
OpenEvolve (Sharma, 2025) and ShinkaEvolve (Lange et al., 2025) on packing circles (n=26)
problem at the first 500 steps for simplicity.
AlphaResearch achieves better performance than
OpenEvolve and slightly surpasses ShinkaEvolve, which demonstrates its advantages on acceler-
ating algorithm discovery.
7


--- Page 8 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
packing circles (n=26)
packing circles (n=32)
minimizing max-min distance raio 
third autocorrelation inequality
autoconvolution peak minimization
littlewood polynomials (n=512)
MSTD (n=30)
spherical code
0
20
40
60
80
100
Percentage (%)
31.7%
31.7%
41.8%
42.3%
37.3%
35.0%
36.7%
34.9%
28.9%
28.9%
52.2%
44.4%
51.7%
53.4%
51.1%
58.7%
39.4%
39.4%
6.0%
13.3%
11.0%
11.6%
12.2%
6.4%
Lower than RM threshold
Execution successful
Execution failed
Figure 5: Reward overview during the discovery process. Each action in AlphaResearch will obtain
3 kinds of reward: (1) idea scrapping due to lower RM score than threshold, (2) idea execution
successes, and (3) idea execution fails.
AlphaResearch vs. Execution-based agent that propose fresh ideas without idea verification.
To compare AlphaResearch with execution-only agent that propose fresh ideas without idea verifica-
tion, we utilize AlphaResearch-RM-7B to evaluate the novelty of ideas generated by the execution-
only agent and ideas produced by AlphaResearch. As illustrated in Figure 4, the ideas generated by
AlphaResearch generally achieves higher scores than execution-only research agent. This illustrates
that AlphaResearch tends to generates better ideas to get higher peer review reward, thus facilitating
a more effective research optimization process.
Analysis of the discovery process.
We analyze the reward distribution in AlphaResearch discov-
ery process. As shown in Figure 5, approximately 30%∼40% of newly proposed ideas fall below
the RM threshold and are thus discarded. The remaining ideas are executed, with the success rate
of execution largely depending on the inherent characteristics of the problems. For exmample,
the execution success rate on “Packing Circles” problem is 28.9%, whereas it reaches 51.7% on
the “Third Autocorrelation Inequality” problem. Figure 3 illustrates the execution-based rewards
for these two examples in AlphaResearch. Despite the substantial variations in execution success
rates, the execution-based rewards in both cases exhibit a consistent increasing trend. These find-
ings demonstrate the interactions between LLM-based autonomous research agent and real-world
environments.
Feedback from AlphaResearch Environment
w/ AlphaResearch-RM-7B
w/o AlphaResearch-RM-7B
151
(37.8%)
194
(48.5%)
151
(37.8%)
98
(24.5%)
206
(51.5%)
Execution successful
Lower than RM threshold
Execution failed
Figure 6: The impact of real-world peer review environment on execution results. AlphaResearch-
RM-7B filters 151 bad ideas where 108 ideas fail to executed and 43 successes.
The impact of real-world peer-review environment.
To assess the effectiveness of reward from
simulated real-world peer-view environment, we ablate AlphaResearch-RM-7B at the first 400 it-
erations on “Packing Circles” problem. Figure 6 presents the execution results of w/ and w/o
AlphaReasearch-RM-7B during the discovery process.
Compared to the baseline without RM,
AlphaResearch-RM-7B successfully filtered 151 ideas below the threshold. This process yielded
108 correct rejections of execution failures while making 43 erroneous rejections of viable ideas.
AlphaResearch attained an accuracy of 71.5% (108/151), a result that aligns closely with its perfor-
8


--- Page 9 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
 
def pack_circles(n = 26):
    """
    Construct a speciﬁc arrangement of 26 circles in a unit square
    that attempts to maximize the sum of their radii.
    """
    …
    return sum_radii, circles
def compute_max_radii(centers):
    """
    Compute the maximum possible radii for each circle position
    such that they don't overlap and stay within the unit square."""...
 
tmp/packing_circles/initial_program.py
Initial Program
 
def verify_circles(circles):
    """Checks that the circles are disjoint and lie inside a unit square.  """
...
def evaluate(program_path: str = "results/initial_program.py"):
    """
    Evaluate the pack_circles function from the given program ﬁle.
    Returns the total radius sum if valid, otherwise raises an exception.
    """
…
    return {"pack_result_26": total_r_26}
 
tmp/packing_circles/evaluator.py
Veriﬁcation  Program
 
Initial Research Idea
The program presents a computational approach to the circle packing problem within a unit 
square, aiming to maximize the sum of radii for a given number of circles. The pack_circles 
function initiates a structured placement of 26 circles: one at the center, eight in an inner ring, 
and sixteen in an outer ring. While this initial arrangement is a predeﬁned pattern, it serves as 
a foundation for further optimization. The core of the algorithm lies in the compute_max_radii 
function, which iteratively determines the largest possible radius for each circle. This is 
achieved by ﬁrst constraining radii based on proximity to the unit square's boundaries and 
then adjusting them to prevent overlap between any pair of circles. Overlapping circles have 
their radii proportionally scaled down to ensure non-intersection, effectively pushing them to 
a just-touching state. The ﬁnal output provides the optimized centers, radii, and the total sum 
of radii for the packed conﬁguration, demonstrating a method for generating dense circle 
arrangements within a conﬁned space.
{"pack_result_26": 0.864}
Execution Results
 
 
    Instruction
You are a research advisor tasked with improving research proposals.
Your goal is to generate a new research proposal that builds upon the current research idea 
while addressing its limitations and incorporating insights from successful approaches.
    MetaData
### Current Research Idea
The program presents a computational approach to the circle packing problem within a unit 
square, aiming to maximize the sum of radii for a given number of circles.
### Current Program
```python
def pack_circles(n = 26):
    """
    Construct a speciﬁc arrangement of 26 circles in a unit square
    that attempts to maximize the sum of their radii
    """…
```
### Current Metrics
packing_result_26=0.864
New Idea Generation
 
     Instruction
You are an expert software developer tasked with iteratively improving a codebase.
Your job is to analyze the current program and suggest improvements based on feedback 
from previous attempts.Focus on making targeted changes that will increase the program's 
performance metrics.
     MetaData
### Current Research Idea
The program presents a computational approach to the circle packing problem within a unit 
square, aiming to maximize the sum of radii for a given number of circles.
### Current Program
```python
def pack_circles(n = 26):
    """
    Construct a speciﬁc arrangement of 26 circles in a unit square
    that attempts to maximize the sum of their radii
    """… ```
### Current Metrics
packing_result_26=0.864
### New Research Idea
We propose a novel computational framework to improve the dense packing of n 
variable‐radius circles within a unit square, targeting maximization of the total sum of radii. 
Our approach addresses key limitations in existing methods—namely, rigid initialization 
patterns, local‐only radius adjustment, and O(n²) pairwise scaling…
     Format
You MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:
<<<<<<< SEARCH
# Original code to ﬁnd and replace (must match exactly)
=======
# New replacement code
>>>>>>> REPLACE
Program Generation
Figure 7: We show an example of a formatted task of AlphaResearch.
mance on the AlphaResearch-RM test set, as shown in Table 2. This outcome effectively demon-
strates the model’s generalization capabilities and the efficacy of incorporating feedback from a
simulated real-world peer-review environment.
4.4
CASE STUDY
We select the successful example from AlphaResearch to better understand the discovery process.
We’ll consider the problem “Packing Circles” where the goal is to pack n disjoint circles inside a
unit square so as to maximize the sum of their radii, shown in Figure 7. We first initialize AlphaRe-
search with original research proposal and an related program that return the a list of circles (x, y, r)
as output, as shown in Appendix D.4. The verification program first employs verify_circles
function to check if the outputs of initial program meet the problem constraints (e.g., all circles
are inside a unit square) and evaluate function to output the sum of their radii. The metadata
including: (1) research ideas, (2) programs, (3) and execution results are subsequently preserved as
candidates which represents the end of one step. At the next step, AlphaResearch will sample from
the candidate pool and generate a new idea to improve the research proposals from the sampled
metadata. After generating the new research ideas, AlphaResearch will further generate a patch to
modify the existing program if the idea obtain a postive score from AlphaResearch-RM. The new
program is then evaluated by the same verification program, thereby generating new metadata. We
select the best program and idea as the final solution of AlphaResearch in this iterative process. s
5
RELATED WORK
LLMs for New Idea Generation and Verification.
Several recent works explored methods to
improve research idea generation, such as iterative novelty refinement (Wang et al., 2024a; Baek
et al., 2024). These works focus on improving the research idea over vanilla prompting but critically
9


--- Page 10 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
miss an effective verification method. To promote more reliable AI genrated research ideas, many
studies have proposed solutions from different perspectives, such as comparisons with any human
expert (Si et al., 2024), using LLMs for executing experiments by generating code with human-
curated research problems (Huang et al., 2024; Tian et al., 2024) and executing LLM-generated
research ideas with LLM-generated programs (Li et al., 2024; Lu et al., 2024; Aygün et al., 2025).
These works either use automatic program evaluation or misaligned LLM evaluator method, which
presents a challenge for their scalability to real-world advanced algorithm discovery. Our AlphaRe-
search presents a more feasible direction by combining program execution with RM training from
real-world peer-reviewed research records.
LLMs for Code Generation.
In autonomous research agents, code generation serves as a funda-
mental step. Previous models (Guo et al., 2024; Yu et al., 2023; Hui et al., 2024) and benchmarks
(Chen et al., 2021; Yu et al., 2025) for code generation are in a longstanding pursuit of synthesizing
code from natural language descriptions. SWE-Bench (Jimenez et al., 2024) introduces the problems
in real-world software development. Many studies on SWE-Bench have greatly contributed to the
emergence of coding agents like SWE-Agent (Yang et al., 2024) and OpenHands (Wang et al., 2025).
These agent framework greatly facilitate the training of agentic LLMs like Kimi-K2 (Team et al.,
2025) and GLM-4.5 (Zeng et al., 2025). The surge of these models on SWE-Bench underscores a
critical need to reassess the future directions of coding agent research. Our AlphaResearchComp
benchmark shows that testing LLMs on open-ended research for algorithm discovery is a promising
direction to adapt language models to real-world tasks.
6
DISCUSSION
Limitations and Future Directions.
Although AlphaResearch successfully discovers novel algo-
rithms, we hope to expand its coverage to more realistic applications like accelerate tensor computa-
tions. Second, our experiments aim to establish the simplest and most straight-forward approaches
for algorithm discovery. Future research should pay more attention to augment the research agents
with useful external tools and the application to more complex problems. Lastly, the training of RM
in AlphaResearch is based on small models (e.g., Qwen-2.5-7B-Instruct) and 24,445 ICLR peer re-
view records. Enhancing the reward model parameter and dataset size are two important directions,
which is left for future research.
Conclusion.
We present AlphaResearch, an autonomous research agent that synergistically com-
bines new idea generation with program-based verification for novel algorithm discovery. Our ap-
proach demonstrates the potential of employing LLM to discover unexplored research area, enabling
language models to effectively tackle complex open-ended tasks. We construct AlphaResearch-
Comp, including 8 open-ended algorithmic problems, where AlphaResearch outperforms human
researchers in 2/8 algorithmic problems but lags behind in the remaining 6 problems. Our system-
atic analysis of the benefits and remaining challenges of autonomous algorithm discovery provides
valuable insights for future research, contributing to the development of more advanced and capable
research agents.
REFERENCES
Eser Aygün, Anastasiya Belyaeva, Gheorghe Comanici, Marc Coram, Hao Cui, Jake Garrison, Re-
nee Johnston Anton Kast, Cory Y McLean, Peter Norgaard, Zahra Shamsi, et al. An ai system to
help scientists write expert-level empirical software. arXiv preprint arXiv:2509.06503, 2025.
Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. ResearchAgent: Iter-
ative Research Idea Generation over Scientific Literature with Large Language Models. ArXiv,
abs/2404.07738, 2024.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit
Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the
10


--- Page 11 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
frontier with advanced reasoning, multimodality, long context, and next generation agentic capa-
bilities. arXiv preprint arXiv:2507.06261, 2025.
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao
Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming–
the rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024.
Sikun Guo, Amir Hassan Shariatmadari, Guangzhi Xiong, Albert Huang, Myles Kim, Corey M
Williams, Stefan Bekiranov, and Aidong Zhang. Ideabench: Benchmarking large language mod-
els for research idea generation. In Proceedings of the 31st ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining V. 2, pp. 5888–5899, 2025.
Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. MLAgentBench: Evaluating Language
Agents on Machine Learning Experimentation. In ICML, 2024.
Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang,
Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186,
2024.
Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Ar-
mando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamina-
tion free evaluation of large language models for code. In The Thirteenth International Confer-
ence on Learning Representations, 2025. URL https://openreview.net/forum?id=
chfJJYC3iL.
Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R
Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth
International Conference on Learning Representations, 2024. URL https://openreview.
net/forum?id=VTF8yNQM66.
Robert Tjarko Lange, Yuki Imajuku, and Edoardo Cetin. Shinkaevolve: Towards open-ended and
sample-efficient program evolution. arXiv preprint arXiv:2509.19349, 2025.
Ruochen Li, Teerth Patel, Qingyun Wang, and Xinya Du. MLR-Copilot: Autonomous Machine
Learning Research based on Large Language Models Agents. ArXiv, abs/2408.14033, 2024.
Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia LI, Mengzhou
Xia, Danqi Chen, Sanjeev Arora, and Chi Jin. Goedel-prover: A frontier model for open-source
automated theorem proving. In Second Conference on Language Modeling, 2025. URL https:
//openreview.net/forum?id=x2y9i2HDjD.
Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scien-
tist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292,
2024.
Alexander Novikov, Ngân V˜u, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt
Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian,
et al.
Alphaevolve: A coding agent for scientific and algorithmic discovery.
arXiv preprint
arXiv:2506.13131, 2025.
OpenAI. Gpt-5. 2025. URL https://openai.com/index/introducing-gpt-5/.
Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin
Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanity’s last exam. arXiv preprint
arXiv:2501.14249, 2025. URL https://arxiv.org/abs/2501.14249.
Asankhaya Sharma. Openevolve: an open-source evolutionary coding agent, 2025. URL https:
//github.com/codelion/openevolve.
Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can llms generate novel research ideas. 2024.
Chenglei Si, Tatsunori Hashimoto, and Diyi Yang. The ideation-execution gap: Execution outcomes
of llm-generated versus human research ideas. arXiv preprint arXiv:2506.20803, 2025.
11


--- Page 12 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen,
Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv
preprint arXiv:2507.20534, 2025.
Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland
Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha
Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Min Zhu, Kil-
ian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, E. A.
Huerta, and Hao Peng. SciCode: A Research Coding Benchmark Curated by Scientists. ArXiv,
abs/2407.13168, 2024.
Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry
without human demonstrations. Nature, 625(7995):476–482, 2024.
Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. Scimon: Scientific inspiration machines
optimized for novelty. In Proceedings of the 62nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pp. 279–299, 2024a.
Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan,
Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng,
Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert
Brennan, Hao Peng, Heng Ji, and Graham Neubig. Openhands: An open platform for AI soft-
ware developers as generalist agents. In The Thirteenth International Conference on Learning
Representations, 2025. URL https://openreview.net/forum?id=OJd3ayDDoF.
Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weim-
ing Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang,
Rongqi Fan, Xiang Yue, and Wenhu Chen. MMLU-pro: A more robust and challenging multi-
task language understanding benchmark. In The Thirty-eight Conference on Neural Information
Processing Systems Datasets and Benchmarks Track, 2024b. URL https://openreview.
net/forum?id=y10DM6R2r3.
An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,
Chang Gao, Chengen Huang, Chenxu Lv, et al.
Qwen3 technical report.
arXiv preprint
arXiv:2505.09388, 2025.
John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan,
and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering.
Advances in Neural Information Processing Systems, 37:50528–50652, 2024.
Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner
Geyer, Chao Huang, Pin-Yu Chen, et al. Justice or prejudice? quantifying biases in llm-as-a-
judge. arXiv preprint arXiv:2410.02736, 2024.
Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, and
Qiufeng Yin. Wavecoder: Widespread and versatile enhancement for code large language models
by instruction tuning. arXiv preprint arXiv:2312.14187, 2023.
Zhaojian Yu, Yilun Zhao, Arman Cohan, and Xiao-Ping Zhang. HumanEval pro and MBPP pro:
Evaluating large language models on self-invoking code generation task. In Wanxiang Che, Joyce
Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Associa-
tion for Computational Linguistics: ACL 2025, pp. 13253–13279, Vienna, Austria, July 2025.
Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.
findings-acl.686. URL https://aclanthology.org/2025.findings-acl.686/.
Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang,
Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation
models. arXiv preprint arXiv:2508.06471, 2025.
12


--- Page 13 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
APPENDIX CONTENTS
1
Introduction
2
2
AlphaResearch
3
2.1
Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2.2
Actions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2.3
Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
2.3.1
Reward from Real-world Research Records . . . . . . . . . . . . . . . . .
4
2.3.2
Reward from Program-based Execution . . . . . . . . . . . . . . . . . . .
5
3
AlphaResearchComp
5
4
Experiments
6
4.1
Setup
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
4.2
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
4.3
Ablations and Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
4.4
Case Study
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
5
Related Work
9
6
Discussion
10
A Examples
14
B
Prompts
15
C The Use of Large Language Models
16
D Curated Problems and Human-Best Values
17
D.1
Spherical Code (S2, n = 30). . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
D.2
Littlewood Polynomials.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
D.3
Sum vs. Difference Sets (MSTD).
. . . . . . . . . . . . . . . . . . . . . . . . . .
22
D.4
Packing Circle in a Square (variable radii). . . . . . . . . . . . . . . . . . . . . . .
23
D.5
Third Autocorrelation Inequality. . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
D.6
Third-Order Autocorrelation Inequality (C3 Upper Bound) . . . . . . . . . . . . .
28
13


--- Page 14 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
A
EXAMPLES
We show an example of the constructions discovered by AlphaResearch on problem “Packing Cir-
cles”.
AlphaEvolve
1
packing_circles_alphaevolve = np.array([[0.09076163, 0.40381803, 0.090761620923837],
[0.07310993, 0.92689178, 0.07310821268917801], [0.08745017, 0.22570576,
0.087381421261857], [0.24855246, 0.30880277, 0.093428060657193], [0.4079865, 0.06300614,
0.063006133699386], [0.47646318, 0.90136179, 0.09863820013617901], [0.89604966,
0.10309934, 0.10309932969006601], [0.9066386, 0.68096117, 0.09336139066386], [0.08962002,
0.76509474, 0.0895289910471], [0.06973669, 0.06965159, 0.06965158303484101],
[0.40979823, 0.21756451, 0.09156283084371601], [0.25742466, 0.88393887,
0.11606111839388701], [0.09064689, 0.58506214, 0.090482500951749], [0.90294698,
0.30231577, 0.09623644037635501], [0.57265603, 0.10585396, 0.105853949414604],
[0.74007588, 0.40129314, 0.09435083056491601], [0.57539962, 0.71183255,
0.115160168483982], [0.7367635, 0.21592191, 0.09104997089500201], [0.41096972,
0.40263617, 0.093512520648747], [0.88664452, 0.88667032, 0.113317128668286], [0.57582722,
0.49961748, 0.09705531029446801], [0.24962585, 0.49417195, 0.09194421080557799],
[0.90546338, 0.49309632, 0.094507120549287], [0.67381348, 0.90149423,
0.09850576014942301], [0.24310147, 0.1077195, 0.10771948922805], [0.40815297, 0.5886157,
0.09248833075116601], [0.24737889, 0.6771266, 0.090994980900501], [0.75801377, 0.7532924,
0.07192969280703], [0.73526642, 0.06243992, 0.062439303756069], [0.57415412, 0.30715219,
0.095403150459684], [0.39239379, 0.75259664, 0.07223814277618501], [0.7439361,
0.58879735, 0.093166630683336]])
AlphaResearch
1
packing_circles_alpharesearch = np.array([[(0.1115677319034151, 0.11156773191787371,
0.11156438489140026), (0.09380224787136374, 0.3161654253705352, 0.09379943380606216),
(0.09485964915877172, 0.5048217088596118, 0.09485680337610973), (0.09657322554702913,
0.6962443020287629, 0.09657032835808858), (0.10365512530384222, 0.8963448746980195,
0.10365201565567386), (0.3334956594919712, 0.09664441783072292, 0.0966415184920332),
(0.26448615440016093, 0.9376113341122044, 0.06238679422590162), (0.5287192731314015,
0.09859146596680078, 0.09858850822808951), (0.591325020569507, 0.9366833118077788,
0.0633147886877468), (0.7427106948954978, 0.11611889563206494, 0.11611541209023483),
(0.7566639864477509, 0.8920585771994192, 0.1079381845606288), (0.9269317750270191,
0.07306822497789416, 0.07306603293080358), (0.9105741716090636, 0.23473376300222965,
0.08942314561430993), (0.9094700615258342, 0.41468336419923396, 0.09052722258939731),
(0.9124275486288124, 0.7738960294683863, 0.08756982419268892), (0.9302276007184027,
0.9302276007259072, 0.06977030612132157), (0.5931627035790205, 0.4107363306659128,
0.09216300786888813), (0.5896628759126524, 0.5965222415947758, 0.09365298106148348),
(0.26303074890883915, 0.783747668079202, 0.09148238826692158), (0.42710033854875884,
0.28662965969327264, 0.1151473780101257), (0.7511102582575875, 0.5051558281448295,
0.09185177348783963), (0.4273023330525072, 0.8937703360976411, 0.10622647700018645),
(0.24372345356089029, 0.24143034678815986, 0.07371479291303436), (0.4260882762526937,
0.6918664604322906, 0.09567746779211372), (0.2572363869779693, 0.4085253312744954,
0.09392364829884896), (0.9094294608754079, 0.5957810763279916, 0.0905678220228201),
(0.42560864125756626, 0.49898110459434486, 0.09720528992590773), (0.7533817110763772,
0.32263902019589896, 0.09067643144615074), (0.5903729314333418, 0.7817733747765757,
0.09159665425215473), (0.7515568081174837, 0.6905957415401818, 0.09358581053778628),
(0.2605636694821685, 0.5973506902903994, 0.09492800518715086), (0.6095540558280068,
0.24805951545091487, 0.07133567304015336)]])
14


--- Page 15 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
A collection of 32 disjoint circles: Radii: 2.937944526205518
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
A collection of 32 disjoint circles: Radii: 2.9395203049320564
Figure 8: New construction of AlphaResearch (right) improving the best known AlphaEvolve (right)
bounds on packing circles to maximize their sum of radii. Left: 32 circles in a unit square with sum
of radii ≥2.9379. Right: 32 circles in a unit square with sum of radii ≥2.9395
B
PROMPTS
Prompt for New Program Generation
You are an expert software developer tasked with iteratively improving a codebase. Your job
is to analyze the current program and suggest improvements based on the current proposal
and feedback from previous round. Focus on making targeted changes that will increase the
program’s performance metrics.
# Previous Proposal:
{previous proposal}
# Previous Program:
{previous program}
# Previous Performance Metrics:
{previous result}
# Current Proposal
{proposal}
# Task
Suggest improvements to the program that will lead to better performance on the specified
metrics.
You MUST use the exact SEARCH/REPLACE diff format shown below to indicate
changes:
1
<<<<<<< SEARCH
2
3
# Original code to find and replace (must match exactly)
4
5
=======
6
7
# New replacement code
8
9
<<<<<<< REPLACE
Example of valid diff format:
1
<<<<<<< SEARCH
2
for i in range(m):
3
for j in range(p):
15


--- Page 16 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
4
for k in range(n):
5
C[i, j] += A[i, k] * B[k, j]
6
7
=======
8
9
# Reorder loops for better memory access pattern
10
11
for i in range(m):
12
for k in range(n):
13
for j in range(p):
14
C[i, j] += A[i, k] * B[k, j]
15
16
>>>>>>> REPLACE
You can suggest multiple changes. Each SEARCH section must exactly match code in the
current program.
Be thoughtful about your changes and explain your reasoning thoroughly.
IMPORTANT: Do not rewrite the entire program - focus on targeted improvements.
Prompt for New Idea Generation
You are a research advisor tasked with evolving and improving research proposals. Your goal
is to generate a new research proposal that builds upon the current proposal while addressing
its limitations and incorporating insights from successful approaches.
Based on the following information, generate an improved research proposal:
Focus on:
1. Identifying weaknesses in the current approach based on performance metrics
2. Proposing novel improvements that could enhance performance
3. Learning from successful inspirations while maintaining originality
4. Ensuring the new proposal is implementable
- Current Proposal:
{proposal}
- Current Program:
{program}
- Current Metrics:
{results}
Please generate a new research proposal that:
1. Addresses the limitations shown in the current metrics
2. Incorporates insights from successful approaches
3. Proposes specific technical improvements
4. Maintains clarity and technical rigor
Return the proposal as a clear, concise research abstract.
Prompt for AlphaResearch-RM-7B
You are an expert reviewer tasked with evaluating the quality of a research proposal.
Your goal is to assign a score between 1 and 10 based on the proposal’s clarity, novelty,
technical rigor, and potential impact. Here are the criteria:
1. Read the following proposal carefully and provide a score from 1 to 10.
2. Score 6 means slightly higher than the borderline, 5 is slightly lower than the borderline.
Write the score in the \boxed{}.
{proposal}
C
THE USE OF LARGE LANGUAGE MODELS
During the preparation of this manuscript, we utilized large language models (LLMs) for grammar
checking and writing suggestions to enhance the readability and clarity of the content.
16


--- Page 17 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
D
CURATED PROBLEMS AND HUMAN-BEST VALUES
We summarize the ten problems used in the ALPHARESEARCH benchmark. For each item we state
the objective, the current human-best value at the benchmark’s default parameters, and whether this
value is proved optimal or only best-known.
17


--- Page 18 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
D.1
SPHERICAL CODE (S2, n = 30).
Problem Description: Place n = 30 points on the unit sphere in R3 to maximize the minimal
pairwise angle θmin.
Human Best: θmin ≈0.673651 radians (≈38.5971◦).
Initial Proposal
Problem definition.
Choose N = 30 points on the unit sphere S2 to maximize the mini-
mum pairwise angle
θmin = min
i<j arccos
 ⟨pi, pj⟩

.
Constraints.
• Points are unit vectors (rows normalized).
• Metric is θmin in radians.
Optimization goal.
Maximize θmin. The evaluator returns {score, θmin, N, dimension},
with score = θmin.
Best-known reference (for N = 30 on S2):
cos(θ∗) ≈0.7815518750949873
⇒
θ∗≈0.6736467551690225 rad.
Reference table:
Henry Cohn’s spherical codes data (https://cohn.mit.edu/
spherical-codes).
Best-known results (human).
• On S2 (3D), small N optima coincide with symmetric polyhedra (e.g., tetrahedron,
octahedron, icosahedron).
• For larger N, best codes come from numerical optimization; exact optimality is
only known in limited cases.
Algorithmic goal.
Construct codes with larger θmin. The baseline seeds with symmetric
configurations and uses farthest-point max–min. Stronger methods include:
• Energy minimization,
• Projected gradient / coordinate descent,
• Stochastic max–min refinement.
Initial Program:
1
2
3
import numpy as np
4
5
def _normalize_rows(P):
6
nrm = np.linalg.norm(P, axis=1, keepdims=True)
7
nrm = np.maximum(nrm, 1e-12)
8
return P / nrm
9
10
def seed_platonic(n):
11
"""Return a good symmetric seed on S^2 for some n; else None."""
12
if n == 2:
# antipodal
13
return np.array([[0,0,1],[0,0,-1]], dtype=float)
14
if n == 3:
# equilateral on equator
15
ang = 2*np.pi/3
16
return np.array([[1,0,0],[np.cos(ang),np.sin(ang),0],[np.cos(2*ang),np.sin(2*ang),0]],
dtype=float)
17
if n == 4:
# tetrahedron
18
return _normalize_rows(np.array([[1,1,1],[1,-1,-1],[-1,1,-1],[-1,-1,1]], dtype=float))
19
if n == 6:
# octahedron
20
return np.array([[1,0,0],[-1,0,0],[0,1,0],[0,-1,0],[0,0,1],[0,0,-1]], dtype=float)
21
if n == 8:
# cube vertices
22
V = np.array([[sx,sy,sz] for sx in (-1,1) for sy in (-1,1) for sz in (-1,1)], dtype=
float)
23
return _normalize_rows(V)
24
if n == 12:
# icosahedron (one realization)
25
phi = (1+np.sqrt(5))/2
18


--- Page 19 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
26
V = []
27
for s in (-1,1):
28
V += [[0, s,
phi],[0, s, -phi],[ s,
phi,0],[ s, -phi,0],[ phi,0, s],[-phi,0, s]]
29
V = np.array(V, dtype=float)
30
return _normalize_rows(V)
31
return None
32
33
def farthest_point_greedy(n, seed=None, rng=np.random.default_rng(0)):
34
"""
35
Greedy max min on S^2: start from seed, then add points that maximize min angle.
36
"""
37
def random_unit(k):
38
X = rng.normal(size=(k,3)); return _normalize_rows(X)
39
40
if seed is None:
41
P = random_unit(1)
# start with one random point
42
else:
43
P = _normalize_rows(seed)
44
while len(P) < n:
45
# generate candidates and pick the one with largest min angle to current set
46
C = random_unit(2000)
# candidates per iteration (tune as needed)
47
# cosines to existing points
48
cos = C @ P.T
49
# min angle to set -> maximize this
50
min_ang = np.arccos(np.clip(np.max(cos, axis=1), -1.0, 1.0))
51
idx = np.argmax(min_ang)
52
P = np.vstack([P, C[idx:idx+1]])
53
return P
54
55
def main():
56
n = 30
57
seed = seed_platonic(n)
58
pts = farthest_point_greedy(n, seed=seed, rng=np.random.default_rng(42))
59
print(f"n={n}, points={len(pts)}")
60
return pts
61
62
if __name__ == "__main__":
63
points = main()
64
65
np.save("points.npy", points)
66
67
# Ensure compatibility with evaluators that expect a global variable
68
try:
69
points
# type: ignore[name-defined]
70
except NameError:
71
points = main()
19


--- Page 20 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
D.2
LITTLEWOOD POLYNOMIALS.
Problem Description For coefficients ck ∈{±1} and Pn(t) = Pn−1
k=0 ckeikt, minimize ∥Pn∥∞=
supt∈R |Pn(t)|.
Human Best: the Rudin–Shapiro construction gives ∥Pn∥∞≤
√
2n. At the benchmark setting n =
512, this yields ∥P512∥∞≤32 (so the “larger-is-better” score 1/∥Pn∥∞is ≥1/32 = 0.03125).
Sharper constants are known for special families, but
√
2n remains a clean baseline.
Initial Proposal
Choose coefficients ck ∈{±1} for
P(z) =
n−1
X
k=0
ckzk,
|z| = 1,
so as to minimize the supremum norm
∥P∥∞= max
|z|=1 |P(z)|.
Constraints.
• Coefficients ck are restricted to ±1.
• The metric ∥P∥∞is estimated by FFT sampling on an equally spaced grid (denser
grid →tighter upper bound).
Optimization Goal. The evaluator returns:
score =



1
∥P∥∞
,
if valid,
−1.0,
otherwise.
Notes on Bounds. For the Rudin–Shapiro construction of length n, a classical identity gives
∥P∥∞≤
√
2n.
For the benchmark default n = 512, this yields
∥P∥∞≤
√
1024 = 32,
so
score =
1
32 = 0.03125.
Initial Program:
1
import numpy as np
2
3
def rudin_shapiro(n: int):
4
"""
5
First n signs of the Rudin-Shapiro sequence.
6
"""
7
a = np.ones(n, dtype=int)
8
for k in range(n):
9
x, cnt, prev = k, 0, 0
10
while x:
11
b = x & 1
12
if b & prev:
# saw ’11’
13
cnt ^= 1
14
prev = b
15
x >>= 1
16
a[k] = 1 if cnt == 0 else -1
17
return a
18
19
def random_littlewood(n: int, seed=0):
20
rng = np.random.default_rng(seed)
21
return rng.choice([-1, 1], size=n).astype(int)
22
23
def main():
20


--- Page 21 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
24
n = 512
25
c = rudin_shapiro(n)
26
print(f"n={n}, coeffs={len(c)}")
27
return c
28
29
if __name__ == "__main__":
30
coeffs = main()
31
32
# Ensure compatibility with evaluators that expect a global variable
33
try:
34
coeffs
# type: ignore[name-defined]
35
except NameError:
36
coeffs = main()
21


--- Page 22 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
D.3
SUM VS. DIFFERENCE SETS (MSTD).
Problem Description For a finite set A ⊂Z, maximize |A+A|/|A−A|.
Human Best: MSTD sets exist; the smallest possible size is |A| = 8 (classification up to affine
equivalence is known). For larger |A|, extremal ratios remain open; our benchmark instance reports
a representative value (≈1.04 for |A| = 30).
Initial Proposal
Objective.
Classical MSTD (enforced): Given A ⊂{0, 1, . . . , N −1} represented by a
0/1 indicator array of length N, maximize the ratio
R = |A + A|
|A −A|.
• Score: score = R (higher is better).
• Comparisons should be made under the same N.
Default setup.
• N = 30.
• Evaluator enforces A = B (classical setting). If a pair (A, B) is provided, B is
ignored and A is used.
Known best for N = 30 (baseline).
Conway’s MSTD set
A = {0, 2, 3, 4, 7, 11, 12, 14}
yields R ≈1.04. This is the baseline included in initial_program.py. Better ratios
may exist for N = 30; pushing R upwards is the optimization goal.
Notes.
• R > 1 is rare and indicates sum-dominance.
• The ratio depends strongly on N; do not compare ratios across different N without
a normalization scheme.
• If cross-N comparison is necessary, consider reporting both R and N, or use log R
as an auxiliary measure.
Initial Program:
1
import numpy as np
2
3
4
def main():
5
N = 30
6
# Conway MSTD set example; we take A=B for classical MSTD
7
A = [0, 2, 3, 4, 7, 11, 12, 14]
8
B = A[:]
9
A_ind = np.zeros(N, dtype=int); A_ind[A] = 1
10
B_ind = np.zeros(N, dtype=int); B_ind[B] = 1
11
return A_ind, B_ind
12
13
14
# Ensure globals for evaluator
15
try:
16
A_indicators; B_indicators
# type: ignore[name-defined]
17
except NameError:
18
A_indicators, B_indicators = main()
22


--- Page 23 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
D.4
PACKING CIRCLE IN A SQUARE (VARIABLE RADII).
Problem Description In the unit square, place n disjoint circles (radii free) to maximize the sum of
radii P ri.
Best-known: for n = 26, P ri = 2.634 (Cantrell, 2011); for n = 32, P ri = 2.936 (Specht,
2012).
Initial Proposal
Problem definition.
Given an integer n, place n disjoint circles in the unit square [0, 1]2
to maximize the total sum of radii.
Objective and metric.
• Score: score = Pn
i=1 ri
(larger is better).
• Validity: circles must be pairwise disjoint and fully contained in the unit square.
Notes on records.
• This variable-radius “sum of radii” objective is not the classical equal-radius pack-
ing; authoritative SOTA tables are not standardized.
• Values reported in code or experiments should be treated as benchmarks rather than
literature SOTA.
Goal.
Create algorithms that increase the total sum of radii for n ∈{26, 32} under the
above validity constraints.
Initial Program:
1
import math
2
import random
3
from concurrent.futures import ThreadPoolExecutor
4
5
6
def pack_circles(n, square_size=1.0):
7
"""
8
Pack n disjoint circles in a unit square using uniform tiling approach.
9
Returns the sum of radii and list of circles (x, y, r).
10
"""
11
12
def max_circle_radius(x, y, circles, square_size=1.0, skip_idx=None):
13
"""
14
Compute the maximum radius for a circle centered at (x, y) that:
15
- Stays within the unit square [0, square_size] \times [0, square_size].
16
- Does not overlap with existing circles.
17
skip_idx: if provided, index in circles[] to ignore (self).
18
"""
19
# Distance to nearest boundary of the unit square
20
r_max = min(x, y, square_size - x, square_size - y)
21
22
# Check distance to existing circles, exit early if r_max \rightarrow 0
23
# early exit if r_max is tiny, and avoid needless sqrt
24
for idx, (cx, cy, cr) in enumerate(circles):
25
if skip_idx == idx:
26
continue
27
if r_max <= 1e-8:
28
break
29
dx = x - cx
30
dy = y - cy
31
sep = r_max + cr
32
if dx*dx + dy*dy < sep*sep:
33
# only compute sqrt when we know we can shrink
34
dist = math.sqrt(dx*dx + dy*dy)
35
r_max = min(r_max, dist - cr)
36
return max(r_max, 0.0)
37
38
def uniform_tiling_circles(n, square_size=1.0):
39
"""
40
Uniformly tile the square with circles using optimal grid placement.
41
"""
42
if n <= 0:
43
return []
44
23


--- Page 24 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
45
circles = []
46
47
# Calculate optimal grid dimensions
48
# For n circles, find the best grid layout (rows x cols)
49
best_layout = None
50
best_total_radius = 0
51
52
# Try different grid configurations
53
for rows in range(1, min(n + 1, 20)):
54
cols = math.ceil(n / rows)
55
if cols > 20:
# Limit grid size
56
continue
57
58
# Calculate spacing
59
spacing_x = square_size / (cols + 1)
60
spacing_y = square_size / (rows + 1)
61
62
# Use the smaller spacing to ensure circles fit
63
min_spacing = min(spacing_x, spacing_y)
64
65
# Calculate maximum radius for this layout
66
max_radius = min_spacing / 2
67
68
# Ensure radius doesn’t exceed boundaries
69
max_radius = min(max_radius,
70
spacing_x / 2 - 1e-6,
71
spacing_y / 2 - 1e-6)
72
73
if max_radius <= 0:
74
continue
75
76
# Place circles in uniform grid
77
temp_circles = []
78
count = 0
79
80
for row in range(rows):
81
for col in range(cols):
82
if count >= n:
83
break
84
85
x = spacing_x * (col + 1)
86
y = spacing_y * (row + 1)
87
88
# Ensure circle stays within bounds
89
if (x - max_radius >= 0 and x + max_radius <= square_size and
90
y - max_radius >= 0 and y + max_radius <= square_size):
91
92
temp_circles.append((x, y, max_radius))
93
count += 1
94
95
if count >= n:
96
break
97
98
# Calculate total radius for this layout
99
total_radius = len(temp_circles) * max_radius
100
101
if total_radius > best_total_radius and len(temp_circles) == n:
102
best_total_radius = total_radius
103
best_layout = temp_circles
104
105
# If we found a valid layout, return it
106
if best_layout:
107
return best_layout
108
109
# Fallback: use hexagonal packing for better density
110
return hexagonal_packing(n, square_size)
111
112
def hexagonal_packing(n, square_size=1.0):
113
"""
114
Use hexagonal close packing for better space utilization.
115
"""
116
circles = []
117
118
# Estimate number of rows and columns for hexagonal packing
119
# Hexagonal packing has rows offset by sqrt(3)/2 * diameter
120
121
rows = int(math.sqrt(n * 2 / math.sqrt(3))) + 2
122
123
count = 0
124
row = 0
125
24


--- Page 25 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
126
while count < n and row < rows:
127
# Calculate y position for this row
128
y = (row + 0.5) * (square_size / (rows + 1))
129
130
# Number of circles in this row
131
if row % 2 == 0:
132
cols = int(math.sqrt(n)) + 1
133
else:
134
cols = int(math.sqrt(n))
135
136
spacing_x = square_size / (cols + 1)
137
138
for col in range(cols):
139
if count >= n:
140
break
141
142
if row % 2 == 0:
143
x = spacing_x * (col + 1)
144
else:
145
x = spacing_x * (col + 1) + spacing_x / 2
146
147
# Calculate maximum radius for this position
148
r = max_circle_radius(x, y, circles, square_size)
149
150
if r > 0:
151
circles.append((x, y, r))
152
count += 1
153
154
row += 1
155
156
return circles
157
158
def optimize_placement(n, square_size=1.0):
159
"""
160
Optimize circle placement using uniform tiling with radius maximization.
161
"""
162
circles = []
163
164
# First, try hexagonal packing for high initial density
165
hex_circles = hexagonal_packing(n, square_size)
166
if len(hex_circles) == n:
167
# Ensure maximum radii for hex layout with stronger refinement
168
hex_refined = refine_circles(hex_circles, square_size, iterations=20)
169
return hex_refined
170
171
# Fallback to uniform grid placement
172
grid_circles = uniform_tiling_circles(n, square_size)
173
if len(grid_circles) == n:
174
return grid_circles
175
176
# If uniform tiling didn’t work perfectly, use adaptive approach
177
# Calculate optimal radius based on density
178
area_per_circle = (square_size * square_size) / n
179
estimated_radius = math.sqrt(area_per_circle / math.pi) * 0.9
# Conservative estimate
180
181
# Create grid with optimal spacing
182
spacing = estimated_radius * 2.1
# Include gap
183
184
cols = int(square_size / spacing)
185
rows = int(square_size / spacing)
186
187
actual_spacing_x = square_size / (cols + 1)
188
actual_spacing_y = square_size / (rows + 1)
189
190
count = 0
191
for row in range(rows):
192
for col in range(cols):
193
if count >= n:
194
break
195
196
x = actual_spacing_x * (col + 1)
197
y = actual_spacing_y * (row + 1)
198
199
# Calculate maximum possible radius
200
r = max_circle_radius(x, y, circles, square_size)
201
202
if r > 0:
203
circles.append((x, y, r))
204
count += 1
205
206
if count >= n:
25


--- Page 26 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
207
break
208
209
# If we still need more circles, use remaining space
210
remaining = n - len(circles)
211
if remaining > 0:
212
# Place remaining circles in remaining spaces
213
for i in range(remaining):
214
# Try different positions systematically
215
best_r = 0
216
best_pos = (0.5, 0.5)
217
218
# Fine grid search (increased resolution)
219
grid_points = 100
220
for gx in range(1, grid_points):
221
for gy in range(1, grid_points):
222
x = gx / grid_points
223
y = gy / grid_points
224
225
r = max_circle_radius(x, y, circles, square_size)
226
if r > best_r:
227
best_r = r
228
best_pos = (x, y)
229
230
if best_r > 0:
231
circles.append((best_pos[0], best_pos[1], best_r))
232
233
return circles
234
235
def refine_circles(circles, square_size, iterations=80, perturb_interval=3):
236
"""
237
Iteratively grow each circle to its maximum radius under non-overlap constraints.
238
Includes randomized update order, periodic micro-perturbation to escape
239
local minima, and a final local-center-perturbation pass for densification.
240
"""
241
for it in range(iterations):
242
# randomize update order to avoid sweep-order bias
243
indices = list(range(len(circles)))
244
random.shuffle(indices)
245
for i in indices:
246
x, y, _ = circles[i]
247
# Compute maximal feasible radius here, skipping self
248
r = max_circle_radius(x, y, circles, square_size, skip_idx=i)
249
circles[i] = (x, y, r)
250
# Periodic micro-perturbation: jiggle a few circles
251
if it % perturb_interval == 0 and len(circles) > 0:
252
subset = random.sample(indices, min(5, len(circles)))
253
for j in subset:
254
x0, y0, r0 = circles[j]
255
dx = random.uniform(-0.03, 0.03)
256
dy = random.uniform(-0.03, 0.03)
257
nx = min(max(x0 + dx, 0), square_size)
258
ny = min(max(y0 + dy, 0), square_size)
259
# Compute maximal radius skipping self
260
nr = max_circle_radius(nx, ny, circles, square_size, skip_idx=j)
261
if nr > r0:
262
circles[j] = (nx, ny, nr)
263
# Full local center-perturbation phase for final densification
264
for i in range(len(circles)):
265
x, y, r = circles[i]
266
best_x, best_y, best_r = x, y, r
267
delta = 0.1
268
for _ in range(20):
269
dx = random.uniform(-delta, delta)
270
dy = random.uniform(-delta, delta)
271
nx = min(max(x + dx, 0), square_size)
272
ny = min(max(y + dy, 0), square_size)
273
# Compute maximal radius skipping self
274
nr = max_circle_radius(nx, ny, circles, square_size, skip_idx=i)
275
if nr > best_r:
276
best_x, best_y, best_r = nx, ny, nr
277
else:
278
delta *= 0.9
279
circles[i] = (best_x, best_y, best_r)
280
281
# Physics-inspired soft relaxation to escape persistent overlaps
282
for i in range(len(circles)):
283
x, y, r = circles[i]
284
fx, fy = 0.0, 0.0
285
for j, (xj, yj, rj) in enumerate(circles):
286
if i == j:
287
continue
26


--- Page 27 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
288
dx = x - xj
289
dy = y - yj
290
d = (dx*dx + dy*dy) ** 0.5
291
overlap = (r + rj) - d
292
if overlap > 0 and d > 1e-8:
293
fx += dx / d * overlap
294
fy += dy / d * overlap
295
# Nudge the center by 10\% of the computed net "repulsive" force
296
nx = min(max(x + 0.1 * fx, 0), square_size)
297
ny = min(max(y + 0.1 * fy, 0), square_size)
298
nr = max_circle_radius(nx, ny, circles, square_size, skip_idx=i)
299
circles[i] = (nx, ny, nr)
300
return circles
301
302
def multi_start_optimize(n, square_size, starts=None):
303
"""
304
Parallel multi-start global \rightarrow local optimization using ThreadPoolExecutor.
305
Number of starts adapts to problem size: max(100, 10*n).
306
"""
307
if starts is None:
308
if n <= 50:
309
starts = max(200, n * 20)
310
else:
311
starts = max(100, n * 10)
312
# precompute hexagonal packing baseline
313
hex_circ = hexagonal_packing(n, square_size)
314
hex_sum = sum(r for _, _, r in hex_circ)
315
best_conf = None
316
best_sum = 0.0
317
318
# single trial: seed \rightarrow refine \rightarrow score
319
def single_run(_):
320
conf0 = optimize_placement(n, square_size)
321
conf1 = refine_circles(conf0, square_size, iterations=40)
322
s1 = sum(r for _, _, r in conf1)
323
return s1, conf1
324
325
# dispatch trials in parallel
326
with ThreadPoolExecutor() as executor:
327
for score, conf in executor.map(single_run, range(starts)):
328
if score > best_sum:
329
best_sum, best_conf = score, conf.copy()
330
# early exit if near the hex-baseline
331
if best_sum >= hex_sum * 0.995:
332
break
333
334
return best_conf
335
336
# Use multi-start global \rightarrow local optimization (adaptive number of starts)
337
circles = multi_start_optimize(n, square_size)
338
339
# Quick 2-cluster remove-and-reinsert densification (extended iterations)
340
for _ in range(8):
341
# remove the two smallest circles to create a larger gap
342
smallest = sorted(range(len(circles)), key=lambda i: circles[i][2])[:2]
343
removed = [circles[i] for i in smallest]
344
# pop in reverse order to keep indices valid
345
for i in sorted(smallest, reverse=True):
346
circles.pop(i)
347
# refine the remaining configuration briefly
348
circles = refine_circles(circles, square_size, iterations=8)
349
# reinsert each removed circle with more sampling
350
for x_old, y_old, _ in removed:
351
best_r, best_pos = 0.0, (x_old, y_old)
352
for _ in range(500):
353
x = random.uniform(0, square_size)
354
y = random.uniform(0, square_size)
355
r = max_circle_radius(x, y, circles, square_size)
356
if r > best_r:
357
best_r, best_pos = r, (x, y)
358
circles.append((best_pos[0], best_pos[1], best_r))
359
# final local polish after reinsertion
360
circles = refine_circles(circles, square_size, iterations=5)
361
# end 2-cluster remove-and-reinsert densification
362
363
# Calculate total radius
364
total_radius = sum(circle[2] for circle in circles)
365
366
return total_radius, circles
27


--- Page 28 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
D.5
THIRD AUTOCORRELATION INEQUALITY.
Problem Description Let C3 be the largest constant such that max|t|≤1/2 |(f ∗f)(t)|
≥
C3
  R 1/4
−1/4 f
2 for all (signed) f.
Best-known: classical 1.4581 upper bound.
D.6
THIRD-ORDER AUTOCORRELATION INEQUALITY (C3 UPPER BOUND)
Initial Proposal
Problem.
For piecewise-constant nonnegative functions on a fixed support with unit mass,
we evaluate an upper bound Cupper_bound derived from the maximum of the autoconvolution
(normalized by squared L1 mass). The benchmark score is
score =
1
Cupper_bound
,
so that larger score indicates a smaller upper bound and hence a better result.
Evaluator.
The evaluator calls find_better_c3_upper_bound() from the target
program to obtain step heights, computes the normalized autoconvolution maximum, and
returns 1/Cupper_bound.
Baseline algorithm.
A simple genetic algorithm over height sequences serves as the base-
line search method. The algorithm includes:
• Tournament selection,
• One-point crossover,
• Gaussian mutation.
Initial Program:
1
import numpy as np
2
import scipy.integrate
3
4
def calculate_c3_upper_bound(height_sequence):
5
6
N = len(height_sequence)
7
delta_x = 1 / (2 * N)
8
9
def f(x):
10
if -0.25 <= x <= 0.25:
11
index = int((x - (-0.25)) / delta_x)
12
if index == N:
13
index -= 1
14
return height_sequence[index]
15
else:
16
return 0.0
17
18
integral_f = np.sum(height_sequence) * delta_x
19
integral_sq = integral_f**2
20
21
if integral_sq < 1e-18:
22
return 0.0
23
24
t_points = np.linspace(-0.5, 0.5, 2 * N + 1)
25
26
max_conv_val = 0.0
27
for t_val in t_points:
28
29
lower_bound = max(-0.25, t_val - 0.25)
30
upper_bound = min(0.25, t_val + 0.25)
31
32
if upper_bound <= lower_bound:
33
convolution_val = 0.0
34
else:
35
def integrand(x):
36
return f(x) * f(t_val - x)
37
38
convolution_val, _ = scipy.integrate.quad(integrand, lower_bound, upper_bound,
limit=100)
28


--- Page 29 ---
AlphaResearch: Accelerating New Algorithm Discovery with Language Models
39
40
if abs(convolution_val) > max_conv_val:
41
max_conv_val = abs(convolution_val)
42
43
return max_conv_val / integral_sq
44
45
def genetic_algorithm(population_size, num_intervals, generations, mutation_rate,
crossover_rate):
46
47
population = np.random.rand(population_size, num_intervals) * 2 - 1
48
49
best_solution = None
50
best_fitness = 0.0
51
52
for gen in range(generations):
53
54
fitness_scores = np.array([calculate_c3_upper_bound(individual) for individual in
population])
55
56
current_best_idx = np.argmax(fitness_scores)
57
if fitness_scores[current_best_idx] > best_fitness:
58
best_fitness = fitness_scores[current_best_idx]
59
best_solution = population[current_best_idx].copy()
60
# print(f"Generation {gen}: New best fitness = {best_fitness}")
61
62
63
new_population = np.zeros_like(population)
64
for i in range(population_size):
65
66
competitors_indices = np.random.choice(population_size, 2, replace=False)
67
winner_idx = competitors_indices[np.argmax(fitness_scores[competitors_indices])]
68
new_population[i] = population[winner_idx].copy()
69
70
for i in range(0, population_size, 2):
71
if np.random.rand() < crossover_rate:
72
parent1 = new_population[i]
73
parent2 = new_population[i+1]
74
crossover_point = np.random.randint(1, num_intervals - 1)
75
new_population[i] = np.concatenate((parent1[:crossover_point], parent2[
crossover_point:]))
76
new_population[i+1] = np.concatenate((parent2[:crossover_point], parent1[
crossover_point:]))
77
78
for i in range(population_size):
79
if np.random.rand() < mutation_rate:
80
mutation_point = np.random.randint(num_intervals)
81
new_population[i, mutation_point] += np.random.normal(0, 0.1)
82
83
new_population[i, mutation_point] = np.clip(new_population[i, mutation_point],
-2, 2)
84
85
population = new_population
86
87
return best_solution
88
89
def find_better_c3_upper_bound():
90
91
NUM_INTERVALS = 4
92
POPULATION_SIZE = 2
93
GENERATIONS = 10
94
MUTATION_RATE = 0.1
95
CROSSOVER_RATE = 0.8
96
97
height_sequence_3 = genetic_algorithm(POPULATION_SIZE, NUM_INTERVALS, GENERATIONS,
MUTATION_RATE, CROSSOVER_RATE)
98
99
return height_sequence_3
29
