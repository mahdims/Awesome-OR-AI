--- Page 1 ---
MAGAZINE OF LATEX CLASS FILES, VOL. 14, NO. 8, JULY 2025
1
CORE: Toward Ubiquitous 6G Intelligence Through
Collaborative Orchestration of Large Language
Model Agents Over Hierarchical Edge
Zitong Yu, Boquan Sun, Yang Li, Zheyan Qu, Xing Zhang
Abstract—Rapid advancements in sixth-generation (6G) net-
works and large language models (LLMs) have paved the way
for ubiquitous intelligence, wherein seamless connectivity and
distributed artificial intelligence (AI) have revolutionized various
aspects of our lives. However, realizing this vision faces significant
challenges owing to the fragmented and heterogeneous computing
resources across hierarchical networks, which are insufficient
for individual LLM agents to perform complex reasoning tasks.
To address this issue, we propose Collaborative Orchestration
Role at Edge (CORE), an innovative framework that employs
a collaborative learning system in which multiple LLMs, each
assigned a distinct functional role, are distributed across mo-
bile devices and tiered edge servers. The system integrates
three optimization modules, encompassing real-time perception,
dynamic role orchestration, and pipeline-parallel execution, to
facilitate efficient and rapid collaboration among distributed
agents. Furthermore, we introduce a novel role affinity scheduling
algorithm for dynamically orchestrating LLM role assignments
across the hierarchical edge infrastructure, intelligently match-
ing computational demands with available dispersed resources.
Finally, comprehensive case studies and performance evaluations
across various 6G application scenarios demonstrated the efficacy
of CORE, revealing significant enhancements in the system
efficiency and task completion rates. Building on these promising
outcomes, we further validated the practical applicability of
CORE by deploying it on a real-world edge-computing platform,
that exhibits robust performance in operational environments.
Index Terms—6G networks, Large language models (LLMs),
AI agents, Hierarchical edge computing, Ubiquitous intelligence.
I. INTRODUCTION
T
HE convergence of sixth-generation (6G) wireless net-
works and large language models (LLMs) heralds a trans-
formative era of ubiquitous intelligence, reshaping interactions
in domains such as smart cities, healthcare, and industrial
automation [4]. Unlike their predecessors, 6G networks offer
ultralow latency, extensive connectivity, and unprecedented
data rates, thereby enabling LLM-powered AI agents at the
network edge. These agents, transitioning from centralized
cloud-based AI to distributed, edge-native entities, leverage
pervasive computational resources to deliver context-aware,
real-time services. Ubiquitous intelligence envisions a future in
which AI agents, empowered by LLMs, operate autonomously
and collaboratively to provide personalized, efficient, and
Zitong Yu, Boquan Sun, Yang Li, Zheyan Qu, Xing Zhang (corresponding
author) are with the Beijing University of Posts and Telecommunications,
Beijing, China. E-mail: hszhang@bupt.edu.cn.This work was supported by
the National Science Foundation of China under Grant 62271062.
scalable solutions. This paradigm shift enhances user expe-
rience and unlocks the full potential of 6G ecosystems [7],
which is promising for redefining human-machine interac-
tions with unprecedented levels of automation, personaliza-
tion, and efficiency. Current approaches to deploying LLMs
in 6G networks often rely on centralized cloud systems or
isolated edge devices, but both face limitations. Centralized
solutions incur high latency owing to the distance between
users and data centers, making them unsuitable for time-
critical applications, such as autonomous driving or healthcare
diagnostics. Conversely, edge devices, constrained by limited
computational power, struggle with complex tasks and can
become overburdened, degrading performance and increasing
energy use. These shortcomings highlight the need for a new
approach for effectively harness 6G’s distributed resources
effectively.
The fragmented and heterogeneous nature of computing
resources across 6G networks presents significant challenges.
Computational power spans resource-constrained mobile de-
vices, edge servers, and cloud infrastructure, each with vary-
ing capabilities and constraints [10]. Individual LLM agents,
tasked with complex reasoning activities such as multi-modal
perception, dynamic decision-making, or natural language
understanding often lack the resources to operate efficiently
in isolation. In time-sensitive applications such as autonomous
vehicles immersive digital twins, or real-time healthcare di-
agnostics, low-latency collaboration among agents is critical.
However, the current landscape, characterized by isolated
and overburdened resources, hinders effective agent coordi-
nation and limits the scalability and reliability of AI-driven
services [9]. Addressing these challenges is technically im-
perative to realize the transformative potential of ubiquitous
intelligence in 6G networks.
To overcome these obstacles, we propose a Collaborative
Orchestration Role at Edge (CORE), an innovative framework
designed to enable collaborative execution of user-agent inter-
active tasks in 6G networks. CORE leverages a collaborative
learning system in which multiple LLMs, each assigned a
distinct functional role, are distributed across mobile devices
and tiered-edge servers. It integrates real-time perception,
dynamic role orchestration, and pipeline-parallel execution
to enable efficient agent collaboration. The effectiveness of
CORE is demonstrated through its deployment on a real-world
edge computing platform for industrial automation, where it
powers a multi-agent system for real-time anomaly detection.
Specifically, CORE decomposes complex tasks into man-
arXiv:2601.21822v1  [cs.AI]  29 Jan 2026


--- Page 2 ---
MAGAZINE OF LATEX CLASS FILES, VOL. 14, NO. 8, JULY 2025
2
ageable subtasks, assigns them to specialized LLM roles,
and orchestrates their execution across the network hierar-
chy. Network-aware orchestration optimizes resource alloca-
tion by considering latency, bandwidth, and device capabili-
ties, whereas pipeline-parallel execution enhances throughput
through concurrent processing across tiers. Furthermore, we
present a novel role-affinity scheduling algorithm that dy-
namically assigns LLM roles to available resources based
on the task requirements, network conditions, and device
capabilities. This algorithm improves system adaptability by
minimizing execution delays while ensuring task completion
quality, even in dynamic 6G environments characterized by
fluctuating workloads and network variability. By harmonizing
these components, CORE enables seamless user-agent interac-
tions and supports the deployment of robust AI-driven services
across diverse 6G scenarios.
Overall, the primary contributions of this article are sum-
marized as follows:
• We introduced CORE to facilitate more efficient collabo-
ration among distributed multi role agents within 6G net-
works and validated its effectiveness through deployment
on an edge-computing platform for industrial automation.
• We present an optimization framework that effectively
integrates real-time perception, dynamic orchestration,
and pipeline-parallel execution to enhance the task com-
pletion rate and system efficiency of multi-agent systems
in diverse 6G scenarios.
• We developed a role-affinity scheduling algorithm de-
signed to optimize role assignments among distributed
agents, thereby effectively utilizing hierarchical edge re-
sources within 6G networks.
II. 6G-ENABLED LLM AGENTS: ADVANCING
UBIQUITOUS INTELLIGENCE
The integration of sixth-generation (6G) wireless networks
with Large Language Model (LLM) agents represents a sig-
nificant advancement in the field of intelligent systems. This
section examines the opportunities and challenges associated
with this convergence, highlighting the technological enablers
of 6G and the crucial role of LLM agents in the development
of smart networks.
A. 6G Technologies Empowering LLM Agents
1) Opportunities of 6G for Advancing LLM Agents: The
advanced capabilities of 6G networks substantially enhance
LLM agents through significant advancements in data capacity,
latency, connectivity, and AI-native designs. With a per-user
bandwidth of 1 Tbps, 6G facilitates the real-time transmission
of multi-modal data, such as 4K/8K video and 3D point
clouds, thereby enabling LLM agents to execute sophisticated
multi-modal fusion and process complex data streams [6].
The ultralow air interface latency of 6G, which is less than
1 ms, in conjunction with edge computing, supports rapid
”perception-decision-execution” cycles [12]. Furthermore, its
extensive connectivity, which accommodates up to 106 devices
per square kilometer, allows LLM agents to aggregate real-
time data from expansive smart city sensor networks, thereby
Fig. 1. Multi AI Agents in Ubiquitous 6G Intelligence.
improving situational awareness and decision accuracy [4].
Notably, 6G’s integrated sensing and communication (ISAC)
facilitates synesthesia-like multi-modal fusion, which is vital
for applications in autonomous driving and healthcare [7]. Its
AI-native architecture, which utilizes network slicing and Net-
work Function Virtualization(NFV), supports self-optimizing
networks, where LLM agents can predict traffic, dynamically
allocate resources, and optimize efficiency, rendering 6G in-
dispensable for enhancing the responsiveness and scalability
of LLM agents [13].
2) Ubiquitous 6G Intelligence: In the 6G era, the con-
cept of ubiquitous intelligence is fundamentally based on
the integration of LLM agents within a hierarchical edge-
cloud architecture designed to optimally balance performance,
efficiency, and latency. This tripartite framework consists of
the device layer, which is responsible for managing highly
latency-sensitive local tasks, such as on-device video analysis;
the edge layer, which performs intermediate processing tasks,
including real-time language understanding for smart homes,
thereby significantly reducing the transmission of raw data to
the cloud [11]; and the cloud layer, which supports complex
cross-domain model training and global optimization. This
proximate layered design minimizes latency while ensuring
scalability and robustness. Leveraging this architecture and
6G’s ultra-low latency and high bandwidth, LLM agents drive
transformative applications across diverse sectors, as illus-
trated in Figure 1. In smart cities, traffic signals are optimized,
energy grids are managed, and anomalies are detected. For
space-air-ground integrated networks, LLMs synthesize data
from satellites, drones, and terrestrial sensors to enhance preci-
sion agriculture and coordinate disaster responses. In industrial
automation, they predict equipment failures and optimize
production lines, whereas in smart healthcare, they enable
remote monitoring, medical image analysis, and telemedicine
[9], [10]. Thus, the synergy between the 6G hierarchical
architecture and LLM agents constitutes the core technolog-
ical pathway towards truly pervasive and adaptive intelligent
systems.


--- Page 3 ---
MAGAZINE OF LATEX CLASS FILES, VOL. 14, NO. 8, JULY 2025
3
3) Coexistence and Tension: LLMs in the URLLC Context:
Sixth-generation (6G) networks necessitate stringent ultra-
reliable low-latency communication (URLLC) requirements,
which demand sub-millisecond latency and near-perfect reli-
ability for real-time control applications. However, even the
most advanced LLMs operating on edge devices typically
require hundreds of milliseconds for inference, rendering them
fundamentally incompatible with these constraints. Conse-
quently, a latency-aware task-allocation strategy is impera-
tive. LLMs should be allocated to delay-tolerant high-value
tasks, such as predictive maintenance scheduling, long-term
urban planning, and offline data analysis, where their supe-
rior reasoning and generalization capabilities offer distinct
advantages. In these contexts, techniques such as pipeline
parallelism can further enhance the throughput without com-
promising the system objectives. Conversely, safety-critical
tasks that necessitate strict URLLC compliance, including
collision avoidance, remote surgery, and industrial safety,
require predictable sub-millisecond responses. These functions
must rely on simple rule-based systems or highly optimized
lightweight models, rather than LLMs. Thus, the effective
integration of LLMs in 6G depends on hybrid architectures:
LLMs function at a higher hierarchical level to evaluate the
system state and issue high-level directives, whereas URLLC-
compliant agents execute immediate perception and actuation.
B. LLM Agents: The Neurons of the Smart Network
1) Roles of LLM Agents in 6G Networks: LLM agents play
a crucial role in harnessing the capabilities of 6G networks
and act as intelligent facilitators across various functional
domains. As perception orchestrators, LLMs synthesize multi-
modal data, including LiDAR point clouds and camera feeds,
into coherent semantic representations. In the realm of traf-
fic monitoring, the integration of CLIP-based models has
markedly enhanced object detection accuracy, achieving im-
provements of 10–20 pp in certain scenarios [13].As dynamic
reasoning engines, LLMs respond instantaneously to network
behaviour to mitigate the resource limitations. For instance,
during virtual reality streaming congestion, LLMs employ
reinforcement learning techniques, such as Proximal Policy
Optimization (PPO) and Dynamic Policy Optimization (DPO),
to dynamically enhance model precision and resource alloca-
tion. This approach reduces bandwidth consumption by 20–
50% while maintaining high frame rates (e.g., 60 FPS) [14].
In addition, LLMs serve as network intelligence coordinators
that manage tasks across distributed edge nodes. In wildfire
response scenarios, LLM agents coordinate drone fleets to
disseminate satellite imagery analyses, thereby significantly re-
ducing processing time. In the context of facilitating federated
learning, LLMs integrate hospital AI models with differential
privacy, thereby enhancing cancer detection performance while
significantly mitigating privacy risks through the application
of differential privacy [8], [10].By leveraging the ultralow
latency and high bandwidth of 6G, LLM agents enable robust,
adaptive, and privacy-preserving operation.
2) Deployment Challenges: Integrating LLM agents into
6G networks holds transformative potential; however, it also
presents significant challenges. The substantial computational
and memory requirements of LLMs impose a considerable
strain on edge infrastructure. Numerous LLM-based edge
tasks fail because of substantial memory demands that fre-
quently exceed the limited resources available on edge servers.
Although quantization alleviates memory pressure, it often
compromises accuracy, as evidenced by potential increases in
error rates in sensitive tasks, such as medical diagnosis [14],
[15]. Furthermore, distributed execution exacerbates LLM er-
ror propagation and inconsistency. In medical diagnostics, dis-
crepancies among edge nodes can increase error rates, partic-
ularly in safety-critical applications such as medical diagnos-
tics, thereby threatening safety-critical reliability. Additionally,
interoperability and synchronization remain problematic; a
notable portion of cross-edge tasks may fail because of timing
mismatches, network delays, and heterogeneous frameworks,
impairing real-time applications such as traffic management,
whereas heterogeneous APIs and frameworks further impede
seamless collaboration and scalability. Addressing these chal-
lenges necessitates concerted advances in model compression,
energy-efficient inference, robust distributed consensus mech-
anisms, and standardized edge-AI interfaces. Only through
such innovations can LLM-enabled 6G systems achieve the
requisite balance of performance, reliability, and efficiency.
III. CORE FRAMEWORK ARCHITECTURE
The Collaborative Orchestration Role at Edge (CORE)
framework constitutes an innovative methodology for orches-
trating Large Language Model (LLM) agents within sixth-
generation (6G) wireless networks, thereby facilitating the effi-
cient and scalable execution of interactive tasks between users
and agents. By addressing the challenges posed by resource
heterogeneity and the demands for ultra-low latency, CORE
enables the integration of ubiquitous intelligence across a wide
range of applications, including smart cities and healthcare.
This section outlines the architecture of CORE, detailing its
system overview, collaborative learning system, multi-tiered
optimization framework, and role affinity scheduling algo-
rithm, as illustrated in the framework architecture (Figure 2).
A. System Overview
The CORE framework is structured into three primary
layers: the Feedback and Optimization Layer, the Primary
Service Layer, and 6G Infrastructure Layer, which are de-
signed to optimize resource utilization and ensure adaptability
in dynamic 6G environments.
The Feedback and Optimization Layer drives continuous
improvement through components such as task evaluation,
reflection and optimization, short-term memory, long-term
memory, and self-evolution. Short-term memory retains recent
task contexts and immediate outcomes within a sliding win-
dow, whereas long-term memory persistently stores validated
agent strategies, historical performance patterns and lessons
learned. Both memories are exposed as invocable tools through
standardized API interfaces, enabling the reflection module to
query past experiences, reinforce successful orchestration de-
cisions, and avoid previously identified erroneous scheduling


--- Page 4 ---
MAGAZINE OF LATEX CLASS FILES, VOL. 14, NO. 8, JULY 2025
4
Fig. 2. Overview of the CORE framework’s three-layer architecture. The top layer is the Feedback and Optimization Layer, the middle layer is the Primary
Service Layer where the main CORE modules reside, and the bottom layer is the 6G Infrastructure Layer providing underlying support.
and role assignments. Task evaluation assesses performance
metrics, whereas Reflection and Optimization leverage these
memory tools to iteratively refine policies and support self-
evolution.
The Primary Service Layer comprises multi-modal percep-
tion, dynamic role orchestration and scheduling, retrieval-
augmented generation and pipeline-parallel task execution.
multi-modal Perception processes diverse data inputs (e.g.,
video, location, and text) via Mobile Agents to perform real-
time anomaly detection. Dynamic Role Orchestration and
Scheduling, enhanced by external knowledge, plans, and as-
signs tasks by coordinating mobile role agents (e.g., Ma-
pAgent, VideoAgent) through edge agents and role affinity
scheduling, thereby optimizing the utilization of fragmented
computational resources and enhancing task completion rates.
Pipeline-parallel task execution leverages directed acyclic
graphs (DAGs) and the Model Context Protocol (MCP) to
coordinate the use of external API tools to achieve this
parallelism.
The
6G
Infrastructure
Layer
underpins
CORE
with
technologies such as Integrated Sensing and Communica-
tion (ISAC), Ultra-Reliable Low-Latency Communication
(URLLC), Digital Twins, Edge Intelligence, and the Cloud-
Edge-Device Continuum, ensuring robust communication and
computation [5].
B. Collaborative Learning System
The collaborative learning system of CORE distributes
LLMs across mobile devices and tiered-edge servers and
assigns distinct functional roles to leverage their specialized
capabilities. This system ensures efficient resource utilization
and seamless inter-agent coordination. LLMs are strategically
deployed on mobile devices for lightweight tasks, such as
local feature extraction, and on edge servers for computation-
ally intensive operations, such as semantic analysis, which
minimizes latency by processing data near its source. Each
LLM is assigned a role based on task requirements and
device capabilities, such as perception, decision making, or
communication.
Inter-agent coordination is facilitated by the Model Context
Protocol (MCP), a domain-agnostic context management and
transfer mechanism that ensures a consistent contextual un-
derstanding across heterogeneous technological domains. The
MCP achieves cross-domain interoperability through standard-
ized semantic abstractions, lightweight protocol adapters, and
ontology-based payload mapping, enabling seamless context
exchange between, for instance, ISAC-generated sensing meta-
data in the infrastructure layer and application-specific API
states in the service layer. Directed acyclic graphs (DAGs)
complement the MCP by explicitly modeling task depen-
dencies and execution order, allowing agents in disparate
domains, ranging from satellite-based MapAgents to ground-
based VideoAgents, to maintain unified workflow awareness
and achieve synergistic task completion [8].
C. Multi-tiered Optimization Framework
The multi-tiered optimization framework integrates real-
time multi-modal perception, dynamic role orchestration, and
pipeline-parallel execution to address the resource heterogene-
ity and latency challenges in 6G networks.
Real-time multi-modal perception employs Mobile Agents
to process diverse data streams, including videos, locations,
and text, using techniques such as object detection and
anomaly identification. Dynamic role orchestration assigns


--- Page 5 ---
MAGAZINE OF LATEX CLASS FILES, VOL. 14, NO. 8, JULY 2025
5
Fig. 3. Case study demonstrating dynamic role orchestration in the CORE framework for drone-based emergency rescue. A vehicle fire incident is processed
through multi-modal perception, task decomposition, parallel sub-task execution by specialized agents via terminal-edge collaboration, and final evaluation.
roles based on task requirements, network conditions, and
device capabilities. Network-Aware Dynamic Role Load Anal-
ysis evaluates bandwidth and latency, whereas Role Affinity
Scheduling optimizes task assignments. In critical scenarios,
such as emergency management, where a digital twin (DT)
may be invoked to test and refine scheduling decisions, the
framework employs a predictive strategy to guarantee ultra-
low latency. Instead of conducting computationally intensive,
online DT simulations during each decision cycle, CORE
leverages historical data and offline DT simulations to pre-
train a lightweight “decision evaluation model” and to generate
a “scheduling policy lookup table.” During online operation,
the scheduling algorithm directly queries this model or table,
achieving the benefits of a DT-based assessment with minimal
computational overhead and thereby avoiding the high latency
of real-time simulations. This approach ensures that even DT-
enhanced orchestration complies with the stringent latency
requirements of time-sensitive applications.
The pipeline-parallel execution decomposes tasks into sub-
tasks that are executed concurrently across agents. DAGs
ensure proper sequencing, and MCP provides contextual
synchronization, thereby enhancing the throughput for data-
intensive tasks, such as real-time video processing in smart
cities. This framework ensures a high performance and low
latency, which are critical for time-sensitive 6G applications.
D. Role Affinity Scheduling Algorithm
The Role Affinity Scheduling algorithm optimizes task
assignments to LLM agents, adapting to the dynamic con-
ditions of 6G environments. Its design leverages an affinity-
driven approach, wherein agents are selected based on their
suitability for specific tasks, which are calculated using task
requirements, network conditions, and device capabilities.
The algorithm considers task complexity, data volume, and
real-time constraints, along with network parameters such as
bandwidth, latency, and reliability. Device capabilities, includ-
ing processing power, memory, and energy efficiency, were
also evaluated to prevent overload or battery depletion. For
instance, a task requiring high-speed processing is assigned to
a powerful edge server, whereas an energy-constrained task
targets a low-power device.
Adaptive mechanisms enable the algorithm to respond to
fluctuating workloads and network variability. Continuous
monitoring of network states and agent performance allows
for the dynamic reassignment of tasks. If an agent becomes
overloaded or experiences latency spikes, the algorithm redis-
tributes the tasks to maintain performance. This adaptability
ensures efficient resource utilization and reliable task execu-
tion across the diverse 6G scenarios.
IV. CASE STUDY
The Collaborative Orchestration Role at Edge (CORE)
framework was implemented on a real-world edge computing
platform, illustrating its potential to transform time-sensitive
applications, such as industrial automation and emergency
response. As shown in Figure 5, the user interface of the
platform offers intuitive control and monitoring. A significant
application of CORE is its use in the analysis of highway
vehicle fire incidents (Figure 3), where CORE integrates multi-
modal data, including video feeds, geographical information,
and weather conditions, to develop comprehensive emergency
response plans. CORE achieves high-accuracy and low-latency


--- Page 6 ---
MAGAZINE OF LATEX CLASS FILES, VOL. 14, NO. 8, JULY 2025
6
inference in real-time decision-making by integrating multi-
level computing power networks, which is crucial for mitigat-
ing the impact of such accidents and enhancing emergency
management.
A. Experiment Deployment Environment
The deployment environment was a hierarchical edge-cloud
architecture tailored for low-latency and high-efficiency task
processing purposes. At the core of this setup, the Deepseek-
R1-distill-llama-70B model served as the task scheduler, run-
ning on eight NVIDIA A40 GPUs, each equipped with 48GB
of VRAM. This configuration provides robust computational
power for dynamic role orchestration and decision-making. At
the edge, the MiniCPM-v2.6 model (8B parameters) was de-
ployed on devices powered by NVIDIA 3090 and 4090 GPUs,
enabling localized processing and reducing the dependency on
cloud resources. Inter-agent coordination is facilitated by the
Model Context Protocol (MCP), which ensures seamless data
sharing and task execution across the system. This setup sup-
ports pipeline parallel execution and adaptive task allocation
and optimizes performance under varying network conditions.
B. Performance Evaluation
1) Metrics and Benchmarks: The performance was eval-
uated using two key metrics: the task completion rate and
latency. The task completion rate measures the percentage
of successfully completed tasks across three difficulty lev-
els: easy (0–3 tools), medium (4–6 tools), and hard (7–
9 tools). Latency is divided into scheduling latency (task
arrival to assignment) and execution latency (assignment to
completion). These metrics were compared with single-agent
algorithms (ReAct [1], LLMCompiler [2]) and multi-agent al-
gorithms (Static Dual Loop, Crew Ai [3]). Static Dual Loop
is is a multi-agent manufacturing dual-loop architecture with
fixed role arrangements. The evaluation dataset comprised
an open-source video dataset (https://github.com/steffensbola/
furg-fire-dataset) of anomalous behaviors (e.g., vehicle fires)
and 300 task instructions generated by a large language model,
manually curated for accuracy and representativeness.
2) Results and Analysis: The experimental results indicate
that CORE demonstrates superior task completion rates and
reduced latency, particularly in complex scenarios (Figure 4).
CORE significantly outperforms single-agent methods, such as
ReAct and LLMCompiler, across all levels of difficulty and
exceeds the performance of Static Dual Loop by 25% and
20% on Medium and Hard tasks, respectively. This superiority
is attributed to CORE’s dynamic task decomposition and spe-
cialized multi-agent collaboration of CORE, which effectively
address the scalability limitations inherent in single-agent
systems and the inflexibility of static scheduling in managing
multi-tool dependencies.
The latency performance, as illustrated in Figure 6, under-
scores the efficiency of the CORE. Subfigure (a) demonstrates
that DynaRole-HEFT, the approach employed by CORE, sur-
passes the traditional HEFT and static assignment methods
across various load conditions, exhibiting significantly reduced
latencies and effective task allocation, which is advantageous
Fig. 4. Task completion rate comparison.
Fig. 5. The CORE platform for real-time anomaly detection.
for time-sensitive applications. Subfigure (b) depicts the scal-
ability of CORE. Compared with other methodologies, CORE
combined with DynaRole-HEFT maintains competitive end-
to-end latency and significantly outperforms the static alloca-
tion of Crew Ai in multi-tool tasks.
3) Summary:
The quantitative findings substantiate the
superiority of CORE over the existing methodologies. Specif-
ically, DynaRole-HEFT achieves a 52% reduction in high-
load latency relative to the traditional HEFT and en-
hances the medium-task success rate by 25% compared to
Static Dual Loop. These improvements are driven by the
pipeline parallel execution and context-aware task manage-
ment of CORE, which collectively enhance the system effi-
ciency and reliability. However, challenges in complex task
performance and hardware generalizability remain, underscor-
ing the need for further optimization to fully realize the
potential of CORE in diverse 6G ecosystems in the future.
Figure 6(a) further demonstrates that although DynaRole-
HEFT substantially decreases the overall latency, the inference
time of the orchestrator (ranging from 180 to 320 ms on
a server equipped with eight NVIDIA A40 GPUs) accounts
for 25–40% of the total end-to-end delay under high-load
conditions. This underscores the need for the development of
lightweight scheduler designs for applications requiring sub-
10 ms latency, such as remote surgery.
V. CONCLUSIONS AND FUTURE DIRECTIONS
The CORE framework facilitates the efficient deployment of
6G technology by collaboratively orchestrating LLM agents


--- Page 7 ---
MAGAZINE OF LATEX CLASS FILES, VOL. 14, NO. 8, JULY 2025
7
Fig. 6. Latency comparison: (a) Latency breakdown under varying loads; (b) End-to-end latency with increasing tool count.
across mobile devices and edge servers. This orchestration
forms a ”collective AI brain” that provides rapid and scal-
able AI services in domains such as smart cities, healthcare,
industrial automation, and emergency response. Empirical
evaluations demonstrate its superior performance in terms of
both the task completion rate and latency. To address the
remaining overhead associated with the orchestrator, future
research should focus on developing lightweight scheduler de-
signs through model distillation and specialization, particularly
for applications requiring sub-10ms latency, such as remote
surgeries. Additional research directions include integration
with 6G network slicing for enhanced resource control, explo-
ration of quantum-inspired algorithms for complex scheduling
tasks, and standardization of inter-agent communication pro-
tocols to ensure seamless operation across heterogeneous 6G
ecosystems.
REFERENCES
[1] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao,
“ReAct: Synergizing Reasoning and Acting in Language Models,” in
Proc. 36th Conf. Neural Inf. Process. Syst. (NeurIPS), New Orleans, LA,
USA, 2022, pp. 1–13.
[2] S. Kim, S. Moon, R. Tabrizi, N. Lee, M. W. Mahoney, K. Keutzer, and
A. Gholami, “An LLM Compiler for Parallel Function Calling,” in Proc.
41st Int. Conf. Mach. Learn. (ICML), Vienna, Austria, PMLR 235, 2024,
pp. 123–135.
[3] CrewAI,
“CrewAI:
A
Framework
for
Orchestrating
Role-
Playing,
Autonomous
AI
Agents,”
2023.
[Online].
Available:
https://github.com/crewAIInc/crewAI
[4] M. Xu, D. Niyato, J. Kang, Z. Xiong, S. Mao, Z. Han, D. I. Kim, and K.
B. Letaief, “When Large Language Model Agents Meet 6G Networks:
Perception, Grounding, and Alignment,” IEEE Wireless Commun., early
access, Aug. 2024, doi: 10.1109/MWC.005.2400019.
[5] L. Lov´en, M. Bordallo L´opez, R. Morabito, J. Sauvola, and S. Tarkoma,
Eds., “Large Language Models in the 6G-Enabled Computing Continuum:
A White Paper,” White Paper, 6G Research Visions, No. 14, Univ. Oulu,
Oulu, Finland, 2025. [Online]. Available: https://urn.fi/URN:NBN:fi:oulu-
202501211268
[6] C.-N. Hang, P.-D. Yu, R. Morabito, and C.-W. Tan, “Large Lan-
guage Models Meet Next-Generation Networking Technologies: A Re-
view,” Future Internet, vol. 16, no. 10, art. no. 365, Oct. 2024, doi:
10.3390/fi16100365.
[7] Q. Cui, X. You, W. Ni, G. Nan, X. Zhang, J. Zhang, X. Lyu, M. Ai, X.
Tao, Z. Feng, P. Zhang, Q. Wu, M. Tao, Y. Huang, C. Huang, G. Liu, C.
Peng, Z. Pan, T. Sun, D. Niyato, T. Chen, M. K. Khan, A. Jamalipour,
M. Guizani, and C. Yuen, “Overview of AI and Communication for 6G
Network: Fundamentals, Challenges, and Future Research Opportunities,”
Sci. China Inf. Sci., vol. 68, no. 4, Apr. 2025, doi: 10.1007/s11432-024-
4337-1.
[8] F. Jiang, K. Wang, Y. Wang, Y. Zhang, Y. Li, Y. Zhang, and Y.
Zhang, “Large Language Model Enhanced Multi-Agent Systems for 6G
Communications,” arXiv:2312.07850, Dec. 2023.
[9] L. Zheng, “Pushing Large Language Models to the 6G Edge: Vision,
Challenges, and Opportunities,” arXiv:2309.16739, Sep. 2023.
[10] Y. Wang, Y. Zhang, Y. Li, Y. Zhang, and K. Wang, “A Review on Edge
Large Language Models: Design, Execution, and Applications,” ACM
Comput. Surv., vol. 57, no. 4, pp. 1–38, Apr. 2025, doi: 10.1145/3719664.
[11] J. Zhang, Z. Liu, Y. Zhu, E. Shi, B. Xu, C. Yuen, D. Niyato, M. Debbah,
S. Jin, B. Ai, and X. Shen, “Multi-Agent Reinforcement Learning in
Wireless Distributed Networks for 6G,” arXiv:2502.05812, Feb. 2025.
[12] R.-A. Stoica and G. T. F. de Abreu, “6G: The Wireless Communications
Network for Collaborative and AI Applications,” arXiv:1904.03413, Apr.
2019.
[13] T. B. Ahammed, R. Patgiri, and S. Nayak, “A Vision on the Artificial
Intelligence for 6G Communication,” ICT Express, vol. 9, no. 2, pp. 197–
210, Apr. 2023, doi: 10.1016/j.icte.2022.05.005.
[14] Y. Wang, Y. Zhang, Y. Li, Y. Zhang, and K. Wang, “Mobile Edge
Intelligence for Large Language Models: A Contemporary Survey,”
arXiv:2407.18921, Jul. 2025.
[15] H. Wang, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez,
H. Zhang, and I. Stoica, “TinyChat: Large Language Model on the Edge,”
arXiv:2309.06179, Sep. 2023.
VI. BIOGRAPHY SECTION
Zitong Yu Zitong Yu (yztong@bupt.edu.cn) is currently pursuing an M.S.
degree at Beijing University of Posts and Telecommunications, China. His
research interests include UAV swarm coordination, multi-agent systems, and
large language models.
Boquan Sun Boquan Sun (boquan sun@bupt.edu.cn) is currently pursuing an
M.S. degree at Beijing University of Posts and Telecommunications, China.
His research interests include edge computing, UAV navigation systems, and
large language models.
Yang Li Yang Li (ly209991@bupt.edu.cn) is currently pursuing a Ph.D.
degree at Beijing University of Posts and Telecommunications, China. His
research interests include device-assisted mobile edge networks, computing
offloading, and resource allocation.
Zheyan Qu Zheyan Qu (zheyanqu@bupt.edu.cn) is currently pursuing an
M.S. degree at Beijing University of Posts and Telecommunications, China.
His current research interests include edge intelligence, multi-agent systems,
and generative AI.
Xing Zhang Xing Zhang (zhangx@ieee.org) is a full professor with the
School of Information and Communications Engineering, Beijing University
of Posts and Telecommunications, China. His research interests are mainly in
5G/6G networks, satellite communications, edge intelligence, and Internet of
Things.
