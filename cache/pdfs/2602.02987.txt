--- Page 1 ---
Large-Scale LLM Inference with Heterogeneous
Workloads: Prefill-Decode Contention and
Asymptotically Optimal Control
Ruihan Lina, Zean Hanb, Zezhen Dingc, Jiheng Zhangd
Department of Industrial Engineering and Decision Analytics, The Hong Kong University of Science and Technology
arlinah@connect.ust.hk, bzhanax@connect.ust.hk, czdingah@connect.ust.hk, djiheng@ust.hk
Abstract.
Large Language Models (LLMs) are rapidly becoming critical infrastructure for enterprise applications,
driving unprecedented demand for GPU-based inference services. A key operational challenge arises from the
two-phase nature of LLM inference: a compute-intensive prefill phase that processes user input, followed by a
memory-bound decode phase that generates output tokens. When these phases share GPU resources, prefill tasks
throttle the processing speed of concurrent decodes, creating state-dependent contention. This contention is further
complicated by workload heterogeneity, as different applications exhibit vastly different input and output lengths.
We develop a stochastic control framework for scheduling heterogeneous LLM workloads across large GPU
clusters. We formulate LLM inference as a multiclass many-server queueing network with state-dependent service
rates, grounded in empirical iteration-time measurements. We analyze the fluid approximation of this system
and solve steady-state linear programs that characterize optimal resource allocation. We design gate-and-route
policies that regulate prefill admission and decode routing, and prove that they are asymptotically optimal in
the many-GPU limit under both bundled and separate token-pricing schemes. We further extend the framework
to incorporate Service Level Indicators (SLIs) such as latency and fairness, providing a general approach to
constrained scheduling. Numerical experiments calibrated to empirical iteration-time data demonstrate that our
policies outperform standard serving heuristics.
Key words: Stochastic control, Queueing network, Large language models, Revenue management
1.
Introduction
Large Language Models (LLMs) have emerged as a foundational technology in contemporary artificial
intelligence, leading to a substantial increase in computational demand (Zhao et al. 2025). To accommodate this
growth, production-scale infrastructures have expanded correspondingly, frequently requiring the concurrent
utilization of thousands of GPUs to sustain worldwide inference workloads (Kwon et al. 2023, Aminabadi
et al. 2022). Since commercial LLM services predominantly adopt per-token pricing (OpenAI 2025, Google
Cloud 2025, Anthropic 2025, Google 2025, DeepSeek 2025), revenue is closely tied to the volume and
composition of tokens processed, making efficient resource allocation critical for both profitability and user
experience.
Prefill‚Äìdecode contention. A distinguishing feature of LLM inference is that each request proceeds in
two stages: a prefill stage, in which the model processes the user‚Äôs input prompt, followed by a decode stage,
1
arXiv:2602.02987v1  [cs.DC]  3 Feb 2026


--- Page 2 ---
2
in which it generates output tokens autoregressively. Modern serving systems batch multiple requests on the
same GPU to exploit the GPU‚Äôs parallel processing capability, but these two stages interact in nontrivial
ways when sharing GPU resources. Prefill is compute-intensive and, when present in a batch, dominates the
iteration time and thereby throttles the processing speed of co-located decode tasks; decode, in contrast, is
memory-bound and proceeds faster when running alone (Kwon et al. 2023, Agrawal et al. 2024). Moreover,
iteration time grows roughly linearly in the amount of prefill work, so adding a second prefill to a batch
would nearly double the iteration time without improving parallelism. For this reason, practical systems
process at most one prefill per GPU at a time. This prefill-decode contention creates a fundamental scheduling
tension: admitting more prefills increases the rate at which new requests enter the system but slows down all
concurrent decodes, and the scheduler must carefully balance how many GPUs are devoted to prefill versus
decode-only operation.
Workload heterogeneity. Real-world LLM services do not see a single homogeneous stream of requests.
They serve a mix of applications, such as summarization, creative writing, and question answering, which
differ widely in typical input and output lengths (Sun et al. 2024, Zheng et al. 2024, Zhao et al. 2024). For
example, summarization tasks average over 1,000 input tokens with moderate output, while creative writing
requires fewer than 100 input tokens yet generates over 900 output tokens on average (see Table EC.1 in
the e-companion for detailed statistics). This heterogeneity amplifies the scheduling challenge: a class with
long prefills and short decodes (e.g., summarization) consumes GPU time during prefill but releases decode
capacity quickly, whereas a class with short prefills and long decodes (e.g., creative writing) admits quickly
but occupies decode slots for extended periods. Maximizing revenue requires serving an appropriate mix of
both to balance pipeline utilization. Consequently, this multiclass resource allocation problem is not well
served by simple static priority rules such as first-come-first-served or shortest-job-first; instead, it calls for
finer control over how resources are distributed across classes.
This paper addresses the following question: How should a large-scale LLM inference system jointly
control admission and scheduling across multiple request classes to maximize token-based revenue while
respecting service level indicators (SLIs)? Answering this question requires overcoming several challenges.
First, the state-dependent service rates arising from prefill-decode contention create analytical difficulties
that preclude direct application of standard queueing results. Second, heterogeneous workloads induce a
multiclass resource allocation problem where the optimal policy depends on the composition of the workload
mix. Third, practical systems must balance revenue maximization against SLIs such as latency and fairness
across request classes.
Our approach. We model the system as a multiclass many-server queueing network where each GPU
operates in one of two modes: mixed (running one prefill alongside decodes) or solo (decode-only). Service
rates are state-dependent, specifically, decodes run slower in mixed mode due to intensive computation


--- Page 3 ---
3
of prefill, and we derive these rates from empirical iteration-time measurements in Section 2.2, capturing
essential GPU physics in a tractable analytical framework.
Because production LLM clusters typically comprise hundreds to thousands of GPUs, we study the system
through fluid approximation in the many-server regime, which is a standard and well-established approach
in operations research for analyzing large-scale stochastic networks. In this scaling, stochastic fluctuations
average out and the system trajectory converges to a deterministic limit, yielding both analytical tractability
and high accuracy at scale. The fluid model reduces to a steady-state linear program (LP) whose solution
prescribes how to partition cluster capacity between mixed and solo modes and how to distribute workload
across request classes.
We translate the fluid solution into implementable control via a gate-and-route architecture: a prefill
gate regulates admission by tracking class-level occupancy targets from the LP, and a decode router directs
completed prefills to available GPU slots. This decomposition into static planning (solving the LP) and
dynamic control (enforcing LP targets) is central to our design and underpins the asymptotic optimality
results we establish.
Our contributions. We make the following contributions.
1. Multiclass many-server model with prefill-decode contention. We develop a queueing network where
each GPU operates in mixed or solo mode, with state-dependent service rates that capture how prefill
operations throttle co-located decodes. The service-rate parameters are calibrated from controlled
experiments on production hardware (A100 GPUs) to closely reflect real system behavior.
2. Fluid approximation and LP-based planning. We establish convergence of the scaled stochastic system
to a deterministic fluid limit in the many-server regime. The steady-state analysis reduces to a linear
program that prescribes optimal capacity partitioning between mixed and solo modes and class-level
occupancy targets. This LP formulation provides analytical tractability and serves as the foundation for
the control policies we develop.
3. Asymptotically optimal control policies. We translate the LP solution into implementable gate-and-
route policies and prove their asymptotic optimality. Under bundled charging (revenue credited upon
completion), an occupancy-tracking gate suffices; under separate charging (revenue credited after
each phase), a priority-based policy is needed to counteract incentive distortions that would otherwise
shift congestion downstream. The framework naturally extends to incorporate service-level indicators
(fairness, latency) as constraints or penalties, and we show that enforcing fairness at the prefill stage is
more revenue-costly than at the decode stage. Simulation experiments confirm that per-GPU revenue
converges to the fluid optimum as the cluster scales and that our policies consistently outperform
industry-standard heuristics.


--- Page 4 ---
4
1.1.
Literature Review
Efficient LLM inference serving has become a critical systems challenge as production deployments face
mounting demands for throughput, latency, and resource efficiency. Early systems work concentrated on
improving performance on single GPUs through architectural innovations. Iteration-level batching made
it practical to schedule at token granularity (Yu et al. 2022), while paged attention reduced KV-cache
fragmentation and enabled high utilization under dynamic workloads (Kwon et al. 2023). Chunked prefill
further enabled interleaving prefill chunks with ongoing decodes (Agrawal et al. 2024).
As deployments scaled from single-device prototypes to large-scale production clusters, designers began
allowing the prefill and decode stages to be executed on different GPUs to better match the compute-bound
prefill phase with the memory-bandwidth-bound decode phase (Patel et al. 2024, Zhong et al. 2024). This
transition from single-GPU optimization to cluster-scale orchestration raises new questions that go beyond
engineering heuristics: how should a service provider allocate GPU capacity and route requests across GPUs
under heterogeneous workloads to maximize long-run revenue while considering customized service-level
indicators? Existing serving stacks acknowledge these trade-offs but do not yet provide a formal framework
for revenue-driven resource allocation at scale.
This operational challenge has attracted growing attention from the operations research (OR) and operations
management (OM) communities. Multiple survey articles underscore the growing intersection of AI/LLM
and OR/OM, covering foundational frameworks for AI and operations (Dai and Swaminathan 2026), LLM
foundations and algorithmic innovations (Zhao et al. 2025), efficient inference techniques (Zhou et al. 2024),
serving system advances and opportunities (Li et al. 2024), and the intersection of queueing theory and
predictive scheduling for LLMs (Mitzenmacher and Shahout 2025). Within this broad landscape, research
efforts unfold along two complementary directions. On one hand, researchers are exploring how LLMs
can enhance traditional OR/OM workflows. For instance, Simchi-Levi et al. (2025) leverage generative AI
to democratize optimization, Huang et al. (2025) train LLMs for automated optimization modeling, and
Simchi-Levi et al. (2026) apply LLMs to supply chain decision-making. On the other hand, and more directly
relevant to our work, is the growing body of research that applies OR/OM methodologies to improve LLM
inference itself. Within this OR-for-LLM strand, efforts can be further categorized into two areas: improving
the quality of LLM outputs, such as aggregating responses via higher-order information beyond majority
voting (Ai et al. 2025) or applying online learning frameworks to address missing covariates in pre-trained
AI model assisted decision-making (Hu and Simchi-Levi 2025), and accelerating LLM inference through
principled resource allocation and scheduling, which is the central focus of the present paper.
In the context of accelerating LLM inference, a recent strand of work employs competitive analysis from the
online algorithms community to formalize LLM serving. Zhou and coauthors model KV-cache-constrained
batching and benchmark online schedulers against a hindsight integer program. They show that under fully
adversarial arrivals, no deterministic online algorithm achieves a constant competitive ratio; under additional


--- Page 5 ---
5
assumptions, they provide a polynomial-time scheduler with a constant competitive ratio (Jaillet et al. 2025).
Follow-up work obtains constant-competitive policies when heterogeneous prefill/decode lengths are modeled
directly (Wang et al. 2025) and logarithmic-competitive guarantees under interval predictions of decode
length via an adaptive policy (A min) contrasted with a conservative upper-bound policy (A max) (Chen
et al. 2025).
Meanwhile, the advancement of queueing theory provides an analytical framework for stochastic systems
and control design. In many-server settings, fluid approximations provide tractable first-order descriptions for
capacity planning and performance analysis (Whitt 2006, Zhang 2013), including accuracy guarantees for
sizing under impatience (Bassamboo and Randhawa 2010) and asymptotically optimal scheduling structures
for multiclass systems with abandonment (Atar et al. 2010, Long et al. 2020, 2024). Subsequent work enriches
these models by exploiting within-queue heterogeneity, allowing dependence between service requirements
and patience (Bassamboo and Randhawa 2016, Wu et al. 2019), and analyzing state-dependent service rates
and slowdowns (Dong et al. 2015). In parallel, delay estimation and information-sharing frameworks have been
developed to manage latency considerations in complex service systems (Ibrahim and Whitt 2009, Ibrahim
2018). Finally, the literature addresses methodological concerns such as robustness to model/input uncertainty
in simulation (Lam 2016, Ghosh and Lam 2019), alongside control-oriented heavy-traffic perspectives on
dynamic admission/sequencing (Harrison and Zeevi 2004, Ata 2006) and state-space-collapse analyses in
parallel-server networks (Dai and Tezcan 2011).
Applying these queueing tools to LLM inference, there are growing contemporary theoretical works
that employ stochastic modeling and fluid approximations. Ao et al. (2025) model KV-memory growth
and batch-time linearity and propose threshold-based policies (WAIT and Nested WAIT) that approach
fluid-optimal throughput on a single GPU while keeping latency and time-to-first-token bounded; their analysis
highlights how batching composition and memory constraints jointly drive performance. Complementary
results argue that work-conserving rules achieve optimal throughput under simplified token-time abstractions
(Li et al. 2025), and empirical iteration-time models justify treating per-iteration latency as a piecewise
linear function of tokens advanced (Li et al. 2025). These works provide valuable theoretical foundations for
understanding single-GPU scheduling dynamics.
In contrast, our work focuses on cluster-scale serving with multiple GPUs, which introduces distinct
modeling considerations. First, in disaggregated multi-GPU deployments where decode tasks can be isolated
on dedicated servers, the performance characteristics differ from single-GPU mixed batching: decode
throughput becomes primarily memory-bandwidth bound rather than compute-bound, exhibiting minimal
degradation with batch size. This motivates the piecewise characterization of prefill and decode service
rates described by Li et al. (2025), which aligns more closely with the hardware-level asymmetry between
the two phases. Second, cluster-scale optimization often calls for a many-server framework where capacity
scales by increasing the number of GPUs (ùëõ‚Üí‚àû) while maintaining constant per-device performance, as


--- Page 6 ---
6
opposed to scaling the service rate of a single GPU to infinity as in traditional heavy-traffic analysis. Finally,
our emphasis on revenue maximization under heterogeneous workloads and customizable SLI constraints
complements the throughput-centric perspective of prior work, offering service providers a framework for
balancing commercial objectives with quality-of-service guarantees.
To realize this vision, we construct a multiclass many-server fluid approximation where service rates are
derived directly from an empirical token-time law. By modeling job classes via expected rather than exact
lengths, our framework remains robust to per-request mispredictions while capturing the essential system
dynamic: admitting prefills forces co-located decodes into a slower mixed mode. The resulting steady-state
optimization problem parametrizes a two-level control policy, consisting of a prefill gate and a decode router,
that provably achieves asymptotic optimality in the many-GPU limit.
1.2.
Organization
The remainder of the paper is organized as follows. Section 2 introduces the iteration-time abstraction, the
multiclass many-server stochastic network with mixed and solo decode modes, and the bundled revenue
objective. Section 3 develops the many-GPU fluid limit, formulates the steady-state linear program (LP) over
per-GPU occupancies, and establishes structural properties such as decode-buffer elimination. Section 4
constructs the occupancy-anchored gate-and-route policy, proves its asymptotic optimality under bundled
pricing, and then analyzes the separate prefill/decode charging scheme together with the corresponding
prioritize-and-route policy. Section 5 extends the fluid LP to an SLI-aware formulation that integrates fairness
and latency proxies, and shows convergence of the stochastic occupancies to the SLI-optimal fluid solution.
Section 6 presents numerical experiments with calibrated parameters and synthetic multiclass workloads,
validating the fluid predictions and exploring revenue‚ÄìSLO tradeoffs. Section 7 concludes and discusses
directions for extending the framework. Technical proofs and additional lemmas are collected in the electronic
companion.
2.
Problem Formulation
This section develops the queueing model in three parts. Section 2.1 provides background on modern LLM
inference systems. Section 2.2 characterizes GPU iteration time. Section 2.3 derives the resulting service
rates and embeds them into a many-server stochastic network, specifying the state, flows, admissible controls,
capacity coupling, and revenue objectives.
2.1.
Preliminaries: Modern LLM Inference Systems
Two stages of LLM inference. We formally introduce the two stages in LLM inference, namely prefill
and decode. In the prefill stage, the model reads the entire input prompt once and, layer by layer, builds an
internal representation of all input tokens. Implementations store per-token intermediate states in a key‚Äìvalue
(KV) cache so that, in the subsequent decode stage, the model can generate output tokens one by one by


--- Page 7 ---
7
attending to this cache instead of recomputing all past KV values (Shazeer 2019, Dai et al. 2019, Kwon et al.
2023). Empirical studies show that these two stages stress the hardware differently: prefill behaves like a
large, single-shot matrix computation, while decode repeatedly accesses and extends the KV cache and is
more sensitive to memory traffic (Kwon et al. 2023, Agrawal et al. 2024). This asymmetry is the root cause
of the scheduling trade-offs that we model.
Continuous batching. State-of-the-art serving systems do not run one request at a time per GPU.
Instead, they use continuous or iteration-level batching: in each short iteration, the GPU advances many active
requests by one output token in parallel, then repeats this process for the next token (Yu et al. 2022, Kwon
et al. 2023). In practice, these systems impose a fixed upper bound on how many requests can be batched on a
GPU, chosen for engineering reasons such as avoiding out-of-memory errors and reducing dynamic memory
management overhead (Kwon et al. 2023, Agrawal et al. 2024). We denote this constant by ùêµ.
Chunked prefill and GPU modes. Long prompts make it inefficient to run prefill in isolation. Recent
systems therefore chunk prefills into smaller pieces (with each ùê∂tokens) that can be interleaved with decode
work on the same GPU (Agrawal et al. 2024). Measurements show that once a prefill chunk is present in
an iteration, it tends to dominate the iteration time; running additional prefills in parallel on the same GPU
brings no extra benefit because compute capacity is already saturated (Agrawal et al. 2024). Following this
evidence, we adopt the standard assumption that each GPU runs at most one prefill chunk at a time.
Figure 1 summarizes the resulting GPU-level architecture, which resembles key elements of systems such
as vLLM and Sarathi-Serve (Kwon et al. 2023, Agrawal et al. 2024). New requests from all classes first enter
prefill queues. A host-side Prefill Scheduler selects some queued requests and starts their prefill stage on
GPUs that are not currently running another prefill. While a prefill is in progress, that GPU is in a mixed
mode: one slot is occupied by the prefill chunk, and the remaining at most ùêµ‚àí1 slots can be used to advance
decodes from decode-ready requests of various classes. When the prefill finishes, the request moves into the
decode buffer, representing the set of requests ready to decode, with their KV caches typically kept resident
in GPU memory. The GPU then returns to a solo decode mode, where all activated slots are devoted to
decode. A separate Decode Scheduler continuously fills empty slots on both mixed and solo GPUs from the
multiclass decode buffer. Because a mixed-mode GPU shares its compute and memory bandwidth between
a large prefill and several decodes, the per-token progress of those decodes is slower than on a solo GPU;
admitting more prefills thus increases the rate at which new requests enter decode but slows down decodes
sharing the same GPU.
2.2.
Token Processing on GPUs
To build a tractable stochastic model, we first characterize GPU iteration time. In each iteration, the GPU
processes a batch that may contain both prefill and decode tasks: all decode tasks in the batch each generate
one output token, and if a prefill chunk is present, the GPU processes one chunk of its input tokens. The
iteration time is defined as the duration required to complete this batch processing step.


--- Page 8 ---
8
GPU CLUSTER POOL (n GPUs)
Prefill Task
Decode Task
Prefill Buffer
Class 1
(Coding)
Qn
p,1(t)
Class 2
(General)
Qn
p,2(t)
Class I
(Finance)
Qn
p,I(t)
...
nŒª1
nŒª2
nŒªI
Prefill
Scheduler
Decode Buffer
Class 1
Qn
d,1(t)
Class 2
Qn
d,2(t)
Class I
Qn
d,I(t)
...
Decode
Scheduler
GPU #k
STATE: MIXED GPU
Prefill
Empty
GPU #j
STATE: SOLO GPU
Target
(Prefill)
Empty
B
Admit
Prefill Completed
Admit Prefill
Prefill Finishes
Figure 1
Schematic of the Dynamic GPU Scheduling Architecture. The system manages a cluster of ùëõGPUs
with batch size ùêµ. GPUs transition between the Solo State (decode-only) and Mixed State (one prefill + decodes)
based on assignments from the Prefill Scheduler. Completed prefills enter a virtual Decode Buffer, from which
the Decode Scheduler populates available slots in either state.
Iteration time. A key empirical finding, documented by Li et al. (2025), is that iteration time depends on
the total number of tokens processed in a batch:
ùúè(ùëè‚Ä≤) = ùëê+ ùëé¬∑ max{0, ùëè‚Ä≤ ‚àíùëè0}.
(1)
Here, ùëè‚Ä≤ denotes the effective token count for the iteration, equal to the prefill chunk size plus the number of
concurrent decode tasks. The constant ùëê> 0 captures fixed overheads (e.g., kernel launches), ùëé> 0 is the
marginal cost per token, and ùëè0 ‚â•0 is a threshold below which overheads dominate. This formula captures
two operating regimes:
‚Ä¢ Decode-only iteration: In a decode-only batch, the effective token count ùëè‚Ä≤ equals the batch size, which
is typically smaller than ùëè0. The max term in Equation (1) then vanishes, and the iteration time reduces
to a near-constant value
ùúèsolo = ùúè(ùëè‚Ä≤) = ùëê,
for ùëè‚Ä≤ ‚â§ùëè0.
(2)
‚Ä¢ Mixed-batch iteration: When a batch includes a prefill chunk of size ùê∂, the chunk dominates the
effective token count (i.e., ùëè‚Ä≤ ‚âàùê∂). For practical chunk sizes where ùê∂> ùëè0, iteration time grows linearly


--- Page 9 ---
9
in ùê∂. This is consistent with empirical observations from Sarathi-Serve, where decode iteration time is
largely independent of batch size but prefill iteration time scales linearly with chunk size (Agrawal et al.
2024).
Our experiments in Section 6.1 confirm both regimes: Figure 3 shows that iteration time remains nearly flat
for small batch sizes and grows linearly in ùê∂once a prefill chunk is present.
Since practical chunk sizes typically satisfy ùê∂‚â•ùëè0, we adopt the linear form for mixed-batch iteration
time:
ùúèmix(ùê∂) = ùõº+ ùõΩùê∂,
where ùõº:= ùëê‚àíùëéùëè0 and ùõΩ:= ùëé> 0.
(3)
This two-regime abstraction captures the key prefill‚Äìdecode interaction while remaining analytically tractable.
2.3.
The Stochastic Model
We now embed the iteration-time characterization from Section 2.2 into a multiclass many-server stochastic
network. The model is indexed by the number of GPUs ùëõ‚ààN.
Primitives and service parameters. Requests belong to a finite set of classes I := {1, . . . , ùêº}. A
class-ùëñrequest is characterized by its representative prompt length ùëÉùëñand decode length ùê∑ùëñ(in tokens). The
system consists of ùëõhomogeneous GPUs. Each GPU can host at most ùêµ‚ààN concurrent decode streams and
at most one prefill at a time. Prefill is executed in fixed-size chunks of ùê∂> 0 tokens per iteration.
Recall from Section 2.2 that the iteration time is ùúèsolo = ùëêin decode-only mode and ùúèmix(ùê∂) = ùõº+ ùõΩùê∂
when a prefill chunk is present. For notational convenience, write ùúè:= ùúèmix(ùê∂) for the mixed-iteration time.
Service rates are derived as follows.
‚Ä¢ Prefill rate. A prefill job of length ùëÉùëñtokens advances ùê∂tokens per iteration, each taking time ùúè.
Completing the prefill requires (ùëÉùëñ/ùê∂) iterations, so the mean service time is (ùëÉùëñ/ùê∂)ùúèand the rate is
ùúáùëù,ùëñ= ùê∂
ùëÉùëñùúè.
‚Ä¢ Mixed decode rate. In mixed mode, a decode job produces one token per iteration. A class-ùëñjob needs
ùê∑ùëñtokens, so the mean service time is ùê∑ùëñùúèand the rate is
ùúáùëö,ùëñ=
1
ùê∑ùëñùúè.
‚Ä¢ Solo decode rate. In decode-only mode, each token takes ùúèsolo seconds. Defining ùõæ:= 1/ùúèsolo to be the
token generation rate per slot, the mean service time for ùê∑ùëñtokens is ùê∑ùëñ/ùõæand the rate is
ùúáùë†,ùëñ= ùõæ
ùê∑ùëñ
.
We collect these rates as
ùúáùëù,ùëñ= ùê∂
ùëÉùëñùúè,
ùúáùëö,ùëñ=
1
ùê∑ùëñùúè,
ùúáùë†,ùëñ= ùõæ
ùê∑ùëñ
.
(4)


--- Page 10 ---
10
For analytical tractability, we model prefill, mixed decode, and solo decode service times as independent
exponential random variables with rates ùúáùëù,ùëñ, ùúáùëö,ùëñ, and ùúáùë†,ùëñ, respectively, for each class ùëñ‚ààI.
We assume Poisson arrivals: in the ùëõth system, class-ùëñarrivals form a Poisson process with rate ùúÜùëõ
ùëñ:= ùëõùúÜùëñ,
where ùúÜùëñ> 0 is the nominal arrival rate per GPU, so the total offered load grows proportionally with ùëõ.
Customers are impatient in both the prefill and decode queues: any class-ùëñjob that is waiting is endowed with
an independent exponential patience time with rate ùúÉùëñ‚â•0. Interarrival times, service times, and patience
times are assumed mutually independent across all jobs and classes.
State and control processes. Fix ùëõ‚ààN. For each class ùëñ‚ààI and time ùë°‚â•0, denote by ùëÑùëõ
ùëù,ùëñ(ùë°) the
number of class-ùëñjobs waiting for prefill, by ùëãùëõ
ùëñ(ùë°) the number in prefill service, by ùëÑùëõ
ùëë,ùëñ(ùë°) the number
waiting for decode (prefill completed), and by ùëåùëõ
ùëö,ùëñ(ùë°) and ùëåùëõ
ùë†,ùëñ(ùë°) the numbers in mixed and solo decode,
respectively. These processes are right-continuous, integer-valued, and change by unit jumps when individual
jobs enter or leave the corresponding stage. The total class-ùëñcontent in prefill and decode is
ùëçùëõ
ùëù,ùëñ(ùë°) := ùëÑùëõ
ùëù,ùëñ(ùë°) + ùëãùëõ
ùëñ(ùë°),
ùëçùëõ
ùëë,ùëñ(ùë°) := ùëÑùëõ
ùëë,ùëñ(ùë°) +ùëåùëõ
ùëö,ùëñ(ùë°) +ùëåùëõ
ùë†,ùëñ(ùë°).
(5)
The cumulative primitive counting processes are defined as follows. Let ùê¥ùëõ
ùëñ(ùë°) be the total number of
exogenous arrivals of class-ùëñjobs by time ùë°. Let ùêµùëõ
ùëù,ùëñ(ùë°) and ùêµùëõ
ùëë,ùëñ(ùë°) denote the total abandonments from the
prefill and decode queues of class ùëñ. Let ùëÜùëõ
ùëù,ùëñ(ùë°) be the total prefill completions, and ùëÜùëõ
ùëë,ùëö,ùëñ(ùë°) and ùëÜùëõ
ùëë,ùë†,ùëñ(ùë°)
the total mixed and solo decode completions; the total decode completions are
ùëÜùëõ
ùëë,ùëñ(ùë°) := ùëÜùëõ
ùëë,ùëö,ùëñ(ùë°) + ùëÜùëõ
ùëë,ùë†,ùëñ(ùë°).
(6)
These counting processes admit a standard random time-change representation. Let
{ùëÅùê¥,ùëñ, ùëÅùêµùëù,ùëñ, ùëÅùêµùëë,ùëñ, ùëÅùëù,ùëñ, ùëÅùëë,ùëö,ùëñ, ùëÅùëë,ùë†,ùëñ}ùëñ‚ààI
be mutually independent unit-rate Poisson processes. Then, for each ùëñ‚ààI and ùë°‚â•0,
ùê¥ùëõ
ùëñ(ùë°) = ùëÅùê¥,ùëñ
 ùúÜùëõ
ùëñùë°,
ùêµùëõ
ùëù,ùëñ(ùë°) = ùëÅùêµùëù,ùëñ
‚à´ùë°
0
ùúÉùëñùëÑùëõ
ùëù,ùëñ(ùë†) ùëëùë†

,
(7)
ùêµùëõ
ùëë,ùëñ(ùë°) = ùëÅùêµùëë,ùëñ
‚à´ùë°
0
ùúÉùëñùëÑùëõ
ùëë,ùëñ(ùë†) ùëëùë†

,
ùëÜùëõ
ùëù,ùëñ(ùë°) = ùëÅùëù,ùëñ
‚à´ùë°
0
ùúáùëù,ùëñùëãùëõ
ùëñ(ùë†) ùëëùë†

,
(8)
ùëÜùëõ
ùëë,ùëö,ùëñ(ùë°) = ùëÅùëë,ùëö,ùëñ
‚à´ùë°
0
ùúáùëö,ùëñùëåùëõ
ùëö,ùëñ(ùë†) ùëëùë†

,
ùëÜùëõ
ùëë,ùë†,ùëñ(ùë°) = ùëÅùëë,ùë†,ùëñ
‚à´ùë°
0
ùúáùë†,ùëñùëåùëõ
ùë†,ùëñ(ùë†) ùëëùë†

.
(9)
Equations (7)‚Äì(9) state that each cumulative count is driven by a unit-rate Poisson process with the
corresponding integrated intensity.
Scheduling decisions are encoded through cumulative control processes. For each class ùëñ, let ùëàùëõ
ùëù,ùëñ(ùë°) be the
number of jobs admitted into prefill service by time ùë°, and ùëàùëõ
ùëë,ùëö,ùëñ(ùë°) and ùëàùëõ
ùëë,ùë†,ùëñ(ùë°) the numbers admitted into
mixed and solo decode. Mode switches between decode submodes are counted by
ùëÄùëõ
ùë†‚Üíùëö,ùëñ(ùë°) and ùëÄùëõ
ùëö‚Üíùë†,ùëñ(ùë°),
(10)


--- Page 11 ---
11
the cumulative numbers of class-ùëñdecodes switched from solo to mixed and from mixed to solo by time
ùë°. These mode-switch processes are endogenous: they have no external Poisson clocks and are induced by
changes in prefill activity on each GPU. In particular, solo-to-mixed switches occur when a prefill is admitted
to a GPU that was previously in pure decode mode, and mixed-to-solo switches occur when that prefill
completes; these transitions are structural consequences of the prefill dynamics rather than direct control
actions of the scheduling policy.
For later use, define the aggregate in-service counts
ùëãùëõ(ùë°) :=
‚àëÔ∏Å
ùëñ
ùëãùëõ
ùëñ(ùë°),
ùëåùëõ
ùëö(ùë°) :=
‚àëÔ∏Å
ùëñ
ùëåùëõ
ùëö,ùëñ(ùë°),
ùëåùëõ
ùë†(ùë°) :=
‚àëÔ∏Å
ùëñ
ùëåùëõ
ùë†,ùëñ(ùë°),
(11)
and similarly ùëÄùëõ
ùë†‚Üíùëö(ùë°) := √ç
ùëñùëÄùëõ
ùë†‚Üíùëö,ùëñ(ùë°) and ùëÄùëõ
ùëö‚Üíùë†(ùë°) := √ç
ùëñùëÄùëõ
ùëö‚Üíùë†,ùëñ(ùë°). The per-GPU physical constraints
imply
0 ‚â§ùëãùëõ(ùë°) ‚â§ùëõ,
(12)
0 ‚â§ùëåùëõ
ùëö(ùë°) ‚â§(ùêµ‚àí1) ùëãùëõ(ùë°),
(13)
0 ‚â§ùëåùëõ
ùë†(ùë°) ‚â§ùêµ ùëõ‚àíùëãùëõ(ùë°).
(14)
Equation (12) enforces at most one prefill per GPU, while (13)‚Äì(14) bound mixed and solo decodes according
to whether a GPU is running a prefill.
Admissible policies. We now formalize the notion of a policy. Let
ùúãùëõ:=

ùëÑùëõ
ùëù,ùëÑùëõ
ùëë, ùëãùëõ,ùëåùëõ
ùëö,ùëåùëõ
ùë†, ùê¥ùëõ, ùêµùëõ
ùëù, ùêµùëõ
ùëë, ùëÜùëõ
ùëù, ùëÜùëõ
ùëë,ùëö, ùëÜùëõ
ùëë,ùë†, ùëàùëõ
ùëù,ùëàùëõ
ùëë,ùëö,ùëàùëõ
ùëë,ùë†, ùëÄùëõ
ùë†‚Üíùëö, ùëÄùëõ
ùëö‚Üíùë†

denote the collection of all non-primitive processes in the ùëõth system (state, cumulative flows, and control
processes), where each symbol stands for the vector over classes ùëñ‚ààI. Let Œ†ùëõdenote the set of policies that
satisfy the admissibility conditions below:
(i) the resulting state processes satisfy the capacity constraints (12)‚Äì(14) and balance equations (15)-(19)
for all ùë°‚â•0;
(ii) the policy is event-driven, i.e., each control process ùëàùëõ
¬∑,ùëñ(ùë°) can change only at arrival epochs,
abandonment epochs, service-completion epochs, or at ùë°= 0;
(iii) within each class, prefill and decode queues are served in first-come-first-served order, and service is
non-preemptive.
We say that any ùúãùëõ‚ààŒ†ùëõis an admissible policy, under which the state processes are then uniquely
determined from the primitives and the controls via the balance equations (15)‚Äì(19).


--- Page 12 ---
12
Balance equations. Under any policy ùúãùëõ‚ààŒ†ùëõ, the state and cumulative processes satisfy the following
flow-balance identities for all ùëñ‚ààI and ùë°‚â•0:
ùëÑùëõ
ùëù,ùëñ(ùë°) = ùëÑùëõ
ùëù,ùëñ(0) + ùê¥ùëõ
ùëñ(ùë°) ‚àíùëàùëõ
ùëù,ùëñ(ùë°) ‚àíùêµùëõ
ùëù,ùëñ(ùë°),
(15)
ùëãùëõ
ùëñ(ùë°) = ùëãùëõ
ùëñ(0) +ùëàùëõ
ùëù,ùëñ(ùë°) ‚àíùëÜùëõ
ùëù,ùëñ(ùë°),
(16)
ùëÑùëõ
ùëë,ùëñ(ùë°) = ùëÑùëõ
ùëë,ùëñ(0) + ùëÜùëõ
ùëù,ùëñ(ùë°) ‚àíùëàùëõ
ùëë,ùëö,ùëñ(ùë°) ‚àíùëàùëõ
ùëë,ùë†,ùëñ(ùë°) ‚àíùêµùëõ
ùëë,ùëñ(ùë°),
(17)
ùëåùëõ
ùëö,ùëñ(ùë°) =ùëåùëõ
ùëö,ùëñ(0) +ùëàùëõ
ùëë,ùëö,ùëñ(ùë°) ‚àíùëÜùëõ
ùëë,ùëö,ùëñ(ùë°) + ùëÄùëõ
ùë†‚Üíùëö,ùëñ(ùë°) ‚àíùëÄùëõ
ùëö‚Üíùë†,ùëñ(ùë°),
(18)
ùëåùëõ
ùë†,ùëñ(ùë°) =ùëåùëõ
ùë†,ùëñ(0) +ùëàùëõ
ùëë,ùë†,ùëñ(ùë°) ‚àíùëÜùëõ
ùëë,ùë†,ùëñ(ùë°) + ùëÄùëõ
ùëö‚Üíùë†,ùëñ(ùë°) ‚àíùëÄùëõ
ùë†‚Üíùëö,ùëñ(ùë°).
(19)
Equation (15) says that the prefill queue-length equals its initial content plus arrivals, minus admissions and
abandonments. Equation (16) tracks prefill jobs in service as admissions minus completions. Equation (17)
balances the decode queue as completed prefills minus admissions into decode and abandonments. Equa-
tions (18)‚Äì(19) track mixed and solo decodes as admissions minus completions, plus net inflow from mode
switches.
Adding (18) and (19) eliminates the mode-switch terms and yields
ùëåùëõ
ùëö,ùëñ(ùë°) +ùëåùëõ
ùë†,ùëñ(ùë°) =ùëåùëõ
ùëö,ùëñ(0) +ùëåùëõ
ùë†,ùëñ(0) +ùëàùëõ
ùëë,ùëö,ùëñ(ùë°) +ùëàùëõ
ùëë,ùë†,ùëñ(ùë°) ‚àíùëÜùëõ
ùëë,ùëö,ùëñ(ùë°) ‚àíùëÜùëõ
ùëë,ùë†,ùëñ(ùë°),
(20)
so mode switches only redistribute ongoing decodes between mixed and solo, without changing their total
number.
Revenue models and objective functions. Commercial LLM services predominantly use token-
based pricing. We consider two revenue models that differ in when revenue is recognized.
(1) Bundled charging scheme. The provider charges a single price per request based on the total number of
tokens, and revenue is recognized only when the entire request completes (after decode). For class ùëñ,
ùë§ùëñ:= ùëêùëùùëÉùëñ+ ùëêùëëùê∑ùëñ,
(21)
where ùëêùëù, ùëêùëë‚â•0 are unit prices per prefill and decode token. The per-GPU average reward over [0,ùëá] under
policy ùúãùëõis
ùëÖùëõ(ùëá; ùúãùëõ) := 1
ùëõùëáEùúãùëõ
"
ùêº‚àëÔ∏Å
ùëñ=1
ùë§ùëñùëÜùëõ
ùëë,ùëñ(ùëá)
#
.
(22)
Only completed requests contribute to (22); prefill work without decode completion yields no revenue.
(2) Separate charging scheme. Alternatively, prefill and decode tokens may be billed and recognized separately.
The corresponding per-GPU average reward is
ÀúùëÖùëõ(ùëá; ùúãùëõ) := 1
ùëõùëáEùúãùëõ
"
ùêº‚àëÔ∏Å
ùëñ=1
 ùëêùëùùëÉùëñùëÜùëõ
ùëù,ùëñ(ùëá) + ùëêùëëùê∑ùëñùëÜùëõ
ùëë,ùëñ(ùëá)
#
.
(23)


--- Page 13 ---
13
Both objectives depend on token throughput but induce different scheduling incentives: in particular, the
separate scheme (23) may encourage aggressive prefill admissions to harvest immediate prefill revenue at the
expense of downstream decode congestion. Asymptotically optimal control for (22) and (23) will be studied
in Section 4.
3.
Fluid Approximation and Steady-State Planning
In large-scale LLM deployments, providers typically operate hundreds or thousands of GPUs in parallel.
At this scale, the system state is high-dimensional and stochastic, with arrivals, service completions, and
abandonments fluctuating across time and devices, making direct stochastic optimization analytically
intractable and hard to interpret. We therefore adopt a many-GPU fluid approximation: consider a sequence
of systems indexed by the number of GPUs ùëõ, scale all queue lengths and in-service counts by 1/ùëõ, and let
ùëõ‚Üí‚àû. In this limit, random fluctuations average out and the network is described by a deterministic set of
flow-balance equations and capacity constraints. Steady-state solutions of this fluid model specify per-GPU
occupancies, which serve as planning targets for the stochastic control policies in Section 4.
Fluid-scaled processes. For any stochastic process ùëäùëõ(ùë°) in the ùëõ-th system, we define its fluid-scaled
version by
¬Øùëäùëõ(ùë°) := 1
ùëõùëäùëõ(ùë°),
ùë°‚â•0.
We use an overbar to indicate such scaled processes (e.g., ¬ØùëÑùëõ
ùëù,ùëñ(ùë°), ¬Øùëãùëõ
ùëñ(ùë°), ¬Øùëåùëõ
ùëö,ùëñ(ùë°)), and we write the
corresponding lowercase letters (e.g., ùëûùëù,ùëñ(ùë°), ùë•ùëñ(ùë°), ùë¶ùëö,ùëñ(ùë°)) for generic deterministic fluid trajectories that
arise as limits of these scaled processes in Section 2.3. These functions satisfy, for all ùë°‚â•0 and all ùëñ‚ààI, the
flow-balance equations:
ùëûùëù,ùëñ(ùë°) = ùëûùëù,ùëñ(0) + ùëéùëñ(ùë°) ‚àíùë¢ùëù,ùëñ(ùë°) ‚àíùëèùëù,ùëñ(ùë°),
(24)
ùë•ùëñ(ùë°) = ùë•ùëñ(0) + ùë¢ùëù,ùëñ(ùë°) ‚àíùë†ùëù,ùëñ(ùë°),
(25)
ùëûùëë,ùëñ(ùë°) = ùëûùëë,ùëñ(0) + ùë†ùëù,ùëñ(ùë°) ‚àíùë¢ùëë,ùëö,ùëñ(ùë°) ‚àíùë¢ùëë,ùë†,ùëñ(ùë°) ‚àíùëèùëë,ùëñ(ùë°),
(26)
ùë¶ùëö,ùëñ(ùë°) = ùë¶ùëö,ùëñ(0) + ùë¢ùëë,ùëö,ùëñ(ùë°) ‚àíùë†ùëë,ùëö,ùëñ(ùë°) + ùëöùë†‚Üíùëö,ùëñ(ùë°) ‚àíùëöùëö‚Üíùë†,ùëñ(ùë°),
(27)
ùë¶ùë†,ùëñ(ùë°) = ùë¶ùë†,ùëñ(0) + ùë¢ùëë,ùë†,ùëñ(ùë°) ‚àíùë†ùëë,ùë†,ùëñ(ùë°) + ùëöùëö‚Üíùë†,ùëñ(ùë°) ‚àíùëöùë†‚Üíùëö,ùëñ(ùë°).
(28)
Here ùëûùëù,ùëñ(ùë°) and ùëûùëë,ùëñ(ùë°) are the prefill and decode queue contents, ùë•ùëñ(ùë°), ùë¶ùëö,ùëñ(ùë°) and ùë¶ùë†,ùëñ(ùë°) are the in-service
masses in prefill, mixed decode and solo decode, ùë¢ùëù,ùëñ(ùë°), ùë¢ùëë,ùëö,ùëñ(ùë°) and ùë¢ùëë,ùë†,ùëñ(ùë°) are the cumulative admissions
into these stages, ùëèùëù,ùëñ(ùë°) and ùëèùëë,ùëñ(ùë°) are cumulative abandonments, ùë†ùëù,ùëñ(ùë°), ùë†ùëë,ùëö,ùëñ(ùë°) and ùë†ùëë,ùë†,ùëñ(ùë°) are
cumulative service completions, and ùëöùë†‚Üíùëö,ùëñ(ùë°), ùëöùëö‚Üíùë†,ùëñ(ùë°) are cumulative mode switches between decode
submodes.


--- Page 14 ---
14
The primitive arrivals, abandonments, and service completions evolve at their mean rates:
ùëéùëñ(ùë°) = ùúÜùëñùë°,
(29)
ùëèùëù,ùëñ(ùë°) =
‚à´ùë°
0
ùúÉùëñùëûùëù,ùëñ(ùë†) ùëëùë†,
ùëèùëë,ùëñ(ùë°) =
‚à´ùë°
0
ùúÉùëñùëûùëë,ùëñ(ùë†) ùëëùë†,
(30)
ùë†ùëù,ùëñ(ùë°) =
‚à´ùë°
0
ùúáùëù,ùëñùë•ùëñ(ùë†) ùëëùë†,
(31)
ùë†ùëë,ùëö,ùëñ(ùë°) =
‚à´ùë°
0
ùúáùëö,ùëñùë¶ùëö,ùëñ(ùë†) ùëëùë†,
ùë†ùëë,ùë†,ùëñ(ùë°) =
‚à´ùë°
0
ùúáùë†,ùëñùë¶ùë†,ùëñ(ùë†) ùëëùë†.
(32)
Equation (29) gives the deterministic arrival rate, (30) the abandonment flows from the prefill and decode
queues, and (31)‚Äì(32) the prefill and decode completion flows driven by the in-service masses.
Let
ùë•(ùë°) :=
‚àëÔ∏Å
ùëñ
ùë•ùëñ(ùë°),
ùë¶ùëö(ùë°) :=
‚àëÔ∏Å
ùëñ
ùë¶ùëö,ùëñ(ùë°),
ùë¶ùë†(ùë°) :=
‚àëÔ∏Å
ùëñ
ùë¶ùë†,ùëñ(ùë°),
and similarly ùë¢ùëù(ùë°) := √ç
ùëñùë¢ùëù,ùëñ(ùë°) and ùë†ùëù(ùë°) := √ç
ùëñùë†ùëù,ùëñ(ùë°). The per-GPU capacity constraints in the fluid model
are
0 ‚â§ùë•(ùë°) ‚â§1,
(33)
0 ‚â§ùë¶ùëö(ùë°) ‚â§(ùêµ‚àí1) ùë•(ùë°),
(34)
0 ‚â§ùë¶ùë†(ùë°) ‚â§ùêµ 1 ‚àíùë•(ùë°),
(35)
which mirror the prefill and decode caps in (12)‚Äì(14) after scaling by ùëõ.
Finally, admission feasibility holds at the fluid level. For all 0 ‚â§ùë†‚â§ùë°and all ùëñ‚ààI,
ùë¢ùëë,ùëö,ùëñ(ùë†,ùë°) + ùë¢ùëë,ùë†,ùëñ(ùë†,ùë°) ‚â§ùëûùëë,ùëñ(ùë†) + ùë†ùëù,ùëñ(ùë†,ùë°) ‚àíùëèùëë,ùëñ(ùë†,ùë°),
(36)
ùë¢ùëù,ùëñ(ùë†,ùë°) ‚â§ùëûùëù,ùëñ(ùë†) + ùëéùëñ(ùë†,ùë°) ‚àíùëèùëù,ùëñ(ùë†,ùë°),
(37)
where ùë¢ùëù,ùëñ(ùë†,ùë°) := ùë¢ùëù,ùëñ(ùë°) ‚àíùë¢ùëù,ùëñ(ùë†) and ùëéùëñ(ùë†,ùë°), ùë†ùëù,ùëñ(ùë†,ùë°), ùëèùëù,ùëñ(ùë†,ùë°), etc. are defined analogously. Inequal-
ities (36)‚Äì(37) state that, over any time interval [ùë†,ùë°], admissions into each stage cannot exceed the fluid
already present in the corresponding buffer plus the net inflow into that buffer.
ASSUMPTION 1 (Convergence of initial state). The initial states of fluid-scaled processes converge to a
deterministic state:   ¬ØùëÑùëõ
ùëù,ùëñ(0), ¬ØùëÑùëõ
ùëë,ùëñ(0), ¬Øùëãùëõ
ùëñ(0), ¬Øùëåùëõ
ùëö,ùëñ(0), ¬Øùëåùëõ
ùë†,ùëñ(0) ‚áí(ùëûùëù,ùëñ(0), ùëûùëë,ùëñ(0),ùë•ùëñ(0), ùë¶ùëö,ùëñ(0), ùë¶ùë†,ùëñ(0));
THEOREM 1 (Fluid limit). Fix any finite horizon ùëá> 0. The sequence of fluid-scaled stochastic processes
¬ØXùëõ(ùë°) :=  { ¬ØùëÑùëõ
ùëù,ùëñ, ¬ØùëÑùëõ
ùëë,ùëñ, ¬Øùëãùëõ
ùëñ, ¬Øùëåùëõ
ùëö,ùëñ, ¬Øùëåùëõ
ùë†,ùëñ}ùëñ‚ààI, { ¬ØùëÜùëõ
ùëù,ùëñ, ¬ØùëÜùëõ
ùëë,ùëö,ùëñ, ¬ØùëÜùëõ
ùëë,ùë†,ùëñ}ùëñ‚ààI, { ¬Øùêµùëõ
ùëù,ùëñ, ¬Øùêµùëõ
ùëë,ùëñ}ùëñ‚ààI,
{ ¬Øùëàùëõ
ùëù,ùëñ, ¬Øùëàùëõ
ùëë,ùëö,ùëñ, ¬Øùëàùëõ
ùëë,ùë†,ùëñ}ùëñ‚ààI, { ¬ØùëÄùëõ
ùë†‚Üíùëö,ùëñ‚àí¬ØùëÄùëõ
ùëö‚Üíùë†,ùëñ}ùëñ‚ààI

is tight in D([0,ùëá], Rùëë) under the Skorokhod ùêΩ1 topology, where ùëëis the total dimension of the vector above.
Moreover, any subsequential weak limit ¬ØX(ùë°) is almost surely continuous and, on [0,ùëá], satisfies the fluid
model equations (24)‚Äì(28) with (29)‚Äì(32), the capacity constraints (33)‚Äì(35), with initial state given by z0.


--- Page 15 ---
15
Fluid Reward Objectives. Consistent with the revenue models defined in Section 2, we formulate the
fluid control objectives for the two charging schemes separately.
(1) Bundled Objective. Under the bundled scheme, value is realized only upon request completion. Thus, the
objective maximizes the weighted throughput of the decode phase:
ùëÖ(ùëá) := 1
ùëá
‚à´ùëá
0
ùêº‚àëÔ∏Å
ùëñ=1
ùë§ùëñ(ùë¶ùëö,ùëñ(ùúè)ùúáùëö,ùëñ+ ùë¶ùë†,ùëñ(ùúè)ùúáùë†,ùëñ) ùëëùúè,
(38)
where ùë§ùëñ= ùëêùëùùëÉùëñ+ ùëêùëëùê∑ùëñis the total reward for a completed class-ùëñrequest. Note that the prefill activity ùë•ùëñ(ùúè)
contributes to the objective only indirectly by feeding the decode queue.
(2) Separate Objective. Under the separate scheme, the system accumulates value continuously as tokens are
processed in both phases. The objective becomes:
ÀúùëÖ(ùëá) := 1
ùëá
‚à´ùëá
0
ùêº‚àëÔ∏Å
ùëñ=1
h
(ùëêùëùùëÉùëñ)
| {z }
prefill value
ùúáùëù,ùëñùë¶ùëù,ùëñ(ùúè) + (ùëêùëëùê∑ùëñ)
|  {z  }
decode value
(ùë¶ùëö,ùëñ(ùúè)ùúáùëö,ùëñ+ ùë¶ùë†,ùëñ(ùúè)ùúáùë†,ùëñ)
i
ùëëùúè.
(39)
3.1.
Fluid Control Problem
We first consider the bundle objective (the separate charging scheme will be discussed in Section 4.2) and
solve a steady-state (fluid) optimization to choose the optimal long-run occupancy shares and the routing
of prefilled tasks across mixed and solo decode pools. This formulation intentionally abstracts away the
transient effects of stochastic variability in interarrival, prefill, and decode times (and abandonment/patience,
if present), and instead enforces constraints only in terms of average arrival and service rates. The solution
delivers capacity-splitting targets that will guide the stochastic control policy developed in the next section.
max
{ùë•ùëñ, ùë¶ùëö,ùëñ, ùë¶ùë†,ùëñ, ùëûùëù,ùëñ, ùëûùëë,ùëñ}
ùêº‚àëÔ∏Å
ùëñ=1
ùë§ùëñ
 ùë¶ùëö,ùëñùúáùëö,ùëñ+ ùë¶ùë†,ùëñùúáùë†,ùëñ

s.t.
ùêº‚àëÔ∏Å
ùëñ=1
ùë•ùëñ‚â§1,
(Prefill Capacity)
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶ùëö,ùëñ‚â§(ùêµ‚àí1)
ùêº‚àëÔ∏Å
ùëñ=1
ùë•ùëñ,
(Mixed Decode Capacity)
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶ùë†,ùëñ‚â§ùêµ

1 ‚àí
ùêº‚àëÔ∏Å
ùëñ=1
ùë•ùëñ

,
(Solo Decode Capacity)
ùúÜùëñ‚àíùúÉùëñùëûùëù,ùëñ= ùúáùëù,ùëñùë•ùëñ,
‚àÄùëñ,
(Prefill Flow Balance)
ùúáùëù,ùëñùë•ùëñ‚àíùúÉùëñùëûùëë,ùëñ= ùúáùëö,ùëñùë¶ùëö,ùëñ+ ùúáùë†,ùëñùë¶ùë†,ùëñ,
‚àÄùëñ,
(Decode Flow Balance)
ùë•ùëñ, ùë¶ùëö,ùëñ, ùë¶ùë†,ùëñ, ùëûùëë,ùëñ, ùëûùëù,ùëñ‚â•0,
‚àÄùëñ.
(Non-negativity)
(40)


--- Page 16 ---
16
The linear program (40) describes the steady-state fluid model for routing prefill and decode work of
multiple classes across GPUs with batch size ùêµ. The decision variables are long-run, per-GPU averages: ùë•ùëñis
the fraction of time a GPU devotes to class-ùëñprefill; ùë¶ùëö,ùëñand ùë¶ùë†,ùëñare the average class-ùëñdecode occupancies
in mixed mode and solo mode; and ùëûùëù,ùëñand ùëûùëë,ùëñare the average prefill and decode queue masses.
The capacity constraints in the first three lines of (40) enforce the per-GPU limits: at most one prefill can
run on a GPU, and, conditional on whether a prefill is present, at most ùêµ‚àí1 (mixed) or ùêµ(solo) decodes
can be served in parallel. The flow-balance constraints for each class ùëñstate that, in steady state, arrivals
net of prefill abandonments equal the prefill completion rate, and that prefill completions, net of decode
abandonments, are exactly matched by the total decode completion rate. The objective in the first line of (40)
maximizes the per-GPU long-run reward by weighting class-ùëñdecode completions in mixed and solo modes
with ùë§ùëñ= ùëêùëùùëÉùëñ+ ùëêùëëùê∑ùëñ, the total value of a completed request under bundled pricing.
PROPOSITION 1 (Decode-buffer elimination). Assume ùõæùúè‚â•(ùêµ‚àí1)/ùêµ(solo decode more efficient).
Then the steady-state fluid LP admits an optimal solution with ùëû‚òÖ
ùëë,ùëñ= 0 for all ùëñ.
The condition in Proposition 1 is natural: solo decode is strictly more efficient than mixed decode. Since
revenue is generated only when a request finishes decode, fluid mass held in the decode buffer yields no
reward and only delays completions. We prove that any LP solution with ùëûùëë,ùëñ> 0 can be improved by moving
this backlog upstream while keeping the capacity constraints, which weakly increases the completion rate;
hence, at optimality, the decode buffer is empty in steady state.
A key implication for control is that the optimal reward rate is determined by occupancy proportions,
i.e. how much GPU time is spent on prefill and how decode slots are filled. In a backlogged system, the
fluid-optimal plan keeps both prefill and decode fully utilized and fixes the fraction of GPUs running prefills
at its target level. The remaining design question is how to route completed prefills between mixed and solo
decode so that capacity is saturated without building a decode backlog. This leads to a simple gate-and-route
architecture: a prefill gate that regulates the target prefill occupancy (and class mix) and a decode router that
splits work between mixed and solo decodes to keep slots busy while preventing persistent decode queues.
4.
Stochastic Control Policy
Building upon the fluid-optimal solution, we now develop an implementable control framework for the
stochastic ùëõ-GPU system. Our approach operationalizes the fluid prescriptions by decomposing the scheduling
problem into two hierarchical stages: a static resource partitioning phase that fixes the cluster configuration,
and a dynamic control phase that manages job admission and routing in real time. We first introduce the
occupancy-based Gate-and-Route policy in Section 4.1, which regulates prefill occupancies and decode
routing to attain the fluid-optimal throughput under the bundled objective. We then extend this architecture to
the separate charging scheme in Section 4.2, where we develop a counterpart optimal policy and analyze how
stage-based revenue recognition fundamentally reshapes scheduling incentives across heterogeneous request
classes relative to the bundled objective.


--- Page 17 ---
17
4.1.
Bundled Charging Scheme
The design of this policy is inspired by the structural insight from Proposition 1, which reveals that the
relative efficiency gain of solo decoding is class-independent. This property suggests a decomposition of the
complex scheduling problem. We can statically partition the cluster resources to ensure that the aggregate
decode capacity is critically loaded in the fluid limit, capable of fully digesting the downstream workload
generated by the optimal prefill throughput. With the decode stage dimensioned to clear the traffic naturally,
the burden of optimization shifts upstream. Consequently, we focus our fine-grained dynamic control on the
prefill admission to strictly regulate the job mix and occupancy, allowing the decode stage to operate under a
simple work-conserving discipline (FCFS) while still guiding the system toward the fluid-optimal state.
Static Planning Let ùëõbe the number of GPUs and ùêµthe per-GPU decode stream cap. Take any optimal
per-GPU solution of the steady-state fluid LP, denoted by  ùë•‚òÖ
ùëñ, ùë¶‚òÖ
ùëö,ùëñ, ùë¶‚òÖ
ùë†,ùëñ, ùëû‚òÖ
ùëù,ùëñ

ùëñ‚ààI.
Fix the number of mixed GPUs as
ùëÄ:=
l
ùëõ
‚àëÔ∏Å
ùëñ‚ààI
ùë•‚òÖ
ùëñ
m
,
choose any subset Gmix ‚äÇ{1, . . . , ùëõ} with |Gmix| = ùëÄ, and set Gsolo := {1, . . . , ùëõ} \ Gmix. A GPU ùëî‚ààGmix
permanently reserves one slot for prefill (or equivalently, those GPUs prioritize new prefill over decode jobs)
and may run at most (ùêµ‚àí1) decodes; a GPU ùëî‚ààGsolo never runs prefills and may run at most ùêµdecodes.
Dynamic Control Let ùëãùëõ
ùëñ(ùë°) denote the number of class-ùëñprefill tasks currently in service and ùëÑùëõ
ùëù,ùëñ(ùë°)
denote the prefill queue length at time ùë°.
Upstream (prefill) gate on mixed GPUs. Prefills run only on Gmix. Whenever a mixed GPU ùëî‚ààGmix
has its reserved prefill slot idle, identify the set of classes with waiting jobs, Iwait = {ùëñ‚ààI : ùëÑùëõ
ùëù,ùëñ(ùë°‚àí) > 0}. If
Iwait is empty, the slot remains idle. Otherwise, compute the occupancy deviation index for each candidate
class:
ùúâùëñ(ùë°‚àí) := 1
ùë•‚òÖ
ùëñ
 ùëãùëõ
ùëñ(ùë°‚àí) ‚àíùëõùë•‚òÖ
ùëñ
.
The scheduler admits the head-of-line prefill of the class ùëñ‚òÖthat minimizes this deviation:
ùëñ‚òÖ‚ààarg min
ùëñ‚ààIwait
ùúâùëñ(ùë°‚àí).
If there are multiple classes achieving the minimum deviation, ties are broken by selecting the class with the
largest queue deviation ùõøùëñ(ùë°‚àí) := ùëÑùëõ
ùëù,ùëñ(ùë°‚àí) ‚àíùëÑ‚Ä†
P,ùëñ. Service is non-preemptive and FCFS within each class.
The gate is a negative‚Äìfeedback rule around the fluid targets ùë•‚òÖ
ùëñ: classes whose prefill occupancy exceeds
their target (large ùúâùëñ) are held back, while under-served classes (smallest ùúâùëñ) are pulled up by being admitted
first. Since the total mixed prefill capacity √ç
ùëñùë•‚òÖ
ùëñis fixed by the static planning, not all classes can be above
target at once, and repeatedly correcting the most deviated class keeps the long-run average occupancies
fluctuating in a small neighborhood of the fluid-optimal levels.


--- Page 18 ---
18
Downstream (decode) routing. Maintain a single decode buffer with class-ùëñqueue length ùëÑùëõ
ùëë,ùëñ(ùë°).
When a class-ùëñjob requires decode (either immediately after prefill completion or upon a decode completion
elsewhere), route as follows:
1. If some GPU in Gsolo has a free decode slot, place the job uniformly at random among such GPUs.
2. Otherwise, if some GPU in Gmix has a free decode slot, place it randomly among such GPUs.
3. Otherwise, append the job to the decode buffer (FCFS across class).
The key insight is that, from a token-level viewpoint, the decode stage only needs to keep up with the
stream of decode tokens created by the prefill gate. Once the policy stabilizes this token production rate and
keeps decode compute fully utilized, the decode workload is always consumable, and the detailed class mix
becomes secondary. This is why a simple work-conserving rule such as FCFS is sufficient at decode.
Two mechanisms make this intuition rigorous. Static planning fixes the mixed versus solo partition so that
decode is never overloaded in the fluid limit, and in the binding case it is critically loaded at an LP-optimal
point with zero steady-state decode buffer. GPU physics further implies that the relative speed advantage of
solo decoding over mixed decoding is the same across classes, which lets us translate capacity between solo
and mixed in a class-agnostic way and treat decode as effectively homogeneous in heavy traffic. Theorem 2
formalizes this insight by proving that the resulting gate-and-route policy, with FCFS decoding, achieves
asymptotic optimality.
THEOREM 2 (Asymptotic optimality of occupancy-based Gate-and-Route Policy). Let ùëÖ‚òÖdenote
the optimal objective value of the steady-state fluid routing LP, and let Assumptions 1 hold. Then the
occupancy-based Gate-and-Route policy ùúãùëõ,‚òÖis asymptotically optimal:
liminf
ùëá‚Üí‚àûliminf
ùëõ‚Üí‚àûùëÖùëõ(ùëá; ùúãùëõ,‚òÖ) = ùëÖ‚òÖ.
4.2.
Separate Charging Scheme
The bundled-revenue formulation studied above serves as our primary benchmark and aligns with the objective
of maximizing the throughput of completed requests. To complement this view, we also study a separate
charging objective in which value is recognized separately at prefill and decode. This variant lets us derive
a counterpart optimal policy and clarify how the timing of revenue recognition changes the incentives for
admission and routing. Formally, we define the per-GPU time-averaged reward as follows.
ÀúùëÖùëõ(ùëá; ùúãùëõ) :=
1
ùëõùëáEùúãùëõ
" ùêº‚àëÔ∏Å
ùëñ=1

ùëêùëùùëÉùëñùëÜùëõ
ùëù,ùëñ(ùëá) + ùëêùëëùê∑ùëñùëÜùëõ
ùëë,ùëñ(ùëá)
#
,
(41)
where ùëêùëù, ùëêùëë‚â•0 are unit prices, and ùëÜùëõ
ùëù,ùëñ(ùëá) and ùëÜùëõ
ùëë,ùëñ(ùëá) are cumulative prefill and decode completions for
class ùëñby time ùëá.
In steady state, we optimize the corresponding fluid objective over the same feasibility constraints as
the bundled LP (i.e., (40)). Substituting the service-rate definitions shows that the objective coefficients


--- Page 19 ---
19
are class-independent, so the separate-charging LP depends on (ùë•ùëñ, ùë¶ùëö,ùëñ, ùë¶ùë†,ùëñ) only through the aggregate
occupancies:
max
(ùë•,ùë¶,ùëû) ùëêùëù
ùê∂
ùúè
ùêº‚àëÔ∏Å
ùëñ=1
ùë•ùëñ+ ùëêùëë
ùúè
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶ùëö,ùëñ+ ùëêùëëùõæ
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶ùë†,ùëñ.
(42)
These structural properties yield three key insights for policy design under separate charging. First, since
prefill occupancy earns the same marginal reward ùëêùëùùê∂/ùúèacross all classes, the pricing structure itself does
not prioritize any specific class mix in the prefill stage. Second, because solo-mode decode iterations are
faster than mixed-mode ones (ùõæ> 1/ùúè), solo decode occupancy is strictly more valuable per unit time. A
revenue-maximizing controller should thus prioritize saturating solo capacity.
Finally, unlike the bundled scheme where revenue is deferred, separate charging incentivizes the system to
maintain a high ‚Äùinventory‚Äù of downstream work. To keep the more valuable decode slots busy, the prefill
gate should favor classes with a larger decode-to-prefill ratio ùê∑ùëñ/ùëÉùëñ, as they generate more future decode
revenue per unit of prefill capacity consumed. This logic motivates the static priority index used in the
Prioritize-and-Route policy described below. Notably, the separate-charging optimum may tolerate persistent
decode backlogs, as these backlogs serve as a buffer to ensure continuous decode revenue.
4.2.1.
Prioritize-and-Route policy Under the separate-charging objective we reuse the same gate-
and-route architecture as in the bundled scheme; the only change is the prefill admission rule (priority). We
briefly summarize the modifications.
Static planning. Solve the separate-charging LP (42). We define the prefill queue targets and partition the
GPUs into ùê∫mix and ùê∫solo following the identical procedure as in the bundled case, using the optimal prefill
occupancies Àúùë•‚òÖ
ùëñto determine the partition size ÀúùëÄ.
Dynamic control. The downstream decode routing is unchanged, and the only modification is the upstream
prefill gate on ùê∫mix. Whenever a reserved prefill slot becomes idle and some prefill queue is nonempty, we
admit the head-of-line job from the class with the largest decode-to-prefill ratio:
ùëñ‚òÖ‚ààargmax
ùëñ‚ààI
n ùê∑ùëñ
ùëÉùëñ
o
,
breaking ties arbitrarily. Service remains FCFS within each class.
THEOREM 3 (Asymptotic optimality under separate charging). Let ÀúùëÖ‚òÖbe the optimal value of the
steady-state fluid LP (42), and let ÀúùëÖùëõ(ùëá; ùúãùëõ) be the per-GPU separate-charging reward in (41). Assume
Assumption 1 holds.
For each ùëõ‚ààN, define the prioritize-and-route policy Àúùúãùëõ,‚òÖas above, then Àúùúãùëõ,‚òÖis asymptotically optimal
for the separate-charging objective:
liminf
ùëá‚Üí‚àûliminf
ùëõ‚Üí‚àû
ÀúùëÖùëõ(ùëá; Àúùúãùëõ,‚òÖ) = ÀúùëÖ‚òÖ.


--- Page 20 ---
20
The Revenue-Congestion Trade-off and Operational Risks. While Theorem 3 guarantees
asymptotic optimality for the separate-charging objective, the underlying incentive shift comes from the
structure of (42): revenue is recognized at prefill and decode separately, so a revenue-driven controller may
exploit any available prefill capacity even when decode is already congested. This decoupling changes where
congestion accumulates. Under bundled charging, the policy tends to regulate admissions so that the prefill
buffer absorbs overload while the downstream decode buffer remains comparatively lean. Under separate
charging, the system may instead build substantial decode backlogs to keep decode slots continuously busy,
as illustrated in Figure 2. Operationally, this is problematic because it can lead to memory pressure and
create long post-prefill waits and, in extreme cases, requests that complete prefill (and generate revenue) but
experience severely delayed decode or abandon before completion. This motivates augmenting the revenue
objective with explicit service constraints. Accordingly, in Section 5 we extend the framework to maximize
revenue subject to SLI constraints, and in Section 6 we use shadow-price analysis to quantify the resulting
economic trade-offs.
0
100
200
300
OPT=300.2
Revenue
0
0.2
0.4
0.6
0.8
1
1.2
0
5
10
Arrival Rate (Œª)
Queue Length
C0 Prefill
C0 Decode
C1 Prefill
C1 Decode
0
100
200
300
400
500
OPT=452.9
Revenue
0
0.2
0.4
0.6
0.8
1
1.2
0
5
10
Arrival Rate (Œª)
Queue Length
C0 Prefill
C0 Decode
C1 Prefill
C1 Decode
Figure 2
Comparison of Revenue and Queue Lengths under Bundled vs. Separate Charging Schemes (C0:
class 0, C1: class 1).
5.
SLI-Aware Optimization and Control Policy
While the preceding sections focused on revenue maximization, practical deployments must also adhere to
different Service Level Indicators (SLIs), such as fairness and latency limits. In this section, we show that our
fluid-based planning framework can naturally accommodate these operational requirements. By formulating
SLIs as explicit constraints (or penalty terms) within the steady-state optimization, we can generate SLI-aware
target occupancies without altering the fundamental structure of the control policy. This approach offers a
flexible way to trade off revenue against diverse service guarantees. We focus our exposition on the bundled
charging setting, though the methodology extends directly to other objectives.


--- Page 21 ---
21
SLIs are widely used in practice, but we emphasize that in our framework they are defined at the level
of the steady-state fluid variables. A service-level indicator (SLI) is a user-facing performance metric (e.g.,
fairness or latency) computed from the system‚Äôs steady-state behavior. In our framework, an SLI is modeled
as either (i) a hard constraint of the form ùëî(x,y,q) ‚â§ùúÇ, or (ii) a soft penalty term subtracted from the revenue
objective, both expressed in terms of the steady-state variables in the fluid optimization problem.
A central modeling principle we adopt throughout this section is that a reasonable SLI should not
rely on persistent decode-buffer buildup. Indeed, in our setting decode-buffer mass generates no value
under completion-based reward, but it increases waiting time and, more importantly, can lead to severe
GPU-memory pressure due to KV-cache residency and migration. Accordingly, we impose the following
standing assumption for the SLI-aware planning problem: the chosen SLI specification is such that the
optimization admits an optimal solution with ùëû‚òÖ
ùëë,ùëñ= 0 for all ùëñ(decode-buffer elimination). For completeness,
we provide an extension that allows ùëû‚òÖ
ùëë,ùëñ> 0 in the electronic companion; this case is particularly relevant
under separate charging. To this end, we illustrate several canonical SLI specifications and then present the
corresponding SLI-aware planning problem and control policy.
Resource fairness (prefill and decode) Fairness SLIs control the dispersion of prefill occupancies
{ùë•ùëñ}ùëñ‚ààI and solo decode occupancies {ùë¶ùë†,ùëñ}ùëñ‚ààI. While attractive at a high level, fairness constraints can be
costly in our setting because they directly constrain the workload mix (ùíôand/or ùíöùë†). This can force the system
away from a hardware-efficient operating point and create a structural mismatch between prefill output and
downstream decode capacity, leading to idling/under-utilization and a reduction in revenue. This effect is
quantified in the Pareto frontiers in Section 6 (Fig. 8): Prefill Fairness has a steep shadow price (Fig. 8a),
whereas Decode Fairness is comparatively cheap (Fig. 8b).
Prefill Fairness.
max
ùëñ, ùëó‚ààI{ùë•ùëñ‚àíùë•ùëó} ‚â§ùúÇ1,
ùëûùëë,ùëñ= 0, ‚àÄùëñ‚ààI.
(43)
Equivalently, the same fairness preference can be modeled in the objective via a penalty term
ùëô1 = ùúÇ‚Ä≤
1 max
ùëñ, ùëó‚ààI{ùë•ùëñ‚àíùë•ùëó},
(44)
with weight ùúÇ‚Ä≤
1 > 0 that tunes the trade-off between revenue and Prefill Fairness.
Decode Fairness.
max
ùëñ, ùëó‚ààI{ùë¶ùë†,ùëñ‚àíùë¶ùë†, ùëó} ‚â§ùúÇ2,
ùëûùëë,ùëñ= 0, ‚àÄùëñ‚ààI.
(45)
In penalty form, we instead add
ùëô2 = ùúÇ‚Ä≤
2 max
ùëñ, ùëó‚ààI{ùë¶ùë†,ùëñ‚àíùë¶ùë†, ùëó},
(46)
with weight ùúÇ‚Ä≤
2 > 0; larger ùúÇ‚Ä≤
2 places more emphasis on equalizing solo decode usage across classes.


--- Page 22 ---
22
Average Time per Output Token While the worst-case Time per Output Token is governed by the
chunk size ùê∂, the average TPOT depends on the cluster-wide balance between prefill and decode activity.
Each unit of prefill occupancy ùë•ùëñintroduces mixed-mode iterations that slow co-resident decodes. A natural
SLI is to cap the average TPOT:
ùúè(ùêµ‚àí1) √çùêº
ùëñ=1 ùë•ùëñ+ 1
ùõæùêµ 1 ‚àí√çùêº
ùëñ=1 ùë•ùëñ

(ùêµ‚àí1) √çùêº
ùëñ=1 ùë•ùëñ+ ùêµ 1 ‚àí√çùêº
ùëñ=1 ùë•ùëñ

‚â§ùúÇ3,
(47)
for some target ùúÇ3 > 0. In this formulation we retain the standard capacity constraints in (40), so idling is
permitted when the TPOT cap is tight; the revenue objective still discourages unnecessary idling whenever
additional work can be served.
Alternatively, we can incorporate TPOT directly into the objective by penalizing total prefill load:
ùëô3 = ùúÇ‚Ä≤
3
ùúè(ùêµ‚àí1) √çùêº
ùëñ=1 ùë•ùëñ+ 1
ùõæùêµ 1 ‚àí√çùêº
ùëñ=1 ùë•ùëñ

(ùêµ‚àí1) √çùêº
ùëñ=1 ùë•ùëñ+ ùêµ 1 ‚àí√çùêº
ùëñ=1 ùë•ùëñ
 ,
(48)
where ùúÇ‚Ä≤
3 > 0 controls the strength of the revenue‚Äìlatency trade-off.
5.1.
SLI-Aware Gate-and-Route Control Policy
To incorporate SLIs into the steady-state optimization, we augment the objective of Section 3.1 (or Section 4.2
for separate charging) by subtracting a weighted sum of penalty terms:
max
(x,y,q)‚ààFK
ùêº‚àëÔ∏Å
ùëñ=1
ùë§ùëñ
 ùúáùëö,ùëñùë¶ùëö,ùëñ+ ùúáùë†,ùëñùë¶ùë†,ùëñ
 ‚àí
‚àëÔ∏Å
ùëò‚ààK
ùëôùëò,
(49)
where K indexes the active SLIs and ùëôùëòare chosen from (44), (46), (48), or other application-specific
penalties. We append the SLI-specific constraints to the feasibility constraints from Section 3.1, and denote
the feasibility region by FK.
To realize the targets (ùë¶‚òÖ
ùëö,ùëñ, ùë¶‚òÖ
ùë†,ùëñ) derived from the SLI-aware planning problem, we employ a randomized
decode router. For clarity of exposition, we focus on the zero-buffer case (ùëû‚òÖ
ùëë,ùëñ= 0) in this section, which
avoids memory pressure and simplifies the tracking mechanism. An extension that accommodates persistent
decode queues (ùëû‚òÖ
ùëë,ùëñ> 0) is provided in Section EC.6.
Static planning. This phase follows the identical procedure as defined in Section 4, determining the
cluster-level queue targets ùëÑ‚Ä†
P,ùëñand the GPU partition sets (Gmix and Gsolo) based on the optimal solution
(ùë•‚òÖ
ùëñ, ùë¶‚òÖ
ùëö,ùëñ, ùë¶‚òÖ
ùë†,ùëñ, ùëû‚òÖ
ùëù,ùëñ) of the corresponding SLI-aware optimization problem in (49).
Dynamic control. The prefill gate remains the same as the Gate-and-Route policy in Section 4.1. The
decode router splits the decode buffer into mixed and solo components and computes the class-ùëñsolo
probability
ùëùùë†,ùëñ:=
Ô£±Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥
ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ
ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ+ ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ
,
if ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ+ ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ> 0,
1,
otherwise.


--- Page 23 ---
23
Upon prefill or decode completion of class ùëñ, draw ùëà‚àºUnif(0,1); route to Gsolo if ùëà‚â§ùëùùë†,ùëñand to Gmix
otherwise, placing the decode uniformly at random among GPUs in the targeted group with free slots (or
queuing in the corresponding buffer if none available). Note that here we logically split the decode buffer into
mixed buffer and solo buffer instead of a single decode buffer mentioned in Section 4.
The router implements a randomized load split that mirrors the fluid targets: each class-ùëñdecode is sent to
solo or mixed with probability ùëùùë†,ùëñchosen so that the long-run fraction of class-ùëñservice provided by solo
vs. mixed matches (ùë¶‚òÖ
ùë†,ùëñ, ùë¶‚òÖ
ùëö,ùëñ). As many decodes are routed over time, the law of large numbers forces the
realized occupancies  ùë¶ùëõ
ùëö,ùëñ/ùëõ, ùë¶ùëõ
ùë†,ùëñ/ùëõ to track these target proportions, effectively ‚Äúreshuffling‚Äù decode work
until the stochastic system hovers around the desired steady-state levels.
The asymptotic optimality guarantee of Theorem 2 extends to the SLI-aware policy under mild regularity
on the optimization problem (bounded penalties, etc.).
THEOREM 4 (Occupancy Convergence and Asymptotic Optimality of SLI-Aware Policy). Let
(ùë•‚òÖ
ùëñ, ùë¶‚òÖ
ùëö,ùëñ, ùë¶‚òÖ
ùë†,ùëñ, ùëû‚òÖ
ùëù,ùëñ, ùëû‚òÖ
ùëë,ùëñ)ùëñ‚ààI denote an optimal solution of the SLI-aware LP (49) with active SLI set K and
corresponding penalties {ùëôùëò}ùëò‚ààK and constraints. Assume the SLI-aware LP satisfies Slater‚Äôs condition
and each penalty ùëôùëòis bounded and Lipschitz continuous in the decision variables. Further assume that the
selected optimal solution satisfies ùëû‚òÖ
ùëë,ùëñ= 0 for all ùëñ(e.g., by including ùëûùëë,ùëñ= 0 in the constraints). Under the
SLI-aware control policy, the scaled steady-state occupancies converge:
lim
ùëõ‚Üí‚àû
1
ùëõE

ùëã(ùëõ)
ùëñ

= ùë•‚òÖ
ùëñ,
lim
ùëõ‚Üí‚àû
1
ùëõE

ùëå(ùëõ)
ùëö,ùëñ

= ùë¶‚òÖ
ùëö,ùëñ,
lim
ùëõ‚Üí‚àû
1
ùëõE

ùëå(ùëõ)
ùë†,ùëñ

= ùë¶‚òÖ
ùë†,ùëñ,
for all ùëñ‚ààI, and the per-GPU SLI-aware objective value converges to optimality:
lim
ùëõ‚Üí‚àû
1
ùëõE
" ùêº‚àëÔ∏Å
ùëñ=1
ùë§ùëñ

ùúáùëö,ùëñùëå(ùëõ)
ùëö,ùëñ+ ùúáùë†,ùëñùëå(ùëõ)
ùë†,ùëñ

‚àíùëõ
‚àëÔ∏Å
ùëò‚ààK
ùëôùëò
 ùëø(ùëõ)/ùëõ,ùíÄ(ùëõ)
ùëö/ùëõ,ùíÄ(ùëõ)
ùë†
/ùëõ
#
=
ùêº‚àëÔ∏Å
ùëñ=1
ùë§ùëñ

ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ+ ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ

‚àí
‚àëÔ∏Å
ùëò‚ààK
ùëôùëò
 ùíô‚òÖ, ùíö‚òÖ
ùëö, ùíö‚òÖ
ùë†
,
where ùëø(ùëõ) = (ùëã(ùëõ)
1
, . . . , ùëã(ùëõ)
ùêº
) and similarly for ùíÄ(ùëõ)
ùëö, ùíÄ(ùëõ)
ùë†
.
6.
Numerical Experiments
In this section, we evaluate the performance of the proposed Gate-and-Route policy through event-driven
simulations calibrated with real-world LLM inference profiles. We compare our approach against standard
industry heuristics across varying cluster sizes, demonstrating that our fluid-based control effectively
maximizes system throughput and revenue while satisfying SLI constraints in the long run. We also conduct
sensitivity analysis to understand the tradeoff between SLIs, GPU configurations, and revenue.


--- Page 24 ---
24
64
128
256
512
1024
2048
Chunk Size (C)
0.02
0.04
0.06
0.08
0.10
0.12
0.14
Time (s/Iteration)
Mean ¬±1 Std Range
Mean Iteration Time
Fitted Line: 
= 0.017406 + 0.000062C
64
128
256
512
1024
2048
Chunk Size (C)
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
Time (s/Iteration)
Mean ¬±1 Std Range
Mean Iteration Time
Fitted Line: 
= 0.015241 + 0.000036C
Figure 3
Calibration of mixed-iteration hyperparameters (ùõº, ùõΩ) for Qwen-8B (left) and Qwen-4B (right). Dots
show the empirical mean iteration time ùúèfor each chunk size ùê∂, and the solid line is the OLS fit ùúè= ùõº+ ùõΩùê∂. For
Qwen-8B, the fitted line is ùúè= 0.0174 + 6.2 √ó 10‚àí5 ùê∂with ùëÖ2 = 0.998; for Qwen-4B, it is ùúè= 0.0152 + 3.6 √ó 10‚àí5 ùê∂with
ùëÖ2 = 0.997.
6.1.
Calibration of Hyperparameters
We begin by calibrating the iteration-time parameters ùõº, ùõΩ, and ùõæused in the subsequent numerical
experiments. Our measurements are conducted on a server equipped with 4√ó NVIDIA A100-SXM4-40GB
GPUs; detailed hardware and software specifications are reported in the EC (Table EC.3). In practice, we use
vLLM version 0.11.0 to evaluate two models: Qwen-4B and Qwen-8B, and for each model we measure both
mixed and solo decode modes.
For mixed-mode calibration, we vary the prefill chunk size ùê∂and record the mean iteration time ùúèon a
single GPU, then fit a linear model ùúè‚âàùõº+ ùõΩùê∂via ordinary least squares. As shown in Figure 3, the fitted
lines closely track the empirical means: for Qwen-8B we obtain ùõº‚âà0.0174 and ùõΩ‚âà6.2 √ó 10‚àí5, and for
Qwen-4B we obtain ùõº‚âà0.0152 and ùõΩ‚âà3.6 √ó 10‚àí5, with coefficients of determination ùëÖ2 > 0.99 in both
cases. This supports the linear mixed-iteration model used in Section 2.2.
For solo decode, we set the per-stream token speed ùõæto the empirical mean across runs. In our setup, this
yields ùõæ‚âà45.45 tokens/s for Qwen-8B and ùõæ‚âà52.63 tokens/s for Qwen-4B.
6.2.
Convergence Analysis
We evaluate the asymptotic performance of our proposed policies using a two-class instance (ùêº= 2) calibrated
to the hardware specifications derived in Section 6.1. The system parameters are ùêµ= 16, ùê∂= 256, ùõº= 0.0174,
and ùõΩ= 6.2 √ó 10‚àí5. The solo decode speed is set to ùõæ‚âà45.45 tokens/s (corresponding to a per-token latency
of 0.022s).
We define two distinct workload classes to represent the heterogeneity discussed in Table EC.1:
‚Ä¢ Class 0 (Decode-Heavy): ùëÉ0 = 300, ùê∑0 = 1000. Represents tasks like code generation.
‚Ä¢ Class 1 (Prefill-Heavy): ùëÉ1 = 3000, ùê∑1 = 400. Represents tasks like paper summarization or context-
heavy analysis.


--- Page 25 ---
25
Arrival rates are symmetric with ùúÜ= [0.5,0.5], and abandonment rates are ùúÉ= [0.1,0.1]. We utilize the
separate charging scheme with prices ùëêùëù= 0.1 and ùëêùëë= 0.2. We simulate the system across varying scales
ùëõ‚àà{5, 20, 50,200, 500}, running 5 random seeds per configuration to capture stochastic variability.
Revenue and Queue Convergence. We first validate the asymptotic optimality of the standard
gate-and-route policy. Figure 4 illustrates the per-GPU revenue and queue lengths as the system scale ùëõ
increases.
As predicted by the fluid limit, the average per-GPU revenue (Left) converges to the optimal value ùëÖ‚òÖ
derived from the steady-state LP. The shaded error bands, representing the standard deviation across seeds,
narrow significantly as ùëõ‚Üí‚àû, confirming the concentration of measure. Simultaneously, the normalized
queue lengths for both Class 0 (Top Right) and Class 1 (Bottom Right) stabilize near their fluid targets.
Notably, the prefill gate effectively regulates the admission process, ensuring that the stochastic queue lengths
track the fluid-optimal backlog required to maintain utilization, while the decode buffer remains negligible.
0
2000
4000
6000
8000
10000
Time (s)
180
200
220
240
260
280
300
320
Revenue Rate (per GPU)
n=500
n=200
n=50
n=20
n=5
OPT
(a) Convergence of per-GPU revenue.
0
2000
4000
6000
8000
10000
Time (s)
0.0
0.2
0.4
0.6
0.8
1.0
Prefill Queue Length (per GPU)
Class 0: Prefill Queue Length
0
2000
4000
6000
8000
10000
Time (s)
0.0
0.2
0.4
0.6
0.8
1.0
Decode Queue Length (per GPU)
Class 0: Decode Queue Length
n=500
n=200
n=50
n=20
n=5
OPT
(b) Convergence of average queue lengths for Class 0.
0
2000
4000
6000
8000
10000
Time (s)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Prefill Queue Length (per GPU)
Class 1: Prefill Queue Length
0
2000
4000
6000
8000
10000
Time (s)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Decode Queue Length (per GPU)
Class 1: Decode Queue Length
n=500
n=200
n=50
n=20
n=5
OPT
(c) Convergence of average queue lengths for Class 1.
Figure 4
Asymptotic convergence of Revenue and Queue Lengths under the Gate-and-Route policy.
Occupancy Convergence Analysis. While the aggregate revenue converges, a deeper inspection
reveals a discrepancy in the convergence behavior of specific resource occupancies under the standard policy.
Figure 5 plots the fluid-scaled prefill occupancy ùëã(ùëõ)
ùëñ
/ùëõand decode occupancy ùëå(ùëõ)
ùëñ
/ùëõ.
We observe that the prefill occupancy (Top) converges tightly to the LP solution ùë•‚òÖ
ùëñ. This is expected, as
the prefill gate explicitly targets these values. However, the decode occupancy (Bottom) exhibits persistent
variance and deviates from the specific class-wise breakdown ùë¶‚òÖ
ùëö,ùëñ+ ùë¶‚òÖ
ùë†,ùëñpredicted by the LP, even though
the total reward is optimal. This occurs because the standard router prioritizes any available solo slot to
maximize throughput (work-conservation) but is indifferent to which class occupies that slot. Consequently,


--- Page 26 ---
26
the specific mix of Class 1 vs. Class 2 in decode slots fluctuates, satisfying the aggregate capacity constraints
but failing to converge to the specific fluid vector ùë¶‚òÖ.
0
2000
4000
6000
8000
10000
Time (s)
0.0000
0.0025
0.0050
0.0075
0.0100
0.0125
0.0150
0.0175
Prefill Resource (per GPU)
Class 0: Prefill Resource
0
2000
4000
6000
8000
10000
Time (s)
0.0
0.5
1.0
1.5
2.0
2.5
Mixed Decode Resource (per GPU)
Class 0: Mixed Decode Resource
0
2000
4000
6000
8000
10000
Time (s)
0
2
4
6
8
10
Solo Decode Resource (per GPU)
Class 0: Solo Decode Resource
n=500
n=200
n=50
n=20
n=5
OPT
0
2000
4000
6000
8000
10000
Time (s)
0.000
0.025
0.050
0.075
0.100
0.125
0.150
0.175
Prefill Resource (per GPU)
Class 1: Prefill Resource
0
2000
4000
6000
8000
10000
Time (s)
0.0
0.5
1.0
1.5
2.0
2.5
Mixed Decode Resource (per GPU)
Class 1: Mixed Decode Resource
0
2000
4000
6000
8000
10000
Time (s)
1
2
3
4
Solo Decode Resource (per GPU)
Class 1: Solo Decode Resource
n=500
n=200
n=50
n=20
n=5
OPT
Figure 5
Occupancy convergence under the Gate-and-Route Policy. Top: Class 0 (Decode-Heavy). Bottom:
Class 1 (Prefill-Heavy). Note the loose convergence for decode occupancy (ùë¶).
SLI-Aware Convergence. To address the decode occupancy drift and enforce strict adherence to the
fluid plan (crucial for fairness SLIs), we employ the SLI-aware gate-and-route policy described in Section 5.1.
This policy utilizes a stochastic router with probabilities ùëùùë†,ùëñderived from the LP solution to probabilistically
route jobs to mixed or solo pools.
Figure 6 demonstrates the impact of this policy. Unlike the standard case, both the prefill occupancy and
the decode occupancy now converge strictly to the optimization solution for both classes. By enforcing the
specific routing split ùë¶‚òÖ
ùëö,ùëñvs. ùë¶‚òÖ
ùë†,ùëñ, the SLI-aware policy ensures that the stochastic system structurally mimics
the fluid limit. This validates Theorem 4, confirming that we can achieve precise control over resource
allocation distributions at scale, which is a prerequisite for satisfying multi-objective SLIs in the long run.
6.3.
Policy Performance: Baselines and Ablations
To evaluate the efficacy of our proposed framework, we compare several alternative scheduling policies
across a variety of instances. These instances encompass diverse infrastructure hyperparameters (ùõºfrom
0.15 to 0.02, ùõΩfrom 10‚àí5 to 10‚àí3, and ùõæfrom 10 to 50), distinct target user compositions (ùëÉùëñ, ùê∑ùëñfrom 200
to 3000), and different arrival rates ùúÜfrom 0.25 to 0.5, and all under GPU number ùëõ= 500. We report the
normalized revenue rate (scaled from 0 to 1) along with standard deviations to demonstrate robustness.


--- Page 27 ---
27
0
2000
4000
6000
8000
10000
Time (s)
0.0000
0.0025
0.0050
0.0075
0.0100
0.0125
0.0150
0.0175
Prefill Resource (per GPU)
Class 0: Prefill Resource
0
2000
4000
6000
8000
10000
Time (s)
0.0
0.1
0.2
0.3
0.4
0.5
Mixed Decode Resource (per GPU)
Class 0: Mixed Decode Resource
0
2000
4000
6000
8000
10000
Time (s)
0
2
4
6
8
10
Solo Decode Resource (per GPU)
Class 0: Solo Decode Resource
n=500
n=200
n=50
n=20
n=5
OPT
0
2000
4000
6000
8000
10000
Time (s)
0.000
0.025
0.050
0.075
0.100
0.125
0.150
0.175
Prefill Resource (per GPU)
Class 1: Prefill Resource
0
2000
4000
6000
8000
10000
Time (s)
0.0
0.5
1.0
1.5
2.0
2.5
Mixed Decode Resource (per GPU)
Class 1: Mixed Decode Resource
0
2000
4000
6000
8000
10000
Time (s)
0.0
0.5
1.0
1.5
2.0
2.5
Solo Decode Resource (per GPU)
Class 1: Solo Decode Resource
n=500
n=200
n=50
n=20
n=5
OPT
Figure 6
Occupancy convergence under the SLI-Aware Gate-and-Route Policy. Top: Class 0 (Decode-Heavy).
Bottom: Class 1 (Prefill-Heavy). Both prefill (ùë•) and decode (ùë¶) occupancies now converge tightly to the LP
targets.
Candidate Policies We evaluate the following five policies together with our proposed policy to isolate
the contributions of our admission control (Gate), decode router (Route), and static planning:
‚Ä¢ Gate-and-Route (Greedy Router) with Static Planning (GG-SP): This is our proposed policy. It
utilizes the queue-length based admission control (Gate) derived from our theoretical analysis. For
routing, it uses a Greedy router in the gate-and-route architecture; specifically, the router prioritizes
dispatching waiting decode tasks to available solo GPUs over initiating new prefill tasks.
‚Ä¢ FCFS-and-Immediate without Static Planning (FI-WSP): This represents the standard industry
baseline in Sarathi-serve (Agrawal et al. 2024). It employs First-Come-First-Served (FCFS) admission
without any queue-length based control. Crucially, it uses an ‚ÄúImmediate‚Äù execution model where
prefill and decode phases are coupled: the decode phase commences immediately on the same GPU slot
following the prefill completion, occupying the resource continuously until the request is finished.
‚Ä¢ Gate-and-Immediate without Static Planning (GI-WSP): This policy introduces our queue-length
based Gate control to the coupled architecture. Like FI-WSP, it executes decode immediately after
prefill on the same slot. Comparing GI-WSP against GG-SP highlights the specific performance gains
achieved by decoupling the prefill and decode phases.
‚Ä¢ Gate-and-FCFS without Static Planning (GF-WSP): This policy utilizes the same decoupled
architecture and queue-length based Gate as our proposed method. However, the routing logic is
inverted to a naive FCFS approach: whenever a GPU slot becomes available, the system prioritizes
admitting a new prefill task rather than processing a waiting decode task. Comparing this against GG-SP
demonstrates the necessity of the decode-first (Greedy) prioritization in a decoupled system.


--- Page 28 ---
28
‚Ä¢ FCFS-and-Route (Greedy Router) with Static Planning (FG-SP): This policy mirrors our proposed
gate-and-route architecture with the same Greedy router, but removes the admission control mechanism
and accepts all arrivals via FCFS. Comparing FG-SP against GG-SP isolates the role of the Gate in
preventing system overload and maintaining revenue stability.
The comparative results, illustrated in Figure 7, reveal a critical hierarchy of design elements. We find that
while the admission gate alone (GI-WSP) provides a meaningful improvement over the industry baseline
(FI-WSP), other components like static planning can actually be counter-productive when applied in isolation
(as seen in FG-SP), where rigid resource partitioning without traffic regulation may lead to localized
imbalances. The full revenue potential is only unlocked in our integrated GG-SP policy: the gate controls the
decode workloads, while the static planning enables the greedy router to extract maximal efficiency from the
hardware pools. Additionally, the consistently low standard deviations observed for GG-SP across varied
instances underscore the robustness of this integrated design.
0
2000
4000
6000
8000
10000
Time (s)
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
Revenue Rate (Normalized to OPT)
GG-SP(Ours)
FI-WSP(Sarathi)
GI-WSP
GF-WSP
FG-SP
OPT
Figure 7
Comparison of normalized average revenue across different policies. The results aggregate
performance over various infrastructure settings (ùõº, ùõΩ, ùõæ) and user class combinations. Error bars indicate the
standard deviation.
6.4.
The Cost of Service Quality and Hardware Resources
To investigate the cost of service quality and fairness, we analyze the trade-off between total revenue and
specific SLIs. We utilize the same system instance defined in the convergence analysis to ensure consistency,
which is representative since it consists of long-prefill-short-decode and short-prefill-long-decode classes.
We formulate this as a constrained optimization problem where we maximize revenue subject to a strict
constraint on exactly one SLI metric at a time: Prefill Fairness, Decode Fairness, or Time Per Output Token
(TPOT), while relaxing the others. This approach allows us to isolate the ‚Äúprice‚Äù of each specific constraint
in terms of lost revenue.


--- Page 29 ---
29
The Shadow Price of SLIs Figure 8 illustrates the Pareto frontiers for three operational constraints. We
interpret the slope of these curves as the shadow price, which is the marginal revenue sacrificed to achieve a
stricter SLI target. It reveals distinct economic sensitivities:
0
0.1
0.2
0.3
0
100
200
Œ∑1
Revenue
(a) Prefill Fairness (ùúÇ1).
0
0.5
1
0
100
200
Œ∑2
Revenue
(b) Decode Fairness (ùúÇ2).
0.02
0.022
0.024
0
100
200
Œ∑3
Revenue
(c) TPOT Limit (ùúÇ3).
Figure 8
Pareto frontiers illustrating the shadow price of Service Level Indicators (SLIs).
‚Ä¢ Asymmetric Costs of Fairness (Figs. 8a and 8b): We observe a stark contrast between the shadow
prices of fairness at different stages of the inference pipeline. Prefill fairness (ùúÇ1) incurs a steep revenue
penalty because the prefill stage acts as the primary bottleneck; imposing rigid class-mix constraints
here prevents the scheduler from aligning admissions with the hardware‚Äôs optimal operating point,
leading to a structural mismatch between job arrivals and downstream capacity. In contrast, the nearly
flat frontier for decode fairness (ùúÇ2) suggests a negligible shadow price. This implies that once a request
has entered the system, rebalancing its processing speed relative to other classes barely degrades total
revenue.
‚Ä¢ The Price of Low TPOT (Fig. 8c): The shadow price of the TPOT constraint increases significantly as
the target latency approaches 0.022s, a lower bound determined by the solo-decode rate ùõæ. Near this
threshold, the feasible region for prefill throughput shrinks rapidly, leading to a substantial decrease in
optimal revenue as the system prioritizes meeting the stringent latency requirement over throughput.
GPU configurations and Revenue We next analyze the sensitivity of the system‚Äôs performance to the
maximum batch size (ùêµ) and the hyperparameters (ùõº, ùõΩ, ùõæ). Figure 9 illustrates the trade-off between total
Revenue (primary objective, left y-axis) and TPOT (latency cost, right y-axis).
The results indicate distinct operational trends. First, revenue increases with batch size ùêµbut saturates
around ùêµ= 16, suggesting diminishing returns on memory scaling beyond this point. Second, the system is
highly sensitive to the computational penalty ùõΩ; as ùõΩincreases, revenue drops sharply. Finally, ùõæacts as a
strong incentive, positively correlating with higher revenue and lower latency.
Figure 10a visualizes the revenue landscape across the joint configuration space of memory capacity
(proxied by ùêµ) and computational speed (proxied by ùõΩ). This mapping serves two critical strategic functions
for infrastructure management.


--- Page 30 ---
30
0
5
10
15
20
25
30
35
100
200
300
Max Batch Size (B)
Revenue
Max Batch Size (B)
0.022
0.0225
0.023
0.0235
0.024
0.0245
TPOT
0
0.02
0.04
0.06
0.08
0.1
180
200
220
Alpha (Œ±)
Revenue
Alpha (Œ±)
0.02
0.04
0.06
0.08
TPOT
0
0.0002
0.0004
0.0006
0.0008
0.001
100
150
200
Beta (Œ≤)
Revenue
Beta (Œ≤)
0.05
0.1
0.15
0.2
TPOT
0
10
20
30
40
50
100
150
200
Gamma (Œ≥)
Revenue
Gamma (Œ≥)
0
2
4
6
8
TPOT
Figure 9
Parameter sensitivity analysis showing the impact of maximum batch size (ùêµ), ùõº, ùõΩ, and ùõæon
revenue and TPOT. Blue lines indicate revenue (higher is better); red lines indicate TPOT (lower is better).
First, the heatmap provides a quantitative basis for hardware selection by allowing operators to overlay GPU
market prices onto the revenue landscape to identify configurations with the highest return on investment
(ROI). Second, its gradient directs hardware upgrades by revealing the steepest path to revenue growth. By
identifying whether this path favors increasing ùêµor decreasing ùõΩ, decision-makers can pinpoint the primary
bottleneck (memory or compute) and prioritize investments to maximize marginal gains.
Optimal Token Pricing We also examine the optimal pricing structure by analyzing the relationship
between the prefill price ùëêùëùand the decode price ùëêùëë. Specifically, if we investigate the common revenue
maximization problem subject to a total price constraint ùëêùëù+ ùëêùëë= ùëò, the heatmap (Figure 10b) results reveal
a striking invariance: regardless of the magnitude of the budget ùëò, the revenue-maximizing prices consistently
yield a unique, constant ratio ùëêùëù/ùëêùëë. This indicates that the optimal economic balance between prefill and
decode is scale-invariant. Consequently, for pricing strategy, managers should focus on maintaining this
intrinsic cost structure ratio, as the optimal split between prefill and decode prices remains stable even as the
total price level varies.
7.
Conclusions
Efficient LLM inference at scale hinges on resolving the resource contention between compute-bound prefill
and memory-bound decode. In this work, we connect an empirically grounded iteration-time abstraction
with stochastic control to study admission and scheduling in large GPU clusters under token-based revenue
objectives and service constraints. A multiclass many-GPU fluid approximation yields a tractable steady-state


--- Page 31 ---
31
10
20
30
40
Max Batch Size (B)
0.0002
0.0004
0.0006
0.0008
0.0010
Beta ( )
B vs Beta
50
100
150
200
250
300
Revenue ($)
(a) Memory-speed tradeoff (ùêµvs. ùõΩ).
0.05
0.10
0.15
0.20
0.25
0.30
Price cp
0.05
0.10
0.15
0.20
0.25
0.30
Price cd
(cp vs cd)
100
200
300
400
500
Revenue ($)
(b) Optimal token pricing (ùëêùëùvs. ùëêùëë).
Figure 10
Sensitivity analysis of hardware configurations and pricing structures on per-GPU revenue.
linear program that prescribes how to split capacity between mixed and solo modes and how to allocate prefill
occupancy across classes.
Building on this planning formulation, we develop a gate-and-route control architecture with a prefill
admission gate that tracks the fluid occupancy targets and a decode router that keeps downstream capacity
work-conserving. Our analysis establishes a set of structural properties that enable this decomposition,
including the existence of an optimal fluid plan with no steady-state decode buffering (Proposition 1) and a
corresponding asymptotic optimality guarantee for the proposed policy in the many-GPU limit.
Our numerical evaluation is empirically calibrated and intended to illustrate the mechanisms highlighted
by the theory: we calibrate the iteration-time model using real deployments on a modern inference stack, and
then run calibrated event-driven simulations to study the resulting control behavior. In particular, simulations
confirm two qualitative predictions from the asymptotic analysis: per-GPU revenue approaches the fluid
optimum as the cluster scales, and persistent decode backlogs are avoided under the proposed control. We
further compare against representative heuristic baselines and ablations in our calibrated setting, with the
goal of clarifying mechanisms rather than providing a production benchmark.
Our results also yield actionable implications for service providers. First, billing and scheduling objectives
need not coincide: while separate charging for prefill and decode tokens is natural for accounting, optimizing
the scheduler against a separate objective can encourage overly aggressive prefill admission and shift
congestion downstream. A practical recommendation is therefore to bill by phase if desired, while scheduling
against an end-to-end (bundled) completion objective to align incentives with user-perceived performance.
Second, incorporating SLIs as constraints or penalties in the planning problem provides a systematic way to
study the revenue implications of fairness and latency requirements; in our setting, enforcing fairness at the
prefill stage is typically more revenue-costly than at the decode stage.


--- Page 32 ---
32
Several avenues exist to extend the theoretical depth and practical scope of this work. First, one can relax
the assumption of exponential service times by employing measure-valued processes. This formulation
would accommodate general service distributions, enabling more granular, state-dependent control policies.
Second, to move beyond mean-value analysis, developing diffusion approximations that characterize stochastic
variability will provide rigorous guarantees for tail-latency SLIs. Finally, the model can be generalized to
heterogeneous infrastructures, orchestrating inference across clusters composed of diverse GPU generations
and distinct agent architectures.
References
Agrawal A, Kedia N, Panwar A, Mohan J, Kwatra N, Gulavani BS, Tumanov A, Ramjee R (2024) Taming throughput-
latency tradeoff in LLM inference with Sarathi-Serve. 18th USENIX Symposium on Operating Systems Design
and Implementation (OSDI 24).
Ai R, Pan Y, Simchi-Levi D, Tambe M, Xu H (2025) Beyond majority voting: LLM aggregation by leveraging
higher-order information.
Aminabadi RY, Rajbhandari S, Awan AA, Li C, Li D, Zheng E, Ruwase O, Smith M, Zhang M, Rasley J, et al.
(2022) DeepSpeed-Inference: Enabling efficient inference of transformer models at unprecedented scale. SC22:
International Conference for High Performance Computing, Networking, Storage and Analysis, 1‚Äì15 (IEEE).
Anthropic (2025) Claude API pricing. URL https://claude.com/pricing#api.
Ao R, Luo G, Simchi-Levi D, Wang X (2025) Optimizing LLM inference: Fluid-guided online scheduling with memory
constraints.
Ata B (2006) Dynamic control of a multiclass queue with thin arrival streams. Operations Research 54(5):876‚Äì892.
Atar R, Giat C, Shimkin N (2010) The cùúá/ùúÉrule for many-server queues with abandonment. Operations Research
58(5):1427‚Äì1439.
Bassamboo A, Randhawa RS (2010) On the accuracy of fluid models for capacity sizing in queueing systems with
impatient customers. Operations Research 58(5):1398‚Äì1413.
Bassamboo A, Randhawa RS (2016) Scheduling homogeneous impatient customers. Management Science 62(7):2129‚Äì
2147.
Chen Z, Ye Y, Zhou Z (2025) Adaptively robust LLM inference optimization under prediction uncertainty. arXiv preprint
URL http://dx.doi.org/10.48550/arXiv.2508.14544.
Conover M, Hayes M, Mathur A, Xie J, Wan J, Shah S, Ghodsi A, Wendell P, Zaharia M, Xin R (2023) Free dolly: Intro-
ducing the world‚Äôs first truly open instruction-tuned LLM. URL https://www.databricks.com/blog/
2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm.
Dai JG, Tezcan T (2011) State Space Collapse in Many-Server Diffusion Limits of Parallel Server Systems. Mathematics
of Operations Research 36(2):271‚Äì320.


--- Page 33 ---
33
Dai T, Swaminathan JM (2026) Artificial intelligence and operations: A foundational framework of emerging research
and practice. Production and Operations Management .
Dai Z, Yang Z, Yang Y, Carbonell JG, Le Q, Salakhutdinov R (2019) Transformer-XL: Attentive language models
beyond a fixed-length context. Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics, 2978‚Äì2988.
DeepSeek (2025) Models & pricing ‚Äî DeepSeek API docs. URL https://api-docs.deepseek.com/quick_
start/pricing/.
Dong J, Feldman P, Yom-Tov GB (2015) Service systems with slowdowns: Potential failures and proposed solutions.
Operations Research 63(2):305‚Äì324.
Ghosh S, Lam H (2019) Robust Analysis in Stochastic Simulation: Computation and Performance Guarantees. Operations
Research 67(1):232‚Äì249.
Google (2025) Billing ‚Äî Gemini API ‚Äî Google AI for developers. URL https://ai.google.dev/
gemini-api/docs/billing/.
Google Cloud (2025) Vertex AI generative AI pricing. URL https://cloud.google.com/vertex-ai/
generative-ai/pricing.
Harrison JM, Zeevi A (2004) Dynamic scheduling of a multiclass queue in the Halfin-Whitt heavy traffic regime.
Operations Research 52(2):243‚Äì257.
Hu H, Simchi-Levi D (2025) Pre-trained AI model assisted online decision-making under missing covariates: A
theoretical perspective.
Huang C, Tang Z, Hu S, Jiang R, Zheng X, Ge D, Wang B, Wang Z (2025) ORLM: A customizable framework in
training large models for automated optimization modeling. Operations Research Articles in Advance.
Ibrahim R (2018) Sharing delay information in service systems: A literature survey. Queueing Systems 89:49‚Äì79.
Ibrahim R, Whitt W (2009) Real-Time delay estimation in overloaded multiserver queues with abandonments.
Management Science 55(10):1729‚Äì1742.
Jaillet P, Jiang J, Mellou K, Molinaro M, Podimata C, Zhou Z (2025) Online scheduling for LLM inference with KV
cache constraints. arXiv preprint URL http://dx.doi.org/10.48550/arXiv.2502.07115.
Kwon W, Li Z, Zhuang S, Sheng Y, Zheng L, Yu CH, Gonzalez J, Zhang H, Stoica I (2023) Efficient memory
management for large language model serving with PagedAttention. Proceedings of the 29th Symposium on
Operating Systems Principles, 611‚Äì626.
Lam H (2016) Robust Sensitivity Analysis for Stochastic Systems. Mathematics of Operations Research 41(4):1248‚Äì1275.
Li B, Jiang Y, Gadepally V, Tiwari D (2024) LLM inference serving: Survey of recent advances and opportunities. 2024
IEEE High Performance Extreme Computing Conference (HPEC), 1‚Äì8.
Li Y, Dai J, Peng T (2025) Throughput-optimal scheduling algorithms for LLM inference and AI agents. arXiv preprint
URL http://dx.doi.org/10.48550/arXiv.2504.07347.


--- Page 34 ---
34
Long Z, Shimkin N, Zhang H, Zhang J (2020) Dynamic scheduling of multiclass many-server queues with abandonment:
The generalized cùúá/h rule. Operations Research 68(4):1218‚Äì1230.
Long Z, Zhang H, Zhang J, Zhang ZG (2024) The generalized c/ùúárule for queues with heterogeneous server pools.
Operations Research 72(6):2488‚Äì2506.
Mitzenmacher M, Shahout R (2025) Queueing, predictions, and large language models: Challenges and open problems.
Stochastic Systems Articles in Advance.
OpenAI (2025) OpenAI API pricing. URL https://openai.com/api/pricing.
Patel P, Choukse E, Zhang C, Shah A, Goiri I, Maleki S, Bianchini R (2024) SplitWise: Efficient generative LLM
inference using phase splitting. Proceedings of the 51st International Symposium on Computer Architecture (ISCA
2024), 3775‚Äì3801.
Shazeer N (2019) Fast transformer decoding: One write-head is all you need. arXiv preprint URL http://dx.doi.
org/10.48550/arXiv.1911.02150.
Simchi-Levi D, Dai T, Menache I, Wu MX (2025) Democratizing optimization with generative AI. SSRN preprint URL
http://dx.doi.org/10.2139/ssrn.5511218.
Simchi-Levi D, Mellou K, Menache I, Pathuri J (2026) Large language models for supply chain decisions. AI in Supply
Chains, volume 27 of Springer Series in Supply Chain Management (Springer).
Sun B, Huang Z, Zhao H, Xiao W, Zhang X, Li Y, Lin W (2024) Llumnix: Dynamic scheduling for large language
model serving. 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), 173‚Äì191.
Wang M, Ye Y, Zhou Z (2025) LLM serving optimization with variable prefill and decode lengths. arXiv preprint URL
http://dx.doi.org/10.48550/arXiv.2508.06133.
Whitt W (2006) Fluid models for multiserver queues with abandonments. Operations Research 54(1):37‚Äì54.
Wu C, Bassamboo A, Perry O (2019) Service system with dependent service and patience times. Management Science
65(3):1151‚Äì1172.
Yu GI, Jeong J, Kim G, Kim S, Chun BG (2022) Orca: A distributed serving system for transformer-based generative
models. 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), 521‚Äì538
(USENIX Association).
Zhang J (2013) Fluid models of many-server queues with abandonment. Queueing Systems 73(2):147‚Äì193.
Zhao W, Ren X, Hessel J, Cardie C, Choi Y, Deng Y (2024) WildChat: 1m ChatGPT interaction logs in the wild. The
Twelfth International Conference on Learning Representations.
Zhao WX, Zhou K, Li J, Tang T, Wang X, Hou Y, Min Y, Zhang B, Zhang J, Dong Z, Du Y, Yang C, Chen Y, Chen Z,
Jiang J, Ren R, Li Y, Tang X, Liu Z, Liu P, Nie JY, Wen JR (2025) A survey of large language models.
Zheng L, Chiang WL, Sheng Y, Li T, Zhuang S, Wu Z, Zhuang Y, Li Z, Lin Z, Xing E, et al. (2024) LMSYS-Chat-
1M: A large-scale real-world LLM conversation dataset. The Twelfth International Conference on Learning
Representations.


--- Page 35 ---
35
Zhong Y, Liu S, Chen J, Hu J, Zhu Y, Liu X, Jin X, Zhang H (2024) DistServe: Disaggregating prefill and decoding for
goodput-optimized large language model serving. 18th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 24), 193‚Äì210.
Zhou Z, Ning X, Hong K, Fu T, Xu J, Li S, Lou Y, Wang L, Yuan Z, Li X, Yan S, Dai G, Zhang XP, Dong Y, Wang Y
(2024) A survey on efficient inference for large language models.


--- Page 36 ---
ec1
EC.1.
Proof of Proposition 1
We mainly adopt the strategy that given an optimal solution with ùëûùëë,ùëñ> 0, we can always construct an
optimal solution with ùëû‚Ä≤
ùëë,ùëñ= 0 based on the solution given above. The proof follows from the following steps:
First, we eliminate ùëûùëëin the original optimization problem to get a new optimization problem (P‚Ä≤)
which better characterizes our target. Fix a class ùëñwith ùúÉùëñ> 0. From the second constraint in (40), we have:
ùúáùëù,ùëñùë•ùëñ‚àíùúÉùëñùëûùëë,ùëñ= ùúáùëö,ùëñùë¶ùëö,ùëñ+ ùúáùë†,ùëñùë¶ùë†,ùëñ,
Considering ùëûùëë,ùëñ‚â•0, we obtain the following inequality
ùúáùëö,ùëñùë¶ùëö,ùëñ+ ùúáùë†,ùëñùë¶ùë†,ùëñ‚â§ùúáùëù,ùëñùë•ùëñ,
(EC.1)
with
ùëûùëë,ùëñ= ùúáùëù,ùëñùë•ùëñ‚àíùúáùëö,ùëñùë¶ùëö,ùëñ‚àíùúáùë†,ùëñùë¶ùë†,ùëñ
ùúÉùëñ
‚â•0.
(EC.2)
Here ùëûùëë,ùëñis exactly the slack variable of (EC.1). Without loss of generality, we focus on ùúÉùëñ> 0.
Define the problem (P‚Ä≤):
max
{ùë•ùëñ,ùë¶ùëö,ùëñ,ùë¶ùë†,ùëñ,ùëûùëù,ùëñ}ùêº
ùëñ=1
ùêº‚àëÔ∏Å
ùëñ=1
ùë§ùëñ
 ùúáùëö,ùëñùë¶ùëö,ùëñ+ ùúáùë†,ùëñùë¶ùë†,ùëñ

s.t.
ùêº‚àëÔ∏Å
ùëñ=1
ùë•ùëñ‚â§1,
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶ùëö,ùëñ‚â§(ùêµ‚àí1)
ùêº‚àëÔ∏Å
ùëñ=1
ùë•ùëñ,
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶ùë†,ùëñ‚â§ùêµ

1 ‚àí
ùêº‚àëÔ∏Å
ùëñ=1
ùë•ùëñ

,
ùúÜùëñ‚àíùúÉùëñùëûùëù,ùëñ= ùúáùëù,ùëñùë•ùëñ,
‚àÄùëñ,
ùúáùëö,ùëñùë¶ùëö,ùëñ+ ùúáùë†,ùëñùë¶ùë†,ùëñ‚â§ùúáùëù,ùëñùë•ùëñ,
‚àÄùëñ,
ùë•ùëñ, ùë¶ùëö,ùëñ, ùë¶ùë†,ùëñ, ùëûùëù,ùëñ‚â•0,
‚àÄùëñ.
(EC.3)
For any feasible solution of (EC.3) we can recover a feasible solution of (40) by defining ùëûùëë,ùëñvia (EC.2).
Conversely, any feasible solution of (40) satisfies (EC.1) and thus yields a feasible solution of (EC.3) by
dropping ùëûùëë,ùëñ. Hence the two problems are equivalent and share the same optimal value.
In particular, ùëûùëë,ùëñ= 0 for a given class ùëñis equivalent to the inequality (EC.1) being tight:
ùëûùëë,ùëñ= 0
‚áê‚áí
ùúáùëö,ùëñùë¶ùëö,ùëñ+ ùúáùë†,ùëñùë¶ùë†,ùëñ= ùúáùëù,ùëñùë•ùëñ.


--- Page 37 ---
ec2
Second, we characterize the above optimization problem by using the KKT condition:
Because (EC.3) is a linear program with a nonempty relative interior (Slater point exists: e.g., take
ùë•ùëñ= ùë¶ùëö,ùëñ= ùë¶ùë†,ùëñ= 0 and ùëûùëù,ùëñ= ùúÜùëñ/ùúÉùëñ), the Karush‚ÄìKuhn‚ÄìTucker (KKT) conditions are necessary and sufficient
for optimality.
The Lagrangian of (EC.3) is
L =
ùêº‚àëÔ∏Å
ùëñ=1
ùë§ùëñ
 ùúáùëö,ùëñùë¶ùëö,ùëñ+ ùúáùë†,ùëñùë¶ùë†,ùëñ
 + ùõº

1 ‚àí
ùêº‚àëÔ∏Å
ùëñ=1
ùë•ùëñ

+ ùõΩ

(ùêµ‚àí1)
ùêº‚àëÔ∏Å
ùëñ=1
ùë•ùëñ‚àí
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶ùëö,ùëñ

+ ùõø

ùêµ‚àí
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶ùë†,ùëñ‚àíùêµ
ùêº‚àëÔ∏Å
ùëñ=1
ùë•ùëñ

+
ùêº‚àëÔ∏Å
ùëñ=1
ùúôùëñ
 ùúÜùëñ‚àíùúÉùëñùëûùëù,ùëñ‚àíùúáùëù,ùëñùë•ùëñ

+
ùêº‚àëÔ∏Å
ùëñ=1
ùúÇùëñ
 ùúáùëù,ùëñùë•ùëñ‚àíùúáùëö,ùëñùë¶ùëö,ùëñ‚àíùúáùë†,ùëñùë¶ùë†,ùëñ
 +
ùêº‚àëÔ∏Å
ùëñ=1
 ùúéùë•,ùëñùë•ùëñ+ ùúéùëö,ùëñùë¶ùëö,ùëñ+ ùúéùë†,ùëñùë¶ùë†,ùëñ+ ùúéùëù,ùëñùëûùëù,ùëñ
.
The KKT conditions at an optimal primal‚Äìdual pair  ùë•‚òÖ
ùëñ, ùë¶‚òÖ
ùëö,ùëñ, ùë¶‚òÖ
ùë†,ùëñ, ùëû‚òÖ
ùëù,ùëñ;ùõº‚òÖ, ùõΩ‚òÖ, ùõø‚òÖ, ùúô‚òÖ
ùëñ,ùúÇ‚òÖ
ùëñ, ùúé‚òÖ
ùë•,ùëñ, ùúé‚òÖ
ùëö,ùëñ, ùúé‚òÖ
ùë†,ùëñ, ùúé‚òÖ
ùëù,ùëñ

are:
(i) Stationarity. For each ùëñ,
ùúïL
ùúïùë¶ùëö,ùëñ
= ùë§ùëñùúáùëö,ùëñ‚àíùõΩ‚àíùúÇùëñùúáùëö,ùëñ+ ùúéùëö,ùëñ= 0,
(EC.4)
ùúïL
ùúïùë¶ùë†,ùëñ
= ùë§ùëñùúáùë†,ùëñ‚àíùõø‚àíùúÇùëñùúáùë†,ùëñ+ ùúéùë†,ùëñ= 0,
(EC.5)
ùúïL
ùúïùë•ùëñ
= ‚àíùõº+ (ùêµ‚àí1)ùõΩ‚àíùêµùõø‚àíùúôùëñùúáùëù,ùëñ+ ùúÇùëñùúáùëù,ùëñ+ ùúéùë•,ùëñ= 0,
(EC.6)
ùúïL
ùúïùëûùëù,ùëñ
= ‚àíùúôùëñùúÉùëñ+ ùúéùëù,ùëñ= 0.
(EC.7)
(ii) Complementary slackness.
ùõº

1 ‚àí
ùêº‚àëÔ∏Å
ùëó=1
ùë•ùëó

= 0,
(C1)
ùõΩ
 ùêº‚àëÔ∏Å
ùëó=1
ùë¶ùëö, ùëó‚àí(ùêµ‚àí1)
ùêº‚àëÔ∏Å
ùëó=1
ùë•ùëó

= 0,
(C2)
ùõø
 ùêº‚àëÔ∏Å
ùëó=1
ùë¶ùë†, ùëó+ ùêµ
ùêº‚àëÔ∏Å
ùëó=1
ùë•ùëó‚àíùêµ

= 0,
(C3)
ùúÇùëñ
 ùúáùëö,ùëñùë¶ùëö,ùëñ+ ùúáùë†,ùëñùë¶ùë†,ùëñ‚àíùúáùëù,ùëñùë•ùëñ
 = 0,
‚àÄùëñ,
(C4)
ùúéùë•,ùëñùë•ùëñ= 0, ùúéùëö,ùëñùë¶ùëö,ùëñ= 0, ùúéùë†,ùëñùë¶ùë†,ùëñ= 0, ùúéùëù,ùëñùëûùëù,ùëñ= 0,
‚àÄùëñ.
(C5)
(iii) Primal feasibility. All constraints in (EC.3) hold.
(iv) Dual feasibility.
ùõº, ùõΩ, ùõø, ùúÇùëñ, ùúéùë•,ùëñ, ùúéùëö,ùëñ, ùúéùë†,ùëñ, ùúéùëù,ùëñ‚â•0,
‚àÄùëñ,
ùúôùëñ‚ààR, ‚àÄùëñ.


--- Page 38 ---
ec3
Third, starting from the characterization above, we can begin to construct a satisfying optimal
solution.
Let
(ùë•‚òÖ
ùëñ, ùë¶‚òÖ
ùëö,ùëñ, ùë¶‚òÖ
ùë†,ùëñ, ùëû‚òÖ
ùëù,ùëñ)
be any optimal solution of (EC.3) (hence also of (40)) and let (ùõº‚òÖ, ùõΩ‚òÖ, ùõø‚òÖ, ùúô‚òÖ
ùëñ,ùúÇ‚òÖ
ùëñ, . . . ) be a corresponding
dual optimal solution satisfying the KKT conditions above.
Suppose that there exists an index ùëñ0 such that the following constraint is slack at the optimum, i.e.,
ùúáùëö,ùëñ0ùë¶‚òÖ
ùëö,ùëñ0 + ùúáùë†,ùëñ0ùë¶‚òÖ
ùë†,ùëñ0 < ùúáùëù,ùëñ0ùë•‚òÖ
ùëñ0.
(EC.8)
Then, by complementary slackness (C4), we must have
ùúÇ‚òÖ
ùëñ0 = 0.
We now show that, under the assumption ùõæùúè‚â•(ùêµ‚àí1)/ùêµ, we can reallocate prefill and decode occupancy
across modes so that:
‚Ä¢ the global capacity constraints remain feasible,
‚Ä¢ the per-class decode completion rates ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ+ ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñremain unchanged for all ùëñ,
‚Ä¢ and the slack in (EC.8) is reduced, ultimately to 0,
without changing the objective value. This yields a new optimal solution in which the ùëñ0-th decode constraint
is tight. By repeating the same procedure for every class with slack inequality (EC.8), we obtain an optimal
solution with
ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ+ ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ= ùúáùëù,ùëñùë•‚òÖ
ùëñ,
‚àÄùëñ,
which is equivalent to ùëû‚òÖ
ùëë,ùëñ= 0 in the original formulation.
For each ùëñwith ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ+ ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ< ùúáùëù,ùëñùë•‚òÖ
ùëñ(i.e., ùëû‚òÖ
ùëë,ùëñ> 0 in the original variables), define the gap
Œîùëñ:=
ùúáùëù,ùëñùë•‚òÖ
ùëñ‚àíùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ‚àíùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ
ùúáùëù,ùëñ
‚â•0.
Consider the modified active prefill and prefill queue
Àúùë•ùëñ:= ùë•‚òÖ
ùëñ‚àíŒîùëñ=
ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ+ ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ
ùúáùëù,ùëñ
,
Àúùëûùëù,ùëñ:= ùëû‚òÖ
ùëù,ùëñ+ ùúáùëù,ùëñŒîùëñ
ùúÉùëñ
= ùëû‚òÖ
ùëù,ùëñ+
ùúáùëù,ùëñùë•‚òÖ
ùëñ‚àíùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ‚àíùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ
ùúÉùëñ
.
(For classes where (EC.1) already holds with equality, set Œîùëñ= 0 and Àúùë•ùëñ= ùë•‚òÖ
ùëñ, Àúùëûùëù,ùëñ= ùëû‚òÖ
ùëù,ùëñ.)
By construction,
ùúáùëù,ùëñÀúùë•ùëñ= ùúáùëù,ùëñ
 ùë•‚òÖ
ùëñ‚àíŒîùëñ
 = ùúáùëù,ùëñùë•‚òÖ
ùëñ‚àí ùúáùëù,ùëñùë•‚òÖ
ùëñ‚àíùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ‚àíùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ

= ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ+ ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ.


--- Page 39 ---
ec4
Hence the inequality(EC.8) becomes tight:
ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ+ ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ= ùúáùëù,ùëñÀúùë•ùëñ.
Moreover,
ùúÜùëñ‚àíùúÉùëñÀúùëûùëù,ùëñ‚àíùúáùëù,ùëñÀúùë•ùëñ= ùúÜùëñ‚àíùúÉùëñ

ùëû‚òÖ
ùëù,ùëñ+
ùúáùëù,ùëñùë•‚òÖ
ùëñ‚àíùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ‚àíùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ
ùúÉùëñ

‚àíùúáùëù,ùëñ
 ùë•‚òÖ
ùëñ‚àíŒîùëñ

= (ùúÜùëñ‚àíùúÉùëñùëû‚òÖ
ùëù,ùëñ‚àíùúáùëù,ùëñùë•‚òÖ
ùëñ) + ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ+ ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ‚àíùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ‚àíùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ
= 0,
so the first constraint remains satisfied. Nonnegativity holds because ùë•‚òÖ
ùëñ‚â•Œîùëñis equivalent to ùúáùëù,ùëñùë•‚òÖ
ùëñ‚â•
ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ+ ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ, which already holds.
At this stage we have decreased √ç
ùëñùë•ùëñto √ç
ùëñÀúùë•ùëñ= √ç
ùëñùë•‚òÖ
ùëñ‚àí√ç
ùëñŒîùëñwhile keeping (ùë¶ùëö,ùëñ, ùë¶ùë†,ùëñ) unchanged. Thus:
‚Ä¢ The constraint √ç
ùëñùë•ùëñ‚â§1 remains feasible;
‚Ä¢ The solo-decode capacity √ç
ùëñùë¶ùë†,ùëñ‚â§ùêµ 1 ‚àí√ç
ùëñùë•ùëñ
 becomes less restrictive, since the right-hand side
increases when √ç
ùëñùë•ùëñdecreases;
‚Ä¢ The mixed-decode capacity
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶ùëö,ùëñ‚â§(ùêµ‚àí1)
ùêº‚àëÔ∏Å
ùëñ=1
ùë•ùëñ
may become more restrictive, because the right-hand side decreases as √ç
ùëñùë•ùëñdecreases.
If the mixed-capacity constraint is still satisfied with the new Àúùë•ùëñ, we are done with this step. Otherwise, let
Œî :=
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶‚òÖ
ùëö,ùëñ‚àí(ùêµ‚àí1)
ùêº‚àëÔ∏Å
ùëñ=1
Àúùë•ùëñ> 0
denote the gap of mixed-mode occupancy between the new capacity.
We now show that, thanks to the assumption ùõæùúè‚â•(ùêµ‚àí1)/ùêµ, we can reduce the total mixed occupancy by
Œî while increasing solo occupancy accordingly, without violating capacity and without changing decode
completion rates.
For each ùëñ, define perturbations (ùõøùë¶ùëö,ùëñ, ùõøùë¶ùë†,ùëñ) such that
ùêº‚àëÔ∏Å
ùëñ=1
ùõøùë¶ùëö,ùëñ= ‚àíŒî,
ùõøùë¶ùëö,ùëñ‚â§0, ‚àÄùëñ.
We choose ùõøùë¶ùë†,ùëñto preserve per-class decode completion rates:
ùúáùëö,ùëñùõøùë¶ùëö,ùëñ+ ùúáùë†,ùëñùõøùë¶ùë†,ùëñ= 0,
‚àÄùëñ.
(EC.9)
Equation (EC.9) implies
ùõøùë¶ùë†,ùëñ= ‚àíùúáùëö,ùëñ
ùúáùë†,ùëñ
ùõøùë¶ùëö,ùëñ.


--- Page 40 ---
ec5
From the speed abstraction, we have
ùúáùëö,ùëñ=
1
ùê∑ùëñùúè,
ùúáùë†,ùëñ= ùõæ
ùê∑ùëñ
,
‚áí
ùúáùëö,ùëñ
ùúáùë†,ùëñ
= 1
ùõæùúè,
which is independent of ùëñ. Hence
ùêº‚àëÔ∏Å
ùëñ=1
ùõøùë¶ùë†,ùëñ= ‚àí1
ùõæùúè
ùêº‚àëÔ∏Å
ùëñ=1
ùõøùë¶ùëö,ùëñ= Œî
ùõæùúè.
(EC.10)
Thus we have decreased total mixed occupancy by Œî and increased total solo occupancy by Œî/(ùõæùúè).
Mixed-decode capacity. By construction, the new mixed occupancy is
ùêº‚àëÔ∏Å
ùëñ=1
 ùë¶‚òÖ
ùëö,ùëñ+ ùõøùë¶ùëö,ùëñ
 =
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶‚òÖ
ùëö,ùëñ‚àíŒî = (ùêµ‚àí1)
ùêº‚àëÔ∏Å
ùëñ=1
Àúùë•ùëñ,
so the mixed capacity is now exactly tight and hence feasible.
Solo-decode capacity. Let ùëã‚òÖ:= √çùêº
ùëñ=1 ùë•‚òÖ
ùëñand Àúùëã:= √çùêº
ùëñ=1 Àúùë•ùëñ. Then
Àúùëã= ùëã‚òÖ‚àí
ùêº‚àëÔ∏Å
ùëñ=1
Œîùëñ.
Originally, the solo capacity constraint was
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶‚òÖ
ùë†,ùëñ‚â§ùêµ(1 ‚àíùëã‚òÖ).
Conducting the steps above, with ùë¶ùë†,ùëñunchanged and ùë•ùëñreplaced by Àúùë•ùëñ, we had
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶‚òÖ
ùë†,ùëñ‚â§ùêµ(1 ‚àíùëã‚òÖ) ‚â§ùêµ(1 ‚àíÀúùëã),
so solo capacity was slack. After applying (ùõøùë¶ùëö,ùëñ, ùõøùë¶ùë†,ùëñ), the new solo occupancy becomes, using (EC.10),
ùêº‚àëÔ∏Å
ùëñ=1
 ùë¶‚òÖ
ùë†,ùëñ+ ùõøùë¶ùë†,ùëñ
 =
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶‚òÖ
ùë†,ùëñ+ Œî
ùõæùúè.
We want to ensure
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶‚òÖ
ùë†,ùëñ+ Œî
ùõæùúè‚â§ùêµ(1 ‚àíÀúùëã).
(EC.11)
Observe that
Œî =
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶‚òÖ
ùëö,ùëñ‚àí(ùêµ‚àí1)
ùêº‚àëÔ∏Å
ùëñ=1
Àúùë•ùëñ
=
 ùêº‚àëÔ∏Å
ùëñ=1
ùë¶‚òÖ
ùëö,ùëñ‚àí(ùêµ‚àí1)
ùêº‚àëÔ∏Å
ùëñ=1
ùë•‚òÖ
ùëñ

+ (ùêµ‚àí1)
ùêº‚àëÔ∏Å
ùëñ=1
(ùë•‚òÖ
ùëñ‚àíÀúùë•ùëñ).


--- Page 41 ---
ec6
By feasibility of the original solution, √ç
ùëñùë¶‚òÖ
ùëö,ùëñ‚â§(ùêµ‚àí1) √ç
ùëñùë•‚òÖ
ùëñ, hence
Œî ‚â§(ùêµ‚àí1)
ùêº‚àëÔ∏Å
ùëñ=1
(ùë•‚òÖ
ùëñ‚àíÀúùë•ùëñ) = (ùêµ‚àí1)
ùêº‚àëÔ∏Å
ùëñ=1
Œîùëñ.
Therefore,
Œî
ùõæùúè‚â§ùêµ‚àí1
ùõæùúè
ùêº‚àëÔ∏Å
ùëñ=1
Œîùëñ.
Using the assumption ùõæùúè‚â•(ùêµ‚àí1)/ùêµ, we obtain
ùêµ‚àí1
ùõæùúè
‚â§ùêµ,
‚áí
Œî
ùõæùúè‚â§ùêµ
ùêº‚àëÔ∏Å
ùëñ=1
Œîùëñ.
Finally, note that
ùêµ(1 ‚àíÀúùëã) = ùêµ(1 ‚àíùëã‚òÖ) + ùêµ
ùêº‚àëÔ∏Å
ùëñ=1
Œîùëñ.
Combining these inequalities,
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶‚òÖ
ùë†,ùëñ+ Œî
ùõæùúè‚â§
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶‚òÖ
ùë†,ùëñ+ ùêµ
ùêº‚àëÔ∏Å
ùëñ=1
Œîùëñ
‚â§ùêµ(1 ‚àíùëã‚òÖ) + ùêµ
ùêº‚àëÔ∏Å
ùëñ=1
Œîùëñ
= ùêµ(1 ‚àíÀúùëã),
which is exactly (EC.11). Thus the solo-decode capacity constraint remains feasible after the reallocation.
By (EC.9), for each ùëñ,
ùúáùëö,ùëñ
 ùë¶‚òÖ
ùëö,ùëñ+ ùõøùë¶ùëö,ùëñ
 + ùúáùë†,ùëñ
 ùë¶‚òÖ
ùë†,ùëñ+ ùõøùë¶ùë†,ùëñ
 = ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ+ ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ.
Hence all class-level decode completion rates are unchanged, and the objective
ùêº‚àëÔ∏Å
ùëñ=1
ùë§ùëñ
 ùúáùëö,ùëñùë¶ùëö,ùëñ+ ùúáùë†,ùëñùë¶ùë†,ùëñ

remains the same. Consequently, we have constructed a new feasible solution with the same objective value,
but with mixed occupancy reduced to exactly match the capacity available under the updated Àúùë•ùëñ.
Moreover, for any class ùëñwith initial slack in (EC.1), we now have
ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ+ ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ= ùúáùëù,ùëñÀúùë•ùëñ,
i.e., its decode inequality is tight.
Applying the above procedure to each class ùëñwhose decode inequality (EC.1) is slack at an optimal
solution, we obtain another optimal solution of (EC.3) such that
ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ+ ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ= ùúáùëù,ùëñùë•‚òÖ
ùëñ,
‚àÄùëñ.


--- Page 42 ---
ec7
Returning to the original formulation (40), this is equivalent (via (EC.2)) to
ùëû‚òÖ
ùëë,ùëñ= 0,
‚àÄùëñ.
Thus the steady-state fluid LP admits an optimal solution in which the decode buffer is empty in the fluid
limit, completing the proof.
‚ñ°
EC.2.
Proofs of Theorem 1
Proof
Fix ùëá> 0. We denote the scaled processes as ¬Øùëäùëõ:= ùëäùëõ/ùëõ. From the model above, we have:
ùê¥ùëõ
ùëñ(ùë°) = ùëÅùëé,ùëñ(ùúÜùëñùëõùë°),
ùêµùëõ
ùëù,ùëñ(ùë°) = ùëÅùëèùëù,ùëñ
‚à´ùë°
0
ùúÉùëñùëÑùëõ
ùëù,ùëñ(ùë†) ùëëùë†

,
ùêµùëõ
ùëë,ùëñ(ùë°) = ùëÅùëèùëë,ùëñ
‚à´ùë°
0
ùúÉùëñùëÑùëõ
ùëë,ùëñ(ùë†) ùëëùë†

,
ùê∑ùëõ
ùëù,ùëñ(ùë°) = ùëÅùëëùëù,ùëñ
‚à´ùë°
0
ùúáùëù,ùëñùëãùëõ
ùëñ(ùë†) ùëëùë†

,
ùê∑ùëõ
ùëë,ùëö,ùëñ(ùë°) = ùëÅùëëùëö,ùëñ
‚à´ùë°
0
ùúáùëö,ùëñùëåùëõ
ùëö,ùëñ(ùë†) ùëëùë†

,
ùê∑ùëõ
ùëë,ùë†,ùëñ(ùë°) = ùëÅùëëùë†,ùëñ
‚à´ùë°
0
ùúáùë†,ùëñùëåùëõ
ùë†,ùëñ(ùë†) ùëëùë†

First we prove the tightness. The GPU capacity implies that 0 ‚â§ùëãùëõ‚â§ùëõ, 0 ‚â§ùëåùëõ
ùëö‚â§(ùêµ‚àí1)ùëãùëõ‚â§(ùêµ‚àí1)ùëõ,
0 ‚â§ùëåùëõ
ùë†‚â§ùêµ(ùëõ‚àíùëãùëõ) ‚â§ùêµùëõ, so the time-changes for ùê∑ùëõ
ùëù,ùëñ, ùê∑ùëõ
ùëë,ùëö,ùëñ, ùê∑ùëõ
ùëë,ùë†,ùëñare bounded. Since ùëÑùëõ
ùëù,ùëñ(ùë†) ‚â§
ùëÑùëõ
ùëù,ùëñ(0) + ùê¥ùëõ
ùëñ(ùë†) and ùëÑùëõ
ùëë,ùëñ(ùë†) ‚â§ùëÑùëõ
ùëë,ùëñ(0) + ùê∑ùëõ
ùëù,ùëñ(ùë†), we have:
1
ùëõ
‚à´ùë°
0
ùúÉùëù,ùëñùëÑùëõ
ùëù,ùëñ(ùë†) ùëëùë†‚â§ùúÉùëù,ùëñ

ùë°¬ØùëÑùëõ
ùëù,ùëñ(0) + ùë°¬Øùê¥ùëõ
ùëñ(ùë°)

,
1
ùëõ
‚à´ùë°
0
ùúÉùëë,ùëñùëÑùëõ
ùëë,ùëñ(ùë†) ùëëùë†‚â§ùúÉùëë,ùëñ

ùë°¬ØùëÑùëõ
ùëë,ùëñ(0) + ùë°¬Øùê∑ùëõ
ùëù,ùëñ(ùë°)

,
hence these time-changes are stochastically bounded on [0,ùëá]. Controls are nondecreasing and satisfy
ùëàùëõ
ùëù,ùëñ(ùë°) ‚â§ùëÑùëõ
ùëù,ùëñ(0) + ùê¥ùëõ
ùëñ(ùë°),
ùëàùëõ
ùëë,ùëö,ùëñ(ùë°) +ùëàùëõ
ùëë,ùë†,ùëñ(ùë°) ‚â§ùëÑùëõ
ùëë,ùëñ(0) + ùê∑ùëõ
ùëù,ùëñ(ùë°),
and structural transfers obey ùëÄùëõ
ùëö‚Üíùë†‚â§ùêµùê∑ùëõ
ùëù, ùëÄùëõ
ùë†‚Üíùëö‚â§ùêµùëàùëõ
ùëù. By the FSLLN for Poisson processes, the family
{ ¬Øùê¥ùëõ
ùëñ}ùëõis tight.
Then by the random time-change theorem, together with the bounds above, we have the tightness of all
other time-changed Poisson coordinates. Also, by Billingsley‚Äôs criterion for nondecreasing processes, the full
vector ¬ØXùëõis tight in D([0,ùëá], Rùëë) under the ùêΩ1-topology.
Second, we focus on the subsequence convergence and continuity. By tightness, we may pick a subsequence
¬ØXùëõ‚áí¬ØX. By Skorokhod representation theorem, we may assume a.s. convergence u.o.c. on [0,ùëá]. Since all
jumps are 1
ùëõ, the limit ¬ØX is a.s. continuous.
Finally, we prove that the stochastic processes satisfy the fluid model constraints. The FSLLN gives
¬Øùê¥ùëõ
ùëñ‚Üíùëéùëñ(ùë°) = ùúÜùëñùë°u.o.c. For the time-changed Poisson processes, if ùêøùëõ(ùë°) := 1
ùëõ
‚à´ùë°
0 ‚Ñìùëõ(ùë†) ùëëùë†‚Üíùêø(ùë°) u.o.c., then
ùëÅ(ùêøùëõ(¬∑)ùëõ)/ùëõ‚Üíùêø(¬∑) u.o.c. Hence
¬Øùêµùëõ
ùëù,ùëñ‚áíùëèùëù,ùëñ(ùë°) =
‚à´ùë°
0
ùúÉùëù,ùëñùëûùëù,ùëñ(ùë†) ùëëùë†,
¬Øùêµùëõ
ùëë,ùëñ‚áíùëèùëë,ùëñ(ùë°) =
‚à´ùë°
0
ùúÉùëë,ùëñùëûùëë,ùëñ(ùë†) ùëëùë†,


--- Page 43 ---
ec8
¬Øùê∑ùëõ
ùëù,ùëñ‚áíùëëùëù,ùëñ(ùë°) =
‚à´ùë°
0
ùúáùëù,ùëñùë•ùëñ(ùë†) ùëëùë†,
¬Øùê∑ùëõ
ùëë,ùëö,ùëñ‚áíùëëùëë,ùëö,ùëñ(ùë°) =
‚à´ùë°
0
ùúáùëö,ùëñùë¶ùëö,ùëñ(ùë†) ùëëùë†,
¬Øùê∑ùëõ
ùëë,ùë†,ùëñ‚áíùëëùëë,ùë†,ùëñ(ùë°) =
‚à´ùë°
0
ùúáùë†,ùëñùë¶ùë†,ùëñ(ùë†) ùëëùë†.
Taking limits in the flow balances yields (24)‚Äì(28) with (29)‚Äì(32).
The remaining inequalities are easy to check since the prelimit constraints satisfy:
0 ‚â§¬Øùëãùëõ‚â§1,
0 ‚â§¬Øùëåùëõ
ùëö‚â§(ùêµ‚àí1) ¬Øùëãùëõ,
0 ‚â§¬Øùëåùëõ
ùë†‚â§ùêµ(1 ‚àí¬Øùëãùëõ),
and the inequalities are preserved under the limit. Hence (33)‚Äì(35) hold.
‚ñ°
EC.3.
Proof of Optimality of Gate and Route Policy
EC.3.1.
Proof of prefill convergence in Theorem 2
We focus on the LP solutions where ùëû‚àó
ùëù,ùëñ> 0. The proof for ùëû‚àó
ùëù,ùëñ= 0 is straightforward.
LEMMA EC.1. There exists time ùë°0, such that for ùë°‚â•ùë°0, we have: ùë•ùëñ(ùë°) ‚â§ùë•‚àó
ùëñfor all i.
Proof
Suppose not, then for any ùë°0, there exists ùë°1, such that ùë•ùëñ(ùë°1) > ùë•‚àó
ùëñ. Take Àúùë°= sup{Àúùë°< ùë°0 : ùë•ùëñ(Àúùë°) ‚â§ùë•‚àó
ùëñ}.
Then it is clear that ùë•ùëñ(Àúùë°) = ùë•‚àó
ùëñand ùë•ùëñ(ùë°) > ùë•‚àó
ùëñfor ùë°‚àà(Àúùë°,ùë°1].
But by our policy, if ùë•ùëñ(ùë°) > ùë•‚àó
ùëñ, we must have ùë•‚Ä≤
ùëñ(ùë°) < 0. This means that ùë•ùëñ(ùë°1) = ùë•ùëñ(Àúùë°) +
‚à´ùë°1
Àúùë°
ùëëùë•ùëñ(ùë°)ùëëùë°<
ùë•ùëñ(Àúùë°) = ùë•‚àó
ùëñ, contradiction.
‚ñ°
LEMMA EC.2. In the fluid system, there exists time ùë°ùëñ, such that ùëûùëù,ùëñ(ùë°ùëñ) > 0. And ùëûùëù,ùëñ(ùë°) > 0 for all ùë°‚â•ùë°ùëñ
and ùëñ‚ààùêº.
Proof
Suppose not, then for some ùëñ, we have ùëûùëù,ùëñ(ùë°) = 0 for all ùë°. In which case, ùë•ùëñ(ùë°) ‚â§ùë•‚àó
ùëñ. By the balance
equation, we have:
ùúÜùëñ= ùúÉùëñùëûùëù,ùëñ(ùë°) + ùúáùëù,ùëñùë•ùëñ(ùë°)
= ùúáùëù,ùëñùë•ùëñ(ùë°)
‚â§ùúáùëù,ùëñùë•‚àó
ùëñ
However, by the structure of the LP solution, ùúÜùëñ= ùúÉùëñùëû‚àó
ùëù,ùëñ+ ùúáùëù,ùëñùë•‚àó
ùëñ, contradiction.
If there exists some index ùëñand Àúùë°ùëñsuch that ùëûùëù,ùëñ(Àúùë°ùëñ) = 0, we can take a small enough interval (Àúùë°‚àíùúñ, Àúùë°) such
that ùëûùëù,ùëñ(ùë°) is decreasing in the interval and ùëûùëù,ùëñ(ùë°) < ùëû‚àó
ùëù,ùëñ. Since in the interval ùëûùëù,ùëñ(ùë°) > 0, ùë•ùëñ(ùë°) = ùë•‚àó
ùëñ. Then
by the balance equations, we have:
¬§ùëûùëù,ùëñ(ùë°) = ùúÉùëñ[ùëû‚àó
ùëù,ùëñ‚àíùëûùëù,ùëñ(ùë°)] > 0
contradiction! Then ùëûùëù,ùëñ(ùë°) > 0 for all ùë°‚â•ùë°ùëñand ùëñ‚ààùêº.
‚ñ°
LEMMA EC.3. Under our policy, ùëûùëù,ùëñ(ùë°) ‚Üíùëû‚àó
ùëù,ùëñand ùë•ùëñ(ùë°) ‚Üíùë•‚àó
ùëñas ùë°‚Üí‚àû.


--- Page 44 ---
ec9
Proof
By the above lemma, we can assume that ùëûùëù,ùëñ(ùë°) > 0. ùë•ùëñ(ùë°) = ùë•‚àó
ùëñas a result. Then the convergence
of ùë•ùëñ(ùë°) follows. By the balance equation,
¬§ùëûùëù,ùëñ(ùë°) = ùúÜùëñ‚àíùúáùëù,ùëñùë•‚àó
ùëñ‚àíùúÉùëñùëûùëù,ùëñ(ùë°)
= ùúÉùëñ(ùëû‚àó
ùëù,ùëñ‚àíùëûùëù,ùëñ(ùë°))
Solving the above ODE, the convergence of ùëûùëù,ùëñ(ùë°) follows in standard.
‚ñ°
EC.3.2.
Proof of decode convergence in Theorem 2
We now show that, under these conditions, the aggregate decode buffer ùëûùëë(ùë°) := √çùêº
ùëñ=1 ùëûùëë,ùëñ(ùë°) converges to
zero along any fluid trajectory of the gate-and-route policy.
LEMMA EC.4. Consider the fluid model under the gate-and-route policy with the static GPU partition
induced by the LP solution: a fraction ùë•‚òÖ:= √çùêº
ùëñ=1 ùë•‚òÖ
ùëñof GPUs are mixed and a fraction 1 ‚àíùë•‚òÖare solo. Let
ùë¶ùëö(ùë°) :=
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶ùëö,ùëñ(ùë°),
ùë¶ùë†(ùë°) :=
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶ùë†,ùëñ(ùë°).
If at some regular time ùë°we have ùëûùëë(ùë°) > 0, then decode capacity is fully utilized:
ùë¶ùëö(ùë°) = ùë•‚òÖ(ùêµ‚àí1),
ùë¶ùë†(ùë°) = (1 ‚àíùë•‚òÖ)ùêµ.
Proof
By construction of the static planning step, exactly a fraction ùë•‚òÖof the GPUs are permanently
assigned to the mixed group, and the remaining fraction 1 ‚àíùë•‚òÖto the solo group. Each mixed GPU holds at
most (ùêµ‚àí1) decodes, while each solo GPU holds at most ùêµdecodes. After fluid scaling by ùëõ, the maximal
aggregate decode occupancies are
ùë¶ùëö(ùë°) ‚â§ùë•‚òÖ(ùêµ‚àí1),
ùë¶ùë†(ùë°) ‚â§(1 ‚àíùë•‚òÖ)ùêµ.
The gate-and-route decode router is work-conserving: as long as the global decode buffer is nonempty (i.e.,
ùëûùëë(ùë°) > 0), any free decode slot in either group is immediately filled with a waiting job. Hence no decode slot
can be idle whenever ùëûùëë(ùë°) > 0. Therefore,
ùë¶ùëö(ùë°) = ùë•‚òÖ(ùêµ‚àí1),
ùë¶ùë†(ùë°) = (1 ‚àíùë•‚òÖ)ùêµ,
whenever ùëûùëë(ùë°) > 0.
‚ñ°
We next introduce a Lyapunov function that measures the total remaining decode work in ‚Äúmixed-mode
time units.‚Äù
DEFINITION EC.1 (WEIGHTED DECODE WORK). Define
ùëäùëë(ùë°) :=
ùêº‚àëÔ∏Å
ùëñ=1
ùëûùëë,ùëñ(ùë°) + ùë¶ùëö,ùëñ(ùë°) + ùë¶ùë†,ùëñ(ùë°)
ùúáùëö,ùëñ
.


--- Page 45 ---
ec10
Here 1/ùúáùëö,ùëñis the mean decode service time of a type-ùëñjob in mixed mode. Thus ùëäùëë(ùë°) is the total
remaining decode work expressed in mixed-mode time units.
We assume the mode speed ratio
ùúáùë†,ùëñ= ùõæùúèùúáùëö,ùëñ=: ùúÖùúáùëö,ùëñ
for all ùëñ‚àà{1, . . . , ùêº},
with ùúÖ= ùõæùúè> 0 constant across classes.
LEMMA EC.5. Along any fluid trajectory, for ùë°‚â•0,
¬§ùëäùëë(ùë°) =
ùêº‚àëÔ∏Å
ùëñ=1
ùúáùëù,ùëñ
ùúáùëö,ùëñ
ùë•ùëñ(ùë°) ‚àí ùë¶ùëö(ùë°) + ùúÖùë¶ùë†(ùë°) ‚àí
ùêº‚àëÔ∏Å
ùëñ=1
ùúÉùëñ
ùúáùëö,ùëñ
ùëûùëë,ùëñ(ùë°).
(EC.12)
Proof
We use the fluid balance equations and the random time-change representation for arrivals,
completions, and abandonments; all fluid coordinates are absolutely continuous, so their derivatives exist.
Arrivals into decode. For class ùëñ, prefills complete at rate ùúáùëù,ùëñùë•ùëñ(ùë°). Each completion creates one decode
job of that class, with mixed-mode work 1/ùúáùëö,ùëñ. Hence the instantaneous inflow into ùëäùëëfrom class ùëñis
ùúáùëù,ùëñùë•ùëñ(ùë°) ¬∑
1
ùúáùëö,ùëñ
,
and summing over ùëñyields the first term on the right-hand side of (EC.12).
Service in mixed mode. For class ùëñ, mixed decodes are in service at rate ùúáùëö,ùëñùë¶ùëö,ùëñ(ùë°), each completion
removing 1/ùúáùëö,ùëñunits of ùëäùëë. Thus the mixed-mode drain is
ùêº‚àëÔ∏Å
ùëñ=1
ùúáùëö,ùëñùë¶ùëö,ùëñ(ùë°) ¬∑
1
ùúáùëö,ùëñ
=
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶ùëö,ùëñ(ùë°) = ùë¶ùëö(ùë°).
Service in solo mode. Using ùúáùë†,ùëñ= ùúÖùúáùëö,ùëñ, solo-mode completions for class ùëñoccur at rate ùúáùë†,ùëñùë¶ùë†,ùëñ(ùë°), each
also removing 1/ùúáùëö,ùëñunits of work. Thus
ùêº‚àëÔ∏Å
ùëñ=1
ùúáùë†,ùëñùë¶ùë†,ùëñ(ùë°) ¬∑
1
ùúáùëö,ùëñ
=
ùêº‚àëÔ∏Å
ùëñ=1
ùúÖùúáùëö,ùëñùë¶ùë†,ùëñ(ùë°) ¬∑
1
ùúáùëö,ùëñ
= ùúÖ
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶ùë†,ùëñ(ùë°) = ùúÖùë¶ùë†(ùë°),
which gives the second term in (EC.12).
Abandonments. Class-ùëñdecode abandonments occur at rate ùúÉùëñùëûùëë,ùëñ(ùë°). Each abandonment removes 1/ùúáùëö,ùëñ
units of ùëäùëë. Thus the total drain from abandonments is
ùêº‚àëÔ∏Å
ùëñ=1
ùúÉùëñùëûùëë,ùëñ(ùë°) ¬∑
1
ùúáùëö,ùëñ
,
which yields the third term in (EC.12) with a minus sign.
Combining these three contributions establishes (EC.12).
‚ñ°


--- Page 46 ---
ec11
LEMMA EC.6. Assume that under the gate-and-route policy, the prefill occupancies satisfy ùë•ùëñ(ùë°) ‚Üíùë•‚òÖ
ùëñas
ùë°‚Üí‚àûfor each ùëñ, and let ùë•‚òÖ:= √çùêº
ùëñ=1 ùë•‚òÖ
ùëñ. Define the per-GPU decode capacity (in mixed-mode time units)
ùê∂‚òÖ:= ùúÖ(1 ‚àíùë•‚òÖ)ùêµ+ ùë•‚òÖ(ùêµ‚àí1).
Then there exists ùëá0 < ‚àûsuch that for all ùë°‚â•ùëá0,
ùëûùëë(ùë°) > 0 =‚áí¬§ùëäùëë(ùë°) ‚â§‚àí
ùêº‚àëÔ∏Å
ùëñ=1
ùúÉùëñ
ùúáùëö,ùëñ
ùëûùëë,ùëñ(ùë°).
Proof
Since ùë•ùëñ(ùë°) ‚Üíùë•‚òÖ
ùëñ, we have
ùê¥(ùë°) :=
ùêº‚àëÔ∏Å
ùëñ=1
ùúáùëù,ùëñ
ùúáùëö,ùëñ
ùë•ùëñ(ùë°) ‚àí‚Üíùê¥‚òÖ:=
ùêº‚àëÔ∏Å
ùëñ=1
ùúáùëù,ùëñ
ùúáùëö,ùëñ
ùë•‚òÖ
ùëñ.
We claim that ùê¥‚òÖ‚â§ùê∂‚òÖ. Indeed, by Proposition 1 we can choose an LP-optimal solution with ùëû‚òÖ
ùëë,ùëñ= 0 for all ùëñ.
Summing the LP decode flow-balance constraints and using ùúáùë†,ùëñ= ùúÖùúáùëö,ùëñgives
ùê¥‚òÖ=
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶‚òÖ
ùëö,ùëñ+ ùúÖ
ùêº‚àëÔ∏Å
ùëñ=1
ùë¶‚òÖ
ùë†,ùëñ‚â§ùë•‚òÖ(ùêµ‚àí1) + ùúÖ(1 ‚àíùë•‚òÖ)ùêµ= ùê∂‚òÖ,
where the inequality is exactly the LP mixed/solo decode capacity constraints.
By Lemma EC.1, there exists ùë°0 such that ùë•ùëñ(ùë°) ‚â§ùë•‚òÖ
ùëñfor all ùëñand all ùë°‚â•ùë°0. Hence
ùê¥(ùë°) ‚â§ùê¥‚òÖ‚â§ùê∂‚òÖ,
ùë°‚â•ùë°0.
By Lemma EC.4, if ùëûùëë(ùë°) > 0 then decode capacity is fully utilized and ùë¶ùëö(ùë°) = ùë•‚òÖ(ùêµ‚àí1), ùë¶ùë†(ùë°) =
(1 ‚àíùë•‚òÖ)ùêµ. Substituting into (EC.12) gives, for such ùë°,
¬§ùëäùëë(ùë°) = ùê¥(ùë°) ‚àí
h
ùë•‚òÖ(ùêµ‚àí1) + ùúÖ(1 ‚àíùë•‚òÖ)ùêµ
i
‚àí
ùêº‚àëÔ∏Å
ùëñ=1
ùúÉùëñ
ùúáùëö,ùëñ
ùëûùëë,ùëñ(ùë°).
Then by using the inequality ùê¥(ùë°) ‚â§ùê¥‚òÖ‚â§ùê∂‚òÖ, we have:
¬§ùëäùëë(ùë°) ‚â§‚àí
ùêº‚àëÔ∏Å
ùëñ=1
ùúÉùëñ
ùúáùëö,ùëñ
ùëûùëë,ùëñ(ùë°),
as claimed.
‚ñ°
We are now ready to show that the decode buffer vanishes in the fluid limit.
PROPOSITION EC.1. Assume ùúÉùëñ> 0 for all ùëñ, and that under the gate-and-route policy the prefill
occupancies satisfy ùë•ùëñ(ùë°) ‚Üíùë•‚òÖ
ùëñas ùë°‚Üí‚àû, with the associated capacity constant ùê∂‚òÖ:= ùúÖ(1‚àíùë•‚òÖ)ùêµ+ùë•‚òÖ(ùêµ‚àí1).
Then along any fluid trajectory,
lim
ùë°‚Üí‚àûùëûùëë(ùë°) = 0.


--- Page 47 ---
ec12
Proof
Define
ùúÉ:= min
1‚â§ùëñ‚â§ùêº
ùúÉùëñ
ùúáùëö,ùëñ
> 0,
using the assumption ùúÉùëñ> 0 for all ùëñ. Then
ùêº‚àëÔ∏Å
ùëñ=1
ùúÉùëñ
ùúáùëö,ùëñ
ùëûùëë,ùëñ(ùë°) ‚â•ùúÉ
ùêº‚àëÔ∏Å
ùëñ=1
ùëûùëë,ùëñ(ùë°) = ùúÉùëûùëë(ùë°).
Then by Lemma EC.6, we have:
¬§ùëäùëë(ùë°) ‚â§‚àíùúÉùëûùëë(ùë°).
(EC.13)
which means that ùëäùëë(ùë°) is nonincreasing whenever ùëûùëë(ùë°) ‚â•0.
We now argue by contradiction. Suppose ùëûùëë(ùë°) does not converge to zero. Then there exists ùõø> 0 and a
sequence ùë°ùëò‚Üí‚àûsuch that ùëûùëë(ùë°ùëò) ‚â•ùõøfor all ùëò. Because of the Lipschitz property of the fluid processes,
there exists ùúÇ> 0 and subintervals [ùë°‚Ä≤
ùëò,ùë°‚Ä≤‚Ä≤
ùëò] with
ùë°‚Ä≤
ùëò‚â§ùë°ùëò‚â§ùë°‚Ä≤‚Ä≤
ùëò,
ùë°‚Ä≤‚Ä≤
ùëò‚àíùë°‚Ä≤
ùëò‚â•ùúÇ,
Then for all ùë°‚àà[ùë°‚Ä≤
ùëò,ùë°‚Ä≤‚Ä≤
ùëò], (EC.13) gives ¬§ùëäùëë(ùë°) ‚â§‚àíùúÉùõø/2 =: ‚àíùëê< 0. Take the integral of the inequality, we
have:
ùëäùëë(ùë°‚Ä≤‚Ä≤
ùëò) ‚àíùëäùëë(ùë°‚Ä≤
ùëò) ‚â§‚àíùëê(ùë°‚Ä≤‚Ä≤
ùëò‚àíùë°‚Ä≤
ùëò) ‚â§‚àíùëêùúÇ< 0,
which means that in each interval determined by the sequence ùë°ùëò, the decrease of ùëäùëë(ùë°) is at least ùëêùúÇ,
which contradicts the fact that ùëäùëë(ùë°) ‚â•0.
Therefore, our assumption is false, and we must have
lim
ùë°‚Üí‚àûùëûùëë(ùë°) = 0.
‚ñ°
Proof of Theorem 2
Combining Proposition EC.1 with the previously established convergence of the
prefill occupancies ùë•ùëñ(ùë°) ‚Üíùë•‚òÖ
ùëñand the fixed decode capacities in Lemma EC.4, we conclude that the fluid
occupancies converge to the LP-optimal point (ùë•‚òÖ
ùëñ, ùë¶‚òÖ
ùëö,ùëñ, ùë¶‚òÖ
ùë†,ùëñ)ùëñ‚àà[ùêº], and the corresponding per-GPU reward
attains the LP optimum. This is the key ingredient in the proof of Theorem 2 (asymptotic optimality of the
gate-and-route policy).
‚ñ°


--- Page 48 ---
ec13
EC.4.
Proof of the Optimality of the Prioritize-and-Route Policy
EC.4.1.
Proof of Theorem 3
Proof
Let Àúùúãùëõ,‚òÖbe the prioritize-and-route policy defined in Section 4.2, parameterized by an optimal
solution of the steady-state fluid LP (42).
Under separate charging, the steady-state objective (42) by which the reward only depends on aggregate
occupancies (√ç
ùëñùë•ùëñ,√ç
ùëñùë¶ùëö,ùëñ,√ç
ùëñùë¶ùë†,ùëñ). What is left is to prove that the prioritize-and-route policy reaches the
maximal workload of the system.
The prioritize-and-route policy is work-conserving at the prefill side, so the prefill occupancy is always full.
The priority index ùúôùëñ= ùê∑ùëñ/ùëÉùëñis proportional to the decode-work generation rate per unit prefill occupancy,
since
ùúáùëù,ùëñ
ùúáùëö,ùëñ
= ùê∂/(ùúèùëÉùëñ)
1/(ùúèùê∑ùëñ) = ùê∂ùê∑ùëñ
ùëÉùëñ
= ùê∂ùúôùëñ.
Therefore, among all feasible ways to allocate a given amount of prefill occupancy across classes, the static
priority rule maximizes the instantaneous inflow of downstream decode workload. This ensures that the
aggregate decode occupancies are maximized whenever sufficient workload is available; if workload is
insufficient, no policy can keep more decode capacity busy.
Consequently, the limiting fluid reward under the prioritize-and-route policy achieves the optimal value
ÀúùëÖ‚òÖof the steady-state LP (42), which yields the asymptotic optimality of the policy, i.e.
liminf
ùëá‚Üí‚àûliminf
ùëõ‚Üí‚àû
ÀúùëÖùëõ(ùëá; Àúùúãùëõ,‚òÖ) = ÀúùëÖ‚òÖ.
which is exactly Theorem 3.
‚ñ°
EC.5.
Proofs of the Optimality of the SLI-Aware Policy
In this section, we prove the optimality of the SLI-aware policy. Since the SLI-aware policy deals with the
prefill stage the same as the Gate-and-Route policy, we will only prove the optimality of the SLI-aware policy
for the decode stage. The optimality of the prefill stage follows from the optimality of the Gate-and-Route
policy.
EC.5.1.
Convergence of the decode occupancies under the SLI-aware policy
In this subsection, we focus on the convergence of the decode occupancies under the SLI-aware policy.
PROPOSITION EC.2. Fix an optimal solution (ùë•‚òÖ
ùëñ, ùë¶‚òÖ
ùëö,ùëñ, ùë¶‚òÖ
ùë†,ùëñ, ùëû‚òÖ
ùëù,ùëñ, ùëû‚òÖ
ùëë,ùëñ)ùëñ‚ààI of the SLI-aware steady-state
program (49) with ùëû‚òÖ
ùëë,ùëñ= 0 for all ùëñ. Let ùë•‚òÖ:= √çùêº
ùëñ=1 ùë•‚òÖ
ùëñand consider the corresponding static planning of the
system. Denote ùëûùëë,ùëö,ùëñ(ùë°) and ùëûùëë,ùë†,ùëñ(ùë°) for the mixed and solo decode buffers under the virtual buffer split of
Section 5.1 respectively. Then along any fluid trajectory of the SLI-aware policy,
lim
ùë°‚Üí‚àûùë¶ùëö,ùëñ(ùë°) = ùë¶‚òÖ
ùëö,ùëñ,
lim
ùë°‚Üí‚àûùë¶ùë†,ùëñ(ùë°) = ùë¶‚òÖ
ùë†,ùëñ,
lim
ùë°‚Üí‚àûùëûùëë,ùë†,ùëñ(ùë°) = 0,
lim
ùë°‚Üí‚àûùëûùëë,ùëö,ùëñ(ùë°) = 0,
‚àÄùëñ‚ààI.


--- Page 49 ---
ec14
Proof
Denote the total mixed and solo decode capacities as
ùê∂ùëö:= ùë•‚òÖ(ùêµ‚àí1),
ùê∂ùë†:= (1 ‚àíùë•‚òÖ)ùêµ.
First we show that the decode buffer converges to zero as ùë°‚Üí‚àû. Define the weighted decode work
respectively:
ùëäùëö(ùë°) :=
ùêº‚àëÔ∏Å
ùëñ=1
ùëûùëë,ùëö,ùëñ(ùë°) + ùë¶ùëö,ùëñ(ùë°)
ùúáùëö,ùëñ
,
ùëäùë†(ùë°) :=
ùêº‚àëÔ∏Å
ùëñ=1
ùëûùëë,ùë†,ùëñ(ùë°) + ùë¶ùë†,ùëñ(ùë°)
ùúáùë†,ùëñ
.
Take the derivative of the formulas above, we have:
¬§ùëäùëö(ùë°) =
ùêº‚àëÔ∏Å
ùëñ=1
(1 ‚àíùëùùë†,ùëñ) ùúáùëù,ùëñ
ùúáùëö,ùëñ
ùë•ùëñ(ùë°) ‚àíùë¶ùëö(ùë°) ‚àí
ùêº‚àëÔ∏Å
ùëñ=1
ùúÉùëñ
ùúáùëö,ùëñ
ùëûùëë,ùëö,ùëñ(ùë°),
(EC.14)
¬§ùëäùë†(ùë°) =
ùêº‚àëÔ∏Å
ùëñ=1
ùëùùë†,ùëñùúáùëù,ùëñ
ùúáùë†,ùëñ
ùë•ùëñ(ùë°) ‚àíùë¶ùë†(ùë°) ‚àí
ùêº‚àëÔ∏Å
ùëñ=1
ùúÉùëñ
ùúáùë†,ùëñ
ùëûùëë,ùë†,ùëñ(ùë°).
(EC.15)
Here ùë¶ùëö(ùë°) := √ç
ùëñùë¶ùëö,ùëñ(ùë°) and ùë¶ùë†(ùë°) := √ç
ùëñùë¶ùë†,ùëñ(ùë°).
Whenever ùëûùëë,ùëö(ùë°) > 0, the mixed group is work-conserving, so all mixed decode slots are busy and
ùë¶ùëö(ùë°) = ùê∂ùëö; similarly, ùëûùëë,ùë†(ùë°) > 0 implies ùë¶ùë†(ùë°) = ùê∂ùë†. Since ùë•ùëñ(ùë°) ‚Üíùë•‚òÖ
ùëñand the LP feasibility constraints
include √ç
ùëñùë¶‚òÖ
ùëö,ùëñ‚â§ùê∂ùëöand √ç
ùëñùë¶‚òÖ
ùë†,ùëñ‚â§ùê∂ùë†, the constants
ùê¥‚òÖ
ùëö:=
ùêº‚àëÔ∏Å
ùëñ=1
(1 ‚àíùëùùë†,ùëñ) ùúáùëù,ùëñ
ùúáùëö,ùëñ
ùë•‚òÖ
ùëñ,
ùê¥‚òÖ
ùë†:=
ùêº‚àëÔ∏Å
ùëñ=1
ùëùùë†,ùëñùúáùëù,ùëñ
ùúáùë†,ùëñ
ùë•‚òÖ
ùëñ
satisfy ùê¥‚òÖ
ùëö‚â§ùê∂ùëöand ùê¥‚òÖ
ùë†‚â§ùê∂ùë†, since they are both the admission limit of the policy which should be less than
the total decode capacity. Therefore,
ùëûùëë,ùëö(ùë°) > 0 =‚áí¬§ùëäùëö(ùë°) ‚â§‚àíùúÉùëöùëûùëë,ùëö(ùë°),
ùëûùëë,ùë†(ùë°) > 0 =‚áí¬§ùëäùë†(ùë°) ‚â§‚àíùúÉùë†ùëûùëë,ùë†(ùë°),
where ùúÉùëö:= minùëñùúÉùëñ/ùúáùëö,ùëñ> 0 and ùúÉùë†:= minùëñùúÉùëñ/ùúáùë†,ùëñ> 0. The same contradiction argument as in Proposi-
tion EC.1 implies
lim
ùë°‚Üí‚àûùëûùëë,ùëö(ùë°) = 0,
lim
ùë°‚Üí‚àûùëûùëë,ùë†(ùë°) = 0.
Now we are ready to show the convergence of the decode occupancies respectively. By our policy, we can
compute the instant admission rates of the mixed and solo decode pools respectively:
¬§ùë¢ùëë,ùëö,ùëñ(ùë°) = (1 ‚àíùëùùë†,ùëñ) (ùúáùëù,ùëñùë•ùëñ(ùë°) ‚àíùúÉùëñùëûùëë,ùë†,ùëñ(ùë°)),
¬§ùë¢ùëë,ùë†,ùëñ(ùë°) = ùëùùë†,ùëñ(ùúáùëù,ùëñùë•ùëñ(ùë°) ‚àíùúÉùëñùëûùëë,ùëö,ùëñ(ùë°))
Take the derivative of the decode occupancies of the balance equations, we have:
¬§ùë¶ùëö,ùëñ(ùë°) = (1 ‚àíùëùùë†,ùëñ) ùúáùëù,ùëñùë•ùëñ(ùë°) ‚àíùúáùëö,ùëñùë¶ùëö,ùëñ(ùë°),
¬§ùë¶ùë†,ùëñ(ùë°) = ùëùùë†,ùëñùúáùëù,ùëñùë•ùëñ(ùë°) ‚àíùúáùë†,ùëñùë¶ùë†,ùëñ(ùë°),
Since ùë•ùëñ(ùë°) ‚Üíùë•‚òÖ
ùëñand ùëûùëë,ùëö,ùëñ(ùë°) ‚Üí0 and ùëûùëë,ùë†,ùëñ(ùë°) ‚Üí0 and the LP feasibility condition:
ùúáùëù,ùëñùë•‚òÖ
ùëñ= ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ+ ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ


--- Page 50 ---
ec15
then for any ùúñ> 0, there exists ùëáùúñsuch that for all ùë°‚â•ùëáùúñ,
| ¬§ùúáùëë,ùë†,ùëñ(ùë°) ‚àíùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ| ‚â§ùúñ
| ¬§ùúáùëë,ùëö,ùëñ(ùë°) ‚àíùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ| ‚â§ùúñ
Then we have:
ùúáùëö,ùëñùë¶‚àó
ùëö,ùëñ‚àíùúñ‚â§¬§ùë¶ùë†,ùëñ(ùë°) + ùúáùëö,ùëñùë¶ùëö,ùëñ(ùë°) ‚â§ùúáùëö,ùëñùë¶‚àó
ùëö,ùëñ+ ùúñ
ùúáùë†,ùëñùë¶‚àó
ùë†,ùëñ‚àíùúñ‚â§¬§ùë¶ùë†,ùëñ(ùë°) + ùúáùë†,ùëñùë¶ùë†,ùëñ(ùë°) ‚â§ùúáùë†,ùëñùë¶‚àó
ùë†,ùëñ+ ùúñ
Solving the above inequalities and let ùúñ‚Üí0 and ùë°‚Üí‚àû, we have:
lim
ùë°‚Üí‚àûùë¶ùëö,ùëñ(ùë°) = ùë¶‚òÖ
ùëö,ùëñ,
lim
ùë°‚Üí‚àûùë¶ùë†,ùëñ(ùë°) = ùë¶‚òÖ
ùë†,ùëñ,
‚àÄùëñ‚ààI.
which means that the decode occupancies converge to the LP targets.
‚ñ°
EC.6.
General SLI-aware Policy
In this section, we analyze the general SLI-aware setting under mild regularity on the optimization problem
(bounded penalties, etc.), in which case the SLI-aware LP may admit an optimal solution with ùëû‚òÖ
ùëë,ùëñ> 0 for
some ùëñ.
General SLI-aware Policy Fix an optimal solution of the SLI-aware LP (ùë•‚òÖ
ùëñ, ùë¶‚òÖ
ùëö,ùëñ, ùë¶‚òÖ
ùë†,ùëñ, ùëû‚òÖ
ùëù,ùëñ, ùëû‚òÖ
ùëë,ùëñ)ùëñ‚ààI,
allowing ùëû‚òÖ
ùëë,ùëñ> 0. As in Section 5.1, we partition GPUs into ùê∫(ùëõ)
mix and ùê∫(ùëõ)
solo according to the static planning.
We split the decode buffer into mixed buffer and solo buffer and track the queue lengths ùëÑùëõ
ùëë,ùëö,ùëñ(ùë°) and
ùëÑùëõ
ùëë,ùë†,ùëñ(ùë°) accordingly. Upon each class-ùëñprefill completion, we route the resulting decode job to the solo
buffer with probability ùëùùë†,ùëñand to the mixed buffer with probability 1 ‚àíùëùùë†,ùëñ.
We set the pool-selection probabilities
ùëùùë†,ùëñ:=
Ô£±Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥
ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ
ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ+ ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ
,
if ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ+ ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ> 0,
1,
otherwise.
When ùëû‚òÖ
ùëë,ùëñ> 0, the LP flow-balance implies
ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ+ ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ= ùúáùëù,ùëñùë•‚òÖ
ùëñ‚àíùúÉùëñùëû‚òÖ
ùëë,ùëñ,
so the definition of ùëùùë†,ùëñabove can be rewritten as
ùëùùë†,ùëñ=
ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ
ùúáùëù,ùëñùë•‚òÖ
ùëñ‚àíùúÉùëñùëû‚òÖ
ùëë,ùëñ
,
i.e., ùëùùë†,ùëñis the solo share of the net decode completion rate. Accordingly, we have the consistent pool-level
queue split
ùëû‚òÖ
ùëë,ùë†,ùëñ:= ùëùùë†,ùëñùëû‚òÖ
ùëë,ùëñ,
ùëû‚òÖ
ùëë,ùëö,ùëñ:= (1 ‚àíùëùùë†,ùëñ) ùëû‚òÖ
ùëë,ùëñ.


--- Page 51 ---
ec16
We additionally define pool class selection weights (with the convention 0/0 := 0)
ùúõùëö,ùëñ:=
ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ
√ç
ùëó‚ààI ùúáùëö, ùëóùë¶‚òÖ
ùëö, ùëó
,
ùúõùë†,ùëñ:=
ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ
√ç
ùëó‚ààI ùúáùë†, ùëóùë¶‚òÖ
ùë†, ùëó
.
Different from the baseline SLI-aware policy, in the general SLI-aware policy, we introduce a pool class
selection rule within each pool.
(i) Solo-pool class selection. Whenever a solo decode slot becomes available and √ç
ùëñùëÑùëõ
ùëë,ùë†,ùëñ(ùë°‚àí) > 0, the
decode scheduler selects a class according to the weights {ùúõùë†,ùëñ}ùëñ‚ààI restricted to currently nonempty solo
decode buffer:
P

select class ùëñ
 {ùëÑùëõ
ùëë,ùë†, ùëó(ùë°‚àí)} ùëó

=
ùúõùë†,ùëñ1{ùëÑùëõ
ùëë,ùë†,ùëñ(ùë°‚àí) > 0}
√ç
ùëó‚ààI ùúõùë†, ùëó1{ùëÑùëõ
ùëë,ùë†, ùëó(ùë°‚àí) > 0} .
It then routes the head-of-line job of that class to the freed solo slot. If the solo decode buffer is empty, the
slot idles.
(ii) Mixed-pool class selection. Whenever a mixed decode slot becomes available and √ç
ùëñùëÑùëõ
ùëë,ùëö,ùëñ(ùë°‚àí) > 0,
the scheduler selects a class according to {ùúõùëö,ùëñ} restricted to nonempty mixed decode buffer (defined
analogously) and routes the corresponding head-of-line job to the freed mixed slot.
This policy is work-conserving within each pool and preserves FCFS within each class buffer. It differs
from the baseline SLI-aware router only in the cross-class selection rule within the mixed/solo decode pools.
Then we focus on the fluid dynamics induced by the general SLI-aware policy. We focus on the case where
the SLI-aware LP solution satisfies ùëû‚òÖ
ùëë,ùëñ> 0 for all ùëñand both decode capacity constraints bind. The proof for
boundary cases (some ùëû‚òÖ
ùëë,ùëñ= 0, etc.) is analogous.
First, we introduce some conventions for the general SLI-aware policy. Let ùëûùëë,ùëö,ùëñ(ùë°) and ùëûùëë,ùë†,ùëñ(ùë°) denote
the mixed/solo decode queue contents, and ùë¢ùëë,ùëö,ùëñ(ùë°) and ùë¢ùëë,ùë†,ùëñ(ùë°) denote the cumulative admissions into
mixed/solo decode service. Abandonments occur from each pool buffer at rate ùúÉùëñ, i.e.,
¬§ùëèùëë,ùëö,ùëñ(ùë°) = ùúÉùëñùëûùëë,ùëö,ùëñ(ùë°),
¬§ùëèùëë,ùë†,ùëñ(ùë°) = ùúÉùëñùëûùëë,ùë†,ùëñ(ùë°),
and the total queue length is ùëûùëë,ùëñ(ùë°) = ùëûùëë,ùëö,ùëñ(ùë°) + ùëûùëë,ùë†,ùëñ(ùë°).
LEMMA EC.7. Fix a regular time ùë°at which all fluid coordinates are differentiable. Define the aggregate
decode admission and completion rates
¬§ùë¢ùëë,ùëö(ùë°) :=
‚àëÔ∏Å
ùëñ‚ààI
¬§ùë¢ùëë,ùëö,ùëñ(ùë°),
¬§ùë¢ùëë,ùë†(ùë°) :=
‚àëÔ∏Å
ùëñ‚ààI
¬§ùë¢ùëë,ùë†,ùëñ(ùë°),
¬§ùëëùëë,ùëö(ùë°) :=
‚àëÔ∏Å
ùëñ‚ààI
ùúáùëö,ùëñùë¶ùëö,ùëñ(ùë°),
¬§ùëëùëë,ùë†(ùë°) :=
‚àëÔ∏Å
ùëñ‚ààI
ùúáùë†,ùëñùë¶ùë†,ùëñ(ùë°).
Define the pool-level decode backlogs
ùëûùëë,ùëö(ùë°) :=
‚àëÔ∏Å
ùëñ‚ààI
ùëûùëë,ùëö,ùëñ(ùë°),
ùëûùëë,ùë†(ùë°) :=
‚àëÔ∏Å
ùëñ‚ààI
ùëûùëë,ùë†,ùëñ(ùë°).


--- Page 52 ---
ec17
If the corresponding pool buffer is positive at time ùë°, then that pool is work-conserving:
ùëûùëë,ùëö(ùë°) > 0 =‚áí¬§ùë¢ùëë,ùëö(ùë°) = ¬§ùëëùëë,ùëö(ùë°),
ùëûùëë,ùë†(ùë°) > 0 =‚áí¬§ùë¢ùëë,ùë†(ùë°) = ¬§ùëëùëë,ùë†(ùë°).
Moreover, under the within-pool class selection rule,
ùëûùëë,ùëö(ùë°) > 0 =‚áí¬§ùë¢ùëë,ùëö,ùëñ(ùë°) = ¬§ùë¢ùëë,ùëö(ùë°)
ùúõùëö,ùëñ1{ùëûùëë,ùëö,ùëñ(ùë°) > 0}
√ç
ùëó‚ààI ùúõùëö, ùëó1{ùëûùëë,ùëö, ùëó(ùë°) > 0},
ùëûùëë,ùë†(ùë°) > 0 =‚áí¬§ùë¢ùëë,ùë†,ùëñ(ùë°) = ¬§ùë¢ùëë,ùë†(ùë°)
ùúõùë†,ùëñ1{ùëûùëë,ùë†,ùëñ(ùë°) > 0}
√ç
ùëó‚ààI ùúõùë†, ùëó1{ùëûùëë,ùë†, ùëó(ùë°) > 0} .
Proof
If ùëûùëë,‚Ä¢(ùë°) > 0 for ‚Ä¢ ‚àà{ùëö, ùë†}, then the corresponding pool buffer is nonempty and the policy is
work-conserving on that pool: If a decode slot is free, then a job will be routed to the slot immediately. Hence
¬§ùë¢ùëë,‚Ä¢(ùë°) = ¬§ùëëùëë,‚Ä¢(ùë°). The instant admission formulas follow directly from the definition of pool selection rule: at
the fluid scale, each pool‚Äôs admissions inherit the fixed weights ùúõ‚Ä¢,ùëñrestricted to nonempty class buffers. ‚ñ°
LEMMA EC.8. Fix ùúá‚ààRùêº
++ and ùë£‚ààRùêº
++ with √çùêº
ùëñ=1 ùë£ùëñ= 1. Let ùê∑:= diag(ùúá1, . . . , ùúáùêº) and ùê¥:= ùë£ùúá‚ä§‚àíùê∑.
Then:
(i) 0 is a simple eigenvalue of ùê¥and all other eigenvalues have strictly negative real parts.
(ii) For the ODE ¬§ùë¶(ùë°) = ùê¥ùë¶(ùë°), 1‚ä§ùë¶(ùë°) is conserved and
ùë¶(ùë°) ‚àí‚Üíùë¶‚àû:=  1‚ä§ùë¶(0)
ùê∑‚àí1ùë£
1‚ä§ùê∑‚àí1ùë£
as ùë°‚Üí‚àû.
Proof
(i):By the matrix determinant lemma,
det(ùúÜùêº‚àíùê¥) = det(ùúÜùêº+ ùê∑‚àíùë£ùúá‚ä§) = det(ùúÜùêº+ ùê∑)

1 ‚àíùúá‚ä§(ùúÜùêº+ ùê∑)‚àí1ùë£

.
Hence ùúÜis an eigenvalue of ùê¥if and only if
1 = ùúá‚ä§(ùúÜùêº+ ùê∑)‚àí1ùë£=
ùêº‚àëÔ∏Å
ùëñ=1
ùë£ùëñ
ùúáùëñ
ùúÜ+ ùúáùëñ
.
At ùúÜ= 0, the right-hand side equals √ç
ùëñùë£ùëñ= 1, so ùúÜ= 0 is an eigenvalue. Its simplicity follows because the
derivative of the right-hand side at ùúÜ= 0 equals ‚àí√ç
ùëñùë£ùëñ/ùúáùëñ‚â†0. Next, let ùúÜsatisfy Re(ùúÜ) ‚â•0 and ùúÜ‚â†0. Then
for each ùëñ,

ùúáùëñ
ùúÜ+ ùúáùëñ
 <
ùúáùëñ
Re(ùúÜ) + ùúáùëñ
‚â§1,
with strict inequality because ùúÜ‚â†0. Therefore

ùêº‚àëÔ∏Å
ùëñ=1
ùë£ùëñ
ùúáùëñ
ùúÜ+ ùúáùëñ
 ‚â§
ùêº‚àëÔ∏Å
ùëñ=1
ùë£ùëñ

ùúáùëñ
ùúÜ+ ùúáùëñ
 <
ùêº‚àëÔ∏Å
ùëñ=1
ùë£ùëñ= 1,
so the eigenvalue equation cannot hold. This shows that 0 is the only eigenvalue with nonnegative real part.


--- Page 53 ---
ec18
(ii): Since ùê¥= ùë£ùúá‚ä§‚àíùê∑and √ç
ùëñùë£ùëñ= 1, we have
1‚ä§ùê¥= 1‚ä§(ùë£ùúá‚ä§) ‚àí1‚ä§ùê∑= (1‚ä§ùë£)ùúá‚ä§‚àíùúá‚ä§= 0‚ä§.
Therefore, along any absolutely continuous solution of ¬§ùë¶(ùë°) = ùê¥ùë¶(ùë°),
ùëë
ùëëùë°1‚ä§ùë¶(ùë°) = 1‚ä§¬§ùë¶(ùë°) = 1‚ä§ùê¥ùë¶(ùë°) = 0
so the total mass ùëÄ:= 1‚ä§ùë¶(ùë°) is conserved: ùëÄ= 1‚ä§ùë¶(0) for all ùë°. The equilibrium set is the kernel of ùê¥. If
ùê¥ùë¶= 0, then
0 = ùê¥ùë¶= ùë£(ùúá‚ä§ùë¶) ‚àíùê∑ùë¶
‚áê‚áí
ùê∑ùë¶= (ùúá‚ä§ùë¶) ùë£.
Let ùë†:= ùúá‚ä§ùë¶; since ùê∑is invertible, this implies
ùë¶= ùë†ùê∑‚àí1ùë£.
Hence Ker(ùê¥) = span{ùëü} where ùëü:= ùê∑‚àí1ùë£‚ààRùêº
++. Imposing the conserved mass ùëÄ= 1‚ä§ùë¶determines ùë†
uniquely:
ùëÄ= 1‚ä§ùë¶= ùë†1‚ä§ùê∑‚àí1ùë£
=‚áí
ùë†=
ùëÄ
1‚ä§ùê∑‚àí1ùë£.
Therefore the hyperplane {1‚ä§ùë¶= ùëÄ} contains a unique equilibrium, namely
ùë¶‚àû= ùëÄ
ùê∑‚àí1ùë£
1‚ä§ùê∑‚àí1ùë£=  1‚ä§ùë¶(0)
ùê∑‚àí1ùë£
1‚ä§ùê∑‚àí1ùë£.
By part (i), all eigenvalues of ùê¥other than 0 have strictly negative real parts, and 0 is a simple eigenvalue.
Let ùëü:= ùê∑‚àí1ùë£be the (right) eigenvector associated with eigenvalue 0, and note that 1‚ä§is a (left) eigenvector
since 1‚ä§ùê¥= 0‚ä§. The corresponding spectral projector is
ùëÉ:= ùëü1‚ä§
1‚ä§ùëü= ùê∑‚àí1ùë£1‚ä§
1‚ä§ùê∑‚àí1ùë£.
Standard linear-systems theory then implies ùëíùê¥ùë°‚ÜíùëÉas ùë°‚Üí‚àû. Consequently,
ùë¶(ùë°) = ùëíùê¥ùë°ùë¶(0) ‚àí‚ÜíùëÉùë¶(0) =  1‚ä§ùë¶(0)
ùê∑‚àí1ùë£
1‚ä§ùê∑‚àí1ùë£= ùë¶‚àû,
‚ñ°
PROPOSITION EC.3. Assume ùúÉùëñ> 0 for all ùëñ‚ààI. Under the refined SLI-aware policy above, every fluid
solution satisfies
lim
ùë°‚Üí‚àûùë•ùëñ(ùë°) = ùë•‚òÖ
ùëñ,
lim
ùë°‚Üí‚àûùë¶ùëö,ùëñ(ùë°) = ùë¶‚òÖ
ùëö,ùëñ,
lim
ùë°‚Üí‚àûùë¶ùë†,ùëñ(ùë°) = ùë¶‚òÖ
ùë†,ùëñ,
lim
ùë°‚Üí‚àûùëûùëë,ùëñ(ùë°) = ùëû‚òÖ
ùëë,ùëñ,
for all ùëñ‚ààI.


--- Page 54 ---
ec19
Proof
The prefill part mainly follows from the proof of Theorem 2. Therefore the prefill argument in the
proof of Theorem 2 applies verbatim, and there exists ùëáùëù< ‚àûsuch that for all ùë°‚â•ùëáùëùand all ùëñ‚ààI,
ùëûùëù,ùëñ(ùë°) > 0,
ùë•ùëñ(ùë°) = ùë•‚òÖ
ùëñ.
(EC.16)
We analyze the decode dynamics on [ùëáùëù, ‚àû) and shift the time origin so that (EC.16) holds for all ùë°‚â•0.
Then the resulting constant pool arrival rates are
ùõºùë†,ùëñ:= ùëùùë†,ùëñùúáùëù,ùëñùë•‚òÖ
ùëñ,
ùõºùëö,ùëñ:= (1 ‚àíùëùùë†,ùëñ)ùúáùëù,ùëñùë•‚òÖ
ùëñ,
ùëñ‚ààI,
and the pool capacities
ùë¶‚òÖ
ùë†:=
‚àëÔ∏Å
ùëñ‚ààI
ùë¶‚òÖ
ùë†,ùëñ,
ùë¶‚òÖ
ùëö:=
‚àëÔ∏Å
ùëñ‚ààI
ùë¶‚òÖ
ùëö,ùëñ,
which coincide with the binding decode capacity constraints at the LP optimum.
The differential version of the balance equations give:
¬§ùëûùëë,ùë†,ùëñ(ùë°) = ùõºùë†,ùëñ‚àí¬§ùë¢ùëë,ùë†,ùëñ(ùë°) ‚àíùúÉùëñùëûùëë,ùë†,ùëñ(ùë°),
¬§ùëûùëë,ùëö,ùëñ(ùë°) = ùõºùëö,ùëñ‚àí¬§ùë¢ùëë,ùëö,ùëñ(ùë°) ‚àíùúÉùëñùëûùëë,ùëö,ùëñ(ùë°),
together with the corresponding in-service dynamics ¬§ùë¶ùë†,ùëñ(ùë°) = ¬§ùë¢ùëë,ùë†,ùëñ(ùë°) ‚àíùúáùë†,ùëñùë¶ùë†,ùëñ(ùë°) and ¬§ùë¶ùëö,ùëñ(ùë°) = ¬§ùë¢ùëë,ùëö,ùëñ(ùë°) ‚àí
ùúáùëö,ùëñùë¶ùëö,ùëñ(ùë°) under the static planning.
We first show that there exists ùëá0 < ‚àûsuch that for all ùë°‚â•ùëá0,
ùëûùëë,ùë†(ùë°) > 0, ùëûùëë,ùëö(ùë°) > 0,
ùë¶ùë†(ùë°) = ùë¶‚òÖ
ùë†, ùë¶ùëö(ùë°) = ùë¶‚òÖ
ùëö,
(EC.17)
where ùëûùëë,ùë†(ùë°) := √ç
ùëñùëûùëë,ùë†,ùëñ(ùë°) and ùëûùëë,ùëö(ùë°) := √ç
ùëñùëûùëë,ùëö,ùëñ(ùë°), and ùë¶ùë†(ùë°) := √ç
ùëñùë¶ùë†,ùëñ(ùë°) and ùë¶ùëö(ùë°) := √ç
ùëñùë¶ùëö,ùëñ(ùë°).
Fix the solo pool (the mixed pool is identical). Using the LP balances and the definitions of ùëû‚òÖ
ùëë,ùë†,ùëñ:= ùëùùë†,ùëñùëû‚òÖ
ùëë,ùëñ,
we have for each ùëñ,
ùõºùë†,ùëñ= ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ+ ùúÉùëñùëû‚òÖ
ùëë,ùë†,ùëñ
‚áí
ùõºùë†,ùëñ
ùúáùë†,ùëñ
‚â•ùë¶‚òÖ
ùë†,ùëñ,
with strict inequality for any ùëñwith ùëùùë†,ùëñ> 0 (equivalently ùë¶‚òÖ
ùë†,ùëñ> 0). Summing gives
‚àëÔ∏Å
ùëñ‚ààI
ùõºùë†,ùëñ
ùúáùë†,ùëñ
>
‚àëÔ∏Å
ùëñ‚ààI
ùë¶‚òÖ
ùë†,ùëñ= ùë¶‚òÖ
ùë†.
(EC.18)
Suppose, toward a contradiction, that ùëûùëë,ùë†(ùë°) = 0 for ùë°‚â•ùëá‚Ä≤ for some ùëá‚Ä≤ < ‚àû. Then ùëûùëë,ùë†,ùëñ(ùë°) = 0 for ùë°‚â•ùëá‚Ä≤
and all ùëñ. The solo-buffer balance implies, for ùë°‚â•ùëá‚Ä≤,
0 = ¬§ùëûùëë,ùë†,ùëñ(ùë°) = ùõºùë†,ùëñ‚àí¬§ùë¢ùëë,ùë†,ùëñ(ùë°) ‚àíùúÉùëñùëûùëë,ùë†,ùëñ(ùë°) = ùõºùë†,ùëñ‚àí¬§ùë¢ùëë,ùë†,ùëñ(ùë°),
so ¬§ùë¢ùëë,ùë†,ùëñ(ùë°) = ùõºùë†,ùëñfor ùë°‚â•ùëá‚Ä≤. Therefore for ùë°‚â•ùëá‚Ä≤,
¬§ùë¶ùë†,ùëñ(ùë°) = ¬§ùë¢ùëë,ùë†,ùëñ(ùë°) ‚àíùúáùë†,ùëñùë¶ùë†,ùëñ(ùë°) = ùõºùë†,ùëñ‚àíùúáùë†,ùëñùë¶ùë†,ùëñ(ùë°).


--- Page 55 ---
ec20
Solving this linear ODE yields ùë¶ùë†,ùëñ(ùë°) ‚Üíùõºùë†,ùëñ/ùúáùë†,ùëñas ùë°‚Üí‚àû. Taking sums and using (EC.18) implies
ùë¶ùë†(ùë°) = √ç
ùëñùë¶ùë†,ùëñ(ùë°) ‚Üí√ç
ùëñùõºùë†,ùëñ/ùúáùë†,ùëñ> ùë¶‚òÖ
ùë†, contradicting the capacity constraint ùë¶ùë†(ùë°) ‚â§ùë¶‚òÖ
ùë†for all ùë°. Therefore
the solo-pool buffer cannot be eventually empty. Moreover, by (EC.18), there exists ùëáùë†< ‚àûsuch that
ùëûùëë,ùë†(ùë°) > 0,
‚àÄùë°‚â•ùëáùë†.
Applying the same reasoning to the mixed pool yields ùëáùëö< ‚àûsuch that ùëûùëë,ùëö(ùë°) > 0 for all ùë°‚â•ùëáùëö. Let
ùëá0 := max{ùëáùë†,ùëáùëö}. Then for all ùë°‚â•ùëá0, both pool buffers are positive. By the work-conserving property of the
policy within each pool, ùë¶ùë†(ùë°) = ùë¶‚òÖ
ùë†and ùë¶ùëö(ùë°) = ùë¶‚òÖ
ùëöfor all ùë°‚â•ùëá0, proving (EC.17).
Second, we focus on the convergence of the decode occupancies. We consider ùë°‚â•ùëá0 for which (EC.17)
holds. Since ùëûùëë,ùëö(ùë°) > 0, Lemma EC.7 gives ¬§ùë¢ùëë,ùëö(ùë°) = ¬§ùëëùëë,ùëö(ùë°). Moreover, since ùëûùëë,ùëö(ùë°) > 0 for all ùë°‚â•ùëá0
by (EC.17), the selection rule is active for all such ùë°. For any class ùëñwith ùõºùëö,ùëñ> 0, if ùëûùëë,ùëö,ùëñ(ùë°) = 0 when
ùë°‚â•ùëá0, then no class-ùëñjob can be pulled from the mixed buffer at time ùë°, so ¬§ùë¢ùëë,ùëö,ùëñ(ùë°) = 0 and the mixed-queue
balance gives
¬§ùëûùëë,ùëö,ùëñ(ùë°) = ùõºùëö,ùëñ‚àí¬§ùë¢ùëë,ùëö,ùëñ(ùë°) ‚àíùúÉùëñùëûùëë,ùëö,ùëñ(ùë°) = ùõºùëö,ùëñ> 0.
Therefore ùëûùëë,ùëö,ùëñ(ùë°) > 0 for ùë°‚â•ùëá0 whenever ùõºùëö,ùëñ> 0. Hence the class-level admission rate simplifies to the
unconditional proportion ùúõùëö,ùëñfor almost every ùë°‚â•ùëá0.
¬§ùë¢ùëë,ùëö,ùëñ(ùë°) = ùúõùëö,ùëñ¬§ùë¢ùëë,ùëö(ùë°) = ùúõùëö,ùëñ
‚àëÔ∏Å
ùëó‚ààI
ùúáùëö, ùëóùë¶ùëö, ùëó(ùë°).
Under the static planning, the mixed in-service masses satisfy
¬§ùë¶ùëö,ùëñ(ùë°) = ¬§ùë¢ùëë,ùëö,ùëñ(ùë°) ‚àíùúáùëö,ùëñùë¶ùëö,ùëñ(ùë°) = ùúõùëö,ùëñ
‚àëÔ∏Å
ùëó‚ààI
ùúáùëö, ùëóùë¶ùëö, ùëó(ùë°) ‚àíùúáùëö,ùëñùë¶ùëö,ùëñ(ùë°).
This is a linear system ¬§ùë¶ùëö(ùë°) = ùê¥ùëöùë¶ùëö(ùë°), where
ùê¥ùëö:= ùúõùëöùúá‚ä§
ùëö‚àídiag(ùúáùëö,1, . . . , ùúáùëö,ùêº),
ùúõùëö:= (ùúõùëö,ùëñ)ùëñ‚ààI, ùúáùëö:= (ùúáùëö,ùëñ)ùëñ‚ààI.
Applying Lemma EC.8 to ùê¥ùëöyields ùë¶ùëö,ùëñ(ùë°) ‚Üíùë¶‚òÖ
ùëö,ùëñas ùë°‚Üí‚àû. The same argument applies to the solo pool
which yields ùë¶ùë†,ùëñ(ùë°) ‚Üíùë¶‚òÖ
ùë†,ùëñ.
Finally, we prove the convergence of the decode queues. For each class ùëñ, the pool-specific decode queues
satisfy, for ùë°‚â•ùëá0,
¬§ùëûùëë,ùë†,ùëñ(ùë°) = ùõºùë†,ùëñ‚àí¬§ùë¢ùëë,ùë†,ùëñ(ùë°) ‚àíùúÉùëñùëûùëë,ùë†,ùëñ(ùë°),
¬§ùëûùëë,ùëö,ùëñ(ùë°) = ùõºùëö,ùëñ‚àí¬§ùë¢ùëë,ùëö,ùëñ(ùë°) ‚àíùúÉùëñùëûùëë,ùëö,ùëñ(ùë°).
Since both pools are work-conserving for all ùë°‚â•ùëá0. Therefore the admission rates satisfy
¬§ùë¢ùëë,ùë†,ùëñ(ùë°) = ùúõùë†,ùëñ
‚àëÔ∏Å
ùëó‚ààI
ùúáùë†, ùëóùë¶ùë†, ùëó(ùë°),
¬§ùë¢ùëë,ùëö,ùëñ(ùë°) = ùúõùëö,ùëñ
‚àëÔ∏Å
ùëó‚ààI
ùúáùëö, ùëóùë¶ùëö, ùëó(ùë°),


--- Page 56 ---
ec21
for ùë°‚â•ùëá0. Substituting gives, for ùë°‚â•ùëá0,
¬§ùëûùëë,ùë†,ùëñ(ùë°) = ùõºùë†,ùëñ‚àíùúõùë†,ùëñ
‚àëÔ∏Å
ùëó‚ààI
ùúáùë†, ùëóùë¶ùë†, ùëó(ùë°)‚àíùúÉùëñùëûùëë,ùë†,ùëñ(ùë°),
¬§ùëûùëë,ùëö,ùëñ(ùë°) = ùõºùëö,ùëñ‚àíùúõùëö,ùëñ
‚àëÔ∏Å
ùëó‚ààI
ùúáùëö, ùëóùë¶ùëö, ùëó(ùë°)‚àíùúÉùëñùëûùëë,ùëö,ùëñ(ùë°).
Since ùë¶ùë†,ùëñ(ùë°) ‚Üíùë¶‚òÖ
ùë†,ùëñand ùë¶ùëö,ùëñ(ùë°) ‚Üíùë¶‚òÖ
ùëö,ùëñ, the pool-level LP balances satisfy
ùëùùë†,ùëñùúáùëù,ùëñùë•‚òÖ
ùëñ‚àíùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ= ùúÉùëñùëû‚òÖ
ùëë,ùë†,ùëñ,
(1 ‚àíùëùùë†,ùëñ)ùúáùëù,ùëñùë•‚òÖ
ùëñ‚àíùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ= ùúÉùëñùëû‚òÖ
ùëë,ùëö,ùëñ.
Using the definition of ùúõ‚Ä¢,ùëñ, we also have ùúõùë†,ùëñ
√ç
ùëó‚ààI ùúáùë†, ùëóùë¶‚òÖ
ùë†, ùëó= ùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñand ùúõùëö,ùëñ
√ç
ùëó‚ààI ùúáùëö, ùëóùë¶‚òÖ
ùëö, ùëó= ùúáùëö,ùëñùë¶‚òÖ
ùëö,ùëñ.
Solving the standard linear ODE, we have for any ùë°‚â•ùëá0,
ùëûùëë,ùë†,ùëñ(ùë°) = ùëí‚àíùúÉùëñ(ùë°‚àíùëá0) ùëûùëë,ùë†,ùëñ(ùëá0) +
‚à´ùë°
ùëá0
ùëí‚àíùúÉùëñ(ùë°‚àíùë¢) 
ùõºùë†,ùëñ‚àíùúõùë†,ùëñ
‚àëÔ∏Å
ùëó‚ààI
ùúáùë†, ùëóùë¶ùë†, ùëó(ùë¢)

ùëëùë¢.
Since ùë¶ùë†,ùëñ(ùë¢) ‚Üíùë¶‚òÖ
ùë†,ùëñfor each ùëñ, the integrand converges to ùõºùë†,ùëñ‚àíùúõùë†,ùëñ
√ç
ùëó‚ààI ùúáùë†, ùëóùë¶‚òÖ
ùë†, ùëó= ùõºùë†,ùëñ‚àíùúáùë†,ùëñùë¶‚òÖ
ùë†,ùëñ= ùúÉùëñùëû‚òÖ
ùëë,ùë†,ùëñ.
Standard arguments for linear convolution with an exponentially decaying kernel (or directly applying
dominated convergence) then yield ùëûùëë,ùë†,ùëñ(ùë°) ‚Üíùëû‚òÖ
ùëë,ùë†,ùëñas ùë°‚Üí‚àû. The same argument applied to the mixed
pool queue gives ùëûùëë,ùëö,ùëñ(ùë°) ‚Üíùëû‚òÖ
ùëë,ùëö,ùëñ. Summing gives ùëûùëë,ùëñ(ùë°) = ùëûùëë,ùëö,ùëñ(ùë°) + ùëûùëë,ùë†,ùëñ(ùë°) ‚Üíùëû‚òÖ
ùëë,ùëñ.
‚ñ°
EC.7.
Additional Experiments and Empirical Validation
This section provides additional empirical validations that complement the main text. We first provide a
comprehensive analysis of workload heterogeneity through task-level statistics and visualization in the (ùëÉ, ùê∑)
space. We then validate the exponential distribution assumption for output lengths by fitting empirical data
and calculating divergence metrics. Finally, we detail the computing infrastructure used for our measurements
and simulations.
Workload heterogeneity and task characteristics. Table EC.1 reports the average prompt (input) and
output lengths for eight task categories from the Databricks Dolly-15k instruction-tuning dataset (Conover
et al. 2023). The data reveals substantial variation across categories. For example, summarization and
information extraction tasks average over 1,000 input tokens with moderate output. In contrast, creative
writing and open QA require fewer than 100 input tokens yet generate longer outputs on average.
To further illustrate this heterogeneity, Figure EC.1 plots the input and output length pairs for two
representative categories: Information Extraction and Creative Writing. These two classes exhibit nearly
opposite characteristics. Information Extraction tasks cluster in the high-input and low-output region. Creative
Writing tasks concentrate in the low-input and high-output zone. This contrast confirms that a uniform
scheduling policy cannot efficiently balance resources across diverse workloads, which motivates the
multiclass framework developed in this paper.


--- Page 57 ---
ec22
Task Category
Avg. ùëÉ(tokens)
Avg. ùê∑(tokens)
Samples
Brainstorming
61
331
1,764
Classification
123
142
2,136
Closed QA
992
182
1,738
Creative writing
89
915
699
General QA
69
572
2,187
Information extraction
1,139
273
1,462
Open QA
45
293
3,739
Summarization
1,177
436
1,150
Table EC.1
Workload heterogeneity: average prompt (ùëÉ) and output (ùê∑) lengths across task categories from
the Databricks Dolly-15k instruction-tuning dataset (Conover et al. 2023).
Figure EC.1
Scatter plot of input versus output lengths for Information Extraction (blue) and Creative Writing
(red) in the Databricks Dolly-15k dataset. The two categories occupy nearly opposite regions of the (ùëÉ, ùê∑) space.
Validation of the exponential distribution assumption. Our model assumes exponentially distributed
output lengths for each task class. This assumption directly determines the decode service rate in our stochastic
network. To check whether this approximation is reasonable, we fit exponential distributions to the actual
output length data from the Databricks Dolly-15k dataset using Maximum Likelihood Estimation.


--- Page 58 ---
ec23
Figure EC.2 compares the empirical CDFs with the fitted exponential CDFs for all eight task categories.
The exponential fit is good for most categories. Brainstorming, Classification, Closed QA, Information
Extraction, Open QA, and Summarization all track closely. General QA shows reasonable agreement despite
some mid-range deviation. Creative Writing deviates more noticeably in the upper tail. This is likely because
its output distribution is heavier-tailed than a pure exponential, which is consistent with its high mean and
variance.
Figure EC.2
Empirical output length CDFs (solid blue) versus fitted exponential CDFs (dashed red) for the
eight task categories in the Databricks Dolly-15k dataset.


--- Page 59 ---
ec24
To complement the visual comparison, Table EC.2 reports the Kullback-Leibler (KL) divergence between
the empirical output-length distribution and its fitted exponential approximation for each category. Smaller
values indicate closer agreement. Overall, the exponential assumption captures the main characteristics of the
distributions well enough to justify its use for analytical tractability.
Table EC.2
KL divergence between empirical output lengths and fitted exponential distributions across task
categories (smaller is better).
Category
KL divergence
Open QA
0.0584
Brainstorming
0.0652
Closed QA
0.1245
Summarization
0.1255
General QA
0.1309
Classification
0.1491
Creative Writing
0.1595
Information Extraction
0.2631
Computing infrastructure. Table EC.3 summarizes the hardware and software environment used for the
empirical measurements and numerical experiments in the paper.
Table EC.3
Computing infrastructure specifications.
Component
Specification
Hardware
CPU
AMD EPYC 7H12 64-core processor
Cores / threads
2 sockets √ó 64 cores √ó 2 threads (256 hardware threads)
Clock frequency
1.5‚Äì2.6 GHz
Cache
L1d/L1i: 4 MiB (128√ó), L2: 64 MiB (128√ó), L3: 512 MiB (32√ó)
GPU
4√ó NVIDIA A100-SXM4-40GB
GPU memory
40 GB per GPU (160 GB total)
System memory
1.0 TiB RAM
Swap
8.0 GiB
Software
Operating system Ubuntu 22.04.5 LTS (Jammy Jellyfish)
Python
Version 3.9.23
CUDA
Version 12.6
NVIDIA driver
Version 560.28.03
