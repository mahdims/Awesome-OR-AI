--- Page 1 ---
ProxyWar: Dynamic Assessment of LLM Code Generation in
Game Arenas
Wenjun Peng‚àó
Xinyu Wang‚àó‚Ä†
Qi Wu
wenjun.peng@adelaide.edu.au
xinyu.wang02@adelaide.edu.au
qi.wu01@adelaide.edu.au
Abstract
Large language models (LLMs) have revolutionized automated
code generation, yet the evaluation of their real-world effective-
ness remains limited by static benchmarks and simplistic metrics.
We present ProxyWar, a novel framework that systematically as-
sesses code generation quality by embedding LLM-generated agents
within diverse, competitive game environments. Unlike existing ap-
proaches, ProxyWar evaluates not only functional correctness but
also the operational characteristics of generated programs, combin-
ing automated testing, iterative code repair, and multi-agent tour-
naments to provide a holistic view of program behavior. Applied
to a range of state-of-the-art coders and games, our approach un-
covers notable discrepancies between benchmark scores and actual
performance in dynamic settings, revealing overlooked limitations
and opportunities for improvement. These findings highlight the
need for richer, competition-based evaluation of code generation.
Looking forward, ProxyWar lays a foundation for research into
LLM-driven algorithm discovery, adaptive problem solving, and the
study of practical efficiency and robustness, including the poten-
tial for models to outperform hand-crafted agents. The project is
available at https://github.com/xinke-wang/ProxyWar.
CCS Concepts
‚Ä¢ Software and its engineering ‚ÜíSoftware testing and debug-
ging; ‚Ä¢ Computing methodologies ‚ÜíArtificial intelligence.
Keywords
LLM-based Code Generation, Program Evaluation Frameworks,
Operational Code Quality
ACM Reference Format:
Wenjun Peng, Xinyu Wang, and Qi Wu. 2026. ProxyWar: Dynamic Assess-
ment of LLM Code Generation in Game Arenas. In 2026 IEEE/ACM 48th
International Conference on Software Engineering (ICSE ‚Äô26), April 12‚Äì18,
2026, Rio de Janeiro, Brazil. ACM, New York, NY, USA, 13 pages. https:
//doi.org/10.1145/3744916.3773220
‚àóThese authors contributed equally to this work.
‚Ä†Xinyu Wang is the corresponding author.
This work is licensed under a Creative Commons Attribution 4.0 International License.
ICSE ‚Äô26, Rio de Janeiro, Brazil
¬© 2026 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-2025-3/26/04
https://doi.org/10.1145/3744916.3773220
1
Introduction
The emergence of Large Language Models (LLMs), such as GPT-
4 [2] and AlphaCode [38], has fundamentally transformed the land-
scape of software development [26]. Unlike traditional code gener-
ation tools that rely on fixed templates or rule-based logic [14, 33],
LLMs are capable of producing diverse, context-aware, and often
highly creative code snippets directly from natural language instruc-
tions [17, 29]. With their remarkable ability to understand complex
programming tasks and synthesize functioning programs across
multiple languages and domains [27, 38], LLM-based code gener-
ation tools are increasingly integrated into modern development
workflows. This shift is not only accelerating the pace of software
engineering, but also reshaping the operational workflows of pro-
grammers, including problem-solving, debugging, code review, and
system design [29].
However, despite the rapid adoption of LLM-based code gen-
eration, the methods for evaluating these models remain largely
inadequate [41]. Current approaches [3, 17, 18, 22, 32, 63] predomi-
nantly rely on metrics borrowed from natural language processing
(e.g., BLEU scores [48]) or simplistic functional tests (e.g., pass@k
on HumanEval [18]). While these metrics provide some insights
into syntactic similarity or basic correctness, they fail to capture the
multifaceted nature of operational characteristics of generated code
(e.g., runtime stability, efficiency, and robustness) that matter in
real-world software development [52]. A model that generates code
passing all test cases might produce solutions that are inefficient,
unmaintainable, or brittle when faced with edge cases [17]. More
critically, many widely used benchmarks evaluate code in isolation,
largely emphasizing function-level correctness without capturing
behavior under dynamic execution constraints, even though some
recent efforts (e.g., agent-based SWE-bench [36] evaluations) begin
to introduce limited interaction. This leaves open the broader ques-
tion of how to evaluate systems where code must not only work but
also operate reliably and competitively under shared resource budgets.
This limitation becomes particularly apparent when consider-
ing domains like algorithmic competitions, game AI development,
performance optimization, or self-improvement workflows, where
the usefulness of code is naturally measured by its comparative
performance against alternative solutions. For example, in compet-
itive programming, solutions are evaluated on hidden test cases,
time/memory limits, and runtime performance, meaning that two
functionally correct programs can differ substantially in their prac-
tical effectiveness [38]. In these contexts, traditional pass/fail met-
rics provide little insight into whether one model produces code
that is more efficient, more stable, or more strategically effective
arXiv:2602.04296v1  [cs.SE]  4 Feb 2026


--- Page 2 ---
ICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil
Wenjun Peng, Xinyu Wang, and Qi Wu
A
Proxy A
def select_action(self, 
observation: Any, action_mask: 
List[bool]) -> Optional[int]:
legal_actions = [i for i, legal in 
enumerate(action_mask) if legal]
if not legal_actions:
    return None
# Convert observation to a 2D 
numpy array for easier 
manipulation
board 
=np.array(observation).reshape(8, 
8)
...
B
Proxy B
def select_action(self, 
observation: List[int], 
action_mask: List[bool]) -> 
Optional[int]:
if len(observation) != 9:
    raise ValueError("Observation 
must be of size 9")
legal_actions = [i for i, legal in 
enumerate(action_mask) if legal]
if not legal_actions:
    return None
...
Game Board
Coder A
Unit Test Passed
Tester
Unit 
Test
Test 
Failed
Test 
Errors
Code 
Revisions
Unit Test Passed
Prompt Manager
#1 Task Description
#2 Game Information
#3 File Structure
#4 Sample Code/Data
#5 Coding Standards
Coder B
Code 
Generation
Code 
Generation
Figure 1: Overview of the ProxyWar framework. The Prompt
Manager provides standardized game specifications to multiple
LLM code generators, which develop game-playing agents. The
Tester validates each agent through unit tests, with failed tests
triggering a repair loop that sends error messages back for revision.
Successfully tested agents compete as proxies on the Game Board
in automated tournaments. Match outcomes determine skill ratings
and provide a comprehensive evaluation of code generation quality
through both functional correctness and competitive performance.
than another. The lack of comprehensive, comparative evaluation
frameworks has created a significant gap between the promise of
LLM-based code generation and our ability to systematically assess
their real-world viability.
To address this challenge, we present ProxyWar, a novel frame-
work that evaluates LLM-generated code through competitive, con-
trolled execution under shared resource budgets. Instead of focusing
solely on static correctness, ProxyWar embeds generated agents
into dynamic game environments, where head-to-head competition
provides a practical and fine-grained signal of a model‚Äôs ability
to produce code that is not only correct but also efficient, stable,
and adaptable. As illustrated in Figure 1, ProxyWar orchestrates a
complete pipeline from code generation to competitive evaluation:
LLMs first generate game-playing agents based on standardized
prompts; these agents undergo hierarchical unit testing with an
optional repair loop; and finally they compete in automated tour-
naments where their performance provides a holistic measure of
operational behavior rather than merely functional success.
Unlike static benchmarks, ProxyWar evaluates code in dynamic,
interactive environments where success depends not just on cor-
rectness but also on algorithmic efficiency, runtime robustness,
adaptability, and decision-making under constraints. The frame-
work supports multiple code generators competing simultaneously,
enabling comprehensive comparisons between different models or
configurations. The integrated testing and revision mechanism en-
sures that we measure not only raw generation capability but also
the ability of a model to debug, refine, and stabilize its own code, a
property increasingly important in autonomous software engineer-
ing workflows. Through automated tournaments and skill-based
rankings using algorithms like TrueSkill [44], ProxyWar provides
a flexible approach to compare different code generation models
across multiple dimensions of code quality. This paper makes the
following contributions:
‚Ä¢ ProxyWar, the first competitive, execution-based evaluation
framework for LLM code generation, which orchestrates auto-
mated execution, testing, and iterative repair mechanisms to
evaluate code quality through competitive gameplay, support-
ing three key capabilities:
(1) It automatically generates, tests, and deploys game-playing
agents from natural language specifications.
(2) It facilitates iterative code improvement through auto-
mated error feedback and repair loops.
(3) It provides skill-based rankings that reflect both func-
tional correctness and operational performance.
‚Ä¢ A multi-dimensional assessment methodology that captures di-
verse aspects of operational characteristics beyond traditional
pass/fail metrics, including algorithmic efficiency, code com-
plexity, runtime performance, and strategic decision-making
capabilities.
‚Ä¢ Implementation of a scalable platform supporting diverse game
environments, from perfect information board games to imper-
fect information card games, enabling systematic assessment
across varied programming challenges.
‚Ä¢ Extensive empirical evaluation on state-of-the-art LLMs in-
cluding ChatGPT, Claude, Gemini, and leading open-source
models. By combining traditional metrics with competitive
performance analysis, our results reveal previously hidden
limitations in LLMs‚Äô code generation capabilities, particularly
in algorithmic creativity, optimization strategies, and adaptive
problem-solving, providing new insights into the gap between
functional correctness and practically deployable behavior.
2
Background and Motivation
Code generation has evolved significantly from early template-
based and pattern-based approaches to recent LLM-based meth-
ods. Prior to the LLM era, code generation systems relied on var-
ious techniques including API pattern mining [56, 65], statistical
language models [7, 33], and neural approaches like sequence-to-
sequence models [39, 62]. However, the evaluation methods for
these diverse code generation systems have not kept pace with
their rapid advancement. While traditional generators could be
evaluated through API coverage metrics and simple correctness
checks, the emergence of LLMs capable of generating diverse, cre-
ative solutions has exposed fundamental limitations in existing
evaluation methodologies. In this section, we examine the evolu-
tion of code generation evaluation, analyze the limitations of cur-
rent approaches, and establish the motivation for our game-based
evaluation framework. This paper primarily focuses on evaluating


--- Page 3 ---
ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas
ICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil
LLM-based code generation, as these models now dominate both
research and practical applications in automated programming.
2.1
Code Generation Quality Assessment
The journey from text-based to execution-based evaluation repre-
sents a fundamental paradigm shift in how we assess generated
code. Early evaluation approaches borrowed heavily from natural
language processing, treating code as sequences of tokens rather
than executable logic.
BLEU and Text-based Metrics. The BLEU score [48], originally de-
signed for machine translation, measures n-gram overlap between
generated and reference code: BLEU = BP¬∑exp
√çùëÅ
ùëõ=1 ùë§ùëõ¬∑ log(ùëùùëõ)

,
where ùëùùëõrepresents the precision of n-grams and BP is a brevity
penalty. However, BLEU treats all tokens equally and ignores code
structure, so two functionally equivalent implementations with dif-
ferent variable names or loop structures receive low scores despite
identical behavior [52].
CodeBLEU: Incorporating Code Structure. Recognizing the
limitations of BLEU, CodeBLEU [52] extends BLEU by incorporat-
ing syntactic and semantic features: CodeBLEU = ùõº¬∑ BLEU + ùõΩ¬∑
BLEUweight + ùõæ¬∑ MatchAST + ùõø¬∑ MatchDF. This metric combines
weighted n-gram matching (with keywords weighted 5√ó higher),
Abstract Syntax Tree (AST) similarity, and data-flow analysis. While
CodeBLEU achieves higher correlation with human evaluation, it
still relies on reference solutions and cannot assess functional cor-
rectness [52].
Pass@k: The Shift to Execution-Based Evaluation. The intro-
duction of HumanEval [18] marked a pivotal transition to execution-
based metrics. The pass@k metric measures the probability that
at least one of k generated samples passes all test cases: pass@ùëò=
1‚àí
ùëõ‚àíùëê
ùëò

ùëõ
ùëò

, where ùëõis the total number of samples, ùëêis the number
of correct samples, and ùëòis the number of samples considered. This
metric directly evaluates functional correctness but introduces new
challenges around test adequacy and coverage.
2.2
Limitations of Current Approaches
Despite advances in metrics, fundamental limitations persist in how
we evaluate LLM-generated code. These limitations span multiple
dimensions, from dataset quality to evaluation scope.
Simplicity Bias and Limited Scope. Current benchmarks exhibit
severe simplicity bias. HumanEval [18] contains only 164 problems,
averaging 7.7 test cases each, focusing on algorithmic puzzles rather
than real programming tasks. Analysis of real codebases reveals
that over 70% of functions are non-standalone, requiring under-
standing of complex dependencies and cross-file relationships [25].
Yet mainstream benchmarks evaluate only isolated functions. The
MBPP benchmark [10], while larger at 974 problems, suffers from
similar limitations with mostly single-function tasks under 50 lines.
Recent work, such as SWE-bench [36] and ClassEval [25] partially
mitigates simplicity bias by introducing multi-file or multi-class set-
tings, but these benchmarks still evaluate code in a non-interactive
manner and do not incorporate execution-time resource constraints.
Dataset Contamination and Memorization. Recent studies have
uncovered widespread dataset contamination across benchmarks [36].
Analysis found that 94% of SWE-bench issues were created before
LLM training cutoffs [6], with clear evidence of memorization af-
fecting results [20]. When evaluated on truly held-out test sets,
model performance drops significantly, for example, StarCoder-
7B [43] achieved Pass@1 scores 4.9 times higher on leaked sam-
ples than non-leaked samples [66]. While game environments are
also publicly available and therefore not immune to contamina-
tion, competitive settings reduce the effectiveness of memorized
solutions: agents must handle diverse opponent behaviors, adhere
strictly to environment interfaces, and operate under runtime con-
straints, making brittle or overfitted memorized implementations
more likely to fail.
Static Evaluation Misses Dynamic Reality. Perhaps most criti-
cally, current evaluation is fundamentally static while real program-
ming is inherently dynamic. Professional development involves it-
erative refinement, debugging based on compiler feedback, and col-
laborative problem-solving, but these dynamic aspects are largely
absent from existing benchmarks [64]. Evaluating models solely
on isolated functions fails to capture their effectiveness in realistic,
end-to-end software engineering workflows. Meanwhile, recent
agent-based evaluations (e.g., SWE-bench agents, CodeAgent [64])
introduce limited forms of interactivity, but they typically operate
under single-agent, non-adversarial settings and do not compare
multiple code generators under shared constraints. Similar chal-
lenges have been noted even for general-purpose LLMs, where static
or narrowly scoped benchmarks often fail to reflect real-world rea-
soning, perception, or interaction capabilities [28, 50, 57, 58]. These
observations parallel our setting: static correctness tests in code
generation likewise miss the dynamic, context-dependent behaviors
required for practical performance.
Limited Quality Dimensions. Existing metrics focus almost ex-
clusively on functional correctness, ignoring other crucial aspects
of operational characteristics, such as:
‚Ä¢ Efficiency: Algorithmic complexity or runtime performance.
‚Ä¢ Robustness: Sensitivity to input variations and edge cases re-
mains largely untested.
‚Ä¢ Maintainability: Code readability, documentation, and modular
design are not assessed.
‚Ä¢ Creativity: Novel algorithmic approaches versus memorized
solutions cannot be distinguished.
2.3
The Gap Between Benchmarks and
Real-World Programming
The disconnect between benchmark performance and practical
utility has become increasingly apparent. GitHub Copilot stud-
ies [67] show that user acceptance rate, not correctness metrics,
best predicts productivity gains. This suggests that current evalua-
tion frameworks fail to capture what makes code genuinely useful.
Missing Contextual Understanding. Real-world programming
tasks demand an understanding of existing codebases, API con-
straints, and architectural decisions. ClassEval [25] demonstrates
that class-level code generation is significantly more challenging
than function-level generation, with even the most capable LLMs


--- Page 4 ---
ICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil
Wenjun Peng, Xinyu Wang, and Qi Wu
Game Environment Layer
Action Space
Game States
Reward
Initial State
Observation
Testing Layer
Agent Layer
Code Generation Layer
#n Behavior
#1 Syntax
‚Ä¶
Pass
Action
Selection
States 
Parsing
Functions
Code 
Implementation
Coder
Error 
Information
Observation
Rule
Action
‚Ä¶
Reward
Code 
Structure
Other 
Requirements
Test Cases
Code
Tournament Management Layer
Coder A
Coder B
Proxy A
Proxy B
Win
Lose
+10
-10
Figure 2: The ProxyWar framework pipeline. ProxyWar evaluates LLM code generation through a multi-layer architecture. The process
begins with the Code Generation Layer, where LLMs (Coders) receive game specifications (rules, observation/action formats, etc.) to generate
agent implementations. Failed attempts trigger an iterative repair loop with detailed error feedback. The Testing Layer validates generated
code through hierarchical test cases. Successfully tested code is deployed in the Agent Layer, which parses game states and executes action
selection logic. Finally, the Tournament Management Layer orchestrates competitions between agents (Proxy A vs. Proxy B) in the Game
Environment, where agents receive observations, select actions, and compete for rewards. Match outcomes are aggregated into rankings,
providing a comprehensive assessment of code generation quality based on actual competitive performance rather than static metrics.
exhibiting notably low success rates when dependencies are in-
volved. This lack of contextual understanding suggests that high
benchmark scores may not accurately reflect practical ability.
Absence of Interactive Development. Professional programmers
spend most of their time debugging and refining code rather than
writing it from scratch [12]. However, few existing benchmarks as-
sess iterative improvement based on test failures or runtime behav-
ior. The recent CodeAgent framework [64] showed that allowing
models to iterate based on feedback significantly improves suc-
cess rates, highlighting the importance of interactive evaluation.
Collectively, these works demonstrate increasing interest in inter-
active and agent-based evaluation. However, they do not provide a
comparative, head-to-head assessment of generated programs un-
der shared resource constraints, nor do they measure operational
robustness in dynamic environments.
Limited Competitive Assessment. In many domains, such as
algorithmic competitions, game AI, and optimization tasks, code
quality is inherently comparative. A solution that merely works
may be vastly inferior to one that works efficiently. Current pass/fail
metrics cannot capture these performance gradients or trade-offs
between different quality dimensions.
2.4
Motivation
The limitations of existing evaluation approaches motivate our
game-based framework. Games provide unique advantages for as-
sessing code generation capabilities:
Natural Performance Gradients. Unlike binary pass/fail tests,
game competitions produce continuous performance metrics through
score differentials. This enables fine-grained comparison between
models that would appear equivalent under traditional metrics. For
example, a chess engine that wins 60% of games clearly outperforms
one winning 40%, even if both implement legal moves correctly.
Reduced Reliance on Memorization. Because the space of pos-
sible game states is large and runtime constraints limit exhaustive
search, game environments reduce the effectiveness of purely mem-
orized or template-based approaches and make brittle, overfitted
strategies more likely to fail.
Multi-Dimensional Assessment. Game environments naturally
evaluate multiple quality dimensions simultaneously:
‚Ä¢ Correctness: Invalid moves are immediately apparent.
‚Ä¢ Efficiency: Slow algorithms lose on time or get outmaneuvered.
‚Ä¢ Robustness: Strategies must handle diverse opponent behaviors.
‚Ä¢ Adaptability: Success requires adapting to situations.
This aligns naturally with evaluating the operational character-
istics of generated code rather than only its syntactic or functional
correctness.
Scalable Complexity. Games provide a controllable range of com-
plexity, from simple ones like tic-tac-toe to more complex ones like
chess, allowing assessment across different skill levels. Even simple
games serve as useful calibration environments for testing legality,
basic decision logic, and robustness under repeated interaction.
By situating code generation evaluation within competitive game
environments, we can assess not just whether models can write
code, but whether they can write good code that performs well
under challenging, dynamic conditions. This approach addresses
the critical gaps in existing evaluation frameworks while providing
actionable insights into model capabilities and limitations.


--- Page 5 ---
ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas
ICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil
3
The ProxyWar Framework
3.1
Design Principles
The ProxyWar framework is built upon four fundamental design
principles that guide its architecture and implementation:
P1: Competitive Evaluation through Direct Competition. Un-
like static benchmarks that evaluate code in isolation, ProxyWar
places generated agents in direct competition. This principle en-
sures that the operational characteristics of generated programs are
measured not by adherence to predetermined patterns but by their
actual performance under shared constraints and against diverse op-
ponents. The competitive nature reveals subtle quality differences
that binary pass/fail tests cannot capture.
P2: Separation of Concerns between Framework and Agent.
ProxyWar adopts a strict separation where the framework handles
all game mechanics, state management, and rule enforcement, while
agents focus on decision-making. This design choice simplifies the
code generation task and ensures fair comparison; all agents operate
under identical conditions with the same computational constraints.
Crucially, this separation treats the environment as a pluggable task
sandbox, so the same protocol can, in principle, be applied to non-
game software engineering tasks (e.g., bug fixing or performance
tuning) that can be executed in a controlled environment.
P3: Multi-Dimensional Quality Assessment. ProxyWar targets
multiple dimensions of behavior beyond functional correctness,
including algorithmic efficiency, robustness to edge cases, runtime
stability, and strategic decision-making. Each dimension contributes
to overall performance but can be analyzed independently.
P4: Iterative Refinement through Automated Feedback. Real-
world programming is iterative. ProxyWar incorporates automated
testing and repair loops, allowing generated code to be refined
based on error feedback. This mirrors professional development
practices and evaluates not just initial outputs but also the model‚Äôs
debugging capabilities.
3.2
System Architecture
As demonstrated in Figure 2, ProxyWar comprises five primary
layers, each with well-defined responsibilities and interfaces.
Game Environment Layer provides a unified interface. Formally,
a game environment can be defined as a tuple: G = ‚ü®S, A, T, R, O,ùë†0‚ü©.
In this formulation, S denotes the state space and A the action
space. The transition function T : ùëÜ√óùê¥‚ÜíR assigns a scalar reward
to each state-action pair. The observation function O : ùëÜ‚ÜíOspace
maps the underlying state to the observation available to an agent,
accommodating both perfect information (O(ùë†) = ùë†) and imperfect
information scenarios. The initial state ùë†0 ‚ààùëÜspecifies where the
environment begins. This formalization supports a wide range of
games with different information structures, while the environ-
ment layer enforces game rules, manages state transitions, and
ensures consistent observations for all agents. In non-game set-
tings, the same abstraction can represent other executable SE tasks,
where states are program or system configurations and actions are
candidate edits or decisions.
Agent Layer defines the minimal interface that all generated code
must implement. ùúãis the policy function: ùúã: Ospace √ó {0, 1}|A| ‚Üí
A, where the second parameter serves as an action mask indicating
legal moves. Additional methods and attributes can be freely added,
while this interface is required to ensure compatibility. This minimal
contract is deliberately simple so that diverse models and coding
styles can be evaluated under a common protocol.
Code Generation Layer manages the transformation from nat-
ural language specifications to executable agent code. For each
game G and model ùëÄ, the Generation Process can be formulated
as: (ùëÄ, Prompt(G)) ‚ÜíCodeùúã. This layer also provides a Repair
Function for bug fixing: (ùëÄ, Codeùúã, Errors) ‚ÜíCodeùúã‚Ä≤, which en-
ables multi-round improvement based on test feedback, effectively
capturing the model‚Äôs debugging capabilities. The same interface
is used regardless of whether the task is a game, a regression bug
fix, or a performance optimization problem, making the pipeline
reusable across different SE settings.
Testing Layer is responsible for validating the generated agent
code through a hierarchical testing strategy. This layer systemati-
cally checks each agent using a test suite T = {ùë°1,ùë°2, ...,ùë°ùëõ}, which
covers multiple levels of validation: syntax checking, import ver-
ification, interface compliance, and behavioral correctness. Each
level is designed to detect different types of errors, ranging from
basic syntax issues to violations of required interfaces or incorrect
game logic. Formally, let T be the set of all test cases. The testing
function is defined as follows:
Test(Codeùúã, T) ‚Üí
(
PASS
if ‚àÄùë°ùëñ‚ààT : ùë°ùëñ(Codeùúã) = true
(FAIL, ùê∏)
otherwise, where ùê∏is the error set
This layer enables early detection of errors, ensuring that only
code meeting all requirements proceeds to further evaluation or
competition. In practice, ProxyWar combines generic tests (e.g.,
AST and interface checks that apply to any environment) with
environment-specific suites (e.g., multi-move interactions or edge-
case scenarios), all executed by a common harness without human
intervention at evaluation time. In addition, any detected errors and
their details are provided to the Code Generation Layer, enabling
the repair function to iteratively refine the agent code based on
concrete feedback.
Tournament Management Layer. The Tournament Manage-
ment Layer orchestrates competitive evaluation by scheduling
matches among agents. While a full round-robin format is em-
ployed for small numbers of agents or two-player games, the frame-
work also supports sampling-based tournament scheduling for sce-
narios with many agents or multiplayer games. In these cases,
a representative subset of matches is sampled to ensure metric
convergence without incurring excessive computational cost. For-
mally, the tournament can be represented as a directed graph
ùê∫= (ùëâ, ùê∏), where ùëâ= {ùúã1, ùúã2, . . . , ùúãùëõ} denotes the set of agents,
and ùê∏= {(ùëñ, ùëó,ùë§) | ùëñ, ùëó‚ààùëâ, ùë§‚àà{ùëñ, ùëó, draw}} captures the observed
outcomes of the sampled matches, with ùë§indicating the winner
or a draw. This layer provides the flexibility to support both ex-
haustive and sampled tournaments, accommodating different game
types and evaluation budgets while ensuring reliable agent ranking
and metric stability. Because the scheduling logic is environment-
agnostic, the same engine can be reused for head-to-head evaluation
on non-game SE tasks (e.g., comparing two bug-fixing agents on
the same issue set under identical resource limits).


--- Page 6 ---
ICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil
Wenjun Peng, Xinyu Wang, and Qi Wu
Table 1: Characteristics of the game environments in ProxyWar. State-space sizes are approximate legal-position counts; action-space sizes
are the maximum number of legal actions from any state.
Game
State Space
Action Space
Information
Complexity
Single-Player Puzzles
Sudoku
6.67 √ó 1021
‚â§9 / cell
Perfect
NP-complete
2048 (4√ó4)
‚àº4.4 √ó 1016
4
Perfect+Random
NP-hard
Tower of Hanoi (ùëõ)
3ùëõ
‚â§6
Perfect
ùëÇ(2ùëõ) optimal
Maze (grid)
ùëä√ó ùêª
‚â§4
Perfect / Partial
ùëÇ(ùëâ+ ùê∏)
Two-Player Board/Spatial Games
Tic-Tac-Toe
1.97 √ó 104
‚â§9
Perfect
Solved / ùëÇ(1)
Connect Four
4.53 √ó 1012
‚â§7
Perfect
PSPACE-complete
Reversi
‚àº1028
‚àº10
Perfect
PSPACE-complete
Snake (2-player, ùëõ√óùëõ)
2ùëõ2
4
Perfect
PSPACE-complete
Multi-Player Card Games
Texas Hold‚Äôem (Limit)
‚àº1.6 √ó 1017
‚â§4
Imperfect
PSPACE-complete
3.3
Multi-Dimensional Evaluation Methodology
ProxyWar evaluates generated code across multiple dimensions,
providing a comprehensive assessment beyond simple win rates.
Competitive Performance Metrics. We employ the TrueSkill
rating system [44] proposed by Microsoft Xbox Network to com-
pute skill ratings. Each agent ùëñis modeled by a skill distribution:
skillùëñ‚àºN (ùúáùëñ, ùúé2
ùëñ). After each match, the ratings are updated via
Bayesian inference. To ensure robust comparisons, we report the
conservative skill estimate defined as ùúáùëñ‚àí3ùúéùëñ, which provides
a lower bound on expected performance with 99.7% confidence.
This framework is applied consistently across both multiplayer and
single-player games. In the multiplayer setting, TrueSkill is used
in the standard manner to update skills based on the outcomes
among all participants. In the case of single-player games, agents
are evaluated by performing their tasks independently under the
same conditions, and their results are compared to determine rela-
tive performance. These comparisons are then incorporated into
the TrueSkill system in the same way as multiplayer outcomes. This
unified methodology enables fair and consistent ranking of agents
across all game types in ProxyWar.
Static Evaluation Metrics. Beyond competitive outcomes, Prox-
yWar evaluates static code quality using a suite of hierarchical,
task-driven metrics tailored to code generation scenarios. These
metrics are designed to complement tournament-based evaluation
by revealing properties that are not always reflected in match re-
sults:
‚Ä¢ Pass@1: The proportion of initial agent submissions that pass
all reference test cases, capturing basic functional correctness.
‚Ä¢ Repair Rate: The rate at which agents successfully fix initial
bugs and achieve a passing solution within a limited number of
iterations, measuring self-debugging ability.
‚Ä¢ Hierarchical Test Layers:
‚Äì Structure: Syntactic correctness and conformity to required
interfaces.
‚Äì Function: Ability to solve fundamental subtasks or simple test
cases.
‚Äì Logic: Success on more complex, game-specific or edge-case
logic tests.
‚Äì Robustness: Stability and correctness across adversarial or
stress test scenarios.
Together, these metrics characterize the operational behavior
of each generated agent in terms of its activity frequency, repair
reliability, and stress tolerance, independent of its relative rank-
ing in tournaments. By combining these static metrics with dy-
namic tournament results, ProxyWar enables comprehensive, multi-
dimensional assessment of LLM-generated code.
4
Implementation
This section details the implementation of ProxyWar, highlighting
the engineering challenges and solutions involved in building a
scalable, game-based code evaluation system. Our entire evaluation
pipeline is based on Python code generation, a deliberate choice
for its simplicity, ecosystem maturity, and strong support among
LLMs. Python‚Äôs wide adoption in both research and industry facili-
tates reproducibility and enables seamless integration with diverse
game environments, agent implementations, and analysis tools.
In this section, we discuss game environment design, coder inte-
gration, prompt engineering, testing infrastructure, and practical
considerations for extensibility and reliability.
4.1
Game Environments
ProxyWar implements ten carefully selected games spanning three
categories: single-player puzzles, two-player board games, and mul-
tiplayer card games. This diversity ensures comprehensive evalu-
ation across a wide range of algorithmic paradigms, information
structures, and programming challenges. Each game is chosen to
stress different aspects of code generation, such as search, planning,
probabilistic reasoning, and collaboration under uncertainty.
Table 1 summarizes the core characteristics of the implemented
game environments, including state space size, action space, in-
formation structure, and computational complexity. The selected


--- Page 7 ---
ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas
ICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil
Table 2: LLM-based coders integrated in ProxyWar. ‚ÄúO/S‚Äù indicates
open source status. Context is the maximum token window.
Release Model
Size O/S Context
General-purpose
2024.9
Qwen2.5-72B [11]
72 B
Y
32,768
2024.10 Claude3.5-Sonnet [8]
‚Äî
N
200,000
2025.1
Phi4 [1]
14 B
Y
16,384
2025.2
Gemini2.0-Flash [21]
‚Äî
N 1,048,576
2025.3
DeepSeekV3-0324 [40]
685 B Y
163,840
2025.4
Llama4-Maverick [5]
400 B Y 1,048,576
2025.4
GPT4.1-Mini-20250414 [45]
‚Äî
N 1,047,576
2025.4
GPT4.1-20250414 [45]
‚Äî
N 1,047,576
Reasoning-enhanced
2025.1
O3-Mini-20250131 [47]
‚Äî
N
200,000
2025.4
Qwen3-235B-0428 [61]
235 B Y
40,960
2025.5
DeepSeek-R1-0528 [31]
671 B Y
128,000
2025.5
Claude4-Sonnet-20250522 [9]
‚Äî
N
200,000
2025.6
Gemini-2.5-Flash [30]
‚Äî
N 1,048,576
2025.6
Magistral-Small-2506 [51]
24 B
Y
40,000
Code-specialized
2024.11 Qwen2.5-Coder [34]
32 B
Y
32,768
2025.1
Codestral-2501 [4]
‚Äî
N
262,144
2025.4
Mercury-Coder [37]
‚Äî
N
32,000
2025.5
Codex-Mini [46]
‚Äî
N
200,000
games collectively cover a broad spectrum of complexity and un-
certainty, from fully solved, perfect-information puzzles to chal-
lenging multiplayer games with hidden information and stochastic
dynamics. This diversity ensures that ProxyWar can evaluate code
generation models on a wide range of algorithmic skills, including
planning, search, probabilistic reasoning, and adaptive decision-
making under uncertainty. By integrating both classic and modern
programming challenges, ProxyWar provides a robust foundation
for comprehensive evaluation of code generation models.
4.2
Coder Integration
ProxyWar is designed to enable flexible evaluation of code gen-
eration systems. While this paper focuses on LLM-based coders,
reflecting their prevalence in current research and deployment, the
framework itself adopts a modular interface that allows for straight-
forward integration of new coders, regardless of the architecture.
To streamline integration and experiment management, Proxy-
War leverages the OpenRouter 1 platform, which aggregates APIs
from major providers, including OpenAI, Anthropic, Google, and
Meta, behind a single endpoint. This design minimizes engineering
overhead, supports unified monitoring and analytics, and allows
new coders to be added or replaced through simple configuration
changes without modifying the core framework code.
Table 2 provides an overview of the coders evaluated in this
work. The selection includes both proprietary and open-source
models and covers a range of parameter scales and context window
lengths. For analysis, we organize coders into three categories:
1https://openrouter.ai/
‚Ä¢ General-purpose LLMs: Models such as GPT-4.1 [45], Claude
3.5 Sonnet [8], Gemini 2.5 Flash [30], and Llama 4 Maverick [5]
are evaluated for their ability to interpret diverse environments
and generate coherent, well-structured code.
‚Ä¢ Reasoning-enhanced LLMs: These include MiniMax-M1 [15],
DeepSeek-R1 [31], O3-Mini [47], and Qwen3-235B [61], which
emphasize explicit reasoning or planning abilities and may en-
hance performance on strategic or long-horizon tasks.
‚Ä¢ Code-specialized LLMs: Models such as Qwen-2.5-Coder-32B [34],
Codestral 2501 [4], Mercury-Coder [23], and Codex-Mini [46]
are optimized for code synthesis and are assessed for their adapt-
ability to full agent implementation.
4.3
Prompt Engineering and Code Generation
1 class BaseAgent(ABC):
2
def __init__(self, name: str):
3
self.name = name
4
5
@abstractmethod
6
def select_action(self, observation: Any,
action_mask: List[bool]) -> Optional[int]:
7
pass
Listing 1: Minimal BaseAgent interface.
The prompt manager in ProxyWar systematically transforms
each game specification into a comprehensive set of coding instruc-
tions, providing LLMs with all information required to generate
robust and compatible agent code. Rather than relying on mini-
mal or loosely structured prompts, ProxyWar employs a consistent
template that integrates the following key elements:
(1) Task Framing and Objectives: Each prompt begins with an
explicit task description, emphasizing not only functional cor-
rectness but also the goal of achieving strong competitive per-
formance. The LLM is instructed to create an agent capable of
winning against diverse opponents, encouraging solutions that
go beyond naive or baseline strategies.
(2) Game Environment Specification: The prompt includes a
detailed description of the game rules, a formal specification
of the observation and action spaces, and concrete examples
of data formats. This ensures the generated code can correctly
interpret the game environment and select valid actions.
(3) Code Structure Constraints: To ensure seamless integration
with the evaluation framework, the prompt specifies required
class inheritance (from BaseAgent), mandatory method signa-
tures (such as select_action), import statements, file struc-
ture, and error handling expectations. These constraints mini-
mize common LLM code generation errors and promote consis-
tency across all submissions. The essential interface required
of every agent is illustrated in Listing 1.
(4) Strategic and Algorithmic Guidance: In addition to imple-
mentation requirements, the prompt encourages algorithmic
creativity, game-theoretic reasoning, efficiency considerations,
and comprehensive code documentation. This guidance aims to
stimulate more sophisticated and maintainable agent designs.


--- Page 8 ---
ICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil
Wenjun Peng, Xinyu Wang, and Qi Wu
Table 3: LLM coder tournament rankings and performance metrics across all game environments. Columns under Average Game-based
Tournament Rankings report each model‚Äôs average ranking based on TrueSkill across nine games. Code Quality & Robustness columns
summarize development and runtime metrics: A.R. is the mean number of code revision iterations (max 3); Part. (%) is the percent of agents
successfully participating in tournament play; Win (%) indicates the overall percentage of matches won by each model, aggregated across all
games and rounds; Error (%) measures the incidence of timeouts and runtime errors during play; Speed (s) denotes the agent‚Äôs average
decision time per step (in seconds) across all games.
Model
Average Game-based Tournament Rankings (‚Üì)
Code Quality & Robustness
Single-Player
Two-Player
Multi-Player
Developing
Runtime
Sudoku
Maze
2048
Hanoi
TTT
Connect4
Reversi
Snake
Hold‚Äôem
A.R.
Part. (%)
Win (%)
Error (%)
Speed (s)
General-purpose
Qwen2.5-72B
6.0
12.2
11.0
9.8
11.4
9.0
11.4
14.4
18.0
0.69
84.4
28.3
0.10
0.052
Claude3.5-Sonnet
10.2
11.0
9.4
4.4
8.2
12.6
11.0
7.8
7.0
0.31
100
21.0
0.21
0.294
Phi4
7.2
10.2
8.4
12.2
5.0
14.5
14.0
9.6
17.4
0.91
84.4
27.6
3.40
0.034
Gemini2.0-Flash
10.2
9.6
8.6
10.2
8.8
9.4
13.8
11.6
7.8
0.22
95.6
28.3
0.21
0.024
DeepSeekV3-0324
3.4
4.4
7.2
8.0
6.0
5.6
9.2
10.2
8.0
0.20
100
35.9
0.07
0.056
Llama4-Maverick
17.4
5.8
6.8
11.4
14.0
8.6
9.0
12.4
14.8
0.62
95.6
21.5
0.07
0.059
GPT4.1-Mini
13.2
3.4
9.8
7.2
14.0
6.6
5.4
7.0
6.2
0.29
100
34.5
1.17
0.122
GPT4.1
14.8
3.2
7.8
7.2
10.4
4.0
6.6
6.6
6.4
0.31
100
31.3
0.21
0.301
Reasoning-enhanced
O3-Mini-20250131
4.6
5.4
4.6
6.4
12.8
7.0
7.2
4.0
9.0
0.07
100
38.8
0.14
0.016
Qwen3-235B-0428
12.6
14.2
6.2
12.8
8.8
8.0
11.0
7.0
13.6
0.38
84.4
34.9
0.33
0.069
DeepSeek-R1-0528
1.4
13.8
9.4
12.8
12.2
5.4
6.4
6.6
3.4
0.53
91.1
39.6
0.14
0.123
Claude4-Sonnet
3.0
6.2
9.8
3.4
3.6
2.6
3.6
7.0
7.4
0.18
100
31.2
0.14
0.211
Gemini-2.5-Flash
13.8
8.6
4.0
7.6
7.8
13.4
7.6
8.8
5.8
0.44
97.8
25.1
0.35
0.176
Magistral-Small
9.0
13.4
16.6
8.4
10.8
14.0
10.4
9.0
8.6
1.20
97.8
19.7
1.64
0.010
Code-specialized
Qwen2.5-Coder
8.4
11.6
17.2
14.2
9.2
17.4
7.6
18.0
6.0
0.98
75.6
13.4
0.98
0.049
Codestral-2501
12.6
14.8
15.0
17.0
9.6
7.0
13.2
8.8
12.2
0.60
86.7
31.1
0.58
0.050
Mercury-Coder
8.4
16.8
13.4
18.0
8.8
16.0
18.0
16.0
12.8
0.67
77.8
18.7
0.16
0.003
Codex-Mini
16.8
6.8
7.2
3.8
9.8
9.0
6.2
7.0
8.8
0.31
97.8
31.2
0.14
0.035
4.4
Testing Infrastructure
The testing infrastructure in ProxyWar uses a hierarchical design to
ensure generated agents are correct and robust before tournaments,
filtering common failure modes at multiple levels:
(1) Code Structure & Validation: This foundational layer verifies
that submitted code meets basic requirements: file existence
and readability, Python syntax correctness using AST parsing,
proper class structure inheriting from BaseAgent, and interface
compliance with required methods like select_action.
(2) Basic Functionality: Tests fundamental agent capabilities, in-
cluding basic action selection behavior, action validation to
ensure returned actions are legal and properly formatted, so-
lution format verification for puzzle games, and basic puzzle
interaction mechanics.
(3) Game Interaction & Logic: Evaluates advanced game-specific
behaviors through multi-move game interaction tests, puzzle-
solving logic validation, maze navigation capabilities, and scenario-
specific testing (e.g., all-in situations in poker).
(4) Robustness & Performance: Tests agent resilience and effi-
ciency through edge case handling (empty action masks, termi-
nal states), error condition recovery, response time validation,
and scalability under increased complexity.
All test results are compiled into structured reports and fed
back into the repair loop, enabling iterative code refinement and
significantly improving the rate of successful agent deployment.
4.5
Tournament Management Engine
The tournament management engine in ProxyWar coordinates com-
petition among generated coders across diverse games and experi-
mental settings. The system is responsible for scheduling matches,
isolating code execution, handling errors, and collecting evaluation
metrics. Key components include:
(1) Flexible Match Scheduling: For two-player games, the engine
uses round-robin tournaments, pairing each coder in both player
roles to mitigate first-move advantage. Multiplayer games adopt
randomized or Swiss-system scheduling for efficient ranking,
while single-player games use standardized challenge sets for
consistent evaluation.
(2) Execution Isolation and Resource Control: Each agent runs
in a separate process with strict resource limits, such as memory
and per-move time constraints. File system and network access
are restricted to prevent information leakage or external calls,
and any violation results in forfeiture or penalty.
(3) Failure Detection and Handling: The engine detects and
records failures such as timeouts, exceptions, illegal actions,
and repeated crashes, applying defined penalties in tournament
scoring and rating.
(4) Performance and Reliability Monitoring: During matches,
the system logs metrics including decision time, resource con-
sumption, move validity, and exception statistics, supporting
post-hoc analysis and reproducible benchmarking.


--- Page 9 ---
ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas
ICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil
5
Empirical Evaluation
In this section, we present an empirical evaluation of ProxyWar
across 18 state-of-the-art LLMs and 9 diverse game environments.
Our experiments aim to answer the following research questions:
‚Ä¢ RQ1 (Discriminative Power): Does ProxyWar‚Äôs competition-
based evaluation provide better discrimination between models
compared to traditional code generation metrics?
‚Ä¢ RQ2 (Quality-Performance Relationship): What is the re-
lationship between static code quality metrics and competitive
performance in game environments?
‚Ä¢ RQ3 (Environmental Sensitivity): How do different game
characteristics (complexity, information structure, player count)
affect model rankings and performance patterns?
5.1
Experimental Setup
Experiments ran on a workstation with an Intel i9-12900KF (3.19 GHz),
64 GB RAM, and an NVIDIA RTX A5000 (24 GB). All code genera-
tion and tournaments ran on this single machine to avoid hardware
variability. For each environment, we ran five rounds; in each round,
every model generated a fresh agent and all agents played a full
tournament under identical settings. Overall, each model partici-
pated in over 10,000 matches, providing robust estimates. We fixed
random seeds for prompt sampling and environment initializa-
tion within each round to reduce uncontrolled randomness. Each
game enforced a 45 s per-decision timeout; agents exceeding it were
disqualified for that round, while faster responses contributed to
observed latency behavior.
5.2
Model Discriminative Power (RQ1)
Table 3 shows that ProxyWar exposes clear performance differ-
ences among models that appear similar by traditional metrics. For
example, Qwen2.5-Coder and DeepSeek-R1 may achieve compara-
ble pass@k on static benchmarks, but ProxyWar reveals a nearly
threefold gap in tournament win rates (13.4% vs. 39.6%).
Fine-grained Differentiation. Some models achieve 100% partic-
ipation, meaning their code always passes all tests and would be
marked as ‚Äútask-complete‚Äù by conventional standards. However,
ProxyWar shows that their operational characteristics can vary
widely among these models. For example, DeepSeekV3 and Claude
3.5 Sonnet both reach perfect participation, yet DeepSeekV3 out-
performs Claude 3.5 Sonnet in win rate by nearly 15 percentage
points. ProxyWar also reveals distinct strengths between similar
models: GPT-4.1 ranks higher than GPT-4.1-Mini in Maze, while
GPT-4.1-Mini performs better in Reversi. These gaps would be
largely invisible in purely pass/fail settings where both models
simply solve most benchmark tasks.
Category-Level Insights. Code-specialized models often under-
perform in competitive, context-rich scenarios. Their average win
rate is 23.6%, which is lower than both general-purpose models
(28.6%) and reasoning-enhanced models (31.6%). This suggests that
being optimized for code completion does not guarantee effective
program synthesis or strategic reasoning. In complex environments
that require broader context and multi-step planning, general mod-
els tend to excel. More broadly, ProxyWar highlights that once
1
# DeepseekR1 Generated
2
def select_action(obs):
3
def backtrack(g):
4
for r in range(9):
5
for c in range(9):
6
if g[r][c]==0:
7
for v in opts(g,r,c):
8
g[r][c]=v
9
if backtrack(g): return
True
10
g[r][c]=0
11
return False
12
return True
13
...
14
backtrack(grid)
1
# GPT4.1 Generated
2
def select_action(obs):
3
empties.sort(key=lambda rc:len(
cand(rc[0],rc[1])))
4
def backtrack(i=0):
5
if i==len(empties): return
True
6
r,c=empties[i]
7
opts=cand(r,c)
8
opts.sort(key=lcv)
9
for v in opts:
10
...
11
if backtrack(i+1): return
True
12
...
13
return False
14
backtrack()
Figure 3: Case study. Left: DeepSeek-R1 generates a minimal,
fast backtracking agent. Right: GPT-4.1 uses advanced heuristics
(MRV/LCV), but the Python implementation is slower in practice.
basic correctness is satisfied, factors such as decision quality un-
der constraints, timeout incidence, and robustness against diverse
opponents become key discriminators between models.
Finding 1:
Competitive, game-based evaluation provides fine-
grained differentiation among LLM coders, revealing substantial per-
formance gaps in operational behavior that are invisible to conven-
tional pass/fail metrics.
5.3
Code Quality Analysis (RQ2)
Table 4 summarizes the results of hierarchical code testing and
clarifies the relationship between static code quality metrics and
competitive performance.
Beyond Functional Correctness. Most models achieve high Pass@1
rates, often exceeding 90%. However, these results do not predict
real-world performance in dynamic environments. For instance,
O3-Mini achieves the highest Pass@1 (97.3%), but only ranks mid-
tier in tournament play. DeepSeek-R1, with a lower Pass@1 (87.9%),
consistently wins more matches. The weak correlation (Spearman‚Äôs
ùúå= 0.23) between Pass@1 and competitive ranking shows that
traditional metrics miss key aspects of practical code effectiveness.
Algorithmic Efficiency versus Practical Performance. As il-
lustrated in Figure 3, competitive evaluation reveals unexpected
gaps between theoretically optimal algorithms and practical perfor-
mance. In Sudoku, DeepSeek-R1 generates a minimalist backtrack-
ing agent, while GPT-4.1 applies advanced search heuristics, with
Minimum Remaining Value (MRV) and Least Constraining Value
(LCV), in a more complex implementation. Despite its theoretical
superiority, GPT-4.1‚Äôs code is nearly 28 √ó slower in Python due to
higher interpretation overhead and complex bookkeeping, result-
ing in worse tournament outcomes. DeepSeek-R1‚Äôs concise design
not only solves puzzles faster but also generalizes better to harder
instances. This demonstrates that, practical efficiency and imple-
mentation overhead can outweigh algorithmic sophistication, and
these differences only emerge clearly under competitive, dynamic
evaluation.
Debugging and Repair Ability. Interactive, multi-stage code gen-
eration provides new insight into LLMs‚Äô debugging skills. Some
models, such as Claude3.5-Sonnet and DeepSeekV3, achieve perfect


--- Page 10 ---
ICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil
Wenjun Peng, Xinyu Wang, and Qi Wu
repair rates, reliably fixing initial bugs via self-correction, while
others like Mercury-Coder almost never succeed. However, even
perfect repair does not guarantee strong tournament performance,
revealing the limits of current debugging capabilities.
Runtime Stability and Computational Efficiency. Models with
low runtime error rates, such as DeepSeekV3 and Llama4-Maverick,
consistently outperform less stable competitors. Beyond correct-
ness and stability, we observe dramatic disparities in computational
efficiency: for the same agent task, the average step time (Speed (s))
across models may differ by up to two orders of magnitude (e.g.,
Claude3.5-Sonnet at 0.294s vs. Magistral-Small at 0.010s per step).
This demonstrates that even for functionally correct code, resource
usage and latency can vary substantially, which may impact deploy-
ment in resource-constrained or latency-sensitive environments.
Finding 2: Traditional static metrics such as pass@k fail to predict
performance in dynamic or competitive settings. Practical efficiency,
debugging ability, runtime stability, and computational resource usage
are all crucial but often overlooked dimensions of the operational
characteristics of generated programs.
5.4
Environmental Sensitivity (RQ3)
Model performance demonstrates pronounced variation across dif-
ferent game environments, highlighting both specialized strengths
and inherent limitations.
Game Complexity Effects. In simple games such as Tic-Tac-Toe
(state space 104), models tend to converge, with rankings tightly
clustered and a low standard deviation (ùúé= 2.8). However, as com-
plexity increases, such as in Reversi (1028 states), the performance
gap widens significantly (ùúé= 3.6). Here, models like Claude4-
Sonnet (rank 3.6) and GPT-4.1-Mini (rank 5.4) stand out for their
strong position evaluation, while code-specialized models, for ex-
ample Mercury-Coder (rank 18.0), consistently underperform.
Impact of Information Structure. Perfect information games fa-
vor models with effective search and planning, as seen with Claude4-
Sonnet‚Äôs dominance. In contrast, games with imperfect information,
such as Texas Hold‚Äôem, shift the advantage to models excelling at
probabilistic reasoning and opponent modeling, with DeepSeek-R1
(rank 3.4) performing especially well. The weak correlation be-
tween model rankings in perfect and imperfect information games
(ùúå= 0.28) further underscores the distinct skill sets required.
Algorithmic Paradigm Specialization. Single-player puzzles re-
veal each model‚Äôs algorithmic tendencies. DeepSeek-R1 excels in
constraint satisfaction tasks (Sudoku, rank 1.4), GPT-4.1 demon-
strates strong graph search abilities (Maze, rank 3.2), while Gemini-
2.5-Flash is most effective in probabilistic planning (2048, rank 4.0).
These trends persist across repeated trials, suggesting that observed
strengths reflect model biases rather than random fluctuation.
Consistency Across Games. The variance in rankings across
all environments varies by model category. General-purpose mod-
els show moderate consistency, reasoning-enhanced models have
slightly higher variance, and code-specialized models display the
most inconsistency, suggesting their optimizations are often narrow
and environment-dependent. Notably, Claude4-Sonnet is an excep-
tion, maintaining high consistency across all games, while O3-Mini
consistently ranks among the top five in two-player games.
Table 4: LLM coder testing report. Pass@1 is the test pass rate of
the initial code. Repair is the bug fix rate from initial to final sub-
mission. Structure, Function, Logic, and Robustness assess code
quality at four hierarchical testing layers, as detailed in Section 4.4.
Model
Pass@1
Repair
Structure
Function
Logic
Robustness
General-purpose
Qwen2.5-72B
0.915
0.710
0.974
0.943
0.733
0.850
Claude3.5-Sonnet
0.948
1
0.983
0.943
0.933
0.913
Phi4
0.844
0.789
0.965
0.871
0.533
0.738
Gemini2.0-Flash
0.959
0.533
0.991
0.943
0.967
0.938
DeepSeekV3-0324
0.940
1
1
0.936
0.933
0.863
Llama4-Maverick
0.918
0.900
0.922
0.900
0.967
0.925
GPT4.1-Mini
0.951
1
0.991
0.950
0.967
0.888
GPT4.1
0.926
1
0.983
0.907
0.900
0.888
Reasoning-enhanced
O3-Mini
0.973
1
1
0.964
0.967
0.950
Qwen3-235B
0.929
0.769
0.887
0.957
0.933
0.938
DeepSeek-R1
0.879
0.955
0.904
0.857
0.967
0.850
Claude4-Sonnet
0.934
1
1
0.921
0.833
0.900
Gemini2.5-Flash
0.932
1
0.991
0.936
0.867
0.863
Magistral-Small
0.811
0.986
0.817
0.879
0.733
0.713
Code-specialized
Qwen2.5-Coder
0.858
0.692
0.957
0.864
0.800
0.725
Codestral-2501
0.890
0.750
1
0.907
0.700
0.775
Mercury-Coder
0.890
0
1
0.893
0.667
0.813
Codex-Mini
0.967
1
0.965
0.986
1
0.925
Finding 3: Environmental factors such as complexity, information
structure, and task type induce large performance swings. No sin-
gle model excels universally, and model strengths are often context-
dependent.
5.5
Implications for Model Selection & Practice
Our results underscore the necessity for multi-dimensional, context-
aware evaluation. Practitioners should consider both the target en-
vironment and the required robustness when selecting LLM coders.
Optimization for static code completion alone is insufficient for
deployment in dynamic or adversarial scenarios.
Finding 4: Model selection should align with the complexity and in-
formation structure of the target application. Game-based evaluation
surfaces weaknesses and strengths that may be missed by conven-
tional metrics, guiding more reliable deployment.
6
Discussion
6.1
Limitations and Future Directions
While ProxyWar provides a powerful new lens for LLM code eval-
uation, several limitations remain. Game environments, though
rich and diverse, may not capture the full complexity of real-world
software development. Scaling evaluation to broader or industry-
scale problems poses engineering and cost challenges. Future work
may extend this approach to larger codebases and more realistic
collaborative scenarios.
6.2
Threats to Validity
While ProxyWar aims to provide a rigorous and comprehensive
framework for evaluating code generation, several threats to valid-
ity remain. First, all LLM-based coders exhibit inherent stochastic-
ity: results may vary across repeated runs due to randomness in
sampling and prompt interpretation. While we use fixed seeds and


--- Page 11 ---
ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas
ICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil
prompt templates to improve consistency, absolute reproducibility
may not be guaranteed, especially for proprietary models. Second,
the selection of game environments, though diverse, may not fully
capture the complexity and variety of real-world programming
tasks. Conclusions drawn from performance on these games may
not generalize to all domains or software engineering problems.
We therefore view our environment suite as a starting point rather
than an exhaustive benchmark, and we encourage future work to
add new, non-game environments. Third, evaluation relies on spe-
cific implementations and external dependencies, such as Python
interpreter versions and third-party APIs. Changes in LLM APIs or
model updates over time could affect the replicability of our results.
Finally, while tournament outcomes and static code metrics provide
multi-dimensional assessment, no evaluation can exhaustively cap-
ture all aspects of code quality, including long-term maintainability
or human factors. Our testing infrastructure remains subject to
the usual limitations of test-based evaluation: inadequate or unrep-
resentative test suites may overestimate capabilities. We partially
mitigate this through layered tests (structure, function, logic, ro-
bustness) and runtime monitoring (timeouts, exceptions, illegal
moves), but independent auditing of the released test suites will
be essential. We encourage future work to expand the benchmark
suite, test additional environments, and further investigate these
sources of variability.
7
Related Work
7.1
Code Generation Evaluation
LLMs have rapidly advanced the state of code generation, with
models such as Codex [38], Code Llama [53], StarCoder [18], and
Qwen-2.5-Coder [34] achieving strong results on standard program-
ming benchmarks. However, evaluating the real-world capabilities
of these models remains an open challenge.
Static function-level benchmarks [10, 32, 38] remain the pri-
mary tools for code generation evaluation. These benchmarks pro-
vide natural language prompts and assess correctness based on
predefined test suites, reporting metrics such as pass@k. While
effective for measuring functional correctness on small, isolated
problems, these settings lack coverage of practical software en-
gineering scenarios, such as class-level generation, dependency
management, or multi-file coordination [25, 36]. Moreover, they do
not capture critical aspects such as code maintainability, readability,
efficiency, or style [63]. To address these limitations, recent efforts
have proposed multi-dimensional and more realistic benchmarks.
ClassEval [25] moves towards class-level evaluation, revealing sig-
nificant drops in model performance compared to function-level
tasks. SWE-bench [36] evaluates models on real-world bug fixes
and repository-level modifications, requiring context-aware reason-
ing and multi-file edits. CoderEval [63] and LiveCodeBench [35]
broaden evaluation to pragmatic tasks and aim to mitigate dataset
contamination. In addition, CodeBLEU [52] augments text-based
metrics with syntax and data-flow similarity, and human preference
studies, such as CodeArena [24], introduce side-by-side solution
comparisons along style and efficiency dimensions.
Nevertheless, most current approaches still evaluate code in
isolation, neglecting the interactive, iterative, and competitive as-
pects of actual programming. Only recently have frameworks like
CodeAgent [64] and EvalPlus [42] begun to model iterative self-
repair and test-driven development. There remains a significant
gap in capturing the collaborative or adversarial settings that shape
software development outcomes.
7.2
Games and Competition-Based Assessment
Games have long served as benchmarks for AI, from classic Elo and
TrueSkill systems [44] to recent breakthroughs in games like Star-
Craft II [54] and Dota 2 [13]. In code generation, competition-based
evaluation has appeared in frameworks such as CodeContests [38]
and CodeContests+ [60], but these largely focus on static, contest-
style problem-solving. More interactive or adversarial frameworks,
such as CodeArena [24], enable head-to-head model comparison,
but often remain limited to specific problem types. Recently, game
environments [16, 19, 49, 55, 59] have been widely adopted to eval-
uate advanced LLM capabilities beyond code generation. Frame-
works such as Voyager [55], GameGPT [16], LVLM-Playground [59],
VARP [19] use complex or interactive games to assess general rea-
soning, open-ended learning, or multimodal understanding in LLM-
driven agents. However, these efforts primarily target general intel-
ligence, agent coordination, or perception, rather than systematic
program synthesis or code quality evaluation. ProxyWar advances
this line of research by embedding code generation and evalua-
tion within diverse, competitive game environments, supporting
automated repair, skill-based ranking, and comprehensive code
analysis. Unlike prior work, ProxyWar unifies competitive, multi-
agent evaluation with static code analysis to more fully capture the
capabilities and limitations of modern LLM-based coders.
8
Conclusion
We present ProxyWar, a unified framework for evaluating LLM-
based code generation through competitive, game-based tourna-
ments. By moving beyond static function-level benchmarks, Prox-
yWar enables multi-dimensional assessment of code correctness,
efficiency, and strategic behavior in dynamic, adversarial environ-
ments. Our approach systematically integrates code generation,
testing, repair, and tournament management, providing more real-
istic and actionable insights into the capabilities of modern code
generation models. Looking ahead, ProxyWar opens new avenues
for research in program synthesis and AI-driven algorithm dis-
covery. Future work may explore whether LLM coders can create
truly novel strategies or outperform hand-crafted agents in com-
plex games, as well as extend the framework to broader domains
and more open-ended tasks. We hope ProxyWar will serve as a
foundation for rigorous, reproducible, and creative evaluation of
next-generation program synthesis systems.
Acknowledgments
This work was supported by the Australian Research Council under
Discovery Project DP260102534.
References
[1] Marah Abdin, Jyoti Aneja, Harkirat Behl, S√©bastien Bubeck, Ronen Eldan, Suriya
Gunasekar, Michael Harrison, Russell J Hewett, Mojan Javaheripi, Piero Kauff-
mann, et al. 2024. Phi-4 Technical Report. arXiv:2412.08905 [cs.CL] arXiv
preprint.


--- Page 12 ---
ICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil
Wenjun Peng, Xinyu Wang, and Qi Wu
[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-
cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
Anadkat, et al. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] arXiv
preprint.
[3] Anisha Agarwal, Aaron Chan, Shubham Chandel, Jinu Jang, Shaun Miller,
Roshanak Zilouchian Moghaddam, Yevhen Mohylevskyy, Neel Sundaresan, and
Michele Tufano. 2024. Copilot Evaluation Harness: Evaluating LLM-Guided
Software Programming. arXiv:2402.14261 [cs.SE] arXiv preprint.
[4] Mistral AI. 2024. Introducing Codestral: Mistral AI‚Äôs Next Generation Code
Model. https://mistral.ai/news/codestral. Accessed: 2025-07-13.
[5] Meta AI. 2024. Llama 4: Advancing Multimodal Intelligence. https://ai.meta.com/
blog/llama-4-multimodal-intelligence/. Accessed: 2025-07-13.
[6] Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias
Uddin, and Song Wang. 2024. SWE-bench+: Enhanced Coding Benchmark for
LLMs. arXiv:2410.06992 [cs.SE] arXiv preprint.
[7] Miltiadis Allamanis and Charles Sutton. 2013. Mining Source Code Repositories
at Massive Scale Using Language Modeling. In Proceedings of the 10th Working
Conference on Mining Software Repositories (MSR). IEEE, San Francisco, CA, USA,
207‚Äì216. doi:10.1109/MSR.2013.6624029
[8] Anthropic. 2024. Claude 3.5 Sonnet. https://www.anthropic.com/news/claude-3-
5-sonnet. Accessed: 2025-07-13.
[9] Anthropic. 2025. Claude 4. https://www.anthropic.com/news/claude-4. Accessed:
2025-07-13.
[10] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk
Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al.
2021. Program Synthesis with Large Language Models. arXiv:2108.07732 [cs.PL]
arXiv preprint.
[11] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai
Dang, Peng Wang, Shijie Wang, Jun Tang, et al. 2025. Qwen2.5-VL Technical
Report. arXiv:2502.13923 [cs.CV] arXiv preprint.
[12] Moritz Beller, Georgios Gousios, and Andy Zaidman. 2015. How (Much) Do
Developers Test?. In Proceedings of the 37th International Conference on Software
Engineering (ICSE). IEEE, Florence, Italy, 559‚Äì562.
[13] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys≈Çaw
Dƒôbiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris
Hesse, et al. 2019.
Dota 2 with Large Scale Deep Reinforcement Learning.
arXiv:1912.06680 [cs.AI] arXiv preprint.
[14] Marcel Bruch, Martin Monperrus, and Mira Mezini. 2009. Learning from Exam-
ples to Improve Code Completion Systems. In Proceedings of the 7th Joint Meeting
of the European Software Engineering Conference and the ACM SIGSOFT Sympo-
sium on the Foundations of Software Engineering (ESEC/FSE). ACM, Amsterdam,
Netherlands, 213‚Äì222.
[15] Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan,
Changqing Yu, Chao Wang, Cheng Zhu, et al. 2025. MiniMax-M1: Scaling Test-
Time Compute Efficiently with Lightning Attention. arXiv:2506.13585 [cs.CL]
arXiv preprint.
[16] Dake Chen, Hanbin Wang, Yunhao Huo, Yuzhao Li, and Haoyang Zhang.
2023. GameGPT: Multi-Agent Collaborative Framework for Game Development.
arXiv:2310.08067 [cs.AI] arXiv preprint.
[17] Liguo Chen, Qi Guo, Hongrui Jia, Zhengran Zeng, Xin Wang, Yijiang Xu, Jian
Wu, Yidong Wang, Qing Gao, Jindong Wang, et al. 2024. A Survey on Evaluating
Large Language Models in Code Generation Tasks. arXiv:2408.16498 [cs.SE]
arXiv preprint.
[18] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021. Evaluating Large Language Models Trained on Code.
arXiv:2107.03374 [cs.CL] arXiv preprint.
[19] Peng Chen, Pi Bu, Jun Song, Yuan Gao, and Bo Zheng. 2024.
Can VLMs
Play Action Role-Playing Games? Take Black Myth: Wukong as a Study Case.
arXiv:2409.12889 [cs.AI] arXiv preprint.
[20] Wentao Chen, Lizhe Zhang, Li Zhong, Letian Peng, Zilong Wang, and Jingbo
Shang. 2025. Memorize or Generalize? Evaluating LLM Code Generation with
Evolved Questions. arXiv:2503.02296 [cs.SE] arXiv preprint.
[21] Google DeepMind. 2024.
Gemini: The Next Chapter in Google AI.
https://blog.google/technology/google-deepmind/google-gemini-ai-update-
december-2024. Accessed: 2025-07-13.
[22] Victor Dibia, Adam Fourney, Gagan Bansal, Forough Poursabzi-Sangdeh, Han
Liu, and Saleema Amershi. 2022. Aligning Offline Metrics and Human Judgments
of Value for Code Generation Models. arXiv:2210.16494 [cs.SE] arXiv preprint.
[23] Mingzhe Du, Anh Tuan Luu, Bin Ji, Qian Liu, and See-Kiong Ng. 2024. Mercury:
A Code Efficiency Benchmark for Code Large Language Models. In Advances in
Neural Information Processing Systems, Vol. 37. Neural Information Processing
Systems Foundation, Inc., Vancouver, Canada, 16601‚Äì16622. doi:10.52202/079017-
0529
[24] Mingzhe Du, Anh Tuan Luu, Bin Ji, Xiaobao Wu, Dong Huang, Terry Yue Zhuo,
Qian Liu, and See-Kiong Ng. 2025. CodeArena: A Collective Evaluation Platform
for LLM Code Generation. arXiv:2503.01295 [cs.SE] arXiv preprint.
[25] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan
Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, and Yiling Lou. 2024. Evaluating
Large Language Models in Class-Level Code Generation. In Proceedings of the
IEEE/ACM 46th International Conference on Software Engineering (ICSE). IEEE,
Lisbon, Portugal, 1‚Äì13.
[26] Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta,
Shin Yoo, and Jie M. Zhang. 2023. Large Language Models for Software Engineer-
ing: Survey and Open Problems. In Proceedings of the IEEE/ACM International
Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE).
IEEE, Melbourne, Australia, 31‚Äì53.
[27] Zhiyu Fan, Xiang Gao, Martin Mirchev, Abhik Roychoudhury, and Shin Hwei Tan.
2023. Automated Repair of Programs from Large Language Models. In Proceedings
of the IEEE/ACM 45th International Conference on Software Engineering (ICSE).
IEEE, Melbourne, Australia, 1469‚Äì1481.
[28] Ling Fu, Zhebin Kuang, Jiajun Song, Mingxin Huang, Biao Yang, Yuzhe Li, Linghao
Zhu, Qidi Luo, Xinyu Wang, Hao Lu, et al. 2024. OCRBench V2: An Improved
Benchmark for Evaluating Large Multimodal Models on Visual Text Localization
and Reasoning. arXiv:2501.00321 [cs.CV] arXiv preprint.
[29] Cuiyun Gao, Xing Hu, Shan Gao, Xin Xia, and Zhi Jin. 2025. The current chal-
lenges of software engineering in the era of large language models. ACM Trans-
actions on Software Engineering and Methodology 34, 5 (2025), 1‚Äì30.
[30] Google Gemini Team. 2025. Gemini 2.5: Pushing the Frontier with Advanced
Reasoning, Multimodality, Long Context, and Next Generation Agentic Capa-
bilities. https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_
report.pdf. Accessed: 2025-07-13.
[31] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin
Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025.
DeepSeek-
R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.
arXiv:2501.12948 [cs.CL] arXiv preprint.
[32] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora,
Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. 2021.
Measuring Coding Challenge Competence with APPS. arXiv:2105.09938 [cs.SE]
arXiv preprint.
[33] Abram Hindle, Earl T Barr, Mark Gabel, Zhendong Su, and Premkumar Devanbu.
2016. On the naturalness of software. Commun. ACM 59, 5 (2016), 122‚Äì131.
[34] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu
Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. 2024. Qwen2.5-Coder Technical
Report. arXiv:2409.12186 [cs.SE] arXiv preprint.
[35] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang,
Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. 2024. Live-
CodeBench: Holistic and Contamination-Free Evaluation of Large Language
Models for Code. arXiv:2403.07974 [cs.SE] arXiv preprint.
[36] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir
Press, and Karthik R. Narasimhan. 2024. SWE-Bench: Can Language Models
Resolve Real-World GitHub Issues?. In Proceedings of the Twelfth International
Conference on Learning Representations (ICLR). OpenReview.net, Vienna, Austria,
12 pages. https://openreview.net/forum?id=VTF8yNQM66
[37] Inception Labs, Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma,
Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha,
et al. 2025.
Mercury: Ultra-Fast Language Models Based on Diffusion.
arXiv:2506.17298 [cs.CL] arXiv preprint.
[38] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R√©mi
Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022.
Competition-level code generation with alphacode. Science 378, 6624 (2022),
1092‚Äì1097.
[39] Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tom√°≈° Koƒçisk`y, Andrew
Senior, Fumin Wang, and Phil Blunsom. 2016. Latent Predictor Networks for
Code Generation. arXiv:1603.06744 [cs.CL] arXiv preprint.
[40] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Cheng-
gang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. DeepSeek-V3
Technical Report. arXiv:2412.19437 [cs.CL] arXiv preprint.
[41] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. Is your
code generated by chatgpt really correct? rigorous evaluation of large language
models for code generation. Advances in Neural Information Processing Systems
36 (2023), 21558‚Äì21572.
[42] Jiawei Liu, Songrun Xie, Junhao Wang, Yuxiang Wei, Yifeng Ding, and Ling-
ming Zhang. 2024. Evaluating Language Models for Efficient Code Generation.
arXiv:2408.06450 [cs.SE] arXiv preprint.
[43] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel
Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxi-
ang Wei, et al. 2024.
StarCoder 2 and The Stack v2: The Next Generation.
arXiv:2402.19173 [cs.SE] arXiv preprint.
[44] Tom Minka, Ryan Cleven, and Yordan Zaykov. 2018. TrueSkill 2: An Improved
Bayesian Skill Rating System. Technical Report. Microsoft Research. Technical
report.
[45] OpenAI. 2024. GPT-4.1 Model Documentation. https://platform.openai.com/
docs/models/gpt-4.1. Accessed: 2025-07-13.


--- Page 13 ---
ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas
ICSE ‚Äô26, April 12‚Äì18, 2026, Rio de Janeiro, Brazil
[46] OpenAI. 2025. Codex Mini Model Documentation. https://platform.openai.com/
docs/models/codex-mini-latest. Accessed: 2025-07-13.
[47] OpenAI. 2025.
Introducing O3 and O4 Mini.
https://openai.com/index/
introducing-o3-and-o4-mini/. Accessed: 2025-07-13.
[48] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU:
A Method for Automatic Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting of the Association for Computational Linguistics (ACL).
Association for Computational Linguistics, Philadelphia, Pennsylvania, USA,
311‚Äì318.
[49] Wenjun Peng, Jing Zhe Lim, Qinghao Liu, and Xinyu Wang. 2025. Can Large
Language Models Play to Win? Game-Theoretic Benchmarks in Poker for Proba-
bilistic Reasoning Evaluation. In Proceedings of the 22nd Pacific Rim International
Conference on Artificial Intelligence (PRICAI). Springer, Wellington, New Zealand,
13 pages.
[50] Yanyuan Qiao, Haodong Hong, Wenqi Lyu, Dong An, Siqi Zhang, Yutong Xie,
Xinyu Wang, and Qi Wu. 2025. NavBench: Probing Multimodal Large Language
Models for Embodied Navigation. arXiv:2506.01031 [cs.AI] arXiv preprint.
[51] Abhinav Rastogi, Albert Q. Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample,
Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi
Chandu, et al. 2025. Magistral. arXiv:2506.10910 [cs.CL] arXiv preprint.
[52] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundare-
san, Ming Zhou, Ambrosio Blanco, and Shuai Ma. 2020. CodeBLEU: A Method
for Automatic Evaluation of Code Synthesis. arXiv:2009.10297 [cs.SE] arXiv
preprint.
[53] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiao-
qing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. 2023.
Code Llama: Open Foundation Models for Code. arXiv:2308.12950 [cs.CL] arXiv
preprint.
[54] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha√´l Mathieu, An-
drew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds,
Petko Georgiev, et al. 2019. Grandmaster level in StarCraft II using multi-agent
reinforcement learning. nature 575, 7782 (2019), 350‚Äì354.
[55] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu,
Linxi Fan, and Anima Anandkumar. 2023. Voyager: An Open-Ended Embodied
Agent with Large Language Models. arXiv:2305.16291 [cs.AI] arXiv preprint.
[56] Jue Wang, Yingnong Dang, Hongyu Zhang, Kai Chen, Tao Xie, and Dongmei
Zhang. 2013. Mining Succinct and High-Coverage API Usage Patterns from
Source Code. In Proceedings of the 10th Working Conference on Mining Software
Repositories (MSR). IEEE, San Francisco, CA, USA, 319‚Äì328.
[57] Xinyu Wang, Yuliang Liu, Chunhua Shen, Chun Chet Ng, Canjie Luo, Lianwen
Jin, Chee Seng Chan, Anton van den Hengel, and Liangwei Wang. 2020. On the
General Value of Evidence, and Bilingual Scene-Text Visual Question Answer-
ing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR). IEEE, Seattle, Washington, USA, 10126‚Äì10135.
[58] Xinyu Wang, Bohan Zhuang, and Qi Wu. 2024. ModaVerse: Efficiently Trans-
forming Modalities with LLMs. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR). IEEE, Seattle, Washington, USA,
26606‚Äì26616.
[59] Xinyu Wang, Bohan Zhuang, and Qi Wu. 2025. Are Large Vision Language
Models Good Game Players?. In Proceedings of the International Conference on
Learning Representations (ICLR). OpenReview.net, Singapore, 12 pages.
[60] Zihan Wang, Siyao Liu, Yang Sun, Hongyan Li, and Kai Shen. 2025. Code-
Contests+: High-Quality Test Case Generation for Competitive Programming.
arXiv:2506.05817 [cs.SE] arXiv preprint.
[61] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng,
Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 Technical
Report. arXiv:2505.09388 [cs.CL] arXiv preprint.
[62] Pengcheng Yin and Graham Neubig. 2017. A Syntactic Neural Model for General-
Purpose Code Generation. arXiv:1704.01696 [cs.CL] arXiv preprint.
[63] Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang,
Ying Li, Qianxiang Wang, and Tao Xie. 2024. CodeEval: A Benchmark of Prag-
matic Code Generation with Generative Pre-trained Models. In Proceedings of
the IEEE/ACM 46th International Conference on Software Engineering (ICSE). IEEE,
Lisbon, Portugal, 1‚Äì12.
[64] Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. 2024. CodeAgent: Enhancing
Code Generation with Tool-Integrated Agent Systems for Real-World Repo-Level
Coding Challenges. arXiv:2401.07339 [cs.SE] arXiv preprint.
[65] Hao Zhong, Tao Xie, Lu Zhang, Jian Pei, and Hong Mei. 2009. MAPO: Mining
and Recommending API Usage Patterns. In Proceedings of the 23rd European
Conference on Object-Oriented Programming (ECOOP). Springer, Genoa, Italy,
318‚Äì343.
[66] Xin Zhou, Martin Weyssow, Ratnadira Widyasari, Ting Zhang, Junda He, Yunbo
Lyu, Jianming Chang, Beiqi Zhang, Dan Huang, and David Lo. 2025. LessLeak-
Bench: A First Investigation of Data Leakage in LLMs across 83 Software Engi-
neering Benchmarks. arXiv:2502.06215 [cs.SE] arXiv preprint.
[67] Albert Ziegler, Eirini Kalliamvakou, X Alice Li, Andrew Rice, Devon Rifkin,
Shawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2024. Measuring
github copilot‚Äôs impact on productivity. Commun. ACM 67, 3 (2024), 54‚Äì63.
