--- Page 1 ---
IEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, OCTOBER 2024
1
Automatic programming via large language models
with population self-evolution for dynamic job shop
scheduling problem
Jin Huang, Xinyu Li, Member, IEEE, Liang Gao, Senior Member, IEEE, Qihao Liu, Yue Teng
Abstract—Heuristic dispatching rules (HDRs) are widely re-
garded as effective methods for solving dynamic job shop schedul-
ing problems (DJSSP) in real-world production environments.
However, their performance is highly scenario-dependent, often
requiring expert customization. To address this, genetic program-
ming (GP) and gene expression programming (GEP) have been
extensively used for automatic algorithm design. Nevertheless,
these approaches often face challenges due to high randomness
in the search process and limited generalization ability, hindering
the application of trained dispatching rules to new scenarios or
dynamic environments. Recently, the integration of large lan-
guage models (LLMs) with evolutionary algorithms has opened
new avenues for prompt engineering and automatic algorithm
design. To enhance the capabilities of LLMs in automatic HDRs
design, this paper proposes a novel population self-evolutionary
(SeEvo) method, a general search framework inspired by the
self-reflective design strategies of human experts. The SeEvo
method accelerates the search process and enhances exploration
capabilities. Experimental results show that the proposed SeEvo
method outperforms GP, GEP, end-to-end deep reinforcement
learning methods, and more than 10 common HDRs from the
literature, particularly in unseen and dynamic scenarios.
Index Terms—dynamic job shop scheduling problems (DJSSP),
large language models (LLMs), automatic heuristic dispatching
rules design, self-evolutionary (SeEvo)
I. INTRODUCTION
T
HE core challenge of production scheduling lies in the
efficient allocation of limited resources, such as machin-
ery, to ensure the completion of tasks within the planning
horizon while optimizing predefined performance metrics [1],
[2]. The job shop scheduling problem (JSSP), recognized as
an NP-hard optimization problem, is typically addressed using
traditional exact algorithms like dynamic programming [3] and
branch-and-bound [4]. However, these methods are unsuitable
for large-scale cases [5], [6]. Consequently, researchers have
turned to metaheuristic algorithms to achieve near-optimal
solutions for large-scale static cases within acceptable com-
putational time [7], [8]. In dynamic job shop scheduling prob-
lems (DJSSP), real-time changes such as randomly arriving
orders and machine breakdowns create additional challenges.
Heuristic dispatching rules (HDRs) have proven effective in
such dynamic environments due to their low computational
complexity and ability to respond quickly to changes [9], [10].
Manuscript received October 26, 2024. This work is supported in part by the
National Natural Science Foundation of China under Grant 52188102.(Corre-
sponding author: Xinyu Li.)
Jin Huang, Xinyu Li, Liang Gao, Qihao Liu, and Yue Teng are with the State
Key Laboratory of Intelligent Manufacturing Equipment and Technology,
Huazhong University of Science and Technology, Wuhan 430074.
Efficiently designing HDRs is crucial for improving man-
ufacturing system performance. Despite the development of
various heuristic methods, most require extensive expert
knowledge and fine-tuning. To overcome this, the automated
generation of HDRs has become a research focus, with genetic
programming (GP) [11] and gene expression programming
(GEP) [5] being two widely used approaches for automatic
algorithm design.
GP simulates natural selection and genetic mechanisms to
automatically generate and evolve complex HDRs. However,
the traditional tree-based representation of GP can lead to
an overly complex search space and increased computational
costs [12]. To address these limitations, GEP is proposed
as an alternative, using fixed-length linear chromosomes that
simplify genetic operations and improve the manageability of
evolved solutions [1].
Despite the success of GP and GEP in automatic algorithm
design, both methods face limitations, particularly in their
generalization performance when applied to unseen DJSSP
cases, where deep reinforcement learning (DRL) often per-
forms better [13]. Moreover, both methods lack the capability
for effective self-guided exploration, relying heavily on ran-
dom searches, which limits the algorithms’ exploration and
exploitation efficiency.
However, although DRL-based methods demonstrate no-
table generalization capabilities, their scheduling performance
still falls short of optimal. In some cases, these methods pro-
vide no substantial improvement over HDRs [14], [15]. This
limitation has led researchers to explore integrated approaches,
incorporating GP-based action spaces within DRL frameworks
[16], as well as end-to-end DRL strategies grounded in graph
neural networks [17], [18]. These innovations offer a par-
tial improvement in exploration and exploitation efficiency,
enhancing DRL’s adaptability and effectiveness in dynamic
scheduling environments.
Recently, the rise of large language models (LLMs) has
introduced new possibilities for integrating evolutionary algo-
rithms with automatic algorithm design [19]. Through prompt
engineering and iterative feedback, LLMs generate highly
adaptive and targeted heuristic rules by leveraging vast do-
main knowledge and pattern recognition [20]. Studies such as
applying LLMs in online bin packing problems, published in
Nature [21], highlight the potential of this approach. However,
the application of LLMs to more complex problems, such as
DJSSP, remains relatively unexplored. Although frameworks
like ReEvo have successfully applied in the traveling salesman
arXiv:2410.22657v1  [cs.NE]  30 Oct 2024


--- Page 2 ---
IEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, OCTOBER 2024
2
problem (TSP), the dynamic and variable nature of DJSSP
introduces extra challenges that require more specialized and
flexible approaches.
Therefore, this paper proposes a novel LLM-based evolu-
tionary framework for automatic algorithm design in DJSSP,
introducing an innovative population self-evolutionary (SeEvo)
method. This method leverages LLMs as hyper-heuristic gen-
erators and employs self-evolution within a population to
automatically design and optimize HDRs. Specifically, the
contributions of this paper are as follows:
1) Novel
LLM-Based
Evolutionary
Framework
for
DJSSP: This paper introduces a novel LLM-based evo-
lutionary framework that addresses the generalization
limitations and stochasticity inherent in the automatic
design of algorithms for DJSSP. The framework leverages
LLMs to generate adaptive heuristic rules, enhancing the
automatic algorithm design.
2) Population Self-Evolution Strategy: A novel population
self-evolution strategy is proposed within the LLM-based
framework, significantly improving the exploration and
exploitation capabilities of the generated heuristics. This
strategy allows for continuous refinement of HDRs based
on real-time feedback during the scheduling process, en-
hancing scheduling efficiency in dynamic environments.
3) Comprehensive Evaluation and Superiority of the
SeEvo Method: The proposed SeEvo method’s effec-
tiveness and superiority are demonstrated through ex-
tensive comparisons with commonly used HDRs, GP,
GEP, and end-to-end DRL methods. Experimental results
demonstrate the SeEvo method’s superior generalization
across unseen and dynamic DJSSP, outperforming other
dynamic scheduling methods.
The remainder of this paper is organized as follows. Section
II provides the problem formulation of DJSSP and reviews
relevant literature on job shop scheduling methods and auto-
matic algorithm design with large language models. Section
III introduces the proposed language-heuristic-based DJSSP
framework, followed by a detailed explanation of the popula-
tion self-evolution method in Section IV. Experimental setup
and performance evaluation are presented in Section V. Finally,
Section VI offers conclusions for this paper.
II. BACKGROUND
A. Mathematical model of Job Shop Scheduling
The DJSSP is an extension of the classical JSP to accommo-
date dynamic changes, such as random job arrivals, machine
breakdowns, and varying processing times [22]. This study
specifically focuses on the dynamic event of random job order
arrivals. The objective is to develop a scheduling policy that
minimizes the makespan Cmax, while simultaneously adapting
to real-time fluctuations in the system [23], [24].
Problem Definition:
• K = {1, 2, ..., m}: A set of m machines.
• I = {1, 2, ..., n}: A set of n jobs, where each job i ∈I.
• Kij ⊆K: The set of possible machines on which the
operation Oij can be processed.
• pik: The processing time of job Oi on machine k.
• tarri: The arrival time of job i.
Decision Variables:
• xik: Start time of job i on machine k.
• zii′k ∈{0, 1}: Binary decision variable. zii′k = 1 if job
i is processed before job i′ on machine k; otherwise,
zii′k = 0.
• Cmax: The makespan, defined as the maximum comple-
tion time among all jobs.
Objective Function:
Minimize Cmax
(1)
Constraints:
xiKij ≥xiKij−1 + piKij−1, ∀j ∈{2, 3, ..., qi}, i ∈I
(2)
xi′k ≥xik + pik −Mzii′k, ∀i, i′ ∈I, i < i′, k ∈K
(3)
xik ≥xi′k +pi′k −M(1−zii′k), ∀i, i′ ∈I, i < i′, k ∈K (4)
Cmax ≥xiKim + piKim, ∀i ∈I
(5)
xik ≥tarri, ∀i ∈I, k ∈K
(6)
zii′k ∈{0, 1}, ∀i, i′ ∈I, k ∈K
(7)
where Constraint (1) defines the objective function, which
aims to minimize the maximum completion time. Constraint
(2) ensures that the operations of the same job are processed
in the predefined sequence. Constraints (3) and (4) guarantee
that at any given time, a machine can only process one
operation. Constraint (5) calculates the maximum completion
time. Constraints (6) and (7) define the range of the two
decision variables, with the initial arrival time for all jobs set
to tarri = 0.
B. Related Works of Job Shop Scheduling
Over the past few decades, numerous approaches have
been proposed to address the JSSP. Exact algorithms, such
as dynamic programming [3] and branch-and-bound [4], can
find optimal solutions but are limited to smaller cases due
to their computational complexity [4], [25]. To overcome
these limitations, meta-heuristic methods, including genetic
algorithm [26], [27], particle swarm optimization algorithm
[28], [29], and memetic algorithm [30], have been widely
employed to provide near-optimal solutions for larger-scale
problems. However, these methods encounter challenges in
dynamic job shop environments where problem conditions
change frequently, often failing to generate high-quality so-
lutions within reasonable computational times.
Heuristic dispatching rules (HDRs), which prioritize jobs
or machines based on simple rules, are widely adopted in dy-
namic environments due to their efficiency and responsiveness
to real-time changes [31]. While effective, the performance of
these heuristic methods is highly scenario-dependent, often re-
quiring expert customization. Among these, GP [32] and GEP
[5], as types of hyper-heuristic methods, have shown particular
promise in DJSSP scenarios. They can generate scheduling
heuristics without domain-specific knowledge, providing an
automated and adaptive means of solving complex scheduling
problems [1], [33], [11]. However, one of the major challenges


--- Page 3 ---
IEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, OCTOBER 2024
3
for these hyper-heuristic algorithms is the expansive feature
space in DJSSP, which can enlarge the search space and limit
exploration efficiency [34]. Moreover, heuristics generated by
these methods may struggle to generalize well to unseen
DJSSP cases, making it challenging to achieve high-quality
dynamic scheduling solutions.
In recent years, DRL has been extensively explored as an
alternative to address the generalization limitations of GP and
GEP in DJSSP scenarios [35]. DRL-based methods allow
agents to interact with the job shop environment, learning poli-
cies that maximize long-term rewards [36]. These methods can
generate scheduling actions at decision points based on real-
time state information and uncertainty, allowing for dynamic
adjustments as new tasks arise. Trained policies can gener-
alize across cases of varying sizes, enabling scalability [15].
However, although the scheduling policies in these DRL-based
methods are size-invariant, the performance of the scheduling
agents remains far from optimal, with some methods offering
no advantage over individual HDR [14], [15]. Therefore, re-
search has integrated GP-based action spaces into DRL frame-
works, yielding better results compared to standalone HDRs
[16]. Additionally, end-to-end DRL methods [17], [18], similar
to the language-heuristic approaches proposed in this work,
have been explored for directly selecting workpieces in DJSSP
environments. These methods leverage graph neural networks
to extract features and select workpieces in an end-to-end
manner, achieving promising results in static generalization
scenarios. Nonetheless, in dynamic cases, their performance
still often falls short compared to common PDRs[17], [18].
C. Automatic Algorithm Design with LLMs
The rise of LLMs has opened new avenues for automatic
algorithm design through their integration with evolutionary
algorithms. LLMs have demonstrated substantial capabilities
in tasks such as code generation [37], code optimization [38],
[39], solving algorithmic competition challenges [40], [41],
generated data [42], and robotics control [43]. These advance-
ments have extended to areas such as prompt optimization
[44], reinforcement learning reward design [45], and algorithm
self-improvement [46]. Specifically, the integration of LLMs
with evolutionary algorithms has shown great promise in
solving combinatorial optimization problems such as the TSP
[47], [46] and the online bin packing problem (BPP) [21].
While LLMs have proven effective in developing heuristic
algorithms for simpler combinatorial problems like TSP and
BPP, their application to more complex problems such as
DJSSP remains relatively unexplored. One notable advance-
ment is the ReEvo framework [20], a language-heuristic
approach that has achieved success in solving TSP by sim-
ulating the reflective processes of human experts. However,
the complexity of DJSSP surpasses that of TSP, as DJSSP
involves scheduling multiple jobs across various machines,
with dynamic processing times, making it significantly more
challenging. Although the ReEvo framework has demonstrated
excellent performance, particularly with its short-term (com-
parative learning of individual differences) and long-term re-
flection (summarization of short-term reflections) mechanisms,
it has not fully explored LLMs’ potential for self-evolution
during individual iterations. Additionally, ReEvo was designed
for static problems and lacked differentiation between train
sets and test sets.
Given DJSSP’s dynamic and variable nature, with frequent
changes in processing scenarios, this paper seeks to introduce a
novel language-heuristic framework that harnesses LLMs’ full
capabilities for automatic algorithm design. We aim to provide
more efficient and generalizable solutions for this problem by
leveraging LLMs’ ability to generate adaptable heuristics.
III. FRAMEWORK OF LANGUAGE-HEURISTIC-BASED
DJSSP
The proposed framework for DJSSP, as shown in Fig.1,
consists of two main phases: the self-evolution phase and the
online application phase. To handle the DJSSP with randomly
arriving orders, a job shop simulation environment is designed,
coupled with a language-heuristic-based SeEvo method that
evolves HDRs automatically.
Fig. 1.
Language-heuristic-based DJSSP framework.
As illustrated in Fig.1, during the self-evolution phase,
the DJSSP environment comprises a job pool and a set of
machines. Incoming jobs are first fed into the LLM, which
generates heuristic rules based on the statistical properties of
the dataset. These rules are used to prioritize and sequence
jobs, with only the top-ranked jobs available for immediate
scheduling. After completing each job, the machine becomes
free and selects the next job from the job pool. Throughout this
phase, the LLM collects extensive training data and iteratively
improves the HDRs to optimize the scheduling performance.
This phase primarily serves as a training period, during
which multiple cases are processed to refine the HDRs. In the
subsequent online application phase, the HDRs and prompts
obtained from training on the individual cases during the
self-evolution phase (20 cases in our experiments) are each
applied in practical scenarios. With well-designed HDRs and
prompts, the system can rapidly generate high-quality HDRs
after a single iteration of the framework. For instance, in tests
involving 50 jobs and 15 machines, the framework delivers a
high-quality solution within 30 seconds.


--- Page 4 ---
IEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, OCTOBER 2024
4
To address challenges associated with generating high-
quality HDRs during the self-evolution phase, this paper
proposes a novel SeEvo method that enhances exploration
through individual co-evolution reflection, individual self-
evolution reflection, and centralized evolution reflection strate-
gies. Detailed explanations of these mechanisms are provided
in the following subsections.
IV. LANGUAGE-HEURISTIC-BASED POPULATION
SELF-EVOLUTION METHOD
The overall structure of the language-heuristic-based popu-
lation SeEvo method is depicted in Fig.2. In this framework,
LLMs play two essential roles: generating guiding prompts
for the population and creating individual heuristic programs.
Unlike conventional hyper-heuristic approaches, SeEvo relies
on the independent generation of heuristic code segments,
which are continuously refined and optimized throughout
the evolution process. The key to SeEvo’s success lies in
leveraging LLMs to generate initial guiding prompts and craft
individual heuristic programs tailored to the scheduling tasks.
The detailed procedure of the SeEvo evolutionary process is
presented in Algorithm 1.
Algorithm 1: SeEvo Evolutionary Process
Input: Population P, Function Evaluations FE,
Maximum Function Evaluations maxFE,
Elitist e, Case Number N, Mutate Probability
Pm
Output: Best Code, Best Code Path
1 while FE < maxFE do
2
if all individuals are invalid then
3
Raise Error: “All individuals are invalid”;
4
end
5
Sp ←Select population from P;
6
if Sp is None then
7
Raise Error: “Selection Failed”;
8
end
9
SR ←Individual Co-Evolution Reflection(Sp);
10
Pinter ←Crossover(SR, Sp);
11
P ←Evaluate Population(Pinter, N);
12
e, Best Code, Best Code Path ←Update Iteration;
13
IR ←Individual Self-Evolution
Reflection(P, R, Sp);
14
Pself ←Crossover(IR, P);
15
P ←Evaluate Population(Pself, N);
16
e, Best Code, Best Code Path ←Update Iteration;
17
MR ←Collective Evolution Reflection(Pm, SR);
18
Pmut ←Mutate(Pm, MR);
19
P ←P∪Evaluate Population(Pmut, N);
20
e, Best Code, Best Code Path ←Update Iteration;
21
FE ←FE + 1;
22 end
23 Return Best Code, Best Code Path;
SeEvo proceeds through eight principal stages, each con-
tributing to refining the population of heuristics to improve
job selection efficacy. Below is an overview of these eight
key steps:
Individual Encoding: The SeEvo method continuously
evolves heuristic algorithms, but its encoding mechanism dif-
fers from traditional evolutionary algorithms. SeEvo individu-
als are heuristic code segments designed to guide job selection
in DJSSP rather than directly determining the final scheduling
plan. Furthermore, these individuals are generated by the LLM
without predefined constraints on encoding length or function
sets, which are common limitations in traditional algorithms
like GEP. The only requirement for the LLM-generated code
is to adhere to the specified function names, input parameters,
and output parameters.
Initialization: In the SeEvo method, population initializa-
tion is carried out using the LLM prompt generator, which
takes task specifications and seed heuristics as inputs. The task
specifications provide details about the JSSP, input parameters,
and heuristic functions. Seed heuristics (example heuristic
codes) serve as a foundation, guiding the LLM to generate
initial heuristic rules in more promising search directions.
Individual Co-Evolution Reflection: In this step, two
randomly selected heuristic strategies are compared, and their
performance is evaluated based on test cases. The results are
then sent to the reflector LLM, which analyzes the differences
and generates suggestions for improvement. SeEvo employs a
“language gradient” feedback mechanism to guide the LLM in
producing more effective code, with the reward signal being
binary (better or worse performance).
Individual Self-Evolution Reflection: For each individual,
SeEvo evaluates its performance before and after co-evolution
reflection and presents the results to the reflector LLM. The
LLM reflects on the changes and offers suggestions for
improvement. If performance worsens or remains stagnant,
the LLM generates reverse prompts to avoid repeating the
same issues. If performance improves, the LLM highlights
successful elements and provides enhanced prompts for further
optimization.
Collective Evolution Reflection: As SeEvo accumulates
experience through multiple iterations, the reflector LLM
synthesizes insights from individual self-evolution reflections
and individual co-evolution reflections. The goal is to generate
prompts that guide the further evolution of heuristic rules. The
collective evolution reflection might initially be empty or pre-
filled with predefined prompts, but it becomes richer as more
iterations are completed.
Crossover: During the crossover step, the LLM generates
a new set of heuristic strategies by combining task specifica-
tions, performance data of parent strategies, reflections, and
detailed generation instructions. Parent heuristic strategies are
selected from the population based on their relative perfor-
mance on test cases, and the LLM integrates insights from
mutual reflections to guide the crossover process.
Mutation: The SeEvo method uses an elite mutation strat-
egy, where the LLM generates new heuristic variants by focus-
ing on the best-performing individuals. The mutation prompts
include task specifications, elite heuristics, performance reflec-
tions, and instructions for generating new solutions.
Individual Evaluation: At both the crossover and muta-


--- Page 5 ---
IEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, OCTOBER 2024
5
Fig. 2.
The population self-evolution method.
tion stages, the effectiveness of each heuristic is rigorously
evaluated against the scheduling task. The LLM compares the
heuristics against a dataset of test cases, ensuring only the best-
performing strategies are preserved. Evaluation at multiple
stages helps ensure that each iteration of the evolutionary
process refines the population toward better solutions.
In summary, the SeEvo method offers a novel, language-
heuristic-based evolutionary approach to solving dynamic
scheduling problems, with continuous feedback loops guiding
the generation and refinement of heuristic solutions. By incor-
porating multiple reflection phases, crossover, and mutation,
SeEvo ensures an evolving population of heuristics that pro-
gressively improves job selection efficiency and performance.
V. EXPERIMENTAL EVALUATION
A. Experimental Setup
To validate the effectiveness of the proposed SeEvo method,
experiments are conducted under both static and dynamic
conditions. For the static experiments, public benchmark
datasets from Taillard (TA) and Demirkol (DMU) are used.
For the dynamic experiments, various randomly generated
DJSSP environments with randomly arriving job orders are
simulated. Two APIs, gpt-3.5-turbo-0125 and GLM-3-Turbo,
are employed to run the LLM models, considering the high
computational cost of gpt-4.0 and previous research findings
indicating limited performance improvement with gpt-4.0 [20].
Comparisons are made between SeEvo, GEP [48], multi-tree
genetic programming (MTGP) [49], more than ten common
HDRs, and three end-to-end DRL methods [17], [18], [50].
Additionally, an ablation study is performed to further validate
the robustness of the proposed method.
TABLE I
PARAMETERS OF SEEVO
Parameter
Value
LLM (generator and reflector)
gpt-3.5 and GLM-3
LLM temperature (generator and reflector)
1
Population size
20
Maximum number of evaluations
20
Mutation rate
0.5
We use Table I parameters for all SeEvo runs. The SeEvo
method is configured with a crossover probability of 1, a
mutation probability of 0.5, a population size of 20, and a
maximum of 20 generations for training under both static and
dynamic conditions. For training, 20 randomly generated cases
are used, with one training case replaced after each iteration.
The same crossover and mutation probabilities are maintained
for testing, but the number of generations is reduced to one
for test cases.
In contrast to DRL approaches, the MTGP and GEP meth-
ods are more tailored to perform well on specific cases and rely
more heavily on training data [13]. However, using test cases
directly as training data would create an unfair comparison
with other methods. To address this, following the approach
of [13], 80 training cases are introduced for the self-generated
algorithm, with each round consisting of 20 generations. After
each round, the training cases are replaced. The population
size for GP and GEP is set to 20, matching that of SeEvo,
and four randomly selected cases are used per round, with


--- Page 6 ---
IEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, OCTOBER 2024
6
TABLE II
EXPERIMENTAL RESULTS ON DMU BENCHMARK, WHERE THE “UB” COLUMN INDICATES THE BEST-KNOWN SOLUTION
Cases
Size
Random
LPT
SPT
STPT
MPSR
DRL-Liu
GP
GEP
SeEvo(GLM3)
SeEvo(GPT3.5)
UB
DMU03 20 × 15
3827
4592
3630
4232
3435
3303
3540
3651
3462
3238
2731
DMU04 20 × 15
3889
4047
3541
4642
3355
3321
3406
3499
3235
3212
2669
DMU08 20 × 20
4228
4551
4714
4459
3999
4098
3802
4023
3728
3728
3188
DMU09 20 × 20
4094
4511
4283
4690
3869
3753
4196
4136
3857
3828
3092
DMU13 30 × 15
5451
5580
4813
5207
4759
4708
4765
4812
4658
4709
3681
DMU14 30 × 15
5306
5591
4583
4811
4238
4124
4289
4213
3980
3980
3394
DMU18 30 × 20
5326
5810
6231
5480
5003
4800
4696
4917
4724
4724
3844
DMU19 30 × 20
5174
5787
5126
5203
4930
4837
4666
5245
4715
4816
3768
DMU23 40 × 15
5948
7045
6250
6521
5383
5240
5391
5595
5151
5258
4668
DMU24 40 × 15
6078
6484
5503
6595
5358
5319
5560
5458
5226
5316
4648
DMU28 40 × 20
6737
7322
6558
7697
5927
5948
6017
6142
5838
5944
4692
DMU29 40 × 20
6602
7386
6565
7690
6107
5824
6236
6224
5941
5825
4691
DMU33 50 × 15
6890
8779
7361
7631
6282
6458
6109
6081
6029
6029
5728
DMU34 50 × 15
7523
7991
7026
7740
6359
6284
6327
6279
6148
6146
5385
DMU38 50 × 20
7685
9051
7954
8555
7604
7275
7267
7501
7168
7170
5713
DMU39 50 × 20
8097
8514
7592
8908
6953
6776
6941
7124
6693
6590
5747
Mean
5803.44
6440.06
5733.13
6253.81 5222.56
5129
5200.50
5306.25
5034.56
5032.06
4227.44
parameters aligned with the existing literature [48], [49]. All
methods are implemented in Python and executed on a server
with an Intel(R) Xeon(R) W-3365 CPU @ 2.70GHz running
Ubuntu 20.04.
B. Generalization
Performance
on
Public
Benchmark
Datasets
In this section, the generalization performance of SeEvo
is first evaluated using randomly generated training cases,
ranging in size from 20 to 100 jobs and 10 to 20 machines,
with processing times for each job randomly selected between
50 and 100 units. For the test sets, 32 benchmark cases from
the TA and DMU datasets are selected, covering 8 different
problem sizes. The TA test set sizes range from 15 × 15 to
100 × 20, while the DMU test set sizes range from 20 × 15
to 50 × 20.
Performance on the DMU Benchmark: This section
compares five well-known HDRs, including Random selection,
longest processing time (LPT), shortest processing time (SPT),
shortest total processing time (STPT), and most process se-
quence remaining (MPSR), along with MTGP, GEP, DRL [18],
and SeEvo methods. The results, shown in Table II, use data
for the heuristic scheduling rules and DRL-Liu sourced from
the literature [18]. The proposed SeEvo method outperforms
the other approaches in 12 of 16 test cases. In the remaining 4
cases, SeEvo ranks second, slightly behind the best-performing
method. These results demonstrate that SeEvo can effectively
schedule and generalize across JSSP cases of different sizes,
showcasing strong generalization capabilities. Additionally,
GLM-3 and GPT-3.5 each show strengths and weaknesses
across different cases, with minimal overall performance dif-
ferences between the two models.
Performance on the TA Benchmark: In addition to the
DMU benchmark experiments, the performance of SeEvo is
also evaluated on the TA benchmark datasets. Comparisons
include three recent DRL-based approaches (DRL-Chen [17],
DRL-Zhang [50], DRL-Liu[18]) and three heuristic dispatch-
ing rules: longest processing time for subsequent operations
(LSO), shortest processing time×minimum total working time
(SPT×TWK), and shortest processing time/minimum total
working time remaining (SPT/TWKR). MTGP and GEP are
also included for comparison. The results, presented in Table
III, source heuristic scheduling rule data from [17] and DRL
data from [18]. As shown in Table III, SeEvo outperforms
other approaches in 14 of the 16 test cases, ranking second in
the remaining 2 cases. These results emphasize the superior
optimization capabilities of SeEvo across a variety of schedul-
ing scenarios.
Notably, across both DMU and TA benchmark datasets,
SeEvo demonstrates consistent and superior performance in
terms of optimization results, indicating that the method is


--- Page 7 ---
IEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, OCTOBER 2024
7
TABLE III
EXPERIMENTAL RESULTS ON TA BENCHMARK, WHERE THE “UB” COLUMN INDICATES THE BEST-KNOWN SOLUTION
Cases
Size
LSO
SPT/TWKR
DRL-Chen
DRL-Zhang
DRL-Liu
GP
GEP
SeEvo(GLM3)
SeEvo(GPT3.5)
UB
TA01
15 × 15
1957
1664
1711
1433
1492
1547
1547
1427
1427
1231
TA02
15 × 15
1759
1538
1639
1544
1425
1565
1486
1465
1437
1244
TA11
20 × 15
2216
1886
1833
1794
1752
1749
1819
1656
1692
1357
TA12
20 × 15
2187
1969
1765
1805
1692
1789
1732
1637
1616
1367
TA21
20 × 20
2647
2206
2145
2252
2097
2090
2089
1977
2007
1642
TA22
20 × 20
2522
2111
2015
2102
1924
2059
1981
1915
1973
1600
TA31
30 × 15
2478
2435
2382
2565
2277
2307
2279
2192
2184
1764
TA32
30 × 15
2634
2512
2458
2388
2203
2400
2396
2279
2274
1784
TA41
30 × 20
2873
2898
2541
2667
2698
2851
2729
2543
2496
2005
TA42
30 × 20
3096
2813
2762
2664
2623
2603
2613
2363
2454
1937
TA51
50 × 15
3844
3768
3762
3599
3608
3603
3668
3364
3412
2760
TA52
50 × 15
3715
3588
3511
3341
3524
3346
3324
3286
3245
2756
TA61
50 × 20
4188
3752
3633
3654
3548
3685
3642
3529
3537
2868
TA62
50 × 20
4217
3925
3712
3722
3557
3636
3723
3446
3474
2869
TA71
100 × 20
6754
6705
6321
6452
6289
6305
6278
6071
6099
5464
TA72
100 × 20
6674
6351
6232
5695
6002
5776
5625
5604
5575
5181
Mean
3360.06
3132.56
3026.38
2979.81
2919.44
2956.94
2933.19
2797.13
2806.38
2364.31
Fig. 3.
Gap Ratio of Different Scheduling Methods in Dynamic Cases.


--- Page 8 ---
IEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, OCTOBER 2024
8
capable of generating high-quality scheduling decisions for
various JSSPs. This is true even when problem sizes and
processing times differ significantly from the training cases,
underscoring the strong generalization ability of SeEvo.
Additionally, the results further validate the LLM’s ability
to understand and generate high-quality heuristic rules. The
strong performance of the SeEvo method is attributed to the
effective prompt engineering and domain knowledge gained
from 20 training sessions, allowing the model to generate high-
quality scheduling decisions efficiently. For instance, on a test
set with 50 jobs and 15 machines, GPT-3.5 produces a high-
quality solution in just 19 seconds, while GLM-3 requires only
29 seconds to generate a similarly high-quality result.
C. Performance Evaluation in Dynamic Cases with Randomly
Arriving Orders
In this section, the performance of the SeEvo method is
evaluated in a dynamic environment characterized by the
uncertainty of randomly arriving job orders. To simulate this,
20 training cases are generated, with order sizes in each batch
ranging between 20 and 50 jobs across 2 to 3 batches. The ar-
rival times for each batch follow a uniform distribution within
the intervals [1, 500] and [501, 1000]. The test environment is
configured with 10 machines, and the job processing times are
random integers between 50 and 100 units. For the test set, 100
cases are generated using the same method as the training set.
The results are displayed in Fig.3, which compares SeEvo with
9 common HDRs, including SPT, TWKR, shortest remaining
machining time (SRM), shortest subsequent operation (SSO),
LPT, LPT/TWK, SPT×TWK, SPT+SSO, and SPT/LSO, as
well as MTGP and GEP. Fig.3 illustrates the gap ratio between
each method and the optimal result across 13 methods for each
case.
As shown in Fig.3, SeEvo consistently demonstrates clear
advantages under dynamic conditions, with significant perfor-
mance improvements in most cases. The proposed method
exhibits minimal relative gaps, with the maximum gap ratio
not exceeding 30% and, in most cases, staying below 10%.
These results indicate that the SeEvo method is effective
in achieving promising scheduling outcomes across various
DJSSP cases, even in dynamic environments. Furthermore,
SeEvo has proven highly capable of handling unseen dynamic
environments and uncertainties, which closely resemble real-
world production shop conditions.
We attribute the promising performance of the proposed
method to several factors. First, the language-heuristic frame-
work is designed to integrate rich domain knowledge, enabling
the generation of highly generalized HDRs and prompts. These
HDRs are derived from training sets of 20 randomly sized
order batches, enhancing the model’s ability to generalize
across new scenarios. As a result, the method can generate
effective decisions with just a single iteration. Second, the
SeEvo method enhances exploration and exploitation capa-
bilities through individual co-evolution reflection, individual
self-evolution reflection, and collective evolution reflection,
ensuring the high quality of the generated HDRs.
D. Ablation Study
The core innovation of the proposed SeEvo method is the
introduction of individual self-evolution. To assess the impact
of this feature, an ablation study is conducted by removing the
individual self-evolution process, with the modified method
referred to as ReEvo. The exploration and exploitation capa-
bilities of both SeEvo and ReEvo are evaluated by iterating
50 times on static training sets, followed by testing the
generalization performance of the methods.
First, training is performed on the datasets from Table II
and Table III, with the convergence curves of these itera-
tions displayed in Fig.4. The x-axis represents the number
of validation iterations, and the y-axis shows the makespan
(the average makespan for each dataset). As shown in Fig.5,
ReEvo, which lacks individual self-evolution, significantly
underperforms compared to SeEvo, even exhibiting worse
performance than GP on the DMU dataset. This highlights that
incorporating individual self-evolution enhances the method’s
exploration capabilities.
Additionally, the generalization performance of ReEvo on
the TA test set is examined, with two different problem sizes:
30 jobs with 20 machines and 50 jobs with 20 machines. Each
problem size includes 10 cases, and the optimization goal
is to minimize the makespan. The results, shown in Table
IV, indicate that SeEvo (GLM-3) significantly outperforms
both ReEvo and two end-to-end DRL methods [18]. This
confirms that individual self-evolution is crucial in enhancing
the method’s ability to explore and exploit solution spaces.
Overall, SeEvo (GLM-3) achieves better average results
than gpt-3.5, primarily due to the clearer heuristic strategies
generated by GLM-3-Turbo. Fig.5 illustrates the HDRs gen-
erated by the two API models, with the lighter top half rep-
resenting HDRs produced by GLM-3, and the darker section
representing those generated by gpt-3.5.
VI. DISCUSSION
To address the challenges of poor generalization and re-
liance on random search in automatic algorithm design for the
DJSSP, this paper proposes a dynamic evolutionary framework
leveraging LLMs. Specifically, the innovative SeEvo method
is introduced and thoroughly validated through both static and
dynamic case studies. The experimental results demonstrate
that the proposed SeEvo method significantly outperforms
commonly used HDRs, MTGP, GEP, and end-to-end DRL
methods in terms of static generalization performance. Ad-
ditionally, in dynamic experiments, SeEvo exhibits substantial
advantages in most cases. These findings confirm the capabil-
ity of LLMs to generate highly targeted and adaptable HDRs,
benefiting from their robust language understanding, genera-
tion capabilities, and extensive domain knowledge acquired
through training on diverse datasets.
Despite these promising results, this study has certain lim-
itations. The LLMs are used via scheduling APIs for training
and validation without direct fine-tuning for specific DJSSP
knowledge. Moreover, only a single HDR is employed in
the dynamic cases, which limits the method’s performance,
preventing it from achieving the absolute superiority observed


--- Page 9 ---
IEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, OCTOBER 2024
9
Fig. 4.
Convergence Curves of Different Methods on Two Training Case Groups.
TABLE IV
ABLATION STUDY RESULTS
Instance
DRL-GAT
DRL-GCN
SeEvo(GLM3)
SeEvo(GPT3.5)
ReEvo(GLM3)
ReEvo(GPT3.5)
30x20
2628
2740
2445
2461
2452
2456
50x20
3547
3703
3352
3367
3372
3387
Fig. 5.
HDRs generated by Two API Models on TA70 Case.
in the static cases. Future research will focus on two key areas:
(1) improving the generalization performance in static cases by
integrating vector databases and employing LLM fine-tuning
techniques to enhance the model’s scheduling knowledge,
and (2) enhancing adaptability to dynamic environments by
utilizing a combination of multiple HDRs to further optimize
performance.
REFERENCES
[1] S. Shady, T. Kaihara, N. Fujii, and D. Kokuryo, “Feature selection
approach for evolving reactive scheduling policies for dynamic job shop
scheduling problem using gene expression programming,” International
Journal of Production Research, vol. 61, no. 15, pp. 5029–5052, 2023.
[2] J. Branke, S. Nguyen, C. W. Pickardt, and M. Zhang, “Automated design
of production scheduling heuristics: A review,” IEEE Transactions on
Evolutionary Computation, vol. 20, no. 1, pp. 110–124, 2015.
[3] J. A. Gromicho, J. J. Van Hoorn, F. Saldanha-da Gama, and G. T. Tim-
mer, “Solving the job-shop scheduling problem optimally by dynamic
programming,” Computers & Operations Research, vol. 39, no. 12, pp.
2968–2977, 2012.
[4] P. Brucker, B. Jurisch, and B. Sievers, “A branch and bound algorithm
for the job-shop scheduling problem,” Discrete Applied Mathematics,
vol. 49, no. 1, pp. 107–127, 1994.
[5] L. Zhang, Z. Li, G. Kr´olczyk, D. Wu, and Q. Tang, “Mathematical
modeling and multi-attribute rule mining for energy efficient job-shop
scheduling,” Journal of Cleaner Production, vol. 241, p. 118289, 2019.
[6] A. Baykaso˘glu, A. Hamzadayi, and S. Y. K¨ose, “Testing the performance
of teaching–learning based optimization (tlbo) algorithm on combinato-
rial problems: Flow shop and job shop scheduling cases,” Information
Sciences, vol. 276, pp. 204–218, 2014.
[7] J. Xie, X. Li, L. Gao, and L. Gui, “A new neighbourhood structure
for job shop scheduling problems,” International Journal of Production
Research, vol. 61, no. 7, pp. 2147–2161, 2023.


--- Page 10 ---
IEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, OCTOBER 2024
10
[8] L. Wang and D.-Z. Zheng, “An effective hybrid optimization strategy
for job-shop scheduling problems,” Computers & Operations Research,
vol. 28, no. 6, pp. 585–596, 2001.
[9] O. Holthaus and C. Rajendran, “Efficient dispatching rules for schedul-
ing in a job shop,” International Journal of Production Economics,
vol. 48, no. 1, pp. 87–105, 1997.
[10] S. Rahal, D. J. Papageorgiou, and Z. Li, “Hybrid strategies using
linear and piecewise-linear decision rules for multistage adaptive linear
optimization,” European Journal of Operational Research, vol. 290,
no. 3, pp. 1014–1030, 2021.
[11] F. Zhang, Y. Mei, S. Nguyen, and M. Zhang, “Survey on Genetic
Programming and Machine Learning Techniques for Heuristic Design in
Job Shop Scheduling,” IEEE Transactions on Evolutionary Computation,
vol. 28, no. 1, pp. 147–167, 2024.
[12] S. Luke and L. Panait, “A comparison of bloat control methods for
genetic programming,” Evolutionary Computation, vol. 14, no. 3, pp.
309–344, 2006.
[13] M. Xu, Y. Mei, F. Zhang, and M. Zhang, “Genetic programming and
reinforcement learning on learning heuristics for dynamic scheduling: A
preliminary comparison,” IEEE Computational Intelligence Magazine,
vol. 19, no. 2, pp. 18–33, 2024.
[14] J. Park, J. Chun, S. H. Kim, Y. Kim, and J. Park, “Learning to schedule
job-shop problems: Representation and policy learning using graph
neural network and reinforcement learning,” International Journal of
Production Research, vol. 59, no. 11, pp. 3360–3377, 2021.
[15] P. Tassel, M. Gebser, and K. Schekotihin, “A reinforcement learning
environment for job-shop scheduling,” arXiv preprint arXiv:2104.03760,
2021.
[16] Y. Li, W. Gu, M. Yuan, and Y. Tang, “Real-time data-driven dynamic
scheduling for flexible job shop with insufficient transportation resources
using hybrid deep Q network,” Robotics and Computer-Integrated Man-
ufacturing, vol. 74, p. 102283, 2022.
[17] R. Chen, W. Li, and H. Yang, “A deep reinforcement learning framework
based on an attention mechanism and disjunctive graph embedding
for the job-shop scheduling problem,” IEEE Transactions on Industrial
Informatics, vol. 19, no. 2, pp. 1322–1331, 2023.
[18] C.-L. Liu, C.-J. Tseng, and P.-H. Weng, “Dynamic job-shop scheduling
via graph attention networks and deep reinforcement learning,” IEEE
Transactions on Industrial Informatics, vol. 20, no. 6, pp. 8662–8672,
2024.
[19] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi,
C. Wang, Y. Wang, et al., “A survey on evaluation of large language
models,” ACM Transactions on Intelligent Systems and Technology,
vol. 15, no. 3, pp. 1–45, 2024.
[20] H. Ye, J. Wang, Z. Cao, and G. Song, “Reevo: Large language
models as hyper-heuristics with reflective evolution,” arXiv preprint
arXiv:2402.01145, 2024.
[21] B. Romera-Paredes, M. Barekatain, A. Novikov, M. Balog, M. P. Kumar,
E. Dupont, F. J. R. Ruiz, J. S. Ellenberg, P. Wang, O. Fawzi, P. Kohli,
and A. Fawzi, “Mathematical discoveries from program search with large
language models,” Nature, vol. 625, no. 7995, pp. 468–475, 2024.
[22] J. Mohan, K. Lanka, and A. N. Rao, “A review of dynamic job shop
scheduling techniques,” Procedia Manufacturing, vol. 30, pp. 34–39,
2019.
[23] C. ¨Ozg¨uven, L. ¨Ozbakır, and Y. Yavuz, “Mathematical models for job-
shop scheduling problems with routing and process plan flexibility,”
Applied Mathematical Modelling, vol. 34, no. 6, pp. 1539–1548, 2010.
[24] Y.-J. Yao, Q.-H. Liu, X.-Y. Li, and L. Gao, “A novel milp model for job
shop scheduling problem with mobile robots,” Robotics and Computer-
Integrated Manufacturing, vol. 81, p. 102506, 2023.
[25] J. A. S. Gromicho, J. J. van Hoorn, F. Saldanha-da-Gama, and G. T. Tim-
mer, “Solving the job-shop scheduling problem optimally by dynamic
programming,” Computers & Operations Research, vol. 39, no. 12, pp.
2968–2977, 2012.
[26] J. Xie, X. Li, L. Gao, and L. Gui, “A hybrid algorithm with a new
neighborhood structure for job shop scheduling problems,” Computers
& Industrial Engineering, vol. 169, p. 108205, 2022.
[27] N. Kundakcı and O. Kulak, “Hybrid genetic algorithms for minimizing
makespan in dynamic job shop scheduling problem,” Computers &
Industrial Engineering, vol. 96, pp. 31–51, 2016.
[28] L. Gao, X. Li, X. Wen, C. Lu, and F. Wen, “A hybrid algorithm based on
a new neighborhood structure evaluation method for job shop scheduling
problem,” Computers & Industrial Engineering, vol. 88, pp. 417–429,
2015.
[29] Z. Wang, J. Zhang, and S. Yang, “An improved particle swarm optimiza-
tion algorithm for dynamic job shop scheduling problems with random
job arrivals,” Swarm and Evolutionary Computation, vol. 51, p. 100594,
2019.
[30] L. Gao, G. Zhang, L. Zhang, and X. Li, “An efficient memetic algorithm
for solving the job shop scheduling problem,” Computers & Industrial
Engineering, vol. 60, no. 4, pp. 699–705, 2011.
[31] M. DHurasevi´c and D. Jakobovi´c, “A survey of dispatching rules for
the dynamic unrelated machines environment,” Expert Systems with
Applications, vol. 113, pp. 555–569, 2018.
[32] S. Nguyen, Y. Mei, B. Xue, and M. Zhang, “A hybrid genetic program-
ming algorithm for automated design of dispatching rules,” Evolutionary
Computation, vol. 27, no. 3, pp. 467–496, 2019.
[33] F. Zhang, Y. Mei, S. Nguyen, K. C. Tan, and M. Zhang, “Instance-
rotation-based surrogate in genetic programming with brood recombi-
nation for dynamic job-shop scheduling,” IEEE Transactions on Evolu-
tionary Computation, vol. 27, no. 5, pp. 1192–1206, 2023.
[34] Y. Mei, S. Nguyen, B. Xue, and M. Zhang, “An efficient feature selection
algorithm for evolving job shop scheduling rules with genetic pro-
gramming,” IEEE Transactions on Emerging Topics in Computational
Intelligence, vol. 1, no. 5, pp. 339–353, 2017.
[35] B. M. Kayhan and G. Yildiz, “Reinforcement learning applications
to machine scheduling problems: A comprehensive literature review,”
Journal of Intelligent Manufacturing, vol. 34, no. 3, pp. 905–929, 2023.
[36] X. Wu, X. Yan, D. Guan, and M. Wei, “A deep reinforcement learning
model for dynamic job-shop scheduling problem with uncertain process-
ing time,” Engineering Applications of Artificial Intelligence, vol. 131,
p. 107790, 2024.
[37] X. Chen, M. Lin, N. Sch¨arli, and D. Zhou, “Teaching large language
models to self-debug,” arXiv preprint arXiv:2304.05128, 2023.
[38] A. Shypula, A. Madaan, Y. Zeng, U. Alon, J. Gardner, M. Hashemi,
G. Neubig, P. Ranganathan, O. Bastani, and A. Yazdanbakhsh, “Learning
performance-improving code edits,” arXiv preprint arXiv:2302.07867,
2023.
[39] J.-B. Mouret, “Large language models help computer programs to
evolve,” Nature, vol. 625, no. 7995, pp. 452–453, 2024.
[40] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,
T. Eccles, J. Keeling, F. Gimeno, A. D. Lago, T. Hubert, P. Choy,
C. d. M. d’Autume, I. Babuschkin, X. Chen, P.-S. Huang, J. Welbl,
S. Gowal, A. Cherepanov, J. Molloy, D. J. Mankowitz, E. S. Robson,
P. Kohli, N. de Freitas, K. Kavukcuoglu, and O. Vinyals, “Competition-
level code generation with AlphaCode,” Science, 2022.
[41] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, “Reflex-
ion: Language agents with verbal reinforcement learning,” Advances in
Neural Information Processing Systems, vol. 36, pp. 8634–8652, 2023.
[42] I. Shumailov, Z. Shumaylov, Y. Zhao, N. Papernot, R. Anderson, and
Y. Gal, “Ai models collapse when trained on recursively generated data,”
Nature, vol. 631, no. 8022, pp. 755–759, 2024.
[43] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,
and A. Zeng, “Code as policies: Language model programs for embodied
control,” in 2023 IEEE International Conference on Robotics and
Automation (ICRA).
London, United Kingdom: IEEE, 2023, pp. 9493–
9500.
[44] Q. Guo, R. Wang, J. Guo, B. Li, K. Song, X. Tan, G. Liu, J. Bian,
and Y. Yang, “Connecting large language models with evolution-
ary algorithms yields powerful prompt optimizers,” arXiv preprint
arXiv:2309.08532, 2023.
[45] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Ja-
yaraman, Y. Zhu, L. Fan, and A. Anandkumar, “Eureka: Human-
level reward design via coding large language models,” arXiv preprint
arXiv:2310.12931, 2023.
[46] F. Liu, X. Tong, M. Yuan, and Q. Zhang, “Algorithm evolution using
large language model,” arXiv preprint arXiv:2311.15249, 2023.
[47] F. Liu, X. Tong, M. Yuan, X. Lin, F. Luo, Z. Wang, Z. Lu, and Q. Zhang,
“An example of evolutionary computation + large language model
beating human: Design of efficient guided local search,” arXiv.org, 2024.
[48] L. Nie, L. Gao, P. Li, and X. Shao, “Reactive scheduling in a job
shop where jobs arrive over time,” Computers & Industrial Engineering,
vol. 66, no. 2, pp. 389–405, 2013.
[49] F. Zhang, Y. Mei, and M. Zhang, “Genetic programming with multi-tree
representation for dynamic flexible job shop scheduling,” in AI 2018:
Advances in Artificial Intelligence, T. Mitrovic, B. Xue, and X. Li, Eds.
Cham: Springer International Publishing, 2018, pp. 472–484.
[50] C. Zhang, W. Song, Z. Cao, J. Zhang, P. S. Tan, and X. Chi, “Learning
to dispatch for job shop scheduling via deep reinforcement learning,” in
Advances in Neural Information Processing Systems, vol. 33.
Curran
Associates, Inc., 2020, pp. 1621–1632.
