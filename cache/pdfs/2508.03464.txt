--- Page 1 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
1
Learning to Incentivize: LLM-Empowered
Contract for AIGC Offloading in Teleoperation
Zijun Zhan, Yaxian Dong, Daniel Mawunyo Doe, Yuqing Hu, Shuai Li, Shaohua Cao, and Zhu Han,
Abstract‚ÄîWith the rapid growth in demand for AI-generated content (AIGC), edge AIGC service providers (ASPs) have become
indispensable. However, designing incentive mechanisms that motivate ASPs to deliver high-quality AIGC services remains a
challenge, especially in the presence of information asymmetry. In this paper, we address bonus design between a teleoperator and an
edge ASP when the teleoperator cannot observe the ASP‚Äôs private settings and chosen actions (diffusion steps). We formulate this as
an online learning contract design problem and decompose it into two subproblems: ASP‚Äôs settings inference and contract derivation.
To tackle the NP-hard setting-inference subproblem with unknown variable sizes, we introduce a large language model
(LLM)-empowered framework that iteratively refines a naive seed solver using the LLM‚Äôs domain expertise. Upon obtaining the solution
from the LLM-evolved solver, we directly address the contract derivation problem using convex optimization techniques and obtain a
near-optimal contract. Simulation results on our Unity-based teleoperation platform show that our method boosts the teleoperator‚Äôs
utility by 5 ‚àº40% compared to benchmarks, while preserving positive incentives for the ASP. The code is available at
https://github.com/Zijun0819/llm4contract.
Index Terms‚ÄîTeleoperation, large language model (LLM), contract theory, AI-generated content offloading, incentive mechanism
‚ú¶
1
INTRODUCTION
T
HE remarkable performance of AI-generated content
(AIGC) services is driving their market growth expo-
nentially. As per the report in [1], the AIGC market will
increase from $1.6B in 2023 to $18.7B in 2030, with an annual
growth rate of 25.39%. To underpin this substantial market,
the AIGC services are gradually burgeoning in the domains
of education [2], healthcare [3], and teleoperation [4], etc.
In this paper, we will dig into a specific case of applying
AIGC services in teleoperation. Concretely, to ensure seam-
less teleoperation in dark areas, such as tunnels, drainage
systems, and nighttime construction sites, a generative dif-
fusion model can restore normal-light images from low-light
conditions [4], [5].
The latency requirement for teleoperation ranges from
100 to 300 ms [6]. AIGC services are generally computa-
tionally intensive, and on-site devices may not have suffi-
cient computational resources. To this end, many scholars
proposed the edge AIGC framework [7], [8], [9], in which
the AIGC service provider (ASP) supplies AIGC services in
‚Ä¢
Zijun Zhan is with the Department of Electrical and Computer Engi-
neering, University of Houston, 4800 Calhoun Rd, Houston, TX 77004.
E-mail: zzhan@uh.edu
‚Ä¢
Yaxian Dong and Yuqing Hu are with the Department of Architectural
Engineering, The Pennsylvania State University, University Park, PA
16802, USA (E-mail: yzd5221@psu.edu and yfh5204@psu.edu)
‚Ä¢
Daniel Mawunyo Doe is with the Department of Electrical and Computer
Engineering, Prairie View A&M University, 100 University Dr, Prairie
View, TX 77446. Email: dmdoe@pvamu.edu
‚Ä¢
Shuai Li is with the Department of Civil & Coastal Engineering, Univer-
sity of Florida, Gainesville, FL 32611. E-mail: shuai.li@ufl.edu
‚Ä¢
Shaohua Cao is with the Qingdao Institute of Software, College of
Computer Science and Technology, China University of Petroleum (East
China), Qingdao 266580, China. E-mail:shaohuacao@upc.edu.cn
‚Ä¢
Zhu Han is with the Department of Electrical and Computer Engineering,
University of Houston, 4800 Calhoun Rd, Houston, TX 77004, and also
with the Department of Computer Science and Engineering, Kyung Hee
University, Seoul, South Korea, 446-701. E-mail: hanzhu22@gmail.com
the edge server. The edge server is not only proximal to
end users but also has ample computational resources [10],
[11]. The introduction of ASP can help reduce the latency of
AIGC-empowered teleoperation. Moreover, a well-designed
incentive mechanism in edge AIGC networks can enable
sustainable and high-quality AIGC service provision [12].
Numerous scholars proposed various incentive mech-
anisms for edge AIGC services [4], [5], [12], [13], [14],
[15]. We categorize them into ASP-centric and user-centric
(teleoperator-centric) types. For the ASP-centric incentive
mechanism, its primary objective is to maximize the utility
of the ASP while ensuring the rationality of user incentives.
In contrast, the user-centric incentive mechanism aims to
maximize user utility while ensuring the rationality of the
ASP‚Äôs incentives. In this paper, we focus on designing
user-centric incentive mechanisms for edge AIGC networks,
so as to ensure user engagement. It is worth noting that
information asymmetry may exist during the design of
incentive mechanisms in edge AIGC networks [4], [5], [12].
For instance, users have limited information regarding the
available computational resources of the ASP.
A mature tool that can tackle the information asymmetry
within the incentive mechanism design is contract theory
[16]. Specifically, from the perspective of teleoperators, we
can formulate a series of contract bundles, (Latency, Re-
ward), with properties of incentive compatibility and in-
centive rationality. The ASP will select the contract bundle
according to its available computational resources, thereby
maximizing its utility. The teleoperator rewards the ASP
only when it delivers the AIGC service within the required
latency. We can formulate an effective incentive mechanism
via contract theory in terms of the AIGC service latency.
However, if we shift the lens to AIGC service quality, it may
be a challenge for us to formulate an incentive mechanism.
The hidden information in the latency case is the type
arXiv:2508.03464v1  [cs.CE]  5 Aug 2025


--- Page 2 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
2
0 . 9 0 - 1 . 0 5
1 . 0 5 - 1 . 2 0
1 . 2 0 - 1 . 3 5
1 . 3 5 - 1 . 5 0
1 . 5 0 - 1 . 6 5
1 . 6 5 - 1 . 8 0
0 . 0
0 . 1
0 . 2
0 . 3
0 . 4
0 . 5
0 . 6
0 . 7
P r o b a b i l i t y  m a s s
A I G C  s e r v i c e  q u a l i t y
 D i f f u s i o n  s t e p s  =  4  
 D i f f u s i o n  s t e p s  =  8
 D i f f u s i o n  s t e p s  =  6  
 D i f f u s i o n  s t e p s  =  1 0
Fig. 1: PMF of the AIGC services quality under varying
diffusion steps, in which the AIGC service quality is the
sum of LPIPS [17] and SSIM [18] of processed AIGC results.
of ASPs; in contrast, the hidden information in the AIGC
service quality case is the actions of ASPs. Concretely, as
the results depicted in Fig. 1, when the ASP undertakes
different diffusion steps (actions), the distribution of the
AIGC services quality varies. In previous works [19], [20],
[21], they modeled this type of contract theory problem as
a moral hazard problem. However, they assume the teleop-
erator (principal) knows the ASP‚Äôs (agent) setting (i.e., the
mapping of diffusion steps to the AIGC service quality dis-
tribution and the cost of varying diffusion steps) in advance.
Without this unrealistic assumption, we need to model the
incentive mechanism design for AIGC service quality as
an online learning contract theory problem, which is APX-
hard [22], [23], [24], [25], [26]. Therefore, we formulate the
research question in this paper as follows: How can we design
an incentive mechanism that elicits high-quality AIGC services,
thereby maximizing the teleoperator‚Äôs utility under the hidden
action of the edge ASP?
The essence of the research question is an online learning
contract design problem, but the off-the-shelf solution rarely
exists. The majority of works [22], [23], [24], [25] focus on the
theoretical level, aiming to prove the lower regret bound
for the online learning contract design problem. A recent
work [26] proposes a deep learning‚Äìbased solution for the
general online learning contract theory problem; however, it
requires thousands of training samples. In other words, we
can address the research question only after thousands of
contract interactions have occurred between the teleoperator
and the edge ASP. To this end, we proposed an LLM-
empowered solution to effectively solve the research ques-
tion, inspired by recent works that utilize LLMs to design
extraordinary heuristics for solving NP-hard problems [27],
[28], [29].
We present our proposed LLM-empowered online learn-
ing contract theory in Fig. 2, which includes four steps.
Firstly, we will randomly generate contracts and observe the
interaction results between the teleoperator and the edge
ASP. After 100 rounds, we can obtain a series of interac-
tion tuples (Contract, Teleoperator utility, ASP response).
Subsequently, upon the historical interaction tuples and the
optimization problem structure, we will design a naive seed
solver to infer the edge ASP‚Äôs setting. Next, we harness the
power of LLM to iteratively refine the seed solver with the
aid of multiple LLM agents. Lastly, we select the solver with
the highest evaluation score during the self-evolution to
infer the optimal edge ASP setting. Based on the inferred
edge ASP setting, we derive a near-optimal contract by
using convex optimization techniques to solve a traditional
moral hazard problem (P3 in Section 3.4). In summary, we
conclude the contributions of this paper as follows:
1. We propose a framework of LLM-empowered incentive
mechanism design under information asymmetry. With
our proposed framework, the teleoperator can derive a
near-optimal bonus (contract) under the hidden action
of edge ASP, which can elicit the edge ASP to take
higher diffusion steps and provide high-quality AIGC
services. In this framework, we utilize the LLM to refine
a solver for inferring the edge ASP‚Äôs setting.
2. We extend the traditional moral hazard problem by
eliminating the assumption that the principal (teleoper-
ator) knows the agent‚Äôs (edge ASP) setting in advance.
Concretely, we model the moral hazard problem as
an online learning contract design problem, in which
the teleoperator needs to learn edge ASP‚Äôs settings via
historical interaction logs prior to deriving the contract.
3. We propose utilizing the LLM to address the online
learning contract design problem. Due to the cou-
pled variables in the online learning contract design
problem, we decompose it into two subproblems: the
edge ASP setting inference problem and the contract
derivation problem. The edge ASP‚Äôs setting inferring
problem is APX-hard; we leverage the LLM to evolve a
solver for it. Upon acquiring the solution from the LLM-
evolved solver, we directly solve the contract derivation
problem using convex optimization techniques.
4. We conduct numerous experiments to demonstrate the
scalability, sensitivity, and effectiveness of our proposed
LLM-empowered framework. The experimental results
demonstrate that our proposed method can augment
the teleoperator utility by around 5 ‚àº40% in compar-
ison with benchmarks under almost all experimental
configurations. Moreover, our proposed method en-
sures a positive incentive to the edge ASP.
We organize the rest of this paper as follows. Section 2
provides a systematic literature review. Section 3 presents
the system model and formulates the optimization problem.
In Section 4, we illustrate the details of how we leverage
the LLM to derive the near-optimal contract, along with the
analysis of algorithms. Section 5 systematically evaluates the
LLM-empowered solution in terms of scalability, effective-
ness, and sensitivity. Finally, Section 6 concludes the paper.
2
BACKGROUND AND RELATED WORKS
2.1
Background of Online Learning Contract Theory
Contract theory examines the design of incentive mecha-
nisms between two parties: a principal, who offers a con-
tract, and an agent, who selects actions in response to
the provided incentives. Contract design problems typically
arise under conditions of information asymmetry, wherein


--- Page 3 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
3
Randomly
Teleoperator
ASP
Interactions
Contract
Interaction logs between the teleoperator and ASP
 (Teleoperator utility)
. . Interaction logs constraints
     Optimization variable constraints
max
T
s t
œÄ
Œ¶ÔÄ•
Optimization problem structure
Inferred ASP settings ‡∑©ùöΩùöΩvia seed solver
Generator LLM
Evaluator LLM
Evolved 
solver
Evaluation 
results
Suggestions
for 
improvement
Inferred optimal ASP settings ‡∑©ùöΩùöΩ‚àó
Solve a nonlinear 
convex problem P3
Obtain the 
optimal contract
1
2
3
4
Seed solver 0
ÔÅì
Best solver
*
ÔÅì
Fig. 2: Framework of LLM-empowered online learning contract theory for deriving the near-optimal bonus (contract)
regarding the AIGC services quality.
the agent possesses private information that is not acces-
sible to the principal [30], [31], [32]. Two canonical forms
of information asymmetry extensively studied in contract
theory are adverse selection and moral hazard [33].
Adverse selection arises when the agent possesses pri-
vate information about their inherent characteristics or
‚Äùtype‚Äù prior to entering into a contract [34], [35]. Such char-
acteristics may include skill level, reliability, cost structure,
or risk profile‚Äîfactors that are not directly observable by
the principal [33], [36], [37]. Because the principal cannot
distinguish between high-quality and low-quality agents
ex ante, contracts intended for high-quality agents may
inadvertently attract low-quality agents, resulting in subop-
timal outcomes. To mitigate adverse selection, the principal
typically designs a menu of contracts (i.e., a screening
mechanism) that encourages each agent type to self-select
the contract best aligned with their private information. This
approach ensures incentive compatibility and can partially
reveal agent types through their contract choices [38].
Moral hazard arises after a contract is signed, when
the agent takes actions that influence the outcome but
these actions are not fully observable or contractible by
the principal [39]. This scenario presents a challenge, as
the agent may act in their own interest rather than that
of the principal, particularly when such actions are costly
or effort-intensive. To address moral hazard, contracts are
typically designed to align the agent‚Äôs incentives with those
of the principal, often by linking rewards or penalties to
observable outcomes that correlate with the agent‚Äôs effort
[33]. Traditional approaches to the moral hazard problem
assume that the principal knows the functional relationship
between the agent‚Äôs actions and the resulting outcomes,
as well as the agent‚Äôs utility function‚Äîincluding the cost
associated with different actions [19], [20], [21], [39], [40],
[41], [42], [43], [44].
Online learning contract theory extends traditional
moral hazard models by considering more practical scenar-
ios in which the principal initially lacks complete knowl-
edge of the agent‚Äôs private parameters [22], [45]. These
private parameters include both the mapping between the
agent‚Äôs actions and the outcomes observed by the principal,
as well as the agent‚Äôs cost of undertaking different actions.
In this context, the principal must iteratively learn and
infer these hidden parameters through repeated interactions
before effectively addressing the moral hazard problem.
The seminal work by Ho et al. [22] introduced the concept
of online learning contract theory by modeling the moral
hazard problem as a repeated principal‚Äìagent interaction
in crowdsourcing markets. Specifically, the principal offers
a contract to a randomly arriving, strategic worker, who
selects an effort level (unobservable to the principal) that
determines both output quality and personal cost. The prin-
cipal observes only the output, and updates the contract in
each round of interaction to maximize long-term utility.
2.2
Related Works
Given the research question in this paper, we will review the
recent literature from three perspectives. In Section 2.2.1, we
review recent works on utilizing contract theory to design
incentive mechanisms in edge AIGC networks. Next, we
revisit recent works that utilize contract theory to address
hidden actions during the design of incentive mechanisms
in Section 2.2.2. Finally, we present recent works regarding
online learning contract design in Section 2.2.3.
2.2.1
Contract Theory in Edge AIGC Networks
With the maturity of AIGC services, the authors in [49]
proposed an edge AIGC services offloading framework,
enabling mobile users to access AIGC services seamlessly.


--- Page 4 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
4
TABLE 1: Comparison Between Our Work and Previous Works.
Ref.
AIGC networks
Adverse selection
Moral hazard
Agent settings
Sample efficiency
[4], [5], [12], [13], [14], [15]
‚úì
‚úì
√ó
N/A
N/A
[19], [20], [40]
√ó
‚úì
‚úì
‚úì
N/A
[21]
‚úì
√ó
‚úì
‚úì
N/A
[41], [42], [43], [44]
√ó
√ó
‚úì
‚úì
N/A
[22], [23], [24], [26], [46], [47], [48]
√ó
√ó
‚úì
√ó
√ó
Our work
‚úì
√ó
‚úì
√ó
‚úì
‚àóN/A indicates not applicable, and the column of Agent settings means whether the principal can access the agent‚Äôs private settings.
To maintain sustainable and high-quality AIGC services,
several works [4], [5], [12], [13], [14], [15] have recognized
the necessity of an incentive mechanism in the edge AIGC
services offloading framework. Meanwhile, in these works,
they also observed that information asymmetry prevails in
incentive mechanism design for edge AIGC networks. Con-
sequently, they proposed different incentive mechanisms for
varying edge AIGC networks by utilizing contract theory.
The authors in [4] observed that edge servers cannot
observe the difficulty distributions of AIGC tasks in ad-
vance. Therefore, they fused a vision‚Äìlanguage model with
contract theory to classify AIGC task difficulty on the fly and
priced a differentiated AIGC task offloading. The authors in
[13] developed an Age of Information (AoI)-driven contract
that rewards UAVs for uploading fresh data to support
AIGC model training. They maximized base-station utility
while curing hidden-cost asymmetry stemming from UAV-
private sensing costs and update frequencies. Similarly, the
authors in [14] proposed a Proximal Policy Optimization
(PPO)-based contract theory to derive the optimal contract,
thereby addressing the issue that the data collection cost of
mobile devices is hidden information to ASPs.
Given that ASPs‚Äô AIGC model complexity is hidden
from users, and users may possess risk-biased valuations,
the authors in [12] combined contract theory with prospect
theory to generate user-centric contracts. The authors in
[15] introduced a contract theory-based two-stage incentive
mechanism for AIGC task allocation. In this work, the users‚Äô
subjective gain per unit of AIGC service quality is hidden
from the ASP. The authors in [5] devised a Wasserstein dis-
tributionally robust optimization (DRO) contract to derive
robust latency-reward contract bundles for teleoperators.
The design addresses the information asymmetry that tele-
operators cannot observe the available capacity of ASPs, as
well as ensuring that the fluctuation of AIGC service quality
will not affect the teleoperators‚Äô utility.
These studies examined the integration of contract the-
ory into edge AIGC networks from various perspectives,
laying a solid foundation for future research. However,
these works focus on the design of incentive mechanisms
under hidden-type information asymmetry. They did not
consider the contract theory-based incentive mechanism de-
sign from the perspective of hidden actions. Our work will
investigate how to design an effective incentive mechanism
to address the hidden action in edge AIGC networks.
2.2.2
Hidden Action Contract Design
The previous works that apply contract theory to design
incentive mechanisms in edge AIGC networks focus on the
hidden type aspect, i.e., the adverse selection problem. In
this section, we will review several recent works that focus
on addressing the moral hazard problem, thereby deriving
incentive mechanisms under hidden action.
The authors in [19] considered that the resource devoted
by a validator for transaction verification in a blockchain
system is a hidden action to the blockchain beacon chain. To
this end, they proposed a bonus mechanism based on the
framework of contract theory to incentivize validators to de-
vote more resources to transaction verification. Analogously,
the authors in [20] designed a bonus mechanism to motivate
blockchain users to exert more resources in maintaining the
full node of the blockchain. The authors in [21] observed that
ASPs might intentionally reduce the resources they invest
(such as computation power) in generating AI outputs,
thereby representing a moral hazard. The authors deployed
contract theory to optimize the payment schemes between
clients and ASPs.
The authors in [40] modeled federated learning (FL)
training as a moral hazard problem, as the training effort
exerted by FL clients was unobservable to the FL server. In
[41], contract theory was applied to address moral hazard
in multi-hop cooperative communications, where the coop-
erative user‚Äôs willingness to contribute energy for content
transmission constituted the hidden action. The work in
[42] introduced a multi-dimensional moral hazard model
for mobile crowdsourcing systems, where mobile workers‚Äô
efforts spanned multiple metrics such as accuracy, time-
liness, and coverage; the authors utilized contract theory
to derive the optimal incentive scheme. In [43], the moral
hazard problem was formulated around the hidden effort of
edge caching nodes in disseminating short videos, which
remained unobservable to service providers. Finally, the
authors in [44] considered a scenario in which virtual ser-
vice providers could reduce service quality after receiving
compensation, due to unobservable post-contract actions.
Similar to [21], we also consider the moral hazard prob-
lem that exists in edge AIGC networks. As depicted in
Fig. 1, the diffusion step of the generative diffusion model
undertaken by ASPs is hidden from teleoperators, and the
action of ASPs will impact the utility of teleoperators. Di-
verging from the previous work [19], [20], [21], [40], [41],
[42], [43], [44], we eliminate the unrealistic assumption that
the teleoperator knows the edge ASP‚Äôs setting.
2.2.3
Online Learning Contract Design
When we do not know the edge ASPs‚Äô settings‚Äîthe map-
ping of diffusion steps to the AIGC service quality distri-
bution and the cost of actions‚Äîthe hidden action contract
design will shift to online learning contract design [45]. In
this way, we need to iteratively learn ASP‚Äôs settings before
addressing the moral hazard problem and then derive the
optimal incentive mechanism (contract). Subsequently, we


--- Page 5 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
5
will review several representative works in this emerging
field.
The authors in [22] first proposed the online learning
contract design problem and modeled it as a multi-armed
bandit problem. Analogously, the authors in [46] modeled
the problem as a continuum-armed bandit problem and
used the upper confidence bound to solve it. The authors
in [47] also modeled a continuous contract space online
learning contract design problem. However, they discretized
the contract space into a finite but rich menu of candidate
contracts and utilized tools from multi-armed bandit theory
to derive the optimal contract. The authors in [48] extended
the work of [46] by focusing on the setting of agents with
small action space and proposed a Discover-and-Cover al-
gorithm to derive the near-optimal contract. The authors in
[23] assumed the principal (teleoperator) has moment-based
information of the agent (ASP) settings, and proposed to
use robust optimization to derive the optimal contract. The
authors in [24] considered a case where the agent‚Äôs (ASP)
action space is small, and proposed a discover-and-cover al-
gorithm to find the optimal contract iteratively. The authors
in [26] observed that the principal utility is discontinuous
with respect to the contract. Therefore, they proposed a
discontinuous neural network to model the principal utility
function and utilized gradient ascent to find the optimal
contract.
The aforementioned works are pioneering work in on-
line learning contract design. However, one primary issue
is that the previous works require substantial contract inter-
action logs between ASPs and teleoperators before deriving
the optimal contract. In this paper, we unleash the power of
LLM to design an effective online learning contract solver,
thereby deriving a near-optimal incentive mechanism with
minimal interaction logs. To the best of our knowledge,
this is the first work to utilize LLM in designing incentive
mechanisms under information asymmetry. Additionally,
we summarized the comparison between our work and
current works in Table 1 for clarity.
3
SYSTEM MODEL AND PROBLEM FORMULATION
In this section, we commence with the introduction of the
background and key notations in Section 3.1. Next, we
present the utility model of the edge ASP and teleoperator
in Sections 3.2 and 3.3. Lastly, we formulate the optimization
problem in Section 3.4.
3.1
System Model and Notations
In this paper, we consider a scenario where a teleopera-
tor company needs to purchase generative diffusion-based
AIGC services from an edge ASP to achieve seamless teleop-
eration working in low-light areas. During the provision of
AIGC services, latency and quality are two crucial metrics.
Previous works [5], [12] have leveraged contract theory to
formulate contracts in a bundle of (Latency, Reward) to
motivate the ASP to meet the latency requirement. Anal-
ogously, we can use contract theory to formulate a bonus
incentive mechanism [19] in the form of (Quality, Bonus),
thereby incentivizing ASP to take larger diffusion steps for
high-quality AIGC services provision. However, the un-
known edge ASPs‚Äô setting Œ¶ = (P, c) hinders the derivation
of an optimal bonus incentive mechanism (contract) for
the teleoperator. Here, P is the mapping between diffusion
steps and the AIGC service quality distribution. c is the
additional cost of the edge ASP when it takes different
actions.
We model the contract derivation as an online learning
contract design problem. By referring to [26], we define the
problem with elements C = (D, R, O, A, Œ¶, q). Since Œ¶ is
unobservable to the teleoperator, we need to leverage the
historical contract interaction logs D between the teleopera-
tor and the edge ASP to infer edge ASP‚Äôs setting ÀúŒ¶ = (ÀúP, Àúc).
Here, we define D = {(rk, œÄT (rk), I(rk))|k ‚àà[K]}, in which
[K] = {1, ¬∑ ¬∑ ¬∑ , k, ¬∑ ¬∑ ¬∑ , K}. Regarding the k-th interaction
log, the teleoperator commences with randomly generate a
contract r = {rm|m ‚àà[M]} ‚ààR ‚äÇRM
‚â•0 over the finite
and discrete outcome space O = {om|m ‚àà[M]}. Next,
the edge ASP selects a diffusion step (action) an from the
discrete and finite action space A as per the contract r, in
which A = {an|n ‚àà[N]}. Action an leads to an AIGC
service quality distribution pn = p(¬∑|an) over O and incurs
an additional processing cost cn ‚ààR‚â•0 to the edge ASP.
Subsequently, the teleoperator perceives an outcome om and
obtains the utility of œÄT (r). Lastly, in light of the teleoperator
cannot observe an, we utilize I(r) = {‚àí1, 1} to indicate
whether the edge ASP rejects or accepts the contract r. For
clarity, we consider P ‚ààRN√óM and c ‚ààRN.
In this paper, we consider each outcome om to be an
AIGC service quality range, defined as om = [lm, um).
We use the median value of om, i.e., Œ¥m = (lm + um)/2,
to calculate the valuation of om for the teleoperator. The
valuation function of om is defined as q : O ‚ÜíR‚â•0. After
calculation, we utilize q = {qm|m ‚àà[M]} to represent the
utility of teleoperator over O.
3.2
Utility Model of Edge ASP
We assume the utility of the edge ASP stems from two parts:
the utility of the contract and the utility of the AIGC services
subscription. Upon this, we define the utility function of the
edge ASP as:
œÄA(ai; r, Œ¶) = Eom‚àºp(¬∑|an)[rm] ‚àícn
|
{z
}
‚àÜb
+ rs ‚àíct
| {z }
‚àÜs
.
(1)
Here, ‚àÜb indicates the utility of the contract. rs is the
subscription fee paid by the teleoperator for accessing the
AIGC services, ct is the training cost of the edge ASP for
training the AIGC model, and ‚àÜs is the utility of the AIGC
services subscription. In this paper, we consider a utility
model designed per image. It is worth noting that the AIGC
service subscription will bundle a default diffusion step a1,
and we set c1 = 0.
In each contract interaction, the edge ASP selects the
action a‚àó(r, Œ¶) that maximizes its utility as per the contract
designed by the teleoperator. Therefore, we define the utility
of the edge ASP in each contract interaction as:
œÄA = max
a
œÄA(a; r, Œ¶).
(2)
We term (2) as the incentive compatibility (IC) constraint.
Additionally, the edge ASP will only accept the contract if
œÄA ‚â•‚àÜs.
(3)


--- Page 6 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
6
We refer to (3) as the incentive rationality (IR) constraint.
We set a‚àó(r, Œ¶) = a1 if the edge ASP reject the contract
r. Without other specifications, we will use a‚àóto represent
a‚àó(r, Œ¶) in the rest of the paper for clarity.
3.3
Utility Model of Teleoperator
We consider the utility model of the teleoperator to consist
of three parts: the gain from the quality of AIGC services,
the bonus pay to the edge ASP, and the cost of AIGC
services subscription. We assume the teleoperator utility is
0 if the edge ASP rejects the contract. Therefore, we model
the utility function of the teleoperator as:
œÄT (r) =
(
Eom‚àºp(¬∑|a‚àó) [qm ‚àírm] ‚àírs,
if a‚àóÃ∏= a1,
0,
otherwise,
(4)
by referring to [5], [12], we set qm as
qm = ln(1 + Œ±Œ¥m).
(5)
Here, Œ± is the coefficient mapping the AIGC services quality
to the teleoperator‚Äôs gain. Œ¥m is the median value of om. Due
to the nature of the logarithm function, the teleoperator‚Äôs
valuation regarding the AIGC service quality has a rapid
initial growth rate that gradually diminishes as Œ¥m increases.
3.4
Optimization Problem Formulation
The optimization problem in this pape r is maximizing
the teleoperator utility without access to Œ¶. We define the
optimization problem as a standard moral hazard form:
P1 : max
r
Eom‚àºp(¬∑|a‚àó) [qm ‚àírm] ‚àírs,
(6a)
s.t.
a‚àó= arg max
a
œÄA(a; r, Œ¶)
(6b)
œÄA ‚â•‚àÜs.
(6c)
The difficulty in addressing P1 lies in the condition that we
cannot directly solve the IC in (6b) and IR in (6c) constraints.
In other words, we can tackle P1 with ease if we know Œ¶,
since then P1 is a convex optimization problem. Therefore,
we reformulate P1 into two sub-problems, P2 and P3, via
the historical contract interaction logs D. Concretely, we
formulate P2 as a Œ¶ approximation problem, which requires
us to find and approximate a feasible ÀúŒ¶ via D. We define P2
as a feasibility problem as follows:
P2 : find
(X, ÀúŒ¶),
(7a)
s.t.
N
X
n=1
xn,k = 1, ‚àÄk,
(7b)
M
X
m=1
Àúpn,m = 1, ‚àÄn,
(7c)
œÄT (rk) ‚â§ÀúpT
n(q ‚àírk) ‚àírs + L(1 ‚àíxn,k),
‚àÄn, k,
(7d)
œÄT (rk) ‚â•ÀúpT
n(q ‚àírk) ‚àírs ‚àíL(1 ‚àíxn,k),
‚àÄn, k,
(7e)
œÄA(an; rk, ÀúŒ¶) ‚â•‚àÜs ‚àíL(1 ‚àíxn,k), ‚àÄn, k,
(7f)
œÄA(an; rk, ÀúŒ¶) ‚â•œÄA(an‚Ä≤; rk) ‚àíL(1 ‚àíxn,k),
‚àÄn Ã∏= n‚Ä≤, ‚àÄn, k,
(7g)
œÄA(an; rk) ‚â§‚àÜs, ‚àÄI(rk) = 0,
(7h)
xn,k ‚àà{0, 1}, Àúpn,m ‚àà[0, 1], Àúcn ‚àà[0, L].
(7i)
Here, Eq. (7b) indicates the selection constraint of the edge
ASP, indicates the edge ASP selects action an in the k-
th interaction regarding the contract rk. Eq. (7c) is the
probability constraint, which means for each action an the
sum of the probabilities over the outcome space O equals 1.
Eqs. (7d) and (7e) is the utility matching constraint, indicates
the teleoperator utility under edge ASP‚Äôs action an, contract
rk, and the probability mapping vector pn under the action
an should match the real teleoperator utility œÄT (rk). Eqs.
(7f) and (7g) is the IR and IC constraints when edge ASP
accepts the contract rk. Eq. (7h) is the contract rejection
constraint, which means the IR constraint is violated when
the teleoperator formulates the contract rk. Eq. (7i) is the
range constraint of variables X, ÀúP, and Àúc. L is a large
constant used to aid the mathematical formulation.
Notably, it remains a challenge for us to solve P2 directly
from twofold. Firstly, P2 is a mixed integer nonlinear pro-
gramming (MINLP) problem, which is NP-hard. Secondly,
we cannot access the size of N, which means the dimension
of the optimization variables X and ÀúŒ¶ are uncertain. There-
fore, similar to recent online learning contract theory works
[22], [23], [24], [25], [26], we should strive to design an algo-
rithm that derives an approximate solution as accurately as
possible.
Once we tackle P2 and acquire ÀúŒ¶, We can formulate P3
in a similar manner as P1:
P3 : arg max
Àúr
Eom‚àºÀúp(¬∑|a‚àó(Àúr,ÀúŒ¶)) [qm ‚àíÀúrm] ‚àírs,
(8a)
s.t.
a‚àó(Àúr, ÀúŒ¶) = arg max
a
œÄA(a;Àúr, ÀúŒ¶),
(8b)
œÄA ‚â•‚àÜs.
(8c)
P3 is a nonlinear convex problem, and we can use con-
vex optimization techniques to tackle it, thereby acquiring
the approximated contract Àúr. Regarding the optimization
problems formulated in this section, we proposed an LLM-
empowered solution and presented the overall framework
diagram in Fig. 2. In the next section, we will dig into the
details of how we unleash the power of LLM to tackle the
optimization problems.


--- Page 7 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
7
Generator LLM
Initialization prompt
Initialized P2 solvers
Ranking-based pair selection 
from P2 solvers population
‚Ä¶
Short-term 
reflector LLM
Crossover LLM
Add
Long-term 
reflector LLM
‚Ä¶
Mutation LLM
Add
Seed solver
0
ÔÅì
Stage I: P2 Solvers Initialization
Stage II: P2 Solver Evolution
I
√ó
Fig. 3: LLM-empowered P2 solver evolution framework, which presents the details of step three in Fig. 2. We present the
description of stages I and II in Sections 4.1 and 4.2, respectively. We present all prompts of LLM agents in Appendix A of
the supplementary material.
4
LLM-EMPOWERED SOLUTION
With the problem reformulation in Section 3.4, the objective
function will shift to maximize œÄT (Àúr) by solving P2 and
P3 progressively, and the key is solving P2. With recent
exciting advancements in leveraging LLM to solve NP-hard
problems [27], [28], [29], we consider leveraging LLM to
tackle P2 effectively. We present the overall framework of
LLM-empowered P2 solver in Fig. 3, which includes P2
solvers initialization and P2 solver evolution in two stages.
Concretely, in this section, we illustrate the initialization of
P2 solvers in Section 4.1 and present the evolution of the
P2 solver in Section 4.2. Lastly, we present and analyze
the algorithm of LLM-empowered P2 solver evolution in
Section 4.3.
4.1
P2 solvers initialization
As per the left panel of Fig. 3, the P2 solvers initialization
is composed of three major components: seed solver, initial-
ization prompt, and solvers evaluation.
4.1.1
Seed Solver
We consider building the seed solver of P2 in a naive manner
and specify it in Algorithm 1.
1. We commence with initializing the parameters in lines
1-3.
2. We screen accepted contracts and calculate the possible
probability mapping vector pk in lines 4-9.
3. We aggregate Œ®p into ÀúN vectors via KMeans [50] and
obtain ÀúP in line 10, in which the vector in ÀúP is in
ascending order.
4. We leverage the IC and IR constraints to approximate
the cost vector Àúc of edge ASP in lines 12-23. When the
edge teleoperator accepts the contract, we utilize both
IC and IR constraints (7f) and (7g), in lines 12-20. When
the edge teleoperator rejects the contract, we use the IR
constraint (7h), in lines 21-23.
Notably, we define the P4 in line 6 as:
P4 : min
pk
pk
T rk,
(9a)
s.t.
M
X
m=1
pk,m = 1,
(9b)
pk
T (q ‚àírk) ‚àírs = œÄT (rk),
(9c)
pk,m ‚àà[0, 1].
(9d)
Eq. (9b) is the probability constraint, which stems from Eq.
(7c). Eq. (9c) is the teleoperator utility matching constraint,
stems from (7d) and (7e). Eq. (9d) is the range constraint,
partial of (7i). By solving P4, we can acquire a potential
probability mapping vector pk between the edge ASP‚Äôs
action and the teleoperator‚Äôs outcome. It is worth noting
that we set the objective function as minimizing the reward
paid to the edge ASP, which is equivalent to maximizing the
teleoperator‚Äôs utility. We designate the P2 seed solver as S0
in the rest of the paper.
4.1.2
Initialization Prompt
The initialization prompt includes several crucial compo-
nents: problem description, function description, input in-
stance, and seed function.
Problem Description: We provide a hint to the LLM
agent that we need to infer a valid edge ASP setting that
satisfies all interaction logs in D regarding an online learn-
ing contract design problem.
Function Description: We describe the input format of
the P2 solver and state its requirements, specifying the
structure of a valid ÀúŒ¶.


--- Page 8 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
8
Algorithm 1: P2 Seed Solver
Input: Teleoperator‚Äôs profit regarding outcomes q
and historical interaction logs D
Output: Inferred edge ASP‚Äôs setting ÀúŒ¶ = (ÀúP, Àúc)
1 Assume the size of action space N = ÀúN
2 Initialize candidate probability vector set Œ®p
3 Initialize edge ASP‚Äôs cost vector Àúc = 0
4 foreach (rk, œÄT (rk), I(rk)) ‚ààD do
5
if I(rk) == 1 then
6
pk ‚ÜêSolve P4 via rk and œÄT (rk)
7
Œ®p ‚Üêpk
8
end
9 end
10 ÀúP ‚ÜêKMeans( ÀúN, Œ®p)
11 foreach (rk, œÄT (rk), I(rk)) ‚ààD do
12
if I(rk) == 1 then
13
nk ‚Üêargmax(ÀúPrk)
14
if Àúc[nk] == 0 then
15
Àúc[nk] = ÀúP[nk]rk)
16
end
17
else
18
Àúc[nk] = min(Àúc[nk], ÀúP[nk]rk))
19
end
20
end
21
else
22
Àúc = max(Àúc, ÀúPrk)
23
end
24 end
25 return ÀúŒ¶ = (ÀúP, Àúc)
Input Instance: We provide the input illustration of the
P2 solver, including the teleoperator‚Äôs profit over the AIGC
service quality q and the historical interaction logs D.
Seed Function: We provide a runnable Python code, i.e.,
Algorithm 1, to the LLM agent for reference.
4.1.3
Solvers Evaluation
Upon completion of P2 solvers initialization, we can acquire
Ni solvers, denotes as {Sni|ni ‚àà[Ni]}. We evaluate the
fitness of Sni by tackling the P3. Concretely, Sni is a Python
function of the P2 solver with the input of q and D, and
returns a predicted edge ASP‚Äôs setting ÀúŒ¶ni. Subsequently,
we can use ÀúŒ¶ni to solve the P3 and derive a contract Àúrni.
Lastly, we observe the teleoperator utility œÄT (Àúrni) under
contract Àúrni and leverage it as the fitness score of Sni.
4.2
P2 solver evolution
As per the right panel of Fig. 3, the P2 solver evolution
comprises one premise and four steps. The premise is that
we need to do a ranking-based pair selection from the old P2
solver population ‚Ñ¶o. Next, we execute the A-D steps in a
manner akin to a genetic algorithm, including short-term
reflection, crossover, long-term reflection, and mutation.
Notably, the P2 solver evolution will run I iterations and
return the elitist P2 solver S‚àóduring the evolution as the
final output. In this paper, we consider that one epoch is
running the premise and steps A-D, which includes multiple
iterations.
Algorithm 2: LLM-Empowered P2 Solver Evolution
Input: Seed solver S0 and iteration rounds I
Output: Elitist P2 solver S‚àó
/* Initialize and evaluate P2 solvers
*/
1 {Sni|ni ‚àà[Ni]} ‚Üê
GeneratorLLM(S0, Init prompt)
2 {ÀúŒ¶ni|ni ‚àà[Ni]} ‚ÜêInvoke {Sni|ni ‚àà[Ni]}
3 {œÄT (Àúrni)|ni ‚àà[Ni]} ‚ÜêTackle P3 via ÀúŒ¶ni and
evaluate Àúrni
4 ‚Ñ¶o ‚Üê{Sni|ni ‚àà[Ni]}, i = Ni
5 S‚àó‚ÜêOpt the best P2 solver from ‚Ñ¶o
6 while i ‚â§I do
/* Begin the epoch of P2 solver
evolution
*/
7
‚Ñ¶n ‚Üê‚àÖ
8
Rank Ss ‚àà‚Ñ¶o in a descending order as per
œÄT (Àúrs)
9
foreach ns ‚àà[Ns/2] do
10
Opt Sb and Sw from ‚Ñ¶o with probability
proportional to their rank
11
‚Ñ¶n ‚Üê{Sb, Sw}
12
Œ∏ns ‚ÜêShortReflectorLLM(Sb, Sw)
13
Sns ‚ÜêCrossoverLLM(Sb, Sw, Œ∏ns)
14
œÄT (Àúrns) ‚ÜêInvoke Sns, tackle P3, and
evaluate Àúrns
15
‚Ñ¶n ‚Üê{Sns}
16
i+ = 1
17
end
18
Œò ‚ÜêShortReflectorLLM({Œ∏ns|ns ‚àà
[Ns/2]}, Œò‚Ä≤)
19
foreach nm ‚àà[Nm] do
20
Snm ‚ÜêMutationLLM(Œò, S‚àó)
21
œÄT (Àúrnm) ‚ÜêInvoke Snm, tackle P3, and
evaluate Àúrnm
22
‚Ñ¶n ‚Üê{Snm}
23
i+ = 1
24
end
25
S‚àó‚ÜêOpt the best P2 solver from ‚Ñ¶n ‚à™{S‚àó}
26
‚Ñ¶o ‚Üê‚Ñ¶n
27 end
28 return S‚àó
4.2.1
Premise Step
At the beginning of an epoch, we will sort all solvers in ‚Ñ¶o
by their fitness score and repeatedly select two P2 solvers,
Sb and Sw, with probability proportional to their rank.
Here, Sb and Sw represent a better and a worse P2 solver,
respectively. We will append Sb and Sw to the new P2 solver
population ‚Ñ¶n, and the selection ends when the size of ‚Ñ¶n
reaches Ns. Remarkably, it is crucial to ensure that the fitness
scores of Sb and Sw are distinct, thereby eliciting the ‚Äúshort
verbal gradient‚Äù in the next step.
4.2.2
Short-Term Reflection (Step A)
Regarding each Sb and Sw selected in the premise step,
we will use a short-term reflector LLM agent to generate a
‚Äúshort verbal gradient‚Äù as insight for P2 solver refinement.


--- Page 9 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
9
Concretely, the reflector LLM will compare and analyze Sb
and Sw, then output less than 20 words as ‚Äúshort verbal
gradient‚Äù Œ∏ns. We present one instance of the ‚Äúshort verbal
gradient‚Äù as follows: ‚ÄúIncorporate LP for p inference, adaptive
clustering with silhouette, simplex projection, and strict IR/IC
cost adjustments.‚Äù In light of we have opted Ns P2 solvers
in the premise step, we can obtain Ns/2 ‚Äúshort verbal
gradient‚Äù, denotes as {Œ∏ns|ns ‚àà[Ns/2]}.
4.2.3
Crossover (Step B)
With Sb and Sw and the associated ‚Äúshort verbal gradient‚Äù
Œ∏ns as input, the crossover LLM agent will generate an off-
spring Sns. Similarly, the crossover step will generate Ns/2
offspring, denotes as {Sns|ns ‚àà[Ns/2]}. After assessing the
fitness score of Ns/2 offspring, we append them to ‚Ñ¶n.
4.2.4
Long-Term Reflection (Step C)
Upon the generation of {Œ∏ns|ns ‚àà[Ns/2]}, the long-term
reflector LLM agent will aggregate them and ‚Äúprevious long
verbal gradient‚Äù Œò‚Ä≤ 1 into ‚Äúcurrent long verbal gradient‚Äù
Œò. We present one instance of the ‚Äúlong verbal gradient‚Äù
as follows: ‚ÄúCombine adaptive density-based clustering (e.g.,
DBSCAN) on inferred distributions with LP-enforced strict IR/IC
constraints. Refine costs upward using rejection margins, normal-
ize outcome probabilities, and prioritize feasibility. Employ sil-
houette scores and hierarchical methods to ensure robust, realistic
agent settings matching all logs.‚Äù
4.2.5
Mutation (Step D)
The mutation LLM agent leverages S‚àóand Œò as input to
generate Nm mutated P2 solvers, denoting as {Snm|nm ‚àà
[Nm]}. Subsequent to assessing the fitness score of muta-
tions, we append them to ‚Ñ¶n. Moreover, we need to update
the elitist P2 solver S‚àó, revise the old population ‚Ñ¶o = ‚Ñ¶n,
and clean the new population ‚Ñ¶n before moving to the next
epoch.
4.3
Algorithm Design and Analysis
Upon the preceding illustrations in Sections 4.1 and 4.2,
we present the LLM-empowered P2 solver evolution in
Algorithm 2 for clarity. It is worth noting that the number
of interaction rounds required for deriving a near-optimal
contract by Algorithm 2 is K + I. Here, K is the number of
historical interaction logs as input to the P2 solver. I is the
number of iterations of Algorithm 2. In each iteration, we
need to evaluate the P2 solver via interaction between the
teleoperator and the edge ASP.
5
EXPERIMENTAL EVALUATION
In this section, we evaluate our proposed LLM-empowered
online learning contract for bonus design. Specifically, we
introduce the experimental configurations in Section 5.1,
analyze the scalability, sensitivity, and effectiveness of the
LLM-empowered solution in Sections 5.2, 5.3, and 5.4, re-
spectively.
1. This long verbal gradient is generated in the previous epoch.
5.1
Experimental Configurations
5.1.1
Parameters Configurations
In this paper, we model the bonus design between the tele-
operator and edge ASPs as an online learning contract de-
sign problem, described by elements C = (D, R, O, A, Œ¶, q).
We consider the size of historical interaction logs |D| = K =
{25, 50, 100}, size of outcome space M = {2, 4, 6, 8, 10, 12},
and the size of edge ASP‚Äôs action space N = {2, 3, 4, 5, 6, 7}.
The outcome om is an interval in [0.9, 1.8], {om|m ‚àà
[2]} = {[0.9, 1.35), [1.35, 1.8)} if M = 2. The edge ASP‚Äôs
action an is the diffusion step of the AIGC model, we set
an ‚àà{4, 5, 6, 7, 8, 9, 10}. The AIGC model is a conditional
diffusion model, and we obtain it by referring to [4]. Re-
garding the edge ASP‚Äôs setting Œ¶ = (P, c), we utilize the
statistical distribution on 200 AIGC tasks [4], [5] under the
diffusion step an as pn. Moreover, we consider the addi-
tional computational energy cost as cn under the diffusion
step an, c = ${0, 1.5‚àí6, 4‚àí6, 5.6‚àí6, 5.1‚àí6, 8.1‚àí6, 7.8‚àí6} per
image. Notably, we acquire the c via the electricity data
from the Energy Information Administration (EIA) 2 and
CodeCarbon 3. We set the AIGC services subscription fee
per image as rs = $1.6‚àí4 by referring to the image pricing
policy of Anthropic 4. With the assumption that the net
profit of the AIGC model is around 30%, we set the AIGC
model training cost spread per image to ct = $1.2‚àí4. We
set the coefficient of mapping AIGC services quality to
teleoperator‚Äôs utility Œ± ‚àà{5‚àí4, 1‚àí3, 5‚àí3}. All LLM agents
in this paper are instances of gpt-4.1-mini-2025-04-14, and
we set the iteration rounds I = 200 by default.
5.1.2
Benchmarks and Evaluation Metrics
To
assess
the
effectiveness
of
our
proposed
LLM-
empowered online learning contract design, we compare it
with the following benchmarks:
1) Seed: We leverage the seed solver, i.e., Algorithm 1 to
infer the edge ASP‚Äôs setting ÀúŒ¶, and then derive the
contract via tackling P3. With this benchmark, we can
evaluate the effectiveness of our proposed solution.
2) Zero-shot: We run the Algorithm 2 under a simulated
scenario, and utilize the output ÀÜS‚àóas the P2 solver for
teleoperator. In this way, we can assess the performance
of our proposed solution in a limited K interaction
rounds between the teleoperator and the edge ASP.
3) Bandit: By referring to [22], [46], we use the multi-
armed bandit method to derive the contract directly.
In this paper, we evaluate our proposed solution in three
metrics, augmented teleoperator utility œÄ%
T , augmented
edge ASP utility œÄ%
A, and percentages to the optimal Œ∑. With-
out a bonus incentive mechanism (contract), we consider
the edge ASP will take a1 = 4 diffusion steps by default.
Therefore, we define œÄ%
T as:
œÄ%
T = (œÄT (Àúr) ‚àíœÄT (0))/œÄT (0).
(10)
Analogously, we define œÄ%
A as:
œÄ%
A = (œÄA(ai; œÄT (Àúr), Œ¶) ‚àíœÄA(a1; 0, Œ¶))/œÄA(a1; 0, Œ¶). (11)
2. https://www.eia.gov/electricity/monthly/update/end-use.php
3. https://codecarbon.io/
4. https://docs.anthropic.com/en/docs/build-with-claude/vision


--- Page 10 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
10
2
3
4
5
1 0
2 0
0 . 0
0 . 2
0 . 4
0 . 6
0 . 8
1 . 0
P e r c e n t a g e  t o  t h e  o p t i m a l
S i z e  o f  o u t c o m e  s p a c e
 N  =  1 0
 N  =  1 0 0
 N  =  1 , 0 0 0
(a)
2
3
4
5
1 0
2 0
0 . 0
0 . 2
0 . 4
0 . 6
0 . 8
1 . 0
P e r c e n t a g e  t o  t h e  o p t i m a l
S i z e  o f  o u t c o m e  s p a c e
(b)
2
3
4
5
1 0
2 0
0 . 0
0 . 2
0 . 4
0 . 6
0 . 8
1 . 0
P e r c e n t a g e  t o  t h e  o p t i m a l
S i z e  o f  o u t c o m e  s p a c e
(c)
Fig. 4: Evaluate the scalability of the proposed method by varying the size of the outcome space M, the number of actions
N, and the number of historical interaction logs K, in which K = {25, 50, 100} in Figs. 4a, 4b, and 4c.
2
4
6
8
1 0
1 2
0 . 0 0
0 . 0 5
0 . 1 0
0 . 1 5
0 . 2 0
0 . 2 5
0 . 3 0
0 . 3 5
0 . 4 0
A u g m e n t e d  t e l e o p e r a t o r  u t i l i t y
S i z e  o f  o u t c o m e  s p a c e
 
 
 
Fig.
5:
Assess
the
sensitivity
of
the proposed method regarding the
mapping coefficient Œ± in terms of
œÄ%
T .
2
4
6
8
1 0
1 2
0
5
1 0
1 5
2 0
2 5
3 0
3 5
A u g m e n t e d  e d g e  A S P  u t i l i t y
S i z e  o f  o u t c o m e  s p a c e
Fig.
6:
Assess
the
sensitivity
of
the proposed method regarding the
mapping coefficient Œ± in terms of
œÄ%
A.
2
4
6
8
1 0
1 2
0 . 0
0 . 1
0 . 2
0 . 3
0 . 4
0 . 5
0 . 6
0 . 7
0 . 8
0 . 9
1 . 0
P e r c e n t a g e  t o  t h e  o p t i m a l
S i z e  o f  o u t c o m e  s p a c e
Fig.
7:
Assess
the
sensitivity
of
the proposed method regarding the
mapping coefficient Œ± in terms of Œ∑.
Regarding the metric of percentages to the optimal Œ∑, we
define it as:
Œ∑ = œÄT (Àúr)/œÄT (r‚àó),
(12)
where r‚àóis the optimal contract under the assumption that
the teleoperator is privy to the edge ASP‚Äôs setting Œ¶.
We utilize Python to run all simulations in this paper
and conduct the experiments on a laptop equipped with a
6-core AMD Ryzen 5 processor, 32 GB of RAM, an RTX 4060
GPU, and a Windows 11 operating system. We ran each
experiment multiple times to enhance the reliability of the
result and take the average value as the final result.
5.2
Scalability of LLM-Empowered Solution
In this section, we simulate an online learning contract prob-
lem, as described in [26], to evaluate the scalability of our
proposed method. Concretely, we apply the SoftMax on a
Gaussian random vector in RM to generate the outcome dis-
tributions pn for action an. We sample the outcome qm uni-
formly from [0, 10]. We set the action cost as a mixture, cn =
(1‚àíŒ≤p)cr(an)+Œ≤pci(an), where cr(an) = Œ≤cEom‚àºpn[qm] is a
correlated cost that proportional to the expected value of the
action. ci(an) is an independent cost and uniform on [0, 1].
We set the coefficients Œ≤c = 0.7 and Œ≤p = 0.3 by default.
To assess the scalability, we run experiments under the size
of outcome space M ‚àà{2, 3, 4, 5, 10, 20}, the size of action
space N ‚àà{10, 100, 1000}, and the number of historical
interaction logs K ‚àà{25, 50, 100}.
In light of our use of the simulated dataset to assess
the scalability of our proposed method, we focus solely on
the metric Œ∑ and present the evaluation results in Fig. 4.
Observing Figs. 4a, 4b, and 4c show that the performance of
our proposed method increases with the number of interac-
tion logs. Moreover, as M and N increase, the performance
of our proposed method declines. Remarkably, observing
Fig. 4c, our proposed method can maintain Œ∑ = 75% under
varying M and N when K = 100. The performance of our
proposed method can even maintain Œ∑ = 95% under vary-
ing sizes of action space when M = 2 and K = {50, 100}.
Therefore, our proposed method possesses scalability in
terms of the size of the action space N under a small number
of interaction rounds (50) between the edge ASP and the
teleoperator.
5.3
Sensitivity of LLM-Empowered Solution
In this section, we evaluate the sensitivity of our proposed
method regarding the bonus design (contract) for edge
ASP in terms of the mapping coefficient Œ±, the size of the
outcome space M, the size of the action space N, and the
number of historical interaction logs K. Remarkably, we set
Œ± = 5‚àí4, M = 12, N = 7, and K = 100 by default if
without other specifications.


--- Page 11 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
11
0 . 2 0 9 5
0 . 3 1 2 6
0
0 . 0 1 8 1
0 . 0 2 7 6
0 . 0 2 4 6
0 . 2 6 9 7
0 . 2 7 9 1
0 . 3 3 4 2
0 . 0 6 7 5
0 . 0 4 3 7
- 0 . 0 0 2 5
0 . 2 7 2 4
0 . 2 3 1 5
0 . 3 5 2 6
- 0 . 0 5 5 8
0 . 1 6 2
0 . 0 0 2 3
0 . 2 7 2 4
0 . 2 2 1 9
0 . 3 4 9 8
- 0 . 0 8 1 1
0 . 1 5 8 8
- 1 E - 4
0 . 2 7 2 4
0 . 2 1 2 2
0 . 3 3 1 6
- 0 . 0 8 5 3
0 . 1 5 2 5
0 . 1 5 2 9
0 . 2 7 2 4
0 . 1 7 9 2
0 . 2 9 0 7
- 0 . 1 0 5
0 . 1 4 3 6
0 . 1 2 1 7
2
4
6
8
1 0
1 2
2
3
4
5
6
7
S i z e  o f  a c t i o n  s p a c e  ( N )
S i z e  o f  o u t c o m e  s p a c e  ( M
)
A u g m e n t e d  t e l e o p e r a t o r  u t i l i t y
(a)
2 . 0 4 7 6
0 . 2 0 9 4
0
3 . 2 6 8 9
1 . 9 2 8 7
3 . 1 9 8 7
2 . 5 5 3 8
1 . 6 9 9 3
0 . 9 8 2 4
3 . 8 2 1 6
2 . 4 3 8 6
2 . 3 2 7 5
2 . 5 5 5 7
2 . 5 4 1 3
1 . 2 6 7 9
3 . 8 4 4 3
1 . 5 3 9
2 . 7 5 8 8
2 . 5 5 5 7
2 . 6 9 5
1 . 2 8 0 4
4 . 1 0 8 5
1 . 5 7 8 3
3 . 2 7 6 6
2 . 5 5 5 7
2 . 8 3 1 6
1 . 5 4 3 4
4 . 2 0 4 8
1 . 6 5 8 5
3 . 3 7 9 2
2 . 5 5 5 7
3 . 4 0 3 6
2 . 1 7 0 3
4 . 5 3 8 9
1 . 8 7 7 4
3 . 8 2 2 1
2
4
6
8
1 0
1 2
2
3
4
5
6
7
S i z e  o f  a c t i o n  s p a c e  ( N )
S i z e  o f  o u t c o m e  s p a c e  ( M
)
A u g m e n t e d  e d g e  A S P  u t i l i t y
(b)
0 . 8 5 7 4
0 . 9 8 4 7
0 . 7 5 0 3
0 . 7 6 2 2
0 . 7 7 9
0 . 7 6 7 7
0 . 8 4 5 5
0 . 8 8 5 5
0 . 9 3 3 5
0 . 7 4 1 9
0 . 7 3 6 7
0 . 6 9 6 3
0 . 8 4 7 2
0 . 8 3 2 7
0 . 9 1 6 8
0 . 6 3 9 4
0 . 7 9 3 4
0 . 6 7 9 7
0 . 8 4 7 2
0 . 8 2 3 2
0 . 9 1 5 9
0 . 6 2 2
0 . 7 9 0 3
0 . 6 7 6 5
0 . 8 4 7 2
0 . 8 1 6 7
0 . 9 0 3 6
0 . 6 1 9 2
0 . 7 8 6
0 . 7 7 7 2
0 . 8 4 7 2
0 . 7 8 6 6
0 . 8 6
0 . 5 9 5 1
0 . 7 6 9 8
0 . 7 4 7 3
2
4
6
8
1 0
1 2
2
3
4
5
6
7
S i z e  o f  a c t i o n  s p a c e  ( N )
S i z e  o f  o u t c o m e  s p a c e  ( M
)
P e r c e n t a g e  t o  t h e  o p t i m a l
(c)
Fig. 8: Assess the sensitivity of the proposed method regarding the size of outcome space M and the size of action space
N in terms of œÄ%
T , œÄ%
A, and Œ∑. We set the number of historical interaction logs K = 25.
0 . 2 3 9 4
0 . 3 1 6 2
0
0 . 1 0 0 9
0
- 0 . 1 3 7 8
0 . 3 0 7 7
0 . 3 0 5 6
0 . 4 0 3 1
0 . 2 0 9 7
0 . 0 3 2
0 . 0 9 7 9
0 . 3 0 9
0 . 2 7 1
0 . 3 6 0 7
0 . 1 9 2 7
0 . 1 9 6 7
0 . 2 2 0 8
0 . 3 0 9
0 . 2 6 3 5
0 . 3 5 8
0 . 1 8 0 3
0 . 0 8 0 2
0 . 1 1 6 6
0 . 3 0 9
0 . 2 5 6 9
0 . 3 4 1 8
0 . 1 7 8 3
0 . 0 8 0 2
0 . 1 1 4 2
0 . 3 0 9
0 . 2 3 2 2
0 . 3 0 4 4
0 . 1 6 8 2
0 . 0 8 0 2
0 . 1
2
4
6
8
1 0
1 2
2
3
4
5
6
7
S i z e  o f  a c t i o n  s p a c e  ( N )
S i z e  o f  o u t c o m e  s p a c e  ( M
)
A u g m e n t e d  t e l e o p e r a t o r  u t i l i t y
(a)
1 . 7 4 3 7
0 . 1 7 2 1
0
0 . 1 1 2 9
0
3 . 7 2 3 6
2 . 1 6 7
1 . 4 2 7 1
0 . 2 7 0 6
0 . 8 5 2 6
1 . 1 1 3 8
3 . 1 1 6 2
2 . 1 6 7 8
2 . 1 3 5 8
1 . 1 8 3 4
1 . 2 8 7
1 . 1 7 8 1
2 . 1 3 6 4
2 . 1 6 7 8
2 . 2 6 8 1
1 . 1 9 5 9
1 . 4 1 6 5
2 . 2 3 1 2
3 . 2 3 5 5
2 . 1 6 7 8
2 . 3 7 2 7
1 . 4 3 8 5
1 . 4 6 3 2
2 . 2 3 1 2
3 . 2 7 9 2
2 . 1 6 7 8
2 . 8 5 9 3
2 . 0 2 8 3
1 . 7 2 7 3
2 . 2 3 1 2
3 . 5 6 4 9
2
4
6
8
1 0
1 2
2
3
4
5
6
7
S i z e  o f  a c t i o n  s p a c e  ( N )
S i z e  o f  o u t c o m e  s p a c e  ( M
)
A u g m e n t e d  e d g e  A S P  u t i l i t y
(b)
0 . 8 7 8 6
0 . 9 8 7 4
0 . 7 5 0 3
0 . 8 2 4 3
0 . 7 5 8 1
0 . 6 4 6
0 . 8 7 0 8
0 . 9 0 3 8
0 . 9 8 1 7
0 . 8 4 0 7
0 . 7 2 8 5
0 . 7 6 6 4
0 . 8 7 1 7
0 . 8 5 9 4
0 . 9 2 2 4
0 . 8 0 7 7
0 . 8 1 7 1
0 . 8 2 8
0 . 8 7 1 7
0 . 8 5 1 2
0 . 9 2 1 5
0 . 7 9 9
0 . 7 3 6 7
0 . 7 5 5 5
0 . 8 7 1 7
0 . 8 4 6 8
0 . 9 1 0 5
0 . 7 9 7 7
0 . 7 3 6 7
0 . 7 5 1 1
0 . 8 7 1 7
0 . 8 2 2
0 . 8 6 9 2
0 . 7 7 6 7
0 . 7 2 7
0 . 7 3 2 9
2
4
6
8
1 0
1 2
2
3
4
5
6
7
S i z e  o f  a c t i o n  s p a c e  ( N )
S i z e  o f  o u t c o m e  s p a c e  ( M
)
P e r c e n t a g e  t o  t h e  o p t i m a l
(c)
Fig. 9: Assess the sensitivity of the proposed method regarding the size of outcome space M and the size of action space
N in terms of œÄ%
T , œÄ%
A, and Œ∑. We set the number of historical interaction logs K = 50.
As shown in Figs. 5, 6, and 7, we present the exper-
imental results regarding œÄ%
T , œÄ%
A, and Œ∑ by varying the
mapping coefficient Œ±. Observing Fig. 5, œÄ%
T is diminishing
along with the increasing of Œ±. The basic rationale is that
the teleoperator places higher importance on AIGC service
quality and is willing to offer a higher bonus, thereby elicit-
ing the edge that ASP takes more diffusion steps. The results
in Fig. 6 demonstrate this point, where the edge ASP‚Äôs
augmented utility is escalating along with the increase in Œ±.
By analyzing Fig. 7, our proposed method is robust against
Œ±, which maintains Œ∑ = 90% under varying Œ±.
In Fig. 8, we set K = 25 and vary M and N simulta-
neously to analyze the sensitivity of our proposed method.
As shown in Fig. 8a, the mounting of M will deteriorate
œÄ%
T , which means our proposed method is sensitive to M.
However, œÄ%
T is escalating along with the increase in N; our
proposed method is robust against N. The basic rationale is
that the rise of M will cause the size of edge ASPs‚Äô setting
Œ¶ to increase, thereby improving the difficulty in deriving a
near-optimal contract. Notably, although the escalations of
N will also increase the size of Œ¶, the mounted action space
indicates that more fine-grained actions are available for the
contract offered by the teleoperator. The results in Figs. 8a
and 8b are akin to the analysis of Figs. 5 and 6, i.e., increasing
œÄ%
A deteriorates œÄ%
T since more utility is transferred from the
teleoperator to the edge ASP via contract. The results in Figs.
9 and 10 are akin to Fig. 8. Notably, the performance of our
proposed method is increasing along with the escalation of
K, and our proposed method can achieve around Œ∑ = 90%
when K = 100 under varying M and N as the results
depicted in Fig. 10c.
5.4
Effectiveness of LLM-Empowered Solution
In this section, we assess the effectiveness of our proposed
method by comparing it with benchmarks, varying Œ±, M,
and N. Notably, the benchmarks in this experiment com-
prise K = 300 historical interaction logs for contract deriva-
tion, as our proposed method utilizes K = 100 historical
interaction logs and I = 200 iteration rounds for Algorithm
2.
Observing Fig. 11, our proposed method consistently
surpasses the benchmarks in terms of œÄ%
T under varying


--- Page 12 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
12
0 . 2 9 0 1
0 . 3 1 6 2
0
0 . 1 3 1 1
0
- 0 . 0 0 1 4
0 . 3 7 2 2
0 . 3 0 5 6
0 . 4 0 6 5
0 . 2 5 5
0 . 1 1 1 2
0 . 2 9 5 7
0 . 3 7 2 2
0 . 2 7 1
0 . 3 7 3 2
0 . 2 5 5 7
0 . 3 0 2 5
0 . 1 8 6 2
0 . 3 7 2 2
0 . 2 6 3 5
0 . 3 7 0 4
0 . 2 4 5 3
0 . 3 0 2 8
0 . 4
0 . 3 7 2 2
0 . 2 5 6 9
0 . 3 5 7 2
0 . 2 4 4 5
0 . 3 0 2 8
0 . 3 9 3 5
0 . 3 7 2 2
0 . 2 3 2 2
0 . 3 2 5 3
0 . 2 3 7
0 . 2 9 9 8
0 . 3 6 9 1
2
4
6
8
1 0
1 2
2
3
4
5
6
7
S i z e  o f  a c t i o n  s p a c e  ( N )
S i z e  o f  o u t c o m e  s p a c e  ( M
)
A u g m e n t e d  t e l e o p e r a t o r  u t i l i t y
(a)
1 . 2 2 7 9
0 . 1 7 2 1
0
0 . 9 5 3 9
0
1 . 1 6 5 7
1 . 5 1 0 5
1 . 4 2 7 1
0 . 2 3 4 6
1 . 8 9 2 3
0 . 2 8 8 4
1 . 4 1 2 5
1 . 5 1 0 5
2 . 1 3 5 8
1 . 0 5 4 9
2 . 2 7 3 4
0 . 0 7 5 9
1 . 3 4 3 1
1 . 5 1 0 5
2 . 2 6 8 1
1 . 0 6 7 4
2 . 3 8 2 2
0 . 0 8 4 2
0 . 8 0 4 5
1 . 5 1 0 5
2 . 3 7 2 7
1 . 2 7 9 2
2 . 4 1 6 9
0 . 0 8 4 2
0 . 9 2 8 5
1 . 5 1 0 5
2 . 8 5 9 3
1 . 8 1 2 5
2 . 6 5 4 3
0 . 2 5 0 7
1 . 3 6 0 4
2
4
6
8
1 0
1 2
2
3
4
5
6
7
S i z e  o f  a c t i o n  s p a c e  ( N )
S i z e  o f  o u t c o m e  s p a c e  ( M
)
A u g m e n t e d  e d g e  A S P  u t i l i t y
(b)
0 . 9 1 4 5
0 . 9 8 7 4
0 . 7 5 0 3
0 . 8 4 6 8
0 . 7 5 8 1
0 . 7 4 8 2
0 . 9 1 3 7
0 . 9 0 3 8
0 . 9 8 4 1
0 . 8 7 2 2
0 . 7 8 4 4
0 . 9 0 4 4
0 . 9 1 3 7
0 . 8 5 9 4
0 . 9 3 0 8
0 . 8 5 0 4
0 . 8 8 9 3
0 . 8 0 4 5
0 . 9 1 3 7
0 . 8 5 1 2
0 . 9 2 9 9
0 . 8 4 3
0 . 8 8 8 5
0 . 9 4 7 3
0 . 9 1 3 7
0 . 8 4 6 8
0 . 9 2 0 9
0 . 8 4 2 5
0 . 8 8 8 5
0 . 9 3 9 3
0 . 9 1 3 7
0 . 8 2 2
0 . 8 8 3 1
0 . 8 2 2 4
0 . 8 7 4 8
0 . 9 1 2 2
2
4
6
8
1 0
1 2
2
3
4
5
6
7
S i z e  o f  a c t i o n  s p a c e  ( N )
S i z e  o f  o u t c o m e  s p a c e  ( M
)
P e r c e n t a g e  t o  t h e  o p t i m a l
(c)
Fig. 10: Assess the sensitivity of the proposed method regarding the size of outcome space M and the size of action space
N in terms of œÄ%
T , œÄ%
A, and Œ∑. We set the number of historical interaction logs K = 100.
- 0 . 2
- 0 . 1
0 . 0
0 . 1
0 . 2
0 . 3
0 . 4
A u g m e n t e d  t e l e o p e r a t o r  u t i l i t y
 O u r s  
 S e e d
 Z e r o - s h o t  
 B a n d i t
Fig. 11: Comparison results by vary-
ing the mapping coefficient Œ± in
terms of œÄ%
T .
0
5
1 0
1 5
2 0
2 5
3 0
3 5
4 0
4 5
5 0
A u g m e n t e d  e d g e  A S P  u t i l i t y
Fig. 12: Comparison results by vary-
ing the mapping coefficient Œ± in
terms of œÄ%
A.
0 . 0
0 . 1
0 . 2
0 . 3
0 . 4
0 . 5
0 . 6
0 . 7
0 . 8
0 . 9
1 . 0
P e r c e n t a g e  t o  t h e  o p t i m a l
Fig. 13: Comparison results by vary-
ing the mapping coefficient Œ± in
terms of Œ∑.
Œ±, which demonstrates the effectiveness of our proposed
method. Compared to the seed solver, our proposed method
can augment œÄ%
T by around 40%, further demonstrating the
effectiveness of our proposed method. Compared with the
traditional bandit algorithm, our proposed method can aug-
ment œÄ%
T around 20 ‚àº25%. As depicted in Fig. 12, although
our proposed method yields the worst improvement in edge
ASP utility, it can still ensure the IR constraint, thereby
incentivizing the edge ASP to supply high-quality AIGC
services at a minimal cost. The results in Fig. 13 are similar
to those in Fig. 11; our proposed method outperforms the
benchmarks and maintains Œ∑ at around 90%.
The results presented in Figs. 14, 15, and 16 is akin
to Figs. 11, 12, and 13. Concretely, observing Fig. 14, our
proposed method can augment œÄ%
T in around 16 ‚àº37% in
comparison with the seed solver under varying M. Com-
paring with the bandit algorithm, our proposed method
can improve œÄ%
T in around 5 ‚àº36% under varying M.
Notably, the zero-shot method can achieve the same œÄ%
T as
our proposed method when M = 2. The results in Fig. 15
are similar to Fig. 12; we will not reiterate them. From Fig.16,
we observe that our proposed method remains maintain Œ∑
at around 90% under varying M.
Observing Fig. 17, one exception occurs, where our pro-
posed method is inferior to the zero-shot when N = 2 and
only achieves the second-best performance in œÄ%
T . However,
our proposed method can enlarge the improvement in œÄ%
T to
around 35 ‚àº58% in comparison with the seed solver under
varying N. In comparison with the bandit algorithm, our
proposed method can improve œÄ%
T by at most 38%. Analysis
of Fig. 18 demonstrates that our proposed method can
provide a positive incentive to the edge ASP by satisfying
the IR constraint. Lastly, as depicted in Fig. 19, our proposed
method can guarantee Œ∑ above 90% under the setting of
N = {3, 5, 6, 7}, which are align with the results in Fig. 17.
In summary, our proposed method can almost surpass
benchmarks in terms of œÄ%
T under varying settings of Œ±,
M, and N. Our proposed method can also guarantee a
positive incentive to the edge ASP under varying settings.
Our proposed method can ensure Œ∑ = 90% in most settings,
which outperforms benchmarks.
6
CONCLUSION
In this paper, regarding the bonus design problem under
the hidden action of the edge ASP, we propose an LLM-
empowered online learning contract theory to design the
bonus (contract). With our proposed method, the teleopera-
tor can design an effective contract with minimal interaction
with the edge ASP, thereby eliciting high-quality AIGC


--- Page 13 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
13
2
4
6
8
1 0
1 2
0 . 0 0
0 . 0 5
0 . 1 0
0 . 1 5
0 . 2 0
0 . 2 5
0 . 3 0
0 . 3 5
0 . 4 0
 O u r s  
 S e e d
 Z e r o - s h o t  
 B a n d i t
A u g m e n t e d  t e l e o p e r a t o r  u t i l i t y
S i z e  o f  o u t c o m e  s p a c e  ( N  =  7 )
Fig. 14: Comparison results by vary-
ing the size of the outcome space M
in terms of œÄ%
T .
2
4
6
8
1 0
1 2
0 . 0
0 . 5
1 . 0
1 . 5
2 . 0
2 . 5
3 . 0
3 . 5
4 . 0
A u g m e n t e d  e d g e  A S P  u t i l i t y
S i z e  o f  o u t c o m e  s p a c e  ( N  =  7 )
Fig. 15: Comparison results by vary-
ing the size of the outcome space M
in terms of œÄ%
A.
2
4
6
8
1 0
1 2
0 . 0
0 . 1
0 . 2
0 . 3
0 . 4
0 . 5
0 . 6
0 . 7
0 . 8
0 . 9
1 . 0
P e r c e n t a g e  t o  t h e  o p t i m a l
S i z e  o f  o u t c o m e  s p a c e  ( N  =  7 )
Fig. 16: Comparison results by vary-
ing the size of the outcome space M
in terms of Œ∑.
2
3
4
5
6
7
- 0 . 3
- 0 . 2
- 0 . 1
0 . 0
0 . 1
0 . 2
0 . 3
0 . 4
A u g m e n t e d  t e l e o p e r a t o r  u t i l i t y
S i z e  o f  a c t i o n  s p a c e  ( M
 =  1 2 )
 O u r s  
 S e e d
 Z e r o - s h o t  
 B a n d i t
Fig. 17: Comparison results by vary-
ing the size of the action space N in
terms of œÄ%
T .
2
3
4
5
6
7
0 . 0
0 . 5
1 . 0
1 . 5
2 . 0
2 . 5
3 . 0
3 . 5
4 . 0
4 . 5
5 . 0
A u g m e n t e d  e d g e  A S P  u t i l i t y
S i z e  o f  a c t i o n  s p a c e  ( M
 =  1 2 )
Fig. 18: Comparison results by vary-
ing the size of the action space N in
terms of œÄ%
A.
2
3
4
5
6
7
0 . 0
0 . 1
0 . 2
0 . 3
0 . 4
0 . 5
0 . 6
0 . 7
0 . 8
0 . 9
1 . 0
P e r c e n t a g e  t o  t h e  o p t i m a l
S i z e  o f  a c t i o n  s p a c e  ( M
 =  1 2 )
Fig. 19: Comparison results by vary-
ing the size of the action space N in
terms of Œ∑.
services. Specifically, in light of the teleoperator‚Äôs inability to
access the setting (mapping of diffusion steps to the AIGC
service quality and the cost of varying diffusion steps) of
the edge ASP, we modeled the bonus design as an online
learning contract design problem. Next, considering the in-
herent APX-hard property of the problem, we decomposed
the original problem P1 into P2 and P3. Subsequently, due
to P2 remaining NP-hard and with uncertain optimization
variables, we proposed an LLM-empowered P2 solver to
approximate the edge ASP setting. Lastly, we applied the
solution from P2 to solve P3 and derive a near-optimal
contract directly.
We evaluated the scalability, sensitivity, and effectiveness
of our method through simulations.
1) Scalability: With an outcome-space size of 2 and 100
historical logs, performance remained consistent as we
increased the ASP‚Äôs action-space size.
2) Sensitivity: Across mapping coefficients Œ±, our method
maintained a performance of approximately Œ∑ = 90%.
Performance improved with larger action spaces but
declined as outcome spaces increased. It was also sen-
sitive to the number of historical logs.
3) Effectiveness: Compared to benchmarks, we boosted
teleoperator utility by 5 ‚àº40% and always ensured
positive incentives for the ASP. In most settings, we
achieved an efficiency rate of Œ∑ ‚â•90%, demonstrating
robustness and cost efficiency.
REFERENCES
[1]
V.
M.
Research,
‚ÄúGlobal
artificial
intelligence
generated
content
(aigc)
market
size
by
application,
by
technology,
by
end-user
industry,
by
geographic
scope
and
forecast,‚Äù
https://www.verifiedmarketresearch.com/product/artificial-
intelligence-generated-content-aigc-market, Jan. 2024, (Accessed
20 June 2025).
[2]
X. Chen, Z. Hu, and C. Wang, ‚ÄúEmpowering education devel-
opment through aigc: A systematic literature review,‚Äù Educ. Inf.
Technol., vol. 29, no. 13, pp. 17 485‚Äì17 537, Feb 2024.
[3]
J. Chen, C. Yi, H. Du, D. Niyato, J. Kang, J. Cai, and X. Shen,
‚ÄúA revolution of personalized healthcare: Enabling human digital
twin with mobile aigc,‚Äù IEEE Netw., vol. 38, no. 6, pp. 234‚Äì242,
Nov 2024.
[4]
Z. Zhan, Y. Dong, Y. Hu, S. Li, S. Cao, and Z. Han, ‚ÄúVision lan-
guage model-empowered contract theory for aigc task allocation
in teleoperation,‚Äù IEEE Trans. Mob. Comput., vol. 24, no. 8, pp.
7742‚Äì7756, Aug 2025.
[5]
Z. Zhan, Y. Dong, D. M. Doe, Y. Hu, S. Li, S. Cao, L. Fan, and
Z. Han, ‚ÄúDistributionally robust contract theory for edge aigc
services in teleoperation,‚Äù arXiv preprint arXiv:2505.06678, May
2025.
[6]
S. B. Kamtam, Q. Lu, F. Bouali, O. C. Haas, and S. Birrell, ‚ÄúNetwork
latency in teleoperation of connected and autonomous vehicles: A
review of trends, challenges, and mitigation strategies,‚Äù Sensors,
vol. 24, no. 12, p. 3957, Jun 2024.
[7]
H. Du, Z. Li, D. Niyato, J. Kang, Z. Xiong, H. Huang, and
S. Mao, ‚ÄúDiffusion-based reinforcement learning for edge-enabled
ai-generated content services,‚Äù IEEE Trans. Mob. Comput., vol. 23,
no. 9, pp. 8902‚Äì8918, Sep 2024.
[8]
H. Du, Z. Li, D. Niyato, J. Kang, Z. Xiong, X. S. Shen, and D. I.
Kim, ‚ÄúEnabling ai-generated content services in wireless edge
networks,‚Äù IEEE Wirel. Commun., vol. 31, no. 3, pp. 226‚Äì234, Feb
2024.


--- Page 14 ---
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
14
[9]
S. Li, X. Lin, H. Xu, K. Hua, X. Jin, G. Li, and J. Li, ‚ÄúMulti-
agent rl-based industrial aigc service offloading over wireless
edge networks,‚Äù in IEEE Conference on Computer Communications
Workshops (INFOCOM WKSHPS), Vancouver, Canada, May. 2024.
[10] Z. Zhou, L. Pan, and S. Liu, ‚ÄúPotential game-based computation
offloading in edge computing with heterogeneous edge servers,‚Äù
IEEE Trans. Netw. Sci. Eng., vol. 12, no. 1, pp. 290‚Äì301, Jan-Feb 2025.
[11] X. Chen, Y. Zhang, C. Jiang, C. Xu, Z. Yuan, and G.-M. Muntean,
‚ÄúRevenue-oriented optimal service offloading based on fog-cloud
collaboration in sd-wan enabled manufacturing networks,‚Äù IEEE
Trans. Netw. Sci. Eng., vol. 12, no. 2, pp. 1237‚Äì1249, Mar-Apr 2025.
[12] J. Wen, J. Nie, Y. Zhong, C. Yi, X. Li, J. Jin, Y. Zhang, and D. Niyato,
‚ÄúDiffusion model-based incentive mechanism with prospect the-
ory for edge aigc services in 6g iot,‚Äù IEEE Internet Things J., vol. 11,
no. 21, pp. 34 187‚Äì34 201, Nov 2024.
[13] J. Wen, J. Kang, M. Xu, H. Du, Z. Xiong, Y. Zhang, and D. Niyato,
‚ÄúFreshness-aware incentive mechanism for mobile ai-generated
content (aigc) networks,‚Äù in IEEE/CIC International Conference on
Communications in China (ICCC), Dalian, China, Aug. 2023.
[14] J. Wen, Y. Zhang, Y. Chen, W. Zhong, X. Huang, L. Liu, and
D. Niyato, ‚ÄúLearning-based big data sharing incentive in mobile
aigc networks,‚Äù in IEEE Global Communications Conference (GLOBE-
COM), Cape Town, South Africa, Dec. 2024.
[15] D. Ye, S. Cai, H. Du, J. Kang, Y. Liu, R. Yu, and D. Niyato, ‚ÄúOpti-
mizing aigc services by prompt engineering and edge computing:
A generative diffusion model-based contract theory approach,‚Äù
IEEE Trans. Veh. Technol., vol. 74, no. 1, pp. 571‚Äì586, Jan 2025.
[16] J. Li, D. Niyato, and Z. Han, Cryptoeconomics: Economic Mechanisms
behind Blockchains.
Cambridge University Press, 2023.
[17] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang,
‚ÄúThe unreasonable effectiveness of deep features as a perceptual
metric,‚Äù in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, Salt Lake City, UT, Jun. 2018.
[18] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, ‚ÄúImage
quality assessment: From error visibility to structural similarity,‚Äù
IEEE Trans. Image Process., vol. 13, no. 4, pp. 600‚Äì612, Apr 2004.
[19] J. Li, T. Liu, D. Niyato, P. Wang, J. Li, and Z. Han, ‚ÄúContract-
theoretic pricing for security deposits in sharded blockchain with
internet of things (iot),‚Äù IEEE Internet Things J., vol. 8, no. 12, pp.
10 052‚Äì10 070, Jun 2021.
[20] D. M. Doe, J. Li, N. Dusit, Z. Gao, J. Li, and Z. Han, ‚ÄúPromoting the
sustainability of blockchain in web 3.0 and the metaverse through
diversified incentive mechanism design,‚Äù IEEE Open J. Comput.
Soc., vol. 4, pp. 171‚Äì184, Mar 2023.
[21] Y. Liu, H. Du, D. Niyato, J. Kang, Z. Xiong, A. Jamalipour, and
X. Shen, ‚ÄúProsecutor: Protecting mobile aigc services on two-layer
blockchain via reputation and contract theoretic approaches,‚Äù
IEEE Trans. Mob. Comput., vol. 23, no. 12, pp. 10 966‚Äì10 983, Apr
2024.
[22] C.-J. Ho, A. Slivkins, and J. W. Vaughan, ‚ÄúAdaptive contract
design for crowdsourcing markets: Bandit algorithms for repeated
principal-agent problems,‚Äù in Proceedings of the fifteenth ACM con-
ference on Economics and computation, Palo Alto, CA, Jun. 2014.
[23] P. D¬®utting, T. Roughgarden, and I. Talgam-Cohen, ‚ÄúSimple versus
optimal contracts,‚Äù in Proceedings of the ACM Conference on Eco-
nomics and Computation, Phoenix, AZ, Jun. 2019.
[24] F. Bacchiocchi, M. Castiglioni, A. Marchesi, and N. Gatti, ‚ÄúLearn-
ing optimal contracts: How to exploit small action spaces,‚Äù in
International Conference on Learning Representations (ICLR), Vienna,
Austria, May. 2024.
[25] G. Guruganesh, Y. Kolumbus, J. Schneider, I. Talgam-Cohen, E.-
V. Vlatakis-Gkaragkounis, J. Wang, and S. Weinberg, ‚ÄúContracting
with a learning agent,‚Äù in Advances in Neural Information Processing
Systems (NeurIPS), Vancouver, Canada, Dec. 2024.
[26] T. Wang, P. D¬®utting, D. Ivanov, I. Talgam-Cohen, and D. C. Parkes,
‚ÄúDeep contract design via discontinuous networks,‚Äù in Advances
in Neural Information Processing Systems (NeurIPS), New Orleans,
LA, Dec. 2023.
[27] F. Liu, X. Tong, M. Yuan, X. Lin, F. Luo, Z. Wang, Z. Lu, and
Q. Zhang, ‚ÄúEvolution of heuristics: Towards efficient automatic
algorithm design using large language model,‚Äù in Proceedings of the
41st International Conference on Machine Learning (ICML), Vienna,
Austria, Jul. 2024.
[28] H. Ye, J. Wang, Z. Cao, F. Berto, C. Hua, H. Kim, J. Park, and
G. Song, ‚ÄúReevo: Large language models as hyper-heuristics with
reflective evolution,‚Äù in Advances in Neural Information Processing
Systems (NeurIPS), Vancouver, Canada, Dec. 2024.
[29] S. Yao, F. Liu, X. Lin, Z. Lu, Z. Wang, and Q. Zhang, ‚ÄúMulti-
objective evolution of heuristic using large language model,‚Äù in
Proceedings of the AAAI Conference on Artificial Intelligence, Philadel-
phia, PA, Feb-Mar. 2025.
[30] O. D. Hart and B. Holmstrm, The theory of contracts.
MIT press,
1986.
[31] J. E. Stiglitz, ‚ÄúThe theory of‚Äù screening,‚Äù education, and the distri-
bution of income,‚Äù The American economic review, vol. 65, no. 3, pp.
283‚Äì300, Jun 1975.
[32] B. Holmstr¬®om, ‚ÄúMoral hazard and observability,‚Äù The Bell journal
of economics, vol. 10, no. 1, pp. 74‚Äì91, Spring 1979.
[33] P. Bolton and M. Dewatripont, Contract theory.
MIT press, 2004.
[34] T. Liu, J. Li, F. Shu, H. Guan, Y. Wu, and Z. Han, ‚ÄúIncentive
mechanism design for two-layer wireless edge caching networks
using contract theory,‚Äù IEEE Trans. Serv. Comput., vol. 14, no. 5, pp.
1426‚Äì1438, Sep-Oct 2021.
[35] Z. Yu, Z. Chang, L. Wang, and G. Min, ‚ÄúContract-based in-
centive design for resource allocation in edge computing-based
blockchain,‚Äù IEEE Trans. Netw. Sci. Eng., vol. 11, no. 6, pp. 6143‚Äì
6156, Nov-Dec 2024.
[36] S. Wang, D. Ye, X. Huang, R. Yu, Y. Wang, and Y. Zhang, ‚ÄúCon-
sortium blockchain for secure resource sharing in vehicular edge
computing: A contract-based approach,‚Äù IEEE Trans. Netw. Sci.
Eng., vol. 8, no. 2, pp. 1189‚Äì1201, Apr-Jun 2021.
[37] Q. Wang, S. Guo, G. Liu, L. Yang, and C. Pan, ‚ÄúMotilearn: Contract-
based incentive mechanism for heterogeneous edge collaborative
training,‚Äù IEEE Trans. Netw. Sci. Eng., vol. 9, no. 4, pp. 2895‚Äì2909,
Jul-Aug 2022.
[38] Y. Zhang and Z. Han, Contract theory for wireless networks.
Springer, 2017.
[39] Y. Zhang, N. H. Tran, D. Niyato, and Z. Han, ‚ÄúMulti-dimensional
payment plan in fog computing with moral hazard,‚Äù in 2016
IEEE International Conference on Communication Systems (ICCS),
Shenzhen, China, Dec. 2016.
[40] M. Tian, Y. Chen, Y. Liu, Z. Xiong, C. Leung, and C. Miao, ‚ÄúA con-
tract theory based incentive mechanism for federated learning,‚Äù
arXiv preprint arXiv:2108.05568, Aug 2021.
[41] Y. Ding, L. Wang, Z. Yang, and Z. Han, ‚ÄúMoral hazard based
incentive mechanism for cooperative caching in multi-hop com-
munications,‚Äù in IEEE Globecom Workshops, Singapore, Dec. 2017.
[42] Y. Zhang, Y. Gu, M. Pan, N. H. Tran, Z. Dawy, and Z. Han, ‚ÄúMulti-
dimensional incentive mechanism in mobile crowdsourcing with
moral hazard,‚Äù IEEE Trans. Mob. Comput., vol. 17, no. 3, pp. 604‚Äì
616, Mar 2018.
[43] Z. Li, Q. Yang, and Z. Li, ‚ÄúOptimal incentive mechanism design for
short video collaborative caching system in mobile edge networks
based on contract theory,‚Äù Peer-to-Peer Networking and Applications,
vol. 18, no. 120, pp. 1‚Äì12, Mar 2025.
[44] L. Ismail, M. Qaraqe, A. Ghrayeb, and D. Niyato, ‚ÄúEnhancing
trust and security in the vehicular metaverse: A reputation-based
mechanism for participants with moral hazard,‚Äù in IEEE Wireless
Communications and Networking Conference (WCNC), Dubai, United
Arab Emirates, Apr. 2024.
[45] P. D¬®utting, M. Feldman, I. Talgam-Cohen et al., ‚ÄúAlgorithmic con-
tract theory: A survey,‚Äù Found. Trends Theor. Comput. Sci., vol. 16,
no. 3-4, pp. 211‚Äì412, Dec 2024.
[46] B. Zhu, S. Bates, Z. Yang, Y. Wang, J. Jiao, and M. I. Jordan,
‚ÄúThe sample complexity of online contract design,‚Äù in 24th ACM
Conference on Economics and Computation, London, UK, Jul. 2023.
[47] A. Cohen, A. Deligkas, and M. Koren, ‚ÄúLearning approximately
optimal contracts,‚Äù in International Symposium on Algorithmic Game
Theory, Colchester, UK, Sep. 2022.
[48] F. Bacchiocchi, M. Castiglioni, N. Gatti, and A. Marchesi, ‚ÄúLearn-
ing optimal contracts with small action spaces,‚Äù Artificial Intelli-
gence, vol. 344, p. 104334, Jul 2025.
[49] H. Du, R. Zhang, D. Niyato, J. Kang, Z. Xiong, D. I. Kim, X. S. Shen,
and H. V. Poor, ‚ÄúExploring collaborative distributed diffusion-
based ai-generated content (aigc) in wireless networks,‚Äù IEEE
Netw., vol. 38, no. 3, pp. 178‚Äì186, May 2024.
[50] K. P. Sinaga and M.-S. Yang, ‚ÄúUnsupervised k-means clustering
algorithm,‚Äù IEEE Access, vol. 8, pp. 80 716‚Äì80 727, Apr 2020.
