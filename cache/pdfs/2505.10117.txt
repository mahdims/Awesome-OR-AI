--- Page 1 ---
Learning Virtual Machine Scheduling in Cloud Computing
through Language Agents
Jiehao Wu1â€ , Ziwei Wang2â€ , Junjie Sheng1, Wenhao Li3, Xiangfeng Wang1,4*, Jun Luo2*
1School of Computer Science and Technology, East China Normal University, Shanghai, 200062,
China.
2Antai College of Economics and Management, Shanghai Jiao Tong University, Shanghai, 200030,
China.
3School of Computer Science and Technology, Tongji University, Shanghai, 200092, China.
4Key Laboratory of Mathematics and Engineering Applications, MoE, East China Normal
University, Shanghai, 200062, China.
*Corresponding author(s). E-mail(s): xfwang@cs.ecnu.edu.cn; jluo ms@sjtu.edu.cn;
Contributing authors: jhwu@stu.ecnu.edu.cn; zoey wangzw@sjtu.edu.cn; jarvis@stu.ecnu.edu.cn;
whli@tongji.edu.cn;
â€ These authors contributed equally to this work.
Abstract
In cloud services, virtual machine (VM) scheduling is a typical Online Dynamic Multidimensional Bin Packing
(ODMBP) problem, characterized by large-scale complexity and fluctuating demands. Traditional optimization
methods struggle to adapt to real-time changes, domain-expert-designed heuristic approaches suffer from rigid
strategies, and existing learning-based methods often lack generalizability and interpretability. To address these
limitations, this paper proposes a hierarchical language agent framework named MiCo, which provides a large
language model (LLM)-driven heuristic design paradigm for solving ODMBP. Specifically, ODMBP is formulated
as a Semi-Markov Decision Process with Options (SMDP-Option), enabling dynamic scheduling through a two-stage
architecture, i.e., Option Miner and Option Composer. Option Miner utilizes LLMs to discover diverse and useful non-
context-aware strategies by interacting with constructed environments. Option Composer employs LLMs to discover
a composing strategy that integrates the non-context-aware strategies with the contextual ones. Extensive experiments
on real-world enterprise datasets demonstrate that MiCo achieves a 96.9% competitive ratio in large-scale scenarios
involving more than 10,000 virtual machines. It maintains high performance even under nonstationary request flows
and diverse configurations, thus validating its effectiveness in complex and large-scale cloud environments.
Keywords: Large Language Model; Online Bin-packing; Cloud Computing; Semi-Markov Decision Process; Virtual Machine
Scheduling
1 Introduction
Cloud computing has been a mainstream way of using computational resources. Cloud computing allows users to
access computing resources over the Internet, without managing the underlying infrastructure. This on-demand,
scalable model has made cloud computing an attractive and mainstream option for both business and individual
consumers. According to Gartner (2025), worldwide end-user spending on public cloud services was $595.7 billion
1
arXiv:2505.10117v2  [cs.LG]  19 May 2025


--- Page 2 ---
Figure 1: (Color online) An Example of VM Scheduling
PM Cluster
VM Sched
Agent
Request Sequence
CPU
memory
PM
PM
PM
1
2
N
VM Request :Â 
IDï¼š453e36ee7ba934a10bbbd8170543c549
Creation/Deletionï¼šCreation
Resource Neededï¼š(16 Cores,Â 32 GiB)
Responseï¼š
Create (16, 32) VM on PM2
Cluster Statusï¼š
N PMs, PM2Â remains (0 Core,16 GiB)
available for allocation
Note. The process can be succinctly described as follows: The VM scheduling agent receives a sequence of VM requests, including creation or deletion commands
and resource requirements. The agent then determines the appropriate PM for VM allocation. In this instance, a new VM is assigned to PM index 2. The decision is
subsequently communicated to the PM cluster, which executes the resource allocation and VM creation accordingly.
in 2024, and is expected to be $723.4 billion in 2025. In the backend, service providers face the challenge of on-
demand dynamic resource allocation for virtual machines (VMs), which we prefer to call VM scheduling, as shown in
Figure 1. As cloud infrastructure expands, the complexity and scale of VM scheduling increase, making it critical for
business efficiency and technological advancement. Correspondingly, the operations management (OM) community
has developed a strong interest in on-demand VM allocation because this issue is closely related to emergency patient
allocation and customer management within the OM field (Chen et al. 2023). Efficiently managing VMs is crucial to
maximizing physical machine (PM) utilization, reducing operational costs, and enhancing system performance (Pietri
and Sakellariou 2016).
The VM scheduling problem can be framed as an online dynamic multidimensional bin packing (ODMBP)
problem, a well-known NP-hard challenge. Figure 1 illustrates the process of handling VM requests, with the objective
of allocating each incoming VMs to a suitable PM to maximize the number of scheduled VMs. The problem is
characterized by its online, and dynamic nature: requests arrive sequentially with uncertain future information and
requests can arrive or leave at any time (Coffman et al. 1983). Additionally, both VMs and PMs have multidimensional
resource requirements such as CPU and memory (Christensen et al. 2017). Figure 2 illustrates the large-scale and
nonstationary of VM scheduling problem, as evidenced by statistical analysis based on 130,000 VM requests. The
temporal fluctuations in VM resource requirements significantly expand the optimization search space, necessitating
the development of an advanced algorithm to address the ODMBP problem under these conditions.
Traditional approaches for solving the ODMBP problem typically fall into three categories: optimization-based,
learning-based, and heuristic algorithms. Optimization-based methods excel in structured scenarios. Offline formu-
lations using mixed-integer programming with branch-and-bound algorithms achieve optimal solutions when item
sequences are fully known (Martello et al. 2000, Zhang et al. 2020). For large-scale problems, decomposition meth-
ods (Pisinger and Sigurd 2007, CË†otÂ´e et al. 2021) and infinite-server models with randomized policies (Stolyar 2013)
provide theoretical guarantees. However, their reliance on complete information and deterministic assumptions lim-
its scalability in dynamic systems with stochastic arrivals (Berg and Denton 2017). Learning-based approaches,
particularly reinforcement learning (RL), offer online adaptability. Hybrid architectures like RL-enhanced constraint
2


--- Page 3 ---
Figure 2: (Color online) (a) VM Arrival Time Distribution; (b) Average Duration Time; and (c) Diverse Demand
Size Distributions Characteristics of VMs over One Year Period
(a)
(b)
(c)
Note. We divide the VM requests into 31 groups (x-axis), each group lasts 14 days, and calculate the proportion of request arrival times within each group relative to
the total requests, as shown in Figure 2(a). The average duration per group is displayed in Figure 2(b).VMs are categorized into small, medium, and large based on
CPU/memory dimensions, and the proportions of these request types over the total requests are depicted in Figure 2(c).
programming (e.g., Jiang et al. 2021) and policy networks combined with simulated annealing (e.g., Cai et al. 2019)
demonstrate improved solution quality for large-scale ODMBP problems. However, these methods suffer from slow
convergence and poor generalization across workload patterns (Sheng et al. 2022). Heuristic algorithms dominate
industrial practice due to their operational simplicity and efficiency. Classical heuristics like First-Fit, Best-Fit, and
Next-Fit remain foundational, with established worst-case bounds providing theoretical guarantees (Azar et al. 2013,
Azar and Vainstein 2019). Industry implementations have evolved to address complex real-world constraints. Microsoft
employs multiple rule-driven heuristics in its Protean system to deliver high resource utilization and low-latency VM
allocation (Hadary et al. 2020). However, these domain experts-designed heuristics exhibit a critical limitation: their
static rule configurations struggle to maintain performance under nonstationary demand patterns, even when aug-
mented with stochastic extensions (Bentley et al. 1984). We defer much of the discussion of the literature from these
methods to Online Appendix A.
Language agents (or Large Language Model-based agents, short for LLM-based agents) provide a transformative
approach to heuristic algorithm design (Wang et al. 2024), as well as innovative solutions in operations research
domains, such as problem modeling and decision-making (Tang et al. 2024, Bertsimas and Margaritis 2024). Unlike
human experts constrained by cognitive biases and limited search space, LLMs leverage pre-trained knowledge to
explore infinite-dimensional algorithmic spaces (Li 2025). As Figure 3 illustrates, the process begins with natural
language prompts that encode problem requirements. These prompts are translated into executable heuristic functions
through code-based function representation, which enables interpretable strategy encoding. Moreover, language agents
can further enhance code performance through reflection-driven iterative optimization via in-context learning and
reinforcement mechanisms that refine strategies based on historical decisions.
We propose a hierarchical language agent framework, which is called MiCo, for automated VM scheduling
that replaces traditional heuristic design. Given the large-scale and nonstationary characteristics of the ODMBP
problem, it presents a highly complex planning challenge. In light of the substantial limitations exhibited by existing
LLMs in addressing complex planning tasks (Valmeekam et al. 2022, Kambhampati et al. 2024, Valmeekam et al.
2024), we formulate ODMBP as a Semi-Markov Decision Process with Options (SMDP-Option) (Sutton et al. 1999,
Baykal-GÂ¨ursoy and GÂ¨ursoy 2010), which serves as the foundational hierarchical framework for MiCo. In SMDP-
Option, an option constitutes a temporally abstract action that executes over several time steps. The framework
3


--- Page 4 ---
Figure 3: (Color online) The Overview of Context-Aware Scheduler - MiCo
User
PromptÂ 
â€¦
â€¦
â€¦
â€¦
Can you design a context-aware schedulerÂ 
in PythonÂ for me, using the historical VM data:
(1) Scenario Generation Tool
(2) Option Miner
(3) Pruning Tool
(4) Option Composer
Context-Aware Scheduler
Â  Â  ELIF Scenario m
IF Scenario 1
cpu_weight = 0.6
mem_weight = 0.4
cpu_score = cpu_weight * (1 / (1 + np.exp...
mem_score = mem_weight * (1 / (1 + np.exp...
lookahead_score = 0
for i in range(min_lookahead_distance , ...):
Â Â Â Â next_item = (item [0] * i, item [1] * i)
lookahead_score = lookahead_factor * (looka...
Â LLM-based Function Optimization
Prompt
LLM
Function
Code 1 (for Scenario 1)
Scenario-SpecifiedÂ 
Policies
lookahead_score = 0
for i in range(min_lookahead_distance , ...):
Â Â Â Â next_item = (item [0] * i, item [1] * i)
lookahead_score = lookahead_factor * (looka...
Code S (for Scenario S)
cpu_weight = 0.6
mem_weight = 0.4
cpu_score = cpu_weight * (1 / (1 + np.exp...
mem_score = mem_weight * (1 / (1 + np.exp...
implements a master policy to orchestrate these options, governing both their initiation and termination conditions.
This temporal abstraction paradigm effectively addresses the unpredictability inherent in nonstationary environments
while substantially extending the planning horizon over which the algorithm can operate effectively. Moreover, this
hierarchical approach facilitates more robust decision-making across extended temporal scales than the original MDP.
The MiCo consists of two language agents: the Option Miner and the Option Composer which correspond to
two fundamental learning processes in SMDP-Option (Sutton et al. 1999, Li et al. 2023): automated option discovery
and master policy learning, respectively (see Figure 3). The Miner emulates domain experts in designing potential
scheduling strategies for candidate options, while the Composer orchestrates these strategies by implementing a
context-aware scheduler. Both the Miner and the Composer rely on LLM-based function optimization. This approach
employs code as the function representation and leverages the reflection, in-context learning, and code generation
capabilities of LLMs. These capabilities enable iterative function optimization (For more details, see Section 3.1).
Specifically, the Miner utilizes a scenario-generation tool to slice raw VM sequences into discrete temporal
segments, thereby creating diverse scenarios (defined as contextually coherent slices of VM sequences). It then mines
optimal scheduling policies for each scenario through LLM-based function optimization. These scenarios, together
with their corresponding optimal scheduling policies, collectively constitute the options. Subsequently, the Composer
composes the scenario-specific scheduling policies to form the master policy. It initially employs a pruning tool to
4


--- Page 5 ---
derive a minimal policy set, followed by LLM-based function optimization to develop a context-aware scheduler
(represented by code) that dynamically selects among these functions based on historical VM patterns. For more details
about Miner and Composer, see Section 3.3 and Section 3.4. Through this integration, our framework automatically
discovers context-aware VM scheduling policies, thereby establishing an alternative approach to heuristic function
design.
We evaluate the proposed framework through comprehensive experiments on a real-world VM dataset, aiming to
uncover policies that can handle large-scale, dynamic environments effectively. The results demonstrate the superior
performance of our framework compared to several established baselines, including Best-Fit, First-Fit, Hindsight, and
SchedRL algorithms. In various scenarios, our method consistently achieves significant performance improvements,
showcasing its robustness across different operational conditions. Moreover, the heuristic functions generated by the
large language model exhibit conceptual similarities with traditional optimization methods found in the literature,
further validating the effectiveness of LLMs in resource allocation tasks. This not only highlights the practical value
of LLMs but also illustrates their potential to transform the way we approach complex, multidimensional scheduling
problems.
This paper advances online bin packing for cloud computing resource management through three key contributions:
1) Innovative LLM-Driven Heuristic Framework for ODMBP: We formulate VM scheduling as an ODMBP problem
and pioneer an LLM-driven heuristic design paradigm. Unlike conventional expert-dependent approaches, our method
automatically generates context-aware scheduling policies through the reasoning capabilities of LLMs, eliminating
heavy reliance on domain-expert predefined rules while maintaining decision interpretability. 2) Hierarchical Decision
Architecture with Smart Composition: We develop MiCo, a hierarchical language agent framework using SMDP-
Option. This architecture addresses system nonstationarity and scalability through automatic policy generation and
dynamic policy composition. Experimental results demonstrate the superiority of MiCo over both heuristic baselines
and pure RL approaches, achieving 96.9% average performance ratio against the upper bound obtained from Gurobi.
3) Open-source Implementation to facilitate further exploration of LLM-based heuristic optimization. The package we
built enables researchers to extend our framework to various combinatorial optimization problems, thereby fostering
innovation in operations management research and practice.
2 Preliminaries
To address the nonstationary arrivals and service durations, we employ an event-driven approach, where the system
states update only when new creation requests occur. The scheduler observes the system states ğ‘ ğ‘¡at decision time
step ğ‘¡and selects a placement action ğ‘ğ‘¡based on the observed states at these time steps. Each decision time step ğ‘¡, for
ğ‘¡âˆˆğ‘‡, corresponds to the arrival of a creation request whose resource demand follows the nonstationary distribution
ğºğ‘¡. If no request arrives, the system state remains unchanged. When a VM deletion request arrives, the specified
VM is removed, but the released resources are only updated upon the next creation request, ensuring synchronization
between resource updates and allocation decisions.
5


--- Page 6 ---
We model the VM scheduling problem as a finite-horizon, discrete-time discounted-reward MDP with Exogenous
Inputs (Sinclair et al. 2023), which is characterized by the tuple âŸ¨S, A, P, RâŸ©, to capture the online sequential
decision-making process.
- State Space (S): The state ğ‘ ğ‘¡âˆˆS at time ğ‘¡integrates both system resources and request context:
ğ‘ ğ‘¡=

{ğ’„ğ‘ğ‘š
ğ‘–,ğ‘¡}ğ‘
ğ‘–=1
|     {z     }
PM resources
, ğ‘£ğ‘šğ‘¡, ğ‘ğ‘¡, ğ’„ğ‘£ğ‘š
ğ‘¡
|         {z         }
request
,
Î¦ğ‘¡
|{z}
allocation history

.
The system consists of ğ‘PMs with multidimensional capacity vectors ğ’„ğ‘ğ‘š
ğ‘–
âˆˆZğ‘‘(where ğ‘–âˆˆ{1, ..., ğ‘}, ğ‘‘denotes
the dimension), and dynamically arriving VMs represented as tuples, âŸ¨ğ‘£ğ‘š, ğ’„ğ‘£ğ‘š, ğ‘âŸ©. ğ‘£ğ‘šdenotes the request ID,
ğ’„ğ‘£ğ‘šâˆˆZğ‘‘denotes the required multidimensional resource demand (e.g., CPU, memory), and ğ‘indicates the operation
type (0 = release, 1 = create). In this context, ğ’„ğ‘ğ‘šcaptures the endogenous system state (PM resource availability),
while ğ’„ğ‘£ğ‘šrepresents the exogenous input (incoming VM resource demand). The mapping hash table Î¦, which denotes
the VM-to-PM allocation information, is defined later in the transitions part.
- Action Space (A): At each decision step ğ‘¡(i.e., ğ‘ğ‘¡= 1), the scheduler selects an action ğ‘ğ‘¡âˆˆA to place the VM.
The action space is defined as the set of all feasible placement decisions A = {1, 2, Â· Â· Â· , ğ‘}. For deletion requests
(i.e., ğ‘ğ‘¡= 0): The PM hosting the VM is automatically located by Î¦ğ‘¡, requiring no explicit action.
- Reward (R): The reward function ğ‘Ÿ(Â·, Â·) is designed similarly to scoring functions in heuristic algorithms, where
each PM is assigned a priority. The general reward function can be written as:
ğ‘Ÿ ğ‘ ğ‘¡, ğ‘ğ‘¡
 = Isuccess + ğ›¼Â· Util ğ‘ğ‘ğ‘š
ğ‘–,ğ‘¡
 + ğ›½Â· Type ğ‘ğ‘£ğ‘š
ğ‘¡
,
where Isuccess = 1 if ğ‘ğ‘¡â‰ 0, Util ğ‘ğ‘ğ‘š
ğ‘–,ğ‘¡
 measures PMâ€™s remaining resource proportion and Type ğ‘ğ‘£ğ‘š
ğ‘¡
 scales with
VMâ€™s resource magnitude at discrete time setp ğ‘¡(ğ›¼, ğ›½represent penalty coefficients). For simplicity, in this paper, we
only consider the matching reward, i.e., ğ‘Ÿ= 1 for each successful allocation. The environment terminates after the
first unsuccessful allocation.
- Transitions (P): The transition updates occur at request arrivals. When a creation request arrives (ğ‘ğ‘¡= 1), if the
action ğ‘ğ‘¡selects a PM ğ‘–(i.e., ğ‘ğ‘¡= ğ‘–) and the PM has sufficient resources (i.e., ğ’„ğ‘ğ‘š
ğ‘–,ğ‘¡â‰¥ğ’„ğ‘£ğ‘š
ğ‘¡
), the resources of the PM
are updated:
ğ’„ğ‘ğ‘š
ğ‘–,ğ‘¡+1 = ğ’„ğ‘ğ‘š
ğ‘–,ğ‘¡âˆ’ğ’„ğ‘£ğ‘š
ğ‘¡
,
and the mapping table Î¦ is updated to include the new VM ğ‘£ğ‘šğ‘¡mapped to the PM ğ‘–: Î¦ğ‘¡+ = Î¦ğ‘¡âˆª{ğ‘£ğ‘šğ‘¡â†¦â†’ğ‘–}, where
ğ‘¡+ represents immediate updates within the ğ‘¡period.
When a deletion request arrives (ğ‘ğ‘¡= 0), the system locates the PM that is currently hosting the VM ğ‘£ğ‘šğ‘¡using
the mapping table Î¦ğ‘¡(ğ‘£ğ‘šğ‘¡) = ğ‘–. Considering that more than one deletion may arrive between two creation arrivals,
we use a queue D to store the VMs to be deleted, Dğ‘¡+ = Dğ‘¡âˆª{ğ‘£ğ‘šğ‘¡}. The actual release of resources occurs when
the next creation request is processed. Specifically, the resources of all VMs in the release queue Dğ‘¡are added back
6


--- Page 7 ---
to the PM ğ‘–that hosted them. The update is given by:
ğ’„ğ‘ğ‘š
ğ‘–,ğ‘¡+1 = ğ’„ğ‘ğ‘š
ğ‘–,ğ‘¡+
âˆ‘ï¸
ğ‘£âˆˆDğ‘¡
ğ’„ğ‘£ğ‘š
ğ‘£
Â· I(Î¦ğ‘¡(ğ‘£) = ğ‘–),
where I(Â·)=1 if Î¦ğ‘¡(ğ‘£) = ğ‘–.
- Objective Function: The objective is to find a scheduling policy ğœ‹that maximizes the expected long-run discounted
reward:
max
ğœ‹
Jğœ‹(ğ‘ ğ‘¡, ğ‘ğ‘¡) = Eğ’„ğ‘£ğ‘šâˆ¼ğºğ‘¡,ğ‘ğ‘¡âˆ¼ğœ‹(Â·|ğ‘ ğ‘¡)
Ãğ‘‡
ğ‘¡=0 ğ›¾ğ‘¡ğ‘Ÿ(ğ‘ ğ‘¡, ğ‘ğ‘¡)

,
(1)
subject to the resource capacity constraints ğ’„ğ‘ğ‘š
ğ‘–
(ğ‘¡+ 1) â‰¥0 for all ğ‘–âˆˆğ‘, where ğ›¾âˆˆ(0, 1] is the discounting factor.
3 The MiCo Framework
We propose a hierarchical language agent framework for function optimization Eq. (1) in Section 2. It relies on five
steps: 1) introduction to function optimization with language agents; 2) problem reformulation via SMDP-Option; 3)
micro-level learning, Scenario-Specific Option Miner for option discovery; 4) macro-level learning, Option Composer
to plan over options; 5) algorithm implementation and execution combining all steps (in Section 3.1-3.5, respectively).
3.1 Context-Aware Function Optimization with LLM Agent
3.1.1 LLM-based Function Optimization
In order to search for the optimal policy for function Eq. (1), we characterize a policy as a function code representation
and leverage LLMs for automated code generation and algorithmic optimization. The language agent implements
policy improvement through structured code evolution, as expressed in Eq. (2). We enhance the LLM reasoning
process through top-M contrastive prompting, where the language agent analyzes and synthesizes insights from
multiple high-performing policies. Upon receiving a prompt that includes top-M policies {ğœ‹(ğ‘š)}ğ‘€
ğ‘š=1) encapsulated
in code form, along with a role description and a task description, a language agent is capable of generating a refined
and presumably more optimal policy ğœ‹â€², which is also represented in code form.
ğœ‹â€² = LLM

role des, task des, {ğœ‹(ğ‘š)}ğ‘€
ğ‘š=1, ğœ‰

,
(2)
where ğœ‰denotes the noise space that controls the exploration of LLM, i.e., the temperature.
The framework operates through a closed-loop optimization process as shown in Figure 4, maintaining a dynamic
set of top-M candidate policies at each iteration. The process begins with an initial policy ğœ‹(0), which is provided in the
form of code along with a role description and a task description. During each iteration, the language agent generates
enhanced candidate policies {ğœ‹â€²} through contrastive analysis of the current top-M policies {ğœ‹(ğ‘š)}ğ‘€
ğ‘š=1 (selected by
Jğœ‹ranking). Then, the next-generation population is formed by selecting the updated top-M performers from the
union of historical and newly generated policies. The evolutionary process continues iteratively until a termination
condition is met.
7


--- Page 8 ---
Figure 4: (Color online) The Framework of LLM-based Function Optimization
Prompt Engineering
Initial Code Snippet
Code Snippet 2Â 
import numpy as np
def priority (bin, item):...
Code Snippet 1 
dynamic_factor = (bin [0] /Â 
(bin [0] + bin [1]...
if notÂ 
terminate
Â  Â  Â  Â  Â Improvement viaÂ 
Â  Â  Â  Â  Pretrained LLMs
Code Snippet N
adjusted_cpu_weight =
cpu_weight * dynam...
Â  Â  Â  Â  Â Evaluating
3.1.2 Context-Aware Policy Learning
To validate the performance of algorithms generated from the language agent, we establish benchmark comparisons
using Best-Fit as a heuristic baseline and FunSearch as the LLM-based optimization framework. As an enhanced
framework for combinatorial optimization (e.g., online bin packing (Romera-Paredes et al. 2024)), FunSearch evolves
scheduling policies through iterative LLM refinement. Moreover, we find that the context-independent scheduling
policies generated by FunSearch perform inadequately in VM scheduling. Directly learning context-aware schedulers
(Contextual FunSearch) also proves challenging.
We conduct comprehensive evaluations using the Huawei Cloud VM Scheduling Dataset â€œHuawei-East-1â€ (see
Section 4.1 for implementation details). As demonstrated in Figure 5(b), the context-independent FunSearch method
achieves marginal performance improvements (1.2%) over the baseline Best-Fit algorithm. However, these results
reveal a fundamental limitation of context-independent scheduling approaches in dynamic environments. The inherent
challenges of VM scheduling are further illustrated by the system load volatility patterns shown in Figure 5(a),
where nonstationary demand characteristics (defined as significant statistical property changes over time) cause
context-independent language agents to exhibit suboptimal adaptation capabilities.
Figure 5: (Color online) (a) Nonstationary Conditions of Size Distributions and (b) Performance of Language Agent
(a)
(b)
Best-Fit
FunSearch
Contextual FunSearch
Sequence Specific FunSearch
90
91
92
93
94
95
96
97
Performance Ratio (%)
8


--- Page 9 ---
To address these limitations, we explore context-aware approaches by incorporating nonstationary data as input,
thereby enabling the model to identify the dynamic and changing conditions. We first attempt to train a single policy
across multiple VM sequences by incorporating real-time concatenating historical VM states as contextual input
(denoted as Contextual FunSearch). This architecture theoretically enables dynamic adaptation through temporal
context awareness. However, as shown in Figure 5(b), the policy fails to maintain stable performance across varying
demand phases. The result reveals potential reasons: 1) The limited prompt context insufficiently captures the overall
patterns and nonstationary trends, making it difficult to extract key information and design the algorithm; 2) The
policy optimization converges to suboptimal solutions where the model prioritizes recent sequence characteristics at
the expense of long-term pattern generalization.
To more clearly evaluate the actual impact of context in the VM Scheduling problem, we further conduct one-
to-one policy training on each VM sequence in the dataset. Each policy effectively captures the unique operational
characteristics of its target VM sequence, demonstrating almost 4% average performance improvement over Best-Fit
methods. The results show that when policies are individually customized for specific scenarios, their performance
improves significantly (the performance of Sequence-Specific FunSearch in Figure 5(b)). However, these findings
also reveal that this approach sacrifices cross-environment robustness, highlighting a critical scalability bottleneck
for real-world deployment. Our current function optimization approach suggests that LLMs still struggle to balance
performance across all scenarios and achieve substantial overall improvement. The difficulty lies in discovering a
generalizable policy that can adapt to all scenarios in the highly dynamic VM scheduling environment within a
high-dimensional function space.
To address the challenges posed by context dependency and nonstationary environments, we propose a hierarchical
language agent framework. Initially, we reformulate the original problem as an SMDP-Options. At the micro-level,
each option represents a temporally abstract action that is executed over several time steps. The process of searching
for scheduling policies for these options is referred to as the Option Miner. At the macro-level, a master policy plans
over these options, which we term the Option Composer.
3.2 Problem Reformulation: SMDP-Option
We reformulate the problem as an SMDP-Option, which represents the problem in a multi-time scale manner
(Sutton et al. 1999), as illustrated in Figure 6. This hierarchical decomposition simplifies computations by abstracting
sequences of actions into macro-level decision-making units.
Figure 6: Diagram of MDP and Options over MDP
action
option
MDP
Options
over MDP
Time t
Time t
Note. The trajectory of an MDP consists of sequential, discrete-time transitions. The option constitutes a temporally abstract action that executes over several time steps.
Options enable an MDP trajectory to be analyzed in either way.
9


--- Page 10 ---
The term options generalizes primitive actions to include temporally extended courses of action. Unlike Markov
options, policies and termination conditions of SMDP-Options may be dependent on all prior contexts since the option
was initiated. In other words, the policy and termination conditions are functions of possible histories. In general,
an option is initiated at some time, say ğ‘¡, determines the actions selected for some number of steps, say ğœ, and then
terminates in ğ‘ ğ‘¡+ğœ. Let Î© denote the history space. At each intermediate time ğ‘¡+ ğ‘¡ğ‘š, 0 â‰¤ğ‘¡ğ‘š< ğœ, the decisions of
a semi-Markov option may depend on the entire sequence â„ğ‘¡
ğ‘¡+ğ‘¡ğ‘š: {ğ‘ ğ‘¡, ğ‘ğ‘¡, ğ‘Ÿğ‘¡+1, ğ‘ ğ‘¡+1, ğ‘ğ‘¡+1, ..., ğ‘Ÿğ‘¡+ğ‘¡ğ‘š, ğ‘ ğ‘¡+ğ‘¡ğ‘š}. We further
define the options:
Definition 1 (Option) An option ğ‘œis a tuple âŸ¨I, ğœ‹, ğ›½âŸ©. Input Set: I = S specifies the states where the option can be initiated.
Intra-Option Policy ğœ‹: Î© Ã— A â†’[0, 1], Termination Condition ğ›½: Î© â†’[0, 1]. The option terminates if either of the following
conditions is met: if there exists any PM ğ‘–whose capacity ğ’„ğ‘ğ‘š
ğ‘–
(ğ‘¡+ ğ‘¡ğ‘š) is less than the required capacity ğ’„ğ‘£ğ‘š(ğ‘¡+ ğ‘¡ğ‘š) at time ğ‘¡+ ğ‘¡ğ‘š,
or if the number of steps since the initiation of the option, i.e., ğ‘¡ğ‘šâ‰¥ğœ.
Lemma 1 (Sutton et al. (1999)) Given any MDP M = âŸ¨S, A, P, RâŸ©and option set O, the decision process that selects options
from O forms a Semi-MDP Mâ€² = âŸ¨S, O, Pâ€², Râ€²âŸ©, where Pâ€²(ğ‘ â€², ğœ|ğ‘ , ğ‘œ) denotes the probability reaching ğ‘ â€² after ğœsteps under
option ğ‘œ, Râ€²(ğ‘ , ğ‘œ) denotes the expected cumulative discounted reward.
As formally established by Lemma 1, the integration of MDPs with temporal abstraction through options inherently
constitutes an SMDP. Within VM scheduling, an option can be formally defined as a temporally extended action
hierarchy that executes coordinated scheduling operations across multiple decision epochs. This theoretical framework
preserves the Markovian property demonstrated in Section 2 while enabling hierarchical control. The fundamental
MDP characteristics of VM scheduling remain intact, with options providing enhanced temporal abstraction through
multi-step action compositions.
At the micro level, the intra-option policy ğœ‹dictates the scheduling actions ğ‘ğ‘¡âˆ¼ğœ‹(Â· | ğ‘ ğ‘¡) to maximize the
cumulative rewards until the option ğ‘œterminates in ğ‘ ğ‘¡+ğœ. Referring to long-term reward Eq. (1), the objective is to
maximize the reward part of the option ğ‘œfor any state ğ‘ âˆˆS:
max
ğœ‹
Jğœ‹(ğ‘œ, ğ‘ ğ‘¡) = Eğ‘ 0âˆ¼I,ğ‘ğ‘¡âˆ¼ğœ‹(Â·|ğ‘ ğ‘¡)
"ğœâˆ’1
âˆ‘ï¸
ğ‘š=0
ğ›¾ğ‘šğ‘Ÿ(ğ‘ ğ‘¡+ğ‘š, ğ‘ğ‘¡+ğ‘š|ğ‘œ)
#
.
(3)
At the macro level, when initiated in a state ğ‘ ğ‘¡, the master policy over options ğœ‡: S Ã— O â†’[0, 1] selects an option
ğ‘œğ‘¡âˆˆOğ‘ such that ğ‘œğ‘¡âˆ¼ğœ‡(Â· | â„ğ‘¡). The objective is to maximize the overall reward:
max
ğœ‡
Jğœ‡(ğ‘ ğ‘¡) = Eğ‘ 0âˆ¼I,ğ‘œâˆ¼ğœ‡(Â·|â„Â·
ğ‘¡)
h
Jğœ‹(ğ‘œ, ğ‘ ğ‘¡) + ğ›¾ğœ(ğ‘œ) Jğœ‡(ğ‘ ğ‘¡+ğœ(ğ‘œ))
i
.
(4)
The hierarchical MDP framework improves scalability and adaptability by dividing the problem into multiple
scenarios, each corresponding to a specific VM size distribution. The objective is to optimize the policy ğœ‡, ğœ‹at two
levels, respectively, to maximize the cumulative reward over all scenarios.
10


--- Page 11 ---
Figure 7: (Color online) The Overview of Option Miner
Scenario K
Scenario 2
Scenario 1
small
medium-
small
medium-
medium
medium-
large
large
Â Scenario Generation ToolÂ 
Code 2 (for Scenario 2)
dynamic_factor = (bin [0] / (bin [0] + bin [1]...
adjusted_cpu_weight = cpu_weight * dynam...
adjusted_mem_weight = mem_weight * (1 - ...
Code 1 (for Scenario 1)
cpu_weight = 0.6
mem_weight = 0.4
cpu_score = cpu_weight * (1 / (1 + np.exp...
mem_score = mem_weight * (1 / (1 + np.exp...
Scenario-SpecifiedÂ 
Policies
Code K (for Scenario K)
lookahead_score = 0
for i in range(min_lookahead_distance , ...):
Â Â Â Â next_item = (item [0] * i, item [1] * i)
lookahead_score = lookahead_factor * (looka...
Â LLM-based Function
Optimization
Scheduler
Template
Option Miner
Source Trajectory
Splitting
Note. We initially partition the entire VM sequence, which includes VMs of sizes small, medium-small, medium, medium-large, and large, into ğ¾distinct scenarios,
each characterized by unique size distribution patterns. For each scenario, we apply the scheduler prompt template to guide LLM-based function optimization, generating
a scenario-specific policy. Each policy is represented in code format, resulting in a total of ğ¾policies.
3.3 Options Discovery: Scenario-Specific Option Miner
The option discovery stage aims to find the scheduling policies that can be composed to obtain good performance in
scheduling. As shown in Figure 7, the option discovery process employs a two-stage paradigm combining scenario
generation and language-agent guided option learning. First, a scenario generation tool partitions the VM request
stream into ğ¾contextual segments, each characterizing distinct demand patterns. Policy learning subsequently operates
within these curated scenarios, enabling the focused optimization of scenario-specific policies. This methodology
directly addresses the challenge of option evaluation under nonstationary constraints by ensuring policy adaptation to
individual scenario characteristics.
Scenario Generation.
Given a request sequence
 
ğ‘£ğ‘šğ‘–, ğ’„ğ‘£ğ‘š
ğ‘–
, ğ‘ğ‘–
 	ğ‘‡
ğ‘–=1 that covers the entire planning horizon, we employ a temporal partition-
ing mechanism to generate ğ¾contiguous scenarios that capture evolving system dynamics. Specifically, the sequence
is segmented using a sliding window approach with configurable temporal granularity. Let ğ‘Šdenote the window
length (in time units) and ğ‘†ğ¿represent the step length between consecutive windows. For non-overlapping partitions
(ğ‘†ğ¿= ğ‘Š), the timeline is divided into ğ¾= âŒŠğ‘‡/ğ‘ŠâŒ‹distinct scenarios. The overlap ratio ğ‘Šâˆ’ğ‘†ğ¿
ğ‘Š
controls the trade-off
between granularity and redundancy. To preserve temporal independence and avoid redundancy between scenarios,
our approach utilizes non-overlapping windows by setting ğ‘Š= ğ‘†ğ¿, ensuring that each partition is distinct and free of
overlap. The resulting scenarios {ğ‘†1, Â· Â· Â· , ğ‘†ğ¾} represent a feasible temporal abstraction of system evolution.
Language-Agent Guided Option Learning.
Once the scenarios have been generated, the Option Miner employs an LLM-based function optimization approach,
which is mentioned in Section 3.1, to learning options within scenarios. The learning process follows an iterative
evaluation-improvement loop to refine scheduling strategies for nonstationary VM request scenarios.
- Policy Evaluation Phase: We first randomly sample ğ‘›ğ‘ representative task sequences from each scenario ğ‘†ğ‘˜with
associated PM capacities {ğ’„ğ‘ğ‘š
ğ‘–
}ğ‘
ğ‘–=1. Then, for each candidate policy ğœ‹(ğ‘›)
ğ‘œğ‘˜in scenario ğ‘†ğ‘˜, we evaluate the policy
performance using the reward function Jğœ‹from Eq. (3), calculated as the average reward across all ğ‘›ğ‘ sequences.
- Policy Improvement Phase: We first replace the template prompt with the scheduler template as shown in Online
Appendix C and encode the top-M performing policies

ğœ‹(ğ‘›,ğ‘š)
ğ‘œğ‘˜
	ğ‘€
ğ‘š=1 (ranked by Jğœ‹) into the prompt. Then, we
11


--- Page 12 ---
generate improved candidate policies via LLM reasoning as follows. Finally, we retain the top-M candidate policies
achieving the highest Jğœ‹(i.e., Eq. (3)) for the next iteration.
ğœ‹(ğ‘›+1)
ğ‘œğ‘˜
= LLM

ğœ‹(ğ‘›,ğ‘š)
ğ‘œğ‘˜
(itemğ‘£ğ‘š, binğ‘ğ‘š)
	ğ‘€
ğ‘š=1, role des, task des, ğœ‰

.
The loop iteratively refines option policies until achieving performance convergence. The final set of iterated
scheduling policies forms a library of options, denoted as {ğ‘œğ‘˜}ğ¾
ğ‘˜=1, which are designed to dynamically adapt to specific
resource demand scenarios.
3.4 Planning over Options: Option Composer
Prior to composing policies, we conduct option pruning to reduce policy search space. The Option Composer
establishes a two-tier decision hierarchy to coordinate the discovered options in nonstationary environments. At the
macro level, it maintains a master policy ğœ‡that selects options ğ‘œbased on real-time context features, while activated
options autonomously handle micro-level resource allocation through their intra-policies ğœ‹ğ‘œ.
Option Pruning.
To further reduce the search space and enhance policy efficiency, the policy set is pruned based on a heuristic filter
criterion. A policy is preserved if either (i) itâ€™s improvement attains at least the threshold ğœsingle in any one evaluation
scenario (single-scenario excellence), or (ii) its performance is no worse than the scenario-wide average in at least a
ğ‘fraction of the ğ¾scenarios (cross-scenario robustness).
Oâˆ—=
ï£±ï£´ï£²
ï£´ï£³
ğ‘œğ‘–

max
1â‰¤ğ‘—â‰¤ğ¾
  J ğœ‹ğ‘–| ğ‘†ğ‘—
 âˆ’ğœğ‘†ğ‘—
ğœğ‘†ğ‘—
 â‰¥ğ›¿
âˆ¨
1
ğ¾
ğ¾
âˆ‘ï¸
ğ‘—=1
I
h
J ğœ‹ğ‘–| ğ‘†ğ‘—
 â‰¥
Â¯Jğ‘†ğ‘—
i
â‰¥ğ‘
ï£¼ï£´ï£½
ï£´ï£¾
.
(5)
The pruning criteria ensure that the selected schedulers are optimal and diverse, effectively handling different scenarios.
Language-Agent Guided Master Policy Learning.
The macro-level policy ğœ‡selects the optimal option ğ‘œâˆˆOâ€² based on the observed system state. We train the
contextual composer to plan over options through LLM-based function optimization (Section 3.1). Unlike the context-
independent strategy optimization of Option Miner, the training of Option Composer takes the VM sequence with
Figure 8: (Color online) The Overview of Option Composer
evaluating
sorting
purning
Pruning ToolÂ 
Code Snippet 1
cpu_weight = 0.6
mem_weight = 0.4
cpu_score = cpu_weight * (1 / (1 + np.exp...
mem_score = mem_weight * (1 / (1 + np.exp...
Code Snippet K'
lookahead_score = 0
for i in range(min_lookahead_distance , ...):
Â Â Â Â next_item = (item [0] * i, item [1] * i)
lookahead_score = lookahead_factor * (looka...
Code SnippetsÂ 
for Composer
LLM-based Function
Optimization
Context-Aware Scheduler
Template
Option Composer
Context-Aware
Scheduler
IF Scenario 1
cpu_weight = 0.6
mem_weight = 0.4
cpu_score = cpu_weight * (1 / (1 + np.exp...
mem_score = mem_weight * (1 / (1 + np.exp...
Â  Â  ELIF Scenario k
lookahead_score = 0
for i in range(min_lookahead_distance , ...):
Â Â Â Â next_item = (item [0] * i, item [1] * i)
lookahead_score = lookahead_factor * (looka...
cpu_weight = 0.6
mem_weight = 0.4
cpu_score = cpu_weight * (1 / (1 + np.exp...
mem_score = mem_weight * (1 / (1 + np.exp...
Code 2 (for Scenario 2)
dynamic_factor = (bin [0] / (bin [0] + bin [1]...
adjusted_cpu_weight = cpu_weight * dynam...
adjusted_mem_weight = mem_weight * (1 - ...
Code 1 (for Scenario 1)
cpu_weight = 0.6
mem_weight = 0.4
cpu_score = cpu_weight * (1 / (1 + np.exp...
mem_score = mem_weight * (1 / (1 + np.exp...
Scenario-SpecifiedÂ 
Policies
Code K (for Scenario K)
lookahead_score = 0
for i in range(min_lookahead_distance , ...):
Â Â Â Â next_item = (item [0] * i, item [1] * i)
lookahead_score = lookahead_factor * (looka...
Note. We employ a heuristic pruning tool to streamline the ğ¾scenario-specific policies, each tailored to a single scenario, down to ğ¾â€² refined policies. By applying the
context-aware scheduler prompt template to guide LLM-based function optimization, we design a composer capable of dynamically switching among these ğ¾â€² policies.
12


--- Page 13 ---
a sampling length of ğ¿before the initial point as contexts, which is denoted as contextğ‘£ğ‘š. The composer agent
learns a master policy ğœ‡to dynamically coordinate scheduling strategies across scenarios, governed by an iterative
evaluation-improvement loop.
- Policy Evaluation Phase: We randomly sample ğ‘›ğ‘ task sequences from the full request stream {

ğ‘£ğ‘šğ‘–, ğ’„ğ‘£ğ‘š
ğ‘–
, ğ‘ğ‘–

}ğ‘‡
ğ‘–=1,
covering diverse exogenous system states, along with PM capacities {ğ’„ğ‘ğ‘š
ğ‘–
}ğ‘
ğ‘–=1. Then, for the master policy ğœ‡(ğ‘›) at
iteration ğ‘›, we evaluate coordination performance Jğœ‡from Eq. (4), calculated as the average reward across all ğ‘›ğ‘ 
sequences.
- Policy Improvement Phase: Replace the template prompt with the context-aware scheduler template as shown in
Online Appendix C. Additionally, it encodes the current top-M policies

ğœ‡(ğ‘›,ğ‘š)
ğ‘œğ‘˜
	ğ‘€
ğ‘š=1 into the prompt. Then, generate
improved candidate policies via LLM reasoning as below. Finally, retain the top-M candidate policy achieving the
highest Jğœ‡for the next iteration.
ğœ‡(ğ‘›+1) = LLM

ğœ‡(ğ‘›,ğ‘š) (itemğ‘£ğ‘š, binğ‘ğ‘š, contextğ‘£ğ‘š)
	ğ‘€
ğ‘š=1, role des, task des, ğœ‰

.
The master policy ğœ‡is iteratively improved by the Option Composer, which searches for the best strategy to
maximize Eq. (4). This training process adjusts ğœ‡over multiple scenarios, optimizing policy selection to improve
scheduling performance over dynamic conditions.
3.5 Execution Stage
The complete execution workflow integrates both hierarchical components through Algorithm 1. The process begins
with initial state ğ‘ 0 and iteratively performs option selection and execution:
1. Option Selection: At decision epoch ğ‘¡, the composer observes current state ğ‘ ğ‘¡and selects option ğ‘œğ‘¡âˆ¼ğœ‡âˆ—(ğ‘ ğ‘¡)
from Oâ€².
2. Option Execution: The agent selects the action following intra-policy ğ‘ğ‘¡âˆ¼ğœ‹ğ‘œğ‘¡(ğ‘ ğ‘¡) until termination condition
ğ›½triggers (resource violation or timeout).
3. State Transition: The environment transitions to ğ‘ ğ‘¡+ğœbased on option execution outcomes.
4. Adaptive Update: The composer dynamically updates ğœ‡âˆ—based on performance feedback using Eq. (4).
This closed-loop process enables continuous policy improvement through real-time interactions between the
composer and miner components. The hierarchical decomposition achieves 4-11% higher scheduling efficiency
compared to heuristics and RL baselines in our experiments (Section 4).
4 Experiments
In this section, we evaluate the performance of our proposed algorithm for VM scheduling. First, we describe the
experimental setup and algorithm performance. Next, we analyze the impact of two key factors: (1) the effect of option
13


--- Page 14 ---
Algorithm 1 Hierarchical LLM Agent for Function Optimization
1: Input: VM request sequence {

ğ‘£ğ‘šğ‘–, ğ’„ğ‘£ğ‘š
ğ‘–
, ğ‘ğ‘–

}ğ‘‡
ğ‘–=1, PM cluster {ğ’„ğ‘ğ‘š
ğ‘–
}ğ‘
ğ‘–=1, top-M policies ğ‘€, option count ğ¾,
quantile ğ‘, iteration number of Option-Miner ğ‘€ğ‘€ğ‘–and Option-Composer ğ‘€ğ¶ğ‘œ.
2: Output: Optimized policy ğœ‡âˆ—, option set Oâ€²
3: Split requests {

ğ‘£ğ‘šğ‘–, ğ’„ğ‘£ğ‘š
ğ‘–
, ğ‘ğ‘–

}ğ‘‡
ğ‘–=1 into {ğ‘†ğ‘˜}ğ¾
ğ‘˜=1
4: function Option-Miner(ğ‘†ğ‘˜, {ğ‘ğ‘ğ‘š
ğ‘–
}ğ‘
ğ‘–=1, ğ¾, ğ‘ƒ)
5:
Initialize empty policy pool ğœ‹ğ‘™ğ‘–ğ‘ ğ‘¡â†{}
6:
for ğ‘˜= 1 to ğ¾do
7:
// Initialize the ğ‘˜-th optionâ€™s starting policy
8:
ğœ‹(0)
ğ‘œğ‘˜â†SeedPolicy(ğ‘†ğ‘˜, {ğ‘ğ‘ğ‘š
ğ‘–
}ğ‘
ğ‘–=1)
9:
ğ‘ (0) â†Score(ğœ‹(0)
ğ‘œğ‘˜)
10:
Insert (ğœ‹(0)
ğ‘œğ‘˜, ğ‘ (0)) into ğœ‹ğ‘™ğ‘–ğ‘ ğ‘¡
11:
for ğ‘›= 0 to ğ‘€ğ‘€ğ‘–âˆ’1 do
12:
TopList â†TopP(ğœ‹ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘ƒ)
13:
ğœ‹(ğ‘›+1)
ğ‘œğ‘˜
â†LLM(TopList)
âŠ²LLM-based function optimization
14:
ğ‘ (ğ‘›+1) â†Evaluate(ğœ‹(ğ‘›+1)
ğ‘œğ‘˜
)
15:
Insert (ğœ‹(ğ‘›+1)
ğ‘œğ‘˜
, ğ‘ (ğ‘›+1)) into ğœ‹ğ‘™ğ‘–ğ‘ ğ‘¡
16:
end for
17:
end for
18:
return ğœ‹ğ‘™ğ‘–ğ‘ ğ‘¡
19: end function
20: Prune options: Oâ€² â†
n
ğ‘œğ‘–
 1
ğ¾
Ãğ¾
ğ‘—=1 I

J (ğœ‹ğ‘–|ğ‘†ğ‘—) â‰¥Â¯Jğ‘†ğ‘—

â‰¥ğ‘
o
21: function Option-Composer(Oâ€², {ğ‘ğ‘£ğ‘š}, {ğ‘ğ‘ğ‘š
ğ‘–
}ğ‘
ğ‘–=1, ğ‘€)
22:
Initialize empty composer pool ğœ‡ğ‘™ğ‘–ğ‘ ğ‘¡â†{}
23:
ğœ‡(0) â†Composer(Oâ€², {ğ‘ğ‘£ğ‘š}, {ğ‘ğ‘ğ‘š
ğ‘–
}ğ‘
ğ‘–=1)
24:
ğ‘ (0) â†Score(ğœ‡(0))
25:
Insert (ğœ‡(0), ğ‘ (0)) into ğœ‡ğ‘™ğ‘–ğ‘ ğ‘¡
26:
for ğ‘›= 0 to ğ‘€ğ¶ğ‘œâˆ’1 do
27:
TopList â†TopP(ğœ‡ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘ƒ)
28:
ğœ‡(ğ‘›+1) â†LLM(TopList)
âŠ²LLM-based function optimization
29:
ğ‘ (ğ‘›+1) â†Evaluate(ğœ‡(ğ‘›+1))
30:
Insert (ğœ‡(ğ‘›+1), ğ‘ (ğ‘›+1)) into ğœ‡ğ‘™ğ‘–ğ‘ ğ‘¡
31:
end for
32:
Let ğœ‡âˆ—be the highest-scoring composition in ğœ‡ğ‘™ğ‘–ğ‘ ğ‘¡
33:
return ğœ‡âˆ—
34: end function
35: {ğ‘œğ‘˜} â†Option-Miner({ğ‘†ğ‘˜, {ğ’„ğ‘ğ‘š
ğ‘–
}ğ‘
ğ‘–=1, ğ¾, ğ‘€)
36: ğœ‡âˆ—â†Option-Composer(Oâ€², {

ğ‘£ğ‘šğ‘–, ğ’„ğ‘£ğ‘š
ğ‘–
, ğ‘ğ‘–

}ğ‘‡
ğ‘–=1, {ğ’„ğ‘ğ‘š
ğ‘–
}ğ‘
ğ‘–=1, ğ‘€)
pruning on policy switching and optimization across multiple scenarios, and its potential to ease the optimization
process; (2) the robustness of the algorithm under varying environmental conditions. Finally, we investigate whether
the strategies discovered by the large language model align with traditional heuristic approaches.
4.1 Experimental Setup
To evaluate the performance of the MiCo algorithm in solving the bin packing problem, we use VM scheduling datasets
â€œHuawei-East-1â€1 from Huawei Cloud (Sheng et al. 2021) and â€œAzurePublicDatasetV2â€2 from Microsoft Azure
(Cortez et al. 2017). For comparison, we benchmark MiCo against traditional heuristic algorithms and reinforcement
learning methods.
1https://github.com/huaweicloud/VM-placement-dataset
2https://github.com/Azure/AzurePublicDataset/blob/master/AzurePublicDatasetV2.md
14


--- Page 15 ---
Figure 9: (Color online) (a) VM Type Classification and (b) Scenario Features in Huawei Datasets
(a)
memory
Small
12
32
64
96
12
32
64
96
CPU
Medium_Small
Medium
Medium_Large
Large
196
128
0
(b)
Scenario 1
Scenario 2
Scenario 3
Scenario 4
Scenario 5
Scenario 6
0.0
0.2
0.4
0.6
0.8
1.0
Ratio
Types
Small
Medium_Small
Medium_Medium
Medium_Large
Large
Note. VMs are categorized into five groups: small, medium, large, and two additional categories for cases where CPU and memory requests do not align, as shown in
9a. According to the classification, the Huawei data set is split into six segments, called six scenarios, as shown in 9b.
All experiments were conducted using Python 3.7, implemented on a server configured with Ubuntu 20.04.4 LTS.
The computational platform comprised four AMD EPYC 7R32 48-Core processors, totaling 192 cores, 514 GB of
RAM, and an NVIDIA A100 GPU.
Data Description.
The Huawei dataset â€œHuawei-East-1â€ (almost 130,000 VM requests) spans over a year, exhibiting significant fluctu-
ations in CPU and memory demands. In contrast, the Azure dataset â€œAzurePublicDatasetV2â€ (almost 400,000 VM
requests) includes VM requests with multiple resource attributes (CPU, memory, storage, and lifetime) but covers
only one month, exhibiting relatively stationary resource patterns. To ensure consistency in the Huawei dataset, we
choose to retain only the CPU and memory data from the two-week Azure dataset. This approach aligns with the con-
figuration of the Huawei dataset and allows for a direct comparison of VM characteristics and performance metrics
across both datasets. The training set and test set are divided in a ratio of 5:1.
Due to the nonstationarity of the Huawei VM datasets, we partition the VM sequence into six scenarios in
chronological order without altering its structure. Each scenario contains 20,000 VM requests and exhibits distinct
VM distribution characteristics, as shown in Figure 9. For instance, Scenario 1 shows a dominant proportion of Small
VMs, while Scenario 5 has a notable spike in Medium Large VMs. When we train the Option Miner, we refer to these
segments as six scenarios, highlighting the variability in VM size distribution across different periods.
Baselines.
Our baseline methods include both traditional heuristics (i.e., Best-Fit, First-Fit, Hindsight) and learning-based
approaches (i.e., SchedRL). Best-Fit selects the server with the highest current allocation rate to maximize resource
utilization (Johnson 1973). First-Fit assigns VMs to the first available server based on index order, without considering
load distribution (Johnson 1973). Hindsight sorts VM requests by descending lifetime and allocates each to the most
suitable machine, minimizing idle periods (Sinclair et al. 2023). SchedRL leverages reinforcement learning for multi-
NUMA VM scheduling, formulating the problem as a structured combinatorial optimization task within a learning
15


--- Page 16 ---
Table 1: Environmental Parameters and Algorithm Parameters List
Parameter
Value
Environmental Parameters
Number of PMs ğ‘
50
Language agent LLM
GPT-4
Tokens of each iteration in Option Miner
1,000
Tokens of each iteration in Option Composer
1,000
General Algorithm Parameters
Temperature ğœ‰
0.8
Top-M
2
Transfer step of option ğœ
50
Number of initial options ğ¾(corresponding to scenarios)
6
Number of independent experiments ğ‘š
30
Option Miner Parameters
Iterations in Option Miner ğ‘€ğ‘€ğ‘–
300
Seed heuristic ğœ‹(0)
Best-fit algorithm
Option Composer Parameters
Iterations in Option Composer ğ‘€ğ¶ğ‘œ
300
Sample length of VM sequence ğ¿(context)
200
framework (Sheng et al. 2022). Additionally, the exact solution obtained by solving the mixed-integer programming
using Gurobi will serve as the offline optimal solution (Gurobi Optimization, LLC 2024).
Environmental Parameters and Algorithm Parameters.
To facilitate result reproduction, we provide environmental parameters (i.e., settings) and algorithm parameters in
Table 1. The settings are consistent with the preceding motivation experiments 3.1.
Performance Metric.
To explore the algorithm limits, we use the performance ratio as our metric on the training set. The performance ratio
is calculated by comparing the scheduled length of the online algorithm to that of the offline algorithm and expressed
as a percentage:
Performance Ratio =
 Scheduled lengthonline
Scheduled lengthoffline

Ã— 100%.
The scheduled length, defined as the total number of VMs processed within a given capacity, serves as a key
performance indicator according to the reward function in Section 2. Furthermore, when analyzing the robustness of
code generation, we include the code valid ratio as an extra performance metric. The code valid ratio is calculated as
follows:
Code Valid Ratio =

Number of Valid Code Samples
Total Number of Generated Code Samples

Ã— 100%.
4.2 Results
This section systematically presents three aspects of evaluation: (1) primary performance outcomes (Section 4.2.1),
ablation experiments (Section 4.2.2), and robustness analysis (Section 4.2.3) of MiCo. Our main analysis focuses on the
16


--- Page 17 ---
â€œHuawei-East-1â€ due to its diverse operational characteristics that best represent real-world scenarios. To demonstrate
the performance gap between our algorithm and the offline optimal solution, we exclusively compute the theoretical
upper bounds using the Gurobi optimizer on the training set. Complete results on the â€œAzurePublicDatasetV2â€ are
provided in Online Appendix B. The best MiCo-generated heuristics are collected in Online Appendix D.
4.2.1 Performance Analysis of MiCo
As shown in Table 2, our algorithm demonstrates consistent superiority in the Huawei dataset. First, our proposed
MiCo algorithm surpasses other baseline methods in both overall performance and proximity to the offline upper bound
across almost all scenarios, it achieves the highest optimality ratio (96.9%) relative to Gurobiâ€™s solution. Second,
when compared with reinforcement learning approaches, our method exhibits amplified advantages. Specifically, it
outperforms SchedRL by 11.1% in mean performance, with particularly notable gaps in complex scenarios (32.6%
improvement in Scenario 4). Third, scenario analysis reveals critical insights. In Scenario 1, homogeneous small
requests dominate, allowing all algorithms to approach the upper bound (within a 0.1% gap), as simplified scheduling
eliminates the need for strategic differentiation. In Scenario 5, the presence of heterogeneous workloads (5 VM types)
exposes the limitations of baseline algorithms, resulting in a performance degradation of more than 20%, while MiCo
demonstrates high stability.
To demonstrate the generalization capability of our algorithmic framework, we also sample sequences from the
Huawei datasets that do not overlap with the training set. These unseen sequences were evaluated, as illustrated in
Figure 10. It can be observed that SchedRL has a relatively concentrated data distribution, with most of its performance
concentrated at lower Scheduled Length, indicating an inferior overall performance. The performance distributions
of First-Fit and HindSight are somewhat similar, but HindSight has a shorter lower whisker, suggesting more stable
performance. Both Best-Fit and MiCo deliver the highest performance. However, MiCo has the highest median and a
higher box position, showing that it generally outperforms the other algorithms. The results show that, even on new
datasets, our framework MiCo is able to maintain a certain degree of performance superiority.
In addition to the Huawei dataset, we also evaluated our algorithm on the Azure public dataset, as shown in Online
Appendix Table B1, Figure B3. The results confirm that our algorithm consistently delivers significant performance
Table 2: Performance Comparison of Best-Fit, First-Fit, HindSight, and MiCo Algorithms with Gurobi Solutions
Across Diverse Scenarios in Huawei Training Dataset
Algorithm
Scenario
S1
S2
S3
S4
S5
S6
Mean
Best-Fit
100.0%
99.3%
87.4%
93.2%
74.4%
84.9%
92.6%
First-Fit
100.0%
99.2%
87.2%
91.8%
63.4%
76.5%
89.7%
HindSight
99.9%
99.2%
87.3%
91.7%
67.5%
78.0%
90.5%
SchedRL
99.9(Â±0.0)%
97.8(Â±0.0)%
85.2(Â±1.8)%
77.3(Â±0.1)%
51.0(Â±6.1)%
69.5(Â±3.0)%
85.8(Â±0.8)%
MiCo
99.9%
99.4%
95.3%
92.1%
83.6%
99.3%
96.9%
Note. Due to the randomness inherent in reinforcement learning, the results of SchedRL exhibit randomness, with plus and minus signs indicating
the confidence intervals. The mean result is calculated by dividing the average score of the online algorithm across all scenarios by the average
score of the offline algorithm across all scenarios, which is effective for evaluating the overall effectiveness of the algorithm. The mean results of
the remaining tables are calculated in the same way.
17


--- Page 18 ---
Figure 10: (Color online) Boxplot of Scheduled Length for Best-Fit, First-Fit, HindSight, SchedRL, and MiCo
Algorithms in Huawei Test Dataset
Best-Fit
First-Fit
SchedRL
HindSight
MiCo
Algorithm
250
500
750
1000
1250
1500
1750
2000
Scheduled Length
improvements across both datasets. Although the Azure dataset covers only two weeks with limited variability,
indicating stationary scenarios, our algorithm still outperforms others.
4.2.2 Ablation Study
The primary motivation behind our design of the hierarchical language agent framework MiCo is the finding that
directly optimizing the results using LLMs without distinguishing the background and in the absence of context yields
poor outcomes. Therefore, Section 3.1 presents the results of the ablation experiment for the segmentation of the
scenario. To validate the effectiveness of our MiCo in intelligently selecting heuristic algorithms under nonstationary
scenarios, we conduct two additional ablation experiments.
Ablation Study on Option Composer.
As shown in Table 3, policies trained for specific scenarios (scenario-specified policies) demonstrate optimal perfor-
mance when their corresponding scene identifiers are known. For instance, Policy4 achieves perfect accuracy (100.0%)
in S4 but exhibits a performance drop in cross-scenario evaluations, underscoring the limitations of scene-bound
strategies. This validates our hypothesis that static policies lack adaptability to dynamic environments, necessitating
an intelligent policy switching mechanism like our Option Composer.
Additionally, the performance of the scenario-specified policy chosen randomly (i.e., Random Policy) is lower
than that of the policy chosen intelligently (i.e., MiCo) across all scenarios (mean 93.4% vs 96.9%). This confirms
that random exploration fails to capture scene-specific optimization opportunities.
Ablation Study on Option Pruning.
To assess the impact of option pruning, we train the MiCo algorithm without pruning (i.e., MiCo& w/o-pruning) for
each scenario. After pruning, the policy set of MiCo retains the original Policy2, Policy3, Policy5, and Policy6. As
shown in Table 3, the mean results demonstrate that the performance of the MiCo algorithm is superior to that of the
18


--- Page 19 ---
Table 3: Performance of Best-Fit, Scenario-Specified Policies (Policyğ‘–, ğ‘–âˆˆ{1, . . . , 6}), Ran-
dom Policy, MiCo without Pruning and MiCo in Huawei Training Dataset
Algorithm
Scenario
S1
S2
S3
S4
S5
S6
Mean
Best-Fit
100.0%
99.3%
87.4%
93.2%
74.4%
84.9%
92.6%
Policy1
100.0%
99.3%
87.4%
93.3%
69.7%
80.5%
91.3%
Policy2
99.9%
99.5%
89.3%
92.0%
70.4%
82.6%
91.9%
Policy3
91.3%
97.7%
95.5%
93.6%
74.5%
87.1%
90.5%
Policy4
85.6%
87.0%
84.1%
100.0%
69.7%
87.0%
84.7%
Policy5
90.8%
95.4%
86.1%
91.6%
89.4%
87.0%
90.8%
Policy6
84.8%
86.8%
81.7%
93.7%
79.7%
99.8%
87.4%
Random Policy
99.8%
97.8%
95.0%
93.2%
74.3%
87.0%
93.2%
MiCo&w/o-pruning
99.9%
97.7%
91.1%
99.8%
74.6%
99.6%
95.4%
MiCo
99.9%
99.4%
95.3%
92.1%
83.6%
99.3%
96.9%
baseline Best-Fit and scenario-specified policies, regardless of whether pruning is applied or not. By using pruning
to identify policies that can address multiple scenarios and reduce the selection space, we observed more substantial
performance gains (mean 95.4% vs. 96.9%). Notably, MiCo without pruning outperforms MiCo under S4 (99.8% vs.
92.1%). This is because MiCo did not select Policy4, and the strategies from other scenarios may have overlapped with
this one, leading to a performance decline. This also explains why MiCo did not achieve the Best-Fit performance in
Table 2. Besides, we further investigate the convergence rates of the proposed algorithms MiCo, the details are shown
in Online Appendix C.
4.2.3 Robustness Analysis
To examine whether our algorithm is sufficiently robust under nonstationary conditions, we conduct experiments
across varying sample lengths, temperatures, and different language agents. The sample length affects the ability of
the language model to capture contextual information. The temperature parameter, which controls the stochasticity
and creativity of language agents, is varied to assess its impact on the performance of the algorithm. Additionally, we
test the performance using multiple language agents to evaluate their generalizability and robustness across different
model architectures.
Robustness Analysis of Sample Length.
The sample length denotes the sequence length of input to the language model. We examine the sequence lengths
ğ¿ranging from 100 to 800. The findings indicate that both excessively long (more than 600) and short (less than
400) sample lengths can degrade algorithmic performance, potentially due to the impact of sample length on the
ability of the model to extract information. However, our evaluation of different ğ¿values demonstrates that all tested
configurations surpass the baseline Best-Fit threshold, indicating the robustness of performance across the sampled
range.
19


--- Page 20 ---
Figure 11: (Color online) Impact of Sample Length ğ¿(Ranging from 100 to 800) on Algorithm Performance
100
200
300
400
500
600
700
800
Sample length
92.5
93.0
93.5
94.0
94.5
95.0
95.5
96.0
Performance Ratio (%)
MiCo
Best Fit
Note. Figure shows the performance ratio in different sample lengths, with all tested configurations exceeding the baseline Best-Fit, thereby confirming the robustness
across the evaluated range.
Robustness Analysis of Temperature.
Temperature is a key factor influencing the stochasticity and creativity of LLMs. We test temperatures of 0.2, 0.4, 0.6,
0.8, and 1.0, as shown in Figure 12. While higher temperatures initially enhance output diversity, performance peaks
at moderate temperatures (around 0.6 to 0.8), after which it declines. Additionally, the valid ratio, which reflects the
proportion of valid outputs, decreases significantly as temperature increases, suggesting that higher temperatures may
introduce excessive randomness, reducing the reliability of outputs. Notably, our framework demonstrates remarkable
robustness within this range.
Robustness Analysis of Language Agent.
We utilize several large language models, including Deepseek Coder V1, Deepseek Coder v2, GPT 3.5 Turbo, and
GPT 4, all of which contributed to performance improvements. As presented in Table 4 and Figure 13. GPT-4
demonstrates superior overall performance compared to DeepSeek Coder V1, DeepSeek Coder V2, and GPT 3.5
Turbo. Notably, GPT 4, GPT 3.5 Turbo, and DeepSeek Coder V2 achieve substantially higher code accuracy rates
Figure 12: (Color online) Impact of Temperatures on (a) Algorithm Performance and (b) Valid Ratio
(a)
0.2
0.4
0.6
0.8
1.0
Temperature Settings
92
93
94
95
96
97
98
99
100
Performance Ratio(%)
MiCo
Best Fit
(b)
0.2
0.4
0.6
0.8
1.0
Temperature Settings
40
50
60
70
80
90
100
Code Valid ratio(%)
MiCo
20


--- Page 21 ---
Figure 13: (Color online) Impact of Different Language Agents on (a) Algorithm Performance and (b) Valid Ratio
(a)
GPT 3.5 Turbo
GPT 4
DeepSeek Coder V1
DeepSeek Coder V2
88
90
92
94
96
Performance Ratio (%)
(b)
GPT 3.5 Turbo
GPT 4
DeepSeek Coder V1
DeepSeek Coder V2
50
60
70
80
90
Code Valid Ratio (%)
relative to DeepSeek Coder V1. This trend suggests that larger models, equipped with a greater number of parameters,
tend to yield improved code accuracy.
As shown in Figure 13, it is evident that GPT 3.5 Turbo exhibits a decline in the number of valid code outputs
toward the end of the sampling process. This behavior is not observed in GPT-4 or DeepSeek Coder V1, as the latter
typically generates shorter code, while GPT-4 maintains superior code generation and processing capabilities. Both
GPT 4 and GPT 3.5 Turbo tend to produce more complex code, often incorporating multiple parameters and intricate
logic structures. This complexity can make code optimization particularly challenging, especially for GPT 3.5 Turbo.
Notably, GPT-4 frequently nests functions within other functions, which can complicate code modification when the
nesting becomes excessive.
4.3 Interpretable Insights
To investigate whether the policies searched by LLMs correspond to traditional heuristic policies, we review classic
scheduling literature and identify well-established heuristic algorithms. We find that the policies evolved by large
language models indeed resonate with these traditional methods. The specific correspondences are summarized as
follows.
Table 4: Performance Metrics across Different Language Agents, including Deepseek
Coder V1, Deepseek Coder V2, GPT 3.5 Turbo, and GPT 4
Algorithm
Scenario
S1
S2
S3
S4
S5
S6
Mean
Best-Fit
100.0%
99.3%
87.4%
93.2%
74.5%
84.9%
92.6%
MiCo-GPT 3.5 Turbo
100.0%
99.3%
95.1%
93.6%
83.6%
85.1%
94.5%
MiCo-GPT 4
99.9%
99.4%
95.3%
92.1%
83.6%
99.4%
96.9%
MiCo-DeepSeek Coder V1
99.9%
99.4%
92.0%
93.3%
79.6%
99.4%
96.2%
MiCo-DeepSeek Coder V2
99.9%
99.5%
94.9%
99.6%
74.9%
99.4%
96.1%
21


--- Page 22 ---
4.3.1 Option Miner Analysis.
During the Option Miner stage, it is observed that the large language model tends to employ optimization techniques
by integrating classical heuristic approaches, including the weighted sum method, dynamic weight adjustment, and
lookahead mechanisms, among others.
Weighted sum.
The weighted sum is a classical concept in optimization, employed to balance multiple criteria by assigning relative
weights to each. In this context, the code utilizes a weighted sum approach to optimize CPU and memory resource
allocation through the calculation of task priority scores. By assigning distinct weights to CPU and memory usage, it
evaluates resource efficiency and allocates resources based on the specific requirements of each task. This weighted
allocation strategy promotes an even distribution of workload across virtual machines, thereby enhancing overall
system performance.
Dynamic weight adjustment.
The dynamic factor in the code adjusts the weights of CPU and memory, aligning with the multidimensional Bin-
Packing algorithm for load balancing (Nehra and Kesswani 2023). This algorithm optimizes the allocation of VMs and
PMs by reducing the number of active PMs and enhancing resource utilization across multiple resource dimensions,
such as CPU and RAM. Similarly, the calculation of the dynamic factor in the code dynamically balances CPU and
memory usage by distributing weights accordingly, achieving a comparable optimization goal. This method effectively
improves resource utilization and reduces the power consumption of physical machines, reflecting the core principles
of coordinated resource allocation proposed in the referenced paper.
Lookahead Mechanism.
The lookahead mechanism of the code and predictive adjustment algorithm are supported by multiple sections of Vidal
(2004) on lookahead strategies in heuristic search. The paper provides a detailed explanation of applying lookahead
Figure 14: Code Snippet: Heuristic Code Generated by Option Miner
Weighted Sum
cpu_weight = 0.6
mem_weight = 0.4
cpu_score = cpu_weight * (1 / (1 + np.exp(-cpu_diff)))
mem_score = mem_weight * (1 / (1 + np.exp(-mem_diff)))
efficiency_score = (cpu_score + mem_score) / (bin[0] + bin[1])
Dynamic Weight Adjustment
dynamic_factor = (bin[0] / (bin[0] + bin[1])) / (item[0] / (item[0] + item[1]))
adjusted_cpu_weight = cpu_weight * dynamic_factor
adjusted_mem_weight = mem_weight * (1 - dynamic_factor)
Lookahead Mechanism
lookahead_score = 0
for i in range(min_lookahead_distance , max_lookahead_distance + 1):
next_item = (item[0] * i, item[1] * i)
lookahead_score = lookahead_factor * (lookahead_score / max_lookahead_distance)
22


--- Page 23 ---
strategies to calculate effective plans for each state, thereby advancing the search toward the goal state. Specifically,
the concept of â€œapplying a plan to obtain a new state and using that state as a successor node of the current nodeâ€
parallels the loop structure in the code, where future resource demands are calculated over a specific predictive range.
Moreover, the lookahead state calculation described in the paper is analogous to the lookahead score in the
code, with both adjusting the influence of predictions through certain weighting factors to improve search efficiency
and accuracy. These mechanisms significantly enhance problem-solving efficiency and scalability, aligning with the
dynamic adjustment of the lookahead score implemented in the code.
Mapping Heuristic Strategies to Scenarios.
The distribution of task types reflects the impact of the current scheduling strategy, particularly its handling of tasks of
varying sizes. In Scenario 6, as shown in the Figure 9, the scenario is dominated by Small and Medium Small tasks,
with a smaller proportion of Medium Medium and Large tasks, highlighting the strategyâ€™s preference for allocating
smaller tasks to resources. This is driven by the strategyâ€™s focus on optimizing the CPU/memory ratio and the task-to-
bin size match, which increases the priority of smaller tasks and prevents them from occupying large resource pools.
The presence of fewer large tasks aligns with the strategyâ€™s large-space reservation tendency, which avoids overfilling
resource bins and preserves larger capacities for future tasks. This is enforced through penalty mechanisms that reduce
scores when too much space remains unused. The overall distribution in Scenario 6 illustrates the strategyâ€™s ability
to balance efficient resource utilization by prioritizing smaller tasks while retaining sufficient space for larger tasks,
ensuring flexibility for future demands. The complete code of this policy can be seen on Online GitHub.
4.3.2 Option Composer Analysis.
The â€˜heuristic selectorâ€™ function dynamically adjusts heuristic values by analyzing multiple factors, including weighted
scores, trends, recent changes, and the acceleration of various request types. Its goal is to optimally match the current
request load characteristics. The function maps five distinct request types: Smallâ€™, Medium small, Medium medium,
Medium large, and Large to heuristic values ranging from 1 to 4, ensuring an adaptive and efficient response to
varying request loads.
Heuristic 1: This value is selected when â€˜smallâ€™ requests dominate, with no indication of a transition towards
larger request types. Scenario 2 aligns with this heuristic, as â€˜Smallâ€™ requests constitute nearly the entire distribution,
with no significant representation of larger requests. This stable dominance of â€˜Smallâ€™ requests corresponds closely
to the conditions described for Heuristic 1.
Figure 15: Code Snippet: Heuristic Code Generated by Option Composer
Option Composer code
heuristic_map = {
"small": 1,
"medium_small": 2,
"medium_medium": 3,
"medium_large": 4,
"large": 4
}
selected_heuristic = heuristic_map.get(dominant_request_type , 2)
23


--- Page 24 ---
Heuristic 2: This heuristic is applied when â€˜Medium smallâ€™ requests become more prevalent or, alternatively, when
the dominant request type is unrecognized. Scenario 3 is an appropriate match for this heuristic, as â€˜Medium Smallâ€™
requests have grown in prominence, while â€˜Smallâ€™ requests still retain a substantial share. The increased frequency of
â€˜Medium Smallâ€™ requests suggests a shift toward this category, consistent with the conditions for Heuristic 2.
Heuristic 3: This value is applied when â€˜medium mediumâ€™ requests are significant, particularly when there is
a discernible transition from smaller to larger requests. Scenario 5 corresponds well to this heuristic, as it shows a
significant presence of â€˜Medium Mediumâ€™ requests alongside a mix of other request types. This pattern indicates an
ongoing transition from smaller to larger request categories. The balance and shift observed in the request distribution
underscore the relevance of Heuristic 3.
Heuristic 4: This value is selected when â€˜medium largeâ€™ or â€˜largeâ€™ requests dominate, or when trends indicate a
substantial increase in these types of requests. Scenario 6 exemplifies this heuristic, with â€˜Medium Largeâ€™ and â€˜Largeâ€™
requests clearly dominating the distribution, while smaller request types constitute a much smaller proportion. The
significant rise in larger request types is consistent with the conditions required for Heuristic 4.
Through these examples, it is evident that the heuristic selection process adapts dynamically to variations in
request load, ensuring that the selected heuristic value accurately reflects the prevailing distribution and trends among
request types. The complete code can be seen on Online GitHub.
5 Conclusion
The innovation of the MiCo framework lies in its ability to automate and optimize processes that traditionally rely
on human expertise. By combining machine learning, data analysis, and simulation techniques, the framework can
produce higher quality, more adaptive scheduling solutions in less time. Moreover, the framework is designed with
scalability and adaptability in mind. It is not only applicable to current VM scheduling problems but can also easily
adapt to future cloud computing scenarios and emerging technological trends. Through continuous learning and self-
optimization, MiCo has the potential to continuously improve its performance and efficiency over time. Further studies
may leverage fine-tuning techniques to develop domain-specific models by incorporating expert knowledge in cloud
scheduling for enhanced resource allocation and optimized scheduling rules. Additional efforts could address the cold
start problem through unsupervised or self-supervised learning methods to automatically generate initial policy sets
without predefined options.
References
AhmadiTeshnizi, A., W. Gao, and M. Udell. 2024. OptiMUS: Scalable optimization modeling with (MI)LP solvers and large
language models. arXiv preprint arXiv:2402.10172 .
Azar, Y., I.R. Cohen, S. Kamara, and B. Shepherd 2013. Tight bounds for online vector bin packing. In Proceedings of the 45th
annual ACM Symposium on Theory of Computing, pp. 961â€“970.
Azar, Y. and D. Vainstein. 2019. Tight bounds for clairvoyant dynamic bin packing. ACM Transactions on Parallel Computing 6(3):
1â€“21 .
24


--- Page 25 ---
Baykal-GÂ¨ursoy, M. and K. GÂ¨ursoy. 2010. Semi-markov decision processes. Wiley Encyclopedia of Operations Research and
Management Sciences 10: 9780470400531 .
Bentley, J.L., D.S. Johnson, F.T. Leighton, C.C. McGeoch, and L.A. McGeoch 1984. Some unexpected expected behavior results
for bin packing. In Proceedings of the 16th Annual ACM Symposium on Theory of Computing, pp. 279â€“288.
Berg, B.P. and B.T. Denton. 2017. Fast approximation methods for online scheduling of outpatient procedure centers. INFORMS
Journal on Computing 29(4): 631â€“644 .
Bertsimas, D. and G. Margaritis. 2024. Robust and adaptive optimization under a large language model lens. arXiv preprint
arXiv:2501.00568 .
Cai, Q., W. Hang, A. Mirhoseini, G. Tucker, J. Wang, and W. Wei. 2019. Reinforcement learning driven heuristic optimization.
arXiv preprint arXiv:1906.06639 .
Chen, S., K. Moinzadeh, J.S. Song, and Y. Zhong. 2023. Cloud computing value chains: Research from the operations management
perspective. Manufacturing & Service Operations Management 25(4): 1338â€“1356 .
Christensen, H.I., A. Khan, S. Pokutta, and P. Tetali. 2017. Approximation and online algorithms for multidimensional bin packing:
A survey. Computer Science Review 24: 63â€“79 .
Coffman, E. and A.L. Stolyar. 2001. Bandwidth packing. Algorithmica 29: 70â€“88 .
Coffman, Jr, E.G., M.R. Garey, and D.S. Johnson. 1983. Dynamic bin packing. SIAM Journal on Computing 12(2): 227â€“258 .
Coffman Jr, E.G., M.R. Garey, and D.S. Johnson. 1984. Approximation algorithms for bin-packingâ€”an updated survey, Algorithm
Design for Computer System Design, 49â€“106.
Cortez, E., A. Bonde, A. Muzio, M. Russinovich, M. Fontoura, and R. Bianchini 2017. Resource central: Understanding and
predicting workloads for improved resource management in large cloud platforms. In Proceedings of the 26th Symposium
on Operating Systems Principles, pp. 153â€“167.
CË†otÂ´e, J.F., M. Haouari, and M. Iori. 2021. Combinatorial benders decomposition for the two-dimensional bin packing problem.
INFORMS Journal on Computing 33(3): 963â€“978 .
Garey, M.R., R.L. Graham, D.S. Johnson, and A.C.C. Yao. 1976. Resource constrained scheduling as generalized bin packing.
Journal of Combinatorial Theory, Series A 21(3): 257â€“298 .
Gartner.
2025.
Gartner
forecasts
worldwide
public
cloud
end-user
spending
to
total
$723
billion
in
2025.
https://www.gartner.com/en/newsroom/press-releases/
2024-11-19-gartner-forecasts-worldwide-public-cloud-end-user-spending-to-total-723-billion-dollars-in-2025.
Gurobi Optimization, LLC. 2024. Gurobi Optimizer Reference Manual.
Hadary, O., L. Marshall, I. Menache, A. Pan, E.E. Greeff, D. Dion, S. Dorminey, S. Joshi, Y. Chen, M. Russinovich, et al.
2020. Protean:{VM} allocation service at scale. In The 14th USENIX Symposium on Operating Systems Design and
Implementation, pp. 845â€“861.
Jiang, Y., Z. Cao, and J. Zhang. 2021. Learning to solve 3D bin packing problem via deep reinforcement learning and constraint
programming. IEEE Transactions on Cybernetics 53(5): 2864â€“2875 .
Johnson, D.S. 1973. Near-optimal bin packing algorithms. Ph. D. thesis, Massachusetts Institute of Technology.
Kambhampati, S., K. Valmeekam, L. Guan, M. Verma, K. Stechly, S. Bhambri, L. Saldyt, and A. Murthy 2024. Position: LLMs
canâ€™t plan, but can help planning in LLM-modulo frameworks. In International Conference on Machine Learning.
25


--- Page 26 ---
Li, C., D. Song, and D. Tao 2023. Hit-MDP: learning the SMDP option framework on MDPs with hidden temporal embeddings.
In International Conference on Learning Representations.
Li, X. 2025. A review of prominent paradigms for LLM-based agents: Tool use (including RAG), planning, and feedback learning.
In Proceedings of the 31st International Conference on Computational Linguistics.
Maguluri, S.T., R. Srikant, and L. Ying 2012. Stochastic models of load balancing and scheduling in cloud computing clusters. In
The 31st Annual IEEE International Conference on Computer Communications, pp. 702â€“710.
Martello, S., D. Pisinger, and D. Vigo. 2000. The three-dimensional bin packing problem. Operations research 48(2): 256â€“267 .
Nehra, P. and N. Kesswani. 2023. Efficient resource allocation and management by using load balanced multi-dimensional bin
packing heuristic in cloud data centers. The Journal of Supercomputing 79(2): 1398â€“1425 .
Pietri, I. and R. Sakellariou. 2016. Mapping virtual machines onto physical machines in cloud computing: A survey. ACM
Computing Surveys 49(3): 1â€“30 .
Pisinger, D. and M. Sigurd. 2007. Using decomposition techniques and constraint programming for solving the two-dimensional
bin-packing problem. INFORMS Journal on Computing 19(1): 36â€“51 .
Puchinger, J. and G.R. Raidl. 2007. Models and algorithms for three-stage two-dimensional bin packing. European Journal of
Operational Research 183(3): 1304â€“1327 .
Ramamonjison, R., T. Yu, R. Li, H. Li, G. Carenini, B. Ghaddar, S. He, M. Mostajabdaveh, A. Banitalebi-Dehkordi, Z. Zhou, et al.
2023. NL4Opt competition: Formulating optimization problems based on their natural language descriptions. In NeurIPS
2022 Competition Track, pp. 189â€“203. PMLR.
Romera-Paredes, B., M. Barekatain, A. Novikov, M. Balog, M.P. Kumar, E. Dupont, F.J. Ruiz, J.S. Ellenberg, P. Wang, O. Fawzi,
et al. 2024. Mathematical discoveries from program search with large language models. Nature 625(7995): 468â€“475 .
Sheng, J., S. Cai, H. Cui, W. Li, Y. Hua, B. Jin, W. Zhou, Y. Hu, L. Zhu, Q. Peng, et al. 2021. VMAgent: Scheduling simulator for
reinforcement learning. arXiv preprint arXiv:2112.04785 .
Sheng, J., Y. Hu, W. Zhou, L. Zhu, B. Jin, J. Wang, and X. Wang. 2022. Learning to schedule multi-NUMA virtual machines via
reinforcement learning. Pattern Recognition 121: 108254 .
Sinclair, S.R., F.V. Frujeri, C.A. Cheng, L. Marshall, H.D.O. Barbalho, J. Li, J. Neville, I. Menache, and A. Swaminathan 2023.
Hindsight learning for mdps with exogenous inputs. In International Conference on Machine Learning, pp. 31877â€“31914.
Stolyar, A.L. 2013. An infinite server system with general packing constraints. Operations Research 61(5): 1200â€“1217 .
Stolyar, A.L. and Y. Zhong. 2021. A service system with packing constraints: Greedy randomized algorithm achieving sublinear
in scale optimality gap. Stochastic Systems 11(2): 83â€“111 .
Sutton, R.S., D. Precup, and S. Singh. 1999. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement
learning. Artificial Intelligence 112(1-2): 181â€“211 .
Tang, Z., C. Huang, X. Zheng, S. Hu, Z. Wang, D. Ge, and B. Wang. 2024. ORLM: Training large language models for optimization
modeling. arXiv preprint arXiv:2405.17743 .
Valmeekam, K., A. Olmo, S. Sreedharan, and S. Kambhampati 2022. Large language models still canâ€™t plan (A benchmark for
LLMs on planning and reasoning about change). In NeurIPS Foundation Models for Decision Making Workshop.
Valmeekam, K., K. Stechly, and S. Kambhampati. 2024. LLMs still canâ€™t plan; can LRMs? A preliminary evaluation of OpenAIâ€™s
o1 on planbench. arXiv preprint arXiv:2409.13373 .
26


--- Page 27 ---
Vidal, V. 2004. A lookahead strategy for heuristic search planning. In International Conference on Automated Planning and
Scheduling, pp. 150â€“160.
Wang, L., C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, et al. 2024. A survey on large language
model based autonomous agents. Frontiers of Computer Science 18(6): 186345 .
Ye, H., J. Wang, Z. Cao, F. Berto, C. Hua, H. Kim, J. Park, and G. Song. 2024. ReEvo: Large language models as hyper-heuristics
with reflective evolution. arXiv preprint arXiv:2402.01145 .
Zhang, Y., R. Bai, R. Qu, C. Tu, and J. Jin. 2022. A deep reinforcement learning based hyper-heuristic for combinatorial optimisation
with uncertainties. European Journal of Operational Research 300(2): 418â€“427 .
Zhang, Z., B.T. Denton, and X. Xie. 2020.
Branch and price for chance-constrained bin packing.
INFORMS Journal on
Computing 32(3): 547â€“564 .
Zhao, H., Q. She, C. Zhu, Y. Yang, and K. Xu 2021. Online 3D bin packing with constrained deep reinforcement learning. In
Proceedings of the AAAI Conference on Artificial Intelligence, Volume 35, pp. 741â€“749.
Zhao, H., Y. Yu, and K. Xu 2021. Learning efficient online 3D bin packing on packing configuration trees. In International
Conference on Learning Representations.
27


--- Page 28 ---
Appendix A
More Detailed Literature Review
The dynamic multidimensional bin packing problem (DMBP) is a complex extension of the classical bin packing
problem. In DMBP, items with multiple dimensions can arrive or depart at any time and must be allocated to bins
sequentially. This problem frequently appears in cloud computing scenarios, where physical resources like CPU,
memory, and bandwidth must be efficiently allocated and shared. Unlike traditional operations management scenarios,
such as healthcare systems where patients dynamically flow but typically occupy a single unit of capacity (e.g., a
hospital bed), DMBP involves more intricate resource sharing. DMBP is a combinatorial optimization problem and is
NP-hard. Researchers often use optimization-based techniques, learning-based methods, or heuristics to approximate
solutions or establish performance bounds (Coffman Jr et al. 1984).
Optimization-based Algorithms for DMBP. Offline DMBP assumes that all items to be packed and their
sequence are known in advance. It can be formulated as a mixed-integer programming problem, which can be solved
exactly using branch-and-bound algorithms (Martello et al. 2000). These algorithms are embedded in commercial
solvers such as Gurobi (Gurobi Optimization, LLC 2024). For large-scale problems, decomposition methods have
proven effective in solving DMBP (Pisinger and Sigurd 2007, Puchinger and Raidl 2007, CË†otÂ´e et al. 2021). In the
online setting, only the current item information is known, and items must be packed sequentially in an unknown
arrival process. When item sizes are stochastic, the problem become even more challenging, and most theoretical
research focuses on analyzing the multidimensional and dynamic characteristics separately (Chen et al. 2023). For
a review of approximation methods for online multidimensional bin packing problems, refer to (Christensen et al.
2017). In contrast, much of the literature on online one-dimensional dynamic bin packing focuses on applications
such as surgery scheduling, where emergency patients arrive randomly and require immediate service with uncertain
operation durations (Berg and Denton 2017, Zhang et al. 2020). From an OM perspective, dynamic bin packing can
be viewed as a service system, as discussed by Maguluri et al. (2012) and Stolyar (2013), Stolyar and Zhong (2021).
For instance, Stolyar (2013) developed an infinite-server system model with homogeneous servers, allowing arriving
VMs to be immediately assigned to a server. A series of studies by Stolyar (2013), Stolyar and Zhong (2021) revealed
that simple randomized policies could achieve asymptotic optimality as the system scale, measured by the VM arrival
rate, increases indefinitely.
Learning-based Algorithms for DMBP. Traditional methods often rely on expert knowledge, which can be
challenging to obtain, while heuristic algorithms may struggle with local optima. With the rapid development of
machine learning (ML), researchers have explored its application to DMBP. One approach directly applies ML to
solve DMBP. Zhao et al. (2021) and Zhao et al. (2021) combined deep learning and reinforcement learning techniques
to handle the dynamic nature and physical constraints of online packing, improving space utilization and operational
efficiency. Another approach integrates reinforcement learning (RL) with heuristics or mathematical models. RL can
optimize the decision-making process for bin packing, with heuristic methods providing initial solutions or baselines.
Then RL algorithms learn from these baselines and explore better solutions due to their exploratory nature (Zhang et al.
2022). Additionally, heuristic searches can provide supplementary reward signals to the RL agent, thereby accelerating
the training process. Cai et al. (2019) proposed an approach where the RL agent creates an initial feasible solution,
28


--- Page 29 ---
which is subsequently optimized further using simulated annealing. Jiang et al. (2021) enhances solution quality by
integrating RL with constraint programming (CP) by treating orientation and position as decision variables. The RL
agent selects their values dynamically during a branch-and-bound search, improving solution efficiency within the CP
framework.
Heuristic Algorithms for DMBP. To efficiently address the online DMBP problem, heuristic algorithms often
derive insights from theoretical analyses to accelerate the search process, solving instances with tens of thousands
of items within a limited time (Coffman et al. 1983, Hadary et al. 2020). Common algorithms include First-Fit
(allocating the next item to the first bin with sufficient capacity) and Best-Fit (allocating the item to the bin with
the smallest remaining capacity that still fits) (Garey et al. 1976). These algorithms provide performance bounds,
making them viable as approximation methods. Notably, Azar et al. (2013) and Azar and Vainstein (2019) established
tight bounds for online multidimensional and dynamic DMBP, respectively. When uncertainty or nonstationarity is
involved, Bentley et al. (1984) analyzed First-Fit and BestFit algorithms for online bin packing with stochastic item
sizes. Coffman and Stolyar (2001) studied these algorithms in stochastic process settings, focusing on item arrivals
governed by stochastic processes.
LLMs for Modeling and Code Generation. The integration of LLMs into combinatorial optimization has
advanced two primary methodologies: automated mathematical modeling and heuristic algorithm generation. A critical
challenge in optimization lies in transforming textual problem descriptions into formal mathematical formulations.
Pioneering work in this domain includes the NL4OPT competition (Ramamonjison et al. 2023), which established a
benchmark for translating natural language into mathematical programs using pre-trained language models. Building
on this foundation, OptiMUS (AhmadiTeshnizi et al. 2024) introduced a modular framework to formulate and
solve mixed-integer linear programming (MILP) problems from natural language inputs, demonstrating end-to-end
capabilities from problem interpretation to solver code generation. Further extending the scope, Tang et al. (2024)
proposed a synthetic data generation framework to train LLMs across diverse optimization problem types, effectively
addressing data scarcity challenges in specialized domains.
Beyond mathematical modeling, LLMs exhibit remarkable potential in designing heuristic algorithms for com-
binatorial optimization problems. For instance, Romera-Paredes et al. (2024) and Ye et al. (2024) leverage LLMs
to iteratively generate and refine heuristic rules, outperforming traditional evolutionary algorithms in generaliza-
tion capability and interpretability for problems like bin packing. Notably, these methods exploit code generation
proficiency of LLMs to evolve context-aware strategies through systematic prompt engineering and fitness-guided
evolutionary iterations.
Limitations of Existing Methods and the Role of LLMs. Optimization-based approaches struggle to solve
large-scale problems while simultaneously guaranteeing theoretical adherence to multidimensional and dynamic
characteristics. Although traditional heuristic methods can quickly handle large-scale problems, they are prone to local
optima and rely on expert knowledge. Learning-based approaches overcome these challenges but consume significant
computational resources during training. RL frameworks, however, offer novel perspectives for designing optimization-
oriented heuristic algorithms. As illustrated in Figure A1, leveraging the expert knowledge embedded in LLMs and
29


--- Page 30 ---
Figure A1: Method Comparison of Heuristic, Reinforcement Learning, and Language Agents Methods
Heuristic methods
expert knowledge
Language agents
expert knowledge +Â 
exploration & exploitation
Reinforcement learning
exploration & exploitation
combining it with the exploration-exploitation of the RL framework, we aim to develop hyper-heuristic algorithms
that can replace domain experts and adapt to dynamic, multidimensional environments. Our work contributes to this
paradigm by proposing a context-aware scheduling algorithm for VM scheduling problems. Unlike existing studies
focused on toy benchmarks, our framework achieves a transition from synthetic to real-world industrial scenarios,
addressing practical constraints such as dynamic resource demands and heterogeneous hardware configurations.
Appendix B
Supplementary Experiments
We conduct synchronous experimental validation on the Azure dataset. Notably, as this dataset contains only two
weeks of virtual machine request records and demonstrates no statistically significant nonstationary characteristics
in request distribution, we adopt a window-sliding approach to partition it into four representative scenarios for
algorithmic evaluation, as shown in Figure B2. Experimental results (detailed in Table B1) reveal that the MiCo
consistently achieves superior scheduling performance across all scenarios. Particularly in Scenario 2, which features
highly diverse virtual machine requests, the algorithm exhibits remarkable effectiveness with a 17.3% performance
gap compared to the heuristic hindsight solution (70.3% vs. 53.0%). This represents an average improvement of about
6% over existing online scheduling algorithms, conclusively demonstrating the robustness advantages of MiCo in
dynamic environments.
Figure B2: Scenario features in Azure datasets
Scenario 1
Scenario 2
Scenario 3
Scenario 4
0.0
0.2
0.4
0.6
0.8
1.0
Ratio
Types
Small
medium_small
medium_medium
medium_large
Large
Note. We extracted some VM sequences from the Azur dataset. We found that the extracted sequences did not have sufficiently diverse scene features and were mainly
composed of small and medium-sized requests.
30


--- Page 31 ---
Table B1: Performance Comparison of Best-Fit, First-Fit, HindSight, and MiCo Algorithms with
Gurobi Solutions Across Scenarios in Azure Training Dataset
Algorithm
Scenario
S1
S2
S3
S4
Mean
Best-Fit
99.2%
58.0%
57.5%
83.1%
65.5%
First-Fit
99.2%
58.2%
57.6%
83.1%
65.6%
HindSight
99.2%
53.0%
58.2%
78.2%
62.8%
SchedRL
98.3(Â±0.3)%
58.1(Â±0.1)%
57.6(Â±0.2)%
83.5(Â±0.7)%
65.7(Â±0.3)%
Mico
99.9%
70.3%
60.4%
86.5%
71.5%
Note. In Scenarios 3 and 4, the increased proportion of requests from medium and large virtual machines leads to a higher likelihood
of placement failures when resources are reserved or allocated inappropriately. This, in turn, results in the premature termination of the
scheduling process due to the inability to accommodate these larger requests in advance.
Additionally, we test the algorithm on Azure datasets. As shown in Figure B3, while the box plot shows similar
median performance across all algorithms, the upper whisker of MiCo is longer and its box is positioned relatively
higher. This further highlights the superiority of MiCo over the other algorithms.
We further investigate the convergence rates of the proposed algorithm, MiCo. As shown in Figure B4, the scenario-
specific policies (denoted as Policy) achieve peak performance without iterative training when using seed algorithms
in homogeneous demand scenarios (Scenarios 1 and 2). In contrast, the heterogeneous demand scenario (Scenario 5)
requires approximately 130 iterations to stabilize. The scenario-agnostic integrated policy (MiCo), which coordinates
all options across scenarios, demonstrates extended convergence requirements, necessitating nearly 200 iterations
to establish robust scenario recognition mechanisms, reflecting its inherent complexity in handling cross-scenario
pattern generalization.
Figure B3: (Color online) Boxplot of Scheduling Length for Best-Fit, First-Fit, HindSight, SchedRL, and MiCo
Algorithms in Azure Test Dataset
Best-Fit
First-Fit
SchedRL
HindSight
MiCo
Algorithm
1000
2000
3000
4000
5000
6000
7000
Scheduled Length
31


--- Page 32 ---
Figure B4: (Color online) Performance Over Iteration in Huawei Training Dataset
0
50
100
150
200
250
300
Iteration Steps
0.75
0.80
0.85
0.90
0.95
1.00
Ratio to the upper bound
MiCo
Policy1
Policy2
Policy3
Policy4
Policy5
Policy6
Appendix C
Prompt Template
Scheduler Template
Given the existing priority v0 function, please generate an optimized version named priority v*. This new
version should be more complex and efficient, incorporating multiple conditional logic and loops as necessary.
The function should calculate priorities for items to be added to bins, considering the item size and bin
capacities. Ensure the function is significantly different and more advanced than the prior versions. Only
the Python code for the function is required, without any additional descriptions or annotations.Existing
priority v0 function for reference:
def priority_v0(bin, item):
"""
Calculate and return the priority score for adding a specific item to a bin
based on available CPU and MEM resources.
Args:
bin (tuple): Tuple representing the binâ€™s available resources , where bin
[0] is CPU and bin[1] is memory.
item (tuple): Tuple representing the itemâ€™s resource requirements , where
item[0] is CPU and item[1] is memory needed.
Returns:
int: The total score for placing the item in the current bin. A higher
score indicates a better fit based on current available resources.
"""
score = -(bin[0] - item[0])
return score
def priority_v1(bin, item):
"""Improved version of â€˜priority_v0 â€˜."""
Your task is to create the optimized priority v* function based on the guidelines above. Remember, only the
Python function code is needed.
32


--- Page 33 ---
Context-Aware Scheduler Template
You are a leadingexpert on this topic.Given the existing heuristic selector v0 function, please generate an
optimized version named heuristic selector v*. This new version should be more complex and efficient,
incorporating multiple conditional logic and loops as necessary. Ensure the function is significantly different
and more advanced than the prior versions. Only the Python code for the function is required, without any
additional descriptions or annotations.Existing heuristic selector v0 function for reference:
import numpy as np
def heuristic_selector_v0(condition):
"""
This function selects the appropriate heuristic scheduling function based on
the input condition.
:param condition: list of dicts, each representing the distribution of
request types over the past 200 requests ,
divided into 4 groups of 50 requests each.
Each dictionary contains keys "small", "medium_small", "
medium_medium", "medium_large", and "large"
with their respective proportions.
Example:
[{"small": 0.4, "medium_small": 0.3, "medium_medium": 0.1,
"medium_large": 0.1, "large": 0.1},
{"small": 0.4, "medium_small": 0.3, "medium_medium": 0.1,
"medium_large": 0.1, "large": 0.1},
{"small": 0.4, "medium_small": 0.3, "medium_medium": 0.1,
"medium_large": 0.1, "large": 0.1},
{"small": 0.4, "medium_small": 0.3, "medium_medium": 0.1,
"medium_large": 0.1, "large": 0.1}]
:return: int, index of the selected heuristic function (only 1,2,3,4)
"""
return 1
def heuristic_selector_v1(condition):
"""Improved version of â€˜heuristic_selector_v0 â€˜."""
The â€œheuristic selectorâ€ function should only return 1 or 2 or 3 or 4. In order to ensure the result, do not
use any â€randomâ€ in â€œheuristic selectorâ€ function. Your task is to create the optimized heuristic selector v*
function based on the guidelines above. Remember, only the Python function code is needed.
Appendix D
Generated Heuristics
1
def priority(bin, item):
2
"""
3
Calculate and return the priority score for adding a specific item to a bin based on available CPU and MEM resources.
4
5
Args:
6
bin (tuple): Tuple representing the binâ€™s available resources , where bin[0] is CPU and bin[1] is memory.
7
item (tuple): Tuple representing the itemâ€™s resource requirements , where item[0] is CPU and item[1] is memory needed.
8
Returns:
9
int: The total score for placing the item in the current bin. A higher score indicates a better fit based on current available resources.
10
"""
11
# Unpack CPU and memory resources from the bin and item
12
cpu_resource , mem_resource = bin
33


--- Page 34 ---
13
cpu_needed , mem_needed = item
14
15
# Check if the itemâ€™s requirements exceed the binâ€™s resources; if so, return a negative infinity score.
16
if cpu_needed > cpu_resource or mem_needed > mem_resource:
17
return float(â€™-infâ€™)
18
19
# Calculate the difference between bin resources and item requirements
20
cpu_diff = cpu_resource - cpu_needed
21
mem_diff = mem_resource - mem_needed
22
23
# Calculate the ratio of CPU to memory for both bin and item to assess fit
24
bin_ratio = cpu_resource / mem_resource
25
item_ratio = cpu_needed / mem_needed
26
27
# Difference between binâ€™s and itemâ€™s resource ratios
28
ratio_diff = abs(bin_ratio - item_ratio)
29
# Adjust weight factor based on the ratio difference
30
weight_factor = 1 + ratio_diff / (bin_ratio + item_ratio)
31
32
# Prepare lists for differences and resources to loop over them
33
resource_diffs = [cpu_diff , mem_diff]
34
resources = [cpu_resource , mem_resource]
35
36
# Initialize the minimum weighted difference variable to compare
37
min_weighted_diff = float(â€™infâ€™)
38
for i in range(2):
39
# Calculate ratio of resource difference and remaining resource ratio
40
diff = resource_diffs[i]
41
res_ratio = diff / resources[i]
42
remaining_resource_ratio = resources[1 - i] / resources[i]
43
44
# Compute weighted difference based on both resource differences and weight factor
45
weighted_diff = -((resource_diffs[0] + resource_diffs[1]) *
46
(1 + (resource_diffs[0] - resource_diffs[1]) / (resources[0] + resources[1])) *
47
weight_factor)
48
49
# Adjust weighted difference if resource ratio is smaller than the remaining resource ratio
50
if res_ratio < remaining_resource_ratio:
51
weighted_diff *= (1 + ratio_diff / (bin_ratio + item_ratio))
52
53
# Update the minimum weighted difference
54
min_weighted_diff = min(min_weighted_diff , weighted_diff)
55
56
# Calculate remaining space as a product of differences , normalized by the binâ€™s total resources
57
remaining_space = (cpu_diff * mem_diff) / (cpu_resource * mem_resource)
58
# Adjust min_weighted_diff based on remaining space
59
min_weighted_diff *= (1 - remaining_space)
60
61
# Calculate the size ratio of the item to the bin to assess item-to-bin size proportion
62
item_size = cpu_needed * mem_needed
63
bin_size = cpu_resource * mem_resource
64
size_ratio = item_size / bin_size
65
66
# Choose weights based on the itemâ€™s size proportion relative to the bin
67
weighted_items = [0.7, 0.3] if size_ratio <= 0.5 else [0.6, 0.4]
68
69
# Define a function to calculate score by applying the weight to min_weighted_diff
70
def score_by_weight(weight):
71
return min_weighted_diff * weight
72
73
# Select the best weight based on the max score and calculate initial score
74
best_weight = max(weighted_items , key=score_by_weight)
75
final_score = score_by_weight(best_weight)
76
77
# Adjust final score by a small factor based on how close the size ratio is to 1 (perfect fit)
78
additional_weight = abs(size_ratio - 1)
79
final_score += additional_weight * 0.2
80
81
# Calculate sum of resource differences
82
resource_diff_sum = sum(resource_diffs)
83
# Apply penalty if resource differences are significantly larger than available resources
84
if resource_diff_sum > 1.5 * (cpu_resource + mem_resource):
34


--- Page 35 ---
85
penalty = 1 - resource_diff_sum / (2 * (cpu_resource + mem_resource))
86
final_score *= penalty
87
88
# Factor based on item size added to bin size as a ratio to original bin size
89
item_bin_size_ratio = (item_size + bin_size) / bin_size
90
# Increase score if item is small relative to bin (favors better fit for smaller items)
91
if item_bin_size_ratio < 0.5:
92
final_score *= 1 + 0.1 * (0.5 - item_bin_size_ratio)
93
94
# Factor in remaining space in the bin as a proportion of the original space
95
remaining_space_factor = 1 - remaining_space
96
# If remaining space factor is above threshold , increase final score
97
if remaining_space_factor > 0.5:
98
final_score *= 1 + (remaining_space_factor - 0.5) * 0.2
99
100
# Add factor for how the itemâ€™s size compares to binâ€™s size
101
item_bin_size_factor = item_size / bin_size
102
final_score *= 1 + item_bin_size_factor * 0.1
103
104
# Return the computed final score, indicating priority for placing the item in the bin
105
return final_score
Listing 1: Heuristic Algorithm Generated by Option Miner
1
def heuristic_selector(condition):
2
import numpy as np
# Import numpy for numerical operations
3
4
# Helper function to calculate statistical features of the data
5
def calculate_stats(condition):
6
# Initialize a dictionary to hold various statistical measures for each key in the input data
7
stats = {
8
"averages": {k: 0 for k in condition[0].keys()},
# Stores average values
9
"trends": {k: [] for k in condition[0].keys()},
# Stores the trend (differences between consecutive values)
10
"accelerations": {k: [] for k in condition[0].keys()},
# Stores acceleration (differences of trends)
11
"recent_changes": {k: 0 for k in condition[0].keys()},
# Recent change for each key
12
"recent_accelerations": {k: 0 for k in condition[0].keys()},
# Recent acceleration for each key
13
}
14
15
# Calculate sums for averages and collect trend data for each group of values
16
for group in condition:
17
for k, v in group.items():
18
stats["averages"][k] += v
# Sum up values for calculating the average
19
stats["trends"][k].append(v)
# Add values to track trends
20
21
# Compute averages , trends, and accelerations
22
for k in condition[0].keys():
23
stats["averages"][k] /= len(condition)
# Finalize average for each key
24
stats["trends"][k] = np.diff(stats["trends"][k])
# Calculate trend as the difference between consecutive values
25
stats["accelerations"][k] = np.diff(stats["trends"][k])
# Calculate acceleration as the difference of trends
26
# Calculate recent changes and accelerations based on the last three data points
27
stats["recent_changes"][k] = condition[-1][k] - condition[-2][k]
28
stats["recent_accelerations"][k] = condition[-1][k] - 2 * condition[-2][k] + condition[-3][k]
29
30
return stats
# Return the calculated statistics
31
32
# Helper function to calculate weighted scores for each metric based on statistical measures
33
def calculate_weighted_scores(stats):
34
# Define weights for the score calculation
35
weights = [0.25, 0.3, 0.3, 0.15]
36
scores = {}
37
38
# Calculate scores for each key based on weighted criteria
39
for k in stats["averages"].keys():
40
scores[k] = (
41
weights[0] * stats["averages"][k]
# Weight average value
42
+ weights[1] * np.std(stats["trends"][k]) / (np.mean(stats["trends"][k]) + 1e-6)
# Weight trend stability
43
+ weights[2] * condition[-1][k]
# Weight recent value
44
+ weights[3] * np.std(stats["accelerations"][k]) / (np.mean(stats["accelerations"][k]) + 1e-6)
# Weight acceleration stability
45
)
46
47
return scores
# Return scores for each key
48
35


--- Page 36 ---
49
# Step 1: Calculate statistics from the condition data
50
stats = calculate_stats(condition)
51
52
# Step 2: Calculate scores for each metric based on weighted criteria
53
scores = calculate_weighted_scores(stats)
54
55
# Step 3: Identify the dominant request type (highest scoring key)
56
dominant_request_type = max(scores, key=scores.get)
57
58
# Define a heuristic mapping based on request types
59
heuristic_map = {
60
"small": 1,
61
"medium_small": 2,
62
"medium_medium": 3,
63
"medium_large": 4,
64
"large": 4
65
}
66
67
# Select heuristic based on the dominant request type
68
selected_heuristic = heuristic_map.get(dominant_request_type , 2)
69
70
# Step 4: Apply additional conditions to refine selected heuristic
71
# If heuristic is 4 but "medium_medium" has a higher score than "medium_small ," downgrade to 3
72
if selected_heuristic == 4 and scores["medium_medium"] > scores["medium_small"]:
73
selected_heuristic = 3
74
75
# Additional checks for specific conditions if the selected heuristic is 1
76
elif selected_heuristic == 1:
77
if scores["medium_small"] > scores["small"]:
78
selected_heuristic = 2
# Promote heuristic if "medium_small" is more prominent
79
if condition[-1]["medium_medium"] > max(condition[-1]["small"], condition[-1]["medium_small"]):
80
selected_heuristic = 3
# Further promotion if "medium_medium" is even higher
81
82
# Check if the latest trend in "large" is significantly above its mean trend
83
if stats["trends"]["large"][-1] > (np.mean(stats["trends"]["large"]) + 1.4 * np.std(stats["trends"]["large"])):
84
selected_heuristic = 4
# Set heuristic to 4 if "large" trend shows strong upward change
85
86
# Calculate the recent changesâ€™ score by squaring and summing recent changes for each key
87
recent_changes = np.array(list(stats["recent_changes"].values()))
88
recent_changes_score = np.sum(recent_changes * recent_changes)
89
90
# If recent changesâ€™ score is high, adjust heuristic based on the most recent dominant metric
91
if recent_changes_score > 1.2 * len(recent_changes):
92
recent_dominant_request_type = max(condition[-1], key=condition[-1].get)
93
selected_heuristic = heuristic_map.get(recent_dominant_request_type , 2)
94
95
# Calculate the recent accelerationsâ€™ score by squaring and summing recent accelerations for each key
96
recent_accelerations = np.array(list(stats["recent_accelerations"].values()))
97
recent_accelerations_score = np.sum(recent_accelerations * recent_accelerations)
98
99
# If recent accelerationsâ€™ score is high, adjust heuristic based on recent dominant acceleration type
100
if recent_accelerations_score > 1.5 * len(recent_accelerations):
101
recent_dominant_acc_type = max(
102
condition[-1],
103
key=lambda k: condition[-1][k] - 2 * condition[-2][k] + condition[-3][k]
104
)
105
selected_heuristic = heuristic_map.get(recent_dominant_acc_type , 2)
106
107
# Check for balanced scores across metrics , refine heuristic if scores are close to "large"
108
balanced = [abs(scores[k] - scores["large"]) < 0.1 for k in scores.keys() if k != "large"]
109
if all(balanced):
110
if scores["medium_medium"] > 0.4:
111
selected_heuristic = 3
112
elif scores["medium_small"] > 0.4:
113
selected_heuristic = 2
114
elif scores["small"] > 0.4:
115
selected_heuristic = 1
116
117
# Ensure the final selected heuristic is within the valid range (1 to 4)
118
if selected_heuristic < 1 or selected_heuristic > 4:
119
selected_heuristic = 2
120
36


--- Page 37 ---
121
return selected_heuristic
# Return the final selected heuristic
Listing 2: Heuristic Algorithm Generated by Option Composer
37
