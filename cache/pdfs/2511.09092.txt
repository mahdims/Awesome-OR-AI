--- Page 1 ---
OR-R1: Automating Modeling and Solving of Operations Research Optimization
Problem via Test-Time Reinforcement Learning
Zezhen Ding1, Zhen Tan2, Jiheng Zhang1*, Tianlong Chen3*
1The Hong Kong University of Science and Technology
2Arizona State University
3University of North Carolina at Chapel Hill
zdingah@connect.ust.hk, ztan36@asu.edu, jiheng@ust.hk, tianlong@cs.unc.edu
Abstract
Optimization modeling and solving are fundamental to the
application of Operations Research (OR) in real-world deci-
sion making, yet the process of translating natural language
problem descriptions into formal models and solver code re-
mains highly expertise intensive. While recent advances in
large language models (LLMs) have opened new opportuni-
ties for automation, the generalization ability and data effi-
ciency of existing LLM-based methods are still limited, as-
most require vast amounts of annotated or synthetic data, re-
sulting in high costs and scalability barriers. In this work,
we present OR-R1, a data-efficient training framework for
automated optimization modeling and solving. OR-R1 first
employs supervised fine-tuning (SFT) to help the model ac-
quire the essential reasoning patterns for problem formula-
tion and code generation from limited labeled data. In addi-
tion, it improves the capability and consistency through Test-
Time Group Relative Policy Optimization (TGRPO). This
two-stage design enables OR-R1 to leverage both scarce la-
beled and abundant unlabeled data for effective learning. Ex-
periments show that OR-R1 achieves state-of-the-art perfor-
mance with an average solving accuracy of 67.7%, using only
1/10 the synthetic data required by prior methods such as
ORLM, exceeding ORLM’s solving accuracy by up to 4.2%.
Remarkably, OR-R1 outperforms ORLM by over 2.4% with
just 100 synthetic samples. Furthermore, TGRPO contributes
an additional 3.1%–6.4% improvement in accuracy, signif-
icantly narrowing the gap between single-attempt (Pass@1)
and multi-attempt (Pass@8) performance from 13% to 7%.
Extensive evaluations across diverse real-world benchmarks
demonstrate that OR-R1 provides a robust, scalable, and cost-
effective solution for automated OR optimization problem
modeling and solving, lowering the expertise and data bar-
riers for industrial OR applications.
Code — https://github.com/SCUTE-ZZ/OR-R1
Introduction
The field of artificial intelligence has witnessed remark-
able advancements, with Large Language Models (LLMs)
emerging as powerful tools across diverse domains (Zhao
*Corresponding author.
Copyright © 2026, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
Labeled Data
UnLabeled Data
(e.g. Test Data)
Figure 1: Overview of OR-R1.
et al. 2025; He et al. 2024; Jiang et al. 2024b). A particu-
larly promising, yet challenging, application lies in automat-
ing the modeling and solving of optimization problems (Ra-
mamonjison et al. 2023). These problems are central to nu-
merous scientific and industrial applications, including lo-
gistics (Lee, Lee, and Zhang 2015; Harrison et al. 2019), re-
source allocation (Bretthauer and Shetty 1995), and schedul-
ing (Brucker et al. 1999; Long et al. 2020), where even mi-
nor improvements can yield significant real-world benefits.
Traditionally, formulating these problems into precise math-
ematical models and subsequently generating executable
solver code has demanded specialized human expertise, of-
ten a time-consuming and error-prone process (Huang et al.
2025; Jiang et al. 2024a). LLMs have emerged as a promis-
ing tool to automate this process, reducing the expertise bar-
rier for solving optimization problems.
Recent research has explored the integration of LLMs
into the optimization pipeline, broadly falling into two
main categories: prompt-based approaches and learning-
based approaches. Prompt-based approaches typically lever-
age LLMs’ in-context learning capabilities through carefully
designed prompts, few-shot examples, or chain-of-thought
methods to generate optimization models or code directly.
Notable works in this category include the Chain-of-Experts
(Xiao et al. 2023), Optimus (AhmadiTeshnizi, Gao, and
Udell 2024), MAMO (Huang et al. 2024b), ORQA (Mosta-
jabdaveh et al. 2025), OR-LLM-Agent (Zhang and Luo
2025), and OptimAI (Thind et al. 2025). These methods of-
arXiv:2511.09092v1  [cs.AI]  12 Nov 2025


--- Page 2 ---
ten utilize general-purpose LLMs without extensive domain-
specific fine-tuning, relying on their pre-trained knowledge
and sophisticated prompting strategies. In contrast, learning-
based approaches involve fine-tuning or training LLMs on
domain-specific datasets to enhance their understanding and
generation capabilities for optimization problems. Key con-
tributions in this area include ORLM (Huang et al. 2025),
LLMOPT (Jiang et al. 2024a), and OptiBench (Yang et al.
2024b). These works aim to build more specialized and per-
formant models through data-driven learning.
Learning-based approaches hold great promise for auto-
mated optimization; for instance, ORLM, using a Llama3-
8B model, has even surpassed GPT-4o (Hurst et al. 2024) in
some tests. Despite this, a major hurdle remains: the heavy
reliance on vast amounts of domain-specific data. While
synthetic data can be mass-produced, it often lacks the real-
world rigor and diversity of human-made examples. Con-
versely, manual annotation of high-quality data is extremely
costly, leading to a shortage of large, useful datasets and lim-
iting adoption. Our main goal is to significantly cut down
on the amount of labeled data needed. Moreover, research,
including insights from ORLM, shows another key chal-
lenge: LLMs can find optimal solutions, but their single-
attempt outputs often lack consistency. This means they’re
more likely to get the right answer if they generate multiple
times. This inconsistency highlights a clear need to improve
the reliability of individual outputs.
To address these critical issues, the large data demands
and inconsistent outputs, we introduce OR-R1. This inno-
vative learning-based framework is specifically designed to
boost LLM performance in Operations Research by requir-
ing far less data and delivering greater consistency. First,
OR-R1 uses Supervised Fine-Tuning (SFT) to teach the
core reasoning of the model for optimization modeling and
code generation with limited labeled data. Then, it integrates
Test-Time Group Relative Policy Optimization (TGRPO).
TGRPO works by having the LLM predict labels for un-
labeled data, using a voting system to create high-quality
pseudo-labels. These pseudo-labels then serve as a reward
function for reinforcement learning. This two-part strategy
not only drastically cuts the need for expensive labeled train-
ing data but also substantially improves the consistency of
the model’s predictions, making single-attempt generations
much more reliable.
Our main contributions and findings are:
• We introduce OR-R1, a novel framework that, for the
first time, integrates SFT and TGRPO for automated op-
erations research problems modeling and solving.
• We design a multi-faceted reward system specifically
tailored for optimization problem scenarios, comprising
Format Reward for structural correctness, Valid-Code
Reward for executability, Majority Voting Reward for
numerical accuracy, enabling robust model learning.
• OR-R1 surpasses previous state-of-the-art methods using
only 1/10 synthetic data of ORLM, and attains an av-
erage solving accuracy of 67.7% across diverse public
benchmarks, while significantly narrowing the gap be-
tween single-attempt and multi-attempt accuracy.
Related Work
Automated OR Problem Modeling and Solving. Large
Language Models (LLMs) have demonstrated impressive
capabilities in formal reasoning and code generation, ad-
vancing from solving math word problems (Cobbe et al.
2021; Hendrycks et al. 2021; Ahn et al. 2024) to generat-
ing competition-level code (Li et al. 2022; Chaudhary 2023;
Bairi et al. 2024). Meanwhile, methods such as few-shot
learning (Brown et al. 2020), Chain-of-Thought prompting
(Wei et al. 2022), Tree-of-Thoughts (Yao et al. 2023), and
Graph-of-Thoughts (Besta et al. 2024) have significantly en-
hanced the reasoning abilities of Large Language Models
(LLMs). Building upon these foundational advancements
in general math and coding, a specialized and highly im-
pactful application of LLMs has emerged in the domain
of Operations Research (OR). Specifically, LLMs are now
being used for automated optimization modeling. This in-
volves translating natural language descriptions of real-
world problems into precise mathematical optimization for-
mulations (e.g., MILP, LP) or directly into executable solver
code. Research in this area can be broadly categorized into
prompt-based and learning-based approaches. Prompt-based
methods leverage large pretrained LLMs with sophisticated
prompting strategies to generate models. Notable exam-
ples include the NL4Opt competition (Ramamonjison et al.
2023), which showcased LLMs’ potential in understand-
ing problem descriptions, and frameworks such as Optimus
(AhmadiTeshnizi, Gao, and Udell 2024), MAMO (Huang
et al. 2024b), ORQA (Mostajabdaveh et al. 2025), OR-
LLM-Agent (Zhang and Luo 2025), OptMATH (Lu et al.
2025), and OptimAI (Thind et al. 2025), which explore var-
ious prompting strategies, agent-based systems, and bench-
marks for OR problem solving. In contrast, learning-based
approaches involve fine-tuning LLMs on domain-specific
datasets to achieve deeper understanding and generation ca-
pabilities. Key contributions include ORLM (Huang et al.
2025), LLMOPT (Jiang et al. 2024a), and OptiBench (Yang
et al. 2024b), which focuses on creating specialized mod-
els or benchmarks for this task. However, these methods
typically rely on large amounts of synthetic data or costly
human-annotated data for training. Synthetic data often suf-
fers from accuracy issues, as highlighted by ORLM, where
their synthetic data had an accuracy of only around 70%.
In contrast, acquiring high-quality human-annotated data is
expensive and time consuming.
RL for LLM Alignment and Reasoning. Reinforcement
Learning (RL) has become a cornerstone for aligning LLMs
with human preferences and improving their reasoning ca-
pabilities. Initial works like RLHF (Ouyang et al. 2022;
Christiano et al. 2017) and Direct Preference Optimiza-
tion (DPO) (Rafailov et al. 2023) have shown the effec-
tiveness of fine-tuning LLMs with human feedback. More
advanced RL techniques, often based on Proximal Policy
Optimization (PPO) (Schulman et al. 2017), are used to
refine model behaviors for complex tasks. DeepSeekMath
(Shao et al. 2024), which introduced Group Relative Pol-
icy Optimization (GRPO), demonstrated how RL can push
the limits of mathematical reasoning by learning from com-
parisons between multiple generated solutions. Subsequent


--- Page 3 ---
c
query
Policy
Model
o1
o2
o8
...
Reference Model
y1
y2
y8
y
Rformate1 + Rcode1 + Rvoting1
Rformate2 + Rcode2 + Rvoting2
Rformate8 + Rcode8 + Rvoting8
A1
A2
A8
TGRPO
...
...
...
voting
formate reward
valid-code reward
majority 
voting reward
execute
code 
group
computation
KL
Below is an operations research question. Build a 
mathematical model and corresponding python 
code using `coptpy` that appropriately addresses 
the question.
# Question:
A furniture factory produces tables and chairs. 
Producing one table requires 4 hours of processing 
time and 3 wooden boards, while producing one 
chair requires 3 hours of processing time and 2 
wooden boards. The factory has a weekly 
processing time limit of 100 hours and 80 wooden 
boards available. Each table brings a profit of $60, 
and each chair brings a profit of $40. How should 
the production be scheduled to maximize profit?
# Response: 
c
## Mathematical Model:
Let's build a mathematical model to 
solve this optimization problem.
## Decision Variables:
•
x1 : Number of tables to produce
•
x2 : Number of chairs to produce
## Objective Function:
•
Maximize total profit: Z=60x1 + 40x2
## Constraints:
•
Processing time constraint: 4x1 + 
3x2 ≤ 100
•
Materials constraint: 3x1 + 2x2 ≤ 80
•
Non-negativity constraints: x1, x2 ≥0
## Python Code Solution Using ‘coptpy‘:
```python
import coptpy as cp
from coptpy import COPT
env = cp.Envr()
model = env.createModel("FurnitureProduction")
x1 = model.addVar(lb=0, name="tables") 
x2 = model.addVar(lb=0, name="chairs") 
model.setObjective(60*x1 + 40*x2, sense=COPT.MAXIMIZE)
model.addConstr(4*x1 + 3*x2 <= 100, name="Time")
model.addConstr(3*x1 + 2*x2 <= 80, name="Materials") 
model.solve()
if model.status == COPT.OPTIMAL:
    print(f"Maximum profit: ${model.objval:.2f}")
else:
    print("No optimal solution found.")
```
Figure 2: The figure illustrates the training process of TGRPO. The blue section shows an example of an operations research
(OR) problem. The green section presents sample outputs, including the mathematical model and corresponding code. The light
yellow section displays the code execution results, while the yellow section shows the majority voting of execution results. The
orange section represents the reward function, and the red section indicates the advantage function.
work, DeepSeek-R1 (Guo et al. 2025), further explores in-
centivizing reasoning capabilities via RL. Other research has
focused on scaling RL systems for LLMs (Yu et al. 2025;
Sheng et al. 2024; Zhang et al. 2024), self-correction mech-
anisms (Kumar et al. 2024; Qu et al. 2024) or formal provers
(Xin et al. 2024) to guide RL training.
Test-Time Adaptation and Confidence in LLMs. Ensur-
ing the reliability and accuracy of LLM outputs, particularly
at inference time, is crucial. In complex problem-solving
tasks, such as mathematical reasoning and code generation,
it’s commonly observed that approaches employing multi-
ple votings over generated candidate solutions, followed by
selection of the most consistent or accurate, achieve signif-
icantly better results than single-attempt methods (Pass@1)
(Chen et al. 2023; Huang et al. 2024a). This phenomenon
highlights that improving the consistency of generated out-
puts can lead to a substantial enhancement in Pass@1 perfor-
mance. To address this, Test-Time Adaptation (TTA) meth-
ods aim to adapt models to new data distributions or im-
prove performance. Examples include TENT (Wang et al.
2020), which minimizes entropy during inference. Build-
ing on this, Test-Time Reinforcement Learning (Zuo et al.
2025; Prabhudesai et al. 2025; Yu et al. 2025; Sheng et al.
2024; Zhang et al. 2024; Kumar et al. 2024; Qu et al. 2024;
Xin et al. 2024) extends TTA by applying reinforcement
learning principles to adapt models at test time. By employ-
ing strategies like majority voting among candidate outputs
and learning from consistency signals, these methods en-
able models to adapt on-the-fly to distribution shifts or chal-
lenging instances without extensive retraining. Despite these
advances, such RL and adaptation techniques have not yet
been systematically explored for solving operations research
(OR) problems. In this work, we conduct the first application
of these methods to automated OR problem solving.
Method
Overview
The overall workflow of the OR-R1 training procedure is
illustrated in Figure 1. Our method utilizes the Qwen3-
8B model as the base. First, we perform supervised fine-
tuning (SFT) using a small, randomly selected subset of
data from the 3000 IndustryOR dataset, an open-source re-
source from ORLM. Second, the model undergoes further
training with TGRPO on unlabeled test set data. TGRPO
training is guided by a composite reward function specifi-
cally designed for OR problem modeling and solving. This
reward integrates several components derived from the gen-
erated outputs: (1) Format Reward, which measures adher-
ence to the required structural format; (2) Valid-Code Re-
ward, which checks the syntactic correctness and executabil-
ity of the generated code; and (3) Majority Voting Reward,
which reflects the consensus among multiple candidate solu-
tions through majority voting. By combining these criteria,
the reward function incentivizes the model to generate well-
structured, functional, and consistent solutions for OR tasks.


--- Page 4 ---
Supervised Fine-tuning (SFT) Phase
The objective of the SFT phase is to maximize the likeli-
hood of generating the correct output given the input. This
is achieved by minimizing the negative logarithmic likeli-
hood loss, a standard approach in supervised learning. The
objective function for SFT is formally defined as:
LSFT(θ) = −E(x,o)∼DSFT


|o|
X
t=1
log P(ot|x, o<t; θ)


(1)
where
• LSFT(θ) represents the loss function for the model pa-
rameters θ.
• E(x,o)∼DSFT denotes the expectation over the input-output
pairs (x, o) sampled from the supervised data set DSFT,
where x is the input prompt and o = (o1, o2, . . . , o|o|) is
the target output sequence.
• P(ot|x, o<t; θ) is the probability of generating the t-th
token ot given the input prompt x, all preceding tokens
y<t, and the model parameters θ.
Test-Time Group Relative Policy Optimization
(TGRPO)
The theoretical foundation of TGRPO is consistent with that
of GRPO. The training process is illustrated in Figure 2.
A key characteristic of TGRPO is its ability to forego a
separate critic model, instead estimating the baseline from
group scores, significantly reducing computational training
resources. The objective of TGRPO is to maximize the ob-
jective of the policy, driving the model to produce higher-
quality outputs based on reward signals. The objective func-
tion for TGRPO is formally defined as:
JT GRP O(θ) = Eq∼P (Q),{oi}G
i=1∼πθold(O|q)
h 1
G
G
X
i=1
 min
  πθ(oi|q)
πθold(oi|q)Ai, clip
  πθ(oi|q)
πθold(oi|q), 1 −ϵ, 1 + ϵ

Ai

−βDKL(πθ||πref)
i
(2)
DKL(πθ||πref) = πref(oi|q)
πθ(oi|q) −log πref(oi|q)
πθ(oi|q) −1
(3)
Ai = Ri −mean({R1, R2, · · · , RG})
std({R1, R2, · · · , RG})
(4)
where
• JTGRPO(θ): Represents the objective function for the pol-
icy model with parameters θ.
• Eq∼P (Q),{oi}G
i=1∼πθold(O|q): Denotes the expectation over
questions q sampled from the distribution P(Q) and
groups of outputs {oi}G
i=1 sampled from the old policy
πθold given q.
• G: The number of generated outputs in a group.
• πθ(oi|q): The probability of generating output oi given
question q under the current policy πθ.
• πθold(oi|q): The probability of generating output oi given
question q under the old policy πθold.
• πref: A fixed reference policy, often the SFT model or
an initial pre-trained model, used for the KL divergence
regularization.
• Ri: The reward for the output oi.
• Ai: The advantage for output oi.
• ϵ: A hyperparameter for the PPO-style clipping.
• β: A hyperparameter controlling the strength of the KL
divergence penalty.
• DKL(πθ||πref): The KL divergence between the current
policy πθ and a reference policy πref.
Reward Function
Our training framework incorporates a composite reward
function, combining several distinct reward components to
guide the model’s learning process:
Format Reward.
The Format Reward encourages the
model to generate outputs that adhere to a predefined struc-
tural or syntactical format. Specifically, we check for the
presence of six key fields: ‘## Mathematical Model:’, ‘##
Decision Variables:’, ‘## Objective Function:’, ‘## Con-
straints:’, ‘## Python Code Solution Using ‘coptpy‘:’, and
‘‘‘‘python’. The reward is calculated as the proportion of
these fields successfully identified in the output:
Rformat(oi) = Number of required fields found
6
(5)
Valid-Code Reward.
The Valid-Code Reward incen-
tivizes the generation of executable or syntactically correct
code, crucial for tasks involving code generation or problem-
solving through programmatic means. This is a binary re-
ward:
Rcode(oi) =
1,
if code can correctly call ‘coptpy‘
0,
otherwise
(6)
Majority Voting Reward.
The Majority Voting Reward
is derived from the Test-Time Reinforcement Learning
(TTRL) framework (Zuo et al. 2025), where a consensus
output is established through majority voting among multi-
ple candidate generations. This estimated consensus output
then serves as a proxy label to compute a rule-based reward.
The reward function is defined as:
Rvoting(yi, y) =
1,
if yi = y
0,
otherwise
(7)
where:
• yi denotes the optimal solution by executing the code.
• y denotes the majority-voted prediction or the consensus
output. For majority voting, only results that produce nor-
mal values from code execution are considered, ignoring
cases with ‘No Best Solution’ or ‘None’ results.
The final composite reward R is the sum of these individ-
ual reward components:
Ri = Rformat(oi) + Rcode(oi) + Rvoting(yi, y)
(8)


--- Page 5 ---
Experiments
To analyze the performance of OR-R1, we conduct experi-
ments based on the open-source LLM Qwen3-8B and com-
pare it with various learning-based methods on extensive
datasets. The experiments aim to answer three questions:
• RQ1: How does OR-R1 perform compared to existing
learning-based methods across diverse real-world opera-
tions research benchmarks?
• RQ2: What is the impact of the TGRPO algorithm on
model performance?
• RQ3: How do different reward formulations affect the
performance of OR-R1?
Experimental Setup
Datasets.
To thoroughly evaluate the capabilities of OR-
R1, we utilize a diverse set of optimization-related bench-
marks that are collected from previously published sources.
These datasets are chosen because they present unique chal-
lenges and comprehensively cover various aspects of opti-
mization problem solving. The specific characteristics and
sizes of each key test set are based on the operations and im-
plementations referenced from the LLMOPT. These bench-
marks collectively provide a comprehensive evaluation of
LLMs in optimization modeling and solving. NL4Opt (Ra-
mamonjison et al. 2023) offers 230 linear programming
word problems, including an ‘objective’ domain for general-
ization assessment. Mamo (Huang et al. 2024b) challenges
LLMs with 652 Easy LP and 211 Complex LP instances
requiring deeper mathematical reasoning. NLP4LP (Ahma-
diTeshnizi, Gao, and Udell 2024) features 242 richly an-
notated linear and mixed-integer linear programming prob-
lems, mitigating data leakage. ComplexOR (Xiao et al.
2023) presents 18 complex real-world operation research
problems with implicit constraints and domain-specific
knowledge requirements. IndustryOR (Huang et al. 2025)
assesses performance on 100 real-world operation research
problems. OptiBench (Yang et al. 2024b) includes 605 opti-
mization modeling word problems across linear, non-linear,
and tabular data types, evaluating iterative optimization. Fi-
nally, the ICML Competition (Yang et al. 2024a) track fo-
cuses on automated optimization problem solving, using 410
evaluable data points from its public leaderboard.
Training DataSets.
The training of OR-R1 primarily in-
volves two critical phases: Supervised Fine-Tuning (SFT)
and a TGRPO-based reinforcement learning stage.
• SFT stage: We utilize ORInstruct, a public dataset from
ORLM. It contains 3,000 synthetic samples, which cor-
responds to 1/10 of the full dataset used in ORLM.
• TGRPO stage: We used unlabeled test data for training.
Base Model.
Our base model is Qwen3-8B (Yang et al.
2025). It’s chosen for its robust general-purpose language
understanding and generation capabilities, demonstrating
strong performance in both Math and Coding Tasks. Fur-
thermore, its model size is comparable to that of previous
related work, making it a suitable choice for our scenario.
Baselines.
We evaluate our method against several strong
baselines, including both general LLMs and specialized
optimization-focused models. Our primary baseline is the
Qwen3-8B model fine-tuned on the ORInstruct(3K) dataset.
Also, we compare with the following methods:
• ORLM (Huang et al. 2025): This framework focuses
on training open-source Large Language Models (LLMs)
for optimization modeling and solver code development.
It uses a semi-automated data synthesis framework called
ORInstruct to generate high-quality training data from
seed industry cases. The authors demonstrate results us-
ing Llama3-8B (Dubey et al. 2024) as their base model.
• LLMOPT (Jiang et al. 2024a): This framework em-
ploys multi-instruction tuning to improve problem for-
malization and solver code generation accuracy. It uses a
‘five-element formulation’ to define optimization prob-
lems and leverages data augmentation with expert /
GPT4-based data labeling (Achiam et al. 2023). The
framework incorporates supervised fine-tuning, model
alignment, and a self-correction mechanism, with their
primary results based on Qwen2.5-14B (Qwen et al.
2025).
• Base Model Variants: To ensure comprehensive com-
parison across different model architectures and sizes,
we also include SFT-tuned versions of Qwen3-8B,
Qwen2.5-7B (Qwen et al. 2025) and Llama3-8B (Dubey
et al. 2024) in our baseline evaluation. This helps isolate
the impact of our methodology from the inherent capa-
bilities of different base models.
Evaluation Metrics.
Aligned with ORLM, we adopt So-
lution Accuracy as our primary evaluation metric. For each
optimization problem, the evaluation follows an end-to-end
process: the LLM generates a response in natural language,
which contains code blocks marked by ‘‘‘python ... ‘‘‘.
A script automatically extracts and executes the generated
Python code to obtain the predicted optimal objective value
yi for problem i. Let y∗
i denote the ground truth optimal
value. We define Solution Accuracy as:
Solution Accuracy = 1
N
N
X
i=1
I(yi = y∗
i )
where N is the total number of problems, and I(·) is the
indicator function, which equals 1 if the condition holds and
0 otherwise. In other words, a problem is considered cor-
rectly solved if and only if the predicted optimal value ex-
actly matches the ground truth optimal value. This metric
directly reflects the model’s end-to-end capability for gener-
ating and solving optimization problems.
Implementation Details.
The training process consists of
two stages: SFT and TGRPO. In the SFT stage, Qwen3-8B
is fine-tuned using the AdamW optimizer, a warmup-decay
scheduler, and standard settings. In the TGRPO stage, the
SFT output is further optimized using AdamW with a co-
sine scheduler and PEFT (LoRA). Both stages are trained on
4×A100 (40G) GPUs with BF16 precision. For all hyperpa-
rameters and detailed settings, please refer to the Appendix.


--- Page 6 ---
Model
NL4OPT
MAMO
MAMO
IndustryOR
NLP4LP
ComplexOR
OptiBench
ICML
AVG
EasyLP
ComplexLP
Competition
Base Model Variants
Qwen3-8B SFT(3K)
86.0±2.0
87.0±1.0
39.9±3.0
33.0±1.0
82.9±0.5
40.7±6.4
61.4±1.2
85.8±2.0
64.6±1.2
Qwen2.5-7B SFT(3K)
83.0±1.9
85.6±0.7
37.3±1.2
32.7±0.6
80.0±0.7
40.7±3.2
57.0±1.6
79.2±1.8
61.9±1.2
Llama3-8B SFT(3K)
80.3±3.6
81.7±2.1
32.2±2.6
24.7±3.1
78.5±1.8
37.0±8.5
54.4±2.2
77.1±1.3
58.2±1.7
Qwen3-8B SFT(100)
81.8±1.7
84.4±3.3
31.9±3.1
29.3±4.2
78.2±0.9
35.2±3.2
56.2±2.0
79.4±2.8
59.5±1.7
Learning Base Model
LLMOPT(Qwen2.5-14B)
80.3
89.5
44.1
29.0
73.4
35.3
53.8
75.3
60.1
ORLM(Llama3-8B)
86.9
81.6
39.3
32.0
82.0
50.0
56.5
79.3
63.5
Proposed Method
OR-R1 SFT(100)-TGRPO
88.0±0.7
87.4±2.5
45.7±8.5
30.3±3.1
84.0±1.0
46.3±8.5
61.2±0.7
84.1±1.5
65.9±2.2
OR-R1 SFT(3K)-TGRPO
88.3±0.9
86.1±1.0
49.9±15.0
35.3±2.9
84.6±0.8
46.3±3.2
62.9±1.0
88.3±1.8
67.7±2.7
Table 1: Main evaluation results on eight operations research benchmarks. Solution accuracy (%) is reported for each method,
with the overall average (AVG) in the last column. Bold indicates the best result. Values with ‘±’ represent the mean and
standard deviation over three independent training.
Main Results
Based on the experimental results (Table 1), our method
demonstrates significant advantages across different test
sets. Specifically, OR-R1 SFT(3K)-TGRPO achieves an av-
erage accuracy of 67.7%, substantially outperforming all
baseline models and achieving optimal performance on mul-
tiple test sets including NL4OPT, MAMO ComplexLP, In-
dustryOR, NLP4LP, OptiBench, and ICML Competition.
Notably, even with only 100 samples for SFT, OR-R1
SFT(100)-TGRPO still achieves an average accuracy of
65.9%, surpassing other baseline methods including ORLM
and LLMOPT, which highlights the effectiveness of the
TGRPO method. By comparing different base models, we
find that Qwen3-8B, after SFT with 3K data, shows supe-
rior performance compared to Qwen2.5-7B and Llama3-8B.
While this validates the importance of advanced base mod-
els, our TGRPO method can further improve performance
by 3.1%-6.4%, demonstrating its effectiveness in optimiz-
ing operations research modeling and solving tasks.
Training Dynamics of TGRPO
0.0
0.2
0.4
0.6
0.8
1.0
(a) Format Reward
0.90
0.92
0.94
0.96
0.98
1.00
0.0
0.2
0.4
0.6
0.8
1.0
(b) Valid-Code Reward
0.6
0.7
0.8
0.9
1.0
0.0
0.2
0.4
0.6
0.8
1.0
(c) Majority Voting Reward
0.5
0.6
0.7
0.8
0.9
1.0
Figure 3: Overview of training dynamics for OR-R1 core
reward components of SFT(3K)-TGRPO.
Figure 3 illustrates the training dynamics of the three core
reward components (format, valid-code, and majority vot-
ing) for SFT(3K)-TGRPO. Format Reward: The format
reward remains consistently high (above 0.98) throughout
training, indicating that the model quickly learns to gener-
ate outputs in the correct format and maintains this ability
stably. Valid-Code Reward: The valid-code reward shows
a clear upward trend in the early stages and gradually sta-
bilizes around 0.9. This suggests that the model becomes
increasingly capable of producing syntactically valid code
as training progresses. Majority Voting Reward: The ma-
jority voting reward starts lower (around 0.7) but steadily
improves, stabilizing near 0.8. This reflects the model’s en-
hanced ability to generate solutions that are favored by ma-
jority voting, i.e., more frequently correct or consensus an-
swers. Across all three plots, the shaded regions indicate
significant variance in reward values during training, but
the overall trajectories of the moving averages are positive
and stable. This demonstrates that SFT(3K)-TGRPO effec-
tively optimizes all three reward components, leading to bet-
ter structured, more valid, and more reliable model outputs.
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
Epoch
60
65
70
75
80
Accuracy(%)
OR-R1 SFT(3K)-TGRPO Pass@8
OR-R1 SFT(3K)-TGRPO Pass@1
OR-R1 SFT(100)-TGRPO Pass@8
OR-R1 SFT(100)-TGRPO Pass@1
Figure 4: Performance of Pass@1 and Pass@8 during
TGRPO Training. Pass@1 measures the accuracy when only
the model’s top prediction is considered, while Pass@8 re-
flects the probability that at least one out of the top 8 gener-
ated solutions is correct.
Our motivation for developing TGRPO stems from the
observed significant gap (13%) between multiple sampling
attempt(Pass@8) and single attempt(Pass@1) performance.
While Pass@8 achieves impressive accuracy, Pass@1 ini-


--- Page 7 ---
Model
NL4OPT
MAMO
MAMO
IndustryOR
NLP4LP
ComplexOR
OptiBench
ICML
AVG
EasyLP
ComplexLP
Competition
Base Model Variants
Qwen3-8B SFT(3K)
86.9
87.2
42.3
34.0
83.4
44.4
62.1
87.6
66.0
Ablation Studies (Base Model Qwen3-8B SFT(3K))
+RL(Rformat)
86.5↓0.4
87.1↓0.1
46.9↑4.6
34.0↓0.0
83.5↑0.1
44.4↓0.0
62.6↑0.5
87.3↓0.3
66.5↑0.5
+RL(Rcode)
86.5↓0.4
87.4↑0.2
52.6↑10.3
38.0↑4.0
84.3↑0.8
38.9↓5.5
63.8↑1.7
86.3↓1.3
67.2↑1.2
+RL(Rvoting)
89.0↑2.1
87.0↓0.2
58.3↑16.0
33.0↓1.0
84.7↑1.3
38.9↓5.5
62.8↑0.7
90.5↑2.9
68.0↑2.0
+RL(Rformat + Rcode)
86.5↓0.4
86.8↓0.4
55.0↑12.7
35.0↑1.0
84.7↑1.3
44.4↓0.0
63.1↑1.0
86.6↓1.0
67.8↑1.8
+RL(Rformat + Rvoting)
87.8↑0.9
87.3↑0.1
51.7↑9.4
33.0↓1.0
86.0↑2.6
50.0↑5.6
64.0↑1.9
88.5↑0.9
68.5↑2.5
+RL(Rcode + Rvoting)
87.3↑0.4
87.1↓0.1
67.8↑25.5
41.0↑7.0
85.5↑2.1
38.9↓5.5
63.3↑1.2
87.6↓0.0
69.7↑3.7
+RL(Rformat + Rcode + Rvoting)
88.6↑1.7
87.0↓0.2
66.8↑24.5
37.0↑3.0
84.3↑0.9
50.0↑5.6
63.5↑1.4
88.8↑1.2
70.8↑4.8
Table 2: Ablation study on the reward formulations of OR-R1, using Qwen3-8B SFT(3K) as the base model. We report solution
accuracy (%) on eight operations research benchmarks and the overall average (AVG). Each row shows the effect of adding
different reward components either individually or in combination. Performance gains or drops relative to the SFT-only baseline
are marked in red (better) and blue (worse).
tially performs substantially lower. This discrepancy indi-
cates that the model possesses the underlying capability but
lacks consistency in single-attempt scenarios. TGRPO was
specifically designed to address this challenge by improving
the model’s deterministic performance and enhancing out-
put consistency. As demonstrated in Fig. 4, TGRPO shows
promising results in achieving this goal. After training, we
successfully reduced this gap to 7%. Pass@1 shows consis-
tent improvement throughout the training process, indicat-
ing the effectiveness of our approach in enhancing single-
generation reliability. It should be noted that the perfor-
mance curve maintains an upward trajectory even at the end
of our training iterations. Due to computational resource
constraints, we had to limit the training duration. How-
ever, the steady positive trend suggests potential for im-
provements with extended training, as the model has not yet
reached a performance plateau.
Data Scale Effect on TGRPO
SFT(3K)
+TGRPO(10)
+TGRPO(20)
+TGRPO(50)
+TGRPO(100)
+TGRPO(ALL)
66
68
70
72
Accuracy(%)
66.0
67.8
68.6
69.1
69.5
69.5
Figure 5: The impact of different data scales on TGRPO per-
formance. In TGRPO(N), N denotes the number of samples
randomly selected from each test set for TGRPO training.
Notably, all models here were trained for 160 steps.
Figure 5 illustrates how model accuracy changes with dif-
ferent data scales for TGRPO training. As the number of
TGRPO training samples (N) increases from 10 to 50, ac-
curacy steadily improves from 66.0% to 69.1%. However,
further increasing the data scale to include all available sam-
ples does not lead to higher accuracy. This suggests that
TGRPO achieves significant performance gains with a rel-
atively small amount of data, and increasing the data size
further brings diminishing returns. In summary, TGRPO not
only eliminates the need for additional labeled data, but
also achieves strong training performance with only a small
amount of in-domain data, showing its high data efficiency.
Ablation Studies
Table 2 summarizes the impact of different reward com-
ponents in OR-R1 using Qwen3-8B SFT(3K) as the base
model. Adding individual rewards, such as format, code, or
voting, each brings some improvement over the baseline of
SFT only, with the voting reward showing the largest gain
of a single component. Combining rewards further boosts
performance: the best results are achieved when all three
components are used together, yielding an average accuracy
of 70.8% (+4.8% over baseline). These findings show that
the rewards are complementary and that a comprehensive
reward design is key to maximizing model performance.
Conclusion
This study introduces OR-R1, a data-efficient training
framework for solving Operations Research (OR) optimiza-
tion problems. By integrating Test-Time Group Relative Pol-
icy Optimization (TGRPO), OR-R1 achieves state-of-the-
art performance, with an average accuracy of 67.7% across
multiple benchmarks, surpassing established methods like
ORLM and LLMOPT. Remarkably, OR-R1 reaches compet-
itive performance using only 100 labeled samples in the SFT
stage, demonstrating its ability to drastically reduce data re-
quirements while maintaining high accuracy. By leveraging
tailored rewards, OR-R1 improves single-attempt reliability
(Pass@1) and narrows the gap to multi-attempt performance
(Pass@8) from 13% to 7%, ensuring consistent and robust
outputs. This framework offers a scalable and cost-effective
solution for training domain-specific large language models,
which could benefit automated optimization applications in
real-world industrial scenarios.


--- Page 8 ---
References
Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.;
Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.;
Anadkat, S.; et al. 2023.
Gpt-4 technical report.
arXiv
preprint arXiv:2303.08774.
AhmadiTeshnizi, A.; Gao, W.; and Udell, M. 2024.
Optimus: Scalable optimization modeling with (mi) lp
solvers and large language models.
arXiv preprint
arXiv:2402.10172.
Ahn, J.; Verma, R.; Lou, R.; Liu, D.; Zhang, R.; and Yin, W.
2024. Large Language Models for Mathematical Reasoning:
Progresses and Challenges. arXiv:2402.00157.
Bairi, R.; Sonwane, A.; Kanade, A.; C, V. D.; Iyer, A.;
Parthasarathy, S.; Rajamani, S.; Ashok, B.; and Shet, S.
2024.
Codeplan: Repository-level coding using llms and
planning. Proceedings of the ACM on Software Engineer-
ing, 1(FSE): 675–698.
Besta, M.; Blach, N.; Kubicek, A.; Gerstenberger, R.;
Podstawski, M.; Gianinazzi, L.; Gajda, J.; Lehmann, T.;
Niewiadomski, H.; Nyczyk, P.; et al. 2024.
Graph of
thoughts: Solving elaborate problems with large language
models. In Proceedings of the AAAI Conference on Artifi-
cial Intelligence, volume 38, 17682–17690.
Bretthauer, K. M.; and Shetty, B. 1995. The nonlinear re-
source allocation problem. Operations research, 43(4): 670–
683.
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; et al. 2020. Language models are few-shot learners. Ad-
vances in neural information processing systems, 33: 1877–
1901.
Brucker, P.; Drexl, A.; M¨ohring, R.; Neumann, K.; and
Pesch, E. 1999. Resource-constrained project scheduling:
Notation, classification, models, and methods.
European
journal of operational research, 112(1): 3–41.
Chaudhary, S. 2023. Code Alpaca: An Instruction-following
LLaMA model for code generation.
https://github.com/
sahil280114/codealpaca.
Chen, X.; Aksitov, R.; Alon, U.; Ren, J.; Xiao, K.; Yin, P.;
Prakash, S.; Sutton, C.; Wang, X.; and Zhou, D. 2023. Uni-
versal Self-Consistency for Large Language Model Genera-
tion. arXiv:2311.17311.
Christiano, P. F.; Leike, J.; Brown, T.; Martic, M.; Legg, S.;
and Amodei, D. 2017. Deep reinforcement learning from
human preferences.
Advances in neural information pro-
cessing systems, 30.
Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.;
Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.;
et al. 2021. Training verifiers to solve math word problems.
arXiv preprint arXiv:2110.14168.
Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.;
Letman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.;
et al. 2024. The llama 3 herd of models. arXiv e-prints,
arXiv–2407.
Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.;
Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; et al. 2025. Deepseek-r1:
Incentivizing reasoning capability in llms via reinforcement
learning. arXiv preprint arXiv:2501.12948.
Harrison, A.; Van Hoek, R.; Skipworth, H.; and Aitken, J.
2019. Logistics management and strategy. Pearson UK.
He, C.; Luo, R.; Bai, Y.; Hu, S.; Thai, Z. L.; Shen, J.; Hu,
J.; Han, X.; Huang, Y.; Zhang, Y.; et al. 2024. Olympiad-
bench: A challenging benchmark for promoting agi with
olympiad-level bilingual multimodal scientific problems.
arXiv preprint arXiv:2402.14008.
Hendrycks, D.; Burns, C.; Kadavath, S.; Arora, A.; Basart,
S.; Tang, E.; Song, D.; and Steinhardt, J. 2021. Measuring
mathematical problem solving with the math dataset. arXiv
preprint arXiv:2103.03874.
Huang, B.; Lu, S.; Chen, W.; Wan, X.; and Duan, N. 2024a.
Enhancing Large Language Models in Coding Through
Multi-Perspective Self-Consistency. arXiv:2309.17272.
Huang, C.; Tang, Z.; Hu, S.; Jiang, R.; Zheng, X.; Ge, D.;
Wang, B.; and Wang, Z. 2025. Orlm: A customizable frame-
work in training large models for automated optimization
modeling. Operations Research.
Huang, X.; Shen, Q.; Hu, Y.; Gao, A.; and Wang, B. 2024b.
Mamo: a mathematical modeling benchmark with solvers.
arXiv preprint arXiv:2405.13144.
Hurst, A.; Lerer, A.; Goucher, A. P.; Perelman, A.; Ramesh,
A.; Clark, A.; Ostrow, A.; Welihinda, A.; Hayes, A.; Rad-
ford, A.; et al. 2024. Gpt-4o system card. arXiv preprint
arXiv:2410.21276.
Jiang, C.; Shu, X.; Qian, H.; Lu, X.; Zhou, J.; Zhou, A.; and
Yu, Y. 2024a. LLMOPT: Learning to Define and Solve Gen-
eral Optimization Problems from Scratch. arXiv preprint
arXiv:2410.13213.
Jiang, J.; Wang, F.; Shen, J.; Kim, S.; and Kim, S. 2024b.
A Survey on Large Language Models for Code Generation.
arXiv:2406.00515.
Kumar, A.; Zhuang, V.; Agarwal, R.; Su, Y.; Co-Reyes, J. D.;
Singh, A.; Baumli, K.; Iqbal, S.; Bishop, C.; Roelofs, R.;
et al. 2024.
Training language models to self-correct via
reinforcement learning. arXiv preprint arXiv:2409.12917.
Lee, C.-Y.; Lee, H. L.; and Zhang, J. 2015. The impact of
slow ocean steaming on delivery reliability and fuel con-
sumption. Transportation Research Part E: Logistics and
Transportation Review, 76: 176–190.
Li, Y.; Choi, D.; Chung, J.; Kushman, N.; Schrittwieser,
J.; Leblond, R.; Eccles, T.; Keeling, J.; Gimeno, F.;
Dal Lago, A.; Hubert, T.; Choy, P.; de Masson d’Autume,
C.; Babuschkin, I.; Chen, X.; Huang, P.-S.; Welbl, J.; Gowal,
S.; Cherepanov, A.; Molloy, J.; Mankowitz, D.; Suther-
land Robson, E.; Kohli, P.; de Freitas, N.; Kavukcuoglu, K.;
and Vinyals, O. 2022. Competition-Level Code Generation
with AlphaCode. arXiv preprint arXiv:2203.07814.
Long, Z.; Shimkin, N.; Zhang, H.; and Zhang, J. 2020. Dy-
namic scheduling of multiclass many-server queues with
abandonment: The generalized cµ/h rule. Operations Re-
search, 68(4): 1218–1230.


--- Page 9 ---
Lu, H.; Xie, Z.; Wu, Y.; Ren, C.; Chen, Y.; and Wen,
Z. 2025.
Optmath: A scalable bidirectional data synthe-
sis framework for optimization modeling.
arXiv preprint
arXiv:2502.11102.
Mostajabdaveh, M.; Yu, T. T. L.; Dash, S. C. B.; Ramamon-
jison, R.; Byusa, J. S.; Carenini, G.; Zhou, Z.; and Zhang,
Y. 2025. Evaluating LLM Reasoning in the Operations Re-
search Domain with ORQA. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 39, 24902–
24910.
Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;
Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;
et al. 2022. Training language models to follow instructions
with human feedback. Advances in neural information pro-
cessing systems, 35: 27730–27744.
Prabhudesai, M.; Chen, L.; Ippoliti, A.; Fragkiadaki, K.; Liu,
H.; and Pathak, D. 2025.
Maximizing Confidence Alone
Improves Reasoning. arXiv preprint arXiv:2505.22660.
Qu, Y.; Zhang, T.; Garg, N.; and Kumar, A. 2024. Recur-
sive introspection: Teaching language model agents how to
self-improve. Advances in Neural Information Processing
Systems, 37: 55249–55285.
Qwen; :; Yang, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.;
Yu, B.; Li, C.; Liu, D.; Huang, F.; Wei, H.; Lin, H.; Yang,
J.; Tu, J.; Zhang, J.; Yang, J.; Yang, J.; Zhou, J.; Lin, J.;
Dang, K.; Lu, K.; Bao, K.; Yang, K.; Yu, L.; Li, M.; Xue,
M.; Zhang, P.; Zhu, Q.; Men, R.; Lin, R.; Li, T.; Tang, T.;
Xia, T.; Ren, X.; Ren, X.; Fan, Y.; Su, Y.; Zhang, Y.; Wan,
Y.; Liu, Y.; Cui, Z.; Zhang, Z.; and Qiu, Z. 2025. Qwen2.5
Technical Report. arXiv:2412.15115.
Rafailov, R.; Sharma, A.; Mitchell, E.; Manning, C. D.;
Ermon, S.; and Finn, C. 2023.
Direct preference opti-
mization: Your language model is secretly a reward model.
Advances in Neural Information Processing Systems, 36:
53728–53741.
Ramamonjison, R.; Yu, T.; Li, R.; Li, H.; Carenini,
G.; Ghaddar, B.; He, S.; Mostajabdaveh, M.; Banitalebi-
Dehkordi, A.; Zhou, Z.; et al. 2023. Nl4opt competition:
Formulating optimization problems based on their natural
language descriptions. In NeurIPS 2022 Competition Track,
189–203. PMLR.
Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and
Klimov, O. 2017. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347.
Shao, Z.; Wang, P.; Zhu, Q.; Xu, R.; Song, J.; Bi, X.; Zhang,
H.; Zhang, M.; Li, Y.; Wu, Y.; et al. 2024. Deepseekmath:
Pushing the limits of mathematical reasoning in open lan-
guage models. arXiv preprint arXiv:2402.03300.
Sheng, G.; Zhang, C.; Ye, Z.; Wu, X.; Zhang, W.; Zhang, R.;
Peng, Y.; Lin, H.; and Wu, C. 2024. HybridFlow: A Flexi-
ble and Efficient RLHF Framework. arXiv preprint arXiv:
2409.19256.
Thind, R.; Sun, Y.; Liang, L.; and Yang, H. 2025. OptimAI:
Optimization from Natural Language Using LLM-Powered
AI Agents. arXiv preprint arXiv:2504.16918.
Wang, D.; Shelhamer, E.; Liu, S.; Olshausen, B.; and Dar-
rell, T. 2020. Tent: Fully test-time adaptation by entropy
minimization. arXiv preprint arXiv:2006.10726.
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;
Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022.
Chain-of-
thought prompting elicits reasoning in large language mod-
els. Advances in neural information processing systems, 35:
24824–24837.
Xiao, Z.; Zhang, D.; Wu, Y.; Xu, L.; Wang, Y. J.; Han, X.;
Fu, X.; Zhong, T.; Zeng, J.; Song, M.; et al. 2023. Chain-of-
experts: When llms meet complex operations research prob-
lems. In The twelfth international conference on learning
representations.
Xin, H.; Ren, Z.; Song, J.; Shao, Z.; Zhao, W.; Wang, H.;
Liu, B.; Zhang, L.; Lu, X.; Du, Q.; et al. 2024. Deepseek-
prover-v1. 5: Harnessing proof assistant feedback for re-
inforcement learning and monte-carlo tree search.
arXiv
preprint arXiv:2408.08152.
Yang, A.; Li, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.;
Yu, B.; Gao, C.; Huang, C.; Lv, C.; et al. 2025.
Qwen3
technical report. arXiv preprint arXiv:2505.09388.
Yang, Z.; Huang, Y.; Shi, W.; Feng, L.; Song, L.; Wang, Y.;
Liang, X.; and Tang, J. 2024a. Benchmarking LLMs for Op-
timization Modeling and Enhancing Reasoning via Reverse
Socratic Synthesis. arXiv preprint arXiv:2407.09887.
Yang, Z.; Wang, Y.; Huang, Y.; Guo, Z.; Shi, W.;
Han, X.; Feng, L.; Song, L.; Liang, X.; and Tang, J.
2024b.
OptiBench meets ReSocratic: Measure and im-
prove LLMs for optimization modeling.
arXiv preprint
arXiv:2407.09887.
Yao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T.; Cao,
Y.; and Narasimhan, K. 2023. Tree of thoughts: Deliber-
ate problem solving with large language models. Advances
in neural information processing systems, 36: 11809–11822.
Yu, Q.; Zhang, Z.; Zhu, R.; Yuan, Y.; Zuo, X.; Yue, Y.; Fan,
T.; Liu, G.; Liu, L.; Liu, X.; et al. 2025. Dapo: An open-
source llm reinforcement learning system at scale.
arXiv
preprint arXiv:2503.14476.
Zhang, B.; and Luo, P. 2025.
Or-llm-agent: Automat-
ing modeling and solving of operations research optimiza-
tion problem with reasoning large language model. arXiv
preprint arXiv:2503.10009.
Zhang, C.; Sheng, G.; Liu, S.; Li, J.; Feng, Z.; Liu, Z.; Liu,
X.; Jia, X.; Peng, Y.; Lin, H.; et al. 2024. A Framework
for Training Large Language Models for Code Generation
via Proximal Policy Optimization. In NL2Code Workshop
of ACM KDD.
Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.;
Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; Du, Y.; Yang, C.;
Chen, Y.; Chen, Z.; Jiang, J.; Ren, R.; Li, Y.; Tang, X.; Liu,
Z.; Liu, P.; Nie, J.-Y.; and Wen, J.-R. 2025. A Survey of
Large Language Models. arXiv:2303.18223.
Zuo, Y.; Zhang, K.; Sheng, L.; Qu, S.; Cui, G.; Zhu, X.; Li,
H.; Zhang, Y.; Long, X.; Hua, E.; et al. 2025. Ttrl: Test-time
reinforcement learning. arXiv preprint arXiv:2504.16084.
