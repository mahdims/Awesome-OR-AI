--- Page 1 ---
1
Large Language Model-Empowered Decision
Transformer for UAV-Enabled Data Collection
Zhixiong Chen, Member, IEEE, Jiangzhou Wang, Fellow, IEEE,
Hyundong Shin, Fellow, IEEE, and Arumugam Nallanathan, Fellow, IEEE
Abstract—The deployment of unmanned aerial vehicles (UAVs)
for reliable and energy-efﬁcient data collection from spatially
distributed devices holds great promise in supporting diverse
Internet of Things (IoT) applications. Nevertheless, the limited
endurance and communication range of UAVs necessitate intelli-
gent trajectory planning. While reinforcement learning (RL) has
been extensively explored for UAV trajectory optimization, its in-
teractive nature entails high costs and risks in real-world environ-
ments. Ofﬂine RL mitigates these issues but remains susceptible
to unstable training and heavily rely on expert-quality datasets.
To address these challenges, we formulate a joint UAV trajectory
planning and resource allocation problem to maximize energy
efﬁciency of data collection. The resource allocation subproblem
is ﬁrst transformed into an equivalent linear programming for-
mulation and solved optimally with polynomial-time complexity.
Then, we propose a large language model (LLM)-empowered
critic-regularized decision transformer (DT) framework, termed
LLM-CRDT, to learn effective UAV control policies. In LLM-
CRDT, we incorporate critic networks to regularize the DT model
training, thereby integrating the sequence modeling capabilities
of DT with critic-based value guidance to enable learning effective
policies from suboptimal datasets. Furthermore, to mitigate the
data-hungry nature of transformer models, we employ a pre-
trained LLM as the transformer backbone of the DT model
and adopt a parameter-efﬁcient ﬁne-tuning strategy, i.e., LoRA,
enabling rapid adaptation to UAV control tasks with small-scale
dataset and low computational overhead. Extensive simulations
demonstrate that LLM-CRDT outperforms benchmark online
and ofﬂine RL methods, achieving up to 36.7% higher energy
efﬁciency than the current state-of-the-art DT approaches.
Index Terms—Data collection, decision transformer, large lan-
guage models, unmanned aerial vehicle, ofﬂine reinforcement
learning.
I. INTRODUCTION
The rapid development of the Internet of Things (IoT) has
driven the emergence of innovative applications such as smart
cities and environmental monitoring, reshaping diverse aspects
of daily life and industrial operations.These applications rely
on large-scale deployments of spatially distributed IoT devices,
such as sensors, to continuously monitor areas of interest
and collect valuable data for analysis and decision-making
[1]. Reliable data collection from these dispersed sensing
Zhixiong Chen and Arumugam Nallanathan are with the School of Elec-
tronic Engineering and Computer Science, Queen Mary University of London,
London, U.K. (emails: {zhixiong.chen, a.nallanathan}@qmul.ac.uk).
Jiangzhou
Wang
is
with
the
National
Mobile
Communications
Research
Laboratory,
Southeast
University,
Nanjing,
China
(email:
j.z.wang@seu.edu.cn).
Hyundong Shin is with the Department of Electronics and Information
Convergence Engineering, Kyung Hee University, Yongin-si, Gyeonggido
17104, Republic of Korea (e-mail: hshin@khu.ac.kr).
nodes is therefore critical to unlocking the full potential
of IoT systems, particularly in regions where ﬁxed network
infrastructure is impractical or prohibitively costly. In this
context, unmanned aerial vehicles (UAVs) offer a compelling
solution by leveraging their deployment ﬂexibility and precise
manoeuvrability to complement ground networks and deliver
ubiquitous connectivity for IoT devices [2]. Serving as aerial
mobile base stations, UAVs can efﬁciently harvest data from
distributed devices and relay it to cloud or edge servers
for further processing, making them well-suited for mission-
critical applications such as disaster response and precision
farming.
Despite the aforementioned advantages, the limited ﬂight
endurance and communication range of UAVs pose critical
challenges for IoT data collection. Towards this end, extensive
research has focused on optimizing UAV trajectory planning
and wireless resource management to balance coverage, la-
tency, and energy efﬁciency throughout the mission. Specif-
ically, in [3], an energy-constrained UAV was dispatched to
collect data from a sparse IoT sensor network, where approxi-
mation and heuristic algorithms were designed to optimize the
UAV’s hovering positions and durations for maximizing the
collected data volume. In [4] and [5], an iterative algorithm
and a dynamic programming-based approach were respectively
proposed to minimize data collection completion time through
jointly optimizing UAV’s trajectory and wireless resource
allocation. By employing convex optimization techniques for
UAV trajectory design, the work in [6] minimized the data
collection time in a multi-UAV setting. In [7], an ant colony
algorithm was developed to minimize the age of information
of data collected from ground sensor nodes. Considering the
security threats in UAV-aided wireless networks, a short-
packet transmission scheme was proposed in [8] to ensure
both the freshness and security of collected data, wherein
a successive convex optimization-based approach was em-
ployed to optimize the UAV’s trajectory. In [9] and [10],
convex stochastic programming and fractional programming
approaches were respectively investigated to optimize the
UAV’s trajectory and maximize the energy efﬁciency of data
collection tasks. Although these traditional optimization and
heuristic-based ofﬂine methods effectively improved data col-
lection performance, they typically require perfect channel
state information over the entire time horizon. This hinders
their practical deployment, as IoT networks usually operate in
highly dynamic wireless environments where communication
conditions change unpredictably.
To address the limitations of traditional methods, rein-
arXiv:2509.13934v2  [eess.SY]  19 Sep 2025


--- Page 2 ---
2
forcement learning (RL) has been widely employed to learn
UAV trajectory planning and resource allocation policies
through interactions with the environment, eliminating the
need for prior knowledge of channel state information and
other environmental dynamics [11]–[13]. Speciﬁcally, deep
Q-network-based approaches were developed in [14], [15]
to learn UAV ﬂight path planning policies, where the ﬂight
path was discretized and the UAV only required to select
actions from a ﬁnite action space. To enable more ﬁne-
grained control, the deep deterministic policy gradient (DDPG)
algorithm was adopted in [16], [17] to directly optimize the
UAV control policies within a continuous action space, thereby
enhancing manoeuvrability. Moreover, the twin delayed deep
deterministic policy gradient (TD3) [18] builds upon DDPG by
incorporating twin critics, delayed policy updates, and target
policy smoothing, leading to more accurate value estimation
and reduced variance. Accordingly, TD3-based UAV control
policies was proposed in [19], achieving a more stable learning
process and higher data collection rewards than DDPG. To
address the low exploration efﬁciency caused by the deter-
ministic policies in DDPG and TD3, soft actor-critic (SAC)-
based UAV control approaches was proposed in [20], [21],
employing a stochastic policy to facilitate better exploration
in wireless environments with sparse rewards and complex
dynamics. Despite their ﬂexibility and effectiveness, the above
approaches are all online RL methods that operate in a trial-
and-error manner and require extensive real-time interactions
with the environment. This makes them less practical for UAV-
enabled IoT data collection due to the high operational costs
and risks associated with continuous exploration. Moreover,
the sample inefﬁciency of online RL demands a large number
of interactions before converging to a high-performing policy,
further limiting their applicability in large-scale IoT scenarios.
To tackle the limitations of online RL, ofﬂine RL offers
a promising alternative by learning effective policies entirely
from pre-collected datasets. This paradigm enables the use of
historical ﬂight logs, network operation records, and simulated
data to train UAV control and wireless network optimization
policies, thereby avoiding the risks and costs inherent to real-
time trial-and-error [22], [23]. However, ofﬂine RL algorithms
are susceptible to value overestimation, particularly for out-
of-distribution actions, and often experience unstable training
due to their reliance on bootstrapping to propagate returns.
Recently, decision transformer (DT) [24] was introduced to
cast the ofﬂine RL problem as a sequence modeling task,
effectively converting it into a supervised learning prob-
lem. By exploiting the powerful sequence representation and
long-range dependency modeling capabilities of transformer
architectures, DT enables the direct prediction of actions
conditioned on desired returns, thereby improving learning
stability and mitigating value overestimation. Inspired by the
advantages of DT, the work in [25] made an initial attempt by
proposing a DT-based approach to jointly optimize UAV posi-
tioning and computation resource allocation in a UAV-assisted
edge computing network, effectively improving fairness-based
throughput.
Despite the impressive performance of DT, it still faces
several challenges when applied to UAV-enabled IoT data
collection scenarios. Firstly, DT lacks the stitching ability,
and its performance is inherently constrained by the quality
of the underlying dataset. When the training data is sub-
optimal, DT struggles to combine actions from suboptimal
trajectories into optimal policies, thereby necessitating expert-
level datasets to achieve satisfactory performance. Moreover,
The transformer architecture is well known to be data-hungry,
requiring massive amounts of training data to achieve satisfac-
tory performance. However, collecting such large-scale and
high-quality training datasets is often difﬁcult and costly in
UAV-enabled wireless networks. To this end, we propose a
critic-regularized DT training paradigm that integrates the se-
quence modeling strengths of DT with the policy improvement
guidance of critic networks, enabling the learning of effective
UAV control policies from suboptimal datasets and removing
the dependence on expert-level training dataset. In addition,
inspired by the impressive few-shot generalization ability of
pre-trained large language models (LLMs), we employ a pre-
trained LLM as the transformer backbone of the DT and adopt
a parameter-efﬁcient ﬁne-tuning approach, namely LoRA [26],
to rapidly adapt the DT model for UAV control. This design
not only reduces data and computational requirements but
also facilitates the acquisition of high-quality policies without
relying on massive training data. Our main contributions are
summarized as follows:
• We formulate a UAV trajectory planning and resource
allocation problem to maximize the energy efﬁciency
of UAV-assisted data collection. Firstly, the resource
allocation subproblem is converted into an equivalent
linear programming, which admits an optimal solution
in polynomial time. The UAV trajectory planning task
is then reformulated as an ofﬂine RL problem, and we
provide a theoretical analysis of the limitations of existing
DT-based methods in solving it.
• To solve the UAV trajectory planning problem and ad-
dress the limitations of DT methods, we propose an
LLM-empowered critic-regularized DT approach, termed
LLM-CRDT. Speciﬁcally, we employ two critic networks
to estimate the state-action values. Unlike standard DT
approaches that rely solely on supervised learning over
return-conditioned trajectories, our method introduces a
state-action value-based regularization term into the train-
ing loss. This encourages the DT model to select actions
that align with high-value predictions from the critics,
thereby improving policy performance beyond that of the
behavior trajectories in the ofﬂine dataset.
• Instead of training LLM-CRDT from scratch, we employ
a pre-trained LLM as the transformer backbone of the
DT model and ﬁne-tune LLM-CRDT with LoRA, where
the pre-trained LLM remains frozen while trainable low-
rank matrices are injected into each transformer block.
Fine-tuning updates only the input data encoder, LoRA
adapters, action decoder, and critic networks, accounting
for only a small fraction of the total model param-
eters. This not only reduces data and computational
requirements but also leverages the universal reasoning
capabilities of pre-trained LLMs to achieve effective UAV


--- Page 3 ---
3
control.
• We conduct extensive simulations to evaluate the effec-
tiveness of the proposed LLM-CRDT and resource allo-
cation approach. The results show that our method out-
performs benchmark online and ofﬂine RL approaches.
Compared to the current state-of-the-art DT method, it
achieves up to a 36.7% improvement in energy efﬁciency
for UAV data collection missions. Moreover, the results
demonstrate that our approach is capable of learning
effective policies from suboptimal datasets.
The remainder of this paper is organized as: Section II
introduces the system model. In Section III, we present the
optimal resource allocation algorithm, then reformulates the
UAV control problem as an ofﬂine RL problem. Section IV
illustrates the proposed LLM-CRDT to learn effective UAV
control policies. Simulation results are presented in Section
V. Finally, Section VI concludes this work.
II. SYSTEM MODEL
As shown in Fig. 1, this work considers a UAV-enabled IoT
network, where a rotary-wing UAV is dispatched to collect
data from N distributed IoT devices within a rectangular
region of side lengths Xmax and Ymax, thereby supporting
intelligent applications such as environmental monitoring and
preventive maintenance. Following prior works, e.g., [13]–
[17], the system runs in discrete time horizon consisting of
T slots, each with duration δ. The IoT devices are indexed
by N = {1, 2, · · ·, N}. Each device n (n ∈N) is associated
with certain amount of data Dn that required to be collected in
real-time, and its location information is denoted using three
dimensional (3D) Cartesian coordinates, i.e., ln = (xn, yn, 0),
where xn and yn are its coordinates along the x- and y-axes,
respectively.
A. UAV Movement Model
In this work, the UAV is assumed to ﬂy at a constant altitude
H (H > 0) throughout the entire data collection mission [6],
[7]. It is worth noting that the following proposed approaches
can be readily extended to variable-altitude trajectories by
introducing an additional control variable for vertical move-
ment. The UAV’s position in time slot t is represented by
lu
t = (xu
t , yu
t , H), where 0 ≤xu
t ≤Xmax and 0 ≤yu
t ≤Ymax.
Each time slot t is divided into two parts, i.e., δ = δﬂy
t +δhover
t
,
where δﬂy
t
is the UAV’s ﬂight time and δhover
t
is the hovering
time for data collection. At the start of each slot t, the UAV
determines the ﬂight duration δﬂy
t
(0 ≤δﬂy
t
≤δ) and ﬂight
direction θt (0 ≤θt ≤2π). It then travels to a new position
at an average speed vt (0 ≤vt ≤Vmax), where Vmax denotes
the maximum allowable ﬂight speed determined by the UAV’s
hardware limitations. Thus, the UAV’s horizontal location in
slot t is calculated as follows:
 xu
t = xu
t−1 + vtδﬂy
t
cos(θt),
yu
t = yu
t−1 + vtδﬂy
t
sin(θt).
(1)
After reaching the desired position, the UAV hovers there
for the remaining time in slot t, i.e., δhover
t
, to collect data
from the IoT devices. According to the energy consumption
UAV trajectory
x
y
z
IoT devices
Coverage range 
of UAV
Data collection
Fig. 1. The considered UAV-enabled IoT networks.
model of UAV [12], [14], the propulsion power consumption
of the UAV with ﬂight speed v is
P(v) = P1

1 + 3v2
U 2
tip

+ P2
s
1 + v4
4v4
0
−v2
2v2
0
 1
2
+ 1
2d0ρgAv3,
(2)
where the three terms on the right-hand side correspond to
the blade proﬁle power, induced power, and parasite power,
respectively. P1 denotes the blade proﬁle power during hov-
ering, and Utip is the tip speed of the rotor blade. P2 and
v0 represent the induced power and the mean rotor induced
velocity in hovering, respectively. For the parasite power, d0,
ρ, g, and A denote the fuselage drag ratio, air density, rotor
solidity, and rotor disc area, respectively. By setting v = 0, the
hovering power consumption of the UAV is given by P1 +P2.
Therefore, the energy consumption of the UAV in time slot t
can be expressed as
Et = δﬂy
t P(vt) + δhover
t
(P1 + P2).
(3)
B. Channel Model
This work adopts the widely used probabilistic line-of-
sight (LoS) channel model [7], [12] to characterize the large-
scale attenuation of the wireless transmission links between
the UAV and IoT devices. The probability of establishing a
geometrical LoS connection depends on both environment-
speciﬁc parameters and the UAV’s elevation angle relative to
each device. Speciﬁcally, the LoS probability for device n in
slot t is given by
P LoS
n,t =
1
1 + a exp (−b(ωn,t −a)),
(4)
where a and b are environment-related parameters, ωn,t de-
notes the elevation angle between the UAV and device n in
time slot t, i.e.,
ωn,t = 180
π arctan

H
p
(xu
t −xn)2 + (yu
t −yn)2

.
(5)
Accordingly, the non-line-of-sight (NLoS) channel probability
of device n in slot t is P NLoS
n,t
= 1 −P LoS
n,t . The expected
channel gain of device n in slot t is
gn,t =
g0
 P LoS
n,t + (1 −P LoS
n,t )κ

(H2 + (xu
t −xn)2 + (yu
t −yn)2)
ι
2 ,
(6)


--- Page 4 ---
4
where ι is the path loss exponent, g0 is the channel gain at
the reference distance d0 = 1 m, 0 < κ < 1 captures the
additional attenuation incurred under NLoS conditions.
C. Data Collection Model
We consider an OFDMA-based network [27], [28] in
which M resource blocks (RBs) or chunks support device-
to-UAV data transmission, the set of RB indices is M =
{1, 2, · · ·, M}. In line with prior studies, such as [29], [30], we
eliminate inter-device interference by restricting the allocation
so that each RB is assigned to at most one device, while each
device uses only one RB per slot to communicate with the
UAV. Let z(t)
n,m ∈{0, 1} denote the RB allocation decision for
device n in time slot t, where z(t)
n,m = 1 indicates that RB m is
allocated to device n, and z(t)
n,m = 0 otherwise. We represent
Zt = {z(t)
n,m : n ∈N, m ∈M} as the RB assignments in
slot t. Denoting by pn the transmit power of device n, its
achievable transmit rate on RB m in slot t is
r(t)
n,m = W log2

1 +
pngn,t
Im,t + WN0

,
(7)
where W is the per-RB bandwidth, N0 is the noise power
spectral density, Im,t represents the co-channel interference
from other services sharing the same frequency resources.
Due to the limited transmit power of IoT devices and
substantial path loss, the UAV can only collect data from
devices located within its coverage area during the hovering
duration of each time slot t. Let N c
t represent the set of devices
covered by the UAV in time slot t, which is deﬁned as
N c
t = {n |dn
t ≤Rmax, n ∈N } ,
(8)
where dn
t
=
p
(xn −xu
t )2 + (yn −yu
t )2 is the horizontal
distance between device n and the UAV in slot t. Rmax is
the maximal horizontal coverage range of the UAV decided
by the maximal azimuth angle ωmax. Thus, Rmax is given by
Rmax = H tan (ωmax) .
(9)
Restricted by limited wireless resources, the UAV may not
collect data from all devices within its coverage area during
each time slot t. Thus, it is crucial to judiciously select the
devices for data collection in each slot. Let βn,t ∈{0, 1}
denote the selection ﬂag for device n in slot t, where βn,t = 1
if device n is chosen by the UAV for data collection, and
βn,t = 0 otherwise. Clearly, βn,t = 0 if n /∈N c
t . Let βt =
{βn,t : n ∈N} represent the device selection decision in slot
t. For each device n, the amount of data collected during slot
t is expressed as:
∆collect
n,t
= min
n
βn,t
XM
m=1 z(t)
n,mr(t)
n,mδhover
t
, Dn,t
o
, (10)
where Dn,t denotes the remaining data on device n at the
beginning of slot t, it evolves as:
Dn,t+1 = max

Dn,t −∆collect
n,t
, 0
	
,
(11)
where Dn,0 = Dn. Accordingly, the energy efﬁciency of the
UAV in slot t can be deﬁned as
φt =
PN
n=1 ∆collect
n,t
Et
,
(12)
which quantiﬁes data successfully gathered per unit energy in
slot t. A higher value of φt indicates more energy-efﬁcient
data collection.
D. Problem Formulation
This work focuses on maximizing the UAV’s energy efﬁ-
ciency during the data collection mission by co-designing the
UAV control and resource allocation policies. The optimization
problem is formulated as follows:
max
{θt,vt,δfly
t
,βt,Zt}
T
t=1
T
X
t=1
PN
n=1 ∆collect
n,t
Et
(13)
s. t. 0 ≤θt ≤2π, ∀1 ≤t ≤T,
(13a)
0 ≤vt ≤Vmax, ∀1 ≤t ≤T,
(13b)
0 ≤δﬂy
t
≤δ, ∀1 ≤t ≤T,
(13c)
0 ≤xu
t ≤Xmax, ∀1 ≤t ≤T,
(13d)
0 ≤yu
t ≤Ymax, ∀1 ≤t ≤T,
(13e)
0 ≤βn,t ≤
1(n ∈N c
t ), ∀n ∈N, 1 ≤t ≤T,
(13f)
βn,t ∈{0, 1}, ∀n ∈N, 1 ≤t ≤T,
(13g)
XN
n=1 z(t)
n,m ≤1, ∀m ∈M, 1 ≤t ≤T,
(13h)
XM
m=1 z(t)
n,m ≤1, ∀n ∈N, 1 ≤t ≤T,
(13i)
z(t)
n,m ∈{0, 1}, ∀n ∈N, m ∈M, 1 ≤t ≤T,
(13j)
wherer constraints (13a)-(13c) correspond to the restrictions of
UAV control policy, ensuring that the UAV operates within its
allowable limits. (13d) and (13e) enforce the UAV’s position
to remain within the predeﬁned operational region. (13f) and
(13g) ensure that only devices within the UAV’s coverage
area can be selected for data collection in each slot. Here,
1(·) denotes the indicator function, which returns 1 if and
only if the condition inside the parentheses is satisﬁed, and
0 otherwise. (13h), (13i) and (13j) are the resource allocation
restrictions.
Directly solving problem (13) requires complete channel
state information between all devices and the UAV across all
time slots and for all possible UAV positions. However, this is
impractical due to the continuous nature of the UAV’s position
space, which contains inﬁnite possible locations. Moreover,
accurate channel state information for future time slots cannot
be obtained in advance. To address these challenges, we
propose efﬁcient approaches to solve problem (13) in the
following sections, without relying on future channel state
information of the device-UAV links.
III. OPTIMAL RESOURCE ALLOCATION AND PROBLEM
TRANSFORMATION
This section proposes an effective approach to derive the
optimal device selection and resource allocation policy, and
reformulate problem (13) as a Markov decision process (MDP)
for UAV control. We then highlight the limitations of existing
RL and DT algorithms, which motivate the development of
the proposed LLM-CRDT to effectively learn UAV control
policies in ofﬂine settings, as detailed in Section IV.


--- Page 5 ---
5
A. Optimal Device Selection and Resource Allocation
According to the formulation of problem (13), for any given
UAV control policy in arbitrary time slot t (1 ≤t ≤T ), the
device selection and resource allocation decisions in slot t
affect only the energy efﬁciency of this slot and are indepen-
dent of other slots. Thus, the device selection and resource
allocation problem can be decoupled across time slots and
solved independently. Accordingly, we focus on deriving the
optimal device selection and resource allocation strategy for
a speciﬁc time slot t under an arbitrary UAV control policy.
Notably, the proposed algorithm can be directly applied to any
time slot. Since βn,t = PM
m=1 z(t)
n,m, once RBs are assigned
the device selection is implied. Accordingly, we decouple the
slot-t resource allocation subproblem from (13) as:
max
Zt
XN
n=1 min
n XM
m=1 z(t)
n,mr(t)
n,mδhover
t
, Dn,t
o
(14)
s. t. (13h), (13i), (13j).
Directly solving (14) is challenging due to its nonlinear
integer structure. We instead cast it as a maximum-weight
perfect matching, which can be solved in polynomial time. We
therefore convert it to a maximum-weight perfect matching
problem, which admits a polynomial-time solution. Speciﬁ-
cally, we deﬁne a bipartite graph G = (V, E), where the vertex
set is deﬁned as V = N ∪¯
M, and E represents the set of
edges connecting vertices in N and
¯
M. In the graph G, each
vertex n ∈N corresponds to device n, while
¯
M = M ∪Mv
is an extended version of M, where each vertex m ∈M
corresponds to RB m. The set Mv contains virtual vertices
introduced to ensure
 ¯
M
 = |N|, thereby forming a balanced
bipartite graph. The weight of each edge (n, m) ∈E is
Ω(t)
n,m =

min{r(t)
n,mτ hover
t
, Dn,t}, if n ∈N c
t , m ∈M,
0,
otherwise.
(15)
Based on graph G, we reformulate (14) as a maximum weight
perfect bipartite matching problem, where the objective is
to seek a perfect matching R maximizing P
(n,m)∈R Ω(t)
n,m.
We introduce the binary connector cn,m ∈{0, 1} for edge
(n, m): cn,m = 1 represents assigning RB m to device n,
cn,m = 0 otherwise. Hence, the bipartite matching problem is
formulated as follows:
max
{cn,m:n∈N,m∈¯
M}
XN
n=1
X| ¯
M|
m=1 cn,mΩ(t)
n,m
(16)
s. t.
XN
n=1 cn,m = 1, ∀m ∈¯
M,
(16a)
X| ¯
M|
m=1 cn,m = 1, ∀n ∈N,
(16b)
cn,m ∈{0, 1} , ∀m ∈¯
M, n ∈N.
(16c)
However, ﬁnding the optimal solution to problem (16) remains
non-trivial, as it is an integer programming problem. By
relaxing the integer constraint (16c), problem (16) can be
transformed into the following linear programming:
max
{cn,m:n∈N,m∈¯
M}N
n=1
XN
n=1
X| ¯
M|
m=1 cn,mΩ(t)
n,m
(17)
s. t. (16a), (16b),
Algorithm 1 Optimal Device Selection and RB Allocation
1: Formulate the RB allocation problem (14)
2: Construct a bipartite graph G = (V, E) and compute the weights
of each edge in E based on (15).
3: Construct the linear programming problem of the bipartite graph
matching problem, i.e., (17)
4: Solve problem (17) and obtain the optimal bipartite perfect
matching {cn}N
n=1
5: Calculate the optimal RB allocation decision as Z∗
t = {z(t),∗
n,m =
cn,m : n ∈N, m ∈M}.
6: Calculate the optimal device selection decision as α∗
t = {α∗
n,t =
PM
m=1 z(t),∗
n,m : ∀n ∈N}
7: Return the optimal device selection decision β∗
t and the optimal
RB allocation decision Z∗
t
0 ≤cn,m ≤1, ∀m ∈¯
M, n ∈N. (17a)
Remark 1. In (17), each row associated with (16a) and
(16b) has exactly one coefﬁcient equal to 1, implying total
unimodularity of the coefﬁcient matrix (since every square
submatrix has determinant 0, 1, or −1). According to [31],
(17) attains an integral optimum that is also optimal for the
original problem (16).
Remark 1 guarantees that solving (17) directly recovers the
optimum of (16). This work employs the current matrix mul-
tiplication time algorithm [32] to ﬁnd the optimal solution of
problem (17), which has a time complexity of O((N 2+1/6)2)
as problem (17) involves N 2 variables.
Based on the above analysis, problem (14) can be efﬁciently
solved to obtain the optimal device selection and resource al-
location decisions. For clarity, we present the detailed solution
procedure in Algorithm 1. Firstly, we transform problem (14)
into its equivalent bipartite graph matching form, i.e., (16),
which involves calculating the edge weights and verifying
the feasibility of assigning each RB to each device. This
incurs a time complexity of O(NM). Hence, we convert
(16) into its equivalent problem, i.e., (17), and ﬁnd the
optimal resource allocation decision Z∗
t . The optimal device
selection policy is then computed as β∗
n,t = PM
m=1 z(t),∗
n,m .
Consequently, the overall time complexity of Algorithm 1 is
O(NM + (N 2+1/6)2).
B. Problem Transformation and Analysis
Based on the above analysis, the optimal device selection
β∗
t = {β∗
n,t : n ∈N} and optimal resource allocation policy
Z∗
t = {z(t),∗
n,m : n ∈N, m ∈M} under any given UAV control
policy can now be obtained using Algorithm 1. By substituting
β∗
t and Z∗
t into the original problem (13), we reduce it to the
following UAV control problem:
max
{θt,vt,δfly
t }
T
t=1
T
X
t=1
N
P
n=1
min
n M
P
m=1
z(t),∗
n,m r(t)
n,mδhover
t
, Dn,t
o
Et
(18)
s. t. (13a), (13b), (13c), (13d), (13e).
Problem (18) is intrinsically a sequential decision-making
problem, which is impractical to solve directly as it requires
precise channel state information for all device-UAV links over
the entire time horizon and across all possible UAV positions.


--- Page 6 ---
6
Moreover, even with full channel state information, the long-
term and non-convex nature of (18) would still lead to sig-
niﬁcant computational challenges. To tackle these issues, we
reformulate problem (18) as a MDP and employ RL algorithms
to learn a satisfactory UAV control policy. Speciﬁcally, we
deﬁne the MDP as a tuple (S, A, P, R, γ): S is the state space,
A is the action space, P : S × A →S is the transition model,
R : S × A →R is the reward function, γ ∈[0, 1) is the
discount factor. In each time slot t, one can choose an action
at ∈A based on the current state st ∈S. This action is
then applied to the environment, resulting in a transition to
st+1 ∈S and returning rt = R(st, at). Speciﬁcally, the state,
action, and reward function are designed as follows:
• State: In each time slot t, the state st consists of the hor-
izontal position of the UAV, the remaining data amount
of devices, and the coverage indicator of devices, i.e.,
st =(xu
t , yu
t ,D1,t, · · · , DN,t,1(1 ∈N c
t ),· · ·,1(N ∈N c
t )).
(19)
• Action: The UAV control action in each time slot t, i.e.,
at ∈A, comprises the ﬂight direction, speed, and ﬂight
time, deﬁned as
at = (θt, vt, δﬂy
t ).
(20)
Note that at must satisfy the constraints (13a)-(13e) in
problem (18).
• Reward: According to the objective of problem (18), we
set the per-slot reward to the energy efﬁciency, i.e.,
rt =
N
P
n=1
min
n
M
P
m=1
z(t),∗
n,m r(t)
n,mδhover
t
, Dn,t
o
Et
.
(21)
Based on the above deﬁned MDP, various online RL ap-
proaches, such as SAC [33] and TD3 [18], can be utilized to
learn effective UAV control policies. These algorithms contin-
uously interact with the environment and update their policy
to maximize the long-term cumulative reward, i.e., PT
t=1 γtrt.
However, their interactive nature limits practical deployment,
particularly for UAV control tasks where real-time exploration
is often infeasible or costly. In addition, online RL requires
persistent access to the environment for hyperparameter tuning
and policy reﬁnement, which is both resource- and time-
intensive. Although ofﬂine RL methods, such as temporal-
difference (TD) learning [11], can learn effective policies
entirely from pre-collected dataset without interacting with the
environment, they often suffer from unstable training due to
their reliance on bootstrapping to propagate returns. Further-
more, the requirement to discount future rewards can induce
undesirable short-sighted behaviors, limiting the agent’s ability
to plan over long horizons. To overcome these issues, the DT
framework [24] treats the ofﬂine RL problem as a generic
conditional sequence modeling task, effectively converting
it into a supervised learning problem. This enables DT to
handle long sequences and avoid stability issues associated
with boostrapping.
In the ofﬂine setting, interactions with the environment are
no longer available. Agents can only learn from a pre-collected
Causal Transformer
(e.g. GPT-2, LLaMA)
t-1
s
a
s
a
ˆR
LayerNorm
Multi-Head Self 
Attention
Add
Linear 1
GeLU
LayerNorm
Linear 2
Add
´ L
ˆR
a
a
t
RTG embedding + position encoding
state embedding + position encoding
action embedding + position encoding
action decoder
Transformer decoder block
t-1
t-1
t-1
t
t
t
FFN
Fig. 2. The architecture of decision transformer.
trajectory dataset D = {τe}E
e=1 with E trajectories generated
by an unknown behavior policy πB. Each trajectory τe (e ∈
{1, 2, · · ·, E}) is an ordered sequence of return-to-go (RTG)
values, states, and actions over the time horizon, deﬁned as:
τe = ( bR1, s1, a1, bR2, s2, a2, · · · , bRT , sT , aT ),
(22)
where bRt is the RTG in time slot t, deﬁned as the sum of
future rewards from t to the end of the episode, i.e.,
bRt =
XT
t′=t rt′, ∀t ∈{1, 2, · · ·, T }.
(23)
Given the ofﬂine trajectory dataset D, we can train a DT
model to learn the UAV control policy. The architecture of
the DT model is illustrated in Fig. 2, which consists of three
components: a data encoder module, a causal transformer
module composed of a stack of transformer layers, and an
action decoder module. For each trajectory τe and each time
slot t, we feed the most recent K timesteps of trajectory data
into the DT model, i.e.,
τe,t = ( bRt−K+1, st−K+1, at−K+1, · · · , bRt, st, at),
(24)
where K is referred to as the context length. The input
sequence τe,t is ﬁrst processed by the data encoder module,
which comprises three linear layers and a learnable time
embedding layer. The time embedding layer encodes the time
information of each step i (t −K + 1 ≤i ≤t) into a
vector whose dimension matches the hidden dimension of the
transformer module, i.e., tv
i ∈R1×dtrans, where dtrans is the
hidden dimension of the transformer module. Each of the three
linear layers maps the RTG bRi, the state si, and the action
ai at each step i to vector representations, also matching the
hidden dimension of the transformer. The time embedding is
then added into each representation to form the input tokens:



bRv
i = Linear( bRi) + tv
i ∈R1×dtrans,
sv
i = Linear(si) + tv
i ∈R1×dtrans,
av
i = Linear(ai) + tv
i ∈R1×dtrans.
(25)
After encoding τe,t, we obtain a token sequence τ v
e,t =
( bRv
t−K+1, sv
t−K+1, av
t−K+1, · · · , bRv
t , sv
t , av
t ), which is then
fed into the transformer module and output a corresponding
sequence of predicted tokens, i.e.,
τ out
e,t = Transformer(τ v
e,t)
= ( bRout
t−K+1, sout
t−K+1, aout
t−K+1, · · · , bRout
t
, sout
t
, aout
t
). (26)


--- Page 7 ---
7
Finally, each output token sout
i
(t−K+1 ≤i ≤t) is passed to
the action decoder, which consists of a linear layer, to predict
the action for timestep i, i.e.,
ˆai = Linear(sout
i
).
(27)
In summary, given a trajectory segment τe,t, the DT model
outputs a sequence of predicted actions (ˆat−K+1, . . . , ˆat),
where ˆat is the predicted action at time step t. The learning
objective of DT model πθ, parameterized by θ, is to predict
next action ˆat from trajectory preﬁx and current state st, using
the true action at as the target, written as:
LDT(θ) =
t
X
i=t−K+1
∥ai −ˆai∥2 .
(28)
After training, the DT model can be deployed to interact
with the environment and generate UAV control actions in
an autoregressive manner. For DT framework, we have the
following theorem:
Theorem
1.
Assume
the
above-deﬁned
MDP
is
ε-
deterministic,
i.e.,
Pr(rt
̸=
R(st, at)
or
st+1
̸=
P(st, at) |st, at)) ≤ε at all st, at for the reward function R
and environment dynamics P. Let g(τ) = PT
t=1 rt. Given
a dataset D pre-collected by behavior policy πB, which
contains a sufﬁcient number of trajectories whose returns
match the DT’s conditioning values
ˆR1, i.e, Pr(g(τ) =
ˆR1 |s1, τ ∈D ) ≥ζ for all initial states s1. Then, the DT
policy πθ learned from this dataset satisfy:
Eτ∼πB [g(τ)] −Eτ∼πθ [g(τ)] ≤ε
1
ζ + 2

T 2
(29)
Proof. See Appendix A.
According to Theorem 1, training a DT model with the loss
function LDT drives the learned policy to gradually converge
toward the behavior policy πB. However, this convergence
inherently restricts the learned DT policy from surpassing the
performance of the underlying behavior policy πB that used to
generate the trajectories in the ofﬂine dataset D. Furthermore,
training exclusively with the LDT limits the DT’s ability to
stitch together high-reward actions across different trajectories,
leaving the learned DT policy πθ largely biased toward actions
observed during training.
IV. LLM-EMPOWERED CRITIC-REGULARIZED DECISION
TRANSFORMER FOR UAV CONTROL
To address the limitations of DT approaches discussed
above, this section presents an LLM-empowered critic-
regularized DT framework, i.e., LLM-CRDT, to solve problem
(18) and facilitate efﬁcient online UAV control. Speciﬁcally,
we ﬁrst propose a critic-regularized DT learning paradigm
that endows the DT model with stitching capabilities. Then,
we employ a pre-trained LLM as the transformer backbone
of the DT model and employ LoRA for ﬁne-tuning, thereby
exploiting the LLM’s few-shot generalization ability to further
enhance UAV control performance.
LoRA adapter
Pretrained LLM
(GPT-2)
Decoder
Encoder
Transformer
DT model
LoRA adapter
Pretrained LLM
(GPT-2)
Decoder
Encoder
Transformer
DT model
LoRA adapter
Pretrained LLM
(GPT-2)
Decoder
Encoder
Transformer
Target DT model
LoRA adapter
Pretrained LLM
(GPT-2)
Decoder
Encoder
Transformer
Target DT model
DT Policy
Trajectory dataset
,e t
t
ˆta
ˆia
ķ
ts
ĸ
Ĺ
loss
Ľ
ľ
Ŀ
ŀ
 
DT loss
DT loss
(
)
,
i
i
s a
Critic networks
Critic networks
Target critic networks
Target critic networks
 Soft 
Update
Critic
ĺ
Ļ
ļ
,e t
t
TD target
TD target
Critic loss
Critic loss
Update
,e t
t
ˆia
Regularization term
Update
 Soft 
Update
Frozen
Trainable
0
B =
2
(0,
)
A
s
=
2
2
(0,
)
A
s
=
2)
2
0
B =
(a)
x
y
z
IoT devices
Data collection
Environment
Ļdevice selection &
resource allocation
Ļdevice selection &
resource allocation
ĹInput sequence assemble
1
1
1
ˆ
ˆ
(
,
,
,
,
,
)
t K
t K
t K
t
t
R
R
-
+
-
+
-
+
ssemble
ˆ
,
,
)
t
t
,
,
s
a
s
ts
1
tr -
LoRA adapter
Pretrained LLM
(GPT-2)
Decoder
Encoder
Transformer
Trained DT model
LoRA adapter
Pretrained LLM
(GPT-2)
Decoder
Encoder
Transformer
Trained DT model
ĸRTG update
1
1
ˆ
ˆ
t
t
t
R
R
r
-
-
=
-
ķ
ĺ
ta
(b)
Fig. 3. The architecture of the proposed LLM-CRDT: (a) Fine-tuning on an
ofﬂine trajectory dataset, (b) Inference to generate action and interact with
environment.
A. Critic-Regularized Decision Transformer
Inspired by the stitching capability inherent in actor-critic
methods [11], we propose a critic-regularized DT learning
paradigm, which integrates the sequence modeling capacities
of DT with the policy improvement guidance provided by
critic networks, as shown in Fig. 3(a). Unlike the standard
DT approach in [24] that rely solely on supervised learning
over return-conditioned trajectories, our approach introduces
a state-action value-based regularization term into the training
loss. This encourages the model to select actions that align
with high value predictions from the critic, thereby improving
policy performance beyond that of the behavior trajectories in
the ofﬂine dataset.
Speciﬁcally, we draw inspiration from the actor-critic al-
gorithms (such as SAC [33] and TD3 [18]) and employ two
critic networks, Qφ1 and Qφ2, along with their corresponding
target networks, Qφ′
1 and Qφ′
2, to learn accurate estimates of
the state-action value function. In addition, we employ a DT
model πθ, parameterized by θ, to predict actions. Following
the common practice in conventional RL approaches, we
maintain a target DT model πθ′ to enhance the stability of the
learning process. Considering that the DT policy conditions on
past trajectory information, we estimate the state-action values
via the n-step Bellman backup, which has been shown to
outperform the 1-step approximation [11]. Given a trajectory
segment τe,t as deﬁned in (24), we use the target DT policy πθ′


--- Page 8 ---
8
to predict the action at time step t, i.e., ˆat. We then compute
the TD target for the i-th step (t −K + 1 ≤i ≤t −1) as
follows:
ˆQi =
t−1
X
j=i
γj−irj + γt−i
min
h∈{1,2} Qφ′
h(st, ˆat).
(30)
Next, we use the critic networks Qφ1 and Qφ2 to estimate the
state-action values at each time step i, and their parameters
are optimized by minimizing the following loss function:
L(φh) =
t−1
X
i=t−K+1
 ˆQi −Qφh(si, ai)

2
, h ∈{1, 2}.
(31)
During the DT model training, we incorporate a state-action
value loss term to regularize the DT towards sampling high-
value actions. Speciﬁcally, we ﬁrst use the DT model πθ to
predict the actions in each time step i ∈{t −K + 1, · · · , t},
i.e., (ˆat−K+1, · · · , ˆat). Then, we use the critic network Qφ1
to evaluate the state-action values for these predicted actions,
i.e, {Qφ1(si, ˆai) : ∀i ∈{t −K + 1, · · · , t}}. Hence, the DT
model is optimized by minimizing the following loss function:
L(θ) = LDT(θ) + λ · LQ(θ)
= LDT(θ) −λ
t
X
i=t−K+1
Qφ1(si, ˆai),
(32)
where LQ(θ) = −Pt
i=t−K+1 Qφ1(si, ˆai) is the regular-
ization term to evaluate the DT policy and promotes high-
reward actions, λ > 0 is a hyperparameter used to balance
the trajectory modeling loss and the regularization loss. By
training with the loss function (32), the DT model acquires
stitching capabilities and is able to outperform the underlying
behavior policy used to collect the dataset.
In addition, to stabilize learning process, we adopt the
soft-update strategy [11] to update the target critic networks
(φ′
1 and φ′
2) and the target policy network θ′. With mixing
coefﬁcient ρ ∈(0, 1), the target networks are updated as

θ′ = ρθ + (1 −ρ)θ′,
φ′
h = ρφh + (1 −ρ)φ′
h, h ∈{1, 2}.
(33)
B. LLM-Empowered Critic-Regularized Decision Transformer
Up to now, the proposed critic-regularized DT gains stitch-
ing capability. However, transformers are known to be data-
hungry and usually require massive amounts of training data
to achieve satisfactory performance. As a result, training the
DT model from scratch can be both challenging and computa-
tionally expensive. To this end, this subsection proposes LLM-
CRDT to leverage the few-shot generalization capabilities of
pre-trained LLMs to enhance the DT model for UAV control.
Similar to the standard DT framework in [24], we adopt
GPT-2 [34] as the transformer module in our proposed LLM-
CRDT. However, unlike prior DT works (e.g., [24], [25]) that
randomly initialize model parameters and train DT models
from scratch, we initialize the transformer with pre-trained
GPT-2 parameters. This enables the DT model to exploit
the general knowledge encoded in LLMs, thereby enhancing
action prediction. Note that the transformer module in DT can
Algorithm 2 Fine-tuning of LLM-CRDT
1: Input: Ofﬂine trajectory dataset D, context length K, soft update
coefﬁcient ρ, LoRA rank r, scaling factor α, batch size B.
2: Create LoRA adapters for the query, key, value projection layers
and attention output layer in each transformer block of the LLM.
3: Freeze the LLM backbone, enable the parameters of data encoder,
LoRA adapters, action decoder, and critics to be trainable.
4: repeat
5:
Randomly sample a batch of B trajectories B = {τe}B
e=1
from D.
6:
For each trajectory τe ∈B, randomly select a time slot te
and extract the corresponding K-step segment τe,te, forming
a batch of trajectory segments, i.e., Bs = {τe,te}B
e=1.
7:
Pass Bs through the target DT πθ′ to predict actions for time
slot te, i.e., {ˆae,te : e ∈{1, · · · , B}}.
8:
Compute the TD target for every time slot of each trajectory
segment τe,te based on (30).
9:
Compute the critic losses for φ1 and φ2 based on (31), and
perform gradient descent to optimize them.
10:
Input the Bs into the DT model πθ to compute the predicted
actions, i.e., ˆa = {ˆah,r : 1 ≤h ≤B, th −K +1 ≤r ≤th}.
11:
Compute the DT loss based on (32), and optimize it via
gradient descent.
12:
update the target critic networks and target actor network
based on (33).
13: until The model is converged.
be ﬂexibly replaced with other casual LLMs, such as LLaMA
[35], with the choice of LLM depending on the tradeoff
between computational cost and performance requirements.
After initialization, we ﬁne-tune LLM-CRDT, which con-
sists of the data encoder, the pre-trained LLM, the action
decoder, and the critic networks. Notably, the pre-trained LLM
accounts for the majority of the model parameters, whereas
the parameter sizes of the other modules are comparatively
negligible. Given the large scale of LLMs, full-parameter ﬁne-
tuning is computationally expensive and risks overwriting the
valuable knowledge embedded in the pre-trained LLM, poten-
tially diminishing its few-shot generalization capabilities. To
this end, we adopt a parameter-efﬁcient ﬁne-tuning approach,
i.e., LoRA [26], to efﬁciently adapt the pre-trained LLM
for UAV control, thereby substantially reducing the number
of trainable parameters. Speciﬁcally, we apply LoRA to the
query, key, and value projection layers, as well as the self-
attention output layer in each transformer block of the LLM.
For a given layer with parameter matrix W ∈RdLLM×dLLM,
where dLLM denotes the LLM’s hidden dimension. LoRA
injects trainable low-rank matrices A ∈RdLLM×r and B ∈
Rr×dLLM into the layer. The original weight matrix W is then
modiﬁed as
W ←W + α
r BA,
(34)
where r is the LoRA adapter rank and α controls its inﬂuence.
During ﬁne-tuning, the weight matrix W remains frozen,
while only the LoRA adapter matrices A and B are updated.
Typically, A is initialized with Gaussian distribution values,
and B is initialized to zeros. Since r ≪dLLM, the number
of trainable parameters introduced by A and B, i.e., 2rdLLM,
is signiﬁcantly smaller than that of W (i.e., d2
LLM). Conse-
quently, LoRA enables efﬁcient and cost-effective ﬁne-tuning.
Furthermore, by keeping the original LLM weights ﬁxed,
LoRA mitigates catastrophic forgetting, thereby enabling us to
retain and exploit the universal reasoning and generalization
capabilities of the pre-trained LLM for UAV control.


--- Page 9 ---
9
Algorithm 3 Inference of LLM-CRDT for UAV control
1: Input: Trained DT model πθ, target RTG bR1, context length K.
2: Initialize the environment and obtain the initial state s1.
3: Initialize the trajectory history as τ = {(s1, ˆR1)} and time slot
t = 1.
4: for t = 1 : T do
5:
Construct the input sequence τt by concatenating the K
most-recent completed triples ( bRi, si, ai) for i
=
t −
K, · · · , t −1 in τ, followed by the current pair ( bRt, st).
6:
Left-pad the input sequence τt if t < K.
7:
Input τt into the DT model and obtain predicted action ˆat.
8:
Execute action at in the environment, apply Algorithm 1 for
device selection and resource allocation, and then obtain the
next state st+1 and reward rt.
9:
Update the RTG as ˆRt+1 = ˆRt −rt
10:
Append the triplets ( bRt, st, ˆat) to trajectory τ.
11:
if termination condition is met then
12:
Break the circulation.
With the assistance of LoRA, we ﬁne-tune the LLM-CRDT
to learn the UAV control policy from an ofﬂine trajectory
dataset D = {τe}E
e=1 with E trajectories. In particular, the
LLM weights are frozen, while only the parameters of the data
encoder, LoRA adapters, action decoder, and critic networks
are updated, as shown in Fig. 3(a). In each ﬁne-tuning epoch,
we randomly sample a batch of B trajectories B = {τe}B
e=1.
For each trajectory τe ∈B, we randomly select a time
slot te and extract the corresponding K-step segment τe,te
ending at te, thereby forming a batch of trajectory segments
Bs = {τe,te}B
e=1. We then pass Bs through the target DT
model to predict the action at time step te for each trajectory
segment τe,te, yielding ˆae,te. Following that, we compute TD
targets for each time step based on (30), evaluate the critic
losses via (31), and optimize the critic networks via gradient
descent. Subsequently, we feed Bs into the DT model to obtain
the predicted actions for each time step of each trajectory
segment, i.e., ˆae,i for e = 1, . . . , B and i = te−K +1, . . . , te.
Finally, we compute the loss of the DT model based on (32)
and perform gradient descent to update the parameters of
DT model. For clarity, the detailed steps for ﬁne-tuning the
proposed LLM-CRDT model are summarized in Algorithm 2.
C. Inference of LLM empowered Decision Transformer
After ﬁne-tuning, we deploy the proposed LLM-CRDT
model for online inference to control the UAV and complete
the data collection mission. In the deployment phase, only
the trained DT model πθ is required, while the target DT
model and critic networks are utilized solely during ﬁne-tuning
and are not involved in inference. During deployment, πθ
interacts with the environment in an autoregressive loop using
a context window of length K, consistent with the context
length adopted in ﬁne-tuning phase, as illustrated in Fig. 3(b).
At the beginning of an episode, we initialize the environ-
ment to obtain the initial state s1 and specify a desired RTG
bR1, which is usually set as the maximum possible return in
the training dataset [24]. Then, we perform the DT model πθ
to generate UAV control actions in each slot to complete the
data collection task. Speciﬁcally, in each time slot t, given the
current state st and RTG bRt, we assemble the model’s input
sequence as the most recent K triplets ( bRi, si, ai) up to time
slot t −1, followed by the pair ( bRt, st). If fewer than K time
slots are available, i.e., t < K, the sequence is left-padded
and masked. The input sequence is then processed in the same
manner as during ﬁne-tuning: it is ﬁrst encoded by the data
encoder, passed through the LLM, and ﬁnally decoded by the
action decoder to yield the predicted action ˆat. The action is
executed in the environment, producing the next state st+1
and reward rt. The RTG is subsequently updated as
ˆRt+1 = ˆRt −rt.
(35)
Furthermore, the new triplet ( ˆRt, st, ˆat) is appended to the
trajectory history and the context window is advanced to retain
at most K steps. This process is repeated until the episode
terminates. For clarity, we summarize the detailed steps of the
LLM-CRDT inference for UAV control in Algorithm 3.
V. SIMULATION RESULTS
This section evaluates the effectiveness of the proposed
LLM-CRDT and resource allocation approach. In the sim-
ulations, we consider the devices are randomly distributed
within a rectangular area with side length Xmax = 1 km
and Ymax = 1 km. The initial data volume of each device
n, i.e., Dn, is randomly selected from [0.5Dmax, Dmax].
Unless speciﬁed otherwise, the default settings of the UAV-
enabled IoT network are summarized in Table I, which follow
widely adopted conﬁgurations for UAV-enabled IoT networks
in prior works, e.g., [8]–[10], [12]. In the proposed LLM-
CRDT model, GPT-2 serves as the LLM backbone, while
both critic networks adopt a three-layer MLP architecture with
Mish activations and 512 hidden units per layer. For dataset
preparation, a behavior policy is ﬁrst trained using the SAC
algorithm, after which an ofﬂine dataset of 1000 trajectories
is collected by rolling out this policy in the environment. The
ﬁne-tuning of LLM-CRDT is performed over 1000 epochs
with a context length of K = 20, a LoRA rank of r = 16,
a LoRA scaling factor of α = 32, a batch size of B = 128,
a soft update coefﬁcient of ρ = 0.005, and a loss coefﬁcient
of λ = 1.0. Both the DT model and the critic networks are
optimized using the AdamW optimizer with a learning rate of
10−5.
TABLE I
SYSTEM PARAMETERS
Parameter
Value
Parameter
Value
N
100
H
100 m
Vmax
25 m/s
ι
2.2
κ
0.2
a
15
b
0.5
g0
-50 dB
pn
0.1 W
Dmax
20 MB
M
10
W
1 MHz
N0
-174 dBm
ωmax
1.0
T
500
δ
5 s
P1
79.86 W
P2
88.63 W
Utip
120 m/s
v0
4.03 m /s
d0
0.6
ρ
1.225
g
0.05
A
0.503 m2
A. Effectiveness of LLM-CRDT
In this subsection, we verify the effectiveness of the pro-
posed LLM-CRDT by comparing it with the following online


--- Page 10 ---
10
and ofﬂine RL approaches for UAV trajectory planning. Note
that, for all benchmark approaches, device selection and re-
source allocation decisions are obtained using our proposed
Algorithm 1, while the UAV control policies are determined
by each respective method.
• TD3 [18]: TD3 is an online RL method that combats
overestimation by using two critic networks and stabilizes
training via delayed policy updates.
• SAC [33]: SAC is a online RL algorithm that combines
off-policy learning with entropy maximization, encourag-
ing exploration and improving robustness.
• TD3+BC [22]: TD3+BC is an ofﬂine reinforcement learn-
ing approach that modiﬁes the policy update of the TD3
by incorporating a behavior cloning term to regularize the
policy, enabling effective learning in ofﬂine settings.
• SAC-N [23]: SAC-N is an ofﬂine RL approach that
modiﬁes the SAC algorithm by using an ensemble of
multiple critic networks to enhance performance in ofﬂine
settings. In this work, we employ four critic networks for
the SAC-N approach.
• Behavior Cloning (BC) [11]: BC is an imitation learning
method which learns a policy via supervised learning to
mimic actions in the training dataset.
• DT [24]: DT is an ofﬂine RL approach that reframes pol-
icy learning as a sequence modeling problem, leveraging
a transformer architecture to predict actions from past
states, actions, and desired returns.
• LLM-DT: This approach uses pretrained LLMs to ini-
tialize the transformer module in DT and applies LoRA
for ﬁne-tuning. The only difference from the proposed
LLM-CRDT is that the critic networks are not involved
for regularization during the ﬁne-tuning process.
0
200
400
600
800
1000
0
0.05
0.1
0.15
(a)
0
200
400
600
800
1000
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
2.2
2.4
(b)
Fig. 4. Comparison of learning performance for ofﬂine RL approaches.
Fig. 4 compares the learning performance of our proposed
LLM-CRDT to benchmark ofﬂine RL approaches. Fig. 4(a)
illustrates the action prediction errors of all ofﬂine approaches,
which gradually decrease and eventually converge as train-
ing progresses. This indicates that all methods are able to
ﬁt the actions in the dataset and ultimately align with the
underlying behavior policy. Fig. 4(b) presents the episode
reward, where the proposed LLM-CRDT outperforms all
benchmark approaches. This demonstrates the effectiveness
of the proposed critic-regularized learning paradigm and the
ﬁne-tuning of pre-trained LLMs in exploiting their universal
20
40
60
80
100
120
140
160
0
0.5
1
1.5
2
2.5
3
3.5
(a)
10
15
20
25
30
35
40
0.5
1
1.5
2
2.5
3
3.5
4
(b)
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
2.2
2.4
(c)
50
100
150
200
250
300
0.6
0.8
1
1.2
1.4
1.6
1.8
2
2.2
2.4
(d)
Fig. 5. Comparison between the proposed LLM-CRDT and benchmarks on
different system settings: (a) The number of IoT devices N; (b) The maximum
data amount of devices Dmax; (c) The maximum azimuth angle of the UAV
ωmax; (d) The ﬂight height of the UAV H.
reasoning capabilities. In addition, although BC, TD3-BC, and
SAC-N are able to ﬁt the behavior policy of the training
dataset, their evaluation rewards are unstable during training
and remain signiﬁcantly lower than those of DT and our pro-
posed LLM-CRDT. This performance gap arises because these
conventional ofﬂine RL methods are highly susceptible to out-
of-distribution states and actions not present in the training
dataset. In essence, these methods perform simple imitation
of the behavior policy without effective generalization beyond
the observed trajectories.
Fig. 5 compares the proposed LLM-CRDT with benchmark
online and ofﬂine RL approaches. Speciﬁcally, Fig. 5(a) shows
energy efﬁciency of all approaches increasing with device
number, since higher device density enables the UAV to collect
more data efﬁciently. Compared with DT, our LLM-CRDT
achieves a 29.5% improvement in energy efﬁciency. Fig. 5(b)
and Fig. 5(c) compare performance under varying maximum
data amounts and maximum azimuth angles, showing that
LLM-CRDT achieves up to 25% and 36.7% higher energy
efﬁciency than the benchmarks, respectively. Fig. 5(d) shows
that the performance of all methods grows with height at ﬁrst,
then deteriorates as ﬂying height increases further. This is
because a higher altitude expands the UAV’s sensing range,
whereas excessive height degrades transmission performance
between devices and the UAV. Moreover, the proposed ap-
proach surpasses DT by 25%. It is also observed that the
performance of DT nearly matches that of SAC, which is
used to collect the training dataset. This observation veriﬁes


--- Page 11 ---
11
0
200
400
600
800
1000
0
0.5
1
1.5
2
(a)
(b)
10%
20%
40%
80%
100%
0
0.5
1
1.5
2
2.5
(c)
Fig. 6. Impacts of training dataset quality and quantity on the proposed LLM-CRDT and DT approaches: (a) Learning performance of the behavior policy
used to generate the ofﬂine dataset; (b) Effect of dataset quality; (c) Effects of training data quantity.
the theoretical results in Theorem 1, namely that DT is
constrained by the quality of its training dataset and cannot
signiﬁcantly outperform the underlying behavior policy. In
contrast, LLM-DT achieves noticeable improvements over DT,
attributed to the universal knowledge embedded in the pre-
trained LLM that enhances action prediction. Finally, LLM-
CRDT consistently outperforms LLM-DT, demonstrating the
effectiveness of critic regularization in endowing the DT model
with stitching capabilities for superior decision-making.
B. Effects of Ofﬂine Dataset Quality
In this subsection, we assess how ofﬂine data quality and
quantity inﬂuence LLM-CRDT’s performance. For compar-
ison, in addition to the SAC-generated dataset used in the
previous simulations, we employ a trained TD3 agent to gen-
erate another dataset by rolling it out in the environment, each
containing 1000 trajectories. For clarity, we hereafter refer to
these two datasets as the SAC dataset and the TD3 dataset,
respectively. To illustrate their quality, Fig. 6(a) presents the
learning performance of the underlying agents, i.e., SAC and
TD3. As observed, SAC outperforms TD3, indicating that the
SAC dataset is of higher quality than the TD3 dataset.
Fig. 6(b) reports how training-data quality affects the pro-
posed LLM-CRDT and the baseline methods. For both the
SAC dataset and TD3 dataset, it is observed that the per-
formance of these approaches consistently follows the order:
LLM-CRDT > LLM-DT > DT. An interesting phenomenon
is that LLM-CRDT trained on the TD3 dataset performs
almost identically to LLM-CRDT trained on the SAC dataset.
This demonstrates that the proposed critic-regularized learning
paradigm equips LLM-CRDT with a stitching ability, allowing
it to combine actions from suboptimal trajectories into better
decisions. Moreover, LLM-CRDT trained on the TD3 dataset
still outperforms both LLM-DT and DT trained on the SAC
dataset, further conﬁrming its capability to learn effective
UAV control policies from suboptimal dataset. Therefore, the
proposed LLM-CRDT is able to address the key limitation
of DT approaches by mitigating the reliance of expert-level
dataset.
Fig. 6(c) evaluates the effect of training data quantity on the
performance of the proposed LLM-CRDT and the benchmark
approaches. Speciﬁcally, all methods are trained using 10%,
20%, 40%, 80%, and 100% of the SAC dataset. The results
show that LLM-CRDT consistently outperforms both LLM-
DT and DT across all settings. Notably, in the low-data regime
(i.e., below 40%), the DT model shows little performance
improvement, whereas the proposed LLM-CRDT and LLM-
DT still achieve satisfactory results. This demonstrates that
leveraging a pre-trained LLM effectively reduces the reliance
on large-scale training data by exploiting the universal knowl-
edge embedded in the model.
C. Effectiveness of device scheduling
This subsection evaluates the proposed device selection and
resource allocation approach, i.e., Algorithm 1, by comparing
it with the following baselines: 1) Random selection: In each
time slot, the UAV draws a random set of covered devices
and allocates the corresponding RBs, ensuring feasibility
with (13f)-(13j) 2) Remaining data-aware selection: The UAV
selects a subset of devices with the largest remaining data
volumes and allocates RBs to them in each slot, subject to
constraints (13f)-(13j). 3) Channel gain-aware selection: The
UAV selects a subset of devices with the highest channel
gains and allocates RBs to them while satisfying constraints
(13f)-(13j). In this evaluation, UAV control is managed by the
proposed LLM-CRDT. Notably, random selection corresponds
to choosing a random perfect matching of the graph G in
Section III-A. The remaining data-aware and channel gain-
aware schemes also construct similar bipartite graphs but
compute the maximum-weight perfect matching based on their
respective policies. The results are shown in Fig. 7, where the
proposed algorithm consistently outperforms all baselines. In
particular, it achieves up to a 55% improvement in energy
efﬁciency compared with the benchmark methods.
D. Effects of Different Conﬁgurations for LLM-CRDT
In this subsection, we investigate the impact of different
conﬁgurations in LLM-CRDT, including the rank of the LoRA
adapters, the context length, and the choice of the pre-trained
LLM. Table II reports the impact of the LoRA rank on the
energy efﬁciency performance of the proposed LLM-CRDT.
Note that r = 0 corresponds to the case where all LLM
parameters are frozen (no LoRA adapters are used), while


--- Page 12 ---
12
10
15
20
25
30
35
40
0
0.5
1
1.5
2
2.5
3
3.5
4
Fig. 7. Comparison of device selection and resource allocation approaches.
r = dLLM represents full-parameter ﬁne-tuning. Interestingly,
a small LoRA rank reliably outperforms both freezing and full
ﬁne-tuning. This phenomenon can be attributed to the general-
izable knowledge embedded in LLMs through large-scale pre-
training, which can be effectively transferred to UAV control
tasks. Since pre-trained LLMs are originally optimized on tex-
tual corpora rather than UAV trajectory optimization, simply
freezing them yields limited task-speciﬁc adaptability, while
full-parameter tuning risks overwriting and degrading this
transferable knowledge. In contrast, selective adaptation with
LoRA safeguards the pretrained model’s broad generalization,
enabling precise task-oriented tuning. As the LoRA adapter
rank r increases, both the total number of model parameters
and the proportion of trainable parameters grow, inevitably
incurring higher computational costs. While larger r values
generally improve performance, the gains diminish beyond a
certain point. Speciﬁcally, when r > 16, the improvements
become marginal. Therefore, the LoRA rank should be chosen
to balance computational budget with expected gains.
TABLE II
EFFECT OF LORA ADAPTER RANK
LoRA r
Energy
efﬁciency(MB/J)
Model
Parameters
%Trainable
Parameters
0
1.1704
126,148,816
1.35%
8
1.9157
126,591,184
1.69%
16
2.2975
127,033,552
2.04%
24
2.3945
127,475,920
2.38%
32
2.4632
127,918,288
2.72%
dLLM = 768
1.77825
338,215,812
100%
In Fig. 8, we explore the effect of context length and the
choice of pre-trained LLM on the performance of the proposed
LLM-CRDT. Speciﬁcally, we test four LLM variants from the
GPT-2 series [34], whose conﬁgurations are summarized in
Table III. It is observed that energy efﬁciency consistently
improves as the size of the pre-trained LLM increases. This
is because that larger LLMs with more parameters usually
possess stronger reasoning and generalization capabilities,
thereby enhancing the performance of LLM-CRDT. However,
the performance gap between GPT-2 XL and GPT-2 Large
is smaller than that between GPT-2 Medium and GPT-2,
showing that the performance gains diminish as the model size
grows. Moreover, larger LLMs also incur substantially higher
computational costs. Thus, the choice of LLM requires care-
fully balancing performance against computational efﬁciency.
Furthermore, increasing the context length yields noticeable
performance gains at ﬁrst, as longer histories provide richer
temporal information, allowing the model to more accurately
capture trajectory correlations. Nevertheless, after the context
length surpasses a certain threshold, i.e., 20, the performance
gain saturates. This is because additional historical information
beyond this point offers limited marginal beneﬁt, as the system
approaches its capacity to improve reward.
TABLE III
COMPARISON OF DIFFERENT LLMS
Model
#Params
#layer
Hidden size
#Heads
GPT-2
124M
12
768
12
GPT-2 Medium
355M
24
1024
16
GPT-2 Large
774M
36
1280
20
GPT-2 XL
1.5B
48
1600
25
5
10
15
20
25
30
35
1
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
Fig. 8.
Comparison of different device selection and resource allocation
approaches.
VI. CONCLUSION
This work studied the energy efﬁciency maximization prob-
lem for UAV-enabled data collection in IoT networks. The
resource allocation subproblem was transformed into an equiv-
alent linear programming and solved optimally, while the UAV
trajectory control problem was reformulated as an ofﬂine RL
problem. To overcome the reliance of expert-quality data and
large-scale training in existing DT methods, we proposed an
LLM-empowered critic-regularized DT framework, i.e., LLM-
CRDT, to learn effective UAV control policies from a ofﬂine
dataset. LLM-CRDT integrates critic-based value guidance
into DT training, enabling effective policy learning from
suboptimal data. Moreover, a pre-trained LLM was adopted
as the transformer backbone of LLM-CRDT, and LoRA-based
ﬁne-tuning was employed to enable rapid adaptation to UAV
control with small datasets and low computational overhead.
Simulations demonstrated that LLM-CRDT signiﬁcantly out-
performs benchmark online and ofﬂine RL approaches.
APPENDIX
A. Proof of Theorem 1
To prove Theorem 1, we ﬁrst introduce a bound that
characterizes the performance gap between return-conditioned


--- Page 13 ---
13
supervised learning methods (such as DT) and their underlying
behavior policy. Consider the MDP formulated in Section
III-B, an unknown behaviour policy πB, and a conditioning
function f. Assuming the following conditions:
1) Let g(τ)
=
PT
t=1 rt denotes the total return of a
trajectory τ. For all initial state s1 ∈S, the conditioning
function satisfy Pr (g(τ) = f(s1) |s1 ) ≥ζ.
2) The
MDP
is
ε-deterministic,
i.e.,
Pr (rt ̸= R(st, at) or st+1 ̸= P(st, at) |st, at ) ≤ε at
all st and at for the reward function R and environment
dynamics P.
3) The conditioning function is consistent of the RTG, i.e.,
f(st) = f(st+1) + rt for all st.
Then, the RCSL methods trained on data generated by the
behavior policy πB satisﬁes the following performance bound:
Es1 [f(s1)] −Eτ∼πRCSL
f
[g(τ)] ≤ε
1
ζ + 2

T 2,
(36)
where πRCSL
f
represents the policy learned by RCSL methods
when conditioned on function f. The proof of this bound is
provided in [36]. Given an ofﬂine datset D collected by the
behaviour policy πB, we choose the conditioning function as
f(s1) = P
r1:T ∼πB(s1) r and substitute it into (36), we have
Es1 [f(s1)] −Eτ∼πRCSL
f
[g(τ)]
= Es1
h X
r1:T ∼πB(s1) r
i
−Eτ∼πRCSL
f
[g(τ)]
= Eτ∼πB
h XT
t=1 rt
i
−Eτ∼πRCSL
f
[g(τ)]
= Eτ∼πB [g(τ)] −Eτ∼πRCSL
f
[g(τ)] .
(37)
Then recall the reward-to-go ˆRt = PT
i=t ri, it is obvious that
the conditioning function f(st) = ˆRt satisfy the requirement
about the consistence of conditioning function, i.e., condition
3). Thus, we have
Es1 [f(s1)]−Eτ∼πRCSL
f
[g(τ)]=Eτ∼πB [g(τ)]−Eτ∼πθ [g(τ)]
≤ε
1
ζ + 2

T 2,
(38)
where πθ is the learned policy of DT. Thus, the proof is
completed.
REFERENCES
[1] N. C. Luong, D. T. Hoang, P. Wang, D. Niyato, D. I. Kim, and Z. Han,
“Data collection and wireless communication in internet of things (iot)
using economic analysis and pricing models: A survey,” IEEE Commun.
Surveys Tuts., vol. 18, no. 4, pp. 2546–2590, 2016.
[2] M. Mozaffari, W. Saad, M. Bennis, Y.-H. Nam, and M. Debbah, “A
tutorial on uavs for wireless networks: Applications, challenges, and
open problems,” IEEE Commun. Surveys Tuts., vol. 21, no. 3, pp. 2334–
2360, 2019.
[3] Y. Li, W. Liang, W. Xu, Z. Xu, X. Jia, Y. Xu, and H. Kan, “Data col-
lection maximization in iot-sensor networks via an energy-constrained
uav,” IEEE Trans. Mobile Computing, vol. 22, no. 1, pp. 159–174, 2023.
[4] X. Yuan, Y. Hu, J. Zhang, and A. Schmeink, “Joint user scheduling and
uav trajectory design on completion time minimization for uav-aided
data collection,” IEEE Trans. Wireless Commun., vol. 22, no. 6, pp.
3884–3898, 2023.
[5] J. Gong, T.-H. Chang, C. Shen, and X. Chen, “Flight time minimization
of uav for data collection over wireless sensor networks,” IEEE J. Sel.
Areas Commun., vol. 36, no. 9, pp. 1942–1954, 2018.
[6] C. Zhan and Y. Zeng, “Completion time minimization for multi-uav-
enabled data collection,” IEEE Trans. Wireless Commun., vol. 18, no. 10,
pp. 4859–4872, 2019.
[7] X. Gao, X. Zhu, and L. Zhai, “Aoi-sensitive data collection in multi-
uav-assisted wireless sensor networks,” IEEE Trans. Wireless Commun.,
vol. 22, no. 8, pp. 5185–5197, 2023.
[8] X. Chen, N. Zhao, Z. Chang, T. H¨am¨al¨ainen, and X. Wang, “Uav-
aided secure short-packet data collection and transmission,” IEEE Trans.
Commun., vol. 71, no. 4, pp. 2475–2486, 2023.
[9] Y. Zhang, J. Lyu, and L. Fu, “Energy-efﬁcient trajectory design for uav-
aided maritime data collection in wind,” IEEE Trans. Wireless Commun.,
vol. 21, no. 12, pp. 10 871–10 886, 2022.
[10] H. Yang, Y. Ye, X. Chu, and S. Sun, “Energy efﬁciency maximization for
uav-enabled hybrid backscatter-harvest-then-transmit communications,”
IEEE Trans. Wireless Commun., vol. 21, no. 5, pp. 2876–2891, 2022.
[11] R. S. Sutton, A. G. Barto, et al., Reinforcement learning: An introduc-
tion.
MIT press Cambridge, 1998, vol. 1, no. 1.
[12] Y. Peng, T. Song, X. Song, Y. Yang, and W. Lu, “Time-effective uav-
irs-collaborative data harvesting: A robust deep reinforcement learning
approach,” IEEE Trans. Wireless Commun., vol. 23, no. 12, pp. 18 592–
18 607, 2024.
[13] Z. Chen, A. S. Alam, and A. Nallanathan, “Dynamic task software
caching-assisted computation ofﬂoading for multi-access edge comput-
ing,” IEEE Trans. Commun., vol. 70, no. 10, pp. 6950–6965, 2022.
[14] K. K. Nguyen, T. Q. Duong, T. Do-Duy, H. Claussen, and L. Hanzo, “3d
uav trajectory and data collection optimisation via deep reinforcement
learning,” IEEE Trans. Commun., vol. 70, no. 4, pp. 2358–2371, 2022.
[15] T. Zhang, J. Lei, Y. Liu, C. Feng, and A. Nallanathan, “Trajectory opti-
mization for uav emergency communication with limited user equipment
energy: A safe-dqn approach,” IEEE Trans. Green Commun. and Net.,
vol. 5, no. 3, pp. 1236–1247, 2021.
[16] Y. Yu, J. Tang, J. Huang, X. Zhang, D. K. C. So, and K.-K. Wong,
“Multi-objective optimization for uav-assisted wireless powered iot
networks based on extended ddpg algorithm,” IEEE Trans. Commun.,
vol. 69, no. 9, pp. 6361–6374, 2021.
[17] H. Kang, X. Chang, J. Miˇsi´c, V. B. Miˇsi´c, J. Fan, and J. Bai, “Improving
dual-uav aided ground-uav bi-directional communication security: Joint
uav trajectory and transmit power optimization,” IEEE Trans. Veh.
Technol., vol. 71, no. 10, pp. 10 570–10 583, 2022.
[18] S. Fujimoto, H. van Hoof, and D. Meger, “Addressing function approxi-
mation error in actor-critic methods,” in Proc. Int. Conf. Mach. Learning
(ICML), vol. 80, 2018, pp. 1587–1596.
[19] M. Sun, X. Xu, X. Qin, and P. Zhang, “Aoi-energy-aware uav-assisted
data collection for iot networks: A deep reinforcement learning method,”
IEEE Internet of Things J., vol. 8, no. 24, pp. 17 275–17 289, 2021.
[20] Y. Qin, Z. Zhang, X. Li, W. Huangfu, and H. Zhang, “Deep rein-
forcement learning based resource allocation and trajectory planning
in integrated sensing and communications uav network,” IEEE Trans.
Wireless Commun., vol. 22, no. 11, pp. 8158–8169, 2023.
[21] A. Mondal, D. Mishra, G. Prasad, and H. Johansson, “Joint trajectory,
user-association, and power control for green uav-assisted data collection
using deep reinforcement learning,” IEEE Trans. Intelligent Vehicles,
vol. 10, no. 1, pp. 548–561, 2025.
[22] S. Fujimoto and S. S. Gu, “A minimalist approach to ofﬂine reinforce-
ment learning,” in Advances in Neural Information Processing Systems,
vol. 34.
Curran Associates, Inc., 2021, pp. 20 132–20 145.
[23] G. An, S. Moon, J.-H. Kim, and H. O. Song, “Uncertainty-based ofﬂine
reinforcement learning with diversiﬁed q-ensemble,” in Advances in
Neural Information Processing Systems, vol. 34, 2021, pp. 7436–7447.
[24] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel,
A. Srinivas, and I. Mordatch, “Decision transformer: Reinforcement
learning via sequence modeling,” in Advances in Neural Information
Processing Systems, vol. 34, 2021, pp. 15 084–15 097.
[25] C. Ni, Z. Wang, Y. Ni, J. Li, L. Shi, and S. Jin, “Fast adaptive
optimization for aav-assisted mec: A constrained decision transformer
approach,” IEEE Wireless Commun. Letters, vol. 14, no. 4, pp. 1084–
1088, 2025.
[26] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,
and W. Chen, “LoRA: Low-rank adaptation of large language models,”
in Proc. Int. Conf. Learning Representations (ICLR), 2022.
[27] H. Zhu and J. Wang, “Chunk-based resource allocation in ofdma systems
- part i: chunk allocation,” IEEE Trans. Commun., vol. 57, no. 9, pp.
2734–2744, 2009.
[28] ——, “Chunk-based resource allocation in ofdma systems—part ii: Joint
chunk, power and bit allocation,” IEEE Trans. Commun., vol. 60, no. 2,
pp. 499–509, 2012.


--- Page 14 ---
14
[29] Z. Chen, W. Yi, H. Shin, and A. Nallanathan, “Adaptive model pruning
for communication and computation efﬁcient wireless federated learn-
ing,” IEEE Trans. Wireless Commun., vol. 23, no. 7, pp. 7582–7598,
2024.
[30] Z. Chen, W. Yi, Y. Liu, and A. Nallanathan, “Robust federated learning
for unreliable and resource-limited wireless networks,” IEEE Trans.
Wireless Commun., vol. 23, no. 8, pp. 9793–9809, 2024.
[31] A. Schrijver et al., Combinatorial optimization: polyhedra and efﬁ-
ciency.
Springer, 2003, vol. 24, no. 2.
[32] M. B. Cohen, Y. T. Lee, and Z. Song, “Solving linear programs in the
current matrix multiplication time,” Journal of the ACM (JACM), vol. 68,
no. 1, pp. 1–39, Jan. 2021.
[33] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic
actor,” in Proc. Int. Conf. Mach. Learning (ICML), vol. 80, 2018, pp.
1861–1870.
[34] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.,
“Language models are unsupervised multitask learners,” OpenAI blog,
vol. 1, no. 8, p. 9, 2019.
[35] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, et al.,
“Llama: Open and efﬁcient foundation language models,” arXiv preprint
arXiv:2302.13971, 2023.
[36] D. Brandfonbrener, A. Bietti, J. Buckman, R. Laroche, and J. Bruna,
“When does return-conditioned supervised learning work for ofﬂine
reinforcement learning?” in Advances in Neural Information Processing
Systems, vol. 35, 2022, pp. 1542–1553.
