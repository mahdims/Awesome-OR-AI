--- Page 1 ---
Best-of-∞– Asymptotic Performance of Test-Time Compute
Junpei Komiyama
Mohamed bin Zayed University of Artificial Intelligence
New York University
RIKEN AIP
junpei@komiyama.info
Daisuke Oba
Institute of Science Tokyo
daisuke.oba@nlp.comp.isct.ac.jp
Masafumi Oyamada
NEC Corporation
stillpedant@gmail.com
September 26, 2025
Abstract
We study best-of-N for large language models (LLMs) where the selection is based on majority vot-
ing. In particular, we analyze the limit N →∞, which we denote as best-of-∞. While this approach
achieves impressive performance in the limit, it requires an infinite test-time budget. To address this, we
propose an adaptive generation scheme that selects N based on answer agreement, thereby efficiently
allocating inference-time computation. Beyond adaptivity, we extend the framework to weighted ensem-
bles of multiple LLMs, showing that such mixtures can outperform any individual model. The optimal
ensemble weighting is formulated and efficiently computed as a mixed-integer linear program. Extensive
experiments demonstrate the effectiveness of our approach.
1
Introduction
The last few years have witnessed remarkable advancements in large language models (LLMs), in their
industrial successes including closed models such as Gemini (Gemini Team, 2025), GPT (OpenAI, 2023),
and Claude (Anthropic, 2025) as well as open-weight models such as Llama (Llama Team, 2024), Deepseek
(DeepSeek-AI, 2025), Qwen (Qwen Team, 2025; Ye et al., 2025; Cheng et al., 2025) and many others
including Liu et al. (2023); Almazrouei et al. (2023); Gao et al. (2023); Jiang et al. (2023a); Biderman et al.
(2023); BigScience Workshop (2023); OpenAI (2025); Wang et al. (2025); NVIDIA (2025); Abdin et al.
(2025); Ji et al. (2025); LG AI Research (2025). One of the largest interests in the realm of LLMs is on their
ability to perform complex reasoning tasks. A breakthrough in the reasoning of LLMs was the introduction
of chain-of-thought prompting (Wei et al., 2022; Kojima et al., 2023), which allows models to generate
intermediate reasoning steps before arriving at an answer. Instruction-tuned LLMs optimized to generate
longer chains of thought have drastically increased performance in these tasks (Muennighoff et al., 2025).
Spending more computational resources at test time, in particular by generating multiple answers, leads
to more reliable inference (Snell et al., 2025). A simple yet effective strategy is the best-of-N (BoN) ap-
proach, where we generate N answers and select the best one based on some criteria. There are several ways
to implement the BoN strategy. One common approach is to use a reward model to select the best answer
(Uesato et al., 2022; Rafailov et al., 2023; Wan et al., 2024; Dong et al., 2024; Liu et al., 2024; Wang et al.,
1
arXiv:2509.21091v1  [stat.ML]  25 Sep 2025


--- Page 2 ---
2024) or asking LLM to choose a preferable answer (Mahan et al., 2024; Son et al., 2024; Guo et al., 2025;
Chen et al., 2025a). Another approach is majority voting (Wang et al., 2023) in which the most frequent
answer is selected.
Despite its simplicity, majority voting has several advantages. First, it does not require any additional
modeling or further text generation. Second, compared with other methods, majority voting is robust to
reward hacking and benefits from additional generations with minimal risk, unlike reward-based models
where increasing N can lead to overfitting (Huang et al., 2025). Across datasets, majority voting perfor-
mance generally increases with N (Figure 1).
While we desire to achieve such Best-of-N performance of N →∞, which we call best-of-∞perfor-
mance, it requires an infinite number of generations (samples), which is infeasible in real-world scenarios.
Yet, for the same test-time budget, we can utilize the available budget more effectively. As shown in Figure
2, we can generate samples adaptively until we determine the majority with some confidence level. We
introduce a principled method to determine when to stop generating answers and when to continue using
Bayesian modeling (Section 2).
Our scheme can be naturally extended to ensembles of multiple LLMs. Importantly, ensemble majority
voting can naturally benefit from complementarity. For example, in the AIME2025 dataset, the best-of-∞
performance of GPT-OSS-20B (OpenAI, 2025) and Nemotron-Nano-9B-v2 (NVIDIA, 2025) are 90.0% and
73.0%, respectively, but their ensemble achieves 93.3%. A weak LLM can contribute to the ensemble if it
has complementary strengths 1. In this paper, we introduce the optimally weighted LLM ensemble for the
asymptotic scheme and show that its optimization reduces to a mixed-integer linear program (Section 3).
This is a notable advantage of considering best-of-∞performance, as opposed to optimizing the weights
for finite N: for moderate N, optimization has to consider a very large number of combinations, which is
infeasible.
Finally, we evaluate the performance of the proposed method (Section 4). Our experimental results in-
clude 11 instruction-tuned LLMs and four heavy-reasoning problem sets (AIME2024, AIME2025, GPQA-
DIAMOND, MATH500), with at least 80 generations for each LLM–problem set combination. This repre-
sents a significantly larger scale of test-time computation than prior work. We release our generation results
for subsequent research. A brief discussion concludes the paper (Section 5). Related work are discussed in
Appendix A.
1See more information in Appendix E.
Figure 1: Accuracy of Best-of-N with majority voting as a function of N (GPT-OSS-20B (Medium)) with
four datasets (Maxwell-Jia, 2024; OpenCompass, 2025; Rein et al., 2023; Hendrycks et al., 2021). Green
line indicates the asymptotic accuracy of N →∞. For each problem, BoN benefits from increasing N, at
least from N = 101 to 102.
2


--- Page 3 ---
(A)          (B)         (C)         (D)
Frequency 
Frequency 
(A)      (A)      (A)
→ Terminate
(B)      (C)      (A)      (C)      (D)     (C)
→ Terminate
Figure 2: An illustration of adaptive sampling (Algorithm 1). The histogram shows the distribution of
answers generated by an LLM for a single problem. Each answer generation can be viewed as a sample
from the underlying distribution. Blue indicates the most frequent answer, and orange indicates the others.
In the top example, three generations agree, so sampling stops. In the bottom example, more samples are
needed to determine the majority. This maximizes the accuracy under a given compute budget. Confidence
in the majority is based on the Bayes factor.
2
Best-of-∞in finite samples
Algorithm 1 Approximated Best-of-∞: Determining answer for single problem
Require: Maximum samples Nmax, concentration parameter α, Bayes factor threshold B.
1: for n = 1, 2, . . . do
2:
if we use LLM Ensemble (Section 3) then
3:
Choose LLM with probability {wi}i∈K.
4:
end if
5:
Ask the LLM for the answer of the problem to obtain answer.
6:
if n = Nmax or BF(n) ≥B then
7:
break
8:
end if
9: end for
10: return The most frequent answer.
While Best-of-∞defines an idealized best-of-N ensemble in the limit N →∞, its literal realization
would require unbounded test-time compute. We now develop a finite-sample procedure that closely tracks
this limit. Our core idea is to adaptively samples (i.e., ask LLM to generate the answers) until we are sure
the population majority vote with a desired confidence level. In other words, we aim to terminate the answer
generation process as soon as sufficient statistical evidence has been obtained to support the conclusion that
the currently most frequent response corresponds to the true majority, which allows different number of
N across problems. A distinctive challenge of this problem lies in the fact that the support of the answer
distribution generated by large language models (LLMs) is unknown. For instance, in one case an LLM may
produce two candidate answers, such as 42 with probability 70% and 105 with probability 30%, whereas
in another case it may yield four distinct outputs, such as 111 with probability 40%, 1 with probability
3


--- Page 4 ---
25%, 2 with probability 20%, and 702 with probability 15%. Given such uncertainty in the variation of
generated responses, a particularly well-suited approach is to employ nonparametric Bayesian modeling. In
particular, we adopt a Dirichlet process DP(H, α) prior over the answer space that captures the unknown
distribution of answers. Here, H is a base distribution2 over the answer space, and α > 0 is a concentration
parameter that controls the likelihood of generating new answers. Intuitively speaking, α is the strength of
the prior belief in the existence of new answers. Assume that, at round n, we observe s(n) different answer
A1, A2, . . . , As(n) with corresponding counts N1 ≥N2 ≥N3 · · · ≥Ns(n). Then, the posterior distribution
is
DP

α
α + nH
|
{z
}
base distribution
+
1
α + n
s(n)
X
j=1
NjδAj
|
{z
}
empirical distribution
, α + n

.
(1)
The first argument of the posterior above states that the posterior is increasingly concentrated around the
observed answers as more data is collected.
We use the Bayes factor (Jeffreys, 1935; Good, 1967; Kass and Raftery, 1995; Lindon and Malek, 2022)
to measure the evidence of true majority.3 Formally, we define the hypotheses as follows:
H0 : The most frequent answer A1 is not the true majority.
(2)
H1 : The most frequent answer A1 is the true majority.
(3)
and define the Bayes factor (BF), which quantifies the strength of evidence in the data for H1, as
BF := P(D(n)|H1)
P(D(n)|H0),
(4)
where D(n) is the observed data so far. Here, P(D(n)|H1), P(D(n)|H0) are the evidence (marginal likeli-
hood) based on the observed data. Then, the Bayes factor of equation 4 can be computed as follows:
BF(n) := P(D(n)|H1)
P(D(n)|H0) = P(H1|D(n))
P(H0|D(n)) · P(H0)
P(H1)
(Bayes’ theorem)
(5)
≈s(n)P(H1|D(n))
P(H0|D(n))
(approximating the prior ratio by uniform prior)
(6)
= s(n)
P(H1|D(n))
1 −P(H1|D(n))
(H0 ∪H1 is the entire space)
(7)
where P(H1|D(n)), P(H0|D(n)) are the corresponding posteriors. Note that, in the second line, we approx-
imated the DP prior with a uniform prior over the existing answers.
When n is sufficiently large compared with α, P(H1|D(n)) of the DP posterior can approximated by a
Dirichlet distribution as:
P(H1|D(n)) ≈Pr[X1 ≥max
i̸=1 Xi, X ∼Dirichlet(N1 + 1, N2 + 1, . . . , Ns(n) + 1, α)],
by approximating the probability of A1 appearing in the base distribution H to be zero. The Dirichlet
distribution is a conjugate distribution of the categorical distribution of s(n) + 1 of answers, where the
2The base distribution can have a possibly infinite support, such as all possible integers. For some tasks, such as GPQA, the
answer is given in a finite domain (e.g., A, B, C, D), and thus the base distribution is of a finite support. In such cases, Dirichlet
process is exactly the same as the Dirichlet distribution. The advantage of the Dirichlet process is to unify the treatment for both
finite and infinite answer spaces, as well as having some reguralization with a hyperparameter α.
3The use of the Bayes factor for categorical data is not new. See Lindon and Malek (2022) for a recent application to A/B
testing. Unlike their case, our case starts from an unknown number of categories, which is handled by the Dirichlet process prior
and via some approximation.
4


--- Page 5 ---
last dimension corresponds to the unobserved answers. Here, the final component of weight α is added to
account for the base distribution H. While this quantity is not trivial to compute, it can be estimated using
Monte Carlo methods by sampling from the Dirichlet distribution.
The following theorem states that, if we set Nmax and B sufficiently large, the algorithm’s performance
converges to the best-of-∞performance. The proof is given in Appendix B.
Theorem 1. (Consistency) Assume that the LLM generates a finite number of answers 1, 2, . . . , s. For ease
of discussion, let pj be the probability of answer j and assume that p1 > p2 ≥p3 ≥. . . ≥ps > 0. Namely,
there are no ties for the most frequent answer, and each answer is generated with a non-zero probability.
Then, as Nmax, B →∞, the algorithm’s performance converges to the best-of-∞performance almost
surely. Namely, the algorithm returns the true majority answer with probability 1.
3
LLM Ensemble
Algorithm 1 is naturally extended to use more than one LLM. Let i ∈K index the LLMs, and let w =
(w1, w2, . . . , wK) be the weight vector, where wi ≥0 and P
i∈K wi = 1. Algorithm 1 with an LLM
ensemble proceeds as follows: for each generation, we first select an LLM i with probability wi, and then
ask the selected LLM for the answer.
Let us consider the optimal weighting scheme for the BoN inference. Let q ∈Q be the problem. Each
problem is associated with answer domain Aq.
Example 1 (AIME2025). For AIME2025, Aq ⊆{1, 2, . . . , 999, U}, where U denotes either an out-of-
range integer, fractional number, or a failure to emit a final answer; U is always incorrect.
For each problem, let gq ∈Aq be the gold answer. Each LLM-problem pair (i, q) is the probability
distribution Piq over Aq.
For each problem q, we obtain multiple generations from the LLMs and take a majority vote to produce
aq. The total number of correct answers is
f({aq}) :=
X
q∈Q
1[aq = gq].
(8)
We aim to maximize it in expectation: E[f({aq})]. Here, the expectation is taken over the randomness in
the generation of LLMs.
3.1
Best-of-one
Before going into Best-of-∞, we first consider the best-of-one (Bo1) policy, which first selects an LLM with
probability proportional to w, and then uses the LLM to generate a single answer. An immediate observation
is that the optimal weight is to put all the weight on the best LLM.
Lemma 1. (Optimal Bo1) The accuracy of Eq. equation 8 is maximized when we choose wi∗= 1 and wj =
0 for all j ̸= i∗, where wi∗is the weight for the best LLM i∗. Namely, let pq
i = (pq
i,1, pq
i,2, . . . , pq
i,|Aq|) ∈∆Aq
be the probability distribution on Aq of the answers that LLM i generates. Then, pq
i,gq be the probability that
LLM i generates the gold answer gq for problem q. The average accuracy of LLM i is P
q pq
i,gq, and the best
LLM, which maximizes this quantity, is i∗= arg maxi∈K
P
q pq
i,gq.
Proof. It is easy to see that
f({aq}) :=
X
i
wi

X
q∈Q
pq
i,gq

≤max
i

X
q∈Q
pq
i,gq

.
5


--- Page 6 ---
For Bo1, the optimal weight is to put all the weight on the best LLM. However, this is no longer the
case for BoN with N > 1. Put differently, under multi-generation majority voting, appropriately mixing
non-optimal LLMs can be beneficial.
3.2
Best-of-∞
As in the Bo1 setting, our design choice is to take a weighted majority vote with w = (w1, . . . , wK). When
we consider the large-sample limit, the answer for problem n is deterministic:4
aq = arg max
j
(X
i∈K
wipi,j
)
.
Consequently f(aq) is also deterministic:
f({aq}) =
X
q∈Q
1[aq = gq].
Here, for ease of discussion, we omit the consideration for a tie. Henceforth, since our design choice is on
the weight vector w, we denote it f(w) and use f({an}) and f(w) interchangeably.
Our central question is how to choose a weight vector w that maximizes the accuracy f(w). The follow-
ing lemma implies the hardness of optimizing f(w).
Lemma 2. (Non-concavity) f(w) is a non-concave function on the simplex space of w.
Proof. Consider a dataset of just one question with two LLMs, where one LLM correctly answer the ques-
tion and the other LLM fails. Namely, f((1, 0)) = 1 and f((0, 1)) = 0. Then the weighted combination is
0 at somewhere in between, which implies it is non-concave.
While the proof above is an extremely simple case of two LLMs with a single problem, we will demon-
strate the non-concavity in more complex cases.
Although non-concavity implies sub-optimality of gradient-based methods, a combinatorial optimiza-
tion approach can be adopted for instances of typical scale. The crux in optimizing f(w) is that the summand
in equation 8 takes value one within a polytope.
Lemma 3. (Polytope lemma) Let {pq
ij}i∈[K],j∈Aq be the arbitrary distributions of the answers. Then, the
following set, which implies that answer j is the most frequent answer, is a polytope:
(
w ∈∆K :
X
i
wipq
ij > max
j′̸=j
X
i
wipq
ij′
)
.
(9)
Proof. The region of equation 9 is an intersection of the following half-spaces:
w :
X
i
wipq
ij >
X
i
wipq
ij′
for all j′ ̸= j, which is a polyhedron. Since the desired space is an intersection of a polyhedron and a
simplex ∆Aq, it is finite. Therefore, it is a polytope.
4We use the term deterministic to describe a non-random quantity.
6


--- Page 7 ---
Figure 3: Visualization of the non-concave objective function f(w) over the weight simplex w. The yellow
simplex corresponds to w in the simplex of the weights of the three LLMs. The gray region of the five
polytopes (= five problems) are the region where the weighted majority of the corresponding weight cor-
rectly answer to the problem. The optimal solution is the intersection of four polytopes at the center, which
corresponds to the case where four out of five problems are correctly answered.
Lemma 3 states that the maximization on the number of correct answers is equivalent to the maxi-
mization on the number of polytopes that contain w (Figure 3). By introducing auxiliary variable yq that
indicates the correctness for each answer, this can be formulated as a mixed-integer linear programming
(MILP) problem.
Lemma 4. (MILP formulation) The equation 8 is equivalent to the following MILP problem:
max
w∈∆K,y∈{0,1}N
X
q
yq
(10)
s.t.
wi ≥0 ∀i
(11)
X
i
wi = 1
(12)
Aqw ≥−m(1 −yq) ∀q
(13)
where Aq is a matrix of size R|Aq|×K such that its j, i entry is pq
i,gq −pq
i,j, and the j-th row corresponds
to the fact that the total weight of the gold answer gq is larger than that of a wrong answer j. The vector
m > 0 is chosen sufficiently large, so that Aqw ≥m is never satisfied when Aqw has a negative component.
The size of the problem instance depends on the number of LLMs K, the number of problems N, and the
size of the possible set of answers Aq. General MILP solving is NP-hard; in practice, however, open-source
solvers scale smoothly to K ≈101 LLMs and N ≈103 problems, where typical size of Aq is ≈101.
Max margin solutions
As we illustrated in Figure 3, the objective function f(w) has continuous region
of optimal solutions. While any interior point on these position is optimal in best-of-∞, its finite-N perfor-
mance can vary. In this paper, we adopt a “max margin” solution, that is at the most interior of the solution.
Namely, we introduce a margin ξ > 0 and replaces Aqw in equation 13 with Aqw −ξ. We choose the
supremum of the margin ξ such that the objective value P
q yq does not decrease, and adopts the solution on
7


--- Page 8 ---
such margin. The optimization of margin can be done a binary search on the space of ξ ∈[0, m] where m is
a sufficiently large constant. This is a binary search problem of a monotone objective, which is practically
feasible.
4
Experiments
This section reports our experimental results. We considered heavy-reasoning tasks on open-weight LLMs
that we can test on our local environment. We set Algorithm 1’s hyperparameter α = 0.3 for all the
experiments. To solve MILPs, we use highspy, an open-source Python interface to the HiGHS optimization
suite (Huangfu and Hall, 2018), which provides state-of-the-art solvers for large-scale LP, MIP, and MILP.
We adopt the max-margin solution described in Section 3.2. Unless specified otherwise, all results are
estimated from 100 independent runs. The Bayes factor is calculated with 1,000 Monte Carlo samples from
the posterior. Due to page limits, we show only several experimental results in the main text. More results
are available in Appendix F.
4.1
Tested open-weight LLMs and datasets
We evaluate open-weight LLMs (≤32B parameters) across four reasoning benchmarks. We use the follow-
ing problem sets: AIME2024 (Maxwell-Jia, 2024), AIME2025 (OpenCompass, 2025), GPQA-DIAMOND
(Graduate-Level Google-Proof Q&A Benchmark; Rein et al. 2023), and MATH500 (Hendrycks et al., 2021).
Details of the LLMs and datasets are provided in Appendix C. These datasets are challenging mathematical
and scientific reasoning tasks. We did not test GSM8K (Cobbe et al., 2021) as it is too easy for the LLMs
we tested.
Large-scale generation dataset
We generate a set of candidate answers by querying the LLM with the
problem statement. For each pair of (LLM, problem), we generate at least 80 answers—an order of magni-
tude greater than the typical 8 generations reported in most LLM technical reports. We believe the difficulty
of the problems as well as the scale of generated tokens are significantly larger than existing work on test-
time computing.5 Table 1 shows the statistics of the datasets used in our experiments. Base performance
(Bo1, best-of-∞) of these LLMs are shown in Appendix D. Every sample of answer in our subsequent
experiments is drawn from this dataset. Best-of-∞performance is also estimated from these samples. We
remove the unparseable answers, which benefits some of the LLMs with lower performance.
4.2
Experimental results
Experimental Set 1: Effectiveness of adaptive sampling
First, we investigate the impact of adaptive
sampling scheme of Algorithm 1 on the performance of majority voting. We set Nmax = 100 and tested
varying Bayes factor B = {2, 3, 5, 7, 10, 30, 100, 300, . . . }. Figure 4 (left) compares the performance of
Algorithm 1 with fixed budget of samples (BoN), where x-axis is the number of average samples per problem
(log-scale), and y-axis is the accuracy. The figure clearly shows that the blue curve (Algorithm 1) achieves
the same accuracy as the red curve (fixed BoN) with substantially fewer samples. Figure 4 (right) shows
the average total number of tokens as a function of accuracy. The adaptive method again demonstrates
a significant reduction in token usage to achieve the same accuracy level compared to the fixed method,
although the gap is smaller than that of the sample count. This is because the adaptive method tends to stop
sampling early for easier problems, which often require fewer tokens per generation.
5Also note that, for adaptive sampling scheme, around 80 samples are usually sufficient to achieve accuracy fairly close to the
best-of-∞performance.
6We do not use chain-of-thought (CoT) in our experiments and thus the file size is small; however, we also include an updated
dataset that contains CoT.
8


--- Page 9 ---
LLM
# of files
total generated tokens
total file size (MB)
AM-Thinking-v1
4,800
79,438,111
185.95
Datarus-R1-14B-preview
4,800
49,968,613
127.03
EXAONE-Deep-32B
60,640
478,575,594
1,372.35
GPT-OSS-20B
68,605
244,985,253
98.596
LIMO-v2
6,095
77,460,567
219.45
MetaStone-S1-32B
60,757
806,737,009
2,458.48
NVIDIA-Nemotron-Nano-9B-v2
60,640
295,466,626
897.82
Phi-4-reasoning
168,138
558,980,037
1,841.06
Qwen3-4B
20,640
547,170,887
1,704.28
Qwen3-14B
44,800
666,466,780
1,822.13
Qwen3-30B-A3B-Thinking-2507
60,640
436,865,220
1,234.28
Table 1: Statistics of the large-scale generation dataset that we used in our experiments. Each file corre-
sponds to a single answer. We release it with our code.
Figure 4: Cost-analysis of our proposed method and fixed BoN. GPT-OSS-20B on MATH500. “Adaptive”
Algorithm 1 with average sample size of ¯N = 3 achieves the same accuracy as “fixed” sample of N = 10,
and the algorithm with average sample size ¯N ≈10 achieves the same accuracy as fixed N = 100. Thus,
the adaptive sampling in this plot reduced the computation times by 2x-5x order. Both approach the best-
of-∞performance (green dashed line).
Experimental Set 2: Advantage of LLM ensemble over single LLM
Second, we investigate the advan-
tage of LLM ensemble over single LLM. We compare the performance of the single LLM with the optimal
mixture of LLMs. The results in Figure 5 show that the ensemble method achieves higher accuracy than any
single LLM, demonstrating the effectiveness of combining multiple models.
Experimental Set 3: Learning a good weight
Third, we investigate the generalization ability of our
weight optimization method (Section 3). Figure 6 shows the performance of the learned weights as a func-
tion of the number of training problems on AIME2025. With five training problems, the learned weights
approach the best single-LLM performance.
Experimental Set 4: Transfer learning of the optimal weight
To assess transferability, we trained
weights on AIME2024 and tested on AIME2025; across 165 three-model combinations, the ensemble
matched or exceeded the strongest individual model in 106 cases (64.2%).
9


--- Page 10 ---
Figure 5: Performance comparison of the LLM ensemble of EXAONE-Deep-32B, MetaStone-S1-32B, Phi-
4-reasoning, Qwen3-30B-A3B-Thinking, and GPT-OSS-20B on GPQA-Diamond. The weight is optimized
to w = (0.0176, 0.0346, 0.2690, 0.4145, 0.2644). The LLM ensemble outperforms any single LLM with
N ≥5 and approaches the blue dashed line of best-of-∞performance.
Figure 6: The number of samples to determine the weight (x-axis) as a performance of best-of-∞(y-axis) on
AIME2025. The x-axis indicates the number of problems used to learn the weight and the y-axis indicates
the best-of-∞performance with all problems. The score is averaged over 100 runs. The optimal weight has
achieved the limit accuracy of 93.3%, whereas the best single LLM has the limit accuracy of 90.0%. Dashed
lines indicate the best-of-∞performance of each LLM.
Experimental Set 5: Comparison with other answer-selection methods
We finally compared the ma-
jority voting scheme with other selection scheme in the best-of-five (Bo5) test-time inference. On AIME2025,
majority voting outperforms random selection, self-certainty, reward models, and LLM-as-a-judge; full ta-
bles and settings are provided in the appendix (Appendix F.5).
5
Conclusion
In this paper, we view the best-of-N strategy with majority voting as sampling from the underlying answer
distribution, with its best-of-∞performance naturally defined. To approximate this limit with a finite num-
ber of samples, we introduce an adaptive sampling method based on the Bayes factor. We also study the
10


--- Page 11 ---
Method
Mean ± CI
Omniscient
91.04 ± 1.32
Majority voting
85.42 ± 2.01
LLM-as-a-judge (tournament)
82.92 ± 2.57
LLM-as-a-judge (set)
81.25 ± 2.42
INF-ORM-Llama3.1-70B
79.79 ± 2.54
Skywork-Reward-V2-Llama-3.1-8B
79.79 ± 2.47
Skywork-Reward-V2-Qwen3-8B
80.00 ± 2.51
Self-certainty
75.83 ± 2.47
Random
76.25 ± 2.71
Table 2: The accuracy of several selection methods on the best-of-five (Bo5) setting on the AIME2025
dataset. Answers are generated by GPT-OSS-20B. The scores are averaged over 16 trials and we report the
two-sigma confidence intervals. Omniscient is a hypothetical upper bound that always selects the correct
answer if it is present in the candidate answers, which requires the gold answer. Random, which selects
one of N answers uniformly at random, should match the performance of Bo1. Details of each method are
described in Appendix F.5.
problem of aggregating responses from multiple LLMs and propose a majority voting that effectively lever-
ages the strengths of individual models. The best-of-∞performance has advantage because the weights
of LLM ensembles can be optimized by solving a mixed-integer linear programming problem. Through
extensive experiments on challenging reasoning tasks, we have verified robust across LLMs and problem
sets. The scale of our generated answers is substantially larger than in existing work, and we release these
generations for future research.
References
Abdin, M., Agarwal, S., Awadallah, A., Balachandran, V., Behl, H., Chen, L., de Rosa, G., Gunasekar, S.,
Javaheripi, M., Joshi, N., Kauffmann, P., Lara, Y., Mendes, C. C. T., Mitra, A., Nushi, B., Papailiopoulos,
D., Saarikivi, O., Shah, S., Shrivastava, V., Vineet, V., Wu, Y., Yousefi, S., and Zheng, G. (2025). Phi-4-
reasoning technical report.
Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Alneyadi, S., Maggioni, M., and ...
(2023). The falcon series of open language models. arXiv preprint arXiv:2311.16867.
Anthropic (2025). Claude opus 4 & claude sonnet 4 system card. System card, May 2025. Includes model
descriptions, safety testing, hybrid reasoning modes.
Biderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., O’Brien, K., Hallahan, E., Khan, M. A., Purohit,
S., Prashanth, U. S., Raff, E., Skowron, A., Sutawika, L., and van der Wal, O. (2023). Pythia: A suite for
analyzing large language models across training and scaling.
BigScience Workshop (2023). Bloom: A 176b-parameter open-access multilingual language model.
Chen, X., Li, G., Wang, Z., Jin, B., Qian, C., Wang, Y., Wang, H., Zhang, Y., Zhang, D., Zhang, T., Tong,
H., and Ji, H. (2025a). Rm-r1: Reward modeling as reasoning.
Chen, Z., Li, J., Chen, P., Li, Z., Sun, K., Luo, Y., Mao, Q., Yang, D., Sun, H., and Yu, P. S. (2025b).
Harnessing multiple large language models: A survey on llm ensemble.
11


--- Page 12 ---
Cheng, Z., Fan, R., Hao, S., Killian, T. W., Li, H., Sun, S., Ren, H., Moreno, A., Zhang, D., Zhong, T.,
Xiong, Y., Hu, Y., Xie, Y., Han, X., Wang, Y., Pimpalkhute, V., Zhuang, Y., Singh, A., Liang, X., Xie, A.,
She, J., Fan, D., Gao, C., Ma, L., Yurochkin, M., Maggs, J., Ma, X., He, G., Hu, Z., Liu, Z., and Xing,
E. P. (2025). K2-think: A parameter-efficient reasoning system.
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J.,
Nakano, R., Hesse, C., and Schulman, J. (2021). Training verifiers to solve math word problems. arXiv
preprint arXiv:2110.14168.
Dawid, A. P. and Skene, A. M. (1979). Maximum likelihood estimation of observer error-rates using the em
algorithm. Applied Statistics, 28(1):20–28.
DeepSeek-AI (2025). Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.
Dong, H., Xiong, W., Pang, B., Wang, H., Zhao, H., Zhou, Y., Jiang, N., Sahoo, D., Xiong, C., and Zhang,
T. (2024). RLHF workflow: From reward modeling to online RLHF. Transactions on Machine Learning
Research.
Fu, Y., Wang, X., Tian, Y., and Zhao, J. (2025). Deep think with confidence.
Gao, L., Black, S., Wang, P., and et al. (2023). Cerebras-gpt: A family of open, compute-efficient, large
language models. arXiv preprint arXiv:2304.03208.
Gemini Team (2025).
Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long
context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261. Submitted July 7,
2025; revised July 22, 2025.
Good, I. J. (1967). A bayesian significance test for multinomial distributions. Journal of the Royal Statistical
Society Series B: Statistical Methodology, 29(3):399–418.
Guha, N., Chen, M. F., Chow, T., Khare, I. S., and Re, C. (2024). Smoothie: Label free language model
routing. In The Thirty-eighth Annual Conference on Neural Information Processing Systems.
Guo, J., Chi, Z., Dong, L., Dong, Q., Wu, X., Huang, S., and Wei, F. (2025). Reward reasoning model.
Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. (2021).
Measuring mathematical problem solving with the math dataset. NeurIPS.
Huang, A., Block, A., Liu, Q., Jiang, N., Krishnamurthy, A., and Foster, D. J. (2025). Is best-of-n the best
of them? coverage, scaling, and optimality in inference-time alignment.
Huangfu, Q. and Hall, J. A. J. (2018). Parallelizing the dual revised simplex method. Mathematical Pro-
gramming Computation, 10(1):119–142.
Inoue, H. (2019). Adaptive ensemble prediction for deep neural networks based on confidence level. In
Chaudhuri, K. and Sugiyama, M., editors, Proceedings of the Twenty-Second International Conference
on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pages
1284–1293. PMLR.
Jeffreys, H. (1935). Some tests of significance, treated by the theory of probability. Mathematical Proceed-
ings of the Cambridge Philosophical Society, 31(2):203–222.
Ji, Y., Tian, X., Zhao, S., Wang, H., Chen, S., Peng, Y., Zhao, H., and Li, X. (2025). Am-thinking-v1:
Advancing the frontier of reasoning at 32b scale.
12


--- Page 13 ---
Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F.,
Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T.,
Wang, T., Lacroix, T., and Sayed, W. E. (2023a). Mistral 7b.
Jiang, D., Ren, X., and Lin, B. Y. (2023b). LLM-blender: Ensembling large language models with pairwise
ranking and generative fusion. In Rogers, A., Boyd-Graber, J., and Okazaki, N., editors, Proceedings
of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 14165–14178, Toronto, Canada. Association for Computational Linguistics.
Kajino, H., Tsuboi, Y., and Kashima, H. (2012).
A convex formulation for learning from crowds.
In
Hoffmann, J. and Selman, B., editors, Proceedings of the Twenty-Sixth AAAI Conference on Artificial
Intelligence, July 22-26, 2012, Toronto, Ontario, Canada, pages 73–79. AAAI Press.
Kass, R. E. and Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical Association,
90(430):773–795.
Kaufmann, E. and Koolen, W. M. (2021). Mixture martingales revisited with applications to sequential tests
and confidence intervals. J. Mach. Learn. Res., 22(1).
Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. (2023). Large language models are zero-shot
reasoners.
Kolter, J. Z. and Maloof, M. A. (2007). Dynamic weighted majority: An ensemble method for drifting
concepts. Journal of Machine Learning Research, 8(91):2755–2790.
LG AI Research (2025).
Exaone deep:
Reasoning enhanced language models.
arXiv preprint
arXiv:2503.12524.
Li, J., Baba, Y., and Kashima, H. (2017). Hyper questions: Unsupervised targeting of a few experts in crowd-
sourcing. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management,
CIKM ’17, page 1069–1078, New York, NY, USA. Association for Computing Machinery.
Li, J., Zhang, Q., Yu, Y., Fu, Q., and Ye, D. (2024). More agents is all you need. Transactions on Machine
Learning Research.
Lindon, M. and Malek, A. (2022). Anytime-valid inference for multinomial count data. In Proceedings of
the 36th International Conference on Neural Information Processing Systems, NIPS ’22, Red Hook, NY,
USA. Curran Associates Inc.
Liu, C. Y., Zeng, L., Liu, J., Yan, R., He, J., Wang, C., Yan, S., Liu, Y., and Zhou, Y. (2024). Skywork-
reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451.
Liu, Z., Qiao, A., Neiswanger, W., Wang, H., Tan, B., Tao, T., Li, J., Wang, Y., Sun, S., Pangarkar, O.,
Fan, R., Gu, Y., Miller, V., Zhuang, Y., He, G., Li, H., Koto, F., Tang, L., Ranjan, N., Shen, Z., Ren,
X., Iriondo, R., Mu, C., Hu, Z., Schulze, M., Nakov, P., Baldwin, T., and Xing, E. P. (2023). Llm360:
Towards fully transparent open-source llms.
Llama Team (2024). The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Released July 23, 2024.
Lv, B., Tang, C., Zhang, Y., Liu, X., Luo, P., and Yu, Y. (2024). URG: A unified ranking and generation
method for ensembling language models. In Ku, L.-W., Martins, A., and Srikumar, V., editors, Findings
of the Association for Computational Linguistics: ACL 2024, pages 4421–4434, Bangkok, Thailand.
Association for Computational Linguistics.
13


--- Page 14 ---
Mahan, D., Phung, D., Rafailov, R., Blagden, C., Lile, N., Castricato, L., Fr¨anken, J., Finn, C., and Albalak,
A. (2024). Generative reward models. CoRR, abs/2410.12832.
Maxwell-Jia (2024). Aime 2024 dataset. https://huggingface.co/datasets/Maxwell-Jia/
AIME_2024. Accessed: 2025-09-07.
Minghao Yang, Chao Qu, X. T. (2024). Inf-orm-llama3.1-70b.
Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Cand`es,
E., and Hashimoto, T. (2025). s1: Simple test-time scaling.
NVIDIA (2025). Nvidia nemotron nano 2: An accurate and efficient hybrid mamba-transformer reasoning
model.
OpenAI (2023). Gpt-4 technical report. arXiv preprint arXiv:2303.08774.
OpenAI (2025). gpt-oss-120b & gpt-oss-20b model card.
OpenCompass
(2025).
Aime
2025
dataset.
https://huggingface.co/datasets/
opencompass/AIME2025. Accessed: 2025-09-07.
Qwen Team (2025). Qwen3 technical report.
Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. (2023). Direct preference
optimization: Your language model is secretly a reward model. In Oh, A., Naumann, T., Globerson, A.,
Saenko, K., Hardt, M., and Levine, S., editors, Advances in Neural Information Processing Systems 36:
Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,
USA, December 10 - 16, 2023.
Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R.
(2023). Gpqa: A graduate-level google-proof q&a benchmark.
Sheng, V. S., Provost, F., and Ipeirotis, P. G. (2008). Get another label? improving data quality and data
mining using multiple, noisy labelers. In Proceedings of the 14th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, KDD ’08, page 614–622, New York, NY, USA. Association
for Computing Machinery.
Si, C., Shi, W., Zhao, C., Zettlemoyer, L., and Boyd-Graber, J. L. (2023). Getting moRE out of mixture of
language model reasoning experts. In The 2023 Conference on Empirical Methods in Natural Language
Processing.
Snell, C. V., Lee, J., Xu, K., and Kumar, A. (2025). Scaling LLM test-time compute optimally can be more
effective than scaling parameters for reasoning. In The Thirteenth International Conference on Learning
Representations.
Son, G., Ko, H., Lee, H., Kim, Y., and Hong, S. (2024). Llm-as-a-judge and reward model: What they can
and cannot do.
Soto, V., Su´arez, A., and Mart´ınez-Mu˜noz, G. (2016). An urn model for majority voting in classification en-
sembles. In Proceedings of the 30th International Conference on Neural Information Processing Systems,
NIPS’16, page 4437–4445, Red Hook, NY, USA. Curran Associates Inc.
14


--- Page 15 ---
Takamatsu, S., Sato, I., and Nakagawa, H. (2012). Reducing wrong labels in distant supervision for relation
extraction. In Li, H., Lin, C.-Y., Osborne, M., Lee, G. G., and Park, J. C., editors, Proceedings of the
50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
721–729, Jeju Island, Korea. Association for Computational Linguistics.
Tekin, S. F., Ilhan, F., Huang, T., Hu, S., and Liu, L. (2024). LLM-TOPLA: Efficient LLM ensemble by
maximising diversity. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N., editors, Findings of the Association
for Computational Linguistics: EMNLP 2024, pages 11951–11966, Miami, Florida, USA. Association
for Computational Linguistics.
Uesato, J., Kushman, N., Kumar, R., Song, H. F., Siegel, N. Y., Wang, L., Creswell, A., Irving, G., and
Higgins, I. (2022). Solving math word problems with process- and outcome-based feedback. CoRR,
abs/2211.14275.
Varshney, N. and Baral, C. (2022). Model cascading: Towards jointly improving efficiency and accuracy of
nlp systems.
Wan, Z., Feng, X., Wen, M., McAleer, S. M., Wen, Y., Zhang, W., and Wang, J. (2024). Alphazero-like
tree-search can guide large language model decoding and training. In Forty-first International Conference
on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net.
Wang, B., Zheng, R., Chen, L., Liu, Y., Dou, S., Huang, C., Shen, W., Jin, S., Zhou, E., Shi, C., Gao, S.,
Xu, N., Zhou, Y., Fan, X., Xi, Z., Zhao, J., Wang, X., Ji, T., Yan, H., Shen, L., Chen, Z., Gui, T., Zhang,
Q., Qiu, X., Huang, X., Wu, Z., and Jiang, Y. (2024). Secrets of RLHF in large language models part II:
reward modeling. CoRR, abs/2401.06080.
Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., Chowdhery, A., and Zhou, D. (2023).
Self-consistency improves chain of thought reasoning in language models. In The Eleventh International
Conference on Learning Representations.
Wang, Z., Wang, Y., Wang, X., Xing, M., Gao, J., Xu, J., Liu, G., Jin, C., Wang, Z., Zhang, S., and Xie, H.
(2025). Test-time scaling with reflective generative model.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. (2022).
Chain-of-thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.
Whitehill, J., Wu, T.-f., Bergsma, J., Movellan, J., and Ruvolo, P. (2009). Whose vote should count more:
Optimal integration of labels from labelers of unknown expertise. In Bengio, Y., Schuurmans, D., Lafferty,
J., Williams, C., and Culotta, A., editors, Advances in Neural Information Processing Systems, volume 22.
Curran Associates, Inc.
Yan, Y., Shen, Y., Liu, Y., Jiang, J., Zhang, M., Shao, J., and Zhuang, Y. (2025). Inftythink: Breaking the
length limits of long-context reasoning in large language models.
Ye, Y., Huang, Z., Xiao, Y., Chern, E., Xia, S., and Liu, P. (2025). Limo: Less is more for reasoning.
Yue, M., Zhao, J., Zhang, M., Du, L., and Yao, Z. (2024). Large language model cascades with mixture of
thought representations for cost-efficient reasoning. In ICLR.
Zhao, W., Aggarwal, P., Saha, S., Celikyilmaz, A., Weston, J., and Kulikov, I. (2025a). The majority is not
always right: Rl training for solution aggregation.
Zhao, X., Kang, Z., Feng, A., Levine, S., and Song, D. (2025b). Learning to reason without external rewards.
15


--- Page 16 ---
A
Related work
This section describes the related work on aggregating multiple answers from different individuals and
LLMs.
A.1
Aggregation of multiple answers from individuals
Controlling N in majority voting
Ensembling multiple predictions is a widely used technique for im-
proving the accuracy of various machine learning tasks. One of the classic papers by Kolter and Maloof
(2007) considers online ensemble learning, where the system can dynamically add or remove experts based
on their performance. Regarding the optimal control of N, the idea closest to ours is the Urn model by
Soto et al. (2016), which calculates the Bayesian probability that the empirical majority matches the true
majority. However, their model samples without replacement, whereas LLM generation fits sampling with
replacement. Their method also requires candidate answers and is thus not directly applicable to our setting.
Motivated by ensembling methods for deep image classifiers, Inoue (2019) proposed an adaptive ensemble
prediction method that adaptively aggregates the outputs of multiple probabilistic classifiers.
Opinion aggregation in crowdsourcing
A relevant lines of works in pre-LLM era is the opinion aggre-
gation in crowdsourcing (Sheng et al., 2008; Li et al., 2017). One of the most popular methods in opinion
aggregation is the method by David and Skene (Dawid and Skene, 1979). They introduced a probabilistic
model that estimates the true labels of items by leveraging the agreement among multiple annotators. It
comprises a confusion matrix π, whose jk entry represents the probability such that each annotator’s label
is j when the true label is k. Such a confusion matrix is not directly applicable to our setting because we
cannot generally assume a fixed domain of answers in LLM generation. For example, in the AIME datasets,
building a confusion matrix of 1,000 rows (possible answers are integers from 0 to 999) is not very prac-
tical. The Dawid-Skene model also assumes that the most frequent answer is the correct one. Subsequent
works (Whitehill et al., 2009; Kajino et al., 2012; Takamatsu et al., 2012) addressed this issue by introduc-
ing a difficulty parameter for each problem. One of the largest difference between crowdsourcing and LLM
ensemble is that the former typically assumes a single answer from each annotator, whereas the latter can
generate multiple answers from the same LLM.
A.2
Aggregation of multiple answers from LLMs
Our method belong to a large umbrella of LLM Ensemble methods, where the forecaster uses multiple
LLMs for a better output. A comprehensive survey on this topic (Chen et al., 2025b) categorizes ensemble
LLM methods into several categories.7 Our method falls into the category of “ensemble after inference”,
where we aggregate the outputs of multiple LLMs after they have generated their responses.
Within this category, Chen et al. (2025b) classified methods into three sub-categories: (1) selection, (2)
selection-then-regeneration, and (3) cascade. The first directly selects the answer from generated outputs
(our setting). The second selects a subset of LLMs and then merges their outputs using another LLM or a
trained model. The third uses a cascade of LLMs, invoking a stronger model only when needed to save cost.
Methods in (1) and (2) typically assume a fixed number of generations per LLM and optimize aggregation.
In contrast, we primarily consider dynamically controlling the number of generations. Methods in (3) focus
on minimizing total cost of calling LLMs.
7Figure 2 therein.
16


--- Page 17 ---
(1) Selection
Li et al. (2024) proposed AgentForest that aggregates the predictions of multiple agents by
using similarity agreement. Guha et al. (2024) introduced Smoothie, a graphical-model based method to
choose the best LLM for each problem. Si et al. (2023) introduced Mixture of Reasoning Experts (MORE)
framework that adopts multiple prompting strategy to obtain a mixture of experts and aggregates them by
random forest classifier. Our methods belongs to this sub-category. Compared with these methods, our
method is a simple average while others may use more complex aggregation strategies. Note also that these
methods primarily consider a single generation per each LLM, whereas our paper primarily considers large
number of generations per each LLM. A recent paper by Zhao et al. (2025a) proposes an aggregation method
of multiple solutions by using reinforcement learning from verifiable rewards.
(2) Selection-then-Regeneration
Jiang et al. (2023b) introduced LLM-Blender, an ensemble LLM method
that comprises two modules: PAIRRANKER and GENFUSER. PairRanker chooses K among N LLMs,
and GenFuser merges the outputs. Tekin et al. (2024) introduced LLM-TOPLA, an LLM ensemble method
that maximizes the diversity of the answers. Based on the answer distribution of N LLMs, they choose K
subset of LLMs that maximizes the diversity, and then train an aggregator (like multi-layer perceptron) that
minimizes the cross-entropy loss. Lv et al. (2024) proposed an end-to-end method that integrates the subset
selection and regeneration. Most of these methods are based on the idea of using many LLMs (or same
LLM with different prompts) and single generation per each prompt, whereas our paper primarily considers
a relatively small subset of LLMs for each prompt, and large number of generations per each LLM.
(3) Cascading
Varshney and Baral (2022) is one of the earliest work that introduced cascading. Yue
et al. (2024) proposed an aggregation of weak and strong LLMs. In their model, if the weak LLM and
the cascade LLM disagree with the answer, then the strong LLM is invoked. The primal motivation in
cascading is to save the cost of calling strong LLMs, which is orthogonal to our goal of improving the
accuracy of maximizing the accuracy given large amount of computation.
Answer selection based on reward models and LLM-as-a-judge
A common approach to aggregate
multiple answers from LLMs is to use reward models or LLM-as-a-judge methods. Typically, reward models
are constructed on top of language models. These approaches can be broadly categorized into two groups:
those in which the reward model directly outputs a scalar value (Rafailov et al., 2023; Liu et al., 2024),
and those in which the reward model provides comparative judgments or rankings over multiple responses
(Mahan et al., 2024; Dong et al., 2024; Son et al., 2024; Guo et al., 2025; Chen et al., 2025a). The methods
of the latter category are referred to as generative reward models, reward reasoning models, or LLM-as-a-
judge. Compared to our approach, these methods incur additional computational cost due to the reliance on
reward models. Also, in our experiments, we did not observe particular advantage of using reward models
(see Table 2).
B
Proof of Theorem 1
Proof of Theorem 1. Let ˆpa(n) = Nj(n)/n be the empirical mean of answer j at round n. Hoeffding’s
inequality implies that
P[|ˆpa(n) −pa| ≥ϵ] ≤2 exp(−2nϵ2).
17


--- Page 18 ---
Let ∆= minj̸=1(p1 −pj) > 0 be the gap between the most frequent answer and the second most frequent
answer. Then it holds that
P


∞
\
n=N0
|ˆpj(n) −pj| ≥∆
2

≤
∞
X
n=N0
P

|ˆpj(n) −pj| ≥∆
2

(Union bound)
≤
∞
X
n=N0
2 exp

−n∆2
2

(Hoeffding’s inequality)
= 2e−N0∆2/2
1 −e−∆2/2 .
(14)
and by choosing N0 = N0(δ) sufficiently large, the right-hand side can be made no larger than δ/s. Union
bound over all s answers implies that, with probability at least 1 −δ, it holds that
ˆp1(n) −ˆpj(n) ≥p1 −pj −2 × ∆
2 ≥0, ∀j ̸= 1, ∀n ≥N0(δ).
Namely, at least with probability 1 −δ, the empirical most frequent answer is indeed the true majority
answer for all n ≥N0(δ), and thus, if stopping time is longer than N0(δ), the algorithm returns the true
majority answer. By choosing Nmax ≥N0(δ) and B sufficiently large8, the algorithm stops after N0(δ)
with probability 1, and thus, the algorithm returns the true majority answer with probability at least 1 −δ.
Since δ > 0 is arbitrary, the algorithm returns the true majority answer with probability arbitrarily close to
1. Proof of Theorem 1 is complete.
Remark 1. (Frequentist stopping criteria) While Dirichlet posterior naturally fits with our task, we may
consider frequentist stopping criteria based on the observed data. Advantages of the frequentist approach
include its closed formula as well as rigorous guarantee in view of a frequentist. A drawback is that its
configuration of the hyperparameter tends to be conservative: the confidence level that it requires is often
higher than what actually is, potentially leading to oversampling. To bound the error probability, it needs
to consider the correction due to adaptive sampling (Kaufmann and Koolen, 2021), as well as a multiple-
testing correction with respect to the size of answer set s(t). The latter seems particularly problematic, as
s(t) is unknown and potentially unbounded. For this reason, we do not see any existing work that adopts a
frequentist approach to testing adaptive majority voting. For example, existing methods on majority voting,
such as Soto et al. (2016), which we will elaborate in Section A.1, also adopt Bayesian approach. Therefore,
we do not pursue this direction in this paper.
C
List of LLMs and Problem sets
We tested the following LLMs. The model temperature is 0.6 unless otherwise specified. We follow the
model recommendation to set the temperature and other hyperparameters. The maximum model length is
min(X, maximum context length of LLM) −2500 tokens, where X = 100000 all but GPQA-DIAMOND,
whereas X = 50000 for GPQA-DIAMOND. The 2500 token margin is reserved for the prompt; we believe
that this does not matter to MATH500 and GPQA-DIAMOND at all, and to AIME2024/2025 very slightly.
• Phi-4-reasoning (Abdin et al., 2025) is a 14-billion-parameter (14B) reasoning-oriented model de-
veloped by Microsoft, released in April 2025. It builds on the Phi-4 base model using supervised
8This is because, the possible combination of answers with the first N0 samples is finite, and thus, the possible value9 of BF
that it can take until the first N0 sample is finite. If we set B larger than that the largest of such values, then the algorithm never
stops before the N0 samples.
18


--- Page 19 ---
fine-tuning on a dataset of chain-of-thought traces and reinforcement learning. We set temperature to
0.8.
• GPT-OSS-20B (OpenAI, 2025) is the smaller version of the two LLMs released in October 2025 by
OpenAI. This model has 21B parameters in total. We set the reasoning effort to be medium (default
setting).
• AM-Thinking-v1 (Ji et al., 2025) is a 32B dense model released in May 2025 by the a-m-team. It
is built upon the pre-trained Qwen 2.5-32B-Base, then enhanced through a specialized post-training
pipeline featuring Supervised Fine-Tuning (SFT) followed by reinforcement learning (RL).
• EXAONE-Deep-32B (LG AI Research, 2025) is a 32B model released in May 2025 by LG AI Re-
search as part of the EXAONE Deep series. Built with 64 Transformer layers, a 102K vocabulary, and
a 32K-token context window, it is designed to excel in reasoning-intensive tasks such as mathematics
and coding.
• Nemotron-Nano-9B (NVIDIA, 2025) is a 9-billion-parameter hybrid reasoning model by NVIDIA,
released in August 2025. It features a Mamba-2 + Transformer hybrid architecture, replacing most
attention layers with efficient Mamba-2 layers. It was pretrained from scratch (using a 12B base model
over 20 trillion tokens) and then compressed via distillation. Post-training includes SFT, GRPO, DPO,
and RLHF.
• MetaStone-S1-32B (Wang et al., 2025) is a 32B reflective generative reasoning model, released around
July 2025. It introduces a novel Reflective Generative Form, merging policy generation and process
reward modeling within a single shared backbone, enabled by a lightweight Self-supervised Process
Reward Model (SPRM).
• Qwen3, released in April 2025 by Alibaba Cloud (Qwen Team, 2025), is the third-generation open-
source large language model family featuring hybrid reasoning, long context support, agentic capa-
bilities, and multilingual fluency. We use three versions of Qwen3. Namely, Qwen3-4B, Qwen3-14B,
and Qwen3-30B-A3B-Thinking-2507.
• LIMO-v2 (Ye et al., 2025) is a 32B Qwen2.5-based reasoning model released in July 2025, fine-
tuned on ∼800 carefully curated samples to achieve top-tier math reasoning with remarkable data
efficiency—embodying the “Less-Is-More” principle.
We tested the following datasets:
• AIME2024 (Maxwell-Jia, 2024) consists of 30 problems that were used American Invitational Mathe-
matics Examination (AIME) held during January 31 and February 1, 2024. AIME2024 tests mathmat-
ical problem-solving skills in vast field of mathmatical topics. High-scoring high-school students are
invited to participate in the United States of America Mathematics Olympiad (USAMO). All answers
are integers between 1–999.
• AIME2025 (OpenCompass, 2025) consists of 30 problems that were used American Invitational
Mathematics Examination (AIME) held from February 10 to February 12, 2025. Its format is identical
to AIME2024.
• GPQA-DIAMOND (Graduate-Level Google-Proof Q&A Benchmark, Rein et al. 2023) is a set of
multiple-choice questions crafted by PhD-level experts in biology, physics, and chemistry. The Dia-
mond is a subset of 198 GPQA problems that distinguishes Ph.D. level experts from the others. The
answers are in multiple-choice format (A–D).
19


--- Page 20 ---
• MATH500 (Hendrycks et al., 2021) is a benchmark derived from the MATH dataset, which con-
tains challenging competition-level mathematics problems covering algebra, geometry, number the-
ory, probability, and other advanced topics. The MATH500 subset consists of 500 carefully selected
problems used in recent evaluation studies, and is designed to test mathematical problem-solving
skills beyond high-school level. All problems require generating detailed reasoning and solutions
rather than multiple-choice responses. The answer format varies, including numeric integers, frac-
tions, complex numbers, and vectors.
Among these datasets, AIME2024/2025 benefits for a long chain of thought (CoT) reasoning, as the
problems are challenging and require multi-step reasoning.10 GPQA-DIAMOND and MATH500 also re-
quire long CoT, but the benefit of it is less significant than AIME2024/2025. We did not include GSM8K
(Cobbe et al., 2021) because these problems are relatively easy and finishes with a short CoT for the tested
LLMs, and thus the benefit of ensemble was not significant.
10Regarding the scaling of performance as a function of CoT length, see, e.g., Figure 1 of Muennighoff et al. (2025) and Figure
7 of Yan et al. (2025).
20


--- Page 21 ---
LLM
AIME2024
AIME2025
GPQA-D
MATH500
Bo1
Bo∞
Bo1
Bo∞
Bo1
Bo∞
Bo1
Bo∞
AM-Thinking-v1
0.789
0.900
0.762
0.867
–
–
–
–
Datarus-R1-14B-preview
0.516
0.733
0.370
0.600
–
–
–
–
EXAONE-Deep-32B
0.715
0.867
0.627
0.767
0.661
0.692
0.945
0.962
GPT-OSS-20B
0.780
0.900
0.744
0.900
0.642
0.722
0.928
0.960
LIMO-v2
0.620
0.800
0.527
0.700
–
–
–
–
MetaStone-S1-32B
0.820
0.867
0.747
0.800
0.670
0.707
0.947
0.950
NVIDIA-Nemotron-Nano-9B-v2
0.716
0.867
0.600
0.733
0.584
0.626
0.938
0.956
Phi-4-reasoning
0.729
0.867
0.643
0.833
0.658
0.727
0.878
0.944
Qwen3-4B
0.735
0.800
0.655
0.733
–
–
–
–
Qwen3-14B
0.830
0.867
0.744
0.800
–
–
0.946
0.956
Qwen3-30B-A3B-Thinking-2507
0.905
0.933
0.858
0.900
0.720
0.732
0.954
0.960
Table 3: Summary performance per model across datasets. The scores are estimated from at least 80 gener-
ation for each model and dataset. GPQA-D is an abbreviation of GPQA-DIAMOND.
D
Bo1 and Best-of-∞performance of each model
We list Bo1 (averaged) and best-of-∞performance of each model in Table 3. We have used the same prompt
(Section D.1) for all models, which might be sub-optimal for some models. The evaluated performance also
depends on the answer parser. While we used the consistent and a reasonably flexible answer parser for all
models, we acknowledge some examples11 where the parsing is imperfect. We have not specified any tool
call option. Also note that GPT-OSS-20B’s reasoning mode is set to medium (default setting), which is the
second best setting. Finally, we clarify our goal is not to argue superiority of some models over the others,
but to give some idea on the performance of each model that we use for the verification of our methods of
adaptive sampling (Algorithm 1).
D.1
Prompts for answer generation
We send the following request to a LLM that we launched as a vllm process:
{”role”: ”user”, ”content”: prompt}
where the examples of the prompt are given below: The first prompt is from AIME2024, and the second
prompt is from GPQA-DIAMOND.
Let $x,y$ and $z$ be positive real numbers that satisfy the following
system of equations:
\[\log_2\left({x \over yz}\right) = {1 \over 2}\]
\[\log_2\left({y \over xz}\right) = {1 \over 3}\]
\[\log_2\left({z \over xy}\right) = {1 \over 4}\]
Then the value of $\left|\log_2(xˆ4yˆ3zˆ2)\right|$ is $\tfrac{m}{n}$
where $m$ and $n$ are relatively prime positive integers. Find $m+n$.
Please reason step by step, and put your final answer within \boxed{}.
11In particular, MATH500 where the answer format varies.
21


--- Page 22 ---
Among the following exoplanets, which one has the highest density?
a) An Earth-mass and Earth-radius planet.
b) A planet with 2 Earth masses and a density of approximately 5.5 g/cm
ˆ3.
c) A planet with the same composition as Earth but 5 times more massive
than Earth.
d) A planet with the same composition as Earth but half the mass of Earth
.
A. d
B. a
C. b
D. c
Please reason step by step, and put your final answer as the letter
choice (A), (B), (C), etc. within \boxed{}.
For NVIDIA Nemotron-Nano-9B, we prepend the recommended system message “/think”.
D.2
Prompts for LLM-as-a-judge
The following illustrates a prompt used to instruct an LLM-as-a-judge to select the best answer among a set
of candidates. In this prompt, last part 1, last part 2, ... denote the final 5000 characters of each
answer preceding the </think> tag.
Please evaluate the following 5 answer excerpts for this mathematical
problem and determine which answer you think is the most correct.
Problem:
Let $x,y$ and $z$ be positive real numbers that satisfy the following
system of equations:
\[\log_2\left({x \over yz}\right) = {1 \over 2}\]
\[\log_2\left({y \over xz}\right) = {1 \over 3}\]
\[\log_2\left({z \over xy}\right) = {1 \over 4}\]
Then the value of $\left|\log_2(xˆ4yˆ3zˆ2)\right|$ is $\tfrac{m}{n}$
where $m$ and $n$ are relatively prime positive integers. Find $m+n$.
Answer 1 (Last 5000 chars before </think>):
{last_part_1}
Answer 2 (Last 5000 chars before </think>):
{last_part_2}
Answer 3 (Last 5000 chars before </think>):
{last_part_3}
Answer 4 (Last 5000 chars before </think>):
{last_part_4}
Answer 5 (Last 5000 chars before </think>):
22


--- Page 23 ---
{last_part_5}
Among the above 5 answer excerpts (showing the last parts before </think>
tag), which answer do you think is the most correct, logical, and
complete?
Please provide detailed reasoning for your judgment, and then output the
number of the answer you think is correct (1, 2, 3, 4, 5) enclosed in
\boxed{}.
Example: \boxed{1}
Judgment:
D.3
Source code
Our source code is available at https://github.com/jkomiyama/BoInf-code-publish.
E
Complementarity in LLM Ensembles for AIME 2025
In the AIME2025 dataset, we explored the combination of Phi-4-reasoning (Table 4) and GPT-OSS-20B
(Table 5) to enhance performance on complex reasoning tasks. By leveraging the strengths of both models,
we aimed to achieve better accuracy and robustness in our predictions. In this case, Phi-4-reasoning can
solve Problem 30 that GPT-OSS-20B cannot solve, and can complement the performance. As a result, its
LLM ensemble achieved 0.933 best-of-∞accuracy, which is higher than the individual accuracies of Phi-
4-reasoning (0.733) and GPT-OSS-20B (0.900). This demonstrates the effectiveness of combining different
models to improve overall performance on challenging tasks.
23


--- Page 24 ---
Problem No.
Total answers
Correct answers
Accuracy
Gold answer
Majority answer
1
160
159
0.994
70
70
2
160
112
0.700
588
588
3
160
154
0.963
16
16
4
160
150
0.938
117
117
5
160
146
0.912
279
279
6
160
158
0.988
504
504
7
160
96
0.600
821
821
8
160
147
0.919
77
77
9
160
134
0.838
62
62
10
160
58
0.362
81
81
11
160
120
0.750
259
259
12
160
137
0.856
510
510
13
160
5
0.031
204
487/3
14
160
5
0.031
60
63
15
160
0
0.000
735
147
16
160
158
0.988
468
468
17
160
157
0.981
49
49
18
160
87
0.544
82
82
19
160
154
0.963
106
106
20
160
114
0.713
336
336
21
160
143
0.894
293
293
22
160
45
0.281
237
60671
23
160
66
0.412
610
610
24
160
77
0.481
149
149
25
160
132
0.825
907
907
26
160
111
0.694
113
113
27
160
136
0.850
19
19
28
160
1
0.006
248
625
29
160
75
0.469
104
104
30
160
48
0.300
240
240
total
4800
3085
0.643
0.833
Table 4: Basic performance for each problem. The final line at column “accuracy” indicates Bo1 perfor-
mance, and the final line at “majority answer” indicates best-of-∞performance. LLM=Phi-4-reasoning,
Dataset=AIME2025.
24


--- Page 25 ---
Problem No.
Total answers
Correct answers
Accuracy
Gold answer
Majority answer
1
85
85
1.000
70
70
2
85
76
0.894
588
588
3
85
85
1.000
16
16
4
85
83
0.976
117
117
5
85
81
0.953
279
279
6
85
85
1.000
504
504
7
85
52
0.612
821
821
8
85
80
0.941
77
77
9
85
75
0.882
62
62
10
85
53
0.624
81
81
11
85
61
0.718
259
259
12
85
56
0.659
510
510
13
85
17
0.200
204
204
14
85
3
0.035
60
74
15
85
0
0.000
735
147
16
85
81
0.953
468
468
17
85
85
1.000
49
49
18
85
62
0.729
82
82
19
85
84
0.988
106
106
20
85
79
0.929
336
336
21
85
72
0.847
293
293
22
85
85
1.000
237
237
23
85
45
0.529
610
610
24
85
58
0.682
149
149
25
85
81
0.953
907
907
26
85
72
0.847
113
113
27
85
81
0.953
19
19
28
85
36
0.424
248
248
29
85
71
0.835
104
104
30
85
14
0.165
240
188
total
2550
1898
0.744
0.900
Table 5: Basic performance for each problem. The final line at column “accuracy” indicates Bo1 perfor-
mance, and the final line at “majority answer” indicates best-of-∞(limit) performance. LLM=GPT-OSS-
20B, Dataset=AIME2025.
25


--- Page 26 ---
F
Additional Experiments
To verify the robustness of our findings, we conducted similar experiments on other LLMs and datasets.
The results are consistent with the main experiments in the paper, confirming the robustness of our proposed
methods across different settings. As is the main paper, all error bars are standard two-sigma confidence
intervals.
F.1
Experimental Set 1: Effectiveness of adaptive sampling
In the following pages, we present the performance comparison between our proposed adaptive algorithm
(Algorithm 1) and the fixed-sample BoN across various LLMs and datasets (Figures 8–11). The results
consistently demonstrate that our adaptive approach outperforms the fixed-sample-size method given the
same number of generation (= samples) or the same token budget. This is because our algorithm is adaptive;
for easy problems where the model always outputs the same answer, it uses fewer samples, while for hard
problems where the model’s answers vary, it uses more samples. This adaptivity leads to better overall
performance compared to a fixed-sample-size approach.
26


--- Page 27 ---
(a) AIME2025
(b) AIME2024
(c) GPQA-Diamond
(d) MATH500
Figure 7: Cost-analysis of our proposed method and fixed BoN for GPT-OSS-20B. The error bars are
standard two-sigma confidence intervals. Green dashed line indicates the best-of-∞performance.
27


--- Page 28 ---
(a) AIME2025
(b) AIME2024
(c) GPQA-Diamond
(d) MATH500
Figure 8: Cost-analysis of our proposed method and fixed BoN for Phi-4-reasoning. The error bars are
standard two-sigma confidence intervals. Green dashed line indicates the best-of-∞performance.
28


--- Page 29 ---
(a) AIME2025
(b) AIME2024
(c) GPQA-Diamond
(d) MATH500
Figure 9: Cost-analysis of our proposed method and fixed BoN for NVIDIA-Nemotron-Nano-9B-v2. The
error bars are standard two-sigma confidence intervals. Green dashed line indicates the best-of-∞perfor-
mance.
29


--- Page 30 ---
(a) AIME2025
(b) AIME2024
(c) GPQA-Diamond
(d) MATH500
Figure 10: Cost-analysis of our proposed method and fixed BoN for Qwen3-30B-A3B-Thinking-2507.
The error bars are standard two-sigma confidence intervals. Green dashed line indicates the best-of-∞
performance.
30


--- Page 31 ---
(a) AIME2025
(b) AIME2024
(c) GPQA-Diamond
(d) MATH500
Figure 11: Cost-analysis of our proposed method and fixed BoN for EXAONE-Deep-32B. The error bars
are standard two-sigma confidence intervals. Green dashed line indicates the best-of-∞performance.
31


--- Page 32 ---
F.2
Experimental Set 2: Advantage of LLM ensemble over single LLM
Figure 12 demonstrates several more examples where the ensemble of LLMs outperforms the best single
LLM. The weights are optimized by the MILP introduced in Section 3. We used Algorithm 1 to adaptively
select and ask LLM for the answers.
(a) Performance of a two-LLM ensemble. We used GPT-OSS-20B and Phi-4-reasoning on AIME2025.
We tested with weight w = (0.7, 0.3). The best-of-∞performance of GPT-OSS-20B is 0.900 (90.0%),
whereas the ensemble’s best-of-∞performance is 0.933 (93.3%).
(b) Performance of two-LLM ensemble. We used LIMO-v2 and Datarus-R1-14B on AIME2024. The
weight was optimized to w = (0.4316, 0.5684).
(c) Performance of four-LLM ensemble (MetaStone-S1-32B, Phi-4-reasoning,
Qwen3-30B-A3B-
Thinking-2507,
and GPT-OSS-20B) on MATH500.
The weight was
optimized to w
=
(0.0193, 0.0411, 0.3771, 0.5625).
Figure 12: Performance of LLM ensembles compared with single-LLM performance. We used Algorithm
1 choosing the LLM. Blue dashed line indicates the best-of-∞performance of the LLM ensemble.
32


--- Page 33 ---
F.3
Experimental Set 3: Learning a good weight
Figure 13 shows several additional examples of sample efficiency of learning the optimal weights in LLM
ensembles. Dashed lines are the best-of-∞performance of the individual LLMs. One can see that, with a
small number of gold answers, the learned weights can outperform the best single LLM.
(a) The mixture of LIMO-v2 and Datarus-R1-14B on AIME2024. Note that the best-of-∞performance of the two
base LLMs is exactly the same and thus overlaps in the figure.
(b) The mixture of Phi-4-reasoning, Qwen3-30B-A3B-Thinking-2507, and GPT-OSS-20B on GPQA-Diamond.
(c) The mixture of seven LLMS on MATH500.
Figure 13: The training of weights in an LLM ensemble. We show the number of samples to determine the
weight (x-axis) versus the best-of-∞performance (y-axis). The x-axis indicates the number of problems
used to learn the weight and the y-axis indicates the best-of-∞performance.
33


--- Page 34 ---
F.4
Experimental Set 4: Transfer learning of the optimal weight
We do not have additional experiments for this set of experiments.
F.5
Experimental Set 5: Comparison with other answer-selection methods
This section reports our comparison of majority voting with other aggregation methods. This appendix
section complements Table 2 of main paper by providing additional experimental results on AIME2025 and
other LLMs, as well as more details on the compared methods. The results are shown in Table 2. The
compared methods are as follows:
• Omniscient is the hypothestical selection method that can always select the correct answer if it is
included in the candidates, which is infeasible unless we know the gold answer. By definition, this is
the best possible performance of any selection method.
• Majority voting is the method that selects the most frequent answer among the candidates. Ties are
broken randomly.
• LLM-as-a-judge is the answer selection method that uses the target LLM itself to select the best
answer among the candidates. Since the concatination of the all answers can exceed the context length,
we extracted the last 5,000 characters before the </think> tag of the answers for each answer.12 To
avoid uninterpretable answer, we ask the LLM twice, which slightly increased the accuracy. There are
two variants: (tournament) compares the answers pairwise and selects the best one, and (set) compares
all answers at once and selects the best one.
• INF-ORM-Llama3.1-70 is one of the state-of-the-art reward model (Minghao Yang, 2024), which
marked the 9th in the RewardBench leaderboard as of September 8 2025.
• Skywork-Reward-V2-Llama-3.1-8B and Skywork-Reward-V2-Qwen3-8B are two of the state-of-the-
art reward model (Liu et al., 2024), which marked the 1st and the 6th in the RewardBench leaderboard
as of September 8 2025.
• Self-certainty is the method that selects the answer with the highest self-certainty score (Zhao et al.,
2025b), which measures intrinsic confidence by how the likelihood differs from the uniform distri-
bution per token. Note that we used the sequence average of self-certainty. Very recently, (Fu et al.,
2025) introduced a version of self-certainty that weights more on the latter part of the sequence, which
we have not tested and may improve the performance.
• Random is the model that randomly selects one of the candidates, whose performance should be close
to the accuracy of a Bo1.
We use the same set of answers for comparing these selection methods, which reduces the variance due to
the randomness in answer generation. Table 6, Table 7, and Table 8 show the comparison of these methods
on GPT-OSS-20B, Phi-4-reasoning, and Qwen3-30B-A3B-Thinking-2507, respectively. The results are
consistent with the main experiments in the paper. All results are Bo5 settings.
12For an answer without </think> tag, we used the final 5,000 characters. We also tested an alternative method that asks LLM
to summarize its own answer before the comparison, which, in our preliminary analysis, did not outperform the proposed method.
34


--- Page 35 ---
Method
AIME2024
GPQA-Diamond
MATH500
Omniscient
91.25 ± 1.03
85.98 ± 1.19
95.56 ± 0.23
Majority voting
88.12 ± 1.49
70.07 ± 2.02
95.31 ± 0.17
LLM-as-a-judge (set)
85.42 ± 1.48
69.14 ± 1.60
94.31 ± 0.28
LLM-as-a-judge (tournament)
–
70.22 ± 1.96
–
INF-ORM-Llama3.1-70B
85.42 ± 2.18
68.38 ± 1.84
94.21 ± 0.29
Skywork-Reward-V2-Llama-3.1-8B
85.42 ± 2.10
68.13 ± 1.95
–
Skywork-Reward-V2-Qwen3-8B
–
68.42 ± 1.93
–
Self-certainty
81.67 ± 2.98
67.65 ± 1.38
93.50 ± 0.47
Random (≈Bo1)
79.17 ± 2.89
67.65 ± 1.38
93.91 ± 0.40
Table 6: The accuracy of several selection methods on the best-of-five (Bo5) setting across three datasets
(AIME2024, MATH500, GPQA-Diamond). Answers are generated by GPT-OSS-20B. The scores are aver-
aged over 16 trials and we report the two-sigma confidence intervals.
Method
AIME2025
AIME2024
Omniscient
85.00 ± 1.72
85.21 ± 1.21
Majority voting
76.67 ± 2.58
80.00 ± 1.72
LLM-as-a-judge (set)
72.92 ± 3.10
80.42 ± 1.81
INF-ORM-Llama3.1-70B
70.42 ± 2.78
78.54 ± 2.51
Skywork-Reward-V2-Qwen3-8B
70.62 ± 2.87
77.29 ± 2.60
Self-certainty
63.12 ± 3.36
73.54 ± 2.31
Random (≈Bo1)
63.96 ± 2.45
73.54 ± 2.31
Table 7: The accuracy of several selection methods on the best-of-five (Bo5) setting on the AIME2025 and
AIME2024 datasets. Answers are generated by Phi-4-reasoning. Scores are averaged over 16 trials and we
report the two-sigma confidence intervals.
Method
AIME2025
AIME2024
Omniscient
92.71 ± 1.09
93.54 ± 0.74
Majority voting
88.75 ± 1.20
92.92 ± 0.57
LLM-as-a-judge (set)
88.13 ± 1.49
92.29 ± 0.80
LLM-as-a-judge (tournament)
87.50 ± 1.29
91.25 ± 1.48
INF-ORM-Llama3.1-70B
89.38 ± 1.09
92.29 ± 1.00
Skywork-Reward-V2-Qwen3-8B
89.38 ± 1.09
92.71 ± 0.67
Self-certainty
87.50 ± 2.06
91.25 ± 1.20
Random (≈Bo1)
86.04 ± 2.04
90.00 ± 1.36
Table 8: The accuracy of several selection methods on the best-of-five (Bo5) setting on the AIME2025 and
AIME2024 datasets. Answers are generated by Qwen3-30B-A3B-Thinking-2507. Scores are averaged over
16 trials and we report the two-sigma confidence intervals.
35
