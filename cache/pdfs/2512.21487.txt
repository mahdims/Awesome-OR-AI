--- Page 1 ---
Efficient MoE Inference with Fine-Grained Scheduling of
Disaggregated Expert Parallelism
Xinglin Pan1, Shaohuai Shi2, Wenxiang Lin2, Yuxin Wang3, Zhenheng Tang4, Wei Wang4,
Xiaowen Chu1,4
1The Hong Kong University of Science and Technology (Guangzhou), China
2Harbin Institute of Technology, Shenzhen, China
3Hong Kong Baptist University, Hong Kong SAR
4The Hong Kong University of Science and Technology, Hong Kong SAR
Abstract
The mixture-of-experts (MoE) architecture is commonly employed
in contemporary large language models (LLMs) due to its advantage
of scaling model size with a sublinear increase in computational
demand. Nevertheless, the inference of MoE models demands sub-
stantial memory, making it memory-intensive in attention layers
due to the necessity of accessing key-value (KV) caches and in
expert layers, utilizing only a limited number of experts. Recent
studies attempt to utilize disaggregated expert parallelism (DEP)
to distribute attention and experts to two dedicated GPU groups,
the attention group (AG) and the expert group (EG), to improve
inference efficiency. However, the existing DEP has limited support
for modern MoE models with shared experts, and it under-explores
task scheduling in both GPU groups, which have complex commu-
nication and computation tasks, leading to suboptimal inference
performance. To address these issues, we propose FinDEP, a fine-
grained task scheduling algorithm for DEP with maximal task over-
lap to improve the inference throughput of MoE models. FinDEP
integrates our three proposed key innovations: 1) partitioning in-
tensive computation and communication tasks to multiple smaller
tasks in both AG and EG to enable fine-grained task pipelining w/
or w/o shared experts, 2) formulating an optimization problem to
the fine-grained task scheduling that should support different task
partition granularity and ordering, and 3) developing an efficient so-
lution to the optimization problem which contains a huge solution
space to derive the near-optimal task schedule of DEP. Experiments
are conducted on four types of GPU systems with two representa-
tive MoE backbones, DeepSeek-V2 and Qwen3-MoE. Experimental
results show that FinDEP improves inference throughput by up
to 1.61Ã— over state-of-the-art methods. Notably, on a 32-GPU sys-
tem, FinDEP still achieves a significant speedup of up to 1.24Ã—,
demonstrating its efficiency at large scales.
CCS Concepts
â€¢ Computing methodologies â†’Distributed artificial intelli-
gence.
Keywords
Mixture-of-Expert, disaggregated expert parallelism, ping-pong
parallelism, throughput
1
Introduction
Large language models (LLMs) are scaling rapidly, and so are
their computational costs. For instance, models like Falcon [1]
Gate
MLA
or
MHA
Add
&
Norm
Add
&
Norm
MoE Expert
Shared
Expert
Expert
Input
Seq.
......
Figure 1: A typical structure of an MoE model. MLA refers to
Multi-Head Latent Attention [5], while MHA denotes Multi-
Head Attention [30]. The â€œSharedâ€ block indicates one shared
expert or several shared experts, which may be optional de-
pending on the MoE configuration.
with 180 billion parameters and Llama-3.1 [27] with 405 billion
parameters exemplify this trend. Mixture-of-Experts (MoE) archi-
tectures [12, 15, 24] address this challenge by activating only a sub-
set of the modelâ€™s expert components for each input. This makes
it possible to build much larger models without making training
or inference more expensive. Recent MoE-based LLMs, such as
DeepSeek-V3 [6] and Qwen3-MoE [28], show that this design can
create highly capable models that are still fast and cheap to use. As
a result, MoE has become a key technique for building future LLMs
in a way that balances power and efficiency.
Despite the advantages, running inference on large MoE models
remains challenging [2, 6, 13, 34, 37] due to its extensive memory
requirement to hold all experts in the MoE layers and key-value
(KV) caches in the attention layers. As a result, distributing the MoE
model across multiple GPUs has been a common practice [6, 15,
36] for efficient inference through expert parallelism (EP), which
assigns experts across GPUs.
Recent research [17, 29, 36] suggests distributing attention layers
and expert layers onto distinct GPUs through disaggregated expert
parallelism (DEP)1, due to the different computational and memory
access patterns of attention layers and expert layers. This approach
enables the modules to scale independently while optimizing the
use of various hardware capabilities. In DEP, a multi-GPU system is
divided into two groups: the attention group (AG), responsible for
storing all attention layers, and the expert group (EG), which holds
all non-shared experts. It is important to mention that in certain
MoE models like DeepSeek-V3 [6], shared experts within the MoE
layer are often placed in the AG as they need to be processed by all
input tokens. The dependency between attention and expert layers
is substantial, as each attention layerâ€™s output serves as the input
1Also referred to as Attention-FFN Disaggregation (AFD); we adopt the term DEP
following [36].
arXiv:2512.21487v1  [cs.DC]  25 Dec 2025


--- Page 2 ---
Xinglin Pan1, Shaohuai Shi2, Wenxiang Lin2, Yuxin Wang3, Zhenheng Tang4, Wei Wang4, Xiaowen Chu1,4
for the subsequent expert layer, which then outputs to another
attention layer as shown in Fig. 1. Consequently, DEP necessitates
bidirectional communication: from AG to EG (A2E) and the reverse
(E2A). Data dependencies and communication overhead easily lead
to the GPU computational resources idle, thereby limiting inference
efficiency.
Existing optimizations try to alleviate the GPU idle duration of
DEP via 1) overlapping computation and communication tasks to
reduce the communication time with the ping-pong pipeline (PP-
Pipe) algorithm proposed in MegaScale-Infer [36] or 2) offloading
communication tasks to CPU resources to enable overlaps between
CPU communications and GPU computations in StepMesh [29].
These techniques enable only coarse-level task scheduling by di-
viding a mini-batch into several micro-batches. As a result, different
tasks from these micro-batches can be executed in a pipeline fashion,
but this does not sufficiently hide A2E/E2A communications, leading
to suboptimal inference efficiency. Moreover, certain cutting-edge
MoE models such as DeepSeek series [4â€“6] introduce shared ex-
perts within the MoE layer, which are required to compute for every
input token, similar to the attention layer, leading to increased GPU
idle time.
In this paper, we propose FinDEP, a fine-grained task scheduling
framework for MoE inference with DEP to address the above two
efficiency problems by three key innovations. (1) We partition time-
consuming tasks including computations in EG, communications
in A2E and E2A, and computations in AG into smaller tasks by
splitting each taskâ€™s input tensor into several segments (denoted as
ğ‘Ÿ). This partitioning of the tensor creates ğ‘Ÿsmaller tasks per origi-
nal task, allowing for dynamic scheduling aimed at improving the
throughput for MoE models, regardless of whether they have shared
experts. (2) Intuitively, increasing ğ‘Ÿallows greater parallelization
for enhanced overlapping. However, this also increases the launch
overheads associated with executing tasks, such as kernel dispatch
on GPUs and communication startup costs. Thus, a balance must
be built between the advantages of overlapping and the execution
overheads. Consequently, we construct performance models for
computation tasks in AG and EG and their A2E/E2A communication
tasks. Using these models, we establish an optimization problem to
characterize the DEP inference time with fine-grained task sched-
uling, including task ordering and tensor partition granularity. (3)
We develop an efficient algorithm to find the near-optimal solution
to the formulated optimization problem with a polynomial time
complexity, thus avoiding the very time-consuming brute-force
search on the huge solution space.
We conduct extensive experiments on four GPU systems with
two representative MoE model backbones, DeepSeek-V2 (with
shared experts) and Qwen3-MoE (without shared experts). Exper-
imental results show that our FinDEP achieves speedups of upto
1.61Ã— over the best-configured PPPipe algorithm in MegaScale-Infer.
Furthermore, on the 32-GPU system, FinDEP consistently provides
a speedup of up to 1.24Ã—. Beyond peak throughput, we also confirm
the computational efficiency of our fine-grained task scheduling
solver. Our solver is highly efficient, taking less than one second to
compute the near-optimal configuration. This minimal overhead
enables real-time adaptation to dynamic workloads, which is crucial
for maximizing throughput in online serving environments with
dynamically varying sequence lengths and batch sizes.
Table 1: Notations.
Name
Description
ğ‘ƒ
# of GPUs in the cluster.
ğ‘ğ‘”
Size of attention group (AG).
ğ‘’ğ‘”
Size of expert group (EG).
ğ‘šğ‘
# of samples per micro-batch per GPU in AG.
ğ‘šğ‘’
# of tokens per micro-batch per expert.
ğ‘†
Sequence length of each sample.
ğ¸
Total number of global experts.
ğ‘‡
Total number of layers.
ğ‘€
Embedding size for each token.
ğ»
Hidden size of the feed-forward layer within experts.
ğ‘¡ğ‘œğ‘ğ‘˜
# of experts activated per token.
ğ‘Ÿ1
Pipeline degree of the AG.
ğ‘Ÿ2
Fine-grained pipeline degree of the EG.
MHA
AG: Attention Group
Add
&
Norm
Shared
Add
&
Norm
(Next Layer)
Gate
EG: Expert Group
A2E
E2A
E0 E1
E3
EG0
EG1
E2
Figure 2: An illustration of DEP. GPUs are partitioned into
two groups: AG and EG. AG handles the attention and shared
expert computation, while EG handles experts computation.
2
Background and Motivations
This section provides an overview of background concepts, followed
by a summary of the motivations for this research. For clarity, Ta-
ble 1 offers a summary of the frequently used notations throughout
the paper.
2.1
MoE Layer
MoE models replace each dense feed-forward network (FFN) in
transformers with sparsely activated FFNs (or experts) by the MoE
layer, as shown in Fig. 1. Each token is routed to ğ‘˜experts via a
gating function: the gate computes routing scores over all experts,
applies a softmax function, and selects the top-ğ‘˜experts for each
token [31]. The input is partitioned accordingly, with each expert
processing only its assigned tokens. Some implementations include
a shared expert that processes all tokens [5, 6, 22], while it is optional
in some implementations like Qwen3-MoE [28]. The layer output of
the MoE layer is the aggregated contributions from selected experts
(and the shared expert if present).


--- Page 3 ---
FinDEP
2.2
Disaggregated Expert Parallelism and
Ping-pong Pipeline
Disaggregated Expert Parallelism. Disaggregated Expert Paral-
lelism (DEP) is a novel parallelization strategy specifically tailored
for the high-throughput, low-latency inference of large MoE-based
models. Its foundational principle is the physical separation and
independent allocation of core model components across distinct
GPU groups. This partitioning divides the available hardware into
two dedicated functional units: the Attention Group (AG) and the
Expert Group (EG) as shown in Fig. 2. The AG is dedicated to stor-
ing and processing the standard components of the Transformer
block, including the Self-Attention layers and the Shared Expert
(if present), (i.e., components that are densely activated across all
tokens). Conversely, the EG houses the entire set of sparse MoE
experts, distributed across its constituent devices. This structural
disaggregation enables to independently scale the computational
resources for each module based on specific memory and com-
putational bottlenecks, a key advantage over monolithic parallel
approaches.
A key architectural benefit of DEP is the elimination of intra-
group communication overhead. Within the AG, parameters are
fully replicated, allowing each device to operate independently
without costly collective operations (e.g., All-Reduce). Similarly,
within the EG, the inherent sparsity of the token-to-expert routing
ensures that an activated expertâ€™s computation is confined to a
single GPU. This confinement prevents the necessity of communi-
cation between expert devices.
The necessary collective communication occurs solely between
the two groups through two defined communication phases: 1)
Attention-to-Expert (A2E), where tokens processed by the AG are
routed to the appropriate expert(s) in the EG, and 2) Expert-to-
Attention (E2A), where the expert outputs are gathered and re-
turned to the AG for subsequent layers. This disaggregation enables
independent scaling of computational resources for each module
and the development of tailored parallel strategies [17, 36]. The
sequential execution of MoE inference with DEP is illustrated in
Fig. 3(a). Due to the data dependency between modules, the EG
remains idle until the AG completes its forward pass and dispatches
tokens via A2E communication. Conversely, the AG must wait idly
for the EG to finish processing and return results via E2A before
it can proceed to the next layer. While this disaggregation offers
significant flexibility, this sequential handoff leads to significant
device idle time in a naive implementation, as computational re-
sources in one group are consistently underutilized while waiting
for the other group to fulfill its part of the pipeline.
Ping-Pong Pipeline Parallelism. To rigorously address the
device idle time inherent in the sequential dependency between
the Attention Group and the Expert Group, the Ping-Pong Pipeline
Parallelism (PPPipe) algorithm [36] serves as a specialized micro-
batch scheduling strategy that enables the concurrent utilization of
both AG and EG as shown in Fig. 3b. Specifically, PPPipe divides
the input mini-batch into ğ‘Ÿ1 micro-batches (e.g., ğ‘Ÿ1 = 2 in Fig. 3b) to
allow GPUs in EG to begin computations without waiting for the full
output of the mini-batch. Thus, in PPPipe, AG and EG computation
tasks can be executed in parallel, and the communication tasks can
â†“
AG
A2E
EG
E2A
â†‘
Idle
(a) Naive DEP without pipelining
â†‘
â†“
AG
A2E
EG
E2A
â†‘
â†“
â†‘
â†“
â†‘
â†“
(b) PPPipe with ğ‘Ÿ1 = 2 (shared expert is a part of attention).
â†‘
â†“
AG
A2E
EG
E2A
â†‘
â†“
â†‘
â†“
â†‘
â†“
(c) FinDEP with ğ‘Ÿ1 = 2 and ğ‘Ÿ2 = 1.
â†‘
â†“
AG
A2E
EG
E2A
â†“
â†“
â†‘
â†“
â†‘
â†‘
â†‘
â†“
â†“
â†“
â†‘
â†“
â†‘
â†‘
(d) FinDEP with ğ‘Ÿ1 = 2 and ğ‘Ÿ2 = 2.
Figure 3: Timeline of naive DEP, PPPipe, and our FinDEP.
also be overlapped with the computation tasks, thus improving the
inference throughput.
2.3
Motivations
While PPPipe in MegaScale-Infer [36] allows AG and EG tasks to
be pipelined to reduce the GPU idle time, it is still suboptimal due
to the following limitations.
Computation tasks of the shared expert are not well scheduled. In
PPPipe [36], it assumes that there is no shared expert in AG, which
does not support recent MoE models like DeepSeek-V3. Built atop
PPPipe, one can support including the shared expert by regarding
it as a part of attention, since both attention and the shared expert
should process all input tokens. As shown in Fig. 3b, A2E can
only begin after the completion of the shared expert computation.
However, the computation of experts in AE has no data dependency
with the shared expert; thus, they can also overlap. This means the
computation tasks of the shared expert should be well-scheduled
to achieve better efficiency.
Micro-batch level pipelining is insufficient to overlap tasks fully.
Existing DEP implementations, including PPPipe, overlook poten-
tial performance gains from overlapping communication and com-
putation between attention and expert modules. While existing
solutions focus on overlapping attention and expert computations,
they underestimate additional benefits from overlapping A2E or
E2A communication with expert computation. This overlap allows
the GPU-resident expert module to begin computation earlier, po-
tentially improving utilization and throughput [25]. We show an ex-
ample in Fig. 3d, where dividing the expert into two micro-batches
reduces end-to-end execution time. However, introducing pipelin-
ing can also incur kernel launch overhead, which in some cases
may increase rather than reduce expert computation time, thereby


--- Page 4 ---
Xinglin Pan1, Shaohuai Shi2, Wenxiang Lin2, Yuxin Wang3, Zhenheng Tang4, Wei Wang4, Xiaowen Chu1,4
worsening bottlenecks. To address this trade-off, a modern and
adaptive pipelining degree is required to balance early computation
benefits and kernel launch costs. Balancing overlap benefits and
launch costs is crucial for further exploiting throughput potential.
Huge search space to find an optimal schedule. The integration
of shared experts and fine-grained pipeline settings significantly
expands the search space for optimal configurations. Consequently,
this expansion stems from numerous interdependent design choices,
including pipeline degrees, microbatch sizes, and task orders. Simi-
larly, decisions regarding the fine-grained degree and micro-size
of experts, alongside the configurations of AG and EG, further
compound this complexity. Due to such entanglement, brute-force
enumeration becomes impractical. Therefore, an adaptive and effi-
cient algorithm is necessary to explore the design space to find the
optimal solution efficiently.
In this paper, we aim to address the above three issues by propos-
ing FinDEP to partition tensors for fine-grained task scheduling
with the support of shared experts. Thus, we split the attention
input for each GPU along the batch dimension to enable a micro-
batch level pipeline. The number of pipelines is denoted by ğ‘Ÿ1, and
the micro-batch size per GPU is denoted by ğ‘šğ‘. Since there are no
data dependencies, the shared expert and A2E of each micro-batch
can run in parallel, as shown in Fig. 3c. Other task orders are dis-
cussed in the next section. Unlike the attention part, which involves
interactions between tokens (i.e., intra-sequence), the expert part
processes samples token by token. Based on this, we can further
partition along the token dimension. The pipeline degree is denoted
by ğ‘Ÿ2, and ğ‘šğ‘’represents the token processed by each expert. An
example is shown in Fig. 3d. A primary challenge within FinDEP is
to define the optimal problem (Â§3) and derive the optimal solution
(Â§4), which we will present in the next two sections.
3
Problem Formulation
The inference time of an MoE model under disaggregated expert
parallel decomposes into three primary components: the computa-
tion time for the expert feed-forward networks, the computation
time for the attention layers, and the communication overhead for
transferring activations between the attention and expert groups.
We formulate each component as a function of its workload and
the underlying hardware characteristics.
3.1
Execution Time Formulation
First, for GEMM, we denote the time function as ğ‘¡ğ‘”ğ‘š(ğ‘¥, ğ¹), where
ğ‘¥= ğ‘šÃ— ğ‘˜Ã— ğ‘›represents the total FLOPs required for multiplying
two matrices ğ´âˆˆRğ‘šÃ—ğ‘˜and ğµâˆˆRğ‘˜Ã—ğ‘›on a GPU with peak floating-
point performance ğ¹.
The second part of the computation involves the self-attention
mechanism. The time function for the attention computation is
denoted as ğ‘¡ğ‘ğ‘¡ğ‘¡ğ‘›(ğ‘¦, ğ¹), where ğ‘¦represents the total workload for
self-attention, and ğ¹is the GPUâ€™s peak performance again. In this
case, the workload is defined based on the dimensions of the query
(ğ‘„), key (ğ¾), and value (ğ‘‰) matrices. These matrices have the fol-
lowing shapes: ğ‘„, ğ¾âˆˆRğ‘â„Ã—ğµÃ—ğ‘†Ã—ğ·ğ‘˜, and ğ‘‰âˆˆRğ‘â„Ã—ğµÃ—ğ‘†Ã—ğ·ğ‘£, where
ğ‘â„is the number of attention heads, ğµis the batch size, ğ‘†is the
sequence length, ğ·ğ‘˜is the dimensionality of the key, and ğ·ğ‘£is the
dimensionality of the value. The core computational burden comes
from two GEMMs: the computation of the attention scores via ğ‘„ğ¾âŠ¤,
which has a complexity of ğ‘â„ğµğ‘†2ğ·ğ‘˜, and the computation of the
attention-weighted values Attention(ğ‘„ğ¾âŠ¤)ğ‘‰, which has a complex-
ity of ğ‘â„ğµğ‘†2ğ·ğ‘£. Therefore, the total workload for self-attention is
ğ‘¦= ğ‘â„ğµğ‘†2(ğ·ğ‘˜+ ğ·ğ‘£).
Third, ğ‘¡ğ‘(ğ‘§,ğ‘’ğ‘”,ğ‘ğ‘”) denotes the communication time required for
data transfer between GPUs. More specifically, it measures the time
taken for ğ‘ğ‘”GPUs to send messages to ğ‘’ğ‘”other GPUs. Here, ğ‘§
represents the communication workload per machine, while ğ‘’ğ‘”and
ğ‘ğ‘”refer to the expert and attention group sizes, respectively.
For any given hardware and group configuration, the GPU per-
formance ğ¹and group sizes (ğ‘’ğ‘”, ğ‘ğ‘”) are constant. This stability al-
lows us to simplify the time functions, specifically ğ‘¡ğ‘”ğ‘š(ğ‘¥), ğ‘¡ğ‘ğ‘¡ğ‘¡ğ‘›(ğ‘¦),
and ğ‘¡ğ‘2ğ‘’(ğ‘§), for convenience. These simplified functions provide
a foundation for modeling the end-to-end performance of MoE
systems.
The Attention Part. The attention component consists of a
sequence of computational operations that include both GEMMs
and attention. To illustrate this process, we consider the standard
Multi-Head Attention (MHA) layer as an example. In the ğ‘¡-th trans-
former layer, the input comprises hidden states hğ‘¡âˆˆRğ‘šğ‘Ã—ğ‘†Ã—ğ‘€. The
total forward pass time for this layer, denoted ğ‘¡ğ‘(ğ‘šğ‘), is a function
of these dimensions and can be decomposed into the cumulative
runtime of several GEMM and self-attention operations:
ğ‘¡ğ‘(ğ‘šğ‘) = 2ğ‘¡ğ‘”ğ‘š(ğ‘šğ‘ğ‘†ğ‘€ğ‘›â„ğ‘‘ğ‘˜) + 2ğ‘¡ğ‘”ğ‘š(ğ‘šğ‘ğ‘†ğ‘€ğ‘›â„ğ‘‘ğ‘£)
+ ğ‘¡ğ‘ğ‘¡ğ‘¡ğ‘›(ğ‘šğ‘ğ‘†2ğ‘›â„(ğ‘‘ğ‘˜+ ğ‘‘ğ‘£)).
(1)
The coefficients 2 and 2 in the ğ‘¡ğ‘”ğ‘šterms account for the four linear
projections required by the MHA operation: ğ‘„and ğ¾projections,
and ğ‘‰and Output (ğ‘‚) projections. Notably, other attention vari-
ants like MLA [5] can also be modeled using similar formulations
involving ğ‘¡ğ‘ğ‘¡ğ‘¡ğ‘›and ğ‘¡ğ‘”ğ‘š, enabling unified analysis across various
attention designs.
The Shared Expert Part. The Shared Expert computation fol-
lows a structure similar to the attention layer, consisting of three
primary linear projections: the gating projection, the up-projection,
and the down-projection. For each expert ğ‘–(1 â‰¤ğ‘–â‰¤ğ‘ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘),
the gating and up-projections are represented by ğ‘Šğ‘”ğ‘ğ‘¡ğ‘’
ğ‘–
and ğ‘Šğ‘ˆ
ğ‘–,
respectively, with dimensionsğ‘Šğ‘”ğ‘ğ‘¡ğ‘’
ğ‘–
,ğ‘Šğ‘ˆ
ğ‘–
âˆˆRğ»Ã—ğ‘€, while the down-
projection is given by ğ‘Šğ·
ğ‘–
âˆˆRğ‘€Ã—ğ».
Each device in AG performs the shared expert transformations
locally. The gating operation computes zğ‘”ğ‘ğ‘¡ğ‘’
ğ‘¡,ğ‘–
= ğ‘Šğ‘”ğ‘ğ‘¡ğ‘’
ğ‘–
hğ‘¡, the up-
projection computes zğ‘¢
ğ‘¡,ğ‘–= ğ‘Šğ‘ˆ
ğ‘–hğ‘¡, and the down-projection com-
putes zğ‘‘
ğ‘¡,ğ‘–= ğ‘Šğ·
ğ‘–Swish(zğ‘”ğ‘ğ‘¡ğ‘’
ğ‘¡,ğ‘–
âŠ—zğ‘¢
ğ‘¡,ğ‘–) [23], where Swish(ğ‘¥) =
ğ‘¥
1+ğ‘’âˆ’ğ‘¥.
These operations result in outputs with dimensions zğ‘”ğ‘ğ‘¡ğ‘’
ğ‘¡,ğ‘–, zğ‘¢
ğ‘¡,ğ‘–âˆˆ
Rğ‘šğ‘Ã—ğ‘†Ã—ğ»and zğ‘‘
ğ‘¡,ğ‘–âˆˆRğ‘šğ‘Ã—ğ‘†Ã—ğ‘€, each taking ğ‘¡ğ‘”ğ‘š(ğ‘šğ‘ğ‘†ğ‘€ğ») time. The
total computation time for the Shared Expert across ğ‘ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘expert
layers is the sum of all layers:
ğ‘¡ğ‘ (ğ‘šğ‘) = 3ğ‘ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘¡ğ‘”ğ‘š(ğ‘šğ‘ğ‘†ğ‘€ğ»).
(2)
The MoE Part. The MoE layer employs conditional computation
through a set of feed-forward networks, known as experts. The total
number of ğ¸experts is distributed across ğ‘’ğ‘”devices. Each device is
responsible for computing ğ¸/ğ‘’ğ‘”distinct experts. Each device in the
expert group receives tokens, represented as hâ€²
ğ‘¡âˆˆR(ğ¸/ğ‘’ğ‘”)Ã—ğ‘šğ‘’Ã—ğ‘€,
which are then partitioned along the first dimension into ğ¸/ğ‘’ğ‘”slices:


--- Page 5 ---
FinDEP
hâ€²
ğ‘¡,1, hâ€²
ğ‘¡,2, . . . , hâ€²
ğ‘¡,ğ¸/ğ‘’ğ‘”. Each slice, hâ€²
ğ‘¡,ğ‘–âˆˆRğ‘šğ‘’Ã—ğ‘€, is assigned to the
corresponding local expertğ‘–. Here,ğ‘šğ‘’denotes the number of tokens
processed by a single expert. For each expert ğ‘–, the computation
involves a feed-forward network with weights including the up-
projection ğ‘Šğ‘ˆ
ğ‘–
âˆˆRğ»Ã—ğ‘€, the gating projection ğ‘Šğ‘”ğ‘ğ‘¡ğ‘’
ğ‘–
âˆˆRğ»Ã—ğ‘€,
and the down-projection ğ‘Šğ·
ğ‘–
âˆˆRğ‘€Ã—ğ», all of which reside on the
assigned device. The total computation time for each device is given
by:
ğ‘¡ğ‘’(ğ‘šğ‘’) = 3(ğ¸/ğ‘’ğ‘”)ğ‘¡ğ‘”ğ‘š(ğ‘šğ‘’ğ‘€ğ»).
(3)
A2E and E2A communication. DEP employs two distinct
communication operations: Attention-to-Expert (A2E) and Expert-
to-Attention (E2A). We denote their respective communication
times as ğ‘¡ğ‘2ğ‘’and ğ‘¡ğ‘’2ğ‘. Since the communication workload is ğ‘§=
ğ¸/ğ‘’ğ‘”Ã— ğ‘šğ‘’Ã— ğ‘€. We have:
ğ‘¡ğ‘2ğ‘’(ğ‘šğ‘’) = ğ‘¡ğ‘(ğ‘šğ‘’ğ¸ğ‘€/ğ‘’ğ‘”).
(4)
Due to the symmetric nature of communication in dual-workload
topologies like PCIe or NVLink [10], where data transfer occurs in
different directions simultaneously, the time taken for A2E equals
that for E2A, i.e., ğ‘¡ğ‘2ğ‘’(ğ‘šğ‘’) = ğ‘¡ğ‘’2ğ‘(ğ‘šğ‘’).
3.2
Optimization Problem Formulation
For a given layer ğ‘¡, we define the timestamps that capture the start
times of major computational and communication stages. Let ğœ(ğ‘¡,ğ‘–)
ğ‘
represent the start time of the ğ‘–-th attention segment within the
ğ‘Ÿ1 pipeline, and ğœ(ğ‘¡,ğ‘–)
ğ‘ 
the start time of the corresponding Shared
Expert computation. The Expert computation within each pipeline
segment is also divided into ğ‘Ÿ2 parts, and we denote the start time
of the expert processing for the ğ‘–-th ğ‘Ÿ1 slice and ğ‘—-th ğ‘Ÿ2 token group
as ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘’
. The communication timestamps are defined as ğœ(ğ‘–,ğ‘—)
ğ‘2ğ‘’and
ğœ(ğ‘–,ğ‘—)
ğ‘’2ğ‘, indicating the start times of the A2E and E2A communication
phases, respectively.
Based on the above execution time formulations, we derive a set
of timing constraints between key scheduling timestamps: ğœ(ğ‘¡,ğ‘–)
ğ‘
,
ğœ(ğ‘¡,ğ‘–)
ğ‘ 
, ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘’
, ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘2ğ‘’
, and ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘’2ğ‘
. These constraints describe how
different stages of computation and communication must be ordered
to avoid conflicts and ensure data dependencies are satisfied. All
constraints are represented as
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
ğœ(ğ‘¡â€²,ğ‘–â€²)
ğ‘ 
,ğœ(ğ‘¡â€²,ğ‘–â€²)
ğ‘
âˆ‰[ğœ(ğ‘¡,ğ‘–)
ğ‘
,ğœ(ğ‘¡,ğ‘–)
ğ‘
+ ğ‘¡ğ‘(ğ‘šğ‘))
ğœ(ğ‘¡â€²,ğ‘–â€²)
ğ‘ 
,ğœ(ğ‘¡â€²,ğ‘–â€²)
ğ‘
âˆ‰[ğœ(ğ‘¡,ğ‘–)
ğ‘ 
,ğœ(ğ‘¡,ğ‘–)
ğ‘ 
+ ğ‘¡ğ‘ (ğ‘šğ‘))
ğœ(ğ‘¡â€²,ğ‘–â€²,ğ‘—â€²)
ğ‘2ğ‘’
, âˆ‰[ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘2ğ‘’
,ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘2ğ‘’
+ ğ‘¡ğ‘2ğ‘’(ğ‘šğ‘’))
ğœ(ğ‘¡â€²,ğ‘–â€²,ğ‘—â€²)
ğ‘’2ğ‘
, âˆ‰[ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘’2ğ‘
,ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘’2ğ‘
+ ğ‘¡ğ‘’2ğ‘(ğ‘šğ‘’))
ğœ(ğ‘¡â€²,ğ‘–â€²,ğ‘—â€²)
ğ‘’
âˆ‰[ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘’
,ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘’
+ ğ‘¡ğ‘’(ğ‘šğ‘’))
ğœ(ğ‘¡,ğ‘–)
ğ‘ 
,ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘2ğ‘’
â‰¥ğœ(ğ‘¡,ğ‘–)
ğ‘
+ ğ‘¡ğ‘(ğ‘šğ‘)
ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘’
â‰¥ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘2ğ‘’
+ ğ‘¡ğ‘2ğ‘’(ğ‘šğ‘)
ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘’2ğ‘
â‰¥ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘’
+ ğ‘¡ğ‘’(ğ‘šğ‘’)
ğœ(ğ‘¡+1,ğ‘–)
ğ‘
â‰¥max(ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘’2ğ‘
+ ğ‘¡ğ‘’2ğ‘(ğ‘šğ‘’),ğœ(ğ‘¡,ğ‘–)
ğ‘ 
+ ğ‘¡ğ‘ (ğ‘šğ‘))
ğ‘šğ‘’Â· ğ‘Ÿ2 Â· ğ¸= ğ‘šğ‘Â· ğ‘ğ‘”Â· ğ‘¡ğ‘œğ‘ğ‘˜Â· ğ‘†
.
(5)
The first five rules prevent different stages from using the same
hardware at the same time. This avoids resource conflicts. Rules
6 to 9 ensure that each stage starts only after the previous one
finishes in the same micro-batch. The final rule ensures that all data
is processed accurately without any loss.
Our goal is to maximize the throughput of the disaggregated
MoE pipeline by jointly optimizing the pipeline degrees and token
partition sizes. This leads to the following optimization formulation:
max
ğ‘Ÿ1,ğ‘šğ‘
ğ‘Ÿ2,ğ‘šğ‘’
ğ‘Ÿ1 Â· ğ‘šğ‘Â· ğ‘ğ‘”
max(ğœ(ğ‘‡,ğ‘Ÿ1)
ğ‘ 
+ ğ‘¡ğ‘ (ğ‘šğ‘), ğœ(ğ‘‡,ğ‘Ÿ1,ğ‘Ÿ2)
ğ‘’2ğ‘
+ ğ‘¡ğ‘’2ğ‘(ğ‘šğ‘’))
s. t.
constraints in Eq. (5).
(6)
For any fixed choice of ğ‘Ÿ1,ğ‘šğ‘,ğ‘Ÿ2,ğ‘šğ‘’, the remaining task is to
assign start times ğœthat satisfy the constraints in Eq. (5) and mini-
mize the makespan max ğœ(ğ‘‡,ğ‘Ÿ1)
ğ‘ 
+ğ‘¡ğ‘ (ğ‘šğ‘), ğœ(ğ‘‡,ğ‘Ÿ1,ğ‘Ÿ2)
ğ‘’2ğ‘
+ğ‘¡ğ‘’2ğ‘(ğ‘šğ‘’). This
scheduling subproblem is a variant of the job-shop problem: each
operation (attention, shared, A2E, expert, E2A) runs on a dedicated
machine (resources) and the operations of each micro-batch follow
the precedence graph implied by rules 6â€“9. It is well known that
job-shop scheduling is NP-hard even with three machines. Our
model involves four distinct resources (e.g., AG, EG, A2E, and E2A),
therefore the subproblem is NP-hard. Consequently, the overall
problem, which additionally optimizes over the integer parameters
ğ‘Ÿ1,ğ‘šğ‘,ğ‘Ÿ2,ğ‘šğ‘’, is also NP-hard, because a polynomial-time algorithm
for the overall problem would yield a polynomial-time solution for
the NP-hard subproblem by fixing those parameters appropriately.
4
Solution
To solve the above problem, we need to explicitly determine the
communication and computation times. Thus, we need to model
the performance for a given communication or computation op-
eration, so that we can predict their execution time with different
sizes of input. In this section, we first build simple yet effective per-
formance models for attention, GEMM computation, and A2E/E2A
communication, then we derive the near-optimal solution to the
problem of minimizing Eq. 6.
4.1
Performance Models
Performance model of computation. Following [16, 20, 25], we
use a linear model (with bias) to represent computation. The model
includes an intercept term (ğ›¼ğ‘”ğ‘š) to account for fixed overheads,
such as kernel launches and memory management, and a scaling
factor (ğ›½ğ‘”ğ‘š) to capture the increase in computational cost as the
input size grows. The model is expressed as:
ğ‘¡ğ‘”ğ‘š(ğ‘¥) = ğ›¼ğ‘”ğ‘š+ ğ›½ğ‘”ğ‘šğ‘¥.
(7)
ğ‘¡ğ‘ğ‘¡ğ‘¡ğ‘›(ğ‘¦) = ğ›¼ğ‘ğ‘¡ğ‘¡ğ‘›+ ğ›½ğ‘ğ‘¡ğ‘¡ğ‘›ğ‘¦.
(8)
Performance model of communication. For both A2E and
E2A operations, the communication time can also be accurately
described using a single ğ›¼-ğ›½model. These operations are essentially
reverse processes that share identical communication structures,
allowing for a unified linear model. We define the communication
time as:
ğ‘¡ğ‘(ğ‘§) = ğ›¼ğ‘+ ğ›½ğ‘ğ‘§,
(9)
where ğ‘§represents the input data size (bytes of elements commu-
nicated), ğ›¼ğ‘is the network startup time (overhead), and ğ›½ğ‘is the
transmission time per byte, which is influenced by factors such as
network bandwidth.


--- Page 6 ---
Xinglin Pan1, Shaohuai Shi2, Wenxiang Lin2, Yuxin Wang3, Zhenheng Tang4, Wei Wang4, Xiaowen Chu1,4
Performance models of different layers. By substituting
Eq. 7 and Eq. 8 into Eq. 1, we derive a simplified linear model for the
performance of the MHA layer, expressed as: ğ‘¡ğ‘(ğ‘šğ‘) = ğ›¼ğ‘+ ğ›½ğ‘ğ‘šğ‘,
where the new coefficients are defined as follows:
ğ›¼ğ‘:= 4ğ›¼ğ‘”ğ‘š+ ğ›¼ğ‘ğ‘¡ğ‘¡ğ‘›
(10)
and
ğ›½ğ‘:=ğ›½ğ‘”ğ‘š(2ğ‘†ğ‘€ğ‘›â„ğ‘‘ğ‘˜+ 2ğ‘†ğ‘€ğ‘›â„ğ‘‘ğ‘£)
+ ğ›½ğ‘ğ‘¡ğ‘¡ğ‘›ğ‘†2ğ‘›â„(ğ‘‘ğ‘˜+ ğ‘‘ğ‘£).
(11)
For clarity and analytical tractability, we absorb all terms that
do not vary with ğ‘šğ‘into the constants ğ›¼ğ‘and ğ›½ğ‘. This yields a
linear performance model that captures the contribution of ğ‘šğ‘to
computation time.
Similarly, by substituting Eq. 7 into Eq. 2, we derive a simpli-
fied linear model for the performance of the shared expert layer,
expressed as ğ‘¡ğ‘ (ğ‘šğ‘) = ğ›¼ğ‘ + ğ›½ğ‘ ğ‘šğ‘, where the new coefficients are
defined as follows: ğ›¼ğ‘ := 3ğ‘ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘ğ›¼ğ‘”ğ‘šand ğ›½ğ‘ := 3ğ‘ğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘ğ›½ğ‘”ğ‘šğ‘†ğ‘€ğ».
Building on this methodology, the MoE layerâ€™s performance is
modeled. Substituting Eq. 7 into Eq. 3 and Eq. 9 into Eq. 4, and
absorbing all terms not varying with ğ‘šğ‘’into constants, we express
its performance as ğ‘¡ğ‘’(ğ‘šğ‘’) = ğ›¼ğ‘’+ ğ›½ğ‘’ğ‘šğ‘’. Here, ğ›¼ğ‘’:= (ğ¸/ğ‘’ğ‘”)ğ›¼ğ‘”ğ‘šand
ğ›½ğ‘’:= (ğ¸/ğ‘’ğ‘”)ğ›½ğ‘”ğ‘š(ğ‘€ğ»). For A2E and E2A, the model is ğ‘¡ğ‘2ğ‘’(ğ‘šğ‘’) =
ğ›¼ğ‘2ğ‘’+ ğ›½ğ‘2ğ‘’ğ‘šğ‘’, where ğ›¼ğ‘2ğ‘’:= ğ›¼ğ‘and ğ›½ğ‘2ğ‘’:= ğ›½ğ‘ğ¸ğ‘€/ğ‘’ğ‘”. These linear
models provide a tractable framework for analyzing and predict-
ing the computational overhead of each architectural component,
laying the groundwork for subsequent performance optimization.
Although streamlined, this model effectively captures the domi-
nant performance determinants of startup latency and workload-
dependent scaling, and its fidelity is empirically validated in (Â§5.2).
4.2
Determine Task Order, ğ‘šğ‘, ğ‘Ÿ1, ğ‘šğ‘’, and ğ‘Ÿ2.
Determine the order of Attention and Shared Expert. We
investigate the optimal execution order of attention and Shared
Expert operations in AG by evaluating two primary scheduling
strategies. The number of possible non-illness computing orders
in a layer is given by ğ¶(ğ‘Ÿ1 + ğ‘Ÿ1 âˆ’1,ğ‘Ÿ1) =
(2ğ‘Ÿ1âˆ’1)!
(ğ‘Ÿ1!) ((ğ‘Ÿ1âˆ’1)!) , which is
cumbersome to verify one by one. However, we can observe that
the advantages of more efficient computing are: (a) it allows for the
earliest possible start of A2E communication, which helps utilize
EG without idle time, and (b) it enables the use of AG (Attention
Gate) without idle time.
We focus on the most representative strategies and explain why
they are effective. The first, AASS (Attention-All, Shared-All), pro-
cesses all attention segments within the same layer before proceed-
ing to all Shared Expert segments. The second, ASAS (Attention-
Shared-Alternating-Sequential), alternates between attention and
Shared Expert operations.
As illustrated in Fig. 4, each schedule presents distinct advan-
tages. The AASS approach enables earlier initiation of A2E com-
munication and expert computation, as evident when comparing
Fig. 4a and Fig. 4b. Conversely, ASAS improves GPU utilization
by interleaving Shared Expert segments during periods in which
attention-ready signals are pending, as shown in Fig. 4c and Fig. 4d.
To determine the better strategy, we independently identify the
best-performing configuration for both AASS and ASAS. We then
â†“
AG
A2E
EG
E2A
â†“
â†‘
â†“
â†‘
â†‘
â†“
â†‘
(a) An example illustrating the limitations of ASAS.
â†“
AG
A2E
EG
E2A
â†“
â†‘
â†“
â†‘
â†‘
â†“
â†‘
(b) An example illustrating the advantages of AASS.
AG
A2E
EG
E2A
â†“
â†“
â†“
â†“
â†“
â†“
â†“
â†“
â†‘
â†‘
â†‘
â†‘
â†‘
â†‘
â†‘
â†‘
(c) An example illustrating the limitations of AASS.
AG
A2E
EG
E2A
â†‘
â†“
â†“
â†‘
â†“
â†“
â†“
â†“
â†“
â†“
â†‘
â†‘
â†‘
â†‘
â†‘
â†‘
(d) An example illustrating the advantages of ASAS.
Figure 4: Comparative examples highlighting the advantages
and limitations of AASS and ASAS scheduling strategies.
compare their performance outcomes to select the superior sched-
uling policy.
Determine ğ‘šğ‘. For illustrative purposes, we focus on optimiz-
ing the ASAS scheduling strategy. The same methodology can be
straightforwardly applied to AASS.
Firstly, our optimization focuses on ğ‘Ÿ1 and ğ‘šğ‘. Given a fixed ex-
ecution order, we can iteratively compute the key timing variables:
ğœ(ğ‘¡,ğ‘–)
ğ‘
, ğœ(ğ‘¡,ğ‘–)
ğ‘ 
, ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘’
, ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘2ğ‘’
, and ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘’2ğ‘
.
We first examine the timing relationships within the 0-th layer.
Since the derivation for each variable follows a similar pattern, we
useğœ(0,ğ‘–,ğ‘—)
ğ‘’2ğ‘
as an illustrative example. This timestamp depends on the
completion of ğ‘–pipeline chunks and additionally on the completion
of ğ‘—fine-grained pipeline steps. Its value therefore decomposes into
three components: an initial latency, the cumulative delay from
the ğ‘Ÿ1 pipeline, and the cumulative delay from the fine-grained ğ‘Ÿ2
pipeline, visualized in Fig. 5.


--- Page 7 ---
FinDEP
â†“
â†“
â†“
))
Figure 5: Diagram of the 0-th layer start timestamp ğœ(0,ğ‘–,ğ‘—)
ğ‘’2ğ‘
, de-
composed into three components: pipeline time, fine-grained
pipeline time, and initial latency.
Proceeding similarly for all variables, we obtain the complete
set of timing expressions for the 0-th layer:
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³
ğœ(0,ğ‘–)
ğ‘
= ğ‘–Â· ğ‘‹(ğ‘šğ‘)
ğœ(0,ğ‘–)
ğ‘ 
= ğ‘–Â· ğ‘‹(ğ‘šğ‘) + ğ‘¡ğ‘(ğ‘šğ‘)
ğœ(0,ğ‘–,ğ‘—)
ğ‘2ğ‘’
= ğ‘¡ğ‘(ğ‘šğ‘) + ğ‘–Â· ğ¹(ğ‘šğ‘,ğ‘šğ‘’) + ğ‘—Â· ğ‘¡ğ‘2ğ‘’(ğ‘šğ‘’)
ğœ(0,ğ‘–,ğ‘—)
ğ‘’
= ğ‘¡ğ‘(ğ‘šğ‘) + ğ‘¡ğ‘2ğ‘’(ğ‘šğ‘’) + ğ‘–Â· ğ¹(ğ‘šğ‘,ğ‘šğ‘’) + ğ‘—Â· ğ‘Œ(ğ‘šğ‘’)
ğœ(0,ğ‘–,ğ‘—)
ğ‘’2ğ‘
= ğ‘¡ğ‘(ğ‘šğ‘) + ğ‘¡ğ‘2ğ‘’(ğ‘šğ‘’) + ğ‘¡ğ‘’(ğ‘šğ‘’)
+ ğ‘–Â· ğ¹(ğ‘šğ‘,ğ‘šğ‘’) + ğ‘—Â· ğ‘Œ(ğ‘šğ‘’)
,
where ğ‘‹(ğ‘šğ‘) = ğ‘¡ğ‘(ğ‘šğ‘) + ğ‘¡ğ‘ (ğ‘šğ‘), ğ‘Œ(ğ‘šğ‘’) = max(ğ‘¡ğ‘’(ğ‘šğ‘’),ğ‘¡ğ‘2ğ‘’(ğ‘šğ‘’)),
and ğ¹(ğ‘šğ‘,ğ‘šğ‘’) = ğ‘šğ‘ğ‘¥(ğ‘‹(ğ‘šğ‘),ğ‘Ÿ2 Â· ğ‘Œ(ğ‘šğ‘’)).
For the ğ‘¡-th layer, the timing variables ğœ(ğ‘¡,ğ‘–)
ğ‘
, ğœ(ğ‘¡,ğ‘–)
ğ‘ 
, ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘’
, ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘2ğ‘’
,
and ğœ(ğ‘¡,ğ‘–,ğ‘—)
ğ‘’2ğ‘
can be derived based on the corresponding variables
from the (ğ‘¡âˆ’1)-th layer. Specifically, each of them is computed by
adding an offset term to their respective (ğ‘¡âˆ’1)-th counterparts.
This offset is given by: max(ğº(ğ‘šğ‘,ğ‘šğ‘’), ğ‘Ÿ1 Â· ğ¹(ğ‘šğ‘,ğ‘šğ‘’)), where
ğº(ğ‘šğ‘,ğ‘šğ‘’) =ğ‘¡ğ‘(ğ‘šğ‘) + ğ‘¡ğ‘2ğ‘’(ğ‘šğ‘’) + ğ‘¡ğ‘’(ğ‘šğ‘’)
+ ğ‘¡ğ‘2ğ‘’(ğ‘šğ‘’) + (ğ‘Ÿ2 âˆ’1) Â· ğ‘Œ(ğ‘šğ‘’).
(12)
Here, ğ¸(ğ‘šğ‘,ğ‘šğ‘’) represents the time required to ensure that the
GPUs in AG are idle and ready for the next attention segment,
while ğ‘Ÿ1 Â· ğ¹(ğ‘šğ‘,ğ‘šğ‘’) denotes the time required for the output of the
expert computation on the 0-th chunk to be sent back.
We can simplify the optimal objective defined in Eq. 6 as follows:
max
ğ‘Ÿ1,ğ‘šğ‘
ğ‘Ÿ2,ğ‘šğ‘’
ğ‘Ÿ1 Â· ğ‘šğ‘
(ğ‘‡âˆ’1) max(ğº(ğ‘šğ‘,ğ‘šğ‘’),ğ‘Ÿ1ğ¹(ğ‘šğ‘,ğ‘šğ‘’))
+ max(ğ‘‹(ğ‘šğ‘),ğº(ğ‘šğ‘,ğ‘šğ‘’)) + (ğ‘Ÿ2 âˆ’1)ğ‘Œ(ğ‘šğ‘’)
+ (ğ‘Ÿ1 âˆ’1)ğ¹(ğ‘šğ‘,ğ‘šğ‘’)
.
(13)
To accelerate the search process, we first identify a crucial prop-
erty: for a fixed value of ğ‘Ÿ1, the objective function defined in Eq. 13
increases monotonically with respect to ğ‘šğ‘. To establish this, we
employ a two-step proof. First, we demonstrate that for any given
pair (ğ‘Ÿ1,ğ‘Ÿ2), the objective function in Eq. 13 is monotonically in-
creasing with respect to ğ‘šğ‘. The detailed proof of this claim is
presented below.
Theorem 1. Given pair (ğ‘Ÿ1,ğ‘Ÿ2), the objective function in Eq. 13 is
monotonically increasing with respect to ğ‘šğ‘.
Proof. To analyze the behavior of the objective function con-
cerning ğ‘šğ‘, we first establish a direct relationship between ğ‘šğ‘’and
ğ‘šğ‘. From the constraintğ‘šğ‘Â·ğ‘ğ‘”Â·ğ‘¡ğ‘œğ‘ğ‘˜Â·ğ‘†= ğ‘šğ‘’Â·ğ‘Ÿ2 Â·ğ¸, we can express
ğ‘šğ‘’as a linear function of ğ‘šğ‘. We have ğ‘šğ‘’= ğ‘˜Â· ğ‘šğ‘, where the
constant ğ‘˜= ğ‘ğ‘”Â·ğ‘¡ğ‘œğ‘ğ‘˜Â·ğ‘†
ğ‘Ÿ2Â·ğ¸
.
The component functions ğ‘‹(ğ‘šğ‘), ğ‘Œ(ğ‘šğ‘’), and ğ¸(ğ‘šğ‘,ğ‘šğ‘’) are de-
fined as sums and maximums of the base linear performance models
ğ‘¡ğ‘(ğ‘šğ‘), ğ‘¡ğ‘ (ğ‘šğ‘), and ğ‘¡ğ‘’(ğ‘šğ‘’). By substituting ğ‘šğ‘’= ğ‘˜Â· ğ‘šğ‘, each of
these components becomes a linear or piecewise linear function
of ğ‘šğ‘. Specifically, the denominator of the objective function is
constructed from additions and max operations on these functions.
Since the sum of linear functions is linear, and the maximum of lin-
ear functions is piecewise linear and convex, the entire denominator
is a positive, piecewise linear, and convex function of ğ‘šğ‘.
Therefore, the objective function takes the form of
ğ‘Ÿ1ğ‘šğ‘
ğ·(ğ‘šğ‘) ,
where ğ·(ğ‘šğ‘) is the piecewise linear denominator. Within any lin-
ear segment of ğ·(ğ‘šğ‘), the objective function can be written as
ğ‘Ÿ1ğ‘šğ‘
ğ›¼ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™+ğ›½ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ğ‘šğ‘, where ğ›¼ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™and ğ›½ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™are positive constants aggre-
gated from the underlying ğ›¼and ğ›½parameters of the performance
models. To demonstrate its monotonic nature, we can rewrite the
previous equation as
ğ‘Ÿ1
ğ›¼ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™
ğ‘šğ‘
+ğ›½ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™. As ğ‘šğ‘increases, the term ğ›¼ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™
ğ‘šğ‘
decreases. This causes the denominator of the overall expression to
decrease, which in turn increases the value of the function. Thus,
the objective function is monotonically increasing concerning ğ‘šğ‘
across each linear segment, and therefore, it is monotonically in-
creasing for all ğ‘šğ‘> 0.
â–¡
Next, we extend the result to show that for a fixed value of ğ‘Ÿ1, the
objective function in Eq. 13 increases monotonically with respect
to ğ‘šğ‘. This follows directly from Theorem 1. The detailed proof of
this generalized claim is provided below.
Theorem 2. Given ğ‘Ÿ1, the objective function in Eq. 13 is monoton-
ically increasing with respect to ğ‘šğ‘.
Proof. Consider any arbitrary value ğ‘šğ‘and the corresponding
pair (ğ‘Ÿ1,ğ‘Ÿâˆ—
2), where
ğ‘Ÿâˆ—
2 = arg max
ğ‘Ÿ2
ğ‘Ÿ1 Â· ğ‘šğ‘
max(ğœ(ğ‘‡,ğ‘Ÿ1)
ğ‘ 
+ ğ‘¡ğ‘ (ğ‘šğ‘), ğœ(ğ‘‡,ğ‘Ÿ1,ğ‘Ÿ2)
ğ‘’2ğ‘
+ ğ‘¡ğ‘’2ğ‘(ğ‘šğ‘’))
,
according to Theorem 1, for any ğ‘šâ€²
ğ‘> ğ‘šğ‘, the following inequality
holds:
ğ‘Ÿ1 Â· ğ‘šâ€²
ğ‘
max(ğœ(ğ‘‡,ğ‘Ÿ1)
ğ‘ 
+ ğ‘¡ğ‘ (ğ‘šâ€²ğ‘), ğœ
(ğ‘‡,ğ‘Ÿ1,ğ‘Ÿâˆ—
2 )
ğ‘’2ğ‘
+ ğ‘¡ğ‘’2ğ‘(ğ‘šâ€²ğ‘’))
> max
ğ‘Ÿ2
ğ‘Ÿ1 Â· ğ‘šğ‘
max(ğœ(ğ‘‡,ğ‘Ÿ1)
ğ‘ 
+ ğ‘¡ğ‘ (ğ‘šğ‘), ğœ(ğ‘‡,ğ‘Ÿ1,ğ‘Ÿ2)
ğ‘’2ğ‘
+ ğ‘¡ğ‘’2ğ‘(ğ‘šğ‘’))
.
It implies that
ğ‘Ÿ1 Â· ğ‘šâ€²
ğ‘
max(ğœ(ğ‘‡,ğ‘Ÿ1)
ğ‘ 
+ ğ‘¡ğ‘ (ğ‘šâ€²ğ‘), ğœ
(ğ‘‡,ğ‘Ÿ1,ğ‘Ÿâˆ—
2 )
ğ‘’2ğ‘
+ ğ‘¡ğ‘’2ğ‘(ğ‘šâ€²ğ‘’))
â‰¤max
ğ‘Ÿ2
ğ‘Ÿ1 Â· ğ‘šâ€²
ğ‘
max(ğœ(ğ‘‡,ğ‘Ÿ1)
ğ‘ 
+ ğ‘¡ğ‘ (ğ‘šâ€²ğ‘), ğœ(ğ‘‡,ğ‘Ÿ1,ğ‘Ÿ2)
ğ‘’2ğ‘
+ ğ‘¡ğ‘’2ğ‘(ğ‘šâ€²ğ‘’))
.
Consequently, the objective function increases monotonically as
ğ‘šğ‘increases, which completes the proof.
â–¡
Determine ğ‘Ÿ1. We now turn our attention to analyzing the
behavior of the objective function with respect to the parameter
ğ‘Ÿ1. Specifically, we aim to demonstrate that, for a fixed value of


--- Page 8 ---
Xinglin Pan1, Shaohuai Shi2, Wenxiang Lin2, Yuxin Wang3, Zhenheng Tang4, Wei Wang4, Xiaowen Chu1,4
Figure 6: The pipeline of FinDEP, which consists of an offline planning phase and an online adaptive phase.
ğ‘šğ‘, the objective function defined in Eq. 13 is monotonically non-
decreasing with respect to ğ‘Ÿ1. To establish this result, we adopt a
two-step proof strategy analogous to that used in prior analysis.
The first step involves showing that, for any fixed pair (ğ‘šğ‘,ğ‘Ÿ2),
the objective function increases or remains constant as ğ‘Ÿ1 increases.
The second step mirrors the approach in Theorem 2. However, due
to space constraints, we omit this proof from the paper. In what
follows, we focus on formally proving the first step.
Theorem 3. Given (ğ‘šğ‘,ğ‘Ÿ2), the objective function in Eq. 13 is
monotonically non-decreasing with respect to ğ‘Ÿ1.
Proof. To analyze monotonicity with respect to ğ‘Ÿ1, observe that
the objective function takes the form ğ‘Ÿ1ğ‘šğ‘
ğ·(ğ‘Ÿ1) , where the denominator
ğ·(ğ‘Ÿ1) is piecewise linear in ğ‘Ÿ1. When expressed as ğ·(ğ‘Ÿ1) = ğµğ‘Ÿ1 +ğ¶
in each linear segment (ğµ> 0), monotonicity depends critically on
the sign of the constant term ğ¶. We demonstrate ğ¶â‰¥0, where
ğ¶= max(ğ‘‹(ğ‘šğ‘),ğº(ğ‘šğ‘,ğ‘šğ‘’) + (ğ‘Ÿ2 âˆ’1)ğ‘Œ(ğ‘šğ‘’)) âˆ’ğ¹(ğ‘šğ‘,ğ‘šğ‘’). (14)
First, from the inequality ğ¸(ğ‘šğ‘,ğ‘šğ‘’) â‰¥ğ‘Œ(ğ‘šğ‘’), we derive:
ğº(ğ‘šğ‘,ğ‘šğ‘’) + (ğ‘Ÿ2 âˆ’1)ğ‘Œ(ğ‘šğ‘’) â‰¥ğ‘Ÿ2ğ‘Œ(ğ‘šğ‘’).
(15)
Consequently, we have:
max(ğ‘‹(ğ‘šğ‘),ğº(ğ‘šğ‘,ğ‘šğ‘’) + (ğ‘Ÿ2 âˆ’1)ğ‘Œ(ğ‘šğ‘’))
â‰¥max(ğ‘‹(ğ‘šğ‘),ğ‘Ÿ2ğ‘Œ(ğ‘šğ‘’)).
(16)
Since ğ¹(ğ‘šğ‘,ğ‘šğ‘’) = max(ğ‘‹(ğ‘šğ‘),ğ‘Ÿ2ğ‘Œ(ğ‘šğ‘’)), it follows directly that
ğ¶â‰¥0. With ğ´= ğ‘šğ‘> 0, ğµ> 0, and ğ¶â‰¥0, the objective function
becomes
ğ´ğ‘Ÿ1
ğµğ‘Ÿ1+ğ¶. Its derivative is:
ğ‘‘
ğ‘‘ğ‘Ÿ1
(
ğ´ğ‘Ÿ1
ğµğ‘Ÿ1 + ğ¶) = ğ´(ğµğ‘Ÿ1 + ğ¶) âˆ’ğ´ğ‘Ÿ1ğµ
(ğµğ‘Ÿ1 + ğ¶)2
=
ğ´ğ¶
(ğµğ‘Ÿ1 + ğ¶)2 â‰¥0,
since ğ´> 0, ğ¶â‰¥0, and the denominator is positive. Therefore, the
objective function is monotonically non-decreasing in ğ‘Ÿ1.
â–¡
Determine ğ‘Ÿ2 and ğ‘šğ‘’. The final parameters to verify are ğ‘Ÿ2
and ğ‘šğ‘’. Given ğ‘šğ‘and ğ‘Ÿ1, we have only one free variable, as the
other is constrained by the relation: ğ‘šğ‘Â· ğ‘ğ‘”Â· ğ‘¡ğ‘œğ‘ğ‘˜Â· ğ‘†/ğ¸= ğ‘šğ‘’Â· ğ‘Ÿ2.
To simplify, we express ğ‘šğ‘’(1/ğ‘Ÿ2) = (ğ‘šğ‘ğ‘ğ‘”topğ‘˜ğ‘†)/(ğ¸Â· ğ‘Ÿ2) = ğ‘˜â€²/ğ‘Ÿ2
thereby reducing the problem to solving for ğ‘Ÿ2 alone. Fortunately,
the objective function is convex with respect to 1/ğ‘Ÿ2.
Theorem 4. Given ğ‘Ÿ1 and ğ‘šğ‘, the objective function in Eq. 13 is
convex with respect to 1/ğ‘Ÿ2.
Proof. To optimize the objective function in Eq. 13, we express
it as the following equivalent form:
min
ğ‘Ÿ2 ((ğ‘‡âˆ’1) max(ğº(ğ‘šğ‘,ğ‘šğ‘’),ğ‘Ÿ1ğ¹(ğ‘šğ‘,ğ‘šğ‘’)))
+ max(ğ‘‹(ğ‘šğ‘),ğº(ğ‘šğ‘,ğ‘šğ‘’)) + (ğ‘Ÿ2 âˆ’1)ğ‘Œ(ğ‘šğ‘’)
+ (ğ‘Ÿ1 âˆ’1)ğ¹(ğ‘šğ‘,ğ‘šğ‘’).
(17)
Algorithm 1 FinDEP Configuration Search
Input: ğ‘ƒ,ğ‘ğ‘”,ğ‘’ğ‘”, ğ›¼âˆ—, ğ›½âˆ—, ğµ,ğ‘†, ğ», ğ‘€, ğ‘shared, ğ¸,ğ‘¡ğ‘œğ‘ğ‘˜,ğ‘‡
Output: best_config = (ğ‘šğ‘,ğ‘Ÿ1,ğ‘šğ‘’,ğ‘Ÿ2, order)
1: best_tps â†0
2: best_config â†âˆ…
3: ğ‘Ÿâ€²
1 â†0
âŠ²Previous ğ‘Ÿ1
4: for ğ‘šğ‘= âˆdownto 1 do
5:
ğ‘Ÿ1 â†getMaxR1(ğ‘ğ‘”,ğ‘’ğ‘”,ğ‘šğ‘, ğ‘ƒ, ğµ,ğ‘†, ğ», ğ‘€, ğ‘shared, ğ¸,ğ‘¡ğ‘œğ‘ğ‘˜,ğ‘‡)
âŠ²Memory-constrained
6:
if ğ‘Ÿ1 == 0 or ğ‘Ÿ1 == ğ‘Ÿâ€²
1 then
7:
continue
âŠ²Skip non-Pareto-optimal (ğ‘šğ‘,ğ‘Ÿ1)
8:
for order âˆˆ{ğ´ğ‘†ğ´ğ‘†,ğ´ğ´ğ‘†ğ‘†} do
âŠ²Evaluate both execution
orders
9:
ğ‘Ÿâˆ—
2, tps â†Solve(minğ‘Ÿ2 Eq. 17) âŠ²Returns optimizer and
optimal value
10:
ğ‘šğ‘’â†ğ‘šğ‘Â·ğ‘ğ‘”Â·ğ‘¡ğ‘œğ‘ğ‘˜Â·ğ‘†
ğ‘Ÿâˆ—
2 Â·ğ¸
11:
if tps > best_tps then
12:
best_tps â†tps
13:
best_config â†(ğ‘šğ‘,ğ‘Ÿ1,ğ‘šğ‘’,ğ‘Ÿâˆ—
2, order)
14:
ğ‘Ÿâ€²
1 â†ğ‘Ÿ1
15: return best_config
We aim to prove the convexity of this objective function. Specif-
ically, we need to verify the convexity of the term (ğ‘Ÿ2 âˆ’1)ğ‘Œ(ğ‘šğ‘’)
and the product ğ‘Ÿ2(ğ‘šğ‘’)ğ‘Œ(ğ‘šğ‘’) within ğ¹(ğ‘šğ‘,ğ‘šğ‘’). The performance
models ğ‘¡ğ‘’(1/ğ‘Ÿ2) = ğ›¼ğ‘’+ ğ›½ğ‘’ğ‘˜â€²/ğ‘Ÿ2 and ğ‘¡ğ‘2ğ‘’(1/ğ‘Ÿ2) = ğ›¼ğ‘2ğ‘’+ ğ›½ğ‘2ğ‘’ğ‘˜â€²/ğ‘Ÿ2
are linear functions of 1/ğ‘Ÿ2. Since their coefficients are positive,
these functions are convex and monotonically increasing. We de-
fine ğ‘Œ(1/ğ‘Ÿ2) = max(ğ‘¡ğ‘’(1/ğ‘Ÿ2),ğ‘¡ğ‘2ğ‘’(1/ğ‘Ÿ2)). The maximum of linear
functions is piecewise linear and convex, and since both are increas-
ing, ğ‘Œ(1/ğ‘Ÿ2) is non-decreasing, preserving convexity.
Next, the product ğ‘Ÿ2 Â·ğ‘Œ(ğ‘šğ‘’) is the maximum of terms of the form
ğ›¼ğ‘Ÿ2 +ğ‘˜â€²ğ›½, which are convex for ğ‘Ÿğ‘’> 0. Hence, ğ‘Ÿ2 Â· ğ‘Œ(ğ‘šğ‘’) is convex.
The function ğº(ğ‘šğ‘,ğ‘šğ‘’) includes terms like ğ‘¡ğ‘’(1/ğ‘Ÿ2), ğ‘¡ğ‘2ğ‘’(1/ğ‘Ÿ2),
and (ğ‘Ÿ2 âˆ’1)ğ‘Œ(1/ğ‘Ÿ2). The linear terms are convex, and (ğ‘Ÿ2 âˆ’1)ğ‘Œ(ğ‘šğ‘’),
which is a maximum of convex functions, is also convex. Thus,
ğº(ğ‘šğ‘,ğ‘šğ‘’) is convex.
The objective function is the sum of three terms: (ğ‘Ÿ1 âˆ’
1)ğ¹(ğ‘šğ‘,ğ‘šğ‘’), which is convex since ğ‘Ÿ1 â‰¥1 and ğ¹(ğ‘šğ‘,ğ‘šğ‘’) is con-
vex; (ğ‘‡âˆ’1) max(ğº(ğ‘šğ‘,ğ‘šğ‘’),ğ‘Ÿ1ğ¹(ğ‘šğ‘,ğ‘šğ‘’)), which is convex since
both ğº(ğ‘šğ‘,ğ‘šğ‘’) and ğ‘Ÿ1ğ¹(ğ‘šğ‘,ğ‘šğ‘’) are convex, and ğ‘‡
â‰¥1; and
max(ğ‘‹(ğ‘šğ‘),ğº(ğ‘šğ‘,ğ‘šğ‘’)), which is convex because it is the max-
imum of a constant and a convex function. Since the sum of convex
functions is convex, the entire objective function is convex with
respect to 1/ğ‘Ÿ2.
â–¡


--- Page 9 ---
FinDEP
4.3
Algorithm
Based on the previous analysis, we propose an efficient algorithm to
find the near-optimal configuration for ğ‘Ÿ1, ğ‘šğ‘, ğ‘Ÿ2, and ğ‘šğ‘’, as shown
in Algorithm 1. Given a computing order, the algorithm provides
the optimal configuration for that order, focusing on maximizing
inference throughput. Specifically, we focus on the Pareto frontier
of (ğ‘šğ‘,ğ‘Ÿ1) under memory constraints, respecting the monotonicity
of ğ‘šğ‘and ğ‘Ÿ1. The algorithm iterates over ğ‘šğ‘in descending order
and calculates the maximum allowable ğ‘Ÿ1 based on memory limits.
We skip configurations with the same ğ‘Ÿ1 as the previous iteration
to avoid redundancy.
For each unique (ğ‘šğ‘,ğ‘Ÿ1) pair, the algorithm evaluates two execu-
tion orders: ASAS and AASS. For each order, we solve a convex opti-
mization problem to find the optimalğ‘Ÿ2 that maximizes the objective
in Eq. 13. Then, we calculate ğ‘šğ‘’as: ğ‘šğ‘’= ğ‘šğ‘Â· ğ‘ğ‘”Â· ğ‘¡ğ‘œğ‘ğ‘˜Â· ğ‘†/(ğ‘Ÿ2 Â· ğ¸).
The configuration with the highest throughput is returned. This
approach efficiently explores the search space and eliminates subop-
timal configurations. With the near-optimal solution derived from
Algorithm 1, we obtain the fine-grained task schedule in FinDEP
for MoE inference.
Complexity Analysis Our complexity analysis is divided into
two steps: first, determining the number of possible (ğ‘Ÿ1,ğ‘šğ‘) posi-
tions on the Pareto frontier, and second, analyzing the time spent on
convex optimization. Since the memory constraint is ğ‘Ÿ1 Â· ğ‘šğ‘â‰¤ğ‘€,
where ğ‘€is the largest micro-batch size that the GPU can hold,
the number of distinct values of ğ‘šğ‘= âŒŠğ‘€
ğ‘Ÿ1 âŒ‹corresponds to the
number of divisors of ğ‘€, denoted as ğ‘‘(ğ‘€). The number of divisors
grows at most as ğ‘‚(
âˆš
ğ‘€). Since convex optimization is performed
for a single parameter ğ‘Ÿ2, the solver operates quickly. Assuming
constant optimization time, denoted as ğ¶, the overall complexity is
ğ‘‚(ğ¶Â· ğ‘‘(ğ‘€)). Given the fast nature of the solver, the inference time
is almost unaffected by this process(Â§5).
Online Pipeline Fig. 6 illustrates our system pipeline, which is
bifurcated into offline and online phases. The offline phase handles
initialization: we first select the serving model (e.g., DeepSeek or
Qwen) and determine the sizes of the Attention Group and Expert
Group (ğ‘ğ‘”,ğ‘’ğ‘”). Subsequently, we utilize an offline performance
model to collect the necessary model coefficients and hardware
parameters, which serve as inputs for the optimization solver.
The online phase addresses runtime adaptation. As input data
shapes are unknown prior to request arrival, configuration deci-
sions must be made in real-time. Upon data arrival, the system
executes the lightweight Algorithm 1 to rapidly derive the optimal
configuration (ğ‘šğ‘,ğ‘Ÿ1,ğ‘šğ‘’,ğ‘Ÿ2, order). This approach allows FinDEP
to dynamically adapt to varying workloads, achieving superior
speedup ratios compared to static settings.
5
Evaluation
5.1
Testbeds
Our experiments leverage four distinct hardware testbeds. Testbed
A uses a single node with eight NVIDIA A6000 GPUs, while Testbed
B is configured with eight NVIDIA A10 GPUs. Testbed C also em-
ploys a single node, equipped with eight NVIDIA H20 GPUs, and
Testbed D scales this configuration across four nodes, each contain-
ing eight H20 GPUs. Further details regarding the server config-
uration can be found in Table 2. Our software environment runs
(a) computing on A6000.
(b) communication on A6000.
Figure 7: Performance models for GEMM, Attention, and
communication. Markers represent measured values, while
the lines correspond to predicted values with estimated pa-
rameters. (a) ğ›¼ğ‘”ğ‘š= 0.17 and ğ›½ğ‘”ğ‘š= 8.59 Ã— 10âˆ’11. ğ›¼ğ‘ğ‘¡ğ‘¡ğ‘›= 0.15 and
ğ›½ğ‘ğ‘¡ğ‘¡ğ‘›= 1.54 Ã— 10âˆ’11. (b) (ğ›¼ğ‘2ğ‘’, ğ›½ğ‘2ğ‘’) take the following values:
(0.10, 9.61 Ã— 10âˆ’7) for (ğ‘’ğ‘”= 7,ğ‘ğ‘”= 1), (0.01, 1.28 Ã— 10âˆ’6) for
(ğ‘’ğ‘”= 6,ğ‘ğ‘”= 2), and (0.37, 2.55 Ã— 10âˆ’6) for (ğ‘’ğ‘”= 4,ğ‘ğ‘”= 4).
on Ubuntu 22.04, with Python 3.10, CUDA 11.3, PyTorch 2.4, and
NCCL 2.27.5. We implement attention using FlashInfer 0.3.0 [32].We
implement Attention-to-Expert and Expert-to-Attention transfer
atop NCCL.
5.2
Verification of Performance Models
We conduct micro-benchmarks to determine the values of ğ›¼ğ‘”ğ‘š,
ğ›½ğ‘”ğ‘š, ğ›¼ğ‘ğ‘¡ğ‘¡ğ‘›, ğ›½ğ‘ğ‘¡ğ‘¡ğ‘›, ğ›¼ğ‘2ğ‘’and ğ›½ğ‘2ğ‘’before solving the algorithm. For the
GEMM component, we test a range of matrix configurations across
all matrix sizes encountered in the MLA. This comprehensive test-
ing ensures our model can effectively handle varying configurations.
The results, shown in Fig. 7a, yield an ğ‘…2 value of 0.997132.
For the communication component, we separately compute ğ›¼ğ‘2ğ‘’
and ğ›½ğ‘2ğ‘’for different ğ‘ğ‘”and ğ‘’ğ‘”settings, as these parameters are
interdependent. Since the time for Expert-to-Attention matches
that for Attention-to-Expert, we do not need to rerun the micro-
benchmark for the former case. The results, shown in Fig. 7b, yield
ğ‘…2 values of 0.999986, 0.999911, and 0.994018, indicating a strong
fit. This demonstrates that simple linear models can accurately
predict execution time, consistent with findings in prior work on
performance modeling [16, 20, 25].
We run 30 trials per data point: 10 for warm-up and 20 for statis-
tics. The full micro-benchmark, including Attention, GEMM, and
communication steps, takes under 2 minutes.
5.3
Monotonicity of Throughput with Respect
to ğ‘šğ‘and ğ‘Ÿ1
Our analysis reveals a key monotonic relationship: under per-
parameter optimization, throughput increases monotonically with
respect to ğ‘šğ‘and ğ‘Ÿ1. Specifically, for a given model and a fixed
(ğ‘ğ‘”,ğ‘’ğ‘”) configuration, if the value of ğ‘šğ‘is held constant, through-
put increases as ğ‘Ÿ1 increases, provided that the (ğ‘šğ‘’,ğ‘Ÿ2) pair and
computation order are optimized for each specific value of ğ‘Ÿ1. Con-
versely, the same monotonic increase holds for ğ‘šğ‘when ğ‘Ÿ1 is fixed
and (ğ‘šğ‘’,ğ‘Ÿ2) and the computation order are optimized accordingly.
In this experiment, for each (ğ‘šğ‘,ğ‘Ÿ1) pair, we performed a brute-
force search over all (ğ‘šğ‘’,ğ‘Ÿ2) values and computation orders to
determine the optimal throughput. To accelerate testing, we used a


--- Page 10 ---
Xinglin Pan1, Shaohuai Shi2, Wenxiang Lin2, Yuxin Wang3, Zhenheng Tang4, Wei Wang4, Xiaowen Chu1,4
Table 2: The server configurations in our testbeds.
Name
Testbed A
Testbed B
Testbed C
Testbed D
Memory
48GB
24GB
96GB
96GB
GPU
8x Nvidia RTXA6000 8x Nvidia A10 8x Nvidia H20 32x Nvidia H20
Architecture Ampere
Ampere
Hopper
Hopper
Boost Clock 1.46GHz
1.41GHz
1.98GHz
1.98GHz
NVlink
YES
NO
YES
YES
PCIe
4.0 (x16)
4.0 (x16)
4.0 (x16)
4.0 (x16)
Table 3: Throughput (tokens/s) of DeepSeek-V2 on Testbed C
and Testbed D for varying ğ‘šğ‘and sequence length ğ‘†.
Testbed
ğ‘º
ğ’ğ’‚= 1
ğ’ğ’‚= 2
ğ’ğ’‚= 4
Testbed C
2048
202.67
245.33
284.00
4096
230.12
254.84
270.35
Testbed D
2048
558.23
690.47
756.35
4096
632.41
682.49
707.57
Table 4: Throughput (tokens/s) of DeepSeek-V2 on Testbed C
and Testbed D for varying ğ‘Ÿ1 and sequence length ğ‘†.
Testbed
ğ‘º
ğ’“1 = 1
ğ’“1 = 2
ğ’“1 = 4
Testbed C
2048
202.67
257.24
282.04
4096
230.12
262.62
269.92
Testbed D
2048
558.23
711.36
760.48
4096
632.41
714.66
735.46
smaller variant of DeepSeek-V2 236B [5], keeping all other hyper-
parameters unchanged and employing only two MoE layers. On
Testbed C, we set (ğ‘ğ‘”,ğ‘’ğ‘”) = (3, 5) and ğ‘†= 2048, 4096. On Testbed
D, we set (ğ‘ğ‘”,ğ‘’ğ‘”) = (8, 24) and ğ‘†= 2048, 4096.
Throughput increases monotonically with ğ‘šğ‘As shown in
Table 3, throughput rises as ğ‘šğ‘increases while ğ‘Ÿ1 is fixed at 1. This
confirms that our theoretical proof aligns with the experimental
results. Demonstrating this monotonic relationship allows us to
constrain the candidate variable space, thereby speeding up the
search process.
Throughput increases monotonically with ğ‘Ÿ1 Similarly, as
shown in Table 4, throughput rises as ğ‘Ÿ1 increases while ğ‘šğ‘is fixed
at 1. This result further validates our theoretical proof. Establishing
this second monotonic relationship similarly constrains the search
space, improving overall optimization efficiency.
5.4
Evaluation on Real-World Models
We evaluate the average end-to-end training iteration time for
the small DeepSeek-V2 236B [6] model, using an 8-layer config-
uration on testbed A, a 4-layer configuration on testbed B, and a
16-layer configuration on testbed C and D. Additionally, we assess
the performance of the small Qwen3-235B-A22B [28] model, with
a 24-layer configuration on Testbed A and a 12-layer configuration
on Testbed B and a 48-layer configuration on Testbed C and D.
Our approach, FinDEP, is compared against the state-of-the-art
PPPipe [36], for which we provide our own reimplementation to
ensure a fair comparison.
Table 5 reports the average iteration throughput (tokens per sec-
ond) across different sequence lengths (specifically 1024, 2048, 4096,
and 8192), two model backbones (DeepSeek-V2 and Qwen3), and
four testbeds (A, B, C, D). Each throughput value represents the av-
erage of three independent runs. The data demonstrate that FinDEP
consistently outperforms the optimally configured PPPipe across
all experimental dimensions. The speedup achieved by FinDEP, in-
dicated in parentheses within the table, ranges from 1.02Ã— to 1.61Ã—.
This performance advantage holds true for varying computational
scales (testbeds A through D) and is evident across the full spec-
trum of tested sequence lengths. When the sequence is very long,
FinDEP is much faster (see the bold numbers 1.53Ã— and 1.61Ã— in
the table). Notably, the solver completes in under 1 second.
Discussion. In our configuration of testbed A with the DeepSeek
backbone, we observe that FinDEP effectively hides communication
costs, approaching near-optimal performance. Compared to PPPipe
under the same conditions (e.g., (ğ‘’ğ‘”,ğ‘ğ‘”)), FinDEP reduces commu-
nication by 1.7Ã— as shown in Table 7. This indicates that, for shorter
sequences, communication optimizations offer limited improve-
ment. However, for longer sequences, communication becomes the
primary bottleneck. For instance, with a sequence length of 4096,
there is a 25.87 ms gap where computation and communication do
not overlap. This emphasizes the near-optimal performance of our
solution.
5.5
Evaluation on Online Settings
In the online setting, reboot costs limit frequent changes to ğ‘ğ‘”
and ğ‘’ğ‘”. Additionally, the unpredictable user prompt length (i.e.,
sequence length) complicates DEP deployment. However, our fast
solver addresses this by quickly adjusting ğ‘Ÿ1, ğ‘Ÿ2, and the execution
order after receiving the prompt length. We evaluate FinDEP with
the following configurations: for DeepSeek-V2, (ğ‘ğ‘”,ğ‘’ğ‘”) = (3, 5),
and for Qwen3-MoE, (ğ‘ğ‘”,ğ‘’ğ‘”) = (4, 4) on Testbeds A, B, and C.
For Testbed D, we set (ğ‘ğ‘”,ğ‘’ğ‘”) = (8, 24) for both DeepSeek-V2
and Qwen3-MoE. Two scenarios highlight differences in the mean
number of arriving tokens. Table 6 shows that our FinDEP, using
the fast solver in Algorithm 1, outperforms the static schedule with
the best PPPipe configuration at a sequence length of 2048. By
adjusting ğ‘Ÿ2 and ğ‘Ÿ1, we improve the throughput up to 1.20Ã—.


--- Page 11 ---
FinDEP
Table 5: Average iteration throughput (tokens per second) comparison, where each number is the average of 3 independent runs.
The values in brackets represent the speedups achieved by FinDEP compared to PPPipe with optimal ğ‘’ğ‘, ğ‘‘ğ‘, ğ‘šğ‘, and ğ‘Ÿ1 settings.
Testbed A
Testbed B
Testbed C
Testbed D
Backbone
ğ‘º
PPPipe
FinDEP
PPPipe
FinDEP
PPPipe
FinDEP
PPPipe
FinDEP
DeepSeek
1024
48.50
53.40 (1.10Ã—)
86.70
93.04 (1.07Ã—)
62.31
63.35 (1.02Ã—)
149.58
161.50 (1.08Ã—)
2048
46.28
50.27 (1.09Ã—)
81.99
86.63 (1.06Ã—)
56.63
58.14 (1.03Ã—)
134.42
150.82 (1.12Ã—)
4096
44.21
51.47 (1.16Ã—)
81.04
85.84 (1.06Ã—)
49.80
54.73 (1.10Ã—)
120.83
132.07 (1.10Ã—)
Qwen
1024
13.94
15.81 (1.13Ã—)
31.52
35.09 (1.11Ã—)
35.70
36.86 (1.03Ã—)
94.97
102.60 (1.08Ã—)
2048
14.00
15.85 (1.20Ã—)
25.46
27.39 (1.08Ã—)
32.78
33.50 (1.02Ã—)
83.12
90.15(1.08Ã—)
4096
13.80
15.55 (1.13Ã—)
22.48
27.64 (1.23Ã—)
28.01
30.06 (1.07Ã—)
61.59
76.53 (1.24Ã—)
8192
8.57
13.14 (1.53Ã—)
15.98
25.71 (1.61Ã—)
20.14
27.12 (1.35Ã—)
37.19
45.26 (1.22Ã—)
Table 6: Average iteration throughput (tokens per second) comparison. The values in brackets represent the speedups achieved
by our FinDEP compared to PPPipe with given ğ‘’ğ‘”, ğ‘ğ‘”settings.
Testbed A
Testbed B
Testbed C
Testbed D
Backbone
Tokens
PPPipe
FinDEP
PPPipe
FinDEP
PPPipe
FinDEP
PPPipe
FinDEP
DeepSeek
3072
28.23
29.34 (1.04Ã—)
44.66
50.13 (1.12Ã—)
30.24
31.13 (1.03Ã—)
98.80
121.07 (1.23Ã—)
6144
41.88
44.25 (1.06Ã—)
69.64
75.99 (1.09Ã—)
36.67
38.13 (1.04Ã—)
124.69
142.36 (1.14Ã—)
Qwen
3072
9.14
10.95 (1.20Ã—)
16.24
18.56 (1.14Ã—)
19.15
19.16 (1.00Ã—)
40.94
50.71 (1.24Ã—)
6144
13.54
15.28 (1.13Ã—)
22.71
30.43 (1.09Ã—)
30.19
30.43 (1.01Ã—)
67.07
78.69 (1.17Ã—)
Table 7: Non-overlapped communication time for naive
DEP (Naive-DEP) without pipelining, PPPipe, and FinDEP in
DeepSeek-V2 on testbed A.
ğ‘º
Naive-DEP
PPPipe
FinDEP
4096
905.49ms
528.94ms
309.81ms
2048
536.22ms
144.32ms
52.60ms
1024
194.95ms
188.65ms
97.33ms
Discussion. In the configuration utilizing the Qwen backbone
on Testbed C, we observe that FinDEP does not achieve significant
performance gains over PPPipe. As indicated in Table 6, FinDEP
attains only 1.0Ã— to 1.1Ã— the throughput of PPPipe under iden-
tical (ğ‘ğ‘”,ğ‘’ğ‘”) settings. This result aligns with the expectations of
Amdahlâ€™s Law, which bounds the maximum speedup achievable
by optimizing only a portion of the system. Specifically, the high-
bandwidth NVLink interconnect on the H20 GPUs shown in Table 2
renders communication time a comparatively minor component
of total runtime. Consequently, further optimization of the execu-
tion schedule yields diminishing returns, as the system is primarily
constrained by other computational factors.
In contrast, performance improves substantially on Testbed D,
where communication and computation overheads are more bal-
anced. Communication overhead increases relative to Testbed C,
while per-GPU computation time decreases because experts are dis-
tributed across more GPUs. With this improved balance, FinDEPâ€™s
throughput increases by up to 1.24Ã— compared to PPPipe. However,
at an extremely large scale, communication would again domi-
nate end-to-end execution time. In that scenario, the relative im-
provement from schedule optimization would diminish because the
proportion of time spent on non-accelerated components would
increase once more.
6
Related Work
Distributed deep learning systems enhance the inference perfor-
mance of MoE Large Language Models (LLMs) primarily through a
triple strategy: one approach involves offloading and optimizing
computation, where large model components or tasks are moved to
the CPU, as demonstrated in works like [7, 9], or through dedicated
computational optimizations such as those found in [2, 13, 18], ef-
fectively addressing critical GPU memory constraints and boosting
overall throughput. A second, parallel strategy focuses on min-
imizing expert decoding latency by identifying and duplicating
frequently utilized "hot" experts across different resources, a tech-
nique leveraged by systems like [6, 8, 19] to ensure quicker access
and processing for high-demand experts. Finally, the third key
method employs model quantization [6, 26, 29], which significantly
reduces the data precision of the model weights often down to 4
bit or 8 bit, thereby shrinking the required communication volume
between devices at the cost of a minor, acceptable trade-off in model
accuracy or performance, ultimately yielding substantial gains in
network efficiency.
Disaggregation is commonly used in LLM serving architectures
to optimize inference performance in key ways [3, 11, 14, 33, 35].
For example, DistServe [35] disaggregates prefill and decode com-
putations onto separate GPUs, boosting parallelism and improving


--- Page 12 ---
Xinglin Pan1, Shaohuai Shi2, Wenxiang Lin2, Yuxin Wang3, Zhenheng Tang4, Wei Wang4, Xiaowen Chu1,4
resource allocation for better performance. Building on this, recent
works have pushed for physical disaggregation. Mooncake [21] uti-
lizes a disaggregated architecture that separates the KVCache pool
from the inference engines, leveraging high-speed interconnects to
enable stateless inference workers.
7
Conclusion
In this paper, we propose FinDEP, a fine-grained task scheduling
framework designed to optimize MoE inference under disaggre-
gated expert parallelism. By partitioning computation and commu-
nication into smaller tasks and formulating a formal optimization
problem, FinDEP maximizes task overlap and resource utilization.
We evaluate FinDEP across four GPU testbeds, including a large-
scale 32-GPU system, using representative MoE backbones such
as DeepSeek-V2 and Qwen3-MoE. Experimental results demon-
strate that FinDEP achieves significant performance gains, pro-
viding speedups of up to 1.61Ã— over the best-configured PPPipe
algorithm. Notably, on the 32-GPU system, FinDEP still delivers a
robust speedup of up to 1.24Ã— in offline scenarios. Furthermore, our
solver derives near-optimal configurations in under one second,
enabling FinDEP to adapt in real-time to dynamic workloads.
References
[1] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, and Alessandro Cap-
pelli et al. 2023. The Falcon Series of Open Language Models. arXiv preprint
arXiv:2311.16867 (2023).
[2] Shiyi Cao, Shu Liu, Tyler Griggs, Peter Schafhalter, Xiaoxuan Liu, Ying Sheng,
Joseph E. Gonzalez, Matei Zaharia, and Ion Stoica. 2025. MoE-Lightning: High-
Throughput MoE Inference on Memory-constrained GPUs. In ASPLOS (1). ACM,
715â€“730.
[3] Shiyang Chen, Rain Jiang, Dezhi Yu, Jinlai Xu, Mengyuan Chao, Fanlong Meng,
Chenyu Jiang, Wei Xu, and Hang Liu. 2024. KVDirect: Distributed Disaggregated
LLM Inference. arXiv preprint arXiv:2501.14743 (2024).
[4] Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen,
Jiashi Li, Wangding Zeng, Xingkai Yu, Yu Wu, et al. 2024. Deepseekmoe: Towards
ultimate expert specialization in mixture-of-experts language models. arXiv
preprint arXiv:2401.06066 (2024).
[5] DeepSeek-AI. 2024. DeepSeek-V2: A Strong, Economical, and Efficient Mixture-
of-Experts Language Model. arXiv preprint arXiv:2405.04434 (2024).
[6] DeepSeek-AI. 2025.
DeepSeek-V3 Technical Report.
arXiv preprint
arXiv:2412.19437 (2025).
[7] Artyom Eliseev and Denis Mazur. 2023. Fast Inference of Mixture-of-Experts
Language Models with Offloading. arXiv preprint arXiv:2312.17238 (2023).
[8] Jiaao He, Jidong Zhai, Tiago Antunes, Haojie Wang, Fuwen Luo, Shangfeng Shi,
and Qin Li. 2022. FasterMoE: modeling and optimizing training of large-scale
dynamic pre-trained models. In PPoPP. ACM, 120â€“134.
[9] Xin He, Shunkang Zhang, Yuxin Wang, Haiyan Yin, Zihao Zeng, Shaohuai Shi,
Zhenheng Tang, Xiaowen Chu, Ivor Tsang, and Ong Yew Soon. 2024. ExpertFlow:
Optimized Expert Activation and Token Allocation for Efficient Mixture-of-
Experts Inference. arXiv preprint arXiv:2410.17954 (2024).
[10] Wentao Hou, Jie Zhang, Zeke Wang, and Ming Liu. 2024. Understanding Routable
PCIe Performance for Composable Infrastructures. In NSDI. USENIX Association,
297â€“312.
[11] Cunchen Hu, Heyang Huang, Liangliang Xu, Xusheng Chen, Jiang Xu, Shuang
Chen, Hao Feng, Chenxi Wang, Sa Wang, Yungang Bao, Ninghui Sun, and Yizhou
Shan. 2024. Inference without Interference: Disaggregate LLM Inference for
Mixed Downstream Workloads. arXiv preprint arXiv:2401.11181 (2024).
[12] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou
Hanna, Florian Bressand, et al. 2024.
Mixtral of Experts.
arXiv preprint
arXiv:2401.04088 (2024).
[13] Keisuke Kamahori, Tian Tang, Yile Gu, Kan Zhu, and Baris Kasikci. 2025. Fiddler:
CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models. In
ICLR. OpenReview.net.
[14] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,
Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient
Memory Management for Large Language Model Serving with PagedAttention.
In SOSP. ACM, 611â€“626.
[15] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat,
Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2021. GShard:
Scaling Giant Models with Conditional Computation and Automatic Sharding.
In ICLR. OpenReview.net.
[16] Ao Li, Bojian Zheng, Gennady Pekhimenko, and Fan Long. 2022. Automatic
Horizontal Fusion for GPU Kernels. In CGO. IEEE, 14â€“27.
[17] Yunkai Liang, Zhangyu Chen, Pengfei Zuo, Zhi Zhou, Xu Chen, and Zhou Yu.
2025. Injecting Adrenaline into LLM Serving: Boosting Resource Utilization
and Throughput via Attention Disaggregation. arXiv preprint arXiv:2503.20552
(2025).
[18] Wenxiang Lin, Xinglin Pan, Shaohuai Shi, Xuan Wang, and Xiaowen Chu. 2024.
Task Scheduling for Efficient Inference of Large Language Models on Single
Moderate GPU Systems. arXiv preprint arXiv:2411.15715 (2024).
[19] Xiaonan Nie, Xupeng Miao, Zilong Wang, Zichao Yang, Jilong Xue, Lingxiao Ma,
Gang Cao, and Bin Cui. 2023. FlexMoE: Scaling Large-scale Sparse Pre-trained
Model Training via Dynamic Device Placement. Proc. ACM Manag. Data 1, 1
(2023), 110:1â€“110:19.
[20] Xinglin Pan, Wenxiang Lin, Lin Zhang, Shaohuai Shi, Zhenheng Tang, Rui Wang,
Bo Li, and Xiaowen Chu. 2025. FSMoE: A Flexible and Scalable Training System
for Sparse Mixture-of-Experts Models. In ASPLOS (1). ACM, 524â€“539.
[21] Ruoyu Qin, Zheming Li, Weiran He, Jialei Cui, Feng Ren, Mingxing Zhang,
Yongwei Wu, Weimin Zheng, and Xinran Xu. 2025. Mooncake: Trading More
Storage for Less Computation â€” A KVCache-centric Architecture for Serving
LLM Chatbot. In 23rd USENIX Conference on File and Storage Technologies (FAST
25). USENIX Association, Santa Clara, CA, 155â€“170. https://www.usenix.org/
conference/fast25/presentation/qin
[22] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani
Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. 2022. DeepSpeed-
MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-
Generation AI Scale. In ICML (Proceedings of Machine Learning Research, Vol. 162).
PMLR, 18332â€“18346.
[23] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. 2018. Searching for Activation
Functions. In ICLR (Workshop). OpenReview.net.
[24] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le,
Geoffrey E. Hinton, and Jeff Dean. 2017. Outrageously Large Neural Networks:
The Sparsely-Gated Mixture-of-Experts Layer. In ICLR. OpenReview.net.
[25] Shaohuai Shi, Xinglin Pan, Xiaowen Chu, and Bo Li. 2023. PipeMoE: Accelerating
Mixture-of-Experts through Adaptive Pipelining. In INFOCOM. IEEE, 1â€“10.
[26] Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. 2024. PowerInfer: Fast
Large Language Model Serving with a Consumer-grade GPU. In SOSP. ACM,
590â€“606.
[27] Llama Team. 2024. The Llama 3 Herd of Models. arXiv preprint arXiv:2407.21783
(2024).
[28] Qwen Team. 2025. Qwen3 Technical Report. arXiv preprint arXiv:2505.09388
(2025).
[29] StepFun Team. 2025. Step-3 is Large yet Affordable: Model-system Co-design
for Cost-effective Decoding. arXiv preprint arXiv:2507.19427 (2025).
[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In NIPS. 5998â€“6008.
[31] Lean Wang, Huazuo Gao, Chenggang Zhao, Xu Sun, and Damai Dai. 2024.
Auxiliary-loss-free load balancing strategy for mixture-of-experts. arXiv preprint
arXiv:2408.15664 (2024).
[32] Zihao Ye, Lequn Chen, Ruihang Lai, and Wuwei Lin et al. 2025. FlashInfer:
Efficient and Customizable Attention Engine for LLM Inference Serving. arXiv
preprint arXiv:2501.01005 (2025).
[33] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang,
Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez,
Clark W. Barrett, and Ying Sheng. 2024. SGLang: Efficient Execution of Structured
Language Model Programs. In NeurIPS.
[34] Shuzhang Zhong, Ling Liang, Yuan Wang, Runsheng Wang, Ru Huang, and Meng
Li. 2024. AdapMoE: Adaptive sensitivity-based expert gating and management
for efficient moe inference. In Proceedings of the 43rd IEEE/ACM International
Conference on Computer-Aided Design. 1â€“9.
[35] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu,
Xin Jin, and Hao Zhang. 2024. DistServe: Disaggregating Prefill and Decod-
ing for Goodput-optimized Large Language Model Serving. In OSDI. USENIX
Association, 193â€“210.
[36] Ruidong Zhu, Ziheng Jiang, Chao Jin, and Peng Wu et al. 2025. MegaScale-Infer:
Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism. arXiv
preprint arXiv:2504.02263 (2025).
[37] Pengfei Zuo, Huimin Lin, Junbo Deng, and Nan Zou et al. 2025. Serving Large
Language Models on Huawei CloudMatrix384. arXiv preprint arXiv:2506.12708
(2025).
