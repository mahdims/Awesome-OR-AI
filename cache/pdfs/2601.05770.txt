--- Page 1 ---
Weights to Code: Extracting Interpretable Algorithms from
the Discrete Transformer
Yifan Zhang1,2*, Wei Bi3, Kechi Zhang1,2, Dongming Jin1,2, Jie Fu4,5‚Ä†, Zhi Jin1,2‚Ä†
1Key Laboratory of High Confidence Software Technology (PKU), MOE, China
2School of Computer Science, Peking University, China
3Kuaishou Technology, 4Shanghai AI Lab, 5Shanghai Innovation Institute
yifanzhang@stu.pku.edu.cn, fujie@pjlab.org.cn, zhijin@pku.edu.cn
Abstract
Algorithm extraction aims to synthesize exe-
cutable programs directly from models trained
on specific algorithmic tasks, enabling de
novo algorithm discovery without relying on
human-written code. However, extending this
paradigm to Transformer is hindered by super-
position, where entangled features encoded in
overlapping directions obstruct the extraction
of symbolic expressions. In this work, we pro-
pose the Discrete Transformer, an architecture
explicitly engineered to bridge the gap between
continuous representations and discrete sym-
bolic logic. By enforcing a strict functional
disentanglement, which constrains Numerical
Attention to information routing and Numeri-
cal MLP to element-wise arithmetic, and em-
ploying temperature-annealed sampling, our
method effectively facilitates the extraction of
human-readable programs. Empirically, the
Discrete Transformer not only achieves perfor-
mance comparable to RNN-based baselines but
crucially extends interpretability to continuous
variable domains. Moreover, our analysis of
the annealing process shows that the efficient
discrete search undergoes a clear phase transi-
tion from exploration to exploitation. We fur-
ther demonstrate that our method enables fine-
grained control over synthesized programs by
imposing inductive biases. Collectively, these
findings establish the Discrete Transformer as
a robust framework for demonstration-free al-
gorithm discovery, offering a rigorous pathway
toward Transformer interpretability.
1
Introduction
Program synthesis is the task to construct a pro-
gram that provably satisfies a given high-level for-
mal specification, a line of work dating back to
Church (1963).
In recent years, this field has
been revolutionized by Large Language Models
(LLMs), which have achieved remarkable success
*Work done during an internship at Kuaishou Technology.
‚Ä†Corresponding authors.
in code generation (Rozi√®re et al., 2024; Guo et al.,
2024; Team et al., 2024). Despite their success,
an alternative paradigm‚Äîalgorithm extraction‚Äî
offers a distinct advantage: the ability to derive
algorithms de novo, thereby potentially uncover-
ing innovative solutions independent of human-
generated data (Michaud et al., 2024). Recent work
has demonstrated the feasibility of this direction
for Recurrent Neural Networks (RNNs) via the
Mechanistic-Interpretability-based Program Syn-
thesis (MIPS), which leverages symbolic regres-
sion to synthesize programs matching network be-
havior (Michaud et al., 2024).
However, extending algorithm extraction to the
dominant Transformer architecture faces signifi-
cant challenges, rooted in a fundamental gap be-
tween the continuous, high-dimensional nature of
Transformer and the discrete, sparse nature of sym-
bolic algorithms. The primary obstacle lies in inter-
preting the Transformer‚Äôs internal representations.
Specifically, the standard Transformer often ex-
hibits ‚Äúsuperposition‚Äù, where features are encoded
in an overlapping, non-orthogonal set of direc-
tions rather than individual neurons (Cunningham
et al., 2023; Elhage et al., 2022). Consequently,
the model‚Äôs internal representations are highly en-
tangled and polysemantic. Unlike the disentangled
input-output mappings required for symbolic re-
gression, these representations render the direct ex-
traction of explicit symbolic expressions infeasible.
This motivates our central question: Is it possible to
synthesize executable and interpretable programs
by extracting algorithms from Transformer?
In this work, we propose the Discrete Trans-
former, an architecture explicitly optimized for al-
gorithm extraction (see Figure 1). Building upon
the framework of Transformer Programs (Fried-
man et al., 2023), our design incorporates critical
modifications to facilitate the transition from con-
tinuous dynamics to discrete logic. Architecturally,
the Discrete Transformer comprises Numerical At-
arXiv:2601.05770v1  [cs.LG]  9 Jan 2026


--- Page 2 ---
I. Discrete Search 
II. Algorithm Extraction 
III. Synthesized Program
Numerical
Attention
Numerical
MLP
Temperature Annealing
Linear Output Head
Python Code
Hypothesis Testing
Symbolic Regression 
par ‚Üê cur ‚äïprev
ùëùùëüùëíùë£!‚Üêùëêùë¢ùëü!"#
def parity_last2(S):
cur = S
prev = np.zeros_like(cur)
prev[1:] = cur[:-1]
par = cur + prev - 2 ‚àócur ‚àóprev
output = par
return output
Figure 1: Illustration of the proposed framework for extracting executable algorithms from a Discrete Transformer.
(I) Discrete Search: Temperature annealing is leveraged to encourage interpretable discretization in both Numerical
Attention and MLP modules. (II) Algorithm Extraction: Attention patterns are characterized via hypothesis testing
(e.g., identifying token shifts), while arithmetic transformations are approximated through symbolic regression. (III)
Synthesized Program: The extracted components are integrated via a linear output head to generate Python code. As
shown, the framework successfully recovers the parity_last2 algorithm, correctly implementing the arithmetic
XOR logic.
tention, Numerical MLP, and a linear output head.
Aligning with the Restricted Access Sequence Pro-
cessing (RASP) computational model (Weiss et al.,
2021), we impose a strict functional disentangle-
ment, where Numerical Attention is responsible for
routing information across positions, while the Nu-
merical MLP is dedicated solely to element-wise
arithmetic operations. Crucially, we incorporate
differentiable sampling mechanisms into each mod-
ule to inject temperature-controlled discreteness.
Through annealing, the model gradually transitions
into a fully discrete representation, ensuring that
the solution to the algorithmic task is implicitly but
clearly encoded within its weights.
Once the model converges to a discrete state, we
employ a modular strategy to recover the underly-
ing algorithm. Recognizing the distinct roles of the
components, we adopt hypothesis testing for the
Attention modules to identify interpretable routing
patterns, and apply symbolic regression to the MLP
modules to infer the specific arithmetic expressions.
Finally, these extracted primitives are aggregated
through the linear output head, yielding a concise,
human-readable, and executable program that veri-
fiably solves the target task.
We validate our approach on a diverse suite
of algorithmic tasks, including the MIPS bench-
mark (Michaud et al., 2024). Crucially, unlike prior
RNN-based methods restricted to discrete data do-
mains, our Discrete Transformer natively processes
continuous variables, substantially broadening the
scope of mechanistic-interpretability-based pro-
gram synthesis. Beyond performance, we provide
a rigorous analysis of the framework‚Äôs theoretical
and empirical properties. In addition to revealing
that functional convergence precedes full structural
discretization (Louizos et al., 2018; Savarese et al.,
2021), we demonstrate that tailoring architectural
constraints imposes strong inductive biases, estab-
lishing the Discrete Transformer as a controllable
framework for interpretable algorithm discovery.
Overall, our main contributions are as follows:
‚Ä¢ We propose the Discrete Transformer, a RASP-
aligned architecture that enforces functional disen-
tanglement to bridge continuous optimization and
discrete logic.
‚Ä¢ We develop a modular extraction pipeline com-
bining hypothesis testing and symbolic regression,
extending mechanistic-interpretability-based syn-
thesis to continuous variables.
‚Ä¢ We provide rigorous analysis validating the ef-
fectiveness of imposed inductive biases for control-
lable algorithm discovery.
2
Related Work
2.1
Mechanistic Interpretability in
Transformer
Foundational to connecting Transformer with pro-
grams is the RASP (Weiss et al., 2021), which ab-
stracts sequence processing into primitives. While
Tracr (Lindner et al., 2023) compiles programs
into weights, Transformer Programs (Friedman
et al., 2023) address the inverse problem, utiliz-
ing Gumbel-Softmax (Jang et al., 2017) to learn
discrete structures, which can be deterministically
mapped to programs. However, we argue that such


--- Page 3 ---
direct translation merely simulates low-level op-
erations on test cases; true algorithm extraction
requires explicit symbolic reasoning to distill ab-
stract, high-level algorithm logic.
2.2
Symbolic Regression and Algorithm
Extraction
Symbolic regression searches for closed-form
expressions balancing accuracy and complex-
ity (Udrescu et al., 2020; Cranmer, 2023). While
traditionally used for scientific discovery (Cranmer
et al., 2020; Ma et al., 2022) or recently enhanced
by LLMs‚Äô scientific priors (Shojaee et al., 2025),
we leverage it as an engine for algorithm extrac-
tion. Here, the objective shifts from data fitting
to synthesizing concise, human-readable programs
directly from trained neural networks.
The innovative MIPS framework (Michaud et al.,
2024) pioneers this approach for RNNs. Analo-
gous to the use of Sparse Autoencoders (SAEs)
in interpreting features (Cunningham et al., 2023;
Bricken et al., 2023), the MIPS employs integer au-
toencoders to approximate continuous latent states
into finite states suitable for symbolic regression.
In contrast to the MIPS, which relies on auxil-
iary quantization modules to approximate discrete-
ness post-hoc, our Discrete Transformer is archi-
tecturally designed to learn interpretable, discrete
representations directly, enabling direct and seam-
less algorithm extraction.
3
Discrete Transformer
In this section, we introduce the Discrete Trans-
former, a specialized architecture purposefully de-
signed for symbolic regression and algorithm ex-
traction. It is structured as a computational graph
with clear functional specialization: Numerical At-
tention performs explicit variable routing, while
the Numerical MLP is responsible for arithmetic
computation.
3.1
Numerical Residual Stream
The essential difference between the Discrete
Transformer and the standard one lies in the struc-
ture of the residual stream and the mechanism of
information processing. To align with the sym-
bolic nature of algorithm extraction, we define
the residual stream not as latent vectors, but as
a collection of explicit scalar variables. Let hl =
[x1, . . . , xNl]‚ä§‚ààRNl denote the residual stream
at layer l, consisting of Nl distinct scalar variables.
In contrast to standard additive updates, we adapt
the concatenation principle (Friedman et al., 2023;
Lai-Dang et al., 2025) to the scalar domain, updat-
ing the residual stream with new outputs ol:
hl+1 = Concat(hl, ol) ‚ààRNl+|ol|.
(1)
This design preserves the full computational his-
tory as disentangled coordinates, alleviating the
information interference induced by superposition
in dense vectors.
Moreover, a critical challenge in algorithm
extraction is to identify which variables serve
as operands. Inspired by learnable input selec-
tion (Friedman et al., 2023), we address this by
designing a discretized reading mechanism tailored
for the numerical residual stream: for any compu-
tational module (Numerical Attention or MLP) at
layer l requiring k inputs, we learn a projection
matrix Wread ‚ààRk√óNl to select inputs from the
residual stream hl. To enable discrete structure
optimization within a differentiable framework, we
apply a temperature-controlled sampling function
S(¬∑, œÑ) (derivation provided in Appendix A) row-
wise to the logits Wread. The input vector u ‚ààRk
is then retrieved from the current residual stream
hl ‚ààRNl via:
u = S(Wread, œÑ) ¬∑ hl,
(2)
where œÑ is the annealing temperature. As tempera-
ture œÑ ‚Üí0, the selection distribution converges to
a deterministic one-hot indicator, effectively func-
tioning as a differentiable pointer over the compu-
tation graph.
3.2
Numerical Attention as Router
Following the architectural paradigm established in
Tracr (Lindner et al., 2023) and Transformer Pro-
grams (Friedman et al., 2023), we employ a hard
attention mechanism designed strictly as an infor-
mation routing operator, rather than a feature mixer.
In the context of algorithm extraction, this design
can help isolate all computational transformations
within simpler MLP, thereby reducing the difficulty
of attention analysis.
Piecewise Linear Encoding. Since the residual
stream consists of raw scalars (hl ‚ààRNl), distinct
scalar variables lack the high-dimensional expres-
sivity required for effective dot-product compar-
isons. To address this, we define the query and
key as selected scalars xq, xk ‚ààR, and project
them into a higher-dimensional space using the


--- Page 4 ---
Piecewise Linear Encoding (PLE) (Gorishniy et al.,
2022):
q = œïPLE(xq),
k = œïPLE(xk),
(3)
where q, k ‚ààRdattn and œïPLE : R ‚ÜíRdattn is a
learnable mapping.
Deterministic Attention Mechanism. The atten-
tion scores araw ‚ààRN over a context length N
are computed via scaled dot-product with T5-style
relative positional biases brel (Raffel et al., 2023):
araw =
Kq
‚àödattn
+ brel,
(4)
where K ‚ààRN√ódattn stacks the projected keys
from the context. To ensure interpretable attention
patterns, we enforce hard attention using the sam-
pling function S(¬∑). Crucially, the value projection
is the identity function for scalars, meaning that the
value vector v ‚ààRN consists directly of the raw
scalars from the history. The output oattn ‚ààR is a
weighted sum:
oattn = S(araw, œÑ) ¬∑ v.
(5)
This design forces the attention head to converge
to a deterministic pointer operation, retrieving spe-
cific raw values from history (e.g., ‚Äúcopy the value
from the token at offset -1‚Äù).
3.3
Numerical MLP as Operator
While the Attention handles data movement, the
Numerical MLP is dedicated to element-wise arith-
metic and logical transformations. Drawing on the
insight that Transformer MLPs contribute additive
updates to the residual stream, which can be decom-
posed into weighted sums of sub-updates (Geva
et al., 2022), we explicitly decompose the MLP
module into a collection of parallel, independent
sub-modules (sub-MLPs).
Each sub-module performs a single elementary
operation. It first selects a small, fixed number
of scalars (typically k = 2) from the stream via
the discretized reading mechanism, forming an
operand vector u ‚ààRk. These operands are pro-
cessed by a shallow network:
o = W2(œÉ(W1u + b1)) + b2,
(6)
where W1 ‚ààRdhid√ók, W2 ‚ààR1√ódhid are learn-
able weights, b1, b2 are biases, and œÉ is a non-linear
activation (e.g., ReLU). The resulting scalar output
o is concatenated back to the residual stream.
Inductive Bias for Symbolic Regression.
By
severely constraining the input dimension k and
the hidden width dhid, we deliberately limit the
complexity of each sub-module. This bottleneck
forces the network to decompose complex func-
tions into a sequence of simple arithmetic steps,
creating ideal conditions for symbolic regression
(e.g., PySR (Cranmer, 2023)) to extract closed-
form expressions.
3.4
Linear Output Head
The architecture concludes with a linear output
head that aggregates the discrete computational
steps into a final prediction:
ÀÜy = w‚ä§
outhfinal,
(7)
where hfinal is the final residual stream, and wout
represents the aggregation weights. To reduce the
length of the extracted program, we impose a spar-
sity threshold œµ on the magnitude of the weights
(i.e., set wi = 0 if |wi| < œµ). This prunes unneces-
sary intermediate variables from the computation
graph, retaining only the paths essential for the task.
The final trained model thus represents a clean, ex-
ecutable computational graph: nodes correspond to
either routing operations (Attention) or arithmetic
functions (MLP), and edges are defined by the dis-
crete selections of the discretized reading modules.
4
From Weights to Code
After training with temperature annealing, the con-
verged Discrete Transformer reaches a fully dis-
crete state, and the algorithmic solution is implic-
itly encoded within its sparse weights and acti-
vation patterns. To recover an explicit, human-
readable program, we treat the trained model as
a computational graph composed of two distinct
types of nodes: routers (Attention) and operators
(MLP). We propose a decoupled extraction pipeline
that first infers the function of each node using
component-specific strategies, then reconstructs the
global program trace via backward traversal.
4.1
Hypothesis Testing for Attention
For the Numerical Attention module, extracting
explicit expressions via direct symbolic regression
is challenging due to the computational complexity
in the PLE of queries and keys, dot-product interac-
tions, and the hard attention mechanism induced by
sampling functions. However, since the structural
role of attention is constrained to be an informa-
tion router, we abstract away from the intermediate


--- Page 5 ---
arithmetic and focus our interpretability analysis
on the resulting routing patterns.
We conceptualize the Numerical Attention mod-
ule as a deterministic pointer performing differ-
entiable addressing on context memory.
Fol-
lowing Neural Turing Machines (Graves et al.,
2014), we categorize addressing into Location-
based (position-dependent) and Content-based
(value-dependent). We hypothesize that our atten-
tion heads specialize into these two modes, and em-
pirically observe that they manifest as two typical
interpretable patterns: Fixed Offset and Windowed
Extrema.
Specifically, for a given head, we analyze its
attention matrices generated over the validation set.
We test the hypotheses by examining the statistical
properties across the dataset:
‚Ä¢ Fixed Offset. This pattern corresponds to relative
positional indexing. We hypothesize that there ex-
ists an integer offset Œ¥ such that, for a large fraction
of query positions i, the head places most of its at-
tention on j = i‚àíŒ¥. We operationalize this by mea-
suring whether the averaged attention mass concen-
trates on a single offset diagonal (corresponding to
j = i ‚àíŒ¥) beyond a predefined threshold.
‚Ä¢ Windowed Extrema. This pattern reflects content-
dependent selection. Let v ‚ààRN be the sequence
of scalar values, where vj is the value at posi-
tion j. We hypothesize that there exists a win-
dow size z ‚ààZ+ such that, for a large fraction
of query positions i, the head attends primarily
to j‚àó= arg min/maxj‚àà{i‚àíz+1,...,i} vj. We verify
this hypothesis by checking the sample-wise agree-
ment between the head‚Äôs selected index and the
true extremum index within the window.
Across the benchmark investigated, we find these
simple routing patterns achieve high coverage of ac-
tive heads. The few ‚Äúunmatched heads‚Äù represent
computational noise‚Äîbeing negligible in magni-
tude or acting as redundant variables‚Äîand do not
influence the logic of the final assembled program
(see Appendix B for detailed analysis).
4.2
Symbolic Regression for MLP
In contrast to the Attention, the Numerical MLP
modules serve as the arithmetic core. Thanks to the
disentangled architecture, each MLP sub-module
functions as an isolated mapping f : Rk ‚ÜíR with
low-dimensional inputs (typically k = 2). This
architectural isolation makes them ideal candidates
for black-box symbolic regression.
For each sub-MLP, we collect a dataset of input-
output pairs D = {(u(i), o(i))}M
i=1 from validation
passes. We employ the PySR (Cranmer, 2023),
a symbolic regression tool based on genetic algo-
rithms, to search for the optimal symbolic expres-
sion ÀÜf that minimizes the error on D. Crucially, the
constrained input dimension and limited model ca-
pacity drastically reduce the search space, enabling
the PySR to reliably converge to exact arithmetic
expressions (e.g., o = 2u1 + u2) rather than ap-
proximate fits.
4.3
Global Program Assembly
The final phase integrates these extracted primi-
tives into a coherent program. The linear output
head, ÀÜy = w‚ä§
outhfinal, serves as the entry point for
extraction. We first apply magnitude-based pruning
to wout (|wi| > œµ) to identify the active variables
contributing to the final prediction.
Starting from these active variables, we perform
a backward traversal of the computational graph.
Recursively, we replace each intermediate variable
in the residual stream with its corresponding sym-
bolic definition, either a deterministic pointer from
Attention or a distilled arithmetic expression from
MLP, until the traversal reaches the raw input to-
kens. This process effectively compiles the neu-
ral network into concise, closed-form algorithmic
expressions that approximate the Discrete Trans-
former‚Äôs behavior with high fidelity across the rele-
vant input domain.
5
Experiments
In this section, we evaluate the Discrete Trans-
former on a diverse suite of algorithmic reasoning
tasks. We aim to demonstrate that, beyond achiev-
ing high performance, our model extracts inter-
pretable and human-readable algorithms, thereby
revealing the underlying structure and logic inher-
ent in the data.
Datasets
We
utilize
the
MIPS
benchmark
(Michaud et al., 2024) to probe capabilities ranging
from local computation to global state tracking1
The tasks are categorized into three groups: (1)
Linear Arithmetic (e.g., sum_last2, sum_last);
(2) Non-Linear Composition (e.g., parity_last2,
add_mod_3) which requires approximating non-
linear logic; and (3) Dynamical Systems (e.g.,
gravity) to evaluate the discovery of physical
1State tracking refers to the maintenance and updating of
an explicitly updated state across the input sequence (e.g., a
cumulative sum). (Zhang et al., 2025), as opposed to purely
local computations.


--- Page 6 ---
Table 1: Performance of the Discrete Transformer on
the algorithm benchmark. For these regression tasks,
loss is reported as MSE on the test set. The model
demonstrates strong predictive performance across the
benchmark, with loss values close to zero.
Task
Loss
Task
Loss
Linear Arithmetic
sum_last2
6.76 √ó 10‚àí9
diff_last2
7.02 √ó 10‚àí16
sum
1.70 √ó 10‚àí14
Non-Linear Composition
parity_last2
1.46 √ó 10‚àí12
max_prev2
4.43 √ó 10‚àí6
min_prev2
1.77 √ó 10‚àí5
bitwise_and
1.06 √ó 10‚àí18
bitwise_or
1.63 √ó 10‚àí6
bitwise_not
< 10‚àí20
bitwise_xor
6.67 √ó 10‚àí20
abs
5.25 √ó 10‚àí14
abs_of_diff
2.01 √ó 10‚àí8
parity
8.09 √ó 10‚àí17
add_mod_3
1.72 √ó 10‚àí7
Physical Problems
freebody
9.57 √ó 10‚àí11
gravity
1.86 √ó 10‚àí11
spring
3.49 √ó 10‚àí9
laws. Detailed definitions for all tasks are provided
in Appendix C.
Training Details All models employ a decoder-
only architecture implemented in PyTorch, opti-
mized via AdamW to minimize the Mean Squared
Error (MSE). We train for 50 epochs (100 for physi-
cal tasks) with a batch size of 512 and cosine learn-
ing rate decay. Crucially, to handle discrete op-
timization, we apply geometric annealing to the
sampling temperature œÑ, decreasing it from 10.0
to 0.1. We report the average performance across
three random seeds, with hyperparameters (layers,
heads, sub-MLPs) selected via grid search. Full hy-
perparameters and grid search ranges are detailed
in Appendix C.
Results We evaluate the Discrete Transformer
across the diverse task suite. As shown in Table 1,
the Discrete Transformer achieves near-perfect con-
vergence across all task categories, with test losses
approaching zero. Beyond numerical accuracy, our
primary contribution lies in demonstrating that the
solution embedded within the Discrete Transformer
can be successfully extracted into human-readable
code, which matches the capabilities of MIPS on
this benchmark. By applying the methodology de-
tailed in Section 4, we successfully compile the
trained weights into executable Python code. We
highlight three key findings below.
‚Ä¢ Discovery of Algorithmic Parsimony in Linear
Tasks. In linear tasks, the discrete optimization pro-
cess demonstrates a strong inductive bias towards
parsimony. The model frequently learns to bypass
the Numerical MLP entirely, leveraging the linear
output head to perform arithmetic directly. For
instance, in sum_last2 (Figure 2), the model as-
signs specific attention heads to retrieve xt‚àí1 (via
a verified ‚ÄúFixed Offset‚Äù pattern) and integrates
it with the current token xt. Symbolic simplifica-
tion yields the exact expression yt = xt + xt‚àí1,2
effectively capturing the underlying addition logic.
‚Ä¢ Exact Recovery of Non-Linear Identities. For
tasks requiring non-linear logic, the extraction
methodology successfully identifies algebraically
equivalent expressions.
For instance, in the
parity_last2 task (xt ‚äïxt‚àí1) with binary inputs
x ‚àà{0, 1}, simplifying the extracted expressions
(Figure 3) yields yt = xt + xt‚àí1 ‚àí2xtxt‚àí1, which
is an exact algebraic formulation of the parity op-
eration. For maximum_prev2 and minimum_prev2
tasks, simplifying the extracted expressions in Fig-
ure 4 reveals a piecewise-linear implementation
of conditional computation via ReLU operations:
max(xt, xt‚àí1) = xt‚àí1 + ReLU(xt ‚àíxt‚àí1) and
min(xt, xt‚àí1) = xt‚àí1 ‚àíReLU(xt‚àí1 ‚àíxt). This
demonstrates the capability to reverse-engineer
black-box neural computations into explicit alge-
braic forms equivalent to logical operations.
‚Ä¢ Modeling Continuous Dynamics. In physical
tasks such as gravity, the Discrete Transformer
effectively identifies the essential computational
variables and successfully constructs the correct
computation graph through the coordination of the
Numerical Attention, Numerical MLP, and output
head, demonstrating strong potential for model-
ing complex physical processes. Moreover, owing
to its inherent model design, it offers a clear ad-
vantage over prior symbolic synthesis approaches.
Specifically, the MIPS is inherently restricted to
discrete data domains, rendering it inapplicable to
floating-point tasks. In contrast, the Discrete Trans-
former natively supports continuous variables. This
distinction enables the extraction of algorithms that
capture continuous dynamics, substantially broad-
ening the scope of mechanistic-interpretability-
based program synthesis.
6
Further Analysis
In this section, we provide a rigorous analysis of
the Discrete Transformer‚Äôs properties, positioning
it at the intersection of program synthesis and con-
tinuous sparsification. We primarily focus on the
2We employ the SymPy library (Meurer et al., 2017) to
automatically simplify the extracted expressions, leading to
more concise and readable mathematical formulae.


--- Page 7 ---
1
import numpy as np
2
def sum_last2(input_seq):
3
# V0_input
4
input_arr = np.array(input_seq, dtype=float)
5
seq_len = input_arr.shape[0]
6
V0 = input_arr[:, 0]
7
# V1_Attn_L0H0
8
V1 = np.zeros(seq_len)
9
# Fixed offset 1
10
V1[1:] = V0[:‚àí1]
11
# V2_Attn_L0H1
12
V2 = np.zeros(seq_len)
13
# Fixed offset 1
14
V2[1:] = V0[:‚àí1]
15
# Output Head
16
output = 1.00 * V0 + 1.00 * V1 + 0.01 * V2
17
return output
‚áìsymbolic simplification
Simplified Expression:
yt = xt + xt‚àí1
Figure 2:
Algorithm extraction results for the
sum_last2 task. Modules are denoted by their type and
indices (e.g., Attn_L0H0 represents the attention head
at index 0 of layer 0). The extracted code reveals that
the model utilizes specific attention heads to retrieve the
previous token. Symbolic simplification (bottom) shows
the mathematically simplified expression, verifying that
the model correctly learns the logic yt = xt + xt‚àí1.
training dynamics, characterizing a distinct phase
where functional convergence is achieved prior
to complete structural discretization. We further
demonstrate how architectural constraints serve as
strong inductive biases, establishing the model as
a controllable testbed for interpretable algorithm
discovery.
6.1
Continuous-to-Discrete Homotopy
We frame program synthesis as a continuous-to-
discrete homotopic transformation. From the per-
spective of differentiable architecture search and
continuous sparsification (Louizos et al., 2018;
Jang et al., 2017), we relax the discrete constraint
by introducing continuous structural parameters,
specifically the projection matrices Wread. The
temperature œÑ serves as a homotopy parameter:
high values define a search space over the con-
tinuous probability simplex for exploration, while
annealing œÑ ‚Üí0 smoothly deforms the distribution
toward simplex vertices for exploitation.
To validate these dynamics, in addition to
the MSE loss (serving as the soft training ob-
jective Lsoft), we monitor two metrics: Struc-
tural Agreement (A(e)) and Discretization Dis-
1
import numpy as np
2
def parity_last2(input_seq):
3
# V0_input
4
input_arr = np.array(input_seq, dtype=float)
5
seq_len = input_arr.shape[0]
6
V0 = input_arr[:, 0]
7
# V1_Attn_L0H0
8
V1 = np.zeros(seq_len)
9
# Fixed offset 1
10
V1[1:] = V0[:‚àí1]
11
# V2_Attn_L0H1
12
V2 = np.zeros(seq_len)
13
# Fixed offset 1
14
V2[1:] = V0[:‚àí1]
15
# V3_MLP_L0M0
16
V3 = ((V1 + ‚àí0.41) * ((V0 ‚àí0.41) * 3.57)) + ‚àí0.61
17
# Output Head
18
output = ‚àí0.56 * V3 + 0.17 * V0 + 0.10 * V2 + 0.07 *
V1
19
return output
‚áìsymbolic simplification
Simplified Expression:
yt = xt + xt‚àí1 ‚àí2xtxt‚àí1
Figure 3:
Algorithm extraction results for the
parity_last2 task. The extracted code reveals that the
model utilizes specific attention heads (e.g., Attn_L0H0)
to retrieve the previous token, and specific sub-MLPs
(e.g., MLP_L0M0) to perform non-linear transformations.
The bottom panel presents the symbolic simplification
yt = xt + xt‚àí1 ‚àí2xtxt‚àí1, which is the algebraic for-
mulation of the parity logic.
crepancy (‚àÜ(e)). Let {p(e)
r }R
r=1 be the set of all
row-wise probability distributions derived from
S(Wread, œÑ) at epoch e, E the total number of
epochs, and Lhard(e) the hard loss computed via
deterministic argmax sampling. We define A(e) =
1
R
PR
r=1 I

arg max p(e)
r
= arg max p(E)
r

. And
we further define ‚àÜ(e) = Lhard(e) ‚àíLsoft(e).
As shown in Figure 5, we observe a distinct
phase transition: the significant decline in ‚àÜ(e)
occurs slightly later than that of Lsoft, concur-
rently with A(e) approaching 1.0 as annealing
proceeds.
This lag suggests a two-stage pro-
cess where the model first achieves soft con-
vergence‚Äîlearning functional mappings via re-
laxed representations‚Äîbefore undergoing struc-
tural crystallization, thereby ensuring a robust tran-
sition from exploration to exploitation.
6.2
Controllability via Inductive Biases
Unlike standard code LLMs, which synthesize pro-
grams based on opaque statistical patterns, our
Discrete Transformer offers a unique advantage:


--- Page 8 ---
1
import numpy as np
2
def relu(x):
3
return np.maximum(0, x)
4
def extrema_prev2(input_seq, mode):
5
# V0_input
6
input_arr = np.array(input_seq, dtype=float)
7
seq_len = input_arr.shape[0]
8
V0 = input_arr[:, 0]
9
# V1_Attn_L0H0
10
V1 = np.zeros(seq_len)
11
# Fixed offset 1
12
V1[1:] = V0[:‚àí1]
13
# V3_MLP_L0M0, Output Head
14
if mode == 'max': # maximum_prev2
15
V3 = (((V0 * ‚àí0.12) + relu((V1 * ‚àí1.00) + V0)) *
‚àí2.21) + ((V1 * ‚àí0.26) / 1.02)
16
output = 0.88 * V1 + ‚àí0.45 * V3 + 0.12 * V0
17
else: # minimum_prev2
18
V3 = 1.43 * (V0 + (((3.49 * relu(V1 ‚àíV0)) ‚àíV1) ‚àí
(V1 * 1.04)))
19
output = 0.42 * V1 + 0.29 * V0 + ‚àí0.20 * V3
20
return output
‚áìsymbolic simplification
Simplified Expression:
max(xt, xt‚àí1) = xt‚àí1 +
ReLU(xt ‚àíxt‚àí1),
min(xt, xt‚àí1)
=
xt‚àí1 ‚àí
ReLU(xt‚àí1 ‚àíxt)
Figure
4:
Algorithm
extraction
results
for
maximum_prev2 and minimum_prev2 tasks.
The
top panel shows the raw code where sub-MLPs utilize
ReLU functions to compare the current token xt with
the previous token xt‚àí1. The bottom panel presents
the simplified expressions, verifying that the model
correctly reconstructs the extrema functions using the
ReLU-based algebraic identities.
interpretability-aware controllability. In scientific
discovery, researchers often seek not just any so-
lution, but a specific form of algorithm that aligns
with domain knowledge (Schmidt and Lipson,
2009; Udrescu and Tegmark, 2020; Cranmer et al.,
2020). We address this need by explicitly manip-
ulating the architectural constraints and training
configurations of our model to impose inductive
biases, thereby steering the solution space.
We demonstrate this controllability through two
intervention scenarios. First, we manipulate ar-
chitectural primitives. In the maximum_prev2 task,
the unconstrained model typically solves maximiza-
tion via MLP-based arithmetic approximation (ex-
ploiting ReLU non-linearity). To test if the model
can switch algorithmic paradigms, we explicitly
set the number of sub-MLPs to zero, removing
its capacity for complex arithmetic. Consequently,
the model adapts by shifting its entire mechanism
to the Numerical Attention module. It discovers
0.1
1.1
2.1
3.1
4.1
5.0
6.0
7.0
8.0
9.0
10.0
Temperature
0.0
1
2
3
4
5
6
7
8
9
10
3k
10k
15k
Loss/discrepancy
0.0
0.2
0.4
0.6
0.8
1.0
Agree
Spring
Sum2
Diff2
Par2
FB
Loss
Discrepancy
Agree
Figure 5: Training dynamics exhibit a clear phase
transition: the loss decreases earlier, while the pro-
nounced drop in Discrepancy occurs slightly later, coin-
ciding with Agreement approaching 1.0 during tem-
perature annealing from 10.0 to 1.0. The abbrevia-
tions Spring, Sum2, Diff2, Par2, and FB denote the
spring, sum_last2, diff_last2, parity_last2, and
freebody tasks, respectively.
a ‚Äúwindowed max‚Äù attention pattern, solving the
task by directly copying the largest value from the
context rather than computing it. Second, we in-
tervene in the information flow. In the spring
task, the model typically learns the standard re-
currence yi = yi‚àí1 ‚àíyi‚àí2 + xi. By masking
the immediate history (yi‚àí1, yi‚àí2), we guide the
model to bypass the standard path and discover a
mathematically equivalent high-order recurrence:
yi = ‚àíyi‚àí3 + xi + xi‚àí1. These findings high-
light the Discrete Transformer as a robust tool for
intervenable algorithm discovery, capable of uncov-
ering multiple equivalent logical paths underlying
the same data distribution.
7
Conclusions
In this work, we present the Discrete Transformer,
a novel framework that enables program synthe-
sis via algorithm extraction directly from Trans-
former architectures. By enforcing a disentangled
numerical residual stream and employing a smooth
discrete optimization curriculum, our model decou-
ples information routing from arithmetic computa-
tion, facilitating the extraction of concise, human-
readable algorithms.
Overall, our experiments
demonstrate that the Discrete Transformer not only
achieves performance comparable to RNN-based
baselines on diverse algorithmic tasks but also of-
fers superior controllability, allowing users to im-


--- Page 9 ---
pose explicit inductive biases to guide the solution
discovery.
Limitations
Despite its promise, the Discrete Transformer
exhibits inherent limitations stemming from its
interpretability-driven design: (1) Complexity Ceil-
ing: The dependence on symbolic regression re-
stricts applicability to tasks decomposable into
low-dimensional, sparse interactions rather than
high-dimensional dense processing. (2) Restricted
Expressivity: Formulating attention as a hard in-
formation router constrains model expressiveness,
inherently limiting performance on tasks that ne-
cessitate the continuous, graded weighting mech-
anisms of soft attention. (3) Capacity Trade-off:
Enforcing disentangled scalar representations sac-
rifices the compressive efficiency of superposition,
thereby limiting the capacity required for general
natural language understanding tasks that rely on
distributed and entangled features.
Impact Statement
This paper advances the fields of mechanistic inter-
pretability and algorithm extraction by enabling the
discovery of verifiable, human-readable programs
from the Discrete Transformer. There are many
potential societal consequences of our work, none
of which we feel must be specifically highlighted
here.
References
Trenton Bricken, Adly Templeton, Joshua Batson,
Brian Chen, Adam Jermyn, Tom Conerly, Nick
Turner, Cem Anil, Carson Denison, Amanda Askell,
Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas
Schiefer, Tim Maxwell, Nicholas Joseph, Zac
Hatfield-Dodds, Alex Tamkin, Karina Nguyen, and
6 others. 2023. Towards monosemanticity: Decom-
posing language models with dictionary learning.
Transformer Circuits Thread. Https://transformer-
circuits.pub/2023/monosemantic-
features/index.html.
Alonzo Church. 1963. Application of recursive arith-
metic to the problem of circuit synthesis. Journal of
Symbolic Logic, 28(4).
Miles Cranmer. 2023.
Interpretable machine learn-
ing for science with pysr and symbolicregression.jl.
Preprint, arXiv:2305.01582.
Miles Cranmer,
Alvaro Sanchez-Gonzalez,
Peter
Battaglia, Rui Xu, Kyle Cranmer, David Spergel,
and Shirley Ho. 2020. Discovering symbolic models
from deep learning with inductive biases. Preprint,
arXiv:2006.11287.
Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert
Huben, and Lee Sharkey. 2023. Sparse autoencoders
find highly interpretable features in language models.
Preprint, arXiv:2309.08600.
Nelson Elhage, Tristan Hume, Catherine Olsson,
Nicholas Schiefer, Tom Henighan, Shauna Kravec,
Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain,
Carol Chen, Roger Grosse, Sam McCandlish, Jared
Kaplan, Dario Amodei, Martin Wattenberg, and
Christopher Olah. 2022. Toy models of superpo-
sition. Preprint, arXiv:2209.10652.
Dan Friedman, Alexander Wettig, and Danqi Chen.
2023. Learning transformer programs. Advances
in Neural Information Processing Systems, 36:49044‚Äì
49067.
Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-
berg. 2022. Transformer feed-forward layers build
predictions by promoting concepts in the vocabulary
space. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Process-
ing, pages 30‚Äì45, Abu Dhabi, United Arab Emirates.
Association for Computational Linguistics.
Yury Gorishniy, Ivan Rubachev, and Artem Babenko.
2022. On embeddings for numerical features in tabu-
lar deep learning. Advances in Neural Information
Processing Systems, 35:24991‚Äì25004.
Alex Graves, Greg Wayne, and Ivo Danihelka. 2014.
Neural turing machines. Preprint, arXiv:1410.5401.
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai
Dong, Wentao Zhang, Guanting Chen, Xiao Bi,
Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wen-
feng Liang. 2024. Deepseek-coder: When the large
language model meets programming ‚Äì the rise of
code intelligence. Preprint, arXiv:2401.14196.
Eric Jang, Shixiang Gu, and Ben Poole. 2017. Cat-
egorical reparameterization with gumbel-softmax.
Preprint, arXiv:1611.01144.
Quoc-Vinh Lai-Dang, Taemin Kang, and Seungah Son.
2025.
Adaptive transformer programs: Bridging
the gap between performance and interpretability in
transformers. In International Conference on Rep-
resentation Learning, volume 2025, pages 62785‚Äì
62807.
David Lindner, J√°nos Kram√°r, Sebastian Farquhar,
Matthew Rahtz, Tom McGrath, and Vladimir Miku-
lik. 2023. Tracr: Compiled transformers as a labora-
tory for interpretability. Advances in Neural Informa-
tion Processing Systems, 36:37876‚Äì37899.
Ilya Loshchilov and Frank Hutter. 2019.
De-
coupled weight decay regularization.
Preprint,
arXiv:1711.05101.


--- Page 10 ---
Christos Louizos, Max Welling, and Diederik P. Kingma.
2018. Learning sparse neural networks through l0
regularization. Preprint, arXiv:1712.01312.
He Ma, Arunachalam Narayanaswamy, Patrick Riley,
and Li Li. 2022. Evolving symbolic density function-
als. Science Advances, 8(36):eabq0279.
Andr√© F. T. Martins and Ram√≥n Fernandez Astudillo.
2016. From softmax to sparsemax: A sparse model
of attention and multi-label classification. Preprint,
arXiv:1602.02068.
Aaron Meurer, Christopher P. Smith, Mateusz Pa-
procki, OndÀárej ÀáCert√≠k, Sergey B. Kirpichev, Matthew
Rocklin, AMiT Kumar, Sergiu Ivanov, Jason K.
Moore, Sartaj Singh, Thilina Rathnayake, Sean Vig,
Brian E. Granger, Richard P. Muller, Francesco
Bonazzi, Harsh Gupta, Shivam Vats, Fredrik Johans-
son, Fabian Pedregosa, and 8 others. 2017. Sympy:
symbolic computing in python. PeerJ Computer Sci-
ence, 3:e103.
Eric J. Michaud, Isaac Liao, Vedang Lad, Ziming Liu,
Anish Mudide, Chloe Loughridge, Zifan Carl Guo,
Tara Rezaei Kheirkhah, Mateja Vukeli¬¥c, and Max
Tegmark. 2024. Opening the ai black box: program
synthesis via mechanistic interpretability. Preprint,
arXiv:2402.05110.
Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas K√∂pf, Edward
Yang, Zach DeVito, Martin Raison, Alykhan Te-
jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,
and 2 others. 2019. Pytorch: An imperative style,
high-performance deep learning library. Preprint,
arXiv:1912.01703.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2023. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. Preprint, arXiv:1910.10683.
Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, Sten
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Romain Sauvestre, Tal Remez, J√©r√©my
Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna
Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron
Grattafiori, Wenhan Xiong, Alexandre D√©fossez, and
7 others. 2024. Code llama: Open foundation models
for code. Preprint, arXiv:2308.12950.
Pedro Savarese, Hugo Silva, and Michael Maire. 2021.
Winning the lottery with continuous sparsification.
Preprint, arXiv:1912.04427.
Michael Schmidt and Hod Lipson. 2009. Distilling free-
form natural laws from experimental data. Science,
324(5923):81‚Äì85.
Parshin Shojaee, Kazem Meidani, Shashank Gupta,
Amir Barati Farimani, and Chandan K Reddy.
2025. Llm-sr: Scientific equation discovery via pro-
gramming with large language models. Preprint,
arXiv:2404.18400.
CodeGemma Team, Heri Zhao, Jeffrey Hui, Joshua
Howland, Nam Nguyen, Siqi Zuo, Andrea Hu,
Christopher A. Choquette-Choo, Jingyue Shen, Joe
Kelley, Kshitij Bansal, Luke Vilnis, Mateo Wirth,
Paul Michel, Peter Choy, Pratik Joshi, Ravin Kumar,
Sarmad Hashmi, Shubham Agrawal, and 8 others.
2024. Codegemma: Open code models based on
gemma. Preprint, arXiv:2406.11409.
Silviu-Marian Udrescu, Andrew Tan, Jiahai Feng,
Orisvaldo Neto, Tailin Wu, and Max Tegmark.
2020.
Ai feynman 2.0: Pareto-optimal symbolic
regression exploiting graph modularity. Preprint,
arXiv:2006.10782.
Silviu-Marian Udrescu and Max Tegmark. 2020. AI
Feynman: A physics-inspired method for symbolic
regression. Science Advances, 6(16):eaay2631.
Gail
Weiss,
Yoav
Goldberg,
and
Eran
Yahav.
2021.
Thinking like transformers.
Preprint,
arXiv:2106.06981.
Yifan Zhang, Wenyu Du, Dongming Jin, Jie Fu, and Zhi
Jin. 2025. Finite state automata inside transformers
with chain-of-thought: A mechanistic study on state
tracking. Preprint, arXiv:2502.20129.
A
Smooth Transition Mechanism for
Discrete Optimization
To train the discrete selection parameters in our
discretized reading and Numerical Attention mod-
ules, we address the non-differentiability of hard
selection while avoiding the pitfalls of standard re-
laxation methods. While Gumbel-Softmax enables
differentiable sampling, it often struggles to escape
local optima and fails to promote the strict sparsity
essential for interpretability. To address this, we
incorporate a smooth transition mechanism (Lai-
Dang et al., 2025), which dynamically interpolates
between Gumbel-Softmax and Sparsemax (Mar-
tins and Astudillo, 2016) throughout the training
process. The hybrid sampling vector p ‚ààRNl is
defined as:
p = (1 ‚àíŒ±(œÑ))psoft + Œ±(œÑ)psparse,
(8)
where psoft
and psparse
denote sample vec-
tors drawn from Gumbel-Softmax and Gumbel-
Sparsemax distributions, respectively. The interpo-
lation coefficient Œ±(œÑ) ‚àà[0, 1] is a scheduler func-
tion of the temperature œÑ, designed to shift strictly
from 0 to 1 as œÑ anneals. This mechanism allows
the model to prioritize exploration via Softmax in
the early stages, before smoothly transitioning to


--- Page 11 ---
Sparsemax to enforce sparsity and deterministic
selection (exploitation) in later stages. Ultimately,
this ensures convergence to concise, interpretable
program structures.
B
Unmatched Heads
While the Fixed Offset and Windowed Extrema pat-
terns achieve high coverage across the algorithmic
benchmark investigated, we observe a subset of
attention heads that do not conform to these in-
terpretability templates. Crucially, our analysis
reveals that these ‚Äúunmatched heads‚Äù do not rep-
resent missing algorithmic primitives, but rather
manifest as computational noise or redundancy.
We observe that these heads are generally rendered
ineffective through two primary mechanisms: by
being suppressed with negligible magnitudes, or by
being ignored by higher-layer modules.
For instance, in the bitwise_and task, the head
labeled Attn_L0H1 exhibits a disordered attention
matrix that matches neither location-based nor
content-based patterns. However, symbolic tracing
of the computational graph confirms that its output
variable is an orphan variable: it is not selected
as an input by any subsequent modules nor aggre-
gated by the final output projection. Similarly, in
the minimum_prev2 task with the number of sub-
MLPs constrained to zero, we observe that the head
labeled Attn_L0H1 behaves as an unmatched head.
Since its output is disconnected from the valid ex-
ecution path, this lack of interpretability does not
impede successful extraction of the underlying al-
gorithm.
C
Experiment Details
Datasets We adopt the algorithmic reasoning suite
from the MIPS benchmark (Michaud et al., 2024)
as our primary testbed. These tasks probe specific
capabilities of neural networks, such as arithmetic
reasoning, variable tracking, and non-linear compo-
sition. We categorize them into three logical tiers
based on their underlying complexity and memory
dependence:
‚Ä¢ Linear Arithmetic (Local vs. Global). This cat-
egory involves linear transformations. Non-state-
tracking tasks (e.g., sum_last2) require only local
attention within a fixed window. In contrast, State-
tracking tasks (e.g., sum_last) require the model
to maintain a persistent memory state‚Äîimplicitly
simulating a finite state automaton (Zhang et al.,
2025)‚Äîto compute cumulative metrics over the
sequence.
‚Ä¢ Non-Linear Composition.
These tasks ne-
cessitate non-linear activation logic.
Variants
range from local operations like parity_last2
and maximum_prev2 to global state-tracking tasks
like add_mod_3. Success here requires the Numeri-
cal MLP to approximate non-linear functions (e.g.,
XOR) rather than simple linear aggregation.
‚Ä¢ Dynamical Systems (Physical Problems). To
evaluate generalization beyond pure arithmetic, we
include tasks derived from classical mechanics
(freebody, gravity, and spring). While math-
ematically reducible to iterative linear updates,
these tasks challenge the model to discover govern-
ing physical laws (e.g., Newton‚Äôs laws of motion)
and simulate continuous dynamical systems purely
from observed data.
Training Details All models adopt a decoder-only
architecture implemented in PyTorch (Paszke et al.,
2019), optimized via AdamW (Loshchilov and Hut-
ter, 2019) to minimize MSE. The training dataset
consists of 1, 000, 000 samples. Unless otherwise
specified, both the input and output sequences have
a fixed length of 10. We employ a cosine annealing
schedule to decay the learning rate from 0.05 to
1 √ó 10‚àí6 over 50 epochs with a batch size of 512.
(For the freebody and gravity tasks, models are
trained for 100 epochs with an initial learning rate
of 0.01.) To handle discrete optimization, we apply
a geometric annealing schedule to the sampling
temperature œÑ, decreasing it from 10.0 to 0.1 to
facilitate a gradual transition from continuous ex-
ploration to discrete selection. For each task, we
conduct a grid search over the number of layers
{1, 2}, attention heads {2, 3}, and sub-MLPs per
layer {2, 3}. We run all experiments across three
random seeds, reporting the mean performance and
selecting the checkpoint with the best validation
performance for subsequent analysis.
Task Formulation To facilitate algorithm extrac-
tion and ensure robustness across varying sequence
lengths and positions, we design our task formula-
tions to encourage the learning of length-invariant
functions. We categorize tasks based on their re-
liance on state-tracking: (1) Non-state-tracking
tasks are formulated as Token-Tagging problems,
requiring independent, element-wise predictions
for each position.
(2) State-tracking tasks are
framed as Language Modeling problems, where
the model performs autoregressive next-token pre-
diction based on the input and historical context.
To facilitate symbolic regression and align with


--- Page 12 ---
the regression nature of the task, we adopt MSE
as the loss function, enabling the model to learn
continuous functional mappings.
D
Sensitivity Analysis of Model Capacity
To evaluate the robustness of our framework under
architectural variations, we investigate the impact
of hyperparameters‚Äîspecifically the number of
layers, attention heads per layer, and sub-MLPs
per layer‚Äîon both the convergence quality, mea-
sured by the MSE loss, and the interpretability,
quantified by the line count of the extracted pro-
gram. We conduct experiments on the sum_last2,
parity_last2, and spring tasks, varying the
number of layers in {1, 2, 3, 4}, attention heads
per layer in {0, 1, 2, 4, 8}, and sub-MLPs per layer
in {0, 1, 2, 4, 8}.
Our results exhibit a clear threshold effect in ex-
pressive capacity. Once the architecture satisfies
the minimal functional requirements of the target
task (e.g., sum_last2 requires at least one atten-
tion head to retrieve the token xt‚àí1), the Discrete
Transformer reliably converges to a near-zero MSE
loss. In contrast, over-parameterization substan-
tially alters the dynamics of the discrete search.
Specifically, excessive capacity tends to inflate the
length of the extracted programs due to structural
redundancy: multiple modules may learn function-
ally equivalent behaviors (e.g., several attention
heads learning the same retrieval pattern for xt‚àí1).
The introduction of redundant modules expands
the search space and introduces noise during the
exploration phase. While this poses a challenge
to the optimization stability, we find that it can be
effectively addressed by adopting a slower temper-
ature annealing schedule, which provides sufficient
time to resolve the competition between redundant
components and settle into a valid discrete solution.
E
Additional Synthesized Programs
This section, we provide additional synthesized
programs.


--- Page 13 ---
1
import numpy as np
2
def maximum_prev2(input_seq):
3
# V0_input
4
input_arr = np.array(input_seq, dtype=float)
5
seq_len = input_arr.shape[0]
6
V0 = input_arr[:, 0]
7
# V1_Attn_L0H0
8
V1 = np.zeros(seq_len)
9
# windowed max
10
for t in range(seq_len):
11
V1[t] = np.max(V0[max(0, t‚àí1): t+1])
12
# V2_Attn_L0H1
13
V2 = np.zeros(seq_len)
14
# windowed max
15
for t in range(seq_len):
16
V2[t] = np.max(V0[max(0, t‚àí1): t+1])
17
# Output Head
18
output = 1.07 * V1 + ‚àí0.07 * V2
19
return output
20
1
import numpy as np
2
def minimum_prev2(input_seq):
3
# V0_input
4
input_arr = np.array(input_seq, dtype=float)
5
seq_len = input_arr.shape[0]
6
V0= input_arr[:, 0]
7
# V1_Attn_L0H0
8
V1 = np.zeros(seq_len)
9
# windowed min
10
for t in range(seq_len):
11
V1[t] = np.min(V0[max(0, t‚àí1): t+1])
12
# Output Head
13
output = 1.00 * V1
14
return output
15
1
import numpy as np
2
def spring(input_seq):
3
# V0_input
4
input_arr = np.array(input_seq, dtype=float)
5
seq_len = input_arr.shape[0]
6
V0 = input_arr[:, 0]
7
# V1_Attn_L0H0
8
V1 = np.zeros(seq_len)
9
# Fixed offset 9
10
V1[9:] = V0[:‚àí9]
11
# V2_Attn_L0H1
12
V2 = np.zeros(seq_len)
13
# Fixed offset 10
14
V2[10:] = V0[:‚àí10]
15
# V3_MLP_L0M0
16
V3 = np.full(seq_len, 0.02)
17
# V4_MLP_L0M1
18
V4 = np.full(seq_len, ‚àí0.11)
19
# V5_Attn_L1H0
20
V5 = np.zeros(seq_len)
21
# Fixed offset 2
22
V5[2:] = V0[:‚àí2]
23
# V6_Attn_L1H1
24
V6 = np.zeros(seq_len)
25
# Fixed offset 2
26
V6[2:] = V4[:‚àí2]
27
# V7_MLP_L1M0
28
V7 = np.full(seq_len, ‚àí0.03)
29
# Output Head
30
output = 1.00 * V1 + 1.00 * V2 + ‚àí1.00 * V5 + 0.03 *
V7 + ‚àí0.01 * V6
31
return output
32
Figure 6: Intervened synthesized programs revealing alternative logical pathways. Left: When MLP-based arithmetic
is prohibited, the model solves maximum_prev2 and minimum_prev2 by shifting to a pure attention mechanism.
The extracted code shows explicit Windowed Extrema attention patterns. Right: For the spring task, intervention in
the information flow (masking recent history) forces the model to learn a high-order recurrence relation, validating
the model‚Äôs ability to uncover multiple equivalent algorithms for the same data distribution.


--- Page 14 ---
1
import numpy as np
2
def sum_last(input_seq):
3
# V0_input
4
input_arr = np.array(input_seq, dtype=float)
5
seq_len = input_arr.shape[0]
6
V0 = input_arr[:, 0]
7
# V1_Attn_L0H0
8
V1 = np.zeros(seq_len)
9
# Fixed offset 9
10
V1[9:] = V0[:‚àí9]
11
# Output Head
12
output = 1.00 * V0 + 1.00 * V1
13
return output
14
1
import numpy as np
2
def bitwise_and(input_seq):
3
# V0_input, V1_input
4
input_arr = np.array(input_seq, dtype=float)
5
seq_len = input_arr.shape[0]
6
V0_input = input_arr[:, 0]
7
V1_input = input_arr[:, 1]
8
# V4_MLP_L0M0
9
V4 = np.full(seq_len, 0.02)
10
# V5_MLP_L0M1
11
V5 = ((((V0 ‚àí0.49) * 3.28) / (V1 + ‚àí0.52)) ‚àí3.06) *
0.19
12
# Output Head
13
output = 0.49 * V1 + 0.48 * V0 + 0.40 * V5 + 0.02 *
V4
14
return output
15
1
import numpy as np
2
def add_mod_3(input_seq):
3
# V0_input
4
input_arr = np.array(input_seq, dtype=float)
5
seq_len = input_arr.shape[0]
6
V0 = input_arr[:, 0]
7
# V1_Attn_L0H0
8
V1 = np.zeros(seq_len)
9
# Fixed offset 9
10
V1[9:] = V0[:‚àí9]
11
# V2_Attn_L0H1
12
V2 = np.zeros(seq_len)
13
# Fixed offset 9
14
V2[9:] = V0[:‚àí9]
15
# V3_MLP_L0M0
16
V3 = ((‚àí0.70 / (((V0 + V1) + 0.59) * ‚àí0.56)) ‚àí1.69) +
(1.05 / (V0 + (V1 ‚àí2.53)))
17
# Output Head
18
output = ‚àí0.53 * V3 + 0.47 * V2 + ‚àí0.32 * V1 + 0.16 *
V0
19
return output
20
Figure 7: Synthesized programs for sum_last (Top Left), bitwise_and (Bottom Left), and add_mod_3 (Right).
Table 2: Performance and parameters of the Discrete Transformer on the algorithmic benchmark. The columns
Layers, Heads, and sub-MLPs correspond to the number of layers, the number of attention heads per layer, and the
number of sub-MLPs per layer, respectively. Loss values indicate MSE for these regression tasks.
Task Name
Description
Layers
Heads
sub-MLPs
Train Loss
Test Loss
Linear Arithmetic
sum_last2
Sum of the last two numbers
1
2
2
8.82 √ó 10‚àí9
6.76 √ó 10‚àí9
diff_last2
Difference between last two numbers
1
2
2
6.59 √ó 10‚àí16
7.02 √ó 10‚àí16
sum
Cumulative sum of the sequence
1
2
2
1.71 √ó 10‚àí14
1.70 √ó 10‚àí14
Non-Linear Composition
parity_last2
Parity check of the last two numbers
1
2
2
4.87 √ó 10‚àí13
1.46 √ó 10‚àí12
maximum_prev2
Maximum of the last two numbers
1
2
2
1.11 √ó 10‚àí6
4.43 √ó 10‚àí6
minimum_prev2
Minimum of the last two numbers
1
2
2
1.64 √ó 10‚àí5
1.77 √ó 10‚àí5
bitwise_and
Bitwise AND
1
2
2
1.10 √ó 10‚àí18
1.06 √ó 10‚àí18
bitwise_or
Bitwise OR
1
2
2
5.66 √ó 10‚àí4
1.63 √ó 10‚àí6
bitwise_not
Bitwise NOT
1
2
2
1.44 √ó 10‚àí17
< 10‚àí20
bitwise_xor
Bitwise XOR
1
2
2
6.74 √ó 10‚àí20
6.67 √ó 10‚àí20
abs
Absolute value of the current number
1
2
2
2.69 √ó 10‚àí14
5.25 √ó 10‚àí14
abs_of_diff
Absolute difference of last two numbers
1
2
2
9.72 √ó 10‚àí9
2.01 √ó 10‚àí8
parity
Cumulative parity
1
2
2
8.15 √ó 10‚àí17
8.09 √ó 10‚àí17
add_mod_3
Cumulative sum modulo 3
1
2
2
9.72 √ó 10‚àí13
1.72 √ó 10‚àí7
Physical Problems
freebody
Simulate freebody dynamics
2
2
3
9.57 √ó 10‚àí11
9.57 √ó 10‚àí11
gravity
Simulate gravity dynamics
2
2
3
2.34 √ó 10‚àí11
1.86 √ó 10‚àí11
spring
Simulate spring dynamics
1
2
2
1.41 √ó 10‚àí7
3.49 √ó 10‚àí9


--- Page 15 ---
1
import numpy as np
2
def freebody(input_seq):
3
# V0_input
4
input_arr = np.array(input_seq, dtype=float)
5
seq_len = input_arr.shape[0]
6
V0 = input_arr[:, 0]
7
# V1_Attn_L0H0
8
V1 = np.zeros(seq_len)
9
# Fixed offset 9
10
V1[9:] = V0[:‚àí9]
11
# V2_Attn_L0H1
12
V2 = np.zeros(seq_len)
13
# Fixed offset 10
14
V2[10:] = V0[:‚àí10]
15
# V4_MLP_L0M1
16
V4 = ‚àí0.26*V0 + 0.04
17
# V5_MLP_L0M2
18
V5 = ‚àí0.21*V0 + ‚àí0.04
19
# V6_Attn_L1H0
20
V6 = np.zeros(seq_len)
21
# Fixed offset 2
22
V6[2:] = V4[:‚àí2]
23
# V7_Attn_L1H1
24
V7 = np.zeros(seq_len)
25
# Fixed offset 2
26
V7[2:] = V5[:‚àí2]
27
# Output Head
28
output = 1.19 * V0 + 1.06 * V7 + 1.06 * V6 + 1.00 *
V1 + ‚àí0.65 * V4 + ‚àí0.65 * V5 + 0.50 * V2
29
return output
30
1
import numpy as np
2
def spring(input_seq):
3
# V0_input
4
input_arr = np.array(input_seq, dtype=float)
5
seq_len = input_arr.shape[0]
6
V0 = input_arr[:, 0]
7
# V1_Attn_L0H0
8
V1 = np.zeros(seq_len)
9
# Fixed offset 9
10
V1[9:] = V0[:‚àí9]
11
# V2_Attn_L0H1
12
V2 = np.zeros(seq_len)
13
# Fixed offset 1
14
V2[1:] = V0[:‚àí1]
15
# V4_MLP_L0M1
16
V4 = (V2 + ((V1 ‚àí((V1 + 2.26) + (V2 ‚àí1.54))) * 2.00)
) + (V2 + 1.43)
17
# Output Head
18
output = ‚àí1.00 * V2 + 1.00 * V0+ 1.00 * V1 + 0.04 *
V4
19
return output
20
1
import numpy as np
2
def gravity(input_seq):
3
# V0_input
4
input_arr = np.array(input_seq, dtype=float)
5
seq_len = input_arr.shape[0]
6
V0 = input_arr[:, 0]
7
# V1_Attn_L0H0
8
V1 = np.zeros(seq_len)
9
# Fixed offset 9
10
V1[9:] = V0[:‚àí9]
11
# V2_Attn_L0H1
12
V2 = np.zeros(seq_len)
13
# Fixed offset 10
14
V2[10:] = V0[:‚àí10]
15
# V4_MLP_L0M1
16
V4 = 0.00
17
# V5_MLP_L0M2
18
V5 = ‚àí0.40*V0 + ‚àí3.05
19
# V6_Attn_L1H0
20
V6 = np.zeros(seq_len)
21
# Fixed offset 2
22
V6[2:] = V5[:‚àí2]
23
# V7_Attn_L1H1
24
V7 = np.zeros(seq_len)
25
# Fixed offset 2
26
V7[2:] = V4[:‚àí2]
27
# Output Head
28
output = 1.24 * V6 + 1.20 * V0 + 1.00 * V1 + ‚àí0.74 *
V5 + 0.50 * V2+ ‚àí0.01 * V4 + 0.01 * V7
29
return output
30
Figure 8: Synthesized programs for freebody (Top Left), spring (Bottom Left), and gravity (Right). By
coordinating Numerical Attention, Numerical MLP and the output head, the model effectively identifies essential
computational variables and constructs the correct computation graph. This confirms the model‚Äôs capability to learn
underlying physical laws and demonstrates its potential for modeling complex physical processes.
