--- Page 1 ---
OSCAR: Optimization-Steered Agentic Planning for
Composed Image Retrieval
Teng Wangâˆ—
wt0318@connect.hku.hk
OPPO
Shenzhen, China
Rong Shanâˆ—
shanrong@sjtu.edu.cn
Shanghai Jiao Tong University
Shanghai, China
Jianghao Linâ€ 
linjianghao@sjtu.edu.cn
Shanghai Jiao Tong University
Shanghai, China
Junjie Wu
wujunjie1@oppo.com
OPPO
Shenzhen, China
Tianyi Xu
crimsonflag@sjtu.edu.cn
Shanghai Jiao Tong University
Shanghai, China
Jianping Zhang
jpzhang1810@gmail.com
Shanghai Jiao Tong University
Shanghai, China
Wenteng Chen
cwt-03@sjtu.edu.cn
Shanghai Jiao Tong University
Shanghai, China
Changwang Zhang
changwangzhang@foxmail.com
OPPO
Shenzhen, China
Zhaoxiang Wang
steven.wangzx@gmail.com
OPPO
Shenzhen, China
Weinan Zhang
wnzhang@sjtu.edu.cn
Shanghai Jiao Tong University
Shanghai, China
Jun Wangâ€ 
junwang.lu@gmail.com
OPPO
Shenzhen, China
Abstract
Composed image retrieval (CIR) requires complex reasoning over
heterogeneous visual and textual constraints. Existing approaches
largely fall into two paradigms: unified embedding retrieval, which
suffers from single-model myopia, and heuristic agentic retrieval,
which is limited by suboptimal, trial-and-error orchestration. To this
end, we propose OSCAR, an optimization-steered agentic planning
framework for composed image retrieval. We are the first to refor-
mulate agentic CIR from a heuristic search process into a principled
trajectory optimization problem. Instead of relying on heuristic
trial-and-error exploration, OSCAR employs a novel offline-online
paradigm. In the offline phase, we model CIR via atomic retrieval
selection and composition as a two-stage mixed-integer program-
ming problem, mathematically deriving optimal trajectories that
maximize ground-truth coverage for training samples via rigorous
boolean set operations. These trajectories are then stored in a golden
library to serve as in-context demonstrations for online steering of
VLM planner at online inference time. Extensive experiments on
three public benchmarks and a private industrial benchmark show
that OSCAR consistently outperforms SOTA baselines. Notably,
it achieves superior performance using only 10% of training data,
demonstrating strong generalization of planning logic rather than
dataset-specific memorization. Our code is available1.
1
Introduction
With the evolution of modern retrieval systems, real-user queries
have become increasingly complex and multimodal, which often
âˆ—Equal contribution.
â€ Corresponding Author
1https://anonymous.4open.science/r/OSCAR-3A55/README.md
(a) Single-Model Myopia of Unified Embedding Retrieval
Reference 
Image
Take the picture closer, 
keep the chickens and 
remove the leaves.
VLM 
Planner
VLM 
Verifier
Fashion
Data
Model A
Model B
Model C
Product
Data
Nature
Data
Unified 
Embedding 
Model
Reference 
Image
Take the picture closer, 
keep the chickens and 
remove the leaves.
Search
Multiple 
Tools
Result
Search
Refine
(b) Suboptimal Orchestration of Heuristic Agentic Retrieval
  Heuristic
Optimal 
Trajectory
Heuristic
Trajectory
Figure 1: The illustration of limitations of existing image
retrieval methods, i.e., (a) single-model myopia of unified
embedding retrieval, and (b) suboptimal orchestration of
heuristic agentic retrieval.
involve compositional descriptions, multiple constraints, and im-
plicit reasoning over both visual and textual information [14, 31].
In response, a growing body of research has emerged to address
composed image retrieval, generally falling into two paradigms:
Unified Embedding Retrieval and Heuristic Agentic Retrieval. While
both have yielded promising advances, they exhibit fundamental
limitations that increasingly bottleneck real-world performance, as
shown in Figure 1.
Firstly, unified embedding retrieval attempts to resolve com-
plex queries via a single, monolithic representation model, whether
using a general-purpose multimodal encoder [19, 20, 27, 43] or a
arXiv:2602.08603v1  [cs.AI]  9 Feb 2026


--- Page 2 ---
Conferenceâ€™17, July 2017, Washington, DC, USA
Teng Wang et al.
domain-specific model fine-tuned for a particular setting [2, 8, 29,
47]. However, they suffer from single-model myopia. Concretely,
this paradigm implicitly assumes that a single latent space can act
as a universal solution, whereas real-world queries are inherently
heterogeneous. User intents vary drastically in granularity, ranging
from high-level stylistic shifts (e.g., more formal) to fine-grained
attribute constraints (e.g., specific color, texture, or pattern), and the
optimal visual evidence shifts across domains [7, 26, 31, 42]. Under
such diversity, relying on a single embedding space inevitably leads
to short-sighted behaviors. As illustrated in Figure 1(a), a model
optimized for natural images may fail to resolve fashion-specific at-
tributes, while a fashion-oriented model may struggle with natural
scene reasoning. Consequently, even state-of-the-art single-model
systems remain brittle in practice, as they are unable to adapt to
intents that evolve or drift across diverse domains.
Secondly, heuristic agentic retrieval seeks to overcome the
representation bottleneck by decomposing the task into multi-step
workflows invoked by Large Language Models (LLMs) [1, 44] or
Visionâ€“Language Models (VLMs) [4, 40]. These methods leverage
external tools (e.g., captioners, rewriters, and retrievers) to handle
complex queries [11, 13, 36, 45]. However, despite their flexibility,
these approaches struggle with suboptimal orchestration, as
shown in Figure 1(b). Current pipelines rely heavily on the agentâ€™s
internal heuristics (e.g., ReAct-style loops [48]), where the model
makes greedy, iterative decisions on which tool to call next based
on intermediate outputs. This lack of a global objective leads to
unstructured and inefficient trajectories characterized by redundant
calls, poor logical ordering, and unreliable handling of set-theoretic
constraints (inclusion/exclusion). Furthermore, the reliance on iter-
ative interaction incurs significant computational overhead without
offering any guarantees towards the optimal retrieval path.
To this end, we propose OSCAR, an optimization-steered agentic
planning framework for composed image retrieval, which, for the
first time, reframes agentic CIR from a heuristic search process into
a principled trajectory planning problem. Unlike prior methods
that rely on heuristic, trial-and-error LLM exploration, OSCAR
introduces a novel offline-online paradigm. During the offline stage,
we treat each individual retriever invocation as the fundamental
unit, and derive optimal tool-call planning trajectories for training
samples by solving a two-stage mixed-integer programming (MIP)
formulation. These optimal trajectories are then stored in a golden
library to serve as reasoning demonstrations during inference. At
test time, OSCAR acquires relevant planning patterns to steer the
VLM agent, enabling it to replicate optimal logical reasoning for CIR
without requiring expensive iterative search or human annotation.
In summary, our contributions are as follows:
â€¢ Optimization Perspective. To the best of our knowledge, we
are the first to formulate agentic CIR as a MIP problem. By shift-
ing from heuristic search to global optimization, we mathemati-
cally derive optimal planning trajectories for training samples
that maximize ground-truth coverage while minimizing com-
putational redundancy, providing strong demonstration signals
without human annotation.
â€¢ Set-Theoretic Composition Logic. We introduce a rigorous
logic for composing CIR results via boolean set operations (i.e.,
union, intersection, and difference). This enables the agent to
perform explicit inclusion and conservative exclusion reason-
ing, a capability that is mathematically intractable for single-
embedding models and heuristic agentic methods.
â€¢ The OSCAR Framework. We propose a novel offline-online
paradigm that bridges MIP optimization and agentic planning. By
storing MIP solutions into a golden library as demonstrations, OS-
CAR effectively steers VLMs to perform complex compositional
planning for CIR in a single inference pass.
â€¢ Empirical Superiority. On three public benchmarks and a pri-
vate industrial benchmark, OSCAR consistently outperforms
SOTA single-embedding and agentic baselines. Notably, it achieves
these gains with only 10% of the training data for library construc-
tion, demonstrating strong generalization of abstract planning
logic rather than simple memorization.
2
Related Works
Composed image retrieval (CIR) aims to retrieve target images
that satisfy specific textual modifications while preserving the vi-
sual context of a reference image. This task demands fine-grained
compositional reasoning over multimodal heterogeneous attributes
with increasing semantic complexity [7, 26, 31, 42].
Unified Embedding Retrieval for CIR. A dominant line of
work formulates CIR by collapsing a composed query, which con-
sists of a reference image and a modification text, into a single
embedding, followed by nearest-neighbor retrieval. Composition
is typically achieved via cross-modal fusion or feature editing in
a shared latent space [8, 9, 15, 23, 38]. Recent approaches further
align visual content with language by projecting images into text-
aligned spaces or performing composition purely in the textual do-
main [2, 17, 18, 22, 25, 30, 32, 33, 46, 47]. Moreover, recent large-scale
benchmarks and leaderboards [20, 27] have driven rapid progress
in multimodal embedding models for CIR, with a series of methods
achieving increasingly strong performance [19, 28, 35, 43]. How-
ever, different models exhibit varying strengths across different
domains and datasets, rendering the single-model myopia problem.
Heuristic Agentic Retrieval for CIR. To overcome the lim-
itations of unified embeddings, recent work explores agentic for-
mulations that decompose CIR into multiple steps with specialized
roles, such as query rewriting, retrieval, and correction [13, 34, 36,
45]. These approaches demonstrate improved flexibility and inter-
pretability by leveraging iterative reasoning and tool use. However,
they are typically driven by heuristic exploration, often requiring
repeated model interactions, incurring high computational cost,
and lacking global optimality guarantees. In contrast, our proposed
OSCAR replaces this heuristic trial-and-error with optimization-
steered planning, using an offline MIP formulation to guide the
agent towards provably efficient and accurate retrieval trajectories.
More detailed related work is provided in Appendix A.
3
Methodology
In this section, we present OSCAR, an Optimization-Steered Agen-
tic Planning Framework for Composed Image Retrieval. The overall
framework of OSCAR is shown in Figure 2. All the prompts for
VLM invocation are provided in Appendix I.


--- Page 3 ---
OSCAR: Optimization-Steered Agentic Planning for
Composed Image Retrieval
Conferenceâ€™17, July 2017, Washington, DC, USA
Negative Atomic Retrieval
Query: On dry leaves, a dog is in front of 
the camera with chickens. 
Top-K Truncation: 3
Polarity: Negative
Tool: Captioning Model B
More Ground Truths
Less False Positives
Higher Diversity 
Recall-Oriented Selective MIP
Intersection of 
Negative Atomic 
Retrieval
Union of 
Positive Atomic 
Retrieval
Precision-Oriented Composition MIP
Difference of 
Pos. and Neg.
Optimal 
Trajectory
Golden 
Library
Similar Golden 
Trajectories
Training 
Data
Negative Atomic Retrieval
Query: On dry leaves, a dog is in front of 
the camera with chickens. 
Top-K Truncation: 3
Polarity: Negative
Tool: Captioning Model B
Add one dog, 
and remove ...
Atomic Retrieval Construction
Take the picture closer, 
keep the chickens and  
remove the leaves.
CIR Results
Inference
Testing 
Data
Offline Optimization for Training Data
Inference Time Steering
Positive Atomic Retrieval
Query: A dog in close, with some chickens 
running on the grass.
Top-K Truncation: 5
Polarity: Positive
Tool: Embedding Model B
Positive Atomic Retrieval
Query: A dog in close, with some chickens 
running on the grass.
Top-K Truncation: 5
Polarity: Positive
Tool: Embedding Model B
VLM Planner
VLM Planner
VLM Verifier
Figure 2: The overall framework of our proposed OSCAR.
3.1
Preliminaries
Problem Formulation. Let ğ‘= {ğ‘ğ‘–ğ‘šğ‘”,ğ‘ğ‘¡ğ‘¥ğ‘¡} denote a composed
query, whereğ‘ğ‘–ğ‘šğ‘”is the reference image andğ‘ğ‘¡ğ‘¥ğ‘¡is the modification
text. The goal of CIR is to retrieve a set of ground-truth images
ğ¼+ âŠ‚I from an image gallery I based on the composed query ğ‘.
Standard approaches typically learn a monolithic retrieval model
to perform CIR in a unified latent space, thereby suffering from
single-model myopia. In response, agentic methods reframe these
existing retrieval models as a set of atomic tools, denoted as T =
{ğ‘“1, ğ‘“2, ..., ğ‘“ğ‘š}. Each tool accepts the composed query input and
returns a set of candidate images. This formulation allows us to
treat the retrieval process not as a single inference step, but as a
compositional planning problem over the space of available tools.
From Heuristic Exploration to Global Optimization. Existing
agentic methods typically navigate this tool space via heuristic
exploration (e.g., ReAct loops [48]), where an LLM/VLM iteratively
decides the next step based on local context. While flexible, they lack
a global view of the solution space: agents may redundantly query
overlapping concepts or fail to enforce strict exclusion constraints.
We observe that for any given query ğ‘with known ground truth
I+, the ideal tool usage can be mathematically formulated as a
Set Cover [6] variation: we seek the minimal subset of atomic
tool outputs whose union covers I+ while their intersection with
non-targets Iâˆ’= I \ I+ is minimized. This realization allows
us to reframe the planning process not as a trial-and-error search,
but as a MIP problem that explicitly solves for the global optimal
trajectory.
The OSCAR Framework. As illustrated in Figure 2, since the
ground-truth set is unknown during the retrieval, OSCAR bridges
the gap via a distinct offline-online paradigm that steers the retrieval
process using golden demonstrations derived from training data.
â€¢ Offline Optimization. For each training sample, we first con-
struct and perform atomic retrievals by varying the query and
top-ğ‘˜truncation. Then, we optimize the global planning trajec-
tory via two-stage MIPs: (1) a recall-oriented MIP that identifies
the optimal subset of atomic retrievals to maximize ground-truth
coverage, and (2) a precision-oriented MIP that combines rigorous
set-theoretic operations (union, intersection, difference) to filter
irrelevant images. The resulting optimal trajectories are stored
in a golden library to steer the online agentic planning for CIR.
â€¢ Online Steering. For a test query with unknown ground truth,
we select optimal trajectories associated with similar queries
from the golden library as in-context demonstrations. In this
way, we effectively generalize the high-level logic derived offline
and steer the agent toward near-optimal planning for CIR.
3.2
Atomic Retrieval Construction
To enable fine-grained planning, OSCAR first decomposes the com-
plex user intent into a search space of discrete, executable actions.
We define an atomic retrieval ğ‘Ÿas the fundamental unit of this
space, formalized as a four-tuple:
ğ‘Ÿ= (ğ‘“, Ë†ğ‘, ğ‘, ğ‘˜),
(1)
where ğ‘“âˆˆT is the retrieval tool (e.g., a caption-based searcher
or a multimodal encoder), Ë†ğ‘= {ğ‘ğ‘–ğ‘šğ‘”, Ë†ğ‘ğ‘¡ğ‘¥ğ‘¡} is derived by rewriting
the textual query, ğ‘âˆˆ{+, âˆ’} indicates the polarity (positive for
inclusion, negative for exclusion), and ğ‘˜is the number of candidate
images to return (i.e., top-ğ‘˜truncation). Each atomic retrieval ğ‘Ÿrep-
resents an indivisible planning unit that returns a specific candidate
image set Sğ‘ŸâŠ‚I.
Query Decomposition and Polarity. Rather than solely rely-
ing on the original query, we employ a VLM to generate a diverse
set of rewritten queries. The VLM is prompted to decompose the
modification instructions into specific visual attributes or semantic
constraints. Crucially, each generated query Ë†ğ‘is assigned a polar-
ity ğ‘. As illustrated in Figure 2, positive queries (ğ‘= +) target
attributes that must be present in the result (e.g., chickens), while
negative queries (ğ‘= âˆ’) target attributes that must be explicitly
excluded (e.g., leaves). This decoupling allows the subsequent opti-
mization stage to leverage boolean set logic for precise inclusion
and exclusion, a capability that a single retrieval model lacks.
Top-ğ‘˜Truncation. The truncation parameter ğ‘˜dictates the
trade-off between recall and precision for a given tool ğ‘“. To enable
the planning over different granularities of scope, we discretize the


--- Page 4 ---
Conferenceâ€™17, July 2017, Washington, DC, USA
Teng Wang et al.
truncation parameter ğ‘˜into a finite set of levels:
K = {ğ‘˜1,ğ‘˜2, . . . ,ğ‘˜ğ‘šğ‘ğ‘¥}.
(2)
For a fixed tool ğ‘“and query Ë†ğ‘, the retrieved results are deterministic
and monotonic w.r.t. ğ‘˜(i.e., Sğ‘˜1 âŠ‚Sğ‘˜2 for ğ‘˜1 < ğ‘˜2). To ensure
computational efficiency, we execute the retrieval once with the
maximum truncation ğ‘˜max and generate the smaller top-ğ‘˜variants
via slicing, avoiding redundant model inferences.
By taking the Cartesian product over available tools ğ‘“, rewritten
queries Ë†ğ‘, polarities ğ‘, and truncations ğ‘˜, we construct a diverse
global set of atomic retrievals, denoted as R (1,182 atomic retrievals
per sample). This set serves as the decision space for the subse-
quent offline optimization stages, where we select and compose the
optimal subset of R to satisfy the global retrieval objective.
3.3
Recall-Oriented Selection MIP
The first-stage recall-oriented MIP aims to identify a compact sub-
set of positive atomic retrievals R+ that maximizes the coverage
of ground-truth images I+ while minimizing the inclusion of ir-
relevant noise Iâˆ’. This selection process effectively prunes the
massive atomic retrieval space constructed in Section 3.2, result-
ing in a recall-oriented candidate set of images U âŠ‚I for the
subsequent fine-grained composition stage.
Optimization Variables. We define binary decision variables ğ‘¥ğ‘Ÿâˆˆ
{0, 1} to indicate whether a positive atomic retrieval ğ‘ŸâˆˆR+ is
selected. We further define auxiliary state variables, which are
logically determined by the decision variable ğ‘¥ğ‘Ÿ, to track image
coverage and tool diversity:
â€¢ ğ‘ğ‘–âˆˆ{0, 1} indicates whether the image ğ‘–âˆˆI is covered by at
least one selected atomic retrieval.
â€¢ ğ‘¡ğ‘“âˆˆ{0, 1} indicates whether the tool ğ‘“is used by at least one
selected atomic retrieval, i.e., active.
Moreover, to prevent tool redundancy, we group atomic retrievals
into different families F . A family ğ¹âˆˆF consists of retrievals that
share the same tool ğ‘“, query Ë†ğ‘, and polarity ğ‘, differing only in
their truncation threshold ğ‘˜. Since larger ğ‘˜values strictly subsume
smaller ones (monotonicity), selecting multiple members from the
same family provides diminishing returns.
MIP Formulation. We formulate the selection problem as a MIP
that balances three competing objectives: (1) maximizing the ground-
truth coverage Ã
ğ‘–âˆˆI+ ğ‘ğ‘–/|I+|, (2) minimizing the irrelevant noise
Ã
ğ‘–âˆˆIâˆ’ğ‘ğ‘–/|Iâˆ’|, and (3) encouraging the tool diversity Ã
ğ‘“âˆˆT ğ‘¡ğ‘“. The
optimization formulation is written as:
max
{ğ‘¥ğ‘Ÿ}ğ‘ŸâˆˆR+
ğ‘¤ğ‘…
|I+|
âˆ‘ï¸
ğ‘–âˆˆI+
ğ‘ğ‘–âˆ’ğ‘¤ğ‘ƒ
|Iâˆ’|
âˆ‘ï¸
ğ‘–âˆˆIâˆ’
ğ‘ğ‘–+ ğœ†div
âˆ‘ï¸
ğ‘“âˆˆT
ğ‘¡ğ‘“
(3)
s.t.
âˆ‘ï¸
ğ‘Ÿâˆˆğ¹
ğ‘¥ğ‘Ÿâ‰¤1,
âˆ€ğ¹âˆˆF,
(4)
ğ‘¥ğ‘Ÿ, ğ‘ğ‘–, ğ‘¡ğ‘“âˆˆ{0, 1}.
(5)
The objective function in Eq. 3 prioritizes high recall (ğ‘¤ğ‘…) to en-
sure the ground truth is present in the candidate universe, while
imposing a penalty (ğ‘¤ğ‘ƒ) on expanding the search space with ir-
relevant images. The term ğœ†div serves as a regularizer to prevent
over-reliance on a single tool, i.e., single-model myopia. The con-
straint in Eq. 4 enforces truncation exclusivity: for any given tool
and query configuration, the solver must pick at most one optimal
ğ‘˜threshold, preventing redundant coverage of nested subsets.
Optimization Output. The solution to this MIP yields the optimal
set of positive atomic retrievals, denoted as R+
âˆ—, which produces the
recall-oriented candidate set of images U = Ã
ğ‘ŸâˆˆR+âˆ—Sğ‘Ÿ. While U
ensures high coverage of the ground truth, it inevitably contains ir-
relevant images since we omit the negative atomic retrieval. Hence,
the next stage performs precision-oriented filtering to remove the
remaining noise.
3.4
Precision-Oriented Composition MIP
While the first stage ensures high recall, the resulting candidate
image set U inevitably contains significant noise since the negative
atomic retrievals are not considered yet. Hence, we perform logical
filtering and refine U by composing atomic retrievals into a rigor-
ous boolean expression. To ensure stability and interpretability, we
restrict the search space to a fixed two-clause structure:
Sğ‘“ğ‘–ğ‘›ğ‘ğ‘™=
 Ã˜
ğ‘ŸâˆˆR+âˆ—âˆ—
Sğ‘Ÿ

|       {z       }
Positive Union
\
 Ã™
ğ‘ŸâˆˆRâˆ’âˆ—
Sğ‘Ÿ

|      {z      }
Negative Intersection
,
(6)
where R+
âˆ—âˆ—âŠ†R+
âˆ—âŠ†R+ and Râˆ’
âˆ—âŠ†Râˆ’denote the optimal subsets
of positive and negative atomic retrievals selected by this stage.
R+
âˆ—is the optimal solution from the previous stage. Crucially, we
employ Intersection for the negative clause to enforce conservative
exclusion: an image is removed only if all selected negative tools
agree it is irrelevant, thereby preventing accidental deletion of
ground-truth samples due to single-tool hallucinations.
Optimization Variables. We define binary decision variables ğ‘¥ğ‘Ÿâˆˆ
{0, 1} to indicate whether an atomic retrieval ğ‘ŸâˆˆR+
âˆ—âˆªRâˆ’is
selected. Based on Eq. 6, these decisions determine the state of each
candidate image ğ‘–âˆˆU via three logically coupled variables:
â€¢ ğ‘¢ğ‘–âˆˆ{0, 1} indicates if image ğ‘–is covered by the Positive Union,
i.e., retrieved by at least one selected positive atomic retrieval.
â€¢ ğ‘£ğ‘–âˆˆ{0, 1} indicates if image ğ‘–falls into the Negative Intersection,
i.e., retrieved by all selected negative atomic retrievals.
â€¢ ğ‘§ğ‘–âˆˆ{0, 1} indicates if image ğ‘–remains in the final set Sğ‘“ğ‘–ğ‘›ğ‘ğ‘™.
Logically, ğ‘§ğ‘–= 1 if and only if ğ‘¢ğ‘–= 1 AND ğ‘£ğ‘–= 0 (set difference).
MIP Formulation. The objective is to maximize the retention of
ground-truth images in the final set Sğ‘“ğ‘–ğ‘›ğ‘ğ‘™. We introduce a regular-
ization term ğœ†ğ‘Ÿğ‘’ğ‘”to mitigate the false positives and thereby prevent
trivial solutions, e.g., selecting no negative tools to maximize recall.
The MIP is formulated as follows:
max
{ğ‘¥ğ‘Ÿ}ğ‘ŸâˆˆR+âˆ—âˆªRâˆ’
âˆ‘ï¸
ğ‘–âˆˆUâˆ©I+
ğ‘§ğ‘–âˆ’ğœ†ğ‘Ÿğ‘’ğ‘”
âˆ‘ï¸
ğ‘–âˆˆIâˆ’
ğ‘§ğ‘–
(7)
s.t.
âˆ‘ï¸
ğ‘ŸâˆˆR+âˆ—
ğ‘¥ğ‘Ÿâ‰¥1,
(8)
ğ‘¥ğ‘Ÿ,ğ‘§ğ‘–âˆˆ{0, 1}.
(9)
The constraint in Eq. 8 ensures that at least one positive atomic
retrieval is selected, enforcing the positive union to be non-empty.
Optimization Output. The solution yields an optimal plan (R+
âˆ—âˆ—, Râˆ’
âˆ—)
consisting of the selected positive and negative atomic retrievals.


--- Page 5 ---
OSCAR: Optimization-Steered Agentic Planning for
Composed Image Retrieval
Conferenceâ€™17, July 2017, Washington, DC, USA
Together, they define an optimal trajectory of agentic composed im-
age retrieval for the training sample, i.e., a sequence of specific tool
calls associated with the rewritten queries and top-ğ‘˜truncations,
and the corresponding set operations leading to the ground truth.
These trajectories are stored in a Golden Library for online steering
during test-time inference for agentic composed image retrieval.
3.5
Optimization-Steered Inference for CIR
The optimization pipeline described in Sections 3.3 and 3.4 is compu-
tationally intensive and relies on ground-truth availability, making
it inapplicable during inference. However, the trajectories produced
by these stages represent golden demonstrations of ideal planning
logic. To transfer this optimized reasoning to the CIR agent, we
construct a Golden Library that distills these offline insights into
retrieval-augmented in-context demonstrations.
For each training instance, we employ Qwen3-Embedding-8B
to encode the problem context, which is the concatenation of the
modification query ğ‘ğ‘¡ğ‘¥ğ‘¡and the caption of reference image ğ‘ğ‘–ğ‘šğ‘”.
The caption is produced by Qwen3-VL-32B. The resulting problem-
context vector serves as the key for indexing, and the value is the
corresponding two-stage MIP solution. At test time, as shown in
Figure 2, given a composed query, we first compute its problem-
context embedding and then retrieve the top-ğ‘most similar cases
from the golden library based on cosine similarity. These trajecto-
ries serve as the in-context demonstrations for the VLM planner.
The output of planner is then fed to VLM verifier to produce the
final ranked results.
Crucially, this process does not merely provide the answers to
similar questions, but rather transfers generalized planning logic.
By observing how the optimal planner handled similar semantic
structures, the planner learns to replicate the underlying reason-
ing strategy, e.g., selecting the appropriate tool types, determining
the correct polarity for exclusion, and calibrating the truncation
threshold ğ‘˜. The robustness of this logic transfer is evidenced by
our experiments: OSCAR achieves state-of-the-art performance
even when the golden library is constructed using only 10% of
the available training data, demonstrating that the system learns
abstract meta-strategies rather than memorizing dataset-specific
solutions.
3.6
Discussion
We clarify key design choices and implementation details to ensure
the reproducibility and scientific rigor of the OSCAR framework.
Rigorous Enforcement of Logical Dependencies. In Sections 3.3
and 3.4, we described the relationships between primary decision
variables (i.e., ğ‘¥ğ‘Ÿ) and state variables (e.g., ğ‘ğ‘–,ğ‘¡ğ‘“, ğ‘§ğ‘–) using logical
implications to prioritize conceptual clarity. In our actual implemen-
tation, these logical dependencies are strictly enforced via standard
linear constraints (e.g., Big-M formulations and logical inequalities).
The complete, mathematically rigorous formulations, including all
auxiliary constraints and bounds, are provided in Appendix B.
Rationale for Two-Stage Optimization. Theoretically, the re-
call and precision objectives could be unified into a monolithic
MIP that solves for the global optimum in a single stage. However,
in practice, a single-stage formulation suffers from a severe com-
binatorial explosion, rendering it computationally intractable for
large-scale datasets. Our two-stage design adopts a prune-and-refine
strategy: the first MIP acts as a high-recall filter to reduce the entire
search space to a manageable candidate set U, making the second
composition MIP computationally tractable.
Negative Results and Failed Explorations. To provide a com-
prehensive view of the problem landscape and benefit the research
community, we include a detailed discussion of negative results
in the Appendix C, such as optimal-but-inefficient formulation
for composition MIP and alternative trajectory retrieval strategies.
By documenting these explorations, we aim to highlight not only
the effective design of OSCAR but also the non-trivial challenges
inherent in optimizing agentic retrieval trajectories.
4
Experiment
4.1
Experiment Setups
4.1.1
Datasets. Our main experiments are conducted on on three
representative public datasets for CIR: CIRCO [7], CIRR [26] and
FashionIQ [42]. Specifically, CIRR is split into a training set, a val-
idation set, and a testing set, while CIRCO has no training set.
FashionIQ is composed of three subsets (dress, shirt and toptee),
each containing a training set and a validation set. Following previ-
ous works [13, 18, 47], we evaluate on the FashionIQ validation set,
and on the CIRR and CIRCO testing sets through the official remote
evaluation server. We leverage the validation set of CIRCO, the
training set of CIRR and FashionIQ to build the golden library (only
10% of them). Details of the datasets are summarized in Table 1.
Table 1: The dataset statistics.
Split
Num
CIRCO
CIRR
FashionIQ
Dress
Shirt
Toptee
Training
#Query
-
28,225
5,985
5,988
6,027
#Image
-
16,939
11,452
19,036
16,121
Validation
#Query
220
4,181
2,017
2,038
1,961
#Image
123,403
2,297
3,817
6,346
5,373
Testing
#Query
800
4,148
-
-
-
#Image
123,403
2,315
-
-
-
4.1.2
Metrics. We adopt Recall@ğ¾to evaluate the retrieval perfor-
mance on CIRR and FashionIQ datasets. Since each query of CIRCO
dataset is assigned with multiple ground truth images, we use mean
average precision (mAP@ğ¾) to evaluate the detailed ranking qual-
ity. This is standard practice following previous works [7, 13, 18,
26, 42, 45, 47]. Additionally, we also provide Recall@ğ‘˜performance
on CIRCO dataset in Appendix D.1.
4.1.3
Baselines. To evaluate the effectiveness of our proposed OS-
CAR framework, we compare it with four types of baselines:
â€¢ Multimodal embedding models. These are general vision-
language embedding models. We choose Ops-MM-embedding-
v1-7B [28], RzenEmbed-v2-7B [19], VLM2Vec [21], B3-Qwen2-
7B [35], and QQMM-embed-v2 [43].
â€¢ Caption-based text embedding models. These baselines re-
duce CIR to a text-only retrieval problem by converting both
the composed query and candidate images into caption-style
textual representations. Specifically, the query is constructed by
concatenating the textual modification query and the reference
image caption, while each target image is represented by its cap-
tion. We choose Qwen3-VL-32B [4] for image captioning, and


--- Page 6 ---
Conferenceâ€™17, July 2017, Washington, DC, USA
Teng Wang et al.
select bge-m3 [12] and Qwen3-Embedding (0.6B, 4B, 8B) [24] as
text embedding baselines.
â€¢ CIR-dedicated methods. These approaches are specifically
designed for CIR, focusing on modeling the image-text query
composition via specialized fusion operators, query rewriting,
or learned transformations that better capture fine-grained mod-
ifications, including Pic2Word [30], SEARLE [7], SEARLE-XL-
OTI [7], CIReVL [22], LinCIR [17], LDRE [47], and FiRE [18].
â€¢ Agentic CIR methods. These methods rely on multi-step agen-
tic pipelines, often orchestrated as iterative loops under a ReAct-
style paradigm. These pipelines rely on heuristically designed
workflows without explicit or optimized tool-call planning, where
VLM-driven agents analyzes the query, calling embedding or
captioning models, and iteratively refines candidate results. We
select MRA-CIR [37], AutoCIR [13], and ğ‘‹ğ‘…[45] as representa-
tives. Our OSCAR framework also falls in this scope.
4.1.4
Implementation Details. Due to the page limitation, we pro-
vide the implementation details in Appendix E.
4.2
Main Results
Table 2 and Table 3 report the main results on standard CIR bench-
marks. We also provide Recall@ğ‘˜performance between the base-
lines and our methods on CIRCO dataset in Appendix D.1. From
the tables we can have the following observations:
â€¢ Overall Performance. Generally, OSCAR achieves the best per-
formance across all datasets, consistently outperforming other
baseline methods, with demonstrations derived from only 10%
training data. Notably, OSCAR is a training-free framework
based on open-sourced models, even surpassing other baselines
that are based on domain-specific-tuning or close-sourced LLMs.
â€¢ Comparison with single-embedding methods. OSCAR con-
sistently outperforms single-embedding baselines, including both
multimodal and text embedding approaches. For instance, OS-
CAR achieves 23.13% relative improvement of mAP@5 on CIRCO,
76.60% improvement of Recall@1 on CIRR, and 1.25% Recall@10
on FashionIQ. These results suggest that significant performance
gains can be achieved by optimization-steered planning and
structured composition over existing retrieval tools, rather than
relying on a single unified representation.
â€¢ Comparison with CIR-dedicated methods. OSCAR achieves
remarkable improvements over CIR-dedicated methods. Unlike
these methods that rely on specialized, task-trained components
for query fusion or rewriting, OSCAR adopts a tool-agnostic plan-
ning approach. It systematizes tool usage by formulating tool-
call planning as an explicit optimization problem and composing
outputs via structured set operations, free from dataset-specific
engineering. The results demonstrate that explicit, optimized
planning over general-purpose tools offers a highly effective and
reusable alternative to designing specialized CIR models.
â€¢ Comparison with agentic approaches. OSCAR also consis-
tently outperforms prior agentic retrieval methods. For example,
OSCAR reaches a 18.67% relative improvement of Recall@1 on
CIRR and a 15.54% improvement of Recall@10 on FashionIQ. No-
tably, these gains are achieved without iterative agent interaction
or multi-round reasoning. In contrast to heuristic exploration
of prior agentic methods, OSCAR guides VLMs using a golden
library of optimization-derived trajectories, resulting in more
effective tool-call planning and improved retrieval performance.
4.3
In-depth Analysis
4.3.1
Ablation Study. Table 4 analyzes the impact of golden trajec-
tory demonstrations and set-theoretic operations in OSCAR, from
which we can have the following observations:
â€¢ Trajectory demonstrations. Removing the optimal trajectory
demonstrations at inference time (w/o Demo.) generally de-
grades the performance, indicating that the optimization-derived
trajectories provide valuable signals about how retrieval tools
should be selected and composed. Moreover, the golden library
implicitly captures dataset- and query-specific patterns, includ-
ing query intent styles, preferred composition strategies, and
top-ğ‘˜selection biases, enabling the planner to adapt its trajectory
planning across distributions.
â€¢ Set-theoretic operations. Performance degrades substantially
when removing the set-difference or set-intersection operations
(w/o Set.Diff. and w/o Set.Diff. & Set.Int.). In particular, re-
moving both reduces composition to a simple union of retrieved
results, which is akin to multi-channel retrieval. As shown in
Table 4, this variant generally performs poorly, with particularly
large drops at Recall@50 across datasets. Naively aggregating
outputs from multiple atomic retrievals is insufficient, as union-
only composition cannot filter out items that match negative
attributes. Overall, these results underscore the importance of
comprehensive set operations in OSCAR.
4.3.2
Generalization of OSCAR. We further investigate the gen-
eralization of our proposed OSCAR in terms of different VLM
planner backbones. We evaluate with Qwen3-4B/8B/32B [44] and
InternVL3.5-38B [40] on the CIRCO dataset, and report the results
in Table 5. We obtain the following observations:
â€¢ OSCAR generalizes well across different VLM backbones and con-
sistently delivers substantial performance gains. This suggests
that our offline-derived golden library captures broadly applica-
ble planning logic, enabling OSCAR to function as a training-free,
plug-and-play framework with strong model compatibility.
â€¢ OSCARâ€™s gains over the VLM backbone largely rely on the
VLMâ€™s agentic reasoning and tool-calling abilities. Accordingly,
a stronger VLM (e.g., Qwen3-VL-32B) may better exploit the
golden library and yield larger improvements. This trend is also
evident in our preliminary experiments with Qwen2.5-VL [5]
and MiniCPM-V [49]: models with weaker tool-calling capabili-
ties often fail to follow tool-use instructions and cannot reliably
execute the required calls, leading to degraded performance.
4.3.3
Robustness to the Number of Demonstrations. We study how
the number of retrieved demonstrations from the golden library
(i.e., the number of shots) affects inference-time steering of the VLM
planner. Specifically, we vary the number of demonstrations in {1,
2, 3, 4} and evaluate CIR performance under otherwise identical
settings. As shown in Figure 3, OSCAR is largely insensitive to
the number of demonstrations. The performance remains stable
across all datasets as the shot count increases, with only minor
fluctuations. This again suggests that the golden library primar-
ily provides generalizable planning trajectories, rather than merely


--- Page 7 ---
OSCAR: Optimization-Steered Agentic Planning for
Composed Image Retrieval
Conferenceâ€™17, July 2017, Washington, DC, USA
Table 2: Performance comparison on CIRCO and CIRR datasets. m@ğ¾and R@ğ¾denote mAP@ğ¾and Recall@ğ¾, respectively.
Best results are highlighted in bold, while the second best are underlined. Relative improvement is calculated against the best
baseline result. â€œ-â€ means that the result is not reported in the original paper.
CIRCO
CIRR
Type
Method
Training
Free
m@5
m@10
m@25
m@50
R@1
R@5
R@10
R@50
Ops-MM-v1-7B
âœ“
13.56
15.96
18.61
19.68
1.90
50.29
66.68
91.64
RzenEmbed-v2-7B
âœ“
32.20
34.19
37.41
38.61
19.30
70.36
83.08
96.77
VLM2Vec
âœ“
3.40
4.07
5.08
5.74
0.10
26.48
41.35
71.74
B3_Qwen2_7B
âœ“
3.67
4.55
5.53
6.13
0.80
37.95
54.39
81.01
Multimodal
Embedding
QQMM-embed-v2
âœ“
45.92
47.13
50.39
51.45
28.98
73.66
82.98
96.72
bge-m3
âœ“
8.15
8.61
9.62
10.21
12.53
34.00
48.63
75.06
Qwen3-Embed-0.6B
âœ“
9.55
10.35
11.61
12.33
14.87
39.11
54.53
82.51
Qwen3-Embed-4B
âœ“
13.55
14.60
16.33
17.20
22.17
51.57
64.84
88.19
Text
Embedding
Qwen3-Embed-8B
âœ“
16.45
17.23
18.98
19.88
23.21
52.48
66.51
89.71
Pic2Word
âœ—
8.72
9.51
10.46
11.29
23.90
51.70
65.30
87.80
SEARLE
âœ—
11.68
12.73
14.33
15.12
24.24
52.48
66.29
88.84
SEARLE-XL-OTI
âœ—
10.18
11.03
12.72
13.67
24.87
52.31
66.29
88.58
CIReVL
âœ“
18.57
19.01
20.89
21.80
24.55
52.31
64.92
86.34
LinCIR
âœ—
12.59
13.58
15.00
15.85
25.04
53.25
66.68
-
LDRE
âœ“
23.35
24.03
26.44
27.50
26.53
55.57
67.54
88.50
CIR
Dedicated
FiRE
âœ—
31.03
32.08
34.40
35.50
43.33
74.02
83.51
95.83
MRA-CIR
âœ—
27.14
28.85
31.54
32.63
37.98
67.45
78.07
93.98
AutoCIR
âœ“
24.05
25.14
27.35
28.36
31.81
61.95
73.86
92.07
ğ‘‹ğ‘…
âœ“
31.38
32.88
35.46
36.50
43.13
73.59
83.09
94.05
Agentic
OSCAR (Ours)
âœ“
56.54
58.53
61.92
62.67
51.18
79.50
87.45
96.56
Relative Improvement (%)
23.13%
24.19%
22.88%
21.81%
18.67%
7.40%
5.25%
-0.22%
1 Shot
2 Shots
3 Shots
4 Shots
Number of Demonstrations
52
54
56
58
60
62
64
mAP (%)
CIRCO
m@5
m@10
m@25
m@50
1 Shot
2 Shots
3 Shots
4 Shots
Number of Demonstrations
40
50
60
70
80
90
100
110
Recall (%)
CIRR
R@1
R@5
R@10
R@50
1 Shot
2 Shots
3 Shots
4 Shots
Number of Demonstrations
40
50
60
70
Recall (%)
FashionIQ (Average)
R@10
R@50
Figure 3: Performance comparison w.r.t. different numbers of demonstrations (i.e., the number of shots) for inference-time
steering. â€œm" and â€œR" denotes mAP and Recall. The red dashed line denotes the zero-shot performance of OSCAR (i.e., mAP@50
on CIRCO, Recall@50 on CIRR and FashionIQ).
memorizing answers to similar queries. Consequently, adding more
demonstrations yields limited additional information beyond a sin-
gle trajectory.
4.4
Case Study
We present the case study on FashionIQ and CIRR datasets in Fig-
ure 4. The left panel shows a successful planning trajectory where
structured set operations retrieve the target image and rank it in
the first place. The right panel highlights the effect of the golden
library: based on the in-context golden demonstration, the planner
is steered towards more appropriate tool-call and set-operation
choices and consequently retrieves the ground-truth image.
A common failure mode we observe is a polarity mismatch in
negative evidence. The negative branch is intended to retrieve the
undesired concept (e.g., â€œmore than one beetleâ€ in Figure 1) and
remove it via Set Difference. However, the planner may instead
retrieve the negated concept (e.g., â€œone beetleâ€) and mistakenly use
it as the set to subtract. This reverses the intended set semantics and
can remove valid targets. With our optimization-derived guidance,
the planner is steered towards consistent negative evidence and set
composition, preventing such erroneous exclusion.


--- Page 8 ---
Conferenceâ€™17, July 2017, Washington, DC, USA
Teng Wang et al.
purple dress with black 
belt, is shorter floral 
patterned and has a belt
Reference Image
Modification Query
<tool_call>
{" name" : " embedding_search_QQMM_embed_v2" , 
" arguments" : {" query" : " Short purple floral dress with 
black belt, sleeveless, tiered, button front, white 
background" , " top_k" : 50 " reference_image" : " 1.png" }}
</tool_call>\n<tool_call>
{" name" : " caption_search_Qwen3_Embedding_8B" , 
" arguments" : {" query" : " Short purple floral dress with 
black belt, tiered skirt, button-front design, embroidered 
neckline " top_k" : 50, " reference_image" : " 1.png" }}
</tool_call>\n<tool_call>
{" name" : " embedding_search_ops_embedding" , 
" arguments" : {" query" : " Mid-long purple dress with 
floral embroidery and no belt " , " top_k" : 10, 
" reference_image" : " 1.png" }}
</tool_call>\n<tool_call>
{" name" : " set_operation" , " arguments" : {" steps" : 
[{" name" : " clause_0_step_0" , " op" : " INTERSECT" , 
" operands" : [0, 1]}, {" name" : " final" , " op" : 
" DIFFERENCE" , " operands" : [" clause_0_step_0" , 2]}]}}
</tool_call>
Planning Trajectory:
Retrieved Image Set
Intersection
Set
Difference Set 
(Correctly Deleted)
Target Image (Ranked to 1st)
Remove one beetle, 
Place  beetle prone on
small clods of dirt
Reference Image
Modification Query
... ...
{" name" : " caption_search_Qwen3_Embedding_8B" , " arguments" : 
{" query" : " two beetles mating, second beetle, partner beetle, mating 
pair" , " top_k" : 10, " reference_image" : " 1.png" }}
... ...
{" name" : " set_operation" , " arguments" : {" steps" : [{" name" : " positive" , 
" op" : " UNION" , " operands" : [0, 1, 2]}, {" name" : " negative" , " op" : 
" INTERSECT" , " operands" : [3, 4]}, {" name" : " final" , " op" : 
" DIFFERENCE" , " operands" : [" positive" , " negative" ]}]}}
Retrieved Image Set w/o Golden Library 
Retrieved Image Set w/ Golden Library 
{" name" : " embedding_search_QQMM_embed_v2" , " arguments" : 
{" query" : " A single beetle lying prone on small clods of dirt, no second 
beetle" , " top_k" : 50, " reference_image" : " 1.png" }}
...  ...
{" name" : " set_operation" , " arguments" : {" steps" : [{" name" : " final" , 
" op" : " UNION" , " operands" : [0, 1, 2, 3, 4, 5, 6, 7, 8]}]}}
Planning Trajectory w/o Golden Library:
Planning Trajectory w/ Golden Library:
Falsely Deleted
by  DIFFERENCE!
Target Image (Ranked to 1st)
remove an insect and another angle 
{" query" : A solitary dung beetle on a dung ball
from an alternate perspective...}
  ...[{" name" : " final" , " op" : " UNION"  ...}
In-Context Demonstration:
Figure 4: Case Studies on FashionIQ (left) and CIRR (right) datasets. The left part shows the correct tool call trajectory with the
ground truth image ranked to the first place. The right part illustrates the effectiveness of golden library, with whose help the
agent can avoid previous wrong tool calls and finally retrieve the ground truth image.
Table 3: Performance comparison on three subsets of Fash-
ionIQ datasets. R@ğ¾denotes Recall@ğ¾. Best results are high-
lighted in bold, while the second best are underlined.
Method
Dress
Shirt
Toptee
Average
R@10
R@50
R@10
R@50
R@10
R@50
R@10
R@50
Ops-MM-v1-7B
19.39
38.13
31.45
48.87
27.69
46.51
26.18
44.50
RzenEmbed-v2-7B
37.38
61.97
45.63
64.97
46.56
67.57
43.19
64.84
VLM2Vec
4.76
14.77
15.60
30.62
11.37
22.95
10.58
22.78
B3_Qwen2_7B
8.53
22.31
19.53
34.49
14.99
29.88
14.35
28.89
QQMM-embed-v2
36.44
60.09
46.07
65.65
46.35
68.49
42.95
64.74
bge-m3
10.81
23.75
20.66
33.66
15.96
27.33
15.81
28.25
Qwen3-Embed-0.6B
9.32
19.83
21.00
34.69
15.96
27.84
15.43
27.45
Qwen3-Embed-4B
12.69
27.02
26.69
40.68
21.88
36.05
20.42
34.58
Qwen3-Embed-8B
12.64
29.20
28.70
43.13
23.41
36.97
21.58
36.43
Pic2Word
20.20
40.20
26.20
43.60
27.90
47.40
24.77
43.73
SEARLE
20.48
43.13
26.89
45.58
29.32
49.97
25.56
46.23
SEARLE-XL-OTI
21.57
44.47
30.37
47.49
30.90
51.76
27.61
47.91
CIReVL
24.79
44.76
29.49
47.40
31.36
53.65
28.55
48.60
LinCIR
20.92
42.44
29.10
46.81
28.81
50.18
26.28
46.48
LDRE
22.93
46.76
31.04
51.22
31.57
53.64
28.51
50.54
FiRE
29.60
50.87
39.84
60.06
35.64
57.83
35.03
56.25
MRA-CIR
31.87
54.23
40.43
60.20
41.25
62.51
37.85
58.98
AutoCIR
24.94
45.81
34.00
53.43
33.10
55.58
30.68
51.61
ğ‘‹ğ‘…
28.71
52.50
38.91
56.82
43.91
62.57
37.18
57.30
OSCAR (Ours)
38.47
65.15
44.50
67.52
48.24
71.24
43.73
67.97
Rel. Improv. (%)
2.92%
5.13%
-3.40%
2.84%
3.61%
4.02%
1.25%
4.82%
4.5
Evaluation on Industrial Photo Galleries
To further assess the robustness of our OSCAR under real-world
industrial scenarios, we evaluate it on a private in-the-wild dataset
consisting of multiple user photo galleries. Each gallery corresponds
to an individual userâ€™s photo collection and is associated with text
queries for within-gallery image retrieval. This setting reflects prac-
tical album search scenarios, and the task becomes general text-
to-image retrieval, where image distributions are highly diverse
and queries often require non-trivial reasoning over user inten-
tions beyond simple visual descriptions. Specifically, we test on
three galleries composed of 1,069/1,466/1,047 real-world images
and 483/336/369 real-user queries.
Table 4: Ablation study on different variants of our OSCAR.
We remove different components to evaluate their contribu-
tion respectively. FIQ.Avg denotes average results over sub-
sets of the FashionIQ dataset. The best result is given in bold,
and the second-best value is underlined.
CIRCO
CIRR
FIQ.Avg
Variants
m@25
m@50
R@10
R@50
R@10
R@50
OSCAR (Ours)
61.92
62.67
87.45
96.56
43.73
67.97
w/o Demo.
59.72
59.84
75.01
82.02
46.57
61.62
w/o Set.Diff.
59.54
59.62
87.32
93.80
46.46
62.08
w/o Set.Diff. & Set.Int.
58.63
58.66
39.11
54.53
49.44
53.96
Table 5: Performance of different VLM backbones on CIRCO
dataset. Better results are given in bold. The best result from
each baseline category is also reported. â€œ-â€ means that the
baseline result is not reported in the original paper.
Model
m@10
m@25
m@50
R@10
R@25
R@50
QQMM-embed-v2
47.13
50.39
51.45
79.75
90.25
95.26
Qwen3-Embed-8B
17.23
18.98
19.88
38.75
53.12
65.62
FiRE
32.08
34.40
35.50
-
-
-
ğ‘‹ğ‘…
32.88
35.46
36.50
-
-
-
Qwen3-VL-4B
58.12
59.22
59.33
84.75
88.62
88.75
w/ OSCAR
58.11
61.18
61.91
84.75
93.75
96.12
Rel.Improv. (%)
-0.01%
3.31%
4.35%
0.00%
5.79%
8.30%
Qwen3-VL-8B
57.85
59.22
59.31
83.75
88.12
88.25
w/ OSCAR
58.28
61.65
62.40
85.38
94.88
97.12
Rel.Improv. (%)
0.074%
4.10%
5.21%
1.95%
7.68%
10.05%
Qwen3-VL-32B
58.32
59.72
59.84
82.75
86.88
87.00
w/ OSCAR
58.53
61.92
62.67
85.50
94.62
97.50
Rel.Improv. (%)
0.36%
3.68%
4.73%
3.32%
8.91%
12.07%
InternVL3.5-38B
57.88
60.29
60.62
84.88
90.88
91.75
w/ OSCAR
58.74
61.20
61.58
85.50
92.38
93.75
Rel.Improv. (%)
1.49%
1.51%
1.58%
0.72%
1.65%
2.18%
We report the performance in Table 6. We can observe that OS-
CAR consistently outperforms strong embedding-based baselines


--- Page 9 ---
OSCAR: Optimization-Steered Agentic Planning for
Composed Image Retrieval
Conferenceâ€™17, July 2017, Washington, DC, USA
Table 6: Performance of real-world text-to-image retrieval
on private industrial photo galleries. N@ğ¾and R@ğ¾denote
NDCG@ğ¾and Recall@ğ¾, respectively. Best results are high-
lighted in bold, while the second best are underlined.
Gallery1
Gallery2
Gallery3
Avg
Model
N@10
R@10
N@10
R@10
N@10
R@10
N@10
R@10
VLM2Vec
44.29
47.78
44.91
48.69
38.96
41.86
42.72
46.11
B3_Qwen2_7B
42.87
47.32
41.84
45.71
37.49
40.76
40.73
44.60
Ops-MM-v1
51.78
53.74
48.95
51.44
43.98
46.99
48.24
50.72
RzenEmbed-v2-7B
51.04
53.79
49.62
51.93
46.23
47.59
48.96
51.10
QQMM-embed-v2
51.64
54.43
48.42
51.29
44.60
46.63
48.22
50.78
bge-m3
39.61
41.98
38.36
41.47
39.87
41.41
39.28
41.62
Qwen3-Embed-0.6B
40.73
42.78
40.11
42.54
39.60
41.11
40.15
42.14
Qwen3-Embed-4B
43.06
44.93
44.55
46.80
41.48
43.07
43.03
44.93
Qwen3-Embed-8B
42.92
45.05
44.57
46.86
41.89
43.13
43.13
45.01
OSCAR (Ours)
56.01
65.28
53.12
57.43
58.70
63.92
55.94
62.21
Rel.Improve. (%)
8.17%
19.93%
7.05%
10.59%
26.97%
34.31%
14.26%
21.74%
across all galleries in terms of both metrics. On average, OSCAR
achieves a relative improvement of 14.26% in NDCG@10 and 21.74%
in Recall@10 over the best baseline. These results demonstrate that
our optimization-steered tool-call trajectory planning and struc-
tured set-theoretic composition remain effective beyond public
benchmarks, and generalize well to real-world user photo collec-
tions with natural data distributions and complex user intentions.
5
Conclusion
We present OSCAR, an optimization-steered planning framework
that reframes agentic composed image retrieval from a heuristic
search process into a principled trajectory planning problem. By
solving a two-stage MIP formulation offline, OSCAR mathemati-
cally derives optimal planning strategies, including query rewriting,
truncation selection, and boolean set composition, without human
annotation. These optimal trajectories are stored as golden demon-
strations to effectively steer the VLM planner during inference to
replicate complex reasoning logic in a single pass. Extensive ex-
periments on three public benchmarks and private industrial user
galleries demonstrate that OSCAR consistently outperforms SOTA
baselines, offering both superior accuracy and robust generalization.
Future work will explore extending this optimization-steered para-
digm to other complex reasoning tasks, such as multi-hop question
answering and open-ended tool use.
References
[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-
cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774
(2023).
[2] Lorenzo Agnolucci, Alberto Baldrati, Marco Bertini, and Alberto Del Bimbo. 2024.
iSEARLE: Improving Textual Inversion for Zero-Shot Composed Image Retrieval.
arXiv preprint arXiv:2405.02951 (2024).
[3] Anthropic. 2024. Introducing the Model Context Protocol. https://www.anthropic.
com/news/model-context-protocol Accessed: 2026-01-29.
[4] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng,
Lianghao Deng, Wei Ding, Chang Gao, and etc. 2025. Qwen3-VL Technical Report.
arXiv preprint arXiv:2511.21631 (2025).
[5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang,
Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang,
Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo
Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu,
and Junyang Lin. 2025. Qwen2.5-VL Technical Report. arXiv:2502.13923 [cs.CV]
https://arxiv.org/abs/2502.13923
[6] Egon Balas and Manfred W Padberg. 1972. On the set-covering problem. Opera-
tions Research 20, 6 (1972), 1152â€“1161.
[7] Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, and Alberto Del Bimbo. 2023.
Zero-shot composed image retrieval with textual inversion. In Proceedings of the
IEEE/CVF International Conference on Computer Vision. 15338â€“15347.
[8] Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Alberto Del Bimbo. 2023.
Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-
based Features. ACM Transactions on Multimedia Computing, Communications
and Applications (2023).
[9] Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Alberto Del Bimbo. 2022.
Effective conditioned and composed image retrieval combining clip-based fea-
tures. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition. 21466â€“21474.
[10] Dimitris Bertsimas and John N Tsitsiklis. 1997. Introduction to linear optimization.
Vol. 6. Athena scientific Belmont, MA.
[11] Chia-Yuan Chang, Zhimeng Jiang, Vineeth Rakesh, Menghai Pan, Chin-
Chia Michael Yeh, Guanchu Wang, Mingzhi Hu, Zhichao Xu, Yan Zheng, Ma-
hashweta Das, and Na Zou. 2025. MAIN-RAG: Multi-Agent Filtering Retrieval-
Augmented Generation. In Proceedings of the 63rd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long Papers). Association for Compu-
tational Linguistics, Vienna, Austria. https://aclanthology.org/2025.acl-long.131/
[12] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu.
2024. BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity
Text Embeddings Through Self-Knowledge Distillation. arXiv:2402.03216 [cs.CL]
[13] Zhangtao Cheng, Yuhao Ma, Jian Lang, Kunpeng Zhang, Ting Zhong, Yong Wang,
and Fan Zhou. 2025. Generative Thinking, Corrective Action: User-Friendly Com-
posed Image Retrieval via Automatic Multi-Agent Collaboration. In Proceedings
of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.
2. 334â€“344.
[14] Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z Wang. 2008. Image retrieval:
Ideas, influences, and trends of the new age. ACM Computing Surveys (Csur) 40,
2 (2008), 1â€“60.
[15] Ginger Delmas, Rafael S Rezende, Gabriela Csurka, and Diane Larlus. 2022.
ARTEMIS: Attention-based Retrieval with Text-Explicit Matching and Implicit
Similarity. In International Conference on Learning Representations.
[16] Dongdong Ge, Qi Huangfu, Zizhuo Wang, Jian Wu, and Yinyu Ye. 2022. Cardinal
Optimizer (COPT) user guide. arXiv preprint arXiv:2208.14314 (2022).
[17] Geonmo Gu, Sanghyuk Chun, Wonjae Kim, , Yoohoon Kang, and Sangdoo Yun.
2024. Language-only Training of Zero-shot Composed Image Retrieval. In Con-
ference on Computer Vision and Pattern Recognition (CVPR).
[18] Bohan Hou, Haoqiang Lin, Xuemeng Song, Haokun Wen, Meng Liu, Yupeng Hu,
and Xiangyu Zhao. 2025. FiRE: Enhancing MLLMs with fine-grained context
learning for complex image retrieval. In Proceedings of the 48th International ACM
SIGIR Conference on Research and Development in Information Retrieval. 803â€“812.
[19] Weijian Jian, Yajun Zhang, Dawei Liang, Chunyu Xie, Yixiao He, Dawei Leng,
and Yuhui Yin. 2025. Rzenembed: Towards comprehensive multimodal retrieval.
arXiv preprint arXiv:2510.27350 (2025).
[20] Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen.
2024. VLM2Vec: Training Vision-Language Models for Massive Multimodal
Embedding Tasks. arXiv preprint arXiv:2410.05160 (2024).
[21] Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu
Chen. 2024. Vlm2vec: Training vision-language models for massive multimodal
embedding tasks. arXiv preprint arXiv:2410.05160 (2024).
[22] Shyamgopal Karthik, Karsten Roth, Massimiliano Mancini, and Zeynep Akata.
2024. Vision-by-Language for Training-Free Compositional Image Retrieval.
International Conference on Learning Representations (ICLR) (2024).
[23] Jongseok Kim, Youngjae Yu, Hoeseong Kim, and Gunhee Kim. 2021. Dual com-
positional learning in interactive image retrieval. In Proceedings of the AAAI


--- Page 10 ---
Conferenceâ€™17, July 2017, Washington, DC, USA
Teng Wang et al.
Conference on Artificial Intelligence, Vol. 35. 1771â€“1779.
[24] Mingxin Li, Yanzhao Zhang, Dingkun Long, Keqin Chen, Sibo Song, Shuai Bai,
Zhibo Yang, Pengjun Xie, An Yang, Dayiheng Liu, et al. 2026.
Qwen3-VL-
Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art
Multimodal Retrieval and Ranking. arXiv preprint arXiv:2601.04720 (2026).
[25] Haoqiang Lin, Haokun Wen, Xuemeng Song, Meng Liu, Yupeng Hu, and Liqiang
Nie. 2024. Fine-grained textual inversion network for zero-shot composed im-
age retrieval. In Proceedings of the 47th International ACM SIGIR Conference on
Research and Development in Information Retrieval. 240â€“250.
[26] Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and Stephen Gould. 2021.
Image Retrieval on Real-Life Images With Pre-Trained Vision-and-Language
Models. In Proceedings of the IEEE/CVF International Conference on Computer
Vision (ICCV). 2125â€“2134.
[27] Rui Meng, Ziyan Jiang, Ye Liu, Mingyi Su, Xinyi Yang, Yuepeng Fu, Can Qin,
Zeyuan Chen, Ran Xu, Caiming Xiong, et al. 2025. Vlm2vec-v2: Advancing
multimodal embedding for videos, images, and visual documents. arXiv preprint
arXiv:2507.04590 (2025).
[28] OpenSearch-AI. 2025.
OpenSearch-AI/Ops-MM-embedding-v1-7B.
https://
huggingface.co/OpenSearch-AI/Ops-MM-embedding-v1-7B.
[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. Learning transferable visual models from natural language supervision.
In International conference on machine learning. PmLR, 8748â€“8763.
[30] Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate
Saenko, and Tomas Pfister. 2023. Pic2Word: Mapping Pictures to Words for
Zero-shot Composed Image Retrieval. arXiv:2302.03084 [cs.CV] https://arxiv.
org/abs/2302.03084
[31] Xuemeng Song, Haoqiang Lin, Haokun Wen, Bohan Hou, Mingzhu Xu, and
Liqiang Nie. 2025. A Comprehensive Survey on Composed Image Retrieval.
arXiv:2502.18495 [cs.MM] https://arxiv.org/abs/2502.18495
[32] Yucheng Suo, Fan Ma, Linchao Zhu, and Yi Yang. 2024. Knowledge-enhanced
dual-stream zero-shot composed image retrieval. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. 26951â€“26962.
[33] Yuanmin Tang, Jing Yu, Keke Gai, Jiamin Zhuang, Gang Xiong, Yue Hu, and Qi
Wu. 2024. Context-i2w: Mapping images to context-dependent words for accurate
zero-shot composed image retrieval. In Proceedings of the AAAI Conference on
Artificial Intelligence, Vol. 38. 5180â€“5188.
[34] Yuanmin Tang, Jue Zhang, Xiaoting Qin, Jing Yu, Gaopeng Gou, Gang Xiong,
Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qi Wu. 2025. Reason-
before-retrieve: One-stage reflective chain-of-thoughts for training-free zero-
shot composed image retrieval. In Proceedings of the Computer Vision and Pattern
Recognition Conference. 14400â€“14410.
[35] Raghuveer Thirukovalluru, Rui Meng, Ye Liu, Karthikeyan K, Mingyi Su, Ping Nie,
Semih Yavuz, Yingbo Zhou, Wenhu Chen, and Bhuwan Dhingra. 2025. Break-
ing the Batch Barrier (B3) of Contrastive Learning via Smart Batch Mining.
arXiv:2505.11293 [cs.CV] https://arxiv.org/abs/2505.11293
[36] Rong-Cheng Tu, Wenhao Sun, Hanzhe You, Yingjie Wang, Jiaxing Huang, Li Shen,
and Dacheng Tao. 2025. Multimodal Reasoning Agent for Zero-Shot Composed
Image Retrieval. arXiv preprint arXiv:2505.19952 (2025).
[37] Rong-Cheng Tu, Wenhao Sun, Hanzhe You, Yingjie Wang, Jiaxing Huang, Li Shen,
and Dacheng Tao. 2025. Multimodal Reasoning Agent for Zero-Shot Composed
Image Retrieval. arXiv:2505.19952 [cs.CV] https://arxiv.org/abs/2505.19952
[38] Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays.
2019. Composing text and image for image retrieval-an empirical odyssey. In
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.
6439â€“6448.
[39] Teng Wang, Wing Yin Yu, Zhenqi He, Zehua Liu, HaileiGong HaileiGong, Han
Wu, Xiongwei Han, Wei Shi, Ruifeng She, Fangzhou Zhu, and Tao Zhong. 2025.
BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical Modeling
Problem Solving. In Proceedings of the 63rd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers). Association for Computational
Linguistics.
[40] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei,
Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, Zhaokai Wang, Zhe Chen,
Hongjie Zhang, Ganlin Yang, Haomin Wang, Qi Wei, Jinhui Yin, Wenhao Li,
Erfei Cui, Guanzhou Chen, Zichen Ding, Changyao Tian, Zhenyu Wu, Jingjing
Xie, Zehao Li, Bowen Yang, Yuchen Duan, Xuehui Wang, Zhi Hou, Haoran Hao,
Tianyi Zhang, Songze Li, Xiangyu Zhao, Haodong Duan, Nianchen Deng, Bin
Fu, Yinan He, Yi Wang, Conghui He, Botian Shi, Junjun He, Yingtong Xiong,
Han Lv, Lijun Wu, Wenqi Shao, Kaipeng Zhang, Huipeng Deng, Biqing Qi, Jiaye
Ge, Qipeng Guo, Wenwei Zhang, Songyang Zhang, Maosong Cao, Junyao Lin,
Kexian Tang, Jianfei Gao, Haian Huang, Yuzhe Gu, Chengqi Lyu, Huanze Tang,
Rui Wang, Haijun Lv, Wanli Ouyang, Limin Wang, Min Dou, Xizhou Zhu, Tong
Lu, Dahua Lin, Jifeng Dai, Weijie Su, Bowen Zhou, Kai Chen, Yu Qiao, Wenhai
Wang, and Gen Luo. 2025. InternVL3.5: Advancing Open-Source Multimodal
Models in Versatility, Reasoning, and Efficiency. arXiv:2508.18265 [cs.CV] https:
//arxiv.org/abs/2508.18265
[41] Laurence A Wolsey and George L Nemhauser. 1999. Integer and combinatorial
optimization. John Wiley & Sons.
[42] Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen
Grauman, and Rogerio Feris. 2021. The Fashion IQ Dataset: Retrieving Images
by Combining Side Information and Relative Natural Language Feedback. CVPR
(2021).
[43] Youze Xue, Dian Li, and Gang Liu. 2025. Improve Multi-Modal Embedding
Learning via Explicit Hard Negative Gradient Amplifying.
arXiv preprint
arXiv:2506.02020 (2025).
[44] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng,
Bowen Yu, Chang Gao, Chengen Huang, and etc. 2025. Qwen3 Technical Report.
arXiv preprint arXiv:2505.09388 (2025).
[45] Zhongyu Yang, Wei Pang, and Yingfang Yuan. 2026. XR: Cross-Modal Agents for
Composed Image Retrieval. arXiv preprint arXiv:2601.14245 (2026).
[46] Zhenyu Yang, Shengsheng Qian, Dizhan Xue, Jiahong Wu, Fan Yang, Weiming
Dong, and Changsheng Xu. 2024. Semantic Editing Increment Benefits Zero-
Shot Composed Image Retrieval. In Proceedings of the 32nd ACM International
Conference on Multimedia. 1245â€“1254.
[47] Zhenyu Yang, Dizhan Xue, Shengsheng Qian, Weiming Dong, and Changsheng
Xu. 2024. LDRE: LLM-based Divergent Reasoning and Ensemble for Zero-Shot
Composed Image Retrieval. In Proceedings of the 47th International ACM SIGIR
Conference on Research and Development in Information Retrieval. 80â€“90.
[48] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language
Models. In International Conference on Learning Representations (ICLR).
[49] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi
Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng
Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang
Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024. MiniCPM-V: A GPT-4V
Level MLLM on Your Phone. arXiv:2408.01800 [cs.CV] https://arxiv.org/abs/
2408.01800


--- Page 11 ---
OSCAR: Optimization-Steered Agentic Planning for
Composed Image Retrieval
Conferenceâ€™17, July 2017, Washington, DC, USA
A
Related Work
A.1
Unified Embedding Retrieval for CIR
TIRG [38] introduces a gated residual composition mechanism,
where the reference image embedding is modified via text-conditioned
residual and gating functions. Building on VLMs, Baldrati et al. [9]
propose a conditioned composition framework that combines CLIP-
based visual and textual features through a learned fusion module.
CLIP4CIR [8] builds upon CLIP by fine-tuning the visionâ€“language
encoders and learning a dedicated combiner network to fuse the
reference image and modification text into a unified retrieval repre-
sentation. ARTEMIS [15] focuses on relative image descriptions and
contrastive supervision to improve sensitivity to semantic changes,
while still collapsing the composed query into a unified embedding.
DC-Net [23] adopts a dual-branch transformation strategy, sepa-
rately processing visual and textual features before fusing them
into a single retrieval representation.
Pic2Word [30] projects image features into the textual embed-
ding space as pseudo-word tokens, enabling imageâ€“text composi-
tion through pretrained text encoders. SEARLE [2] further extends
this idea by learning task-specific textual inversion or composition
modules that encode visual modifications implicitly within a sin-
gle embedding. Context-I2W [33] further maps images to context-
dependent pseudo-word tokens conditioned on the modification
intent, improving zero-shot composed retrieval under diverse edits.
FTI4CIR [25] introduces fine-grained textual inversion by map-
ping an image into a subject-oriented token together with multiple
attribute-oriented tokens, capturing finer visual details.
Beyond explicit composition mechanisms, some works focus
on improving embedding learning strategies for CIR without al-
tering the underlying retrieval formulation. Suo et al. [32] enrich
pseudo-word token mapping with an external database and an
additional concept-alignment stream, improving fine-grained se-
mantic alignment in the text embedding space. In contrast to token-
based mapping, SEIZE [46] adopts a training-free textual reasoning
pipeline that generates diverse captions and leverages LLM-based
compositional reasoning before retrieval.
Some works improve embedding learning for CIR without alter-
ing the composition mechanism. LinCIR [17] adopts a language-
only self-supervised training strategy for zero-shot CIR, while
FiRE [18] fine-tunes VLMs to enhance multimodal retrieval repre-
sentations.
Within the unified embedding paradigm, CIReVL [22] proposes a
training-free framework following this paradigm, while LDRE [47]
explores LLM-based divergent reasoning and ensemble-style aggre-
gation to improve coverage of possible semantic interpretations.
Caption-based approaches make intermediate reasoning steps ex-
plicit in the language space; however, their effectiveness depends
heavily on the quality of caption generation and rewriting, and
may suffer from information loss when compressing visual content
into text.
Recent advances in large-scale multimodal embedding models
have substantially improved zero-shot retrieval performance. How-
ever, evaluations such as MMEB [20, 27] show that no single em-
bedding consistently performs well across diverse CIR scenarios;
instead, models such as QQMM-embed [43], Rzenembed [19], Ops-
MM-embedding [28] and VLM2Vec [20] display complementary
strengths, motivating the use of multiple embedding tools rather
than a single retriever.
Despite architectural and training differences, all these approaches
ultimately reduce CIR to the unified-embedding model search,
where complex compositional constraints such as inclusion and
exclusion are implicitly encoded in a continuous representation.
A.2
Heuristic Agentic Retrieval for CIR
Recent work has explored agentic formulations for CIR, where the
retrieval process is decomposed into multiple steps with special-
ized roles (e.g., planning, retrieval, and correction), often involving
iterative refinement or feedback. AutoCIR [13] exemplifies this
approach by structuring CIR as a multi-component pipeline that
iteratively revises queries and retrieval results. MRA-CIR [36] fur-
ther advances this paradigm by introducing a multimodal reasoning
agent that explicitly decouples intent reasoning from retrieval ex-
ecution, bridging LLM-based logic and retrieval tools. Tang et al.
[34] propose a reflective reasoning-before-retrieval paradigm for
training-free zero-shot CIR, while ğ‘‹ğ‘…[45] introduces a cross-modal
agent framework that coordinates specialized tools through syner-
gistic planning across visual and textual modalities. While this line
of work demonstrates the potential of multi-step reasoning, includ-
ing generating alternative hypotheses and correcting failure cases,
such agentic pipelines are typically iterative and computationally
expensive due to repeated model interactions. Furthermore, these
methods largely rely on heuristic exploration rather than explicit
planning with optimality guarantees, leaving open the challenge of
how to formalize and optimize tool execution under explicit global
constraints.
A.3
Optimization-Based Planning
From a different perspective, Operations Research formulations
model decision-making problems by explicitly defining decision
variables, constraints, and objective functions, and solving them via
combinatorial optimization such as MIP [10, 39, 41]. This paradigm
enables principled and interpretable decision-making with explicit
optimization objectives, rather than relying on heuristic or trial-
and-error search.
Our work adopts this optimization perspective by formulating
tool-call trajectory generation for CIR as a two-stage MIP problem.
Each atomic retrieval corresponds to a discrete decision variable,
while inclusion, exclusion, and composition constraints are explic-
itly modeled through set-level constraints. This formulation allows
retrieval trajectories to be generated with explicit optimality objec-
tives, distinguishing our approach from heuristic agentic planning.
B
Implementation Details of two-stage MIPs
The common notations are summarized in Table 7.
B.1
Recall-Oriented Atomic Retrieval Selection
We introduce binary decision variables ğ‘¥ğ‘Ÿto indicate if a positive
atomic retrieval ğ‘ŸâˆˆR+ is selected. For image coverage, we use
binary indicators ğ‘¦ğ‘–for ğ‘–âˆˆI+ and ğ‘§ğ‘–for ğ‘–âˆˆIâˆ’to denote whether
a ground-truth image or a non-ground-truth image, respectively, is
retrieved by at least one selected atomic retrieval.


--- Page 12 ---
Conferenceâ€™17, July 2017, Washington, DC, USA
Teng Wang et al.
Table 7: Notation for the Problem Formulation
Symbol
Description
I
An image gallery
R+
Set of positive atomic retrievals
Râˆ’
Set of negative atomic retrievals.
ğ‘ŸâˆˆR+ âˆªRâˆ’
An atomic retrieval.
Sğ‘Ÿ
Set of candidate images returned by atomic re-
trieval call ğ‘Ÿ, where Sğ‘ŸâŠ†I.
I+
Set of ground-truth images, I+ âŠ†I.
Iâˆ’
Set of non-ground-truth images, Iâˆ’= I \ I+.
ğ‘–âˆˆI
A candidate image.
ğ‘ğ‘–ğ‘Ÿ
ğ‘ğ‘–ğ‘Ÿ= 1[ ğ‘–âˆˆI+ âˆ©Sğ‘Ÿ], ğ‘ŸâˆˆR+.
ğ‘ğ‘–ğ‘Ÿ
ğ‘ğ‘–ğ‘Ÿ= 1[ ğ‘–âˆˆIâˆ’âˆ©Sğ‘Ÿ], ğ‘ŸâˆˆR+.
R+
âˆ—
Set of positive atomic retrievals from Stage I.
U
Positive candidate universe induced by the se-
lected positive retrievals from Stage I.
ğ‘ğ‘–ğ‘Ÿ
ğ‘ğ‘–ğ‘Ÿ= 1[ ğ‘–âˆˆU âˆ©Sğ‘Ÿ], ğ‘ŸâˆˆR+
âˆ—.
ğ‘‘ğ‘–ğ‘Ÿ
ğ‘‘ğ‘–ğ‘Ÿ= 1[ ğ‘–âˆˆU âˆ©Sğ‘Ÿ], ğ‘ŸâˆˆRâˆ’.
R+
âˆ—âˆ—
Set of positive atomic retrievals from Stage II.
Râˆ’
âˆ—
Set of negative atomic retrievals from Stage II.
Moreover, we group atomic retrievals into different families F .
A family ğ¹âˆˆF consists of retrievals that share the same tool ğ‘“,
query Ë†ğ‘, and polarity ğ‘, differing only in their truncation threshold
ğ‘˜. We also introduce binary variables ğ‘¡ğ‘“to indicate whether any
selected atomic retrieval is associated with tool ğ‘“.
Based on the incidence definitions in Table 7, we define the
following upper-bound coefficients:
Ë†ğ‘ğ‘–:=
âˆ‘ï¸
ğ‘ŸâˆˆR+
ğ‘ğ‘–ğ‘Ÿ,
Ë†ğ‘ğ‘–:=
âˆ‘ï¸
ğ‘ŸâˆˆR+
ğ‘ğ‘–ğ‘Ÿ,
which respectively count the number of positive atomic retrievals
that retrieve image ğ‘–as a target or non-target instance.
The recall-oriented atomic retrieval selection is formulated in
the following MIP:
max
ğ‘¥,ğ‘¦,ğ‘§,ğ‘¡
ğ‘¤ğ‘…
1
|I+|
âˆ‘ï¸
ğ‘–âˆˆI+
ğ‘¦ğ‘–âˆ’ğ‘¤ğ¹
1
|Iâˆ’|
âˆ‘ï¸
ğ‘–âˆˆIâˆ’
ğ‘§ğ‘–+ ğœ†div
âˆ‘ï¸
ğ‘“
ğ‘¡ğ‘“
(1a)
s.t.
ğ‘¦ğ‘–â‰¤
âˆ‘ï¸
ğ‘ŸâˆˆR+
ğ‘ğ‘–ğ‘Ÿğ‘¥ğ‘Ÿâ‰¤Ë†ğ‘ğ‘–ğ‘¦ğ‘–,
âˆ€ğ‘–âˆˆI+,
(1b)
ğ‘§ğ‘–â‰¤
âˆ‘ï¸
ğ‘ŸâˆˆR+
ğ‘ğ‘–ğ‘Ÿğ‘¥ğ‘Ÿâ‰¤Ë†ğ‘ğ‘–ğ‘§ğ‘–,
âˆ€ğ‘–âˆˆIâˆ’,
(1c)
âˆ‘ï¸
ğ‘Ÿâˆˆğ¹
ğ‘¥ğ‘Ÿâ‰¤1,
âˆ€ğ¹âˆˆF,
(1d)
ğ‘¡ğ‘“â‰¤
âˆ‘ï¸
ğ‘ŸâˆˆTğ‘“
ğ‘¥ğ‘Ÿâ‰¤|Tğ‘“| ğ‘¡ğ‘“,
âˆ€ğ‘“,
(1e)
ğ‘¥ğ‘Ÿ, ğ‘¦ğ‘–, ğ‘§ğ‘–, ğ‘¡ğ‘“âˆˆ{0, 1}.
(1f)
The objective in (1a) prioritizes recall to retain sufficient can-
didates for subsequent composition, with diversity serving as a
tie-breaker.
Constraints (1b)â€“(1c) ensure that an image is treated as retrieved
if it is returned by at least one selected positive atomic retrieval.
Constraint (1d) therefore selects at most one variant per family,
avoiding redundant inclusion of nested result sets.
Constraint (1e) introduces a mild tool-type diversity bias, en-
couraging solutions that draw from multiple retrieval tools.
The solution to this MIP yields the optimal set of positive atomic
retrievals, denoted as R+
âˆ—, which produces the recall-oriented can-
didate set of images U = Ã
ğ‘ŸâˆˆR+âˆ—Sğ‘Ÿ.
B.2
Precision-Oriented Logic Composition
We use binary variablesğ‘¥ğ‘Ÿandğ‘¤ğ‘Ÿto indicate if a positive or negative
atomic retrieval ğ‘ŸâˆˆR+
âˆ—or ğ‘ŸâˆˆRâˆ’is selected, respectively. For each
imageğ‘–âˆˆU, the binary variableğ‘’ğ‘–indicates whetherğ‘–is included in
the final composed result. We further introduce an auxiliary binary
variable ğ‘”ğ‘–to indicate whether image ğ‘–lies in the intersection of
the selected negative atomic retrieval result sets.
The precision-oriented logic composition is formulated in the
following MIP:
max
ğ‘¥,ğ‘¤,ğ‘Ÿ,ğ‘”
âˆ‘ï¸
ğ‘–âˆˆUâˆ©I+
ğ‘’ğ‘–
(2a)
s.t.
ğ‘’ğ‘–â‰¤
âˆ‘ï¸
ğ‘ŸâˆˆR+âˆ—
ğ‘ğ‘–ğ‘Ÿğ‘¥ğ‘Ÿ,
âˆ€ğ‘–âˆˆU
(2b)
ğ‘’ğ‘–â‰¤1 âˆ’ğ‘”ğ‘–,
âˆ€ğ‘–âˆˆU
(2c)
ğ‘”ğ‘–â‰¤ğ‘‘ğ‘–ğ‘Ÿ+ (1 âˆ’ğ‘¤ğ‘Ÿ),
âˆ€ğ‘–âˆˆU, âˆ€ğ‘ŸâˆˆRâˆ’
(2d)
ğ‘”ğ‘–â‰¥1 âˆ’
âˆ‘ï¸
ğ‘ŸâˆˆRâˆ’
(1 âˆ’ğ‘‘ğ‘–ğ‘Ÿ) ğ‘¤ğ‘Ÿ,
âˆ€ğ‘–âˆˆU
(2e)
âˆ‘ï¸
ğ‘ŸâˆˆR+âˆ—
ğ‘¥ğ‘Ÿâ‰¥1
(2f)
ğ‘¥ğ‘Ÿ, ğ‘¤ğ‘Ÿ, ğ‘’ğ‘–, ğ‘”ğ‘–âˆˆ{0, 1}.
(2g)
The objective (2a) is precision-oriented by design and encourages
retaining as many target images as possible after filtering.
Constraint (2b) allows an item to be included in the final result
if it is retrieved by at least one selected positive atomic retrieval.
Constraint (2c) implements exclusion by preventing items that
satisfy the negative clause from being returned.
Constraints (2d)â€“(2e) ensure that ğ‘”ğ‘¢= 1 if and only if image ğ‘¢is
retrieved by all selected negative atomic retrievals.
Constraint (2f) prevents degenerate solutions by enforcing that
at least one positive tool is selected, ensuring a non-empty positive
clause in the two-clause composition.
The solution yields an optimal plan (R+
âˆ—âˆ—, Râˆ’
âˆ—) consisting of the
selected positive and negative atomic retrievals, which can be used
to construct the golden library.
C
Alternative Stage II: F1-Optimized DNF
Composition
Stage II adopts a fixed set-composition structure for stability. Here,
we consider a more flexible alternative that represents logical com-
position in disjunctive normal form (DNF) and selects clauses via
an MIP to directly optimize an F1-style objective.
To handle negative atomic retrievals, we convert set difference
into intersection via relative complements. Specifically, for any two
sets ğ´and ğµwith ğ´, ğµâŠ†U, we apply the identity
ğ´\ ğµ= ğ´âˆ©(U \ ğµ) = ğ´âˆ©ğµ,
(10)


--- Page 13 ---
OSCAR: Optimization-Steered Agentic Planning for
Composed Image Retrieval
Conferenceâ€™17, July 2017, Washington, DC, USA
where U denotes the positive candidate universe induced by Stage
I, and ğµdenotes the relative complement of ğµwith respect to U,
i.e., ğµ:= U \ ğµ.
Accordingly, each clause ğ¶ğ‘is defined as
ğ¶ğ‘=
Ã™
ğ‘Ÿâˆˆğ‘ƒğ‘
ËœSğ‘Ÿ,
ËœSğ‘Ÿ=
(
Sğ‘Ÿ,
ğ‘ŸâˆˆR+
âˆ—,
U \ Sğ‘Ÿ,
ğ‘ŸâˆˆRâˆ’,
(11)
where ğ‘ƒğ‘âŠ†R+
âˆ—âˆªRâˆ’denotes the set of atomic retrievals included
in clause ğ‘.
We enumerate clauses prior to optimization by imposing con-
straints on the maximum clause length and the number of negative
atomic retrievals per clause, yielding a finite candidate set {ğ¶ğ‘}ğ¶
ğ‘=1.
We introduce a binary indicator ğ‘¢ğ‘to denote whether clause ğ‘is
selected, and define the final retrieval result as the union of selected
clauses:
O =
Ã˜
ğ‘:ğ‘¢ğ‘=1
ğ¶ğ‘.
(12)
We define clause-level incidence matrices
ğ›¼ğ‘–ğ‘= 1[ ğ‘–âˆˆI+ âˆ©ğ¶ğ‘],
ğ›½ğ‘–ğ‘= 1[ ğ‘–âˆˆIâˆ’âˆ©ğ¶ğ‘],
together with the corresponding upper-bound coefficients
Ë†ğ›¼ğ‘–:=
âˆ‘ï¸
ğ‘
ğ›¼ğ‘–ğ‘,
Ë†ğ›½ğ‘–:=
âˆ‘ï¸
ğ‘
ğ›½ğ‘–ğ‘.
Binary indicators ğ‘¦ğ‘–and ğ‘§ğ‘–are used to indicate whether image ğ‘–is
included in the final result O. We further impose a budget constraint
that limits the number of selected clauses to at most ğ‘€.
We formulate logic composition in the following MIP:
max
ğ‘¢,ğ‘¦,ğ‘§
2
âˆ‘ï¸
ğ‘–âˆˆI+
ğ‘¦ğ‘–âˆ’ğœ†

|I+| +
âˆ‘ï¸
ğ‘–âˆˆI+
ğ‘¦ğ‘–+
âˆ‘ï¸
ğ‘–âˆˆIâˆ’
ğ‘§ğ‘–

âˆ’ğ›¼
âˆ‘ï¸
ğ‘
|ğ‘ƒğ‘|ğ‘¢ğ‘
(3a)
s.t.
ğ‘¦ğ‘–â‰¤
âˆ‘ï¸
ğ‘
ğ›¼ğ‘–ğ‘ğ‘¢ğ‘â‰¤Ë†ğ›¼ğ‘–ğ‘¦ğ‘–,
âˆ€ğ‘–âˆˆI+,
(3b)
ğ‘§ğ‘–â‰¤
âˆ‘ï¸
ğ‘
ğ›½ğ‘–ğ‘ğ‘¢ğ‘â‰¤Ë†ğ›½ğ‘–ğ‘§ğ‘–,
âˆ€ğ‘–âˆˆIâˆ’,
(3c)
âˆ‘ï¸
ğ‘
ğ‘¢ğ‘â‰¤ğ‘€,
(3d)
ğ‘¢ğ‘, ğ‘¦ğ‘–, ğ‘§ğ‘–âˆˆ{0, 1}.
(3e)
The F1 score corresponding to the selected clause union O =
Ã
ğ‘:ğ‘¢ğ‘=1 ğ¶ğ‘is given by
ğ¹1(O) =
2 Ã
ğ‘–âˆˆI+ ğ‘¦ğ‘–
|I+| + Ã
ğ‘–âˆˆI+ ğ‘¦ğ‘–+ Ã
ğ‘–âˆˆIâˆ’ğ‘§ğ‘–
,
(13)
and the parameter ğœ†in (3a) is updated via Dinkelbach iteration as
ğœ†â†
2 Ã
ğ‘–âˆˆI+ ğ‘¦ğ‘–
|I+| + Ã
ğ‘–âˆˆI+ ğ‘¦ğ‘–+ Ã
ğ‘–âˆˆIâˆ’ğ‘§ğ‘–
.
(14)
The objective in (3a) is obtained via the Dinkelbach reformulation
of the fractional F1 objective in (13). At each iteration, an MIP is
solved to maximize the difference between the numerator and the
denominator scaled by the current value of ğœ†, and ğœ†is updated to
the resulting F1 score until convergence. The complexity penalty
ğ›¼Ã
ğ‘|ğ‘ƒğ‘|ğ‘¢ğ‘discourages selecting long clauses. Constraints (3b)â€“
(3c) link clause selection to image-level inclusion through ğ‘¦ğ‘–and ğ‘§ğ‘–,
and Constraint (3d) limits the number of selected clauses.
In practice, allowing unions over many enumerated clauses often
produces bloated and unintuitive DNF expressions that are difficult
to interpret or learn. For example, the resulting composition may
take the form
(ğ´\ ğµ) âˆª(ğ´âˆ©ğ¶) âˆªğ·,
where the underlying retrieval logic is fragmented across multiple
overlapping clauses. To facilitate effective learning of tool-call tra-
jectories by the planner, we therefore adopt the less flexible but
more structured Stage II formulation in Section 3.4 in the main
method.
D
Additional Experiments
D.1
CIRCO Recall
In addition to the widely used mAP metric on CIRCO, we also report
Recall@K in Table 8. As shown in the table, OSCAR consistently
achieves higher recall than unified embedding baselines, indicating
its ability to retrieve a larger portion of target images.
Table 8: Recall comparison on CIRCO dataset. Best results
are highlighted in bold, while the second best are underlined.
Rel.Impr denotes the relative improvement over the best
baseline.
Method
R@5
R@10
R@25
R@50
Ops-MM-v1-7B
37.88
54.25
69.75
78.00
RzenEmbed-v2-7B
60.75
72.38
85.75
92.25
VLM2Vec
11.00
16.38
26.75
38.62
B3_Qwen2_7B
12.00
18.00
29.00
38.75
QQMM-embed-v2
71.12
79.75
90.25
95.26
bge-m3
14.62
20.62
32.25
40.62
Qwen3-Embed-0.6B
17.12
25.00
38.00
50.62
Qwen3-Embed-4B
25.62
37.38
51.25
62.62
Qwen3-Embed-8B
27.25
38.75
53.12
65.62
OSCAR (Ours)
75.50
85.50
94.62
97.50
Rel.Impr (%)
6.16
6.72
4.84
2.35
E
Implementation Details
For the tool planning process, we mainly use Qwen3-VL-32B [4]
as the planner agent. We adopt COPT [16] as the MIP solver and
discretize the top-ğ‘˜parameter described in Section 3.2 into the
range from 5 to 50 with a step size of 5. The detailed prompt for the
planner is demonstrated in Appendix I. We adopt Qwen3-VL-32B as
the verifier agent to perform a binary relevance judgment between
the candidates and queries. A detailed explanation of this binary
scoring formulation is provided in Appendix F.
For the tools used by the planner, we select Ops-MM-v1-7B [28],
RzenEmbed-v2-7B [19], QQMM-embed-v2 [43] as embedding-based
retrieval tools, and bge-m3 [12], Qwen3-Embed-4B [24], Qwen3-
Embed-8B [24] as caption-based retrieval tools. In addition, we
introduce a dedicated tool that performs structured set operations
on retrieved result sets (details are provided in Appendix H). All
retrieval and composition tools are deployed via MCP servers [3].
All experiments are conducted on A100 GPUs.
To emphasize generalization, we construct the golden library
using only a limited subset of the available training data: specifically,
we generate 220 cases for CIRCO, and sample 10% of the training
set to construct cases for CIRR and FashionIQ.


--- Page 14 ---
Conferenceâ€™17, July 2017, Washington, DC, USA
Teng Wang et al.
F
Binary Logit-Based Relevance Scoring
Our verifier computes a relevance score by comparing the likeli-
hood of generating â€œyesâ€ and â€œnoâ€ as the next decoded token. Let
V denote the vocabulary, and let ğ‘§ğ‘¡be the modelâ€™s output logit
for token ğ‘¡âˆˆV under a fixed context which consists of a ref-
erence image, a modification query, and a candidate image. The
corresponding prompt for the verifier is provided in Appendix I.
Step 1: Next-token distribution over the whole vocabulary.
By definition, VLM induces a normalized probability distribution
over the next token via softmax:
ğ‘ƒ(ğ‘¡) =
exp(ğ‘§ğ‘¡)
Ã
ğ‘£âˆˆV
exp(ğ‘§ğ‘£) ,
ğ‘¡âˆˆV.
(15)
In particular,
ğ‘ƒ(yes) =
exp(ğ‘§yes)
Ã
ğ‘£âˆˆV
exp(ğ‘§ğ‘£) ,
ğ‘ƒ(no) =
exp(ğ‘§no)
Ã
ğ‘£âˆˆV
exp(ğ‘§ğ‘£) .
(16)
Step 2: Restricting to a binary decision and renormaliza-
tion. We interpret relevance as a binary decision within the subset
{yes, no}.
ğ‘ƒ(yes | yes or no) =
ğ‘ƒ(yes)
ğ‘ƒ(yes) + ğ‘ƒ(no)
(17)
=
exp(ğ‘§yes)
Ã
ğ‘£âˆˆV exp(ğ‘§ğ‘£)
exp(ğ‘§yes)
Ã
ğ‘£âˆˆV exp(ğ‘§ğ‘£) +
exp(ğ‘§no)
Ã
ğ‘£âˆˆV exp(ğ‘§ğ‘£)
(18)
=
exp(ğ‘§yes)
exp(ğ‘§yes) + exp(ğ‘§no)
(19)
=
1
1 + exp ğ‘§no âˆ’ğ‘§yes

(20)
= ğœ ğ‘§yes âˆ’ğ‘§no
 .
(21)
where ğœ(ğ‘¥) =
1
1+exp(âˆ’ğ‘¥) is the sigmoid function. In our imple-
mentation, we use this binary probability as the relevance score
ğ‘ ğ‘¢= ğœ(ğ‘§ğ‘¦ğ‘’ğ‘ âˆ’ğ‘§ğ‘›ğ‘œ).
G
Statistics of Candidate Pruning
Table 9 reports the average number of candidates removed by the
difference operation across different datasets. When applied appro-
priately, the difference operation can effectively reduce the size of
the candidate set, thereby alleviating the computational burden on
the verifier.
Table 9: Statistics of candidate pruning by the difference
operation. Avg. Removed denotes the average number of
candidates excluded when difference is used.
Dataset
Avg. Removed
CIRR
10.20
CIRCO
3.69
FashionIQ-Shirt
3.67
FashionIQ-Dress
3.05
FashionIQ-Toptee
2.51
H
Set Operations
We consider three types of set operations: union, intersection, and
difference. These operations enable explicit composition of results
returned by multiple retrieval tools, producing a concise candidate
set for subsequent ranking. Specifically, union is used to improve
recall by aggregating candidates from different tools, intersection
increases precision by retaining only commonly retrieved results,
and difference filters out candidates that are likely to be irrelevant.
I
Prompts
For completeness, we present the planner, image captioning, and
verifier prompts used in our OSCAR framework below.


--- Page 15 ---
OSCAR: Optimization-Steered Agentic Planning for
Composed Image Retrieval
Conferenceâ€™17, July 2017, Washington, DC, USA
Planner Agent Prompt
Role: You are a search planner for Composed Image Retrieval (CIR).
Task: Given a reference image and a modification query describing â€œChangesâ€ and â€œShared conceptâ€, find target images that match the modified concept.
Available Models:
â€¢ Embedding models: rzenembed, ops_embedding, QQMM_embed_v2
â€¢ Caption models: bge_m3, Qwen3_Embed_4B, Qwen3_Embed_8B
Available Tools:
â€¢ embedding_search_{model_name}(query, top_k, reference_image): Visual similarity search. Best for visual attributes, colors, spatial relationships.
â€¢ caption_search_{model_name}(query, top_k, reference_image): Semantic text search on captions. Best for semantic descriptions, conceptual changes.
â€¢ set_operation(steps): Combine results (INTERSECT, UNION, DIFFERENCE). MUST be the last tool call when using 2+ tools.
CRITICAL - Set Operation Rules:
â€¢ INTERSECT (AND): Return items in ALL operands. Use when results must satisfy multiple criteria.
â€¢ UNION (OR): Return items in ANY operand. Use for maximum coverage.
â€¢ DIFFERENCE (A - B): Return items in first but NOT in second. Use to exclude specific unwanted features.
Strategy - How to Handle CIR Queries:
Step 1: Analyze Additions, Removals, or Changes
â€¢ When to use DIFFERENCE: Only when an object is COMPLETELY ABSENT (e.g., â€œhas no Xâ€, â€œwithout Xâ€).
â€¢ When NOT to use DIFFERENCE: If the object exists but changed attributes (e.g., â€œhas fewer peopleâ€, â€œis in greyscaleâ€, â€œhas different colorâ€).
â€¢ Negative Query Rule: Negative queries must ONLY contain the absent object. NEVER include shared concepts (e.g., if shared is â€œcakeâ€, negative
query should be â€œtoppingsâ€, NOT â€œcake with toppingsâ€).
Step 2: Generate Tool Calls
â€¢ Positive tools: Search for â€œShared conceptâ€ + additions.
â€¢ Negative tools: ONLY search for objects that DONâ€™T EXIST.
Step 3: Set Operation (Mandatory)
â€¢ Union all positive tools â†’â€œpositiveâ€.
â€¢ Intersect all negative tools â†’â€œnegativeâ€.
â€¢ DIFFERENCE(positive, negative) â†’â€œfinalâ€.
Static Reference Examples (Learn the format!)
Below are examples showing the correct tool call FORMAT (not related to your query):
Query: Changes: shows two people and has a more colorful background. Shared concept: a girl with a
traditional Chinese umbrella.
Reference image: 000000271520. jpg
Reference caption: A young woman in traditional Chinese attire performs gracefully on a dimly lit
stage , holding a large , ornate red parasol.
Analysis:
- Shared: girl with Chinese umbrella/parasol , traditional attire
- Changes: ADD two people , ADD colorful background (instead of dimly lit stage)
- No removals , No DIFFERENCE needed , just UNION
- Search strategy: Find images with girl + Chinese umbrella + multiple people + colorful setting
<tool_call >
{"name": "embedding_search_QQMM_embed_v2", "arguments": {"query": "Two people with traditional
Chinese umbrella parasol , colorful vibrant background", "top_k": 20, "reference_image": "
000000271520. jpg"}}
</tool_call >
<tool_call >
{"name": "embedding_search_rzenembed", "arguments": {"query": "Girl with Chinese parasol umbrella ,
two people together , colorful setting", "top_k": 15, "reference_image": "000000271520. jpg"}}
</tool_call >


--- Page 16 ---
Conferenceâ€™17, July 2017, Washington, DC, USA
Teng Wang et al.
<tool_call >
{"name": "embedding_search_ops_embedding", "arguments": {"query": "Chinese umbrella , traditional
dress , multiple people , colorful background", "top_k": 15, "reference_image": "000000271520. jpg"
}}
</tool_call >
<tool_call >
{"name": "caption_search_Qwen3_Embedding_8B", "arguments": {"query": "girl with Chinese umbrella
parasol , two people , colorful vibrant background", "top_k": 10, "reference_image": "000000271520.
jpg"}}
</tool_call >
<tool_call >
{"name": "set_operation", "arguments": {"steps": [
{"name": "final", "op": "UNION", "operands": [0,
1, 2, 3]}]}}
</tool_call >
Example2 - DIFFERENCE (Remove elements)
Query: Changes: has no toppings and shows the rest in the background. Shared concept: a slice of cake
on a plate in the foreground.
Reference image: 000000119289. jpg
Reference caption: A slice of chocolate cake with white frosting and cherry toppings sits on a white
plate.
Analysis:
- Shared: slice of cake on plate , foreground
- Changes: REMOVE toppings (no cherry , no frosting decorations), ADD background elements
- has no toppings , toppings are ABSENT: Use DIFFERENCE
-
CRITICAL: cake EXISTS in target! NEVER include cake in negative query!
Search strategy: Find cake slices WITHOUT toppings/decorations , with background visible.Use
DIFFERENCE to exclude images with visible toppings.
Warning: Negative query ONLY contains toppings/cherries/decorations , NOT cake!
<tool_call >
{"name": "embedding_search_QQMM_embed_v2", "arguments": {"query": "Plain slice of cake on plate ,
simple cake , background visible", "top_k": 20, "reference_image": "000000119289. jpg"}}
</tool_call >
<tool_call >
{"name": "embedding_search_rzenembed", "arguments": {"query": "Simple cake slice on plate in
foreground , background scene", "top_k": 15, "reference_image": "000000119289. jpg"}}
</tool_call >
<tool_call >
{"name": "embedding_search_ops_embedding", "arguments": {"query": "Cake slice plate foreground , plain
simple , background visible", "top_k": 25, "reference_image": "000000119289. jpg"}}
</tool_call >
<tool_call >
{"name": "caption_search_Qwen3_Embedding_8B", "arguments": {"query": "plain cake slice on plate ,
background visible", "top_k": 10, "reference_image": "000000119289. jpg"}}
</tool_call >
<tool_call >
{"name": "embedding_search_QQMM_embed_v2", "arguments": {"query": "cherry topping , frosting
decorations , fruit toppings , garnish", "top_k": 20, "reference_image": "000000119289. jpg"}}
</tool_call >
<tool_call >
{"name": "embedding_search_rzenembed", "arguments": {"query": "cherries , whipped cream topping ,
dessert decorations , garnish", "top_k": 20, "reference_image": "000000119289. jpg"}}
</tool_call >
<tool_call >
{"name": "caption_search_Qwen3_Embedding_8B", "arguments": {"query": "toppings , cherries , frosting
decorations , garnished dessert", "top_k": 10, "reference_image": "000000119289. jpg"}}
</tool_call >
<tool_call >


--- Page 17 ---
OSCAR: Optimization-Steered Agentic Planning for
Composed Image Retrieval
Conferenceâ€™17, July 2017, Washington, DC, USA
{"name": "set_operation", "arguments": {"steps": [{"name": "positive", "op": "UNION", "operands": [0,
1, 2, 3]},
{"name": "negative", "op": "INTERSECT", "operands": [4, 5, 6]}, {"name": "final", "
op": "DIFFERENCE", "operands": ["positive", "negative"]}]}}
</tool_call >
RETRIEVED SIMILAR CASES
Below are some similar cases retrieved based on your query and reference image. You can learn these cases patterns for:
â€¢ Query Rewrite
â€¢ Tool selection and composition
â€¢ top_k values
{retrieved_cases}
Now solve the following task: Modification Query: {query}
Reference Image: {ref_img}
Reference Image Caption: {ref_caption}
Now generate tool_calls to retrieve target images based on the provided Modification Query and Reference Image.
Image Captioning Prompt
Describe this image comprehensively:
1. Main subjects and their attributes
2. Actions, interactions, and relationships
3. Scene, background, and context
4. Any visible text (signs, labels, watermarks, etc.)
Return your response as JSON:
```json
{
"caption": "your comprehensive description here"
}
```
Verifier Agent Prompt
{Reference image}
{Candidate image}
Modification query: {<query>}
Instruction:
You are given two images.
The first image is the reference image.
The second image is a candidate image.
Given the modification query: {<query>}, determine whether the candidate image matches what you would expect after applying the modification to the
reference image.
Answer with â€œyesâ€ or â€œnoâ€ only.
