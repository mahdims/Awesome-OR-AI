--- Page 1 ---
Relatively-Secure LLM-Based Steganography via
Constrained Markov Decision Processes
Yu-Shin Huang, Chao Tian, and Krishna Narayanan
Dept. of Electrical and Computer Engineering
Texas A&M University
College Station, TX 77845, USA
Email: {yushin1002, chao.tian, krn}@tamu.edu
Lizhong Zheng
Dept. of Electrical Engineering and Computer Science
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
Email: lizhong@mit.edu
Abstractâ€”Linguistic steganography aims to conceal informa-
tion within natural language text without being detected. An
effective steganography approach should encode the secret mes-
sage into a minimal number of language tokens while preserving
the natural appearance and fluidity of the stego-texts. We present
a new framework to enhance the embedding efficiency of stego-
texts generated by modifying the output of a large language
model (LLM). The novelty of our approach is in abstracting the
sequential steganographic embedding process as a Constrained
Markov Decision Process (CMDP), which takes into consideration
the long-term dependencies instead of merely the immediate
effects. We constrain the solution space such that the discounted
accumulative total variation divergence between the selected
probability distribution and the original distribution given by
the LLM is below a threshold. To find the optimal policy,
we first show that the functional optimization problem can be
simplified to a convex optimization problem with a finite number
of variables. A closed-form solution for the optimal policy is
then presented to this equivalent problem. It is remarkable that
the optimal policy is deterministic and resembles water-filling
in some cases. The solution suggests that usually adjusting the
probability distribution for the state that has the least random
transition probability should be prioritized, but the choice should
be made by taking into account the transition probabilities at all
states instead of only the current state.
I. INTRODUCTION
In steganography, a sender Alice attempts to discreetly
communicate a hidden message to Bob, the receiver, using
a carrier signal, which can be in the form of text, image,
audio, or video [1]â€“[3]. Alice transmits the encoded signal
to Bob through a public channel, which is under surveillance
by an eavesdropper, Eve, who seeks to detect the existence of
any hidden content. A well-known example is the prisoner
problem [4] where Eve is the prisoner guard in charge of
deciding whether to deliver a letter to a prisoner. In this work,
we consider using natural language text as the carrier signal,
where the encoded message is referred to as the stego-text;
see Fig. 1 for an illustration.
Traditional text steganography methods usually embed hid-
den messages into an existing cover text [5]â€“[7]. However,
with the development of generative models, especially large
language models (LLMs), a new approach known as coverless
steganography has emerged, proving to be highly effective and
secure. Generative-method-based approaches produce stego-
texts that mimic natural language while also allowing for
Public Channel
ğŸ‘©
ğŸ‘¨
ğŸ”
Letâ€™s meet tomorrow @ 3
I made myself 
a cup of coffee 
today.
011011010010101101010010101101
Encrypt
Stega 
Encode
Stega 
Decode
Decrypt
I made myself 
a cup of coffee 
today.
011011010010101101010010101101
Letâ€™s meet tomorrow @ 3
Fig. 1: Steganography system
more efficient encoding of hidden information compared to
traditional approaches [8]â€“[18]. In this approach (discussed
in detail in Section II), a language model (LM) is used to
produce the conditional probabilities p(xt|xtâˆ’1, . . . , xtâˆ’k) for
the token at time t and the secret message is embedded in the
sampling process that samples from pt to generate xt. It is
known that P
t H(pt) bits of information can be hidden in an
imperceptible manner, where H is the binary entropy function.
In many scenarios, the security requirement in steganog-
raphy can be relaxed, particularly when the eavesdropper
Eve is computation-bounded (e.g., in a mobile device). This
consideration is in fact already implicit in several previous
works that adopt â€œnear-imperceptibilityâ€ steganography [19],
[20] to improve computation efficiency. Further generalizing
this idea, we can view the steganography coding problem
with a less-stringent security requirement as replacing the
LM-based probability distribution pt with another distribution
pâ€²
t such that E[H(pâ€²
t)] â‰¥E[H(pt)], while maintaining an
imperceptibility constraint (discussed later) and we sample
the next token according to pâ€²
t. We refer to this approach
as relatively-secure steganography. In this work, we consider
choosing such a replacement pâ€²
t for the purpose of more effi-
cient information embedding. This consideration is particularly
important in LLM-based approaches, as several works [21]â€“
[23] have highlighted the low embedding capacity, since pt
becomes more deterministic, leading to lower entropy and a
reduced secret message embedding rate.
In a recent work [24], we considered the relatively-secure
steganography coding problem, and optimized the replacement
probability distribution of each token for maximum (con-
ditional) entropy, under a constraint on the deviation from
arXiv:2502.01827v1  [cs.IT]  3 Feb 2025


--- Page 2 ---
Sample next token xt+1
Entropy of pâ€²t
Agent
     Total 
  Variation                   
LLM
pt
St = {x0, x1, â‹¯xt}
Ct
Rt
St+1
Rt+1
Ct+1
Adjusted Probability pâ€²t
Fig. 2: Relatively-secure LM-based steganography system
the LM-based next token distribution. While this approach
increases the embedding capacity, the distribution is optimized
for each token position independently of others. One critical
observation that motivated the current work is that the choice
for the replacement probability distribution for the current
token impacts not only the embedding efficiency of the im-
mediate token, but also all future tokens.
To take into account such future impact, we propose to
model the process as a decision-making agent adjusting the
probability distribution for generating the next token. Selecting
a particular probability distribution induces a reward Rt and
a cost Ct: the reward is the entropy of the distribution (higher
embedding efficiency), and the cost is the divergence from
the original LM-generated distribution (unnatural text). The
state of the system is the preceding tokens in the context
window of the LM. As demonstrated in Fig. 2, this in fact
closely resembles a Constrained Markov Decision Process
(CMDP) [25]. Therefore, in this work, we formulate the
relative steganography coding problem as a CMDP.
LMs are clearly too complex to model precisely. To gain
insights into how pâ€²
t should be selected, we abstract and
simplify the LM into the simplest finite state model, i.e., a two-
state model. Our main contribution is a closed-form solution
for the optimal policy in this setting. To obtain this optimal
policy, we simplify the problem from a functional optimization
problem into a finite convex problem, and then establish its
optimal solution in closed form. Remarkably, this implies that
the optimal policy is deterministic, whereas the optimal policy
for general CMDPs is usually not deterministic. Moreover,
depending on the transition probability structure, the optimal
policy resembles a water-filling solution in some cases. The
solution suggests that usually adjusting the probability dis-
tribution for the state that has the least random transition
probability should be prioritized, but the choice should be
made by taking into account the transition probabilities at all
states instead of only the current state.
II. PRELIMINARIES
A. LLM-based Steganography
An LLM produces the next token probability distribution
pt = P(xt|xtâˆ’1, xtâˆ’2, . . . , xtâˆ’k) conditioned on the preced-
ing tokens in the context window of size k, which can be used
to sample tokens auto-regressively to produce natural language
texts [26]â€“[28]. We assume that the secret message bits are
first encrypted using a shared random key between Alice and
Bob, such that the encrypted message m is a sequence of i.i.d.
uniformly distributed bits. The key idea in embedding m into
a stego-text is to first use the decompressor of a compression
algorithm to map the sequence of i.i.d bits m into a sequence
of random variables {Xt} such that Xt âˆ¼pt at time t. The
realization of the random variable at time t, namely xt, is the
token in the stego-text at time t, and we continue this process
auto-regressively.
Arithmetic coding [29] is an elegant way to generate xtâ€™s
for any sequence of distributions pt. Consider the example
shown in Fig. 3 where a 5-bit encrypted message 10111 is to
be embedded. The bit string is represented by the interval I =
[0.101110000 Â· Â· Â·2 , 0.1011111111 Â· Â· Â·2) â‰ƒ[0.71875, 0.75). We
input the starting prompt â€œWhat is the probability ofâ€ to the
LLM which generates p1 at time t = 1, which induces a
partition of the interval [0, 1) and the interval I corresponds
to the token â€œwinningâ€, implying the first token of the stego-
text is x1 = â€œwinningâ€. Once x1 is selected, the probability
distribution p2 is generated by adding x1 to the prompt. The
next token x2 is chosen based on the partition of [0, 1) induced
by p2. This process is repeated to generate a stego-text of
length n. During the decoding phase, Bob parses the received
text into tokens and reproduces the secret bits using the exact
same sequence of distributions {pt}.
This procedure is optimal in that nearly Pn
t=1 H(pt) bits
from m can be mapped into a stego-text of length n. It
produces perfectly-secure steganography since xt âˆ¼pt.
B. Embedding Efficiency, Deviation, and State Space Model
Low entropy distributions ptâ€™s imply that fewer bits of the
secret message can be embedded in the stego-text. To mitigate
this issue, we propose to sample from a different distribution
pâ€²
t, instead of the LLM-produced next token distribution pt. At
time-t, the problem can be naturally formulated as maximizing
the entropy of pâ€²
t, subject to the constraint that the deviation
from pt is small; this token-wise problem was studied in [24].
Mathematically, the sequential LLM-based sampling pro-
cess can be viewed as a state-space model, where the state
is the preceding length-k token sequence. In this work, we
further simplify the state-space model to the binary state-
space model where ps = P(St+1 = 0|St = s). We write
ps â‰œ[ps, 1 âˆ’ps] for simplicity.
C. An Introduction to CMDP
CMDP consists of several key components: the state space
S, action space A, transition probabilities P, reward function
r : S Ã— A â†’R, a cost constraint g : S Ã— A â†’R, a policy
Ï€ : S â†’âˆ†A, the initial state distribution d, and the discount
factor Î³ âˆˆ[0, 1), where âˆ†A represents a probability simplex
with |A| dimensions. The goal of policy optimization is to find
the optimal policy Ï€(a|s) that guides the agent in selecting


--- Page 3 ---
P1
P2
Pn
Encrypted : 0.10111
[0.71875, 0.75)
1
0.8
0.65
0
â‹®
0.80
0.65
0.69
â‹®
0.77
0.70
0.75
0.72
â‹®
Starting Prompt: What is the probability of 
winning
 a
?
success
 a
â‹®
the
this
ticket
in
draw
â‹®
P3
0.735
Stega-text : What is the probability of winning a major prize in
winning
x1 =
a
x2 =
in
xn =
1
0.8
0.65
0
â‹®
0.80
0.65
0.69
â‹®
0.77
0.70
0.75
0.72
â‹®
winning
 a
?
success
 a
â‹®
the
this
â‹®
ticket
in
draw
â‹®
0.735
[0.72, 0.735) = [0.1011100.., 0.10111100..)
â€¦
â€¦
P1
P2
Pn
P3
Public 
Channel
Fig. 3: Arithmetic coding based steganography
actions a âˆˆA(s) for states s âˆˆS in order to maximize the
cumulative discounted reward over the entire timeframe, i.e.,
R = E
" âˆ
X
t=0
Î³tr(st, at)
#
,
(1)
subject to the constraint
C = E
" âˆ
X
t=0
Î³tg(st, at)
#
â‰¤b.
(2)
It is often more convenient to consider the discounted state-
action pair visitation frequency d(s, a) than the policy Ï€(a|s),
also known as the occupancy measure. This quantity deter-
mines the likelihood of visiting a specific state-action pair
(s, a) over time, defined as
d(s, a) = (1 âˆ’Î³)
âˆ
X
t=0
Î³tP(St = s, At = a)
(3)
The reward and cost functions can now be represented in a
structured form using d(s, a).
R =
X
sâˆˆS
X
aâˆˆA(s)
r(s, a)d(s, a),
(4)
C =
X
sâˆˆS
X
aâˆˆA(s)
g(s, a)d(s, a).
(5)
Note that d(s, a) must be non-negative, and the flow conserva-
tion constraint (closely related to the Bellmanâ€™s equation [30]
in the dual optimization) requires that the total inflow to state
s equals the total outflow:
X
aâˆˆA(s)
d(s, a) = d(s) + Î³
X
sâ€²âˆˆS
X
aâ€²âˆˆA(sâ€²)
P(s | sâ€², aâ€²) Â· d(sâ€², aâ€²).
(6)
For a stationary policy, Ï€(a|s) can be obtained from d(s, a)
as follows
Ï€(a|s) =
d(s, a)
P
aâˆˆA(s) d(s, a).
(7)
We refer the readers to [31] for more details on MDPs.
III. PROBLEM FORMULATION
A. States Space, Action Space, and Policies
To formulate steganography coding as a CMDP problem, we
first specify the relative components as outlined above; Fig. 4
shows the CMDP state transitions. As mentioned earlier, we
abstract and simplify the state space to the simplest setting
with only two states S = {0, 1}. In our scenario, we define
an action a âˆˆA = [0, 1] as representing the likelihood of
transitioning to state 0 at the subsequent time step at a given
state s âˆˆS (also the probability of the next token being 0).
Choosing action a impacts the transition probability:
Pa(s, sâ€²) â‰œP(St+1 = sâ€²|St = s, At = a)
= p(St+1 = sâ€²|At = a)
= a1(sâ€² = 0) + (1 âˆ’a)1(sâ€² = 1),
(8)
and the action set A in fact contains uncountably many actions.
For simplicity, we shall write pa â‰œ(a, 1 âˆ’a).
Since the entropy of the distribution Pa(s, sâ€²) directly
reflects the amount of information that can be embedded in
the stego-text, the reward function is simply
r(s, a) = H(pa),
s âˆˆ{0, 1}, a âˆˆ[0, 1]
(9)
where H(Â·) is the binary entropy function. A standard discount
factor Î³, 0 â‰¤Î³ < 1 is assumed. The chosen probability pa at
time t should not deviate too far from the original transition
probability pst, therefore, the constraint cost function is
g(s, a) = TV(pa, ps),
s âˆˆ{0, 1}, a âˆˆ[0, 1],
(10)
where we measure the deviation by the total variation distance
TV(pa, ps) = |a âˆ’ps| + |(1 âˆ’a) âˆ’(1 âˆ’ps)| = 2|a âˆ’ps|.
(11)
A (probabilistic) policy Ï€ is a probability distribution over the
action space at state s, which can be written as
Ï€(a|s) â‰œP(At = a|St = s),
a âˆˆA,
(12)
i.e., the probability of choosing action a at state s, which is
time-invariant in this setting.


--- Page 4 ---
0
1
ğ– (0)
Ï€(a âˆ£0)
a
0
1
a
1 âˆ’a
ğ– (1)
Ï€(a âˆ£1)
a
0
1
a
1 âˆ’a
Fig. 4: Steganography with CMDP
B. CMDP Formulation
With the components given above, we can utilize the general
CMDP framework and write out our specific problem in terms
of the visitation frequency d(s, a), denoted as P1 below.
P1 :
max
d(s,a) :
1
X
s=0
Z 1
0
H(pa)d(s, a) da
(13)
subject to:
Z 1
0
 
d(0, a) âˆ’
1
X
s=0
Î³ad(s, a)
!
da = dÎ³
0 (14)
Z 1
0
 
d(1, a) âˆ’
1
X
s=0
Î³(1 âˆ’a)d(s, a)
!
da = dÎ³
1
(15)
1
X
s=0
Z 1
0
TV(pa, ps)d(s, a)da â‰¤b
(16)
d(s, a) â‰¥0, âˆ€a âˆˆ[0, 1], s âˆˆ{0, 1}
(17)
where we denote the discounted initials (1 âˆ’Î³)d(s) as dÎ³
s.
In the formulation, (14) and (15) reflect the flow conserva-
tion constraint, aligning with (6) by substituting the transition
probability P(s | sâ€², aâ€²) with a and 1 âˆ’a. It is clear that (15)
can be replaced by (18)
Z 1
0
d(0, a) + d(1, a)da = 1,
(18)
in order to simplify the problem, since it is simply the
summation of (14) and (15).
By determining the optimal d(s, a) from problem P1, the
optimal policy Ï€ can be computed via (7). For each fixed s,
the visitation frequency d(s, a) is a function of the state a
on [0, 1]. Therefore, finding the optimal visitation frequency
d(s, a) is a functional optimization problem, even though it is
a linear program in terms of d(s, a).
IV. THE OPTIMAL POLICY
We establish the optimal policy Ï€ in two steps: 1) First
we simplify the functional optimization problem to a standard
convex optimization problem in R4, and 2) We provide an ex-
plicit optimal solution to this simplified optimization problem.
A. A Simplified Optimization Problem
We first show that the optimization problem P1 can be
simplified to an optimization in R4, given in P3. Mathemati-
cally, P3 is a standard finite-dimensional convex optimization
problem, which is significantly simpler than the functional
optimization problem P1. Operationally, once we solve P3,
we can obtain the parameters (a0, a1) that immediately gives
us the optimal policy: at state s, choose the probability
distribution (as, 1âˆ’as) to generate the next token (state). The
policy itself is deterministic, and the randomness in generating
the next token comes purely from the encrypted message bits.
We note the generic de-randomization technique given in [25]
will lead to a CMDP with two possible actions at each state,
instead of only one as in our reduction.
We define an intermediate optimization problem as follows.
P2 :
max
a0,a1,d0,d1 : d0H(a0) + d1H(a1)
(19)
subject to: (1 âˆ’Î³a0)d0 âˆ’Î³a1d1 = dÎ³
0
(20)
d0 + d1 = 1
(21)
d0TV0(a0) + d1TV1(a1) â‰¤b
(22)
d0, d1 â‰¥0
(23)
0 â‰¤a0, a1 â‰¤1,
(24)
where we write H(pa) as H(a), and TV(pa, ps) as TVs(a)
for simplicity,.
Theorem 1. (P1) = (P2), where â€œ=" stands for equivalence
between the two problems.
The proof of Theorem 1 is given in Appendix A1 of the
accompanying long version, where we utilize the convexity
of the objective function and the constraint functions to show
that a weighted average of the probabilistic policy is without
loss of optimality. The problem P2 is however not necessarily
convex. We next show that a reparametrization of the problem
leads to another equivalent problem that is indeed convex. The
equivalent problem is given below.
P3 :
max
x0,x1,d0,d1 : d0H(x0
d0
) + d1H(x1
d1
)
(25)
subject to: (d0 âˆ’Î³x0) âˆ’Î³x1 = dÎ³
0
(26)
d0 + d1 = 1
(27)
d0TV0(x0
d0
) + d1TV1(x1
d1
) â‰¤b
(28)
ds â‰¥0, s âˆˆ{0, 1}
(29)
0 â‰¤xs
ds
â‰¤1, s âˆˆ{0, 1}.
(30)
Clearly, we have simply replaced a0, a1 by x0 = a0d0 and
x1 = a1d1. Since the constraints (26), (27), (29), and (30) are
linear, we only need to consider the objective function (25)
and the constraint (28). Recall that the perspective function
f2(x, t) of a function f1(x) is defined as
f2(x, t) = tf1
x
t

, âˆ€t > 0
(31)
1Long version:https://github.com/Yu-Shin-Huang/Stega-via-CMDP.git


--- Page 5 ---
A well-known property of the perspective function [32] is
that if f1(Â·) is convex, f2(Â·, Â·) is likewise convex. Since the
binary entropy function H(Â·) is concave and the total variation
function is convex, (25) is indeed concave and (28) is indeed
convex, implying that the problem P3 is a convex optimization
problem. When substituting as with xs
ds , it is essential to ensure
ds is non-zero, as ds indicates the frequency of visiting state s.
If ds = 0, it implies state s is never visited. In such a scenario,
the state can be excluded from the system. Thus, without loss
of generality, we assume ds is positive.
B. The Optimal Policy
Before presenting the solution, we first note that by sym-
metry, we only need to consider the case p0, p1 â‰¥
1
2 and
p1 â‰¥
1
2 > p0, since otherwise, we can simply rename the
two states; more details on this relabeling can be found in
Appendix C1 in the accompanying longer version. Let us also
define the following auxiliary functions:
Î·Â±
0 (b) =
b
2(1 + Î³p1) Â± p0(dÎ³
0 + Î³p1)
Â±(Î³p1 + dÎ³
0) + Î³ b
2
(32)
Î·Â±
1 (b) =
b
2(1 âˆ’Î³p0) Â± p1(1 âˆ’Î³p0 âˆ’dÎ³
0)
Â±(1 âˆ’Î³p0 âˆ’dÎ³
0) âˆ’Î³ b
2
.
(33)
The next two theorems give the optimal policy, the proofs of
which can be found in the appendix of the accompanying long
version1.
Theorem 2. When p0, p1 â‰¥1
2, the optimal policy choices at
the two states are:
1) b âˆˆ[0, bl)
(
a0 = p0, a1 = Î·âˆ’
1 (b),
if | 1
2 âˆ’p1| â‰¥| 1
2 âˆ’p0|
a0 = Î·âˆ’
0 (b), a1 = p1,
if | 1
2 âˆ’p1| < | 1
2 âˆ’p0|
(34)
2) b âˆˆ[bl, bh)
a0 = a1 = âˆ’M( b
2 âˆ’dÎ³
0p0 âˆ’dÎ³
1p1)
(35)
3) b âˆˆ[bh, âˆ)
a0 = a1 = 1
2
(36)
where we defined
bl = 2 max{(1 âˆ’Î³p0 âˆ’dÎ³
0)(p1 âˆ’p0),
(dÎ³
0 + Î³p1)(p0 âˆ’p1)}
(37)
bh = (2dÎ³
0 + Î³)(p0 âˆ’p1) + 2p1 âˆ’1
(38)
M =
1
1 âˆ’Î³p0 + Î³p1
.
(39)
There are three regimes in the solution:
1) The highly-constrained regime: The optimal policy is to
replace the distribution at the state that generates lower
entropy tokens with a more uniform distribution, while
for the other state, the distribution is maintained;
2) The intermediately-constrained regime: the distributions
at both states should be replaced by the same (more
uniform) distribution;
3) The unconstrained regime: the distributions at both states
should be replaced by the uniform distribution.
The behavior resembles a water-filling solution in some sense;
see Fig. 5a. Since p1 is situated farther from 1
2, in the highly-
constrained regime, the optimal policy is to retain a0 = p0
and adjust only a1. Once a1 = p0, the two states become
equivalent, and a0 = a1 needs to be adjusted together, until
they both become 1
2.
Theorem 3. When p1 â‰¥1
2 > p0, the optimal policy choices
at the two states are:
1) b âˆˆ[0, bâ€²
l)
(
a0 = p0, a1 = Î·âˆ’
1 (b),
if | 1
2 âˆ’p1| â‰¥| 1
2 âˆ’p0|
a0 = Î·+
0 (b), a1 = p1,
if | 1
2 âˆ’p1| < | 1
2 âˆ’p0|
(40)
2) b âˆˆ[bâ€²
l, bâ€²
h]
(a0, a1) âˆˆSb â‰œ

(a0, a1) âˆˆ[p0, 1
2] Ã— [1
2, p1] :
Î¦âˆ’(a1) = Î¦+(a0) âˆ©m(a0, a1) = b
2

(41)
3) b âˆˆ(bâ€²
h, âˆ)
a0 = a1 = 1
2
(42)
where we define
bâ€²
l = 2

dÎ³
0 + Î³p1
1 âˆ’Î³Ïˆ0 + Î³p1

(Ïˆ0 âˆ’p0)u(1 âˆ’p0 âˆ’p1)
+ 2
 1 âˆ’Î³p0 âˆ’dÎ³
0
1 âˆ’Î³p0 + Î³Ïˆ1

(p1 âˆ’Ïˆ1)u(âˆ’1 + p0 + p1) (43)
bâ€²
h = (2dÎ³
0 + Î³)(1 âˆ’p0 âˆ’p1) + 2p1 âˆ’1
(44)
and
Î¦Â±(a) = (Â±1 + Î³p0 + Î³p1) log
1 âˆ’a
a

âˆ’2Î³ log(1 âˆ’a)
(45)
Î¨p0(a1) = Î¦+(p0) âˆ’Î¦âˆ’(a1), Ïˆ0 = Î¨âˆ’1
p1 (0)
(46)
Î¨p1(a0) = Î¦+(a0) âˆ’Î¦âˆ’(p1), Ïˆ1 = Î¨âˆ’1
p0 (0)
(47)
m(a0, a1) = d0(a0 âˆ’p0) + d1(p1 âˆ’a1)
(48)
and u(Â·) is a unit step function in bâ€²
l.
There are again three regimes, and the only difference from
the previous case is in the second regime, the policy a0 and a1
are not the same; see Fig. 5b for an illustration. In the second
regime, a0 and a1 are the solution of a pair of equations given
above, which can not be solved explicitly. However, we show
in the proof of this theorem that it admits a unique solution
for any b âˆˆ[bâ€²
l, bâ€²
h], which guarantees that a0 and a1 can
be found. Notice also that since Ïˆ0 and Ïˆ1 are the solutions
to Î¨p0 = 0 and Î¨p1 = 0, respectively, they can be written
as Î¦+(p0) = Î¦âˆ’(Ïˆ1) and Î¦+(Ïˆ0) = Î¦âˆ’(p1); the solution
determines the boundary of the regimes.


--- Page 6 ---
0
0.1
0.2
0.3
0.4
0.5
b constraint
0.3
0.4
0.5
0.6
p0
0.8
p1
1
a0 and a1 value
a0 and a1 value v.s. b constraint (  = 0.5) 
a0
a1
(a) p0, p1 â‰¥1
2
0
0.1
0.2
0.3
0.4
0.5
0.6
b constraint
0
0.1
0.2
p0
0.4
0.5
0.6
0.7
0.8
p1
1
a0 and a1 value
a0 and a1 value v.s. b constraint(  = 0.5)
a0
a1
(b) p1 â‰¥1
2 > p0
Fig. 5: a0, a1 value v.s. b constraint with d = [0.8, 0.2]
We further note that the visitation frequency ds can be
expressed in terms of a0 and a1 using equation (49):
d0 =
dÎ³
0 + Î³a1
1 âˆ’Î³a0 + Î³a1
,
d1 = 1 âˆ’Î³a0 âˆ’dÎ³
0
1 âˆ’Î³a0 + Î³a1
(49)
V. CONCLUSION
In this study, we presented a novel formulation for rela-
tively secure LLM-based steganography in the Constrained
Markov Decision Process (CMDP) framework. This approach
considers the impact on both the immediate and future token
generations, and provides insights into how the optimal policy
should be chosen. The solution suggests that when choosing
the adjustment of the probability distribution, the choice
should be made considering the LLM-generated transition
probabilities at all states, and usually adjusting the distribution
for the state that has the least random transition probability
distribution should be prioritized.
REFERENCES
[1] R. J. Anderson and F. A. Petitcolas, â€œOn the limits of steganography,â€
IEEE Journal on selected areas in communications, vol. 16, no. 4, pp.
474â€“481, 1998.
[2] I. Cox, M. Miller, J. Bloom, J. Fridrich, and T. Kalker, Digital water-
marking and steganography.
Morgan kaufmann, 2007.
[3] N. Provos and P. Honeyman, â€œHide and seek: An introduction to
steganography,â€ IEEE security & privacy, vol. 1, no. 3, pp. 32â€“44, 2003.
[4] G. J. Simmons, â€œThe prisonersâ€™ problem and the subliminal channel,â€
in Advances in Cryptology: Proceedings of Crypto 83.
Springer, 1984,
pp. 51â€“67.
[5] U. Topkara, M. Topkara, and M. J. Atallah, â€œThe hiding virtues of
ambiguity: quantifiably resilient watermarking of natural language text
through synonym substitutions,â€ in Proceedings of the 8th workshop on
Multimedia and security, 2006, pp. 164â€“174.
[6] C. Y. Chang and S. Clark, â€œLinguistic steganography using automatically
generated paraphrases,â€ in Human Language Technologies: The 2010
Annual Conference of the North American Chapter of the Association
for Computational Linguistics, 2010, pp. 591â€“599.
[7] I. Safaka, C. Fragouli, and K. Argyraki, â€œMatryoshka: Hiding secret
communication in plain sight,â€ in 6th USENIX Workshop on Free and
Open Communications on the Internet (FOCI 16), 2016.
[8] T. Fang, M. Jaggi, and K. Argyraki, â€œGenerating steganographic text
with LSTMs,â€ arXiv preprint arXiv:1705.10742, 2017.
[9] Z.-L. Yang, X.-Q. Guo, Z.-M. Chen, Y.-F. Huang, and Y.-J. Zhang,
â€œRNN-stega: Linguistic steganography based on recurrent neural net-
works,â€ IEEE Transactions on Information Forensics and Security,
vol. 14, no. 5, pp. 1280â€“1295, 2018.
[10] Z. Ziegler, Y. Deng, and A. M. Rush, â€œNeural linguistic steganography,â€
in Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 1210â€“1215.
[11] L. Xiang, X. Wang, C. Yang, and P. Liu, â€œA novel linguistic steganog-
raphy based on synonym run-length encoding,â€ IEICE transactions on
Information and Systems, vol. 100, no. 2, pp. 313â€“322, 2017.
[12] Z.-L. Yang, S.-Y. Zhang, Y.-T. Hu, Z.-W. Hu, and Y.-F. Huang, â€œVAE-
Stega: linguistic steganography based on variational auto-encoder,â€ IEEE
Transactions on Information Forensics and Security, vol. 16, pp. 880â€“
895, 2020.
[13] Z. Yang, N. Wei, Q. Liu, Y. Huang, and Y. Zhang, â€œGAN-TStega:
Text steganography based on generative adversarial networks,â€ in Digital
Forensics and Watermarking: 18th International Workshop, IWDW 2019,
Chengdu, China, November 2â€“4, 2019, Revised Selected Papers 18.
Springer, 2020, pp. 18â€“31.
[14] C. S. de Witt, S. Sokota, J. Z. Kolter, J. Foerster, and M. Strohmeier,
â€œPerfectly secure steganography using minimum entropy coupling,â€
arXiv preprint arXiv:2210.14889, 2022.
[15] G. Kaptchuk, T. M. Jois, M. Green, and A. D. Rubin, â€œMeteor:
Cryptographically secure steganography for realistic distributions,â€ in
Proceedings of the 2021 ACM SIGSAC Conference on Computer and
Communications Security, 2021, pp. 1529â€“1548.
[16] X. Zhang, K. Chen, J. Ding, Y. Yang, W. Zhang, and N. Yu, â€œProvably
secure public-key steganography based on elliptic curve cryptography,â€
IEEE Transactions on Information Forensics and Security, 2024.
[17] S. Zhang, Z. Yang, J. Yang, and Y. Huang, â€œProvably secure generative
linguistic steganography,â€ arXiv preprint arXiv:2106.02011, 2021.
[18] J. Ding, K. Chen, Y. Wang, N. Zhao, W. Zhang, and N. Yu, â€œDis-
cop: Provably secure steganography in practice based on" distribution
copies",â€ in 2023 IEEE Symposium on Security and Privacy (SP). IEEE,
2023, pp. 2238â€“2255.
[19] F. Dai and Z. Cai, â€œTowards near-imperceptible steganographic text,â€
in Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, 2019, pp. 4303â€“4308.
[20] J. Shen, H. Ji, and J. Han, â€œNear-imperceptible neural linguistic
steganography via self-adjusting arithmetic coding,â€ in Proceedings
of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP), 2020, pp. 303â€“313.
[21] G. Liao, J. Yang, K. Pang, and Y. Huang, â€œCo-Stega: Collaborative
linguistic steganography for the low capacity challenge in social media,â€
in Proceedings of the 2024 ACM Workshop on Information Hiding and
Multimedia Security, 2024, pp. 7â€“12.
[22] M. Bai, J. Yang, K. Pang, Y. Huang, and Y. Gao, â€œSemantic steganogra-
phy: A framework for robust and high-capacity information hiding using
large language models,â€ arXiv preprint arXiv:2412.11043, 2024.
[23] K. Pang, â€œFreStega: A plug-and-play method for boosting impercepti-
bility and capacity in generative linguistic steganography for real-world
scenarios,â€ arXiv preprint arXiv:2412.19652, 2024.
[24] Y.-S. Huang, P. Just, K. Narayanan, and C. Tian, â€œOD-Stega: LLM-
based near-imperceptible steganography via optimized distributions,â€
arXiv preprint arXiv:2410.04328, 2024.
[25] E. Altman, Constrained Markov decision processes.
Routledge, 2021.
[26] A. Vaswani, â€œAttention is all you need,â€ Advances in Neural Information
Processing Systems, 2017.
[27] T. B. Brown, â€œLanguage models are few-shot learners,â€ arXiv preprint
arXiv:2005.14165, 2020.
[28] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. RoziÃ¨re, N. Goyal, E. Hambro, F. Azhar et al.,
â€œLlama: Open and efficient foundation language models,â€ arXiv preprint
arXiv:2302.13971, 2023.
[29] J. Rissanen and G. G. Langdon, â€œArithmetic coding,â€ IBM Journal of
research and development, vol. 23, no. 2, pp. 149â€“162, 1979.
[30] R. E. Bellman and S. E. Dreyfus, Applied dynamic programming.
Princeton university press, 2015, vol. 2050.
[31] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.
MIT press, 2018.
[32] S. Boyd and L. Vandenberghe, â€œConvex optimization,â€ Cambridge
University Press, 2004.


--- Page 7 ---
APPENDIX A
PROOF OF THEOREM 1
Assuming a general optimal solution Ëœd(s, a) exists for
problem P1, we claim that a new solution dâ‹†(s, a) = dâ‹†
sÎ´(aâ‹†
s)
is at least as good, where
(
dâ‹†
s =
R 1
0
Ëœd(s, a) da
aâ‹†
s =
R 1
0
a
Ëœd(s,a)
dâ‹†
s
da
(50)
and Î´(Â·) is a Dirac-delta function.
To prove this claim, we need to show that the function
dâ‹†(s, a) meets the constraints (14)-(17), and the objective
function value (13) is at least as large.
1) Constraints (14), (17) and (18): It is clear that dâ‹†
s is
positive for all s âˆˆ{0, 1} and P1
s=0 dâ‹†
s sum up to 1.
Therefore dâ‹†(s, a) satisfies the constraint (17) and (18).
For constraint (14),
Z 1
0
(1 âˆ’Î³a)dâ‹†(0, a)da âˆ’
Z 1
0
Î³adâ‹†(1, a)da
= dâ‹†
0 âˆ’Î³aâ‹†
0dâ‹†
0 âˆ’Î³aâ‹†
1dâ‹†
1
=
Z 1
0
Ëœd(0, a)da âˆ’Î³dâ‹†
0
Z 1
0
a
Ëœd(0, a)
dâ‹†
0
da
âˆ’Î³dâ‹†
1
Z 1
0
a
Ëœd(1, a)
dâ‹†
1
da
=
Z 1
0
Ëœd(0, a)da âˆ’Î³
Z 1
0
a Ëœd(0, a)da âˆ’Î³
Z 1
0
a Ëœd(1, a)da
=
Z 1
0
(1 âˆ’Î³a) Ëœd(0, a)da âˆ’Î³
Z 1
0
a Ëœd(1, a)da = dÎ³
0,
(51)
where the equality in (51) follows from the fact that
Ëœd(s, a) satisfies the constraint (14).
2) Constraint (16): We write
b â‰¥
Z 1
0
TV0(a) Ëœd(0, a) + TV1(a) Ëœd(1, a)da
=
Z 1
0
dâ‹†
0TV0(a)
 Ëœd(0, a)
dâ‹†
0
!
+ dâ‹†
1TV1(a)
 Ëœd(1, a)
dâ‹†
1
!
da
â‰¥dâ‹†
0TV0
 Z 1
0
a
 Ëœd(0, a)
dâ‹†
0
!
da
!
+ dâ‹†
1TV1
 Z 1
0
a
 Ëœd(1, a)
dâ‹†
1
!
da
!
= dâ‹†
0TV0(aâ‹†
0) + dâ‹†
1TV1(aâ‹†
1)
=
Z 1
0
dâ‹†(0, a)TV0(a) + dâ‹†(1, a)TV1(a)da
(52)
where the inequality follows the Jensenâ€™s inequality since
the total variation divergence is convex.
3) The objective function: We wish to show that dâ‹†(s, a)
achieves a value equal to or exceeding that of Ëœd(s, a).
For this purpose, we write:
Z 1
0
H(a) Ëœd(0, a)da +
Z 1
0
H(a) Ëœd(1, a)da
= dâ‹†
0
Z 1
0
H(a)
Ëœd(0, a)
dâ‹†
0
da + dâ‹†
1
Z 1
0
H(a)
Ëœd(1, a)
dâ‹†
1
da
â‰¤dâ‹†
0H(
Z 1
0
a
Ëœd(0, a)
dâ‹†
0
da) + dâ‹†
1H(
Z 1
0
a
Ëœd(1, a)
dâ‹†
1
da)
= dâ‹†
0H(aâ‹†
0) + dâ‹†
1H(aâ‹†
1)
=
Z 1
0
H(a)dâ‹†(0, a)da +
Z 1
0
H(a)dâ‹†(1, a)da,
(53)
where again the inequality is due to Jensenâ€™s inequality
since the entropy function is concave.
We can now conclude that dâ‹†(s, a) is also an optimal solution
to the optimization problem. The solution dâ‹†(s, a) further
implies that the optimal policy is
Ï€(a|s) =
dâ‹†(s, a)
R 1
0 dâ‹†(s, a)da
=
(
1,
a = aâ‹†
s
0,
else
(54)
which indicates that there is only one action aâ‹†
s in interval [0, 1]
to take in each state s, i.e., a deterministic policy is optimal.
Substituting the variable d(s, a) in problem P1 with dâ‹†(s, a)
will yield an equivalent problem to P2. In other words, this
optimization problem reduces the number of variables from
uncountably many to merely four: the action points aâ‹†
0, aâ‹†
1,
and the discounted state visitation distribution dâ‹†
0, dâ‹†
1.
APPENDIX B
PROOF OF THEOREM 2 AND THEOREM 3
To solve the convex optimization problem P3, our plan is
to find a solution to the KKT conditions, since the problem
is convex. However, the total variation constraint poses a
challenge as it is not differentiable when x0
d0 = p0 and x1
d1 = p1,
preventing us from performing a direct partial differentiation
on this inequality constraint. Therefore, we introduce a set of
auxiliary variables c0, c1 such that d0| x0
d0 âˆ’p0| = |x0âˆ’d0p0| â‰¤
c0 and d1| x1
d1 âˆ’p1| = |x1 âˆ’d1p1| â‰¤c1. The optimization
problem P3 can be reformulated as follows.
max
x0,x1,d0,d1,c0,c1 d0H(x0
d0
) + d1H(x1
d1
)
(55)
subject to c0 + c1 â‰¤b
2
(56)
âˆ’c0 â‰¤x0 âˆ’d0p0 â‰¤c0
(57)
âˆ’c1 â‰¤x1 âˆ’d1p1 â‰¤c1
(58)
d0 âˆ’Î³x0 âˆ’Î³x1 = dÎ³
0
(59)
d0 + d1 = 1
(60)
0 â‰¤x0 â‰¤d0, 0 â‰¤x1 â‰¤d1
(61)
The Lagrangian function of this problem is:
L = âˆ’

d0H(x0
d0
) + d1H(x1
d1
)

+ Î»

c0 + c1 âˆ’b
2

Î±0(âˆ’c0 âˆ’x0 + d0p0) + Î²0(x0 âˆ’d0p0 âˆ’c0)
+ Î±1(âˆ’c1 âˆ’x1 + d1p1) + Î²0(x1 âˆ’d1p1 âˆ’c1)
+ Ï‰0 (d0 âˆ’dÎ³
0 âˆ’Î³(x0 + x1)) + Ï‰1 (d0 + d1 âˆ’1)
âˆ’Î½0x0 âˆ’Î½1x1 + Âµ0(x0 âˆ’d0) + Âµ1(x1 âˆ’d1)
(62)


--- Page 8 ---
where Î», Î±s, Î²s, Ï‰s, Î½s, Âµs are the Lagrange multipliers of
constraints (56) to (61). The KKT conditions are written as
follows:
1) Stationarity:
âˆ‚L
âˆ‚x0
= âˆ’log
d0 âˆ’x0
x0

âˆ’Î±0 + Î²0
âˆ’Î³Ï‰0 âˆ’Î½0 + Âµ0 = 0
(63)
âˆ‚L
âˆ‚x1
= âˆ’log
d1 âˆ’x1
x1

âˆ’Î±1 + Î²1
âˆ’Î³Ï‰0 âˆ’Î½1 + Âµ1 = 0
(64)
âˆ‚L
âˆ‚d0
= âˆ’H(x0
d0
) + x0
d0
log
d0 âˆ’x0
x0

+ Î±0p0 âˆ’Î²0p0 + Ï‰0 + Ï‰1 âˆ’Âµ0 = 0
(65)
âˆ‚L
âˆ‚d1
= âˆ’H(x1
d1
) + x1
d1
log
d1 âˆ’x1
x1

+ Î±1p1 âˆ’Î²1p1 + Ï‰1 âˆ’Âµ1 = 0
(66)
âˆ‚L
âˆ‚c0
= Î» âˆ’Î±0 âˆ’Î²0 = 0
(67)
âˆ‚L
âˆ‚c1
= Î» âˆ’Î±1 âˆ’Î²1 = 0
(68)
2) Primal feasibility:
c0 + c1 â‰¤b
2
(69)
âˆ’c0 âˆ’x0 + d0p0 â‰¤0, âˆ’c1 âˆ’x1 + d1p1 â‰¤0
(70)
x0 âˆ’d0p0 âˆ’c0 â‰¤0, x1 âˆ’d1p1 âˆ’c1 â‰¤0
(71)
d0 = dÎ³
0 + Î³(x0 + x1)
(72)
d0 + d1 = 1
(73)
x0 â‰¥0, x1 â‰¥0
(74)
x0 âˆ’d0 â‰¤0, x1 âˆ’d1 â‰¤0
(75)
3) Dual feasibility:
Î» â‰¥0
(76)
Î±0 â‰¥0, Î±1 â‰¥0
(77)
Î²0 â‰¥0, Î²1 â‰¥0
(78)
Î½0 â‰¥0, Î½1 â‰¥0
(79)
Âµ0 â‰¥0, Âµ1 â‰¥0
(80)
4) Complementary slackness:
Î»

c0 + c1 âˆ’b
2

= 0
(81)
Î±0(âˆ’c0 âˆ’x0 + d0p0)
= Î±1(âˆ’c1 âˆ’x1 + d1p1) = 0
(82)
Î²0(x0 âˆ’d0p0 âˆ’c0) = Î²1(x1 âˆ’d1p1 âˆ’c1) = 0
(83)
Ï‰0 (d0 âˆ’dÎ³
0 âˆ’Î³(x0 + x1)) = 0
(84)
Ï‰1 (d0 + d1 âˆ’1) = 0
(85)
Î½0x0 = Î½1x1 = 0
(86)
Âµ0(x0 âˆ’d0) = Âµ1(x1 âˆ’d1) = 0
(87)
The solution that satisfies the KKT conditions will be pre-
sented in several cases, depending on the values of p0 and p1,
separated into different regimes of the cost constraint b. The
expression formulas for the primal variables xs, ds and the
dual variables Ï‰s, Î½s, Âµs where s âˆˆ{0, 1} are identical across
all scenarios, as presented below.
x0 = a0d0
(88)
x1 = a1d1
(89)
d0 =
dÎ³
0 + Î³a1
1 âˆ’Î³a0 + Î³a1
(90)
d1 = 1 âˆ’d0
(91)
Ï‰0 = âˆ’1
Î³ log(1 âˆ’a0
a0
) âˆ’1
Î³ Î±0 + 1
Î³ Î²0
= âˆ’1
Î³ log(1 âˆ’a1
a1
) âˆ’1
Î³ Î±1 + 1
Î³ Î²1
(92)
Ï‰1 = âˆ’log(1 âˆ’a1) âˆ’Î±1p1 + Î²1p1
(93)
Î½s = Âµs = 0, s âˆˆ{0, 1}
(94)
Therefore, we will not write them out particularly in each case,
since they can derived from as, Î±s, Î²s, s âˆˆ{0, 1}.
We define the mathematical expressions below for simplifi-
cations:
z0(a0, a1) = (âˆ’1 âˆ’Î³p1) log
1 âˆ’a0
a0

+ Î³p1 log
1 âˆ’a1
a1

+ Î³ log
1 âˆ’a0
1 âˆ’a1

(95)
z1(a0, a1) = (âˆ’Î³p0) log
1 âˆ’a0
a0

+ (âˆ’1 + Î³p0) log
1 âˆ’a1
a1

+ Î³ log
1 âˆ’a0
1 âˆ’a1

(96)
A. Proof of Theorem 2
In Theorem 2, the assumption is that p0, p1 â‰¥1
2.
There are three regimes:
1) The regime b âˆˆ[0, bl): In this regime, the two cases
p1 â‰¥p0 â‰¥1
2 and p0 > p1 â‰¥1
2 lead to different solution
expressions, and let us consider the first case for now.
With the solution of a0, a1 given in the theorem, and in
the given range of b, the range of a0 and a1 are
a0 = p0, a1 âˆˆ[p0, p1],
(97)
The variable assignments are as follows.
a) Primal variable assignments:
a0 = p0,
a1 = Î·âˆ’
1 (b)
(98)
c0 = 0,
c1 = b
2,
(99)
Using (88)-(91), all the primal variables are assigned. It
is important to note that x0 and x1 are non-zero in this
setting because a0 and a1 are not zero, and similarly,
d0 and d1 can be assumed to be non-zero stated in
section IV-A. As a result, we are able to derive the
stationary conditions (63) and (64).


--- Page 9 ---
b) Dual variable assignments:
Î±0 = max{0, Mz0(a0, a1)}
(100)
Î²0 = Î±1 âˆ’Î±0
(101)
Î±1 = Mz1(a0, a1)
(102)
Î²1 = 0
(103)
Î» = Î±1
(104)
With (92)-(94), all the dual variables are assigned.
We wish to show that the assignments satisfy all the KKT
conditions. With the primal and dual variables displayed
above, one can conveniently confirm that (63)-(68), (69)-
(75), (79)-(80), and (81)-(87) are satisfied by substituting
these variables. It remains to demonstrate the validity of
(76)-(78). It is evident that Î±0 and Î²1 are non-negative.
Thus, we need only establish that Î±1, Î²0 â‰¥0.
For Î±1, we use the following partial derivative to show
that z1 is increasing in a1:
âˆ‚z1
âˆ‚a1
= 1 âˆ’Î³p0 + Î³a1
(1 âˆ’a1)a1
â‰¥0
(105)
This immediately implies that z1(a0 = p0, a1) is mono-
tonically increasing. It suffices to show that z1(p0, 1
2)
is positive, which is the minimum value of z1(p0, a1)
achieved in the range a1 âˆˆ[p0, p1]. We write
z1(p0, 1
2) =

(âˆ’Î³p0) log
1 âˆ’p0
p0

+ Î³ log
1 âˆ’p0
1
2

=

(1 âˆ’p0) log(1 âˆ’p0) + p0 log(1 âˆ’p0) âˆ’log 1
2

= Î³(âˆ’H(p0) + H(1
2)) â‰¥0,
(106)
from which we conclude that Î±1 is non-negative since
M is clearly positive. For Î²0,
Î²0 = Î±1 âˆ’Î±0
(107)
=
(
âˆ’log

1âˆ’a1
a1

+ log

1âˆ’a0
a0

, z0(a0, a1) â‰¥0
Î±1
, z0(a0, a1) < 0
(108)
From equation (108), it is clear that Î²0 â‰¥0 when z0 < 0
as Î±1 â‰¥0. Similarly, for z0 â‰¥0, it follows that Î²0 is
non-negative since a1 â‰¥a0 = p0 â‰¥1
2.
Now let us turn our attention to the case when p0 â‰¥p1 â‰¥
1
2. We can determine the range of a0 and a1 as
a0 = [p1, p0], a1 = p1.
(109)
The variable assignments are as follows.
a) Primal variable assignments:
a0 = Î·âˆ’
0 (b),
a1 = p1
(110)
c0 = b
2,
c1 = 0
(111)
Using (88)-(91), all the primal variables are assigned.
Observe that x0 and x1 will not be zero here as well.
b) Dual variable assignments:
Î±0 = Mz0(a0, a1)
(112)
Î²0 = 0
(113)
Î±1 = Mz1(a0, a1)
(114)
Î²1 = log
1 âˆ’a1
a1

âˆ’log
1 âˆ’a0
a0

(115)
Î» = Î±0
(116)
With (92)-(94), all the dual variables are assigned.
Same as the previous case, we wish to show that the as-
signments satisfy all the KKT conditions. With the primal
and dual variables displayed above, one can conveniently
confirm that (63)-(68), (69)-(75), (79)-(80), and (81)-(87)
are satisfied by substituting these variables. It remains to
demonstrate the validity of (76)-(78). It is evident that Î²0
satisfies the non-negativity. Thus, we need only establish
that Î±0, Î±1, Î²1 â‰¥0.
For Î±0, we use the following partial derivative to show
that z0 is increasing in a0:
âˆ‚z0
âˆ‚a0
= 1 + Î³p1 âˆ’Î³a0
(1 âˆ’a0)a0
â‰¥0
(117)
(117) implies that z0(a0, a1 = p1) is increasing in a0.
It suffices to show that z0(p1, p1) is positive, which is
the minimum value of z0(a0, p1) achieved in the range
a0 âˆˆ[p1, p0].
z0(p1, p1) = âˆ’log
1 âˆ’p1
p1

â‰¥0
(118)
For Î±1, it is demonstrated that z1(a0, p1) increases and
remains non-negative at its lowest value a0 âˆˆ[ 1
2, p0], as
shown in (119) and (120).
âˆ‚z1
âˆ‚a0
= Î³p0 âˆ’Î³a0
(1 âˆ’a0)a0
â‰¥0, âˆ€a0 âˆˆ[p1, p0]
(119)
z1(1
2, p1) = (âˆ’1 + Î³p0) log
1 âˆ’p1
p1

+ Î³ log

1
2
1 âˆ’p1

â‰¥0
(120)
Furthermore, it is evident that Î²1 is greater than zero since
a0 â‰¥a1 = p1 â‰¥1
2. This establishes that Î±0, Î±1, Î²1, Î» are
all non-negative.
2) b âˆˆ[bl, bh): In this regime, we have the following
assignments.
a) Primal variable assignments:
a0 = a1 = âˆ’M( b
2 âˆ’dÎ³
0p0 âˆ’dÎ³
1p1)
(121)
c0 = d0(p0 âˆ’a0),
c1 = d1(p1 âˆ’a1).
(122)
Using (88)-(91), all the primal variables are assigned.
Observe that x0 and x1 will not be zero here, indicating
that stationary conditions can be derived.


--- Page 10 ---
b) Dual variable assignments:
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
Î±0 = Î±1 = M log

a0
1âˆ’a0

Î²0 = Î²1 = 0
Î» = Î±1
(123)
With (92)-(94), all the dual variables are assigned.
Similar to the previous discussion, we wish to show that
the assignments satisfy all the KKT conditions. With
the primal and dual variables displayed above, one can
conveniently confirm that (63)-(68), (69)-(75), (78)-(80),
and (81)-(87) are satisfied by substituting these variables.
Since a0 â‰¥
1
2, we have that Î±s, Î» are non-negative,
leading to (76) and (77) in this case.
3) b âˆˆ[bh, âˆ): In this regime, we have
a) Primal variables:
a0 = a1 = 1
2
(124)
c0 = d0(p0 âˆ’1
2),
c1 = d1(p1 âˆ’1
2).
(125)
Using (88)-(91), all the primal variables are assigned.
Observe that x0 and x1 will not be zero here as well.
b) Dual variables:
Î±0 = Î±1 = Î²0 = Î²1 = Ï0 = Ï1 = Î» = 0.
(126)
All the dual variables are assigned using (92)-(94).
With the primal and dual variables displayed above, one
can conveniently confirm that (63)-(68), (69)-(75), (76)-
(80), and (81)-(87) are satisfied by substituting these
variables.
B. Proof of Theorem 3
Theorem 3 holds under the assumption p0 < 1
2 â‰¤p1. There
are again three regimes:
1) The regime b âˆˆ[0, bâ€²
l): In this regime, the two cases
| 1
2 âˆ’p1| â‰¥| 1
2 âˆ’p0| and | 1
2 âˆ’p1| < | 1
2 âˆ’p0| lead to
different solution expressions, and let us consider the first
case for now. Observe that in the first case implies that
p1 + p0 âˆ’1 â‰¥0, bâ€²
l in this case can be written as:
bâ€²
l = 2
 1 âˆ’Î³p0 âˆ’dÎ³
0
1 âˆ’Î³p0 + Î³Ïˆ1

(p1 âˆ’Ïˆ1)
(127)
by the definition given in (43).
With the solution of a0, a1 given in the theorem, and in
the given range of b, the range of a0 and a1 are
a0 = p0, a1 âˆˆ[Ïˆ1, p1] âŠ‚[1
2, p1]
(128)
The variable assignments are as follows.
a) Primal variable assignments:
a0 = p0,
a1 = Î·âˆ’
1 (b)
(129)
c0 = 0,
c1 = b
2
(130)
Using (88)-(91), all the primal variables are assigned.
Note that in this scenario, x0 and x1 are non-zero,
enabling us to address the stationarity conditions in
(63) and (64).
b) Dual variable assignments:
Î±0 = z1(a0, a1) + z0(a0, a1) = âˆ’MÎ¨p0(a1)
(131)
Î±1 = z1(a0, a1)
(132)
Î²0 = âˆ’z0(a0, a1)
(133)
Î²1 = 0
(134)
Î» = Î±1
(135)
With (92)-(94), all the dual variables are assigned.
We wish to show that the assignments satisfy all the KKT
conditions. With the primal and dual variables displayed
above, one can conveniently confirm that (63)-(68), (69)-
(75), (79)-(80), and (81)-(87) are satisfied by substituting
these variables. It remains to verify (76)-(78). It is evident
that Î²1 is non-negative. Thus, we need only establish that
Î±0, Î±1, Î²0 â‰¥0.
(105) shows that z1 is monotonically increasing in a1,
while (106) confirmed that the minimum value of z1 is
non-negative, occurring when a1 = 1
2 within the interval
a1 âˆˆ[Ïˆ1, p1] âŠ‚[ 1
2, p1]. Therefore, indeed Î±1 â‰¥0.
For Î²0, the following partial derivative shows that âˆ’z0 is
monotonically non-decreasing in a1:
âˆ‚z0
âˆ‚a1
= âˆ’Î³p1 + Î³a1
(1 âˆ’a1)a1
â‰¤0, âˆ€a1 âˆˆ[Ïˆ1, p1]
(136)
It suffices to show that âˆ’z0(p0, a1 = 1
2) is positive, which
is the minimum value of âˆ’z0(p0, a1) achieved in the
range a1 âˆˆ[ 1
2, p1]. We write
âˆ’z0(p0, 1
2)
= (1 + Î³p1) log
1 âˆ’p0
p0

âˆ’Î³ log
1 âˆ’a0
1
2

(137)
= Î³

log
1 âˆ’p0
p1

âˆ’log
1 âˆ’a0
1
2

+ (1 âˆ’Î³ + Î³p1) log
1 âˆ’p0
p0

â‰¥0.
(138)
Therefore, Î²0 â‰¥0.
To demonstrate Î±0 â‰¥0, it is necessary to establish
that âˆ’MÎ¨p0(a1) is non-negative. Employing a similar
approach as earlier, we first show that âˆ’MÎ¨p0(a1)
increases when a1 âˆˆ[Ïˆ1, p1] by way of (139). Then
(140) confirms that its minimum value is zero, occurring
at a1 = Ïˆ1.
âˆ’âˆ‚
âˆ‚a1
MÎ¨p0(a1) = âˆ’MÎ¨â€²
p0(a1) = MÎ¦â€²
âˆ’(a1)
= 2Î³a1 + 1 âˆ’Î³p0 âˆ’Î³p1
(1 âˆ’a1)a1
â‰¥0 (139)
âˆ’MÎ¨p0(Ïˆ1) = âˆ’M(Î¦+(p0) âˆ’Î¦âˆ’(Ïˆ1)) = 0
(140)
The inequality in (139) follows from the fact that a1 â‰¥
1
2. The above establishes that Î±0, Î±1, Î²0, Î» are all non-
negative.


--- Page 11 ---
Now let us turn our attention to the case when | 1
2 âˆ’p1| <
| 1
2 âˆ’p0|, implying 1 âˆ’p1 âˆ’p0 â‰¥0. bâ€²
l in this case can
be written as:
bâ€²
l = 2

dÎ³
0 + Î³p1
1 âˆ’Î³Ïˆ0 + Î³p1

(Ïˆ0 âˆ’p0)
(141)
again by the definition given in (43).
We can determine the range of a0 and a1 as
a0 âˆˆ[p0, Ïˆ0], a1 = p1.
(142)
The variable assignments are as follows.
a) Primal variable assignments:
a0 = Î·+
0 (b), a1 = p1
(143)
c0 = b
2, c1 = 0
(144)
Using (88)-(91), all the primal variables are assigned.
Note that x0 and x1 are also non-zero in this context,
allowing us to derive stationary conditions (63) and
(64).
b) Dual variable assignments:
Î±0 = 0
(145)
Î±1 = z1(a0, a1)
(146)
Î²0 = âˆ’z0(a0, a1)
(147)
Î²1 = âˆ’z0(a0, a1) âˆ’z1(a0, a1) = MÎ¨p1(a0)
(148)
Î» = Î²0
(149)
With (92)-(94), all the dual variables are assigned.
Same as in the previous case, we wish to show that
the assignments satisfy all the KKT conditions. With
the primal and dual variables displayed above, one can
conveniently confirm that (63)-(68), (69)-(75), (79)-(80),
and (81)-(87) are satisfied by substituting these variables.
It remains to verify (76)-(78). It is evident that Î±0
satisfies the non-negativity condition. Thus, we need only
establish that Î±1, Î²0, Î²1 â‰¥0.
For Î±1, (119) showed that z1 is monotonically non-
increasing when a0 âˆˆ[p0, Ïˆ0]. Moreover, (120) implies
that z1( 1
2, p1) is positive, which is the minimum value of
z1(a0, p1) achieved in the range a0 âˆˆ[p0, Ïˆ0] âŠ‚[p0, 1
2].
Therefore Î±1 is non-negative.
For Î²0, (117) showed that z0(a0, p1) is monotonically
non-decreasing in a0, implying that Î²0 is decreasing in
a0. Moreover, we can write
âˆ’z0(1
2, p1) = âˆ’Î³p1 log
1 âˆ’p1
p1

âˆ’Î³ log

1
2
1 âˆ’p1

= Î³

H(1
2) âˆ’H(p1)

â‰¥0,
(150)
which implies thatthe minimum value of âˆ’z0(a0, p1)
in the range a0 âˆˆ[p0, Ïˆ0] âŠ‚[p0, 1
2] is non-negative.
Combining these two facts, we conclude that Î²0 is also
non-negative.
For Î²1, a similar procedure can be used to show its non-
negativity. (151) implies that MÎ¨p1(a0) is monotonically
non-increasing in a0 âˆˆ[p0, Ïˆ0], given that a0 â‰¤
1
2.
Furthermore, as shown in (152), the minimum value of
MÎ¨p1(a0) over the interval a0 âˆˆ[p0, Ïˆ0] is zero, which
is derived from the definition of Ïˆ0 in Theorem 3.
âˆ‚
âˆ‚a0
MÎ¨p1(a0) = MÎ¦â€²
+(a0)
= 2Î³a0 âˆ’1 âˆ’Î³p0 âˆ’Î³p1
(1 âˆ’a0)a0
â‰¤0,
âˆ€a0 âˆˆ[p0, Ïˆ0] âŠ‚[p0, 1
2]
(151)
MÎ¨p1(Ïˆ0) = M(Î¦+(Ïˆ0) âˆ’Î¦âˆ’(a1)) = 0
(152)
Therefore (76)-(78) hold in this case.
2) b âˆˆ[bâ€²
l, bâ€²
h]: Let us first define the following set
Sb â‰œ

(a0, a1) âˆˆ[p0, 1
2] Ã— [1
2, p1] |
Î¦âˆ’(a1) = Î¦+(a0) and m(a0, a1) = b
2

(153)
It will be shown shortly that the set Sb is a singleton set
in this regime. We have the following assignments.
a) Primal variable assignments:
(a0, a1) âˆˆSb
(154)
c0 = d0(a0 âˆ’p0),
c1 = d1(p1 âˆ’a1)
(155)
Using (88)-(91), all the primal variables are assigned.
Note that x0 and x1 will not be zero here as well, thus
allowing the derivation of stationary conditions.
b) Dual variable assignments:
Î±0 = 0
(156)
Î±1 = 1
2 log
1 âˆ’a0
a0

âˆ’1
2 log
1 âˆ’a1
a1

(157)
Î²0 = Î±1
(158)
Î²1 = 0
(159)
Î» = Î±1
(160)
With (92)-(94), all the dual variables are assigned. We
can confirm that (63)-(68), (69)-(75), (79)-(80), and
(81)-(87) are satisfied by substituting these variables.
According to the definition of Sb, if (a0, a1) is an
element in Sb, then it follows that a0 â‰¤
1
2 â‰¤a1.
Clearly, the conditions (76)-(78) remain valid since
a0 â‰¤1
2 â‰¤a1.
3) b âˆˆ(bh, âˆ): In this regime, we have
a) Primal variables:
a0 = a1 = 1
2
(161)
c0 = d0(1
2 âˆ’p0),
c1 = d1(p1 âˆ’1
2).
(162)
Using (88)-(91), all the primal variables are assigned.
Observe that x0 and x1 will not be zero here as well.


--- Page 12 ---
0
0.1
0.2
p0
0.4
0.5
0.6
0.7
0.8
p1
a0  and  a1
-2
0
2
4
6
8
10
+ and 
- value
+  and 
- (  = 0.5)
+(a0)
-(a1)
m(a0,a1) = bl'
m(a0,a1) = bh'
1
Fig. 6: The proof in appendix B-C
b) Dual variables:
Î±0 = Î±1 = Î²0 = Î²1 = Ï0 = Ï1 = Î» = 0.
(163)
All the dual variables are assigned using (92)-(94).
With the primal and dual variables displayed above, we
confirm that (63)-(68), (69)-(75), (76)-(80), and (81)-(87)
are satisfied by substituting these variables.
C. Proof That Sb Is a Singleton Set
Let us first consider the two functions Î¦+(a0) and Î¦âˆ’(a1),
which are shown in Fig.6. We can see that the two functions
intersect at the point a0 = a1 = 1
2. Their intersection with
the darker green dash line is (a0, a1) = ( 1
2, 1
2), which is the
only element in Sbâ€²
h; here the two functions take the same
value. Similarly, their intersections with the lighter green dash
line give (a0, a1) = (p0, Ïˆ1), which is the only element in
Sbâ€²
l; again the two functions take the same value. The plot
illustrates that any horizontal line positioned between the two
green dashed lines intersects with these two functions, and the
intersection points represent the unique element in Sb for b âˆˆ
[bâ€²
l, bâ€²
h]. Given this behavior, it suffices to prove the following
two claims.
1) Claim 1: Î¦+ and Î¦âˆ’are monotonic functions that
increase in opposing directions in (0, 1
2] and [ 1
2, 1) re-
spectively. In other words, Î¦+ decreasing in a âˆˆ(0, 1
2]
and Î¦âˆ’increasing in a âˆˆ[ 1
2, 1). We prove this claim by
computing the derivatives.
âˆ‚Î¦+
âˆ‚a
= âˆ’(1 âˆ’2Î³ + Î³p0 + Î³p1)
1 âˆ’a
âˆ’(1 + Î³p0 + Î³p1)
a
= 2Î³a âˆ’1 âˆ’Î³p0 âˆ’Î³p1
(1 âˆ’a)a
â‰¤0,
âˆ€a âˆˆ(0, 1
2) (164)
âˆ‚Î¦âˆ’
âˆ‚a
= âˆ’(âˆ’1 âˆ’2Î³ + Î³p0 + Î³p1)
1 âˆ’a
âˆ’(âˆ’1 + Î³p0 + Î³p1)
a
= 2Î³a + 1 âˆ’Î³p0 âˆ’Î³p1
(1 âˆ’a)a
â‰¥0,
âˆ€a âˆˆ[1
2, 1)
(165)
Moreover, as already mention, Î¦+ and Î¦âˆ’intersects at
the point 1
2, i.e., Î¦+( 1
2) = Î¦âˆ’( 1
2) = âˆ’2Î³ log 1
2.
2) Claim 2: There is only one solution (a0, a1) âˆˆ[p0, 1
2] Ã—
[ 1
2, p1] satisfying m(a0, a1) = b
2 for b in the range [bâ€²
l, bâ€²
h]
specified in Theorem 3. We next prove this claim. In (166)
and (167) given below, we show that for different cases
| 1
2 âˆ’p1| â‰¥| 1
2 âˆ’p0| and | 1
2 âˆ’p0| > | 1
2 âˆ’p1|, (Ïˆ0, p1) and
(p0, Ïˆ1) are the solutions to m(a0, a1) = bl
2 :
If |1
2 âˆ’p1| â‰¥|1
2 âˆ’p0|,
d0(p0 âˆ’p0) + d1(p1 âˆ’Ïˆ1) = d1(p1 âˆ’Ïˆ1)
=
 1 âˆ’Î³p0 âˆ’dÎ³
0
1 âˆ’Î³p0 + Î³Ïˆ1

(p1 âˆ’Ïˆ1) = bâ€²
l
2
(166)
If |1
2 âˆ’p0| > |1
2 âˆ’p1|,
d0(Ïˆ0 âˆ’p0) + d1(p1 âˆ’p1) = d0(Ïˆ0 âˆ’p0)
=

dÎ³
0 + Î³p1
1 âˆ’Î³Ïˆ0 + Î³p1

(Ïˆ0 âˆ’p0) = bâ€²
l
2
(167)
Moreover, from equation (168) below, we see that ( 1
2, 1
2)
is the solution to m(a0, a1) = bâ€²
h
2 .
d0(1
2 âˆ’p0) + d1(p1 âˆ’1
2) = 1
2(d0 âˆ’d1) âˆ’d0p0 + d1p1
= 1
2
dÎ³
0 + 1
2Î³ âˆ’1 + 1
2Î³ + dÎ³
0
1 âˆ’1
2Î³ + 1
2Î³

âˆ’p0

dÎ³
0 + 1
2Î³
1 âˆ’1
2Î³ + 1
2Î³

+ p1
 1 âˆ’1
2Î³ âˆ’dÎ³
0
1 âˆ’1
2Î³ + 1
2Î³

=

dÎ³
0 + Î³
2

(1 âˆ’p0 âˆ’p1) + p1 âˆ’1
2 = bâ€²
h
2
(168)
As a consequence, when b = bâ€²
l and b = bâ€²
h, the solutions to
the equation are situated within the interval [p0, 1
2] Ã— [ 1
2, p1].
Since Î¦+(a0) and Î¦âˆ’(a1) are monotonic, and the mapping
m(a0, a1) is continuous, we conclude that there is a unique
solution (a0, a1) to the equations Î¦+(a0) = Î¦âˆ’(a1) and
m(a0, a1) = b
2 for each b âˆˆ[bâ€²
l, bâ€²
h].
APPENDIX C
PROOF OF REDUCTION OF CASES
In scenarios where p0, p1 â‰¤
1
2 and p0 â‰¥
1
2 â‰¥p1, we
interchange state 0 to 1â€² and state 1 to 0â€² as the order of states
is inconsequential. We first note that in our setting, p0 = p0â†’0
and p1 = p1â†’0, meaning that p0 is the probability from state 0
to 0 and p1 is the probability from state 1 to 0. After renaming
the state from 0, 1 to 1â€², 0â€², the probability p0, p1 now becomes
p0 = p0â†’0
rename
=
p1â€²â†’1â€² â‰œ1 âˆ’pâ€²
1
(169)
p1 = p1â†’0
rename
=
p0â€²â†’1â€² â‰œ1 âˆ’pâ€²
0
(170)


--- Page 13 ---
Therefore, for case p0, p1 â‰¤
1
2, the renamed optimization
problem will have pâ€²
0, pâ€²
1 â‰¥
1
2. Use the results in Theorem
2 to solve for optimal aâ€²
0, aâ€²
1, then convert back to a0, a1.
aâ€²
0 = a0â€²â†’0â€² recover
=
a1â†’1 = 1 âˆ’a1 â‡’a1 = 1 âˆ’aâ€²
0
(171)
aâ€²
1 = a1â€²â†’0â€² recover
=
a0â†’1 = 1 âˆ’a0 â‡’a0 = 1 âˆ’aâ€²
1
(172)
Similar steps for case p0 â‰¥1
2 â‰¥p1.
