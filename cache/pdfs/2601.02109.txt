--- Page 1 ---
Finite-State Decentralized Policy-Based Control With
Guaranteed Ground Coverage
Hossein Rastgoftar
Abstractâ€”We propose a finite-state, decentralized decision
and control framework for multi-agent ground coverage. The
approach decomposes the problem into two coupled components:
(i) the structural design of a deep neural network (DNN) induced
by the agentsâ€™ reference configuration, and (ii) policy-based
decentralized coverage control. Agents are classified as anchors
and followers, yielding a generic and scalable communication
architecture in which each follower interacts with exactly three
in-neighbors from the preceding layer, forming an enclosing
triangular communication structure. The DNN training weights
implicitly encode the spatial configuration of the agent team,
thereby providing a geometric representation of the environmen-
tal target set. Within this architecture, we formulate a compu-
tationally efficient decentralized Markov decision process (MDP)
whose components are time-invariant except for a time-varying
cost function defined by the deviation from the centroid of the
target set contained within each agentâ€™s communication triangle.
By introducing the concept of Anyway Output Controllability
(AOC), we assume each agent is AOC and establish decentralized
convergence to a desired configuration that optimally represents
the environmental target.
I. INTRODUCTION
Multi-agent ground coverage is a fundamental problem in
distributed control with applications in environmental moni-
toring, surveillance, and distributed sensing. A classical and
widely adopted approach is Voronoi-based coverage control,
in which agents iteratively move toward the centroids of their
Voronoi cells to optimize a spatial coverage objective in a
decentralized manner. This paradigm admits strong geomet-
ric interpretability and convergence guarantees and has been
extensively studied and extended, including density-weighted
coverage, constrained environments, and event-triggered im-
plementations [7], [14], [29].
Despite these advantages, Voronoi-based methods typically
rely on continuous-time dynamics, frequent neighbor recom-
putation, and explicit geometric constructions, which limit
scalability under communication constraints and complicate
integration with discrete decision-making and learning mech-
anisms. These limitations have motivated the development
of policy-based and learning-augmented decentralized frame-
works, including finite-state and Markov decision process
formulations, to address uncertainty and scalability in multi-
agent coordination [22], [28]. While such approaches provide
increased flexibility, they often lack explicit mechanisms for
encoding formation geometry into decentralized policies or for
imposing interpretable and structured information flow with
provable convergence properties.
Hossein Rastgoftar is with the Department of Aerospace and Mechanical
Engineering, Tucson, AZ 85719, USA hrastgoftar@arizona.edu
This paper addresses these challenges by introducing a
structured, policy-based framework that tightly couples inter-
agent communication, decision-making, and physical evolu-
tion, enabling scalable decentralized coverage while preserv-
ing geometric meaning and analytical tractability.
A. Related Work
Diffusion-based convergence and stability results for multi-
agent coverage are reported in [11], while decentralized cov-
erage using local density feedback and mean-field approxima-
tions is studied in [5]. Leaderâ€“follower coverage strategies,
including explicit separation between coordination and cover-
age objectives, are investigated in [3]. Adaptive decentralized
coverage methods are explored in [10], [26], and multiscale
continuous-time convergence analyses are presented in [16].
Applications to human-centered sensing and zone coverage
planning appear in [12], [25]. A substantial body of work
adopts Voronoi-based coverage control [1], [4], [18], [19],
typically establishing convergence via Lyapunov-based argu-
ments under kinematic or single-integrator agent abstractions.
Extensions addressing obstacles, failures, and leaderâ€“follower
structures are considered in [4], while experimental compar-
isons in complex urban environments are reported in [21].
Coverage control for heterogeneous agent teams has also re-
ceived increasing attention. Authors of [24] propose a hetero-
geneous coverage control framework that encodes qualitatively
different sensing capabilities through agent-specific density
functions in a locational cost, deriving a distributed gradient-
descent controller with additional boundary terms that ensures
convergence to critical points of the heterogeneous coverage
objective and demonstrates improved performance over het-
erogeneous Lloyd-type methods in experiments. A Voronoi-
based coverage control method for heterogeneous disk-shaped
robots, leveraging power diagrams and constrained centroidal
motion to ensure collision-free convergence to locally optimal
sensing configurations, is proposed in [2]. Sadeghi and Smith
address coverage control for multiple event types with hetero-
geneous robots by formulating an event-specific Voronoi par-
titioning framework and deriving distributed algorithms with
provable convergence to locally optimal sensing configurations
in both continuous and discrete environments [23]. A coverage
control framework for robots with heterogeneous maximum
speeds is presented in [15], formulating a temporal cost based
on multiplicatively weighted Voronoi diagrams and deriving
a gradient-based controller that yields time-optimal coverage
configurations. More recent work considers multi-resource and
persistent surveillance objectives [6], [13].
Learning-based approaches have formulated multi-agent
coverage as a decision process using reinforcement learning
arXiv:2601.02109v1  [eess.SY]  5 Jan 2026


--- Page 2 ---
and Markov decision models [8], [9], [17], [20], [27]. While
these methods offer scalability and adaptability, they typically
rely on unstructured communication, large or continuous state
spaces, and gradient-based optimization, limiting interpretabil-
ity and convergence analysis.
To clarify the distinction from existing learning-based cov-
erage approaches, we emphasize that this paper does not treat
multi-agent coverage as a generic reinforcement learning or
function approximation problem. Instead, inter-agent commu-
nication is explicitly architected through a hierarchical anchorâ€“
follower structure induced by a reference configuration, yield-
ing unidirectional, feedforward information flow. This struc-
ture allows the multi-agent system itself to be interpreted
as a dynamical neural network whose neurons correspond
to physical agents and whose activations are governed by
agent dynamics rather than algebraic mappings. Learning is
performed via forward-only, local updates without gradient
backpropagation or centralized critics, and each agent solves
a finite, time-invariant local Markov decision process defined
geometrically within its communication triangle. These fea-
tures fundamentally distinguish the proposed framework from
existing RL- and MDP-based coverage methods.
B. Contributions
This paper proposes a policy-based, decentralized frame-
work for coverage of unknown ground targets that scales to
arbitrarily large teams and is independent of individual agent
dynamics. The key idea is to reinterpret multi-agent coverage
as a structured dynamical system in which communication,
decision-making, and physical evolution are intrinsically cou-
pled. By organizing inter-agent communication according to a
reference configuration, the proposed approach induces a hi-
erarchical, feedforward coordination architecture that admits a
dynamical deep neural network interpretation while remaining
fully decentralized.
The main contributions are summarized as follows:
â€¢ Structured Communication and Dynamical DNN Rep-
resentation: A hierarchical anchorâ€“follower communica-
tion architecture is introduced that induces unidirectional,
feedforward information flow, enabling the multi-agent
system to be interpreted as a dynamical neural network
whose neurons correspond to physical agents.
â€¢ Forward-Only Learning with Local Observability:
Communication weights are learned using exclusively
forward, local updates without gradient backpropagation,
centralized critics, or global information.
â€¢ Decentralized Policy Learning via Local MDPs: Each
follower agent independently learns a transition policy
by solving a finite, time-invariant local Markov decision
process defined geometrically within its communication
triangle.
â€¢ Dynamics-Agnostic Coverage via Anyway Output
Controllability: The notion of Anyway Output Control-
lability decouples policy learning from specific agent
dynamics, enabling uniform application to heterogeneous
teams with nonlinear, underactuated, or high-order dy-
namics.
Fig. 1: Geometric representation of the first- and second-
tier communication weights, ğ‘¤ğ‘–,ğ‘–1 âˆˆWğ‘–,1 and ğ‘¤ğ‘–,ğ‘–2 âˆˆWğ‘–,2,
for ğ‘€ğ‘–= 5, where agent ğ‘–âˆˆV \ V0 has three in-neighbors
N (ğ‘–) = {ğ‘–1,ğ‘–2,ğ‘–3}.
C. outline
This paper is organized as follows: The problem statement is
presented in Section II. An algorithmic approach for structur-
ing the DNN based on the agent teamâ€™s reference configuration
is developed in Section III. Training the DNN weighsts is
defined as and MDP and presented in Section IV. Stability
and convergence of the proposed policy-based decentralized
coverage solution are proven in Section V. Simulation results
are presented in Section VI, followed by the conclusion in
Section VII.
II. PROBLEM STATEMENT
We consider a team of ğ‘agents indexed by
V = {1,..., ğ‘},
tasked with providing aerial coverage of a finite set of ground
targets D. Agents are classified as boundary or interior ac-
cording to a reference configuration, and their interactions are
structured via a Delaunay neighbor network (DNN) to enable
scalable coverage. Low-level control dynamics are abstracted,
and each agent is assumed to satisfy the following output
reachability property.
Definition 1 (Anyway Output Controllability (AOC)). Let
agent ğ‘–âˆˆV be described by
(
xğ‘–[ğ‘¡+1] = fğ‘–(xğ‘–[ğ‘¡],uğ‘–[ğ‘¡]) ,
rğ‘–[ğ‘¡] = hğ‘–(xğ‘–[ğ‘¡]) ,
(1)
where xğ‘–, uğ‘–, and rğ‘–denote the state, input, and output,
respectively. Agent ğ‘–is said to be Anyway Output Controllable
if, for any initial state xğ‘–[ğ‘¡] âˆˆXğ‘–, there exist an admissible input
sequence uğ‘–(Â·) and a finite time ğ‘‡ğ‘–(xğ‘–[ğ‘¡]) < âˆsuch that
rğ‘–[ğ‘¡+ğ‘‡ğ‘–(xğ‘–[ğ‘¡])] âˆˆPğ‘–,
where Xğ‘–âŠ‚R2 and Pğ‘–âŠ‚R2 are compact sets.


--- Page 3 ---
Assumption 1. The time discretization is chosen uniformly
across agents and sufficiently large such that the output
reachability time satisfies
ğ‘‡ğ‘–(xğ‘–[ğ‘¡]) = 1,
for all ğ‘–âˆˆV and all initial states xğ‘–[ğ‘¡] âˆˆXğ‘–. Consequently,
rğ‘–[ğ‘¡+1] âˆˆPğ‘–,
holds for any admissible initial condition.
The objective of this paper is to design a decentralized
framework that enables structured agent interactions and adap-
tive coverage of distributed targets. Specifically, we address the
following problems.
Problem 1 (DNN Structuring).
Given a reference configuration, a deterministic algorithm
uniquely induces a DNN communication architecture from
the agentsâ€™ initial spatial distribution. The agent set V is
partitioned into ğ‘€+1 disjoint subsets
{Vğ‘™}ğ‘€
ğ‘™=0,
ğ‘€
Ã˜
ğ‘™=0
Vğ‘™= V,
where V0 consists of anchor nodes, and each agent ğ‘–âˆˆVğ‘™,
ğ‘™â‰¥1, has exactly three in-neighbors.
Problem
2
(Decentralized
Coverage
via
Discrete
DNN
Weights).
Design a decentralized control and learning mechanism that
enables the agent team to achieve high-level coverage of the
distributed target set D. For each agent ğ‘–âˆˆV \V0, the DNN
training weights are restricted to finite discrete sets
Wğ‘–=
3ğ‘âˆ’ğ‘
3ğ‘€ğ‘–
: ğ‘= 1,Â·Â·Â· , ğ‘€ğ‘–, ğ‘= 1,2

(2)
where ğ‘€ğ‘–âˆˆN determines the discretization resolution. These
set Wğ‘–consists of uniformly distributed values in (0,1),
ensuring strictly positive and bounded training weights. For
each agent ğ‘–âˆˆV \V0, let N (ğ‘–) denote its set of in-neighbors.
The communication weight between agent ğ‘–and neighbor
ğ‘—âˆˆN (ğ‘–) is denoted by ğ‘¤ğ‘–, ğ‘—and satisfies
ğ‘¤ğ‘–, ğ‘—âˆˆWğ‘–,
âˆ€ğ‘–âˆˆV \V0, âˆ€ğ‘—âˆˆN (ğ‘–),
(3a)
âˆ‘ï¸
ğ‘—âˆˆN(ğ‘–)
ğ‘¤ğ‘–, ğ‘—= 1,
âˆ€ğ‘–âˆˆV \V0.
(3b)
For clarity, Fig. 1 illustrates the geometric representation
of the communication weights for ğ‘€ğ‘–= 5, where agent ğ‘–âˆˆ
V \ V0 interacts with three in-neighbors, N (ğ‘–) = {ğ‘–1,ğ‘–2,ğ‘–3}.
The corresponding discrete weight set
Wğ‘–= {0.067,0.133,0.267,0.333,0.467,0.533,0.667,0.733,0.867}
is obtained from (2). Problem 2 formulates the coverage prob-
lem as a decentralized MDP with time-invariant state space,
action space, state transition model, and discount factor, and a
time-varying cost function capturing coverage performance.
The detailed MDP formulation and DNN weight training
procedure are presented in Section IV.
III. STRUCTURING OF THE COVERAGE DNN
The DNN communication architecture is induced by parti-
tioning the agent set V, based on a reference configuration,
into ğ‘€+1 disjoint groups indexed by M := {0,1,..., ğ‘€}. This
induces the decomposition
V =
Ã˜
ğ‘™âˆˆM
Vğ‘™,
Vğ‘™âˆ©Vâ„= âˆ…, ğ‘™â‰ â„,
with Vğ‘™âŠ‚V and cardinality |Vğ‘™| = ğ‘ğ‘™for all ğ‘™âˆˆM. Define
the cumulative index
ğ‘ƒğ‘™=
(Ãğ‘™
â„=0 ğ‘â„,
ğ‘™âˆˆM \ {0},
0,
ğ‘™= 0,
and index the agents by {ğ‘1,...,ğ‘ğ‘}. Then
Vğ‘™= {ğ‘ğ‘ƒğ‘™âˆ’1+1,...,ğ‘ğ‘ƒğ‘™}.
(4)
To define inter-agent communication, introduce the nested
sets
Lğ‘™=
(
Vğ‘™,
ğ‘™âˆˆ{0, ğ‘€},
Vğ‘™âˆªLğ‘™âˆ’1,
otherwise,
âˆ€ğ‘™âˆˆM.
(5)
Let I(ğ‘–,ğ‘™) âŠ†Lğ‘™âˆ’1 denote the set of neurons in layer ğ‘™âˆ’1 con-
nected to neuron ğ‘–âˆˆLğ‘™. The DNN architecture is induced from
the agentsâ€™ initial formation via the algorithmic procedure
in Algorithm 1, which constructs a directed graph G(V,E)
that admits a DNN representation. In particular, G(V,E)
determines: (i) the number of DNN layers (ğ‘€+ 1), (ii) a
partition of V into subsets V0,...,Vğ‘€, and (iii) the inter-
layer neuron connectivity.
Given E âŠ‚V Ã— V, the in-neighbor set of agent ğ‘–âˆˆV is
defined as
N (ğ‘–) := { ğ‘—âˆˆV | ( ğ‘—,ğ‘–) âˆˆE }.
(6)
Then, for each layer ğ‘™âˆˆM \{0}, the interconnection set I(ğ‘–,ğ‘™)
for neuron ğ‘–âˆˆLğ‘™is given by
I(ğ‘–,ğ‘™) =
(
N (ğ‘–),
ğ‘–âˆˆVğ‘™= Lğ‘™\ Lğ‘™âˆ’1,
{ğ‘–},
ğ‘–âˆˆLğ‘™\Vğ‘™,
ğ‘™âˆˆM \ {0}.
(7)
Remark 1. Algorithm 1 applies to decentralized multi-agent
systems in Rğ‘›; the ground coverage setting considered here
corresponds to ğ‘›= 2.
For each agent ğ‘–âˆˆV, the following position-related quan-
tities are used throughout the paper:
â€¢ ağ‘–: reference position of agent ğ‘–in the initial (reference)
configuration.
â€¢ rğ‘–[ğ‘¡]: actual position of agent ğ‘–at discrete time ğ‘¡, given
by the output of its control system.
â€¢ cğ‘–[ğ‘¡]: reference input to the control system of agent ğ‘–at
time ğ‘¡; for ğ‘–âˆˆV0, cğ‘–[ğ‘¡] is constant, while for ğ‘–âˆˆV \V0,
it is defined as a weighted average of the actual positions
of its in-neighbor agents.
â€¢ pğ‘–: desired position of agent ğ‘–; pğ‘–is known for ğ‘–âˆˆV0 and
unknown for ğ‘–âˆˆV \V0.
Moreover, for all ğ‘–âˆˆV \ V0, the reference input satisfies
cğ‘–[ğ‘¡] = pğ‘–.


--- Page 4 ---
(a) ğ‘™= 0 âˆˆM
(b) ğ‘™= 1 âˆˆM
(c) ğ‘™= 2 âˆˆM
(d) M = {0, 1, 2}
Fig. 2: Cell decompositions of the convex hull defined by the boundary agents for specifying DNN layer interconnections.
A. Step 1: Agent Classification
The agent set V is decomposed as V = VğµâˆªVğ¼, where Vğµ
and Vğ¼are disjoint. The set Vğµ= {ğ‘1,...,ğ‘ğ‘ğµ} consists of
the boundary agents located at the vertices of the convex hull
enclosing the interior agents in Vğ¼. The polytope defined by
Vğµis referred to as the leading polytope. Given the boundary
set Vğµ, the core agent is identified using one of two criteria:
(i) the interior agent minimizing the aggregate distance to the
boundary agents,
ğ‘ğ‘ğµ+1 = arg
min
ğ‘–âˆˆV\Vğµ
âˆ‘ï¸
ğ‘—âˆˆVğµ
âˆ¥ağ‘–âˆ’ağ‘—âˆ¥,
(8)
or (ii) the agent located near the center of the target domain
D.
Given the set Vğµand the designated core agent ğ‘ğ‘ğµ+1, we
define the set V0 as V0 = Vğµâˆª

ğ‘ğ‘ğµ+1
	
. According to Eq. (5),
we have V0 = L0. The leading polytope can be partitioned
into ğ‘š0 distinct simplex cells. Consequently, the set L0 can
be expressed as a union of these simplices:
L0 =
ğ‘š0
Ã˜
â„=1
R0,â„,
(9)
where R0,â„determines vertices of the â„-th simplex cell of
the leading polytope. For better clarification, an agent team
with ğ‘= 13 agents forms a 2-dimensional formation shown
in Fig. 2 (a), where Vğµ= {1,Â·Â·Â· ,4} (ğ‘ğµ= 4) defines the
boundary agents. Agent ğ‘5 = 12 âˆˆV is assigned by (8) as
the core leader, therefore, V0 = L0 = {1,Â·Â·Â· ,4,12} defines
agents of the first layer. The convex hull defined by L0 can be
decomposed into ğ‘š0 = 4 triangular cells with vertices defined
by R0,1 = {1,2,12}, R0,2 = {2,3,12}, R0,3 = {3,4,12}, and
R0,4 = {4,1,12}.
B. Step 2: Expansion and Structuring
Set V can be expressed as V = Lğ‘™âˆ’1 âˆªÂ¯Lğ‘™âˆ’1, for every
ğ‘™âˆˆM \ {0}, where
Â¯Lğ‘™âˆ’1 = V \ Lğ‘™âˆ’1 defines the agents not
belonging to Lğ‘™âˆ’1. Note that Vğ‘™âŠ‚Â¯Lğ‘™âˆ’1, if
Â¯Lğ‘™âˆ’1 â‰ âˆ…. Also,
Lğ‘™âˆ’1 consists of ğ‘šğ‘™âˆ’1 distinct simplices that cover the domain
contained by Lğ‘™âˆ’1. Therefore, Lğ‘™âˆ’1 can be expressed as:
Lğ‘™âˆ’1 =
ğ‘šğ‘™âˆ’1
Ã˜
â„=1
Rğ‘™âˆ’1,â„,
ğ‘™âˆˆM \ {0}
(10)
where Rğ‘™âˆ’1,1 through Rğ‘™âˆ’1,ğ‘šğ‘™âˆ’1 are vertices of distinct simplex
cells that cover the domain contained by Lğ‘™âˆ’1. Given a set
Rğ‘™âˆ’1,â„for each â„= 1,...,ğ‘šğ‘™âˆ’1, we denote by CONV(Rğ‘™âˆ’1,â„)
the convex hull formed by the elements of Rğ‘™âˆ’1,â„. We also
define Hğ‘™âˆ’1,â„âŠ‚Â¯Lğ‘™âˆ’1 as the set of all nodes that lie within
this convex hull, i.e., all nodes contained in CONV(Rğ‘™âˆ’1,â„).
If Hğ‘™âˆ’1,â„â‰ âˆ…, then:
â€¢ Rğ‘™âˆ’1,â„has a mentee that is determined by:
ğœ‡ğ‘™âˆ’1,â„= argmin
ğ‘—âˆˆHğ‘™âˆ’1,â„
Â©Â­
Â«
âˆ‘ï¸
ğ‘ŸâˆˆRğ‘™âˆ’1,â„
âˆ¥ağ‘Ÿâˆ’ağ‘—âˆ¥ÂªÂ®
Â¬
, â„= 1,Â·Â·Â· ,ğ‘šğ‘™âˆ’1.
(11)
â€¢ In-neighbors of ğœ‡ğ‘™âˆ’1,â„âˆˆLğ‘™is defined by N  ğœ‡ğ‘™âˆ’1,â„
 =
Rğ‘™âˆ’1,â„.
Note that the mentee of Rğ‘™âˆ’1,â„, denoted by ğœ‡ğ‘™âˆ’1,â„, does not
exist if Hğ‘™âˆ’1,â„= âˆ…). Then, for every ğ‘™âˆˆM\{0}, Vğ‘™aggengates
the mentess of all non-empty simplices of Lğ‘™âˆ’1 and defined
as follows:
Vğ‘™=

ğ‘–âˆˆHğ‘™âˆ’1,â„: Hğ‘™âˆ’1,â„â‰ âˆ…, ğ‘–= argmin
ğ‘—âˆˆHğ‘™âˆ’1,â„â‰ âˆ…
Â©Â­
Â«
âˆ‘ï¸
ğ‘ŸâˆˆRğ‘™âˆ’1,â„
âˆ¥ağ‘Ÿâˆ’ağ‘—âˆ¥ÂªÂ®
Â¬
,
â„= 1,Â·Â·Â· ,ğ‘šğ‘™âˆ’1

.
(12)
Therefore, for every ğ‘™âˆˆM \ {0}, the number of agents in Vğ‘™
satisfies ğ‘ğ‘™= |Vğ‘™| â‰¤ğ‘šğ‘™âˆ’1. This inequality holds because not
all simplices in Lğ‘™âˆ’1 necessarily have mentee agents assigned
to them. By knowing Vğ‘™and Lğ‘™âˆ’1, Lğ‘™is defined by Eq. (5).
C. Step 3: Cell Decomposition Update
If Hğ‘™âˆ’1,â„â‰ âˆ…, then, ğœ‡ğ‘™âˆ’1,â„exists and ğ¶ğ‘‚ğ‘ğ‘‰(Rğ‘™âˆ’1,â„) can
be decomposed into ğ‘›+ 1 new simplex cells all sharing
ğœ‡ğ‘™âˆ’1,â„. Therefore, the leading polytope is deterministically
decomposed into ğ‘šğ‘™distinct simplices by knowing Vğ‘™, where
ğ‘šğ‘™â‰¤(ğ‘›+1)ğ‘šğ‘™âˆ’1.
For better clarification, Fig. 2 shows how Algorithm 1 is
implemented to specify the inter-agent communication based
on the agent team reference configuration. As shown in Fig. 2
(a), ğ¶ğ‘‚ğ‘ğ‘‰ R0,â„
 is a traingular cell that contains at least one
agent for â„= 1,Â·Â·Â· ,4. Therefore, ğ‘š1 = 12 and ğ¶ğ‘‚ğ‘ğ‘‰ R0,â„

is decomposed into three tringular cells shown in Fig. 2 (a),
each shown in blue. For layer ğ‘™= 1, V1 = {11,10,6,5} where
11, 10, 6, and 5 are mentors of R0,1, R0,2, R0,3, and R0,4, re-
spectively. Also, ğ‘š1 = 12; R1,1 = {1,2,11}, R1,2 = {2,12,11},
R1,3 = {12,1,11}, R1,4 = {2,3,10}, R1,5 = {3,12,10}, R1,6 =
{12,2,10}, R1,7 = {3,4,6}, R1,8 = {4,12,6}, R1,9 = {12,3,6},
R1,10 = {4,1,5}, R1,11 = {1,12,5}, and R1,12 = {4,5,12} define
the verities of 12 tringular cells shown in Fig. 2 (b). As
shown in Fig. 2 (b), ğ¶ğ‘‚ğ‘ğ‘‰ R1,â„
 contains a single agent


--- Page 5 ---
Algorithm 1 DNN Structure based on reference formation.
1: Get: Agentsâ€™ initial positions a1 through ağ‘
2: Obtain: Edge set E, ğ‘€= |M|, and V0 through Vğ‘€.
3: Assign boundary agents Vğµ=

ğ‘1,Â·Â·Â· ,ğ‘ğ‘ğµ
	
.
4: Assign core agent ğ‘ğ‘ğµ+1 using Eq. (8).
5: Define V0 = Vğµâˆª

ğ‘ğ‘ğµ+1
	
.
6: Define L0 = V0 and Â¯L0 = V \ L0.
7: Decompose the leading polytope into ğ‘š0 simplex cells
with vertices defined by R0,1, Â·Â·Â·, and R0,ğ‘š0.
8: ğ‘™= 1.
9: while Â¯Lğ‘™âˆ’1 â‰ âˆ…do
10:
ğ‘šğ‘™= 0, ğ‘ğ‘™= 0, and Vğ‘™= âˆ…;
11:
for < â„= 1,Â·Â·Â· ,ğ‘šğ‘™âˆ’1> do
12:
if ğ¶ğ‘‚ğ‘ğ‘‰(Rğ‘™âˆ’1,â„) contains at least one agent then
13:
Assign mentee of Rğ‘™âˆ’1,â„, denoted by ğœ‡ğ‘™âˆ’1,â„;
14:
Define
neighbors
of
ğœ‡ğ‘™âˆ’1,â„:
N  ğœ‡ğ‘™âˆ’1,â„
 =
Rğ‘™âˆ’1,â„;
15:
ğ‘ğ‘™â†ğ‘ğ‘™+1;
16:
ğ‘šğ‘™â†ğ‘šğ‘™+ğ‘›+1;
17:
Vğ‘™= Vğ‘™âˆª

ğœ‡ğ‘™âˆ’1,â„
	
;
18:
Specify Rğ‘™,(ğ‘›+1) (ğ‘ğ‘™âˆ’1)+1, Â·Â·Â·, and Rğ‘™,(ğ‘›+1) ğ‘ğ‘™
19:
end if
20:
end for
21:
Obtain Lğ‘™, and Â¯Lğ‘™.
22:
ğ‘™â†ğ‘™+1.
23: end while
24: ğ‘€= ğ‘™âˆ’1.
if â„= 3,7,10,11 and ğ¶ğ‘‚ğ‘ğ‘‰ R1,â„
 does not contain an agent
otherwise. Therefore, V2 = {7,8,9,13}, and 7, 8, 9, and 13
are mentees of R1,3 = N (7), R1,7 = N (8), R1,10 = N (9), and
R1,11 = N (13) for ğ‘™= 2 (see Fig. 2 (c)). Because
Â¯L = âˆ…,
the while loop of Algorithm 1 stops at ğ‘™= ğ‘€= 2, and as
a result, the DNN shown in Fig. 2 (d) specifies the inter-agent
communications.
IV. TRAINING THE DNN WEIGHTS
The DNN is trained in a fully decentralized and agent-
centric manner, wherein each agent ğ‘–âˆˆV \ V0 indepen-
dently optimizes its local communication strategy. Specifically,
agent ğ‘–âˆˆV \ V0 assigns adaptive communication weights
ğ‘¤ğ‘–, ğ‘—âˆˆWğ‘–to its in-neighbors ğ‘—âˆˆN (ğ‘–) by solving a local
MDP whose components and operation are described in Sec-
tions IV-A and IV-B, respectively. This formulation enables
scalable and communication-aware learning without central-
ized coordination, while guaranteeing coverage convergence
via the theoretical results established in Section V.
A. MDP Components
Agent ğ‘–âˆˆV \V0 is associated with an MDP defined as
Mğ‘–
 Sğ‘–,Dğ‘–,Ağ‘–,ğ‘”ğ‘–, ğ‘ƒğ‘–,Cğ‘–,ğ›¾ğ‘–
,
where the components of Mğ‘–are detailed below.
State Set: The state space of agent ğ‘–âˆˆV \ V0 is defined by
Sğ‘–and partitioned as
Sğ‘–= Sğ¶
ğ‘–Â¤âˆªSğ‘ˆ
ğ‘–,
(13)
where Sğ¶
ğ‘–and Sğ‘ˆ
ğ‘–denote the contained and uncontained sub-
spaces, respectively. For each agent ğ‘–âˆˆV \V0, the contained
subspace Sğ¶
ğ‘–
is obtained by discretizing the communication
triangle Tğ‘–[ğ‘¡] formed by the instantaneous positions of its in-
neighbors N (ğ‘–). Specifically,
Sğ¶
ğ‘–= {ğ‘ 1,...,ğ‘ ğ‘€2
ğ‘–},
where ğ‘€ğ‘–= |Wğ‘–| (see (2)), yielding |Sğ¶
ğ‘–| = ğ‘€2
ğ‘–. Each state
ğ‘ âˆˆSğ¶
ğ‘–
corresponds to a triangular cell with centroid
cğ‘–(ğ‘ ) =
âˆ‘ï¸
ğ‘—âˆˆN(ğ‘–)
ğ‘¤ğ‘–, ğ‘—(ğ‘ )rğ‘—,
ğ‘ âˆˆSğ¶
ğ‘–,
(14)
where the barycentric weights ğ‘¤ğ‘–, ğ‘—âˆˆWğ‘–satisfy (3). The
uncontained subspace Sğ‘ˆ
ğ‘–is represented by a single aggregate
state capturing all positions outside Tğ‘–[ğ‘¡]. Fig. 3 illustrates the
resulting discretization.
Local Target Set: Let d ğ‘—: D â†’R2 denote the position
of environmental target ğ‘—âˆˆD. The set of targets locally
observable by agent ğ‘–âˆˆV \V0 is defined as
Dğ‘–[ğ‘¡] = { ğ‘—âˆˆD : d ğ‘—[ğ‘¡] âˆˆTğ‘–[ğ‘¡]},
âˆ€ğ‘–âˆˆV \V0.
(15)
Action Set: The action space Ağ‘–encodes admissible transi-
tions over Sğ‘–and is defined as a mapping Ağ‘–: Sğ‘–â†’Sğ‘–. Two
triangular cells are considered neighbors if they share a com-
mon edge. Accordingly, each state in Sğ¶
ğ‘–admits at most three
neighboring cells, resulting in at most four actions (including
self-transition). If ğ‘ âˆˆSğ¶
ğ‘–, then Ağ‘–(ğ‘ ) âŠ†Sğ¶
ğ‘–, whereas actions
from ğ‘ âˆˆSğ‘ˆ
ğ‘–transition into the contained subspace. Fig. 3(b)â€“
(d) illustrates this construction.
Goal State: The goal state ğ‘”ğ‘–âˆˆSğ¶
ğ‘–
is selected to maximize
the coverage quality of the local target set Dğ‘–. Define
hğ‘–[ğ‘¡] =
ï£±ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£³
1
|Dğ‘–[ğ‘¡]|
âˆ‘ï¸
ğ‘—âˆˆDğ‘–[ğ‘¡]
d ğ‘—[ğ‘¡],
Dğ‘–[ğ‘¡] â‰ âˆ…,
1
3
âˆ‘ï¸
ğ‘—âˆˆN(ğ‘–)
rğ‘—[ğ‘¡],
Dğ‘–[ğ‘¡] = âˆ…,
(16)
which represents the centroid of the locally sensed targets
when available, and otherwise the centroid of the communi-
cation triangle. The goal state ğ‘”ğ‘–is then defined as the unique
triangular cell containing hğ‘–[ğ‘¡].
Transition Dynamics: The transition kernel is modeled as
a linear combination of fixed base transition measures. Let
Î¦ğ‘–: Sğ‘–Ã—Ağ‘–â†’Rğ‘‘denote a feature map, and let {ğœ‡ğ‘–, ğ‘—}ğ‘‘
ğ‘—=1 be
graph-constrained base transition distributions. The resulting
transition kernel is
ğ‘ƒğ‘–(Â· | ğ‘ ,ğ‘) =
ğ‘‘
âˆ‘ï¸
ğ‘—=1
ğœ™ğ‘–, ğ‘—(ğ‘ ,ğ‘) ğœ‡ğ‘–, ğ‘—(Â·),
âˆ€ğ‘–âˆˆV \V0.
(17)
Cost Function: Let Â¯r(ğ‘ ) denote the centroid of the triangular
cell associated with state ğ‘ âˆˆSğ‘–. The MDP employs a state-
dependent cost function defined as
Cğ‘–(ğ‘ ) =
(
ğ›¼
Â¯r(ğ‘ ) âˆ’Â¯r(ğ‘”ğ‘–)
,
ğ‘ â‰ ğ‘”ğ‘–,
ğ›¼
Â¯r(ğ‘ ) âˆ’Â¯r(ğ‘”ğ‘–)
âˆ’ğ›½,
ğ‘ = ğ‘”ğ‘–,
(18)
where ğ›¼> 0 and ğ›½> 0 are design parameters. The term
proportional to ğ›¼penalizes deviation from the goal state ğ‘”ğ‘–,
while the terminal reward ğ›½incentivizes reaching the goal.


--- Page 6 ---
(a)
(b)
(c)
(d)
Fig. 3: (a) Schematic of Sğ‘ˆ
ğ‘–, shown by light red, defining a single state outside the communication triangle of agent ğ‘–âˆˆV \V0.
Schematic illustration of (b) an interior state, (c) a boundary state, and (d) a nodal state ğ‘ âˆˆSğ¶
ğ‘–, having three, two, and one
neighboring states.
Discount Factor: The parameter ğ›¾ğ‘–âˆˆ(0,1) denotes the dis-
count factor.
B. Operation
An agent ğ‘–âˆˆV \ V0 may lie either inside or outside its
communication triangle Tğ‘–[ğ‘¡] at time ğ‘¡. If the agent position
satisfies rğ‘–[ğ‘¡] âˆ‰Tğ‘–[ğ‘¡], we set ğ‘€ğ‘–= 1. In this case, the corre-
sponding MDP state satisfies ğ‘ âˆˆSğ‘ˆ
ğ‘–, the contained subspace
reduces to a singleton |Sğ¶
ğ‘–| = 1, and the action mapping
Ağ‘–(ğ‘ ) = Sğ¶
ğ‘–
assigns a single admissible successor state to the
uncontained state. Consequently, the optimal action is trivial
and no Bellman recursion is required.
In contrast, when ğ‘ âˆˆSğ¶
ğ‘–, the admissible action set satisfies
Ağ‘–(ğ‘ ) âŠ†Sğ¶
ğ‘–. In this case, the optimal value function is
computed via the Bellman optimality equation
ğ‘‰âˆ—
ğ‘–(ğ‘ ) =
min
ğ‘âˆˆAğ‘–(ğ‘ )

Cğ‘–(ğ‘ ,ğ‘) + ğ›¾Ã
ğ‘ â€²âˆˆSğ‘–
ğ‘ƒğ‘–(ğ‘ â€² | ğ‘ ,ğ‘)ğ‘‰âˆ—
ğ‘–(ğ‘ â€²)

,
ğ‘ âˆˆSğ¶
ğ‘–,
(19)
with the corresponding optimal policy given by
ğœ‹âˆ—
ğ‘–(ğ‘ ) = arg
min
ğ‘âˆˆAğ‘–(ğ‘ )

Cğ‘–(ğ‘ ,ğ‘) + ğ›¾Ã
ğ‘ â€²âˆˆSğ‘–
ğ‘ƒğ‘–(ğ‘ â€² | ğ‘ ,ğ‘)ğ‘‰âˆ—
ğ‘–(ğ‘ â€²)

,
ğ‘ âˆˆSğ¶
ğ‘–.
(20)
V. STABILITY AND CONVERGENCE
In this section, we provide the proofs for the stablility and
converegence of the proposed decentralized coverage method.
Definition 2. Let V = {ğ‘1,...,ğ‘ğ‘}. Define
y = vec
rğ‘1
Â·Â·Â·
rğ‘ğ‘
ğ‘‡
âˆˆR2ğ‘.
(21)
Definition 3. Let Vğ‘™be defined as in (4). Define
yğ‘™= vec
rğ‘ğ‘ƒğ‘™âˆ’1+1
Â·Â·Â·
rğ‘ğ‘ƒğ‘™
ğ‘‡
âˆˆR2ğ‘ğ‘™.
(22)
Assumption 2. For any ğ‘ âˆˆSğ‘ˆ
ğ‘–, the target set Pğ‘–is a triangle
strictly contained in the communication triangle Tğ‘–(ğ‘¡) and
edge-aligned with Tğ‘–(ğ‘¡).
Assumption 3. For any ğ‘ âˆˆSğ¶
ğ‘–, the target set Pğ‘–is a triangle
strictly contained in the target triangle induced by the optimal
next state ğœ‹âˆ—
ğ‘–(ğ‘ ) and edge-aligned with it.
Theorem 1. Assume each agent ğ‘ğ‘–âˆˆV \V0 satisfies the AOC
property and, in Assumption 1, the target set Pğ‘ğ‘–is replaced
by the time-varying communication triangle Tğ‘ğ‘–[ğ‘¡]. Then the
coverage evolution satisfies
y[ğ‘¡+1] = ğšª[ğ‘¡]y[ğ‘¡],
(23)
where ğšª[ğ‘¡] is row-stochastic for all ğ‘¡. Consequently, (23)
defines a time-inhomogeneous Markov process.
Proof. For each anchored agent ğ‘ğ‘–âˆˆV0, rğ‘ğ‘–[ğ‘¡] = pğ‘ğ‘–for all
ğ‘¡. For any ğ‘ğ‘–âˆˆV \ V0, the AOC property and the transition
rules in Section IV-B imply
rğ‘ğ‘–[ğ‘¡+1] =
âˆ‘ï¸
ğ‘ğ‘—âˆˆN(ğ‘ğ‘–)
ğ›¼ğ‘ğ‘–,ğ‘ğ‘—[ğ‘¡]rğ‘ğ‘—[ğ‘¡],
ğ›¼ğ‘ğ‘–,ğ‘ğ‘—[ğ‘¡] â‰¥0,
âˆ‘ï¸
ğ‘ğ‘—âˆˆN(ğ‘ğ‘–)
ğ›¼ğ‘ğ‘–,ğ‘ğ‘—[ğ‘¡] =1.
Stacking all agent positions yields (23) with entries
Î“ğ‘–, ğ‘—[ğ‘¡] =
ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³
1,
ğ‘ğ‘–âˆˆV0, ğ‘–= ğ‘—,
ğ›¼ğ‘ğ‘–,ğ‘ğ‘—[ğ‘¡],
ğ‘ğ‘–âˆˆV \V0, ğ‘ğ‘—âˆˆN (ğ‘ğ‘–),
0,
otherwise.
Each row of ğšª[ğ‘¡] is nonnegative and sums to one; hence ğšª[ğ‘¡]
is row-stochastic.
Theorem 2. Consider y[ğ‘¡+1] = ğšª[ğ‘¡]y[ğ‘¡], where each ğšª[ğ‘¡] âˆˆ
Rğ‘Ã—ğ‘is row-stochastic. Let V0 denote anchored agents
satisfying rğ‘ğ‘–[ğ‘¡] â‰¡pğ‘ğ‘–for all ğ‘ğ‘–âˆˆV0 and all ğ‘¡. After reordering
agents, write
ğšª[ğ‘¡] =
 I
0
B[ğ‘¡]
A[ğ‘¡]

,
y[ğ‘¡] =

p
yğ¹[ğ‘¡]

.
(24)
Assume there exist ğ‘‡â‰¥1 and ğœ‚âˆˆ(0,1) such that:
(C1) Assumptions 2â€“3 hold.
(C2) For every ğ‘¡and every follower index ğ‘–,
âˆ‘ï¸
ğ‘—âˆˆV0
 ğšª[ğ‘¡+ğ‘‡âˆ’1] Â·Â·Â·ğšª[ğ‘¡]
ğ‘–ğ‘—â‰¥ğœ‚.
(25)
Then the follower subsystem
yğ¹[ğ‘¡+1] = A[ğ‘¡]yğ¹[ğ‘¡] +B[ğ‘¡]p
(26)
is globally exponentially stable: for all ğ‘¡â‰¥ğ‘ ,
âˆ¥ğš½(ğ‘¡,ğ‘ )âˆ¥âˆ= âˆ¥A[ğ‘¡âˆ’1] Â·Â·Â·A[ğ‘ ]âˆ¥âˆâ‰¤(1âˆ’ğœ‚)âŒŠğ‘¡âˆ’ğ‘ 
ğ‘‡âŒ‹.
(27)


--- Page 7 ---
Fig. 4: The initial formation of the agent team and the
communication links.
Moreover, yğ¹[ğ‘¡] converges, and each follower coordinate con-
verges to a convex combination of the anchorsâ€™ coordinates.
Proof. From (24) and anchor invariance, the follower dynam-
ics are (26). Row-stochasticity of ğšª[ğ‘¡] implies A[ğ‘¡] â‰¥0 and
A[ğ‘¡]1 â‰¤1, i.e., A[ğ‘¡] is substochastic.
Define the ğ‘‡-step product
M[ğ‘¡] =ğšª[ğ‘¡+ğ‘‡âˆ’1] Â·Â·Â·ğšª[ğ‘¡] =

I
0
Bğ‘‡[ğ‘¡]
Ağ‘‡[ğ‘¡]

,
Ağ‘‡[ğ‘¡] =A[ğ‘¡+ğ‘‡âˆ’1] Â·Â·Â·A[ğ‘¡].
Since M[ğ‘¡] is row-stochastic, for each follower row ğ‘–âˆˆV \V0,
âˆ‘ï¸
ğ‘—âˆˆV0
(M[ğ‘¡])ğ‘–ğ‘—+
âˆ‘ï¸
â„“
(Ağ‘‡[ğ‘¡])ğ‘–â„“= 1.
By (C2), Ã
ğ‘—âˆˆV0(M[ğ‘¡])ğ‘–ğ‘—â‰¥ğœ‚, hence Ã
â„“(Ağ‘‡[ğ‘¡])ğ‘–â„“â‰¤1 âˆ’ğœ‚.
Therefore,
âˆ¥Ağ‘‡[ğ‘¡]âˆ¥âˆ= max
ğ‘–
âˆ‘ï¸
â„“
(Ağ‘‡[ğ‘¡])ğ‘–â„“â‰¤1âˆ’ğœ‚.
(28)
Let ğš½(ğ‘¡,ğ‘ ) = A[ğ‘¡âˆ’1] Â·Â·Â·A[ğ‘ ]. Grouping the product into
blocks of length ğ‘‡and using submultiplicativity of âˆ¥Â· âˆ¥âˆ
with (28) gives (27), proving exponential contraction of the
homogeneous system.
Unrolling (26) yields
yğ¹[ğ‘¡] = ğš½(ğ‘¡,0)yğ¹[0] +
ğ‘¡âˆ’1
âˆ‘ï¸
ğœ=0
ğš½(ğ‘¡,ğœ+1)B[ğœ]p.
Because rows of [B[ğ‘¡] A[ğ‘¡]] sum to 1, âˆ¥B[ğ‘¡]âˆ¥âˆâ‰¤1, and
(27) implies âˆ¥ğš½(ğ‘¡,ğœ+ 1)âˆ¥âˆdecays geometrically, the series
converges; hence yğ¹[ğ‘¡] converges.
Finally, each follower update is a convex combination of
neighbor states and fixed anchors, so each follower coordinate
remains in the convex hull of the anchors (and the shrinking
follower contribution), and the limit is a convex combination
of the anchorsâ€™ coordinates.
To establish asymptotic convergence, we introduce pğ‘–as
the desired position of each agent ğ‘–âˆˆV, which is constant
when the target set D is stationary. The desired positions
pğ‘–are known to all anchored agents ğ‘–âˆˆV0. In contrast, the
desired positions of follower agents ğ‘–âˆˆV \V0 are not locally
available to them. Nevertheless, the quantities pğ‘–are used
solely as analytical constructs to characterize the decentralized
convergence of the follower dynamics.
Definition 4 (Desired communication triangle). For any agent
ğ‘ğ‘–âˆˆV \ V0, let N (ğ‘ğ‘–) = {ğ‘ğ‘–1,ğ‘ğ‘–2,ğ‘ğ‘–3} denote its in-neighbor
set. The desired communication triangle of ğ‘ğ‘–is defined as
ËœTğ‘ğ‘–â‰œconv

pğ‘ğ‘–1 , pğ‘ğ‘–2, pğ‘ğ‘–3
	
,
(29)
i.e., the convex hull of the in-neighbor positions.
Definition 5 (Induced target subset). Given the environmental
target set D, the subset of targets covered by ËœTğ‘ğ‘–is defined as
ËœDğ‘ğ‘–â‰œ

ğ‘—âˆˆD : dğ‘—âˆˆËœTğ‘ğ‘–
	
.
(30)
The desired position of every agent ğ‘ğ‘–âˆˆV \V0 is obtained
by
Ëœhğ‘ğ‘–=
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£³
1
 ËœDğ‘ğ‘–

âˆ‘ï¸
ğ‘—âˆˆËœDğ‘ğ‘–
dğ‘—[ğ‘¡],
ËœDğ‘ğ‘–â‰ âˆ…,
1
3
âˆ‘ï¸
ğ‘ğ‘—âˆˆN(ğ‘ğ‘–)
pğ‘ğ‘—,
ËœDğ‘ğ‘–= âˆ….
(31)
Algorithm 2 provides an abstract representation of the
environmental target set D by assigning ğ‘desired positions
pğ‘1,...,pğ‘ğ‘to the agent set.
Definition 6. Let V = {ğ‘1,...,ğ‘ğ‘}. Define
z = vec
pğ‘1
Â·Â·Â·
pğ‘ğ‘
ğ‘‡
âˆˆR2ğ‘.
(32)
Definition 7. Let Vğ‘™be defined as in (4). Define
zğ‘™= vec
pğ‘ğ‘ƒğ‘™âˆ’1+1
Â·Â·Â·
pğ‘ğ‘ƒğ‘™
ğ‘‡
âˆˆR2ğ‘ğ‘™.
(33)
Definition 8. For each agent ğ‘–âˆˆV \ V0, let N (ğ‘–) denote
its set of communication in-neighbors, and let ËœTğ‘–denote the
associated desired communication triangle. The center of the
goal state Ëœğ‘”ğ‘–, corresponding to a cell enclosing Ëœhğ‘–, defines pğ‘–
and is expressed as the convex combination
pğ‘–=
âˆ‘ï¸
ğ‘—âˆˆN(ğ‘–)
Ëœğ‘¤ğ‘–, ğ‘—pğ‘—,
âˆ€ğ‘–âˆˆV \V0,
(34)
where Ëœğ‘¤ğ‘–, ğ‘—âˆˆWğ‘–.
Definition 9. For each agent ğ‘ğ‘–âˆˆV, let N (ğ‘ğ‘–) = {ğ‘ğ‘–1,ğ‘ğ‘–2,ğ‘ğ‘–3}
denote its set of communication in-neighbors, and let
Tğ‘ğ‘–[ğ‘¡] = conv

rğ‘ğ‘–1 [ğ‘¡], rğ‘ğ‘–2 [ğ‘¡], rğ‘ğ‘–3 [ğ‘¡]
	
denote the communication triangle. The center of the goal state
ğ‘”ğ‘ğ‘–âˆˆSğ‘ğ‘–, corresponding to a cell enclosing hğ‘ğ‘–, is denoted by
Ë†cğ‘–and expressed as the convex combination
rğ‘ğ‘–=
âˆ‘ï¸
ğ‘—âˆˆN(ğ‘ğ‘–)
Ë†ğ‘¤ğ‘ğ‘–, ğ‘—rğ‘—
(35)
where Ë†ğ‘¤ğ‘ğ‘–, ğ‘—âˆˆWğ‘–


--- Page 8 ---
Fig. 5: The DNN structure consistent with the agentsâ€™ initial formation in Fig. 4.
Fig. 6: The initial formation of the agent team and the
communication links.
Definition 10. We define Ëœğšª=
 ËœÎ“ğ‘–, ğ‘—

with the (ğ‘–, ğ‘—) entry
ËœÎ“ğ‘–, ğ‘—[ğ‘¡] =
ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³
1,
ğ‘ğ‘–âˆˆV0, ğ‘–= ğ‘—,
Ëœğ‘¤ğ‘ğ‘–,ğ‘ğ‘—[ğ‘¡],
ğ‘ğ‘–âˆˆV \V0, ğ‘ğ‘—âˆˆN (ğ‘ğ‘–),
0,
otherwise.
(36)
Definition 11. We define Ë†ğšª=
 Ë†Î“ğ‘–, ğ‘—

with the (ğ‘–, ğ‘—) entry
Ë†Î“ğ‘–, ğ‘—[ğ‘¡] =
ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³
1,
ğ‘ğ‘–âˆˆV0, ğ‘–= ğ‘—,
Ë†ğ‘¤ğ‘ğ‘–,ğ‘ğ‘—[ğ‘¡],
ğ‘ğ‘–âˆˆV \V0, ğ‘ğ‘—âˆˆN (ğ‘ğ‘–),
0,
otherwise.
(37)
Matrices ğšª,
Ë†ğšª, and
Ëœğšªshare the same strictly lower
blockâ€“triangular structure with an identity block in the (0,0)
position and a zero last block column. Specifically, for ğ‘™, â„âˆˆ
M,
Î“ğ‘™,â„=
ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³
I,
ğ‘™= â„= 0,
Î“ğ‘™,â„,
0 â‰¤â„< ğ‘™â‰¤ğ‘€,
0,
otherwise,
(38)
where Î“ âˆˆ{ğšª, Ë†ğšª, Ëœğšª}.
Algorithm 2 Environmental Target Representation by ğ‘points
1: Get: Target set D and reference position of V0â€™s agents,
denoted by ağ‘1 through ağ‘0, the DNN structure.
2: Obtain: Agentsâ€™ desired positions pğ‘1 through pğ‘ğ‘.
3: for <ğ‘™= 0,Â·Â·Â· , ğ‘€> do
4:
if ğ‘™= 0 then
5:
for <ğ‘–= 1,Â·Â·Â· , ğ‘0> do
6:
pğ‘ğ‘–= ağ‘ğ‘–.
7:
end for
8:
else
9:
for <ğ‘–= ğ‘ƒğ‘™âˆ’1,Â·Â·Â· , ğ‘ƒğ‘™> do
10:
Assign ËœTğ‘ğ‘–, by (29), and ËœDğ‘ğ‘–, by (30).
11:
Assign pğ‘ğ‘–by (31).
12:
end for
13:
end if
14: end for
Proposition 1. Given z0, the desired configuration of the agent
team satisfies
zğ‘™=
ğ‘™âˆ’1
âˆ‘ï¸
â„=0
Ëœğšªğ‘™,â„zâ„,
âˆ€ğ‘™âˆˆM \ {0}.
(39)
Proof. Following Algorithm 2, each agent position satisfies
pğ‘ğ‘–=
âˆ‘ï¸
ğ‘—âˆˆN(ğ‘ğ‘–)
Ëœğ‘¤ğ‘ğ‘–, ğ‘—pğ‘—.
(40)
Stacking the agent positions yields
z = Ëœğšªz,
(41)
from which the recursive relation (39) follows directly.
Theorem 3. Let each AOC agent ğ‘ğ‘–âˆˆV \V0 be able to move
from any triangle associated with a state ğ‘ âˆˆSğ‘ğ‘–to the centroid
of the triangle associated with its optimal next state ğœ‹âˆ—
ğ‘ğ‘–(ğ‘ ) âˆˆ
Sğ‘ğ‘–. Then, for every ğ‘ğ‘–âˆˆV \ V0, the desired actual position
rğ‘ğ‘–[ğ‘¡] converges asymptotically to pğ‘ğ‘–.
Proof. Under the assumptions of the theorem, the subgroup
dynamics satisfy
yğ‘™[ğ‘¡+1] =
ğ‘™âˆ’1
âˆ‘ï¸
â„=0
ğšªğ‘™,â„yâ„[ğ‘¡],
âˆ€ğ‘™âˆˆM \ {0}.
(42)


--- Page 9 ---
Fig. 7: Agent paths under the single-step reachability assump-
tion. All agents ğ‘–âˆˆV asymptotically converge to their desired
positions pğ‘–.
For ğ‘™= 1, y0 = z0 is constant. Hence,
ËœDğ‘ğ‘–= Dğ‘ğ‘–and
Ëœğ‘”ğ‘ğ‘–= ğ‘”ğ‘ğ‘–âˆˆS(ğ‘ğ‘–) define fixed goal states for all ğ‘ğ‘–âˆˆV1. By the
MDP framework in Section IV, each agent ğ‘ğ‘–âˆˆV1 converges
to pğ‘ğ‘–, the center of ğ‘”ğ‘ğ‘–enclosing Ëœhğ‘ğ‘–= hğ‘ğ‘–, implying y1 â†’z1.
Assume for some ğ‘™â‰¥2 that yğ‘™âˆ’1 = zğ‘™âˆ’1. Then, for all ğ‘ğ‘–âˆˆVğ‘™,
the data sets Dğ‘ğ‘–[ğ‘¡] â†’ËœDğ‘ğ‘–and hğ‘ğ‘–[ğ‘¡] â†’Ëœhğ‘ğ‘–, which implies
convergence of the associated goal states ğ‘”ğ‘ğ‘–â†’Ëœğ‘”ğ‘ğ‘–. Conse-
quently, ğšªğ‘™,â„[ğ‘¡] â†’Ëœğšªğ‘™,â„for â„= 0,...,ğ‘™âˆ’1, and (42) yields
yğ‘™[ğ‘¡] â†’zğ‘™. By induction, yğ‘™[ğ‘¡] â†’zğ‘™for all ğ‘™âˆˆM \ {0}, and
therefore rğ‘–[ğ‘¡] â†’pğ‘–for all ğ‘–âˆˆV \V0.
VI. SIMULATION RESULTS
We consider an 57-agent system with the initial configu-
ration shown in Fig. 4. Based on the reference formation,
Vğµ= {1,2,3,4} and ğ‘5 = 5 denote the boundary and core
agents, respectively, while all remaining agents are classified
as interior. The resulting inter-agent communication structure
is encoded by the DNN shown in Fig. 5 (arrows in Fig. 4),
constructed using the framework of Section III. It is desired
that the multi-agent system cover the triangular domain shown
in Fig. 6, where the environmental target data defined by D
are shown in green. The desired positions of the agent team,
denoted p1 through p57, are shown by black. To define the
state space, we choose ğ‘€ğ‘–= 35, for every ğ‘–âˆˆV \ V0, which
in turn implies that
Sğ¶
ğ‘–
 = 352 = 1225.
A. Evolution under Finite-Time Reachability of hğ‘–
In this section, we assume that each agent ğ‘–âˆˆV \ V0 can
reach hğ‘–[ğ‘¡] in a single time step, which implies Pğ‘–= hğ‘–[ğ‘¡].
Under this assumption, the resulting agent paths are shown in
Fig. 7, where all agents ğ‘–âˆˆV asymptotically converge to their
desired positions pğ‘–.
To further illustrate convergence, Figs. 8(a)â€“(b) show the
ğ‘¥- and ğ‘¦-components of the actual and desired positions of
agent 29, r29[ğ‘¡] and p29, respectively, as functions of discrete
time ğ‘¡. The trajectories demonstrate rapid convergence, with
r29[ğ‘¡] reaching p29 in fewer than 40 time steps.
(a)
(b)
Fig. 8: Time evolution of the ğ‘¥- and ğ‘¦-components of the actual
and desired positions of agent 29, illustrating convergence to
p29.
Fig. 9: Agent trajectories under the single-step reachability
assumption and Assumption 3. All agents ğ‘–âˆˆV asymptotically
converge to their desired positions pğ‘–.
B. Evolution under AOC Assumption
In this section, we analyze the agentsâ€™ evolution under
Assumption 3, wherein the desired position pğ‘–is constrained to
lie within an edge-aligned triangular region strictly contained
in Tğ‘–(ğ‘¡), guaranteeing ğœ‚= 0.05 for all ğ‘–âˆˆV \V0. Under this
condition, the resulting closed-loop trajectories of all agents in
the ğ‘¥â€“ğ‘¦plane are shown in Fig. 9, demonstrating coordinated
motion and spatial containment. Moreover, Fig. 10 depicts the
temporal evolution of the ğ‘¥- and ğ‘¦-components of the desired
trajectory for agent 43, illustrating precise tracking behavior
over discrete time ğ‘¡.
VII. CONCLUSION
This paper presented a structured learningâ€“based framework
for decentralized coordination and ground coverage in multi-
agent systems, in which inter-agent communication is encoded
through a geometrically induced deep neural network. By
exploiting the reference formation, agents are systematically
classified into boundary and interior groups, yielding a hierar-
chical communication architecture with explicitly constrained
and interpretable communication weights. These weights are
selected from finite sets and governed by a decentralized
Markov decision process, ensuring normalized interactions


--- Page 10 ---
(a)
(b)
Fig. 10: Time evolution of the ğ‘¥- and ğ‘¦-components of the
actual and desired positions of agent 43, illustrating conver-
gence to p43.
and well-posed local decision making. Within this framework,
convergence of agent trajectories to desired goal configurations
associated with environmental target data was established
under AOC assumptions. Numerical simulations validate the
proposed policy-based decentralized coverage strategy and
demonstrate its ability to capture geometric structure and
achieve effective coverage of complex domains.
REFERENCES
[1] Farshid Abbasi, Afshin Mesbahi, and Javad Mohammadpour Velni.
A new voronoi-based blanket coverage control method for moving
sensor networks. IEEE Transactions on Control Systems Technology,
27(1):409â€“417, 2017.
[2] Onur Arslan and Daniel E. Koditschek. Voronoi-based coverage control
of heterogeneous disk-shaped robots. IEEE Transactions on Robotics,
29(2):395â€“411, 2013.
[3] GÂ¨okhan M AtÄ±ncÂ¸, DuË‡san M StipanoviÂ´c, and Petros G Voulgaris.
A
swarm-based approach to dynamic coverage control of multi-agent
systems. Automatica, 112:108637, 2020.
[4] Yang Bai, Yujie Wang, Mikhail Svinin, Evgeni Magid, and Ruisheng
Sun. Adaptive multi-agent coverage control with obstacle avoidance.
IEEE Control Systems Letters, 6:944â€“949, 2021.
[5] Shiba Biswal, Karthik Elamvazhuthi, and Spring Berman. Decentralized
control of multiagent systems using local density feedback.
IEEE
Transactions on Automatic Control, 67(8):3920â€“3932, 2021.
[6] William Coffey, Matthew A. McCourt, and Magnus Egerstedt. Heteroge-
neous coverage and multi-resource allocation using voronoi tessellations.
In IEEE International Conference on Robotics and Automation (ICRA),
pages 11342â€“11348, 2023.
[7] Jorge CortÂ´es.
Coverage optimization and spatial load balancing by
robotic sensor networks.
IEEE Transactions on Automatic Control,
55(3):749â€“754, 2010.
[8] Anna Dai, Rongpeng Li, Zhifeng Zhao, and Honggang Zhang. Graph
convolutional multi-agent reinforcement learning for uav coverage con-
trol. In 2020 International Conference on Wireless Communications and
Signal Processing (WCSP), pages 1106â€“1111. IEEE, 2020.
[9] Ahmad Din, Muhammed Yousoof Ismail, Babar Shah, Mohammad
Babar, Farman Ali, and Siddique Ullah Baig.
A deep reinforcement
learning-based multi-agent area coverage control for smart agriculture.
Computers and Electrical Engineering, 101:108089, 2022.
[10] Alireza Dirafzoon, Mohammad Bagher Menhaj, and Ahmad Afshar.
Decentralized coverage control for multi-agent systems with nonlin-
ear dynamics.
IEICE TRANSACTIONS on Information and Systems,
94(1):3â€“10, 2011.
[11] Karthik Elamvazhuthi and Spring Berman. Nonlinear generalizations of
diffusion-based coverage by robotic swarms. In 2018 IEEE Conference
on Decision and Control (CDC), pages 1341â€“1346. IEEE, 2018.
[12] YuZe Feng, Gang Lu, WenJie Bai, JianHang Zhao, YuMo Bai, and
Tao Xu. Rapid coverage control with multi-agent systems based on k-
means algorithm. In 2020 7th International Conference on Information,
Cybernetics, and Computational Social Systems (ICCSS), pages 870â€“
874, 2020.
[13] Jia Hua, Yifan Xu, and Xiaoming Hu.
Persistent surveillance and
coverage control for heterogeneous robot teams.
IEEE Transactions
on Robotics, 2025. Early Access.
[14] Solmaz S. Kia and Jorge CortÂ´es. Distributed algorithms for coverage
control with time-varying density functions. Automatica, 89:210â€“220,
2018.
[15] Hyeongseok Kim, Marco Santos, Jose Guerrero-Bonilla, Anthony Yezzi,
and Magnus Egerstedt. Coverage control of mobile robots with different
maximum speeds for time-sensitive applications. IEEE Robotics and
Automation Letters, 7(2):3136â€“3143, 2022.
[16] Vishaal Krishnan and Sonia MartÂ´Ä±nez. A multiscale analysis of multi-
agent coverage control algorithms. Automatica, 145:110516, 2022.
[17] Martin Lauer and Martin Riedmiller. An algorithm for distributed rein-
forcement learning in cooperative multi-agent systems. In International
Conference on Machine Learning (ICML), pages 535â€“542, 2000.
[18] Wenhao Luo and Katia Sycara. Voronoi-based coverage control with
connectivity maintenance for robotic sensor networks. In 2019 Inter-
national Symposium on Multi-Robot and Multi-Agent Systems (MRS),
pages 148â€“154. IEEE, 2019.
[19] Minh Tri Nguyen, Luis Rodrigues, Cristina Stoica Maniu, and Sorin
Olaru. Discretized optimal control approach for dynamic multi-agent
decentralized coverage.
In 2016 IEEE International Symposium on
Intelligent Control (ISIC), pages 1â€“6. IEEE, 2016.
[20] Reza Olfati-Saber. Distributed kalman filter with embedded consensus
filters. Proceedings of the IEEE Conference on Decision and Control,
2002. Early MDP-style decision-theoretic formulations for distributed
sensing and coverage.
[21] Shivang Patel, Senthil Hariharan, Pranav Dhulipala, Ming C Lin, Dinesh
Manocha, Huan Xu, and Michael Otte. Multi-agent coverage in urban
environments. arXiv preprint arXiv:2008.07436, 2020.
[22] Arash Rahmani and Mehran Mesbahi.
Learning-based decentralized
control of multi-agent systems.
IEEE Transactions on Control of
Network Systems, 6(3):1027â€“1038, 2019.
[23] Hamed Sadeghi, Mahdi Sadraddini, and Calin Belta. Coverage control
for multiple event types with heterogeneous robots. In IEEE Interna-
tional Conference on Robotics and Automation (ICRA), pages 7992â€“
7998, 2019.
[24] Marco Santos and Magnus Egerstedt. Coverage control for multirobot
teams with heterogeneous sensing capabilities.
In IEEE/RSJ Inter-
national Conference on Intelligent Robots and Systems (IROS), pages
5856â€“5861, 2018.
[25] Esmaeil Seraj and Matthew Gombolay. Coordinated control of uavs for
human-centered active sensing of wildfires. In 2020 American Control
Conference (ACC), pages 1645â€“1652, 2020.
[26] Cheng Song, Gang Feng, Yuan Fan, and Yong Wang. Decentralized
adaptive awareness coverage control for multi-agent networks. Auto-
matica, 47(12):2749â€“2756, 2011.
[27] Jian Xiao, Gang Wang, Ying Zhang, and Lei Cheng. A distributed multi-
agent dynamic area coverage algorithm based on reinforcement learning.
IEEE Access, 8:33511â€“33521, 2020.
[28] Ying Zhang and Sonia MartÂ´Ä±nez.
Distributed learning and decision
making in networked systems. Automatica, 125:109417, 2021.
[29] Meng Zhong and Christos G. Cassandras. Distributed coverage control
and data collection with mobile agents. IEEE Transactions on Automatic
Control, 56(10):2449â€“2464, 2011.
Hossein Rastgoftar an Assistant Professor at the
University of Arizona. Prior to this, he was an
adjunct Assistant Professor at the University of
Michigan from 2020 to 2021. He was also an
Assistant Research Scientist (2017 to 2020) and
a Postdoctoral Researcher (2015 to 2017) in the
Aerospace Engineering Department at the University
of Michigan Ann Arbor. He received the B.Sc. de-
gree in mechanical engineering-thermo-fluids from
Shiraz University, Shiraz, Iran, the M.S. degrees
in mechanical systems and solid mechanics from
Shiraz University and the University of Central Florida, Orlando, FL, USA,
and the Ph.D. degree in mechanical engineering from Drexel University,
Philadelphia, in 2015.
