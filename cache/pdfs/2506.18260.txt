--- Page 1 ---
arXiv:2506.18260v1  [cs.AI]  23 Jun 2025
ADVANCED FOR-LOOP FOR QML ALGORITHM SEARCH
A PRIMER
FuTe Wong*
Institute of Medical Science, University of Toronto
Vector Institute
Department of Computer Science, University of Toronto
fute@cs.toronto.edu
ABSTRACT
This paper introduces an advanced framework leveraging Large Language Model-based Multi-Agent
Systems (LLMMA) for the automated search and optimization of Quantum Machine Learning (QML)
algorithms. Inspired by Google DeepMind’s FunSearch, the proposed system works on abstract
level to iteratively generates and refines quantum transformations of classical machine learning
algorithms (concepts), such as the Multi-Layer Perceptron, forward-forward and backpropagation
algorithms. As a proof of concept, this work highlights the potential of agentic frameworks to
systematically explore classical machine learning concepts and adapt them for quantum computing,
paving the way for efficient and automated development of QML algorithms. Future directions
include incorporating planning mechanisms and optimizing strategy in the search space for broader
applications in quantum-enhanced machine learning.
Keywords =
Agentic framework, Quantum Machine Learning, Machine Learning, LLM, Reasoning
1
Introduction
Large Language Models (LLMs) have recently shown remarkable potential in reasoning and planning capabilities
across a wide array of tasks (Ahn et al., 2024; Yao et al., 2023). Leveraging on a single LLM-based agent, LLM-
based Multi-Agents (LLMMA) utilizing collective intelligence and specialized persona with differentiated skills offer
advanced capabilities (Hong et al., 2023; Mandi et al., 2023; Qian et al., 2024; Wang et al., 2024; H. Zhang et al., 2024).
Recently, LLMs have also risen to prominence in scientific discovery for their expansive knowledge bases, advanced
reasoning capabilities, and human-friendly natural language interface (AI4Science & Quantum, 2023). Inspired by
Google DeepMind’s FunSearch (Romera-Paredes et al., 2024), which looking for solutions for algorithmic problems via
iteratively program code generation from LLM, we adapted a LLM-based Multi-Agents system for Quantum Machine
Learning algorithms search (Figure 1). Rather than studies like Quantum Architecture Search (QAS) in the gate sets of
Parametrized Quantum Circuits (PQC; Du et al., 2022; Nakaji et al., 2024; Wu et al., 2023), search in program coding
space allows agents to build more expressive resultant model architecture. Compared with a single LLM optimizer
for neural architecture search (NAS; Yang et al., 2024; M. R. Zhang et al., 2023), our approach extends agents with
the capacities of tool use, memory via retrieval-augmented generation (RAG; Yan et al., 2024), and optimization
techniques like multi-agent in-context learning (Dong et al., 2024), direct preference optimization (DPO; Rafailov et al.,
2023), evolutionary algorithm (van Stein & Bäck, 2024), or feature steering (Templeton et al., 2024). We construct
the LLMMA QML algorithm search in a way that given a classical deep learning algorithm, such as forward-forward
algorithm or backpropagation algorithm as described in the following parts, the system can facilitate the development
workflow to find its optimized quantum counterpart.
∗


--- Page 2 ---
Advanced For-Loop for QML algorithm search
LLM-based Multi-Agent System
for QML Algorithm Search
Manage
Agents
Capabilities
Acquisition
Feedback
Adjustment
Agents
Communication
Paradigms
Strucure
Content
Agents Orchestration
Agents
Capabilities
Acquisition
Feedback
Adjustment
Agents
Communication
Paradigms
Strucure
Content
Agents Profiling
Abstraction Schema
Theorist
- algorithm generator
Coder
Validator
Investor
- reward function
generator
Belief
Modeling other
agents, environment,
etc
Retrieved
Information, etc
Communication
Goal
Profile
Memory
Documents
Retrieval
Learning
LLM
Prompt
Retrieval
Decisions
Human
Environment
Actions
Observations/Feedback
Physical
Sandbox
(MineDojo, Minecraft,
Code environment, quantum runtime, etc)
Figure 1: The architecture of the LLM-based Multi-Agent systems for QML search with dynamic role updating. (Figure
adapted form Guo et al., 2024)
2
Can LLM-based Agent find quantum machine learning algorithm from its classical
counterpart?
Given the name of the classical algorithm, we have the agentic system is to generate the program code of the algorithm
as initial condition, then conducting the evolution loop of searching and optimization to find its quantum counterpart
(Figure 2)
Figure 2: Flow chart of QML searching process.
2.1
Multi-Layer Perceptron
Firstly, we did an experiment on classical multilayer perceptron as given input to the agentic system. For example,
given specification of classical machine learning algorithm, which is Multi-Layer Perceptron (MLP) with the following
sample snippet:
2


--- Page 3 ---
Advanced For-Loop for QML algorithm search
Figure 3: A snippet of Python implementation of MLP
The system would generate an implementation of quantum-enhanced version of the multi-layer perceptron and training
the model. The following figure shows the quantum counter part and a piece of training dynamic that demonstrated that
the model is learning during the training steps.
(a) A snippet of Python implementation of Quantum MLP
(b) Learning dynamics of the Quantum MLP
Figure 4
2.2
Forward-Forward Algorithm
The initial motivation for the development of the forward-forward algorithm (Hinton, 2022) is the reflection of the
biological implausibility of backpropagation. Although the backpropagation algorithm is a fundamental and powerful
learning procedure in the machine learning field, there is no convincing evidence that the cortex of the human brain
explicitly propagates error derivatives or stores neural activities for use in a backward propagation of error information.
To approximate how the human cortex learns, the forward-forward algorithm replaces the forward and backward passes
3


--- Page 4 ---
Advanced For-Loop for QML algorithm search
of backpropagation with two forward passes that operate in the same way as each other, and applies a contrast learning
scheme with different data with opposite objectives. For example, the positive pass operates on real data and adjusts the
weights to increase the goodness in every hidden layer. The negative pass operates on “negative data” and adjusts the
weights to decrease the goodness in every hidden layer.
Following is the simplified version of Python implementation of the forward-forward algorithm and a schematic
illustration.
(a) A snippet of Python implementation of Forward-Forward layer
x1
x2
Positive Forward Pass
Negative Forward Pass
Forward-Forwad
(b) Illustration
Figure 5: Forward-Forward Algorithm
The transformed quantum version of the forward-forward algorithm is shown in the following figure. Noticed the
implementation of the positive and negative pass is quantized with quantum circuits.
Figure 6: A snippet of simplified Quantum Forward-Forward layer
2.3
Backpropagation Algorithm
The backpropagation algorithm (Rumelhart et al., 1986) is probably the most fundamental optimization algorithm for
deep neural networks. It is also more generalizable than the forward-forward algorithm, which is intended for networks
where weight-sharing is not feasible. The backpropagation algorithm is a special case of the chain rule applied to
neural networks. Through the efficient reuse of intermediate information, it facilitates network weight updating through
4


--- Page 5 ---
Advanced For-Loop for QML algorithm search
gradient computation at a total cost approximately proportional to twice the runtime of the feedforward model (Figure
7). Without incurring cost to an additional factor proportional to the number of parameters, it is effective in performing
stochastic gradient descent with a large number of parameters and a lot of data, which brings the astonishing success of
deep learning over the last decade.
(a) A snippet of Python implementation of neural network
x1
x2
Forward Pass
Backward Pass
Backpropagation
(b) Illustration
Figure 7: Backpropagation Algorithm
When specified with backpropagation algorithm, the agentic system would generate a quantum version of the back-
propagation algorithm, which is shown in Figure 8. The quantum version of the backpropagation algorithm is based
on the quantum circuit model and uses quantum gates to perform the computation. And noticed that the gradient
backpropagation was replaced with parameter-shift rule, which is a method for computing the gradient of a quantum
circuit with respect to its parameters, and it is used to optimize the parameters of the quantum circuit.
Figure 8: A snippet of the Quantum backpropagation with gradient
5


--- Page 6 ---
Advanced For-Loop for QML algorithm search
2.4
Performance Evaluation
We compare the algorithms generated by the agentic system with the baseline human-crafted post-variational quantum
neural network (from Pennylane Demos). The original demonstration employs a dataset from UCI ML hand-written
digits datasets and pre-processes the dataset to make it a binary task, instead of original 10 classes. Here, the task was
revert back to classification of the original 10 classes.
We can spot that the quantum forward-forward algorithm got performance (average accuracy on the test set) similar to
the baseline one (Table 1). Others are inferior to the baseline algorithm. Noted that we didn’t training these model with
massive training steps as the scope of this work is to demonstrate a proof of concept that our agentic framework can
reach the ability to translate classical implementation of machine learn algorithms to its quantum counterpart. At the
time when we run the agentic experiment, the base model was claude-3-5-sonnet-20240620.
Table 1: Comparison of Model Performance
Model
Average Accuracy (%)
Baseline (QNN)
15.55
QMLP
9.40
QFF
15.17
QBP
12.37
3
Discussion
Quantum-enhanced machine learning is an emerging field that explores the potential of quantum computing to improve
machine learning algorithms. Quantum machine learning algorithms leverage the principles of quantum mechanics,
such as superposition and entanglement, to perform computations that are infeasible for classical computers. The
agentic framework we proposed can be used to search for quantum machine learning algorithms with aims to be more
efficient and effective than classical counterparts. Since there are rich abundance of legacy classical machine learning
concepts to explore, this primer work, as a proof of concept, focuses on the initial translation of the classical machine
learning algorithms to its quantum counterpart. This operation shows the potential that allows us to adapt a plug
and play searching strategy to automatically screen through classical machine learning concepts when developing
quantum-enhanced machine learning algorithms. Thus, we consider the current agentic operation as a for-loop to
iteratively search through machine learning concepts for the quantum building.
In abstraction, the operation can be abstracted as the following formulation.
A =
X
ij
αijLj
A (CML) = argmin
π
Qπ
ML
The LLMMA (A) is composed by several large language models (Lj; distinct agents, or lobes in the context of neural
architecture) where αij specifies the interaction among the lobes. Given a classical machine learning algorithm (CML),
A can facilitate to find the corresponding quantum machine learning algorithm (QML) with optimized policy (π).
The work of AI Scientist (Lu et al., 2024) has demonstrated an agentic operation to continually optimize on a template
of code and ideas to find good performance machine learning model. Our work can be viewed as an extension that
starts from a more primitive state of the initial condition, and perform transformation across knowledge domain. Then
consider the transformative results as another session from which the agentic system operate on tasks of optimization
and evolution.
Currently, we are working on a variation that incorporates planning mechanism, either automatically generated serious
of component to test through or synthesized with human guided, to run monte-carlo tree searching. The future direction
of this work will be research on how to efficiently search through the space of classical machine learning concepts and
translate them to quantum machine learning algorithms.
6


--- Page 7 ---
Advanced For-Loop for QML algorithm search
References
Ahn, J., Verma, R., Lou, R., Liu, D., Zhang, R., & Yin, W. (2024, April 5). Large Language Models for Mathematical
Reasoning: Progresses and Challenges. arXiv: 2402.00157 [cs]. https://doi.org/10.48550/arXiv.2402.00157
AI4Science, M. R., & Quantum, M. A. (2023, December 8). The Impact of Large Language Models on Scientific
Discovery: A Preliminary Study using GPT-4. arXiv: 2311.07361 [cs]. https://doi.org/10.48550/arXiv.2311.
07361
Dong, Q., Li, L., Dai, D., Zheng, C., Ma, J., Li, R., Xia, H., Xu, J., Wu, Z., Chang, B., Sun, X., Li, L., & Sui, Z. (2024,
June 18). A Survey on In-context Learning. arXiv: 2301.00234 [cs]. https://doi.org/10.48550/arXiv.2301.
00234
Du, Y., Huang, T., You, S., Hsieh, M.-H., & Tao, D. (2022). Quantum circuit architecture search for variational quantum
algorithms. npj Quantum Information, 8(1), 1–8. https://doi.org/10.1038/s41534-022-00570-y
Guo, T., Chen, X., Wang, Y., Chang, R., Pei, S., Chawla, N. V., Wiest, O., & Zhang, X. (2024, April 19). Large
Language Model based Multi-Agents: A Survey of Progress and Challenges. arXiv: 2402 . 01680 [cs].
https://doi.org/10.48550/arXiv.2402.01680
Hinton, G. (2022, December 26). The Forward-Forward Algorithm: Some Preliminary Investigations. arXiv: 2212.13345
[cs]. https://doi.org/10.48550/arXiv.2212.13345
Hong, S., Zhuge, M., Chen, J., Zheng, X., Cheng, Y., Zhang, C., Wang, J., Wang, Z., Yau, S. K. S., Lin, Z., Zhou, L.,
Ran, C., Xiao, L., Wu, C., & Schmidhuber, J. (2023, November 6). MetaGPT: Meta Programming for A
Multi-Agent Collaborative Framework. arXiv: 2308.00352 [cs]. https://doi.org/10.48550/arXiv.2308.00352
Lu, C., Lu, C., Lange, R. T., Foerster, J., Clune, J., & Ha, D. (2024, August 12). The AI Scientist: Towards Fully
Automated Open-Ended Scientific Discovery. arXiv: 2408.06292 [cs]. https://doi.org/10.48550/arXiv.2408.
06292
Mandi, Z., Jain, S., & Song, S. (2023, July 10). RoCo: Dialectic Multi-Robot Collaboration with Large Language
Models. arXiv: 2307.04738 [cs]. https://doi.org/10.48550/arXiv.2307.04738
Nakaji, K., Kristensen, L. B., Campos-Gonzalez-Angulo, J. A., Vakili, M. G., Huang, H., Bagherimehrab, M., Gorgulla,
C., Wong, F., McCaskey, A., Kim, J.-S., Nguyen, T., Rao, P., & Aspuru-Guzik, A. (2024, January 17). The
generative quantum eigensolver (GQE) and its application for ground state search. arXiv: 2401.09253
[quant-ph]. https://doi.org/10.48550/arXiv.2401.09253
Qian, C., Liu, W., Liu, H., Chen, N., Dang, Y., Li, J., Yang, C., Chen, W., Su, Y., Cong, X., Xu, J., Li, D., Liu, Z., &
Sun, M. (2024, June 5). ChatDev: Communicative Agents for Software Development. arXiv: 2307.07924 [cs].
https://doi.org/10.48550/arXiv.2307.07924
Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., & Finn, C. (2023, December 13). Direct Preference
Optimization: Your Language Model is Secretly a Reward Model. arXiv: 2305.18290 [cs]. https://doi.org/10.
48550/arXiv.2305.18290
Romera-Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar, M. P., Dupont, E., Ruiz, F. J. R., Ellenberg, J. S.,
Wang, P., Fawzi, O., Kohli, P., & Fawzi, A. (2024). Mathematical discoveries from program search with large
language models. Nature, 625(7995), 468–475. https://doi.org/10.1038/s41586-023-06924-6
Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. Nature,
323(6088), 533–536. https://doi.org/10.1038/323533a0
Templeton, A., Conerly, T., Marcus, J., Lindsey, J., Bricken, T., Chen, B., Pearce, A., Citro, C., Ameisen, E., Jones,
A., Cunningham, H., Turner, N. L., McDougall, C., MacDiarmid, M., Freeman, C. D., Sumers, T. R., Rees,
E., Batson, J., Jermyn, A., ... Henighan, T. (2024). Scaling monosemanticity: Extracting interpretable
features from claude 3 sonnet. Transformer Circuits Thread. https://transformer-circuits.pub/2024/scaling-
monosemanticity/index.html
van Stein, N., & Bäck, T. (2024, June 1). LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically
Generating Metaheuristics. arXiv: 2405.20132 [cs]. https://doi.org/10.48550/arXiv.2405.20132
Wang, Q., Wang, Z., Su, Y., Tong, H., & Song, Y. (2024, February 28). Rethinking the Bounds of LLM Reasoning: Are
Multi-Agent Discussions the Key? (1). arXiv: 2402.18272 [cs]. https://doi.org/10.48550/arXiv.2402.18272
Wu, W., Yan, G., Lu, X., Pan, K., & Yan, J. (2023). QuantumDARTS: Differentiable Quantum Architecture Search for
Variational Quantum Algorithms. Proceedings of the 40th International Conference on Machine Learning,
37745–37764. Retrieved July 3, 2024, from https://proceedings.mlr.press/v202/wu23v.html
Yan, S.-Q., Gu, J.-C., Zhu, Y., & Ling, Z.-H. (2024, February 16). Corrective Retrieval Augmented Generation. arXiv:
2401.15884 [cs]. https://doi.org/10.48550/arXiv.2401.15884
Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D., & Chen, X. (2024, April 15). Large Language Models as
Optimizers. arXiv: 2309.03409 [cs]. https://doi.org/10.48550/arXiv.2309.03409
Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., & Narasimhan, K. (2023, December 3). Tree of Thoughts:
Deliberate Problem Solving with Large Language Models. arXiv: 2305.10601 [cs]. https://doi.org/10.48550/
arXiv.2305.10601
7


--- Page 8 ---
Advanced For-Loop for QML algorithm search
Zhang, H., Du, W., Shan, J., Zhou, Q., Du, Y., Tenenbaum, J. B., Shu, T., & Gan, C. (2024, February 17). Building
Cooperative Embodied Agents Modularly with Large Language Models. arXiv: 2307.02485 [cs]. https:
//doi.org/10.48550/arXiv.2307.02485
Zhang, M. R., Desai, N., Bae, J., Lorraine, J., & Ba, J. (2023, December 7). Using Large Language Models for
Hyperparameter Optimization. arXiv: 2312.04528 [cs]. https://doi.org/10.48550/arXiv.2312.04528
8
