--- Page 1 ---
1
Efﬁcient LLM Inference over Heterogeneous Edge
Networks with Speculative Decoding
Bingjie Zhu, Zhixiong Chen, Member, IEEE, Liqiang Zhao, Member, IEEE,
Hyundong Shin, Fellow, IEEE, and Arumugam Nallanathan, Fellow, IEEE
Abstract—Large language model (LLM) inference at the net-
work edge is a promising serving paradigm that leverages dis-
tributed edge resources to run inference near users and enhance
privacy. Existing edge-based LLM inference systems typically
adopt autoregressive decoding (AD), which only generates one
token per forward pass. This iterative process, compounded
by the limited computational resources of edge nodes, results
in high serving latency and constrains the system’s ability to
support multiple users under growing demands. To address these
challenges, we propose a speculative decoding (SD)–based LLM
serving framework that deploys small and large models across
heterogeneous edge nodes to collaboratively deliver inference
services. Speciﬁcally, the small model rapidly generates draft
tokens that the large model veriﬁes in parallel, enabling multi-
token generation per forward pass and thus reducing serving
latency. To improve resource utilization of edge nodes, we incor-
porate pipeline parallelism to overlap drafting and veriﬁcation
across multiple inference tasks. Based on this framework, we
analyze and derive a comprehensive latency model incorporating
both communication and inference latency. Then, we formulate a
joint optimization problem for speculation length, task batching,
and wireless communication resource allocation to minimize
total serving latency. To address this problem, we derive the
closed-form solutions for wireless communication resource allo-
cation, and develop a dynamic programming algorithm for joint
batching and speculation control strategies. Experimental results
demonstrate that the proposed framework achieves lower serving
latency compared to AD-based serving systems. In addition, the
proposed joint optimization method delivers up to 44.9% latency
reduction compared to benchmark schemes.
Index Terms—LLM, speculative decoding, batching, edge net-
works, wireless resource allocation.
I. INTRODUCTION
Recent advancements in large language models (LLMs),
exempliﬁed by ChatGPT, have revolutionized natural language
understanding and generation capabilities [1]. Open-source
LLMs, such as the LLaMA family [2], further extend this
potential by enabling task-speciﬁc ﬁne-tuning and supporting
Bingjie
Zhu
is
with
the
State
Key
Laboratory
of
Integrated
Service
Networks,
Xidian
University,
Xi’an
710071,
China.
(e-mail:
bjzhu1@stu.xidian.edu.cn).
Zhixiong Chen and Arumugam Nallanathan are with the School of
Electronic Engineering and Computer Science, Queen Mary University of
London, London, U.K. Arumugam Nallanathan is also with the Department of
Electronic Engineering, Kyung Hee University, Yongin-si, Gyeonggido 17104,
Korea. (emails: {zhixiong.chen, a.nallanathan}@qmul.ac.uk).
Liqiang Zhao is with the State Key Laboratory of Integrated Service
Networks, Xidian University, Xi’an 710071, China, and also with Guangzhou
Institute of Technology, Xidian University, Guangzhou 510100, China.
(lqzhao@mail.xidian.edu.cn).
Hyundong Shin is with the Department of Electronics and Information
Convergence Engineering, Kyung Hee University, Yongin-si, Gyeonggido
17104, Republic of Korea (e-mail: hshin@khu.ac.kr).
diverse applications across healthcare, ﬁnance, and virtual
assistance domains [3]. However, these advances are ac-
companied by increasingly large model sizes, resulting in
substantial computational and memory requirements for LLM
inference. Consequently, LLMs are currently deployed in cen-
tralized cloud centers with powerful computational capacity
to provide services. Nevertheless, this centralized architecture
necessitates transmitting user data to remote cloud servers for
inference, thereby introducing signiﬁcant transmission latency
and raising critical data privacy concerns. These limitations
may hinder the widespread application of LLMs in latency-
and privacy-sensitive scenarios such as autonomous vehicles,
real-time robotic control, and healthcare systems [4].
Edge inference has emerged as a compelling paradigm that
mitigates these issues by utilizing edge computing resources
to provide inference services closer to users, which reduces
transmission overhead and enhances privacy protection. Gen-
erally, edge inference can be categorized into single-node
inference and multi-node collaborative inference. In single-
node inference, the complete model is deployed on an edge
server to process user requests using received task data and
transmit inference results back to users via wireless links.
Several studies have explored optimization techniques for
single-node inference. In [5], the authors investigated batching
and task scheduling techniques to improve the completion rate
of deep neural network (DNN) inference tasks in single-cell,
multi-user scenarios. To maximize system throughput under
limited resource constraints, Liu et al. [6] combined batching
with early exiting, and jointly optimized communication and
computation resources for multi-user edge inference. Despite
advances such as early exiting and batching, a single resource-
constrained edge server may struggle to support numerous
concurrent inference tasks while maintaining low-latency serv-
ing. To mitigate this issue, multi-node collaborative inference
distributes models among edge nodes to complete inference
tasks cooperatively based on the computing capacity of edge
servers. In [7], the authors investigated the joint optimization
of model caching and request routing to maximize through-
put in multi-edge server scenarios. Li et al. [8] proposed
a device-edge co-inference framework that jointly optimizes
model partitioning and resource allocation to minimize energy
consumption in wireless sensing systems.
Although the aforementioned approaches in [5]–[8] effec-
tively enhance the performance of edge inference systems,
they primarily focus on traditional DNNs. Traditional DNNs,
such as MobileNet and ResNet, typically require only a single
forward pass through the network with ﬁxed computational
arXiv:2510.11331v1  [eess.SY]  13 Oct 2025


--- Page 2 ---
2
requirements to generate inference results. In contrast, LLMs
are characterized by larger model sizes and reliance on au-
toregressive decoding (AD) for inference. This AD process
generates inference results, i.e., output tokens, through iterative
forward passes, with each forward pass producing one token
conditioned on both the initial input prompt and all previ-
ously generated tokens. To accelerate this inherently sequential
process, LLMs utilize key-value (KV) caches that store the
hidden states of previous tokens, enabling only the newest
token to be processed at each step while avoiding redundant
computation. However, the KV cache grows linearly with the
output sequence length, increasing memory consumption. Due
to large model sizes, the AD mechanism, and KV cache, LLM
inference often demands substantially greater computational
and memory resources than traditional DNN inference. For
example, a widely adopted DNN such as MobileNetV2, re-
quires approximately 7 MB of memory and 0.3 GFLOPs for
single-image inference whereas even a compact 3B-parameter
LLM such as Phi-2 demands at least 6.3 GB of memory and 6
TFLOPs to generate an output sequence with 1000 tokens in
16-bit ﬂoating-point (FP16) precision. The signiﬁcant disparity
in resource requirements and inference mechanisms renders
existing DNN-oriented edge inference approaches ineffective
for LLM inference at the resource-limited edge.
Existing works have explored approaches to enhance the
efﬁciency of LLM inference over edge networks [9]–[14],
typically targeting edge-deployable models up to 13B parame-
ters, such as LLaMA-1.1B, LLaMA-7B, and OPT-13B. These
works mainly focus on model quantization [9], [10], model
parallelism [11], [12], and resource management [13], [14].
Speciﬁcally, the authors in [9] developed model quantization
approaches to reduce the computational costs of LLM in-
ference and improve system throughput. In [10], the authors
proposed an LLM scheduling framework that integrates model
quantization with heterogeneous edge resource allocation to
reduce operational costs. Pipeline parallelism was employed in
[11] to reduce inference latency by partitioning model layers
among edge nodes, enabling efﬁcient overlapped execution of
different computational stages. In [12], the authors employed
tensor parallelism to distribute LLM tensors across edge nodes
and used over-the-air computation to reduce communication
overhead incurred by all-reduce operations. The authors in
[13] proposed a mixture-of-experts–enabled LLM inference
framework, incorporating a Stackelberg game–based resource
allocation mechanism to improve inference efﬁciency and
resource utilization. An active inference-based algorithm was
proposed in [14] to optimize task ofﬂoading and resource
allocation policies for LLM inference tasks in cloud-edge
collaborative architectures.
Although the above approaches improve LLM inference
efﬁciency at the edge, model lightweight techniques [9], [10]
may compromise output accuracy while model parallel meth-
ods [11], [12] suffer from frequent synchronization overhead
between edge nodes. Furthermore, all methods in [9]–[14]
rely on AD to complete inference tasks, which is inherently
constrained by sequential token generation. This sequential
pattern limits GPUs to single-token generation per forward
pass of LLMs, underutilizing their parallel processing capabil-
ities designed for simultaneous computation across thousands
of cores. Consequently, this bottleneck may result in high
serving latency, making efﬁcient LLM serving system on
resource-constrained edge devices challenging. To address the
above issues, this work aims to develop an efﬁcient distributed
LLM inference system over edge networks that provide low-
latency services without accuracy degradation. Speciﬁcally,
we adopt speculative decoding (SD) for LLM inference over
edge networks. SD employs a lightweight draft model to
generate multiple speculative tokens, which a larger LLM
(referred to as the target model) then veriﬁes in parallel based
on acceptance criteria [15], [16]. This parallel veriﬁcation
approach effectively utilizing GPU parallel computing capabil-
ities, and enables simultaneous generation of multiple tokens
in a forward pass of the target model while maintaining target
model accuracy [17], [18]. In addition, since the small draft
model incurs low computational costs, SD could efﬁciently
accelerate LLM inference compared to AD. To leverage the
advantages of SD, we deploy draft and target models across
heterogeneous edge nodes. To improve resource utilization of
edge nodes, we incorporate pipeline parallelism in the SD-
based LLM inference. The pipeline parallelism overlaps draft
speculation and target veriﬁcation across different inference
tasks, allowing the system to process multiple inference tasks
concurrently and thus reducing total serving latency. The main
contributions of this paper are summarized as follows:
• We propose a novel SD-based LLM inference framework
at the heterogeneous edge networks, in which draft and
target models are deployed across different edge nodes
to collaboratively provide LLM inference services. This
framework effectively reduces LLM inference latency
while preserving model accuracy.
• We design a pipeline-parallel LLM serving mechanism
with SD, which efﬁciently improves computational re-
source utilization across edge nodes. Based on this serv-
ing mechanism, we analyze and derive the total serving
latency, incorporating both communication and inference
latency. We then formulate a joint optimization problem
for speculation length, batching, and wireless commu-
nication resource allocation to minimize total serving
latency. This optimization problem is challenging to solve
due to the tight coupling among decision variables in the
latency formulation.
• To address this problem, we ﬁrst derive the optimal
wireless communication resource allocation policy for
each inference task. Subsequently, we develop a dynamic
programming-based algorithm to obtain efﬁcient batch-
ing decisions and speculation control policies with low
complexity.
• We conduct extensive simulations to validate the effec-
tiveness of the proposed LLM serving framework. Specif-
ically, compared to the AD-based serving systems, the
proposed LLM serving system consistently delivers lower
serving latency across three LLaMA model combinations
ranging from 68M to 13B parameters. In addition, the
proposed joint optimization policy achieves up to 44.9%
latency reduction compared to benchmark schemes.


--- Page 3 ---
3
The rest of this paper is organized as follows: Section
II introduces the system model, the proposed LLM serving
system, and the problem formulation. Section III illustrates
the proposed speculation length, batch scheduling, and com-
munication allocation algorithm. Section IV evaluates the
effectiveness of the proposed approaches by simulations. The
conclusion is presented in Section V.
II. SYSTEM MODEL AND PROBLEM FORMULATION
This work investigates an LLM serving system in an edge
network, which consists of a macro base station (MBS) and a
small base station (SBS) to provide LLM services for multiple
users, as shown in Fig. 1. The SBS is co-located with an edge
server having certain computing and memory resources, while
the MBS possesses strong computing and memory capabilities
as it is usually ﬁber-optically connected to multiple edge
servers [19]. The MBS and the SBS are interconnected via
a wired fronthaul link to collaboratively serve users. The
set of LLM inference tasks from users is denoted by K =
{1, 2, ..., K}, where each inference task k is characterized
by a 2-item tuple {Ik, Ok}. Here, Ik and Ok denote the
length of the input token sequence and the expected number
of output tokens for inference task k, respectively. LLM
inference typically adopts AD, generating one output token
per forward pass given an input sequence. However, AD incurs
high inference latency, as generating a complete output token
sequence requires multiple forward passes through the model.
SD mitigates this issue by introducing a small draft model to
predict future output tokens. The target LLM (referred to as
the verify model) then veriﬁes which of those tokens to accept
in parallel, thereby accelerating LLM inference. The verify
model generally has substantial computational requirements,
exceeding the processing capabilities of the SBS. For example,
many existing works use LLaMA-7B as the verify model and
LLaMA-68M as the draft model [20]. LLaMA-7B requires
approximately 14 GB of memory to load its model parameters
in FP16 precision during inference, exceeding the capacity of
most edge servers, such as the NVIDIA Jetson TX2 with only
8 GB of memory [21]. Motivated by this, we deploy the verify
model at the MBS with strong computational capacity, and the
small draft model at the SBS. By executing LLM inference
with SD, the SBS and MBS collaboratively generate output
tokens for each task.
A. Inference Mechanism with SD
To characterize the inference procedure, we ﬁrst describe
the structures of the draft model and the verify model. Similar
to many existing works [16], [17], the two models adopt a
decoder-only architecture composed of multiple stacked iden-
tical Transformer decoder blocks, as illustrated in Fig. 2. For
ease of presentation, we deﬁne a variable Ξ ∈{v, d} to repre-
sent the model index, where v and d denote the verify model
and the draft model, respectively. As shown in Fig. 3, each
decoder block in model Ξ consists of a multi-head attention
(MHA) module with weight matrices WQ, WK, WV , WO ∈
RhΞ
1 ×hΞ
1 , followed by a fully connected feed-forward network
(FFN) parameterized by W1 ∈RhΞ
1 ×hΞ
2 and W2 ∈RhΞ
2 ×hΞ
1 .
MBS
SBS
Draft 
Models
Edge Server
Uplink 
Fronthaul
Downlink 
Fronthaul
Verify 
Models
User
Transfer 
draft tokens
Transfer 
verification results
ķUpload
ĺDownload
Repeat until generating the required tokens
Output tokens
User
ĸSBS: generate drafts auto-regressively
ĹMBS:verify draft tokens in parallel
Accepted tokens 
Bonus token
Input sequence
Output sequence
Draft tokens
Input
Fig. 1.
Illustration of the proposed LLM serving system with speculative
decoding.
Decoder
Decoder
Verify model
You should
always
be
always
be
bad
Decoder
Decoder
You should
always
be
always
be
bad
Draft phase: autoregressive
Verification phase: non-autoregressive
F1
F2
F3
F1
F1
F1
Decoder
Decoder
Draft model
Prefill stage Decoding stage 
Fig. 2. The typical workﬂow of speculative decoding.
Here, hΞ
1 and hΞ
2 represent the hidden dimensions of the
Transformer decoder and FFN in model Ξ, respectively. With
the pre-trained draft and verify models, the inference process
of the considered system comprises the following procedures.
1) Input Sequence Uploading: For each inference task k,
the corresponding input sequence is ﬁrst transmitted from the
requesting user to the SBS via wireless uplink.
2) Draft Phase: Based on the received input sequences, the
draft model at the SBS adopts AD to generate l draft tokens for
each task k. The inference procedure with AD consists of two
stages, i.e., a preﬁll stage and a decoding stage. In the preﬁll
stage, the draft model processes the input sequence of each
task k through its Jd cascaded decoder blocks to generate the
ﬁrst draft token. The input of each decoder block j for task k
is speciﬁed by Cj ∈RIk×hd
1, where Cj = {cj
i : 1 ≤i ≤Ik}
and cj
i is the embedding of decoder block j for i-th input
token. The query, key, and value of Cj are computed by
Qj = CjWj
Q; Kj = CjWj
K; Vj = CjWj
V .
(1)
With Qj, Kj and Vj, the output of the MHA is given by
Cj
out = fsoftmax(Qj(Kj)T /
p
d0)VjWj
O + Cj.
(2)
Using Cj
out as the input of the FFN and residual connection,
the output of decoder block j is given by
Cj+1 = frelu(Cj
outWj
1)Wj
2 + Cj
out.
(3)
Based on the output of the ﬁnal decoder block, i.e., CJd+1,
the draft model produces a set of conditional probability


--- Page 4 ---
4
Decoder block *
KV Cache
MatMul
Softmax
Linear Project
O
W
K
V
Q
Input
Output
Decoder 
block
Multi-Head 
Attention
LayerNorm
LayerNorm
FFN
JX
Q
W
K
W
V
W
Fig. 3. The computation procedure of transformer decoder-only models.
distributions {q(·|c≤i) : 1 ≤i ≤Ik}, where q(·|c≤i) denotes
the probability distribution over the vocabulary for predicting
the next token given the ﬁrst i input tokens, i.e., c≤i. The ﬁrst
draft token is sampled from q(·|c≤Ik), since it is conditioned
on all input tokens. To reduce computation, the preﬁll stage
constructs the KV cache for each decoder block, which is
reused for draft token generation during the decoding stage.
In the decoding stage, the draft model utilizes and updates the
KV cache to generate subsequent draft tokens. Let tj ∈R1×hd
1
denote the embedding of the newly generated token in decoder
block j. The query, key, and value of tj are computed as
qj
new=tjWj
Q, kj
new=tjWj
K, vj
new=tjWj
V , respectively. The
KV cache is then updated by
Kj =Concat(Kj, kj
new);
Vj =Concat(Vj, vj
new).
(4)
Based on (4), the output of decoder block j is computed as
tj
out = fsoftmax(qj
new(Kj)T /
p
d0)VjWj
O + tj,
(5)
tj+1 = frelu(tj
outWj
1)Wj
2 + tj
out.
(6)
Given that the newly generated token is the (l−1)-th
draft token, the draft model utilizes the output of the last
decoder block, i.e., tJd+1, to generate the probability distri-
bution for predicting l-th draft token tl, denoted by qk,l =
q(·|c≤Ik, t≤l−1). Once l draft tokens are generated, the SBS
transmits them and the corresponding probability distributions
to the MBS via the uplink fronthaul link.
3) Veriﬁcation Phase: The verify model at the MBS
evaluates l draft tokens in parallel. Similar to the preﬁll stage
of AD, the verify model generates probability distributions
over the vocabulary for each position of the current input,
including the input sequence and l draft tokens. The set of
probability distributions is denoted by {p(·|c≤i) : 1 ≤i ≤
Ik} ∪{p(·|c≤Ik, t≤l : 1 ≤l ≤l}. Each draft token tl
is evaluated using the corresponding probability distribution,
i.e., pk,l = p(·|c≤Ik, t≤l−1). Speciﬁcally, each draft token tl
is accepted with probability min(1, pk,l/qk,l). In addition to
the accepted tokens, the generated output tokens include a
bonus token. The bonus token is either the correction token
for the ﬁrst unaccepted draft token or the reward token when
all the draft tokens are accepted based on the speculative
sampling methods [15]. According to [15], [18], the average
number of generated output tokens depends on the number of
draft tokens, i.e., speculation length l, and the average token






Draft phase
Verification phase 
Step 1
Step n
Step N
Fig. 4. Illustration of the total latency of pipeline-enabled LLM inference.
acceptance rate, denoted by α ∈(0, 1). The acceptance rate is
determined by the distributional similarity between the draft
model and the verify model. A higher acceptance rate and a
longer speculation length contribute to generating more output
tokens. With l and α, the average number of generated output
tokens after one veriﬁcation is given by [15]
L = 1 −α1+l
1 −α .
(7)
Then, the MBS transmits L output tokens to the SBS
over the downlink fronthaul link. Similar to the draft model,
the KV-cache mechanism is adopted for the verify model to
reduce computation. For both the draft and verify models, the
rejected draft tokens are removed from the KV cache after the
veriﬁcation phase to save memory resources [22].
4) Output Token Downloading: Upon receiving L output
tokens, the SBS immediately delivers them to users via the
wireless downlink. For clarity, we refer to a draft phase fol-
lowed by a veriﬁcation phase as a decoding step. The decoding
step is repeated until the complete output token sequence for
task k is generated. Note that, at the next decoding step, the
draft model takes the bonus token as input and iteratively
generates l new draft tokens. The verify model then takes the
bonus token together with the newly generated l draft tokens
as input to produce the next output tokens.
B. Pipeline-enabled Serving Mechanism with SD
To improve resource utilization and reduce serving latency,
we propose a pipeline-enabled serving mechanism with SD,
in which the draft phase and the veriﬁcation phase are orches-
trated in parallel across different tasks. Speciﬁcally, all infer-
ence tasks are ﬁrst partitioned into M batches, which are then
processed by the MBS and SBS at the batch level. For sim-
plicity, we denote the set of batches as M = {1, 2, · · ·, M}.
Since batching requires the input of all tasks in a batch to
have the same shape, the input sequence of each task in batch
m is right-padded to a ﬁxed length Im = max
k {xm
k Ik}, where
xm
k ∈{0, 1} indicates whether the inference task k is assigned
to batch m. For ease of presentation, let x = {xm
k : ∀m ∈
M, k ∈K} and N represent all tasks’ batching scheduling
decisions and total decoding step number for completing all
tasks, respectively.
Then, all batches are processed sequentially in a two-stage
pipeline at each decoding step, with the draft phase as the
ﬁrst stage and the veriﬁcation phase as the second stage, as
illustrated in Fig. 4. During the ﬁrst decoding step, all batches
participate in the pipeline. In subsequent decoding steps,
only unﬁnished batches remain in the pipeline, enabling early
completion for some tasks. The total decoding step number N


--- Page 5 ---
5
depends on the latest completed batch, i.e., N = max
m {nm},
where nm denotes the number of decoding steps required
to complete batch m. The value of nm is related to the
maximum expected output token length among the tasks in
batch m, and the number of generated output tokens for
each task per decoding step, i.e., L, in (7). The maximum
expected number of output tokens for batch m is expressed
as Om = max
k {xm
k Om
k }. Given Om and L, the decoding step
number of batch m is given by
nm = ⌈Om/L⌉,
(8)
where ⌈·⌉is the ceiling function.
C. Inference Cost Model
This subsection presents the inference cost in terms of
memory consumption and computational cost.
1) Memory Cost: The memory consumption of LLM infer-
ence with SD stems from the execution of both the draft and
verify models. For the draft model, memory usage is primarily
attributed to model weights and KV-cache. The model param-
eters are predominantly determined by the weight matrices of
its Jd decoder blocks, namely WQ, WK, WV , WO, W1, and
W2. Accordingly, the total parameter count of the draft model
is formulated as Jd(4(hd
1)2 +2hd
1hd
2), where the expression in
parentheses represents the number of parameters per decoder
block [23]. Assuming each parameter is stored with FP16
precision (2 bytes), the memory consumption for loading the
draft model parameters is given by
Γd
p = Jd(8(hd
1)2 + 4hd
1hd
2).
(9)
In addition to parameter storage, the KV-cache is a major
contributor to memory consumption. For each batch m at any
decoding step n, the memory usage of the KV-cache depends
on the total number of tokens requiring KV-cache storage.
These tokens include the input sequence of length Im, all
output tokens generated prior to decoding step n, and the
draft tokens produced at decoding step n for future output
prediction. For each task k in batch m, the combined number
of generated output tokens and current draft tokens does not
exceed the maximum expected output length of the batch,
i,e., Om. Thus, the number of tokens requiring KV-cache
for each task in batch m is upper-bounded by (Im + Om).
With FP16 precision, the maximum memory consumption for
KV-cache per task in batch m is 4Jdhd
1(Im + Om), where
4Jdhd
1 represents the memory usage for caching each token’s
key–value pair [23]. In addition, the number of tasks in batch
m is expressed as PK
k=1 xm
k . Therefore, the peak memory
consumption of KV-cache for the draft model when processing
batch m is given by
Γd
kv,m =
XK
k=1 4Jdhd
1(Im + Om)xm
k .
(10)
Based on (9) and (10), the peak memory consumption of
the draft model for batch m is expressed as Γd
m = Γd
p +
Γd
kv,m. Note that the memory analysis does not account for the
memory consumption of the verify model. This is because the
MBS is typically equipped with more powerful computational
resource and has more memory space than the SBS [19].
2) Computational Cost: During the inference procedure,
the computational cost is deﬁned as the ﬂoating-point op-
erations (FLOPs) consumed by the draft and verify models
across all decoding steps. For batch m, each decoding step
involves l sequential forward passes through the draft model
to autoregressively generate l draft tokens, followed by a
single forward pass through the verify model to evaluate
these draft tokens. Notably, for the draft model, the initial
forward pass during the ﬁrst decoding step corresponds to the
preﬁll stage, while the subsequent forward passes belong to
the decoding stage, as discussed in Section II-A. Similar to
many existing works [9], [23], we focus on FLOPs incurred
by matrix multiplications. Given that matrix multiplication
between A ∈Ra×b and B ∈Rb×c requires 2abc FLOPs,
the FLOPs incurred by the draft model during the preﬁll stage
for batch m are given by
F1 = Jd(8Im(hd
1)2 + 4I2
mhd
1 + 4Imhd
1hd
2),
(11)
where (8Im(hd
1)2+4I2
mhd
1) represents the FLOPs required for
computing Qj, Kj, Vj, and Cj
out, while 4Imhd
1hd
2 accounts
for the FLOPs to compute Cj+1 in each decoder block j.
Similarly, the FLOPs for the draft model to process batch
m during the decoding stage are expressed as
F2 = Jd(8(hd
1)2 + 4(Ikv,d + 1)hd
1 + 4hd
1hd
2),
(12)
where (8(hd
1)2 + 4(Ikv,d + 1)hd
1) represents FLOPs for com-
puting kj
new, qj
new, vj
new and tj
out, and 4hd
1hd
2 corresponds to
the FLOPs for computing tj+1, and Ikv,d denotes the length
of KV-cache for each decoder block. Based on (11) and (12),
the FLOPs required by i-th forward pass of the draft model
at decoding step n for each task in batch m are given by
F d
i,n,m =
(
4Jdhd
1Im[2hd
1+Im+hd
2],
if n=1, i=1,
4Jdhd
1[2hd
1+Ikv,d
i,n,m+1+hd
2], else,
(13)
where Ikv,d
i,n,m = Im+((n−1)L−1)+i−1 is the current KV-cache
length of i-th forward pass to process batch m at decoding step
n. Here, the ﬁrst term represents the initial input length, the
second term accounts for all output tokens generated prior to
the bonus token generated in the (n −1)-th step, and the last
term corresponds to the (i−1) draft tokens generated before
the i-th forward pass within step n.
Similar to the draft model, we analyze the FLOPs of the
verify model at each decoding step. Recalling Section II-A,
when processing batch m, the verify model at the ﬁrst de-
coding step follows the same computational procedure as the
preﬁll stage, with an input of length (Im+l). At the subsequent
decoding steps, the verify model follows a computational
procedure similar to the decoding stage, with an input of length
(1 + l). Thus, the FLOPs required by the verify model at any
decoding step can be derived by adjusting the input length and
substituting the verify model’s hyperparameters, i.e., hv
1, hv
2
and Jv, into the FLOPs expressions of the preﬁll and decoding
stages, i.e., (11) and (12). The FLOPs required by the verify
model at decoding step n for each task in batch m are given


--- Page 6 ---
6
by
F v
n,m=
(
4Jvhv
1(Im+l)[2hv
1+Im+l + hv
2],
if n=1,
4Jvhv
1(1+l)[2hv
1+Ikv,v
n,m+l+1+hv
2],
else,
(14)
where Ikv,v
n,m =Im+(n−1)L−1 is the current KV-cache length of
batch m at decoding step n. The current KV-cache includes the
input sequence of length Im and all output tokens preceding
the bonus token generated in step (n −1), with a total length
of ((n −1)L −1).
D. Serving Latency Model
In practice, improving quality of service (QoS) under mem-
ory and computational cost constraints is essential in LLM
serving systems. This work measures QoS using total serving
latency, encompassing communication and inference latencies.
1) Communication Latency: The wireless communication
process involves the uploading of input sequences from users
to the SBS via wireless uplink, and the downloading of output
tokens from the SBS to users through wireless downlink.
Notably, the downlink transmission latency is negligible, as the
SBS operates with higher transmit power than users, enabling
a high downlink transmission rate. Moreover, output tokens
are streamed to users immediately after each veriﬁcation
phase, without waiting for the complete output sequences.
This streaming transmission mechanism, widely adopted in
LLM serving systems, effectively reduces downlink trans-
mission load [24]. Hence, this work focuses on the uplink
transmission latency. Similar to [25], [26], the orthogonal
frequency division multiple access protocol is employed to
upload users’ input sequences with total uplink bandwidth
Bw Hz. Let w = {wk : ∀k ∈K} denote the system-wide
uplink bandwidth resource allocation, where wk represents
the bandwidth ratio allocated to task k. For each task k, the
uplink channel gain and transmit power are represented by gk
and pk, respectively. The achievable uplink rate of task k is
expressed as rk = wkBwlog2
 1 + pkgk/σ2
, where σ2 is the
noise power. Thus, the uploading latency of the input sequence
for task k is given by
Tk,com = λIk
rk
=
λIk
wkBw log2 (1 + pkgk/σ2),
(15)
where λIk is the data size (in bits) of the input sequence for
task k, and λ denotes the number of bits required to store
embedding data per input token. For instance, under FP16
precision, λ = 16(hd
1 +hv
1), where 16hd
1 and 16hv
1 denotes the
embedding size per token for the draft model and the verify
model, respectively.
Since the batch processing requires input sequences of
all tasks to be transmitted to the SBS before inference, the
communication latency determined by the latest-arriving task
is given by
Tcom = max
k∈K {Tk,com} .
(16)
2) Inference Latency: Once the input sequences for all
tasks are uploaded, the SBS and MBS collaboratively execute
the inference process illustrated in Fig. 4. The inference
latency is deﬁned as the cumulative processing latency of both
1
4
8
16
32
48
Batch Size
0
0.2
0.4
0.6
0.8
1
1.2
Model runtime(s)
(a)
1.31 TFLOPs (Measured)
0.66 TFLOPs (Measured)
0.33 TFLOPs (Measured)
1.31 TFLOPs (Fitted)
0.66 TFLOPs (Fitted)
0.33 TFLOPs (Fitted)
0
0.5
1
1.5
2
2.5
TFLOPs
0
0.2
0.4
0.6
0.8
Model runtime (s)
(b)
Batch size=4 (Measured)
Batch size=8 (Measured)
Batch size=16 (Measured)
Batch size=4 (Fitted)
Batch size=8 (Fitted)
Batch size=16 (Fitted)
Fig. 5.
Model runtime of LLaMA-7B versus batch size and FLOPs on
NVIDIA RTX4500.
the SBS and MBS across all decoding steps. The processing
latency at each decoding step corresponds to the execution
latency of a two-stage pipeline. The latency of the ﬁrst stage
encompasses the runtime of the draft model for generating
draft tokens and the uplink fronthaul transmission delay for
sending draft tokens to the MBS. For the second stage, the
corresponding latency includes the verify model’s runtime
and the downlink fronthaul transmission latency for delivering
veriﬁcation results to the SBS. Note that the latency of uplink
and downlink fronthaul transmission latency is negligible. This
is because both the uploaded draft tokens and the downloaded
veriﬁcation results are small in size and transmitted over high-
speed fronthaul links. For example, when running LLaMA-
2.7B on a Jetson Orin Nano, the average per-token inference
latency is approximately 30 ms, while transmitting the data
of a draft token, including the token id and its probability
distribution, over a fronthaul link with a rate of 10 Gbps takes
only about 10 µs in FP16 precision [27]. Hence, we use the
runtime of the draft and verify models to characterize the
processing latency at each decoding step. As found in many
existing works [5], [28], [29], there exists an approximately
linear relationship between the model runtime and the batch
size for different models on a given hardware platform. How-
ever, to the best of our knowledge, no prior work characterizes
the relationship among model runtime, batch size, and FLOPs.
To bridge this gap, we model this relationship as follows
T (b, F) = c1Fb + c2,
(17)
where b represents the batch size, F denotes the required
FLOPs for a forward pass of models, and c1, c2 are ﬁtting
coefﬁcients that depend on the speciﬁc hardware.
To validate the relationship, we conducted experiments
with LLaMA-7B on an NVIDIA RTX 4500, with the model
details provided in the experimental setup in Section IV.
Fig. 5 presents the measured runtime as a function of (a)
batch size and (b) FLOPs, along with linear regression ﬁts.
The results conﬁrm that model runtime scales near-linearly
with both batch size and FLOPs. Therefore, we adopt (17)


--- Page 7 ---
7
to characterize the runtime of each forward pass in LLM
inference. Accordingly, the runtime of the verify model to
process batch m at any decoding step n is given by
T v
n,m = cv
1F v
n,m
XK
k=1 xm
k + cv
2,
(18)
where PK
k=1 xm
k is the size of batch m, and cv
1, cv
2 are ﬁtting
coefﬁcients speciﬁc to the GPU at the MBS.
Similarly, the runtime of the draft model for processing
batch m at decoding step n is given by
T d
n,m =
Xl
i=1(cd
1F d
i,n,m
XK
k=1 xm
k + cd
2),
(19)
where (cd
1F d
i,n,m
PK
k=1 xm
k +cd
2) represents the runtime of the
draft model to generate the i-th draft token for each task in
batch m at decoding step n. cd
1 and cd
2 are ﬁtting coefﬁcients
related to the GPU deployed at the SBS. Note that the required
FLOPs for the draft and verify models to process batch m at
each decoding step n, i.e., F d
i,n,m and F v
n,m, can be found in
Section II-C.
For the two-stage pipeline at decoding step n, the latencies
of the ﬁrst and second stages to process batch m are T d
n,m
and T v
n,m, respectively. According to the classical pipeline
scheduling theory [30], the second stage of batch m can only
start after both the ﬁrst stage of batch m and the second stage
of batch (m −1) have completed. Let Cd
n,m = Pm
i=1 T d
n,i
denote the completion time for processing the ﬁrst m batches
in the ﬁrst stage at decoding step n. Correspondingly, the
completion time of the ﬁrst (m −1) batches through both
stages of decoding step n is deﬁned as Cn,m−1. Note that
the initial condition is Cn,0 = 0 for each decoding step n.
Thus, the start time of each batch m at the second stage of
decoding step n is expressed as max

Cd
n,m, Cn,m−1
	
. Given
the processing latency of batch m at the second stage, i.e.,
T v
n,m, the latency of processing the ﬁrst m batches at decoding
step n is given by
Cn,m = max

Cd
n,m, Cn,m−1
	
+ T v
n,m.
(20)
The total execution latency for decoding step n equals the
latency of processing all active batches of decoding step n,
which is given by
Tn = Cn,Mn,
(21)
where Mn = |Mn| is the number of active batches at decoding
step n, with Mn = {m : nm ≥n, ∀m ∈M} denoting the
set of active batches whose required decoding steps, i.e., nm
are not less than n.
Then, we obtain the inference latency by summing the
pipeline execution latencies across all decoding steps, i.e.,
Tinf =
XN
n=1 Tn.
(22)
Thus, the total serving latency for all tasks is given by
T = Tinf + Tcom.
(23)
E. Problem Formulation
This work aims to minimize the total serving latency of all
tasks, i.e., T . Speciﬁcally, we jointly optimize the decisions
of batching scheduling, the number of batches, speculation
length, and communication resource allocation under the lim-
ited memory and communication resources at the edge. The
optimization problem is formulated as follows
P :
min
{x,M,l,w} T
(24)
s. t.
XK
k=1 wk ≤1,
(24a)
Γd
m ≤Γs, ∀m,
(24b)
XM
m=1 xm
k = 1, ∀k,
(24c)
xm
k ∈{0, 1}, ∀k, m,
(24d)
0 ≤wk ≤1, ∀k,
(24e)
1 ≤l ≤lmax, ∀l ∈Z+,
(24f)
M ≤K, ∀M ∈Z+,
(24g)
where (24a) guarantees that the total wireless uplink band-
width allocated to tasks is not greater than the available band-
width resources. Furthermore, (24b) ensures that the memory
consumption of the draft model for each batch m cannot
exceed the available memory capacity of the SBS, i.e., Γs.
(24c) is to ensure that each task must be assigned to a batch
for processing. Meanwhile, (24d) and (24e) deﬁne the feasible
value ranges for batching and bandwidth allocation decisions,
respectively. (24f) ensures that the speculation length does
not exceed the maximum threshold lmax, thereby preventing
excessive computational resource waste due to veriﬁcation
failures. (24g) indicates that the number of batches cannot
surpass the task number. Problem P is a mixed-integer nonlin-
ear programming problem, which is generally NP-hard [31].
The difﬁculty in solving problem P mainly arises from the
strong coupling among decision variables and the complexity
of the objective function, which lacks an explicit closed
form due to the recursive nature of inference latency. To
address these challenges, we propose an efﬁcient algorithm for
communication resource allocation, batching, and speculation
length control to solve problem P.
III. EFFICIENT RESOURCE ALLOCATION, BATCHING AND
SPECULATION LENGTH CONTROL POLICY
In this section, we ﬁrst decompose the communication
resource allocation subproblem from problem P, and derive
its closed-form optimal solution. Then, we develop a dynamic
programming-based algorithm to obtain efﬁcient batching and
speculation length control strategies.
A. Optimal Communication Resource Allocation
In problem P, the communication resource allocation vari-
able, i.e., w, only impacts the communication latency and
the start time of inference procedure, and does not affect the
inference latency. Therefore, we decouple the communication
resource allocation subproblem from problem P, i.e.,
P1 :
min
w Tcom = max
k∈K {Tk,com}
(25)
s. t. (24a), (24e).


--- Page 8 ---
8
According to (15), it is obtained that the uploading latency
of each task, i.e., Tk,com =
λIk
wkBw log2(1+pkgk/σ2), monotoni-
cally decreases with the increase of the allocated bandwidth
ratio wk. If a task completes the upload of its input sequence
earlier than others, a portion of its allocated bandwidth can
be reallocated to slower tasks, thereby reducing the overall
communication latency determined by the slowest task. The re-
allocation process continues until all tasks complete uploading
their input sequences simultaneously. Therefore, the optimal
solution to P1 is achieved when bandwidth allocation ensures
that all tasks ﬁnish uploading at the same time. Hence, the
optimal communication resource allocation policy satisﬁes
(
wk =
λIk
t∗comBwlog2(1+pkgk/σ2), ∀k,
PK
k=1 wk = 1,
(26)
where t∗
com is the optimal communication latency.
By solving the linear equation in (26), we obtain the optimal
communication latency, i.e., t∗
com = PK
k=1
λIk
Bwlog2(1+pk/gkσ2).
By substituting t∗
com into (26), the optimal communication
resource allocation of each task k is given by
w∗
k =
Ik
log2(1+pkgk/σ2)
PK
k′=1
Ik′
log2(1+pk′ gk′ /σ2)
.
(27)
Remark 1. From (27), the allocated communication resource
of each task k is monotonically decreasing with its transmit
power pk and uplink channel gain gk. It means the tasks with
low transmit power and poor channel condition should be
allocated by more communication resources.
B. Optimal Batching and Speculation Length
According to the formulation of problem P, batching and
speculation length decision variables do not coupled with
the communication resource allocation variables, i.e., wk,
and only impact inference latency, i.e., Tinf. Therefore, the
batching and speculation optimization subproblem can be
decoupled from problem P. However, it is difﬁcult to directly
solve the subproblem due to the inherent coupling between
these decision variables in the model runtime, i.e., T v
n,m
and T d
n,m, which collectively determine the inference latency.
To efﬁciently address the subproblem, we ﬁrst optimize the
batching decisions, i.e., batch scheduling variables {xm
k } and
the number of batches M, under any given speculation length
l. Thus, the batching optimization subproblem is expressed as
P2 : min
{x,M}Tinf =
XN
n=1 Cn,Mn
(28)
s. t. (24b), (24c), (24d), (24g).
Solving problem P2 is challenging for three reasons: 1) the
variable batch number M introduces dynamic dimensionality
in the scheduling variables {xm
k }, 2) the recursive nature of the
formulation for Cn,Mn complicates direct optimization, and
3) the varying number of active batches Mn across decoding
steps further complicates the objective function. To address
these difﬁculties, we ﬁrst adopt a conservative approach by
setting a maximum possible output length for each task, i.e.,
Ok = Omax, ∀k ∈K. Under this simpliﬁcation, the active
batch set at each decoding step n becomes identical to the
complete batch set, i.e., Mn = M, since assigning Omax to
all tasks ensures that each batch undergoes the same number
of decoding steps, as deﬁned in (8). It is worth noting that
this approach is reasonable for system optimization since the
output length Ok of each task k is uncertain and difﬁcult to
predict accurately in practice [32]. In addition, the approach
ensures that the memory constraint, i.e., (24b), is satisﬁed for
any actual output length.
With this simpliﬁcation, we then propose a dynamic
programming algorithm that efﬁciently solves problem P2.
Speciﬁcally, all tasks are ﬁrst sorted in ascending order accord-
ing to their input sequence lengths. For ease of presentation,
the set of sorted task is denoted by task list. This preprocess-
ing step is to ensure that tasks with similar input lengths are
batched together, thereby reducing padding-induced compu-
tational latency. Subsequently, we deﬁne a three-dimensional
matrix Υ with dimensions K × (N + 1) × 2, where Υ[i, 0, 0]
represents the minimum inference latency achieved by the
algorithm for processing the ﬁrst i tasks in task list. To
compute Υ[i, 0, 0], we examine all possible batch boundary
positions j ∈{1, 2, · · ·, i}, where tasks j through i form
the ﬁnal batch. The size of this batch is (i −j + 1), and its
maximum input length is Ii. Substituting (i−j+1) and Ii into
(18) and (19), we obtain the latencies of the veriﬁcation and
draft phases, respectively, for processing the batch at decoding
step n, denoted by T v
n, j≤k≤i and T d
n, j≤k≤i. According to
the deﬁnition of the inference latency, i.e., (22) and (20), the
inference latency for processing the ﬁrst i tasks with each
batch boundary j is expressed as
Ti,j =
XN
n=1[max{Υ[j −1, n, 0], Υ[j −1, n, 1]
(29)
+ T d
n,j≤k≤i} + T v
n,j≤k≤i],
where Υ[j−1, n, 0] and Υ[j−1, n, 1] represent the completion
time in the ﬁrst and second stage at decoding step n for
processing ﬁrst (j −1) tasks, as previously stored in Υ.
Thus, the minimum inference latency for processing the ﬁrst
i tasks is given by
Υ[i, 0, 0] = min
1≤j≤i Ti,j.
(30)
Let j∗= arg min1≤j≤i Ti,j denote the batch boundary
that yields this minimum latency. To facilitate subsequent
computations, we store the completion time of the ﬁrst and
second stage at decoding step n for processing the ﬁrst i tasks,
i.e.,
Υ[i, n, 0] =Υ[j∗−1, n, 0] + T d
n,j∗≤k≤i,
(31)
Υ[i, n, 1] =max{Υ[i, n, 0], Υ[j∗−1, n, 1]}+T v
n,j∗≤k≤i. (32)
The core idea of the algorithm is to systematically evaluate
all possible batch boundary positions for each task i and
select the one that minimizes the latency for processing the
ﬁrst i tasks. During this process, batch boundaries that would
cause memory consumption to exceed the maximum available
memory of the SBS are discarded to ensure feasibility. By
iteratively updating (30), (31), and (32) for i ∈{1, 2, · · ·, K},


--- Page 9 ---
9
Algorithm 1 Dynamic Programming Algorithm for Obtaining
the Batching Decisions
1: Input: {Ik : k ∈K}, l, Omax
2: Output: The batching decision {xm
k : ∀k, m};
3: Sort tasks in increasing order according to the input sequence
length Ik, denoted by task list.
4: Initialize Υ = [0]K×(N+1)×2, S = [0]K, and i = 1;
5: for i = 1, 2, · · · , K do
6:
for j = 1 to i do
7:
Group task j to i in a batch.
8:
Compute the batch size, i.e., (i −j + 1).
9:
Obtain the maximum input length of the batch, i.e., Ii.
10:
Compute the memory consumption for the batch, i.e.,
Γd
j≤k≤i, based on (9) and (10).
11:
if Γd
j≤k≤i > Γs then
12:
continue
⊲skip infeasible batch (out of memory)
13:
Compute veriﬁcation phase latency T v
n,j≤k≤i using (18).
14:
Compute draft phase latency T d
n,j≤k≤i using (19).
15:
Compute the total latency temp = PN
n=1[max{Υ [j −
1, n, 0], Υ [j −1, n, 1] + T d
n,j≤k≤i} + T v
n,j≤k≤i]
16:
if j == 1 then
17:
Υ [i, 0, 0] = temp.
18:
j∗←1
19:
else
20:
if Υ [i, 0, 0] >= temp then
21:
Υ [i, 0, 0] = temp
22:
j∗←j, S[i] = j
23:
Update Υ [i, n, 0] and Υ [i, n, 1] substituting j∗into (31)
and (32).
24: for i = K, K −1, · · · , 1 do
25:
end idx ←i; start idx ←S[i]
26:
pack task list[start idx : end idx] into a batch.
27: return Optimal batching decisions, i.e., {xm
k } and M.
we obtain the minimum inference latency for processing all
tasks, yielding Υ[K, 0, 0]. To reconstruct the corresponding
batching decisions, we deﬁne a vector S with dimension K,
where S[i] stores the selected batch boundary j∗for task
i. By backtracking from S[K], the batching decisions, i.e.,
{xm
k } and M, can be recovered. For clarity, the detailed steps
of the batching algorithm are summarized in Algorithm 1
with a time complexity of O(K2N). This complexity arises
from the K2 iterations (lines 5–6) required to explore all
possible batching boundaries for each task i. Each iteration
is dominated by the computation of the total inference latency
for processing the ﬁrst i tasks (line 15), contributing an O(N)
factor per iteration. Compared with exhaustive search over all
batching decisions, whose complexity is O(KK), the proposed
algorithm signiﬁcantly reduces computational overhead.
Based on Algorithm 1 and (27), we can obtain the commu-
nication resource allocation and batching policies, denoted by
w∗, x∗, and M ∗, for any given speculation length l. Thus, the
total serving latency can be computed under any speculation
length. With w∗, x∗and M ∗, problem P is transformed into
the following speculation length optimization problem
P3 : min
l
Tinf
(33)
s. t. 1 ≤l ≤lmax, l ∈Z+.
(33a)
Directly solving problem P3 is challenging due to the pres-
ence of the non-covex term l(1 −α1+l)/(1 −α) in Tinf and
the ceiling operation in N = ⌈Omax (1 −α) /(1 −α1+l) ⌉,
which renders the objective function non-convex and non-
TABLE I
LLM SETTINGS IN THE SIMULATION
Model
Layer number
Hidden dimension
FFN dimension
LLaMA-68M
2
768
3072
LLaMA-1.1B
22
2048
5632
LLaMA-7B
32
4096
11008
LLaMA-13B
40
5120
13824
TABLE II
SYSTEM PARAMETERS
Parameter Value Parameter Value
Parameter Value
K
100
σ2
-106 dBm
cv
1
2.08 ×10−14
Omax
2048
pk
0.2 W
cv
2
1.28×10−2
lmax
10
Γs
16 GB
cd
1
4.11×10−13
Imax
512
Bw
20 MHz
cd
2
0.56×10−3
smooth. Since the feasible domain is small and contains only
lmax positive integers, the optimal speculation length can be
obtained by enumerating all integer values of l. Note that this
approach does not incur signiﬁcant computational overhead, as
the maximum speculation length lmax typically ranges from 10
to 20 tokens in practice, which helps avoid excessive resource
waste from veriﬁcation failures [20].
IV. SIMULATION RESULTS
This section evaluates the performance of the LLM serving
system with SD and the proposed algorithm. We consider
K = 100 inference tasks generated by users who are randomly
distributed within a cell of radius 400 m [25]. An SBS is
deployed at the center of the cell and connected to an MBS
via high-speed ﬁber links. Following [33], the wireless channel
gain from users to the SBS is modeled as gk = g0̺k(dk)−2,
where g0 = −30 dBm is the path loss constant, ̺k ∼Exp(1)
represents the Rayleigh fading component, and dk is the dis-
tance between the SBS and the user associated with task k. The
edge servers located at the SBS and MBS are equipped with an
NVIDIA GeForce RTX 3080 GPU and an NVIDIA RTX 4500
GPU, respectively, for processing inference tasks. For each
task, the input length and expected output length are randomly
chosen from the intervals [1, Imax] and [1, Omax], respectively.
To process these tasks and evaluate performance, we consider
three different draft-verify model pair deployment schemes
across the SBS and the MBS. Speciﬁcally, the model pairs
are (LLaMA-68M, LLaMA-7B), (LLaMA-1.1B, LLaMA-7B),
and (LLaMA-1.1B, LLaMA-13B), which are commonly used
for evaluating SD performance [20], [34]. The speciﬁcations
of these models are summarized in Table I, while other system
parameters (unless otherwise stated) are listed in Table II,
following existing works [9], [20], [33].
A. Performance of the Proposed Serving Mechanism
To verify the effectiveness of the proposed LLM serving
system with SD, we introduce four benchmarks: 1) SD without
pipeline (SD w/o Pipeline) [35]: The SBS and MBS sequen-
tially execute the draft and veriﬁcation phases for each batch.
The batching policies remain the same as those in the proposed
serving mechanism. 2) No batching [5]: The SBS and MBS


--- Page 10 ---
10
0.5
0.6
0.7
0.8
0.9
Acceptance Rate
0
50
100
150
Latency
(LLaMA-68M, LLaMA-7B)
Proposed
ADS
0.5
0.6
0.7
0.8
0.9
Acceptance Rate
0
50
100
150
200
250
Latency
(LLaMA-1.1B, LLaMA-7B)
Proposed
ADS
0.5
0.6
0.7
0.8
0.9
Acceptance Rate
0
50
100
150
200
250
300
Latency
(LLaMA-1.1B, LLaMA-13B)
Proposed
ADS
Fig. 6. Comparison of latency of the proposed scheme and AD under different
acceptance rate.
execute the proposed LLM serving mechanism without batch-
ing, i.e., processing one task at a time. 3) Fixed speculation
length (FSL) [20]: The SBS and MBS serve all tasks using the
proposed serving mechanism with a ﬁxed speculation length
of l = 7, regardless of task characteristics or the number of
arriving tasks. This scheme also adopts the proposed batching
strategy in III-B. 4) AD-based serving mechanism (ADS):
The verify model at the MBS processes all tasks using AD.
This baseline ensures fair latency comparison by achieving
the same output accuracy as the proposed mechanism, since
both rely on the verify model for ﬁnal output generation.
The batching strategy follows a heuristic approach [34] that
begins with two batches, progressively increases the batch
size while computing latency, and selects the optimal batch
conﬁguration when latency performance starts to degrade. For
a fair comparison, these benchmarks employ the proposed
communication resource allocation policy in Section III-A.
Fig. 6 presents the impacts of the acceptance rate (i.e., α)
on total latency for both the proposed serving mechanism and
AD-based serving mechanism for three draft–verify model
pairs. Note that different acceptance rates reﬂect the draft
model’s performance on various task types, with models typ-
ically exhibiting high acceptance rates for simple tasks (e.g.,
text completion) and low acceptance rates for complex tasks
(e.g., mathematical reasoning). It is observed that compared
to the AD-based serving mechanism, the proposed serving
mechanism achieves consistently lower latency across all draft-
verify model combinations and acceptance rates. In addition,
as α grows, the latency reduction achieved by the proposed
serving mechanism becomes more signiﬁcant compared to
ADS. The latent reason is that higher acceptance rates result in
more draft tokens being accepted by the verify model in each
decoding step, thereby generating more output tokens per de-
coding step. Consequently, fewer decoding steps are required
to complete all tasks, reducing both inference and total latency.
We also observe that, at the same acceptance rate, (LLaMA-
1.1B, LLaMA-13B) exhibits higher latency than the other two
model combinations. This is attributed to the larger verify
model in (LLaMA-1.1B, LLaMA-13B), which contains more
parameters and provides higher output accuracy. However, this
larger verify model requires more FLOPs during execution,
resulting in increased inference latency and consequently
higher total latency. These results indicate that the proposed
serving mechanism not only consistently outperforms the AD-
based serving system, but also effectively reduces inference
latency across tasks of varying difﬁculty.
Fig. 7 shows the performance comparison of the proposed
method against baseline serving mechanisms under different
task conﬁgurations for the model pair (LLaMA-1.1B, LLaMA-
7B) with an acceptance rate of α = 0.8. From Fig. 7(a), the
latency of all serving mechanisms exhibits an approximately
linear increase with the task number (i.e., K). This is be-
cause a larger number of tasks necessitates uploading more
input sequences from users, thereby increasing communication
latency. Additionally, more tasks increase the computational
load, consequently increasing inference latency. It is also
seen that the proposed scheme consistently achieves lower
latency than the three baseline methods across different task
numbers. The underlying reasons for this performance gain
are as follows: 1) The proposed serving mechanism overlaps
the draft and veriﬁcation phases across batches at each de-
coding step. This effectively reduces the idle time of GPUs
at both the SBS and MBS, thus achieving lower inference
latency compared to the SD w/o pipeline scheme. Moreover,
the proposed scheme incorporates adaptive speculation length
control to effectively balance the latencies of both draft and
veriﬁcation phases, further minimizing GPU idle periods and
achieving lower inference latency than the FSL scheme. 2)
Compared to the no-batching mechanism, the proposed serving
mechanism dynamically partitions tasks into multiple batches
and utilizes the parallel computing capacity of GPUs for
batched processing. This enables processing multiple tasks
with only minimal latency overhead compared to processing
tasks one by one, thereby reducing inference latency.
Fig. 7(b) presents how the latency varies with the maximum
input length (i.e., Imax) across different serving mechanisms.
One observation is that the latency of all serving mechanisms
increases as Imax increases. Compared with the best base-
line, i.e., SD w/o pipeline, the proposed serving mechanism
achieves a 31.6% latency reduction when Imax
=
512.


--- Page 11 ---
11
10
20
30
40
50
60
70
80
90
100
Task Number
0
100
200
300
400
500
600
700
800
Latency (s)
Proposed
SD w/o Pipeline
FSL
No Batching
31.3%
128 256
512
768
1024
1280
1536
1792
Maximum Input Length
0
100
200
300
400
500
600
700
800
900
Latency (s)
Proposed
SD w/o Pipeline
FSL
No Batching
31.6%
256
512
768
1024
1280
1536
1792
2048
Maximum Output Length
0
100
200
300
400
500
600
700
800
Latency (s)
Proposed
SD w/o Pipeline
FSL
No Batching
30.7%
(a)
(b)
(c)
Fig. 7.
Performance comparison of the proposed method against baseline serving mechanisms under different parameters: (a) task number, (b) maximum
input length, and (c) maximum output length.
The reason is that increasing Imax leads to longer input
sequences of tasks, which impose higher transmission loads
and greater computational demands, thereby increasing both
communication and inference latencies. Fig. 7(c) exhibits
the latency versus maximum output length (i.e., Omax) for
different serving mechanisms. As expected, the latency of all
schemes increases as the growth of Omax. This is because the
larger output length requires more decoding steps to process all
tasks, thereby increasing the computational load and inference
latency. Notably, as maximum output length increases, the
performance advantage of the proposed mechanism becomes
more pronounced compared to all benchmarks. Speciﬁcally,
the proposed serving mechanism reduces latency by about
30.7% compared with SD w/o pipeline when maximum output
length is 1792. These results verify that the proposed serving
mechanism achieves low latency under varying computational
and communication loads.
B. Performance of the Proposed Batching Algorithm
This subsection evaluates the proposed batching algorithm
by comparing it with the following benchmarks. 1) Static
batching [9]: The SBS and MBS implement the proposed LLM
serving mechanism, where all tasks are assembled into batches
with a small ﬁxed batch size to prevent memory overﬂow.
2) Max batching [11]: This scheme determines the maximum
feasible batch size based on arriving tasks’ input and output
lengths, then uses this batch size to partition tasks in the
proposed LLM serving mechanism. 3) Heuristic batching [34]:
The batch size is heuristically conﬁgured using the same
strategy as in the AD-based serving mechanism. Tasks are
subsequently assembled into batches using this batch size in
the proposed LLM serving mechanism.
Fig. 8 plots the impact of the batching schemes on the
latency under different task conﬁgurations for model com-
bination (LLaMA-1.1B, LLaMA-7B) under acceptance rate
α = 0.8. From Fig. 8(a), we see that the latency of all batching
schemes increases as the number of tasks grows, primarily due
to the increased communication and computational loads. In
addition, it is observed that the static batching scheme exhibits
the highest latency when the task number is large. This is
because the static batching scheme groups all tasks into a
small ﬁxed batch size without considering either the current
computational load or the characteristics of the tasks, which
ultimately leads to signiﬁcant inference latency. In contrast,
the proposed batching scheme consistently outperforms all
benchmarks across different task numbers. Speciﬁcally, when
the task number reaches 90, the proposed batching scheme
achieves up to a 21.4% reduction in latency compared with the
max batching scheme. The performance gain comes from the
proposed scheme’s ability to efﬁciently group tasks with sim-
ilar input lengths into the same batch, which reduces padding
and avoids inefﬁcient computational overhead, thereby further
lowering inference latency. In addition, the synergy between
the proposed batching scheme and adaptive speculation length
control enables efﬁcient coordination of the draft phases of
the current batch and the veriﬁcation phase of the next batch.
This reduces GPU idle time at both the SBS and MBS and
thus decreases the overall serving latency.
Fig. 8(b) and Fig. 8(c) present the latency versus the max-
imum input length and output length, respectively. As before,
the proposed scheme outperforms all benchmarks under dif-
ferent maximum input lengths and output lengths. Speciﬁcally,
compared with the best benchmark (i.e., heuristic batching
scheme), the proposed scheme reduces latency by up to 19.6%
when the maximum input length is 1792, and by 20.5% when
the maximum output length is 1024. Moreover, it is observed
that the proposed scheme also achieves lower latency than
the AD-based serving mechanism under different settings of
task number, maximum input length, and maximum output
length. These results verify that the proposed batching scheme
efﬁciently adapts to system load and task characteristics, i.e.,
input and output lengths, to achieve lower latency.
C. Performance of the Proposed Communication Resource
Allocation Algorithm
In this subsection, we evaluate the proposed communication
resource allocation policy by comparison with a uniform
resource allocation scheme. In the uniform scheme, the avail-
able uplink bandwidth is evenly divided among the uplink
transmissions of all tasks.
Fig. 9 illustrates how latency varies with the available
bandwidth (i.e., Bw) for different task numbers (i.e., K)


--- Page 12 ---
12
10
20
30
40
50
60
70
80
90
100
Task Number
0
100
200
300
400
500
Latency (s)
Proposed
Static batching
ADS
Max Batching
Heuristic Batching
21.4%
128 256
512
768
1024
1280
1536
1792
Maximum Input Length
0
100
200
300
400
500
600
700
Latency (s)
Proposed
Static batching
ADS
Max Batching
Heuristic Batching
19.6%
256
512
768
1024
1280
1536
1792
2048
Maximum Output Length
0
100
200
300
400
500
Latency (s)
Proposed
Static batching
ADS
Max Batching
Heuristic Batching
20.5%
(a)
(b)
(c)
Fig. 8. Performance comparison of the proposed batching scheme against baseline batching schemes under different parameters: (a) task number, (b) maximum
input length, and (c) maximum output length.
15
20
25
30
35
40
Bandwidth (MHz)
50
80
110
140
170
200
230
Latency (s)
44.9%
15
20
25
30
35
40
Bandwidth (MHz)
100
130
160
190
220
250
280
310
340
Latency (s)
29.3%
15
20
25
30
35
40
Bandwidth (Mbps)
130
170
210
250
290
330
370
Latency (s)
25.2%
(a)
(b)
(c)
Fig. 9. Performance comparison of the proposed communication resource allocation scheme against uniform schemes under different bandwidth: (a) model
combination (LLaMA-68M, LLaMA-7B), (b) model combination (LLaMA-1.1B, LLaMA-7B), and (c) model combination (LLaMA-1.1B, LLaMA-13B).
across three model combinations at α = 0.8. As shown
in Fig. 9, for each model combination, the latency of both
resource allocation schemes decreases as the bandwidth in-
creases. This is attributed to the fact that larger bandwidth
enables higher wireless transmission rates for uploading tasks’
input sequences, thereby reducing communication and total
latency. It is also observed that the latency of both schemes
increases as the number of tasks grows. The reason is that
a larger number of tasks results in less wireless bandwidth
allocated per task, leading to lower transmission rates and
consequently higher communication latency. Moreover, the
proposed resource allocation scheme consistently outperforms
the uniform allocation scheme across different bandwidths and
task numbers. Speciﬁcally, Fig. 9(a) shows that the proposed
scheme reduces latency by 44.9% compared with the uniform
allocation scheme when the number of tasks is K = 100,
the wireless bandwidth is Bw = 25 MHz, and the model
combination is (LLaMA-68M, LLaMA-7B). This performance
gain arises because the proposed scheme adaptively allocates
wireless bandwidth based on users’ channel conditions, en-
suring that tasks within the same batch arrive at the SBS
simultaneously and thereby reducing communication latency.
The results validate that the proposed scheme effectively
adapts to varying numbers of tasks and achieves low latency
even when bandwidth is small.
The experiments with (LLaMA-1.1B, LLaMA-7B) and
(LLaMA-1.1B,
LLaMA-13B),
shown
in
Fig.
9(b)
and
Fig. 9(c), yield similar conclusions. Speciﬁcally, when the
system contains K = 100 tasks and the bandwidth is Bw = 25
MHz, the proposed scheme reduces latency by up to 29.3%
and 25.2% for (LLaMA-1.1B, LLaMA-7B) and (LLaMA-
1.1B, LLaMA-13B), respectively, compared with the uniform
scheme. Furthermore, we see that among the three model
combinations, (LLaMA-68M, LLaMA-7B) exhibits the lowest
latency. This can be attributed to the relatively small model
dimensions of both LLaMA-68M and LLaMA-7B, i.e., hd
1 and
hv
1. According to (15), smaller model hidden dimensions re-
duce the embedding size per input token, i.e., λ = 16(hd
1+hv
1),
thereby decreasing the total uplink transmission data size
and communication latency. In addition, the small model
dimensions lead to low computational cost, further decreasing
inference latency. Consequently, (LLaMA-68M, LLaMA-7B)
achieves the minimum latency among the three combinations.
V. CONCLUSION
In this work, we developed a novel SD-based LLM serving
framework that distributes draft and verify models on a hetero-
geneous edge network to deliver low-latency LLM inference


--- Page 13 ---
13
services. Then, we proposed a pipeline parallel-enabled serv-
ing mechanism that overlaps the draft and veriﬁcation phases
of different inference tasks, thereby reducing GPU idle time
at both the SBS and MBS and thus reduce serving latency.
Based on this framework, we developed a latency model
to effectively characterize the serving latency of inference
tasks. Then, we developed effective joint optimization policies
for wireless communication resource allocation, batching, and
speculation length control to minimize the total serving latency
of inference tasks under limited communication and memory
resources. Experimental results veriﬁed that the proposed LLM
serving system efﬁciently reduces latency compared with AD-
based LLM serving systems. Additionally, experimental results
demonstrated that the proposed optimization policies achieved
lower latency than baseline methods.
REFERENCES
[1] G. Qu, Q. Chen, W. Wei, Z. Lin, X. Chen, and K. Huang, “Mobile edge
intelligence for large language models: A contemporary survey,” IEEE
Communications Surveys & Tutorials, 2025.
[2] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al.,
“Llama: Open and efﬁcient foundation language models,” arXiv preprint
arXiv:2302.13971, 2023.
[3] H. Zhou, C. Hu, Y. Yuan, Y. Cui, Y. Jin, C. Chen, H. Wu, D. Yuan,
L. Jiang, D. Wu et al., “Large language model (llm) for telecommu-
nications: A comprehensive survey on principles, key techniques, and
opportunities,” IEEE Communications Surveys & Tutorials, vol. 27,
no. 3, pp. 1955–2005, 2024.
[4] Z. Lin, G. Qu, Q. Chen, X. Chen, Z. Chen, and K. Huang, “Pushing large
language models to the 6g edge: Vision, challenges, and opportunities,”
IEEE Communications Magazine, vol. 63, no. 9, pp. 52–59, 2025.
[5] Y. Cang, M. Chen, and K. Huang, “Joint batching and scheduling for
high-throughput multiuser edge ai with asynchronous task arrivals,”
IEEE Transactions on Wireless Communications, vol. 23, no. 10, pp.
13 782–13 795, 2024.
[6] Z. Liu, Q. Lan, and K. Huang, “Resource allocation for multiuser edge
inference with batching and early exiting,” IEEE Journal on Selected
Areas in Communications, vol. 41, no. 4, pp. 1186–1200, 2023.
[7] M. Yao, L. Chen, J. Zhang, J. Huang, and J. Wu, “Loading cost-aware
model caching and request routing for cooperative edge inference,” in
ICC 2022-IEEE International Conference on Communications.
IEEE,
2022, pp. 2327–2332.
[8] X. Li and S. Bi, “Optimal ai model splitting and resource allocation for
device-edge co-inference in multi-user wireless sensing systems,” IEEE
Transactions on Wireless Communications, vol. 23, no. 9, pp. 11 094–
11 108, 2024.
[9] X. Zhang, J. Nie, Y. Huang, G. Xie, Z. Xiong, J. Liu, D. Niyato, and
X. S. Shen, “Beyond the cloud: Edge inference for generative large
language models in wireless networks,” IEEE Transactions on Wireless
Communications, 2024.
[10] M. Hu, Q. He, and D. Wu, “Qllms: Quantization-adaptive llm scheduling
for partially informed edge serving systems,” in IEEE INFOCOM 2025
- IEEE Conference on Computer Communications, 2025, pp. 1–10.
[11] M. Zhang, X. Shen, J. Cao, Z. Cui, and S. Jiang, “Edgeshard: Efﬁcient
llm inference via collaborative edge computing,” IEEE Internet of Things
Journal, 2024.
[12] K. Zhang, H. He, S. Song, J. Zhang, and K. B. Letaief, “Communication-
efﬁcient distributed on-device llm inference over wireless networks,”
IEEE Journal of Selected Topics in Signal Processing, pp. 1–16, 2025.
[13] G. Xie, Z. Xiong, R. Xie, X. Deng, S. Guo, M. Guizani, and Z. Han,
“Mixture of experts-enabled parallel scheduling and processing for
vehicular generative ai services,” IEEE Transactions on Cognitive Com-
munications and Networking, 2025.
[14] Y. He, J. Fang, F. R. Yu, and V. C. Leung, “Large language models
(llms) inference ofﬂoading and resource allocation in cloud-edge com-
puting: An active inference approach,” IEEE Transactions on Mobile
Computing, 2024.
[15] Y. Leviathan, M. Kalman, and Y. Matias, “Fast inference from transform-
ers via speculative decoding,” in International Conference on Machine
Learning.
PMLR, 2023, pp. 19 274–19 286.
[16] S. Kim, K. Mangalam, S. Moon, J. Malik, M. W. Mahoney, A. Gholami,
and K. Keutzer, “Speculative decoding with big little decoder,” Advances
in Neural Information Processing Systems, vol. 36, pp. 39 236–39 256,
2023.
[17] B. Spector and C. Re, “Accelerating llm inference with staged specula-
tive decoding,” arXiv preprint arXiv:2308.04623, 2023.
[18] R. Sadhukhan, J. Chen, Z. Chen, V. Tiwari, R. Lai, J. Shi, I. E.-
H. Yen, A. May, T. Chen, and B. Chen, “Magicdec: Breaking the
latency-throughput tradeoff for long context generation with speculative
decoding,” arXiv preprint arXiv:2408.11049, 2024.
[19] X. Hu, L. Wang, K.-K. Wong, M. Tao, Y. Zhang, and Z. Zheng, “Edge
and central cloud computing: A perfect pairing for high energy efﬁciency
and low-latency,” IEEE Transactions on Wireless Communications,
vol. 19, no. 2, pp. 1070–1083, 2020.
[20] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, Z. Zhang, R. Y. Y.
Wong, A. Zhu, L. Yang, X. Shi et al., “Specinfer: Accelerating large
language model serving with tree-based speculative inference and ver-
iﬁcation,” in Proceedings of the 29th ACM International Conference
on Architectural Support for Programming Languages and Operating
Systems, Volume 3, 2024, pp. 932–949.
[21] NVIDIA
Corporation,
“NVIDIA
Jetson
TX2
Developer
Kit,”
https://developer.nvidia.com/embedded/jetson-tx2,
2017,
accessed:
2024-12-19.
[22] B. Butler, S. Yu, A. Mazaheri, and A. Jannesari, “Pipeinfer: Accelerating
llm inference using asynchronous pipelined speculation,” in SC24:
International Conference for High Performance Computing, Networking,
Storage and Analysis.
IEEE, 2024, pp. 1–19.
[23] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang,
C. R´e, I. Stoica, and C. Zhang, “Flexgen: High-throughput generative
inference of large language models with a single gpu,” in International
Conference on Machine Learning.
PMLR, 2023, pp. 31 094–31 116.
[24] H. Li, Y. Liu, Y. Cheng, S. Ray, K. Du, and J. Jiang, “Eloquent: A more
robust transmission scheme for llm token streaming,” in Proceedings of
the 2024 SIGCOMM Workshop on Networks for AI Computing, 2024,
pp. 34–40.
[25] Z. Chen, W. Yi, Y. Liu, and A. Nallanathan, “Knowledge-aided federated
learning for energy-limited wireless networks,” IEEE Transactions on
Communications, vol. 71, no. 6, pp. 3368–3386, 2023.
[26] B. Zhu, Z. Chen, L. Zhao, H. Shin, and A. Nallanathan, “Joint caching
and inference for large language models in wireless networks,” in ICC
2025-IEEE International Conference on Communications.
IEEE, 2025,
pp. 6285–6290.
[27] J. A. Lab, “Deploying small language models on jetson: End-to-end tu-
torial,” https://www.jetson-ai-lab.com/tutorial˙slm.html, 2024, accessed:
2025-06-17.
[28] W. Shi, S. Zhou, Z. Niu, M. Jiang, and L. Geng, “Multiuser co-
inference with batch processing capable edge server,” IEEE Transactions
on Wireless Communications, vol. 22, no. 1, pp. 286–300, 2022.
[29] Y. Xu, J. Sun, S. Zhou, and Z. Niu, “Smdp-based dynamic batching
for efﬁcient inference on gpu-based platforms,” in ICC 2023-IEEE
International Conference on Communications.
IEEE, 2023, pp. 5483–
5489.
[30] S. M. Johnson, “Optimal two-and three-stage production schedules with
setup times included,” Naval research logistics quarterly, vol. 1, no. 1,
pp. 61–68, 1954.
[31] J. Du, L. Zhao, J. Feng, and X. Chu, “Computation ofﬂoading and
resource allocation in mixed fog/cloud computing systems with min-max
fairness guarantee,” IEEE Transactions on Communications, vol. 66,
no. 4, pp. 1594–1608, 2017.
[32] H. Oh, K. Kim, J. Kim, S. Kim, J. Lee, D.-s. Chang, and J. Seo, “Exegpt:
Constraint-aware resource scheduling for llm inference,” in Proceedings
of the 29th ACM International Conference on Architectural Support for
Programming Languages and Operating Systems, Volume 2, 2024, pp.
369–384.
[33] Z. Chen, W. Yi, and A. Nallanathan, “Exploring representativity in
device scheduling for wireless federated learning,” IEEE Transactions
on Wireless Communications, vol. 23, no. 1, pp. 720–735, 2023.
[34] F. Chen, P. Li, T. H. Luan, Z. Su, and J. Deng, “Spin: Accelerating large
language model inference with heterogeneous speculative models,” in
IEEE INFOCOM 2025-IEEE Conference on Computer Communications.
IEEE, 2025, pp. 1–10.
[35] W. Zhao, W. Jing, Z. Lu, and X. Wen, “Edge and terminal cooper-
ation enabled llm deployment optimization in wireless network,” in
2024 IEEE/CIC International Conference on Communications in China
(ICCC Workshops).
IEEE, 2024, pp. 220–225.
