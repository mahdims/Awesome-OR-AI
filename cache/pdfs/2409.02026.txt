--- Page 1 ---
Preprint 
 
1 
FOUNDATIONS OF LARGE LANGUAGE MODEL 
COMPRESSIONâ€”PART 1: WEIGHT QUANTIZATION 
Sean I. Young 
siyoung@csail.mit.edu 
ABSTRACT 
In recent years, compression of large language models (LLMs) has emerged as an 
important problem to enable language model deployment on resource-constrained 
devices, reduce computational costs, and mitigate the environmental footprint of 
large-scale AI infrastructure. In this paper, we lay down the foundation for LLM 
quantization from a convex optimization perspective and propose a quantization 
technique that builds on this foundation for optimum quantization outcomes. Our 
quantization framework, CVXQ, scales to models containing hundreds of billions 
of weight parameters and provides users with the ï¬‚exibility to compress models 
to any speciï¬ed model size, post-training. A reference implementation of CVXQ 
can be obtained from github.com/seannz/cvxq. 
1 
INTRODUCTION 
Large language Models (LLMs) have become a versatile framework for solving a large number of 
problems in natural language processing, from text translation and summarization to conversational 
AI and automatic generation of radiologistsâ€™ reports. While LLMs have surpass traditional methods 
in many of these tasks, they can involve tens or hundreds of billions of weight parameters (!), and 
this makes their deployment onto devices with limited resources challengingâ€”model weights and 
activations far exceed the available device memory so that activations need to be loaded from and 
saved to oï¬€-chip memory throughout inference, rendering LLM inference memory-bound (Yuan et 
al., 2024). Vis greatly hinders the usability of LLMs particularly in time-sensitive applications and 
exacerbates the environmental footprint of large-scale AI infrastructure required by LLMs. 
One way to reduce the memory requirements of large models for inference is by compressing (that 
(is, simplifying) the representation of the model weights and activations after training. Vis can be 
achieved via weight pruning, quantization of activations and weights, or PCA-type dimensionality 
reduction of weight matrices. Out of these, quantization of weights and activation has proven to be 
particularly useful for compressing models to very low bit depths or arbitrary user-speciï¬ed model 
sizes (Dettmers et al., 2022; Yao et al., 2022; Frantar et al., 2022; Frantar & Alistarh, 2022; Lin et 
al., 2024; Kim et al., 2024; Shao et al., 2024; Lee et al., 2024; Guan et al., 2024). Using state-of-
the-art quantization techniques, it is now possible to compress 10â€“100 billion-parameter LLMs to 
four bits per weight on average with negligible loss of model accuracy (Frantar et al., 2022; Lin et 
al., 2024), facilitating LLM inference on a single consumer-grade GPU for example. 
Although signiï¬cant advances have been made in LLM quantization recently, current approaches to 
model quantization still lead to considerably reduced model accuracy at low bit depths, with many 
methods ï¬ne-tuning model weights during quantization (Frantar et al., 2022; Lee et al., 2024). Vis 
makes such quantization methods unsuited to the quantization of activations during inference, where 
ï¬ne-tuning the values of activations would lead to unacceptable delays in the inference pipeline. 
Given the symmetry between weights and hidden states in matrix multiplications, achieving fast and 
accurate quantization of both weights and activations can be crucial for enhancing computational 
eï¬ƒciency and prediction accuracy of LLMs, as well as for informing hardware design. Vis work 
aims to address gaps in the current model compression literature and advance compression methods 
further to enable accurate and eï¬ƒcient inference on quantized LLMs. 
In this paperâ€”the ï¬rst of a three-part seriesâ€”we tackle the problem of LLM compression using the 


--- Page 2 ---
Preprint 
 
2 
framework of convex optimization. We begin with the problem of weight quantization and analyze 
how a modelâ€™s weights should be quantized to maximize quantized model accuracy for a given bit 
size. We then propose a stochastic gradient descent-type algorithm to solve this problem exactly and 
eï¬ƒciently, post-trainingâ€”in minutes for billion-parameter models and in a few hours for 10â€“100-
billion-parameter models. Compared with the recent OPTQ family of quantization methods (Frantar 
et al., 2022; Frantar & Alistarh, 2022; Huang et al., 2024; Lee et al., 2024), which ï¬ne-tune weights 
during quantization, our approach spends almost zero time on actual quantization once the optimal 
bit depths have been determined. Vis makes our framework also suited to quantizing intermediate 
activations, which can further reduce the memory footprint of LLM inference. Part 2: Activation 
Quantization discusses this and other hardware aspects of quantization in detail. 
2 
PREVIOUS WORK 
Early work on neural network model quantization can be attributed to Vanhoucke et al. (2011), who 
demonstrated that 8-bit integer arithmetic can be used for network training and inference without 
incurring a signiï¬cant loss of accuracy. More generally, quantization-aware training (QAT) (Zhou 
et al., 2017; Jacob et al., 2018; D. Zhang et al., 2018; Esser et al., 2019; Y. Choi et al., 2017; Wang 
et al., 2019) integrates the quantization process into training by allowing the model to adapt to the 
reduced precision in weights (Esser et al., 2019; Jacob et al., 2018; D. Zhang et al., 2018; Zhou et 
al., 2017) and activations (Y. Choi et al., 2017; Wang et al., 2019) by determining the optimum bit 
depth (Wang et al., 2019; D. Zhang et al., 2018) and step size (Esser et al., 2019) using back-prop 
to facilitate the gradient to ï¬‚ow through quantization operators. One shortcoming of QAT methods 
is that model training needs to be repeated for diï¬€erent quantized model sizes and accuracy, which 
makes them less suitable for quantizing larger models that require substantial computation and time 
for training. 
More recent quantization techniques for language and vision models aim to facilitate compression 
of already trained models for rapid deployment without further training (Dong et al., 2019; Chen et 
al., 2021; Dettmers et al., 2022; Yao et al., 2022; Frantar et al., 2022; Dettmers et al., 2023; Xiao et 
al., 2023; Lin et al., 2024; Kim et al., 2024; Shao et al., 2024; Lee et al., 2024). Vese approaches 
quantize model weights to 3â€“4 or 8 bits for integer-arithmetic-only inference (Jacob et al., 2018) 
using mixed bit depth quantization (Wang et al., 2019; Chen et al., 2021) or by a separate handling 
of outlier channels (Zhao et al., 2019) to improve the accuracy of the quantized model. Loss-aware 
quantization techniques (Hou & Kwok, 2018; Nahshan et al., 2020; Qu et al., 2020) seek to minimize 
accuracy loss in quantized models by calibrating quantization and biases on calibration data. Data-
free quantization methods (Nagel et al., 2019; Xu et al., 2020; K. Choi et al., 2021; Qian et al., 2023) 
attempt to remove the need for real calibration data by matching the distribution of weights instead 
(Nagel et al., 2019) or using synthetic data in place of real calibration data (K. Choi et al., 2021).  
For LLM compression in particular, an extension to the Optimum Brain Surgeon (OBS) algorithm 
(Hassibi & Stork, 1992) known as GPTQ (Frantar et al., 2022) was proposed for the quantization of 
1â€“100 billion parameter models. Further recent extensions to GPTQ (Dettmers et al., 2023; Lee et 
al., 2024) optionally incorporate the handling of sensitive weights by scaling or simply by retaining 
the original weight values, with other approaches (Lin et al., 2024; Xiao et al., 2023) independently 
incorporating similar ideas. Later, we will see that our convex optimization formulation provides a 
disciplined approach to incorporating channel sensitivity for mixed precision weight quantization. 
3 
QUANTIZATION FRAMEWORK 
Here, we use the task of next-token prediction in language modeling as a running example. For our 
purposes, the end-to-end mapping of input token embeddings to predicted next-token embeddings 
by a pretrained language model ğ‘“ can be expressed in the most general form as 
 
ğ™= ğ‘“(ğ—) = ğ‘“(ğ—, ğš¯1, ğš¯2, . . . , ğš¯ğ‘) = ğ‘“(ğ—, ğš¯1, ğš¯2, . . . , ğš¯ğ‘, ğ1, ğ2, . . . , ğğ‘) 
(1) 
in which ğ—âˆˆâ„ğ¿Ã—ğ¸ denotes a sequence of ğ¿ tokens, each of which resides in some ğ¸-dimensional 
embedding space, and ğ™âˆˆâ„ğ¿Ã—ğ¸, embeddings of ğ¿ predicted next tokens. Ve ğ‘šth block of weight 
matrices ğš¯ğ‘šğ‘€+1, . . . , ğš¯(ğ‘š+1)ğ‘€ and bias vectors ğğ‘šğ‘€+1, . . . , ğ(ğ‘š+1)ğ‘€ jointly parameterize the ğ‘šth 
transformer block, which reï¬nes the embeddings produced by the (ğ‘šâˆ’1)th transformer block. In 


--- Page 3 ---
Preprint 
 
3 
practice, LLM frameworks used in language modeling also require an embedder ğš¯0 âˆˆâ„ğ¸Ã—ğ‘‰ and a 
prediction head ğš¯ğ‘+1 âˆˆâ„ğ‘‰Ã—ğ¸ to transform between embeddings and tokens from a vocabulary of 
size ğ‘‰, but for now, we focus on the compression of transformer block weights as is typically done 
in model weight quantization work (Frantar et al., 2022; Lee et al., 2024; Lin et al., 2024). 
To get a sense of the number of weight matrices and their sizes in a typical language model, the 13 
billion-parameter model in the OPT family (OPT-13B) contains ğ‘= 240 weight matrices in blocks 
of ğ‘€= 6, with each block comprising 12ğ¸2 weights in an embedding dimension of ğ¸= 5120. Ve 
embedder and prediction head are parameterized by a shared matrix containing ğ‘‰ğ¸ weights, where 
the vocabulary size ğ‘‰= 50272. Note that each transformer block also contains 9ğ¸ bias parameters 
but due to their relative scarcity, bias parameters can be communicated losslessly and still have little 
to no impact on the overall compression performance (Frantar et al., 2022). 
Notionally, the elements of a weight matrix ğš¯ are continuously valued so they require quantization 
for eï¬ƒcient communication and storage. Compared with vector and lattice quantization techniques 
(Egiazarian et al., 2024; Gong et al., 2015), scalar quantization (Frantar et al., 2022; Lin et al., 2024) 
can simplify decoding and even enable operations directly on quantization indices, which obviates 
the need for a separate dequantization process. Ve mid-rise uniform scalar quantization of a weight 
ğœƒ at a bit-depth of ğµ bits and step size ğ· can be expressed as 
 
ğœƒğ‘(ğµ, ğ·) = ğ·(clip(floor(ğ·âˆ’1ğœƒ) , âˆ’2ğµâˆ’1, 2ğµâˆ’1 âˆ’1) + 2âˆ’1),   ğµ= 0, 1, 2, . . .  
(2) 
and ğœƒğ‘(ğµ, ğ·) = ğœƒ if ğµ= âˆ (for notational convenience). Ve problem of compressing a model ğ‘“ 
now amounts to determining the optimal bit depth ğµ and the associated quantization step size ğ· for 
model weights. It would be impractical, however, to determine a separate (ğµ, ğ·) for each weight ğœƒ 
in the model since the cost of signaling the choice of (ğµ, ğ·) for each one would far outweigh the bit 
savings derived from quantization. Typically, a single (ğµ, ğ·) pair is used to quantize a small group 
of weights (an entire matrix or rows or columns thereof) in which case the cost of signaling (ğµ, ğ·) 
can be borne by a group of quantized weight parameters as a negligible per-weight overhead. 
3.1 BIT DEPTH ASSIGNMENT 
Suppose we want to compress ğ‘“ by quantizing each matrix ğš¯ğ‘› containing ğ‘ƒğ‘› elements according to 
its own bit depth ğµğ‘› and step size ğ·ğ‘›
âˆ—(ğµğ‘›). How should ğµğ‘› be decided? Roughly speaking, weights 
that are more sensitive to output distortion should be allotted more bits to â€œbalance the scalesâ€ while 
keeping the total number of bits under a given model bit budget. We can formalize this notion by 
expressing the weight quantization task at hand as a constrained non-linear least-squares problem: 
 
minimize ğ‘‘(ğµ1, . . . , ğµğ‘) = ğ”¼ğ—â€–ğ‘“(ğ—, ğš¯1
ğ‘(ğµ1, ğ·1
âˆ—(ğµ1)), . . . , ğš¯ğ‘
ğ‘(ğµğ‘, ğ·ğ‘
âˆ—(ğµğ‘))) âˆ’ğ‘“(ğ—)â€–ğ¹
2  
subject toâ€Š ğ‘Ÿ(ğµ1, . . . , ğµğ‘) =âˆ‘
ğ‘ƒğ‘›ğµğ‘›âˆ’(âˆ‘
ğ‘ƒğ‘›
ğ‘
ğ‘›=1
)ğ‘…= 0
ğ‘
ğ‘›=1
   
(3) 
in which ğ‘… denotes a user-speciï¬ed average model bit depth (bit rate). Vis problem is reminiscent 
of optimal resource allocation, where the objective is to maximize some utility (or minimize output 
Algorithm 1. CVXQ: Convex Optimization for Weight Quantization 
1 Input: f(â€†â‹…â€†,Î˜1,â€Š.â€Š.â€Š.â€Š,Î˜N) (model), {X} (calibration set), R (target bit rate), Bmax (max bit depth) 
2 Output: B1,â€Š.â€Š.â€Š.â€Š,BN (bit depths), S1,â€Š.â€Š.â€Š.â€Š,SN (weight scales) , Âµ1,â€Š.â€Š.â€Š.â€Š,ÂµN (weight means) 
3 Initialize: U â†pca_basis({X}) âˆˆâ„EÃ—E', S â†sub_sample(ILÃ—L) âˆˆâ„LÃ—L', Vâ€Š â†10"6 
4  
Bn â†âˆ, Gn
2 â†0,  Âµn â†mean(Î˜n), Sn â†std(Î˜n), Î˜n
q â†Î˜n, Bn
q â†Bn, X&n â†0 for n in 1,â€Š.â€Š.â€Š.â€Š,N 
5 for iter in 1,â€Š.â€Š.â€Š.â€Š,max_iter do 
6  
for X in minibatch do 
7  
 
Z,X1,â€Š.â€Š.â€Š.â€Š,XN â†f (X,Î˜1
q,â€Š.â€Š.â€Š.â€Š,Î˜N
q ,B1
q,â€Š.â€Š.â€Š.â€Š,BN
q ) 
8  
 
X&n â†(1 â€“ Î±)X&n + (Î±/L)1TXn for n in 1,â€Š.â€Š.â€Š.â€Š,N 
9  
 
Î“1,â€Š.â€Š.â€Š.â€Š,Î“N â†autograd(STZU,â€ŠÎ˜1
q,â€Š.â€Š.â€Š.â€Š,â€ŠÎ˜N
q ) 
 
10  
 
Gn
2 â€Šâ† (1 â€“Î±)Gn
2 + (Î±/Pn)â€Štrace(Î“n
TÎ“n) for n in 1,â€Š.â€Š.â€Š.â€Š,N 
11  
for _ in 1,â€Š.â€Š.â€Š.â€Š,10 do 
12  
 
Bn â†clamp( 1
2 log2 (Gn
2Sn
2/V),0,â€ŠBmax) for n in 1,â€Š.â€Š.â€Š.â€Š,N 
13  
 
Vâ€†â€Šâ€† â†V + Î² (sum(PnBn) âˆ’(sum(Pn))R)  
14  
Î˜n
q â†compand_quantize(Î˜n,Bn,Sn,Âµn), Bn
q â€Š â†Bn + (Î˜n
q âˆ’Î˜n)X&n for n in 1,â€Š.â€Š.â€Š.â€Š,N 
 


--- Page 4 ---
Preprint 
 
4 
distortion in our case) by optimally spending down a given budget (the total number of bits). In this 
section and next, we provide insights into problem (3) and discuss its solution; see Algorithm 1. 
To apply the machinery of numerical optimization to (3), we will relax the discrete constraint on the 
bit depths ğµ1, . . . , ğµğ‘ while solving the problem and round the solution ğµ1
âˆ—, . . . , ğµğ‘
âˆ— to the nearest 
integers after they have been obtained. Let us write the Lagrangian of (3) as â„’(ğµ1, . . . , ğµğ‘, ğ‘‰) =
ğ‘‘(ğµ1, . . . , ğµğ‘) + ğ‘‰ğ‘Ÿ(ğµ1, . . . , ğµğ‘), where ğ‘‰ is a dual variable associated with the equality constraint 
of (3). Setting to 0 the partials of â„’ with respect to ğµ1, . . . , ğµğ‘, ğ‘‰ yields the optimality conditions 
 
1
ğ‘ƒ1
ğœ•ğ‘‘(ğµ1, ğµ2, . . . , ğµğ‘)
ğœ•ğµ1
= â‹…â€†â‹…â€†â‹… = 1
ğ‘ƒğ‘
ğœ•ğ‘‘(ğµ1, ğµ2, . . . , ğµğ‘)
ğœ•ğµğ‘
= âˆ’ğ‘‰,         ğ‘Ÿ(ğµ1, . . . , ğµğ‘) = 0 
(4) 
so, problem (3) can be solved by alternately updating the bit depths ğµ1, . . . , ğµğ‘ (primal variables) 
and the trade-oï¬€ ğ‘‰ (dual variable) until all optimality conditions are met. In words, the optimality 
conditions are reached once the marginal decrease in the output distortion from an inï¬nitesimal bit 
is equal across layers at âˆ’ğ‘‰ and once we have assigned exactly ğ‘… bits per weight on average. 
Since the quantization function (2) is constant almost everywhere, a naive computation of the partial 
derivatives of ğ‘‘ with respect to ğµ1, . . . , ğµğ‘ using the chain rule of diï¬€erentiation does not provide 
a useful direction for descent. One result from rateâ€“distortion theory (Gersho & Gray, 1991) is that 
for any random variable of ï¬nite variance, quantization error decreases by half with every additional 
bit at a suï¬ƒciently high bit depth. More speciï¬cally to our problem, we can write (Appendix A) 
 âˆ’
1
2 ln 2
ğœ•ğ‘‘(ğµ1, . . . , ğµğ‘)
ğœ•ğµğ‘›
â‰ˆğ”¼ğ—â€–
ğœ•ğ‘“(ğš¯1
ğ‘(ğµ1), . . . , ğš¯ğ‘
ğ‘(ğµğ‘))
ğœ•ğš¯ğ‘›
Î”ğ‘›
â€Šğ‘(ğµğ‘›)
â€–ğ¹
2
â‰ˆğ‘ƒğ‘›ğ»ğ‘›ğºğ‘›
2ğ‘†ğ‘›
22âˆ’2ğµğ‘›
âŸâŸâŸâŸâŸâŸâŸ
= ğ‘‘ğ‘›(ğµğ‘›)
 
(5) 
in which ğš¯ğ‘›
ğ‘(ğµğ‘›) = ğš¯ğ‘›
ğ‘(ğµğ‘›, ğ·ğ‘›
âˆ—(ğµğ‘›)) for brevity, ğºğ‘›
2 and ğ‘†ğ‘›
2 represent the variances of the elements 
of ğœ•ğš¯ğ‘›ğ‘“(ğ—, ğš¯1
ğ‘, . . . , ğš¯ğ‘
ğ‘), and of ğš¯ğ‘›
ğ‘, respectively, and ğ»ğ‘› is a quantization coeï¬ƒcient that depends 
on the type of weight distribution, with ğ»ğ‘›= 1.42 for Gaussian, 0.72 for Laplace, etc. (Gersho & 
Gray, 1991). Assuming weights are distributed similarly across layers with ğ»1 = â‹…â€†â‹…â€†â‹… = ğ»ğ‘, factors 
ğ»ğ‘› and constant âˆ’
1
2 ln 2 can be removed the above expression without aï¬€ecting the solution of (3). 
Coupled with the above closed-form expression for the partial derivatives, optimality conditions (4) 
naturally lend themselves to dual ascent-type methods for solving problem (3). Ve idea behind dual 
ascent (Boyd et al., 2011) is to alternately update the primal ğµ1, . . . , ğµğ‘, and dual ğ‘‰ variables, with 
one set held ï¬xed while updating the other. After initializing ğµ1 = â‹…â€†â‹…â€†â‹…ğµğ‘= âˆ, ğ‘‰ to some small 
positive number, and computing ğº1
2, . . . , ğºğ‘
2 , we update the bit depths and trade-oï¬€ iteratively via  
 ğµğ‘›â†clamp (
1
2 log2 (
ğºğ‘›
2ğ‘†ğ‘›
2
ğ‘‰
) , 0, ğµmax)      for ğ‘›= 1, . . . , ğ‘  
ğ‘‰â€†â€Š â†ğ‘‰+ ğ›¼(âˆ‘
ğ‘ƒğ‘›ğµğ‘›
ğ‘
ğ‘›=1
âˆ’(âˆ‘
ğ‘ƒğ‘›
ğ‘
ğ‘›=1
)ğ‘…)  
(6) 
in which ğ›¼ denotes a step size for dual update. Figure 1 illustrates the optimality conditions for bit 
Figure 1: Optimal bit depths. Consider two weight matrices whose distortion functions are given 
by ğ‘‘1 and ğ‘‘2, where ğ‘‘ğ‘›(ğµğ‘›) = ğºğ‘›
2ğ‘†ğ‘›
22âˆ’2ğµğ‘›. For any given value of the dual variable ğ‘‰, optimal bit 
depths ğµ1
âˆ— and ğµ2
âˆ— are found where the derivative of ğ‘‘1 and ğ‘‘2 is âˆ’ğ‘‰, respectively (left). Vese 
points correspond to the intersections between ğ‘‰ and âˆ’ğ‘‘ğ‘›
â€² = (2 ln2)â€Šğ‘‘ğ‘› (center). Integerized bit 
depths occur on the rounded curves âˆ’ğ‘‘Ì‚ğ‘›
â€² (right). 
10
1 
10
0 
10
â€“1 
 
0 
1 
2 
3 
4 
Bit depth (ğµ) 
Derivative of distortion 
4 
3 
2 
1 
0 
 
0 
1 
2 
3 
4 
Normalized distortion 
Bit depth (ğµ) 
ğ‘‘2 
ğ‘‘1 
Feasible 
region 
ğ‘‰= 1 
10
1 
10
0 
10
â€“1 
 
0 
1 
2 
3 
4 
Derivative of distortion 
Bit depth (ğµ) 
âˆ’ğ‘‘2
â€² 
âˆ’ğ‘‘1
â€² 
ğ‘‰= 1 
ğ‘‰= 4 
ğµ1
âˆ— 
ğµ2
âˆ— 
 ğµ2
âˆ— 
ğµ1
âˆ— 
ğ‘‰= 1 
ğ‘‰= 4 
ğµ2
âˆ— 
âˆ’ğ‘‘2
â€² 
âˆ’ğ‘‘Ì‚2
â€² 


--- Page 5 ---
Preprint 
 
5 
depths. With ğºğ‘›
2 and ğ‘†ğ‘›
2 ï¬xed, dual ascent steps (6) typically converge within a few iterations (tol =
10âˆ’6 bit, step size Î± = 2) after which the obtained ğµğ‘› are rounded to integers. Ve non-linear nature 
of the least squares objective ğ‘‘ (3) means that iteration (6) should be repeated after the bit depths 
ğµğ‘› are updated. Using the updated ğµğ‘›, we ï¬rst obtain the re-quantized weights ğš¯ğ‘›
ğ‘(ğµğ‘›) along with 
the re-computed gradient variances ğºğ‘›
2, based on which ğµğ‘› can be further updated via (6).  
Evaluating ğœ•ğš¯ğ‘›ğ‘“(ğ—, ğš¯1
ğ‘(ğµ1), . . . , ğš¯ğ‘
ğ‘(ğµğ‘)) across the entire calibration set at every iteration is 
prohibitively expensive given the dimensionality of the output ğ‘“(ğ—) âˆˆâ„ğ¿Ã—ğ¸ and the cost of back-
propagating each element through ğ‘“. To overcome this diï¬ƒculty, we perform PCA on ğ‘“(ğ—) along 
the embedding dimension (of ğ¸) and sub-sample along the token dimension (of ğ‘‡), and accumulate 
gradient variances by back-propagating only a mini-batch of calibration examples every time: 
 ğºğ‘›
2 â†(1 âˆ’ğ›½)ğºğ‘›
2 + ğ›½â€Šğ”¼ğ—âˆ¼batch â€–
ğœ•ğ’ğ‘‡ğ‘“(ğš¯1
ğ‘(ğµ1), . . . , ğš¯ğ‘
ğ‘(ğµğ‘))ğ”
ğœ•ğš¯ğ‘›
â€–ğ¹
2
  for ğ‘›= 1, . . . , ğ‘ 
(7) 
in which ğ›½ denotes the learning rate, and ğ’ğ‘‡ and ğ” represent the PCA projection and sub-sampling 
operators, respectively. In practice, we further accelerate variance accumulation by cycling through 
PCA coeï¬ƒcients and back-propagating only one coeï¬ƒcient per sample in every minibatch. 
3.2 STEP SIZES AND BIASES 
Suppose now the weight matrices ğš¯1, . . . , ğš¯ğ‘ are to be assigned bit depths ğµ1, . . . , ğµğ‘ (which are 
not necessarily optimum.) We now investigate how the quantization step size ğ·ğ‘› should be decided 
given bit depth ğµğ‘›. In the round-to-nearest scheme (RTN, Figure 2, left), ğ·ğ‘› is always chosen such 
that the quantizerâ€™s 2ğµğ‘› steps just cover the entire range of weight values, and this step size halves 
as ğµğ‘› is increased by one. Vese criteria optimize step sizes when weights are distributed uniformly 
across a range and the objective is to minimize distortion in quantized weights.  
For LLMs, the elements ğœƒ of a weight matrix typically exhibit a light-tailed distribution ğ‘ğœƒ (normal 
or Laplace) (Zhao et al., 2019), which renders partitioning the entire weight range into 2ğµğ‘› equal 
steps sub-optimal especially at low bit depths (Cover & Vomas, 2006; Gersho & Gray, 1991). One 
alternative to the computationally expensive Lloydâ€“Max algorithm (Lloyd, 1982; Max, 1960) is 
companded quantization (Gray & Neuhoï¬€, 1998), which applies a sigmoid transform to ğœƒ prior to 
uniform quantization to achieve ï¬ner quantization in regions of larger ğ‘ğœƒ and coarser quantization 
in regions where is smaller ğ‘ğœƒ; see Figure 2 (right). When the weights ğœƒ are Laplace-distributed with 
mean ğœ‡ and variance ğ‘†2, an asymptotically optimal choice of sigmoid function is (Appendix B): 
 
ğœ(ğœƒ, ğ‘†, ğœ‡) = 1 + sgn(ğœƒâˆ’ğœ‡) 
2
exp
(
âˆ’âˆš2 abs(ğœƒâˆ’ğœ‡)
3ğ‘†
)
âˆˆ(0, 1),    ğœƒâˆˆ(âˆ’âˆ, âˆ), 
(8) 
that is, the normalized cubic root of the cumulative distribution function for a Laplace distribution 
of the same mean and variance. Companded weights ğœƒğœ= ğœ(ğœƒ, ğ‘†, ğœ‡) are then quantized uniformly 
Figure 2: Companding quantization. Illustrated for a 4-bit quantizer (16 quantization levels) on
Gaussian weights with zero mean and unit variance. Uniform quantization across the entire range
of weight values (left) leads to unduly large quantization bins (hence quantization errors) for more
probable weights. Companding the weights to the range (0,1) prior to uniform quantization (middle)
reduces quantization errors for more probable weights (right), reducing the mean square error. 
1 
 
 
 
 
0 
 
â€“6 
â€“3 
0 
3 
6 
4 
2 
0 
â€“2 
â€“4 
0.2 
 
0.1 
 
  0 
 
â€“4 
â€“2 
0 
2 
4 
Probability 
Quantized value 
Companded value 
4 
2 
0 
â€“2 
â€“4 
 
â€“4 
â€“2 
0 
2 
4 
Probability 
Compand-quantized value 
0.2 
 
0.1 
 
  0 
Original weight 
Original weight 
Original weight 


--- Page 6 ---
Preprint 
 
6 
in the range (0, 1) and signaled together with the bit depth ğµ and scale ğ‘† for eï¬ƒcient dequantization 
using lookup tables. In practice, ğ‘†, ğœ‡ are treated as hyper-parameters and ï¬ne-tuned eï¬ƒciently on 
coarse 1D grids as a post-processing step (Young et al., 2021) once Algorithm 1 has completed. 
Quantization invariably causes small deterministic diï¬€erences to arise between the original (non-
quantized) ğš¯ and quantized ğš¯ğ‘ weights. While these errors are often modeled as zero-mean noise 
in theoretical analyses, they are seldom zero-mean in practice and can lead to systematically biased 
model output, which signiï¬cantly reduces prediction accuracy. To compensate for these non-zero 
diï¬€erences, we compute new bias vectors for the model as ğğ‘›
ğ‘â†ğğ‘›+ (ğš¯ğ‘›
ğ‘âˆ’ğš¯ğ‘›) X"n each time the 
matrix ğš¯ğ‘› is quantized. Here, X"n is a vector of running means of the inputs to the ğ‘›th layer, which 
is accumulated during the forward pass in a manner analogous to the accumulation of ğºğ‘›
2 during the 
backward pass. Ve corrected biases ğğ‘›
ğ‘ are then used whenever the corresponding quantized weight 
matrices ğš¯ğ‘›
ğ‘ are used during gradient variance accumulation and inference. 
3.3 MATRIX PARTITIONING 
Rather than quantize optimally at the granularity of a whole weight matrix, we can split each matrix 
into a collection of row or column matrices, assigning optimum bit depth and step size to each sub-
matrix. In this case, the total number of matrices ğ‘ in (3) can be reinterpreted as the total number 
of sub-matrices collected across all layers, with the quantities ğµğ‘›, ğ·ğ‘› and ğ‘ƒğ‘›, similarly interpreted 
as the bit-depth, step size and number of elements of the ğ‘›th sub-matrix. Note that quantizing at the 
granularity of row or column sub-matrices does not noticeably increase the complexity of variance 
accumulation, as the same squared gradients computed via back-propagation can be averaged per 
sub-matrix to produce the corresponding sub-matrix variances. Here, without loss of generality, we 
assume that each matrix is split into a collection of column matrices. 
For a weight matrix ğš¯ with gradient and weight variances ğº2 and ğ‘†2, whose per-column variances 
are ğº1
2, . . . , ğºğ‘
2  and ğ‘†1
2, . . . , ğ‘†ğ‘
2 , respectively, the theoretical gain (average bit depth savings) from 
partitioning can be expressed as 
 
ğ›¾partition = 1
2 (log2(ğº2ğ‘†2) âˆ’1
ğ‘âˆ‘
log2(ğºğ‘›
2ğ‘†ğ‘›
2)
ğ‘
ğ‘›=1
), 
(9) 
a non-negative quantity as a direct result of Jensenâ€™s inequality. Vis quantity represents the bit-rate 
(average bit-depth) savings when the ğ‘›th column is assigned ğµğ‘›= 1
2 log2(ğºğ‘›
2ğ‘†ğ‘›
2) + ğµ bits for some 
ğµ, compared to assigning a uniform bit depth ğµğ‘›= 1
2 log2(ğº2ğ‘†2) + ğµ bits across all columns under 
the assumption that the weights of its ğ‘ columns are identically distributed. Figure 3 (left) plots the 
per-matrix bit-depth savings derived by partitioning the (ğ‘„, ğ¾, ğ‘‰ and ğ‘‚) projection matrices of the 
OPT-125m model by rows or columns. Ve per-channel breakdown of the savings is also shown. 
In addition to primary splitting of matrices into columns, we may want to further split each column 
into a ï¬xed number of groups of weight elements given the presence of row bit savings as well. To 
split the columns of a weight matrix ğš¯âˆˆâ„ğ‘Ã—ğ‘, one can simply cluster its rows into ğ‘€ similarly 
Figure 3: Bit savings from partition. Plotted for OPT-125m. Savings are derived by partitioning
each weight matrix into a collection of row or column matrices and assigning each sub-matrix its
own bit depth. Savings diï¬€er across the (ğ‘„, ğ¾, ğ‘‰ and ğ‘‚) projection matrices of the modelâ€™s 12
transformer blocks (left). Per-column (middle) and row (right) bit savings (shown for block 3, ğ‘‚-
proj) can dip below zero but are always positive on average due to Jensenâ€™s inequality (see text). 
4 
3 
2 
1 
0  0 1 2 3 4 5 6 7 8 9 1011 
Per-matrix bit savings 
Transformer block index 
 Row 
Column 
Per-column bit savings 
 
0 
192 
384 
576 
768 
6 
4 
2 
0 
â€“2 
Sorted column index 
Column partition 
Average 
ğ‘‰ proj (block 0) 
6 
4 
2 
0 
â€“2 
 
0 
192 
384 
576 
768 
Per-row bit savings 
Sorted row index 
Row partition 
Average 
ğ‘‰ proj (block 0) 


--- Page 7 ---
Preprint 
 
7 
sized groups based on their row variances ğº1
2ğ‘†1
2, . . . , ğºğ‘
2 ğ‘†ğ‘
2 . By applying the same clustering to all 
columns of a matrix, we can signal the cluster index for each row using âŒˆlog2 ğ‘€âŒ‰ bitsâ€”a negligible 
per-weight overhead for a typical number of columns in a large matrix and the number of groups 
used in practice. We illustrate partitioning and subdivision in Figure 4. Later in Section 4, we show 
the accuracy of OPT models quantized using diï¬€erent numbers of row clusters, demonstrating that 
clustering in addition to partitioning is crucial for improved model accuracy. 
4 
QUANTIZATION EXPERIMENTS 
To study the behavior of quantized LLMs, we apply CVXQ (Algorithm 1) to the quantization of the 
Meta Open Pretrained Transformer (OPT) (S. Zhang et al., 2022) and Llama-2 (Touvron et al., 2023) 
families of language models (from the Hugging Face Hub), comparing the performance of CVXQ 
against other model quantization methods on language modeling and math-word problem solving 
tasks. For calibration data, we source 100 examples from the training split of the C4 dataset (Raï¬€el 
et al., 2020) for both tasks and test using the test split of WikiText2 (Merity et al., 2022) for language 
modeling and the test split of GSM8K (Cobbe et al., 2021) for word math problems. A number of 
studies on the sensitivity of algorithm hyperparameters are also conducted using the C4 dataset.  
Language Modeling. As our main set of experiments, we quantize Metaâ€™s OPT and Llama 2 models 
to 3 and 4 bits and measure the performance of the quantized models using perplexity, a stringent 
accuracy metric. We use row clusters with a cluster size of 512 for OPT (768 for OPT-125M) and 
256 for Llama 2 models, accumulation batch size of 16, and 17 tokens from each sequence of tokens 
of length 2048, and optimize for 64 iterations maximum. Table 1 lists the perplexity of our quantized 
models (CVXQ) on the WikiText2 test set. While we perform quantized model selection based on 
the WikiText2 validation set, selecting the last quantized model produces similar test accuracy. For 
comparison, we include the perplexities of the same models quantized with round-to-nearest, GPTQ 
(Frantar et al., 2022), OWQ (Lee et al., 2024), and AWQ (Lin et al., 2024) methods, using the code 
provided by the respective authors; see Appendix D for details. Relative to the next best performing 
methods OWQ and AWQ, the proposed method provides a perplexity reduction of up to 4.55 for the 
3-bit OPT-125M model although a minor perplexity gain (0.01â€“0.02) is observed for 3-bit OPT-66B 
and Llama 2 70B models. Note that AWQ in this comparison uses a group size of 128, incurring 2â€“
4 times as many overhead bits as the proposed method, and OWQ by its nature operates at average 
per-weight bit depths that are 0.01â€“0.05 bits higher than the proposed method. 
Eï¬€ect of Hyperparameters. To study the eï¬€ect of CVXQ hyperparameters on the accuracy of the 
quantized models, we quantize the OPT-1.3B and -13B models over a range of minibatch sizes and 
token counts (optimization hyperparameters) and cluster sizes (quantization hyperparameter), with 
each hyperparameter varied while keeping the others ï¬xed at their optimized values. (Ve optimal 
hyperparameter values are batch size: 16, token count: 17, and cluster size: 512.) Ve perplexity of 
the quantized models is then measured on the C4 test data. Table 2 (aâ€“b) demonstrates that CVXQ 
is largely insensitive to the values of optimization hyperparameters over a wide range. From Table 
2 (c), we see that smaller cluster sizes generally improve the performance of the quantized models 
at lower average bit depths, but this also leads to higher overheads (discussed later). Figure 5 plots 
quantized model accuracy across optimization iterations when the baseline hyperparameter values 
are used, showing that about 20 iterations are needed for quantization parameters (clustering and bit 
ğµ0 = 5 bits 
ğµ:,1 = 2 bits 
ğµ:,2 = 6 bits 
ğµ:,1 = 2 bits 
ğµ:,2 = 6 bits 
ğµ1,1 ğµ1,2 ğµ1,1 ğµ1,2 
ğµ2,1 ğµ2,2 ğµ2,1 ğµ2,2 
ğµ3,1 ğµ3,2 ğµ3,1 ğµ3,2 
ğµ4,1 ğµ4,2 ğµ4,1 ğµ4,2 
ğµ1,: = 4 bits 
ğµ2,: = 1 bits 
ğµ3,: = 3 bits 
ğµ4,: = 2 bits 
 (a) No partitions or clusters 
(b) Row partitioning 
(c) Column clustering 
(d) Partition and cluster 
 
No clusters 
No clusters 
Clusters 1 
2 
1 
2 Clusters 1 
2 
1 
2 
1 
2 
3 
4 
1 
2 
3 
4 
Row partition 
Row partition 
No partitions 
No partitions 
Figure 4: Partitioning and clustering. Illustrated for a 4 Ã— 4 weight matrix. Rather than assign the 
same bit depth to all elements of a weight matrix (a), we can assign a separate bit depth to each row 
of weights (b), or to a cluster of columns (c), and even combine partitioning and clustering (d) to 
realize row- and column-based bit savings.  
 


--- Page 8 ---
Preprint 
 
8 
depth decisions) to reach their optimal values.  
Pruning Due to Quantization. CVXQ quantizes low-variance weights of weight matrices to zero 
and eï¬€ects a form of weight pruning, which has been shown to improve generalization (Hassibi & 
Stork, 1992). Table 3 (a) lists the percentages of zero-quantized weights in the OPT-1.3B and 13B 
models quantized to 3 and 4 bits per weight on average. We observe that using smaller cluster sizes 
increases the number of pruned weights since this enables low-variance weights in each column to 
be clustered together and quantized to zero. However, smaller clusters lead to higher overheads so 
that small improvements in generalization due to pruning come at the cost of signaling the overhead 
bits. Table 3 (b) lists the number of overhead bits (cluster indices and FP16 encodings of the location 
Figure 5: Test perplexity across iterations. Calibrated on C4 (train) using a batch size of 16. Row 
clusters of size 512 used. Perplexity decreases rapidly within the ï¬rst 30 iterations, monotonically 
for C4 (test), whose distribution is similar to the calibration data, and with some oscillations in the 
case of WikiText2 (test), whose distribution is less similar. 
Test Perplexity 
16
15
14
13
12
 
0 
10 
20 
30 
Iter 
C4 
WT2 
C4 (Full) 
OPT-2.7B (4 bits) 
WT2 (Full) 
14 
13 
12 
11 
10 
 
0 
10 
20 
30 
Iter 
Test Perplexity 
OPT-6.7B (4 bits) 
WT2 
C4 
C4 (Full) 
WT2 (Full) 
Test Perplexity 
13 
12 
11 
10 
9 
 
0 
10 
20 
30 
Iter 
OPT-30B (3 bits) 
WT2 
C4 
C4 (Full) 
WT2 (Full) 
Perplexity (PPL) 
WikiText2 (â†“) 
Meta OPT (Open Pretrained Transformer) 
Meta Llama 2 
 125M 350M  1.3B  2.7B  6.7B  
13B  
30B 
66B 
7B 
13B 
70B 
 
Full precision (FP16) 
27.65 22.00 
14.63 
12.47 
10.86 
10.13 
9.56 
9.34 
5.47 
4.88 
3.32 
4 bits 
RTN 
37.28 25.94 
48.17 
16.92 
12.10 
11.32 
10.98 
111.36 
5.73 
4.98 
3.46 
GPTQ 
32.05 23.87 
15.47 
12.83 
11.14 
10.29 
9.57 
9.34 
5.69 
4.98 
3.42 
GPTQ/256 
30.53 23.83 
14.91 
12.52 
11.02 
10.22 
9.60 
9.46 
5.69 
5.02 
3.44 
OWQ (4.01 bits) 
29.47 23.19 
15.01 
12.39 
10.87 
10.26 
9.50 
9.25 
5.63 
5.01 
3.43 
AWQ 
29.11 
â€“ 
14.95 
12.74 
10.93 
10.22 
9.59 
9.39 
5.60 
4.97 
3.41 
CVXQ (Ours) 
27.90 22.89 
14.20 
12.12 
10.52 
10.08 
9.45 
9.21 
5.57 
4.97 
3.40 
3 bits 
RTN 
1284.92 64.57 119.47 298.00 
23.54 
46.04 
18.80 6122.33  
6.66 
5.52 
3.98 
GPTQ 
53.43 32.28 
20.90 
16.55 
12.88 
11.58 
10.29 
9.90 
6.43 
5.48 
3.88 
GPTQ/256 
41.22 29.96 
16.98 
13.94 
11.39 
10.41 
9.81 
11.13 
6.75 
5.59 
4.00 
OWQ (3.01 bits) 
35.26 26.59 
16.40 
13.21 
11.21 
11.48 
9.59 
9.28 
6.21 
5.36 
3.77 
AWQ 
36.77 
â€“ 
16.32 
13.54 
11.41 
10.67 
9.85 
9.63 
6.24 
5.32 
3.74 
CVXQ (Ours) 
30.71 25.96 
14.75 
12.42 
11.07 
10.28 
9.56 
9.29 
6.00 
5.26 
3.74 
Table 1: WikiText2 perplexity. We quantize the Meta OPT and Llama 2 families of LLMs to 3â€“4 
bits per weight on average using the proposed quantization method, reporting the perplexity of each 
quantized model on the WikiText2 dataset (test). For comparison, we also include the perplexities 
of models quantized using other approaches. 
Table 2: Eï¬€ect of hyperparameters on quantized model accuracy. Quantized model accuracy is
relatively insensitive to the minibatch size (a) and number of tokens per sequence (b) used for the
optimization. Smaller clusters improve quantized model accuracy at low average bit depths (c). All
perplexity measured on the C4 test set. 
 
(a) Minibatch size and PPL 
(b) Number of tokens and PPL 
(c) Cluster size and PPL 
PPL 
C4 (â†“) 
OPT (4 bits)  OPT (3 bits) 
1.3B 13B  1.3B 13B 
FP16 16.07 12.06  16.07 12.06 
Batch size 
2 16.24 12.12  16.94 12.36 
4 16.24 12.12  16.94 12.35 
8 16.25 12.11  16.90 12.34 
16 16.22 12.11  16.86 12.32 
32 16.24 12.12  16.88 12.36 
PPL 
C4 (â†“) 
OPT (4 bits)  OPT (3 bits) 
1.3B 13B  1.3B 13B 
FP16 16.07 12.06  16.07 12.06 
Cluster size 
64 16.16 12.10  16.62 12.26 
128 16.17 12.10  16.70 12.29 
256 16.20 12.10  16.77 12.32 
512 16.22 12.11  16.86 12.32 
1024 16.23 12.11  16.99 12.42 
PPL 
C4 (â†“) 
OPT (4 bits)  OPT (3 bits) 
1.3B 13B  1.3B 13B 
FP16 16.07 12.06  16.07 12.06 
Num tokens  
3 16.40 12.29  17.05 12.47 
5 16.28 12.18  16.93 12.37 
9 16.24 12.12  16.91 12.35 
17 16.22 12.11  16.86 12.32 
33 16.21 12.10  16.87 12.34 


--- Page 9 ---
Preprint 
 
9 
and scale parameters of each weight cluster) as a percentage of the total quantized weight bits. Vese 
overheads are in line with those of other algorithms which must similarly signal zero points and step 
sizes of the quantization grid (Lee et al., 2024). 
Downstream Tasks (Grade School Math 8K). To study the impact of quantization on downstream 
tasks, we list in Table 4 (a) the accuracy of CVXQ-quantized Llama-2 models on the GSM8K (Grad 
School Math 8K) task (Cobbe et al., 2021), designed to evaluate the ability of language models to 
solve grade-level math word problems. Evaluation is performed in a 5-shot setup (ï¬‚exible-extract 
ï¬lter). We set our cluster size and the group size of GPTQ and AWQ to 256. We observe that CVXQ 
produces slightly higher scores than the GPTQ and AWQ quantized 3-bit models while RTN leads 
to severely diminished scores despite having similar perplexity scores as CVXQ on WikiText2; see 
Table 1. We include examples of output from diï¬€erent quantized models in Appendix E. 
2.x-bit Llama-2. We study the accuracy of Llama 2 models quantized to 2.x bits using CVXQ and 
OWQ, both of which are capable of quantizing models to fractional average bit depths. To enable a 
more comprehensive study, we compare against OWQ with no grouping, as well as with group sizes 
of 128 and 256. We see from Table 4 (b) that CVXQ-quantized Llama-2 models are considerably 
more accurate at these bit depths than their OWQ counterparts. Vis is expected since CVXQ assigns 
bit depths from the range (0, ğµmax) commensurately with gradient variances whereas OWQ opts to 
preserve the most sensitive (highest-variance) weights in FP16 and quantize the rest to 2 bits (Lee 
et al., 2024). In terms of execution time, CVXQ (64 iterations) and OWQ/GPTQ require 47 minutes 
and 18 minutes, respectively (excluding validation), to quantize the 7B model on an Nvidia A100. 
5 
DISCUSSION 
Formulating weight quantization as a convex optimization problem, as we have done here, bestows 
several beneï¬ts. First, it explicates the objective we seek to optimize (minimizing output distortion 
in our case) and sets us on a path to solve the right problem using modern automatic diï¬€erentiation 
tools e.g. PyTorchâ€™s autograd package. Second, our formulation enables us to interpret many earlier 
Hessian-based methods (Frantar et al., 2022; Lee et al., 2024; Dong et al., 2019; Chen et al., 2021) 
as heuristics for approximate optimization of the true underlying quantization objective. Note that 
(2) is a nonlinear system of equations in the bit depth variables, so that any non-iterative solution is 
Table 3: Pruning and overhead bits. A small fraction of weights is quantized to zero and pruned 
away due to low variance, with smaller clusters increasing the degree of pruning (a). Quantization 
incurs overhead bits for signaling cluster indices and location and scale parameters of each weight 
cluster (b). 
 
(a) Pruned columns (%) in quantized models 
(b) Overhead bits (%) from quantization parameters 
Overhead 
bits (%) 
OPT (4 bits) 
 
OPT (3 bits) 
350M 1.3B 13B  350M 1.3B 13B 30B 
Cluster size 
64 10.33 10.30 10.28  13.77 13.73 13.71 13.70 
128 
 5.18  5.16  5.15   6.91  6.88  6.87  6.86 
256 
 2.60  2.59  2.58   3.47  3.45  3.44  3.44 
512 
 1.30  1.30  1.30   1.73  1.73  1.73  1.72 
1024 
 0.64  0.65  0.65   0.85  0.87  0.87  0.86 
Pruned 
(%) 
OPT (4 bits) 
 
OPT (3 bits) 
350M 1.3B 13B  350M 1.3B 13B 
Cluster size 
64 
0.57 
2.13 
2.18  
0.64 
3.70 
3.12 
128 
0.61 
2.19 
2.31  
0.68 
3.81 
3.04 
256 
0.67 
2.10 
2.16  
0.69 
3.06 
2.69 
512 
0.68 
2.07 
2.00  
0.70 
2.85 
2.57 
1024 
0.68 
2.08 
1.92  
0.70 
2.39 
2.26 
Table 4: Grade School Math 8K (GSM8K) and 2.x-bit quantization. Quantized model accuracy 
measured by performance on tasks such as GSM8K (a). Cluster size of 256 is used). Quantized to 
2.x bits per weight on average, CVXQ reduces perplexity considerably when compared with OWQ 
models quantized to the same (b). 
 
(a) Percentage score of correct answers on GSM8K 
(b) Perplexity of 2.1â€“2.8 bit-quantized models  
Score (%) 
GSM8K (â†‘) 
Llama 2 (4 bits) 
 
Llama 2 (3 bits) 
7B 
13B 
70B  
7B 
13B 
70B 
FP16 
14.10 23.43 53.90  14.10 23.43 53.90 
RTN 
7.05 19.11 46.93  1.82 
1.67 
6.14 
GPTQ/256 
11.60 21.46 52.01  6.60 14.48 46.47 
AWQ/256 
14.33 23.12 50.34  6.97 16.76 48.07 
CVXQ/256 
12.74 23.05 53.37  8.87 18.04 48.60 
Perplexity 
WikiText2 (â†“) 
Llama 2 7B (2.1â€“2.8 bits) 
2.1 
2.2 
2.4  
2.6  
2.8 
FP16 
5.47 
5.47 
5.47 
5.47 
5.47 
OWQ 
39.56 11.25 10.79 10.43 10.24 
OWQ/256 
10.34 10.01 
9.98 
9.50 
9.26 
OWQ/128 
10.01 
9.66 
9.42 
9.38 
9.14 
CVXQ/256 
9.47 
8.39 
7.05 
6.56 
6.21 


--- Page 10 ---
Preprint 
 
10 
necessarily only an approximate one if oneâ€™s goal is to optimize an objective similar to (2). Recent 
high-performing model quantization methods (Frantar et al., 2022; Frantar & Alistarh, 2022; Lee et 
al., 2024) ultimately trace their lineage back to the classic Optimal Brain Surgeon algorithm (Hassibi 
& Stork, 1992), which is a convex formulation of weight pruning, as opposed to quantization (see 
Appendix C). As a result, these methods inherit the need for ï¬ne-tuning as part of the quantization 
process, making them less suitable for the quantization of activations during inference, where ï¬ne-
tuning of activations would lead to unacceptable delays in the inference pipeline. 
Our experimental results indicate that an accurate characterization of the quantization problem can 
indeed lead to better compression outcomes. While the smaller OPT-125M model is too limited for 
practical use in many situations, its relative incompressibility helps contrast the performance of the 
diï¬€erent weight quantization methods themselves (Table 1). With larger models like OPT-66B and 
Llama 2-66B, most approaches (including RTN) perform similarly, suggesting that larger language 
models are more compressible in general. At ï¬rst glance, RTN may seem suï¬ƒcient for quantizing 
larger models. However, RTN-quantized models lead to severely reduced accuracy on downstream 
tasks such as GSM8K (Table 4 (a)), which highlights the importance of validating the accuracy of 
quantized models across multiple tasks and datasets. Increasing the number of calibration examples 
(from 100 to 1000) does not appear to noticeably aï¬€ect the quantized modelâ€™s performance, which 
agrees with ï¬ndings from previous reports (Hubara et al., 2021). 
Limitations and future work. Ve end-to-end nature of our optimization algorithm can also be its 
own weakness, as convergence can be slower for lower-bit (2 bit) instances of problem (3) and the 
algorithm can require more hardware resources than previous approaches. Extension work currently 
underway for weight quantization includes faster optimizers and eï¬ƒcient compander design. Unlike 
GPTQ and its extensions, in which each linear layer is essentially retrained during quantization, the 
proposed method spends most of its running time on the collection of channel statistics (squares of 
gradients) and very little time on the actual quantization process. Vis allows us to apply CVXQ also 
to activation quantization, where quantization eï¬ƒciency becomes paramount. We discuss activation 
quantization and CUDA implementation of joint weight and activation quantization in Part 2. 
REPRODUCIBILITY 
To ensure the reproducibility of results in this work, we make our PyTorch CVXQ code available 
on our GitHub project website, where readers can also ask questions about this work. Appendices 
Aâ€“B provide derivations for our main theoretical results. Appendix D additionally details the code 
and command line options used to obtain the results of GPTQ (Frantar et al., 2022), OWQ (Lee et 
al., 2024), and AWQ (Lin et al., 2024). 
REFERENCES 
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed 
optimization and statistical learning via the alternating direction method of multipliers. Found. 
TrendsÂ® Mach. Learn., 3(1):1â€“122, 2011. 
Weihan Chen, Peisong Wang, and Jian Cheng. Towards mixed-precision quantization of neural 
networks via constrained optimization. In Proc ICCV, 2021. 
Kanghyun Choi, Deokki Hong, Noseong Park, Youngsok Kim, and Jinho Lee. Qimera: Data-free 
quantization with synthetic boundary supporting samples. In Proc. NeurIPS, 2021. 
Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. Towards the limits of network quantization. In 
Proc. ICLR, 2017. 
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian et al. Training veriï¬ers to solve math word 
problems. http://arxiv.org/abs/2110.14168, 2021. 
Vomas M. Cover, and Joy A. Vomas. Elements of Information Veory (Wiley Series in 
Telecommunications and Signal Processing). Wiley-Interscience, USA 2006. 
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. GPT3.int8(): 8-bit matrix 
multiplication for transformers at scale. In Proc. NeurIPS, 2022. 


--- Page 11 ---
Preprint 
 
11 
Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian et al. SpQR: A Sparse-Quantized 
Representation for near-lossless LLM weight compression. http://arxiv.org/abs/2306.03078, 
2023. 
Zhen Dong, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. HAWQ: Hessian 
AWare Quantization of neural networks with mixed precision. In Proc. ICCV, 2019. 
Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, and Dan 
Alistarh. Extreme compression of large language models via additive quantization. In Proc. 
ICML, 2024. 
Steven K. Esser, Jeï¬€rey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and 
Dharmendra S. Modha. Learned step size quantization. In Proc. ICLR, 2019. 
Elias Frantar, and Dan Alistarh. Optimal Brain Compression: A framework for accurate post-training 
quantization and pruning. In Proc. NeurIPS, 2022. 
Elias Frantar, Saleh Ashkboos, Torsten Hoeï¬‚er, and Dan Alistarh. OPTQ: Accurate quantization for 
generative pre-trained transformers. In Proc. ICLR, 2022. 
Allen Gersho, and Robert M. Gray. Vector Quantization and Signal Compression. Kluwer, Norwell, 
MA, USA 1991. 
Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional 
networks using vector quantization. In Proc. ICLR, 2015. 
R.M. Gray, and D.L. Neuhoï¬€. Quantization. IEEE Trans. Inf. Meory, 44(6):2325â€“2383, 1998. 
Ziyi Guan, Hantao Huang, Yupeng Su, Hong Huang, Ngai Wong, and Hao Yu. APTQ: Attention-
aware post-training mixed-precision quantization for large language models. In Proc. DAC, 2024. 
Babak Hassibi, and David Stork. Second order derivatives for network pruning: Optimal Brain 
Surgeon. In Proc. NIPS, 1992. 
Lu Hou, and James T. Kwok. Loss-aware weight quantization of deep networks. In Proc. ICLR, 
2018. 
Wei Huang, Haotong Qin, Yangdong Liu et al. SliM-LLM: Salience-driven mixed-precision 
quantization for large language models, https://arxiv.org/abs/2405.14917v1, 2024. 
Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training 
quantization with small calibration sets. In Proc. ICML, 2021. 
Benoit Jacob, Skirmantas Kligys, Bo Chen et al. Quantization and training of neural networks for 
eï¬ƒcient integer-arithmetic-only inference. In Proc. CVPR, 2018. 
Sehoon Kim et al. SqueezeLLM: Dense-and-sparse quantization. In Proc. ICML, 2024. 
Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park. OWQ: Outlier-aware 
weight quantization for eï¬ƒcient ï¬ne-tuning and inference of large language models. In Proc. 
AAAI, 2024. 
Ji Lin, Jiaming Tang, Haotian Tang et al. AWQ: Activation-aware Weight Quantization for on-device 
LLM compression and acceleration. In Proc. MLSys, 2024. 
S. Lloyd. Least squares quantization in PCM. IEEE Trans. Inf. Meory, 28(2):129â€“137, 1982. 
J. Max. Quantizing for minimum distortion. IRE Trans. Inf. Meory, 6(1):7â€“12, 1960. 
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture 
models. In Proc. ICLR, 2022. 
Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization 
through weight equalization and bias correction. In Proc. CVPR, 2019. 
Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Alex M. 
Bronstein, and Avi Mendelson. Loss aware post-training quantization. In Mach Learn 110 3245â€“
3262, Springer, 2020. 
Jorge Nocedal, and Stephen J. Wright. Numerical Optimization. Springer, New York, NY, USA 2009. 


--- Page 12 ---
Preprint 
 
12 
Biao Qian, Yang Wang, Richang Hong, and Meng Wang. Adaptive data-free quantization. In Proc. 
CVPR, 2023. 
Zhongnan Qu, Zimu Zhou, Yun Cheng, and Lothar Viele. Adaptive loss-aware quantization for 
multi-bit networks. In Proc. CVPR, 2020. 
Colin Raï¬€el, Noam Shazeer, Adam Roberts et al. Exploring the limits of transfer learning with a 
uniï¬ed text-to-text transformer. J. Mach. Learn. Res., 21(140):1â€“67, 2020. 
Wenqi Shao et al. OmniQuant: Omnidirectionally calibrated quantization for large language models. 
In Proc. ICLR, 2024. 
Hugo Touvron, Louis Martin, Kevin Stone et al. Llama 2: Open foundation and ï¬ne-tuned chat 
models. http://arxiv.org/abs/2307.09288, 2023. 
Vincent Vanhoucke, Andrew Senior, and Mark Z. Mao. Improving the speed of neural networks on 
CPUs. In Proc. NIPS Workshops, 2011. 
Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. HAQ: Hardware-aware automated 
quantization with mixed precision. In Proc. CVPR, 2019. 
Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant: 
Accurate and eï¬ƒcient post-training quantization for large language models. In Proc. ICML, 2023. 
Shoukai Xu, Haokun Li, Bohan Zhuang, Jing Liu, Jiezhang Cao, Chuangrun Liang, and Mingkui 
Tan. Generative low-bitwidth data free quantization. In Proc. ECCV, 2020. 
Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. 
ZeroQuant: Eï¬ƒcient and aï¬€ordable post-training quantization for large-scale transformers. In 
Proc. NeurIPS, 2022. 
Sean I. Young, Wang Zhe, David Taubman, and Bernd Girod. Transform quantization for CNN 
compression. IEEE Trans. Pattern Anal. Mach. Intell., 44(9):5700â€“5714, 2019. 
Zhihang Yuan, Yuzhang Shang, Yang Zhou et al. LLM inference unveiled: Survey and rooï¬‚ine 
model insights. https://arxiv.org/abs/2402.16363, 2024. 
Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. LQ-Nets: Learned quantization 
for highly accurate and compact deep neural networks. In Proc. ECCV, 2018. 
Susan Zhang, Stephen Roller, Naman Goyal et al. OPT: Open Pre-trained Transformer Language 
Models. http://arxiv.org/abs/2205.01068, 2022. 
Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network 
quantization without retraining using outlier channel splitting. In Proc. ICML, 2019. 
Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental Network 
Quantization: towards lossless CNNs with low-precision weights. In Proc. ICLR, 2017. 
A DERIVATION OF EQUATION (5) 
To derive our main equation (5), we appeal to a linearized relationship between model weights and 
output, as well as standard results from rateâ€“distortion theory (Gersho & Gray, 1991) that relate the 
quantization error of a random source to output distortion at a high bit depth, where the linearized 
model relationship is a good approximation. Let us start with our quantization objective 
 
ğ‘‘(ğµ1, ğµ2, . . . , ğµğ‘) = ğ”¼ğ—â€–ğ‘“(ğ—, ğš¯1
ğ‘(ğµ1), ğš¯2
ğ‘(ğµ2), . . . , ğš¯ğ‘
ğ‘(ğµğ‘)) âˆ’ğ‘“(ğ—)â€–ğ¹
2 , 
(10) 
in which ğ‘“(ğ—) = ğ‘“(ğ—, ğš¯1(ğµ1), ğš¯2(ğµ2), . . . , ğš¯ğ‘(ğµğ‘)) denotes the output of the unquantized model 
given input ğ—. We can write the residual and Jacobian of ğ‘“ at (ğ—, ğš¯1
ğ‘(ğµ1), ğš¯2
ğ‘(ğµ2), . . . , ğš¯ğ‘
ğ‘(ğµğ‘)) as 
ğ‘Ÿâ€†(ğ—, ğš¯1
ğ‘, ğš¯2
ğ‘, . . . , ğš¯ğ‘
ğ‘)â€Š= (ğ‘Ÿ1, . . . , ğ‘Ÿğ‘€)â€Š(ğ—, ğš¯1
ğ‘, ğš¯2
ğ‘, . . . , ğš¯ğ‘
ğ‘) = ğ‘“(ğ—, ğš¯1
ğ‘, ğš¯2
ğ‘, . . . , ğš¯ğ‘
ğ‘) âˆ’ğ‘“(ğ—) 
(11) 
ğ½(ğ—, ğš¯1
ğ‘, ğš¯2
ğ‘, . . . , ğš¯ğ‘
ğ‘) = (
ğœ•ğ‘“(ğ—, ğš¯1
ğ‘, . . . , ğš¯ğ‘
ğ‘)
ğœ•ğš¯1
, ğœ•ğ‘“(ğ—, ğš¯1
ğ‘, . . . , ğš¯ğ‘
ğ‘)
ğœ•ğš¯2
, . . . , ğœ•ğ‘“(ğ—, ğš¯1
ğ‘, . . . , ğš¯ğ‘
ğ‘)
ğœ•ğš¯ğ‘
) 


--- Page 13 ---
Preprint 
 
13 
and proceed to write the gradient and Hessian of the objective (10) in terms of the ğ‘Ÿ and ğ½above as 
â€† âˆ‡ğ‘‘(ğ—, ğš¯1
ğ‘, ğš¯2
ğ‘, . . . , ğš¯ğ‘
ğ‘) = (ğ½ğ‘‡ğ‘Ÿ)â€†(ğ—, ğš¯1
ğ‘, ğš¯2
ğ‘, . . . , ğš¯ğ‘
ğ‘) 
(12) 
âˆ‡2ğ‘‘(ğ—, ğš¯1
ğ‘, ğš¯2
ğ‘, . . . , ğš¯ğ‘
ğ‘) = (ğ½ğ‘‡ğ½)(ğ—, ğš¯1
ğ‘, ğš¯2
ğ‘, . . . , ğš¯ğ‘
ğ‘) + âˆ‘
(ğ‘Ÿğ‘šâˆ‡2ğ‘Ÿğ‘š)(ğ—, ğš¯1
ğ‘, ğš¯2
ğ‘, . . . , ğš¯ğ‘
ğ‘)
ğ‘€
ğ‘š=1
âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ
â‰ˆ 0
 
in which the second term of âˆ‡2ğ‘“ is approximately zero either because the residuals ğ‘Ÿğ‘š are relatively 
small, or they are close to aï¬ƒne in (ğš«1
ğ‘, ğš«2
ğ‘, . . . , ğš«ğ‘
ğ‘) so that âˆ‡2ğ‘Ÿğ‘š are relatively small, which is the 
case in the vicinity of the solution. 
Using (12), we can now express the local quadratic approximation of (10) about (ğµ1, . . . , ğµğ‘) as 
ğ‘‘Ì‚(ğµ1, . . . , ğµğ‘) =
(a)
ğ”¼ğ—[(ğš«1
ğ‘(ğµ1), . . . , ğš«ğ‘
ğ‘(ğµğ‘))((ğ½ğ‘‡ğ½)(ğ—, ğš¯1
ğ‘, . . . , ğš¯ğ‘
ğ‘))(ğš«1
ğ‘(ğµ1), . . . , ğš«ğ‘
ğ‘(ğµğ‘))
ğ‘‡
] 
 
+ ğ”¼ğ—[(Î”1(ğµ1), . . . , Î”ğ‘(ğµğ‘))
ğ‘‡
((ğ½ğ‘‡ğ‘Ÿ)(ğ—, ğš¯1
ğ‘, ğš¯2
ğ‘, . . . , ğš¯ğ‘
ğ‘))]
âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ
= 0
 
(13) 
=
(b)
âˆ‘
ğ”¼ğ—[(ğ½ğ‘‡ğ½)ğ‘›ğ‘›(ğ—, ğš¯1
ğ‘, . . . , ğš¯ğ‘
ğ‘)]ğ”¼[Î”ğ‘›
2 (ğµğ‘›)]
ğ‘
ğ‘›=1
=
(c)
âˆ‘
ğ‘ƒğ‘›ğºğ‘›
2ğ»ğ‘›ğ‘†ğ‘›
22âˆ’2ğµğ‘›
âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ
= ğ‘‘ğ‘›(ğµğ‘›)
ğ‘
ğ‘›=1
 
 
in which the zero expectation of the linear term in (a) follows from the zero means of quantization 
errors Î”1, . . . , Î”ğ‘, (b) follows from the uncorrelatedness of Î”1, . . . , Î”ğ‘, and (c) follows from our 
deï¬nition of gradient variance ğºğ‘›
2 = ğ‘ƒğ‘›
âˆ’1ğ”¼ğ—[(ğ½ğ‘‡ğ½)ğ‘›ğ‘›(ğ—, ğš¯1
ğ‘, . . . , ğš¯ğ‘
ğ‘)] together with the result from 
rateâ€“distortion theory (Gersho & Gray, 1991) that relates the variance of random quantization error 
ğ”¼[Î”ğ‘›
2 (ğµğ‘›)] = ğ»ğ‘›ğ‘†ğ‘›
22âˆ’2ğµğ‘› to the variance ğ‘†ğ‘›
2 of the random source, and the coeï¬ƒcient ğ»ğ‘›, and bit 
depth ğµğ‘› of quantization. Expression (5) for the partial derivatives of ğ‘‘ with respect to ğµğ‘› follows 
directly from the properties of the derivative of an exponential. 
Since (10) is a non-linear least-squares objective and its gradient depends on the gradient variances 
ğº1
2, ğº2
2, . . . , ğºğ‘
2 , its minimization requires an iterative update of ğš¯1
ğ‘, ğš¯2
ğ‘, . . . , ğš¯ğ‘
ğ‘ via the choice of 
ğµ1, ğµ2, . . . , ğµğ‘ and re-evaluation of the gradient variances ğº1
2, ğº2
2, . . . , ğºğ‘
2  at ğš¯1
ğ‘, ğš¯2
ğ‘, . . . , ğš¯ğ‘
ğ‘. Vis 
is similar to the local Hessian evaluated by the Gaussâ€“Newton method (Nocedal & Wright, 2006) 
every time the descent direction is re-computed. One can think of ğº1
2, ğº2
2, . . . , ğºğ‘
2  as the diagonal 
elements of a non-diagonal Hessian matrix used in e.g. the Gaussâ€“Newton method, but whose oï¬€-
diagonal elements disappear in the expectation due to multiplication by uncorrelated quantization 
errors ğš«1
ğ‘, . . . , ğš«ğ‘
ğ‘.  
B 
DERIVATION OF EQUATION (8) 
To derive our sigmoid companding function (8), we turn to results from rateâ€“distortion theory that 
relate the mean square error of quantization of weights ğœƒ to the density ğ‘ğœƒ of ğœƒ and the density ğœ†(ğœƒ) 
of quantization levels, where 2ğµâˆ«ğœ†(ğœƒ) dğœƒ
ğ‘
ğ‘
 expresses the number of quantization levels of a ğµ-bit 
quantizer within any interval [ğ‘, ğ‘]. Writing Î ğ‘– for the ğ‘–th quantization cell and Î (ğœƒ) for the width 
of the cell containing ğœƒ, we can write the mean square error of quantized weights as 
 
ğ”¼|ğœƒâˆ’ğœƒğ‘|2 = âˆ‘
â„™[ğœƒâˆˆÎ ğ‘–]
2ğµ
ğ‘–=1
ğ”¼[|ğœƒâˆ’ğœƒğ‘–
ğ‘|2â€†|â€†ğœƒâˆˆÎ ğ‘–] 
(14) 
 â‰ˆ
(a)
âˆ‘
â„™[ğœƒâˆˆÎ ğ‘–] |Î ğ‘–|2
12
2ğµ
ğ‘–=1
â‰ˆ
(b)
âˆ«ğ‘ğœƒ(ğœƒ) Î 2(ğœƒ)
12
dğœƒ 
 â‰ˆ
(c) 1
22ğµâˆ«ğ‘ğœƒ(ğœƒ) ğœ†âˆ’2(ğœƒ)
12
dğœƒ 
in which (a) follows from our assumption that weight distribution is approximately uniform within 
each quantization cell, (b) follows from an integral approximation of the ï¬nite sum, and (c) follows 
from the relationship 2ğµğœ†âˆ’1(ğœƒ) = Î (ğœƒ), all of which hold approximately when ğµ is suï¬ƒciently large. 


--- Page 14 ---
Preprint 
 
14 
To ï¬nd the density ğœ† of quantization levels that leads to the minimum quantization error when ğœƒ has 
density ğ‘ğœƒ, we use HÃ¶lderâ€™s inequality: âˆ«ğ‘ğœƒ
1/3 â‰¤(âˆ«ğ‘ğœƒğœ†âˆ’2)1/3(âˆ«ğœ†)2/3. Since âˆ«ğœ†= 1, we have that 
âˆ«ğ‘ğœƒğœ†âˆ’2 â‰¥(âˆ«ğ‘ğœƒ
1/3)3, which sets a lower bound on the last term of (14). Vis lower bound and hence 
minimum quantization error is attained iï¬€ ğ‘ğœƒğœ†âˆ’2 âˆğœ†. Ve optimal density for quantization levels is 
therefore given by 
 
ğœ†(ğœƒ) âˆğ‘ğœƒ
1/3(ğœƒ) âŸºÎ âˆ’1(ğœƒ) âˆğ‘ğœƒ
1/3(ğœƒ). 
(15) 
Rather than optimize the density ğœ† to minimize the quantization error for a given ğ‘ğœƒ, we could 
equivalently transform the weights ğœƒ as ğœƒğœ= ğœ(ğœƒ) via a non-linear ğœ, so that uniform quantization 
applied to ğœƒğœâˆ¼ğ‘ğœƒğœ leads to the same minimum quantization error. Ve width Î (ğœƒ) of non-uniform 
quantization cells quantizing ğœƒ relate to the width of uniform quantization cells of the companded 
(transformed) weights ğœƒğœ= ğœ(ğœƒ) as 
 
dğœ(ğœƒ) = dğœƒ
Î (ğœƒ) âˆğ‘ğœƒ
1/3(ğœƒ)â€ŠdğœƒâŸ¹ğœâ€²(ğœƒ) âˆğ‘ğœƒ
1/3(ğœƒ), 
(16) 
in which the ï¬rst proportionality follows from (15). We can ï¬nd the optimal nonlinear transform ğœ 
by integrating ğ‘ğœƒ
1/3(ğœƒ) and normalizing (for convenience) the range of the integral to [0, 1]: 
 
ğœ(ğœƒ) = (âˆ«ğ‘ğœƒ
1/3(ğ‘¡) dğ‘¡
âˆ
âˆ’âˆ
)
âˆ’1
( âˆ«ğ‘ğœƒ
1/3(ğ‘¡) dğ‘¡
ğœƒ
âˆ’âˆ
) 
(17) 
(Gersho & Gray, 1991). Finally, we obtain (8) by substituting the expression for the density of a 
Laplace distribution (parameterized by mean ğœ‡ and standard deviation ğ‘†) into ğ‘! above. Transform 
ğœ is asymptotically optimal as ğµâ†’âˆ in (14). 
C 
CONVEX WEIGHT PRUNING (HASSABI & STORK, 1992) 
To facilitate comparison between convex weight quantization (this work) and the convex weight 
pruning work of Hassabi & Stork (1992), we provide a derivation of Hassabi & Storkâ€™s Optimum 
Brain Surgeon (OBS) algorithm (presented slightly diï¬€erently), together with our commentary for 
additional clariï¬cation. 
For simplicity, let us rewrite model (4) as ğ‘“( â‹… , ğš¯1, ğš¯2, . . . , ğš¯ğ‘) = ğ‘“( â‹… , ğš¯), where ğš¯ is a vector of 
all model weights across diï¬€erent layers of the model. Ve objective of convex weight pruning is to 
set some number of elements of ğš¯ to zero while ï¬ne-tuning the remaining elements to minimize the 
diï¬€erence between the output of the pruned model ğ‘“( â‹… , ğš¯ğ‘) and the output of the unpruned model 
ğ‘“( â‹… , ğš¯). Writing the pruned weights as ğš¯ğ‘= ğš¯+ ğš«ğ‘, where ğš«ğ‘ is a vector of updates to be made 
to weights ğš¯, it is apparent that ğ›¥ğ‘–
ğ‘= âˆ’ğœƒğ‘– if the ğ‘–th weight is to be pruned, otherwise ğ›¥ğ‘–
ğ‘ should be 
chosen to maximally compensate for the eï¬€ect of other pruned weights on the output. Suppose we 
have decided to prune the ğ‘th element of ğš¯. Ve updated set of weights ğš¯ğ‘ can be found by solving 
 minimize  ğ‘‘(ğš«ğ‘) = ğ”¼ğ—â€–ğ‘“(ğ—, ğš¯+ ğš«ğ‘) âˆ’ğ‘“(ğ—)â€–2
2 â‰ˆğ”¼ğ—[ğš«ğ‘ğ‘‡(ğ½ğ‘‡ğ½)(ğ—, ğš¯)ğš«ğ‘] 
(18) 
 subject to  ğ‘Ÿ(ğš«ğ‘)â€Š= ğğ‘
ğ‘‡ğš«ğ‘âˆ’ğœƒğ‘= 0 
in which ğ½(ğ—, ğš¯) represents the Jacobian of ğ‘“(ğ—, ğš¯) with respect to ğš¯, and ğğ‘
ğ‘‡ is an operator that 
picks out the ğ‘th element of a vector. Ve Lagrangian of this problem becomes 
 
â„’(ğš«ğ‘, Î») = 1
2 ğ”¼ğ—[ğš«ğ‘ğ‘‡(ğ½ğ‘‡ğ½)(ğ—, ğš¯)ğš«ğ‘] + Î»(ğğ‘
ğ‘‡ğš«ğ‘âˆ’ğœƒğ‘) 
(19) 
in which Î» represents the dual variable associated with the equality constraint ğğ‘
ğ‘‡ğš«ğ‘âˆ’ğœƒğ‘= 0.  
To solve (18), we diï¬€erentiate â„’ with respect to ğš«ğ‘, Î» and set all obtained derivatives equal to 0 to 
obtain the ï¬rst-order optimality conditions ğ”¼ğ—[(ğ½ğ‘‡ğ½)(ğ—, ğš¯)]ğš«ğ‘+ ğğ‘Î» = ğŸ and ğğ‘
ğ‘‡ğš«ğ‘âˆ’ğœƒğ‘= 0. After  
some algebraic manipulations, we obtain the optimizing values 
 
ğš«ğ‘= âˆ’ğ”¼ğ—[(ğ½ğ‘‡ğ½)(ğ—, ğš¯)]
âˆ’1ğğ‘Î»,
Î» = âˆ’
ğœƒğ‘
ğ”¼ğ—[(ğ½ğ‘‡ğ½)(ğ—, ğš¯)]ğ‘ğ‘
âˆ’1, 
(20) 


--- Page 15 ---
Preprint 
 
15 
in which the expression for Î» is obtained by substituting the expression for ğš«ğ‘ above into the second 
optimality condition ğğ‘
ğ‘‡ğš«ğ‘âˆ’ğœƒğ‘= 0 and solving for Î». Combining both expressions ï¬nally produces 
an update ğš«ğ‘ that minimizes the objective in (18):  
 ğš«ğ‘= âˆ’
ğœƒğ‘
ğ”¼ğ—[(ğ½ğ‘‡ğ½)(ğ—, ğš¯)]ğ‘ğ‘
âˆ’1 ğ”¼ğ—[(ğ½ğ‘‡ğ½)(ğ—, ğš¯)]
âˆ’1ğğ‘,
ğ‘‘(ğš«ğ‘) = 1
2
ğœƒğ‘
2
ğ”¼ğ—[(ğ½ğ‘‡ğ½)(ğ—, ğš¯)]ğ‘ğ‘
âˆ’1. (21) 
So far, we assumed that we were given the index ğ‘ of the weight to prune from ğš¯. To actually pick 
the best weights to prune away, we can compute the pruning loss ğ‘‘(ğš«ğ‘–) for all indices ğ‘–, picking the 
index ğ‘– associated with minimum loss. Vat is, 
 
ğ‘= argmin
ğ‘–
1
2
ğœƒğ‘–
2
ğ”¼ğ—[(ğ½ğ‘‡ğ½)(ğ—, ğš¯)]ğ‘–ğ‘–
âˆ’1, 
(22) 
after which ğš«ğ‘ can be calculated (and consequently ğš¯ğ‘), and further weights to prune can be picked 
by initializing ğš¯â†ğš¯ğ‘ and repeating the process until some pruning criterion has been met. 
D ALGORITHM PARAMETERS 
To aid the reproducibility of the results in Table 1, we document the code we used for all algorithms 
(RTN, GPTQ, OWQ, and AWQ) along with the command line arguments. 
RTN. We use the OWQ code from https://github.com/xvyaward/owq/tree/03cfc99 in 
the provided owq conda environment. In the case of e.g. Llama-2-7b-hf quantized to 3 bits, we run 
python main.py meta-llama/Llama-2-7b-hf c4 --wbits 3 --nearest. 
GPTQ. We use the OWQ code from https://github.com/xvyaward/owq/tree/03cfc99 in 
the provided owq conda environment. In the case of e.g. Llama-2-7b-hf quantized to 3 bits, we run 
the provided command python main.py meta-llama/Llama-2-7b-hf c4 --wbits 3. For 
results based on the group size of 256, we run python main.py meta-llama/Llama-2-7b-hf 
c4 --wbits 3 â€“groupsize 256. 
OWQ. We use the OWQ code from https://github.com/xvyaward/owq/tree/03cfc99 in 
the provided owq conda environment. In the case of e.g. Llama-2-7b-hf quantized to 3.01 bits, we 
run the provided command python main.py meta-llama/Llama-7b-hf c4 --wbits 3 --
target_bit 3.01. 
AWQ. We use the AWQ code https://github.com/mit-han-lab/llm-awq/tree/3665e1a 
in the provided awq conda environment. In the case of e.g. Llama-2-7b-hf quantized to 3 bits, we 
run the provided command python -m awq.entry â€“model_path meta-llama/Llama-7b-
hf --w_bit 3 --q_group_size 128 --run_awq --tasks wikitext. 
E 
OUTPUT PRODUCED BY DIFFERENT QUANTIZED MODELS 
Table 5 lists output produced by diï¬€erent quantized Llama-2-70b models in response to questions 
taken from the GSM8K dataset. For each question, a prompt is created by prepending the question 
text with ï¬ve other question and target pairs from the dataset (known as a 5-shot evaluation). Vis 
allows the model to establish a context for the required output and format. It is interesting to note 
that severe quantization errors (as in the case of RTN) manifest as non sequiturs and errors in logic 
rather than unintelligible output.


--- Page 16 ---
Preprint 
 
16 
Table 5: Output to questions from the GSM8K dataset (test). Scores shown parentheses are computed across 1319 answers obtained using the ï¬‚exible extract ï¬lter.  
Question 
Target 
FP16 (Score: 53.90%) 
RTN (Score: 6.14%) 
GPTQ (Score: 46.47%) 
CVXQ (Score: 48.60%) 
Question: Every day, Wendi 
feeds each of her chickens 
three cups of mixed chicken 
feed, containing seeds, 
mealworms and vegetables to 
help keep them healthy.  She 
gives the chickens their feed 
in three separate meals. In the 
morning, she gives her ï¬‚ock 
of chickens 15 cups of feed.  
In the afternoon, she gives 
her chickens another 25 cups 
of feed.  How many cups of 
feed does she need to give 
her chickens in the ï¬nal meal 
of the day if the size of 
Wendiâ€™s ï¬‚ock is 20 
chickens?\nAnswer: 
 
If each chicken eats 3 cups of 
feed per day, then for 20 
chickens they would need 
3*20=<<3*20=60>>60 cups 
of feed per day.\nIf she feeds 
the ï¬‚ock 15 cups of feed in 
the morning, and 25 cups in 
the afternoon, then the ï¬nal 
meal would require 60-15-
25=<<60-15-25=20>>20 
cups of chicken feed.\n#### 
20 
 
Wendi feeds each chicken 3 
cups of feed per day.\nShe 
feeds her ï¬‚ock 15 cups of 
feed in the morning and 25 
cups of feed in the 
afternoon.\nie total amount 
of feed she gives her ï¬‚ock is 
15 cups + 25 cups = 
<<15+25=40>>40 
cups.\nSince each chicken 
gets 3 cups of feed per day, 
the total number of chickens 
in her ï¬‚ock is 40 cups / 3 
cups = 
<<40/3=13.33333333333333
33333333333333333333333
33333333333333333333333
33333333333333333333333
33333333333333333333333
3333333333333333333 
15 cups + 25 cups + 10 cups 
= <<15+25+10=50>>50 cups 
of feed.\n#### 50\n 
15 cups in the morning + 25 
cups in the afternoon = 
<<15+25=40>>40 cups of 
feed\nEach chicken gets 3 
cups of feed, so 40 cups of 
feed / 3 cups per chicken = 
<<40/3=13.33>>13.33 
chickens\nSince there are 20 
chickens, the number of cups 
of feed in the ï¬nal meal is 20 
- 13.33 = <<20-
13.33=6.67>>6.67 
cups\n#### 6.67\n\n 
15 cups + 25 cups = 
<<15+25=40>>40 cups of 
feed\n20 chickens x 3 cups = 
<<20*3=60>>60 cups of 
feed\n60 cups - 40 cups = 
<<60-40=20>>20 cups of 
feed\n#### 20\n\n 
Question: A robe takes 2 
bolts of blue ï¬ber and half 
that much white ï¬ber.  How 
many bolts in total does it 
take?\nAnswer: 
It takes 2/2=<<2/2=1>>1 bolt 
of white ï¬ber\nSo the total 
amount of fabric is 
2+1=<<2+1=3>>3 bolts of 
fabric\n#### 3 
2 + 2/2 = <<2+2/2=3>>3 
bolts\n#### 3\n\n 
2 blue bolts + 2 white bolts = 
<<2+2=4>>4 bolts in 
total.\n#### 4\np 
2 + 1 = <<2+1=3>>3 
bolts\n#### 3\n\n 
2 + 2/2 = <<2+2/2=3>>3 
bolts\n#### 3\n\n 
Question: Josh decides to try 
ï¬‚ipping a house.  He buys a 
house for $80,000 and then 
puts in $50,000 in repairs.  
iis increased the value of 
the house by 150%.  How 
much proï¬t did he 
make?\nAnswer: 
ie cost of the house and 
repairs came out to 
80,000+50,000=$<<80000+5
0000=130000>>130,000\nHe 
increased the value of the 
house by 
80,000*1.5=<<80000*1.5=1
20000>>120,000\nSo the 
new value of the house is 
120,000+80,000=$<<120000
+80000=200000>>200,000\n
So he made a proï¬t of 
200,000-
130,000=$<<200000-
130000=70000>>70,000\n##
## 70000 
ie house is now worth 
150% of $80,000, which is 
$80,000 * 150/100 = 
<<80000*150/100=120000>
>$120,000.\nSo he made 
$120,000 - $80,000 - $50,000 
= <<120000-80000-
50000=20000>>$20,000.\n##
## 20000\n\n 
Josh bought the house for 
$80,000 and spent $50,000 
on repairs. So he spent 
$80,000 + $50,000 = 
$<<80,000+50,000=130,000
>>$130,000 on the 
house.\nSo Josh spent 
$130,000 on the 
house.\n#### $130,000\n 
Josh bought the house for 
$80,000 and put in $50,000 
in repairs.\nie value of the 
house increased by 150%, so 
the new value is 
1.5*$80,000+$50,000 = 
$<<1.5*80,000+50,000=180,
000>>180,000.\nSo Josh 
made $180,000 - $80,000 - 
$50,000 = $<<180,000-
80,000-
50,000=50,000>>50,000 in 
proï¬t.\n#### 50,000\n\n 
ie value of the house 
increased by 150%, so itâ€™s 
now 150% of $80,000, which 
is $120,000.\nJosh spent 
$80,000 + $50,000 = 
<<80000+50000=130000>>$
130,000 on the house, so he 
made $120,000 - $130,000 = 
<<120000-130000=-
10000>>-$10,000.\n#### -
10000\n\n 


--- Page 17 ---
Preprint 
 
17 
Kylar went to the store to buy 
glasses for his new 
apartment. One glass costs 
$5, but every second glass 
costs only 60% of the price. 
Kylar wants to buy 16 
glasses. How much does he 
need to pay for them?", 
"answer 
ie discount price of one 
glass is 60/100 * 5 = 
$<<60/100*5=3>>3.\nIf 
every second glass is cheaper, 
that means Kylar is going to 
buy 16 / 2 = <<16/2=8>>8 
cheaper glasses.\nSo for the 
cheaper glasses, Kylar is 
going to pay 8 * 3 = 
$<<8*3=24>>24.\nAnd for 
the regular-priced glasses, 
Kylar will pay 8 * 5 = 
$<<8*5=40>>40.\nSo in total 
Kylar needs to pay 24 + 40 = 
$<<24+40=64>>64 for the 
glasses he wants to 
buy.\n#### 64 
ie ï¬rst glass costs $5.\nie 
second glass costs $5 * 60% 
= $<<5*60%=3>>3.\nie 
third glass costs $5.\nie 
fourth glass costs $5 * 60% = 
$<<5*60%=3>>3.\nie ï¬fth 
glass costs $5.\nie sixth 
glass costs $5 * 60% = 
$<<5*60%=3>>3.\nie 
seventh glass costs $5.\nie 
eighth glass costs $5 * 60% = 
$<<5*60%=3>>3.\nie ninth 
glass costs $5.\nie tenth 
glass costs $5 * 60% = 
$<<5*60%=3>>3.\nie 
eleventh glass costs $5.\nie 
twelfth glass costs $5 * 60% 
= $<<5*60%=3>>3.\nie 
thirteenth glass costs 
$5.\nie fourteenth glass 
costs $5 * 60% = 
$<<5*60%=3>>3.\nie 
ï¬fteenth glass costs $5.\nie 
sixteenth glass costs $5 * 
60% 
ie cost of one glass is $5, 
and the cost of the second 
glass is $5 * 
$<<5*2=10>>10>>10 is the 
cost of the third glass.\nie 
cost of the fourth glass is $5 
* $<<5*3=15>>15 is the cost 
of the ï¬fth glass.\nie cost of 
the sixth glass is $5 * 
$<<5*4=20>>20 is the cost 
of the seventh glass.\nie 
cost of the eighth glass is $5 
* $<<5*5=25>>25 is the cost 
of the ninth glass.\nie cost 
of the tenth glass is $5 * 
$<<5*6=30>>30 is the cost 
of the eleventh glass.\nie 
cost of the twelfth glass is $5 
* $<<5*7=35>>35 is the cost 
of the thirteenth glass.\nie 
cost of the fourteenth glass is 
$5 * $<<5*8=40>>40 is the 
cost of the ï¬fteenth 
glass.\nie cost of the 
sixteenth glass is $5 * 
$<<5*9=45>>4 
ie ï¬rst glass costs $5.\nie 
second glass costs $5 * 60% 
= $<<5*60%=3>>3.\nie 
third glass costs $5 * 60% * 
60% = 
$<<5*60%*60%=1.8>>1.8.\
nie fourth glass costs $5 * 
60% * 60% * 60% = 
$<<5*60%*60%*60%=1.08
>>1.08.\nie ï¬fth glass costs 
$5 * 60% * 60% * 60% * 
60% = 
$<<5*60%*60%*60%*60%
=0.648>>0.648.\nie sixth 
glass costs $5 * 60% * 60% * 
60% * 60% * 60% = 
$<<5*60%*60%*60%*60%*
60%=0.3888>>0.3888.\nie 
seventh glass costs $5 * 6 
ie ï¬rst glass costs $5.\nie 
second glass costs 60% of $5, 
which is 
$<<5*60%=3>>3.\nie third 
glass costs 60% of $5, which 
is $<<5*60%=3>>3.\nie 
fourth glass costs 60% of $5, 
which is 
$<<5*60%=3>>3.\nie ï¬fth 
glass costs 60% of $5, which 
is $<<5*60%=3>>3.\nie 
sixth glass costs 60% of $5, 
which is 
$<<5*60%=3>>3.\nie 
seventh glass costs 60% of 
$5, which is 
$<<5*60%=3>>3.\nie 
eighth glass costs 60% of $5, 
which is 
$<<5*60%=3>>3.\nie ninth 
glass costs 60% of $5, which 
is $<<5*60%=3>>3.\nie 
tenth glass costs 60% of $5, 
which is 
$<<5*60%=3>>3.\nie 
eleventh glass costs 60% 
Question: Janetâ€™s ducks lay 
16 eggs per day. She eats 
three for breakfast every 
morning and bakes muï¬ƒns 
for her friends every day with 
four. She sells the remainder 
at the farmersâ€™ market daily 
for $2 per fresh duck egg. 
How much in dollars does 
she make every day at the 
farmersâ€™ market?\nAnswer: 
Janet sells 16 - 3 - 4 = <<16-
3-4=9>>9 duck eggs a 
day.\nShe makes 9 * 2 = 
$<<9*2=18>>18 every day at 
the farmerâ€™s market.\n#### 
18 
Janet eats 3 eggs for 
breakfast and bakes 4 
muï¬ƒns, so she uses 3 + 4 = 
<<3+4=7>>7 eggs.\nShe 
sells the remainder, so she 
sells 16 - 7 = <<16-7=9>>9 
eggs.\nShe sells each egg for 
$2, so she makes $2 * 9 = 
<<2*9=18>>18 
dollars.\n#### 18\n\n 
Janet sold 16 eggs per day. 
She sold three eggs for 
breakfast every morning, so 
she sold 16 - 3 = <<16-
3=13>>13 eggs for 
breakfast.\nShe sold four 
eggs for her friends every 
day, so she sold 16 - 4 = 
<<16-4=12>>12 eggs for her 
friends.\nShe sold twelve 
eggs at the farmersâ€™ market 
daily, so she sold twelve eggs 
at the farmersâ€™ market daily, 
so she sold twelve eggs at the 
farmersâ€™ market daily, so she 
sold twelve eggs at the 
farmersâ€™ market daily, so she 
sold twelve eggs at 
Janet eats 3 eggs and bakes 4, 
so she sells 16 - 3 - 4 = <<16-
3-4=9>>9 eggs.\nShe sells 9 
eggs for $2 each, so she 
makes $2 * 9 = 
<<2*9=18>>18 
dollars.\n#### 18\n\n 
Janet eats 3 eggs for 
breakfast and bakes 4 
muï¬ƒns, so she uses 3 + 4 = 
<<3+4=7>>7 eggs.\nShe 
sells the remainder at $2 per 
egg, so she makes $2 * (16 - 
7) = <<2*(16-
7)=2*9=18>>$18 per 
day.\n#### 18\n\n 
 
