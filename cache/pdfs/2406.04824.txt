--- Page 1 ---
FunBO: Discovering Acquisition Functions for
Bayesian Optimization with FunSearch
Virginia Aglietti∗, Ira Ktena, Jessica Schrouff, Eleni Sgouritsa,
Francisco J. R. Ruiz, Alan Malek, Alexis Bellot, Silvia Chiappa
Google DeepMind
Abstract
The sample efficiency of Bayesian optimization algorithms depends on carefully
crafted acquisition functions (AFs) guiding the sequential collection of function
evaluations. The best-performing AF can vary significantly across optimization
problems, often requiring ad-hoc and problem-specific choices. This work tackles
the challenge of designing novel AFs that perform well across a variety of experi-
mental settings. Based on FunSearch, a recent work using Large Language Models
(LLMs) for discovery in mathematical sciences, we propose FunBO, an LLM-based
method that can be used to learn new AFs written in computer code by leveraging
access to a limited number of evaluations for a set of objective functions. We
provide the analytic expression of all discovered AFs and evaluate them on various
global optimization benchmarks and hyperparameter optimization tasks. We show
how FunBO identifies AFs that generalize well in and out of the training distribution
of functions, thus outperforming established general-purpose AFs and achieving
competitive performance against AFs that are customized to specific function types
and are learned via transfer-learning algorithms.
1
Introduction
Bayesian optimization (BO) [16, 25] is a powerful methodology for optimizing complex and expensive-
to-evaluate black-box functions which emerge in many scientific disciplines. BO has been used across
a large variety of applications ranging from hyperparameter tuning in machine learning [1, 32, 8] to
designing policies in robotics [2] and recommending new molecules in drug design [17]. Two main
components lie at the heart of any BO algorithm: a surrogate model and an acquisition function (AF).
The surrogate model expresses assumptions about the objective function, e.g., its smoothness, and it
is often given by a Gaussian Process (GP) [29]. Based on the surrogate model, the AF determines the
sequential collection of function evaluations by assigning a score to potential observation locations.
BO’s success heavily depends on the AF’s ability to efficiently balance exploitation (i.e. assigning a
high score to locations that are likely to yield optimal function values) and exploration (i.e. assigning
a high score to regions with higher uncertainty about the objective function in order to inform future
decisions), thus leading to the identification of the optimum with the minimum number of evaluations.
Existing AFs aim to provide either general-purpose optimization strategies or approaches tailored to
specific objective types. For example, Expected Improvement (EI) [25], Upper Confidence Bound
(UCB) [19] and Probability of Improvement (PofI) [18] are all widely adopted general-purpose AFs
that can be used out-of-the-box across BO algorithms and objective functions. The performance
of these AFs varies significantly across different types of black-box functions, making the AF
choice an ad-hoc, empirically driven, decision. There exists an extensive literature on alternative
AFs outperforming EI, UCB and PofI, for instance entropy-based [42] or knowledge-gradient [12]
∗Corresponding author, aglietti@google.com.
Preprint. Under review.
arXiv:2406.04824v2  [cs.LG]  1 Jul 2024


--- Page 2 ---
optimizers, see Garnett [13, Chapter 7] for a review. However, these are generally hard to implement
and expensive to evaluate, partly defeating the purpose of replacing the expensive original optimization
with the optimization of a much cheaper and faster to evaluate AF. Other prior works [14, 40, 43] have
instead proposed learning new AFs tailored to specific objectives by transferring information from a set
of related functions with a given training distribution via, e.g., reinforcement learning or transformers.
While such learned AFs can outperform general-purpose AFs, their generalization performance to
objectives outside of the training distribution is often poor (see experimental section and discussion
on generalization behaviour in Volpp et al. [40]). Defining methodologies that automatically identify
new AFs capable of outperforming general-purpose and function-specific alternatives, both in and out
of the training distribution, remains a significant and unaddressed challenge.
Contributions. We tackle this challenge by formulating the problem of learning novel AFs as
an algorithm discovery problem and address it by extending FunSearch [30], a recently proposed
algorithm that uses LLMs to solve open problems in mathematical sciences. In particular, we
introduce FunBO, a novel method that explores the space of AFs written in computer code. FunBO
takes an initial AF as input and, with a limited number of evaluations for a set of objective functions,
iteratively modifies the AF to improve the performance of the resulting BO algorithm. Unlike existing
algorithms, FunBO outputs code snippets corresponding to improved AFs, which can be inspected to
(i) identify differences with respect to known AFs and (ii) investigate the reasons for their observed
performance, thereby enforcing interpretability, and (iii) be easily deployed in practice without
additional infrastructure overhead. We extensively test FunBO on a range of optimization problems
including standard global optimization benchmarks and hyperparameter optimization (HPO) tasks.
For each experiment, we report the explicit functional form of the discovered AFs and show that
they generalize well to the optimization of functions both in and out of the training distribution,
outperforming general-purpose AFs while comparing favorably to function-specific ones. To the
best of our knowledge, this is the first work exploring AFs represented in computer code, thus
demonstrating a novel approach to harness the power of LLMs for sampling policy design.
2
Preliminaries
We consider an expensive-to-evaluate black-box function f : X →R over the input space X ⊆Rd
for which we aim at identifying the global minimum x∗= arg minx∈X f(x). We assume access
to a set of auxiliary black-box and expensive-to-evaluate objective functions, G = {gj}J
j=1, with
gj : Xj →R, Xj ⊆Rdj for j = 1, . . . , J, from which we can obtain a set of evaluations.
Bayesian optimization. BO seeks to identify x∗with the smallest number T of sequential evaluations
of f given N initial observations D = {xi, yi}N
i=1, with yi = f(xi).2 BO relies on a probabilistic
surrogate model for f which in this work is set to a GP with prior distribution over any batch of input
points X = {x1, . . . , xN} given by p(f|X) = N(m(X), Kθ(X, X′)) with prior mean m(X)
and kernel Kθ(X, X′) with hyperparameters θ. The posterior distribution p(f|D) is available in
closed form via standard GP updates. At every step t in the optimization process, BO selects the next
evaluation location by optimizing an AF α(·|Dt) : X →R, given the current posterior distribution
p(f|Dt), with Dt denoting the function evaluations collected up to trial t (including D). A commonly
used AF is the Expected Improvement (EI), which is defined as αEI(x|Dt) = (y∗−m(x|Dt))Φ(z) +
σ(x|Dt)ϕ(z), where y∗denotes the best function value observed in Dt, also called incumbent,
z = (y∗−m(x|Dt))/σ(x|Dt), ϕ and Φ are the standard Normal density and distribution functions,
and m(x|Dt) and σ(x|Dt) are the GP posterior mean and standard deviation computed at x ∈X.
Other general-purpose AFs proposed in the literature are: UCB (αUCB(x|Dt) = m(x|Dt) −βσ(x|Dt)
with hyperparameter β), PofI (αPofI(x|Dt) = Φ((y∗−m(x|Dt))/σ(x|Dt))) and the posterior mean
αMEAN(x|Dt) = m(x|Dt) (denoted by MEAN hereinafter).3
Unlike general-purpose AFs, several works have proposed increasing the efficiency of BO for a specific
optimization problem, say the optimization of f, by learning problem-specific AFs [14, 40, 43]. These
AFs are trained on the set G, whose functions are assumed to be drawn from the same distribution
or function class associated to f, reflecting a meta-learning setup. “Function class” here refers to
a set of functions with a shared structure and obtained by, e.g., applying scaling and translation
2We focus on noiseless observations but the method can be equivalently applied to noisy outcomes.
3We focus on AFs that can be evaluated in closed form given the posterior parameters of a GP surrogate
model and exclude those whose computation involve approximations, e.g., Monte-Carlo sampling.
2


--- Page 3 ---
Inputs: GTr, GV, NDB, B, T
Setup: Initialize h (Left), e (Fig. 9-10) and DB with NDB islands.
Assign h to each island.
while τ < T do
1. Sample two programs from DB and create prompt (Fig. 2)
2. Get a batch of B samples from the LLM
3. For each correct hτ in the batch compute shτ (GTr)
4. Add correct hτ to DB and update it (see Appendix B)
5. Update step τ = τ + 1
end
Output: Return h in DB with score in the top 20th percentile for
GTr and highest score on GV.
Figure 1: Left: The FunBO algorithm. Right: Graphical representation of FunBO. The different
FunBO component w.r.t. FunSearch [30, Fig. 1] are highlighted in color.)
transformations to their input and output values or evaluating the loss function of the same machine
learning model, e.g., AdaBoost, on different data sets. For instance, Wistuba et al. [44] learns an AF
that is a weighted superposition of EIs by exploiting access to a sufficiently large dataset for functions
in G. Volpp et al. [40] considered settings where the observations for functions in G are limited and
proposed MetaBO, a reinforcement learning based algorithm that learns a specialized neural AF, i.e.,
a neural network representing the AF. The neural AF takes as inputs a set of potential locations (with
a given d), the posterior mean and variance at those points, the trial t and the budget T and is trained
using a proximal policy optimization algorithm [31]. Similarly, Hsieh et al. [14] proposed FSAF, an
AF obtained via few-shot adaptation of a learned AF using a small number of function instances in G.
Note that, while general-purpose AFs are used to optimize objectives across function classes, learned
AFs aim at achieving high performance for the single function class to which f and G belong.
FunSearch.
FunSearch [30] is a recently proposed evolutionary algorithm for searching in the
functional space by combining a pre-trained LLM used for generating new computer programs with
an efficient evaluator, which guards against hallucinations and scores fitness. An example problem
that FunSearch tackles is the online bin packing problem [9], where a set of items of various sizes
arriving online needs to be packed into the smallest possible number of fixed sized bins. A set of
heuristics have been designed for deciding which bin to assign an incoming item to, e.g., “first fit.”
FunSearch aims at discovering new heuristics that improve on existing ones by taking as inputs:
(i) the computer code of an evolve function h(·) representing the initial heuristic to be improved
by the LLM, e.g., “first fit” and (ii) an evaluate function e(h, ·), also written in computer code,
specifying the problem at hand (also called “problem specification”) and scoring each h(·) according
to a predefined performance metric, e.g., the number of bins used in h(·). The inputs of both h(·)
(denoted by h hereinafter) and e(h, ·) (denoted by e hereinafter), are problem specific. A description
of h’s inputs is provided in the function’s docstring4 together with an explanation of how the function
itself is used within e. Given these initial components, FunSearch prompts an LLM to propose an
improved h, scores the proposals on a set of inputs, e.g., on different bin-packing instances, and
adds them to a programs database. The programs database stores correct h functions5 together
with their respective scores. In order to encourage diversity of programs and enable exploration of
different solutions, a population-based approach inspired by genetic algorithms [36] is adopted for
the programs database (DB). At a subsequent step, functions in the database are sampled to create
a new prompt, LLM’s proposals are scored and stored again. The process repeats for τ = 1, . . . , T
until a time budget T is reached and the heuristic with the highest score on a set of inputs is returned.
3
FunBO
FunBO is a FunSearch-based method for discovering novel AFs that increase BO efficiency by exploit-
ing the set of auxiliary objectives G. In particular, FunBO (i) uses the same prompt and DB structure
as FunSearch, but (ii) proposes a new problem specification by viewing the learning of AFs as a
4We focus on Python programs.
5The definition of a correct function is also problem specific. For instance, a program can be considered
correct if it compiles.
3


--- Page 4 ---
def acquisition_function(predictive_mean , predictive_var , incumbent , beta=1.0):
"""Returns the index of the point to collect ... (Full docstring in Fig. 8)."""
z = (incumbent −predictive_mean) / np.sqrt(predictive_var)
predictive_std = np.sqrt(predictive_var)
vals = (incumbent −predictive_mean) * stats.norm.cdf(z) + predictive_std * stats.norm.pdf(z)
return np.argmax(vals)
"""Improve Bayesian Optimization by discovering a new acquisition function."""
def acquisition_function_v0(predictive_mean , predictive_var , incumbent , beta=1.0):
"""Returns the index of the point to collect ... (Full docstring in Fig. 8)"""
# Code for lowest−scoring sampled AF.
return ...
def acquisition_function_v1(predictive_mean , predictive_var , incumbent , beta=1.0):
"""Improved version of ‘acquisition_function_v0 ‘."""
# Code for highest−scoring sampled AF.
return ...
def acquisition_function_v2(predictive_mean , predictive_var , incumbent , beta=1.0):
"""Improved version of the previous ‘acquisition_function ‘."""
Figure 2: Top: FunBO’s initial AF takes the functional form of EI with inputs given by the posterior
parameter of the GP at a set of potential sample locations, the incumbent and a parameter β = 1.
Bottom: FunBO prompt includes two previously generated AFs which are sampled from DB and
are sorted in ascending order based on the score achieved on GTr. The LLM generates a third AF,
acquisition_function_v2, representing an improved version of the highest scoring program.
algorithm discovery problem, and (iii) introduces a novel initialization and evaluation pipeline that is
used within the FunSearch structure. FunBO does not make assumptions about similarities between f
and G, nor assumes access to a large dataset for each function in G. Therefore, FunBO can be used to
discover both general-purpose and function-specific AFs as well as to adapt AFs via few-shots.
Method overview. FunBO sequentially prompts an LLM to improve an initial AF expressed in code
so as to enhance the performance of the corresponding BO algorithm when optimizing objectives
in G. At every step τ of FunBO, an LLM’s prompt is created by including the code for two AF
instances generated and stored in a programs database (DB) at previous iterations. With this prompt,
a number (B) of alternative AFs are sampled from the LLM and are evaluated based on their average
performance on a subset GTr ⊆G, which acts as training dataset. The evaluation process for an
AF, say hτ at step τ, on GTr gives a numeric score shτ (GTr) that is used to store programs in DB
and sample them for subsequent prompts. The “process” of prompt creation, LLM sampling, and AF
scoring and storing repeats until time budget T is reached. Out of the top performing6 AFs on GTr,
the algorithm returns the AF performing the best, on average, in the optimization of GV = G\GTr,
which acts as a validation dataset. When no validation functions are used (G = GTr), the AF with
the highest average performance on GTr is returned. Each FunBO component highlighted in bold is
described below in more details, along with the complete algorithm and graphical representation in
Fig. 1. We denote the AF returned by FunBO as αFunBO.
Initial AF. FunBO’s initial program h determines the input variables that can be used to generate
alternative AFs while imposing a prior on the programs the LLM will generate at successive steps. We
consider acquisition_function in Fig. 2 (top) which takes the functional form of the EI and has
as inputs the union of the inputs given to EI, UCB and PofI. The AF returns an integer representing
the index of the point in a vector of potential locations that should be selected for the next function
evaluation. All programs generated by the LLM share the same inputs and output, but vary in their
implementation, which defines different optimization strategies, see for instance the AF generated for
one of our experiments in Fig. 3 (left).
6In this work we consider the programs with score in the top 20th percentile.
4


--- Page 5 ---
Prompt. At every algorithm iteration, a prompt is constructed by sampling two AFs, hi and hj,
previously generated and stored in DB. hi and hj are sampled from DB in a way that favours higher
scoring and shorter programs (see paragraph below for more details) and are sorted in the prompt
in ascending order based on their scores shi(GTr) and shj(GTr), see the prompt skeleton7 in Fig. 2
(bottom). The LLM is then asked to generate a new AF representing an improved version of the last,
higher scoring, program.
Evaluation. As expected, the evaluation protocol is critical for the discovery of appropriate AFs. Our
novel evaluation setup, unlike the one used in FunSearch, entails performing a full BO loop to evaluate
program fitness. In particular, each function generated by the LLM is (i) checked to verify it is correct,
i.e., it compiles and returns a numerical output; (ii) scored based on the average performance of a
BO algorithm using hτ as an AF on GTr. Evaluation is performed by running a full BO loop with hτ
for each function gj ∈GTr and computing a score that contains two terms: a term that rewards AFs
finding values close to the true optimum, and a term that rewards AFs finding the optimum in fewer
evaluations (often called trials). Specifically, we use the score:
shτ (GTr) =
1
|GTr|
J
X
j=1
" 
1 −
gj(x∗
j,hτ ) −y∗
j
gj(xt=0
j
) −y∗
j
!
+

1 −Thτ
T
#
(1)
where, for each gj, y∗
j is the known true optimum, xt=0
j
gives the optimal input value at t = 0 which
is assumed to be different from the true one, x∗
j,hτ is the found optimal input value with hτ and Thτ
gives the number of trials out of T that hτ selected before reaching y∗
j (if the optimum was not found,
then Thτ = T to indicate that all available trials have been used). The first term in the square brackets
of Eq. (1) quantifies the discrepancy between the function values at the returned optimum and the
true optimum. This term becomes zero when x∗
j,hτ equals xt=0
j
, indicating a failure to explore the
search space. Conversely, if hτ successfully identifies the true optimum, such that gj(x∗
j,hτ ) = y∗
j ,
this term reaches its maximum value of one. The second term in Eq. (1) captures how quickly hτ
identifies y∗
j . When Thτ = T, indicating the algorithm has not converged, this term becomes zero,
and the score is solely determined by the discrepancy between the discovered and true optimum. If,
instead, the algorithm reaches the global optimum, this term represents the proportion of trials, out of
the total budget T, needed to do so. Code for the evaluation process is presented in Appendix A.
Programs database. Similar to FunSearch, scored AFs are added to DB, which keeps a population of
correct programs following an island model [36]. DB is initialized with a number NDB of islands that
evolve independently. Sampling of hi and hj from DB is done by first uniformly sampling an island
and, within that island, sampling programs by favouring those that are shorter and higher scoring. A
new program generated when using hi and hj in the prompt is added to the same island and, within
that, to a cluster of programs performing similarly on GTr, see Appendix B for more details.
4
Experiments
Our experiments explore FunBO’s ability to generate novel and efficient AFs across a wide variety
of settings. In particular, we demonstrate its potential to generate AFs that generalize well to the
optimization of functions both in distribution (ID, i.e. within function classes) and out of distribution
(OOD, i.e. across function classes) by running three different types of experiments:
1. OOD-Bench tests generalization across function classes by running FunBO with G containing
different standard global optimization benchmarks and testing on a set F that similarly comprises
diverse functions in terms of smoothness, input ranges and dimensionality and output magnitudes.
We do not scale the output values nor normalise the input domains to facilitate learning, but rather
use the objective functions as available in standard BO packages out-of-the-box. In this case G and
F do not share any particular structure, thus the generated AFs are closer to general-purpose AFs.
2. ID-Bench, HPO-ID and GPs-ID test FunBO-generated AFs within function classes for standard
global optimization benchmarks, HPO tasks, and general function classes, respectively. As this
setting is closer to the one considered by meta-learning approaches introduced in Section 2, we
compare FunBO against MetaBO [40],8 the state-of-the-art transfer acquisition function.
7Note that, when τ = 1, only the initial program will be available in DB thus the prompt in Fig. 2 will be
simplified by removing acquisition_function_v1 and replacing v_2 with v_1.
8We used the author-provided implementation at https://github.com/boschresearch/MetaBO.
5


--- Page 6 ---
def acquisition_function(predictive_mean
predictive_var , incumbent , beta=1.0):
"""Returns the index of the point to collect..."""
predictive_std = np.sqrt(predictive_var)
diff_mean_std = (incumbent −predictive_mean
+ beta * predictive_std)
z = diff_mean_std / predictive_std
vals = (diff_mean_std * stats.norm.cdf(z)
+ predictive_std * stats.norm.pdf(z))
return np.argmax(vals)
Figure 3: OOD-Bench. Left: Code for αFunBO. Right: Different AFs trading-off exploration and
exploitation for two one-dimensional objective functions (green lines). Blue and gray trajectories
track the points queried by αFunBO, EI and UCB over 150 steps (right y-axis). All AFs behave similarly
for Styblinski-Tang (top, note that trajectories are overlapping), converging to the true optimizer (red
vertical line) in fewer than 25 trials. Instead, for Weierstrass (bottom), EI and UCB get stuck after a
few trials while αFunBO continues to explore, eventually converging to the ground truth optimum.
3. FEW-SHOT demonstrates how FunBO can be used in the context of few-shot fast adaptation of an
AF. In this case, the AF is learnt using a general function class as G and is then tuned, using a
very small (5) number of examples, to optimize a specific synthetic function. We compare our
approach to Hsieh et al. [14],9 the most relevant few-shot learning method.
Figure 4: OOD-Bench. Average BO performance
when using known general purpose AFs and αFunBO.
Shaded area gives ± standard deviations/2. The
red line gives ¯Rt = 0, i.e. zero average regret.
We report all results in terms of normalized aver-
age simple regret on a test set, ¯Rt, as a function
of the trial t. For an objective function f, this
is defined as Rt = f(x∗
t ) −y∗where y∗is the
true optimum and x∗
t is the best selected point
within the data collected up to t. As F might
include functions with different scales, we nor-
malize the regret values to be in [0, 1] before
averaging them. To isolate the effects of differ-
ent acquisition functions, we employ the same
setting across all methods in terms of (i) number
of trials T, (ii) hyperparameters of the GP surro-
gate models (tuned offline), (iii) evaluation grid
for the AF, which is set to be a Sobol grid [33]
on the input space, and (iv) initial design, which
includes the input point giving the maximum
function value on the grid. All experiments are conducted using FunSearch with default hyperparam-
eters in Romera-Paredes et al. [30]10 unless otherwise stated. We employ Codey, an LLM fine-tuned
on a large code corpus and based on the PaLM model family [37], to generate AFs.11
OOD-Bench. We test the capabilities of FunBO to generate an AF that performs well across function
classes by including the one-dimensional functions Ackley, Levy, and Schwefel in GTr and using the
one-dimensional Rosenbrock function for GV. We test the resulting αFunBO on nine very different
objective functions: Sphere (d = 1), Styblinski-Tang (d = 1), Weierstrass (d = 1), Beale (d = 2),
Branin (d = 2), Michalewicz (d = 2), Goldstein-Price (d = 2) and Hartmann with both d = 3 and
d = 6. We do not compare against MetaBO as (i) it was developed for settings in which the functions
in G and F belong to the same class and, (ii) the neural AF is trained with evaluation points of a
given dimension, thus it cannot be deployed for the optimization of functions across different d. For
9We used the author-provided implementation at https://github.com/pinghsieh/FSAF.
10See code at https://github.com/google-deepmind/funsearch.
11Codey is publicly accessible via its API [39]. For AF sampling, we used 5 Codey instances running on tensor
processing units on a computing cluster. For scoring, we used 100 CPUs evaluators per LLM instance.
6


--- Page 7 ---
Figure 5: ID-Bench. Average BO performance when using known general purpose AFs (gray lines),
the AF learned by MetaBO (black dashed line) and αFunBO (blue line) on 100 function instances.
Shaded area gives ± standard deviations/2. The red line represents ¯Rt = 0, i.e. zero average regret.
completeness, we report a comparison with a dimensionality-agnostic version of MetaBO in Appendix
C.1 (Fig. 11) together with all experimental details, e.g., input ranges and hyperparameter settings.
AF interpretation: In this experiment, αFunBO (Fig. 3, left) represents a combination of EI and UCB
which, due to the beta*predictive_std term, is more exploratory than EI but, considering the
incumbent value, still factors in the expected magnitude of the improvement and reduces to EI
when beta=0. This determines the way αFunBO trades-off exploration and exploitation which can
be visualized by looking at the "exploration path", i.e., the sequence of x values selected over t, as
shown in the right plots of Fig. 3 (t measured on the secondary y-axis). For objective functions that
are smooth, for example Styblinski-Tang (top plot), the exploration path of αFunBO matches those of
EI and UCB. In this scenario, all AFs exhibit similar behavior, converging to x∗(red vertical line)
with less than 25 trials. When instead the objective function has a lot of local optima (bottom plot)
as in Weierstrass, both EI and UCB get stuck after a few trials while FunBO keeps on exploring the
search space eventually converging to x∗. Notice how in this plot the convergence paths of all AFs
differ and only the blue line aligns with the red line, i.e., converges to x∗, after a few trials.
Using αFunBO to optimize the nine functions in F leads to a fast and accurate convergence to the
global optima (Fig. 4). The same is confirmed when extending the test set to include 50 scaled and
translated instances of the functions in F (Fig. 11, right).
ID-Bench. Next we evaluate FunBO capabilities to generate AFs that perform well within function
classes using Branin, Goldstein-Price and Hartmann (d = 3). For each of these three functions, we
train both FunBO and MetaBO with |G| = 25 instances of the original function obtained by scaling
and translating it with values in [0.9, 1.1] and [−0.1, 0.1]d respectively.12 For FunBO we randomly
assign 5 functions in G to GV and keep the rest in GTr. We test the performance of the learned AFs on
another 100 instances of the same function, with randomly sampled values of scale and translation
from the same ranges. We additionally compare against a BO algorithm that uses EI, UCB, PofI,
MEAN or a random selection of points. All hyper-parameter settings for this experiment are provided
in Appendix C.2. Across all objective functions, αFunBO leads to a convergence performance that is
close to or outperform both general purpose and meta-learned AFs (Fig. 5). The AFs found in this
experiment (code in Figs. 13-15) are “customized” to a given function class thus being closer, in
spirit, to the transfer AF. To further validate the generalizability of αFunBO found in OOD-Bench, we
tested such AF across instances of Branin, Goldstein-Price and Hartmann (Fig. 12, green line). We
found it to perform well against general purpose AFs thus confirming the strong results observed in
OOD-Bench while being, as expected, slower than AFs customized to a specific objective.
HPO-ID. We test FunBO on two HPO tasks where the goal is to minimize the loss (d = 2) of an
RBF-based SVM and an AdaBoost algorithm.13 As in ID-Bench, we test the ability to generate AFs
that generalize well within function classes. Therefore, we train FunBO and MetaBO with losses
computed on a random selection of 35 of the 50 available datasets and test on losses computed on
the remaining 15 datasets. For FunBO we randomly assign 5 dataset to GV and keep the rest in GTr.
FunBO identifies AFs (code in Fig. 17-18) that outperform all other AFs in AdaBoost (Fig. 7, left)
12Throughout the paper we adopt MetaBO’s translation and scaling ranges.
13We use precomputed loss values across 50 datasets given as part of the HyLAP project
(http://www.hylap.org/). For SVM, the two hyperparameters are the RBF kernel parameter and the penalty
parameter while for AdaBoost they correspond to the number of product terms and the number of iterations.
7


--- Page 8 ---
Figure 7: Average BO performance when using known general purpose AFs (gray lines), the AF learned
by MetaBO (black dashed line) and αFunBO (blue line). Shaded area gives ± standard deviations/2.
The red line represents ¯Rt = 0, i.e. zero average regret. Left: HPO-ID. Right: GPs-ID with d = 4.
while performing similarly to general purpose or meta-learned AFs for SVM (Fig. 16). Across the
two tasks, αFunBO found in OOD-Bench still outperforms general-purpose AFs while yielding slightly
worse performance compared to MetaBO and FunBO customized AFs (Fig. 16, green lines).
GPs-ID. Similar results are obtained for general function classes whose members do not exhibit
any particular shared structure. We let GTr include 25 functions sampled from a GP prior with
d = 3, RBF kernel and length-scale drawn uniformly from [0.05, 0.5]. We test the found AF on
100 other GP samples defined both for d = 3 and d = 4 and length-scale values sampled similarly.
Figure 6: FEW-SHOT.
As done by [40], we consider a dimensionality-
agnostic version of MetaBO that allows deploy-
ing the function learned from d = 3 functions
on d = 4 objectives. We found αFunBO to outper-
form all other AFs (code in Fig. 20) in d = 4
(Fig. 7, right) while matching EI and outper-
forming MetaBO in d = 3 (Fig. 19, left).
FEW-SHOT.
We conclude our experimental
analysis by demonstrating how FunBO can be
used in the context of few-shot adaptation. In
this setting, we aim at learning an AF customized
to a specific function class by “adapting” an ini-
tial AF with a small number of instances from
the target class. We consider Ackley (d = 2)
as the objective function and compare against FSAF [14], which is the closest few-shot adaptation
method for BO. FSAF trains the initial AF with a set of GPs, adapts it using 5 instances of scaled and
translated Ackley functions, then tests the adapted AF on 100 additional Ackley instances, generated
in the same manner. Note that FSAF uses a large variety of GP functions with different kernels and
various hyperparameters for training the initial AF. On the contrary, FunBO few-shot adaptation is
performed by setting the initial h function to the one found in GPs-ID (Fig. 6, green line) using 25 GPs
with RBF kernel, and including the 5 instances of Ackley used by FSAF in GTr. Despite the limited
training set, FunBO adapts very quickly to the new function instances, identifying an AF (code in Fig.
21) that outperforms both general purpose AFs and FSAF (Fig. 6, blue line).
5
Related work
LLMs as mutation operators. FunBO expands FunSearch [30], an evolutionary algorithm pairing
an LLM with an evaluator to solve open problems in mathematics and algorithm design. Prior to
FunSearch, the idea of using LLMs as mutation operators paired with a scoring mechanism had been
explored to a create a self-improvement loop [20], to optimize code for robotic simulations, or to
evolve stable diffusion images with simple genetic algorithms [24]. Other works explore the use of
LLMs to search over neural network architectures described with Python code [26, 48, 3], find formal
proofs for automatic theorem proving [27, 15] or automatically design heuristics [21].
8


--- Page 9 ---
Meta-learning for BO. Our work is also related to the literature on meta-learning for BO. In this
realm, several studies have focused on meta-learning an accurate surrogate model for the objective
function exploiting observations from related functions, for instance by using standard multi-task
GPs [35, 46] or ensembles of GP models [11, 44, 43]. Others have focused on meta-learning general
purpose optimizers by using recurrent neural networks with access to gradient information [5] or
transformers [6]. More relevant to our work are studies focusing on transferring information from
related tasks by learning novel AFs that more efficiently solve the classic exploration-exploitation
trade-off in BO algorithms [40, 14, 23]. In contrast to prior works in this literature, FunBO produces
AFs that are more interpretable, simpler and cheaper to deploy than neural network-based AFs and
generalize not only within specific function classes but also across different classes.
LLMs and black-box optimization. Several works investigated the use of LLMs to solve black-box
optimization problems. For instance, both Liu et al. [22] and Yang et al. [45] framed optimization
problems in natural language and asked LLMs to iteratively propose promising solutions and/or
evaluate them. Similarly, Ramos et al. [28] replaced surrogate modeling with LLMs within a BO
algorithm targeted at catalyst or molecule optimization. Other works have focused on exploiting
black-box methods for prompt optimization [34, 4, 7, 10], solving HPO tasks with LLMs [47] or
identifying optimal LLM hyperparameter settings via black-box optimization approaches [41, 38].
Unlike these approaches, we do not use LLMs to replace the entire optimization process, but rather
leverage their creative capabilities to enhance a critical component within an existing BO algorithm,
thus narrowing the search space and ensuring interpretability.
6
Conclusions and Discussion
We tackled the problem of discovering novel, well performing AFs for BO through FunBO, a
FunSearch-based algorithm which explores the space of AFs by letting an LLM iteratively mod-
ify the AF expression in native computer code to improve the efficiency of the corresponding BO
algorithm. We have shown across a variety of settings that FunBO learns AFs that generalize well
within and across function classes while being easily adaptable to specific objective functions of
interest with only a few training examples.
Limitations. FunBO inherits the strengths of FunSearch along with some of its inherent constraints.
While FunSearch allows finding programs that are concise and interpretable, it works best for
programs that can be quickly evaluated and for which the score provides an accurate quantification of
the improvement achieved. Therefore, a potential limitation of FunBO is the computational overhead
associated with running a full BO loop for each function in G, which significantly increases the
evaluation time of every sampled AF (especially when T is high). This limits the scalability of FunBO
for larger sets G and hinders its application to more complex optimization problems, such as those
with multiple objectives. In addition, the simple metric considered in this work in Eq. (1), only
captures the distance from the true optimum and the number of trials needed to identify it. More
research needs to be done to understand if a metric that better characterizes the convergence path
for a given AF can improve FunBO performance. Furthermore, each FunBO experiment shown in
this work required obtaining a large number of LLM samples. This means that the overall cost of
experiments, which depends on the LLM used as well as the algorithm’s implementation (e.g. single
threaded or distributed, as originally proposed by FunSearch), can be high. Finally, as reported in
[30], the variance in the quality of the AF found by FunBO is high. This is due to the randomness in
both the LLM sampling and the evolutionary procedure. While we were able to reproduce the results
shown for ID-Bench, HPO-ID and GPs-ID with different FunBO experiments, finding AFs that perform
well across function classes required multiple FunBO runs.
Future work. This work opens up several promising avenues for future research. While our focus
here was on the simplest single-output BO algorithm with a GP surrogate model, FunBO can be
extended to learn new AFs for various adaptations of this problem, such as constrained optimization,
noisy evaluations, or alternative surrogate models. Additionally, FunBO demonstrates the potential
to harness the power of LLMs while maintaining the interpretability of AFs expressed in code. This
opens an exciting avenue for exploring how and what assumptions can be encoded within AFs, based
on the desired program characteristics and prior knowledge about the objective function. Finally,
the discovered AFs might have intrinsic value, independently on how they were discovered. Future
work could focus on more extensively test their properties and identify those that can be added to the
standard suite of AFs available in BO packages.
9


--- Page 10 ---
References
[1] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-
parameter optimization. In Advances in Neural Information Processing Systems, volume 24,
2011.
[2] Roberto Calandra, André Seyfarth, Jan Peters, and Marc Peter Deisenroth. Bayesian optimiza-
tion for learning gaits under uncertainty: An experimental comparison on a dynamic bipedal
walker. Annals of Mathematics and Artificial Intelligence, 76:5–23, 2016.
[3] Angelica Chen, David Dohan, and David So. Evoprompting: Language models for code-level
neural architecture search. In Advances in Neural Information Processing Systems, volume 36,
2024.
[4] Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. Instructzero:
Efficient instruction optimization for black-box large language models.
arXiv preprint
arXiv:2306.03082, 2023.
[5] Yutian Chen, Matthew W Hoffman, Sergio Gómez Colmenarejo, Misha Denil, Timothy P
Lillicrap, Matt Botvinick, and Nando Freitas. Learning to learn without gradient descent by
gradient descent. In International Conference on Machine Learning, pages 748–756, 2017.
[6] Yutian Chen, Xingyou Song, Chansoo Lee, Zi Wang, Richard Zhang, David Dohan, Kazuya
Kawakami, Greg Kochanski, Arnaud Doucet, Marc' Aurelio Ranzato, Sagi Perel, and Nando
de Freitas. Towards learning universal hyperparameter optimizers with transformers. In
Advances in Neural Information Processing Systems, volume 35, pages 32053–32068, 2022.
[7] Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning Wang, Yuxiao Dong, Jie Tang, and
Minlie Huang. Black-box prompt optimization: Aligning large language models without model
training. arXiv preprint arXiv:2311.04155, 2023.
[8] Hyunghun Cho, Yongjin Kim, Eunjung Lee, Daeyoung Choi, Yongjae Lee, and Wonjong Rhee.
Basic enhancement strategies when using Bayesian optimization for hyperparameter tuning of
deep neural networks. IEEE Access, 8:52588–52608, 2020.
[9] E G Coffman, M R Garey, and D S Johnson. Approximation algorithms for bin-packing —
an updated survey. In G Ausiello, M Lucertini, and P Serafini, editors, Algorithm Design for
Computer System Design, pages 49–106. Springer Vienna, 1984.
[10] C. Fernando, D. Banarse, H. Michalewski, S. Osindero, and T. Rocktäschel. Promptbreeder:
Self-referential self-improvement via prompt evolution. arXiv preprint arXiv:2309.16797, 2023.
[11] Matthias Feurer, Benjamin Letham, and Eytan Bakshy. Scalable meta-learning for Bayesian
optimization using ranking-weighted Gaussian process ensembles. In AutoML Workshop at
ICML, volume 7, 2018.
[12] Peter I Frazier, Warren B Powell, and Savas Dayanik.
A knowledge-gradient policy for
sequential information collection. SIAM Journal on Control and Optimization, 47(5):2410–
2439, 2008.
[13] Roman Garnett. Bayesian Optimization. Cambridge University Press, 2023.
[14] Bing-Jing Hsieh, Ping-Chun Hsieh, and Xi Liu. Reinforced few-shot acquisition function
learning for Bayesian optimization. In Advances in Neural Information Processing Systems,
volume 34, pages 7718–7731, 2021.
[15] Albert Qiaochu Jiang, Wenda Li, Szymon Tworkowski, Konrad Czechowski, Tomasz
Odrzygó´zd´z, Piotr Miło´s, Yuhuai Wu, and Mateja Jamnik.
Thor: Wielding hammers to
integrate language models and automated theorem provers. In Advances in Neural Information
Processing Systems, volume 35, pages 8360–8373, 2022.
[16] Donald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of
expensive black-box functions. Journal of Global Optimization, 13:455–492, 1998.
10


--- Page 11 ---
[17] Ksenia Korovina, Sailun Xu, Kirthevasan Kandasamy, Willie Neiswanger, Barnabas Poczos,
Jeff Schneider, and Eric Xing. Chembo: Bayesian optimization of small organic molecules
with synthesizable recommendations. In International Conference on Artificial Intelligence and
Statistics, pages 3393–3403, 2020.
[18] Harold J Kushner. A new method of locating the maximum point of an arbitrary multipeak
curve in the presence of noise. Journal Basic Engineering, 86(1):97–106, 1964.
[19] Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Ad-
vances in Applied Mathematics, 6(1):4–22, 1985.
[20] Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and Kenneth O
Stanley. Evolution through large models. In Handbook of Evolutionary Machine Learning,
pages 331–366. Springer, 2023.
[21] Fei Liu, Xialiang Tong, Mingxuan Yuan, Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and
Qingfu Zhang. Evolution of heuristics: Towards efficient automatic algorithm design using
large language mode. arXiv preprint arXiv:2401.02051, 2024.
[22] Tennison Liu, Nicolás Astorga, Nabeel Seedat, and Mihaela van der Schaar. Large language
models to enhance Bayesian optimization. In International Conference on Learning Represen-
tations, 2024.
[23] Alexandre Maraval, Matthieu Zimmer, Antoine Grosnit, and Haitham Bou Ammar. End-to-
end meta-Bayesian optimisation with transformer neural processes. In Advances in Neural
Information Processing Systems, volume 36, 2024.
[24] Elliot Meyerson, Mark J Nelson, Herbie Bradley, Adam Gaier, Arash Moradi, Amy K Hoover,
and Joel Lehman. Language model crossover: Variation through few-shot prompting. arXiv
preprint arXiv:2302.12170, 2023.
[25] Jonas Mockus. On Bayesian methods for seeking the extremum. Proceedings of the IFIP
Technical Conference, pages 400–404, 1974.
[26] Muhammad U Nasir, Sam Earle, Julian Togelius, Steven James, and Christopher Cleghorn. LL-
Matic: Neural architecture search via large language models and quality diversity optimization.
arXiv preprint arXiv:2306.01102, 2023.
[27] Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving.
arXiv preprint arXiv:2009.03393, 2020.
[28] Mayk Caldas Ramos, Shane S Michtavy, Marc D Porosoff, and Andrew D White. Bayesian
optimization of catalysts with in-context learning. arXiv preprint arXiv:2304.05341, 2023.
[29] Carl Edward Rasmussen and Christopher KI Williams. Gaussian Processes for Machine
Learning. MIT Press Cambridge, MA, 2006.
[30] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog,
M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang,
Omar Fawzi, et al. Mathematical discoveries from program search with large language models.
Nature, pages 1–3, 2023.
[31] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. CoRR, 2017.
[32] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of machine
learning algorithms. In Advances in Neural Information Processing Systems, volume 25, 2012.
[33] I. M. Sobol’. On the distribution of points in a cube and the approximate evaluation of integrals.
Zhurnal Vychislitel’noi Matematiki i Matematicheskoi Fiziki, 7, 1967.
[34] Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. Black-box tuning
for language-model-as-a-service. In International Conference on Machine Learning, pages
20841–20855, 2022.
11


--- Page 12 ---
[35] Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task Bayesian optimization. In
Advances in Neural Information Processing Systems, volume 26, 2013.
[36] Reiko Tanese. Distributed Genetic Algorithms for Function Optimization. PhD thesis, University
of Michigan, 1989.
[37] Google PaLM-2 Team. PaLM 2 technical report. arXiv preprint arXiv:2305.10403, 2023.
[38] Christophe Tribes, Sacha Benarroch-Lelong, Peng Lu, and Ivan Kobyzev. Hyperparameter
optimization for large language model instruction-tuning. In AAAI Conference on Artificial
Intelligence, 2024.
[39] Google Cloud Vertex AI. Code models overview. 2023. URL https://cloud.google.com/
vertex-ai/docs/generative-ai/code/code-models-overview.
[40] Michael Volpp, Lukas P Fröhlich, Kirsten Fischer, Andreas Doerr, Stefan Falkner, Frank Hutter,
and Christian Daniel. Meta-learning acquisition functions for transfer learning in Bayesian
optimization. In International Conference on Learning Representations, 2020.
[41] Chi Wang, Xueqing Liu, and Ahmed Hassan Awadallah. Cost-effective hyperparameter op-
timization for large language model generation inference. In International Conference on
Automated Machine Learning, pages 21–1, 2023.
[42] Zi Wang and Stefanie Jegelka. Max-value entropy search for efficient Bayesian optimization.
In International Conference on Machine Learning, pages 3627–3635, 2017.
[43] Martin Wistuba and Josif Grabocka. Few-shot Bayesian optimization with deep kernel surro-
gates. In International Conference on Learning Representations, 2021.
[44] Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Scalable Gaussian process-based
transfer surrogates for hyperparameter optimization. Machine Learning, 107(1):43–78, 2018.
[45] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun
Chen. Large language models as optimizers. In International Conference on Learning Repre-
sentations, 2024.
[46] Dani Yogatama and Gideon Mann. Efficient transfer learning method for automatic hyper-
parameter tuning. In International Conference on Artificial Intelligence and Statistics, pages
1077–1085, 2014.
[47] Michael R Zhang, Nishkrit Desai, Juhan Bae, Jonathan Lorraine, and Jimmy Ba. Using large
language models for hyperparameter optimization. In NeurIPS 2023 Foundation Models for
Decision Making Workshop, 2023.
[48] Mingkai Zheng, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu, and Samuel Albanie. Can
GPT-4 perform neural architecture search? arXiv preprint arXiv:2304.10970, 2023.
12


--- Page 13 ---
import numpy as np
from scipy import stats
def acquisition_function(predictive_mean , predictive_var , incumbent , beta=1.0):
"""Returns the index of the point to collect in a vector of eval points.
Given the posterior mean and posterior variance of a GP model for the objective function ,
this function computes an heuristic and find its optimum. The next function evaluation
will be placed at the point corresponding to the selected index in a vector of
possible eval points.
Args:
predictive_mean: an array of shape [num_points , dim] containing the predicted mean
values for the GP model on the objective function for ‘num_points ‘ points of
dimensionality ‘dim‘.
predictive_var: an array of shape [num_points , dim] containing the predicted variance
values for the GP model on the objective function for ‘num_points ‘ points
of dimensionality ‘dim‘.
incumbent: current optimum value of objective function observed.
beta: a possible hyperparameter to construct the heuristic.
Returns:
An integer representing the index of the point in the array of shape [num_points , dim]
that needs to
be selected for function evaluation.
"""
z = (incumbent −predictive_mean) / np.sqrt(predictive_var)
predictive_std = np.sqrt(predictive_var)
vals = (incumbent −predictive_mean) * stats.norm.cdf(z) + predictive_std * stats.norm.pdf(z)
return np.argmax(vals)
Figure 8: Python code for FunBO initial h function with full docstring.
A
Code for FunBO components
Fig. 8 gives the Python code for the initial acquisition function used by FunBO, including the full
docstring. The docstring describes the inputs of the function and the way in which the function itself
is used within the evaluate function e. Evaluation of the functions generated by FunBO is done by
first running a full BO loop (see Fig. 9 for Python code) and then, based on its output (the initial
optimal input value, the true optimum, the found optimum and the percentage of steps taken before
finding the latter), computing the score as in the Python code of Fig. 10. Note how the latter captures
how accurately and quickly a BO algorithm using the proposed AF finds the true optimum.
B
Programs Database
The DB structure matches the one proposed by FunSearch [30]. We discuss it here for completeness. A
multiple-deme model [36] is employed to preserve and encourage diversity in the generated programs.
Specifically, the program population in DB is divided into NDB islands, each initialized with the
given initial h and evolved independently. Within each island, programs are clustered based on
their scores on the functions in GTr, with AFs having the same scores grouped together. Sampling
from DB involves first uniformly selecting an island and then sampling two AFs from it. Within the
chosen island, a cluster is sampled, favoring those with higher score values, followed by sampling
a program within that cluster, favoring shorter ones. The newly generated AF is added to the same
island associated with the instances in the prompt, but to a cluster reflecting its scores on GTr. Every
4 hours, all programs from the NDB/2 islands with the lowest-scoring best AF are discarded. These
islands are then reseeded with a single program from the surviving islands. This procedure eliminates
under-performing AFs, creating space for more promising programs. See the Methods section in [30]
for further details.
13


--- Page 14 ---
"""Evaluate an AF with a full BO loop for the objective f."""
import GPy
import numpy as np
import utils
def run_bo(
f,
# objective function to minimize
acquisition_function , # h given by LLM
num_eval_points = 1000,
num_trials = 30):
"""Run a BO loop and return the minimum objective functions found and the percentage of
trials required to reach it."""
# Get evaluation points for AF. get_eval_points() returns a given number of points on a
# Sobol grid on the f’s input space
eval_points = utils.get_eval_points(f, num_eval_points)
# Get the initial point with get_initial_design(). This is set to be the point giving the
# maximum (worst) function evaluation among eval_points
initial_x , initial_y = utils.get_initial_design(f)
# Initialize GP hyper−parameters. We pre−compute the RBF kernel hyper−parameters
# for each given f. These are returned by get_hyperparameters().
hp = utils.get_hyperparameters(f)
# Initialize kernel and model.
kernel = GPy.kern.RBF(input_dim=input_dim , variance=hp[’variance’],
lengthscale=hp[’lengthscale’], ARD=hp[’ard’])
model = GPy.models.GPRegression(initial_x , initial_y , kernel)
# Get initial predictive mean and var.
predictive_mean , predictive_var = model.predict(eval_points)
# Get initial optimum value.
found_min = initial_min_y = float(np.min(model.Y))
# Get true optimum value.
true_min = np.min(f(eval_points))
# Optimization loop.
for _ in range(num_trials):
new_input = acquisition_function(eval_points ,
# Get new point using AF.
predictive_mean , predictive_var , found_min)
new_output = f(new_input)
# Evaluate new point.
model.set_XY(np.concatenate((model.X, new_input), axis=0), # Append to dataset.
np.concatenate((model.Y, new_output), axis=0))
# Get updated mean and var
predictive_mean , predictive_var = model.predict(eval_points)
found_min = float(np.min(model.Y))
# Get current optimum value.
# Get percentage of trials (note that we remove the number of given points in the
initial design) needed to identify the optimum.
percentage_steps_before_converging = (np.argmin(model.Y) −len(
initial_design_inputs)) / (num_trials) if found_min == true_min else 1.0
return (found_min , true_min , initial_min_y , percentage_steps_before_converging)
Figure 9: Python code for the first part of e used in FunBO. This function runs a full BO loop with
a given number of trials and points on a Sobol grid to assess how efficiently a given AF allows
optimizing f.
14


--- Page 15 ---
"""Score an AF given the output of run_bo()."""
import numpy as np
def score(found_min , true_min , initial_min_y , percentage_steps_before_converging):
"""Compute a score based on the output of run_bo()."""
# Get score based on how close the found optimum is to the true one (first term
# in Eq. (1)).
score_min_reached = 1.0 −np.abs(found_min −true_min) / (initial_min_y −true_min)
# Get score based on how the percentage of trials needed to identify the true
# optimum (second term in Eq. (1)).
score_steps_needed = 1.0 −percentage_steps_needed
return score_min_reached + score_steps_needed
Figure 10: Python code for the second part of e used in FunBO. Based on the output of run_bo(), this
function computes a score capturing how accurately and quickly an AF allows identifying the true
optimum.
C
Experimental details
In this section, we provide the experimental details for all our experiments.
We run FunBO
with T
= 48hrs, B = 12 and NDB = 10. To isolate the effect of using different AFs and
eliminate confounding factors related to AF maximization or surrogate models, we maximized
all AFs on a fixed Sobol grid (of size NSG) over each function’s input space. We also ensure
the same initial design across all methods (including the point with the highest/worst function
value on the Sobol grid) and used consistent GP hyperparameters which are tuned offline and
fixed. In particular, we use a GP model with zero mean function and RBF kernel defined as
Kθ(X, X′) = σ2
fexp(−||X −X′||2/2ℓ2) with θ = (ℓ, σ2
f) where ℓand σ2
f are the length-scale and
kernel variance respectively. The Gaussian likelihood noise σ2 is set to 1e−5 unless otherwise stated.
We set T = 30 for all experiments apart for HPO-ID and GPs-ID for which we use T = 20 to ensure
faster evaluations of generated AFs. We used the MetaBO implementation provided by the authors at
https://github.com/boschresearch/MetaBO, retaining default parameters except for remov-
ing the local maximization of AFs and ensuring consistency in the initial design. We followed the
same procedure for FSAF, using code available at https://github.com/pinghsieh/FSAF. We
ran UCB with β = 1. Experiment-specific settings are detailed below.
C.1
OOD-Bench
The parameter configurations adopted for each objective function used in this experiment, either
in G or in F, are given in Table 1. Notice that for Hartmann with d = 3 we use an ARD kernel.
Scaled and translated functions are obtained with translations sampled uniformly in [−0.1, 0.1]d and
scalings sampled uniformly in [0.9, 1.1]. Fig. 11 gives the results achieved by αFunBO (blue line) and
a dimensionality agnostic version of MetaBO that does not take the possible evaluation points as input
of the neural AF. This allows the neural AF to be trained on one-dimensional functions and be used to
optimize functions across input dimensions.
C.2
ID-Bench
The parameter configurations for Branin, Goldstein-Price and Hartmann are given in Table 2. For
this experiment, we adopt the parameters used by Volpp et al. [40] thus optimize the functions in the
unit-hypercube and use ARD RBF kernels. Fig. 12 gives the results achieved by αFunBO (blue line) and
the AF found by FunBO for OOD-Bench (green). The Python code for the found AFs is given in Figs.
13-15.
15


--- Page 16 ---
Figure 11: OOD-Bench. Average BO performance when using known general purpose AFs (gray lines
with different patterns), the AF learned by a dimensionality agnostic version of MetaBO (MetaBO-DA,
black dashed line) and αFunBO (blue line). Shaded area gives ± standard deviations/2. The red line
represents ¯Rt = 0, i.e., zero average regret. Left: F includes nine different synthetic functions. Right:
Extended test set including, for each function in F, 50 randomly scaled and translated instances.
Figure 12: ID-Bench. Average BO performance when using known general purpose AFs (gray lines
with different patterns), αFunBO found in OOD-Bench (green line), the AF learned by MetaBO (black
dashed line) and αFunBO (blue line) on 100 instances of Branin, Goldstein-Price and Hartmann. Shaded
area gives ± standard deviations/2. The red line represents ¯Rt = 0, i.e., zero average regret.
import numpy as np
from scipy import stats
def acquisition_function(predictive_mean , predictive_var , incumbent , beta=1.0):
"""Returns the index of the point to collect ..."""
y_pred = predictive_mean + 2 * predictive_var
diff_current_best_y_pred = incumbent −y_pred
bound_standard_deviation = np.maximum(np.sqrt(predictive_var), 1e−15)
z = diff_current_best_y_pred / bound_standard_deviation
vals = (diff_current_best_y_pred * stats.norm.cdf(z)
+ np.sqrt(predictive_var) * stats.norm.cdf(z + 0.5)
+ (stats.norm.cdf(z) −stats.norm.cdf(z + 0.5)) * predictive_var / 2)
a = np.maximum(diff_current_best_y_pred , incumbent)
alpha = diff_current_best_y_pred if incumbent > 0.0 else −np.inf
alpha = np.maximum(alpha, 0.) * (−alpha + 0.5 * a) −y_pred
y_vals = np.absolute(alpha + a + np.abs(y_pred)) * (a >= 0.)
for y_val in y_vals:
idx = np.argmax(vals −(y_val −y_pred) / bound_standard_deviation)
vals[idx] = 0
return np.argmax(vals)
Figure 13: ID-Bench. Python code for αFunBO for Branin. The BO performance corresponding to this
AF is given in Fig. 5 (left).
16


--- Page 17 ---
Table 1: Parameters used for OOD-Bench.
d
X
NSG
ℓ
σ2
f
σ2
f
Ackley
1
[−4, 4]
1000
0.21
28.19
1e −5
Levy
1
[−10, 10]
1000
1.05
83.32
1e −5
Schwefel
1
[−500, 500]
1000
18.46
76868.65
1e −5
Rosenbrock
1
[−5, 10]
1000
1.20
87328.20
1e −5
Sphere
1
[−5, 5]
1000
18.46
924202.43
1e −5
Styblinski-Tang
1
[−5, 5]
1000
7.34
119522207.86
1e −5
Weierstrass
1
[−0.5, 0.5]
1000
0.01
0.39
1e −5
Beale
2
[−4, 5]2
10000
0.46
546837.32
1e −5
Branin
2
[−5, 10] × [0, 15]
10000
4.65
155233.52
1e −5
Michalewicz
2
[0, π]2
10000
0.22
0.10
1e −5
Goldstein-Price
2
[−2, 2]2
10000
0.27
117903.96
1e −5
Hartmann-3
3
[0, 1]3
1728
[0.716, 0.298, 0.186]
0.83
1.688e −11
Hartmann-6
6
[0, 1]6
729
1.0
1.0
1e −5
Table 2: Parameters used for ID-Bench.
d
X
NSG
ℓ
σ2
f
σ2
f
Branin
2
[0, 1]2
961
[0.235, 0.578]
2.0
8.9e −16
Goldstein-Price
2
[0, 1]2
961
[0.130, 0.07]
0.616
1e −6
Hartmann-3
3
[0, 1]3
1728
[0.716, 0.298, 0.186]
0.83
1.688e −11
C.3
HPO-ID
For this experiment, we adopt the GP hyperparameters used by Volpp et al. [40]. From the training
datasets used in MetaBO, we assign “bands”, “wine”, “coil2000”, “winequality-red” and “titanic” for
Adaboost, and “bands”, “breast-cancer”, “banana”, “yeast” and “vehicle’ for SVM to GV. We keep
the rest in GTr. Fig. 16 gives the results achieved by αFunBO (blue lines) and the AF found by FunBO
for OOD-Bench (green lines). The Python code for the found AFs is given in Figs. 17-18.
import numpy as np
from scipy import stats
def acquisition_function(predictive_mean , predictive_var , incumbent , beta=1.0):
"""Returns the index of the point to collect ..."""
shape, dim = predictive_mean.shape
best_score = 0.0
g_i = 0.0
predictive_var[(shape−10)//2] *= dim
predictive_var[~ np.isfinite(predictive_var)] = 1.0
for i in range(predictive_mean.shape[0]):
curr_z = (incumbent −predictive_mean[i]) / np.sqrt(predictive_var[i])
new_score = predictive_var[i] *
stats.norm.cdf(curr_z , 0.5)
if new_score > best_score:
best_score = new_score
g_i = i
return g_i
Figure 14: ID-Bench. Python code for αFunBO for Goldstein-Price. The BO performance corresponding
to this AF is given in Fig. 5 (middle).
17


--- Page 18 ---
import numpy as np
from scipy import stats
def acquisition_function(predictive_mean , predictive_var , incumbent , beta=1.0):
"""Returns the index of the point to collect ..."""
diff_current_best_mean = incumbent −predictive_mean
standard_deviation = np.sqrt(predictive_var)
z = diff_current_best_mean / standard_deviation
vals = diff_current_best_mean * stats.norm.cdf(z)**3 + (
stats.norm.cdf(z)**2 + stats.norm.cdf(z) + 1) * stats.norm.pdf(z)
index = np.argmax(stats.truncnorm.cdf(vals, a=−0.1, b=0.1))
return index
Figure 15: ID-Bench. Python code for αFunBO for Hartmann. The BO performance corresponding to
this AF is given in Fig. 5 (right).
Figure 16: HPO-ID. Average BO performance when using known general purpose AFs (gray lines with
different patterns), a meta-learned AF by MetaBO (black dashed line), αFunBO found in OOD-Bench
(green lines) and αFunBO (blue lines). Shaded area gives ± standard deviations/2. The red line
represents ¯Rt = 0, i.e., zero average regret.
import numpy as np
from scipy import stats
def acquisition_function(predictive_mean , predictive_var , incumbent , beta=1.0):
"""Returns the index of the point to collect ..."""
c1 = np.exp(−beta)
c2 = 2.0 * beta * np.exp(−beta)
alpha = np.sqrt(2.0) * beta * np.sqrt(predictive_var)
z = (incumbent −predictive_mean) / alpha
vals = −abs(c1 * np.exp( −np.power(z, 2)) −1.0 + c1 + incumbent
) + 2.0 * beta * np.power(z+c2, 2)
vals −= np.log(np.power(alpha, 2))
vals[np.argmin(vals)] = 1.0
return np.argmin(vals)
Figure 17: HPO-ID. Python code for αFunBO for AdaBoost. The BO performance corresponding to this
AF is given in Fig. 7 (left).
18


--- Page 19 ---
import numpy as np
from scipy import stats
def acquisition_function(predictive_mean , predictive_var , incumbent , beta=1.0):
"""Returns the index of the point to collect ..."""
z = (incumbent −predictive_mean) / np.sqrt(predictive_var)
vals = (incumbent −predictive_mean) * stats.norm.cdf(z
) + np.sqrt(predictive_var) * stats.norm.pdf(z)
t0_val = stats.norm(loc=incumbent , scale=np.sqrt(predictive_var)).pdf(incumbent)
t1_val = z * stats.norm.pdf(z)
vals = ((vals * t1_val −t0_val) / (1 −2 * t1_val)
+ t1_val*(vals/(1−2*t1_val))
−vals/(1 −2*t1_val)**2 + t1_val*(t1_val −z)/beta)
return np.argmax(vals)
Figure 18: HPO-ID. Python code for αFunBO for SVM. The BO performance corresponding to this AF
is given in Fig. 16 (right).
Figure 19: Average BO performance when using known general purpose AFs (gray lines with different
patterns), the AF learned by MetaBO (black dashed line), αFunBO found in OOD-Bench (green lines)
and αFunBO (blue lines). Shaded area gives ± standard deviations/2. The red line represents ¯Rt = 0,
i.e. zero average regret. Left: GPs-ID. F includes functions with d = 3. Right: F includes functions
with d = 4.
C.4
GPs-ID
The functions included in both G and F are sampled from a GP prior with RBF kernel and length-scale
values drawn uniformly from [0.05, 0.5]. The functions are optimized in the input space [0, 1]3 with
NSG = 1728 points. In terms of GP hyperparameters, we set σ2
f = 1.0, σ2 = 1e −20 and use the
length-scale value used to sample each function as ℓ. Fig. 19 gives the results achieved by αFunBO and
the AF found by FunBO for OOD-Bench. The Python code for αFunBO is given in Fig. 20.
C.5
FEW-SHOT
For this experiment, the 5 Ackley functions used to “adapt” the initial AF are obtained by scaling
and translating the output and inputs values with translations and scalings uniformly sampled in
[−0.1, 0.1]d and [0.9, 1.1] respectively. The test set includes 100 instances of Ackley similarly
obtained with scale and translations values in [0.7, 1.3] and [−0.3, 0.3]d respectively. Furthermore,
we consider [0, 1]2 as input space and use NSB = 1000. The GP hyperparameters are set to ℓ=
[0.07, 0.018] (ARD kernel), σ2
f = 1.0 and σ2 = 8.9e −16. Python code for αFunBO is given in Fig.
21.
19


--- Page 20 ---
import numpy as np
from scipy import stats
def acquisition_function(predictive_mean , predictive_var , incumbent , beta = 1.0):
"""Returns the index of the point to collect ..."""
z = (incumbent −predictive_mean) / np.sqrt(predictive_var)
vals = ((incumbent −predictive_mean) * stats.norm.cdf(z
) + np.sqrt(predictive_var) * stats.norm.pdf(z))**2
vals = vals / (1 + (z / beta)**2 * np.sqrt(predictive_var))**2
return np.argmax(vals)
Figure 20: GPs-ID. Python code for αFunBO. The BO performance corresponding to this AF is given in
Fig. 7 (right).
import numpy as np
from scipy import stats
def acquisition_function(predictive_mean , predictive_var , incumbent , beta=1.0):
"""Returns the index of the point to collect ..."""
num_points , _ = predictive_mean.shape
a = 10
z = (predictive_mean + 0.000001 −incumbent) / np.sqrt(predictive_var)
vals = 1 / ((1 + (z / beta)**2 * np.sqrt(a * predictive_var + 0.00001)) **2)
beta_sqrt_p_z = np.sqrt(beta) * z
vals *= (1 + (z / beta)**2)*predictive_var/(
(1+ (beta_sqrt_p_z / np.sqrt(predictive_var))**2 * predictive_var) * (
1+(beta_sqrt_p_z / np.sqrt(predictive_var))**2))
vals += (1 −beta_sqrt_p_z / np.sqrt(predictive_var))**2 * predictive_var/ (
1 + (beta_sqrt_p_z / np.sqrt(predictive_var))**2 * predictive_var)**2
vals = (1 + (z / beta)**2) * vals−(1 −(z / beta)**2) * np.exp(−1) ** 2
vals = np.sqrt(a * predictive_var) * vals / np.sqrt(
a * predictive_var + 0.00001)
vals *= np.sqrt(np.sqrt(a * predictive_var) * predictive_var)
vals *= predictive_var**2
vals[:num_points // 2] = 0
return np.argmax(vals)
Figure 21: FEW-SHOT. Python code for αFunBO. The BO performance corresponding to this AF is
given in Fig. 6.
20
