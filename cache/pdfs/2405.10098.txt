--- Page 1 ---
When Large Language Model Meets Optimization
Sen Huanga, Kaixiang Yangb, Sheng Qic and Rui Wangc,‚àó
aSchool of Electronic and Information Engineering, South China University of Technology
bSchool of Computer Science and Engineering, South China University of Technology
cCollege of Systems Engineering, National University of Defense Technology
A R T I C L E I N F O
Keywords:
Large Language Model
Optimization Algorithm
Evolutionary Computation
A B S T R A C T
Optimization algorithms and large language models (LLMs) enhance decision-making in dynamic
environments by integrating artificial intelligence with traditional techniques. LLMs, with extensive
domain knowledge, facilitate intelligent modeling and strategic decision-making in optimization,
while optimization algorithms refine LLM architectures and output quality. This synergy offers novel
approaches for advancing general AI, addressing both the computational challenges of complex
problems and the application of LLMs in practical scenarios. This review outlines the progress and
potential of combining LLMs with optimization algorithms, providing insights for future research
directions.
1. Introduction
Optimization algorithms (OA) is becoming increasingly
important as a class of heuristic search algorithms in the
broad field of artificial intelligence and machine learn-
ing [89, 1, 114]. OA draws on the natural mechanisms
of biological evolution, including processes such as nat-
ural selection, heredity, mutation, and hybridization, for
solving complex optimization problems. These algorithms
are widely used in many fields due to their global search
capability, low dependence on problem structure, and ease
of parallelization. Optimization algorithms pivotal in diverse
fields such as logistics, finance [84], healthcare [88], and
artificial intelligence [25], aim to identify the best solution
from available alternatives. They are essential for making
decisions efficiently and effectively in an era of rapidly
increasing data complexity and volume. The continuous
advancement in optimization techniques has resulted in
significant enhancements to algorithmic strategies, each
customized to address specific types of problems and opera-
tional constraints. From deterministic methods addressing
linear problems to stochastic approaches for global opti-
mization under uncertainty, optimization algorithms hold
promise across a broad spectrum of research and practical
applications. With the development of technology, espe-
cially when dealing with large-scale, high-dimensional and
dynamically changing optimization problems, traditional al-
gorithms often face performance bottlenecks. Evolutionary
computing provides effective solutions to these problems
with its unique search strategy. In addition, the flexibility
and adaptability of evolutionary computation enable it to be
combined with a variety of other computational techniques
to form hybrid algorithms for further performance enhance-
ment.
In the rapidly evolving field of artificial intelligence,
Large Language Models (LLMs) such as GPT (Generative
‚àóCorresponding author.
huangsen@scut.edu.cn ( Sen Huang); yangkx@scut.edu.cn (
Kaixiang Yang); qisheng@nudt.edu.cn ( Sheng Qi); ruiwangnudt@gmail.com
( Rui Wang)
Pre-trained Transformer) [14] provide a significant break-
through with their advanced natural language understanding
and generation capabilities. These models have revolution-
ized applications ranging from automated writing assistants
to sophisticated conversational agents. LLMs have become
pivotal in advancing fields like natural language process-
ing, image recognition with their extensive parameters and
deep learning capabilities, and machine learning, offering
robust solutions for complex data-driven challenges. LLMs
have achieved breakthroughs in traditional NLP tasks like
text generation and language translation through extensive
data training, and they also show promising potential in
the emerging fields of algorithm design and optimization.
Traditional optimization algorithm design, dependent on
human expertise [92], is both time-consuming and poten-
tially limited by the experts‚Äô knowledge. The advent of
large-scale language models, however, has transformed this
arena. These models learn extensive algorithmic patterns
and strategies, enabling them to devise new algorithms and
tailor solutions to specific challenges.
Furthermore, constructing and training large language
models require significant computational resources and large
datasets [156], which escalate research and development
costs and constrain the applicability and generalization of
LLMs. Optimization algorithms are crucial in developing
LLMs, enabling researchers to efficiently tailor and refine
model structures for specific applications. By enhancing
the training process, boosting computational efficiency, and
lowering resource consumption, these algorithms facilitate
the construction and application of large-scale language
models. These algorithms enhance the models‚Äô generaliza-
tion capabilities and robustness, enabling improved perfor-
mance amidst real-world complexity and uncertainty. The
goal in designing optimization algorithms for LLMs is to
enhance their operational efficiency and reduce resource
consumption, without compromising, and possibly improv-
ing, model performance.
Huang et al.: Preprint submitted to Elsevier
Page 1 of 23
arXiv:2405.10098v1  [cs.NE]  16 May 2024


--- Page 2 ---
When Large Language Model Meets Optimization
This review aims to systematically analyze research on
developing optimization algorithms with LLMs, and opti-
mizing LLMs with optimization algorithms. It summarizes
related research and application scenarios, and explores the
diverse aspects of these applications. Section 2 provides
a comprehensive review of large language model develop-
ment, also known as macromodels, tracing their progression
from basic predictive models to sophisticated systems that
can comprehend and generate human-like text. Section 3
focus to optimization algorithms, concisely tracing their
evolution from basic iterative methods to advanced algo-
rithms essential for efficiently scaling AI models. Section 4
examines research that approaches large language models
as optimization problems, highlighting innovative methods
employed as search operators and in designing optimization
algorithms. Section 5 discusses recent advances in opti-
mization algorithms tailored for refining large-scale models.
It highlights how these algorithms can enhance design,
boost efficiency, and improve the performance of LLMs.
Section 6 examines the practical applications of integrating
optimization algorithms with LLMs, highlighting real-world
implementations and their benefits. Section 7 is future out-
look and research trends, in which summarizes the insights
gained from this exploration and provides an outlook on the
potential future developments in this exciting intersection of
AI research.
In summary, we conducted a comprehensive study on
the development and application of optimization algorithms
for large models, aiming to provide valuable references and
insights for future research.
2. Large language models
Language has a crucial role in human cognition, en-
abling communication and expression from early childhood
to adulthood [83]. Teaching robots to imitate human-like
language skills is a difficult task due to their intrinsic lack
of cognitive capacity for understanding and expressing lan-
guage. Computational linguistics aims to close this divide
by using advanced Artificial Intelligence (AI) algorithms to
enable machines to possess reading, writing, and communi-
cation skills that like those of humans [126].
The rise of Large Language Models (LLMs) undoubt-
edly represents an important milestone in the evolution of
Natural Language Processing (NLP). These models, such as
GPT-3 and GPT-4 of the GPT family [14], are built on the
Transformer architecture and have up to billions of parame-
ters. They achieve a deep understanding of natural language
and generative capabilities by pre-training on massive text
datasets. The evolution of LLMs has gone through several
notable stages: from the early days of Statistical Language
Models (SLMs) and Neuro-Linguistic Models (NLMs), to
Pre-Trained Language Models (PLMs), and ultimately to to-
day‚Äôs large-scale language models. While PLMs like BERT
and GPT-2 have achieved remarkable success in NLP tasks,
the emergence of LLMs has revolutionized the game in
this field [23]. Not only can they be adapted to a wide
range of tasks through large-scale pre-training, but they are
also further optimised through fine-tuning, demonstrating
a wide range of potential in application scenarios such as
chatbots, search engine optimization and office automation
The rise of Large Language Models (LLMs) undoubtedly
represents an important milestone in the evolution of Natural
Language Processing (NLP). These models, such as GPT-
3 and GPT-4 of the GPT family [14], are built on the
Transformer architecture and have up to billions of parame-
ters. They achieve a deep understanding of natural language
and generative capabilities by pre-training on massive text
datasets. The evolution of LLMs has gone through several
notable stages: from the early days of Statistical Language
Models (SLMs) and NLMs, PLMs, and ultimately to today‚Äôs
large-scale language models. While PLMs like BERT and
GPT-2 have achieved remarkable success in NLP tasks, the
emergence of LLMs has revolutionized the game in this
field. Not only can they be adapted to a wide range of tasks
through large-scale pre-training, but they are also further
optimized through fine-tuning, demonstrating a wide range
of potential in application scenarios such as chatbots, search
engine optimization, and office automation [156].
The excellence of Large Language Models (LLMs) in the
field of Natural Language Processing (NLP) is due to several
key components of their design, which together give LLMs
powerful language understanding and generation capabili-
ties [156]. First, ‚Äúpre-training‚Äù is one of the core processes
of LLMs. By pre-training on large-scale textual datasets,
LLMs are able to learn the basic structures and patterns
of language. These datasets typically contain billions of
words covering a wide range of topics and language styles,
allowing the models to capture the diversity and complex-
ity of the language. Second,‚Äúadaptability‚Äù is another key
characteristic of LLMs. After pre-training, LLMs can be
further fine-tuned to adapt to specific downstream tasks,
such as text classification, sentiment analysis, or machine
translation. This adaptability allows LLMs to optimise their
performance for specific tasks, leading to better results in
various NLP challenges [91]. In terms of ‚Äúapplications‚Äù,
the broad applicability of LLMs is another reason for their
popularity. Not only do they perform well in traditional
NLP tasks, but they can also be applied to a wider range
of domains, such as the development of chatbots, the opti-
mization of search engines, the construction of content rec-
ommendation systems, and the development of automated
office tools. Finally, ‚Äúperformance evaluation‚Äù is critical to
ensure the reliability and effectiveness of LLMs. Through
a series of standardised testing and evaluation protocols,
researchers are able to quantify the performance of LLMs
and ensure that they work consistently across a range of tasks
and conditions. Performance evaluation also includes studies
of model bias, fairness, and interpretability, which are key
factors in improving model quality and trust.
LLMs are becoming a key driver in the field of AI, and
their development and application are attracting widespread
attention from industry and academia. LLMs represented
by ChatGPT and GPT-4 have not only made significant
Huang et al.: Preprint submitted to Elsevier
Page 2 of 23


--- Page 3 ---
When Large Language Model Meets Optimization
progress in technology but also promoted in-depth discus-
sions on artificial general intelligence (AGI) conceptually
[156]. OpenAI‚Äôs technical article proposes that GPT-4 [2]
may be an early attempt to move towards AGI, which all
indicates the critical position of LLMs in the development
of AI [16]. In the field of Natural Language Processing
(NLP), LLMs are becoming a common tool for solving
various linguistic tasks, changing the previous research and
application paradigm. The Information Retrieval (IR) field
is also feeling the winds of change, with traditional search
engines facing the challenge of emerging information access
methods such as AI chatbots, such as New Bing3, which is
an attempt to enhance search results based on LLMs [17].
In addition, the field of computer vision (CV) is exploring
multimodal models that combine vision and language, and
the multimodal input support of GPT-4 is a manifestation of
this trend. The rise of LLMs heralds the birth of a whole
new ecosystem of apps based on these advanced models.
Microsoft 365 leverages LLMs (e.g., Copilot) to automate
the office, while OpenAI introduces plug-in functionality in
ChatGPT, all of which demonstrate the potential of LLMs
to enhance productivity and extend application scenarios
[136].
While LLMs have brought about many positive changes,
they also present a number of challenges, particularly in
terms of the security and accuracy of the generated con-
tent. In addition, the training of LLMs requires substantial
computational resources, which is a challenge for research
institutes as it limits the ability to perform extensive ex-
periments and optimization of the models [22].We have
summarised the relevant restrictions below: 1) Computa-
tional resources: LLMs demand substantial computational
resources for training and inference, which can pose chal-
lenges for implementing optimization algorithms at scale.
Ensuring access to adequate computational infrastructure re-
mains a significant hurdle. 2) Data efficiency: While LLMs
have demonstrated impressive performance, they often re-
quire large amounts of data for effective training. This re-
liance on extensive datasets can be a bottleneck for op-
timization algorithms, especially in scenarios where data
availability is limited or costly to obtain. 3) Interpretability
and explainability: The inherent complexity of LLMs poses
challenges for the interpretability and explainability of op-
timization algorithms. Understanding the decision-making
process of these models and interpreting their outputs can be
challenging, particularly in critical applications where trans-
parency is essential. 4) Generalization and Robustness:
Ensuring the generalization and robustness of optimization
algorithms trained using LLMs is another key challenge.
Over-reliance on specific patterns in the training data may
lead to poor generalization performance on unseen data or
vulnerability to adversarial attacks.
3. Classic Optimization Algorithm
3.1. Traditional optimization algorithms for
optimization problems
Optimization algorithms have wide applications in fields
such as industry, economics, and management. With the
rapid development of artificial intelligence, optimization
algorithms play a crucial role in achieving intelligence and
automation. Traditional optimization algorithms include de-
terministic optimization algorithms, approximation algo-
rithms, and heuristic algorithms. Deterministic optimiza-
tion algorithms include Linear Programming (LP)[98], In-
teger Programming (IP)[107], Mixed Integer Programming
(MIP)[45], Convex Optimization[130], Adaptive Dynamic
Pro gramming (ADP)[74], etc. Deterministic optimization
algorithms can guarantee finding the global optimal so-
lution, but they are generally not suitable for large-scale
problems. Polynomial-time approximation algorithms can
find a good solution within a reasonable time frame, but
they do not guarantee optimality. In other words, approxima-
tion algorithms do not guarantee finding the global optimal
solution. For some specific problems, optimality guaran-
tees may not exist at all. Heuristic algorithms employ spe-
cially designed functions to intelligently explore the solu-
tion space [35]. Heuristic Algorithm rely on intuitive rules,
trial-and-error strategies, and practical insights to approx-
imate solutions within acceptable bounds. Heuristic algo-
rithms are particularly effective for problems with high-
dimensional search spaces or combinatorial complexities
where exact solutions are impractical. Heuristic algorithms
include greedy algorithm[20], Tabu Search[4, 5, 49], genetic
algorithm[131, 63], differential evolution algorithm[97, 33],
Cooperative co-evolution algorithm[111, 18, 125]. Heuris-
tic algorithms have been widely used due to their excel-
lent computational performance, but they typically require
customization and domain expertise for specific problems.
Additionally, heuristic algorithms may converge to local
optimal solutions and have high time complexity.
Recently, the superiority of using Estimation of Distri-
bution Algorithm (EDA) in solving optimization problems
has been demonstrated. EDA is a prominent optimization
technique that employs probabilistic models to guide the
search process. Unlike traditional evolutionary algorithms,
which rely on mutation and recombination operators, EDA
focuses on building and updating a probabilistic model of
promising solutions. This model is then sampled to generate
new candidate solutions, effectively balancing exploration
and exploitation. Yang et al. [145] proposed ACSEDA based
on the Gaussian distribution model, and calculates the co-
variance according to an enlarged number of promising in-
dividuals. In contrast to solely relying on solutions from the
current generation to estimate the Gaussian model, ùê∏ùê∑ùê¥2
[69] incorporates a strategy where a set number of high-
quality solutions from previous generations are retained in
an archive. These historical solutions are then utilized to aid
in estimating the covariance matrix of the Gaussian model.
Huang et al.: Preprint submitted to Elsevier
Page 3 of 23


--- Page 4 ---
When Large Language Model Meets Optimization
Dong et al. [38] introduced a latent space-based EDA (LS-
EDA), which converts the multivariate probabilistic model
of Gaussian-based EDA into a principal component latent
subspace with reduced dimensionality. This transformation
effectively decreases the complexity of EDA while preserv-
ing its probability model, ensuring that crucial information
is retained. Consequently, LS-EDA enhances performance
scalability for large-scale global optimization problems. In
recent times, hybrid EDAs have emerged as a prominent re-
search focus. Li et al. [67] proposed IDE-EDA, an enhanced
version of DE achieved by integrating the EDA. Zhang et
al. [151] introduced a new hybrid evolutionary algorithm
for continuous global optimization problems, Zhou et al.
[160] proposed a fusion of an Estimation of Distribution
Algorithm (EDA) with both economical and costly local
search (LS) methods. This integration aims to leverage both
global statistical insights and individual location-specific
information for enhanced optimization performance.
3.2. Reinforcement learning methods for
optimization problems
Another famous learning paradigm is reinforcement
learning. Reinforcement learning learns and improves its
strategy by interacting with the environment, aiming to max-
imize long-term cumulative rewards. The aforementioned
heuristic algorithms typically find optimal or approximate
solutions by searching the solution space, guided by heuristic
information. However, they do not consider interaction with
the environment during the search process. Unlike super-
vised and unsupervised learning, in reinforcement learning,
agents can only learn through trial and error, rather than
relying on labeled data or searching for the inherent structure
of the data. Furthermore, reinforcement learning can be
categorized into classical reinforcement learning methods
and deep reinforcement learning methods, which will be
introduced below.
According to [65], classic RL methods can be divided
into model-based and model-free approaches. Model-free
methods can be further categorized into value-based, policy-
based, and actor-critic methods. Value-based methods seek
the optimal policy by estimating the value function. This
approach is suitable for smaller state spaces and discrete
action spaces but faces challenges to extend to continuous
action spaces and has the "high bias" problem. The error
between the estimated value function and the actual value
function is difficult to eliminate. Policy-based methods, on
the other hand, do not require estimating the value function.
Instead, they directly fit the policy function using neural
networks. By training and updating the policy parameters,
the optimal policy is generated. This method is suitable for
continuous action spaces but requires sampling a large num-
ber of trajectories, and there is a huge difference between
each trajectory, leading to the "high variance" problem.
To address the contradiction between high bias and high
variance, the actor-critic method emerges. The actor-critic
method constructs an agent that can both output policies and
evaluate their quality in real-time using the value function.
Generally, an actor-critic network consists of two parts: the
actor network and the critic network. The actor network is
used to generate policies to approximate the policy function,
while the critic network is used to evaluate policies to ap-
proximate the value function. Representative works of these
methods will be introduced below.
Q-learning[132] is a classic value-based RL algorithm
and is currently the most widely used model-free RL al-
gorithm. Q-learning first initializes a Q-function, typically
represented as a Q-table. It selects an action based on the
current state using the ùúñ-greedy strategy, performs the se-
lected action, and observes the reward obtained and the next
state transitioned to. The Q-function is updated using the
Bellman equation, repeat the above steps and update the Q
value until the stop condition is reached. Finally, the optimal
policy is extracted based on the learned Q-function. Double
Q-learning[52] is an improved version of the Q-learning
algorithm which alleviates the overestimation problem by
using two Q-functions. In each round of interaction with the
environment, one of the value function estimators is alter-
nately selected to choose actions, while the other is used for
action value estimation. Existing research has demonstrated
that the Double Q-learning method achieves higher stability
and greater long-term returns.
The REINFORCE algorithm[135], also known as the
Monte Carlo Policy Gradient Reinforcement Learning al-
gorithm, is a classical policy gradient method. The goal of
the REINFORCE is to update parameters along the gradient
direction to maximize the objective function. On the basis of
the classical policy gradient algorithms, Trust Region Pol-
icy Optimization (TRPO)[108] ensures that the Kullback-
Leibler Divergence between the new and the old policy
does not exceed a predefined threshold during each pol-
icy update. This threshold represents the "trust region" of
policy updates, indicating the similarity between the new
and old policies, thereby ensuring the stability of policy
optimization. Proximal Policy Optimization (PPO)[109] is
an improvement over TRPO, which is simpler to implement
and requires less computation in practical use.
Actor-Critic (AC)[60] algorithm combines the advan-
tages of both policy-based and value-based methods. It
learns both the policy and the value function simultaneously.
The actor trains the strategy based on the value function
feedback from the critic, while the critic trains the value
function using the Time Difference Method (TD) for single
step updates. The aforementioned REINFORCE algorithm
uses a stochastic policy function, which outputs the proba-
bility distribution of actions for a given state, and then selects
an action based on the probability distribution. In contrast to
the REINFORCE algorithm, Deterministic Policy Gradient
(DPG)[112] algorithm employs deterministic policy func-
tion, which directly outputs a deterministic action for given
states and update policy parameters by maximizing expected
returns. DPG integrates deterministic policy gradients only
in the state space, greatly reducing the need for sampling
and enabling the handling of larger action spaces. However,
the coupling between policy updates and value estimation
Huang et al.: Preprint submitted to Elsevier
Page 4 of 23


--- Page 5 ---
When Large Language Model Meets Optimization
in DPG leads to insufficient stability, particularly being
highly sensitive to hyperparameters. The difficulty of tuning
hyperparameters in actor-critic algorithms and challenges in
reproducibility make them hard to apply in practical scenar-
ios. When extended to application fields, the robustness of
the algorithm is also one of the most concerned core issues.
The above methods are relatively simple and intuitive,
easy to understand and implement, and suitable for smaller-
scale problems. In relatively fewer samples, classical RL
methods typically achieve good performance, especially in
stable environments and reward settings. More importantly,
the above method exhibits strong interpretability, allow-
ing for a clear understanding of the agent‚Äôs behavior and
decision-making process within the environment.
Although RL has achieved remarkable achievements,
previous methods have often struggled to handle high-
dimensional data such as images, text, etc. This limitation
has constrained its ability to deal with complex tasks and en-
vironments. Classical RL methods often find it challenging
to strike a good balance between exploration and exploita-
tion, leading to susceptibility to local optima, especially in
high-dimensional and complex environments where issues
of insufficient exploration are more pronounced. The reason
for the aforementioned situation is that RL algorithms, like
other algorithms, face challenges such as memory complex-
ity, computational complexity, sample complexity, etc[6].
However, the powerful representation learning capability
and function approximation capability of deep learning
bring a completely new solution for RL.
Deep learning is a branch of machine learning aimed at
using multi-layer neural network models to learn representa-
tions and features of data, and solving various tasks through
these representations and features. Deep learning models
have strong nonlinear function approximation capabilities,
allowing them to learn more complex and accurate data
representations and features. Deep learning models have
strong generalization and representation learning capabil-
ities, enabling them to learn more accurate and effective
policies or value functions, thereby allowing RL agents to
tackle more complex and high-dimensional tasks and envi-
ronments, achieving higher performance and accuracy. Deep
reinforcement learning (DRL) is the product of integrating
both RL and DL. Similarly, DRL methods are also divided
into three types: value-based, policy-based, and actor-critic
methods. Among them, value-based DRL employs DL to ap-
proximate value functions, while policy-based DRL uses DL
to approximate policies and solve decision-making policies
based on policy gradient rules. The following will introduce
representative works in DRL.
In 2013, Mnih et al. from DeepMind combined DL with
Q-learning, proposing the groundbreaking Deep Q-network
(DQN)[94]. DQN is a DRL algorithm based on Q-learning.
On one hand, it utilizes deep neural networks as value
function estimators, and on the other hand, it introduces ex-
perience replay and target networks. The experience replay
mechanism breaks the high dependency between sampled
samples, while the target network alleviates the instability
of neural networks during training. These two mechanisms
work together to enable the DQN algorithm to achieve
performance close to or even surpassing human levels in
most Atari games. Double DQN[127], based on Double Q-
learning, is an improvement over DQN. Similar to how DQN
extends Q-learning, DDQN addresses the overestimation is-
sue by using two Q-networks. DDQN achieves better stabil-
ity and algorithm performance compared to DQN. In 2017,
Dai et al. combined RL with graph embedding and proposed
S2V-DQN[58]. They utilized a graph embedding network
called structure2vec (S2V) to represent the policy in greedy
algorithms and employed multi-step DQN to learn greedy
policies parameterized by the graph embedding network.
The S2V-DQN algorithm generates high-quality solutions
faster, sometimes finding better solutions than commercial
solvers within a longer timeframe.
In 2015, inspired by the ideas of DQN, Lillicrap et
al. combined neural networks with the DPG algorithm to
propose DDPG[71]. DDPG employs two different parame-
terized deep neural networks to represent the value network
and the deterministic policy network. The policy network
is responsible for updating the policy, while the value net-
work outputs the Q-values for state-action pairs. Similar to
DQN, DDPG also utilizes target networks to overcome the
instability issues during network updates. Zhang et al. [152]
considered the feasibility constraints of NP-hard problems,
embedding heuristic functions into the transformer architec-
ture, and applying DRL to combinatorial optimization prob-
lems with feasibility constraints. Ma et al.[85] introduced
a Graph Pointer Network (GPN) to solve the classical TSP
problem and combined it with a hierarchical RL framework
to address the TSP problem with time window constraints.
Multiple Traveling Salesman Problems (MTSP) are more
complex, and Hu et al.[54] designed a network consisting
of a shared graph neural network and distributed policies
to learn a common policy expression suitable for MTSP.
Experimental results demonstrated the effectiveness of this
approach on large-scale problems.
In 2016, Mnih et al.[93] developed an improved Actor-
Critic algorithm called A3C (Asynchronous Advantage
Actor-Critic). By utilizing asynchronous gradient descent to
optimize the parameters of deep neural networks (DNNs),
A3C significantly improved the efficiency of policy opti-
mization. Vinyals et al. [129] proposed the Pointer Network
model for solving combinatorial optimization problems,
which initiated a series of research studies on utilizing
DNN for solving combinatorial optimization problems. This
model was inspired by the Seq2Seq model in machine trans-
lation. It employs a deep neural network-based encoder to
encode the input sequence of the combinatorial optimization
problem (such as city coordinates), then utilizes a decoder
and attention mechanism to compute the selection proba-
bilities of each node. Finally, it selects nodes in an autore-
gressive manner until obtaining a complete solution. Due
to the supervised nature of the training method proposed by
Vinyals et al.[129], the quality of the solutions it obtains will
Huang et al.: Preprint submitted to Elsevier
Page 5 of 23


--- Page 6 ---
When Large Language Model Meets Optimization
never exceed the quality of the sample solutions. Recogniz-
ing this limitation, Bello et al.[9] employed a RL approach to
train the Pointer Network model. They treated each problem
instance as a training sample, using the objective function of
the problem as feedback signals, and trained the model based
on REINFORCE. They also introduced a critic network as
a baseline to reduce training variance. Furthermore, Nazari
et al.[96] extended the Pointer Network to handle dynamic
VRP problems. They replaced the LSTM in the Encoder in-
put layer with a simple one-dimensional convolutional layer,
effectively reducing computational costs. While maintaining
optimization effectiveness, the training time was reduced by
60%.
In recent years, the Transformer[128] has achieved
tremendous success in the field of natural language pro-
cessing. Its multi-head attention mechanism enables better
extraction of deep features from problems. In view of this,
several recent studies have drawn inspiration from the Trans-
former for solving combinatorial optimization problems.
Deudon et al. [36] improved traditional pointer network
models by incorporating ideas from the Transformer. They
utilized a similar structure to the Transformer in the encoder,
while the decoder employed linear mapping of the deci-
sions from the last three steps to obtain a reference vector,
thereby reducing model complexity. The attention calcu-
lation method remained the same as in traditional Pointer
Network models, and the classic REINFORCE method
is still used to train the model. Kool et al.[61] proposed
a new method capable of solving multiple combinatorial
optimization problems using attention mechanisms. The
attention calculation method in this model adopted the self-
attention computation method from the transformer, with
additional computational layers to enhance performance.
They further designed a greedy rollout baseline to replace
the Critic network, leading to significant improvements in
optimization performance.
Deep reinforcement learning, as one of the most popu-
lar research directions in the field of artificial intelligence,
has shown great potential in solving complex tasks and
addressing various real-world problems. However, DRL also
has its limitations, such as data requirements, sample ef-
ficiency, computational resources, interpretability, etc. De-
spite achieving some success in both research and applica-
tion domains, DRL fundamentally remains constrained to
simulated environments with ideal, highly structured exper-
imental data design. They heavily rely on the design and
training of specific models. Therefore, there is a growing
interest in the design and optimization of automatic algo-
rithms.
4. LLMs as Optimization
4.1. LLMs as the Black-box Optimization Search
Model
There is a strong alignment between Large Language
Models (LLMs), which are powerful in generating cre-
ative texts, and Evolutionary Algorithms (EAs), capable of
Figure 1: The split of LLMs assist optimization algorithms.
discovering diverse solutions to complex real-world prob-
lems [24]. With their powerful knowledge storage and
generation capabilities, LLMs can support optimization
algorithms in problem decomposition, parameter search,
and solution generation. As show in Fig. 1, LLMs can be
classified into two categories for enhancing optimization
algorithms: 1) One category is to use the large model as
the search operator of the black-box optimization model,
which makes full use of the knowledge storage capacity and
experience of LLMs and thus can effectively reduce the input
of workforce. 2) The other category of approach is to give
full play to the generative capacity of the large model and
to make full use of the understanding of the optimization
problem by the large model as the input of model and
then generate suitable optimization algorithm configurations
or generate optimization algorithms for solving specific
problems. The use of large models to assist the design of
optimization algorithms has achieved preliminary research
and widespread attention. How to give full play to the
advantages of large models in the design of algorithms and
integrate them into the optimization algorithm framework
has become the key to the research in this field. A detailed
classification of what is the goal of the auxiliary optimization
model with LLM is shown in Table 1.
Yang et al. [143] proposed the Optimization by PROmpt-
ing (OPRO), which utilizes natural language descriptions to
guide LLMs in searching for solutions for optimization prob-
lems. This method is particularly effective for derivative-
free optimization scenarios that are common in real-world
applications. Technically, OPRO includes previously gen-
erated solutions and their values, allowing the LLM to
iteratively improve upon solutions. The framework also in-
telligently balances benefits and costs, crucial for effective
optimization, by adjusting the sampling temperature of the
LLM to encourage both refinement of existing solutions
and exploration of new ones. Chen et al. [27] explored cue
optimization methods in multi-step tasks to improve task
execution efficiency. By constructing a discrete LLM-based
prompt optimization framework, the framework automati-
cally provides suggestions for improvement by integrating
human-designed feedback rules and preference alignment.
It is noted that while LLM performs well in single-step
tasks, real-world multi-step tasks pose new challenges, such
as more complex cueing content, difficulty in evaluating
individual step impacts, and the fact that different people
may have different task execution preferences. To address
Huang et al.: Preprint submitted to Elsevier
Page 6 of 23


--- Page 7 ---
When Large Language Model Meets Optimization
Table 1
The detailed classification of what is the goal of the auxiliary optimization model with LLM
Research Field
Category
LLM‚Äôs Goal
Related Works
LLMs as the Black-box
Optimization Search Model
Guided Search Operator
LLMs combined with natural language
descriptions or manual a priori knowl-
edge search for suitable optimization al-
gorithms
[143, 27, 153, 3]
Heuristic Search Operators
LLMs rely on knowledge storage and
problem analysis capabilities to search for
suitable optimization algorithms
[146,
159,
90,
79]
Multi-Objective
Optimization
LLMs search for multiple objectives and
trade-offs between the importances of the
inter-objectives
[13, 75, 11]
LLMs as the Generator of
Optimization Algorithms
Option Generate with the
Cognitive of LLMs
LLMs analyze the problem and generate
suitable optimization algorithm options
based on a priori optimization algorithm
knowledge
[150, 80, 86]
Algorithm Generate with
with Chain-of-Thought
Generate optimization algorithms, opti-
mization steps, or code with the help of
the chain of thought ability of LLM to
solve complex problems
[77, 76, 41, 101,
12, 53]
these issues, the researchers introduced human feedback,
leveraging human expertise in providing input and combin-
ing it with a genetic algorithm-style framework to optimize
cues. For the current emerging optimization problem of
neural architecture search, Zhang et al. [153] designed an au-
tomated machine learning (AutoML) system based on large-
scale language models (LLMs), AutoML-GPT. AutoML-
GPT utilizes a GPT model as a bridge to connect multiple
AI models and dynamically uses optimized hyperparameters
to train the models. The system automatically generates
corresponding prompt paragraphs to search for the optimal
model architecture and parameters by dynamically taking
inputs from user requests and data cards. It then automat-
ically executes the entire experimental process, from data
processing to model architecture, hyperparameter tuning,
and prediction of training logs. AhmadiTeshnizi et al. [3]
proposed a large language model (LLM)-based agent called
OptiMUS. OptiMUS is designed to formulate and solve
mixed-integer linear programming (MILP) problems from
natural language descriptions. It is capable of developing
mathematical models, writing and debugging solver code,
developing tests, and checking the validity of the generated
solutions.
4.1.1. Heuristic Search Operators
To tackle NP-hard combinatorial optimization problems
(COPs) using large language models (LLMs) as Hyper-
Heuristics (LHHs), Ye et al. [146] proposed ReEvo, a frame-
work that emulates the reflective design approach of hu-
man experts. It leverages the scalable inference capabilities
of LLMs, Internet-scale domain knowledge, and powerful
evolutionary search strategies. ReEvo operates by generating
heuristics through LLMs with minimal human intervention,
offering an open-ended heuristic space and the potential for
Knowledge and competence beyond that of human experts.
The research aims to address the complexity and heterogene-
ity of COPs by automating the design process of heuristics,
which traditionally requires extensive trial and error from
domain experts. ReEvo incorporates a dual-level reflection
mechanism, where short-term reflections are used to analyze
heuristics‚Äô relative performance, and long-term reflections
are accumulated to guide their evolution. This reflective pro-
cess allows ReEvo to adapt and improve heuristics over time,
leading to smoother fitness landscapes and more effective
search results. Zhong et al. [159] introduced a groundbreak-
ing approach to the design of metaheuristic algorithms by
utilizing the capabilities of the large language model (LLM)
ChatGPT-3.5. They proposed a animal-inspired metaheuris-
tic algorithm named Zoological Search Optimization (ZSO),
which is developed to tackle continuous optimization prob-
lems. The ZSO algorithm is designed to mimic the collective
behaviors of animals, incorporating two key search opera-
tors: the prey-predator interaction operator and the social
flocking operator, which together balance exploration and
exploitation effectively. A similar approach called Language
Model Crossover (LMX) utilizes large pre-trained Language
Models (LLMs) to generate new candidate solutions. [90]
LMX does this by combining the parent solutions into a
prompt and then feeding that prompt into the LLM to col-
lect offspring from the output. This approach is simple
to implement and generates high-quality progeny across
various domains, including binary strings, mathematical
expressions, English sentences, image generation prompts,
and Python code. Liu et al. [79] explore the potential of
using Large Language Models (LLMs), such as GPT-4, to
generate novel hybrid population intelligence optimization
algorithms. The research focuses on using GPT-4 to iden-
tify and decompose six population algorithms that perform
well in sequential optimization: particle swarm optimization
(PSO), cuckoo search (CS), artificial bee colony algorithm
Huang et al.: Preprint submitted to Elsevier
Page 7 of 23


--- Page 8 ---
When Large Language Model Meets Optimization
(ABC), grey wolf optimizer (GWO), self-organizing migra-
tion algorithm (SOMA), and whale optimization algorithm
(WOA) by constructing hints to guide the LLMs to search
for the optimal from the current population‚Äôs parent solution.
INSTINCT (INSTruction optimization using Neural bandits
Coupled with Transformers) [72] is a black-box LLMs cue
optimization method. The method employs a novel neural
band algorithm, Neural Upper Confidence Bound (Neu-
ralUCB), in place of the Gaussian Process (GP) model in
BO.NeuralUCB uses a neural network (NN) as a proxy
while retaining the theoretical basis of the trade-off between
exploration and exploitation in BO. Theoretical basis for
the trade-off between exploration and exploitation. More
importantly, NeuralUCB allows for the natural coupling of
NN agents with hidden representations learned from pre-
trained transformers (i.e., open-source LLMs), significantly
improving algorithmic performance.
4.1.2. Multi-Objective Optimization
Brahmachary et al. [13] introduces an approach to nu-
merical optimization using Large Language Models (LLMs)
called Language-Model-Based Evolutionary Optimizer (LEO),
which leverages the reasoning capabilities of LLMs to
perform zero-shot optimization across a variety of scenarios,
including multi-objective and high-dimensional problems.
LEO lies in its population-based strategy, which incorpo-
rates an elitist framework consisting of separate explore and
exploit pools of solutions. This strategy not only harnesses
the optimization capabilities of LLMs but also mitigates the
risk of getting stuck in local optima. The method is distinct
from other auto-regressive, evolutionary, or population-
based methods as it uses LLMs to generate new candidate so-
lutions, providing a unique balance between exploration and
exploitation. It is shown through a series of test cases that
LEO is not only capable of handling single-objective but also
of solving multi-objective optimization problems well. Liu
et al. [75] leverage the capabilities of large language models
(LLMs) to design operators for multi-objective evolutionary
algorithms (MOEAs). The research addresses the challenges
associated with the manual design of search operators in
MOEAs, which often require extensive domain knowledge
and can be time-consuming. The authors propose a method
for decomposing the multi-objective optimization problem
(MOP) into several single-objective subproblems (SOPs).
LLMs are employed as search operators for each subproblem
through prompt engineering. This allows the LLM to serve
as a black-box search operator in a zero-shot manner without
problem-specific training. Bradley et al. [11] introduce a
new approach called Quality-Diversity through AI Feedback
(QDAIF) that combines evolutionary algorithms and large-
scale language models (LLMs) to generate high-quality and
diverse candidates for optimization algorithms. The core
idea of QDAIF is to use language models to create variants
and evaluate the quality and diversity of the candidates. The
EA is responsible for maintaining the library of optimization
algorithms and replacing the newly generated higher quality
and more diverse solutions to the relevant positions in the
library based on the evaluation of the LLMs to achieve an
iterative optimization search process.QDAIF can find a set of
higher quality and more diverse solutions within the search
space, which is a successful application of the LLMs in QD
problems.
4.2. LLMs as the Generator of Optimization
Algorithms
Optimization algorithms usually require the design of
suitable optimization schemes based on the specified tasks.
Due to the problem-understanding and algorithmic analysis
capabilities of LLMs, the design of optimization methods
based on LLMs can generate suitable optimization method
selection and combination schemes compared to the opti-
mization methods in the pre-LLM era [124, 123].
4.2.1. Option Generate with the Cognitive of LLMs
Zhang et al. [150] explore the use of large language mod-
els (LLMs) to generate optimal configurations during Hy-
perparametric Optimization (HPO). The generation method
does not rely on a predefined search space and consists
of selecting parameters that can be optimized and specify-
ing bounds for these parameters. Furthermore, the process
treats the code of the specified model as hyperparameters
to be output by the LLM, which goes beyond the capabil-
ities of existing HPO methods. Liu et al. [80] proposes a
system named AgentHPO, which leverages the advanced
capabilities of LLMs to streamline the HPO process, tra-
ditionally a labor-intensive and complex task that requires
significant computational resources and expert knowledge.
AgentHPO lies in its unique architecture that incorporates
two specialized agents: the Creator and the Executor. The
Creator agent interprets task-specific details provided in nat-
ural language and generates initial hyperparameters (HPs),
emulating the role of a human expert. It utilizes extensive
domain knowledge and sophisticated reasoning to propose
HP configurations that are expected to yield optimal model
performance. Ma et al. [86] explored the effectiveness of
Large Language Models (LLMs) as cueing optimizers. They
find that LLM optimizers struggle to accurately identify
the root cause of errors during reflection and often fail to
generate appropriate cues for the target model through a
single cueing optimization step, even when semantically
valid. Further, they proposed a new paradigm of "automated
behavioral optimization" designed to more controllably and
directly optimize the behavior of the target model.
4.2.2. Algorithm Generate with with
Chain-of-Thought
Liu et al. [77] proposed a approach called Algorithmic
Evolution using Large Language Models (AEL), which aims
to automate the generation of efficient algorithms for spe-
cific optimization problems.AEL creates and improves algo-
rithms by interacting with Large Language Models (LLMs)
within an evolutionary framework, eliminating the need
for model training and significantly reducing the need for
Huang et al.: Preprint submitted to Elsevier
Page 8 of 23


--- Page 9 ---
When Large Language Model Meets Optimization
domain knowledge and expert skills. The constructive algo-
rithms generated by AEL outperform hand-crafted heuris-
tics and algorithms generated directly from LLMs based
on the Traveling Provider Problem (TSP) example. Fur-
ther, they introduce a improved framework for automated
algorithm design that combines evolutionary computation
and LLMs [76]. By automating algorithm design, combi-
nation, and modification, the AEL framework significantly
reduces manual effort and eliminates the need for model
training. The researchers used the AEL framework to de-
sign a Guided Local Search (GLS) algorithm for solving
the TSP. The PromptBreeder (PB) system [41] utilizes the
Chain-of-Thought Prompting strategy, which can signifi-
cantly improve the reasoning ability of Large Language
Models (LLMs) in different domains. It is a generalized
mechanism for self-referential self-improvement of large
language models (LLMs). At the heart of the system is
self-reference: not only do the task prompts evolve, but
the mutation prompts used to generate them also improve
over time.PB outperforms existing state-of-the-art prompt-
ing strategies, such as Chain-of-Thought and Plan-and-Solve
prompts, in several commonly used benchmark tests‚Äîand-
Solve prompts. Pluhacek et al. [101] identified and de-
composed six population algorithms that perform well on
sequential optimization problems through LLM by aug-
menting the population. Enhanced Swarm Exploration and
Exploitation Optimizer(ESEEO) aims to maintain popula-
tion diversity and effectively balance exploration and ex-
ploitation by combining elements of Particle Swarm Op-
timization (PSO), Cuckoo Search (CS), and Artificial Bee
Colony (ABC). Combining Limited Evaluation Population
Optimizer (LESO) Designed to solve expensive optimiza-
tion problems with a limited number of objective function
evaluations, LESO combines the features of PSO, Grey Wolf
Optimizer (GWO), and ABC to achieve effective exploration
and exploitation within a limited number of assessments.
LLMs are a challenge in Genetic Programming (GP) ap-
proaches. Bradley et al. [12] present an optimized algorithm
generation tool for implementing LLMs that converts nat-
ural language descriptions into implementation code and
automatically repairs program errors. In addition, this paper
presents two uses of LLMs as evolutionary operators: differ-
ence modeling and LMX crossover. The former is a language
model specialized for predicting code discrepancies, and the
latter is a technique for generating candidate solutions using
multiple parents. Similarly, Hemberg et al. [53] proposed a
LLM-based GP algorithm, called LLM-GP, for generating
optimization algorithm code. Unlike conventional GP algo-
rithms, LLM-GP harnesses the power of pre-trained pattern
matching and sequence complementation capabilities of the
LLM. This unique feature allows for the design and im-
plementation of genetic operators, paving the way for more
efficient and effective algorithm generation.
5. Optimization algorithms optimize large
language models
Large Language Models (LLMs) have exhibited excep-
tional proficiency in many natural language processing tasks,
encompassing text generation and sentiment analysis. Nev-
ertheless, the task of optimizing their performance and ef-
ficiency continues to be a crucial obstacle, especially in
intricate and unpredictable circumstances. Optimization Al-
gorithms (OA) have emerged as a promising method to
improve LLMs. OA utilizes the concepts of natural selection
to perform repeated searches inside the parameter space
of LLMs [24, 139]. It focuses on tasks like prompt engi-
neering, model architecture optimization, hyperparameter
setting, and multi-task learning. This holistic strategy seeks
to discover optimal solutions for problems that have exten-
sive search spaces, thereby enhancing the effectiveness of
LLMs in several fields, such as natural language processing,
software engineering, and neural architecture search. In this
section, we will delve into the complexities of OA optimiza-
tion for LLMs, exploring various strategies such as model
tuning, prompt tuning and network architecture search to
unlock the full potential of these transformative language
models. The framework of the optimization algorithm to
optimize LLMs is shown in Fig. 2.
5.1. Optimize model tuning
5.1.1. Multi-task learning optimization
Multi-task learning (ML) optimization of large mod-
els is an approach that uses optimization methods such as
evolutionary algorithms to optimize model architectures for
multiple tasks simultaneously. Through multi-task learning,
researchers identify optimal architectures for multiple target
tasks simultaneously in large pre-trained models [24, 78].
The advantage of this approach is the ability to share and
interactively learn relevant information between different
tasks, thus improving the generalization ability and effi-
ciency of the model. Table 2 summarises the main optimiza-
tion model tuning class methods
The optimization strategy of multi-task learning reduces
training costs, enhances model performance and improves
adaptability across diverse domains and tasks by concur-
rently identifying optimal model architectures for multi-
ple objectives using evolutionary algorithms [133]. For ex-
ample, Choong et al. [30] proposed the concept of a di-
verse set of compact machine learning model sets designed
to efficiently address multiple target architectures in large
pre-trained models through an evolved multi-task learning
paradigm. Baumann et al. [8] present an evolutionary multi-
objective approach designed to optimize prompts in large
language models (e.g., ChatGPT), demonstrating its effec-
tiveness in crafting prompts in a sentiment analysis task
that effectively captures conflicting emotions. Yang et al.
[144] treated instruction generation as an evolutionary multi-
objective optimization problem, using a large-scale language
model to simulate instruction operators in order to improve
the quality and diversity of generated instructions.
Huang et al.: Preprint submitted to Elsevier
Page 9 of 23


--- Page 10 ---
When Large Language Model Meets Optimization
Figure 2: OAs methodological framework for optimizing LLMs
Table 2
Summary of model tuning based on OAs
Algorithms
Main tasks
0bjectives
Using OAs
Multi-task
learning
[30]
JATs Implementation of One-off Diverse Com-
pact Set of Sets Machine Learning Models
Multi-tasking
and
multi-objective
optimization
Neuroevolutionary
multitasking
EMO-Prompts [8]
Generate prompts that lead LLM to produce
text with two conflicting emotions
Evolutionary
Multi-
Objective
NSGA-II
Structured
Pruning
[59]
Subparts of the network with optimal perfor-
mance after fine-tuning for model compression
Multi-objective NAS
Weight Shared NAS
LLM-Pruner [87]
Model compression in a task agnostic manner
Single-objective
Low-Rank Adaptation
Meanwhile, Gupta and Bali et al. [7, 48] have inves-
tigated the use of multi-objective multi-task evolutionary
algorithms, such as the MO-MFEA algorithm, in creating
task-specific, small-scale models derived from Large Lan-
guage Models (LLMs) in the field of online search archi-
tecture study. These specialized models are created from
the LLM as a general base model and exhibit enhanced
performance or more compression in different application
fields and neural network designs.
5.1.2. Based on structural pruning
Structural pruning optimizes large language models
(LLMs) by selectively removing non-critical coupled struc-
tures based on gradient information, effectively reducing
model size while preserving functionality and ensuring task-
agnosticism. Structural pruning is an essential optimization
technique used to enhance pre-trained LLMs for subsequent
tasks, such as text categorization and sentiment analysis.
Structural pruning, as suggested by Klein [59], seeks to
uncover numerous subnetworks of LLMs that achieve a com-
promise between performance and size, making them easier
to use in different real-world applications. This approach
employs a multi-objective local search algorithm to identify
numerous Pareto-optimal subnetworks effectively. It does
this by minimizing evaluation costs via weight sharing. Ma
et al. [87] propose the LLM-Pruner approach to optimize
large language models by selectively removing non-critical
coupling structures based on gradient information to achieve
efficient compression while preserving functionality and
task agnosticism. Gholami et al. [44] demonstrate that
weight pruning can be used as an optimisation strategy for
the Transfer architecture, proving that judicious pruning can
significantly reduce model size without sacrificing perfor-
mance, thus contributing to bridging the gap between model
efficiency and performance.
5.2. Prompt optimization
Prompt tuning, often referred to as optimization for
prompt tuning, is a method used to fine-tune large language
model (LLMs) prompts. This methodology does not need
access to the underlying model parameters and gradients.
This strategy is especially beneficial for closed-source mod-
els that have limited access. Prompt optimisation enhances
the efficacy of model creation in fewer or zero-shot scenarios
by fine-tuning the input prompts. Optimization algorithms
(OAs), renowned for their adaptability and efficiency in situ-
ations where the internal workings are unknown, are used to
discover prompts that improve job performance just by using
Huang et al.: Preprint submitted to Elsevier
Page 10 of 23


--- Page 11 ---
When Large Language Model Meets Optimization
Table 3
Summary of optimization prompts
Algorithms
Main tasks
Types
Using OAs
BBT [119]
Optimising large pre-trained language models
published as a service
Blackbox optimization
Derivative-Free
Optimization, CMAES
BBTv2 [118]
Adjustment of cues by gradient-independent
methods
Blackbox optimization
CMAES
BBT-VLMS [148]
Black-box optimization of visual language mod-
els
Blackbox optimization
Derivative-Free
Optimization,CMAES
Textual
inversion
[40]
Optimizing the embedded representation of spe-
cific concepts in text
Continuous
Iterative
Evolutionary
Strategy,CMAES
Clip-tuning [21]
Enabling
optimisation
of
prompts
without
model weight access rights
Continuous
Derivative-free optimiza-
tion, CMAES
RGLF [110]
Effective hint tuning for LLMs in resource-
constrained black-box API settings
Continuous
CMAES
GrIPS [102]
Prompts for Improving Large Language Models
Discrete
Edit-based
Search,
Ge-
netic Algorithms
GPS [155]
Automatically
search
for
high-performance
prompts to optimize model performance for
specific tasks
Discrete
Genetic Algorithm
Plum [99]
Optimizing and customising large pre-trained
language models
Discrete
Metaheuristics
the outcomes of LLMs reasoning. Prompt tuning may be
categorized into two distinct types: continuous and discrete.
Continuous prompt tuning employs continuous optimisation
algorithms, such as CMAES [51], to improve the quality of
embedding prompts [24]. This process involves techniques
like partitioning and subspace decomposition to enhance the
embedding space. Conversely, discrete prompt tuning uses
discrete optimization algorithms to explore the prompt space
directly. It utilizes specialized genetic operators to adjust
prompts in order to address the problem of combinatorial
explosions in the search space. Table 3 summarises the main
methods of optimizing prompts
5.2.1. Continuous prompt optimization
Continuous prompt tuning is used to optimize the perfor-
mance of LLMs and is often employed to tune the embed-
ding of prompts. The embedding vectors of the prompts are
iteratively tuned to maximize the performance of the model
on a particular task, thus improving the quality and per-
formance of the model generation [24]. Continuous prompt
tuning typically explores different strategies and techniques,
such as stochastic embedding, subspace decomposition, and
knowledge distillation, in order to improve the embedding
quality and the search performance for the optimization
of large models. For example, Sun et al. [118] presented
BBTv2, an improved version of Black-Box Tuning, which
uses a divide-and-conquer gradient-free algorithm to op-
timize prompts at different layers of pre-trained models,
achieving comparable performance under few-shot settings.
Fei et al. [40] introduced a gradient-free framework for
optimizing continuous textual inversion in an iterative evolu-
tionary strategy. It accelerates the optimization process with
minimal performance loss and compares performance with
gradient-based models with variant GPU/CPU platforms.
Pryzant et al. [103] proposed Automatic Prompt Optimi-
sation (APO) using numerical gradient descent techniques
to automatically improve the prompts of Large Language
Models (LLMs), obtaining significant performance gains
in various NLP tasks and jailbreak detection. Zheng et al.
[158] Black-box prompt optimization using subspace learn-
ing (BSL) enhances the versatility of prompt optimization
across tasks and LLMs by identifying common subspaces
through meta-learning, ensuring competitiveness across a
variety of downstream tasks.
Recently, some researchers have used techniques such as
knowledge distillation, variational reasoning, and federated
learning to improve search efficiency, generalization, and
security. For example, Shen et al. [110] presented techniques
for adapting large pre-trained language models (PLMs) to
downstream tasks using only black-box API access, achiev-
ing competitive performance with gradient-based methods
while also considering predictive uncertainty in prompts.
Sun et al. [117] present a set of techniques for improving the
efficiency and performance of black-box optimization (BBT-
RGB) for tuning large language models without access to
gradient and hidden representations, demonstrating its ef-
fectiveness in a variety of natural language understanding
tasks. Han et al. [50] proposed GDFO, which ensembles
gradient descent and derivative-free optimization for opti-
mising task-specific successive prompts of large pre-trained
language models in black-box tuning scenarios, obtaining
significant performance gains over previous state-of-the-
art approaches. Sun et al. [116] proposed FedBPT for fed-
erated black-box prompt tuning, a framework for efficient
and privacy-preserving fine-tuning of pre-trained language
models through collaborative prompt optimization without
access to model parameters, reducing communication and
memory costs while maintaining competitive performance.
Huang et al.: Preprint submitted to Elsevier
Page 11 of 23


--- Page 12 ---
When Large Language Model Meets Optimization
Chai et al. [21] introduced the Clip-Tuning technique to
enhance search efficiency and offer more detailed and di-
verse evaluation feedback throughout the black-box tuning
procedure. Clip-tuning differs from utilizing a single random
projection matrix to reduce dimensionality in BBT. Instead,
it utilizes pre-trained sampling on dropout models during
inference. This process generates many subnetworks that act
as predictive projections of samples in the original high-
dimensional space. The search technique achieves faster
convergence to the ideal solution by aggregating rewards
from predictions made by several subnetworks.
Continuous prompt tuning is a method for optimising the
performance of large language models where prompts are
represented as continuous vectors that exist in a continuous
embedding space. These vectors are optimized in a con-
tinuous embedding space. The search space is continuous
and can be differentiated, allowing the use of gradient-
based optimization techniques. Continuous prompt tuning
is suitable for black-box optimization scenarios where the
internal structure of the model and gradient information are
not accessible. However, this approach may require more
computational resources due to the complex mathematical
operations and gradient calculations involved.
5.2.2. Discrete prompt optimization
Discrete prompt optimization is a method for finding
optimal prompts in a pre-trained language model, where the
prompts are represented as discrete text sequences. These
methods typically use genetic algorithms, particle swarm
optimization, or other heuristic-based search methods to
search in a discrete prompt space to find the optimal prompt
sequence [139, 24]. Unlike continuous prompt tuning, dis-
crete prompt optimization focuses more on tuning prompts
at the level of text sequences and is suitable for tasks based
on text sequences, such as text generation or classification.
Before the emergence of large language models, researchers
have been inclined to investigate the application of opti-
mization methods to enhance the performance of pre-trained
language models. For instance, Greedy Teaching Prompt
Search (GrlPS) [102] uses a stepwise search approach with-
out gradients, whereas Genetic Prompt Search (GPS) [142]
is grounded in the ideas of genetic algorithms. The majority
of these research employ evolutionary algorithms (EAs) as
the primary search engine, whereas the language model
is tasked with generating and assessing potential prompts
[155]. Within the discrete prompt space, specialized genetic
operators are employed to fine-tune heuristics and directly
identify the most optimal prompts. This, in turn, enhances
the quality of the model‚Äôs response to a given task.
Typically, these studies employ optimization algorithms
as a search framework, where Large Language Models
(LLMs) are used to generate and evaluate prompts. Nev-
ertheless, it is important to acknowledge that this research
mainly concentrates on particular rapid engineering sit-
uations and has restricted breadth. To fully harness the
potential of discrete optimization in black-box prompt opti-
mization, it is necessary to tackle the issue of combinatorial
explosion in discrete search spaces. For example, Zhou et
al. [161] proposed a simple black-box search method called
ClaPS, which achieves state-of-the-art performance on a
variety of tasks and LLMs while significantly reducing the
search cost by clustering and pruning the search space to
focus on key prompting tokens that affect LLM prediction.
Yu et al. [148] proposed a black-box prompt tuning frame-
work for visual-verbal models, which optimizes visual and
verbal prompts in an intrinsic parameter subspace through an
evolutionary strategy, enabling task-relevant prompt learn-
ing without back-propagation. Pan et al. [99] introduced
meta-heuristic algorithms as a generic prompt learning
method, and demonstrated their effectiveness in black-box
prompt learning and Chain-of-Thought prompt tuning by
testing six typical methods and were able to discover more
understandable prompts, opening up more possibilities for
prompt optimisation. Lapid et al. [62] proposed a method for
attacking large language models using genetic algorithms to
reveal the vulnerability of the models to malicious manip-
ulation and to provide a diagnostic tool for assessing and
enhancing the consistency of language models with human
intentions. Guo et al. [46] presented EvoPrompt, a discrete
prompt optimization framework for the automatic optimiza-
tion of LLMs prompts using evolutionary algorithms. By
linking LLMs and EAs, the method achieved significant
performance improvements over manually designed prompts
and existing automatic prompt generation methods on 31
datasets, demonstrating the potential of combining LLMs
and EAs. Pinna et al. [100] present a method for improving
the generation of code for large language models using ge-
netic improvement techniques, which significantly improves
the quality of the generated code through user-supplied test
cases, demonstrating the potential of combining LLM with
evolutionary techniques.
Generally, discrete prompt optimisation is a method to
optimise the performance of large language models in a
black-box environment, which searches for optimal prompt
words or phrases in a discrete prompt space through tech-
niques such as genetic algorithms, heuristic search, and
clustering pruning without the need for internal gradient
information of the model. The advantages include effective
performance improvement without relying on the internal
information of the model in a black-box environment, and
adaptability to tasks with a small number of samples or zero
samples, while the disadvantages include the possibility of
facing a huge search space, the tendency to fall into local
optimums, the sensitivity of hyper-parameters, the limited
ability of generalisation, the difficulty of interpreting the
results, and the high dependence on the choice of evaluation
metrics.
5.2.3. Blackbox optimization prompt tuning
Black-box optimization of large models refers to the pro-
cess of optimizing and tuning large pre-trained models (e.g.,
large language models) within a black-box optimization
framework. Compared to traditional black-box optimization,
black-box optimization of large models is more challenging
Huang et al.: Preprint submitted to Elsevier
Page 12 of 23


--- Page 13 ---
When Large Language Model Meets Optimization
because the complexity of large pre-trained models and a
large number of parameters makes the optimization process
more complex and time-consuming. In black-box optimiza-
tion of large models, optimization algorithms typically op-
timize the performance of pre-trained models by interacting
with them and adjusting their inputs or parameters step-by-
step without having direct access to the internal structure or
parameters of the model.
In recent years, a number of researchers have focused on
the application of black-box optimization to large-scale lan-
guage models (LLMs) and visual-linguistic models, propos-
ing a variety of methods to optimize model performance
without accessing the model‚Äôs internal parameters or gradi-
ents. Yu et al. [149] optimize a visual language model using
a dialogue-feedback-based approach. Guo et al. [47] intro-
duced the collaborative black-box tuning (CBBT) technique.
Sun et al. [119] develop the a black-box tuning framework
for Language Models as a Service (LMaaS). Diao et al. [37]
proposed a black-box discrete prompt learning (BDPL) al-
gorithm. And the work of Yu et al. [148] introduces a black-
box prompt tuning framework for visual language models.
These studies demonstrate that in black-box scenarios where
model weights cannot be directly modified, external prompt
learning and optimization are used to effectively improve
model performance in image classification, text-to-image
generation, and adaptation to different downstream tasks.
5.3. Self-Tuning optimization
Compared to the initialization method under the Evo-
Prompt framework, which relies on hand-prompted opti-
mization. In recent years some researchers have proposed au-
tomated prompting methods. Use as a gene operator in EAs
to automatically create high-quality prompts for yourself and
others. Table 4 summarises the main Self-Tuning optimiza-
tion methods. For example, Singh et al. [113] applied an
interpretable auto-prompt (iPrompt) to generate a natural
language string that explains the data. Fernando et al. [41]
propose a self-improving mechanism for PromptBreder that
evolves and adapts cues for different domains, outperform-
ing existing strategies on arithmetic, common-sense reason-
ing, and hate speech classification tasks. Pryzant et al. [104]
proposed a simple and non-parametric solution, Automated
Prompt Optimisation (APO), which automatically performs
fast improvement prompts by using techniques inspired by
numerical gradient descent. Li et al. [68] proposed SPELL, a
black-box evolutionary algorithm that uses a large language
model to automatically optimize text style cues, demonstrat-
ing rapid improvements for a variety of text tasks.
Furthermore, LLMs can serve as a flexible prompt se-
lector for jobs that are not inside the domain it was trained
on. Self-tuning can operate inside a versatile language do-
main without being dependent on parameter updates [24].
For example, Zhang et al. [154] proposed Auto-Instruct,
an approach that utilizes the generative power of LLMs
to automatically improve the quality of instructions for a
variety of tasks, going beyond manually written instructions
and existing baselines in a variety of out-of-domain tasks,
with significant generalisability to other LLMs.
5.4. Optimize network architecture search
Prompt-based optimization tools improve the quality of
model output by optimizing the input format. Another ap-
proach known as LLM Network Architecture Search (NAS)
focuses on directly optimizing the architecture of LLM mod-
els, and in the context of Large Language Models (LLMs),
NAS can take a different form by optimizing the architecture
of the model directly rather than by tuning the parameters of
the model. Table 5 summarises the main optimized network
architecture search search methods
As the complexity of neural network models increases,
manually designing efficient network architectures becomes
time-consuming and challenging. NAS eases the burden on
researchers by automating the design process, allowing effi-
cient exploration of the vast search space to discover more
efficient, generalized and less resource-intensive model ar-
chitectures [81, 139, 163]. Previously NAS was optimized
by simulating the process of natural selection. It involves
the steps of randomly generating an initial population, se-
lection, crossover (or recombination, as it is called), and
mutation until termination conditions are met [39]. With the
development of deep learning techniques and the increase
of arithmetic power, the NAS field is also exploring new
optimization strategies to optimize large models. With the
increase of computational resources and the proposal of new
algorithms, the efficiency and effectiveness of NAS have
been significantly improved. Nasir et al. [95] proposed a new
NAS algorithm that effectively combines the advantages
of LLMs and Quality Diversity (QD) algorithms to auto-
mate the search and discovery of high-performance neural
network architectures. So et al. [115] proposed an evolu-
tionary Transformer discovered through evolutionary archi-
tectural search in multilingual tasks superior Transformer
that achieves better performance with fewer parameters and
maintains high quality even at smaller sizes.
More sophisticated and effective search strategies have
been proposed by researchers in recent years to improve
the performance of large models. For example, Gao et al.
[43] proposed an automatic method (AutoBERT-Zero) for
discovering the backbone structure of a general-purpose
language model (LLM) using a well-designed search space
and an operation-first evolutionary strategy, as well as a two-
branch weight-sharing training strategy, to improve search
efficiency and performance. Ganesan et al. [42] perform
task-independent pre-training of BERT models while gener-
ating differently shaped sub-networks by varying the hidden
dimensions in the Transformer layer. Rather than optimizing
for a specific task, it generates a series of different-sized
models by varying the hidden dimensions of the network,
which can be fine-tuned for various downstream tasks. Yin
et al. [147] proposed the use of one-shot Neural Architecture
Search (one-shot NAS) to automatically search for archi-
tectural hyperparameters. A large SuperPLM is obtained
through one-shot learning, which can be used as a proxy for
Huang et al.: Preprint submitted to Elsevier
Page 13 of 23


--- Page 14 ---
When Large Language Model Meets Optimization
Table 4
Summary of self-Tuning optimization
Algorithms
Main tasks
Objectives
Using OAs
SPELL [68]
Combining Evolutionary Algorithms and Text
Generation Capabilities of LLMs to Optimise
Prompts for LLMs in a Black Box Environment
Classification accuracy
Variants based on evolu-
tionary algorithms
Promptbreeder [41]
Improves LLM performance on specific tasks
by automating the cue search and optimisation
process without the need to manually engineer
prompts
Performance score
Genetic Algorithm
Interpretable Auto-
prompting [113]
Iteratively use LLM to generate explanations
and reorder them based on their performance
when used as prompts
Accuracy
and
interpretability
Iterative local search al-
gorithm
APO [104]
Automatically improves prompts to reduce the
heavy trial and error required to write prompts
manually
Performance of Initial
Prompts
Textual gradient descent
Auto-Instruct [154]
Automatic generation and optimisation of in-
structions for LLMs
Classification accuracy
Metaheuristic algorithms
all potential sub-architectures. An evolutionary algorithm is
also used to search for the best architectures on the Super-
PLM, and then the corresponding sub-models are extracted
based on these architectures and further trained. Javaheripi
et al. [55] proposed a no-training Neural Architecture Search
(NAS) algorithm for finding Transformer architectures that
have an optimal balance between task performance (per-
plexity) and hardware constraints (e.g., peak memory usage
and latency). Zhou et al. [162] proposed a Transformer
architecture search method called T-Razor, which uses zero-
cost agent-guided evolution to improve the search efficiency
and evaluates and ranks Transformers by introducing met-
rics such as synaptic diversity and synaptic saliency to
efficiently find optimized architectures in the Transformer
search space. Klein et al. cites klein2023structural proposed
Neural Architecture Search (NAS) based on weight shar-
ing as a structural pruning method for finding the optimal
balance between optimization efficiency and generalization
performance to achieve compression of large language mod-
els (LLMs) in order to reduce the model size and inference
latency.
Overall, the main advantage of NAS for optimising large
models is its ability to automate the exploration and dis-
covery of efficient network architectures for specific tasks,
significantly improving model performance while reducing
manual design and tuning efforts. With intelligent search
strategies, NAS helps save computational resources and
time. However, this approach also faces challenges, includ-
ing the large search space, the possibility of falling into local
optimal solutions, and the large amount of computational
resources required in the initial search and training phases.
In addition, the selection and tuning of optimization algo-
rithms require expertise, and the generalization ability of
the network architecture obtained from the search still needs
further validation. Future research may focus on improving
the search efficiency, reducing the computational cost, and
enhancing the generalisability and adaptability of the model.
6. Application of LLMs-based Optimization
Algorithms
As show in Fig. 3, optimization algorithms are pivotal
in various applications, broadly categorized into software
programming, neural architecture search and content gen-
eration. LLM-based optimization algorithms are becoming
increasingly important in artificial intelligence, especially in
machine learning. They are used for software programming
and neural architecture search to help design efficient net-
work architectures. Furthermore, these algorithms are em-
ployed as innovative tools in content generation, optimizing
the creation process to produce relevant and engaging con-
tent. This bifurcation in application highlights the versatility
and evolving role of optimization algorithms in addressing
both conventional challenges and pioneering technological
advancements.
Figure 3: The split of LLMs assist optimization algorithms.
6.1. Assisted Optimization Programming:
Software Programming
In the wave of artificial intelligence, optimization algo-
rithms based on LLMs have gradually become an impor-
tant research area to promote code generation and software
development. With many parameters and deep learning ca-
pabilities, LLMs have shown powerful capabilities in many
fields, such as natural language processing, image recogni-
tion, etc. Especially in the process of software development,
the application of large models can improve the efficiency
Huang et al.: Preprint submitted to Elsevier
Page 14 of 23


--- Page 15 ---
When Large Language Model Meets Optimization
Table 5
Summary of OAs-based network architecture for LLMs
Algorithms
Main tasks
Objectives
Using OAs
Autobert-zero [43]
Automatic exploration of new self-attentive
structures and overall efficient pre-trained lan-
guage model backbone architecture
Classification
performance
Operation-Priority NAS
AutoTinyBERT
[147]
Automatic hyperparameter optimization for effi-
cient compression of pre-trained language mod-
els
Classification
performance
One-shot NAS
Llmatic [95]
Discover diverse and powerful neural network
architectures
Network architecture
Quality-Diversity
optimization
SuperShaper [42]
Discovering networks with effective trade-offs
between accuracy and model size
Classification accuracy
Evolutionary algorithm
EvoPrompting [25]
Exploring the use of LLMs as generalised adap-
tive variation and crossover operators for NAS
algorithms
Classification accuracy
Evolutionary algorithm
of code generation and further enhance the model perfor-
mance through optimization algorithms. By automatically
generating code for training models, non-professionals can
also easily train efficient machine learning models, greatly
reducing the technical threshold and expanding the audience
for machine learning technology. Meanwhile, code integra-
tion practices in model development, such as static code
integration and dynamic code integration, also play a key
role in improving the efficiency and quality of software
development.
Weyssow et al. [134] explore using Large Language
Models (LLMs) for code generation tasks, focusing on
Parameter-Efficient Fine-Tuning (PEFT) optimization tech-
niques. The methodology aims to optimize the fine-tuning
process of LLMs by updating a small portion of the model‚Äôs
parameters instead of all of them using the PEFT tech-
nique to achieve efficient fine-tuning in resource-constrained
environments. Cassano et al. [19] present a system called
MultiPL-E, which is a system for translating code generation
benchmark tests from Python to other programming lan-
guages. Further, Pinna et al. [100] point out the application
of automatic code generation based on problem descriptions
and that even the most efficient LLMs often fail to generate
correct code. Therefore, to address the question of how to
enhance code generation based on Large Language Models
(LLMs) through a Genetic Improvement (GI) approach,
an evolutionary algorithm-based approach is proposed that
uses Genetic Improvement (GI) to improve LLM-generated
code using a collection of user-supplied test cases. Program
synthesis (PS), a form of automation, aims to reduce the
time and effort required for software development while im-
proving code quality. Although Genetic Programming (GP)
is a competing approach to solving the program synthesis
problem, it has limitations in evolving syntactically correct
and semantically meaningful programs. Tao et al. [122]
used a combination of Generative Pre-trained Transform-
ers (GPTs) and Grammar-Guided Genetic Programming
(G3P) to solve the program synthesis problem. GPTs) and
Grammar-Guided Genetic Programming (G3P) to address
the program synthesis problem. The OpenAI team has
developed a language model called CodeX [26], which has
been fine-tuned using publicly available code on GitHub
and investigated for its ability to write Python code. A
special production version of Codex is GitHub Copilot,
a programming aid. Brownlee et al. [15] explored how
large language models (LLMs) can be applied to varia-
tion operations in Genetic Improvement (GI) to improve
the efficiency of the search process.GI is a search-based
technique used to improve the non-functional attributes,
such as execution time, and functional attributes, such as
fixing defects, of existing software. Ji et al. [57] specifi-
cally presents a research overview of the assessment and
interpretation of code generation capabilities based on large
language models (LLMs), including two main phases: data
collection and analysis. In the data collection phase, the
prompts‚Äô features are quantified by extracting their linguistic
features and performance metrics from the generated code.
In the analysis phase, causal diagrams are constructed using
causal discovery algorithms and further analyzed to identify
principles of hint design.
Codex [26] was fine-tuned using publicly available
GitHub code to enhance its Python coding capabilities. The
method evaluated Codex using a new test set, HumanEval,
which assesses the functional correctness of programs syn-
thesized from documentation strings.In this dataset, Codex
successfully solved 28.8% of the problems, compared to
GPT-3, which solved none, and GPT-J, which solved 11.4%.
CODAMOSA [66] integrates a pre-trained Codex with
Search-Based Software Testing (SBST) to enhance test case
code coverage. SBST generates high-coverage test cases by
combining test case generation with mutation for programs
under test. However, SBST may face stagnant coverage,
meaning it struggles to produce new test cases that increase
coverage. When SBST‚Äôs coverage improvement stagnates,
the CODAMOSA algorithm aids in relocating to more
advantageous search space areas by using Codex to generate
example test cases for functions with lower coverage. Wu
et al. [137] highlight the advances in code generation by
Huang et al.: Preprint submitted to Elsevier
Page 15 of 23


--- Page 16 ---
When Large Language Model Meets Optimization
large-scale language models (LLMs) and the associated
security risks, particularly the critical vulnerabilities in the
generated code. Although some LLM providers have sought
to mitigate these issues through human guidance, their
efforts have yet to yield robust and reliable code LLMs
in practical applications. They introduce the DeceptPrompt
algorithm, designed to generate adversarial natural language
instructions that prompt code LLMs to produce functionally
correct yet vulnerable code. DeceptPrompt employs a sys-
tematic, evolution-based algorithm with a fine-grained lossy
approach. The algorithm uniquely excels at identifying natu-
ral prefixes/suffixes with benign, non-directional semantics
and effectively induces code LLMs to generate vulnerable
code. This feature enables researchers to conduct near-worst-
case red-team tests on these LLMs in real-world scenarios
through natural language.
In summary, in software programming, large language
models (LLMs) enhance code generation efficiency via op-
timization algorithms and reduce the complexity of machine
learning technology. This simplification allows non-experts
to train efficient models, thereby broadening the reach of
machine learning.
6.2. Assisted Optimization Framework: Neural
Architecture Search
Neural Architecture Search (NAS), an important tech-
nique for the automatic design of neural networks, is un-
dergoing a transformation driven by combining LLMs and
optimization algorithms. With their massive parameters and
deep learning capabilities, LLMs show unprecedented po-
tential in handling complex tasks. At the same time, the
combination of well-designed optimization algorithms can
further accelerate the Neural Architecture Search process,
improving the search efficiency and the performance of the
resulting models. With the continuous application of big
models in NAS, we see their great potential in the automatic
search and optimization of neural network structures, which
not only greatly saves labor costs but also improves the
innovation and diversity of model design.
Nasir et al. [95] present LLMatic, a large model-based
Neural Architecture Search (NAS) algorithm that uses two
QD archives to search for competitive networks, which com-
bines the code generation capabilities of Large Language
Models (LLMs) with the diversity and robustness of Quality
Diversity (QD) algorithms.LLMatic utilizes the LLMs to
generate new architectural variants and combines the QD al-
gorithms (especially the MAP-Elites algorithm) to discover
diverse and robust solutions. Chen et al. [25] found that an
approach combining evolutionary prompt engineering and
soft prompt tuning, EvoPrompting, consistently discovers
diverse and high-performance models. A method for creat-
ing and curating data using evolutionary search to improve
in-context prompting examples for LM is presented. While
focused on neural architecture design tasks, this approach is
equally applicable to LM tasks that rely on in-context learn-
ing (ICL) or cue tuning. Jawahar et al. [56] build new uses
for Performance Predictors (PP) by using Large Language
Models (LLMs) that predict the performance of specific
Deep Neural Network (DNN) architectures on downstream
tasks. A hybrid search algorithm (HS-NAS) is proposed,
which uses LLM-Distill-PP in the initial phase of the search
and a baseline predictor for the remainder of the search.HS-
NAS reduces the search time by about 50% with perfor-
mance comparable to that of the SOTA NAS and sometimes
improves latency, GFLOPs, and model size. Jawahar et
al. [56] introduced LLM-PP, a precise performance predictor
developed using LLM for few-shot prompting. It achieves
a mean absolute error (MAE) comparable to the state of
the art (SOTA). LLMDistill-PP, developed as a more cost-
effective predictor, caters to applications like Neural Ar-
chitecture Search (NAS) that require numerous predictions.
Additionally, the new HS-NAS algorithm is introduced. It
leverages the strengths of LLMDistill-PP and the state-of-
the-art performance estimator, reducing NAS search times
by half and identifying more efficient architectures.
Zheng et al. [157] explored the potential of GPT-4 mod-
els for the Neural Architecture Search (NAS) task of design-
ing effective neural network architectures. At the same time,
they propose an approach called GPT-4 Enhanced Neural
archItectUre Search (GENIUS), which leverages the gener-
ative power of GPT-4 as a black-box optimizer to navigate
the architectural search space quickly, identify promising
candidate architectures, and iteratively refine these candidate
architectures to improve performance. EvoPrompting [25]
employs advanced Language Models (LMs) for code-level
Neural Architecture Search (NAS). This approach integrates
evolutionary prompt engineering with soft-prompt tuning.
It aims to iteratively refine contextual prompts and enhance
prompt tuning on LMs, thereby boosting their capacity to
generate innovative and diverse solutions for complex rea-
soning tasks. Radford et al. [105] describe a method to en-
hance Natural Language Understanding (NLU) using Gen-
erative Pre-Training. They show that this approach signif-
icantly boosts performance across various NLU tasks by
initially pre-training a language model on a vast corpus
of unlabeled text, then applying supervised fine-tuning for
particular tasks. Chowdhery et al. [31] proposed PaLM
(Pathways Language Model), a large-scale language model,
PaLM has demonstrated excellent performance on a variety
of Natural Language Processing (NLP) tasks, and PaLM
also has very good performance on network structure design,
structure search.
In summary, integrating LLMs with optimization algo-
rithms in neural network architecture search (NAS) enhances
search efficiency, fosters innovation, diversifies model de-
signs, and opens new avenues for the automated design of
complex neural architectures.
6.3. Assisted Optimization Generation: Content
Innovation Generation
Innovative content generation has become a key driver
for developing media, entertainment, arts, and scientific dis-
covery. Applying big artificial intelligence models combined
with optimization algorithms is increasingly important in
Huang et al.: Preprint submitted to Elsevier
Page 16 of 23


--- Page 17 ---
When Large Language Model Meets Optimization
this process. In summary, using optimization algorithms
based on large models in innovative content generation is
not only about the innovation and diversity of content but
also promotes and facilitates scientific and technological
innovation development.
Xiao et al. [141] proposed a pattern-centric text gen-
eration framework, PatternGPT, to address the error-prone
nature of Large Language Models (LLMs) and the inability
to use external knowledge in text generation tasks directly.
The framework uses algorithms to search for or generate
high-quality patterns based on judgmental criteria. It lever-
ages the pattern extraction capabilities of LLMs to develop
a diverse set of structured and formalized patterns, which
can help to bring in external knowledge for computation.
Chen et al. [28] enhance the performance of Large Lan-
guage Models (LLMs) in language generation tasks through
Model-Adaptive Prompt Optimization (MAPO), a prompt
optimization method that can be widely applied to various
downstream generation tasks. Similarly, they propose a new
paradigm for news summary generation that uses Large
Language Models (LLMs) to improve the quality of news
summary generation through evolutionary fine-tuning [140].
The method uses LLM to extract multiple structured event
patterns from news passages, evolves a population of event
patterns via a genetic algorithm, and selects the most adapted
event patterns to input into LLM to generate news sum-
maries. PanGu Drug Model [73] is a graph-to-sequence
asymmetric conditional variational autoencoder designed to
improve molecular property representation and performance
in drug discovery tasks. The model is inspired by conver-
sions between molecular formulas and structural formulae
in the chemistry classroom and can appropriately character-
ize molecules from both representations. Liang et al. [70]
presented a prototype of a DrugChat system designed to
provide ChatGPT-like capabilities for drug compound analy-
sis.DrugChat, by combining graph neural networks (GNNs),
large language models (LLMs), and adapters, enables users
to upload molecular maps of compounds and ask various
questions during multiple rounds of interaction. Diagrams
and ask different questions in numerous rounds of interac-
tion, which the system then answers. To break the bottleneck
of literate graph technology, Berger et al. [10] proposes the
framework of StableYolo, which aims to optimize the image
generation quality of large language models (LLMs) by
applying evolutionary computation to the Stable Diffusion
model while adjusting the prompts and model parameters.
The core idea of StableYolo is to improve the image gen-
eration quality of photo-realistic styles by combining visual
evaluation with multi-objective search. The core concept of
StableYolo is to enhance the quality of image generation
in photo-realistic style by combining visual evaluation with
multi-objective search. The system uses the confidence esti-
mate of the Yolo model as a fitness function and searches for
the optimal combination of cue words and model parameters
using a Genetic Algorithm (GA).
To explore additional research related to LLMs, includ-
ing cognitive functions of LLMs, behavior and learning
in game-theoretic environments, and Big Five personality
traits, Suzuki et al. [120] propose a model for the evolution of
personality traits based on Large Language Models (LLMs),
specifically those related to cooperative behavior. The ap-
proach demonstrates how LLMs can enhance the study of
human behavioral evolution and is based on evolutionary
game theory by using an evolutionary model that assumes
that human behavioral choices in game-theoretic situations
can be simulated by providing LLMs with high-level psy-
chological and cognitive trait descriptions. De et al. [34]
explored the phenomenon of the self-organized formation
of scale-free networks in social interactions between large
language models (LLMs). Scale-free networks are a typical
emergent behavior in complex systems, especially in online
social media, where users can follow each other and form so-
cial networks with specific structural features. Lu et al. [82]
propose a novel learning framework, SELF (Self-Evolution
with Language Feedback), which aims to continuously en-
able large-scale language models (LLMs) to improve them-
selves through self-feedback and self-improvement. The
SELF framework is inspired by the human self-driven learn-
ing process, which consists of an initial attempt, reflective
feedback, and The SELF framework is inspired by the human
self-driven learning process, which involves a cycle of initial
attempts, reflective feedback, and behavioral improvement to
improve the model‚Äôs capabilities. The ELF framework also
enables smaller LLMs to improve themselves, which can be
reversed to facilitate the development of larger predictive
models.
In summary, the proposed systems and frameworks,
including DrugChat and SELF, illustrate the development
of personalized, intelligent tools for analyzing drug com-
pounds, generating news summaries, and mimicking human
behaviors. These tools continuously improve their perfor-
mance through self-learning and feedback mechanisms, en-
hancing efficiency and accuracy in related fields.
7. Future Outlook and Research Trends
In the previous sections, we have examined recent ad-
vances in the fields of long-term memory models (LLMs)
and optimization algorithms (OAs). Nonetheless, there are
still many challenges and unresolved issues between these
two fields. Therefore, the aim of this section is to explore di-
rections for future research in order to provide scholars with
the opportunity to explore new areas beyond the boundaries
of current knowledge, to ask new research questions, and to
reinvigorate the field.
Theoretical Foundations and Methodologies. Experi-
mental studies have confirmed the effectiveness of combin-
ing large-scale language models (LLMs) with OAs in solv-
ing small-scale problems [79, 90]. However, the motivation
for their interaction has not yet been clarified. To further
promote the performance of algorithms, we need to deeply
explore the mechanism of mutual reinforcement between
LLMs and OAs in theoretical studies and analyze their com-
plementary advantages and potential problems in practical
Huang et al.: Preprint submitted to Elsevier
Page 17 of 23


--- Page 18 ---
When Large Language Model Meets Optimization
applications in detail through large-scale empirical studies.
In addition, it is crucial to conduct in-depth theoretical analy-
ses of algorithms combining LLMs and OAs, which includes
evaluating their convergence, time complexity and space
complexity. Also, investigating the impact of algorithmic
parameter settings on performance, as well as performance
guarantees or theoretical limitations of the algorithms on
different problem types, are key steps in advancing the algo-
rithms. Further, exploring optimization theory [78], such as
clarifying the definition and characterization of the objective
function, dealing with constraints, and analyzing the feasible
solution space of a problem, will provide a solid theoretical
foundation for the design and application of algorithms
to achieve better algorithmic performance in solving more
complex problems.
Automated Intelligent Optimization. In the optimiza-
tion context, large language models (LLMs) show signifi-
cant potential, especially in enhancing the automation and
intelligence of optimization algorithms (OAs). Learning
from multimodal data during the pre-training phase allows
LLMs to understand and generate cross-modal content
[138]. This provides a new search and mutation strategy
for OAs when performing cross-modal operations. This
capability of LLMs can facilitate OAs in achieving a more
efficient global search in multimodal optimization problems.
At the same time, as the technology of LLMs continues to
advance, it is expected that they will drive the performance
of OAs in modeling complex evolutionary mechanisms,
especially when dealing with optimization problems with
large-scale search spaces [32]. However, current research
has yet to explore the potential of LLMs in evolutionary
optimization, and there remain challenges, such as how to
combine LLMs and OAs better and how to handle complex
search spaces.
In addition, the pre-training of LLMs on large amounts
of textual data embeds them with rich domain knowledge,
which provides a robust knowledge base for OAs.LLMs
can assist OAs in better integrating domain-specific knowl-
edge in the optimization process, thus improving the ef-
ficiency of optimization and the quality of solutions. For
example, LLMs can generate high-quality initial solutions,
improve problem formulation, and provide solution coding
and definition of solution spaces. In addition, LLMs can
provide guiding principles for algorithm design, enabling
EAs to handle complex optimization problems such as multi-
objective, discrete and dynamic more effectively [121]. With
the rapid development of LLMs technology, they are ex-
pected to play an even more critical role in the future evo-
lutionary optimization field, driving the field toward higher
levels of automation and intelligence.
Robustness and Prompt Engineering. Utilizing opti-
mization techniques is a crucial method for improving the
capabilities of LLMs in engineering applications. Common
approaches involve utilizing LLMs as optimization opera-
tors within EIA frameworks to consistently produce fresh
prompt. This technique has consistently shown efficacy and
superiority in numerous investigations. Nevertheless, certain
obstacles persist. Firstly, it is crucial to pay close attention to
the initialization of the optimization process as it will have
a substantial impact on the outcomes [25, 68]. It is crucial
to have cue templates that are generic and customizable
in order to provide accurate and valid prompts. Random
initialization may not be capable of using existing informa-
tion, and manual seeding may add bias. In addition, when
confronted with issues that contain a significant amount
of previous knowledge, the range of possible prompts to
consider increases exponentially as the length of the cue
and the size of the vocabulary grow. This can result in
over fitting or becoming trapped in local optimal solutions.
Furthermore, these approaches lack stability and strongly
depend on the capabilities of the LLM, rendering them
susceptible to stochasticity [68]. If the LLM lacks the abil-
ity to comprehend and efficiently employ the cues, it may
undermine the effectiveness of the approach.
Further research should strive to tackle these obstacles
by creating more resilient techniques. For instance, in the
context of initialization, the technique of multisource seed-
ing can be investigated to automatically improve the size and
quality of the initial population utilising LLM. When dealing
with intricate search spaces, it is essential to develop efficient
optimisation algorithms. This may involve combining more
comprehensive sets of optimisation operators, using the ad-
vantages of different evolutionary algorithms, and utilising
adaptive optimisation techniques.
Generality and Architecture Search. The combined
efforts of Large Language Models (LLMs) and OAs have
accelerated progress in the field of code generation, leading
to notable improvements in downstream applications such as
software engineering and OAs design. An commonly used
method in this collaboration involves employing LLMs to
create large training datasets, and then refining the LLMs
using reinforcement learning approaches [64, 29]. Never-
theless, this approach has challenges with the variety and
quantity of training data, which could result in a failure to
cover all possible scenarios. An alternate approach involves
utilizing the strong code generation capabilities of LLMs in
combination with the powerful search architecture of OAs to
continually improve the code generation process. However,
this method has difficulties when it comes to generating
code for sophisticated algorithmic logic that may require
the combined work of numerous code snippets. In order to
overcome these obstacles, it is possible to develop a modular
strategy that breaks down large activities into smaller, more
manageable sub-tasks. An interactive interface might be
added to allow users to clearly define the breakdown of tasks
[29]. This would enable LLMs and OAs to generate code for
each sub-task in a coordinated manner.
Neural Architecture Search (NAS) is an important ap-
plication scenario that arises from the combination of LLMs
and OAs. Although LLMs have shown remarkable effective-
ness in other tasks, they have not been specifically designed
for NAS [25]. The performance of current LLM models
varies significantly when used for NAS tasks, and there
is a clear difference between LLM-based approaches and
Huang et al.: Preprint submitted to Elsevier
Page 18 of 23


--- Page 19 ---
When Large Language Model Meets Optimization
conventional NAS methods in terms of their application
area and ability to generalize [106]. In order to enhance the
overall effectiveness of LLMs and EAs in NAS projects,
a comprehensive strategy could be implemented. This in-
volves assessing the effectiveness of various LLM models
in NAS tasks, enhancing LLM‚Äôs NAS skills by incorporating
more training data, optimizing the structure of LLMs during
the fine-tuning stage, and investigating the utilization of past
search knowledge to speed up future searches and provide
clearly defined search spaces for LLMs.
Interdisciplinary Applications and Innovations. The
incorporation of Large Language Models (LLMs) with op-
timization algorithms (OAs) shows potential in several in-
terdisciplinary domains, providing a powerful synergy to
stimulate innovation and improve performance in intricate
jobs.
In the realm of computational creativity and generative
design, LLMs are adept at generating creative content, such
as artwork, music, and literary pieces. The collaboration
with OA brings methods of variation and selection, which
can promote creative diversity and ignite innovation. This
collaborative approach can result in the production of unique
and groundbreaking artistic and design works, thereby pro-
moting innovation and fostering the growth of creativity.
Within the domain of robotics, intelligent and adaptable
robot systems can be produced through the collaboration
of OAs, which have the ability to refine control strategies
and action sequences, and LLMs, which are capable of
generating instructive dialogues and task-oriented directives
[139]. These systems have enhanced capabilities to adjust
to various tasks and participate in complex interactions
with humans, enhancing collaboration between humans and
robots and establishing the foundation for advanced robotic
applications.
Moreover, in the field of drug design, the ability of
LLMs to produce new chemical structures, combined with
the multi-objective optimization capabilities of OAs, can
accelerate the process of discovering new drugs. This com-
prehensive technique has the capability to recognize drug
candidates with higher potential, so decreasing the time and
expenses linked to conventional trial-and-error procedures
and promoting progress in pharmaceutical research and de-
velopment. The combination of LLMs and OAs offers a
versatile instrument that has the ability to transform various
fields by offering inventive solutions and improving effi-
ciency in problem-solving. As research explores the joint
capabilities of LLMs and OAs, it is expected that more
significant advancements and innovative uses will arise,
revolutionizing industries and expanding the limits of human
accomplishment.
References
[1] Abdel-Basset, M., Ding, W., El-Shahat, D., 2021. A hybrid harris
hawks optimization algorithm with simulated annealing for feature
selection. Artificial Intelligence Review 54, 593‚Äì637.
[2] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman,
F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.,
2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 .
[3] AhmadiTeshnizi, A., Gao, W., Udell, M., 2023. Optimus: Optimiza-
tion modeling using mip solvers and large language models. arXiv
preprint arXiv:2310.06116 .
[4] Alba, E., Mart√≠, R., 2006.
Metaheuristic procedures for training
neural networks. volume 35. Springer Science & Business Media.
[5] Amuthan, A., Thilak, K.D., 2016.
Survey on tabu search meta-
heuristic optimization, in: 2016 International Conference on Sig-
nal Processing, Communication, Power and Embedded System
(SCOPES), IEEE. pp. 1539‚Äì1543.
[6] Arulkumaran, K., Deisenroth, M.P., Brundage, M., Bharath, A.A.,
2017. Deep reinforcement learning: A brief survey. IEEE Signal
Processing Magazine 34, 26‚Äì38.
[7] Bali, K.K., Gupta, A., Ong, Y.S., Tan, P.S., 2020.
Cognizant
multitasking in multiobjective multifactorial evolution: Mo-mfea-ii.
IEEE transactions on cybernetics 51, 1784‚Äì1796.
[8] Baumann, J., Kramer, O., 2024. Evolutionary multi-objective opti-
mization of large language model prompts for balancing sentiments,
in: International Conference on the Applications of Evolutionary
Computation (Part of EvoStar), Springer. pp. 212‚Äì224.
[9] Bello, I., Pham, H., Le, Q.V., Norouzi, M., Bengio, S., 2016. Neural
combinatorial optimization with reinforcement learning.
arXiv
preprint arXiv:1611.09940 .
[10] Berger, H., Dakhama, A., Ding, Z., Even-Mendoza, K., Kelly, D.,
Menendez, H., Moussa, R., Sarro, F., 2023. Stableyolo: Optimizing
image generation for large language models, in: International Sym-
posium on Search Based Software Engineering, Springer. pp. 133‚Äì
139.
[11] Bradley, H., Dai, A., Teufel, H., Zhang, J., Oostermeijer, K.,
Bellagente, M., Clune, J., Stanley, K., Schott, G., Lehman, J.,
2023.
Quality-diversity through ai feedback.
arXiv preprint
arXiv:2310.13032 .
[12] Bradley, H., Fan, H., Galanos, T., Zhou, R., Scott, D., Lehman, J.,
2024. The openelm library: Leveraging progress in language models
for novel evolutionary algorithms, in: Genetic Programming Theory
and Practice XX. Springer, pp. 177‚Äì201.
[13] Brahmachary, S., Joshi, S.M., Panda, A., Koneripalli, K., Sagotra,
A.K., Patel, H., Sharma, A., Jagtap, A.D., Kalyanaraman, K., 2024.
Large language model-based evolutionary optimizer: Reasoning
with elitism. arXiv preprint arXiv:2403.02054 .
[14] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhari-
wal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.,
2020. Language models are few-shot learners. Advances in neural
information processing systems 33, 1877‚Äì1901.
[15] Brownlee, A.E., Callan, J., Even-Mendoza, K., Geiger, A., Hanna,
C., Petke, J., Sarro, F., Sobania, D., 2023. Enhancing genetic im-
provement mutations using large language models, in: International
Symposium on Search Based Software Engineering, Springer. pp.
153‚Äì159.
[16] Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E.,
Kamar, E., Lee, P., Lee, Y.T., Li, Y., Lundberg, S., et al., 2023.
Sparks of artificial general intelligence: Early experiments with gpt-
4. arXiv preprint arXiv:2303.12712 .
[17] Cao, Y., Li, S., Liu, Y., Yan, Z., Dai, Y., Yu, P.S., Sun, L., 2023. A
comprehensive survey of ai-generated content (aigc): A history of
generative ai from gan to chatgpt. arXiv preprint arXiv:2303.04226
.
[18] Cao, Z., Wang, L., Shi, Y., Hei, X., Rong, X., Jiang, Q., Li, H.,
2015. An effective cooperative coevolution framework integrating
global and local search for large scale optimization problems, in:
2015 IEEE Congress on Evolutionary Computation (CEC), IEEE.
pp. 1986‚Äì1993.
[19] Cassano, F., Gouwar, J., Nguyen, D., Nguyen, S., Phipps-Costin, L.,
Pinckney, D., Yee, M.H., Zi, Y., Anderson, C.J., Feldman, M.Q.,
Guha, A., Greenberg, M., Jangda, A., 2023. Multipl-e: A scalable
and polyglot approach to benchmarking neural code generation.
IEEE Transactions on Software Engineering 49, 3675‚Äì3691.
Huang et al.: Preprint submitted to Elsevier
Page 19 of 23


--- Page 20 ---
When Large Language Model Meets Optimization
[20] Cerrone, C., Cerulli, R., Golden, B., 2017.
Carousel greedy: A
generalized greedy algorithm with applications in optimization.
Computers & Operations Research 85, 97‚Äì112.
[21] Chai, Y., Wang, S., Sun, Y., Tian, H., Wu, H., Wang, H., 2022. Clip-
tuning: Towards derivative-free prompt learning with a mixture of
rewards. arXiv preprint arXiv:2210.12050 .
[22] Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H.,
Yi, X., Wang, C., Wang, Y., et al., 2023. A survey on evaluation of
large language models. ACM Transactions on Intelligent Systems
and Technology .
[23] Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H.,
Yi, X., Wang, C., Wang, Y., et al., 2024. A survey on evaluation of
large language models. ACM Transactions on Intelligent Systems
and Technology 15, 1‚Äì45.
[24] Chao, W., Zhao, J., Jiao, L., Li, L., Liu, F., Yang, S., 2024.
A
match made in consistency heaven: when large language models
meet evolutionary algorithms. arXiv preprint arXiv:2401.10510 .
[25] Chen, A., Dohan, D., So, D., 2024a.
Evoprompting: Language
models for code-level neural architecture search. Advances in Neural
Information Processing Systems 36.
[26] Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H.P.d.O., Kaplan,
J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al., 2021.
Evaluating large language models trained on code. arXiv preprint
arXiv:2107.03374 .
[27] Chen, Y., Arkin, J., Hao, Y., Zhang, Y., Roy, N., Fan, C.,
2024b.
Prompt optimization in multi-step tasks (promst): Inte-
grating human feedback and preference alignment. arXiv preprint
arXiv:2402.08702 .
[28] Chen, Y., Wen, Z., Fan, G., Chen, Z., Wu, W., Liu, D., Li, Z.,
Liu, B., Xiao, Y., 2023.
Mapo: Boosting large language model
performance with model-adaptive prompt optimization, in: Findings
of the Association for Computational Linguistics: EMNLP 2023, pp.
3279‚Äì3304.
[29] CHen, Z., Cao, L., Madden, S., Fan, J., Tang, N., Gu, Z., Shang, Z.,
Liu, C., Cafarella, M., Kraska, T., 2023. Seed: Simple, efficient, and
effective data management via large language models. arXiv preprint
arXiv:2310.00749 .
[30] Choong, H.X., Ong, Y.S., Gupta, A., Chen, C., Lim, R., 2023. Jack
and masters of all trades: One-pass learning sets of model sets
from large pre-trained models.
IEEE Computational Intelligence
Magazine 18, 29‚Äì40.
[31] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G.,
Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S.,
et al., 2023.
Palm: Scaling language modeling with pathways.
Journal of Machine Learning Research 24, 1‚Äì113.
[32] Cui, C., Ma, Y., Cao, X., Ye, W., Zhou, Y., Liang, K., Chen, J., Lu,
J., Yang, Z., Liao, K.D., et al., 2024. A survey on multimodal large
language models for autonomous driving, in: Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer Vision,
pp. 958‚Äì979.
[33] Das, S., Mullick, S.S., Suganthan, P.N., 2016. Recent advances in
differential evolution‚Äìan updated survey. Swarm and evolutionary
computation 27, 1‚Äì30.
[34] De Marzo, G., Pietronero, L., Garcia, D., 2023. Emergence of scale-
free networks in social interactions among large language models.
arXiv preprint arXiv:2312.06619 .
[35] Desale, S., Rasool, A., Andhale, S., Rane, P., 2015. Heuristic and
meta-heuristic algorithms and their relevance to the real world: a
survey. Int. J. Comput. Eng. Res. Trends 351, 2349‚Äì7084.
[36] Deudon, M., Cournut, P., Lacoste, A., Adulyasak, Y., Rousseau,
L.M., 2018. Learning heuristics for the tsp by policy gradient, in:
Integration of Constraint Programming, Artificial Intelligence, and
Operations Research: 15th International Conference, CPAIOR 2018,
Delft, The Netherlands, June 26‚Äì29, 2018, Proceedings 15, Springer.
pp. 170‚Äì181.
[37] Diao, S., Huang, Z., Xu, R., Li, X., Lin, Y., Zhou, X., Zhang, T.,
2022. Black-box prompt learning for pre-trained language models.
arXiv preprint arXiv:2201.08531 .
[38] Dong, W., Wang, Y., Zhou, M., 2019. A latent space-based esti-
mation of distribution algorithm for large-scale global optimization.
Soft Computing 23, 4593‚Äì4615.
[39] Elsken, T., Metzen, J.H., Hutter, F., 2019. Neural architecture search:
A survey. Journal of Machine Learning Research 20, 1‚Äì21.
[40] Fei, Z., Fan, M., Huang, J., 2023. Gradient-free textual inversion,
in: Proceedings of the 31st ACM International Conference on Mul-
timedia, pp. 1364‚Äì1373.
[41] Fernando, C., Banarse, D., Michalewski, H., Osindero, S., Rock-
t√§schel, T., 2023. Promptbreeder: Self-referential self-improvement
via prompt evolution. arXiv preprint arXiv:2309.16797 .
[42] Ganesan, V., Ramesh, G., Kumar, P., 2021.
Supershaper: Task-
agnostic super pre-training of bert models with variable hidden
dimensions. arXiv preprint arXiv:2110.04711 .
[43] Gao, J., Xu, H., Shi, H., Ren, X., Philip, L., Liang, X., Jiang, X., Li,
Z., 2022. Autobert-zero: Evolving bert backbone from scratch, in:
Proceedings of the AAAI Conference on Artificial Intelligence, pp.
10663‚Äì10671.
[44] Gholami, S., Omar, M., 2023. Can pruning make large language
models more efficient? arXiv preprint arXiv:2310.04573 .
[45] Gleixner, A., Hendel, G., Gamrath, G., Achterberg, T., Bastubbe, M.,
Berthold, T., Christophel, P., Jarck, K., Koch, T., Linderoth, J., et al.,
2021. Miplib 2017: data-driven compilation of the 6th mixed-integer
programming library. Mathematical Programming Computation 13,
443‚Äì490.
[46] Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu, G.,
Bian, J., Yang, Y., 2024. Connecting large language models with
evolutionary algorithms yields powerful prompt optimizers, in: The
Twelfth International Conference on Learning Representations.
[47] Guo, Z., Wei, Y., Liu, M., Ji, Z., Bai, J., Guo, Y., Zuo, W., 2023.
Black-box tuning of vision-language models with effective gradient
approximation. arXiv preprint arXiv:2312.15901 .
[48] Gupta, A., Ong, Y.S., Feng, L., Tan, K.C., 2016.
Multiobjective
multifactorial optimization in evolutionary multitasking.
IEEE
transactions on cybernetics 47, 1652‚Äì1665.
[49] Gupta, T.K., Raza, K., 2020. Optimizing deep feedforward neural
network architecture: A tabu search based approach. Neural Pro-
cessing Letters 51, 2855‚Äì2870.
[50] Han, C., Cui, L., Zhu, R., Wang, J., Chen, N., Sun, Q., Li, X.,
Gao, M., 2023. When gradient descent meets derivative-free op-
timization: A match made in black-box scenario.
arXiv preprint
arXiv:2305.10013 .
[51] Hansen, N., M√ºller, S.D., Koumoutsakos, P., 2003.
Reducing
the time complexity of the derandomized evolution strategy with
covariance matrix adaptation (cma-es). Evolutionary computation
11, 1‚Äì18.
[52] Hasselt, H., 2010. Double q-learning. Advances in neural informa-
tion processing systems 23.
[53] Hemberg, E., Moskal, S., O‚ÄôReilly, U.M., 2024. Evolving code with
a large language model. arXiv preprint arXiv:2401.07102 .
[54] Hu, Y., Yao, Y., Lee, W.S., 2020. A reinforcement learning approach
for optimizing multiple traveling salesman problems over graphs.
Knowledge-Based Systems 204, 106244.
[55] Javaheripi, M., de Rosa, G., Mukherjee, S., Shah, S., Religa, T.,
Teodoro Mendes, C.C., Bubeck, S., Koushanfar, F., Dey, D., 2022.
Litetransformersearch: Training-free neural architecture search for
efficient language models. Advances in Neural Information Process-
ing Systems 35, 24254‚Äì24267.
[56] Jawahar, G., Abdul-Mageed, M., Lakshmanan, L.V., Ding, D., 2023.
Llm performance predictors are good initializers for architecture
search. arXiv preprint arXiv:2310.16712 .
[57] Ji, Z., Ma, P., Li, Z., Wang, S., 2023. Benchmarking and explaining
large language model-based code generation: A causality-centric
approach. arXiv preprint arXiv:2310.06680 .
[58] Khalil, E., Dai, H., Zhang, Y., Dilkina, B., Song, L., 2017. Learning
combinatorial optimization algorithms over graphs.
Advances in
neural information processing systems 30.
Huang et al.: Preprint submitted to Elsevier
Page 20 of 23


--- Page 21 ---
When Large Language Model Meets Optimization
[59] Klein, A., Golebiowski, J., Ma, X., Perrone, V., Archambeau, C.,
2023.
Structural pruning of large language models via neural
architecture search .
[60] Konda, V., Tsitsiklis, J., 1999. Actor-critic algorithms. Advances in
neural information processing systems 12.
[61] Kool, W., Van Hoof, H., Welling, M., 2018. Attention, learn to solve
routing problems! arXiv preprint arXiv:1803.08475 .
[62] Lapid, R., Langberg, R., Sipper, M., 2023. Open sesame! universal
black box jailbreaking of large language models.
arXiv preprint
arXiv:2309.01446 .
[63] Lee, S., Kim, J., Kang, H., Kang, D.Y., Park, J., 2021.
Genetic
algorithm based deep learning neural network structure and hyper-
parameter optimization. Applied Sciences 11, 744.
[64] Lehman, J., Gordon, J., Jain, S., Ndousse, K., Yeh, C., Stanley,
K.O., 2023.
Evolution through large models, in: Handbook of
Evolutionary Machine Learning. Springer, pp. 331‚Äì366.
[65] Lei, K., Guo, P., Wang, Y., Wu, X., Zhao, W., 2022. Solve rout-
ing problems with a residual edge-graph attention neural network.
Neurocomputing 508, 79‚Äì98.
[66] Lemieux, C., Inala, J.P., Lahiri, S.K., Sen, S., 2023.
Codamosa:
Escaping coverage plateaus in test generation with pre-trained large
language models, in: 2023 IEEE/ACM 45th International Confer-
ence on Software Engineering (ICSE), IEEE. pp. 919‚Äì931.
[67] Li, Y., Han, T., Tang, S., Huang, C., Zhou, H., Wang, Y., 2023. An
improved differential evolution by hybridizing with estimation-of-
distribution algorithm. Information Sciences 619, 439‚Äì456.
[68] Li, Y.B., Wu, K., 2023. Spell: Semantic prompt evolution based on
a llm. arXiv preprint arXiv:2310.01260 .
[69] Liang, Y., Ren, Z., Yao, X., Feng, Z., Chen, A., Guo, W., 2018. En-
hancing gaussian estimation of distribution algorithm by exploiting
evolution direction with archive. IEEE transactions on cybernetics
50, 140‚Äì152.
[70] Liang, Y., Zhang, R., Zhang, L., Xie, P., 2023. Drugchat: towards
enabling chatgpt-like capabilities on drug molecule graphs. arXiv
preprint arXiv:2309.03907 .
[71] Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa,
Y., Silver, D., Wierstra, D., 2015.
Continuous control with deep
reinforcement learning. arXiv preprint arXiv:1509.02971 .
[72] Lin, X., Wu, Z., Dai, Z., Hu, W., Shu, Y., Ng, S.K., Jaillet, P.,
Low, B.K.H., 2023.
Use your instinct: instruction optimization
using neural bandits coupled with transformers.
arXiv preprint
arXiv:2310.02905 .
[73] Lin, X., Xu, C., Xiong, Z., Zhang, X., Ni, N., Ni, B., Chang, J., Pan,
R., Wang, Z., Yu, F., et al., 2022. Pangu drug model: learn a molecule
like a human. Biorxiv , 2022‚Äì03.
[74] Liu, D., Xue, S., Zhao, B., Luo, B., Wei, Q., 2020. Adaptive dynamic
programming for control: A survey and recent advances.
IEEE
Transactions on Systems, Man, and Cybernetics: Systems 51, 142‚Äì
160.
[75] Liu, F., Lin, X., Wang, Z., Yao, S., Tong, X., Yuan, M., Zhang,
Q., 2023a. Large language model for multi-objective evolutionary
optimization. arXiv preprint arXiv:2310.12541 .
[76] Liu, F., Tong, X., Yuan, M., Lin, X., Luo, F., Wang, Z., Lu, Z., Zhang,
Q., 2024a. An example of evolutionary computation+ large language
model beating human: Design of efficient guided local search. arXiv
preprint arXiv:2401.02051 .
[77] Liu, F., Tong, X., Yuan, M., Zhang, Q., 2023b. Algorithm evolution
using large language model. arXiv preprint arXiv:2311.15249 .
[78] Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G., 2023c.
Pre-train, prompt, and predict: A systematic survey of prompting
methods in natural language processing. ACM Computing Surveys
55, 1‚Äì35.
[79] Liu, S., Chen, C., Qu, X., Tang, K., Ong, Y.S., 2023d.
Large
language models as evolutionary optimizers.
arXiv preprint
arXiv:2310.19046 .
[80] Liu, S., Gao, C., Li, Y., 2024b.
Large language model agent for
hyper-parameter optimization. arXiv preprint arXiv:2402.01881 .
[81] Liu, Y., Sun, Y., Xue, B., Zhang, M., Yen, G.G., Tan, K.C., 2021. A
survey on evolutionary neural architecture search. IEEE transactions
on neural networks and learning systems 34, 550‚Äì570.
[82] Lu, J., Zhong, W., Huang, W., Wang, Y., Mi, F., Wang, B., Wang,
W., Shang, L., Liu, Q., 2023. Self: Language-driven self-evolution
for large language model. arXiv preprint arXiv:2310.00533 .
[83] Lupyan, G., 2016. The centrality of language in human cognition.
Language Learning 66, 516‚Äì553.
[84] Lv, M., Wang, J., Wang, S., Gao, J., Guo, H., 2024. Developing
a hybrid system for stock selection and portfolio optimization with
many-objective optimization based on deep learning and improved
nsga-iii. Information Sciences , 120549.
[85] Ma, Q., Ge, S., He, D., Thaker, D., Drori, I., 2019. Combinatorial
optimization by graph pointer networks and hierarchical reinforce-
ment learning. arXiv preprint arXiv:1911.04936 .
[86] Ma, R., Wang, X., Zhou, X., Li, J., Du, N., Gui, T., Zhang, Q., Huang,
X., 2024. Are large language models good prompt optimizers? arXiv
preprint arXiv:2402.02101 .
[87] Ma, X., Fang, G., Wang, X., 2023. Llm-pruner: On the structural
pruning of large language models. Advances in neural information
processing systems 36, 21702‚Äì21720.
[88] Massim, Y., Yalaoui, F., Ch√¢telet, E., Yalaoui, A., Zeblah, A.,
2012. Efficient immune algorithm for optimal allocations in series-
parallel continuous manufacturing systems. Journal of intelligent
manufacturing 23, 1603‚Äì1619.
[89] Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H.,
Teller, E., 1953. Equation of state calculations by fast computing
machines. The journal of chemical physics 21, 1087‚Äì1092.
[90] Meyerson, E., Nelson, M.J., Bradley, H., Gaier, A., Moradi, A.,
Hoover, A.K., Lehman, J., 2023. Language model crossover: Varia-
tion through few-shot prompting. arXiv preprint arXiv:2302.12170
.
[91] Min, B., Ross, H., Sulem, E., Veyseh, A.P.B., Nguyen, T.H., Sainz,
O., Agirre, E., Heintz, I., Roth, D., 2023.
Recent advances in
natural language processing via large pre-trained language models:
A survey. ACM Computing Surveys 56, 1‚Äì40.
[92] Mirsadeghi, E., Khodayifar, S., 2021. Hybridizing particle swarm
optimization with simulated annealing and differential evolution.
Cluster Computing 24, 1135‚Äì1163.
[93] Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
Silver, D., Kavukcuoglu, K., 2016. Asynchronous methods for deep
reinforcement learning, in: International conference on machine
learning, PMLR. pp. 1928‚Äì1937.
[94] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou,
I., Wierstra, D., Riedmiller, M., 2013.
Playing atari with deep
reinforcement learning. arXiv preprint arXiv:1312.5602 .
[95] Nasir, M.U., Earle, S., Togelius, J., James, S., Cleghorn, C., 2023.
Llmatic: Neural architecture search via large language models and
quality-diversity optimization. arXiv preprint arXiv:2306.01102 .
[96] Nazari, M., Oroojlooy, A., Snyder, L., Tak√°c, M., 2018. Reinforce-
ment learning for solving the vehicle routing problem. Advances in
neural information processing systems 31.
[97] Neri, F., Tirronen, V., 2010. Recent advances in differential evo-
lution: a survey and experimental analysis. Artificial intelligence
review 33, 61‚Äì106.
[98] Newman, A.M., Weiss, M., 2013. A survey of linear and mixed-
integer optimization tutorials. INFORMS Transactions on Education
14, 26‚Äì38.
[99] Pan, R., Xing, S., Diao, S., Liu, X., Shum, K., Zhang, J., Zhang, T.,
2023. Plum: Prompt learning using metaheuristic. arXiv preprint
arXiv:2311.08364 .
[100] Pinna, G., Ravalico, D., Rovito, L., Manzoni, L., De Lorenzo, A.,
2024.
Enhancing large language models-based code generation
by leveraging genetic improvement, in: European Conference on
Genetic Programming (Part of EvoStar), Springer. pp. 108‚Äì124.
[101] Pluhacek, M., Kazikova, A., Kadavy, T., Viktorin, A., Senkerik,
R., 2023. Leveraging large language models for the generation of
novel metaheuristic optimization algorithms, in: Proceedings of the
Huang et al.: Preprint submitted to Elsevier
Page 21 of 23


--- Page 22 ---
When Large Language Model Meets Optimization
Companion Conference on Genetic and Evolutionary Computation,
pp. 1812‚Äì1820.
[102] Prasad, A., Hase, P., Zhou, X., Bansal, M., 2022. Grips: Gradient-
free, edit-based instruction search for prompting large language
models. arXiv preprint arXiv:2203.07281 .
[103] Pryzant, R., Iter, D., Li, J., Lee, Y.T., Zhu, C., Zeng, M., 2023a.
Automatic prompt optimization with" gradient descent" and beam
search. arXiv preprint arXiv:2305.03495 .
[104] Pryzant, R., Iter, D., Li, J., Lee, Y.T., Zhu, C., Zeng, M., 2023b.
Automatic prompt optimization with "gradient descent" and beam
search, in: Conference on Empirical Methods in Natural Language
Processing.
[105] Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al., 2018.
Improving language understanding by generative pre-training .
[106] Ren, P., Xiao, Y., Chang, X., Huang, P.Y., Li, Z., Chen, X., Wang,
X., 2021. A comprehensive survey of neural architecture search:
Challenges and solutions. ACM Computing Surveys (CSUR) 54,
1‚Äì34.
[107] Savelsbergh, M., 1997. A branch-and-price algorithm for the gener-
alized assignment problem. Operations research 45, 831‚Äì841.
[108] Schulman, J., Levine, S., Abbeel, P., Jordan, M., Moritz, P., 2015.
Trust region policy optimization, in: International conference on
machine learning, PMLR. pp. 1889‚Äì1897.
[109] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.,
2017.
Proximal policy optimization algorithms.
arXiv preprint
arXiv:1707.06347 .
[110] Shen, M., Ghosh, S., Sattigeri, P., Das, S., Bu, Y., Wornell, G., 2023.
Reliable gradient-free and likelihood-free prompt tuning.
arXiv
preprint arXiv:2305.00593 .
[111] Shi, Y.j., Teng, H.f., Li, Z.q., 2005. Cooperative co-evolutionary
differential evolution for function optimization, in: Advances in
Natural Computation: First International Conference, ICNC 2005,
Changsha, China, August 27-29, 2005, Proceedings, Part II 1,
Springer. pp. 1080‚Äì1088.
[112] Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., Riedmiller,
M., 2014. Deterministic policy gradient algorithms, in: International
conference on machine learning, Pmlr. pp. 387‚Äì395.
[113] Singh, C., Morris, J.X., Aneja, J., Rush, A.M., Gao, J., 2023.
Explaining data patterns in natural language with language models,
in: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and
Interpreting Neural Networks for NLP, pp. 31‚Äì55.
[114] Singh, N., Singh, S., Houssein, E.H., 2022. Hybridizing salp swarm
algorithm with particle swarm optimization algorithm for recent
optimization functions. Evolutionary Intelligence 15, 23‚Äì56.
[115] So, D., Le, Q., Liang, C., 2019.
The evolved transformer, in:
International conference on machine learning, PMLR. pp. 5877‚Äì
5886.
[116] Sun, J., Xu, Z., Yin, H., Yang, D., Xu, D., Chen, Y., Roth, H.R.,
2023a. Fedbpt: Efficient federated black-box prompt tuning for large
language models. arXiv preprint arXiv:2310.01467 .
[117] Sun, Q., Han, C., Chen, N., Zhu, R., Gong, J., Li, X., Gao, M.,
2023b.
Make prompt-based black-box tuning colorful: Boosting
model generalization from three orthogonal perspectives.
arXiv
preprint arXiv:2305.08088 .
[118] Sun, T., He, Z., Qian, H., Zhou, Y., Huang, X., Qiu, X., 2022a.
Bbtv2: Towards a gradient-free future with large language models.
arXiv preprint arXiv:2205.11200 .
[119] Sun, T., Shao, Y., Qian, H., Huang, X., Qiu, X., 2022b.
Black-
box tuning for language-model-as-a-service, in: International Con-
ference on Machine Learning, PMLR. pp. 20841‚Äì20855.
[120] Suzuki, R., Arita, T., 2024. An evolutionary model of personality
traits related to cooperative behavior using a large language model.
Scientific Reports 14, 5989.
[121] Tan, K.C., Feng, L., Jiang, M., 2021.
Evolutionary transfer
optimization-a new frontier in evolutionary computation research.
IEEE Computational Intelligence Magazine 16, 22‚Äì33.
[122] Tao, N., Ventresque, A., Saber, T., 2023. Program synthesis with
generative pre-trained transformers and grammar-guided genetic
programming grammar, in: 2023 IEEE Latin American Conference
on Computational Intelligence (LA-CCI), IEEE. pp. 1‚Äì6.
[123] Tian, Y., Cheng, R., Zhang, X., Cheng, F., Jin, Y., 2017.
An
indicator-based multiobjective evolutionary algorithm with refer-
ence point adaptation for better versatility. IEEE Transactions on
Evolutionary Computation 22, 609‚Äì622.
[124] Tian, Y., Si, L., Zhang, X., Tan, K.C., Jin, Y., 2022. Local model-
based pareto front estimation for multiobjective optimization. IEEE
Transactions on Systems, Man, and Cybernetics: Systems 53, 623‚Äì
634.
[125] Trunfio, G.A., Topa, P., WƒÖs, J., 2016. A new algorithm for adapting
the configuration of subcomponents in large-scale optimization with
cooperative coevolution. Information Sciences 372, 773‚Äì795.
[126] Turing, A.M., 2021. Computing machinery and intelligence (1950)
.
[127] Van Hasselt, H., Guez, A., Silver, D., 2016. Deep reinforcement
learning with double q-learning, in: Proceedings of the AAAI con-
ference on artificial intelligence.
[128] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,
Gomez, A.N., Kaiser, ≈Å., Polosukhin, I., 2017. Attention is all you
need. Advances in neural information processing systems 30.
[129] Vinyals, O., Fortunato, M., Jaitly, N., 2015.
Pointer networks.
Advances in neural information processing systems 28.
[130] Wang, B., Ye, Q., 2020.
Stochastic gradient descent with
nonlinear conjugate gradient-style adaptive momentum.
ArXiv
abs/2012.02188. URL: https://api.semanticscholar.org/CorpusID:
227255095.
[131] Wang, H., He, C., Li, Z., 2020. A new ensemble feature selection
approach based on genetic algorithm. Soft Computing 24, 15811‚Äì
15820.
[132] Watkins, C.J.C.H., 1989. Learning from delayed rewards .
[133] Wei, T., Wang, S., Zhong, J., Liu, D., Zhang, J., 2021. A review on
evolutionary multitask optimization: Trends and challenges. IEEE
Transactions on Evolutionary Computation 26, 941‚Äì960.
[134] Weyssow, M., Zhou, X., Kim, K., Lo, D., Sahraoui, H., 2023. Explor-
ing parameter-efficient fine-tuning techniques for code generation
with large language models. arXiv preprint arXiv:2308.10462 .
[135] Williams, R.J., 1992.
Simple statistical gradient-following algo-
rithms for connectionist reinforcement learning. Machine learning
8, 229‚Äì256.
[136] Wu, C., Yin, S., Qi, W., Wang, X., Tang, Z., Duan, N., 2023a. Visual
chatgpt: Talking, drawing and editing with visual foundation models.
arXiv preprint arXiv:2303.04671 .
[137] Wu, F., Liu, X., Xiao, C., 2023b. Deceptprompt: Exploiting llm-
driven code generation via adversarial natural language instructions.
arXiv preprint arXiv:2312.04730 .
[138] Wu, J., Gan, W., Chen, Z., Wan, S., Philip, S.Y., 2023c. Multimodal
large language models: A survey, in: 2023 IEEE International Con-
ference on Big Data (BigData), IEEE. pp. 2247‚Äì2256.
[139] Wu, X., Wu, S.h., Wu, J., Feng, L., Tan, K.C., 2024. Evolution-
ary computation in the era of large language model: Survey and
roadmap. arXiv preprint arXiv:2401.10034 .
[140] Xiao, L., Chen, X., Shan, X., . Enhancing large language models
with evolutionary fine-tuning for news summary generation. Journal
of Intelligent & Fuzzy Systems , 1‚Äì13.
[141] Xiao, L., Shan, X., Chen, X., 2023. Patterngpt: A pattern-driven
framework for large language model text generation, in: Proceedings
of the 2023 12th International Conference on Computing and Pattern
Recognition, pp. 72‚Äì78.
[142] Xu, H., Chen, Y., Du, Y., Shao, N., Wang, Y., Li, H., Yang, Z., 2022.
Gps: Genetic prompt search for efficient few-shot learning. arXiv
preprint arXiv:2210.17041 .
[143] Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q.V., Zhou, D., Chen,
X., 2023.
Large language models as optimizers.
arXiv preprint
arXiv:2309.03409 .
[144] Yang, H., Li, K., 2023. Instoptima: Evolutionary multi-objective
instruction optimization via large language model-based instruction
operators. arXiv:2310.17630.
Huang et al.: Preprint submitted to Elsevier
Page 22 of 23


--- Page 23 ---
When Large Language Model Meets Optimization
[145] Yang, Q., Li, Y., Gao, X.D., Ma, Y.Y., Lu, Z.Y., Jeon, S.W., Zhang,
J., 2021. An adaptive covariance scaling estimation of distribution
algorithm. Mathematics 9, 3207.
[146] Ye, H., Wang, J., Cao, Z., Song, G., 2024. Reevo: Large language
models as hyper-heuristics with reflective evolution. arXiv preprint
arXiv:2402.01145 .
[147] Yin, Y., Chen, C., Shang, L., Jiang, X., Chen, X., Liu, Q., 2021.
Autotinybert: Automatic hyper-parameter optimization for efficient
pre-trained language models. arXiv preprint arXiv:2107.13686 .
[148] Yu, L., Chen, Q., Lin, J., He, L., 2023a. Black-box prompt tuning
for vision-language model as a service, in: Proceedings of the Thirty-
Second International Joint Conference on Artificial Intelligence, pp.
1686‚Äì1694.
[149] Yu, S., Liu, S., Lin, Z., Pathak, D., Ramanan, D., 2023b. Language
models as black-box optimizers for vision-language models. arXiv
preprint arXiv:2309.05950 .
[150] Zhang, M.R., Desai, N., Bae, J., Lorraine, J., Ba, J., 2023a. Using
large language models for hyperparameter optimization, in: NeurIPS
2023 Foundation Models for Decision Making Workshop.
[151] Zhang, Q., Sun, J., Tsang, E., Ford, J., 2004.
Hybrid estimation
of distribution algorithm for global optimization.
Engineering
computations 21, 91‚Äì107.
[152] Zhang, R., Prokhorchuk, A., Dauwels, J., 2020. Deep reinforcement
learning for traveling salesman problem with time windows and re-
jections, in: 2020 International Joint Conference on Neural Networks
(IJCNN), IEEE. pp. 1‚Äì8.
[153] Zhang, S., Gong, C., Wu, L., Liu, X., Zhou, M., 2023b. Automl-
gpt: Automatic machine learning with gpt.
arXiv preprint
arXiv:2305.02499 .
[154] Zhang, Z., Wang, S., Yu, W., Xu, Y., Iter, D., Zeng, Q., Liu, Y.,
Zhu, C., Jiang, M., 2023c.
Auto-instruct: Automatic instruction
generation and ranking for black-box language models, in: Findings
of the 2023 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2023, Singapore, December 6-10, 2023, Asso-
ciation for Computational Linguistics.
[155] Zhao, J., Wang, Z., Yang, F., 2023a. Genetic prompt search via ex-
ploiting language model probabilities, in: Proceedings of the Thirty-
Second International Joint Conference on Artificial Intelligence,
IJCAI. pp. 5296‚Äì5305.
[156] Zhao, W.X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y.,
Zhang, B., Zhang, J., Dong, Z., et al., 2023b.
A survey of large
language models. arXiv preprint arXiv:2303.18223 .
[157] Zheng, M., Su, X., You, S., Wang, F., Qian, C., Xu, C., Albanie, S.,
2023a. Can gpt-4 perform neural architecture search? arXiv preprint
arXiv:2304.10970 .
[158] Zheng, Y., Tan, Z., Li, P., Liu, Y., 2023b. Black-box prompt tuning
with subspace learning. arXiv preprint arXiv:2305.03518 .
[159] Zhong, R., Xu, Y., Zhang, C., Yu, J., 2024.
Leveraging large
language model to generate a novel metaheuristic algorithm with
crispe framework. arXiv preprint arXiv:2403.16417 .
[160] Zhou, A., Sun, J., Zhang, Q., 2015. An estimation of distribution
algorithm with cheap and expensive local search methods. IEEE
Transactions on Evolutionary Computation 19, 807‚Äì822.
[161] Zhou, H., Wan, X., Vuliƒá, I., Korhonen, A., 2023.
Survival of
the most influential prompts: Efficient black-box prompt search
via clustering and pruning, in: The 2023 Conference on Empirical
Methods in Natural Language Processing.
[162] Zhou, Q., Sheng, K., Zheng, X., Li, K., Tian, Y., Chen, J., Ji, R.,
2024. Training-free transformer architecture search with zero-cost
proxy guided evolution. IEEE Transactions on Pattern Analysis and
Machine Intelligence .
[163] Zhou, X., Qin, A.K., Sun, Y., Tan, K.C., 2021. A survey of advances
in evolutionary neural architecture search, in: 2021 IEEE congress
on evolutionary computation (CEC), IEEE. pp. 950‚Äì957.
Huang et al.: Preprint submitted to Elsevier
Page 23 of 23
