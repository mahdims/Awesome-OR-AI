--- Page 1 ---
Automated Optimization Modeling through Expert-Guided Large Language
Model Reasoning
Beinuo Yang 1*, Qishen Zhou 1*, Junyi Li 2, Chenxing Su 3, Simon Hu 1†,
1ZJU-UIUC Institute, Zhejiang University
2Singapore-MIT Alliance for Research and Technology (SMART)
3Link.AI, Minimal Future Tech., Hong Kong
{beinuo.24, qishenzhou}@intl.zju.edu.com, junyi.li@smart.mit.edu, scx@suchenxing.com,simonhu@zju.edu.cn
Abstract
Optimization Modeling (OM) is essential for solving com-
plex decision-making problems. However, the process re-
mains time-consuming and error-prone, heavily relying on
domain experts. While Large Language Models (LLMs)
show promise in addressing these challenges through their
natural language understanding and reasoning capabilities,
current approaches face three critical limitations: high bench-
mark labeling error rates reaching up to 42%, narrow eval-
uation scope that only considers optimal values, and com-
putational inefficiency due to heavy reliance on multi-agent
systems or model fine-tuning. In this work, we first en-
hance existing datasets through systematic error correction
and more comprehensive annotation. Additionally, we in-
troduce LogiOR, a new optimization modeling benchmark
from the logistics domain, containing more complex prob-
lems with standardized annotations. Furthermore, we present
ORThought, a novel framework that leverages expert-level
optimization modeling principles through chain-of-thought
reasoning to automate the OM process. Through extensive
empirical evaluation, we demonstrate that ORThought out-
performs existing approaches, including multi-agent frame-
works, with particularly significant advantages on complex
optimization problems. Finally, we provide a systematic anal-
ysis of our method, identifying critical success factors and
failure modes, providing valuable insights for future research
on LLM-based optimization modeling.
Code — https://github.com/BeinuoYang/ORThought
Datasets — https://huggingface.co/datasets/LabMem012/LogiOR
Introduction
Optimization Modeling (OM) is a fundamental method-
ology in management science for solving complex decision-
making problems across various domains, from logistics
management to industrial production. However, the OM pro-
cess faces several key challenges: understanding and for-
malizing unstructured problems, constructing mathematical
models, and implementing solutions through programming
tools. Each phase requires substantial domain expertise and
is often time-intensive and error-prone. These limitations re-
strict the wider application of optimization methods, creat-
ing a need for more efficient approaches to OM practice.
*Both authors contributed equally to this research.
†Corresponding author.
Large Language Models (LLMs), with their fundamen-
tal capabilities, are well-positioned to solve these OM chal-
lenges. Their strong natural language understanding abil-
ity enables domain knowledge acquisition and process-
ing, while structured reasoning frameworks like Chain-
of-Thought (CoT) reasoning (Wei et al. 2022), Tree of
Thoughts (Yao et al. 2023), and Graph of Thoughts (Besta
et al. 2024) enhance their complex problem-solving abili-
ties in mathematics and reasoning. Furthermore, with mature
tool integration capabilities, LLMs can now effectively in-
teract with programming environments and external solvers.
This allows LLMs to automate the OM in a closed-loop
pipeline.
Following the remarkable advances in LLMs, the NL4Opt
competition (Ramamonjison et al. 2023) initiated the explo-
ration of using LLMs to transform natural language prob-
lems into mathematical optimization models. This pioneer-
ing study stimulates more explorations in two main di-
rections: problem understanding/analysis and modeling au-
tomation. In problem understanding, works like OptiGu-
ide (Li et al. 2023) and EOR (Zhang et al. 2024) focused on
analyzing how optimal solutions respond to varying condi-
tions, while ORQA (Mostajabdaveh et al. 2025) aims to ex-
tract and analyze key optimization elements from problem
descriptions. On the automation front, several frameworks
have emerged: CAFA (Deng et al. 2024) introduced effi-
cient prompting strategies for problem formalization, while
Chain-of-Experts (CoE) (Xiao et al. 2023), OptiMUS (Ah-
madiTeshnizi, Gao, and Udell 2024), and ORMind (Wang
et al. 2025) proposed different multi-agent architectures
where different specialized LLM-based agents collaborate
with each other. Some studies like ORLM (Huang et al.
2025) and LLMOPT (Jiang et al. 2025) have further ex-
plored fine-tuning open-source models for optimization
tasks. Moreover, the evaluation benchmarks have evolved
from simple textbook problems to complex industrial cases
with implicit information and constraints. These advance-
ments highlight the potential of LLMs to serve as powerful
tools for OM, enabling the seamless conversion of natural
language problem statements into formal optimization mod-
els.
However, as an emerging research direction, current
work faces three main challenges: First, existing bench-
mark datasets show labeling error rates of up to 42%, and
arXiv:2508.14410v2  [cs.AI]  22 Aug 2025


--- Page 2 ---
Solve Agent
Tool
Usage
Detection
Diagnosis
Success
(Max try)
Repair
N
Y
Problem described in natural language
Code
Model Agent
Human
Multi-level human
readable results
Feedback
Propose
Sandbox
Solver
Description of core
elements
Interpreable modeling
process
Mathmatical model
Code for model sovling
Optimal solution
Reasoning Process
A large cross-docking center is responsible for assigning docks for the
next hour's operations, which involve 4 arriving shipments and 3
departing shipments... The efficiency of forklift operations within a
zone decreases with congestion, which can be treat as a cost that
increases with the square of the total number of items passing
through that zone. The congestion cost coefficient is 0.5 for Zone A
and 0.4 for Zone B... How should the center assign each shipment to a
unique dock to minimize the total internal handling cost?
A. Core Optimization Objective:
The goal is to assign each of the 4 arriving
shipments and 3 departing shipments to a
unique dock such that the total internal...
B. Key Decision Variables: 
Assignment Variables: For each shipment...
Mathematical Model
Figure 1: The framework of ORThought.
some benchmarks provide only optimal solution, which lim-
its comprehensive evaluation of model capabilities. Second,
current evaluation frameworks rely primarily on comparing
solver outputs with known optimal values, overlooking the
need for systematic analysis of the OM process itself, mak-
ing it difficult to gain insights into LLMs’ modeling capa-
bilities and identify directions for improvement. Third, most
existing methods either use multi-agent systems with high
token consumption or require high-end computational in-
frastructure for LLM fine-tuning, limiting their widespread
adoption in real-world applications.
To address these challenges, we focus on automated mod-
eling and propose a comprehensive solution that focuses on
improving benchmark quality, developing systematic evalu-
ation, and designing efficient modeling frameworks. Instead
of relying on multi-agent systems or model fine-tuning, we
explore the potential of chain-of-thought reasoning inspired
by expert OM practices.
Our work makes four primary contributions: (1) We en-
hance existing datasets through systematic error correction
and more comprehensive annotation. Additionally, we in-
troduce LogiOR, a new optimization modeling benchmark
from the logistics domain, containing more complex prob-
lems. For all datasets, we provide standardized annota-
tions including mathematical models, solution code, prob-
lem characteristics, optimal solution. This unified annota-
tion scheme enables comprehensive evaluation including er-
ror analysis, facilitates peer verification of benchmark qual-
ity, and provides richer supervision signals for training rea-
soning LLM. (2) We propose ORThought, a novel frame-
work that leverages expert-level optimization modeling prin-
ciples through chain-of-thought reasoning to automate the
OM process, achieving high modeling accuracy with sig-
nificantly lower computational cost. (3) We evaluate our
framework through extensive experiments and demonstrate
that ORThought outperforms existing approaches, includ-
ing multi-agent frameworks, exhibiting consistent advan-
tages across different problem types, sizes, and base LLMs,
with particularly significant advantages on complex opti-
mization problems. (4) We provide systematic analysis of
our method, identifying critical success factors and fail-
ure modes through comprehensive ablation studies, offer-
ing valuable insights for future research on LLM-based op-
timization modeling.
Methodology
In this section, we introduce ORThought, a novel frame-
work for automated optimization problem modeling and
solving. ORThought leverages expert knowledge and a for-
malized solution process to steer the collaboration between
LLMs and Operation Research (OR) solvers. As illustrated
in Fig. 1, ORThought consists of two primary components:
the Model Agent and the Solve Agent. The Model Agent,
leveraging CoT reasoning embedded with expert optimiza-
tion modeling knowledge, comprehends real-world prob-
lems and translates them into precise mathematical mod-
els with corresponding solution code. The Solve Agent,
equipped with a Python execution environment and ad-
vanced solvers like Gurobi, proceeds to execute these mod-
els and iteratively refine the solutions through an intelli-
gent Detection-Diagnosis-Repair workflow. This integrated
approach ensures accurate and efficient decision-making in
complex optimization scenarios. Both agents are designed
using prompts, which has been proven effective in guiding
LLMs to perform complex reasoning tasks and enhancing
their domain-specific problem-solving capabilities(Liu et al.
2023). The specific prompts can be found in the Appendix.
Model agent
Model Agent converts natural language to optimization code
through three modules: problem understanding, mathemati-
cal modeling, and code generation. This modular pipeline
ensures systematic and reliable transformation at each stage.


--- Page 3 ---
Problem understanding
We first guide the Agent to de-
velop an initial understanding of the optimization problem,
extracting key information from natural language descrip-
tions to form a preliminary problem cognition. Building
upon this foundation, we leverage real-world operations re-
search experts’ modeling experience to guide the model in
systematically identifying three core elements of optimiza-
tion problems: optimization objectives, decision variables,
and constraints. Specifically, we guide the model to clearly
distinguish between maximization and minimization direc-
tions, strictly classify decision variables into continuous, in-
teger, and binary types, while systematically organizing con-
straints from diverse problem aspects (e.g., resource limita-
tions, operational requirements, etc.) Through this profes-
sional guidance, we ensure that the model maintains struc-
tured problem expressions, accurately defines the properties
of each variable, and conducts modeling based on actual
problem contexts rather than oversimplified assumptions.
Mathematical modeling
We design a progressive mod-
eling process that evolves from basic formulation to pro-
fessional refinement. First, we guide the Agent to analyze
the results from problem understanding, then through an in-
cremental modeling process, direct the Agent to systemati-
cally construct mathematical expressions for decision vari-
ables, objective functions, and constraints. Building upon
this foundation and adhering to the principles of clarity,
interpretability, and notational consistency, we ensure the
Agent provides rigorous derivation explanations for each ob-
jective function term and constraint. Finally, through this
systematic approach, we guide the Agent to integrate all
components - decision variables, objective functions, and
constraints - using standardized optimization notation to for-
mulate the complete mathematical model.
Code generation
We guide the Agent to accurately trans-
late the standardized mathematical formulation into exe-
cutable Python code, leveraging existing mathematical op-
timization solvers. In this work, we utilize Gurobi through
its Python interface, a widely-adopted commercial solver
known for its robust performance and comprehensive doc-
umentation (Gurobi Optimization, LLC 2024). The Agent
aims to generate function-based implementations, where
each function encapsulates complete model logic with
proper parameter definition and standardized return for-
mats. This structured approach ensures consistent solu-
tion reporting across different scenarios (optimal, infeasi-
ble, or unbounded) and maintains compatibility with alter-
native solver interfaces, including commercial solvers like
CPLEX (Manual 1987) and open-source alternatives.
Through the systematic design and professional imple-
mentation of these three phases, we have successfully de-
veloped an end-to-end framework that accurately transforms
problem descriptions into executable optimization solutions.
Notably, the integration of domain expertise in the problem
understanding and mathematical model construction phases
effectively guarantees the accuracy and interpretability of
the final model.
LogiOR ComplexOR NLP4LP IndustryOR
LP
22
10
61
18
ILP
43
7
203
47
MILP
11
1
0
14
NLP
16
0
0
4
Toy
14
13
264
33
Small
39
5
0
36
Medium 39
0
0
14
Table 1: Characteristics of the test benchmarks.
Solve agent
The Solve Agent serves as the computational engine of OR-
Thought, operating through a three-phase iterative work-
flow: Detection, Diagnosis, and Repair. This design ensures
robust solution execution and automatic error recovery.
Detection
In the detection phase, the agent executes the
Gurobipy code within a secure Python sandbox environ-
ment, which enables safe code execution while capturing
runtime exceptions and solver-specific issues.
Diagnosis
Following execution, the diagnosis phase per-
forms comprehensive analysis on the solution status, exam-
ining whether optimal solutions are achieved, infeasibility
conditions exist, or other termination scenarios occur.
Repair
Upon error detection, the repair phase is activated,
which implements an intelligent error recovery process
through root cause analysis based on error messages, prob-
lem descriptions, mathematical models, and faulty code. The
agent leverages optimization theory and programming ex-
pertise to generate corrective code, maintaining the integrity
of the original mathematical formulation while resolving
identified code defects.
This iterative workflow continues until a valid solution is
obtained or a terminal condition is reached, ensuring robust
and reliable optimization problem solving.
Experiments and Analysis
In this section, extensive experiments are conducted to an-
swer the following questions:
• RQ1: Does the optimization modeling performance of
ORThought outperform other benchmark methods?
• RQ2: How does the optimization modeling performance
of ORThought vary with problem types and sizes?
• RQ3: What are the main types of errors made by OR-
Thought?
• RQ4: How do different components of ORThought con-
tribute to its overall performance?
• RQ5: How do different hyperparameters (LLM choice,
model size, and temperature) affect ORThought’s perfor-
mance?


--- Page 4 ---
Datasets
To facilitate comprehensive evaluation, we propose a new
benchmark dataset and enhance three well-known existing
datasets for testing. Specifically:
• LogiOR. We construct LogiOR, a comprehensive bench-
mark dataset comprising 92 logistics and supply chain
optimization problems, which was developed over two
months under the guidance of three Operations Research
(OR) experts. The problems are adapted from classical
OR solver test datasets (Beasley 1990), textbook exam-
ples (Williams 2013), research papers, and real-world ap-
plications. LogiOR covers a broad spectrum of optimiza-
tion types including Linear Programming (LP), Integer
Linear Programming (ILP), Mixed-Integer Linear Pro-
gramming (MILP), and Nonlinear Programming (NLP).
Each problem is equipped with standardized annotations
including mathematical formulations, executable Gurobi
implementation code, optimal solution, problem charac-
teristics (type, size metrics). This enables comprehensive
evaluation, facilitates peer verification, and provides rich
supervisory signals for reasoning LLM training.
• ComplexOR. The ComplexOR dataset (Xiao et al.
2023), originally developed with three OR experts, spans
domains of supply chain, industry scheduling and logis-
tics. The original annotations include mathematical for-
mulations, executable solver code, and optimal solution.
We utilized all 18 problems from its GitHub repository,
corrected annotation errors in 6 instances, and enhanced
the dataset by adding problem type and size annotations
following the LogiOR annotation standard.
• NLP4LP. The NLP4LP dataset (Ramamonjison et al.
2023) comprises optimization problems across retail, en-
ergy, and other industrial domains. The original annota-
tions include executable solver code and optimal solu-
tion. From its HuggingFace repository, we selected all
269 problems, removed 5 problems with insufficient in-
formation for modeling, corrected annotation errors in
44 problems, and enriched the dataset with mathematical
formulations, problem type, and size annotations follow-
ing the LogiOR standard.
• IndustryOR. The IndustryOR dataset (Huang et al.
2025), the first industrial dataset specifically designed
for optimization modeling, contains 100 real-world OR
problems from eight sectors including education, trans-
portation, and finance. The original annotations only
include optimal solution. We enhanced the dataset
by removing 17 problems with insufficient informa-
tion, corrected annotation errors in 23 problems, and
adding mathematical formulations, executable solver
code, problem type, and size annotations following the
LogiOR standard.
Table 1 shows the distribution of problems by their types
and sizes. The problem size is classified as Toy (< 5 vari-
ables, 10 constraints, 20 non-zero coefficients), Small (<
25 variables, 40 constraints, 80 non-zero coefficients), or
Medium (otherwise). Compared to other datasets, LogiOR
stands out with more challenging MILP and NLP problems,
as well as a larger portion of medium-size instances. An ex-
ample problem from LogiOR is shown in Appendix.
Experiment setup
Unless otherwise specified, GPT-4.1-nano (OpenAI 2024)
with temperature 0 is used as the backbone. Each experi-
ment is repeated three times to account for potential random-
ness. We use average success rate as the primary evaluation
metric, where a trial is considered successful if the LLM-
generated solution achieves the ground-truth optimal objec-
tive value verified by OR experts. Additionally, we measure
computational efficiency by tracking the average token con-
sumption of each method.
We compare our approach against three categories of
baselines: multi-agent methods including Chain-of-Experts
(CoE) (Xiao et al. 2023) and OptiMUS(AhmadiTeshnizi,
Gao, and Udell 2024); reasoning methods including Chain-
of-Thought (CoT) (Wei et al. 2022), Self-Consistency
(SC) (Wang et al. 2023), and Reflexion (Shinn et al. 2023);
and a vanilla baseline that directly generates the mathemat-
ical model with a simple prompt. For CoE and OptiMUS,
we strictly follow their official implementations, with minor
adaptations to CoE’s code to ensure a fair comparison.
Overall performance (RQ1)
Success rate1
As shown in Table 2, ORThought consis-
tently outperforms existing methods across all datasets, with
relative improvements of 13-28 percentage points over Stan-
dard baseline. The performance varies significantly across
datasets: while achieving a high success rate of 89.02%
on NLP4LP, the performance decreases on more complex
datasets (57.83% on IndustryOR and 46.01% on LogiOR).
These results demonstrate the effectiveness of our method
while also revealing the persistent challenges in handling
complex optimization problems.
A detailed examination of different methodologies reveals
distinct patterns. Among multi-agent approaches, OptiMUS
shows degraded performance compared to the Standard
baseline, with a drop of 3.31 percentage points on NLP4LP.
This deterioration can be attributed to its workflow-based
design, where errors cascade through sequential agent inter-
actions. While CoE mitigates this cascading error problem
through a shared information pool, its unconstrained agent
collaboration often leads to illogical operation sequences,
such as code generation before mathematical model formu-
lation, limiting its performance improvement.
Reasoning methods generally demonstrate better perfor-
mance than multi-agent approaches, with Reflexion and CoT
emerging as the most competitive baseline methods across
different datasets. While we do not view these results as
definitive evidence of the superiority of reasoning methods
over multi-agent approaches, they do highlight that reason-
ing methods are easier to design and implement, and still
1OptiMUS requires an intermediate step of converting prob-
lems in natural language into JSON format. As this conversion
consistently produces JSON parsing errors, we only evaluated Op-
tiMUS on the NLP4LP dataset using the authors’ provided JSON
outputs to bypass this problematic transformation step.


--- Page 5 ---
Method
NLP4LP (264)
IndustryOR (83)
LogiOR (92)
ComplexOR (18)
Standard
72.73%
42.17%
33.34%
50.00%
OptiMUS
69.42% ↓3.31
/
/
/
Chain-of-Experts
75.00% ↑2.27
40.96% ↓1.21
34.78% ↑1.44
55.56% ↑5.56
Chain-of-Thought
75.00% ↑2.27
43.37% ↑1.20
37.32% ↑3.98
61.11% ↑11.11
Self-Consistency
71.21% ↓1.52
48.19% ↑6.02
32.25% ↓1.09
50.00% - 0.00
Reflexion
77.65% ↑4.92
48.19% ↑6.02
36.59% ↑3.25
61.11% ↑11.11
ORThought (ours)
89.02% ↑16.29
57.83% ↑15.66
46.01% ↑12.67
77.78% ↑27.78
Table 2: Comparison of success rates across different methods on various datasets. Numbers in parentheses indicate sample
size. Bold: best performance; Underline: second best. For each method, the first number shows success rate, while arrows (↑/↓)
followed by numbers indicate percentage points increase/decrease compared to Standard baseline.
Method
LP (111)
ILP (300)
MILP (26)
NLP (20)
Standard
62.16%
66.00%
19.23%
13.35%
CoE
56.76%
67.67%
19.23%
15.00%
CoT
66.67%
64.67%
23.08%
26.65%
Reflexion
68.47%
67.00%
28.19%
26.65%
ORThought 66.67%
82.33%
30.77%
51.65%
Improve
↓1.80
↑14.66
↑2.58
↑25.00
Table 3: Comparison of success rates across different opti-
mization problem types. Improve: percentage points gained
by ORThought compared to the most competitive baseline.
Method
Toy (324)
Small (80)
Medium (53)
Standard
68.93%
36.66%
26.42%
CoE
69.44%
40.00%
32.08%
CoT
70.68%
41.66%
32.08%
Reflexion
74.07%
43.34%
28.30%
ORThought
85.39%
49.59%
43.40%
Imporve
↑11.32
↑6.25
↑11.32
Table 4: Comparison of success rates across different opti-
mization problem size.
achieve competitive performance in optimization modeling
tasks. ORThought builds upon these insights by incorporat-
ing expert-level planning into the reasoning process, achiev-
ing 9-17 percentage points improvements over these strong
baselines while maintaining implementation simplicity.
Token cost
To evaluate computational efficiency, Fig. 2
analyzes the average token consumption per problem across
all datasets. The average prompt token represents the aver-
age number of input tokens for task instructions and exam-
ples, while the average completion token indicates the aver-
age number of output tokens generated by the model. Multi-
agent methods like CoE and OptiMUS are excluded from
the figure due to their inherently higher token usage granted
by free exploration privileges - for instance, CoE’s aver-
age prompt token reaches around 50,000. Among reasoning
methods, ORThought demonstrates low average prompt to-
ken usage, slightly higher than CoT, and significantly lower
than Reflexion, the method achieves the second-best accu-
Standard
CoT
Reflexion
SC
ORThought
0
2K
4K
6K
Token Count
Average Prompt Tokens
Average Completion Tokens
Figure 2: Token consumption comparison across methods.
racy. Its average completion token is slightly higher than
CoT but significantly lower than SC and Reflexion. These
results demonstrate that ORThought achieves superior per-
formance while maintaining excellent token efficiency.
Performance analysis by problem characteristics
(RQ2)
To better understand ORThought’s advantages, we analyze
its performance across different problem types and sizes.
As shown in Table 3, ORThought significantly outperforms
baselines on three out of four problem types, achieving the
highest success rate on ILP, MILP and NLP problems. For
LP problems, ORThought matches the second-best perfor-
mance while falling 1.80 percentage points behind Reflex-
ion. The performance gains are particularly pronounced for
ILP and NLP problems, where ORThought improves upon
the second-best methods by 14.66 and 25.00 percentage
points respectively. This suggests that ORThought’s struc-
tured reasoning approach is especially effective for more
challenging problem types.
Table 4 further reveals ORThought’s performance across
different problem sizes. ORThought consistently achieves
the highest success rates, with notable improvements of
11.32, 6.25, and 11.32 percentage points over the second-
best methods for toy, small, and medium problems respec-
tively.
Examining performance across problem sizes reveals a
consistent pattern: success rates decline sharply as prob-
lem complexity increases, with all methods showing signif-


--- Page 6 ---
Var.
Const.
Obj.
Par. Code
0
20
40
60
Number of Errors
35
21
7
52
31
4
28
4
4
13
8
Incorrect
Missing
Spurious
Figure 3: Analysis of modeling failures in ORThought
across different components. Here, Var., Obj., Const., Par.,
and Code represent variables, objective functions, con-
straints, parameters in optimization model, and Gurobi
Python code, respectively.
LP
ILP MILP NLP
40
30
20
10
0
Performance Decline
(Percent Point)
(a)
Toy
Small Medium
20
15
10
5
0
(b)
standard
w/o understanding
w/o repair
w/o expert knowledge in
understanding
w/o expert knowledge in
mathematical modeling
Figure 4: Impact of different components in ORThought
across (a) problem types and (b) problem sizes.
icant performance degradation from toy to medium prob-
lems. This universal trend underscores that scaling to larger
optimization problems remains a fundamental challenge in
the field, pointing to a critical direction for future research.
Error analysis (RQ3)
To systematically analyze ORThought’s failure patterns, we
examine errors from two dimensions: error types and opti-
mization elements. We classify modeling errors into three
types: “incorrect” denotes cases where LLM correctly iden-
tifies the need for certain elements (e.g., variables or con-
straints) but implements them improperly, “missing” repre-
sents omitted necessary elements, and “spurious” indicates
falsely introduced elements that should not exist. The results
show that spurious cases are the rarest, with merely 15 in-
stances. This primarily reflects ORThought’s solid grasp of
optimization modeling principles. The predominance of in-
correct cases (136) over missing cases (56) indicates that,
when facing uncertainty, ORThought tends to attempt mod-
eling rather than abandon it. This proactive approach, though
not always successful, promotes active exploration and in-
creases opportunities for potential solutions. Examining the
distribution across optimization elements reveals that con-
straints are the most error-prone, followed by variables, and
objective functions. This highlights the particular challenge
of translating logical relationships among variables into pre-
cise mathematical expressions.
Ablation study (RQ4)
To systematically evaluate the contribution of different com-
ponents in ORThought, we conduct four ablation experi-
ments: (1) removing the understanding module in Model
Agent, (2) removing the repair functionality in Solve Agent,
(3) removing expert knowledge in the understanding mod-
ule in Model Agent, and (4) removing expert knowledge
in the mathematical modeling module in Model Agent. For
each variant, we compare its performance against the full
ORThought framework across different problem types and
sizes, with results shown in Fig. 4. The standard baseline is
also included as a reference. Detailed experimental settings
for each ablation variant can be found in Appendix.
First, we evaluate the impact of complete module re-
moval. The removal of the understanding module in Model
Agent leads to significant performance degradation across
all problem types except LP, with performance drops partic-
ularly pronounced in more complex problems, demonstrat-
ing that problem understanding serves as a cornerstone for
successful optimization modeling. The repair functionality
shows relatively modest contribution. However, it is worth
noting that this repair mechanism only activates when the
LLM-generated solution code fails to execute. Therefore,
we conducted a dedicated evaluation of code execution er-
rors and found that this functionality successfully corrected
execution errors in 25 out of 31 problems, with 11 of these
ultimately yielding the correct optimal solution, highlighting
its crucial role in enhancing system robustness.
We further examine the contribution by removing expert
knowledge from the understanding and mathematical mod-
eling modules respectively. The results show that expert
knowledge in the understanding phase substantially con-
tributes to model performance, especially as problem size
increases from toy to medium size. While expert knowl-
edge in the mathematical modeling phase generally shows a
smaller impact compared to that in the understanding phase,
it demonstrates comparable or even stronger impact in NLP
problems and medium-size instances, revealing its crucial
value in handling complex scenarios.
For LP problems, while ORThought consistently outper-
forms the standard baseline, the contribution of individ-
ual components is less distinctive. This suggests that OR-
Thought’s superior performance in LP problems stems from
the synergistic effect of its components rather than any sin-
gle module, warranting further investigation into the under-
lying mechanisms.
Hyperparameter analysis (RQ5)
LLM choice
We evaluate ORThought across three rep-
resentative LLMs: DeepSeek-V3 (DeepSeek-AI 2025),
Qwen3-32B (Qwen Team 2025), and GPT4.1 Nano. As


--- Page 7 ---
Method
GPT-4.1-nano
DeepSeek-V3
Qwen3-32b
Standard
58.35%
58.75%
64.55%
CoT
62.12% ↑3.77
59.96% ↑1.21
67.83% ↑3.28
SC
58.35% - 0.00
55.47% ↓3.28
68.93% ↑4.38
Reflexion
63.39% ↑5.04
65.86% ↑7.11
68.27% ↑3.72
ORThought 74.25% ↑15.90
71.01% ↑12.26
73.38% ↑8.83
Table 5: Performance across Different LLMs.
shown in Table 5, the standard approach results demonstrate
inherent differences in optimization modeling capabilities
across base LLMs. However, ORThought effectively bridges
these gaps, achieving consistently high performance across
all models. Notably, our method maintains superior perfor-
mance regardless of the underlying LLM, outperforming the
second-best baseline by a substantial margin (4-10 percent-
age points).
LLM size
We conduct experiments using Qwen3 model
series, spanning from 1.7B to 32B parameters, to investigate
how ORThought’s solving capabilities evolve across model
sizes. As shown in Fig. 5, our experiments reveal several key
findings:
From the perspective of problem types, simpler problems
(LP and ILP) show strong performance even with smaller
models. Their success rates increase most rapidly in the
early scaling stages (1.7B to 4B), followed by continued
but more moderate improvements at larger sizes. In contrast,
more complex problems (MILP and NLP) are unsolvable by
small models and show slow initial improvement, but exhibit
a notable performance jump at the 8-14B size.
Problem size analysis reveals similar scaling behaviors.
Toy-size problems show rapid early improvements followed
by gradual gains. Small-size problems demonstrate a dis-
tinct performance leap at 8-14B, while medium-size prob-
lems show consistent improvements with model size, indi-
cating that larger models are particularly beneficial for han-
dling increased problem complexity.
Two notable plateaus emerge in the scaling curves: one at
4-8B and another at 14-32B. These plateaus, combined with
the observed performance jumps, suggest that certain rea-
soning capabilities may emerge at specific parameter thresh-
olds rather than scaling smoothly with model size.
These results establish clear relationships between model
size and optimization capabilities, revealing that perfor-
mance improvements do not size uniformly across problem
types and sizes. This insight suggests that while scaling up
models can enhance performance, the choice of model size
should be carefully considered based on the specific opti-
mization task at hand.
Temperature
As shown in Figure 6, we examine the effect
of LLM temperature on ORThought’s performance across
the range [0,1], analyzing different problem types and sizes.
The results reveal varying sensitivity to LLM temperature
changes across different categories. For problem types ex-
cept MILP, despite some fluctuations in the curves, the over-
all trend shows declining performance with higher tempera-
1.7B
4B
8B 14B 32B
Model Size (Billion)
0
25
50
75
100
Success Rate(%)
(a)
LP
ILP
MILP
NLP
1.7B
4B
8B 14B 32B
Model Size (Billion)
0
25
50
75
100
(b)
Toy
Small
Medium
Figure 5: Performance of ORThought under Different LLM
Model Sizes (x-axis in log scale).
0 0.2 0.4 0.6 0.8 1
Temperature
0
25
50
75
100
Success Rate (%)
(a)
LP
ILP
MILP
NLP
0 0.2 0.4 0.6 0.8 1
Temperature
0
25
50
75
100
(b)
Toy
Small
Medium
Figure 6: Impact of LLM temperature on ORThought per-
formance.
tures. This general downward trend is also observed across
different problem sizes. MILP problems exhibit a distinct U-
shaped pattern, with performance reaching its lowest point
at temperature 0.4 before showing slight recovery. Despite
these variations, temperature 0 yields the best performance
for most problem categories, with LP problems showing rel-
atively stable performance across temperatures from 0 to
0.6. These results suggest that deterministic LLM genera-
tion generally produces more reliable optimization modeling
outcomes.
Conclusion
In this work, we address critical challenges in automat-
ing optimization modeling through Large Language Mod-
els by enhancing benchmark quality, introducing LogiOR,
and proposing ORThought, an efficient framework leverag-
ing expert-level principles through chain-of-thought reason-
ing. Our extensive experiments demonstrate ORThought’s
superior performance with 9-17 percentage point improve-
ments over sub-optimal baselines while maintaining excel-
lent token efficiency. While the framework exhibits consis-
tent advantages across different problem types, sizes, and
base LLMs, the observed performance degradation on com-
plex problems and challenges in constraint formulation point


--- Page 8 ---
to important future research directions, including develop-
ing more sophisticated in-context learning strategies, such
as Retrieval Augmented Generation (RAG), and exploring
human-machine collaboration systems.
References
AhmadiTeshnizi, A.; Gao, W.; and Udell, M. 2024. Opti-
MUS: scalable optimization modeling with (MI)LP solvers
and large language models.
In Proceedings of the 41st
International Conference on Machine Learning, ICML’24.
JMLR.org.
Beasley, J. E. 1990. OR-Library: distributing test problems
by electronic mail. Journal of the operational research so-
ciety, 41(11): 1069–1072.
Besta, M.; Blach, N.; Kubicek, A.; Gerstenberger, R.;
Podstawski, M.; Gianinazzi, L.; Gajda, J.; Lehmann, T.;
Niewiadomski, H.; Nyczyk, P.; et al. 2024.
Graph of
thoughts: Solving elaborate problems with large language
models. In Proceedings of the AAAI conference on artificial
intelligence, volume 38, 17682–17690.
DeepSeek-AI. 2025.
DeepSeek-V3 Technical Report.
arXiv:2412.19437.
Deng, H.; Zheng, B.; Jiang, Y.; and Tran, T. H. 2024.
CAFA: Coding as Auto-Formulation Can Boost Large Lan-
guage Models in Solving Linear Programming Problem. In
The 4th Workshop on Mathematical Reasoning and AI at
NeurIPS’24.
Gurobi Optimization, LLC. 2024. Gurobi Optimizer Refer-
ence Manual.
Huang, C.; Tang, Z.; Hu, S.; Jiang, R.; Zheng, X.; Ge, D.;
Wang, B.; and Wang, Z. 2025.
ORLM: A Customizable
Framework in Training Large Models for Automated Op-
timization Modeling. Operations Research.
Jiang, C.; Shu, X.; Qian, H.; Lu, X.; Zhou, J.; Zhou, A.; and
Yu, Y. 2025. LLMOPT: Learning to Define and Solve Gen-
eral Optimization Problems from Scratch. In Proceedings of
the Thirteenth International Conference on Learning Repre-
sentations (ICLR). Singapore, Singapore.
Li, B.; Mellou, K.; Zhang, B.; Pathuri, J.; and Menache, I.
2023. Large Language Models for Supply Chain Optimiza-
tion. arXiv:2307.03875.
Liu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig,
G. 2023. Pre-Train, Prompt, and Predict: A Systematic Sur-
vey of Prompting Methods in Natural Language Processing.
ACM Comput. Surv., 55(9): 195:1–195:35.
Manual, C. U. 1987. Ibm ilog cplex optimization studio.
Version, 12(1987-2018): 1.
Mostajabdaveh, M.; Yu, T. T. L.; Dash, S. C. B.; Ramamon-
jison, R.; Byusa, J. S.; Carenini, G.; Zhou, Z.; and Zhang,
Y. 2025. Evaluating LLM Reasoning in the Operations Re-
search Domain with ORQA. Proceedings of the AAAI Con-
ference on Artificial Intelligence, 39(23): 24902–24910.
OpenAI. 2024. GPT-4 Technical Report. arXiv:2303.08774.
Qwen
Team.
2025.
Qwen3
Technical
Report.
arXiv:2505.09388.
Ramamonjison, R.; Yu, T.; Li, R.; Li, H.; Carenini,
G.; Ghaddar, B.; He, S.; Mostajabdaveh, M.; Banitalebi-
Dehkordi, A.; Zhou, Z.; and Zhang, Y. 2023.
NL4Opt
Competition: Formulating Optimization Problems Based on
Their Natural Language Descriptions. In Proceedings of the
NeurIPS 2022 Competitions Track, 189–203. PMLR.
Shinn, N.; Cassano, F.; Gopinath, A.; Narasimhan, K.; and
Yao, S. 2023. Reflexion: Language agents with verbal re-
inforcement learning. Advances in Neural Information Pro-
cessing Systems, 36: 8634–8652.
Wang, X.; Wei, J.; Schuurmans, D.; Le, Q. V.; Chi, E. H.;
Narang, S.; Chowdhery, A.; and Zhou, D. 2023.
Self-
Consistency Improves Chain of Thought Reasoning in Lan-
guage Models. In The Eleventh International Conference on
Learning Representations.
Wang, Z.; Chen, B.; Huang, Y.; Cao, Q.; He, M.; Fan,
J.; and Liang, X. 2025.
ORMind: A Cognitive-Inspired
End-to-End Reasoning Framework for Operations Research.
arXiv:2506.01326.
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;
Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022.
Chain-of-
thought prompting elicits reasoning in large language mod-
els. Advances in neural information processing systems, 35:
24824–24837.
Williams, H. P. 2013. Model building in mathematical pro-
gramming. Hoboken, N.J.: Wiley. ISBN 9781118443330
1118443330.
Xiao, Z.; Zhang, D.; Wu, Y.; Xu, L.; Wang, Y. J.; Han, X.;
Fu, X.; Zhong, T.; Zeng, J.; Song, M.; and Chen, G. 2023.
Chain-of-Experts: When LLMs Meet Complex Operations
Research Problems. In The Twelfth International Confer-
ence on Learning Representations.
Yao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T.; Cao,
Y.; and Narasimhan, K. 2023. Tree of thoughts: Deliber-
ate problem solving with large language models. Advances
in neural information processing systems, 36: 11809–11822.
Zhang, Y.; Kang, Q.; Yu, W. Y.; HaileiGong; Fu, X.; Han,
X.; Zhong, T.; and Ma, C. 2024. Decision Information Meets
Large Language Models: The Future of Explainable Opera-
tions Research. In The Thirteenth International Conference
on Learning Representations.
LogiOR
An example (prob 081) from the proposed LogiOR dataset
is presented below, featuring rich annotation information.
These annotations provide strong support for model evalu-
ation and facilitate peer review to identify potential errors in
the dataset, while also offering potential assistance for train-
ing reasoning models.
Problem description
A large manufacturing enterprise needs to ship a consign-
ment of 1,000 tons of goods from its factory in City F to
a distribution center in City D. There are three potential
transportation routes available. Each route is composed of
segments using different modes of transport (road or rail).


--- Page 9 ---
Route
Segment
Mode
Distance (km)
Base Fee ($/ton-km)
Cong. Coeff.
Background (tons)
1
1A
Road
150
2.00
5 × 10−7
2,000
1B
Rail
500
0.80
–
–
2
2A
Road
200
2.10
–
–
2B
Road
350
1.90
8 × 10−7
1,500
3
3A
Road
100
2.20
–
–
3B
Rail
600
0.75
–
–
Table 6: Route Details
The transportation cost for each segment is calculated based
on a per ton-kilometer rate. Furthermore, certain road seg-
ments are subject to a congestion fee to account for the so-
cial costs of traffic. This fee is non-linear and is calculated
as Congestion Coefficient × (Total Traffic
Flow)2. The total traffic flow on a segment is the sum of the
cargo shipped by the enterprise and the existing background
traffic from other companies.
The details for the three routes are provided in Table 6.
For congested segments, the existing background traffic is
also listed.
How should the enterprise allocate the shipment of 1,000
tons of goods across the three routes to minimize the total
transportation
Mathematical model
Set:
1. Route
The set of available transportation routes,
\( R = \{1, 2, 3\} \)
Parameter:
1. TotalTonnage
# The total amount of goods to be shipped
100
2. LinearCostPerTon
# The base transportation cost per ton for
each route, excluding congestion fees.
This is calculated by summing the costs
of all segments for a given route (‘
distance * base_fee‘).
[700, 1085, 670] # in dollars per ton for
Route 1, 2, and 3 respectively.
3. CongestionCoeff
# The coefficient for calculating the
congestion fee on applicable routes.
[5e-7, 8e-7, 0] # Route 3 has no congestion
fee.
4. BackgroundTraffic
# The existing traffic volume on congested
routes.
[2000, 1500, 0] # in tons. Route 3 has no
congested segment.
Decision variable:
1. TonnageOnRoute
Continuous variable, \( TonnageOnRoute[r] \
forall r \in R \), representing the
amount of goods in tons transported via
route \( r \).
Objective:
1. Minimize the total transportation cost.
The objective is to minimize the sum of
the linear transportation costs and the
non-linear congestion fees for all
routes.
min: \( \sum_{r \in R} (LinearCostPerTon[r]
\times TonnageOnRoute[r] +
CongestionCoeff[r] \times (
BackgroundTraffic[r] + TonnageOnRoute[r
])ˆ2) \)
Constraint:
1. Total Shipment Constraint. The sum of
goods transported on all routes must
equal the total tonnage required to be
shipped.
\( \sum_{r \in R} TonnageOnRoute[r] =
TotalTonnage \)
2. Non-negativity constraint. The amount of
goods transported on any route cannot be
negative.
\( TonnageOnRoute[r] \geq 0 \forall r \in R
\)
Type:
Continuous, non-linear, linear
NP
Gurobipy code
import gurobipy as gp
from gurobipy import GRB
def solve_logistics():
"""
Solves the transportation logistics
problem with congestion pricing.
"""
# Create a new model
model = gp.Model("LogisticsOptimization
")
# --- Sets ---
routes = ["Route1", "Route2", "Route3"]
# --- Parameters ---
total_tonnage_to_ship = 1000.0
# Total
tons of goods to transport
# Linear part of the cost for each route


--- Page 10 ---
($ per ton)
linear_cost_per_ton = {
"Route1": 700,
# (150km * $2.0) +
(500km * $0.8)
"Route2": 1085,
# (200km * $2.1) +
(350km * $1.9)
"Route3": 670
# (100km * $2.2) +
(600km * $0.75)
}
# Congestion parameters
congestion_coeff = {
"Route1": 5e-7,
"Route2": 8e-7,
"Route3": 0
# No congestion on
Route 3
}
background_traffic = {
"Route1": 2000,
"Route2": 1500,
"Route3": 0
}
# --- Decision Variables ---
# Amount of goods to ship on each route
(in tons)
tonnage_on_route = model.addVars(routes,
name="TonnageOnRoute", lb=0)
# --- Objective Function ---
# Minimize Total Transportation Cost,
Linear cost component
total_linear_cost = gp.quicksum(
linear_cost_per_ton[r] *
tonnage_on_route[r] for r in
routes
)
# Congestion cost component (this makes
the objective quadratic)
total_congestion_cost = gp.quicksum(
congestion_coeff[r] *
(background_traffic[r] +
tonnage_on_route[r]) *
(background_traffic[r] +
tonnage_on_route[r])
for r in routes if congestion_coeff[
r] > 0
)
# Set the complete objective function
model.setObjective(total_linear_cost +
total_congestion_cost, GRB.MINIMIZE)
# --- Constraints ---
# 1. Total Tonnage Constraint: Must ship
the exact total amount of goods
model.addConstr(
tonnage_on_route.sum(’*’) ==
total_tonnage_to_ship,
name="TotalTonnageRequirement"
)
# Non-negativity is handled by lb=0 in
variable definition.
# Optimize the model
model.optimize()
# --- Print Results ---
if model.status == GRB.OPTIMAL:
print(f"Optimal Total Transportation
Cost: ${model.objVal:,.2f}\n")
elif model.status == GRB.INFEASIBLE:
print("Model is infeasible. Check
constraints.")
elif model.status == GRB.UNBOUNDED:
print("Model is unbounded. The
objective can be improved
infinitely.")
else:
print(f"Optimization ended with
status {model.status}")
# Run the solver function
if __name__ == ’__main__’:
solve_logistics()
Problem characteristics and optimal solution
"prob_081": {
"ground_truth": 670003.8,
"problem_type": "NP",
"problem_size": "Toy",
"details": {
"variables_num": 3,
"constraints_num": 1,
"nonzeros_num": 3
}
}
Design of ORThought
The following are the prompts for two agents of ORThought:
Model agent
You are an expert in optimization modeling
and programming. Please carefully
analyze the following optimization
problem:
‘‘‘text
{nlp}
‘‘‘
Your task is to provide a comprehensive
solution that includes your detailed
solution path, a formal mathematical
model, and executable Gurobipy Python
code. Please structure your response as
follows:
Enclose your entire solution path within **<
solution_path>** and **</solution_path
>** tags. This section should detail
your approach to understanding and
modeling the problem:
1. Understanding the Problem


--- Page 11 ---
- **Core Optimization Objective:** What is
your understanding of the primary goal
of this optimization problem (e.g., what
is being maximized or minimized)?
- **Key Decision Variables:**
- Identify all the distinct choices or
quantities that need to be decided.
- For each decision variable, explain why
it’s a variable, its meaning in the
context of the problem, and its type (
e.g., continuous, integer, binary).
- **Main Constraints:** List and briefly
describe the critical limitations,
restrictions, or conditions imposed by
the problem statement.
2. Building the Mathematical Model (Step by
Step)
- **Decision Variables Definition:**
Formally define each decision variable
using appropriate symbols. Clearly state
its meaning and mathematical type (e.g
., $x_{{ij}} \ge 0$ and continuous, or
$y_k \in {{0,1}}$).
- **Objective Function Construction:**
- Clearly state whether the objective is
to maximize or minimize.
- Provide the mathematical expression for
the objective function.
- Explain the derivation of each term in
the objective function, linking it
directly to the problem description
and the defined decision variables.
Clarify how each part contributes to
the overall goal.
- **Constraint Construction:**
- For each constraint identified from the
problem description:
- Translate it into a mathematical
equation or inequality involving the
decision variables.
- Explain the logic behind its
formulation, ensuring it accurately
reflects the corresponding
limitation in the problem statement.
Address aspects like fund
availability, investment limits, and
cash flow between years.
- **Summary of the Mathematical Model:**
Compile the complete mathematical model.
This section should clearly present all
components of your optimization model.
Enclose the entire model within **‘‘‘
model** and **‘‘‘** tags.
3. Gurobipy Python Code
Translate your mathematical model into a
complete and executable Gurobipy Python
function(Everything should be defined
inside of the function).
- The function has arguments **with default
values extracted directly from the
provided problem description**
- The function should return only the
optimal objective function value if a
feasible solution is found, or ‘None‘ if
the problem is infeasible or unbounded.
- Enclose the Python code within **‘‘‘python
** and **‘‘‘** tags.
Solve agent
The repair part’s prompt shows as follows:
You are an expert Gurobipy developer and
debugger. Your task is to analyze the
provided mathematical model, Gurobipy
code, and error message to identify and
fix the bug in the Gurobipy code. The
corrected code must accurately implement
the given mathematical model.
The problem description:
‘‘‘text
{nlp}
‘‘‘
The mathematical model:
‘‘‘model
{model_text}
‘‘‘
The Gurobipy code:
‘‘‘code
{code_text}
‘‘‘
The error message during code execution:
‘‘‘text
{error_message}
‘‘‘
Your Task:
1. Identify the Bug.
2. Provide Corrected Code: Offer a complete,
corrected version of the Gurobipy code,
and provide a brief explanation of the
changes made.
3. Ensure Model Adherence: The corrected
code must accurately reflect the
provided mathematical model.
Output Format:
1. A brief explanation of fixes.
2. Corrected Gurobipy Code
- Enclose the corrected code within **‘‘‘
code** and **‘‘‘** tags.
- The code should be a callable function
whose parameters have default values
and whose return value is the optimal
objective function value of the model
(if it exists), otherwise return None.


--- Page 12 ---
Setting of Ablation Study
In this section, we introduce the settings of four ORThought
variants employed in the ablation experiments, where vari-
ant specifications are implemented through prompt modifi-
cations.
w/o repair
Remove the component where the solve agent retrieves error
information from the sandbox and conducts repairs.
w/o understanding
Keep the other parts unchanged, remove the “1. Understand-
ing the Problem” part in ORThought.
w/o expert knowledge in understanding
Keep the other parts unchanged, replace the “1. Understand-
ing the Problem” part in ORThought with the following con-
tent:
1. Understanding the Problem
From an optimization perspective, what is
your understanding of this optimization
problem?
w/o expert knowledge in formulation
Keep the other parts unchanged, replace the “2. Building
the Mathematical Model (Step by Step)” part in ORThought
with the following content:
2. Building the Mathematical Model (Step by
Step)
Please define the mathematical model. This
section should clearly present all
components of your optimization model.
Enclose the entire model within **‘‘‘
model** and **‘‘‘** tags.
Examples for ORThought solution
A correctly solved instance
Problem Description
A company has three industries located in different lo-
cations and needs to deliver goods to five shops in dif-
ferent locations. The delivery costs from the first in-
dustry to the shops are 4, 9, 2, 6, and 5, respectively,
with a storage capacity of 60. The delivery costs from
the second industry to the shops are 2, 6, 1, 7, and 9,
respectively, with a storage capacity of 30. The deliv-
ery costs from the third industry to the shops are 2, 4,
9, 8, and 3, respectively, with a storage capacity of 45.
The demands of the five shops are 15, 35, 20, 5, and
40. How can the goods be transported to minimize the
cost?
Solution Process
1. Understanding the Problem
A. Core Optimization Objective:
The goal is to minimize the total transportation
cost of delivering goods from three industries to five
shops, respecting capacity and demand constraints.
B. Key Decision Variables:
• Transportation quantities: For each industry i ∈
{1, 2, 3} and each shop j ∈{1, 2, 3, 4, 5}, define:
xij ≥0
representing the amount of goods transported from
industry i to shop j.
• Variables are continuous because quantities can
be fractional in typical transportation models.
C. Main Constraints:
• Supply capacity constraints: The total goods
shipped from each industry cannot exceed its stor-
age capacity:
5
X
j=1
xij ≤Capacityi
• Demand fulfillment constraints: Each shop’s de-
mand must be met exactly:
3
X
i=1
xij = Demandj
• Non-negativity constraints:
xij ≥0
2. Building the Mathematical Model – Step by
Step
Decision Variables:
xij ≥0,
i = 1, 2, 3;
j = 1, 2, 3, 4, 5
Objective Function:
Minimize total transportation cost:
Minimize
Z =
3
X
i=1
5
X
j=1
cijxij
where cij is the cost from industry i to shop j.
Given costs:
• Industry 1: c1 = [4, 9, 2, 6, 5]
• Industry 2: c2 = [2, 6, 1, 7, 9]
• Industry 3: c3 = [2, 4, 9, 8, 3]
Total cost (expanded):
Z = 4x11 + 9x12 + 2x13 + 6x14 + 5x15
+ 2x21 + 6x22 + 1x23 + 7x24 + 9x25
+ 2x31 + 4x32 + 9x33 + 8x34 + 3x35
Constraints:


--- Page 13 ---
• Supply constraints:



x11 + x12 + x13 + x14 + x15 ≤60
x21 + x22 + x23 + x24 + x25 ≤30
x31 + x32 + x33 + x34 + x35 ≤45
• Demand constraints:













x11 + x21 + x31 = 15
x12 + x22 + x32 = 35
x13 + x23 + x33 = 20
x14 + x24 + x34 = 5
x15 + x25 + x35 = 40
• Non-negativity:
xij ≥0
Mathematical Model
min
Z = 4x11 + 9x12 + 2x13 + 6x14 + 5x15
+ 2x21 + 6x22 + 1x23 + 7x24 + 9x25
+ 2x31 + 4x32 + 9x33 + 8x34 + 3x35
s.t.
x11 + x12 + x13 + x14 + x15 ≤60
x21 + x22 + x23 + x24 + x25 ≤30
x31 + x32 + x33 + x34 + x35 ≤45
x11 + x21 + x31 = 15
x12 + x22 + x32 = 35
x13 + x23 + x33 = 20
x14 + x24 + x34 = 5
x15 + x25 + x35 = 40
xij ≥0
∀i, j
Gurobipy Code
def optimize_transportation(costs=None,
capacities=None, demands=None):
from gurobipy import Model, GRB
# Default data based on the problem
statement
if costs is None:
costs = [
[4,9,2,6,5],
# Industry 1
[2,6,1,7,9],
# Industry 2
[2,4,9,8,3]
# Industry 3
]
if capacities is None:
capacities = [60, 30, 45]
if demands is None:
demands = [15, 35, 20, 5, 40]
num_industries = len(costs)
num_shops = len(demands)
# Create model
m = Model("Transportation")
# Decision variables: x[i,j]
# Using m.addVars for more efficient
variable creation
x = m.addVars(num_industries,
num_shops, lb=0, name="x")
# Capacity constraints
m.addConstrs(
(x.sum(i, ’*’) <= capacities[i]
for i in range(
num_industries)),
name="Capacity"
)
# Demand constraints
m.addConstrs(
(x.sum(’*’, j) == demands[j] for
j in range(num_shops)),
name="Demand"
)
# Objective function
# Using x.prod() for more efficient
objective creation
m.setObjective(x.prod(costs), GRB.
MINIMIZE)
# Optimize the model
m.optimize()
# Check and return the result
if m.status == GRB.OPTIMAL:
print(f"Optimal total cost found
: {m.objVal}")
return m.objVal
else:
print("No optimal solution found
.")
return None
Part of incorrectly solved instances
Here, we present examples of various types of errors ob-
served during the solution process of ORThought.
Problem description:
LogiCorp operates a logistics network that ships
two types of products: Basic and Advanced. The
raw materials needed to produce these products can
be purchased for $5 per unit. Processing 1 unit of
raw material requires 2 hours of warehouse labor.
Each unit of processed raw material yields 2 units
of Basic Product and 3 units of Advanced Product.
Basic Product can be sold for $10/unit, and Advanced
Product can be sold for $12/unit. LogiCorp also has
the option of further processing Basic and Advanced
Products to produce Premium Basic and Premium
Advanced, which sell for $20/unit and $25/unit,
respectively. Each unit of Basic Product processed
further requires an additional 3 hours of warehouse


--- Page 14 ---
labor and $5 processing cost, yielding 1 unit of
Premium Basic. Each unit of Advanced Product
processed further requires an additional 4 hours of
warehouse labor and $6 processing cost, yielding 1
unit of Premium Advanced. Each year, LogiCorp has
8,000 hours of warehouse labor available and can
purchase up to 3,000 units of raw material. How can
LogiCorp maximize its profits? Assume that the cost
of warehouse labor is a fixed cost, raw materials can
only be purchased in whole units.
Output errors:
1. Understanding the Problem
. . .
Key Decision Variables:
• Raw Material Purchase:
R: Number of raw material units purchased (inte-
ger, 0 ≤R ≤3000)
• Processing Raw Material into Basic and Advanced
Products:
B: Units of processed raw material producing Basic
Product (continuous, B ≥0)
A: Units of processed raw material producing Ad-
vanced Product (continuous, A ≥0)
. . .
Error analysis:
Incorrect variable
According to the problem
description, ”each unit of processed raw material
yields 2 units of Basic Product and 3 units of Ad-
vanced Product.” This is a joint production process,
meaning that processing one unit of raw material
simultaneously produces both intermediate products.
However, in the incorrect solution, variables A and B
model this as a choice process, which does not align
with the problem description.
Output errors:
1. Understanding the Problem
. . .
Main Constraints:
• Raw Material Processing:
The total raw material processed into Basic and Ad-
vanced products:
B ≤2R,
A ≤2R
. . .
Error analysis:
Incorrect constraint
The
correct
relationship
should be ”equal to”, not ”less than or equal to”.
Output errors:
2. Building the Mathematical Model – Step by Step
. . .
Objective Function:
Maximize profit, which includes revenues minus
costs:
• Revenue:
– Basic Product: 10 × 2B
– Advanced Product: 12 × 3A
– Premium Basic: 20 × PB
– Premium Advanced: 25 × PA
• Basic Product: 10 × 2B
– Advanced Product: 12 × 3A
– Premium Basic: 20 × PB
– Premium Advanced: 25 × PA
• Costs:
– Raw materials: 5 × R
– Additional processing costs: 5×PB for Premium
Basic, 6 × PA for Premium Advanced
Profit:
Maximize Z = (10 × 2B) + (12 × 3A) + (20 × PB)
+ (25 × PA) −5R −5PB −6PA
...
Error analysis:
Incorrect objective
Variables
A
and
B
are
incorrect, and therefore the objective function that
calculates revenue based on them also be wrong. It
fails to correctly capture the core profit logic: ”the
products produced from the total raw material R are
partially sold directly and partially sold after further
processing”.
Problem description:
A national e-commerce company operates three dis-
tribution centers in the Metro region. Each center cur-
rently stores different quantities of standard and pre-
mium products as shown below:
Center
Standard Products
Premium Products
Center 1
150
30
Center 2
250
100
Center 3
300
70
The company plans to open three retail fulfillment
hubs that will serve customers with same-day deliv-
ery. Corporate policy requires that each fulfillment hub
must maintain exactly 300 products in total inventory.
The shipping distances (in kilometers) between distri-
bution centers and fulfillment hubs are shown as fol-
lows:
From / To
Hub 1
Hub 2
Hub 3
Center 1
0
12
18
Center 2
12
0
15
Center 3
18
15
0


--- Page 15 ---
What is the minimum total transportation distance
required to move the products under the company’s
inventory requirements?
Output Errors:
...
Main Constraints:
• Each hub must have the same number of premium
products:
X
i
xP
ij = P,
∀j
where P is the number of premium products per
hub (unknown, but equal across hubs).
...
Error analysis:
Spurious constraint Added a spurious constraint
that does not exist in the problem.
Problem description:
A logistics company needs to pack 12 rectangular
parcels into a shipping container with fixed width of
100 cm and variable height. The parcels have different
dimensions and must be placed orthogonally (no rota-
tions allowed). The goal is to arrange all parcels within
the container while minimizing the total height used.
The parcels have the following dimensions (width ×
height in cm): 1. 30×40, 2. 25×35, 3. 40×20, 4. 15×25,
5. 35×30, 6. 20×15, 7. 25×20, 8. 30×25, 9. 15×10,
10. 20×30, 11. 10×15, 12. 25×10. Parcels cannot
overlap and must be completely contained within the
container’s width. The container has unlimited height
capacity, but the objective is to minimize the used
height.
Output Errors:
1. Understanding the Problem
. . .
Key Decision Variables:
• Overlap: Parcels cannot overlap.
. . .
Main Constraints:
• Non-overlapping: For any two parcels i ̸= j, either
parcel i is to the left of parcel j, or to the right, or
above or below, which can be modeled with binary
variables.
. . .
2. Building the Mathematical Model – Step by Step
Decision Variables:
• zij ∈{0, 1}: binary variables indicating relative
positioning between parcels i and j.
. . .
Constraints:
• Non-overlap constraints:
For each pair (i, j), i < j:
xi + wi ≤xj + M(1 −zij)
xj + wj ≤xi + Mzij
where M is a large constant (e.g., 1000) to deacti-
vate constraints depending on zij.
Additionally, enforce that for each pair, either i is
to the left of j or vice versa:
zij + zji = 1
with zji = 1 −zij.
...
Error analysis:
Missing variables & Missing constraints During
the Understanding the problem phase, the solve agent
identified that non-overlapping constraints should be
added for parcels in two directions. However, during
the Building the mathematical model stage, this was
not strictly implemented — only a single-direction re-
lationship variable z was introduced, and only non-
overlapping constraints in one direction were added.
The variables and constraints that represent parcel re-
lationships in the other direction were missing.
