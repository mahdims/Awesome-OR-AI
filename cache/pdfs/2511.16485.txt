--- Page 1 ---
Online Operator Design in Evolutionary Optimization for Flexible Job Shop
Scheduling via Large Language Models
Rongjie Liao1 , Junhao Qiu2∗and Zhenguo Yang1 and Xin Chen2 and Xiaoping Li1
1School of Computer Science, Guangdong University of Technology
2Department of Computer Science, City University of Hong Kong
liaorongjie1@mails.gdut.edu.cn, junhaoqiu2-c@my.cityu.edu.hk, yzg@gdut.edu.cn,
xchen3252-c@my.cityu.edu.hk, xpli@gdut.edu.cn
Abstract
Customized static operator design has enabled
widespread application of Evolutionary Algorithms
(EAs), but their search effectiveness often deteri-
orates as evolutionary progresses.
Dynamic op-
erator configuration approaches attempt to allevi-
ate this issue, but they typically rely on prede-
fined operator structures and localized parameter
control, lacking sustained adaptive optimization
throughout evolution. To overcome these limita-
tions, this work leverages Large Language Mod-
els (LLMs) to perceive evolutionary dynamics and
enable operator-level meta-evolution.
The pro-
posed framework, LLMs for online operator design
in Evolutionary Optimization, named LLM4EO,
comprises three components: knowledge-transfer-
based operator design, evolution perception and
analysis, and adaptive operator evolution. Firstly,
operators are initialized by leveraging LLMs to dis-
till and transfer knowledge from well-established
operators.
Then, search behaviors and potential
limitations of operators are analyzed by integrating
fitness performance with evolutionary features, ac-
companied by suggestions for improvement. Upon
stagnation of population evolution, an LLM-driven
meta-operator dynamically optimizes gene selec-
tion of operators by prompt-guided improvement
strategies.
This approach achieves co-evolution
of solutions and operators within a unified opti-
mization framework, introducing a novel paradigm
for enhancing the efficiency and adaptability of
EAs. Finally, extensive experiments on multiple
benchmarks of flexible job shop scheduling prob-
lem demonstrate that LLM4EO accelerates popula-
tion evolution and outperforms tailored EAs.
1
Introduction
The differentiated design of operators has enabled Evolution-
ary Algorithms (EAs) to be widely applied in fields, such
as production scheduling, logistics optimization, and medi-
cal resource allocation [Gendreau et al., 2010]. Traditional
operator design, relies on expert knowledge to balance explo-
ration and exploitation by controlling the search direction and
Problem
EAs
Solution
Trial & Error
Operator
(a) Manual Design
Problem
EAs
Solution
Operator
Data
Training
(b) Offline Auto-design
Problem
EAs
Solution
Operator
Solution
LLM
Perception
Refinement
Eovl. state
Operator
thought
code
(c) Online Automatic Operator Design
Figure 1: Traditional operator design (a) relies on expertise knowl-
edge and trial-and-error; offline automatic design (b) method trains
operators on validation data before solving target problems; while
our approach (c) online evolves operators by LLMs in the search.
intensity of neighborhood solutions[Venugopal et al., 2009].
However, this search paradigm typically employs fixed oper-
ator structures and static parameter configurations, making it
difficult to adapt to evolutionary dynamics, such as changes
in population structure, neighborhood space and feature dis-
tribution. As a result, the adaptability and generalization of
search operators are limited in complex and diverse scenarios
[Tian et al., 2025]. This raises an intriguing question: How
to utilize evolutionary information to achieve operator evo-
lution during the search process?
Dynamically adjusting the search paradigm is crucial for
improving the adaptability of EAs and addressing the limi-
tations of statically designed search operators. Existing dy-
namic operator configuration approaches mainly include pa-
rameter control methods and evolutionary procedure induc-
tion algorithms. Although parameter control methods guide
the search by adjusting initial and process parameters [Eiben
et al., 1999], their effectiveness is fundamentally constrained
by the rigid structure of the operator.
In addition, evolu-
tionary program induction algorithms [Graff and Poli, 2011],
arXiv:2511.16485v3  [cs.NE]  22 Jan 2026


--- Page 2 ---
such as Genetic Programming and Genetic Evolutionary Pro-
gramming, search for operators expressed by symbols within
an evolutionary framework. However, these algorithms often
suffer from inefficient search due to excessive iterations, and
the resulting operator quality is highly dependent on valida-
tion sets. This limitation arises mainly from two key issues:
1) Evolutionary information is challenging to be perceived
and utilized to guide operator evolution: Static strategies
are incapable of simultaneously meeting the optimization de-
mands of different search stages. 2) Lack of operator-level
evolutionary adaptation mechanisms: Existing approaches
primarily rely on local parameter tuning or predefined oper-
ator structures, which prevents search operators from being
continuously refined based on evolutionary feedback during
the optimization process.
To bridge these research gaps, we propose a novel large
language model for online operator design in evolutionary
optimization framework, termed LLM4EO, which enables
operator-level meta-evolution in evolutionary algorithms. By
leveraging the semantic understanding capabilities of Large
Language Models (LLMs), LLM4EO perceives population-
level evolutionary dynamics and guides the adaptive refine-
ment of search operators. In contrast to methods that directly
construct solutions [Yang et al., 2024] or generate complete
solving algorithms [Liu et al., 2026], LLM4EO focuses on
dynamically analyzing and improving operators throughout
the search process. When population evolution stagnates, an
LLM-driven meta-operator is activated to adaptively modify
the gene selection strategies and behavioral patterns of un-
derlying operators, thereby accelerating convergence and en-
hancing the adaptability of EAs.
We further design an LLM-driven meta-operator and a col-
laborative evolution architecture that couples the evolution
of solutions with the adaptive refinement of operators. The
framework consists of three tightly integrated components.
1) Knowledge-transfer-based operator design, where the
LLM generates a high-quality initial operator population by
leveraging prior knowledge of problem structures and classi-
cal heuristics. 2) Evolution perception and analysis, where
fitness indicators of both solutions and operators are evalu-
ated to capture critical evolutionary information, including
population distribution, convergence trends, and performance
variations across search stages. Based on these signals, the
LLM perceives the current evolutionary state and diagnoses
potential deficiencies in operator behaviors. 3) Adaptive op-
erator evolution, where the LLM-driven meta-operator dy-
namically updates the gene selection strategies and behav-
ioral patterns of underlying search operators through prompt-
guided optimization. These components are integrated via
a structured prompt framework, forming a closed-loop pro-
cess of perception, analysis, and refinement that continu-
ously improves operator effectiveness during the evolution-
ary search. Furthermore, the effectiveness of LLM4EO is
validated on multiple benchmark datasets of the flexible job
shop scheduling problem. Experimental results demonstrate
that LLM4EO consistently outperforms mainstream methods
in both convergence speed and solution quality, particularly
when the search process encounters stagnation or local op-
tima. Overall, this work illustrates how LLMs can be sys-
tematically leveraged to enable operator-level adaptive refine-
ment, offering a new perspective on enhancing the perfor-
mance and adaptability of evolutionary algorithms across di-
verse optimization scenarios.
2
Related Work
2.1
Operator Control and Design.
Operators are central to balancing exploration and exploita-
tion in EAs, directly determining convergence and optimiza-
tion performance. Heuristic operators traditionally designed
to rely on expert knowledge and iterative trials achieve excel-
lent performance in specific problems, but have limited gener-
alization capabilities. Enhancing the adaptability of the oper-
ator to the dynamic features of the search process has become
the focus of operator control and design. Parameter control
methods as one of the solutions can select appropriate param-
eter combinations for different search stages [Jin et al., 2021],
such as crossover/mutation rates [Samsuria et al., 2025;
Li et al., 2025d], step sizes [Li et al., 2025a], strategy se-
lection [Li et al., 2025b], and individual selection [Feng
et al., 2025], but still constrained by human-defined rules.
Another class of solutions is the use of evolutionary pro-
gram induction algorithms to construct dynamically adapt-
able operators. Neighborhood strategy optimization is im-
plemented by automatically evolving rules in a finite ter-
minal symbol space, e.g., Genetic Programming (GP) and
Gene Expression Programming (GEP) [Duan et al., 2024;
Zhang et al., 2023].
Recent integrations of LLMs into EAs primarily follow
two paradigms: as direct search operators [Yang et al., 2024;
Liu et al., 2023; Ye et al., 2024] and as offline strategy de-
signers [Zhang et al., 2025; Kiet et al., 2025]. In the first
paradigm, leveraging LLMs for high-frequency solution gen-
eration often incurs prohibitive computational latency and ex-
ecution overhead [Liu et al., 2025]. Conversely, the second
paradigm relies on a protracted pre-training phase, rendering
algorithmic efficacy contingent upon validation data quality
[Pei et al., 2025]. Consequently, integrating online opera-
tor automated design that perceives and responds to dynamic
search features is critical for flexible neighborhood adjust-
ments. This paradigm eliminates offline training dependen-
cies and facilitates state-aware responses to search dynamics,
thereby balancing execution efficiency with adaptability.
2.2
Automatic Heuristic Design.
Automatic heuristic design (AHD) selects, tunes, or con-
structs effective heuristics, addressing application bottlenecks
of traditional manual design that relies on labor-intensive and
expert experience [Liu et al., 2024]. GP has been applied to
AHD and generates heuristics by crossover and mutation of
a set of allowed symbols, requiring precise fitness landscapes
and expensive search costs [O’Neill et al., 2010].
Due to the powerful comprehension and generation ca-
pabilities of LLMs, prior works have used them to design
heuristics in an evolutionary search framework [Li et al.,
2025c; Liu et al., 2024; Yao et al., 2025; Ye et al., 2024;
Dat et al., 2025; Romera-Paredes et al., 2023]. Specifically,
EOH introduces thoughts to achieve co-evolution with the


--- Page 3 ---
Operator 
initialization
Solution 
generation
Fitness 
calculation
Population 
selection
Iteration 
completed
Solution
Output 
Operators 
Update
Opt. State 
Evaluation
Operator 
Evolution
Y
N
 
Convergence curve
Meta-operator 
evolution
Acceleration 
 
 
Perception & Analysis
Operator Evolution
Operator Design
Knowledge
Prompt
Generation
problem
heuristics
Operators
Design
thought
code
max
C
t
Evo. features
Operator fitness
LLM
Analysis
Expert 
understanding
Operator
limitation
Design
suggestion
Evolution
state
Operator population
p1
p2
p
c
Evo. Optimization
Evo. Info.
Prompt Mutation
thought
code
Refine
update 
operators
operator n
...
operator 1
operator 2
Apply
Figure 2: The framework of LLM4EO.
codes and achieves outstanding performance on several clas-
sical problems [Liu et al., 2024; Liu et al., 2025]. Further-
more, [Ye et al., 2024] introduces fitness landscape analy-
sis and black-box prompting via the added LLM agent for
reliable heuristic evaluations. The integration of LLMs and
evolutionary computation introduces a novel algorithmic de-
sign paradigm. Their capabilities in summarizing and refin-
ing semantic knowledge provide new insights for the contin-
uous online design of operators within complex optimization
spaces.
3
Problem
The flexible job shop scheduling problem (FJSP) involves
scheduling I jobs, where each job i ∈{1, ..., I} consists of
Ji operations that must be processed in a specific sequence.
Each operation Oi,j(j = 1, .., Ji) can be performed on any
machine m in a set of eligible machines Mi,j, with process-
ing time Ti,j,m > 0. The start and finish time of operation
Oi,j are Si,j ≥0 and Fi,j = Si,j + Ti,j,m × xi,j,m, respec-
tively. The binary variable xi,j,m = 1 if operation Oi,j is
processed in machine m, and 0 otherwise. The optimization
objective is to minimize the makespan Cmax.
4
LLM4EO
The proposed framework, LLMs for online operator design
in evolutionary optimization (LLM4EO), leverages the LLM
to generate and refine operators in the search of EAs, thereby
enhancing the algorithmic performance, as shown in Figure
2. There are three core components: 1) knowledge-transfer-
based operator design, 2) evolution perception and analysis,
and 3) adaptive operator evolution. First, the LLM is used to
design gene selection strategies of operators based on prior
knowledge, then construct a high-quality initial operator pop-
ulation to generate new solutions. To prevent premature con-
vergence, a threshold is set to trigger the adaptive operator
evolution mechanism. The LLM is employed to perceive the
evolutionary state of solution population, as well as to ana-
lyze the search preferences and limitations of operator pop-
ulation. The resulting suggestions guide the LLM to create
new promising operators, thus accelerating the search pro-
cess. LLM4EO facilitates the co-evolution of solutions and
operators, providing an intelligent optimization approach.
4.1
Solution Representation and Initialization
In LLM4EO, Genetic Algorithm (GA) is adopted to solve
the FJSP, where solutions are encoded into two parts: op-
eration sequence vector (OSV) and machine assignment vec-
tor (MAV). Both vectors are arrays of integers, the length of
which is the total number of operations.
The initial solution population is generated by two ma-
chine assignment rules and three operation dispatching rules
[Pezzella et al., 2008]. The fitness of each solution is cal-
culated as fsoln = 1/Cmax, ensuring the shorter makespan
corresponds to the higher fitness. To speed up convergence,
the tournament selection method is used to select individuals
to generate solutions. After each iteration, the best individual
is retained and directly enters the next generation.
4.2
Meta-operator
To reduce dependence on manual design and enhance evo-
lutionary adaptability, we propose a meta-operator that em-
ploys an LLM-driven gene selection strategy to dynamically
explore high-fitness regions of the search space while gener-
ating better solutions through neighborhood moves.
Neighborhood Moves.
In this work, four typical neighbor-
hood moves are adopted as structural transformation opera-
tions on selected genes to generate new solutions.


--- Page 4 ---
1) OSV crossover:
The Precedence Preserving Order-
based crossover (POX) [Lee et al., 1998]. First, a job is se-
lected from the first parent, and all its operations are copied to
the first offspring. Then, the remaining operations are filled
in according to the operation sequence of the second parent.
2) MAV crossover: Select some operations and exchange
their machines between the two parents.
3) OSV mutation: The Precedence Preserving Shift muta-
tion (PPS) [Lee et al., 1998]. The sequence of operations is
changed by moving one of them to a different position, while
satisfying the process path constraints.
4) MAV mutation: Select an alternative machine to replace
the original machine assigned to an operation.
5) Critical operation swapping: The critical path is the
longest path in the scheduling scheme. Two critical opera-
tions are selected to swap. If a better solution is found, the
local search process continues along the new critical path.
Gene Selection.
Gene selection strategies define the search
scope and direction by identifying key decision variables to
adjust, thereby guiding the search toward more promising re-
gions. A gene is selected if its probability γ is greater than the
random number u in [0, 1]. Considering the characteristics of
FJSP, we design a hierarchical gene selection method based
on jobs and operations, which is defined as:
δ =
{Oi,j | γi > ui, ∀i, j ≤Ji}
{Oi,j | γi,j > ui,j, ∀i, j}
(1)
If operation Oi,j is selected, it is added to the gene set δ.
Moreover, if job i is selected, all of its operations are in-
cluded. In crossover, offsprings s′
x and s′
y are generated by
exchanging selected genes δx and δy of individuals sx and sy:
s′
x, s′
y = crossover(sx, sy, δx, δy)
(2)
In mutation, the genes δ of individual s are modified, result-
ing in offspring s′:
s′ = mutation(s, δ)
(3)
The gene selection controls the size of the search neigh-
borhood to be explored, which directly impacts solution qual-
ity and algorithm performance. However, random or single-
heuristic approaches often lead to poor exploration and early
convergence. To address this issue, we use the LLM to con-
struct gene selection heuristics, enabling the algorithm to
adaptively explore potential neighborhoods. Gene selection
is formulated as a probability function g(t, Pop, N), where t
denotes the iteration, Pop is the operator population, and N
represents genetic features. This function maps the hidden
correlations among genes to their selection probabilities. Ge-
netic features N capture the core properties of the FJSP and
guide the inference of the model. For job i, there are three
genetic features: process span Ci, minimal process span C′
i
and number of operations Ji. For operation Oi,j, there are
four genetic features: start time Si,j, earliest start time Fi,j−1,
processing time Ti,j and number of optional machines |Mi,j|.
Notably, process span Ci is the time interval between starting
the first operation and completing the last operation of job i:
Ci = Fi,Ji −Si,1, ∀i
(4)
The minimal process span C′
i represents the sum of the short-
est processing times for all operations of job i:
C′
i =
X
j≤Ji
min
m∈Mi,j Ti,j,m, ∀i
(5)
The processing time Ti,j refers to the duration of the opera-
tion Oi,j on the machine m:
Ti,j =
X
m∈Mi,j
xi,j,mTi,j,m, ∀i, j
(6)
Fitness Evaluation.
To guide the evolution toward high-
quality solutions, the fitness of operator population is evalu-
ated, and superior operators are preferentially selected using
the roulette wheel method. The optimization success rate of
an operator is defined as its fitness fop = ns/nv, where ns is
the number of better solutions it generates and nv represents
how many times it is selected. A higher fop reflects a stronger
performance of the operator.
4.3
Collaborative Evolution Architecture
Transcending the limitations of static search strategies in tra-
ditional EAs, we propose a collaborative evolutionary archi-
tecture for operators and solutions that leverages LLMs for
experience transfer, perception, analysis, and generation.
Knowledge-transfer-based Operator Design.
A struc-
tured initial prompt R is designed to activate prior knowledge
of LLMs, enabling the generation of the high-quality initial
operator population Pop, which is denoted as:
Pop = {on | on = DesignByLLM(R), n = 1, 2, ..., pop}
(7)
where pop is the operator population size and on
=
DesignByLLM(R) represents operator on generated by the
LLM under the prompt R that includes a task description, ex-
pected output, prior knowledge and code template.
• Task description: First, the target problem and require-
ments are described, such as “For the flexible job shop
scheduling problem aiming to minimize makespan, de-
sign an operator to calculate the adjustment priority for
jobs and operations.”
• Expected output: The output incorporates the thought I
and the function F of the operator o. To avoid long and
noisy responses, the LLM must summarize the thought
I in one sentence within {} and give a structured Python
function F using the given code template:
• Prior knowledge: To improve the quality of initial op-
erators, LLM can refer to classical heuristics hc, such
as the Shortest Processing Time (SPT) rule. Moreover,
brief descriptions of the task T and the genetic features
N guide the LLM to get a novel thought I:
I = LLM(T , N, hc | R)
(8)
• Code template: The name, input and output of code
block are defined, with detailed explanations of meaning
and type of each parameter. Given thought I and code


--- Page 5 ---
template D, function F comprises two components: job
priority calculation Fx and operation priority calculation
Fy. Given genetic features N, F outputs probabilities
that are normalized by the cumulative normal distribu-
tion function to ensure values in the interval [0, 1]:
F = (Fx, Fy) = LLM(I, N, D | R)
(9)
Evolution Perception and Analysis.
Evolutionary changes
in distribution of solution population degrade the effective-
ness of search strategies, necessitating the identification of
when, why, and how to improve operators. To address this,
we design the following steps:
1) Convergence evaluation: The convergence state is de-
fined as the number of consecutive iterations ∆t without im-
provement in the global best makespan, and then introduce
a threshold θ = 1/(ϵ × ∆t) to adaptively trigger operator
evolution, where ϵ is a coefficient controlling the frequency
of evolution. Based on extensive experimental experience,
ϵ = 0.05 is selected to balance the frequency of LLM invoca-
tion and the overall algorithmic efficiency. In each iteration,
if a random number u in [0,1] is greater than the threshold θ,
the operator evolution mechanism will be triggered:
o′ = RefineByLLM(Pop, Psoln, R′ | u > θ)
(10)
Here, LLMs refine and create a novel operator o′ according to
information about operation population Pop, changes in solu-
tion population Psoln, and improved prompt R′.
2) Perception and analysis: The pseudo-code for percep-
tion and analysis is shown in Algorithm 1. To guild the LLM
to perceive the evolutionary state of the current search stage,
we record changes in solution population Psoln, such as mini-
mum fitness, average fitness, and their respective change rates
since the last evolution of operators. In addition, information
about operator population Pop is provided, including the fit-
ness and thought of each operator. Then, the task description
T ′ is given as follows: “Characterize the evolutionary state
of solution population, describe the limitations of each oper-
ator, and provide a suggestion for designing a new operator”.
Algorithm 1 Perception and analysis
Input: solution population Psoln, operator population Pop,
task description T ′
Output: response result Tas
1: Create text Tsoln of changes in Psoln
2: Create text Top of information about Pop
3: Tas = AnalysisByLLM(Tsoln, Top, T ′)
4: return Tas
Adaptive Operator Evolution.
The pseudo-code for the
operator evolution is shown in Algorithm 2. A new prompt
R′ retains the structure of initial prompt R and incorporates
the response result Tas from the perception and analysis. The
task description of R′ explicitly emphasizes “develop a com-
pletely new algorithm distinct from the previous ones.” To
mitigate response inertia caused by a fixed prompt, the LLM
further fine-tunes the task description to increase output di-
versity. Using mutation-based prompting, a potential opera-
tor is generated and replaces the worst-performing one, form-
ing a new operator population. With LLM assistance, the
adaptive evolution of operators guides EAs to continuously
search towards potential regions in the solution space.
Algorithm 2 Operator evolution
Input: result Tas of perception and analysis, prompt R
Output: new operator o′
1: R ←Task description of R is fine-tuned by LLM
2: Improved prompt R′ = Tas ⊕R
3: Generate a new valid operator o′ based on R′
4: return o′
5
Experiments
5.1
Experimental Details
Experimental Design.
In this section, extensive computa-
tional experiments are conducted to evaluate our proposed
LLM4EO. First, the setting of experimental environment and
algorithm parameters is described. Then, an ablation study
is designed to analyze the impact of each improvement on
the algorithm. Subsequently, LLM4EO is compared with the
mainstream algorithms for automatic program generation to
demonstrate the potential of LLMs in designing operators. Fi-
nally, the superior performance of LLM4EO is verified by
comparing it with other algorithms.
Parameter Settings.
The parameters are set as follows: op-
erator population size = 3, solution population size = 100, and
maximum iterations = 500. The machine allocation and oper-
ation scheduling rules for solution initialization are consistent
with [Pezzella et al., 2008]. The probabilities of crossover
and mutation are both 0.9. All experiments are implemented
with Python 3.9 and executed on a computer with an Intel
core i5-12400 @ 2.50 GHz and 32 GB of RAM.
Table 1: Comparison results of different LLMs.
LLM
RPD
Cost
Ratio
GPT-4.1-mini
36.97 0.0564$ 0.0015
GPT-4o
36.57 0.1770$ 0.0048
DeepSeek-Chat
37.58 0.2847$ 0.0076
Qwen-Max
37.78 0.3721$ 0.0098
Claude-4-Sonnet 41.62 0.5914$ 0.0142
Gemini 2.5 Pro
36.16 2.7024$ 0.0747
LLM Selection.
To ensure operator performance, the qual-
ity and cost of the algorithm are evaluated in different LLMs,
as shown in Table 1. The tested models include GPT-4.1-
mini, GPT-4o, DeepSeek-Chat, Qwen-Max, Claude-4-Sonnet
and Gemini 2.5 Pro. LLM4EO is repeated three times on the
MK10 instance with the largest size of the benchmark from
[Brandimarte, 1993]. Model performance is compared based
on three metrics: average relative percent deviation, running
cost and quality-price ratio. The relative percentage devia-
tion (RPD) is used to quantify the deviation of each solution


--- Page 6 ---
100
200
Iteration
Operator 1: 
Calculates job priority by combining relative slack and operation counts...; 
Calculates operation priority by difference between actual and earliest start times...
Other operators: …
Operation initialization
           GA
           LLM4EO
           Operator evolution
Evolution perception: The population is 
demonstrating clear convergence pressure, with the 
overall fitness improving significantly..., which has 
a slower improvement rate. 
Operator analysis: 
Operator 1, its reliance on relative slack and 
operation counts may insufficiently capture dynamic 
scheduling complexities; 
Other operators: …
Design suggestion: Design a new algorithm that 
calculates job priority and operation priority based on 
processing order, start time, and processing time...
Operator 4: 
Calculate job priority... 
Calculate operation priority..., 
emphasizing sequential 
bottlenecks and machine 
substitution potential...
Mutation task: To minimize 
the makespan, please design a 
completely different operator 
from the above ones to 
calculate sequential adjustment 
priority for jobs and operations.
Perception & Analysis
Prompt mutation
Operator evolution
def calculate_priority(process_span,…):
   for i in range(job_number):  # Job priority
      slack = process_span[i] – minimal_process_span[i]
      density = process_span[i] / operation_number[i]
      priority = slack / max(process_span) * density
      job_priority.append(priority)
   for i in range(operation_number):  # Operation priority
      waiting.append(start_time - earliest_start_time)
   for i in range(operation_number):
      wait = waiting[i]/max(waiting)
      proc = processing_time[i]/max(processing_time)
      priority = wait * proc * / machine_number[i]
      operation_priority.append(priority)
  Return job_priority, operation_priority
220
240
260
280
300
0
Makespan
Operator 5: Combining job 
stretch ratio, normalized 
operation wait time before start, 
and machine flexibility penalty...
Operator 6: Based on the ratio of 
actual delay to minimal span and 
operation urgency by weighted 
remaining process time...
LLM
Figure 3: Evolution of LLM4EO for FJSP. Key information on operator initialization, perception and analysis, and adaptive operator evolution
is presented, including core thoughts and code snippets of operators produced in some generations during evolution.
from the known optimal solution:
RPD = Cmax −LB
LB
× 100%
(11)
Here, LB is the lower bound and the lower RPD reflects
better algorithm performance. GPT-4.1-mini offers the best
cost-effectiveness. Despite its small model size, it achieves
relatively satisfactory results, primarily due to the inherent
robustness of EAs, which maintain strong performance under
limited resources. In contrast, Gemini-2.5-Pro achieves the
minimum average RPD, but incurs significant costs. Taking
into account both algorithmic effectiveness and economic vi-
ability, GPT-4.1-mini is considered the most suitable option.
Visualization of Operator Evolution.
Figure 3 visualizes
the evolution of operators in a running time of the MK10 in-
stance, including the perception and analysis of the first op-
erator evolution, as well as the key thoughts and correspond-
ing codes of operators in different generations. It can be ob-
served that LLM4EO outperforms GA in convergence effi-
ciency, owing to the timely updates of gene selection strate-
gies driven by meta-operators. This enables the algorithm to
escape local optima and explore more promising areas. The
significant differences in thoughts among various operators
and their intricate heuristics reveal the strong comprehension
and creativity of LLMs.
5.2
Performance Comparisons and Analysis
Ablation Study.
To evaluate the effectiveness of each im-
provement, we compare algorithm variants with different
components. First, the LLM autonomously designs an ini-
tial operator for GA, resulting in a variant algorithm named
LLM4OD. Then, the single operator is expanded to an op-
erator population, forming LLM4OPD. Finally, the operator
population is further evolved by LLM in the search, yielding
LLM4EO. Each instance is solved 10 times to obtain the best
makespan (BM) and average makespan (AM).
Table 2 shows the RPDBM and RPDAM of instances
from [Brandimarte, 1993] and [Hurink et al., 1994].
The
algorithms are ranked from best to worst:
LLM4EO,
LLM4OPD, and LLM4OD. Notably, LLM4OPD outper-
forms LLM4OD, as LLM4OD can only select the unique op-
erator per iteration, limiting search scope and reducing pop-
ulation diversity.
To balance exploration and exploitation,
LLM4EO maintains a diverse operator pool and adaptively
generates effective operators tailored to the current search.
Table 2: Comparison results of variant algorithms.
Algorithm
Brandimarte
Hurink-vdata
RPDBM RPDAM RPDBM RPDAM
LLM4OD
19.11%
22.53%
3.32%
5.05%
LLM4OPD
18.93%
22.33%
3.31%
4.54%
LLM4EO
18.54%
21.37%
3.03%
4.35%
Figure 4 illustrates the convergence curves of MK02,
MK07, and MK10, depicting the variation of the best
makespan with iterations and the evolution of the average
makespan over time.
At the same number of iterations,
LLM4EO converges faster than other algorithms and finally
achieves better solutions.
In the time dimension, despite
the computational overhead of online operator evolution,
LLM4EO demonstrates significant advantages in both the op-
timization speed and the average objective of the solution
population. For larger-scale problems, the proportion of time
consumed by operator evolution is significantly reduced. The


--- Page 7 ---
0
2 0 0
4 0 0
3 0
3 5
4 0
4 5
 L L M
4 O D
 L L M
4 O P D
 L L M
4 E O
T i m e  ( s )
A v e r a g e  m a k e s p a n
0
5 0
1 0 0
1 5 0
2 0 0
2 8
3 0
3 2
3 4
B e s t  m a k e s p a n
I t e r a t i o n
(a) MK02
0
4 0 0
8 0 0
1 2 0 0
1 5 0
1 8 0
2 1 0
 L L M
4 O D
 L L M
4 O P D
 L L M
4 E O
T i m e  ( s )
A v e r a g e  m a k e s p a n
0
5 0
1 0 0
1 5 0
2 0 0
2 5 0
1 5 0
1 6 0
1 7 0
B e s t  m a k e s p a n
I t e r a t i o n
(b) MK07
0
1 5 0 0
3 0 0 0
4 5 0 0
2 4 0
2 6 0
2 8 0
3 0 0
3 2 0
3 4 0
T i m e  ( s )
 L L M
4 O D
 L L M
4 O P D
 L L M
4 E O
A v e r a g e  m a k e s p a n
0
1 0 0
2 0 0
3 0 0
4 0 0
2 2 0
2 3 0
2 4 0
2 5 0
2 6 0
2 7 0
2 8 0
2 9 0
B e s t  m a k e s p a n
I t e r a t i o n
(c) MK10
Figure 4: Mean convergence curves over the 10 runs for variant algorithms in MK02, MK07, and MK10 of Brandimarte benchmark. The
best makespan per iteration and the average makespan over time are shown in each instance.
results show that the collaborative evolution of solutions and
operators effectively improves the optimization performance.
Comparison of Automatic Operator Design Methods.
Genetic Programming (GP) and Gene Expression Program-
ming (GEP) are employed to create operators and compared
with LLM4EO to assess the potential of LLM in operator de-
sign. Since the LLM generates initial operators according to
the SPT heuristic, it is also adopted as the initial operator for
GP and GEP. To ensure fairness, each method uses the legiti-
mate result produced by a single iteration as the new operator.
Each instance from [Fattahi et al., 2007] is solved 10 times.
Since these algorithms find solutions equal to LB of SFJS
instances with relatively simple nature, only the box plots of
RPD for MFJS instances are shown in Figure 5.
M
F J S 0 1
M
F J S 0 2
M
F J S 0 3
M
F J S 0 4
M
F J S 0 5
M
F J S 0 6
M
F J S 0 7
M
F J S 0 8
M
F J S 0 9
M
F J S 1 0
1 0
2 0
3 0
4 0
5 0
6 0
7 0
R P D  ( %
)
 G P    
 G E P    
 L L M
4 E O
Figure 5: Partial box plots of GP, GEP and LLM4EO on Fattahi.
Figure 5 shows that LLM4EO outperforms both GP and
GEP in terms of BM and AM, demonstrating its superior
generation capability. It is closely related to the inherent char-
acteristics of each method. GP and GEP lack semantic under-
standing, resulting in lower-quality operators without suffi-
cient iterations. In contrast, LLMs have robust representation
and understanding, overcoming the limitations of traditional
methods in text comprehension and program generation, and
exhibiting promising prospects for operator design.
Comparison with Other Algorithms.
To further explore
the performance and generalization of LLM4EO, it is com-
pared with various optimization algorithms such as SLABC
[Long et al., 2022], SLGA [Chen et al., 2020], HGIN-RS [Ho
et al., 2024], GRU-DRL [Yu et al., 2025], HTS/SA [Fattahi
et al., 2007], AIA [Bagheri et al., 2010], IGAR [Amjad et al.,
2021], DDEA-PMI [Zhao et al., 2025] and the basic genetic
algorithm (GA) of LLM4EO. Table 3 shows the average BM
and RPDBM across 10 runs on the datasets. It can be seen
that LLM4EO outperforms the compared optimization algo-
rithms. In the same framework, LLM4EO outperforms GA
solely through meta-operators. Specifically on Brandimarte
dataset, RPDBM decreases from 19.13% to 18.54%, result-
ing in a 3.08% improvement, demonstrating its effectiveness.
Table 3: Comparison results of other algorithms and LLM4EO.
Algorithm
Brandimarte
Fattahi
BM
RPDBM
BM
RPDBM
SLABC (2022)
192.8
33.90%
-
-
SLGA (2020)
181.3
22.93%
-
-
HGIN-RS (2024)
182.5
27.57%
-
-
GRU-DRL (2025)
179.5
23.61%
-
-
HTS/SA (2007)
-
-
518.70
19.92%
AIA (2010)
-
-
490.20
14.54%
IGAR (2021)
-
-
490.45
14.49%
DDEA-PMI (2025)
-
-
554.15
25.24%
GA
176.2
19.13%
487.95
14.09%
LLM4EO
175.7
18.54%
487.75
14.06%
6
Conclusion
We propose LLM4EO that utilizes LLM-driven online oper-
ator design to enhance the optimization performance of Evo-
lutionary Algorithms (EAs). By leveraging prior knowledge
of problem structures and heuristics, it construct high-quality
gene selection strategies of operators. When population evo-
lution stagnates, the LLM4EO perceives evolutionary states,
analyzes operator limitations, and provides improvement sug-
gestions.
The resulting new operators are tailored to the
current search stage and guide the EAs to explore potential
neighborhoods. On different FJSP benchmarks, LLM4EO
outperforms other mainstream algorithms, demonstrating its
superior performance. LLM-driven meta-operators consis-
tently accelerate population evolution and enhance solution
quality, even under limited iterations.


--- Page 8 ---
References
[Amjad et al., 2021] Muhammad Amjad, Shahid Butt, and
Naveed Anjum.
Improved genetic algorithm integrated
with scheduling rules for flexible job shop scheduling
problems. E3S Web of Conferences, 243:02010, 01 2021.
[Bagheri et al., 2010] A. Bagheri, M. Zandieh, Iraj Mahdavi,
and M. Yazdani. An artificial immune algorithm for the
flexible job-shop scheduling problem. Future Generation
Computer Systems, 26(4):533–541, 2010.
[Brandimarte, 1993] Paolo Brandimarte.
Routing and
scheduling in a flexible job shop by tabu search. Annals
of Operations Research, 41(3):157–183, 1993.
[Chen et al., 2020] Ronghua Chen, Bo Yang, Shi Li, and
Shilong Wang. A self-learning genetic algorithm based
on reinforcement learning for flexible job-shop schedul-
ing problem.
Computers & Industrial Engineering,
149:106778, 2020.
[Dat et al., 2025] Pham Vu Tuan Dat, Long Doan, and
Huynh Thi Thanh Binh.
Hsevo:
elevating automatic
heuristic design with diversity-driven harmony search
and genetic algorithm using llms.
In Proceedings of
the Thirty-Ninth AAAI Conference on Artificial Intelli-
gence and Thirty-Seventh Conference on Innovative Ap-
plications of Artificial Intelligence and Fifteenth Sympo-
sium on Educational Advances in Artificial Intelligence,
AAAI’25/IAAI’25/EAAI’25. AAAI Press, 2025.
[De Giovanni and Pezzella, 2010] L.
De
Giovanni
and
F. Pezzella.
An improved genetic algorithm for the
distributed and flexible job-shop scheduling problem. Eu-
ropean Journal of Operational Research, 200(2):395–408,
2010.
[Duan et al., 2024] Jianguo Duan,
Fanfan Liu,
Qinglei
Zhang, and Jiyun Qin. Tri-objective lot-streaming schedul-
ing optimization for hybrid flow shops with uncertainties
in machine breakdowns and job arrivals using an enhanced
genetic programming hyper-heuristic. Computers & Oper-
ations Research, 172:106817, 2024.
[Eiben et al., 1999] A.E.
Eiben,
R.
Hinterding,
and
Z. Michalewicz.
Parameter control in evolutionary
algorithms. IEEE Transactions on Evolutionary Compu-
tation, 3(2):124–141, 1999.
[Fattahi et al., 2007] Parviz
Fattahi,
Mohammad
Saidi
Mehrabad, and Fariborz Jolai.
Mathematical modeling
and heuristic approaches to flexible job shop scheduling
problems.
JOURNAL OF INTELLIGENT MANUFAC-
TURING, 18(3):331–342, JUN 2007.
[Feng et al., 2025] Zeyu Feng, Zhiyuan Zou, and Xu Liang.
Solving machine overload for re-scheduling of dynamic
flexible job shop by adaptive tripartite game theory-based
genetic algorithm. Swarm and Evolutionary Computation,
96:101938, 2025.
[Gendreau et al., 2010] Michel Gendreau, Jean-Yves Potvin,
et al. Handbook of metaheuristics, volume 2. Springer,
2010.
[Graff and Poli, 2011] Mario Graff and Riccardo Poli. Per-
formance models for evolutionary program induction al-
gorithms based on problem difficulty indicators. In Pro-
ceedings of the 14th European Conference on Genetic Pro-
gramming, EuroGP’11, page 118–129, Berlin, Heidelberg,
2011. Springer-Verlag.
[Ho et al., 2024] Kuo-Hao Ho, Jui-Yu Cheng, Ji-Han Wu,
Fan Chiang, Yen-Chi Chen, Yuan-Yu Wu, and I-Chen Wu.
Residual scheduling: A new reinforcement learning ap-
proach to solving job shop scheduling problem. IEEE Ac-
cess, 12:14703–14718, 2024.
[Hurink et al., 1994] Johann Hurink, Bernd Jurisch, and
Monika Thole. Tabu search for the job-shop scheduling
problem with multi-purpose machines. OR Spectrum =
OR Spektrum, 15(4):205–215, February 1994.
[Jin et al., 2021] Yaochu Jin, Handing Wang, and Chaoli
Sun. Evolutionary and Swarm Optimization, pages 53–
101. Springer International Publishing, Cham, 2021.
[Kiet et al., 2025] Nguyen Viet Tuan Kiet, Dao Van Tung,
Tran Cong Dao, and Huynh Thi Thanh Binh. Motif: Multi-
strategy optimization via turn-based interactive frame-
work. ArXiv, abs/2508.03929, 2025.
[Lee et al., 1998] K.-M. Lee, T. Yamakawa, and Keon-
Myung Lee.
A genetic algorithm for general machine
scheduling problems. In 1998 Second International Con-
ference. Knowledge-Based Intelligent Electronic Systems.
Proceedings KES’98 (Cat. No.98EX111), volume 2, pages
60–66 vol.2, 1998.
[Li et al., 2025a] Bingdong Li, Yan Zhang, Peng Yang,
Xin Yao, and Aimin Zhou.
A two-population algo-
rithm for large-scale multiobjective optimization based on
fitness-aware operator and adaptive environmental selec-
tion.
IEEE Transactions on Evolutionary Computation,
29:631–645, 2025.
[Li et al., 2025b] Qiu-Ying Li, Quan-Ke Pan, Ling Wang,
Liang Gao, and Wei-Min Li.
Dynamic cascaded flow-
shop scheduling using an evolutionary greedy algorithm.
IEEE Transactions on Evolutionary Computation, pages
1–1, 2025.
[Li et al., 2025c] Rui Li,
Ling Wang,
Hongyan Sang,
Lizhong Yao, and Lijun Pan.
Llm-assisted automatic
memetic algorithm for lot-streaming hybrid job shop
scheduling with variable sublots. IEEE Transactions on
Evolutionary Computation, pages 1–1, 2025.
[Li et al., 2025d] Zhixiao Li, Guohui Zhang, Nana Yu,
Shenghui Guo, and Wenqiang Zhang.
A knowledge-
guided evolutionary algorithm incorporating reinforce-
ment learning for energy efficient dynamic flexible job
shop scheduling problem with machine breakdowns.
Swarm and Evolutionary Computation, 97:102050, 2025.
[Liu et al., 2023] Shengcai Liu, Caishun Chen, Xinghua Qu,
Ke Tang, and Yew Soon Ong. Large language models as
evolutionary optimizers. 2024 IEEE Congress on Evolu-
tionary Computation (CEC), pages 1–8, 2023.


--- Page 9 ---
[Liu et al., 2024] Fei Liu, Xialiang Tong, Mingxuan Yuan,
Xi Lin, Fu Luo, Zhenkun Wang, Zhichao Lu, and Qingfu
Zhang.
Evolution of heuristics: towards efficient auto-
matic algorithm design using large language model.
In
Proceedings of the 41st International Conference on Ma-
chine Learning, ICML’24. JMLR.org, 2024.
[Liu et al., 2025] Fei Liu, Xi Lin, Shunyu Yao, Zhenkun
Wang, Xialiang Tong, Mingxuan Yuan, and Qingfu Zhang.
Large language model for multiobjective evolutionary op-
timization. In International Conference on Evolutionary
Multi-Criterion Optimization, pages 178–191. Springer,
2025.
[Liu et al., 2026] Fei Liu, Yiming Yao, Ping Guo, Zhiyuan
Yang, Xi Lin, Zhe Zhao, Xialiang Tong, Kun Mao,
Zhichao Lu, Zhenkun Wang, Mingxuan Yuan, and Qingfu
Zhang. A systematic survey on large language models for
algorithm design. ACM Comput. Surv., January 2026. Just
Accepted.
[Long et al., 2022] Xiaojun Long, Jingtao Zhang, Xing Qi,
Wenlong Xu, Tianguo Jin, and Kai Zhou. A self-learning
artificial bee colony algorithm based on reinforcement
learning for a flexible job-shop scheduling problem. Con-
currency and Computation:
Practice and Experience,
34(4):e6658, 2022.
[Lu et al., 2018] Po-Hsiang Lu, Muh-Cherng Wu, Hao Tan,
Yong-Han Peng, and Chen-Fu Chen.
A genetic algo-
rithm embedded with a concise chromosome representa-
tion for distributed and flexible job-shop scheduling prob-
lems.
Journal of Intelligent Manufacturing, 29:19–34,
2018.
[O’Neill et al., 2010] Michael O’Neill, Leonardo Vanneschi,
Steven Gustafson, and Wolfgang Banzhaf. Open issues in
genetic programming. Genetic Programming and Evolv-
able Machines, 11(3):339–363, 2010.
[Pei et al., 2025] Jiyuan Pei, Yi Mei, Jialin Liu, Mengjie
Zhang, and Xin Yao. Adaptive operator selection for meta-
heuristics: A survey. IEEE Transactions on Artificial In-
telligence, 6(8):1991–2012, 2025.
[Pezzella et al., 2008] F.
Pezzella,
G.
Morganti,
and
G. Ciaschetti.
A genetic algorithm for the flexible
job-shop scheduling problem. Computers & Operations
Research, 35(10):3202–3212, 2008.
Part Special Issue:
Search-based Software Engineering.
[Romera-Paredes et al., 2023] Bernardino Romera-Paredes,
Mohammadamin Barekatain, Alexander Novikov, Matej
Balog, M. Kumar, Emilien Dupont, Francisco Ruiz, Jor-
dan Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet
Kohli, and Alhussein Fawzi.
Mathematical discoveries
from program search with large language models. Nature,
625, 12 2023.
[Samsuria et al., 2025] Erlianasha
Samsuria,
Mohd
Sai-
ful Azimi Mahmud, Norhaliza Abdul Wahab, Muham-
mad Zakiyullah Romdlony, Mohamad Shukri Zainal
Abidin, and Salinda Buyamin.
An improved adaptive
fuzzy-genetic algorithm based on local search for inte-
grated production and mobile robot scheduling in job-shop
flexible manufacturing system. Computers & Industrial
Engineering, 204:111093, 2025.
[Tian et al., 2025] Ye Tian, Xuhong Qi, Shangshang Yang,
Cheng He, Kay Chen Tan, Yaochu Jin, and Xingyi
Zhang. A universal framework for automatically gener-
ating single-and multi-objective evolutionary algorithms.
IEEE Transactions on Evolutionary Computation, pages
1–1, 2025.
[Venugopal et al., 2009] K. R. Venugopal, K. G. Srinivasa,
and L. M. Patnaik.
Self Adaptive Genetic Algorithms,
pages 19–50. Springer Berlin Heidelberg, Berlin, Heidel-
berg, 2009.
[Yang et al., 2024] Chengrun Yang, Xuezhi Wang, Yifeng
Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun
Chen.
Large language models as optimizers.
In The
Twelfth International Conference on Learning Represen-
tations, 2024.
[Yao et al., 2025] Shunyu Yao, Fei Liu, Xi Lin, Zhichao Lu,
Zhenkun Wang, and Qingfu Zhang. Multi-objective evo-
lution of heuristic using large language model. In Pro-
ceedings of the AAAI Conference on Artificial Intelligence,
volume 39, pages 27144–27152, 2025.
[Ye et al., 2024] Haoran Ye, Jiarui Wang, Zhiguang Cao,
Federico Berto, Chuanbo Hua, Haeyeon Kim, Jinkyoo
Park, and Guojie Song. Reevo: large language models as
hyper-heuristics with reflective evolution. In Proceedings
of the 38th International Conference on Neural Informa-
tion Processing Systems, NIPS ’24, Red Hook, NY, USA,
2024. Curran Associates Inc.
[Yu et al., 2025] Haoyang Yu, Na Tang, Zixu Zhu, and
Zhenyang Guo. Flexible job-shop scheduling via gated re-
current unit and deep reinforcement learning. Knowledge-
Based Systems, 330:114734, 2025.
[Zhang et al., 2023] Fangfang Zhang, Yi Mei, Su Nguyen,
Kay Chen Tan, and Mengjie Zhang.
Task relatedness-
based multitask genetic programming for dynamic flexible
job shop scheduling. IEEE Transactions on Evolutionary
Computation, 27:1705–1719, 2023.
[Zhang et al., 2025] Junjie Zhang, Canhui Luo, Zhouxing
Su, Qingyun Zhang, Zhipeng L¨u, Junwen Ding, and Yan
Jin. Ns4s: Neighborhood search for scheduling problems
via large language models. 2025.
[Zhao et al., 2025] Ruxin Zhao, Lixiang Fu, Jiajie Kang,
Chang Liu, Wei Wang, Haizhou Wu, Yang Shi, Chao
Jiang, and Rui Wang.
Data-driven evolutionary algo-
rithms based on initialization selection strategies, pox
crossover and multi-point random mutation for flexible
job shop scheduling problems. Applied Soft Computing,
172:112901, 2025.
[Zhu et al., 2023] Kaikai Zhu,
Guiliang Gong,
Ningtao
Peng, Liqiang Zhang, Dan Huang, Qiang Luo, and Xiao-
qiang Li. Dynamic distributed flexible job-shop schedul-
ing problem considering operation inspection. Expert Sys-
tems with Applications, 224:119840, 2023.


--- Page 10 ---
A
Algorithm Details
In this part, we elaborate on the main framework, gene selection and critical operation swapping used in LLM4EO, as illustrated
in Algorithm 3, Algorithms 4 and Algorithms 5, respectively.
Main Framework.
The solution-operator co-evolution framework of LLM4EO, as presented in Algorithm 3, employs GA
to optimize the solution population and leverages LLMs to autonomously improve operators in iterations to accelerate opti-
mization. First, the initial solution and operator populations are generated during the initialization phase. Then, new solution
population is produced through crossover, mutation, and critical operation swapping. In this process, new individuals are gener-
ated by operators that select which genes to perturb. When a predefined threshold is reached, the operator evolution mechanism
is triggered, leveraging the LLM to design new operators and replace poor performers within the operator population. Finally,
output the optimal solution once the termination condition is satisfied.
Gene Selection.
For gene selection, as shown in Algorithm 4, features of jobs and operations are leveraged by LLM-driven
operators to create linear or non-linear relationships between genes, generating probabilistic distributions. The potential genes
are more likely to be selected for perturbation, thereby generating more diverse and superior solutions.
Critical Operation Swapping.
For critical operation swapping, as shown in Algorithm 5, genes of the critical path are
selected by meta-operators and then two of them are randomly exchanged. If a better individual is obtained, the search process
is repeated on the new critical path. Otherwise, the process is terminated. This approach combining local search with LLM-
driven operators helps accelerate convergence.
B
Prompt and Generation Details
This section provides examples of the carefully crafted prompts and LLM responses for each module of the co-evolutionary
architecture. These modules include operator design, evolutionary perception and analysis, and adaptive operator evolution.
B.1
Operator Design
In this subsection, we introduce the prompt for operator initialization and the details of the initial operators, as shown in Figure
8. The five different colors represent five different components of the prompt, including Task description, Prior knowledge,
Expected output, Code template, and Special hints. Based on the structured prompt, the LLM generates three distinct initial
operators and provides descriptions of these, as well as executable functions that follow the code template. These operators
establish linear or nonlinear relationships among genes based on seven genetic features and output gene selection priorities.
B.2
Evolution Perception and Analysis
This subsection presents the prompt and the result for the perception and analysis of evolution in Figure 9. The five different
colors represent five different components of the prompt, including Solution population changes, Operator population infor-
mation, Task description, Expected output, and Special hints. Population changes can be described using metrics such as the
iteration count, minimum value, average value, and rate of change. For the operator population performance, the fitness value
and the computational approach of each operator are described. Given the above information, the LLM is expected to assess
the optimization dilemma, analyze the limitations of the operator population, and provide a design suggestion.
B.3
Adaptive Operator Evolution
As shown in Figure 10, most of the content in the prompts for adaptive operator evolution comes from operator design, as
well as evolutionary perception and analysis. Notably, the task descriptions are derived from variations of the initial operator
design task via the LLM, which helps mitigate the inertia of the LLM and generate diverse operators, as illustrated in Figure 11.
According to the suggestions provided by the LLM, the priority calculation rules for new operators focus on process order, start
time, and processing time, as indicated by the bolded content in the code. It emphasizes “sequential bottlenecks and machine
substitution potential to minimize makespan”.
C
Other Experimental Details
C.1
Comparison with Genetic Algorithms
To further evaluate the impact of LLM-generated meta-operators on the convergence of evolutionary algorithms, LLM4EO is
compared with several genetic algorithms. These algorithms include the traditional genetic algorithm proposed by F.Pezzella
et al. (P’GA) [Pezzella et al., 2008], the self-learning genetic algorithm based on reinforcement learning (SLGA) [Chen et
al., 2020], and the basic genetic algorithm (GA) of LLM4EO. The components of GA and LLM4EO are essentially the same,
with the only difference being that GA generates offspring by randomly selecting genes rather than relying on LLM-generated
operators. The operator population size is set to 3, the solution population size to 100 and the maximum number of iterations
to 200. These algorithms run independently 10 times on each Brandimarte benchmark instance.
As shown in Table 4, LLM4EO achieves optimal BM and AM values for all instances except MK07, which demonstrates
its outstanding performance. Furthermore, the average relative percentage deviation (RPDaver) of LLM4EO is lower than


--- Page 11 ---
that of the comparison algorithms, thus validating its effectiveness in solving the FJSP. Notably, LLM4EO is based on GA but
employs meta-operators to select promising genes for generating individuals. With the assistance of LLMs, the RPDaver of
BM decreased from 13.19 to 12.71, achieving a 3.64% performance improvement; the RPDaver of AM dropped from 14.58
to 14.12, yielding a 3.16% performance gain. These improvements demonstrate that the proposed meta-operator effectively
enhances the exploration capability and stability of evolutionary algorithms.
Figure 6 presents the average convergence of four algorithms after running 10 times on the MK01, MK02, MK04, MK05,
MK06, MK07, MK09, and MK10 instances. For the MK03 and MK08 instances, no significant convergence process is observed
since the initial solutions already attain the theoretical lower bound. It can be seen that, in most cases, LLM4EO converges
faster than other algorithms and ultimately yields higher-quality solutions. This phenomenon validates that meta-operators
based on LLMs effectively accelerate the algorithmic evolution process.
Table 4: Comparison results of genetic algorithms with LLM4EO.
Instance
Size
LB
BM
AM
F’GA SLGA
GA
LLM4EO F’GA SLGA
GA
LLM4EO
MK01
10×6
36
42
42
40
40
42.6
42.1
40.8
40.6
MK02
10×6
24
29
28
27
27
30.6
28.8
28.0
27.6
MK03
15×8
204
204
204
204
204
204.0
204.0
204.0
204.0
MK04
15×8
48
69
66
62
60
72.0
67.0
64.1
62.6
MK05
15×4
168
177
176
174
173
178.5
177.7
176.0
175.8
MK06
10×15
33
71
74
63
63
75.1
78.1
65.2
65.2
MK07
20×5
133
150
145
143
144
154.1
149.5
144.9
145.1
MK08
20×10 523
523
523
523
523
527.4
523.0
523.0
523.0
MK09
20×10 299
362
360
311
311
368.1
366.1
313.2
313.0
MK10
20×15 165
251
269
224
217
258.7
273.9
229.0
225.5
RPDaver
18.36
17.97
13.19
12.71
20.19
19.18
14.58
14.12
0
1 0 0
2 0 0
4 0
4 2
4 4
4 6
M
a k e s p a n
I t e r a t i o n
M
K 0 1
0
1 0 0
2 0 0
2 8
3 0
3 2
3 4
M
a k e s p a n
I t e r a t i o n
M
K 0 2
0
1 0 0
2 0 0
6 4
6 8
7 2
7 6
8 0
8 4
8 8
M
a k e s p a n
I t e r a t i o n
M
K 0 6
0
1 0 0
2 0 0
1 4 4
1 5 0
1 5 6
1 6 2
1 6 8
1 7 4
 P ' G A     
 S L G A   
M
a k e s p a n
I t e r a t i o n
M
K 0 7
0
1 0 0
2 0 0
6 0
6 4
6 8
7 2
7 6
8 0
8 4
M
a k e s p a n
I t e r a t i o n
M
K 0 4
0
1 0 0
2 0
1 7 6
1 8 0
1 8 4
1 8 8
1 9 2
M
a k e s p a n
I t e r a t i o n
M
K 0 5
0
1 0 0
2 0 0
3 0 0
3 3 0
3 6 0
3 9 0
 
 G A     
 L L M
4 E O
M
a k e s p a n
I t e r a t i o n
M
K 0 9
0
1 0 0
2 0
2 2 0
2 3 0
2 4 0
2 5 0
2 6 0
2 7 0
2 8 0
2 9 0
M
a k e s p a n
I t e r a t i o n
M
K 1 0
Figure 6: Convergence curves of genetic algorithms on Brandimarte benchmark.
C.2
LLM4EO on Distributed Flexible Job Shop Scheduling
The experimental scope is expanded to the distributed flexible job shop scheduling problem (DFJSP), with the aim of exploring
the generalization and robustness of LLM4EO. The benchmark from [De Giovanni and Pezzella, 2010] is used to test per-
formance. Three comparison algorithms include Giovanni’GA [De Giovanni and Pezzella, 2010], Lu’GA [Lu et al., 2018],
CRO [Zhu et al., 2023] and the basic genetic algorithm (GA) of LLM4EO. Three key components of the proposed algorithm
are adjusted: 1) Add factory crossover: Select multiple operations from the parent and swap their factories to generate new
offspring. 2) Add factory mutation: Assign the selected operations to other machines of the current factory. 3) Modified critical


--- Page 12 ---
operation swapping: In the critical path, select an operation and sequentially swap it with other operations. If fitness improves,
repeat this local search process on the new critical path.
Table 5 presents the results for 20 instances with two factories. It can be seen that all algorithms achieve LB in LA01-LA05
and LA16-LA20. Additionally, LLM4EO obtains the optimal BM in LA06 and LA8-LA15, demonstrating its outstanding
generalization. Figure 7 shows the box plot of the makespan values obtained by LLM4EO and GA in ten runs. It is evident that
the result distribution of LLM4EO is more concentrated than that of GA, with a shorter box positioned closer to the lower end.
This indicates that it exhibits superior performance and fewer fluctuations in most cases. Therefore, the proposed LLM-driven
meta-operator is still effective in enhancing the performance of evolutionary algorithms when solving the DFSP.
Table 5: Computational results for the DFJSP with two factories.
Instance
Size
LB
LLM4EO
GA
Giovanni’GA
Lu’GA
CRO
BM
AM
BM
AM
BM
AM
BM
AM
LA01
10 × 5
413
413
413
413
413
413
413
413
413
413
LA02
10 × 5
394
394
394
394
394
394
394
394
394
394
LA03
10 × 5
349
349
349
349
349
349
349
349
349
349
LA04
10 × 5
369
369
369
369
369
369
369
369
369
369
LA05
10 × 5
380
380
380
380
380
380
380
380
380
380
LA06
15 × 5
413
419
430.4
423
434
445
449.6
424
435.8
424
LA07
15 × 5
376
399
405
394
405.6
412
419.2
398
408.5
398
LA08
15 × 5
369
404
417.2
404
421
420
427.8
406
417.4
406
LA09
15 × 5
382
442
456
445
463.2
469
474.6
447
459
463
LA10
15 × 5
443
443
443.6
443
444.1
445
448.6
443
444.1
445
LA11
20 × 5
413
543
548.7
544
549.1
570
571.6
548
557.1
553
LA12
20 × 5
408
472
479.4
474
484
504
508
480
492.5
500
LA13
20 × 5
382
530
535.7
533
537
542
552.2
533
538.4
551
LA14
20 × 5
443
540
547.7
543
549.2
570
576
542
557.3
581
LA15
20 × 5
378
556
564.9
559
568.5
584
588.8
562
568.7
597
LA16
10 × 10 717
717
717
717
717
717
717
717
717
717
LA17
10 × 10 646
646
646
646
646
646
646
646
646
646
LA18
10 × 10 663
663
663
663
663
663
663
663
663
663
LA19
10 × 10 617
617
617
617
617
617
617.2
617
622.1
617
LA20
10 × 10 756
756
756
756
756
756
756
756
756
756
L A 0 6
L A 0 7
L A 0 8
L A 0 9
L A 1 0
L A 1 1
L A 1 2
L A 1 3
L A 1 4
L A 1 5
4 0 0
4 5 0
5 0 0
5 5 0
6 0 0
 G A    
 L L M
4 E O
M
a k e s p a n
Figure 7: Partial box plots of LLM4EO and GA on LA06-LA15 instances.


--- Page 13 ---
Algorithm 3 The main framework of LLM4EO
Input: solution population size psoln, operator population size pop
Output: the best solution s′
1: Initialize a solution population Psoln = {s1, s2, ...spsoln} and an operator population Pop = {o1, o2, ...opop}
2: while the termination condition is satisfied do
3:
P ′
soln ←∅
4:
s′ ←select the best solution among Psoln
5:
// Crossover
6:
for i = {1, 2, ..., psoln/2} do
7:
Select two solutions sx and sy from Psoln
8:
Randomly generate a decimal rc ∈[0, 1]
9:
// pc is the crossover probability
10:
if rc < pc then
11:
Randomly generate a 0-1 variable vc
12:
if vc = 0 then
13:
Select two gene sets δx and δy of sx and sy based on job features
14:
P ′
soln = P ′
soln∪Crossover OSV(sx, sy, δx, δy)
15:
else
16:
Select two gene sets δx and δy of sx and sy based on operation features
17:
P ′
soln = P ′
soln∪Crossover MAV(sx, sy, δx, δy)
18:
end if
19:
else
20:
P ′
soln = P ′
soln ∪(sx, sy)
21:
end if
22:
end for
23:
// Mutation
24:
for i = {1, 2, ..., psoln} do
25:
Randomly generate a decimal rm ∈[0, 1]
26:
// pm is the mutation probability
27:
if rm < pm then
28:
Randomly generate a 0-1 variable vm
29:
if vm = 0 then
30:
Select a gene set δ′
i of s′
i based on job features
// P ′
soln = {s′
1, s′
2, ..., s′
psoln}
31:
s′
i ←Mutation OSV(s′
i, δ′
i)
32:
else
33:
Select a gene set δ′
i of s′
i based on operation features
34:
s′
i ←Mutation MAV(s′
i, δ′
i)
35:
end if
36:
end if
37:
end for
38:
// Critical operation swapping
39:
for i = {1, 2, ..., psoln} do
40:
s′
i ←CriticalOperationSwapping(s′
i, Pop)
41:
end for
42:
// Population update
43:
sb ←select the best solution among Psoln
44:
s′
w ←select the worst solution among P ′
soln
45:
s′
w ←sb, Psoln ←P ′
soln
46:
if Cmax(sb) < Cmax(s′) then
47:
s′ ←sb
48:
end if
49:
// Operator evolution by LLMs
50:
if the operator evolution condition is satisfied then
51:
Generate a new operator o′ using the operator evolution procedure.
52:
ow ←select the worst operator among Pop
53:
ow ←o′
54:
end if
55: end while
56: return s′


--- Page 14 ---
Algorithm 4 Gene selection
Input: solution s, operator population Pop, feature type {job, operation}
Output: selected genes δ
1: // I represent the numbers of jobs and Ji is the operation count of job i, respectively
2: // n and m denote the dimensional numbers of job features and operation features, respectively
3: Compute job features Njob = [{β1
1, β1
2, ..., β1
I}, ..., {βn
1 , βn
2 , ..., βn
I }] of solution s
4: Compute operation features Noperation = [{β1
1,1, β1
1,2, ..., β1
I,JI}, ..., {βm
1,1, βm
1,2, ..., βm
I,JI}] of solution s
5: Select an operator o from Pop by the roulette wheel method
6: // CalculatePriority is the function of operator o
7: {α1, α2, ..., αI}, {α1,1, α1,2, ..., αI,JI} ←CalculatePriority(Njob, Noperation)
8: δ ←∅
9: if feature type is job then
10:
{γ1, γ2, ...γI} ←Normalisation({α1, α2, ...αI})
11:
for i ≤I do
12:
Randomly generate a decimal ui ∈[0, 1]
13:
if γi > ui then
14:
δ′ ←select all genes of operations belonging to the job i
15:
δ = δ ∪δ′
16:
end if
17:
end for
18: else if feature type is operation then
19:
{γ1,1, γ1,2, ...γI,JI} ←Normalisation({α1,1, α1,2, ..., αI,JI)
20:
for Oij(i ≤I, j ≤Ji)) do
21:
Randomly generate a decimal uij ∈[0, 1]
22:
if γij > uij then
23:
δ′ ←select the gene of operation Oij
24:
δ = δ ∪δ′
25:
end if
26:
end for
27: end if
28: return δ
Algorithm 5 Critical operation swapping
Input: solution s, operator population Pop
Output: the best solution s′
1: s′ ←s
2: while s′ achieves the lower makespan do
3:
s ←s′
4:
Identify the critical path of s
5:
Select an operator o from Pop by roulette
6:
Select two genes gx and gy of the critical path by operator o
7:
Swap gx and gy
8:
if Cmax(s) < Cmax(s′) then
9:
s′ ←s
10:
end if
11: end while
12: return s′


--- Page 15 ---
In order to minimize the makespan in the flexible job-shop scheduling problem, please design an operator to calculate the adjustment priority 
for jobs and operations.
Refer to some heuristics, such as the Shortest Processing Time (SPT) rule. 
1. First, describe your new operator and main steps in one sentence, The description must be inside within boxed {}. 
2. Next, implement the following Python function:
def calculate_priority(process_span: list, minimal_process_span: list, operation_number: list, earliest_start_time: list, start_time: list, 
processing_time: list, machine_number: list)-> (list[float],list[float]): 
Job Args:
process_span: The time interval between starting the first operation and completing the last operation of each job. 
minimal_process_span: The sum of the shortest processing times for all operations of each job.
operation_number: Number of operations for each job.
Operation Args:
earliest_start_time: Completion time of the previous operation for each operation.
start_time: Processing start time on the current machine for each operation. The start_time is greater than or equal to the earliest_start_time.
processing_time: Processing time on the current machine for each operation.
machine_number: The number of optional machines for each operation.
Return: The types of all return variables must be a list, not an array.
job_priority: Priority of processing sequence adjustment for each job. Length of job_priority is job number. The value is not negative. 
operation_priority: Priority of the current machine being replaced by other machines for each operation. Length of operation_priority is 
operation number. The value is not negative.
The function must not be empty. 
The operator description must not be empty. 
Do not give additional explanations.
Prompt for Operator Design
Initial Operators
Description 1
The operator calculates job priorities by combining their relative slack 
(difference between actual process span and minimal process span per 
operation) and counts of operations, and determines operation priorities 
using the difference between actual and earliest start times weighted by the 
number of optional machines available.
def calculate_priority(process_span, minimal_process_span, 
  operation_number, earliest_start_time, start_time, processing_time, 
  machine_number):
  # Calculate job priority 
  job_priority = []
  for i in range(job_number):
      opnum = operation_number[i]
      min_span = minimal_process_span[i]
      relative_slack = (process_span[i] - minimal_process_span[i]) / opnum
      priority = relative_slack + operation_number[i] /  min_span
      job_priority.append(priority)
  # Calculate operation priority
  operation_priority = []
  for i in range(operation_number):
      machine_count = machine_number[i]
      delay = max(0, start_time[i] - earliest_start_time[i])
      priority = delay * machine_count
      operation_priority.append(priority)
  return job_priority, operation_priority
Description 2
The operator assigns job priorities by combining the job's process span 
ratio and normalized operation count, and operation priorities by combining 
the slack between earliest and actual start times with the inverse normalized 
machine options and relative operation process time.
def calculate_priority(process_span, minimal_process_span, 
  operation_number, earliest_start_time, start_time, processing_time, 
  machine_number):
  # Calculate job priority 
  job_priority = []
  for i in range(job_number):
      op_num_norm = operation_number[i] / max(operation_number)
      priority = (process_span[i] / minimal_process_span[i]) * op_num_norm
      job_priority.append(priority)
  # Calculate operation priority
  operation_priority = []
  for est, ast, pt, mn in zip(earliest_start_time, start_time, processing_time, 
    machine_number):
      slack = max(0, ast - est)
      slack_norm = slack / max(start_time)
      inv_machine = (max_machine - mn + 1) / max_machine
      pt_norm = pt / max(processing_time)
      priority = slack_norm + inv_machine * pt_norm
      operation_priority.append(priority)
  return job_priority, operation_priority
Description 3
Calculate job_priority by weighting relative stretch and operation count, 
and calculate operation_priority by combining slack and machine flexibility 
to minimize makespan.
def calculate_priority(process_span, minimal_process_span, 
  operation_number, earliest_start_time, start_time, processing_time, 
  machine_number):
  # Calculate job priority 
  job_priority = []
  for i in range(job_number):
      span = process_span[i]
      minimal = minimal_process_span[i]
      priority = (span - minimal) * operation_number[i]
      job_priority.append(priority)
  # Calculate operation priority    
  operation_priority = []
  for est, ast, pt, mNum in zip(earliest_start_time, start_time, 
    processing_time, machine_number):
      slack = ast - est
      priority = (1 / (slack+1)) * max(mNum, 1)
      operation_priority.append(priority)
  return job_priority, operation_priority
LLM
Code 1
Code 2
Code 3
Figure 8: Prompt and output for initial operator design.


--- Page 16 ---
The characteristics of the population are as follows:
1. The current iteration progress is 46/200.
2. The minimum makespan is 241 and the average makespan is 263.62.
3. The average makespan change rate is 22.12%.
4. The minimum makespan change rate is 14.68%.
Given 3 operators: 
1. (Successful evolution rate: 0.0721847) The operator calculates job priorities by combining their relative slack (difference between actual 
process span and minimal process span per operation) and counts of operations, and determines operation priorities using the difference 
between actual and earliest start times weighted by the number of optional machines available.
2. (Successful evolution rate: 0.0749139) The operator assigns job priorities by combining the job's process span ratio and normalized 
operation count, and operation priorities by combining the slack between earliest and actual start times with the inverse normalized machine 
options and relative operation process time.
3. (Successful evolution rate: 0.0548252)  "Calculate job priorities by weighting relative stretch and operation count, and calculate operation 
priorities by combining slack and machine flexibility to minimize makespan." 
Please characterize the evolutionary state of the solution population, describe the limitations of each operator, and provide a suggestion for 
designing a new operator 
1. First, characterize the evolution of populations in one sentence. 
2. Next, describe the limitations of each operator in one sentence respectively. For example: First operator, <Limitation of the operator>. 
3. Finally, a suggestion for designing a new operator is given in one sentences. The operator calculates sequential priority and machine 
replacement priority for all jobs to minimize the makespan, according to processing order, start time and processing time for all jobs.
Do not give additional explanations.
Prompt for Evolution Perception and Analysis
LLM
Operator Analysis
Operator 1, its reliance on relative slack and operation counts may 
insufficiently capture dynamic scheduling complexities; 
Operator 2, the combined ratios may dilute critical priority indicators 
leading to suboptimal selection; 
Operator 3, the lower evolution rate indicates its weighting of stretch 
and machine flexibility is less effective in guiding improvements.  
Evolution Assessment
The population evolution has 
stagnated with no improvement 
in minimum or average 
makespan over recent iterations.  
Design suggestion
Design a new operator that 
calculates sequential priority 
and machine replacement 
priority based on processing 
order, start time, and processing 
time for all jobs to adaptively 
minimize the makespan.
Output
Figure 9: Prompt and output for evolution perception and analysis.


--- Page 17 ---
New Operator
# Evolution characteristics
Given characteristics of the population:
1. Character description
2. …
# Existing operators
Given 3 operators: 
1. <Operator fitness> <Operator description>
2. …
3. …
Given population evaluation, operator limitations, and suggestions for 
operator design are presented below: 
# Evolution assessment
The population evolution has stagnated with no improvement in minimum 
or average makespan over recent iterations. 
# Operator analysis
1. Operator 1, its reliance on relative slack and operation counts may 
insufficiently capture dynamic scheduling complexities; 
2. Operator 2, the combined ratios may dilute critical priority indicators 
leading to suboptimal selection; 
3. Operator 3, the lower evolution rate indicates its weighting of stretch and 
machine flexibility is less effective in guiding improvements.
# Design suggestion
Design a new operator that calculates sequential priority and machine 
replacement priority based on processing order, start time, and processing 
time for all jobs to adaptively minimize the makespan.
# Mutation task
To minimize the makespan, please design a completely different operator 
from the above ones to calculate the sequential adjustment priority for jobs 
and operations:
# Expected output
1. First, describe your new operator and main steps in one sentence, The 
description must be inside within boxed {}. 
2. Next, implement the following Python function:
# Code template
def name(input: type, …, input: type)-> (output: type, …, output: type):
    Description of all input parameters
Return: Description of all output parameters
# Other hints
The function must not be empty. 
The operator description must not be empty. 
Do not give additional explanations.
Prompt for Operator Evolution
Description
Calculate job_ priority by combining normalized slack time and 
inverse operation density, and calculate operation_priority based on 
operation waiting time and processing time weighted by machine 
replaceability, emphasizing sequential bottlenecks and machine 
substitution potential to minimize makespan.
Code
def calculate_priority(process_span, minimal_process_span, 
  operation_number, earliest_start_time, start_time, processing_time,
  machine_number):
  # Calculate job priority 
  job_priority = []
  for i in range(job_number):
      slack = process_span[i] – minimal_process_span[i]
      slack_norm = slack / max(process_span)
      density = process_span[i] / operation_number[i]
      priority = max(slack_norm * density, 0)
      job_priority.append(priority)
  # Calculate operation priority
  operation_waiting = []
  for i in range(operation_number):
      operation_waiting.append(start_time[i] - earliest_start_time[i])
  operation_priority = []
  for i in range(operation_number):
      wait_norm = operation_waiting[i] / max(operation_waiting)
      proc_norm = processing_time[i] / max(processing_time)
      priority = wait_norm * proc_norm / machine_number[i]
      operation_priority.append(priority)
  return job_priority, operation_priority
LLM
Figure 10: Prompt and output for operator evolution.
Prompt for Task Mutation
Rewrite the following sentence without changing its 
original meaning and do not give additional explanations:
“In order to minimize the makespan, please design a 
new operator totally different from the above operators to 
calculate the sequential adjustment priority for jobs and 
operations:”
Mutation task
To minimize the makespan, 
please design a completely 
different operator from the 
above ones to calculate the 
sequential adjustment priority 
for jobs and operations:
LLM
Figure 11: Prompt and output for task mutation.
