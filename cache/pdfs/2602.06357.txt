--- Page 1 ---
Evaluating LLM-persona Generated Distributions for
Decision-making∗
Jackie Baek†
Yunhan Chen‡
Ziyu Chi§
Will Ma¶
Abstract
LLMs can generate a wealth of data, ranging from simulated personas imitating human
valuations and preferences, to demand forecasts based on world knowledge. But how well do such
LLM-generated distributions support downstream decision-making? For example, when pricing
a new product, a firm could prompt an LLM to simulate how much consumers are willing to
pay based on a product description, but how useful is the resulting distribution for optimizing
the price?
We refer to this approach as LLM-SAA, in which an LLM is used to construct
an estimated distribution and the decision is then optimized under that distribution. In this
paper, we study metrics to evaluate the quality of these LLM-generated distributions, based
on the decisions they induce. Taking three canonical decision-making problems (assortment
optimization, pricing, and newsvendor) as examples, we find that LLM-generated distributions
are practically useful, especially in low-data regimes. We also show that decision-agnostic metrics
such as Wasserstein distance can be misleading when evaluating these distributions for decision-
making.
1
Introduction
In many problems, the optimal decision depends on a distribution of possible outcomes. Examples
include deciding a single product assortment for a distribution of customer preferences, setting
a price for a distribution of willingness-to-pay, or selecting an inventory level for a distribution
of demand scenarios.
Here, outcomes can vary either across a population (e.g., heterogeneous
preferences) or over time (e.g., stochastic demand), and the true distribution F governing these
outcomes is typically unknown to the decision-maker, and difficult to characterize. While historical
data is the canonical foundation for quantifying uncertainty about F, it is frequently sparse, costly
to collect, or entirely unavailable for new products and evolving market conditions.
Large language models (LLMs) offer a new data source in precisely these low-data regimes.
When real observations are limited or unavailable, an LLM can be prompted to generate synthetic
samples or to describe a distribution using its knowledge of the world. For example, in a pric-
ing problem, one can prompt an LLM to generate samples of willingness-to-pay given a product
description and/or customer characteristics, which induces an empirical distribution that can be
fed into a pricing optimizer. This suggests a simple pipeline which we call LLM-SAA: elicit an
LLM-generated distribution bF for the outcome of interest, and then optimize the decision based
on bF.
∗Code: https://github.com/yunhanchen2/Evaluating-LLM-persona-Generated-Distributions-for-Decision-making
†Stern School of Business, New York University, baek@stern.nyu.edu
‡Department of Computer Science, Columbia University, yc4512@columbia.edu
§Stern School of Business, New York University, zc3725@stern.nyu.edu
¶Graduate School of Business and Data Science Institute, Columbia University, wm2428@gsb.columbia.edu
1
arXiv:2602.06357v1  [cs.LG]  6 Feb 2026


--- Page 2 ---
This idea is related to a growing literature that uses LLMs to generate synthetic data by
“simulating humans”, for example to pilot social science experiments or generate survey samples
to understand public opinion (see subsection 2.1). Our work uses LLMs similarly but for a new
application: making downstream operational decisions. For this application, a central challenge is
evaluation: how do we determine whether an LLM-generated distribution bF is “good” for decision-
making when the true distribution is F? Prior work on LLM-based simulation has largely relied
on decision-agnostic notions of “distributional alignment”, measuring statistical distances between
the distributions bF and F. For our application, however, statistical closeness is not the goal. What
ultimately matters is decision quality: whether optimizing on bF yields actions that perform well
under F.
This landscape motivates two connected research questions:
1. How should we evaluate LLM-generated distributions when the objective is downstream deci-
sion quality?
2. Can LLM-generated distributions be helpful for making downstream operational decisions?
Answering these questions is subtle. On one hand, bF can lead to the right decision for completely
the wrong reason, yielding a “lucky guess” when bF had no resemblance to F. On the other hand,
expecting bF to look exactly like F may be unnecessary for systematically making good decisions.
We balance between these extremes by testing a range of exogenous problem parameters that
change the optimal decision under F. This makes it harder to repeatedly make a “lucky guess”,
yet does not require bF to be identical to F. As an example, for the pricing problem, we can adjust
the cost of procuring the product, which affects its optimal price. For bF to yield a good decision
under a range of costs, it just needs to correctly capture the relevant features of F, such as where
purchase probabilities drop sharply; see Section F for an illustration of this.
Next, we must define a reasonable range of problem parameters to evaluate. To avoid arbitrari-
ness, we resort to worst-case analysis, where we identify the problem parameters that make the
decision suggested by bF look pessimal relative to the optimal decision knowing F. To our knowl-
edge, this is a new concept in data-driven and robust optimization (see Subsection 2.2); computing
these worst-case parameters for a given bF and F poses an interesting problem.
Contributions and approach.
Our main contribution is to develop a evaluation methodology
for the LLM-SAA pipeline, whereby we measure downstream decision quality rather than statistical
alignment.
We apply this methodology to three well-studied problems in operations research:
assortment, pricing, and newsvendor.
We define decision-aware performance metrics based on competitive ratios that compare the
performance of the action optimized under bF to the true optimal action under F. We study both
the worst-case competitive ratio over problem parameters, and an average-case competitive ratio
over a parameter distribution. Our main technical contribution is showing how to compute the
worst-case competitive ratio under the three problem settings that we study. Moreover, we show
that this worst-case metric is reasonably concordant with average-case parameter ranges that one
may deem relevant (cf. Roughgarden, 2021).
We consider multiple generation methods for eliciting the LLM distribution bF; these methods
differ in what information is provided to the LLM and how it is prompted. We leverage real-world
datasets across all three problem settings to evaluate LLM-generated distributions against baselines
such as uniform distributions and d-sample empirical distributions. We evaluate performance using
the worst-case and average-case competitive ratios, and also contrast these with standard decision-
agnostic distances such as Wasserstein and Kolmogorov distances.
2


--- Page 3 ---
We summarize our main takeaways:
I. LLM-SAA can be practically useful in low-data regimes: it consistently outperforms unin-
formed baselines and can be competitive with empirical distributions on a small number of
real samples;
II. Decision-agnostic distances can be misleading in measuring whether a generated distribution
is good for decision-making;
III. Steering with personas can improve decision-making, even though the persona-based LLM
responses do not accurately match true responses at an individual level.
2
Related Literature
2.1
Using LLMs to Simulate Humans
A growing literature proposes using LLMs as “synthetic respondents” to approximate human survey
responses and choice behavior in social science and market research (Aher et al., 2023; Argyle et al.,
2023; Brand et al., 2023; Horton, 2023; Park et al., 2023; Manning et al., 2024; Wang et al., 2024).
Despite their promise, some papers caution against using LLMs as direct human replacements,
citing concerns about representativeness and behavioral fidelity (Dillion et al., 2023; Sarstedt et al.,
2024; Gao et al., 2025). Consequently, a substantial line of recent work focuses on benchmarking and
enhancing LLM-based human simulation (Samuel et al., 2024; Xie et al., 2024; Qu and Wang, 2024;
Meister et al., 2025; Suh et al., 2025; Toubia et al., 2025). While these existing benchmarks assess
fidelity at the individual or aggregate level, our work introduces decision-aware evaluation metrics
to assess the utility of LLM-generated population distributions for downstream decision-making.
2.2
Data-driven and Robust Optimization
We study stochastic optimization problems where the distribution F is unknown. When historical
samples are available, a standard approach is Sample Average Approximation (SAA), replacing
expectations under F with empirical averages Shapiro et al. (2014). In the absence of data, one
typically resorts to robust optimization Ben-Tal et al. (2009) or online algorithms analyzed via
competitive ratios Borodin and El-Yaniv (1998). In the age of LLMs, however, SAA can be used
even in no-data regimes. We refer to this approach as LLM-SAA: we use an LLM to construct an
estimated distribution bF (often via synthetic samples), and then optimize the downstream decision
under bF via SAA. We study how different generation methods (prompting strategies and LLM
models) affect bF. For evaluation, we consider worst-case competitive ratios over known instance
parameters θ while holding F and bF fixed, which is different from typical analyses that consider
worst cases over unknowns in F.
2.3
LLMs for Operational Decisions
There is a growing literature and application for LLMs inside decision-making pipelines. Work that
evaluates LLMs directly as decision makers in canonical OR tasks finds that plausible language
outputs can still correspond to biased or unreliable actions, for example in the newsvendor problem
(Liu et al., 2025). However, recent work suggests that LLMs can straight-up outperform humans in
the challenging beer game for inventory management (Long et al., 2025). An extensive study using
an LLM to directly manage a real vending machine is described in Backlund and Petersson (2025).
3


--- Page 4 ---
Related to pricing, Cao and Hu (2026) study the potentially collusive consequences of using LLMs
to make pricing decisions.
A complementary set of papers argues that the central opportunity is to pair LLMs with op-
timization to reduce modeling and communication frictions. Wang et al. (2025) use a transformer
(the architecture behind LLMs) to predict optimal decisions. Duan et al. (2025) propose a hybrid
approach in which an LLM elicits and clarifies inputs and then a solver computes the decision,
reporting large cost improvements over an end-to-end LLM baseline. A line of work explicitly uses
LLMs to formulate textbook optimization (Huang et al., 2025) and dynamic programming (Zhou
et al., 2025) problems, feeding them into optimization solvers. Broader discussion positions LLMs
as a planning layer around optimization tools (Simchi-Levi et al., 2026), describing research agendas
for AI in supply chains (Cohen et al., 2025).
Our work fits into the latter “LLM plus optimization” literature, but in contrast, focuses on
using LLMs to generate data for optimization problems under uncertainty, i.e., LLM-SAA. Eval-
uating the generated distributions wholistically is subtle, which is the main investigation of our
work.
3
Problems, Methods, and Metrics
We consider problems where there is an unknown distribution F over outcomes ξ (e.g., ξ is a
customer preference ranking). There is a set of known parameters θ, which dictate the feasible
action set Aθ and the reward rθ(a, ξ) that is earned when action a ∈Aθ (e.g., a is an assortment
of products) is taken under realized outcome ξ. The objective is to decide an action to maximize
the expected reward Rθ(a) := Eξ∼F [rθ(a, ξ)], where a∗∈argmaxa∈AθRθ(a) denotes an optimal
action. Our overall approach is LLM-SAA: we use an LLM to construct an estimated distribution
bF, then choose the action ˆa by solving the corresponding sample-average approximation under bF.
Specifically, letting bRθ(a) := Eξ∼bF [rθ(a, ξ)] be the estimated expected rewards, the action is chosen
as ˆa ∈argmaxa∈Aθ bRθ(a).
We consider three well-studied problems in decision-making under uncertainty.
• Assortment.1 There are n items, indexed by [n] := {1, . . . , n}. An outcome ξ is a strict
ranking of the items, written ξ = (ξ1, . . . , ξn), where ξ1 is the most-preferred item and ξn is
least-preferred. An action a ⊆[n] is the set of offered items, where a must lie in some feasible
family Aθ (e.g., Aθ = {a : |a| ≤k}, where k is a size constraint). Given assortment a, a
customer with ranking ξ derives highest utility if their most-preferred item ξ1 is found in a,
2nd-highest utility if ξ1 /∈a but ξ2 ∈a, and so forth. Formally,
rθ(a, ξ) = rmin{p∈[n]:ξp∈a}
where (rp)p∈[n] denotes the position-dependent rewards satisfying r1 ≥· · · ≥rn ≥0. The
parameters θ consist of the reward vector (rp)p∈[n] and any parameters, e.g. k, that define the
feasible family Aθ.
• Pricing. The outcome ξ ≥0 denotes a buyer’s willingness-to-pay for a product. The action
is a price a ≥0 for the product. The parameter θ = c ≥0 defines the item’s unit cost, and
the reward is the profit from selling the item,
rθ(a, ξ) = (a −c)1{ξ ≥a}.
1Standard assortment optimization focuses on revenue maximization (Talluri and van Ryzin, 2004). We instead
study utility maximization like Sumida et al. (2021), but define it for the ranking-based choice model (Farias et al.,
2013).
4


--- Page 5 ---
• Newsvendor. The outcome ξ ≥0 denotes the realized demand for an item over a selling
period. The action is an inventory level a ≥0. The parameter θ = q ∈(0, 1) defines a reward
function
rθ(a, ξ) = −
 q[ξ −a]+ + (1 −q)[a −ξ]+
,
where [z]+ := max{z, 0} for all z ∈R. Here, the reward is the negative of a newsvendor loss
function in which the unit cost of understocking inventory (leading to [ξ −a]+ > 0) is q and
the unit cost of overstocking inventory (leading to [a −ξ]+ > 0) is 1 −q.
For each problem, we define the ground-truth distribution F based on real dataset(s). The optimal
action a∗depends on both the known instance parameters θ and the unknown F.
3.1
Generation Methods and Baselines
We estimate the unknown F using a distribution bF generated by an LLM, which can be prompted
in different ways and given varying levels of information. We compare against baselines that are
given similar levels of information. The exact prompts and baselines are problem-dependent and
deferred to Sections 4 to 6 for assortment, pricing, and newsvendor respectively. Here we only give
a high-level taxonomy of the different prompting methods and baselines, noting that we capture
all of the prompting methods described in Meister et al. (2025, §3.1–3.2).
In terms of information, we always give the LLM background about where the ground-truth
distribution F came from, including description of product(s) and geographical location of cus-
tomers. For the pricing and newsvendor problems which have real-valued outcomes, we give the
LLM numerical values to anchor on, but give the same anchors to the random baseline that is
compared against. In the Few-shot setting, the LLM is given additional information in the form
of examples, either of outcomes similar to those in F, or of distributions similar to F.
Orthogonal to the information setting, we test the following ways of prompting an LLM to
generate a distribution.
1. Sampling. The LLM is prompted to generate a single outcome ξ many times independently,
and then these ξ’s are combined to form the estimated distribution bF.
2. Persona-sampling. Same as sampling, but the LLM is prompted to imitate a different
customer “persona” each time (e.g. if the persona is “rich person”, then the LLM might
generate a high willingness-to-pay ξ). We use a collection of personas that resembles, but
is not identical to, the true characteristics of customers that generated the ground-truth
distribution F.
3. Batch-generation. The LLM is prompted to generate a batch of outcomes ξ using a single
query; bF is the uniform distribution over this batch of outcomes.
4. Description. The LLM is asked to describe the distribution bF, where the prompt is problem-
dependent.
We compare against the following baselines, which do not employ an LLM but may be given
additional information.
• Random. A distribution that consists of uniformly random outcomes, drawn over a reasonably-
anchored numerical range if the outcome is real-valued.
• d real data. A distribution that consists of d samples drawn IID from the true distribution
F.
5


--- Page 6 ---
For the assortment problem, we also compare against a baseline that gives each item a univariate
score (see Subsection 4.2).
3.2
Metrics
There are different ways of measuring how far off the estimated distribution bF is from the true
distribution F. We consider the following (asymmetric) distance metrics between F and bF.
Decision-aware metrics, via competitive ratios.
Given the LLM-SAA decision ˆa, we ulti-
mately care about Rθ(ˆa), the expected reward of our decision ˆa on the true distribution F. We
compare Rθ(ˆa) against Rθ(a∗), which is the expected reward of an optimal action a∗knowing the
ground-truth distribution F, by measuring the competitive ratio
Cθ(F, bF) := min{|Rθ(a∗)|, |Rθ(ˆa)|}
max{|Rθ(a∗)|, |Rθ(ˆa)|}.
(1)
For problems with positive reward functions (assortment, pricing), we have Cθ(F, bF) = Rθ(ˆa)/Rθ(a∗),
while for problems with negative rewards (newsvendor), we have Cθ(F, bF) = Rθ(a∗)/Rθ(ˆa). In both
cases, Cθ(F, bF) ≤1, with bigger competitive ratios being better and indicating that optimizing on
bF leads to a good decision on F.
Given F and bF, the evaluation Cθ(F, bF) still depends on the instance parameters θ. We measure
both worst-case and average-case competitive ratios over θ, where Θ denotes the parameter space
and Π denotes a distribution over Θ:
WorstCR(F, bF) := inf
θ∈Θ Cθ(F, bF),
AvgCR(F, bF) := Eθ∼Π
h
Cθ(F, bF)
i
.
Measuring WorstCR(F, bF) is difficult in general, but we show how to compute it for our three
specific problems of interest—assortment, pricing, and newsvendor. We formulate the problem as
minimizing (1) subject to
a∗∈argmaxa∈AθRθ(a); ˆa ∈argmaxa∈Aθ bRθ(a),
(2)
where θ ∈Θ and a∗, ˆa ∈Aθ are treated as decision variables while the optimality conditions (2) are
treated as constraints. Under this formulation of WorstCR(F, bF), tiebreaking favors the adversary,
as they can select the ˆa ∈argmaxa∈Aθ bRθ(a) with lowest value of Rθ(ˆa).
By contrast, when
measuring AvgCR(F, bF), we fix an arbitrary rule for selecting ˆa given problem parameters θ.
Decision-agnostic metrics.
We also report standard distributional distances between F and bF
that do not depend on the decision problem. We consider the Wasserstein distance
Wasserstein(F, bF) :=
inf
G∈Γ(F, bF)
E(ξ,ˆξ)∼G

∥ξ −ˆξ∥

,
where Γ(F, bF) denotes the set of couplings with marginals F and bF. For pricing and newsvendor,
we let ∥ξ −ˆξ∥:= |ξ −ˆξ|, and for assortment, we let ∥ξ −ˆξ∥denote the Kendall-tau distance between
rankings.
6


--- Page 7 ---
For pricing and newsvendor which have real-valued outcomes, we define F(z) := Prξ∼F [ξ ≤z]
to be the CDF, and additionally report the Kolmogorov distance
Kolmogorov(F, bF) := sup
ξ∈R
F(ξ) −bF(ξ)
.
Finally, for the assortment and pricing problems, we also consider a metric for how well the LLM
is able to imitate specific customer personas (see Subsections 4.3 and 5.2).
4
Assortment problem: Details and Results
Dataset.
We use the SUSHI Preference Data Sets, Dataset A (Kamishima, 2016), collected via
a questionnaire survey of respondents in Japan. Each respondent provides a strict ranking over
n = 10 sushi items. We consider a single ground-truth F defined by taking the uniform distribution
over the first 600 respondents’ rankings.
Details of LLM generation methods and baselines.
As background information, we always
give the LLM a description of each sushi, and tell it that the respondents are from Japan. In the
Few-shot setting, we additionally give the LLM 6 randomly-drawn rankings of respondents outside
the first 600 (i.e., not lying in F). Orthogonally, we consider all 4 ways of prompting described
in Subsection 3.1, noting that Persona-sampling gives the LLM the gender, age group, current
residence, and childhood residence of a respondent outside F; Batch-generation uses a batch size
of 30; and Description asks the LLM to generate a score for each sushi, based on which it generates
bF using a Plackett-Luce model (Plackett, 1975). Exact prompts are provided in section H.
Here, the Random baseline is composed of 600 rankings drawn uniformly among all n! per-
mutations. We also compare against a baseline that computes a univariate Popularity score for
each item based on the true F. Specifically, for each sushi j ∈[n] we define
Sj := Rθ({j}) = Eξ∼F


n
X
p=1
rp1(ξp = j)

,
(3)
which is the expected reward earned from offering only j. The baseline outputs the feasible assort-
ment a ∈Aθ that maximizes the total score P
j∈a Sj, which is quite a strong baseline in that it is
based on the ground truth F.
Further problem details.
We consider feasible families Aθ defined by “knapsack” constraints,
where each item j ∈[n] has a size sj ≥0, and feasible assortments have total size bound by some
budget. We express the budget as a multiple B of the average size 1
n
Pn
j=1 sj, so formally, we have
Aθ = {a ⊆[n] : P
j∈a sj ≤B 1
n
Pn
j=1 sj}. We interpret B as the average number of items that
would fit in the knapsack.
Unless otherwise stated, we use position-dependent rewards rp = 10/p for all p ∈[n].
In
Section D, we additionally report results for alternative reward sequences (rp)p∈[n]; across those
tables, the only change is the choice of (rp).
For AvgCR(F, bF), we consider three regimes of how the sizes (sj)j∈[n] are drawn, which induce
different distributions Π over the parameters:
1. Unit: sj = 1 ∀j ∈[n];
7


--- Page 8 ---
2. Random: sj is drawn independently and uniformly from [0.8, 1.2] ∀j ∈[n], repeating 100
times so that Π is a distribution over 100 random instances;
3. Hard: sj = Sj ∀j ∈[n], where Sj is the popularity score defined in (3).
For each of the above regimes, we use two different budgets B ∈{2, 5}. The third regime is
hard because the size of an item is exactly equal to its popularity score; hence a simple strategy of
including popular items may not perform well.
Theoretical result.
We show how to efficiently compute WorstCR(F, bF) = inf(sj)j∈[n],B Rθ(ˆa)/Rθ(a∗),
for any fixed position-dependent rewards (rp)p∈[n]. Note that the functions Rθ(·), bRθ(·) are fully
fixed based on (rp)p∈[n], so we can sort the ratios Rθ(ˆa)/Rθ(a∗) over all pairs a∗, ˆa ⊆[n], and
find the pair with the smallest ratio for which there exists Aθ (constructed from (sj)j∈[n], B) such
that (2) holds. Clearly, a pre-requisite to (2) holding is that
Rθ(a∗) ≥Rθ(ˆa); bRθ(ˆa) ≥bRθ(a∗).
(4)
Lemma 1. Let a∗, ˆa ⊆[n] be assortments for which (4) holds. If min{|a∗\ ˆa|, |ˆa \ a∗|} ≤1, then
there exist sizes (sj)j∈[n] and a budget B such that only the sets a∗, ˆa can be maximal in Aθ, and
hence (2) can also hold. Otherwise, Aθ must contain maximal sets other than a∗, ˆa.
Proof.
First suppose min{|a∗\ ˆa|, |ˆa \ a∗|} ≤1. Without loss, let |a∗\ ˆa| ≤1. Set B = 1. For
all j ∈[n], set
cj =











0,
if j ∈a∗∩ˆa;
2,
if j /∈a∗∪ˆa;
1,
if j ∈a∗\ ˆa;
1/n,
if j ∈ˆa \ a∗.
Under the resulting Aθ, both a∗and ˆa is feasible (because |a∗\ ˆa| ≤1). Moreover, elements outside
a∗∪ˆa can never be included. Thus, for a set a /∈{a∗, ˆa} to be maximal, it needs to contain elements
from both a∗\ ˆa and ˆa \ a∗. But any such set would have total size exceeding 1, completing the
proof that only the sets a∗, ˆa can be maximal in Aθ.
Conversely, suppose min{|a∗\ ˆa|, |ˆa \ a∗|} ≥2. For distinct items j1, j2 ∈a∗\ ˆa with sj1 ≤sj2,
we know sj1 + sj2 + P
j∈a∗∩ˆa sj ≤B, and hence 2sj1 + P
j∈a∗∩ˆa sj ≤B. Similarly, for distinct
items j3, j4 ∈ˆa \ a∗with sj3 ≤sj4, we know 2sj3 + P
j∈a∗∩ˆa sj ≤B. This collectively implies
sj1 + sj3 + P
j∈a∗∩ˆa sj ≤B, and hence {j1, j3} ∪(a∗∩ˆa) (or a set containing it) must be maximal
in Aθ.
Theorem 1 implies that for any a∗, ˆa satisfying (4) and min{|a∗\ ˆa|, |ˆa \ a∗|} ≤1, we can
immediately show the existence of Aθ for which (2) holds, because maxa∈Aθ is always achieved at
a maximal set (given that Rθ, bRθ are monotonically increasing in a).
Equipped with Theorem 1, our algorithm iterates2 over a∗, ˆa ⊆[n]. If (4) holds and Rθ(ˆa)/Rθ(a∗)
undershoots the smallest ratio found so far, both of which are easily checkable, then we see if there
exists Aθ for which (2) holds. By Theorem 1, the answer is immediately Yes if min{|a∗\ˆa|, |ˆa\a∗|} ≤
1. Otherwise, we show how to write a Linear Program (LP) to check if (2) can hold. Details are
found in Section A, where we also present some tricks to prune the search over a∗, ˆa ⊆[n].
2Our algorithm requires enumerating all 2n assortments, leading to exponential runtime (reasonable in our setting
with n = 10). Without our algorithm, enumerating all possible feasible families Aθ would take doubly-exponential
runtime.
8


--- Page 9 ---
AvgCR
Unit
 B=2
AvgCR
Unit
B=5
AvgCR
Random
B=2
AvgCR
Random
B=5
AvgCR
Hard
B=2
AvgCR
Hard
B=5
WorstCR
Wasserstein
0.0
0.2
0.4
0.6
0.8
1.0
CR
LLM: Sampling
LLM: Persona-sampling
Baseline: Random
Baseline: Popularity score
Baseline: 5 real data
Baseline: 15 real data
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Wasserstein
Figure 1:
Assortment results, displaying means across 20 generations, with 95% confidence in-
tervals around the mean. Higher is better for the CR metrics while lower is better for the
Wasserstein metric. The LLM is GPT-4o.
4.1
Assortment problem: Empirical Results
We compare 8 generation methods (2 information settings × 4 ways of prompting) across 4 LLM
models (GPT-4o, GPT-5-mini, Gemini 3 Flash, Mistral Large 3), and 5 baseline methods (Random,
3 real data sizes, Popularity score), for generating an estimated distribution bF. We note that some
methods are random and produce a different bF each time, so we report averages over multiple bF’s.
Full generation details and results are deferred to Section D.
Figure 1 summarizes the findings, where we compare representative LLM generation methods to
baselines, across decision-aware and decision-agnostic metrics. We see that the LLM-SAA methods
consistently outperform the Random baseline under the decision-aware competitive ratio (CR)
metrics, indicating that the LLM’s generated rankings based on world knowledge are definitely
better than a random guess, when it comes to making assortment decisions. This pattern robustly
extends to other generation methods and LLM models (see Table 1 for full results), noting that
additional information (personas, few-shot examples) is not necessary for the LLM to beat the
Random baseline. Meanwhile, 15 real data provide a strong baseline, beatable only by select LLM
methods.
We remark that WorstCR provides a fairly concordant “ranking” of methods in terms of how
well they would perform across different instance parameters, as captured in the AvgCR metrics.
On the other hand, the decision-agnostic Wasserstein metric paints a very different picture: the
Random baseline actually has the best (smallest) Wasserstein distance! This highlights a central
takeaway of our paper: standard distributional distances can be a very poor proxy for downstream
decision quality in operational problems.
4.2
Popularity score can be insufficient
The Popularity score baseline based on the ground-truth F is generally hard to beat, but as we
see from Figure 1, it performs poorly in the Hard regime, where each item’s size exactly equals its
popularity score. In this regime, it is not sufficient to simply select popular items, since popular
items are the largest and hence penalized by the budget constraint.
More broadly, a single scalar score per item cannot capture substitution and interaction effects
across items, which are crucial for determining a good assortment. For example, even if sushi’s j
9


--- Page 10 ---
and j′ are the most popular overall, if all customers rank j above j′, then once j is offered, adding
j′ provides no incremental value, since j′ is never chosen when j is available. Another example
is the existence of “polarizing” items that tend to be ranked either first or last. The fact that
LLM-generated ranking distributions outperform the Popularity score baseline in the Hard regime
suggests that they encode some information about these subtle interaction effects, rather than only
reproducing marginal popularity. That being said, they are generally unable to beat this baseline
in the other regimes, as the LLMs are unable to perfectly predict popularity.
4.3
Is the LLM able to imitate specific personas?
We showed that the LLM can generate good aggregate preference distributions; and in particular
Persona-sampling performed very well. In Persona-sampling, we prompt the LLM to generate
a ranking by conditioning on an individual’s observed features, like in the literature discussed in
Subsection 2.1. In this section, we check how well the LLM is imitating personas at the individual
level by comparing predicted rankings ˆξi to the true rankings ξi for each individual i.
To quantify this, we compute two metrics:
• PersonaMAE(F, bF) := 1
m
Pm
i=1 ∥ξi −ˆξi∥: the average distance between the LLM’s prediction
for a persona and that specific individual’s true preference;
• ShuffledMAE(F, bF) :=
1
m2
Pm
i=1
Pm
i′=1 ∥ξi −ˆξi′∥: the average distance between the LLM’s
prediction for a persona and the preference of a randomly selected individual from the pop-
ulation.
If the LLM accurately tailors rankings to each individual, we would expect PersonaMAE to
be substantially smaller than ShuffledMAE.
Instead, we find that these metrics are essentially
identical: 0.367 for PersonaMAE and 0.369 for ShuffledMAE. (For comparison, the ShuffledMAE
of the Random baseline is 0.497.)
Despite this failure at the individual level, Persona-sampling achieves the highest CR metrics.
These results imply that high-fidelity “human simulation” is not a prerequisite to adding value
for downstream decision-making. While existing literature often focuses on the accuracy of the
individual imitation, our results show that persona-steering can be valuable because it encourages
the LLM to represent the broader diversity and range of the population distribution. In summary,
an LLM-generated distribution ˆF can be valuable for making good decisions even if it cannot
accurately predict which specific individual holds which preference.
5
Pricing problem: Details and Results
Dataset.
We use data of willingness-to-pay for cocoa liquor in the Philippines, from Ballesteros
et al. (2023). To elaborate, respondents bid premiums that they were willing to pay for 6 different3
“upgraded” cocoa liquor products in truthful second-price auctions. The basic cocoa liquor product
had a known market price of PhP 44.00, and customer willingness-to-pay for the upgraded products
is interpreted to equal 44 plus the premium they bid. For each of the 6 products, we construct a
ground-truth F by taking the uniform distribution over the first 100 respondents’ willingness-to-pay.
3Technically it’s 3 different products with 2 different labeling conditions (award vs. origin labels).
10


--- Page 11 ---
Details of LLM generation methods and baselines.
As background information, we always
tell the LLM the basic product description and its price of PhP 44.00, the award or origin label of
the upgraded product, and importantly, the range of premiums bid (0 to 100). In the Few-shot
setting, we additionally give the LLM 6 randomly-drawn willingness-to-pay of respondents outside
F. Orthogonally, we consider all 4 ways of prompting described in Subsection 3.1, noting that
Persona-sampling gives the LLM detailed characteristics of a respondent outside F; Batch-
generation uses a batch size of 25; and Description asks the LLM for 5 plausible willingness-to-
pay values and how it would split the probability mass among them. Exact prompts are provided
in Section I. Here, the Random baseline is the uniform distribution over [44,144].
Theoretical result: computing WorstCR(F, bF).
The problem can be formulated as minimizing
Rc(ˆa)/Rc(a∗) over decision variables c, a∗, ˆa ≥0, subject to (2). We show how to solve this efficiently
and precisely assuming F and bF have finite non-negative supports denoted by supp(F) and supp( bF),
respectively.
Lemma 2. For any cost c, all maximizers of Rc(a) = (a −c) Prξ∼F [ξ ≥a] lie within supp(F) or
have a = ∞(interpreted as setting a price a > max{supp(F)} so that Prξ∼F [ξ ≥a] =⇒Rc(a) = 0).
Analogously, argmaxa≥0 bRc(a) ⊆supp( bF) ∪{∞}.
Proof.
Let F be a discrete distribution supported on supp(F) = {ξ1, . . . , ξm}, where 0 ≤ξ1 <
· · · < ξm, and ξ0 is understood to be 0. Without loss of generality, restrict a > 0. First suppose
a ∈(ξn, ξn+1] for some n ∈{0, . . . , m −1}. We know Prξ∼F [ξ ≥a] is constant on this interval,
hence argmaxa∈(ξn,ξn+1]Rc(a) = ξn+1. Next, when a > max{supp(F)}, Prξ∼F [ξ ≥a] = 0, then
a can be set to ∞. Thus, argmaxa≥0Rc(a) ⊆supp(F) ∪{∞}. The result for argmaxa≥0 bRc(a) is
proved analogously based on bF.
Therefore, constraint (2) is equivalent to
a∗∈argmaxa∈supp(F)∪{∞}Rc(a);
ˆa ∈argmaxa∈supp( bF)∪{∞} bRc(a).
Now, because Rc(a) is an affine function in c for each fixed price a, we have that maxa∈supp(F)∪{∞} Rc(a)
is piecewise-linear in c, defined by breakpoints 0 =: c1 < · · · < cn, and that each a ∈supp(F)∪{∞}
lies in the argmax over an interval defined by [ci, ci′] for i, i′ ∈[n] (that can be empty if i′ < i,
or a single point if i′ = i). We can make analogous statements about bRc(a) having breakpoints
0 =: ˆc1 < · · · < ˆcˆn.
Lemma 3. Given fixed values of a∗and ˆa, the function Rc(ˆa)/Rc(a∗) is monotonic in c.
Proof.
Fix a∗, ˆa ≥0 and write D∗:= Prξ∼F [ξ ≥a∗], ˆD := Prξ∼F [ξ ≥ˆa]. Then
Rc(ˆa)
Rc(a∗) =
(ˆa −c) ˆD
(a∗−c)D∗= α + βc
γ + δc ,
where α := ˆa ˆD, β := −ˆD, γ := a∗D∗, δ := −D∗. On any interval where γ + δc ̸= 0, the derivative
satisfies
d
dc
α + βc
γ + δc

= βγ −αδ
(γ + δc)2 ,
11


--- Page 12 ---
whose sign is constant. Hence, Rc(ˆa)/Rc(a∗) is monotone in c (or constant in the degenerate case).
Therefore, given decision variables a∗, ˆa, c satisfying (2), if c does not lie in {ci}i∈[n] ∪{ˆci}i∈[ˆn],
then we can improve the objective Rc(ˆa)/Rc(a∗) by shifting c in either direction (by Theorem 3),
while continuing to satisfy (2) (by the definition of breakpoints). For each value of c ∈{ci}i∈[n] ∪
{ˆci}i∈[ˆn], we can directly evaluate Rc(a∗), while to evaluate Rc(ˆa) we must find the ˆa ∈argmaxa∈supp( bF)∪{∞} bRc(a)
that minimizes Rc(ˆa). Full details of this simple search algorithm for computing WorstCR(F, bF)
are in Section B.
5.1
Pricing problem: Empirical Results
We repeat the setup from assortment (see Subsection 4.1), with full generation details and results
deferred to Section E. Here there are 6 products with different distributions F, so the reported
values also take an average over these.
AvgCR [0, 32]
AvgCR [0, 66]
AvgCR [0, 100]
WorstCR
Kolmogorov
Wasserstein
0.0
0.2
0.4
0.6
0.8
1.0
CR / Kolmogorov
LLM: Sampling
LLM: Persona-sampling
Baseline: Random
Baseline: 5 real data
Baseline: 20 real data
0
5
10
15
20
Wasserstein
Figure 2:
Pricing results, displaying means across 20 generations and 6 ground truths, with 95%
confidence intervals around the mean. Higher is better for the CR metrics while lower is better
for the Kolmogorov and Wasserstein metrics. The LLM is GPT-4o.
Figure 2 summarizes the findings, where we see that LLM-SAA methods outperform the Ran-
dom baseline on the decision-aware CR metrics, but less so on the decision-agnostic Kolmogorov
and Wasserstein metrics. Here, we cap the cost parameter c at 32 for WorstCR, and consider dif-
ferent ranges of c for the AvgCR metrics. Higher costs create degenerate instances where the price
has to be set very precisely to earn any reward; this makes LLM-based methods less reliable at
outperforming the Random baseline, and never able to beat a small amount of real data (see the
full results in Table 4). Interestingly, Section F reveals that the main gain of the LLM comes from
generating willingness-to-pay numbers that end in 5 or 0, closely resembling those of humans (at
least when asked for a value).
5.2
Is the LLM able to imitate specific personas?
We compute the PersonaMAE and ShuffledMAE metrics (see Subsection 4.3) to assess how well the
LLM tailors its outputs to individual-level features. Averaged across the six products, PersonaMAE
and ShuffledMAE are 22.44 and 23.11 respectively, again reinforcing that the LLM can improve
decisions despite not imitating individuals.
(For comparison, the ShuffledMAE of the Random
baseline is 36.01.)
12


--- Page 13 ---
6
Newsvendor problem: Details and Results
Dataset.
We use sales data from a fashion retailer H&M (Ling et al., 2022). We pre-process the
data to remove items with sudden discounts or inventory stockouts (details in Section G), so that
the aggregate purchases reflect total demands under a stable price. We take all products with type
trouser, which is the largest type with 300 items after the pre-processing. For each item, we take
their weekly demand over 30 weeks, and use this empirical distribution to form F. Each item is
associated with metadata such as the product name, type, and color (details in Section G).
Details of LLM generation methods and baselines.
We apply LLM-SAA using the De-
scriptive generation method with Few-shot examples of other distributions. Specifically, for a
target item, the LLM is asked predict the mean and standard deviation for a normal distribution
corresponding to the item’s weekly demand. The LLM is provided with the target item’s metadata,
and also a reference set of 100 other example items, including the metadata and historical demand
parameters (mean and standard deviation) for each reference item.
We do not employ the other generation methods because this application is structurally different
than the previous settings. In assortment and pricing, each data point represented the choice or
valuation of a single individual, hence those settings were aligned with the literature on using
LLMs to simulate humans, which motivates the sampling-based methods. In newsvendor, a single
observation is a weekly demand realization for an item, which makes the “human simulation”
paradigm less applicable, and hence we omit the agent-based sampling methods.
We compare against a Random baseline which uses the empirical distribution of the demands
of all 300 items.
Theoretical result: computing WorstCR(F, bF).
The problem can be formulated as minimizing
Rq(a∗)/Rq(ˆa) over decision variables q ∈(0, 1) and a∗, ˆa ≥0, subject to (2). We show how to solve
this efficiently and precisely assuming F and bF are discrete uniform distributions over m and bm
real numbers, respectively.
Lemma 4. If a∗∈argmaxa≥0Rq(a) for some q ∈(i−1
m , i
m) and i ∈[m], then a∗continues to lie
in the argmax for all q ∈[i−1
m , i
m]. Analogously, if ˆa ∈argmaxa≥0 bRq(a) for some q ∈(i−1
bm , i
bm) and
i ∈[ bm], then ˆa lies in the argmax for all q ∈[i−1
bm , i
bm].
Proof.
Let F be uniform over values ξ1, . . . , ξm where 0 ≤ξ1 ≤· · · ≤ξm. It can be checked
that
argmaxa≥0Rq(a) =
(
ξi,
if q ∈(i−1
m , i
m), i ∈[m];
[ξi, ξi+1],
if q =
i
m, i ∈[m] ∪{0};
where ξ0 is understood to be 0 and ξm+1 is understood to be ∞.
From this the result about
argmaxa≥0Rq(a) follows. The result for argmaxa≥0 bRq(a) is proved analogously based on bF.
Lemma 5. Given fixed values of a∗and ˆa, the function Rq(a∗)/Rq(ˆa) is monotonic in q.
Proof.
Fix a∗, ˆa ≥0 and define U∗:= E[[ξ −a∗]+], O∗:= E[[a∗−ξ]+], ˆU := E[[ξ −ˆa]+], and
ˆO := E[[ˆa −ξ]+] to be the expected understockings and overstockings of a∗and ˆa respectively.
Then
Rq(a∗)
Rq(ˆa) = qU∗+ (1 −q)O∗
q ˆU + (1 −q) ˆO
= O∗+ q(U∗−O∗)
ˆO + q( ˆU −ˆO)
= α + βq
γ + δq ,
13


--- Page 14 ---
where α = O∗, β = U∗−O∗, γ = ˆO, δ = ˆU −ˆO. On any interval where γ + δq ̸= 0, the derivative
is
d
dq
α + βq
γ + δq

= βγ −αδ
(γ + δq)2 ,
whose sign is constant; hence the ratio is monotone in q (or constant in the degenerate case).
Given decision variables a∗, ˆa, q satisfying (2), if q is a multiple of neither 1/m nor 1/ bm, then
we can improve the objective Rq(a∗)/Rq(ˆa) by shifting q in either direction (by Theorem 5), while
continuing to satisfy (2) (by Theorem 4). Then, we can restrict the search to q ∈{ 1
m, . . . , m−1
m } ∪
{ 1
bm, . . . , bm−1
bm }. For each such value of q, we must find the ˆa ∈argmaxa≥0 bRq(a) that minimizes
Rq(ˆa). It can be shown that argmaxa≥0 bRq(a) is an interval and Rq(·) is unimodal, and hence it
suffices to check the endpoints of the interval. Full algorithm details are in Section C.
6.1
Newsvendor problem: Empirical Results
AvgCR
WorstCR
Kolmogorov
Wasserstein
0.0
0.2
0.4
0.6
CR / Kolmogorov
GPT-4o
GPT-5-mini
Gemini 3 Flash
Mistral Large 3
Baseline: Random
0
10
20
30
40
Wasserstein
Figure 3:
Newsvendor results across four LLM models, displaying means across 300 items with
95% confidence intervals. Higher is better for the CR metrics while lower is better for the
Kolmogorov and Wasserstein metrics. The AvgCR uses the distribution q ∼Unif[0.01, 0.99].
In general we follow the same evaluation protocol as in the previous sections, comparing decision-
aware CR metrics against decision-agnostic distributional distances. Figure 3 reports results for the
CR metrics and decision-agnostic distributional distances for four LLM models, averaging across
the 300 items.
We see that LLM-SAA performs similarly to the Random baseline for AvgCR
for all models, as all confidence bars are overlapping, but LLM-SAA improves upon WorstCR
substantially. The two decision-unaware distances point in opposite directions (Random baseline is
better under Kolmogorov, but worse under Wasserstein), suggesting that decision-unaware metrics
can be unreliable indicators of downstream decision quality.
7
Concluding Discussion
When no historical data is available, LLM-SAA uses an LLM to construct an estimated distribution
bF and then optimizes the downstream decision under bF using SAA (see Subsection 2.2). Evaluating
the performance of LLM-SAA is subtle, and requires decision-aware analysis of how well optimizing
on bF performs under the ground truth F. Our approach to making this evaluation robust is to
sweep over other parameters in the optimization problem, in both a worst- and average- case sense,
for fixed bF and F.
Our work flows into an emerging stream on using LLMs to make business-relevant decisions, and
makes progress toward the important goal of benchmarking LLM progress at this task. More specif-
14


--- Page 15 ---
ically, we evaluate how well LLMs can use their world knowledge to generate data/distributions
that feed into an SAA optimizer. The performance is already highly promising, and we emphasize
that we did not optimize prompts or LLM models—our evaluation metrics can be used to prompt
and train LLMs to generate better distributions for decision-making in the future.
References
G. V. Aher, R. I. Arriaga, and A. T. Kalai. Using large language models to simulate multiple
humans and replicate human subject studies. In International conference on machine learning,
pages 337–371. PMLR, 2023.
L. P. Argyle, E. C. Busby, N. Fulda, J. R. Gubler, C. Rytting, and D. Wingate. Out of one, many:
Using language models to simulate human samples. Political Analysis, 31(3):337–351, 2023.
A. Backlund and L. Petersson.
Vending-bench: A benchmark for long-term coherence of au-
tonomous agents, Feb. 2025. URL https://arxiv.org/abs/2502.15840.
J. F. Ballesteros, J. J. Schouteten, A. Otilla, R. I. Ramirez, X. Gellynck, J. Casaul, and H. De Steur.
Does award and origin labeling influence consumers’ willingness-to-pay beyond sensory cues? an
experimental auction on improved philippine tablea (cocoa liquor). Journal of Behavioral and
Experimental Economics, 102:101965, 2023.
A. Ben-Tal, L. El Ghaoui, and A. Nemirovski. Robust Optimization. Princeton University Press,
2009.
A. Borodin and R. El-Yaniv. Online Computation and Competitive Analysis. Cambridge University
Press, 1998.
J. Brand, A. Israeli, and D. Ngwe.
Using llms for market research.
Harvard business school
marketing unit working paper, (23-062), 2023.
S. Cao and M. Hu. Llm collusion. arXiv preprint arXiv:2601.01279, 2026.
M. C. Cohen, T. Dai, G. Perakis, N. Agrawal, G. Allon, R. N. Boute, G. P. Cachon, Z. Chen, M. A.
Cohen, R. Cristian, et al. Supply chain management in the ai era: A vision statement from the
operations management community. 2025.
D. Dillion, N. Tandon, Y. Gu, and K. Gray. Can ai language models replace human participants?
Trends in Cognitive Sciences, 27(7):597–600, 2023.
Y. Duan, Y. Hu, and J. Jiang. Ask, clarify, optimize: Human-llm agent collaboration for smarter
inventory control. arXiv preprint arXiv:2601.00121, 2025.
V. F. Farias, S. Jagabathula, and D. Shah. A nonparametric approach to modeling choice with
limited data. Management Science, 59(2):305–322, 2013. doi: 10.1287/mnsc.1120.1625.
Y. Gao, D. Lee, G. Burtch, and S. Fazelpour. Take caution in using llms as human surrogates.
Proceedings of the National Academy of Sciences, 122(24):e2501660122, 2025.
J. J. Horton. Large language models as simulated economic agents: What can we learn from homo
silicus? Technical report, National Bureau of Economic Research, 2023.
15


--- Page 16 ---
C. Huang, Z. Tang, S. Hu, R. Jiang, X. Zheng, D. Ge, B. Wang, and Z. Wang. Orlm: A customizable
framework in training large models for automated optimization modeling. Operations Research,
2025.
T. Kamishima. SUSHI Preference Data Sets. https://www.kamishima.net/sushi/, 2016. Ac-
cessed 20 November 2025.
C. G. Ling, ElizabethHMGroup, FridaRim, inversion, J. Ferrando, Maggie, neuraloverflow, and
xlsrln.
H&m personalized fashion recommendations.
https://kaggle.com/competitions/
h-and-m-personalized-fashion-recommendations, 2022. Kaggle.
J. Liu, Z. Chen, and Y. Zhong. Large language newsvendor: Decision biases and cognitive mecha-
nisms. arXiv preprint arXiv:2512.12552, 2025.
C. Long, D. Simchi-Levi, A. P. Calmon, and F. P. Calmon.
When supply chains be-
come autonomous.
Harvard Business Review, Dec. 2025.
URL https://hbr.org/2025/12/
when-supply-chains-become-autonomous. Accessed: 2026-01-31.
B. S. Manning, K. Zhu, and J. J. Horton. Automated social science: Language models as scientist
and subjects. Working Paper 32381, National Bureau of Economic Research, 2024.
N. Meister, C. Guestrin, and T. B. Hashimoto. Benchmarking distributional alignment of large
language models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter
of the Association for Computational Linguistics: Human Language Technologies (Volume 1:
Long Papers), pages 24–49, 2025.
J. S. Park, J. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Generative agents:
Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on
user interface software and technology, pages 1–22, 2023.
R. L. Plackett. The analysis of permutations. Journal of the Royal Statistical Society: Series C
(Applied Statistics), 24(2):193–202, 1975. doi: 10.2307/2346567.
Y. Qu and J. Wang.
Performance and biases of large language models in public opinion sim-
ulation.
Humanities and Social Sciences Communications, 11:1095, 2024.
doi:
10.1057/
s41599-024-03609-x.
T. Roughgarden. Beyond the worst-case analysis of algorithms. Cambridge University Press, 2021.
V. Samuel, H. P. Zou, Y. Zhou, S. Chaudhari, A. Kalyan, T. Rajpurohit, A. Deshpande,
K. Narasimhan, and V. Murahari. Personagym: Evaluating persona agents and llms. arXiv
preprint arXiv:2407.18416, 2024.
M. Sarstedt, S. J. Adler, L. Rau, and B. Schmitt. Using large language models to generate sil-
icon samples in consumer and marketing research: Challenges, opportunities, and guidelines.
Psychology & Marketing, 41(6):1254–1270, 2024.
A. Shapiro, D. Dentcheva, and A. Ruszczy´nski. Lectures on Stochastic Programming: Modeling
and Theory. SIAM, 2 edition, 2014.
D. Simchi-Levi, K. Mellou, I. Menache, and J. Pathuri. Large language models for supply chain
decisions. pages 93–104, 2026.
16


--- Page 17 ---
J. Suh, E. Jahanparast, S. Moon, M. Kang, and S. Chang. Language model fine-tuning on scaled
survey data for predicting distributions of public opinions. arXiv preprint arXiv:2502.16761,
2025.
M. Sumida, G. Gallego, P. Rusmevichientong, H. Topaloglu, and J. Davis. Revenue-utility tradeoff
in assortment optimization under the multinomial logit model with totally unimodular con-
straints. Management Science, 67(5):2845–2869, 2021. doi: 10.1287/mnsc.2020.3657.
K. T. Talluri and G. J. van Ryzin. The Theory and Practice of Revenue Management. Springer,
2004.
O. Toubia, G. Z. Gui, T. Peng, D. J. Merlau, A. Li, and H. Chen. Database report: Twin-2k-500:
A data set for building digital twins of over 2,000 people based on their answers to over 500
questions. Marketing Science, 44(6):1446–1455, 2025.
H. Wang, G. Chen, K. Talluri, and X. Li. Omgpt: A sequence modeling framework for data-driven
operational decision making. arXiv preprint arXiv:2505.13580, 2025.
M. Wang, D. J. Zhang, and H. Zhang.
Large language models for market research: A data-
augmentation approach. arXiv preprint arXiv:2412.19363, 2024.
Q. Xie, Q. Feng, T. Zhang, Q. Li, L. Yang, Y. Zhang, R. Feng, L. He, S. Gao, and Y. Zhang.
Human simulacra: Benchmarking the personification of large language models. arXiv preprint
arXiv:2402.18180, 2024.
C. Zhou, J. Yang, L. Xin, Y. Chen, Z. He, and D. Ge. Auto-formulating dynamic programming
problems with large language models. arXiv preprint arXiv:2507.11737, 2025.
17


--- Page 18 ---
A
Computing WorstCR(F, bF) for Assortment
To find WorstCR(F, ˆF), we enumerate candidate ratios only over pairs (a∗, ˆa) that satisfy (4) to
check whether (2) holds, which allows us to discard nearly half of all candidate pairs. Furthermore,
here is a pruning trick for not checking all the valid pairs of (4): we perform a double-loop search,
with Rθ(ˆa) ordered increasingly and Rθ(a∗) ordered decreasingly. For a fixed numerator Rθ(ˆa), we
iterate over candidate denominators Rθ(a∗) and check whether there exists a parameter θ such that
(2) holds. Once such a θ is found, we terminate the inner loop for this ˆa, since under the current
enumeration order any subsequent denominator would yield a larger ratio and cannot improve the
result. If the corresponding ratio Rθ(ˆa)/Rθ(a∗) is strictly smaller than the current WorstCR, we
update it accordingly, breaking ties by choosing the pair with the smaller Rθ(ˆa). Figure4 illustrates
this pruning procedure.
Figure 4: Illustration of the enumeration and pruning procedure, in our special case where n =
10 and hence there are 1023 non-empty assortments.
Rows correspond to Rθ(ˆa) and columns
correspond to Rθ(a∗) , while each cell represents a ratio Rθ(ˆa)/Rθ(a∗). Assortments are ordered
by Rθ(·), with ˆa1 and a∗
1 denoting the smallest rewards in their respective orders. We traverse only
the red and green cells; light-blue cells violate (2) and are pruned. The boundary is illustrative and
not necessarily diagonal.
For validity checking, if min{|a∗\ ˆa|, |ˆa \ a∗|} ≤1, then by Lemma1, (2) holds. If min{|a∗\
ˆa|, |ˆa \ a∗|} > 1, a candidate ratio can be invalid if there exists an assortment ˆabad that satisfies
Rθ(abad) > Rθ(a∗) or ˆRθ(abad) > ˆRθ(ˆa) and there do not exist sizes (sj)j∈[n] and a budget B that
can exclude all “bad” assortments from Aθ. In this case,(2) can’t hold and the corresponding ratio
Rθ(ˆa)/Rθ(a∗) cannot actually be realized by the adversary.
To determine whether such sizes (sj)j∈[n] and a budget B that can exclude all “bad” assortments
exist, we formulate a linear feasibility program:
18


--- Page 19 ---
n
X
i=1
si xi(a∗) ≤B,
n
X
i=1
si xi(ˆa) ≤B,
n
X
i=1
si xi(abad) > B,
∀abad
si ≥0,
i = 1, . . . , n.
Here x(a) ∈{0, 1}n is the indicator vector of assortment a (xi(a) = 1 if item i is included, and
0 otherwise).
The overall algorithm is given in Algorithm 1.
B
Computing WorstCR(F, bF) for Pricing
The overall algorithm is given in Algorithm 2.
C
Computing WorstCR(F, bF) for Newsvendor
The overall algorithm is given in Algorithm 3.
19


--- Page 20 ---
Algorithm 1: Worst-Case Competitive Ratio Search for Assortment
Require: Ground-truth distribution F and estimation distribution bF
Ensure: WorstCR correspond to each bF
1: Step 1: Compute rewards and sort subsets
2: for F, bF do
3:
for each subset atemp ⊆[n] do
4:
Compute reward R(a∗
temp), R(ˆatemp);
5:
end for
6: end for
7: Sort all subsets in ascending order of R(a∗
temp) and R(ˆatemp) to obtain list
L∗= {a∗
1, a∗
2, . . . , a∗
1023} and Lˆa = {ˆa1, ˆa2, . . . , ˆa1023}.
8: Step 2: Find WorstCR
9: for each bF do
10:
Initialize lower bound ←1.
11:
for ˆai in Lˆa from smallest reward to largest do
12:
for a∗
j in L∗from largest reward to smallest do
13:
Compute ratio ρ = R(ˆai)/R(a∗
j);
14:
if ρ ≥lower bound then
15:
continue.
16:
end if
17:
if
bRθ(ˆai) < bRθ(a∗
j) then
18:
continue.
19:
end if
20:
if min{|a∗
j \ ˆai|, |ˆai \ a∗
j|} ≤1 then
21:
Update lower bound ←ρ.
22:
break.
23:
else
24:
Initialize set of abad ←∅.
25:
for atemp ⊆P(a∗
j ∪ˆai) do
26:
if R(atemp) > R(a∗) or bR(atemp) > bR(ˆa) then
27:
Add atemp to the set of abad
28:
end if
29:
end for
30:
for abad in the set of abad do
31:
if LP is feasible then
32:
Update lower bound ←ρ.
33:
break.
34:
end if
35:
end for
36:
end if
37:
end for
38:
end for
39: end for
40:
41: return lower bound
20


--- Page 21 ---
Algorithm 2: Worst-Case Competitive Ratio Search for Pricing
Require: Ground-truth distribution F and estimation distribution bF.
Ensure: WorstCR(F, bF)
1: xmin ←0
2: Step1: Build affine-line families
3:
LF = {ℓa(c) = (a −c) Prξ∼F [ξ ≥a] : a ∈supp(F) ∪{∞}}.
4:
L b
F = {bℓa(c) = (a −c) Prξ∼b
F [ξ ≥a] : a ∈supp( bF) ∪{∞}}.
5: C ←{0}
6: Step2: Find Upper Envelope
7: for each family L ∈{LF , L b
F } do
8:
Stack ←∅; BreakPtr ←∅.
9:
Sort L by slope (tail prob.) descending; handle ties by keeping max intercept.
10:
for each line ℓj ∈L do
11:
if Stack is empty then
12:
push ℓj to Stack
13:
continue
14:
end if
15:
x⋆←intersect(ℓj, top(Stack))
16:
if |Stack| = 1 and x⋆≤xmin then
17:
pop Stack; push ℓj to Stack
18:
continue
19:
end if
20:
while |Stack| ≥2 do
21:
x⋆←intersect(ℓj, top(Stack))
22:
if x⋆≤last(BreakPtr) then
23:
pop Stack; pop BreakPtr
24:
else
25:
break
26:
end if
27:
end while
28:
x⋆←intersect(ℓj, top(Stack))
29:
push x⋆to BreakPtr; push ℓj to Stack
30:
end for
31:
C ←C ∪{x ∈BreakPtr | x ≥xmin}.
32:
if L = LF then
33:
StackF ←Stack; BreakPtrF ←BreakPtr
34:
else
35:
Stack b
F ←Stack; BreakPtr b
F ←BreakPtr
36:
end if
37: end for
38: Step 3: WorstCR Search
39: Sort C and remove duplicates: c1 < c2 < · · · < cm.
40: p ←1, ˆp ←1; WorstRatio ←∞.
41: for each ct ∈C do
42:
while p ≤|BreakPtrF | and BreakPtrF [p] ≤ct do
43:
p ←p + 1
44:
end while
45:
while ˆp ≤|BreakPtr b
F | and BreakPtr b
F [ˆp] ≤ct do
46:
ˆp ←ˆp + 1
47:
end while
48:
a⋆←price of StackF [p]; b
A ←{price of Stack b
F [ˆp]}
49:
if ˆp ≤|BreakPtr b
F | and ct = BreakPtr b
F [ˆp] then
50:
b
A ←b
A ∪{ price of Stack b
F [ˆp + 1] }
51:
end if
52:
WorstRatio ←min

WorstRatio,
mina∈b
A(a−ct) Prξ∼F [ξ≥a]
(a⋆−ct) Prξ∼F [ξ≥a⋆]

53: end for
54: return WorstRatio.
21


--- Page 22 ---
Algorithm 3: WorstCR-Newsvendor(F, bF) (discrete uniform supports)
Require: F uniform over m demand values; bF uniform over bm demand values.
Ensure: WorstCR(F, bF) over q ∈(0, 1).
1: Define rewards (negative loss):
2:
Rq(a) = −(q Eξ∼F [(ξ −a)+] + (1 −q) Eξ∼F [(a −ξ)+]),
3:
and analogously bRq(a) under bF.
4: Candidate parameters:
5:
Q ←{1/m, . . . , (m −1)/m} ∪{1/ bm, . . . , ( bm −1)/ bm}.
6: best ←+∞.
7: for each q ∈Q do
8:
Compute a⋆∈arg maxa≥0 Rq(a).
9:
Compute bA(q) = arg maxa≥0 bRq(a).
10:
Let a = min bA(q) and a = max bA(q).
11:
ba ←arg min{Rq(a), Rq(a)}.
12:
ratio ←Rq(a⋆)
Rq(ba) .
13:
best ←min(best, ratio).
14: end for
15: return best.
22


--- Page 23 ---
D
Assortment problem: Generation Details and Full Results
We consider 20 different distributions bF for each LLM generation method and each baseline, calling
them multiple times to handle the randomness in the distribution generated.
• Sampling: We generate a pool of 600 rankings using the LLM. We subsample 200 rankings
to form an estimated distribution bF, and repeat to form 20 estimated distributions in this
way.
• Persona-sampling: We generate a pool of 600 rankings, prompting the LLM with a different
persona each time. We subsample 200 rankings to form an estimated distribution bF, and
repeat to form 20 estimated distributions in this way.
• Batch-generation: We generate 30 rankings per query and repeat this process 20 times,
yielding 20 distributions. For each probability distribution, we sample 50 rankings according
to the specified proportions, resulting in 20 distributions.
• Description: We ask the LLM to generate a score vector for the sushi items, repeating 20
times. For each score vector, we use a Plackett–Luce model to sample 200 rankings, resulting
in 20 distributions.
• Few-shot Sampling: We draw 5 example sets, each containing 6 examples. For each example
set, we generate a pool of 300 rankings and construct 4 subsamples of size 200, resulting in a
total of 20 distributions.
• Few-shot Persona-sampling: Same as Few-shot Sampling, except we also prompt the LLM
with a different persona each time.
• Few-shot Batch-generation: We draw 5 example sets, each containing 6 examples. We
run Batch-generation 4 times with each example set, resulting in a total of 20 distributions.
• Few-shot Description: We draw 5 example sets, each containing 6 examples.
We run
Description 4 times with each example set, resulting in a total of 20 distributions.
• Random baseline: We generate a pool of 600 rankings uniformly at random. We subsample
200 rankings from it 20 times, to form 20 estimated distributions.
• d real data baseline: We randomly sample d rankings from the ground-truth F, generating
20 distributions in this way.
This appendix reports three versions of the assortment results table, which differ only in
the choice of the position-dependent rewards (rp)p∈[n] used in the reward function rθ(a, ξ) =
rmin{p∈[n]:ξp∈a}. Specifically, the three tables use rp = 10/p, rp = 10 −(p −1), and rp = 10 −
0.1(p −1)2, respectively.
E
Pricing problem: Generation Details and Full Results
We consider 20 different distributions bF for each LLM generation method and each baseline, calling
them multiple times to handle the randomness in the distribution generated.
• Sampling: We generate a pool of 100 willingness-to-pay values for each product using the
LLM. We subsample 50 willingness-to-pay values to form an estimated distribution bF, and
repeat to form 20 estimated distributions in this way.
23


--- Page 24 ---
Wasserstein
WorstCR
AverageCR (higher is better)
(lower is
(higher is
Unit
Random
Hard
Model
Method
better)
better)
B = 2
B = 5
B = 2
B = 5
B = 2
B = 5
GPT-4o
Sampling
0.23
0.42
0.96
0.93
0.91
0.92
0.74
0.95
Sampling, Few-shot
0.23
0.39
0.97
0.94
0.90
0.94
0.82
0.95
Persona
0.24
0.44
0.96
0.99
0.98
0.98
1.00
0.96
Persona, Few-shot
0.22
0.43
0.97
0.98
0.91
0.97
0.82
0.94
Batch
0.34
0.37
0.83
0.89
0.78
0.91
0.71
0.91
Batch, Few-shot
0.34
0.35
0.75
0.84
0.77
0.87
0.69
0.82
Description
0.20
0.58
0.95
0.96
0.93
0.96
0.76
0.95
Description, Few-shot
0.19
0.52
0.94
0.95
0.92
0.94
0.84
0.90
GPT-5-mini
Sampling
0.30
0.44
0.96
0.86
0.97
0.89
1.00
0.93
Sampling, Few-shot
0.26
0.47
0.92
0.97
0.95
0.96
0.95
0.96
Persona
0.22
0.38
0.69
0.96
0.76
0.95
0.73
0.97
Persona, Few-shot
0.22
0.41
0.86
0.98
0.83
0.96
0.78
0.96
Batch
0.26
0.54
0.91
0.92
0.89
0.94
0.76
0.85
Batch, Few-shot
0.25
0.50
0.90
0.92
0.85
0.93
0.73
0.80
Description
0.19
0.62
0.97
0.99
0.95
0.99
0.89
0.96
Description, Few-shot
0.19
0.61
0.95
0.97
0.94
0.97
0.88
0.96
Gemini
Sampling
0.31
0.43
0.96
0.86
0.94
0.89
0.87
0.95
Sampling, Few-shot
0.28
0.34
0.70
0.82
0.74
0.80
0.71
0.78
Persona
0.24
0.51
0.93
0.96
0.92
0.97
0.80
0.97
Persona, Few-shot
0.24
0.37
0.71
0.85
0.76
0.85
0.73
0.86
Batch
0.25
0.51
0.95
0.99
0.88
0.98
0.75
0.95
Batch, Few-shot
0.25
0.54
0.97
0.98
0.92
0.97
0.81
0.91
Description
0.20
0.51
0.90
0.99
0.88
0.98
0.75
0.94
Description, Few-shot
0.21
0.40
0.78
0.94
0.78
0.94
0.73
0.90
Mistral
Sampling
0.28
0.34
0.96
0.97
0.83
0.97
0.71
0.97
Sampling, Few-shot
0.27
0.35
0.76
0.92
0.77
0.91
0.72
0.88
Persona
0.23
0.47
0.90
0.97
0.87
0.97
0.65
0.92
Persona, Few-shot
0.24
0.45
0.87
0.97
0.86
0.97
0.76
0.93
Batch
0.34
0.35
0.81
0.87
0.79
0.88
0.68
0.83
Batch, Few-shot
0.35
0.30
0.69
0.79
0.73
0.85
0.67
0.80
Description
0.19
0.57
0.93
0.95
0.91
0.96
0.79
0.92
Description, Few-shot
0.24
0.45
0.82
0.88
0.81
0.88
0.72
0.87
Baseline
Random
0.23
0.31
0.71
0.83
0.65
0.82
0.55
0.76
Popularity score
0.53
0.96
0.99
1.00
1.00
0.56
0.89
5 real data
0.29
0.40
0.91
0.86
0.91
0.90
0.83
0.90
15 real data
0.24
0.53
0.98
0.94
0.95
0.95
0.91
0.97
30 real data
0.19
0.65
0.96
0.98
0.97
0.97
0.90
0.97
Table 1: Assortment optimization results for position-dependent rewards rp = 10/p, averaged over
20 runs.
Entries report the mean value for each model and method.
Cell shading is column-
wise, with darker indicating better performance within that metric. The model “Gemini” refers to
Gemini 3 Flash, and “Mistral” refers to Mistral Large 3.
24


--- Page 25 ---
Wasserstein
WorstCR
AverageCR (higher is better)
(lower is
(higher is
Unit
Random
Hard
Model
Method
better)
better)
B = 2
B = 5
B = 2
B = 5
B = 2
B = 5
GPT-4o
Sampling
0.23
0.69
0.97
0.99
0.97
0.98
0.89
0.98
Sampling, Few-shot
0.23
0.65
0.97
0.98
0.96
0.98
0.92
0.99
Persona
0.24
0.69
0.97
1.00
0.98
1.00
0.88
0.99
Persona, Few-shot
0.22
0.66
0.97
1.00
0.96
0.99
0.93
0.99
Batch
0.34
0.57
0.92
0.97
0.91
0.97
0.87
0.98
Batch, Few-shot
0.34
0.57
0.90
0.96
0.91
0.97
0.84
0.95
Description
0.20
0.71
0.95
0.99
0.96
0.99
0.95
0.98
Description, Few-shot
0.19
0.64
0.96
0.99
0.97
0.99
0.96
0.97
GPT-5-mini
Sampling
0.30
0.60
0.97
0.96
0.98
0.97
1.00
0.98
Sampling, Few-shot
0.26
0.61
0.96
0.99
0.98
0.99
0.98
0.98
Persona
0.22
0.67
0.93
0.99
0.93
0.99
0.89
0.99
Persona, Few-shot
0.22
0.69
0.96
0.99
0.96
0.99
0.91
0.98
Batch
0.26
0.63
0.95
0.98
0.96
0.98
0.94
0.97
Batch, Few-shot
0.25
0.65
0.94
0.98
0.94
0.98
0.91
0.97
Description
0.19
0.69
0.96
1.00
0.97
0.99
0.97
0.99
Description, Few-shot
0.19
0.67
0.96
0.99
0.97
0.99
0.97
0.98
Gemini
Sampling
0.31
0.60
0.97
0.96
0.97
0.97
1.00
0.97
Sampling, Few-shot
0.28
0.62
0.89
0.95
0.91
0.95
0.83
0.92
Persona
0.24
0.64
0.97
0.99
0.98
0.99
1.00
0.99
Persona, Few-shot
0.24
0.66
0.90
0.96
0.94
0.96
0.93
0.96
Batch
0.25
0.67
0.96
0.99
0.95
0.99
0.91
0.99
Batch, Few-shot
0.25
0.66
0.97
0.99
0.97
0.99
0.92
0.99
Description
0.20
0.64
0.94
0.99
0.94
0.99
0.91
0.99
Description, Few-shot
0.21
0.62
0.91
0.98
0.92
0.98
0.87
0.98
Mistral
Sampling
0.28
0.47
0.97
0.99
0.93
0.99
0.88
0.99
Sampling, Few-shot
0.27
0.48
0.94
0.99
0.94
0.98
0.87
0.98
Persona
0.23
0.58
0.97
0.99
0.96
0.99
0.85
0.99
Persona, Few-shot
0.24
0.56
0.94
0.99
0.95
0.99
0.87
0.99
Batch
0.34
0.48
0.92
0.96
0.92
0.97
0.88
0.96
Batch, Few-shot
0.35
0.47
0.88
0.95
0.90
0.96
0.84
0.94
Description
0.19
0.67
0.95
0.99
0.96
0.99
0.94
0.98
Description, Few-shot
0.24
0.53
0.90
0.96
0.91
0.96
0.86
0.96
Baseline
Random
0.23
0.46
0.85
0.96
0.85
0.96
0.83
0.94
Popularity score
0.70
0.97
1.00
1.00
1.00
0.87
0.98
5 real data
0.29
0.61
0.94
0.96
0.95
0.97
0.93
0.97
15 real data
0.24
0.71
0.97
0.98
0.98
0.98
0.99
0.99
30 real data
0.19
0.80
0.98
0.99
0.99
0.99
0.98
0.99
Table 2: Assortment optimization results for position-dependent rewards rp = 10−(p−1), averaged
over 20 runs. Entries report the mean value for each model and method. Cell shading is column-
wise, with darker indicating better performance within that metric. The model “Gemini” refers to
Gemini 3 Flash, and “Mistral” refers to Mistral Large 3.
25


--- Page 26 ---
Wasserstein
WorstCR
AverageCR (higher is better)
(lower is
(higher is
Unit
Random
Hard
Model
Method
better)
better)
B = 2
B = 5
B = 2
B = 5
B = 2
B = 5
GPT-4o
Sampling
0.23
0.75
0.98
1.00
0.98
1.00
0.96
1.00
Sampling, Few-shot
0.23
0.74
0.98
1.00
0.98
1.00
0.96
1.00
Persona
0.24
0.74
0.98
1.00
0.98
1.00
0.95
1.00
Persona, Few-shot
0.22
0.74
0.98
1.00
0.98
1.00
0.96
1.00
Batch
0.34
0.70
0.96
0.99
0.95
0.99
0.93
0.99
Batch, Few-shot
0.34
0.70
0.95
0.99
0.96
0.99
0.92
0.99
Description
0.20
0.77
0.97
1.00
0.97
1.00
0.97
0.99
Description, Few-shot
0.19
0.74
0.98
1.00
0.98
1.00
0.97
0.99
GPT-5-mini
Sampling
0.30
0.71
0.98
0.99
0.98
0.99
1.00
1.00
Sampling, Few-shot
0.26
0.71
0.97
1.00
0.99
1.00
0.99
0.99
Persona
0.22
0.77
0.97
1.00
0.98
1.00
0.96
0.99
Persona, Few-shot
0.22
0.76
0.97
1.00
0.98
1.00
0.96
1.00
Batch
0.26
0.70
0.97
1.00
0.98
0.99
0.97
0.99
Batch, Few-shot
0.25
0.72
0.97
0.99
0.97
1.00
0.96
0.99
Description
0.19
0.75
0.97
1.00
0.98
1.00
0.96
1.00
Description, Few-shot
0.19
0.74
0.97
1.00
0.98
1.00
0.97
0.99
Gemini
Sampling
0.31
0.71
0.98
0.99
0.98
0.99
1.00
0.99
Sampling, Few-shot
0.28
0.72
0.95
0.99
0.97
0.99
0.99
0.98
Persona
0.24
0.73
0.98
1.00
0.99
1.00
1.00
0.99
Persona, Few-shot
0.24
0.73
0.97
0.99
0.98
0.99
0.98
0.99
Batch
0.25
0.74
0.97
1.00
0.98
1.00
0.96
1.00
Batch, Few-shot
0.25
0.74
0.98
1.00
0.98
1.00
0.96
1.00
Description
0.20
0.73
0.96
1.00
0.96
1.00
0.94
0.99
Description, Few-shot
0.21
0.75
0.96
1.00
0.97
1.00
0.95
0.99
Mistral
Sampling
0.28
0.61
0.98
1.00
0.98
1.00
0.96
1.00
Sampling, Few-shot
0.27
0.61
0.97
1.00
0.97
1.00
0.94
1.00
Persona
0.23
0.66
0.97
1.00
0.98
1.00
0.92
0.99
Persona, Few-shot
0.24
0.65
0.97
1.00
0.98
1.00
0.94
1.00
Batch
0.34
0.60
0.96
0.99
0.97
0.99
0.94
0.99
Batch, Few-shot
0.35
0.61
0.95
0.99
0.96
0.99
0.92
0.99
Description
0.19
0.74
0.97
1.00
0.98
1.00
0.96
0.99
Description, Few-shot
0.24
0.63
0.94
0.99
0.95
0.99
0.93
0.99
Baseline
Random
0.23
0.58
0.94
0.99
0.92
0.99
0.91
0.98
Popularity score
0.74
0.98
1.00
1.00
1.00
0.94
0.98
5 real data
0.29
0.71
0.97
0.99
0.97
0.99
0.96
0.99
15 real data
0.24
0.78
0.98
1.00
0.99
1.00
1.00
1.00
30 real data
0.19
0.87
0.99
1.00
0.99
1.00
0.99
1.00
Table 3: Assortment optimization results for position-dependent rewards rp = 10 −0.1(p −1)2,
averaged over 20 runs. Entries report the mean value for each model and method. Cell shading is
column-wise, with darker indicating better performance within that metric. The model “Gemini”
refers to Gemini 3 Flash, and “Mistral” refers to Mistral Large 3.
26


--- Page 27 ---
• Persona-sampling: We generate a pool of 100 willingness-to-pay values for each products,
prompting the LLM with a different persona each time. We subsample 50 willingness-to-pay
values to form an estimated distribution bF, and repeat to form 20 estimated distributions in
this way.
• Batch-generation: We generate 25 willingness-to-pay values for each product per query
and repeat this process 20 times, yielding 20 distributions.
• Description: We ask the LLM to generate 5 most likely willingness-to-pay values and their
corresponding probabilities for each product, repeating 20 times.
• Few-shot Sampling: We draw 5 example sets, each containing 6 examples. For each example
set, we generate a pool of 100 willingness-to-pay values for each product and construct 4
subsamples of size 50, resulting in a total of 20 distributions.
• Few-shot Persona-sampling: Same as Few-shot Sampling, except we also prompt the LLM
with a different persona each time.
• Few-shot Batch-generation: We draw 5 example sets, each containing 6 examples. We
run Batch-generation 4 times with each example set, resulting in a total of 20 distributions.
• Few-shot Description: We draw 5 example sets, each containing 6 examples.
We run
Description 4 times with each example set, resulting in a total of 20 distributions.
• Random baseline: We generate a pool of 100 willingness-to-pay values for each product
uniformly at random. We subsample 50 willingness-to-pay values from it 20 times, to form
20 estimated distributions.
• d real data baseline: We randomly sample d willingness-to-pay values from the ground-
truth F for each product, generating 20 distributions in this way.
The reported values take an average over the 6 products, with different ground-truth distribu-
tions F.
F
Plots of Willigness-to-Pay Distributions
Recall that Rc(a) = (a −c) Prξ∼F [ξ ≥a] in the pricing problem. We will refer to Prξ∼F [ξ ≥a] as
the survival function of a willingness-to-pay distribution F, depicting the probability of purchase
as a function of the price a.
Figure 5 plots an example of the survival functions of the ground truth willingness-to-pay
distribution F and the estimated distributions ˆF for the Sampling and Persona sampling methods.
A notable feature is that the true and estimated curves exhibit sharp “drops” at many of the
same x-values. These drops occur at willingness-to-pay values that are multiples of 5, reflecting
the common phenomenon that when asked for willingness to pay, people disproportionately report
round numbers ending in 0 or 5 — and we see that the LLM-generated samples display the same
phenomenon.
This matters for pricing because the optimal price often lies exactly at a drop in the survival
function. A drop in the survival function at price p corresponds to a drop in the demand when the
price increases past p, and hence the optimal price will often occur right at the drop. Consequently,
when ˆF places drops at the same prices as F, the maximizing price is more likely to coincide with
the optimal price under F. This provides intuition for how the LLM’s world knowledge about
round-number reporting can translate into improved downstream pricing decisions.
27


--- Page 28 ---
Figure 5:
Survival functions of the willingness-to-pay distributions for the ground truth distribu-
tion F, and the estimated distributions ˆF from the Sampling and Persona sampling methods for the
product Bohol. The “drops” in the functions are at technically numbers that end in 4 or 9 — this is
because in the data, respondents were asked about their willingness-to-pay premiums over the base
price of PhP 44 (for an “upgraded” product), and this figure plots their total willingness-to-pay
(44 + their response).
G
Newsvendor problem: Data Pre-processing
We use a sales dataset at the fashion retailer H&M (Ling et al., 2022), which contains daily sales
numbers at the product level. We first aggregate the sales into weekly totals, and we focus on items
categorized as Trousers over a 30-week period, from March 3, 2019, to September 22, 2019. Each
item is associated with metadata providing a description of the product, which includes the fields:
product name, product type name, graphical appearance name, color group name, department
name, index name, section name, garment group name, and detailed description.
We used the following data pre-processing pipeline to ensure we only consider items without
large price fluctuations or missing values:
• Price Outlier Removal: To mitigate the impact of pricing anomalies or data entry errors,
any price Pi falling outside a 20% threshold of the mean price µp was flagged as a missing
value (NA). Specifically, values were discarded if:
|Pi −µp| > 0.20µp.
• Zero-Value Removal: Rows containing a value of 0 in any weekly sales count column were
removed. This process excludes products that were likely inactive or out of stock during the
observed period.
28


--- Page 29 ---
• Missing Value Threshold: We enforced a data density requirement by calculating the total
number of NA entries per product record. Any item missing more than 10 weekly data points
(NAs > 10) was excluded.
Following the application of these filters, the final refined dataset consists of 300 items.
H
Assortment problem: Prompts
1. Background Information:
“We study sushi preference patterns among Japanese consumers. The sushi items
listed below are those considered.
Sushi items (Target items):
• 0: {sushi 0 description}
• 1: {sushi 1 description}
• ...
• 9: {sushi 9 description}
Sushi item descriptions:
• Name and ID
• Style: maki roll or non-maki
• Major category: seafood or non-seafood
• Minor group: one of 12 ingredient groups
• Taste intensity: very heavy / heavy / moderate / light / very light
• Consumption frequency: rarely / sometimes / often / very frequently eaten
• Availability: very rarely to very commonly found
• Price score: from 0 to 6
2. Additional Information in Few-shot Setting:
“To help with the estimation task, we provide some examples of a respondent’s
decision as guidance for the model.
The examples are listed below:
Examples (totally 6 examples):
• 0: A person made a decision of: {a ranking}
• 1: A person made a decision of: {a ranking}
• ...
• 5: A person made a decision of: {a ranking}
For each of the two information settings, we test four ways of prompting an LLM to generate
a distribution.
1. Sampling:
“Based on the information provided, please simulate a Japanese respondent’s sushi
preference ranking.
Task: Generate one ranking to represent a potential preference ordering over the
10 sushi items.
INSTRUCTIONS:
29


--- Page 30 ---
(a) First, explain your reasoning. Consider the sushi item descriptions (and exam-
ples and persona if we provided).
(b) Second, provide the final ranking as exactly 10 unique integers from 0 to 9,
ordered from most preferred to least preferred.
Output Format: Reasoning: [...] Final Answer: [10 integers separated by spaces]
2. Persona-sampling: Same as sampling, but we provide the following additional instructions.
“To help with the estimation task, we provide a persona description of the one you
should pretend to be.
So, pretend you are {persona description}.
Persona description (outside the distribution):
• Gender and age group, where age is discretized into six bins (15–19, 20–29, 30–39, 40–49,
50–59, 60+)
• Current residence, including prefecture, region, and east–west classification within Japan.
• Childhood residence, including prefecture, region, and east–west classification, when avail-
able.
3. Batch-generation:
“Based on the information provided, please generate a representative batch of sushi
preference rankings.
Task: Generate a batch of 30 independent preference rankings over the 10 sushi
items.
INSTRUCTIONS:
(a) First, explain your reasoning. Consider the sushi item descriptions (and exam-
ples if we provided).
(b) Second, please output exactly 30 rankings, each consisting of 10 unique integers
from 0 to 9, ordered from most preferred to least preferred.
Output Format: Reasoning: [...] Final Answer: [30 lines of rankings, each line
corresponding to one ranking.]
4. Description:
“Based on the information provided, please verbalize the utilities for the 10 sushi.
Task: Provide utilities to 10 sushi, and utilities can be any real numbers (scale/shift
invariant) and must be floats in [0, 5].
INSTRUCTIONS:
(a) First, explain your reasoning. Consider the sushi item descriptions (and exam-
ples if we provided).
(b) Second, provide the utilities.
Output Format: Reasoning: [...] Final Answer: [ \0":
<float>, \1":
<float>,
..., \9":
<float> ]
I
Pricing problem: Prompts
1. Background Information:
30


--- Page 31 ---
“We study willingness-to-pay premiums for tablea chocolate products among con-
sumers from Central Bicol State University of Agriculture in the Philippines. You
will help us simulate willingness-to-pay decision. Here are three target items with
there awards/origin:
Chocolate products (providing awards):
• Bohol: won Academy of Chocolate
• Davao: won Great Taste
• Improved Bicol: no award
Chocolate products (providing origin):
• Bohol: Bohol island cacao
• Davao: Davao region cacao
• Improved Bicol: Bicol region cacao
2. Additional Information in Few-shot Setting:
“To help with the estimation task, we provide some examples of a respondent’s
decision as guidance for the model.
Examples (totally 6 examples):
• 0: A person made a decision of: {three WTPs correspond to three chocolate
products}
• 1: A person made a decision of: {three WTPs correspond to three chocolate
products}
• ...
• 5: A person made a decision of: {three WTPs correspond to three chocolate
products}
For each of the two information settings, we test four ways of prompting an LLM to generate
a distribution.
1. Sampling:
“Based on the information provided, please finish the task below. Suppose you hold
an endowment chocolate (regular Bicol) worth 44 PHP.
Task: For each target product, report the premium (additional PHP over 44) you’d
be willing to pay to exchange for it.
INSTRUCTIONS:
(a) First, explain your reasoning. Consider the chocolate item descriptions (and
examples and persona if we provided).
(b) Second, provide the premium (additional PHP over 44) you’d be willing to pay
to exchange for each target product, and each premium value must be between
0 and 100.
Use non-negative numbers; if you would not exchange for that
product, use 0.
Output Format: Reasoning: [...] Final Answer: [{“Bohol”: X, “Davao”: Y ,
“ImprovedBicol”: Z}]
2. Persona-sampling: Same as sampling, but we provide the following additional instructions.
31


--- Page 32 ---
“To help with the estimation task, we provide a persona description of the one you
should pretend to be.
So, pretend you are {persona description}.
Persona description (outside the distribution):
• Gender and age group, age is divided into four bins (18–24, 25–34, 35–44, 45+).
• Income, classified as either “low” or “middle-to-high”.
• Main shopper status, whether the user acts as the main shopper for the household (yes/no).
• Consumption frequency of Tablea, categorized into low, middle, or high levels.
• Origin sensitivity, the importance placed on product origin, rated on a 5-point scale
ranging from “not important” to “very important”.
• Chocolate preference, intensity of chocolate preference (derived from dark chocolate data),
classified as low, middle, or high.
• Award influence, influence of industry awards, classified into low, moderate, or high.
3. Batch-generation:
“Based on the information provided, please finish the task below. Suppose you hold
an endowment chocolate (regular Bicol) worth 44 PHP.
Task: For each target product, provide 25 independent premiums (additional PHP
over 44) you’d be willing to pay to exchange for it.
INSTRUCTIONS:
(a) First, explain your reasoning. Consider the sushi item descriptions (and exam-
ples if we provided).
(b) Second, provide 25 premiums (additional PHP over 44) you’d be willing to pay
to exchange for each target product, and each premium value must be between
0 and 100.
Use non-negative numbers; if you would not exchange for that
product, use 0.
Output Format: Reasoning: [...] Final Answer: [{“Bohol”: X1, “Davao”: Y1,
“ImprovedBicol”: Z1},{“Bohol”: X2, “Davao”: Y2, “ImprovedBicol”: Z2},...,{“Bo-
hol”: X25, “Davao”: Y25, “ImprovedBicol”: Z25}]
4. Description:
“Based on the information provided, please finish the task below. Suppose you hold
an endowment chocolate (regular Bicol) worth 44 PHP.
Task: For each target product, provide a discrete distribution over the premium
(additional PHP over 44) you would be willing to pay to exchange for it.
INSTRUCTIONS:
(a) First, explain your reasoning. Consider the sushi item descriptions (and exam-
ples if we provided).
(b) Second, for each product, output exactly 5 most likely premium values. The 5
values must include 0 (meaning you would not exchange), and each premium
value must be between 0 and 100. Also, output the probability for each of the
5 values. Probabilities must be non-negative and sum to 1 for each product.
Please avoid identical distributions across the three products unless strongly
justified.
Output Format: Reasoning: [...] Final Answer: [
32


--- Page 33 ---
{
"premium_support": {
"Bohol": [v1, v2, ..., v5],
"Davao": [v1, v2, ..., v5],
"ImprovedBicol": [v1, v2, ..., v5]
},
"probabilities": {
"Bohol": [p1, p2, ..., p5],
"Davao": [p1, p2, ..., p5],
"ImprovedBicol": [p1, p2, ..., p5]
}
}
]
J
Newsvendor problem: Prompts
“You are an expert in inventory demand forecasting. Use the provided reference items
to estimate statistics for the Target Item.
Context (100 Reference Items with known Demand Statistics):
[Reference Item 1]
Features:
- prod_name: [Value]
- product_type_name: [Value]
- graphical_appearance_name: [Value]
- colour_group_name: [Value]
- department_name: [Value]
- index_name: [Value]
- section_name: [Value]
- garment_group_name: [Value]
- detail_desc: [Value]
KNOWN STATISTICS -> Mean: [Value], Std: [Value]
------------------------------
... [Items 2-100] ...
------------------------------
Target Item Task:
Please estimate the demand distribution for:
- Product Name: [Target Item Name]
- Features:
- prod_name: [Value]
- product_type_name: [Value]
- graphical_appearance_name: [Value]
- colour_group_name: [Value]
- department_name: [Value]
- index_name: [Value]
33


--- Page 34 ---
- section_name: [Value]
- garment_group_name: [Value]
- detail_desc: [Value]
Task: Estimate the underlying Normal distribution parameters for the Target Item.
STRICT OUTPUT FORMAT:
You must start your response with exactly this line:
PREDICTION →Mean: [Value], Std: [Value]
Then, on a new line, provide your:
REASONING →[Your detailed explanation here]”
34


--- Page 35 ---
Wasserstein
Kolmogorov
WorstCR
AverageCR (higher is better)
(lower is
(lower is
(higher is
Unif[0, 32]
Unif[0, 66]
Unif[0, 100]
Model
Method
better)
better)
better)
GPT-4o
Sampling
17.13
0.42
0.85
0.92
0.78
0.61
Sampling, Few-shot
16.51
0.41
0.84
0.94
0.77
0.60
Sampling, Persona
15.66
0.41
0.84
0.96
0.79
0.65
Persona, Few-shot
17.57
0.43
0.78
0.94
0.75
0.62
Batch
13.84
0.33
0.70
0.89
0.79
0.67
Batch, Few-shot
13.33
0.31
0.72
0.89
0.80
0.69
Description
12.13
0.31
0.75
0.88
0.83
0.75
Description, Few-shot
12.40
0.31
0.77
0.91
0.82
0.73
GPT-5-mini
Sampling
18.57
0.46
0.82
0.94
0.74
0.56
Sampling, Few-shot
17.43
0.44
0.81
0.90
0.78
0.59
Sampling, Persona
17.13
0.43
0.83
0.95
0.77
0.59
Persona, Few-shot
11.90
0.31
0.81
0.90
0.82
0.72
Batch
11.48
0.29
0.68
0.88
0.83
0.74
Batch, Few-shot
10.54
0.25
0.71
0.89
0.83
0.76
Description
13.15
0.33
0.77
0.92
0.84
0.76
Description, Few-shot
12.94
0.33
0.76
0.93
0.85
0.78
Gemini
Sampling
16.25
0.46
0.80
0.89
0.79
0.67
Sampling, Few-shot
15.28
0.40
0.78
0.88
0.85
0.79
Sampling, Persona
14.17
0.36
0.84
0.91
0.79
0.64
Persona, Few-shot
17.44
0.43
0.77
0.90
0.72
0.56
Batch
9.31
0.23
0.72
0.89
0.82
0.73
Batch, Few-shot
10.29
0.26
0.73
0.90
0.83
0.75
Description
14.12
0.38
0.80
0.92
0.80
0.67
Description, Few-shot
15.90
0.42
0.79
0.93
0.75
0.58
Mistral
Sampling
17.23
0.43
0.82
0.95
0.76
0.60
Sampling, Few-shot
17.19
0.44
0.81
0.93
0.77
0.55
Sampling, Persona
21.17
0.55
0.74
0.93
0.68
0.46
Persona, Few-shot
18.95
0.47
0.76
0.94
0.73
0.51
Batch
17.27
0.39
0.63
0.87
0.76
0.64
Batch, Few-shot
16.12
0.36
0.65
0.88
0.75
0.62
Description
14.93
0.38
0.78
0.92
0.79
0.66
Description, Few-shot
17.65
0.42
0.74
0.93
0.74
0.58
Baseline
Random
20.89
0.35
0.58
0.72
0.68
0.67
5 real data
11.93
0.34
0.74
0.88
0.80
0.70
10 real data
8.49
0.24
0.79
0.90
0.87
0.79
20 real data
6.15
0.18
0.84
0.94
0.91
0.86
Table 4: Pricing results over 6 products, each product averaged over 20 runs. Entries report the
mean value over the 6 products for each model and method. Cell shading is column-wise, with
darker indicating better performance within that metric. The model “Gemini” refers to Gemini 3
Flash, and “Mistral” refers to Mistral Large 3.
35
