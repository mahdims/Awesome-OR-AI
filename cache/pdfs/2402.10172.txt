--- Page 1 ---
OptiMUS: Scalable Optimization Modeling
with (MI)LP Solvers and Large Language Models
Ali AhmadiTeshnizi 1 Wenzhi Gao 2 Madeleine Udell 1 2
Abstract
Optimization problems are pervasive in sectors
from manufacturing and distribution to health-
care.
However, most such problems are still
solved heuristically by hand rather than opti-
mally by state-of-the-art solvers because the ex-
pertise required to formulate and solve these
problems limits the widespread adoption of op-
timization tools and techniques. This paper in-
troduces OptiMUS, a Large Language Model
(LLM)-based agent designed to formulate and
solve (mixed integer) linear programming prob-
lems from their natural language descriptions.
OptiMUS can develop mathematical models,
write and debug solver code, evaluate the gener-
ated solutions, and improve its model and code
based on these evaluations.
OptiMUS utilizes
a modular structure to process problems, allow-
ing it to handle problems with long descriptions
and complex data without long prompts. Exper-
iments demonstrate that OptiMUS outperforms
existing state-of-the-art methods on easy datasets
by more than 20% and on hard datasets (includ-
ing a new dataset, NLP4LP, released with this pa-
per that features long and complex problems) by
more than 30%.
1. Introduction
Optimization problems are common in many fields such
as operations, economics, engineering, and computer sci-
ence. Important applications of optimization include reduc-
ing energy costs in smart grids, improving supply chains,
and increasing profits in algorithmic trading (Singh, 2012;
Antoniou & Lu, 2007). Major advances in optimization
algorithms over the last several decades have led to reli-
able and efficient optimization methods for a wide vari-
1Department of Management Science and Engineering, Stan-
ford University, CA, USA 2Institute for Computational and Math-
ematical Engineering, Stanford University, CA, USA. Correspon-
dence to: Ali AhmadiTeshnizi <teshnizi@stanford.edu>.
ety of structured optimization problems, including linear
programming (LP) and mixed-integer linear programming
(MILP), among many others. Unfortunately, optimization
modeling, transforming a business problem into a mathe-
matical optimization problem, still requires expert knowl-
edge. According to a recent survey, 81% of Gurobi’s com-
mercial solver users have advanced degrees, with 49 % of
them holding a degree in operations research (Gurobi Op-
timization, 2023). This expertise gap prevents many or-
ganizations from using optimization, even when it could
significantly improve their operations. Examples include
inventory management in supermarkets, patient operations
in hospitals, transportation policies in small municipali-
ties, energy management in local solar farms, and opera-
tions in small businesses or NGOs (Saghafian et al., 2015;
Aastrup & Kotzab, 2010; Yao et al., 2020; Shakoor et al.,
2016).
Automating optimization modeling would allow
sectors that cannot afford to have access to optimization ex-
perts to improve efficiency using optimization techniques.
Large language models (LLMs) offer a promising way to
make optimization more accessible. LLMs have demon-
strated the ability to understand, generate, and interpret
natural language for many tasks. In the optimization do-
main, LLMs can make it easier to formulate problems and
obtain solutions, making expert-level optimization more
accessible (Ramamonjison et al, 2023) However, several
challenges remain before LLMs can reliably model real-
life optimization problems:
• Ambiguous Terms: An optimization problem can be
described in many ways. For example, a user might use
different terms (e.g. vehicle vs. car vs. truck vs. carrier),
notations (e.g. price and capacity vs. p and c vs. x and
y), or omit common-sense assumptions (e.g. capacity of
a vehicle is non-negative, number of employees is an in-
teger, etc.). Moreover, defining the right variables can
be a challenge. For instance, information flow through
a network requires a different set of variables than phys-
ical goods, as the quantity of information need not be
conserved.
• Long Problem Descriptions: LLMs have a limited con-
text size.
However, real-world problems can be long
and complex: for example, the energy system problem
in (Holzer et al., 2023) has a 60-page documentation.
1
arXiv:2402.10172v1  [cs.AI]  15 Feb 2024


--- Page 2 ---
Large Language Model for Optimization Modeling
Manager
A factory
produces several products.
Each product
requires different amounts
of raw materials, machine
time, and labor. Each
product has a price.
The factory needs
to determine how much
of each product to
produce to maximize the
revenue while not
exceeding resource
capacities.
Pre-processing
Initial
Structured
Problem
Formulator
Evaluator
Programmer
Agent Team
Revenue: $4124
Production:
[0, 7, ...,2]
P: 131,
MachineTime: [4,8,...,5],
...
MaterialRequirement:
[[3,2,..., 4],
...,
[1,4,..., 2]]
Problem Description
Parameter Data
Figure 1. OptiMUS uses a structured sequence of LLM agents to effectively model and solve optimization problems. First, the natural
language representation of the problem is preprocessed into a structured problem. Then, a team of agents iteratively augments the
structured problem with a connection graph, mathematical formulations of each clause, and code for each clause. The agents continue
work until the problem is solved. (Dashed lines represent communications that can occur multiple times.)
Even for long-context models, performance decreases
substantially as the input context grows (Liu et al., 2023).
Consequently, LLMs tend to make more mistakes as the
length of the problem description increases and perform
poorly on complex problems.
• Large Problem Data:
The specification of an opti-
mization problem often involves large amounts of data,
such as customer attributes or sales of goods.
Previ-
ous approaches to optimization modeling using LLMs,
which pass numerical data to the LLM directly, are thus
restricted to the simplest of toy problems.
• Unreliable Outputs: The solutions provided by LLMs
are not always reliable. The generated code may be in-
correct or even not executable. It is especially challeng-
ing to verify the solution when the code runs, but the out-
put is incorrect. For instance, if the code runs and claims
that the problem is unbounded, perhaps a constraint has
been accidentally omitted from the formulation.
Contributions.
This paper develops a novel perspective
on optimization modeling that addresses each of these lim-
itations and makes the following contributions:
• Existing datasets for natural language optimization mod-
eling are too easy to capture the challenge of long prob-
lem descriptions and large problem data. This work in-
troduces NLP4LP, an open source dataset of 67 complex
optimization problems. Table 1 compares NLP4LP to
existing datasets and Section 4.1 describes NLP4LP.
• We develop a modular, LLM-based agent to model and
solve optimization problems, which we call OptiMUS.
OptiMUS beats the previous state-of-the-art methods on
existing datasets by over 20% and on our more challeng-
ing dataset by 30%. OptiMUS employs a novel connec-
tion graph that allows it to process each constraint and
objective independently. Using this connection graph,
and separating data from the problem description, Op-
tiMUS can solve problems with long descriptions and
large data files without excessively long prompts.
Structue of the Paper
This paper is organized as fol-
lows: Section 2 discusses the background and related work;
Section 3 describes the details of our LLM-based optimiza-
tion agent; Section 4 discusses the datasets and presents
the experiments and analysis; Section 5 concludes the pa-
per with future directions and implications. The appendix
includes prompts, details on the experiments’ setup, and
further analysis.
2. Background and Related Work
Optimization problems are mathematically defined by an
objective function and a set of constraints. For example, an
MILP can be written mathematically as
minimize
{xj}
n
X
j=1
cjxj
subject to
n
X
j=1
aijxj (≤, =, ≥) bi, i = 1, . . . m
lj ≤xj ≤uj, j = 1, . . . , n
xj ∈Z, j ∈I
An optimization workflow consists of 1) formulating an op-
timization problem in mathematical form by identifying its
objective and constraints, and then 2) solving the realiza-
tion of problem from real data, generally using code that
calls an optimization solver.
2


--- Page 3 ---
Large Language Model for Optimization Modeling
Table 1. A comparison on different aspects of complexity for various datasets. The unit for description length is characters
Dataset
Description Length
Instances (#MILP)
Multi-dimensional Parameters
NL4Opt
518.0 ± 110.7
1101 (0)
×
ComplexOR
497.1 ± 247.5
37 (12)
✓
NLP4LP (Ours)
908.9 ± 504.6
67 (13)
✓
Progress in LLMs.
Recent progress in Natural Language
Processing (NLP) has led to the development of large lan-
guage models (LLMs) useful for tasks such as answer-
ing questions, summarizing text, translating languages, and
coding.
(OpenAI, 2023; Touvron et al., 2023; Chowdhery et al.,
2022; Wei et al., 2023; Gao et al., 2023; Borgeaud et al.,
2022). Connections to other software tools extend the reach
and accuracy of LLMs, as demonstrated by plug-ins for
code writing and execution (Paranjape et al., 2023; Wei
et al., 2023).
(Yang et al., 2023) use LLMs to directly
generate solutions to optimization problems without call-
ing traditional solvers through prompt optimization to im-
prove performance. The approach is limited to small prob-
lems since the performance of LLMs degrades as the input
context grows, even for explicitly long-context models (Liu
et al., 2023).
Chatbots for Optimization.
In a recent paper, Chen
et al. (2023) developed a chatbot to help users detect and
fix infeasible optimization problems expressed in Pyomo
code and servers as an AI assistant rather than as a solver.
Li et al. (2023) designed a chatbot to answer natural-
language queries about an optimization model. Alibaba
Cloud (2022) also developed a chatbot to facilitate opti-
mization modeling, but there is no public paper or docu-
mentation available on it.
Benchmark-driven
Optimization
Modeling.
More
closely related to our approach, (Ramamonjison et al,
2023) introduced a dataset of 1101 natural language rep-
resentations of LP problems. They proposed a two-stage
mapping from the natural-language representation to the
problem formulation using an intermediate representa-
tion.
(Ramamonjison et al, 2022) designed a system to
simplify and improve the modeling experience for oper-
ations research, but did not offer an end-to-end solution.
(Anonymous, 2024) presented a multi-agent cooperative
framework to automatically model and program complex
operation research (OR) problems, and evaluated it on
NL4Opt and another more complex dataset they curate.
In terms of traditional MILP benchmarking, it should be
noted that MIPLIB is widely recognized as a benchmark
for evaluating the performance of MILP solvers. MIPLIB
offers a diverse collection of MILP instance realizations
that are, for the most part, detached from their original
formulations. This paper focuses primarily on the model-
ing aspects of MILPs and therefore does not have a direct
correlation with MIPLIB.
3. Methodology
This section details the design of OptiMUS. See Figure 1
for an illustration.
The problem presented in Figure 2
serves as a running example. OptiMUS starts with a nat-
ural language description of the optimization problem. The
problem is first preprocessed to extract the parameters, con-
straints, objective function, and background information.
Then OptiMUS uses a multi-agent framework to process
and solve the structured problem. Appendix B includes all
prompts used in OptiMUS. For brevity, we use the word
clause to refer to a constraint or objective.
3.1. Structured Problem
The OptiMUS preprocessor converts a natural language de-
scription of the problem into a structured problem (Fig-
ure 2) with the following components:
• Parameters: A list of parameters of the optimization
problem. Each parameter has three components: 1) sym-
bol, 2) shape, and 3) text definition. OptiMUS can choose
symbols, infer the shape, and define the parameters if
they are not explicitly included in the problem statement.
Importantly, numerical data that may be included in the
problem statement is omitted from the parameters and
stored for later use. This ensures that the parameters are
short and easy to include in future prompts.
• Clauses: A list of the clauses (objective and constraints)
of the optimization problem. The preprocessor initializes
each clause with its natural language description. Later
these clauses will be augmented with LATEX formulations
and code as well.
• Background: A short string explaining the real-world
context of the problem. This string is included in every
prompt to improve common sense reasoning.
The preprocessing uses three prompts: the first prompt ex-
tracts the parameters, the second segments the problem into
objective and constraints, and the third eliminates redun-
dant (e.g., two restatements of the constraint that produc-
3


--- Page 4 ---
Large Language Model for Optimization Modeling
 A factory produces
several products. Each
product requires
different amounts of raw
materials, machine time,
and labor. Each product
generates a specific
amount of revenue. The
factory needs to
determine how much of
each product to produce
to maximize profits
while not exceeding
resource capacities.
Pre-processing
M
Scalar
Number of different machine types
MachineTimeCap
[M]
Capacity of machine time
MaterialReq
[R, P]
Amount of raw material required per unit of product
Parameters
The factory aims to maximize its
profits.
Objective
Constraints
Production quantities of products are
non-negative.
Production quantities of products are
integral.
Total raw materials used for all
products cannot exceed
MaterialCapacity.
Total labor used for all products
cannot exceed LaborCapacity.
A factory produces different
products, each requiring various
amounts of raw materials, machine
time, and labor. These products
generate specific revenues upon sale.
Background
Figure 2. OptiMUS preprocesses natural language representations of a problem into a modular state. The components of the modular
state are: 1) parameters and their shape, 2) objective, 3) background and context, and 4) implicit and explicit constraints.
Algorithm 1 Workflow of OptiMUS
1: Input: Natural language description of problem P
2: P (0) ←PREPROCESS(P)
3: Initialize msg ←“”
4: Initialize conversation ←[]
5: for t = 1, . . . do
6:
AGENT, task ←MANAGER(conversation)
7:
P (t+1), msg ←AGENT(P (t), task)
8:
conversation += msg
9:
if msg = Done then break
10: end
tion quantity is nonnegative), unnecessary (such as facts
about the problem parameters, e.g., that price is nonneg-
ative), and incorrect constraints (e.g., production quantity
must exactly equal demand). The second step can also be
a challenge: for example, in the factory example shown in
Figure 2, the production amount for each product should be
a positive value, but this is not stated explicitly.
3.2. Agents
After preprocessing, OptiMUS defines problem variables,
formulates and codes each clause. To ensure consistency
of the formulations, OptiMUS constructs and maintains a
connection graph to record which variables and parameters
appear in each constraint. This connection graph is key to
performance and scalability of OptiMUS, as it allows the
LLM to focus only on the relevant context for each prompt,
generating more stable results. The list of variables and
the LATEX formulations and code are initially empty; when
all clauses are formulated, programmed, and validated, the
process is complete.
Manager.
Inspired by (Wu et al., 2023), OptiMUS uses
a manager agent to coordinate the work of formulation,
programming, and evaluation, recognizing that these steps
may need to be repeated to ensure consistency and correct-
ness (see Algorithm 1). At each step, the manager looks
at the conversation so far and chooses the next agent (for-
mulator, programmer, or evaluator) to process the problem.
The manager also generates and assigns a task to the cho-
sen agent, for example:
Review and fix the formulation of the objective.
Formulator.
The formulator agent must:
1. Write and correct mathematical formulations for vari-
ables and clauses.
2. Define new variables and auxiliary constraints.
3. Update the links in the connection graph.
If the assigned task is to formulate new clauses, the formu-
lator iterates over the clauses that have not yet been formu-
lated and generates new formulations for them. During this
process, it will also define auxiliary constraints and new
variables when necessary. Moreover, it decides which pa-
rameters and variables are related to the clause (see Fig-
ure 3). This information is used to update the connection
graph. On the other hand, if the task is to fix incorrect
formulations reported by the evaluator or the programmer,
the agent iterates through the clauses marked as incorrect,
4


--- Page 5 ---
Large Language Model for Optimization Modeling
For each material, the amount
used should not exceed the
available capacity.
MaterialReq
[R, P]
MaterialCap
[R]
Parameters
LaborCap
[L]
Maximize the total revenue
from producing various
products
Variables
Objective
Constraints
Production
[P]
For each material, the amount
used should not exceed the
available capacity.
MaterialReq
[R, P]
MaterialCap
[R]
Parameters
LaborCap
[L]
Maximize the total revenue
from producing various
products
Variables
Objective
Constraints
Figure 3. The formulation process for a single constraint. The formulation agent identifies any parameters and variables appearing in the
constraint, including new variables that it may need to define. It defines new variables as needed, updates the connection graph which
records which constraints use which parameters and which variables, and annotates the constraint with a LATEX formulation. (dashed
lines represent new connections and variables)
fixes their formulations, and updates the connection graph.
OptiMUS also has an extra modeling layer that captures
special model structures (e.g., special-ordered-set and indi-
cator variables) and we leave a more detailed discussion to
the Appendix A.
Programmer.
The responsibility of the programmer
agent is to write and debug the solver code. When the pro-
grammer is called by the manager, it first reads the task. If
the task is to program new clauses, the agent iterates over
the clauses that have not yet been coded and generates code
from their formulations. If the task is to fix incorrect formu-
lations reported by the evaluator, the agent iterates through
the clauses marked as bogus and fixes their codes.
In our experiments, the programmer uses Python as the
programming language and Gurobi as the solver. Opti-
MUS can target other solvers and programming languages
as long as they are supported by the LLM.
Evaluator.
The evaluator agent’s responsibility is to ex-
ecute the generated code on the data and to identify any
errors that occur during the execution. If evaluator faces a
runtime error, it flags the variable or clause with the bogus
code and responds to the manager with appropriate expla-
nation of the error. The information will later be used by
the other agents to fix the formulation and debug the code.
3.3. The connection graph
Recall from Section 3.2 that OptiMUS maintains a con-
nection graph over constraints, objectives, parameters, and
variables. OptiMUS uses this graph to retrieve the rele-
vant context for each prompt so prompts remain short. This
graph is used also to generate and debug code and to correct
wrong formulations. Figure 4 provides an example.
4. Experiments
In this section, we conduct a comprehensive evaluation of
OptiMUS. We begin by detailing the datasets used in our
experiments and showcase the superior performance of Op-
tiMUS across these datasets, highlighting its strengths. An
ablation study demonstrates the impact of different system
components on our results, and a sensitivity analysis probes
the internal dynamics of OptiMUS. We conclude this sec-
tion by identifying failure cases and potential areas for fur-
ther improvement.
4.1. Dataset
NL4OPT. This dataset is a collection of 1101 easy linear
programming problems proposed as part of the NL4OPT
competition (Ramamonjison et al, 2023). The dataset con-
tains a natural language description of each problem, along
with an annotated intermediate representation that lists pa-
rameters, variables, and clauses.
ComplexOR. ComplexOR is a collection of 37 complex
operations research problems in a variety of application
domains (Anonymous, 2024). At the time of writing this
paper, the publicly available version of this dataset is in-
complete. We gathered 21 problems from the ComplexOR
dataset to use in our experiments by augmenting the prob-
5


--- Page 6 ---
Large Language Model for Optimization Modeling
Task: Debug the runtime error for the
material capacity limit constraint 
MaterialCap
MachineTimeReq
LaborReq
MaterialReq
RevenuePerProd
For each
material, the
amount used
should not exceed
the available
capacity
LaborCap
MachineTimeCap
Production
Prod = model.addVars(P, vtype=gp.GRB.CONTINUOUS, name="production")
# Add constraints for the quantity of raw material usage not exceeding available
amounts
for j in range(N):
    model.addConstr(gp.quicksum(MaterialReq[j, i] * Prod[i] for i in range(P)) \\
<= Available[j], name=f"material_usage_limit_{j}")
Here is the error message: 
IndexError: index 4 is out of bounds for axis 0 with size 4
Identify the error and fix it.
import numpy as np
import gurobipy as gp
R = data["R"] # scalar parameter
P = data["P"] # scalar parameter
MaterialCap = np.array(data["MaterialCap"]) # ['R']
MaterialReq = np.array(data["MaterialReq"]) # ['R', 'P']
The execution of the following code results in a runtime error:
Prompt
Figure 4. OptiMUS uses the connection graph to extract and use only the relevant context in each prompt. In this example, the pro-
grammer agent fetches the context via the connection graph to debug a bogus constraint code. Without the graph, the LLM would have
needed to process the whole code, including the code for the other parameters, variables, constraints, and the objective.
lems that lack data with synthetic data.
This modified
dataset is available in our supplementary materials.
NLP4LP. As shown in Table 1, existing datasets for natural
language optimization modeling lack problems with long
descriptions. Real-world problems often are much longer,
see e.g. (Holzer et al., 2023). To address this issue, we
create NLP4LP (Natural Language Processing for Linear
Programming), a benchmark consisting of 54 LP and 13
MILP problems (67 instances in total). NLP4LP problems
are drawn from textbooks and lecture notes on optimiza-
tion (Bertsimas & Tsitsiklis, 1997b; Williams, 2013; Nace,
2020), including facility location, network flow, schedul-
ing, portfolio management, and energy optimization prob-
lems. These resources were created before 2021, so it is
possible parts of these books have been used to train LLMs.
However, none of these textbooks includes code. More-
over, our results show that LLMs still find it challenging
to formulate and solve these problems. For each instance,
NLP4LP includes the description, a sample parameter data
file, and the optimal value, obtained either from the text-
book solution manual or by solving the instance by hand.
Together, NLP4LP and ComplexOR offer a variety of chal-
lenging optimization problems with different lengths, facil-
itating the research on automated optimization modeling.
4.2. Overall Performance
To evaluate the overall performance of OptiMUS, we com-
pare it with standard prompting, Reflexion, and Chain-of-
Experts (CoE) (Shinn et al., 2023; Anonymous, 2024). Re-
flexion is the highest-performing general-purpose frame-
work and CoE is the state-of-the-art method for natural-
language optimization modeling. Three main metrics have
been used in the literature: accuracy, compilation error
(CE) rate, and runtime error (RE) rate. However, a method
can generate a totally irrelevant short code that runs, or fix
runtime and complication errors by completely removing
relevant sections of the code. Hence, we only compare the
models’ accuracy. Results are presented in Table 2. Op-
tiMUS outperforms all other methods in all datasets by a
large margin. This remarkable performance improvement
highlights the importance of modularity and structure com-
pared to a single prompt to solve complex problems using
LLMs. The next experiments clarify which features of Op-
tiMUS contribute to its good performance.
4.3. Ablation Study
Table 3 shows the impact of debugging and of the choice
of LLM on the performance of OptiMUS. One interesting
observation is the significant performance drop that occurs
when smaller LLMs are used instead of GPT-4. The first
reason is that the OptiMUS prompts are on average longer
than the other methods and involve more complicated rea-
soning. Smaller LLMs are worse at reasoning (Wang et al.,
2023; OpenAI, 2023). The second reason is the novel and
modular structure of OptiMUS’s prompts. Prompts used in
the other methods mostly adhere to a questions answering
format that is abundant in the public domain (e.g. post-
ing the whole bogus code snippet and asking for the cor-
6


--- Page 7 ---
Large Language Model for Optimization Modeling
Table 2. Performance of OptiMUS and the baselines using GPT4.
NL4OPT
ComplexOR
NLP4LP
Standard
47.3%
9.5%
35.8%
Reflexion
53%
19.1%
46.3%
CoE
64.2%
38.1%
53.1%
OptiMUS (Ours)
78.8%
66.7%
72.0%
Table 3. Ablation study of OptiMUS.
NL4OPT
ComplexOR
NLP4LP
OptiMUS (GPT-4)
78.7%
66.7%
71.6 %
w/o debugging
72.3%
57.1%
58.2%
w/ GPT-3.5 Mngr
74.9%
52.4%
53.7%
w/ GPT-3.5
28.6%
9.5%
14.4%
w/ Mixtral-8x7B
6.6%
0.0%
3.0%
rect version is common on StackOverflow, or writing the
whole problem description and then the complete formula-
tion is common in optimization textbooks). However, in
OptiMUS, the prompts are more complex and not com-
mon in human-human interactions. Smaller LLMs have
limited generalization and reasoning abilities and, there-
fore, show poor performance on such prompts (OpenAI,
2023). Fine-tuning smaller models on these novel prompt
templates might improve their performance and reduce the
cost of running a system like OptiMUS.
We also evaluated a version of OptiMUS which uses GPT-
3.5 for the manager and GPT-4 for the other agents. We
can see that in NL4OPT the difference in performance is
small. The reason is that most instances of NL4OPT are
solved with a simple chain of formulation-programming-
evaluation. However, in ComplexOR and NLP4LP where
more complicated interactions between agents are required,
the manager’s importance becomes more visible. More-
over, we did experiments in which the debugging feature
of the programmer agent was disabled. Similarly to the
manager, we see that debugging is more important in more
complicated datasets.
4.4. Sensitivity Analysis
Figure 5 shows how the maximum number of times the
manager is allowed to select agents affects the accuracy.
For NL4OPT, most problems are solved by selecting each
of the formulator, programmer, and evaluator agents only
once. However, for ComplexOR and NLP4LP, OptiMUS
often makes mistakes at the beginning and iteratively fixes
them by selecting the other agents multiple times.
Section 4.5 shows the number of times each agent is se-
lected per instance. As expected, the average selection fre-
quency is higher in ComplexOR and NLP4LP. Moreover,
3
4
5
6
7
8
9
10
0
0.2
0.4
0.6
0.8
1
Maximum number of agent calls
Accuracy
NL4OPT
ComplexOR
NLP4LP
Figure 5. OptiMUS can solve more problems on difficult datasets
(ComplexOR, NLP4OPT) when more agent calls are allowed,
demonstrating the importance of self-improvement.
programmer and evaluator agents are selected more often
than the formulator. This bias is reasonable:
• Coding errors are more common. LLMs often generate
code with trivial bugs that are easy to fix. In OptiMUS,
the programmer agent fixes such bugs.
• Coding errors are easier to identify and fix. In contrast,
identifying bugs in the formulation require deeper rea-
soning and is harder. Hence the manager in OptiMUS
is prompted to prioritize fixing the code before consid-
ering errors in the formulation. The formulator is only
selected for debugging if the programmer claims that the
code is correct.
Hence in our experiments, we observe the programmer is
selected more often than the formulator.
Table 4 shows the average prompt length of OptiMUS
and CoE for different data sets. Observe that the prompt
length for OptiMUS barely changes across datasets, while
the prompt length for CoE increases on more challenging
datasets. The reason is the modular approach, which allows
OptiMUS to extract and process only the relevant context
for each LLM call. Unlike non-modular methods, Opti-
MUS can scale to larger and longer problems.
4.5. Failure Cases
To understand its strengths and weaknesses, we analyze the
most common reasons why OptiMUS fails (Table 5). We
categorize failure cases into the following groups:
• Missing or wrong constraints:
OptiMUS generates
wrong constraint in the preprocessing step (e.g.,
price ≥0 where price is a parameter), or fails to ex-
tract all of the constraints from the description.
• Incorrect model: OptiMUS tackles the problem with
7


--- Page 8 ---
Large Language Model for Optimization Modeling
Formulator
Programmer
Evaluator
0
1
2
3
Average calls per instances
NL4OPT
ComplexOR
NLP4LP
Figure 6. Average number of calls to each agent among solved
problems. OptiMUS only requires one call per agent on the sim-
ple problems of NL4OPT. On the more complex datasets, it relies
more heavily on the programmer to fix errors identified by the
evaluator, but rarely improves by fixing formulation errors.
an incorrect mathematical model (e.g., defining binary
variables for visiting cities instead of links in TSP).
• Coding error: OptiMUS does not generate error-free
code even after debugging. Coding errors often occur
when the LLM is confused by the language used (e.g.,
in the “prod” problem in ComplexOR, the description
explicitly refers to “parameters” and “variables”).
We normalize the failure rates to sum to 1.0. Incorrect mod-
eling is more common on datasets with more complicated
problems, while on the easier dataset NLP4OPT, the model
is less likely to be wrong.
Understanding and interpreting the problems is also chal-
lenging for LLMs, resulting in formulations with missing
constraints and wrong constraints. Fine-tuning might im-
prove the performance of LLMs on this task, and is an im-
portant direction for future research.
5. Conclusion
How can LLMs collaborate and divide work in order to
achieve complex goals? This paper interrogates this ques-
tion in the domain of optimization and showcases the im-
portance of modular structure. We develop OptiMUS, a
modular LLM-based agent designed to formulate and solve
optimization problems from natural language descriptions.
Our research serves as a proof-of-concept, illustrating the
potential for automating various stages of the optimiza-
tion process by combining LLMs with traditional solvers.
To showcase the performance of OptiMUS, we released
NLP4LP, a dataset of long and challenging optimization
problems to demonstrate the efficacy of the techniques im-
plemented within OptiMUS. OptiMUS achieves SOTA per-
formance across all existing datasets, and scales to prob-
Table 4. CoE requires longer prompts on difficult datasets, while
OptiMUS barely increases its prompt length.
NL4OPT
ComplexOR
NLP4LP
CoE
2003 ± 456
3288 ± 780
3825 ± 1002
OptiMUS
2838 ± 822
3241 ± 1194
3146 ± 1145
Table 5. When OptiMUS fails, why?
Mistake
NL4OPT
ComplexOR
NLP4LP
Incorrect modeling
43.0%
62.5%
53.8%
Missing constraints
36.0%
12.6%
15.4%
Coding errors
21.0%
24.9%
30.8%
lems with large amounts of data and long descriptions.
Real-world optimization problems are often complex and
multifaceted. Developing LLM-based solutions for these
problems requires domain-specific considerations, includ-
ing integrating existing optimization techniques to lever-
age problem structure. We are at the early stages of this
research, but anticipate significant developments that will
enable these systems to address more complex, industrial-
level problems. It is interesting to notice that the challenge
of using AI for an applied domain is much larger in safety-
critical domains such as self-driving, which demand ex-
tremely high accuracy, than in domains where AI can func-
tion as an assistant and where answers are easy to check, as
in theorem-proving or optimization. Here, AI systems with
moderate accuracy can still usefully augment human effort.
Future directions.
Smaller LLMs are faster and cheaper,
but our experiments indicate that they perform poorly in
optimization modeling out-of-the-box. Identifying which
prompts might benefit from fine-tuned small models and
which require large (and expensive) LLM calls is an im-
portant topic for future research. Furthermore, we believe
that integrating user feedback into the process can improve
the performance of agents on natural-language optimiza-
tion modeling. Studying interactions between such agents
and their users is an exciting avenue. Another important
direction is to automatically select the best solver based on
a comprehensive evaluation of both accuracy and runtime
requirements. Additionally, it would be interesting to see
how the modular LLM structure presented here can be en-
hanced using reinforcement learning to teach the manager
how to choose the next agent.
6. Impact Statement
This paper presents work whose goal is to advance the field
of optimization modeling. There are many potential soci-
8


--- Page 9 ---
Large Language Model for Optimization Modeling
etal consequences of our work, none which we feel must be
specifically highlighted here.
References
Aastrup, J. and Kotzab, H.
Forty years of out-of-stock
research–and shelves are still empty. The International
Review of Retail, Distribution and Consumer Research,
20(1):147–164, 2010.
Alibaba Cloud. Alibaba cloud mindopt copilot, 2022. URL
https://opt.alibabacloud.com/chat.
Anonymous. Chain-of-experts: When LLMs meet com-
plex operations research problems. In The Twelfth In-
ternational Conference on Learning Representations,
2024. URL https://openreview.net/forum?
id=HobyL1B9CZ.
Antoniou, A. and Lu, W.-S.
Practical optimization:
algorithms and engineering applications, volume 19.
Springer, 2007.
Beale, E. and Forrest, J. J. Global optimization using spe-
cial ordered sets. Mathematical Programming, 10:52–
69, 1976.
Bertsimas, D. and Tsitsiklis, J. N. Introduction to linear
optimization, volume 6. Athena scientific Belmont, MA,
1997a.
Bertsimas, D. and Tsitsiklis, J. N. Introduction to linear
optimization, volume 6. Athena scientific Belmont, MA,
1997b.
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Ruther-
ford, E., Millican, K., Van Den Driessche, G. B.,
Lespiau, J.-B., Damoc, B., Clark, A., et al.
Improv-
ing language models by retrieving from trillions of to-
kens. In International conference on machine learning,
pp. 2206–2240. PMLR, 2022.
Chen, H., Constante-Flores, G. E., and Li, C. Diagnosing
infeasible optimization problems using large language
models. arXiv preprint arXiv:2308.12923, 2023.
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton,
C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,
S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,
N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B.,
Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari,
G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev,
S., Michalewski, H., Garcia, X., Misra, V., Robinson,
K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim,
H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D.,
Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pel-
lat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov,
O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M.,
Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck,
D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling
language modeling with pathways, 2022.
Gamrath, G., Berthold, T., Heinz, S., and Winkler, M.
Structure-based primal heuristics for mixed integer pro-
gramming. Springer, 2016.
Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang,
Y., Callan, J., and Neubig, G. Pal: Program-aided lan-
guage models. In International Conference on Machine
Learning, pp. 10764–10799. PMLR, 2023.
Gurobi
Optimization.
2023
state
of
mathe-
matical
optimization
report,
2023.
URL
https://www.gurobi.com/resources/
report-state-of-mathematical-optimization-2023/.
Holzer, J., Coffrin, C., DeMarco, C., Duthu, R., El-
bert, S., Eldridge, B., Elgindy, T., Greene, S., Guo,
N., Hale, E., Lesieutre, B., Mak, T., McMillan,
C., Mittelmann, H., Oh, H., O’Neill, R., Overbye,
T., Palmintier, B., Safdarian, F., Tbaileh, A., Hen-
tenryck, P. V., Veeramany, A., and Wert, J.
Grid
optimization competition challenge 3 problem for-
mulation.
https://gocompetition.energy.
gov/sites/default/files/Challenge3_
Problem_Formulation_20230126.pdf, 2023.
Accessed: Access Date.
Li, B., Mellou, K., Zhang, B., Pathuri, J., and Menache,
I. Large language models for supply chain optimization.
arXiv preprint arXiv:2307.03875, 2023.
Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua,
M., Petroni, F., and Liang, P. Lost in the middle: How
language models use long contexts, 2023.
Nace,
D.
Lecture
notes
in
linear
programming
modeling,
2020.
URL
https://www.hds.
utc.fr/˜dnace/dokuwiki/_media/fr/
lp-modelling_upt_p2021.pdf.
OpenAI. Gpt-4 technical report, 2023.
Paranjape, B., Lundberg, S., Singh, S., Hajishirzi, H.,
Zettlemoyer, L., and Ribeiro, M. T.
Art: Automatic
multi-step reasoning and tool-use for large language
models. arXiv preprint arXiv:2303.09014, 2023.
Ramamonjison et al, .
Augmenting operations research
with auto-formulation of optimization models from
problem descriptions. In Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Language
Processing:
Industry Track, pp. 29–62, Abu Dhabi,
UAE, December 2022. Association for Computational
Linguistics. URL https://aclanthology.org/
2022.emnlp-industry.4.
9


--- Page 10 ---
Large Language Model for Optimization Modeling
Ramamonjison et al, .
Nl4opt competition: Formulat-
ing optimization problems based on their natural lan-
guage descriptions, 2023.
URL https://arxiv.
org/abs/2303.08233.
Saghafian, S., Austin, G., and Traub, S. J. Operations re-
search/management contributions to emergency depart-
ment patient flow optimization: Review and research
prospects. IIE Transactions on Healthcare Systems En-
gineering, 5(2):101–123, 2015.
Shakoor, R., Hassan, M. Y., Raheem, A., and Wu, Y.-K.
Wake effect modeling: A review of wind farm layout
optimization using jensen’ s model. Renewable and Sus-
tainable Energy Reviews, 58:1048–1059, 2016.
Shinn, N., Cassano, F., Berman, E., Gopinath, A.,
Narasimhan, K., and Yao, S.
Reflexion: Language
agents with verbal reinforcement learning, 2023.
Singh, A. An overview of the optimization modelling ap-
plications. Journal of Hydrology, 466:167–182, 2012.
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,
Azhar, F., et al.
Llama: Open and efficient founda-
tion language models. arXiv preprint arXiv:2302.13971,
2023.
Wang, Y., Ivison, H., Dasigi, P., Hessel, J., Khot, T.,
Chandu, K. R., Wadden, D., MacMillan, K., Smith,
N. A., Beltagy, I., et al. How far can camels go? ex-
ploring the state of instruction tuning on open resources.
arXiv preprint arXiv:2306.04751, 2023.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,
Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thought
prompting elicits reasoning in large language models,
2023.
Williams, H. P. Model building in mathematical program-
ming. John Wiley & Sons, 2013.
Wu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E.,
Li, B., Jiang, L., Zhang, X., and Wang, C. Autogen:
Enabling next-gen llm applications via multi-agent con-
versation framework. arXiv preprint arXiv:2308.08155,
2023.
Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou,
D., and Chen, X. Large language models as optimizers,
2023.
Yao, E., Liu, T., Lu, T., and Yang, Y.
Optimization of
electric vehicle scheduling with multiple vehicle types
in public transport. Sustainable Cities and Society, 52:
101862, 2020.
10


--- Page 11 ---
Large Language Model for Optimization Modeling
I have an optimization problem and here’s the description 
I have a network with a set of edges and vertices. The network 
has a source and a sink, and I also have information of the edge 
connectivity, say C_i = 1 if vertex i and j are connected……
formulated as follows 
We know there is a type of mixed integer programming satisfying 
totally unimodularity", which we describe as follows  
Integer programming problems satisfying total unimodularity often 
come from the following background. 
1. Network Flow Problems: The constraint matrices of network 
flow problems, such as the maximum flow problem and the 
minimum cost flow problem, are totally unimodular. 
2. Assignment Problems: The constraint matrices for assignment 
problems, where tasks are to be assigned to agents 
3. ……
If the problem satisfies the aforementioned structure, please update the 
formulation accordingly
Pool of modeling techniques
SOS
Indicator
Abs
Pool of structures
TU
……
SAT
Figure 7. OptiMUS iterates through a pool of advanced optimization techniques
A. Optimization Techniques
Optimization solvers exploit problem-specific structure to improve performance when solving MILPs (Gamrath et al.,
2016) and often provide a customized interface for these special structures. Using the interface not only reduces the
complexity of (and potential for errors in) auxiliary variables or constraints, but also informs the solver about the existence
of structure that can be exploited to solve the problem faster. Moreover, the solver’s performance can suffer when these
structures are not signaled in the model. For example, a bad choice of big-M coefficient when reformulating an indicator
variable can reduce the strength of the linear relaxation. Typical examples of structure include Special Ordered Set (SOS)
(Beale & Forrest, 1976), indicator variables, and general constraints (Bertsimas & Tsitsiklis, 1997a).
Although state-of-the-art optimization solvers can detect some problem structures automatically, it works better to specify
structure during problem formulation. Hence the formulator is prompted to leverage advanced optimization techniques
and structures, including 1) Special Ordered Set. 2) Indicator variable. 3) General constraints. 4) SAT and constraint
programming problem. 5) Totally unimodular problem detection.
OptiMUS iterates through a sequence of “cheatsheet” prompts (Figure 7), each corresponding to one of these structures.
Within each prompt, the LLM is provided with the description of the structure, explained by an example illustrating how the
structure should be exploited. The LLM is asked to decide whether the structure can be applied to the existing formulation.
Upon identifying the appropriate structure, the formulation is adjusted to utilize the customized solver interface when
available.
11


--- Page 12 ---
Large Language Model for Optimization Modeling
B. prompts
B.1. Manager Prompt
You're a manager in a team of optimization experts. The goal of the team is to solve an optimization
problem. Your task is to choose the next expert to work on the problem based on the current situation.
- The user has already given us the problem description, the objective function, and the parameters. Only
call the user proxy if there is a problem or something ambiguous or missing.
Here's the list of agents in your team:
{agents}
And here's the history of the conversation so far:
{history}
Considering the history, if you think the problem is solved, type DONE. Otherwise, generate a json file
with the following format:
{{
"agent_name": "Name of the agent you want to call next",
"task": "The task you want the agent to carry out"
}}
to identify the next agent to work on the problem, and also the task it has to carry out.
- If there is a runtime error, ask the prorammer agent to fix it.
- Only generate the json file, and don't generate any other text.
- If the latest message in history says that the code is fixed, ask the evaluator agent to evaluate the
code!
12


--- Page 13 ---
Large Language Model for Optimization Modeling
B.2. Formulation generation prompt
You are an expert mathematical formulator and an optimization professor at a top university. Your task is to model
{clausType} of the problem in the standard LP or MILP form.
Here is a {clausType} we need you to model:
{targetDescription}
Here is some context on the problem:
{background}
Here is the list of available variables:
{variables}
And finally, here is list of input parameters:
{parameters}
First, take a deep breath and explain how we should define the {clausType}. Feel free to define new variables if you
think it is necessary. Then, generate a json file accordingly with the following format (STICK TO THIS FORMAT!):
{{
"{clausType}": {{
"description": "The description of the {clausType}",
"formulation": "The LaTeX mathematical expression representing the formulation of the {clausType}"
}},
"auxiliary_constraints": [
{{
"description": "The description of the auxiliary constraint",
"formulation": "The LaTeX mathematical expression representing the formulation of the auxiliary constraint"
}}
]
"new_variables": [
{{
"definition": "The definition of the variable",
"symbol": "The symbol for the variable",
"shape": [ "symbol1", "symbol2", ... ]
}}
],
}}
- Your formulation should be in LaTeX mathematical format (do not include the $ symbols).
- Note that I'm going to use python json.loads() function to parse the json file, so please make sure the format is
correct (don't add ',' before enclosing '}}' or ']' characters.
- Generate the complete json file and don't omit anything.
- Use '```json' and '```' to enclose the json file.
- Important: You can not define new parameters. You can only define new variables.Use CamelCase and full words for
new variable symbols, and do not include indices in the symbol (e.g. ItemsSold instead of itemsSold or items_sold or
ItemsSold_i)
- Use \\textup{{}} when writing variable and parameter names. For example (\\sum_{{i=1}}^{{N}}
\\textup{{ItemsSold}}_{{i}} instead of \\sum_{{i=1}}^{{N}} ItemsSold_{{i}})
- Use \\quad for spaces.
- Use empty list ([]) if no new variables are defined.
- Always use non-strict inequalities (e.g. \\leq instead of <), even if the constraint is strict.
- Define auxiliary constraints when necessary. Set it to an empty list ([]) if no auxiliary constraints are needed.
If new auxiliary constraints need new variables, add them to the "new_variables" list too.
Take a deep breath and solve the problem step by step.
13


--- Page 14 ---
Large Language Model for Optimization Modeling
B.3. Formulation fixing prompt
You are a mathematical formulator working with a team of optimization experts. The objective is to tackle a complex
optimization problem, and your role is to fix a previously modelled {target}.
Recall that the {target} you modelled was
{constraint}
and your formulation you provided was
{formulation}
The error message is
{error}
Here are the variables you have so far defined:
{variables}
Here are the parameters of the problem
{parameters}
Your task is carefully inspect the old {target} and fix it when you find it actually wrong.
After fixing it modify the formulation. Please return the fixed JSON string for the formulation.
The current JSON is
{json}
Take a deep breath and solve the problem step by step.
14


--- Page 15 ---
Large Language Model for Optimization Modeling
B.4. Clause Coding prompt
You're an expert programmer in a team of optimization experts. The goal of the team is to solve an
optimization problem. Your responsibility is to write {solver} code for different {target}s of the problem.
Here's a {target} we need you to write the code for, along with the list of related variables and
parameters:
{context}
- Assume the parameters and variables are defined, and gurobipy is imported as gp. Now generate a code
accordingly and enclose it between "=====" lines.
- Only generate the code and the ===== lines, and don't generate any other text.
- If the {target} requires changing a variable's integrality, generate the code for changing the variable's
integrality rather than defining the variable again.
- If there is no code needed, just generate the comment line (using # ) enclosed in ===== lines explaining
why.
- Variables should become before parameters when defining inequality {target}s in gurobipy (because of the
gurobi parsing order syntax)
Here's an example:
**input**:
{{
"description": "in month m, it is possible to store up to storageSize_{{m}} tons of each raw oil for use
later.",
"formulation": "\(storage_{{i,m}} \leq storageSize, \quad \\forall i, m\)",
"related_variables": [{{
"symbol": "storage_{{i,m}}",
"definition": "quantity of oil i stored in month m",
"shape": [
"I",
"M"
]
}}],
"related_parameters": [{{
"symbol": "storageSize_{{m}}",
"definition": "storage size available in month m",
"shape": [
"M"
]
}}]
}}
***output***:
=====
# Add storage capacity constraints
for i in range(I):
for m in range(M):
model.addConstr(storage[i, m] <= storageSize[m], name="storage_capacity")
=====
Take a deep breath and approach this task methodically, step by step.
15


--- Page 16 ---
Large Language Model for Optimization Modeling
B.5. Variable coding prompt
You're an expert programmer in a team of optimization experts. The goal of the team is to solve an
optimization problem. Your responsibility is to write {solver} code for defining variables of the problem.
Here's a variable we need you to write the code for defining:
{variable}
Assume the parameters are defined. Now generate a code accordingly and enclose it between "=====" lines.
Only generate the code, and don't generate any other text. Here's an example:
**input**:
{{
"definition": "Quantity of oil i bought in month m",
"symbol": "buy_{{i,m}}",
"shape": ["I","M"]
}}
***output***:
=====
buy = model.addVars(I, M, vtype=gp.GRB.CONTINUOUS, name="buy")
=====
- Note that the indices in the symbol (what comes after _) are not a part of the variable name in code.
- Use model.addVar instead of model.addVars if the variable is a scalar.
Take a deep breath and solve the problem.
16


--- Page 17 ---
Large Language Model for Optimization Modeling
B.6. Debugging prompt
You're an expert programmer in a team of optimization experts. The goal of the team is to solve an
optimization problem. Your responsibility is to debug the code for the problem.
When running the following code snippet, an error happened:
{context_code}
{error_line}
and here is the error message:
{error_message}
We know that the code for importing packages and defining parameters and variables is correct, and the
error is because of the this last part which is for modeling the {target}:
{error_line}
First reason about the source of the error. Then, if the code is correct and the problem is likely to be in
the formulation, generate a json in this format (the reason is why you think the problem is in the
formulation):
{{
"status": "correct",
"reason": "A string explaining why you think the problem is in the formulation"
}}
otherwise, fix the last part code and generate a json file with the following format:
{{
"status": "fixed",
"fixed_code": "A sting representing the fixed {target} modeling code to be replaced with the last part
code"
}}
- Note that the fixed code should be the fixed version of the last part code, not the whole code snippet.
Only fix the part that is for modeling the {target}.
- Do not generate any text after the json file.
- Variables should become before parameters when defining inequality constraints in gurobipy (because of
the gurobi parsing order syntax)
- The parameter shapes are parameters definitions are correct.
Take a deep breath and solve the problem step by step.
17
