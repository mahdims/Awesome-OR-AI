--- Page 1 ---
AutoRNet: Automatically Optimizing Heuristics for Robust Network Design via
Large Language Models
He Yua,b, Jing Liua,b
aSchool of Artificial Intelligence, Xidian University, 2 South Taibai Road, Xi’an, 710071, Shaanxi, China
bGuangzhou Institute of Technology, Xidian University, Knowledge City, Guangzhou, 510555, Guangdong, China
Abstract
Achieving robust networks is a challenging problem due to its NP-hard nature and the vast, complex, high-dimensional
solution space. Current methods, from handcrafted feature extraction to deep learning approaches, have made certain
advancements but remain rigid and complex, often requiring manual design, trial and error, and large amounts of
labeled data. To deal with these problems, we propose AutoRNet, a novel framework that integrates large language
models (LLMs) with evolutionary algorithms to automatically generate complete heuristics for robust network design.
With the intrinsic properties of robust network structure in mind, effective network optimization strategy-based varia-
tion operations are designed to provide domain specific prompts for LLMs to help them make use of domain knowl-
edge to generate advanced complete heuristics. Moreover, to deal with the difficulty brought by the hard constraint
of maintaining degree distributions, an adaptive fitness function, which can progressively strengthening constraints
to balance convergence and diversity, is designed. We evaluate the robustness of networks generated by AutoRNet’s
heuristics on both sparse and dense initial scale-free networks. These solutions outperform those from current meth-
ods. AutoRNet reduces the need for manual design and large datasets, offering a more flexible and adaptive approach
for generating robust network structures.
Keywords: evolutionary algorithms, large language models, complex network, network robustness, prompt
engineering, deep learning
1. Introduction
Modern networked systems form the backbone of contemporary society. Understanding the robustness of these
networks is crucial for ensuring stability and reliability [1, 2, 3]. Improving network robustness to prevent catastrophic
disruptions is inherently challenging due to its NP-hard nature, and related research has seen significant advance-
ments. Key contributions include approximate theoretical models [4], which simplify the complex interactions within
networks to provide insights into enhancing resilience. Optimization algorithms, employing approaches such as sim-
ulated annealing (SA) [5], genetic algorithms (GAs) [6], and greedy approaches [7], offer near-optimal solutions in
a reasonable time frame. In addition, machine learning techniques, particularly deep reinforcement learning[8], have
been used to dynamically adapt and improve network configurations. However, these methods highly depend on
manual work, expert knowledge, training data, and trial-and-error processes.
The emergence of coding-oriented Large Language Models (LLMs)[9] has garnered significant attention for their
potential in addressing combinatorial optimization problems [10, 11]. FunSearch [12] combines a pre-trained LLM
with evolutionary algorithms (EAs) to evolve initial low-scoring programs into high-scoring ones. Evolution of
Heuristics (EoH) [13] co-evolves both heuristic descriptions and their corresponding code implementations. How-
ever, both FunSearch and EoH depend primarily on existing optimization algorithms, only scoring or weighting
mechanisms influencing the search algorithm behavior are designed by LLMs to guide the related operations, without
creating a new algorithm, hereby limiting their applicability to complex domain-specific problems, such as network
robustness. There is a need to generate whole algorithms directly rather than merely modifying data through weight-
ing and scoring.
In this paper, we address these issues and make a significant step by proposing AutoRNet, an integrated frame-
work that combines the contextual intelligence of LLMs with the adaptive optimization capabilities of EAs. Specifi-
Preprint submitted to Elsevier
October 24, 2024
arXiv:2410.17656v1  [cs.AI]  23 Oct 2024


--- Page 2 ---
cally, we design Network Optimization Strategy(NOS)-based variation operations tailored for complex network prob-
lems, which can create domain-specific prompts for LLMs, generating a variety of heuristics suitable for different
network-related challenges. Moreover, we design an Adaptive Fitness Function (AFF) to evaluate these heuristics,
progressively tightening constraints to balance the convergence and diversity, thereby discovering superior heuristics.
AutoRNet can automatically make use of special network characteristics to design effective operations. The major
contributions of this paper are summarized as follows:
• A hybrid framework is developed where EAs and LLMs iteratively collaborate to generate and refine heuristics
for network robustness.
• NOS-based variation operations are designed, which can generate problem-specific prompts to guide LLMs in
divergent thinking. These NOSs are equally applicable to other network-related challenges.
• AFF tailored to network issues is designed, to transform hard constraints into soft ones, enhancing the diversity
and resilience of heuristics.
• Solutions generated by AutoRNet’s heuristics are evaluated across eight scale-free networks with varying sizes
and densities and a real-world network, demonstrating that they outperform current methods.
The remainder of this paper is organized as follows. Sections 2 and 3 introduce the related work and network
robustness measures, respectively. Section 4 introduces AutoRNet in details. Section 5 presents the experiments.
Finally, Section 6 concludes the paper with a summary of our findings.
2. Related Work
2.1. Related Work on Improving Network Robustness
Traditional methods for improving network robustness focus on improving local redundancy and connectivity,
such as maximizing clustering coefficient to form tightly knit groups of nodes. These methods also leverage high-
order network structures like triangles and quadrilaterals to identify and protect critical nodes and edges to withstand
targeted attacks. Techniques like greedy and local search algorithms systematically improve network robustness.
Fortunato et al.[14] proposed a greedy algorithm that improves the robustness of social networks by forming tightly-
knit communities. Zhang et al.[15] proposed a lazy-greedy algorithm that enhances the robustness of transportation
networks by protecting the most critical nodes from failure. Hau Chan et al.[16] introduced a local search algorithm
by iteratively improving the subgraph structure to improve the robustness of the network.
Metaheuristic algorithms, including Genetic Algorithms(GAs)[17], and Simulated Annealing(SA)[18], have also
been extensively employed to solve network robustness problems. Pizzuti et al.[19] proposed using GAs to enhance
the robustness of complex networks by simulating the process of natural selection to evolve network configurations
over generations, improving resilience against failures and attacks. Zhou et al.[6] proposed using Memetic Algo-
rithms, which combine global and local search strategies, to enhance the robustness of scale-free networks by in-
tegrating global and local search operators. Buesser et al.[5] proposed the use of SA to optimize the robustness of
scale-free networks by rewiring the network edges, thus improving the resilience of the network to fragmentation and
intentional damage. Pizzuti et al.[20] introduced RobGA, a GA designed to enhance network robustness by adding
edges in a way that minimizes disruption risk.
However, these methods often require manual design and multiple trial-and-error processes, making them ineffi-
cient and time-consuming. The need for extensive experimentation and tuning reduces their practicality for large-scale
and dynamic network environments, highlighting the need for more automated and adaptive approaches to network
robustness optimization.
Deep learning, particularly Graph Neural Networks (GNNs), has emerged as a powerful approach to address
network robustness problems. Tang et al.[21] explored methods to improve the robustness of GNNs against poisoning
attacks by leveraging clean graphs from similar domains. This approach helps the GNNs detect adversarial edges
more effectively, enhancing their resistance to attacks. Wang et al.[22] developed certifiably robust GNNs to defend
against attacks that perturb the graph structure by adding or deleting edges. Their approach ensures that the GNNs’
performance remains stable even under adversarial conditions.
2


--- Page 3 ---
Although GNN-based methods for network robustness are powerful, they face several challenges. These models
require large amounts of labeled data for training, which can be difficult to obtain in practice. The high computational
demands for training and inference can also be a limitation, especially in real-time applications. Moreover, GNNs can
still be vulnerable to sophisticated adversarial attacks, where small, carefully crafted perturbations in the input data
can lead to incorrect classifications.
2.2. The Application of LLMs in Combinatorial Optimization
LLMs have shown a significant impact in addressing various problems. One prevalent approach involves engi-
neering in-context learning prompts (EILP)[23] using techniques such as zero-shot, few-shot, and chain-of-thought
(CoT) prompting[24, 25, 26]. Enhancing EILP with fine-tuning further improves response accuracy by adapting LLMs
to specific datasets or tasks. Furthermore, combining EILP with ensemble learning techniques increases robustness
and consistency by aggregating multiple model predictions. Integration with Retrieval-Augmented Generation (RAG)
[27] uses external knowledge sources, allowing LLMs to generate more accurate and informed contextual responses.
Many researchers have also focused on solving combinatorial optimization problems using LLMs. Shengcai Liu
et al.[28] presented the first investigation into LLMs as evolutionary combination optimizers for solving the Travelling
Salesman Problem (TSP). FunSearch[12] leverages LLMs to generate and optimize mathematical functions, discov-
ering new constructions and improving existing solutions by iteratively refining function constructions. EoH[13]
facilitates the simultaneous evolution of concept description and code implementations, emulating the process by
which humans develop heuristics, to achieve efficient automatic heuristic design. These approaches have demon-
strated competitive performance compared to traditional heuristics in finding high-quality solutions.
All of these methods are based on existing algorithms, scoring or weighting mechanisms are used to modify the
original data (e.g., bin capacities, city distances), influencing the search algorithm behavior but without creating a new
algorithm. There is a need to generate whole algorithms directly rather than merely modifying data through weighting
and scoring. These approaches restrict their optimization improvements to relatively simple combinatorial problems
and cannot extend their applicability to more complex or domain-specific optimization scenarios.
3. Network Robustness Measures
A network is often represented as a graph G = (V, E), where V denotes the set of nodes and E represents the set
of edges. To measure network robustness, several methodologies have been developed. [3] introduced a measure R to
assess network robustness by evaluating the size of the largest connected component during sequential node attacks,
R = 1
N
N
X
q=1
s(q)
(1)
where N is the number of nodes in the network and s(q) is the fraction of nodes in the largest connected cluster after
removing q nodes. The normalization factor 1/N ensures that the robustness of networks with different sizes can be
compared. The range of R values is between 1/N and 0.5.
Complex networks can exhibit different topological structures, with random and scale-free networks being two
of the most studied types. Random networks, characterized by a homogeneous degree distribution, are robust to tar-
geted attacks on high-degree nodes. In contrast, scale-free networks, with a power-law degree distribution, exhibit
exceptional robustness against random failures due to the low probability of removing a hub but are extremely sus-
ceptible to targeted attacks that focus on their few high-degree hubs. These networks display distinct characteristics
that significantly impact their robustness and vulnerability to failures or attacks.
In this paper, we adopt R to evaluate network robustness, specifically focusing on targeted attacks through the
sequential removal of the highest-degree nodes. We employ scale-free networks for both training and testing graph
sets to accurately reflect the structure of many real-world networks.
3


--- Page 4 ---
Figure 1: A schematic illustration of AutoRNet
4. AutoRNet
4.1. The Framework of AutoRNet
AutoRNet uses EAs to search heuristics and maintains a population of popsize individuals, denoted as P =
{h1, h2, . . . , hpopsize}. Each individual hi, i = 1, 2, . . . , popsize, includes a heuristic. There are T generations in to-
tal, with Pt = {ht,1, ht,2, . . . , ht,popsize} representing the population at generation t. AutoRNet evolves the population
generation by generation, and the whole framework is summarized in Algorithm 1.
First, InitializePopulation() initializes the initial population P1 of popsize individuals using a task specification and
prompt interaction with the LLM. Then, based on the training graph set G, AFF() uses the adaptive fitness function,
introduced in the following text, to calculate the fitness of each individual in P1. Next, the population is evolved T
generations, and in each generation, NOS_Variation() first conducts the NOS-based variation operations designed in
the following text on the current population, obtaining the offspring population. Then, SelectNextPopulation() uses
the roulette wheel selection according to the fitness to select popsize individuals from Pt and Po f f spring together to
form the population for the next generation. Figure 1 schematically illustrates the framework of AutoRNet.
4


--- Page 5 ---
4.2. Individual Encoding and Population Initialization
Figure 2: Individual structure schematic.
Figure 3: Variation Operation prompt for M1
The individual’s heuristic codes are generated by LLMs. Each individual is structured as follows, which is
schematically illustrated in Figure 2:
• Algorithm Description: A natural language explanation of the heuristic’s objective, providing context and
understanding of what the heuristic aims to achieve.
• Heuristic Code: The actual implementation of the heuristic, detailing the logic and operations needed to im-
prove network robustness.
• Fitness: The fitness of the heuristic evaluated by AFF.
AutoRNet initializes its heuristic population by providing LLMs with a detailed task specification that includes a
description of the problem, function signature, and functionality of the heuristic. The LLM generates initial heuristics
by creating and implementing them as Python code, with the process repeated multiple times to ensure diversity.
Optionally, heuristic seed codes, either simple or expert-designed, are included to guide the LLM towards effective
methods. Existing hand-crafted heuristics can be also incorporated to enhance the initial population with additional
domain knowledge. The detailed population initialization prompts are provided in the Appendix.
4.3. Adaptive Fitness Function
In the EA assisting LLMs for heuristic code generation, individuals are encoded as methods rather than solutions,
and the search is based on LLMs to generate new or similar methods. It does not inherently provide a mechanism to
measure the similarity between methods, making it difficult to navigate the method space. Maintaining a consistent
degree distribution is a constraint in optimizing network robustness. Enforcing this constraint strictly from the outset
can be overly restrictive, leading to a large number of invalid individuals, impeding the evolutionary process.
To overcome these issues, we design an adaptive fitness function (AFF), which can dynamically adjust the degree
distribution constraint throughout the evolutionary process. The AFF initially relaxes this constraint, facilitating
broader exploration of the method space, and then progressively tightens this constraint as the evolution proceeds. By
doing so, AFF improves the ability of AutoRNet to explore various heuristics and avoid dropping into local optima.
5


--- Page 6 ---
AFF uses a training graph set to evaluate the performance of heuristics obtained by each individual adaptively with
the evolution process. The training graph set G = {G1,G2, . . . ,GM} consists of BA scale-free graphs. To calculate the
fitness of ht,i, the heuristic Ht,i of ht,i is first used to optimize G to ˜Gt,i
Ht,i(G) = ˜Gt,i
(2)
where ˜Gt,i = { ˜Gt,i,1, . . . , ˜Gt,i,M}. Here, ˜Gt,i,j denotes the j-th graph obtained by optimizing G j using Ht,i. The AFF
evaluates the fitness of ht,i as follows:
f( ˜Gt,i, t) =
M
X
j=1

R( ˜Gt,i,j) ·

2 −w(t) · Y( ˜Gt,i, j)

(3)
where Y( ˜Gt,i,j) quantifies the deviation of the optimized graph ˜Gt,i, j from the original graph G j, defined as:
Y( ˜Gt,i, j) = Ddiff( ˜Gt,i, j)
Dmax( ˜Gt,i,j) + Ediff( ˜Gt,i,j)
Emax( ˜Gt,i, j)
(4)
where Ddiff( ˜Gt,i,j) is the difference in degree distribution between original graph G j and optimized graph ˜Gt,i,j.
Ddiff( ˜Gt,i,j) = 1
N
N
X
k=1
|dk(G j) −dk( ˜Gt,i, j)|
(5)
The maximum possible deviation in degree distribution Dmax( ˜Gt,i, j) is calculated as follows:
Dmax( ˜Gt,i, j) =
max
k∈{1,2,...,N} |dk(G j) −dk( ˜Gt,i, j)|
(6)
The normalized degree distribution difference Ddiff( ˜Gt,i,j)
Dmax( ˜Gt,i,j) has a value range between 0 and 1.
Similarly, Ediff( ˜Gt,i,j) represents the difference in edge count between the original graph G j and the optimized
graph ˜Gt,i, j. Ediff( ˜Gt,i, j) is defined as:
Ediff( ˜Gt,i,j) = |E(G j) −E( ˜Gt,i,j)|
(7)
where E(G j) and E( ˜Gt,i,j) represent the edge count in the original and optimized graphs, respectively.
Emax( ˜Gt,i, j) is the maximum possible deviation in edge count, calculated as follows:
Emax( ˜Gt,i,j) = max(E(G j), E( ˜Gt,i,j))
(8)
The normalized edge number difference Ediff( ˜Gt,i,j)
Emax( ˜Gt,i,j) has a value range between 0 and 1.
The weight function w(t) increases the penalty on structural deviations as generations progress, defined as:
w(t) =
 t
T
p
(9)
where T is the total number of generations and p is a parameter controlling the rate of increase. Typically, p is chosen
in the range 0.5 ≤p ≤2.
The key idea of f( ˜Gt,i, t) in Equation 3 is: If the optimized ˜Gt,i, j satisfies the consistency constraints for both edge
count and degree distribution (that is, both Ddiff and Ediff are 0), it will receive a reward double the base score (2 ×
R( ˜Gt,i, j)). If these constraints are not met, the penalty for structural deviations increases over successive generations.
Consequently, the fitness value ranges from 0 to 2 × R( ˜Gt,i, j).
6


--- Page 7 ---
4.4. Network Optimization Strategy-based Variation Operations
The heuristic method searching space in the EA assisting LLMs for heuristic code generation is vast, complex,
and high-dimensional. In simpler problem domains, such as those addressed by EoH and FunSearch, this space is
reduced by limiting function codes to straightforward tasks like weighting or scoring data. For complex problems like
network robustness, simplifying the problem is not feasible due to the intricate and domain-specific nature of tasks.
Through experiments we find, to deal with the complex problem like network robustness, by providing just general
prompts without domain knowledge, current LLMs can only design simple operations on nodes or links, and lack
of the ability to make deep use of domain knowledge to design advanced operations. Therefore, it is important to
design mechanism which can provide LLMs domain knowledge effectively to further release LLMs’ ability in design
optimization methods for complex problems.
To cure this problem, we design Network Optimization Strategies (NOSs) with the intrinsic properties of networks
in mind. Networks have features such as degree distribution, path characteristics, clustering coefficient, centrality
measures, and community structure. Based on these features, strategies such as high-degree node priority, shortest
path optimization, and betweenness centrality priority can be designed. Guided by these strategies, actions such as
adding edges, rewiring edges, and swapping edges are then taken. This combination of features, strategies, and actions
forms the NOS. Detailed information is provided in the Appendix.
Based on NOSs, variation operations are designed to generate offspring heuristics by integrating NOSs into
prompts to guide the LLMs with domain knowledge. Three types of variation operations, namely E1, M1, and
M2, are designed. E1 and M1 form the prompt for LLMs by integrating randomly select 12 NOSs with the general
purpose prompts, and M2 just use the general prompts.
E1 (Exploration with NOS Integration): By providing 2 parent individuals, E1 prompts the LLM to create
entirely new heuristics with randomly selecting 12 NOSs, ensuring that the generated offspring are diverse and inno-
vative. E1 help AutoRNet escape local optima by introducing new strategies and methods into the population.
M1 (Guided Modification with NOS): M1 prompts the LLM to refine existing heuristics by incorporating NOSs
leading to targeted improvements and optimizations. This type of variation operation provides guided local search,
leveraging domain-specific knowledge to enhance the effectiveness of the current heuristic.
M2 (Local Adjustment): M2 prompts the LLM to make minor adjustments to existing heuristics, focusing on
small-scale improvements and refinements. This type of variation operation is a pure local search, making incremental
adjustments to optimize the heuristic’s performance.
In each generation, pe1×popsize, pm1×popsize, pm2×popsize individuals are selected from the current population
using the tournament selection to conduct E1, M1, M2, respectively. In this way, prompts of E1, M1, and M2 can be
provided to the LLM server simultaneously. Figure 3 illustrates the prompt schematic for M1, and these for E1 and
M2 are given in the Appendix. By utilizing these three types of variation operations, AutoRNet effectively balances
the need for innovation and refinement, ensuring robust and efficient network optimization.
5. Experiments
AutoRNet generates heuristic methods and we analyze these methods to illustrate the design capabilities of Au-
toRNet. Simultaneously, we select three existing algorithms as baselines: the Hill Climbing Algorithm (HC)[29], the
Simulated Annealing Algorithm (SA)[5], and the Smart Rewiring Algorithm (SR)[30]. R is used to evaluate the ro-
bustness of networks in the test graph set after optimization by all algorithms. By comparing their network robustness,
we assess the effectiveness of the methods generated by AutoRNet.
5.1. Experimental Settings
The training graph set G consists of BA scale-free networks in two different sizes: 50 and 100 nodes. For each
network size, the number of initial nodes M0 varies from 2 to 5. For each combination of network size and M0, we
create three instances, resulting in a total of M = 2 × 4 × 3 = 24 training graphs. The test graph set consists of three
types of networks: sparse BA networks with 100, 200, 300, and 500 nodes, N0 = 3, M0 = 2; BA networks with 100
nodes, N0 = 6, M0 ranging from 2 to 5; and a real world EU power grid network[31] with 1,494 nodes and 2,066
edges.
7


--- Page 8 ---
Each heuristic method performs a series of network modifications, such as edge addition, relocation, and swap-
ping. After each modification, R is used to evaluate the network’s robustness. If R improves, the modification is
accepted; otherwise, it is rolled back. Consequently, the R function is called multiple times to guide the optimization
process. The total number of such evaluations for each heuristic is defined as max_attempts. For the training phase,
max_attempts is set to 100. For the testing phase, max_attempts is set to 3 × 104 for the BA networks and 5 × 104 for
the EU power grid network, which is the same with those of the three baseline algorithms used.
The parameters of AutoRNet were set as follows: popsize, T, pe1, pm1, and pm2 are set to 10, 50, 0.8, 0.1, and 0.1,
respectively. The GPT-4 Turbo model was used with a temperature setting of 1. p in Equation 9 was set to 1.5. The
experiments involving networks with 100 to 300 nodes were conducted over 100 independent runs, for networks with
500 nodes, over 30 independent runs, and for the EU power grid network, over 10 independent runs.
5.2. Evaluation Results
5.2.1. Design Capabilities of AutoRNet
After running AutoRNet for 50 generations, we selected three highly valuable heuristics based on their fitness
values, named as Heuristic-v1, Heuristic-v2 and Heuristic-v3, which significantly improved the network robustness
of the test graph set. Heuristic-v1 leverages network features of critical nodes and similar nodes, Heuristic-v2 utilizes
node connectivity and degree distribution, and Heuristic-v3 optimizes local topology by manipulating edges among
neighbors. All these three heuristics maintain the same number of edges, with Heuristic-v1 also preserving the degree
distribution. This demonstrates AutoRNet’s ability to design complete algorithms that effectively utilize network
features.
Heuristic-v1, shown in Algorithm 2, employs an advanced strategy that combines edge swapping with SA while
preserving the degree distribution. This heuristic identifies critical nodes(those with the highest degrees) and pairs sim-
ilar nodes based on their degree deviation. It dynamically adjusts the max_diff parameter(Step 4), which controls the
tolerance for degree deviation in pairing similar nodes(Steps 22-34). The algorithm iteratively swaps edges between
the neighbors of these paired nodes(Steps 18-19), evaluating new configurations based on robustness improvements
or probabilistic acceptance criteria derived from simulated annealing(Step 22). This sophisticated approach ensures
a balance between exploration and exploitation in the search space. Without NOSs, it would be impossible to evolve
such “intelligent” heuristics that smartly leverage network features for robust network design.
Heuristic–v2 and Heuristic–v3 optimize the network through edge relocation, which, while slightly altering
the degree distribution, significantly improves the robustness of the networks.
Importantly, edge-relocation ac-
tions incur the same real-world costs as edge-swapping. This shows that AutoRNet is not constrained by theoreti-
cal conditions, but instead explores a variety of methods, making it more suitable for solving real-world problems.
8


--- Page 9 ---
Heuristic-v2, shown in Algorithm 3, improves network robustness by redistributing edges between high-degree
and low-degree nodes. The heuristic involves adding an edge between a high-degree node and a low-degree node
while removing an edge from the high-degree node.
Heuristic-v3, shown in Algorithm 4, enhances robustness by redistributing edges among the neighbors of high-
degree nodes. This method identifies high-degree nodes and strategically adds edges between their neighbors, main-
taining the overall edge count but improving resilience to failures.
The capabilities of AutoRNet to design heuristics that either strictly adhere to or flexibly navigate AFF constraints
highlights its versatility. AutoRNet not only matches the complexity of manually designed algorithms but also ex-
plores new strategies that yield better performance at the same practical cost. This leads to broader considerations
about the potential of automated heuristic design in optimizing complex networks of the real world.
5.2.2. Performance Comparison of Algorithms
The robustness results over all test graphs, summarized in Tables 1-3, demonstrate the effectiveness of the heuris-
tics designed by AutoRNet.
Heuristic-v1’s performance is not as strong as those of Heuristic-v2 and Heuristic-v3, but it is comparable to
those of the three manually designed algorithms, often surpassing their in certain scenarios. In Table 3, Heuristic-v1
performs comparably well on the EU Power Grid Network, maintaining robustness with an average of 0.205911 and
a low variance, often surpassing the baseline algorithms in stability.
Specifically, Heuristic-v1 maintains stable performance with low variance across all network sizes and densities.
For instance, in Table 1, Heuristic-v1 surpasses the baseline algorithms in the 100 nodes network scenario by achieving
a lower variance 0.000069. In Table 2, for edge density M0 = 2, Heuristic-v1 shows better average robustness
0.262601 with lower variance than the baseline algorithms.
Heuristic-v2 consistently achieves the best performance across most test cases, demonstrating its superior ro-
bustness enhancement capabilities. However, it does not always outperform other algorithms in every scenario. For
9


--- Page 10 ---
example, in Table 2, for denser networks with M0 = 4 and M0 = 5, Heuristic-v2 does not achieve the highest ro-
bustness, being outperformed by SA and Heuristic-v3. Nonetheless, Heuristic-v2 remains one of the top-performing
algorithms overall, excelling in the majority of scenarios.
Heuristic-v3, although not as strong as Heuristic-v2, consistently outperforms the baseline algorithms. For ex-
ample, in Table 1, Heuristic-v3 shows strong performance in larger networks, achieving an average robustness of
0.307550 for 200 nodes and 0.307023 for 300 nodes, outperforming all baseline algorithms. In Table 2, for denser
networks with M0 = 4 and M0 = 5, Heuristic-v3 achieves the highest robustness with average values of 0.412866
and 0.427933, respectively, demonstrating its effectiveness in denser networks. Similarly, in Table 3, Heuristic-v3
achieves a robustness of 0.219386 on the EU Power Grid Network, outperforming the baseline algorithms.
Table 1: R obtained on networks of different sizes.
N
Algs
Best
Worst
Average ± Variance
HC
0.276153
0.213245
0.251327 ± 0.000211
SA
0.307163
0.221321
0.274463 ± 0.000269
100
SR
0.286163
0.211427
0.255401 ± 0.000229
Heuristic-v1
0.274800
0.262700
0.270066 ± 0.000069
Heuristic-v2
0.366399
0.355299
0.360699 ± 0.000020
Heuristic-v3
0.324999
0.324599
0.324766 ± 0.000011
HC
0.279601
0.214745
0.247127 ± 0.000141
SA
0.281831
0.227268
0.257501 ± 0.000089
200
SR
0.277896
0.220101
0.253213 ± 0.000091
Heuristic-v1
0.264925
0.254250
0.260791 ± 0.000058
Heuristic-v2
0.354849
0.348724
0.351099 ± 0.000021
Heuristic-v3
0.315075
0.303625
0.307550 ± 0.000028
HC
0.264859
0.223717
0.243451 ± 0.000064
SA
0.265958
0.227663
0.249812 ± 0.000091
300
SR
0.267519
0.230698
0.251128 ± 0.000063
Heuristic-v1
0.249233
0.244899
0.247299 ± 0.000042
Heuristic-v2
0.355855
0.348866
0.351699 ± 0.000027
Heuristic-v3
0.309299
0.303022
0.307023 ± 0.000022
HC
0.249601
0.222911
0.236645 ± 0.000038
SA
0.249788
0.221655
0.238325 ± 0.000028
500
SR
0.249093
0.228155
0.238423 ± 0.000033
Heuristic-v1
0.233875
0.227803
0.230369 ± 0.000033
Heuristic-v2
0.348228
0.344426
0.346460 ± 0.000024
Heuristic-v3
0.305000
0.301996
0.303977 ± 0.000023
Table 2: R obtained on networks of varying edge densities.
M0
Algs
Best
Worst
Average ± Variance
HC
0.267699
0.182465
0.231189 ± 0.000272
SA
0.301401
0.195479
0.251081 ± 0.000367
2
SR
0.270889
0.188913
0.234173± 0.000269
Heuristic-v1
0.266000
0.260500
0.262601 ± 0.000132
Heuristic-v2
0.346599
0.312231
0.333293 ± 0.000068
Heuristic-v3
0.310200
0.302000
0.306233 ± 0.000088
HC
0.361012
0.284971
0.331501 ± 0.000214
SA
0.377793
0.315786
0.357099 ± 0.000132
3
SR
0.364866
0.291878
0.338351 ± 0.000182
Heuristic-v1
0.359299
0.358399
0.358866 ± 0.000041
Heuristic-v2
0.402699
0.385199
0.394033 ± 0.000051
Heuristic-v3
0.387299
0.379999
0.382566 ± 0.000063
HC
0.400000
0.365644
0.386593 ± 0.000059
SA
0.413366
0.385940
0.401635 ± 0.000033
4
SR
0.402673
0.370594
0.388876 ± 0.000030
Heuristic-v1
0.400299
0.395199
0.397299 ± 0.000025
Heuristic-v2
0.308147
0.302105
0.305465 ± 0.000014
Heuristic-v3
0.416999
0.410799
0.412866 ± 0.000021
HC
0.425451
0.399378
0.412409 ± 0.000024
SA
0.436733
0.410297
0.423445 ± 0.000017
5
SR
0.421089
0.400198
0.412844 ± 0.000022
Heuristic-v1
0.420999
0.419599
0.420266 ± 0.000005
Heuristic-v2
0.338199
0.331899
0.335066 ± 0.000004
Heuristic-v3
0.429299
0.426099
0.427933 ± 0.000005
6. Conclusion
This paper proposes AutoRNet, an innovative framework designed to generate complete heuristics automatically
which can improve the robustness of networks by integrating LLMs with EAs. AutoRNet uses NOS-based variation
operations to create domain specific prompts for LLMs and an AFF to transfer hard constraint to soft one so that the
searching space is relaxed and the searching process is more effective. The experimental results show that AutoRNet
not only can design complete heuristics matching the complexity of manually ones by making use of advanced domain
knowledge, but also can explore new strategies that yield better performance at the same practical cost benefiting
from the AFF. Three best complete heuristics with different properties generated by AutoRNet were evaluated on
both synthetic networks with varying sizes and densities and a real-world network, showing better performance over
baseline algorithms. AutoRNet can significantly reduce the need for manual design and large datasets, providing a
Table 3: R obtained on the EU Power Grid Network.
Algs
Best
Worst
Average ± Variance
HC
0.213618
0.210541
0.212316 ± 0.000001
SA
0.215369
0.195734
0.206183 ± 0.000067
SR
0.214041
0.212116
0.212764 ± 0.000001
Heuristic-v1
0.211907
0.195907
0.205911 ± 0.000058
Heuristic-v2
0.272040
0.270839
0.271440 ± 0.000002
Heuristic-v3
0.220805
0.217966
0.219386 ± 0.000002
10


--- Page 11 ---
more flexible and adaptive solution. This leads to broader considerations about the potential of automated heuristic
design in optimizing complex networks of the real world.
References
[1] D. J. Watts, S. H. Strogatz, Collective dynamics of ’small-world’ networks, Nature 393 (6684) (1998) 440–442.
[2] A.-L. Barabási, R. Albert, Emergence of scaling in random networks, Science 286 (5439) (1999) 509–512.
[3] C. M. Schneider, A. A. Moreira, J. S. Andrade, S. Havlin, H. J. Herrmann, Mitigation of malicious attacks on networks, Proceedings of the
National Academy of Sciences 108 (10) (2011) 3838–3841. doi:10.1073/pnas.1009440108.
URL http://dx.doi.org/10.1073/pnas.1009440108
[4] T. Stojan, M. H. Javier, W. Wynand, M. Piet, Robustness envelopes of networks, Journal of Complex Networks 1 (2013) 44–62. doi:
10.1093/comnet/cnt004.
[5] P. Buesser, F. Daolin, M. Tomassini, Optimizing the robustness of scale-free networks with simulated annealing, in: Proceedings of the 10th
International Conference on Adaptive and Natural Computing Algorithms, Vol. Part II, 2011, pp. 167–176.
[6] M. Zhou, J. Liu, A memetic algorithm for enhancing the robustness of scale-free networks against malicious attacks, Physica A: Statistical
Mechanics and its Applications 410 (2014) 131–143. doi:10.1016/j.physa.2014.05.002.
[7] A. Zeng, W. Liu, Enhancing network robustness against malicious attacks, Physical Review E 85 (2012) 066130. doi:10.1103/PhysRevE.
85.066130.
[8] N. Liu, W. Sun, Y. Chen, M. Zhu, Y. Jia, R. Pan, Deep reinforcement learning based reconfiguration to enhance network robustness, CoRR
abs/1906.09959 (2019). arXiv:1906.09959.
URL http://arxiv.org/abs/1906.09959
[9] H. Naveed, et al., A comprehensive overview of large language models, arXiv preprint arXiv:2307.06435 (2023).
URL https://arxiv.org/abs/2307.06435
[10] F. Silva, A. Gonçalves, S. Nguyen, D. Vashchenko, R. Glatt, T. Desautels, M. Landajuela, B. Petersen, D. Faissol, Leveraging language
models to efficiently learn symbolic optimization solutions, in: Learning and Adaptive Agents (ALA) Workshop at AAMAS, 2022.
[11] A. AhmadiTeshnizi, W. Gao, M. Udell, Optimus: Optimization modeling using mip solvers and large language models, arXiv preprint
arXiv:2310.06116 (2023).
URL https://arxiv.org/pdf/2310.06116
[12] B. Romera-Paredes, M. Barekatain, A. Novikov, M. Balog, M. Kumar, E. Dupont, F. Ruiz, J. Ellenberg, P. Wang, O. Fawzi, P. Kohli,
A. Fawzi, Mathematical discoveries from program search with large language models, Nature 625 (2024) 468–475.
doi:10.1038/
s41586-023-06924-6.
[13] F. Liu, X. Tong, M. Yuan, X. Lin, F. Luo, Z. Wang, Z. Lu, Q. Zhang, Evolution of heuristics: Towards efficient automatic algorithm design
using large language model, in: International Conference on Machine Learning (ICML), 2024.
URL https://arxiv.org/abs/2401.02051
[14] S. Fortunato, C. Castellano, Community detection in graphs, Physics Reports 486 (3-5) (2006) 75–174.
[15] J. Zhang, S. Wang, Y. Hou, Lazy-greedy algorithms for large-scale problems, in: Proceedings of the 25th International Conference on
Machine Learning, 2009, pp. 1105–1112.
[16] H. Chan, L. Akoglu, Optimizing network robustness by edge rewiring: a general framework, Data Mining and Knowledge Discovery 30
(2016) 1395–1425.
URL https://api.semanticscholar.org/CorpusID:14653006
[17] J. H. Holland, Adaptation in Natural and Artificial Systems, MIT Press, Cambridge, MA, 1975.
[18] S. Kirkpatrick, C. D. Gelatt, M. P. Vecchi, Optimization by simulated annealing, Science 220 (1983) 671–680.
[19] C. Pizzuti, A. Socievole, A genetic algorithm for improving robustness of complex networks, in: Proceedings of the 2018 IEEE 30th Interna-
tional Conference on Tools with Artificial Intelligence (ICTAI), 2018, pp. 514–521. doi:10.1109/ICTAI.2018.00085.
[20] C. Pizzuti, M. Canonaco, Robga: A genetic algorithm for improving robustness of complex networks, IEEE Transactions on Systems, Man,
and Cybernetics, Part B 38 (3) (2008) 610–621.
[21] X. Tang, Y. Li, Y. Sun, H. Yao, P. Mitra, S. Wang, Transferring robustness for graph neural network against poisoning attacks, in: Proceedings
of the 33rd Annual Conference on Neural Information Processing Systems, 2019, pp. 1877–1887.
[22] B. Wang, J. Jia, X. Cao, N. Z. Gong, Certified robustness of graph neural networks against adversarial structural perturbation, in: Proceedings
of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2021, pp. 1645–1653.
[23] S. Schulhoff, M. Ilie, N. Balepur, K. Kahadze, A. Liu, C. Si, Y. Li, A. Gupta, H. Han, S. Schulhoff, P. S. Dulepet, S. Vidyadhara, D. Ki,
S. Agrawal, C. Pham, G. Kroiz, F. Li, H. Tao, A. Srivastava, H. D. Costa, S. Gupta, M. L. Rogers, I. Goncearenco, G. Sarli, I. Galynker,
D. Peskoff, M. Carpuat, J. White, S. Anadkat, A. Hoyle, P. Resnik, The prompt report: A systematic survey of prompting techniques (2024).
arXiv:2406.06608.
URL https://arxiv.org/abs/2406.06608
[24] T. Kojima, et al., Large language models are zero-shot reasoners, arXiv preprint arXiv:2205.11916 (2022).
URL https://arxiv.org/abs/2205.11916
[25] T. Brown, et al., Language models are few-shot learners, arXiv preprint arXiv:2005.14165 (2020).
URL https://arxiv.org/abs/2005.14165
[26] J. Wei, et al., Chain-of-thought prompting elicits reasoning in large language models, arXiv preprint arXiv:2201.11903 (2022).
URL https://arxiv.org/abs/2201.11903
[27] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, Q. Guo, M. Wang, H. Wang, Retrieval-augmented generation for large language
models: A survey, arXiv preprint arXiv:2312.10997 (2023).
URL https://arxiv.org/abs/2312.10997
11


--- Page 12 ---
[28] S. Liu, C. Chen, X. Qu, K. Tang, Y.-S. Ong, Large language models as evolutionary optimizers, arXiv preprint arXiv:2310.19046v3 (2024).
[29] J. Paterson, B. Ombuki-Berman, Optimizing scale-free network robustness with the great deluge algorithm, in: Recent Trends and Future
Technology in Applied Intelligence, Springer, 2018, pp. 495–506.
[30] V. Louzada, F. Daolio, H. Herrmann, M. Tomassini, Smart rewiring for network robustness, Journal of Complex Networks 1 (2013) 150–159.
doi:10.1093/comnet/cnt007.
[31] Q. Zhou, J. Bialek, Approximate model of european interconnected system as a benchmark system to study effects of cross-border trades,
IEEE Transactions on Power Systems 20 (2) (2005) 782–788. doi:10.1109/TPWRS.2005.846236.
12


--- Page 13 ---
Appendix A. The detailed population initialization prompt
The population initialization is achieved by providing Large Language Models (LLMs) with detailed task specifi-
cations, which include a description of the problem, function signature, and functionality.
• Problem Description: An explanation of the network robustness problem, detailing the optimization goals.
• Function Signature: Providing the function signature of the heuristic method to guide LLMs in generating the
correct code.
• Functionality: Listing the specific functions and operations that the heuristic methods can directly call or use.
A typical task specification is shown in Figure A1:
Optionally, heuristic seed codes can be included in the prompt to guide the LLMs. It is already executable and
serves as a template. The seed code used in the experiment is designed to modify a given network graph by randomly
swapping edges and utilizes the compute_robustness function to evaluate the network’s robustness after each modifi-
cation, ensuring that only beneficial changes are kept, which is shown in Algorithm A1.
Figure A.4: Task Specification for population initialization prompt
Appendix B. The detailed information of NOSs
Network Optimization Strategies (NOSs) are a crucial component of AutoRNet. By integrating domain-specific
knowledge into the variation operations. NOSs provide structured guidance that helps navigate the complex method
space more effectively. Each NOS is composed of three main components: features, strategies, and actions.
1. Features:
• Degree: The number of connections each node has.
• Path Characteristics: Shortest path, average path length, and network diameter.
• Clustering Coefficient: Local and global measures of how nodes tend to cluster together.
13


--- Page 14 ---
• Connectivity: Connected components and the strength of connections between them.
• Centrality Measures: Degree centrality, betweenness centrality, closeness centrality, and eigenvector
centrality.
• Edge Attributes: Weight and direction of edges.
• Dynamic Characteristics: Robustness to failures and ability to recover.
• Community Structure: Tightly-knit groups within the network.
2. Strategies:
• High-Degree Node Priority: Focus on nodes with many connections.
• Low-Degree Node Priority: Focus on nodes with fewer connections.
• Betweenness Centrality Priority: Focus on nodes that frequently appear on shortest paths.
• Closeness Centrality Priority: Focus on nodes that have short average distances to all other nodes.
• Eigenvector Centrality Priority: Focus on nodes that have high influence over the network.
• High-Weight Edge Priority: Focus on edges with higher weights.
• Low-Weight Edge Priority: Focus on edges with lower weights.
• Shortest Path Optimization: Optimize the shortest paths in the network.
• Critical Path Optimization: Optimize paths that are crucial for network performance.
• Similarity-Based Node Selection: Focus on nodes with similar attributes or roles.
• Boundary Node Optimization: Focus on nodes at the boundary of communities or clusters.
• Homophily-Based Edge Optimization: Focus on edges connecting nodes with similar attributes.
• Heterophily-Based Edge Optimization: Focus on edges connecting nodes with different attributes.
• Hub-Peripheral Optimization: Optimize the connectivity between hub nodes and peripheral nodes.
• Random Node Selection: Randomly select nodes for optimization to introduce variability.
• Central Node Optimization: Focus on nodes that are centrally located within their respective communi-
ties.
3. Actions:
• Edge Addition: Involves the addition of new edges to a network, thereby increasing its redundancy and
robustness.
• Edge Relocation: Refers to the process of moving existing edges from one pair of nodes to another. This
strategy alters the degree distribution of the nodes involved.
• Edge Swapping: Involves exchanging the endpoints of two edges within the network. This technique
preserves the original degree distribution.
The example of NOSs is shown in Figure A2:
Appendix C. C.The detailed Variation Operation prompt
We define three types of Variation Operation prompts: E1 generates offspring heuristics that are entirely different
from the parent heuristics. M1 modifies the current heuristic based on NOSs to enhance its effectiveness. M2 fine-
tunes the current heuristic to optimize its efficiency. Each type plays a specific role in the evolution process, balancing
exploration and exploitation to enhance network robustness. M1 is given in Figure 3 of the main text, E1, M2 are
given in Figures A3 and A4:
14


--- Page 15 ---
Figure B.5: Eight examples of NOSs. NOSs are randomly selected and included in the prompt for variation operation.
Figure C.6: Variation Operation Prompt for E1. It helps to es-
cape local optima by introducing new strategies and methods from
NOSs.
Figure C.7: Variation Operation Prompt for M2. It is a pure local
search, making incremental adjustments to optimize the heuristic
and don’t have NOSs.
15
