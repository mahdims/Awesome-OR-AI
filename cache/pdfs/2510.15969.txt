--- Page 1 ---
LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear
Reformulation of Nonlinear Optimization Problems
Paul-Niklas Ken Kandora 1 Simon Caspar Zeller 1 Aaron Jeremias Elsing 1 Elena Kuss 2 Steffen Rebennack 1
Abstract
Reformulating nonlinear optimization problems
into solver-ready linear optimization problems
is often necessary for practical applications, but
the process is often manual and requires domain
expertise. We propose LinearizeLLM, an agent-
based LLM framework that produces solver-ready
linear reformulations of nonlinear optimization
problems. Agents first detect the nonlinearity
pattern (e.g., bilinear products) and apply non-
linearity pattern-aware reformulation techniques,
selecting the most suitable linearization technique.
We benchmark on 40 instances: 27 derived from
ComplexOR by injecting exactly-linearizable op-
erators, and 13 automatically generated instances
with deeply nested nonlinearities. LinearizeLLM
achieves 73% mean end-to-end overall success
(OSR) across nonlinearity depths (8.3× higher
than a one-shot LLM baseline; 4.3× higher than
Pyomo). The results suggest that a set of pattern-
specialized agents can automate linearization, sup-
porting natural-language-based modeling of non-
linear optimization.
1. Introduction
Complex real-world optimization problems often involve
nonlinear relationships between decision variables that make
optimization problems computationally challenging to solve.
A standard approach in Operations Research (OR) is to re-
formulate nonlinear optimization problems (NLOPs), trans-
forming them into more tractable optimization problems
(Williams, 2013; Vielma, 2015). These reformulations al-
low solvers such as Gurobi 12 (Gurobi Optimization, LLC,
2024) to apply LP/MILP algorithms on problem types, such
as Linear Optimization Problems (LPs) or Mixed-Integer
Linear Optimization Problems (MILPs). In practice, how-
1Institute for Operations Research, Karlsruhe Institute of Tech-
nology, Karlsruhe, Germany 2Institute for Information Systems,
Reutlingen University, Reutlingen, Germany. Correspondence to:
Paul-Niklas Kandora <paul-niklas.kandora@kit.edu>.
Preprint. February 3, 2026.
ever, reformulating NLOPs into LPs typically requires ex-
pert knowledge. Practitioners without OR expertise often
encounter intractable models, because they are unaware
of the sophisticated reformulation techniques. This gap be-
tween advanced OR theory and what non-experts can readily
apply limits broader adoption of optimization in industry
and science (Chen et al., 2023).
Meanwhile, LLMs may help reduce this expertise gap
(Wasserkrug et al., 2025). In particular, Wasserkrug et al.
(2025) highlight using LLMs to reformulate nonlinear opti-
mization problems into linear ones as an open research di-
rection. Recent LLM-based approaches have demonstrated
the ability to perform complex reasoning tasks in OR. (Xiao
et al., 2024) have begun to position LLMs as high-level
translators of textual problem descriptions into formal op-
timization problems and even solve them with Chain-of-
Experts. Additionally, OptiMUS introduced by (Ahma-
diTeshnizi et al., 2024) is an LLM-driven framework that
can formulate an MILP from a textual description, write cor-
responding solver code, and iteratively refine the optimiza-
tion problem based on solution feedback. These systems
have already shown that LLMs can alleviate the heavy de-
pendence on domain experts in the modeling phase, making
optimization tools more accessible to non-experts. However,
a related challenge remains under-explored: Once an initial
complicated NLOP is formulated from language, how can
we reformulate that optimization problem into an equiva-
lent linear model? In other words, beyond producing an
optimization problem from language through LLM-based
systems we ask whether an LLM-based system can also
improve the model’s tractability by linearizing NLOPs?
In this study we focus on nonlinear patterns that admit exact
linear reformulations. Concretely, the current implementa-
tion supports exact reformulation techniques for absolute
value, min / max, binary-continuous products, linear frac-
tional terms (under sign conditions), and monotone objective
transformations; see Appendix C. However, note that inex-
act linearization techniques, e.g., McCormick envelopes for
purely continuous bilinear terms (McCormick, 1976), are
also compatible with the agent architecture. The above ques-
tion of whether LLMs can be used to linearize problems is
important because even an algebraically correct formulated
1
arXiv:2510.15969v2  [cs.LG]  2 Feb 2026


--- Page 2 ---
LinearizeLLM
optimization problem may be impractical to be solved if it
involves nonlinear functions. In practice, many practitioners
do not apply these techniques, and current LLM systems
do not consistently perform this reformulation step. Re-
cent evaluations highlight this gap (Wasserkrug et al., 2025):
When asked to reformulate a given optimization problem,
an LLM often fails to produce an equivalent (or “nearly”
equivalent) optimization problem.
This paper presents LinearizeLLM, an LLM-driven frame-
work that reformulates a broad class of NLOPs into equiva-
lent solver-ready LP/MILP models. We evaluate reformu-
lations using a solver-based suite that combines end-to-end
objective preservation with stage-wise success metrics for
detection, reformulation validity, and compilation; we sum-
marize this combined criterion as the Overall Success Rate
(OSR). Since multiple optima and auxiliary-variables make
“argmin equality” subtle to certify, we do not directly com-
pare solution-sets but additionally report argmin compar-
isons in Appendix H as additional empirical evidence that
the obtained solutions are consistent across formulations
up to renaming and auxiliaries. Specifically, as our main
contributions, we
(i) propose an agent architecture in which each nonlinear
term is handled by a specialized reformulation agent
instructed to derive an exact linearization pattern after
reading in the original problem formulation in LATEX
code; the agents then coordinate to assemble a solver-
ready model;
(ii) release a benchmark dataset of 40 nonlinear optimiza-
tion problems consisting of 27 manually constructed in-
stances obtained by injecting exactly-linearizable non-
linearity patterns into selected ComplexOR problems
(Xiao et al., 2024), and 13 automatically generated in-
stances featuring highly nested nonlinearities for eval-
uating robustness under deeper nesting;
(iii) conduct an empirical evaluation of LinearizeLLM on
the dataset, benchmarking end-to-end overall success
across nonlinearity nesting depths, and comparing
against both a one-shot LLM reformulator and a de-
terministic Pyomo-based linearization pipeline. We
further assess robustness under controlled LATEX per-
turbations of the optimization problem and perform
a context-ablation study to quantify how structured
metadata affects reformulation reliability.
2. Mixed-Integer Nonlinear Programming
Mixed-Integer Nonlinear Problems (MINLPs) appear in
many real-world applications, including process systems
engineering, energy operations, logistics, and finance, where
discrete decisions must be made under nonlinear dynamics
(Belotti et al., 2013).
Formally, a general MINLP can be written as:
min
x∈Rn, y∈Zm
f(x, y)
s.t.
gi(x, y) = 0,
∀i
hj(x, y) ≤0,
∀j
where f(x, y), gi(x, y) and hj(x, y) are potentially non-
linear functions in (x, y), and y contains integer-valued
decision variables. If all functions are linear, the problem is
a MILP; if, in addition, there are no y variables, i.e., m = 0,
then the problem is an LP.
3. Motivation
First, one practical limitation is solver support for a broad
class of nested nonlinearities. While modern NLOP/MINLP
solvers accept broad classes of nonlinear terms, certain
nested constructs, such as compositions involving monotone
transformations and piecewise linear functions either violate
standard differentiability assumptions or fall outside sup-
ported operator sets in common solver interfaces (W¨achter
& Biegler, 2006). As a result, such models often require
an explicit exact reformulation before they can be reliably
handed to a solver such as Gurobi or open-source engines
like HiGHS (Huangfu & Hall, 2018) and CBC (Forrest &
Ralphs, 2022).
Modern MILP solvers incorporate presolve routines and
even automatic detection of certain nonlinear patterns. How-
ever, these built-in features are limited and difficult to audit.
By contrast, an explicit reformulation handled outside the
solver supports transparency and auditability. The user (or
model auditor) can inspect the introduced auxiliary variables
and linear constraints, verifying that they correctly represent
the original nonlinear relations. The auditability supports re-
view: the reformulated model is human-readable and can be
double-checked line by line, unlike solver-internal transfor-
mations that are hidden from view. Auditability is important
in high-stakes applications where one must ensure the refor-
mulation has not altered the problem’s intent or feasibility
region.
LinearizeLLM (i) generates a fully documented set of auxil-
iary variables and constraints, giving auditors line-by-line
traceability; (ii) outputs its model in standard algebraic form,
so the same file can be fed to any LP/MILP engine or embed-
ded as a linear sub-problem inside decomposition schemes;
and (iii) employs pattern-specialized LLM agents that rec-
ognize and linearize nonlinear (nested) patterns beyond the
cases typically handled by solver presolve. In doing so, Lin-
earizeLLM encodes reformulation rules into an automated,
transparent, and portable pipeline, enabling practitioners
to use LP/MILP solvers on models that would otherwise
2


--- Page 3 ---
LinearizeLLM
require nonlinear optimization support.
4. Related Work
4.1. LLMs for Model Formulation
Early work on natural language to optimization focused
on the modeling task, i.e., formulating MILPs from prob-
lems described with natural language. (Ramamonjison et al.,
2022) introduced NL4Opt, a publicly available dataset and
NeurIPS 2022 competition with the task of translating real-
world problems into LPs. Several other studies later focused
on this problem. (Li et al., 2023) developed a three-phase
framework to formulate MILPs from natural text and ex-
tended the NL4Opt dataset to evaluate their approach. Ad-
ditional work on this task includes OptiMUS, introduced
by (AhmadiTeshnizi et al., 2024), an LLM based agent,
that models natural language problems as MILPs, writes
and evaluates the solver-ready code. (Xiao et al., 2024) is
closest to our work. The authors introduced a multi-agent
framework Chain-of-Experts, where each agent is assigned
to a specific task. Their framework is capable of generating
solver-ready code for OR problems and is evaluated on the
new ComplexOR dataset.
However, these approaches assume linearity and therefore
do not address nonlinear formulations. In the OptimAI ap-
proach (Thind et al., 2025), a suitable solver is selected
based on the structure and requirements of the given opti-
mization problem. However, no reformulation of the prob-
lem is performed, and the problem is solved in its original
mathematical form. A recent study by (Wasserkrug et al.,
2025) evaluated ChatGPT’s (OpenAI, 2022) ability to per-
form algebraic reformulations. When asked to replace a
nonlinear absolute-value constraint with linear constraints
(a common linearization task), ChatGPT produced an equiv-
alent formulation in convex cases. However, for non-convex
cases that require integer auxiliary variables, the model’s
answers were often incomplete; it tended to omit the neces-
sary binary variables, yielding incorrect formulations. This
suggests that LLMs can recognize certain reformulation pat-
terns (indeed, ChatGPT knew that “max” constraints can
be rewritten as linear inequalities) but may fail to enforce
logical consistency unless explicitly guided.
These results motivate explicit guidance or agent-based de-
composition when exact linearization is required. Moti-
vated by this gap, we present LinearizeLLM, a multi-agent
pipeline that reformulates each detected nonlinear pattern
individually to produce solver-ready linear models. The
next section details its workflow.
5. LinearizeLLM Workflow
LinearizeLLM transforms an NLOP into an equivalent
LP/MILP in three stages: a detection agent parses the
original NLOP and reports each nonlinear pattern; a de-
terministic structural policy partitions detected nonlinearity
patterns into distinct nonlinear terms and determines the
reformulation order; and the loop repeats until no nonlinear
patterns remain after being processed by the reformulation
agents, yielding a linear optimization problem. We consider
a fixed set of exactly linearizable patterns: absolute value,
min / max, binary–continuous products, linear fractional
terms (with denominator sign constraints), and monotone
objective transformations. Reformulation recipes and re-
quired preconditions are given in Appendix C.
5.1. Detection agent
The detection agent is an LLM-based agent tasked for iden-
tifying and mapping the hierarchy of nonlinearities within
a given optimization problem. The agent performs a scan
to distinguish between actual decision variables and con-
stant parameters. The agent detects nonlinear patterns that
can be reformulated exactly into linear or mixed-integer
linear forms; if no such patterns are detected, the workflow
terminates.
Parameter-aware Contextualization. To minimize false
positives, where linear terms are incorrectly identified as
nonlinear (e.g., x · p, where p is a constant parameter and
x is a decision variable), the agent is provided with a pa-
rameter context that explicitly defines the set of decision
variables. The agent is instructed to ignore terms involving
only parameters or constants, so that mathematically linear
forms are excluded from the results.
Hierarchical Pattern-Scanning. The agent is prompted to
reason about the nesting structure of nonlinearities. For each
detected term π ∈Π, the agent reports its mathematical ex-
pression, its specific nonlinearity type τ(π), and its nesting
depth d(π). A top-level nonlinearity (one not contained
within another nonlinear operator) is assigned a depth of 1,
while inner nonlinearities have increasing depth values.
The agent is instructed to output a dependency structure for
each π by tracking parent and child node. In this context,
a parent node is an outer nonlinear operator (e.g., a square
root in the objective function) whose argument contains
another nonlinear expression, which is then defined as its
child node (e.g., a bilinear product nested inside that square
root). Nested terms can then be decomposed recursively and
replaced by auxiliary variables to maintain mathematical
equivalence of the optimization problems. We can see a
simple example for this structure in Figure 1.
3


--- Page 4 ---
LinearizeLLM
Parent: √·
(Depth 1)
Child: x · y
(Depth 2)
Top-level operator
Nested nonlinearity
Figure 1. Dependency tree for the nested expression √x · y. This
hierarchy illustrates how the agent tracks the relationship between
operators, ensuring that the inner child node (the bilinear product)
is resolved before the outer parent node (the monotone transforma-
tion).
5.2. Structural Policy
The structural policy is a deterministic sequencing mech-
anism that defines the order in which detected nonlinear
terms Π are processed. Unlike the detection agent, which
relies on the semantic reasoning of an LLM, the structural
policy applies a fixed rule based on the dependency struc-
ture of the (nested) nonlinear patterns constructed during
detection.
Depth-Based Sequencing. To preserve equivalence of the
model during reformulation, nested nonlinearities must be
resolved such that any child term is linearized and replaced
by an auxiliary variable before its parent term is processed.
The policy achieves this by sorting all terms π ∈Π in
descending order of their nesting depth d(π). This ensures
a bottom-up traversal of the dependency structure, where
the most nested terms are handled first.
Why bottom-up matters. Consider max{0, xy} with bi-
linear (continuous-binary) child xy: linearizing the outer
max first via a binary selector z forces activation of the xy
branch only when z = 1, introducing continuous–binary
bilinearities such as z · (xy). Processing bottom-up in-
stead reformulates xy into an auxiliary variable w first, after
which max{0, w} is piecewise-linear and can be linearized
without creating new nonlinear terms.
State Consistency. By enforcing this specific sequence,
the policy ensures that at any iteration k, the specialized
reformulation agent Φτ receives a model state p(k−1) where
the arguments of the current term πk have already been
simplified into linear forms. This reduces the instruction
complexity and limits growth of nested expressions.
5.3. Reformulation agent
While the detection agent identifies nonlinear patterns, the
actual transformation of these patterns is performed by the
type-specific reformulation agents {Φτ}. Rather than rely-
ing on static reformulation templates, the system invokes an
agent specialized for the specific nonlinearity type τ (e.g.,
bilinear). This ensures that each transformation is handled
by an expert module tailored to the unique mathematical
requirements of that specific pattern.
General reformulation instructions The agent is guided by
a universal instruction that emphasizes producing a linear
LP/MILP and preserving equivalence between the origi-
nal NLOP and the resulting LP/MILP. This generalized ap-
proach allows the system to handle the supported nonlinear-
ities; from standard bilinear products to complex monotone
transformations, without requiring a separate, specialized
agent for every possible mathematical form. This agent
operates on general principles of optimization theory rather
than fixed ”lookup tables.”
Iterative Reformulation. The reformulation process is it-
erative: at each step k, the specialized agent Φτ replaces
the nonlinear term πk in the current optimization problem
p(k−1) with a linear auxiliary variable and introduces the
corresponding linear constraints. The agent receives the
nonlinear term’s exact location in the model, along with the
relevant problem parameters, to support accurate reformu-
lation. An important feature of these agents is their ability
to provide ”post-reformulation” instructions, for example,
when simplifying an objective function through a monotone
transformation, the agent provides the exact reconstruction
formula (e.g., recovering the original value from a squared
objective). Each step is documented within the evolving
LaTeX model, ensuring that the final linear formulation ˆp is
mathematically transparent and ready for solver execution.
5.4. The Workflow of LinearizeLLM
Let P be the set of solvable nonlinear optimization problems.
For any p ∈P the original problem instance is denoted by
p ∈P. We denote by Θ the set that contains all parameter
sets θp, ∀p. The parameter set θp collects all parameters
(e.g., index sets) associated with problem p and is assumed
to remain invariant with respect to the reformulation.
Each problem p ∈P may contain nonlinear patterns, po-
tentially in nested form. We collect all detected nonlinear
patterns in a set Π, where each π ∈Π represents a (nested)
nonlinear pattern and is annotated with (i) a nonlinearity
type τ(π) ∈T (e.g., bilinear, absolute value, etc.), (ii) its lo-
cation in the model (e.g. constraint or objective), and (iii) its
nesting depth d(π) ∈N. To represent nested structures, we
store for each term a parent node parent(π) ∈Π ∪{None}
and a set of children nodes children(π) ⊆Π.
We define a detection agent as the following mapping
Detect : P × Θ →2Π,
which is called once on the original problem and returns the
set of all nonlinear terms Π := Detect(p, θp). If Π = ∅, the
problem is linear and the workflow terminates.
To ensure that children nodes are linearized before their
4


--- Page 5 ---
LinearizeLLM
parents, we define a structural policy that orders the terms in
Π by their nesting depth. This results in an ordered sequence
of terms (π1, . . . , π|Π|) such that d(πk) ≥d(πk+1) for all
k.
We employ a generalized reformulation agent {Φτ | τ ∈
T }. The reformulation agent
Φτ : P × Θ × Π →P
is instructed to derive exact linear formulations based on
the nonlinearity pattern τ as well as the problem p(k−1)
and the context θp the agent receives. The reformulation
process is viewed as a sequence of intermediate problems
(p(k))|Π|
k=0, where p(0) = p is the original model and, for
each k = 1, . . . , |Π|,
p(k) := Φτ(πk)
 p(k−1), θp, πk

.
In this sequence, at each iteration k, the agent Φτ(πk) re-
places the nonlinear term πk in the current model p(k−1)
with its exact linear reformulation, yielding the updated
model p(k). The final reformulated linear model is then
ˆp := p(|Π|).
Algorithm 1 LinearizeLLM Structure-Driven Workflow
Require: Initial problem p, parameter set θp
1: Π ←Detect(p, θp)
2: (π1, . . . , π|Π|) ←SortByDepthDescending(Π)
3: p(0) ←p
4: for k = 1, . . . , |Π| do
5:
τ ←Type(πk)
6:
p(k) ←Φτ(p(k−1), θp, πk)
7: end for
8: ˆp ←p(|Π|)
9: return ˆp
The workflow operates as follows. The detection agent iden-
tifies the set of all nonlinear terms Π within the original
problem p using the context θp. A structural policy sorts
these terms by their nesting depth in descending order to es-
tablish a bottom-up processing sequence. The ordering pro-
cedure is performed by SortByDepthDescending(·). p(0)
as the initial nonlinear optimization problem is initialized
for the iterative reformulation process. The algorithm iter-
ates through each sorted nonlinear term πk in the sequence.
The specific nonlinearity type τ is identified for the current
term. The reformulation agent Φτ linearizes the term πk
within the current problem state, replacing it with an exact
linear equivalent and necessary auxiliary variables. Once all
terms are processed, the final state p(|Π|) is captured as the
fully linearized model ˆp. The resulting linear formulation is
returned for subsequent code generation and optimization.
p(0)
Π
(π1, . . . , π|Π|)
p(1)
p(2)
...
p(|Π|)
p(|Π|)
→ˆp
Figure 2. Illustration of the LinearizeLLM workflow. We initialize
LinearizeLLM with the NLOP/MINLP p(0) and terminate with the
LP/MILP reformulation ˆp.
6. Experiments
6.1. ComplexOR-NL dataset
Our ComplexOR-NL dataset is derived from the ComplexOR
dataset by (Xiao et al., 2024). We created 40 nonlinear in-
stances by injecting exactly linearizable nonlinearities into
selected base instances. In total, the benchmark comprises
27 manually constructed instances based on ComplexOR
and 13 automatically generated instances designed to in-
clude more deeply nested and syntactically diverse com-
positions of nonlinearities. Each instance is feasible, has
a finite optimum and admits an exact linear reformulation.
The dataset is designed to both cover realistic optimization
use-cases, obtained by augmenting real-world–motivated
ComplexOR problems, and to evaluate baselines and Lin-
earizeLLM with nested nonlinearities through automatically
generated instances. Table 1 summarizes how frequently
each nonlinearity appears across the 40 instances (counted
once per file).
Nonlinearity
Count
Bilinear terms
18
Min operator
26
Max operator
21
Absolute-value
24
Linear fractional
8
Monotone transformations
5
Table 1. Number of COMPLEXOR-NL problem files (out of 40)
that contain each nonlinear construct at least once; multiple occur-
rences within the same file (e.g., two max terms) are counted only
once.
6.2. Performance Metrics
Rather than relying on manual inspection of reformulations,
which is difficult to make exhaustive and reproducible, we
evaluate LinearizeLLM primarily using Overall Success
Rate (OSR), which measures whether the pipeline pro-
5


--- Page 6 ---
LinearizeLLM
duces a solver-ready LP/MILP whose optimal objective
value (after any stated objective mappings) matches that
of the original nonlinear model within a numerical toler-
ance. For each instance, we build a reference model and
solve it with Gurobi Version 12 using either hand-written
(hand-crafted benchmarks) or auto-generated (synthetic
benchmarks) Python code, applying exact reformulations
into Gurobi-supported primitives (including general con-
straints and bounded big-M for binary–continuous products,
with required domain checks). For additional diagnostics,
we report three component metrics that localize errors to
(i) nonlinearity detection (DSR), (ii) validity of the pro-
duced LP/MILP as judged by solver status and classification
(RSR), and (iii) compilation of the LATEX model into exe-
cutable solver code (CSR). We do not provide a formal proof
of argmin-set equivalence. Since (MI)LPs often admit mul-
tiple optima and our reformulations introduce auxiliary vari-
ables, exact equality of the solver-returned minimizer vec-
tors is not a well-posed requirement. Instead, OSR provides
a practical end-to-end check for optimal-value preservation.
Appendix H additionally reports side-by-side optimal solu-
tions (objective values and variable assignments) for both
the original and linearized models; in OSR-successful cases,
these traces are typically consistent after accounting for aux-
iliary variables and renaming, providing additional evidence
for solution equivalence. DSR/RSR/CSR help attribute fail-
ures to specific stages of the workflow:
Detection Success Rate (DSR): This metric measures the
proportion of instances in which the correct nonlinearity
pattern is successfully identified by the detection agents.
Reformulation Success Rate (RSR): This metric assesses
whether the reformulation agents were able to successfully
produce a valid LP/MILP formulation (model is not classi-
fied as infeasible or unbounded).
Compiler Success Rate (CSR): This metric captures the
success rate of compiling the LATEX-formulated optimization
problems into executable code.
Overall Success Rate (OSR): Our overall performance
measure, OSR denotes the proportion of total runs that are
free of detection, reformulation, or compilation errors and
yield a reformulated model whose optimal objective value,
after mapping back to the original objective when monotone
transformations are applied, matches (with a tolerance of
ϵ = 10−4) that of the original nonlinear problem, p(0), as
solved by Gurobi. A tolerance of 10−4 accounts for numeri-
cal precision differences introduced during reformulation,
such as auxiliary variables, while still validating optimal-
value agreement of the reformulated model with the original
formulation on the evaluated instances. In addition to the
other metrics, OSR therefore tests that the linearized model
attains the original optimum value, which is necessary but
not sufficient for equivalence; Appendix H provides com-
plementary solution comparisons as additional empirical
evidence beyond objective-value matching.
Each experiment is evaluated over three independent runs;
we report averages.
6.3. Baselines
We compare LinearizeLLM against the following baselines.
One-shot LLM. We evaluate the necessity of our multi-
agent orchestration by introducing a one-shot baseline using
the same LLM as used for the agents within LinearizeLLM.
This agent is tasked with simultaneous detection and exact
reformulation in a single LLM-based agent.
Deterministic rules-based linearizer. We compare against
a deterministic baseline using pyomo.gdp, a rule-based
reformulation engine. We translate the LATEX model into a
Generalized Disjunctive Programming (GDP) representa-
tion and apply Pyomo’s standard reformulations to obtain a
MILP, providing a reproducible non-LLM reference.
We report DSR for LinearizeLLM and the one-shot base-
line, since these methods generate an explicit pattern set
that can be scored against the ground-truth nonlinearities.
In contrast, the pyomo baseline is a transformation-based
pipeline without a separable detection stage (the model is
directly encoded into GDP constructs), so DSR is not well-
defined; we therefore report only RSR/CSR/OSR for this
baseline. For pyomo, we flag reformulation success when
the instance is successfully encoded as a valid GDP model
and Pyomo’s standard GDP-to-MILP transformation applies
without error.
We provide detailed descriptions for our baselines in Ap-
pendix A.
6.4. Experimental Setup
We evaluate our pipeline (Algorithm 1), which converts op-
timization problems formulated in LATEX into solver-ready
Python and executes them with Gurobi (gurobipy). Un-
less stated otherwise, LLM agents use Gemini 2.5 Flash with
temperature = 0.05. Reported metrics are averaged over
3 random seeds. Prompt templates and exact reformulation
recipes are provided in Appendix B and C, respectively.
7. Results
7.1. Head-to-Head baseline comparison
Table 2 reports a head-to-head comparison of LinearizeLLM
against the baselines using the success metrics from Sec-
tion 6.2. A comprehensive breakdown of computational
efficiency, including per-agent inference times and token
consumption across all nesting depths, is provided in Ap-
6


--- Page 7 ---
LinearizeLLM
Nonlinearity Composition Depth
Method
d = 1
d = 2
d ≥3
DSR
RSR
CSR
OSR
DSR
RSR
CSR
OSR
DSR
RSR
CSR
OSR
LinearizeLLM
89.47
100
100
78.95
96.97
84.85
96.97
66.67
96.67
100.00
100.00
73.33
One-shot LLM
12.28
96.49
100
10.53
12.12
69.70
100.00
9.09
6.67
90.00
100.00
6.67
Pyomo
–
29.82
29.82
22.81
–
30.3
30.3
15.15
–
16.67
16.67
13.33
Table 2. Head-to-head baseline comparison across nonlinearity composition depths. We report Detection Success Rate (DSR), Reformula-
tion Success Rate (RSR), Compiler Success Rate (CSR), and Overall Success Rate (OSR), where OSR measures end-to-end optimal
objective-value preservation (within tolerance) after any stated objective mappings. Results are averaged over 3 runs.
pendix E.
The results in Table 2 demonstrate a clear difference in
performance, with LinearizeLLM outperforming both the
one-shot and the Pyomo approach across all complexity
depths.
LinearizeLLM maintains consistently high detection accu-
racy (∼90–97%) across nesting depths, whereas the one-
shot baseline degrades sharply (∼6–12%). This gap is con-
sistent with the possibility that in a one-shot setup, pattern
identification competes with reformulation and code genera-
tion for limited model capacity and attention.
We additionally observe that LinearizeLLM exhibits higher
Reformulation Success Rate (RSR) than Detection Success
Rate (DSR) for d = 1. Mathematically, this appears counter-
intuitive; however, it indicates a self-correction capability
within the pipeline. Even if the detection agent fails to
explicitly label a nonlinearity pattern (leading to a DSR
failure), the subsequent reformulation and code generation
agent are still provided with the full LATEX optimization
problem. Because these agents re-evaluate the mathematics
while generating logic, they may recognize and linearize the
missed nonlinear pattern. This indicates that the multi-stage
pipeline can tolerate some early-stage errors.
The Pyomo approach shows a distinct profile where RSR
and CSR are identical (e.g., 29.8% at d = 1). This indicates
that the primary bottleneck for problem reformulation ap-
pears to be syntactic. If the LLM successfully navigates the
complex Pyomo syntax (CSR), the underlying Pyomo trans-
formation routines usually succeed (RSR). The subsequent
drop to OSR (22.8%) suggests that even when the code
runs, the LLM occasionally maps the objective function or
constraints incorrectly.
LinearizeLLM is stable across depths, maintaining an OSR
of 73.33% even at d ≥3. In contrast, both the One-shot and
Pyomo methods struggle as the “nesting” of nonlinearities
increases. The 100% RSR for LinearizeLLM at d = 1 and
d ≥3 suggests that once the system correctly identifies the
problem structure, the implemented reformulation rules are
reliable on this benchmark.
7.2. Robustness to Input Perturbations
We test robustness to syntactic variation in LATEX by per-
turbing each instance at five levels: L0 (clean), L1 (for-
mat noise), L2 (macros/refactoring), L3 (structure-
preserving reordering), and L4 (combined). We evaluate
5 randomly selected instances per depth category (d = 1,
d = 2, d ≥3). Formal perturbation definitions and exam-
ples are given in Appendix F.
In Figure 3, OSR is similar under pure formatting noise (L1)
and structure-preserving reordering (L3) across all depth
groups, indicating that the pipeline is insensitive to super-
ficial LATEX variation. The main degradation occurs under
L2 (macro + refactor), most notably for d = 2 where OSR
drops to 33%. Since these perturbations preserve the full
problem semantics, this indicates that surface-form refac-
toring (macro aliases and operator rewrites) is challenging
for end-to-end compilation, and can be more disruptive than
constraint reordering. Even under the combined setting (L4),
OSR does not consistently degrade (and returns close to the
clean baseline for d = 1 and d ≥3), suggesting that fail-
ures are associated with specific refactoring transformations
rather than perturbation density alone.
7.3. Context ablation study
We ablate the structured metadata provided to the agents
to measure how context affects end-to-end reformulation
success. We consider six settings: C1 (Minimal) provides
only the LATEX model (objective/constraint type only); C2
(Variables Only) adds decision variable definitions; C3
(Parameters Only) provides parameter names; C4 (Vars +
Param Names) combines C2 and C3; C5 (Vars + Param
Values) additionally provides numerical parameter values;
and C6 (Full Context) further includes local constraint
information. We focus on DSR and OSR to isolate the effect
of context on (i) correctly identifying nonlinear patterns and
(ii) achieving end-to-end solver-verified execution.
We evaluate all settings on 10 randomly sampled instances
(5 with d = 2 and 5 with d ≥3). Full details concerning
the study are given in Appendix G.
7


--- Page 8 ---
LinearizeLLM
L0
L1
L2
L3
L4
0.0
0.2
0.4
0.6
0.8
1.0
OSR
80%
93%
87%
87%
80%
L0
L1
L2
L3
L4
67%
80%
33%
80%
67%
L0
L1
L2
L3
L4
80%
87%
67%
87%
80%
Figure 3. OSR across perturbation levels (L0-L4) for three groups of nonlinear instances, stratified by nesting depth: left shows problems
with nested depth d = 1, middle shows d = 2, and right shows d ≥3.
Context setting
Depth = 2
Depth ≥3
C1: None
0.00%
0.00%
C2: Vars only
0.00%
0.00%
C3: Params only
0.00%
0.00%
C4: Vars + Params
0.00%
0.00%
C5: Vars + Params
86.67%
33.33%
C6: Full
73.33%
46.67%
Table 3. Context ablation on compositional instances: Overall
Success Rate (OSR) across depth categories.
Context setting
Depth = 2
Depth ≥3
C1: None
100.00%
93.33%
C2: Vars only
100.00%
100.00%
C3: Params only
100.00%
93.33%
C4: Vars + Params
100.00%
86.67%
C5: Vars + Params
100.00%
86.67%
C6: Full
100.00%
93.33%
Table 4. Context ablation on compositional instances: Detection
Success Rate (DSR) across depth categories.
According to Table 3, the main observation is 0% OSR
when concrete parameter values are withheld, regardless
of other metadata. Even with full knowledge of variable
names and indexing (C4), the agents do not achieve OSR
success without the numerical values (C5). This suggests
that providing parameter values is important for generat-
ing executable reformulations in this setting. In Depth 2
instances from Table 4, detection (DSR) remains a perfect
100% even with zero context (C1), suggesting that these
nonlinearities are easier to detect from surface form. How-
ever, in Depth 3 instances, the DSR fluctuates and even
drops when metadata is incomplete (C4/C5). This indicates
that as nesting depth increases, the agent’s ability to even
correctly identify the nonlinear terms depend in parts on its
understanding of the model’s underlying structure.
Overall, this study demonstrates that while syntactic aware-
ness is sufficient for basic term identification, numerical
grounding and semantic localization are the important pre-
requisites for successful end-to-end optimization reformula-
tion.
Limitations.
LinearizeLLM currently handles a fixed set
of nonlinear patterns with known exact LP/MILP reformu-
lations (Appendix C), and does not aim to linearize general
nonlinear functions beyond these cases or to obtain (tight)
relaxations when exact reformulations are not possible. Our
main automatic metric (OSR) checks optimal objective-
value agreement (within tolerance) between the original and
reformulated models under a solver run; it does not certify
argmin-set equivalence. Results are based on 40 instances
and one LLM and could differ for other problem instances
or LLMs.
8. Conclusion
We introduced LinearizeLLM, an agent-based framework
for producing exact LP/MILP reformulations of nonlinear
optimization problems written in LATEX. LinearizeLLM de-
composes reformulation into (i) nonlinearity detection with
explicit nesting structure, (ii) a deterministic depth-based
policy that enforces bottom-up processing, and (iii) pattern-
specialized reformulation agents that generate linear models.
We released a benchmark dataset of 40 nonlinear optimiza-
tion problems that admit exact linearization, combining man-
ually constructed variants of ComplexOR problems with
automatically generated, deeply nested instances. Across
different levels of nesting-depths, LinearizeLLM outper-
forms both a one-shot LLM reformulator and a determinis-
tic Pyomo-based pipeline in overall success rate, with high
stage-wise success rates. Robustness experiments show that
the pipeline is insensitive to superficial LATEX changes, with
the main sensitivity arising from macro-based refactoring.
Finally, a context ablation study highlights that numerical
grounding and structured metadata is important for reliable
end-to-end reformulation on more compositional instances.
These results suggest that pattern-specialized LLM agents,
orchestrated by simple structural rules, can reduce the gap
between nonlinear modeling convenience and linear-solver
tractability.
8


--- Page 9 ---
LinearizeLLM
Impact Statement
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be
specifically highlighted here.
References
AhmadiTeshnizi, A., Gao, W., and Udell, M.
Opti-
MUS: Scalable optimization modeling with (MI)LP
solvers and large language models.
In Salakhutdi-
nov, R., Kolter, Z., Heller, K., Weller, A., Oliver,
N., Scarlett, J., and Berkenkamp, F. (eds.), Proceed-
ings of the 41st International Conference on Machine
Learning, volume 235 of Proceedings of Machine
Learning Research, pp. 577–596. PMLR, 21–27 Jul
2024. URL https://proceedings.mlr.press/
v235/ahmaditeshnizi24a.html.
Belotti, P., Kirches, C., Leyffer, S., Linderoth, J.,
Luedtke,
J.,
and
Mahajan,
A.
Mixed-integer
nonlinear
optimization.
Acta
Numerica,
22:1–
131,
2013.
doi:
10.1017/S0962492913000032.
URL
https://www.cambridge.org/core/
journals/acta-numerica/article/
mixedinteger-nonlinear-optimization/
2D0CE8CDA53363A31ADE8689565517BD.
Chen, X., Deng, T., Shen, Z.-J. M., and Yu, Y. Mind the gap
between research and practice in operations management.
IISE Transactions, 55(1):32–42, 2023.
Forrest, J. and Ralphs, T. K. CBC (COIN-OR Branch and
Cut) 2.10.7 User Guide. COIN-OR Foundation, 2022.
URL https://github.com/coin-or/Cbc.
Gurobi Optimization, LLC. Gurobi optimizer. https:
//www.gurobi.com, 2024. Accessed: 2025-07-14.
Hart, W. E., Watson, J.-P., and Woodruff, D. L. Pyomo:
modeling and solving mathematical programs in python.
Mathematical Programming Computation, 3(3):219–260,
2011. doi: 10.1007/s12532-011-0026-8. URL https:
//doi.org/10.1007/s12532-011-0026-8.
Huangfu, Q. and Hall, J. A. J. Parallelizing the dual revised
simplex method. Mathematical Programming Computa-
tion, 10(1):119–142, 2018.
Li, Q., Zhang, L., and Mak-Hau, V. Synthesizing mixed-
integer linear programming models from natural language
descriptions. arXiv preprint arXiv:2311.15271, 2023.
McCormick, G. P. Computability of global solutions to
factorable nonconvex programs: Part i—convex underes-
timating problems. Mathematical programming, 10(1):
147–175, 1976.
OpenAI.
Introducing ChatGPT.
https://openai.
com/index/chatgpt/, November 2022. Accessed:
2026-01-28.
Ramamonjison, R., Yu, T., Li, R., Li, H., Carenini, G., Ghad-
dar, B., He, S., Mostajabdaveh, M., Banitalebi-Dehkordi,
A., Zhou, Z., and Zhang, Y. Nl4opt competition: For-
mulating optimization problems based on their natural
language descriptions. In Ciccone, M., Stolovitzky, G.,
and Albrecht, J. (eds.), Proceedings of the NeurIPS 2022
Competitions Track, volume 220 of Proceedings of Ma-
chine Learning Research, pp. 189–203. PMLR, 28 Nov–
09 Dec 2022. URL https://proceedings.mlr.
press/v220/ramamonjison23a.html.
Thind, R., Sun, Y., Liang, L., and Yang, H. Optimai: Op-
timization from natural language using llm-powered ai
agents. arXiv preprint arXiv:2504.16918, 2025.
Vielma, J. P. Mixed-integer linear programming formulation
techniques. SIAM Review, 57(1):3–57, 2015. doi: 10.
1137/130915303.
W¨achter, A. and Biegler, L. T. On the implementation of an
interior-point filter line-search algorithm for large-scale
nonlinear programming. Mathematical Programming,
106(1):25–57, 2006. doi: 10.1007/s10107-004-0559-y.
URL https://link.springer.com/article/
10.1007/s10107-004-0559-y.
Wasserkrug, S., Boussioux, L., den Hertog, D., Mirzazadeh,
F., Birbil, S¸. ˙I., Kurtz, J., and Maragno, D. Enhanc-
ing decision making through the integration of large lan-
guage models and operations research optimization. Pro-
ceedings of the AAAI Conference on Artificial Intelli-
gence, 39(27):28643–28650, Apr. 2025. doi: 10.1609/
aaai.v39i27.35090. URL https://ojs.aaai.org/
index.php/AAAI/article/view/35090.
Williams, H. P. Model Building in Mathematical Program-
ming. John Wiley & Sons, 5 edition, 2013. ISBN 978-1-
118-44333-0.
Xiao, Z., Zhang, D., Wu, Y., Xu, L., Wang, Y. J., Han,
X., Fu, X., Zhong, T., Zeng, J., Song, M., and Chen, G.
Chain-of-experts: When LLMs meet complex operations
research problems. In The Twelfth International Confer-
ence on Learning Representations, 2024. URL https:
//openreview.net/forum?id=HobyL1B9CZ.
9


--- Page 10 ---
LinearizeLLM
A. Baselines
A.1. One-shot LLM
The one-shot baseline replaces the orchestrated LinearizeLLM workflow with a single LLM agent. This agent receives
the full LATEX optimization problem and is instructed to provide the full exact linear reformulation in a single response.
The resulting reformulation is passed to the standard code-generation agent to produce executable Python code. The full
instruction templates for this agent are provided in Section B.3.
A.2. Deterministic rules-based linearizer
As a deterministic benchmark, we utilize the pyomo.gdp module, a specialized engine within the Pyomo ecosystem
designed for logic-based optimization problem transformations. Because this engine requires models to be defined in a
specific “logical form,” our pipeline first performs symbolic preprocessing via SymPy to normalize nonlinear expressions.
These are then translated into Generalized Disjunctive Programming (GDP) constructs, specifically disjuncts and logical
constraints. We then invoke Pyomo’s internal transformation suite to apply exact linear reformulations (Hart et al., 2011).
This baseline is particularly strong for logical and piecewise-linear nonlinearities, though it is inherently limited to patterns
supported by the pyomo.gdp layer. It serves as a check on whether LLM-based reformulation can compare against the
precision of established symbolic optimization tools. The full instruction templates for this agent are provided in Section B.4.
B. Prompt templates
This section contains all the prompts used in the LinearizeLLM for pattern detection, reformulation, and code generation.
The original markdown prompt files have been transformed into pure LATEX code with proper mathematical formatting and
structured documentation. The content is the same.
B.1. Detection Prompt
You are the DETECT agent. Given a LaTeX optimization problem, find **all
nonlinear terms** Your goal is to identify everything that prevents the model
from having a clearly maintained linear structure (MILP/LP).
,→
,→
**CRITICAL:** If a term (e.g., a square root, product, or quotient) involves only
parameters/constants, it is a constant coefficient and should be ignored.
Similarly, terms that are mathematically linear (e.g., a variable divided by
a constant parameter) must not be flagged. **Do not return or mention these
linear terms.**
,→
,→
,→
,→
For each detected term π, return:
- term_id: unique id
- expression: the nonlinear expression (as written)
- type: one of {{bilinear, minmax, absolute_value, quotient,
monotone_transformation, affine, other}}
,→
- location:
- component: objective OR constraint
- constraint_id: index/name if component=constraint
- sense: one of {{<=, >=, =}} if component=constraint
- lhs_text: full LHS text of the constraint (if component=constraint)
- rhs_text: full RHS text of the constraint (if component=constraint)
- side: "lhs" if term is on LHS, "rhs" if on RHS, "objective" if in objective
- rhs: the RHS expression/token (e.g., nrhs_31) if component=constraint
- needs_value: true if the exact value of the term must be represented
(objective, equality, or reused), false otherwise.
,→
- depth d(π): 0 for top-level; increase with nesting
- parent_id: id of parent term if nested, else null
- children_ids: ids of direct children (including affine branches)
10


--- Page 11 ---
LinearizeLLM
- pattern_role: semantic role of the node:
- "definition": sense is "=" and node covers almost full LHS or RHS
- "lhs_upper_bounded": node is on LHS and sense is <=
- "lhs_lower_bounded": node is on LHS and sense is >=
- "rhs_upper_bounding_expr": node is on RHS and sense is <=
- "rhs_lower_bounding_expr": node is on RHS and sense is >=
- "embedded": otherwise
- notes: brief clue if type=other
**DAG Construction Rules:**
- Construct a complete expression DAG.
- Every operator node (min, max, abs, etc.) MUST list ALL of its arguments as
children.
,→
- If an argument is a linear/affine expression (e.g., aˆ\top x + b), represent it
as a lightweight node:
,→
- type: "affine"
- children_ids: []
- This ensures arity is preserved (e.g., a min{{...}} with 4 branches has 4
children).
,→
What counts as nonlinear (non-exhaustive):
- Products of different **decision variables** (bilinear) such as x*y where x and
y are decision variables.
,→
- Min/Max, absolute value involving decision variables.
- Quotients and divisions where the **denominator contains decision variables**.
- Monotone transforms of decision variables or linear forms: sqrt(\cdot),
log(\cdot), exp(\cdot), positive powers, other general smooth monotone
functions.
,→
,→
- Any nested combination of the nonlinear patterns mentioned above (e.g.,
sqrt(max_i |...|), log(sum of products), etc.).
,→
What counts as a decision variable:
- Decision variables are symbols that appear in the outer optimization goal
operator, i.e., below the main operator $\min/\max$ (e.g., $\min_{{x}}$ or
$\min_{{x,y}}$), and are chosen by the optimizer to minimize/maximize the
objective.
,→
,→
,→
- In contrast, $\max(\cdot)$/$\min(\cdot)$ patterns that occur **within** the
objective or constraints are part of the optimization problem's structure
(nonlinear patterns) and do not introduce additional decision variables,
unless they are replaced by auxiliary (epigraph) variables in a
reformulation.
,→
,→
,→
,→
- **CRITICAL**: If we have something like $\min_{{x}}$ where $x$ represents a
vector/array, and within the objective or constraints $x$ appears indexed
such as $x_{{ij}}$, then each indexed component (e.g., $x_{{ij}}$) is still a
decision variable (a component of $x$).
,→
,→
,→
Requirements:
- Cover every nonlinear term involving decision variables; include intermediate
nested subterms IF and ONLY IF they are also nonlinear.
,→
- Differentiate clearly: ignore and **do not return** expressions that
mathematically just contain parameters and no decision variables.
,→
- Preserve nesting: children_ids capture the inner nonlinear subterms; depth
increases inward.
,→
11


--- Page 12 ---
LinearizeLLM
- If a subterm is linear (e.g., the argument of a sqrt(\cdot) or exp(\cdot) is a
linear sum), it is NOT a nonlinear subterm and should NOT be returned as a
separate term.
,→
,→
- If no nonlinearities exist, return an empty list (has_nonlinearities=false).
Input:
- LaTeX model: {latex_model}
- Parameter context: {parameter_context}
Output format:
{format_instructions}
B.2. Reformulation Prompt
You are an expert in exact linearization of nonlinear terms for
mixed-integer/linear optimization.
,→
Task:
- Reformulate the given nonlinear pattern(s) exactly into a linear (MILP/LP)
form. Use the most compact linear formulation of the nonlinear pattern in
terms of variables and constraints.
,→
,→
- **SURGICAL REPLACEMENT RULE (CRITICAL):** Identify the exact `pattern_text`
within the model. Replace **ONLY** this specific occurrence with a new,
unique auxiliary variable (e.g., aux_1). Do **NOT** reformulate other
nonlinear parts of the model that do not match the current `pattern_text`. Do
**NOT** apply global transformations (like Charnes-Cooper) unless the
`pattern_text` itself is the root of that transformation.
,→
,→
,→
,→
,→
- **BOTTOM-UP REFORMULATION CONTEXT:** You are processing a nested tree of
nonlinearities from the leaves up. The `full LaTeX model` may already contain
auxiliary variables (e.g., aux_1) from deeper reformulations. Treat these as
standard linear variables.
,→
,→
,→
- **ROOT-LEVEL OPTIMIZATION:** If the `location context` indicates this term is
the root of an objective or a constraint, you may reformulate the model
structure directly (e.g., removing a monotone square root from the objective)
instead of using an auxiliary, if it leads to a more efficient MILP.
,→
,→
,→
- Clearly maintain a linear structure in the updated optimization problem; ensure
all terms, constraints, and the objective function are linear after
reformulation.
,→
,→
- If you introduce new variables, the resulting constraints and objective MUST be
linear in those variables.
,→
- Keep the full LaTeX model consistent; return the **ENTIRE** updated model
including all original linear parts and all added auxiliary variables and
constraints. **DO NOT truncate the model or return only the reformulated
part.**
,→
,→
,→
- The ultimate goal is to find linear optimization problems (LPs/MILPs) that are
equivalent to the nonlinear optimization problems (NLPs) handed-over by the
user.
,→
,→
- Both formulations (the LP/MILP and NLP versions) should have the same optimal
points and therefore optimal objective values.
,→
**Pattern-Specific Requirements:**
12


--- Page 13 ---
LinearizeLLM
- **MONOTONE TRANSFORMATION:** When simplifying the objective by removing a
monotone function (e.g., \( \min \sqrt{{f(x)}} \)), you MUST provide the
exact Python expression required to reconstruct the original objective value.
Enter this in the `post_processing` field (e.g., "math.sqrt(model.ObjVal)").
,→
,→
,→
Input:
- pattern_type: {pattern_type}
- pattern_text: {pattern_text}
- full LaTeX model to update:
{latex_model}
- location context (where the term appears):
{location_info}
- parameter info (if provided, for validation only):
{param_info}
Output requirements:
{format_instructions}
Notes:
- If Big-M is used, derive and state M explicitly.
- Preserve semantic equivalence; do not approximate.
B.3. One-shot Prompt
You are an Operations Research expert specialized in **exact linear
reformulations** of nonlinear patterns in optimization models.
,→
## TASK
From the given LaTeX model and parameter context:
1. Detect all linearizable nonlinear patterns.
2. Reformulate each nonlinear pattern you detect **exactly** as a
linear/mixed-integer linear problem LP/MILP using the tightest valid
technique.
,→
,→
3. Utilize the best reformulation strategy in order to end-up with a fully linear
optimization problem (LP/MILP).
,→
4. Output a **single fully linear LaTeX model** that is mathematically
equivalent.
,→
## INPUT
- **LaTeX model:** {latex_model}
- **Parameter context:** {parameter_context}
- **Parameter values:** {param_info}
- **Decision variables:** {sets_info}
## PATTERNS
Please check for nonlinearity patterns that can be linearized exact and apply
appropriate reformulation techniques on those patterns.
,→
## OUTPUT FORMAT
**DETECTION**
13


--- Page 14 ---
LinearizeLLM
- Nonlinearities: YES/NO
- Patterns: List specific patterns found, grouped by type
- If no patterns found, write "NONE"
- If patterns found, list them like: "Bilinear: x*y, Min: min(x,y), Max:
max(x,y)"
,→
**REFORMULATION REPORT**
For each detected pattern family:
- Pattern: expression and indices
- Technique: chosen method
- Bounds / M: numeric or "n/a"
- Aux vars: new variables
- Verification: one-line justification
- For monotone transformations: apply the transformation in the code after
deriving the optimal objective function value
,→
**REFORMULATED MODEL (LaTeX)**
- Full linearized model with:
- Decision variables (including new ones)
- Linear objective
- Original constraints
- New reformulation constraints
**REFORMULATION INFORMATION**
- Concise summary of transformations applied, bounds, Big-M values, and domain
conditions.
,→
**POST-CODE HANDOVER (MONOTONE)**
- Apply: YES/NO
- Direction: increasing/decreasing
- Targets: expressions/variables to transform (list)
- Bounds/domain: required bounds or domain conditions
- Transformation note: brief formula or mapping to apply after code generation
**FINAL CHECKS**
- Residual nonlinearities: NONE (or list if any)
- Big-M constants numeric and tightened: YES/NO
- Index sets consistent and defined: YES/NO
## RULES
- Output must be LP/MILP only.
- Stop at first technique that is Applicable=YES and Exact=YES.
- Focus only on mathematical reformulation - code generation will be handled
separately.
,→
B.4. Pyomo Conversion Prompt
You are an expert in Operations Research and Mixed-Integer Optimization. Convert
the provided LaTeX-formulated optimization model into efficient and
immediately executable Python code using Gurobi's gurobipy API.
,→
,→
TASK REQUIREMENTS:
- Accurately translate the LaTeX model into gurobipy Python code.
- Maintain complete mathematical equivalence between LaTeX and Python.
- Clearly maintain a linear structure in the generated code; ensure that all
reformulated terms are represented using linear variables and constraints.
,→
14


--- Page 15 ---
LinearizeLLM
- Clearly define all parameters, variables, constraints, and objectives.
- Integrate the provided parameters explicitly and correctly.
- Ensure the Python code is clear, efficient, and ready to run without
modification.
,→
PROVIDED INFORMATION:
PARAMETER CONTEXT:
{parameter_context}
AVAILABLE PARAMETERS:
{param_info}
{sets_info}
REFORMULATION INFORMATION:
{reformulation_context}
LATEX MODEL:
{latex_model}
GUIDELINES FOR CODE GENERATION:
- Start with imports:
from gurobipy import *
- Define parameters with provided numeric values.
- Map abstract indices and summations from LaTeX to concrete Python loops or
efficient vectorized expressions.
,→
- Use gurobipy methods effectively:
- model.addVars() for indexed variables.
- model.addVar() for single variables.
- quicksum() for efficient summations.
- Clearly comment reformulations if applicable (e.g., "# McCormick envelopes for
bilinear terms").
,→
- Handle parameter dictionaries safely (e.g., param.get(key, default)).
- Finish your script with:
model.optimize()
CRITICAL QUICKSUM RULES:
- NEVER use conditional expressions inside quicksum() like: quicksum(x[i] for i
in range(n) if condition)
,→
- Instead, use explicit loops with LinExpr or separate quicksum calls
- For conditional summations, use this pattern:
```python
# WRONG: quicksum(x[i] for i in range(n) if condition[i])
# CORRECT:
expr = LinExpr()
for i in range(n):
if condition[i]:
expr += x[i]
```
- Or use separate quicksum calls for different conditions
15


--- Page 16 ---
LinearizeLLM
IMPORTANT:
- Do NOT include markdown formatting or fences.
- Output should be only executable Python code with explanatory comments where
necessary.
,→
Generate ONLY executable Python code without any formatting, explanations, or
markdown:
,→
C. Exact Linearization Recipes
We collect compact examples showing how to derive exact LP/MILP counterparts for the nonlinear patterns used in the
paper.
C.1. Bilinear products
Case A: binary × binary.
Let b1, b2 ∈{0, 1} and introduce w = b1b2. An exact linearization is
w ≤b1,
w ≤b2,
w ≥b1 + b2 −1,
0 ≤w ≤1.
(1)
(Integrality of w is implied when b1, b2 are binary.)
Case B: binary × bounded continuous.
Let b ∈{0, 1}, x ∈[L, U] with known bounds, and z = b x. Then the following
four inequalities are exact:
z ≤Ub,
z ≥Lb,
z ≤x −L(1 −b),
z ≥x −U(1 −b).
(2)
When b = 0 these force z = 0; when b = 1 they force z = x.
C.2. Minimum and maximum of linear functions
Constraint splitting for min.
For linear functions fk(x), k ∈K, the constraint
t ≤min
k∈K{fk(x)}
is equivalent to the set of linear constraints t ≤fk(x) ∀k ∈K.
Epigraph for max (objective).
To solve minx maxk∈K fk(x) introduce an auxiliary z and use
min
x,z
z
s.t.
z ≥fk(x) ∀k ∈K.
(3)
This epigraph encoding is the standard exact reformulation when max appears in the objective.
C.3. Absolute value
Objective form.
To minimize |t| (with t linear), introduce y and write
min y
s.t.
y ≥t, y ≥−t.
(4)
Equality form.
To enforce y = |t| exactly without binaries, use the positive/negative parts:
t = t+ −t−,
y = t+ + t−,
t+, t−≥0.
(5)
C.4. Linear fractional (Charnes–Cooper)
Consider the fractional objective
min
x
a⊤x + b
c⊤x + d
s.t.
Fx ≤g, c⊤x + d > 0.
16


--- Page 17 ---
LinearizeLLM
Charnes–Cooper sets t =
1
c⊤x+d > 0, y = xt, giving the LP
min
y,t
a⊤y + b t
(6)
s.t. Fy ≤g t,
c⊤y + d t = 1,
t ≥0.
(7)
(Contrast with a partial cross-multiplication u(c⊤x + d) ≥a⊤x + b, which leaves the bilinear term u x and is therefore
nonlinear.)
C.5. Monotone transformations
Let ϕ : R →R be strictly monotone.
Objectives.
If ϕ is strictly increasing, then min ϕ(g(x)) is equivalent to min g(x) (solve in g and, if needed, report back
the original value via ϕ). Example: minimizing exp(s) is equivalent to minimizing s (and reporting exp(s⋆)).
If ϕ is strictly decreasing, then min ϕ(g(x)) is equivalent to max g(x).
Constraints.
For increasing ϕ, the constraint ϕ(g(x)) ≤α is equivalent to g(x) ≤ϕ−1(α) (and similarly ϕ(g(x)) ≥
α ⇐⇒g(x) ≥ϕ−1(α)). Example: log(y) ≤α ⇐⇒y ≤eα.
Notes.
The bilinear case with one binary and one bounded continuous variable is exact with the four McCormick
inequalities above; if both variables are continuous, McCormick gives a relaxation. For fractional objectives, positivity of
c⊤x + d on the feasible set is required.
D. Technical results generation methodology
This section describes the detailed experimental methodology for evaluating the LinearizeLLM framework with Google’s
Gemini 2.5 Flash (temperature = 0.05, max tokens = 64000, timeout = 120 s, top-p=0.9) across 40 optimization
problems and three information scenarios.
Experimental Framework.
Each experiment uses three fixed seeds (1, 2, 3) to ensure reproducibility. All experiments
were conducted on a system with AMD Ryzen Threadripper PRO 7955WX processor @ 4.5 GHz, 512 GB RAM, and PNY
NVIDIA T1000 8 GB graphics card. Detailed instructions for executing these experiments can be found in the project
README.md.
Performance Evaluation.
We evaluate against reference solves using OSR, DSR, RSR, and CSR. For each instance, we
construct a reference formulation in gurobipy (hand-written for hand-crafted benchmarks; auto-generated for synthetic
benchmarks) by applying Gurobi-supported exact reformulations (including general constraints and bounded big-M for
binary–continuous products, with required domain checks for linear-fractional constraints). We solve with Gurobi Optimizer
12 with default parameters and use the resulting objective value as the OSR reference.
17


--- Page 18 ---
LinearizeLLM
E. Computational Efficiency and Token Analysis
Depth
Method
Agent
Tokens
Avg. Time
d = 1
LinearizeLLM
Pattern Detection
6,219 ± 3,028
16.37s
LinearizeLLM
Reformulation
8,556 ± 3,975
27.54s
LinearizeLLM
Code Generation
4,792 ± 2,190
15.90s
One-Shot
One-Shot
5,973 ± 2,409
21.88s
d = 2
LinearizeLLM
Pattern Detection
14,037 ± 8,388
45.30s
LinearizeLLM
Reformulation
63,437 ± 49,267
193.64s
LinearizeLLM
Code Generation
11,401 ± 5,204
34.70s
One-Shot
One-Shot
16,180 ± 5,274
59.74s
d ≥3
LinearizeLLM
Pattern Detection
14,486 ± 3,748
47.85s
LinearizeLLM
Reformulation
71,776 ± 41,938
222.59s
LinearizeLLM
Code Generation
9,274 ± 3,860
27.19s
One-Shot
One-Shot
15,530 ± 10,793
56.74s
Table 5. Performance statistics per agent across nesting depths. Token counts are reported as mean ± standard deviation.
F. Perturbation Levels
We define five levels of LATEX input perturbation, ranging from clean baseline models to highly modified versions that
combine multiple noise types.
• L0 (Clean): The original, unchanged LATEX optimization problems from our dataset.
• L1 (Format Noise): Introduces syntactic variations that do not change the mathematical symbols (e.g. arbitrary
whitespace changes or inconsistent line breaks).
• L2 (Macro + Refactor): Introduces local macro definitions (e.g. using \def) for variables and parameters, rewrites
operator formatting (e.g., \max vs. max), and splits long expressions into multiple lines.
• L3 (Structure-preserving Reorder): Modifies the logical structure of the model without altering its semantics. This
includes reordering the sequence of constraints, swapping terms within summations, and switching between set notation
(e.g., i ∈I) and index notation (e.g., i = 1, . . . , I).
• L4 (Combined): A composite level that simultaneously applies all perturbations from L1, L2, and L3 to test the
system’s performance under extreme syntactic variance.
F.1. Perturbation example L1
\min_{x, z} \quad & \sum_{(i,j) \in \mathcal{L}} \sum_{p \in \mathcal{P}} \left(
ShipmentCost_{i,j,p} * x_{i,j,p} - 0.2 * ShipmentCost_{i,j,p} * x_{i,j,p} *
z_{i,j} \right) + z_{i,j} * ContractCosts_{i,j} \\
,→
,→
\text{s.t.} \quad & \sum_{j:\,(j,i)\in\mathcal{L}} x_{j,i,p} + Supply_{i,p} =
\sum_{j:\,(i,j)\in\mathcal{L}} x_{i,j,p} + Demand_{i,p} && \forall
i\in\mathcal{C}, p\in\mathcal{P}, \\
,→
,→
& x_{i,j,p} \leq Capacity_{i,j,p} && \forall (i,j)\in\mathcal{L},
p\in\mathcal{P}, \\
,→
& \sum_{p\in\mathcal{P}} x_{i,j,p} \leq JointCapacity_{i,j} && \forall
(i,j)\in\mathcal{L}, \\
,→
& x_{i,j,p} \geq 0 && \forall (i,j)\in\mathcal{L}, p\in\mathcal{P} \\
& z_{i,j} \in \{0,1\} && \forall (i,j)\in\mathcal{L}
18


--- Page 19 ---
LinearizeLLM
F.2. Perturbation example L2
\def\scost{ShipmentCost}
\def\ccost{ContractCosts}
\min_{x, z}\quad & \sum_{(i,j)\in\mathcal{L}}\sum_{p\in\mathcal{P}}
\left(\scost_{i,j,p} \cdot x_{i,j,p} - (0.2 \cdot \scost_{i,j,p} \cdot
x_{i,j,p} \cdot z_{i,j})\right) + (z_{i,j} \cdot \ccost_{i,j})\\
,→
,→
\text{s.t.}\quad
& \sum_{j:(j,i)\in\mathcal{L}} x_{j,i,p} + Supply_{i,p} =
\sum_{j:(i,j)\in\mathcal{L}} x_{i,j,p} + Demand_{i,p}&& \forall
i\in\mathcal{C}, p\in\mathcal{P},\\
,→
,→
& x_{i,j,p} \leq Capacity_{i,j,p} && \forall (i,j)\in\mathcal{L},
p\in\mathcal{P},\\
,→
& \sum_{p\in\mathcal{P}} x_{i,j,p} \leq JointCapacity_{i,j} && \forall
(i,j)\in\mathcal{L}, \\[6pt]
,→
& x_{i,j,p} \geq 0 && \forall (i,j)\in\mathcal{L}, p\in\mathcal{P} \\[6pt]
& z_{i,j} \in \{0,1\} && \forall (i,j)\in\mathcal{L}
F.3. Perturbation example L3
\min_{x, z}\quad & \sum_{p\in\mathcal{P}}\sum_{(i,j)\in\mathcal{L}}
\left(ShipmentCost_{i,j,p} * x_{i,j,p} - 0.2 * ShipmentCost_{i,j,p} *
x_{i,j,p} * z_{i,j}\right) + z_{i,j} * ContractCosts_{i,j}\\
,→
,→
\text{s.t.}\quad
& x_{i,j,p} \;\leq\; Capacity_{i,j,p} && \forall
(i,j)\in\mathcal{L},\;p\in\mathcal{P},\\
,→
& \sum_{p\in\mathcal{P}} x_{i,j,p} \;\leq\; JointCapacity_{i,j} && \forall
(i,j)\in\mathcal{L}, \\
,→
& \sum_{j:\,(j,i)\in\mathcal{L}} x_{j,i,p} + Supply_{i,p} =
\sum_{j:\,(i,j)\in\mathcal{L}} x_{i,j,p} + Demand_{i,p}&& \forall
i\in\mathcal{C},\;p\in\mathcal{P},\\
,→
,→
& z_{i,j} \;\in\; \left\{0,1\right\} && \forall (i,j)\in\mathcal{L}, \\
& x_{i,j,p} \;\geq\; 0 && \forall (i,j)\in\mathcal{L},\;p\in\mathcal{P}
F.4. Perturbation example L4
\def\scost{ShipmentCost}
\def\ccost{ContractCosts}
\min_{x, z} \quad & \sum_{p \in \mathcal{P}} \sum_{(i,j) \in \mathcal{L}} \left(
(\scost_{i,j,p} \cdot x_{i,j,p}) - (0.2 \cdot \scost_{i,j,p} \cdot x_{i,j,p}
\cdot z_{i,j}) \right) + (z_{i,j} \cdot \ccost_{i,j}) \\
,→
,→
\text{s.t.} \quad & x_{i,j,p} \leq Capacity_{i,j,p} && \forall (i,j) \in
\mathcal{L}, p \in \mathcal{P}, \\
,→
& \sum_{p \in \mathcal{P}} x_{i,j,p} \leq JointCapacity_{i,j} && \forall (i,j)
\in \mathcal{L}, \\
,→
& \sum_{j:(j,i) \in \mathcal{L}} x_{j,i,p} + Supply_{i,p} = \sum_{j:(i,j) \in
\mathcal{L}} x_{i,j,p} + Demand_{i,p} && \forall i \in \mathcal{C}, p \in
\mathcal{P}, \\
,→
,→
& z_{i,j} \in \{0,1\} && \forall (i,j) \in \mathcal{L}, \\
& x_{i,j,p} \geq 0 && \forall (i,j) \in \mathcal{L}, p \in \mathcal{P}
G. Context ablation levels
We define six incremental context categories used in the context ablation study (Section 7.3):
• C1 (Minimal): Provides only the mathematical model with no external metadata; agents receive only the component
19


--- Page 20 ---
LinearizeLLM
type (objective vs. constraint) for each term.
• C2 (Variables Only): Adds decision variable definitions.
• C3 (Parameters Only): Provides only the names of concrete parameters to distinguish them from decision variables.
• C4 (Vars + Param Names): Combines metadata from C2 and C3.
• C5 (Vars + Param Values): Extends C4 by including the concrete numerical values of all parameters.
• C6 (Full Context): Includes all previous metadata plus the full local constraint data.
We evaluate these context levels on 10 randomly sampled instances (5 with d = 2 and 5 with d ≥3).
H. Arg min comparison for original vs. linearized models
We report side-by-side optimal solutions (objective value and variable assignments) for the original nonlinear model as
additional supporting evidence for equivalent reformulations beyond OSR. Since Gurobi and LinearizeLLM introduce
auxiliary variables and may use different variable names, comparisons should be made after accounting for these naming
differences and auxiliaries. For conciseness, we show only the first run per depth-1 problem, regardless of whether the run
satisfies the OSR criterion; the remaining runs are provided in the supplementary code.
H.1. problem blend problem abs
Original
Objective
10.0
Variable
Value
absolute deviations.0
0.0
absolute deviations.1
0.0
alloy amounts.0
1.0
alloy amounts.1
0.0
signed deviations.0
0.0
signed deviations.1
0.0
LinearizeLLM
Objective
10.0
Variable
Value
x[0]
1.0
x[1]
0.0
z[0]
0.0
z[1]
0.0
H.2. problem blend problem frac
Original
Objective
0.05749999999999999
Variable
Value
cc variables.0
0.0
cc variables.1
0.027777777777777693
cc variables.2
0.1111111111111112
original mix.0
0.0
original mix.1
199.9999999999994
original mix.2
800.0000000000007
ratio variable
0.0001388888888888889
LinearizeLLM
Objective
0.0
Variable
Value
t
0.0
y[0]
0.0
y[1]
0.0
y[2]
0.0
20


--- Page 21 ---
LinearizeLLM
H.3. problem diet problem min abs
Original
Objective
9.333333333333334
Variable
Value
absolute cost gap
9.333333333333334
cost difference
−9.333333333333334
food quantities.0
9.333333333333334
food quantities.1
1.3333333333333333
min values.0
50.0
min values.1
30.0
nutrient intake.0
100.0
nutrient intake.1
60.0
LinearizeLLM
Objective
9.333333333333334
Variable
Value
b[0]
0.0
b[1]
1.0
x[0]
9.333333333333334
x[1]
1.3333333333333333
z abs 1
9.333333333333334
H.4. problem diet problem monotone
Original
Objective
30740.40934396602
Variable
Value
0
4.666666666666667
1
0.6666666666666664
LinearizeLLM
Objective
30740.40934396602
Variable
Value
x[0]
4.666666666666667
x[1]
0.6666666666666664
H.5. problem diet problem nonlinear frac
Original
Objective
1.5
Variable
Value
cost ratio
1.5
food quantities.0
0.0
food quantities.1
3.6698321894119537
LinearizeLLM
Objective
1.5
Variable
Value
t
1.0
y 1
0.0
y 2
1.0
H.6. problem knapsack problem nonlinear min 1
Original
Objective
280.0
Variable
Value
item selections.0
1.0
item selections.1
1.0
item selections.2
1.0
min x1 x3
1.0
LinearizeLLM
Objective
280.0
Variable
Value
aux 1
1.0
x[0]
1.0
x[1]
1.0
x[2]
1.0
H.7. problem knapsack problem nonlinear min 2
Original
Objective
220.0
Variable
Value
item selections.0
−0.0
item selections.1
1.0
item selections.2
1.0
min x0 x1
0.0
LinearizeLLM
Objective
220.0
Variable
Value
aux 1
0.0
x[0]
−0.0
x[1]
1.0
x[2]
1.0
21


--- Page 22 ---
LinearizeLLM
H.8. problem media selection nonlinear binbin
Original
Objective
20.0
Variable
Value
media selections.0
1.0
media selections.1
1.0
media selections.2
0.0
LinearizeLLM
Objective
20.0
Variable
Value
aux 1
1.0
x[0]
1.0
x[1]
1.0
x[2]
0.0
H.9. problem media selection nonlinear bincon
Original
Objective
15.0
Variable
Value
media selections.0
1.0
media selections.1
1.0
media selections.2
0.0
LinearizeLLM
Objective
15.0
Variable
Value
q[0]
0.0
q[1]
5.0
q[2]
0.0
x[0]
1.0
x[1]
1.0
x[2]
−0.0
y[0]
0.0
y[1]
5.0
y[2]
0.0
H.10. problem multi nonlinear abs
Original
Objective
240.0
Variable
Value
deviations.(0, 0)
0.0
deviations.(0, 1)
5.0
deviations.(1, 0)
0.0
deviations.(1, 1)
5.0
differences.(0, 0)
0.0
differences.(0, 1)
−5.0
differences.(1, 0)
0.0
differences.(1, 1)
5.0
shipments.(0, 0, 0)
20.0
shipments.(0, 0, 1)
15.0
shipments.(0, 1, 0)
0.0
shipments.(0, 1, 1)
15.0
shipments.(1, 0, 0)
10.0
shipments.(1, 0, 1)
10.0
shipments.(1, 1, 0)
30.0
shipments.(1, 1, 1)
0.0
LinearizeLLM
Objective
240.0
Variable
Value
aux[0,0]
0.0
aux[0,1]
5.0
aux[1,0]
0.0
aux[1,1]
5.0
x[0,0,0]
20.0
x[0,0,1]
15.0
x[0,1,0]
0.0
x[0,1,1]
15.0
x[1,0,0]
10.0
x[1,0,1]
10.0
x[1,1,0]
30.0
x[1,1,1]
0.0
22


--- Page 23 ---
LinearizeLLM
H.11. problem netasgn nonlinear abs
Original
Objective
297.0
Variable
Value
assignments.(0, 0)
2.0
assignments.(0, 1)
6.0
assignments.(1, 0)
3.0
assignments.(1, 1)
4.0
deviation pairs.(0, 1)
1.0
differences.(0, 1)
1.0
max deviation
1.0
LinearizeLLM
Objective
297.0
Variable
Value
aux 1
1.0
x[0,0]
2.0
x[0,1]
6.0
x[1,0]
3.0
x[1,1]
4.0
H.12. problem netasgn nonlinear max
Original
Objective
286.0
Variable
Value
assignments.(0, 0)
2.0
assignments.(0, 1)
6.0
assignments.(1, 0)
3.0
assignments.(1, 1)
4.0
deviations.dev 0 1
1.0
deviations.diff 0 1
1.0
max deviation
1.0
total hours.0
8.0
total hours.1
7.0
LinearizeLLM
Objective
297.0
Variable
Value
aux 1
1.0
x[0,0]
2.0
x[0,1]
6.0
x[1,0]
3.0
x[1,1]
4.0
H.13. problem netmcol nonlinear bincon
Original
Objective
10.0
Variable
Value
contracts.(0, 1)
0.0
contracts.(1, 0)
0.0
shipments.(0, 1, 0)
10.0
shipments.(1, 0, 0)
0.0
LinearizeLLM
Objective
10.0
Variable
Value
aux 1[0,0,0]
0.0
aux 1[0,1,0]
0.0
aux 1[1,0,0]
0.0
aux 1[1,1,0]
0.0
x[0,0,0]
0.0
x[0,1,0]
10.0
x[1,0,0]
0.0
x[1,1,0]
0.0
z[0,0]
0.0
z[0,1]
0.0
z[1,0]
0.0
z[1,1]
0.0
23


--- Page 24 ---
LinearizeLLM
H.14. problem netmcol nonlinear frac
Original
Objective
0.2
Variable
Value
demand.(0, 0)
0.0
demand.(1, 0)
10.0
denominator
10.0
numerator
2.0
ratio
0.2
shipments.(0, 0)
10.0
LinearizeLLM
Objective
0.19980019980019983
Variable
Value
t
0.09990009990009992
x hat[0,0]
0.9990009990009991
y hat[0,0]
0.0
y hat[1,0]
0.9990009990009991
H.15. problem nltrans nonlinear bincon
Original
Objective
305.0
Variable
Value
investment
0.0
shipments.(0, 0)
5.0
shipments.(0, 1)
15.0
shipments.(1, 0)
25.0
shipments.(1, 1)
5.0
LinearizeLLM
Objective
305.0
Variable
Value
aux1
305.0
aux2
0.0
x[0,0]
5.0
x[0,1]
15.0
x[1,0]
25.0
x[1,1]
5.0
z
0.0
H.16. problem nltrans nonlinear max
Original
Objective
305.0
Variable
Value
differences.(0, 0)
−10.0
differences.(0, 1)
−10.0
differences.(1, 0)
0.0
differences.(1, 1)
−15.0
excess.(0, 0)
0.0
excess.(0, 1)
0.0
excess.(1, 0)
0.0
excess.(1, 1)
0.0
shipments.(0, 0)
5.0
shipments.(0, 1)
15.0
shipments.(1, 0)
25.0
shipments.(1, 1)
5.0
LinearizeLLM
Objective
305.0
Variable
Value
x[0,0]
5.0
x[0,1]
15.0
x[1,0]
25.0
x[1,1]
5.0
y[0,0]
0.0
y[0,1]
0.0
y[1,0]
0.0
y[1,1]
0.0
H.17. problem prod nonlinear bincon
Original
Objective
55.66666666666667
Variable
Value
campaign
0.0
production.0
4.0
production.1
1.1666666666666667
production.2
3.0
LinearizeLLM
Objective
55.66666666666667
Variable
Value
aux 1
55.66666666666667
aux 2
0.0
x[0]
4.0
x[1]
1.1666666666666667
x[2]
3.0
z
0.0
24


--- Page 25 ---
LinearizeLLM
H.18. problem prod nonlinear max
Original
Objective
89.5
Variable
Value
consumption minus b
4.833333333333333
excess
4.833333333333333
production.0
4.0
production.1
6.0
production.2
3.0
LinearizeLLM
Objective
89.5
Variable
Value
aux 1
4.833333333333333
x[0]
4.0
x[1]
6.0
x[2]
3.0
H.19. problem revenue maximization nonlinear bincon
Original
Objective
8000.0
Variable
Value
campaign
1.0
package sales.0
20.0
package sales.1
40.0
LinearizeLLM
Objective
8000.0
Variable
Value
aux 1
0.0
aux 2
8800.0
x[0]
20.0
x[1]
40.0
z
1.0
25
