--- Page 1 ---
1
Tool-Aided Evolutionary LLM for Generative
Policy Toward Efficient Resource Management in
Wireless Federated Learning
Chongyang Tan, Ruoqi Wen, Rongpeng Li, Zhifeng Zhao, Ekram Hossain, and Honggang Zhang
Abstract—Federated Learning (FL) enables distributed model
training across edge devices in a privacy-friendly manner. How-
ever, its efficiency heavily depends on effective device selection
and high-dimensional resource allocation in dynamic and hetero-
geneous wireless environments. Conventional methods demand a
confluence of domain-specific expertise, extensive hyperparame-
ter tuning, and/or heavy interaction cost. This paper proposes
a Tool-aided Evolutionary Large Language Model (T-ELLM)
framework to generate a qualified policy for device selection in a
wireless FL environment. Unlike conventional optimization meth-
ods, T-ELLM leverages natural language-based scenario prompts
to enhance generalization across varying network conditions.
The framework decouples the joint optimization problem mathe-
matically, enabling tractable learning of device selection policies
while delegating resource allocation to convex optimization tools.
To facilitate the evolutionary process, T-ELLM interacts with a
sample-efficient, model-based virtual learning environment that
captures the relationship between device selection and learning
performance. This developed virtual environment reduces re-
liance on real-world interactions, thus minimizing communication
overhead while refining the LLM-based decision-making policy
through group relative policy optimization. Theoretical analysis
proves that the discrepancy between virtual and real environ-
ments is bounded, ensuring the advantage function learned in the
virtual environment maintains a provably small deviation from
real-world conditions. Experimental results demonstrate that T-
ELLM outperforms benchmark methods in energy efficiency and
exhibits robust adaptability to environmental changes.
Index Terms—Large language model, generative policy, wire-
less federated learning, resource management, convex optimiza-
tion, reinforcement learning.
I. INTRODUCTION
N
EXT-Generation (xG) wireless communication systems
are envisioned to support various intelligent applications
and services [1], empowered by the exponential growth of
wireless edge devices, such as mobile phones and sensors.
As a prominent paradigm [2], [3], Federated Learning (FL)
emerges by facilitating collaborative model training across
decentralized devices while maintaining data locality in a
privacy-friendly manner. Nevertheless, the deployment of FL
in wireless networks faces severe challenges, arising from
the underlying heterogeneous computation and communication
capabilities across devices [4], [5]. Correspondingly, a proper
C. Tan, R. Wen, and R. Li are with the College of Information Science
and Electronic Engineering, Zhejiang University, Hangzhou 310058, China
(email: {cyatan, wenruoqi, lirongpeng}@zju.edu.cn).
Z. Zhao is with Zhejiang Lab, Hangzhou 311121, China, and also with
the College of Information Science and Electronic Engineering, Zhejiang
University, Hangzhou 310058, China (email: zhaozf@zhejianglab.org).
Ekram Hossain is with the University of Manitoba, Winnipeg, Canada
(email: ekram.hossain@umanitoba.ca).
Honggang Zhang is with Macau University of Science and Technology,
China (email: hgzhang@must.edu.mo).
resource management policy for FL, which selects the partici-
pating devices and calibrates the utilized communications and
computing resources, will need to be developed in dynamic
wireless environments [6]. Typically, such a problem results
in a high-dimensional optimization problem that can be par-
tially solved by heuristic solutions [7], [8] or Reinforcement
Learning (RL)-based approaches [9]–[13]. However, heuristic
solutions often demand a confluence of domain-specific exper-
tise and extensive tuning before adapting to unseen scenarios,
while RL-based approaches require heavy interactions with the
environment and become sluggish for large system dimensions
and changing system dynamics [14]–[16]. Therefore, there is
a strong incentive to find alternative solutions [17].
A. Related Works
To implement FL over wireless networks, in each training
round, edge devices upload the locally trained updates to a cen-
tralized server via wireless links, in exchange for aggregated
models. Through several training rounds, the performance of
the eventually learned global model is primarily impacted by
resource and data heterogeneity. For example, disparities in
computational and communication capabilities across devices
lead to imbalanced time and energy consumption, while vary-
ing dataset sizes and non-Independent and Identically Dis-
tributed (non-IID) data distributions can result in biased model
updates. Both would degrade the learning effectiveness [4],
[5]. Additionally, the deployment of FL encounters constraints
due to the energy and time [8] budget [10], [18]. For instance,
the authors in [19], [20] formulate the joint learning and
communication problem as the minimization of total energy
consumption under latency constraints and provide compu-
tationally efficient closed-form solutions to optimize critical
resources, including bandwidth allocation, CPU frequency, and
transmission power. But, full device participation is bluntly
assumed in [19], [20]. However, mobilizing the participation
of a subset of qualified edge devices in each training round
can contribute to improving learning efficiency. For example,
biasing client selection towards clients with higher local loss
[21] proves to yield faster convergence than the random
selection in Federated Averaging (FedAvg) [2]. Also, a timer
can be heuristically set to avoid the participation of straggler
devices [8]. In [7], a guided participant selection scheme
improves FL performance by jointly optimizing system and
data utility through an epsilon-greedy algorithm. Moreover,
a joint device scheduling and bandwidth allocation policy,
which maximizes model accuracy within a fixed training time
budget by a hybrid greedy-optimization approach, is pro-
posed in [22], while energy constraints are neglected therein.
arXiv:2505.11570v2  [cs.LG]  11 Nov 2025


--- Page 2 ---
2
More importantly, adapting these heuristics to unexpected
scenarios, such as tuning hyperparameters to approximate FL
performance limits [8], [22] or setting appropriate timers [7]
when system bandwidth or device resources (e.g., maximum
transmission power, dataset size) change, typically demands
iterative adjustments and domain-specific expertise.
RL provides an alternative methodology to solve wire-
less FL problems. Through continuously interacting with the
environment, RL agents evaluate candidate policies in the
wireless FL environment and gradually learn appropriate real-
time policies. For example, [12] develops a Deep RL (DRL)
method to select devices to upload their local model for
aggregation, to maximize validation accuracy. Nevertheless,
it overlooks energy and time constraints and assumes all
devices participate in local training. Similarly, [11] adopts a
Q-learning-based method to identify near-optimal participating
devices. Furthermore, to ameliorate the impact of high di-
mensionality on single-agent RL with large-scale devices, [9]
introduces a Multi-Agent Reinforcement Learning (MARL)
framework that jointly optimizes model accuracy, processing
latency, and communication efficiency. On the other hand,
[13] improves the energy efficiency of FL by managing the
CPU-cycle frequency of mobile devices based on DRL. To
further improve the efficiency of FL, the works in [23]–
[25] leverage DRL-based approaches to jointly optimize the
device selection and bandwidth allocation. However, the above
DRL-based methods only optimize partial parameters and
rely on extensive training in a given environment. Therefore,
the changes in environment or data often imply cumbersome
retraining or even architectural redesign. Unfortunately, a real-
world wireless scenario constantly evolves, and it inevitably
induces substantial engineering cost. Meanwhile, direct inter-
action with the environment incurs substantial communication
overhead. Although imitation learning [15] can acquire a
policy by learning from the optimal trajectories to reduce
interaction overhead, there are no such expert systems strictly
in this FL scenario. Besides, the training difficulty, which
increases exponentially along with the potentially involved
devices and managed resources, limits the practicability [14],
[16].
The remarkable capabilities of transformer-based Large
Language Models (LLMs) [26]–[29] demonstrate the feasi-
bility of a single general-purpose model, trained on extensive
text corpora, for diverse, generalizable task accomplishment
[30]. Capitalizing on the success of LLMs, we resort to
an LLM-driven textual description approach to leverage its
inherent flexibility and generalization. Leveraging fine-tuned
LLMs for FL optimization starkly contrasts with those efforts,
which focus on enhancing the training capabilities of LLMs
in wireless FL [31], [32]. However, directly applying standard
LLM methodologies to this specialized domain presents sev-
eral fundamental obstacles. Prominently, common approaches
like training and fine-tuning require vast real-world interaction
datasets, which are prohibitively time-consuming and costly
to collect [33], [34]. The non-availability of expert decision-
making data for FL efficiency can add to the difficulty of
reliably teaching an LLM to interpret wireless FL states
and generate effective actions. Furthermore, general-purpose
LLMs are inherently weak at precise mathematical reasoning
and can be prone to hallucination [35], making them ill-suited
for optimization-centric tasks, such as resource management
in wireless FL. Meanwhile, the high latency associated with
processing overlong token sequences for complex reasoning
makes the sole adoption of LLM infeasible in time-sensitive
wireless FL systems [36]. Therefore, due to the domain-
specific expertise and computationally intensive optimization
operations, extra effort is still needed to make LLMs qualified
for decision-making in wireless FL.
B. Contributions
In this paper, we propose a Tool-aided Evolutionary LLM
(T-ELLM) approach to realize efficient device selection and
resource allocation in wireless FL. Specifically, T-ELLM
adopts a natural language-based scenario prompt, where lin-
guistic flexibility helps to improve the generalization capa-
bility [37]. Afterward, on top of a mathematics-established
problem decoupling, T-ELLM capably yields device selection
and resource management results by answering the scenario
query and invoking convex optimization tools, respectively.
Furthermore, to better tackle possible environmental changes,
alongside available convex optimization tools, T-ELLM builds
a sample-efficient model-based virtual learning environment,
which characterizes the relationship between learning effec-
tiveness and device selection. Once this high-fidelity model
is established, the T-ELLM agent can be trained for a vast
number of episodes entirely offline, at virtually zero com-
munication cost. In other words, since the interaction cost is
entirely induced by collecting samples for the simpler virtual
environment modeling, this model-based approach yields high
sample efficiency by drastically reducing the communication
and time overhead for practically interacting LLMs with the
real world. The main contributions of this paper are summa-
rized as follows:
• We consider a high-dimensional decision-making prob-
lem in wireless FL scenarios, and adopt a tool-based
evolutionary LLM approach, which is referred to as
T-ELLM. In particular, we fine-tune an LLM, which
can respond to the linguistic description of wireless FL
scenarios with an appropriate output of device selection.
The astonishing contextual understanding capability in
LLM empowers T-ELLM to cope with environmental
changes flexibly, significantly reducing manual design
effort and improving adaptability.
• We establish the mathematical rationale to decouple the
joint device selection and resource management problem,
which significantly reduces the decision-making space
to be learned. On this basis, augmented with contextual
convex optimization-based resource management tools,
T-ELLM invokes and adapts tools to perform joint op-
timization.
• We incorporate a sample-efficient model-based virtual
learning environment to ground the LLM in wireless FL
scenarios. In combination with the resource management
tool, the model-based approach enables the LLM to learn
from a simulated environment by Group Relative Policy


--- Page 3 ---
3
TABLE I: The summary of differences with related literature on efficient FL decision-making.
Performance
Guarantee
Adaptability to
Environments
Artificial Design
Complexity
Interaction
Cost
Brief Description
Heuristics-based [7], [8]
H
#
 
H
#
 
Reliance on expert hand-designed utility func-
tions
Optimization-based
[18]–[20], [22]
H
#
 
#
 
Mainly applicable for optimization problems
with expressions
RL-based
[9]–[13], [23]–[25]
 
#
H
#
#
Environment-specific interactions with calibrated
states
T-ELLM
 
H
#
 
 
Evolution in virtual environments through GRPO
with tool assistance
Notations:  Excellent; H
# Medium; # Poor.
Optimization (GRPO) [38], due to its merits in efficient
trajectory collection. The continual learning in a high-
fidelity simulation environment improves the decision-
making capabilities of LLM, while significantly reducing
communication overhead from practical interactions in
the real world.
• We prove that the overall discrepancy between the virtual
and real environment is provably bounded, yielding a
limited deviation for the advantage function learned from
the virtual environment from its counterpart in the real
environment. Besides, the experimental results show that
the proposed framework outperforms other benchmarks
in terms of energy consumption, while demonstrating its
adaptability to environmental changes.
C. Paper Structure
The remainder of this paper is organized as follows. We
introduce the system model and problem formulation in Sec-
tion II. Afterward, Section III presents how to decouple the
optimization problem and explains the rationale behind it. In
Section IV and Section V, we propose the T-ELLM scheme
and analyze its convergence properties. Afterward, we present
the simulation results in Section VI. Finally, conclusions and
future works are stated in Section VII.
II. SYSTEM MODEL AND PROBLEM FORMULATION
In this section, we first present the FL model, followed
by the definition of a wireless communication system model.
Finally, we formulate the problem of efficient wireless FL
resource management.
A. System Model
1) Wireless FL Model: We consider a wireless FL system
consisting of a Base Station (BS) equipped with a cen-
tral server and N distributed devices represented by N =
{0, 1, · · · , n, · · · , N −1}. Each device n has its local dataset,
denoted by Dn, with a size |Dn|. For a given Neural Network
(NN) training task, define the loss function as l(ω, x), where
ω represents the parameters of the NN and x is a sample in
the dataset. Subsequently, the local training loss of device n
can be calculated as:
Fn(ωn) =
1
|Dn|
X
x∈Dn l(ωn, x),
(1)
where ωn is the local parameters of device n. While FL aims
to find global parameters ωG such that the loss is minimized
TABLE II: blueList of key notations used in the paper.
Notion
Definition
t
Index of the rounds
K
The total number of rounds
n
Index of device
N
Total number of devices
Dn
Local dataset of device n
D
Entire dataset
|D|
The size of D
ωn
Weight parameter of local model of device n
ωG
Weight parameter of global model
St
The set of indexes of the selected devices in round t
Ξ(St)
Test accuracy of global model in round t
I
The number of iterations for local training
C
The number of CPU cycles to process one sample
ζ
The effective switch capacitance constant
ft,n
CPU frequency of device n during the t-th round
fn,max
The maximum CPU frequency of device n
pt,n
Transmission power of device n during the t-th round
pn,max
The maximum transmission power of device n
Bt,n
Bandwidth of device n during the t-th round
B
The total bandwidth of the system
Gt,n
Channel coefficient of device n during the t-th round
N0
The noise power spectral density
sn
The size (in bits) of the model parameters transmitted by
device n
T cmp
t,n
The time for local training at device n during the t-th round
Ecmp
t,n
The energy for local training at device n during the t-th
round
T com
t,n
The time for data uploading at device n during the t-th
round
Ecom
t,n
The energy for data uploading at device n during the t-th
round
vt,n
The transmission rate of device n during the t-th round
Et
The total energy consumption during the t-th round
Tt
The total time consumption during the t-th round
sm
t
The statistical parameters during FL training
sc
t
State parameters related to communication and computa-
tion
Rt
The allocated resources
at
Joint action of device selection and resource allocation
ot
Linguistic representations of environmental state
˜at
Text-based decision action for device selection
across the entire dataset D = S
n∈N Dn without sharing
any datasets, the corresponding optimization problem can be
formulated as:
ω∗
G = arg min
ωG F(ωG),
(2)
where F(ωG)
=
P
n∈N
|Dn|
|D| Fn(ωG) is the global loss
function.
As illustrated in Fig. 1, FL algorithms typically solve (2)
through an iterative three-step process, which encompasses
local training, aggregation, and synchronization, repeated over
multiple rounds. Taking the example of FedAvg [2], in the t-th


--- Page 4 ---
4
Aggregation
…
Policy Generation
Device Selection 
Resource Allocation
Global Model 
Broadcasting
Local Training
Local Model
Uploading
Decision Making
Observation
Fig. 1: An illustration of the wireless FL environment with a
device selection and resource management policy generator.
communication round, the central server broadcasts the (t−1)-
th global model parameters ωt−1
G
to each device, where ω0
G
indicates an initialized model. After getting the global model,
each device n uses its local dataset to update the parameters
of the local model I times, i.e.,
ωt
n(i) = ωt
n(i −1) −η∇Fn(ωt
n(i −1)),
(3)
where i ∈[1, I] represents the index of local updates and
ωt
n(0) = ωt−1
G . It is worth noting that only a portion of the
devices truly participate in the training, and these selected
devices in the t-th round are denoted as St ⊂N. All these
selected devices transmit the trained local models to the central
server, in exchange for a new, aggregated global model, i.e.,
ωt
G =
1
|D|
X
n∈St |Dn|ωt
n,
(4)
where ωt
n = ωt
n(I). Considering the impact of the subset St
on the convergence behavior of FL [5], the central server shall
calibrate a means to determine device participation, thereby
optimizing training performance.
2) Computation and Communication Model for FL:: We
consider the energy and time consumption associated with
the computation and communication processes in the wireless
FL system. Due to the high transmission power of the BS
and sufficient downlink bandwidth for global model broadcast,
the consumed time and energy at the BS are assumed to be
negligible [39], [40]. Therefore, we only focus on the device
computation and communication.
• Local Computation [23]: Let ft,n denote the CPU frequency
(in CPU cycles per second) of device n in the t-th round.
The time required for I times of local model training on de-
vice n in the t-th round can be expressed as T cmp
t,n = C|Dn|I
ft,n ,
where C (cycles per sample) represents the number of CPU
cycles needed to process one sample by the backpropagation
algorithm. Additionally, the energy consumption of device
n for local model training in the t-th round is given by
Ecmp
t,n
= ζC|Dn|If 2
t,n, where ζ is the effective switched
capacitance that depends on the chip architecture.
• Model Transmission: After completing local model training,
the selected devices upload their local model parameters to
the BS via Frequency Division Multiple Access (FDMA).
The achievable transmission rate of local device n in the
t-th round is given by vt,n = Bt,n log2

1 + pt,nGt,n
N0Bt,n

,
where Bt,n and pt,n denotes the bandwidth and transmission
power allocated to device n in the t-th communication round
respectively, Gt,n is the channel coefficient of device n in
the t-th round under the current environmental conditions,
and N0 is the noise power spectral density. Let sn denote the
size (in bits) of the model parameters transmitted by device
n. Therefore, the time required for device n to upload its
model parameters in the t-th communication round is given
by T com
t,n =
sn
vt,n . Subsequently, the energy consumption of
device n in the t-th round for uploading model parameters
can be expressed as Ecom
t,n = pt,nT com
t,n = pt,ksn
vt,n .
Above all, for each communication round, the total energy
consumption is modeled as:
Et =
X
n∈St(Ecom
t,n + Ecmp
t,n ).
(5)
Similarly, the total time consumption is given by
Tt = max
n∈St{T com
t,n + T cmp
t,n }.
(6)
B. Problem Formulation
We tackle the joint optimization of device selection and
resource allocation in wireless FL. In particular, we aim
to maximize energy efficiency while adhering to diverse
computational, communication, and task-specific constraints.
For example, some scenarios prioritize minimizing runtime,
while others only require meeting time-related Quality-of-
Service (QoS). Similarly, bandwidth allocation may be equal
or flexible, contingent on the scenario. First, we formulate an
objective function that balances FL learning performance and
device energy consumption, expressed as:
r(Ξt, Et) = (1 −σ)Ξt
Et
+ σΞt,
(7)
where σ ∈[0, 1] is a weight factor, Ξt is the performance of
the global model, i.e., test accuracy in the t-th round. Sub-
sequently, we need to optimize the objective function under
resource and task requirements, and the resource constraints
essentially affect resource allocation strategies. Towards an
exemplified scenario, which has limited CPU operating fre-
quency, transmission power, and bandwidth, and should satisfy
some time-related QoS requirements, we optimize the Cumu-
lative Weighted Performance-Energy Metric (CWPEM) as:
P1 :
max
St,ft,n,pt,n,Bt,n
XK
t=1 r(Ξt, Et)
(8a)
s.t. Tt ≤TQoS,
(8b)
ft,n ≤fn,max,
(8c)
pt,n ≤pn,max,
(8d)
X
n∈St Bt,n = B,
(8e)


--- Page 5 ---
5
where K is the total round of FL, TQoS denote the QoS
requirement for the FL competition time, B is the total band-
width of the system, fn,max and pn,max denote the maximum
local computation capacity and maximum transmission power
of device n, respectively. Constraint (8b) ensures that the
time consumption complies with the time-related QoS require-
ment. Constraints (8c) and (8d) specify that the allocated
computational frequency and transmission power must not
exceed the device’s resource limitations. Finally, Constraint
(8e) guarantees that the aggregate bandwidth utilized by the
selected devices adheres to the system’s bandwidth constraints.
A widely applied solver to the joint optimization problem
P1 is DRL [10], [19], [20]. Generally, the dynamic process
of the wireless FL can be formally modeled as the Markov
Decision Process (MDP), denoted by M = (S, A, P, R, γ),
where S is the state space with st ∈S, and st can be composed
of two parts, i.e.,
st = [sc
t, sm
t ],
(9)
where sc
t represents parameters related to communication and
computation, including ft,max, pt,max, Gt,n, Ecom
t,n , and Ecmp
t,n
in Section II-A2, while sm
t
denotes the statistical parame-
ters during training, which correspond to |Dn|, Fn(ωn), and
Ξ(St−1) in Section II-A1. A is the action space with at ∈A
encompassing device selection and corresponding resource
allocation results, i.e.,
at = [St, Rt],
(10)
where Rt represents the allocated resources, such as ft,n, pt,n,
and Bt,n, to enable the full participation of devices within the
subset St, while we call such a subset as a feasible subset1.
P is the transition function P(st+1|st, at). R is the reward
function calculated by (7) and γ is the discount factor. Gener-
ally, the NNs trained by DRL are only suitable for the current
interactive environment, and thus, they need to be re-trained
in a new environment. In addition, for different scenarios
with distinctive resource constraints and task requirements, the
NNs might be redesigned from scratch. For example, for two
scenarios with different managed resources, the action space
must be redesigned. Unfortunately, for a complicated problem
that involves multiple interdependent decision-making tasks,
the design and training of NNs becomes increasingly complex.
Besides, since the number of neurons grows exponentially
with the number of decision variables, such as fn, pn, and
Bn per device, it commonly encounters scalability issues.
Consequently, an excessively large decision-making action
space adversely affects the efficacy of DRL. Therefore, we
propose to introduce LLM to address these challenges.
III. DECOUPLING OF THE OPTIMIZATION PROBLEM
In this section, to make LLM compatible with the decision-
making environment of wireless FL, we decouple the op-
timization problem. In particular, we decompose the joint
optimization into two distinct sub-problems. Beforehand, we
1Notably, if Rt can only support the participation of a subset ˆSt ⊂St of
devices, we will denote the action as [ˆSt, Rt] to avoid the ambiguity while
the feasible subset is denoted as ˆSt.
show that under a feasible device selection subset St, there
exists independence between the global model performance
and the resource allocation.
Lemma 1. Given the feasible device selection action St,
the performance of global model Ξ is independent of the
valid resource allocation action Rt, where Rt enable the full
participation of devices within the subset St, i.e.,
P(Ξt, Rt|St) = P(Ξt|St)P(Rt|St).
(11)
Proof. By definition, all devices in St are involved in the
aggregation of FL, which implies no resource constraints in
(8a) are violated. In this case, the parameters of the global
model are completely determined by (4). Mathematically,
P(Ξt|Rt, St) = P(Ξt|St).
(12)
Therefore, we have
P(Ξt, Rt|St) = P(Ξt, Rt, St)
P(St)
= P(Rt, St)P(Ξt|St)
P(St)
= P(Ξt|St)P(Rt|St).
(13)
We have the lemma.
■
Corollary 1. The performance of the global model Ξ is solely
determined by the selected subset of participating devices St,
i.e.,
Ξt = Ξ(St).
(14)
Afterward, we introduce some fundamental assumptions for
the problem decoupling.
Assumption 1. For a general resource management problem
with a feasible device subset St, there always exists a function
g(·) that can find the optimal solution of the joint optimization
problem P1.
Assumption 2. For the joint optimization problem P1, given
fixed and unchanging device resources (i.e., ft,n = fn, pt,n =
pn and Bt,n = Bn), an optimal feasible subset S∗
t can always
be found.
Based on these assumptions, we have the following theorem.
Theorem 1. Under Assumption 1 and Assumption 2, the joint
optimization problem P1 can be equivalently decomposed into
an energy-efficiency oriented resource management subprob-
lem P2 and a device selection subproblem P3, where P2 and
P3 are respectively defined as:
P2 :
min
ft,n,pt,n,bt,n
XK
t=1 Et
s.t. St ⊂N,
(15)
constraints (8b), (8c), (8d), (8e),
and
P3 :
max
St
XK
t=1 r(Ξ(St), E∗
t (St))
s.t. Tt,n ≤TQoS,
ft,n, pt,n, Bt,n = g(St).
(16)


--- Page 6 ---
6
Proof. We first discuss the resource management issue in the
problem P1. Assumption 1 implies that for a given feasible de-
vice subset St, through g(·), the optimal resources can always
be allocated to the device to minimize energy consumption,
while meeting the required QoS and resource constraints. In
other words, given any device subset St, the minimum energy
consumption can be calculated as E∗
t (St) = Et(St, g(St)) as
in the problem P2.
Next, recalling the optimization objective r(Ξ(St), Et) in
the problem P1, it monotonically decreases with respect to Et
and increases with respect to Ξ(St), since Ξ is independent
of resource allocation by Corollary 1. Based on Assumption
2, the optimal feasible subset S∗
t can always be found for the
sub-problem P3. Afterward, for any given device subset S∗
t ,
it follows that
max
St,ft,n,pt,n,bt,n
XK
t=1 r(Ξ(St), Et)
=
max
ft,n,pt,n,bt,n
XK
t=1 r(Ξ(S∗
t ), Et)
=
XK
t=1 r(Ξ(S∗
t ), E∗
t (S∗
t )).
(17)
Therefore, P1 is equivalent to solving P2 and P3 separately,
and the optimal solution is given by (S∗
t , g(S∗
t )).
■
Remark: Theorem 1 provides the formal basis for decoupling
the joint optimization problem. This allows us to deal with
the two sub-problems separately and reduces computational
complexity by searching over a smaller-dimensional solution
space. Accordingly, we can introduce LLM to solve P3 conve-
niently, while resorting to a tool-aided solution of P2. Notably,
both sub-problems still involve a calibrated joint design for
decision making: For the subproblem P2, the function g(·)
satisfying Assumption 1 can be practically guaranteed by
optimization tools. For P2, the policy itself is required to make
optimal device selection decisions under the condition of the
resource allocation mechanism g(·).
IV. TOOL-AIDED EVOLUTIONARY LLM SCHEME
In this section, we first introduce the tool-aided LLM
towards generating an effective device selection and resource
allocation policy. Afterward, on top of convex optimization-
based resource management tools, we introduce a model-
based, tool-assisted training framework. Finally, we discuss
how to train the LLM by GRPO, thus improving its decision-
making capabilities.
A. Tool-Aided LLM for Efficient Policy Generation
The decision-making pipeline of the proposed tool-aided
LLM is shown in Fig. 2, which is motivated by the mathe-
matical findings in Section III and can be divided into two
parts: a generalizable LLM to provide device section results
for P3 and an optimization tool to yield scenario-specific
contextual resource allocation outcomes for P2. To generate a
resource-efficient policy, at each communication round t, the
current observation of the FL environment will be captured
and described directly in language. Correspondingly, the state
Device Selection
……
<EOS>
LLM Agent
Resource Tools
+
Observation    
<Token1>
Resource Allocation
<Auxiliary Action>
Joint Decision   
Wireless FL Environment
…
Human Language
•
System Prompt
•
Task Prompt 
•
Observation Prompt
Environment to Language Description
Fig. 2: An illustration of the inference process by the tool-
aided LLM.
space S in the MDP M is converted to text-based observation
O with ot ∈O, and
ot = [sc
t, sm
t , stext
t ],
(18)
where stext
t
represents the added natural language description.
This linguistic representation ot, which will be elaborated
later, serves as the input to the LLM. Consequently, it effi-
ciently adapts to diverse state spaces across various scenarios
without requiring additional tuning of NN architectures. Let
V represent the LLM’s entire token space. For a given obser-
vation ot, the LLM first computes a vector of logits over all
tokens in V, i.e., Lv(ot). To ensure that the generated action
corresponds to a valid device, we define a subset of this token
space I ⊂V, as the set of valid action tokens, where each
token represents a unique device index n ∈N. Therefore, the
logits over all valid tokens can be expressed as:
L′
v(ot) =
(
Lv(ot)
if v ∈I
−∞
if v /∈I .
(19)
A softmax function is then applied to this masked logit
vector to produce the final probability distribution over the
vocabulary, that is, πθ(·|ot) := πθ(v|ot) =
exp(L′
v(ot))
P
u∈V exp(L′u(ot)).
Then, the LLM generates a text response ˜at based on the
current language description according to its strategy πθ, i.e.,
˜at ∼πθ(·|ot),
(20)
which is interpreted as the decision action for device selection
St. Upon selecting the subset St of devices, the LLM invokes
an external resource tool to derive the resource allocation
action Rt for the selected devices. Finally, the joint decisions
at = [St, Rt] are applied to the wireless FL environment. Thus,
the next observation can be obtained by the environment, i.e.,
P(ot+1|ot, at).
(21)
Afterward, the feedback provided to the LLM-based policy
generator triggers the next round of efficiency optimization.


--- Page 7 ---
7
Observation Prompt: For last round, the select devices are [1, 6, 4, 8], and the time consumption is 15.0, the energy
consumption is 3.8031, the accuracy of global model is 0.4372. The total bandwidth of the system is 2e7. The reference
QoS time is 15.00s. The current communication is round 2. The states information of each device is shown in the
following table:
System Prompt: As a federated training agent, you are responsible for selecting the most suitable devices from the
device pool to optimize performance in the current round.
Task Prompt: Select 4 indexes from given 20 devices to participate in federated learning training based on the return
information in last round and the devices' states. Please give 4 indexes as a result directly without additional information.
‘5’ ‘2’ ‘4’ ‘18’
FL Agent Response
Question
Output Limitation: Please select 4 indexes from given 20 device indexes as a result. Give one result directly. Your
response consists only of indexes.
device 
index
max 
power
max frequency
channel 
gain
number of 
compute 
cycles
data 
size
local loss
local loss 
after 
trained
inner-product 
between local 
model and 
global
percentage of 
same sign between 
local model and 
global model
last selected 
round
selected 
times
0
0.829053
2049452125.9596 4.8439e-07
700000
2232
1874.7955
\
\
\
\
\
1
0.564018
1229342133.7070 1.1921e-06
700000
1527
2114.6651 6401.0288
0.3673
0.4415
1
1
……
18
0.588518
3479022664.3894 1.4571e-06
700000
2970
2844.2611
\
\
\
\
\
19
0.014806
1573110420.5726 4.9860e-07
700000
2668
6298.7290
\
\
\
\
\
Fig. 3: An example of input and output for device selection decision-making by LLM.
1) The LLM for P3: As shown in Fig. 3, the LLM takes
the linguistic description ot of the environmental observations
as input and generates the device selection decisions St,
corresponding to the indices of devices. Prominently, the
prompts, which include system prompt, task prompt, and
observation prompt, are specially tailored for the wireless FL
problem P1. Notably, system and task prompts configure the
LLM as an agent for FL. These prompts guide the LLM in
analyzing problems based on the provided task description,
thereby increasing the likelihood of generating task-related
token outputs. Besides, the observation prompt integrates all
the state parameters sc
t and sm
t
required for the current task,
furnishing the LLM with a comprehensive description of
the current environment for decision-making. Additionally, to
ensure compliance with operational constraints, the prompt
also specifies the desired output format of St as a token-based
response ˜at. On this basis, we fine-tune the LLM to improve
the performance.
2) The Tool for P2: For the tool-aided LLM, the tool shall
have the capability to compute optimal resource allocation
solutions to P2 under a unique scenario, so that LLM can
be compatible with the specific tool to make efficient joint
decisions in response to this specific scenario. Numerous
qualified algorithms [19], [20] can be leveraged. In this work,
due to its implementation simplicity, we adopt the alternative
direction algorithm ALTD proposed by [20]. Notably, for
P2, based on device selection St, ALTD optimizes allocated
resources Rt including the CPU frequency ft,n, transmission
power pt,n and bandwidth allocation Bt,n of networking
devices under the constraints of average bandwidth allocation
and QoS requirements, thus allowing the participating devices
to perform FL with the lowest energy consumption.
B. Model-Based Virtual Environment
Due to the significant communication overhead associated
with online interactions in wireless FL, this section proposes a
virtual environment based on an offline model-based approach
and tool-assisted computations. Benefiting from the following
lemma, the learning of the virtual environment can avoid the
extensive use of huge end-to-end expert data.
Lemma 2. The state transition of the wireless FL environment
can be decomposed into two parts:
P(st+1|st, at) = P(sc
t+1|sc
t, (St, Rt))P(sm
t+1|sm
t , St).
(22)
Proof. The state transition of the wireless RL environment can
be represented as:
P(st+1|st, at) = P(sm
t+1, sc
t+1|st, at)
= P(sm
t+1, sc
t+1|(sm
t , sc
t), (St, Rt)).
(23)
By definition, sc and sm are conditional independent. There-
fore, we can have
P(sm
t+1, sc
t+1|(sm
t , sc
t), (St, Rt))
=P(sm
t+1|(sm
t , sc
t), (St, Rt))P(sc
t+1|(sm
t , sc
t), (St, Rt)). (24)


--- Page 8 ---
8
Action   
Fine 
Tuning
Actor   
    
    
 
  
LLM Agent
Resource Tool
…
Simulated Model
…
Linear Layer
Linear Layer
Model-based
…
…
Virtual environment
     
 
  
       
System Part
Statistics Part
     
 
  
    
  
Self-Attention
Decoder ×N
Feed Forward
Add & Normalize
    
  
Offline 
interaction
Advantage
Replay Buffer
Episode
Episodes' records
             
           
…
Advantage
Episode
…
Collect
          
Sample
Intermediate 
value
…
   
   
…
     
…
          
    
    
…
        …
Output
        
…
   
            
     
        
Fig. 4: Illustration of the evolutionary virtual environment and GRPO-based training in T-ELLM.
Similarly, according to the independence of sm
t+1 and sc
t, as
well as sc
t+1 and sm
t , we have
P(st+1|st, at) = P(sm
t+1|sm
t , (St, Rt))P(sc
t+1|sc
t, (St, Rt)).
(25)
Recalling that by definition, sm is {|Dn|, Fn(ωn), Ξ(St−1)}-
related. Then, by Lemma 1, we have the conclusion.
■
Lemma 2 implies that the wireless FL environment can be
simulated by a system part and an FL statistics part, which is
illustrated in Fig. 4. For the system part, it only focuses on the
communication and computation status of devices according to
the given resources, i.e., P(sc
t+1|sc
t, (St, Rt)). Therefore, the
observation and consumption can be recorded and simulated
by the resource tool. Specifically, based on Rt and St, we can
leverage the tool to calculate the consumed time Tt and energy
Et, and update the state sc
t+1.
However,
for
characterizing
the
statistics
part
P(sm
t+1|sm
t , St), it remains challenging when dynamically
selected devices participate in training and aggregation.
To address this, we employ a model to approximate the
convergence behavior and performance of the global model
during FL training. In other words, simulating the statistics
part P(sm
t+1|sm
t , St) means to predict sm
t+1 including Ξ(St)
based on the current observation sm
t , device selection action
St and the historical accuracy Ξ(St−1). Generally, sm
t
can be classified as the state information pertinent to the
selected devices sm
t (s) and that of all devices sm
t (l). In
particular, sm
t (s) includes post-training local loss Fn(ωn),
the inner-product ⟨∇Fn(ωt
n), ∇FG(ωt
G)⟩, and the percentage
of the same sign e(ωt
n, ωt
G) shared between local and
global models (i.e., ωt
n and ωt
G)) for all selected devices
n ∈St. Meanwhile, sm
t (l) consists of the local dataset size
|Dn| (which remains constant during training) and locally
computed loss ln of other non-selected devices. We denote
the tuple ((sm
t , Ξ(St−1), St), (sm
t+1, Ξ(St))) as one sample
of the environment model, while several continuous samples
constitute a trajectory. The NN architecture of the simulated
model is shown in the right part of Fig. 4, while the details
of the training process are shown in Algorithm 1. During
training, we optimize the model with a Mean Squared Error
(MSE) loss.
Generally, the reward rt can be computed as in (7),
except when the resource tool finds no solution exists for
P2, the action St will be given a penalty, i.e., rt = 0.
Then, alongside the reward rt, the virtual environment can
respond to (sm
t , Ξ(St−1), St), and accurately yield the next
state (sc
t+1, sm
t+1).
C. GRPO-Based LLM Training
The aforementioned virtual environment lays the very
foundation for updating the LLM agent in an RL manner.
Specifically, we employ GRPO [38] to enhance the inference
capability of LLM. As a variant of Proximal Policy Opti-
mization (PPO) [41], GRPO also adopts twin policy models,
where an old policy πθold is used for sampling actions ˜at and
accumulating transitional records (ot, ˜at, rt, ot+1) in a replay
buffer. Here, rt is the reward in the t-th round calculated
as in Section IV-B. All samples are described in language,
a specific example as shown in Fig. 3. For a batch of K-
length episodes’ records, where each episode i is denoted as
{r1, · · · , rt, · · · , rK}i, we normalize these rewards with the
t-wise average and standard deviation across episodes within
the batch, i.e., { ˜rt}i = {rt}i−mean
std
. Then the t-th advantage in
i-th episode can be calculated as:
{At}i =
X
j≥t{ ˜rj}i.
(26)
Based on the πθold-induced records in the relay buffer, it
becomes ready to update the current policy model πθ. Notably,
due to the autoregressive characteristics of language tasks,


--- Page 9 ---
9
GRPO [38] can regressively update LLMs by taking a single
token as an action. While in a wireless FL environment, an
effective action is the entire response rather than a token.
Therefore, wireless FL is completely different from natural
language processing tasks. Let at,j represent the action taken
in round t and j-th token, thus ot,<j = [ot,<j−1, ˜at,j], and
ot,<0 = ot. In this case, the conditional probability of each
action ˜at can be calculated as:
πθ(˜at|ot) =
Y
j
πθ(˜at,j|ot,<j−1).
(27)
Next, we adopt GRPO to optimize the LLM by maximizing
JGRPO(θ) = E

min
 πθ(˜at|ot)
πθold(˜at|ot)At,
clip
 πθ(˜at|ot)
πθold(˜at|ot), 1 −ϵ, 1 + ϵ

At

,
(28)
where ϵ is a clipping-related hyperparameter for stabilizing
training.
Finally, the implementation process of the T-ELLM scheme
is illustrated in Algorithm 1.
V. THEORETICAL ACCURACY OF THE POLICY LEARNED
FROM THE VIRTUAL ENVIRONMENT
This section theoretically validates the proposed model-
based virtual environment by establishing a generalized sim-
ulation lemma. Specifically, we demonstrate that the overall
discrepancy between the simulated environment and the real
environment is provably bounded, i.e., the convergence proper-
ties of FL can be faithfully replicated under the model-based
environment. Therefore, we further show that the advantage
function, i.e., cumulative normalized reward under the learned
virtual environment in (26), remains within a bounded devia-
tion from the real environment, ensuring similar convergence
behavior of LLMs in both scenarios.
The MDP of the entire wireless FL system is expressed
as M = (O, A, P, R, γ). Specially, the state space is text-
based observation ot = [sc
t, sm
t , stext
t ]. And the added natural
language description stext
t
as the prompt for LLM is indepen-
dent of the wireless FL. Similarly, let
ˆ
M denote the MDP
representing the complete model-based virtual environment,
and
ˆP its corresponding transition function. To formally
characterize the difference between the real environment M
and the model-based approximation
ˆ
M, we first establish the
following assumptions and define the return-based advantage
function used in our analysis.
Assumption 3. Let M and
ˆ
M be two MDPs defined over
the same state and action spaces. Suppose that the normalized
reward functions satisfy the following uniform deviation bound
for all state-action pairs (ot, at), the reward functions satisfy
|˜r ˆ
M(ot, at) −˜rM(ot, at)| < ˜Rmax,
(29)
where ˜Rmax denotes a constant.
Definition 1. For any policy π and state ot, consistent with
(26), define the return-based advantage function as:
Aπ
P (o, a) := EP,π
XK−1
j=t ˜r (oj, aj)

,
(30)
Algorithm 1 GRPO-based T-ELLM scheme in the model-
based virtual environment.
Training for model-based virtual environment
Collect Tm trajectories [((sm
t , Ξ(St−1), St), (sm
t+1, Ξ(St)))]
as the training dataset.
1: Add two linear layers (i.e., linear1 and linear2) for
embedding to GPT-2 model and initialize all parameters
ϕ.
2: for each epoch do
3:
for each batch do
4:
parfor all samples in the same trajectory do
5:
int = linear1(sm
t , Ξ(St−1), St).
6:
outt = GPT(in1, · · · , int−1).
7:
(blueˆsm
t+1, ˆΞ(St)) = linear2(outt).
8:
losst = MSE((ˆsm
t+1, ˆΞ(St)), (sm
t+1, Ξ(St))).
9:
end parfor
10:
losstraj = mean(losst).
11:
end for
12:
lossbatch = mean(losstraj).
13:
lossbatch backward.
14:
update ϕ and two linear layers.
15: end for
Fine-tuning for tool-aided evolutionary LLM
Initialize all the parameters: policy model πθ, hyperparame-
ters ϵ, λ, the trained simulated model.
1: Initialize πθold = πθ.
2: for each iteration do
3:
πθold = πθ.
4:
Episode set Epis = ∅.
5:
for i = 1, · · · , batch do
6:
Epii = ∅
7:
for communication round t = 1, · · · , K do
8:
Obtain the observation ot.
9:
Sample at ∼πθold(·|ot).
10:
Execute at, receive rt.
11:
Get ot+1 via resource tool and simulated
model.
12:
recordt = (ot, ˜at, rt, ot+1)
13:
Store Epii ←Epii ∪recordt.
14:
end for
15:
Epis ←Epis S Epii.
16:
end for
17:
Calculate all advantages At based on (26).
18:
for each update step do
19:
Update πθ by maximing (28) with Epis.
20:
end for
21: end for
where (ot, at) = (o, a), ˜r(o, a) denotes a normalized reward
function satisfying |˜r(o, a)| ≤˜Rmax.
Our approach separately characterizes the system and statis-
tics parts instead of using a real wireless FL environment. The
system part uses resource tools to simulate communication
and computing costs, where fixed resource (e.g., bandwidth
and transmission power) settings often lead to deterministic


--- Page 10 ---
10
state transitions P(sc
t+1|sc
t, (St, Rt)), due to the explicit math-
ematical expressions in Section II-A2. On the other hand, the
MDP corresponding to the statistical component is denoted as
Md = (Sm, Am, P m, Rm, γ), where, as described in Section
IV, the state is defined as sm
t
= {sm
t (s), sm
t (l)} ∈Sm. The
decision action corresponds to selecting a subset St of devices.
The transition probability is given by P m = P(sm
t+1|sm
t , St),
and the reward is defined as the accuracy of the global model,
denoted by Ξ(St). To approximate this statistical MDP, we
employ a trained model
ˆ
Md, whose transition dynamics are
captured by the learned function ˆP m.
Assumption 4 (Bounded Transition Discrepancy in TV Dis-
tance). Let P m and ˆP m denote the transition functions of
Md and
ˆ
Md, respectively. Suppose the transition model
discrepancy is bounded in Total Variation (TV) as:
sup
sm
t ,St
DTV

ˆP(sm
t+1|sm
t , St), P(sm
t+1|sm
t , St)

≤ε,
(31)
where DTV(p, q) denotes the TV [42] distance between prob-
ability distributions p and q.
Lemma 3. Under Assumption 4, let P and ˆP denote the
transition functions of M and ˆ
M, respectively. The transition
model discrepancy is bounded in total variation as
sup
ot,at
 ˆP(ot+1|ot, at) −P(ot+1|ot, at)

1 ≤2ε.
(32)
Proof. By definition, the transition probability function P of
the full environment model can be decomposed as:
P(ot+1|ot, at) = P(sm
t+1, sc
t+1, stext
t+1, |(sm
t , sc
t, stext
t ), (St, Rt)),
where stext
t
denotes the language inputs to the LLM, which
is independent of other variables and can thus be treated as a
constant. Accordingly, consistent with Lemma 2, the transition
function P can be simplified without loss of theoretical
generality as follows:
P(ot+1|ot, at) = P(sm
t+1, sc
t+1|(sm
t , sc
t)(St, Rt))
= P(sc
t+1|sc
t, (St, Rt))P(sm
t+1|sm
t , St), (33)
where P(sc
t+1|sc
t, (St, Rt)) is the transition function of system
part, which is determined by the resource tool. Similarly,
the transition function ˆP in the virtual environment can be
obtained as:
ˆP(ot+1|ot, at) = P(sc
t+1|sc
t, (St, Rt)) ˆP(sm
t+1|sm
t , St).
Therefore, under Assumption 4, we have
 ˆP(ot+1|ot, at) −P(ot+1|ot, at)

1
=
X
sc
t+1,sm
t+1
P
 sc
t+1 | sc
t, (St, Rt)

· ˆP
 sm
t+1 | sm
t , St

−P
 sc
t+1 | sc
t, (St, Rt)

· P
 sm
t+1 | sm
t , St
 
=
X
sc
t+1
P
 sc
t+1 | sc
t, (St, Rt)

·
X
sm
t+1
 ˆP
 sm
t+1 | sm
t , St

−P
 sm
t+1 | sm
t , St
 
(34)
=
X
sc
t+1
P
 sc
t+1 | ·

·
 ˆP (· | sm
t , St) −P (· | sm
t , St)

1
(a)
=
 ˆP (· | sm
t , St) −P (· | sm
t , St)

1
(b)
=2DTV

ˆP(sm
t+1|sm
t , St), P(sm
t+1|sm
t , St)
 (c)
≤2ε.
Specifically, (a) results from isolating the system transition
probability as a common factor. (b) is derived by converting
the ℓ1-norm into TV distance via its standard definition
DTV(p, q) = 1
2∥p−q∥1. (c) applies Assumption 4 that the TV
between the model and true statistical transitions is at most ε.
Hence, we have the corollary.
■
The following theorem characterizes the bounded deviation
of the return-based advantage under model-based dynamics.
Theorem 2 (Generalized Simulation Lemma for Return-Based
Advantage Functions). Under Assumption 3 and Assumption
4, for any fixed policy π, the difference in cumulative normal-
ized rewards over a finite horizon K between the model-based
MDP
ˆ
M and the true environment M is bounded by
Aπ
ˆ
P (o, a) −Aπ
P (o, a)
 ≤(K2 −K) ˜Rmax · ε.
(35)
Proof. By definition,
Aπ
ˆ
P (o, a) −Aπ
P (o, a)

(36)
=
E ˆ
P ,π
XK−1
j=t ˜r (oj, aj)

−EP,π
XK−1
j=t ˜r (oj, aj)

≤
XK−1
t=0
E ˆ
P ,π [˜rt] −EP,π [˜rt]
 .
Since ˜r(oj, aj) is bounded by ˜Rmax according to Assump-
tion 3, and by TV Inequality, shown in Prop. 4.2 of [43], we
have E ˆ
P ,π [˜rt] −EP,π [˜rt]
 ≤˜Rmax ·
 ˆP π
t −P π
t

1 ,
(37)
where P π
t and ˆP π
t denote the distributions over t-step state-
action trajectories induced by policy π under transition models
P and ˆP, respectively. The total variation between K-step
distributions grows at most linearly with the horizon length,
as the distributional shift at step t reflects the accumulation of
per-step transition errors over the entire trajectory up to time
t [44]. In other words, by Lemma 3,
 ˆP π
t −P π
t

1 ≤2t · ε.
(38)
By merging (37) and (38) into (36), we have the theorem
after simple mathematical manipulations.
■
Remark: Theorem 2 result shows that the cumulative advan-
tage error is controllable, growing polynomially with horizon
K and linearly with the reward bound and transition error
ε. Therefore, there are several practical implications. First, the
framework is most effective for scenarios with a finite horizon,
as the cumulative error may become significant in very long-
running tasks. Second, the performance bound is directly tied
to the fidelity of the virtual environment. Finally, the reliance
on ˜Rmax underscores the importance of a well-designed and
stable reward function, as volatility or poor scaling can loosen
the bound. Fortunately, with normalized rewards (Rmax ≤1)
and a Transformer-based transition model, ε can typically be
reduced to a sufficiently accurate level. Together with proper
control of K, the deviation between model-based and true
advantages remains tightly bound.


--- Page 11 ---
11
TABLE III: Default values of simulation parameters.
Parameters
Value
Total number of devices N
20
Number of rounds K (Episodic length)
100
Number of local iteration I
5
The number of CPU cycles to process one
sample C
7 × 105
The effective switch capacitance constant ζ
1 × 10−28
The maximum CPU frequency of device n
fn,max
[0.5, 4.0] GHZ
The maximum transmission power of device n
pn,max
[0.001, 1] W
The total bandwidth of the system B
2 MHz
Channel coefficient of device n during the t-h
round Gt,n
[10−7, 10−6] dB
The noise power spectral density N0
−174 dbm/MHz
Weight size of the local model in bits
53.21 Mbit
Time-related QoS TQoS
15 s
Weight factor σ
0.8
VI. SIMULATION RESULTS
A. Experimental Setup
We consider an image classification task in a wireless FL
scenario, as discussed in Section II, where the total number of
devices is N = 20. In the experiments, we use the well-known
MNIST dataset for FL training of a Convolutional Neural
Network (CNN) model with the cross-entropy loss function.
To simulate data heterogeneity across devices, we sample
label ratios and dataset sizes from a Dirichlet distribution
parameterized by α, which controls the degree of non-IIDness.
Notably, a smaller α leads to more non-IID data, while a larger
α results in more homogeneous data. We set α = 0.2 as the
default to simulate the non-IID data distribution. Other default
configurations of simulation parameters are specified in the
Table III.
For the training of T-ELLM, we employ the ALTD [20]
as a complementary tool for dynamic management of CPU
frequency and transmission power, as well as equal, fixed
bandwidth allocation. In addition, the environmental model,
which is adapted from the decoder-only part of the GPT-
2-small architecture [26], [45] with two extra linear layers,
is trained first. Subsequently, a LLaMA-3-1B-based [28] T-
ELLM is used for GRPO [38]-based policy learning and gen-
eration. Besides, to evaluate the performance of the proposed
T-ELLM, we compare it with the following baselines.
• FedAvg Tool [2]: A specific proportion of clients are
randomly selected to participate in each round of FL
training. Since it does not have the function of resource
allocation itself, we use the ALTD tool to make resource
allocation decisions.
• ϵ-Greedy Tool [7]: A variant of the Oort algorithm.
Specifically, energy considerations are added, and tools
are leveraged to obtain time and energy reference. More-
over, we assume all the device time and energy con-
sumption can be known in advance, so that the ϵ-greedy
algorithm can be executed [7].
• FL-DLT3 [25]: FL-DLT3 enables a twin-delayed deep
deterministic policy gradient (TD3) framework to op-
timize accuracy and energy balance in a continuous
domain. Compared to transmission power allocation for
FL efficiency optimization [25], we further expand it
to manage CPU frequency with equal, fixed bandwidth
allocation.
• SAC (Soft Actor-Critic) [10], [46]: SAC is an off-policy
actor-critic deep RL algorithm based on the maximum
entropy framework [46]. [10] applies SAC to wireless
FL scenarios for resource allocation. We implement SAC
on top of a transformer decoder-only neural network.
• T-ELLM PPO: It uses PPO [41] rather than GRPO [38]
to fine-tune T-ELLM. Compared to GRPO, PPO employs
an additional critic network to optimize the NN.
B. Performance Comparison
We first show the overall performance comparison, in terms
of CWEPM in (8a), learning accuracy, and energy consump-
tion per round, and Fig. 5 presents the corresponding results.
Notably, as shown in Fig. 6, significantly heterogeneous label
distributions and dataset sizes exist for evaluation. As shown
in Fig. 5(a), the proposed T-ELLM GRPO and T-ELLM PPO
achieve the highest CWPEM values, suggesting that these
methods require less energy to attain target performance levels.
Fig. 5(b) illustrates the convergence of wireless FL under
different algorithms. The proposed T-ELLM GRPO and T-
ELLM PPO exhibit a remarkable convergence rate towards
T-ELLM GRPO
T-ELLM PPO
Fedavg Tool
0-Greedy Tool
FL-DLT3 SAC
0
10
20
30
40
50
60
70
80
90
100
CWPEM
(a) Energy efficiency
0
20
40
60
80
100
Round
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Accuracy
T-ELLM GRPO
T-ELLM PPO
Fedavg Tool
0-greedy Tool
FL-DLT3
SAC
(b) Convergence of Wireless FL
0
20
40
60
80
100
Round
1
2
3
4
5
6
7
8
9
Energy (J)
T-ELLM GRPO
T-ELLM PPO
Fedavg Tool
0-greedy Tool
FL-DLT3
SAC
(c) Energy consumption per round
Fig. 5: Performance comparison of different algorithms for wireless FL.


--- Page 12 ---
12








	









	

 








 
 
 
 
 
 
 
 
 
 	
 
Fig. 6: Label and dataset size distributions for non-IID data
on different devices.
0
10
20
30
40
50
60
70
80
90
100
Iteration
10-5
10-4
10-3
10-2
Loss
Accuracy %
State sm
0
20
40
60
80
100
120
140
160
180
200
Iteration
0
10
20
30
40
50
60
70
Reward
T-ELLM GRPO
(b) Convergence of the LLM-based GRPO
(a) Convergence of the model-based environment
Fig. 7: Convergence of the model-based virtual environment
and GRPO-based T-ELLM.
the desired accuracy. Additionally, Fig. 5(c) presents the
energy consumption per round of wireless FL, where the solid
curves therein represent the mean testing accuracy across 10
experimental trials and the shaded part represents the 95%
confidence interval calculated from these experiments (the
other figures are likewise). The proposed T-ELLM GRPO and
T-ELLM PPO consume less energy per round, demonstrating
their efficiency in resource utilization during the FL process.
Furthermore, the narrower confidence interval observed for T-
ELLM GRPO and T-ELLM PPO, relative to the FL-DLT3 and
Fedavg Tool baseline, indicates stable and consistent decision-
making, while maintaining a level of stability comparable to
the heuristic methods.
To implement the proposed T-ELLM, 600 trajectories of
((sm
t , Ξ(St−1), St), (sm
t+1, Ξ(St))) are collected to train the
GPT model as the statistics part of the virtual environment.
The convergence of the model-based environment is shown
in Fig. 7(a). It can be observed from Fig. 7(a) that the MSE
loss for both state sm
t and accuracy Ξ(St) decreases steadily.
While the accuracy metric consists of a single value, thus
0
2
4
6
8
10
12
14
16
18
20
Device ID
-4
-3
-2
-1
0
1
2
3
4
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
Scenario 1
Scenario 2
Fig. 8: Comparison of changing environment with respect to
fn,max and pn,max.
being relatively easier to predict, the state encompasses more
complex information. Nevertheless, the state also converges
to a satisfactory result. Based on the trained environmental
model, the evolution of T-ELLM can be carried out offline.
Fig. 7(b) presents the reward progression during GRPO fine-
tuning. It can be seen that under the optimization of GRPO,
the reward value gradually stabilizes at a higher level, which
indicates the satisfactory convergence of T-ELLM.
C. Generalizable Capability to Environmental Changes
1) Adaptation to Resource Changes: To verify the general-
ization of the proposed T-ELLM to the environment, we first
change the computing and communication capabilities of each
device in the environment. Fig. 8 highlights the differences
in terms of the maximum CPU frequency and transmission
power of devices. Compared to the environment (i.e., Scenario
1) for evaluation in Section VI-B, the modified environment
is termed Scenario 2. Notably, all algorithms are directly
transferred from the training outcome in Scenario 1 and have
no prior interaction with Scenario 2, ensuring an unbiased
assessment of adaptability. The corresponding performance in
Scenario 2 is shown in Fig. 9. As demonstrated in Fig. 9(a),
the CWPEM of T-ELLM GRPO and T-ELLM PPO outper-
forms that of other baseline algorithms, indicating superior
performance with lower energy consumption. Furthermore,
Fig. 9(b) confirms that the learning accuracy of T-ELLM
remains robust, without sacrificing the learning efficiency.
Furthermore, Fig. 9(c) shows that the energy consumption
per round yielded by T-ELLM is also lower than that of
other algorithms, demonstrating its ability to maintain high
accuracy while optimizing energy efficiency. These results
highlight T-ELLM’s adaptability to varying environmental
conditions. Additionally, the FedAvg tool and ϵ-Greedy tool
are not learning-based algorithms, naturally, their performance
is less affected by environmental changes. In comparison, as
shown in Fig. 9(c), due to its training only in the default
Scenario 1, the energy consumption of FL-DLT3 and SAC is
significantly higher than that of other algorithms. This further
underscores the advantages of the proposed T-ELLM in terms
of environmental adaptability and energy efficiency.


--- Page 13 ---
13
T-ELLM GRPO
T-ELLM PPO
Fedavg Tool
0-Greedy Tool
FL-DLT3 SAC
0
10
20
30
40
50
60
70
80
90
100
CWPEM
(a) Energy efficiency
0
20
40
60
80
100
Round
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Accuracy
T-ELLM GRPO
T-ELLM PPO
Fedavg Tool
0-greedy Tool
FL-DLT3
SAC
(b) Convergence of Wireless FL
0
20
40
60
80
100
Round
0
2
4
6
8
10
12
Energy (J)
T-ELLM GRPO
T-ELLM PPO
Fedavg Tool
0-greedy Tool
FL-DLT3
SAC
(c) Energy consumption per round
Fig. 9: Performance comparison of different algorithms for wireless FL with Scenario 2.
T-ELLM GRPO
T-ELLM PPO
Fedavg Tool
0-Greedy Tool
FL-DLT3 SAC
0
20
40
60
80
100
120
CWPEM
(a) Energy efficiency
0
20
40
60
80
100
Round
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Accuracy
T-ELLM GRPO
T-ELLM PPO
Fedavg Tool
0-Greedy Tool
FL-DLT3
SAC
(b) Convergence of Wireless FL
1
10
20
30
40
50
60
70
80
90
100
Round
0
1
2
3
4
5
6
7
8
9
Energy (J)
T-ELLM GRPO
T-ELLM PPO
Fedavg Tool
0-Greedy Tool
FL-DLT3
SAC
(c) Energy consumption per round
Fig. 10: Performance comparison of different algorithms for wireless FL with TQoS = 20 s.
Qos 16s
Qos 17
Qos 18s
Qos 19s
0
50
100
150
Energy (J)
0.9
0.92
0.94
0.96
0.98
Accuracy
,= 0.17
,= 0.18
,= 0.19
0
20
40
60
80
100
120
140
Energy (J)
0.9
0.92
0.94
0.96
0.98
1
Accuracy
Energy
Accuracy
Fig. 11: Performance of the proposed T-ELLM GRPO with
different QoS requirements and data heterogeneity.
2) Adaptation to Changed Task Requirements: In this part,
we further demonstrate the ability of T-ELLM to cope with
different QoS requirements. Fig. 10 provides the results after
changing TQoS in the default Scenario 1 from 15 seconds to 20
seconds. The results show that the proposed T-ELLM GRPO
and T-ELLM PPO achieve the highest CWPEM values in Fig.
10(a), indicating superior energy efficiency in wireless FL. Fig.
10(b) illustrates the convergence of wireless FL under different
algorithms. The proposed T-ELLM GRPO and T-ELLM PPO
exhibit a significant convergence rate, achieving the desired
accuracy. Fig. 10(c) shows that the proposed T-ELLM GRPO
and T-ELLM PPO consume less energy per round, while it
can be seen that the FedAvg Tool also maintains a low energy
consumption per round. The reason lies in that the resource
tool is used therein for resource allocation, contributing to
saving overall energy consumption required for the anticipated
QoS. On the other hand, although the ϵ-Greedy tool algorithm
also gets help from the tool, the extensive reliance on the
artificial setting of greedy strategy parameters undermines the
potential benefit. Additionally, as shown in Fig. 10(c), FL-
DLT3 and SAC demonstrate notably higher energy consump-
tion, as they depend solely on their NNs for resource allocation
and lack adaptability to varying QoS demands. In comparison,
the proposed T-ELLM framework dynamically adjusts to QoS
requirements, enabling more efficient decision-making.
We also evaluate the performance of the proposed T-ELLM
with different QoS requirements and heterogeneity. The per-
formance of the proposed T-ELLM is shown in Fig. 11. It can
be seen in Fig. 11(a) that the proposed T-ELLM maintains


--- Page 14 ---
14
GPT-Linear
GPT-Small
GPT-Medium
GPT-Large
0
20
40
60
80
100
CWPEM
(a) Energy efficiency
0
20
40
60
80
100
Round
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Accuracy
GPT-Linear
GPT2-Small
GPT2-Medium
GPT2-Large
(b) Convergence of Wireless FL
0
20
40
60
80
100
Round
0
0.5
1
1.5
2
2.5
Energy (J)
GPT-Linear
GPT2-Small
GPT2-Medium
GPT2-Large
(c) Energy consumption per round
Fig. 12: Performance comparison of large models with different scales for wireless FL.
comparable accuracy with different QoS requirements. Fur-
thermore, the energy consumption decreases with the increase
of QoS requirements. Fig. 11(b) shows the performance of the
proposed T-ELLM with different levels of data heterogeneity,
and similar observations can be attained. These results show
that the proposed T-ELLM can adapt to environmental changes
by yielding appealing accuracy.
D. Performance of TELLM in Different Model Sizes
In order to validate the effect of LLM size and language
training approach on the T-ELLM, we evaluate the perfor-
mance of three versions of GPT-2, including GPT-2 Small,
GPT-2 Medium, and GPT-2 Large, corresponding to parameter
scales of 0.1B, 0.3B, and 0.7B, respectively. Additionally, we
compare the performance of GPT-2 models against a more
lightweight, task-specific baseline we call GPT-Linear, which
uses a Transformer decoder-only architecture (like GPT-2).
The result is shown in Fig. 12. It can be seen that the
performance for the GPT-2 series is all very similar, with no
significant difference. Compared with the GPT-2 series, the
CWPEM and convergence results of GPT-Linear are slightly
lower, while the confidence interval of GPT-Linear is narrower
than that of the GPT-2 series. These results validate the ad-
vantage of adopting a linguistic model, and a computationally
efficient GPT-2 Small model already yields satisfactory results.
E. The Adaptability of TELLM to Different FL Algorithms
We investigate the adaptability and robustness of our T-
ELLM framework for FL with gradient quantization [10] and
Differential Privacy (DP) [4].
1) Adaptation to Gradient Quantization: We present the
performance of T-ELLM with varying levels of gradient
quantization in Fig. 13. As shown in Fig. 13(a), consistent
with our intuition, the energy efficiency of T-ELLM decreases
as the number of quantization bits decreases, which is ex-
pected since lower bit quantization introduces more noise,
potentially degrading model accuracy. However, even with
2-bit quantization, T-ELLM maintains a reasonable balance
between energy consumption and accuracy. Fig. 13(b) shows
that T-ELLM converges to a satisfactory accuracy level across
all quantization levels, although higher bit quantization leads
to faster convergence and higher final accuracy. Fig. 13(c)
indicates that energy consumption per round increases with
the number of quantization bits, as higher precision requires
more data transmission. Overall, T-ELLM demonstrates robust
performance even under aggressive gradient quantization.
2) Adaptation to Differential Privacy: We implement (ϵ, δ)-
DP in the wireless FL process, where δ accounts for the
probability that plain ϵ-DP is broken, and ϵ controls the level
of privacy, where a smaller ϵ signifies stronger privacy (and
more noise). We evaluate the performance with various privacy
budgets. Fig.14(a) shows that stronger privacy (a lower ϵ) leads
to lower final model accuracy, as the increased noise makes
learning more difficult. Fig. 14(b) demonstrates that while all
settings eventually reach a stable accuracy, stronger privacy
constraints slow down the learning process. Fig. 14(c) illus-
trates that energy consumption is not significantly affected by
the DP level. Overall, T-ELLM maintains robust performance
when differential privacy is applied.
F. Scalability for Large-Scale Scenario
To evaluate the scalability of T-ELLM, we simulate a larger
weirless FL environment consisting of 100 devices, where
we distribute the CIFAR-10 dataset across these devices and
configure the task to select 10 participating devices in each
training round. Meanwhile, we compare the performance of
three algorithms:
• T-ELLM Retrain: The T-ELLM agent is retrained using
the virtual environment for the 100-device scenario.
• T-ELLM Original: The original T-ELLM agent, trained
for the 20-device scenario, was directly applied without
any retraining to test its zero-shot transferability.
• SAC-Transformer: A baseline using a Transformer archi-
tecture with a linear head, trained from scratch on the
100-device scenario using the SAC algorithm.
The results of these experiments are presented in Fig. 15.
As shown in Fig. 15(a), the T-ELLM Retrain model achieves
the highest energy efficiency and CWPEM value. Meanwhile,
the T-ELLM Original model also performs well, demonstrating
strong transferability. The SAC-Transformer baseline is com-
paratively less efficient. Fig.15(b) shows that both T-ELLM
Retrain and T-ELLM Original achieve higher convergence
accuracy than SAC-Transformer. In other words, the T-ELLM


--- Page 15 ---
15
Quantified bit = 2
Quantified bit = 4
Quantified bit = 8
Quantified bit = 16
0
10
20
30
40
50
60
70
80
90
100
Energy (J)
0.9
0.91
0.92
0.93
0.94
0.95
0.96
0.97
0.98
0.99
1
Accuracy
Energy
Accuracy
(a) Energy and accuracy
0
20
40
60
80
100
Round
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Accuracy
Quantified bit = 2
Quantified bit = 4
Quantified bit = 8
Quantified bit = 16
(b) Convergence of Wireless FL
0
20
40
60
80
100
Round
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
Energy (J)
Quantified bit = 2
Quantified bit = 4
Quantified bit = 8
Quantified bit = 16
(c) Energy consumption per round
Fig. 13: Performance of the T-ELLM algorithm for wireless FL using gradient quantization.
0 = 30
0 = 40
0 = 50
0 = 60
0
20
40
60
80
100
120
Energy (J)
0.9
0.92
0.94
0.96
0.98
1
Accuracy
Energy
Accuracy
(a) Energy and accuracy
0
20
40
60
80
100
Round
0.2
0.4
0.6
0.8
1
Accuracy
0 = 30
0 = 40
0 = 50
0 = 60
(b) Convergence of Wireless FL
0
20
40
60
80
100
Round
1
1.05
1.1
1.15
1.2
1.25
1.3
Energy (J)
0 = 30
0 = 40
0 = 50
0 = 60
(c) Energy consumption per round
Fig. 14: Performance of the T-ELLM algorithm for wireless FL using differential privacy.
T-ELLM Retrain
T-ELLM Original
SAC-Trasformer
0
50
100
150
CWPEM
(a) Energy efficiency
0
20
40
60
80
100
Round
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Accuracy
T-ELLM Retrain
T-ELLM Original
SAC-Trasformer
(b) Convergence of Wireless FL
4.5
5
5.5
Energy (J)
0
20
40
60
80
100
Round
0
0.5
1
T-ELLM Retrain
T-ELLM Original
SAC-Trasformer
/
/
/
/
(c) Energy consumption per round
Fig. 15: Performance comparison of learning-based algorithms migrated to the scenario with 100 devices.
Original model can directly make effective device selections
in a much larger environment. Finally, Fig.15(c) illustrates
that the T-ELLM Retrain is the most energy-efficient. On
the contrary, the SAC-Transformer consumes drastically more
energy. These results indicate that the T-ELLM framework
provides a robust and scalable solution for large-scale FL
resource management.
VII. CONCLUSION AND FUTURE WORKS
We have presented T-ELLM, a novel tool-aided evolutionary
LLM framework for efficient device selection and resource al-
location in wireless FL. Notably, T-ELLM has been developed
on top of a mathematics-driven decoupling of LLM-based
device selection and tool-based resource allocation. Mean-
while, the combination of the linguistic reasoning capabilities
in LLMs and mathematical optimization tools contributes to
boosting the generalization capability of decision-making in
environmental changes. In addition, T-ELLM takes advantage
of a model-based virtual environment to support GRPO-based
fine-tuning at minimal communication cost during interactions.
Our theoretical analysis has proved the bounded discrepancy
between virtual and real environments, which ensures the
advantage function learned in the virtual environment main-
tains a provably small deviation from real-world conditions.
Extensive experimental results have demonstrated that T-
ELLM can further improve the energy efficiency and exhibit
robust adaptability to environmental changes. Future work


--- Page 16 ---
16
will focus on extending the T-ELLM framework to more
complicated wireless resource management tasks, such as task
offloading [47], [48]. Key priorities also include investigat-
ing its scalability, enhancing its safety against attacks, and
validating its performance on real-world hardware. We will
also continue to refine its algorithmic complexity to ensure
practical deployment.
REFERENCES
[1] Q. Cui, et al., “Overview of AI and communication for 6G network:
fundamentals, challenges, and future research opportunities,” Sci. China
Inf. Sci., vol. 68, no. 7, p. 171301:1–171301:61, Jan. 2025.
[2] B. McMahan, et al., “Communication-efficient learning of deep net-
works from decentralized data,” in Proc. Int. Conf. Artif. Intell. Stat.
(AISTATS), Fort Lauderdale, Florida, USA, Feb. 2016, pp. 1273–1282.
[3] H. Guan, et al., “Federated learning for medical image analysis: A
survey,” Pattern Recognit., vol. 151, no. 3, p. 110424, Jul. 2024.
[4] M. Ye, et al., “Heterogeneous federated learning: State-of-the-art and
research challenges,” ACM Comput. Surv., vol. 56, no. 3, pp. 1–44, Oct.
2023.
[5] H. T. Nguyen, et al., “Fast-convergent federated learning,” IEEE J. Sel.
Areas Commun., vol. 39, no. 1, pp. 201–218, Jan. 2021.
[6] X. Yi, et al., “RHFedMTL: Resource-aware hierarchical federated multi-
task learning,” IEEE Internet Things J., vol. 11, no. 14, pp. 25 227–
25 238, Jul. 2024.
[7] F. Lai, et al., “Oort: Efficient federated learning via guided participant
selection,” in Proc. USENIX Symp. Oper. Syst. Des. Implement. (OSDI),
Virtual Edition, Jul. 2021.
[8] T. Nishio, et al., “Client selection for federated learning with hetero-
geneous resources in mobile edge,” in Proc. IEEE Int. Conf. Commun.
(ICC), Shanghai, China, Apr. 2018.
[9] S. Q. Zhang, et al., “A multi-agent reinforcement learning approach for
efficient client selection in federated learning,” in Proc. AAAI Conf. Artif.
Intell., Vancouver, Canada, Feb. 2022.
[10] X. Hou, et al., “Efficient federated learning for metaverse via dynamic
user selection, gradient quantization and resource allocation,” IEEE J.
Sel. Areas Commun., vol. 42, no. 4, pp. 850–866, Apr. 2024.
[11] Y. G. Kim, et al., “AutoFL: Enabling heterogeneity-aware energy
efficient federated learning,” in Proc. Annu. IEEE/ACM Int. Symp.
Microarchit. (MICRO-54), Virtual Edition, Oct. 2021.
[12] H. Wang, et al., “Optimizing federated learning on non-IID data with
reinforcement learning,” in Proc. IEEE Int. Conf. Comput. Commun.
(INFOCOM), Toronto, Canada, Jul. 2020.
[13] Y. Zhan, et al., “Experience-driven computational resource allocation of
federated learning by deep reinforcement learning,” in IEEE Int. Parallel
Distrib. Process. Symp. (IPDPS), New Orleans, LA, USA, May 2020.
[14] R. Figueiredo Prudencio, et al., “A survey on offline reinforcement
learning: Taxonomy, review, and open problems,” IEEE Transactions
on Neural Networks and Learning Systems, vol. 35, no. 8, pp. 10 237–
10 257, Mar. 2024.
[15] A. Hussein, et al., “Imitation learning: A survey of learning methods,”
ACM Comput. Surv., vol. 50, no. 2, Apr. 2017.
[16] K. Zhang, et al., “Multi-agent reinforcement learning: A selective
overview of theories and algorithms,” in Handb. Reinforcem. Learn.
Control.
Springer, Jun. 2021, vol. 295, pp. 321–384.
[17] J. Li, et al., “A comprehensive survey on client selection strategies in
federated learning,” Comput. Netw., vol. 251, p. 110663, Sep. 2024.
[18] M. Chen, et al., “A joint learning and communications framework
for federated learning over wireless networks,” IEEE Trans. Wirel.
Commun., vol. 20, no. 1, pp. 269–283, Jan. 2021.
[19] Z. Yang, et al., “Energy efficient federated learning over wireless
communication networks,” IEEE Trans. Wirel. Commun., vol. 20, no. 3,
pp. 1935–1949, Mar. 2021.
[20] J. Yao, et al., “Enhancing federated learning in fog-aided IoT by CPU
frequency and wireless power control,” IEEE Internet Things J., vol. 8,
no. 5, pp. 3438–3445, Mar. 2021.
[21] Y. Jee Cho, et al., “Towards understanding biased client selection in
federated learning,” in Proc. Intl. Conf. on Artif. Intell. Statis. (AISTATS),
Valencia, Spain, Mar. 2022.
[22] W. Shi, et al., “Joint device scheduling and resource allocation for
latency constrained wireless federated learning,” IEEE Trans. Wirel.
Commun., vol. 20, no. 1, pp. 453–467, Jan. 2021.
[23] T. Zhang, et al., “Joint device scheduling and bandwidth allocation
for federated learning over wireless networks,” IEEE Trans. Wirel.
Commun., vol. 24, no. 1, pp. 3–18, Jul. 2025.
[24] W. Mao, et al., “Joint client selection and bandwidth allocation of
wireless federated learning by deep reinforcement learning,” IEEE Trans.
Serv. Comput., vol. 17, no. 1, pp. 336–348, Jan. 2024.
[25] J. Zheng, et al., “Exploring deep-reinforcement-learning-assisted fed-
erated learning for online resource allocation in privacy-preserving
edgeiot,” IEEE Internet Things J., vol. 9, no. 21, pp. 21 099–21 110,
Nov. 2022.
[26] A.
Radford,
et
al.,
“Improving
language
understanding
by
generative pre-training,” OpenAI Blog, Jun. 2018. [Online]. Avail-
able:
https://cdn.openai.com/research-covers/language-unsupervised/
language understanding paper.pdf
[27] A.
Radford,
et
al.,
“Language
models
are
unsupervised
multitask
learners,”
OpenAI
Blog,
Aug.
2019.
[Online].
Avail-
able: https://cdn.openai.com/better-language-models/language models
are unsupervised multitask learners.pdf
[28] A. Grattafiori, et al., “The Llama 3 herd of models,” ArXiv, vol.
abs/2407.21783, Nov. 2024.
[29] DeepSeek-AI, et al., “DeepSeek-V3 technical report,” ArXiv, vol.
abs/2412.19437, Feb. 2025.
[30] S. Reed, et al., “A generalist agent,” Transact. Mach. Learn. Res., pp.
1–42, Nov. 2022.
[31] Z. Wang, et al., “Federated fine-tuning for pre-trained foundation models
over wireless networks,” IEEE Trans. Wirel. Commun., vol. 24, no. 4,
Apr. 2025.
[32] J.-Y. Zheng, et al., “Safely learning with private data: A federated
learning framework for large language model,” in Proc. Conf. Empir.
Methods Nat. Lang. Process. (EMNLP), Miami, Florida, USA, Nov.
2024.
[33] C. E. Mower, et al., “ROS-LLM: A ROS framework for embodied AI
with task feedback and structured reasoning,” Jul. 2024.
[34] M. T. R. Laskar, et al., “A systematic survey and critical review on
evaluating large language models: Challenges, limitations, and rec-
ommendations,” in Proc. Conf. Empir. Methods Nat. Lang. Process.
(EMNLP), Miami, Florida, USA, Nov. 2024.
[35] J. Ahn, et al., “Large language models for mathematical reasoning:
Progresses and challenges,” in Proc. Eur. Chapter Assoc. Comput.
Linguist (EACL), St. Julian’s, Malta, Mar. 2024, pp. 225–237.
[36] D. Bandyopadhyay, et al., “Thinking machines: A survey of LLM based
reasoning strategies,” ArXiv, vol. abs/2503.10814, Mar. 2025.
[37] M. Ahn, et al., “Do as I can, not as I say: Grounding language in robotic
affordances,” in Proc. Conf. Robot Learn. (CoRL 2022), Auckland, New
Zealand, Dec. 2022.
[38] Z. Shao, et al., “DeepSeekMath: Pushing the limits of mathematical
reasoning in open language models,” ArXiv, vol. abs/2402.03300, Apr.
2024.
[39] N. H. Tran, et al., “Federated learning over wireless networks: Opti-
mization model design and analysis,” in Proc. IEEE Int. Conf. Comput.
Commun. (INFOCOM), Paris, France, Apr. 2019.
[40] A. Albaseer, et al., “Data-driven participant selection and bandwidth
allocation for heterogeneous federated edge learning,” IEEE Trans. Syst.
Man Cybern. Syst., vol. 53, no. 9, pp. 5848–5860, Sep. 2023.
[41] J.
Schulman,
et
al.,
“Proximal
policy
optimization
algorithms,”
arXiv:1707.06347 [cs.LG], Jul. 2017.
[42] L. Devroye, et al., “The total variation distance between high-
dimensional gaussians with the same mean,” ArXiv, vol. abs/1810.08693,
10 2018.
[43] D. A. Levin, et al., Markov Chains and Mixing Times.
Providence, RI:
American Mathematical Society, 2009.
[44] M. Kearns, et al., “Near-optimal reinforcement learning in polynomial
time,” Mach. Learn., vol. 49, no. 2, pp. 209–232, Jan. 2002.
[45] L. Chen, et al., “Decision transformer: Reinforcement learning via
sequence modeling,” in Proc. Adv. Neural Inf. Proces. Syst. (NeurIPS),
Virtual Edition, Dec. 2021.
[46] T. Haarnoja, et al., “Soft actor-critic: Off-policy maximum entropy deep
reinforcement learning with a stochastic actor,” in Proc. Int. Conf. Mach.
Learn. (ICML), vol. 80, 10–15 Jul. 2018, pp. 1861–1870.
[47] M. Asif, et al., “CAMP-GNN: A constraint-aware message-passing
model for optimal resource allocation in software projects,” IEEE
Access, vol. 13, pp. 87 965–87 977, 2025.
[48] M. Asif, et al., “Advanced zero-shot learning (AZSL) framework for
secure model generalization in federated learning,” IEEE Access, vol. 12,
pp. 184 393–184 407, 2024.
