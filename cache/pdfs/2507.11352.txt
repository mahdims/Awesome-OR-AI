--- Page 1 ---
Proceedings of Machine Learning Research vol vvv:1â€“14, 2026
Foundation Models for Logistics Operations:
Toward Certifiable, Conversational Interfaces
Yunhao Yang1,2*, Neel P. Bhatt1,2, Christian Ellis1,2, Samuel Li2, Alvaro Velasquez1,3,
Zhangyang Wang2, Ufuk Topcu1,2
1 Neurosymbolic Intelligence, 2 The University of Texas at Austin, 3 University of Colorado Boulder
Abstract
Logistics operators â€“ from battlefield coordinators re-routing airlifts ahead of a storm to ware-
house managers juggling late trucks â€“ need to make mission-critical decisions. Prevailing methods
for logistics planning such as integer programming yield plans that satisfy user-defined logical
constraints, assuming an idealized mathematical model of the environment. On the other hand,
foundation models lower the intermediate processing barrier by translating natural-language user
utterances into executable plans, yet they remain prone to misinterpretations and hallucinations
that jeopardize safety and cost. We introduce a Visionâ€“Language Logistics (VLL) agent, built on a
neurosymbolic framework that pairs the accessibility of natural-language dialogue with verifiable
guarantees on user-objective interpretation. The agent interprets user requests and converts them
into structured planning specifications, quantifies the uncertainty of the interpretation, and invokes
an interactive clarification loop when the uncertainty exceeds an adaptive threshold. Drawing on a
lightweight airlift logistics planning use case as an illustrative case study, we highlight a practical
path toward certifiable and user-aligned decision-making for complex logistics. Our lightweight
model, fine-tuned on just 100 training samples, surpasses the zero-shot performance of 20x larger
models in logistic planning tasks while cutting inference latency by nearly 50%. Our code and
video demonstrations are available at here.
Keywords: Foundation Model, Logistics, Uncertainty Quantification, Formal Methods, Auto-
mated Fine-Tuning, Prompt Optimization
1. Introduction
Toward Conversational, Certifiable Logistics. From humanitarian air-lifts that must beat incom-
ing storms to e-commerce warehousing that re-optimizes routes every minute, modern logistics is
increasingly a race against time, uncertainty, and domain complexity ( Â¨Ozdamar et al., 2004; Ghiani
et al., 2004). Operators juggle heterogeneous dataâ€”live satellite imagery, weather forecasts, inven-
tory databases, and a lattice of hard constraints on safety, cost, and regulation. Today, interacting
with these systems demands fluency in specialized planning languages and manual cross-checks,
leaving non-experts on the sidelines and slowing even seasoned professionals when crises strike
(Nicoletti and Appolloni, 2024, 2025). This challenge motivates growing interest in more accessi-
ble, conversational interfaces for logistics planning.
We develop Visionâ€“Language-Logistics (VLL) agents: multimodal AI co-pilots that can (i) under-
stand rich, free-form instructions; (ii) ground those instructions in real-time perceptual data; (iii)
synthesize and visualize executable plans through tight neurosymbolic integration; and (iv) prove
* This work was initiated during the internship of Yunhao Yang at the Neurosymbolic Intelligence.
Â© 2026 Y.Y.N.P.B.C.E.S.L.A.V..Z.W.U.T. .
arXiv:2507.11352v2  [cs.AI]  30 Jan 2026


--- Page 2 ---
Plan 
Solver
Vision-Language Logistic (VLL) Agent 
Graphic-Signal Database
Map ð’¢t
Constraints Ct
Goal yt
Goal Spec   rt
Solution Ï€t
External Information 
User
Textual, Visual Input 
(It, Ut)
Foundation Model fÎ¸
Weather
Trafï¬c Condition
News
â€¦â€¦
Map ð’¢t
Constraints Ct
(MOVE veh1 loc1 loc3) 
(MOVE veh2 loc1 loc2) 
(LOAD pkg3 veh1 loc 3) 
â€¦â€¦
Figure 1: Overview of an VLL agent. Language and visual inputs are converted into structured
goals, filtered through an uncertainty-aware verifier, and dispatched to symbolic planners.
to the user, via explicit uncertainty signals and formal checks, that the inferred goals (i.e., user ob-
jectives) align with the userâ€™s true intent. VLL agents build on parallel advances in visionâ€“language
modeling, uncertainty estimation, and neurosymbolic verification, situating these techniques within
the emerging landscape of conversational, certifiable logistics systems.
A VLL agent is a closed perceptionâ€“reasoningâ€“action loop. At the perception stage, vision models
extract salient entities (e.g., damaged runways or congested highways) that ground the referents
in a natural-language command. A language model then translates the grounded request into a
structured goal specification (e.g., PDDL or SQL) that downstream planners can solve. Finally, a
symbolic verifier or constraint solver confirms feasibility and compliance, returning either a certified
plan or a targeted clarification query. This architecture, depicted in Figure 1, enables conversational
replanning at the cadence of operational tempo, i.e., users can issue follow-up directives, receive
intermediate analyses, and iterate until the plan is both satisfactory and formally sound. In short,
VLL agents promise accessibility without sacrificing reliability.
A Key Component: Uncertainty-Aware Intentâ€“Verification. Among the many moving parts
of a VLL agent, we concentrate on a critical yet often underserved component: measuring and
reducing the uncertainty in goal classification. Standard LLM/VLM pipelines silently commit to
their first parse of a request; if that parse is wrong, downstream optimizers will dutifully compute
an optimal solution to the wrong problem. The VLL agent augments its VLM backbone with a
probabilistic field-level uncertainty estimator. Whenever the estimated uncertainty value in any
essential slotâ€”e.g., destination airbase or delivery deadlineâ€”falls below an adaptive threshold, the
agent pauses, asks a pointed follow-up question, and only proceeds once uncertainty is resolved.
Technically, the verification loop maps user goals to a learned latent space, where different goal
types form distinct clusters. Building on the latent space, we introduce a Probabilistic Guarantee: a
theoretical lower bound of correct goal classification measured via the distance from a new inputâ€™s
latent vector to the nearest cluster centroid. Samples with high guarantee indicate low uncertainty.
These low-uncertainty traces feed a contrastive self-training loop that steadily sharpens the VLL
agentâ€™s goal classification ability, allowing it (using the GPT-4o-mini backbone) to outperform a
20x larger GPT-4.1 model on goal-match accuracy while halving inference latency.
2


--- Page 3 ---
FOUNDATION MODELS FOR CERTIFIABLE LOGISTICS
Contributions.
We (i) introduce the VLL agents as a neurosymbolic framework for conversational
logistics planning, (ii) propose an uncertainty-aware intentâ€“verification loop that maps multimodal
inputs into a learned latent space and provides a probabilistic guarantee on correct goal interpreta-
tion, enabling proactive clarification before planning, and (iii) empirically show how this guarantee
can guide rapid refinement through fine-tuning and prompt optimization. Experiments show that
lightweight models trained on a limited number of high-guarantee samples achieve higher goal
classification accuracy.
2. Related Work
Practical applications and benchmarks in logistics.
Research in logistics and transportation
supports practical demands and provides benchmarks for evaluation. Classical formulations, for
example dynamic pickup-and-delivery Berbeglia et al. (2010) and airlift planning under operational
constraints Bertsimas et al. (2019) â€“ establish the cost, capacity, and timing structures that real
systems must comply with. Recently, the Airlift Challenge frames cargo delivery as a community
benchmark with dynamic conditions Delanovic et al. (2024). Against this backdrop, LLM/VLM-
based systems are increasingly explored as front-ends that translate informal directives into struc-
tured goals. Yet, most existing demonstrations treat models as black-box solvers (Nicoletti and
Appolloni, 2025, 2024). We emphasizes transparent goal interpretation and compatibility with sym-
bolic verifiers, aiming to make conversational interfaces viable for logistics operations.
Natural language interfaces for planning.
Recent advances in LLMs have significantly im-
proved the ability to convert natural language into formal planning representations. Prior work
has explored using LLMs to translate high-level goals into formats such as PDDL, JSON, or action
graphs, enabling non-expert users to interact with complex systems (Song et al., 2022; Ichter et al.,
2022; Shah et al., 2022; Huang et al., 2022; Hao et al., 2024). Collectively, these works lower bar-
riers for non-experts and hence improve accessibility of the planner. On the downside, they assume
that the LLMs interpret user objectives correctly and lack verification mechanisms to ensure plan
correctness or goal alignment.
Uncertainty estimation in LLMs.
Uncertainty estimation in LLMs has gained increasing atten-
tion as a mechanism to detect hallucinations, flag low-confidence predictions, or guide interaction.
Techniques such as token-level entropy, Monte Carlo dropout, and calibration-based methods have
been applied to identify unreliable outputs (Gupta et al., 2024; MilanÂ´es-Hermosilla et al., 2021;
Yang et al., 2024b; Bhatt et al., 2025). Together, these works motivate proactive interaction loopsâ€”
surfacing ambiguity before committing to a planâ€”but most stop short of offering guarantees on
intent correctness in logistics contexts.
Neurosymbolic and verification-informed planning.
Recent works combines neural models
with symbolic verification or formal reasoning systems for robotics and planning tasks, offering
correctness guarantees (Chen et al., 2025; Yang et al., 2024a; Li et al., 2024; Yang et al., 2023).
The VLL agent extends the neurosymbolic and verification ideas to the problem of natural lan-
guage goal interpretation in logistics, using uncertainty-aware learning to balance accessibility and
verifiability.
3


--- Page 4 ---
User
Foundation Model fÏ€
Interpreted 
Goal yt
Representation zt ð’¢Ì‚n
Uncertainty Estimation
0
1
Calibration Distribution FC
Probabilistic Guarantee 
âˆˆp(yt|zt)
If 
 below a threshold , ask to clarify or trigger reï¬nement
âˆˆp(yt|zt)
Î¸
Embedding Model P
There is a severe weather 
condition in IST
Textual and Visual Input 
(It, Ut)
 
 
y1
y2
y3
Latent Space Ì‚n
Figure 2: Overview of the uncertainty-aware intent-verification loop.
3. Problem Formulation
We develop a Visionâ€“Language Logistics (VLL) agent: a neurosymbolic system that transforms free-
form, multimodal user instructions into certified, executable logistics decisions. The key challenge
is that user intent is often implicit or ambiguous, while downstream planners and solvers require
precise, structured specifications. We formalize two coupled problems: (P1) end-to-end VLL agent
construction with a three-stage architecture, and (P2) uncertainty-aware intent verification concen-
trated in the reasoning stage.
Notation and Model
Let t be an index. The user provides a multimodal prompt
xt â‰œ(It, Ut) âˆˆX,
where It is a natural-language instruction and Ut is a visual context (e.g., satellite imagery, camera
feeds, annotated maps). Let R denote a space of structured planning representations (e.g., PDDL
instances, SQL queries, or symbolic task graphs), and let Y = {1, . . . , K} denote a finite set of user
goals (e.g., query database, update database, solve a routing problem). We denote by yâ‹†
t âˆˆY the
userâ€™s true goal.
Design an End-to-End VLL Agent with Three Stages.
A VLL agent is a closed-loop system
composed of a perception module, a foundation model, and downstream solver.
A perception module processes visual inputs Ut to extract salient entities, attributes, and situational
cues, producing a grounded state abstraction st âˆˆS. This stage resolves linguistic referents such as
â€œthe nearest operational airbaseâ€ or â€œavoid flooded regions.â€
A foundation model fÎ¸ reasons the multimodal inputs and maps them into a structured specification
rt and a goal label yt:
(rt, yt) = fÎ¸(It, st) âˆˆR Ã— Y.
A downstream solver produces a candidate plan Ï€t = Solve(rt), which is checked against domain
constraints Ct = C âˆªrt (C is a set of universal constraints like safety rules, regulations, resource
limits) via a symbolic verifier. If verification fails, the agent returns diagnostic feedback and updates
the representation or user prompt.
4


--- Page 5 ---
FOUNDATION MODELS FOR CERTIFIABLE LOGISTICS
Develop an Uncertainty-Aware Intent Verification Loop.
Downstream planners assume that
the inferred goal yt correctly reflects the userâ€™s intent. However, misclassification at the reasoning
stage can lead to optimal solutions for the wrong problem. We therefore require the reasoning stage
to be uncertainty-aware and certifiable.
Given input xt, the reasoning module outputs not only a predicted goal yt, but also a probabilistic
guarantee
Ë†pt â‰œË†p(yt | xt),
which lower-bounds the probability that yt equals the true goal yâ‹†
t :
Ë†p(yt | xt) â‰¤Pr(yt = yâ‹†
t | xt).
Uncertainty-Guided Refinement.
Develop a reasoning-stage mechanism that computes Ë†p(yt |
xt), and uses this guarantee to guide both interactive clarification and uncertainty-filtered fine-tuning
of the foundation model fÎ¸. Formally, given a stream of interaction data, we aim to maximize correct
goal inference conditioned on acceptance:
max
Î¸
Pr(fÎ¸(x) = yâ‹†| Ë†p(fÎ¸(x) | x) â‰¥Ï„) ,
while controlling the clarification frequency induced by Ï„.
4. Vision-Language Logistic Agent
We design the VLL agent as a modular, closed-loop pipeline that integrates multimodal perception,
symbolic planning, and formal verification.
At each interaction round t, the agent receives visual+textual inputs (It, Ut). These inputs are pro-
cessed to a foundation model fÎ¸ that translates the multimodal input into a structured goal specifi-
cation rt expressed in the PDDL (Haslum et al., 2019), which serves as the interface to downstream
planners. Then, given a PDDL goal specification rt, an off-the-shelf PDDL solver is invoked to
synthesize a plan Ï€t. We present the architecture in Figure 1.
Crucially, the reasoning stage does not commit to a PDDL goal unless the inferred user intent
(i.e., goal) has been verified with sufficient confidence. We propose an uncertainty-aware intent-
verification loop that acts as a gatekeeper between multimodal interpretation and symbolic planning,
ensuring that formal verification is applied only to goals that are certified to reflect user intent. We
present the online interface of the VLL agent with more demonstrations in the code repository.
4.1. Uncertainty-Aware Intent-Verification
We develop an uncertainty-aware intent-verification loop that (i) infers the user goal from mul-
timodal inputs, (ii) computes a calibrated probabilistic guarantee on intent correctness, and (iii)
triggers interactive clarification when the guarantee is insufficient. The goal is to ensure that down-
stream planning operates only on objectives that are both well-formed and certified to reflect user
intent. We present this loop in Figure 2.
5


--- Page 6 ---
We focus exclusively on the reasoning stage of the VLL agent. Perception and symbolic verification
are treated as black-box components that provide grounded inputs and constraint checks, respec-
tively. Our contribution is an intent-verification loop that sits between perception and planning and
provides formal guarantees on goal classification.
User Goal Classification.
Given a multimodal input xt = (It, Ut), the VLL agent uses a founda-
tion model fÎ¸ to infer the userâ€™s goal. Concretely, the model treats intent inference as a classification
problem over a set of goals Y. The foundation model outputs yt = fÎ¸(xt), where yt is the predicted
label corresponding to the userâ€™s high-level goal (e.g., information retrieval, database update, or
logistics planning).
Latent Space Learning.
We train an embedding model P to map multimodal inputs and their pre-
dicted goals into an n-dimensional latent space Rn in which distinct user goals form well-separated
clusters. Using 400 manually labeled examples, we optimize P with a supervised contrastive loss.
Let zi = P(yi, Ii, Ui) âˆˆRn denote the latent representation with label yâˆ—
i . For a minibatch B and
positive set P(i) = {j âˆˆB \ {i} : yj = yi}, we minimize
Lcon =
X
iâˆˆB
âˆ’1
|P(i)|
X
jâˆˆP(i)
log
exp
 zâŠ¤
i zj/Ï„

P
kâˆˆB\{i} exp
 zâŠ¤
i zk/Ï„
,
where Ï„ > 0 is a temperature parameter. Figure 3 illustrates the learned latent space for a three-class
goal classification task (see Empirical Analysis).
Guarantee on Classification Correctness.
Building on the clustered latent space, we formalize
how distances in the latent space are calibrated to a likelihood of classification correctness.
We define the probabilistic guarantee as a lower bound on the probability that the predicted goal yt
matches the userâ€™s true goal yâˆ—
t given the latent representation zt = P(yt, It, Ut) âˆˆRn:
Ë†p(yt | zt) = Pr

yt = yâˆ—
t
 zt

.
To estimate Ë†p(yt | zt), we use a small calibration set of 200 manually labeled examples to construct
a calibration distribution with probability density function (PDF) FC. FC maps the distance dt =
||zt âˆ’câˆ—||2 from zt to its nearest cluster centroid câˆ—to the probability that a sample from another
class lies beyond that distance
Pr

||zi âˆ’câˆ—||2 > dt
 yi Ì¸= yâˆ—
c

,
zi denotes any latent vector and yâˆ—
c denotes the label of the cluster where câˆ—lies in. Figure 3 shows
an example of FC obtained from the latent representations.
Given a new input, we compute its distance dt to the nearest centroid and evaluate
Ë†p(yt | zt) = 1 âˆ’
 1 âˆ’FC(dt)

Â· Pr

yi Ì¸= yâˆ—
c

Pr

||zi âˆ’câˆ—||2 â‰¤dt

.
(1)
Pr

yi Ì¸= yâˆ—
c

is the number of other-class samples over the total number of samples, and Pr

||zi âˆ’
câˆ—||2 â‰¤dt

is the percentage of samples whose distance to câˆ—is within dt. Higher values of Ë†p(yt |
zt) indicate greater confidence in the predicted goal. We present the theorems and proofs for the
probabilistic guarantee in the Appendix A.
6


--- Page 7 ---
FOUNDATION MODELS FOR CERTIFIABLE LOGISTICS
Proactive Clarification.
When the probabilistic guarantee Ë†pt = Ë†p(yt | zt) falls below a predefined
threshold Ï„, the VLL agent proactively enters a clarification mode before passing the goal to the
planner. In this mode, the agent implements a clarification loop governed by a threshold Ï„ âˆˆ(0, 1):
â€¢ If Ë†pt â‰¥Ï„, the inferred goal yt is accepted and passed to the planner.
â€¢ If Ë†pt < Ï„, the agent issues a targeted clarification query to the user (e.g., â€œDo you mean to
minimize cost or time?â€ or â€œShould weather conditions be considered?â€).
This loop terminates at a stopping time T such that Ë†p(yT | xT ) â‰¥Ï„, ensuring that planning proceeds
only after sufficient confidence in the intent correctness.
4.2. Uncertainty-Guided Refinement
We then develop two refinement methods that leverage the calibrated uncertainty signal, i.e., prob-
abilistic guarantee, to improve the VLL agentâ€™s goal classification.
DPO with Guarantee Ranking.
The first method uses the probabilistic guarantee to induce pref-
erence rankings for Direct Preference Optimization (DPO) (Rafailov et al., 2023). Given a user
input xt and a set of candidate interpretations {y(k)
t
}K
k=1 produced by the foundation model, we
compute their guarantees Ë†p(k)
t
â‰œË†p

y(k)
t
| xt

. For any pair (i, j) such that Ë†p(i)
t
> bp(j)
t , we rank a
pair y(i)
t
â‰»y(j)
t . These ranked pairs are used to fine-tune the model via DPO by maximizing the
likelihood ratio between preferred and dispreferred goals.
TextGrad with Online Uncertainty Feedback.
In addition to model-parameter fine-tuning, we
employ TextGrad (Yuksekgonul et al., 2024) to optimize the system prompt using online interac-
tion feedback. We convert the probabilistic guarantee into auto-generated textual feedback (e.g.,
â€œthe inferred goal is correct with approximately 75% confidenceâ€), which serves as a differentiable
supervision signal for prompt optimization.
Generality Beyond Logistics.
Although demonstrated in a logistics setting, the uncertainty-aware
intent-verification loop is domain-agnostic and can be applied to any multimodal system requiring
reliable goal interpretation, such as robotic execution, autonomous driving, database querying, and
decision-support systems.
5. Empirical Analysis
We evaluate the intent verification pipeline through quantitative improvements in model perfor-
mance, demonstrating that proactive clarification and uncertainty-guided refinement significantly
improve the modelâ€™s ability to infer user goals in the context of logistics planning.
Experimental Setting.
We evaluate the VLL agent on a lightweight airlift logistics planning
domain designed to stress goal interpretation under uncertainty, rather than downstream opti-
mization quality. The learning task is to infer the userâ€™s high-level goal yt âˆˆY, where Y =
{Query, Update, Plan} corresponds to (i) extracting information from a logistics database, (ii) up-
dating the database, or (iii) solving a logistics planning problem. These categories capture the
dominant failure modes in conversational logistics systems, where an incorrect goal classification
leads to valid but operationally irrelevant plans.
7


--- Page 8 ---
Figure 3: The left plot is a latent space before learning. The middle plot is a learned space produced
from a fine-tuned model, where three classes of user goals are separated. The right plot shows the
calibration distributions of the three classes estimated via the learned latent space.
Figure 4: Comparison on goal classification accuracies and re-query frequencies. The fine-tuned
VLL achieves better performance than all other baselines with a lower frequency of re-query, show-
ing the effectiveness of uncertainty-guided fine-tuning in aligning model behavior with user goals.
We construct a dataset of 400 manually labeled interaction traces to learn the latent space, and an
additional 200 labeled samples to estimate the calibration distribution for the probabilistic guaran-
tee. For refinement, we follow Section 4.2 to create 100 samples for each refinement approach.
During evaluation, we use 200 testing samples that are independent to the training and calibration
data.
Baselines.
We compare the VLL agent against a set of foundation-model baselines summarized
in Figure 5. For all baselines, we directly query the model for goal classification without any
verification or clarification loop. Specifically, given a multimodal input xt = (It, Ut), the model is
prompted with an instruction of the form: â€œClassify the user goal based on the given inputs into
one of the following categories: Query, Update, or Plan.â€ We extract the predicted goal yt from the
modelâ€™s output.
We consider 6 baseline models as presented in Figure 5, all evaluated in a zero-shot setting using
the same prompt template.
Evaluation Metric.
We evaluate the impact of the proposed uncertainty-guided fine-tuning on the
VLL agentâ€™s foundation model fÎ¸ (GPT-4o-mini backbone) and use three metrics:
Threshold is a scalar cut-off value Ï„ to the probabilistic guarantee Ë†pt = Ë†p(yt | zt). Only samples
with scores above Ï„ are retained for accuracy evaluation.
8


--- Page 9 ---
FOUNDATION MODELS FOR CERTIFIABLE LOGISTICS
Initial Prompt
Ë†pt
Optimized Prompt
Ë†pt
A blocking condition appears in
the route between airports 1 and 5
that needs to be shown.
0.607
Change the value of â€˜route avail-
ableâ€™ to false for all entries where
â€˜origin airport idâ€™ equals 1 and
â€˜destination airport idâ€™ equals 5
0.722
Analyze routes from airport 3 are
consistent and cross-check incon-
sistent prices and cargoes.
0.669
Retrieve all routes where airport id
= 3; compare entries for matching
destinations to identify discrepan-
cies in price and cargo type.
0.738
Table 1: Examples of uncertainty-guided prompt optimization.
Accuracy is the proportion of data samples whose predicted intent yt matches the ground-truth intent
yâˆ—
t , computed over the subset of samples that pass the threshold filter:
Accuracy = #{ yt = yâˆ—
t âˆ§Ë†p(yt | zt) â‰¥Ï„ } / #{ Ë†p(yt | zt) â‰¥Ï„ }.
Re-query Frequency is the proportion of evaluation samples for which the uncertainty signal falls
below the threshold Ï„, triggering a clarification query to the user. Lower values indicate fewer
interruptions for clarification, while higher values suggest more frequent user interaction to resolve
intent ambiguity.
Results and Analysis.
Figures 4 and 5 present the quantitative results of uncertainty-aware refine-
ment on goal classification. Figure 4 illustrates the trade-off between goal classification accuracy
and re-query frequency as the acceptance threshold Ï„ varies. Across all thresholds, VLL consistently
achieve higher accuracy for a fixed clarification rate compared to zero-shot foundation models.
Figure 5 compares the VLL agents against increasingly larger foundation-model baselines. Despite
using a significantly smaller backbone, the VLL outperforms 20x larger models such as GPT-5 in
goal classification accuracy while inducing faster response time. Notably, the combined refinement
strategy (fine-tuning + prompt optimization) yields the strongest overall performance, showing the
effectiveness of our uncertainty-guided refinement strategies.
Table 1 provides qualitative insight into this improvement by illustrating prompt optimization driven
by uncertainty feedback. In each example, the refined prompt produced by the VLL agent is more
explicit and operationally grounded, leading to a higher probabilistic guarantee Ë†pt. Together, these
results indicate that uncertainty-aware verification and refinement improve both the correctness of
inferred goals and the clarity of model reasoning, which are critical for reliable conversational lo-
gistics systems.
Ablation Study.
We conduct an ablation study to assess the necessity of each component in the
uncertainty-aware intent-verification and refinement. All variants share the same foundation model
backbone and training data. We present the classification accuracy and average classification latency
per query in Table 2. For each variant, we use the corresponding uncertainty signal to fine-tune the
backbone foundation model and present the accuracy and the average inference time. There is no
fine-tuning if the signal is not specified. We show that removing any single component degrades the
classification accuracy, while does not significantly reduce the latency.
9


--- Page 10 ---
Figure 5: Comparison between the baseline foundation models and our refined VLL agents. â€œVLL
(combined)â€ refers to the fine-tuned GPT-4o-mini backbone with optimized prompts. The refined
VLL with 4o-mini backbone outperforms the 20x larger models such as GPT-5.
System Variant
Projector
Loss
Signal
Fine-Tuned
Accuracy
Latency (s)
VLL (fine-tune)
Learned
Contrastive
Guarantee
âœ“
0.96
1.33
FM only (zero-shot)
â€“
â€“
â€“
0.73
1.20
Projector only
Learned
Contrastive
â€“
âœ“
0.65
0.15
No projector P
â€“
â€“
Softmax
âœ“
0.75
1.28
Frozen projector
Frozen
Contrastive
Guarantee
âœ“
0.81
1.33
Supervised
Learned
Cross Entropy
Guarantee
âœ“
0.91
1.34
Softmax
Learned
Contrastive
Softmax
âœ“
0.92
1.29
Table 2: Ablation study over different projectors, projector training losses, uncertainty signals for
ranking DPO data and clarification, and fine-tuned/zero-shot foundation models. Among the vari-
ants, FM only refers to directly querying the backbone foundation model for classification. Projector
only is an MLP trained for goal classification using labeled data. No projector removes latent space
and relies on model confidence alone. Frozen projector uses a fixed, untrained latent mapping.
Supervised replaces contrastive learning with supervised learning. Softmax replaces probabilistic
guarantee with the softmax confidence of each prediction.
6. Conclusion
We introduce a neurosymbolic framework for conversational logistics that prioritizes certifiable goal
interpretation over black-box plan generation. By mapping multimodal user inputs into a learned
latent space and computing a calibrated probabilistic guarantee on goal correctness, the VLL agent
can proactively clarify ambiguous user requests and guide uncertainty-aware refinement to its back-
bone foundation model. Empirical results show that these mechanisms enable smaller foundation
models to outperform substantially larger ones in goal classification accuracy while reducing un-
necessary clarification, demonstrating that structured uncertainty signals are more critical than raw
model scale for reliable conversational logistics.
Future work will extend VLL agents beyond intent verification to end-to-end certifiable logistics
pipelines. Potential directions include integrating symbolic proof engines to generate machine-
checkable certificates of plan correctness, supporting real-time grounding from streaming percep-
tual inputs, and enabling cache-guided plan updates under changing conditions.
10


--- Page 11 ---
FOUNDATION MODELS FOR CERTIFIABLE LOGISTICS
References
Gerardo Berbeglia, Jean-FrancÂ¸ois Cordeau, and Gilbert Laporte. Dynamic pickup and delivery
problems. European Journal of Operational Research, 202(1):8â€“15, 2010.
Dimitris Bertsimas, Allison Chang, Velibor V MiË‡siÂ´c, and Nishanth Mundru. The airlift planning
problem. Transportation Science, 53(3):773â€“795, 2019.
Neel P. Bhatt, Yunhao Yang, Rohan Siva, Daniel Milan, Ufuk Topcu, and Zhangyang Wang. Know
where youâ€™re uncertain when planning with multimodal foundation models: A formal framework.
In Proc. MLSys, 2025.
Xin Chen, Yarden As, and Andreas Krause. Learning safety constraints for large language models.
arXiv preprint arXiv:2505.24445, 2025.
Adis Delanovic, Carmen Chiu, and Andre Beckus. Airlift challenge: A competition for optimizing
cargo delivery. arXiv preprint arXiv:2404.17716, 2024.
Gianpaolo Ghiani, Gilbert Laporte, and Roberto Musmanno. Introduction to logistics systems plan-
ning and control. John Wiley & Sons, 2004.
Neha Gupta, Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Aditya Krishna
Menon, and Sanjiv Kumar.
Language model cascades: Token-level uncertainty and beyond.
arXiv preprint arXiv:2404.10136, 2024.
Yilun Hao, Yang Zhang, and Chuchu Fan. Planning anything with rigor: General-purpose zero-shot
planning with llm-based formalized programming. arXiv preprint arXiv:2410.12112, 2024.
Patrik Haslum, Nir Lipovetzky, Daniele Magazzeni, and Christian Muise. An Introduction to the
Planning Domain Definition Language. Synthesis Lectures on Artificial Intelligence and Ma-
chine Learning. Morgan & Claypool Publishers, 2019.
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan
Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown,
Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter.
Inner monologue: Embodied
reasoning through planning with language models. In Conference on Robot Learning, volume
205 of Proceedings of Machine Learning Research, pages 1769â€“1782, Auckland, New Zealand,
2022. PLMR.
Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog,
Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine,
Yao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander Toshev, Vincent Vanhoucke,
Fei Xia, Ted Xiao, Peng Xu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas
Sievers, Clayton Tan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Jornell Quiambao, Peter
Pastor, Linda Luu, Kuang-Huei Lee, Yuheng Kuang, Sally Jesmonth, Nikhil J. Joshi, Kyle Jeffrey,
Rosario Jauregui Ruano, Jasmine Hsu, Keerthana Gopalakrishnan, Byron David, Andy Zeng,
and Chuyuan Kelly Fu. Do as I can, not as I say: Grounding language in robotic affordances.
In Conference on Robot Learning, volume 205 of Proceedings of Machine Learning Research,
pages 287â€“318, Auckland, New Zealand, 2022. PLMR.
11


--- Page 12 ---
Zelong Li, Wenyue Hua, Hao Wang, He Zhu, and Yongfeng Zhang.
Formal-llm: Integrat-
ing formal language and natural language for controllable llm-based agents.
arXiv preprint
arXiv:2402.00798, 2024.
Daily MilanÂ´es-Hermosilla, Rafael Trujillo CodorniÂ´u, RenÂ´e LÂ´opez-Baracaldo, Roberto SagarÂ´o-
Zamora, Denis Delisle-Rodriguez, John Jairo Villarejo-Mayor, and JosÂ´e Ricardo NÂ´uËœnez- Â´Alvarez.
Monte carlo dropout for uncertainty estimation and motor imagery classification. Sensors, 21
(21):7241, 2021.
Bernardo Nicoletti and Andrea Appolloni. Green logistics 5.0: a review of sustainability-oriented
innovation with foundation models in logistics. European Journal of Innovation Management,
27(9):542â€“561, 2024.
Bernardo Nicoletti and Andrea Appolloni. The impact of ai foundation models on the future of
digital engineering for logistics and supply chain. Digital Engineering, page 100058, 2025.
Linet Â¨Ozdamar, Ediz Ekinci, and Beste KÂ¨ucÂ¸Â¨ukyazici. Emergency logistics planning in natural dis-
asters. Annals of operations research, 129(1):217â€“245, 2004.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and
Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.
Advances in neural information processing systems, 36:53728â€“53741, 2023.
Dhruv Shah, Blazej Osinski, Brian Ichter, and Sergey Levine. Lm-nav: Robotic navigation with
large pre-trained models of language, vision, and action. In Conference on Robot Learning,
volume 205 of Proceedings of Machine Learning Research, pages 492â€“504, Auckland, New
Zealand, 2022. PMLR.
Chan Hee Song, Jiaman Wu, Clay Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su. Llm-
planner: Few-shot grounded planning for embodied agents with large language models, 2022.
Yunhao Yang, Cyrus Neary, and Ufuk Topcu. Multimodal pretrained models for verifiable sequen-
tial decision-making: Planning, grounding, and perception. In Proc. AAMAS, pages 2011â€“2019,
2023.
Yunhao Yang, Neel P. Bhatt, Tyler Ingebrand, William Ward, Steven Carr, Zhangyang Wang, and
Ufuk Topcu. Fine-tuning language models using formal methods feedback: A use case in au-
tonomous systems. In Proc. MLSys, 2024a.
Yunhao Yang, Yuxin Hu, Mao Ye, Zaiwei Zhang, Zhichao Lu, Yi Xu, Ufuk Topcu, and Ben Snyder.
Uncertainty-guided enhancement on driving perception system via foundation models. arXiv
preprint arXiv:2410.01144, 2024b.
Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and
James Zou. Textgrad: Automaticâ€ differentiationâ€ via text, 2024.
12


--- Page 13 ---
FOUNDATION MODELS FOR CERTIFIABLE LOGISTICS
Appendix A. Theoretical Support for Probabilistic Guarantee
We provide theoretical supports for the probabilistic guarantee Ë†p(yt | zt) used in our uncertainty-
aware intentâ€“verification pipeline. Specifically, we show how distances in the learned latent space,
constructed from multimodal embeddings of textual and visual inputs, can be mapped to calibrated
confidence estimates on user intent classification. By leveraging geometric separation properties be-
tween intent clusters, we establish bounds on misclassification probability, which is the probabilistic
guarantee Ë†p(yt | zt).
Definition 1 (Distance to Centroid)
Let Z âŠ‚Rn be the latent embedding space induced by the
multimodal encoder P, which jointly embeds textual and visual inputs (It, Ut) along with their
predicted intent labels yt. Suppose C = {c1, c2, . . . , cK} denotes the set of cluster centroids for K
intent classes, estimated from a calibration dataset Dc of size Nc. For a given input (It, Ut) with
latent embedding zt = P(It, Ut, yt), define the Euclidean distance to the nearest centroid as:
dt = min
ckâˆˆC âˆ¥zt âˆ’ckâˆ¥2.
Definition 2 (Calibration Distribution) A calibration distribution, whose probability density func-
tion is FC : R+ â†’[0, 1], is the distribution of distances from correctly classified calibration sam-
ples to their corresponding class centroids. Given a new input, the function FC takes the distance
dt to its nearest centroid câˆ—as input, and returns a probability that a sample from another class lies
beyond dt:
Pr

||zi âˆ’câˆ—||2 > dt
 yi Ì¸= yâˆ—
c

,
yâˆ—
c is the ground truth label. This distribution is approximated by using a labeled calibration dataset.
Theorem 3 (Latent Distance to Probabilistic Guarantee)
Given a new input, we compute its distance dt to the nearest centroid and compute the probabilistic
guarantee
Ë†p(yt | zt) =
 1 âˆ’FC(dt)

Â· Pr

yi Ì¸= yâˆ—
c

/ Pr

||zi âˆ’câˆ—||2 â‰¤dt

.
(2)
Pr

yi Ì¸= yâˆ—
c

is the number of other-class samples over the total number of samples, and Pr

||zi âˆ’
câˆ—||2 â‰¤dt

is the percentage of samples whose distance to câˆ—is within dt.
Proof According to Definition 2, given a new input whose latent space distance to its nearest
centroid is dt, then
FC(dt) = Pr

||zi âˆ’câˆ—||2 > dt
 yi Ì¸= yâˆ—
c

.
Recall that the probabilistic guarantee Ë†p(yt | zt) is
Pr

yi = yâˆ—
c
 ||zi âˆ’câˆ—||2 â‰¤dt

,
which is equal to
1 âˆ’Pr

yi Ì¸= yâˆ—
c
 ||zi âˆ’câˆ—||2 â‰¤dt

,
13


--- Page 14 ---
where yi is the latent representation of a randomly selected sample. By the Bayesian rule, we know
that
Pr

yi Ì¸= yâˆ—
c
 ||zi âˆ’câˆ—||2 â‰¤dt

= Pr

||zi âˆ’câˆ—||2 â‰¤dt
 yi Ì¸= yâˆ—
c

Â· Pr

yi Ì¸= yâˆ—
c

Pr

||zi âˆ’câˆ—||2 â‰¤dt

(3)
= (1 âˆ’Pr

||zi âˆ’câˆ—||2 > dt
 yi Ì¸= yâˆ—
c

) Â· Pr

yi Ì¸= yâˆ—
c

Pr

||zi âˆ’câˆ—||2 â‰¤dt

(4)
= (1 âˆ’FC(dt)) Â· Pr

yi Ì¸= yâˆ—
c

Pr

||zi âˆ’câˆ—||2 â‰¤dt

.
(5)
Pr

yi Ì¸= yâˆ—
c

and Pr

||zi âˆ’câˆ—||2 â‰¤dt

are known constants obtained via the calibration dataset.
Hence, we get the probabilistic guarantee
Ë†p(yt | zt) = 1 âˆ’Pr

yi Ì¸= yâˆ—
c
 ||zi âˆ’câˆ—||2 â‰¤dt

(6)
= 1 âˆ’(1 âˆ’FC(dt)) Â· Pr

yi Ì¸= yâˆ—
c

Pr

||zi âˆ’câˆ—||2 â‰¤dt

.
(7)
Proved.
14
