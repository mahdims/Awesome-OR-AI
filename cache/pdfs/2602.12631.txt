--- Page 1 ---
AI Agents for Inventory Control:
Human-LLM-OR Complementarity
Jackie Baek∗
Yaopeng Fu†
Will Ma‡
Tianyi Peng§
Abstract
Inventory control is a fundamental operations problem in which ordering decisions are tradi-
tionally guided by theoretically grounded operations research (OR) algorithms. However, such
algorithms often rely on rigid modeling assumptions and can perform poorly when demand
distributions shift or relevant contextual information is unavailable. Recent advances in large
language models (LLMs) have generated interest in AI agents that can reason flexibly and in-
corporate rich contextual signals, but it remains unclear how best to incorporate LLM-based
methods into traditional decision-making pipelines.
We study how OR algorithms, LLMs, and humans can interact and complement each other
in a multi-period inventory control setting. We construct InventoryBench, a benchmark of
over 1,000 inventory instances spanning both synthetic and real-world demand data, designed to
stress-test decision rules under demand shifts, seasonality, and uncertain lead times. Through
this benchmark, we find that OR-augmented LLM methods outperform either method in isola-
tion, suggesting that these methods are complementary rather than substitutes.
We further investigate the role of humans through a controlled classroom experiment that
embeds LLM recommendations into a human-in-the-loop decision pipeline. Contrary to prior
findings that human-AI collaboration can degrade performance, we show that, on average,
human-AI teams achieve higher profits than either humans or AI agents operating alone. Beyond
this population-level finding, we formalize an individual-level complementarity effect and derive
a distribution-free lower bound on the fraction of individuals who benefit from AI collaboration;
empirically, we find this fraction to be substantial.
Taken together, our results provide evidence that effective inventory management benefits
from a complementary system in which OR provides heuristics and calculators, LLMs contribute
world knowledge and contextual reasoning, and human judgment adds value beyond automated
decision making.
1
Introduction
Inventory control is the fundamental problem of ordering supply to match demand. To solve this
problem, the field of operations research (OR) has developed a rich toolkit of heuristics and ap-
proximations for handling supply and demand uncertainty, forming the backbone of modern inven-
tory management systems (see Subsection 2.1). These methods—base-stock policies, newsvendor
models, (s, S) rules, and their data-driven variants—are theoretically grounded, computationally
efficient, and widely deployed in practice. However, they rely on fixed modeling assumptions (e.g.,
stationary demand, known lead times) and on historical data that may not reflect the current
∗Stern School of Business, New York University, baek@stern.nyu.edu
†Columbia University, yf2726@columbia.edu
‡Graduate School of Business and Data Science Institute, Columbia University, wm2428@gsb.columbia.edu
§Graduate School of Business and Data Science Institute, Columbia University, tianyi.peng@columbia.edu
1
arXiv:2602.12631v1  [cs.AI]  13 Feb 2026


--- Page 2 ---
environment. When demand distributions shift, contextual information is difficult to encode in a
formal model, or supply conditions change unexpectedly, these heuristics can perform poorly. For
this reason, the core of modern inventory management is not purely algorithmic but human-in-the-
loop (see e.g. Boute and Van Mieghem, 2021): OR tools generate recommendations, and human
experts make the final call, applying contextual judgment to accept, adjust, or override what the
algorithm suggests.
Recent advances in large language models (LLMs) introduce a fundamentally different form of
reasoning into this pipeline.
Unlike traditional OR algorithms, LLMs can incorporate natural-
language context—product descriptions, calendar information, qualitative market signals—and
draw on broad world knowledge to form reasonable judgments even when the formal model is
incomplete or the environment shifts in unexpected ways. At the same time, LLMs are a nascent
technology for operational decision-making, and it remains unclear when they help, how they should
be deployed alongside existing OR heuristics, and how they interact with human decision makers.
This paper studies the complementarity among OR algorithms, LLM-based agents, and humans
in a multi-period inventory control setting. The ways in which these three components can be
combined are diverse: an OR heuristic can feed a structured recommendation to an LLM, which
decides whether to follow or override it; alternatively, an LLM can generate demand forecasts or
contextual estimates that an OR algorithm acts on. When humans enter the loop, further design
choices arise: a human can retain full decision authority and use AI-generated recommendations as
advisory input, or delegate operational decisions to an AI system while providing high-level strategic
guidance at periodic checkpoints. Understanding which methods work best—and why—requires
systematic experimentation across a range of problem environments. We organize our investigation
into two parts.
Part 1: OR × LLM interactions. We begin by understanding the interaction between OR
and LLM without a human in the loop. We consider four methods of combining these tools:
• OR alone: a simple OR pipeline that does data-driven demand estimation followed by a
well-established inventory heuristic;
• LLM alone: the LLM directly makes all inventory decisions;
• OR→LLM: the OR pipeline above is described to an LLM, who treats its output as a
recommendation but can override it in the final decision;
• LLM→OR: an LLM estimates all uncertain parameters such as demand, and these are fed
into the well-established inventory heuristic to make the final decision.
To understand the strengths and weaknesses of each method, we construct InventoryBench, a
benchmark of 1,320 instances (720 based on synthetic data, 600 based on real data) spanning non-
stationary demand patterns (changepoints, trends, seasonality), varying cost structures (leading to
different under- vs. over- stocking tradeoffs), and three lead-time regimes (zero, fixed, and stochas-
tic with lost orders). We evaluate all four methods using three frontier LLMs (Gemini 3 Flash,
Grok 4.1 Fast, GPT-5 Mini). Table 1a summarizes the performance for Gemini 3 Flash.
We find that combining OR and LLM is broadly beneficial: both combination methods (OR→LLM
and LLM→OR) outperform either method in isolation. The OR→LLM pipeline achieves the best
overall performance (0.538), a 21% improvement over OR alone. Drilling into the results, the two
components have complementary strengths. LLMs excel at detecting demand shifts, incorporating
product-specific world knowledge (e.g., seasonality of swimwear), and identifying supply disrup-
tions such as lost orders—capabilities absent from the OR heuristic.1 OR, in turn, provides the
1One could, in principle, augment the OR method’s data-driven estimation with statistical changepoint detection,
trend fitting, or other extensions, but it is difficult to capture all possibilities in one general methodology.
Our
goal here is to let LLMs represent that “one general methodology”, and contrast it with the precision of the simple
data-driven estimation.
2


--- Page 3 ---
Table 1: Summary of normalized reward (mean ± 95% CI). Higher is better.
(a) OR × LLM: 1,320 InventoryBench instances
(Gemini 3 Flash).
Method
Mean
95% CI
OR
0.445
±0.018
LLM
0.494
±0.018
OR →LLM
0.538
±0.016
LLM →OR
0.501
±0.018
(b) Human-in-the-loop: classroom experiment (69
participants, 3 instances).
Setting
Mean
95% CI
Mode A (OR→Human)
0.466
±0.025
Mode B (OR→LLM→Human)
0.534
±0.020
Mode C (OR→LLM + Guidance)
0.464
±0.018
OR (no human)
0.376
—
OR→LLM (no human)
0.482
±0.007
mathematical precision needed for base-stock calculations under stable conditions and long lead
times, whereas LLMs can struggle with inventory pipeline tracking and sometimes falsely identify
demand shifts when it is pure noise. Our results also suggest that LLMs alone are less “calibrated”
to the under- vs. over- stocking tradeoffs implied by different cost structures, echoing the message
of Liu et al. (2025).
Part 2: Human-in-the-loop experiment. Next, we study how humans should interact with
these tools. We build a web-based inventory game and conduct a controlled classroom experiment
with 69 participants. Each participant plays three real-data instances under three collaboration
modes with varying decision authority:
• Mode A (OR→Human): the human treats the output of the OR pipeline as a recommen-
dation, and makes the final decision;
• Mode B (OR→LLM→Human): the human treats the output of the OR→LLM method as
a recommendation, reads the LLM’s reasoning, and then makes the final decision;
• Mode C (OR→LLM + Human Guidance): the LLM decides as in OR→LLM, but the human
can provide strategic guidance every few periods.
Figure 1 shows the decision panels for each mode. Mode–instance assignments are randomized so
that each participant experiences each mode once.
Table 1b summarizes the results. Mode B achieves the best overall performance, significantly
outperforming both Mode A and Mode C. Crucially, human collaboration modes also outper-
form their automated counterparts: Mode A outperforms OR alone and Mode B outperforms
OR→LLM, demonstrating that human judgment adds value beyond what automated methods
alone can achieve. (These differences are all statistically significant according to a pre-registered
experiment, as we describe in Subsection 5.2.)
Beyond this population-level finding, we ask whether individual participants genuinely benefit
from AI collaboration, or whether the aggregate gains simply reflect a selection effect where weaker
participants follow the AI and stronger ones ignore it, without anyone actually exceeding what they
or the AI could achieve alone.
We formalize a notion of individual-level complementarity: whether a person’s collaborative per-
formance exceeds the better of their solo performance and the AI’s solo performance. Because each
participant is observed in only one condition (with or without AI), we cannot directly measure this
for any specific individual. However, we prove a theorem (Theorem 1) that provides a distribution-
free lower bound on the fraction of individuals who experience positive complementarity. The key
insight is that this bound can be estimated by comparing the distribution of collaborative perfor-
mance to the distribution of solo performance across different people, without needing to observe
the same person under both conditions. Applying this bound to our data, we estimate that at least
20.3% of individuals experience strictly positive complementarity.
Overall. Taken together, our results provide a systematic study of the emerging question of
3


--- Page 4 ---
(a) Mode A
(b) Mode B
(c) Mode C
Figure 1: Decision panels for the three collaboration modes. Mode A: the participant sees the OR
recommendation and enters a final order. Mode B: the participant sees the OR-augmented LLM
recommendation and reasoning before entering a final order. Mode C: the AI makes ordering
decisions autonomously; the participant provides optional strategic guidance at scheduled pauses
(every 4 periods).
OR Algorithm
LLM Agent
Human
Structural discipline:
base-stock heuristics,
mathematical
precision
Contextual reasoning:
pattern detection,
world knowledge
Validation
& oversight:
catches LLM failures,
detects anomalies
recommendation
reasoning / guidance
Figure 2: The complementary interaction pattern of OR algorithms, LLM agents, and humans.
how OR algorithms, LLMs, and humans should interact for operational decision-making (Figure 2).
On one hand, LLMs add substantial value to inventory control: they detect demand regime changes,
incorporate world knowledge, and identify supply disruptions—capabilities that are generally dif-
ficult for traditional OR algorithms. On the other hand, the traditional players remain essential:
OR algorithms provide the mathematical precision that LLMs lack for base-stock calculations, and
humans provide situational judgment and serve as a safeguard—our analysis shows that humans
are able to step in precisely when the LLM fails to reason about the situation correctly, e.g. when
they fail to identify lost orders. The finding that humans add value is particularly noteworthy:
achieving genuine human–AI complementarity is widely recognized as difficult, with systematic
evidence showing that human–AI teams frequently fail to outperform the better of human and AI
acting independently (Bansal et al., 2021; Vaccaro et al., 2024; Hemmer et al., 2025). Our results
suggest that inventory control—with its sequential decisions, delayed feedback, and rich contextual
signals—is a setting where human judgment genuinely complements AI.
Open-source benchmark and game. As a by-product, we release two resources. First, In-
ventoryBench2—our benchmark of 1,320 inventory instances, together with a public leaderboard
to track progress—is available for the community to test and compare frontier LLMs, fine-tuning
2InventoryBench is available at https://tianyipeng.github.io/InventoryBench/.
4


--- Page 5 ---
approaches, advanced OR heuristics, and other machine learning methods—we welcome researchers
to try and share their approaches. Second, we open-source the web-based AI-Human Inventory
Game3 used in our experiment (Figure 6), which can serve as a basis to be further developed for
teaching and research in supply chain management.
Roadmap and summary of contributions.
We design LLM agents for inventory control (Sub-
section 3.1), construct a benchmark of 1,320 instances to evaluate them (Subsection 4.1), and build
a web-based game for human–AI interaction (Subsection 5.1).
Findings without humans.
The
OR→LLM pipeline achieves the best overall performance (Subsection 4.2), but disaggregating by
lead time (Subsection 4.3), instance family (Subsection 4.4), and critical fractile (Subsection 4.5)
reveals the complementary strengths of each component: OR excels at handling long determinis-
tic lead times and avoids overfitting to noise, while LLMs excel under stochastic lead times and
at detecting sudden demand shifts and leveraging world knowledge. Findings with humans. Pre-
registered classroom experiments yield clean, statistically significant separations between collabo-
ration modes (Subsection 5.2), demonstrating that both the LLM and the human add value to the
decision pipeline. We develop a theoretical framework for measuring human–AI complementarity
(Subsection 6.1) and apply it to establish individual-level complementarity in our setting (Subsec-
tion 6.2). Finally, we provide illustrative anecdotes of LLM reasoning (Appendix H) and analyze
the mechanisms through which humans improve upon LLM recommendations (Subsection 6.3).
2
Related Work
2.1
Supply Chain and Inventory Management
Inventory can be the first topic mentioned in operations and supply chain management textbooks
(Simchi-Levi et al., 2008; Snyder and Shen, 2019, e.g.), and is also a problem in which the un-
certainty in supply and demand can be used to test the judgment and contextual knowledge of
humans or LLMs. We consider a canonical inventory model in which there are lead times and lost
sales (i.e., unmet demands cannot be recouped), both the norm in practice, and this exact model is
widely deployed at modern enterprises to recommend inventory decisions (Qi et al., 2023; Madeka
et al., 2022; Liu et al., 2023; Xie et al., 2025), noting that it is generally unnecessary to optimize
different items’ inventories jointly. Academically, even optimizing for a single item is notoriously
difficult (Zipkin, 2008a,b), which is why there is a large OR literature on heuristics for this prob-
lem (Bijvank and Vis, 2011; Huh and Janakiraman, 2009; Goldberg et al., 2016). We make our
heuristic of choice the capped base-stock policy (Xin, 2021), a recent development that is simple
and powerful, and considered the state of the art in several papers since (Xin and Goldberg, 2022;
Lyu et al., 2024; Alvo et al., 2023).
We note that some of the aforementioned papers use Reinforcement Learning to solve this
inventory model, often with high-dimensional covariates. We are essentially using the LLM’s con-
textual knowledge in place of these high-dimensional covariates, leaving the integration of LLMs
with Reinforcement learning to future work.
2.2
LLMs for Operational Decisions
There is a growing literature on using LLMs to formulate optimization problems (AhmadiTeshnizi
et al., 2024; Zhou et al., 2025; Huang et al., 2025) or to directly make operational decisions (Long
3AI-Human Inventory Game is available at https://github.com/TianyiPeng/AI-human-inventory-game.git.
5


--- Page 6 ---
et al., 2025; Backlund and Petersson, 2025; Kumar et al., 2025; Fish et al., 2025).
Our paper
lies somewhere in-between, where we are relying on the structure and calculation of simple OR
heuristics, and testing the ability of LLMs to integrate contextual world knowledge into them.
More precisely, our paper differs from the literature by: (i) considering OR heuristics instead of
exact optimization solvers; (ii) exploiting the world knowledge and generality of LLMs to adapt to
changing or anomalous conditions, instead of using them to do the formulation of the optimization
problem. Our punchline that OR+LLM is best echoes what is found in Duan et al. (2025); Baek
et al. (2026), but in very different contexts—Duan et al. (2025) also study inventory but use the
LLM to extract uncertain parameters (e.g., holding cost) instead of make daily decisions, while
Baek et al. (2026) focus on single-period problems and use the LLM only to generate data for a
standard downstream optimization. More broadly, the power of combining OR with LLMs has
been discussed in Dai et al. (2025); Cohen et al. (2025); Hu et al. (2025).
2.3
Human-AI Decision-making for Operations
Importantly, our paper tests how OR+LLM integrates with humans, and to the best of our knowl-
edge, breaks new ground by testing this in a controlled lab experiment. In particular, we compare
Mode A (OR→Human) to Mode B (OR→LLM→Human), which empirically measures the value
add of the LLM when a human makes the final decision, even though this idea has been seen in
enterprise settings long before (Li et al., 2023). The indispensability of humans in operational
pipelines is discussed in Boute and Van Mieghem (2021), and they distinguish between human
control vs. oversight, which is exactly what we test in Modes B vs. C, in an inventory management
problem.
A broader literature shows that achieving genuine human–AI complementarity is difficult. Sur-
veys of empirical studies find mixed results (Lai et al., 2023), a meta-analysis finds that human–AI
teams frequently fail to outperform the better of the two components operating independently
(Vaccaro et al., 2024). Recent works have formalized theoretical models of complementarity to
design better systems or to provide fundamental limits of human-AI performance (Wilder et al.,
2020; Donahue et al., 2022; Peng et al., 2025; Guo et al., 2024, 2025; Hemmer et al., 2025). We add
to this literature by formalizing notions of population and individual-level complementarity effects.
Within operations, a line of work studies how humans interact with algorithms in practice.
Fildes et al. (2009) analyze demand forecasts from four supply-chain companies and find that small
judgmental adjustments to algorithmic forecasts tend to decrease accuracy, while larger adjustments
tend to improve it. Sun et al. (2022) study packing decisions at a warehouse and demonstrate the
value of an algorithm designed to anticipate and incorporate human deviations. A complementary
stream studies how to improve better human–AI systems through modifying how human decisions
are elicited (Ibrahim et al., 2021), providing algorithm transparency (Balakrishnan et al., 2026),
modifying systems loads (Snyder et al., 2026) or modifying the algorithms (McLaughlin and Spiess,
2024; Bastani et al., 2026; Grand-Cl´ement and Pauphilet, 2026).
3
Problem Formulation, Algorithms, and Instances
We study a standard (see Subsection 2.1) multi-period inventory control problem in which a decision
maker manages a single product over a finite horizon of T periods.
State and dynamics.
In each period t = 1, 2, . . . , T, events occur in the following sequence:
6


--- Page 7 ---
1. Observe state and context.
The decision maker knows the on-hand inventory It, the
history of past demands, the history of past orders and the arrival times of the orders that
have arrived, and contextual information xt (a free-form text string that may include static
product descriptions and calendar dates, which give information about upcoming demand).
2. Place an order. The decision maker chooses an order quantity qt ≥0.
3. Receive arrivals. Shipments from earlier orders arrive. Each order placed at time τ ≤t has
a lead time ℓτ ∈{0, 1, 2, . . .}∪{∞}, which is deterministic but unknown to the decision maker
at the time the order is placed. If τ + ℓτ = t, then order qτ arrives and is added to on-hand
inventory; if ℓτ = ∞, the order never arrives (is lost). Let At = P
τ≤t:τ+ℓτ=t qτ denote total
arrivals in period t.
4. Realize demand.
Demand dt is realized and observed (even under a stockout, i.e., we
assume uncensored demands). Sales equal st = min{dt, It + At}, and unsatisfied demand is
lost.
5. Update inventory. Leftover inventory carries over to the next period:
It+1 = It + At −st = max{0, It + At −dt}.
Objective.
The per-period profit equals sales revenue p · st minus holding cost h · It+1, where
p > 0 is the fixed per-unit profit margin and h > 0 is the fixed per-unit per-period holding cost.
The decision maker’s objective is to maximize total profit over the horizon,
T
X
t=1
(p · st −h · It+1) .
An important parameter to highlight is the critical fractile ρ := p/(p + h), which is a ratio in [0, 1]
indicating how desirable it is to overstock (instead of understock).
Initialization.
Each instance begins with five realized demand values {d−4, d−3, d−2, d−1, d0}
provided as historical data. The initial state is I1 = 0 (no on-hand inventory) with no in-transit
orders. The decision maker is informed of the profit margin p, the holding cost h, and an anticipated
lead time L. However, the actual lead times {ℓt}T
t=1 may differ from L and are revealed only as
orders arrive (or fail to arrive).
3.1
LLM Agents
OR heuristic.
We first describe an OR-only baseline, which will be used by some of the LLM
agents. This baseline takes all historical data to build a per-period demand distribution (mean &
standard deviation). It then determines a base-stock level (i.e., inventory “target”) based on this
per-period distribution, the anticipated lead time L, and the critical fractile ρ (which determines
the tradeoff between under- vs. over- stocking). Finally, the order quantity is decided by subtracting
the total in-transit inventory from this base-stock target, and the order quantity is also capped from
above to avoid unstable inventory flow. All in all, we end up with a data-driven capped base-stock
policy (Xin, 2021), which is considered a simple and powerful heuristic (see Subsection 2.1), that
is fully explained in Appendix A.
7


--- Page 8 ---
Role & objective: maximize P
t(p · st −h · It+1)
Game mechanics: period sequence, ob-
servation delay, lead-time definition
OR baseline math & limitations
(stationary demand, promised
lead time, no lost-order detection)
Decision checklist & output format
System prompt (fixed at initialization)
Carry-over insights from prior periods
Observation: It, in-transit inventory,
demand history, conclude msg
OR recommendation: Bt, IPt, ¯d, sd, cap
User message (changes each period t)
LLM (single call per period)
Order quantity qt + rationale
Updated carry-over insights
next period
Figure 3: Architecture of the OR→LLM agent. Each period involves a single LLM call with a
fixed system prompt and a period-specific user message, where the “user” here need not involve
a human. The agent outputs an order quantity with rationale and optionally updates carry-over
insights, which persist to the next period’s input.
LLM agent setup.
Next, we design LLM agents that interact with the inventory environment as
sequential decision-makers. Figure 3 illustrates the OR→LLM agent architecture, which has three
components: a structured system prompt, a stateless per-period decision loop, and a carry-over
insight mechanism for cross-period memory.
System prompt.
Each agent receives a fixed system prompt describing the game mechanics
(period execution sequence, observation delays, lead-time inference rules), the OR baseline’s math-
ematical formulation, a four-step decision checklist, and the required JSON output format. In the
OR→LLM mode, the prompt also explains the OR algorithm’s assumptions—stationary demand,
promised lead time, no lost-order detection—so the LLM knows when to override. The complete
system prompts are provided in Appendix B.
Per-period decision loop.
Each period t involves a single, stateless LLM call. The user message
contains: (1) carry-over insights from prior periods, (2) the current observation (It, in-transit
inventory, demand history, period-conclusion messages), and (3) the OR recommendation with
statistics. The agent returns a JSON response with the order quantity, a step-by-step rationale, a
short human-readable summary, and an optional carry-over insight update.
Carry-over insights.
To provide cross-period memory without maintaining a full conversation
history over the T-period horizon, the agent can record concise memos about sustained discoveries
(e.g., “demand shifted from ∼100 to ∼200” or “lead time is 3 periods, not the promised 1”).
These are prepended to the next period’s observation and can be updated or removed as conditions
change.
8


--- Page 9 ---
Approaches.
In Part 1 (Section 4), we evaluate four methods: OR (data-driven capped base-
stock policy), LLM (agent alone, no OR input), OR→LLM (agent receives and may override OR
recommendation), and LLM→OR (inputs to capped base-stock policy are determined by LLM
instead of directly by data; see Appendix C). All LLM-based methods share the same prompt
structure shown in Figure 3, adapted to their role: the LLM-only variant omits the OR baseline
sections, while LLM→OR modifies the output format to return parameter estimates instead of order
quantities. The human collaboration modes in Part 2 (Section 5.1) build on the same OR→LLM
pipeline, adding a human who either makes the final decision (Mode B) or provides strategic
guidance (Mode C).
4
Part 1: Algorithmic Experiment
4.1
InventoryBench Instances
An instance provides one complete specification of the inventory control problem, defining: a fi-
nite horizon T; fixed realizations of all demands {dt}T
t=1 and lead times {ℓt}T
t=1; natural-language
contextual information {xt}T
t=1; parameters (p, h); and the initial data {d−4, . . . , d0}. All random-
ness is resolved in advance to ensure full reproducibility and enable controlled comparisons across
decision-making methods.
We construct two families of instances:
• Synthetic instances (720 total): designed from 10 parametric demand pattern families
including stationary processes, abrupt mean shifts, gradual trends, seasonal patterns, etc.
Tests a method’s ability to react to demand shifts and different patterns, which may be ad
hoc. See Appendix D for details.
• Real instances (600 total): derived from historical sales data in the H&M Personalized
Fashion Recommendations dataset (Ling et al., 2022), using 200 distinct products across di-
verse categories (swimwear, knitwear, tailored clothing, basics, accessories). Tests a method’s
ability to react to demand shifts and patterns that may be explainable by world knowledge,
e.g. holidays and seasons. See Appendix E for details.
Each instance family is crossed with three lead-time configurations:
• L = 0: immediate delivery (orders arrive in the same period they are placed)
• L = 4: fixed 4-period delay
• Stochastic lead times: each order independently has realized lead time ℓt ∈{1, 2, 3, ∞} with
equal probability, where ℓt = ∞indicates a lost order
Orthogonally, each instance family is crossed with three possible (p, h) values defining different
critical fractiles: ρ ∈{0.50, 0.80, 0.95}, representing different degrees of preference for overstocking.
4.2
Overall results
All results in the main body are reported for Gemini 3 Flash; analogous figures and tables for Grok
4.1 Fast and GPT-5 Mini appear in Appendix F, and detailed performance breakdowns for all three
models appear in Appendix G.
9


--- Page 10 ---
Figure 4: Gemini 3 Flash: overall normalized reward (mean ± 95% CI) across all 1,320 instances, by
method. OR→LLM achieves the highest mean (0.538), with the other hybrid method (LLM→OR)
coming second.
Performance metric.
We refer to the total profit PT
t=1(p · st −h · It+1) of a method on an
instance as its reward. To enable comparison across instances with different demand scales and
parameters, we define
normalized reward := max
(
reward
p · PT
t=1 dt
, 0
)
.
The denominator p · PT
t=1 dt is the revenue obtained by capturing all demand while never holding
any leftover inventory—a very optimistic upper bound on optimal profit. Clipping at zero prevents
instances with large losses from producing extreme negative ratios (e.g., −100) that would dominate
the average. For any set of instances, we report the mean normalized reward together with a 95%
confidence interval for the mean.
Figure 4 summarizes performance across all 1,320 instances, pooling real and synthetic instances.
4.3
Disaggregation by Lead Time
Figure 5 disaggregates performance by lead time, unveiling when different methods shine.
Under deterministic lead times (L = 0 and L = 4), the primary value of the LLM lies in
predicting demand shifts—using common-sense reasoning to detect ad hoc patterns in synthetic
instances and world knowledge (e.g., seasonality) in real instances—while the final ordering decision
is best left to the OR heuristic, which can translate these forecasts into precise capped base-
stock calculations. This is why LLM→OR is the strongest method under both settings. It really
dominates when L = 0, under which the OR heuristic is essentially perfect given a perfect prediction,
so it makes sense that the LLM focuses fully on prediction, as in the LLM→OR method. L = 4 is
similar, but highlights more the downside of not having the structured calculations of the capped
base-stock OR heuristic, where the LLM alone really lags behind.
The script flips under stochastic lead times. The capped base-stock heuristic is not designed
for this out-of-distribution setting: it assumes a known, fixed lead time and has no mechanism to
detect or adapt to lost orders. Methods in which OR computes the final order suffer accordingly
10


--- Page 11 ---
Figure 5: Gemini 3 Flash: normalized reward by lead time setting (440 instances per setting).
11


--- Page 12 ---
Table 2: Gemini 3 Flash: normalized reward by synthetic demand pattern (See Appendix D for
definitions), each corresponding to 48 instances (8 instances for each of 3 critical fractiles and 2
lead times, with stochastic lead time excluded). The All Synthetic column averages over all
480 instances while the Real column averages over 400 real instances (again excluding stochastic
lead time). Cell shading per column:
green = best, red = worst. Best method per column in
bold.
Stationary IID
Mean ↑
Mean ↓
Trend ↑
Trend ↓
Variance Change
Seasonal
Multi Changepoint
Spike/Dip
Autocorr.
All Synthetic
Real
(p01)
(p02)
(p03)
(p04)
(p05)
(p06)
(p07)
(p08)
(p09)
(p10)
OR
.802
.774
.557
.626
.521
.706
.676
.700
.651
.756
.677
.486
LLM
.653
.703
.567
.749
.597
.583
.585
.628
.595
.637
.630
.488
OR→LLM
.771
.789
.611
.761
.600
.666
.644
.699
.644
.708
.689
.525
LLM→OR
.780
.800
.656
.791
.657
.660
.661
.699
.668
.717
.709
.537
(OR: 0.154, LLM→OR: 0.218). By contrast, methods in which the LLM makes the final ordering
decision prove far more robust (LLM: 0.353, OR→LLM: 0.385), because general-purpose LLMs can
reason about unexpected events—such as recognizing that an order has been lost—and adjust their
behavior accordingly (see Example 1 for a reasoning anecdote).
4.4
Disaggregation by Instance Family
Table 2 disaggregates performance by synthetic demand pattern, restricting to deterministic lead
times (L ∈{0, 4}) so that differences reflect demand-forecasting ability rather than the lead time
effects analyzed in Subsection 4.3. LLM→OR achieves the highest overall synthetic reward (.709)
and the highest real-instance reward (.537), consistent with the aggregate findings above.
However, OR alone is the best method for several synthetic families: Stationary IID (.802),
where the OR heuristic’s demand model is correctly specified, and the LLM may overfit to false
patterns (see Example 2). OR alone is also best for Variance Change (.706), Seasonal (.676), Multi
Change-Point (.700), and Autocorrelated (.756), where these patterns are difficult for an LLM to
anticipate.
The LLM-based methods excel precisely where changes are directional and describable: Mean
Shift Up (.800 for LLM→OR vs. .774 for OR), Mean Shift Down (.656 vs. .557), Trend Up (.791
vs. .626), and Trend Down (.657 vs. .521). These families have the largest absolute gaps in the
table, with LLM→OR improving over OR by 3–17 percentage points. This pattern is intuitive: an
upward or downward shift in mean or trend is exactly the kind of structural break that an LLM
can detect from context and translate into an updated demand forecast (see Example 3).
On real H&M instances, all LLM-involving methods outperform OR (.486), with LLM→OR
reaching .537. Indeed, the LLMs leverage world knowledge to improve demand predictions beyond
what the simple statistical OR heuristic can achieve (see Example 4).
12


--- Page 13 ---
Table 3: Gemini 3 Flash: implicit critical fractiles, averaged across the instances with a given value
of ρ.
Method
ρ = 0.50
ρ = 0.80
ρ = 0.95
Range
OR
0.470
0.527
0.597
0.127
LLM
0.736
0.752
0.764
0.028
OR→LLM
0.672
0.712
0.741
0.069
LLM→OR
0.586
0.643
0.691
0.105
4.5
Calibration to Critical Fractiles
Finally, we test whether the LLM is able to change its under- vs. over- stocking tendencies depending
on the critical fractile ρ for the instance at hand. To do so, we define each method’s implicit critical
fractile as the fraction of periods in which it has excess inventory (as opposed to stocking out). A
higher implicit critical fractile implies a greater tendency to overstock, which should be desired for
higher values of ρ if the decision-making method is “calibrated”.
We display the average implicit critical fractiles for the four methods, disaggregating instances
by ρ. Table 3 reveals a striking spectrum. The four methods can be ordered by how much LLM in-
fluence they contain—OR (none), LLM→OR (LLM provides a forecast, OR makes the final order),
OR→LLM (OR provides a suggestion, LLM makes the final order), LLM (full LLM control)—and
along this spectrum, responsiveness to ρ monotonically decreases. OR’s implicit critical fractile
rises from 0.470 to 0.597 as ρ increases from 0.50 to 0.95 (range = 0.127); LLM→OR’s rises from
0.586 to 0.691 (range = 0.105); OR→LLM’s from 0.672 to 0.741 (range = 0.069); and LLM’s from
0.736 to 0.764 (range = 0.028). Every method responds positively to increases in ρ, but the response
attenuates as the final decision moves further from the OR heuristic. We note that due to the lead
times, it is impossible for the implicit critical fractile to match ρ exactly, so we are just measuring
the range to rank the methods.
5
Part 2: Human-in-the-loop Experiment and Inventory Game
In practice, human decision makers typically interact with OR algorithms while retaining final
decision authority. As LLMs enter the loop, a natural question arises: how should these inter-
actions be designed? Should humans provide high-level strategic guidance, or continue to make
low-level operational decisions as before? More fundamentally, can human–AI collaboration gener-
ate performance gains that neither humans nor AI can achieve on their own—so that the two act
as complements rather than substitutes?
We describe the experimental design and present the results, followed by a rigorous analysis of
human-AI complementarity in Section 6.1.
5.1
Game and Experiment Setup
We conduct a classroom experiment in which human participants play the inventory control problem
from Section 3 under the three human-AI collaboration modes introduced in Section 1 (Figure 1).
The web-based game interface (Figure 6) displays all information needed for each ordering
decision: the product description, current inventory state (on-hand, in-transit, cumulative reward),
parameters (p, h, L), recent demand, and interactive charts of historical demand and inventory
status.
13


--- Page 14 ---
Figure 6: Decision panel of the inventory game. Each period, the participant sees the product
description, current inventory state (on-hand, in-transit, cumulative reward), parameters, and the
most recent demand, along with AI recommendations that vary by collaboration mode. See also
Figure 12 in the appendix for the analytics panel.
Collaboration modes (treatments).
The three modes vary how decision authority is shared:
1. Mode A: OR →Human (Figure 1a). The participant sees the OR baseline recommendation
and makes the final order decision. This represents the status quo where humans receive OR
recommendations and retain full authority.
2. Mode B: OR →LLM →Human (Figure 1b). The participant sees the OR recommen-
dation, the LLM recommendation (from the same OR→LLM pipeline as Part 1), and the
LLM’s written rationale, then makes the final decision.
3. Mode C: OR+LLM →Human guidance (Figure 1c). The LLM agent autonomously
makes ordering decisions each period.
The participant shifts to a strategic advisor role,
providing optional free-form guidance at scheduled pauses (every four periods), which the
LLM incorporates into subsequent decisions.
Experiment procedure.
Each participant plays three game instances (Appendix I), with one
collaboration mode randomly assigned per instance so that each participant experiences each mode
exactly once. The complete decision trajectory—every order quantity, timestamp, and any text
guidance—is recorded. There were 69 participants and 187 subject–instance observations in to-
tal.4
The game was voluntary with no identification saved or grade impact; hypotheses were
pre-registered prior to the experiment.5
4Some participants did not complete every instance due to time constraints or other factors.
5See https://aspredicted.org/gs5tm3.pdf.
14


--- Page 15 ---
5.2
Results
We first present the overall performance of five decision-making methods: three human collabo-
ration modes (Modes A, B, and C) and two automated benchmarks (OR and OR→LLM). We
consider normalized rewards as defined in Subsection 4.2. Table 1b from Section 1 reports the nor-
malized reward averaged across the three instances for each mode. The results show that Mode B
achieves the best performance overall, outperforming both the other human collaboration modes
and the automated methods. Moreover, the human collaboration modes outperform their non-
human counterparts: Mode A outperforms OR, and Mode B outperforms OR→LLM. We now test
whether these findings hold more rigorously using regression analysis.
5.2.1
Comparing human collaboration modes
We run the following pooled OLS specification with subject fixed effects and instance fixed effects:
Yij = αi + βj + τc(i,j) + εij,
where Yij is the normalized reward obtained by subject i on instance j, αi captures subject-level
heterogeneity, βj captures instance difficulty, and τc(i,j) is the treatment effect for the decision-
support mode used in that subject-instance pair. Standard errors are clustered at the subject level
to account for within-subject correlation across instances. The omitted (baseline) category is Mode
A on Instance 1.
Mode B > Mode A
Mode B yields a statistically significant improvement over Mode A, with an
estimated treatment effect bτB = 0.0675 (SE = 0.0185, one-sided p = 0.00025), which is positive and
significant. While the previous section showed that LLMs add value on top of OR algorithms, this
result shows that adding LLM-based decision support on top of a traditional OR recommendation
improves performance even when a human remains the final decision maker. Since Mode A closely
mirrors the status quo in many operational settings (OR provides a prescriptive recommendation
and a human executes the final action), this finding suggests that incorporating an LLM layer can
improve performance without removing humans from the decision process.
Mode B > Mode C
Next, we compare the two modes with LLM support, Mode B and Mode C.
We find a statistically significant difference between the two modes, with an estimated treatment
effect bτC −bτB = −0.0722 (SE = 0.0177); this difference is significant (two-sided p = 0.00012). This
result indicates that where the LLM is placed in the pipeline matters. In our experiment, LLMs
appear to be most effective as a decision-support tool that the human can selectively follow, adjust,
or override, rather than as the primary controller with only occasional human steering. At the
same time, we acknowledge that this comparison is technology- and interface-dependent: stronger
models, better mechanisms for translating human intent into concrete actions, or more frequent
and structured guidance channels could narrow or potentially reverse the gap between Modes C
and B.
5.2.2
Comparing to no-human modes.
Next, we compare the human collaboration modes to the algorithm-only methods from Section 4.
These comparisons isolate the incremental value of adding a human at the final step, holding
the underlying method fixed: we compare Mode A to OR, and Mode B to OR→LLM. For both
methods, we evaluate the same three instances used in the human experiment. We run OR→LLM
15


--- Page 16 ---
100 times per instance (to average over stochasticity in the LLM) and OR once per instance (since
it is deterministic).
Human-in-the-loop adds value.
We estimate regressions with instance fixed effects and an
indicator for the human mode (Mode A or Mode B). Standard errors are cluster-robust, clustering
human observations by subject and automated observations by run (see Appendix J for details).
We conduct one-sided tests for improvement and find that the human modes outperform their non-
human counterparts: Mode A > OR (p = 0.0098) and Mode B > OR→LLM (p = 0.000009).
These results indicate that human judgment adds meaningful value beyond what can be achieved
by automated decision-making alone, whether based on traditional OR methods or enhanced with
LLM capabilities.
6
Human-AI Complementarity
The previous section established that human collaboration with LLM support (Mode B) outper-
forms both traditional human decision-making with OR alone (Mode A) and fully automated LLM-
based decisions (OR→LLM). A key question arises: does collaboration create genuine synergy, or
does it merely allow weaker decision-makers to adopt AI recommendations while stronger ones
ignore them? In this section, we formalize this complementarity by developing metrics that distin-
guish between population-level and individual-level effects, and we apply them to our experiment
results.
6.1
Theoretical Framework
Setup and notation.
Consider a decision-making task parameterized by a scenario z, which
encodes the task context and exogenous randomness (e.g., demand realizations and lead-time out-
comes). For a given person i and scenario z, let:
• f(i)
H (z): the performance of person i acting alone on scenario z,
• fAI(z): the performance of the AI acting alone on scenario z,6
• f(i)
H+AI(z): the performance of person i collaborating with AI on scenario z.
We write fH(z) = Ei[f(i)
H (z)] and fH+AI(z) = Ei[f(i)
H+AI(z)] for the population averages over per-
sons.
Population-level complementarity.
Following Hemmer et al. (2025), the population-level com-
plementarity effect is defined as
CE = Ez

fH+AI(z) −max
 fH(z), fAI(z)

.
(1)
When CE > 0, the average human-AI team outperforms both the average human alone and the AI
alone across scenarios.
6We interpret fAI(z) as a deterministic quantity that represents the expected AI performance on scenario z (even
though LLM outputs can be stochastic, its realization is not known ex-ante).
16


--- Page 17 ---
Individual-level complementarity.
We are also interested in whether complementarity arises
at the individual level. Define the individual-level complementarity effect for individual i as
CEi = Ez

f(i)
H+AI(z) −max
 f(i)
H (z), fAI(z)

.
(2)
A positive CEi means that person i genuinely benefits from interacting with the AI: their collab-
orative performance exceeds both what they could achieve alone and what the AI could achieve
alone.
Population-level complementarity does not imply individual-level complementarity.
Importantly, CE > 0 does not imply the existence of any individual i with CEi > 0. To see this,
consider a population in which weak decision-makers (those with f(i)
H (z) < fAI(z)) simply adopt
the AI’s recommendation, achieving f(i)
H+AI(z) = fAI(z), while strong decision-makers (those with
f(i)
H (z) > fAI(z)) ignore the AI entirely, achieving f(i)
H+AI(z) = f(i)
H (z). In this case, f(i)
H+AI(z) =
max(f(i)
H (z), fAI(z)) for every person, so CEi = 0 for all i. Yet the population average fH+AI(z) can
still exceed max(fH(z), fAI(z)), because the average of maxima exceeds the maximum of averages.
We argue that this pattern—where complementarity arises purely from selection rather than from
genuine interaction—does not reflect true human-AI synergy.
The missing-data challenge.
Measuring CEi directly requires observing both f(i)
H (z) and f(i)
H+AI(z)
for the same person i on the same scenario z, which is a fundamental missing-data problem: each
person is observed in only one condition.7 Nevertheless, we can still estimate meaningful statistics
about the distribution of CEi across the population.
Statistical framework.
For a person i drawn from the population, define
a(i) = Ez

f(i)
H+AI(z)

,
a ∼P,
b(i) = Ez

max
 f(i)
H (z), fAI(z)

,
b ∼Q,
where P and Q denote the population distributions of a and b, respectively. We assume that P and
Q can be estimated reliably by sampling individuals from the population, observing each person in
one condition. Note that CEi = a(i) −b(i).
Average individual-level complementarity. The average individual-level complementarity
effect is
Ei[CEi] = E[a] −E[b],
(3)
which can be estimated directly from the sample means of the two groups.
Lower bound on the probability of positive complementarity. Although we cannot
identify CEi for any specific individual, we can provide a distribution-free lower bound on the
fraction of individuals who benefit from human-AI collaboration.
Theorem 1. Let FP and FQ denote the CDFs of P and Q, respectively. Then for any δ ≥0,
P

CEi > δ

≥sup
t∈R
 FQ(t) −FP (t + δ)

.
7One approach in the literature is to let the human decide first, reveal the AI’s recommendation, and ask whether
the human would revise their decision. However, this protocol does not apply to settings with sequential, multi-step
decisions, where earlier choices affect later states.
17


--- Page 18 ---
Moreover, this bound is tight: for any marginal distributions P and Q, there exists a joint distri-
bution of (a, b) with these marginals that achieves equality.
Proof sketch. The lower bound follows from two observations. First, for any threshold t, the event
{a(i) > t+δ}∩{b(i) ≤t} implies CEi > δ. Second, the Fr´echet inequality gives P[a(i) > t+δ, b(i) ≤
t] ≥(1 −FP (t + δ)) + FQ(t) −1 = FQ(t) −FP (t + δ). Optimizing over t yields the bound. For
tightness, one constructs a joint distribution of (a, b) that achieves equality by coupling the tails of
P and Q at the optimal threshold t∗. See Appendix K for the full proof.
Corollary 1. Setting δ = 0 in Theorem 1 yields P

CEi > 0

≥supt∈R
 FQ(t) −FP (t)

.
The bound in Corollary 1 has an intuitive graphical interpretation: P[CEi > 0] is at least as
large as the maximum vertical gap where FQ lies above FP —precisely the one-sided Kolmogorov–
Smirnov statistic between the two distributions. This can be estimated from finite samples using
the empirical CDFs of a and b.
6.2
Evaluating Complementarity
We apply the framework of the previous section to our experiment results. We interpret “AI” as
“LLM” in our experiments, and hence we use the following mapping:8
• Human-only (H): Mode A (OR →Human)
• AI-only (AI): OR →LLM
• Human-AI team (H+AI): Mode B (OR →LLM →Human)
6.2.1
Population-level complementarity.
We evaluate the population-level complementarity effect defined in Section 6.1. In our experiment,
each run corresponds to one of three scenarios z, one per instance, where the demand realizations
and inventory arrivals are held fixed across decision-making methods. We plug in estimates of the
population means fH(z) = Ei[f(i)
H (z)] and fH+AI(z) = Ei[f(i)
H+AI(z)] using sample averages over
the participants for each z. Averaging over the three scenarios gives Ez[ ˆfH+AI(z)] = 0.5338 and
Ez[max{ ˆfH(z), fAI(z)}] = 0.4842, so
c
CE = 0.0496.
To test whether this effect is significantly positive, we bootstrap over participants9 and obtain a one-
sided p-value of 0.00010. Thus, our results exhibit population-level human–AI complementarity:
averaged across scenarios, the human–AI team outperforms the better of the human-only and AI-
only benchmarks. In relative terms, our estimate corresponds to a 0.0496/0.4842 ≈10.2% gain
over the best non-collaborative baseline.
8We treat Mode A (OR→Human) as the “human alone” baseline, since it represents the classic status quo in
which a human decision maker receives OR recommendations without LLM involvement. Any mode involving an
LLM is considered AI collaboration.
9The bootstrap is run with 10,000 replicates, resampled within each mode-instance cell.
18


--- Page 19 ---
6.2.2
Individual-level complementarity.
As discussed in Section 6.1, population-level complementarity does not, by itself, imply that any
given individual benefits from collaborating with the AI. In principle, a positive CE could arise
purely from a selection effect: lower-performing participants follow the LLM’s recommendation
and match the AI-only benchmark, while higher-performing participants ignore it and match their
own human-only baseline. Under this explanation, no individual would achieve collaborative per-
formance that exceeds the better of their standalone performance and the AI’s performance.
We first estimate the average individual-level complementarity effect, Ei[CEi], using (3). The
first term, E[a], represents the average performance of the Human-AI team over the three instances.
Next, for each individual i and scenario z, the quantity max
 f(i)
H (z), fAI(z)

represents the best
performance attainable by a simple ex-ante choice between human-only and AI-only for that indi-
vidual. Averaging max{f(i)
H (z), fAI(z)} over individuals and scenarios yields an estimate of E[b].
We plug in the empirical estimates for E[a] and E[b], which yields
ˆEi[CEi] = 0.0192.
To test whether this effect is significantly positive, we bootstrap over participants and obtain
a one-sided p-value of 0.0643.
This provides suggestive evidence that individuals benefit from
collaborating with the AI on average, although the statistical significance is marginal.
Fraction who benefit from human-AI collaboration.
We apply the lower bound from Corol-
lary 1 to estimate the probability that a randomly drawn individual experiences strictly positive
complementarity, Pr(CEi > 0). Since each participant is observed in only one collaboration mode
per instance, we cannot compute CEi = a(i) −b(i) for any individual directly. However, we can
estimate the marginal distributions P and Q of a and b from the two groups of participants: those
assigned to Mode B on a given instance contribute observations of a, and those assigned to Mode A
contribute observations from which we construct b (by taking the maximum of their Mode A perfor-
mance and the fixed OR→LLM benchmark for that instance). Using these empirical distributions,
we find that
sup
t∈R
  ˆFQ(t) −ˆFP (t)

= 0.203.
The 95% confidence interval for this estimate, constructed using bootstrap, is between 0.147 and
0.329. This finding implies that at least 20.3% of individuals in the population are estimated to
experience strictly positive complementarity: their collaborative performance exceeds both what
they would achieve with OR support alone and what the LLM would achieve without them. By
Theorem 1, this bound is tight: there exists a joint distribution consistent with the observed
marginals for which exactly 20.3% of individuals benefit from collaboration.
This provides evidence that human–AI complementarity in our setting is not purely a population-
level artifact driven by selection. A nontrivial fraction of participants neither simply follow the
LLM’s recommendation nor ignore it, but use the LLM’s reasoning to make adjustments that im-
prove upon both the human-only and AI-only benchmarks. We note that the 20.3% lower bound
is conservative by construction; it represents the minimum fraction of beneficiaries consistent with
the observed marginal distributions, and the true fraction may be substantially higher.
6.3
How do Humans Add Value?
The preceding analysis established that human decision makers improve upon AI recommendations
(Section 5.2.1) and exhibit true complementarity with LLMs (Section 6.2). We now ask: what
specific capabilities enable humans to add value beyond what the LLM achieves alone?
19


--- Page 20 ---
100
50
0
50
100
Average Deviation
Instance 1 (Swimwear bottom, lead time = 0)
Average deviation
Demand
100
0
100
200
Average Deviation
Instance 2 (Blazer, lead time = 1)
Average deviation
Demand
03-18
04-01
04-15
04-29
05-13
05-27
06-10
06-24
07-08
07-22
08-05
08-19
09-02
09-16
09-30
10-14
10-28
11-11
11-25
12-09
12-23
Period
0
500
1000
Average Deviation
Instance 3 (Trousers, lead time = 1 with lost orders)
Average deviation
Demand
0
250
500
750
1000
1250
Demand
200
400
600
800
Demand
1000
2000
3000
Demand
Figure 7: Average deviation of the participant’s final decision from the LLM recommendation in
Mode B, by instance and period. The black line shows realized demand for each period.
Humans and LLMs as co-pilots.
Our evidence suggests that humans and LLMs function as
co-pilots: they add value through the same type of reasoning.
Recall from Part 1 that LLMs
improve upon OR algorithms through contextual, judgment-based capabilities—detecting demand
regime changes, incorporating product seasonality knowledge, and identifying supply disruptions.
Our experiments reveal that humans improve upon LLMs through the same channels. Rather than
contributing orthogonal skills, humans and LLMs are imperfectly correlated reasoners on a shared
contextual dimension, which generates complementarity.
We support this claim using two complementary sources of evidence. First, we study when
humans override the LLM in Mode B by examining deviations between the participant’s final
order and the LLM recommendation (Figure 7). Second, we analyze the strategic guidance that
participants provided in Mode C (Table 12 in Appendix H), where the LLM makes autonomous
ordering decisions but humans provide high-level guidance every 4 periods. Despite coming from
different collaboration modes, both sources of evidence converge on the same mechanisms that we
describe below.
20


--- Page 21 ---
Reacting to demand trends and shocks.
Across instances, we can see from Figure 7 that
deviations often spike immediately after demand spikes, suggesting that participants intervene when
recent demand realizations contain information not fully captured by the LLM’s reasoning. The
first column of Table 12 (in Appendix H) shows guidance messages in which participants explicitly
direct the LLM’s attention to recent demand changes.
Detecting anomalies: lost orders.
Instance 3 exhibits sustained positive deviations in later
periods. In this instance, the lead-time configuration was particularly challenging: orders would
sometimes never arrive. These “lost orders” were still counted as inventory-in-transit, which the
OR algorithm factored into its order recommendation. Both the LLM and the human can poten-
tially detect such lost orders and write them off, increasing the order quantity relative to the OR
recommendation. However, the LLM does not always detect them and instead follows the OR rec-
ommendation, resulting in significant positive human deviations in the later periods of Instance 3.
Seasonal and world-knowledge reasoning.
Instance 2 (blazer) features a demand pattern
shaped by fashion seasonality: lower demand in summer months and higher demand heading into
fall. Fig. 7 shows that human deviations are negative during summer and positive in later periods
as demand rises toward fall. The second column of Table 12 (in Appendix H) corroborates this
with guidance messages that explicitly reference seasonality.
Summary.
Across all three instances, the pattern is consistent: humans add value through con-
textual, judgment-based reasoning—the same type of capability that makes LLMs valuable over
OR. The co-pilot structure works because the two agents’ contextual reasoning is correlated but
not identical. Neither alone captures the full picture, but together they cover each other’s blind
spots.
Acknowledgements
We thank Omar Mouchtaki for pointing us to the H&M dataset. We also acknowledge support
from the Columbia AI Agents Initiative at Columbia Business School.
References
A. AhmadiTeshnizi, W. Gao, and M. Udell. Optimus: Scalable optimization modeling with (mi)
lp solvers and large language models. arXiv preprint arXiv:2402.10172, 2024.
G. Allon and J. A. Van Mieghem. The mexico-china sourcing game: Teaching global dual sourcing.
INFORMS Transactions on Education, 10(3):105–112, 2010. doi: 10.1287/ited.1100.0045.
M. Alvo, D. Russo, and Y. Kanoria. Neural inventory control in networks via hindsight differentiable
policy optimization. arXiv preprint arXiv:2306.11246, 2023.
A. Backlund and L. Petersson.
Vending-bench: A benchmark for long-term coherence of au-
tonomous agents, Feb. 2025. URL https://arxiv.org/abs/2502.15840.
J. Baek, Y. Chen, Z. Chi, and W. Ma. Evaluating llm-persona generated distributions for decision-
making, 2026. URL https://arxiv.org/abs/2602.06357.
21


--- Page 22 ---
M. Balakrishnan, K. J. Ferreira, and J. Tong. Human-algorithm collaboration with private infor-
mation: Na¨ıve advice-weighting behavior and mitigation. Management Science, 72(1):265–284,
2026.
G. Bansal, T. Wu, J. Zhou, R. Fok, B. Nushi, E. Kamar, M. T. Ribeiro, and D. Weld. Does the
whole exceed its parts? the effect of ai explanations on complementary team performance. In
Proceedings of the 2021 CHI conference on human factors in computing systems, pages 1–16,
2021.
H. Bastani, O. Bastani, and W. P. Sinchaisri. Improving human sequential decision making with
reinforcement learning. Management Science, 72(1):733–755, 2026.
M. Bijvank and I. F. A. Vis. Lost-sales inventory theory: A review. European Journal of Operational
Research, 215(1):1–13, 2011. doi: 10.1016/j.ejor.2011.02.004.
R. N. Boute and J. A. Van Mieghem. Digital operations: Autonomous automation and the smart
execution of work. MIT Sloan Management Review, 62(2):1–10, 2021.
M. C. Cohen, T. Dai, G. Perakis, N. Agrawal, G. Allon, R. N. Boute, G. P. Cachon, Z. Chen, M. A.
Cohen, R. Cristian, et al. Supply chain management in the ai era: A vision statement from the
operations management community. 2025.
T. Dai, D. Simchi-Levi, M. X. Wu, and Y. Xie. Assured autonomy: How operations research powers
and orchestrates generative ai systems. arXiv preprint arXiv:2512.23978, 2025.
K. Donahue, A. Chouldechova, and K. Kenthapadi. Human-algorithm collaboration: Achieving
complementarity and avoiding unfairness. In Proceedings of the 2022 ACM Conference on Fair-
ness, Accountability, and Transparency, pages 1639–1656, 2022.
Y. Duan, Y. Hu, and J. Jiang. Ask, clarify, optimize: Human-llm agent collaboration for smarter
inventory control. arXiv preprint arXiv:2601.00121, 2025.
R. Fildes, P. Goodwin, M. Lawrence, and K. Nikolopoulos. Effective forecasting and judgmental
adjustments: an empirical evaluation and strategies for improvement in supply-chain planning.
International journal of forecasting, 25(1):3–23, 2009.
S. Fish, J. Shephard, M. Li, R. I. Shorrer, and Y. A. Gonczarowski. Econevals: Benchmarks and
litmus tests for llm agents in unknown environments. In 2nd Workshop on Models of Human
Feedback for AI Alignment, 2025.
D. A. Goldberg, D. A. Katz-Rogozhnikov, Y. Lu, M. Sharma, and M. S. Squillante. Asymptotic
optimality of constant-order policies for lost sales inventory models with large lead times. Math-
ematics of Operations Research, 41(3):898–913, 2016. doi: 10.1287/moor.2015.0754.
J. Grand-Cl´ement and J. Pauphilet. The best decisions are not the best advice: Making adherence-
aware recommendations. Management Science, 72(1):667–692, 2026.
Z. Guo, Y. Wu, J. D. Hartline, and J. Hullman. A decision theoretic framework for measuring ai
reliance. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Trans-
parency, pages 221–236, 2024.
Z. Guo, Y. Wu, J. Hartline, and J. Hullman. The value of information in human-ai decision-making.
arXiv preprint arXiv:2502.06152, 2025.
22


--- Page 23 ---
P. Hemmer, M. Schemmer, N. K¨uhl, M. V¨ossing, and G. Satzger. Complementarity in human-ai
collaboration: Concept, sources, and evidence. European Journal of Information Systems, 34(6):
979–1002, 2025.
M. Hu, X. Liu, and G. Shen. Ai as flesh, om as bone: Semiparametric regression with deep neural
networks. OM as Bone: Semiparametric Regression with Deep Neural Networks (September 08,
2025), 2025.
C. Huang, Z. Tang, S. Hu, R. Jiang, X. Zheng, D. Ge, B. Wang, and Z. Wang. Orlm: A customizable
framework in training large models for automated optimization modeling. Operations Research,
2025.
W. T. Huh and G. Janakiraman. Asymptotic optimality of order-up-to policies in lost sales inven-
tory systems. Management Science, 55(3):404–420, 2009. doi: 10.1287/mnsc.1080.0945.
R. Ibrahim, S.-H. Kim, and J. Tong. Eliciting human judgment for prediction algorithms. Man-
agement Science, 67(4):2314–2325, 2021.
A. Kumar, T. Peng, Y. Wu, and A. Zeevi. Performance of llms on stochastic modeling operations
research problems: From theory to practice. arXiv preprint arXiv:2506.23924, 2025.
V. Lai, C. Chen, A. Smith-Renner, Q. V. Liao, and C. Tan. Towards a science of human-ai decision
making: An overview of design space in empirical human-subject studies. In Proceedings of the
2023 ACM conference on fairness, accountability, and transparency, pages 1369–1385, 2023.
B. Li, K. Mellou, B. Zhang, J. Pathuri, and I. Menache. Large language models for supply chain
optimization. arXiv preprint arXiv:2307.03875, 2023.
C. G. Ling, ElizabethHMGroup, FridaRim, inversion, J. Ferrando, Maggie, neuraloverflow, and
xlsrln.
H&m personalized fashion recommendations.
https://kaggle.com/competitions/
h-and-m-personalized-fashion-recommendations, 2022. Kaggle.
J. Liu, S. Lin, L. Xin, and Y. Zhang.
Ai vs. human buyers: A study of alibaba’s inventory
replenishment system. INFORMS Journal on Applied Analytics, 2023. doi: 10.1287/inte.2023.
1160. Special Issue—2022 Daniel H. Wagner Prize for Excellence in the Practice of Advanced
Analytics and Operations Research.
J. Liu, Z. Chen, and Y. Zhong. Large language newsvendor: Decision biases and cognitive mecha-
nisms. arXiv preprint arXiv:2512.12552, 2025.
C. Long, D. Simchi-Levi, A. P. Calmon, and F. P. Calmon.
When supply chains be-
come autonomous.
Harvard Business Review, Dec. 2025.
URL https://hbr.org/2025/12/
when-supply-chains-become-autonomous. Accessed: 2026-01-31.
J. Lyu, R. Zhang, and L. Xin.
Ucb-type learning algorithms with kaplan–meier estimator for
lost-sales inventory models with lead times. Operations Research, 2024. To appear / in press.
D. Madeka, K. Torkkola, C. Eisenach, A. Luo, D. P. Foster, and S. M. Kakade. Deep inventory
management. arXiv preprint arXiv:2210.03137, 2022. Available at https://arxiv.org/abs/
2210.03137.
B. McLaughlin and J. Spiess. Designing algorithmic recommendations to achieve human-ai com-
plementarity. arXiv preprint arXiv:2405.01484, 2024.
23


--- Page 24 ---
K. Peng, N. Garg, and J. Kleinberg. A no free lunch theorem for human-ai collaboration. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 14369–14376,
2025.
M. Qi, Y. Shi, Y. Qi, C. Ma, R. Yuan, D. Wu, and Z.-J. M. Shen. A practical end-to-end inventory
management model with deep learning. Management Science, 69(2):759–773, 2023. doi: 10.1287/
mnsc.2022.4564.
D. Simchi-Levi, P. Kaminsky, and E. Simchi-Levi. Designing and Managing the Supply Chain: Con-
cepts, Strategies, and Case Studies. McGraw-Hill/Irwin, 3 edition, 2008. ISBN 9780072982398.
C. Snyder, S. Keppler, and S. Leider. Algorithm reliance: Fast and slow. Management Science, 72
(1):368–385, 2026.
L. V. Snyder and Z.-J. M. Shen. Fundamentals of Supply Chain Theory. John Wiley & Sons, 2
edition, 2019.
J. D. Sterman. Modeling managerial behavior: Misperceptions of feedback in a dynamic decision
making experiment. Management Science, 35(3):321–339, 1989.
J. Sun, D. J. Zhang, H. Hu, and J. A. Van Mieghem.
Predicting human discretion to adjust
algorithmic prescription: A large-scale field experiment in warehouse operations. Management
Science, 68(2):846–865, 2022.
M. Vaccaro, A. Almaatouq, and T. Malone. When combinations of humans and ai are useful: A
systematic review and meta-analysis. Nature Human Behaviour, 8(12):2293–2303, 2024.
B. Wilder, E. Horvitz, and E. Kamar.
Learning to complement humans.
arXiv preprint
arXiv:2005.00582, 2020.
Y. Xie, X. Hao, J. Liu, W. Ma, L. Xin, L. Cao, and Y. Zhang. Deepstock: Reinforcement learning
with policy regularizations for inventory management. Working paper, November 2025, 2025.
L. Xin. Technical note—understanding the performance of capped base-stock policies in lost-sales
inventory models. Operations Research, 69(1):61–70, 2021. doi: 10.1287/opre.2020.2019.
L. Xin and D. A. Goldberg. 1.79-approximation algorithms for continuous review single-sourcing
lost-sales and dual-sourcing inventory models. Operations Research, 70(1):111–128, 2022. doi:
10.1287/opre.2021.2150.
C. Zhou, J. Yang, L. Xin, Y. Chen, Z. He, and D. Ge. Auto-formulating dynamic programming
problems with large language models. arXiv preprint arXiv:2507.11737, 2025.
P. Zipkin. Old and new methods for lost-sales inventory systems. Operations Research, 56(5):
1256–1263, 2008a. doi: 10.1287/opre.1070.0471.
P. Zipkin. On the structure of lost-sales inventory models. Operations Research, 56(4):937–944,
2008b. doi: 10.1287/opre.1070.0482.
24


--- Page 25 ---
A
OR Baseline: Data-driven Capped Base-stock Policy
We consider a data-driven capped base-stock policy (Xin, 2021) as a default OR baseline to use
for this inventory problem. Indeed, because we have lead times and lost sales, exact optimization
is intractable. The capped base-stock policy is a simple heuristic that orders up to a “target”
inventory position (accounting for in-transit orders) while also implementing a cap to prevent the
algorithm from ordering too much at any one time, which would cause unstable inventory flow.
At each period t, the data-driven version computes an order quantity as follows. Let ˆµt denote
the estimated mean demand over the lead time horizon (specifically, the expected total demand
from period t through period t + L), and let ˆσt denote the corresponding estimated standard
deviation, recalling that L is the anticipated lead time. These estimates are computed from the
historical demand observations {d−4, . . . , d0, d1, . . . , dt−1} as follows:
• Let ¯d be the empirical mean of all observed demands.
• Let sd be the empirical standard deviation of all observed demands.
• Set ˆµt = (1 + L) ¯d and ˆσt =
√
1 + L sd, where the scaling factors (1 + L) and
√
1 + L arise
from summing independent demands over 1 + L periods (Snyder and Shen, 2019).
Define the critical fractile ρ = p/(p + h), which represents the optimal service level in the
classical newsvendor model, and let z∗= Φ−1(ρ) where Φ is the cumulative distribution function of a
standard normal random variable. The base-stock level is then Bt = ˆµt+z∗ˆσt, which is the inventory
target (for on-hand plus in-transit inventory) that balances overstocking and understocking costs.
To build intuition for readers unfamiliar with inventory theory: the critical fractile ρ determines
the optimal frequency of overstocking versus stocking out. Under an optimal policy, the decision
maker carries leftover inventory on a ρ fraction of periods and experiences a stockout on the
remaining 1 −ρ fraction. When ρ is high (e.g., 0.95), stockouts are very costly relative to holding
(because p is high relative to h), so the optimal policy maintains a large safety buffer and stocks out
rarely. This is generally the case, which is why Newsvendor literature tends to place asymmetric
focus on high values of ρ. When demand over the lead time horizon follows a normal distribution
N(ˆµt, ˆσ2
t ) and lead time is zero, the base-stock level Bt = ˆµt + Φ−1(ρ) ˆσt is exactly optimal.
Let IPt denote the inventory position at time t, defined as on-hand inventory plus all outstanding
orders:
IPt = It +
X
τ<t:τ+ℓτ≥t
qτ.
The uncapped base-stock policy would order qt = max{Bt −IPt, 0} to raise the inventory position
to the target Bt. However, when lead times are positive, this can result in excessive orders during
periods of high recent demand. To address this, we implement a capped base-stock policy that
limits each order to at most one period’s worth of demand plus a safety margin:
qt = min

max{Bt −IPt, 0},
ˆµt
1 + L + Φ−1(0.95)
ˆσt
√
1 + L

.
This cap prevents overreaction to short-term demand fluctuations while maintaining sufficient safety
stock (Xin, 2021). We use the 0.95 ratio as a default in the formula for the cap, irrespective of the
critical fractile ρ.
25


--- Page 26 ---
B
LLM Prompts
This appendix presents the system prompt used for the OR→LLM agent.
The LLM-only and
LLM→OR variants use the same prompt with minor modifications: the LLM-only variant omits
the OR recommendation sections, and the LLM→OR variant modifies the output format to return
parameter estimates instead of order quantities.
Values in {braces} are filled in at initialization based on the instance configuration (e.g., prod-
uct ID, promised lead time, historical demand samples).
System Prompt: Role & Game Mechanics
You control the vending machine for a single SKU "{item id}" while collaborating
with an OR baseline.
Maximize total reward Rt = Profit × units sold −HoldingCost ×
ending inventory each period.
Period execution sequence:
1. VM Decision Phase:
You receive observation (including OR recommendation) and
place orders for Period N
2. Arrival Resolution:
Orders scheduled to arrive in Period N are added to
on-hand inventory
3. Demand Resolution:
Customer demand is satisfied from on-hand inventory
4. Period Conclusion:
System generates "Period N conclude" message (visible in
Period N+1)
Important:
Steps 2--4 happen AFTER your decision.
You will see their results in
the next period.
Lead time definition.
Promised lead time:
{L} period(s).
An order placed in
Period N arrives during Period (N+L)’s arrival resolution, becomes visible in the
‘‘Period (N+L) conclude’’ message, and is read at the start of Period (N+L+1)’s
decision phase.
There is always a 1-period observation delay.
Actual lead time
may differ from promised; orders may also be lost (never arrive).
System Prompt: OR Baseline Explained to LLM
The OR agent uses a capped base-stock policy:
1.
Demand estimation (from historical samples ξ1, . . . , ξn):
Empirical mean:
¯µ = 1
n
P
i ξi,
std dev:
¯σ =
q
1
n−1
P
i(ξi −¯µ)2
Over lead time horizon:
ˆµ = (1+L)¯µ,
ˆσ =
√
1+L ¯σ
2.
Safety factor:
q = p/(p + h),
z∗= Φ−1(q)
3.
Base stock:
B = ˆµ + z∗ˆσ
4.
Capped order:
qt = max
 0, min(B −IPt, cap)

where cap = ˆµ/(1+L) + Φ−1(0.95) ˆσ/
√
1+L
5.
OR limitations:
Uses promised (not actual) lead time; weights all historical
samples equally; cannot detect lost orders or regime shifts; assumes i.i.d. demand.
Your role: The OR recommendation is a data-driven baseline. Override it when you detect: actual
vs. promised lead time discrepancies, demand regime changes, seasonality (from dates + product
description), or lost shipments.
26


--- Page 27 ---
System Prompt: Reasoning & Output
Decision checklist:
1. Use world knowledge and SKU description to assess demand outlook.
2. Reconcile on-hand + pipeline with expected arrivals; flag overdue/lost
shipments.
3. Inspect the OR recommendation (quantity + stats) and decide how to adapt it.
4. Justify final quantity by tying it to demand outlook, lead-time belief, and OR’s
baseline.
Carry-over insights:
Record only NEW, evidence-backed insights about sustained
shifts (demand mean/variance, lead time, seasonality).
Stay conservative; provide
concrete stats.
Remove insights once they stop being true.
Output format (JSON):
"rationale":
full step-by-step analysis
"short rationale for human":
1--3 sentence summary
"carry over insight":
new sustained discoveries, or ‘‘’’
"action":
{"{item id}":
quantity}
Human-in-the-loop additions.
For Mode B (OR→LLM→Human), the system prompt ap-
pends instructions for a two-stage interaction: in Stage 1, the agent provides its initial rationale
and decision; in Stage 2, if the human provides feedback, the agent incorporates it and outputs
a revised action. For Mode C (autonomous with strategic guidance), the prompt instructs the
agent to follow any strategic guidance from the human supervisor that appears at the top of the
observation.
Per-period user message.
Each period, the user message provided to the LLM contains three
blocks: (1) carry-over insights from prior periods (if any), formatted as a header listing each
insight with its originating period; (2) the current observation from the game environment (on-hand
inventory, in-transit orders, the previous period’s conclusion message reporting arrivals, demand,
sales, and ending inventory); and (3) the OR algorithm’s recommendation with statistics (base-
stock level, current inventory position, empirical demand mean and standard deviation, and order
cap).
C
LLM →OR Parameter Interface
In the LLM →OR method, the LLM agent analyzes contextual information, demand history,
and observed arrival patterns to provide parameter estimates that inform the OR algorithm’s
calculations. The OR algorithm then uses these LLM-provided estimates to compute the order
quantity according to the capped base-stock policy described in Section 3.
The LLM can provide estimates for the following parameters:
Lead time (L).
The LLM provides an estimate of the effective lead time for the current order.
This estimate may differ from the anticipated lead time if the LLM detects supply disruptions,
seasonal delays, or patterns in past delivery performance.
For example, if recent orders have
consistently arrived late or been lost, the LLM may recommend using a longer effective lead time
or adjusting the demand forecast accordingly.
27


--- Page 28 ---
Mean demand over lead time horizon (ˆµt).
The LLM provides a forecast of the expected total
demand from period t through the period when the current order is expected to arrive (typically
period t + L). This forecast may incorporate:
• Pattern detection in recent demand (trends, changepoints, seasonality)
• Product-specific knowledge (e.g., swimwear demand peaks in spring/summer)
• Calendar effects (holidays, promotional periods)
• Context from the text description xt
The LLM’s forecast replaces or augments the simple empirical mean used by the default OR algo-
rithm, allowing for adaptive forecasting that responds to non-stationary demand patterns.
Standard deviation over lead time horizon (ˆσt).
The LLM provides an estimate of demand
variability over the lead time horizon. This estimate may reflect recent changes in demand volatility
or structural shifts in the demand process. For instance, if demand has become more erratic in
recent periods, the LLM may recommend a higher ˆσt to increase safety stock.
Implementation note.
The LLM may choose to specify only L, in which case ˆµt and ˆσt are
computed using the OR baseline’s default empirical estimates: ˆµt = (1 + L) ¯d and ˆσt =
√
1 + L sd,
where ¯d and sd are the sample mean and standard deviation of observed demand. Alternatively,
if the LLM provides ˆµt and ˆσt directly, then the lead time estimate L serves only as contextual
information and is not used in the base-stock calculation. This allows the LLM to bypass the
assumption of stationary, IID demand implicit in the default formulas.
Once the LLM provides these parameter estimates, the OR algorithm computes the base-stock
level Bt = ˆµt+z∗ˆσt (where z∗= Φ−1(ρ)), determines inventory position IPt, and applies the capped
base-stock policy to determine the order quantity qt.
D
Synthetic Instance Specifications
This appendix provides the complete distributional specifications for all 40 synthetic demand vari-
ants used in the benchmark. We generate 10 demand pattern families, each with 4 parametric
variants, yielding 40 unique distributions. For each, we create 2 independent demand realizations,
cross them with 3 cost ratios (p:h = 1:1, 4:1, 19:1), and evaluate under 3 lead time settings (L = 0,
L = 4, stochastic), producing 40 × 2 × 3 × 3 = 720 synthetic instances in total. Each instance
consists of a T = 50 period test trajectory and 5 historical training samples. The 10 demand
pattern families are: (1) stationary IID, (2–3) abrupt mean shifts (increase/decrease at period 16),
(4–5) gradual trends (increasing/decreasing), (6) variance changes, (7) seasonal/cyclical patterns,
(8) multiple changepoints, (9) temporary spikes or dips, and (10) autocorrelated AR(1) processes.
Several of these patterns feature ad hoc demand shifts of the kind commonly encountered
in operations management games—for example, the step-function demand increase in the Beer
Game (Sterman, 1989) or the non-stationary demand environment in the Mexico-China sourcing
game (Allon and Van Mieghem, 2010)—while others feature more predictable seasonality or gradual
trends. The OR baseline described in Appendix A treats demand as stationary and IID, so it cannot
detect any of these patterns. One could, in principle, augment the OR method with statistical
changepoint detection, trend estimation, or models for correlated processes, but each extension
requires its own methodology and careful tuning—there is no single off-the-shelf OR procedure
28


--- Page 29 ---
that handles all 10 pattern families. By contrast, an LLM can attempt pattern recognition out of
the box, without case-specific statistical machinery. Example 3 illustrates this capability with an
example in which the LLM detects a mean shift, confirms the regime change over multiple periods,
and adapts its forecast accordingly.
Generation Procedure
Demand realizations are generated as follows. For normal distributions, Dt ∼N(µ(t), σ(t)), trun-
cated at zero and rounded to the nearest integer. For uniform distributions, Dt ∼Uniform[a, b],
similarly truncated and rounded. For AR(1) processes, Dt = φ · Dt−1 + c + εt where εt ∼N(0, σ)
and D0 = 100. Changepoint patterns apply the appropriate segment’s distribution based on period
t.
Training data generation depends on the pattern type. For stationary patterns (p01), training
consists of 5 i.i.d. samples from the distribution. For changepoint patterns (p02, p03, p06, p08,
p09), training consists of 5 i.i.d. samples from the first segment only—so the algorithm has no
advance notice of the regime change. For trend (p04, p05), seasonal (p07), and AR(1) patterns
(p10), training consists of sequential samples at t = 1, 2, 3, 4, 5. All random number generation uses
base seed 42 for reproducibility.
Pattern Specifications
Pattern 1: Stationary IID (p01).
Constant distribution throughout all periods.
Variant
Distribution
Parameters
v1
Normal
µ = 100, σ = 25
v2
Normal
µ = 100, σ = 40
v3
Normal
µ = 100, σ = 15
v4
Uniform
a = 50, b = 150
Pattern 2: Mean Increase at t = 16 (p02).
Sudden increase in mean demand at period 16.
Variant
Before (t ≤15)
After (t ≥16)
Change
v1
N(100, 25)
N(200, 35)
+100% mean
v2
N(100, 25)
N(150, 30)
+50% mean
v3
N(100, 25)
N(300, 50)
+200% mean
v4
N(100, 25)
N(200, 25)
+100% mean, same σ
Pattern 3: Mean Decrease at t = 16 (p03).
Sudden decrease in mean demand at period 16.
Variant
Before (t ≤15)
After (t ≥16)
Change
v1
N(100, 25)
N(50, 18)
−50% mean
v2
N(100, 25)
N(70, 20)
−30% mean
v3
N(100, 25)
N(30, 15)
−70% mean
v4
N(150, 30)
N(80, 22)
−47% mean
29


--- Page 30 ---
Pattern 4: Increasing Trend (p04).
Gradual increase in demand over time.
Variant
µ(t)
σ(t)
v1
100t
25
√
t
v2
50 + 3t
20
v3
100 × 1.05t
25
v4
100 + 2t
25
√
t
Pattern 5: Decreasing Trend (p05).
Gradual decrease in demand over time.
Variant
µ(t)
σ(t)
v1
max(200 −3t, 50)
25
v2
200 × 0.97t
20
v3
max(150 −2t, 30)
20
v4
200/
√
t
15
Pattern 6: Variance Change at t = 16 (p06).
Change in demand variability at period 16.
Variant
Before (t ≤15)
After (t ≥16)
Change
v1
N(100, 25)
Uniform[0, 200]
Normal →Uniform
v2
N(100, 25)
N(100, 50)
σ doubles
v3
N(100, 50)
N(100, 20)
σ decreases
v4
Uniform[50, 150]
N(100, 15)
Uniform →Normal
Pattern 7: Seasonal/Cyclical (p07).
Periodic demand with sinusoidal variation. All variants
use σ = 25.
Variant
µ(t)
Period
Amplitude
v1
100 + 30 sin(2πt/10)
10
30
v2
100 + 50 sin(2πt/5)
5
50
v3
100 + 40 sin(2πt/25)
25
40
v4
100 × (1 + 0.3 sin(2πt/10))
10
30% mult.
Pattern 8: Multiple Changepoints (p08).
Two changepoints creating three distinct demand
regimes.
Variant
t ∈[1, 15]
t ∈[16, 35]
t ∈[36, 50]
Pattern
v1
N(100, 25)
N(150, 30)
N(80, 20)
Up then down
v2
N(100, 25)
N(60, 20)
N(140, 30)
Down then up
v3
N(100, 25)
N(100, 50)
N(100, 20)
Variance only
v4
N(80, 20)
N(120, 25)
N(100, 22)
Mild fluctuation
Pattern 9: Temporary Spike/Dip (p09).
Temporary demand anomaly followed by return to
baseline.
30


--- Page 31 ---
Variant
t ∈[1, 15]
t ∈[16, 25]
t ∈[26, 50]
Pattern
v1
N(100, 25)
N(200, 35)
N(100, 25)
Temporary surge
v2
N(100, 25)
N(50, 18)
N(100, 25)
Temporary dip
v3
N(100, 25)
N(250, 40)
N(120, 28)
Surge →new normal
v4
N(100, 25)
N(40, 15)
N(80, 22)
Dip →partial recovery
Pattern 10: Autocorrelated AR(1) (p10).
Demand follows Dt = φ · Dt−1 + c + εt, with
εt ∼N(0, σ) and D0 = 100. The long-run mean is c/(1 −φ) = 100 for all variants.
Variant
φ
c
σ
Behavior
v1
0.7
30
20
Strong positive autocorrelation
v2
0.5
50
25
Moderate positive
v3
0.3
70
30
Weak positive
v4
−0.3
130
25
Negative (alternating)
E
Real Instance Specifications
The real portion of our benchmark comprises 600 instances derived from the H&M Personalized
Fashion Recommendations dataset (Ling et al., 2022). We selected 200 distinct articles (SKUs),
each with weekly aggregated sales data spanning 52 weeks: the first 5 weeks serve as historical
training data and the remaining 47 weeks form the test trajectory (T = 47).
Each article is
evaluated under the same 3 lead time settings (0, 4, and stochastic), and cost ratios are uniformly
randomly assigned (with fixed seed) corresponding to ρ ∈{0.50, 0.80, 0.95} (p:h = 1:1, 4:1, and
19:1), distributed evenly, producing 200 × 3 = 600 instances.
Data Preprocessing
We preprocessed the raw H&M transaction data as follows. We aggregated individual transactions
into weekly sales counts per article for all of 2019, then applied two filters: (i) articles must have
positive sales in at least 50 of 52 weeks (eliminating items with frequent stockouts or discontin-
uations), and (ii) articles must have stable prices, defined as a max-to-min weekly average price
ratio of at most 1.2 after excluding up to 4 outlier holiday weeks (eliminating items whose demand
is confounded by heavy discounting or price changes). We assume that observed sales is the true
demand. From the articles passing both filters, we selected the top 200 by total annual sales vol-
ume. For each article, the first 5 weeks (2019-01-07 through 2019-02-04) form the training set and
the remaining weeks (2019-02-11 through 2019-12-30) form the test trajectory. Each test instance
also includes a text product description drawn from the H&M article metadata (product name,
type, color, garment group, and a natural-language detail description), which the LLM can read
but the OR algorithm ignores. The contextual information for each week also includes the actual
real dates.
LLM World Knowledge
A key advantage of LLM-based methods on real instances is their ability to leverage world knowl-
edge about product seasonality. Example 4 provides an illustrative example in which the LLM
reasons about swimwear demand using calendar dates and seasonal knowledge to adjust its fore-
casts throughout the year.
31


--- Page 32 ---
F
Additional Model Results
This appendix presents the full results for Grok 4.1 Fast and GPT-5 Mini, analogous to the Gemini
3 Flash results in Section 4.
The qualitative findings for Grok 4.1 Fast are broadly consistent with Gemini. OR→LLM is the
best method overall (.514), followed by LLM→OR (.493), mirroring Gemini’s overall ranking. The
same structural patterns hold across lead time settings: LLM→OR dominates under deterministic
lead times, while OR→LLM excels under stochastic lead times where the LLM’s ability to detect lost
orders is decisive. On the pattern table (which excludes stochastic lead times), LLM→OR achieves
the highest synthetic (.707) and real (.539) rewards; OR alone remains best on families where
its stationary demand model is well-specified (Stationary IID, Variance Change, Seasonal, Multi
Changepoint, Autocorrelated), while LLM-based methods excel on directional demand shifts (Mean
Shift Up/Down, Trend Up/Down). The calibration spectrum also mirrors Gemini: responsiveness
to ρ decreases with more LLM influence, with ranges of 0.127 (OR), 0.101 (LLM→OR), 0.078
(LLM), and 0.070 (OR→LLM).
GPT-5 Mini is the weakest of the three models overall, with OR→LLM (.461), LLM (.460),
and LLM→OR (.458) all performing similarly. The near-parity suggests that GPT-5 Mini tends
to ignore the OR recommendation, so that incorporating OR adds little. This is also reflected in
its calibration: the LLM and OR→LLM ranges are only 0.037 and 0.042, respectively, indicating
near-constant stocking behavior that largely disregards the critical fractile.
F.1
Grok 4.1 Fast
Figure 8: Grok 4.1 Fast: overall normalized reward (mean ± 95% CI) across all 1,320 instances,
by method.
32


--- Page 33 ---
Figure 9: Grok 4.1 Fast: normalized reward by lead time setting (440 instances per setting).
Table 4: Grok 4.1 Fast: normalized reward by synthetic demand pattern (See Appendix D for
definitions), each corresponding to 48 instances (8 instances for each of 3 critical fractiles and 2
lead times, with stochastic lead time excluded). The All Synthetic column averages over all
480 instances while the Real column averages over 400 real instances (again excluding stochastic
lead time). Cell shading per column:
green = best, red = worst. Best method per column in
bold.
Stationary IID
Mean ↑
Mean ↓
Trend ↑
Trend ↓
Variance Change
Seasonal
Multi Changepoint
Spike/Dip
Autocorr.
All Synthetic
Real
(p01)
(p02)
(p03)
(p04)
(p05)
(p06)
(p07)
(p08)
(p09)
(p10)
OR
.802
.774
.557
.626
.521
.706
.676
.700
.651
.756
.677
.486
LLM
.607
.686
.493
.697
.522
.512
.543
.573
.536
.610
.578
.431
OR→LLM
.790
.788
.602
.789
.604
.678
.665
.689
.647
.732
.698
.492
LLM→OR
.779
.806
.653
.770
.665
.666
.654
.696
.662
.719
.707
.539
33


--- Page 34 ---
Table 5: Grok 4.1 Fast: implicit critical fractiles, averaged across the instances with a given value
of ρ.
Method
ρ = 0.50
ρ = 0.80
ρ = 0.95
Range
OR
0.470
0.527
0.597
0.127
LLM
0.747
0.796
0.825
0.078
OR→LLM
0.672
0.709
0.742
0.070
LLM→OR
0.643
0.700
0.744
0.101
F.2
GPT-5 Mini
Figure 10: GPT-5 Mini: overall normalized reward (mean ± 95% CI) across all 1,320 instances, by
method.
34


--- Page 35 ---
Figure 11: GPT-5 Mini: normalized reward by lead time setting (440 instances per setting).
Table 6: GPT-5 Mini: normalized reward by synthetic demand pattern (See Appendix D for
definitions), each corresponding to 48 instances (8 instances for each of 3 critical fractiles and 2
lead times, with stochastic lead time excluded). The All Synthetic column averages over all
480 instances while the Real column averages over 400 real instances (again excluding stochastic
lead time). Cell shading per column:
green = best, red = worst. Best method per column in
bold.
Stationary IID
Mean ↑
Mean ↓
Trend ↑
Trend ↓
Variance Change
Seasonal
Multi Changepoint
Spike/Dip
Autocorr.
All Synthetic
Real
(p01)
(p02)
(p03)
(p04)
(p05)
(p06)
(p07)
(p08)
(p09)
(p10)
OR
.802
.774
.557
.626
.521
.706
.676
.700
.651
.756
.677
.486
LLM
.703
.743
.627
.738
.651
.615
.616
.649
.625
.667
.664
.526
OR→LLM
.728
.746
.608
.738
.607
.644
.637
.676
.638
.709
.673
.524
LLM→OR
.794
.789
.582
.709
.560
.691
.672
.697
.639
.743
.688
.506
35


--- Page 36 ---
Table 7: GPT-5 Mini: implicit critical fractiles, averaged across the instances with a given value of
ρ.
Method
ρ = 0.50
ρ = 0.80
ρ = 0.95
Range
OR
0.470
0.527
0.597
0.127
LLM
0.544
0.546
0.581
0.037
OR→LLM
0.549
0.550
0.591
0.042
LLM→OR
0.508
0.530
0.600
0.092
G
Detailed Performance Tables
Tables 8–10 provide complete per-pattern, per-lead-time, per-critical-fractile breakdowns for all 720
synthetic instances under each of the three LLMs. Table 11 provides the corresponding breakdown
for real instances.
H
LLM and Human Reasoning Anecdotes
This appendix presents representative examples of LLM reasoning from the benchmark experi-
ments, illustrating key capabilities (and one downside) that distinguish LLM-based decision-making
from pure OR methods. Table 12 further shows examples of human guidance messages from the
classroom experiment, illustrating how humans add value through the same contextual reasoning
channels.
Example 1 (LLM detects lost orders). In instance 108775044 (an H&M strap top) under stochastic
lead times, at period 10, the OR algorithm recommends ordering 0 units because it counts 936 units
as pipeline inventory. The Gemini-based OR→LLM hybrid detects this error and overrides:
“Critically, several shipments (P1, P3, P4) are confirmed lost, as they never arrived despite
being 4+ periods old. . . The OR recommends 0 because it counts the full 936 units as pipeline
inventory. I must ignore the OR recommendation because the pipeline is corrupted by lost orders.
With an effective pipeline of 390 units and demand averaging ∼230 per period. . . Current effective
inventory = 390. Shortfall ≈300 units.”
The LLM recalculates effective inventory (936 →390 units) and overrides OR’s recommendation
of 0, ordering 210 units instead.
Example 2 (LLM overfits). While LLMs excel at detecting genuine demand shifts, they can also
overfit to natural variance in stationary settings. Consider a stationary IID instance drawn from
N(100, 252) (actual mean: 102.4, std: 22.9). In Period 9, demand was 145—a high draw but well
within two standard deviations. Gemini interpreted this as a regime shift, reasoning at Period 10:
“Demand analysis shows an upward trend over the last three periods: P7 (122), P8 (121), and
P9 (145), indicating the mean has shifted from the previous ∼100 range to approximately 130+.”
It created a carry-over insight: “Demand regime shift at Period 9: Demand increased from an
average of ∼100 to 145 (+45%), potentially indicating a new higher demand baseline,” and forecast
Period 10 demand at 145–150.
However, subsequent periods (P10: 115, P11: 89, P12: 102, P13: 73, P14: 63) all fell near the true
mean of 100, confirming no regime shift occurred. Despite this, Gemini continued pattern-seeking,
updating its carry-over insights at Period 12 (“Demand regime stabilized further: average demand
from Period 9–12 is ∼113, adjusting previous Period 10 insight down from 145”), Period 14 (“Recent
36


--- Page 37 ---
Table 8: Complete performance breakdown for synthetic instances (Gemini 3 Flash). Each cell
shows the average normalized reward over 8 instances. Best method per row is in bold.
Pat
L
ρ
OR
LLM
OR→
LLM
LLM→
OR
Pat
L
ρ
OR
LLM
OR→
LLM
LLM→
OR
p01
L=0
0.50
0.778
0.619
0.727
0.765
p06
L=0
0.50
0.684
0.552
0.623
0.664
p01
L=0
0.80
0.905
0.883
0.900
0.896
p06
L=0
0.80
0.856
0.838
0.845
0.850
p01
L=0
0.95
0.971
0.966
0.971
0.967
p06
L=0
0.95
0.950
0.935
0.948
0.947
p01
L=4
0.50
0.519
0.015
0.408
0.445
p06
L=4
0.50
0.294
0.000
0.166
0.145
p01
L=4
0.80
0.764
0.593
0.751
0.740
p06
L=4
0.80
0.637
0.414
0.614
0.563
p01
L=4
0.95
0.876
0.842
0.870
0.870
p06
L=4
0.95
0.813
0.761
0.798
0.793
p01
L=S
0.50
0.069
0.028
0.219
0.105
p06
L=S
0.50
0.077
0.001
0.137
0.096
p01
L=S
0.80
0.094
0.509
0.523
0.236
p06
L=S
0.80
0.113
0.483
0.499
0.254
p01
L=S
0.95
0.122
0.705
0.612
0.380
p06
L=S
0.95
0.143
0.663
0.607
0.381
p02
L=0
0.50
0.710
0.760
0.803
0.818
p07
L=0
0.50
0.699
0.584
0.641
0.683
p02
L=0
0.80
0.875
0.909
0.915
0.915
p07
L=0
0.80
0.866
0.852
0.860
0.862
p02
L=0
0.95
0.955
0.961
0.964
0.964
p07
L=0
0.95
0.959
0.954
0.957
0.958
p02
L=4
0.50
0.574
0.110
0.468
0.495
p07
L=4
0.50
0.144
0.000
0.079
0.108
p02
L=4
0.80
0.721
0.661
0.747
0.754
p07
L=4
0.80
0.576
0.367
0.536
0.560
p02
L=4
0.95
0.809
0.819
0.838
0.853
p07
L=4
0.95
0.814
0.751
0.788
0.794
p02
L=S
0.50
0.059
0.077
0.246
0.093
p07
L=S
0.50
0.057
0.013
0.091
0.056
p02
L=S
0.80
0.079
0.511
0.482
0.237
p07
L=S
0.80
0.100
0.409
0.484
0.216
p02
L=S
0.95
0.098
0.645
0.524
0.336
p07
L=S
0.95
0.142
0.687
0.641
0.346
p03
L=0
0.50
0.574
0.542
0.609
0.709
p08
L=0
0.50
0.708
0.667
0.677
0.740
p03
L=0
0.80
0.804
0.839
0.847
0.868
p08
L=0
0.80
0.865
0.871
0.873
0.877
p03
L=0
0.95
0.937
0.953
0.950
0.953
p08
L=0
0.95
0.950
0.950
0.953
0.955
p03
L=4
0.50
0.033
0.000
0.023
0.093
p08
L=4
0.50
0.231
0.000
0.241
0.193
p03
L=4
0.80
0.286
0.325
0.477
0.539
p08
L=4
0.80
0.630
0.497
0.639
0.625
p03
L=4
0.95
0.707
0.744
0.762
0.775
p08
L=4
0.95
0.815
0.784
0.813
0.807
p03
L=S
0.50
0.105
0.004
0.092
0.051
p08
L=S
0.50
0.065
0.031
0.189
0.131
p03
L=S
0.80
0.147
0.421
0.500
0.284
p08
L=S
0.80
0.099
0.433
0.484
0.246
p03
L=S
0.95
0.185
0.715
0.629
0.492
p08
L=S
0.95
0.131
0.623
0.569
0.370
p04
L=0
0.50
0.506
0.737
0.759
0.771
p09
L=0
0.50
0.700
0.669
0.692
0.752
p04
L=0
0.80
0.724
0.879
0.898
0.885
p09
L=0
0.80
0.846
0.873
0.868
0.881
p04
L=0
0.95
0.896
0.950
0.952
0.947
p09
L=0
0.95
0.944
0.950
0.954
0.952
p04
L=4
0.50
0.408
0.390
0.431
0.569
p09
L=4
0.50
0.105
0.000
0.026
0.073
p04
L=4
0.80
0.556
0.704
0.719
0.742
p09
L=4
0.80
0.542
0.341
0.561
0.572
p04
L=4
0.95
0.670
0.835
0.806
0.830
p09
L=4
0.95
0.769
0.737
0.766
0.779
p04
L=S
0.50
0.061
0.189
0.284
0.146
p09
L=S
0.50
0.065
0.044
0.168
0.099
p04
L=S
0.80
0.083
0.487
0.445
0.248
p09
L=S
0.80
0.104
0.441
0.450
0.256
p04
L=S
0.95
0.106
0.595
0.482
0.314
p09
L=S
0.95
0.133
0.567
0.571
0.308
p05
L=0
0.50
0.514
0.689
0.604
0.766
p10
L=0
0.50
0.770
0.674
0.714
0.755
p05
L=0
0.80
0.787
0.888
0.850
0.894
p10
L=0
0.80
0.900
0.889
0.895
0.890
p05
L=0
0.95
0.937
0.962
0.958
0.967
p10
L=0
0.95
0.968
0.964
0.968
0.959
p05
L=4
0.50
0.000
0.000
0.000
0.046
p10
L=4
0.50
0.369
0.010
0.192
0.242
p05
L=4
0.80
0.231
0.310
0.449
0.513
p10
L=4
0.80
0.688
0.487
0.656
0.645
p05
L=4
0.95
0.655
0.730
0.739
0.754
p10
L=4
0.95
0.840
0.796
0.821
0.813
p05
L=S
0.50
0.100
0.000
0.043
0.055
p10
L=S
0.50
0.077
0.057
0.217
0.099
p05
L=S
0.80
0.151
0.418
0.459
0.337
p10
L=S
0.80
0.108
0.491
0.485
0.282
p05
L=S
0.95
0.197
0.671
0.591
0.417
p10
L=S
0.95
0.132
0.676
0.598
0.431
Pattern abbreviations: p01=Stationary IID, p02=Mean Increase, p03=Mean Decrease, p04=Increasing
Trend, p05=Decreasing Trend, p06=Variance Change, p07=Seasonal, p08=Multi Changepoint, p09=Temp
Spike/Dip, p10=Autocorrelated
37


--- Page 38 ---
Table 9: Complete performance breakdown for synthetic instances (Grok 4.1 Fast). Each cell shows
the average normalized reward over 8 instances. Best method per row is in bold.
Pat
L
ρ
OR
LLM
OR→
LLM
LLM→
OR
Pat
L
ρ
OR
LLM
OR→
LLM
LLM→
OR
p01
L=0
0.50
0.778
0.716
0.760
0.766
p06
L=0
0.50
0.684
0.594
0.654
0.670
p01
L=0
0.80
0.905
0.896
0.901
0.898
p06
L=0
0.80
0.856
0.844
0.852
0.848
p01
L=0
0.95
0.971
0.957
0.969
0.960
p06
L=0
0.95
0.950
0.934
0.952
0.944
p01
L=4
0.50
0.519
0.022
0.472
0.447
p06
L=4
0.50
0.294
0.004
0.180
0.180
p01
L=4
0.80
0.764
0.298
0.762
0.736
p06
L=4
0.80
0.637
0.151
0.623
0.562
p01
L=4
0.95
0.876
0.752
0.876
0.865
p06
L=4
0.95
0.813
0.543
0.808
0.792
p01
L=S
0.50
0.069
0.107
0.099
0.000
p06
L=S
0.50
0.077
0.070
0.110
0.000
p01
L=S
0.80
0.094
0.512
0.444
0.132
p06
L=S
0.80
0.113
0.498
0.388
0.117
p01
L=S
0.95
0.122
0.707
0.637
0.548
p06
L=S
0.95
0.143
0.703
0.563
0.486
p02
L=0
0.50
0.710
0.790
0.795
0.820
p07
L=0
0.50
0.699
0.647
0.678
0.684
p02
L=0
0.80
0.875
0.904
0.908
0.914
p07
L=0
0.80
0.866
0.857
0.860
0.860
p02
L=0
0.95
0.955
0.951
0.960
0.963
p07
L=0
0.95
0.959
0.943
0.959
0.951
p02
L=4
0.50
0.574
0.207
0.465
0.521
p07
L=4
0.50
0.144
0.000
0.119
0.094
p02
L=4
0.80
0.721
0.462
0.750
0.760
p07
L=4
0.80
0.576
0.152
0.560
0.549
p02
L=4
0.95
0.809
0.802
0.848
0.856
p07
L=4
0.95
0.814
0.656
0.812
0.788
p02
L=S
0.50
0.059
0.178
0.169
0.000
p07
L=S
0.50
0.057
0.012
0.051
0.023
p02
L=S
0.80
0.079
0.499
0.457
0.186
p07
L=S
0.80
0.100
0.399
0.412
0.210
p02
L=S
0.95
0.098
0.665
0.562
0.430
p07
L=S
0.95
0.142
0.683
0.571
0.505
p03
L=0
0.50
0.574
0.610
0.641
0.700
p08
L=0
0.50
0.708
0.680
0.716
0.744
p03
L=0
0.80
0.804
0.859
0.851
0.873
p08
L=0
0.80
0.865
0.860
0.869
0.878
p03
L=0
0.95
0.937
0.952
0.949
0.948
p08
L=0
0.95
0.950
0.939
0.952
0.952
p03
L=4
0.50
0.033
0.000
0.032
0.079
p08
L=4
0.50
0.231
0.008
0.165
0.172
p03
L=4
0.80
0.286
0.076
0.400
0.539
p08
L=4
0.80
0.630
0.261
0.618
0.618
p03
L=4
0.95
0.707
0.459
0.739
0.778
p08
L=4
0.95
0.815
0.689
0.815
0.812
p03
L=S
0.50
0.105
0.009
0.095
0.010
p08
L=S
0.50
0.065
0.032
0.059
0.010
p03
L=S
0.80
0.147
0.388
0.403
0.313
p08
L=S
0.80
0.099
0.407
0.373
0.275
p03
L=S
0.95
0.185
0.723
0.543
0.555
p08
L=S
0.95
0.131
0.671
0.550
0.525
p04
L=0
0.50
0.506
0.753
0.765
0.752
p09
L=0
0.50
0.700
0.698
0.706
0.749
p04
L=0
0.80
0.724
0.889
0.892
0.877
p09
L=0
0.80
0.846
0.869
0.869
0.879
p04
L=0
0.95
0.896
0.951
0.949
0.945
p09
L=0
0.95
0.944
0.942
0.954
0.949
p04
L=4
0.50
0.408
0.202
0.558
0.549
p09
L=4
0.50
0.105
0.000
0.024
0.049
p04
L=4
0.80
0.556
0.566
0.744
0.689
p09
L=4
0.80
0.542
0.150
0.558
0.574
p04
L=4
0.95
0.670
0.819
0.825
0.807
p09
L=4
0.95
0.769
0.557
0.773
0.773
p04
L=S
0.50
0.061
0.158
0.242
0.000
p09
L=S
0.50
0.065
0.025
0.053
0.006
p04
L=S
0.80
0.083
0.452
0.430
0.160
p09
L=S
0.80
0.104
0.392
0.381
0.232
p04
L=S
0.95
0.106
0.651
0.516
0.465
p09
L=S
0.95
0.133
0.627
0.529
0.498
p05
L=0
0.50
0.514
0.714
0.673
0.753
p10
L=0
0.50
0.770
0.717
0.754
0.764
p05
L=0
0.80
0.787
0.897
0.868
0.894
p10
L=0
0.80
0.900
0.890
0.898
0.891
p05
L=0
0.95
0.937
0.957
0.960
0.966
p10
L=0
0.95
0.968
0.952
0.965
0.960
p05
L=4
0.50
0.000
0.000
0.000
0.063
p10
L=4
0.50
0.369
0.067
0.276
0.238
p05
L=4
0.80
0.231
0.053
0.395
0.541
p10
L=4
0.80
0.688
0.362
0.665
0.645
p05
L=4
0.95
0.655
0.511
0.730
0.770
p10
L=4
0.95
0.840
0.670
0.831
0.815
p05
L=S
0.50
0.100
0.012
0.058
0.000
p10
L=S
0.50
0.077
0.079
0.153
0.026
p05
L=S
0.80
0.151
0.416
0.359
0.191
p10
L=S
0.80
0.108
0.483
0.438
0.195
p05
L=S
0.95
0.197
0.651
0.556
0.510
p10
L=S
0.95
0.132
0.666
0.524
0.486
Pattern abbreviations: p01=Stationary IID, p02=Mean Increase, p03=Mean Decrease, p04=Increasing
Trend, p05=Decreasing Trend, p06=Variance Change, p07=Seasonal, p08=Multi Changepoint, p09=Temp
Spike/Dip, p10=Autocorrelated
38


--- Page 39 ---
Table 10: Complete performance breakdown for synthetic instances (GPT-5 Mini). Each cell shows
the average normalized reward over 8 instances. Best method per row is in bold.
Pat
L
ρ
OR
LLM
OR→
LLM
LLM→
OR
Pat
L
ρ
OR
LLM
OR→
LLM
LLM→
OR
p01
L=0
0.50
0.778
0.751
0.766
0.766
p06
L=0
0.50
0.684
0.664
0.676
0.679
p01
L=0
0.80
0.905
0.866
0.887
0.902
p06
L=0
0.80
0.856
0.816
0.840
0.856
p01
L=0
0.95
0.971
0.923
0.956
0.970
p06
L=0
0.95
0.950
0.883
0.921
0.946
p01
L=4
0.50
0.519
0.187
0.229
0.493
p06
L=4
0.50
0.294
0.053
0.090
0.238
p01
L=4
0.80
0.764
0.682
0.702
0.761
p06
L=4
0.80
0.637
0.528
0.588
0.614
p01
L=4
0.95
0.876
0.809
0.831
0.875
p06
L=4
0.95
0.813
0.744
0.751
0.815
p01
L=S
0.50
0.069
0.031
0.066
0.093
p06
L=S
0.50
0.077
0.060
0.085
0.105
p01
L=S
0.80
0.094
0.189
0.163
0.145
p06
L=S
0.80
0.113
0.232
0.187
0.172
p01
L=S
0.95
0.122
0.257
0.223
0.179
p06
L=S
0.95
0.143
0.338
0.256
0.213
p02
L=0
0.50
0.710
0.816
0.819
0.805
p07
L=0
0.50
0.699
0.693
0.696
0.704
p02
L=0
0.80
0.875
0.887
0.899
0.909
p07
L=0
0.80
0.866
0.838
0.845
0.861
p02
L=0
0.95
0.955
0.919
0.952
0.961
p07
L=0
0.95
0.959
0.902
0.946
0.956
p02
L=4
0.50
0.574
0.323
0.336
0.490
p07
L=4
0.50
0.144
0.018
0.082
0.133
p02
L=4
0.80
0.721
0.714
0.689
0.739
p07
L=4
0.80
0.576
0.501
0.507
0.572
p02
L=4
0.95
0.809
0.802
0.782
0.832
p07
L=4
0.95
0.814
0.742
0.742
0.808
p02
L=S
0.50
0.059
0.085
0.070
0.084
p07
L=S
0.50
0.057
0.030
0.019
0.091
p02
L=S
0.80
0.079
0.132
0.125
0.133
p07
L=S
0.80
0.100
0.246
0.171
0.155
p02
L=S
0.95
0.098
0.157
0.157
0.141
p07
L=S
0.95
0.142
0.360
0.211
0.220
p03
L=0
0.50
0.574
0.715
0.698
0.663
p08
L=0
0.50
0.708
0.739
0.744
0.721
p03
L=0
0.80
0.804
0.851
0.865
0.845
p08
L=0
0.80
0.865
0.858
0.870
0.870
p03
L=0
0.95
0.937
0.911
0.945
0.951
p08
L=0
0.95
0.950
0.906
0.941
0.952
p03
L=4
0.50
0.033
0.030
0.047
0.032
p08
L=4
0.50
0.231
0.023
0.105
0.189
p03
L=4
0.80
0.286
0.509
0.345
0.290
p08
L=4
0.80
0.630
0.596
0.610
0.633
p03
L=4
0.95
0.707
0.748
0.746
0.712
p08
L=4
0.95
0.815
0.773
0.784
0.814
p03
L=S
0.50
0.105
0.019
0.057
0.122
p08
L=S
0.50
0.065
0.057
0.095
0.105
p03
L=S
0.80
0.147
0.252
0.257
0.218
p08
L=S
0.80
0.099
0.199
0.171
0.161
p03
L=S
0.95
0.185
0.395
0.330
0.260
p08
L=S
0.95
0.131
0.307
0.179
0.216
p04
L=0
0.50
0.506
0.771
0.761
0.644
p09
L=0
0.50
0.700
0.734
0.734
0.722
p04
L=0
0.80
0.724
0.854
0.853
0.838
p09
L=0
0.80
0.846
0.849
0.869
0.865
p04
L=0
0.95
0.896
0.892
0.912
0.938
p09
L=0
0.95
0.944
0.904
0.938
0.945
p04
L=4
0.50
0.408
0.479
0.480
0.460
p09
L=4
0.50
0.105
0.000
0.019
0.025
p04
L=4
0.80
0.556
0.661
0.668
0.639
p09
L=4
0.80
0.542
0.524
0.524
0.505
p04
L=4
0.95
0.670
0.773
0.756
0.733
p09
L=4
0.95
0.769
0.741
0.741
0.773
p04
L=S
0.50
0.061
0.114
0.086
0.108
p09
L=S
0.50
0.065
0.042
0.075
0.083
p04
L=S
0.80
0.083
0.168
0.173
0.165
p09
L=S
0.80
0.104
0.204
0.197
0.149
p04
L=S
0.95
0.106
0.238
0.184
0.178
p09
L=S
0.95
0.133
0.308
0.322
0.201
p05
L=0
0.50
0.514
0.786
0.758
0.671
p10
L=0
0.50
0.770
0.762
0.771
0.762
p05
L=0
0.80
0.787
0.892
0.900
0.847
p10
L=0
0.80
0.900
0.867
0.890
0.896
p05
L=0
0.95
0.937
0.943
0.962
0.943
p10
L=0
0.95
0.968
0.920
0.958
0.966
p05
L=4
0.50
0.000
0.012
0.007
0.000
p10
L=4
0.50
0.369
0.075
0.233
0.322
p05
L=4
0.80
0.231
0.514
0.309
0.242
p10
L=4
0.80
0.688
0.604
0.612
0.678
p05
L=4
0.95
0.655
0.760
0.707
0.657
p10
L=4
0.95
0.840
0.776
0.790
0.832
p05
L=S
0.50
0.100
0.042
0.077
0.127
p10
L=S
0.50
0.077
0.052
0.070
0.096
p05
L=S
0.80
0.151
0.258
0.192
0.219
p10
L=S
0.80
0.108
0.223
0.164
0.149
p05
L=S
0.95
0.197
0.342
0.341
0.300
p10
L=S
0.95
0.132
0.290
0.223
0.191
Pattern abbreviations: p01=Stationary IID, p02=Mean Increase, p03=Mean Decrease, p04=Increasing
Trend, p05=Decreasing Trend, p06=Variance Change, p07=Seasonal, p08=Multi Changepoint, p09=Temp
Spike/Dip, p10=Autocorrelated
39


--- Page 40 ---
Table 11: Performance breakdown for real instances (H&M fashion products) across all three LLMs.
Each group of 3 rows (same lead time) averages over all 200 articles. Best method per row is in
bold.
Lead Time
ρ
OR
LLM
OR→
LLM
LLM→
OR
Gemini 3 Flash
L=0
0.50
0.433
0.546
0.554
0.642
L=0
0.80
0.698
0.779
0.770
0.769
L=0
0.95
0.882
0.889
0.909
0.901
L=4
0.50
0.046
0.000
0.016
0.020
L=4
0.80
0.255
0.117
0.293
0.275
L=4
0.95
0.647
0.629
0.667
0.666
Stochastic
0.50
0.152
0.001
0.070
0.055
Stochastic
0.80
0.222
0.279
0.385
0.236
Stochastic
0.95
0.250
0.623
0.578
0.404
Grok 4.1 Fast
L=0
0.50
0.433
0.501
0.452
0.628
L=0
0.80
0.698
0.727
0.722
0.777
L=0
0.95
0.882
0.892
0.902
0.903
L=4
0.50
0.046
0.000
0.012
0.024
L=4
0.80
0.255
0.065
0.246
0.286
L=4
0.95
0.647
0.417
0.665
0.668
Stochastic
0.50
0.152
0.021
0.058
0.006
Stochastic
0.80
0.222
0.248
0.312
0.136
Stochastic
0.95
0.250
0.622
0.517
0.431
GPT-5 Mini
L=0
0.50
0.433
0.659
0.631
0.556
L=0
0.80
0.698
0.778
0.779
0.744
L=0
0.95
0.882
0.857
0.884
0.895
L=4
0.50
0.046
0.004
0.012
0.014
L=4
0.80
0.255
0.268
0.266
0.225
L=4
0.95
0.647
0.644
0.624
0.643
Stochastic
0.50
0.152
0.039
0.061
0.118
Stochastic
0.80
0.222
0.166
0.199
0.167
Stochastic
0.95
0.250
0.275
0.274
0.214
40


--- Page 41 ---
demand (P12–P14) shows a notable downward trend (avg 79) compared to the previous regime (P9–
P11 avg 116), suggesting a cooling of the earlier demand spike”), and Period 24 (“Demand stability
extended: Period 20–24 demand mean is 105.0 with a range of 102–107, confirming a high-stability
regime following earlier volatility”). This illustrates how LLMs can impose narratives on random
noise, a weakness that helps explain why pure OR outperforms LLM on stationary IID instances
(OR: 0.78 vs. LLM: 0.62 at ρ = 0.50, L = 0; see Table 8).
Example 3 (LLM detects synthetic demand shift). Consider a mean increase instance (pattern
p02) where demand shifts from 100 to 200 units at period 16. At period 17, the LLM reasons:
“The recent demand in Period 16 spiked massively to 273 units, far exceeding the previous range
of 46–137. This suggests a potential high-demand outlier or new seasonal regime. . . Before the P16
spike, the demand average was approximately 100.”
By period 18, the LLM confirms the shift:
“Sustained high-demand regime confirmed: Period 16 (273) and Period 17 (170) indicate a
significant upward shift from the previous average of ∼100. Calculating safety stock based on a
mean of ∼220 for the near term.”
Over successive periods, the LLM detects the changepoint, adapts its forecast (100 →220), and
monitors for confirmation.
Example 4 (LLM uses world knowledge). Consider instance 599580017, the “Timeless Midrise
Brief” (women’s swimwear). In early spring, the LLM reasons:
“Based on the date 2019-03-11, we are entering the Spring season (Northern Hemisphere),
which is a high-demand period for beachwear.”
By November, the same LLM adjusts:
“We are currently in mid-November (Period 40), which is deep into the off-season for swimwear.”
This calendar-aware reasoning allows the LLM to incorporate seasonal knowledge that the OR
algorithm cannot access.
Instance 1 (Swimwear)
Instance 2 (Blazer)
Instance 3 (Trousers)
Demand trend detection
Seasonal reasoning
Detecting
anomalies
(lost
orders)
“it seems that the demand is go-
ing down, so I would rather keep
the conservative strategy”
“demand decreased sharply... go
one week without ordering”
“As demand has shifted to a low
level, reduce the inventory tar-
get”
“seems like the demand is lower
than before, so make conserva-
tive choices”
“Blazers are a very seasonal
product,
make sure to rather
over estimate during fall/win-
ter.
Estimate very conserva-
tively during spring/summer.”
“given
that
a
blazer
is
for
spring/autumn weather...
Be
conservative for summer,
but
don’t understock in autumn.”
“less blazers in summer... peak
before fall/autumn”
“some orders may never arrive,
make more generous orders”
“in-transit inventory is inaccu-
rate...
ignore the in-transit in-
ventory”
“make way more generous or-
ders... hedge against orders that
may not arrive.”
“be very aggressive since very
few orders are arriving”
Table 12: Examples of human guidance messages for the three instances illustrating three types of
reasoning.
41


--- Page 42 ---
Figure 12: Analytics panel of the inventory game: historical demand chart with summary statistics
(left) and historical inventory status (right). Together with the decision panel shown in Figure 6,
these displays give participants all information needed for each ordering decision.
I
Details for Human Experiments and Inventory Game
Fig. 13 shows the product image and description for each of the three instances that were displayed
to the users, and Fig. 14 plots their realized demand patterns over time. The instances also differ
in their lead-time configurations: Instance 1 has a lead time of 0, Instance 2 has a lead time of 1,
and Instance 3 has a stochastic lead time that equals 1 with probability 75% and ∞otherwise.
J
Details of Regression Analysis
We assess whether Mode B outperforms an automated baseline by comparing human-in-the-loop
Mode B outcomes to automated runs conducted on the same three instances (100 runs per instance).
We pool these observations and estimate a linear model with instance fixed effects and a Mode B
indicator:
Outcomei = γ0 + γ2 1[instance = 2] + γ3 1[instance = 3] + δ 1[Mode B] + ui,
(4)
where instance 1 is the baseline. The coefficient δ captures the average advantage of Mode B over
the automated method after controlling for instance differences.
We conduct a one-sided test of H0 : δ ≤0 versus H1 : δ > 0. Standard errors are cluster-
robust: Mode B observations are clustered by subject, and automated observations are clustered
by run. This accounts for within-subject dependence on the human side and repeated structure
across automated runs.
Mode A vs. OR (Deterministic Algorithm).
We additionally compare Mode A to a deter-
ministic OR baseline. The analysis mirrors the specification above, replacing the Mode B indicator
42


--- Page 43 ---
Figure 13: Product images and descriptions for the three instances used in the human experiments.
43


--- Page 44 ---
0
250
500
750
1000
1250
Demand
Instance 1 (Swimwear Bottom)
200
400
600
800
Demand
Instance 2 (Blazer)
2019-01
2019-03
2019-05
2019-07
2019-09
2019-11
2020-01
Date
1000
2000
3000
Demand
Instance 3 (Trousers)
Figure 14: The demand realizations for the three instances used for human experiments. Each
panel shows realized demand over time for one instance.
44


--- Page 45 ---
with a Mode A indicator and using the OR outcomes for each instance. The coefficient on the
Mode A indicator captures the average advantage of Mode A over OR after controlling for instance
fixed effects. We test H0 : δ ≤0 vs. H1 : δ > 0 with cluster-robust standard errors, clustering Mode
A observations by subject and OR observations by run.
K
Proof of Theorem 1
Proof. Lower bound. Recall that CEi = a(i)−b(i). Fix an arbitrary threshold t ∈R. We establish a
chain of inequalities. First, observe that if a(i) > t+δ and b(i) ≤t, then a(i)−b(i) > (t+δ)−t = δ,
so the event {a(i) > t + δ} ∩{b(i) ≤t} is a subset of {CEi > δ}. Therefore,
P[CEi > δ] ≥P

a(i) > t + δ and b(i) ≤t

.
(5)
Next, we apply the Fr´echet inequality, which states that for any two events A and B, P[A ∩B] ≥
P[A] + P[B] −1. Setting A = {a(i) > t + δ} and B = {b(i) ≤t}:
P

a(i) > t + δ, b(i) ≤t

≥P[a(i) > t + δ] + P[b(i) ≤t] −1
=
 1 −FP (t + δ)

+ FQ(t) −1
= FQ(t) −FP (t + δ).
(6)
Combining (5) and (6), we obtain P[CEi > δ] ≥FQ(t)−FP (t+δ) for every t. Since t was arbitrary
and probabilities are nonnegative, taking the supremum over t yields
P[CEi > δ] ≥sup
t∈R
 FQ(t) −FP (t + δ)

.
Note that the supremum is always nonnegative, since both CDFs tend to 0 as t →−∞.
Tightness. Let t∗attain the supremum and write p∗= FQ(t∗)−FP (t∗+δ). We construct a joint
distribution of (a, b) with marginals P and Q such that P[a −b > δ] = p∗. Partition the population
into two groups. In the first group (fraction p∗), draw a from P conditioned on a > t∗+ δ and
independently draw b from Q conditioned on b ≤t∗; this ensures a −b > δ for every individual in
this group. In the second group (fraction 1 −p∗), couple the remaining conditional distributions
of a and b so that a −b ≤δ almost surely (this is feasible because the remaining mass of P is
concentrated on (−∞, t∗+ δ] and the remaining mass of Q is concentrated on (t∗, ∞), so b ≥a −δ
can be maintained). By construction, P[a −b > δ] = p∗.
L
Use of AI Writing Assistance
Portions of this manuscript were edited and polished with the assistance of large language models
(including Claude and GPT-5 Mini). The AI tools were used to help with prose editing, LaTeX
formatting, and stylistic improvements. All AI-generated suggestions were reviewed, verified, and
revised by the authors, who take full responsibility for the content of this paper.
45
