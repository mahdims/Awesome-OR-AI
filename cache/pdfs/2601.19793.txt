--- Page 1 ---
CASTER: Breaking the Cost-Performance Barrier in Multi-Agent Orchestration
via Context-Aware Strategy for Task Efficient Routing
Shanyv Liu 1 Xuyang Yuan 1 Tao Chen 1 Zijun Zhan 2 Zhu Han 2 Danyang Zheng 3 Weishan Zhang 1 4
Shaohua Cao 1 4
Abstract
Graph-based Multi-Agent Systems (MAS) enable
complex cyclic workflows but suffer from inef-
ficient static model allocation, where deploying
strong models uniformly wastes computation on
trivial sub-tasks. We propose CASTER (Context-
Aware Strategy for Task Efficient Routing), a
lightweight router for dynamic model selection
in graph-based MAS. CASTER employs a Dual-
Signal Router that combines semantic embed-
dings with structural meta-features to estimate
task difficulty. During training, the router self-
optimizes through a Cold Start to Iterative Evo-
lution paradigm, learning from its own routing
failures via on-policy negative feedback. Exper-
iments using LLM-as-a-Judge evaluation across
Software Engineering, Data Analysis, Scientific
Discovery, and Cybersecurity demonstrate that
CASTER reduces inference cost by up to 72.4%
compared to strong-model baselines while match-
ing their success rates, and consistently out-
performs both heuristic routing and FrugalGPT
across all domains.
1. Introduction
From
Multi-Agent
Collaboration
to
the
Cost-
Performance
Paradox.
The
evolution
of
Large
Language Models (LLMs) has shifted the AI frontier
toward Multi-Agent Systems (MAS). By decomposing
complex objectives into sub-tasks, MAS achieves emergent
intelligence essential for long-horizon domains like
software engineering (Hong et al., 2023) and scientific
discovery (Zhou et al., 2024). However, this scalability
1Qingdao Institute of Software, College of Computer Sci-
ence and Technology, China University of Petroleum (East China)
2Electrical and Computer Engineering Department, University of
Houston 3School of computing and artificial intelligence, South-
west Jiaotong University 4Shandong Key Laboratory of Intelligent
Oil & Gas Industrial Software. Correspondence to: Shaohua Cao
<shaohuacao@upc.edu.cn>.
Preprint. January 28, 2026.
is constrained by the Cost-Performance Paradox. MAS
workflows generate exponential context accumulation
(Packer et al., 2023), forcing a rigid binary choice: relying
exclusively on Strong Models (e.g., GPT-4o) incurs
prohibitive costs and latency (Chen et al., 2023), while
switching to Weak Models introduces a ”fragility of logic,”
where a single upstream error cascades into total task failure
(Yao et al., 2022). Balancing this trade-off is a significant
challenge for industrial MAS adoption.
Limitations of Existing Routing. Current routing tech-
niques are ill-suited for the dynamic nature of MAS. Heuris-
tic approaches relying on static metrics like query length
often fail to capture semantic complexity, as a concise,
logic-heavy prompt frequently demands more reasoning
power than a lengthy summarization task. Similarly, cas-
cading strategies such as FrugalGPT (Chen et al., 2023)
employ a ”try-and-fail” mechanism that introduces unac-
ceptable latency and risks polluting the shared context
with erroneous intermediate steps, thereby confusing subse-
quent agents. Furthermore, preference-based methods like
RouteLLM (Ong et al., 2024), which depend on human feed-
back (RLHF) or chatbot arena data, prove inadequate for
MAS; while effective for aligning single-turn conversations
with subjective user preferences, they lack the objective pre-
cision required for the rigorous, multi-step reasoning chains
essential to agentic workflows.
Our Approach: Context-Aware Neural Routing. We pro-
pose CASTER (Context-Aware Strategy for Task Efficient
Routing), a lightweight neural module designed to break the
rigid trade-off between performance and cost. Unlike static
configurations, CASTER acts as a dynamic decision-maker,
mapping task semantics, agent roles, and evolving context
to the most cost-effective model. By predicting the neces-
sity of expert-level reasoning, it dispatches simple sub-tasks
to weak models while reserving strong models for critical
reasoning bottlenecks (System overview in Figure 1).
Contributions.
• Framework: We introduce CASTER for MAS (e.g.,
LangGraph), integrating semantic embeddings with
role-specific features for granular, dynamic model allo-
1
arXiv:2601.19793v1  [cs.AI]  27 Jan 2026


--- Page 2 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
cation.
• Dataset: We construct a comprehensive benchmark
across four domains (Software, Data, Science, Secu-
rity) with stratified difficulty levels to evaluate routing
generalization.
• Methodology: We propose an On-Policy iterative
training pipeline. We empirically demonstrate that
naive random exploration leads to data pollution, val-
idating our approach of labeling difficulty based on
performance divergence.
• Results: Experiments show CASTER reduces token
costs by 72.4% while maintaining success rates com-
parable to an all-GPT-4o baseline, significantly outper-
forming cascading and random strategies.
2. Related Work
LLMs-based Agents and Multi-Agent Systems. The emer-
gence of Large Language Models (LLMs) has catalyzed the
development of autonomous agents capable of solving com-
plex tasks. Early works focused on single-agent capabilities,
but recent research has shifted towards Multi-Agent Sys-
tems (MAS) to emulate human-like collaboration. MetaGPT
(Hong et al., 2023) and ChatDev (Qian et al., 2024) in-
troduce Standard Operating Procedures (SOPs) into agent
workflows, assigning specific roles (e.g., product manager,
engineer) to LLMs to simulate software development com-
panies. Similarly, AutoGen (Wu et al., 2024) and Camel (Li
et al., 2023) facilitate complex task solving through com-
municative agents and role-playing frameworks. Further-
more, LangGraph 1advances this field by modeling agent
workflows as stateful graphs, enabling cyclic execution and
fine-grained control over multi-agent interactions. In the
domain of software engineering, TaskWeaver (Qiao et al.,
2023) and SWE-agent (Yang et al., 2024) further specialize
agents for code-first tasks and automated issue resolving.
A comprehensive review of these LLM-based multi-agent
systems and their applications in software engineering is
provided by (He et al., 2025).
Optimization of Multi-Agent Systems. While foundation
models like GPT-4 (Achiam et al., 2023) and open-weight
counterparts like Qwen (Bai et al., 2023) serve as robust
cognitive engines, the complexity of Multi-Agent Systems
(MAS) demands optimization beyond individual model in-
ference. Early efficiency efforts focused on cost-centric rout-
ing, such as FrugalGPT (Chen et al., 2023) and RouteLLM
(Ong et al., 2024), which dynamically allocate queries be-
tween strong and weak models. However, recent advance-
ments target the holistic optimization of agentic workflows.
1https://github.com/langchain-ai/langgraph
To automate system design, Agentic Supernet(Zhang et al.,
2025a) introduces an architecture search framework that
identifies optimal agent topologies, reducing the reliance
on manual engineering. Improving collaborative efficiency,
OSC(Zhang et al., 2025b) proposes a cognitive orchestra-
tion mechanism that dynamically aligns knowledge across
agents to mitigate communication overhead. Furthermore,
Multi-Agent Consensus Alignment(Samanta et al., 2025)
internalizes the self-consistency of agents by leveraging con-
sensus data, effectively distilling the reasoning capabilities
of a swarm into a single efficient model.
Benchmarks and Evaluation. Evaluation paradigms have
shifted from static response quality to dynamic agent behav-
iors. MT-Bench (Zheng et al., 2023) validated the ”LLM-
as-a-judge” approach for open-ended conversations. For
autonomous agents, AgentBoard (Chang et al., 2024) intro-
duced fine-grained progress metrics to measure incremental
advancements in partially observable environments. Re-
cently, MultiAgentBench (Zhu et al., 2025) expanded the
scope to collective intelligence, systematically assessing
coordination protocols and strategies within multi-agent
workflows.
Limitations of Prior Arts. Despite these significant strides,
a critical gap persists in the efficient deployment of MAS.
First, existing efficient inference strategies like FrugalGPT
and RouteLLM are predominantly tailored for single-turn
queries or independent tasks. They often treat requests
in isolation, failing to capture the evolving state depen-
dencies and long-horizon context inherent in cyclic agent
workflows. Secondly, while recent architectural optimiza-
tions (e.g., Agentic Supernet) address system topology, they
lack the granularity to dynamically adjust resource allo-
cation at the individual step level based on real-time task
difficulty. Ultimately, while recent advancements in agent
topology and communication protocols contribute to sys-
tem efficiency, the performance ceiling of MAS remains
fundamentally bounded by the intrinsic capabilities of the
underlying LLMs(Zhu et al., 2025). This necessitates a
routing mechanism that directly addresses model capability
rather than merely optimizing structure or conversational
preference.
3. Methodology
3.1. System Architecture
We build upon the LangGraph framework to implement a
stateful, cyclic multi-agent workflow. Unlike linear chains,
this graph-based structure supports the iterative loops es-
sential for complex problem-solving. Within this graph,
CASTER functions as a dynamic interceptor. Before con-
trol enters any agent node, CASTER analyzes the real-time
shared state to determine the optimal model backend (e.g.,
2


--- Page 3 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
C A S T E R
Router
Meta
 Features
Semantic
 Features
On-policy Negative Feedback
Cold Start
Iterative
 Evolution
Dynamic Task
 Generate
Prompt
GPT-4o
ProjectManager,
algorithm, injection,
x gboost, CISO,
BlueTeamLead,
simulation, firewall,
scapy, async...
Embedding
text-embedding-3-small
Software Engineering
" role" : " ProjectManager"
 " task" :" Analyze the user's request..."
Data Analysis
" role" : " DataScientist"
 " task" :" Develop a distributed trainin..."
Scientific Discovery
" role" : " Engineer"
 " task" :" Write a script to calculate..."
Cybersecurity
" role" : " SecurityEngineer"
 " task" :" Implement a multi-threaded..."
Software Engineering
Weak Model
Data Analysis
Strong Model
Scientific Discovery
Weak Model
Cybersecurity
Strong Model
COST
Multi-Agent
 Design
Benchmark
Software
 Engineering
Data
 Analysis
Scientific
 Discovery
Cybersecurity
Category
Capability
Overall
Multi-Model
GPT-4o
EXPERIMENT
Figure 1. The overall architecture of the CASTER framework. The system begins with mock data and dynamic task generation via
GPT-4o. The core Router integrates semantic and meta-features to dispatch tasks, evolving through cold start and on-policy negative
feedback mechanisms. Tasks are executed by domain-specific agents (Software, Data, Science, Security) and evaluated against a
comprehensive benchmark to ensure multi-model capability.
GPT-4o vs. GPT-4o-mini). This granular, step-level routing
not only optimizes costs on-demand but also enhances re-
silience: if a low-cost model leads to failure (e.g., rejection
by a Reviewer), the workflow seamlessly rolls back with an
adjusted strategy.
3.2. CASTER Design
3.2.1. CASTER ARCHITECTURE: DUAL-BRANCH
FEATURE FUSION NETWORK
In contrast to existing approaches relying on Reinforce-
ment Learning from Human Feedback (RLHF) (such as
RouteLLM (Ong et al., 2024)), our method is better posi-
tioned to leverage user interactions within the chat domain.
We implement a Dual-Branch Feature Fusion Network to
explicitly model the interaction between semantic and meta-
features. As shown in the implementation the model con-
sists of two parallel extraction branches followed by a fu-
sion classifier. We denote the learnable parameter set as
θ = {Wt, bt, Wm, bm, Wfuse, bfuse, wout}.
Feature Extraction Branches. The model processes het-
erogeneous inputs through separate encoding streams:
• Text Branch (Semantic): Given the input text (de-
noted as Xaug or T), we extract its high-dimensional
embedding xsem ∈RDin. This vector is projected into
a latent space of dimension Dsem via a dense layer
with Dropout regularization to prevent overfitting:
hsem = Dropout(ReLU(Wtxsem + bt)) ∈RDsem
(1)
where Wt ∈RDsem×Din.
• Meta Branch (Structural): The sparse meta-vector
vmeta ∈RDmeta is processed by a lightweight non-
linear projection to capture basic feature interactions:
hmeta = ReLU(Wmvmeta + bm) ∈RDstruct
(2)
where Wm ∈RDstruct×Dmeta.
Fusion and Inference.
The latent representations are
concatenated to form a joint feature vector hjoint
=
[hsem; hmeta] ∈RDsem+Dstruct. This vector is passed
through a fusion layer to learn non-linear dependencies be-
fore the final probability estimation:
p(Strong|x) = σ
 wT
out · ReLU(Wfusehjoint + bfuse)

(3)
where Wfuse ∈RDfuse×(Dsem+Dstruct) maps the joint
vector to a hidden bottleneck of dimension Dfuse.
3.3. Training Strategy
We established a two-stage paradigm: ”Cold Start” fol-
lowed by ”Iterative Evolution.” The router is initialized with
synthetic rule-based data for fundamental reasoning. Sub-
sequently, we transition to the critical stage of real-world
iteration. We observed that traditional ”random exploration”
strategies introduce significant noise—such as strong mod-
els successfully solving trivial tasks—which mislead the
router into becoming overly conservative and expensive.
Consequently, we discard random data collection in favor of
On-Policy trajectory data derived from the current optimal
router. We specifically target ”high-value” boundary sam-
ples, such as failures caused by misjudgment or successful
instances of cost reduction. This active learning mechanism,
3


--- Page 4 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
based on boundary cases, significantly enhances both train-
ing efficiency and the model’s generalization capabilities.
3.3.1. PRE-TRAINING AND COLD START STRATEGY
In reinforcement learning or online learning systems, the
stochastic nature of initial policies often leads to high ex-
ploration costs and slow convergence (the ”Cold Start Prob-
lem”). To equip the CASTER with basic discriminative
capabilities prior to real-world deployment, we propose a
supervised pre-training method based on Heuristic Data
Augmentation.
Seed Dataset Construction. We define three categories of
seed tasks based on complexity:
(1) Easy Tasks: Basic syntactic operations, labeled for the
Weak model (Label ≈0.1).
(2) Medium Tasks: Data cleaning and routine algorithms,
labeled as fuzzy boundaries (Label ≈0.5).
(3) Hard Tasks: Distributed architecture design and dead-
lock debugging, labeled for the Strong model (Label ≈0.9).
Automated Data Augmentation. To overcome the sparsity
of seed data, we designed a data augmentation engine. This
engine expands a single seed entry into 4–6 training samples
with identical semantics but varied phrasing by randomly
combining instruction prefixes (e.g., ”Write a Python script
to...”) with suffix constraints. Furthermore, we introduce
Uniform noise ϵ ∼U(−0.05, 0.05) to perturb the labels,
preventing the model from overfitting to specific difficulty
levels.
Meta-Feature Simulation. Since real-time runtime con-
text is unavailable during the cold-start phase, we built a
simulator to generate meta-features. Based on keyword
matching (e.g., detecting ”thread”, ”async”) and task diffi-
culty distributions, the simulator probabilistically generates
corresponding Role Vectors and Context Lengths, provid-
ing a complete input view for the Wide & Deep network.
Through these methods, we constructed an initial training set
of hundreds of samples with zero real-world data, enabling
the router to achieve baseline accuracy upon deployment
and effectively avoiding blind random exploration.
3.3.2. ITERATIVE FINE-TUNING VIA NEGATIVE
FEEDBACK
While the router acquires basic discriminative ability after
the cold start, it may still misjudge complex edge cases in
real-world scenarios. To further refine routing precision,
we designed a fine-tuning pipeline based on Real-world
Trajectories.
This pipeline introduces a ”Negative Feedback Learning”
mechanism, the core logic of which lies in the re-labeling
of historical data:
Reinforcing Success: If a task is successfully solved and
the label was Strong, we reinforce it as a positive sample
(Label=1.0). If the model was Weak, it is treated as a neg-
ative sample (Label=0.0 relative to the ”need for strong
model” probability) to encourage low-cost paths.
Correcting Failure: This is the crux of the fine-tuning.
If the system selected the Weak model but the task ulti-
mately resulted in failure (Outcome="FAILURE" and
Model="Fast"), we forcibly rectify the Ground Truth
of this sample to Strong (Label=1.0).
Essentially, this mechanism signals to the neural network:
”You chose a weak model to save costs on this task and failed;
next time you encounter similar features, you must select
the strong model.”
Furthermore, to prevent Catastrophic Forgetting during fine-
tuning, we employ a dynamic learning rate adjustment
strategy (StepLR), progressively decaying the learning rate
(γ = 0.5) as epochs increase, ensuring smoother conver-
gence of model weights around the optimal solution.
4. Results & Analysis
4.1. Experiments Setup
To validate robustness, we benchmarked CASTER across
Software, Data, Science, and Cybersecurity domains against
static baselines and the FrugalGPT cascade strategy. Using
a multi-modal ”LLM-as-a-Judge” framework, we assessed
performance and cost efficiency. The system employs a
lightweight router trained via offline pre-training and online
iterative refinement. Primary experiments standardized on
the Qwen and GPT-4o families; to mitigate self-preference
bias(Wataoka et al., 2024), we extended generalization tests
to diverse provider models, including Gemini, Claude, and
DeepSeek. We also conducted a comparative evaluation
against the FrugalGPT cascade strategy. Detailed configura-
tions are provided in Section A.
Some experiments results are shown in Section G. Likewise,
Table 2, Table 3, Table 1 provides an overview.
4.2. Model Evaluation
To validate the decision boundary, we analyzed CASTER’s
confidence scores across diverse domains (Figure 2). Re-
sults demonstrate a sharp, intuition-aligned polarization:
trivial tasks (e.g., ”Hello World”, ”Simple Calc”) yield
minimal probabilities (≈0.02), routing to weak models.
Conversely, complex scenarios like ”Multi-thread Crawler”
(0.91) or ”Three-Body Sim” (0.91) trigger scores signifi-
cantly above the threshold. In Security, the router success-
fully distinguishes routine summaries (0.33) from critical
exploit reviews (0.86). This confirms that the router captures
latent complexity features—such as logical dependency and
4


--- Page 5 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
operational risk—rather than merely memorizing templates.
Case 1: PM -
Make a Plan
(Simple)
Case 2: Coder -
Hello World
(Simple)
Case 3: Coder -
Multi-thread
Crawler
(Complex)
Case 4:
Reviewer -
Audit Code
(Critical)
0.0
0.2
0.4
0.6
0.8
1.0
Model Confidence Score
0.1315
0.0312
0.8186
0.9117
Neural Router Confidence (SOFTWARE Mode)
Threshold (0.5)
(a) Software Engineering
Case 1: Analyst
- Simple Plan
Case 2:
Scientist -
Print Head
Case 3:
Scientist -
Distributed
Training
Case 4:
Reviewer -
Subtle Logic
Trap
0.0
0.2
0.4
0.6
0.8
1.0
Model Confidence Score
0.1012
0.0334
0.8779
0.6802
Neural Router Confidence (DATA Mode)
Threshold (0.5)
(b) Data Analysis
Case 1:
Researcher -
Basic Laws
Case 2:
Engineer -
Simple Calc
Case 3:
Engineer -
Three-Body Sim
Case 4:
Reviewer -
Quantum Chaos
0.0
0.2
0.4
0.6
0.8
1.0
Model Confidence Score
0.1041
0.0220
0.8729
0.9072
Neural Router Confidence (SCIENCE Mode)
Threshold (0.5)
(c) Scientific Discovery
Case 1: CISO -
Policy Summary
Case 2: SecEng
- Simple Hash
Case 3: SecEng
- SSH Brute
Force
Case 4: Auditor
- Exploit
Review
0.0
0.2
0.4
0.6
0.8
1.0
Model Confidence Score
0.3326
0.1198
0.7080
0.8628
Neural Router Confidence (SECURITY Mode)
Threshold (0.5)
(d) Cybersecurity
Figure 2. CASTER Confidence Validation. Inference scores
across Software, Data, Science, and Security. The threshold (y =
0.5, dashed) separates simple tasks (blue, Weak Model) from
complex ones (yellow, Strong Model). Results confirm the router’s
efficacy in distinguishing task complexity.
4.3. Cost Evaluation
4.3.1. CUMULATIVE COST AND AVERAGE COST
We evaluated economic efficiency by analyzing both cu-
mulative expenditure over sequential tasks and unit-level
economics across all domains (Figure 10, Table 6). The re-
sults demonstrate that CASTER effectively breaks the rigid
trade-off between performance and cost. By dynamically
offloading trivial queries, our method achieves a significant
reduction in total expenditure—approximately 23.1% to
54.4% across all four domains—effectively flattening the
cost curve compared to the Force Strong baseline. This
efficiency is further corroborated by unit analysis(Figure 11,
Table 7), where CASTER lowers average inference costs,
exemplified by a drop from $0.039 to $0.018 per task in
Software Engineering and a 38.1% reduction in Science. In
stark contrast to the Force Weak strategy, CASTER occupies
a strategic ”sweet spot,” sustaining high-performance rea-
soning without the prohibitive financial overhead of static
allocation.
4.3.2. COST DISTRIBUTION
We visualized decision-making granularity via cost distribu-
tion (Figure 12, Table 8). Unlike the Force Weak baseline,
which discriminately allocates minimal resources regard-
less of difficulty, CASTER demonstrates a broad dynamic
range spanning the spectrum between weak and strong base-
lines. This confirms that the router achieves efficiency not
by being uniformly ”cheap,” but by intelligently reserving
expensive compute solely for high-complexity scenarios.
4.4. Quality Evaluation
While cost reduction is desirable, maintaining quality parity
is paramount. We conducted a comprehensive evaluation
across four semantic dimensions (e.g., Functional Correct-
ness in Software, Scientific Validity in Science, and Com-
pliance in Security), as detailed in Figure 13, Figure 14,
Figure 15,Figure 16,Table 9, Table 10,Table 11 Table 12.
4.4.1. OVERALL EVALUATION
Quantitative analysis (Figure 14,Table 10) confirms that
CASTER effectively bridges the performance gap and, no-
tably, surpasses the static strong baseline in specific domains.
In Software Engineering and Data Analysis, CASTER
achieves scores of 85.0 and 78.0, significantly recovering
the quality drops seen in weak baselines (83.8 and 76.8) and
closely approaching the upper bound. Crucially, in Science
and Security, CASTER outperforms the Force Strong strat-
egy, achieving 95.3 (vs. 95.2) and 86.2 (vs. 85.5). These
results validate that dynamic routing not only optimizes re-
source allocation but can also mitigate the ”over-thinking”
or overfitting sometimes exhibited by strong models on sim-
pler sub-tasks.
4.4.2. GRANULAR ANALYSIS: CATEGORIES,
CAPABILITIES, AND MULTI-MODEL EVALUATE
Decomposing performance into fine-grained dimensions
reveals that CASTER effectively mitigates the ”worst-case”
risks of weak models.
Category Evaluation. Granular analysis (Figure 13, Ta-
ble 9) identifies critical vulnerability points. In special-
ized scenarios like Web Security and Software Concurrency,
weak baselines exhibit severe instability, plummeting to 48.0
and 67.0 due to logic deficits. CASTER effectively shields
the system from these failures, restoring robust scores of
86.0 and 83.0, respectively. Furthermore, in categories like
Software Security and Data Analytics, CASTER not only re-
covers performance but surpasses the strong baseline (reach-
ing 98.0 and 91.0), demonstrating superior routing preci-
sion.
Capability Evaluation. Further multi-dimensional assess-
ment (Figure 15,Table 11) shows that weak models suf-
fer from ”logic collapse,” particularly in Science Robust-
ness (16.0 vs. 19.2) and Science Parameter Accuracy (35.2
vs. 38.5), failing to adhere to strict physical constraints.
CASTER successfully mitigates these deficits, restoring
Software Functional Correctness to 34.5 (near the Strong
baseline’s 35.2). Notably, in Security, CASTER even outper-
forms the Strong baseline in Safety & Ethical Compliance
5


--- Page 6 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
Table 1. Comprehensive comparison of Cost, Reduction, and Quality. This table integrates economic efficiency with performance
metrics. CASTER consistently achieves significant cost reductions while maintaining (or surpassing) the quality scores of Strong baselines
across most scenarios.
Metrics
Scenario
Model
Strategy
Cost ($)
Red. (%)
Score
Software
claude
Force Strong
2.8204
-
100.0
Force Weak
0.8858
68.6%
93.3
CASTER
1.3738
51.3%
96.4
deepseek
Force Strong
0.2346
-
98.6
Force Weak
0.1391
40.7%
98.3
CASTER
0.1106
52.9%
98.1
gemini
Force Strong
1.5087
-
100.0
Force Weak
0.4412
70.8%
99.8
CASTER
0.8783
41.8%
100.0
openai
Force Strong
1.4658
-
95.3
Force Weak
0.0498
96.6%
82.2
CASTER
0.4052
72.4%
97.0
qwen
Force Strong
0.0397
-
99.3
Force Weak
0.0138
65.2%
97.8
CASTER
0.0186
53.1%
100.0
Data
claude
Force Strong
3.2237
-
83.3
Force Weak
0.4641
85.6%
80.7
CASTER
0.9186
71.5%
84.6
deepseek
Force Strong
0.2193
-
73.4
Force Weak
0.2268
-3.4%
69.2
CASTER
0.1332
39.3%
78.1
gemini
Force Strong
1.8052
-
53.6
Force Weak
0.4665
74.2%
52.1
CASTER
0.9466
47.6%
53.6
openai
Force Strong
1.1514
-
75.5
Force Weak
0.0452
96.1%
73.1
CASTER
0.4533
60.6%
75.4
qwen
Force Strong
0.0456
-
73.5
Force Weak
0.0136
70.2%
70.4
CASTER
0.0344
24.6%
73.6
Metrics
Scenario
Model
Strategy
Cost ($)
Red. (%)
Score
Science
claude
Force Strong
1.6530
-
96.7
Force Weak
0.4987
69.8%
94.8
CASTER
1.1596
29.8%
95.8
deepseek
Force Strong
0.1648
-
84.6
Force Weak
0.1660
-0.7%
89.5
CASTER
0.1429
13.3%
97.5
gemini
Force Strong
1.1737
-
94.2
Force Weak
0.2311
80.3%
95.0
CASTER
0.9259
21.1%
95.0
openai
Force Strong
1.2679
-
95.3
Force Weak
0.0550
95.7%
87.5
CASTER
1.0416
17.8%
95.4
qwen
Force Strong
0.1026
-
96.7
Force Weak
0.0276
73.1%
97.5
CASTER
0.0695
32.3%
97.6
Security
claude
Force Strong
2.1948
-
94.3
Force Weak
0.3346
84.8%
94.4
CASTER
0.9129
58.4%
95.1
deepseek
Force Strong
0.0983
-
91.1
Force Weak
0.1118
-13.7%
89.3
CASTER
0.1105
-12.4%
94.8
gemini
Force Strong
0.9837
-
95.6
Force Weak
0.1408
85.7%
94.8
CASTER
0.3064
68.9%
96.2
openai
Force Strong
0.4886
-
93.7
Force Weak
0.0207
95.8%
92.9
CASTER
0.2234
54.3%
93.9
qwen
Force Strong
0.0274
-
95.9
Force Weak
0.0100
63.5%
94.1
CASTER
0.0257
6.2%
95.2
(27.6 vs. 26.8), demonstrating that the router can leverage
model-specific strengths to enhance safety protocols beyond
static strong models.
Multi-Model Results. A multi-model component-wise in-
spection (Figure 16,Table 12) confirms that the decline in
weak models (average score 226.2) is primarily driven by
inferior CSV Data and Code Quality segments. CASTER
effectively restores the mass of these specific components,
achieving a total multi-modal score of 230.9—virtually iden-
tical to the Force Strong upper bound (232.4). This demon-
strates the router’s ability to recognize tasks requiring rigor-
ous data handling, ensuring high-fidelity outputs across all
granularities.
4.5. Compare to FrugalGPT
Economic Efficiency. The ”Double-Billing” Penalty. Fig-
ure 17, Table 13 exposes a critical vulnerability in Frugal-
GPT’s ”fail-then-retry” mechanism. When weak models fail
on difficult queries (e.g., in Science and Security), the cas-
cade triggers a fallback, incurring a ”double-billing” penalty
for both the failed weak inference and the subsequent strong
inference. Consequently, FrugalGPT’s cost curve steepens
significantly, eventually surpassing CASTER. In contrast,
CASTER leverages predictive capability to identify com-
plexity a priori. By implementing ”one-shot routing” for
hard tasks, it eliminates redundant weak calls, proving that
intelligent discrimination is economically superior to reac-
tive cascading.
Quality & Capability. Avoiding the ”Good Enough” Trap.
CASTER consistently outperforms FrugalGPT across all
domains (Figure 18, Figure 19, Table 14, Table 15). While
both strategies perform similarly on simple tasks, a distinct
gap emerges in complex scenarios, driving overall score
improvements of +0.7 to +1.2 points. This lead stems from
the cascade’s limitation: FrugalGPT often settles for ”good
enough” outputs that marginally pass thresholds but suffer
from weak-model noise. Conversely, CASTER avoids this
quality dilution by directly assigning complex tasks to the
6


--- Page 7 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
Table 2. Combined analysis of Unit Cost and Performance.
This table demonstrates the trade-off efficiency of CASTER. It sig-
nificantly reduces average unit costs (by 23.4%–54.3%) compared
to the Strong baseline while maintaining matching or superior
performance scores across all domains.
Cost
Performance
Scenario
Strategy
Avg. Cost
Reduction
Avg. Score
Software
Force Strong
$0.0392
-
87.5
Force Weak
$0.0029
92.6%
83.8
CASTER
$0.0179
54.3%
85.0
Data
Force Strong
$0.0466
-
78.5
Force Weak
$0.0043
90.8%
76.8
CASTER
$0.0255
45.3%
78.0
Science
Force Strong
$0.1339
-
95.2
Force Weak
$0.0054
96.0%
90.2
CASTER
$0.0831
37.9%
95.3
Security
Force Strong
$0.0064
-
85.5
Force Weak
$0.0021
67.2%
83.5
CASTER
$0.0049
23.4%
86.2
Table 3. FrugalGPT vs. CASTER. CASTER achieves a Pareto-
superior outcome, reducing total costs by 20.7%–48.0% while
surpassing FrugalGPT’s quality in all domains. Unlike cascade
methods, it improves efficiency without performance trade-offs.
Cost
Performance
Scenario
Strategy
Total Cost
Reduction
Avg. Score
Gain
Software
FrugalGPT
$1.11
-
79.8
-
CASTER
$0.58
48.0%
80.8
+1.0
Data
FrugalGPT
$0.66
-
72.3
-
CASTER
$0.41
38.4%
73.0
+0.7
Science
FrugalGPT
$0.91
-
90.9
-
CASTER
$0.59
35.3%
92.1
+1.2
Security
FrugalGPT
$0.29
-
81.2
-
CASTER
$0.23
20.7%
82.0
+0.8
strong model. Deeper metric decomposition reveals that this
advantage extends to critical dimensions such as Safety &
Compliance (e.g., 25.9 vs. 23.6 in Security) and Scientific
Validity (e.g., 29.1 vs. 28.1 in Science). CASTER’s pre-
emptive routing ensures deliverables benefit from superior
engineering standards, yielding solutions that are not just
correct, but production-ready.
4.6. Comparative Analysis of LLMs
In this section, we present a comprehensive evaluation based
on a curated benchmark of 10 tasks, balanced equally be-
tween simple and complex difficulty levels (five tasks each).
Detailed experimental results are provided in the Section G.
4.6.1. COST ANALYSIS ACROSS PROVIDERS
Our evaluation of cumulative and average costs (Figure 20,
Figure 21, Table 16, Table 17) reveals critical insights into
economic efficiency. First, the Strong baseline consistently
exhibits steep cost trajectories across all domains, whereas
CASTER demonstrates robust generalizability, achieving
significant cost reductions ranging from 6.2% to 72.4% for
providers with distinct price tiers (excluding DeepSeek). We
observe pronounced disparities among providers: Claude,
OpenAI, and Gemini incur substantially higher costs than
Qwen and DeepSeek. Notably, Claude is the most expensive
and volatile due to frequent timeouts, with its per-task cost
in Cybersecurity reaching $0.219—nearly double OpenAI’s
baseline even after CASTER’s 58.4% reduction. In stark
contrast, Qwen and DeepSeek display exceptionally smooth
and affordable trajectories, with total accumulated costs
consistently remaining below $0.15. DeepSeek’s identical
pricing for strong and weak models links cost directly to
token usage, resulting in a counter-intuitive ”cost inversion”.
Ultimately, these findings validate CASTER’s robustness as
a cost-optimization framework, demonstrating particularly
potent savings for providers where the price gap between
strong and weak models is substantial.
4.6.2. PERFORMANCE EVALUATION (SCORE)
To address multi-agent timeouts, we adopted a ”milestone”
scoring methodology(Zhu et al., 2025) that credits partial
sub-task completion (Figure 22, Table 18). CASTER con-
sistently matches or exceeds Strong baselines. Notably, the
DeepSeek Force Strong (R1) baseline underperformed due
to reasoning-induced latency/timeouts, whereas CASTER
achieved higher scores by leveraging the efficient DeepSeek-
V3. Claude justified its premium cost with high quality de-
spite occasional timeouts, while Gemini collapsed in Data
Analysis despite strong performance elsewhere. Although
Force Weak occasionally outperformed strong models (e.g.,
Qwen in Science) due to stochasticity, it remains prone to
catastrophic failures, highlighting CASTER’s necessity for
stability. The general score decline reflects the complexity
of evaluating multi-modal artifacts in data analysis.
5. Conclusion and Future Work
We propose CASTER to address MAS cost challenges via
semantic and meta-feature integration and On-Policy train-
ing. This framework achieves an optimal cost-intelligence
balance. Validated across Software, Data, Science, and Se-
curity domains, CASTER is inherently domain-agnostic,
offering significant potential for broader applications such
as legal analysis and creative writing.
Impact Statement
This paper introduces CASTER to enhance LLM efficiency.
The primary societal impact is positive: by significantly
reducing inference costs and computational resource usage,
our method supports Green AI initiatives and lowers the
7


--- Page 8 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
carbon footprint of large-scale deployments. Additionally,
by making high-performance agentic workflows more af-
fordable, CASTER promotes the democratization of AI,
enabling resource-constrained individuals and organizations
to leverage advanced capabilities. We foresee no specific
ethical risks beyond those inherent to the underlying LLMs.
References
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,
Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,
Anadkat, S., et al. Gpt-4 technical report. arXiv preprint
arXiv:2303.08774, 2023.
Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan,
Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical
report. arXiv preprint arXiv:2309.16609, 2023.
Chang, M., Zhang, J., Zhu, Z., Yang, C., Yang, Y., Jin, Y.,
Lan, Z., Kong, L., and He, J. Agentboard: An analytical
evaluation board of multi-turn llm agents. Advances in
neural information processing systems, 37:74325–74362,
2024.
Chen, L., Zaharia, M., and Zou, J. Frugalgpt: How to use
large language models while reducing cost and improving
performance. arXiv preprint arXiv:2305.05176, 2023.
He, J., Treude, C., and Lo, D. Llm-based multi-agent sys-
tems for software engineering: Literature review, vision,
and the road ahead. ACM Transactions on Software Engi-
neering and Methodology, 34(5):1–30, 2025.
Hong, S., Zhuge, M., Chen, J., Zheng, X., Cheng, Y., Wang,
J., Zhang, C., Wang, Z., Yau, S. K. S., Lin, Z., et al.
Metagpt: Meta programming for a multi-agent collabora-
tive framework. In The Twelfth International Conference
on Learning Representations, 2023.
Li, G., Hammoud, H., Itani, H., Khizbullin, D., and Ghanem,
B. Camel: Communicative agents for” mind” exploration
of large language model society. Advances in Neural
Information Processing Systems, 36:51991–52008, 2023.
Liu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., and Zhu, C.
G-eval: Nlg evaluation using gpt-4 with better human
alignment. arXiv preprint arXiv:2303.16634, 2023.
Ong, I., Almahairi, A., Wu, V., Chiang, W.-L., Wu, T., Gon-
zalez, J. E., Kadous, M. W., and Stoica, I. Routellm:
Learning to route llms with preference data.
arXiv
preprint arXiv:2406.18665, 2024.
Packer, C., Fang, V., Patil, S., Lin, K., Wooders, S., and
Gonzalez, J. Memgpt: Towards llms as operating systems.
2023.
Qian, C., Liu, W., Liu, H., Chen, N., Dang, Y., Li, J., Yang,
C., Chen, W., Su, Y., Cong, X., et al. Chatdev: Commu-
nicative agents for software development. In Proceed-
ings of the 62nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pp.
15174–15186, 2024.
Qiao, B., Li, L., Zhang, X., He, S., Kang, Y., Zhang,
C., Yang, F., Dong, H., Zhang, J., Wang, L., et al.
Taskweaver:
A code-first agent framework.
arXiv
preprint arXiv:2311.17541, 2023.
Samanta, A., Magesh, A., Yu, Y., Wu, R., Jain, A., Jiang, D.,
Vidolov, B., Sajda, P., Efroni, Y., and Hassani, K. Internal-
izing self-consistency in language models: Multi-agent
consensus alignment. arXiv preprint arXiv:2509.15172,
2025.
Wataoka, K., Takahashi, T., and Ri, R. Self-preference
bias in llm-as-a-judge. arXiv preprint arXiv:2410.21819,
2024.
Wu, Q., Bansal, G., Zhang, J., Wu, Y., Li, B., Zhu, E., Jiang,
L., Zhang, X., Zhang, S., Liu, J., et al. Autogen: Enabling
next-gen llm applications via multi-agent conversations.
In First Conference on Language Modeling, 2024.
Yang, J., Jimenez, C. E., Wettig, A., Lieret, K., Yao,
S., Narasimhan, K., and Press, O. Swe-agent: Agent-
computer interfaces enable automated software engineer-
ing. Advances in Neural Information Processing Systems,
37:50528–50652, 2024.
Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,
K. R., and Cao, Y. React: Synergizing reasoning and
acting in language models. In The eleventh international
conference on learning representations, 2022.
Zhang, G., Niu, L., Fang, J., Wang, K., Bai, L., and Wang,
X. Multi-agent architecture search via agentic supernet.
arXiv preprint arXiv:2502.04180, 2025a.
Zhang, J., Fan, Y., Cai, K., Tang, J., Sun, X., and Wang, K.
Osc: Cognitive orchestration through dynamic knowledge
alignment in multi-agent llm collaboration. Rn, 100(R1):
R2, 2025b.
Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,
Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging
llm-as-a-judge with mt-bench and chatbot arena. Ad-
vances in neural information processing systems, 36:
46595–46623, 2023.
Zhou, Y., Liu, H., Srivastava, T., Mei, H., and Tan, C. Hy-
pothesis generation with large language models. arXiv
preprint arXiv:2404.04326, 2024.
8


--- Page 9 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
Zhu, K., Du, H., Hong, Z., Yang, X., Guo, S., Wang, D. Z.,
Wang, Z., Qian, C., Tang, R., Ji, H., et al. Multiagent-
bench: Evaluating the collaboration and competition of
llm agents. In Proceedings of the 63rd Annual Meeting
of the Association for Computational Linguistics (Volume
1: Long Papers), pp. 8580–8622, 2025.
9


--- Page 10 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
A. Experiment.
We compared CASTER against Force Strong and Force Weak strategies, evaluating metrics such as Token Cost, Success
Rate, and Output Quality. While primary experiments utilized the Qwen and GPT-4o families, we further conducted
extensive generalization tests on other leading architectures to verify cross-model adaptability.
A.1. Experimental Implementation and Training Protocol
To validate the effectiveness of our proposed Context-Aware Strategy for Task Efficient Routing (CASTER), we implemented
a rigorous training strategy. The protocol consists of two distinct phases: offline supervised pre-training and online iterative
refinement. The description of training algorithm is provided in Algorithm 2.
A.1.1. DATASET PREPARATION
Synthetic Warm-up Dataset (Dpre). For the cold start phase, we constructed a focused seed dataset containing repre-
sentative tasks across Easy, Medium, and Hard tiers. To overcome data sparsity, we applied the automated augmentation
engine to expand this seed set by a factor of 4 to 6. Crucially, we injected Uniform noise ϵ ∼U(−0.05, 0.05) to the labels
during augmentation. This prevents the router from overfitting to the discrete difficulty levels of the seed tasks, fostering a
continuous probability distribution.
Dynamic Trajectory Dataset (Dtraj). To mitigate the critical scarcity of high-quality training trajectories, we developed
an automated Dynamic Task Generator. This stochastic adversarial pipeline synthesizes a full spectrum of tasks—ranging
from trivial routines to high-complexity challenges—to accurately map the decision boundary between weak and strong
models. Executed within an isolated Workspace Sandbox to prevent conflicts, the system captures real-time interaction logs
(including Task Context, Model Used, and Execution Outcome) to construct the Dynamic Trajectory Dataset (Dtraj). This
closed-loop mechanism creates a self-evolving data flywheel, ensuring diverse and balanced training coverage. Detailed
generation logic is provided in Section C.
A.1.2. TRAINING PROTOCOL
Phase 1: Cold Start Pre-training. We initialized the CASTER using Dpre. The model was trained for 200 epochs using
Binary Cross Entropy (BCE) loss with an initial learning rate of 1e−3. This extended training duration ensures the model
sufficiently fits the ”Hard” samples which are under-represented in the initial distribution.
Phase 2: Online Iterative Refinement. In this phase, we engaged the Negative Feedback Learning mechanism. We
employed the Re-labeling Logic to correct ”False Negative” samples (where a weak model was chosen but failed) to a label
of 1.0 (Strong). The router was fine-tuned on these trajectories using an Adam optimizer with a conservative learning rate of
1e−4. To ensure stable convergence, we utilized a StepLR scheduler, decaying the learning rate by a factor of γ = 0.5 every
50 epochs.
A.1.3. IMPLEMENTATION DETAILS
The CASTER is implemented as a lightweight Dual-Branch network using PyTorch. We provide the specific hyperparameter
settings and their rationales below:
Dimensionality Configuration. The input dimension Din is set to 1536, consistent with the output vector size of the
text-embedding-3-small model used for semantic extraction. The meta-feature dimension Dmeta is fixed at 6,
which explicitly encodes the task structure: a 4-dimensional one-hot vector for agent roles (Product Manager, Architect,
Engineer, Reviewer), combined with 1 normalized context length scalar and 1 high-risk keyword indicator. To balance
inference latency and representational capacity, we employed a bottleneck design for the hidden layers. The semantic
branch projects high-dimensional text vectors into a compact Dsem = 128 space (with Dropout p = 0.2), while the
sparse meta-features are mapped to Dstruct = 16. These are fused into a joint representation of 144 dimensions and then
compressed into a bottleneck of Dfuse = 64 before the final classification. This lightweight architecture ensures negligible
overhead during routing.
Experimental Setup. Our experimental framework is built upon a diverse ecosystem of Large Language Models (LLMs)
accessed via standardized APIs, ensuring reproducibility and simulating real-world Model-as-a-Service (MaaS) environments.
The model configuration is divided into three specific phases:
10


--- Page 11 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
• Data Construction & Accumulation: We adopted a dual-model strategy for training data generation. GPT-4o served
as the ”Task Generator” (Questioner) to synthesize diverse problem sets with stratified difficulty. Subsequently, we
primarily utilized the Qwen model family (qwen-max and qwen-plus) to generate high-quality supervision signals and
difficulty labels for these tasks.
• Primary Evaluation & Benchmarking: For the core comparative experiments, we standardized on the GPT-4o
series, designating gpt-4o as the ”Strong Model” baseline and gpt-4o-mini as the cost-effective ”Weak Model”.
Crucially, consistent with the ”LLM-as-a-Judge” paradigm, GPT-4o was also exclusively employed as the ”Reviewer”
to assess the multi-dimensional quality of the generated outputs.
• Generalization Testing: To verify the router’s robustness across different architectures, we extended our evaluation to
include other leading model families, including Gemini, Claude, and DeepSeek.
Table 4. Hyperparameter settings and structural specifications of the CASTER.
Parameter
Value
Network Architecture
Input Dimension (Din)
1536
Meta Dimension (Dmeta)
6
Semantic Hidden (Dsem)
128
Structural Hidden (Dstruct)
16
Fusion Hidden (Dfuse)
64
Output Dimension
1
Table 5. Pricing structure for Large Language Models used in experiments. Costs are denoted in USD per 1 million tokens.
Provider
Model
Input Cost ($/1M)
Output Cost ($/1M)
OpenAI
gpt-4o
$3.750
$15.000
gpt-4o-mini
$0.225
$0.900
Anthropic
claude-sonnet-4-5
$4.500
$22.500
claude-3-5-haiku-20241022
$1.500
$7.500
Google
gemini-2.5-pro
$1.875
$15.000
gemini-2.5-flash
$0.450
$3.750
DeepSeek
deepseek-reasoner (R1)
$0.825
$2.550
deepseek-chat (V3.2)
$0.825
$2.550
Alibaba
qwen3-max
$0.440
$1.780
qwen-plus
$0.110
$0.280
A.2. Basic Model Test
Following the cold-start training phase, we conduct a preliminary evaluation to verify that the model possesses the
foundational capabilities required for processing batch tasks, which in turn facilitates further performance improvements.
Detailed testing procedures are provided in Section E.
A.3. Benchmark
We developed a comprehensive evaluation framework (Zheng et al., 2023; Liu et al., 2023) to assess the execution
performance of agents across four distinct domains. Each domain comprises a curated suite of 20 manually selected tasks,
balanced equally between ”Easy” and ”Hard” difficulty levels to ensure a rigorous assessment spectrum. Crucially, to
validate true generalization, we enforced a strict separation between training and evaluation data: these benchmark tasks
were explicitly held out from both the router’s offline pre-training corpus and its online refinement pipeline, preventing
11


--- Page 12 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
any potential data leakage. While providing a unified standard, this module is specifically adapted to handle multi-modal
outputs (e.g., tabular data and visualizations) inherent to data analysis tasks. For comparative analysis, we tailored the test
subsets to specific experimental goals: the FrugalGPT comparison utilizes the 10 most challenging tasks per domain to test
high-complexity reasoning, while the cross-provider LLM benchmarking employs a representative 10-task subset (5 Easy, 5
Hard) to evaluate generalizability. Detailed specifications are provided in Section F, with the respective benchmark prompts
illustrated in Figure 6, Figure 7, Figure 8, and Figure 9.
B. Multi-agent Design.
As illustrated in Figure 3, we design domain-specific multi-agent workflows for diverse complex tasks. We depict the
architectures for Software Engineering, Data Analysis, Scientific Discovery, and Cybersecurity, respectively. Despite their
domain differences, these architectures collectively adhere to a ”Linear Initialization + Iterative Loop” design pattern:
1. Initialization Phase: This phase defines the scope and strategy. In the Software Engineering workflow, the Project
Manager and Architect decompose tasks and design the system architecture. In Data Analysis, the Lead Analyst plans
requirements, followed by the Data Strategist for data profiling. For Scientific Discovery, the Researcher conducts a
literature review, enabling the Theorist to formulate precise hypotheses. In Cybersecurity, the CISO establishes the
engagement rules, and a Team Router assigns the mission to either a Red Team Lead or Blue Team Lead to generate a
specialized toolset.
2. Iterative Execution Loop: This is the core engine of the workflow. A domain-specific execution agent—such as a
Coder, Data Scientist, Computational Scientist, or Security Engineer—generates the primary content (code, scripts, or
simulations). This output is then rigorously evaluated by a corresponding Reviewer (e.g., Peer Reviewer or Compliance
Officer). The review process integrates our Reviewer Router logic:
• Credit Assignment: Upon Acceptance or Rejection by the Reviewer, the system automatically logs a SUCCESS
or FAILURE tag, facilitating data collection for training the CASTER.
• Circuit Breaker: To prevent infinite loops, a circuit breaker is triggered when the retry count reaches a maximum
threshold. This forces a logical termination or transition while logging a ”Failure” experience, ensuring system
robustness and accumulating negative samples.
3. State Transition: For multi-step tasks, specialized utility nodes like advance file, advance script, or advance tool (in
Security) act as stateless managers. They are responsible for cleaning up feedback from the previous iteration, resetting
retry counters, and advancing the task index, ensuring that the next sub-task begins in a clean and isolated state.
C. Automated Dataset for Training Construction via Adversarial Generation.
The scarcity of high-quality, open-ended evaluation datasets for vertical domains remains a significant bottleneck in Multi-
Agent System (MAS) research. Existing benchmarks are often static, generic, or saturated, failing to adequately differentiate
between the reasoning capabilities of state-of-the-art models. To address this, we introduce a Dynamic Adversarial Task
Generation Pipeline. This pipeline leverages a strong teacher model (GPT-4o) to synthesize domain-specific, high-complexity
challenges designed to stress-test agent workflows. (Algorithm 3 and Figure 5)
C.1. Stochastic Difficulty Stratification
To mimic real-world request distributions, we implement a stochastic difficulty injection mechanism. The generator
dynamically toggles between Normal Mode (p = 0.3) and Hard Mode (p = 0.7).
• Normal Mode: Focuses on standard, linear tasks (e.g., basic API implementation or descriptive statistics) to verify
baseline functional correctness.
• Hard Mode (Expert Level): Explicitly designed to widen the performance gap between strong and weak models. In
this mode, the generator acts as an adversarial agent, injecting complex constraints that require multi-step reasoning,
long-context understanding, and edge-case handling—areas where weaker models typically hallucinate or fail.
C.2. Domain-Specific Adversarial Constraints
A key contribution of our benchmark is the formulation of Domain-Specific Constraints that target the unique vulnerabilities
of LLMs in each vertical field. As shown in Algorithm 3, we prompt the generator with specialized ”Personas” and ”Critical
12


--- Page 13 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
START
ProjectManager
Architect
Coder
Reviewer
Log SUCCESS
Log FAILURE
Force to Pass
 (Log FAILURE)
Accept
 or Reject?
Retry >= MAX?
END
Is there a
 next file?
Advance File
 Node
YES
NO
NO
Reject
Accept
YES
1. index + 1
 2. Clean Feedback
 3. Clean Pending Log
Software
 Engineering
START
Lead Analyst
Data Strategist
Data Scientist
Insight Reviewer
Log SUCCESS
Log FAILURE
Force to Pass
 (Log FAILURE)
Accept
 or Reject?
Retry >= MAX?
END
Is there a
 next script?
Advance Script
 Node
YES
NO
NO
Reject
Accept
YES
1. index + 1
 2. Clean Feedback
 3. Clean Pending Log
Data
 Analysis
START
Researcher
Theorist
Engineer
Reviewer
Log SUCCESS
Log FAILURE
Force to Pass
 (Log FAILURE)
Accept
 or Reject?
Retry >= MAX?
END
Is there a
 next file?
YES
NO
NO
Reject
Accept
YES
Scientific
 Discovery
START
Engineer
Compliance Officer
Log SUCCESS
Log FAILURE
Force to Pass
 (Log FAILURE)
Audit Router
Team Router
Retry >= MAX?
END
Is there a
 next tool?
Advance Tool
 Node
YES
NO
NO
Reject
Accept
YES
1. index + 1
 2. Clean Feedback
 3. Clean Pending Log
Red Team Lead
Blue Team Lead
CISO
Cybersecurity
Figure 3. Overview of the Domain-Specific Multi-Agent Workflows.
13


--- Page 14 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
Requirements”:
• Software Engineering (The ”Concurrency Trap”): Unlike simple algorithmic problems, our Hard Mode forces the
generation of tasks involving asynchronous programming (asyncio), race conditions, and system design patterns (e.g.,
Caching Decorators). These tasks test the agent’s ability to manage shared state and prevent deadlocks, a common
failure mode for smaller models.
• Data Analysis (The ”Dirty Data Trap”): We reject clean, textbook datasets. The generator is instructed to simulate
”Real-world Entropy”, including inconsistent date formats, missing values, and outliers. Furthermore, we forbid
high-level wrappers (like Scikit-Learn shortcuts) for certain tasks, forcing agents to implement mathematical logic (e.g.,
Cosine Similarity) from scratch using NumPy, thereby testing mathematical derivation capabilities over library recall.
• Scientific Discovery (The ”Computational Rigor Trap”): Moving beyond simple fact-retrieval, tasks in this domain
require solving Differential Equations (ODEs), performing Monte Carlo simulations, or modeling Complex Systems
(e.g., Three-Body Problem). These tasks demand high precision in numerical computing and the ability to formulate
scientific hypotheses.
• Cybersecurity (The ”Offensive/Defensive Trap”): To evaluate safety and logic simultaneously, tasks include writing
functional Proof-of-Concepts (PoCs) for vulnerabilities (e.g., SQL Injection) or parsing raw firewall logs to identify
attack patterns. This tests the agent’s capability to reason about adversarial system interactions within a sandbox
environment.
C.3. Self-Evolving Diversity Control
To ensure the benchmark’s breadth, the pipeline employs a sliding window memory mechanism. The generator receives
a context of the N previously generated topics (e.g., previous topics[-3:]) and is penalized for semantic repetition. This
ensures a continuous stream of diverse challenges, preventing the agents from overfitting to specific problem types.
14


--- Page 15 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
D. Algorithm.
Algorithm 1 Context-Aware Strategy for Task Efficient Routing Inference
Input: Current Task xt, Context History H, Threshold τ
Output: Agent Action at, Updated History H′
{Step 1: Feature Extraction & Routing Decision}
Extract semantic embedding v ←Encoder(xt, H)
Compute confidence score s ←RouterNet(v)
if s > τ then
{High complexity: Route to Strong Model}
Select Model M ←Mstrong
else
{Low complexity: Route to Weak Model}
Select Model M ←Mweak
end if
{Step 2: Execution & State Update}
Generate action at ←M(xt, H)
Update history H′ ←H ∪{(xt, at)}
return at, H′
Operational Details of the CASTER:As formally presented in Algorithm 1, the inference mechanism transforms the static
model selection problem into a dynamic, context-dependent decision process.
(1) Semantic Perception: Unlike rule-based routers, our method first aggregates the current query xt with the entire
interaction history H. This ensures that the Encoder captures not just the explicit instruction, but also implicit constraints
from previous turns.
(2) Adaptive Routing: The core logic resides in Lines 6–12. By modulating the threshold parameter τ, the system allows
for a flexible trade-off between performance and cost. A higher τ creates a conservative router that only engages the
Mstrong backend for the most challenging tasks, thereby minimizing computational overhead.
(3) Closed-Loop Update: The algorithm concludes by executing the chosen model and appending the generated action at
back into the history buffer. This state update (Line 15) is crucial for multi-turn scenarios, ensuring that future routing
decisions remain consistent with the ongoing dialogue trajectory.
15


--- Page 16 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
Algorithm 2 Three-Stage Hybrid Training Strategy
1: Input: Seed Data Dseed, Target Size N, Teacher LLM MLLM
2: Output: Optimized Router Parameters θ∗
3: {Stage 1: Pre-training & Cold Start (Sec 2.1)}
4: Initialize dataset Dpre ←∅
5: for each seed task (x, ybase) ∈Dseed do
6:
Generate variations Xaug ←Augment(x)
7:
Add label noise y′ ←ybase + ϵ, where ϵ ∼U(−0.05, 0.05)
8:
Simulate meta-features vmeta ←Simulate(x)
9:
Dpre ←Dpre ∪{(Xaug, vmeta, y′)}
10: end for
11: Pre-train θ ←SupervisedTrain(Dpre)
12: {Stage 2: Automated Trajectory Generation (Sec 2.2)}
13: Initialize trajectory buffer Dtraj ←∅
14: while |Dtraj| < N do
15:
Generate dynamic task T ←MLLM(history)
16:
Execute T in Sandbox, capture Model used M and Outcome O
17:
Dtraj ←Dtraj ∪{(T, M, O)}
18: end while
19: {Stage 3: Fine-tuning via Negative Feedback (Sec 2.3)}
20: for each epoch e = 1 to Emax do
21:
for each sample (T, M, O) ∈Dtraj do
22:
Re-labeling Logic:
23:
if O = FAILURE and M = Fast then
24:
ygt ←1.0 {Correction: Force Strong}
25:
else if O = SUCCESS then
26:
if M = Strong then
27:
ygt ←1.0
28:
else
29:
ygt ←0.0
30:
end if
31:
end if
32:
Update θ minimizing L(Router(T), ygt)
33:
end for
34:
{Updated to match code: StepLR every 50 epochs}
35:
if e (mod 50) = 0 then
36:
η ←η · γ
37:
end if
38: end for
39: return θ
Algorithm 2 details the complete training lifecycle of the CASTER. The process is divided into three distinct phases to
ensure robustness from initialization to deployment.
Stage 1 (Cold Start) addresses the initialization problem by augmenting a small set of seed tasks with heuristic uniform
noise and simulated meta-features, establishing an initial decision boundary.
Stage 2 (Trajectory Generation) constructs a self-evolving data engine. A Teacher LLM dynamically generates diverse
tasks, which are executed in a sandbox to capture ground-truth trajectories.
Stage 3 (Iterative Refinement) implements our novel Negative Feedback mechanism. Crucially, the re-labeling logic
identifies instances where the ”Weak” model caused a failure and forcibly corrects the ground truth to 1.0 (Strong),
thereby penalizing under-provisioning in future routing decisions. Finally, a dynamic learning rate schedule ensures stable
convergence during fine-tuning.
You can see the Algorithm 2 process in the Figure 4.
16


--- Page 17 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
SEED DATA
SYNTHETIC DATASET
 (D_pre)
Generate
 Variations
Supervised
 Pre-training
StepLR Scheduler
 (Every 50 epochs: η ←η·γ)
Gradient Update θ
Initialized
 Params
Optimized Parameters
Simulate
 Meta-features
Inject Uniform Noise
 (ε ~ U[-0.05, 0.05])
Label = 1.0
 (Force Strong correction)
TRAJECTORY BUFFER
 (D_traj)
Dynamic Task Generator
Augmentation
 Engine
(X_aug, v_meta, y')
Capture (T, Model M, Outcome O)
Task (T)
Load Params for Inference
Iterative Refinement Loop
IF Failure AND Fast Model
Re-labeling Logic
 (Negative Feedback)
ELSE IF Success
Next Epoch
Label = (M==Strong ? 1.0 : 0.0)
 (Reinforce Success)
Supervised Pre-training & Cold Start
Automated Trajectory Generation
Iterative Fine-tuning
 via Negative Feedback
Teacher LLM
Sandbox Execution
2
1
3
Figure 4. Three-Stage Hybrid Training Strategy
17


--- Page 18 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
Algorithm 3 Dynamic Adversarial Task Generation
Input: Iteration i, History H
Parameters: Temperature τ = 1.1, HardModeProb p = 0.7
r ∼U(0, 1)
if r < p then
Mode ←HARD MODE
Constraints ←InjectAdversarialConstraints(Domain) {e.g., Dirty Data, Concurrency, PoC}
else
Mode ←NORMAL MODE
Constraints ←StandardConstraints(Domain)
end if
Prompt ←ConstructPrompt(Mode, Constraints, H)
Taski ←LLMGPT-4o(Prompt, τ)
H ←H ∪{Taski.topic}
return Taski
Algorithm 3 illustrates the workflow of the Dynamic Adversarial Task Generation pipeline. To mitigate the saturation
and redundancy often seen in static benchmarks, the algorithm employs a Stochastic Difficulty Stratification mechanism.
With a high probability of p = 0.7, the generator enters ”Hard Mode,” actively injecting domain-specific adversarial
constraints—such as dirty data in analytics or concurrency deadlocks in software engineering—to maximize the dis-
criminative gap between strong and weak models. Furthermore, by utilizing a high temperature setting (τ = 1.1) and
maintaining a sliding history window H, the algorithm ensures both the diversity and non-repetitiveness of the tasks,
facilitating a continuously evolving evaluation environment.
You can see the Algorithm 3 process in the Figure 5.
E. Basic Model Test.
E.1. Software Engineering Scenario Model Test
E.1.1. INFERENCE CAPABILITY VALIDATION
To visually assess the discriminative ability of the CASTER regarding task complexity, we designed a set of inference tests
covering four typical scenarios.
Test Case Design:
• Simple Cases: Case 1 (PM creating a plan) and Case 2 (Coder writing ”Hello World”). These tasks feature short
contexts and lack complex keywords. The expected score is low (< 0.5).
• Complex Cases: Case 3 (Coder fixing a multi-thread deadlock) and Case 4 (Reviewer conducting a security audit).
These tasks artificially increase context length (> 4000 tokens) via detailed design documents or vulnerable code
snippets and inject high-risk keywords (e.g., ”asyncio”, ”RACE CONDITION”). The expected score is high (> 0.6).
E.2. Data Analysis Scenario Model Test
E.2.1. INFERENCE CAPABILITY VALIDATION
To assess the CASTER’s ability to discriminate computational density and logical depth in data tasks, we designed a similar
set of four inference tests, focusing on the identification of ”computationally intensive” and ”logic trap” tasks.
Test Case Design:
• Simple Cases: Case 1 (LeadAnalyst planning a simple CSV load task) and Case 2 (DataScientist printing the head of a
DataFrame). These tasks involve only basic API calls with low computational load. The expected score is low (< 0.2).
18


--- Page 19 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
Start: Iteration i
End
Input: History H
Return Task i
Init Params: τ=1.1, p=0.7
Generate Random r ~ U(0,1)
Construct Prompt
Update History H
Set: HARD_MODE
Inject Adversarial
 Constraints
Set: NORMAL_MODE
Set Standard
Constraints
r < p ?
LLM Generation: GPT-4o
Dynamic Adversarial
 Task Generation
Figure 5. Dynamic Adversarial Task Generation
19


--- Page 20 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
• Complex Cases: Case 3 (DataScientist developing a Ray-based distributed anomaly detection pipeline) and Case
4 (InsightReviewer reviewing look-ahead bias and causal inference in high-frequency trading strategies). These
tasks feature artificially injected long contexts (> 8000 tokens) and contain keywords related to high computing
demand (e.g., ”Distributed Training”, ”Bayesian Optimization”) and advanced statistical terms (e.g., ”Look-ahead
Bias”, ”Do-Calculus”). The router is expected to recognize the high requirement for reasoning and domain knowledge,
yielding a very high score (> 0.8).
E.3. Scientific Discovery Scenario Model Test
E.3.1. INFERENCE CAPABILITY VALIDATION
To evaluate the CASTER’s precision in distinguishing between basic scientific fact-retrieval and complex computational
simulation tasks, we designed a specific set of inference tests covering Physics, Chemistry, and Quantum Mechanics
domains.
Test Case Design:
• Simple Cases: Case 1 (Researcher explaining Newton’s Second Law) and Case 2 (Engineer calculating the Molar Mass
of water). These tasks involve standard knowledge retrieval or elementary arithmetic operations with short context
lengths (< 300 tokens). The expected score is low (< 0.3), indicating they can be handled by the Weak Model.
• Complex Cases: Case 3 (Engineer simulating the Three-Body Problem with Figure-8 stability) and Case 4 (Reviewer
auditing a Quantum Chaos simulation code). These tasks involve long contexts (6000 −9000 tokens) and highly
specialized keywords (e.g., ”Runge-Kutta”, ”Hamiltonian diagonalization”, ”Unitary evolution”). The router is expected
to identify the need for high-precision numerical reasoning and deep theoretical understanding, yielding a high score
(> 0.8).
E.4. Cybersecurity Scenario Model Test
E.4.1. INFERENCE CAPABILITY VALIDATION
In the domain of cybersecurity, the router must distinguish between routine administrative tasks and high-risk offen-
sive/defensive operations that require strict logical robustness. We designed inference tests to validate the router’s sensitivity
to attack vectors and system-level auditing.
Test Case Design:
• Simple Cases: Case 1 (CISO summarizing a password policy) and Case 2 (SecurityEngineer writing a script for
a simple MD5 hash). These tasks represent standard compliance checks or trivial utility generation with minimal
execution risk. The expected score is low (< 0.3).
• Complex Cases: Case 3 (SecurityEngineer developing a multi-threaded SSH brute force tool) and Case 4 (Com-
plianceOfficer reviewing a C-language Kernel Exploit). These scenarios involve complex engineering requirements
(e.g., concurrency management with paramiko, handling socket timeouts) or deep vulnerability analysis (e.g., ”buffer
overflow”, ”ASLR/DEP bypass”, ”ring-0 instructions”). Due to the high requirement for code safety and logical rigor,
the expected score is high (> 0.85).
F. Quality Benchmark in Experiments
To ensure a rigorous and fair comparison, we established a standardized evaluation protocol. The execution environment,
dependencies, and scoring logic were isolated for each of the four scenarios. The evaluation logic is implemented using a
”Judge-Model” (GPT-4o) with structured outputs (Pydantic models) to ensure consistency. We selected GPT-4o specifically
for its superior alignment with human judgment benchmarks (Zheng et al., 2023) and its native multimodal capabilities,
which are critical for verifying non-textual artifacts (e.g., plots and charts) in the Data Analysis domain.
20


--- Page 21 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
F.1. Software Engineering Scenario
F.1.1. BENCHMARK FRAMEWORK
• Test Suite: 20 tasks covering Logic & Math (e.g., N-Queens), OOP Design (e.g., LRU Cache), and Scripting.
• Control Groups: CASTER (Ours), Force Strong (GPT-4o), and Force Weak (GPT-4o-mini).
• Metrics: Total Cost, Success Rate (based on exit codes/tracebacks), and Duration.
F.1.2. CODE QUALITY ASSESSMENT PROTOCOL
Since software tasks focus on logic implementation and production readiness, we employ a Code-Centric Evaluation. The
LLM Judge acts as a ”Principal Software Engineer” conducting a strict code review, evaluating the generated script against a
100-point rubric defined as follows:
• Functional Correctness (0-40 pts): The dominant metric. It assesses whether the implementation is flawless and meets
all strict constraints (e.g., thread safety, specific regex patterns), severely penalizing logical errors or hallucinations.
• Robustness & Security (0-30 pts): A critical dimension focusing on defensive programming. It evaluates input
validation, system error handling (e.g., IO/Network failures), and resource leak prevention (e.g., proper use of context
managers).
• Engineering Quality (0-20 pts): Assesses the elegance and modularity of the code. High scores are awarded for
”Pythonic” implementations (efficient use of generators, decorators, list comprehensions) and clear separation of
concerns, contrasting with verbose or ”spaghetti” logic.
• Code Style (0-10 pts): Evaluates compliance with professional documentation standards, specifically requiring full
Type Hints (e.g., typing.List), Google-style Docstrings, and strict PEP8 adherence.
• Final Score: SSE = Scorrectness + Srobustness + Sengineering + Sstyle.
The prompt is shown in Figure 6.
21


--- Page 22 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
You are a Principal Software Engineer conducting a strict Code Review.
 Most candidates fail. You must differentiate between 'working code' and
'production-grade code'.
 Evaluate based on the following rubric (Total 100):
 1. Functional Correctness (0-40 pts):
 - 40: Flawless implementation. Meets ALL constraints (e.g., strictly
thread-safe, precise regex).
 - 20-39: Functional but misses a minor constraint (e.g., writer starvation
in RWLock).
 - 0-19: Major logic errors, hallucinations (non-existent libs), or fails core
test case.
 2. Robustness & Security (0-30 pts) [CRITICAL]:
 - 30: Paranoid defensive programming. Validates ALL inputs, handles
system errors (e.g., IO, Network), prevents resource leaks (context
managers).
 - 15-29: Basic try-except blocks but misses edge cases (e.g., negative
inputs, empty files).
 - <15: Naive implementation. Assumes perfect input. Crashes on edge
cases.
 3. Engineering Quality (0-20 pts):
 - 20: Pythonic elegance. Uses generators, decorators, list
comprehensions efficiently. Modular design (separation of concerns).
 - 10-19: Working code but 'Java-style' verbose Python. Spaghetti logic.
 - <10: Redundant code, hardcoded values, poor variable naming.
 4. Code Style (0-10 pts):
 - 10: Full Type Hints (typing.List/Optional), Google-style Docstrings,
PEP8 compliant.
 - 5: Partial typing or docs.
 - 0: No types/docs.
Figure 6. Software Engineering Quality Assessment Prompt
22


--- Page 23 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
F.2. Data Analysis Scenario
F.2.1. BENCHMARK FRAMEWORK
The suite includes tasks ranging from Basic Visualization to Statistical Inference (e.g., A/B Testing) and Signal Processing
(e.g., FFT).
F.2.2. MULTI-MODAL ARTIFACT QUALITY ASSESSMENT
Unlike other domains, Data Analysis produces heterogeneous outputs (Code, CSV Data, Charts). We implemented a Hybrid
Evaluation Protocol that aggregates scores from three distinct evaluators:
1. Code Logic Evaluator (40% Weight):
Analyzes the Python script for:
• Correctness (40 pts): Methodology validation (e.g., checking for data leakage, correct statistical test selection).
• Code Style (30 pts): Pandas best practices (avoiding chained assignment).
• Robustness (20 pts): Handling of ‘NaNs‘ and dirty data.
• Efficiency (10 pts): Utilization of vectorization over loops.
2. Data Quality Evaluator (30% Weight):
We inspect generated CSV files using a ”Statistical Rules + Semantic Judge”
approach:
• Realism (0-10): The LLM judges if data distributions (e.g., sales figures) follow statistical realism rather than random
noise.
• Integrity (0-10): Checks for alignment between columns and data types.
• Hard Constraint: A penalty is applied if the missing value ratio exceeds 10%.
• Formula: Scsv = 40 + 3 × (Srealism + Sintegrity).
3. Visual Judge (30% Weight):
We employ GPT-4o-Vision to assess generated charts (PNG/JPG) as a human reviewer
would:
• Clarity: Visibility of legends, labels, and lack of overlapping elements.
• Completeness: Data coverage without truncation.
• Insight: Appropriateness of chart type for the underlying data trend.
Final Data Score:
ScoreData = 0.4 × Scode + 0.3 × Scsv + 0.3 × Simage
The evaluation prompt is shown in Figure 7.
23


--- Page 24 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
You are a strict Senior Data Scientist Reviewer. Evaluate the Python code
based on the following detailed rubric (Total 100):
 1. Correctness (0-40 pts):
 - 40: Perfect analysis logic. Correctly uses Pandas/Numpy/Sklearn APIs.
Generates/Loads data correctly. Meets ALL analysis goals.
 - 30-39: Logic is mostly correct but has minor issues (e.g., deprecated
API, slight calculation error).
 - <30: Major logic errors (e.g., data leakage, wrong statistical test), or
code fails to produce output.
 2. Code Style & Visualization (0-30 pts):
 - 30: Clean PEP8. Clear variable names. Visualization: Uses
`plt.savefig()` (Headless), includes titles/labels.
 - 15-29: Messy code. Visualization lacks labels or uses `plt.show()`.
 - <15: Unreadable code or missing required visualization.
 3. Robustness & Data Safety (0-20 pts):
 - 20: Path Safety: Relative paths only. Data Hygiene: Handles NaNs/Inf.
Checks file existence.
 - 10-19: Misses some checks (e.g., assumes perfect data).
 - <10: Hardcoded paths, ignores NaNs (crash risk), or uses
subdirectories without creation.
 4. Efficiency (0-10 pts):
 - 10: Vectorization: Uses Pandas/Numpy built-ins. No `for` loops over
rows.
 - <10: Uses inefficient loops (`iterrows`) or redundant copying.
Figure 7. Data Analysis Assessment Prompt (Including Artifact Eval)
24


--- Page 25 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
F.3. Scientific Discovery Scenario
F.3.1. BENCHMARK FRAMEWORK
Tasks emphasize numerical precision, covering Physical Simulation (N-Body), Mathematical Derivation (Runge-Kutta), and
Data Fitting.
F.3.2. RIGOR-ORIENTED ASSESSMENT PROTOCOL
In scientific computing, ”running without errors” is insufficient; the simulation must reflect physical reality. We adjusted the
rubric to prioritize rigor:
• Parameter & Constraint Accuracy (0-40 pts): A strict check ensuring the code uses exact physical constants (e.g.,
G = 6.674 × 10−11) and initial conditions provided in the prompt. Hardcoding errors result in severe penalties.
• Scientific Validity (0-30 pts): Evaluates the correctness of physical formulas (e.g., correct implementation of Kinetic
Energy equations).
• Robustness (0-20 pts): Focuses on numerical stability (e.g., setting random seeds, handling division-by-zero).
• Code Quality (0-10 pts): Checks if variable names carry physical meaning (e.g., ‘velocity‘ vs ‘v‘) for readability.
The prompt is shown in Figure 8.
25


--- Page 26 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
You are a Distinguished Scientific Reviewer (Nature/Science Editor).
Evaluate the simulation code logic. NOTE: Do NOT evaluate whether files
are actually saved (a separate system checks that). Focus on the CODE
LOGIC and PARAMETERS.
 1.Parameter & Constraint Accuracy (0-40 pts):
 - 40: Used the EXACT parameters (Mass, G, L, Initial Conditions, Time
span) provided in the prompt. No deviation allowed.
 [-5 pts] Parameter Deviation: Hardcoded values differ from the prompt
(e.g., used g=9.8 instead of g=9.81).
 [-5 pts] Blocking Code: Includes plt.show() (which blocks automation).
 [-10 pts] Wrong Logic: Solves a different problem than requested (e.g.,
Euler method when asked for Runge-Kutta).
 - <20: Completely ignores the prompt's scenario.
 2.Scientific Validity (0-30 pts):
 - 30: The physical/mathematical implementation is flawless. Formulas
are correct. Units are handled consistently.
 [-10]: Naive implementation (e.g., O(N^2) loops for simple sums) or
lack of vectorization where numpy is used.
 [-15]: Fundamental science error (e.g., Kinetic Energy = m*v, forgot the
0.5 or squared).
 3.Robustness (0-20 pts):
 - 20: Handles numerical edges (e.g., division by zero checks). Sets
random seeds for reproducibility.
 [-10]: Fragile code (no error handling for obvious failure modes).
 4.Code Quality (0-10 pts):
 - 10: Clean, Pythonic, PEP8-compliant, variable names have physical
meaning (e.g., velocity not v).
 - 0: Spaghetti code, unused imports, or extremely messy formatting.
Figure 8. Scientific Discovery Assessment Prompt
26


--- Page 27 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
F.4. Cybersecurity Scenario
F.4.1. BENCHMARK FRAMEWORK
The suite covers offensive and defensive operations, including Vulnerability Assessment, Cryptography, and Code Auditing.
F.4.2. SAFETY-FIRST ASSESSMENT PROTOCOL
For security tasks, the evaluation logic shifts from ”Efficiency” to ”Safety” and ”Operational Hygiene”.
• Functional Efficacy (0-40 pts): Logic correctness, such as valid protocol handling (IPv6 support) and correct crypto
implementation.
• Safety & Compliance (0-30 pts): Critical Dimension. Scripts must target ONLY ‘localhost‘ and contain explicit
authorization checks. Destructive commands without confirmation trigger a zero score for this dimension.
• Robustness & Automation (0-20 pts): Evaluates ”Zero-Config” capability—scripts must handle network timeouts
and permission errors gracefully.
• Cleanliness (0-10 pts): Differentiates between Professional Artifacts and Debris.
– Allowed/Encouraged: Structured logs (‘.log‘), report files (‘.json‘, ‘.csv‘).
– Penalized: Leaked file handles, raw debug dumps, or temporary files left on disk.
The prompt is shown in Figure 9.
27


--- Page 28 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
You are a Lead Security Auditor & Red Team Operator.
 Evaluate the submission based on the following FINE-GRAINED rubric.
 First, identify if the submission is a Script or a Static Artifact.
 1. Functional Efficacy & Logic (0-40 pts):
 - 35-40 (Expert): Flawless implementation. Handles edge cases (e.g.,
IPv6, varying banners). Crypto/Logic is standard-compliant.
 - 25-34 (Proficient): Works for the happy path but misses edge cases or
lacks efficiency (e.g., single-threaded when multi-threaded was implied).
 - 15-24 (Junior): Logic exists but has minor bugs (e.g., protocol
mismatch, off-by-one error) or uses lazy methods (`os.system` instead of
libs).
 - <15 (Fail): Stub code, hallucinated syntax, or non-functional logic.
 2. Safety & Ethical Compliance (0-30 pts):
 - 25-30 (Strict): Targets ONLY `localhost`/`127.0.0.1`. Includes clear
authorization checks or comments.
 - 15-24 (Loose): Targets localhost but lacks explicit safety guards or
disclaimers.
 - 0-14 (Dangerous):
 [-30 Penalty]: Targets real-world IPs (Google, Gov) without instruction.
 [-10 Penalty]: Destructive operations (e.g., `rm -rf`, `iptables -F`)
without user confirmation.
 3. Robustness & Automation (0-20 pts):
 - 18-20 (Production-Ready): Zero-Config. Runs without args. Handles
`socket.timeout`, `PermissionError`, and empty inputs gracefully.
 - 10-17 (Fragile): Runs but crashes on missing args or network timeouts.
Lacks `try-except` blocks.
 - <10 (Brittle): Hardcoded paths, syntax errors in config files, or infinite
loops.
  4. Quality & Cleanliness (0-10 pts):
 - 9-10 (Clean): Resource cleanup (sockets closed). Professional Artifacts:
Comprehensive logs (`.log`), detailed reports (`.json`, `.csv`), and
READMEs are Encouraged and counted as valid outputs. Only penalize True
Debris (e.g., empty temp files, raw debug dumps, `__pycache__`).
 - 5-8 (Messy): Spaghetti code, hardcoded credentials, or functional but
disorganized file outputs.
 - <5 (Dirty): Leaks file handles, leaves pollution ( junk files, random
binary dumps), or mixed indentation.
Figure 9. Cybersecurity Assessment Prompt
28


--- Page 29 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
G. Experiments Results.
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
Tasks Processed
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Total Accumulated Cost (USD)
Cumulative Cost Growth (SOFTWARE)
Strategy
force_weak
force_strong
neural
(a) Software Engineering
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
Tasks Processed
0.0
0.2
0.4
0.6
0.8
Total Accumulated Cost (USD)
Cumulative Cost Growth (DATA)
Strategy
force_strong
force_weak
neural
(b) Data Analysis
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
Tasks Processed
0.0
0.5
1.0
1.5
2.0
2.5
Total Accumulated Cost (USD)
Cumulative Cost Growth (SCIENCE)
Strategy
force_weak
neural
force_strong
(c) Scientific Discovery
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
Tasks Processed
0.00
0.02
0.04
0.06
0.08
0.10
0.12
Total Accumulated Cost (USD)
Cumulative Cost Growth (SECURITY)
Strategy
force_strong
force_weak
neural
(d) Cybersecurity
Figure 10. Cumulative Cost Trajectory Analysis Across Four Domains. The plots depict the accumulated token cost (in USD) over
a sequence of 20 tasks for three strategies: Force Strong (orange), Force Weak (grey), and the proposed CASTER (green). The results
highlight the economic efficiency of the neural routing mechanism, which significantly suppresses the cost growth rate compared to the
strong-model-only baseline while adapting to task complexity.
29


--- Page 30 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
Table 6. Cost efficiency analysis across strategies. The table summarizes the total accumulated reasoning cost after processing 20 tasks.
The CASTER significantly reduces financial overhead compared to the Force Strong baseline, achieving cost reductions ranging from
23.1% to 54.4% across all four domains without compromising task success rates.
Scenario
Strategy
Total Cost (USD)
Avg. Cost/Task
Cost Reduction
Software
Force Strong
$0.79
$0.040
-
Force Weak
$0.06
$0.003
92.4%
CASTER
$0.36
$0.018
54.4%
Data
Force Strong
$0.94
$0.047
-
Force Weak
$0.09
$0.005
90.4%
CASTER
$0.50
$0.025
46.8%
Science
Force Strong
$2.68
$0.134
-
Force Weak
$0.11
$0.006
95.9%
CASTER
$1.66
$0.083
38.1%
Security
Force Strong
$0.13
$0.006
-
Force Weak
$0.04
$0.002
69.2%
CASTER
$0.10
$0.005
23.1%
Table 7. Average unit inference cost analysis. Values represent the mean cost per task (in USD). While the Force Strong baseline incurs a
fixed high cost, the CASTER dynamically adjusts usage, achieving a unit cost significantly lower than the upper bound across all domains.
The implicit high variance (as seen in error bars) for the Neural strategy reflects its adaptive nature—invoking expensive models only
when necessary.
Scenario
Strategy
Avg. Cost per Task
Cost Savings
Software
Force Strong
$0.0392
-
Force Weak
$0.0029
92.6%
CASTER
$0.0179
54.3%
Data
Force Strong
$0.0466
-
Force Weak
$0.0043
90.8%
CASTER
$0.0255
45.3%
Science
Force Strong
$0.1339
-
Force Weak
$0.0054
96.0%
CASTER
$0.0831
37.9%
Security
Force Strong
$0.0064
-
Force Weak
$0.0021
67.2%
CASTER
$0.0049
23.4%
30


--- Page 31 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
force_weak
force_strong
neural
0.00
0.01
0.02
0.03
0.04
0.05
Cost (USD)
$0.0029
$0.0392
$0.0179
Average Cost per Task (SOFTWARE)
(a) Software Engineering
force_strong
force_weak
neural
0.00
0.01
0.02
0.03
0.04
0.05
Cost (USD)
$0.0466
$0.0043
$0.0255
Average Cost per Task (DATA)
(b) Data Analysis
force_weak
neural
force_strong
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
Cost (USD)
$0.0054
$0.0831
$0.1339
Average Cost per Task (SCIENCE)
(c) Scientific Discovery
force_strong
force_weak
neural
0.000
0.001
0.002
0.003
0.004
0.005
0.006
0.007
0.008
Cost (USD)
$0.0064
$0.0021
$0.0049
Average Cost per Task (SECURITY)
(d) Cybersecurity
Figure 11. Comparison of Average Cost per Task Across Domains. The bar charts illustrate the average token cost (in USD) incurred for
processing a single task under three strategies: Force Weak, Force Strong, and CASTER. Across the Software, Data, Science, and Security
domains, the CASTER consistently acts as a cost-efficient intermediary. It achieves a substantial reduction in expenditures—lowering the
average cost by approximately 38-54% compared to the Force Strong baseline—by dynamically allocating expensive compute resources
only when necessitated by task complexity.
31


--- Page 32 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
force_weak
force_strong
neural
0.00
0.02
0.04
0.06
0.08
0.10
Cost per Task (USD)
Cost Distribution (SOFTWARE)
(a) Software Engineering
force_strong
force_weak
neural
0.00
0.02
0.04
0.06
0.08
0.10
0.12
Cost per Task (USD)
Cost Distribution (DATA)
(b) Data Analysis
force_weak
neural
force_strong
0.00
0.05
0.10
0.15
0.20
0.25
Cost per Task (USD)
Cost Distribution (SCIENCE)
(c) Scientific Discovery
force_strong
force_weak
neural
0.000
0.005
0.010
0.015
0.020
0.025
Cost per Task (USD)
Cost Distribution (SECURITY)
(d) Cybersecurity
Figure 12. Distribution of reasoning costs per task across four domains. The box plots illustrate the cost variance for Force Weak,
Force Strong, and CASTER strategies. Unlike the static baselines which exhibit narrow, rigid cost ranges, CASTER displays a broad
dynamic range (e.g., spanning from $0.003 to $0.158 in Science). This high variance confirms that the router adaptively allocates
resources: spending minimally on simple queries while reserving budget for complex reasoning, effectively breaking the ”fixed-cost”
paradigm.
32


--- Page 33 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
Table 8. Distribution statistics of reasoning costs. Unlike static baselines which exhibit narrow cost variances (Low Std. Dev.), the
CASTER demonstrates a broad Dynamic Range. Its cost span covers the full spectrum (e.g., $0.004 - $0.172 in Science), confirming its
ability to adaptively switch between cheap and expensive models based on task difficulty.
Scenario
Strategy
Median Cost
Min Cost
Max Cost
Std. Dev. (σ)
Software
Force Weak
$0.002
$0.001
$0.008
0.002
Force Strong
$0.037
$0.012
$0.105
0.026
CASTER
$0.010
$0.003
$0.075
0.020
Data
Force Weak
$0.004
$0.000
$0.014
0.003
Force Strong
$0.043
$0.020
$0.117
0.023
CASTER
$0.018
$0.008
$0.103
0.027
Science
Force Weak
$0.005
$0.004
$0.010
0.002
Force Strong
$0.111
$0.071
$0.251
0.058
CASTER
$0.054
$0.004
$0.172
0.066
Security
Force Weak
$0.002
$0.001
$0.004
0.001
Force Strong
$0.005
$0.001
$0.024
0.005
CASTER
$0.005
$0.001
$0.008
0.002
Logic
OOP
Algo
Utility
DataStruct
Concurrency
Security
Architecture
Task Category
0
20
40
60
80
100
Average Score
75
90
99
62
90
88
95
82
100
88
97
62
80
67
85
80
78
90
96
65
70
83
98
86
Performance by Category (SOFTWARE)
Strategy
force_strong
force_weak
neural
(a) Software Engineering
Basic_Easy
Viz_Easy
Processing_Easy
Analytics_Hard
Viz_Hard
Engineering_Hard
Processing_Hard
Task Category
0
20
40
60
80
100
Average Score
69
88
61
80
91
75
88
64
87
68
80
91
64
88
70
90
62
91
88
62
78
Performance by Category (DATA)
Strategy
force_strong
force_weak
neural
(b) Data Analysis
Physics_Easy
Bio_Easy
Chem_Easy
Astro_Easy
Math_Easy
Env_Easy
Physics_Hard
Bio_Hard
Material_Hard
Chem_Hard
Astro_Hard
Env_Hard
Quantum_Hard
Task Category
0
20
40
60
80
100
Average Score
92
92
100
100
98
90
95
96
96
97
97
96
94
92
83
90
88
88
89
87
94
96
97
89
88
94
97
100
100
100
98
100
87
96
96
97
86
91
95
Performance by Category (SCIENCE)
Strategy
force_strong
force_weak
neural
(c) Scientific Discovery
Recon_Easy
Crypto_Easy
Web_Easy
Net_Easy
Defense_Easy
Exploit_Hard
Recon_Hard
Defense_Hard
Network_Hard
Crypto_Hard
Task Category
0
20
40
60
80
100
Average Score
92
88
84
85
86
85
83
82
95
78
88
90
48
84
92
87
90
83
90
90
88
88
86
89
90
85
88
82
85
80
Performance by Category (SECURITY)
Strategy
force_strong
force_weak
neural
(d) Cybersecurity
Figure 13. Performance breakdown by task category across four domains. The grouped bar charts detail the average scores across
specific sub-tasks in Software Engineering, Data Analysis, Scientific Discovery, and Cybersecurity. The results highlight the robustness of
CASTER (green): in high-complexity categories such as Concurrency, Defense Operations, and Quantum Simulation, the Force Weak
baseline (grey) suffers catastrophic performance drops (e.g., dropping to 48.0 in Security). In contrast, CASTER successfully identifies
these challenges and routes them to the strong model, achieving scores comparable to the Force Strong upper bound (orange).
33


--- Page 34 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
Table 9. Breakdown of task performance scores by category. The table highlights the robustness of the CASTER. In complex categories
such as Concurrency, Exploitation, and Quantum Sim, the Force Weak baseline suffers significant performance drops (highlighted in
red-like low scores). The CASTER successfully detects these complexities and routes them to the strong model, restoring performance to
near-upper-bound levels while maintaining high scores in simpler categories.
Scenario
Category
Force Strong
Force Weak
CASTER
Software
Logic
75
100
78
OOP
90
88
90
Algorithm
99
97
96
Utility
62
62
65
Data Struct.
90
80
70
Concurrency
88
67
83
Security
95
85
98
Architecture
82
80
86
Data
Basic
69
64
70
Visualization (Easy)
88
87
90
Processing (Easy)
61
68
62
Analytics
80
80
91
Visualization (Hard)
91
91
88
Engineering
75
64
62
Processing (Hard)
88
88
78
Science
Physics (Easy)
92
92
97
Biology (Easy)
92
83
100
Chemistry (Easy)
100
90
100
Astrophysics (Easy)
100
88
100
Mathematics (Easy)
98
88
98
Environmental Science (Easy)
90
89
100
Physics (Hard)
95
87
87
Biology (Hard)
96
94
96
Material Science (Hard)
96
96
96
Chemistry (Hard)
97
97
97
Astrophysics (Hard)
97
89
86
Environmental Science (Hard)
96
88
91
Quantum (Hard)
94
94
95
Security
Reconnaissance (Easy)
92
88
88
Cryptography (Easy)
88
90
88
Web Security
84
48
86
Network Security (Easy)
85
84
89
Defensive Security (Easy)
86
92
90
Exploitation
85
87
85
Reconnaissance (Hard)
83
90
88
Defensive Security (Hard)
82
83
82
Network Security (Hard)
95
90
85
Cryptography (Hard)
78
90
80
34


--- Page 35 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
force_strong
force_weak
neural
Strategy
70.0
72.5
75.0
77.5
80.0
82.5
85.0
87.5
90.0
Average Score
87.5
83.8
85.0
Overall Score (SOFTWARE Mode)
(a) Software Engineering Scenario
force_strong
force_weak
neural
Strategy
60.0
62.5
65.0
67.5
70.0
72.5
75.0
77.5
80.0
Average Score
78.5
76.8
78.0
Overall Score (DATA Mode)
(b) Data Analysis Scenario
force_strong
force_weak
neural
Strategy
80.0
82.5
85.0
87.5
90.0
92.5
95.0
97.5
100.0
Average Score
95.2
90.2
95.3
Overall Score (SCIENCE Mode)
(c) Scientific Discovery
force_strong
force_weak
neural
Strategy
70.0
72.5
75.0
77.5
80.0
82.5
85.0
87.5
90.0
Average Score
85.5
83.5
86.2
Overall Score (SECURITY Mode)
(d) Cybersecurity
Figure 14. Comparison of Overall Task Scores across Four Domains. The bar charts illustrate that CASTER (green) consistently
bridges the quality gap between the Force Weak baseline (grey) and the Force Strong upper bound (orange).
Table 10. Comparison of overall task performance scores. The table validates the effectiveness of our routing strategy. In the
Software Engineering scenario, the CASTER recovers most of the performance drop caused by the weak model. Notably, in Science and
Security scenarios, the CASTER achieves performance (95.3 and 86.2) that matches or even slightly surpasses the Force Strong baseline,
demonstrating that cost-aware routing can effectively mitigate overfitting or ”over-thinking” in simple tasks.
Scenario
Strategy
Average Score
Software
Force Weak
83.8
CASTER
85.0
Force Strong
87.5
Data
Force Weak
76.8
CASTER
78.0
Force Strong
78.5
Science
Force Weak
90.2
CASTER
95.3
Force Strong
95.2
Security
Force Weak
83.5
CASTER
86.2
Force Strong
85.5
35


--- Page 36 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
Functional Correctness
Code Style
Robustness & Security
Engineering Quality
0
5
10
15
20
25
30
35
40
Average Score (0-100)
35.2
8.5
26.0
17.8
31.8
8.5
25.2
16.5
34.5
9.2
25.2
16.1
Capability Comparison (SOFTWARE)
Strategy
force_strong
force_weak
neural
(a) Software Engineering
Correctness
Code Style & Visualization Robustness & Data Safety
Efficiency
0
5
10
15
20
25
30
35
40
Average Score (0-100)
37.4
27.1
16.8
10.0
37.9
27.6
17.8
9.9
37.4
27.4
17.4
9.9
Capability Comparison (DATA)
Strategy
force_strong
force_weak
neural
(b) Data Analysis
Parameter & Constraint Accuracy Scientific Validity
Robustness
Code Quality
0
5
10
15
20
25
30
35
40
Average Score (0-100)
38.5
28.8
19.2
9.4
35.2
27.5
16.0
9.8
38.2
29.5
18.2
9.9
Capability Comparison (SCIENCE)
Strategy
force_strong
force_weak
neural
(c) Scientific Discovery
Functional Efficacy & LogicSafety & Ethical ComplianceRobustness & Automation
Quality & Cleanliness
0
5
10
15
20
25
30
35
40
Average Score (0-100)
35.3
26.8
17.9
8.9
35.6
27.1
18.1
9.0
35.5
27.6
18.1
9.1
Capability Comparison (SECURITY)
Strategy
force_strong
force_weak
neural
(d) Cybersecurity
Figure 15. Multi-dimensional capability assessment across four domains. The grouped bar charts compare the performance of Force
Strong, Force Weak, and CASTER strategies across four key metrics. The results demonstrate that CASTER (green) achieves a dual
advantage: it not only recovers the deficit of the weak baseline (e.g., improving from 35.2 to 38.2 in Parameter & Constraint Accuracy of
Science) but also surpasses the Force Strong baseline in some scores across the Security and Software domains (e.g., 27.6 vs. 26.8 in
Cleanliness of Security), suggesting an optimization in output formatting and compliance.
force_strong
force_weak
neural
0
50
100
150
200
250
300
Average Points (0-100)
232.4
226.2
230.9
Score Composition (Data Mode)
Component
Code
CSV
Img
Figure 16. Component-wise score composition in Data Analysis tasks. The total score is decomposed into Code quality (grey), CSV
data quality (yellow), and Visualization quality (purple). The CASTER (230.9) successfully mitigates the degradation observed in the
weak baseline (226.2), achieving a multi-modal output quality nearly indistinguishable from the strong model (232.4).
36


--- Page 37 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
Table 11. Multi-dimensional quality evaluation. The results illustrate CASTER’s ability to maintain high-performance standards while
optimizing specific output traits. In complex reasoning tasks, CASTER closely mirrors the Force Strong baseline, achieving 34.5 in
Software ’Functional Correctness’ (vs. 35.2) and 38.2 in Science ’Parameter Accuracy’ (vs. 38.5), effectively avoiding the performance
dip seen in the Force Weak strategy (31.8 and 35.2, respectively). Notably, CASTER outperforms the Force Strong baseline in qualitative
metrics, securing the highest scores in Software ’Code Style’ (9.2) and Security ’Safety & Compliance’ (27.6), suggesting that dynamic
routing can leverage model-specific strengths to enhance formatting and adherence to safety protocols.
Scenario
Metric
Force Strong
Force Weak
CASTER
Software
Functional Correctness
35.2
31.8
34.5
Robustness & Security
26.0
25.2
25.2
Engineering Quality
17.8
16.5
16.1
Code Style
8.5
8.5
9.2
Data
Correctness
37.4
37.9
37.4
Code Style & Visualization
27.1
27.6
27.4
Robustness & Data Safety
16.8
17.8
17.4
Efficiency
10.0
9.9
9.9
Science
Parameter & Constraint Accuracy
38.5
35.2
38.2
Scientific Validity
28.8
27.5
29.5
Robustness
19.2
16
18.2
Code Quality
9.4
9.8
9.9
Security
Functional Efficacy & Logic
35.3
35.6
35.5
Safety & Ethical Compliance
26.8
27.1
27.6
Robustness & Automation
17.9
18.1
18.1
Quality & Cleanliness
8.9
9.0
9.1
Table 12. Component-wise score composition in Data Analysis. The evaluation metric aggregates scores from three generated outputs:
Executable Code, CSV Data Files, and Image Plots. The breakdown reveals that while the Force Weak baseline lags behind due to lower
quality in data and visual artifacts, the CASTER effectively recovers performance, particularly in CSV generation quality, matching the
Force Strong upper bound closely.
Strategy
Total Score
Component Breakdown
Code
CSV File
Image Plot
Force Weak
226.2
93.2
88.0
45.0
CASTER
230.9
92.0
93.5
45.4
Force Strong
232.4
91.3
92.7
48.5
Table 13. Comparative analysis of dynamic routing strategies. FrugalGPT (Cascade) vs. CASTER. The table summarizes the total
accumulated cost after processing 10 tasks. Results indicate that the CASTER consistently outperforms the Frugal strategy. By predicting
complexity upfront rather than relying on a ”fail-then-retry” cascade, the CASTER avoids the overhead of double-billing, achieving cost
reductions ranging from 20.7% to 48.0%.
Scenario
Strategy
Total Cost (USD)
Cost Reduction
Software
FrugalGPT (Cascade)
$1.11
-
CASTER
$0.58
48.0%
Data
FrugalGPT (Cascade)
$0.66
-
CASTER
$0.41
38.4%
Science
FrugalGPT (Cascade)
$0.91
-
CASTER
$0.59
35.3%
Security
FrugalGPT (Cascade)
$0.29
-
CASTER
$0.23
20.7%
37


--- Page 38 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
2
4
6
8
10
Tasks Processed
0.0
0.2
0.4
0.6
0.8
1.0
Total Accumulated Cost (USD)
Cumulative Cost Growth (SOFTWARE)
Strategy
frugal
neural
(a) Software Engineering
2
4
6
8
10
Tasks Processed
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Total Accumulated Cost (USD)
Cumulative Cost Growth (DATA)
Strategy
frugal
neural
(b) Data Analysis
2
4
6
8
10
Tasks Processed
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Total Accumulated Cost (USD)
Cumulative Cost Growth (SCIENCE)
Strategy
frugal
neural
(c) Scientific Discovery
2
4
6
8
10
Tasks Processed
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Total Accumulated Cost (USD)
Cumulative Cost Growth (SECURITY)
Strategy
frugal
neural
(d) Cybersecurity
Figure 17. Cumulative cost growth comparison between CASTER and FrugalGPT across a sequence of complex tasks. The
plot illustrates the accumulated token cost (USD) over 10 consecutive hard tasks (e.g., concurrency control, security architecture).
FrugalGPT(Cascade) exhibits a steeper cost trajectory due to the ”cascading overhead” incurred by failing with weak models before
upgrading. In contrast, CASTER minimizes cost by identifying task complexity upfront and directly routing to the strong model,
effectively bypassing wasteful trial-and-error iterations.
Table 14. Performance comparison of dynamic strategies. FrugalGPT (Cascade) vs. CASTER. The table compares the average
quality scores (0-100) across four domains. While FrugalGPT often settles for ”acceptable” outputs from weaker models to save cost, the
CASTER consistently achieves higher quality scores. Combined with Table 13 (Cost Analysis), this confirms that our approach yields a
Pareto-superior outcome: lower cost AND higher performance.
Scenario
Strategy
Average Score
Quality Gain
Software
FrugalGPT (Cascade)
79.8
-
CASTER
80.8
+1.0
Data
FrugalGPT (Cascade)
72.3
-
CASTER
73.0
+0.7
Science
FrugalGPT (Cascade)
90.9
-
CASTER
92.1
+1.2
Security
FrugalGPT (Cascade)
81.2
-
CASTER
82.0
+0.8
38


--- Page 39 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
frugal
neural
Strategy
70.0
72.5
75.0
77.5
80.0
82.5
85.0
87.5
90.0
Average Score
79.8
80.8
Overall Score (SOFTWARE Mode)
(a) Software Engineering
frugal
neural
Strategy
60.0
62.5
65.0
67.5
70.0
72.5
75.0
77.5
80.0
Average Score
72.3
73.0
Overall Score (DATA Mode)
(b) Data Analysis
frugal
neural
Strategy
80.0
82.5
85.0
87.5
90.0
92.5
95.0
97.5
100.0
Average Score
90.9
92.1
Overall Score (SCIENCE Mode)
(c) Scientific Discovery
frugal
neural
Strategy
70.0
72.5
75.0
77.5
80.0
82.5
85.0
87.5
90.0
Average Score
81.2
82.0
Overall Score (SECURITY Mode)
(d) Cybersecurity
Figure 18. Comparison of overall performance scores between CASTER and FrugalGPT across varying task difficulties. The chart
displays the average success rates achieved by both strategies on a benchmark containing 10 groups of high-difficulty tasks. CASTER
maintains a high performance level comparable to the strong model baseline (GPT-4o), particularly in complex logic and architectural
tasks. In contrast, FrugalGPT (Cascade) exhibits performance fluctuations on hard tasks, suggesting that relying on weak models for
initial attempts not only incurs higher costs but may also compromise final output quality due to flawed initial reasoning.
39


--- Page 40 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
Functional Correctness
Code Style
Robustness & Security
Engineering Quality
0
5
10
15
20
25
30
35
40
Average Score (0-100)
33.5
8.0
22.5
15.8
33.5
8.5
23.0
15.8
Capability Comparison (SOFTWARE)
Strategy
frugal
neural
(a) Software Engineering
Correctness
Code Style & Visualization
Robustness & Data Safety
Efficiency
0
5
10
15
20
25
30
35
40
Average Score (0-100)
37.8
26.8
16.3
10.0
37.1
26.3
16.4
10.0
Capability Comparison (DATA)
Strategy
frugal
neural
(b) Data Analysis
Parameter & Constraint Accuracy
Scientific Validity
Robustness
Code Quality
0
5
10
15
20
25
30
35
40
Average Score (0-100)
37.8
28.1
19.0
9.1
38.5
29.1
19.4
9.5
Capability Comparison (SCIENCE)
Strategy
frugal
neural
(c) Scientific Discovery
Functional Efficacy & Logic Safety & Ethical Compliance Robustness & Automation
Quality & Cleanliness
0
5
10
15
20
25
30
35
40
Average Score (0-100)
32.5
23.6
16.5
8.2
33.5
25.9
17.1
8.5
Capability Comparison (SECURITY)
Strategy
frugal
neural
(d) Cybersecurity
Figure 19. Comprehensive Cost-Performance comparison of CASTER, FrugalGPT, and GPT-4 Baseline across Software Engi-
neering and Data Analysis domains. The bar charts illustrate the Average Success Rate and Average Cost per Task for three strategies
in the Software and Data Analysis benchmarks. Results indicate that CASTER (green) achieves a ”Pareto optimal” balance in both
distinct domains: it rivals the strong baseline (GPT-4) in success rate while maintaining a cost profile comparable to the cost-aggressive
FrugalGPT, demonstrating its capability as a domain-agnostic CASTER.
Table 15. Multi-dimensional quality breakdown. FrugalGPT vs. CASTER. The table details the performance on four sub-metrics. The
CASTER consistently outperforms FrugalGPT in complex domains, particularly in Science (e.g., Scientific Validity +1.0) and Security
(e.g., Safety +2.3).
Scenario
Metric
FrugalGPT
CASTER
Software
Functional Correctness
33.5
33.5
Robustness & Security
22.5
23.0
Engineering Quality
15.8
15.8
Code Style
8.0
8.5
Data
Correctness
37.8
37.1
Code Style & Visualization
26.8
26.3
Robustness & Data Safety
16.3
16.4
Efficiency
10.0
10.0
Science
Parameter & Constraint Accuracy
37.8
38.5
Scientific Validity
28.1
29.1
Robustness
19.0
19.4
Code Quality
9.1
9.5
Security
Functional Efficacy & Logic
32.5
33.5
Safety & Ethical Compliance
23.6
25.9
Robustness & Automation
16.5
17.1
Quality & Cleanliness
8.2
8.5
40


--- Page 41 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
2
4
6
8
10
Task Sequence ID
0.0
0.5
1.0
1.5
2.0
2.5
Cumulative Cost (USD)
Cumulative Cost Trend by LLM & Strategy (SOFTWARE)
Strategy
force_strong
force_weak
neural
Provider
claude
deepseek
gemini
openai
qwen
(a) Software Engineering
2
4
6
8
10
Task Sequence ID
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Cumulative Cost (USD)
Cumulative Cost Trend by LLM & Strategy (DATA)
Strategy
force_strong
force_weak
neural
Provider
claude
deepseek
gemini
openai
qwen
(b) Data Analysis
2
4
6
8
10
Task Sequence ID
0.00
0.25
0.50
0.75
1.00
1.25
1.50
Cumulative Cost (USD)
Cumulative Cost Trend by LLM & Strategy (SCIENCE)
Strategy
force_strong
force_weak
neural
Provider
claude
deepseek
gemini
openai
qwen
(c) Scientific Discovery
2
4
6
8
10
Task Sequence ID
0.0
0.5
1.0
1.5
2.0
Cumulative Cost (USD)
Cumulative Cost Trend by LLM & Strategy (SECURITY)
Strategy
force_strong
force_weak
neural
Provider
claude
deepseek
gemini
openai
qwen
(d) Cybersecurity
Figure 20. Comparative analysis of cumulative cost trends across four domains. (a) Software Engineering, (b) Data Analysis, (c)
Science Discovery, and (d) Cybersecurity. The results consistently demonstrate that Claude, OpenAI, and Gemini incur significantly
higher costs compared to Qwen and DeepSeek.
41


--- Page 42 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
Table 16. Comparative analysis of dynamic routing strategies. The results are split into two columns to optimize space. CASTER
consistently shows cost reductions compared to the Strong baseline.
Scenario
Model
Strategy
Cost
Red.
Software
claude
Force Strong
$2.8204
-
Force Weak
$0.8858
68.6%
CASTER
$1.3738
51.3%
deepseek
Force Strong
$0.2346
-
Force Weak
$0.1391
40.7%
CASTER
$0.1106
52.9%
gemini
Force Strong
$1.5087
-
Force Weak
$0.4412
70.8%
CASTER
$0.8783
41.8%
openai
Force Strong
$1.4658
-
Force Weak
$0.0498
96.6%
CASTER
$0.4052
72.4%
qwen
Force Strong
$0.0397
-
Force Weak
$0.0138
65.2%
CASTER
$0.0186
53.1%
Data
claude
Force Strong
$3.2237
-
Force Weak
$0.4641
85.6%
CASTER
$0.9186
71.5%
deepseek
Force Strong
$0.2193
-
Force Weak
$0.2268
-3.4%
CASTER
$0.1332
39.3%
gemini
Force Strong
$1.8052
-
Force Weak
$0.3009
74.2%
CASTER
$0.3258
47.6%
openai
Force Strong
$1.1514
-
Force Weak
$0.0452
96.1%
CASTER
$0.4533
60.6%
qwen
Force Strong
$0.0456
-
Force Weak
$0.0136
70.2%
CASTER
$0.0344
24.6%
Scenario
Model
Strategy
Cost
Red.
Science
claude
Force Strong
$1.6530
-
Force Weak
$0.4987
69.8%
CASTER
$1.1596
29.8%
deepseek
Force Strong
$0.1648
-
Force Weak
$0.1660
-0.7%
CASTER
$0.1429
13.3%
gemini
Force Strong
$1.1737
-
Force Weak
$0.2311
80.3%
CASTER
$0.9259
21.1%
openai
Force Strong
$1.2679
-
Force Weak
$0.0550
95.7%
CASTER
$1.0416
17.8%
qwen
Force Strong
$0.1026
-
Force Weak
$0.0276
73.1%
CASTER
$0.0695
32.3%
Security
claude
Force Strong
$2.1948
-
Force Weak
$0.3346
84.8%
CASTER
$0.9129
58.4%
deepseek
Force Strong
$0.0983
-
Force Weak
$0.1118
-13.7%
CASTER
$0.1105
-12.4%
gemini
Force Strong
$0.9837
-
Force Weak
$0.1408
85.7%
CASTER
$0.3064
68.9%
openai
Force Strong
$0.4886
-
Force Weak
$0.0207
95.8%
CASTER
$0.2234
54.3%
qwen
Force Strong
$0.0274
-
Force Weak
$0.0100
63.5%
CASTER
$0.0257
6.2%
42


--- Page 43 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
claude
deepseek
gemini
openai
qwen
LLM Provider
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Average Cost per Task (USD)
$0.282
$0.023
$0.151
$0.147
$0.004
$0.089
$0.014
$0.044
$0.005
$0.001
$0.137
$0.014
$0.088
$0.041
$0.002
Average Cost per Task by LLM (SOFTWARE)
Strategy
force_strong
force_weak
neural
(a) Software Engineering
claude
deepseek
gemini
openai
qwen
LLM Provider
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Average Cost per Task (USD)
$0.322
$0.022
$0.181
$0.115
$0.005
$0.046
$0.023
$0.047
$0.005
$0.001
$0.092
$0.013
$0.095
$0.045
$0.003
Average Cost per Task by LLM (DATA)
Strategy
force_strong
force_weak
neural
(b) Data Analysis
claude
deepseek
gemini
openai
qwen
LLM Provider
0.00
0.05
0.10
0.15
0.20
Average Cost per Task (USD)
$0.165
$0.016
$0.117
$0.127
$0.010
$0.050
$0.017
$0.023
$0.006
$0.003
$0.116
$0.014
$0.093
$0.104
$0.007
Average Cost per Task by LLM (SCIENCE)
Strategy
force_strong
force_weak
neural
(c) Scientific Discovery
claude
deepseek
gemini
openai
qwen
LLM Provider
0.00
0.05
0.10
0.15
0.20
0.25
Average Cost per Task (USD)
$0.219
$0.010
$0.098
$0.049
$0.003
$0.033
$0.011
$0.014
$0.002
$0.001
$0.091
$0.011
$0.031
$0.022
$0.003
Average Cost per Task by LLM (SECURITY)
Strategy
force_strong
force_weak
neural
(d) Cybersecurity
Figure 21. Comparison of average cost per task across four domains. (a) Software Engineering, (b) Data Analysis, (c) Science
Discovery, and (d) Cybersecurity. The bar charts clearly illustrate that the CASTER strategy (green) achieves substantial cost reductions
compared to the strong model baseline (orange), particularly for high-cost providers such as Claude, Gemini, and OpenAI. This
demonstrates the effectiveness of dynamic routing in controlling average budgets without solely relying on expensive models.
43


--- Page 44 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
Table 17. Comparative analysis of Average Cost per Task. This table details the average expenditure for each task across different
models and routing strategies. The results highlight the cost-efficiency of the CASTER strategy.
Scenario
Model
Strategy
Avg. Cost
Red.
Software
claude
Force Strong
$0.2820
-
Force Weak
$0.0886
68.6%
CASTER
$0.1374
51.3%
deepseek
Force Strong
$0.0235
-
Force Weak
$0.0139
40.7%
CASTER
$0.0111
52.9%
gemini
Force Strong
$0.1509
-
Force Weak
$0.0441
70.8%
CASTER
$0.0878
41.8%
openai
Force Strong
$0.1466
-
Force Weak
$0.0050
96.6%
CASTER
$0.0405
72.4%
qwen
Force Strong
$0.0040
-
Force Weak
$0.0014
65.2%
CASTER
$0.0019
53.1%
Data
claude
Force Strong
$0.3224
-
Force Weak
$0.0464
85.6%
CASTER
$0.0919
71.5%
deepseek
Force Strong
$0.0219
-
Force Weak
$0.0227
-3.4%
CASTER
$0.0133
39.3%
gemini
Force Strong
$0.1805
-
Force Weak
$0.0301
83.3%
CASTER
$0.0326
82.0%
openai
Force Strong
$0.1151
-
Force Weak
$0.0045
96.1%
CASTER
$0.0453
60.6%
qwen
Force Strong
$0.0046
-
Force Weak
$0.0014
70.2%
CASTER
$0.0034
24.6%
Scenario
Model
Strategy
Avg. Cost
Red.
Science
claude
Force Strong
$0.1653
-
Force Weak
$0.0499
69.8%
CASTER
$0.1160
29.8%
deepseek
Force Strong
$0.0165
-
Force Weak
$0.0166
-0.7%
CASTER
$0.0143
13.3%
gemini
Force Strong
$0.1174
-
Force Weak
$0.0231
80.3%
CASTER
$0.0926
21.1%
openai
Force Strong
$0.1268
-
Force Weak
$0.0055
95.7%
CASTER
$0.1042
17.8%
qwen
Force Strong
$0.0103
-
Force Weak
$0.0028
73.1%
CASTER
$0.0070
32.3%
Security
claude
Force Strong
$0.2195
-
Force Weak
$0.0335
84.8%
CASTER
$0.0913
58.4%
deepseek
Force Strong
$0.0098
-
Force Weak
$0.0112
-13.7%
CASTER
$0.0111
-12.4%
gemini
Force Strong
$0.0984
-
Force Weak
$0.0141
85.7%
CASTER
$0.0306
68.9%
openai
Force Strong
$0.0489
-
Force Weak
$0.0021
95.8%
CASTER
$0.0223
54.3%
qwen
Force Strong
$0.0027
-
Force Weak
$0.0010
63.5%
CASTER
$0.0026
6.2%
44


--- Page 45 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
claude
deepseek
gemini
openai
qwen
LLM Provider
80
85
90
95
100
105
Average Score (0-100)
100.0
98.6
100.0
95.3
99.3
93.3
98.3
99.8
82.2
97.8
96.4
98.1
100.0
97.0
100.0
Average Quality Score per Task by LLM (SOFTWARE)
Strategy
force_strong
force_weak
neural
(a) Software Engineering
claude
deepseek
gemini
openai
qwen
LLM Provider
50
60
70
80
90
100
Average Score (0-100)
83.3
73.4
53.6
75.5
73.5
80.7
69.2
52.1
73.1
70.4
84.6
78.1
53.6
75.4
73.6
Average Quality Score per Task by LLM (DATA)
Strategy
force_strong
force_weak
neural
(b) Data Analysis
claude
deepseek
gemini
openai
qwen
LLM Provider
80.0
82.5
85.0
87.5
90.0
92.5
95.0
97.5
100.0
Average Score (0-100)
96.7
84.6
94.2
95.3
96.7
94.8
89.5
95.0
87.5
97.5
95.8
97.5
95.0
95.4
97.6
Average Quality Score per Task by LLM (SCIENCE)
Strategy
force_strong
force_weak
neural
(c) Scientific Discovery
claude
deepseek
gemini
openai
qwen
LLM Provider
80.0
82.5
85.0
87.5
90.0
92.5
95.0
97.5
100.0
Average Score (0-100)
94.3
91.1
95.6
93.7
95.9
94.4
89.3
94.8
92.9
94.1
95.1
94.8
96.2
93.9
95.2
Average Quality Score per Task by LLM (SECURITY)
Strategy
force_strong
force_weak
neural
(d) Cybersecurity
Figure 22. Comparison of average quality scores across four domains. (a) Software Engineering, (b) Data Analysis, (c) Science
Discovery, and (d) Cybersecurity. The results indicate that despite the significant cost reductions shown in Figure 21a, the CASTER
strategy (green) maintains high performance levels comparable to the strong model baseline (orange) and significantly outperforms the
weak model baseline (grey). This validates the strategy’s capability to optimize costs without compromising task quality.
45


--- Page 46 ---
CASTER: Context-Aware Strategy for Task Efficient Routing
Table 18. Comparative analysis of Average Quality Score. The table presents the average performance scores (0-100) across different
models and routing strategies. CASTER maintains high performance comparable to Strong baselines while significantly reducing costs.
Scenario
Model
Strategy
Avg. Score
Software
claude
Force Strong
100.0
Force Weak
93.3
CASTER
96.4
deepseek
Force Strong
98.6
Force Weak
98.3
CASTER
98.1
gemini
Force Strong
100.0
Force Weak
99.8
CASTER
100.0
openai
Force Strong
95.3
Force Weak
82.2
CASTER
97.0
qwen
Force Strong
99.3
Force Weak
97.8
CASTER
100.0
Data
claude
Force Strong
83.3
Force Weak
80.7
CASTER
84.6
deepseek
Force Strong
73.4
Force Weak
69.2
CASTER
78.1
gemini
Force Strong
53.6
Force Weak
52.1
CASTER
53.6
openai
Force Strong
75.5
Force Weak
73.1
CASTER
75.4
qwen
Force Strong
73.5
Force Weak
70.4
CASTER
73.6
Scenario
Model
Strategy
Avg. Score
Science
claude
Force Strong
96.7
Force Weak
94.8
CASTER
95.8
deepseek
Force Strong
84.6
Force Weak
89.5
CASTER
97.5
gemini
Force Strong
94.2
Force Weak
95.0
CASTER
95.0
openai
Force Strong
95.3
Force Weak
87.5
CASTER
95.4
qwen
Force Strong
96.7
Force Weak
97.5
CASTER
97.6
Security
claude
Force Strong
94.3
Force Weak
94.4
CASTER
95.1
deepseek
Force Strong
91.1
Force Weak
89.3
CASTER
94.8
gemini
Force Strong
95.6
Force Weak
94.8
CASTER
96.2
openai
Force Strong
93.7
Force Weak
92.9
CASTER
93.9
qwen
Force Strong
95.9
Force Weak
94.1
CASTER
95.2
46
