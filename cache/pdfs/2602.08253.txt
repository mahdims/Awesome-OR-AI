--- Page 1 ---
G-LNS: Generative Large Neighborhood Search for
LLM-Based Automatic Heuristic Design
Baoyun Zhao 1 He Wang 2 3 Liang Zeng 4
Abstract
While Large Language Models (LLMs) have re-
cently shown promise in Automated Heuristic De-
sign (AHD), existing approaches typically formu-
late AHD around constructive priority rules or
parameterized local search guidance, thereby re-
stricting the search space to fixed heuristic forms.
Such designs offer limited capacity for structural
exploration, making it difficult to escape deep
local optima in complex Combinatorial Optimiza-
tion Problems (COPs). In this work, we propose
G-LNS, a generative evolutionary framework that
extends LLM-based AHD to the automated de-
sign of Large Neighborhood Search (LNS) opera-
tors. Unlike prior methods that evolve heuristics
in isolation, G-LNS leverages LLMs to co-evolve
tightly coupled pairs of destroy and repair opera-
tors. A cooperative evaluation mechanism explic-
itly captures their interaction, enabling the discov-
ery of complementary operator logic that jointly
performs effective structural disruption and recon-
struction. Extensive experiments on challenging
COP benchmarks, such as Traveling Salesman
Problems (TSP) and Capacitated Vehicle Routing
Problems (CVRP), demonstrate that G-LNS sig-
nificantly outperforms LLM-based AHD methods
as well as strong classical solvers. The discov-
ered heuristics not only achieve near-optimal so-
lutions with reduced computational budgets but
also exhibit robust generalization across diverse
and unseen instance distributions.1
1Software College, Northeastern University, 110819, Shenyang,
China 2International Centre for Theoretical Physics Asia-Pacific,
University of Chinese Academy of Sciences, 100190, Beijing,
China 3Taiji Laboratory for Gravitational Wave Universe, Uni-
versity of Chinese Academy of Sciences, 100049, Beijing,
China 4Tsinghua University, 100084, Beijing, China.
Corre-
spondence to: He Wang <hewang@ucas.ac.cn>, Liang Zeng
<zengliangcs@gmail.com>.
1Our code are available at https://github.com/
zboyn/G-LNS.
A. Constructive (Rules)
Step0
Empty Solution
Step1
Choose Item A
Step2
Choose Item B
Step9
Final
B
D
E
C
A
G
F
I
H
B. GLS (Penalties)
Manually Designed
Neighborhood Operators(e.g. 2-opt)
Global Opt.
update_penalty_term(I) = ‚Ä¶
Existing AHD: Restricted Search
Our G-LNS: Structural Reshaping
Destroy(ùùÖùíÖ)
Destroy/Repair Population
B
D
E
C
A
G
F
I
H
B
E
C
G
F
Partial Solution
Current Solution
Repair(ùùÖùíì)
SA Accept
Large Neighborhood Search
(Via Efficient Operators Designed by LLMs)
Current Sol.
A
NO
BACKTRACKING
B
A
score(next_item) = ‚Ä¶
Global Opt.
Current Sol.
Figure 1. Comparison of G-LNS and traditional AHD methods.
For combinatorial optimization problems, unlike existing AHD
methods that are largely restricted to local search, G-LNS enables
structural reshaping through LLM-generated LNS operators, al-
lowing the search to escape local optima.
1. Introduction
Combinatorial Optimization Problems (COPs) are ubiqui-
tous in industrial manufacturing and logistics scheduling,
where computational efficiency directly impacts operational
costs (Dr¬¥eo, 2006; Desale et al., 2015). Since many practical
COPs are NP-hard (Garey & Johnson, 2002), hand-crafted
heuristics have long been the dominant approach for solving
large-scale instances (Blum & Roli, 2003; Gendreau et al.,
2010). However, traditional heuristic design relies heavily
on domain expertise and is often tailored to specific problem
structures, which substantially limits generalization across
diverse tasks (St¬®utzle & L¬¥opez-Ib¬¥aÀúnez, 2018).
Recent advances in Large Language Models (LLMs), par-
ticularly in logical reasoning (Wei et al., 2022; Zeng et al.,
2024; Zhang & Zeng, 2024) and code generation (Chen,
2021; Zeng et al., 2025), have spurred growing interest in
Automated Heuristic Design (AHD) (Burke et al., 2013). By
leveraging LLMs to automatically generate and refine algo-
rithmic code, AHD searches for high-performance heuristics
within a discrete algorithm space (Yang et al., 2023). Pio-
neering frameworks such as FunSearch (Romera-Paredes
et al., 2024) and EoH (Liu et al., 2024b) introduced the
evolutionary ‚ÄúThought‚ÄìCode‚Äù paradigm and demonstrated
1
arXiv:2602.08253v1  [cs.AI]  9 Feb 2026


--- Page 2 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
promising results on canonical tasks such as Bin Packing.
Subsequent work extended this paradigm through reflective
evolution (Ye et al., 2024), tree-based exploration strategies
(Zheng et al., 2025), and heuristic set evolution to improve
generalization (Liu et al., 2025).
Despite this progress, existing AHD methods exhibit a fun-
damental structural bottleneck (Figure 1). Most approaches
instantiate AHD around either Constructive Heuristics
(Glover et al., 2001), which evolve priority rules for sequen-
tial decision-making, or Guided Local Search (Voudouris
& Tsang, 1996), which optimizes penalty functions under
fixed neighborhood operators. Constructive heuristics fol-
low an irreversible trajectory: early suboptimal decisions
are difficult to correct through later rule adjustments(Mart¬¥ƒ±
& Reinelt, 2011). Conversely, while local search enables it-
erative refinement(Voudouris & Tsang, 1999), current AHD
methods typically treat neighborhood structures (e.g., 2-
opt(Johnson & McGeoch, 1997)) as fixed priors, restricting
the LLM to parameter tuning rather than enabling structural
algorithmic innovation(Liu et al., 2024c).
To overcome these limitations, we draw inspiration from
Large Neighborhood Search (LNS) (Shaw, 1998), a meta-
heuristic that achieves strong structural reshaping through
alternating destroy and repair operations(Ropke & Pisinger,
2006). The effectiveness of LNS critically depends on the
coupling between these two operators: the destroy phase
determines the structural defects introduced into the solu-
tion, while the repair phase must be specifically adapted
to reconstruct them(Pisinger & Ropke, 2018). This strong
interdependence makes automated LNS design particularly
challenging and has largely prevented its adoption within
existing AHD frameworks(Da Ros et al., 2025).
In this work, we propose G-LNS, an evolutionary framework
that enables LLMs to automatically design problem-specific
LNS operators. Instead of optimizing scalar heuristics or
fixed templates, G-LNS prompts the LLM to generate ex-
ecutable code for both destroy and repair operators. To
explicitly model their coupling, the framework maintains
separate populations for destroy and repair operators and
evaluates them jointly within an adaptive LNS process. A co-
operative evaluation mechanism records the performance of
operator pairs, allowing G-LNS to identify complementary
destroy‚Äìrepair logic and guide their co-evolution through
synergy-aware crossover. This design allows the search
process to move beyond local adjustments and discover
heuristics capable of effective structural disruption and re-
construction. Our contributions are summarized as follows:
‚Ä¢ Generative LNS for AHD. We propose G-LNS, an evo-
lutionary framework that extends LLM-based AHD to the
automated design of Large Neighborhood Search (LNS)
operators, enabling structural solution perturbation be-
yond constructive rules and fixed local moves.
‚Ä¢ Synergy-aware Co-evolution. We introduce a coopera-
tive evaluation mechanism with a synergy matrix to ex-
plicitly capture and exploit the coupling between destroy
and repair operators during evolution.
‚Ä¢ Empirical Effectiveness and Generalization. Extensive
experiments on TSP and CVRP demonstrate that G-LNS
outperforms state-of-the-art AHD methods and strong
classical solvers, achieving near-optimal solutions with
substantially reduced computational budgets.
2. Background
2.1. Automatic Heuristic Design
Automatic Heuristic Design (AHD) aims to automatically
discover high-performance heuristics for combinatorial op-
timization problems (COPs)(Burke et al., 2013; St¬®utzle &
L¬¥opez-Ib¬¥aÀúnez, 2018; Chen et al., 2025b). Let I denote
the instance space and S the solution space. A heuristic
h ‚ààH is a mapping h : I ‚ÜíS within a discrete algorithm
space H. The objective of AHD is to identify a heuristic
h‚àóthat minimizes the expected objective value over a target
distribution D:
h‚àó= arg min
h‚ààH EI‚àºD

f(h(I) | I)

.
(1)
While AHD enables systematic search over algorithmic
logic, the expressiveness of the search space is determined
by how heuristics are parameterized. Many existing ap-
proaches restrict the search to predefined templates, limiting
the ability to induce fundamental changes in the underlying
search dynamics. This work focuses on expanding the de-
sign space from fixed heuristic forms to structurally adaptive
search operators.
2.2. Large Neighborhood Search
Large Neighborhood Search (LNS) is a meta-heuristic that
explores the solution space through iterative destroy-and-
repair operations (Shaw, 1998). Given a solution x, a
destroy operator d removes a subset of components (con-
trolled by a destruction ratio œµ), producing a partial solution
xpartial = d(x). A repair operator r then reconstructs a
complete solution x‚Ä≤ = r(xpartial).
To balance exploration and exploitation, LNS typically
employs a Simulated Annealing (SA) acceptance crite-
rion (Henderson et al., 2003), where non-improving solu-
tions may be accepted with a probability that decreases over
time. The performance of LNS critically depends on the
coupling between d and r: effective operators must intro-
duce targeted structural disruption while enabling efficient
reconstruction. Designing such complementary operator
pairs remains a central challenge and motivates automation.
2


--- Page 3 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
2.3. Problem Formulation
We formulate the automated design of LNS operators as an
optimization problem over a discrete code space. Let Œòd
and Œòr denote the spaces of executable code implementing
destroy and repair operators, respectively. A policy œÄ =
(d, r) ‚ààŒòd √ó Œòr is evaluated by running an LNS algorithm
A with inherent stochasticity Œæ on instances I ‚àºD.
The expected performance of œÄ is defined as
J(œÄ) = EI‚àºD,Œæ

f(A(I, œÄ; Œæ) | I)

,
(2)
and the overall objective is to identify
œÄ‚àó= arg
min
œÄ‚ààŒòd√óŒòr J(œÄ).
(3)
2.4. Related Work
Recent work has integrated large language models (LLMs)
into evolutionary frameworks for automated heuristic de-
sign (Novikov et al., 2025; Chen et al., 2025a). Meth-
ods such as FunSearch (Romera-Paredes et al., 2024) and
EoH (Liu et al., 2024b) use LLMs as variation operators to
generate heuristic code conditioned on prior implementa-
tions and performance feedback, forming a thought‚Äìcode
co-evolution paradigm that improves exploration efficiency
in large discrete algorithm spaces. However, most existing
approaches rely on fixed algorithmic templates, focusing
on constructive heuristics or parameter tuning within prede-
fined local search frameworks(Ye et al., 2024; Zheng et al.,
2025), and thus largely overlook the structural design of
neighborhood operators. In contrast, our work targets the
automated generation of tightly coupled destroy-and-repair
operators, enabling LLMs to reshape the search process at
the structural level. Further discussion is provided in the
appendix (Section A).
3. Methodology
3.1. Overview of the G-LNS Framework
We propose G-LNS, a framework for automating the dis-
covery of high-performance destroy and repair operators
in Large Neighborhood Search (LNS). G-LNS formulates
heuristic design as an evolutionary process over algorith-
mic structures, where large language models (LLMs) act
as intelligent variation operators to explore the space of
algorithmic logic beyond fixed heuristic templates. This
evolutionary formulation requires a reliable mechanism to
assess the quality of newly generated operators.
To this end, we instantiate the LNS algorithm with an adap-
tive scoring mechanism inspired by Adaptive Large Neigh-
borhood Search (ALNS) (Ropke & Pisinger, 2006), which
provides a quantitative fitness signal by measuring each
operator‚Äôs contribution to optimization performance. As
illustrated in Figure 2, the overall methodology consists of
four phases‚ÄîInitialization, Evaluation, Population Man-
agement, and Evolution.
3.2. Initialization
To facilitate the co-evolution of these interdependent com-
ponents, G-LNS establishes a dual-population architec-
ture(Potter & De Jong, 1994), maintaining distinct reposi-
tories for destroy operators (Pd) and repair operators (Pr),
each with a capacity of N. The initialization phase begins
by injecting a compact set of classic domain-expert heuris-
tics as seeds. For instance, in TSP/VRP tasks, we utilize
Random Removal and Worst Removal for Pd, and Greedy
Insertion for Pr. These seeds serve a dual purpose: they
provide foundational search capabilities and function as In-
Context Examples(Brown et al., 2020) to align the LLM
with the specific task logic. To fully populate the pools
(when the number of seeds < N), we employ the Initial-
ization Action (denoted as i1 for destroy and i2 for repair).
Through i1, i2, the LLM is prompted to conceptualize novel
algorithmic logic and translate it into executable Python
code, thereby ensuring the diversity of the search space
from the onset. (See Appendix D.1 for specific details on
prompt engineering).
Simultaneously, we initialize three metric structures to guide
the evolutionary process. (1) Global Fitness Scores (F =
{F d, F r} with F d, F r ‚ààRN): Initialized to zeros, these
vectors track the cumulative performance of each operator in
Pd and Pr for population management. (2) Synergy Matrix
(S ‚ààRN√óN): Initialized to zeros, this matrix records the
cooperative performance of specific pairs (di, rj) to guide
joint crossover. (3) Adaptive Weights (W = {W d, W r}
with W d, W r ‚ààRN): Initialized to ones, these weights are
used solely within each evaluation episode to regulate the
roulette wheel selection probabilities.
3.3. Evaluation Phase
The objective of this phase is twofold: to quantify the in-
dividual contribution of each operator for population man-
agement, and to capture the coupling effectiveness between
destroy and repair operators for synergistic evolution. Given
the inherent stochasticity of LNS, we employ a Multi-
Episode Evaluation Mechanism.
We conduct K independent evaluation episodes.
Each
episode starts from a random initial solution x0 and exe-
cutes for T iterations. Importantly, the Adaptive Weights
W are reset to 1 at the beginning of each episode to en-
sure independent exploration, whereas the Global Fitness
F and Synergy Matrix S accumulate statistics across all K
episodes to obtain robust performance metrics that smooth
out single-episode stochasticity.
3


--- Page 4 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
Phase 1: Initialization & Seeds
Destroy Pool (ùí´ùëë, Size N)
‚Ä¶
ùëë1
ùëë2
ùëëùëõ
Repair Pool (ùí´ùëü, Size N)
‚Ä¶
ùëü1
ùëü2
ùëüùëõ
ùíäùüè, ùíäùüê
Init Actions
LLM
Expert Seeds
Phase 2: Evaluation (Multi-Episode)
LLM Agent
Roulette Selection
LNS Step
(T Iterations)
ùë•
ùëëùëñ,ùëüùëóùë•‚Ä≤
ùëëùëñ, ùëüùëó
Scoring
Reward (œÉ)
ùúé
Update W (Eq. 6)
Adaptive Selection
ùëÉùëëùëñ, ùëÉùëüùëó
Accumulate Global Fitness (F)
(Eq. 7)
Global Fitness (ùêπ= {ùêπùëë, ùêπùëü})
Synergy Matrix (S)
K 
Episodes
Phase 3: Population Management
Phase 4: LLM-Driven Evolution
Mutation (ùíéùüè, ùíéùüê)
Homo-Crossover (ùíÑùüè)
Joint-Crossover (ùíÑùüê)
Validation Filter
Rank 1 (pair)
Random
Local Exploitation
m1: Logic Evolution
m2: Parameter Calibration
Feature Recombination
(via F-based Parents)
Structural Coupling
(via Synergy Matrix S)
Sanity Check
Reset F, S ‚Üê 0
‚Ä¶
ùëü1
ùëü2
‚Ä¶
ùëë1
ùëë2
Pruning Action
Record Synergy (S)
(Eq. 8)
New Operators
Ranking & Pruning
Prune Bottom M
Population ùí´ùëë, ùí´ùëü
Global Fitness (F)
Figure 2. The overall workflow of the G-LNS framework. The framework operates in a cyclic manner consisting of four phases: (1)
Initialization, where the dual populations (Pd, Pr) are seeded with domain expertise and LLM-generated operators; (2) Evaluation,
where operator pairs are dynamically selected and scored via an Adaptive LNS process; (3) Population Management, which ranks and
prunes low-performing operators; and (4) Evolution, leveraging LLMs to perform mutation and crossover strategies to replenish the
population with novel heuristics.
Adaptive Selection.
In each iteration t of an episode, we
select a destroy operator di and a repair operator rj based on
their current weights. The selection probability P(di) (and
similarly for rj) follows the Roulette Wheel mechanism:
P(di) =
wd
i
PN
k=1 wd
k
(4)
Scoring and Update.
The selected pair (di, rj) is applied
to the current solution to generate a neighbor x‚Ä≤. Its ac-
ceptance and the corresponding reward œÉ are determined
hierarchically. If x‚Ä≤ improves the global best, we update
both x‚àóand xcurr and assign œÉ1. If x‚Ä≤ only improves the
current solution, we update xcurr and assign œÉ2. For deterio-
rating solutions, acceptance is governed by the Metropolis
criterion; if accepted, we update xcurr with reward œÉ3. Oth-
erwise, the solution is discarded and assigned the lowest
reward (œÉ4). Formally:
œÉ =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
œÉ1,
if f(x‚Ä≤) < f(x‚àó)
œÉ2,
if f(x‚Ä≤) < f(xcurr)
œÉ3,
if x‚Ä≤ is accepted by SA
œÉ4,
otherwise
(5)
Based on œÉ, we perform three distinct updates:
1. Adaptive Weights Update: To guide the search di-
rection during the current episode, the weights of the
selected operators are updated dynamically using a
smoothing factor Œª:
wd
i ‚ÜêŒªwd
i +(1‚àíŒª)œÉ,
wr
j ‚ÜêŒªwr
j +(1‚àíŒª)œÉ (6)
2. Global Fitness Accumulation: To evaluate the overall
quality of an operator for the subsequent Population
Management phase, we accumulate the reward into a
global fitness score F:
F(di) ‚ÜêF(di) + œÉ,
F(rj) ‚ÜêF(rj) + œÉ
(7)
3. Synergy Recording: To identify high-performing
structural couplings, we update the corresponding entry
in the synergy matrix:
Sij ‚ÜêSij + œÉ
(8)
High values in Sij indicate that destroy operator i and
repair operator j possess complementary logic, which
will be exploited in the Synergistic Joint Crossover
phase.
3.4. Population Management
As the evolutionary process progresses, the operator pools
Pd and Pr inevitably accumulate sub-optimal or redundant
4


--- Page 5 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
heuristics. Retaining these inefficient operators restricts the
population‚Äôs capacity to accommodate new, potentially supe-
rior logic. Therefore, upon the completion of every K eval-
uation episodes, we rank the operators in both populations
based on their accumulated Global Fitness F. The system
then prunes the bottom M operators from each pool, thereby
freeing up population slots for the subsequent LLM-driven
Evolution phase while preserving the high-performing elites.
3.5. LLM-Driven Evolution Mechanism
This phase leverages the code reasoning capabilities of
LLMs to replenish the population slots vacated during man-
agement. We frame this as a heuristic search process within
the algorithm space. To fill the M empty slots, we ran-
domly sample from three targeted evolutionary strategies,
ensuring a diverse mix of local refinements and structural
recombinations.
Mutation (Local Exploitation) (m1, m2). Mutation fo-
cuses on fine-tuning within the local algorithm space by
modifying a single parent operator sampled from the elite
pool. The specific action adapts to the parent‚Äôs rank to bal-
ance stability and innovation: Logic Evolution (m1) is ap-
plied to lower-ranking elites to generate novel mechanisms
for exploration; intermediate elites are assigned randomly;
Parameter Calibration (m2) is applied to top-ranking elites
to adjust hyperparameters (e.g., randomization ratios) for
stability.
Homogeneous Crossover (Feature Recombination) (c1).
This strategy facilitates information exchange between oper-
ators of the same type. Two parents are selected via fitness-
proportionate sampling based on F. The LLM is prompted
to fuse the logical strengths of both parents, synthesizing a
new operator that inherits hybrid characteristics (e.g., com-
bining spatial clustering with random perturbation).
Synergistic Joint Crossover (Structural Coupling) (c2).
Addressing the inherent coupling between destroy and re-
pair actions is a core innovation of G-LNS. We select the
Destroy-Repair pair with the highest accumulated synergy
score from S. By conditioning the repair generation on
the destroy logic, the LLM evolves this pair as a unified
entity, ensuring the repair operator is specifically tailored to
reconstruct the structural defects introduced by the destroy
operator, thereby maximizing their synergistic performance.
Robustness Guarantee. To mitigate the risk of LLM hal-
lucinations, we implement a Pre-evaluation Filter(Chen,
2021). All generated operators are subjected to a sanity
check on a small-scale instance set(Romera-Paredes et al.,
2024). Only those that are error-free and satisfy time com-
plexity constraints are admitted to the population; otherwise,
the regeneration process is triggered.
State Reset. Once the populations are fully replenished,
the Global Fitness scores F and the Synergy Matrix S are
reset to zero. This ensures that the newly generated oper-
ators start on equal footing with the surviving elites in the
subsequent evaluation cycle, preventing historical bias from
dominating the search.
4. Experiments
To ensure a rigorously fair comparison, our experimental
design strictly follows the experimental settings established
in the aforementioned LLM-based AHD baselines. We eval-
uate G-LNS across three distinct domains: TSP, CVRP, and
OVRP. Following these conventions, we utilize randomly
generated instances for the evolution phase. During the
testing phase, the learned operators are evaluated on both
held-out generated instances and widely-used benchmark
datasets (i.e., TSPLib(Reinelt, 1991) and CVRPLib(Uchoa
et al., 2017)) to verify their cross-distribution generaliza-
tion. To mitigate the impact of randomness, we conduct
three independent evolutionary runs for each task, capping
the evolution process at Gmax = 200 generations. This
configuration represents a significant reduction compared to
the 1,000 generations typically employed by the baselines,
demonstrating that our framework achieves superior sample
efficiency with a substantially lower token budget. In the
final testing phase, the best-found operator pair is applied to
test instances with a fixed budget of Ttest = 500 LNS itera-
tions (increased from the T = 100 iterations configured for
the evolution phase) to rigorously measure solution quality.
Settings. We employ DeepSeek-V3.2(Liu et al., 2024a)
as the core LLM for operator generation. Regarding the
evolutionary framework defined in Section 3, we maintain
a population size of N = 5 for both destroy and repair
operator pools, and prune the bottom M = 2 operators in
each management phase. For the evaluation process, we
conduct K = 10 independent episodes, each consisting
of T = 100 iterations. The hyperparameters for the inner
LNS loop are configured as follows: initial temperature
T0 = 100, cooling rate Œ± = 0.97, destruction ratio œµ = 0.2,
and weight update parameter Œª = 0.5. The scoring vector
is set to œÉ = {1.5, 1.2, 0.8, 0.1}. Detailed descriptions of
the evaluation datasets are provided in Appendix C.1. To
ensure a fair comparison, all LLM-based AHD baselines
utilize the same DeepSeek-V3.2 backbone.
Baseline. To verify the performance of G-LNS, we com-
pare it against baselines from three categories: (1) Hand-
crafted heuristics. We employ LKH-3(Helsgaun, 2017) as
the state-of-the-art baseline for TSP. For CVRP and OVRP,
we utilize the widely-adopted solver OR-Tools to provide
high-quality reference solutions. (2) Neural Combinatorial
Optimization (NCO) methods, specifically POMO(Kwon
et al., 2020), which serves as a representative constructive
learning baseline for routing problems. (3) LLM-based
5


--- Page 6 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
AHD methods, including FunSearch(Romera-Paredes et al.,
2024), EoH(Liu et al., 2024b), ReEvo(Ye et al., 2024), EVO-
MCTS(Wang & Zeng, 2025), and MCTS-AHD(Zheng et al.,
2025). Regarding the latter, we also evaluate its variant ap-
plied to Ant Colony Optimization (ACO), which evolves
pheromone update rules and serves as a representative iter-
ative AHD baseline. Additionally, we include a Standard
ALNS equipped with classic domain-specific operators as a
baseline to demonstrate the effectiveness of the generated
operators(Ropke & Pisinger, 2006). It is worth noting that
while existing AHD methods primarily focus on evolving
Constructive priority rules or tuning parameters for guided
search, G-LNS extends the design space to the structural
destroy-and-repair logic of LNS.
4.1. Experimental Results
Performance on synthetic held-out instances. As ev-
idenced in Table 1 (Top), G-LNS (Ours) consistently
achieves the lowest optimality gaps among all LLM-driven
methods across all problem sizes on TSP tasks. For large-
scale instances such as TSP100 and TSP200, our framework
significantly outperforms both Evo-MCTS and the iterative
MCTS-AHD(ACO), effectively overcoming the scalability
challenges that often lead existing AHD methods to exhibit
gaps exceeding 10%. This superior performance is driven
by the evolution of state-dependent destroy and repair logic,
which allows G-LNS to outperform Standard ALNS de-
tailed in Appendix E. Specifically, it identifies Adaptive
Continuous-Segment Removal and Diversity-Aware Inser-
tion for TSP, and Progressive Stochastic-Worst Removal
paired with Context-Aware Greedy Insertion for CVRP. Un-
like static rules, these operators dynamically adjust destruc-
tion magnitude and exploration noise based on real-time
solution states, enabling superior escape from local optima.
On CVRP tasks (Table 1, Bottom), G-LNS exhibits remark-
able scalability and robustness in the face of complex ca-
pacity constraints. These constraints often pose significant
challenges for traditional constructive heuristics, which tend
to get trapped in local optima due to the irreversible na-
ture of their sequential decisions. While G-LNS performs
slightly below the optimal reference solutions on small-
scale instances, its advantages become more pronounced
as the problem complexity increases. On the largest in-
stances (CVRP100/200), G-LNS not only outperforms all
LLM-based baselines‚Äîwhich typically show optimality
gaps exceeding 10%‚Äîbut also identifies solutions superior
to those provided by the OR-Tools solver. The ability to
navigate this constrained solution space suggests that the
learned destroy-and-repair operators can successfully cor-
rect structural defects that constructive methods are unable
to address.(See Appendix F.1 for details on OVRP).
G-LNS achieves superior solution quality while exhibiting
significantly higher computational efficiency compared to
benchmark methods. In CVRP experiments, while the OR-
Tools baseline utilizes a fixed budget of 320 seconds for the
evaluation batch (5 seconds per instance for 64 instances),
G-LNS requires substantially less time across all problem
scales: its total inference time for 500 iterations ranges from
merely 3.23s for CVRP10 to 280.91s for CVRP200. In
contrast, MCTS-AHD, another iterative approach, incurs a
much heavier computational burden, requiring 84.16s for
CVRP10 and 2407.14s for CVRP200, making G-LNS over
an order of magnitude faster. These results demonstrate
that G-LNS can identify higher-quality solutions than both
the standard solver‚Äîwhich fails to fully converge within
the time limit and leaves a gap of 1.27%‚Äì2.09% ‚Äîand
expensive iterative heuristics, all while consuming a much
smaller computational budget.
Robust generalization to real-world benchmarks. To fur-
ther assess the cross-distribution generalization capability
of G-LNS, we extended our evaluation to widely recognized
standard benchmarks, including TSPLib(Reinelt, 1991) and
CVRPLib(Uchoa et al., 2017). These datasets feature di-
verse problem distributions and scales that differ signifi-
cantly from the random instances used during evolution.
As detailed in Appendix F.2, G-LNS demonstrates supe-
rior generalization performance compared to state-of-the-art
AHD methods, including the strong baseline EoH-S (Liu
et al., 2025). G-LNS consistently achieves the lowest opti-
mality gaps across all evaluated datasets. Notably, on the
challenging CVRPLib Set F, our method reduces the opti-
mality gap from 40.1% (EoH-S) to 15.9%. Similarly, on
TSPLib, G-LNS maintains a low gap of 2.8%, significantly
outperforming baselines that struggle with unseen distri-
butions. These results confirm that the destroy-and-repair
operators evolved by G-LNS capture intrinsic structural
properties of combinatorial problems rather than merely
overfitting to the training distribution.
4.2. Ablation Studies
To validate the necessity of each evolutionary strategy and
component within the G-LNS framework, we conducted
ablation studies on the TSP50 and CVRP50 dataset. We
established the full G-LNS as the baseline and compared it
against four degenerated variants:
w/o Mut. (No Mutation) excludes the local refinement
strategies (i.e., Logic Evolution and Parameter Calibration).
The evolution relies entirely on crossover operations, assess-
ing the impact of fine-tuning single operators. w/o Homo.
(No Homogeneous Crossover) removes the mechanism of
fusing operators of the same type. New operators are gener-
ated solely through mutation or synergistic pairing, testing
the benefit of recombining high-level logic features within
the same operator class. w/o Syn. (No Synergistic Joint
6


--- Page 7 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
Table 1. Performance comparison on TSP and CVRP instances across five problem sizes. Top: Results for TSP, where G-LNS is evolved
on TSP50 and evaluated on 64 held-out instances for each size. Optimal solutions are derived using LKH-3. Bottom: Results for CVRP
(C = 50) under the same evolution and evaluation protocol. Reference solutions are obtained using OR-Tools (total 320 s; 5 s per
instance). For all LLM-based AHD methods, reported values represent the average of three independent runs. The overall best results are
underlined, and the best results among LLM-based AHD methods are highlighted in bold.
Traveling Salesman Problem (TSP)
TSP10
TSP20
TSP50
TSP100
TSP200
Method
Gap
Obj.
Gap
Obj.
Gap
Obj.
Gap
Obj.
Gap
Obj.
Optimal (LKH-3)
0.00%
2.833
0.00%
3.825
0.00%
5.717
0.00%
7.813
0.00%
10.665
POMO
0.00%
2.833
0.05%
3.827
0.43%
5.741
2.34%
7.996
20.35%
12.835
ALNS
0.25%
2.840
3.53%
3.960
5.65%
6.040
4.85%
8.192
5.96%
11.290
FunSearch
5.38%
2.986
11.78%
4.276
15.27%
6.590
17.24%
9.160
17.62%
12.544
EoH
4.73%
2.967
9.07%
4.172
14.73%
6.559
17.28%
9.163
17.84%
12.568
ReEvo
3.15%
2.922
6.97%
4.092
10.86%
6.338
12.88%
8.820
14.77%
12.240
MCTS-AHD
2.85%
2.914
7.84%
4.125
11.24%
6.359
12.02%
8.753
13.16%
12.068
Evo-MCTS
2.09%
2.892
4.96%
4.015
7.82%
6.164
9.54%
8.599
10.20%
11.753
MCTS-AHD(ACO)
0.11%
2.836
0.27%
3.836
1.21%
5.786
3.45%
8.083
6.22%
11.329
Ours
0.00%
2.833
0.01%
3.826
0.37%
5.738
1.10%
7.899
1.31%
10.805
Capacitated Vehicle Routing Problem (CVRP, C = 50)
CVRP10
CVRP20
CVRP50
CVRP100
CVRP200
Method
Gap
Obj.
Gap
Obj.
Gap
Obj.
Gap
Obj.
Gap
Obj.
OR-Tools
0.00%
3.096
0.00%
4.606
0.00%
8.145
2.09%
14.106
1.27%
25.088
POMO
2.42%
3.171
3.81%
4.781
4.27%
8.493
4.69%
14.466
28.11%
31.738
ALNS
2.45%
3.172
4.62%
4.818
4.26%
8.492
3.43%
14.291
3.63%
25.674
FunSearch
11.94%
3.466
26.34%
5.819
33.49%
11.014
31.75%
18.205
24.87%
30.936
EoH
11.31%
3.446
21.38%
5.591
26.88%
10.468
27.27%
17.585
19.50%
29.606
ReEvo
11.10%
3.440
22.00%
5.619
24.94%
10.308
24.42%
17.192
17.60%
29.135
MCTS-AHD
11.10%
3.440
20.65%
5.557
23.88%
10.221
23.77%
17.102
16.37%
28.831
Evo-MCTS
8.07%
3.346
17.52%
5.412
21.67%
10.038
19.50%
16.512
12.79%
27.942
MCTS-AHD(ACO)
2.80%
3.183
6.85%
4.921
11.65%
9.094
12.90%
15.600
10.52%
27.380
Ours
1.44%
3.141
2.20%
4.707
1.29%
8.250
0.00%
13.817
0.00%
24.774
Crossover) decouples the evolution of destroy and repair op-
erators. Instead of evolving complementary pairs based on
the synergy matrix S, it treats the populations independently.
w/o Adapt. (No Adaptive Weights) disables the Adaptive
LNS scoring mechanism during the evaluation phase, where
operators are selected with uniform probability to verify the
importance of dynamic resource allocation.
Furthermore, to evaluate the robustness of the adaptive
mechanism, we conducted a sensitivity analysis on the scor-
ing vector œÉ = {œÉ1, œÉ2, œÉ3, œÉ4} which governs operator
weight updates. We compared our default setting against a
Flat configuration ({1, 1, 1, 0.1}), where rewards for differ-
ent success levels are indistinguishable, and an Aggressive
configuration ({10, 5, 2, 0}), amplifying the reward variance
to verify the necessity of a hierarchical reward system.
Table 2 shows G-LNS achieves the lowest gaps, validating
our evolutionary strategies. (1) Evolutionary Components:
Significant drops in w/o Mut. and w/o Homo. highlight
the importance of logic fine-tuning and feature recombina-
tion. The degradation in w/o Syn. confirms the structural
Table 2. Ablation study of the key components in G-LNS. Each
variant removes one component‚ÄîMutation (Mut.), Homogeneous
Crossover (Homo.), Synergistic Crossover (Syn.), or Adaptive
Weights (Adapt.)‚Äîto assess its impact on solution quality for
TSP50 and CVRP50. The best results are highlighted in bold.
TSP50
CVRP50
G-LNS (Original)
0.37%
1.29%
ALNS
5.65%
4.26%
w/o Mut.
1.55%
1.96%
w/o Homo.
1.40%
2.03%
w/o Syn.
1.24%
1.87%
w/o Adapt.
0.95%
1.68%
G-LNS Flat
1.69%
2.31%
G-LNS Aggressive
0.51%
1.50%
coupling between destroy and repair operators; independent
evolution disrupts their synergy. (2) Scoring Mechanism:
The decline in w/o Adapt. verifies the feedback loop‚Äôs
value. Notably, the Flat vector performs worse than re-
moving adaptivity entirely, proving that indistinguishable
7


--- Page 8 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
0
50
100
150
200
Generation
8.4
8.6
8.8
9.0
9.2
9.4
Objective Value (Train)
(a) Evolutionary Progress
Best Operator Score
0
100
200
300
400
500
Iteration
14
16
18
20
22
24
Objective Value (Test)
(b) Evaluation Progress
ALNS (20s)
MCTS-AHD(ACO) (1110s)
GLNS (70s)
ORTools (320s)
EoH (<1s)
Evo-MCTS (<3s)
Figure 3. Convergence and Evolutionary Analysis. (a) Evolu-
tionary Progress: Validation score trajectory of the best operator
over 200 generations; the steady decline confirms the LLM‚Äôs ca-
pacity to evolve high-performance heuristics. (b) Evaluation
Progress: Convergence comparison on CVRP100 instances. G-
LNS identifies the best solution in 70s across all 64 instances,
significantly outperforming both the Solver (320s) and MCTS-
AHD(ACO) (1110s) in terms of search efficiency.
rewards mislead the search. Meanwhile, the Aggressive
setting falls short of the default, confirming the necessity of
a balanced hierarchical reward system.
4.3. Convergence Analysis
Figure 3 illustrates the dual performance characteristics of
G-LNS: the progressive improvement of operator quality
during evolution (Fig. 3a) and the rapid convergence of the
final evolved operators during evaluation (Fig. 3b).
Evolutionary Efficiency. Figure 3(a) depicts the validation
performance of the elite operators across 200 generations.
We observe a rapid quality improvement in the initial phase
(Generations 0-50), indicating that the LLM quickly grasps
the fundamental logic of destroy-and-repair operations from
the seed examples. Subsequently, the curve exhibits a steady
refinement trend (Generations 50-200), where the frame-
work fine-tunes the operator logic to escape local optima.
The narrowing variance (shaded area) suggests that the popu-
lation converges towards a set of robust and high-performing
heuristics.
Solving Convergence. Figure 3(b) compares the conver-
gence behavior of G-LNS against representative baselines
on CVRP100 instances. Several key observations can be
drawn: (1) Superior Convergence Rate: Compared to
Standard ALNS and MCTS-AHD(ACO), G-LNS demon-
strates a significantly steeper descent in the early iterations.
This suggests that the LLM-generated destroy operators pos-
sess stronger structural perturbation capabilities, allowing
the search to quickly identify promising regions in the so-
lution space. (2) Beating the Solver: G-LNS surpasses
the solution quality of constructive baselines within the first
50 iterations. More importantly, it eventually converges to
a solution (Obj ‚âà13.8) superior to that of the OR-Tools
solver (Obj ‚âà14.1). (3) Computational Efficiency: G-
LNS achieves state-of-the-art performance in approximately
70 seconds, which is not only 4.5√ó faster than the 320-
second budget allocated to OR-Tools , but also orders of
0.0
0.2
0.4
0.6
0.8
1.0
X
0.0
0.2
0.4
0.6
0.8
Y
Depot
Customer
(a) Before (C = 11.26)
0.0
0.2
0.4
0.6
0.8
1.0
X
0.0
0.2
0.4
0.6
0.8
Y
Depot
Customer
Removed
(b) Destroy
0.0
0.2
0.4
0.6
0.8
1.0
X
0.0
0.2
0.4
0.6
0.8
Y
Depot
Customer
(c) Repair (C = 9.96)
Figure 4. Case Study on Structural Reshaping. Visualizing
a snapshot of the evolutionary process on CVRP50. (a-b) The
generated repair operator targets the entangled region for destruc-
tion. (c) The destroy operator resolves the crossing by optimizing
node-to-vehicle assignments, reducing the cost from 11.26 to 9.96.
magnitude more efficient than MCTS-AHD(ACO), which
requires 1110 seconds. This dramatic reduction in computa-
tional overhead highlights the practical value of our evolved
heuristics for time-critical applications.
4.4. Case Study
Figure 4 illustrates a representative snapshot of a single
optimization iteration during the evolutionary process of
G-LNS on a CVRP50 instance. The iteration begins with a
solution trapped in a local optimum (Cost = 11.26, Fig. 4a),
characterized by crossing edges and inefficient routings.
In the Destroy phase (Fig. 4b), applying a destruction rate
of œµ = 0.2, the evolved PSWR operator (Appendix E.2)
exhibits a targeted strategy. It specifically identifies and
removes nodes involved in the most entangled regions, ef-
fectively dismantling the sub-optimal structures to facilitate
re-optimization. Subsequently, the ACAGI repair operator
(Fig. 4c) reconstructs the solution. Crucially, this process
goes beyond merely re-sequencing nodes within their orig-
inal paths. As shown in the transition from (b) to (c), the
operator dynamically reassigns customers to different routes,
correcting sub-optimal node-to-vehicle assignments. This
simultaneous optimization of clustering and sequencing suc-
cessfully untangles the crossings, significantly reducing the
cost to 9.96.
5. Conclusion
In this paper, we introduced G-LNS, a generative evolution-
ary framework that overcomes the structural limitations
of existing LLM-based Automated Heuristic Design by
co-evolving tightly coupled destroy and repair operators.
Through a synergy-aware evaluation mechanism and novel
crossover strategies, G-LNS successfully discovers sophis-
ticated heuristic logic that significantly outperforms state-
of-the-art baselines and strong classical solvers on complex
routing problems, while demonstrating superior generaliza-
tion capabilities. For future work, we plan to extend G-LNS
to multi-objective optimization settings and investigate its
applicability to a broader range of combinatorial problems
beyond routing tasks.
8


--- Page 9 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
6. Acknowledge
We express our gratitude to the team behind the LLM4AD
platform(Liu et al., 2024d). Their open-source framework
significantly facilitated the implementation of baseline meth-
ods and provided a robust environment for our comparative
experiments. We also thank the DeepSeek team for devel-
oping the DeepSeek-V3.2 model(Liu et al., 2024a), which
served as the core LLM in our framework. This work was
supported by the National Key Research and Development
Program of China under Grant No. 2021YFC2203004. HW
acknowledges support from the National Natural Science
Foundation of China (NSFC) under Grant Nos. 12547104
and 12405076.
References
Beck, J. C., Feng, T., and Watson, J.-P.
Combining
constraint programming and local search for job-shop
scheduling. INFORMS Journal on Computing, 23(1):
1‚Äì14, 2011.
Bello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio,
S. Neural combinatorial optimization with reinforcement
learning. arXiv preprint arXiv:1611.09940, 2016.
Bengio, Y., Lodi, A., and Prouvost, A. Machine learning
for combinatorial optimization: a methodological tour
d‚Äôhorizon. European Journal of Operational Research,
290(2):405‚Äì421, 2021.
Blot, A., Hoos, H. H., Jourdan, L., Kessaci-Marmion, M.-
¬¥E., and Trautmann, H. Mo-paramils: A multi-objective
automatic algorithm configuration framework. In Interna-
tional Conference on Learning and Intelligent Optimiza-
tion, pp. 32‚Äì47. Springer, 2016.
Blum, C. and Roli, A. Metaheuristics in combinatorial opti-
mization: Overview and conceptual comparison. ACM
computing surveys (CSUR), 35(3):268‚Äì308, 2003.
Bresson, X. and Laurent, T.
The transformer network
for the traveling salesman problem.
arXiv preprint
arXiv:2103.03012, 2021.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:
1877‚Äì1901, 2020.
Burke, E. K., Gendreau, M., Hyde, M., Kendall, G., Ochoa,
G., ¬®Ozcan, E., and Qu, R. Hyper-heuristics: A survey of
the state of the art. Journal of the Operational Research
Society, 64(12):1695‚Äì1724, 2013.
Burke, E. K., Hyde, M. R., Kendall, G., Ochoa, G., ¬®Ozcan,
E., and Woodward, J. R.
A classification of hyper-
heuristic approaches: revisited. In Handbook of meta-
heuristics, pp. 453‚Äì477. Springer, 2018.
Chen, C., Zhong, M., Sun, J., Fan, Y., and Shi, J. Hifo-
prompt: Prompting with hindsight and foresight for
llm-based automatic heuristic design.
arXiv preprint
arXiv:2508.13333, 2025a.
Chen, H., Wang, Y., Cai, Y., Hu, H., Li, J., Huang, S., Deng,
C., Liang, R., Kong, S., Ren, H., et al. Heurigym: An
agentic benchmark for llm-crafted heuristics in combi-
natorial optimization. arXiv preprint arXiv:2506.07972,
2025b.
Chen, M. Evaluating large language models trained on code.
arXiv preprint arXiv:2107.03374, 2021.
Da Ros, F., Soprano, M., Di Gaspero, L., and Roitero, K.
Large language models for combinatorial optimization:
A systematic review. arXiv preprint arXiv:2507.03637,
2025.
Desale, S., Rasool, A., Andhale, S., and Rane, P. Heuristic
and meta-heuristic algorithms and their relevance to the
real world: a survey. Int. J. Comput. Eng. Res. Trends,
351(5):2349‚Äì7084, 2015.
Dr¬¥eo, J. Metaheuristics for hard optimization: methods and
case studies. Springer Science & Business Media, 2006.
Duflo, G., Kieffer, E., Brust, M. R., Danoy, G., and Bou-
vry, P. A gp hyper-heuristic approach for generating tsp
heuristics. In 2019 IEEE International Parallel and Dis-
tributed Processing Symposium Workshops (IPDPSW),
pp. 521‚Äì529. IEEE, 2019.
Fu, Z.-H., Qiu, K.-B., and Zha, H. Generalize a small
pre-trained model to arbitrarily large tsp instances. In
Proceedings of the AAAI conference on artificial intelli-
gence, volume 35, pp. 7474‚Äì7482, 2021.
Garey, M. R. and Johnson, D. S. Computers and intractabil-
ity, volume 29. wh freeman New York, 2002.
Gendreau, M., Potvin, J.-Y., et al. Handbook of metaheuris-
tics, volume 2. Springer, 2010.
Glover, F., Gutin, G., Yeo, A., and Zverovich, A. Construc-
tion heuristics for the asymmetric tsp. European Journal
of Operational Research, 129(3):555‚Äì568, 2001.
Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu,
G., Bian, J., and Yang, Y. Connecting large language mod-
els with evolutionary algorithms yields powerful prompt
optimizers. arXiv preprint arXiv:2309.08532, 2023.
9


--- Page 10 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
Helsgaun, K. An extension of the lin-kernighan-helsgaun
tsp solver for constrained traveling salesman and vehicle
routing problems. Roskilde: Roskilde University, 12:
966‚Äì980, 2017.
Henderson, D., Jacobson, S. H., and Johnson, A. W. The
theory and practice of simulated annealing. In Handbook
of metaheuristics, pp. 287‚Äì319. Springer, 2003.
Huang, Z., Wu, W., Wu, K., Wang, J., and Lee, W.-B. Calm:
Co-evolution of algorithms and language model for auto-
matic heuristic design. arXiv preprint arXiv:2505.12285,
2025.
Jiang, X., Wu, Y., Wang, Y., and Zhang, Y. Unco: Towards
unifying neural combinatorial optimization through large
language model. 2024.
Johnson, D. S. and McGeoch, L. A. The traveling salesman
problem: A case study in local optimization. Local search
in combinatorial optimization, 1(1):215‚Äì310, 1997.
Joshi, C. K., Cappart, Q., Rousseau, L.-M., and Lau-
rent, T.
Learning the travelling salesperson prob-
lem requires rethinking generalization. arXiv preprint
arXiv:2006.07054, 2020.
Kambhampati, S., Valmeekam, K., Guan, L., Verma, M.,
Stechly, K., Bhambri, S., Saldyt, L., and Murthy, A. Llms
can‚Äôt plan, but can help planning in llm-modulo frame-
works. arXiv preprint arXiv:2402.01817, 2024.
Kool, W., Van Hoof, H., and Welling, M.
Attention,
learn to solve routing problems!
arXiv preprint
arXiv:1803.08475, 2018.
Koza, J. R. Genetic programming as a means for program-
ming computers by natural selection. Statistics and com-
puting, 4(2):87‚Äì112, 1994.
Kwon, Y.-D., Choo, J., Kim, B., Yoon, I., Gwon, Y., and
Min, S. Pomo: Policy optimization with multiple optima
for reinforcement learning. Advances in Neural Informa-
tion Processing Systems, 33:21188‚Äì21198, 2020.
Li, F., Golden, B., and Wasil, E. The open vehicle routing
problem: Algorithms, large-scale test problems, and com-
putational results. Computers & operations research, 34
(10):2918‚Äì2930, 2007.
Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao,
C., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-
v3 technical report. arXiv preprint arXiv:2412.19437,
2024a.
Liu, F., Tong, X., Yuan, M., Lin, X., Luo, F., Wang, Z.,
Lu, Z., and Zhang, Q. Evolution of heuristics: Towards
efficient automatic algorithm design using large language
model. arXiv preprint arXiv:2401.02051, 2024b.
Liu, F., Yao, Y., Guo, P., Yang, Z., Lin, X., Zhao, Z., Tong,
X., Mao, K., Lu, Z., Wang, Z., et al. A systematic survey
on large language models for algorithm design. ACM
Computing Surveys, 2024c.
Liu, F., Zhang, R., Xie, Z., Sun, R., Li, K., Lin, X., Wang,
Z., Lu, Z., and Zhang, Q. Llm4ad: A platform for algo-
rithm design with large language model. arXiv preprint
arXiv:2412.17287, 2024d.
Liu, F., Liu, Y., Zhang, Q., Tong, X., and Yuan, M. Eoh-
s: Evolution of heuristic set using llms for automated
heuristic design. arXiv preprint arXiv:2508.03082, 2025.
L¬¥opez-Ib¬¥aÀúnez, M., Dubois-Lacoste, J., C¬¥aceres, L. P., Bi-
rattari, M., and St¬®utzle, T. The irace package: Iterated
racing for automatic algorithm configuration. Operations
Research Perspectives, 3:43‚Äì58, 2016.
Mart¬¥ƒ±, R. and Reinelt, G. The linear ordering problem: ex-
act and heuristic methods in combinatorial optimization,
volume 175. Springer Science & Business Media, 2011.
Matai, R., Singh, S. P., and Mittal, M. L. Traveling salesman
problem: an overview of applications, formulations, and
solution approaches. Traveling salesman problem, theory
and applications, 1(1):1‚Äì25, 2010.
Mei, Y., Chen, Q., Lensen, A., Xue, B., and Zhang, M. Ex-
plainable artificial intelligence by genetic programming:
A survey. IEEE Transactions on Evolutionary Computa-
tion, 27(3):621‚Äì641, 2022.
Nazari, M., Oroojlooy, A., Snyder, L., and Tak¬¥ac, M. Rein-
forcement learning for solving the vehicle routing prob-
lem. Advances in neural information processing systems,
31, 2018.
Novikov, A., VÀúu, N., Eisenberger, M., Dupont, E., Huang,
P.-S., Wagner, A. Z., Shirobokov, S., Kozlovskii, B., Ruiz,
F. J., Mehrabian, A., et al. Alphaevolve: A coding agent
for scientific and algorithmic discovery. arXiv preprint
arXiv:2506.13131, 2025.
O‚ÄôNeill, M. Riccardo poli, william b. langdon, nicholas
f. mcphee: A field guide to genetic programming: Lulu.
com, 2008, 250 pp, isbn 978-1-4092-0073-4, 2009.
Pisinger, D. and Ropke, S. A general heuristic for vehicle
routing problems. Computers & operations research, 34
(8):2403‚Äì2435, 2007.
Pisinger, D. and Ropke, S. Large neighborhood search. In
Handbook of metaheuristics, pp. 99‚Äì127. Springer, 2018.
Potter, M. A. and De Jong, K. A. A cooperative coevolution-
ary approach to function optimization. In International
conference on parallel problem solving from nature, pp.
249‚Äì257. Springer, 1994.
10


--- Page 11 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
Reinelt, G. Tsplib‚Äîa traveling salesman problem library.
ORSA journal on computing, 3(4):376‚Äì384, 1991.
Romera-Paredes, B., Barekatain, M., Novikov, A., Balog,
M., Kumar, M. P., Dupont, E., Ruiz, F. J., Ellenberg, J. S.,
Wang, P., Fawzi, O., et al. Mathematical discoveries from
program search with large language models. Nature, 625
(7995):468‚Äì475, 2024.
Ropke, S. and Pisinger, D. An adaptive large neighborhood
search heuristic for the pickup and delivery problem with
time windows. Transportation science, 40(4):455‚Äì472,
2006.
Shaw, P. Using constraint programming and local search
methods to solve vehicle routing problems. In Interna-
tional conference on principles and practice of constraint
programming, pp. 417‚Äì431. Springer, 1998.
Song, J., Yue, Y., Dilkina, B., et al. A general large neigh-
borhood search framework for solving integer linear pro-
grams. Advances in Neural Information Processing Sys-
tems, 33:20012‚Äì20023, 2020.
St¬®utzle, T. and L¬¥opez-Ib¬¥aÀúnez, M. Automated design of
metaheuristic algorithms. In Handbook of metaheuristics,
pp. 541‚Äì579. Springer, 2018.
Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to se-
quence learning with neural networks. Advances in neural
information processing systems, 27, 2014.
Toth, P. and Vigo, D. The vehicle routing problem. SIAM,
2002.
Toth, P. and Vigo, D. Vehicle routing: problems, methods,
and applications. SIAM, 2014.
Uchoa, E., Pecin, D., Pessoa, A., Poggi, M., Vidal, T., and
Subramanian, A. New benchmark instances for the ca-
pacitated vehicle routing problem. European Journal of
Operational Research, 257(3):845‚Äì858, 2017.
Vinyals, O., Fortunato, M., and Jaitly, N. Pointer networks.
Advances in neural information processing systems, 28,
2015.
Voudouris, C. and Tsang, E. Partial constraint satisfaction
problems and guided local search. Proc., Practical Appli-
cation of Constraint Technology (PACT‚Äô96), London, pp.
337‚Äì356, 1996.
Voudouris, C. and Tsang, E. Guided local search and its
application to the traveling salesman problem. European
journal of operational research, 113(2):469‚Äì499, 1999.
Wang, H. and Zeng, L. Automated algorithmic discovery
for gravitational-wave detection guided by llm-informed
evolutionary monte carlo tree search. 2025.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,
E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting
elicits reasoning in large language models. Advances in
neural information processing systems, 35:24824‚Äì24837,
2022.
Wen, M., Linde, E., Ropke, S., Mirchandani, P., and Larsen,
A. An adaptive large neighborhood search heuristic for
the electric vehicle scheduling problem. Computers &
Operations Research, 76:73‚Äì83, 2016.
Wouda, N. A. and Lan, L. Alns: a python implementation
of the adaptive large neighbourhood search metaheuristic.
Journal of Open Source Software, 8(81):5028, 2023.
Wu, Z., Qi, Q., Zhuang, Z., Sun, H., and Wang, J. Pre-
tokenization of numbers for large language models. In
The Second Tiny Papers Track at ICLR 2024, 2024.
Xie, Z., Liu, F., Wang, Z., and Zhang, Q. Llm-driven neigh-
borhood search for efficient heuristic design. In 2025
IEEE Congress on Evolutionary Computation (CEC), pp.
1‚Äì8. IEEE, 2025.
Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D.,
and Chen, X. Large language models as optimizers. In
The Twelfth International Conference on Learning Repre-
sentations, 2023.
Ye, H., Wang, J., Cao, Z., Berto, F., Hua, C., Kim, H., Park,
J., and Song, G. Reevo: Large language models as hyper-
heuristics with reflective evolution. Advances in neural
information processing systems, 37:43571‚Äì43608, 2024.
Ye, H., Xu, H., Yan, A., and Cheng, Y. Large language
model-driven large neighborhood search for large-scale
milp problems. In Forty-second International Conference
on Machine Learning, 2025.
Zeng, L., Zhong, L., Zhao, L., Wei, T., Yang, L., He, J.,
Cheng, C., Hu, R., Liu, Y., Yan, S., et al. Skywork-
math: Data scaling laws for mathematical reasoning in
large language models‚Äìthe story goes on. arXiv preprint
arXiv:2407.08348, 2024.
Zeng, L., Li, Y., Xiao, Y., Li, C., Liu, C. Y., Yan, R., Wei, T.,
He, J., Song, X., Liu, Y., et al. Skywork-swe: Unveiling
data scaling laws for software engineering in llms. arXiv
preprint arXiv:2506.19290, 2025.
Zhang, L. and Zeng, L. Sat-ldm: Provably generalizable
image watermarking for latent diffusion models with self-
augmented training. arXiv preprint arXiv:2501.00463,
2024.
Zheng, Z., Xie, Z., Wang, Z., and Hooi, B. Monte carlo tree
search for comprehensive exploration in llm-based auto-
matic heuristic design. arXiv preprint arXiv:2501.08603,
2025.
11


--- Page 12 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
A. More Discussion on Related Work
A.1. AHD
Automated Heuristic Design (AHD), frequently referred to as Hyper-heuristics (Burke et al., 2013), aims to automate the
discovery of optimization algorithms. Formally, instead of searching for an optimal solution in the solution space S, AHD
searches for a high-quality heuristic algorithm h within an algorithm space H. The objective is to identify heuristics that
generalize well across a target distribution of problem instances, rather than overfitting to a single case.
Historically, Evolutionary Computation (EC) has served as the primary search strategy for AHD. Among various EC
methods(L¬¥opez-Ib¬¥aÀúnez et al., 2016; Blot et al., 2016; Burke et al., 2018), Genetic Programming (GP)(Koza, 1994; O‚ÄôNeill,
2009) has long been considered the prevailing approach. In the GP paradigm, heuristics are typically represented as
syntax trees, which are evolved through genetic operations such as subtree crossover and point mutation to optimize their
performance on training instances(Mei et al., 2022).
Despite its success, traditional GP-based AHD faces a significant bottleneck: the reliance on hand-crafted genetic operators.
The mutation and crossover operators often require substantial domain expertise to ensure that the modified heuristics remain
syntactically valid and semantically meaningful (Duflo et al., 2019). This dependency on manual design limits the flexibility
of the search process, motivating the recent shift towards more intelligent, generative approaches for algorithm discovery.
A.2. Neural Combinatorial Optimization (NCO)
Neural Combinatorial Optimization (NCO) has emerged as a promising paradigm to address the computational prohibitive-
ness of traditional exact solvers. The core motivation of NCO is to learn heuristics from data offline, enabling the generation
of high-quality approximate solutions in real-time inference (Bello et al., 2016; Bengio et al., 2021).
Sequence-to-Sequence Modeling. Early NCO approaches formulated the construction of solutions as a sequence-to-
sequence (Seq2Seq) prediction task, similar to neural machine translation (Sutskever et al., 2014). However, standard
Seq2Seq models struggled with combinatorial problems because the output vocabulary (e.g., the specific cities to visit)
varies for each input instance, unlike the fixed vocabulary in translation tasks (Vinyals et al., 2015). To overcome this
limitation, Vinyals et al. (2015) introduced the Pointer Network (Ptr-Net), which employs a pointer mechanism to select
input elements directly as outputs using attention scores.
From Supervised Learning to RL. While Ptr-Nets laid the foundation, optimizing them via Supervised Learning proved
impractical due to the high cost of obtaining optimal labels for large-scale instances (Bello et al., 2016). Consequently, the
field shifted towards Reinforcement Learning (RL), treating the generation process as a Markov Decision Process (Nazari
et al., 2018). Bello et al. (2016) proposed an Actor-Critic framework where the neural network acts as a policy to minimize
the tour length, using the negative tour length as the reward signal. They also introduced Active Search to refine solutions
during inference by sampling multiple trajectories to find the best candidate (Bello et al., 2016).
Symmetry-Aware Transformer (POMO). Modern NCO methods have evolved to leverage the Transformer architecture
for better feature extraction and long-range dependency modeling (Kool et al., 2018; Bresson & Laurent, 2021). Notably,
POMO (Kwon et al., 2020) addressed a critical start node bias inherent in previous constructive policies. By exploiting
the rotational symmetry of routing problems, POMO generates diverse trajectories in parallel (one for each starting node)
and uses the average reward as a low-variance shared baseline (Kwon et al., 2020). This approach significantly stabilizes
training and achieves state-of-the-art performance among constructive NCO methods.
Limitations and The Shift to AHD. Despite their fast inference speed, NCO models fundamentally operate as black boxes
and often suffer from poor generalization (Bengio et al., 2021). Their performance typically degrades significantly when
applied to problem scales or distributions unseen during training (distributional shift) (Joshi et al., 2020; Fu et al., 2021).
These limitations regarding interpretability and scalability have motivated the recent surge in LLM-based Automated Heuristic
Design, which aims to evolve explicit, generalizable algorithmic code instead of opaque neural weights (Romera-Paredes
et al., 2024; Liu et al., 2024b).
A.3. LLM for Combinatorial Optimization
Recent advancements have spurred a paradigm shift in applying Large Language Models (LLMs) to Combinatorial
Optimization (CO). We categorize existing methodologies into two distinct streams: LLM as Solver and LLM as Designer.
12


--- Page 13 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
Table 3. Comparison of LLM-based approaches for Combinatorial Optimization. Our G-LNS is unique in targeting the structural design
of LNS operators.
Method
LLM Role
Target Heuristic Type
Search Strategy
Key Characteristic / Focus
LLM as Solver (Direct Inference)
OPRO (Yang et al., 2023)
Solver
N/A (Direct Solution)
Iterative Prompting
Optimizes solutions via natural language feedback history.
Fine-tuned LLMs (Jiang et al., 2024)
Solver
N/A (Direct Solution)
Supervised Fine-tuning
Enhances LLM‚Äôs domain knowledge for specific CO constraints.
LLM as Designer (Automated Heuristic Design)
FunSearch (Romera-Paredes et al., 2024)
Designer
Constructive
Evolution
The first Thought-Code evolution for mathematical discovery.
EoH (Liu et al., 2024b)
Designer
Constructive
Evolution
Applies AHD to standard COPs with purely constructive rules.
ReEvo (Ye et al., 2024)
Designer
Constructive
Reflective Evolution
Introduces reflexivity to guide mutations via historical analysis.
MCTS-AHD (Zheng et al., 2025)
Designer
Constructive
MCTS
Uses Tree Search to balance global exploration and exploitation.
EoH-S (Liu et al., 2025)
Designer
Heuristic Set
Evolution
Evolves a complementary set of heuristics for better generalization.
LHNS (Xie et al., 2025)
Designer
Constructive
LNS
Applies LNS logic to perturb code blocks (Algorithm-level LNS).
G-LNS (Ours)
Designer
LNS (Destroy & Repair)
Synergistic Evolution
Designs structural operators for solution-level LNS.
LLM as Solver (Direct & Iterative Optimization). This paradigm treats LLMs as black-box optimizers, prompting them
to output solutions directly based on problem descriptions. Early attempts employed zero-shot or few-shot prompting
to solve small-scale instances (Guo et al., 2023). To improve performance, Yang et al. (2023) introduced Optimization
by PROmpting (OPRO), where the LLM iteratively refines solutions using natural language feedback and optimization
trajectories as in-context information. Other works explore fine-tuning LLMs specifically for CO tasks to enhance their
understanding of constraints (Jiang et al., 2024).
However, the LLM as Solver paradigm faces intrinsic limitations. First, LLMs struggle with the tokenization of high-
precision coordinates and numerical reasoning, often perceiving numbers as linguistic tokens rather than mathematical
values (Wu et al., 2024). Second, as noted by Kambhampati et al. (2024), LLMs function better as idea generators than
reliable planners; they lack the rigorous backtracking capabilities required for NP-hard problems. Consequently, these
methods are prone to hallucinations and scale poorly to large instances due to limited context windows.
LLM as Designer (Automated Heuristic Design). Acknowledging the limitations of direct inference, the field has
gravitated towards LLM-based AHD, repurposing LLMs to generate executable code. This paradigm ensures correctness and
scalability by offloading execution to standard Python interpreters. Pioneering frameworks like FunSearch (Romera-Paredes
et al., 2024) and EoH (Liu et al., 2024b) established the foundational Thought-Code evolutionary paradigm, treating LLMs
as mutation operators to evolve populations of constructive heuristics(Huang et al., 2025).
Advanced Search Strategies. To overcome the tendency of standard population-based methods to converge into local optima,
recent works have introduced more sophisticated search mechanisms. ReEvo (Ye et al., 2024) integrates a reflective evolution
mechanism, mimicking human thinking to retrospectively analyze historical performance and guide more effective code
mutations. Furthermore, MCTS-AHD (Zheng et al., 2025) and Evo-MCTS (Wang & Zeng, 2025) introduce Monte Carlo
Tree Search (MCTS) into the evolutionary process. By organizing heuristics in a tree structure, these methods balance
exploration and exploitation, allowing for the comprehensive development of temporarily underperforming heuristics that
standard populations might prematurely discard.
Heuristic Set Evolution. Addressing the generalization bottleneck where a single heuristic fails to cover diverse instance
distributions, EoH-S (Liu et al., 2025) proposes the concept of Automated Heuristic Set Design (AHSD). Instead of
optimizing a solitary algorithm, EoH-S evolves a complementary set of heuristics, ensuring that each problem instance is
covered by at least one effective strategy in the set, thereby achieving superior cross-distribution performance.
Differentiation from LLM-driven Heuristic Neighborhood Search. It is crucial to distinguish our approach from the recently
proposed LHNS (Xie et al., 2025). LHNS applies the logic of neighborhood search to the algorithm space itself‚Äîiteratively
perturbing heuristic code blocks to find better functions. While LHNS uses LNS-like concepts to guide the code search
process, the output remains a standard heuristic function.
Summary: A Paradigm Shift in Framework. As summarized in Table 3, in contrast to prior arts, our G-LNS does not
merely evolve a better scoring function or a local guidance rule; it fundamentally alters the algorithmic framework. By
explicitly tasking the LLM with designing structural destroy and repair operators, G-LNS enables the solver to perform
complex topological transformations on the solutions. This represents a shift from optimizing parameters/rules within a
fixed skeleton to automating the design of the solver‚Äôs structural components, a capability that extends beyond the scope of
previous AHD methods.
13


--- Page 14 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
A.4. Large Neighborhood Search (LNS)
Origins and Evolution. The Large Neighborhood Search (LNS) paradigm, originally introduced by Shaw (1998) for
Vehicle Routing Problems, utilizes a ruin and recreate principle to escape local optima. Unlike local search methods that
rely on small moves (e.g., 2-opt), LNS rearranges a significant portion of the solution structure (Gendreau et al., 2010). A
pivotal advancement was the Adaptive LNS (ALNS) (Ropke & Pisinger, 2006), which maintains a portfolio of operators and
dynamically adjusts their selection probabilities based on historical performance. This adaptive mechanism established LNS
as a robust framework capable of handling diverse constraints without extensive parameter tuning (Pisinger & Ropke, 2007).
Applications and Robustness. Due to its flexibility, LNS has become a dominant meta-heuristic for a wide array of
NP-hard combinatorial optimization problems. In the domain of logistics, it has been successfully applied to the Pickup and
Delivery Problem with Time Windows (Ropke & Pisinger, 2006) and the Electric Vehicle Routing Problem (Wen et al.,
2016). Beyond routing, LNS has demonstrated exceptional performance in scheduling tasks, particularly for the Job Shop
Scheduling Problem (Beck et al., 2011).
Data-Driven and LLM-Enhanced LNS. Recent advancements have integrated Machine Learning with LNS, particularly
for Mixed Integer Linear Programming (MILP). Approaches such as the general LNS framework by Song et al. (2020) and
the LLM-LNS framework by Ye et al. (2025) employ learning-based techniques‚Äîranging from imitation learning to LLM
reasoning‚Äîto automate the neighborhood selection process. These methods focus on learning a policy to select a subset of
variables for optimization, typically relying on off-the-shelf solvers (e.g., Gurobi) to solve the resulting sub-problems. In
contrast, our G-LNS utilizes the generative capabilities of LLMs to explicitly write code for domain-specific destroy and
repair operators, thereby evolving the underlying algorithmic logic independent of external solvers.
B. Details of Optimization Problem
B.1. Traveling Salesman Problem
The Traveling Salesman Problem (TSP)(Matai et al., 2010) is a quintessential NP-hard combinatorial optimization problem
that serves as a standard benchmark for heuristic algorithms. Given a set of N cities, the objective is to find the shortest
possible closed tour that visits every city exactly once and returns to the starting point.
Formally, an instance of TSP can be modeled as a complete undirected graph G = (V, E), where V = {v1, . . . , vN} is the
set of N nodes (cities) and E represents the edges connecting every pair of nodes. Each edge (i, j) ‚ààE is associated with a
distance dij, where node i is represented by a coordinate vector xi ‚ààR2, and the cost dij = ‚à•xi ‚àíxj‚à•2 corresponds to the
Euclidean distance between cities i and j.
Let œÄ = (œÄ1, œÄ2, . . . , œÄN) denote a permutation of the node indices {1, . . . , N}, representing the sequence of visited cities.
The optimization goal is to find a permutation œÄ‚àóthat minimizes the total tour length:
min
œÄ J (œÄ) =
N‚àí1
X
i=1
dœÄi,œÄi+1 + dœÄN,œÄ1
(9)
LNS Application Example.
To apply the LNS framework to the TSP, the search process iterates through a Destroy and
Repair phase to escape local optima(Wouda & Lan, 2023). Unlike local search methods (e.g., k-opt) that perform small
edge swaps, LNS structurally decomposes the solution:
‚Ä¢ Destroy Phase (Ruin): Given a complete tour œÄ, a destroy operator d(¬∑) removes a subset of k cities (denoted as
Vrem ‚äÇV), leaving a partial tour œÄpartial. For example, a Random Removal operator might stochastically select k
nodes to remove, while a Segment Removal operator removes a contiguous sequence of cities to disrupt a specific
sub-path.
‚Ä¢ Repair Phase (Recreate): A repair operator r(¬∑) takes the partial tour œÄpartial and the set of removed cities Vrem as
input, re-inserting the nodes into the tour to form a new feasible solution œÄ‚Ä≤. A classic example is the Greedy Insertion,
which iteratively inserts each node v ‚ààVrem into the position (i, i + 1) in œÄpartial that minimizes the incremental cost
‚àÜd = dœÄi,v + dv,œÄi+1 ‚àídœÄi,œÄi+1.
Through this mechanism, LNS can perform large moves in the search space, effectively reshaping the tour structure to find
superior global solutions.
14


--- Page 15 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
B.2. Capacitated Vehicle Routing Problem
The Capacitated Vehicle Routing Problem (CVRP)(Toth & Vigo, 2002; 2014) is a generalization of the TSP and a
fundamental problem in logistics and transportation. Unlike TSP, CVRP involves multiple vehicles serving a set of
customers, subject to vehicle capacity constraints. The objective is to design a set of optimal routes that minimize the total
travel cost while satisfying customer demands.
Formally, a CVRP instance is defined on a graph G = (V, E), where V = {v0, v1, . . . , vN}. Here, node v0 represents the
central depot, and Vc = {v1, . . . , vN} represents the set of N customers. Each customer vi has a positive demand qi, and
the depot has a demand q0 = 0. We are given a fleet of identical vehicles, each with a maximum capacity Q.
A solution consists of a set of routes S = {R1, R2, . . . , RK}, where each route Rk starts and ends at the depot v0. Let dij
denote the travel distance (cost) between node i and j. The objective is to minimize the total distance of all routes:
min
K
X
k=1
Cost(Rk) = min
K
X
k=1
X
(i,j)‚ààRk
dij
(10)
subject to the following constraints:
1. Coverage: Every customer vi ‚ààVc must be visited exactly once by exactly one vehicle.
2. Capacity: The total demand of customers served in any single route Rk must not exceed the vehicle capacity, i.e.,
P
vi‚ààRk qi ‚â§Q.
B.3. Open Vehicle Routing Problem
The Open Vehicle Routing Problem (OVRP)(Li et al., 2007) is a distinct variant of the classical CVRP. The fundamental
difference lies in the route structure: in OVRP, vehicles are not required to return to the depot after servicing the last
customer on their route. This problem formulation typically arises in third-party logistics scenarios where vehicles are hired
for one-way trips, or in situations where drivers use their personal vehicles and return home directly after the last delivery.
Formally, the problem is defined on the same graph G = (V, E) as the CVRP, with a depot v0 and a customer set Vc. The
constraints regarding customer coverage and vehicle capacity Q remain identical to those in CVRP. However, a route
Rk = (v0, vk1, vk2, . . . , vkm) in OVRP forms a Hamiltonian path rather than a cycle.
The objective is to minimize the total travel distance of the open routes. Mathematically, this can be expressed as minimizing
the sum of edge weights in the active routes, excluding the return arcs to the depot:
min
K
X
k=1
Ô£´
Ô£≠
mk‚àí1
X
j=0
dvkj ,vkj+1
Ô£∂
Ô£∏
(11)
where vk0 = v0 is the depot, and vkm is the last customer visited by vehicle k. Unlike CVRP, the cost term dvkm,v0 is
omitted. Consequently, the OVRP seeks to find a set of paths that cover all customers with minimum total length, ending at
any customer node.
C. Details of Experiments
C.1. Dataset Generation and Benchmarks
Dataset Settings.
Following the experimental protocols established in MCTS-AHD(Zheng et al., 2025), we adopt a
consistent data generation mechanism to ensure a rigorous comparison. We strictly distinguish between the datasets used for
discovering operators and those used for final evaluation.
Training Set (Operator Discovery).
The search for high-quality LNS operators is conducted on a compact Training Set
consisting of 16 instances with a fixed problem size of N = 50. Limiting the training to a specific scale and a small number
of instances ensures that the discovered operators capture generalizable logic rather than overfitting to massive datasets.
15


--- Page 16 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
Testing Set (Performance Evaluation).
The learned heuristics are subsequently evaluated on a held-out Testing Set to
assess their performance and scalability. This set comprises 64 instances for each problem scale N ‚àà{10, 20, 50, 100, 200},
allowing us to verify whether the operators trained on N = 50 can effectively generalize to both smaller and larger instances.
Instance Characteristics.
The specific parameters for instance generation are configured as follows:
‚Ä¢ TSP: Node coordinates are sampled uniformly from the unit square [0, 1]2.
‚Ä¢ CVRP: Consistent with standard NCO settings, node coordinates are sampled from [0, 1]2 with the depot fixed at
(0.5, 0.5).
‚Ä¢ OVRP: The instance settings are identical to those of CVRP. Customer demands are integers sampled uniformly from
[1, 9], and the vehicle capacity is set to Q = 50.
For both CVRP and OVRP, customer demands are integers sampled uniformly from {1, . . . , 9}, and the vehicle capacity is
set to Q = 50.
To further assess cross-distribution generalization, we extend our evaluation to the standard TSPLib(Reinelt, 1991) and
CVRPLib(Uchoa et al., 2017) benchmarks(See F.2 for details).
C.2. Implementation Details
For the baseline methods, we strictly adhered to their official open-source implementations to guarantee the reliability of the
results:
‚Ä¢ LKH-3: We utilized the official executable(Helsgaun, 2017) with default parameters.
‚Ä¢ Deep Learning Baselines (POMO): We used the pre-trained models provided by the original authors and performed
inference with greedy decoding (batch size = 1) to measure the raw inference speed without augmentation.
‚Ä¢ ALNS:
‚Ä¢ ALNS: We implemented the Adaptive Large Neighborhood Search based on the classic framework proposed by
Pisinger & Ropke (2007). The operator portfolio includes a diverse set of removal (random, worst, and related removal)
and insertion (greedy and regret-k insertion) heuristics. To ensure a competitive baseline, we adopted the standard
parameter settings for the adaptive weight adjustment and the simulated annealing acceptance criterion, as tuned in
the original work. This ensures that the performance of ALNS reflects its robust general capability rather than a
sub-optimal implementation.
‚Ä¢ LLM-based AHD: For methods like EoH and MCTS-AHD, we reproduced the evolutionary process using the same
LLM backend (DeepSeek-V3.2) and prompt engineering settings as described in their respective papers, ensuring that
performance differences stem from the algorithm structure rather than the language model capability.
C.3. Evaluation Budget and Efficiency
To demonstrate the superior sample efficiency of G-LNS, we enforced a strict constraint on the computational budget
compared to standard baselines.
LLM Interaction Budget.
Existing LLM-based AHD methods typically rely on extensive trial-and-error, requiring
a substantial budget of 1,000 evolutionary generations (or interactions) to ensure convergence. In contrast, G-LNS is
configured with a significantly reduced budget of only 200 generations.
Efficiency Analysis.
Despite utilizing only 20% of the interaction budget required by baseline methods, G-LNS achieves
superior performance as evidenced in Table 1 and Table 4. This 5√ó reduction in LLM queries translates directly to
significantly lower token consumption and operational costs. It indicates that evolving high-level structural operators
(Destroy and Repair) allows the search process to navigate the algorithm space much more efficiently than evolving low-level
constructive rules, avoiding the need for massive brute-force sampling.
16


--- Page 17 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
D. Details of Algorithm
D.1. Prompts of G-LNS Actions
G-LNS employs distinct evolutionary actions to discover high-performance heuristics. Next, we describe the meaning
and prompt engineering of each action. To ensure robustness and standardized outputs, these prompts strictly contain the
task background, existing heuristic references as contexts, function identification, input/output specifications, and logical
constraints according to the specific optimization task. We execute the LLM calls through these structured prompts to obtain
both the algorithmic design idea and its executable Python implementation. The rest of this subsection provides examples
for prompts, in which we highlight the functional components in distinct colors:
‚Ä¢ Initial Action i1 (Destroy Initialization): Action i1 represents directly getting an idea of designing a valid Destroy
Operator from scratch and a Python implementation to populate the heuristic pool when domain-expert seeds are
insufficient.
Prompt for Operator i1(Destroy Initialization)
The task is to design a novel Destroy Operator for a Large Neighborhood Search (LNS) framework. Given a
complete solution sequence (a tour of cities for TSP) and a target number of elements to remove (destroy cnt),
the function must determine which elements to remove. The objective is to develop a removal strategy that effectively
perturbs the current solution. This allows the subsequent Repair operator to reconstruct the solution in a way that
helps escape local optima and minimizes the total cost.
You are an expert in heuristic optimization algorithms, specifically Adaptive Large Neighborhood Search (ALNS).
Your task is to design a new ‚ÄôDestroy Operator‚Äô (removal operator) for the following problem:
Problem Description: {task description}
Existing Destroy Operators (Reference): {operators str}
Requirements:
1. First, describe your new algorithm and main steps in one sentence. The description must be inside a brace. Next,
implement it in Python as a function named destroy.
2. This function must accept 3 inputs: ‚Äôcurrent solution‚Äô, ‚Äôdestroy cnt‚Äô, ‚Äôdistance matrix‚Äô.
3. The function must return 2 outputs: ‚Äôremoved elements‚Äô, ‚Äôpartial solution‚Äô.
4. The logic should be strictly different from the existing ones provided in the reference to improve population
diversity.
5. Do not give additional explanations.
‚Ä¢ Initial Action i2 (Repair Initialization): Action i2 focuses on initializing the Repair Operator population (Pr). It
prompts the LLM to design a constructive strategy for re-inserting removed elements into a partial solution, ensuring
the reconstructed tour minimizes total cost.
Prompt for Operator i2 (Repair Initialization)
The task is to design a novel Repair Operator (Insertion Operator) for a Large Neighborhood Search (LNS)
framework. Given a partial solution partial solution (where some elements have been removed) and a list
of removed elements, the function must determine the best positions to re-insert these elements to restore a
complete solution. The objective is to reconstruct the solution in a way that minimizes the total cost
You are an expert in heuristic optimization algorithms, specifically Large Neighborhood Search (LNS). Your task is
to design a new ‚ÄôRepair Operator‚Äô (insertion operator) for the following problem:
Problem Description: {task description}
17


--- Page 18 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
Existing Repair Operators (Reference): {operators str}
Requirements:
1. First, describe your new algorithm and main steps in one sentence. The description must be inside a brace. Next,
implement it in Python as a function named repair.
2. This function must accept 3 inputs: ‚Äôpartial solution‚Äô, ‚Äôremoved elements‚Äô, ‚Äôdistance matrix‚Äô.
3. The function must return 1 output: ‚Äôcomplete solution‚Äô.
4. The logic should be innovative and distinct from the reference operators to ensure diverse reconstruction paths.
5. Do not give additional explanations.
‚Ä¢ Mutation Actions m1 & m2 (Adaptive Refinement): Actions m1 and m2 focus on fine-tuning a single parent
operator. The specific mutation strategy is adaptively selected based on the operator‚Äôs performance rank within the
population: top-tier operators trigger Parameter Calibration (m2) to consolidate gains, bottom-tier operators trigger
Logic Evolution (m1) to escape local optima, while intermediate operators are assigned a strategy stochastically.
Prompt for Mutation Actions (m1 & m2)
You are an algorithm optimizer. We have a {operator type} operator for LNS.
Problem Description: {task description}
Strategy: {advice}
(The {advice} slot is dynamically filled with one of the following strict instructions based on the rank:)
‚Ä¢ m1 (Logic Evolution): ‚ÄùGenerate novel algorithmic mechanisms or formulas to replace existing logic
components.‚Äù
‚Ä¢ m2 (Parameter Calibration): ‚ÄùAdjust current parameter settings (e.g., the degree of randomization or greedy
thresholds) to optimize operator behavior.‚Äù
Current Code: {operator code}
Task: Refine and improve this operator code based on the strategy above.
1. Refine and improve this operator strictly following the strategy provided above.
2. If you need helper functions, define them INSIDE the main function.
3. Do not give additional explanations.
‚Ä¢ Homogeneous Crossover (c1) facilitates feature recombination within the same operator type. To ensure the propa-
gation of high-quality traits, two parent operators are selected from the population using Roulette Wheel Selection
based on their historical fitness scores. The LLM is then prompted to synthesize a hybrid operator by preserving the
structural form of Parent 2 while integrating the high-level logical insights from Parent 1.
Prompt for Action c1 (Homogeneous Crossover)
You are an expert in heuristic optimization. Your task is to create a NEW {operator type} operator by
combining the ideas/logic of two parent operators.
Problem Description: {task description}
Parent 1 Code (Inspiration Source): {parent1 code}
Parent 2 Code (Structural Base): {parent2 code}
Task: Please create a new algorithm that has a similar form to Parent 2 and is inspired by Parent 1. The new
18


--- Page 19 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
algorithm should outperform both parents.
1. First, list the common ideas in Parent 1 that may give good performances. 2. Second, based on the common idea,
describe the design idea of the new algorithm and its main steps in one sentence. 3. Next, implement it in Python.
Requirements:
1. The new operator MUST follow the standard LNS {operator type} signature strictly.
2. Define all helper functions INSIDE the main function.
3. Do not give additional explanations.
‚Ä¢ Synergistic Joint Crossover (c2) addresses the inherent structural dependency between destroy and repair actions,
representing a core innovation of G-LNS. Instead of evolving operators in isolation, this strategy selects a coupled
Destroy-Repair pair using Roulette Wheel Selection based on Synergy Scores (accumulated during the evaluation
phase). The LLM is explicitly prompted to co-evolve these operators as a unified entity, ensuring the repair mechanism
is tailor-made to reconstruct the specific topological defects introduced by the destroy mechanism.
Prompt for Action c2 (Synergistic Joint Crossover)
You are an expert in heuristic optimization. We are employing a ‚ÄùSynergistic Joint Crossover (Structural Coupling)‚Äù
strategy to evolve LNS operators.
Problem Description: {task description}
Selected High-Synergy Pair:
‚Ä¢ Parent Destroy Operator: {destroy code}
‚Ä¢ Parent Repair Operator: {repair code}
Task: Evolve this pair as a UNIFIED ENTITY to create a new Destroy-Repair pair. The goal is to address the
inherent coupling between destroy and repair actions. Specifically, ensure that the generated Repair operator is
specifically tailored to reconstruct the structural defects introduced by the generated Destroy operator, thereby
maximizing their synergistic performance.
Requirements:
1. Design a NEW Destroy operator and a NEW Repair operator.
2. The new Destroy operator should create specific structural defects.
3. The new Repair operator must be designed to fix these specific defects efficiently.
4. Both must follow standard LNS signatures strictly.
5. Define all helper functions INSIDE the main functions.
6. Return ONE code block containing BOTH the new Destroy operator and the new Repair operator.
7. Do not give additional explanations.
19


--- Page 20 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
D.2. Main Framework Algorithm
Algorithm 1 presents the detailed pseudo-code for the proposed G-LNS framework. The procedure begins with the
initialization of operator populations and global metrics (Lines 1‚Äì3). The core execution flow alternates between two phases:
the Evaluation Phase (Lines 5‚Äì22), where operators are dynamically selected and scored via independent Adaptive LNS
episodes, and the Evolution Phase (Lines 23‚Äì38), where the LLM evolves the population topology based on accumulated
fitness and synergy scores periodically.
Algorithm 1 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
1: Input: Task I; Max Generations Gmax; Population Size N; Eval Episodes K; Inner Steps L = 100; Destruction Ratio
œµ = 0.2; Initial Temp T0 = 100; Cooling Rate Œ± = 0.97; Weight Update Œª = 0.5; Rewards Œ® = {œÉ1, . . . , œÉ4}; Pruning
Count M.
2: Output: Best solution x‚àóand optimized operator populations P‚àó
d, P‚àó
r .
3: Initialize: Pd, Pr with seeds; Weights W d, W r ‚Üê1; Fitness F ‚Üê0; Synergy S ‚Üê0; x‚àó‚ÜêInit(I); g ‚Üê1.
4: while g ‚â§Gmax do
5:
// Phase 1: Evaluation (Adaptive LNS Episode)
6:
Initialize episode: xcurr ‚ÜêRandomSolution(I); T ‚ÜêT0; W d, W r ‚Üê1.
7:
for l = 1 to L do
8:
Select di ‚ààPd and rj ‚ààPr via roulette wheel selection (p ‚àùw).
9:
Generate x‚Ä≤ ‚Üêrj(di(xcurr, œµ)).
10:
// Score & Acceptance
11:
if f(x‚Ä≤) < f(x‚àó) then
12:
x‚àó‚Üêx‚Ä≤; xcurr ‚Üêx‚Ä≤; œÉ ‚ÜêœÉ1 (Global Best).
13:
else if f(x‚Ä≤) < f(xcurr) then
14:
xcurr ‚Üêx‚Ä≤; œÉ ‚ÜêœÉ2 (Better).
15:
else if exp(‚àí(f(x‚Ä≤) ‚àíf(xcurr))/T) > rand(0, 1) then
16:
xcurr ‚Üêx‚Ä≤; œÉ ‚ÜêœÉ3 (Accepted).
17:
else
18:
œÉ ‚ÜêœÉ4 (Rejected).
19:
end if
20:
Update Metrics: Update W d, W r, F, and Sij using œÉ, Œª.
21:
T ‚ÜêT ¬∑ Œ±.
22:
end for
23:
// Phase 2 & 3: Evolution (Triggered every K generations/episodes)
24:
if g (mod K) = 0 then
25:
Management: Rank Pd, Pr by fitness F; prune bottom M operators.
26:
Reset fitness F, S ‚Üê0 for fair competition.
27:
while |Pd| < N or |Pr| < N do
28:
Sample strategy s ‚àà{Mutation, Homo-Cross, Joint-Cross}.
29:
if s = Mutation then
30:
Select parent op; opnew ‚ÜêLLM(Promptmut(op)).
31:
else if s = Homo-Cross then
32:
Select opa, opb; opnew ‚ÜêLLM(Prompthomo(opa, opb)).
33:
else if s = Joint-Cross then
34:
Select (di, rj) via synergy Sij; (d‚Ä≤, r‚Ä≤) ‚ÜêLLM(Promptjoint(di, rj)).
35:
end if
36:
Validation: Run sanity check on generated code; add operator if valid.
37:
end while
38:
end if
39:
g ‚Üêg + 1.
40: end while
41: Return x‚àóand elite operators.
20


--- Page 21 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
E. Designed Operators
In this section, we compile the most successful heuristics produced by G-LNS, spanning the entire suite of experimental
settings.
E.1. Discovered Operators for TSP
For the TSP, G-LNS evolved a sophisticated pair of operators that utilize State-Dependent Adaptation to navigate the search
space. We term these Adaptive Continuous-Segment Removal (ACSR) and Diversity-Adaptive Probabilistic Insertion
(DAPI).
Mechanism Analysis. The ACSR operator implements a magnitude-dependent strategy: for moderate perturbation, it
precisely identifies and removes the single most expensive continuous segment to refine local connections; for aggressive
destruction, it automatically switches to multi-segment fragmentation to prevent structural lock-in. Cooperatively, the DAPI
operator transcends fixed-parameter logic by monitoring real-time solution diversity. It dynamically tunes the temperature
of its Softmax selection mechanism‚Äîincreasing exploration (high temperature) when the solution becomes clustered, and
focusing on exploitation (low temperature) when diversity is high.
def destroy(x, destroy_cnt, dist_mat=None):
"""
Hybrid destroy operator combining:
1. Continuous removal strategy from Parent 1
2. Adaptive distance-based selection from Parent 2
3. Edge distance optimization for selecting the best continuous segment
"""
def simple_random_destroy(x, cnt):
new_x = copy.deepcopy(x)
removed = []
for _ in range(min(cnt, len(new_x))):
idx = random.randint(0, len(new_x) - 1)
removed.append(new_x[idx])
new_x.pop(idx)
return removed, new_x
if len(x) <= destroy_cnt:
return list(range(len(x))), []
if dist_mat is None or len(x) <= 1:
return simple_random_destroy(x, destroy_cnt)
new_x = copy.deepcopy(x)
removed_cities = []
n = len(new_x)
if destroy_cnt >= len(new_x):
removed_cities = copy.deepcopy(new_x)
new_x = []
return removed_cities, new_x
if destroy_cnt <= n * 0.4:
# Moderate destruction - use distance-based continuous removal
segment_scores = []
for start_idx in range(n):
segment_dist = 0
for i in range(destroy_cnt - 1):
idx1 = (start_idx + i) % n
idx2 = (start_idx + i + 1) % n
segment_dist += dist_mat[new_x[idx1]][new_x[idx2]]
if destroy_cnt < n:
idx_before = (start_idx - 1) % n
idx_start = start_idx % n
segment_dist += dist_mat[new_x[idx_before]][new_x[idx_start]]
# Edge after segment
idx_end = (start_idx + destroy_cnt - 1) % n
idx_after = (start_idx + destroy_cnt) % n
segment_dist += dist_mat[new_x[idx_end]][new_x[idx_after]]
segment_scores.append(segment_dist)
if random.random() < 0.7:
# 70% chance: remove worst segment (highest distance)
start_index = np.argmax(segment_scores)
else:
# 30% chance: probabilistic selection
scores_array = np.array(segment_scores)
weights = scores_array / scores_array.sum()
start_index = np.random.choice(range(n), p=weights)
else:
# Aggressive destruction - combine continuous removal with random elements
segments_to_remove = []
remaining_cnt = destroy_cnt
while remaining_cnt > 0 and len(segments_to_remove) < n:
max_segment_size = min(remaining_cnt, max(2, int(destroy_cnt * 0.3)))
segment_size = random.randint(1, max_segment_size)
start_idx = random.randint(0, n - 1)
segments_to_remove.append((start_idx, segment_size))
21


--- Page 22 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
remaining_cnt -= segment_size
all_indices = set()
for start_idx, segment_size in segments_to_remove:
for i in range(segment_size):
idx = (start_idx + i) % n
all_indices.add(idx)
target_indices = list(all_indices)[:destroy_cnt]
target_indices.sort(reverse=True)
for idx in target_indices:
if idx < len(new_x):
removed_cities.append(new_x[idx])
new_x.pop(idx)
return removed_cities, new_x
removal_indices = []
for i in range(destroy_cnt):
removal_idx = (start_index + i) % n
# Adjust for previous removals if we‚Äôre in circular context
if removal_idx >= len(new_x):
removal_idx = removal_idx % len(new_x)
removal_indices.append(removal_idx)
removal_indices.sort(reverse=True)
for idx in removal_indices:
if idx < len(new_x):
removed_cities.append(new_x[idx])
new_x.pop(idx)
if len(removed_cities) < destroy_cnt and len(new_x) > 0:
additional_needed = destroy_cnt - len(removed_cities)
additional_removed, new_x = simple_random_destroy(new_x, additional_needed)
removed_cities.extend(additional_removed)
return removed_cities, new_x
Listing 1. Generated Destroy Operator for TSP (ACSR)
def repair_diversity_adaptive(x, removed_cities, dist_mat):
"""
Monitors solution diversity to dynamically adjust Softmax temperature
and exploration thresholds.
"""
new_x = list(x)
# Helper: Calculate ‚Äôclustered-ness‚Äô (Diversity Metric)
def _calculate_diversity(path):
if len(path) <= 1: return 0.5
total = sum(dist_mat[path[i]][path[(i+1)%len(path)]] for i in range(len(path)))
avg = total / len(path)
return min(avg / np.max(dist_mat), 1.0)
# Helper: Softmax selection with Temperature
def _select_softmax(costs, T):
min_c = min(costs)
# Higher T -> Flatter distribution (More Exploration)
# Lower T -> Sharper distribution (More Exploitation)
weights = [math.exp(-(c - min_c)/max(1, min_c)/T) for c in costs]
total = sum(weights)
probs = [w/total for w in weights]
return random.choices(range(len(costs)), weights=probs)[0]
# [Step 1] State Analysis
diversity = _calculate_diversity(new_x)
# [Step 2] Parameter Adaptation
# Low diversity (<0.5) triggers high randomness to escape clustering
random_threshold = 0.1 + 0.4 * (1.0 - diversity)
# Inverse relationship: Low diversity -> High Temperature
temperature = 3.0 - 2.0 * diversity
# [Step 3] Insertion Loop
random.shuffle(removed_cities)
for city in removed_cities:
n = len(new_x)
# Calculate insertion costs for all positions
costs = []
for i in range(n + 1):
prev = new_x[i-1] if i > 0 else new_x[-1]
curr = new_x[i] if i < n else new_x[0]
delta = dist_mat[prev][city] + dist_mat[city][curr] - dist_mat[prev][curr]
costs.append(delta)
# Decision: Random vs. Strategic
if random.random() < random_threshold:
22


--- Page 23 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
# Pure Exploration
insert_pos = random.randint(0, n)
else:
# Strategic Phase
if random.random() < 0.8:
# Softmax Probabilistic Selection (Parent 2 Logic)
insert_pos = _select_softmax(costs, temperature)
else:
# Pure Greedy (Argmin)
insert_pos = costs.index(min(costs))
new_x.insert(insert_pos, city)
# [Step 4] Probabilistic Local Search
# Trigger 2-opt more frequently if randomness was high
if random.random() < (0.2 + 0.5 * random_threshold):
# (Fast 2-opt implementation omitted for brevity)
pass
return new_x
Listing 2. Generated Destroy Operator for TSP (DAPI)
E.2. Discovered Operators for CVRP
For the CVRP, G-LNS evolved a sophisticated pair of operators that exhibit Dynamic Adaptation to manage the trade-off
between exploration and exploitation. We term these Progressive Stochastic-Worst Removal (PSWR) and Adaptive
Context-Aware Greedy Insertion (ACAGI).
Mechanism Analysis. The PSWR operator introduces a time-dependent strategy: it initiates with random removal to
perform global perturbation and progressively shifts towards worst-cost removal to refine local inefficiencies, controlled by
a dynamic ratio œÅt. Cooperatively, the ACAGI operator transcends static logic by monitoring real-time insertion difficulty. It
adaptively increases its search depth (Regret-k) and exploration noise when the solution space becomes constrained, while
employing a multi-objective scoring function (balancing distance and capacity waste) to minimize the number of vehicles
used.
def destroy(x, destroy_cnt, problem_data):
"""Hybrid Random-Worst Removal: Combines random exploration with worst-distance exploitation"""
def calculate_saving(route, node_idx, dist_mat, depot):
node = route[node_idx]
prev_node = route[node_idx-1] if node_idx > 0 else depot
next_node = route[node_idx+1] if node_idx < len(route)-1 else depot
cost_with = dist_mat[prev_node][node] + dist_mat[node][next_node]
cost_without = dist_mat[prev_node][next_node]
return cost_with - cost_without
# Initialization
new_x = [route[:] for route in x]
removed_customers = []
depot = problem_data.get(‚Äôdepot_idx‚Äô, 0)
dist_mat = problem_data.get(‚Äôdistance_matrix‚Äô)
all_customers = [c for route in new_x for c in route]
if len(all_customers) <= destroy_cnt:
return all_customers, [[]]
# Initial calculation of savings
node_savings = {}
for r_idx, route in enumerate(new_x):
for i, node in enumerate(route):
saving = calculate_saving(route, i, dist_mat, depot)
node_savings[node] = saving
removed_count = 0
# Progressive Removal Strategy
while removed_count < destroy_cnt and len(all_customers) - removed_count > 0:
greedy_ratio = removed_count / destroy_cnt
remaining_customers = [c for route in new_x for c in route]
if not remaining_customers:
break
# Re-evaluate savings for current partial solution accuracy
if removed_count > 0:
node_savings = {}
for r_idx, route in enumerate(new_x):
for i, node in enumerate(route):
saving = calculate_saving(route, i, dist_mat, depot)
node_savings[node] = saving
# Probabilistic Switch based on Progress
if random.random() < greedy_ratio:
23


--- Page 24 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
candidate_nodes = [(node_savings[node], node) for node in remaining_customers]
candidate_nodes.sort(key=lambda x: x[0], reverse=True)
top_k = min(3, len(candidate_nodes))
selected_node = random.choice(candidate_nodes[:top_k])[1]
else:
selected_node = random.choice(remaining_customers)
removed_customers.append(selected_node)
for route in new_x:
if selected_node in route:
route.remove(selected_node)
break
removed_count += 1
new_x = [r for r in new_x if len(r) > 0]
return removed_customers, new_x
Listing 3. Generated Destroy Operator for CVRP (PSWR)
def insert(x, removed_customers, problem_data):
"""
Hybrid Greedy-Regret Insertion with Cached Loads and Adaptive Exploration
"""
new_x = [list(route) for route in x]
demands = problem_data[‚Äôdemands‚Äô]
capacity = problem_data[‚Äôcapacity‚Äô]
dist_mat = problem_data[‚Äôdistance_matrix‚Äô]
depot = problem_data.get(‚Äôdepot_idx‚Äô, 0)
def insertion_cost_delta(route, pos, customer):
if not route:
# Empty route
return dist_mat[depot][customer] + dist_mat[customer][depot]
prev_node = route[pos-1] if pos > 0 else depot
next_node = route[pos] if pos < len(route) else depot
added = dist_mat[prev_node][customer] + dist_mat[customer][next_node]
removed = dist_mat[prev_node][next_node]
return added - removed
route_loads = [sum(demands[c] for c in route) for route in new_x]
k_regret = 2
exploration_factor = 0.3
insertion_difficulty = 0.0
difficulty_decay = 0.8
cust_demands = [demands[c] for c in removed_customers]
for customer_idx, (customer, cust_demand) in enumerate(zip(removed_customers, cust_demands)):
feasible_insertions = []
# Phase 1: Fast feasibility check with cached loads
for r_idx, (route, route_load) in enumerate(zip(new_x, route_loads)):
if route_load + cust_demand > capacity:
continue
route_len = len(route)
positions = range(route_len + 1)
for pos in positions:
cost_inc = insertion_cost_delta(route, pos, customer)
feasible_insertions.append({
‚Äôcost‚Äô: cost_inc,
‚Äôroute_idx‚Äô: r_idx,
‚Äôposition‚Äô: pos,
‚Äôroute_load‚Äô: route_load,
‚Äôroute_length‚Äô: route_len
})
new_route_cost = dist_mat[depot][customer] + dist_mat[customer][depot]
feasible_insertions.append({
‚Äôcost‚Äô: new_route_cost,
‚Äôroute_idx‚Äô: len(new_x),
‚Äôposition‚Äô: 0,
‚Äôroute_load‚Äô: 0,
‚Äôroute_length‚Äô: 0
})
if not feasible_insertions:
new_x.append([customer])
route_loads.append(cust_demand)
insertion_difficulty = insertion_difficulty * difficulty_decay + 1.0
continue
# Phase 2: Adaptive selection strategy
feasible_insertions.sort(key=lambda x: x[‚Äôcost‚Äô])
best_cost = feasible_insertions[0][‚Äôcost‚Äô]
remaining_customers = len(removed_customers) - customer_idx - 1
if remaining_customers > 0 and len(feasible_insertions) < 3:
insertion_difficulty = insertion_difficulty * difficulty_decay + 1.0
else:
insertion_difficulty = insertion_difficulty * difficulty_decay
difficulty_threshold = len(removed_customers) * 0.3
24


--- Page 25 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
if insertion_difficulty > difficulty_threshold:
k_regret = min(4, len(feasible_insertions))
exploration_factor = 0.4
# More exploration
else:
k_regret = min(3, max(2, len(feasible_insertions) // 3))
exploration_factor = 0.2
# More greedy
if len(feasible_insertions) >= k_regret:
regret_values = []
max_regret_candidates = min(k_regret, len(feasible_insertions))
for i in range(max_regret_candidates):
ins = feasible_insertions[i]
cost_diff = ins[‚Äôcost‚Äô] - best_cost
load_ratio = ins[‚Äôroute_load‚Äô] / capacity if capacity > 0 else 0
load_penalty = 0.15 * load_ratio
length_penalty = 0.05 * (ins[‚Äôroute_length‚Äô] / 20) if ins[‚Äôroute_length‚Äô] > 0 else 0
regret_score = cost_diff + load_penalty + length_penalty
if random.random() < exploration_factor:
noise = random.uniform(-0.1, 0.1) * best_cost if best_cost > 0 else 0
regret_score += noise
regret_values.append((regret_score, i))
if regret_values:
regret_values.sort(key=lambda x: x[0])
if random.random() < (0.8 - 0.2 * (insertion_difficulty / difficulty_threshold)):
selected_idx = regret_values[0][1]
else:
top_m = min(3, len(regret_values))
weights = [1.0 / (i + 1) for i in range(top_m)]
total_weight = sum(weights)
rand_val = random.random() * total_weight
cumulative = 0
for j, w in enumerate(weights):
cumulative += w
if rand_val <= cumulative:
selected_idx = regret_values[j][1]
break
else:
selected_idx = regret_values[0][1]
selected = feasible_insertions[selected_idx]
else:
selected = feasible_insertions[0]
else:
selected = feasible_insertions[0]
# Phase 3: Apply insertion with cache update
if selected[‚Äôroute_idx‚Äô] == len(new_x):
new_x.append([customer])
route_loads.append(cust_demand)
else:
route = new_x[selected[‚Äôroute_idx‚Äô]]
route.insert(selected[‚Äôposition‚Äô], customer)
route_loads[selected[‚Äôroute_idx‚Äô]] += cust_demand
final_routes = []
final_loads = []
for route, load in zip(new_x, route_loads):
if route:
final_routes.append(route)
final_loads.append(load)
if len(final_routes) > 1:
consolidated_routes = []
consolidated_loads = []
used = [False] * len(final_routes)
for i in range(len(final_routes)):
if used[i]:
continue
current_route = final_routes[i]
current_load = final_loads[i]
for j in range(i + 1, len(final_routes)):
if used[j]:
continue
if current_load + final_loads[j] <= capacity:
if len(current_route) + len(final_routes[j]) < 15:
current_route.extend(final_routes[j])
current_load += final_loads[j]
used[j] = True
consolidated_routes.append(current_route)
consolidated_loads.append(current_load)
used[i] = True
final_routes = consolidated_routes
final_loads = consolidated_loads
if not final_routes:
final_routes = [[]]
final_loads = [0]
25


--- Page 26 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
return final_routes
Listing 4. Generated Repair Operator for CVRP (ACAGI)
F. Details of Results
F.1. Details on OVRP
Open Vehicle Routing Problem (OVRP). It is pertinent to emphasize that OVRP often presents a greater challenge for
heuristic design compared to the standard CVRP, particularly for constructive approaches. In the standard CVRP, the
requirement to return to the depot forces the route to form a closed loop. This naturally prevents the algorithm from creating
overly elongated paths, as the cost of returning to the depot effectively limits how far a vehicle can wander. In contrast,
OVRP removes this return requirement. Without the need to close the loop, sequential constructive heuristics (like those
employed by the baselines) often fail to maintain a compact route structure. They tend to greedily extend the path to the
nearest neighbors without considering the global shape, resulting in loose, fragmented routes that are difficult to optimize.
Following the experimental setup described in Appendix C.1, we compare G-LNS against the rigorous OR-Tools solver
(configured with a time limit consistent with CVRP settings) and LLM-based AHD methods. The quantitative results are
summarized in Table 4.
Table 4. Performance comparison on Open Vehicle Routing Problem (OVRP) across five problem sizes. The Gap is calculated relative to
the best solution found among all methods. The best results are highlighted in bold.
OVRP10
OVRP20
OVRP50
OVRP100
OVRP200
Method
Gap
Obj.
Gap
Obj.
Gap
Obj.
Gap
Obj.
Gap
Obj.
OR-Tools
0.00%
2.2885
0.00%
3.4277
0.00%
5.6736
0.00%
8.8563
2.05%
15.1893
EoH
15.21%
2.6366
27.21%
4.3605
34.89%
7.6530
33.30%
11.8056
36.67%
20.3424
ReEvo
15.15%
2.6353
31.17%
4.4960
29.30%
7.3360
31.50%
11.6463
36.33%
20.2910
Ours (G-LNS)
0.02%
2.2890
0.27%
3.4371
0.75%
5.7163
0.47%
8.8979
0.00%
14.8841
Performance Analysis. On small to medium-scale instances (N ‚àà{10, 20, 50, 100}), the exact solver logic of OR-Tools
remains highly effective, achieving optimal or near-optimal solutions. In this regime, G-LNS exhibits robust competitiveness,
maintaining optimality gaps consistently below 0.8%. However, the superior scalability of G-LNS becomes evident on
large-scale instances (N = 200). While OR-Tools begins to struggle under the computational time limit‚Äîyielding a
suboptimal gap of 2.05%‚ÄîG-LNS successfully identifies significantly better solutions, reducing the objective cost from
15.19 (OR-Tools) to 14.88, thereby establishing a new best-known frontier (Gap 0.00%).
In stark contrast, the constructive LLM-based baselines fail to adapt to the open-route structure. As anticipated, their reliance
on sequential decision-making leads to poor performance, with optimality gaps exceeding 30% on instances where N ‚â•50.
This significant performance disparity highlights the critical advantage of the LNS paradigm: by explicitly evolving Destroy
and Repair operators to iteratively reshape existing topologies rather than constructing them step-by-step, G-LNS effectively
avoids the local optima that trap constructive methods.
F.2. Details on Benchmarks
In this subsection, we provide the comprehensive results on the standard TSPLib and CVRPLib benchmarks. We compare
G-LNS against LLM-based AHD methods, including EoH, ReEvo, and the state-of-the-art heuristic set method EoH-S.
Summary of Results. Table 5 presents the average optimality gap across all instances within each respective category.
G-LNS outperforms all baselines across all benchmark sets.
Experimental Setup. We adopt the evaluation configuration directly from EoH-S (Liu et al., 2025). Consistent with their
protocol, all baseline methods are evaluated using normalized node coordinates mapped to the range [0, 1]2. The scaling
factor is derived from the maximum spatial extent of each instance:
Scaling factor = max(xmax ‚àíxmin, ymax ‚àíymin)
(12)
26


--- Page 27 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
Table 5. Comparison of results on TSPLib and CVRPLib Benchmarks. The best values are highlighted in bold.
Benchmarks
EoH
ReEvo
EoH-S
Ours
TSPLib
18.1%
21.3%
9.1%
2.8%
CVRPLib A
31.7%
31.8%
22.5%
7.9%
CVRPLib B
36.2%
33.9%
18.3%
8.7%
CVRPLib E
32.3%
29.7%
24.3%
7.9%
CVRPLib F
53.9%
64.6%
40.1%
15.9%
CVRPLib M
44.2%
43.0%
29.1%
15.1%
CVRPLib P
26.8%
26.0%
16.7%
8.1%
CVRPLib X
26.8%
26.5%
19.1%
11.2%
This normalization step is standard for constructive heuristics to ensure numerical stability. Furthermore, following the
standard protocol in these baselines, the reported optimality gaps are calculated relative to the best-known solutions obtained
by the LKH-3 solver.
In contrast, our proposed G-LNS operates directly on the raw, unnormalized instance data. This distinction highlights
that the operators evolved by G-LNS capture the intrinsic topological logic of the routing problems rather than relying on
specific coordinate scales.
Detailed Results. The detailed optimality gaps are presented as follows:
‚Ä¢ TSPLib: Table 6 reports the results on symmetric TSPLib instances.
‚Ä¢ CVRPLib: Table 7 (Sets A, B, E, F, M, P), Table 8 (Set X) report the results on the Capacitated Vehicle Routing
Problem benchmarks.
Table 6. Detailed optimality gaps (%) on TSPLib instances. Baselines are evaluated with coordinate normalization, while Ours operates
on raw data.
Instance
EoH
ReEvo
EoH-S
Ours
Instance
EoH
ReEvo
EoH-S
Ours
TSPLib Results
a280
24.7
30.3
13.7
4.2
pr136
16.4
9.9
6.0
1.9
berlin52
16.9
20.6
10.3
3.1
pr144
17.4
14.2
4.9
1.5
bier127
18.2
14.7
11.4
2.8
pr152
20.7
28.9
11.1
3.4
ch130
15.8
28.0
4.9
1.5
pr226
19.2
19.5
9.8
2.8
ch150
20.2
23.2
7.3
2.0
pr264
22.2
15.2
7.7
2.2
d198
15.6
17.3
16.9
5.4
pr299
28.5
18.5
11.9
3.7
d493
18.4
19.0
12.2
3.5
pr439
24.3
20.5
11.0
3.3
d657
19.5
17.6
15.5
3.9
rat99
14.2
22.3
13.0
3.8
eil51
15.8
12.2
4.3
1.1
rat195
6.7
9.6
6.5
6.8
eil76
12.2
14.4
6.6
1.9
rat575
14.1
20.5
9.9
3.0
eil101
13.3
22.4
9.7
2.6
rat783
18.9
24.6
11.4
3.5
fl417
29.4
31.0
14.3
4.5
rd100
15.3
28.1
8.5
2.1
gil262
20.9
21.6
11.7
3.2
rd400
14.1
20.4
12.5
3.4
kroA100
11.9
34.7
6.1
1.8
st70
9.3
17.5
2.6
0.8
kroB100
22.5
28.4
11.6
2.4
ts225
10.5
18.7
3.4
0.9
kroC100
18.2
17.9
4.4
1.2
tsp225
21.3
16.5
9.1
2.7
kroD100
17.0
15.4
9.1
2.7
u159
28.3
27.3
8.5
2.9
kroE100
19.3
22.0
6.7
2.1
u574
21.6
22.8
14.0
4.1
kroA150
14.9
23.5
9.1
2.5
u724
16.4
24.0
12.7
3.6
kroB150
12.0
28.0
9.0
2.3
pr1002
21.2
23.3
14.0
4.3
kroA200
21.3
16.1
8.3
3.0
pcb442
18.5
18.1
9.9
3.1
kroB200
19.7
15.6
6.6
1.9
p654
27.8
18.9
9.6
3.6
lin105
14.7
40.3
3.7
1.1
lin318
12.6
33.5
10.6
2.9
pr76
16.2
31.3
4.0
1.4
pr107
14.6
2.6
4.5
3.5
pr124
22.9
22.1
5.4
1.8
Average.
18.1
21.3
9.1
2.8
27


--- Page 28 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
Table 7. Detailed optimality gaps (%) on CVRPLib instances (Set A, B, E, F, M, P). Results are grouped by dataset. The best results are
highlighted in bold.
Instance
EoH
ReEvo
EoH-S
Ours
Instance
EoH
ReEvo
EoH-S
Ours
Set A
A-n32-k5
33.3
35.1
32.3
7.0
A-n48-k7
30.9
31.5
27.4
15.1
A-n33-k5
29.7
21.4
20.5
2.6
A-n53-k7
22.9
38.2
19.4
7.4
A-n33-k6
24.1
29.9
21.0
7.1
A-n54-k7
20.5
34.5
12.7
9.7
A-n34-k5
21.2
20.2
16.9
5.4
A-n55-k9
34.8
25.3
26.5
5.9
A-n36-k5
39.8
35.7
29.5
8.8
A-n60-k9
38.8
30.2
32.0
9.3
A-n37-k5
40.5
36.4
31.2
8.1
A-n61-k9
48.3
45.5
23.4
5.1
A-n37-k6
35.5
35.2
15.0
4.2
A-n62-k8
33.4
33.0
22.0
8.5
A-n38-k5
37.0
31.2
25.8
8.6
A-n63-k9
28.4
18.1
13.3
2.6
A-n39-k5
27.1
19.7
22.1
11.9
A-n63-k10
30.4
38.3
20.4
18.3
A-n39-k6
24.9
39.2
29.4
8.7
A-n64-k9
30.6
31.8
17.6
6.6
A-n44-k6
19.0
20.0
19.3
10.7
A-n65-k9
37.7
47.3
33.7
11.6
A-n45-k6
28.9
44.4
18.0
3.6
A-n69-k9
34.7
28.0
20.7
6.9
A-n45-k7
22.0
19.7
11.2
4.2
A-n80-k10
31.6
32.1
20.0
7.4
A-n46-k7
50.5
36.5
26.8
6.7
Average.
31.7
31.8
22.5
7.9
Set B
B-n31-k5
13.8
8.8
7.1
2.3
B-n51-k7
28.8
33.2
11.6
1.1
B-n34-k5
15.3
28.1
8.1
6.7
B-n52-k7
50.3
73.2
16.8
5.7
B-n35-k5
18.4
31.2
19.6
2.5
B-n56-k7
76.0
46.1
23.8
15.2
B-n38-k6
43.1
31.8
22.6
6.4
B-n57-k7
46.7
38.0
19.3
6.2
B-n39-k5
90.8
90.7
24.7
20.5
B-n57-k9
16.0
16.9
16.6
5.6
B-n41-k6
18.4
21.7
13.2
8.3
B-n63-k10
29.3
35.4
19.5
9.2
B-n43-k6
21.2
20.3
20.1
8.0
B-n64-k9
60.8
41.1
18.6
10.2
B-n44-k7
22.9
16.6
19.0
8.4
B-n66-k9
22.6
19.6
11.7
7.6
B-n45-k5
28.1
28.8
19.1
18.0
B-n67-k10
39.8
38.9
23.2
12.8
B-n45-k6
44.1
55.5
21.5
13.5
B-n68-k9
27.6
18.7
16.3
9.1
B-n50-k7
46.8
38.5
24.0
5.4
B-n78-k10
52.7
23.9
27.6
8.2
B-n50-k8
19.3
22.9
10.6
8.6
Average.
36.2
33.9
18.0
8.7
Set E
E-n22-k4
16.8
31.1
26.3
3.3
E-n76-k8
36.5
31.0
28.6
7.8
E-n23-k3
20.8
18.8
21.1
7.9
E-n76-k10
31.9
28.0
34.0
10.3
E-n30-k3
22.6
15.9
12.5
1.9
E-n76-k14
34.4
24.7
20.7
8.0
E-n33-k4
26.6
20.8
13.8
5.2
E-n101-k8
42.5
37.5
28.1
10.7
E-n51-k5
29.4
25.1
15.7
15.0
E-n101-k14
51.5
47.2
32.5
9.1
E-n76-k7
41.7
46.6
33.7
7.2
Average.
32.3
29.7
24.3
7.9
Set F
F-n45-k4
41.0
62.7
45.9
7.0
F-n135-k7
39.9
61.4
28.9
22.0
F-n72-k4
78.9
69.7
45.6
18.6
Average.
53.3
64.6
40.1
15.9
Set M
M-n101-k10
44.4
48.6
22.3
10.4
M-n200-k16
45.3
40.2
31.6
18.9
M-n121-k7
38.0
47.6
22.9
22.3
M-n200-k17
46.2
35.7
35.4
13.7
M-n151-k12
46.8
42.8
33.2
10.1
Average.
44.2
43.0
29.1
15.1
Set P
P-n16-k8
3.3
2.9
3.1
2.9
P-n51-k10
36.1
31.6
20.5
9.1
P-n19-k2
14.3
30.7
10.2
21.7
P-n55-k7
29.8
30.4
17.2
4.9
P-n20-k2
13.5
23.7
6.1
13.0
P-n55-k10
28.6
16.8
23.7
3.9
P-n21-k2
20.8
27.2
12.5
3.6
P-n55-k15
28.3
13.8
10.3
2.7
P-n22-k2
18.4
24.1
9.4
5.9
P-n60-k10
32.4
32.2
17.0
6.0
P-n22-k8
28.7
34.7
20.8
4.4
P-n60-k15
38.2
30.8
14.3
11.2
P-n23-k8
20.5
15.6
8.9
6.0
P-n65-k10
28.6
33.1
26.8
7.7
P-n40-k5
19.7
23.4
21.5
4.0
P-n70-k10
29.2
32.6
14.3
8.0
P-n45-k5
34.0
45.1
25.1
7.1
P-n76-k4
43.6
20.7
20.1
15.9
P-n50-k7
30.0
19.8
21.8
8.2
P-n76-k5
26.0
27.8
21.8
9.9
P-n50-k8
26.7
33.3
18.4
6.9
P-n101-k4
36.8
29.1
19.9
12.2
P-n50-k10
28.0
19.5
20.1
10.7
Average.
26.8
26.0
16.7
8.1
28


--- Page 29 ---
G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design
Table 8. Detailed optimality gaps (%) on CVRPLib instances (Sets X). Results are grouped by dataset. The best results are highlighted in
bold.
Instance
EoH ReEvo EoH-S Ours
Instance
EoH ReEvo EoH-S Ours
Set X
X-n101-k25
48.7
40.0
24.9
7.4
X-n204-k19
23.4
19.8
19.2
14.2
X-n106-k14
7.7
10.8
6.5
3.6
X-n209-k16
20.4
17.0
11.7
8.1
X-n110-k13
20.2
24.8
16.3
12.8
X-n214-k11
30.3
32.5
23.0
18.9
X-n115-k10
50.8
46.3
39.4
11.3
X-n219-k73
1.8
48.9
1.4
7.1
X-n120-k6
20.4
19.3
18.3
16.8
X-n223-k34
26.2
17.1
14.1
9.7
X-n125-k30
16.0
21.0
14.2
7.4
X-n228-k23
38.5
26.9
26.2
11.9
X-n129-k18
20.2
14.0
15.4
13.0
X-n233-k16
53.6
64.3
34.2
16.4
X-n134-k13
61.0
50.8
24.6
13.0
X-n237-k14
23.1
19.0
18.3
16.6
X-n139-k10
23.5
24.8
19.7
14.3
X-n242-k48
15.1
10.4
8.8
6.1
X-n143-k7
53.5
43.8
35.4
13.5
X-n247-k50
35.8
33.2
29.7
9.2
X-n148-k46
30.7
17.0
18.6
10.8
X-n251-k28
11.5
14.3
10.7
8.2
X-n153-k22
44.5
39.0
28.7
10.0
X-n256-k16
19.9
17.2
15.0
8.4
X-n157-k13
6.6
21.6
6.3
3.7
X-n261-k13
33.7
35.0
26.3
15.6
X-n162-k11
12.7
24.4
17.5
9.5
X-n266-k58
9.2
10.7
8.0
7.4
X-n167-k10
27.5
23.8
19.7
16.4
X-n270-k35
19.4
23.4
13.9
12.1
X-n172-k51
43.4
40.8
35.9
14.2
X-n275-k28
13.8
21.4
11.7
5.9
X-n176-k26
35.2
37.0
27.0
11.9
X-n280-k17
23.3
22.1
25.0
14.8
X-n181-k23
7.5
15.6
6.6
2.5
X-n284-k15
27.7
23.6
20.7
16.7
X-n186-k15
22.9
20.7
17.0
14.7
X-n289-k60
22.1
25.0
11.8
9.6
X-n190-k8
17.0
14.9
18.2
8.2
X-n294-k50
50.7
36.4
22.1
17.4
X-n195-k51
27.5
37.5
29.6
13.5
X-n298-k31
38.7
18.8
19.9
10.0
X-n200-k36
18.3
16.9
10.4
7.7
Average.
26.8
26.5
19.1
11.2
29
