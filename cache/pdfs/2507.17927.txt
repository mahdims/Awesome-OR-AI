--- Page 1 ---
SmartAPS: Tool-augmented LLMs for Operations
Management
AAAI25 - Combining AI and OR/MS for Better Trustworthy Decision Making
Timothy T. Yu1, Mahdi Mostajabdaveh1, Serge J. Byusa1, Rindra Ramamonjison1,
Giuseppe Carenini1,2, Kun Mao3, Zirui Zhou1, Yong Zhang1
1Huawei Technologies Canada
2University of British Columbia
3Huawei Cloud Computing Technologies
{timothyt.yu, mahdi.mostajabdaveh1, jabo.byusa,
rindranirina.ramamonjison, zirui.zhou, yong.zhang3}@huawei.com
giuseppe.carenini@huawei.com, maokun@huawei.com
Abstract
Artificial intelligence is well-positioned to support every stage of the entire OR
project life cycle. Specific to this paper, recent advancements in natural language
processing has presented intriguing opportunities to enhance the interface between
operations planners and traditional OR algorithms and tools. An advanced planning
system (APS) is a sophisticated software that leverages OR tools to help operations
planners define, create, and interpret an operational plan. While highly beneficial,
many customers are priced out of using an APS due to the ongoing costs of OR con-
sultants for maintenance. We present SMARTAPS – a conversational system built on
an agentic framework to reduce operations planners dependency on OR consultants.
Our system provides operations planners with an intuitive, natural language chat
interface, allowing them to make queries, perform counterfactual reasoning, receive
recommendations, and execute scenario analysis (what-if and why-not analyses) on
the operational plans created by the APS. SMARTAPS utilizes generic tools created
by OR consultants to generalize to the specific problem. It features a module that
generates and recommends new tools when faced with an unsupported query in a
human-in-the-loop manner.
1
Introduction
Operations research (OR) is a discipline within applied mathematics that delivers ad-
vanced analytical tools to aid in daily decision-making and problem-solving. It has seen
many successful applications and has been beneficial in different domains such as housing
[7], emergency response [14, 22], and healthcare operations [15]. Such success is largely
attributed to the availability of OR tools and solvers, which have been instrumental in
providing an analytical lens for decision-making.
While beneficial, using OR tools and solvers requires a high level of expertise specific to
the domain and custom software. Artificial intelligence (AI) is well-positioned to support
every stage of the OR project life cycle [3]. Specifically, large language models (LLMs)
have a crucial role in making these OR tools and solvers more accessible and user-friendly.
For instance, propose an LLM-based framework [17] has been proposed that takes in
1
arXiv:2507.17927v1  [cs.AI]  23 Jul 2025


--- Page 2 ---
a natural language problem description and formulates the corresponding optimization
model code. GPT-3.5-TURBO [2] was tested on this relatively simple modeling dataset
and was able to accurately convert the natural language descriptions into modeling code
[18]. However, LLMs have been reported to be unable to do so reliably for real-world
problems that tend to be more complex [24].
In operations management, OR methods are utilized to determine the optimal deci-
sions over a period of time (planning horizon) to produce an organization’s goods and
services [19]. This is typically outlined by an operational plan, which is a detailed roadmap
satisfying the requirements and strategic goals of the organization. Advanced planning
systems (APSs) were developed to help create, manage, and interpret such operational
plans, and are primarily used to model decisions in operations planning and supply chain
management. APSs are useful for tracking and planning business details (e.g., bills of ma-
terials, customer orders, and inventory stock) and business specifications (e.g., resource
shortages, equipment maintenance). They also commonly provide visualizations (e.g.,
Gantt charts) and a graphical user interface (GUI) so the user can inspect and manipu-
late a plan. However, the high costs associated with implementing and maintaining an
APS often hinder their widespread adoption. These costs stem from limited automation,
need for customization, and reliance on expert consultants [23].
In collaboration with OR experts and supply chain planners, we introduce SMAR-
TAPS to address these shortcomings by automating analyses and making the interface
for APS more intuitive. SMARTAPS is a conversational interface that allows the user to
perform advanced tasks (e.g., scenario analysis, feasibility relaxation) using natural lan-
guage. SMARTAPS is made up of three main modules, each built on either decoder-based
[5] or encoder-based [25] relatively light LLMs.
Tool-augmented LLM. An application programming interface (API), or tool, can be
called to provide external functionalities to the LLM. For instance, answering simple
mathematical questions is challenging for LLMs because it requires creativity, reasoning,
and numerical calculation [10]. By grounding an LLM’s response to a calculator API,
an LLM may act as an intermediary between the user and the tool—often resulting in
more efficient and accurate responses [11, 1, 21]. Inspired by this work, SMARTAPS
leverages customized algorithms and tools to perform complex analyses (e.g., running an
optimization solver).
2
Background in Artificial Intelligence
Advanced planning systems. Industry leaders of APS solutions include Kinaxis’ Plan-
ning One [8], SAP’s S/4HANA [20], and Oracle’s Fusion Cloud Supply Chain & Planning
[16]. The most significant trends in the APS market are the customization capabilities for
users through microservices and AI [23]. For instance, Microsoft Copilot [13] introduces
an AI copilot for querying inventory stock levels using natural language. While Microsoft
Copilot [13] leverages AI for Dynamics 365 Supply Chain Management, it is currently only
used to query details of the system. In addition to simply understanding the current data
and plan, planners can use SMARTAPS for insightful tasks like operational forecasting,
scenario analysis, adding restrictions, and plan comparison.
Retrieval-augmented generation.
Retrieval-augmented generation (RAG) is a
technique that retrieves relevant information from an external knowledge base to improve
an LLM’s applicability in knowledge-intensive domains [4]. RAG has been reported to
improve the correctness of an LLM’s output in knowledge-intensive tasks such as gen-
erating preoperative clinical instructions [6] and scientific literature question answering
[12] specific to operations management, this technique could be used to search for avail-
able and relevant code or information to help an LLM answer questions about an APS.
Real-world applications of APS require more sophisticated analyses beyond what a gen-
eral external database can provide. Thus, SMARTAPS uses retrieval to select the most
relevant customized tools from a catalog carefully curated by an OR consultant.
2


--- Page 3 ---
Figure 1: SMARTAPS planning application. On the left is the chat interface, where the
top shows a blue table detailing the operations plan at a given plant. The expanded
“Took 4 steps” shows the process being executed. The right panel contains the task list
that logs the tools executed throughout the conversation.
3
Methodology
3.1
System Overview
Operations planners and supply chain experts from industry were consulted throughout
the iterative design process of SMARTAPS, from building the database of useful tools
to designing the UI. Based on their requirements and feedback, we developed our sys-
tem using a technology stack of Chainlit (chat interface), Python, Poetry (dependency
management), and ChromaDB (database).
Leveraging Chainlit on the client side, a chat interface is used to accept natural lan-
guage as input to interact with an APS. The chat interface and other functionalities are
powered with MISTRAL-7B-INSTRUCT-V0.1 [5], a relatively lightweight LLM using the
Hugging Face inference endpoint. We employ ChromaDB for its capability to store em-
bedding vectors for tool retrieval. BGE-LARGE-EN-V1.5 [25] is the embedding model
deployed on a single Tesla P100 PCIe 16GB GPU and used by ChromaDB to create the
embedding vectors. To solve and analyze the underlying optimization model, we utilize
Huawei Cloud’s OptVerse AI Solver [9].
3.2
System Implementation
Figure 1 presents our interactive chat interface that facilitates conversations between the
user (Planner) and SMARTAPS (OptVerse AI). Users pose questions and receive answers
in various formats like text, tables, or graphs. A notable feature is the detailed step-
by-step procedure list, which reveals the process behind the response (Figure 1 – center
expanded “Took 4 steps” pane). Additionally, the interface includes a task list on the
right panel (Figure 1), visually indicating the history and status of tasks.
Figure 2 demonstrates the SMARTAPS framework at a high level. The framework
3


--- Page 4 ---
Figure 2: The SMARTAPS framework. The conversation manager handles the interac-
tion with the user by detecting the user intent and refining the corresponding tool output
response. The tool retriever selects the required tool from the API catalog database.
Some tools would interact with OR engine endpoints (e.g., optimization solver) which
must be deployed and accessible by the system. The tool manager detects the corre-
sponding optimization model, APS data, and API input parameters from the conversation
history and executes the tool.
allows a planner to interact with an APS through a chat interface via APIs created by the
OR consultant. The framework contains three main modules: (1) conversation manager,
(2) tool retriever, and (3) tool manager.
At a high level, the conversation manager (Section 4.1) keeps track of the state of
the conversation (e.g., user intent, message log) and operation (e.g., current plan, APS
data, optimization models). Specifically, it uses an LLM for understanding the intent of
the user and explaining the solution within the context of the conversation. The tool
retriever (Section 4.2) uses an encoder to compute the sentence embedding from the
natural language user query, upon which semantic textual similarity search is utilized to
retrieve the most relevant tool API. The tool manager (Section 4.3) receives the retrieved
API, and an LLM is used to extract the required input parameters from the user query
in the context of the conversation. The tool manager will then execute the tool API and
return the output to the session manager.
Tool API. The tools are saved as one collection in ChromaDB but may be described as
five categories: query plan, why-not, what-if, compare plan, and display plan. Query plan
tools inspect the operational plan and data (e.g., How many sets of tires are made today?).
Why-not tools analyze scenarios where new requirements need changes in the optimization
model (e.g., I want to only use the plant in Vancouver). What-if tools analyze scenarios
where the APS data changes (e.g., How would receiving 100 kg of natural rubber on
2024-04-17 impact my plan?). Compare plan tools compare two generated plans (e.g.,
How many more tires are produced in the new plan?). Display plan tools generate tables
and charts for the user (e.g., Show me the operations plan).
Figure 3 shows the contents of a tool contract that specifies the function, requirements,
and outputs of the tool. Each contract contains a description, example queries, NL output,
function call, input, and output. The description and example queries are concatenated
and used for tool retrieval. The “NL output” is a template used to generate an initial
structured natural language response. The “function call” is used by the tool manager to
execute the tool. The “input” and “output” schemas provide the user with information
about the request to make and the response to expect from the tool. Notably, the output
4


--- Page 5 ---
Figure 3: Example of an API contract that provides the description, examples, natural
language output, function call, input schema, and output object pf the API.
schema may return a dataframe that Chainlit can render into a plot, table, or figure.
4
System Components Details
4.1
Conversation Manager
Figure 4 demonstrates the two tasks of the conversation manager: (1) user intent detec-
tion (top), and (2) refine the tool output (bottom) and return a conversation-grounded
response. Both tasks are executed by prompting an LLM (in our case, we use MISTRAL-
7B-INSTRUCT-V0.1 [5]) with the prompts as described in Figure 4 to the right of their
corresponding tasks.
Intent detection aims to categorize the user query into one of two INTENT OPTIONS,
either categorized as CASUAL CONVERSATION or OPERATIONS PLANNING, where
CASUAL CONVERSATION is to continue the current conversation without the need to
execute a new tool. OPERATIONS PLANNING is when the user has a request about
the APS. The prompt to extract this intent is shown in the green classification prompt
of Figure 4.
Response refining. If the intent is to continue a casual conversation, the LLM will
reference the conversation when responding to the user query. Conversely, if the intent
is to ask something about operations planning, the tool retriever will be called.
The
output object returned to the conversation manager may contain some graphics (e.g.,
table), which can be rendered using Chainlit. Each output object also contains a natural
language response that must be refined and returned to the user. These natural language
outputs are simple (e.g., “5 hours extra delay”). The tool API does not have knowledge
of the state of the conversation. Therefore, to refine the response to be suitable within
the context of the conversation, the tool output and the conversation log are passed to
the LLM as outlined in the blue refine response prompt of Figure 4. For example, the
tool output “5 hours extra delay” may be refined to be “The expected delivery time for
Order A will be delayed by 5 hours if Order B is prioritized.”
5


--- Page 6 ---
Figure 4: Conversation Manager. It is responsible for user intent detection (top) and
tool output refinement (bottom). Both tasks leverage LLMs - the prompt template is
shown to the right of each corresponding task. When refining the tool output (bottom),
the tool output is the output of calling a tool API. Thus, the entire conversation manager
module can be seen as a tool-augmented LLM that calls a tool and leverages it to enhance
its response.
Figure 5: Tool Retriever. Retrieves the tool most relevant to the input query. Semantic
similarity is calculated between the query and catalog of API metadata.
4.2
Tool Retriever
Figure 5 details the components of the tool retriever.
The concatenated description
and examples of each tool API are encoded into low-dimensional dense vectors using
BGE-LARGE-EN-V1.5 [25] when the server is initialized. When a query is received by
the tool retriever, BGE-LARGE-EN-V1.5 is used once again to convert the user query
into an embedding vector. Then, semantic similarity (Squared L2 norm) is calculated
between the user query embedding and the embeddings representing each tool API in the
ChromaDB collection, and the most similar tool is selected (minimum distance). Table
1 presents the performance of this retrieval method on an annotated test set containing
150 instances of user queries for APIs created for our case study.
4.3
Tool Manager
Figure 6 shows the details of the tool manager. Each tool has an API contract that defines
the tool API. Each contract details the accepted input parameters. Any required input
6


--- Page 7 ---
Figure 6: Tool Manager. It is responsible for selecting the relevant model and APS
data, extracting the input parameters for the selected tool, and executing the tool. The
tool manager also infers missing parameters from the user query and the conversation
history.
parameters must be extracted either from the user query or inferred from the conversation,
model, or APS data. If the tool manager is unable to infer a required missing parameter,
it would return the missing parameter to the conversation manager. The conversation
manager will ask clarifying questions to confirm that the retrieved tool is correct and
attempt to infer the missing parameter. The prompt for extracting input parameters is
shown in the green parameter extraction prompt in Figure 6. The blue prompt is used to
identify the model that is associated with the user query. Within a conversation, there
may be multiple models and/or APS data saved.
The name (or ID) of the model is
detected from the conversation and used to determine the model that corresponds to the
current user query. The APS data is extracted in a similar manner. After selecting the
correct model, APS data, and extracting the required input parameters, the tool manager
executes the selected tool and returns the output to the conversation manager.
5
Case Study
Production planning, a key APS application, demands daily updates by planners to ac-
commodate new information from manufacturing, warehouse, logistics, and sales, along-
side managing requests like tight customer deadlines, machine downtime, and material
sharing across plans. Addressing these requests and updating the production plan neces-
sitates thorough analysis.
Through our discussions with Huawei’s supply chain planners and observations of
their workflows, we identified their primary challenge with APS: a heavy reliance on OR
consultants to conduct analyses. The most common types of analyses required by planners
include finding reasons for customer order production delays and identifying resolutions.
To answer production planners’ needs with SMARTAPS, we asked OR consultants to
develop tools with APIs and API contracts specifically designed for production planning.
The tool categories and the number of instances for each category are presented in Table
1.
We conducted a user study by deploying SMARTAPS in a realistic production plan-
ning scenario and granting planners access to it. They reported that SMARTAPS enabled
7


--- Page 8 ---
them to query plans more efficiently and more readily identify the reasons for customer
order production delays.
Users particularly highlighted the advantages of supporting
’why-not’ and ’what-if’ analyses, which could reduce the time required for analysis from
potentially 1-2 days—due to dependence on OR consultants—to just a few hours. Over-
all, the feedback from both production planners and OR consultants was positive, and
we are collaborating with them to integrate SMARTAPS into their daily tool stack.
5.1
Limitation and Future Works
Some limitations of SMARTAPS highlight important research directions that would greatly
benefit systems built upon a tool-augmented LLM like SMARTAPS. Tool-augmented
frameworks must have APIs to call; they are unable to respond to requests not covered
by the APIs. Especially in a technical domain like OR, APIs must be created and cus-
tomized by experts. Some sophisticated code generation methods should be investigated
for their ability to automatically create these advanced APIs with complex algorithms.
Optimization solvers often take a long time when calculating the optimal solution. In
practice, due to the solve time, especially for larger operations, many support consultants
submit a job overnight to the APS that requires a new solution to be calculated. The plan
is only ready to be reviewed by the next morning. A task manager can be incorporated
into SMARTAPS to keep track and allow jobs to be run in parallel.
Finally, the conversation is currently between the system and one user. In real-world
operations, it is common to have multiple planners, each with different objectives. Multi-
user approaches should be explored to handle this scenario.
6
Conclusion
In this paper, we present a demonstration of SMARTAPS, a useful and intuitive chat
interface that helps users interact with an APS using natural language. It is built upon a
tool-augmented LLM, and we present three main modules that have been developed using
learning-based NLP methods, inspired by relevant prior works like retrieval-augmented
generation and tool-augmented LLM. We will continue iteratively improving SMARTAPS
based on feedback from real-world planners, with the goal of ensuring that it helps plan-
ners more efficiently manage their operations, thereby reducing OR consultation costs
and turnaround time.
References
[1] Daniel Andor, Luheng He, Kenton Lee, and Emily Pitler. Giving bert a calculator:
Finding operations and arguments with reading comprehension, 2019.
[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Pra-
fulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christo-
pher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and
Dario Amodei. Language models are few-shot learners, 2020.
[3] Zhenan Fan, Bissan Ghaddar, Xinglu Wang, Linzi Xing, Yong Zhang, and Zirui
Zhou. Artificial intelligence for operations research: Revolutionizing the operations
research process, 2024.
[4] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai,
Jiawei Sun, Meng Wang, and Haofen Wang.
Retrieval-augmented generation for
large language models: A survey, 2024.
8


--- Page 9 ---
[5] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Deven-
dra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume
Lample, Lucile Saulnier, L´elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock,
Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth´ee Lacroix, and William El
Sayed. Mistral 7b, 2023.
[6] YuHe Ke, Liyuan Jin, Kabilan Elangovan, Hairil Rizal Abdullah, Nan Liu, Alex
Tiong Heng Sia, Chai Rick Soh, Joshua Yi Min Tung, Jasmine Chiat Ling Ong, and
Daniel Shu Wei Ting. Development and testing of retrieval augmented generation in
large language models – a case study report, 2024.
[7] Namesh Killemsetty, Michael Johnson, and Amit Patel. Understanding housing pref-
erences of slum dwellers in india: A community-based operations research approach.
European Journal of Operational Research, 298(2):699–713, 2022.
[8] Kinaxis.
Planning one: A low-risk, low-cost entry point to advanced planning.
https://www.kinaxis.com/, 2024. Accessed: 2025-07-14.
[9] Xijun Li, Fangzhou Zhu, Hui-Ling Zhen, Weilin Luo, Meng Lu, Yimin Huang, Zhenan
Fan, Zirui Zhou, Yufei Kuang, Zhihai Wang, Zijie Geng, Yang Li, Haoyang Liu,
Zhiwu An, Muming Yang, Jianshu Li, Jie Wang, Junchi Yan, Defeng Sun, Tao
Zhong, Yong Zhang, Jia Zeng, Mingxuan Yuan, Jianye Hao, Jun Yao, and Kun Mao.
Machine learning insides optverse ai solver: Design principles and applications, 2024.
[10] Yixin Liu, Avi Singh, C. Daniel Freeman, John D. Co-Reyes, and Peter J. Liu.
Improving large language model fine-tuning for solving math problems, 2023.
[11] Yining Lu, Haoping Yu, and Daniel Khashabi. Gear: Augmenting language models
with generalizable and efficient tool resolution, 2024.
[12] Jakub L´ala, Odhran O’Donoghue, Aleksandar Shtedritski, Sam Cox, Samuel G. Ro-
driques, and Andrew D. White. Paperqa: Retrieval-augmented generative agent for
scientific research, 2023.
[13] Microsoft.
Copilot
in
dynamics
365
supply
chain
management.
https://learn.microsoft.com/en-us/microsoft-cloud/dev/copilot/
copilot-for-dynamics365, 2024. Accessed: 2025-07-14.
[14] Mahdi Mostajabdaveh, F. Sibel Salman, and Walter J. Gutjahr. A branch-and-price
algorithm for fast and equitable last-mile relief aid distribution. European Journal
of Operational Research, 324(2):522–537, 2025.
[15] Max O’Donnell and Barun Mathema. Operational research on the treatment of drug-
resistant tuberculosis: Exciting results that need to be protected. American Journal
of Respiratory and Critical Care Medicine, 203(1):11–13, Jan 2021.
[16] Oracle.
Oracle
supply
chain
planning.
https://www.oracle.com/scm/
supply-chain-planning/, 2024. Accessed: 2025-07-14.
[17] Rindra Ramamonjison, Haley Li, Timothy Yu, Shiqi He, Vishnu Rengan, Amin
Banitalebi-dehkordi, Zirui Zhou, and Yong Zhang. Augmenting operations research
with auto-formulation of optimization models from problem descriptions. In Yunyao
Li and Angeliki Lazaridou, editors, Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing: Industry Track, pages 29–62, Abu Dhabi,
UAE, December 2022. Association for Computational Linguistics.
[18] Rindranirina Ramamonjison, Timothy T. Yu, Raymond Li, Haley Li, Giuseppe
Carenini, Bissan Ghaddar, Shiqi He, Mahdi Mostajabdaveh, Amin Banitalebi-
Dehkordi, Zirui Zhou, and Yong Zhang.
Nl4opt competition: Formulating opti-
mization problems based on their natural language descriptions, 2023.
9


--- Page 10 ---
[19] Dan Reid and Nada Sanders. Operations Management. John Wiley & Sons, Inc.,
4th edition, 2009.
[20] SAP. Sap s/4hana manufacturing for planning and scheduling. https://www.sap.
com/products/scm/manufacturing-for-planning-and-scheduling.html, 2024.
Accessed: 2025-07-14.
[21] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli,
Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language
models can teach themselves to use tools, 2023.
[22] Danuphon Tippong, Sanja Petrovic, and Vahid Akbari. A review of applications of
operational research in healthcare coordination in disaster management. European
Journal of Operational Research, 301(1):1–17, 2022.
[23] Jiafu Wan, Xiaomin Li, Hong-Ning Dai, Andrew Kusiak, Miguel Mart´ınez-Garc´ıa,
and Di Li. Artificial-intelligence-driven customized manufacturing factory: Key tech-
nologies, applications, and challenges. Proceedings of the IEEE, 109(4):377–398, 2021.
[24] Segev Wasserkrug, Leonard Boussioux, Dick den Hertog, Farzaneh Mirzazadeh, Ilker
Birbil, Jannis Kurtz, and Donato Maragno. From large language models and opti-
mization to decision optimization copilot: A research manifesto, 2024.
[25] Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun
Nie. C-pack: Packed resources for general chinese embeddings, 2024.
10
