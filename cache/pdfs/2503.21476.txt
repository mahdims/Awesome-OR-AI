--- Page 1 ---
1
Robust DNN Partitioning and Resource Allocation
Under Uncertain Inference Time
Zhaojun Nan, Member, IEEE, Yunchu Han, Student Member, IEEE, Sheng Zhou, Senior Member, IEEE,
and Zhisheng Niu, Fellow, IEEE
Abstract‚ÄîIn edge intelligence systems, deep neural network
(DNN) partitioning and data ofÔ¨Çoading can provide real-time
task inference for resource-constrained mobile devices. However,
the inference time of DNNs is typically uncertain and can-
not be precisely determined in advance, presenting signiÔ¨Åcant
challenges in ensuring timely task processing within deadlines.
To address the uncertain inference time, we propose a robust
optimization scheme to minimize the total energy consumption
of mobile devices while meeting task probabilistic deadlines. The
scheme only requires the mean and variance information of the
inference time, without any prediction methods or distribution
functions. The problem is formulated as a mixed-integer nonlin-
ear programming (MINLP) that involves jointly optimizing the
DNN model partitioning and the allocation of local CPU/GPU
frequencies and uplink bandwidth. To tackle the problem, we
Ô¨Årst decompose the original problem into two subproblems:
resource allocation and DNN model partitioning. Subsequently,
the two subproblems with probability constraints are equivalently
transformed into deterministic optimization problems using the
chance-constrained programming (CCP) method. Finally, the
convex optimization technique and the penalty convex-concave
procedure (PCCP) technique are employed to obtain the optimal
solution of the resource allocation subproblem and a stationary
point of the DNN model partitioning subproblem, respectively.
The proposed algorithm leverages real-world data from popular
hardware platforms and is evaluated on widely used DNN models.
Extensive simulations show that our proposed algorithm effec-
tively addresses the inference time uncertainty with probabilistic
deadline guarantees while minimizing the energy consumption of
mobile devices.
Index Terms‚ÄîEdge intelligence, DNN partitioning, uncer-
tain inference time, chance-constrained programming, convex-
concave procedure.
I. INTRODUCTION
D
EEP neural networks (DNNs) have been extensively
applied across various innovative applications, including
speech recognition [1], object detection [2], image segmenta-
tion [3], etc. With the penetration of these applications, there
is a critical demand to deploy DNN models on mobile devices
with limited computing capacity and battery power, such
as energy-harvesting sensors, micro-robots, and unmanned
This work is supported in part by the National Natural Science Foundation
of China under Grants 62341108, in part by the China Postdoctoral Science
Foundation under Grant 2023M742011, and in part by Hitachi Ltd. (Corre-
sponding author: Sheng Zhou.)
Zhaojun
Nan,
Yunchu
Han,
Sheng
Zhou,
and
Zhisheng
Niu
are
with
the
Beijing
National
Research
Center
for
Information
Science
and
Technology,
Department
of
Electronic
Engineering,
Tsinghua
University, Beijing 100084, China (e-mail: nzj660624@mail.tsinghua.edu.cn;
hyc23@mails.tsinghua.edu.cn;
sheng.zhou@tsinghua.edu.cn;
niuzhs@tsinghua.edu.cn).
aerial vehicles, to achieve real-time task inference and intelli-
gent decision-making. However, these DNN models usually
have high computing capacity requirements. For example,
GoogleNet, ResNet101, and VGG16 require 3.0, 15.2, and
31.0 giga Ô¨Çoating point of operations (GFLOPs), respectively
[4]. On the Raspberry Pi platform, the inference time of
GoogleNet is about 0.8 seconds [5], while for tiny YOLOv2, it
is up to 1.8 seconds [6]. Due to the disparity between the high
computing capacity requirements of DNNs and the resource-
limited mobile devices, achieving fast task inference on these
mobile devices is highly challenging.
To address this challenge, edge-device collaborative infer-
ence has recently been proposed [6], [7]. The key idea of
edge-device collaborative inference is to adaptively partition
the DNN model in response to varying channel states, thereby
achieving an efÔ¨Åcient balance of the inference computing
capacity and transmission data size between mobile devices
and the edge server. This facilitates the coordination of timely
task inference between weak mobile devices and the powerful
edge server. The important objective of collaborative inference
is to determine the optimal partitioning point and allocate
communication and computation resources, ensuring that the
inference results meet task deadlines and enabling the timely
processing of subsequent tasks. However, most existing work
on collaborative inference assumes that the inference time of
a task is precisely known, overlooking the impact of inference
time uncertainty on collaborative inference [8], [9], [10], [11],
[12], [13], [14], [15].
In practical systems, the inference time of DNNs is variable
and uncertain, and it cannot be determined until the inference
task is executed [16], [17]. In [16], the authors assess the
inference time of convolutional neural networks on the SoCs,
observing signiÔ¨Åcant performance variations under inference
time outliers. In [17], the authors observe that the inference
time of various DNN models applied to autonomous driving
are uncertain, and analyze several factors that inÔ¨Çuence the
Ô¨Çuctuations in DNN inference time. Different from the object
detection tasks in [17], we verify the variations in inference
time of several DNN models for the classiÔ¨Åcation task on
the CIFAR-10 dataset using the CPU and GPU platforms,
as shown in Fig. 1. We also Ô¨Ånd that the uncertainty of
DNN inference time is affected by the model structure, I/O
speed, hardware platform, etc. Moreover, it can be observed
from Fig. 1 that the inference time of different models
on different hardware exhibits signiÔ¨Åcant randomness, which
makes its distribution function difÔ¨Åcult to obtain accurately.
Indeed, uncertain inference time brings a signiÔ¨Åcant challenge
arXiv:2503.21476v2  [cs.DC]  23 Sep 2025


--- Page 2 ---
2
AlexNet
ResNet50
InceptionV3
ResNet152
VGG19
(a) Jetson Xavier NX CPU
0
500
1000
1500
Inference time (ms)
AlexNet
ResNet50
InceptionV3
ResNet152
VGG19
(b) Jetson Xavier NX GPU
0
50
100
150
200
Inference time (ms)
Fig. 1. The variation in inference time for the classiÔ¨Åcation task on
the CIFAR-10 dataset using the CPU and GPU of the NVIDIA Jetson
Xavier NX platform, respectively.
to edge-device collaboration. It is well known that deciding
DNN model partitioning based on worst-case inference time
tends to be overly conservative, and extending task deadlines
can compromise the timeliness of the system. Therefore, it
is necessary to consider the robust DNN partitioning and
resource allocation to provide performance guarantees.
In this paper, we address the issue of uncertain inference
time in DNN model partitioning and resource allocation by
providing probabilistic guarantees on deadlines. In this way,
inference time is not strictly bound by hard deadlines; rather,
occasional violations of task deadlines are tolerated. This
approach is deemed reasonable in practical systems. For image
or video processing, occasional violations of deadlines can be
mitigated through error control techniques at the application
layer [33]. SpeciÔ¨Åcally, we allow the probability of task
execution time (consisting of local inference, uplink transmis-
sion, and edge inference delays) violating the task deadline
to remain under a predeÔ¨Åned threshold while minimizing
the total energy consumption of mobile devices. Considering
that the accurate inference time cannot be obtained and its
distribution function is difÔ¨Åcult to characterize, we design a
robust DNN partitioning, uplink bandwidth, and computing
resource allocation policy, utilizing only the mean and variance
information of the inference time. The main contributions of
this work are summarized below.
‚Ä¢ To the best of our knowledge, this is the Ô¨Årst work explic-
itly considering inference time uncertainty in optimizing
DNN partitioning. To this end, we formulate a joint
optimization problem involving the DNN model parti-
tioning and the allocation of local CPU/GPU frequencies
and uplink bandwidth under uncertain inference time,
aiming to minimize the expected energy consumption of
all mobile devices while meeting probabilistic deadline
constraints. Due to the probabilistic deadline constraints
arising from uncertain inference time and the combina-
torial nature of DNN model partitioning and resource
allocation decisions, the problem is a challenging mixed-
integer nonlinear programming (MINLP) problem.
‚Ä¢ Considering that DNN inference time cannot be precisely
determined a priori and its probabilistic distribution
is difÔ¨Åcult to estimate accurately, we characterize the
mean and variance of the inference time across different
CPU/GPU frequencies based on real-world data from
DNN models. SpeciÔ¨Åcally, the nonlinear least squares
method is used to Ô¨Åt a function that describes the rela-
tionship between the mean inference time and CPU/GPU
frequency. Then, we present an efÔ¨Åcient method for
estimating the variance and covariance of the inference
time across different CPU/GPU frequencies.
‚Ä¢ To deal with the combinatorial nature of the MINLP
problem, we Ô¨Årst propose decomposing the original prob-
lem into a resource allocation subproblem with Ô¨Åxed
partitioning decisions and a DNN model partitioning sub-
problem that optimizes the expected energy consumption
corresponding to the resource allocation problem. Then,
the two subproblems with probabilistic constraints are
equivalently transformed into deterministic optimization
problems using the mean and variance information of
inference time and the chance-constrained programming
(CCP) method.
‚Ä¢ Finally, we obtain the optimal solution to uplink band-
width and the CPU/GPU frequencies of the resource
allocation subproblem using the convex optimization
technique. By exploring the structural properties of the
DNN model partitioning subproblem, a stationary point
of the problem is obtained using the penalty convex-
concave procedure (PCCP) method. The PCCP method
has low computational complexity and can achieve the
near-optimal solution in polynomial time.
Simulations are carried out on real-world data from Nvidia
hardware platforms and are evaluated on widely used DNN
models. Through extensive simulations, we demonstrate that
the proposed robust policy exhibits faster convergence and
lower computational complexity. The simulation results show
that the probability guarantee of the task deadline can be
successfully achieved under DNN inference time uncertainty,
which means that the proposed policy is more robust. Com-
pared to the state-of-the-art approach, our proposed policy has
a signiÔ¨Åcant improvement in energy saving on mobile devices.
The remainder of this paper is organized as follows. Section
II reviews related work. Section III describes the system
model and problem formulation. Section IV derives the mean
and variance of inference time. Section V develops a robust
DNN partitioning and resource allocation algorithm. Section
VI shows the simulation results, followed by the conclusion
in Section VII.
II. RELATED WORK
In this section, we summarize the existing work on DNN
model partitioning and resource allocation, and introduce the
work related to inference time uncertainty.
A. DNN Model Partitioning
Extensive research focuses on DNN model partitioning
and resource allocation in collaboration inference. Various


--- Page 3 ---
3
works are investigated from different perspectives, such as
collaborative paradigms, DNN model structures, and inference
approaches. The cloud-end collaboration paradigm partially
shifts DNN inference from the device to the cloud [7], [18].
In [7], the neurosurgeon algorithm is proposed to Ô¨Ånd an
intermediate partitioning point in the DNN model, keeping
the front-end model on the device and ofÔ¨Çoading the back-
end model to the cloud. Leveraging a similar principle, [18]
proposes a distributed partitioning strategy that divides the
DNN model into the cloud, the edge server, and the end
devices. To reduce the latency of cloud inference, the edge-
end collaboration paradigm is studied [5], [15]. Edgent [5]
utilizes mobile edge computing (MEC) for DNN collaborative
inference through device-edge synergy by adaptive partitioning
and right-sizing DNNs to reduce latency. Based on [5], [15]
proposes a learning-based method that optimizes DNN par-
titioning, early exit point selection, and computing resource
allocation. [45] proposes an integrated sensing, computing,
and communication (ISCC) architecture that jointly optimizes
AI model partitioning, as well as integrated sensing and
communication, to deliver low-latency intelligent services at
the network edge.
Different DNNs may have various structures, so a suitable
model structure is needed for effective partitioning. Therefore,
[6] and [14] use the directed acyclic graph (DAG) to model
the relationship between layers in DNN, and transform the
DNN partitioning problem into the solution of the minimum
cut problem in graph theory. To reduce the complexity of
DAG modeling, [10] and [19] divide the DAG into multiple
blocks, thereby simplifying the DNN model into a block-based
chain structure. The above studies generally adopt a sequence
inference approach, where local inference is before the par-
titioning point and edge inference is behind the partitioning
point. Unlike sequence inference, a few works investigate
parallel and batch inference approaches. Taking advantage
of the parallelism of the input sequence, [20] partitions the
transformer model according to location to accelerate the
inference speed. [12] considers appropriate partitioning point
selection, aggregates multiple inference tasks into one batch,
and processes them concurrently on the edge server. However,
most of these studies assume that the inference time of DNNs
is deterministic and known in advance.
B. Inference Time Uncertainty
A few works that focus on the inference time uncertainty
[16], [17], [21], [22]. In [16] and [17], the authors discover
earlier the uncertainty in inference time and analyze the causes
of inference time uncertainty. [16] evaluates the inference time
performance of convolutional neural networks on multiple
generations of iPhone SoC chips, observing signiÔ¨Åcant per-
formance variations through numerous outliers. The analysis
shows that the inference time, particularly on the A11 chip,
follows an approximately Gaussian distribution. [17] observes
that the inference time of various DNN models applied to
autonomous driving is uncertain, and the inÔ¨Çuence on the
inference time Ô¨Çuctuation is analyzed from six aspects: data,
I/O, model, runtime, hardware, and end-to-end perception

	


	


	







	

	
	





	

	
	
	
	
	
	
	


	

 !!

 !!

"
Fig. 2. An example of the considered DNN model partitioning under
inference time uncertainty in edge intelligence systems.
system. Uncertainty in inference time brings a signiÔ¨Åcant
challenge to time-critical tasks. To address this challenge, [21]
designs a kernel-based prediction method to estimate DNN
inference time on different devices, addressing the issue of
not being able to obtain inference time a priori. [22] develops
a method to estimate end-to-end inference time by training
machine learning models to predict the time of each neural
architecture component with limited proÔ¨Åling data and across
different machine learning frameworks.
However, inference time exhibits signiÔ¨Åcant randomness
across different DNN models and hardware platforms, and
the prediction methods proposed by [21] and [22] have not
satisÔ¨Åed the requirements of high precision. The above studies
do not involve the impact of computing resources on inference
time and uncertainty, nor do they consider DNN partitioning
decisions under inference time uncertainty. In this paper, our
goal is to jointly optimize DNN model partitioning and the
allocation of computing and communication resources to min-
imize energy consumption on mobile devices while satisfying
probabilistic task deadlines. To the best of our knowledge, this
issue has not been explored in the context of DNN partitioning.
III. SYSTEM MODEL AND PROBLEM FORMULATION
As illustrated in Fig. 2, we consider a multi-device edge in-
telligence system consisting of N mobile devices, represented
by the set N ‚âú{1, . . ., N}, and one edge node integrated
with an MEC server, where the mobile devices and the edge
node only have one single antenna. The Frequency Division
Multiple Access (FDMA) system is considered, where the
channel interference between mobile devices can be negligible.
We consider that each mobile device possesses a DNN model
(e.g., AlexNet [23], ResNet [24], or VGG [25]) that can handle
a certain number of inference tasks (e.g., image recognition).
Meanwhile, the DNN model of each mobile device has an
identical backup stored on the edge node.
In DNNs, the size of the output data (i.e., feature data)
from some intermediate layers or blocks is typically smaller
than the size of the input data (i.e., raw data). As the number
of layers or blocks increases, the required computing capacity
(i.e., GFLOPs) gradually rises. As shown in Fig. 3, the input


--- Page 4 ---
4
TABLE I
SUMMARY OF MAIN SYMBOLS
Symbol
Description
Symbol
Description
n
Index of the nth mobile device
Œ∫n
Energy efÔ¨Åciency coefÔ¨Åcient
N
Set of N mobile devices
B
Total communication bandwidth
M
Set of M partitioning points
bn
Bandwidth allocated to mobile device n
xn,m
Partitioning decision of mobile device n
fmin
Minimum CPU/GPU frequency of the mobile device
tloc
n,m
Local inference time of mobile device n
fmax
Maximum CPU/GPU frequency for the mobile device
eloc
n,m
Local energy consumption of mobile device n
fn
CPU/GPU frequency allocated to mobile device n
toÔ¨Ä
n,m
OfÔ¨Çoading time of mobile device n
dn,m
Output data size by the mth block of the DNN model
eoÔ¨Ä
n,m
OfÔ¨Çoading energy consumption of mobile device n
pn
Transmission power of mobile device n
tvm
n,m
Edge inference time of mobile device n
hn
Channel gain of mobile device n
Dn
Deadline of the inference task
N0
Noise power spectral density
Input
Block1 Block2 Block3 Block4 Block5 Block6 Block7 Block8
(a) Data size and GFLOPs of AlexNet
0
0.2
0.4
0.6
0.8
Data size (MB)
0
0.2
0.4
0.6
GFLOPs
Data size
GFLOPs
Input Block1 Block2 Block3 Block4 Block5 Block6 Block7 Block8 Block9
(b) Data size and GFLOPs of ResNet152
0
1
2
3
4
Data size (MB)
0
2
4
6
GFLOPs
Data size
GFLOPs
Fig. 3. The data size and required GFLOPs of each block in AlexNet
and ResNet152.
data size of AlexNet and ResNet152 are both 0.574 MB. The
feature data size of AlexNet‚Äôs block 2 and ResNet152‚Äôs block
5 are 0.18 MB and 0.19 MB, representing 69% and 67% reduc-
tions compared to the input data size. Correspondingly, after
block 2, AlexNet requires GFLOPs that account for 90% of
the total GFLOPs, whereas ResNet152 needs 81% of its total
GFLOPs after block 5. Therefore, inference tasks generated by
resource-limited mobile devices can be ofÔ¨Çoaded to the MEC
server with a powerful computing capacity for processing.
More speciÔ¨Åcally, we can execute a part of the DNN inference
task locally on the mobile device, ofÔ¨Çoad a small amount of
intermediate feature data to the MEC server, and then execute
the remaining DNN inference task. The partitioning of DNN
models needs to consider the tradeoff between computation
and communication. From a more practical perspective, our
work addresses the policy of DNN model partitioning and
resource allocation when the inference time is not precisely
known in advance. For ease of reference, the main symbols
are summarized as Table I.
A. DNN Model Partitioning
Different DNNs exhibit a range of structures. For example,
AlexNet and VGG are organized as single chains [23], [25],









	



	
		

	



	









	


Fig. 4. An example of the block-based DNN modeling and its
partitioning points.
while ResNet features two asymmetric branches [24]. Typi-
cally, the structure of DNNs is modeled as DAGs [14], [26].
However, this DAG-based modeling can be quite complex. For
simplicity, we use the block-based modeling approach [10],
[12]. This method involves dividing the DAG into multiple
blocks, effectively transforming it into a serial chain structure.
As shown in Fig. 4, each block we construct consists of multi-
ple layers, including convolutional layers (Conv), pooling lay-
ers (Pool), batch normalization layers (BN), activation layers
(such as ReLU), etc. Denote M as the number of blocks in the
DNN model. Then, the set of partitioning points is represented
as M ‚âú{0, 1, . . ., M}. Let xn,m ‚àà{0, 1}, n ‚ààN, m ‚ààM
be the partitioning decision, and there is only one partitioning
point for each mobile device, i.e., P
m‚ààM xn,m = 1, ‚àÄn ‚ààN.
SpeciÔ¨Åcally, xn,m = 1 indicates that mobile device n executes
partitioning at the mth point, and xn,m = 0 otherwise. For
instance, xn,0 = 1 means that mobile device n only executes
edge inference, xn,M = 1 means that mobile device n only
executes local inference, and xn,m = 1 means that the Ô¨Årst m
blocks execute local inference, and the remaining (M ‚àím)
blocks execute edge inference.
B. Inference Time and Energy Consumption
As shown in Fig. 5, the inference time of each block of
AlexNet and ResNet152 on different hardware platforms is
tested. The inference time of each block exhibits signiÔ¨Åcant
uncertainty and randomness, making it challenging to predict
and understand the distribution of inference time precisely.
However, it is pleasing that on the higher-computing platform
(i.e., GeForce RTX 4080), the inference time and variation
for each block of AlexNet and ResNet152 are signiÔ¨Åcantly


--- Page 5 ---
5
Block1
Block2
Block3
Block4
Block5
Block6
Block7
Block8
(a) Inference time on two platforms for AlexNet
0
40
80
120
Inference time (ms)
0
0.2
0.4
0.6
0.8
Inference time (ms)
Jetson Xavier NX CPU
GeForce RTX4080
Block1
Block2
Block3
Block4
Block5
Block6
Block7
Block8
Block9
(b) Inference time on two platforms for ResNet152
0
10
20
30
40
Inference time (ms)
0
1
2
3
Inference time (ms)
Jetson Xavier NX GPU
GeForce RTX4080
Fig. 5. The variations in inference time on different platforms of each
block for AlexNet and ResNet152.
reduced compared to the lower-computing platform (i.e., Jet-
son Xavier NX CPU/GPU). Therefore, dynamic voltage and
frequency scaling (DVFS) can be employed to optimize local
computing resource allocation on mobile devices, while task
ofÔ¨Çoading can be used to transfer computing to the MEC
server, thereby reducing both inference time and its variation.
We assume that the partitioning point is m ‚ààM, and then
the partitioning point set M is divided into two mutually
exclusive sets M0 ‚âú{0, 1, . . ., m} and M1 ‚âúM\M0 ‚âú
{m + 1, . . . , M}. Let uloc
n,k denote the local inference time of
the mobile device n in the kth block. Then, the local inference
time of mobile device n can be written as
tloc
n,m =
X
k‚ààM0
uloc
n,k, ‚àÄn ‚ààN, m ‚ààM,
(1)
where tloc
n,0 = uloc
n,0 = 0. The dynamic power consumption of
the COMS circuit is denoted as Œ±cV 2f, where Œ± is the activity
factor, c is the load capacitance, V is the supply voltage,
and f is the CPU/GPU clock frequency [27]. Moreover, V
is approximately linear to the frequency when the CPU/GPU
operates in the non-low frequency range, i.e., V = kf [27],
[28]. Thus, the corresponding energy consumption of mobile
device n to execute local inference is
eloc
n,m = Œ∫nf 3
ntloc
n,m, ‚àÄn ‚ààN, m ‚ààM,
(2)
where Œ∫n = Œ±ncnk2
n is an energy efÔ¨Åciency coefÔ¨Åcient that
depends on the chip architecture.
Let bn denote the uplink bandwidth allocated by the edge
node to mobile device n for edge inference. The uplink
bandwidth allocated to each mobile device is constrained
by total bandwidth resource B, i.e., P
n‚ààN bn ‚â§B. The
spectral efÔ¨Åciency of wireless uplink between the edge node
and mobile device n is Œ∑oÔ¨Ä
n
= log2 (1 + pnhn/bnN0), where
pn is the transmission power, hn is the channel gain, and N0 is
the noise power spectral density. The ofÔ¨Çoading time of mobile
device n to transmit data to the edge node can be given as
toff
n,m = dn,m
bnŒ∑oÔ¨Ä
n
, ‚àÄn ‚ààN, m ‚ààM,
(3)
where dn,m is the output data size by the mth block of the
DNN model of mobile device n. Based on the partitioning
decision xn,m, dn,m can represent the size of the raw data,
feature data, or result data. For instance, dn,0 denotes the
size of the raw data, while dn,M represents the size of the
result data. The corresponding ofÔ¨Çoading energy consumption
of mobile device n is
eoff
n,m = pndn,m
bnŒ∑oÔ¨Ä
n
, ‚àÄn ‚ààN, m ‚ààM.
(4)
The MEC server can generate a virtual machine (VM)
for each mobile device that executes edge inference. Each
VM is conÔ¨Ågured with the corresponding DNN model to its
associated mobile device and performs the ofÔ¨Çoading task in
parallel. For simplicity, we assume that the total computing
resources on the edge server are equally allocated to each VM,
and each VM is dedicated to serving its corresponding mobile
device [46], [47]. The delay introduced by VM conÔ¨Åguration
is usually small (e.g., ‚àº10 ms), so its impact on the overall
delay is negligible and has been disregarded.1 Let uvm
n,k denote
the edge inference time of mobile device n in the kth block.
The edge inference time of mobile device n can be expressed
as
tvm
n,m =
X
k‚ààM1
uvm
n,k, ‚àÄn ‚ààN, m ‚ààM,
(5)
where tvm
n,M = 0. The time spent downloading the task com-
putation results from the edge node (e.g., 5G Base Station) to
the device is negligible, due to the strong transmission power
and high bandwidth of the edge node, as well as the relatively
small output data size (compared to the raw or feature data
size). For example, in object recognition applications, the
output typically consists of vehicles, pedestrians, etc., which
requires only a few bytes and is much smaller than the
corresponding input raw data or feature data, the latter of
which may be several megabytes. In addition, since the MEC
server is powered by the grid, the energy consumption of edge
inference and result downloading is not considered [29], [30].
C. Problem Formulation
From the above analysis, the energy consumption of mobile
device n is
En =
X
m‚ààM
xn,m
 eloc
n,m + eoÔ¨Ä
n,m

, ‚àÄn ‚ààN.
(6)
Meanwhile, the inference time of mobile device n is
Tn =
X
m‚ààM
xn,m
 tloc
n,m + toÔ¨Ä
n,m + tvm
n,m

, ‚àÄn ‚ààN.
(7)
Due to the uncertainty of inference time, the actual inference
time Tn of mobile device n is a random variable. Conse-
quently, we would like to provide a probabilistic guarantee
1For example, advanced VM conÔ¨Åguration techniques can make its latency
less than 10 ms, and techniques such as hot start and lightweight conÔ¨Åguration
can further reduce the latency of VM conÔ¨Åguration [48], [49].


--- Page 6 ---
6
for the inference task with a hard deadline constraint under
uncertainty of inference time, which is given as follows
P {Tn ‚â§Dn} ‚â•1 ‚àíŒµn, ‚àÄn ‚ààN,
(8)
where Dn is the deadline of the inference task, and Œµn is the
violation probability that mobile device n can tolerate, which
is a small positive constant also called risk level. In robust
optimization, constraint (8) is generally called the chance
constraint [34], [35].
The objective is to jointly optimize DNN partitioning de-
cision x ‚âú{xn,m}n‚ààN,m‚ààM, uplink bandwidth allocation
b ‚âú{bn}n‚ààN , and local computing resource allocation
f ‚âú{fn}n‚ààN to minimize the expected energy consumption
of all mobile devices while satisfying the chance constraints.
The optimization problem is formulated as
min
x,b,f E
" X
n‚ààN
En
#
(9a)
s.t. P {Tn ‚â§Dn} ‚â•1 ‚àíŒµn, ‚àÄn ‚ààN,
(9b)
X
m‚ààM
xn,m = 1, ‚àÄn ‚ààN,
(9c)
X
n‚ààN
X
m‚ààM
xn,mbn ‚â§B,
(9d)
xn,m ‚àà{0, 1}, ‚àÄn ‚ààN, m ‚ààM,
(9e)
bn ‚â•0, ‚àÄn ‚ààN,
(9f)
fmin ‚â§fn ‚â§fmax, ‚àÄn ‚ààN,
(9g)
where (9b) corresponds to the guarantee of chance constraints
for hard deadlines under uncertain inference time, (9c) and
(9e) correspond to constraints on DNN partitioning decisions,
(9d) represents constraints on uplink bandwidth allocation, and
(9f) and (9g) indicate uplink bandwidth and local computing
resources that can be allocated to mobile devices, respectively.
Although problem (9) is easy to understand, solving it in
practice is quite challenging. First and foremost, constraint
(9b) indicates the need to provide chance constraints for
the inference of each task with a hard deadline, which is
difÔ¨Åcult to handle. Second, similar to [12], [13] and [36],
the corresponding DNN partitioning and resource allocation
remains a mixed-integer non-linear programming (MINLP)
problem even given the deterministic inference time, which
is generally NP-hard. To address above challenges, in the ab-
sence of precise inference time and its complete distributional
knowledge, we develop a robust DNN model partitioning
and resource allocation policy that relies solely on the mean
and variance information of the inference time. The speciÔ¨Åc
solutions are presented in Section IV and Section V.
Remark 1: It is worth noting that the problem (9) we propose
can be simpliÔ¨Åed to the case of the previous work by setting
the risk level of each mobile device to zero, and the mean and
variance of the inference time for each block to true and zero,
respectively. In this regard, the problem of uncertain inference
time explored in this paper is both more meaningful and more
challenging.
0.5
1
1.5
2
(a) CPU frequency (GHz)
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Inference time of the device (s)
AlexNet
Partitioning point m = 2
Partitioning point m = 1
0
0.2
0.4
0.6
0.8
(b) GPU frequency (GHz)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Inference time of the device (s)
ResNet152
Partitioning point m = 3
Partitioning point m = 2
Partitioning point m = 1
Fig. 6. The mean inference time at different partitioning points on
Jeston Xavier NX CPU/GPU. Discrete points represent the sample
mean, while the continuous lines represent the Ô¨Åtted functions.
IV. MEAN AND VARIANCE OF INFERENCE TIME WITH
FREQUENCY SCALING
In this section, we Ô¨Årst provide a Ô¨Åtting function of relation-
ship between mean inference time and CPU/GPU frequency
using nonlinear least squares method. Then, we present an
efÔ¨Åcient method to estimate the variance and covariance of
inference time across different CPU/GPU frequencies.
A. Mean Inference Time
The DVFS technology can be used to optimize inference
time and energy consumption. Therefore, it is essential to give
a model that accurately characterizes the relationship between
CPU/GPU frequency and inference time. Most existing work
models the inference time as a function of the workload and
the CPU/GPU frequency, typically expressed as their ratio.
The speciÔ¨Åc model is deÔ¨Åned as t =
w
gf , where w (in
GFLOPs) is the workload of the task, f (in GHz) is the
CPU/GPU frequency, and g (in FLOPs/cycle) is the workload
it can process per cycle [31], [32]. However, we Ô¨Ånd that the
parameter g in the above model varies across different DNNs
and within different blocks of the same DNN. As illustrated in
Fig. 3 and Fig. 5, the total inference time of different DNNs
is not necessarily proportional to the total GFLOP under a
Ô¨Åxed CPU/GPU frequency. Similarly, the inference time of
each block within the same DNN does not necessarily scale
proportionally with its respective GFLOP. For example, the
inference time of ResNet152 is 6-fold that of AlexNet, while
the required GFLOPs are 16-fold higher than those of AlexNet.
For AlexNet, the inference time of block 8 is higher than that
of other blocks, yet the required data size and GFLOPs are
quite small.
Therefore, we utilize real-world data to model the functional
relationship between inference time and CPU/GPU frequency.
The inference time of widely used DNNs (i.e., AlexNet
and ResNet152) is tested on multiple devices (e.g., Jeston
Xavier NX CPU and GPU) by frequency scaling. SpeciÔ¨Åcally,
AlexNet and ResNet152 are partitioned into 2 and 3 blocks,
respectively. The sets of partitioning points are deÔ¨Åned as
m ‚ààM ‚âú{0, 1, 2} for AlexNet and m ‚ààM ‚âú{0, 1, 2, 3}
for ResNet152, where m = 0 indicates that the inference is
executed on the VM, resulting in the inference time of the


--- Page 7 ---
7
0.5
1
1.5
2
(a) CPU frequency (GHz)
0
20
40
60
80
100
120
140
Variance of inference time (ms2)
AlexNet
Partitioning point m = 2
Partitioning point m = 1
0
0.2
0.4
0.6
0.8
(b) GPU frequency (GHz)
0
10
20
30
40
Variance of inference time (ms2)
ResNet152
Partitioning point m = 3
Partitioning point m = 2
Partitioning point m = 1
Fig. 7. The variance of inference time at different partitioning points
on Jeston Xavier NX CPU/GPU.
device being 0.2 We use nonlinear least square method to Ô¨Åt
the measured data above. Fig. 6 illustrates the Ô¨Åtting curve and
coefÔ¨Åcient of AlexNet and ResNet152 for different partitioning
points on CPU and GPU. For AlexNet, the squared 2-norm of
the residual at m = 1 and m = 2 is 2.0e-4 s2 and 9.7e-4
s2, respectively. For ResNet152, the squared 2-norm of the
residual at m = 1, m = 2, and m = 3 is 5.7e-4 s2, 8.0e-4 s2,
and 2.9e-3 s2, respectively.
According to the above results, the mean inference time
when mobile device n selects partitioning point m is modeled
as follows:
¬Øtloc
n,m = wn,m
gn,mfn
, ‚àÄn ‚ààN, m ‚ààM,
(10)
where wn,m is the GFLOPs required for local inference, fn is
the local CPU/GPU frequency of mobile device n, and gn,m is
the FLOPs that can be processed per cycle, which is decided
by the partitioning point, the DNN model, and the CPU/GPU
hardware.
B. Variance and Covariance of Inference Time
Based on the measurement of mean inference time, the
variance of inference time for AlexNet and ResNet152 at
different CPU and GPU frequencies is calculated, as shown
in Fig. 7. It can be observed that the variance of AlexNet is
higher at low CPU frequencies, while the maximum variance
of ResNet152 occurs at around 0.7 GHz on the GPU. The
results indicate that the variance of inference time is not
a monotonic function of CPU/GPU frequency. In addition,
compared to inference on the CPU, the variance of inference
time on the GPU is relatively lower. However, the variance
of inference time exhibits random and irregular Ô¨Çuctuations in
response to variations in CPU/GPU frequency. Therefore, it is
difÔ¨Åcult to Ô¨Åt the relationship between variance and CPU/GPU
frequency as a function, in contrast to the modeling of the
mean inference time.
To solve the above problem, we use the mean value in
the CPU/GPU frequency scaling range as the variance of the
2Due to space limitations, only the cases with 2 and 3 blocks are presented
here. However, in the experiments where the blocks are partitioned into 8 or
9, each block demonstrated a similar curve, as depicted in Fig. 6.
inference time. The variance of inference time when mobile
device n selects partitioning point m is obtain by
vloc
n,m =
1
|F|
X
‚àÄfn‚ààF
vloc
n,m (fn) , ‚àÄn ‚ààN, m ‚ààM,
(11)
where vloc
n,m(¬∑) = E
h tloc
n,m(¬∑) ‚àí¬Øtloc
n,m(¬∑)
2i
and F is the set of
measurable frequency points of the CPU/GPU. This approxi-
mation may introduce some errors; however, simulation results
show the error is acceptable. The experiments and analysis are
discussed in Section VI. Note that in this work, we assume
the CPU/GPU frequencies of mobile devices can be scaled,
whereas the CPU/GPU frequencies of VMs remain constant.
Therefore, t
vm
n,m and vvm
n,m can be obtained through simple
online measurement.
During collaborative inference, covariance information be-
tween the mobile device and the VM is also required. Thus,
we designate Jetson Xavier and Nano as the mobile device and
RTX4080 as the VM, and calculate the covariance at different
partitioning points. The experimental results show that the
covariance curve closely matches the variance curve in Fig.
7. It is because the computing capacity of the VM is higher
than mobile devices, leading to lower inference time and
Ô¨Çuctuations. Therefore, similar to variance, the covariance of
inference time at different partitioning points is approximated
by
wn,m,m‚Ä≤ =
1
|F|
X
‚àÄfn‚ààF
wn,m,m‚Ä≤ (fn) , ‚àÄn ‚ààN, m, m‚Ä≤ ‚ààM,
(12)
where wn,m,m‚Ä≤(¬∑) = E [tn,m(¬∑)tn,m‚Ä≤(¬∑)] ‚àí¬Øtn,m(¬∑)¬Øtn,m‚Ä≤(¬∑).
V. ROBUST DNN PARTITIONING AND RESOURCE
ALLOCATION
To tackle the challenges posed by the chance constraints and
combinatorial complexity of problem (9), we Ô¨Årst decompose
problem (9) into two subproblems: resource allocation and
DNN model partitioning. Subsequently, the two subproblems
with probabilistic constraints are equivalently transformed into
deterministic optimization problems using the CCP method.
Finally, convex optimization technique and PCCP technique
are applied to obtain the optimal solution of the resource
allocation subproblem and a stationary point of the DNN
model partitioning subproblem respectively.
A. Problem Decomposition
By leveraging the structure of the objective function and
constraints in problem (9), we Ô¨Ånd that it can be decom-
posed into two subproblems with separated objectives and
constraints. We use the Tammer decomposition method [37]
to transform the high-complexity original problem into two
lower-complexity subproblems and solve these subproblems
alternately. First, the resource allocation subproblem is written
as
min
b,f E(b, f | x)
s.t.(9b), (9d), (9f), (9g).
(13)


--- Page 8 ---
8
	

	




	

	

	






	
 	!
	



"#

	
	




	

	

"#

	
	

 !"
#$%&$
''&$
"#

	(

$

Fig. 8. The general schematic of the optimization problem and corresponding solution.
where E(b, f | x) is the optimal value function corresponding
to the resource allocation subproblem. Then, the DNN model
partitioning subproblem is expressed as
min
x E(x | b, f)
s.t.(9b), (9c), (9d), (9e).
(14)
where E(x | b, f) is the optimal value function corresponding
to the DNN model partitioning subproblem. Note that the
decomposition from the original problem (9) to problem
(13) and problem (14) does not change the optimality of
the solution [37]. In the following, we will give solutions
of the resource allocation subproblem and the DNN model
partitioning subproblem. The general schematic of the solution
is shown in Fig. 8.
B. Resource Allocation Subproblem
We deÔ¨Åne the set G that contains the partitioning points for
all mobile devices as G ‚âú{mn ‚ààM | xn,mn = 1, ‚àÄn ‚ààN}.
For
a
given
DNN
model
partitioning
decision
x
‚âú
{xn,mn}n‚ààN,mn‚ààG, the expected energy consumption of mo-
bile devices is
E
" X
n‚ààN
En
#
=
X
n‚ààN

Œ∫n
wn,mn
gn,mn
f 2
n + pndn,mn
bnŒ∑oÔ¨Ä
n

.
(15)
Then, problem (13) is rewritten as
min
b,f
X
n‚ààN

Œ∫n
wn,mn
gn,mn
f 2
n + pndn,mn
bnŒ∑oÔ¨Ä
n

(16a)
s.t. P {tn,mn ‚â§Dn} ‚â•1 ‚àíŒµn, ‚àÄn ‚ààN, mn ‚ààG,
(16b)
X
n‚ààN
bn ‚â§B,
(16c)
bn ‚â•0, ‚àÄn ‚ààN,
(16d)
fmin ‚â§fn ‚â§fmax, ‚àÄn ‚ààN,
(16e)
where tn,mn ‚âútloc
n,mn + toÔ¨Ä
n,mn + tvm
n,mn is the total inference
time of mobile device n.
Due to the lack of the distribution of inference time, a dif-
Ô¨Åcult step is to reformulate the intractable chance constraints
in (16b) into the deterministic constraints. To address this, we
introduce a novel CCP technique [38], which does not intro-
duce any relaxation in the optimization space when the chance
constraint is transformed into a deterministic constraint. It
allows that the mean and covariance of random variables can
be measured without any assumptions. The details are given
as follows:
Theorem 1: Given random variables Œª ‚âú[Œª1, Œª2, . . . , Œªn]T
with known mean Œª ‚âú

Œª1, Œª2, . . . , Œªn
T and covariance
matrix C ‚âúE

(Œª ‚àíŒª)(Œª ‚àíŒª)T
, a deterministic vector
a ‚âú[a1, a2, ¬∑ ¬∑ ¬∑ , an]T, a constant z, and the risk level «´, we
can have the standard form of the Exact Conic Reformulation
(ECR) as follows
PŒª‚àº(Œª,C)

aTŒª ‚â§z
	
‚â•1 ‚àí«´,
(17)
if and only if
aTŒª +
r
1 ‚àí«´
«´
‚àö
aTCa ‚â§z,
(18)
where Œª ‚àº(Œª, C) indicates that the mean and covariance of
the random variable Œª are Œª and C, respectively.
Proof. The proof of Theorem 1 is given in [38].
Inspired by the CCP technique, we formulate the constraint
(16b) into the standard form of the ECR, as follows:
P¬µn‚àº(¬µn,Vn)

cT
n¬µn ‚â§Dn
	
‚â•1 ‚àíŒµn, ‚àÄn ‚ààN,
(19)
where cT
n ‚âú[1, 1, 1] and ¬µn ‚âú

tloc
n,mn, toÔ¨Ä
n,mn, tvm
n,mn
T for all
mn ‚ààG. The mean vector of ¬µn is
¬µn ‚âú
h
t
loc
n,mn, t
oÔ¨Ä
n,mn, t
vm
n,mn
iT
, ‚àÄn ‚ààN, mn ‚ààG,
(20)
where t
loc
n,mn can be obtained by (10), t
oÔ¨Ä
n,mn = toÔ¨Ä
n,mn =
dn,mn/bnŒ∑oÔ¨Ä
n,mn is the real ofÔ¨Çoading time.3, t
vm
n,mn is the
measured mean of tvm
n,mn. Accordingly, the covariance matrix
is constructed as
Vn ‚âú
Ô£Æ
Ô£ØÔ£∞
vloc
n,mn
0
0
0
0
0
0
0
vvm
n,mn
Ô£π
Ô£∫Ô£ª, ‚àÄn ‚ààN, mn ‚ààG,
(21)
3This work does not consider channel state uncertainty and assumes that
channel state information can be accurately obtained. However, our method
can be extended to scenarios that jointly consider inference time and channel
state uncertainty.


--- Page 9 ---
9
where vloc
n,mn is given by (11) and vvm
n,mn is the measured
variances of vvm
n,mn.
Based on Theorem 1, the chance constraints in (16b) with
respect to bandwidth allocation b and computing resource
allocation f are equivalently transformed to the following
deterministic constraints:
 wn,mn
gn,mnfn
+
dn,mn
bnŒ∑oÔ¨Ä
n,mn
+ t
vm
n,mn

+
œÉn
q vloc
n,mn + vvm
n,mn

‚â§Dn, ‚àÄn ‚ààN, mn ‚ààG,
(22)
where œÉn =
p
(1 ‚àíŒµn) /Œµn. After removing all random
variables and considering ¬µn and Vn as known constants, we
derive an equivalent deterministic problem of problem (16)
with the given DNN partitioning decision as follows:
min
b,f
X
n‚ààN

Œ∫n
wn,mn
gn,mn
f 2
n + pndn,mn
bnŒ∑oÔ¨Ä
n

(23a)
s.t. (16c), (16d), (16e), (22).
(23b)
Note that problem (23) is convex, so the optimal resource
allocation can be solved via an interior point (IPT) algorithm.
The computational complexity of solving problem (23) using
an IPT algorithm is O
 N 3)

, and the number of iterations of
the IPT algorithm is O(
‚àö
N log(1/Œæ)), where Œæ is the conver-
gence accuracy. Therefore, the total computational complexity
is O
 N 3.5 log(1/Œæ)

[39].
C. DNN Model Partitioning Subproblem
In the previous subsection, we obtained the optimal solution
to the bandwidth allocation b and computing resource alloca-
tion f under a given x. Next, we use the solutions b and f
obtained from resource allocation subproblem (16) to optimize
x. The DNN model partitioning subproblem of problem (14)
can be rewritten as
min
x
X
n‚ààN
X
m‚ààM
xn,m

Œ∫n
wn,m
gn,m
f 2
n + pndn,m
bnŒ∑oÔ¨Ä
n

(24a)
s.t. P
( X
m‚ààM
xn,mtn,m ‚â§Dn
)
‚â•1 ‚àíŒµn, ‚àÄn ‚ààN, (24b)
X
m‚ààM
xn,m = 1, ‚àÄn ‚ààN,
(24c)
X
n‚ààN
X
m‚ààM
xn,mbn ‚â§B,
(24d)
xn,m ‚àà{0, 1}, ‚àÄn ‚ààN, m ‚ààM,
(24e)
where tn,m ‚âútloc
n,m + toÔ¨Ä
n,m + tvm
n,m. Note that in addition to
the intractable chance constraints in (24b), problem (24) is
non-convex due to the binary variable x. We Ô¨Årst transform
the chance constraints into equivalent deterministic constraints.
The constraint (24b) can be formulated as
PœÑ n‚àº(œÑ n,Wn)

xT
nœÑ n ‚â§Dn
	
‚â•1 ‚àíŒµn, ‚àÄn ‚ààN,
(25)
where xT
n ‚âú[xn,0, xn,1, . . . , xn,M] is the partitioning decision
vector, and œÑ n ‚âú[tn,0, tn,1, . . . , tn,M]T is the inference time
vector. The mean vector of œÑ n is
œÑ n ‚âú

tn,0, tn,1, . . . , tn,M
T , ‚àÄn ‚ààN,
(26)
where tn,m ‚âút
loc
n,m +t
oÔ¨Ä
n,m +t
vm
n,m for all m ‚ààM. t
loc
n,m can be
given by (10), t
oÔ¨Ä
n,m = toÔ¨Ä
n,m = dn,m/bnŒ∑oÔ¨Ä
n,m, and t
vm
n,m is the
measured mean of tvm
n,m. Consequently, the covariance matrix
is deÔ¨Åned as
Wn =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
wn,0,0
wn,0,1
¬∑ ¬∑ ¬∑
wn,0,M
wn,1,0
wn,1,1
¬∑ ¬∑ ¬∑
wn,1,M
...
...
...
...
wn,M,0
wn,M,1
¬∑ ¬∑ ¬∑
wn,M,M
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
, ‚àÄn ‚ààN,
(27)
where wn,m,m‚Ä≤ is obtained by (12).
Based on Theorem 1, the chance constraints in (24b)
with respect to DNN model partitioning x are equivalently
transformed to the following deterministic constraints:
X
m‚ààM
xn,mtn,m + œÉn
s X
m‚ààM
wn,m,mx2n,m ‚â§Dn, ‚àÄn ‚ààN.
(28)
where wn,m,m is the diagonal element of the Wn matrix.
Then, we replace constraint (24b) in problem (24) with the
constraint (28), and reformulate problem (24) as an equivalent
deterministic problem as follows:
min
x
X
n‚ààN
X
m‚ààM
xn,m

Œ∫n
wn,m
gn,m
f 2
n + pndn,m
bnŒ∑oÔ¨Ä
n

(29a)
s.t. (24c), (24d), (24e), (28).
(29b)
To handle the combinatorial nature of the binary variable x
of problem (29), we transform problem (29) into an equivalent
difference-of-convex (DC) problem and obtain a stationary
point of problem (29) using the PCCP technique. In what
follows, we Ô¨Årst replace the binary constraints in (24e) with
the following constraints:
xn,m ‚àà[0, 1], ‚àÄn ‚ààN, m ‚ààM,
(30)
xn,m (1 ‚àíxn,m) ‚â§0, ‚àÄn ‚ààN, m ‚ààM,
(31)
Then, we introduce auxiliary variables y ‚âú{yn}n‚ààN :
yn =
s X
m‚ààM
wn,m,mx2n,m, ‚àÄn ‚ààN,
(32)
where yn > 0 for all n ‚ààN. Therefore, problem (29) can be
equivalently transformed into the problem as follows:
min
x,y
X
n‚ààN
X
m‚ààM
xn,m

Œ∫n
wn,m
gn,m
f 2
n + pndn,m
bnŒ∑oÔ¨Ä
n

(33a)
s.t. (24c), (24d), (30),
(33b)
X
m‚ààM
xn,m¬Øtn,m + œÉnyn ‚â§Dn, ‚àÄn ‚ààN,
(33c)
X
m‚ààM
wn,m,mx2
n,m ‚àíy2
n ‚â§0, ‚àÄn ‚ààN,
(33d)
y2
n ‚àí
X
m‚ààM
wn,m,mx2
n,m ‚â§0, ‚àÄn ‚ààN,
(33e)
xn,m ‚àíx2
n,m ‚â§0, ‚àÄn ‚ààN, m ‚ààM,
(33f)
yn > 0, ‚àÄn ‚ààN,
(33g)
where objective function (33a), constraints (33b), (33c) and


--- Page 10 ---
10
(33g) are convex. However, there are concave functions in
constraints (33d), (33e) and (33f), for which problem (33) is
identiÔ¨Åed as a DC problem that can be solved using the PCCP
technique [40].
We Ô¨Årst relax problem (33) by adding relaxation variables
to the DC constraints and penalizing the sum of violations to
avoid the infeasibility of each iteration. The penalty function
can be given as
P =
X
n‚ààN
Œ±n +
X
n‚ààN
Œ≤n +
X
n‚ààN
X
m‚ààM
Œ≥n,m,
(34)
where Œ±
‚âú
{Œ±n}n‚ààN , Œ≤
‚âú
{Œ≤n}n‚ààN , and Œ≥
‚âú
{Œ≥n,m}n‚ààN,m‚ààM are slack variables added for constraints
(33d), (33e), and (33f), respectively. Accordingly, the penalty
DC problem can be obtained as
min
x,y
Œ±,Œ≤,Œ≥
X
n‚ààN
X
m‚ààM
xn,m

Œ∫n
wn,m
gn,m
f 2
n + pndn,m
bnŒ∑oÔ¨Ä
n

+ œÅP
(35a)
s.t. (24c), (24d), (30), (33c), (33g)
(35b)
X
m‚ààM
wn,m,mx2
n,m ‚àíy2
n ‚â§Œ±n, ‚àÄn ‚ààN,
(35c)
y2
n ‚àí
X
m‚ààM
wn,m,mx2
n,m ‚â§Œ≤n, ‚àÄn ‚ààN,
(35d)
xn,m ‚àíx2
n,m ‚â§Œ≥n,m, ‚àÄn ‚ààN, m ‚ààM,
(35e)
Œ±n ‚â•0, Œ≤n ‚â•0, Œ≥n,m ‚â•0, ‚àÄn ‚ààN, m ‚ààM,
(35f)
where œÅ > 0 is a penalty parameter. Then, the concave terms of
the constraints (35c), (35d), and (35e) are linearized to obtain
convex constraints for a minimization problem and to solve a
sequence of convex problems successively. SpeciÔ¨Åcally, at ith
iteration, update

x(i), y(i)	
by solving the following approx-
imate problem, which is parameterized by

x(i‚àí1), y(i‚àí1)	
obtained at (i ‚àí1)th iteration.
min
x,y
Œ±,Œ≤,Œ≥
X
n‚ààN
X
m‚ààM
xn,m

Œ∫n
wn,m
gn,m
f 2
n + pndn,m
bnŒ∑oÔ¨Ä
n

+ œÅ(i‚àí1)P
(36a)
s.t. (24c), (24d), (30), (33c), (33g), (35f),
(36b)
X
m‚ààM
wn,m,mx2
n,m ‚àíy(i‚àí1)
n

2yn ‚àíy(i‚àí1)
n

‚â§Œ±n, ‚àÄn ‚ààN,
(36c)
y2
n ‚àí
X
m‚ààM
wn,m,mx(i‚àí1)
n,m

2xn,m ‚àíx(i‚àí1)
n,m

‚â§Œ≤n, ‚àÄn ‚ààN,
(36d)
xn,m

1 ‚àí2x(i‚àí1)
n,m

+

x(i‚àí1)
n,m
2
‚â§Œ≥n,m, ‚àÄn ‚ààN, m ‚ààM,
(36e)
where œÅ(i‚àí1) is the penalty parameter at the (i ‚àí1)th iter-
ation. Problem (36) is a convex problem that can be solved
efÔ¨Åciently by an IPT algorithm. The pseudo-code for solving
problem (36) is presented in Algorithm 1. The computational
complexity of solving problem (36) using an IPT algorithm
is O
 N 3M 3
, and the number of iterations of the IPT
algorithm is O(
‚àö
NM log(1/Œæ)), where Œæ is the convergence
Algorithm 1 PCCP Algorithm for Solving Problem (24)
1: Initialize: Set the initial penalty œÅ(0) > 0, the maximum
penalty œÅmax > 0, the weight ŒΩ > 1, and the convergence
criteria as Œ∏err > 0; Choose an arbitrary initial point

x(0), y(0)	
of problem (29).
2: Set i = 1.
3: repeat
4:
Obtain

x(i), y(i)	
by solving the problem (36) using
an IPT algorithm.
5:
Set œÅ(i) = min

ŒΩœÅ(i‚àí1), œÅmax
	
.
6:
Set i = i + 1.
7: until
x(i) ‚àíx(i‚àí1) < Œ∏err with i ‚â•1.
8: Set x = x(i).
Algorithm 2 Overall Algorithm for Solving Problem (9)
1: Initialize: Number of mobile devices N, partitioning point
M, communication bandwidth B, task deadline Dn, risk
level Œµn, and the convergence criteria Œ∏err > 0;
2: Set k = 0.
3: Choose any feasible solution

x(0), b(0), f (0)	
to problem
(9).
4: repeat
5:
Resource allocation subproblem (16):
6:
With Ô¨Åxed

x(k)	
, problem (16) is equivalently trans-
formed to problem (23) by the CCP method.
7:
Obtain

b(k+1), f (k+1)	
by solving the problem (23)
using an IPT method.
8:
DNN model partitioning subproblem (24):
9:
With Ô¨Åxed

b(k+1), f (k+1)	
, problem (24) is equiva-
lently transformed to problem (29) by the CCP method.
10:
Obtain

x(k+1)	
using Algorithm 1.
11:
Set k = k + 1.
12: until The objective value of problem (9) meets the conver-
gence criteria Œ∏err.
accuracy. Therefore, the total computational complexity of
Algorithm 1 is O
 (NM)3.5 log(1/Œæ)

[39]. Note that the
sequence solution

x(i)	‚àû
i=1 to problem (36) can converge
to a stationary point of problem (33), as shown in [40]. Since
problem (33) and problem (24) are equivalent, Algorithm 1
can also converge to a stationary point of problem (24).
In summary, the pseudo-code for solving the original prob-
lem (9) is provided in Algorithm 2, which is achieved by
iteratively solving the resource allocation subproblem and the
DNN model partitioning subproblem.
VI. SIMULATION RESULTS
In this section, we Ô¨Årst give the values of simulation
parameters, then show the convergence and complexity of the
proposed algorithms, and Ô¨Ånally evaluate the performance of
the proposed algorithms under different parameter settings.
A. Simulation Setup
We simulate a 400 m √ó 400 m square area with the edge
node located at the center of the area. The mobile devices are


--- Page 11 ---
11
distributed uniformly and randomly across the coverage area
of the edge node. The uplink wireless channel gain between
mobile device n and the edge node is modeled as hn = 38 +
30 √ó log10 rn [41], where hn and rn are the path-loss (in dB)
and distance between device n and the edge node (in meters),
respectively. Additionally, the total uplink wireless bandwidth
is set to B = 10 MHz and B = 30 MHz for different DNN
models, the transmit power pn of mobile device n is set to 1
W, and the noise power density is N0 = ‚àí174 dBm/Hz [12],
[15].
TABLE II
CONFIGURATIONS OF DNNS AND HARDWARE
DNN model
Mobile device
VM
AlexNet
Jetson Xavier NX CPU
f ‚àà[0.1, 1.2]GHz
GeForce
RTX 4080
ResNet152
Jetson Xavier NX GPU
f ‚àà[0.2, 0.8]GHz
GeForce
RTX 4080
ViT-B/32
Jetson Nano GPU
f ‚àà[0.1, 0.8]GHz
GeForce
RTX 4080
Three widely-used DNNs, AlexNet [23], ResNet152 [24]
and ViT-B/32 [50], are considered. The three DNNs are
fully deployed on mobile devices and the MEC server. The
task of the mobile device is image recognition, which is
extracted from the object recognition dataset CIFAR-10 [42].
The processing unit of mobile devices adopts Jetson Xavier
NX CPU and GPU [43], as well as Jetson Nano GPU [51].
We assume that AlexNet is deployed on the Jetson Xavier NX
CPU, ResNet152 is deployed on Jetson Xavier NX GPU, and
ViT-B/32 is deployed on Jetson Nano GPU. The VM assigned
to the mobile device uses the GeForce RTX 4080. SpeciÔ¨Åc
conÔ¨Ågurations are shown in Table II.
The energy efÔ¨Åciency coefÔ¨Åcient Œ∫n of Jetson Xavier NX
CPU and GPU is evaluated using the power testing tool
Tegrastats of NVIDIA [44]. SpeciÔ¨Åcally, Jetson Xavier NX is
Ô¨Årst set to a Ô¨Åxed power consumption mode. Then, the power
consumption of the CPU and GPU at different frequencies is
measured, and Ô¨Ånally, Œ∫n is obtained based on the measured
data. By estimation, the average Œ∫n of the Jetson Xavier
NX CPU, Jetson Xavier NX GPU and Jetson Nano GPU are
0.8√ó10‚àí27W/(cycle/sec)3, 2.8√ó10‚àí27W/(cycle/sec)3, and
3.2 √ó 10‚àí27W/(cycle/sec)3, respectively.
AlexNet, ResNet152 and ViT-B/32 are divided into 8, 9 and
6 blocks, corresponding to 9, 10 and 7 partitioning points,
respectively. The feature data size of each block can be
calculated based on its output data shape. The mean inference
time for each block is obtained through 500 experiments, and
then the variance and covariance can also be calculated based
on the mean and measured data. The speciÔ¨Åc parameters are
shown in Table III, IV and V. Unless otherwise speciÔ¨Åed, the
above parameters are used by default.
To evaluate the performance of Algorithm 1, we consider
the following two policies as benchmark:
1) Random policy: the DNN partitioning point is randomly
selected from the set of points where the feature data is
smaller than the raw data.
2) Optimal policy: the DNN partitioning point is obtained
using the exhaustive search method, which can Ô¨Ånd the
6
12
18
24
30
5
6
7
8
9
10
11
12
13
14
15
ViT-B/32
AlexNet
ResNet152
Fig. 9. The average number of iterations of Algorithm 1 under
different numbers of mobile devices in the three models with ViT-
B/32, AlexNet, and ResNet152.
optimal partitioning point, but its computational complex-
ity is exponential.
To evaluate the performance of Algorithm 2, we consider
the following two methods as benchmark:
1) Worst-case optimization: the upper bound of tloc
n,m and
tvm
n,m obtained by the experiment is taken as the inference
time, and the task deadline is not allowed to be violated.
2) Mean-value optimization: the inference time only adopts
the mean value, without considering the uncertainty in
inference time. This is a primitive method that bypasses
the uncertainty, without any probabilistic guarantees.
B. Convergence and Complexity
First, we show the convergence of the proposed algorithms.
Fig. 9 illustrates the average number of iterations of Algorithm
1 versus the number of mobile devices. Although the number
of iterations of Algorithm 1 cannot be analytically charac-
terized, we can see from Fig. 9 that even when the number
of devices N = 30, Algorithm 1 can terminate after a few
iterations. Moreover, the average number of iterations for ViT-
B/32, AlexNet, and ResNet152 are not signiÔ¨Åcantly different.
In addition, the average number of iterations of Algorithm 1
increases slightly as the number of mobile devices increases
signiÔ¨Åcantly. This indicates that Algorithm 1 based on PCCP
has better scalability.
Fig. 10(a) and Fig. 10(b) illustrate the convergence trajec-
tories of Algorithm 2 from different initial points in AlexNet,
ResNet152, and ViT-B/32 models, respectively. We select three
different points as the initial points from all the partitioning
points of AlexNet, ResNet152, and ViT-B/32, respectively. For
example, the initial points for AlexNet are 3, 7, and 9; for
ResNet152, they are 1, 8, and 9; and for ViT-B/32, they are 3,
4, and 5. From Fig. 10, it can be observed that the advantage of
using Algorithm 2 is its ability to converge quickly in the early
stages of iteration. In addition, Algorithm 2 almost converges
to the same objective function value for different initial points.
Then, we show the computational complexity of the pro-
posed algorithms. Fig. 11 illustrates the runtime of Algorithm
2 on AlexNet, ResNet152, and ViT-B/32. The simulation
experiments are implemented using MATLAB and conducted
on a laptop computer with an Intel Core i7-8700 3.2 GHz
CPU and 16 GB RAM. In Fig. 11, the runtime for AlexNet


--- Page 12 ---
12
TABLE III
THE PARAMETERS OF VIT-B/32 ON JETSON NANO GPU.
Parameter
point 0
point 1
point 2
point 3
point 4
point 5
point 6
dn,m (MB)
0.574
0.146
0.146
0.146
0.146
0.146
0.001
wn,m (GFLOPs)
‚Äì
3.0954
3.8114
5.2435
7.3916
8.1077
8.8253
gn,m (FLOPs/cycle)
‚Äì
171.967
174.837
175.369
181.168
178.191
135.983
vloc
n,m (ms)2
‚Äì
11.059
18.931
33.337
65.814
75.867
153.434
TABLE IV
THE PARAMETERS OF ALEXNET ON JETSON XAVIER NX CPU.
Parameter
point 0
point 1
point 2
point 3
point 4
point 5
point 6
point 7
point 8
dn,m (MB)
0.574
0.74
0.18
0.53
0.12
0.25
0.17
0.04
0.001
wn,m (GFLOPs)
‚Äì
0.1407
0.1411
0.5891
0.5894
0.8137
1.3122
1.3123
1.4214
gn,m (FLOPs/cycle)
‚Äì
6.8994
6.3283
13.6064
13.1861
14.6624
16.4237
16.1219
7.1037
vloc
n,m (ms)2
‚Äì
37.341
43.084
59.616
63.942
74.801
95.073
98.876
105.886
TABLE V
THE PARAMETERS OF RESNET152 ON JETSON XAVIER NX GPU.
Parameter
point 0
point 1
point 2
point 3
point 4
point 5
point 6
point 7
point 8
point 9
dn,m (MB)
0.574
3.06
0.77
1.53
0.38
0.19
0.19
0.19
0.1
0.001
wn,m (GFLOPs)
‚Äì
0.2392
1.4864
3.6585
5.3099
9.9984
13.9389
17.8794
21.9228
23.1064
gn,m (FLOPs/cycle)
‚Äì
315.4525
309.6695
323.7640
329.8090
325.6815
324.1615
322.7340
318.6457
307.6753
vloc
n,m (ms)2
‚Äì
0.097
1.310
5.677
13.934
14.076
15.881
23.408
32.256
32.727
1
3
5
7
9
(a) Number of iterations
0.4
0.8
1.2
1.6
Energy Consumption (J)
Algorithm 2 with ALexNet (initail point=3)
Algorithm 2 with ALexNet (initail point=7)
Algorithm 2 with ALexNet (initail point=9)
1
3
5
7
9
(b) Number of iterations
0.3
0.5
0.7
0.9
Algorithm 2 with ResNet152 (initail point=1)
Algorithm 2 with ResNet152 (initail point=8)
Algorithm 2 with ResNet152 (initail point=9)
1
3
5
7
9
(c) Number of iterations
0.3
0.5
0.7
0.9
Algorithm 2 with ViT (initail point=3)
Algorithm 2 with ViT (initail point=4)
Algorithm 2 with ViT (initail point=5)
Fig. 10. The convergence trajectories of Algorithm 2 for AlexNet
(Dn = 220 ms), ResNet152 (Dn = 160 ms), and ViT-B/32 (Dn =
130 ms).
with N = 6 is normalized to 1. From Fig. 11, it can be seen
that the runtime of the proposed algorithm increases linearly
with the number of mobile devices, despite the exponentially
growing search space. Since the ResNet152 model has 10
partitioning points, its average runtime is slightly higher than
that of the AlexNet model, which has 9 partitioning points.
Despite having only 7 partitioning points, the ViT-B/32 model
actually exhibits a higher runtime. It is because, as shown in
Fig. 10, the ViT-B/32 model requires more iterations than both
AlexNet and ResNet152. Combining this with the computa-
tional complexity analysis in Section IV, we observed that the
complexity of the proposed Algorithm 1 and Algorithm 2 are
polynomial time with respect to the number of partitioning
points and mobile devices.
In addition, we found that although the theoretical compu-
tational complexity of the IPT method is proportional to M
and N to the 3.5th power, i.e., O
 (NM)3.5 log(1/Œæ)

, Fig. 11
shows a much lower practical increase in runtime. SpeciÔ¨Åcally,
6
12
18
24
30
36
0
1
2
3
4
5
6
7
8
9
AlexNet
ResNet152
ViT-B/32
Fig. 11. The average runtime of Algorithm 2 under different number
of mobile devices.
when the number of devices increases from N
= 6 to
N = 36, the runtimes of AlexNet, ResNet152, and ViT-B/32
increase by less than 5 times. This observed runtime growth
is much lower than what would be expected based solely on
the theoretical computational complexity. It may be related
to factors such as the optimization methods employed by the
solver and the performance of the hardware platform. For
larger-scale networks, such as M ‚â´10, and N ‚â´36, we can
consider combining the proposed algorithm with distributed
optimization to reduce the computational complexity of the
algorithm.
C. Performance Evaluation
In this subsection, we investigate the impact of variance
approximation, different device numbers, risk levels, and task
deadlines on the total energy consumption under the AlexNet,
ViT-B/32, and ResNet152 models. In addition, we further
analyze the violation probability of the task deadline under
varying risk levels.
1) Impact of variance approximation: The approximation
performance of (11) and (12) in Section IV. B is evaluated
below. Fig. 12 shows the energy consumption and violation
probability of AlexNet, ResNet152, and ViT-B/32 using the


--- Page 13 ---
13
0.03
0.06
0.09
0.12
0.15
0
0.03
0.06
0.09
0.12
0.15
Maximum variance
Mean variance
Risk level
(a) Violation probability with AlexNet
0.03
0.06
0.09
0.12
0.15
0.45
0.5
0.55
0.6
0.65
0.7
Maximum variance
Mean variance
(b) Energy consumption with AlexNet
0.03
0.06
0.09
0.12
0.15
0
0.03
0.06
0.09
0.12
0.15
Maximum variance
Mean variance
Risk level
(c) Violation probability with ViT-B/32
0.03
0.06
0.09
0.12
0.15
0.45
0.46
0.47
0.48
0.49
0.5
0.51
0.52
Maximum variance
Mean variance
(d) Energy consumption with ViT-B/32
0.03
0.06
0.09
0.12
0.15
0
0.03
0.06
0.09
0.12
0.15
0.18
0.21
Maximum variance
Mean variance
Risk level
(e) Violation probability with ResNet152
0.03
0.06
0.09
0.12
0.15
0.48
0.5
0.52
0.54
0.56
0.58
0.6
0.62
Maximum variance
Mean variance
(f) Energy consumption with ResNet152
Fig. 12. Impact of variance approximation on AlexNet (N = 12,
B = 10 MHz, Dn = 180 ms), ViT-B/32 (N = 12, B = 20 MHz,
Dn = 100 ms), and ResNet152 (N = 12, B = 30 MHz, Dn = 115
ms).
maximum and mean variance. The mean variance method is
an approximation of (11) and (12). The maximum variance
method is approximated as follows:
(
vloc
n,m = max‚àÄfn‚ààF

vloc
n,m (fn)
	
,
wn,m,m‚Ä≤ = max‚àÄfn‚ààF {wn,m,m‚Ä≤ (fn)} .
where ‚àÄn ‚ààN, ‚àÄm, m‚Ä≤ ‚ààM.
In Figs. 12 (a), (c), and (e), we Ô¨Årst show the violation
probability as Œµn increases. It can be observed that the
maximum variance method has a lower violation probability
than the mean variance method, as the maximum variance
method is more conservative. For AlexNet and ResNet152,
even when Œµn increases to 0.15, the violation probability of
the mean variance method remains lower than the risk level.
However, for ViT-B/32, when Œµn exceeds 0.13, the violation
probability of the mean variance method is slightly higher
than the risk level. Then, in Figs. 12 (b), (d), and (f), we
illustrate the changes in energy consumption as Œµn increases.
It can be observed that the mean variance method results
in lower energy consumption compared to the maximum
variance method. For AlexNet, ResNet, and ViT-B/32, the
mean variance method can reduce energy consumption by
up to 17%, 6.8%, and 4%, respectively, compared to the
maximum variance method when Œµn = 0.03.
3
4
5
6
7
8
0
0.2
0.4
0.6
0.8
1
1.2
Random policy
Algorithm 1
Optimal policy
(a) AlexNet
3
4
5
6
7
8
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Random policy
Algorithm 1
Optimal policy
(b) ResNet152
3
4
5
6
7
8
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Random policy
Algorithm 1
Optimal policy
(c) ViT-B/32
Fig. 13. Performance of Algorithm 1 for AlexNet (Dn = 200 ms,
B = 5 MHz), ResNet152 (Dn = 150 ms, B = 15 MHz), and ViT-
B/32 (Dn = 130 ms, B = 10 MHz).
2) Performance of Algorithm 1: Fig. 13 evaluates the
performance of Algorithm 1 under different DNN models.
Firstly, we can observe that the total energy consumption
increases with the number of mobile devices. Compared to
the random policy, the total energy consumption of Algorithm
1 is much lower. Moreover, as the number of devices increases,
the energy consumption of Algorithm 1 increases at a slower
rate than the random policy. Secondly, the performance of the
proposed Algorithm 1 is very close to the optimal policy. The
computational complexity of the optimal policy is O
 M N
,
which is exponential, while the proposed PCCP-based Al-
gorithm 1 can Ô¨Ånd a stationary point of the DNN model
partitioning subproblem, and its computational complexity is
polynomial.
3) Impact of risk levels: Fig. 14(a), Fig. 15(a), and Fig.
16(a) show the total energy consumption at different risk
levels. Currently, there is no effective solution that guaran-
tees the deadline for DNN partitioning under inference time
uncertainty. For comparison, we compare the worst-case and
mean optimization methods with our proposed Algorithm 2.
As shown in Fig. 14(a), the total energy consumption of
Algorithm 2 is always lower than that of the worst-case
optimization. Even when the risk level Œµ = 0.03, total energy
consumption can be reduced by nearly 44.8%. When the risk
level increases to 0.09, the total energy consumption of Algo-
rithm 2 saves 53.1% compared to the worst-case optimization.
As the risk level increases from 0.03 to 0.09, we observe
that the total energy consumption monotonically decreases,
which is as expected with (22) and (28) that we derived.
From (22), it can be observed that under a given partitioning
decision, œÉn decreases as Œµn increases, which means that
the variance term of the uncertainty in inference time (i.e.,
the second term on the left side of (22)) becomes smaller.
Mobile devices can save energy consumption by reducing
CPU/GPU frequency. Similarly, it can be seen from (28) that
under given communication and computing resources, mobile


--- Page 14 ---
14
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
Mean-value optimization
Algorithm 2
Worst-case optimization
(a)
160
180
200
220
240
260
280
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
Mean-value optimization
Algorithm 2
Worst-case optimization
(b)
0.03
0.06
0.09
0.12
0.15
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
Worst-case optimization
Algorithm 2
Mean-value optimization
Risk level
(c)
Fig. 14. The performance of the proposed policy on AlexNet (N = 12 and B = 10 MHz). (a) Energy consumption under different risk levels
with Dn = 180 ms. (b) Energy consumption under different deadlines with Œµn = 0.03. (c) Deadline violation probability under different
risk levels with Dn = 180 ms.
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.46
0.49
0.52
0.55
0.58
0.61
Mean-value optimization
Algorithm 2
Worst-case optimization
(a)
105
115
125
135
145
155
165
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
Mean-value optimization
Algorithm 2
worst-case optimization
(b)
0.03
0.05
0.07
0.09
0.11
0
0.1
0.2
0.3
0.4
0.5
Worst-case optimization
Algorithm 2
Mean-value optimization
Risk level
(c)
Fig. 15. The performance of the proposed policy on ResNet152 (N = 12 and B = 30 MHz). (a) Energy consumption under different
risk levels with Dn = 115 ms. (b) Energy consumption under different deadlines with Œµn = 0.04. (c) Deadline violation probability under
different risk levels with Dn = 115 ms.
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.45
0.46
0.47
0.48
0.49
0.5
Mean-value optimization
Algorithm 2
Worst-case optimization
(a)
90
100
110
120
130
140
150
0.4
0.45
0.5
0.55
Mean-value optimization
Algorithm 2
Worst-case optimization
(b)
0.03
0.06
0.09
0.12
0.15
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Worst-case optimization
Algorithm 2
Mean-value optimization
Risk level
(c)
Fig. 16. The performance of the proposed policy on ViT-B/32 (N = 12 and B = 20 MHz). (a) Energy consumption under different risk
levels with Dn = 100 ms. (b) Energy consumption under different deadlines with Œµn = 0.05. (c) Deadline violation probability under
different risk levels with Dn = 100 ms.
devices can save energy consumption by selecting appropriate
DNN partitioning points.
Fig. 15(a) and Fig. 16(a) show that the energy consumption
of ResNet152 and ViT-B/32 also gradually decreases as Œµn
increases. At Œµn = 0.06, compared to the worst-case optimiza-
tion, ResNet152 and ViT-B/32 can save energy consumption
by nearly 9.3% and 5.8%, respectively. As shown in Figs.
14(c), 15(c), and 16(c), similar to the worst-case optimization,
the violation probability of AlexNet and ResNet152 remains
close to 0 even when Œµn = 0.06, while the violation probability
of ViT-B/32 is only 3.5%. Moreover, we observe that AlexNet
on CPU exhibits a higher proportion of energy saving than
ResNet152 and ViT-B/32 on GPU. This is because (1) GPUs
are more energy-efÔ¨Åcient during DNN inference than CPUs,
leading to lower relative energy savings when switching to
our method, and (2) as discussed in Section IV, the variance
in inference time for ResNet152 and ViT-B/32 is relatively
small, which limits the potential for our method to further
reduce energy consumption. These results demonstrate that our
proposed algorithms are particularly effective for DNNs with


--- Page 15 ---
15
considerable variations in inference time.
Additionally, it can be seen from Figs. 14(a), 15(a), and
16(a) that the energy consumption of mean-value optimization
is lower than that of Algorithm 2, but mean-value optimization
lacks probabilistic guarantees. It will be discussed in detail in
the following.
4) Impact of task deadlines: Figs. 14(b), 15(b), and 16(b)
show total energy consumption at various task deadlines for
a given risk level. It can be observed that for AlexNet,
ResNet152, and ViT-B/32, the total energy consumption de-
creases monotonically as task deadlines increase. This is
because, as the task deadline increases, mobile devices have
more opportunities to select blocks with high computing power
requirements and large inference time Ô¨Çuctuations for DNN
partitioning and ofÔ¨Çoading these blocks to the MEC server
for execution, thereby reducing local inference energy con-
sumption. Additionally, the energy consumption of Algorithm
2 is lower than that of the worst-case policy at various task
deadlines. For AlexNet, the energy consumption of Algorithm
2 decreases by 54.6% when the deadline Dn varies from 160
ms to 280 ms. For ResNet152, the energy consumption of
Algorithm 2 decreases by 37.6% when the deadline Dn varies
from 105 ms to 165 ms. For ViT-B/32, the energy consumption
of Algorithm 2 decreases by 17.8% when the deadline Dn
varies from 90 ms to 150 ms.
5) Deadline violation probability: Leveraging real-world
data from Nvidia hardware platforms, we analyze the deadline
violation probability of Algorithm 2 under various risk level
settings. As illustrated in Fig. 14(c), 15(c) and 16(c), we
present the deadline violation probabilities at different risk
levels achieved by Algorithm 2 and the other 2 benchmarks.
As expected, the violation probabilities of the worst-case
optimization for inference on AlexNet, ResNet152, and ViT-
B/32 are all 0. It is because worst-case optimization is the
most conservative method, but it also has the highest energy
consumption, as shown in Figs. 14(b), 15(b), and 16(b).
As for the mean-value optimization, it leads to more than
40.5%, 49.5%, and 32.7% violation probabilities for AlexNet,
ResNet152, and ViT-B/32, respectively. This is due to the fact
that the mean-value optimization approach fails to account
for uncertainty in inference time. Although it achieves lower
energy consumption compared to Algorithm 2, it is a primitive
method that bypasses the uncertainty, without any probabilistic
guarantees.
Then, we observe that the violation probability of Algorithm
2 is always lower than the risk level, which afÔ¨Årms the desired
probabilistic guarantees and demonstrates the robustness of
Algorithm 2 in handling uncertain DNN inference time. The
gap between the risk level and the violation probability can be
attributed to the fact that the actual inference time of DNNs
does not invariably result in the maximum violation probabil-
ity. This observation is consistent with our design in Section
IV-A, where we approximate the variance of DNN inference
time using the mean value in the CPU/GPU frequency scaling
range. Although this approximation introduces some errors,
it further reduces energy consumption compared with the
maximum variance approximation while showing high robust-
ness. It can be seen that as the risk level gradually increases,
0
50
100
150
200
(b) Number of experiments
0
500
1000
1500
Inference time (ms)
AlexNet inference with workload
0
50
100
150
200
(a) Number of experiments
140
160
180
200
Inference time (ms)
AlexNet inference w/o workload
Fig. 17. The impact of dynamic workload on inference time and its
variation. (a) AlexNet executes local inference without any workload.
(b) AlexNet executes local inference under a dynamic workload.
0.02
0.04
0.06
0.08
0.1
0
0.02
0.04
0.06
0.08
0.1
0.69
0.7
0.71
0.72
0.73
0.74
0.75
Violation probability
Risk level
Energy consumption
Fig. 18. Deadline violation probability and energy consumption under
different risk levels in a dynamic workload with Dn = 350 ms,
B = 10 MHz, and N = 12.
although the violation probability rises, it is still far below
the risk level. As shown in Figs. 14(a) and 15(a), Algorithm
2 compared with worst-case optimization can achieve nearly
50.4% and 9.4% energy savings for AlexNet and ResNet152,
respectively, when the actual violation probability is less than
0.5% (i.e., Œµn = 0.06). Furthermore, Fig. 16(a) shows that
Algorithm 2 achieves about 6.4% energy savings at an actual
violation probability of 0.5% (i.e., Œµn = 0.03).
6) Performance under dynamic workload: Fig. 17(a) and
Fig. 17(b) show the changes in latency and variance of local
inference executed by AlexNet before and after the application
of a workload. In Fig. 17(a), only AlexNet inference is
executed on Jetson Xavier NX CPU, with no other workload.
In Fig. 17(b), while the AlexNet inference task is being
executed, a trajectory planning task is also added to the CPU
for parallel processing. It can be found that the mean inference
time and the variation in inference time of AlexNet increase
signiÔ¨Åcantly when a workload is added.
Fig. 18 shows the performance of the proposed policy
under the dynamic workload. It can be observed that as Œµn
increases, energy consumption gradually decreases. Moreover,
even when the inference time Ô¨Çuctuates signiÔ¨Åcantly, the actual
deadline violation probability is lower than the risk level.
This is because, through reasonable model partitioning, the
system ofÔ¨Çoads DNN blocks that are heavily affected by the
workload to the MEC server, thereby reducing the impact of
high workload on the inference task.


--- Page 16 ---
16
VII. CONCLUSION
In this paper, we investigated the problem of edge-device
collaborative inference under uncertain inference time. Our ex-
periments demonstrate that executing DNN inference tasks on
high-performance GPUs can signiÔ¨Åcantly enhance inference
speed and reduce variations in inference time. This motivates
us to develop an effective scheme for DNN model partitioning
and resource allocation to achieve a balance among commu-
nication costs, computational requirements, and variations in
inference time within edge intelligence systems. Therefore, we
formulate the problem as an optimization problem that mini-
mizes the total energy consumption of mobile devices while
meeting task probabilistic deadlines. To solve this problem,
we employ chance-constrained programming (CCP), which
permits occasional violations of the target capacity threshold
with a low probability, thereby reformulating the probabilistic
constraint problem as a deterministic optimization problem.
Then, the optimal solution of local CPU/GPU frequencies and
uplink bandwidth allocation and a stationary point of DNN
partitioning decisions are obtained using convex optimization
and penalty convex-concave procedure (PCCP) techniques,
respectively. We evaluate our proposed algorithm with real-
world data and widely used DNN models. Extensive simula-
tions demonstrate that, compared to worst-case optimization,
our proposed algorithm achieves approximately 50.4%, 9.4%,
and 6.4% energy savings for AlexNet, ResNet152, and ViT-
B/32, respectively, while maintaining an actual violation prob-
ability of not exceeding 0.5%.
Finally, we conclude the paper with several promising
directions for future research. First, edge intelligence systems
involve high-speed mobile scenarios, such as connected vehi-
cles and drones. These mobile devices introduce uncertainties
due to their dynamic nature. Consequently, our method can
be extended to high-speed mobile scenarios and subject to
joint design and optimization. Second, in multi-user edge in-
telligence systems, edge servers may experience computational
resource competition when generating virtual machines (VMs)
for each device, thereby introducing additional uncertainty-
induced delays. Therefore, joint optimization and allocation
of device-edge resources can further enhance system perfor-
mance. Third, the proposed algorithm can be further extended
to cross-continuous inference tasks, such as video streaming.
To achieve this, time-related information needs to be incorpo-
rated into the system model, and the impact of autocorrelation
on inference time uncertainty and the probabilistic constraints
in the CCP model should be analyzed.
REFERENCES
[1] D. Amodei et al., ‚ÄúDeep speech 2: End-to-end speech recognition in
english and mandarin,‚Äù in Proc. Int. Conf. Mach. Learn. (ICML), Jun.
2016, pp. 173-182.
[2] M. Tan, R. Pang, and Q. V. Le, ‚ÄúEfÔ¨ÅcientDet: Scalable and efÔ¨Åcient
object detection,‚Äù in Proc. IEEE Conf. Comp. Vis. Patt. Recogn. (CVPR),
Jun. 2020, pp. 10778-10787.
[3] L. -C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,
‚ÄúDeepLab: Semantic image segmentation with deep convolutional nets,
atrous convolution, and fully connected CRFs,‚Äù IEEE Trans. Pattern
Anal. Mach. Intell., vol. 40, no. 4, pp. 834-848, Apr. 2018.
[4] R. Desislavov, F. Mart¬¥ƒ±nez-Plumed, and J. Hernandez-Orallo, ‚ÄúTrends
in AI inference energy consumption: Beyond the performance-vs-
parameter laws of deep learning,‚Äù Sustain. Comput.: Inform. Syst., vol.
38, p. 100857, Apr. 2023.
[5] E. Li, L. Zeng, Z. Zhou, and X. Chen, ‚ÄúEdge AI: On-demand acceler-
ating deep neural network inference via edge computing,‚Äù IEEE Trans.
Wireless Commun., vol. 19, no. 1, pp. 447-457, Jan. 2020.
[6] C. Hu, W. Bao, D. Wang, and F. Liu, ‚ÄúDynamic adaptive DNN surgery
for inference acceleration on the edge,‚Äù in Proc. IEEE Int. Conf.
Commun. (INFOCOM), Apr. 2019, pp. 1423-1431.
[7] Y. Kang, et al., ‚ÄúNeurosurgeon: Collaborative intelligence between the
cloud and mobile edge,‚Äù in Proc. 22nd Int. Conf. Archit. Support
Program. Lang. Oper. Syst. (ASPLOS), Apr. 2017, pp. 615-629.
[8] X. Tang, X. Chen, L. Zeng, S. Yu, and L. Chen, ‚ÄúJoint multiuser DNN
partitioning and computational resource allocation for collaborative edge
intelligence,‚Äù IEEE Internet Things J., vol. 8, no. 12, pp. 9511-9522, Jun.
2021.
[9] L. Zeng, X. Chen, Z. Zhou, L. Yang, and J. Zhang, ‚ÄúCoEdge: Co-
operative DNN inference with adaptive workload partitioning over
heterogeneous edge devices,‚Äù IEEE/ACM Trans. Netw., vol. 29, no. 2,
pp. 595-608, Apr. 2021.
[10] S. Zhang, S. Zhang, Z. Qian, J. Wu, Y. Jin, and S. Lu, ‚ÄúDeepSlicing:
Collaborative and adaptive CNN inference with low latency,‚Äù IEEE
Trans. Parallel Distrib. Syst., vol. 32, no. 9, pp. 2175-2187, Sep. 2021.
[11] T. Mohammed, C. Joe-Wong, R. Babbar, and M. D. Francesco, ‚ÄúDis-
tributed inference acceleration with adaptive DNN partitioning and
ofÔ¨Çoading,‚Äù in Proc. IEEE Int. Conf. Commun. (INFOCOM), Jul. 2020,
pp. 854-863.
[12] W. Shi, S. Zhou, Z. Niu, M. Jiang, and L. Geng, ‚ÄúMultiuser co-
inference with batch processing capable edge server,‚Äù IEEE Trans.
Wireless Commun., vol. 22, no. 1, pp. 286-300, Jan. 2023.
[13] Y. Su, W. Fan, L. Gao, L. Qiao, Y. Liu, and F. Wu, ‚ÄúJoint DNN partition
and resource allocation optimization for energy-constrained hierarchical
edge-cloud systems,‚Äù IEEE Trans. Veh. Technol., vol. 72, no. 3, pp.
3930-3944, Mar. 2023.
[14] J. Li, W. Liang, Y. Li, Z. Xu, X. Jia, and S. Guo, ‚ÄúThroughput
maximization of delay-aware DNN inference in edge computing by
exploring DNN model partitioning and inference parallelism,‚Äù IEEE
Trans. Mobile Comput., vol. 22, no. 5, pp. 3017-3030, May 2023.
[15] X. Xu, K. Yan, S. Han, B. Wang, X. Tao and P. Zhang, ‚ÄúLearning-
based edge-device collaborative DNN inference in IoVT networks,‚Äù
IEEE Internet Things J., vol. 11, no. 5, pp. 7989-8004, Mar. 2024.
[16] C. -J. Wu et al., ‚ÄúMachine learning at facebook: Understanding inference
at the edge,‚Äù in Proc. IEEE Int. Symp. High Perform. Comput. Archit.
(HPCA), Feb. 2019, pp. 331-344.
[17] L. Liu, Y. Wang, and W. Shi, ‚ÄúUnderstanding time variations of DNN
inference in autonomous driving,‚Äù in Proc. 4th Wkshp. Benchmark.
Mach. Learn. Workloads Emerg. Hardw. (MLBench), Jun. 2023. [On-
line]. Available: https://arxiv.org/abs/2209.05487v1.
[18] S. Teerapittayanon, B. McDanel, and H. T. Kung, ‚ÄúDistributed deep
neural networks over the cloud, the edge and end devices,‚Äù in Proc.
IEEE 37th Int. Conf. Distrib. Comput. Syst. (ICDCS), Jun. 2017, pp.
328-339.
[19] Z. Zhao, K. M. Barijough, and A. Gerstlauer, ‚ÄúDeepThings: Distributed
adaptive deep learning inference on resource-constrained IoT edge
clusters,‚Äù IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst., vol.
37, no. 11, pp. 2348-2359, Nov. 2018.
[20] C. Hu and B. Li, ‚ÄúWhen the edge meets transformers: Distributed
inference with transformer models,‚Äù in Proc. IEEE 44th Int. Conf.
Distrib. Comput. Syst. (ICDCS), Jul. 2024, pp. 82-92.
[21] L. Zhang et al., ‚Äúnn-Meter: Towards accurate latency prediction of deep-
learning model inference on diverse edge devices,‚Äù in Proc. 19th Annu.
Int. Conf. Mobile Syst., Appl., Services (MobiSys), Jun. 2021, pp. 81-93.
[22] Z. Li, M. Paolieri, and L. Golubchik, ‚ÄúInference latency prediction for
CNNs on heterogeneous mobile devices and ML frameworks, Perform.
Eval., vol 165, p. 102429, Aug. 2024.
[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImageNet classiÔ¨Åcation
with deep convolutional neural networks,‚Äù Commun. ACM, vol 60, no.
6, pp. 84-90, May 2017.
[24] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image
recognition,‚Äù in Proc. IEEE Conf. Compt. Vis. Pattern Recogn. (CVPR),
Jun. 2016, pp. 770-778.
[25] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks for
large-scale image recognition,‚Äù in Proc. 3rd Int. Conf. Learn. Represent.
(ICLR), May 2015, pp. 1-14.


--- Page 17 ---
17
[26] H. Liang et al., ‚ÄúDNN surgery: Accelerating DNN inference on the edge
through layer partitioning,‚Äù IEEE Trans. Cloud Comput., vol. 11, no. 3,
pp. 3111-3125, Jul.-Sep. 2023.
[27] J. Haj-Yahya, A. Mendelson, Y. B. Asher, and A. Chattopadhyay,
Energy EfÔ¨Åcient High Performance Processors: Recent Approaches
for Designing Green High Performance Computing. New York, U.S:
Springer, 2018.
[28] Y. Wen, W. Zhang and H. Luo, ‚ÄúEnergy-optimal mobile application
execution: Taming resource-poor mobile devices with cloud clones,‚Äù in
Proc. IEEE Int. Conf. Commun. (INFOCOM), Mar. 2012, pp. 2716-2720.
[29] X. Chen, L. Jiao, W. Li, and X. Fu, ‚ÄúEfÔ¨Åcient multi-user computation
ofÔ¨Çoading for mobile-edge cloud computing,‚Äù IEEE/ACM Trans. Netw.,
vol. 24, no. 5, pp. 2795-2808, Oct. 2016.
[30] Z. Nan, Y. Han, J. Yan, S. Zhou, and Z. Niu, ‚ÄúRobust task ofÔ¨Çoading and
resource allocation under imperfect computing capacity information in
edge intelligence systems,‚Äù IEEE Trans. Mobile Comput., early access,
Feb. 2025, doi: 10.1109/TMC.2025.3539296.
[31] Q. Zeng, Y. Du, K. Huang, and K. K. Leung, ‚ÄúEnergy-efÔ¨Åcient resource
management for federated edge learning with CPU-GPU heterogeneous
computing,‚Äù IEEE Trans. Wireless Commun., vol. 20, no. 12, pp. 7947-
7962, Dec. 2021.
[32] Y. Han, Z. Nan, S. Zhou, and Z. Niu, ‚ÄúDVFS-aware DNN inference on
GPUs: Latency modeling and performance analysis,‚Äù arXiv: 2502.06295,
2025.
[33] H. R. Wu, A. R. Reibman, W. Lin, F. Pereira, and S. S. Hemami,
‚ÄúPerceptual visual signal compression and transmission,‚Äù Proc. IEEE,
vol. 101, no. 9, pp. 2025-2043, Sep. 2013.
[34] A. Nemirovski and A. Shapiro, ‚ÄúConvex approximations of chance
constrained programs,‚Äù SIAM J. Optim., vol. 17, no. 4, pp. 969-996,
2006.
[35] A. Ben-Tal, L. E. Ghaoui, and A. Nemirovski, Robust Optimization.
Princeton, U.S.: Princeton Univ. Press, 2009.
[36] X. Zhang, M. Mounesan, and S. Debroy, ‚ÄúEFFECT-DNN: Energy-
efÔ¨Åcient edge framework for real-time DNN inference,‚Äù in Proc. IEEE
24th Int. Symp. World Wireless, Mobile Multimedia Netw. (WoWMoM),
Jun. 2023, pp. 10-20.
[37] K. Tammer, ‚ÄúThe application of parametric optimization and imbedding
to the foundation and realization of a generalized primal decomposition
approach,‚Äù Math. Res., vol. 35, pp. 376-386, 1987.
[38] S. Li, Y. Huang, C. Li, B. A. Jalaian, Y. T. Hou, and W. Lou, ‚ÄúCoping
uncertainty in coexistence via exploitation of interference threshold
violation,‚Äù in Proc. 20th ACM Int. Symp. Mobile Ad Hoc Netw. Comput.,
Jul. 2019, pp. 71-80.
[39] Y. Nesterov and A. Nemirovski, Interior-Point Polynomial Algorithms
in Convex Programming. Philadelphia, U.S.: SIAM, 1994.
[40] T. Lipp and S. Boyd, ‚ÄúVariations and extension of the convex-concave
procedure,‚Äù Optim. Eng., vol. 17, no. 2, pp. 263-287, 2016.
[41] 3GPP. (Apr. 2022). TR 36.931: Radio Frequency (RF) Requirements for
LTE Pico Node B. Version 17.0.0. Accessed: Oct. 2022.
[42] A. Krizhevsky, ‚ÄúLearning multiple layers of features from tiny images,‚Äù
M.S. thesis, Univ. Toronto, Toronto, ON, CA, 2009.
[43] NVIDIA. NVIDIA Jetson Xavier NX. Accessed: Sep. 21, 2024. [On-
line]. Available: https://www.nvidia.com/en-us/autonomous-machines/
embedded-systems/jetson-xavier-nx/
[44] NVIDIA.
Tegrastats
Utility.
Accessed:
Oct.
20,
2024.
[Online].
Available:
https://docs.nvidia.com/drive/drive-os-5.2.6.0L/drive-os/
index.html#page/DRIVE OS Linux SDK NGC Development Guide/
Utilities/util tegrastats.html
[45] D. Wen, P. Liu, G. Zhu, Y. Shi, J. Xu, Y. C. Eldar, and S. Cui, ‚ÄúTask-
oriented sensing, computation, and communication integration for multi-
device edge AI,‚Äù IEEE Trans. Wireless Commun., vol. 23, no. 3, pp.
2486-2502, Mar. 2024.
[46] Z. Liang, Y. Liu, T. -M. Lok and K. Huang, ‚ÄúMultiuser computation
ofÔ¨Çoading and downloading for edge computing with virtualization,‚Äù
IEEE Trans. Wireless Commun., vol. 18, no. 9, pp. 4298-4311, Sep.
2019.
[47] J. Yan, S. Bi, L. Duan and Y. -J. A. Zhang, ‚ÄúPricing-driven service
caching and task ofÔ¨Çoading in mobile edge computing,‚Äù IEEE Trans.
Wireless Commun., vol. 20, no. 7, pp. 4495-4512, Jul. 2021.
[48] J. Long, H. Tai, S. Hsieh, M. J. Yuan, ‚ÄúA lightweight design for
serverless function-as-a-service,‚Äù arXiv:2010.07115, Oct. 2020.
[49] Z. Li, L. Guo, J. Cheng, Q. Chen, B. He, and M. Guo, ‚ÄúThe serverless
computing survey: A technical primer for design architecture ,‚Äù ACM
Comput. Surv., vol. 54, no. 10, pp. 1-34, Sep. 2022.
[50] A. Dosovitskiy et al., ‚ÄúAn image is worth 16x16 words: Transformers
for image recognition at scale,‚Äù in Proc. 9th Int. Conf. Learn. Represent.
(ICLR), May 2021, pp. 1-21.
[51] NVIDIA. NVIDIA Jetson Nano. Accessed: Jun. 21, 2025. [On-
line]. Available: https://www.nvidia.com/en-us/autonomous-machines/
embedded-systems/jetson-nano/product-development/
Zhaojun Nan (S‚Äô20-M‚Äô23) received the B.S. de-
gree in automation from Jiamusi University, Jiamusi,
China, in 2009, the M.S. degree in navigation, guid-
ance and control from Harbin Engineering Univer-
sity, Harbin, China, in 2012, and the Ph.D. degree
in information and communication engineering from
Chongqing University, Chongqing, China, in 2022.
From 2012 to 2017, he worked for Tianjin 712
Communication & Broadcasting Co.,Ltd., Tianjin,
China. He is currently a Postdoctoral Researcher
with the Network Integration for Ubiquitous Linkage
and Broadband Laboratory, Department of Electronic Engineering, Tsinghua
University, Beijing, China. His research interests include mobile edge com-
puting, vehicular networks and autonomous driving, and green wireless
communications. He has served as the TPC member for IEEE Globecom,
ICC, VTC and WCNC.
Yunchu Han (S‚Äô24) received the B.E. degree from
the Department of Electronic Engineering, Tsinghua
University in 2023. He is currently pursuing the
Ph.D. degree with the Network Integration for Ubiq-
uitous Linkage and Broadband Laboratory, Depart-
ment of Electronic Engineering, Tsinghua Univer-
sity. His research interests include edge comput-
ing, edge intelligence, vehicular networks and green
wireless communication.
Sheng Zhou (S‚Äô06-M‚Äô12) received the B.E. and
Ph.D. degrees in electronic engineering from Ts-
inghua University, Beijing, China, in 2005 and 2011,
respectively. In 2010, he was a Visiting Student
with the Wireless System Lab, Department of Elec-
trical Engineering, Stanford University, Stanford,
CA, USA. From 2014 to 2015, he was a Visiting
Researcher with Central Research Lab, Hitachi Ltd.,
Tokyo, Japan. He is currently an Associate Profes-
sor with the Department of Electronic Engineering,
Tsinghua University, Beijing, China. His research
interests include cross-layer design for multiple antenna systems, mobile edge
computing, vehicular networks, and green wireless communications. He was
the recipient of the IEEE ComSoc Asia-PaciÔ¨Åc Board Outstanding Young
Researcher Award in 2017, and IEEE ComSoc Wireless Communications
Technical Committee Outstanding Young Researcher Award in 2020.
Zhisheng Niu (M‚Äô98-SM‚Äô99-F‚Äô12) graduated from
Beijing Jiaotong University, China, in 1985, and
got his M.E. and D.E. degrees from Toyohashi
University of Technology, Japan, in 1989 and 1992,
respectively. From 1992 to 1994, he worked for
Fujitsu Laboratories Ltd., Japan, and in 1994 joined
with Tsinghua University, Beijing, China, where he
is now a Professor at the Department of Electronic
Engineering. His major research interests include
queueing theory, trafÔ¨Åc engineering, mobile Internet,
radio resource management of wireless networks,
and green communication and networks.
Dr. Niu has been serving IEEE Communications Society since 2000 as
Chair of Beijing Chapter (2000-2008), Director of Asia-PaciÔ¨Åc Board (2008-
2009), Director for Conference Publications (2010-2011), Chair of Emerging
Technologies Committee (2014-2015), Director for Online Contents (2018-
2019) and the Editor-in-Chief of IEEE TRANSACTIONS ON GREEN COM-
MUNICATIONS AND NETWORKING (2020-2022). He received the Best Paper
Award of Asia-PaciÔ¨Åc Board in 2013, Distinguished Technical Achievement
Recognition Award of Green Communications and Computing Technical
Committee in 2018, and Harold Sobol Award for Exemplary Service to
Meetings & Conferences in 2019, all from the IEEE Communications Society.
He was selected as a distinguished lecturer of IEEE Communications Society
(2012-2015) as well as IEEE Vehicular Technologies Society (2014-2018).
He is a fellow of both IEEE and IEICE.
