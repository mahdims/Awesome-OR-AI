--- Page 1 ---
1
Automatic Operator-level Parallelism Planning for
Distributed Deep Learning – A Mixed-Integer
Programming Approach
Ruifeng She, Bowen Pang, Kai Li, Zehua Liu, and Tao Zhong
Abstract—As the artificial intelligence community advances
into the era of large models with billions of parameters, dis-
tributed training and inference have become essential. While
various
parallelism
strategies—data,
model,
sequence,
and
pipeline—have been successfully implemented for popular neural
networks on main-stream hardware, optimizing the distributed
deployment schedule requires extensive expertise and manual
effort. Further more, while existing frameworks with most simple
chain-like structures, they struggle with complex non-linear
architectures. Mixture-of-experts and multi-modal models feature
intricate MIMO and branch-rich topologies that require fine-
grained operator-level parallelization beyond the capabilities of
existing frameworks.
We propose formulating parallelism planning as a schedul-
ing optimization problem using mixed-integer programming.
We propose a bi-level solution framework balancing optimality
with computational efficiency, automatically generating effective
distributed plans that capture both the heterogeneous struc-
ture of modern neural networks and the underlying hardware
constraints. In experiments comparing against expert-designed
strategies like DeepSeek’s DualPipe, our framework achieves
comparable or superior performance, reducing computational
bubbles by half under the same memory constraints.
The framework’s versatility extends beyond throughput op-
timization to incorporate hardware utilization maximization,
memory capacity constraints, and other considerations or po-
tential strategies. Such capabilities position our solution as both
a valuable research tool for exploring optimal parallelization
strategies and a practical industrial solution for large-scale AI
deployment.
Index Terms—Automated Scheduling, Distributed Computa-
tion, Mixed-Integer Programming.
I. INTRODUCTION
Large Language Models (LLMs) have emerged as a trans-
formative force in artificial intelligence, revolutionizing capa-
bilities across natural language processing tasks [2], [3]. These
models have demonstrated remarkable abilities in text gener-
ation, comprehension, and reasoning by scaling to unprece-
dented sizes, with state-of-the-art architectures now reaching
trillions of parameters [4], [5]. The phenomenal emergence of
LLMs has triggered exponential growth in their applications
across industries, from conversational agents to content cre-
ation and sophisticated reasoning systems. However, this rapid
expansion comes with substantial computational challenges as
deploying larger models demands increasingly sophisticated
hardware infrastructures and efficient computing strategies [6].
Organizations seeking to leverage these powerful models face
R. She, B. Pang, K. Li, Z. Liu, and T. Zhong are with Noah’s Ark Lab,
Huawei. E-mail: she.ruifeng@huawei.com.
0
10
20
30
40
50
60
Time
0
1
2
3
4
5
6
7
Machine
0,0
1,0
2,0
3,0
4,0
5,0
6,0
7,0
7,0
7,0
6,0
6,0
5,0
5,0
4,0
4,0
3,0
3,0
2,0
2,0
1,0
1,0
0,0
0,0
0,1
1,1
2,1
3,1
4,1
5,1
6,1
7,1
7,1
7,1
6,1
6,1
5,1
5,1
4,1
4,1
3,1
3,1
2,1
2,1
1,1
1,1
0,1
0,1
0,2
1,2
2,2
3,2
4,2
5,2
6,2
7,2
7,2
7,2
6,2
6,2
5,2
5,2
4,2
4,2
3,2
3,2
2,2
2,2
1,2
1,2
0,2
0,2
0,3
1,3
2,3
3,3
4,3
5,3
6,3
7,3
7,3
7,3
6,3
6,3
5,3
5,3
4,3
4,3
3,3
3,3
2,3
2,3
1,3
1,3
0,3
0,3
0,4
1,4
2,4
3,4
4,4
5,4
6,4
7,4
7,4
7,4
6,4
6,4
5,4
5,4
4,4
4,4
3,4
3,4
2,4
2,4
1,4
1,4
0,4
0,4
0,5
1,5
2,5
3,5
4,5
5,5
6,5
7,5
7,5
7,5
6,5
6,5
5,5
5,5
4,5
4,5
3,5
3,5
2,5
2,5
1,5
1,5
0,5
0,5
0,6
1,6
2,6
3,6
4,6
5,6
6,6
7,6
7,6
7,6
6,6
6,6
5,6
5,6
4,6
4,6
3,6
3,6
2,6
2,6
1,6
1,6
0,6
0,6
0,7
1,7
2,7
3,7
4,7
5,7
6,7
7,7
7,7
7,7
6,7
6,7
5,7
5,7
4,7
4,7
3,7
3,7
2,7
2,7
1,7
1,7
0,7
0,7
0,8
1,8
2,8
3,8
4,8
5,8
6,8
7,8
7,8
7,8
6,8
6,8
5,8
5,8
4,8
4,8
3,8
3,8
2,8
2,8
1,8
1,8
0,8
0,8
0,9
1,9
2,9
3,9
4,9
5,9
6,9
7,9
7,9
7,9
6,9
6,9
5,9
5,9
4,9
4,9
3,9
3,9
2,9
2,9
1,9
1,9
0,9
0,9
0,10
1,10
2,10
3,10
4,10
5,10
6,10
7,10
7,10
7,10
6,10
6,10
5,10
5,10
4,10
4,10
3,10
3,10
2,10
2,10
1,10
1,10
0,10
0,10
0,11
1,11
2,11
3,11
4,11
5,11
6,11
7,11
7,11
7,11
6,11
6,11
5,11
5,11
4,11
4,11
3,11
3,11
2,11
2,11
1,11
1,11
0,11
0,11
0,12
1,12
2,12
3,12
4,12
5,12
6,12
7,12
7,12
7,12
6,12
6,12
5,12
5,12
4,12
4,12
3,12
3,12
2,12
2,12
1,12
1,12
0,12
0,12
0,13
1,13
2,13
3,13
4,13
5,13
6,13
7,13
7,13
7,13
6,13
6,13
5,13
5,13
4,13
4,13
3,13
3,13
2,13
2,13
1,13
1,13
0,13
0,13
0,14
1,14
2,14
3,14
4,14
5,14
6,14
7,14
7,14
7,14
6,14
6,14
5,14
5,14
4,14
4,14
3,14
3,14
2,14
2,14
1,14
1,14
0,14
0,14
0,15
1,15
2,15
3,15
4,15
5,15
6,15
7,15
7,15
7,15
6,15
6,15
5,15
5,15
4,15
4,15
3,15
3,15
2,15
2,15
1,15
1,15
0,15
0,15
0,16
1,16
2,16
3,16
4,16
5,16
6,16
7,16
7,16
7,16
6,16
6,16
5,16
5,16
4,16
4,16
3,16
3,16
2,16
2,16
1,16
1,16
0,16
0,16
0,17
1,17
2,17
3,17
4,17
5,17
6,17
7,17
7,17
7,17
6,17
6,17
5,17
5,17
4,17
4,17
3,17
3,17
2,17
2,17
1,17
1,17
0,17
0,17
0,18
1,18
2,18
3,18
4,18
5,18
6,18
7,18
7,18
7,18
6,18
6,18
5,18
5,18
4,18
4,18
3,18
3,18
2,18
2,18
1,18
1,18
0,18
0,18
0,19
1,19
2,19
3,19
4,19
5,19
6,19
7,19
7,19
7,19
6,19
6,19
5,19
5,19
4,19
4,19
3,19
3,19
2,19
2,19
1,19
1,19
0,19
0,19
Task Type
forward
backward_input
backward_weight
(a) Automatically searched schedule
(b) DeepSeek v3’s DualPipe (figure taken from [1])
Fig. 1: Comparison of automatically searched parallelization
schedule against expert-crafted schedule Dualpipe [1]. Colored
blocks represent computation on each device, while white gaps
indicate pipeline bubbles. The searched schedule achieved half
of bubble counts as Dualpipe’s, with similar scheduling pattern
but different bubble distribution.
critical constraints in computational efficiency, as training and
inference costs scale dramatically with model size. Single-
device training and inference have become impractical for
these massive models, making distributed computation not
merely an optimization but a fundamental necessity [7], [8].
To address these challenges, a typical top-to-bottom frame-
work for employing parallel computation is illustrated in
Figure 2. The neural network module, encapsulating the archi-
tecture and learned parameters, can be represented as a graph,
where nodes and edges capture the computational dependen-
cies and data flow within the network. To facilitate efficient
parallelization across multiple computing devices, a paral-
lel schedule is designed to manage the communication and
synchronization between nodes, ensuring efficient interaction
and data exchange. The schedule is then recompiled through
the optimized parallel interface into command sequences for
the hardware level. Several pioneering companies have de-
veloped sophisticated systems to enable parallel training and
deployment of neural networks. Microsoft’s DeepSpeed offers
a comprehensive suite of parallelism techniques including
arXiv:2503.09357v1  [cs.LG]  12 Mar 2025


--- Page 2 ---
2
Zero Redundancy Optimizer (ZeRO), tensor, pipeline, and
data parallelism that can be combined to enable training of
extremely large models [9]. Similarly, Google’s GShard and
Nvidia’s Megatron-LM represent other notable contributions
that leverage model parallelism and sharding techniques to
distribute computation efficiently across multiple devices [10],
[11]. While these approaches have pushed the boundaries of
what’s possible, they remain largely manual, requiring signif-
icant expertise to implement and tune for specific hardware
configurations.
The current framework for designing parallel computation
systems for LLMs involves significant manual effort and
domain expertise. Engineering teams must navigate com-
plex design spaces involving distributed computing archi-
tectures, memory constraints, communication patterns, and
hardware-specific optimizations [7], [12]. A successful exam-
ple is DeepSeek’s DualPipe, featuring a hybrid bi-directional
pipeline mechanism that optimizes both memory utilization
and computational throughput [1]. Evidently, this process
typically requires iterative experimentation and fine-tuning by
specialists with deep understanding of both the underlying
hardware and the specific characteristics of language models.
The lack of an automatic designing method creates substantial
barriers to entry for many organizations and researchers, limit-
ing innovation and accessibility in the field [11]. Furthermore,
as models continue to grow in size and complexity, manual
design approaches become increasingly impractical and error-
prone. Recent frameworks orchestrate sophisticated rules and
strategies to automate parallel deployment up to certain ex-
tents. However, these frameworks typically presume chain-like
neural network structures and make specific simplifications,
which may be limited in applicability or lead to suboptimal
utilization for complex architectures.
In this paper, we introduce an automatic framework for
designing parallelization strategies for LLM training and infer-
ence, fitting in the colored module in Figure 2. We bridge the
gap between theoretical scheduling optimization and practical
distributed deep learning by formulating the parallelization
problem as a mixed-integer program. By formalizing the
parallelization strategy as a variant of job shop scheduling
problem (JSSP), we enable automatic discovery of efficient
execution plans tailored to specific hardware configurations
and model architectures, considering various hardware con-
straints and performance objectives. We applied our proposed
framework to optimize the training process of a neural net-
work under memory constraints, and recovered parallelization
plans achieving the same utility as expert-tailored strategy
Dualpipe by Deepseek [1]. Furthermore, by continuing the
search, we discovered plans achieving even better utility that
halved the bubble counts than that of Dualpipe, as shown in
Fig. 1, demonstrating the benefit of automating parallelization
designing from an expanded solution space. Our framework
represents a significant step toward automating the design of
efficient distributed computation strategies for modern neu-
ral networks facilitating formal mathematical programming.
By formalizing the parallelization problem within the well-
established MIP methodology, we enable systematic explo-
ration of the design space and identification of optimal or near-
optimal solutions without relying exclusively on heuristics or
expert intuition.
Fig. 2: High-level illustration of parallel computation.
Our main contributions are summarized as follows:
• We propose a comprehensive mixed-integer programming
(MIP) formulation that captures the essential aspects of
distributed neural network computation, including opera-
tor dependencies, device capabilities, and communication
costs.
• We design a bi-level solution framework that balances
optimality with computational efficiency, making the ap-
proach practical for real-world model sizes and hardware
configurations.
• We conduct experimental validation of the effectiveness
of our approach. Through an experiment to reproduce the
Dualpipe scheduling strategy by DeepSeek, we demon-
strate the capability of our framework to automatically
discover comparable or potentially better performance
than expert-designed strategies.
The remainder of the paper is structured as follows. A
comprehensive literature review is given in section II. We
present our problem formulation for automatically paralleliza-
tion design as a MIP in section III along with a few extensions.
We introduce our solution approach to solve the proposed
model in section IV. Numerical results are laid out in section V
demonstrating how this framework can advance both research
and practical deployment of large-scale deep learning models.
Finally, we make conding remakrs in section VI.
II. LITERATURE REVIEW
A. Distributed Deep Learning Paradigms
Researchers and practitioners have developed several paral-
lelism strategies to address the challenges of distributed deep
learning. These approaches can be broadly categorized into
data parallelism, model parallelism, pipeline parallelism, and
tensor parallelism, each with distinct characteristics and trade-
offs [12], [17].
Data parallelism represents the most straightforward ap-
proach, where identical copies of the model operate on dif-
ferent data batches across devices, with periodic gradient


--- Page 3 ---
3
TABLE I: Literature Review
Work
Automatic
Scheduling
Theoretical
Bound
Makespan
Minimization
Throughput
Maximization
Memory
Management
Nonlinear
Structure
Flexible
Parallelization
Megatron-LM [12]
✓
DeepSpeed [9]
✓
✓
✓
✓
Alpa [8]
✓
✓
✓
✓
✓
FlexFlow [13]
✓
✓
✓
✓
✓
Galvatron [14]
✓
✓
✓
✓
✓
GSPMD [15]
✓
✓
✓
PipeDream [16]
✓
✓
✓
✓
DualPipe [1]
✓
✓
✓
✓
OURS
✓
✓
✓
✓
✓
✓
✓
synchronization [17]. While conceptually simple and offering
good scaling for computation-bound scenarios, data paral-
lelism faces significant communication overhead as model
sizes grow, limiting its effectiveness for the largest models
[7].
Model parallelism divides the neural network across de-
vices, with each device responsible for specific layers or
components [12]. This approach directly addresses memory
constraints but introduces synchronization points that can lead
to device underutilization. Tensor parallelism, a specialized
form of model parallelism, splits individual operations (such
as matrix multiplications) across devices, enabling more fine-
grained distribution [12].
Pipeline parallelism organizes computation into stages ex-
ecuted across different devices, with activations flowing be-
tween stages in a pipelined manner [16], [18]. This approach
can improve device utilization but requires careful scheduling
to minimize ”bubbles” (idle periods) in the pipeline, particu-
larly when dealing with complex model architectures [19].
Recent approaches have combined these strategies into
hybrid parallelism schemes [8], [11], [20], recognizing that
no single strategy is optimal for all scenarios. These hybrid
approaches typically rely on heuristics or expert knowledge to
determine the parallelization strategy, often tailored to specific
model architectures and hardware configurations.
The development of automated frameworks for designing
and implementing distributed deep learning strategies has
accelerated in recent years. We compare and tabulate a set
of representative ones in Tabel I. Systems like Megatron-
LM [12] and DeepSpeed [9] have established foundational
approaches for model and pipeline parallelism in transformer
architectures. These frameworks often incorporate specific
optimization techniques tailored to their target architectures
but generally require manual configuration of parallelization
strategies. More recent systems have aimed to automate as-
pects of this decision-making process. FlexFlow [13] pio-
neered the use of search-based methods to explore paral-
lelization strategies for neural networks, considering multiple
dimensions of parallelism simultaneously. This approach was
further expanded by GSPMD [15], which uses a sharding
propagation technique to automate tensor partitioning deci-
sions. Alpa [8] introduced a hierarchical approach that sepa-
rates intra-operator and inter-operator parallelism decisions,
using a combination of dynamic programming and integer
linear programming to search for efficient configurations.
Unity [21] extended these ideas to heterogeneous clusters by
incorporating hardware-aware cost modeling. Other systems
like PipeDream [16] and Galvatron [14] have focused on
specific aspects such as memory optimization, communication
reduction, or heterogeneous resource allocation. Despite these
advances, most existing frameworks either rely on simplified
models of the underlying computation graphs, focus on spe-
cific model architectures, or employ heuristic search methods
that may yield suboptimal solutions for complex, non-linear
neural network topologies. Additionally, many approaches
encounter scalability challenges when applied to the largest
models, highlighting the need for more principled optimization
frameworks that can efficiently navigate the vast design space
of distributed deep learning configurations.
B. Challenges in Modern Model Architectures
While existing parallelism approaches have proven effective
for models with relatively simple, sequential structures (such
as standard transformers), they face significant challenges
when applied to modern architectures with more complex
topologies [10], [22], [23]. Mixture-of-expert models [10],
[22], for example, introduce conditional branching patterns
that create imbalanced computation paths. Multi-modal ar-
chitectures [23] combine different network structures with
varying computational characteristics. These models feature
Multiple-Input Multiple-Output (MIMO) patterns and branch-
rich topologies that fundamentally complicate the paralleliza-
tion process.
The standard approaches to pipeline parallelism, which
typically partition models into sequential stages, struggle with
these non-linear structures. Efficiently distributing computa-
tion for such models requires operator-level decisions that
consider the unique characteristics of each component and
their interactions [8], [11]. Manual design of parallelization
strategies for these complex architectures demands significant
expertise and experimentation, often leading to suboptimal
solutions due to the vast design space [24]. Furthermore, hard-
ware heterogeneity introduces additional complexity. Modern
AI accelerators vary in memory capacity, compute capability,
and interconnect bandwidth. Effective parallelization must
account for these differences to achieve optimal performance,
further complicating the design process.


--- Page 4 ---
4
C. Scheduling Problems in Distributed Computing
The challenge of optimally distributing neural network
computation across multiple devices bears strong resemblance
to classical scheduling problems, particularly the job shop
scheduling problem (JSSP) [25]. In JSSP, a set of jobs must
be processed on a set of machines, with each job comprising
a sequence of operations with specified processing times.
The objective is typically to minimize the total completion
time (often referred to as the makespan) while respecting
precedence constraints and machine availability. Such a re-
semblance is particularly obvious when jointly considering
model and pipeline parallelism for neural networks. The
forward and backward passes through network layers can be
viewed as operations with specific processing requirements
and precedence constraints, while accelerator devices represent
the machines on which these operations must be scheduled
[16], [19]. The goal of minimizing training or inference time
corresponds directly to the makespan minimization objective
in JSSP. On the other hand, data parallelism poses a top-
level load-balancing assignment problem, aiming to partition
the machines such that the job loads are equally assigned
to each machine and are completed simultaneously. Tensor
parallelism instead can be viewed as a bottom-level sub-
problem of JSSP, where a single operation is processed across
multiple machines.
Several researchers have recognized this connection and
applied scheduling techniques to distributed deep learning.
[19] formulated pipeline parallelism as a scheduling problem
and developed heuristic solutions. [16] proposed PipeDream,
which uses dynamic programming to find efficient pipeline
configurations. [8] developed a two-level approach that com-
bines intra-operator and inter-operator parallelism through
hierarchical search. [26] formulate the inference serving sys-
tem as an online scheduling problem and provided heuristics
based on decomposition method. However, these approaches
typically employ heuristics or simplified models due to the
computational complexity of the underlying scheduling prob-
lem, which is known to be NP-hard [25]. They also often focus
on specific model architectures or parallelism strategies rather
than providing a general framework applicable to diverse
scenarios. Mixed-integer programming (MIP), on the other
hand, offers a powerful framework for modeling and solving
scheduling problems with complex constraints [27]. While
MIP approaches have been successfully applied in various
manufacturing and service scheduling contexts [28], [29],
their application to distributed deep learning has been limited,
primarily due to scalability concerns for large models.
III. PROBLEM FORMULATION
Consider a neural network consisting of a set of operations,
each requiring certain time to execute, that all of which must
be executed to complete the inference or training task. Some
operations produce intermediate outputs that are required as
inputs by downstream operations, hence must be executed in
order, while operations do not have such an dependency can be
executed in any order. The operations are executed on cluster
of computation resources, consisting of a set of machines
(e.g., GPUs or NPUs) interconnected via communication chan-
nels, through which the data flow between operations can
be transferred. Meanwhile, executing each operation requires
allocating some space on the machine (i.e., memory) to store
the output (e.g., activations) and the asset used by the operation
(e.g., the weights and bias of a hidden layer). In addition, we
assume the following assumptions:
• All machines and communication channels are available
at the beginning.
• Each machine can process at most one operation at any
time, and each communication channel can process one
communication task at any time1.
• Operations and communication tasks cannot be inter-
rupted once started (i.e., non-preemptive).
Such a problem falls into the class of Flexible Distributed
Job Shop Scheduling Problems (FDJSSP), where flexible in-
dicates that the order of executing some operations can be
interchanged, and distributed indicates that each operation can
be executed on one of many candidate machines.
Following the convenient representation of the disjunctive
graph model in the literature of JSSP, we use a directed acyclic
graph (DAG) Gm(I, B) to represent the computation graph of
a neural network, where I denotes the set of operations, and
B denotes the data dependency between operations. Similarly,
we use a directed graph Gh(J , C) to denote the computation
cluster, where J denotes the devices and C denotes the
communication channels. Note that Gh is likely cyclic due
to the presence of bi-directional communication channels in
most modern hardware. Throughout this paper, we will use
upper-case symbols to denote known constant parameters, and
lower-case symbols for decision variables and indices. We
denote Do
i as the execution duration required for completing
operation i, and Dc
i1,i2 as that for the communication task
from operation i1 to i2
2. For simplicity, we assume the
most memory-consuming asset to store are the weights of the
operations and neglect the rest, and denote Ai and Wi as the
memory requirement to store the output and the weight of
operation i. Note that Ai can be either positive, negative or
0, indicating an increasing, decreasing or unchanged memory
level after executing the operation. For the communication
channel, flow control and congestion management are adopted
in most modern multi-thread devices, hence we assume that
any arbitrarily large dataflow can be sent/received over longer
communication duration without causing memory outflow.
The automatic design of parallelization strategy thus aims to
construct a valid arrangement (i.e., not exceeding the memory
limit of any machine) for the operations onto machines that
optimizes certain metric, such as the total completion time
(often referred to as the makespan) of the neural network, or
the throughput of a batch of requests, or many others.
1Here we assume single-thread machines for simplicity. One can model
multi-thread machines by grouping multiple machines and form a sub-cluster
accordingly.
2Here we assume that the machines and communication channels are
homogeneous for simplicity. The proposed model can be easily extended to
heterogeneous situations by including the machine dimension relating to the
operation allocation decisions.


--- Page 5 ---
5
TABLE II: Notation Table: Sets, Parameters and Decision Variables
Sets
Description
I
Set of operations.
B
Set of data-dependent operations.
J
Set of machines.
C
Set of communication channels.
Parameters
Do
i, ∀i ∈I
Duration of operation i.
Dc
i1,i2, ∀(i1, i2) ∈B
Duration of communication task (i1, i2).
Wi, ∀i ∈I
Memory requirement of operation i.
Ai, ∀i ∈I
Memory adjustment after operation i.
Decision Variables
xi,j ∈{0, 1}, ∀i ∈I, j ∈J
Allocation of operation i to machine j.
yi1,i2 ∈{0, 1}, ∀(i1, i2) ∈I × I
Precedence of operation i1 before i2.
si, ∀i ∈I
Start time of operation i.
ei, ∀i ∈I
End time of operation i.
ti1,i2, ∀(i1, i2) ∈I × I
Slack time between operations i1 and i2.
zi1,i2,j1,j2 ∈{0, 1}, ∀(i1, i2) ∈B, (j1, j2) ∈J × J
Allocation of communication task (i1, i2) to channel (j1, j2).
wi1,i2,i3,i4 ∈{0, 1}, ∀(i1, i2), (i3, i4) ∈B × B
Precedence of communication task (i1, i2) before (i3, i4).
ci1,i2, ∀(i1, i2) ∈B
Start time of communication task (i1, i2).
di1,i2, ∀(i1, i2) ∈B
End time of communication task (i1, i2).
ui1,i2 ∈{0, 1}, ∀(i1, i2) ∈I × I
Immediate precedence of operation i1 before i2.
m−
i , ∀i ∈I
Memory level before operation i.
m+
i , ∀i ∈I
Memory level after operation i.
The FDJSSP minimizing the makespan can be formulated
as the following MIP:
min z
(1)
s.t.
z ≥ei, ∀i ∈I,
(2)
ei −si = Do
i, ∀i ∈I,
(3)
si1 −si2 −ti1,i2 = 0, ∀i1 ̸= i2 ∈I,
(4)
ti1,i2 ≥0, ∀(i1, i2) ∈B,
(5)
X
j∈J
xi,j = 1, ∀i ∈I,
(6)
−M(3 −yi1,i2 −xi1j −xi2j)
+ ei1 −si2 ≤0, ∀i1 ̸= i2 ∈I, j ∈J ,
(7)
yi1,i2 + yi2i1 = 1, ∀i1 ̸= i2 ∈I,
(8)
zi1,i2,j1,j2 = xi1j1 · xi2j2,
∀(i1, i2) ∈B, (j1, j2) ∈C
(9)
zi1,i2,j1,j2 ≤0, ∀(j1, j2) /∈C,
(10)
di1,i2 −ci1,i2 ≥Dc
i1,i2, ∀(i1, i2) ∈B,
(11)
si2 −di1,i2 ≥0, ∀(i1, i2) ∈B,
(12)
ei1 −ci1,i2 ≤0, ∀(i1, i2) ∈B,
(13)
M(3 −wi1,i2,i3,i4 −zi1,i2,j1,j2 −zi3i4j1j2) + ci3i4
−di1,i2 ≥0, ∀(i1, i2) ̸= (i3, i4) ∈B, (j1, j2) ∈C,
(14)
wi1,i2,i3,i4 + wi3i4i1i2 = 1, ∀(i1, i2) ̸= (i3, i4) ∈B,
(15)
M(1 −xi,j) + m−
i ≥
X
i
Wixi,j, ∀i ∈I, ∀j ∈J ,
(16)
M(1 −ui1,i2) + m−
i2 −m+
i1 ≥0, ∀i1 ̸= i2, i1, i2 ∈I,
(17)
−M(1 −ui1,i2) + m−
i2 −m+
i1 ≤0, ∀i1 ̸= i2 ∈I,
(18)
m+
i −m−
i −Ai = 0, ∀i ∈I,
(19)
where M denotes a large positive number. In the above, Eqs.
1-2 state that the objective is to minimize the makespan, which
equals the ending time of the last operation; Eqs. 3 relate the
starting time, ending time, and the execution duration of an
operation; Eqs. 4 define the slack time between operations,
and Eqs. 5 enforces the ordering between data-dependent
operations; Eqs. 6 guarantee that each operation is allocated
to exactly one machine; Eqs. 7-8 together state that if any two
operations are executed on the same machine, their execution
time must not overlap; Eqs. 9 relates the operation-machine
allocation to the corresponding communication-channel allo-
cation3; Eqs. 10 negates communication between machines
that are not connected by a communication channel; Eqs. 11
relate the starting time, ending time, and the execution duration
of a communication task; Eqs. 12-13 relate the starting/ending
of a communication task with its up/downstream dependent
task, respectively; Eqs. 14-15 together state that if any two
communication tasks are executed on the same communication
channel, their execution time must not overlap; Eqs. 16 initial-
izes the memory level for operations on each machine; Eqs.
17-19 simulate the change of memory level on the machine.
IV. SOLUTION METHOD
The proposed MIP formulation provides a comprehensive
and general framework for addressing the automatic paral-
lelization problem. However, due to the combinatorial na-
ture of distributed scheduling problems, the computational
complexity grows significantly as the number of operations
increases, making it challenging to obtain solutions within a
limited time budget. To address this challenge, we first intro-
duce a heuristic approach that pre-merges certain operations,
3Eqs. 9 is written as quadratic constraints for conciseness, and they can be
solved by modern solvers such as Gurobi. Such constraints can be linearized
using techniques suggested in [30]


--- Page 6 ---
6
thereby reducing the problem size and achieving a balance
between solution optimality and computational cost.
Inspired by priority dispatch rule-based algorithms, our
heuristic iteratively identifies pairs of nodes for merging based
on a set of predefined rules. The core of the algorithm
consists of two key functions: GetCandidateEdge(I, B)
and GetCandidateNonEdge(I, B). These functions suggest
candidate node pairs for merging, with the distinction that
GetCandidateEdge focuses on pairs connected by an edge
in the graph, while GetCandidateNonEdge considers pairs
without a direct edge. During each function call, topologically
redundant edges, defined as {(i1, i2) ∈B | ∃i3 : (i1, i3) ∈
B, (i3, i2) ∈B}, are temporarily removed. This step is crucial
to reveal structurally important edges and prevent the forma-
tion of cycles after node merging.
The function GetCandidateEdge identifies edges where
the head node has a single input and the tail node has a single
output, subject to predefined thresholds on operation duration
and memory requirements. Such node pairs are deemed ”safe”
for merging, as their combination does not increase the critical
path length of the graph. Conversely, merging nodes that
are not directly connected is considered ”unsafe,” as it may
inadvertently extend the critical path length. To mitigate this
risk, GetCandidateNonEdge imposes stricter thresholds to
avoid creating overly large operations that could become
performance bottlenecks.
The function MergeNodes is responsible for merging the
selected node pairs and updating the graph structure accord-
ingly. It ensures that the new node is correctly connected
to the rest of the graph based on the original connectivity,
while also updating node and edge attributes. For instance,
the duration and memory requirements of the newly merged
node are computed as the sum of the corresponding attributes
of the constituent nodes. The pseudo-code of the algorithm is
provided in Algorithm 1.
We acknowledge that this heuristic is inherently greedy.
Effectively simplifying the graph Gm requires solving a graph
partitioning problem with potentially complex constraints,
which we leave as a direction for future research. Once Gm
is reduced to a manageable size—typically fewer than 100
nodes—we employ a commercial MIP solver to solve the
proposed model efficiently.
V. NUMERICAL STUDY
In this section, we present the numerical results obtained
from our proposed framework. We employed the commercial
solver Gurobi to solve the MIP. The framework is implemented
in Python on PC with an Intel Core i7 processor, 32 GB of
RAM.
A. Reproducing and Enhancing Dualpipe Strategy
We first validate our framework’s ability to rediscover
expert-designed parallelization strategies and potentially im-
prove upon them. We compare our approach with Dualpipe, a
pipeline parallelism technique implemented in DeepSeek V3.
Dualpipe represents a state-of-the-art manual parallelization
Algorithm 1 Heuristic to merge operations in JSSP
Require: Gm(I, B), N
while True do
while GetCandidateEdge(I, B) returns do ▷Loop over
to merge edges
(node1, node2) ←GetCandidateEdge(I, B)
I, B ←MergeNodes(I, B, node1, node2)
Break if |I| ≤N
end while
while GetCandidateEdge(I, B) returns do ▷Loop over
to merge non-edges
(node1, node2) ←GetCandidateNonEdge(I, B)
I, B ←MergeNodes(I, B, node1, node2)
Break if |I| ≤N
end while
end while
strategy designed to balance memory constraints and compu-
tational efficiency for large language models.
Our first objective was to verify if our mathematical pro-
gramming framework could automatically derive the Du-
alpipe strategy when given identical constraints. We follow
the technical report of DeepSeek V3 and assume a neural
network divided into stages, where the number of stages =
the number of machines = the pipeline parallelization rank,
denoted by PP. For simplicity, we assume that the execution
time of forward, backward for input, and backward for weights
of each stage all equals to one unit of time, denoted by
Tf = Ti = Tw = 1, and that of a full backward chunk equals
Tb = Ti + Tw = 2. We formulated the optimization problem
using Dualpipe’s memory bound, which allocates memory on
each device equal to 2× model parameters plus (PP + 1)
activation memory. This specific memory allocation supports
Dualpipe’s characteristic forward-backward scheduling while
maintaining activation checkpoints. Additionally, we set a
known primal bound on overall makespan of the sum of
execution time of all stages plus the bubble size, which equals
to (PP/2 −1) × (Tf + 2Tb −3Tw) = 2(PP/2 −1), forcing
the solving process of the MIP to terminate when reaching the
same performance.
Figure 3 presents Gantt charts of our optimizer’s solutions
across three different pipeline parallelism configurations (PP
= 2, 4, and 8). The horizontal axis represents time, while each
row shows operations scheduled on a single device. Colored
blocks indicate computation, and the gaps between operations
represent pipeline bubbles—idle periods where devices wait
due to dependencies. Evidently, our framework successfully
reproduces Dualpipe’s parallelization strategy across all con-
figurations. The computational boundaries between pipeline
stages and the resulting bubble patterns closely match those of
the manually designed Dualpipe approach. The exact schedul-
ing of forward and backward passes slightly differ from that
of Dualpipe, which is expected because both solutions are
equivalent from the perspective of the MIP. Interestingly,
the schedules manifest a bi-directional interweaving pattern
similar to Dualpipe without explicit arrangement. This result
demonstrates our framework’s ability to automatically discover


--- Page 7 ---
7
0
2
4
6
8
10
12
14
Time
0
1
Machine
0,0
1,0
1,0
1,0
0,0
0,0
0,1
1,1
1,1
1,1
0,1
0,1
0,2
1,2
1,2
1,2
0,2
0,2
0,3
1,3
1,3
1,3
0,3
0,3
0,4
1,4
1,4
1,4
0,4
0,4
(a) PP-rank = 2
0
5
10
15
20
25
30
Time
0
1
2
3
Machine
0,0
1,0
2,0
3,0
3,0
3,0
2,0
2,0
1,0
1,0
0,0
0,0
0,1
1,1
2,1
3,1
3,1
3,1
2,1
2,1
1,1
1,1
0,1
0,1
0,2
1,2
2,2
3,2
3,2
3,2
2,2
2,2
1,2
1,2
0,2
0,2
0,3
1,3
2,3
3,3
3,3
3,3
2,3
2,3
1,3
1,3
0,3
0,3
0,4
1,4
2,4
3,4
3,4
3,4
2,4
2,4
1,4
1,4
0,4
0,4
0,5
1,5
2,5
3,5
3,5
3,5
2,5
2,5
1,5
1,5
0,5
0,5
0,6
1,6
2,6
3,6
3,6
3,6
2,6
2,6
1,6
1,6
0,6
0,6
0,7
1,7
2,7
3,7
3,7
3,7
2,7
2,7
1,7
1,7
0,7
0,7
0,8
1,8
2,8
3,8
3,8
3,8
2,8
2,8
1,8
1,8
0,8
0,8
0,9
1,9
2,9
3,9
3,9
3,9
2,9
2,9
1,9
1,9
0,9
0,9
(b) PP-rank = 4
0
10
20
30
40
50
60
Time
0
1
2
3
4
5
6
7
Machine
0,0
1,0
2,0
3,0
4,0
5,0
6,0
7,0
7,0
7,0
6,0
6,0
5,0
5,0
4,0
4,0
3,0
3,0
2,0
2,0
1,0
1,0
0,0
0,0
0,1
1,1
2,1
3,1
4,1
5,1
6,1
7,1
7,1
7,1
6,1
6,1
5,1
5,1
4,1
4,1
3,1
3,1
2,1
2,1
1,1
1,1
0,1
0,1
0,2
1,2
2,2
3,2
4,2
5,2
6,2
7,2
7,2
7,2
6,2
6,2
5,2
5,2
4,2
4,2
3,2
3,2
2,2
2,2
1,2
1,2
0,2
0,2
0,3
1,3
2,3
3,3
4,3
5,3
6,3
7,3
7,3
7,3
6,3
6,3
5,3
5,3
4,3
4,3
3,3
3,3
2,3
2,3
1,3
1,3
0,3
0,3
0,4
1,4
2,4
3,4
4,4
5,4
6,4
7,4
7,4
7,4
6,4
6,4
5,4
5,4
4,4
4,4
3,4
3,4
2,4
2,4
1,4
1,4
0,4
0,4
0,5
1,5
2,5
3,5
4,5
5,5
6,5
7,5
7,5
7,5
6,5
6,5
5,5
5,5
4,5
4,5
3,5
3,5
2,5
2,5
1,5
1,5
0,5
0,5
0,6
1,6
2,6
3,6
4,6
5,6
6,6
7,6
7,6
7,6
6,6
6,6
5,6
5,6
4,6
4,6
3,6
3,6
2,6
2,6
1,6
1,6
0,6
0,6
0,7
1,7
2,7
3,7
4,7
5,7
6,7
7,7
7,7
7,7
6,7
6,7
5,7
5,7
4,7
4,7
3,7
3,7
2,7
2,7
1,7
1,7
0,7
0,7
0,8
1,8
2,8
3,8
4,8
5,8
6,8
7,8
7,8
7,8
6,8
6,8
5,8
5,8
4,8
4,8
3,8
3,8
2,8
2,8
1,8
1,8
0,8
0,8
0,9
1,9
2,9
3,9
4,9
5,9
6,9
7,9
7,9
7,9
6,9
6,9
5,9
5,9
4,9
4,9
3,9
3,9
2,9
2,9
1,9
1,9
0,9
0,9
0,10
1,10
2,10
3,10
4,10
5,10
6,10
7,10
7,10
7,10
6,10
6,10
5,10
5,10
4,10
4,10
3,10
3,10
2,10
2,10
1,10
1,10
0,10
0,10
0,11
1,11
2,11
3,11
4,11
5,11
6,11
7,11
7,11
7,11
6,11
6,11
5,11
5,11
4,11
4,11
3,11
3,11
2,11
2,11
1,11
1,11
0,11
0,11
0,12
1,12
2,12
3,12
4,12
5,12
6,12
7,12
7,12
7,12
6,12
6,12
5,12
5,12
4,12
4,12
3,12
3,12
2,12
2,12
1,12
1,12
0,12
0,12
0,13
1,13
2,13
3,13
4,13
5,13
6,13
7,13
7,13
7,13
6,13
6,13
5,13
5,13
4,13
4,13
3,13
3,13
2,13
2,13
1,13
1,13
0,13
0,13
0,14
1,14
2,14
3,14
4,14
5,14
6,14
7,14
7,14
7,14
6,14
6,14
5,14
5,14
4,14
4,14
3,14
3,14
2,14
2,14
1,14
1,14
0,14
0,14
0,15
1,15
2,15
3,15
4,15
5,15
6,15
7,15
7,15
7,15
6,15
6,15
5,15
5,15
4,15
4,15
3,15
3,15
2,15
2,15
1,15
1,15
0,15
0,15
0,16
1,16
2,16
3,16
4,16
5,16
6,16
7,16
7,16
7,16
6,16
6,16
5,16
5,16
4,16
4,16
3,16
3,16
2,16
2,16
1,16
1,16
0,16
0,16
0,17
1,17
2,17
3,17
4,17
5,17
6,17
7,17
7,17
7,17
6,17
6,17
5,17
5,17
4,17
4,17
3,17
3,17
2,17
2,17
1,17
1,17
0,17
0,17
0,18
1,18
2,18
3,18
4,18
5,18
6,18
7,18
7,18
7,18
6,18
6,18
5,18
5,18
4,18
4,18
3,18
3,18
2,18
2,18
1,18
1,18
0,18
0,18
0,19
1,19
2,19
3,19
4,19
5,19
6,19
7,19
7,19
7,19
6,19
6,19
5,19
5,19
4,19
4,19
3,19
3,19
2,19
2,19
1,19
1,19
0,19
0,19
Task Type
forward
backward_input
backward_weight
(c) PP-rank = 8
Fig. 3: Gantt charts of parallelization strategies automatically
generated by our framework under identical memory con-
straints as Dualpipe. Colored blocks represent computation
on each device, while white gaps indicate pipeline bubbles.
The solutions achieved a bubble ratio to Dualpipe’s scheduling
pattern, with similar pipeline bubble distribution.
sophisticated parallelization strategies through mathematical
optimization, potentially eliminating weeks of expert engineer-
ing effort.
Then, we relax the constraint on the primal bound and let the
MIP solver further search the solution space. Figure 4 presents
Gantt charts of the scheduling at convergence, reaching a lower
bubble size of (PP/2−1) across all pipeline depths, which is
half of that achieved by Dualpipe, leading to substantially im-
proved hardware utilization and reduced end-to-end execution
time.. It is worth noting that the new solution is found under
the same memory bound constraints and without any manual
adjustment, demonstrating the capability of the framework to
automatically discover potentially better scheduling.
While our framework can reproduce Dualpipe’s strategy
0
2
4
6
8
10
12
14
Time
0
1
Machine
0,0
1,0
1,0
1,0
0,0
0,0
0,1
1,1
1,1
1,1
0,1
0,1
0,2
1,2
1,2
1,2
0,2
0,2
0,3
1,3
1,3
1,3
0,3
0,3
0,4
1,4
1,4
1,4
0,4
0,4
(a) PP-rank = 2, relaxed memory
0
5
10
15
20
25
30
Time
0
1
2
3
Machine
0,0
1,0
2,0
3,0
3,0
3,0
2,0
2,0
1,0
1,0
0,0
0,0
0,1
1,1
2,1
3,1
3,1
3,1
2,1
2,1
1,1
1,1
0,1
0,1
0,2
1,2
2,2
3,2
3,2
3,2
2,2
2,2
1,2
1,2
0,2
0,2
0,3
1,3
2,3
3,3
3,3
3,3
2,3
2,3
1,3
1,3
0,3
0,3
0,4
1,4
2,4
3,4
3,4
3,4
2,4
2,4
1,4
1,4
0,4
0,4
0,5
1,5
2,5
3,5
3,5
3,5
2,5
2,5
1,5
1,5
0,5
0,5
0,6
1,6
2,6
3,6
3,6
3,6
2,6
2,6
1,6
1,6
0,6
0,6
0,7
1,7
2,7
3,7
3,7
3,7
2,7
2,7
1,7
1,7
0,7
0,7
0,8
1,8
2,8
3,8
3,8
3,8
2,8
2,8
1,8
1,8
0,8
0,8
0,9
1,9
2,9
3,9
3,9
3,9
2,9
2,9
1,9
1,9
0,9
0,9
(b) PP-rank = 4, relaxed memory
0
10
20
30
40
50
60
Time
0
1
2
3
4
5
6
7
Machine
0,0
1,0
2,0
3,0
4,0
5,0
6,0
7,0
7,0
7,0
6,0
6,0
5,0
5,0
4,0
4,0
3,0
3,0
2,0
2,0
1,0
1,0
0,0
0,0
0,1
1,1
2,1
3,1
4,1
5,1
6,1
7,1
7,1
7,1
6,1
6,1
5,1
5,1
4,1
4,1
3,1
3,1
2,1
2,1
1,1
1,1
0,1
0,1
0,2
1,2
2,2
3,2
4,2
5,2
6,2
7,2
7,2
7,2
6,2
6,2
5,2
5,2
4,2
4,2
3,2
3,2
2,2
2,2
1,2
1,2
0,2
0,2
0,3
1,3
2,3
3,3
4,3
5,3
6,3
7,3
7,3
7,3
6,3
6,3
5,3
5,3
4,3
4,3
3,3
3,3
2,3
2,3
1,3
1,3
0,3
0,3
0,4
1,4
2,4
3,4
4,4
5,4
6,4
7,4
7,4
7,4
6,4
6,4
5,4
5,4
4,4
4,4
3,4
3,4
2,4
2,4
1,4
1,4
0,4
0,4
0,5
1,5
2,5
3,5
4,5
5,5
6,5
7,5
7,5
7,5
6,5
6,5
5,5
5,5
4,5
4,5
3,5
3,5
2,5
2,5
1,5
1,5
0,5
0,5
0,6
1,6
2,6
3,6
4,6
5,6
6,6
7,6
7,6
7,6
6,6
6,6
5,6
5,6
4,6
4,6
3,6
3,6
2,6
2,6
1,6
1,6
0,6
0,6
0,7
1,7
2,7
3,7
4,7
5,7
6,7
7,7
7,7
7,7
6,7
6,7
5,7
5,7
4,7
4,7
3,7
3,7
2,7
2,7
1,7
1,7
0,7
0,7
0,8
1,8
2,8
3,8
4,8
5,8
6,8
7,8
7,8
7,8
6,8
6,8
5,8
5,8
4,8
4,8
3,8
3,8
2,8
2,8
1,8
1,8
0,8
0,8
0,9
1,9
2,9
3,9
4,9
5,9
6,9
7,9
7,9
7,9
6,9
6,9
5,9
5,9
4,9
4,9
3,9
3,9
2,9
2,9
1,9
1,9
0,9
0,9
0,10
1,10
2,10
3,10
4,10
5,10
6,10
7,10
7,10
7,10
6,10
6,10
5,10
5,10
4,10
4,10
3,10
3,10
2,10
2,10
1,10
1,10
0,10
0,10
0,11
1,11
2,11
3,11
4,11
5,11
6,11
7,11
7,11
7,11
6,11
6,11
5,11
5,11
4,11
4,11
3,11
3,11
2,11
2,11
1,11
1,11
0,11
0,11
0,12
1,12
2,12
3,12
4,12
5,12
6,12
7,12
7,12
7,12
6,12
6,12
5,12
5,12
4,12
4,12
3,12
3,12
2,12
2,12
1,12
1,12
0,12
0,12
0,13
1,13
2,13
3,13
4,13
5,13
6,13
7,13
7,13
7,13
6,13
6,13
5,13
5,13
4,13
4,13
3,13
3,13
2,13
2,13
1,13
1,13
0,13
0,13
0,14
1,14
2,14
3,14
4,14
5,14
6,14
7,14
7,14
7,14
6,14
6,14
5,14
5,14
4,14
4,14
3,14
3,14
2,14
2,14
1,14
1,14
0,14
0,14
0,15
1,15
2,15
3,15
4,15
5,15
6,15
7,15
7,15
7,15
6,15
6,15
5,15
5,15
4,15
4,15
3,15
3,15
2,15
2,15
1,15
1,15
0,15
0,15
0,16
1,16
2,16
3,16
4,16
5,16
6,16
7,16
7,16
7,16
6,16
6,16
5,16
5,16
4,16
4,16
3,16
3,16
2,16
2,16
1,16
1,16
0,16
0,16
0,17
1,17
2,17
3,17
4,17
5,17
6,17
7,17
7,17
7,17
6,17
6,17
5,17
5,17
4,17
4,17
3,17
3,17
2,17
2,17
1,17
1,17
0,17
0,17
0,18
1,18
2,18
3,18
4,18
5,18
6,18
7,18
7,18
7,18
6,18
6,18
5,18
5,18
4,18
4,18
3,18
3,18
2,18
2,18
1,18
1,18
0,18
0,18
0,19
1,19
2,19
3,19
4,19
5,19
6,19
7,19
7,19
7,19
6,19
6,19
5,19
5,19
4,19
4,19
3,19
3,19
2,19
2,19
1,19
1,19
0,19
0,19
Task Type
forward
backward_input
backward_weight
(c) PP-rank = 8, relaxed memory
Fig. 4: Gantt charts of enhanced parallelization strategies by
continuing searching after reaching the utility by Dualpipe.
The optimization framework discovers more efficient schedul-
ing patterns, reducing pipeline bubbles by approximately 50%
compared to the strict Dualpipe memory configuration.
under identical constraints, we also investigated whether re-
laxing certain constraints could yield improved efficiency. We
hypothesized that Dualpipe’s strict memory bound might be
unnecessarily conservative for certain deployment scenarios.
Knowing that the model parameter requires dominant mem-
ory comparing to activation in most situations (except for
extremely long sequence generation), we modified our opti-
mization model to maintain the parameter memory allocation
(2× parameters) but relaxed the activation memory constraint
by doubling the allowed activation memory from (PP + 1)
to 2(PP + 1), representing a modest increase that remains
practical for modern accelerator hardware. Figure 5 shows the
resulting Gantt charts with these relaxed memory constraints
across the same three pipeline configurations. Surprisingly, the
new schedule form a tighter and regular pattern, but achieving


--- Page 8 ---
8
0
2
4
6
8
10
12
14
Time
0
1
Machine
0,0
1,0
1,0
1,0
0,0
0,0
0,1
1,1
1,1
1,1
0,1
0,1
0,2
1,2
1,2
1,2
0,2
0,2
0,3
1,3
1,3
1,3
0,3
0,3
0,4
1,4
1,4
1,4
0,4
0,4
(a) PP-rank = 2
0
5
10
15
20
25
30
Time
0
1
2
3
Machine
0,0
1,0
2,0
3,0
3,0
3,0
2,0
2,0
1,0
1,0
0,0
0,0
0,1
1,1
2,1
3,1
3,1
3,1
2,1
2,1
1,1
1,1
0,1
0,1
0,2
1,2
2,2
3,2
3,2
3,2
2,2
2,2
1,2
1,2
0,2
0,2
0,3
1,3
2,3
3,3
3,3
3,3
2,3
2,3
1,3
1,3
0,3
0,3
0,4
1,4
2,4
3,4
3,4
3,4
2,4
2,4
1,4
1,4
0,4
0,4
0,5
1,5
2,5
3,5
3,5
3,5
2,5
2,5
1,5
1,5
0,5
0,5
0,6
1,6
2,6
3,6
3,6
3,6
2,6
2,6
1,6
1,6
0,6
0,6
0,7
1,7
2,7
3,7
3,7
3,7
2,7
2,7
1,7
1,7
0,7
0,7
0,8
1,8
2,8
3,8
3,8
3,8
2,8
2,8
1,8
1,8
0,8
0,8
0,9
1,9
2,9
3,9
3,9
3,9
2,9
2,9
1,9
1,9
0,9
0,9
(b) PP-rank = 4
0
10
20
30
40
50
60
Time
0
1
2
3
4
5
6
7
Machine
0,0
1,0
2,0
3,0
4,0
5,0
6,0
7,0
7,0
7,0
6,0
6,0
5,0
5,0
4,0
4,0
3,0
3,0
2,0
2,0
1,0
1,0
0,0
0,0
0,1
1,1
2,1
3,1
4,1
5,1
6,1
7,1
7,1
7,1
6,1
6,1
5,1
5,1
4,1
4,1
3,1
3,1
2,1
2,1
1,1
1,1
0,1
0,1
0,2
1,2
2,2
3,2
4,2
5,2
6,2
7,2
7,2
7,2
6,2
6,2
5,2
5,2
4,2
4,2
3,2
3,2
2,2
2,2
1,2
1,2
0,2
0,2
0,3
1,3
2,3
3,3
4,3
5,3
6,3
7,3
7,3
7,3
6,3
6,3
5,3
5,3
4,3
4,3
3,3
3,3
2,3
2,3
1,3
1,3
0,3
0,3
0,4
1,4
2,4
3,4
4,4
5,4
6,4
7,4
7,4
7,4
6,4
6,4
5,4
5,4
4,4
4,4
3,4
3,4
2,4
2,4
1,4
1,4
0,4
0,4
0,5
1,5
2,5
3,5
4,5
5,5
6,5
7,5
7,5
7,5
6,5
6,5
5,5
5,5
4,5
4,5
3,5
3,5
2,5
2,5
1,5
1,5
0,5
0,5
0,6
1,6
2,6
3,6
4,6
5,6
6,6
7,6
7,6
7,6
6,6
6,6
5,6
5,6
4,6
4,6
3,6
3,6
2,6
2,6
1,6
1,6
0,6
0,6
0,7
1,7
2,7
3,7
4,7
5,7
6,7
7,7
7,7
7,7
6,7
6,7
5,7
5,7
4,7
4,7
3,7
3,7
2,7
2,7
1,7
1,7
0,7
0,7
0,8
1,8
2,8
3,8
4,8
5,8
6,8
7,8
7,8
7,8
6,8
6,8
5,8
5,8
4,8
4,8
3,8
3,8
2,8
2,8
1,8
1,8
0,8
0,8
0,9
1,9
2,9
3,9
4,9
5,9
6,9
7,9
7,9
7,9
6,9
6,9
5,9
5,9
4,9
4,9
3,9
3,9
2,9
2,9
1,9
1,9
0,9
0,9
0,10
1,10
2,10
3,10
4,10
5,10
6,10
7,10
7,10
7,10
6,10
6,10
5,10
5,10
4,10
4,10
3,10
3,10
2,10
2,10
1,10
1,10
0,10
0,10
0,11
1,11
2,11
3,11
4,11
5,11
6,11
7,11
7,11
7,11
6,11
6,11
5,11
5,11
4,11
4,11
3,11
3,11
2,11
2,11
1,11
1,11
0,11
0,11
0,12
1,12
2,12
3,12
4,12
5,12
6,12
7,12
7,12
7,12
6,12
6,12
5,12
5,12
4,12
4,12
3,12
3,12
2,12
2,12
1,12
1,12
0,12
0,12
0,13
1,13
2,13
3,13
4,13
5,13
6,13
7,13
7,13
7,13
6,13
6,13
5,13
5,13
4,13
4,13
3,13
3,13
2,13
2,13
1,13
1,13
0,13
0,13
0,14
1,14
2,14
3,14
4,14
5,14
6,14
7,14
7,14
7,14
6,14
6,14
5,14
5,14
4,14
4,14
3,14
3,14
2,14
2,14
1,14
1,14
0,14
0,14
0,15
1,15
2,15
3,15
4,15
5,15
6,15
7,15
7,15
7,15
6,15
6,15
5,15
5,15
4,15
4,15
3,15
3,15
2,15
2,15
1,15
1,15
0,15
0,15
0,16
1,16
2,16
3,16
4,16
5,16
6,16
7,16
7,16
7,16
6,16
6,16
5,16
5,16
4,16
4,16
3,16
3,16
2,16
2,16
1,16
1,16
0,16
0,16
0,17
1,17
2,17
3,17
4,17
5,17
6,17
7,17
7,17
7,17
6,17
6,17
5,17
5,17
4,17
4,17
3,17
3,17
2,17
2,17
1,17
1,17
0,17
0,17
0,18
1,18
2,18
3,18
4,18
5,18
6,18
7,18
7,18
7,18
6,18
6,18
5,18
5,18
4,18
4,18
3,18
3,18
2,18
2,18
1,18
1,18
0,18
0,18
0,19
1,19
2,19
3,19
4,19
5,19
6,19
7,19
7,19
7,19
6,19
6,19
5,19
5,19
4,19
4,19
3,19
3,19
2,19
2,19
1,19
1,19
0,19
0,19
Task Type
forward
backward_input
backward_weight
(c) PP-rank = 8
Fig. 5: Gantt charts of enhanced parallelization strategies
with relaxed activation memory constraints. The optimization
framework discovers more regular scheduling patterns, but did
not further improve bubble rate.
the same bubble size of (PP/2 −1) and did not improve
in efficiency with the relaxed memory bound. Such a result
indicates that the potential optimality of the bi-directional
interweaving pattern in balancing throughput and memory
efficiency.
B. Random Graph
We now demonstrate the capability of the proposed frame-
work to automatically parallelize a neural network with arbi-
trary structure. We generate a random computational graph
with 200 nodes, where the maximum in- and out-degrees
of nodes equal 3, representing of various types of neural
network operations with randomized compute and memory
requirements. We then optimize scheduling of the random
computational graph on a 2-machine cluster without memory
constraints. Figure 6 shows the coarsen graph of 40 nodes
yielded by Alg 1. Figure 7 presents the Gantt chart visual-
ization utilizing the interactive trace viewing tool available
at chrome://tracing. The horizontal axis represents time, while
the vertical axis shows the allocation of operations to different
processing units. The top two rows show the two chips
for computation, and the bottom two rows show the two
communication channels. The automatic partitioning strategy
generated by our optimizer perfectly balances computational
load across the available hardware while respecting all depen-
dency constraints inherent in the network structure. As evident
from the visualization, our approach minimizes idle time
between operations and achieves efficient pipeline parallelism,
demonstrating the framework’s ability to extract parallelization
opportunities even in arbitrary graph structures without prede-
fined patterns. Notably, the optimizer successfully determined
the optimal sequence of dependent operations such that the
communication time is fully hidden under computation, in
so doing minimizing bubbles on the computation machines
and minimizing total execution time. The resulting schedule
achieves the theoretical maximum speedup of 2× for this two-
chip configuration.
Fig. 6: Visualization of the randomly generated computation
graph coarsen from 200 nodes down to 40 nodes.
Fig. 7: Gantt chart of an optimized network inference schedule
on 2 chips
VI. CONCLUSION AND FUTURE WORK
This work introduces a novel mathematical programming
framework that formalizes the neural network parallelization
problem, providing a principled approach to automatically
generate efficient distributed computation strategies. Our pri-
mary contribution lies in developing a comprehensive mathe-
matical formulation that captures the necessary and sufficient
conditions for valid parallelization plans across diverse neural
network architectures and hardware configurations.


--- Page 9 ---
9
The mathematical program we have developed represents
a significant advancement over existing approaches in sev-
eral key dimensions. First, it offers a general formulation
that accommodates arbitrary computational graphs, moving
beyond the limitations of techniques designed for specific
network architectures. Second, it provides a complete model
that simultaneously addresses multiple aspects of paralleliza-
tion—including operation placement, scheduling, memory
management, and communication—within a unified optimiza-
tion framework. Third, it establishes an expandable foundation
that can be readily extended to incorporate additional con-
straints, objectives, or parallelization dimensions as required
by specific deployment scenarios.
We have demonstrated the versatility of our framework
through several variants that address practical deployment con-
siderations. The memory-bounded formulation enables effi-
cient parallelization while respecting hardware memory limita-
tions, a critical constraint for large-scale models. The dynamic
loading/unloading extension optimizes activation memory us-
age through strategic recomputation, allowing for more effi-
cient execution of memory-intensive networks. Each variant
maintains the mathematical rigor of the core formulation while
adapting to specific operational requirements.
Our empirical evaluation confirms the effectiveness of the
proposed approach across different scenarios. When applied
to randomly generated computational graphs, our framework
successfully identifies efficient parallelization strategies that
balance computational load while minimizing communica-
tion overhead. This result highlights the advantage of our
mathematical programming approach over heuristic methods,
as it can discover non-obvious parallelization strategies that
might be difficult to identify manually, particularly for com-
plex and irregular network architectures where intuition-based
approaches often falter.
More remarkably, our comparison with Dualpipe—an
expert-designed
parallelization
strategy
implemented
in
DeepSeek V3—demonstrates both the validation and innova-
tion capabilities of our approach. The framework successfully
reproduces Dualpipe’s sophisticated scheduling patterns when
given identical constraints, validating its ability to automati-
cally derive strategies that previously required extensive expert
engineering effort. Furthermore, by systematically exploring
relaxations to memory constraints, our optimizer discovers
enhanced parallelization strategies that substantially reduce
pipeline bubbles and improve hardware utilization.
The enhanced parallelization strategies discovered by our
framework demonstrate the value of automated optimization
approaches. While expert-designed strategies like Dualpipe
incorporate important domain knowledge, they often reflect
specific design constraints that may not be optimal across
all deployment scenarios. Our mathematical programming
framework enables systematic exploration of this design space,
automatically discovering improved parallelization strategies
tailored to specific hardware configurations and application
requirements.
This work bridges the gap between theoretical optimization
and practical deployment of distributed neural networks. By
formalizing parallelization as a mathematical program, we
enable both researchers and practitioners to reason systemati-
cally about trade-offs in the design space, identify efficiency
opportunities that may be missed by heuristic approaches, and
automatically generate optimized execution plans for diverse
neural network architectures.
Looking forward, the expandable nature of our framework
provides a foundation for future research directions. Potential
extensions include incorporating heterogeneous hardware con-
siderations, dynamic load balancing for unpredictable work-
loads, and automated exploration of hybrid parallelization
strategies that combine different dimensions of parallelism. As
neural networks continue to grow in scale and complexity, the
principled optimization approach presented in this work offers
a promising path toward efficient and automated deployment
across increasingly diverse computational environments.
REFERENCES
[1] DeepSeek-AI,
“Deepseek-v3
technical
report,”
2024.
[Online].
Available: https://arxiv.org/abs/2412.19437
[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in
Neural Information Processing Systems, vol. 30, 2017.
[3] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-
Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu,
C. Winter et al., “Language models are few-shot learners,” Advances in
Neural Information Processing Systems, vol. 33, pp. 1877–1901, 2020.
[4] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,
P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi,
S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer,
V. Prabhakaran et al., “Palm: Scaling language modeling with path-
ways,” arXiv preprint arXiv:2204.02311, 2022.
[5] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez,
A. Joulin, E. Grave, and G. Lample, “Llama: Open and efficient
foundation language models,” arXiv preprint arXiv:2302.13971, 2023.
[6] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,
S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws for neural
language models,” arXiv preprint arXiv:2001.08361, 2020.
[7] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, “Zero: Memory
optimizations toward training trillion parameter models,” in Proceedings
of the International Conference for High Performance Computing,
Networking, Storage and Analysis, 2020, pp. 1–16.
[8] L. Zheng, Z. Jia, M. Zaharia, and A. Aiken, “Alpa: Automating inter- and
intra-operator parallelism for distributed deep learning,” in Proceedings
of the 16th USENIX Symposium on Operating Systems Design and
Implementation, 2022, pp. 559–578.
[9] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He, “Deepspeed: System
optimizations enable training deep learning models with over 100 billion
parameters,” in Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, 2020, pp. 3505–
3506.
[10] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun,
N. Shazeer, and Z. Chen, “Gshard: Scaling giant models with conditional
computation and automatic sharding,” arXiv preprint arXiv:2006.16668,
2020.
[11] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary,
V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro,
A. Phanishayee, and M. Zaharia, “Efficient large-scale language model
training on gpu clusters using megatron-lm,” in Proceedings of the
International Conference for High Performance Computing, Networking,
Storage and Analysis, 2021, pp. 1–15.
[12] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catan-
zaro, “Megatron-lm: Training multi-billion parameter language models
using model parallelism,” arXiv preprint arXiv:1909.08053, 2019.
[13] Z. Jia, O. Padon, J. Thomas, T. Warszawski, M. Zaharia, and A. Aiken,
“Exploring hidden dimensions in parallelizing convolutional neural
networks,” in International Conference on Machine Learning, 2018, pp.
2279–2288.


--- Page 10 ---
10
[14] X. Feng, S. Zhang, H. Zhang, L. Lan, J. Zhu, Z. Yu, Y. Liang, J. Gao,
D. Lin, and J. Zhou, “Galvatron: Efficient transformer training over
multiple gpus using automatic parallelism,” in Proceedings of the 29th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining,
2023, pp. 421–433.
[15] Y. Xu, H. Lee, D. Chen, C.-W. Blake, U. Alon, Y. Huang, V. P.
Srinivasasainagendra, J. Zhou, P. Y.-P. Zhou, C. Keeler et al., “Gspmd:
General and scalable parallelization for ml computation graphs,” in
Proceedings of the 5th Conference on Machine Learning and Systems,
2021.
[16] D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur,
G. R. Ganger, P. B. Gibbons, and M. Zaharia, “Memory-efficient
pipeline-parallel dnn training,” in Proceedings of the 38th International
Conference on Machine Learning, 2021, pp. 7937–7947.
[17] S. Li, Y. Zhao, R. Varma, O. Salpekar, P. Noordhuis, T. Li, A. Paszke,
J. Smith, B. Vaughan, P. Damania, and S. Chintala, “Pytorch distributed:
Experiences on accelerating data parallel training,” Proceedings of the
VLDB Endowment, vol. 13, no. 12, pp. 3005–3018, 2020.
[18] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. J. Lee,
J. Ngiam, Q. V. Le, Y. Wu, and Z. Chen, “Gpipe: Efficient training of
giant neural networks using pipeline parallelism,” in Advances in Neural
Information Processing Systems, vol. 32, 2019, pp. 103–112.
[19] S. Fan, Y. Rong, C. Meng, Z. Cao, S. Wang, Z. Zheng, C. Wu, G. Long,
J. Yang, L. Xia, L. Diao, X. Liu, and W. Lin, “Dapple: A pipelined
data parallel approach for training large models,” in Proceedings of the
26th ACM SIGPLAN Symposium on Principles and Practice of Parallel
Programming, 2021, pp. 431–445.
[20] D. Li, T. Xu, S. Zhou, S. Zhou, D. Niu, S. Yavuz, B. Li, B. Wang,
and X. Xie, “Terapipe: Token-level pipeline parallelism for training
large-scale language models,” in Proceedings of the 38th International
Conference on Machine Learning, 2021, pp. 6543–6552.
[21] M. Unger, H. Anzt, E. S. Quintana-Orti, and C. Yang, “Unity: Accelerat-
ing dnn training through joint optimization of algebraic transformations
and parallelization,” in Proceedings of the International Conference for
High Performance Computing, Networking, Storage and Analysis, 2023,
pp. 1–14.
[22] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to
trillion parameter models with simple and efficient sparsity,” Journal of
Machine Learning Research, vol. 23, no. 120, pp. 1–39, 2022.
[23] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever,
“Learning transferable visual models from natural language supervi-
sion,” in Proceedings of the 38th International Conference on Machine
Learning, 2021, pp. 8748–8763.
[24] Z. Jia, M. Zaharia, and A. Aiken, “Beyond data and model parallelism
for deep neural networks,” in Proceedings of the 2nd Conference on
Systems and Machine Learning (SysML), 2019.
[25] M. L. Pinedo, Scheduling: theory, algorithms, and systems.
Springer,
2012.
[26] B. Pang, K. Li, R. She, and F. Wang, “Hybrid offline-online scheduling
method for large language model inference optimization,” arXiv preprint
arXiv:2502.15763, 2025.
[27] Y. Pochet and L. A. Wolsey, Production planning by mixed integer
programming.
Springer, 2006.
[28] A. S. Manne, “On the job-shop scheduling problem,” Operations Re-
search, vol. 8, no. 2, pp. 219–223, 1960.
[29] W.-Y. Ku and J. C. Beck, “Mixed integer programming models for job
shop scheduling: A computational analysis,” Computers & Operations
Research, vol. 73, pp. 165–173, 2016.
[30] H. D. Sherali, “Reformulation-linearization technique for mips,” in Wiley
Encyclopedia of Operations Research and Management Science. Wiley,
2010.
