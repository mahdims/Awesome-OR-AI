--- Page 1 ---
1
Hybrid Offline-online Scheduling Method for Large
Language Model Inference Optimization
Bowen Pang, Kai Li, Ruifeng She, and Feifan Wang, Member, IEEE
Abstract—With the development of large language models
(LLMs), it has become increasingly important to optimize
hardware usage and improve throughput. In this paper, we
study the inference optimization of the serving system that
deploys LLMs. To optimize system throughput and maximize
hardware utilization, we formulate the inference optimization
problem as a mixed-integer programming (MIP) model and
propose a hybrid offline-online method as solution. The offline
method improves large-scale inference systems by introducing a
Minimizing Makespan Bin Packing Problem. We further provide
a theoretical lower bound computation method. Then, we propose
an online sorting and preemptive scheduling method to better
utilize hardware. In the online iteration scheduling process, a
Lagrangian method is applied to evaluate the cost efficiency
of inserting prefill stages versus decode stages at each iteration
and dynamically determine when to preempt decoding tasks and
insert prefill tasks. Experiments using real-world data from the
LLaMA-65B model and the GSM8K dataset demonstrate that
system utilization improves from 80.2% to 89.1%, and the total
inference time decreases from 201.00 to 190.58 seconds. A 100-
cases study shows that our method consistently outperforms the
baseline method and improves the utilization rate by 8.0% on
average. Finally, we discuss potential future extensions, including
stochastic modeling, reinforcement learning-based schedulers,
and dynamic decision-making strategies for system throughput
and hardware utilization.
Note to Practitioners—This work provides optimization tools
for enhancing the efficiency of LLM inference systems through
advanced scheduling techniques. From the perspective of LLM
inference service providers, improved hardware utilization can
reduce operational costs by requiring less hardware to maintain
the same level of service. From the user’s perspective, reduced
inference time translates to faster response times and improved
service quality. Furthermore, the proposed scheduling techniques
are adaptable to various LLM models, hardware platforms, and
datasets, making them highly scalable and broadly applicable to
real-world LLM inference scenarios.
Index Terms—Large Language Model, Inference System, On-
line Scheduling, Mixed-integer Programming.
I. INTRODUCTION
Recent advancements in large language models (LLMs),
including GPT-4, LLaMA, and Qwen, have significantly
transformed the landscape of natural language processing by
enabling more sophisticated text generation, comprehension,
and interaction capabilities. These models serve as founda-
tional technologies in a wide range of applications, such as
chatbots, machine translation, and content creation. Despite
their transformative potential, the inference process for LLMs
B. Pang, K. Li, and R. She are with Noah’s Ark Lab, Huawei.
F. Wang is with the Department of Industrial Engineering, Tsinghua
University, Beijing, 100084, China. E-mail: wangfeifan@tsinghua.edu.cn.
Computing Infrastructure
Serving Configurations
System resource & model Configurations
GPU computing ability 
Node size
Network topology
Memory
Network bandwidth
…
Layers
Head
Model type
DP/TP/PP
Service level
Quant
BS
Requests scheduling
Iteration scheduling
basis
Inference Service
Input requests
Offline requests scheduling
Prefill queue
Decode queue
Scheduling strategy
Online requests scheduling
Online iteration scheduling
KV cache
Multi-objectives
Service level
SJF
LLM workers
Batching
Profiler
Response tokens
Decode iteration
Information 
collecting
(optional)
Fig. 1: Illustration of LLM inference service system
is computationally intensive and resource-demanding, result-
ing in high costs and increased latency. These factors pose
significant challenges when scaling their deployment across
various applications. Effective inference scheduling is crucial
for optimizing resource usage, reducing operational costs, and
ensuring high-quality service delivery.
As illustrated in Fig. 1, the current inference service system
comprises three components: system resource, serving con-
figuration, and inference service. System resource determines
the extent of computational resources available in the system
including GPU computing ability, memory, node size, network
bandwidth, etc. The serving configuration specifies the desired
model deployment method and service level expectations, such
as data/tensor/pipeline parallel settings, quantization, batch
size, and scheduling configurations. The inference service
processes input requests from users, utilizes hardware capa-
bilities, fulfills user configurations, and returns responses to
LLM users. The scheduling strategy module is the core of the
inference service, managing prefill and decode queues, offline
and online request scheduling, and iteration-level hardware
management. The offline scheduling method is optional, only
for inference tasks where all requests are known in advance.
Conversely, the two online scheduling methods, i.e., requests
scheduling and iteration scheduling, are more versatile and
arXiv:2502.15763v1  [cs.DC]  14 Feb 2025


--- Page 2 ---
2
applicable to a wide range of scenarios. The online methods
acquire information from the online profiler and dispatch
requests and inference tasks to LLM workers. In practice
hardware utilization includes up to 20% “bubbles”, which
means the hardware is idle during inference service, and
detail is later shown in Section IV. By designing an efficient
LLM inference scheduling system to reduce these bubbles,
computational resource consumption can be decreased, leading
to reduced latency and increased throughput. For maintaining
logic flow, detailed technical introductions to prefill and de-
code operations are provided in Section III.
Current works, such as vLLM [1] and ORCA [2], provide
a robust foundation for LLM inference serving systems. The
primary contributions of vLLM and ORCA to LLM inference
are their enhancements in resource allocation and execution
efficiency, which significantly increase throughput and de-
crease latency in large-scale language model deployments.
These improvements are achieved through advanced memory
management and continuous batching techniques, which en-
hance model parallelism and effectively leverage hardware
resources [1], [2]. However, the state-of-the-art scheduler
in vLLM predominantly employs a First-Come-First-Serve
(FCFS) and prefill-first policy, prioritizing prefill tasks but
not fully leveraging the scheduling system’s potential. We
identify the causes of low hardware utilization rates. From an
offline scheduling perspective, client loads are unbalanced, and
request sequencing can be improved. From an online schedul-
ing angle, the prefill-first policy lacks parallel processing of
multiple requests at prefill stage, and there are no dynamic
sequencing adjustments or preemption methods.
The optimization of LLM inference presents two major
challenges. The first challenge pertains to the offline method,
which involves managing a large number of requests from up
to 200 parallel clients. This task is particularly time-consuming
when using a mixed-integer programming (MIP) scheduling
model. The second challenge is the need for much faster
decision-making for online methods compared to traditional
online scheduling problems. For example, in LLM inference
scenarios, online decisions about whether to send prefill or
decode requests to LLM workers typically occur every 50
milliseconds. In contrast, in healthcare or production systems,
online decisions usually occur every thirty seconds or even
several minutes. For online scheduling methods, the higher
frequency of decision-making requires algorithms that are
efficient and capable of delivering results within extremely
short time frames.
Accompanied by these challenges, developing a scheduling
method for LLM inference could yield substantial benefits.
According to NVIDIA Financial Results in 2024 [3], the
revenue from GPU retail and cloud computation service has
reached $60.9 billion per year. Improving computational re-
source efficiency by 5% will earn up to $3.05 billion in revenue
annually. Therefore, research in this area is crucial, especially
for scheduling researchers.
In this paper, we aim at enhance service system through-
put and optimize hardware utilization. We define the LLM
inference optimization problem using a MIP model as its
uncertainty equivalence. Then, we propose a hybrid offline-
online method as solution. This work is the first to define
this problem from a scheduling perspective. To address the
high real-time demands in LLM inference, we propose a
hybrid offline-online scheduling method. In numerical exper-
iments, we demonstrate that our method can improve system
throughput by 5.46% and improve hardware utilization by
11.0%. A 100-cases study shows that our method outperforms
baseline method consistently with an improvement of 8.0% on
utilization rate.
The major contributions of this work include the followings.
To the best of our knowledge, we are the first to formulate a
mathematical model that describe the scheduling problem in
the area of LLM inference. We design a two-stage approach
to manage both offline and online scheduling challenges. An
Minimizing Makespan Bin Packing model is developed to
efficiently solve the offline scheduling problem. Additionally,
we introduce a sorting and preemption method to handle
the online request scheduling problem, and we develop a
Lagrangian-based heuristic technique for solving the online
iteration scheduling issue. In the online scheduling module,
our method can provide decisions within 5 milliseconds, meet-
ing the real-time decision-making requirements of practical
application in LLM inference.
The remainder of the paper is structured as follows. We
review literature on efficient LLM inference in Section II.
The problem definition and model formulation are presented
in Section III. Then, we illustrate the difficulty of solving this
problem and introduce our hybrid offline-online scheduling
method to provide a timely solution in Section IV. Numerical
studies using real cases and 100 generated cases are presented
in Section V. Finally, the conclusion and future directions are
provided in Section VI.
II. LITERATURE REVIEW
This section provides an overview of existing research and
prevalent methodologies in inference optimization for LLMs.
Firstly, we introduce the general techniques commonly em-
ployed in model serving, which can be seamlessly integrated
with our scheduling strategies. Subsequently, we elucidate
several classical techniques widely adopted in LLM inference
systems, which constitute the cornerstone of our framework
and methodologies. In the following, we succinctly introduce
recent advancements in inference optimization. Finally, we
examine scheduling methods within the existing operations
research domain.
As LLM inference falls within the broader scope of model
serving, a variety of general inference optimization techniques
can be effectively utilized. Model compression is one of
the quintessential optimization strategies for reducing model
size, encompassing techniques such as quantization [11], spar-
sification [12], [13], and distillation [14]. In addition, the
design of more compact structures to replace the original
ones is also common. For instance, employing multi-query
attention [15] or grouped-query attention [16] in place of
the original multi-head attention in Transformer architecture
can reduce key-value heads, resulting in a more streamlined
model. Nevertheless, both model compression and the design


--- Page 3 ---
3
TABLE I: Literature Review
Work
Throughput
Latency
Cache
Management
Dynamic
Decision
Uncertainty
Request
Management
Iteration
Management
Batching
Distributed
Strategies
Orca
✓
✓
✓
✓
✓
vLLM
✓
✓
✓
Sarathi-Serve [4]
✓
✓
✓
✓
Llumnix [5]
✓
✓
✓
✓
✓
InfiniGen [6]
✓
✓
dLoRA [7]
✓
✓
✓
✓
✓
✓
VTC [8]
✓
✓
FastServe [9]
✓
✓
✓
✓
✓
✓
DistServe [10]
✓
✓
✓
✓
✓
✓
MoonCake
✓
✓
✓
✓
✓
OURS
✓
✓
✓
✓
✓
✓
✓
✓
of compact structures can alter model weights, potentially
leading to a decline in accuracy. Instead of optimizing the
model size, data parallelism (DP) and model parallelism aim
to fully leverage the computational power of the devices. In
DP [17], the model weights are replicated across multiple
devices, allowing different inference requests to be processed
in parallel on different devices. Model parallelism distributes
the model weights across several devices to minimize the per-
device memory footprint of the model weights. Consequently,
each device can operate more efficiently by executing a smaller
portion of the task. Several model parallelization methods
exist, such as pipeline parallelism (PP) [18], tensor parallelism
(TP) [19], sequence parallelism (SP) [20], and context paral-
lelism (CP) [21]. Since our scheduling methods are orthogonal
to the aforementioned techniques, both model optimization
and parallelization strategies can be employed seamlessly in
conjunction with our methods to enhance inference efficiency.
In addition to general model serving techniques, the opti-
mization of LLM inference serving systems primarily involves
enhancing the model forward pass. Researchers have improved
system efficiency from various perspectives, including kernel
fusion, Key-Value (KV) cache management, request manage-
ment, iteration management, batching, distributed strategies,
etc. Here, we present several classic techniques that are
prevalent in LLM inference serving systems. FlashAttention
[22] amalgamates the operations of data transfer between hard-
ware components within the attention mechanism to expedite
operation execution without compromising model accuracy.
Speculative decoding [23], [24] employs an auxiliary model to
generate a preliminary draft, followed by a verification process
executed by the main model. This technique enables the
serving system to output multiple tokens in a single forward
pass instead of one. Orca [2] pioneers continuous batching
by aggregating different requests at the iteration level. Rather
than awaiting the completion of an entire batch before starting
the execution of new requests, continuous batching allows new
requests to be inserted into the batch while other requests are
still in progress. Inspired by memory management strategies
in operating systems, vLLM [1] introduces PagedAttention,
wherein the attention key and value vectors are stored as
non-contiguous blocks in memory. Continuous batching and
PagedAttention significantly increase overall GPU memory
utilization during the execution of LLM. SarathiServe [4]
introduces chunked-prefills, also known as dynamic SplitFuse
or prefill-decode (PD) Fusion, batching together prefill and
decode chunks to maximize both computation and bandwidth
utilization. Since serving systems are commonly deployed
on distributed platforms, numerous strategies have been pro-
posed to exploit distributed characteristics. For example, recent
works [10], [25], [26] advocate for separating prefill servers
from decode servers, also known as PD Separation, due
to the distinct computational and bandwidth characteristics
of these two stages. For a comprehensive review of these
techniques, we recommend [27] for further reference. These
classic techniques form the foundation of inference services
and our methods.
Recently, with ongoing advancements in AI system re-
search, numerous innovative inference techniques have been
developed, particularly those related to schedulers. We present
some representative works and highlight the differences be-
tween these approaches and our method in TABLE I. Inspired
by context switching across CPU cores, Llumnix [5] proposes
a live migration mechanism and a dynamic scheduling policy
to reschedule requests across multiple model instances of LLM
deployed on GPU clusters, thereby enhancing load balancing
and isolation. InfiniGen [6] addresses the challenge of large
KV cache sizes for long-text generation by speculating and
prefetching critical KV cache entries. For LoRA models,
dLoRA [7] addresses the challenges of serving multiple LoRA
models by dynamically merging and unmerging adapters with
the base model and migrating requests and adapters between
replicas. Sheng et al. [8] study the fairness problem in LLM
serving concerning clients and proposes a novel scheduling
algorithm called the virtual token counter (VTC). FastServe [9]
proposed an innovative skip-join MLFQ scheduler to enable
preemption during the autoregression process of LLM infer-
ence. In distributed systems, DistServe [10] tackles the issues
of PD interference and resource coupling by disaggregating
prefill and decoding computation, and proposes placement
algorithms to optimize resource allocation and parallelism
strategies for different phases. Mooncake [28] also proposes
a disaggregated architecture that separates the prefill and
decode clusters and utilizes a KV cache-centric scheduler to
manage the cache flow. While these innovative techniques
attempt to address the issues faced by the scheduler to some
extent, they scarcely model the scheduling problem formally


--- Page 4 ---
4
Client 1
Client 2
Bin 1
Bin 2
Bin 3
Prefill
stage
Decode
stage
Prefill
stage
Decode
stage
Decode
stage
Prefill
stage
Request
Request
Request
Request
Request
Fig. 2: Illustration of LLM inference
and are limited to solve the inference optimization problem
theoretically.
In the domain of operations research, scheduling is a well-
established and extensively utilized approach. For instance, of-
fline scheduling methods are applied in manufacturing systems
[29], healthcare systems [30], and operations management
systems [31]. To address real-time decision-making or multi-
stage stochastic scenarios, online scheduling methods have
been introduced in these systems [32], [33]. Nevertheless,
none of the systems we examined achieve the rapid decision
frequency—down to 10 milliseconds—that is observed in
LLM inference systems. Furthermore, traditional stochastic
programming models are not suitable for sequential decision-
making problems where real-time information is continuously
revealed. Therefore, it is imperative to develop and tailor
traditional methods for application in this emergent domain of
LLM inference. Research on LLM inference optimization can
not only enhance hardware resource utilization in the LLM
domain but also expand the repertoire available to existing
operations research algorithms.
III. PROBLEM FORMULATION
A. Background of LLM inference techniques
High efficiency and low latency are critical in LLM infer-
ence, and they depend not only on performance of hardware,
such as GPU, but also on how well the hardware is used.
As pushing the limit of hardware computing power is costly
and faced with slow progress, improved inference scheduling
becomes a promising means of achieving the same outcome.
Three factors are involved when designing inference schedul-
ing methods: PD management approaches, cache management,
and batching strategy.
LLM inference alternately goes through two stages, i.e.,
prefill and decode stages. In the prefill stage, initial input to-
kens are processed, and the LLM model’s internal states, such
as hidden states, are prepared. In the decode stage, the LLM
model generates output tokens based on the context established
during the prefill stage. The time lengths for prefill stage and
decode stage are uncertain. The alternating process continues
in parallel, until either the end of the sequence is reached
or a predefined stopping criterion is satisfied. The process of
LLM inference is illustrated in Fig. 2, where requests, each
with a prefill phase and a decode phase, are sent to clients
and processed in parallel. The whole process is divided into
multiple bins, and each bin consists of a prefill stage and a
prefill
decode
makespan
…
Client 1
Client 2
Client 3
Client 4
Client 200
…
Up Stream: Clients 
sending batched 
P/D requests to 
GPU node
Down Stream: 
GPU node 
returning results 
processed by 
LLM
GPU Node
GPU 1 GPU 2 GPU 3 GPU 4
GPU 8
GPU 5 GPU 6 GPU 7
Fig. 3: Illustration of PD Competition
decode stage. Requests in prefill phase can be served only
when the process is in prefill stage, and the same requirement
is applied to the decode phase and stage. A client may be idle
as either the prefill phase or decode phase is completed but
the process still stays in the same stage, causing compromised
utilization of computing resources. A single LLM inference
in practice typically contains a large number of requests
processed by parallel clients. Thus, there is great potential
in better utilizing computing resources in LLM inference, but
the scheduling problem has a high complexity and, as a real-
time decision making in milliseconds, the computing time is
limited. The most commonly used approaches to managing
prefill are provided below.
• PD Competition: As illustrated in Fig. 3, in this approach,
either prefill or decode stage is processed at any given
time for all clients on a single hardware node (typically
consisting of 8 GPUs). PD Competition allows decode
stage to be preempted by the prefill stage to enhance
hardware utilization.
• PD Fusion: This approach integrates the prefill and de-
code stages into a single cohesive operation, aimed at
reducing overhead and enhancing throughput by stream-
lining the inference pipeline. This approach also attempts
to decrease latency through alignment of processes. How-
ever, this integration compromises flexibility, restricting
the ability to independently optimize each process or
tailor responses to varying workload demands.
• PD Separation: This approach separates the prefill and
decode stages across exclusive sets of GPUs. However,
it introduces additional communication or coordination
overhead, which increases latency if not properly man-
aged.
As a widely used approach, PD Competition has a high
flexibility in effectively utilizing computing resources. Such
an approach also allows an inference scheduling method to fit
in and further enhance its performance. As is aforementioned,


--- Page 5 ---
5
inference scheduling for LLM inference is challenging. This
study focuses on the inference scheduling under the PD
Competition approach.
The second factor that influences the efficiency of LLM
inference is cache management. The KV cache is instrumental
during the decoding stage by storing intermediate hidden states
from preceding token generation steps. It allows the LLM
model to reuse states, significantly accelerating the inference
process. Despite its advantages, the KV cache requires to be
properly managed. First, the KV Cache size increases with the
length of input and output sequences and the number of LLM
model layers. This cache size growth results in significant
memory consumption, especially in the context of large-sized
models and extended sequences. Effective KV cache man-
agement avoids memory overflow and sustains high inference
speed. Cache management may involve caching only the most
relevant hidden states, while discarding or compressing less
critical information to optimize resource use. Second, concur-
rency issues should be addressed. The complexity of manag-
ing the KV cache escalates with concurrent inference tasks.
Ensuring consistent and conflict-free partitioning and access
to the cache for each task is important for performance and
accuracy of LLM inference. Besides, although the KV cache
alleviates computational load during decoding, it introduces
cache access and management overheads. Cache management
requires taking into account overall latency. In this study,
we proactively compute the KV cache and determine the
optimal maximum number of parallel requests, equivalent to
the number of clients handled in subsequent stages. Thus, we
assume in the inference scheduling problem shown in Fig. 2
that the total number of clients is predetermined.
The third factor that influences LLM inference efficiency is
batching strategy. Continuous batching, which is introduced by
ORCA [2], has emerged as an innovative technique to enhance
the inference efficiency by dynamically aggregating incom-
ing requests in real time. Unlike traditional static batching,
which waits for a fixed number of requests before processing,
continuous batching minimizes latency by reducing idle times
between batch executions and adapting to fluctuating work-
loads. It ensures efficient use of computational resources. By
maximizing parallel processing capabilities and aligning with
the model architecture’s latency and throughput trade-offs,
continuous batching significantly enhances the scalability and
responsiveness of LLM deployments. Using the continuous
batching technique, decoding is allowed to be preempted by
the prefill stage. Such a case is shown by the first request of
client 2 in Fig. 2, where the decode phase is separated by the
prefill stage of bin 2 and is assigned to both the decode stages
of bin 1 and 2. In this work, the decision-making process
allows decoding to be preempted by the prefill stage, facilitated
by the continuous batching technique.
B. Inference scheduling problem description
Inference scheduling aims to schedule inference requests
using continuous batching and PD Competition, with the
goal of minimizing the total inference time while adhering
to operational constraints. The problem settings are given as
follows and related notations are presented in TABLE II.
1) Requests and processing time:
• Let I = {1, 2, · · · } be the set of inference requests.
Inference request i, for i ∈I, has a fixed input token
number N p
i ∈Z+ for prefill phase and output token
number N d
i ∈Z+ for decode phase. The input token
number is assumed to be known, but the output token
number is unknown.
• Each request has a known prefilling time, linearly
related to the total number of input tokens in a bin.
• The decoding time is approximately linearly related
to the output token number. Let J = {1, 2, · · · } be
the set of clients and T d be the decode time per token.
The minimal unit of decoding time is equal to the
amount of time that each client processes a single
token, i.e., T d|J |.
2) Bin and batch size:
• Let K = {1, 2, · · · } be the set of bins. The inference
process is divided into a total of K = |K| bins.
• The batch size |J | represents the maximum number
of requests that can be processed simultaneously in
a batch.
3) Stages:
• There are two stages in hardware operation system:
prefill and decode. Prefill and decode stages must
alternate, ensuring that each stage is dedicated ex-
clusively to one type of operation.
• At any given time, the system can perform either
prefill or decode stages, but not both.
• At any time as the system starts a decode stage, the
length of this operation is determined. Meanwhile,
all requests processed at this stage will be sched-
uled. The decision is made based on system state,
including information of the duration of the current
bin, type of requests being processed, and the set of
remaining requests. It is illustrated in Fig. 4.
4) Assignment and allocation:
• Each request must be assigned to exactly one prefill
stage for processing.
• For every request, the prefill phase must be com-
pleted by the same client that will subsequently carry
out its decode phase. Hence, both phases must be
allocated to the same client.
• A client can process only one request at a time. Once
a client begins processing a request, it remains occu-
pied and cannot preempt tasks until both the prefill
and decode stages of that request are completed.
C. Deterministic equivalence
In this hybrid offline-online problem, the offline decision
component involves determining the assignment and sequence
of given requests on clients. As a sequential decision-making
problem, the online decision component focuses on determin-
ing the length of each bin and the sequence of remaining
requests in the future, with the aim of minimizing the total
inference time.


--- Page 6 ---
6
TABLE II: Notation Table: Sets, Parameters and Decision Variables
Sets
Description
I = {1, 2, · · · }
Set of requests.
J = {1, 2, · · · }
Set of clients.
K, Kp, Kd = {1, 2, · · · }
Set of bins, prefill stages, and decode stages, respectively. K = Kp = Kd.
L = {1, 2, · · · }
Set of all possible levels for the prefill stage.
Parameters
I =| I |
Total number of requests.
J =| J |
Total number of clients.
K =| K |
Total number of bins.
Np
i ∈Z+
Input token number of request i, for i ∈I.
Nd
i ∈Z+
Output token number of request i, for i ∈I.
Ncap
l
∈Z+
Maximum token capacity of level l, for l ∈L.
T d ∈R+
Decode time per token.
T p ∈R+
Prefill time per token.
T p
l ∈R+
Prefill time of level l, for l ∈L.
Decision Variables
pi,j,k ∈{0, 1}
Assignment of prefill stage k to request i on client j for prefill phase, for i ∈I, j ∈J , and k ∈Kp.
di,j,k ∈{0, 1}
Assignment of decode stage k to request i on client j for decode phase, for i ∈I, j ∈J , and k ∈Kd.
ts,p
k ∈R+
Start time of the kth prefill stage, for k ∈Kp.
ts,d
k ∈R+
Start time of the kth decode stage, for k ∈Kd.
np
k ∈R+
Time length of the kth prefill stage, for k ∈Kp.
nd
k ∈R+
Time length of the kth decode stage, for k ∈Kd.
wi,j,k ∈[0, 1]
The proportion of the decoding phase of request i executed in the kth decode stage on client j, for i ∈I, j ∈J , and k ∈Kd.
xi,j ∈{0, 1}
The assignment of request i to client j, for i ∈I and j ∈J .
yk,l ∈{0, 1}
Indicator variable specifying if the kth prefill stage is at level l, for k ∈Kp and l ∈L.
tmax ∈R+
Total inference time for all requests completed.
Bin k
Bin (k + 1)
Current time t
Action: the time to 
end the current bin
Action: the requests 
scheduled to the 
next bin and clients
State: duration of 
current bin
State: type of 
requests being 
processed
State: set of remaining requests
Fig. 4: Online scheduling
Without considering uncertainty, the offline-online inference
scheduling can be formulated as a deterministic equivalence,
which is a form of an MIP model as follows.
min tmax
(1)
s.t.
tmax ≥ts,d
k + nd
k, for k ∈Kd,
(2)
ts,p
k −

ts,d
k−1 + nd
k−1

≥0, for k = 2, ..., K,
(3)
ts,d
k −
 ts,p
k + np
k

≥0, for k = 1, 2, ..., K,
(4)
np
k ≥
X
l∈L
T p
l yk,l, for k ∈Kp,
(5)
X
i∈I
X
j∈J
N p
i pi,j,k ≤
X
l∈L
N cap
l
yk,l, for k ∈Kp,
(6)
X
l∈L
yk,l = 1, for k ∈Kp,
(7)
nd
k ≥T d X
i∈I
N d
i wi,j,k, for j ∈J , and k ∈Kd,
(8)
di,j,k −pi,j,k ≥0, for i ∈I, j ∈J , and k ∈Kd,
(9)
M (2 −di,j,k1 −di,j,k2) +
k2
X
k′=k1
di,j,k′
≥k2 −k1 + 1,
for i ∈I, j ∈J , k1, k2 ∈Kd, and k1 < k2,
(10)
M (pi,j,k1 −1) + di,j,k2 ≤0,
for i ∈I, j ∈J , k1 ∈Kp, k2 ∈Kd, and k1 > k2,
(11)
X
i∈I
di,j,k ≤1, for j ∈J , and k ∈Kd,
(12)
X
k∈Kd
di,j,k ≤K, for i ∈I, and j ∈J ,
(13)
X
k∈Kd
wi,j,k = xi,j, for i ∈I, and j ∈J ,
(14)
X
j∈J
X
k∈Kd
wi,j,k = 1, for i ∈I,
(15)
X
i∈I
pi,j,k ≤1, ∀j, k,
(16)
X
k
pi,j,k = xi,j, for j ∈J , and k ∈Kp,
(17)
X
j∈J
xi,j = 1, for i ∈I,
(18)


--- Page 7 ---
7
xi,j ∈{0, 1}, for i ∈I, and j ∈J ,
(19)
yk,l ∈{0, 1}, for k ∈Kp, and j ∈J ,
(20)
pi,j,k ∈{0, 1}, for i ∈I, j ∈J , k ∈Kp,
(21)
di,j,k ∈{0, 1}, for i ∈I, j ∈J , k ∈Kd,
(22)
wi,j,k ∈[0, 1], for i ∈I, j ∈J , k ∈Kd,
(23)
ts,p
k , np
k, for k ∈Kp,
(24)
ts,d
k , nd
k, for k ∈Kd.
(25)
The total inference time is denoted by tmax, which is min-
imized in the objective function given in Eq. (1). We let
Kd = {1, 2, · · · } be the set of decode stages. For any decode
stage k ∈Kd, let ts,d
k
∈R+ and nd
k ∈R+ denote the start
time and time length of the kth decode stage, respectively,
and thus its end time is

ts,d
k + nd
k

. Eq. (2) requires that the
total inference time is greater than or equal to the end time of
any decode stage. Let K ∈Z+ be the total number of bins,
prefill stages, and also decode stages. Any prefill stage should
start after the end of its previous decode stage, suggested by
Eq. (3). For any prefill stage k ∈Kp, let np
k ∈R+ be the time
length of the kth prefill stage. Thus, Eq. (4) suggests that,
within the same bin, the decode stage should start after the
end of prefill stage. We use L = {1, 2, · · · } to represent the
set of all possible levels for a prefill stage, and the time length
of a prefill stage depends on the level. Let yk,l ∈{0, 1}, for
k ∈Kp and l ∈L, be an indicator variable. Prefill stage k
is at level l, if yk,l = 1. Otherwise, the prefill stage is not at
level l. We denote by T p
l ∈R+ the prefill time of level l, for
l ∈L, and thus the time length of a prefill stage is determined
in Eq. (5). We use N cap
l
∈Z+ to denote the maximum token
capacity of level l, for l ∈L, and let pi,j,k ∈{0, 1} denote
assignment of prefill stage k to request i on client j for prefill
phase, for i ∈I, j ∈J , and k ∈Kp. The relationship between
pi,j,k and yk,l is given in Eq. (6). Eq. (7) suggests that any
prefill stage can only be at one level in L. Different than the
prefill phase, the decode phase of a request can be served
in multiple decode stages. Thus, we introduce wi,j,k ∈[0, 1]
to represent the proportion of the decoding phase of request
i executed in the kth decode stage on client j, for i ∈I,
j ∈J , and k ∈Kd. The time length of a decode stage is
provided by Eq. (8). Let di,j,k ∈{0, 1} be the assignment of
decode stage k to request i on client j for decode phase, for
i ∈I, j ∈J , and k ∈Kd. Eqs. (9)-(11) together sets the rule
that the decoding phase of a request must immediately follow
the consecutive prefilling phase or its previous decoding phase.
Eqs. (12)-(18) relate the assignment decisions among requests,
clients, and bins. Eqs. (19)-(25) define the domain for each set
of variables, respectively.
Eqs (1)-(25) will hereafter be referred to as “the original
model”. We attempted to solve the original model using com-
mercial MIP solvers, such as Gurobi. However, we found it
nearly impossible to solve such a large-scale problem directly.
The number of requests, denoted by |I|, is around 1,000 in
small cases, while in larger cases, it can be up to 10,000. The
number of batch sizes or client numbers, denoted by |J |, can
reach up to 200. The number of exclusive bins, denoted by
|K|, often is on the same order of magnitude as |I|. To probe
the solving cost, we solved a down-scaled toy case model with
merely 100 requests and 20 clients, which took Gurobi more
than 3,600 seconds to yield a near-optimal solution without
fully closing the dual gap. Solving this problem in its original
form is hence evidently impractical and cannot be solved in
hours. Therefore, it is necessary to decompose this problem
into stages and address it sequentially.
IV. SOLUTION METHOD
A. Method overview
The original model, provided by Eqs. (1)-(25), demands
a solution method capable of making decisions within 10
milliseconds in an asynchronous cycle, as the decoding time
per client batch can be around 50 milliseconds. However, the
original model is a large-scale MIP model with over 100,000
integer decision variables and constraints, making it difficult to
solve even within several hours. Hence, it is vital to develop
an efficient solution method that provides the best possible
outcomes within the required time frame. As illustrated in Fig.
5, we propose a hybrid offline-online method that structures
the scheduling process for large model inference into two main
methods: offline requests assignment and online scheduling.
Each method involves a subset of decision variables of the
original model and provides timely solutions at each stage. In
the figure, we illustrate how the offline-online information, as
well as the decision making given by the scheduling models,
is obtained and shared in the system.
Offline Requests Scheduling: In this method, a prede-
termined batch size of clients determines the number of
parallel requests. Each client is allocated a balanced number
of requests, resulting in an equitable task distribution. This
method considers the assignment decisions in the original
model as described in constraints (2), (12), and (15)–(16). We
isolate this part of the model to demonstrate offline training
scenarios, such as RLHF (Reinforcement Learning with Hu-
man Feedback) training. In this task, requests are typically
given and known in advance. Users can manually send their
request prompts to the LLMs and wait to receive the outputs.
These tasks can implement offline request assignment methods
to achieve better throughput. However, for most scenarios such
us user using GPT, using the offline method is still limited
since solving the MIP model usually takes 10 minutes or more,
which cannot meet the rapid iteration requirements of the LLM
online decision-making process. Therefore, it is necessary to
develop an online method to fill this gap.
Online Scheduling: The online scheduling process com-
prises two major parts corresponding to two types of online
decisions. The first part, online requests scheduling, deter-
mines which requests are scheduled in the upcoming bin and
identifies the next client to serve once a previous request
is completed. The second part, online iteration scheduling,
decides when to conclude the current decoding bin and start a
preemptive prefilling bin to enhance overall utilization rates.
In the online requests scheduling part, heuristic methods
are employed to determine the optimal order for processing
requests in real-time. This approach considers factors such as
task priority, current system load, and anticipated resource


--- Page 8 ---
8
Offline information
Online information
Execution
Inference Service
Profiler
Simulator
Input requests 
Prefill Queue
Decode Queue
Original MIP 
Model
Offline requests 
scheduling
Online requests 
scheduling
Online iteration 
scheduling
LLM workers
Decision Variables
Fig. 5: Illustration of Solution Method
availability. By effectively prioritizing requests, the system
can minimize waiting times and maximize throughput under
dynamic operational conditions. This method emphasizes the
implementation of the relaxed solutions provided by job as-
signment and illustrates the constraints (3)–(11) in the original
model.
The online iteration scheduling part aims to minimize idle
time on machines by strategically allocating computational re-
sources. By dynamically adjusting prefilling and decoding task
priorities based on real-time feedback and system constraints,
this method enhances overall system efficiency and responsive-
ness. This proactive scheduling approach minimizes machine
idle time and optimizes the utilization of processing resources,
thereby improving the overall performance of large language
model inference tasks. This method underscores iteration-
based optimization and considers the constraints (14)–(15) and
(17)–(18) in the original model.
The overarching goal of this structured approach is to mini-
mize the total inference time for a specific benchmark, thereby
maximizing throughput. By integrating thorough data analy-
sis, efficient task allocation, and adaptive online scheduling
strategies, this scheduling solution optimizes the performance
of LLM inference processes. This holistic approach not only
enhances system efficiency but also supports scalability and
reliability in handling complex computational tasks.
B. Offline requests scheduling & theoretical lower bound
As previously introduced, we begin by examining the offline
request assignment decisions within the original model, with
a specific focus on the constraints described in (2), (12), and
(15–16). This part of the model is isolated to demonstrate
offline training scenarios, such as RLHF training. In this sce-
nario, we tackle the Minimizing Makespan Bin Packing Prob-
lem to efficiently address the workload balancing challenge.
We assume that the output length is predetermined, and that
prefill decode stages do not conflict during problem-solving.
Nevertheless, in practical applications and simulations used
to evaluate performance, we adhere to these constraints by
allocating workload to clients without affecting the uncertainty
of output length.
In the offline model outlined in Eqs. (26)–(30), we introduce
a new parameter, denoted by Ti ∈R+ for i ∈I, representing
the estimated decode completion time for request i. We also
introduce a new decision variable, denoted by tj ∈R+ for
j ∈J , to indicate the total decoding time for client j.
min max
j∈J tj
(26)
s.t.
X
j∈J
xi,j = 1, for i ∈I,
(27)
X
i∈I
xi,jTi ≤tj, for j ∈J ,
(28)
xi,j ∈{0, 1}, for i ∈I, and j ∈J ,
(29)
tmax, tj ∈R+, for j ∈J .
(30)
The offline model also provides a method to calculate the
theoretical lower bound for a given set of requests, I. In this
method, we assume that prefill and decode phases for all the
requests can be separated into two groups, and we calculate
the optimal inference time for each group.
Let tp∗∈R+ and td∗∈R+ represent the optimal total
prefill and total decode times for all the requests in set I,
respectively. The value of td∗= maxj∈J tj is obtained from
the objective function value from Eqs. (26)–(30). Let L =
arg maxl∈L N cap
l
, and then the largest prefill time across all


--- Page 9 ---
9
levels in T p
l is denoted by T p
L. N cap
L
is the number of maximum
number of tokens that can be processed in T p
L. Then, tp∗can
be calculated by the following equation.
tp∗≥T p
L
P
i∈I N p
i
N cap
L

.
(31)
It yields a tight theoretical lower bound T LB as follows.
T LB = tp∗+ td∗.
(32)
C. Online requests and iteration scheduling
In this online part of the LLM inference optimization
problem, two critical considerations arise. First, we need to
determine which request to send to an available client once
the previous request is completed. Second, when a round of
decoding stage is finished, we must decide whether to send a
preemptive prefill stage or continue this decode stage to the
LLM workers for the subsequent time frame.
The first issue presents an online scheduling problem, as
illustrated by Eqs. (14)–(15) and (16)–(17). The primary
decision in this context is whether to select a new request
to override the original assignment in order to achieve better
machine utilization.
A sorting and online preemptive method is illustrated in
Algorithm 1. This online algorithm first selects the future
available requests, denoted as Ij, for client j ∈J . The set
Ij is sorted by N p
i +N d
i , that is, N p
i1 +N d
i1 > N p
i2 +N d
i2∀i1 <
i2
∈Ij. Then, for each client, the algorithm calculates
future requests and counts the expected remaining tokens
remain token(j) for j ∈J to be processed. Idle clients
then greedily select the longest request from busy clients to
process. This algorithm utilizes offline information on request
assignment to provide timely online assignment decisions.
Algorithm 1 Sorting and Online Preemptive Method
Require: {Ij = {i | xij = 1}, ∀j ∈J }
for client j in clients J do
if queue for client j is empty and Ij ̸= ∅then
pop Ij to client j
remain token(j) ←remain token(j) −(N p
i +
N d
i )
else if max(remain token(j)) > 0 then
pop arg max(remain token) to client j
remain token(j) ←remain token(j) −(N p
i +
N d
i )
end if
end for
Continuing from the previous discussion, the second prob-
lem involves a sequential decision-making process, as outlined
by Eqs. (2)–(11). The main challenge here is to deliver timely
and efficient decisions in real time. As previously mentioned,
each round of decoding takes approximately 50 milliseconds.
Thus, it is essential to ensure that decisions are made within
10 milliseconds to sustain system efficiency. To achieve this,
we employ the following method to integrate quick decision-
making into the process.
This aspect of decision-making corresponds to the following
problem.
min tmax
s.t.
tmax ≥ts,d
k + nd
k, for k ∈Kd,
(33)
ts
p,k −(ts
d,k−1 + nd
k−1) ≥0, for k = 2, ..., K,
(34)
ts,d
k −(ts,p
k + np
k) ≥0, for k ∈Kp,
(35)
np
k ≥
X
l∈L
T p
l yk,l, for k ∈Kp,
(36)
X
i∈I,j∈J
N p
i pi,j,k ≤
X
l∈L
N cap
l
yk,l, for k ∈Kp,
(37)
X
l∈L
yk,l = 1, for k ∈Kp,
(38)
nd
k ≥T d X
i
N d
i wi,j,k, for j ∈J , and k ∈Kd,
(39)
di,j,k −pi,j,k ≥0, for i ∈I, j ∈J , and k ∈K.
(40)
By combining Eqs. (33) and (35), we derive that tmax ≥
maxk∈K
 ts,p
k + np
k + nd
k

. In the context of online decision-
making, the start time ts,p
k
is typically influenced by the
completion time of preceding tasks. The primary objective
is to minimize the total time cost of prefill and decode
stages. Consequently, we establish the following equation by
integrating the calculations of np
k and nd
k from Eqs. (36) and
(39).
min tmax ≥max
k∈K

ts,p
k +
X
l∈L
T p
l yk,l + T d
X
i∈I,j∈J
N d
i wi,j,k

.
(41)
In this problem, the time cost is divided into two components.
The cost for adding a prefill task at any point is given by
∂tmax
∂yk,l
=
X
l∈L
T p
l ,
(42)
and the cost for adding a decode task at the decision-making
moment is expressed by
∂tmax
∂wi,j,k
= T d X
i∈I
N d
i .
(43)
Thus, our heuristic method for deciding whether to dispatch a
prefill or decode stage to the LLM worker involves comparing
the prefill cost Cp = P
l T p
l with the waited decode time Cd =
T d P
i∈I,j∈J N d
i wi,j,k. If Cp ≥Cd, the algorithm advises
continuing with a round of the decode task and waiting for
additional prefill tasks; otherwise, the algorithm recommends
executing a round of the prefill task.
V. NUMERICAL EXPERIMENT
A. Experiment settings and baseline
Before the model is solved by our hybrid method, extensive
analysis is conducted to evaluate the time taken by the decode
and prefill stages on hardware. This analysis provides crucial
insights into the computational demands and performance
characteristics of each stage. By quantifying these metrics,


--- Page 10 ---
10
TABLE III: Experiment Settings
Parameter
Number
Brief Description
|I|
1319
The GSM8K dataset
|J |
200
Due to hardware memory limit
E(Np
i )
68.43
The GSM8K dataset input
E(Nd
i )
344.83
The Llama 65B output
T p
0.13 ms/token
The hardware performance on prefilling
T d
0.21 ms/token
The hardware performance on decoding
such as processing times and resource utilization, the data
analysis establishes a solid foundation of empirical data. The
data serve as reliable support for subsequent decision-making
in optimizing scheduling strategies. The basic experiment
setting is given in TABLE III.
We demonstrate our improvements by utilizing the GSM8K
dataset [34], a comprehensive collection of mathematical word
problems specifically designed to assess the problem-solving
and reasoning capabilities of language models. This dataset
serves as a benchmark for evaluating both arithmetic and log-
ical reasoning skills, essential attributes for advanced language
models. Each problem in the dataset is intricately constructed
to simulate real-world situations involving numerical rela-
tionships, necessitating that models comprehend the problem
contextually and perform accurate calculations to derive the
correct solution.
The GSM8K dataset comprises 1,319 unique problems as
input requests, with an average input length of 68.43 tokens
and a standard deviation of 25.04 tokens. For our experiments,
we selected the LLaMA-65B language model due to its open-
source nature and wide accessibility, making it a suitable
candidate for academic and reproducible research. In our tests,
the LLaMA-65B model generated responses averaging 344.83
tokens, with a standard deviation of 187.99 tokens. To ensure
consistency and focus on quality responses, we constrained
the maximum output length to 512 tokens during testing.
Our computational setup is characterized by a robust hard-
ware configuration, consisting of eight Ascend processing
units, each equipped with a maximum memory capacity of
64 GB. This formidable hardware infrastructure is essential
for facilitating the efficient processing and testing necessary
for our experiments. Additionally, we have assessed the KV
cache usage for input in this experiment, establishing baseline
settings that are also utilized in practical applications. The
current hardware, along with the LLM employed, imposes a
memory constraint of 1024 blocks of KV cache. Each block
can accommodate a maximum of 128 tokens. For the GSM-8k
benchmark, the combined maximum input and output for each
request requires five blocks. Consequently, this configuration
limits us to a maximum of approximately 200 clients running
concurrently, calculated by the expression 1024/5 ≈200.
In our experimental setup, we conduct an estimation of the
operation time required for prefill and decode stages using
over 400 data groups. We find that both prefill and decode
times exhibit a linear relationship with the number of tokens
involved. Specifically, the prefill time can be calculated as 0.13
milliseconds per token, plus a fixed overhead of 25 millisec-
onds. For the decode process, the time required for each batch
0
25000
50000
75000
100000 125000 150000 175000 200000
Time(ms)
0
25
50
75
100
125
150
175
200
Client
Total Inference Time
prefill
decode
Utilization rate: 80.2%. Total inference time: 201.00 seconds.
Fig. 6: Result Gantt: Baseline
of clients can be estimated as 0.21 milliseconds per token, with
an additional fixed overhead of 29 milliseconds. For instance,
when processing a parallel batched decode stage involving 200
clients, where each client produces one token per round, the
operation would take approximately 200 × 0.21 + 29 = 71
milliseconds. In the case of prefill stages, if a batch consists
of inputs totaling 5,000 tokens, the estimated time required
would be 5000 × 0.13 + 25 = 675 milliseconds.
We present a Gantt chart in Fig. 6, generated from an
experiment using real-world data and open source LLM, to
illustrate the current state of online inference services with-
out the implementation of our proposed method. This chart
demonstrates that, in practical scenarios, a significant number
of idle periods, or “bubbles”, occur when no scheduling
strategy is employed. Furthermore, in offline scenarios, if the
workload among clients is not evenly distributed, substantial
machine idle time is observed after the early completion of
some client’s tasks. Our analysis of this Gantt chart reveals
that the overall machine utilization rate is only 80.2%.
B. Offline request scheduling result
This offline request scheduling model given by Eqs. (26)–
(30) can be solved using open-source solver SCIP. Due to
significantly reduced complexity, optimal solutions can be
achieved within 20 minutes comparing to original problem
which is not possible to be solved within hours. Although
this offline model only addresses workload balancing using
estimations of output length, its performance surpasses that of
the original version. As illustrated in Fig. 7, the system shows
a significant reduction of idle times, and machine utilization
is enhanced to 85.5%. Comparing to the baseline method, this
method provides a more balanced request assignment across
clients and reduce “bubbles”. The total inference time can be
reduced from 201.00 seconds to 197.08 seconds. Since solving
the model still takes relatively long time, we list this method
as optional and suggest practitioners use the offline model in
typical scenarios such as RLHF training.


--- Page 11 ---
11
0
25000
50000
75000
100000 125000 150000 175000
Time(ms)
0
25
50
75
100
125
150
175
200
Client
Total Inference Time
prefill
decode
Utilization rate: 85.5%. Total inference time: 197.08 seconds.
Fig. 7: Result Gantt: Offline Request Scheduling
C. Online scheduling results
Incorporating online requests and iteration scheduling meth-
ods, as depicted in Fig. 9, results in a marked improvement in
total inference time, showing reductions 190.58s compared to
201.00s in the baseline scenario. Additionally, machine utiliza-
tion is enhanced to 89.06%. Comparing to offline scheduling
method, these two online methods do not require additional
computing and can be used for current online inference.
In Fig. 8, we present the results obtained using only
the online scheduling method, without employing the offline
scheduling method. As shown, compared to the baseline, the
utilization rate improves to 86.19%, and the total inference
time decreases to 193.33 seconds. These results demonstrate
that the online method performs well even in the absence of
prior knowledge about requests. This scenario is common in
the area of LLM inference.
We also calculate the theoretical lower bound using Eq.
(32). In the specified numerical case utilizing GSM8K, the
theoretical bound is 180 seconds, in which T p∗= 13 seconds
and T d∗= 167 seconds. In this scenario, we reduce the total
inference time from 201.00 seconds in the baseline to 190.08
seconds with the hybrid online-offline method. The gap to the
optimal value is thus reduced from 201 −180 = 21 seconds
to 190−180 = 10 seconds, representing a reduction of 52.4%
in this “primal dual” gap.
To better demonstrate the performance of our online
scheduling methods, we present a numerical experiment in-
volving 100 cases in Figs. 10 and 11. These cases are randomly
generated with the input and output length distributions shown
in TABLE III. As illustrated in the figures, despite some vari-
ations across the 100 cases, our hybrid offline-online method
consistently outperforms in both utilization and generation
speed. The unit for generation speed is tokens per second,
indicating how many tokens the LLM can generate each sec-
ond. On average, our method achieves an 8.0% improvement
in utilization and an increase of 100.63 tokens per second in
generation speed.
0
25000
50000
75000
100000 125000 150000 175000
Time(ms)
0
25
50
75
100
125
150
175
200
Client
Total Inference Time
prefill
decode
Utilization rate: 86.19%. Total inference time: 193.33 seconds.
Fig. 8: Result Gantt: Online only Scheduling
0
25000
50000
75000
100000 125000 150000 175000
Time(ms)
0
25
50
75
100
125
150
175
200
Client
Total Inference Time
prefill
decode
Utilization rate: 89.06%. Total inference time: 190.58 seconds.
Fig. 9: Result Gantt: Offline+Online Scheduling
Fig. 10: Utilization rate with 100 cases


--- Page 12 ---
12
Fig. 11: Generate speed with 100 cases
VI. CONCLUSION AND FUTURE WORK
In this paper, we study the inference optimization problem
in the service system when deploying LLMs. To enhance
the system throughput and better utilize the hardware, we
formulate an MIP model to describe this inference optimiza-
tion problem. To the best of our knowledge, this is the first
formulation of the problem from a scheduling perspective.
To tackle the complex and high real-time demands of LLM
inference, we introduce a hybrid offline-online method.
In the offline method, we demonstrate how large-scale infer-
ence systems can be improved using a Minimizing Makespan
Bin Packing Problem and how a theoretical lower bound can
be provided. In the online request scheduling and iteration
scheduling methods, the solution time efficiency is crucial.
We propose a sorting and online preemptive method to more
effectively utilize clients that finish early. Then, we focus on
the iteration scheduling component of the original model and
employ a Lagrangian method to compare the costs of adding a
prefill stage versus a decode stage at each iteration. We provide
a time efficient heuristic method to determine when to insert
a prefill task and interrupt ongoing decoding tasks.
In real-world experiments, we deploy the LlaMA-65B LLM
model and infer the GSM 8K dataset, which includes 1,319
unique math problems. Our offline model increases machine
utilization rates from a baseline of 80.2% to 85.5%, and
reduces the total inference time from 201 seconds to 197
seconds. Utilizing the online scheduling methods, the system
utilization rate can be further increased to 89.1%, and the
total inference time for the dataset can be reduced to 191
seconds. As demonstrated, if all our methods are implemented,
system throughput can be improved by 5.46%, and hardware
utilization can increase by 11.0%. A 100-cases study shows
that our method consistently outperforms the baseline method
and improves the utilization rate by 8.0% on average.
The future directions of this research can be extended along
three key aspects:
• Stochastic Model & Efficient Solution Method: The orig-
inal MIP model we proposed is a deterministic equiva-
lence formulation. While solving this model is already
computationally challenging, developing a stochastic pro-
gramming model could further enhance its accuracy by
better accounting for uncertainties. Additionally, more
efficient solution methods for tackling the original MIP
model are needed to meet the millisecond-level real-time
requirements of the decision-making process.
• Reinforcement Learning on Iteration Scheduling: the cur-
rent iteration scheduling approach relies on a heuristic
online method. Notably, the decision-making process
in this method involves choosing between two options:
prefill or decode. Since online state variables such as
prefill task waiting time, the number of decoding clients,
expected decoding time, and expected prefill time are
relatively easy to derive, a simple reinforcement learning
(RL) model could be trained to assist the scheduler in
making decisions dynamically.
• Online Hardware Utilization Method: We observed dur-
ing hardware experiments that the system’s hardware
is often underutilized when a static number of clients
is employed, due to stochastic variations in the output
length. In scenarios where dynamic and continuous batch-
ing methods are applicable, investigating online decision-
making for hardware utilization could further optimize
performance. Specifically, determining the optimal num-
ber of clients that can be allocated concurrently to the
system at any given time could help enhance resource
utilization and overall efficiency.
REFERENCES
[1] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez,
H. Zhang, and I. Stoica, “Efficient memory management for large
language model serving with pagedattention,” in Proceedings of the 29th
Symposium on Operating Systems Principles, 2023, pp. 611–626.
[2] G.-I. Yu, J. S. Jeong, G.-W. Kim, S. Kim, and B.-G. Chun, “Orca: A
distributed serving system for {Transformer-Based} generative models,”
in 16th USENIX Symposium on Operating Systems Design and Imple-
mentation (OSDI 22), 2022, pp. 521–538.
[3] NVIDIA, “NVIDIA Announces Financial Results for Fourth Quarter
and
Fiscal
2024,”
https://investor.nvidia.com/news/press-release-
details/2024/NVIDIA-Announces-Financial-Results-for-Fourth-Quarter-
and-Fiscal-2024/, Feb. 2024, [Online; accessed 09-01-2025].
[4] A. Agrawal, N. Kedia, A. Panwar, J. Mohan, N. Kwatra, B. S. Gulavani,
A. Tumanov, and R. Ramjee, “Taming throughput-latency tradeoff in llm
inference with sarathi-serve,” arXiv preprint arXiv:2403.02310, 2024.
[5] B. Sun, Z. Huang, H. Zhao, W. Xiao, X. Zhang, Y. Li, and W. Lin,
“Llumnix: Dynamic scheduling for large language model serving,” arXiv
preprint arXiv:2406.03243, 2024.
[6] W. Lee, J. Lee, J. Seo, and J. Sim, “{InfiniGen}: Efficient generative
inference of large language models with dynamic {KV} cache manage-
ment,” in 18th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 24), 2024, pp. 155–172.
[7] B. Wu, R. Zhu, Z. Zhang, P. Sun, X. Liu, and X. Jin, “{dLoRA}:
Dynamically orchestrating requests and adapters for {LoRA}{LLM}
serving,” in 18th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 24), 2024, pp. 911–927.
[8] Y. Sheng, S. Cao, D. Li, B. Zhu, Z. Li, D. Zhuo, J. E. Gonzalez, and
I. Stoica, “Fairness in serving large language models,” in 18th USENIX
Symposium on Operating Systems Design and Implementation (OSDI
24), 2024, pp. 965–988.
[9] B. Wu, Y. Zhong, Z. Zhang, S. Liu, F. Liu, Y. Sun, G. Huang, X. Liu, and
X. Jin, “Fast distributed inference serving for large language models,”
arXiv preprint arXiv:2305.05920, 2023.
[10] Y. Zhong, S. Liu, J. Chen, J. Hu, Y. Zhu, X. Liu, X. Jin, and
H. Zhang, “{DistServe}: Disaggregating prefill and decoding for
goodput-optimized large language model serving,” in 18th USENIX
Symposium on Operating Systems Design and Implementation (OSDI
24), 2024, pp. 193–210.


--- Page 13 ---
13
[11] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam,
and D. Kalenichenko, “Quantization and training of neural networks for
efficient integer-arithmetic-only inference,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2018, pp. 2704–
2713.
[12] R. Child, S. Gray, A. Radford, and I. Sutskever, “Generating long
sequences with sparse transformers,” arXiv preprint arXiv:1904.10509,
2019.
[13] G. Bai, Y. Li, C. Ling, K. Kim, and L. Zhao, “Sparsellm: Towards global
pruning of pre-trained language models,” in The Thirty-eighth Annual
Conference on Neural Information Processing Systems, 2024.
[14] G. Hinton, “Distilling the knowledge in a neural network,” arXiv preprint
arXiv:1503.02531, 2015.
[15] N. Shazeer, “Fast transformer decoding: One write-head is all you need,”
arXiv preprint arXiv:1911.02150, 2019.
[16] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebr´on, and
S. Sanghai, “Gqa: Training generalized multi-query transformer models
from multi-head checkpoints,” arXiv preprint arXiv:2305.13245, 2023.
[17] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary,
V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro
et al., “Efficient large-scale language model training on gpu clusters
using megatron-lm,” in Proceedings of the International Conference for
High Performance Computing, Networking, Storage and Analysis, 2021,
pp. 1–15.
[18] Y. Huang, Y. Cheng, A. Bapna, O. Firat, M. X. Chen, D. Chen, H. Lee,
J. Ngiam, Q. V. Le, Y. Wu et al., “Gpipe: Easy scaling with micro-batch
pipeline parallelism,” proceeding of Computer Science¿ Computer Vision
and Pattern Recognition, 2019.
[19] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catan-
zaro, “Megatron-lm: Training multi-billion parameter language models
using model parallelism,” arXiv preprint arXiv:1909.08053, 2019.
[20] V. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch,
M. Shoeybi, and B. Catanzaro, “Reducing activation recomputation
in large transformer models,” Proceedings of Machine Learning and
Systems, vol. 5, pp. 341–353, 2023.
[21] H. Liu, M. Zaharia, and P. Abbeel, “Ring attention with blockwise
transformers for near-infinite context,” arXiv preprint arXiv:2310.01889,
2023.
[22] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R´e, “Flashattention: Fast and
memory-efficient exact attention with io-awareness,” Advances in Neural
Information Processing Systems, vol. 35, pp. 16 344–16 359, 2022.
[23] Y. Leviathan, M. Kalman, and Y. Matias, “Fast inference from transform-
ers via speculative decoding,” in International Conference on Machine
Learning.
PMLR, 2023, pp. 19 274–19 286.
[24] C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre, and J. Jumper,
“Accelerating large language model decoding with speculative sam-
pling,” arXiv preprint arXiv:2302.01318, 2023.
[25] P. Patel, E. Choukse, C. Zhang, A. Shah, ´I. Goiri, S. Maleki, and
R. Bianchini, “Splitwise: Efficient generative llm inference using phase
splitting,” in 2024 ACM/IEEE 51st Annual International Symposium on
Computer Architecture (ISCA).
IEEE, 2024, pp. 118–132.
[26] C. Hu, H. Huang, L. Xu, X. Chen, J. Xu, S. Chen, H. Feng, C. Wang,
S. Wang, Y. Bao et al., “Inference without interference: Disaggre-
gate llm inference for mixed downstream workloads,” arXiv preprint
arXiv:2401.11181, 2024.
[27] Z. Zhou, X. Ning, K. Hong, T. Fu, J. Xu, S. Li, Y. Lou, L. Wang,
Z. Yuan, X. Li et al., “A survey on efficient inference for large language
models,” arXiv preprint arXiv:2404.14294, 2024.
[28] R. Qin, Z. Li, W. He, M. Zhang, Y. Wu, W. Zheng, and X. Xu, “Moon-
cake: A kvcache-centric disaggregated architecture for llm serving,”
arXiv preprint arXiv:2407.00079, 2024.
[29] F. Wang, S. Fathizadan, F. Ju, K. Rowe, and N. Hofmann, “Print surface
thermal modeling and layer time control for large-scale additive manu-
facturing,” IEEE Transactions on automation science and engineering,
vol. 18, no. 1, pp. 244–254, 2020.
[30] B. Pang, X. Xie, Y. Song, and L. Luo, “Surgery scheduling under case
cancellation and surgery duration uncertainty,” IEEE Transactions on
Automation Science and Engineering, vol. 16, no. 1, pp. 74–86, 2018.
[31] S. A. Erdogan, A. Gose, and B. T. Denton, “Online appointment
sequencing and scheduling,” IIE Transactions, vol. 47, no. 11, pp. 1267–
1286, 2015.
[32] B. Pang, X. Xie, F. Ju, and J. Pipe, “A dynamic sequential decision-
making model on mri real-time scheduling with simulation-based opti-
mization,” Health Care Management Science, vol. 25, no. 3, pp. 426–
440, 2022.
[33] K. Lee, F. Zheng, and M. L. Pinedo, “Online scheduling of ordered flow
shops,” European Journal of Operational Research, vol. 272, no. 1, pp.
50–60, 2019.
[34] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,
M. Plappert, J. Tworek, J. Hilton, R. Nakano et al., “Training verifiers
to solve math word problems,” arXiv preprint arXiv:2110.14168, 2021.
Bowen Pang received his Bachelor’s degree and
Ph.D. degree from the Department of Industrial
Engineering at Tsinghua University, Beijing, China,
in 2016 and 2022, respectively. He is currently a
researcher at Noah’s Ark Lab, Huawei Technology.
His research interests include modeling, analyzing,
and solving problems in engineering systems, with
applications in healthcare, supply chain, manufactur-
ing, and artificial intelligence. He has been a member
of IEEE, IISE, and INFORMS. Please feel free to
contact his email pzkaixin@foxmail.com if you are
interested in LLM inference optimization area.
Kai Li received the bachelor’s degree from YingCai
Honors College, University of Electronic Science
and Technology of China, Chengdu, China, in 2015,
and the Ph.D. degree from the Department of Com-
puter Science and Engineering, Shanghai Jiao Tong
University, Shanghai, China, in 2021. He is currently
a researcher at Noah’s Ark Lab, Huawei Technology.
His research interests include game theory, rein-
forcement learning, and large language models.
Ruifeng She received his B.S. and Ph.D. degrees
from the Department of Civil Engineering of the
University of Illinois at Urbana-Champaign, USA,
in 2018 and 2023, respectively. He is currently a
researcher at Noah’s Ark Lab, Huawei Technology.
His research interests include optimization of com-
plex systems, reinforcement learning, and the inte-
gration of operations research and machine learning.
Feifan Wang received the bachelor’s degree from
the Department of Industrial Engineering, Zhejiang
University of Technology, Hangzhou, China, in
2013, the master’s degree from the Department of
Industrial and Systems Engineering, Zhejiang Uni-
versity, Hangzhou, China, in 2016, and the Ph.D.
degree from the School of Computing, Informatics,
and Decision Systems Engineering, Arizona State
University, Tempe, AZ, USA, in 2021. He is cur-
rently an Assistant Professor with the Department
of Industrial Engineering at Tsinghua University,
Beijing, China. His research focuses on modeling, analysis, optimization, and
control of complex systems, with applications in healthcare delivery systems
and production systems. He is a member of IEEE, IISE, and INFORMS. He
was a recipient of multiple awards, including the Design and Manufacturing
Best Paper Award from the IISE Transactions, the Best Student Paper Award
from IEEE CASE, and the Dean’s Dissertation Award from ASU. He has
twice been a finalist for the Best Paper Award on Healthcare Automation
from IEEE CASE.
