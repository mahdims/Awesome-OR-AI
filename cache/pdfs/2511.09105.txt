--- Page 1 ---
Cost-Minimized Label-Flipping Poisoning Attack to LLM Alignment
Shigeki Kusaka*1, Keita Saito*1, Mikoto Kudo*1,2, Takumi Tanabe3, Akifumi Wachi3,
Youhei Akimoto1,2,4
1University of Tsukuba, 2RIKEN AIP, 3LY Corporation, 4Institute of Science Tokyo
{shigeki.kusaka, keita.saito, mikoto}@bbo.cs.tsukuba.ac.jp,
{akifumi.wachi, takumi.tanabe}@lycorp.co.jp,
akimoto@cs.tsukuba.ac.jp
Abstract
Large language models (LLMs) are increasingly deployed in
real-world systems, making it critical to understand their vul-
nerabilities. While data poisoning attacks during RLHF/DPO
alignment have been studied empirically, their theoretical foun-
dations remain unclear. We investigate the minimum-cost poi-
soning attack required to steer an LLMâ€™s policy toward an
attackerâ€™s target by flipping preference labels during RLH-
F/DPO, without altering the compared outputs. We formulate
this as a convex optimization problem with linear constraints,
deriving lower and upper bounds on the minimum attack cost.
As a byproduct of this theoretical analysis, we show that any
existing label-flipping attack can be post-processed via our
proposed method to reduce the number of label flips required
while preserving the intended poisoning effect. Empirical re-
sults demonstrate that this cost-minimization post-processing
can significantly reduce poisoning costs over baselines, par-
ticularly when the reward modelâ€™s feature dimension is small
relative to the dataset size. These findings highlight fundamen-
tal vulnerabilities in RLHF/DPO pipelines and provide tools
to evaluate their robustness against low-cost poisoning attacks.
Code â€” https:
//github.com/akimotolab/PoisoningCostMinimization
Introduction
Vulnerability of LLM
As large language models (LLMs)
are increasingly deployed in real-world applications, under-
standing their vulnerabilities is essential for ensuring their
effective and safe use. Adversarial attacks expose these vul-
nerabilities, supporting red-teaming efforts (Shayegani et al.
2023a). Representative adversarial attacks at inference time
include jail-breaking (Wei, Haghtalab, and Steinhardt 2023;
Chao et al. 2023; Mehrotra et al. 2024; Zou et al. 2023)
and prompt injection (Greshake et al. 2023; Liu et al. 2024),
where attackers craft malicious inputs to elicit unintended
outputs from LLMs. At training time, data poisoning attacks
modify the training dataset to induce undesired behaviors or
embed backdoor triggers in the resulting LLM. In this paper,
we focus on the data poisoning attack on LLMs.
*These authors contributed equally.
Copyright Â© 2026, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
Poisoning attacks on preference alignment
LLMs are
susceptible to data poisoning attacks due to their multi-
stage training pipeline, which typically includes pre-training,
supervised fine-tuning (SFT), and alignment via reinforce-
ment learning from human feedback (RLHF) (Ouyang et al.
2022) or direct preference optimization (DPO) (Rafailov
et al. 2023). Recent empirical studies have demonstrated
that LLMs can be compromised through poisoning during
both the SFT phase (Wan et al. 2023; Shu et al. 2023) and
the RLHF/DPO phase (Wu et al. 2025; Wang et al. 2024;
Pathmanathan et al. 2024a; BaumgÃ¤rtner et al. 2024; Path-
manathan et al. 2024b; TramÃ¨r and Rando Ramirez 2024),
raising concerns about their robustness to adversarial manip-
ulation. However, the theoretical foundations of poisoning
attacks in the RLHF/DPO phase remain largely unexplored,
leaving open questions about the fundamental vulnerabilities
of these methods and potential defenses. A theoretical under-
standing is crucial to ascertain the worst-case scenarios for
victims, which empirical studies cannot fully reveal.
Objective
We theoretically investigate the minimum cost
of the attack to successfully steer the optimal LLM policy
toward the attackerâ€™s target policy during the RLHF/DPO
phase. We consider that an attacker who operates as an anno-
tator, tasked with evaluating two outputs y and z in a given
context x, and providing a binary preference label (w = 1 if
y is preferred; otherwise w = âˆ’1). While the attacker can not
modify (x, y, z), they can arbitrarily set the preference label
w. Our goal is to determine the minimum number of label
flips from the benign labels required to induce the attackerâ€™s
desired behavior and to design a method for constructing a
malicious dataset that achieves this objective with minimal
cost. By quantifying these costs, our analysis is expected
to guide the design of robust RLHF/DPO pipelines that can
detect or mitigate such low-cost poisoning attacks, ensuring
the safe deployment of LLMs in real-world settings.
Contributions
In this work, we provide the first theoretical
analysis of the minimal cost required to steer LLM policies
via label-flipping attacks during RLHF/DPO alignment. By
formulating the problem as a convex (or linear) optimization,
we derive tight lower and upper bounds on the minimum
number of label flips needed to induce a target policy. As
a byproduct of this analysis, we develop a post-processing
method that can be applied to any existing label-flipping
arXiv:2511.09105v1  [cs.LG]  12 Nov 2025


--- Page 2 ---
attack to reduce its cost while preserving its intended poison-
ing effect. This approach is particularly effective in practical
LLM alignment pipelines, where the dataset size is signifi-
cantly greater than the feature dimension of the reward model,
enabling attackers to exploit redundancy in the data in the
feature space to minimize the cost of targeted poisoning.
Preliminaries
LLM alignment is often conducted in two stages: supervised
fine-tuning (SFT) and learning from human feedback. Given
an LLM pre-trained with a large corpus in an unsupervised
manner, it is fine-tuned using human-annotated input-output
pairs (x, y) to produce more relevant output for specific
downstream tasks of interest. Learning from human feedback
then aims to further align LLMs with human preferences.
In this step, the LLM is trained using a dataset of prefer-
ences, DL = {(x, y, z, w)}, where x is the input, y and z are
two candidate outputs, and w âˆˆ{âˆ’1, 1} is the preference
label indicating whether y is preferred to z (w = 1) or not
(w = âˆ’1). The LLM is trained to assign higher probabilities
to preferred outputs. Reinforcement learning from human
feedback (RLHF) is often employed for this purpose.
RLHF first trains a reward model r(x, y) from the prefer-
ence dataset. The human preference is typically modeled as
the Bradley-Terry model:
Pr[w = 1 | x, y, z] = Ïƒ(r(x, y) âˆ’r(x, z)),
(1)
where Ïƒ(t) =
1
1+exp(âˆ’t) is the sigmoid function. The re-
ward model is trained via maximum likelihood estimation by
minimizing
L(r) = âˆ’
X
(x,y,z,w)âˆˆDL
log Ïƒ(w(r(x, y) âˆ’r(x, z))).
(2)
Let br denote the obtained reward model. The LM policy Ï€
is then trained to maximize the obtained reward under the
KL-regularization:
Exâˆ¼Ï[Eyâˆ¼Ï€(y|x)[br(x, y)] âˆ’Ï„DKL(Ï€ âˆ¥Ï€ref)],
(3)
where Ï is a distribution over the context; Ï€ref is the reference
policy, typically the SFT policy used to initialize RLHF; and
Ï„ is a parameter controlling the deviation from Ï€ref.
Direct Preference Optimization (DPO) is an alternative
to RLHF that directly optimizes the LM policy from the
preference dataset. The optimal policy that maximizes (3) is:
Ï€r(y | x) =
1
Zr,Ï€ref(x)Ï€ref(y | x) exp(Ï„ âˆ’1r(x, y)),
(4)
where
Zr,Ï€ref(x) =
X
y
Ï€ref(y | x) exp(Ï„ âˆ’1r(x, y)).
(5)
Therefore, an LM policy Ï€ can be viewed as the optimal
policy under the reward function:
r(x, y) = Ï„ log Ï€(y | x)
Ï€ref(y | x) + Ï„ log Zr,Ï€ref(x).
(6)
By substituting this expression into (2), one obtains the corre-
sponding DPO objective. It is known that the optimal policies
for RLHF and DPO coincide (Gheshlaghi Azar et al. 2024),
and several variants of DPO have been proposed in the lit-
erature (Gheshlaghi Azar et al. 2024; Swamy et al. 2024;
Ethayarajh et al. 2024).
Related Works
Most prior work on data poisoning attacks in machine learn-
ing focuses on supervised learning settings, particularly re-
gression or classification tasks (Shayegani et al. 2023b). Typ-
ical poisoning objectives include degrading model perfor-
mance or injecting backdoor triggers. In these settings, at-
tackers can add malicious data to the original dataset, train-
ing the victim model on the combined dataset to induce the
desired malicious behavior. In contrast, in our setting, the
attacker can only flip preference labels in the dataset, making
the attack surface more constrained.
Within the LLM alignment pipeline, poisoning attacks
have been studied in both the SFT phase (Wan et al. 2023;
Shu et al. 2023) and the RLHF phase (Wu et al. 2025; Wang
et al. 2024; Pathmanathan et al. 2024a; BaumgÃ¤rtner et al.
2024; Pathmanathan et al. 2024b; TramÃ¨r and Rando Ramirez
2024). In the SFT phase, human annotators are expected to
produce high-quality outputs y for given contexts x, creating
a natural risk of maliciously crafted pairs (x, Ëœy). In the RLHF
phase, annotators evaluate candidate outputs y, z in a given
context x and provide preference labels. While a strong adver-
sary may replace or inject malicious triplets (x, y, z) into the
preference dataset (BaumgÃ¤rtner et al. 2024; Pathmanathan
et al. 2024b; TramÃ¨r and Rando Ramirez 2024), arguably
the most realistic scenario involves a malicious annotator
who can only manipulate the preference label w while the
triplet itself remains unchanged (Wu et al. 2025; Wang et al.
2024; Pathmanathan et al. 2024a). This setting is analogous
to label-flipping attacks (Xiao, Xiao, and Eckert 2012), but
while traditional label-flipping attacks focus on classifica-
tion, our context involves reward model learning from paired
preference data, introducing a distinct problem structure.
The most relevant prior works are Wu et al. (2025) and
Wang et al. (2024), who consider the same setting and de-
velop attack algorithms under both white-box and black-box
scenarios. Their empirical investigations reveal that RLHF
alignment is vulnerable to label-flipping attacks and that ex-
isting defense strategies provide limited protection. However,
while these studies empirically demonstrate the vulnerability
of RLHF pipelines, our work is the first, to the best of our
knowledge, to provide a theoretical analysis of the minimal
cost of label-flipping attacks required to guide the reward
model toward an attacker-specified target. Moreover, we in-
troduce a convex (or linear) programming framework that
not only characterizes these costs but also enables practi-
cal reduction of attack costs in existing poisoning methods.
Specifically, given a candidate malicious dataset, our frame-
work can propose an alternative dataset that induces the same
reward function while requiring fewer label flips, highlight-
ing a fundamental and previously unquantified vulnerability
in RLHF/DPO pipelines.
Threat Model
Victim
The victim has access to a labeled dataset con-
structed from an unlabeled dataset DU = {(xi, yi, zi)}N
i=1,
where xi âˆˆX is a context, and yi, zi âˆˆY are two candi-
date outputs. Each triplet is labeled by external annotators,
with the label wi = 1 if yi is preferred to zi in context


--- Page 3 ---
xi, and wi = âˆ’1 otherwise. We denote by Î·(x, y, z) the
probability that the label for (x, y, z) is w = 1. The set of
labeled data (x, y, z, w) forms the labeled dataset DL. With-
out loss of generality, we assume anti-symmetry of Î·, i.e.,
Î·(x, y, z) = 1 âˆ’Î·(x, z, y). In case that multiple (m â©¾1) an-
notations are provided for each triplet, the size of the labeled
dataset becomes m Â· N.
The victim aims to optimize a policy Ï€ under the labeled
dataset DL using RLHF. First, a reward model r is trained by
minimizing the empirical loss (2). Once the reward model is
trained, the LM policy is optimized to maximize the expected
reward under KL regularization as in (3). For technical com-
pleteness, we assume Ï€ref(y | x) > 0 for all (x, y) âˆˆX Ã— Y.
The optimal policy under r is thus given by (4).
Following Ouyang et al. (2022), we model the reward
function using a pre-trained baseline LLM (typically a fine-
tuned model) from which the final unembedding layer is
removed to obtain an embedding Ï• : X Ã— Y â†’Rn. A linear
output layer is then added, resulting in the reward model
r(x, y) = râŠ¤Ï•(x, y), where r âˆˆRn is referred to as the
reward vector. During the reward training phase, we consider
two settings: (1) only the reward vector r is trained while the
embedding Ï• is fixed, and (2) both the reward vector r and
the embedding Ï• are trained.
Attacker
The attackerâ€™s objective is to guide the victimâ€™s
optimal policy toward a target policy Ï€A with minimal cost.
Due to the structure of the optimal policy (4), only policies
that are optimal under reward functions representable by the
victimâ€™s reward model are valid. Thus, the attack reduces
to steering the reward model to a target reward function rA
at minimum cost. The target reward function may be hand-
crafted or obtained using attack methods such as Wu et al.
(2025); Wang et al. (2024). In the latter case, the objective
becomes minimizing the cost required to achieve the same
poisoning effect.
We assume the attacker has access to the labeled dataset
DL. The attacker can flip labels wi to wA
i
âˆˆ{âˆ’1, 1} at a
cost defined later. However, the attacker cannot modify the
input x or candidate outputs y, z in the dataset.
Discrepancy Between Theory and Practice
For theoret-
ical analysis in the following section, we idealize both the
attackerâ€™s capabilities and the victimâ€™s training process. We
assume the attacker can directly modify the annotation prob-
ability Î· rather than flipping individual labels w (i.e., the
attackerâ€™s action is not binary {âˆ’1, 1}, but continuous [0, 1]),
and that the victim minimizes the expected loss under Î·
(i.e., population version) rather than the empirical loss (2). In
practice, due to the finite dataset, realizable annotation prob-
abilities are discrete, being multiples of the reciprocal of the
count m of each (x, y, z) in the dataset (i.e., m is the number
of annotations per datum). These idealizations enable us to
derive theoretical guarantees, while we acknowledge the risk
of deviation from practical scenarios, evaluated empirically.
Minimum Cost Attack
The objective of this study is to identify the cost of the at-
tacker to realize the target policy Ï€rA by label flipping. Ar-
guably, the most natural choice for the cost of the attacker is
the amount of flipped labels, formulated as
(practical)
X
(xi,yi,zi,wi)âˆˆDL
|wi âˆ’wA
i |
(7)
or (ideal)
X
(xi,yi,zi)âˆˆDU
|Î·(xi, yi, zi) âˆ’Î·A(xi, yi, zi)|,
(8)
where wA
i is the label after the attack and Î·A(xi, yi, zi) is the
probability of wA
i being 1.
More generally, the cost can be measured by using a norm
âˆ¥Â·âˆ¥. From now on, we focus on the ideal situation where the
labels wi follow the probabilities Î·(xi, yi, zi) and the loss
function is defined by the expectation of L(r) with respect
to wi, denoted as L(r; Î·). The attack target rA may be given
explicitly, or derived by some other poisoning attack. Let Î¸O
be a vector of dimension N whose elements are Î·(xi, yi, zi)
for (xi, yi, zi, wi) âˆˆDL and Î¸A a vector of dimension N
whose elements are Î·A(xi, yi, zi). The attack cost measured
by âˆ¥Â·âˆ¥is defined as âˆ¥Î¸A âˆ’Î¸Oâˆ¥, which recovers (8) if the
norm is the â„“1-norm.
The attacker tries to minimize the cost while realizing
the desired policy Ï€rA. Because of the form of the optimal
policy (4), different reward functions can lead to the same
optimal policy. For example, r(x, y) and r(x, y) + R(x) for
any R : X â†’R admit the same optimal policy. Let R(Ï€) =
{r : Ï€r = Ï€} be the set of reward functions for which the
optimal policy is Ï€. The attackerâ€™s cost minimization problem
is formulated with a vector representation Î¸ of Î· as follows:
min
Î¸
âˆ¥Î¸ âˆ’Î¸Oâˆ¥,
(9a)
s.t.
argmin
r
L(r; Î¸) âŠ†R(Ï€rA),
(9b)
âˆ¥2Î¸ âˆ’1âˆ¥âˆžâ©½1,
(9c)
where, with abuse of notation, L(r; Î¸) means L(r; Î·), and
(9c) is introduced to ensure Î· âˆˆ[0, 1]. In the following,
we investigate the minimum cost of the poisoning attack
theoretically. In particular, we are interested in the lower
and upper bounds of the minimum cost to realize the target
reward function.
Fixed Embedding
First, we consider the situation where the embedding Ï• is
fixed during the training. The proofs for the theoretical results
in this section can be found in Appendix.
The optimization problem (9) can be infeasible in cases
that the loss function L admits multiple minimum solutions
that lead to different optimal policies. However, we can derive
that the optimal reward function for the loss (2) is uniquely
determined, showing that (9) is valid, under a mild assump-
tion. Hereafter, we consider the fixed embedding situation.
Let Î¦ be a matrix of dimension n Ã— N whose i-th column
is Ï•(xi, yi) âˆ’Ï•(xi, zi). Its Moore-Penrose pseudo-inverse is
denoted as Î¦â€ . The row and column spaces of Î¦ are denoted
as row(Î¦) and col(Î¦), respectively.
To proceed theoretical analysis, we assume the following,
which is naturally satisfied if n < N.1 Intuitively, it implies
1It holds when Î¦ is of full row rank, which occurs with proba-


--- Page 4 ---
that if two reward functions agree on all data points in DU,
they must agree everywhere up to a context-dependent offset
R(x), meaning that the reward function is fully determined
by its values on the dataset.
Assumption 1. For any (x, y, z) âˆˆX Ã— Y Ã— Y, Ï•(x, y) âˆ’
Ï•(x, z) âŠ†col(Î¦).
The optimization problem (9) can be transformed as a
convex optimization problem.
Theorem 1. Suppose that Assumption 1 holds. Let Î¶ =
Î¸ âˆ’Î¸O. Then, the minimum cost poisoning attack problem (9)
is equivalently formulated as a convex optimization problem
with linear equality and inequality conditions:
min
Î¶
âˆ¥Î¶âˆ¥
s.t.
Î¦Î¶ = Î¦(Î¸A âˆ’Î¸O),
(10a)
âˆ’Î¸O â©½Î¶ â©½(1 âˆ’Î¸O),
(10b)
where â©½denotes element-wise comparison, as used hereafter.
That is, by solving the convex optimization problem (10)
and letting Î¶âˆ—be its optimal solution, one can obtain the
preference probability Î¸âˆ—
A = Î¸O + Î¶âˆ—that leads to the same
target reward function in R(Ï€rA) as Î¸A with a reduced or
equal cost âˆ¥Î¸âˆ—
A âˆ’Î¸Oâˆ¥â©½âˆ¥Î¸A âˆ’Î¸Oâˆ¥. It is irrelevant to the
way of crafting Î¸A.
This optimization problem has a convex objective function
and linear constraints. Therefore, one can obtain a solution
to this problem, Î¶âˆ—, by using a standard convex optimization
solver. In case of the â„“1 attack cost, this can be reformu-
lated as a linear programming problem by decomposing Î¶
as Î¶ = Î¶+ âˆ’Î¶âˆ’where Î¶+, Î¶âˆ’âˆˆRN
+ and rewriting âˆ¥Î¶âˆ¥as
âˆ¥Î¶âˆ¥= 1T
N(Î¶+ + Î¶âˆ’). Therefore, one can employ a linear
programming solver to obtain the optimum solution. See
Appendix for details.
By considering the Lagrangian dual problem of the primal
problem (10), we can derive the minimum attack cost bounds.
It is derived as follows. The primal problem is a convex
optimization problem with linear equality and inequality con-
straints. Therefore, if a relaxed Slater condition is satisfied,
i.e., a feasible solution exists, then the strong duality holds
and the solution to the dual problem provides the minimum
value of the primal problem. In our case, Î¶ = Î¸A âˆ’Î¸O is
a feasible solution. Hence, the strong duality holds and the
maximum value of the dual problem is the minimum value
of the primal problem.
Based on the above argument, a lower bound and an upper
bound of the minimum cost are derived.
Theorem 2. The minimum cost of (10) is lower bounded by
âˆ¥(Î¦â€ Î¦)(Î¸A âˆ’Î¸O)âˆ¥2
2
âˆ¥(Î¦â€ Î¦)(Î¸A âˆ’Î¸O)âˆ¥âˆ—
.
(11)
Theorem 3. Let Î¸âˆ—= Î¸O + (Î¦â€ Î¦)(Î¸A âˆ’Î¸O). Let Î±âˆ—=
max{âˆ¥Î¸âˆ—âˆ’0.5 Â· 1âˆ¥âˆžâˆ’0.5, 0} and Â¯Î± = 0.5 âˆ’âˆ¥Î¸A âˆ’0.5âˆ¥.
The minimum cost of (10) is upper bounded by
min

Î±âˆ—I + Â¯Î±Î¦â€ Î¦
Î±âˆ—+ Â¯Î±

(Î¸A âˆ’Î¸O)
, âˆ¥Î¸A âˆ’Î¸Oâˆ¥

.
(12)
bility 1 for random Î¦. We confirmed that all LLM feature matrices
used in the experiments satisfied this condition.
Remark 1. The matrix Î¦â€ Î¦ defines the orthogonal projec-
tion from RN to a subspace spanned by the rows of Î¦, whose
rank is at most n. If the cost is defined by the â„“2 norm, we al-
ways have âˆ¥(Î¦â€ Î¦)(Î¸A âˆ’Î¸O)âˆ¥2 â©½âˆ¥Î¸A âˆ’Î¸Oâˆ¥2. The discrep-
ancy between âˆ¥(Î¦â€ Î¦)(Î¸Aâˆ’Î¸O)âˆ¥2
2
âˆ¥(Î¦â€ Î¦)(Î¸Aâˆ’Î¸O)âˆ¥âˆ—in (11) and âˆ¥(Î¦â€ Î¦)(Î¸A âˆ’
Î¸O)âˆ¥in (12) comes from the primal-dual norm relation, i.e.,
âˆ¥Î¶âˆ¥2
2 â©½âˆ¥Î¶âˆ¥âˆ¥Î¶âˆ¥âˆ—. In this sense, these bounds are tight be-
cause the equality holds for some Î¶.
These theorems indicate that the cost of a poisoning attack
can be reduced significantly. From the defense perspective,
it suggests that the victim must be prepared for the attack to
guide the reward model arbitrarily in
Î˜A
k =

Î¸ | âˆ¥(Î¦â€ Î¦)(Î¸ âˆ’Î¸O)âˆ¥2
2
âˆ¥(Î¦â€ Î¦)(Î¸ âˆ’Î¸O)âˆ¥âˆ—
â©½k

(13)
if k data points are annotated by an untrusted annotator. It
can be significantly wider than the set corresponding to the
naive attack with cost no greater than k, i.e.,
eÎ˜A
k = {Î¸ | âˆ¥Î¸ âˆ’Î¸Oâˆ¥â©½k} ,
(14)
in particular, when the rank of Î¦â€ Î¦ is significantly smaller
than its dimension, which corresponds to the situation that
the number n of features is significantly smaller than the
number N of data points. It helps us assess the security risk
of allowing untrusted individuals to annotate data.
We investigate the influence of the choice of the embedding
Ï• (i.e., Î¦) on the minimum attack cost. Theorem 4 states that,
if the feature extractor is fixed, then the more capable the
feature extractorâ€™s representation power is, the more cost the
attacker needs to spend to realize the same target reward. It
suggests that a greater number n of features results in models
that are more robust against label flipping attacks.
Proposition 4. Suppose that Ï•1 : X Ã— Y â†’Rn1 and
Ï•2 : X Ã— Y â†’Rn2 satisfies Assumption 1 and row(Î¦1) âŠ†
row(Î¦2). If the target reward function is rA = rT
A,1Ï•1 =
rT
A,2Ï•2, then a feasible solution Î¶2 to the problem under Ï•2
is also feasible under Ï•1. Therefore, the minimum value of
the problem (10) under Î¦1 is no greater than that under Î¦2.
Adaptive Embedding
Now we consider a more general reward model
r(x, y) = âŸ¨r, Ï•Ï‰(x, y)âŸ©,
(15)
where Ï• is an adaptive embedding parameterized by Ï‰. Sup-
pose that the attackerâ€™s target reward is expressed as
rA(x, y) = âŸ¨rA, Ï•Ï‰A(x, y)âŸ©
(16)
with the reward vector rA and the parameter Ï‰A of the embed-
ding. In this situation, however, the optimization problem (9)
is not always well-defined as the reward function minimizing
L(r; Î·) may not be uniquely determined. In such situations,
whether the attack succeeds or not depends on which solution
the victimâ€™s reward model converges to. Therefore, it may
depends on the initial value of the victimâ€™s reward model and
its learning algorithm. To make the optimization problem
feasible, we relax the notion of attack success as having a


--- Page 5 ---
potential to obtain the target reward function. The relaxed
optimization problem is formulated as follows:
min
Î·
âˆ¥Î¸ âˆ’Î¸Oâˆ¥,
(17a)
s.t.
rA âˆˆargmin L(r; Î·),
(17b)
0 â©½Î·(x, y, z) â©½1, âˆ€x, y, z.
(17c)
Considering a lower bound of the minimum cost of such a
relaxed problem provides the guarantee from the victimâ€™s per-
spective that the attack cannot be successful without paying
a derived cost.
Similarly to Theorem 1, we can transform the attackerâ€™s
optimization problem (17) as follows. First, we realize that
(17b) holds if and only if a reward model r = rTÏ•Â¯Ï‰ satis-
fying rTÎ¦Â¯Ï‰ = rT
AÎ¦Ï‰A is included in argmin L(r; Î·). For
these reward functions, we have Î¸rA = Î¸r. Choose one such
Â¯Ï‰. Then, analogously to the derivation of Theorem 1, the
attackerâ€™s optimization problem reduces to
min
Î¶
âˆ¥Î¶âˆ¥
s.t. Î¦Â¯Ï‰Î¶ = Î¦Â¯Ï‰(Î¸A âˆ’Î¸O),
(18a)
Â¯Ï‰ âˆˆ{Ï‰ : âˆƒÂ¯r s.t. Â¯rTÏ•Â¯Ï‰ = rT
AÏ•Ï‰A}, (18b)
âˆ’Î¸O â©½Î¶ â©½(1 âˆ’Î¸O).
(18c)
The point here is that the attacker does not need to set Â¯Ï‰ = Ï‰A
and may choose Â¯Ï‰ such that the attack cost is the smallest.
The following result is a straightforward consequence.
Proposition 5. The minimum cost of (18) is upper bounded
by the minimum cost of (10) where Î¦ is replaced with Î¦Ï‰A.
Theorem 5 indicates that we can reduce the cost of the
attack to realize the target reward rA by solving convex (or
linear) programming (10) with Î¦ = Î¦Ï‰A. However, we em-
phasize that, differently from the case of the fixed embedding,
it does not provide the minimum cost attack due to the de-
grees of freedom in (18b), and it provides the solution to a
â€œrelaxedâ€ optimization problem (17). Therefore, guarantees
from the perspective of attackers are hard to obtain.
Now we consider the worst situation for the victim. In
light of Theorem 4, the minimum attack cost is no greater
for Â¯Ï‰ than for Ï‰A if row(Î¦Â¯Ï‰) âŠ†row(Î¦Ï‰A). Suppose that
the representational capacity of Ï•Ï‰ is high enough that there
exists Â¯Ï‰ such that col(Î¦Â¯Ï‰) = {rT
AÎ¦Ï‰A}. By assuming the
existence of such Â¯Ï‰, the attackerâ€™s optimization problem reads
min
Î¶
âˆ¥Î¶âˆ¥
s.t.
rT
AÎ¦Ï‰AÎ¶ = rT
AÎ¦Ï‰A(Î¸A âˆ’Î¸O),
(19a)
âˆ’Î¸O â©½Î¶ â©½(1 âˆ’Î¸O),
(19b)
where we used the fact that Î¦Â¯Ï‰Î¶ = Î¦Â¯Ï‰(Î¸Aâˆ’Î¸O) is equivalent
to rT
AÎ¦Ï‰AÎ¶ = rT
AÎ¦Ï‰A(Î¸Aâˆ’Î¸O). This problem can be solved
with a convex programming or a linear programming solver
as for (10) but possibly with reduced cost.
Theorem 6. Suppose that the attackerâ€™s target reward func-
tion is expressed as (16). Then, the minimum cost of (18) is
lower bounded by that of (19). Moreover, if there exists Â¯Ï‰
such that col(Î¦Â¯Ï‰) = {rT
AÎ¦Ï‰A}, the minimum cost of (18) is
equal to that of (19).
The upper and lower bounds for the cost of (19) are derived
in Theorem 2 and Theorem 3, respectively. The attacker need
not Â¯Ï‰ explicitly; solving (19) requires only the target reward
function rA. However, in reality, considering the minimum
cost of (19) as the minimum cost for the attack Î¸A may be
too conservative from the defense perspective. Such an attack
will not realize the target reward function in practice due
to the solution multiplicity, suboptimal optimization, and
the fact that a Â¯Ï‰ satisfying col(Î¦Â¯Ï‰) = {rT
AÎ¦Ï‰A} does not
necessarily exist in general.
Practical Post-Processing Method
Based on the above theoretical analysis, we propose a prac-
tical post-processing method to minimize the cost of any
existing label-flipping attack. Given a target preference prob-
ability vector Î¸A, either hand-crafted or generated by an
existing attack, we solve the convex optimization problem in
(10) to obtain a cost-minimized vector Î¸âˆ—
A that induces the
same target reward model while requiring fewer label flips.
In the case of adaptive embeddings, we use the initial embed-
ding Ï• of the reward model to form the optimization problem
(10) as if it were fixed. A discrepancy exists between our the-
oretical analysis and practical implementation, as the analysis
assumes knowledge of the embedding parameter Ï‰A corre-
sponding to the target reward rA. However, as demonstrated
later, the practical approach remains effective in reducing the
cost without deteriorating the attackâ€™s performance.
After obtaining Î¸âˆ—
A, we discretize it by rounding so that the
preference vector takes values in
Î˜m =

Î¸ âˆˆRN : [Î¸]k = i
m for i âˆˆ{0, . . . , m}

,
(20)
where [Î¸]k indicates the kth element of Î¸ and m is the number
of annotations per datum, referred to as the granularity. Fi-
nally, we flip the preference labels in the dataset to follow the
discretized vector. We refer to this post-processing method
as Poisoning Cost Minimization (PCM).
Importantly, PCM is agnostic to how the target Î¸A is gen-
erated and can be layered onto any label-flipping attack, pro-
viding a systematic way to reduce poisoning costs while
preserving attack efficacy. We apply this post-processing in
our empirical evaluations to demonstrate its effectiveness
across synthetic and real LLM alignment datasets.
Numerical Analysis on Synthetic Data
We demonstrate the tightness of the bounds and how small
the minimum cost can be compared to the cost of the naive
attack. In particular, we show that the minimum cost can be
significantly reduced from the naive cost when the number of
data points is significantly greater than the number of features.
For this purpose, we generate synthetic data and compare the
cost of the cost minimized attack and the naive attack.
Dataset
We produce a synthetic dataset DU with embed-
dings Ï•(xi, yi) and Ï•(xi, zi) âˆˆRn for (xi, yi, zi) âˆˆDU
generated randomly from the standard normal distribution.
Without loss of generality, the first response yi is considered
to be the preferred response in the original annotation, i.e.,
Î¸O = 1. The dataset size is N = |DU|. To simulate the situa-
tion where multiple annotators are assigned to provide their
preferences for the same tuples, we duplicate each datum m


--- Page 6 ---
times. That is, Î¸O as well as annotation probability after the
poisoning attack, Î¸A, can take values in Î˜m.
Attack Scenario
By nature of this synthetic dataset, we
consider the situation where the embedding is fixed. We
suppose that we have a target attack preference probability
Î¸A âˆˆÎ˜m. The attacker tries to minimize the attack cost
measured by â„“1-norm, âˆ¥Î¸ âˆ’Î¸Oâˆ¥1. We consider two attack
targets: 1) Î¸A is generated by flipping each element of Î¸O
with probability 0.1; 2) Î¸A is generated by RLHFPoison
(Wang et al. 2024) with quality filter parameter a = 0.25 and
final poisoning ratio b = 0.1. RLHFPoison originally gen-
erates a dataset to make the LLM output a longer response
without significantly changing the other aspects. Here, in-
stead of the output length, the first feature of the output is
to be maximized. That is, the reward signals for data with
greater first feature values are to be maximized. For each Î¸A,
we apply the proposed post-processing, PCM, to obtain the
cost-minimized preference probability Î¸âˆ—
A.
Performance Metric
We focus on two metrics. The first
one is the â„“1-cost âˆ¥Î¸ âˆ’Î¸Oâˆ¥1 after the discretization. Due to
the discretization, the performance of the attack by PCM may
degrade. To measure the performance degradation by PCM,
we minimize the loss function (2) for r = rTÏ• with respect
to r, where the preference labels are given by Î¸O, Î¸A, and Î¸âˆ—
A
(discretized). Letting the optimal reward functions obtained
with the above preference datasets be denoted as rO, rA, and
râˆ—
A. Then, we compute the performance loss rate
PN
i=1|Ïƒ(râˆ—
A(x, y) âˆ’râˆ—
A(x, z)) âˆ’Ïƒ(rA(x, y) âˆ’rA(x, z))|
PN
i=1|Ïƒ(rA(x, y) âˆ’rA(x, z)) âˆ’Ïƒ(rO(x, y) âˆ’rO(x, z))|
.
(21)
It measures the average preference difference between the
trained reward models using the target Î¸A and its cost-
minimized Î¸âˆ—
A relative to that between the trained reward
models using Î¸A and the original Î¸O.
Results
The results are shown in Figures 1 and 2. Although
the algorithms are all deterministic, the datasets are randomly
generated. Therefore, we performed 5 trials with different
dataset generations. The findings are summarized as follows.
(1) Though it is not guaranteed by Theorem 3, âˆ¥(Î¦â€ Î¦)(Î¸A âˆ’
Î¸O)âˆ¥1 provides a good upper bound of the proposed scheme
when it is smaller than the naive cost âˆ¥Î¸A âˆ’Î¸Oâˆ¥1. The results
all fit between the lower bound provided in Theorem 2 and
this value, and the discrepancy between them is around the
factor of 3 to 4. (2) There is a higher chance to reduce the
attack cost for a larger data set, i.e., greater N if the original
attack cost âˆ¥Î¸A âˆ’Î¸Oâˆ¥1 is more or less constant (i.e., the
flip rate is fixed). It may be understood as that the rank of
Î¦â€ Î¦ (â©½n) will be smaller than its dimension (N). (3) The
cost does not depend heavily on the granularity m, but a
smaller performance loss rate can be achieved with a greater
m. It is intuitive because the cost minimization process does
not affect the trained reward function if no discretization
is performed. Even with m = 1 (i.e., each data point is
annotated by a single annotator), the performance loss rate
can be around 0.1 if N is sufficiently large. (4) The cost
reduction effect increases linearly in the dataset size N for
104
105
number of samples N
10âˆ’2
10âˆ’1
performance loss rate
104
105
number of samples N
10âˆ’2
10âˆ’1
cost: âˆ¥Î¸ âˆ’Î¸Oâˆ¥1/N
m=1
m=2
m=3
m=4
m=5
m=10
minimized
original Î¸A
lower bound
âˆ¥Î¦â€ Î¦(Î¸A âˆ’Î¸O)âˆ¥1
(a) #features n = 1000
104
105
number of samples N
10âˆ’1
performance loss rate
104
105
number of samples N
10âˆ’1
cost: âˆ¥Î¸ âˆ’Î¸Oâˆ¥1/N
(b) #features n = 3000
Figure 1: Cost (right) and performance loss rate (21) (left) of
the proposed cost minimization, PCM, for random flip attack.
Results of 5 trials (points) as well as their median (lines).
Minimized: the cost of Î¸âˆ—
A before discretization, Original: the
cost of Î¸A, Lower bound: (11), âˆ¥Î¦â€ Î¦(Î¸A âˆ’Î¸O)âˆ¥1: a term
appearing in the upper bound (12). The other lines are the
performance loss rate and the cost of the proposed attack with
discretization using different granularity m. Missing data
points in the preference loss rate indicate no performance
loss because Î¸A = Î¸âˆ—
A (no cost reduction as well).
the random-flip attack, whereas its scaling is lower for the
attack by RLHFPoison. Nevertheless, a similar trend (the
cost can be reduced when N â‰³5n) is observed.
Evaluation on Public LLMs and Open Dataset
We demonstrate the cost reduction achieved by the pro-
posed framework across different LLMs on publicly available
datasets. In particular, we show that the proposed method
remains effective even when the entire LLM is trained using
DPO (adaptive embedding scenario), despite the framework
itself being derived under the fixed embedding assumption.
Datasets and Models
We employ three datasets of vary-
ing size and three models of varying size. The datasets are
SOCIAL-REASONING-RLHF (N = 3, 820) (ProlificAI 2024),
PKU-SAFERLHF (N = 73, 907) (Ji et al. 2024), and HH-
RLHF (N = 160, 800) (Bai et al. 2022). Since SOCIAL-
REASONING-RLHF dataset does not contain a test set, we
used 20% of the training data for testing. The models are
Phi-3.5-mini-instruct (n = 3072) (Abdin et al.
2024), LLaMA-2-7b (n = 4096), and LLaMA-2-13b
(n = 5120) (Touvron et al. 2023).


--- Page 7 ---
104
105
number of samples N
10âˆ’2
10âˆ’1
performance loss rate
104
105
number of samples N
10âˆ’1
2 Ã— 10âˆ’2
3 Ã— 10âˆ’2
4 Ã— 10âˆ’2
6 Ã— 10âˆ’2
m=1
m=2
m=3
m=4
m=5
m=10
minimized
original Î¸A
lower bound
âˆ¥Î¦â€ Î¦(Î¸A âˆ’Î¸O)âˆ¥1
(a) #features n = 1000
104
105
number of samples N
10âˆ’2
10âˆ’1
performance loss rate
104
105
number of samples N
10âˆ’1
cost: âˆ¥Î¸ âˆ’Î¸Oâˆ¥1/N
(b) #features n = 3000
Figure 2: Cost (right) and performance loss rate (left) of the
proposed cost minimization, PCM, for RLHFPoison attack.
See the caption of Figure 1 for details.
Attack Scenario
The attack target is generated using RL-
HFPoison, which aims to increase the output length of an
LLM without significantly affecting other behavioral charac-
teristics. The quality filter parameter and final poisoning ratio
are set to a = 0.25 and b = 0.05, respectively. Consequently,
5% of the preference labels in the training dataset are flipped.
PCM is applied after obtaining Î¸A via RLHFPoison.
Performance Metric
LLMs are trained by DPO (3 epochs
with Ï„ = 0.1 and learning rate 10âˆ’6) with the original pref-
erence Î¸O, the malicious preference Î¸A generated by RLHF-
Poison, and the preference Î¸âˆ—
A generated by PCM from Î¸A.
We measure the cost (flip rate) reduction rate of Î¸âˆ—
A over Î¸A
(5%), i.e., flip rate of Î¸âˆ—
A divided by 0.05. Moreover, we mea-
sure the average output length of LLMs on the test dataset to
assess the performance drop due to the cost minimization. As
the output length significantly varies over contexts, we stan-
dardize each output length â„“with the corresponding output
length â„“O of the LLM trained on the original preference by
(â„“âˆ’â„“O)/â„“O. We call it the output length increase rate.
Results
We confirm that PCM is still effective in this prac-
tical scenario, summarized in Table 1. Although the RLHF-
Poison+PCM resulted in reducing the output length increase
rates compared to RLHFPoison itself on HH-RLHF dataset,
it keeps the effect of increasing the output length. PCM suc-
cessfully reduces the label flip rate compared to RLHFPoison.
As expected, a greater dataset size allows more cost reduc-
tion. Meanwhile, no cost reduction effect can be observed on
SOCIAL-REASONING-RLHF, where the number n of features
RLHFPoison
RLHFPoison+PCM
PKU-SafeRLHF
Phi-3.5-mini
0.44 Â± 0.01
0.40 Â± 0.01 (âˆ’13.4%)
Llama-2-7b
0.29 Â± 0.02
0.29 Â± 0.01 (âˆ’10.6%)
Llama-2-13b
0.25 Â± 0.01
0.37 Â± 0.01 (âˆ’8.2%)
HH-RLHF
Phi-3.5-mini
0.55 Â± 0.02
0.27 Â± 0.02 (âˆ’30.4%)
Llama-2-7b
1.08 Â± 0.36
0.87 Â± 0.05 (âˆ’29.8%)
Llama-2-13b
1.63 Â± 0.55
1.27 Â± 0.15 (âˆ’20.0%)
Table 1: Average Â± standard error of output length increase
rate for RLHFPoison and RLHFPoison+PCM, and cost re-
duction rate for PCM (in parenthesis).
of the models is greater than the number of training data,
hence the results are omitted. Further details and results are
provided in Appendix.
Discussion
This work establishes a theoretical foundation for understand-
ing the vulnerability of RLHF/DPO pipelines to label-flipping
poisoning attacks. Our theoretical results include: lower and
upper bounds for the minimum attack cost in the fixed em-
bedding case (Theorem 2, Theorem 3); the implication that
attacks on models with smaller feature dimensions may suc-
ceed at a smaller cost than those targeting models with greater
feature dimensions (Theorem 4, fixed embedding case); the
finding that attacks in the adaptive embedding scenario are
no more difficult than in the fixed embedding scenario if the
target embedding is known (Theorem 5); and that attacks
can succeed with a significantly small cost in the worst-case
adaptive embedding scenario (Theorem 6). As a byproduct of
this analysis, we propose a general post-processing method
to minimize attack cost. This framework can be combined
with any label-flipping attack to reduce its cost while pre-
serving the intended poisoning effect, thereby significantly
improving the efficiency of existing attacks and contributing
to more effective stress-testing for red-teaming efforts.
We conclude by outlining the limitations of the current
study and identifying promising future directions. Our current
analysis relies on idealizationsâ€”assuming optimal reward
model recovery, exact attacker knowledge of reward function
structure, and direct preference probability modificationâ€”
while practical factors are not theoretically accounted for,
despite empirical validation of our cost minimization on syn-
thetic data. Future work should tackle these discrepancies,
possibly by evaluating performance loss (deviation from the
target reward function) to deepen our understanding of vulner-
ability. Furthermore, while our adaptive embedding analysis
reveals crucial worst-case scenarios, the results (e.g., Theo-
rem 6) are conservative; evaluating performance loss under
realistic assumptions such as bounded embedding changes
represents another promising avenue. These identified areas
highlight that a significant contribution of this work lies in
opening up numerous critical directions for future research.


--- Page 8 ---
Acknowledgements
This work was partially supported by JSPS KAKENHI Grant
Number 23H00483 and JST K Program Japan Grant Number
JPMJKP24C3.
References
Abdin, M.; Aneja, J.; Awadalla, H.; Awadallah, A.; Awan,
A. A.; Bach, N.; Bahree, A.; Bakhtiari, A.; Bao, J.; Behl, H.;
Benhaim, A.; Bilenko, M.; Bjorck, J.; Bubeck, S.; Cai, M.;
Cai, Q.; Chaudhary, V.; Chen, D.; Chen, D.; Chen, W.; Chen,
Y.-C.; Chen, Y.-L.; Cheng, H.; Chopra, P.; Dai, X.; Dixon, M.;
Eldan, R.; Fragoso, V.; Gao, J.; Gao, M.; Gao, M.; Garg, A.;
Giorno, A. D.; Goswami, A.; Gunasekar, S.; Haider, E.; Hao,
J.; Hewett, R. J.; Hu, W.; Huynh, J.; Iter, D.; Jacobs, S. A.;
Javaheripi, M.; Jin, X.; Karampatziakis, N.; Kauffmann, P.;
Khademi, M.; Kim, D.; Kim, Y. J.; Kurilenko, L.; Lee, J. R.;
Lee, Y. T.; Li, Y.; Li, Y.; Liang, C.; Liden, L.; Lin, X.; Lin, Z.;
Liu, C.; Liu, L.; Liu, M.; Liu, W.; Liu, X.; Luo, C.; Madan, P.;
Mahmoudzadeh, A.; Majercak, D.; Mazzola, M.; Mendes, C.
C. T.; Mitra, A.; Modi, H.; Nguyen, A.; Norick, B.; Patra, B.;
Perez-Becker, D.; Portet, T.; Pryzant, R.; Qin, H.; Radmilac,
M.; Ren, L.; de Rosa, G.; Rosset, C.; Roy, S.; Ruwase, O.;
Saarikivi, O.; Saied, A.; Salim, A.; Santacroce, M.; Shah,
S.; Shang, N.; Sharma, H.; Shen, Y.; Shukla, S.; Song, X.;
Tanaka, M.; Tupini, A.; Vaddamanu, P.; Wang, C.; Wang, G.;
Wang, L.; Wang, S.; Wang, X.; Wang, Y.; Ward, R.; Wen, W.;
Witte, P.; Wu, H.; Wu, X.; Wyatt, M.; Xiao, B.; Xu, C.; Xu,
J.; Xu, W.; Xue, J.; Yadav, S.; Yang, F.; Yang, J.; Yang, Y.;
Yang, Z.; Yu, D.; Yuan, L.; Zhang, C.; Zhang, C.; Zhang, J.;
Zhang, L. L.; Zhang, Y.; Zhang, Y.; Zhang, Y.; and Zhou, X.
2024. Phi-3 Technical Report: A Highly Capable Language
Model Locally on Your Phone. arXiv:2404.14219.
Bai, Y.; Jones, A.; Ndousse, K.; Askell, A.; Chen, A.; Das-
Sarma, N.; Drain, D.; Fort, S.; Ganguli, D.; Henighan, T.;
Joseph, N.; Kadavath, S.; Kernion, J.; Conerly, T.; El-Showk,
S.; Elhage, N.; Hatfield-Dodds, Z.; Hernandez, D.; Hume, T.;
Johnston, S.; Kravec, S.; Lovitt, L.; Nanda, N.; Olsson, C.;
Amodei, D.; Brown, T.; Clark, J.; McCandlish, S.; Olah, C.;
Mann, B.; and Kaplan, J. 2022. Training a Helpful and Harm-
less Assistant with Reinforcement Learning from Human
Feedback. arXiv:2204.05862.
BaumgÃ¤rtner, T.; Gao, Y.; Alon, D.; and Metzler, D. 2024.
Best-of-Venom: Attacking RLHF by Injecting Poisoned Pref-
erence Data. In First Conference on Language Modeling.
Chao, P.; Robey, A.; Dobriban, E.; Hassani, H.; Pappas, G. J.;
and Wong, E. 2023. Jailbreaking black box large language
models in twenty queries. arXiv preprint arXiv:2310.08419.
Ethayarajh, K.; Xu, W.; Muennighoff, N.; Jurafsky, D.; and
Kiela, D. 2024. Model alignment as prospect theoretic opti-
mization. In Proceedings of the 41st International Confer-
ence on Machine Learning, ICMLâ€™24. JMLR.org.
Gheshlaghi Azar, M.; Daniel Guo, Z.; Piot, B.; Munos, R.;
Rowland, M.; Valko, M.; and Calandriello, D. 2024. A Gen-
eral Theoretical Paradigm to Understand Learning from Hu-
man Preferences. In Dasgupta, S.; Mandt, S.; and Li, Y., eds.,
Proceedings of The 27th International Conference on Artifi-
cial Intelligence and Statistics, volume 238 of Proceedings
of Machine Learning Research, 4447â€“4455. PMLR.
Greshake, K.; Abdelnabi, S.; Mishra, S.; Endres, C.; Holz, T.;
and Fritz, M. 2023. Not What Youâ€™ve Signed Up For: Com-
promising Real-World LLM-Integrated Applications with
Indirect Prompt Injection. In Proceedings of the 16th ACM
Workshop on Artificial Intelligence and Security, AISec â€™23,
79â€“90. New York, NY, USA: Association for Computing
Machinery. ISBN 9798400702600.
Ji, J.; Liu, M.; Dai, J.; Pan, X.; Zhang, C.; Bian, C.; Chen, B.;
Sun, R.; Wang, Y.; and Yang, Y. 2024. Beavertails: Towards
improved safety alignment of llm via a human-preference
dataset. Advances in Neural Information Processing Systems,
36.
Lin, Y.; Seto, S.; Ter Hoeve, M.; Metcalf, K.; Theobald, B.-J.;
Wang, X.; Zhang, Y.; Huang, C.; and Zhang, T. 2024. On
the Limited Generalization Capability of the Implicit Reward
Model Induced by Direct Preference Optimization. In Al-
Onaizan, Y.; Bansal, M.; and Chen, Y.-N., eds., Findings
of the Association for Computational Linguistics: EMNLP
2024, 16015â€“16026. Miami, Florida, USA: Association for
Computational Linguistics.
Liu, Y.; Jia, Y.; Geng, R.; Jia, J.; and Gong, N. Z. 2024. For-
malizing and Benchmarking Prompt Injection Attacks and
Defenses. In 33rd USENIX Security Symposium (USENIX
Security 24), 1831â€“1847. Philadelphia, PA: USENIX Associ-
ation. ISBN 978-1-939133-44-1.
Mehrotra, A.; Zampetakis, M.; Kassianik, P.; Nelson, B.;
Anderson, H.; Singer, Y.; and Karbasi, A. 2024. Tree of
Attacks: Jailbreaking Black-Box LLMs Automatically. In
Globerson, A.; Mackey, L.; Belgrave, D.; Fan, A.; Paquet,
U.; Tomczak, J.; and Zhang, C., eds., Advances in Neural
Information Processing Systems, volume 37, 61065â€“61105.
Curran Associates, Inc.
Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;
Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;
Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens, M.;
Askell, A.; Welinder, P.; Christiano, P. F.; Leike, J.; and Lowe,
R. 2022. Training language models to follow instructions
with human feedback. In Koyejo, S.; Mohamed, S.; Agarwal,
A.; Belgrave, D.; Cho, K.; and Oh, A., eds., Advances in
Neural Information Processing Systems, volume 35, 27730â€“
27744. Curran Associates, Inc.
Pathmanathan, P.; Chakraborty, S.; Liu, X.; Liang, Y.; and
Huang, F. 2024a. Is poisoning a real threat to LLM align-
ment? Maybe more so than you think.
arXiv preprint
arXiv:2406.12091.
Pathmanathan, P.; Sehwag, U. M.; Panaitescu-Liess, M.-A.;
and Huang, F. 2024b. AdvBDGen: Adversarially Fortified
Prompt-Specific Fuzzy Backdoor Generator Against LLM
Alignment. arXiv preprint arXiv:2410.11283.
ProlificAI. 2024. ProlificAI/social-reasoning-rlhf. https://
huggingface.co/datasets/ProlificAI/social-reasoning-rlhf. Ac-
cessed: 2025-07-23.
Rafailov, R.; Sharma, A.; Mitchell, E.; Manning, C. D.; Er-
mon, S.; and Finn, C. 2023. Direct Preference Optimiza-
tion: Your Language Model is Secretly a Reward Model. In
Thirty-seventh Conference on Neural Information Processing
Systems.


--- Page 9 ---
Shayegani, E.; Mamun, M. A. A.; Fu, Y.; Zaree, P.; Dong, Y.;
and Abu-Ghazaleh, N. 2023a. Survey of Vulnerabilities in
Large Language Models Revealed by Adversarial Attacks.
arXiv:2310.10844.
Shayegani, E.; Mamun, M. A. A.; Fu, Y.; Zaree, P.; Dong, Y.;
and Abu-Ghazaleh, N. 2023b. Survey of Vulnerabilities in
Large Language Models Revealed by Adversarial Attacks.
arXiv:2310.10844.
Shu, M.; Wang, J.; Zhu, C.; Geiping, J.; Xiao, C.; and Gold-
stein, T. 2023. On the Exploitability of Instruction Tuning. In
Oh, A.; Naumann, T.; Globerson, A.; Saenko, K.; Hardt, M.;
and Levine, S., eds., Advances in Neural Information Process-
ing Systems, volume 36, 61836â€“61856. Curran Associates,
Inc.
Swamy, G.; Dann, C.; Kidambi, R.; Wu, Z. S.; and Agar-
wal, A. 2024. A minimaximalist approach to reinforcement
learning from human feedback. In Proceedings of the 41st
International Conference on Machine Learning, ICMLâ€™24.
JMLR.org.
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;
Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,
S.; Bikel, D.; Blecher, L.; Ferrer, C. C.; Chen, M.; Cucurull,
G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.; Gao,
C.; Goswami, V.; Goyal, N.; Hartshorn, A.; Hosseini, S.; Hou,
R.; Inan, H.; Kardas, M.; Kerkez, V.; Khabsa, M.; Kloumann,
I.; Korenev, A.; Koura, P. S.; Lachaux, M.-A.; Lavril, T.; Lee,
J.; Liskovich, D.; Lu, Y.; Mao, Y.; Martinet, X.; Mihaylov, T.;
Mishra, P.; Molybog, I.; Nie, Y.; Poulton, A.; Reizenstein, J.;
Rungta, R.; Saladi, K.; Schelten, A.; Silva, R.; Smith, E. M.;
Subramanian, R.; Tan, X. E.; Tang, B.; Taylor, R.; Williams,
A.; Kuan, J. X.; Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y.; Fan,
A.; Kambadur, M.; Narang, S.; Rodriguez, A.; Stojnic, R.;
Edunov, S.; and Scialom, T. 2023. Llama 2: Open Foundation
and Fine-Tuned Chat Models. arXiv:2307.09288.
TramÃ¨r, F.; and Rando Ramirez, J. 2024. Universal Jailbreak
Backdoors from Poisoned Human Feedback. In The Twelfth
International Conference on Learning Representations (ICLR
2024). OpenReview.
Wan, A.; Wallace, E.; Shen, S.; and Klein, D. 2023. Poisoning
language models during instruction tuning. In International
Conference on Machine Learning, 35413â€“35425. PMLR.
Wang, J.; Wu, J.; Chen, M.; Vorobeychik, Y.; and Xiao, C.
2024. RLHFPoison: Reward Poisoning Attack for Reinforce-
ment Learning with Human Feedback in Large Language
Models. In Ku, L.-W.; Martins, A.; and Srikumar, V., eds.,
Proceedings of the 62nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long Papers),
2551â€“2570. Bangkok, Thailand: Association for Computa-
tional Linguistics.
Wei, A.; Haghtalab, N.; and Steinhardt, J. 2023. Jailbroken:
How Does LLM Safety Training Fail? In Oh, A.; Naumann,
T.; Globerson, A.; Saenko, K.; Hardt, M.; and Levine, S.,
eds., Advances in Neural Information Processing Systems,
volume 36, 80079â€“80110. Curran Associates, Inc.
Wu, J.; Wang, J.; Xiao, C.; Wang, C.; Zhang, N.; and Vorob-
eychik, Y. 2025. Preference Poisoning Attacks on Reward
Model Learning . In 2025 IEEE Symposium on Security
and Privacy (SP), 94â€“94. Los Alamitos, CA, USA: IEEE
Computer Society.
Xiao, H.; Xiao, H.; and Eckert, C. 2012. Adversarial label
flips attack on support vector machines. In Proceedings
of the 20th European Conference on Artificial Intelligence,
ECAIâ€™12, 870â€“875. NLD: IOS Press. ISBN 9781614990970.
Zou, A.; Wang, Z.; Carlini, N.; Nasr, M.; Kolter, J. Z.; and
Fredrikson, M. 2023. Universal and transferable adversar-
ial attacks on aligned language models.
arXiv preprint
arXiv:2307.15043.


--- Page 10 ---
Proofs
Proof of Theorem 1
To prove Theorem 1, we show the following lemmas, whose proofs are provided in the following subsections.
Lemma 7. If Assumption 1 holds, then rT
1 Î¦ = rT
2 Î¦ implies that rT
1 (Ï•(x, y) âˆ’Ï•(x, z)) = rT
2 (Ï•(x, y) âˆ’Ï•(x, z)) for all
(x, y, z) âˆˆX Ã— Y Ã— Y.
Lemma 8. For two reward functions Â¯r and r, r âˆˆR(Ï€Â¯r) if and only if we have for any (x, y, z),
r(x, y) âˆ’r(x, z) = Â¯r(x, y) âˆ’Â¯r(x, z).
(22)
Equivalently, letting Î·r be the preference probability associated with r, defined as Î·r(x, y, z) = Ïƒ(r(x, y) âˆ’r(x, z)), we have
R(Ï€Â¯r) = {r : Î·r = Î·Â¯r}.
Lemma 9. If Assumption 1 holds, then for a given r(x, y) = rTÏ•(x, y),
R(Ï€r) = {râ€² : Î·râ€²(x, y, z) = Î·r(x, y, z), âˆ€(x, y, z) âˆˆDU}.
(23)
Lemma 10. Let Â¯r be a reward vector and Î¸A a vector of dimension N whose i-th element is Â¯Î·(xi, yi, zi) = Ïƒ(âŸ¨Â¯r, Ï•(xi, yi) âˆ’
Ï•(xi, zi)âŸ©). Then, Â¯r is a solution to argminr L(rTÏ•; Î¸) if and only if Î¸ satisfies
Î¦Î¸ = Î¦Î¸A,
(24)
Moreover, if Assumption 1 holds and Î· satisfies (24), then
argmin
r
L(r; Î¸) = {Â¯r = Â¯rTÏ•}.
(25)
Utilizing these lemmas, we prove Theorem 1 as follows.
Proof. Let Î¸r denote the vector representation of the preference probability Î·r defined by the reward function r. Theorem 9
states that R(Ï€rA) = {r : Î¸r = Î¸rA}. Because Î¸A is the preference probability that leads to the target reward function rA, we
have Î¸A = Î¸rA That is, for any reward function included in R(Ï€rA), the vector representation of the corresponding probability
is Î¸A. Theorem 10 states that argminr L(rTÏ•; Î¸) = {r : Î¦Î¸r = Î¦Î¸}. Therefore, (9b) is satisfied if and only if Î¸ is such that
{r : Î¦Î¸r = Î¦Î¸} âŠ†{r : Î¸r = Î¸A}. It reduces to select Î¸ such that Î¦Î¸A = Î¦Î¸. It is equivalent to Î¦(Î¸A âˆ’Î¸O) = Î¦(Î¸ âˆ’Î¸O). The
rest is trivial. This completes the proof.
Proof of Theorem 7
Proof. Suppose that rT
1 Î¦ = rT
2 Î¦. It implies that r2 = r1 + (I âˆ’Î¦Î¦â€ )âˆ†for some âˆ†âˆˆRn. Because of Assumption 1,
Ï•(x, y) âˆ’Ï•(x, z) âˆˆcol(Î¦) for any (x, y, z) âˆˆX Ã— Y Ã— Y, implying that (I âˆ’Î¦Î¦â€ )(Ï•(x, y) âˆ’Ï•(x, z)) = 0. Therefore,
rT
2 (Ï•(x, y) âˆ’Ï•(x, z)) = (r1 + (I âˆ’Î¦Î¦â€ )âˆ†)T(Ï•(x, y) âˆ’Ï•(x, z))
(26a)
= rT
1 (Ï•(x, y) âˆ’Ï•(x, z)) + âˆ†T(I âˆ’Î¦Î¦â€ )(Ï•(x, y) âˆ’Ï•(x, z))
(26b)
= rT
1 (Ï•(x, y) âˆ’Ï•(x, z))
(26c)
for any (x, y) âˆˆX Ã— Y.
Proof of Theorem 8
Proof. The optimal policies under the reward model r and Â¯r are coincide if and only if for all (x, y, z),
Ï€r(y | x)
Ï€r(z | x) = Ï€Â¯r(y | x)
Ï€Â¯r(z | x).
(27)
Because of the form of the optimal policy (4), it is equivalent to have
r(x, y) âˆ’r(x, z) = Â¯r(x, y) âˆ’Â¯r(x, z).
(28)
This completes the proof.
Proof of Theorem 9
Proof. Suppose that Ï€r = Ï€râ€². Then, in light of Theorem 8,
Ï€r = Ï€râ€² â‡â‡’(r(x, y) âˆ’r(x, z)) âˆ’(râ€²(x, y) âˆ’râ€²(x, z)) = 0
âˆ€(x, y, z) âˆˆX Ã— Y Ã— Y
(29a)
=â‡’(r(x, y) âˆ’r(x, z)) âˆ’(râ€²(x, y) âˆ’râ€²(x, z)) = 0
âˆ€(x, y, z) âˆˆDU
(29b)
â‡â‡’Î¦T(r âˆ’râ€²) = 0N.
(29c)
In light of Theorem 7, the right-most side of the above relation implies that r(x, y) = râ€²(x, y) for all (x, y) âˆˆX Ã— Y. This
completes the proof.


--- Page 11 ---
Proof of Theorem 10
Proof. Because
Î¦Î¸ =
N
X
i=1
(Ï•(xi, yi) âˆ’Ï•(xi, zi))Î¸i
(30)
and
Î¦Â¯Î¸ =
N
X
i=1
(Ï•(xi, yi) âˆ’Ï•(xi, zi))Ïƒ(âŸ¨Â¯r, Ï•(xi, yi) âˆ’Ï•(xi, zi)âŸ©),
(31)
(24) is equivalent to
N
X
i=1
(Ï•(xi, yi) âˆ’Ï•(xi, zi)) (Î¸i âˆ’Ïƒ(âŸ¨Â¯r, Ï•(xi, yi) âˆ’Ï•(xi, zi)âŸ©))) = 0.
(32)
We first show that (32) is a necessary condition for Â¯r to minimize L(r; Î·) for a given Î·. The partial derivative of the loss with
respect to rk is
âˆ‚
âˆ‚rk
L(r; Î·)= âˆ’âˆ‚
âˆ‚rk
N
X
i=1
 Î¸i log(Ïƒ(âŸ¨r, Ï•(xi, yi) âˆ’Ï•(xi, zi)âŸ©))
+ (1 âˆ’Î¸i) log(Ïƒ(âŸ¨r, Ï•(xi, zi) âˆ’Ï•(xi, zi)âŸ©))

(33a)
= âˆ’
N
X
i=1
Î¸i(1 âˆ’Ïƒ(âŸ¨r, Ï•(xi, yi) âˆ’Ï•(xi, zi)âŸ©))(Ï•k(xi, yi) âˆ’Ï•k(xi, zi))
+ (1 âˆ’Î¸i)(1 âˆ’Ïƒ(âŸ¨r, Ï•(xi, zi) âˆ’Ï•(xi, yi)âŸ©))(Ï•k(xi, zi) âˆ’Ï•k(xi, yi))
(33b)
= âˆ’
N
X
i=1
(Ï•k(xi, yi) âˆ’Ï•k(xi, zi))

Î¸i(1 âˆ’Ïƒ(âŸ¨r, Ï•(xi, yi) âˆ’Ï•(xi, zi)âŸ©))
âˆ’(1 âˆ’Î¸i)(1 âˆ’Ïƒ(âŸ¨r, Ï•(xi, zi) âˆ’Ï•(xi, yi)âŸ©))

(33c)
= âˆ’
N
X
i=1
(Ï•k(xi, yi) âˆ’Ï•k(xi, zi))

Î¸i(1 âˆ’Ïƒ(âŸ¨r, Ï•(xi, yi) âˆ’Ï•(xi, zi)âŸ©))
âˆ’(1 âˆ’Î¸i)Ïƒ(âŸ¨r, Ï•(xi, yi) âˆ’Ï•(xi, zi)âŸ©))

(33d)
= âˆ’
N
X
i=1
(Ï•k(xi, yi) âˆ’Ï•k(xi, zi))

Î¸i âˆ’Ïƒ(âŸ¨r, Ï•(xi, yi) âˆ’Ï•(xi, zi)âŸ©))

.
(33e)
Therefore, the first order necessary condition for optimality is given by (32).
Then, we show that (32) is also a sufficient condition for optimality by showing that the loss function L is convex. The
second-order derivative is
âˆ‚2
âˆ‚rkâˆ‚râ„“
L(r; Î·) = âˆ’âˆ‚
âˆ‚râ„“
N
X
i=1
(Ï•k(xi, yi) âˆ’Ï•k(xi, zi)) (Î¸i âˆ’(Ïƒ(âŸ¨r, Ï•(xi, yi) âˆ’Ï•(xi, zi)âŸ©)))
(34a)
=
N
X
i=1
(Ï•k(xi, yi) âˆ’Ï•k(xi, zi)) âˆ‚
âˆ‚râ„“
Ïƒ(âŸ¨r, Ï•(xi, yi) âˆ’Ï•(xi, zi)âŸ©))
(34b)
=
N
X
i=1
(Ï•k(xi, yi) âˆ’Ï•k(xi, zi))(Ï•â„“(xi, yi) âˆ’Ï•â„“(xi, zi))
Â· Ïƒ(âŸ¨r, Ï•(xi, yi) âˆ’Ï•(xi, zi)âŸ©)(1 âˆ’Ïƒ(âŸ¨r, Ï•(xi, yi) âˆ’Ï•(xi, zi)âŸ©)).
(34c)
Let Î¸r be the preference probability corresponding to r = rTÏ•. Letting diag(Î¸r) be the diagonal matrix whose diagonal elements
are the elements of Î¸r, we can write the Hessian matrix of L as
âˆ‡âˆ‡L(r; Î·) = Î¦ diag(Î¸r)(I âˆ’diag(Î¸r))Î¦T.
(35)


--- Page 12 ---
Because diag(Î¸r)(I âˆ’diag(Î¸r)) is positive semi-definite as each element of Î¸r takes a value in (0, 1), so is Î¦ diag(Î¸r)(I âˆ’
diag(Î¸r))Î¦T. Therefore, L is a convex function and the first-order optimality condition is also a sufficient condition.
Finally, we show that Â¯r = Â¯rTÏ• is the unique solution under Assumption 1. Suppose that there is a reward vector r such that it
minimizes L and Â¯r(x, y) Ì¸= rTÏ•(x, y) for some (x, y) âˆˆX Ã— Y. In light of Theorem 7, it implies that Î¦TÂ¯r Ì¸= Î¦Tr. However,
(Â¯r âˆ’r)Tâˆ‡âˆ‡L(Â¯r; Î·)(Â¯r âˆ’r) = (Î¦TÂ¯r âˆ’Î¦Tr)T diag(Î¸r)(I âˆ’diag(Î¸r))(Î¦TÂ¯r âˆ’Î¦Tr) Ì¸= 0
(36)
because diag(Î¸Â¯r)(I âˆ’diag(Î¸Â¯r)) is positive definite. It contradicts to that r is a solution to minr L(r; Î·). This completes the
proof.
Proof of Theorem 11
Lemma 11. The minimum value of (10) is
sup âˆ’Î»TÎ¦(Â¯Î¸ âˆ’Î¸O) âˆ’Î½T
1 (1 âˆ’Î¸O) âˆ’Î½T
0 Î¸O,
(37)
where sup is taken over Î» âˆˆRn and Î½1, Î½0 âˆˆRm
+ satisfying âˆ¥âˆ’Î¦TÎ» âˆ’Î½1 + Î½0âˆ¥âˆ—â©½1, and âˆ¥Â·âˆ¥âˆ—is the dual norm of âˆ¥Â·âˆ¥defined
as
âˆ¥Î¾âˆ¥âˆ—= max
âˆ¥Î¶âˆ¥â©½1 Î¾TÎ¶.
(38)
Proof. Let C =

I
âˆ’I

and b =

1 âˆ’Î¸O
Î¸O

. The Lagrangian of (10) is defined as
L(Î¶, Î», Î½) = âˆ¥Î¶âˆ¥+ Î»T(Î¦Î¶ âˆ’Î¦(Î¸A âˆ’Î¸O)) + Î½T(CÎ¶ âˆ’b),
(39)
where Î» âˆˆRn and Î½ = (Î½1, Î½0) âˆˆR2N
+ are the dual variables. The Lagrangian dual function is
g(Î», Î½) = inf
Î¶ L(Î¶, Î», Î½)
(40a)
= âˆ’Î»TÎ¦(Î¸A âˆ’Î¸O) âˆ’Î½Tb + inf
Î¶

âˆ¥Î¶âˆ¥+ Î»TÎ¦Î¶ + Î½TCÎ¶
	
(40b)
= âˆ’Î»TÎ¦(Î¸A âˆ’Î¸O) âˆ’Î½Tb + inf
Î¶

âˆ¥Î¶âˆ¥+ (Î»TÎ¦ + Î½TC)Î¶
	
(40c)
= âˆ’Î»TÎ¦(Î¸A âˆ’Î¸O) âˆ’Î½Tb âˆ’sup
Î¶

(âˆ’Î»TÎ¦ âˆ’Î½TC)Î¶ âˆ’âˆ¥Î¶âˆ¥
	
(40d)
= âˆ’Î»TÎ¦(Î¸A âˆ’Î¸O) âˆ’Î½Tb âˆ’f âˆ—(âˆ’Î»TÎ¦ âˆ’Î½TC)
(40e)
where f âˆ—is the convex conjugate of f(Î¶) = âˆ¥Î¶âˆ¥, which is
f âˆ—(Î¾) = sup
Î¶
{Î¾TÎ¶ âˆ’f(Î¶)} =
0
âˆ¥Î¾âˆ¥âˆ—â©½1
âˆž
otherwise.
(41)
Because there is a feasible solution, i.e., Î¶ = Î¸A âˆ’Î¸O, the relaxed Slater condition holds. Because the constraints are all affine, it
implies that the strong duality holds. Therefore, we have that the optimal solution Î¶âˆ—satisfies
f(Î¶âˆ—) = min
Î¶
sup
Î»,Î½
L(Î¶, Î», Î½)
(42a)
= sup
Î»,Î½
g(Î», Î½)
(42b)
=
sup
Î»âˆˆRn,Î½âˆˆR2N
+ ,âˆ¥âˆ’Î»TÎ¦âˆ’Î½TCâˆ¥âˆ—â©½1
âˆ’Î»TÎ¦(Î¸A âˆ’Î¸O) âˆ’Î½Tb
(42c)
=
sup
Î»âˆˆRn,Î½1,Î½0âˆˆRN
+ ,âˆ¥âˆ’Î»TÎ¦âˆ’Î½T
1 +Î½T
0 âˆ¥âˆ—â©½1
âˆ’Î»TÎ¦(Î¸A âˆ’Î¸O) âˆ’Î½T
1 (1 âˆ’Î¸O) âˆ’Î½T
0 Î¸O
(42d)
This completes the proof.
Proof of Theorem 2
Proof. By letting Î½1 = Î½0 = 0 in (37), we have
f(Î¶âˆ—) =
sup
Î½1,Î½0âˆˆRN
+
sup
Î»âˆˆRn,âˆ¥(âˆ’Î»TÎ¦âˆ’Î½T
1 +Î½T
0 )âˆ¥âˆ—â©½1

âˆ’Î»TÎ¦(Î¸A âˆ’Î¸O)
	
âˆ’Î½T
1 (1 âˆ’Î¸O) âˆ’Î½T
0 Î¸O
(43a)
â©¾
sup
Î»âˆˆRn,âˆ¥âˆ’Î»TÎ¦âˆ¥âˆ—â©½1

âˆ’Î»TÎ¦(Î¸A âˆ’Î¸O)
	
.
(43b)


--- Page 13 ---
Let.
Î» = âˆ’
(Î¦â€ )T(Î¸A âˆ’Î¸O)
âˆ¥(Î¦â€ Î¦)(Î¸A âˆ’Î¸O)âˆ¥âˆ—
.
(44)
Noting that
Î¦T(Î¦â€ )T = (Î¦â€ Î¦)T = Î¦â€ Î¦,
(45)
we have âˆ¥âˆ’Î¦TÎ»âˆ¥âˆ—= 1. Moreover,
âˆ’Î»TÎ¦(Î¸A âˆ’Î¸O) = (Î¸A âˆ’Î¸O)TÎ¦T(Î¦â€ )T(Î¸A âˆ’Î¸O)
âˆ¥(Î¦â€ Î¦)(Î¸A âˆ’Î¸O)âˆ¥âˆ—
(46a)
= (Î¸A âˆ’Î¸O)Î¦â€ Î¦Î¦â€ Î¦(Î¸A âˆ’Î¸O)
âˆ¥(Î¦â€ Î¦)(Î¸A âˆ’Î¸O)âˆ¥âˆ—
(46b)
= âˆ¥Î¦â€ Î¦(Î¸A âˆ’Î¸O)âˆ¥2
2
âˆ¥(Î¦â€ Î¦)(Î¸A âˆ’Î¸O)âˆ¥âˆ—
.
(46c)
This completes the proof.
Proof of Theorem 3
Proof. Let Î¶relax = Î¸âˆ—âˆ’Î¸O and Î¶trivial = Î¸A âˆ’Î¸O. It is straightforward to see that Î¶relax is a feasible solution to (10)
without the inequality constraints. Moreover, it is trivial to see that Î¶trivial is a feasible solution to (10). We show that Î¶ =
(1 âˆ’Î±)Î¶relax + Î±Î¶trivial is a feasible solution to (10) for Î± =
Î±âˆ—
Î±âˆ—+Â¯Î±.
First, we check the equality constraints. Note that Î¦Î¶relax = Î¦Î¶trivial = Î¦(Î¸A âˆ’Î¸O). Therefore,
Î¦Î¶ = (1 âˆ’Î±)Î¦Î¶relax + Î±Î¦Î¶trivial
(47a)
= (1 âˆ’Î±)Î¦(Î¸A âˆ’Î¸O) + Î±Î¦(Î¸A âˆ’Î¸O)
(47b)
= Î¦(Î¸A âˆ’Î¸O).
(47c)
Hence, the equality constraints are satisfied.
Next, we check the inequality constraints. The inequality constraints can be written as âˆ¥Î¸ âˆ’0.5âˆ¥âˆžâ©½0.5. Letting Î¸ =
Î¸O + Î¶ =
Â¯Î±
Î±âˆ—+Â¯Î±Î¸âˆ—+
Î±âˆ—
Î±âˆ—+Â¯Î±Î¸A, we have

Â¯Î±
Î±âˆ—+ Â¯Î±Î¸âˆ—+
Î±âˆ—
Î±âˆ—+ Â¯Î±Î¸A âˆ’0.5

âˆž
(48a)
=

Â¯Î±
Î±âˆ—+ Â¯Î±(Î¸âˆ—âˆ’0.5) +
Î±âˆ—
Î±âˆ—+ Â¯Î±(Î¸A âˆ’0.5)

âˆž
(48b)
â©½
Â¯Î±
Î±âˆ—+ Â¯Î±âˆ¥Î¸âˆ—âˆ’0.5âˆ¥âˆž+
Î±âˆ—
Î±âˆ—+ Â¯Î±âˆ¥Î¸A âˆ’0.5âˆ¥âˆž
(48c)
â©½
Â¯Î±
Î±âˆ—+ Â¯Î±(Î±âˆ—+ 0.5) +
Î±âˆ—
Î±âˆ—+ Â¯Î±(0.5 âˆ’Â¯Î±)
(48d)
= 0.5.
(48e)
Hence, the inequality constraints are satisfied with Î¶. Rewriting Î¶ as Î¶ = (Î¸âˆ—âˆ’Î¸O) + Î±(Î¸A âˆ’Î¸âˆ—), we completes the proof.
Proof of Theorem 4
Proof. A feasible solution to the problem under Î¦2 is also feasible under Î¦1 because
Î¦2Î¶ = Î¦2(Î¸A âˆ’Î¸O)
(49a)
â‡â‡’Î¦â€ 
2Î¦2Î¶ = Î¦â€ 
2Î¦2(Î¸A âˆ’Î¸O)
(49b)
=â‡’Î¦â€ 
1Î¦1Î¦â€ 
2Î¦2Î¶ = Î¦â€ 
1Î¦1Î¦â€ 
2Î¦2(Î¸A âˆ’Î¸O)
(49c)
â‡â‡’Î¦â€ 
1Î¦1Î¶ = Î¦â€ 
1Î¦1(Î¸A âˆ’Î¸O)
(49d)
â‡â‡’Î¦1Î¶ = Î¦1(Î¸A âˆ’Î¸O).
(49e)
Therefore, the optimal solution Î¶âˆ—
2 to the problem under Î¦2 is a feasible solution to the problem under Î¦1. The optimal solution
to the latter problem, Î¶âˆ—
1, must hold âˆ¥Î¶âˆ—
1âˆ¥â©½âˆ¥Î¶âˆ—
2âˆ¥.


--- Page 14 ---
Proof of Theorem 6
Proof. To express the attackerâ€™s reward function rA, Ï•Ï‰ needs to be selected such that the row space of Î¦Ï‰ includes rT
AÎ¦Ï‰A.
Indeed, it is a necessary and sufficient condition for the existence of a reward vector Â¯r satisfying Â¯rTÎ¦Â¯Ï‰ = rT
AÎ¦Ï‰A.
Then, if Â¯Î¶ satisfies (18a), we have
Î¦Â¯Ï‰Î¶ = Î¦Â¯Ï‰(Î¸A âˆ’Î¸O)
(50a)
â‡â‡’Î¦â€ 
Â¯Ï‰Î¦Â¯Ï‰Î¶ = Î¦â€ 
Â¯Ï‰Î¦Â¯Ï‰(Î¸A âˆ’Î¸O)
(50b)
=â‡’(rT
AÎ¦Ï‰A)Î¦â€ 
Â¯Ï‰Î¦Â¯Ï‰Î¶ = (rT
AÎ¦Ï‰A)Î¦â€ 
Â¯Ï‰Î¦Â¯Ï‰(Î¸A âˆ’Î¸O)
(50c)
â‡â‡’rT
AÎ¦Ï‰AÎ¶ = rT
AÎ¦Ï‰A(Î¸A âˆ’Î¸O),
(50d)
where we used the fact that (rT
AÎ¦Ï‰A)Î¦â€ 
Â¯Ï‰Î¦Â¯Ï‰ = rT
AÎ¦Ï‰A because rT
AÎ¦Ï‰A âˆˆrow(Î¦Â¯Ï‰) and Î¦â€ 
Â¯Ï‰Î¦Â¯Ï‰ is the projection onto the row
space of Î¦Â¯Ï‰. Therefore, Â¯Î¶ satisfies (19a) as well. Analogously to the proof of Theorem 4, a solution to (18) is a solution to (19).
Therefore, the minimum cost of (18) is no greater than that of (19). This completes the proof.
Linear Programming Transformation
We consider the following minimization problem:
min
x
âˆ¥A(x âˆ’a)âˆ¥1 + Î»âˆ¥B(x âˆ’b)âˆ¥1
(51a)
s.t.
0 â©½x â©½1,
(51b)
C(x âˆ’c) = 0
(51c)
where Î» â©¾0, a, b âˆˆ[0, 1]n, A, B âˆˆRnÃ—n. This problem can be transformed as a linear programming problem as
min
(x,u,w)
[0
. . .
0
1
. . .
1
Î»
. . .
Î»]
"x
u
w
#
(52a)
s.t.
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
I
O
O
âˆ’I
O
O
A
âˆ’I
O
âˆ’A
âˆ’I
O
B
O
âˆ’I
âˆ’B
O
âˆ’I
ï£¹
ï£ºï£ºï£ºï£ºï£ºï£»
"x
u
w
#
â©½
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
1
0
Aa
âˆ’Aa
Bb
âˆ’Bb
ï£¹
ï£ºï£ºï£ºï£ºï£ºï£»
(52b)
[C
O
O]
"x
u
w
#
= Cc
(52c)
It is derived as follows. First, we introduce slack variables, u = (u1, . . . , un) and w = (w1, . . . , wn), that satisfy
âˆ’ui â©½[A(x âˆ’a)]i â©½ui
(53a)
âˆ’wi â©½[B(x âˆ’b)]i â©½wi
(53b)
0 â©½ui
(53c)
0 â©½wi
(53d)
for all i = 1, . . . , n. Then, we have
âˆ¥A(x âˆ’a)âˆ¥1 + Î»âˆ¥B(x âˆ’b)âˆ¥1 = min
(u,w)
n
X
i
ui + Î»
n
X
i=1
wi,
(54)
where (u, w) must satisfy the above conditions. Combining them with (51), we obtain (52).
Poisoning Cost Minimization Algorithm
Algorithm 1 describes the algorithm of the proposed cost minimization post-processing.
Additional Experiment Details and Results
Datasets
To test the effectiveness of the proposed cost reduction mechanism for different dataset sizes, we selected SOCIAL-
REASONING-RLHF, PKU-SAFERLHF and HH-RLHF, the latter two of which are often used for the context of safety alignment in
related works, including Wang et al. (2024). SOCIAL-REASONING-RLHF is selected to test the performance of the proposed
approach on a small dataset. Because SOCIAL-REASONING-RLHF does not have a predefined test dataset, we split the dataset
into the training dataset and the test dataset, which is constructed by randomly selecting 20% of the data.


--- Page 15 ---
Algorithm 1: Poisoning Cost Minimization (PCM)
Require: Target preference probability vector Î¸A (either hand-crafted or from an existing attack such as RLHFPoison
Require: Initial embedding Ï• of the reward model
Require: Granularity m (number of annotations per datum)
Ensure: Cost-minimized and discretized preference vector Î¸âˆ—
A, and modified dataset
1: Solve the convex optimization problem (10) to obtain a cost-minimized vector Î¶âˆ—. If the cost is defined by â„“1-norm, turn the
original problem into the corresponding linear programming problem (52), then solve the linear programming problem.
2: Construct the continuous version of Î¸âˆ—
A as Î¸âˆ—
A = Î¸O + Î¶âˆ—
3: Discretize the continuous version of Î¸âˆ—
A by rounding each element [Î¸âˆ—
A]k for k = 1, . . . , N as: [Î¸âˆ—
A]k â†1
m ROUND([Î¸âˆ—
A]k Â· m)
4: Assign m preference labels for each datum (xi, yi, zi) âˆˆDU by following the preference probability [Î¸âˆ—
A]i as
wi,j =
1
j â©½m Â· [Î¸âˆ—
A]i
âˆ’1
otherwise
for j = 1, . . . , m
5: Construct the labeled dataset DL as DL = {(xi, yi, zi, wi,j)}j=1,...,m
i=1,...,N
DPO Training Settings
LLMs are fine-tuned using DPO for 3 epochs with an inverse temperature of Ï„ = 0.1. AdamW
optimizer is used with an initial learning rate of 10âˆ’6 and a linear learning rate scheduling. These hyperparameter settings follow
common practice in related work, such as Lin et al. (2024). The training is done by using trl library (version 0.11.4) with
transformers (version 4.45.2) provided by Hugging Face. The minibatch size is 8, selected based on available GPU memory.
Experiments are conducted on a Red Hat Enterprise Linux 9.4 system equipped with 8Ã— NVIDIA H200 SXM5 GPUs with
141GB HBM3e memory each and Intel Xeon Platinum 8558 Processor (260 MB Cache, 2.1 GHz, 48 Cores, 96 Threads).
Evaluation Metric
To compare the output lengths of models trained on the original dataset, the malicious dataset constructed
via RLHFPoison, and the PCM-refined malicious dataset, we fine-tune each model using DPO from the same initial SFT
checkpoint. We then provide the input prompts from the test set to each trained model and collect the outputs. To prevent
excessive resource consumption, the maximum output length is capped at 1024 tokens. We compute the output length increase
rate for each model. For HH-RLHF, a few test prompts result in zero-length outputs from the model trained on the original dataset
(i.e., the first token is EOS). To avoid division-by-zero errors, we omit these cases from the evaluation.
Output Length Distribution
Figure 3 shows the histograms of output lengths for each model and dataset. Compared to the
model trained on the original dataset, those trained on the malicious datasets exhibit a shift in probability mass from shorter
outputs to longer ones. The output length distributions for models trained on RLHFPoison and RLHFPoison+PCM are similar,
but PCM achieves this effect with reduced poisoning cost.


--- Page 16 ---
0
1000
2000
3000
4000
5000
0.0000
0.0002
0.0004
0.0006
0.0008
0.0010
0.0012
Original
RLHFPoison
RLHFPoison+PCM
0
1000
2000
3000
4000
5000
6000
0.00000
0.00025
0.00050
0.00075
0.00100
0.00125
0.00150
Original
RLHFPoison
RLHFPoison+PCM
0
1000
2000
3000
4000
5000
6000
0.0000
0.0002
0.0004
0.0006
0.0008
Original
RLHFPoison
RLHFPoison+PCM
0
2000
4000
6000
8000
0.0000
0.0002
0.0004
0.0006
0.0008
Original
RLHFPoison
RLHFPoison+PCM
0
2000
4000
6000
8000
10000
0.0000
0.0001
0.0002
0.0003
0.0004
0.0005
Original
RLHFPoison
RLHFPoison+PCM
0
2000
4000
6000
0.0000
0.0002
0.0004
0.0006
Original
RLHFPoison
RLHFPoison+PCM
0
2500 5000 7500 100001250015000
0.0000
0.0001
0.0002
0.0003
0.0004
0.0005
Original
RLHFPoison
RLHFPoison+PCM
0
2500 5000 7500 100001250015000
0.0000
0.0001
0.0002
0.0003
0.0004
Original
RLHFPoison
RLHFPoison+PCM
0
5000
10000
15000
0.0000
0.0001
0.0002
0.0003
0.0004
Original
RLHFPoison
RLHFPoison+PCM
Figure 3: Output length distribution. Top: SOCIAL-REASONING-RLHF, Middle: PKU-SAFERLHF, Bottom: HH-RLHF. Left:
Phi-3.5-mini, Center: LLaMA-2-7b, Right: LLaMA-2-13b.
