--- Page 1 ---
Automated Algorithmic Discovery for
Scientific Computing through LLM-Guided
Evolutionary Search: A Case Study in
Gravitational-Wave Detection
He Wang1,2* and Liang Zeng3*
1International Centre for Theoretical Physics Asia-Pacific,
University of Chinese Academy of Sciences, 100190, Beijing,
China.
2Taiji Laboratory for Gravitational Wave Universe, University of
Chinese Academy of Sciences, 100049, Beijing, China.
3Tsinghua University, 100084, Beijing, China.
*Corresponding author(s). E-mail(s): hewang@ucas.ac.cn;
zengliangcs@gmail.com;
Abstract
Automated algorithm discovery in scientific computing faces fun-
damental challenges: vast design spaces with expensive evaluations,
domain-specific physical constraints requiring expert knowledge, and
the necessity for interpretable solutions that scientists can validate and
understand. We present the Evo-MCTS (Evolutionary Monte Carlo
Tree Search) framework, integrating large language models (LLMs)
with tree-structured evolutionary search for interpretable algorithm dis-
covery. Evo-MCTS combines reflective code synthesis leveraging LLM
domain knowledge, multi-scale evolutionary operations on structured
code representations, and interpretable algorithmic pathways emerg-
ing from tree-guided exploration. When applied to gravitational wave
detection—a challenging domain with continuous parameter spaces
and strict physical constraints—Evo-MCTS achieves 20.2% improve-
ment over domain-specific methods and 59.1% over LLM-based opti-
mization frameworks. This improvement arises from its ability to
consistently converge toward interpretable algorithmic structures that
1
arXiv:2508.03661v3  [cs.AI]  16 Nov 2025


--- Page 2 ---
2
LLM-guided automated algorithmic discovery
integrate multiple functional components. Our domain-agnostic archi-
tecture establishes a generalizable methodology for automated algo-
rithm discovery in scientific computing, where algorithmic transparency
and physical validity are as essential as performance optimization.
1 Introduction
The pursuit of scientific discovery increasingly demands computational
approaches that can navigate complex, high-dimensional data spaces while pre-
serving physical interpretability [1–4]. Across diverse scientific domains—from
molecular dynamics to astrophysical signal detection—algorithm design rep-
resents a critical bottleneck where manual engineering approaches struggle
to navigate vast combinatorial spaces of possible computational strategies
while maintaining domain validity. Gravitational wave astronomy exemplifies
this computational challenge: detection systems must identify faint astrophys-
ical signals buried in detector noise while leveraging theoretical predictions
from general relativity [5, 6], yet remain adaptable to unexpected signal
morphologies that could reveal new physics beyond current theoretical models.
Despite the impressive progress in gravitational wave detection [7], cur-
rent approaches face core limitations stemming from restrictive assumptions.
Matched filtering techniques [8, 9] exhibit optimal sensitivity under Gaussian
stationary noise but fundamentally depend on accurate prior knowledge of
signal morphologies. Non-template methods [10] attempt model-independent
detection, but typically accept decreased sensitivity in exchange for generality.
More recently, deep neural network approaches [11–13] bring computational
efficiency, yet operate as black-box systems that conceal decision-making logic
and introduce potential hidden biases [14]. These strategies all manifest trade-
offs between sensitivity, generality, and interpretability; thus far, none fully
resolve the challenge of discovering innovative detection algorithms beyond
traditional boundaries.
While advances in computational power and data availability continue to
accelerate scientific progress, the design of effective algorithms has become the
central bottleneck in modern scientific computing [3, 4]. Current gravitational
wave detection pipelines represent decades of manual engineering yet still miss
potentially discoverable signals due to three critical limitations: (i) combinato-
rial explosion of possible signal processing combinations that exceeds human
exploration capacity [15, 16], (ii) cognitive biases that constrain designers to
familiar paradigms [17], and (iii) local optimization traps where manual refine-
ment leads to incremental improvements while missing global optima [18].
These factors highlight the need for approaches that can automate the sys-
tematic exploration of algorithm design spaces without sacrificing scientific
rigor.
Automated algorithm discovery in scientific domains confronts three inter-
twined demands: physical constraint adherence, efficient exploration of vast


--- Page 3 ---
LLM-guided automated algorithmic discovery
3
design spaces, and interpretable solutions that remain scientifically transpar-
ent. Meeting these requirements in unison is essential for credible scientific
progress. Large Language Models (LLMs) embed domain knowledge and phys-
ical constraints directly into the generative process [19, 20], while Monte
Carlo Tree Search (MCTS) drives efficient, structured navigation through
the algorithmic landscape [21–23]. Their integration enables an evolutionary
search process that systematically uncovers interpretable, high-performing,
and physically valid scientific algorithms.
Existing approaches to algorithm discovery span multiple paradigms. Tra-
ditional approaches including genetic programming [24], neural architecture
search [15], and evolutionary computation [16] suffer from critical limitations:
generating syntactically invalid code, lacking domain knowledge integration, or
focusing on network topology rather than algorithmic logic. Recent advances
have integrated LLMs with structured search and evolutionary methods,
including FunSearch [25], EoH [26], AEL [27], ReEvo [28], and MCTS-AHD [29]
for algorithm discovery, which combine LLMs’ code generation capabilities
with systematic optimization frameworks. However, the existing LLM-based
frameworks focus on combinatorial optimization tasks with discrete deci-
sion sequences and well-defined mathematical formulations. Scientific signal
processing problems like gravitational wave detection present fundamentally
different challenges, requiring continuous parameter optimization, domain-
specific physical constraints, and interpretable algorithmic pathways that can
be validated against theoretical predictions.
We propose Evo-MCTS (Evolutionary Monte Carlo Tree Search), a frame-
work that realizes synergistic integration through three key innovations:
(i) Reflective Code Synthesis that leverages LLM domain knowledge for
performance-driven algorithm generation, adapting to optimization landscapes
while maintaining scientific validity, (ii) Multi-Scale Evolutionary Operations
(Parent/Sibling/Path-wise Crossover, Point Mutation) that operate on struc-
tured code representations through MCTS tree traversal, enabling semantic-
aware algorithmic transformations, and (iii) Interpretable Algorithm Pathways
that emerge naturally from the tree structure, enabling post-hoc analysis of
algorithmic evolution while providing cumulative learning for future discover-
ies. The framework addresses interdependent challenges through architectural
design—each element enhances others’ capabilities while compensating for
individual limitations.
We demonstrate the effectiveness of Evo-MCTS through application to
gravitational wave detection—a challenging scientific computing domain with
continuous parameter spaces and strict physical constraints. Comprehensive
evaluation shows the framework achieves 20.2% performance improvement over
domain-specific state-of-the-art algorithms and a remarkable 59.1% improve-
ment over other LLM-based algorithm optimization frameworks, validating
the methodological innovations through systematic algorithmic exploration.
The framework demonstrates consistent breakthrough discoveries across inde-
pendent executions, generating human-interpretable algorithmic pathways


--- Page 4 ---
4
LLM-guided automated algorithmic discovery
that reveal distinct evolutionary patterns organized by functional categories.
These interpretable pathways illustrate the framework’s capability to navi-
gate complex algorithmic design spaces while maintaining scientific validity
and transparency, identifying multi-component integration strategies through
performance-driven optimization.
While this study focuses on gravitational wave detection as a challenging
testbed, Evo-MCTS offers a transferable framework for interpretable algorith-
mic discovery that generalizes to broader domains of scientific computing. Its
capacity to generate interpretable and high-performing algorithmic pathways
makes it especially valuable for scientific applications where understanding
algorithmic reasoning is as important as achieving optimal performance. Our
approach opens new avenues for automated scientific discovery across physics,
chemistry, biology, and engineering, providing a principled methodology that
respects domain-specific constraints while systematically exploring algorithmic
possibilities beyond human intuition.
2 Results
2.1 Framework Design and Implementation
Framework Overview. The Evo-MCTS framework systematically trans-
forms raw time-series data into a comprehensive catalog of optimized detection
algorithms through an automated discovery pipeline that integrates domain
knowledge encoded in LLMs (Figure 1a, see Methods Section 4.1 for formal
problem definition). To understand this automated transformation process,
the framework can be conceptualized through two complementary perspectives
(Figure 1b): as an MCTS-guided tree search where nodes represent complete
algorithmic implementations and edges encode LLM-driven transformations
(detailed implementation in Methods Section 4.2), or as an evolutionary
algorithm where populations of algorithms undergo sophisticated selection,
crossover, and mutation operations guided by domain knowledge (detailed
implementation in Methods Section 4.3).
Evolutionary Search Operations. The core innovation of our Evo-
MCTS framework lies in reformulating algorithm design as a tree search
problem, where each node represents an executable algorithm and edges cor-
respond to code transformations (Figure 1c). Starting from a seed algorithm,
the framework employs four specialized evolutionary operations to expand the
search tree:
• Parent Crossover (PC): Combines algorithmic features from parent nodes to
generate offspring that inherit successful detection strategies while exploring
new combinations.
• Sibling Crossover (SC): Enables horizontal knowledge transfer between
algorithms at the same tree depth, promoting diversity while maintaining
comparable complexity levels.


--- Page 5 ---
LLM-guided automated algorithmic discovery
5
Fig. 1 LLM-Guided Evolutionary Monte Carlo Tree Search Framework for
Automated Algorithm Discovery. (a) Overview of the algorithm discovery pipeline.
Starting from raw gravitational wave strain data (left), the framework applies automated
algorithmic transformations through LLM-generated code synthesis (center) to produce opti-
mized detection statistics (right). (b) Core architectural components showing the integration
of MCTS exploration with evolutionary optimization through dual perspectives of tree search
and population evolution. (b.1) UCT-based node selection from initial algorithmic vari-
ants including seed algorithms and individual variants, each represented as nodes containing
baseline signal processing code. (b.2) MCTS expansion phase where new algorithmic vari-
ants are generated through evolutionary operations. Each node contains executable Python
code implementing specific detection strategies. (b.3) Algorithm evaluation phase where
generated variants are tested against benchmark data to compute fitness scores, determin-
ing performance-based selection for subsequent iterations. (b.4) MCTS backpropagation
and elite node updates after multiple evolutionary cycles, propagating performance feedback
through the tree structure and maintaining diverse high-performing detection strategies.
(c) Detailed view of the reflection mechanism during MCTS expansion, showing four evo-
lutionary operations: Parent Crossover, Sibling Crossover, Path-wise Crossover, and Point
Mutation.


--- Page 6 ---
6
LLM-guided automated algorithmic discovery
Fig. 2 LLM-Driven Algorithmic Evolution Through Reflective Code Synthesis.
Demonstration of a single Parent Crossover evolutionary step showing the transformation
from two parent algorithms to an enhanced offspring algorithm. (Top row) Code segments
from two parent nodes highlighting complementary algorithmic components that will be
combined through the crossover operation. (Bottom left, black box) Reflective analysis
process showing how the LLM identifies strengths and limitations in the parent algorithms,
synthesizing insights about their respective detection strategies and potential synergies.
(Bottom right) Generated offspring algorithm code incorporating successful elements from
both parents while addressing identified limitations through domain-aware synthesis. This
example illustrates the framework’s capability to generate physically-motivated algorith-
mic improvements through automated reasoning, demonstrating how LLM-guided reflection
enables discovery of sophisticated signal processing techniques by combining and enhanc-
ing existing algorithmic components without manual intervention. The complete reflection
prompts and additional evolution examples are provided in Supplementary Information
Section S1.
• Path-wise Crossover (PWC): Synthesizes information across complete root-
to-leaf trajectories, capturing long-range dependencies and enabling global
optimization strategies.
• Point Mutation (PM): Introduces targeted modifications to individual
algorithms based on performance analysis, leveraging insights from elite
algorithms to enable fine-grained optimization of specific components.
These operations are fundamentally different from traditional genetic algo-
rithms because they operate on structured code representations rather than
abstract encodings, and modifications are guided by LLM-based reasoning
rather than random perturbations (see Methods Section 4.3 and Supplemen-
tary Information Section S1 for operation details).
Reflective Code Generation. Central to the Evo-MCTS framework’s
effectiveness is the reflection mechanism that analyzes algorithm performance
patterns and guides subsequent explorations (Figure 1b-(2) and Figure 1c).
This dual-component system comprises: (i) a performance reflection module
that identifies strengths and weaknesses in current algorithms through sys-
tematic evaluation across diverse signal conditions, and (ii) a code synthesis
module that leverages these insights to generate improved implementations


--- Page 7 ---
LLM-guided automated algorithmic discovery
7
(An example is shown in Figure 2). The reflection process operates at mul-
tiple scales—from individual algorithmic components to complete detection
pipelines—ensuring both local optimization and global coherence (detailed
prompts and examples in Supplementary Information Section S1).
2.2 Optimization Dynamics and Benchmark Comparison
We evaluated our Evo-MCTS framework’s algorithmic discovery capabilities
using the Machine Learning Gravitational-Wave Search Mock Data Challenge
(MLGWSC-1) benchmark dataset [30]—a standardized evaluation environ-
ment that provides a controlled testbed for systematic framework validation.
The MLGWSC-1 benchmark offers a challenging application domain that com-
bines realistic detector noise characteristics with diverse signal morphologies,
enabling demonstration of the framework’s effectiveness on scientific comput-
ing problems requiring sophisticated signal processing under domain-specific
physical constraints.
Performance Evaluation Protocol. Figure 3a illustrates the Evo-
MCTS framework’s adaptation to the MLGWSC-1 evaluation protocol,
demonstrating the framework’s capability to integrate with domain-specific
assessment procedures. The system transforms raw dual-channel strain data
through evolved algorithms, producing detection catalogs evaluated against
ground truth labels. The evaluation pipeline employs the area under the curve
(AUC) metric as the performance indicator, computed from sensitivity dis-
tance versus false alarm rate curves spanning 4-1000 events per month. This
metric provides a comprehensive performance measure that balances multiple
detection objectives, enabling systematic framework validation through stan-
dardized benchmark assessment (detailed experimental configuration provided
in Methods Section 4.5).
Evolutionary Optimization Trajectory. Figure 3b presents the com-
prehensive optimization trajectory across 877 total evaluations from five
independent Evo-MCTS runs, revealing systematic algorithmic discovery with
progressive increases in both algorithmic sophistication and detection perfor-
mance. The optimization process exhibits four distinct phase transitions (PT
1-4), each marking algorithmic breakthroughs that represent the discovery
of increasingly sophisticated algorithmic components building upon previous
innovations. The combined fitness trajectory demonstrates the framework’s
capability to navigate the complex algorithmic design space while maintaining
consistent improvement patterns across multiple independent runs, systemati-
cally exploring and integrating complex detection strategies (detailed analysis
of algorithmic evolution patterns provided in Supplementary Information
Section S3).
Population Diversity Analysis. The diversity analysis reveals sophisti-
cated exploration patterns that balance algorithmic variety with performance
optimization throughout the search process. Peak diversity occurs during
the intermediate optimization phase between PT 2 and PT 3, where both
Shannon diversity index and Complexity Index of Diversity reach maximum


--- Page 8 ---
8
LLM-guided automated algorithmic discovery
values, indicating the framework maintains diversity in both component selec-
tion and structural complexity while systematically exploring combinations of
successful components before converging on optimal configurations. Fitness-
stratified analysis demonstrates systematic convergence: diversity decreases
progressively from broad initial exploration in lower-performing variants to
focused refinement in highest-performing configurations, confirming effective
balance between exploration and exploitation (diversity metric definitions and
computational details provided in Methods Section 4.5).
Experimental Comparison. Figure 3c compares the Evo-MCTS frame-
work’s performance against seven algorithms on MLGWSC-1, Set 4 dataset,
demonstrating the framework’s effectiveness in navigating complex algorithmic
design spaces. The PT-4 configuration achieves 20.2% performance improve-
ment over competing approaches including Sage [14], Virgo-AUTh [30, 31],
PyCBC [30, 32], TPI FSU Jena [30, 33], cWB [30, 34], MFCNN [30, 35], and
CNN-Coinc [30, 36, 37]. Progressive algorithmic sophistication is demonstrated
through four milestone configurations (PT-1 through PT-4) across the false
alarm rate range of 4-1000 events per month, validating the framework’s sys-
tematic exploration and refinement capabilities (detailed performance metrics
provided in Supplementary Information Section S3).
Algorithmic Evolution. The systematic improvement from PT-1 to
PT-4 demonstrates the framework’s capability to discover and integrate sophis-
ticated algorithmic components through interpretable evolutionary pathways.
The evolved algorithms incorporate nonlinear transformations and adaptive
processing strategies that respond to the benchmark’s realistic noise char-
acteristics [5, 38], illustrating how LLM-guided evolutionary search can sys-
tematically explore complex algorithmic design spaces under domain-specific
constraints.
The framework’s performance relative to existing approaches validates
key methodological capabilities: compared to template-based methods like
PyCBC [32] that optimize for specific theoretical assumptions [9, 39], our
framework demonstrates automated exploration of alternative algorithmic
paradigms. Relative to template-free approaches like coherent WaveBurst
(cWB) [10, 40], the framework shows how systematic LLM-guided search can
navigate algorithmic spaces beyond manual heuristic design [26, 41]. Com-
pared to deep learning methods with millions of opaque parameters [42], the
evolved algorithms maintain interpretability through explicit mathematical
formulations [43, 44]. These results confirm that automated discovery can
achieve competitive performance while preserving transparency, which is essen-
tial for scientific applications where algorithmic reasoning must be validated
by domain experts (detailed quantitative analysis and algorithm specifications
are provided in Supplementary Information Section S3).
2.3 Generalization Study and Component Analysis
Training-Test Correlation. We evaluated 877 algorithmic configurations on
an independent 1-day test dataset, distinct from the 7-day training corpus,


--- Page 9 ---
LLM-guided automated algorithmic discovery
9
Fig. 3 Framework Optimization Dynamics and Performance Validation on
MLGWSC-1 Benchmark. (a) Evo-MCTS framework adaptation pipeline demonstrating
integration with domain-specific evaluation protocols through standardized fitness assess-
ment. The pipeline processes input data through evolved algorithms to produce outputs
evaluated against ground truth labels using area under the curve (AUC) metrics. (b)
Framework optimization trajectory and diversity analysis across 877 evaluations from 5 inde-
pendent runs. Combined fitness trajectory (blue dots) with best objective envelope (red line)
showing four phase transitions (PT 1-4, orange stars) marking algorithmic breakthroughs
with fitness gains ≥400 units. Maximum fitness of 5,241 units achieved, representing a 6-
fold improvement from baseline. Diversity metrics include Shannon diversity index (blue,
left axis) and Complexity Index of Diversity (CID, red, right axis) with error bars showing
standard deviation across runs. Right panel shows fitness-stratified diversity analysis reveal-
ing systematic exploration patterns across performance levels. (c) Performance comparison
on MLGWSC-1, Set 4 dataset showing framework validation against seven benchmark
algorithms (Sage, Virgo-AUTh, PyCBC, TPI FSU Jena, cWB, MFCNN, CNN-Coinc). Opti-
mization milestones PT-1 through PT-4 demonstrate progressive algorithmic refinement,
with PT-4 achieving 20.2% improvement over SOTA baselines. Grey curves represent inter-
mediate solutions explored during optimization, while the red dotted line shows seed function
baseline. Vertical dashed lines indicate evaluation range boundaries (4-1000 events per
month). Results validate the framework’s systematic exploration capabilities, interpretable
algorithmic pathways, and effective convergence toward high-performing solutions through
progressive complexity enhancement and multi-component integration strategies.
under a 0.2-second trigger arrival time uncertainty constraint to assess robust-
ness and interpretability (data specifications are provided in Supplementary
Information Section S2).
Figure 4a demonstrates strong training-test performance correlation (r =
0.840) across all algorithms, with each point representing fitness scores (com-
puted from AUC metrics) for individual configurations. This correlation vali-
dates robust generalization despite significant domain shift between datasets,
with performance variation reflecting the non-stationary, non-Gaussian noise


--- Page 10 ---
10
LLM-guided automated algorithmic discovery
Fig. 4 Generalization Validation and Component Effectiveness Analysis. (a)
Training versus test performance correlation for 877 algorithmic configurations evaluated
under 0.2-second trigger arrival time uncertainty constraint. Each point represents an indi-
vidual algorithm’s fitness scores (computed from AUC metrics) on training (7-day dataset)
and test (1-day independent dataset) data. Linear correlation coefficient r = 0.840 indicates
strong generalization capability, while variance reflects expected performance variation due
to non-stationary, non-Gaussian noise characteristics. Red dashed line shows the empirical
trend relationship, while the grey dashed line represents perfect correlation (y=x). High-
performing algorithms (fitness > 4000) demonstrate particularly robust generalization across
different noise realizations and signal parameters. (b) MCTS depth-stratified performance
analysis across optimization phases. Fitness distribution of algorithms organized by MCTS
tree depth groups (Depth I: depths 1-4, Depth II: depths 5-7, Depth III: depths 8-10) and
phase transitions (PT1-PT4). Training performance (teal) and test performance (pink) are
shown with violin plots for sample sizes n ≥10 and scatter plots (circles/rectangles) for
n < 10. The analysis reveals systematic migration of high-fitness algorithms toward deeper
tree layers as optimization progresses, with elite algorithms (fitness > 5,000) emerging exclu-
sively in deeper layers during PT4. Enhanced generalization capability is observed in deeper
layers during later optimization phases, as evidenced by improved training-test performance
alignment in Depth III compared to shallower depth groups. (c) Algorithmic component
impact analysis. Violin plots comparing normalized fitness distributions between algorithms
with specific techniques (left) versus without (right). Techniques categorized as conditioning
methods (teal), time-frequency analysis (orange), and trigger detection (green). Technique
effectiveness is determined by distributional separation: wider gaps between left and right dis-
tributions indicate stronger performance impact. Conditioning techniques (Savitzky-Golay
filtering, Adaptive Gain Regularization) and trigger detection methods (Curvature Analysis,
Continuous Wavelet Transform Validation) demonstrate the most substantial improvements
through clear distributional shifts toward higher fitness values. Statistical validation across
1,000 resampling iterations confirms significance (p < 0.001) and practical importance.


--- Page 11 ---
LLM-guided automated algorithmic discovery
11
characteristics inherent in realistic gravitational wave detection environ-
ments. The 0.2-second timing constraint ensures temporal precision essential
for astrophysical parameter estimation while preserving detection sensitiv-
ity, empirically determined through systematic constraint-correlation analysis
(detailed in Supplementary Information Section S4). These results confirm that
optimized algorithms achieve genuine performance improvements transferable
to independent datasets, validating our evolutionary framework’s practical
utility for real-world gravitational wave detection applications.
Depth-Stratified
Analysis. We analyzed the relationship between
MCTS tree depth and algorithm fitness across optimization phases. Figure 4b
reveals systematic evolution patterns in performance and generalization capa-
bility.
The analysis demonstrates clear progression in algorithmic quality through
successive phase transitions. During PT1, algorithms are predominantly in
shallow depth groups with modest fitness values (< 2,000). As optimization
progresses to PT2 and PT3, high-performing algorithms (fitness > 3,000)
increasingly emerge in deeper tree layers, indicating successful identification
and refinement of promising algorithmic directions.
Elite algorithms (fitness > 5,000) emerge exclusively in deeper tree lay-
ers during PT4, suggesting that sophisticated solutions require extensive
refinement through multiple decision levels. Training (teal) and test (pink)
performance distributions show robust generalization across all depth groups,
with algorithms maintaining relative performance rankings regardless of tree
depth.
High-performing algorithms become increasingly rare but more consistent
in deeper layers, reflecting natural convergence toward superior solutions.
Critically, deeper layers exhibit superior generalization capability during
later optimization phases, with training-test performance gaps narrowing
significantly in Depth III compared to shallower groups. This improved gener-
alization suggests extensive algorithmic refinement enhances both performance
and robustness across different observational conditions, confirming the MCTS
framework effectively balances exploration breadth with exploitation depth.
Technique Impact Analysis. We conducted comprehensive technique
impact analysis using controlled comparative methodology, systematically
evaluating algorithms with specific signal processing techniques against
matched controls (details provided in Supplementary Information Section S5).
Figure 4c reveals distinct performance impacts across algorithmic com-
ponents. Conditioning techniques demonstrate the most pronounced positive
effects, with Savitzky-Golay filtering showing clear distributional separation
and asymmetric violin plots shifted toward higher fitness values. This tech-
nique has been applied in gravitational wave counterpart studies for smoothing
two-dimensional dispersed images from the Hubble Space Telescope, effectively
removing high-frequency structure while preserving underlying emission pat-
terns [45]. These findings establish quantitative benchmarks for algorithmic
component selection in automated gravitational wave detection systems.


--- Page 12 ---
12
LLM-guided automated algorithmic discovery
2.4 Evolutionary Pathways and Reproducibility
Knowledge Accumulation Pathways. To understand the mechanistic basis
of algorithmic discovery, we conducted comprehensive analysis of the complete
MCTS exploration pathway leading to the PT4 algorithm (node 486, fitness
= 5241.4). Figure 5a presents the full tree structure encompassing all nodes
associated with the selected algorithm, revealing systematic patterns in knowl-
edge accumulation and technique integration across multiple optimization
phases (full MCTS tree data and visualization are available in Supplementary
Information Section S6).
Critical to understanding the framework’s discovery mechanism is the iden-
tification of five key algorithmic breakthroughs that emerge at specific nodes
and propagate through subsequent generations. These innovations demon-
strate systematic knowledge accumulation, with breakthrough techniques
subsequently incorporated into descendant algorithms through evolution-
ary operations. The propagation patterns reveal progressive sophistication
through iterative refinement and technique combination, with superior algo-
rithmic components showing robust inheritance and integration capabilities
that directly influence fitness improvements across generations. The complete
tree structure demonstrates effective LLM-guided exploration that balances
exploitation of promising directions with exploration of novel algorithmic terri-
tories, leading to high-performance detection strategies significantly exceeding
conventional approaches.
Stochastic Reproducibility Assessment. Through comprehensive re-
execution analysis of three pivotal evolutionary transitions using 100 indepen-
dent runs each, we validated the consistency of breakthrough innovations in
Figure 5b. These analyses confirm that breakthrough algorithmic innovations
emerge through systematic discovery processes rather than fortuitous random
variations, demonstrating stable technique inheritance and high-probability
performance improvements across all re-executions despite increased algorith-
mic complexity, validating the framework’s reliability for automated algorithm
discovery and providing confidence in the generalizability of discovered tech-
niques to broader gravitational wave detection challenges.
2.5 Ablation Studies
To validate Evo-MCTS’s effectiveness, we conducted systematic component
analysis across three critical dimensions: integrated optimization architecture,
LLM model selection, and domain knowledge incorporation. These analyses
reveal that superior performance emerges from synergistic component inter-
actions rather than simple additive effects - addressing the vast search space
problem through MCTS-guided reflection structuring, ensuring code genera-
tion quality via optimal LLM selection, and maintaining scientific relevance
through domain knowledge integration. This multi-faceted approach demon-
strates that successful automated discovery requires intelligent integration


--- Page 13 ---
LLM-guided automated algorithmic discovery
13
Fig. 5 Algorithmic Evolutionary Pathway of MCTS and Edge Robustness Anal-
ysis. (a) Complete MCTS tree structure showing all nodes associated with the PT4
algorithm (node 486, fitness=5241.4) discovered in an optimization run. Node sizes encode
fitness values (larger circles = higher performance), with evaluation times displayed inside
circles. Node colors indicate expansion operation types: Parent Crossover (orange), Sib-
ling Crossover (cyan), Path-wise Crossover (green), and Point Mutation (purple). Solid
black lines represent the selected MCTS exploration path, while dashed gray lines indicate
nodes referenced in expansion prompts for knowledge synthesis. Five key algorithmic break-
throughs are annotated: Multi-resolution Thresholding (first appearing at node 12), CWT
using Ricker Wavelet (node 28), Tikhonov Regularization (node 140), Curvature Boosting
(node 151), and Savitzky-Golay Filter (node 333). These techniques propagate through sub-
sequent generations, demonstrating systematic knowledge accumulation and refinement. The
tree visualization reveals how sophisticated detection algorithms emerge through progres-
sive technique integration across multiple MCTS depth levels. (b) Edge robustness analysis
for three critical evolutionary transitions. Each subplot shows fitness distributions from 100
independent re-executions of specific edges: Edge 47→69 (early breakthrough, mean fitness
1034.6, 89.25% variants exceeding preceding node performance), Edge 140→151 (inter-
mediate advancement, mean fitness 1646.8, 52.81% achieving superior fitness with 100%
regularization technique inheritance), and Edge 485→486 (final optimization stage, mean
fitness 3274.6, 70.65% variants outperforming node 204, 25.00% surpassing node 485). Ver-
tical reference lines indicate the original node fitness values and key ancestral nodes. The
distributions demonstrate the stochastic nature of LLM-driven code generation while con-
firming consistent discovery of high-performance algorithmic variants with robust knowledge
transfer across independent executions.


--- Page 14 ---
14
LLM-guided automated algorithmic discovery
of search strategy, reasoning capability, and domain expertise beyond mere
computational power.
Synergistic Architecture Design. Figure 6a demonstrates the sub-
stantial benefits of combining evolutionary optimization with MCTS in
LLM-guided algorithm discovery. Evo-MCTS achieves superior performance
compared to its constituent components operating in isolation - pure MCTS-
AHD [29] and pure evolutionary optimization (ReEvo [28]), all of which
leverage LLMs for code generation and algorithmic reasoning.
The performance hierarchy reveals critical insights about LLM-based opti-
mization strategy effectiveness. Pure evolutionary approaches struggle with
the vast search space and become trapped in local optima despite LLM guid-
ance, exhibiting high variance with frequent suboptimal exploration in the
LLM-generated algorithm space. MCTS-AHD provides improvement through
systematic search space organization but with limited diversity in LLM-
generated solutions. The full Evo-MCTS combines the best of both worlds:
maintaining population diversity through evolutionary mechanisms while
focusing computational resources on promising algorithmic directions through
tree search guidance, all while maximizing the utilization of LLM reasoning
capabilities. This integration achieves a remarkable 59.1% improvement over
MCTS-AHD alone, demonstrating that combining population-based diversity
maintenance with tree-structured exploitation creates emergent optimization
capabilities that exceed the sum of individual LLM-powered components.
LLM Selection. Figure 6b investigates the impact of different foundation
models on algorithmic discovery performance, revealing significant variations
in code generation capability across state-of-the-art language models. The
analysis establishes o3-mini-medium as our fiducial model configuration.
The
performance
hierarchy
reveals
insights
about
the
relation-
ship
between
model
architecture
and
scientific
code
generation
capability.
Particularly
intriguing
is
the
superior
performance
of
claude-3-7-sonnet-20250219-thinking over o1-2024-12-17, despite both
being reasoning-enhanced models. This suggests that Claude’s specific train-
ing methodologies and architectural choices may be better suited for sustained
algorithmic reasoning tasks in gravitational wave detection algorithm develop-
ment. The substantial performance gap between reasoning-enhanced models
and general-purpose models underscores the critical importance of model
architecture selection for scientific code generation tasks.
This demonstrates that framework effectiveness depends not merely on
access to LLMs, but on selecting models with appropriate reasoning archi-
tectures for complex scientific applications. The robustness analysis shows
consistent performance rankings across multiple runs, validating our model
selection strategy while revealing that different LLM architectures exhibit
distinct strengths in algorithmic reasoning and code synthesis.
Necessity of Domain Knowledge. Figure 6c presents one of the most
striking results: the dramatic impact of domain knowledge integration on algo-
rithmic discovery performance. The comparison between frameworks with and


--- Page 15 ---
LLM-guided automated algorithmic discovery
15
Fig. 6 Framework Ablation Study and Design Validation. (a) Integrated archi-
tecture validation comparing Evo-MCTS (red) against constituent components: MCTS-
AHD (purple, 1,677.73 fitness) and ReEvo (green, 1,082.61 fitness). Evo-MCTS achieves
superior performance (2,670.37 fitness) through synergistic combination of evolution-
ary population dynamics and tree-structured search. (b) LLM model selection anal-
ysis showing performance variation across foundation models: o3-mini-medium (red,
2,670.37), claude-3-7-sonnet-20250219-thinking (yellow, 2,220.24), o1-2024-12-17 (pur-
ple, 1,395.88), and gpt-4o-2024-11-20 (green, 1,070.97). Results demonstrate the superior
performance of reasoning-enhanced models, with o3-mini-medium achieving 150% improve-
ment over general-purpose models. (c) Domain knowledge integration impact comparing
frameworks with external knowledge (red, 2,670.37) versus without domain-specific guid-
ance (orange, 1,244.50). The 115% performance improvement demonstrates the essential role
of scientific domain expertise in automated algorithm discovery. All curves represent aver-
ages over at least five independent runs with shaded regions indicating standard deviation.
Results validate the framework’s three core design principles: integrated optimization archi-
tecture, optimal model selection, and domain knowledge incorporation.
without external knowledge reveals a performance difference of 115%, demon-
strating that domain-specific guidance is essential for effective automated
algorithm discovery in specialized scientific domains.
The framework without external knowledge exhibits relatively flat opti-
mization trajectories, while domain knowledge integration demonstrates sus-
tained improvement by serving as a constraint mechanism that guides the
vast search space toward physically meaningful solutions. The emphasis on
non-linear processing significantly enhances gravitational wave signal detection
in real-world non-Gaussian, non-stationary noise environments, successfully
leveraging scientific expertise to accelerate discovery beyond pure compu-
tational search (domain knowledge templates and integration methodology
detailed in Supplementary Information Section S1).
3 Discussion
The Evo-MCTS framework demonstrates that LLM-guided evolutionary
search can effectively navigate complex algorithmic design spaces in scien-
tific computing applications. Our results establish key methodological insights
about interpretable algorithm discovery that extend beyond the gravitational
wave detection demonstration to broader scientific computing domains.
Methodological Validation. The Evo-MCTS framework’s performance
on gravitational wave detection demonstrates key methodological capabilities
transferable to broader scientific computing domains. The 59.1% improve-
ment over existing LLM-based frameworks (MCTS-AHD, ReEvo) validates the


--- Page 16 ---
16
LLM-guided automated algorithmic discovery
synergistic integration of tree-structured search with evolutionary population
dynamics, while the 20.2% improvement over domain-specific methods con-
firms that automated discovery can navigate complex design spaces effectively
when guided by appropriate domain constraints.
The systematic convergence toward similar algorithmic structures across
independent runs—despite stochastic LLM variations—provides empirical
evidence for the framework’s reliability as a discovery mechanism. This con-
vergent behavior contrasts sharply with random search or pure evolutionary
approaches, demonstrating that MCTS guidance combined with LLM rea-
soning enables focused exploration of promising algorithmic regions while
maintaining solution diversity.
Practical Considerations. We acknowledge that competition-based
benchmarks like MLGWSC-1, while providing standardized evaluation envi-
ronments, have inherent limitations in capturing the full complexity of oper-
ational gravitational wave detection. The benchmark’s simplified evaluation
metrics enable controlled framework validation but do not encompass real-time
processing requirements, comprehensive parameter estimation, or integra-
tion with multi-messenger astronomy infrastructure required for production
deployment. Our framework’s primary contribution lies in the methodology
for automated algorithm discovery rather than proposing production-ready
gravitational wave detection pipelines, with the benchmark serving as a chal-
lenging testbed that demonstrates framework capabilities on realistic scientific
computing problems.
Future Directions. The evolutionary MCTS approach demonstrates
broad potential for scientific algorithm discovery beyond gravitational waves.
As Browne et al. note, MCTS “can be used with little or no domain knowl-
edge, and has succeeded on difficult problems where other techniques have
failed” [21]. The framework’s domain-agnostic architecture suggests applica-
tions in molecular optimization for drug discovery, materials design algorithms,
and signal detection across astrophysical domains. The demonstrated inter-
pretability advantages enable hybrid human-AI systems where algorithmic
discoveries inform theoretical understanding while domain insights guide
optimization, potentially accelerating scientific algorithm development across
multiple disciplines.
4 Methods
4.1 Problem Formulation
This section formalizes automated algorithm discovery as a constrained opti-
mization problem and introduces the LLM-guided evolutionary framework
architecture, using gravitational wave detection as a concrete instantiation of
the general framework.
Algorithm Discovery Problem Instantiation. To demonstrate the
framework’s capabilities on a challenging scientific computing problem, we
instantiate the algorithm discovery framework for gravitational wave detection.


--- Page 17 ---
LLM-guided automated algorithmic discovery
17
Given dual-detector strain data d(t) ∈R2×N, where d(t) = [dH(t), dL(t)]T
represents the strain data from Hanford and Livingston interferometers sam-
pled at fs = 2048 Hz over finite observation windows of length N samples,
we seek to discover optimal detection algorithms that maximize performance
while satisfying operational constraints.
Optimization Objective. The algorithm discovery problem is formulated
as:
a∗= arg max
a∈A
F(a, d)
(1)
subject to
∥∆tarrival∥≤0.2 seconds
(2)
Tcomp(a) ≤Tmax
(3)
E(a) ≤Emax
(4)
a : R2×N →R3
(5)
where:
• A = {a | a is executable and satisfies constraints} represents the space of
executable detection algorithms
• F(a, d) =
R F ARmax
F ARmin dL(FAR; a, d) d(FAR) measures detection performance
as the area under the sensitive distance versus false alarm rate curve
following the MLGWSC-1 protocol [30]
• ∆tarrival denotes trigger arrival time uncertainty for astrophysical parameter
estimation
• Tcomp(a) and E(a) represent computational time and error handling trial
count constraints with thresholds Tmax and Emax
• Each algorithm a maps dual-detector strain data to a three-column detection
catalog table containing peak times, signal ranking statistics, and timing
uncertainties
The fitness function F evaluates algorithms against ground truth labels
ytrue using the area under the curve (AUC) of sensitive distance versus false
alarms per month, where dL(FAR; a, d) represents the sensitive distance at
a given false alarm rate FAR, providing more physically meaningful perfor-
mance assessment than traditional Receiver Operating Characteristic curves
for gravitational wave detection applications.
Algorithm Discovery Framework. The framework implements a gen-
eral iterative search procedure that combines Monte Carlo Tree Search
exploration with evolutionary population dynamics:
Pt+1 = Evolve(Pt, L, Kdomain)
where Pt = {a(t)
1 , a(t)
2 , . . . , a(t)
k } represents the algorithm population at
iteration t.
The framework comprises the following key components:


--- Page 18 ---
18
LLM-guided automated algorithmic discovery
• LLM Code Generation: L : (prompt, context) →code represents the lan-
guage model for code generation, with operation-specific prompting strate-
gies σ : O →P mapping evolutionary operations O = {PC, SC, PWC, PM}
to specialized prompt templates P.
• Domain Knowledge: Kdomain = (Kphysics, Kmethods, Kconstraint) encapsu-
lates domain-specific expertise including physical principles, algorithmic
techniques, and computational constraints. For the gravitational wave detec-
tion instantiation, this includes detector physics, signal processing methods,
and operational requirements.
• MCTS Selection: Node selection follows Upper Confidence bounds applied
to Trees (UCT) with adaptive exploration:
πUCT(n) = arg
max
c∈children(n) [normalized fitness(c) + adaptive exploration(c)]
where the complete implementation with fitness normalization, adaptive
exploration constants, and epsilon regularization is detailed in Section 4.4.
Evolutionary Operations. The framework employs four evolutionary
operations for algorithmic transformation, each utilizing operation-specific
prompting strategies with the same underlying language model:
PC(Pt) :
anew = L(promptPC(ap, ar), Kdomain)
(6)
SC(Pt) :
anew = L(promptSC(ac, {asi}), Kdomain)
(7)
PWC(Pt) :
anew = L(promptPWC({adi}), Kdomain)
(8)
PM(Pt) :
anew = L(promptPM(ac, ae), Kdomain)
(9)
where PC = Parent Crossover, SC = Sibling Crossover, PWC = Path-wise
Crossover, and PM = Point Mutation. Here, ap and ar denote parent and
reference algorithms, ac represents the current algorithm, {asi} are sibling
algorithms, ae is an elite algorithm, and {adi} are distant algorithm sets.
Each operation employs tailored prompt templates promptop(·) that guide
the language model to generate appropriate algorithmic variants: Parent
Crossover prompts emphasize combining successful features from parent
algorithms, Sibling Crossover focuses on lateral exploration within similar
algorithmic families, Path-wise Crossover promotes paradigm shifts by inte-
grating distant algorithmic approaches, and Point Mutation targets localized
refinements of existing implementations. Domain knowledge Kdomain ensures
adherence to domain-specific principles and physical constraints across all
operations.
Reflection and Adaptation. The system incorporates analytical reason-
ing through specialized reflection using the DeepSeek-R1 model:
K(t+1)
domain = K(t)
domain ∪{insights(R(historyt, performancet, Kdomain))}


--- Page 19 ---
LLM-guided automated algorithmic discovery
19
where R : H × F∗× Kdomain →I represents the reflection function mapping
MCTS history H, fitness evaluations F∗, and domain knowledge to actionable
insights I.
This reflection mechanism analyzes performance patterns across the MCTS
tree to identify successful algorithmic principles and guide subsequent evolu-
tionary operations toward promising regions of the solution space, enabling
cumulative learning that incorporates both performance feedback and domain-
specific constraints.
Population Management. At each iteration, the algorithm population
is updated through elite preservation with selection pressure β:
Pt+1 = Elite(Pt ∪{anew}, k, β)
(10)
where
Elite(·, k, β)
selects
the
top-k
algorithms
based
on
fit-
ness
scores
F(ai, d)
with
selection
probability:
pselect(ai)
=
exp(β · F(ai, d))/P–P˝ t
j=1
exp(β · F(aj, d))
This formulation establishes a general framework for automated algorithm
discovery as a constrained optimization problem in the space of executable
algorithms, solved through LLM-guided evolutionary search with MCTS explo-
ration and domain knowledge integration. The gravitational wave detection
application demonstrates this framework’s effectiveness on challenging scien-
tific computing problems requiring continuous parameter optimization under
strict physical constraints.
4.2 LLM Integration for Code Generation
The framework leverages state-of-the-art language models to transform algo-
rithmic concepts into executable code, implementing a multi-model strategy
that capitalizes on the complementary strengths of different architectures.
This subsection details the model selection, prompting strategies, and error
handling mechanisms that enable robust algorithmic discovery.
Model
Architecture
and
Task
Allocation.
The
frame-
work
employs
a
heterogeneous
ensemble
of
four
language
models:
o3-mini-medium,
o1-2024-12-17,
gpt-4o-2024-11-20,
and
claude-3-7-sonnet-20250219-thinking for code generation tasks. For
reflection operations, we utilize deepseek-r1-250120 exclusively due to its
analytical reasoning capabilities.
Prompting Strategy and Temperature Control. All models operate
with temperature 1.0 to optimize the trade-off between algorithmic diversity
and code validity.
The prompting framework employs depth-aware adaptation mechanisms
applicable across scientific computing domains. Task descriptions clarify opti-
mization objectives, while depth information guides exploration scope: shallow
nodes emphasize paradigm shifts and architectural changes, deeper nodes focus
on parameter optimization and mathematical refinements. External domain


--- Page 20 ---
20
LLM-guided automated algorithmic discovery
knowledge integration provides optimization directives referencing estab-
lished algorithmic principles and domain-specific constraints. This adaptive
architecture enables systematic solution space exploration while maintaining
domain coherence and physical validity (complete templates in Supplementary
Information Section S1, demonstrated through gravitational wave detection
application).
Error Handling and Iterative Refinement. The framework imple-
ments a robust error recovery mechanism to handle code generation failures.
When syntax errors or runtime exceptions occur during algorithm evalua-
tion, the system captures detailed error information including stack traces
and execution context. This diagnostic information is then incorporated into
subsequent conversation rounds with the LLM to generate corrected imple-
mentations. The system attempts up to three correction iterations per failed
algorithm. If all correction attempts fail, the corresponding node expansion is
skipped to maintain computational efficiency. This approach ensures that the
majority of generated algorithms remain executable while preventing infinite
correction loops that could stall the evolutionary process (complete templates
in Supplementary Information Section S1).
The analysis captures algorithmic innovations, signal processing tech-
niques, performance expectations, and computational characteristics in com-
pressed form. This enables efficient reference to previous discoveries without
overwhelming the LLM context, facilitating continued exploration while
maintaining algorithmic memory across generations (complete templates in
Supplementary Information Section S1).
Domain Knowledge Integration. The framework incorporates domain-
specific expertise through three structured prompt categories: initialization
prompts defining problem-specific principles and data characteristics, evo-
lution prompts encouraging effective algorithmic strategies and adaptive
processing, and reflection prompts evaluating performance and computational
efficiency (complete templates in Supplementary Information Section S1).
For the gravitational wave detection demonstration, the knowledge base
prioritizes techniques appropriate for transient signal processing in non-
stationary noise: adaptive thresholds, multi-scale analysis, and robust statis-
tical estimators. Implementation emphasizes adaptive parameters over fixed
values while maintaining computational efficiency.
This approach ensures domain constraint compliance while enabling explo-
ration beyond conventional algorithmic paradigms, demonstrating the frame-
work’s capability to integrate scientific expertise with automated discovery.
4.3 Evolutionary Operations Design and Seed Algorithm
Seed Algorithm Architecture. The optimization process begins with a
deliberately simple seed function that establishes a baseline for algorithmic
improvement (detailed implementation in Supplementary Information Section
S1). This seed algorithm implements a conventional signal processing pipeline


--- Page 21 ---
LLM-guided automated algorithmic discovery
21
consisting of three sequential operations that represent standard approaches
in gravitational wave data analysis.
The first operation performs frequency-domain whitening to normalize the
detector noise characteristics:
Xwhite(f) = X(f)
p
S(f)
(11)
where X(f) represents the Fourier transform of the input strain data, and S(f)
is the power spectral density estimate obtained via Welch’s method using a
4096-sample window with 50% overlap and Hann windowing. This whitening
operation ensures that all frequency components contribute equally to sub-
sequent analysis, compensating for the detector’s frequency-dependent noise
characteristics.
The second operation applies time-frequency decomposition using the
short-time Fourier transform (STFT) to capture transient signal characteris-
tics:
Sxx(f, τ) =

N−1
X
n=0
x(n + τ) w(n) e−j2πfn/N

2
(12)
where w(n) is a window function (256 samples with 128-sample overlap), τ
is the time shift, and N is the window length. The algorithm independently
processes both Hanford (H1) and Livingston (L1) detector data, then combines
the resulting spectrograms using simple averaging:
TFmetric = 1
2

SH1
xx + SL1
xx

f
where ⟨·⟩f denotes frequency-bin averaging.
The final operation identifies candidate events through basic peak detec-
tion with fixed thresholds. The algorithm estimates background levels using
the median and applies simple peak finding with predetermined height and
prominence criteria. This approach represents a minimalist detection strategy
that lacks the sophistication necessary for robust gravitational wave identifi-
cation, particularly in the presence of non-Gaussian noise transients and weak
signals.
Initial Population Generation. The evolutionary framework initial-
izes with a single seed algorithm, then generates 8 diverse variants through
systematic prompting variations (Figure 1a). Each variant maintains iden-
tical input-output interfaces while implementing distinct signal processing
approaches: alternative whitening schemes, varied time-frequency decompo-
sition methods, and different peak detection strategies. Following initial
generation, two Point Mutation operations are applied to create additional
variants, resulting in a total population of 10 algorithms that form the depth-1
initial population for MCTS exploration.
Elite Preservation Strategy. The framework maintains an elite indi-
vidual representing the best-performing algorithm discovered throughout


--- Page 22 ---
22
LLM-guided automated algorithmic discovery
evolution. This elite serves as a performance benchmark for new variants and
provides genetic material during Point Mutation operations, which specifi-
cally leverage elite characteristics to guide targeted algorithmic improvements
(Figure 1c). The elite is updated whenever a new algorithm demonstrates supe-
rior performance, ensuring monotonic progress while maintaining access to the
current best solution (Figure 1b.4).
Operation Sequence and Execution Strategy. Each MCTS expan-
sion level follows a structured sequence of evolutionary operations: Parent
Crossover (PC) executes 5 times, Path-wise Crossover (PWC) executes 2
times, Sibling Crossover (SC) executes once, and Point Mutation (PM) exe-
cutes twice. As observed in Figure 5a, this sequence balances four distinct
algorithmic improvement mechanisms: (i) vertical knowledge transfer through
PC operations that combine features from algorithms at different tree depths,
(ii) long-range dependency capture via PWC operations that synthesize infor-
mation along complete evolutionary trajectories, (iii) horizontal exploration
using SC operations that facilitate exploration between algorithms of simi-
lar complexity based on nodes already generated at the same tree level, and
(iv) fine-grained optimization through PM operations that introduce targeted
modifications based on performance analysis. The systematic application of
these operations ensures comprehensive exploration of the algorithmic solution
space while maintaining computational efficiency through controlled expansion
rates.
4.4 Implementation of Monte Carlo Tree Search
MCTS Framework and Tree Policy. The framework implements a stan-
dard Monte Carlo Tree Search algorithm adapted for algorithmic discovery,
where each node represents an executable algorithm and tree expansion cor-
responds to algorithmic evolution (Figure 1b). The MCTS operates through
four canonical phases: selection, expansion, simulation, and backpropagation.
However, our implementation modifies the traditional simulation phase by
replacing random rollouts with direct algorithm evaluation on the gravitational
wave detection task.
The selection phase traverses the tree from root to leaf using the Upper
Confidence Bound applied to Trees (UCT) policy, balancing exploitation
of high-performing algorithms with exploration of under-visited branches
(Figure 1c). Unlike traditional MCTS applications where leaf nodes represent
terminal game states, our leaf nodes represent algorithms that can be further
evolved through the four evolutionary operations (PC, SC, PWC, PM).
UCT Score Calculation and Adaptive Exploration. The UCT score
for each node combines exploitation and exploration terms with an adaptive
exploration strategy that accounts for the finite evaluation budget:
UCT(n) =
Q(n) −Qmin
Qmax −Qmin + ϵ + c ·
s
ln(N(p) + 1)
N(n) + ϵ
(13)


--- Page 23 ---
LLM-guided automated algorithmic discovery
23
where Q(n) represents the normalized fitness value of node n, Qmin and
Qmax are the minimum and maximum fitness values observed across all nodes,
N(p) and N(n) are the visit counts of the parent and current node respectively,
ϵ is a small constant preventing division by zero, and c is the exploration
constant.
The exploration constant c adapts dynamically based on the remaining
evaluation budget:
c = c0 · max

1 −t
T , 0

(14)
where c0 is the initial exploration constant, t is the current evaluation count,
and T is the maximum evaluation budget. This adaptive mechanism ensures
that the algorithm emphasizes exploration early in the search when the bud-
get is abundant, then gradually shifts toward exploitation as evaluations are
consumed.
Fitness Normalization and Q-Value Management. The fitness values
(corresponding to the objective function in MCTS) are normalized to the range
[0, 1] to ensure consistent UCT calculations regardless of the absolute scale of
performance metrics. The normalization uses running minimum and maximum
values:
Qnormalized =
Qraw −Qmin
Qmax −Qmin + ϵ
(15)
This normalization is crucial for maintaining meaningful exploration-
exploitation balance, as it prevents algorithms with vastly different perfor-
mance scales from skewing the selection process.
Backpropagation and Value Updates. The backpropagation phase
updates node values along the path from the newly evaluated leaf to the root.
Our implementation uses a discount factor approach that balances immediate
performance with long-term potential:
Q(p) = Q(p) · (1 −γ) +
max
c∈children(p) Q(c) · γ
(16)
where Q(p) is the parent node’s Q-value, γ is the discount factor, and the
maximum is taken over all child nodes. This update rule ensures that parent
nodes reflect the performance of their best children while maintaining some
memory of their previous estimates.
The backpropagation also maintains global statistics by updating the
minimum and maximum Q-values across the entire tree, enabling consistent
normalization for future UCT calculations. Additionally, the algorithm main-
tains a ranked list of all observed fitness values to support advanced selection
strategies and performance analysis.
Tree Expansion Strategy. Node expansion occurs when the UCT selec-
tion phase reaches a leaf node that has been visited multiple times, indicating
sufficient confidence in its potential. The expansion strategy creates one new


--- Page 24 ---
24
LLM-guided automated algorithmic discovery
child node per expansion operation, chosen through the evolutionary opera-
tions framework. This conservative expansion approach prevents explosive tree
growth while ensuring thorough exploration of promising regions.
Each newly created node inherits structural information from its parent,
including the algorithmic context and performance history. The initial Q-value
for new nodes is set to the parent’s Q-value, providing a reasonable starting
estimate that will be refined through subsequent evaluations.
Memory Management and Tree Pruning. To maintain computational
efficiency with limited memory resources, the implementation includes mech-
anisms for selective tree pruning. Nodes that demonstrate consistently poor
performance relative to their siblings are marked for potential removal, while
maintaining sufficient diversity to avoid premature convergence.
The tree structure also maintains subtree references that enable efficient
traversal and analysis of evolutionary pathways. These references support
the PWC operation by providing access to complete root-to-leaf trajectories
without requiring expensive tree traversals.
Convergence Detection and Termination. The MCTS continues until
either the evaluation budget is exhausted or convergence is detected through
analysis of the Q-value distribution. Convergence is identified when the state-
of-the-art (SOTA) algorithms show minimal improvement over a specified
number of iterations, indicating that the search has reached a local optimum
within the current exploration strategy.
This MCTS implementation creates a systematic framework for explor-
ing the algorithmic space while maintaining computational efficiency and
ensuring reproducible results. The combination of adaptive exploration, nor-
malized fitness values, and efficient tree management enables the discovery of
high-performing algorithms within reasonable computational budgets.
4.5 Experimental Setup
Benchmark Selection and Evaluation. We selected the Machine Learn-
ing Gravitational-Wave Search Mock Data Challenge (MLGWSC-1) bench-
mark [30] as a challenging testbed for our approach. This benchmark provides
a controlled evaluation environment with realistic complexity: actual detec-
tor noise, diverse signal morphologies, and standardized assessment protocols.
Dataset 4 was selected for evaluation as it incorporates operational detector
noise from the O3a observing run, providing a realistic application scenario
for showcasing algorithmic discovery on scientific computing problems with
domain-specific physical constraints.
Dataset 4 incorporates noise from both Hanford (H1) and Livingston (L1)
detectors with quality filtering applied, spanning overlapping segments of
minimum 2-hour duration. Signal injection parameters create a challenging
evaluation scenario where at least 33% of injected signals have optimal network
SNR < 4, testing the framework’s ability to discover algorithms that balance
sensitivity with false-alarm control under realistic operational conditions.


--- Page 25 ---
LLM-guided automated algorithmic discovery
25
Algorithm Input/Output Interface. Following the MLGWSC-1 speci-
fication, each algorithm processes HDF5 files containing raw detector data from
both H1 and L1 detectors. The input files contain grouped datasets organized
by integer start times, with each dataset including strain data and metadata
attributes (start time, delta t). Our evolved algorithms output HDF5 files
containing exactly three datasets of equal length: (i) time - GPS times of sus-
pected gravitational wave events, (ii) stat - ranking statistics where larger
values indicate higher detection confidence, and (iii) var - timing accuracy tol-
erance representing the maximum allowed separation between predicted and
true event times.
Evaluation Metrics and Performance Assessment. The framework
employs two primary evaluation metrics consistent with MLGWSC-1 stan-
dards. The false-alarm rate (FAR) measures the expected frequency of
false-positive events exceeding a given ranking statistic threshold, calculated by
applying algorithms to pure noise data and dividing event counts by total ana-
lyzed time. The sensitive distance quantifies detection capability at specified
false-alarm rates, representing the maximum distance at which sources can be
reliably detected. For uniformly distributed signals in volume, this reduces to
the fraction of detected signals multiplied by the volume of a sphere with radius
equal to the maximum injected distance. The area under the curve (AUC)
metric integrates sensitive distance across the false-alarm rate range, provid-
ing a comprehensive performance indicator that balances detection sensitivity
against false-alarm tolerance.
Configuration of Training and Test Sets. To ensure efficient opti-
mization while maintaining statistical rigor, we partitioned the MLGWSC-1
Dataset 4 into training and test subsets. The training set comprises the first 7
days of data from early injection indices, enabling rapid algorithmic evaluation
during the optimization process. This temporal partitioning ensures that the
minimum false-alarm rate boundary is set at 4 events per month, correspond-
ing to the statistical requirements for meaningful AUC calculation. The test
set consists of 1 day of data selected from later temporal segments, provid-
ing independent validation of optimized algorithms (detailed data partitioning
specifications provided in Supplementary Information Section S2).
Diversity Metrics in Evolutionary Computation. We implemented
sophisticated diversity measurement following established practices in evolu-
tionary computation [46]. Population encoding involves three preprocessing
steps: (i) removing comments and docstrings using abstract syntax tree pars-
ing to focus on functional algorithmic content, (ii) standardizing code snippets
into common coding style following PEP 8 conventions to eliminate stylistic
variations, and (iii) converting normalized code snippets to vector representa-
tions using the CodeT5+ embedding model to enable quantitative similarity
analysis.
We employ two complementary diversity metrics: the Shannon Diversity
Index, calculated as H = −Pn
i=1 pi log pi where pi represents the frequency
of the i-th unique algorithmic variant in the population, and the Complexity


--- Page 26 ---
26
LLM-guided automated algorithmic discovery
Index of Diversity (CID), computed as CID = 1
n
Pn
i=1
xi−¯x
¯x+ϵ where xi repre-
sents the embedding vector of the i-th algorithm, ¯x is the population centroid,
and ϵ prevents division by zero. These metrics provide complementary per-
spectives on population diversity: Shannon index captures algorithmic variety
while CID measures structural complexity differences.
LLM
API
Configuration
and
Model
Selection.
Our
frame-
work
integrates
multiple
state-of-the-art
language
models
through
official
APIs
from
OpenAI,
Anthropic,
and
DeepSeek
[47–50].
For
code
generation
tasks,
we
employ
thinking-enhanced
models
includ-
ing
o3-mini-medium,
o1-2024-12-17,
gpt-4o-2024-11-20,
and
claude-3-7-sonnet-20250219-thinking, selected for their demonstrated
capabilities in multi-step reasoning and code synthesis [19, 51]. For reflection
mechanisms, we exclusively utilize deepseek-r1-250120 due to its supe-
rior performance in analytical reasoning tasks [50], particularly its ability
to identify subtle performance patterns and propose targeted algorithmic
improvements based on empirical observations. We designate o3-mini-medium
as our fiducial model configuration, providing the baseline reference for per-
formance comparisons and ensuring consistency across experimental runs. All
models operate with temperature parameter set to 1.0 to balance creative
exploration with syntactic reliability [52].
Hyperparameter Configuration and Experimental Design. The
Evo-MCTS framework employs carefully tuned hyperparameters optimized
for gravitational wave detection algorithm discovery. Initial population size
is set to 10 algorithms, balancing computational efficiency with adequate
diversity for effective exploration. MCTS depth is limited to 10 levels, pro-
viding sufficient hierarchical structure for complex algorithmic development
while maintaining computational tractability. Each experimental configuration
undergoes 5 independent runs with different random seeds to ensure statistical
robustness and enable confidence interval estimation.
The 5-run experimental design serves multiple purposes: (i) quantifying
algorithmic discovery reliability across different initialization conditions, (ii)
enabling statistical analysis of optimization trajectories and convergence pat-
terns, and (iii) providing robust diversity measurements that account for
stochastic variations in the evolutionary process. Comprehensive results from
all individual runs are documented in Supplementary Information Section S3,
enabling detailed analysis of inter-run variability and optimization consistency.
Ablation Studies and Comparative Analysis. To validate the effec-
tiveness of our integrated Evo-MCTS approach, we conduct comprehensive
ablation studies comparing against two established frameworks: ReEvo [28]
(pure evolutionary mechanism) and MCTS-AHD [29] (pure MCTS mecha-
nism). These comparisons isolate the contributions of different algorithmic
components while maintaining consistent evaluation protocols.
ReEvo represents the evolutionary optimization baseline, employing genetic
algorithms with traditional crossover and mutation operations powered by
LLMs but without tree-structured exploration. The framework implements


--- Page 27 ---
LLM-guided automated algorithmic discovery
27
population-based optimization through iterative code generation and selection,
focusing on evolutionary diversity maintenance and fitness-driven selection
pressure. MCTS-AHD implements pure Monte Carlo Tree Search for auto-
mated heuristic design without evolutionary population dynamics or reflection
mechanisms. This approach emphasizes tree-based exploration with UCT-
guided node selection but lacks the multi-generational insight synthesis and
population diversity maintenance that characterizes evolutionary approaches.
By comparing Evo-MCTS against these component frameworks using
identical LLM integration protocols and evaluation budgets, we quantify
the synergistic benefits of combining evolutionary population dynamics with
structured tree search exploration. The experimental design maintains con-
sistent evaluation metrics, hardware configurations, and statistical analysis
procedures across all three frameworks, with computational fairness ensured
through equivalent LLM evaluation counts as the unified iteration metric. This
approach enables direct performance comparison while isolating the specific
contributions of different optimization strategies under identical resource con-
straints. The comprehensive multi-run analysis results across all frameworks
are presented in Supplementary Information Section S3, demonstrating the
statistical robustness of our comparative evaluation.
Protocol for Edge Robustness Analysis. To validate the consistency of
algorithmic breakthroughs, we developed a systematic edge re-execution pro-
tocol. For each selected evolutionary transition, we perform 100 independent
re-executions using identical prompt templates while maintaining all exper-
imental conditions constant except for the LLM sampling parameters. Each
re-execution employs the same parent algorithms, evolutionary operation type,
and external knowledge integration templates used in the original optimization
run.
The re-execution protocol preserves all deterministic components (fitness
evaluation, data preprocessing, statistical analysis) while allowing natural vari-
ation in LLM-generated code through different random seeds. This approach
enables quantification of discovery mechanism robustness while accounting
for the inherent stochasticity in large language model outputs. Statistical
analysis employs standard descriptive metrics (mean, standard deviation) com-
bined with confidence interval estimation to characterize the reliability of
breakthrough discovery patterns, as illustrated in Figure 5(b).
Computational Resources and Parallelization Techniques. The
numerical calculations in this study were carried out on the ORISE Super-
computer, equipped with 32-core x86 processors and 4 GPGPU accelerators
per node, enabling efficient parallel execution of LLM API calls and algorithm
evaluations. The Evo-MCTS framework employs asynchronous parallelization
to maximize resource utilization, allowing multiple LLM requests to be pro-
cessed concurrently while maintaining synchronization for tree updates and
performance analysis. This parallelization strategy significantly reduces over-
all computational time while ensuring that all evaluations are performed under
consistent conditions.


--- Page 28 ---
28
LLM-guided automated algorithmic discovery
5 Data availability
The MLGWSC-1 Dataset 4 used in this study is publicly available at
https://github.com/gwastro/ml-mock-data-challenge-1/. The dataset includes
real detector noise from LIGO Hanford (H1) and Livingston (L1) observatories
during the O3a observing run.
The
ReEvo
framework
implementation
is
available
at
https://github.com/ai4co/reevo with the original codebase and documen-
tation. The MCTS-AHD framework can be accessed through its official
repository at https://github.com/zz1358m/MCTS-AHD-master. Both frame-
works were used for comparative analysis following their original specifications
and hyperparameter configurations.
Training and test data partitions, along with detailed preprocessing spec-
ifications, are provided in the Supplementary Information Section S2. All
experimental results and algorithm performance metrics are available upon
reasonable request to the corresponding author.
6 Code availability
The complete source code for the Evo-MCTS framework is publicly avail-
able at https://github.com/iphysresearch/evo-mcts. The repository includes
all implementation details, experimental configurations, and reproducibility
instructions. The code is written in Python and is fully reproducible with the
provided environment and dependencies. The repository contains comprehen-
sive documentation, example usage scripts, and all necessary configuration files
to replicate the experimental results presented in this study.
Acknowledgments
We thank the LIGO Scientific Collaboration for providing the MLGWSC-
1 dataset and establishing the evaluation framework that enabled this
research. We acknowledge the computational resources provided by the ORISE
Supercomputer facility, which were essential for the large-scale LLM-based
optimization experiments.
We are grateful to OpenAI, Anthropic, and DeepSeek for providing API
access to their language models, which formed the core of our algorithmic
discovery framework. We also thank the developers of the ReEvo and MCTS-
AHD frameworks for making their implementations publicly available for
comparative analysis.
H.W. is supported by the National Key Research and Development Pro-
gram of China (Grant No. 2021YFC2203004), the National Natural Science
Foundation of China (NSFC) (Grant Nos. 12405076, 12247187, 12147103), the
National Astronomical Data Center (Grant No. NADC2023YDS-01), and the
Fundamental Research Funds for the Central Universities.


--- Page 29 ---
LLM-guided automated algorithmic discovery
29
Supplementary information
S1
LLM Prompting Templates
This section provides comprehensive details of the prompting strategies
employed across different phases of the algorithmic discovery process. The tem-
plates are designed to guide language models through systematic reasoning
while incorporating domain-specific knowledge and maintaining consistency
across evolutionary operations.
System Context and Task Definition. All interactions with the LLM
ensemble begin with a standardized system prompt that establishes the expert
role and problem context:
You are an expert in gravitational wave signal detection algorithms. Your task is to design
heuristics that can effectively solve optimization problems. The task involves
constructing a pipeline for gravitational wave signal detection. This pipeline will
encompass data conditioning and time-frequency transformations as part of the signal
processing workflow. The input will consist of raw, finite-length dual-channel
gravitational wave data from the H1 and L1 detectors. The pipeline will be tested on
segmented data spanning several weeks, with each segment having variable length (7000s
-30000s). Each segment's dual-channel data will be directly used as input. The ultimate
goal is to produce a catalog of potential gravitational wave signals, where each trigger
includes information such as GPS time, ranking statistic, and the timing accuracy of the
prediction. This systematic approach is essential for effectively identifying and
cataloging candidate gravitational wave signals.
This system prompt serves multiple purposes: (i) establishing domain expertise
expectations, (ii) defining the specific optimization context, (iii) specifying
input data characteristics, and (iv) clarifying the expected output format and
evaluation criteria.
S1.1
Initial Algorithm Generation Prompts
Seed Function Template and Analysis Framework. The initial algorithm
generation process begins with a structured analysis of the seed function to
establish baseline understanding. The seed function analysis template guides
the LLM through systematic examination of the foundational algorithm:
## Seed Function Analysis Task
Analyze the foundational algorithm's design strategy to establish baseline understanding
for Monte Carlo Tree Search (MCTS) exploration. This first-level analysis will guide
subsequent optimization directions.
## Seed Function Implementation
```python
{prompt_seed_func}
```
- **Technical implementation details**: {prompt_other_inf}
- **Performance impact rationale**: {prompt_inout_inf}
## Context for Analysis
This initial analysis at MCTS depth first-level should:
- Identify core algorithmic mechanisms
- Extract fundamental processing stages
- Surface high-level optimization opportunities
- Establish baseline for diversity generation
{external_knowledge}


--- Page 30 ---
30
LLM-guided automated algorithmic discovery
## Analysis Requirements
1. Characterize the seed's core approach in one sentence containing:
- Primary computational strategy
- Key transformation stages
- Fundamental signal processing techniques
- Overall optimization philosophy
2. Focus on architectural-level characteristics rather than implementation details
3. Description must fit within single braces and avoid:
- Code references
- Parameter-level details
- Performance assessments
- Comparative statements
## Output Format Rules
- Return optimization strategies within SINGLE BRACE
- Ensure entire response can be parseable by regex: \\{{(.*?)\\}} with DOTALL flag
Seed Algorithm Specification. The seed function implements a three-
stage linear signal processing pipeline that serves as the evolutionary starting
point:
• Stage 1: Data Conditioning and Whitening
1
def data_conditioning(strain_h1: np.ndarray, strain_l1: np.ndarray, times: np.ndarray) -> tuple[np.ndarray, np.ndarray,
np.ndarray]:
,→
2
window_length = 4096
3
dt = times[1] - times[0]
4
fs = 1.0 / dt
5
6
def whiten_strain(strain):
7
strain_zeromean = strain - np.mean(strain)
8
freqs, psd = signal.welch(strain_zeromean, fs=fs, nperseg=window_length,
9
window='hann', noverlap=window_length//2)
10
smoothed_psd = np.convolve(psd, np.ones(32) / 32, mode='same')
11
smoothed_psd = np.maximum(smoothed_psd, np.finfo(float).tiny)
12
white_fft = np.fft.rfft(strain_zeromean) / np.sqrt(np.interp(np.fft.rfftfreq(len(strain_zeromean), d=dt), freqs,
smoothed_psd))
,→
13
return np.fft.irfft(white_fft)
14
15
whitened_h1 = whiten_strain(strain_h1)
16
whitened_l1 = whiten_strain(strain_l1)
17
18
return whitened_h1, whitened_l1, times
• Stage 2: Time-Frequency Decomposition
1
def compute_metric_series(h1_data: np.ndarray, l1_data: np.ndarray, time_series: np.ndarray) -> tuple[np.ndarray,
np.ndarray]:
,→
2
fs = 1 / (time_series[1] - time_series[0])
3
f_h1, t_h1, Sxx_h1 = signal.spectrogram(h1_data, fs=fs, nperseg=256, noverlap=128, mode='magnitude', detrend=False)
4
f_l1, t_l1, Sxx_l1 = signal.spectrogram(l1_data, fs=fs, nperseg=256, noverlap=128, mode='magnitude', detrend=False)
5
tf_metric = np.mean((Sxx_h1**2 + Sxx_l1**2) / 2, axis=0)
6
gps_mid_time = time_series[0] + (time_series[-1] - time_series[0]) / 2
7
metric_times = gps_mid_time + (t_h1 - t_h1[-1] / 2)
8
9
return tf_metric, metric_times
• Stage 3: Peak Detection and Trigger Generation
1
def calculate_statistics(tf_metric, t_h1):
2
background_level = np.median(tf_metric)
3
peaks, _ = signal.find_peaks(tf_metric, height=background_level * 1.0, distance=2, prominence=background_level * 0.3)
4
peak_times = t_h1[peaks]
5
peak_heights = tf_metric[peaks]
6
peak_deltat = np.full(len(peak_times), 10.0)
# Fixed uncertainty value
7
return peak_times, peak_heights, peak_deltat


--- Page 31 ---
LLM-guided automated algorithmic discovery
31
Template Variables and Customization. The prompting template
incorporates several customizable variables that enable systematic variation
generation:
• prompt seed func: Complete seed function implementation
• prompt other inf: Technical implementation details including sampling
rates, window parameters, and algorithmic constraints
• prompt inout inf: Performance impact rationale explaining the relation-
ship between input characteristics and expected output quality
• external knowledge: Domain-specific knowledge injection including grav-
itational wave physics, detector characteristics, and signal morphology
constraints
Output Format Requirements. All generated algorithms must conform
to the standardized interface:
1
def pipeline_v{N}(strain_h1: np.ndarray, strain_l1: np.ndarray, times: np.ndarray) -> tuple[np.ndarray, np.ndarray,
np.ndarray]:
,→
2
# Algorithm implementation for seed function
3
# ...
4
# Stage 1: Data Conditioning and Whitening
5
whitened_h1, whitened_l1, data_times = data_conditioning(strain_h1, strain_l1, times)
6
# Stage 2: Time-Frequency Decomposition
7
tf_metric, metric_times = compute_metric_series(whitened_h1, whitened_l1, data_times)
8
# Stage 3: Peak Detection and Trigger Generation
9
peak_times, peak_heights, peak_deltat = calculate_statistics(tf_metric, metric_times)
10
return peak_times, peak_heights, peak_deltat
This interface consistency ensures that all generated algorithms can be
evaluated within the same framework while enabling diverse internal imple-
mentations.
S1.2
Parent Crossover Implementation
The Parent Crossover (PC) operation represents a sophisticated evolutionary
operation that combines algorithmic components from two reference imple-
mentations at different levels of the MCTS hierarchy. This operation is
designed to preserve successful characteristics from both parent algorithms
while introducing novel enhancements that exceed simple interpolation.
Template Structure and Crossover Strategy. The PC operation
employs a structured template that guides the LLM through systematic
analysis and synthesis of two parent algorithms:
## Task Overview
Develop a novel algorithm that strategically combines components from two reference
implementations while introducing innovative enhancements. The solution must demonstrate
measurable improvements beyond simple interpolation of existing approaches.
Current Depth Level: [Level {depth}]
## Implementation Analysis
### Code Comparison
1. VERSION A (Baseline Implementation):
```python
{worse_code}
```
2. VERSION B (Enhanced Implementation):


--- Page 32 ---
32
LLM-guided automated algorithmic discovery
```python
{better_code}
```
### Strengths to Combine
```text
{reflection}
```
Key Synthesis Requirements:
- Preserve 2 distinct advantages from Version A
- Incorporate 3 critical enhancements from Version B
- Identify 1 synergistic improvement opportunity
## Architecture Strategy
{external_knowledge}
### Depth-Specific Synthesis Guidelines (Depth={depth})
1. Structural Synthesis (Depth 1-2):
- Create hybrid control flow combining best elements from both versions
- Example: "Combine Version A's iteration structure with Version B's termination
conditions"
- Forbid direct replication of either version's architecture
2. Implementation Fusion (Depth 3-4):
- Develop novel parameter hybridization techniques
- Example: "Blend Version A's exploration mechanism with Version B's exploitation
strategy"
- Require at least one innovative combination per functional module
3. Mathematical Innovation (Depth 5+):
- Derive new computational operators through version synthesis
- Example: "Fuse Version A's approximation method with Version B's error correction"
- Mandate 10-20% computational complexity reduction
This template structure ensures that the crossover operation is not merely
concatenative but involves intelligent analysis and strategic combination of
algorithmic strengths.
Depth-Adaptive Synthesis Guidelines. The PC operation implements
depth-specific strategies that adapt the crossover complexity based on the
current position in the MCTS tree:
• Structural Synthesis (Depth 1-2):
– Focuses on combining high-level architectural elements from both parent
algorithms
– Creates hybrid control flow structures that merge the best organizational
patterns
– Example directive: “Combine Version A’s iteration structure with Version
B’s termination conditions”
– Explicitly forbids direct replication of either parent’s complete architec-
ture
• Implementation Fusion (Depth 3-4):
– Emphasizes parameter hybridization and functional module integration
– Develops novel approaches to blend algorithmic strategies
– Example directive: “Blend Version A’s exploration mechanism with
Version B’s exploitation strategy”


--- Page 33 ---
LLM-guided automated algorithmic discovery
33
– Requires at least one innovative combination per functional module
• Mathematical Innovation (Depth 5+):
– Derives new computational operators through sophisticated version syn-
thesis
– Focuses on mathematical justification for algorithmic improvements
– Example directive: “Fuse Version A’s approximation method with Version
B’s error correction”
– Mandates 10-20% computational complexity reduction alongside perfor-
mance gains
Innovation Requirements and Quality Assurance. The PC opera-
tion enforces strict innovation standards to ensure that generated algorithms
represent genuine improvements:
• Core Innovation Targets:
– Synthesize 3+ novel elements not present in either parent version
– Resolve 2 fundamental limitations identified through comparative analysis
– Introduce 1 breakthrough enhancement with rigorous mathematical jus-
tification
– Demonstrate non-trivial performance gains over both parent algorithms
– Prohibit direct replication of complete code blocks from either parent
Reflection Generation Process. Before conducting the crossover syn-
thesis, the system generates analytical insights through a depth-adaptive
reflection template:
## Task Objective
Analyze optimization patterns across algorithm versions and generate depth-specific
improvement strategies. Current MCTS Depth: depth/max_depth={depth}/{max_depth}
## Depth-Specific Focus
- Shallow (Depth 1-2): Structural patterns & control flow
- Medium (Depth 3-4): Implementation techniques & parameterization
- Deep (Depth 5+): Mathematical formulations & computational primitives
## Algorithm Comparison
- Original (Suboptimal)
```python
{code_worse}
```
- Improved (Optimized)
```python
{code_better}
```
## Depth-Adaptive Analysis
### 1. Core Pattern Extraction
For {depth}-level analysis:
- Shallow: Compare control structures/algorithmic paradigms
- Medium: Analyze parameter configurations/function compositions
- Deep: Examine mathematical operators/numerical methods
### 2. Optimization Principle Generation
Generate 3-5 transferable rules that:
- Directly address {depth}-specific limitations


--- Page 34 ---
34
LLM-guided automated algorithmic discovery
- Contain concrete parameter values from improved version
- Maintain functional equivalence
## Output Format Rules
- Return optimization strategies within SINGLE BRACE
- Ensure entire response can be parseable by regex: \\{{(.*?)\\}} with DOTALL flag
This reflection generation produces the reflection variable used in the
main crossover template.
Reflection-Guided Analysis. The crossover process incorporates a
reflection component that analyzes the strengths and weaknesses of both
parent algorithms:
## Requirements
1. Core Innovation Targets:
- Synthesize 3+ novel elements not present in either version
- Resolve 2 fundamental limitations identified in analysis
- Introduce 1 breakthrough enhancement with mathematical justification
- Demonstrate non-trivial performance gain over both versions
- Prohibit direct replication of complete code blocks
This reflection analysis is generated through the deepseek-r1-250120 model
and provides crucial insights that guide the synthesis process. The reflection
identifies:
• Computational advantages in each parent algorithm
• Structural design patterns that contribute to performance
• Potential synergistic combinations that could yield emergent benefits
• Limitation patterns that should be addressed in the offspring
Output Format and Validation. The PC operation enforces a stan-
dardized output format that ensures both human readability and automated
processing:
2. Output Format:
- Place the core design idea in a sentence within a brace BEFORE the function definition
- For the core design idea format: \\{{A hybrid gravitational wave detection pipeline...}}
- Implement as Python function: {func_name}
- Inputs: {input_count} parameter(s) ({joined_inputs})
- Outputs: {output_count} return value(s) ({joined_outputs})
- Follow: {inout_inf}
- Constraints: {other_inf}
- IMPORTANT: All output code MUST be valid Python syntax. Do not place description text
inside curly braces within the function body.
- Example of correct format:
\\{{Core design description here}}
```python
def pipeline_v2(strain_h1: np.ndarray, strain_l1: np.ndarray, times: np.ndarray) ->
tuple[np.ndarray, np.ndarray, np.ndarray]:
"""Core design description can alternatively be placed here as a docstring"""
# Function implementation...
```
S1.3
Sibling Crossover Implementation
The Sibling Crossover (SC) operation implements a sophisticated two-phase
approach that leverages peer algorithm insights to generate improved offspring.
Unlike Parent Crossover, which combines algorithms from different hierarchi-
cal levels, SC focuses on horizontal knowledge transfer between algorithms


--- Page 35 ---
LLM-guided automated algorithmic discovery
35
at the same MCTS depth, promoting diversity while maintaining comparable
complexity levels.
Two-Phase Architecture. The SC operation employs a unique two-stage
process: first generating optimization hints through multi-level reflection anal-
ysis (Phase 1), then implementing concrete algorithmic improvements based
on these insights (Phase 2). This separation enables more targeted optimiza-
tion by allowing the system to first identify improvement opportunities before
implementing solutions.
Phase 1: Multi-Level Reflection Analysis. The first phase gener-
ates depth-specific optimization hints by synthesizing insights from sibling
algorithms and parent-level analysis:
## Task Overview
Generate depth-specific optimization hints for gravitational wave detection algorithms by
synthesizing multi-level reflections.
Current Optimization Depth: {parent_depth}/{max_depth} (shallow: structural patterns,
medium: implementation techniques, deep: mathematical details)
## Contextual Insights
1. Peer Algorithm Reflections (Depth {parent_depth}):
- Formatted as performance-annotated entries: [No.N Brother Reflection
Score: X]<
reflection>
- Time-ordered weighting (newest=highest priority) with objective score-based ranking
- Includes full technical post-mortems from immediate ancestors
{parent_reflections}
2. Father Algorithm Analysis (Depth {father_depth}):
{father_reflection}
## Hint Generation Requirements
1. Produce 3-5 executable optimization directives that:
- Integrate cross-depth insights from peer implementations
- Target {parent_depth}-level (shallow: structural patterns, medium: implementation
techniques, deep: mathematical details) components for improvement
- Formulate mathematically sound enhancements
- Align with gravitational wave data processing objectives
2. Output Format Rules
- Return optimization strategies within SINGLE BRACE
- Ensure entire response can be parseable by regex: \\{{(.*?)\\}} with DOTALL flag
- Focus on {parent_depth}-appropriate modifications
- Emphasize time-domain processing optimizations
## Critical Constraints
- Each directive must correspond to concrete code changes
- Explicitly connect to reflection insights where applicable
- Maintain strict {parent_depth}-level focus in all suggestions
- Exclude explanatory text within the hint brace
- Prioritize modifications matching current depth's optimization type
Sibling Selection and Weighting Strategy. The SC operation employs
a sophisticated parent selection mechanism that prioritizes high-performing
sibling algorithms:
1
# Select parents based on objective value weights
2
other = [ind for ind in pop if ind['code'] != father['code']]
3
weights = [1.0 / (-ind['fitness'] + 1e-10) for ind in other]
# Lower objective = higher weight
4
normalized_weights = [w / sum(weights) for w in weights]
5
parents = random.choices(other, weights=normalized_weights, k=min(self.m, len(other)))


--- Page 36 ---
36
LLM-guided automated algorithmic discovery
This weighting strategy ensures that successful algorithmic patterns from
high-performing siblings are more likely to influence the offspring generation
process.
Depth-Adaptive Optimization Focus. The first phase implements
depth-specific optimization strategies that adapt to the current position in the
MCTS tree:
• Shallow Depth (1-2): Focuses on structural patterns and control flow
restructuring
• Medium Depth (3-4): Emphasizes implementation techniques and numerical
optimizations
• Deep Depth (5+): Concentrates on mathematical details and advanced
computational methods
Phase 2: Concrete Algorithm Implementation. The second phase
transforms the optimization hints into executable algorithms:
## Algorithm Optimization Task
Develop an enhanced gravitational signal processing algorithm for interferometer data
analysis by implementing concrete improvements from multi-level code analysis.
## Technical Context
1. Optimization Depth Specifications:
- Current Focus Level: {depth} (max_depth={max_depth})
(1-2: Control flow restructuring, 3-4: Numerical computation optimizations, 5+:
Advanced linear algebra methods)
- Code Analysis Insights from Prior Level:
```text
{reflection}
```
2. Base Implementation Details:
[Functional Purpose] {algorithm_description}
[Core Implementation]
```python
{algorithm_code}
```
## Implementation Directives (Depth {depth}):
- Shallow (1-2): Restructure control flow using reflection suggestion (e.g., split data
conditioning/analysis phases)
- Medium (3-4): Apply numerical optimizations from reflection (e.g., FFT window size
optimization)
- Deep (5+): Implement matrix computation improvements from reflection (e.g., regularized
inverse covariance)
{external_knowledge}
## Output Format
- Place the core design idea in a sentence within a brace BEFORE the function definition
- For the core design idea format: \\{{A hybrid gravitational wave detection pipeline...}}
- Implement as Python function: {func_name}
- Inputs: {input_count} parameter(s) ({joined_inputs})
- Outputs: {output_count} return value(s) ({joined_outputs})
- Follow: {inout_inf}
- Constraints: {other_inf}
- IMPORTANT: All output code MUST be valid Python syntax. Do not place description text
inside curly braces within the function body.
- Example of correct format:
\\{{Core design description here}}
```python


--- Page 37 ---
LLM-guided automated algorithmic discovery
37
def pipeline_v2(strain_h1: np.ndarray, strain_l1: np.ndarray, times: np.ndarray) ->
tuple[np.ndarray, np.ndarray, np.ndarray]:
"""Core design description can alternatively be placed here as a docstring"""
# Function implementation...
```
## Important Notes
- Focus on algorithmic improvements rather than code style changes
- Ensure the new implementation directly addresses the reflection insights
Reflection Processing and Template Variables. The SC operation
processes multiple sources of algorithmic insight through structured template
variables:
• parent reflections: Performance-annotated reflections from peer algo-
rithms, formatted as ranked entries with objective scores
• father reflection: Analysis from the immediate parent algorithm at
depth-1
• reflection: Synthesized optimization hints generated in Phase 1
• algorithm description: Functional description of the base algorithm
• algorithm code: Complete implementation of the parent algorithm
Quality Assurance and Validation. The two-phase approach enables
comprehensive quality control:
• Phase 1 Validation:
– Ensures reflection insights are depth-appropriate
– Validates mathematical soundness of optimization suggestions
– Confirms alignment with gravitational wave processing objectives
• Phase 2 Validation:
– Verifies syntactic correctness of generated code
– Confirms interface compliance with standardized function signatures
– Tests algorithmic improvements against reflection insights
– Validates computational efficiency claims
Temporal Weighting and Performance Ranking. The SC oper-
ation implements sophisticated temporal weighting that prioritizes recent
algorithmic discoveries while maintaining objective score-based ranking:
• Time-ordered weighting: Newer algorithms receive higher priority in the
reflection synthesis
• Performance-based ranking: Algorithms with better objective scores con-
tribute more heavily to the optimization hints
• Cross-depth integration: Insights from both peer algorithms and parent-level
analysis are systematically combined
This comprehensive approach ensures that SC operations generate algorithms
that not only improve upon their immediate ancestors but also incorporate
the collective intelligence of high-performing siblings, leading to more robust
and efficient gravitational wave detection strategies.


--- Page 38 ---
38
LLM-guided automated algorithmic discovery
S1.4
Point Mutation Implementation
Point Mutation (PM) operations introduce targeted modifications to indi-
vidual algorithms based on performance analysis, implementing two distinct
approaches that offer different levels of sophistication and computational
investment. The framework provides both single-stage direct improvement and
two-stage reflection-driven enhancement strategies.
Single-Stage Point Mutation: Direct Algorithm Improvement.
The operation implements a straightforward approach that directly com-
pares an original algorithm with a high-performing elite algorithm to generate
improvements. This method prioritizes computational efficiency while main-
taining effective algorithmic enhancement.
Template Structure:
## Task Overview
You will analyze an original algorithm, an improved version of it, and create a new
enhanced algorithm. Below are the key components:
## Algorithm Details
1. ORIGINAL ALGORITHM:
- Description: {original_algorithm_description}
- Code:
```python
{original_algorithm_code}
```
- **Objective Value**: {original_objective_value}
2. BETTER ALGORITHM (Reference Implementation):
- Description: {better_algorithm_description}
- Code:
```python
{better_algorithm_code}
```
- **Objective Value**: {better_objective_value}
- Improvement Insights:
```text
{better_algorithm_reflection}
```
## Implementation Requirements
1. Analyze the differences between the original and better algorithms
2. Create a new algorithm that:
- Incorporates successful elements from the better algorithm
- Addresses limitations revealed in the improvement insights
- Produces better results than the original algorithm
3. Output format requirements:
- Place the core design idea in a sentence within a brace BEFORE the function
definition
- For the core design idea format: \\{{A hybrid gravitational wave detection pipeline
...}}
- Implement as Python function: {func_name}
- Inputs: {input_count} parameter(s) ({joined_inputs})
- Outputs: {output_count} return value(s) ({joined_outputs})
- Follow: {inout_inf}
- Constraints: {other_inf}
- IMPORTANT: All output code MUST be valid Python syntax. Do not place description text
inside curly braces within the function body.
- Example of correct format:
\\{{Core design description here}}
```python
def pipeline_v2(strain_h1: np.ndarray, strain_l1: np.ndarray, times: np.ndarray) ->
tuple[np.ndarray, np.ndarray, np.ndarray]:
"""Core design description can alternatively be placed here as a docstring"""


--- Page 39 ---
LLM-guided automated algorithmic discovery
39
# Function implementation...
```
{external_knowledge}
## Important Notes
- Focus on algorithmic improvements rather than code style changes
- Ensure the new implementation directly addresses the reflection insights
Two-Stage Point Mutation: Reflection-Driven Enhancement. The
operation implements a sophisticated two-phase approach that mirrors the sib-
ling crossover methodology but focuses on individual algorithm improvement
rather than horizontal knowledge transfer.
Phase 1: Strategic Reflection Generation. The first phase synthe-
sizes insights from multiple sources to generate comprehensive optimization
guidelines:
## Task Overview
Generate optimized technical guidelines for gravitational wave detection algorithms through
systematic analysis of multi-generational reflection insights. Focus on enhancing data
conditioning pipelines, time-frequency analysis methods, noise suppression techniques, and
H1-L1 detector coherence optimization. Produce executable directives addressing: waveform
recognition precision, computational complexity management, and non-stationary noise
differentiation while maintaining strict API compliance.
## Input Context
1. NEW INSIGHTS FROM RECENT ITERATIONS:
- Formatted as performance-annotated entries: [Parent N Reflection
Score: X]<
reflection>
- Time-ordered weighting (newest=highest priority) with objective score-based ranking
- Includes full technical post-mortems from immediate ancestors
{parent_reflections}
2. LONG-TERM REFLECTION REPOSITORY:
- Contains battle-tested insights from top 1% performers
- 3x weighting factor for architectural-level insights
- Curated through 3-stage filtration:
1. Statistical significance validation
2. Cross-generational effectiveness verification
3. Compatibility check with current detector configurations
{elite_reflection}
## Implementation Requirements
1. Perform weighted synthesis of reflections
2. Generate 3-5 technically-grounded optimization directives
3. Prioritize:
- Mitigation of historical implementation flaws
- Amplification of proven effective patterns
- Weighted integration of multi-generational insights
## Output Format
- Return all guidelines within SINGLE BRACE
- Ensure entire response can be parseable by regex: \\{{(.*?)\\}} with DOTALL flag
- Concrete technical directives only
- No explanatory text or formatting
Phase 2: Concrete Algorithm Implementation. The second phase
transforms the strategic insights into executable algorithmic improvements:
## Task Overview
Leverage insights from prior strategic reflection to architecturally enhance the
gravitational wave detection algorithm. Develop improvements that directly address
identified limitations in CRITICAL REFLECTION INSIGHTS while preserving core functionality
through:
1. Stage-level architectural modifications informed by reflection analysis


--- Page 40 ---
40
LLM-guided automated algorithmic discovery
2. Reflection-driven noise reduction and coherence enhancement strategies
3. Time-frequency analysis variations targeting specific weaknesses identified
4. H1-L1 synthesis improvements based on cross-detector insights
Generate architecturally distinct variants that implement reflection-derived concepts
through fundamental structural changes.
## Input Context
1. CRITICAL REFLECTION INSIGHTS (Improvement Basis):
```text
{reflection}
```
2. REFERENCE IMPLEMENTATION:
[Description] {elite_algorithm_description}
[Baseline Code]
```python
{elite_algorithm_code}
```
## Implementation Requirements
1. Execute reflection-guided analysis:
- Map reflection insights to specific code components
- Identify 2-3 architectural limitations in current implementation
2. Propose improvements that directly convert reflection insights into:
- Enhanced signal path architecture
- Novel noise handling structures
- Optimized computational patterns
- Advanced detector synergy mechanisms
3. Maintain strict interface compatibility with existing system integration
{external_knowledge}
## Output Format
- Place the core design idea in a sentence within a brace BEFORE the function definition
- For the core design idea format: \\{{A hybrid gravitational wave detection pipeline...}}
- Implement as Python function: {func_name}
- Inputs: {input_count} parameter(s) ({joined_inputs})
- Outputs: {output_count} return value(s) ({joined_outputs})
- Follow: {inout_inf}
- Constraints: {other_inf}
- IMPORTANT: All output code MUST be valid Python syntax. Do not place description text
inside curly braces within the function body.
- Example of correct format:
\\{{Core design description here}}
```python
def pipeline_v2(strain_h1: np.ndarray, strain_l1: np.ndarray, times: np.ndarray) ->
tuple[np.ndarray, np.ndarray, np.ndarray]:
"""Core design description can alternatively be placed here as a docstring"""
# Function implementation...
```
## Important Notes
- Focus on algorithmic improvements rather than code style changes
- Ensure the new implementation directly addresses the reflection insights
Selection Strategies and Elite Integration. Both PM operations lever-
age the elite offspring as a performance benchmark and source of successful
algorithmic patterns. The key distinction lies in their selection strategies:
• Single-Stage Selection Strategy:
– Selects a single parent algorithm from the population (excluding the elite)
– Directly compares parent performance with elite offspring
– Implements immediate improvement through direct analysis


--- Page 41 ---
LLM-guided automated algorithmic discovery
41
• Two-Stage Selection Strategy:
– Selects multiple parent algorithms for comprehensive reflection analysis
– Incorporates both recent algorithmic insights and long-term elite patterns
– Implements sophisticated multi-generational knowledge synthesis
Computational Efficiency Considerations. The two PM approaches
offer different computational trade-offs:
• Single-Stage Advantages:
– Single-stage processing reduces computational overhead
– Direct comparison enables rapid algorithm improvement
– Simplified prompting reduces LLM token consumption
• Two-Stage Advantages:
– Two-stage processing enables more sophisticated optimization
– Multi-generational insight integration leads to more robust improvements
– Reflection-driven approach produces more interpretable algorithmic mod-
ifications
Template Variable Integration. Both PM operations incorporate com-
prehensive template variables that enable systematic algorithmic improve-
ment:
• Common Variables:
– func name, input count, output count: Interface specification
– joined inputs, joined outputs: Parameter documentation
– better algorithm description, better algorithm code: Elite algo-
rithm details
– original objective value,
better objective value:
Performance
metrics
– better algorithm reflection: Elite algorithm insights
• Two-Stage Variables:
– parent reflections: Multi-parent reflection synthesis
– elite reflection: Long-term elite insights
– reflection: Generated optimization guidelines
Quality Assurance and Validation. Both PM operations implement
rigorous validation procedures:
• Single-Stage Validation:
– Direct performance comparison with both parent and elite algorithms
– Verification of improvement insight integration
– Confirmation of interface compliance
• Two-Stage Validation:


--- Page 42 ---
42
LLM-guided automated algorithmic discovery
– Two-stage validation covering both reflection generation and implemen-
tation
– Cross-generational consistency checking
– Architectural improvement verification
The dual PM approach provides flexibility in algorithmic improvement
strategies, enabling the framework to adapt to different optimization scenarios
while maintaining consistent quality standards and interface compliance.
S1.5
Path-wise Crossover Implementation
Path-wise Crossover (PWC) operations synthesize information along complete
root-to-leaf trajectories in the MCTS tree, capturing long-range dependen-
cies and enabling global optimization strategies. The framework implements
two distinct PWC approaches that differ in their analytical methodologies:
reflection-based synthesis and comprehensive algorithm analysis.
Reflection-Based Path-wise Crossover: Multi-Algorithm Insight
Synthesis. The operation implements a two-stage process that analyzes reflec-
tion patterns across multiple algorithms in a complete MCTS path to identify
generalizable optimization principles.
Phase 1: Cross-Algorithm Pattern Analysis. The first phase extracts
recurring technical strategies from multiple algorithm reflections:
## Task Overview
Analyze and synthesize technical reflections from multiple algorithm iterations to identify
cross-algorithm optimization patterns and guide next-generation algorithm design.
Prioritize extraction of generalizable technical principles over implementation-specific
details.
Current Optimization Depth: depth/max_depth={depth}/{max_depth} (shallow: structural
patterns, medium: implementation techniques, deep: mathematical details)
## Input Context
Analyzing {num_algorithms} algorithm reflections from MCTS exploration trajectories.
Technical reflections follow depth-specific analysis requirements. Structural format: [No.N
algorithm's reflection (depth: X)]<reflection>
{algorithm_reflections}
## Reflection Requirements
1. **Pattern Identification** (Key Observed Patterns):
- Extract 2-3 recurring technical strategies (e.g. "Multi-scale wavelet decomposition"
not "used Morlet wavelet")
- Categorize by analysis level:
* Structural: Component architecture (e.g. "Parallel filter banks")
* Implementation: Algorithmic choices (e.g. "Adaptive thresholding")
* Mathematical: Core transforms (e.g. "Orthogonal matching pursuit")
2. **Technical Pathway Analysis** (Promising Technical Pathways):
- Identify under-utilized but theoretically sound approaches (e.g. "Sparse
representation in frequency domain")
- Specify required technical components without code details (e.g. "Requires:
Overcomplete basis construction")
3. **Optimization Principles** (Strategic Optimization Principles):
- Formulate depth-specific guidelines (e.g. "At mathematical level: Maximize time-
frequency product $\leq$ 0.5")
- Relate physical constraints to algorithmic parameters (e.g. "Wavelet duration should
match typical glitch durations")
4. **Specificity Balance**:


--- Page 43 ---
LLM-guided automated algorithmic discovery
43
- Technical specificity: Name mathematical concepts (e.g. "Gabor uncertainty") and
signal processing domains
- Implementation avoidance: Omit code structures (e.g. "Avoid: 'Use 3 nested loops'")
## Output Format Rules
- Return optimization strategies within SINGLE BRACE
- Ensure entire response can be parseable by regex: \\{{(.*?)\\}} with DOTALL flag
- Do not include markdown formatting or additional explanations
Phase 2: Algorithm Implementation. The second phase transforms
the synthesized insights into concrete algorithmic improvements:
## Task Overview
Develop an enhanced gravitational wave detection algorithm through targeted modifications
addressing specific technical shortcomings identified in the reflection analysis.
## Input Context
[Critical Reflection Insights]
```text
{reflection}
```
[Baseline Implementation]
[Functional Description] {algorithm_description}
[Current Codebase]
```python
{algorithm_code}
```
{external_knowledge}
## Output Format
- Place the core design idea in a sentence within a brace BEFORE the function definition
- For the core design idea format: \\{{A hybrid gravitational wave detection pipeline...}}
- Implement as Python function: {func_name}
- Inputs: {input_count} parameter(s) ({joined_inputs})
- Outputs: {output_count} return value(s) ({joined_outputs})
- Follow: {inout_inf}
- Constraints: {other_inf}
- IMPORTANT: All output code MUST be valid Python syntax. Do not place description text
inside curly braces within the function body.
- Example of correct format:
\\{{Core design description here}}
```python
def pipeline_v2(strain_h1: np.ndarray, strain_l1: np.ndarray, times: np.ndarray) -> tuple
[np.ndarray, np.ndarray, np.ndarray]:
"""Core design description can alternatively be placed here as a docstring"""
# Function implementation...
```
## Important Notes
- Focus on algorithmic improvements rather than code style changes
- Ensure the new implementation directly addresses the reflection insights
Comprehensive Algorithm Analysis Path-wise Crossover: Multi-
Level Technical Synthesis. The operation implements a more sophisticated
analytical approach that examines complete algorithm implementations across
different depth levels.
Phase 1: Multi-Level Technical Analysis. The first phase conducts
comprehensive analysis of algorithms along the complete MCTS path:
## Task Objective
Synthesize technical insights from algorithm evolution MCTS path to guide targeted
improvements. Current Analysis Level: depth/max_depth={depth}/{max_depth} (1-2: structural,
3-4: implementation, 5+: mathematical)


--- Page 44 ---
44
LLM-guided automated algorithmic discovery
## Depth-Specific Focus
- Shallow (Depth 1-2): Structural patterns & control flow
- Medium (Depth 3-4): Implementation techniques & parameterization
- Deep (Depth 5+): Mathematical formulations & computational primitives
## Input Context
Analyzing {num_algorithms} algorithm reflections from MCTS exploration trajectories.
Technical reflections follow depth-specific analysis requirements. Structural format: [No.N
algorithm's reflection (depth: X)]<description><objective><code>
{parent_info}
## Synthesis Process
1. Cross-Level Insight Integration:
- Identify key recurring technical strategies across abstraction levels
- Note level-specific constraints affecting current implementations
2. Domain Compliance Verification:
- Validate approaches against gravitational wave signal characteristics
- Check numerical reliability across different implementation levels
3. Improvement Planning:
- Structural: Adjust data processing pipelines
- Implementation: Optimize critical parameter relationships
- Mathematical: Enhance core transformation components
## Technical Workflow
### 1. Multi-Level Technical Analysis
Structural -> Compare module composition and interaction patterns
Implementation -> Assess parameter sensitivity and adaptation logic
Mathematical -> Examine transformation kernels and precision handling
### 2. Level-Appropriate Optimization
For current depth={depth}:
- Select 2-4 improvement focus areas with technical rationale
- Define implementation requirements for each focus area
- Establish verification criteria with domain constraints
## Output Format Rules
- Return optimization strategies within SINGLE BRACE
- Ensure entire response can be parseable by regex: \\{{(.*?)\\}} with DOTALL flag
- Do not include markdown formatting or additional explanations
Phase 2: Algorithm Implementation. The operation shares the same
implementation phase as the reflection-based path-wise crossover, utilizing
the reflection-based PWC template for consistent output formatting and
algorithmic generation.
Methodological
Distinctions.
The
key
differences
between
the
reflection-based path-wise crossover and the comprehensive algorithm analysis
PWC lie in their analytical strategies:
• Reflection-Based PWC:
– Focuses on synthesizing existing reflection insights from multiple algo-
rithms
– Emphasizes pattern recognition across previously analyzed algorithmic
behaviors
– Prioritizes extraction of generalizable technical principles over implemen-
tation details
– Categorizes insights by structural, implementation, and mathematical
analysis levels


--- Page 45 ---
LLM-guided automated algorithmic discovery
45
• Comprehensive Algorithm Analysis PWC:
– Conducts direct analysis of complete algorithm implementations
– Examines algorithmic components across multiple depth levels simultane-
ously
– Integrates cross-level insights through systematic technical workflow
– Emphasizes domain compliance verification and improvement planning
Depth-Adaptive Processing. Both PWC operations implement depth-
specific analysis strategies that adapt to the current position in the MCTS
tree:
• Shallow Depth Focus (1-2):
– Structural patterns and component architecture analysis
– Control flow restructuring and module composition optimization
– Data processing pipeline adjustments
• Medium Depth Focus (3-4):
– Implementation techniques and algorithmic parameter optimization
– Critical parameter relationship assessment
– Numerical computation enhancement strategies
• Deep Depth Focus (5+):
– Mathematical formulation analysis and computational primitive optimiza-
tion
– Transformation kernel examination and precision handling
– Advanced linear algebra method integration
Path Trajectory Analysis. Both operations process algorithms along
complete MCTS paths, with depth tracking that enables comprehensive
evolutionary analysis:
• Reflection-Based PWC Path Processing:
– Analyzes reflection patterns from algorithms at decreasing depth levels
– Tracks depth-specific insights through structured format annotations
– Synthesizes cross-depth technical strategies for optimization guidance
• Comprehensive Algorithm Analysis PWC Path Processing:
– Examines complete algorithm implementations with performance metrics
– Integrates algorithmic descriptions, objective values, and code analysis
– Conducts multi-level technical synthesis across the entire path trajectory
Template Variable Integration. Both PWC operations incorporate
sophisticated template variables that enable comprehensive path analysis:
• Common Variables:
– depth, max depth: Depth-specific processing parameters


--- Page 46 ---
46
LLM-guided automated algorithmic discovery
– num algorithms: Path length and analysis scope
– func name, input count, output count: Interface specifications
– external knowledge: Domain knowledge integration
• Reflection-Based PWC Variables:
– algorithm reflections: Multi-algorithm reflection synthesis
– reflection: Generated optimization insights
• Comprehensive Algorithm Analysis PWC Variables:
– parent info: Complete algorithm implementation details
– current algorithm description, current algorithm code: Baseline
algorithm specifications
– current objective value: Performance reference metrics
Quality Assurance and Validation. Both PWC operations implement
comprehensive validation procedures:
• Reflection-Based PWC Validation:
– Pattern identification verification across multiple algorithm reflections
– Technical pathway analysis consistency checking
– Optimization principle formulation validation
• Comprehensive Algorithm Analysis PWC Validation:
– Multi-level technical analysis coherence verification
– Domain compliance checking across different implementation levels
– Cross-level insight integration validation
The dual PWC approach provides complementary strategies for capturing
long-range dependencies in the MCTS tree, enabling the framework to syn-
thesize insights across complete evolutionary trajectories while maintaining
depth-specific optimization focus and domain knowledge integration.
S1.6
Domain Knowledge Integration
Domain knowledge integration serves as a critical component that ensures gen-
erated algorithms remain grounded in gravitational wave detection principles
while encouraging exploration beyond traditional linear processing methods.
The framework incorporates specialized domain expertise through structured
knowledge templates that guide algorithmic development toward physically
meaningful and computationally efficient solutions.
External Knowledge Template Structure. The domain knowledge
integration employs a comprehensive template that emphasizes non-linear
processing approaches and adaptive algorithmic strategies:
### External Knowledge Integration
1. **Non-linear** Processing Core Concepts:
- Signal Transformation:
* Non-linear vs linear decomposition
* Adaptive threshold mechanisms


--- Page 47 ---
LLM-guided automated algorithmic discovery
47
* Multi-scale analysis
- Feature Extraction:
* Phase space reconstruction
* Topological data analysis
* Wavelet-based detection
- Statistical Analysis:
* Robust estimators
* Non-Gaussian processes
* Higher-order statistics
2. Implementation Principles:
- Prioritize adaptive over fixed parameters
- Consider local vs global characteristics
- Balance computational cost with accuracy
Non-linear Processing Emphasis. The domain knowledge framework
explicitly prioritizes non-linear algorithmic approaches over traditional lin-
ear methods, recognizing that gravitational wave signals exhibit complex,
transient characteristics that require sophisticated analysis techniques. This
emphasis addresses fundamental limitations in conventional matched filtering
approaches that rely heavily on linear processing assumptions.
Signal Transformation Guidance. The domain knowledge provides spe-
cific guidance on signal transformation strategies that leverage advanced signal
processing concepts:
• Non-linear vs Linear Decomposition: The framework encourages exploration
of non-linear decomposition methods that can capture complex signal mor-
phologies beyond the capabilities of traditional Fourier-based approaches.
This includes techniques such as empirical mode decomposition, intrinsic
mode functions, and adaptive basis construction.
• Adaptive Threshold Mechanisms: Rather than employing fixed threshold
values, the domain knowledge promotes adaptive thresholding strategies
that respond to local signal characteristics and noise conditions. This
approach enables more robust detection performance across diverse obser-
vational scenarios.
• Multi-scale Analysis: The framework emphasizes multi-scale signal analysis
techniques that can simultaneously capture both short-duration transient
signals and longer-duration continuous wave sources. This includes wavelet-
based methods, time-frequency analysis, and hierarchical decomposition
strategies.
• Feature Extraction Methodologies. The domain knowledge incorporates
advanced feature extraction approaches that extend beyond traditional
signal processing paradigms:
– Phase Space Reconstruction: The framework encourages exploration of
phase space reconstruction techniques that can reveal hidden dynamical
structures in gravitational wave data. This includes embedding dimension
analysis, recurrence analysis, and attractor reconstruction methods.
– Topological Data Analysis: The domain knowledge promotes topologi-
cal data analysis approaches that can identify persistent features and


--- Page 48 ---
48
LLM-guided automated algorithmic discovery
structural patterns in high-dimensional gravitational wave data. This
includes persistent homology, Mapper algorithms, and topological feature
extraction.
– Wavelet-based Detection: The framework emphasizes wavelet-based detec-
tion strategies that can provide optimal time-frequency resolution for
transient signal analysis. This includes continuous wavelet transforms,
discrete wavelet decomposition, and wavelet packet analysis.
• Statistical Analysis Enhancement. The domain knowledge integrates sophis-
ticated statistical analysis techniques that account for the complex noise
characteristics of gravitational wave detectors:
– Robust Estimators: The framework promotes robust statistical estima-
tors that can maintain performance in the presence of outliers and
non-Gaussian noise distributions. This includes median-based estimators,
M-estimators, and trimmed mean approaches.
– Non-Gaussian Processes: The domain knowledge emphasizes analysis
techniques that can handle non-Gaussian noise processes commonly
encountered in gravitational wave data. This includes heavy-tailed distri-
butions, skewed probability models, and non-stationary noise characteri-
zation.
– Higher-order Statistics: The framework encourages exploration of higher-
order statistical moments and cumulants that can capture subtle signal
characteristics beyond second-order analysis. This includes bispectrum
analysis, higher-order moment estimation, and polyspectral techniques.
• Implementation Principles and Constraints. The domain knowledge pro-
vides specific implementation principles that guide algorithmic development
toward practical and efficient solutions:
– Adaptive Parameter Prioritization: The framework emphasizes adaptive
parameter selection over fixed parameter values, enabling algorithms to
respond dynamically to changing signal and noise conditions. This prin-
ciple encourages exploration of learning-based parameter adjustment,
feedback control mechanisms, and online adaptation strategies.
– Local vs Global Characteristics: The domain knowledge promotes con-
sideration of both local signal characteristics and global data patterns,
enabling algorithms to balance fine-grained analysis with comprehen-
sive signal understanding. This includes local stationarity analysis, global
trend estimation, and multi-resolution processing approaches.
– Computational Cost-Accuracy Balance: The framework provides guidance
on balancing computational efficiency with detection accuracy, ensuring
that generated algorithms remain practical for real-time implementa-
tion while maintaining scientific rigor. This includes complexity analysis,
algorithmic optimization, and performance benchmarking considerations.
Integration Across Evolutionary Operations. The domain knowl-
edge template is systematically integrated across all evolutionary operations


--- Page 49 ---
LLM-guided automated algorithmic discovery
49
(PC, SC, PM, PWC) through the external knowledge template variable.
This ensures consistent application of gravitational wave detection principles
regardless of the specific evolutionary strategy employed.
Physical Validity Assurance. The domain knowledge template ensures
that all generated algorithms respect fundamental physical constraints related
to gravitational wave signal characteristics, detector limitations, and noise
properties.
Computational Feasibility. The implementation principles guide algo-
rithmic development toward computationally feasible solutions that can be
practically implemented within the constraints of current computational
resources and real-time processing requirements.
This comprehensive domain knowledge integration creates a robust frame-
work for scientifically grounded algorithmic discovery, ensuring that the evolu-
tionary process generates algorithms that are both innovative and practically
applicable to gravitational wave detection challenges.
S1.7
Error Handling and Iterative Refinement
The Evo-MCTS framework incorporates a robust error handling mechanism
that enables iterative refinement of generated algorithms through automated
debugging and correction processes. When generated code encounters execu-
tion errors, fails to detect signals, or exceeds computational time limits, the
system employs a rechat strategy using advanced reasoning models to diagnose
and resolve issues.
Error Detection and Classification. The framework monitors three
primary failure modes during algorithm execution: (i) runtime exceptions and
syntax errors that prevent code execution, (ii) algorithmic failures where no
gravitational wave signals are detected despite their presence in the data, and
(iii) computational timeout scenarios where algorithms exceed predefined exe-
cution limits. Each failure mode triggers specific diagnostic protocols tailored
to the underlying issue type.
Iterative Refinement Protocol. Upon error detection, the system
implements a structured refinement process through a carefully designed
prompt content structure. The system constructs conversation messages in a
specific format to facilitate effective error correction:
Initially, when no system content is provided, the framework creates a
message list containing a single user role entry with the original prompt content
(prompt content):
messages = [{"role": "user", "content": prompt content}]
When a rechat response is available (indicating a previous failed attempt),
the system extends the conversation by first appending the assistant’s previous
response (rechat response):
messages.append({"role": "assistant", "content":
rechat response})
Subsequently, the system adds a new user message that explicitly requests
debugging and issue resolution (prompt content):


--- Page 50 ---
50
LLM-guided automated algorithmic discovery
messages.append({"role": "user", "content": "Your previous
code had execution errors, couldn’t find signals, or timed out.
Please debug and fix the issues:\n\n" + prompt content})
This structured approach maintains the conversational context while
providing clear guidance for error correction, ensuring that the assistant under-
stands both the original requirements and the specific issues that need to be
addressed.
Automated Debugging Integration. The error handling system lever-
ages advanced reasoning capabilities to analyze failed algorithms and propose
targeted corrections. This approach maintains the evolutionary optimization
trajectory while addressing immediate technical obstacles that could otherwise
terminate the search process. The iterative refinement ensures that promising
algorithmic concepts are not discarded due to implementation errors, instead
receiving corrective guidance to achieve functional implementations.
S1.8
Post-Generation Analysis and Knowledge
Extraction
The post-generation analysis phase extracts interpretable insights from evolved
algorithms through automated knowledge distillation. This process trans-
forms the raw algorithmic implementations into concise, human-readable
descriptions that capture the essential design principles and operational
characteristics of discovered solutions.
Algorithm Description Generation. The framework employs a struc-
tured prompt template to generate concise algorithm descriptions that high-
light critical design decisions and implementation strategies. The prompt
construction follows a systematic format:
Following is the Design Idea of a heuristic algorithm for the problem and the code with
function name 'pipeline_v2' for implementing the heuristic algorithm.
{prompt_inout_inf} {prompt_other_inf}
Design Idea:
{algorithm}
Code:
```python
{code}
```
The content of the Design Idea cannot fully represent what the algorithm does. So, now you
should re-describe the algorithm using less than 3 sentences.
Hint: You should reference the given Design Idea and highlight the most critical design
ideas of the code. You can analyse the code to describe which variables are given higher
priorities and which variables are given lower priorities, the parameters and the structure
of the code.
This template systematically combines the original design concept with
the implemented code, requesting a refined description that captures the
algorithm’s core operational principles. The analysis focuses on parameter
prioritization, structural characteristics, and critical design decisions that
distinguish the evolved solution.
Knowledge Extraction Protocol. The post-generation analysis cap-
tures key design principles and compresses algorithmic representations into


--- Page 51 ---
LLM-guided automated algorithmic discovery
51
human-readable summaries. This reflection process identifies algorithmic inno-
vations, signal processing techniques, and computational characteristics while
reducing token consumption to prevent context window overflow in subsequent
LLM interactions.
Interpretability Enhancement. The generated descriptions provide
concise algorithmic summaries that enable efficient reference to previous
discoveries without overwhelming the LLM context, facilitating continued
exploration while maintaining algorithmic memory across generations.
S1.9
Code Examples and Case Studies
This section presents a detailed examination of the highest-performing algo-
rithm discovered during the Evo-MCTS optimization process, corresponding
to node 486 (as shown in Figure 5a in the main text) which achieved the maxi-
mum fitness score of 5,241.37 units. The algorithm demonstrates sophisticated
multi-stage signal processing techniques that emerged through evolutionary
optimization.
Algorithm
Overview. The evolved algorithm implements a four-
stage pipeline combining robust baseline detrending, adaptive whitening
with enhanced power spectral density (PSD) smoothing, coherent time-
frequency analysis with frequency-conditioned regularization, and multi-
resolution thresholding with octave-spaced dyadic wavelet validation. This
architecture represents a novel synthesis of classical signal processing tech-
niques with adaptive parameter selection mechanisms.
Stage 1: Robust Baseline Detrending. The algorithm initiates with
median filtering-based detrending to remove long-term instrumental drifts and
environmental variations. The median filter kernel size of 101 samples provides
robust trend removal while preserving transient gravitational wave signatures.
This preprocessing stage establishes a stable baseline for subsequent whitening
operations.
Stage 2: Adaptive Whitening with Enhanced PSD Smoothing.
The core innovation lies in the adaptive whitening mechanism that dynami-
cally adjusts window parameters based on data characteristics. The algorithm
implements Tukey windowing with 75% overlap and adaptive segment lengths
constrained between 5-30 seconds, optimizing spectral estimation for vary-
ing noise conditions. The PSD smoothing employs exponential filtering with
stationarity-dependent coefficients (0.75-0.85 range), while Tikhonov regular-
ization provides frequency-dependent gain control. Savitzky-Golay filtering
generates causal-like gradients, and sigmoid-based nonlinear scaling enhances
spectral features through adaptive gain factors.
Stage 3: Coherent Time-Frequency Analysis. The algorithm com-
putes complex spectrograms preserving phase information across both detec-
tors, enabling coherent analysis of gravitational wave signatures. Phase
difference calculations and coherence estimation provide cross-detector valida-
tion, while frequency-conditioned regularization balances phase alignment with


--- Page 52 ---
52
LLM-guided automated algorithmic discovery
noise characteristics. The integration of axial curvature estimates through sec-
ond derivatives and nonlinear activation functions (tanh-based boost) enhances
signal discrimination capabilities.
Stage 4: Multi-Resolution Validation. The final stage implements
sophisticated peak detection using robust statistical measures (median abso-
lute deviation) combined with octave-spaced dyadic wavelet validation. Con-
tinuous wavelet transform coefficients across scales 1-8 provide multi-resolution
signal verification, while Gaussian-weighted uncertainty estimation quantifies
detection confidence intervals.
1
import numpy as np
2
import scipy.signal as signal
3
from scipy.signal.windows import tukey
4
from scipy.signal import savgol_filter
5
6
def pipeline_v2(strain_h1: np.ndarray, strain_l1: np.ndarray, times: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
7
"""
8
The pipeline function processes gravitational wave data from the H1 and L1 detectors to identify potential gravitational
wave signals.
,→
9
It takes strain_h1 and strain_l1 numpy arrays containing detector data, and times array with corresponding time points.
10
The function returns a tuple of three numpy arrays: peak_times containing GPS times of identified events,
11
peak_heights with significance values of each peak, and peak_deltat showing time window uncertainty for each peak.
12
"""
13
eps = np.finfo(float).tiny
14
dt = times[1] - times[0]
15
fs = 1.0 / dt
16
# Base spectrogram parameters
17
base_nperseg = 256
18
base_noverlap = base_nperseg // 2
19
medfilt_kernel = 101
# odd kernel size for robust detrending
20
uncertainty_window = 5
# half-window for local timing uncertainty
21
22
# -------------------- Stage 1: Robust Baseline Detrending --------------------
23
# Remove long-term trends using a median filter for each channel.
24
detrended_h1 = strain_h1 - signal.medfilt(strain_h1, kernel_size=medfilt_kernel)
25
detrended_l1 = strain_l1 - signal.medfilt(strain_l1, kernel_size=medfilt_kernel)
26
27
# -------------------- Stage 2: Adaptive Whitening with Enhanced PSD Smoothing --------------------
28
def adaptive_whitening(strain: np.ndarray) -> np.ndarray:
29
# Center the signal.
30
centered = strain - np.mean(strain)
31
n_samples = len(centered)
32
# Adaptive window length: between 5 and 30 seconds
33
win_length_sec = np.clip(n_samples / fs / 20, 5, 30)
34
nperseg_adapt = int(win_length_sec * fs)
35
nperseg_adapt = max(10, min(nperseg_adapt, n_samples))
36
37
# Create a Tukey window with 75% overlap.
38
tukey_alpha = 0.25
39
win = tukey(nperseg_adapt, alpha=tukey_alpha)
40
noverlap_adapt = int(nperseg_adapt * 0.75)
41
if noverlap_adapt >= nperseg_adapt:
42
noverlap_adapt = nperseg_adapt - 1
43
44
# Estimate the power spectral density (PSD) using Welch's method.
45
freqs, psd = signal.welch(centered, fs=fs, nperseg=nperseg_adapt,
46
noverlap=noverlap_adapt, window=win, detrend='constant')
47
psd = np.maximum(psd, eps)
48
49
# Compute relative differences for PSD stationarity measure.
50
diff_arr = np.abs(np.diff(psd)) / (psd[:-1] + eps)
51
# Smooth the derivative with a moving average.
52
if len(diff_arr) >= 3:
53
smooth_diff = np.convolve(diff_arr, np.ones(3)/3, mode='same')
54
else:
55
smooth_diff = diff_arr
56
57
# Exponential smoothing (Kalman-like) with adaptive alpha using PSD stationarity.
58
smoothed_psd = np.copy(psd)
59
for i in range(1, len(psd)):


--- Page 53 ---
LLM-guided automated algorithmic discovery
53
60
# Adaptive smoothing coefficient: base 0.8 modified by local stationarity (±0.05)
61
local_alpha = np.clip(0.8 - 0.05 * smooth_diff[min(i-1, len(smooth_diff)-1)], 0.75, 0.85)
62
smoothed_psd[i] = local_alpha * smoothed_psd[i-1] + (1 - local_alpha) * psd[i]
63
64
# Compute Tikhonov regularization gain based on deviation from median PSD.
65
noise_baseline = np.median(smoothed_psd)
66
raw_gain = (smoothed_psd / (noise_baseline + eps)) - 1.0
67
68
# Compute a causal-like gradient using the Savitzky-Golay filter.
69
win_len = 11 if len(smoothed_psd) >= 11 else ((len(smoothed_psd)//2)*2+1)
70
polyorder = 2 if win_len > 2 else 1
71
delta_freq = np.mean(np.diff(freqs))
72
grad_psd = savgol_filter(smoothed_psd, win_len, polyorder, deriv=1, delta=delta_freq, mode='interp')
73
74
# Nonlinear scaling via sigmoid to enhance gradient differences.
75
sigmoid = lambda x: 1.0 / (1.0 + np.exp(-x))
76
scaling_factor = 1.0 + 2.0 * sigmoid(np.abs(grad_psd) / (np.median(smoothed_psd) + eps))
77
78
# Compute adaptive gain factors with nonlinear scaling.
79
gain = 1.0 - np.exp(-0.5 * scaling_factor * raw_gain)
80
gain = np.clip(gain, -8.0, 8.0)
81
82
# FFT-based whitening: interpolate gain and PSD onto FFT frequency bins.
83
signal_fft = np.fft.rfft(centered)
84
freq_bins = np.fft.rfftfreq(n_samples, d=dt)
85
interp_gain = np.interp(freq_bins, freqs, gain, left=gain[0], right=gain[-1])
86
interp_psd = np.interp(freq_bins, freqs, smoothed_psd, left=smoothed_psd[0], right=smoothed_psd[-1])
87
denom = np.sqrt(interp_psd) * (np.abs(interp_gain) + eps)
88
denom = np.maximum(denom, eps)
89
white_fft = signal_fft / denom
90
whitened = np.fft.irfft(white_fft, n=n_samples)
91
return whitened
92
93
# Whiten H1 and L1 channels using the adapted method.
94
white_h1 = adaptive_whitening(detrended_h1)
95
white_l1 = adaptive_whitening(detrended_l1)
96
97
# -------------------- Stage 3: Coherent Time-Frequency Metric with Frequency-Conditioned Regularization
--------------------
,→
98
def compute_coherent_metric(w1: np.ndarray, w2: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
99
# Compute complex spectrograms preserving phase information.
100
f1, t_spec, Sxx1 = signal.spectrogram(w1, fs=fs, nperseg=base_nperseg,
101
noverlap=base_noverlap, mode='complex', detrend=False)
102
f2, t_spec2, Sxx2 = signal.spectrogram(w2, fs=fs, nperseg=base_nperseg,
103
noverlap=base_noverlap, mode='complex', detrend=False)
104
# Ensure common time axis length.
105
common_len = min(len(t_spec), len(t_spec2))
106
t_spec = t_spec[:common_len]
107
Sxx1 = Sxx1[:, :common_len]
108
Sxx2 = Sxx2[:, :common_len]
109
110
# Compute phase differences and coherence between detectors.
111
phase_diff = np.angle(Sxx1) - np.angle(Sxx2)
112
phase_coherence = np.abs(np.cos(phase_diff))
113
114
# Estimate median PSD per frequency bin from the spectrograms.
115
psd1 = np.median(np.abs(Sxx1)**2, axis=1)
116
psd2 = np.median(np.abs(Sxx2)**2, axis=1)
117
118
# Frequency-conditioned regularization gain (reflection-guided).
119
lambda_f = 0.5 * ((np.median(psd1) / (psd1 + eps)) + (np.median(psd2) / (psd2 + eps)))
120
lambda_f = np.clip(lambda_f, 1e-4, 1e-2)
121
# Regularization denominator integrating detector PSDs and lambda.
122
reg_denom = (psd1[:, None] + psd2[:, None] + lambda_f[:, None] + eps)
123
124
# Weighted phase coherence that balances phase alignment with noise levels.
125
weighted_comp = phase_coherence / reg_denom
126
127
# Compute axial (frequency) second derivatives as curvature estimates.
128
d2_coh = np.gradient(np.gradient(phase_coherence, axis=0), axis=0)
129
avg_curvature = np.mean(np.abs(d2_coh), axis=0)
130
131
# Nonlinear activation boost using tanh for regions of high curvature.
132
nonlinear_boost = np.tanh(5 * avg_curvature)
133
linear_boost = 1.0 + 0.1 * avg_curvature
134


--- Page 54 ---
54
LLM-guided automated algorithmic discovery
135
# Cross-detector synergy: weight derived from global median consistency.
136
novel_weight = np.mean((np.median(psd1) + np.median(psd2)) / (psd1[:, None] + psd2[:, None] + eps), axis=0)
137
138
# Integrated time-frequency metric combining all enhancements.
139
tf_metric = np.sum(weighted_comp * linear_boost * (1.0 + nonlinear_boost), axis=0) * novel_weight
140
141
# Adjust the spectrogram time axis to account for window delay.
142
metric_times = t_spec + times[0] + (base_nperseg / 2) / fs
143
return tf_metric, metric_times
144
145
tf_metric, metric_times = compute_coherent_metric(white_h1, white_l1)
146
147
# -------------------- Stage 4: Multi-Resolution Thresholding with Octave-Spaced Dyadic Wavelet Validation
--------------------
,→
148
def multi_resolution_thresholding(metric: np.ndarray, times_arr: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
149
# Robust background estimation with median and MAD.
150
bg_level = np.median(metric)
151
mad_val = np.median(np.abs(metric - bg_level))
152
robust_std = 1.4826 * mad_val
153
threshold = bg_level + 1.5 * robust_std
154
155
# Identify candidate peaks using prominence and minimum distance criteria.
156
peaks, _ = signal.find_peaks(metric, height=threshold, distance=2, prominence=0.8 * robust_std)
157
if peaks.size == 0:
158
return np.array([]), np.array([]), np.array([])
159
160
# Local uncertainty estimation using a Gaussian-weighted convolution.
161
win_range = np.arange(-uncertainty_window, uncertainty_window + 1)
162
sigma = uncertainty_window / 2.5
163
gauss_kernel = np.exp(-0.5 * (win_range / sigma) ** 2)
164
gauss_kernel /= np.sum(gauss_kernel)
165
weighted_mean = np.convolve(metric, gauss_kernel, mode='same')
166
weighted_sq = np.convolve(metric ** 2, gauss_kernel, mode='same')
167
variances = np.maximum(weighted_sq - weighted_mean ** 2, 0.0)
168
uncertainties = np.sqrt(variances)
169
uncertainties = np.maximum(uncertainties, 0.01)
170
171
valid_times = []
172
valid_heights = []
173
valid_uncerts = []
174
n_metric = len(metric)
175
176
# Compute a simple second derivative for local curvature checking.
177
if n_metric > 2:
178
second_deriv = np.diff(metric, n=2)
179
second_deriv = np.pad(second_deriv, (1, 1), mode='edge')
180
else:
181
second_deriv = np.zeros_like(metric)
182
183
# Use octave-spaced scales (dyadic wavelet validation) to validate peak significance.
184
widths = np.arange(1, 9)
# approximate scales 1 to 8
185
for peak in peaks:
186
# Skip peaks lacking sufficient negative curvature.
187
if second_deriv[peak] > -0.1 * robust_std:
188
continue
189
local_start = max(0, peak - uncertainty_window)
190
local_end = min(n_metric, peak + uncertainty_window + 1)
191
local_segment = metric[local_start:local_end]
192
if len(local_segment) < 3:
193
continue
194
try:
195
cwt_coeff = signal.cwt(local_segment, signal.ricker, widths)
196
except Exception:
197
continue
198
max_coeff = np.max(np.abs(cwt_coeff))
199
# Threshold for validating the candidate using local MAD.
200
cwt_thresh = mad_val * np.sqrt(2 * np.log(len(local_segment) + eps))
201
if max_coeff >= cwt_thresh:
202
valid_times.append(times_arr[peak])
203
valid_heights.append(metric[peak])
204
valid_uncerts.append(uncertainties[peak])
205
206
if len(valid_times) == 0:
207
return np.array([]), np.array([]), np.array([])
208
return np.array(valid_times), np.array(valid_heights), np.array(valid_uncerts)
209


--- Page 55 ---
LLM-guided automated algorithmic discovery
55
210
peak_times, peak_heights, peak_deltat = multi_resolution_thresholding(tf_metric, metric_times)
211
return peak_times, peak_heights, peak_deltat
Reflective
Reasoning
Integration.
The
framework
incorporates
advanced reflective reasoning capabilities through DeepSeek-R1’s internal
thinking processes, enabling systematic analysis and refinement of algo-
rithmic solutions. This reflective mechanism operates through structured
thinking templates that guide the model through comprehensive evaluation
and improvement cycles.
Thinking Process Template. The reflective reasoning follows a sys-
tematic template structure that captures the model’s internal deliberation
process:
<thinking>
Okay, let's tackle this problem. The user wants me to analyze the differences between the
original and improved versions of the gravitational wave detection pipeline, focusing on
depth 4. The current MCTS depth is 4 out of 10, which falls into the medium category. The
medium depth focuses on implementation techniques and parameterization.
First, I need to compare the two code versions. Let's look at each stage of the pipeline.
Starting with Stage 2: Adaptive Whitening. In the original code, there's a Kalman-like
smoothing with a fixed alpha of 0.8. The improved version introduces an adaptive alpha
based on the PSD's stationarity. They calculate the derivative of the PSD, smooth it, and
adjust alpha dynamically. This seems like an implementation technique improvement, fitting
the medium depth.
Next, the gain calculation. The original uses a fixed scaling factor of 3.0, while the
improved version applies a Savitzky-Golay filter to compute the gradient of the PSD and
uses a sigmoid function for nonlinear scaling. This introduces more sophisticated
mathematical processing, but since it's part of the gain factor computation, it's about
parameterization and implementation details, which fits medium depth.
In Stage 3: Coherent Metric. The original uses Tikhonov regularization with a fixed lambda.
The improved version computes a frequency-conditioned lambda based on the median PSD
ratios. This dynamic regularization is a parameterization improvement. Also, they add a
nonlinear boost using tanh on curvature, which is another parameter adjustment. These are
implementation techniques.
Stage 4: Thresholding. The original checks the second derivative for curvature. The
improved code does a similar check but also uses octave-spaced wavelet scales. Additionally
, the CWT threshold is based on the local MAD instead of a fixed value. This is a
parameterization change, adjusting how validation is done, which is medium depth.
Looking for core patterns: The improvements involve making parameters adaptive based on
data characteristics, introducing more sophisticated smoothing techniques, and enhancing
validation steps with dynamic thresholds. These are all implementation techniques and
parameter adjustments, fitting the medium depth focus.
Now, generating optimization principles. Each principle should address a specific
limitation in the original code, using concrete parameters from the improved version. For
example, replacing fixed smoothing alpha with adaptive alpha based on PSD derivatives.
Another is using Savitzky-Golay for gradient computation. Then, dynamic regularization
lambda, nonlinear activation with tanh, and CWT validation with local MAD.
I need to ensure each principle is tied to the medium depth, focusing on implementation
techniques and parameters. Avoid diving into deep mathematical formulations unless
necessary. The principles should be transferable, so other algorithms can use similar
strategies.
Checking the improved code's parameters: adaptive alpha ranges from 0.75 to 0.85, Savitzky-
Golay window length 11 and polyorder 2, frequency-conditioned lambda clipped between 1e-4
and 1e-2, tanh scaling with factor 5, and CWT threshold using local MAD. These concrete
values should be included in the principles.


--- Page 56 ---
56
LLM-guided automated algorithmic discovery
Finally, structure the answer into numbered items within a single brace, adhering to the
output format. Ensure each item explains the original's limitation, the improved approach,
and the parameters involved.
</thinking>
Reflection Content Generation. Following the thinking process, the
system generates structured reflection content that synthesizes insights and
proposes concrete improvements:
1. **Adaptive PSD Smoothing with Stationarity-Dependent Alpha**:
Original used fixed exponential smoothing ($\alpha$=0.8). Improved version implements
Kalman-like adaptation where smoothing coefficient (0.75-0.85 range) dynamically adjusts
based on local PSD stationarity measured through normalized differentials (3-point moving
average of dPSD/PSD). Enhances noise floor estimation stability during non-stationary
segments while preserving transient features.
2. **Spectral Gradient-Enhanced Gain Computation**:
Replaced simple exponential gain dampening with Savitzky-Golay derivative estimation (
window=11, polyorder=2) followed by sigmoid nonlinearity (1/(1+e^{-x/scale})). Original
used fixed scaling factor of 3.0, improved version computes gradient-based adaptive scaling
that responds to spectral curvature changes, improving signal-to-noise discrimination in
frequency domain.
3. **Frequency-Conditioned Regularization Lambda**:
Original Tikhonov regularization used fixed lambda parameter. Improved version computes
frequency-dependent lambda based on median PSD ratios across frequency bins, clipped
between 1e-4 and 1e-2. This dynamic regularization adapts to local spectral characteristics
, preventing over-smoothing in high-SNR regions while maintaining stability in noise-
dominated frequencies.
4. **Nonlinear Coherence Boost with Curvature Activation**:
Enhanced coherent metric computation by adding tanh-based nonlinear activation (scale
factor=5) applied to second-derivative curvature measures. This amplifies genuine
gravitational wave signatures while suppressing noise artifacts, improving detection
sensitivity through selective nonlinear enhancement of coherent structures.
5. **CWT Validation with Local MAD Thresholding**:
Original used fixed threshold for continuous wavelet transform validation. Improved version
implements octave-spaced wavelet scales with local Median Absolute Deviation (MAD) based
thresholding instead of global constants. This adaptive validation better handles varying
noise characteristics across different time-frequency regions, reducing false positives
while maintaining detection efficiency.
Iterative Refinement Protocol. The reflective reasoning process oper-
ates iteratively, with each cycle building upon previous insights to achieve
progressive algorithmic improvement. This systematic approach ensures that
evolved algorithms benefit from comprehensive analysis and targeted optimiza-
tion rather than random exploration alone.
S2
Data Partitioning Strategy
The MLGWSC-1 Dataset 4 partitioning strategy balances optimization
efficiency with statistical validity through careful temporal segmentation.
Figure B1 illustrates the systematic approach employed to divide the dataset
into training and test subsets while maintaining representative signal charac-
teristics across both partitions.
Training Set Definition and Statistical Characteristics. The train-
ing set encompasses the first 39 injection indices, corresponding to a cumulative


--- Page 57 ---
LLM-guided automated algorithmic discovery
57
0
20
40
60
80
100
120
Injection Index
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Duration (days)
Long duration injection
1.18 days
Individual Duration
Cumulative Duration
0
5
10
15
20
25
30
Cumulative Duration (days)
First 7 days
for optimization
(39 injections)
Fig. B1 MLGWSC-1 Dataset 4 Partitioning Strategy for Training and Test Set
Configuration. Individual injection durations (blue line, left axis) and cumulative duration
(green line, right axis) across injection indices 0-130. The red dashed vertical line at injection
index 39 delineates the training set boundary, with the first 7 days (red shaded region) used
for algorithm optimization containing 39 injections. The test set consists of a single long-
duration injection of 1.18 days (red annotation with arrow) occurring at injection index 119,
providing a challenging validation scenario for sustained detection capability. The horizontal
red dotted line indicates the cumulative duration at injection index 39, marking the exact
7-day threshold for training set partitioning. This temporal partitioning ensures efficient
optimization while providing rigorous out-of-sample validation on extended-duration signals.
duration of 7 days. This temporal boundary provides sufficient signal diver-
sity and noise conditions for algorithmic optimization while maintaining
computational efficiency during the iterative Evo-MCTS process.
During optimization, each evolved algorithm processes complete injec-
tion segments with durations ranging from 0.1 to 0.4 days. The statistical
characteristics of the training set (mean: 0.179 days, median: 0.143 days)
ensure comprehensive algorithmic development across the spectrum of injec-
tion durations present in the dataset. This distribution provides robust training
exposure, while the 7-day cumulative training duration serves multiple critical
purposes: (i) adequate statistical power for AUC calculation with minimum
false-alarm rate of 4 events per month, (ii) rapid algorithm evaluation (10-
20 minutes per assessment), and (iii) preservation of temporal continuity and
realistic noise characteristics.
Test Set Configuration and Validation Rigor. The test set comprises
a single 1.18-day continuous injection at index 119, containing 3,782 signal


--- Page 58 ---
58
LLM-guided automated algorithmic discovery
injections. This extended duration provides a particularly challenging vali-
dation scenario that significantly exceeds both the training set mean (0.179
days) and the overall dataset mean (0.229 days) by factors of 6.6x and 5.2x,
respectively. The test injection’s duration of 1.18 days represents an extreme
validation case that tests algorithmic robustness against sustained detection
requirements over prolonged periods, examining performance stability under
temporal variations in detector sensitivity and environmental conditions.
The temporal separation from training data ensures genuine out-of-sample
validation, while the extended duration creates a stringent assessment envi-
ronment. Compared to the overall dataset statistics (mean: 0.229 days,
median: 0.169 days), the test injection’s 1.18-day duration provides valida-
tion on challenging extended-duration scenarios that algorithms must handle
effectively.
S3
Additional Experimental Results
To ensure statistical robustness and assess the reliability of our Evo-MCTS
framework across different stochastic conditions, we conducted five inde-
pendent optimization runs with distinct random seeds. Figure C2 presents
comprehensive results from all runs, demonstrating both the consistency of
our approach and the natural variation inherent in stochastic optimization
processes.
(Note: To maintain consistency with the main text’s PT1-PT4 framework
while providing detailed combined individual run analysis, this supplemen-
tary analysis presents five phase transitions labeled as PT1, PT2.1, PT2.2,
PT3, and PT4, where PT2.1 and PT2.2 represent two independent algorith-
mic breakthroughs that are distinct from the PT2 phase described in the main
text.)
Best-Performing Run: Phase Transition Characterization. The
primary run (top panel) achieved the most comprehensive optimization tra-
jectory, discovering five distinct phase transitions that represent qualitative
algorithmic breakthroughs. PT1 occurs at evaluation 69 with fitness 1,635.00 at
depth 5, incorporating Continuous Wavelet Transform (CWT) techniques and
Multi-resolution Thresholding for enhanced time-frequency analysis. PT2.1
emerges at evaluation 151 with fitness 2,612.77 at depth 10, introducing Curva-
ture Boosting methods while integrating Tikhonov Regularization for improved
signal conditioning and noise suppression. PT2.2 manifests at evaluation 211
with fitness 3,439.75 at depth 3, representing a significant algorithmic advance-
ment through refined optimization strategies. PT3 develops at evaluation
333 with fitness 4,559.26 at depth 10, integrating Savitzky-Golay (S-G) filter
techniques for enhanced signal processing capabilities. Finally, PT4 achieves
maximum fitness of 5,241.37 at evaluation 486 and depth 4, representing the
culmination of all previously discovered techniques including CWT, Multi-
resolution Thresholding, Curvature Boosting, Tikhonov Regularization, and
S-G filtering in a comprehensive algorithmic framework.


--- Page 59 ---
LLM-guided automated algorithmic discovery
59
Fig. C2 Statistical Analysis of Evo-MCTS Optimization Performance Across
Multiple Runs. Top panel: Primary run showing complete optimization trajectory with
five phase transitions (PT1, PT2.1, PT2.2, PT3, PT4) achieving maximum fitness of 5,241.37
units. Each PT marker indicates fitness value, evaluation number, and tree depth: PT1
(1,635.00, eval 69, depth 5), PT2.1 (2,612.77, eval 151, depth 10), PT2.2 (3,439.75, eval 211,
depth 3), PT3 (4,559.26, eval 333, depth 10), PT4 (5,241.37, eval 486, depth 4). (Top right)
Shannon diversity index analysis showing systematic exploration patterns across fitness lev-
els, with aggregate peak diversity (≃3.8) in lower performance range (0-1,500 fitness). Lower
panels: Four additional independent runs demonstrating framework robustness and natural
optimization variability. Run 2 (middle-left) achieves moderate optimization (˜2,500 fitness),
Run 3 (middle-right) shows more substantial performance (˜3,500 fitness) with algorithmic
breakthroughs comparable to the PT2 phase described in the main text, Run 4 (bottom-left)
exhibits limited optimization progress (˜1,100 fitness), Run 5 (bottom-right) demonstrates
modest improvement from lower baseline (˜900 fitness). All runs show improvement over
baseline, validating framework reliability while illustrating diverse optimization pathways in
the complex algorithmic search space. Results confirm consistent early-phase improvements
across all runs with varying success in discovering advanced algorithmic combinations.


--- Page 60 ---
60
LLM-guided automated algorithmic discovery
The depth progression (5→10→3→10→4) reveals an interesting pattern
where major breakthroughs occur across various tree levels, suggesting that
the MCTS structure successfully balances exploration at different algorithmic
complexity levels. The fitness improvements at each phase transition represent
single-step gains from the immediate predecessor node rather than cumula-
tive improvements between phases: PT1 (+639.69), PT2.1 (+370.81), PT2.2
(+910.37), PT3 (+1,045.72), and PT4 (+456.85). These single-step improve-
ments demonstrate variable discovery magnitudes as individual algorithmic
innovations are identified, with the pattern showing that breakthrough dis-
coveries can occur with different intensities as the algorithmic space becomes
more thoroughly explored through the tree search process.
Table C1 provides a comprehensive performance comparison across all
benchmark models, the seed function baseline, and the five phase transition
levels achieved during optimization. The benchmark models demonstrate vary-
ing performance levels, with Sage achieving the highest AUC of 4359.2749,
followed by Virgo-AUTh at 4101.4810. Notably, our evolved algorithms at
PT Level 4 (5241.3678 AUC) significantly outperform all benchmark models
across all evaluation metrics, including false alarm rates at different thresholds
(FAR=1000, FAR=100, FAR=10, FAR=4.3), achieving relative improvements
of 20.2% over the top-performing Sage benchmark (4359.27 AUC) and 23.4%
enhancement in sensitive distance detection capability at node 486. The pro-
gressive improvement from the seed function baseline (926.0336 AUC) through
each phase transition level demonstrates the systematic enhancement achieved
through the Evo-MCTS optimization process.
Shannon Diversity Analysis Across Optimization Phases. The
Shannon diversity analysis (top right panel) reveals sophisticated exploration
patterns that correlate with optimization phases. The scatter plot demon-
strates that algorithmic diversity varies systematically with fitness levels,
with peak diversity (Shannon ∼2.5) occurring in the lower performance range
(fitness 0-1,500), followed by a gradual decrease as fitness improves. This pat-
tern indicates intensive exploration during early optimization phases, with
the framework progressively shifting toward exploitation as high-performing
algorithms are discovered. The diversity trend confirms that the Evo-MCTS
methodology effectively balances exploration and exploitation, with broader
algorithmic sampling during initial discovery phases and more focused refine-
ment during later breakthrough periods.
Analysis of Multi-Run Consistency and Variability. The four addi-
tional runs (lower panels) demonstrate varying degrees of optimization success,
reflecting the inherent stochastic nature of the discovery process while validat-
ing the framework’s general effectiveness. Run 2 (middle-left panel) exhibits
steady progression with fitness values reaching approximately 2,500 units,
characterized by a gradual upward trajectory punctuated by several modest
phase transitions. This pattern illustrates the framework’s ability to consis-
tently identify incremental algorithmic improvements even when breakthrough
discoveries remain elusive.


--- Page 61 ---
LLM-guided automated algorithmic discovery
61
Table C1 Performance Comparison Across Benchmark Models and Phase Transition
Levels
Model
AUC
Sensitive Distance (Mpc) at FAR
(units)
1000
100
10
4.3
Benchmark Models
Sage
4359.27
1996.1
1846.6
1688.8
1672.4
Virgo-AUTh
4101.48
1990.2
1818.7
1635.0
1609.5
PyCBC
4069.90
1832.3
1721.8
1609.3
1573.4
TPI FSU Jena
3744.99
1796.0
1581.6
1426.4
1382.4
CWB
3225.01
1451.6
1406.5
1351.8
1303.2
MFCNN
2890.33
1541.1
1269.0
997.2
900.3
CNN-Coinc
1997.02
1067.2
959.9
620.4
450.8
Phase Transition (PT) Levels
PT Level 4
5241.37
2323.9
2295.8
2080.9
2065.3
PT Level 3
4559.26
1932.0
1932.0
1932.0
1932.0
PT Level 2.2
3439.75
1537.2
1460.1
1407.9
1402.3
PT Level 2.1
2612.77
1107.2
1107.2
1107.2
1107.2
PT Level 1
1635.00
—
769.8
769.8
769.8
Seed function
926.03
786.9
368.2
238.3
158.3
Run 3 (middle-right panel) achieves more substantial performance with fit-
ness values approaching 3,500 units, displaying well-defined phase transitions
that correspond to significant algorithmic innovations. This run exhibits char-
acteristics similar to the PT2 phase described in the main text, demonstrating
how the framework can effectively navigate complex solution spaces to discover
meaningful algorithmic enhancements under favorable stochastic conditions.
Run 4 (bottom-left panel) presents a particularly instructive case despite
showing more limited optimization progress, with fitness values reaching
approximately 1,100 units. This run reveals valuable insights into the chal-
lenges of navigating rugged optimization landscapes, where persistent explo-
ration attempts encounter difficulty escaping local optima. Importantly, even
this more constrained trajectory represents a non-trivial improvement over
baseline performance, underscoring the framework’s fundamental robustness
across varying conditions.
Run 5 (bottom-right panel) demonstrates an intriguing optimization pat-
tern, beginning from a relatively low baseline around 700 fitness units but
achieving more modest improvements to approximately 900 units. Rather than
indicating failure, this trajectory illustrates the challenges encountered in cer-
tain regions of the optimization landscape, where despite starting from a lower


--- Page 62 ---
62
LLM-guided automated algorithmic discovery
performance level, the algorithm struggles to discover pathways to substan-
tial improvements, possibly due to being trapped in a difficult-to-escape local
optimum region.
Statistical Validation and Performance Reliability. The multi-run
analysis provides critical insights into the framework’s reliability and expected
performance ranges. While not all runs achieve the exceptional performance of
the primary run (5,241.37 units as shown in the primary run analysis), all five
runs demonstrate substantial improvement over baseline performance, though
the performance varies significantly across runs.
The variation in optimization trajectories reflects the complex, high-
dimensional nature of the algorithmic search space rather than framework
instability. Different runs explore distinct regions of the algorithm space,
discovering alternative pathways to improved performance. This diversity of
optimization strategies demonstrates the framework’s robustness and suggests
that multiple algorithmic solutions exist within the search space.
Optimization Pattern Analysis and Success Factors. Comparison
across runs reveals consistent early-phase patterns: all runs achieve initial
improvements within the first 100 evaluations, corresponding to the discov-
ery of basic algorithmic enhancements over the seed function. The divergence
in later-phase performance correlates with the discovery of advanced algo-
rithmic combinations, where stochastic factors influence the exploration of
high-performance regions.
The most successful runs (primary run reaching 5,241.37 and run 3 reaching
∼3,500 fitness) share common characteristics: sustained exploration diversity
throughout optimization, discovery of multiple phase transitions, and achieve-
ment of higher fitness levels. Less successful runs (such as run 5 with only
∼900 fitness) typically exhibit earlier convergence to local optima, suggesting
that maintaining exploration diversity is crucial for discovering breakthrough
algorithmic innovations.
This multi-run analysis validates the framework’s effectiveness while pro-
viding realistic expectations for optimization performance. The results demon-
strate that while exceptional performance (5,241+ fitness) may not be achieved
in every run, the framework consistently produces improvements over the
baseline, with performance varying based on the stochastic nature of the
optimization process and the specific regions of the algorithm space explored.
S4
Temporal Constraint Analysis and
Robustness Validation
The 0.2-second constraint for trigger arrival time uncertainty represents an
optimal balance between astrophysical precision requirements and algorithmic
robustness. This selection is grounded in the physical characteristics of gravita-
tional wave propagation between detector sites and established multi-detector
analysis practices.


--- Page 63 ---
LLM-guided automated algorithmic discovery
63
1000
2000
3000
4000
5000
Fitness (training)
500
1000
1500
2000
2500
3000
Fitness (test)
var = 0.02
r = 0.640
Trend line
y=x
1000
2000
3000
4000
5000
Fitness (training)
500
1000
1500
2000
2500
3000
3500
4000
Fitness (test)
var = 0.05
r = 0.792
Trend line
y=x
1000
2000
3000
4000
5000
Fitness (training)
1000
2000
3000
4000
5000
Fitness (test)
var = 0.1
r = 0.813
Trend line
y=x
1000
2000
3000
4000
5000
Fitness (training)
1000
2000
3000
4000
5000
6000
Fitness (test)
var = 0.2
r = 0.840
Trend line
y=x
1000
2000
3000
4000
5000
Fitness (training)
1000
2000
3000
4000
5000
6000
Fitness (test)
var = 0.5
r = 0.842
Trend line
y=x
1000
2000
3000
4000
5000
Fitness (training)
1000
2000
3000
4000
5000
6000
Fitness (test)
var = 1.0
r = 0.836
Trend line
y=x
Algorithm Performance: Training vs Test Fitness Across Trigger Time Uncertainty
Fig. D3 Temporal Constraint Impact on Algorithm Generalization Perfor-
mance. Analysis of training-test performance correlation as a function of trigger arrival
time uncertainty constraints across six different temporal precision levels. Six panels: Pear-
son correlation coefficients between training and test algorithm fitness scores across 877
algorithms for temporal constraint values ∆t ∈{0.02, 0.05, 0.1, 0.2, 0.5, 1.0} seconds. Each
panel displays scatter plots of training vs. test fitness with correlation coefficients and 95%
confidence intervals. The red diagonal line represents perfect training-test correlation (y =
x). The analysis demonstrates that the 0.2-second constraint (r = 0.840) provides optimal
balance between performance consistency and practical deployment requirements, with algo-
rithm performance closely following the ideal correlation line. Key finding: While tighter
constraints (0.02-0.1s) show high correlation coefficients, the 0.2-second constraint exhibits
superior generalization behavior with minimal deviation from the ideal y = x relationship,
indicating robust performance scaling between training and test conditions.
Physical Foundation. The temporal constraint must account for the
maximum light travel time between LIGO Hanford (H1) and Livingston (L1)
detectors. For the two-detector LHO-LLO network, signals must be detected in
both detectors within a time difference of 15 ms: 10 ms maximum travel time
between detectors and 5 ms padding to account for timing errors [53]. How-
ever, the 0.2-second window provides a significantly more conservative margin
to accommodate additional systematic uncertainties from detector calibration,
signal processing delays, timing measurement precision [6], and algorith-
mic robustness requirements while maintaining alignment with operational
gravitational wave detection pipelines [53, 54].
Experimental Analysis. We evaluated algorithmic performance under
six temporal constraints: ∆t ∈{0.02, 0.05, 0.1, 0.2, 0.5, 1.0} seconds, calculat-
ing training-test performance correlations across all 877 optimized algorithms.
Results and Optimal Selection. Figure D3 demonstrates systematic
relationships between temporal constraints and algorithm generalization. Cor-
relation coefficients increase from r = 0.640 (0.02s) to r = 0.840 (0.2s,
optimal), then plateau at r = 0.842 (0.5s) and r = 0.836 (1.0s). Critically, the


--- Page 64 ---
64
LLM-guided automated algorithmic discovery
0.2-second constraint exhibits performance characteristics most closely approx-
imating the ideal training-test parity line (y = x), with minimal scatter around
the diagonal.
Tighter constraints (0.02-0.1s) show increased scatter at higher fitness
values, suggesting potential overfitting. Looser constraints (0.5-1.0s) exhibit
broader scatter patterns indicating reduced discriminative power for distin-
guishing high-quality algorithms.
S5
Algorithmic Component Effectiveness
Analysis
LLM-Based Code Analysis Pipeline. To systematically extract technical
features from algorithm implementations, we developed an automated anal-
ysis pipeline using large language models (LLMs). Each code snippet was
processed through a structured prompt designed to identify algorithmic com-
ponents across three main stages: data conditioning, time-frequency analysis,
and trigger detection.
LLM Analysis Prompt:
Please analyze the following Python code snippet for gravitational wave detection and
extract technical features in JSON format.
The code typically has three main stages:
1. Data Conditioning: preprocessing, filtering, whitening, etc.
2. Time-Frequency Analysis: spectrograms, FFT, wavelets, etc.
3. Trigger Analysis: peak detection, thresholding, validation, etc.
For each stage present in the code, extract:
- Technical methods used
- Libraries and functions called
- Algorithm complexity features
- Key parameters
Code to analyze:
```python
{code_snippet}
```
Please return a JSON object with this structure:
{
"algorithm_id": "{algorithm_id}",
"stages": {
"data_conditioning": {
"present": true/false,
"techniques": ["technique1", "technique2"],
"libraries": ["lib1", "lib2"],
"functions": ["func1", "func2"],
"parameters": {"param1": "value1"},
"complexity": "low/medium/high"
},
"time_frequency_analysis": {...},
"trigger_analysis": {...}
},
"overall_complexity": "low/medium/high",
"total_lines": 0,
"unique_libraries": ["lib1", "lib2"],
"code_quality_score": 0.0
}
Only return the JSON object, no additional text.


--- Page 65 ---
LLM-guided automated algorithmic discovery
65
The analysis was performed using deepseek-r1-250120 model with tem-
perature=1.0 for balanced creativity and consistency. We processed 877 valid
code snippets (code snippet) using parallel processing with 30 workers to
ensure efficient analysis while maintaining API rate limits.
Data Preparation and Normalization. For each identified technique,
algorithms were classified into binary groups: those incorporating the technique
(“with”) versus those without (“without”). Performance metrics (fitness values
or AUC scores) were normalized to [0,1] range using min-max scaling across
all algorithms to enable fair comparison between different evaluation metrics.
Combined Performance Analysis. To increase statistical power, we
combined normalized AUC scores of both training and test into unified
performance datasets for each comparison group. This approach leverages
all available performance information while maintaining the comparative
structure necessary for statistical testing.
Adaptive Statistical Testing Protocol. Our testing framework adapts
to data characteristics through a decision tree approach:
1. Normality Assessment: Shapiro-Wilk test for samples n ≤5000
2. Test Selection:
• Both groups normal and n ≥30: Welch’s t-test with Cohen’s d effect size
• Otherwise: Mann-Whitney U test with rank-biserial correlation
3. Effect Size Interpretation:
• Cohen’s d: negligible (< 0.2), small (0.2 −0.5), medium (0.5 −0.8), large
(> 0.8)
• Rank-biserial: negligible (< 0.1), small (0.1 −0.3), medium (0.3 −0.5),
large (> 0.5)
Imbalance Detection and Mitigation. We identify problematic com-
parisons using two criteria: (i) sample ratio exceeding 3:1, or (ii) minimum
group size below 30. For such cases, we implement balanced resampling
analysis:
Resampling Protocol:
1. Undersample the larger group to match the smaller group size
2. Perform 1,000 independent resampling iterations with replacement
3. Calculate test statistics and p-values for each iteration
4. Assess robustness based on proportion of significant results
Robustness Criteria: A technique effect is considered robust if:
• > 80% of resampling iterations show statistical significance (p < 0.05)
• Median effect size maintains consistent direction and magnitude
• 95% confidence interval of effect sizes excludes zero
Technique Effectiveness Classification and Visualization. The com-
prehensive technique impact analysis is presented through violin plot dis-
tributions comparing performance between algorithms incorporating specific


--- Page 66 ---
66
LLM-guided automated algorithmic discovery
techniques (“with”) versus those without (“without”) across all identified
techniques (Figure E4). Based on our multi-criteria evaluation framework,
techniques are classified into three effectiveness tiers:
High-Effectiveness Techniques demonstrate clear distributional sepa-
ration with minimal overlap, statistical significance > 80% across resampling
iterations, and large effect sizes (r > 0.5). Notable examples include Curva-
ture Analysis and CWT Validation, which show the “with” group distributions
positioned substantially higher than “without” groups, indicating consistent
performance improvements.
Medium-Effectiveness Techniques exhibit moderate distributional
separation, statistical significance between 50-80%, and medium effect sizes
(0.3 < r < 0.5). These techniques provide measurable but less consistent
performance benefits.
Low-Effectiveness Techniques display substantial distributional over-
lap between “with” and “without” groups, statistical significance < 50%, and
small effect sizes (r < 0.1), indicating limited practical utility.
Table E2 Technique Abbreviations Used in Figure E4. Complete mapping of
abbreviated technique names to their full descriptions, organized by methodological
category.
Abbreviation
Full Name
Data Conditioning
Spline Interp
Spline Interpolation
Tikhonov Reg
Tikhonov Regularization
Kalman Smooth
Kalman-inspired Smoothing
Tukey
Tukey Windowing
S-G Filter
Savitzky-Golay Filtering
Adaptive Gain
Adaptive Gain Regularization
Median Smooth
Uniform/Median Smoothing
Gauss Smooth
Gaussian Smoothing
Median Baseline
Median-based Baseline Correction
SNR Reg
SNR-adaptive Regularization
Gauss Conv
Gaussian Convolution
Time-Frequency Analysis
Phase Coherence
Phase Coherence Analysis
CWT
Continuous Wavelet Transform
RMS Coherence
RMS Coherence Metric
Dual Align
Dual-channel Alignment
Spectral Entropy
Spectral Entropy
Freq Reg
Frequency-dependent Regularization
Log Compress
Logarithmic Compression
CWT Valid
CWT Validation
Trigger Detection
Curve Boost
Curvature Boosting
Curvature
Curvature Analysis
Sigmoid
Sigmoid Enhancement
MAD Threshold
MAD-based Robust Thresholding
MAD
Median Absolute Deviation
Distributional Analysis Methodology.


--- Page 67 ---
LLM-guided automated algorithmic discovery
67
Fig. E4 Comprehensive Technique Effectiveness Analysis via Violin Plot Dis-
tributions. Performance distributions comparing algorithms with and without specific
techniques across three methodological categories: data conditioning (blue), time-frequency
analysis (orange), and trigger detection (green). Each violin plot pair reveals technique
effectiveness through distributional characteristics: wider sections indicate higher probabil-
ity density regions, clear vertical separation between “with” and “without” groups indicates
strong technique effects, while substantial overlap suggests limited effectiveness. Statisti-
cal robustness metrics (significance percentages from resampling analysis) and effect sizes
(rank-biserial correlations) quantify technique reliability. High-effectiveness techniques (e.g.,
Curvature Analysis, CWT Validation) demonstrate clear distributional separation and large
effect sizes, while low-effectiveness techniques show substantial overlap and negligible effect
sizes. Technique abbreviations are defined in Table E2.
• Violin plots constructed using Gaussian kernel density estimation with
adaptive bandwidth selection
• Performance metrics normalized to [0,1] scale enabling cross-technique
comparison
• Color-coded categorical organization: data conditioning (blue), time-
frequency analysis (orange), trigger detection (green)
• Statistical annotations include resampling-based significance percentages
and rank-biserial effect sizes


--- Page 68 ---
68
LLM-guided automated algorithmic discovery
• Median performance indicators highlight central tendency differences
between technique groups
• Distributional
separation
quantified
through
overlap
coefficients
and
Kolmogorov-Smirnov distances
• Technique abbreviations facilitate visual clarity while maintaining compre-
hensive coverage (Table E2)
This multi-dimensional effectiveness assessment framework enables system-
atic identification of high-impact techniques while distinguishing them from
those with marginal or inconsistent benefits, providing clear guidance for
algorithmic development priorities.
S6
Complete MCTS Tree Structure and
Algorithmic Evolution
Figure F5 presents the complete MCTS search tree evolution with node-by-
node fitness values and technique compositions. Each node displays its fitness
score (marked in red) alongside the specific algorithmic techniques discovered
at that search depth. The five core technique categories [1-5] correspond to:
1 Multi-resolution Thresholding
2 Continuous Wavelet Transform (CWT) using Ricker Wavelet
3 Tikhonov Regularization
4 Curvature Boosting
5 Savitzky-Golay Filter
These techniques demonstrate the systematic evolution of algorithmic com-
plexity through the MCTS exploration process, with detailed implementation
examples provided in Section S1.9.
References
[1] Zheng, Y., Koh, H.Y., Ju, J., Nguyen, A.T.N., May, L.T., Webb, G.I., Pan,
S.: Large language models for scientific discovery in molecular property
prediction. Nature Machine Intelligence (2025) 2310.07984 [cs]. https://
doi.org/10.1038/s42256-025-00994-z
[2] Wang, H., Fu, T., Du, Y., Gao, W., Huang, K., Liu, Z., Chandak, P.,
Liu, S., Van Katwyk, P., Deac, A., Anandkumar, A., Bergen, K., Gomes,
C.P., Ho, S., Kohli, P., Lasenby, J., Leskovec, J., Liu, T.-Y., Manrai, A.,
Marks, D., Ramsundar, B., Song, L., Sun, J., Tang, J., Veliˇckovi´c, P.,
Welling, M., Zhang, L., Coley, C.W., Bengio, Y., Zitnik, M.: Scientific
discovery in the age of artificial intelligence. Nature 620(7972), 47–60
(2023). https://doi.org/10.1038/s41586-023-06221-2
[3] Karniadakis, G.E., Kevrekidis, I.G., Lu, L., Perdikaris, P., Wang, S., Yang,


--- Page 69 ---
LLM-guided automated algorithmic discovery
69
Fig. F5 Complete MCTS search tree with node fitness values and technique
compositions. Each node shows its fitness score (red annotations) and constituent algo-
rithmic techniques organized by category [1-5]. Node size reflects fitness magnitude. The
tree demonstrates systematic technique evolution and cross-branch knowledge transfer, with
optimal performance achieved through multi-technique integration at terminal nodes.
L.: Physics-informed machine learning. Nature Reviews Physics 3(6), 422–
440 (2021). https://doi.org/10.1038/s42254-021-00314-5
[4] Baker, N., Alexander, F., Bremer, T., Hagberg, A., Kevrekidis, Y., Najm,
H., Parashar, M., Patra, A., Sethian, J., Wild, S., et al.: Workshop report
on basic research needs for scientific machine learning: Core technologies
for artificial intelligence. Technical report, USDOE Office of Science (SC),
Washington, D.C. (United States) (February 2019). https://doi.org/10.
2172/1478744. https://www.osti.gov/biblio/1478744
[5] Abbott, B.P., et al.: Observation of gravitational waves from a binary
black hole merger. Phys. Rev. Lett. 116, 061102 (2016). https://doi.org/
10.1103/PhysRevLett.116.061102
[6] Abbott, B.P., et al.: Gwtc-1: A gravitational-wave transient catalog of
compact binary mergers observed by ligo and virgo during the first and
second observing runs. Phys. Rev. X 9, 031040 (2019). https://doi.org/
10.1103/PhysRevX.9.031040
[7] Abbott, B.P., et al.: A guide to ligo-virgo detector noise and extraction
of transient gravitational-wave signals. Classical and Quantum Gravity
37(5), 055002 (2020). https://doi.org/10.1088/1361-6382/ab685e
[8] Owen, B.J.: Search templates for gravitational waves from inspiraling


--- Page 70 ---
70
LLM-guided automated algorithmic discovery
binaries: Choice of template spacing. Phys. Rev. D 53, 6749–6761 (1996).
https://doi.org/10.1103/PhysRevD.53.6749
[9] Cutler, C., Flanagan, E.E.: Gravitational waves from merging compact
binaries: How accurately can one extract the binary’s parameters from
the inspiral waveform? Phys. Rev. D 49, 2658–2697 (1994). https://doi.
org/10.1103/PhysRevD.49.2658
[10] Klimenko, S., Vedovato, G., Drago, M., Salemi, F., Tiwari, V., Prodi,
G.A., Lazzaro, C., Ackley, K., Tiwari, S., Da Silva, C.F., Mitselmakher,
G.: Method for detection and reconstruction of gravitational wave tran-
sients with networks of advanced detectors. Phys. Rev. D 93, 042004
(2016). https://doi.org/10.1103/PhysRevD.93.042004
[11] George, D., Huerta, E.A.: Deep neural networks to enable real-time
multimessenger astrophysics. Phys. Rev. D 97, 044039 (2018). https:
//doi.org/10.1103/PhysRevD.97.044039
[12] Gabbard, H., Williams, M., Hayes, F., Messenger, C.: Matching matched
filtering with deep networks for gravitational-wave astronomy. Phys.
Rev. Lett. 120, 141103 (2018). https://doi.org/10.1103/PhysRevLett.
120.141103
[13] Huerta, E.A., Khan, A., Huang, X., Tian, M., Levental, M., Chard, R.,
Wei, W., Heflin, M., Katz, D.S., Kindratenko, V., Mu, D., Blaiszik, B.,
Foster, I.: Accelerated, scalable and reproducible AI-driven gravitational
wave detection. Nature Astronomy 5(10), 1062–1068 (2021) 2012.08545v1
[gr-qc]. https://doi.org/10.1038/s41550-021-01405-0
[14] Nagarajan, N., Messenger, C.: Identifying and Mitigating Machine Learn-
ing Biases for the Gravitational-wave Detection Problem (2025). https:
//arxiv.org/abs/2501.13846
[15] Elsken, T., Metzen, J.H., Hutter, F.: Neural architecture search: A survey.
Journal of Machine Learning Research 20(55), 1–21 (2019)
[16] Eiben, A.E., Smith, J.: From evolutionary computation to the evolution
of things. Nature 521(7553), 476–482 (2015). https://doi.org/10.1038/
nature14544
[17] Kahneman, D.: Thinking, Fast and Slow. Macmillan, New York (2011)
[18] Wolpert, D.H., Macready, W.G.: No free lunch theorems for optimization.
IEEE transactions on evolutionary computation 1(1), 67–82 (2002)
[19] Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H.P., Kaplan,
J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri,


--- Page 71 ---
LLM-guided automated algorithmic discovery
71
R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan,
B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M.,
Winter, C., Tillet, P., Such, F.P., Cummings, D., Plappert, M., Chantzis,
F., Barnes, E., Herbert-Voss, A., Guss, W.H., Nichol, A., Paino, A., Tezak,
N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C.,
Carr, A.N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A.,
Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew,
B., Amodei, D., McCandlish, S., Sutskever, I., Zaremba, W.: Evaluating
Large Language Models Trained on Code (2021). https://arxiv.org/abs/
2107.03374
[20] Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H.,
Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y.,
Neyshabur, B., Gur-Ari, G., Misra, V.: Solving quantitative reasoning
problems with language models 35, 3843–3857 (2022). https://doi.org/
10.48550/arXiv.2206.14858
[21] Browne, C.B., Powley, E., Whitehouse, D., Lucas, S.M., Cowling, P.I.,
Rohlfshagen, P., Tavener, S., Perez, D., Samothrakis, S., Colton, S.: A
Survey of Monte Carlo Tree Search Methods. IEEE Transactions on
Computational Intelligence and AI in Games 4(1), 1–43 (2012). https:
//doi.org/10.1109/TCIAIG.2012.2186810
[22] Wang, L., Zhao, Y., Jinnai, Y., Tian, Y., Fonseca, R.: Neural architecture
search using deep neural networks and monte carlo tree search. Proceed-
ings of the AAAI Conference on Artificial Intelligence 34(06), 9983–9991
(2020). https://doi.org/10.1609/aaai.v34i06.6554
[23] Li, Y., Du, D., Song, L., Li, C., Wang, W., Yang, T., Mi, H.: Hunyuan-
Prover: A Scalable Data Synthesis Framework and Guided Tree Search for
Automated Theorem Proving (2025). https://arxiv.org/abs/2412.20735
[24] Koza, J.R.: Genetic programming as a means for programming computers
by natural selection. Statistics and Computing 4(2), 87–112 (1994). https:
//doi.org/10.1007/BF00175355
[25] Romera-Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar,
M.P., Dupont, E., Ruiz, F.J.R., Ellenberg, J.S., Wang, P., Fawzi, O.,
Kohli, P., Fawzi, A.: Mathematical discoveries from program search with
large language models. Nature 625(7995), 468–475 (2024). https://doi.
org/10.1038/s41586-023-06924-6
[26] Liu, F., Tong, X., Yuan, M., Lin, X., Luo, F., Wang, Z., Lu, Z., Zhang, Q.:
Evolution of Heuristics: Towards Efficient Automatic Algorithm Design
Using Large Language Model (2024). https://arxiv.org/abs/2401.02051
[27] Liu, F., Tong, X., Yuan, M., Zhang, Q.: Algorithm Evolution Using Large


--- Page 72 ---
72
LLM-guided automated algorithmic discovery
Language Model (2023). https://arxiv.org/abs/2311.15249
[28] Ye, H., Wang, J., Cao, Z., Berto, F., Hua, C., Kim, H., Park, J., Song,
G.: ReEvo: Large Language Models as Hyper-Heuristics with Reflective
Evolution (2024). https://arxiv.org/abs/2402.01145
[29] Zheng, Z., Xie, Z., Wang, Z., Hooi, B.: Monte Carlo Tree Search for Com-
prehensive Exploration in LLM-Based Automatic Heuristic Design (2025).
https://arxiv.org/abs/2501.08603
[30] Sch¨afer, M.B., Zelenka, O.c.v., Nitz, A.H., Wang, H., Wu, S., Guo, Z.-
K., Cao, Z., Ren, Z., Nousi, P., Stergioulas, N., Iosif, P., Koloniari, A.E.,
Tefas, A., Passalis, N., Salemi, F., Vedovato, G., Klimenko, S., Mishra,
T., Br¨ugmann, B., Cuoco, E., Huerta, E.A., Messenger, C., Ohme, F.:
First machine learning gravitational-wave search mock data challenge.
Phys. Rev. D 107, 023021 (2023). https://doi.org/10.1103/PhysRevD.
107.023021
[31] Nousi, P., Koloniari, A.E., Passalis, N., Iosif, P., Stergioulas, N., Tefas,
A.: Deep residual networks for gravitational wave detection. Phys. Rev.
D 108, 024022 (2023). https://doi.org/10.1103/PhysRevD.108.024022
[32] Nitz, A.H., Kumar, S., Wang, Y.-F., Kastha, S., Wu, S., Sch¨afer, M.,
Dhurkunde, R., Capano, C.D.: 4-ogc: Catalog of gravitational waves from
compact binary mergers. The Astrophysical Journal 946(2), 59 (2023).
https://doi.org/10.3847/1538-4357/aca591
[33] Zelenka, O.c.v., Br¨ugmann, B., Ohme, F.: Convolutional neural networks
for signal detection in real ligo data. Phys. Rev. D 110, 024024 (2024).
https://doi.org/10.1103/PhysRevD.110.024024
[34] Drago, M., Klimenko, S., Lazzaro, C., Milotti, E., Mitselmakher, G., Nec-
ula, V., O’Brian, B., Prodi, G.A., Salemi, F., Szczepanczyk, M., Tiwari,
S., Tiwari, V., V, G., Vedovato, G., Yakushin, I.: coherent waveburst, a
pipeline for unmodeled gravitational-wave data analysis. SoftwareX 14,
100678 (2021). https://doi.org/10.1016/j.softx.2021.100678
[35] Wang, H., Wu, S., Cao, Z., Liu, X., Zhu, J.-Y.: Gravitational-wave sig-
nal recognition of ligo data by deep learning. Phys. Rev. D 101, 104003
(2020). https://doi.org/10.1103/PhysRevD.101.104003
[36] Sch¨afer, M.B., Nitz, A.H.: From one to many: A deep learning coincident
gravitational-wave search. Phys. Rev. D 105, 043003 (2022). https://doi.
org/10.1103/PhysRevD.105.043003
[37] Sch¨afer, M.B., Zelenka, O.c.v., Nitz, A.H., Ohme, F., Br¨ugmann, B.:
Training strategies for deep learning gravitational-wave searches. Phys.


--- Page 73 ---
LLM-guided automated algorithmic discovery
73
Rev. D 105, 043002 (2022). https://doi.org/10.1103/PhysRevD.105.
043002
[38] Dal Canton, T., Nitz, A.H., Gadre, B., Cabourn Davies, G.S., Villa-
Ortega, V., Dent, T., Harry, I., Xiao, L.: Real-time search for compact
binary mergers in advanced ligo and virgo’s third observing run using
pycbc live. The Astrophysical Journal 923(2), 254 (2021). https://doi.
org/10.3847/1538-4357/ac2f9a
[39] Finn, L.S.: Detection, measurement, and gravitational radiation. Phys.
Rev. D 46, 5236–5249 (1992). https://doi.org/10.1103/PhysRevD.46.
5236
[40] Klimenko, S., Vedovato, G., Drago, M., Salemi, F., Tiwari, V., Prodi,
G.A., Lazzaro, C., Ackley, K., Tiwari, S., Da Silva, C.F., Mitselmakher,
G.: Method for detection and reconstruction of gravitational wave tran-
sients with networks of advanced detectors. Phys. Rev. D 93, 042004
(2016). https://doi.org/10.1103/PhysRevD.93.042004
[41] Zhang, R., Liu, F., Lin, X., Wang, Z., Lu, Z., Zhang, Q.: Understanding
the Importance of Evolutionary Search in Automated Heuristic Design
with Large Language Models (2024). https://arxiv.org/abs/2407.10873
[42] LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521(7553),
436–444 (2015). https://doi.org/10.1038/nature14539
[43] Rudin, C.: Stop explaining black box machine learning models for
high stakes decisions and use interpretable models instead. Nature
Machine Intelligence 1(5), 206–215 (2019). https://doi.org/10.1038/
s42256-019-0048-x
[44] Molnar, C.: Interpretable Machine Learning, 2nd edn. Leanpub, Munich,
Germany (2020). https://christophm.github.io/interpretable-ml-book/
[45] Troja, E., Piro, L., van Eerten, H., Wollaeger, R.T., Im, M., Fox, O.D.,
Butler, N.R., Cenko, S.B., Sakamoto, T., Fryer, C.L., Ricci, R., Lien, A.,
Ryan, R.E., Korobkin, O., Lee, S.-K., Burgess, J.M., Lee, W.H., Watson,
A.M., Choi, C., Covino, S., D’Avanzo, P., Fontes, C.J., Gonz´alez, J.B.,
Khandrika, H.G., Kim, J., Kim, S.-L., Lee, C.-U., Lee, H.M., Kutyrev,
A., Lim, G., S´anchez-Ram´ırez, R., Veilleux, S., Wieringa, M.H., Yoon,
Y.: The X-ray counterpart to the gravitational-wave event GW170817.
Nature 551(7678), 71–74 (2017). https://doi.org/10.1038/nature24290
[46] Dat, P.V.T., Doan, L., Binh, H.T.T.: HSEvo: Elevating Automatic Heuris-
tic Design with Diversity-Driven Harmony Search and Genetic Algorithm
Using LLMs (2024). https://arxiv.org/abs/2412.14995


--- Page 74 ---
74
LLM-guided automated algorithmic discovery
[47] OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Ale-
man, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila,
R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavar-
ian, M., Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner,
C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.-L., Brockman, G.,
Brooks, T., Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A.,
Carey, B., Carlson, C., Carmichael, R., Chan, B., Chang, C., Chantzis,
F., Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess, B., Cho, C.,
Chu, C., Chung, H.W., Cummings, D., Currier, J., Dai, Y., Decareaux,
C., Degry, T., Deutsch, N., Deville, D., Dhar, A., Dohan, D., Dowling,
S., Dunning, S., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus,
L., Felix, N., Fishman, S.P., Forte, J., Fulford, I., Gao, L., Georges, E.,
Gibson, C., Goel, V., Gogineni, T., Goh, G., Gontijo-Lopes, R., Gordon,
J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S.S., Guo, Y., Hal-
lacy, C., Han, J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse, C.,
Hickey, A., Hickey, W., Hoeschele, P., Houghton, B., Hsu, K., Hu, S., Hu,
X., Huizinga, J., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H.,
Jin, D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T.,  Lukasz Kaiser, Kamali,
A., Kanitscheider, I., Keskar, N.S., Khan, T., Kilpatrick, L., Kim, J.W.,
Kim, C., Kim, Y., Kirchner, J.H., Kiros, J., Knight, M., Kokotajlo, D.,
 Lukasz Kondraciuk, Kondrich, A., Konstantinidis, A., Kosic, K., Krueger,
G., Kuo, V., Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D.,
Li, C.M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe, R.,
Lue, P., Makanju, A., Malfacini, K., Manning, S., Markov, T., Markovski,
Y., Martin, B., Mayer, K., Mayne, A., McGrew, B., McKinney, S.M.,
McLeavey, C., McMillan, P., McNeil, J., Medina, D., Mehta, A., Menick,
J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V., Morikawa, E.,
Mossing, D., Mu, T., Murati, M., Murk, O., M´ely, D., Nair, A., Nakano,
R., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Ouyang, L., O’Keefe,
C., Pachocki, J., Paino, A., Palermo, J., Pantuliano, A., Parascandolo,
G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng, A., Perel-
man, A., de Avila Belbute Peres, F., Petrov, M., de Oliveira Pinto, H.P.,
Michael, Pokorny, Pokrass, M., Pong, V.H., Powell, T., Power, A., Power,
B., Proehl, E., Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond,
C., Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ryder, N.,
Saltarelli, M., Sanders, T., Santurkar, S., Sastry, G., Schmidt, H., Schnurr,
D., Schulman, J., Selsam, D., Sheppard, K., Sherbakov, T., Shieh, J.,
Shoker, S., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama,
K., Sohl, I., Sokolowsky, B., Song, Y., Staudacher, N., Such, F.P., Sum-
mers, N., Sutskever, I., Tang, J., Tezak, N., Thompson, M.B., Tillet, P.,
Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe,
J.F.C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang,
J.J., Wang, A., Wang, B., Ward, J., Wei, J., Weinmann, C., Welihinda,
A., Welinder, P., Weng, J., Weng, L., Wiethoff, M., Willner, D., Winter,
C., Wolrich, S., Wong, H., Workman, L., Wu, S., Wu, J., Wu, M., Xiao,


--- Page 75 ---
LLM-guided automated algorithmic discovery
75
K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R., Zhang, C.,
Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk, W., Zoph, B.: GPT-4
Technical Report (2024). https://arxiv.org/abs/2303.08774
[48] OpenAI:
Learning
to
Reason
with
LLMs.
https://openai.com/index/learning-to-reason-with-llms/. Accessed: 2024
(2024)
[49] Anthropic:
Claude
3.7
Sonnet
and
Claude
Code.
https://www.anthropic.com/news/claude-3-7-sonnet.
Accessed:
2025
(2025)
[50] DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R.,
Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z.F.,
Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B.,
Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen,
D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G.,
Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu,
H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J.,
Cai, J.L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan,
K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang,
L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M.,
Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge,
R., Zhang, R., Pan, R., Wang, R., Chen, R.J., Jin, R.L., Chen, R., Lu, S.,
Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S.S.,
Zhou, S., Wu, S., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W.,
Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W.L.,
An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie,
X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X.Q., Jin, X., Shen, X.,
Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li,
Y.K., Wang, Y.Q., Wei, Y.X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun,
Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y.,
Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y.,
Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y.X.,
Xu, Y., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y.,
Yan, Y., Ren, Z.Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z.,
Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z.,
Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., Zhang, Z.: DeepSeek-R1:
Incentivizing Reasoning Capability in LLMs via Reinforcement Learning
(2025). https://arxiv.org/abs/2501.12948
[51] Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E.,
Kamar, E., Lee, P., Lee, Y.T., Li, Y., Lundberg, S., Nori, H., Palangi, H.,
Ribeiro, M.T., Zhang, Y.: Sparks of Artificial General Intelligence: Early
experiments with GPT-4 (2023). https://arxiv.org/abs/2303.12712


--- Page 76 ---
76
LLM-guided automated algorithmic discovery
[52] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin,
P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J.,
Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano,
P.F., Leike, J., Lowe, R.: Training language models to follow instructions
with human feedback 35, 27730–27744 (2022). https://doi.org/10.48550/
arXiv.2203.02155
[53] Usman, S.A., Nitz, A.H., Harry, I.W., Biwer, C.M., Brown, D.A., Cabero,
M., Capano, C.D., Canton, T.D., Dent, T., Fairhurst, S., Kehl, M.S., Kep-
pel, D., Krishnan, B., Lenon, A., Lundgren, A., Nielsen, A.B., Pekowsky,
L.P., Pfeiffer, H.P., Saulson, P.R., West, M., Willis, J.L.: The pycbc
search for gravitational waves from compact binary coalescence. Classical
and Quantum Gravity 33(21), 215004 (2016). https://doi.org/10.1088/
0264-9381/33/21/215004
[54] Messick, C., Blackburn, K., Brady, P., Brockill, P., Cannon, K., Car-
iou, R., Caudill, S., Chamberlin, S.J., Creighton, J.D.E., Everett, R.,
Hanna, C., Keppel, D., Lang, R.N., Li, T.G.F., Meacher, D., Nielsen,
A., Pankow, C., Privitera, S., Qi, H., Sachdev, S., Sadeghian, L., Singer,
L., Thomas, E.G., Wade, L., Wade, M., Weinstein, A., Wiesner, K.:
Analysis framework for the prompt discovery of compact binary merg-
ers in gravitational-wave data. Phys. Rev. D 95, 042001 (2017). https:
//doi.org/10.1103/PhysRevD.95.042001
