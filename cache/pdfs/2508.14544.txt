--- Page 1 ---
Adaptively Robust LLM Inference Optimization
under Prediction Uncertainty
Zixi Chen
Department of Mathematics, Peking University, chenzixi22@stu.pku.edu.cn
Yinyu Ye
Department of Management Science and Engineering, Stanford University & Department of Industrial Engineering and
Decision Analytics, HKUST, yyye@stanford.edu
Zijie Zhou
Department of Industrial Engineering and Decision Analytics, HKUST, jerryzhou@ust.hk
We study the problem of optimizing Large Language Model (LLM) inference scheduling to minimize total
latency. LLM inference is an online and multi-task service process and also heavily energy consuming by
which a pre-trained LLM processes input requests and generates output tokens sequentially. Therefore, it is
vital to improve its scheduling efficiency and reduce the power consumption while a great amount of prompt
requests are arriving. A key challenge in LLM inference scheduling is that while the prompt length is known
upon arrival, the output length, which critically impacts memory usage and processing time, is unknown. To
address this uncertainty, we propose algorithms that leverage machine learning to predict output lengths,
assuming the prediction provides an interval classification (min-max range) for each request.
We first design a conservative algorithm, Amax, which schedules requests based on the upper bound
of predicted output lengths to prevent memory overflow. However, this approach is overly conservative:
as prediction accuracy decreases, performance degrades significantly due to potential overestimation. To
overcome this limitation, we propose Amin, an adaptive algorithm that initially treats the predicted lower
bound as the output length and dynamically refines this estimate during inferencing. We prove that Amin
achieves a log-scale competitive ratio. Through numerical simulations, we demonstrate that Amin often
performs nearly as well as the hindsight scheduler, highlighting both its efficiency and robustness in practical
scenarios. Moreover, Amin relies solely on the lower bound of the prediction interval—an advantageous design
choice since upper bounds on output length are typically more challenging to predict accurately.
Key words : Robust and Online Scheduling, LLM Inference, Operations Management in AI with Prediction
1
arXiv:2508.14544v2  [cs.LG]  1 Sep 2025


--- Page 2 ---
2
1. Introduction
Recent advances in large language models (LLMs) (Brown et al. 2020, Chowdhery et al. 2023,
OpenAI 2023, Kaplan et al. 2020, Wei et al. 2022) have redefined the boundaries of artificial
intelligence, showcasing exceptional proficiency in generating human-like text across a wide spec-
trum of languages and contexts. These advanced neural architectures, built upon vast and diverse
textual datasets, have become integral to a wide range of practical applications. Their influence
extends to AI-driven conversational interfaces (Anthropic 2023, Character 2021, OpenAI 2019,
2023), next-generation search technologies (Microsoft 2023, Google 2023, Perplexity 2022), intel-
ligent programming assistants (Amazon 2023, GitHub 2021), and operations service management
(Huang et al. 2025, Cascella et al. 2023, Sallam 2023), highlighting their adaptability across both
specialized and everyday use cases.
Large Language Model (LLM) inference—the process by which a pre-trained LLM processes
input requests and generates output tokens sequentially—is not just a computational task but a
critical operational challenge with real-world cost and energy implications. Optimizing this process
can significantly reduce infrastructure expenses and power consumption, especially given the scale
of modern LLM deployments. This problem sits at the intersection of AI systems and operations
research (OR), where classical scheduling techniques, adapted to LLM-specific constraints, can
unlock substantial efficiency gains.
For each request, the inference consists of two key phases:
1. Input (Prefill) Phase: The LLM reads the input prompt, computes the query, key, and value
representations for each token in the prompt, and produces the first output token.
2. Output (Decode) Phase:
The LLM generates subsequent tokens autoregressively—using all
previously generated tokens as context—until completion.
For example, given the input prompt “KMeans is used”, the LLM might first generate the token
“for”, then use “KMeans is used for” to produce “clustering”, and so on. Crucially, while the
input length is known upfront, the output length is inherently unknown until generation finishes.
Recent work has explored methods to predict output lengths in advance, often employing auxiliary
machine learning models or even LLMs themselves (Zheng et al. (2023b), Qiu et al. (2024), Fu
et al. (2024), Chen et al. (2024), Shahout et al. (2024)). Most of these approaches typically classify
requests into predefined output-length intervals based on their predictions.
Given the high volume of requests we typically handle, simultaneous processing is often unfeasi-
ble. To optimize efficiency, it is essential to design scheduling algorithms using operations research
techniques to prioritize request sequences. Accurate output length prediction is critical for efficient


--- Page 3 ---
3
LLM inference scheduling, especially when handling large volumes of requests. Such predictions
enable effective scheduling policies by providing two key insights: (i) Execution Time Esti-
mation:
Since tokens are generated sequentially, the output length directly correlates with the
request’s completion time. (ii) Memory Usage Forecasting: The key-value (KV) cache mem-
ory grows linearly during decoding, as each new token’s KV embeddings are stored and retained
for all subsequent steps. Thus, predicting output length allows systems to anticipate both the
computational duration and memory footprint of each request.
The design of efficient scheduling policies for LLM inference—which determines the processing
order of a great number of input prompts under output length predictions—has been explored under
idealized assumptions. For instance, when predictions are assumed to be perfectly accurate, prior
work Jaillet et al. (2025), Shahout et al. (2024) proposes shortest-first algorithms that prioritize
requests with smaller predicted output lengths. This approach offers two advantages: (i) shorter jobs
finish faster, reducing the total waiting time; (ii) the system can process more requests concurrently
in larger batches at early stage since shorter requests consume less KV cache memory. However,
real-world predictions are inherently imperfect. Inaccurate predictions introduce two key challenges:
• Suboptimal Scheduling: Misclassify a long output as short disrupts the processing order,
delaying other requests.
• Memory Overflows and Underflows:
Underestimated memory demands can exceed
GPU KV cache limits, forcing request cancellations and halting execution. Overestimated memory
demands can reduce the concurrency, increasing the total waiting time.
This paper rigorously addresses these challenges by developing robust scheduling algorithms
grounded in mathematical theory, ensuring efficiency even under prediction uncertainty.
1.1. Main Contribution
We introduce the main contribution and the outline in this section.
Naive Conservative Algorithm Amax and its Competitive Analysis.
In Section 3, we
propose a naive benchmark algorithm, Amax, that conservatively treats the predicted upper bound
of each job’s output length as its true length. This approach guarantees no memory overflows by
over-provisioning KV cache resources for all requests. However, by overestimating the memory
usage for each request, the algorithm hurts the concurrency if the prediction error is large. Our
analysis reveals two key results: (i) We construct an adversarial arrival sequence proving that
Amax achieves a competitive ratio of at least α−1(1+α−1/2)
2
, where α is the ratio of the prediction
interval’s lower bound to its upper bound. (ii) Moreover, we rigorously show the upper bound


--- Page 4 ---
4
of the competitive ratio of Amax is
α−1(1+α−1)
2
. The proof leverages a novel memory-preserving
combinatorial technique (Section EC.1), which may inspire further research in LLM inference
scheduling under prediction uncertainty.
Robust Advanced Algorithm Amin and its Competitive Analysis. Motivated by the obser-
vation that the competitive ratio of Amax becomes unbounded as α shrinks (i.e., when predictions
lack confidence), we design Amin, a robust algorithm that leverages only the lower prediction bound.
Initially, Amin underestimates each request’s output length as its lower bound, dynamically refining
this estimate during execution. In Section 4, we prove that Amin achieves asymptotic optimal-
ity with a competitive ratio of O(log(α−1)), demonstrating significantly stronger robustness than
Amax. The analysis in Subsection 4.1 is notably more challenging than that of Amax: we derive
the competitive ratio’s closed form as a Rayleigh quotient and then develop a novel estimation
technique to bound this quotient logarithmically. In Section 5, we also study the performance of
Amin over some specific output distributions. We show that under two-point distribution, geometric
distribution, linear weighted geometric distribution, the competitive ratios of Amin is uniformly
bounded by 1.5, 1.7, and 1.56 respectively.
Numerical Experiments. In Section 6, we evaluate our algorithms through three sets of experi-
ments on a real-world dataset Zheng et al. (2023a). First, we establish a baseline where all requests
share an identical prediction interval, representing perfect prediction uniformity. Second, we imple-
ment a binned configuration where requests are categorized into ten discrete prediction intervals
based on their predictions. Third, we evaluate a fully individualized setting where each request is
assigned its own unique prediction interval centered around the true output length, better reflect-
ing practical scenarios where prediction confidence varies per request. Notably, while the second
experiment groups requests by interval similarity, the third preserves each request’s distinct inter-
val characteristics. Across all three configurations, Amax demonstrates strong performance only
when predictions are highly accurate, while Amin proves remarkably robust, consistently matching
or approaching the performance of the hindsight algorithm that operates with perfect knowledge
of the true output lengths.
1.2. Other Related Work
Optimization in LLM Inference. While existing LLM inference literature has predominantly
focused on system design and engineering optimizations Patel et al. (2023), Zhong et al. (2024),
Yu et al. (2022), Agrawal et al. (2023, 2024b), recent work has begun establishing theoretical foun-
dations for LLM inference scheduling. The seminal work of Jaillet et al. (2025) first formalized a


--- Page 5 ---
5
mathematical framework for analyzing algorithm performance in this context. Our work extends
this foundation by introducing prediction intervals—where algorithms must operate robustly with-
out knowledge of true output lengths. Parallel developments include Ao et al. (2025), which incor-
porates multi-metric scheduling constraints, Wang et al. (2025), which designs efficient scheduling
algorithms for the setting where the input size is heterogeneous, and Li et al. (2025), which charac-
terizes stability conditions for LLM inference systems. Together, these works mark the emergence
of theoretical optimization approaches complementing the field’s traditional systems focus.
Decision-Making and Operations Management with Prediction Uncertainty. Our work
connects to broader research on learning-augmented algorithms, where predictions inform decision-
making under uncertainty. While Shahout et al. (2024) develops prediction methods for LLM
inference without considering robust scheduling, other domains have deeply explored this paradigm:
Lykouris and Vassilvitskii (2021) analyzes online caching with access predictions, Golrezaei et al.
(2023) examines online resource allocation with predicted demand, Agrawal et al. (2011) studies the
prediction market design, and Jin and Ma (2022), Antoniadis et al. (2020) investigates prediction-
enhanced secretary problem and online matching. The robust optimization literature (Bertsimas
and Sim 2004) also provides foundational techniques for handling uncertainty, while recent advances
by Zhou et al. (2022) and Zhang et al. (2023) demonstrate applications to scheduling problems. Our
work bridges these perspectives by developing robust policies specifically for prediction-informed
LLM scheduling frameworks.
Online and Offline Scheduling. The operations research community has extensively studied
online and offline scheduling problems (Chen et al. 1998, Blazewicz et al. 2007, Mak et al. 2015,
Leung et al. 2010). These problems involve determining the processing order and start times for
a large set of jobs that cannot be executed simultaneously. Prior work includes batch scheduling,
where jobs are processed in parallel or grouped into batches (Xing and Zhang 2000, Yang et al. 2003,
Cao et al. 2005, Chen and Lee 2008), and resource-constrained scheduling, where jobs consume
limited resources during execution (Brucker et al. 1999, Hiermann et al. 2015).
The LLM inference scheduling problem studied in our work combines these two dimensions: it is
both batch-based and resource-constrained, with GPU memory acting as a reusable resource. Cru-
cially, the memory usage patterns of LLM inference exhibit unique characteristics that distinguish
our problem from classical scheduling models. These distinct features necessitate the development
of new algorithmic approaches tailored to this setting.


--- Page 6 ---
6
2. Problem Settings
In this paper, we study an extension of the LLM inference scheduling model proposed in Jaillet
et al. (2025). We begin by reviewing the mathematical framework presented in Jaillet et al. (2025)
and then describe the modifications introduced in our model. We consider a setting with a single
computational worker equipped with a KV cache of size M > 0, capable of storing up to M tokens.
In practice, the value of M depends on the complexity of the large language model and the available
hardware memory of the computational worker. We assume that M is known to the decision-
maker. A total of n jobs (prompts) are waiting in the queue, where each job i has a size si > 0,
representing the number of tokens in the prompt. Following the assumptions in Jaillet et al. (2025),
and motivated by the observation that in real-world parallel computing environments prompts
assigned to a worker typically have similar lengths, we assume si = s > 0 for all i ∈[n]. The value
of s is known to the decision-maker.
Prediction Interval for Output Length.
To process each prompt, the model generates the
output token by token. Let oi > 0 denote the realized output length of job i, representing the
number of tokens generated in its response. Since the realized value oi is not revealed until the last
output token is generated, it remains unknown to the decision-maker during the process. In Jaillet
et al. (2025), it is assumed that the realized output length oi is perfectly predicted and known in
advance. In contrast, our current model assumes that oi is unknown a priori but can be predicted
to lie within an interval [ℓ,u], and we denote α = ℓ
u. For clarity and to build intuition regarding
the complexity of the problem and the design of our algorithm, we first consider the case where all
jobs share the same predicted interval [ℓ,u]. We then generalize the result in Section 4.2 to allow
each job’s output length to fall within different predicted intervals.
Batch Processing, Memory Constraint, and Cancellation of Requests.
Following the
model in Jaillet et al. (2025), we allow jobs to be processed in batches. At each discrete time step t,
the scheduler selects a subset of jobs S(t) to form a batch for processing. Each batch takes exactly
one unit of time to process. For any job i ∈S(t), let a(t)
i
∈{0,1,...,oi} denote the number of output
tokens it has generated by time t. If a(t)
i
< oi, then processing the batch at time t results in the
generation of the (a(t)
i + 1)-th token for job i. Once a(t)
i
= oi, job i is fully completed.
Each batch must also satisfy a memory constraint determined by the KV cache limit M. At any
time t, let A(t) denote the set of active jobs—i.e., those that have begun processing in some earlier
batch. For each active job i ∈A(t), the memory required is s+a(t)
i , accounting for both the prompt
size and the number of output tokens generated so far. The total memory usage at time t must
not exceed the limit M, yielding the following constraint:
X
i∈A(t)
(s + a(t)
i ) ≤M,
∀t ≥0.
(1)


--- Page 7 ---
7
Since our setting assumes that only interval predictions of oi are available, it is hard to guarantee
feasibility of constraint (1) under some non-preemptive policies. For example, suppose the total
memory usage at time t is M −1 with two active jobs, each with at least two tokens remaining.
Even if the decision-maker delays one job and processes the other, both jobs will eventually require
at least 2 units of additional memory. Thus, there is no way to complete both without exceeding
the memory constraint in some future time step.
To address this challenge, based on the non-preemptive structure, our model permits the cancel-
lation of jobs. That is, the scheduler may cancel any active job i at time t, discarding all previously
generated output tokens a(t)
i
and resetting the job’s state to unprocessed. We emphasize that
since the sampling method is greedy, the total realized output length oi of each job remains fixed,
regardless of how many times it is cancelled and restarted.
Evaluation Metrics. We use total end-to-end latency as our performance metric. Denote the
vector o = (o1,o2,...,on) which contains the true value of output length of all requests. W.L.O.G,
we assume that o1 ≤o2 ≤... ≤on. Under a scheduling policy π, all prompts are arranged into an
input queue I = (I0,I1,··· ,IT), where It denotes the set of prompts whose last (and final) starting
time is at time t—that is, they are not cancelled after t. For each request i ∈It, we define its latency
as Li = t + oi, which corresponds to the completion time of its last output token. Since all jobs
arrive at time t = 0, this is also the total end-to-end latency of request i, and the total end-to-end
latency of the system is then given by
TEL(o;π) :=
X
i∈[n]
Li =
T
X
t=0
t · |It| +
X
i∈[n]
oi.
(2)
The second equality follows by grouping all jobs according to their final starting time: each job in
It contributes latency t from waiting and oi from processing. The first term sums all waiting times,
while the second term sums all output lengths.
3. Benchmark Algorithms
In this section, we introduce several benchmark algorithms to evaluate a given scheduling policy π.
A central challenge in our model is that the decision-maker does not know the exact output length
oi of each request i in advance. Instead, the only information available is that oi ∈[ℓ,u], and we
define the uncertainty parameter as α = ℓ
u. The true value of each oi is selected adversarially from
this interval at the outset. To assess performance under this uncertainty, Subsection 3.1 introduces
a hindsight benchmark algorithm from Jaillet et al. (2025), in which all output lengths oi are known
ahead of time. We also define the competitive ratio as a metric for evaluating policies that operate
without access to the true values of oi. In Subsection 3.2, we propose a naive benchmark algorithm
that lacks knowledge of oi, and analyze its competitive ratio.


--- Page 8 ---
8
3.1. Review of Hindsight Benchmark Algorithm and Competitive Ratio
In the hindsight setting—where the decision-maker has complete knowledge of the output lengths
oi for all i ∈[n]—our model closely aligns with the setting in Jaillet et al. (2025), with one key
distinction: we allow for job cancellations. However, when the exact values of oi are known in
advance, cancellations are unnecessary. This is because the memory usage over time can be precisely
anticipated and managed using the approach introduced in Jaillet et al. (2025) as follows: at each
time t, let Rt be the set of pending requests awaiting processing. Suppose we consider adding a
subset U ⊂Rt to the batch. Define tmax(U) := maxi∈U{t + oi}, which represents the latest time
any job in U will complete if processing starts at time t. To ensure that the memory constraint
is respected throughout this interval, we must verify that the total memory consumption remains
below the KV cache limit M at every time t′ ∈[t,tmax(U)]. This requirement is formalized by the
following constraint:
X
i∈S(t)
(s + t′ −pi) · 1{oi≥t′−pi} +
X
i∈U
(s + t′ −t) · 1{oi≥t′−t} ≤M,
∀t′ ∈[t,tmax(U)],
(3)
where S(t) denotes the set of jobs already in progress at time t, and pi is the last start time of job
i. The first summation captures memory usage from ongoing jobs, while the second accounts for
the new jobs in U. As long as this constraint is satisfied for all relevant time points, the batch is
guaranteed to respect the memory limit and no cancellations are needed.
Furthermore, when the decision-maker has full knowledge of the values oi, either in theory (Jaillet
et al. (2025)) or in practical implementations (Shahout et al. (2024), Zheng et al. (2023b), Qiu
et al. (2024), Fu et al. (2024), Chen et al. (2024)), one effective batching strategy is shortest-job-
first. The classical OR heuristic attempts to include as many of the shortest remaining jobs with
respect to oi as possible in each batch without violating constraint (3). The rationale is twofold:
first, shorter jobs contribute less to waiting time, reducing overall latency; second, since the peak
memory usage of a job is s + oi, shorter jobs also occupy less memory, allowing more requests to
be packed into earlier batches. We refer to this hindsight benchmark as (Hindsight-Shortest First)
H-SF, and describe it formally in Appendix EC.2. Next, we describe the definition of competitive
ratio:
Definition 1 (Competitive Ratio). Let o = (o1,o2,...,on) ∈[ℓ,u]n denote the vector of true
output lengths for all requests. For a scheduling policy π, let TEL(o;π) denote the total end-to-end
latency under policy π, and let TEL(o;H-SF) denote the latency achieved by H-SFwhich has full
knowledge of o. Then, the competitive ratio of policy π is defined as:
CR(π) :=
sup
o∈[ℓ,u]n
E[TEL(o;π)]
TEL(o;H-SF).


--- Page 9 ---
9
3.2. Naive Benchmark Algorithm
Now consider the setting where the decision-maker only has access to the prediction interval [ℓ,u]
for each job. Since all intervals are identical, the decision-maker cannot distinguish between jobs
and possesses no information to prioritize one over another. It can only reason about the range
within which each job’s output length may fall.
To handle this uncertainty, we introduce a naive benchmark algorithm, Max-Length Based Algo-
rithm, denoted by Amax. This algorithm assumes the worst-case scenario by treating every job as if
its output length equals u. At each time step, it selects the maximum number of prompts that can
be processed without violating the memory constraint in Equation (3). Since the output lengths
are overestimated, the algorithm guarantees that the memory limit is never exceeded, and thus no
cancellations are required. The detail information can be found in Algorithm 1.
Algorithm 1: Amax
Input: Memory Capacity M, prompt size s, output length lower and bound ℓ≤oi ≤u for
all i ∈[n].
Output: Processing sequence I = (I0,I1,··· ,IT).
while there exist waiting requests do
Let Rt be the set of waiting prompts at time t. Let St be the set of tokens currently
processing at time t.
Take all oi = u, find the largest cardinality value mt such that there exists a set U ⊂Rt
such that |U| = mt and Equation (3) hold for all t′ ≥t.
Randomly sample It ⊂Rt with |It| = mt.
Process the requests in It ∪St and update Rt+1 = Rt\It.
end
Although Amax guarantees feasibility, it may suffer from inefficiency due to its overly conserva-
tive estimation of output lengths. By assuming each job has the maximum possible length u, it
overestimates the peak memory usage per job and may significantly under-utilize the available KV
cache memory. The following example illustrates this inefficiency:
Example 1. Consider a setting with n = 5 requests, each with input size s = 1, and output lengths
oi ∈[1,4]. Suppose the true values are oi = 1 for all i ∈[5], and the memory capacity is M = 10.
The hindsight-optimal algorithm H-SF, having access to the true output lengths, knows that the
peak memory usage per job is s + oi = 2. Hence, it can batch all 5 jobs together, fully utilizing the


--- Page 10 ---
10
memory M = 2 × 5 = 10. Each job completes after 1 unit of processing time, resulting in a latency
of 1 per job and total end-to-end latency of 5 × 1 = 5.
In contrast, Amax assumes oi = 4 for all jobs, leading to a peak memory estimate of s+u = 5 per
job. It therefore batches only ⌊M/5⌋= 2 jobs at a time. It first processes 2 jobs, then the next 2,
and finally the last remaining job. The resulting job completion times are 1, 1, 2, 2, and 3, yielding
a total latency of 1 + 1 + 2 + 2 + 3 = 9.
This example highlights the inefficiency of Amax, particularly in cases where the prediction inter-
val is wide. As the gap between ℓand u increases—i.e., as α = ℓ/u decreases—the degree of memory
under-utilization becomes more severe. The following theorem establishes an upper bound on the
competitive ratio of Amax.
Theorem 1. The competitive ratio of Amax is upper bounded by:
CR(Amax) ≤α−1(1 + α−1)
2
+ O( 1
M ).
While the example above provides intuition for where the inefficiency of Amax arises, formally
analyzing its competitive ratio is extremely challenging. The difficulty stems from the complex
dynamics of the model, particularly the fact that each job’s memory usage increases linearly over
time during processing. Therefore, it is hard to find the closed-form connection between the total
end-to-end latency and the memory constraint. To address this challenge, we introduces a combi-
natorial structure that captures the underlying memory evolution in a concise and analyzable way.
This memory-preserving structure serves as a key proof technique and offers a general framework
for analyzing scheduling problems under similar memory constraints. We use a separate section to
make the full proof, and one can find the proof in Appendix EC.1.
While Theorem 1 provides an upper bound of competitive ratio of Amax, this ratio is not tight
because it treats all output lengths as equal to u, which causes a mismatch between the effect of
output lengths on latency and the effect of the input order. As a result, our analysis cannot fully
capture how the actual input sequence influences the scheduling performance, leading to a loose
upper bound. Then, we also provide a lower bound of competitive ratio of Amax.
Theorem 2. The competitive ratio of Amax is lower bounded by
CR(Amax) ≥α−1(1 + α−1/2)
2
.


--- Page 11 ---
11
Proof of Theorem 2
Take s = 0, recall
ℓ
u = α. Let M = ℓu, and the algorithm Amax is assigned
with Nℓ= au prompts with output length ℓand Nu = bℓprompts with output length u. Here, we
take a
b ≈α1/2 and a,b are large enough. Because Amax regards all prompts as being of size u, it will
only initiate a new request if there is memory u available. This is equivalent to considering it as a
combination of M
u = ℓparallel workers, where any one of them can only process one prompt at a
time and start a new one randomly after finishing a prompt.
Then it is simple to calculate E[TEL(o;Amax)] = (au + bℓ) auℓ+bℓu
2ℓ
, since all prompts have the
expected output time auℓ+bℓu
2ℓ
. Also, TEL(o;H-SF) = uℓ(1 + ··· + a) + bℓaℓ+ uℓ(1 + ··· + b).
We get
CR(Amax) ≥u
l
(au + bℓ)(a + b)
ua2 + ℓ2ab + ub2 = α−1(1 + α−1/2)
2
.
□
4. Robust Advanced Scheduling Algorithm
Previously, we introduced a naive scheduling algorithm, Amax, and analyzed its competitive ratio,
which is asymptotically between α−1(1+α−1/2)
2
and α−1(1+α−1)
2
according to Theorems 1 and 2, where
α = ℓ
u captures the prediction accuracy of the output lengths oi. This result shows that the perfor-
mance of Amax is highly sensitive to the quality of the prediction model. In the ideal case where
the prediction is perfect (i.e., ℓ= u, so α = 1), the competitive ratio equals to 1, indicating optimal
performance. However, in practice, decision-makers often prioritize speed over accuracy in predic-
tion. When the prediction interval is wide (i.e., α →0), the competitive ratio grows unbounded
with the rate between O(α−2) and O(α−1.5), demonstrating that Amax is highly non-robust. This
observation raises a key question: can we design a scheduling algorithm that performs well not only
under accurate predictions, but also maintains a better competitive ratio under poor prediction
quality? In this section, we introduce an advanced and robust policy denoted Amin.
Recall that in Amax, each request is pessimistically assumed to have the maximum output length
u. When the true output length oi is much smaller than u, this conservative estimate leads to
significant memory overprotection, reducing the number of simultaneously processed requests and
increasing latency. To address this, we approach the problem from the opposite perspective. If the
goal is to maximize the number of requests in each batch, it is natural to use the lower bound ℓ
as a proxy for each request’s output length. Unlike the upper bound u, the lower bound can be
dynamically updated: initially, we know only that oi ≥ℓ, we create a variable ˜oi for each request i as
its output length lower bound and initialize ˜oi = ℓat beginning. But once a request has generated
˜ℓi tokens, we know oi ≥˜ℓi, and we can update the lower bound accordingly, namely ˜oi = ˜ℓi. Based
on this idea, our advanced algorithm Amin proceeds in two key steps:


--- Page 12 ---
12
1. Memory Overflow Resolution via Ordered Eviction. Because the true output length oi is not
known in advance and may exceed ˜oi, the algorithm may encounter a situation where continuing to
process the current batch would exceed the memory limit. In this case, Amin removes jobs from the
batch to bring the memory usage back within the constraint. Specifically, it orders the currently
active jobs by increasing ˜oi, breaks ties randomly, and removes jobs one by one in this order until
the projected memory usage becomes feasible. This strategy ensures that the algorithm prefers
to retain requests with larger accumulated lower bounds, which are more expensive to restart.
Moreover, for each canceled request i, we update the value of ˜oi to reflect the number of output
tokens already generated, which is a lower bound of the true output length oi.
2. Batch Formation Based on Sorted Lower Bounds. At time t = 0, the algorithm sets ˜oi = ℓfor
all requests i. To form a batch, it sorts the current set of unprocessed or restartable requests in
ascending order of ˜oi, breaking ties uniformly at random. It then greedily selects as many requests
as possible from the sorted list—starting from the smallest ˜oi—until the memory constraint is
reached.
Importantly, since the sampling process is greedy and deterministic, removing a request i does
not change its true output length oi. Thus, even if a request i is removed and later restarted, the
algorithm retains its current lower bound ˜oi at the time of removal. This updated value is used for
future batch formation. The auxiliary lower bound ˜oi is updated only when a job i is cancelled,
since a job continues processing uninterrupted once selected into a batch; thus, there is no need to
update ˜oi while the job remains active. Upon cancellation, however, the number of output tokens
already generated provides a new certified lower bound on the true output length oi, and this
updated ˜oi is used to inform future batch decisions. This strategy avoids unnecessary computation
while preserving the algorithm’s adaptiveness and memory efficiency.
Key Design Advantage: No Need for Upper Bound Prediction. An important feature of Amin is
that it operates solely using the predicted lower bounds ℓ, without ever relying on the predicted
upper bounds u. This makes the algorithm significantly more practical and robust: in many real-
world settings, generating reliable lower bounds is much easier and faster than estimating sharp
upper bounds. By avoiding dependence on u, Amin remains effective even when the prediction
intervals are highly uncertain or asymmetric, making it more suitable for deployment in systems
where inference-time estimates must be generated quickly or under weak supervision. The formal
pseudocode for Amin is presented in Algorithm 2.
Next, we start to analyze the theoretical performance of Amin. First, the following theorem states
the computational complexity of Amin is polynomial, and the proof can be found in Appendix EC.4.


--- Page 13 ---
13
Algorithm 2: Amin
Input: Memory capacity M, prompt size s, output length lower bounds oi ≥ℓfor all i ∈[n].
Output: Processing sequence I = (I0,I1,··· ,IT).
Initialize ˜oi ←ℓfor all i ∈[n].
while there exist unfinished requests do
Let Rt be the set of waiting prompts at time t. Let St be the set of tokens currently
processing at time t.
if projected memory usage of St at time t + 1 exceeds M then
Sort St in ascending order of ˜oi; break ties uniformly at random.
Remove jobs one by one from St (following the sorted order) until projected memory
usage at t + 1 satisfies the memory constraint.
For each removed request i, update ˜oi ←number of tokens already generated by
request i.
end
Let S′
t be the set of remaining active requests after any removals.
Sort Rt in ascending order of ˜oi; break ties uniformly at random.
Select the largest subset It ⊂Rt (in order) such that adding It to S′
t satisfies the
memory constraint in Equation (3).
Process the requests in It ∪S′
t.
Update Rt+1 = Rt \ It.
end
Theorem 3. Consider the KV cache memory limit is M, Algorithm Amin has a computational
complexity of O(MlogM).
Next, Theorem 4 states that no algorithm can achieve a better performance than Amin when
M →∞, showing the asymptotically optimality. The proof can also be found in Appendix EC.4.
Theorem 4. For any feasible policy π without the full information of o = (o1,o2,...,on), when
M →∞, the competitive ratio is lower bounded by
CR(π) ≥CR(Amin).
Although Theorem 4 proves the optimality of Amin, it is still necessary to analyze its competitive
ratio to quantify its improvement over Amax. However, like Amax, deriving an upper bound on this
ratio is highly challenging. The next subsection presents our proof approach and theoretical results.


--- Page 14 ---
14
4.1. Logarithm Competitive Ratio Bound of Amin
In this subsection, our main purpose is to introduce our novel method to prove the following
theorem:
Theorem 5. While M →∞, the asymptotical competitive ratio of Amin is O (log(α−1)).
To prove Theorem 5, we divide the analysis into two parts. In Part 1, we derive a closed-form
expression for the competitive ratio of Amin. In Part 2, we rigorously bound this expression and
show that it scales logarithmically.
Part 1: Get a Closed-form expression for the competitive ratio.
We begin with two key observations:
• Observation 1: The execution of Amin can be partitioned into several periods: τℓ,τℓ+1,··· ,τu.
Before the start of the period τi, all requests q with real output length oq < i have already been
completed. In period τi, Amin attempts to initialize all requests with a lower bound on output
length ˜o = i. For example, in the first period, all requests are loaded into memory and processed
until they are either completed or deleted. If a request q is deleted, its associated lower bound
is updated to a new value ˜oq > i. During τi, we denote by bij, cij, and dij the numbers of input,
completed, and deleted requests with the true output length o = j, respectively. These quantities
satisfy the following relationship.
Lemma 1. We have
E[cij|bij] = s + i
s + j bij,
E[dij|bij] =

1 −s + i
s + j

bij.
The proof of Lemma 1 can be found in Appendix EC.4.
• Observation 2: In period τi, if a request q is deleted and reassigned a new lower bound ˜oq = k,
its true output length must satisfy oq ≥˜o. Again, by input symmetry, the probability that its true
output length is o = j ≥k is proportional to bij. This is because Amin breaks ties uniformly when
selecting inputs. Formally, we have
Pi(o = j | ˜o = k) =
bij
P
t≥k bit
.
(4)
For a fixed request set on of n requests, where xi is the proportion of the number of requests with
output length i in the request set on, the above two observations allow us to derive the expected
state transitions across all periods.


--- Page 15 ---
15
Proposition 1. Given the request set on, we have
E[cij] =







s + i
s + j xjn,
if i = ℓ,
1
s + j xjn,
otherwise.
(5)
The proof of Proposition 1 can be found in Appendix EC.4. Proposition 1 indicates that in the
initial period τℓ, Amin successfully processes more requests with shorter lengths. Moreover, the
proportion of completed requests decreases with j according to the factor s+ℓ
s+j. In the subsequent
periods, the expected number of completed requests remains constant at
1
s+j times xjn. Although
this expected completion ratio does not change across periods, Amin achieves it by continually
updating the lower bound ˜o in earlier periods. This demonstrates that Amin can effectively defer
the input of longer requests by actively learning and utilizing structural information from previous
steps. We can now derive the competitive ratio of Amin.
Lemma 2. Let
δs,l(k) =





s + l,
if k = l
1,
if k ̸= l
We have
CR(Amin) =
Pu
k=ℓδs,l(k)(Pu
i=k ixi)(Pu
i=k
δs,l(k)
s+i xi + 2Pu
i=k+1
i−k
s+ixi)
Pu
k=ℓk(s + k)xk(xk + 2Pu
i=k+1 xi)
+ O( 1
M ).
Proof of Lemma 2
We compute the two components of
E[TEL(on;Amin)]
TEL(on;H-SF) .
For the hindsight algorithm, which processes requests starting from the shortest ones, the total
processing time for requests of output length k is k(s+k)xkn
M
. At this point, requests with length k
are symmetric, and the average latency increase for these requests is k(s+k)xkn
2M
. Additionally, the
remaining Pu
i=k+1 xin requests must wait, incurring a further latency of k(s+k)xkn
M
.
Summing over all k from ℓto u, we obtain
TEL(on;H-SF) = n2
2M
 u
X
k=ℓ
k(s + k)xk
 
xk + 2
u
X
i=k+1
xi
!!
(1 + O( 1
M )).
For the algorithm Amin, it successfully completes a portion of the requests in each stage, leading
to an expected time cost of
E[τk] =
u
X
i=k
i(s + i)
M
E[cii] = δs,l(k)
M
u
X
i=k
ixin.


--- Page 16 ---
16
Furthermore, the expected number of remaining requests is Pu
i=k+1
i−k
s+ixin.
Summing over k from ℓto u, we get
E[TEL(on;Amin)] = n2
2M
 u
X
k=ℓ
δs,l(k)
 u
X
i=k
ixi
! u
X
i=k
δs,l(k)
s + i xi + 2
u
X
i=k+1
i −k
s + ixi
!!
(1 + O( 1
M )).
Dividing by TEL(on;H-SF) and taking the limit as n →∞, we conclude that
CR(Amin) =
Pu
k=ℓδs,l(k)(Pu
i=k ixi)(Pu
i=k
δs,l(k)
s+i xi + 2Pu
i=k+1
i−k
s+ixi)
Pu
k=ℓk(s + k)xk(xk + 2Pu
i=k+1 xi)
+ O( 1
M ).
□
To see the asymptotic performance of CR(Amin), W.L.O.G., we take s = 0, ℓ= 1, then, α = ℓ
u = 1
u.
The next corollary writes the competitive ratio of Amin in the matrix form:
Corollary 1. Let ⃗x = (x1,··· ,xu)⊤, and define Au = (aij)u×u and Bu = (bij)u×u, where
aij = min(i,j)2
ij
1
2(i + j)2 −min(i,j)2

,
bij = min(i,j)2.
The competitive ratio of Amin simplifies to the following Rayleigh quotient,
CR(Amin) = max
⃗x
⃗x⊤Au⃗x
⃗x⊤Bu⃗x + O( 1
M ).
The proof of Corollary 1 can be found in Appendix EC.4. Our goal is to show that the Rayleigh
quotient
R(⃗x) := ⃗x⊤Au⃗x
⃗x⊤Bu⃗x
is uniformly bounded by O(log u) for all ⃗x > 0.
Part 2: Bound the Rayleigh quotient.
To provide the upper bound of R(⃗x), we start by showing that the two matrices Au and Bu are
both positive definite.
Lemma 3 (Positive Definiteness of Au and Bu). For all integers u ≥1, both Au and Bu are
positive definite.
The proof of Lemma 3 can be found in Appendix EC.4.
Consider the matrix
Cu := B−1
u Au.


--- Page 17 ---
17
Since both Au and Bu are symmetric and positive definite (by Lemma 3), the matrix Cu is similar to
a symmetric positive definite matrix, and thus has real, strictly positive eigenvalues. In particular,
the maximum of the Rayleigh quotient satisfies
max
⃗x>0 R(⃗x) ≤max
⃗x∈Ru R(⃗x) = ρ(Cu),
where ρ(Cu) denotes the spectral radius, i.e., the largest eigenvalue of Cu.
Since all eigenvalues of Cu are positive, its spectral radius is bounded above by the sum of its
eigenvalues. Therefore, it suffices to estimate the trace:
ρ(Cu) ≤tr(Cu) = tr(B−1
u Au).
We now compute the trace of Cu = B−1
u Au explicitly using the known expressions for Au and
the inverse of Bu. Recall that B−1
u
is tridiagonal, with entries
(B−1
u )ij =





















4i
4i2 −1,
i = j < u,
1
2u −1,
i = j = u,
−
1
2min(i,j) + 1,
|i −j| = 1,
0,
otherwise.
Using the symmetry of Au, the trace can be written as the sum of products over all entries of
B−1
u
and Au along the main and immediate subdiagonals:
tr(B−1
u Au) =
u
X
i=1
(B−1
u )iiaii + 2
u−1
X
i=1
(B−1
u )i,i+1ai,i+1.
We define the two contributions as:
(diagonal terms),
T1(u) :=
u
X
i=1
(B−1
u )iiaii =
u−1
X
i=1
4i
4i2 −1i2 +
1
2u −1u2,
(subdiagonal terms),
T2(u) :=
u−1
X
i=1
(B−1
u )i,i+1ai,i+1 = −
u−1
X
i=1
1
2i + 1
i
i + 1(i2 + 2i + 1
2),
so that
tr(B−1
u Au) = T1(u) + 2T2(u).
Explicit computation yields the following closed-form expressions:
- For the diagonal term:
T1(u) = u2 + 1
2
+ 1
2
u−1
X
i=1
1
2i + 1.


--- Page 18 ---
18
- For the subdiagonal term:
2T2(u) = −u2 −1
2
+
u−1
X
i=1
1
i + 1 −1
2
u−1
X
i=1
1
2i + 1.
Combining all terms:
tr(B−1
u Au) = T1(u) + 2T2(u) =
u
X
i=1
1
i = O(log u).
Therefore, the spectral radius of Cu is also O(log u), and the Rayleigh quotient satisfies:
max
⃗x>0
⃗x⊤Au⃗x
⃗x⊤Bu⃗x = O(log u),
which implies that the competitive ratio of the algorithm Amin satisfies:
CR(Amin;D) = O (log u) = O
 log(α−1)

.
□
We now have analyzed the Rayleigh quotient theoretically, establishing a logarithmic competitive
ratio for Amin. Finally, we numerically estimate the Rayleigh quotient for the case s = 0 and ℓ= 1
to verify the theoretical result. As shown in Figure 1, the logarithmic function provides an excellent
fit, with an R2 value exceeding 0.9999.
Figure 1
Competitive ratio of Amin versus upper bound u, fitted by 0.2555log(u + 0.2793) + 1.1587, assuming
s = 0, l = 1.


--- Page 19 ---
19
4.2. Heterogeneous Prediction Intervals
Now, we extend our framework by replacing the single prediction interval with m disjoint intervals:
[ℓ1,u1],[ℓ2,u2],...,[ℓm,um], satisfying uj ≤ℓj+1. This formulation naturally models classification-
based prediction approaches, where a machine learning model first predicts an output length for
each request and then assigns it to the corresponding interval.
Our algorithm, Amin, naturally extends to this generalized setting with minimal modifications.
For each request i classified into interval [ℓj,uj], we initialize its lower bound as ℓj rather than
using a uniform initial value across all requests. This modified version (Algorithm 5 in Appendix
EC.4) guarantees that the initial underestimation becomes more precise for each individual request,
and the algorithm consequently achieves better overall performance. As formalized in the following
corollary, this adaptation yields a competitive ratio at least equals to the basic setting. We validate
the numerical performance in Experiment 2 in Section 6, demonstrating the practical benefits of
incorporating refined prediction intervals.
Corollary 2. Under prediction intervals [ℓ1,u1],[ℓ2,u2],...,[ℓm,um], satisfying uj ≤ℓj+1, take
α = ℓ1
um . While M →∞, the asymptotical competitive ratio of the revised Amin is O (log(α−1)).
5. Extensions: Amin and New Algorithms under Specific Output Distributions
In previous sections, we assumed the adversary selects request output values within the prediction
interval [ℓ,u]. Here, we examine another variant where outputs follow some specific distributions D
over [ℓ,u], and analyze Amin’s performance under this setting. Notably, Amin operates solely using
ℓwithout distributional knowledge. This motivates our investigation of new algorithms that can
leverage D to improve performance.
We focus on three representative distributions:
1. Two-point distribution: All outputs are either ℓor u, representing an extreme case;
2. Geometric distribution: Outputs exhibit exponential decay, a pattern commonly observed in
real-world datasets.
3. Linearly weighted Geometric distribution: Combines exponential decay with a linearly
weighted preference for values near the lower bound ℓ. This modified distribution better captures
real-world phenomena where extreme values occur more frequently than standard geometric decay
would predict.
5.1. Two-Point Distribution
In this section, we examine a special case where the output length o ∈{ℓ,u}. We denote this
two-point distribution family as D2. We starts by evaluating the theoretical performance of Amin.


--- Page 20 ---
20
Theorem 6. Under any distribution D ∈D2, the competitive ratio of Amin is bounded by:
CR(Amin) ≤3 −α
2
+ O( 1
M ).
The complete proof appears in Appendix EC.5. Theorem 6 establishes that for any α ∈[0,1], the
competitive ratio of Amin is bounded above by 3
2. This represents a significant improvement over
Amax, whose competitive ratio is unbounded in the worst case – a scenario that occurs precisely
within the two-point distribution family D2. Our result thus demonstrates that Amin achieves
bounded competitiveness where Amax fails.
Next, we introduce the promote–ℓpolicy Aℓ, which uses the information that the output distri-
bution belongs to D2. Under this policy, the server processes each incoming request sequentially.
As soon as the currently served request generates ℓtokens, the policy infers that the true length
of this request must be u > ℓ. At that point, Aℓremoves the request, appends it to the end of the
queue, and proceeds to the next one. Any request of length ≤ℓis completed normally. In effect,
Aℓdefers all long jobs after ℓtokens, postponing them until all shorter jobs are finished. The
detailed algorithm can be found in Algorithm 6 in Appendix EC.5. Next, we provide the theoretical
competitive analysis of Aℓ.
Theorem 7. Under any distribution D ∈D2, the competitive ratio of Aℓis bounded by:
CR(Aℓ) ≤









1 +
α
2(1 −α),
0 < α ≤1
2,
1 + 2α2,
1
2 ≤α ≤1,
+ O
  1
M

,
where α = ℓ/u.
The proof of Theorem 7 can be found in Appendix EC.5. Next, based on the results in Theorems
6 and 7, we can study when to choose Amin and Aℓ. From Theorem 6, we know that CR(Amin) ≤
(3 −α)/2 + O(1/M), while Theorem 7 gives a piecewise upper bound for CR(Aℓ). To compare the
two when 0 ≤α ≤1
2, we consider:
CR(Aℓ) < CR(Amin)
⇐⇒
1 +
α
2(1 −α) < 3 −α
2
,
which reduces to the quadratic inequality α2 −3α + 1 = 0. Solving yields the critical threshold:
α⋆= 3 −
√
5
2
≈0.382.
Since α = ℓ/u, the condition α < α⋆translates to u ≳2.62ℓ; that is, when the long request is more
than 2.6 times larger than the short one, Aℓachieves a smaller competitive ratio. For α ≥α⋆, the
monotonic bound 1 + 2α2 dominates, making Amin the preferable choice.


--- Page 21 ---
21
Figure 2
Upper-bound curves for the competitive ratios under D2. When α < α⋆≈0.38 (left of the dashed line),
Aℓoutperforms Amin; beyond that point, the inequality reverses.
Practical takeaway. Rather than committing to a single scheduling policy, one can adaptively
choose between Amin and Aℓbased on the empirical ratio α = ℓ/u observed in the workload. This
simple switching strategy ensures the smaller of the two analytical bounds shown in Figure 2,
improving the worst-case guarantee from 1.5 to 1 +
α
2(1−α) in high-skew scenarios, while never
exceeding the red curve in low-skew regimes. More broadly, this illustrates a general principle:
taking advantage of distributional features, such as the length ratio α, to select among multiple
algorithms, can significantly enhance the robustness of competitive-ratio guarantees.
5.2. Geometric Distribution
Let G(p) denote the geometric distribution with parameter p, such that o ∼G(p) and P(o = k) =
pqk−1, where q = 1 −p. The G(p) describes an exponential decay of o on the discrete space [1,u].
Under the geometric distribution, it is hard to provide a theoretical result for the competitive ratio.
However, the left plot in Figure 3 indicates that the competitive ratio of Amin increases when q
increases, and is bounded by 1.7.
5.3. Linearly Weighted Geometric Distribution
Let LG(p) denote the linearly weighted geometric distribution with the parameter p, o ∼LG(p)
and P(o = k) = kp2qk−1, where q = 1 −p. Compared to G(p), the linearly weighted geometric
distribution more accurately reflects practical scenarios, as it exhibits increasing mass for small
values of o and reaches its peak around log 1
q.


--- Page 22 ---
22
Figure 3
Competitive ratio as a function of parameter q: (Left) geometric distribution G(p); (Right) linearly
weighted geometric distribution LG(p).
The right plot of Figure 3 shows that the competitive ratio of Amin again increases when q
increases, and is bounded by 1.6. Furthermore, a closed-form expression can be derived for the
linearly weighted geometric distribution LG(p), and the tight competitive ratio upper bound is
about 1.56. The following theorem shows the result and the proof can be found in Appendix EC.5.
Theorem 8. As u →∞and M →∞, the competitive ratio of Amin under any linearly weighted
geometric distribution D ∈LG(p) is given by
CR(Amin) =
(1 + q)2(1 + 3q + 6q2 + 3q3 + q4)
1 + 2q + 11q2 + 8q3 + 11q4 + 2q5 + q6 ≤14
9 ≈1.56.
6. Numerical Experiments
In this section, we evaluate the performance of different scheduling algorithms using a real-world
dataset under various assumptions about the accuracy of output length predictions.
Dataset Overview. We use the LMSYS-Chat-1M dataset released by Zheng et al. (2023a), avail-
able at https://huggingface.co/datasets/lmsys/lmsys-chat-1m, which consists of conversa-
tions collected from over 210,000 unique IP addresses via the Vicuna demo and the Chatbot Arena
platform. For our numerical experiments, we randomly sample a subset of 2,000 conversations.
For each conversation, we define the input size as the number of tokens in the prompt and the
output length as the number of tokens in the corresponding model response. Among the selected
2,000 conversations, the input sizes range from 1 to 468 tokens, with a mean of 41, median of 11,
and variance of 4,961. The output lengths range from 1 to 883 tokens, with a mean of 85, median


--- Page 23 ---
23
Figure 4
Distribution of the number of words of input prompt and output response respectively
of 43, and variance of 9,702. Figure 4 displays the empirical distributions of input sizes and output
lengths.
Experiment Setup. We evaluate the scheduling algorithms under three prediction settings that
reflect varying levels of output length accuracy:
1. Rough Prediction: Each request is assumed to have a coarse prediction interval of [1,1000],
representing minimal information about the output length.
2. Non-Overlapping Classification:
Based on the true output length of each request, we
assign its prediction interval to one of the fixed buckets: [1,100],[101,200],...,[901,1000], simulat-
ing a multi-class classification model with non-overlapping intervals.
3. Overlapping Interval Prediction:
For each request i with true output length oi, the
prediction interval is set as [(1 −x)oi,(1 + x)oi], where x ∈(0,1) controls the accuracy of the
prediction. Larger values of x correspond to more precise predictions.
To observe latency trends under different load levels, we vary the number of requests in each
simulation, selecting random subsets of size 200, 400, ..., up to 2,000 from the original dataset.
We report the average end-to-end latency, defined as the total latency divided by the number of
requests.
In all experiments, we compare the average latency of Amax and Amin. As a benchmark, we include
the H-SF algorithm, which assumes perfect knowledge of each request’s output length and serves as
a lower bound on achievable latency. Batch processing time is estimated using the linear regression
predictor in Vidur simulator from Agrawal et al. (2024a) under the simulation environment of the
LLaMA2-70B model on two linked A100 GPUs.
Results for Experiment 1. In Experiment 1, the prediction interval for all requests is set to the
broad range [1,1000]. Under this setting, Amax pessimistically assumes that each request has an


--- Page 24 ---
24
output length of 1000. While this conservative approach avoids memory overflows, it leads to poor
memory utilization—each batch includes very few requests, resulting in high latency. In contrast,
Amin initializes the lower bound of each request as 1 and adaptively updates this value during the
inference process as tokens are generated.
Figure 5 shows that Amin achieves average latency nearly identical to the benchmark H-SF,
which has full knowledge of the true output lengths. This result is particularly striking given that
Amin operates with minimal information—only that each request’s output lies somewhere in the
interval [1,1000]. It highlights the robustness and adaptiveness of Amin even under highly uncertain
predictions.
Figure 5
Averaged latency between scheduling algorithms when the prediction for every request’s output length
is [1,1000].
Results for Experiment 2. In Experiment 2, we classify requests into 10 groups, each asso-
ciated with a non-overlapping output length prediction interval: [1,100],[101,200],...,[901,1000].
Compared to Experiment 1, this setting provides significantly more accurate predictions. Figure 6
shows the average latency of the three scheduling algorithms.
First, we observe that Amax achieves a substantial improvement over its performance in Exper-
iment 1—the average latency is nearly halved. This is because Amax now treats each request as
having an output length equal to the upper bound of its assigned group interval, allowing more
requests to be packed into each batch compared to the uniform assumption of 1000 tokens in
Experiment 1.


--- Page 25 ---
25
More importantly, Figure 6 also demonstrates that as prediction accuracy improves, the perfor-
mance of Amin closely approaches that of the benchmark H-SF. This highlights a key strength of
Amin: its ability to adaptively leverage better predictions to achieve latency performance nearly
identical to an ideal scheduler with full information.
Figure 6
Averaged latency between scheduling algorithms when the prediction for every request’s output length
is one of [1,100],[101,200],...,[901,1000].
Results for Experiment 3. In Experiment 3, each request i is assigned a prediction interval of
the form [(1 −x)oi,(1 + x)oi], where x ∈{0.1,0.95,0.99} controls the accuracy of the prediction. A
smaller value of x corresponds to more accurate predictions.
As shown in Figure 7, when x = 0.1, indicating highly accurate predictions, both Amax and
Amin perform well, achieving low average latency. However, as the prediction accuracy decreases
(i.e., x = 0.95 or x = 0.99), the performance of Amax deteriorates significantly. This is because the
algorithm treats each request pessimistically by assuming its output length is near the upper bound
of its interval, which leads to low memory utilization and small batch sizes.
In contrast, Amin continues to maintain low average latency even under highly uncertain predic-
tions. Its performance remains close to the hindsight benchmark H-SF, highlighting the robustness
of Amin in the face of imprecise output length estimates.


--- Page 26 ---
26
Figure 7
Averaged latency between scheduling algorithms when the prediction for every request i’s output length
is [(1 −x)oi,(1 + x)oi].
7. Conclusion
This paper extends the LLM inference scheduling model introduced by Jaillet et al. (2025) to a more
realistic setting where only interval-based predictions of output lengths are available. We first ana-
lyze Amax, a conservative algorithm that avoids memory overflow by overestimating request lengths,
but demonstrate its fundamental limitation: poor robustness when prediction intervals are wide,
leading to excessive resource allocation and reduced concurrency. Our key contribution is Amin,
an adaptive algorithm that employs strategic underestimation followed by progressive refinement
during execution. Through both theoretical analysis and comprehensive numerical experiments,
we establish that Amin achieves robust performance across varying prediction quality while main-
taining high system efficiency. These results provide practical insights for deploying LLM inference
systems under prediction uncertainty.


--- Page 27 ---
27
References
Amey Agrawal, Nitin Kedia, Jayashree Mohan, Ashish Panwar, Nipun Kwatra, Bhargav Gulavani,
Ramachandran Ramjee, and Alexey Tumanov. 2024a. Vidur: A Large-Scale Simulation Framework For
LLM Inference. Proceedings of Machine Learning and Systems 6 (2024), 351–366.
Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, Alexey
Tumanov, and Ramachandran Ramjee. 2024b. Taming throughput-latency tradeoff in LLM inference
with Sarathi-Serve. arXiv preprint arXiv:2403.02310 (2024).
Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, and Ramachandran
Ramjee. 2023. Sarathi: Efficient LLM inference by piggybacking decodes with chunked prefills. arXiv
preprint arXiv:2308.16369 (2023).
Shipra Agrawal, Erick Delage, Mark Peters, Zizhuo Wang, and Yinyu Ye. 2011. A unified framework for
dynamic prediction market design. Operations research 59, 3 (2011), 550–568.
Amazon. 2023. Amazon CodeWhisperer. https://aws.amazon.com/codewhisperer/.
Anthropic. 2023. Claude. https://claude.ai.
Antonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. 2020. Secretary and online matching
problems with machine learned advice. Advances in Neural Information Processing Systems 33 (2020),
7933–7944.
Ruicheng Ao, Gan Luo, David Simchi-Levi, and Xinshang Wang. 2025. Optimizing LLM Inference: Fluid-
Guided Online Scheduling with Memory Constraints. arXiv preprint arXiv:2504.11320 (2025).
Dimitris Bertsimas and Melvyn Sim. 2004. The price of robustness. Operations research 52, 1 (2004), 35–53.
Jacek Blazewicz, Klaus H Ecker, Erwin Pesch, Gunter Schmidt, and Jan Weglarz. 2007.
Handbook on
scheduling: from theory to applications. Springer.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing systems 33 (2020), 1877–1901.
Peter Brucker, Andreas Drexl, Rolf M¨ohring, Klaus Neumann, and Erwin Pesch. 1999. Resource-constrained
project scheduling: Notation, classification, models, and methods.
European journal of operational
research 112, 1 (1999), 3–41.
Dong Cao, Mingyuan Chen, and Guohua Wan. 2005.
Parallel machine selection and job scheduling to
minimize machine cost and job tardiness. Computers & operations research 32, 8 (2005), 1995–2012.
Marco Cascella, Jonathan Montomoli, Valentina Bellini, and Elena Bignami. 2023. Evaluating the feasibility
of ChatGPT in healthcare: an analysis of multiple clinical and research scenarios. Journal of medical
systems 47, 1 (2023), 33.
Character. 2021. Character AI. https://character.ai.
Bo Chen and Chung-Yee Lee. 2008. Logistics scheduling with batching and transportation. European journal
of operational research 189, 3 (2008), 871–876.


--- Page 28 ---
28
Bo Chen, Chris N Potts, and Gerhard J Woeginger. 1998. A review of machine scheduling: Complexity,
algorithms and approximability. Handbook of Combinatorial Optimization: Volume1–3 (1998), 1493–
1641.
Shiyang Chen, Rain Jiang, Dezhi Yu, Jinlai Xu, Mengyuan Chao, Fanlong Meng, Chenyu Jiang, Wei Xu, and
Hang Liu. 2024. KVDirect: Distributed Disaggregated LLM Inference. arXiv preprint arXiv:2501.14743
(2024).
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language
modeling with pathways. Journal of Machine Learning Research 24, 240 (2023), 1–113.
Yichao Fu, Siqi Zhu, Runlong Su, Aurick Qiao, Ion Stoica, and Hao Zhang. 2024. Efficient LLM Scheduling
by Learning to Rank. arXiv preprint arXiv:2408.15792 (2024).
GitHub. 2021. GitHub Copilot. https://github.com/features/copilot.
Negin Golrezaei, Patrick Jaillet, and Zijie Zhou. 2023. Online resource allocation with convex-set machine-
learned advice. arXiv preprint arXiv:2306.12282 (2023).
Google. 2023. Bard. https://bard.google.com.
Gerhard Hiermann, Matthias Prandtstetter, Andrea Rendl, Jakob Puchinger, and G¨unther R Raidl. 2015.
Metaheuristics for solving a multimodal home-healthcare scheduling problem. Central European Journal
of Operations Research 23, 1 (2015), 89–113.
Chenyu Huang, Zhengyang Tang, Shixi Hu, Ruoqing Jiang, Xin Zheng, Dongdong Ge, Benyou Wang, and
Zizhuo Wang. 2025. Orlm: A customizable framework in training large models for automated optimiza-
tion modeling. Operations Research (2025).
Patrick Jaillet, Jiashuo Jiang, Konstantina Mellou, Marco Molinaro, Chara Podimata, and Zijie Zhou. 2025.
Online Scheduling for LLM Inference with KV Cache Constraints. arXiv preprint arXiv:2502.07115
(2025).
Billy Jin and Will Ma. 2022. Online bipartite matching with advice: Tight robustness-consistency tradeoffs
for the two-stage model. Advances in Neural Information Processing Systems 35 (2022), 14555–14567.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv
preprint arXiv:2001.08361 (2020).
Joseph Y-T Leung, Michael Pinedo, and Guohua Wan. 2010. Competitive two-agent scheduling and its
applications. Operations Research 58, 2 (2010), 458–469.
Yueying Li, Jim Dai, and Tianyi Peng. 2025. Throughput-optimal scheduling algorithms for llm inference
and ai agents. arXiv preprint arXiv:2504.07347 (2025).
Thodoris Lykouris and Sergei Vassilvitskii. 2021. Competitive caching with machine learned advice. Journal
of the ACM (JACM) 68, 4 (2021), 1–25.
Ho-Yin Mak, Ying Rong, and Jiawei Zhang. 2015.
Appointment scheduling with limited distributional
information. Management Science 61, 2 (2015), 316–334.


--- Page 29 ---
29
Microsoft. 2023. Bing AI. https://www.bing.com/chat.
OpenAI. 2019. ChatGPT. https://chat.openai.com.
OpenAI. 2023. GPT-4 technical report. arxiv 2303.08774. View in Article 2, 5 (2023).
Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, ´I˜nigo Goiri, Saeed Maleki, and Ricardo
Bianchini. 2023. Splitwise: Efficient generative LLM inference using phase splitting. Power 400, 700W
(2023), 1–75.
Perplexity. 2022. Perplexity AI. https://www.perplexity.ai/.
Haoran Qiu, Weichao Mao, Archit Patke, Shengkun Cui, Saurabh Jha, Chen Wang, Hubertus Franke, Zbig-
niew T Kalbarczyk, Tamer Ba¸sar, and Ravishankar K Iyer. 2024. Efficient interactive LLM serving
with proxy model-based sequence length prediction. arXiv preprint arXiv:2404.08509 (2024).
Malik Sallam. 2023. The utility of ChatGPT as an example of large language models in healthcare education,
research and practice: Systematic review on the future perspectives and potential limitations. MedRxiv
(2023), 2023–02.
Rana Shahout, Eran Malach, Chunwei Liu, Weifan Jiang, Minlan Yu, and Michael Mitzenmacher. 2024.
Don’t Stop Me Now: Embedding Based Scheduling for LLMs. arXiv preprint arXiv:2410.01035 (2024).
Meixuan Wang, Yinyu Ye, and Zijie Zhou. 2025. LLM Serving Optimization with Variable Prefill and Decode
Lengths. arXiv preprint arXiv:2508.06133 (2025).
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent abilities of large language models.
arXiv preprint arXiv:2206.07682 (2022).
Wenxun Xing and Jiawei Zhang. 2000. Parallel machine scheduling with splitting jobs. Discrete Applied
Mathematics 103, 1-3 (2000), 259–269.
Heng Yang, Yinyu Ye, and Jiawei Zhang. 2003. An approximation algorithm for scheduling two parallel
machines with capacity constraints. Discrete Applied Mathematics 130, 3 (2003), 449–467.
Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. 2022. Orca: A
distributed serving system for {Transformer-Based} generative models. In 16th USENIX Symposium
on Operating Systems Design and Implementation (OSDI 22). 521–538.
David W Zhang, Corrado Rainone, Markus Peschl, and Roberto Bondesan. 2023. Robust scheduling with
gflownets. arXiv preprint arXiv:2302.05446 (2023).
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zhuohan Li, Zi Lin, Eric Xing, et al. 2023a. LMSYS-Chat-1M: A large-scale real-world LLM conversa-
tion dataset. arXiv preprint arXiv:2309.11998 (2023).
Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, and Yang You. 2023b. Response length
perception and sequence scheduling: An llm-empowered llm inference pipeline. Advances in Neural
Information Processing Systems 36 (2023), 65517–65530.


--- Page 30 ---
30
Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. 2024.
Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving.
arXiv preprint arXiv:2401.09670 (2024).
Minglong Zhou, Melvyn Sim, and Shao-Wei Lam. 2022. Advance admission scheduling via resource satisficing.
Production and Operations Management 31, 11 (2022), 4002–4020.


--- Page 31 ---
e-companion to xxx: Adaptively Robust LLM Inference Optimization under Prediction Uncertainty
ec1
Proofs of Statements
EC.1. Memory-Preserving Proof Technique for Theorem 1
In this section, we study the relationship between the total end-to-end latency and the memory con-
straint M. Although it is generally intractable to derive a closed-form expression for total latency as
a function of M, we can still make meaningful comparisons between algorithms that satisfy certain
structural properties, which will be formally defined in Definitions EC.1 and EC.2. Specifically, by
analyzing how total latency scales under different memory capacities for such algorithms, we can
bound their relative performance. Based on this approach, we construct a benchmark algorithm
and compare its total latency with that of Amax to establish the competitive ratio.
To introduce the structural ideas underlying our analysis, we first define the notion of the cor-
responding order between two schedules in Definition EC.1. This concept formalizes when two
schedules maintain the same relative processing order, even if their exact starting times differ.
Definition EC.1 (Corresponding Order). Let I = (I0,I1,··· ,IT) and I′ = (I′
0,I′
1,··· ,I′
T) be
two schedules produced by possibly different algorithms. We say that I and I′ preserve the cor-
responding order if, for any two jobs n1 ∈Ii ∩I′
i′ and n2 ∈Ij ∩I′
j′, it holds that
(i −j)(i′ −j′) ≥0.
In other words, if n1 is scheduled earlier than n2 in I, it is also scheduled no later than n2 in I′,
and vice versa. Furthermore, we say that I′ is delayed relative to I, denoted I ≤I′, if for every
job n0 ∈Ii ∩I′
i′, we have i ≤i′.
Definition EC.1 focuses on the relative order of job processing rather than their exact starting
times. This abstraction is crucial because, when comparing scheduling strategies (e.g., shortest-
job-first or longest-job-first) under different memory capacities M, the exact batch compositions
and starting times may vary significantly. Due to the linearly increasing memory usage during
processing, tracking exact timings is highly complex. However, the key observation is that for
scheduling strategies based purely on job ordering, the relative order of job processing remains
stable even as M changes. For instance, shortest-job-first will always prioritize jobs according to
their output lengths, independent of the specific value of M.
Example EC.1. Given the memory capacity M = 7 with n = 4, s = 1 and o = (1,2,3,4). Consider
the following different input schedules I1 = ({1,3},∅,{2},{4}) and I2 = ({3},{1,2},∅,{4}). We find


--- Page 32 ---
ec2
e-companion to xxx: Adaptively Robust LLM Inference Optimization under Prediction Uncertainty
that every request in I1 scheduled earlier does not start processing later in I2 and vice versa. This
means that I1 and I2 preserve the corresponding order. However, the change of requests 1 and 2
shows that neither I1 nor I2 are delayed relative to the other. Moreover, let I3 = ({1,2,3},∅,{4})
which is the optimal input sequence. We can see that I1,I2,I3 preserve the corresponding order
pairwise. In addition, because all jobs in I3 begin earlier, both I1 and I2 are delayed relative to I3,
I3 ≤I1,I2.
Next, to meaningfully compare the total latency of two schedules, it is not sufficient for them
to simply preserve the same corresponding order. We must also ensure that both schedules fully
utilize the available memory—that is, they do not intentionally leave memory unused in a way that
artificially increases latency. However, because the memory usage of each job grows linearly during
processing, it is nontrivial to characterize what it means for a schedule to fully utilize memory. To
formalize this notion, we introduce the concept of an optimally packed schedule in Definition EC.2.
Definition EC.2 (Optimally Packed Schedule). Let M denote the KV cache memory limit.
A schedule I = (I0,I1,··· ,IT) is said to be optimally packed if for every job n0 ∈Ii with i ∈[T],
the following holds: Let
j := max{k ∈[0,i −1] : Ik ̸= ∅}
be the most recent time before i such that Ij is non-empty. Then, for any k ∈[j,i), moving n0 to
batch Ik would result in exceeding the memory limit M at some point during processing.
Definition EC.2 captures the idea that a schedule is memory-efficient in a myopic sense: a job can-
not be moved earlier into any nearby batch—specifically, between its original batch and the nearest
previous non-empty batch—without violating the memory constraint. The definition focuses only
on local perturbations to ensure that no memory is intentionally left unused in a way that could
have allowed earlier scheduling within immediate reach. This notion of optimal packing is thus
well-suited for analyzing algorithms that follow natural, sequential batching processes. Combining
Definitions EC.1 and EC.2, we are now prepared to compare the total latency of two schedules
that (i) preserve the same corresponding order and (ii) are both optimally packed, but operate
under different memory capacities.
Proposition EC.1 (Latency Scaling under Reduced Memory). Let I and I′ be two sched-
ules that preserve the corresponding order and are both optimally packed. Suppose I satisfies a
memory constraint of M, while I′ satisfies a memory constraint of βM, where β ∈(0,1). Then,
the total end-to-end latencies satisfy the following inequality:
TEL(o;I′) ≤β−1 TEL(o;I).
(EC.1)


--- Page 33 ---
e-companion to xxx: Adaptively Robust LLM Inference Optimization under Prediction Uncertainty
ec3
Proof of Proposition EC.1
We aim to show that, under the given memory constraints, the total
end-to-end latency of the input sequence I′ is at most β−1 times that of the input sequence I.
First, fix the input order o = (o1,o2,...,on). Assume that both schedules I and I′ are optimally
packed, meaning that at every time step, the system processes as many requests as permitted by
the memory constraint.
Since the memory capacity in I′ is scaled by a factor β ∈(0,1) relative to that in I, and each
request oi consumes the same amount of memory in both schedules, it follows that at each moment,
the number of requests being processed in I′ is β times the number in I.
Let Li and L′
i denote the completion times of request i under schedules I and I′, respectively.
Our goal is to relate L′
i to Li for each i ∈[n].
Because I′ has a memory capacity reduced by a factor β, and because both schedules are fully
packed, the overall processing rate in I′ is slowed down by a factor of β compared to I. Thus,
to complete the same workload, the time needed under I′ is multiplied by a factor of β−1. This
implies that the completion time of each request satisfies
L′
i ≤β−1Li,
∀i ∈[n].
(EC.2)
Now, recall that the total end-to-end latency TEL for an input sequence is defined as the sum
of the completion times for all requests:
TEL(o;I) =
n
X
i=1
Li,
TEL(o;I′) =
n
X
i=1
L′
i.
(EC.3)
Substituting the earlier established relation between Li and L′
i, we have:
TEL(o;I′) =
n
X
i=1
L′
i ≤
n
X
i=1
β−1Li = β−1TEL(o;I).
(EC.4)
Thus, we conclude that
TEL(o;I′) ≤β−1 TEL(o;I).
□
To apply Proposition EC.1 in our analysis, we first need to construct an auxiliary algorithm
that (i) preserves the same corresponding order as Amax and (ii) has a competitive ratio that is
easier to analyze. By comparing Amax with this auxiliary algorithm, we can bound the performance
of Amax. Noting that Amax treats all jobs as identical and selects a batch by randomly sampling
the maximum feasible number of requests, we construct a fully randomized benchmark algorithm,
Arandom designed specifically for this comparison. The details of Arandom can be found in Algorithm
4 in Appendix EC.3.


--- Page 34 ---
ec4
e-companion to xxx: Adaptively Robust LLM Inference Optimization under Prediction Uncertainty
Algorithm Arandom is an auxiliary algorithm constructed for analysis purposes, and it is not
intended to be practically implementable. In particular, it requires full knowledge of all true output
lengths o = (o1,o2,...,on) as input, similar to a hindsight algorithm that perfectly predicts the
realized output lengths of all jobs. More specifically, Algorithm Arandom first randomly samples a
permutation of all n requests and then processes the jobs according to this fixed order. At each
step, it sequentially adds requests following the permutation, ensuring that the memory constraint
in Equation (3) is satisfied at all times. Since it has access to the true values of o, the algorithm
can achieve an optimally packed schedule. Next, we present a theorem which characterizes the
competitive ratio of Algorithm Arandom and states that for any scheduling policy, the worst case
always happens when the true output length oi is either ℓor u for all i ∈[n].
Theorem EC.1 (Worst-Case Realizations and Competitive Ratio of Arandom). For any
feasible scheduling policy π, the worst-case competitive ratio is attained when each output length oi
takes either the lower bound ℓor the upper bound u. That is,
CR(π) :=
sup
o∈[ℓ,u]n
E[TEL(o;π)]
TEL(o;H-SF) =
sup
o∈{ℓ,u}n
E[TEL(o;π)]
TEL(o;H-SF).
(EC.5)
Moreover, the competitive ratio of the auxiliary algorithm Arandom satisfies
CR(Arandom) ≤1 + α−1
2
+ O( 1
M ),
(EC.6)
as M →+∞, where α = ℓ/u.
The proof of Theorem EC.1 can be found in Appendix EC.3. Next, we compare two versions of
Algorithm Arandom:
• Arandom(M): Algorithm Arandom run with memory capacity M,
• Arandom(αM): Algorithm Arandom run with memory capacity αM, where α = ℓ/u.
Both versions produce randomized schedules based on random permutations of the input
requests. Recall that Proposition EC.1 states that for any pair of schedules that preserve the corre-
sponding order and are optimally packed, we can directly compare their total latencies. To formalize
the connection between Arandom(M) and Arandom(αM), we introduce the following probabilistic
coupling:
Definition EC.3 (Permutation Coupling). Sample a random permutation σ of [n] uniformly
at random.
• Under Arandom(M), schedule the jobs according to σ, subject to memory capacity M.


--- Page 35 ---
e-companion to xxx: Adaptively Robust LLM Inference Optimization under Prediction Uncertainty
ec5
• Under Arandom(αM), schedule the jobs according to the same permutation σ, subject to memory
capacity αM.
By this probabilistic coupling, each random schedule generated by Arandom(M) is paired with a
corresponding schedule generated by Arandom(αM), and each pair occurs with the same probability.
Now, since the coupled schedules preserve the same corresponding order and are optimally packed,
Proposition EC.1 implies that the total latency of each realization under Arandom(αM) is at most
α−1 times the corresponding latency under Arandom(M). Taking expectations over the coupling, we
conclude that the expected total latency under Arandom(αM) is at most α−1 times the expected
total latency under Arandom(M), namely,
E[TEL(o;Arandom(αM))] ≤α−1E[TEL(o;Arandom(M))].
(EC.7)
As Theorem EC.1 establishes the competitive ratio of Arandom(M), to bound the competitive
ratio of Amax, it suffices to compare the expected total latency between Amax and Arandom(M). By
Equation (EC.7), we can introduce Arandom(αM) as an intermediate benchmark. The next lemma
shows that it is easier to compare the expected total latency between Amax with memory capacity
M and Arandom(αM).
Lemma EC.1. Denote Arandom(αM) as Algorithm Arandom run with memory capacity αM, where
α = ℓ
u, then we have
E[TEL(o;Amax)] ≤E[TEL(o;Arandom(αM))].
(EC.8)
The proof of Lemma EC.1 can be found in Appendix EC.3. Combining the result in Lemma
EC.1 and Equation (EC.8), we are ready to give the competitive ratio upper bound to Amax, which
states in Theorem 1. The proof can also be found in Appendix EC.3.
EC.2. Supplementary Materials for Section 3
EC.3. Supplementary Materials for Section EC.1
Proof of Theorem EC.1
We split the proof into two parts, where in part 1, we show that the
worst case happens only if the output length of each oi is either ℓor u. In part 2, we prove for the
upper bound of the competitive ratio of Arandom.
—Part 1—
Without loss of generality, assume the output lengths are ordered as o1 ≤o2 ≤··· ≤on. We
analyze how the total end-to-end latency changes when the output length of a single request is
increased or decreased by one unit.


--- Page 36 ---
ec6
e-companion to xxx: Adaptively Robust LLM Inference Optimization under Prediction Uncertainty
Algorithm 3: H-SF
Input: Memory Capacity M, prompt requests o = (o1,o2,...,on).
Output: Processing Sequence: I(I0,I1,··· ,IT).
while there exist waiting requests do
Let Rt be the set of waiting prompts at time t. Let St be the set of tokens currently
processing at time t.
Find the set It ⊂Rt with largest cardinality such that Equation (1) hold for all t′ ≥t.
Process the requests in It ∪St and update Rt+1 = Rt\It.
end
First, denote π as the scheduling policy Arandom and fix an input vector o = (o1,...,on). Consider
decreasing the output length of job i from oi to oi −1, producing a new instance o−. Because the
job order remains unchanged, the start time of job i is unaffected. The main effect is that some
requests scheduled after job i may now complete earlier, potentially reducing their latency by one
unit.
Let F denote the set of jobs whose completion times are advanced by one unit due to this
reduction. Then, the total latency under o−satisfies
E[TEL(o−;π)] = E[TEL(o;π)] −1 −|F|.
Similarly, consider increasing oi to oi + 1, producing a new instance o+. Let B denote the set of
jobs whose completion times are delayed by one unit. Then,
E[TEL(o+;π)] = E[TEL(o;π)] + 1 + |B|.
Since the scheduling policy π does not know the true output lengths, and because jobs with the
same output size are symmetric under random permutations, we can pair the effects of decreasing
and increasing a job’s output length. Taking expectations yields:
E[TEL(o−;π)] + E[TEL(o+;π)] = 2E[TEL(o;π)].
Now consider the hindsight shortest-job-do-first’s latency TEL(o;H-SF) for each instance. Since
the hindsight policy knows all output lengths in advance, changing the output length of a single
job affects only the scheduling of subsequent jobs. Moreover, because jobs with smaller output
lengths consume less memory and are generally processed earlier under H-SF, reducing a job’s
output length tends to bring forward more jobs than increasing it delays. This gives
TEL(o−;H-SF) + TEL(o+;H-SF) ≤2TEL(o;H-SF).


--- Page 37 ---
e-companion to xxx: Adaptively Robust LLM Inference Optimization under Prediction Uncertainty
ec7
Using these observations, we can now compare the competitive ratios:
E[TEL(o;π)]
TEL(o;H-SF) ≤max
 E[TEL(o−;π)]
TEL(o−;H-SF), E[TEL(o+;π)]
TEL(o+;H-SF)

.
This inequality shows that either decreasing or increasing the output length of a single job by one
unit leads to a higher or equal competitive ratio.
Finally, suppose there exists an instance o ∈[ℓ,u]n achieving the supremum in the definition
of the competitive ratio. If there exists a job i such that ℓ< oi < u, then by perturbing oi to
oi −1 or oi + 1 (and reordering the jobs if necessary), we can produce an instance with a higher or
equal competitive ratio. Repeating this process finitely many times eventually produces an instance
where each oi ∈{ℓ,u}.
Therefore,
sup
o∈[ℓ,u]n
E[TEL(o;π)]
TEL(o;H-SF) =
sup
o∈{ℓ,u}n
E[TEL(o;π)]
TEL(o;H-SF),
completing the proof.
—Part 2—
By Part 1, suppose there are Nℓrequests with output length ℓand Nu requests with output
length u, where Nℓand Nu are large. By definition, the expected total latency under Algorithm
Arandom can be decomposed as
E[TEL(o;Arandom)] = NℓE[Li | oi = ℓ] + Nu E[Lj | oj = u],
where Li and Lj denote the end-to-end latencies of requests with output lengths ℓand u, respec-
tively.
Since Algorithm Arandom generates a random permutation and processes requests sequentially
while ensuring optimal packing, the schedule is optimally packed by construction. The total amount
of work, measured in terms of memory consumption over time, is upper bounded by
ℓ⌈Nℓ
⌊M
s+ℓ⌋⌉+ u⌈Nu
⌊M
s+u⌋⌉≤

1 + O( 1
M )
Nℓ
M ℓ(s + ℓ) + Nu
M u(s + u)

,
where M is the memory capacity, and s + ℓand s + u represent the peak memory footprint for
requests of type ℓand u, respectively.
Due to the symmetry of random sampling, each request of a given type has the same expected
latency. Furthermore, the expected latency for each request is at most half of the total processing
time for all requests, because in expectation, a request will be located near the middle of the
cumulative schedule. Thus, we have
E[Li | oi = ℓ], E[Lj | oj = u] ≤1
2

1 + O( 1
M )
Nℓ
M ℓ(s + ℓ) + Nu
M u(s + u)

.


--- Page 38 ---
ec8
e-companion to xxx: Adaptively Robust LLM Inference Optimization under Prediction Uncertainty
Next, consider the hindsight shortest-job-do-first’s total latency TEL(o;H-SF). Since the offline
scheduler knows all output lengths in advance and it can prioritize smaller jobs first, requests with
output length l have latency at least Nℓ
1
2ℓ⌊
Nℓ
⌈M
s+ℓ⌉⌋. Also, requests with output length u are subject
to a delay of ℓ⌊
Nℓ
⌈M
s+ℓ⌉⌋periods, and their latency is more than Nℓ

ℓ⌊
Nℓ
⌈M
s+ℓ⌉⌋+ 1
2u⌊
Nu
⌈M
s+u ⌉⌋

. The
corresponding total latency is at least
TEL(o;H-SF) ≥1
2

1 + O( 1
M )
−1 
l(s + ℓ)N 2
ℓ
M + 2l(s + ℓ)NℓNu
M
+ u(s + u)N 2
u
M

.
Taking the ratio, we obtain
E[TEL(o;Arandom)]
TEL(o;H-SF)
≤

1 + O( 1
M )

(Nℓ+ Nu)(Nℓℓ(s + ℓ) + Nuu(s + u))
l(s + ℓ)N 2
ℓ+ 2l(s + ℓ)NℓNu + u(s + u)N 2
u
.
Since α = ℓ/u, simplifying the expression yields
E[TEL(o;Arandom)]
TEL(o;H-SF)
≤

1 + O( 1
M )

1 +
1 −α2
2(α + α2)

= 1 + α−1
2
+ O( 1
M ).
Thus, the competitive ratio of Arandom is at most 1+α−1
2
+ O( 1
M ), as claimed.
□
Proof of Lemma EC.1
We first define a probabilistic coupling between Amax and Arandom(αM):
Definition EC.4 (Permutation Coupling for Amax and Arandom(αM)). Sample a random
permutation σ of [n] uniformly at random.
• Under Amax, schedule the jobs according to σ, treating all output lengths as u and applying
memory constraint M.
• Under Arandom(αM), schedule the jobs according to the same permutation σ, using the true
output lengths and memory constraint αM.
By construction, under this coupling, the two schedules share the same processing order.
Denote by IAmax the input sequence generated by Amax with memory M, and IArandom(αM) the
input sequence generated by Arandom(αM) with memory αM. We claim that, under the coupling,
IAmax ≤IArandom(αM),
meaning that every request in Amax is delayed to Arandom(αM). The definition of one schedule is
delayed to the other is defined in Definition EC.1.
The reasoning is as follows. At any time, if there remains sufficient memory (at least s+u) under
Amax, the algorithm immediately initiates a new request, treating it as requiring output length u.


--- Page 39 ---
e-companion to xxx: Adaptively Robust LLM Inference Optimization under Prediction Uncertainty
ec9
Since u is the worst-case (largest) output length and Amax batches jobs based on this conservative
estimate, the memory utilization rate per job under Amax is at least s+ℓ
s+u ≥α.
In contrast, under Arandom(αM), the algorithm uses the true output lengths, which can be as
small as ℓ, and thus may utilize the memory less aggressively. Therefore, Amax tends to process
more requests earlier compared to Arandom(αM). As a result, for every request, the number of
preceding completed jobs is at least as large under Amax as under Arandom(αM), leading to earlier
or equal start times.
Consequently, we have
TEL(o;IAmax) ≤TEL(o;IArandom(αM))
for each realization.
Taking expectations over the randomness in the permutation coupling, we conclude
E[TEL(o;Amax)] ≤E[TEL(o;Arandom(αM))],
which completes the proof.
□
Proof of Theorem 1
By Lemma EC.1, we have
E[TEL(o;Amax)] ≤E[TEL(o;Arandom(αM))].
(EC.9)
In addition, we define a probabilistic coupling between Arandom(M) and Arandom(αM).
Definition EC.5 (Permutation Coupling for Arandom(M) and Arandom(αM)). Sample
a
random permutation σ of [n] uniformly at random.
• Under Arandom(M), schedule the jobs according to the same permutation σ, using the true
output lengths and memory constraint M.
• Under Arandom(αM), schedule the jobs according to the same permutation σ, using the true
output lengths and memory constraint αM.
Denote by IArandom(M) the input sequence generated by Arandom(M) with memory M, and
IArandom(αM) the input sequence generated by Arandom(αM) with memory αM. By construction, we
see that they preserve the corresponding order and are both optimally packed. Recall Proposition
EC.1, we have
TEL(o;IArandom(M)) ≤α−1 TEL(o;IArandom(αM)).
(EC.10)
Taking expectations over the randomness in the permutation coupling, we arrive at
E[TEL(o;Arandom(αM))] ≤α−1E[TEL(o;Arandom(M))].


--- Page 40 ---
ec10
e-companion to xxx: Adaptively Robust LLM Inference Optimization under Prediction Uncertainty
Combining two results, we have
E[TEL(o;Amax)] ≤α−1E[TEL(o;Arandom(M))].
Taking the competitive ratio, we obtain
CR(Amax) ≤α−1CR(Arandom).
According to Theorem EC.1, the competitive ratio of the algorithm Amax satisfies
CR(Amax) ≤α−1CR(Arandom) ≤α−1(1 + α−1)
2
+ O( 1
M ),
(EC.11)
as M →+∞, which is the desired conclusion.
□
Algorithm 4: Auxiliary Arandom
Input: Memory Capacity M, prompt size s, output length o = (o1,o2,...,on).
Output: Processing sequence I = (I0,I1,··· ,IT).
Randomly generate a permutation (i1,...,in) of [n].
while there exist waiting requests do
Let Rt be the set of waiting prompts at time t. Let St be the set of tokens currently
processing at time t.
Following the permutation order, successively add requests from Rt into the memory
until no further addition is possible without violating Equation (3) at any t′ ≥t.
Let It ⊂Rt be the selected set of prompts.
Process the requests in It ∪St and update Rt+1 = Rt \ It.
end
EC.4. Supplementary Materials for Section 4
Proof of Theorem 3
We analyze the computational complexity of Algorithm Amin step by step.
First, at each time step, to select a batch of new requests, the algorithm sorts the available
prompts based on the auxiliary values ˜oi. Since the number of concurrent prompts is at most
proportional to the memory capacity M (specifically, at most M/s), sorting requires at most
O(M log M) operations.
Second, when verifying whether adding a request would violate the memory constraint, the algo-
rithm must compute the projected memory usage at the next time step. Simulating the projected
memory usage across all currently active requests requires O(M) time, since the number of active
requests is again at most proportional to M.


--- Page 41 ---
e-companion to xxx: Adaptively Robust LLM Inference Optimization under Prediction Uncertainty
ec11
Third, if a memory overflow is predicted, the algorithm must remove requests to bring memory
usage back within the constraint. Since requests are already maintained in sorted order by ˜oi,
selecting and removing jobs can be performed efficiently. The removal operation involves traversing
the sorted list and updating memory usage estimates, which takes at most O(M) time overall.
Finally, updates to the auxiliary variables ˜oi occur when output tokens are generated. These
updates involve only local modifications and require O(1) time per token, for a total of at most
O(M) operations per time step.
Combining all steps, the dominant operation per time step is the initial sorting, leading to an
overall computational complexity of O(M log M).
□
Proof of Theorem 4
We proceed by contradiction. Suppose there exists a feasible policy π that
achieves a strictly smaller competitive ratio than Amin. Then, there must exist a time t at which π
and Amin make different decisions for the first time.
We first consider the case in which they choose to add different requests. By definition, Amin
always adds the request with the smallest pseudo-output length ˜o. Suppose π chooses to start
request i, while Amin starts request j, with ˜oi > ˜oj. Then, from the property of conditional expec-
tation, we have
E[oi | oi ≥˜oi] > E[oj | oj ≥˜oj].
Let the completion time of request j under policy π be t′. If we modify the schedule so that request
j is started at time t and request i is delayed to complete at time t′, then the expected net change
in latency is
E[oi | oi ≥˜oi] −E[oj | oj ≥˜oj],
which is strictly positive, implying that such a switch would reduce total expected latency.
Next, consider the case in which they choose to delete different requests at time t. Again, recall
that Amin always deletes the request with the smallest ˜o. Suppose π deletes request i, and Amin
deletes request j, with ˜oi > ˜oj. This implies that request i has been processed for a longer time
than request j. In this case, we observe that
E[oi | oi ≥˜oi] −˜oi ≤E[oj | oj ≥˜oj] −˜oj.
Moreover, the deleted request must be restarted, resulting in increased latency. The expected
latency difference between the two policies is given by
((E[oj | oj ≥˜oj] −˜oj) + E[oi | oi ≥˜oi]) −((E[oi | oi ≥˜oi] −˜oi) + E[oj | oj ≥˜oj]) ≥0,


--- Page 42 ---
ec12
e-companion to xxx: Adaptively Robust LLM Inference Optimization under Prediction Uncertainty
which simplifies to ˜oi −˜oj > 0. Hence, switching the deletion from i to j would again yield a lower
expected latency.
Finally, we exclude the case in which only one of the two algorithms performs an action at time t,
while the other remains idle. By construction, Amin always acts whenever an insertion or deletion
is feasible, since its objective is to exploit as much useful information as possible under capacity
constraints. Hence, if policy π attempts an insertion earlier than Amin, the action cannot increase
information gain: the inserted request will either need to be deleted prematurely or will consume
memory without benefit. If, on the other hand, π inserts later than Amin, then idle slots arise and
the system is no longer optimally packed. Similarly, if π deletes a request earlier than Amin, then
the partially processed work is wasted and no additional information is gained; deleting later than
Amin exceeds the memory budget, which necessarily deteriorates performance.
Combining both cases, we reach a contradiction to the assumption that π outperforms Amin.
Therefore, we conclude that
E[TEL(o;Amin)] ≤E[TEL(o;π)].
Taking the ratio of both sides, it follows that
CR(Amin) ≤CR(π).
□
Proof of Lemma 1
Since in period τi all requests are treated as having a lower bound ˜o = i, the
symmetry among inputs with true length o = j implies that only a fraction s+i
s+j can be completed
without deletion. That is,
E[cij|bij] = s + i
s + j bij.
Similarly, for deletions, we obtain the following,
E[dij|bij] =

1 −s + i
s + j

bij.
□
Proof of Proposition 1
We proceed by induction. In the initial period τℓ, all requests are pro-
cessed directly, and the result follows from Lemma 1. In the next period τℓ+1, all remaining requests
with o = ℓ+ 1 must be completed, yielding E[cℓ+1,ℓ+1] = E[dℓ,ℓ+1] =
1
s+ℓ+1xℓ+1n.
Since the deletion operation preserves the distribution, Equation 4 implies that
E[cℓ+1,k] =
1
s + ℓ+ 1 · s + ℓ+ 1
s + k
xkn =
1
s + kxkn.
This suggests that in each period, E[cjj] =
1
s+jxjn, and the remaining requests with o = j + 1
satisfy E[cj+1,j+1] =
1
s+j+1xj+1n, which will be fully completed in the subsequent period. By the
distribution preserving property, the result follows.
□


--- Page 43 ---
e-companion to xxx: Adaptively Robust LLM Inference Optimization under Prediction Uncertainty
ec13
Proof of Corollary 1
We first calculate the numerator:
u
X
k=1
 u
X
i=k
ixi
! u
X
i=k
1
i xi + 2
u
X
i=k+1
i −k
i
xi
!
.
The coefficient of the cross term xixj (for i ̸= j) is
1
ij
min(i,j)
X
k=1
 i2(1 + 2j −2k) + j2(1 + 2i −2k)

= 1
ij
 min(i,j)(i2 + j2) + 2min(i,j)ij(i + j) −(i2 + j2)min(i,j)(min(i,j) + 1)

= min(i,j)2
ij
 (i + j)2 −2min(i,j)2
= 2aij.
In the case i = j, we have i2x2
i = aiix2
i .
For the denominator, the coefficient of xixj for i ̸= j is
2min(i,j)2 = 2bij,
and for i = j we have i2 = bii.
In conclusion, the competitive ratio can be written as a Rayleigh quotient,
CR(Amin) = max
⃗x
⃗x⊤Au⃗x
⃗x⊤Bu⃗x + O( 1
M ).
□
Proof of Lemma 3
We begin with Bu. It is known that the matrix M = (min(i,j))u×u is positive
definite. Since Bu is the Hadamard square of M, i.e., B = M◦M, and Hadamard powers of positive
definite matrices preserve positive definiteness, we conclude that Bu ≻0.
To prove that Au is also positive definite, we define a normalized matrix A′
u = (a′
ij)u×u,
a′
ij := 2
ij · aij −1.
A direct calculation shows:
a′
ij = 2 · min(i,j)
max(i,j) −
 min(i,j)
max(i,j)
2
.
Let rij := min(i,j)
max(i,j). Then:
a′
ij = 2rij −r2
ij = g(log i −log j),
where we use the identity
rij = exp(−|log i −log j|),


--- Page 44 ---
ec14
e-companion to xxx: Adaptively Robust LLM Inference Optimization under Prediction Uncertainty
and define
g(d) := 2e−|d| −e−2|d|.
We claim that g(d) is a positive definite function on R. Since g is even and continuous, by
Bochner’s theorem, it suffices to verify that its Fourier transform is nonnegative. Computing:
bg(ξ) =
Z ∞
−∞
g(d)e−iξd dd = 4 ·
1
1 + ξ2 −4 ·
1
4 + ξ2 =
12
(1 + ξ2)(4 + ξ2) > 0
∀ξ ∈R.
Hence g is positive definite.
By Schoenberg’s theorem, for any finite set of real numbers {z1,...,zu} ⊂R, the matrix (g(zi −
zj))u×u is positive semi-definite. Moreover, if the points zi are mutually distinct and g(0) > 0, then
the matrix is strictly positive definite.
It follows that the matrix
A′
u = (a′
ij)u×u = (g(log i −log j))u×u
is strictly positive definite.
Since Au = ij
2 (a′
ij + 1), and the factor ij
2 > 0, the transformation preserves positive definiteness.
Therefore, Au ≻0.
□
EC.5. Supplementary Materials for Section 5
Proof of Theorem 6
Let ⃗x = (xℓ,0,...,0,xu) with 0 < ℓ< u and set s = 0. Since xk = 0 unless
k ∈{ℓ,u}, Theorem 2 yields
num = ℓ(ℓxℓ+ uxu)

xℓ+ ℓ
uxu + 2u −ℓ
u
xu

+ u2xu
u −ℓ
u
2
xu,
den = ℓ2x2
ℓ+ 2ℓ2xℓxu + u2x2
u.
Set α = ℓ/u ∈(0,1) and t = xℓ/xu (with t ≥0). Dividing numerator and denominator yields:
CR(Amin) = 1 +
α(1 −α2)t
α2t2 + 2α2t + 1 + O
  1
M

.
Since (αt −1)2 ≥0 implies α2t2 + 2α2t + 1 ≥2α(1 + α)t, we obtain:
α(1 −α2)t
α2t2 + 2α2t + 1 ≤1 −α
2
(∀t ≥0).
Therefore,
CR(Amin) ≤1 + 1 −α
2
+ O
  1
M

= 3 −α
2
+ O
  1
M

,
which completes the proof.
□


--- Page 45 ---
e-companion to xxx: Adaptively Robust LLM Inference Optimization under Prediction Uncertainty
ec15
Algorithm 5: Revised Amin under Multiple Prediction Intervals
Input: Memory capacity M, prompt size s, output length lower bounds
ℓj = max{ℓk|ℓk ≤oi,k ∈[m]} for all i ∈[n].
Output: Processing sequence I = (I0,I1,··· ,IT).
Initialize ˜oi ←ℓj for all i ∈[n].
while there exist unfinished requests do
Let Rt be the set of waiting prompts at time t. Let St be the set of tokens currently
processing at time t.
if projected memory usage of St at time t + 1 exceeds M then
Sort St in ascending order of ˜oi; break ties uniformly at random.
Remove jobs one by one from St (following the sorted order) until projected memory
usage at t + 1 satisfies the memory constraint.
For each removed request i, update ˜oi ←number of tokens already generated by
request i.
end
Let S′
t be the set of remaining active requests after any removals.
Sort Rt in ascending order of ˜oi; break ties uniformly at random.
Select the largest subset It ⊂Rt (in order) such that adding It to S′
t satisfies the
memory constraint in Equation (3).
Process the requests in It ∪S′
t.
Update Rt+1 = Rt \ It.
end
Proof of Theorem 7
Under Aℓ, the request of length ℓis executed first and thus waits exactly
ℓbatches; its average output token is generated at time 1
2ℓ. The request of length u is delayed by
all ℓtokens of the first request, in addition to its own u tokens, resulting in an average completion
time of ℓ+ u/2. Multiplying these average sojourn times by the per-batch workload (xℓ+ xu) and
then by the corresponding request sizes gives:
TEL(o;Aℓ) =

ℓ( 1
2ℓ)(xℓ+xu)

xℓ+

(ℓ+ 1
2u)(xℓ+xu)

xu = u2
2
h
α2x2
ℓ+3α2xℓxu +(2α2 +1)x2
u
i
, (1)
where α := ℓ/u ∈(0,1).
In comparison, the Hindsight-Shortest First benchmark (H-SF) schedules every token as soon as
capacity permits:
TEL(o;H-SF) = u2
2
h
α2x2
ℓ+ 2α2xℓxu + x2
u
i
.
(2)


--- Page 46 ---
ec16
e-companion to xxx: Adaptively Robust LLM Inference Optimization under Prediction Uncertainty
Algorithm 6: Aℓ
Input: Memory capacity M, prompt size s, output lengths oi ∈{ℓ,u} for all i ∈[n].
Output: Processing sequence I = (I0,I1,··· ,IT).
Initialize ˜oi ←ℓfor all i ∈[n].
while there exist unfinished requests do
Let Rt be the queue of waiting prompts at time t. Let St be the set of tokens currently
processing.
For any i ∈St with more than ℓtokens but not yet finished, remove i from St, set
˜oi ←u, and move i to the end of Rt.
Let S′
t be the active set after updates.
Starting from the front of Rt, successively add requests to a set It as long as It ∪S′
t
satisfies the memory constraint, using ˜oi for each i ∈It.
Activate and process the requests in It; update Rt+1 = Rt \ It.
end
Taking the ratio gives:
CR(Aℓ) = 1 +
α2xℓxu + 2α2x2
u
α2x2
ℓ+ 2α2xℓxu + x2
u
+ O
  1
M

.
(3)
To bound the fractional term, let t := xℓ/xu ≥0 and define:
h(t) =
α2t + 2α2
α2t2 + 2α2t + 1.
For 0 ≤α ≤1
2, the inequality α2t2 + 2α2t + 1 ≥2α(1 + α)t implies h(t) ≤α/[2(1 −α)] for all t ≥0.
For 1
2 ≤α ≤1, h(t) is maximized at t = 0, yielding h(t) ≤2α2. Substituting these bounds produces
the desired piecewise estimate:
CR(Aℓ) ≤







1 +
α
2(1 −α),
0 ≤α ≤1
2,
1 + 2α2,
1
2 ≤α ≤1,
+ O
  1
M

,
completing the proof.
□
Proof of Theorem 8
We use the result of Theorem 2 to compute the denominator:
∞
X
k=1
k3qk−1
 
2kqk−1 +
∞
X
i=k+1
iqi−1
!
= 1 + 2q + 11q2 + 8q3 + 11q4 + 2q5 + q6
(1 −q)6(1 + q)4
,
and the numerator:
∞
X
k=1
 ∞
X
i=k
i2qi−1
! ∞
X
i=k
qi−1 + 2
∞
X
i=k+1
(i −k)qi−1
!
= 1 + 3q + 6q2 + 3q3 + q4
(1 −q)6(1 + q)2
.


--- Page 47 ---
e-companion to xxx: Adaptively Robust LLM Inference Optimization under Prediction Uncertainty
ec17
Thus, we have
CR(Amin) =
(1 + q)2(1 + 3q + 6q2 + 3q3 + q4)
1 + 2q + 11q2 + 8q3 + 11q4 + 2q5 + q6 ≤14
9 ≈1.56.
□
