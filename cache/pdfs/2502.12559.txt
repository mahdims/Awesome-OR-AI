--- Page 1 ---
Distributed On-Device LLM Inference With
Over-the-Air Computation
Kai Zhang, Hengtao He, Shenghui Song, Jun Zhang, Fellow, IEEE, and Khaled B. Letaief, Fellow, IEEE
Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology.
Email: kzhangbn@connect.ust.hk, {eehthe, eeshsong, eejzhang, eekhaled}@ust.hk
Abstractâ€”Large language models (LLMs) have achieved re-
markable success across various artificial intelligence tasks. How-
ever, their enormous sizes and computational demands pose sig-
nificant challenges for the deployment on edge devices. To address
this issue, we present a distributed on-device LLM inference
framework based on tensor parallelism, which partitions neural
network tensors (e.g., weight matrices) of LLMs among multiple
edge devices for collaborative inference. Nevertheless, tensor
parallelism involves frequent all-reduce operations to aggregate
intermediate layer outputs across participating devices during
inference, resulting in substantial communication overhead. To
mitigate this bottleneck, we propose an over-the-air computa-
tion method that leverages the analog superposition property
of wireless multiple-access channels to facilitate fast all-reduce
operations. To minimize the average transmission mean-squared
error, we investigate joint model assignment and transceiver
optimization, which can be formulated as a mixed-timescale
stochastic non-convex optimization problem. Then, we develop
a mixed-timescale algorithm leveraging semidefinite relaxation
and stochastic successive convex approximation methods. Compre-
hensive simulation results will show that the proposed approach
significantly reduces inference latency while improving accuracy.
This makes distributed on-device LLM inference practical for
resource-constrained edge devices.
Index Termsâ€”6G, distributed inference, large language models,
over-the-air computation, tensor parallelism.
I. INTRODUCTION
Large language models (LLMs) have achieved remarkable
success in various fields of artifical intelligence (AI), such
as natural language processing [1] and embodied intelligence
[2]. However, most existing LLMs rely on cloud-based in-
frastructure due to their enormous computational and mem-
ory requirements, which pose significant challenges for the
deployment on edge devices. This cloud-based approach raises
critical concerns regarding scalability and privacy, particularly
in sensitive fields like healthcare and finance. To address
these limitations, distributed LLM inference has recently been
proposed as a promising solution, which distributes inference
workloads across multiple devices [3]. This strategy reduces
the burden on the individual device and strengthens privacy
protections. Furthermore, the advanced communication capa-
bilities of 5G and future 6G wireless networks have made
distributed LLM inference increasingly promising for real-time
applications [4], [5].
The performance of distributed LLM inference systems is
largely determined by communication overhead. To enhance
the communication efficiency of distributed LLM inference,
several recent studies have been conducted [6]â€“[10]. In [6],
the authors proposed a collaborative edge computing method
for distributing different layers of LLMs across the edge
device and cloud server, and developed a device selection
and model partitioning algorithm to reduce inference latency
and optimize the throughput. In [7], an active inference-based
method was proposed to address the task offloading and re-
source allocation problem for LLM inference in cloud-edge
computing frameworks. Similarly, the authors of [8] proposed
a reinforcement learning algorithm that optimizes the splitting
point of LLMs between the edge device and cloud server
to reduce the communication overhead. Furthermore, task-
oriented communications have been utilized to optimize end-
to-end inference throughput, accuracy, and latency, which can
further enhance the communication efficiency of distributed
LLM inference [9], [10].
Despite significant advances in distributed LLM inference,
existing works [6]â€“[10] primarily focus on the device-cloud
collaborative inference, which faces scalability challenges due
to the reliance on the centralized cloud server. To address this
limitation, distributed on-device LLM inference with tensor
parallelism has recently been proposed [11]. This approach
divides neural network tensors (e.g., weight matrices) of LLMs
into smaller segments and distributes them across multiple edge
devices. However, a critical challenge in tensor parallelism is
the frequent all-reduce operations required to aggregate inter-
mediate outputs across devices, which can cause substantial
latency in practical wireless networks and hinder the real-time
inference.
To solve the problem, we present a communication-efficient
framework for distributed on-device LLM inference in this
paper. By exploiting the analog superposition property of
wireless multiple-access channels, we propose an over-the-air
computation method to facilitate fast all-reduce operations in
tensor parallelism. The performance of the distributed LLM
inference is highly determined by the transmission error dur-
ing the over-the-air computation. To minimize the average
transmission mean-squared error (MSE) with limited energy
supply for edge devices, we investigate a joint model assign-
ment and transceiver design problem, which can be formu-
lated as a mixed-timescale stochastic non-convex optimiza-
tion. Specifically, the model assignment is determined at the
beginning of inference based on long-term statistical channel
state information (CSI), while the transceiver design adapts
dynamically to the CSI. To solve the problem, we develop
a mixed-timescale algorithm leveraging semidefinite relaxation
(SDR) and stochastic successive convex approximation (SCA)
methods. Extensive simulation results demonstrate that the
proposed approach effectively reduces inference latency and
improves inference accuracy.
Notations: Column vectors and matrices are denoted by
arXiv:2502.12559v1  [cs.DC]  18 Feb 2025


--- Page 2 ---
ğŠà¯‡
ğ‘¸à¯‡
ğ•à¯‡
ğŠà¬µ
ğà¬µ
ğ•à¬µ
Device 1
CPU
RAM
SSD
Device ğ‘µ
CPU
RAM
SSD
(1) User initiates a task request.
Edge Server
(2) Edge server detects available devices 
and allocates model dynamically. 
(3) Load model segments
(4) Perform forward computation
(5) All-reduce operation
. . .
ğ—
(a) MLP
(b) Self-Attention
ğ—
ğ—
ğ—ğ–à¬µ
ğ—ğ–à¯‡
ReLU
ğ˜à¯‡
ğ˜à¬µğ”à¬µ
ğ˜à¯‡ğ”à¯‡
ğ™à¬µ
ğ™à¯‡
All-reduce
ğ™
Device 1
Device ğ‘µ
ğ—
ğ—
ğ—
ğ˜à¬µ
Softmax
ğ˜à¬µ
Softmax
ğ˜à¯‡
ğ˜à¬µğ”à¬µ
ğ˜à¯‡ğ”à¯‡
ğ™à¬µ
ğ™à¯‡
All-reduce
Device 1
Device ğ‘µ
ReLU
ğ™
Fig. 1. An illustration of the distributed on-device LLM inference system, showing the system workflow and visualizing tensor
parallelism for (a) MLP and (b) self-attention layers.
boldface lowercase and boldface capital letters, respectively.
CMÃ—N represents the space of the M Ã— N complex-valued
matrices. (Â·)T and (Â·)H stand for the transpose and the conjugate
transpose of their arguments, respectively. tr(A) denote the
trace of matrix A. E[Â·] denotes the expectation operation. âˆ‡
represents the gradient operator.
II. SYSTEM MODEL AND PROBLEM FORMULATION
In this section, we first elaborate on the distributed on-device
LLM inference system, followed by the proposal of an over-the-
air computation approach to accelerate all-reduce operations
during inference. Subsequently, we formulate a joint model
assignment and transceiver design problem to minimize the
average transmission MSE.
A. Proposed Distributed On-Device LLM Inference System
To deploy LLMs on resource-limited edge devices, dis-
tributed on-device inference with tensor parallelism has been
proposed. This method involves partitioning neural network
tensors (e.g., weight matrices) of LLMs into smaller segments
and distributing them across multiple devices for simultaneous
processing. The complete workflow of the distributed on-device
LLM inference system is illustrated in Fig. 1. When a device
initiates an inference request, the edge server dynamically iden-
tifies available local devices and partitions the model parame-
ters. Then, each device loads its assigned model segment into
memory and performs forward computation. After each layer
of LLM is computed, an all-reduce operation aggregates the
intermediate outputs from all devices, ensuring synchronization
and consistency across devices during inference.
LLMs are primarily built on the Transformer architecture,
which typically consists of dozens of Transformer layers. Each
Transformer layer includes a self-attention mechanism and a
multi-layer perceptron (MLP). To achieve efficient distributed
inference, tensor parallelism partitions both the self-attention
layer and the MLP layer of each Transformer block into smaller
tensor segments, as shown in Fig. 1. For a typical 2-layer MLP
within the Transformer block, the forward computation involves
two main linear transformations, separated by a non-linear
activation function (e.g., ReLU or GeLU). Mathematically, it
can be expressed as follows,
Z = max(0, XW)U,
(1)
where X is the input to the MLP layer, Z is the output,
and W and U are the weight matrices, respectively. The
traditional centralized inference involves loading the entire
weight matrices W and U into memory and performing full
matrix multiplications on a single device, which is usually
impractical for resource-limited edge devices. To overcome this
challenge, tensor parallelism distributes the weight matrices W
and U across N devices, as illustrated below,
W = [W1, . . . , WN],
U = [UT
1 , . . . , UT
N]T,
(2)
where Wn and Un represent the portions of the weight
matrices assigned to device n. Then, each device performs
the forward computation on its respective segment. The input
tensor X for the MLP layer is first broadcasted to all devices,
and each device n computes its local contribution as follows,
Zn = max(0, XWn)Un,
(3)
where Zn is the partial output produced by device n. Once all
devices obtain their local outputs Zn, an all-reduce operation
is performed to aggregate the partial outputs from all devices
as follows,
Z =
N
X
n=1
Zn.
(4)
After aggregation, the final output Z of the MLP layer is broad-
casted to all devices, ensuring synchronization and consistency
across devices for the subsequent layerâ€™s computation.
For the self-attention layer, tensor parallelism similarly par-
titions the query (Q), key (K), and value (V) matrices across
devices. Since the computation in the self-attention layer shares
a similar matrix-multiplication structure to the MLP layer, the
partitioning, local computation, and aggregation steps follow
the same principles. A similar all-reduce operation is required
to gather and combine the partial outputs from devices.
2


--- Page 3 ---
B. Over-the-Air All-Reduce
Employing tensor parallelism for distributed inference re-
quires frequent all-reduce operations, which causes significant
latency in practical wireless networks. To address this issue,
we propose an over-the-air computation approach to accelerate
the all-reduce steps. Over-the-air computation aggregates dis-
tributed data efficiently by leveraging signal superposition in
multiple-access channels, allowing simultaneous transmissions
to compute nomographic functions (e.g., arithmetic mean) [12].
In the distributed LLM inference system, the aggregation of
partial intermediate outputs across devices aligns with this
operation, making over-the-air computation suitable to mitigate
the communication overhead.
Specifically, we assume the edge server and each edge
device are equipped with Nr and Nt antennas, respectively,
constructing a MIMO multiple-access channel. We consider
the block-fading channel model, where channel statistics re-
main constant throughout the inference process, with channel
states varying independently across different time intervals.
Let sn = [sn,1, . . . , sn,L]T denote the per-round transmitted
L entries of device nâ€™s intermediate output, where the com-
plete intermediate output has a dimensionality of L0. Given
synchronized symbol boundaries, all devices transmit their
intermediate outputs simultaneously. To mitigate the distortion
of received signals caused by channel noise at the server,
aggregation beamforming is applied. Let A âˆˆCNrÃ—L and
Bn âˆˆCNtÃ—L denote the aggregation beamforming matrix at
the edge server and the data precoding matrix at device n,
respectively. Then, the received signal at the server after the
over-the-air computation is given by,
Ë†s = AH
N
X
n=1
HnBnsn + AHn,
(5)
where Hn âˆˆCNrÃ—Nt denotes the uplink MIMO channel from
device n to the edge server, and n âˆ¼CN
 0, Ïƒ2
zI

denotes the
additive white Gaussian noise vector with Ïƒ2
z being the noise
power. The distortion of Ë†s with respect to the desired target
vector s = PN
n=1 sn is measured by the MSE, defined as
MSE(Ë†s, s) = E

(Ë†s âˆ’s)T(Ë†s âˆ’s)

.
(6)
The MSE serves as a metric to evaluate the performance
of the all-reduce operation with over-the-air computation. By
substituting (5) into (6), the MSE can be explicitly represented
as a function of transceiver beamforming matrices as follows,
MSE(A, {Bn})
=
N
X
n=1
tr
 AHHnBn âˆ’I
 AHHnBn âˆ’I
H
+Ïƒ2
ztr
 AHA

.
(7)
Edge devices involved in inference tasks typically have
limited energy supply. Thus, we assume that for each device
n, the energy consumption for both the forward computation
of each LLM layer and the transmission of the intermediate
output cannot exceed the maximum power budget P max
n
. To
model the computation energy consumption, we first introduce
a model assignment vector m = [m1, . . . , mN], which enables
more flexible and efficient model distribution based on device
capabilities (e.g., memory size and compute power). The entry
mn âˆˆ[0, 1] of m represents the proportion of the model
allocated to device n. Consequently, the computation energy
consumption for device n is given by enmnstot, where en
denotes the device-specific energy coefficient that reflects the
energy cost associated with accessing and processing each
weight during computation, and stot is the number of parameters
(weights) for each layer. The communication energy consump-
tion of device n can be derived as L0
L tr
 BnBH
n

. Accordingly,
the energy constraint is given by
enmnstot + L0
L tr
 BnBH
n

â‰¤P max
n
, âˆ€n.
(8)
C. Problem Formulation
In the proposed distributed LLM inference system, the over-
all performance depends on the model assignment policy m
and the transceiver beamformers, A and {Bn}. The transceiver
optimization focuses on minimizing signal misalignment errors
while suppressing noise, and thus it adapts dynamically to in-
stantaneous CSI, making it a fast-timescale variable. However,
real-time adaptation of model assignment to instantaneous CSI
is impractical due to the high latency caused by loading model
segments. Thus, model assignment should be determined at the
start of the inference process and should depend on long-term
channel statistics, functioning as a slow-timescale decision. The
resulting problem is a mixed-timescale joint optimization of
short-term transceiver beamformers A, {Bn} and long-term
model assignment m, with the goal of minimizing the average
MSE as follows,
P : min
m
EH

min
A,{Bn} MSE(A, {Bn})

s.t. enmnstot + L0
L tr
 BnBH
n

â‰¤P max
n
, âˆ€n,
mT1 = 1, m â‰¥0,
(9)
where the expectation EH [Â·] is taken over all random channel
realizations H = {Hn}N
n=1. Problem P is challenging to solve
for three reasons: 1) the inherent non-convexity caused by the
coupling between transceiver beamformers, 2) the presence of
expectations over the random channel state in the objective
function, and 3) the interdependence between the short-term
beamforming matrices and the long-term model assignment
policy within the per-device power constraints. To address these
issues, we develop an efficient mixed-timescale algorithm in the
following section.
III. ALGORITHM DEVELOPMENT
In this section, we develop a mixed-timescale algorithm to
solve the joint model assignment and transceiver optimization
problem P. We start by decomposing problem P into a family
of short-term subproblems and a long-term subproblem as
follows.
1) Short-term transceiver optimization for given model as-
signment policy m and channel condition H:
Ps :
min
A,{Bn} MSE(A, {Bn})
s.t.
enmnstot + L0
L tr
 BnBH
n

â‰¤P max
n
, âˆ€n.
(10)
3


--- Page 4 ---
2) Long-term model assignment optimization based on the
optimal solution Aâˆ—(m), {Bâˆ—
n(m)} to problem Ps:
Pl : min
m
EH [MSE(Aâˆ—(m), {Bâˆ—
n(m)})]
s.t. enmnstot + L0
L tr
 Bâˆ—
n(m)Bâˆ—
n(m)H
â‰¤P max
n
, âˆ€n,
mT1 = 1, m â‰¥0.
(11)
The short-term problem Ps remains non-convex, which we
address using the SDR technique. The long-term model as-
signment problem Pl is similarly challenging, as the optimal
Aâˆ—(m), {Bâˆ—
n(m)} cannot be derived in closed form. Addition-
ally, the distribution of the channel state is difficult to obtain
in practical wireless systems. To address these challenges, we
propose a stochastic SCA algorithm that operates without re-
quiring prior knowledge of the channel state distribution. In the
following subsections, we provide a detailed implementation of
the proposed algorithms.
A. Short-Term Transceiver Optimization for Ps
We first simplify problem Ps by demonstrating that the zero-
forcing (channel inversion) precoder is optimal conditioned on
the aggregation beamformer.
Lemma 1. For a given aggregation beamformer A, the trans-
mission MSE is minimized by using the zero-forcing precoders
as follows,
Bâˆ—
n =
 AHHn
H  AHHnHH
nA
âˆ’1 , âˆ€n.
(12)
Proof. Lemma 1 can be proved by following the same steps
as in [13, Appendix A]. The detailed proof is omitted due to
space limitation.
Let G represent the normalized aggregation beamformer that
satisfies tr(GGH) = 1, and consequently A = âˆšÎ±G with Î±
denoting the norm of A. By substituting (12), problem Ps can
be reformulated as follows,
min
Î±,G
Î±
s.t. enmnstot + L0
Î±Ltr
 GHHnHH
nG
âˆ’1
â‰¤P max
n
, âˆ€n,
tr
 GGH
= 1.
(13)
Problem (13) remains challenging to solve due to its non-
convex constraints involving the term tr((GHHnHH
nG)âˆ’1). To
address the problem, we develop a tractable approximation of
the problem by employing the following inequality,
tr
 GHHnHH
nG
âˆ’1
â‰¤
L
Î»min (HHnGGHHn),
(14)
where the equality holds when the channel is well-conditioned,
i.e., the singular values of Hn are identical. By utilizing (14),
we reformulate an approximated version of problem (13) as
follows,
min
Î±,G
Î±
s.t.
L0
Î±Î»min (HHnGGHHn) â‰¤P max
n
âˆ’enmnstot, âˆ€n,
tr
 GGH
= 1.
(15)
Then, by introducing a new variable Ë†G = GGH, an equivalent
formulation of problem (15) is obtained as follows,
min
Î±, Ë†G
Î±
s.t.
L0
Î±Î»min

HHn Ë†GHn
 â‰¤P max
n
âˆ’enmnstot, âˆ€n,
tr( Ë†G) = 1, rank( Ë†G) = L, Ë†G âª°0.
(16)
We observe that the only non-convex constraint in problem (16)
is rank( Ë†G) = L. Therefore, we remove this constraint to obtain
a relaxed version of problem (16) as follows,
min
Î±, Ë†G
Î±
s.t.
L0
Î±Î»min

HHn Ë†GHn
 â‰¤P max
n
âˆ’enmnstot, âˆ€n,
tr( Ë†G) = 1, Ë†G âª°0.
(17)
Then, problem (17) can be proved to be a convex problem.
After solving problem (17) using a convex solver (e.g., the
CVX toolbox in MATLAB) and obtaining the globally optimal
solution Ë†Gâˆ—, we apply the Gaussian randomization algorithm
[14] to map the solution to a feasible, near-optimal solution for
the original non-convex problem.
B. Long-Term Model Assignment Optimization for Pl
The long-term model assignment problem Pl is intractable
since the optimal Aâˆ—(m) and {Bâˆ—
n(m)} cannot be derived
in closed form. Additionally, the channel state distribution is
difficult to obtain in practical wireless systems. To address these
challenges, we employ a stochastic SCA algorithm that solves
problem Pl recursively without requiring prior knowledge of
the channel state distribution. For clearer algorithmic descrip-
tion, we first reformulate the long-term problem Pl into an
equivalent form as follows,
min
m
f0(m) = EH [MSE(Aâˆ—(m), {Bâˆ—
n(m)})]
s.t. f1(m) = stotdiag(emT) + L0
L ec (m) â‰¤pmax,
mT1 = 1, m â‰¥0,
(18)
where
ec(m)=[tr
 Bâˆ—
1(m)(Bâˆ—
1(m))H
, . . . , tr
 Bâˆ—
N(m)(Bâˆ—
N(m))H
]T,
pmax = [P max
1
, . . . , P max
N ]T, and e = [e1, . . . , eN]T. The pro-
posed stochastic SCA algorithm iteratively performs the follow-
ing steps: First, quadratic surrogate functions are constructed
to approximate the non-convex components of the problem.
Then, the resulting convex quadratic approximation is solved,
and the long-term model assignment policy is updated based
on the solution. The details of these two steps are explained as
follows.
1) Step 1: In each iteration Ï„, the edge server first generates
a channel sample HÏ„, and then calculates the short-term
transceiver beamformers Aâˆ—(mÏ„) and {Bâˆ—
n(mÏ„)} by solving
the short-term problem Ps. To address the channel randomness
in the objective function f0(m), we approximate the expected
MSE by using the MSE computed from a specific channel real-
ization. Specifically, in the Ï„-th iteration, f0(mÏ„) is evaluated
4


--- Page 5 ---
Algorithm 1: Mixed-Timescale Model Assignment and
Transceiver Optimization Algorithm
1 Initialize: Model assignment policy m0 and iteration
index Ï„ = 0;
2 Step 1 (long-term model assignment optimization at the
beginning of inference task)
3 Obtain a channel sample HÏ„ and calculate the short-term
transceiver beamformers Aâˆ—(mÏ„), {Bâˆ—
n(mÏ„)} by solving
the short-term problem Ps;
4 Update the surrogate function Ë†f Ï„
i (m) according to (19);
5 Solve problem (21) to obtain the optimal Ë†mÏ„ and update
mÏ„ according to (22);
6 Let Ï„ = Ï„ +1 and return to Step 1. Repeat the above steps
until convergence;
7 Step 2 (short-term transceiver optimization at each all-
reduce step):
8 Obtain the channel condition H and apply the short-term
algorithm to solve the optimal transceiver beamformers
with the determined model assignment policy m.
as MSE(Aâˆ—(mÏ„), {Bâˆ—
n(mÏ„)}) using the given channel sample
HÏ„. Then, the recursive convex approximations of the original
objective function f0(m) and the power constraint function
f1(m) are defined as follows,
Ë†f Ï„
i (m) = fi(mÏ„) + (uÏ„
i )T (mâˆ’mÏ„) + Î·i âˆ¥m âˆ’mÏ„âˆ¥2 ,
âˆ€i âˆˆ{0, 1}, (19)
where Î·i is a constant that ensures convexity. Furthermore, uÏ„
i
is an approximation of the gradient âˆ‡fi(mÏ„), which is updated
recursively as follows,
uÏ„
i = (1 âˆ’ÏÏ„)uÏ„âˆ’1
i
+ ÏÏ„âˆ‡mfi(m; Aâˆ—(mÏ„), {Bâˆ—
n(mÏ„)}).
(20)
The sequence ÏÏ„ is decreasing in Ï„, satisfying limÏ„â†’âˆÏÏ„ = 0,
Pâˆ
Ï„=0 ÏÏ„ = âˆ, and Pâˆ
Ï„=0(ÏÏ„)2 < âˆ.
2) Step 2: After obtaining the convex approximation of the
objective function and the constraint function, we formulate a
convex approximation of the original problem as follows:
Ë†mÏ„ = min
m
Ë†f Ï„
0 (m)
s.t.
Ë†f Ï„
1 (m) â‰¤pmax,
mT1 = 1, m â‰¥0.
(21)
After solving for Ë†mÏ„, the model assignment policy is updated
as follows,
mÏ„+1 = (1 âˆ’Î³Ï„)mÏ„ + Î³Ï„ Ë†mÏ„,
(22)
where Î³Ï„ âˆˆ(0, 1) satisfies limÏ„â†’âˆÎ³Ï„ = 0, Pâˆ
Ï„=0 Î³Ï„ = âˆ,
and Pâˆ
Ï„=0(Î³Ï„)2 < âˆ.
The above two steps iterate until convergence, and the overall
process is outlined in Algorithm 1. Moreover, the convergence
of the stochastic SCA algorithm has been rigorously analyzed
in [15].
IV. SIMULATION RESULTS
A. Simulation Setups
1) LLM Inference Model Setting: In the simulations, N
virtual machines (VMs) are set up on a single desktop, with
each VM simulating a distinct edge device. For evaluation,
we utilize the LLaMA2 and LLaMA3 models, along with
the WikiText-2 dataset. The primary performance metric for
inference accuracy is perplexity, a widely recognized measure
of a LLMâ€™s capability to predict the next word in a sequence.
It is defined mathematically as follows,
Perplexity = exp
 
âˆ’1
Ltxt
Ltxt
X
k=1
log P (wk |w1, . . . , wkâˆ’1)
!
,
(23)
where P (wk | w1, . . . , wkâˆ’1) is the modelâ€™s predicted proba-
bility for the next word, and Ltxt is the text length. Lower per-
plexity values indicate better inference performance, reflecting
the modelâ€™s accuracy in generating subsequent tokens.
2) Communication Model Setting: The antenna number at
the edge server is Nr = 20, and each edge device has Nt = 4
antennas. The bandwidth between the edge server and edge
devices is B = 10 MHz. The uplink channels are assumed to
be independent and identically distributed (i.i.d.) Rician fading,
modeled as i.i.d. complex Gaussian random variables with non-
zero mean Âµ = 1 and variance Ïƒ2 = 1. Moreover, the maximum
power budget is set as P max
n
= 1 and the noise variance at the
edge server is assumed to be 1.
B. Performance Evaluation
We compare the performance of the proposed air all-reduce
approach with the following two benchmark schemes.
â€¢ Digital All-Reduce: All devices upload intermediate layer
outputs using a traditional broadband digital multi-access
scheme, with each transmitted symbol quantized to Q = 8
bits. To prevent multi-user interference, orthogonal fre-
quency division multiple-access (OFDMA) is used, assign-
ing each sub-channel to one device.
â€¢ Uncoded FDMA: This scheme similarly employs the
OFDMA technique, with each device occupying a ded-
icated sub-channel to upload intermediate layer outputs in
an uncoded analog manner.
In Fig. 2, we employ the LLaMA3 model with 8 billion
parameters to compare the inference performance of different
algorithms, encompassing three key performance metrics: trans-
mission MSE, perplexity, and average generation time. In Fig.
2(a), the proposed air all-reduce approach consistently achieves
low MSE across all device counts, significantly outperforming
the uncoded FDMA scheme, which exhibits a near-linear
increase in MSE as the number of devices grows. The digital
all-reduce method achieves near-zero MSE, but it will incur
higher time costs (discussed later). In Fig. 2(b), perplexity
follows the same trend as the transmission MSE. The air
all-reduce method maintains stable, low perplexity across all
device configurations, while the perplexity of uncoded FDMA
rises sharply with more devices. Digital all-reduce performs
similarly to air all-reduce, maintaining low perplexity. For
the average generation time as shown in Fig. 2(c), air all-
reduce offers the lowest latency among the three methods,
5


--- Page 6 ---
Fig. 2. The MSE, perplexity, and average generation time versus the number of edge devices
Average generation time per token (ms)
Model
LLaMA2-7B
LLaMA2-13B
LLaMA2-70B
LLaMA3-70B
Device Number
1
2
4
8
1
2
4
8
1
2
4
8
1
2
4
8
Digital All-Reduce
114.2
85.2
79.5
108.3
217.3
174.0
176.6
261.4
N/Aâˆ—
807.3
729.7
981.6
N/A
893.2
783.8
1033.6
Air All-Reduce
114.2
69.7
45.7
37.8
217.3
128.5
81.3
66.4
N/A
660.9
423.0
354.2
N/A
746.8
477.1
406.0
*: Not available due to insufficient memory.
TABLE I. Average generation time for different models across varying device numbers, with the shortest average generation
time for each model being highlighted in bold.
particularly as the number of devices increases. Digital all-
reduce has significant latency increases with more devices,
while uncoded FDMA shows moderate latency improvements
but still lags behind air all-reduce. Overall, air all-reduce strikes
a balance between low latency and high inference accuracy,
outperforming both benchmarks in practical wireless scenarios.
To further validate the effectiveness of the proposed algo-
rithm, we conduct additional experiments using larger models,
including LLaMA2 with 7, 13, and 70 billion parameters, and
LLaMA3 with 70 billion parameters. In Table I, it can be
observed that the proposed air all-reduce method consistently
outperformed digital all-reduce in terms of inference speed.
Across various device configurations, air all-reduce achieves
up to 3x faster generation time, demonstrating its significant
advantages for distributed LLM inference, especially with
large-scale models.
V. CONCLUSION
In this paper, we investigated communication-efficient dis-
tributed on-device LLM inference over wireless networks. We
proposed an over-the-air computation approach to acceler-
ate the frequent all-reduce operations required in the tensor
parallelism-based distributed inference. To minimize the aver-
age transmission MSE, we formulated a joint model assignment
and transceiver design problem, which can be derived as a
mixed-timescale stochastic non-convex optimization. We devel-
oped a mixed-timescale algorithm by leveraging the SDR and
stochastic SCA methods to address this problem. Simulation
results demonstrate that the proposed approach significantly
reduces inference latency while improving accuracy, making
distributed on-device LLM inference feasible for resource-
constrained edge devices.
REFERENCES
[1] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz,
E. Agirre, I. Heintz, and D. Roth, â€œRecent advances in natural language
processing via large pre-trained language models: A survey,â€ ACM
Comput. Surv., vol. 56, no. 2, pp. 1â€“40, 2023.
[2] H. Fan, X. Liu, J. Y. H. Fuh, W. F. Lu, and B. Li, â€œEmbodied intelligence
in manufacturing: leveraging large language models for autonomous
industrial robotics,â€ J. Intell. Manuf., pp. 1â€“17, 2024.
[3] B. Wu, Y. Zhong, Z. Zhang, G. Huang, X. Liu, and X. Jin, â€œFast
distributed inference serving for large language models,â€ arXiv preprint
arXiv:2305.05920, 2023.
[4] K. B. Letaief, W. Chen, Y. Shi, J. Zhang, and Y.-J. A. Zhang, â€œThe
roadmap to 6G: AI empowered wireless networks,â€ IEEE Commun. Mag.,
vol. 57, no. 8, pp. 84â€“90, 2019.
[5] K. B. Letaief, Y. Shi, J. Lu, and J. Lu, â€œEdge artificial intelligence for
6G: Vision, enabling technologies, and applications,â€ IEEE J. Sel. Areas
Commun., vol. 40, no. 1, pp. 5â€“36, 2021.
[6] M. Zhang, J. Cao, X. Shen, and Z. Cui, â€œEdgeshard: Efficient LLM infer-
ence via collaborative edge computing,â€ arXiv preprint arXiv:2405.14371,
2024.
[7] Y. He, J. Fang, F. R. Yu, and V. C. Leung, â€œLarge language models
inference offloading and resource allocation in cloud-edge computing:
An active inference approach,â€ IEEE Trans. Mobile Comput., 2024.
[8] Y. Chen, R. Li, X. Yu, Z. Zhao, and H. Zhang, â€œAdaptive layer splitting for
wireless LLM inference in edge computing: A model-based reinforcement
learning approach,â€ arXiv preprint arXiv:2406.02616, 2024.
[9] J. Shao, Y. Mao, and J. Zhang, â€œLearning task-oriented communication
for edge inference: An information bottleneck approach,â€ IEEE J. Sel.
Areas Commun., vol. 40, no. 1, pp. 197â€“211, 2021.
[10] H. Li, J. Shao, H. He, S. Song, J. Zhang, and K. B. Letaief, â€œTack-
ling distribution shifts in task-oriented communication with information
bottleneck,â€ arXiv preprint arXiv:2405.09514, 2024.
[11] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catan-
zaro, â€œMegatron-lm: Training multi-billion parameter language models
using model parallelism,â€ arXiv preprint arXiv:1909.08053, 2019.
[12] M. Goldenbaum, H. Boche, and S. StaÂ´nczak, â€œHarnessing interference for
analog function computation in wireless sensor networks,â€ IEEE Trans.
Signal Process., vol. 61, no. 20, pp. 4893â€“4906, 2013.
[13] X. Li, G. Zhu, Y. Gong, and K. Huang, â€œWirelessly powered data
aggregation for iot via over-the-air function computation: Beamforming
and power control,â€ IEEE Trans. Wireless Commun., vol. 18, no. 7,
pp. 3437â€“3452, 2019.
[14] Z.-Q. Luo, W.-K. Ma, A. M.-C. So, Y. Ye, and S. Zhang, â€œSemidefinite
relaxation of quadratic optimization problems,â€ IEEE Signal Process.
Mag., vol. 27, no. 3, pp. 20â€“34, 2010.
[15] A. Liu, V. K. Lau, and M.-J. Zhao, â€œStochastic successive convex
optimization for two-timescale hybrid precoding in massive mimo,â€ IEEE
J. Sel. Topics Signal Process., vol. 12, no. 3, pp. 432â€“444, 2018.
6
