--- Page 1 ---
MAXIMUM A POSTERIORI INFERENCE FOR FACTOR GRAPHS VIA
BENDERS’ DECOMPOSITION ∗
Harsh Vardhan Dubey
Department of Mathematics & Statistics
University of Massachusetts, Amherst
Amherst
hdubey@umass.edu
Ji Ah Lee
Department of Mathematics & Statistics
University of Massachusetts, Amherst
Amherst
jlee@umass.edu
Patrick Flaherty
Department of Mathematics & Statistics
University of Massachusetts, Amherst
Amherst
pflaherty@umass.edu
ABSTRACT
Many Bayesian statistical inference problems come down to computing a maximum a-posteriori
(MAP) assignment of latent variables. Yet, standard methods for estimating the MAP assignment do
not have a finite time guarantee that the algorithm has converged to a fixed point. Previous research
has found that MAP inference can be represented in dual form as a linear programming problem
with a non-polynomial number of constraints. A Lagrangian relaxation of the dual yields a statistical
inference algorithm as a linear programming problem. However, the decision as to which constraints
to remove in the relaxation is often heuristic. We present a method for maximum a-posteriori inference
in general Bayesian factor models that sequentially adds constraints to the fully relaxed dual problem
using Benders’ decomposition. Our method enables the incorporation of expressive integer and
logical constraints in clustering problems such as must-link, cannot-link, and a minimum number of
whole samples allocated to each cluster. Using this approach, we derive MAP estimation algorithms
for the Bayesian Gaussian mixture model and latent Dirichlet allocation. Empirical results show that
our method produces a higher optimal posterior value compared to Gibbs sampling and variational
Bayes methods for standard data sets and provides certificate of convergence.
Keywords Bayesian · a posteriori · Benders’ decomposition · graphical model · constrained optimization · mixed
integer programming
1
Introduction
In many applications, the maximum a-posteriori (MAP) assignment is the primary object of inference. For example,
decoding error-correcting codes, identifying protein coding regions from DNA sequence data, part-of-speech tagging,
and image segmentation [48]. To address these applications, we have two objectives for the optimal MAP estimation
algorithm. First, it should produce an estimate that is in support of the actual posterior distribution rather than an
approximation that must then be rounded to a feasible estimate. Second, it should accommodate “hard” constraints
such as logical constraints or must-link constraints that are enforced regardless of the data. Third, it should provide a
finite-time certificate of convergence - that is, it should report that it has converged to a local optimum. Even without
these requirements, however, MAP inference is known to be NP-hard except for graphs of special structure [41].
Therefore, we expect to pay a computational price for the convenience of these conditions, but we aim to minimize the
computational impacts and retain the ability to solve problems of a size that are of practical importance.
∗Citation: Authors. Title. Pages.... DOI:000000/11111.
arXiv:2410.19131v1  [stat.ML]  24 Oct 2024


--- Page 2 ---
MAP VIA BENDERS’ DECOMPOSITION
The first condition on the estimation algorithm is that the MAP estimate is a feasible point in the true posterior
distribution. In the case of clustering models, the assignment variables are discrete and the MAP estimation problem is
typically a mixed-integer nonlinear programming problem [34]. Simulated annealing [45] and genetic algorithms [28]
converge to the global optimum for non-convex problems, but the convergence guarantees are generally asymptotic
rather than finite-time, and assessing convergence is challenging [12]. [51] addresses the issue of the non-convexity
of the MAP problem domain by approximating the domain with a convex hull, solving the nonlinear program over a
continuous domain, and then rounding to a feasible MAP estimate. Randomized rounding can significantly improve the
expected performance in some non-convex problems [26], however, rounding can lead to a suboptimal solution that is
distant from the true MAP value [32]. Our focus in this work is on the optimal MAP estimate for the particular data set
in hand rather than a high probability of good performance across data sets.
The second condition we would like to incorporate into the estimation algorithm is the ability to add both hard
constraints to restrict the domain and soft constraints to guide the optimal solution based on prior information. [5]
shows that incorporating hard constraints can dramatically improve both accuracy (comparing to known labels) and
computational time because the search space is significantly reduced. Other clustering algorithms can adaptively
incorporate user feedback as hard constraints which can also improve clustering accuracy and timing results [2]. Most
clustering algorithms, however, incorporate must-link and cannot-link constraints as soft constraints so as to simplify
the search space [16]. Therefore, incorporating both hard and soft constraints can improve clustering performance and
computational efficiency.
The third condition is that our algorithm should have a finite-time certificate of convergence. Gradient-based clustering
algorithms have good computational timing performance [34] and can have good average population performance [4].
However, such clustering algorithms are not able to distinguish between convergence towards a point near the global
optimum, or to a point near a bad local optimum. Moreover, they also require suitable initialization values which are not
always available for a high dimensional non-convex problem. Therefore, developing an algorithm that can guarantee
convergence even to a local optimum in finite time becomes important.
The rest of the paper is structured as follows. In section 2 we derive Generalized Benders’ Decomposition for factor
graphs, present a theorem outlining when our proposed algorithm yields an ϵ-global optimal value, and detail a step-wise
implementation of our algorithm. In section 3 we derive and implement our algorithm for the Bayesian Gaussian
Mixture Model (BGMM) and present experimental results on three standard data sets by comparing our proposed
algorithm to other standard Bayesian inference methods. Finally, in section 4 we derive and implement our algorithm
on the Latent Dirichlet Allocation(LDA) model and present experimental results on the 20 newsgroups data set by
comparing our proposed algorithm to other standard Bayesian inference methods. The remainder of this introduction is
concerned with the general MAP inference problem setup for factor graphs and related work on inference algorithms
for factor graphs.
1.1
Problem Setup
The maximum a-posteriori (MAP) estimate is a solution to
max
x
log p(x|y; ϕ),
(1)
where x is the vector of latent variables, y is the observed data, and ϕ is the vector of parameters of a Bayesian model, p.
Latent variables, z, that are in the model, but not subjects of inference are marginalized as p(x|y; ϕ) =
R
z p(x, z|y; ϕ)dz.
The log posterior distribution can always be decomposed into the sum of local interactions [43],
log p(x|y; ϕ) =
X
v∈V
θv(xv) +
X
f∈F
θf(xf),
(2)
where V is the set of latent variables, F is the set of factors, and xf is the subset of the latent variables that are involved
in factor f. The factorization separates the joint log density function into singleton functions, θv(xv), which only
depend on a single element of V, and factor functions, θf(xf), which depend on a subset f ∈V of latent variables.
The functions θv and θf incorporate the fixed data, y, and parameters, ϕ. The resulting MAP inference problem in
factorized form is
MAP(θ) = max
x
X
v∈V
θv(xv) +
X
f∈F
θf(xf).
(3)
To construct the dual decomposition [43], the model is augmented by duplicating all the latent variables that are
involved in factors xf ∀f ∈F; we denote this set of factor variables xF = {xf : f ∈F}, where xf = {xv : xv ∈f}.
2


--- Page 3 ---
MAP VIA BENDERS’ DECOMPOSITION
Constraints are added to tie the newly created factor variables to the original latent variables.
MAP(θ)
=
max
x,xF
X
v∈V
θv(xv) +
X
f∈F
X
v∈f
θf(xf
v)
(4)
subject to
xv = xf
v,
∀v ∈f, f ∈F
(5)
The constraints, xv = xf
v, are “complicating” because were it not for those constraints, the problem would be a convex
optimization problem if θv and θf were concave and the domain of x was convex. Indeed, many algorithms for MAP
inference can be formulated as relaxations of the constraints, the domain, and the objective function [48].
Having represented the MAP inference problem as a factor graph, the Lagrange dual can then be formulated with the
Lagrange function:
L(δ, x, xF) =
X
v∈V
θv(xv) +
X
f∈F
X
v∈f
θf(xf
v) +
X
f∈F
X
v∈f
δfv(xv −xf
v),
(6)
where δfv is Lagrange variable associated with the constraint gfv(xv, xf
v) := xv −xf
v = 0.
The MAP estimation problem can be solved by relaxing (removing) all of the constraints and solving the dual problem.
The resulting optimization problem completely separates and the size is proportional to the largest factor. However, the
fully relaxed solution may be non-feasible for the original problem.
The problem is then how should we select constraints to add back to the fully relaxed Lagrange dual problem. [44]
presents an approach that starts with a fully relaxed dual problem and adds constraints back in according to a clustering
rule. However, that approach is heuristic and only developed for discrete latent variables. Here, we present a method
based on Benders’ decomposition that optimally selects violated constraints and iteratively tightens the relaxation while
maintaining rigorous bounds on the value of the optimum.
1.2
Contributions
This work has three main contributions. First, we derive a dual decomposition MAP inference algorithm that preserves
hard domain constraints and produces feasible points in the posterior without relaxations of the domain to a convex
set. Second, we show theoretically and empirically that the algorithm yields a certificate of convergence to the ϵ-local
optimal value in finite time. Third, we evaluate the algorithm on two common Bayesian models: the Bayesian Gaussian
mixture model, and latent Dirichlet allocation. A comparison with Gibbs sampling and variational inference methods
shows that our proposed method produces a better optimal value on common test data sets than these other methods.
1.3
Related Work
We now provide a brief overview of three widely used methods for maximum a-posteriori (MAP) inference for Bayesian
factor models.
Gibbs sampling is an algorithm to generate a sequence of samples from an arbitrary multivariate probability distribution
without requiring the normalization constant [23, 14]. It is particularly well-adapted to sampling the posterior distribution
of a Bayesian network since Bayesian networks are typically specified as a collection of conditional distributions and
the sampler samples each variable, in turn, conditional on the current value of other variables. The usefulness and
practical applicability of Gibbs sampling depends on the ease with which samples can be drawn from the conditional
distributions. If the graph of a Bayesian network is constructed using distributions from the exponential family, and if
the parent-child relationships preserve conjugacy, then sampling from the conditional distributions that arise in Gibbs
sampling can be done with a known distribution function [8].
The generality of Gibbs sampling comes with some costs. It is guaranteed to converge in distribution to the posterior,
but assessing when that convergence has occurred is difficult [12, 22]. It naturally provides an estimate of the moments
of a posterior distribution, but estimating the mode of a high dimensional distribution is challenging because simple
averaging does not provide a feasible point if the domain is non-convex [1]. In clustering models, permutations
of the cluster assignment labels have equivalent posterior probabilities. So, sampling algorithms can “mode-hop”
which produces samples from different domains, but without practical differences in meaning of the different sample
values [42]. Finally, sampling methods often become highly inefficient or completely unable to sample from high-
dimensional discrete random variables. The use of a momentum term which requires a derivative improves the efficiency,
but limits the applicability of the sampler for clustering applications unless special transformations are employed [35].
Variational Bayesian methods approximate the complicated posterior distribution, p(x|y; ϕ), with a simpler surrogate
distribution q(x) such that the KL divergence between the two, KL(q∥p), is minimized [8].
3


--- Page 4 ---
MAP VIA BENDERS’ DECOMPOSITION
When q is taken as a parametric family, the minimization is over the finite set of parameters. Given the best-fit
approximation, the mode of the density (the MAP value) can be obtained using search methods or analytical values if
the distribution is simple enough. It is scalable to large data sets and applicable to complex models where it would be
too computationally expensive [47]. The minimization of KL(q∥p) is accomplished by iterative updates to a fixed point;
the update equations typically make use of gradient information [10]. There are some notable results on the consistency
of variational inference [50, 37].
There are two drawbacks of variational inference methods for MAP estimation. First, the choice of the surrogate
distribution is a tradeoff between computability and fidelity to the true posterior [48]. A simple surrogate is easy to
optimize, but can yield a MAP estimate that is distant from the true value. It is difficult to assess what is and is not a
good surrogate function. Second, convergence to the fixed point is typically assessed by the change in the value of the
surrogate parameters [10]. The approach does not provide an upper bound on the optimal value to assess whether better
values of the parameters of q may be available. The algorithm is typically rerun from multiple initial points to guard
against convergence to local optima, but selecting initial points is challenging in high dimensional parameter space.
Since the MAP problem is a mixed-integer nonlinear program in general, methods for solving that general class of
problems can be used for MAP inference. The cutting plane method [7] was used in [44] to solve the MAP inference
problem. However, the problem considered in that work only involved discrete variables. When there are continuous
variables in the problem, a separate optimization problem is solved for each setting of the discrete variables and no
information is passed from the continuous solution to the discrete solver [25]. As a result, for MINLP problems, cutting
plane methods can be inefficient.
Branch and bound methods use a divide and conquer approach to search the feasible set. In the case of the maximization
problem, it uses an upper bound on the optimal cost to exclude parts of the domain and focus the search on areas
where improvement is possible [7, 32]. Branch and bound algorithms work by breaking down an optimization problem
into a series of smaller sub-problems and using a bounding function to eliminate sub-problems that cannot contain
the optimal solution [15]. Branch and bound methods have steadily improved in terms of computational efficiency
and can now handle very large problem sizes [6]. Branch-and-Reduce Optimization Navigator (BARON) which is
based on the branch and bound philosophy is the state-of-the-art solver for general mixed-integer nonlinear programs
(MINLPs) [31]. It exploits the general problem structure to Branch and bound MINLPs with domain reduction on
both discrete and continuous decision variables [40]. While branch and bound methods can be used for mixed integer
nonlinear programming problems, they seem to be best suited for nonlinear programming problems with no discrete
variables because the branching process can require very deep searches. In the case of clustering, moving a single data
point from one cluster to another can have a very small impact on the optimal value and thus the branching process is
not able to exclude large parts of the domain close to the root of the branch and bound tree.
Outer approximation methods require the user to explicitly separate the problem into integer and continuous variables [18,
20]. Once this is done, the integer variables are held fixed while the solution to a simpler continuous problem is solved.
The outer approximation serves as an upper bound on the maximum of the true objective. By sequentially tightening
the bound, the lower and upper bounds converge to a fixed point.
Generalized Benders’ decomposition provides a solution to certain mixed-integer nonlinear programming problems
by separating the original problem variables into “complicating” and “not-complicating variables” [24]. The original
optimization problem is decomposed into a so-called “master” problem and “sub-problem”. The Lagrange multiplier
from the subproblem is used to form constraints on the fully relaxed master problem and the approximation of the
original problem is tightened as constraints are added. In this way, the method resembles the outer approximation
method, but there are meaningful differences [19]. Generalized Benders’ decomposition has been extended to a
more general class of mixed-integer nonlinear programs and has been used to solve large-scale problems in chemical
engineering and other domains [20].
2
Generalized Benders’ Decomposition for Factor Graphs
In this section, the Generalized Benders’ decomposition approach is extended to problems in statistical inference - in
particular maximum a-posterior inference in factor graph models. As stated, Generalized Benders’ decomposition [24]
is used for solving problems of the form
max
x,y f(x, y)
subject to G(x, y) ≥0,
x ∈X, y ∈Y,
(7)
where the following conditions on the objective and constraints hold:
4


--- Page 5 ---
MAP VIA BENDERS’ DECOMPOSITION
1. for a fixed y, f(x, y) separates into independent optimization problems each involving a different subvector of
x,
2. for a fixed y, f(x, y) is of a special structure that can be solved efficiently, and
3. fixing y renders the optimization problem convex in x.
The constraint function G(x, y) captures all of the constraints that involve both x and y, while the constraints x ∈X
and y ∈Y capture the constraints that involve either variable but not both simultaneously.
Theorem 1. If the posterior is convex in the latent variables, maximum a-posteriori inference for Bayesian models in
factor graph form satisfies the conditions of generalized Benders’ decomposition, and thus global optimality can be
achieved.
Proof. The factor graph representation of maximum a-posteriori inference for a Bayesian model is
MAP(θ)
=
max
x,xF
X
v∈V
θv(xv) +
X
f∈F
X
v∈f
θf(xf
v)
subject to
xv = xf
v,
∀v ∈f, f ∈F
and the general form for generalized Benders’ decomposition is
max
x,y
f(x, y)
subject to
G(x, y) ≥0,
x ∈X, y ∈Y.
Let xF in the MAP inference problem be x in the generalized Benders’ decomposition problem, and let x in the MAP
inference problem be y in the generalized Benders’ decomposition problem.
Condition 1 is satisfied because if x in the MAP problem is held fixed, the maximization is over a sum of factors, each
of which is a subvector of xF. If x is fixed, the optimization problem is trivial because the constraints xv = xf
v,
∀v ∈
f, f ∈F set the value of xF; constraint 2 is satisfied. Finally, the condition that the posterior be convex in the latent
variables renders it convex in xF for fixed x.
Corollary 1. If the posterior is not convex in the latent variables, maximum a-posteriori inference for Bayesian models
in factor graph form does not satisfy the conditions of generalized Benders’ decomposition, and thus local optimality
may be achieved.
Generalized Benders’ decomposition separates the complicating variables in the optimization problem with the idea
that holding these variables fixed renders the optimization problem much simpler. In the Bayesian factor graph model
formulation, the original latent variables x are construed as complicating variables. The factor variables, xF - the
clones that were created in the formation of the factor graph - are the non-complicating variables. The approach is
best demonstrated by examples. First, we present the general purpose algorithm. Then in the next sections, we derive
specific MAP inference algorithms for the Bayesian Gaussian mixture model and the latent Dirichlet allocation model.
Proofs and technical details of the specific derivations are found in the Appendices.
2.1
Algorithm
Benders’ decomposition applied to the factor graph yields an algorithm for solving the problem 3.
1. Pick a feasible point ¯x. Solve the subproblem (1 −¯x),
max
xF
X
f∈F
θf(xf
f),
s.t.
¯xv = xf
v, ∀v ∈f, f ∈F.
(8)
Obtain the optimal multiplier vector ¯u and the function L∗(x, ¯u), where
L∗(x; ¯u) = max
xF



X
v∈V
θv(xv) +
X
f∈F
X
v∈f
θf(xf) + ¯uT G(x, xF)


.
Since the constraints are all of the form xv = xf
v, the function L∗has the following decomposition
X
v∈V
θv(xv) +
X
f∈F
X
v∈f
¯uxf
vxv + max
xF



X
f∈F
θf(xf) −
X
f∈F
X
v∈f
¯uxf
vxf
v


.
(9)
And, since the objective is separable, the optimization problem can be solved efficiently. Set p = 1, q = 1,
u1 = ¯u, and LBD = v(¯x).
5


--- Page 6 ---
MAP VIA BENDERS’ DECOMPOSITION
2. Solve the relaxed master problem
max
x,x0
x0
subject to
x0
≤L∗(x, uj),
j = 1, . . . , p
0
≤L∗(x, λj),
j = 1, . . . , q.
Let (ˆx, ˆx0) be an optimal solution; ˆx0 is an upper bound on the optimal value of (7). If LBD ≥ˆx0 −ϵ, then
terminate.
3. Solve the revised subproblem (1 −ˆx). One of two outcomes is possible:
(a) The quantity v(ˆx) is finite. If v(ˆx) ≥x0 −ϵ, then terminate. Otherwise, determine the optimal Lagrange
multiplier ˆu (or a near-optimal multiplier), and the function L∗(x, ˆu). Increment p by 1 and set up = ˆu.
If v(ˆx) > LBD, set update the lower bound, LBD = v(ˆx). Return to step 2.
(b) Problem (1 −ˆx) is infeasible. Determine ˆλ and the function L∗(x; ˆλ). Increase q by 1 and set λq = ˆλ.
Return to step 2.
This algorithm selects the most violated constraint as assessed by the Lagrange multiplier and adds that constraint to the
master problem. In this way, Benders’ decomposition addresses the challenge of selecting constraints in the variational
approximation of factor graph models.
3
Bayesian Gaussian Mixture Model
In this section, we formulate and derive a Generalized Benders decomposition algorithm for the Bayesian Gaussian
mixture model.
3.1
Problem Formulation
A Bayesian Gaussian mixture model has the form:
yi|zi, µ, Λ
∼
Gaussian(µzi, Λ−1
zi )
(10)
zi|π
∼
CategoricalK(π)
(11)
π|α0
∼
DirichletK(α0)
(12)
µk|Λk
∼
Gaussian(µ0, (β0Λk)−1)
(13)
Λk
∼
Wishart(W0, ν0)
(14)
The object of inference is the full set of latent variables x = (z1, . . . , zN, µ1, . . . , µK, Λ1, . . . , ΛK, π1, . . . , πK). The
value of zi gives the cluster assignment for observation yi, the values of µk and Λk give the cluster location and
spread, and the value of πk gives the relative proportion of the data in cluster k. The prior, p(x|ϕ) is parameterized by
ϕ = (α0, β0, W0, ν0). Given this model, the posterior density function factorizes as
p(x|y, ϕ) ∝p(x, y, ϕ) = p(y|z, µ, Λ)p(z|π)p(π; α0)p(µ|Λ; β0)p(Λ; W0, ν0).
(15)
and can be represented as a factor graph (for full derivations, see Appendix A).
A graphical representation of the factor graph is shown in Figure 2.
Lemma 1. The posterior of BGMM is not convex in the latent variables. (See Appendix B for the proof.)
Thus, by Corollary 1, we note that our proposed method achieves local optimality. Even so, the value it converges to is
quite good.
3.2
Experiments
We compare our proposed approach to variational Bayes and the Gibbs sampler on three standard clustering data sets.
Data collection and preprocessing
We obtained the Iris (iris, n = 150, d = 4), Wine Quality (wine, n = 178, d =
13), and Wisconsin Breast Cancer (brca, n = 569, d = 30) data sets from the UCI Machine Learning Repository [17].
A 3-d projection of iris was obtained by projecting on the first three principal components, a 6-d projection of wine
was obtained by projecting on the first six principal components, and only the following features were employed for the
6


--- Page 7 ---
MAP VIA BENDERS’ DECOMPOSITION
Figure 1: Graphical model representation of the BGMM model.
Figure 2: Factor graph representation of the BGMM model.
brca data set: worst area, worst smoothness, and mean texture, then standardized. The reduction in dimensions was
conducted to obtain not highly correlated (< 0.7) dimensions but to keep a good portion of overall variability (> 0.85).
Since our goal is to obtain the global maximum a posteriori (MAP) clustering given the data set rather than a prediction,
all of the data was used for clustering.
7


--- Page 8 ---
MAP VIA BENDERS’ DECOMPOSITION
Experimental protocol
We used the scikit-learn [38] implementation of variational Bayes and implemented
the Gibbs sampler for the Bayesian Gaussian mixture model in python as per the conditionals derived in [13]. The
convergence of the chains was assessed using standard trace plots [39] (See Appendix G). The variational Bayes
experiments were run using algorithms defined in python/scikitlearn and the Gibbs sampler experiments were run
using generative models defined in section 3.1 on 1,000 iterations (with a burn-in period of 100 iterations) in python.
Our approach was implemented in GAMS [9, 21], a standard optimization problem modeling language by initializing it
after running the branch and bound algorithm (BNB) for six hundred seconds and adding the constraints detailed in
Appendix B.
Clustering Comparison Metrics
Evaluating the quality of a clustering algorithm is inherently more challenging
because more often than not there is no true label of the data; so we compare clustering solutions to one another
rather than to a ground truth. [33] proposed an information theoretic criterion for comparing two clustering solutions.
Variation of Information (VoI) measures the amount of information lost and gained in changing from clustering C to
clustering C′:
VoI(C, C′) = H(C) + H(C′) −2I(C, C′),
(16)
where
H(C) = −
K
X
k=1
PC(k) log PC(k)
is the entropy associated with clustering C, and
I(C, C′) =
K
X
k=1
K
X
k′=1
PC,C′(k, k′) log PC,C′(k, k′)
PC(k)PC′(k′)
is the mutual information between the clustering solutions. In the entropy and mutual information computations,
PC,C′(k, k′) =
|Ck∩C′
k′|
n
is the joint probability that a point belongs to Ck and C′
k′, and PC(k) = |Ck|
n
is proportion of
points in cluster k. Along with Variation of Information, we also report three additional quantities: log MAP (calculated
using 38 - 43), running time - the running time taken by each algorithm in seconds, and optimality guaranteed - whether
optimality is achieved by each algorithm, which helps us understand the nature of these algorithms compared to our
proposed method better. Running time and optimality guarantee analyses are provided in Appendix E.
Comparison to other algorithms
Table 1 and Table 2 show comparisons of our proposed method (GBD) with other
standard Bayesian estimation methods (variational Bayes, Gibbs sampler). Table 1 shows a comparison of inference
algorithms using the variation of information (VoI) distance for three standard clustering test data sets: iris, wine, and
brca. A large VoI score indicates that the sample assignment configuration between two inference algorithms has little
overlap and a small VoI score indicates a large degree of overlap. The VoI distance between GBD (our method) and
the provided label is comparable to the distance between other inference methods and the label. In all tests, the GBD
solution is closer to the label and closer to variational Bayes than the Gibbs sampler. We note the caveat that labels are
typically generated by human classification and the labels may be erroneous when the differences between groups are
difficult to ascertain based on features alone.
Table 2 shows the estimated log MAP values for the three different methods on all three data sets along with the log
MAP values for branch and bound (BNB) since we use it to initialize our method. In terms of log MAP, GBD identifies
a MAP estimate that is better than the other inference methods.
Impact of must-link constraints.
One of the benefits of our methodology, aside from a certificate of convergence,
is the capability to naturally add constraints. In Table 3, we outline the log MAP values after adding 2, 4, 8, 16, and
32 must-link constraints on the iris data set. We also report the same values from the branch and bound algorithm
after adding the same constraints. The lower bound for GBD does not change except for the case with 16 constraints
indicating that the optimal MAP is identified in all cases The upper bound shows a downward trend indicating that the
constraints improve the ability to provide a certificate of (local) optimality. Even so, GBD outperforms BNB in all
cases and is able to decrease the upper bound of the log MAP with the addition of new constraints.
4
Latent Dirichlet Allocation
In this section, we formulate and derive a Generalized Benders decomposition algorithm for the Latent Dirichlet
allocation (LDA) model [11].
8


--- Page 9 ---
MAP VIA BENDERS’ DECOMPOSITION
Label
Var Bayes
Sampler
GBD
Label
0
0.124
0.146
0.149
Var Bayes
0
0.122
0.116
Sampler
0
0.122
GBD
0
(a) VoI distance for iris data set.
Label
Var Bayes
Sampler
GBD
Label
0
0.033
0.261
0.048
Var Bayes
0
0.252
0.032
Sampler
0
0.264
GBD
0
(b) VoI distance for wine data set.
Label
Var Bayes
Sampler
GBD
Label
0
0.097
0.206
0.080
Var Bayes
0
0.210
0.079
Sampler
0
0.196
GBD
0
(c) VoI distance for brca data set.
Table 1: Comparison of inference algorithms for the Bayesian Gaussian mixture model using variation of information
(VoI) scores.
iris
wine
brca
Var Bayes
-150.997
-676.813
-681.272
Sampler
-171.044
-1172.079
-1195.411
BNB (UB)
-90.149 (230925.3)
-646.203 (1156346.8)
-660.976 (127090.3)
GBD (UB)
-89.411 (-81.5)
-646.200 (-200.4)
-660.971 (-657.3)
Table 2: Comparison of inference algorithms for the Bayesian Gaussian mixture model using log MAP values. GBD
outperforms both variational Bayes and the Gibbs sampler.
Must-Link
GBD (UB)
BNB (UB)
2
-90.146 (-81.751)
-90.149 (230000.6)
4
-90.146 (-78.566)
-90.149 (229878.16)
8
-90.146 (-78.566)
-90.149 (228843.24)
16
-89.055 (-81.486)
-90.149 (227252.85)
32
-90.146 (-78.089)
-90.149 (207962.17)
Table 3: Comparison of log MAP values by adding a different number of must-link constraints for the iris data set.
4.1
Problem Formulation
A latent Dirichlet allocation model has the form:
βk|η0
∼
DirichletV (η0)
(17)
Θd|α0
∼
DirichletK(α0)
(18)
zdn|Θd
∼
CategoricalK(Θd)
(19)
wdn|zdn, βk
∼
CategoricalV (βzdn).
(20)
9


--- Page 10 ---
MAP VIA BENDERS’ DECOMPOSITION
Figure 3: Graphical model representation of the smoothed LDA model.
The object of inference is the full set of latent variables x = (z11, . . . , zMN, Θ1, . . . , ΘM, β1, . . . , βK). The values of
βk are the distribution of words in topic k, the values of Θd are the probability distribution of topics in document d,
and the value of zdn gives the topic assignment of an observed n-th word in document d, wdn. The prior, p(x|ϕ) is
parameterized by ϕ = (α0, η0). Given this model, the posterior density function factorizes as
p(x|w, ϕ) ∝p(x, w, ϕ) = p(w|z, β)p(z|Θ)p(Θ; α0)p(β; η0)
(21)
and can be represented as a factor graph (for full derivations, see Appendix C).
A graphical representation of the factor graph is shown in Figure 4.
Figure 4: Factor graph representation of the smoothed LDA model.
Lemma 2. The posterior of BGMM is not convex in the latent variables. (See Appendix D for the proof.)
Thus, by Corollary 1, we note that our proposed method may achieve a local optimality.
4.2
Experiments
We compare our proposed approach to variational Bayes and the Gibbs sampler on the 20 newsgroups data set.
10


--- Page 11 ---
MAP VIA BENDERS’ DECOMPOSITION
Topic 1
Topic 2
GBD
Sampler
Var Bayes
GBD
Sampler
Var Bayes
israel
like
israel
edu
key
just
time
time
key
mail
line
like
just
just
like
send
like
time
like
data
time
3d
mail
mail
image
com
just
com
format
key
format
format
line
objects
time
help
3d
gov
image
image
send
edu
ac
help
message
file
objects
gov
available
objects
mail
format
message
israel
based
send
help
stuff
israel
com
Table 4: Comparison of the top 10 words from Topic 1 and Topic 2 for all three methods
Data collection and preprocessing
We obtained the 20 newsgroups (news20, n = 18, 846, d = 1) data set from
the UCI KDD Archive [30]. We only look at 50 randomly selected documents from the original data set, and run all
methods under consideration on a list of 5 topics and 25 most frequently used words within the selected data.
Experimental protocol
We used the scikit-learn [38] implementation of variational Bayes and implemented
the Gibbs sampler for latent Dirichlet allocation model based on Griffiths and Steyvers [27]. The variational Bayes
experiment was run using algorithms defined in python/scikitlearn, and the Gibbs sampler experiments were run
via python for 1,000 iterations (with a burn-in period of 100 iterations). Our approach was implemented in GAMS
[9, 21], a standard optimization problem modeling language, with a solver BARON [46, 40]. For each problem solved
using BARON, we limit the process to end in 600 seconds. Like the experiments for the BGMM, our approach was
implemented by initializing it after running the branch and bound algorithm and adding the constraints detailed in
Appendix D.
Comparison Metrics
To compare our proposed method to variational Bayes and the Gibbs sampler we calculate the
log MAP - calculated using 45 - 48 and we also report the top ten words for two topics generated by the three different
methods under consideration. To make sure the two topics that we select align across the three different methods, we
sort the topics in descending order and then pick the top ten words from each topic according to their probabilities.
Another important metric to compare LDA models is held-out test perplexity which has been analyzed for all three
methods in Appendix F.
Comparison to other algorithms
In terms of log MAP, GBD outperforms the other methods under consideration
with a log MAP value of -1.10113E+6 as compared to -1.11275E+6 for variational Bayes and -1.1124280E+6 for the
Gibbs sampler. Table 4 shows the top ten words from topic 1 and topic 2 generated by each of the three methods under
consideration. We observe that all three methods under consideration show similar performance. We highlight those
words in Table 4 that are common between at least two of the three methods under consideration. For both topic 1 and
topic 2 there is a high degree of overlap between the top ten selected words. One important thing to note however is
that our proposed method provides a certificate of local optimality in finite time. On the other hand, variational Bayes
provides a locally optimal solution but to an approximation of the posterior, not to the exact posterior. And, the Gibbs
sampler does not provide an optimality guarantee for a finite sample size. Also important to consider is the fact that our
proposed method allows the addition of hard constraints based on some prior knowledge of the problem which other
competing methods do not allow.
5
Discussion
In this section, we summarize the main contributions of our work and discuss some limitations. In this paper, we first
provide a computationally efficient algorithm for maximum a-posteriori inference in Bayesian models that yields an
optimal value. The method sequentially adds optimal constraints to the fully relaxed dual problem using Benders’
decomposition. Second, we show when the algorithm yields an ϵ-globally optimal value and when it yields a locally
optimal value. Third, we derive and implement our method for two Bayesian factor models (BGMM and LDA) with the
incorporation of hard domain constraints to reflect the properties of the models. Finally, for each model, we evaluate the
algorithm on standard data sets and compare the results to other standard methods. In terms of log MAP, our proposed
11


--- Page 12 ---
MAP VIA BENDERS’ DECOMPOSITION
method outperforms other competing methods for both Bayesian factor models on real-world data sets. For BGMM,
the Variation of Information results show that the final clustering provided by our method is similar to the suggested
labels. And, for the LDA model, the comparison of the top ten words assigned for both topics shows a high degree of
similarity between our proposed method and other competing methods.
There are a few important points to note when applying our method though. Firstly, because we turn our problem into
an optimization problem, it allows us to add hard constraints appropriately. For example, we can force the final solution
to have two data points being clustered into the same cluster, or into different clusters based on prior knowledge. This
flexibility allows our method to become quite useful as it is not always easy to enforce such constraints using other
standard methods. However, adding such constraints may limit the domain space in a way that the optimization solver
runs less efficiently. Secondly, GBD is not guaranteed to always provide the global optimum and when it provides a
local optimum, the solution may be influenced by the initial points chosen. Thus, it is important to provide good initial
points to start with for the proposed method to work best. Finally, as is the case with all Bayesian mixture models,
it is important to set appropriate values for parameters of the prior probability distributions for parameter estimation
and inference on posterior probability distribution. In many cases, one can determine them with prior knowledge or
based on common practices in literature. However, when neither is available, one would need to come up with some
methodology to choose the prior parameter values that are appropriate for the model and for the data set.
Acknowledgments
This work was funded by the National Science Foundation TRIPODS award 1934846.
References
[1] Christophe Andrieu, Nando De Freitas, Arnaud Doucet, and Michael I Jordan. An introduction to MCMC for
machine learning. Machine learning, 50:5–43, 2003.
[2] Rico Angell, Nicholas Monath, Nishant Yadav, and Andrew McCallum. Interactive correlation clustering with
existential cluster constraints. In Proceedings of the 39th International Conference on Machine Learning, volume
162 of Proceedings of Machine Learning Research, pages 703–716. PMLR, 17–23 Jul 2022.
[3] Arthur Asuncion, Max Welling, Padhraic Smyth, and Yee Whye Teh. On smoothing and inference for topic
models. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 27–34,
2009.
[4] Sivaraman Balakrishnan, Martin J. Wainwright, and Bin Yu. Statistical guarantees for the EM algorithm: From
population to sample-based analysis. The Annals of Statistics, 45(1):77 – 120, 2017.
[5] Philipp Baumann. A binary linear programming-based K-means algorithm for clustering with must-link and
cannot-link constraints. In 2020 IEEE International Conference on Industrial Engineering and Engineering
Management (IEEM), pages 324–328, 2020.
[6] Dimitris Bertsimas and Angela King. Logistic regression: From art to science. Statistical Science, 32(3):367–384,
2017.
[7] Dimitris Bertsimas and Robert Weismantel. Optimization over integers. Dynamic Ideas, Belmont, 2005.
[8] Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning, volume 4. Springer,
2006.
[9] Johannes Bisschop and Alexander Meeraus. On the development of a general algebraic modeling system in a
strategic planning environment. In Applications, volume 20 of Mathematical Programming Studies, pages 1–29.
Springer Berlin Heidelberg, 1982.
[10] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of
the American statistical Association, 112(518):859–877, 2017.
[11] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine Learning
research, 3(Jan):993–1022, 2003.
[12] Stephen P Brooks and Andrew Gelman. General methods for monitoring convergence of iterative simulations.
Journal of computational and graphical statistics, 7(4):434–455, 1998.
[13] Mattia Bruno. Auto-encoding variational Bayes. PhD thesis, Università degli Studi di Padova, 2021.
[14] Siddhartha Chib and Edward Greenberg. Understanding the Metropolis-Hastings algorithm. The American
Statistician, 49(4):327–335, 1995.
12


--- Page 13 ---
MAP VIA BENDERS’ DECOMPOSITION
[15] Jens Clausen. Branch and bound algorithms-principles and examples. Technical report, Department of Computer
Science, University of Copenhagen, 1999.
[16] Ian Davidson and SS Ravi. Clustering with constraints: Feasibility issues and the k-means algorithm. In
Proceedings of the 2005 SIAM international conference on data mining, pages 138–149. SIAM, 2005.
[17] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
[18] Marco A Duran and Ignacio E Grossmann. An outer-approximation algorithm for a class of mixed-integer
nonlinear programs. Mathematical programming, 36:307–339, 1986.
[19] Marco Duran-Pena. A mixed-integer nonlinear programming approach for the systematic synthesis of engineering
systems. PhD thesis, Department of Chemical Engineering, Carnegie-Mellon University, 1984.
[20] Christodoulos A Floudas. Deterministic global optimization: theory, methods and applications, volume 37.
Springer Science & Business Media, 2013.
[21] GAMS Development Corporation. GAMS — A User’s Guide. GAMS Development Corporation, 2017.
[22] Andrew Gelman and Donald B Rubin. Inference from iterative simulation using multiple sequences. Statistical
science, 7(4):457–472, 1992.
[23] Stuart Geman and Donald Geman. Stochastic relaxation, gibbs distributions, and the bayesian restoration of
images. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-6(6):721–741, 1984.
[24] Arthur M. Geoffrion. Generalized benders decomposition. Journal of Optimization Theory and Applications, 10,
1972.
[25] Paul C Gilmore and Ralph E Gomory. A linear programming approach to the cutting-stock problem. Operations
research, 9(6):849–859, 1961.
[26] Michel X Goemans and David P Williamson.
Improved approximation algorithms for maximum cut and
satisfiability problems using semidefinite programming. Journal of the ACM (JACM), 42(6):1115–1145, 1995.
[27] Thomas Griffiths and Mark Steyvers. Prediction and semantic association. Advances in Neural Information
Processing Systems (NeurIPS), 2002.
[28] Prabhat Hajela. Genetic search-an approach to the nonconvex optimization problem. AIAA journal, 28(7):1205–
1210, 1990.
[29] Gregor Heinrich. Parameter estimation for text analysis. Technical report, Fraunhofer IGD, 2005.
[30] S. Hettich and S. D. Bay. UCI kdd archive, 1999.
[31] Jan Kronqvist, David E. Bernal, Andreas Lundell, and Ignacio E. Grossmann. A review and comparison of solvers
for convex MINLP. Optimization and Engineering, 20(2):397–455, Jun 2019.
[32] A. H. Land and A. G. Doig. An automatic method of solving discrete programming problems. Econometrica,
28(3):497, 1960.
[33] Marina Meil˘a. Comparing clusterings by the variation of information. In Learning Theory and Kernel Machines:
16th Annual Conference on Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003, Washington, DC,
USA, August 24-27, 2003. Proceedings, pages 173–187. Springer, 2003.
[34] Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.
[35] Radford M Neal et al. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2(11):2,
2011.
[36] Yannis Papanikolaou, James R Foulds, Timothy N Rubin, and Grigorios Tsoumakas. Dense distributions from
sparse samples: improved gibbs sampling parameter estimators for lda. The Journal of Machine Learning
Research, 18(1):2058–2115, 2017.
[37] Debdeep Pati, Anirban Bhattacharya, and Yun Yang. On statistical optimality of variational bayes. In International
Conference on Artificial Intelligence and Statistics, pages 1579–1588. PMLR, 2018.
[38] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:
Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.
[39] Vivekananda Roy. Convergence diagnostics for markov chain monte carlo. Annual Review of Statistics and Its
Application, 7:387–412, 2020.
[40] N. V. Sahinidis. BARON 17.8.9: Global Optimization of Mixed-Integer Nonlinear Programs, User’s Manual,
2017.
13


--- Page 14 ---
MAP VIA BENDERS’ DECOMPOSITION
[41] Solomon Eyal Shimony. Finding maps for belief networks is NP-hard. Artificial intelligence, 68(2):399–410,
1994.
[42] Cristian Sminchisescu, Max Welling, and Geoffrey Hinton. A mode-hopping MCMC sampler. Technical Report
CSRG-478, University of Toronto, 2003.
[43] David Sontag, Amir Globerson, and Tommi Jaakkola. Introduction to dual decomposition for inference. In
Optimization for Machine Learning, pages 219–254. MIT Press, 2012.
[44] David Sontag, Talya Meltzer, Amir Globerson, Yair Weiss, and Tommi Jaakkola. Tightening LP relaxations for
MAP using message-passing. In 24th Conference in Uncertainty in Artificial Intelligence, pages 503–510. AUAI
Press, 2008.
[45] H.H. Szu and R.L. Hartley. Nonconvex optimization by fast simulated annealing. Proceedings of the IEEE,
75(11):1538–1540, 1987.
[46] Mohit Tawarmalani and Nikolaos V. Sahinidis. A polyhedral branch-and-cut approach to global optimization.
Mathematical Programming, 103(2):225–249, 2005.
[47] Minh-Ngoc Tran, Trong-Nghia Nguyen, and Viet-Hung Dao. A practical tutorial on variational bayes. arXiv
preprint arXiv:2103.01327, 2021.
[48] Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational inference.
Foundations and Trends® in Machine Learning, 1(1–2):1–305, 2008.
[49] Hanna M Wallach, Iain Murray, Ruslan Salakhutdinov, and David Mimno. Evaluation methods for topic models.
In Proceedings of the 26th annual international conference on machine learning, pages 1105–1112, 2009.
[50] Yixin Wang and David M Blei. Frequentist consistency of variational bayes. Journal of the American Statistical
Association, 114(527):1147–1161, 2019.
[51] Argyrios Zymnis, Stephen Boyd, and Dimitry Gorinevsky. Mixed state estimation for a linear gaussian markov
model. In 2008 47th IEEE Conference on Decision and Control, pages 3219–3226. IEEE, 2008.
14


--- Page 15 ---
MAP VIA BENDERS’ DECOMPOSITION
A
BGMM posterior density in a factor graph form
Term 1.
The density function of p(y|z, µ, Λ) is
p(y|z, µ, Λ) =
N
Y
i=1
K
Y
k=1

N(yi; µk, Λ−1
k )
zik .
(22)
The log-density function is
log p(y|z, µ, Λ)
=
N
X
i=1
K
X
k=1
zik

−D
2 log(2π) −1
2 log det Λ−1
k
−1
2(yi −µk)T Λ(yi −µk)

+ =
N
X
i=1
K
X
k=1
zik
1
2 log det Λk −1
2yT
i Λkyi + µT
k Λkyi −1
2µT
k Λkµk

=
1
2
N
X
i=1
K
X
k=1
zik log det Λk −1
2
N
X
i=1
K
X
k=1
zikyT
i Λkyi +
N
X
i=1
K
X
k=1
zikµT
k Λkyi −1
2
N
X
i=1
K
X
k=1
zikµT
k Λkµk
The first factor, zik log det Λk, only involves the latent variables xAik = {zik, Λk}. Grouping the terms from the
density that only involve those terms gives
θAik(xAik) := 1
2zik

log det Λk −yT
i Λkyi

.
(23)
Similarly, the term zikµT
k Λkyi involves latent variables xBik = {zik, µk, Λk}. Grouping all of the terms that involve
xBik gives
θBik(xBik) := zik

µT
k Λkyi −1
2µT
k Λkµk

.
(24)
Now, the density function can be written in factor form as
log p(y|z, µ, Λ) =
X
f∈F1
θf(xf),
(25)
where F1 = {Aik, Bik}N,K
i=1,k=1.
Term 2.
The log-density function of p(z|π) is
log p(z|π) =
N
X
i=1
K
X
k=1
zik log πk.
(26)
The factors only involve latent variables xCik = {zik, πk} and the factors are
θCik(xCik) := zik log πk.
(27)
The log-density function can then be written as
log p(z|π) =
X
f∈F2
θf(xf),
(28)
where F2 = {Cik}N,K
i=1,k=1.
Term 3.
The log-density function of p(π|α0) is
log p(π|α0) = log Γ(
k
X
k=1
α0k) −
K
X
k=1
log Γ(α0k) +
K
X
k=1
(α0k −1) log πk.
(29)
The factors in this log-density only involve singleton latent variables, so they can be written as
θπk = (α0k −1) log πk
(30)
and the log-density is
log p(π|α0) =
X
v∈V1
θv(v),
(31)
where V1 = {π1, . . . , πK}.
15


--- Page 16 ---
MAP VIA BENDERS’ DECOMPOSITION
Term 4.
The log-density function is
log p(µ|Λ; β0)
=
K
X
k=1

−D
2 log(2π) −1
2 log det(β0Λk)−1 −β0
2 (µk −µ0)T Λ(µk −µ0)

+ =
K
X
k=1
1
2 log det(β0Λk) −β0
2 µT
k Λkµk + β0µT
k Λkµ0 −β0
2 µT
0 Λkµ0

=
1
2
K
X
k=1
log det(β0Λk) −β0
2
K
X
k=1
µT
k Λkµk + β0
K
X
k=1
µT
k Λkµ0 −β0
2
K
X
k=1
µT
0 Λkµ0.
The first term only involves a single latent variable and the log-density can be reformulated by defining
θΛk := 1
2 log det(β0Λk) −β0
2
K
X
k=1
µT
0 Λkµ0
(32)
The second term involves xDk = {µk, Λk}. Grouping all the terms involving those latent variables gives
θDk(xDk) := −β0
2 µT
k Λkµk + β0µT
k Λkµ0.
(33)
The log-density function can then be written as
log p(µ|Λ; β0) =
X
v∈V2
θv(xv) +
X
f∈F3
θf(xf),
(34)
where V2 = {Λ1, . . . , ΛK} and F3 = {Dk}K
k=1.
Term 5.
The log-density function of p(Λ) is
log p(Λ; W0, ν0) =
K
X
k=1
log B(W0, ν0) + (ν0 −D −1)
2
log det Λk −1
2 Tr(W −1
0
Λk),
where
B(W0, ν0) =
"
det W ν0/2
0
2ν0D/2πD(D−1)/4
D
Y
d=1
Γ
ν0 + 1 −d
2
#−1
.
The factors in this log-density only involve singleton latent variables, so they can be written as
θΛk+ = ν0 −D −1
2
log det Λk −1
2 Tr
 W −1
0
Λk

(35)
and the log-density is
log p(Λ; W0, ν0) =
X
v∈V2
θv(v),
(36)
where V2 = {Λ1, . . . , ΛK}.
Combining factors.
Recall ϕ = (α0, W0, ν0, µ0, β0),
x = (z11, . . . , zNK, µ1, . . . , µK, Λ1, . . . , ΛK, π1, . . . , πK), and y = (y1, . . . , yN). Combining the factorized represen-
tations of each of the terms in the joint log-density gives
p(x|y, ϕ) = f(x, θ) =
X
v∈V
θv(xv) +
X
f∈F
θf(xf),
(37)
16


--- Page 17 ---
MAP VIA BENDERS’ DECOMPOSITION
where V = {z11, . . . , zNK, µ1, . . . , µK, Λ1, . . . , ΛK, π1, . . . , πK}, and
F = {[Aik]K,N
k=1,i=1, [Bik]K,N
k=1,i=1, [Cik]K,N
k=1,i=1, [Dk]K
k=1}. The factors are
θπk(πk)
=
(α0k −1) log πk,
(38)
θΛk(Λk)
=
1
2 log det(β0Λk) −β0
2
K
X
k=1
µT
0 Λkµ0
+ν0 −D −1
2
log det Λk −1
2 Tr
 W −1
0
Λk

,
(39)
θAik(xAik)
=
1
2zik

log det Λk −yT
i Λkyi

,
(40)
θBik(xBik)
=
zik

µT
k Λkyi −1
2µT
k Λkµk

,
(41)
θCik(xCik)
=
zik log πk,
(42)
θDk(xDk)
=
−β0
2 µT
k Λkµk + β0µT
k Λkµ0.
(43)
The non-singleton factors are θAik(xAik), θBik(xBik), θCik(xCik) and θDk(xDk). There are |F| = 3NK + K non-
singleton factors. The dimension of the posterior space is |x| = NK + 3K.
B
BGMM in a factor graph form does not satisfy the conditions for Benders’ decomposition
Recall the MAP inference problem for BGMM in factor graphs is
MAP(θ)
=
max
x,xF
X
v∈V
θv(xv) +
X
f∈F
X
v∈f
θf(xf
v)
subject to
xv = xf
v,
∀v ∈f, f ∈F,
and generalized Bender’s decomposition solves problems of the form
max
x,y
f(x, y)
subject to
G(x, y) ≥0,
x ∈X, y ∈Y,
where the following conditions on the objective and constraints hold:
1. for a fixed y, f(x, y) separates into independent optimization problems each involving a different subvector of
x,
2. for a fixed y, f(x, y) is of a special structure that can be solved efficiently, and
3. fixing y renders the optimization problem concave in x.
Let xF and x in the MAP inference problem be x and y in the generalized Benders’ decomposition problem respectively.
Assume that x is fixed. If x is fixed, the Hessian matrix, H, is















zAik
ik
ΛAik
k
zBik
ik
µBik
k
ΛBik
k
zCik
ik
πCik
k
µDik
k
ΛDik
k
zAik
ik
0
(1)
0
0
0
0
0
0
0
ΛAik
k
(1)
(2)
0
0
0
0
0
0
0
zBik
ik
0
0
0
(3)
(5)
0
0
0
0
µBik
k
0
0
(3)
(4)
(6)
0
0
0
0
ΛBik
k
0
0
(5)
(6)
0
0
0
0
0
zCik
ik
0
0
0
0
0
0
(7)
0
0
πCik
k
0
0
0
0
0
(7)
(8)
0
0
µDik
k
0
0
0
0
0
0
0
(9)
(10)
ΛDik
k
0
0
0
0
0
0
0
(10)
0















, where
17


--- Page 18 ---
MAP VIA BENDERS’ DECOMPOSITION
(1) = 1
2
N
X
i=1
K
X
k=1
log Tr(adj(ΛAik
k
)) −Tr(yiyT
i )
;
(2) =
∂
∂ΛAik
k
1
2
N
X
i=1
K
X
k=1
zAik
ik (log Tr(adj(ΛAik
k
)) −Tr(yiyT
i ))
(3) =
N
X
i=1
K
X
k=1
ΛBik
k
yi −µBik
k
T ΛBik
k
;
(4) = −
N
X
i=1
K
X
k=1
zBik
ik ΛBik
k
(5) =
N
X
i=1
K
X
k=1
µBik
k
T yi −1
2 Tr(µBik
k
µBik
k
T )
;
(6) =
N
X
i=1
K
X
k=1
zBik
ik (yi −µBik
k
)
(7) =
N
X
i=1
K
X
k=1
1
πCik
k
;
(8) = −
N
X
i=1
K
X
k=1
zCik
ik
πCik
k
2
(9) = −β0
K
X
k=1
ΛDik
k
;
(10) = −β0
K
X
k=1
µDik
k
−µ0,
where adj(A) is the adjugate of the matrix A.
Considering H as a block matrix, the eigenvalues of H are collections of each block’s eigenvalues. Consider the first
block as
H1 = {Hij : i ∈1, 2, j ∈1, 2} =

0
(1)
(1)
(2)

.
Then the characteristic equation is
|H1 −λI| =


0
(1)
(1)
(2)

−

λ
0
0
λ
 =


−λ
(1)
(1)
(2) −λ
 = λ2 −(2)λ −(1)2 = 0,
and the two eigenvalues are
(2)±√
(2)2+4(1)2
2
. Because one of the eigenvalues,
(2)+√
(2)2+4(1)2
2
, is always greater than
0, H is not negative definite and thus this violates the condition 3 of the generalized Bender’s decomposition.
C
LDA posterior density in a factor graph form
Term 1.
The log-density function of p(w|z, β) is log p(w|z, β) = PM
d=1
PN
n=1
PV
v=1 wdnv log βzdn, where wdnv is
a binary representation of v-categorical variable of wdn.
The term involves latent variables xAdnk = {zdn, βk} and the factors are θAdnk(xAdnk) := wdn log βzdn. and the
log-density can be written as log p(w|z, β) = P
f∈F1 θf(xf), where F1 = {Adnk}M,N,K
d=1,n=1,k=1.
Term 2.
The log-density function of p(z|Θ) is log p(z|Θ) = PM
d=1
PN
n=1
PK
k=1 zdnk log Θdk where zdnk is a binary
representation of k-categorical variable of zdn. The term involves latent variables xBdnk = {Θdk, zdnk} and the factors
are θBdnk(xBdnk) := zdnk log Θdk. The log-density function can then be written as log p(z|Θ) = P
f∈F2 θf(xf),
where F2 = {Bdnk}M,N,K
d=1,n=1,k=1.
Term 3.
The log-density function of p(Θ; α0) is log p(Θ; α0) ∝PM
d=1
PK
k=1(α0k −1) log Θdk. The factors in
this log-density only involve singleton latent variables, so they can be written as θΘdk = (α0k −1) log Θdk and the
log-density is log p(Θ; α0) = P
v∈V1 θv(v), where V1 = {Θ11, . . . , ΘMK}.
Term 4.
The log-density function of p(β; η0) is log p(β; η0) ∝PK
k=1
PV
v=1(η0v −1) log βkv. The factors in this
log-density only involve singleton latent variables, so they can be written as θβkv = (η0v −1) log βkv and the log-density
is log p(β; η0) = P
v∈V2 θv(v), where V2 = {β11, . . . , βKV }.
18


--- Page 19 ---
MAP VIA BENDERS’ DECOMPOSITION
Combining factors
Recall ϕ
=
(α0, η0), x
=
(z11, . . . , zMN, Θ1, . . . , ΘM, β1, . . . , βK), and w
=
(w11, . . . , wMN). Combining the factorized representations of each of the terms in the joint log-density gives
p(x|w, ϕ) = f(x, θ) =
X
v∈V
θv(xv) +
X
f∈F
θf(xf),
(44)
where V = {Θ11, . . . , ΘMK, β11, . . . , βKV }, and F = {[Adnk]M,N,K
d=1,n=1,k=1 [Bdnk]M,N,K
d=1,n=1,k=1}. The factors are
θβkv
=
(η0v −1) log βkv,
(45)
θΘdk(Θdk)
=
(α0k −1) log Θdk,
(46)
θAdnk(xAdnk)
=
wdn log βzdn,
(47)
θBdnk(xBdnk)
=
zdnk log Θdk.
(48)
D
LDA in a factor graph form does not satisfy the conditions for Benders’ decomposition
Recall the MAP inference problem for BGMM in factor graphs is
MAP(θ)
=
max
x,xF
X
v∈V
θv(xv) +
X
f∈F
X
v∈f
θf(xf
v)
subject to
xv = xf
v,
∀v ∈f, f ∈F,
and generalized Bender’s decomposition solves problems of the form
max
x,y
f(x, y)
subject to
G(x, y) ≥0,
x ∈X, y ∈Y,
where the following conditions on the objective and constraints hold:
1. for a fixed y, f(x, y) separates into independent optimization problems each involving a different subvector of
x,
2. for a fixed y, f(x, y) is of a special structure that can be solved efficiently, and
3. fixing y renders the optimization problem concave in x.
Let xF and x in the MAP inference problem be x and y in the generalized Benders’ decomposition problem respectively.
Assume that x is fixed. If x is fixed, the Hessian matrix, H, is




zAdnk
dn
βAdnk
k
ΘBdnk
dk
zBdnk
dnk
zAdnk
dn
(1)
(2)
0
0
βAdnk
k
(2)
(3)
0
0
ΘBdnk
dk
0
0
(4)
(5)
zBdnk
dnk
0
0
(5)
0



, where
(1)
=
M
X
d=1
N
X
n=1
V
X
v=1
wdnv[ z′′
dn
βzdn
−(z′
dn)2β−2
zdn]
(For simple notation ’Adnk’ is omitted)
(2)
=
−
M
X
d=1
N
X
n=1
V
X
v=1
wdnvz′
dnβ−2
zdn
(For simple notation ’Adnk’ is omitted)
(3)
=
−
M
X
d=1
N
X
n=1
V
X
v=1
wdnvβ−2
zdn
(For simple notation ’Adnk’ is omitted)
(4)
=
−
M
X
d=1
N
X
n=1
K
X
k=1
zBdnk
dnk
ΘBdnk
dk
2
(5)
=
M
X
d=1
N
X
n=1
K
X
k=1
1
ΘBdnk
dk
.
19


--- Page 20 ---
MAP VIA BENDERS’ DECOMPOSITION
Running time (s)
Optimality Guaranteed
Var Bayes
0.064
(N/A)
Sampler
74.250
(N/A)
GBD
4074.856
epoch2, iter6
(a) iris
Running time (s)
Optimality Guaranteed
Var Bayes
0.017
(N/A)
Sampler
94.99
(N/A)
GBD
3238.427
epoch4, iter4
(b) wine
Running time (s)
Optimality Guaranteed
Var Bayes
0.044
(N/A)
Sampler
191.504
(N/A)
GBD
9837.053
epoch3, iter3
(c) brca
Table 5: Comparison of clustering methods by running time and optimality guarantee.
Considering H as a block matrix, the eigenvalues of H are collections of each block’s eigenvalues. Consider the second
block as
H2 = {Hij : i ∈3, 4, j ∈3, 4} =

(4)
(5)
(5)
0

.
Then the characteristic equation is
|H2 −λI| =


(4)
(5)
(5)
0

−

λ
0
0
λ
 =


(4) −λ
(5)
(5)
−λ
 = λ2 −(4)λ −(5)2 = 0,
and the two eigenvalues are
(4)±√
(4)2+4(5)2
2
. Because one of the eigenvalues,
(4)+√
(4)2+4(5)2
2
, is always greater than
0, H is not negative definite and thus this violates the condition 3 of the generalized Bender’s decomposition.
E
Running time and Optimality for the BGMM
Table 5 shows the running time and optimality guarantees for all three methods under consideration. The running time
of the three methods shows a stark difference between them across all three data sets. variational Bayes is the fastest as
expected and the Gibbs sampler, which is run for 1,000 iterations, is also quicker than GBD mainly because the number
of data points in consideration is not very large. Our proposed method, while not as quick as some of the other methods
presented, still performs the best in terms of log MAP and most importantly guarantees optimality which the other two
methods don’t.
F
Perplexity analysis for LDA
The typical way to compare multiple LDA models involves assessing the probability of a set of held-out test documents,
given an already trained LDA model [49]. In our context, given estimates of βk (the distribution of words in topic k)
and Θd (distribution of topics in document d), we calculate the posterior predictive likelihood of all words in the test set
20


--- Page 21 ---
MAP VIA BENDERS’ DECOMPOSITION
Log Likelihood
Perplexity
Var Bayes
-2237.730
177.670
Sampler
-2220.319
170.652
GBD
-2271.184
191.976
Table 6: Comparison of variational Bayes, Gibbs sampler, and GBD using log-likelihood and perplexity
(a) iris
(b) wine
(c) brca
Figure 5: Trace plots of sampled mixture weights from the Gibbs sampler for all three data sets
[36]. Given a set of test documents Dtest and an estimated model (β, Θ), the log-likelihood is defined as [29]:
lDtest(β, Θ) :=
Dtest
X
d=1
log p(wd|β, Θ) =
Dtest
X
d=1
Nd
X
i=1
log
K
X
k=1
βkvΘdk
with wdi = v. Hence, the perplexity is
perplexity := exp
 
−lDtest(β, Θ)
PDtest
d=1 Nd
!
,
where lower values of perplexity signify a better model. In this case, wd indicates the vector of word assignments in a
document d, v is a word type, and Nd is the number of word tokens in document d. Important to note here is the fact
that Θd is unknown for the set of test documents, Dtest. Hence, following the methodology developed by [3], we run the
methods under consideration on the first half of each test document and then compute the perplexity of the held-out
data (the second half of each document) based on the trained model’s posterior predictive distribution.
For this experiment, we randomly sample 300 documents from the 20 newsgroups data set and run all methods under
consideration on a list of 5 topics and 150 most frequently used words within the selected data. Next, we assign ninety
percent of the documents to our training set and ten percent to the test set. Table 6 shows the log-likelihood and the
perplexity values for those thirty selected test documents, i.e. Dtest = 30. For the LDA model, we observe that all
three methods under consideration show similar performance. The log-likelihood and perplexity scores for each method
are fairly close to one another which is also reflected in Table 4 where the top ten words assigned to two topics shows a
high degree of similarity between the three methods under consideration.
G
Convergence Analysis for the BGMM Gibbs Sampler
In this section, we present a convergence analysis for the Gibbs sampler of the BGMM model. We ran our sampler for
1,000 iterations with a burn-in period of 100 iterations. In figure 5 we observe that the sampled mixture weights for
all three standard data sets converge very quickly over 1,000 iterations. Hence, to find the mode of the distribution
over these 1,000 iterations, we multiply the sampled weights by a factor to ensure that we have integer values and then
take the mode over those integer values. Similarly, we observe in figures 6, 7, and 8 that the sampled means for each
component across all three data sets converge very quickly over 1,000 iterations. In this case as well, to find the mode
of the distribution over these 1,000 iterations, we multiply the sampled mean components by a factor to ensure that we
have integer values and then take the mode over those integer values.
21


--- Page 22 ---
MAP VIA BENDERS’ DECOMPOSITION
Figure 6: Trace plots of sampled mean components from the Gibbs sampler for iris
Figure 7: Trace plots of sampled mean components from the Gibbs sampler for wine
Figure 8: Trace plots of sampled mean components from the Gibbs Sampler for brca
22
