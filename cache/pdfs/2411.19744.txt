--- Page 1 ---
2024-12-2
Amplifying human performance in
combinatorial competitive programming
Petar VeliÄkoviÄ‡*,1, Alex Vitvitskyi*,1, Larisa Markeeva*,1, Borja Ibarz*,1, Lars Buesing1, Matej Balog1 and
Alexander Novikov*,1
*Equal contributions, 1Google DeepMind
Recent years have seen a significant surge in complex AI systems for competitive programming, capable
of performing at admirable levels against human competitors. While steady progress has been made,
the highest percentiles still remain out of reach for these methods on standard competition platforms
such as Codeforces. Here we instead focus on combinatorial competitive programming, where the target
is to find as-good-as-possible solutions to otherwise computationally intractable problems, over specific
given inputs. We hypothesise that this scenario offers a unique testbed for human-AI synergy, as human
programmers can write a backbone of a heuristic solution, after which AI can be used to optimise the
scoring function used by the heuristic. We deploy our approach on previous iterations of Hash Code, a
global team programming competition inspired by NP-hard software engineering problems at Google,
and we leverage FunSearch to evolve our scoring functions. Our evolved solutions significantly improve
the attained scores from their baseline, successfully breaking into the top percentile on all previous
Hash Code online qualification rounds, and outperforming the top human teams on several. Our method
is also performant on an optimisation problem that featured in a recent held-out AtCoder contest.
Introduction
Competitive programmingâ€”the art of writing
highly specialised code for solving challenging
problems under various constraintsâ€”represents
a critical test of advanced reasoning skills. It has
sparked interest as an evaluation of AI systemsâ€”
with systems like AlphaCode (Li et al., 2022)
demonstrating good performance given a lan-
guage model equipped with a filtering procedure.
In recent years, capabilities of AI-powered
competitive programming systems have been
steadily increasingâ€”breaking into and beyond
the 85th percentile on platforms such as Code-
forces (Leblond et al., 2023; OpenAI, 2024),
largely relying on improved base model capabili-
ties, data curation and test-time compute.
While such results are certainly impressive,
they are still insufficient for consistently outper-
forming the highest echelons of human compet-
itive programmers. The largest reported Code-
forces ELO rating for an AI system at the time
of writing is still below 1, 900. The most pow-
erful human competitors are capable of signif-
icantly higher feats than that, with one â€“ Gen-
nady Korotkevich (tourist) â€“ achieving ELO
above 4, 0001. It is our opinion that further break-
throughs are needed before AI systems reach such
levels at Codeforces-style competitions.
Meanwhile, another type of impressive AI sys-
tem has recently emerged: leveraging evolution-
ary algorithms for searching in the function space
(Romera-Paredes et al., 2024, FunSearch), ex-
pressed using code implementations written by
language models. We believe that FunSearch
opens up an opportunity for a different kind of
top-level competitive programming result: one
on combinatorial optimisation challenges, made
in collaboration with human competitors.
We present a successful result of this kind: by
applying FunSearch on a human-designed solu-
tion backbone across several Hash Code compe-
tition tasks, we are able to significantly amplify
the scores obtained by the backbone. In several
contests, we recover solutions that would have
outperformed the top human teams. We also val-
idate our method on a variant of a recently-held
AtCoder Heuristic Contest.
1https://codeforces.com/profile/tourist
Corresponding author(s): petarv@google.com, anovikov@google.com
Â© 2024 Google DeepMind. All rights reserved
arXiv:2411.19744v1  [cs.LG]  29 Nov 2024


--- Page 2 ---
Amplifying human performance in combinatorial competitive programming
Combinatorial competitive program-
ming
Before diving into the specifics of our approach,
we briefly discuss why combinatorial tasks are a
more immediate fit for present-day AI systems to
achieve top-tier levels, compared to Codeforces.
Codeforces-style tasks typically require writing
tractable, polynomial-time algorithmsâ€”with no
reward given for partially correct or inefficient
solutions. As many of the testcases for such tasks
rely on hidden edge cases, it is generally not suffi-
cient for AI to produce code that works correctly
in most casesâ€”careful understanding is required
of many constraints which might not be obvious
at all from the task statement. Further, many such
tasks follow specialised design patterns, which
well-versed human competitors readily recognise
and applyâ€”making it harder for AI to catch up.
In contrast, combinatorial optimisation prob-
lems are typically intractable, NP-hard problems,
with testcases for which optimal solutions are usu-
ally unknown. As such, valid-but-suboptimal so-
lutions are inevitable, and they are all rewarded
depending on their measured performance on
these testcases. Further, the testcases are often
visible to the competitors, avoiding any hidden
edge-case situations. In summary, even if it might
seem counter-intuitive, combinatorial optimisa-
tion competitions offer a more immediate oppor-
tunity for exceptional results: they do not punish
suboptimality, all cases of importance are known
upfront, and even the strongest human competi-
tors do not know how to find optimal solutions.
Next, we describe the specific competitionâ€”
Hash Codeâ€”we used to substantiate this claim.
Hash Code competitions
Hash Code is a former Google-organised global
(2â€“4 person) team programming competition. It
features NP-hard optimisation problems inspired
by software engineering tasks at Google. Teams
produce outputs for a given set of inputs to an
intractable optimisation problem, and are evalu-
ated on the quality of their outputs. At the peak
of its popularity, it was globally renowned as one
of the most popular combinatorial optimisation
Competitor
Input
Parser
Backbone
+
Score fn.
Eval
FunSearch
Evolved
score fn.
Figure 1 | High-level overview of the collaborative
competitor + AI approach explored in our work.
competitions, inviting over 125,000 competitors,2
and featuring some of the most successful com-
petitive programmersâ€”including tourist.3
Hash Code is, at its core, a contest requiring
design of a strong heuristic.
Top teams tend
to write scoring functions for a locally-optimal
greedy search, often followed by randomised hill-
climbing. We believe devising strong scoring func-
tions for greedy search is highly non-trivial, and
hence amenable to automated genetic program-
ming tools such as FunSearch. Under the Hash
Code rules, use of external tools (e.g. combinato-
rial solver packages) was allowed, which would
have made our approach legal at the time as well.
Hash Code has had two phases in most yearsâ€”
first, an online qualification round, followed by a
final round for the top âˆ¼50 teams in the qualifica-
tion. For our purposes, we use only the qualifica-
tions, as they have a much wider pool of competi-
tors to compare against, with their tasks still be-
ing intractable in their nature. We access all prob-
lems and their inputs at https://github.com/
google/coding-competitions-archive.
Experimental setup
Now we can dive deeper into how we leveraged
FunSearch to amplify typical strategies that com-
petent competitors may leverage in such contests.
Please refer to Figure 1 for a high-level overview.
2www.youtube.com/watch?v=YPOVd-hQUjA
3https://x.com/que_tourist/status/
1230760203903070208
2


--- Page 3 ---
Amplifying human performance in combinatorial competitive programming
def score_greedy(project: Project,
role_id: int,
assignments: list[int, list[int]],
rate_project: bool) -> int:
"""Scorer function for the greedy algorithm.
Return either the value of choosing a particular project, or the value of a
particular role within that project, conditioned on the already scheduled
project assignments. The behaviour is controlled by a boolean, `rate_project`:
if it is True, we rate the provided project, if it is False, we rate the
provided role ID.
Args:
project: the Project currently considered for choosing.
role_id: the currently considered role ID from the current project.
assignments: the already-scheduled project assignments thus far.
rate_project: whether weâ€™re scoring a project (True) or a role (False).
Returns:
score
"""
if rate_project:
return 1
else:
skill, level = project.roles[role_id]
return 1
Figure 2 | The base scoring function used within one of the backbones for the Hash Code 2022
Qualification Round (Mentorship and Teamwork). Note the split on the rate_project variable in
order to enable two different choice points to be evolved within the same scoring function.
Overall workflow
The workflow of our approach is as follows:
â€¢ We implement a backbone of a greedy algo-
rithm that tackles the given problem, along
with functions to parse the input file(s) and
evaluate the fitness of candidate solutions;
â€¢ The greedy algorithm depends on a scoring
function that weighs in on each of its possible
next steps. Initially, we can make this a sim-
ple functionâ€”see Figure 2 for an example;
â€¢ We use FunSearch to evolve this function,
using the number of points achieved on a
given testcase as the fitness function;
â€¢ When relevant, we may analyse the outputs
of FunSearch to iterate on the backbone
structure, strap on local search,4 and sim-
ilar.
â€¢ Note that we consider each of the provided
inputs as a separate problem; where appro-
priate, different inputs to the same qualifica-
tion round may feature different backbones.
4Local search was only used on one previous testcase of
one previous problemâ€”testcase d_tough_choices.txt
of the 2020 Hash Code Qualification.
This approach, in fact, closely mimics what a real
competitorâ€™s workflow during Hash Code might
look like; the main part being automated is the
evolution of the scoring function, which is a hard
task for humans.
Note that the collaboration between humans
and AI was important to achieve the results with
this workflowâ€”the competitors can leverage their
strengths and develop the backbone, whereas
AI can automate the complex search in program
space (likely to be necessary unless P = NP) that
can extract higher returns out of the backbone.
Evolving heuristics
Next, we describe the methodology for evolving
the scoring functions used for Hash Code.
Introduction to FunSearch
FunSearch (Romera-Paredes et al., 2024) is an
optimisation technique that pairs a pre-trained
large language model (LLM) with a systematic eval-
uator. It is designed to address the limitations of
LLMs, such as confabulations, through its usage
3


--- Page 4 ---
Amplifying human performance in combinatorial competitive programming
of the evaluator to check the correctness of the
LLMâ€™s output.
FunSearch operates by evolving a population
of candidate programs over multiple iterations.
The LLM proposes modifications or improvements
to the programs, and the evaluator assesses the
quality of these modifications. The process can in
principle continue without termination, though
in practice we would terminate it once either a
desirable high-quality program is found, or no
significant progress has been observed over a suf-
ficient time frame.
The success of FunSearch is attributed to sev-
eral key components, including best-shot prompt-
ingâ€”where the best-performing programs are
fed back into prompts for the LLM to improve
uponâ€”and the use of program backbones that fo-
cus the LLMâ€™s attention only on evolving critical
program logic, leaving the backbone itself fixed.
Additionally, FunSearch employs an island-based
evolutionary method to maintain program diver-
sity and avoid local optima. The asynchronous
and parallel nature of FunSearch allows for effi-
cient exploration of the program space, especially
across multiple devices.
Configuration specifics
The setup and hyperparameters of our evolu-
tionary program search largely match Romera-
Paredes et al. (2024). There are, however, three
key differences, specifically tuned towards the
competitive programming setting and latest de-
velopments in LLMs. We describe them here:
â€¢ While Romera-Paredes et al. (2024) rely on
a variant of PaLM 2 (Anil et al., 2023) specif-
ically fine-tuned on code, we find that the
code-writing capabilities of modern general-
ist fine-tuned LLMs are sufficient to not re-
quire dedicated fine-tuning anymore.
As
such, we used Gemini 1.5 Flash 002 (Gemini
Team et al., 2024) as the LLM that proposes
modifications to programs in the population.
â€¢ As combinatorial contests often feature very
large inputs â€“ in order to make exhaustive
search intractable â€“ we needed to increase
the evaluation limits for the generated pro-
grams from their defaults in Romera-Paredes
et al. (2024). Specifically, we limit memory
usage of the entire program to 10 GB, and
wall-clock execution time to 1, 800 seconds.
â€¢ It is rather common that greedy solutions
to combinatorial contests require multiple
choice points. This is in contention with the
structure of Romera-Paredes et al. (2024),
which instead only optimises one scoring
function. We are able to practically work
around this by introducing a switching vari-
able that the scoring function can branch on
depending on which choice point needs to be
scored at this point in time. This can be seen
in Figure 2, where the switching variable is
rate_project and it determines whether
weâ€™re scoring a project or a project-role pair.
The backbone takes care of invoking the scor-
ing function with the correctly set switching.
Results
We deploy our approach on all eight Hash Code
online qualification rounds (from 2015 until
2022). The main results, which we present in
Figure 3, will aim to quantify two key elements:
Can FunSearch deliver meaningful improve-
ments in the combinatorial competitive pro-
gramming setting? To measure this, we will
compare the scores and ranks we obtain against
the ones achieved by the backbone solution (with
any iterative improvements applied) and our ini-
tial scoring function alone. We can assess the
significance of this improvement by checking ei-
ther the percentiles achieved, or whether the re-
covered scores would be sufficient to qualify in
the finals (i.e. in the top 50 ranks5 that year). It
is worth noting that, prior to 2019, Hash Code
online qualification results comprised fewer than
5, 000 participating teams, meaning that achiev-
ing the top percentile was harder than qualifying
to the finalsâ€”afterwards it became easier.
Could these improvements be meaningfully
obtained under contest conditions? To mea-
sure this, we will also compare against the scores
and ranks obtained after only running the evo-
lutionary computation for two hours. Given that
5Note that this is an approximationâ€”the exact number
of finalist teams varied between 38 and 65 across the years.
4


--- Page 5 ---
Amplifying human performance in combinatorial competitive programming
0%
20%
40%
60%
80%
100%
Percentile of contestants below score
0%
20%
40%
60%
80%
100%
Normalized score
Finalists
Backbone
2h
Hash Code 2015 Qualification: Optimize a Data Center
Human contestants
0%
20%
40%
60%
80%
100%
Percentile of contestants below score
0%
20%
40%
60%
80%
100%
Normalized score
Finalists
Backbone
2h
Hash Code 2016 Qualification: Delivery
Human contestants
0%
20%
40%
60%
80%
100%
Percentile of contestants below score
0%
20%
40%
60%
80%
100%
Normalized score
Finalists
Backbone
2h
Hash Code 2017 Qualification: Streaming videos
Human contestants
0%
20%
40%
60%
80%
100%
Percentile of contestants below score
0%
20%
40%
60%
80%
100%
Normalized score
Finalists
Backbone
2h
Hash Code 2018 Qualification: Self-driving rides
Human contestants
0%
20%
40%
60%
80%
100%
Percentile of contestants below score
0%
20%
40%
60%
80%
100%
Normalized score
Finalists
Backbone
2h
Hash Code 2019 Qualification: Photo slideshow
Human contestants
0%
20%
40%
60%
80%
100%
Percentile of contestants below score
0%
20%
40%
60%
80%
100%
Normalized score
Finalists
Backbone
2h
Hash Code 2020 Qualification: Book scanning
Human contestants
0%
20%
40%
60%
80%
100%
Percentile of contestants below score
0%
20%
40%
60%
80%
100%
Normalized score
Finalists
Backbone
2h
Hash Code 2021 Qualification: Traffic signaling
Human contestants
0%
20%
40%
60%
80%
100%
Percentile of contestants below score
0%
20%
40%
60%
80%
100%
120%
140%
Normalized score
Finalists
Backbone
2h
Hash Code 2022 Qualification: Mentorship and Teamwork
Human contestants
Figure 3 | Rankings and scores of our backbone solutions with base scoring functions, and solutions
evolved by FunSearch, across all eight Hash Code online qualification rounds. We plot the Hash Code
fitness scores obtained by human competitor teamsâ€”normalised to the [0, 1] range by dividing by
the best teamâ€™s score per contestâ€”against the teamsâ€™ rank in the contest. We then compute the fitness
scores obtained by the backbone with base scoring function, as well as the best fitness we were able
to achieve after evolving (as â€œâˆâ€) and the fitness scores obtained after no more than two hours of
evolving (as â€œ2hâ€). We report these scores on the ranking axis, and compare them against the ranks
required to qualify into the finals. Our evolved solutions are consistently ranked in the top percentile,
and outperform the top-scoring human team in five iterations (2015, 2018, 2020, 2021 and 2022).
5


--- Page 6 ---
Amplifying human performance in combinatorial competitive programming
Hash Code had a time limit of four hours, this
leaves two hours for implementing the essential
three parts of each backbone:
â€¢ Parsing the input file into appropriate data
class objects for further processing;
â€¢ A greedy algorithm with a base scoring func-
tion to produce candidate solutions;
â€¢ The systematic evaluator6 of candidate solu-
tions, to be used as a fitness function;
Given that the combined backbone implementa-
tions we develop never exceeded âˆ¼400 lines of
Python code including docstrings (and were usu-
ally around âˆ¼200 lines)â€”and Hash Code teams
comprise up to four membersâ€”we find this to be
a reasonable undertaking for a team of competent
competitive programmers. As a reference, within
two hours, our method evaluates around 10, 500
programs on average across all backbones.
Result analysis and discussion
The results outlined in Figure 3 provide conclu-
sive evidence towards settling our two questions
positively. Specifically, we note that:
â€¢ The backbones with base scoring functions
are not capable of achieving the top per-
centile or a finals qualification in any Hash
Code year we investigated;
â€¢ The evolved scoring functions make signifi-
cant progress compared to the base scoring
function in terms of both rank and fitness;
â€¢ The evolved functions reach the top per-
centile of competing teams, as well as sur-
passing the rank required to qualify in the
finals, for every Hash Code iteration.
Beyond this, our evolved solutions were capable
of outperforming the rank-1 team on five online
qualification rounds: 2015 (Optimize a Data Cen-
ter), 2018 (Self-driving rides), 2020 (Book scan-
ning), 2021 (Traffic signaling) and 2022 (Mentor-
ship and Teamwork). This is clearly a result that
6While it may be argued that this part is not strictly
necessary, as the contest environment already provides an
evaluator, there are strict limits on how often solutions can
be submitted to the evaluation server, and providing such an
evaluator within the prompt allows FunSearch a significantly
richer picture of how its solutions will be evaluated.
is beyond reach of the backbone developers (i.e.
the authors of this paper) without AI assistance.
It is interesting to note that the starting point of
our backbone solutions varies significantly across
different Hash Code iterations. To give just two
examples: in 2018, they start below the 20th per-
centile; while in 2022, they start above the 90th.
This phenomenon nicely illustrates the diversity
in challenge and contestant cohorts offered in var-
ious Hash Code iterations. The complexity of writ-
ing a working backbone may in and of itself pose
quite a challengeâ€”for example, writing an appro-
priate evaluator and greedy algorithm proved a
significant undertaking in Hash Code 2016 (for
reference, our own implementation has 415 lines
of Python). Accordingly, having a working and
complete backbone allowed for a relatively high
rank compared to the competition cohort.
Further, after Hash Code significantly rose
in popularityâ€”especially from 2019 onwardâ€”a
longer tail of competitor teams with low relative
scores started to emerge. This naturally uplifted
the percentile of any backbone solution which
achieves a modest relative score. For example,
achieving a normalised score of 0.6 would be in-
sufficient even for the 30th perecentile in Hash
Code 2018, while it is sufficient for the 90th per-
centile in Hash Code 2022.
We also believe that many of our methodâ€™s
benefits should be recoverable under contest
conditionsâ€”considering the proximity of the
â€œtwo-hoursâ€ solution to the maximal fitness we
were able to obtain, and its relative merit against
participating teams. In all but two iterations of
Hash Code, this capped fitness would have been
sufficient to qualify into the finals. Additionally,
as we now know that sufficiently better solutions
are within reach of the method given more compu-
tational resources, we consider that discovering
them under the two-hour time constraint now
amounts to an engineering challenge, rather than
a research one.
A possible limitation of our result is that, since
the fitness on these contests was evaluated post-
hoc rather than in real time, it is not unlikely that
our base Gemini 1.5 Flash 002 model had been
exposed to Hash Code subroutines within its train-
6


--- Page 7 ---
Amplifying human performance in combinatorial competitive programming
ing data. We believe this does not diminish the
significance of our results, and elaborate further
on why that is in the following paragraphs.
Firstly, unlike the context of code generation on
Codeforcesâ€”where the model is asked to output
the entirety of the solutionâ€”in our collaborative
setting the model is prompted to evolve only the
scoring function, in a way that is compatible to the
rest of the backbone. This means that the solution
needs to conform to the API prescribed by the
backbone as well as utilising dataclasses defined
within it, which is a prompt setting completely
unseen in prior training, as our backbones are
not available in LLM pre-training data.
Further, unlike the Codeforces setting, no in-
formation specific to the Hash Code tasks is given
to the model in the promptâ€”only the associated
backbone code and previous best-performing scor-
ing function, which makes it unlikely that a re-
trieval approach can even be successfully invoked
in the first place. The fact that our models do not
generate fully optimised solutions after one LLM
mutation but rather tend to make steady, itera-
tive progress towards improving solution fitness
throughout training is further evidence to the fact
that no final solutions are immediately recalled
by the models. Indeed, most of our optimised so-
lutions required a chain of at least 10â€“30 iterative
improvement calls to Gemini 1.5 Flash 002.
In addition, towards the end of the paper we
will provide a held-out contest case study, on a vari-
ant of the recently held AtCoder Heuristic Contest
039 (which took place on 10 November 2024, af-
ter the release of Gemini 1.5 Flash 002). This will
serve to illustrate how our FunSearch-augmented
method still yields tangible and significant bene-
fits, even over a recent contest with a substantially
different setup than Hash Code.
Evolved Hash Code heuristics: qualita-
tive case studies
We will now present several case studies elaborat-
ing on the solutions evolved by FunSearch across
several (sub)problem backbones in prior Hash
Code qualification rounds. This analysis will both
elucidate the insight discovered within some of
the discovered solutions, and further emphasise
the low likelihood that these solutions could have
been derived through retrieval or recall.
Qualification 2015: Optimizing a Data Center
2015 marked the first Hash Code qualification
round, where the task was to find the most fault-
tolerant way to arrange servers within an abstrac-
tified data center.
Specifically, it is assumed that the data cen-
ter has a certain number of rows, with each row
having a certain number of slots (some of which
may be blocked). Each server has a certain ca-
pacity, can be assigned to a particular pool, and
occupies a certain contiguous number of slots.
The task is to decide on locations where every
server is installed, such that the guaranteed ca-
pacityâ€”the minimal remaining capacity across
all of the pools, should any data center row stop
workingâ€”is optimised.
There
was
exactly
one
input
testcase,
hashcode_2015_qualification_round.txt,
and we focus on it here. We provide the input
parsing and greedy algorithm parts of the
backbone we used in the Appendix (Figure 5).
The greedy backbone operates in two phases:
first, it determines where to place each server,
then it assigns each server to a pool. The servers
are iteratively assigned to each free space in a
row, repeatedly cycling between rows to promote
evenly distributed servers across rows.
Much like in Figure 2, here we really need two
scoring functions: one to decide which server
to place in the current position, and another to
decide which pool to allocate a server to. We han-
dle this using a boolean variable rate_server.
Our base scoring function rates servers based on
their capacity relative to amount of spaces they
occupy, and it rates pools based on how much
would they additionally improve their pool with-
out over-representing it in this particular row:
if rate_server:
return server.capacity / server.size
else:
total_sum = 0
for c_row in pools_per_row:
total_sum += pools_per_row[c_row][pool]
7


--- Page 8 ---
Amplifying human performance in combinatorial competitive programming
return -total_sum + pools_per_row[row][pool]
This scoring function, while simple, achieves a
commendable score of 348 points. After evolving
this function for two hours â€“ roughly remaining
under contest conditions â€“ the optimal function
becomes significantly more elaborate:
assert server.size > 0
assert server.capacity > 0
if pool is not None:
total_sum = pools_per_row[row][pool]
max_sum = 0
pool_size = 0
for p in pools_per_row:
total_sum += pools_per_row[p][pool]
max_sum = max(pools_per_row[p][pool], max_sum)
if p == pool:
pool_size += pools_per_row[p][pool]
if total_sum == 0:
return -100
# Compute max gain after adding 1 server in row:
row_score = -total_sum / server.size + max_sum / server.size
if row not in pools_per_row:
return row_score
else:
pool_score = -0.5 * total_sum / server.size + (
0.5 * max_sum / server.size)
pool_bonus = 0.015 * (total_sum - pool_size)
if server.capacity >= total_sum / 2.0:
pool_bonus *= 1.2
elif server.capacity >= (total_sum - pool_size) / 4.0:
pool_bonus *= 1.5
return (row_score + pool_score + pool_bonus / 1000. + 0.00005 *
(server.capacity / server.size + total_sum / min(
total_sum, server.capacity * 1.1)) - 0.004 * row /
len(pools_per_row) * (
server.size >= total_sum / 10.0) - 0.0004 * pool /
len(pools_per_row) - 0.0007 * server.size / len(
pools_per_row) * (server.capacity >= (
total_sum / 2.0) * 1.005))
elif rate_server:
return server.capacity / server.size * (
2.0 + 2.0 * server.size / 3.0)
else:
raise ValueError(â€™should not call this function with None poolâ€™)
This scoring function significantly improves the
score to 405, sufficient to achieve second place!
This solution is also substantially mutated, re-
quiring a chain of 15 LLM calls starting from the
original. This implementation supports an elabo-
rate algebraic expression for a score function with
evolved constants assigned to each component.
It is also interesting to note that the evolved func-
tion inserts assertions and throws exceptions that
would trigger under invalid inputsâ€”though such
inputs will never appear in practice.
Finally, it is interesting to make note of the
return -100 statement which, while not likely
to make a big difference in this particular setting
(it is only invoked when a pool has no servers
assigned to it yet), it sets up the model for a more
peculiar kind of mechanism to be discovered later.
If we allow for more time for evolving the scor-
ing function, it is possible to achieve a rank-1
score of 413, with the following snippet:
assert row >= 0
cap = 0
prev_row = row - 1
if rate_server:
if server.size > 125:
return -100
else:
if server.size > 23:
cap = 0.5
cap += (2.7 - 4.95 * server.size / 125) * (
server.capacity / 125)
if server.capacity / server.size > 7.5:
cap *= 5.0
if server.capacity / server.size > 7.95:
cap *= 11.0
return cap
else:
n_pools_full = sum(1 if pool_cap > 7357 else (
0 for pool_cap in pools_per_row[row].values()))
assert pool is not None
max_cap = max(pools_per_row[row][pool] / 1150, 0.475)
assert max_cap > 0
total_cap = 1.16 - (1.16 - 1.1) * (0.9 - n_pools_full / 5)
min_cap = 13000.0
for c_row, pool_cap in sorted(pools_per_row.items()):
total_cap += max(pool_cap[pool], 0.1)
if prev_row is not None and (
c_row != row) and (c_row != prev_row):
assert c_row in pools_per_row
assert pool in pools_per_row[c_row]
max_cap = max(max_cap, pools_per_row[c_row][pool])
min_cap = min(min_cap, pools_per_row[c_row][pool])
prev_row = c_row
if min_cap > server.capacity:
min_cap = server.capacity * 0.7
total_cap += max(pools_per_row[row][pool] * 0.95, 0.03)
total_cap -= max(pools_per_row[row][pool] / 650, 0.005)
cap += max(max(
pools_per_row[row][pool], 0.1) / 1.32, 710.0 / (
server.size + 1.0))
assert cap > 0
if server.size > max_cap:
return -100
elif total_cap < server.capacity:
return -10000
else:
return (server.capacity - (total_cap - max_cap)) + cap
This function also implements an elaborate al-
gebraic score expression with evolved constants.
However, it is interesting to note how the neg-
ative numbers are being used in this case. For
example, with the check if server.size >
125:
return -100, the system is implement-
ing a hard threshold which automatically dis-
courages from scheduling all servers occupying
more than 125 slots. Similarly, with the final
if/elif/else chain, it is discouraged to allo-
cate large servers to any particular pool (if),
and even more so to allocate servers to a pool
if they have substantially larger capacity than a
computed cap value (elif).
Qualification 2018: Self-driving rides
Next, we analyse the evolution of the scoring func-
tion for the 2018 Hash Code qualification round,
where the objective is to find an optimal schedule
of rides for a fleet of self-driving cars.
Specifically, we are given a fleet of a certain
number of self-driving cars, which need to com-
8


--- Page 9 ---
Amplifying human performance in combinatorial competitive programming
plete a certain amount of ordered rides. Each ride
order consists of a start and end point, earliest
start time and the latest finish time. Each ride
completed on time accrues a certain number of
points, with a certain bonus obtained for rides
that start in the earliest possible moment. The
task is to decide a schedule in which each of the
cars will complete a certain list of rides sequen-
tially, always driving along the shortest path to
their next destination.
We focus on the testcase d_metropolis.in,
where the model made the most significant jump
in score compared to the backbone. As before,
in the Appendix we provide the key parts of the
backbone we used for this input (Figure 6).
The backbone greedy solution in this case main-
tains a priority queue of all cars in the fleet, keyed
by the time at which they finish their latest ride
(initially zero for all cars). At each step, the top
car in the priority queue is taken, and a ride is
allocated to it. Our base scoring function simply
picks the first feasible ride:
for i, r in enumerate(rides):
pickup_time = time + r.distance_to_start(
coords)
if (pickup_time >= r.earliest_start) and (
pickup_time + (
r.length()) < r.latest_finish
):
return i
# We failed to find any feasible ride.
return -1
and it achieves 3, 528, 556 points on this testcase.
After evolving for two hours, we recover the fol-
lowing intuitive solution:
best_time, best_idx = -1, -1
for i, r in enumerate(rides):
pickup_time = max(
r.earliest_start, time + r.distance_to_start(coords))
if pickup_time + r.length() >= r.latest_finish:
continue
if best_time < 0 or pickup_time < best_time:
best_time, best_idx = pickup_time, i
return best_idx
which corrects for the fact that the pick-up time
doesnâ€™t have to be after the earliest start to be
feasible (the car simply has to wait), and the
loop does not break early when a ride is foundâ€”
instead, the best solution is retrieved based on its
proximity to the carâ€™s present point.
This solution amplifies the performance on this
input significantly, to 11, 739, 630 points.
From this point, a significant amount of time
and additional evolution steps are needed to dis-
cover a useful non-trivial solution.
best_score = float("-inf")
best_ride = -1
free_time = 0
# Rides in descending order by distance to the starting point.
rides_by_length = [(i, r.length(), distance(coords, r.start))
for i, r in enumerate(rides)
if r.latest_finish >= time // 2.]
rides_by_length.sort(reverse=True)
for (i, ride_length, distance_to_start) in rides_by_length:
r = rides[i]
pickup_time = time + distance_to_start
if pickup_time < r.earliest_start:
pickup_time = r.earliest_start
free_time = pickup_time + ride_length
bonus_points = 20000 if time < 3600 or time > 20900 else 0
bonus_points = bonus_points if (
r.latest_finish <= time + 1.5 * ride_length) else 0
if time <= 3600:
bonus_points = bonus_points + 75 * free_time
else:
bonus_points = bonus_points - 7.5 * free_time
if free_time <= r.latest_finish and (
r.earliest_start <= pickup_time):
score = ride_length + bonus_points -\
15 * (abs(
r.start[0] - coords[0]) + abs(
r.start[1] - coords[1])) -\
200 * max([0, pickup_time - time]) -\
1. * sum([
abs(pickup_time - r.earliest_start),
abs(free_time - r.latest_finish),
sum([abs(
r.start[j] - r.end[j]) for j in range(2)])
])
# distance penalty from ending location
score = score + 15 * (
1100 - r.distance_to_start(
coords)) + 90 * r.distance_to_start(coords) / 1200.
# penalty for driving far in the late night and early morning
if rides_by_length[0][0] == i:
score = score - 25 * free_time
if time >= 39480 and free_time > time // 2.:
score = score + 10 * (
free_time - 1100 + r.distance_to_start(coords))
if r.latest_finish <= time + 2 * r.length():
score = score + 8000
if r.length() < 1500:
score == score + 2000
if r.length() > 6000:
score = score - 3000
if r.length() > 7000:
score = score - 3000
if r.length() > 5000:
score = score - 5000
if score > best_score:
best_ride = i
best_score = score
return best_ride
This solution achieves a score of 12, 296, 845
points on the metropolis input, and is gener-
ally sufficient for outperforming the rank-1 team
when aggregating across all inputs.
While it clearly implements a more elaborate
scoring function than its predecessors, it also
makes some peculiar or redundant decisions. For
9


--- Page 10 ---
Amplifying human performance in combinatorial competitive programming
example, while its comment indicates it will sort
rides in descending order by distance, the effect
of the sorting function is to sort in reverse using
the first key â€“ the index. This only has the ef-
fect of processing the feasible rides in the reverse
order from the one that they were given.
Then, the evolved function computes a bonus
which can be allocated for rides without a lot of
margin for error that are queried early or late.
Besides this bonus, several other heuristics, in-
cluding the distance of the car to the starting
point, and whether the endpoint of one ride can
be chained to the start point of another (-200 *
max([0, pickup_time - time])), all factor
into the computed score. There are also several
penalties applied to the score designed to encour-
age choosing shorter rides (e.g. if r.length()
> 6000:
score = score - 3000), with an
interesting no-op command being accidentally
executed in score == score + 2000.
Qualification 2021: Traffic signaling
Finally, we study how the scoring function evolves
on the 2021 Hash Code qualification round,
where the objective is to find an optimal traffic
light schedule at various city intersections.
Specifically, we are given a graph of one-way
streets connecting intersections, as well as a col-
lection of cars making their commuteâ€”for each
car, we are given the list of streets they need to
traverse. At each intersection there is a traffic
light which can only be green for one of its in-
coming streets at a time, at which point it lets
through one waiting car per timestep. The task
is to decide on a traffic light scheduleâ€”in which
order do the traffic lights turn green, and for how
long is each intersection greenâ€”to optimise the
number of cars completing their trips in time.
Further, bonus points are allocated for each car
completing their trip, depending on how quickly
they finish.
For
this
round,
we
focus
on
the
test-
case f_forever_jammed.in, where the model
made the most gradual improvement in score
compared to the backboneâ€”likely due to the
higher incidence of heavily congested intersec-
tions. Just as in the previous examples, we pro-
vide essential parts of our backbone function in
the Appendix (Figure 7).
For this task, the backbone greedy solution re-
lies on two separate predictions for each street:
which position will it have in the traffic light sched-
ule for its incoming intersection, and which dura-
tion will it be green for. These are not computed
at the same time for every street, but rather, a
three-phase solution is executed:
â€¢ First, a simulation of the system is performed
with all the cars, under the assumption the
green light durations will all be 1. Whenever
a car arrives at an intersection that hasnâ€™t
previously been assigned a position, the po-
sition prediction is invoked on it. If the pre-
dicted position is already occupied, it is in-
cremented until a free slot is found.
â€¢ Second, with the predicted positions as-
sumed fixed, the duration is predicted for
every incoming street of every intersection.
â€¢ Finally, another full simulation is performed
under the computed parameters.
Some
cars may fail to finish their trips under
this simulationâ€”if there are any streets for
which only failed cars enter, we remove these
streets from the schedule (i.e., assume a con-
stant red light for them) to avoid these cars
further congesting the traffic.
The base scoring function simply attempts to
place every incoming street in position zero, and
assigns to every street a green light time of one:
if give_pos:
return 0
else:
return 1
Note the variable give_pos which controls
whether we are predicting position or green light
duration. This simple scoring, coupled with the
elaborate backbone, is sufficient to score an im-
pressive 1, 019, 868 points on this input.
We can improve this score significantly after
evolving for two hoursâ€”after a chain of over 20
mutation calls to the LLM, the following scoring
is obtained:
if give_pos:
return int(used_streets[street.name] / 1000 + 0.5)
10


--- Page 11 ---
Amplifying human performance in combinatorial competitive programming
else:
return int(
(used_streets[street.name] * 0.001 * curr_size + 0.1)
* math.log((used_streets[street.name] + 1)) + 1)
This solution subtly biases the semantics of the
give_pos == True case: it allocates the posi-
tion to âŒŠğ‘cars/1000 + 1/2âŒ‹where ğ‘cars is the num-
ber of cars using this street in total. This has some
level of bias towards later positions for more heav-
ily used streets, but a load of at least 500 cars is
needed before any shift is triggered.
A
significantly
wider
shift
may
be
ob-
served for predicting green light durations:
âŒŠ(ğ‘carsğ‘str/1000 + 1/10) ln(ğ‘cars + 1) + 1âŒ‹where
ğ‘str is the total number of streets entering this
intersection. This formula amplifies the priority
of the streets based on how congested they can
get (ğ‘cars) as well as how much waiting there
could be while the light cycles through all other
streets (ğ‘str). Another key improvement is that
this light duration should grow logarithmically
with the level of congestion expected.
With these two formulae in place, a score of
1, 463, 336 is achievedâ€”and it is one that takes
a significant time to meaningfully outperform.
Given sufficient time, our model has managed to
discover functions that fine-tune the learnt for-
mula somewhat to release additional traffic:
l_used = used_streets.get(street.name, 0)
if give_pos:
return max(0, int(min(curr_size - 1, l_used // 200 * 2)))
else:
return max(1, int(min(1000, (
l_used * 0.001 * curr_size + 0.1) * math.log(
(l_used + 1)) + 1)))
In this case, the prediction for green light duration
(give_pos == False) is largely unchanged:
the main difference is capping the length at a
maximum value of 1, 000. A more substantial
change occurs in predicting the scheduling or-
der of the green lights: the predicted position be-
comes max(0, min(eğ‘strâˆ’1, âŒŠğ‘cars/200âŒ‹âˆ—2), where
eğ‘str is the total number of streets that entered this
intersection so far (note the difference between
what the variable curr_size will have when as-
signing positions vs. assigning durations!).
This intricate evolved formula gives precedence
to streets that are encountered earlier for a partic-
ular intersection (smaller eğ‘str) as well as streets
with a lower volume of cars using it (smaller
ğ‘cars). One reasoning behind this could be to
allow early, less-frequently traversed streets to
be passed quickly, while delaying the more busy
streets for later in the schedule so that the cars are
more likely to queue up and make full advantage
of the longer green cycle.
These tuned constants additionally amplify the
score on this input to 1, 465, 888. Taken together
with all evolved functions on individual inputs,
our method achieves a comfortable rank-1 result.
Held-out combinatorial contest case
study: AtCoder Heuristic Contest 039
As an additional piece of evidence that our ap-
proach provides meaningful improvements re-
gardless of whether prior data relevant to the
contest exists, we attempt to apply our FunSearch-
backed method to a variant of the recently-held
AtCoder Heuristic Contest (AHC 039).
This AHC took place on 10 November 2024,
which was after the global release of Gemini 1.5
Flash 002. Hence, any information or strategies
pertaining to the contest are not present in the
data used to pre-train or fine-tune the model.
AtCoder Heuristic Contests
In addition to the held-out nature of the contest
w.r.t. Gemini 1.5 Flash 002â€™s training, it is also a
substantially different setup to Hash Code:
â€¢ The input test-cases are no longer given but
are kept private â€“ though the procedure used
to generate them is known in advance.
â€¢ The contestants are required to submit their
code to the testing system, and the code
needs to be able to compile and execute in
its entiretyâ€”it is not possible to offload any
computation to an external tool.
This setting is expected to be more challenging
for FunSearch, as its internal fitness function will
not correspond to the actual evaluation that will
be used to determine its solutionsâ€™ rank.
11


--- Page 12 ---
Amplifying human performance in combinatorial competitive programming
AHC 039: Purse Seine Fishing
In this task, we are given a collection of points
on the 2D integer grid (i.e., all points have to
be of the form (ğ‘, ğ‘), for ğ‘, ğ‘âˆˆâ„•). In addition,
each point is one of two types (â€œmackerelsâ€ and
â€œsardinesâ€ in the task statement). The objective
is to discover a polygon with only horizontal and
vertical edges, which covers as many points of
type 1 and as little points of type 2.
There are also constraints in terms of the max-
imal number of vertices and maximal total edge
length that such a polygon can have, encouraging
polygons that are more strongly contiguous.
For the testcases used to evaluate solutions,
all points are generated from a mixture of Gaus-
sians, with randomly generated parameters (e.g.
cluster centers and standard deviations, mixture
weights...) for points of type 1 and type 2.
The strategy our backbone will pursue is to
start from one grid cell, then maintaining a con-
nected polygon by iteratively adding more points
to it. Since the raw grid is too dense for building
directly in it, we partition the grid into coarser
square cells of size ğ‘Ã— ğ‘, and only consider
adding entire such cells at once to the polygon.
We provide several aspects of our backbone so-
lution for FunSearch in the Appendix (Figure 8).
Note that this solution is not the fastest possible
implementationâ€”in several locations, data points
are overwritten in order to prevent FunSearch
from deliberately modifying internal state. Our
starting scoring function simply rates each cell
of the coarse grid by their difference of points of
type 1 and 2, without taking into account which
cells are already in the polygon:
def score_greedy(
grid: Grid,
row: int,
col: int,
picked_cells: set[tuple[int, int]]):
"""Returns the score of picking a cell."""
return grid.mackerels[row][
col] - grid.sardines[row][col]
We evolve this scoring function using FunSearch
on a hill-climbing dataset of 150 examples which
we pre-generated following the procedure in the
task statement. As can be seen in Figure 4, our
0
5,000
10,000
15,000
20,000
25,000
30,000
Programs generated
400,000
420,000
440,000
460,000
480,000
500,000
520,000
540,000
Score
Score on AHC 039 hill-climbing set
Size 5, 000 Ã— 5, 000
Size 4, 000 Ã— 4, 000
Size 3, 000 Ã— 3, 000
Size 2, 000 Ã— 2, 000
Size 1, 500 Ã— 1, 500
Figure 4 | The improvement in score obtained
by FunSearch on the hill-climbing dataset we
used for AHC 039, over the first 30, 000 programs.
Each line corresponds to a particular cell size.
models can make steady progress on improving
this function, across many different choices of cell
size ğ‘Ã— ğ‘.
The best-performing function we obtained (on
cell size 2, 000Ã—2, 000) was of the following form:
if (row, col) in picked_cells:
return 0
m = grid.mackerels[row][col]
s = grid.sardines[row][col]
score = m - 1.3 * s
num_picked = len(picked_cells)
score_multiplier = 1.0 + 0.01 * min(num_picked, 100)
if num_picked == 0:
score *= 1.5
dist_center = abs(grid.rows // 2 - row) + abs(
grid.cols // 2 - col)
center_bias = max(0, 1.0 / (1.0 + dist_center**2))
score += center_bias * score - center_bias * 0.4 * s
dist_weight = 1.3
for r in range(max(0, row - 15), min(grid.rows, row + 16)):
for c in range(max(0, col - 15), min(grid.cols, col + 16)):
if (r, c) not in picked_cells and (r, c) != (row, col):
dist = abs(row - r) + abs(col - c)
adj_macks = 0
for dx, dy in [(-1, 0), (0, -1), (+1, 0), (0, +1)]:
test_r = r + dx
test_c = c + dy
if (0 <= test_r < grid.rows) and (
0 <= test_c < grid.cols) and (
test_r, test_c) in picked_cells:
adj_macks += grid.mackerels[test_r][test_c]
weight = max(1e-6, 1 / (
1 + dist**2 + m + s + adj_macks + 0.001 * num_picked))
score += max(0, (
grid.mackerels[r][c] - 1.6 * grid.sardines[
r][c]) * dist_weight * weight)
adjacent_cells = 0
for dx, dy in [(-1, 0), (0, -1), (+1, 0), (0, +1)]:
if (0 <= row + dx < grid.rows) and (
0 <= col + dy < grid.cols) and (
row + dx, col + dy) in picked_cells:
adjacent_cells += 1
score += 7 * adjacent_cells
return max(0, score * score_multiplier)
Since in this case the hill-climbing set containts
12


--- Page 13 ---
Amplifying human performance in combinatorial competitive programming
many individual testcases rather than just one,
this evolved function features novel, interpretable
mechanisms which were not in any way suggested
at the starting point. In comparison, it exploits
the specifics of testcase distributions less.
We describe three of these key mechanisms
that can be noticed in the evolved code snippet:
â€¢ Adjusting the score depending on how proxi-
mal the currnt cell is to the center of the grid
(center_bias).
â€¢ Expressing the score using a weighted average
over a subgrid of 30 Ã— 30 cells around the
current cell, encapsulated in the for loops:
for r in range(...):
for c in range(...):
This aligns well with the assumption that the
data is generated from a mixture of Gaus-
sians, and is more likely to identify cluster
centers.
â€¢ Preferring cells that have a greater num-
ber
of
adjacent
cells
in
the
polygon
(adjacent_cells). This encourages poly-
gons to be more strongly contiguous, min-
imising total edge length and number of ver-
tices, and increasing likelihood that the poly-
gon will be valid.
This function achieved a score of 541, 173 on
our pre-generated hill-climbing set of data points.
Even though the pre-generated dataset is diverse,
it is still likely that the hill climber performed
some spurious mutations, that optimise score on
those specific instances without better fitting the
distribution overall.
Accordingly, to estimate what is the likely rank
our evolved scoring function would have achieved
in the actual contest, we generate a new dataset
of 1, 500 data points (10Ã— the size of what was
used in AHC 039) and use it to bootstrap our
rank estimates. Note that we also make changes
to the backbone by removing any inefficiencies
induced by protecting FunSearch from overwrit-
ing global state, and we also attempt to use our
evolved scoring function with several grid size
lengths ({1, 500, 2, 000, 3, 000, 4, 000}) and re-
port the maximal-scoring polygon across them.
Across our larger test dataset, we found that
our final solution achieves an average score of
3, 521.9 Â± 424.4 (where the error bars are stan-
dard deviation).
To estimate the 95% confidence interval of the
total score across a randomly chosen dataset of
150 data points, we multiply the mean value by
150 and the standard deviation by 2
âˆš
150.
Performing this calculation, we conclude the
95% confidence interval of the final score is
528, 285.4Â±10, 396.6. This means that, with 95%
confidence, our solution would rank between the
9th place and the 17th placeâ€”a significantly better
score than the backbone, and a very commend-
able performance.
Conclusions
In this technical report, we explored a collab-
orative approach between human competitors
and evolutionary search in the program space,
to significantly amplify the competitorsâ€™ perfor-
mance in combinatorial competitive program-
ming. Our approach yields significant perfor-
mance improvements on all previous Hash Code
Online Qualification Roundsâ€”even when we re-
strict the amount of time allocated to the exe-
cution of the AI method to under two hours. In
many cases, it is sufficient for a rank-1 result. The
method also proves potent on a recent held-out
AtCoder Heuristic Contest, with a setting substan-
tially different to the one of Hash Code.
References
R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lep-
ikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,
Z. Chen, et al. Palm 2 technical report. arXiv
preprint arXiv:2305.10403, 2023.
Gemini Team, P. Georgiev, V. I. Lei, R. Burnell,
L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan,
S. Wang, et al. Gemini 1.5: Unlocking multi-
modal understanding across millions of tokens
of context. arXiv preprint arXiv:2403.05530,
2024.
R. Leblond et al. AlphaCode 2 Technical Report.
2023. URL https://storage.googleapis.
13


--- Page 14 ---
Amplifying human performance in combinatorial competitive programming
com/deepmind-media/AlphaCode2/
AlphaCode2_Tech_Report.pdf.
Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrit-
twieser, R. Leblond, T. Eccles, J. Keeling, F. Gi-
meno, A. Dal Lago, et al. Competition-level
code generation with alphacode. Science, 378
(6624):1092â€“1097, 2022.
OpenAI.
Learning to Reason with LLMs.
2024. URL https://openai.com/index/
learning-to-reason-with-llms/.
B. Romera-Paredes, M. Barekatain, A. Novikov,
M. Balog, M. P. Kumar, E. Dupont, F. J. Ruiz,
J. S. Ellenberg, P. Wang, O. Fawzi, et al. Mathe-
matical discoveries from program search with
large language models. Nature, 625(7995):
468â€“475, 2024.
Acknowledgements
The authors wish to thank Bernardino Romera-
Paredes, Amin Barekatain and Alhussein Fawzi
for their helpful discussions on evolutionary pro-
gramming with LLMs, as well as NgÃ¢n VËœu, George
Holland, Simon Osindero, Shakir Mohamed and
Pushmeet Kohli for their many helpful remarks
and comments on the work. We highly appreci-
ate the efforts of RÃ©mi Leblond, Juanita Bawagan,
Zoubin Ghahramani, Bakhodir Ashirmatov, Prze-
mek Pietrzkiewicz and Petr Mitrichev for review-
ing our paper prior to the release. Last but not
least, we are indebted to the entire Hash Code
team for making the data relevant to previous
competitions broadly available.
Author Contributions
P.V. concieved the research direction, with helpful
steering from M.B. and A.N. in terms of framing
it in a scope where the approach from Romera-
Paredes et al. (2024) would be mostly applicable.
The backbones were designed by P.V., A.V., L.M.,
B.I. and L.B., with M.B. and A.N. providing sig-
nificant assistance in terms of executing them
scalably. P.V. analysed the four case studies. All
authors contributed to writing the paper.
Funding
This research was funded by Google DeepMind.
Supplementary backbone examples
â€¢ Hash Code 2015 Qualification Round (Opti-
mize a Data Center): Figure 5.
â€¢ Hash Code 2018 Qualification Round (Self-
driving rides): Figure 6.
â€¢ Hash Code 2021 Qualification Round (Traffic
scheduling): Figure 7.
â€¢ AtCoder Heuristic Contest 039 (Purse Seine
Fishing): Figure 8.
14


--- Page 15 ---
Amplifying human performance in combinatorial competitive programming
@dataclasses.dataclass(frozen=True)
class Server:
index: int
size: int
capacity: int
def parse_data(file_path: str):
"""Parse data file.
Args:
file_path: path to the datafile.
Returns:
row_blocks
servers
n_pools
"""
with open(file_path, â€™rtâ€™) as f:
main_line = f.readline()
n_rows, n_slots, n_unavailable, n_pools, n_servers = map(
int, main_line.split())
row_blocks = []
servers = []
for _ in range(n_rows):
row_blocks.append([])
for _ in range(n_unavailable):
u_x, u_y = map(int, f.readline().split())
row_blocks[u_x].append(u_y)
for i_r in range(n_rows):
row_blocks[i_r].append(n_slots)
row_blocks[i_r] = sorted(row_blocks[i_r])
for i_s in range(n_servers):
s_size, s_capacity = map(int, f.readline().split())
servers.append(Server(i_s, s_size, s_capacity))
return row_blocks, servers, n_slots, n_pools
def evaluate(file_path: str) -> int:
"""Returns data center value when using the evolved optimisation heuristic."""
row_blocks, servers, n_slots, n_pools = parse_data(file_path)
curr_ind = [0 for _ in row_blocks]
curr_block = [0 for _ in row_blocks]
open_rows = set(range(len(row_blocks)))
open_servers = set(range(len(servers)))
placement = {}
while open_rows and open_servers:
for row in set(open_rows):
if not open_servers:
break
while curr_ind[row] == row_blocks[row][curr_block[row]]:
curr_ind[row] += 1
curr_block[row] += 1
if curr_ind[row] >= n_slots:
open_rows.remove(row)
break
if row not in open_rows:
continue
next_pos = row_blocks[row][curr_block[row]]
best_server = None
best_score = -1
for s_id in open_servers:
if curr_ind[row] + servers[s_id].size <= next_pos:
curr_score = score_greedy(servers[s_id], row, None, None, True)
if best_server is None or curr_score > best_score:
best_score = curr_score
best_server = s_id
if best_server is None:
curr_ind[row] = next_pos
if curr_ind[row] >= n_slots:
open_rows.remove(row)
else:
placement[best_server] = (row, curr_ind[row], -1)
curr_ind[row] = curr_ind[row] + servers[best_server].size
open_servers.remove(best_server)
pools_per_row = {}
for row in range(len(row_blocks)):
pools_per_row[row] = {}
for p in range(n_pools):
pools_per_row[row][p] = 0
for s_id in dict(placement):
row, col, _ = placement[s_id]
best_pool = None
best_score = -1
for p in range(n_pools):
curr_score = score_greedy(servers[s_id], row, p, pools_per_row, False)
if best_pool is None or curr_score > best_score:
best_score = curr_score
best_pool = p
assert best_pool is not None
placement[s_id] = (row, col, best_pool)
pools_per_row[row][best_pool] += servers[s_id].capacity
return get_guaranteed_capacity(placement, file_path)
Figure 5 | The input parsing function and the greedy algorithm backbone for the 2015 Hash Code
online qualification (Optimizing a Data Center). Note that the backbone is calling score_greedyâ€”
the scoring function to optimiseâ€”and get_guaranteed_capacityâ€”the evaluation function.
15


--- Page 16 ---
Amplifying human performance in combinatorial competitive programming
@dataclasses.dataclass(frozen=True)
class Ride:
start: tuple[int, int]
end: tuple[int, int]
earliest_start: int
latest_finish: int
def length(self):
return distance(self.start, self.end)
def distance_to_start(self, coords: tuple[int, int]):
return distance(self.start, coords)
def distance(start: tuple[int, int], end: tuple[int, int]) -> int:
return abs(end[0] - start[0]) + abs(end[1] - start[1])
def parse_file(path: str):
"""Parses file.
Args:
path: Path to file.
Returns:
r, c, f, b, t, rides.
"""
with open(path, â€™rtâ€™) as fp:
r, c, f, n, b, t = [int(x) for x in fp.readline().split()]
rides = []
for _ in range(n):
rides.append(tuple(int(x) for x in fp.readline().split()))
return r, c, f, b, t, rides
def evaluate(path: str) -> float:
"""Assign cars to rides."""
unused_r, unused_c, f, b, tot_time, rides = parse_file(path)
rides = [
Ride(start=x[:2], end=x[2:4], earliest_start=x[4], latest_finish=x[5])
for x in rides
]
cars = [(0, (0, 0))] * f
score = 0
while True:
time, coords = heapq.heappop(cars)
if time >= tot_time:
break
idx = pick_ride(coords, time, tuple(rides))
assert isinstance(idx, int)
if idx < 0 or idx >= len(rides):
heapq.heappush(cars, (tot_time, coords))
else:
r = rides.pop(idx)
pickup_time = time + r.distance_to_start(coords)
if pickup_time < r.earliest_start:
pickup_time = r.earliest_start
free_time = pickup_time + r.length()
new_car = (free_time, r.end)
if free_time <= r.latest_finish:
score += r.length()
if pickup_time == r.earliest_start:
score += b
heapq.heappush(cars, new_car)
return score
Figure 6 | The input parsing function and the greedy algorithm backbone for the 2018 Hash Code
online qualification (Self-driving rides). Note that the backbone is calling pick_rideâ€”the function
which chooses the next ride for a given car. We do not require a bespoke evaluation function in this
case because it can be calculated on-the-fly during the execution.
16


--- Page 17 ---
Amplifying human performance in combinatorial competitive programming
@dataclasses.dataclass(frozen=True)
class Street:
name: str
length: int
start_id: int
end_id: int
@dataclasses.dataclass(frozen=True)
class Car:
index: int
route: list[Street]
@dataclasses.dataclass(frozen=True)
class Intersection:
index: int
roads_in: list[Street]
roads_out: list[Street]
def evaluate(file_path: str) -> int:
"""Returns traffic scheduling value when using the evolved heuristic."""
intersections, streets, cars, deadline, bonus = parse_data(file_path)
used_streets = {}
# Make sure we do not schedule unused streets
for car in cars:
travel_time = 0
for street in car.route[1:]:
travel_time += street.length
if travel_time > deadline:
continue
for street in car.route[:-1]:
if street.name not in used_streets:
used_streets[street.name] = 0
used_streets[street.name] += 1
curr_index = []
curr_time_at_index = []
deques = {}
open_streets = set(used_streets)
cand_streets = set()
schedule = []
for _ in range(len(intersections)):
schedule.append([])
for name in used_streets:
schedule[streets[name].end_id].append(None)
deques[name] = collections.deque()
# Initialise intersections and empty queues
assert len(intersections) == len(schedule)
for i_ind in range(len(intersections)):
if schedule[i_ind]:
curr_index.append(0)
curr_time_at_index.append(1)
else:
curr_index.append(None)
curr_time_at_index.append(deadline + 1)
# Initialise queues
for car in cars:
if car.route[0].name not in used_streets:
deques[car.route[0].name] = collections.deque()
deques[car.route[0].name].append(car.index)
cand_streets.add(car.route[0].name)
# Initialise cars
curr_road_id = [0] * len(cars)
curr_travel = [0] * len(cars)
moving_cars = set()
score = 0
# Perform a simulation, scheduling streets as they are hit
for time in range(deadline):
# process any streets that are now open
for s_name in set(open_streets).intersection(cand_streets):
i_ind = streets[s_name].end_id
s_pos = pos_greedy(
streets[s_name],
cars,
intersections,
used_streets,
bonus,
time,
len(schedule[i_ind]),
True)
s_pos = s_pos % len(schedule[i_ind])
while schedule[i_ind][s_pos] is not None:
s_pos = (s_pos + 1) % len(schedule[i_ind])
schedule[i_ind][s_pos] = (streets[s_name], 1)
open_streets.remove(s_name)
# unblock any available traffic
for i_ind in range(len(intersections)):
if curr_index[i_ind] is None:
continue
if schedule[i_ind][curr_index[i_ind]] is None:
# unlikely, but could happen under more eccentric schedulers...
continue
curr_open_street = schedule[i_ind][curr_index[i_ind]][0].name
if deques[curr_open_street]:
top_car_id = deques[curr_open_street].popleft()
moving_cars.add(top_car_id)
curr_road_id[top_car_id] += 1
curr_travel[top_car_id] = cars[top_car_id].route[
curr_road_id[top_car_id]].length
# progress any moving traffic
for car_id in set(moving_cars):
assert curr_travel[car_id] > 0
curr_travel[car_id] -= 1
if curr_travel[car_id] == 0:
moving_cars.remove(car_id)
if curr_road_id[car_id] == len(cars[car_id].route) - 1:
score += bonus + (deadline - time - 1)
else:
deques[cars[car_id].route[curr_road_id[car_id]].name].append(car_id)
cand_streets.add(cars[car_id].route[curr_road_id[car_id]].name)
# process traffic lights
for i_ind in range(len(intersections)):
curr_time_at_index[i_ind] -= 1
if curr_time_at_index[i_ind] == 0:
assert curr_index[i_ind] is not None
curr_index[i_ind] = (curr_index[i_ind] + 1) % len(schedule[i_ind])
curr_time_at_index[i_ind] = 1
# Schedule any unused streets.
for s_name in used_streets:
if s_name in open_streets:
i_ind = streets[s_name].end_id
s_pos = pos_greedy(
streets[s_name],
cars,
intersections,
used_streets,
bonus,
deadline,
len(schedule[i_ind]),
True)
s_pos = s_pos % len(schedule[i_ind])
while schedule[i_ind][s_pos] is not None:
s_pos = (s_pos + 1) % len(schedule[i_ind])
schedule[i_ind][s_pos] = (streets[s_name], 1)
# Assign the durations for each green light.
for i_ind in range(len(schedule)):
for s_pos in range(len(schedule[i_ind])):
l_len = pos_greedy(
schedule[i_ind][s_pos][0],
cars,
intersections,
used_streets,
bonus,
deadline,
len(schedule[i_ind]),
False)
schedule[i_ind][s_pos] = (schedule[i_ind][s_pos][0], l_len)
# Make sure we do not count streets with only unfinished cars
_, fin_cars = count_blocked(
streets,
intersections,
cars,
schedule,
deadline,
bonus,
used_streets)
n_used_streets = {}
for car in cars:
if car.index in fin_cars:
for street in car.route[:-1]:
if street.name not in n_used_streets:
n_used_streets[street.name] = 0
n_used_streets[street.name] += 1
n_schedule = []
for i_ind in range(len(schedule)):
n_schedule.append([])
for s_ind in range(len(schedule[i_ind])):
street = schedule[i_ind][s_ind][0]
if street.name in n_used_streets:
n_schedule[i_ind].append(schedule[i_ind][s_ind])
return get_value(
streets, intersections, cars, n_schedule, deadline, bonus, file_path)
Figure 7 | A snippet of the greedy algorithm backbone for the 2021 Hash Code online qualification
(Traffic scheduling). Note that the backbone is calling pos_greedyâ€”the function which chooses
either the position of a given street in the traffic lightâ€™s schedule (if the final boolean is True) or its
green light duration (if it is False). It also calls count_blocked, a function which returns all cars
that successfully completed their route, and get_value, the evaluation function.
17


--- Page 18 ---
Amplifying human performance in combinatorial competitive programming
import dataclasses
import heapq
import random
import numpy as np
@dataclasses.dataclass(frozen=True)
class Grid:
rows: int
cols: int
mackerels: list[list[int]]
sardines: list[list[int]]
def parse_data(file_path: str):
"""Parse data file.
Args:
file_path: path to the datafile.
Returns:
mackerels
sardines
"""
with open(file_path, â€™rtâ€™) as f:
n_fishes = int(f.readline())
mackerels = {}
sardines = {}
for _ in range(n_fishes):
coord_line = f.readline()
x, y = map(int, coord_line.split())
mackerels[(x, y)] = 1
for _ in range(n_fishes):
coord_line = f.readline()
x, y = map(int, coord_line.split())
sardines[(x, y)] = 1
return mackerels, sardines
def catch_value(file_path: str, cell_size: int) -> int:
"""Returns catch value for a given cell size."""
mackerels, sardines = parse_data(file_path)
min_x, max_x = 100_000, 0
min_y, max_y = 100_000, 0
for (x, y) in mackerels:
min_x = min(min_x, x)
max_x = max(max_x, x)
min_y = min(min_y, y)
max_y = max(max_y, y)
rows = 0
cols = 0
grid_mackerels = []
grid_sardines = []
curr_x = min_x
while curr_x <= max_x:
grid_mackerels.append([])
grid_sardines.append([])
rows += 1
curr_y = min_y
while curr_y <= max_y:
grid_mackerels[-1].append(0)
grid_sardines[-1].append(0)
curr_y += cell_size
if rows == 1:
cols = len(grid_mackerels[-1])
curr_x += cell_size
for (x, y) in mackerels:
x -= min_x
y -= min_y
assert x >= 0 and y >= 0
grid_mackerels[x // cell_size][y // cell_size] += 1
for (x, y) in sardines:
if x < min_x or x > max_x or y < min_y or y > max_y:
continue
x -= min_x
y -= min_y
assert x >= 0 and y >= 0
grid_sardines[x // cell_size][y // cell_size] += 1
grid = Grid(rows, cols, grid_mackerels, grid_sardines)
scores = []
best_cell = None
best_score = 0
grid_2 = Grid(rows, cols,
list([list(row) for row in grid_mackerels]),
list([list(row) for row in grid_sardines]))
for i in range(rows):
scores.append([])
for j in range(cols):
curr_score = score_greedy(grid_2, i, j, set())
scores[i].append(curr_score)
if best_cell is None or curr_score > best_score:
best_cell = (i, j)
best_score = curr_score
picked_cells = set()
pq = []
heapq.heappush(pq, (-best_score, best_cell))
curr_mackerels = 0
curr_sardines = 0
def push_point(cell, picked_cells, pq):
delta_mackerels = 0
delta_sardines = 0
def track_point(test_cell, picked_cells):
assert test_cell not in picked_cells
queue = [test_cell]
visited = set()
while queue:
curr_cell = queue.pop()
if curr_cell in visited:
continue
visited.add(curr_cell)
for dx, dy in [(-1, 0), (0, -1), (+1, 0), (0, +1)]:
next_x = curr_cell[0] + dx
next_y = curr_cell[1] + dy
if (next_x < 0) or (next_x >= len(scores)) or (
next_y < 0) or (next_y >= len(scores[0])):
return []
if (next_x, next_y) in picked_cells:
continue
if (next_x, next_y) in visited:
continue
queue.append((next_x, next_y))
return list(visited)
assert cell not in picked_cells
picked_cells.add(cell)
delta_mackerels += grid.mackerels[cell[0]][cell[1]]
delta_sardines += grid.sardines[cell[0]][cell[1]]
grid_2 = Grid(rows, cols,
list([list(row) for row in grid_mackerels]),
list([list(row) for row in grid_sardines]))
for dx, dy in [(-1, 0), (0, -1), (+1, 0), (0, +1)]:
next_x = cell[0] + dx
next_y = cell[1] + dy
if (next_x < 0) or (next_x >= len(scores)) or (
next_y < 0) or (next_y >= len(scores[0])):
continue
if (next_x, next_y) in picked_cells:
continue
for pt in track_point((next_x, next_y), picked_cells):
picked_cells.add(pt)
delta_mackerels += grid.mackerels[pt[0]][pt[1]]
delta_sardines += grid.sardines[pt[0]][pt[1]]
if (next_x, next_y) not in picked_cells:
scores[next_x][next_y] = score_greedy(
grid_2, next_x, next_y, set(list(picked_cells)))
heapq.heappush(pq, (-scores[next_x][next_y], (next_x, next_y)))
return delta_mackerels, delta_sardines
max_val = -1
while pq:
_, best_cell = heapq.heappop(pq)
if best_cell in picked_cells:
continue
delta_mackerels, delta_sardines = push_point(best_cell, picked_cells, pq)
curr_mackerels += delta_mackerels
curr_sardines += delta_sardines
val = max(0, curr_mackerels - curr_sardines + 1)
# Convert polygon cells to boundary line, return -1 if invalid or too long
traversal = decode_to_output(
picked_cells,
cell_size,
min_x, max_x,
min_y, max_y)
if traversal != -1 and val >= max_val:
max_val = val
return max_val
Figure 8 | A snippet of the input parsing function and the greedy algorithm backbone for the AtCoder
Heuristic Contest 039 (Purse Seine Fishing) for a given cell_size. Note that the backbone is calling
score_greedyâ€”the function which provides the score of adding a particular cell to the polygon.
18
