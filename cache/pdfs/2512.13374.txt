--- Page 1 ---
BEHAVIOR AND REPRESENTATION IN LARGE LANGUAGE
MODELS FOR COMBINATORIAL OPTIMIZATION: FROM FEATURE
EXTRACTION TO ALGORITHM SELECTION
Francesca Da Ros
University of Udine, Italy
francesca.daros@uniud.it
Luca Di Gaspero
University of Udine, Italy
luca.digaspero@uniud.it
Kevin Roitero
University of Udine, Italy
kevin.roitero@uniud.it
ABSTRACT
Recent advances in Large Language Models (LLMs) have opened new perspectives for automation
in optimization. While several studies have explored how LLMs can generate or solve optimization
models, far less is understood about what these models actually learn regarding problem structure
or algorithmic behavior. This study investigates how LLMs internally represent combinatorial
optimization problems and whether such representations can support downstream decision tasks. We
adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly
extract instance features, with probing analyses that examine whether such information is implicitly
encoded within their hidden layers. The probing framework is further extended to a per-instance
algorithm selection task, evaluating whether LLM-derived representations can predict the best-
performing solver. Experiments span four benchmark problems and three instance representations.
Results show that LLMs exhibit moderate ability to recover feature information from problem
instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-
layer representations proves comparable to that achieved through traditional feature extraction,
suggesting that LLMs capture meaningful structural information relevant to optimization performance.
Keywords Combinatorial Optimization · Large Language Models · Direct Querying · Probing · Feature Extraction ·
Algorithm Selection
1
Introduction
Combinatorial Optimization (CO) has long been used to tackle decision-making problems across many domains,
including logistics, scheduling, and resource allocation [35, 62, 64]. Despite major advances in exact and heuristic
methods, as well as the growing integration of Machine Learning (ML) techniques [54], developing and analyzing
algorithms remains a largely manual, expertise-driven process. This process is time-consuming and difficult to automate,
particularly when deep domain knowledge plays a critical role in performance.
In parallel, recent developments in Large Language Models (LLMs) have demonstrated their capacity to process text,
generate executable code, and generalize across domains [31]. These capabilities have prompted researchers to explore
how LLMs can support optimization-related tasks, from model formulation to heuristic generation and algorithmic
analysis [18]. While many of these studies have produced encouraging results, they primarily assess the functional
performance of the models (i.e., what they can do) without investigating their representational understanding, that is,
what they internally learn about problem structure and algorithmic behavior.
Understanding how LLMs represent optimization problems is particularly relevant. If LLMs encode meaningful
information about problem structure or algorithmic performance, they could serve as general-purpose feature extractors
for tasks such as algorithm selection, performance prediction, or hybrid optimization frameworks that combine learning
and search. Conversely, if their internal representations lack such structure, this would highlight their current limitations.
In this study, we are concerned with COs and we investigate the extent to which LLMs capture information about them,
focusing on whether such information is reflected in LLMs internal representations or can be accessed through direct
querying, with attention to instance features and per-instance algorithm selection. We address three research questions:
arXiv:2512.13374v1  [cs.AI]  15 Dec 2025


--- Page 2 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
RQ1 To what extent can LLMs identify salient structural or numerical characteristics of CO instances from their
representations?
RQ2 Can LLMs encode information about CO instances, and is this information accessible through probing?
RQ3 Can representations extracted from the hidden layers of LLMs serve as effective instance features for per-
instance algorithm selection, and how does their predictive power compare to that of traditional handcrafted
features?
RQ1 captures the behavioral dimension of our analysis, examining explicit feature awareness and generation capabilities.
RQ2 investigates the internal, latent encoding of instance-level information. RQ3 assesses the practical utility of
LLMs, testing whether their learned representations can inform algorithmic decision. To answer these questions, we
combine direct querying and probing analyses. In the first part of our study, we evaluate whether models exhibit
awareness of instance-level features typically indirectly associated with algorithmic performance, and whether such
features can be explicitly and implicitly retrieved from instance representations. In the second part, we use probing
to examine whether the internal representations can act as surrogates for handcrafted features traditionally obtained
through explicit feature engineering. We conduct experiments across four representative Combinatorial Optimization
Problems (COPs), using publicly available instance sets and algorithm portfolios from prior Instance Space Analysis
(ISA) studies. Our evaluation spans multiple feature complexities, instance representations, and probing configurations,
providing a comprehensive perspective on the relationship between explicit and implicit representations in LLMs.
The main contributions of this work are as follows:
• We introduce a unified experimental framework for evaluating what LLMs learn about COPs through direct
querying and probing.
• We provide the first large-scale analysis of how LLMs encode structural, numerical, and algorithmic informa-
tion across multiple problem domains and input representations.
• We release the dataset, code, and prompts to foster reproducibility and further exploration at the intersection of
LLMs and optimization [16].
This study extends our earlier conference publication [14], which focused solely on direct querying of LLMs for feature
extraction. Here, we expand the analysis along several dimensions: we introduce probing analyses to explore internal
representations, generalize the experimental setup to multiple problems and input representations, include a downstream
algorithm-selection task, and provide a broader comparative analysis.
The remainder of this paper is organized as follows. Section 2 reviews the background and related work. Section 3
describes the proposed methodology. Section 4 presents the experimental analysis and discussion. Finally, Section 5
concludes the paper and outlines directions for future research.
2
Background and Related Work
This section reports background and related work on the topics of LLM (Section 2.1) and their interpretability
(Section 2.2). Following, it summarizes relevant notions regarding LLM usage in CO (Section 2.3) and algorithm
selection (Section 2.4).
2.1
Large Language Models
LLMs have fundamentally transformed the landscape of Natural Language Processing (NLP) and, increasingly, fields
beyond it, including reasoning and programming [10, 24].
LLMs are Neural Network (NN) architectures designed to model the probability distributions of natural language
sequences. The fundamental processing unit of an LLM is the token, namely a sub-word or symbol representing a
fragment of text, code, or other structured input. Given a sequence of tokens t1, t2, . . . , tn, an LLM estimates the
conditional probability P(tn|t1, . . . , tn−1), thereby learning to predict the next token conditioned on its preceding
context. Through large-scale training on heterogeneous textual and symbolic corpora, such models acquire internal
representations that capture syntactic, semantic, and relational regularities, which in turn enable them to perform a
broad spectrum of reasoning, generation, and comprehension tasks [12, 32].
At their core, most LLMs implement the Transformer architecture [57], which relies on self-attention mechanisms to
compute contextual dependencies between all tokens in parallel [5]. This design allows LLMs to handle long-range
relationships efficiently and to generate outputs that are sensitive to the entire input sequence.
2


--- Page 3 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
Users interact with the model through a prompt, which is a textual sequence that defines the input context and often
specifies the desired task. The model then produces one or more output tokens that, when decoded, yield natural
language or symbolic text. The process can be guided or restricted through prompt engineering or constrained decoding,
the latter enforcing structural, syntactic, or semantic constraints on the generated output, e.g., requiring valid JSON,
enforcing type consistency, or limiting values to predefined domains.
2.2
Large Language Models Interpretability
Despite their remarkable capabilities, the internal mechanisms by which LLMs encode and organize knowledge remain
only partially understood. Two complementary approaches are commonly employed to investigate these mechanisms:
direct querying (also referred to as direct questioning or prompting), which analyzes model behavior through carefully
designed prompts, and probing, which examines the information embedded within the model hidden representations.
Direct querying is a methodology used to assess the behavior of LLMs by presenting them with a prompt and analyzing
their generated responses. It aims to evaluate what models know or can recall when explicitly prompted, without
inspecting attention weights, internal activations, or any other internal mechanisms of the model itself. A well-known
example of direct querying is the needle in a haystack task,1 in which a factual statement (i.e., a short text fragment)
is embedded within a long context, and the model is prompted to retrieve it verbatim. This methodology has been
extended to other applications, such as document summarization and citation generation [25], to evaluate whether
relevant information can be surfaced by the model when explicitly prompted to recall it.
Probing offers a systematic and empirically grounded framework for investigating the internal representations of LLMs.
It provides a structured methodology to examine what kinds of information are captured within the models internal
mechanisms, where such information is stored, and how it is organized and retrieved across layers [20, 26, 30]. Unlike
the output-based evaluation described above, which focuses on model behavior elicited through direct querying, probing
operates by extracting the hidden layer activations produced as the model processes an input, and analyzing these
activations using lightweight predictive models known as probes. This approach enables researchers to assess whether
specific properties, such as syntactic, semantic, numerical, or domain-specific features, are implicitly encoded within
the model latent representations.
A typical probing setup comprises three main stages: (i) a controlled set of inputs is fed into a frozen LLM to collect
activations from one or more internal layers of interest, typically the final layer; (ii) an external model (i.e., the probe) is
trained on these representations to predict a target property; and (iii) the probe’s performance is evaluated on unseen
data to assess its generalization on the same property. Common probe architectures include linear classifiers and simple
Multi Layer Perceptrons (MLPs), each suited to investigating different aspects of how information is encoded within
LLMs [6, 21, 59]. Simpler probes, such as linear regressors, are typically used to examine whether a property is linearly
accessible from the model representations, whereas more complex, non-linear probes can explore relationships that may
be distributed or non-linearly structured within the latent space [19].
Although originally developed for linguistic analysis, probing has been applied to a broad range of interpretability tasks.
Several studies have examined whether hidden layers encode syntactic categories or dependency relations, showing that
intermediate layers tend to capture structural information, whereas deeper layers increasingly specialize in semantic
content [6, 58]. The same framework has also been extended to investigate higher-level capabilities such as discourse
awareness [23], factual and relational knowledge [11], and hierarchical reasoning [4]. For instance, a probe may be
trained to distinguish correct from incorrect factual triples (e.g., “Paris is the capital of France” vs. “France is the
capital of Paris”) or to recover the latent coordinates of entities mentioned in text, thereby revealing whether spatial or
relational concepts are implicitly represented within the model. Such applications demonstrate that probing can shed
light on the organization of latent knowledge in LLMs, extending well beyond surface-level linguistic features.
A key advantage of probing lies in its architecture-agnostic nature: it treats the model as a frozen feature extractor and
requires neither fine-tuning nor parameter updates. Consequently, it ensures that the subsequent analysis based on the
model weights accurately reflects its learned internal state [7].
2.3
Large Language Models for Combinatorial Optimization
Several recent studies have investigated the use of LLMs in CO [15, 18, 60]. Early interest in combining LLMs
with CO emerged from the Natual Language for Optimization (NL4Opt) competition [38]. It explored whether NLP
techniques could assist in mathematical modeling through entity recognition and automatic problem formulation,
initially focusing on Linear Programming (LP) tasks. Building upon this, subsequent research has diversified along two
main directions: (i) Model definition and formulation, where LLMs generate or translate optimization models from
1https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main, accessed 14 Oct 2025.
3


--- Page 4 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
textual descriptions [36]; and (ii) Heuristic and algorithm construction, where LLMs design, adapt, or parameterize
search procedures [28]. With respect to model formulation, recent works have extended beyond LP to encompass other
paradigms such as Constraint Programming (CP) [33, 56] and Mixed Integer Linear Programming (MILP) [1]. In the
second direction, applications include the automated discovery of novel heuristics — as exemplified by FunSearch [40]
— although Sim et al. [46] later showed that several simple, classical heuristics outperform those generated by the model
across extensive benchmark experiments. Alternative approaches focus on generating and evolving metaheuristics
components, as in LLaMEA [50], or on enhancing existing algorithms [42]. A smaller body of research has explored
supportive and analytical roles of LLMs within CO, extending beyond problem solving. For instance, Sartori et al.
[44] employed LLMs to assist users in interpreting and navigating Search Trajectory Networks (STNs), which are
graph-based structures that capture the temporal evolution of (meta-)heuristic search processes [43].
Despite the growing body of research, most existing studies prioritize functional performance over representational
or explanatory understanding. Notable exceptions include the analysis of LLM-generated code through graph-based
representations [51] and behavioral investigations of generated heuristics using STNs [52]. However, how LLMs
internally encode combinatorial structure remains largely unexplored. The limited number of systematic examinations
of their hidden representations leaves a substantial gap between observable competence and latent understanding. This
work aims to bridge this gap by investigating not only how combinatorial structures are represented within LLMs, but
also how such information can be effectively extracted and leveraged, e.g., as predictive features for algorithm selection
and performance estimation.
2.4
Algorithm Selection in Combinatorial Optimization
Within CO, algorithmic performance varies substantially across instances: an algorithm that performs well on one
subset of instances may be ineffective on another, depending on factors such as instance size, structure, or constraint
tightness. This variability arises from the inherent complexity of most COPs, which are NP-hard and exhibit widely
differing computational characteristics across instances. The phenomenon is formalized by the No Free Lunch (NFL)
theorem [61], which states that no single algorithm can achieve superior performance across all possible problems or
instance distributions.
The challenge of selecting or configuring algorithms according to instance characteristics was systematically con-
ceptualized by Rice [39], who defined the Algorithm Selection Problem as the task of learning a mapping between
instance features and algorithm performance. In Rice’s framework, each problem instance is represented by a vector of
measurable characteristics, and each algorithm is evaluated according to one or more performance metrics; the objective
is to predict which algorithm is expected to perform best for a given instance, effectively framing the problem as a
supervised learning task. This formulation laid the foundation for per-instance algorithm selection and inspired subse-
quent developments in algorithm portfolios: collections of complementary algorithms designed to exploit performance
diversity across instances [45, 63]. Successful algorithm selection relies on quantitative instance characterization, that
is, the identification of features that capture the structural and behavioral properties of problem instances. To support
this process, frameworks such as ISA [49] and tools such as MATILDA2 have been developed, offering integrated
environments that go beyond algorithm selection to include feature extraction, visualization, and mapping of algorithm
performance across low-dimensional instance spaces [13, 27].
Despite these advances, feature extraction still relies heavily on handcrafted, problem-specific descriptors that demand
substantial domain expertise. To address this limitation, recent research has explored automated feature learning, where
models learn informative representations directly from data. For instance, NNs have been employed to infer latent
instance representations [3, 37], extending also to black-box optimization settings [8]. Our work differs both in scope
and realization: to the best of our knowledge, this is the first study to employ LLMs for this purpose, leveraging their
representational capacity across a four COPs and three distinct instance textual representations, i.e., natural language,
code-like, and standard.
3
Methodology
In this section, we present the methodology adopted to investigate how LLMs represent and process COPs, with the
broader goal of assessing whether these models encode information that can support algorithm selection. Specifically,
our analysis is structured around two complementary perspectives: (i) the explicit reasoning abilities of the models, i.e.,
how they respond to targeted prompts; and (ii) their implicit representations, i.e., the extent to which internal layers
encode information about problem instances and their properties. To address these perspectives, we employ a twofold
experimental strategy: (i) Direct querying, wherein models are explicitly prompted to infer instance-level features; and
2https://matilda.unimelb.edu.au/matilda/, accessed 13 Oct 2025.
4


--- Page 5 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
(ii) Probing, wherein the models internal activations are analyzed to assess whether features or algorithmic performance
signals are implicitly represented.
We begin by outlining the CO setting, including the problems considered, the instance representations, the feature sets,
and the algorithm portfolios (Section 3.1). We then describe the direct querying procedure (Section 3.2), followed by
two probing analyses: the first focusing on the representation of problem features (Section 3.3), and the second on
information relevant to algorithm selection (Section 3.4). The overall structure of our methodology is summarized in
Table 1, which provides an overview of the specific objectives, approaches, and evaluation procedures associated with
each experimental component.
Table 1: Overview of the methodological components of this study. Each component addresses a specific research goal
and employs a dedicated evaluation strategy.
Goal
Methodology
Evaluation
Section
Assess whether LLMs can explicitly
identify problem features from in-
stance representations
Direct Querying:
models are
prompted to extract features directly
from the instance
Comparison between model-
predicted and ground-truth fea-
tures
Section 3.2
Determine whether internal represen-
tations of LLMs encode problem fea-
tures
Feature Probing: regressors are
trained on last-layer activations to
predict instance features
Comparison between predicted
and ground-truth features
Section 3.3
Determine whether internal represen-
tations of LLMs encode information
predictive of algorithm performance
Algorithm Selection Probing: clas-
sifiers are trained on last-layer ac-
tivations to predict the per-instance
best algorithm
Classification accuracy, compar-
ison with baseline methods
Section 3.4
3.1
Combinatorial Optimization Concepts
We recall here the main concepts and definitions underlying CO to establish a common terminology and clarify how
these notions are instantiated in this study.
Optimization problems aim to identify the best solution(s) from a search space, i.e., the set of feasible solutions, by
optimizing one or more objective functions subject to a set of constraints. Each solution corresponds to an assignment
of values to a collection of decision variables. In CO, these decision variables take discrete values, resulting in a finite,
yet typically exponentially large, search space. Such a formulation encompasses both well-known benchmark problems,
such as the seminal Maximum Satisfiability Problem (max-SAT), and a variety of real-world applications [2, 9, 22].
Each COP defines a general problem structure that is instantiated through specific instances, which provide the concrete
input data. For example, the Graph Coloring Problem (GCP) is defined on an undirected graph G = (V, E), where V
denotes the set of nodes and E the set of edges connecting pairs of nodes. The task is to assign a color to each node
such that adjacent nodes receive different colors, yielding a valid coloring that minimizes the total number of colors
used. An instance of the GCP therefore specifies the graph topology, i.e., the nodes and edges that define G. Instances
are typically stored in standardized formats and parsed into data structures suitable for algorithmic processing.
For example, GCP instances are available in the DIMACS format,3 an abridged description of which is reported below:
In this format, nodes are numbered from 1 up to n. There are m edges in the graph.
Files are assumed to be well-formed and internally consistent: node identifier values are valid, nodes are defined
uniquely, exactly m edges are defined, and so forth.
Comment lines give human-readable information about the file and are ignored by programs. Comment lines can
appear anywhere in the file. Each comment line begins with a lower-case character c. There is one problem line
per input file.
The problem line must appear before any node or arc descriptor lines. The problem line has the following format:
p FORMAT NODES EDGES. The lower-case character p signifies that this is the problem line. The FORMAT
indicates the graph format. The NODES field contains an integer value specifying the number of nodes in the
graph. The EDGES field contains an integer value specifying the number of edges in the graph.
There is one edge descriptor line for each edge in the graph, each with the following format: e u v. The lower-case
character e signifies that this is an edge descriptor line. For an edge (u, v) the fields u and v specify its endpoints.
Each edge (u, v) appears exactly once in the input file and is not repeated as (v, u).
3http://archive.dimacs.rutgers.edu/, accessed 14 Oct 2025.
5


--- Page 6 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
Table 2: Graph Coloring Problem features.
Name
Description
feat_nodes
The total number of nodes in the graph.
feat_edges
The total number of edges in the graph.
feat_degree_1
The degree of node 1 in the graph, representing the number of connections per node.
feat_density
The density of the graph, calculated as the ratio of 2 times the number of edges to the number of
nodes times the number of nodes minus 1.
feat_ratio_1
The ratio of the number of nodes to the number of edges in the graph.
feat_ratio_2
The ratio of the number of edges to the number of nodes in the graph.
feat_degree_mean
The average degree of the nodes in the graph, representing the mean number of connections per
node.
feat_degree_max
The maximum degree of the nodes in the graph, indicating the highest number of connections any
node has.
feat_degree_min
The minimum degree of the nodes in the graph, indicating the lowest number of connections any
node has.
An instance in this representation for the GCP is illustrated below:
p edge 100 1902
e 1 2
e 1 6
e 1 10
. . .
In the remainder of this work, we refer to this standardized format as standard. In addition to it, we also consider
two alternative instance representations: natural language descriptions (natural language) and code-like formulations
derived from the MiniZinc modeling language (code-like). The same instance introduced above can be expressed in
natural language form as follows:
The instance is named name. The graph has 100 nodes and 1902 edges. There is an edge between node 1 and
node 2. There is an edge between node 1 and node 6. There is an edge between node 1 and node 10. ...
The correspondent code-like format is:4
int: n = . . .;
set of int: V = ...;
int: num_edges = ...;
array[1..num_edges, 1..2] of V: E = ...;
Problem instances can be further characterized through features, i.e., measurable attributes that capture structural or
statistical properties of an instance and are often problem-specific. Features enable comparisons across instances and
support tasks such as ISA and algorithm selection. In this study, features are considered in two complementary ways:
1. Feature extraction and interpretation: we investigate whether, and to what extent, LLMs can identify or
reconstruct relevant instance features directly from the instance representations. Features are grouped by
their extraction complexity5: (i) directly extractable features, which appear explicitly in the instance ( );
(ii) low-effort features, which require minimal computation, such as counts or extrema ( ); and (iii) high-effort
features, which involve aggregation or derived calculations, such as averages ( ). Table 2 reports examples of
features used for the GCP.
2. Baseline for algorithm selection: we use both the above features and additional descriptors defined in prior
ISA studies (e.g., Smith-Miles and Bowly, [47]) as inputs to traditional per-instance algorithm selection
models. These baselines serve as reference points against which we compare models that instead rely on LLM
hidden-layer representations.
COPs are typically solved by algorithms that explore the search space to identify high-quality solutions. These
include exact methods, which guarantee optimality but are often computationally prohibitive, and (meta-)heuristic
methods, which trade optimality for efficiency and scalability. In this study, we adopt an algorithm portfolio perspective,
4https://www.hakank.org/minizinc/coloring_ip.mzn, accessed 15 Oct 2025.
5Extraction complexity is defined relative to the LLM, reflecting how difficult a feature is for the model to infer.
6


--- Page 7 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
Problem-specific
From the literature
Combinatorial 
Optimization Problems
BP
GCP
KP
JSSP
instantiated 
by
Instance
(input data)
Instance representation
Structured
Code-like
Natural language
Features
Described 
with
Crafted for this study
ISA and algorithm 
selection studies
Problem-independent
LLMs hidden layers
Solved with
Portfolio of algorithms
Evaluated
by
Performance metrics
Raw 
objective 
value
Performance 
gap
Instantiation of a concept
Legend
Figure 1: Conceptual structure of the case studies and their instantiation in this work.
considering sets of algorithms with complementary performance across instances. Algorithmic performance is evaluated
by comparing the objective values obtained on each instance.
For illustration, in the case of the GCP, we consider two graph-coloring heuristics, DSATUR (degree saturation) and
MAXIS (maximal independent set), which have been demonstrated to exhibit complementary strengths and weaknesses
through ISA [47]. The performance metric is the raw objective value, namely the number of colors in a valid coloring,
computed on the same set of benchmark instances used in that study [47].
Figure 1 provides a schematic overview of this methodological framework, illustrating how the main CO concepts
are organized and instantiated across the case studies considered in this work. Specifically, for each problem, we
retrieve benchmark instances from prior ISA studies to ensure diversity in structural characteristics. Each instance
is expressed through different representations, either structured (e.g., standard or code-like) or natural language
(natural language). Moreover, each instance is characterized by two complementary types of features. The first are
handcrafted features, conceived, designed, and extracted through manually written code, as in traditional algorithm
selection analyses. The second are automatically derived features, obtained directly from LLM hidden-layer activations
without the need for explicit feature engineering. Quantitative characterization of the instances relies on these feature
sets, while algorithm portfolios comprise methods identified in the literature as exhibiting complementary performance,
evaluated through standard metrics such as raw objective value and performance gap.
Table 3 summarizes the main characteristics of the case studies considered in this work. We selected four COP that
cover different combinatorial structures and search spaces (assignments, permutations, and graphs): namely, the Bin
Packing Problem (BPP), Graph Coloring Problem (GCP), Jobshop Scheduling Problem (JSP), and Knapsack Problem
(KP). These are well-established benchmark problems and are therefore not described here in detail. The corresponding
instances, together with their ISA features and algorithm performance data used for algorithm selection, are available
on the MATILDA6 website. For each COP, we report the number of benchmark instances, the number of algorithms
included in the portfolio, and the number of features considered (— means no feature of that type). The column ISA
refers to the study from which the instances, algorithm portfolios, and the features labeled as ISA were originally
derived. The columns STANDARD DESC. and CODE-LIKE indicate the source of the standardized and code-like
representations, respectively.
3.2
Direct Querying of Large Language Models
Direct querying is employed to assess the extent to which LLMs can identify or describe relevant characteristics of
COPs based on given instance representations. This approach focuses on externally observable behavior.
Prompts are constructed according to the Reasoning–Task–Format (RTF) framework to ensure consistent task framing
and output structure across all models. Each prompt comprises three components:
• Reasoning preamble: defines the assumed role/domain expertise;
6https://matilda.unimelb.edu.au/matilda/, accessed 13 Oct 2025.
7


--- Page 8 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
Table 3: Summary of the considered problems.
Problem
Instances
Algor-
ithms
Features for the direct querying and probing extraction
ISA
Standard
desc.
Code
-like
List
Type
Compl.
Int
Float
BP
8,815
2
Bin capacity, items, maximum/mini-
mum/average item weight
4
1
2
2
1
[29]
GCP
8,278
2
Nodes, edges, graph density, ratio of
the number of nodes to the number
of edges in the graph and vice versa,
degree of the first node, maximum /
minimum / average degree
5
4
2
—
7
[47]
[17]
JSP
4,467
12
Jobs, machines, operations, maxi-
mum / minimum / average duration
5
1
2
3
1
[53]
[34]
KP
5,300
3
Capacity, maximum / minimum / av-
erage weight/profit, weight / profit /
efficiency of the first item, average
efficiency
7
4
1
7
3
[48]
[34]
• Task description: specifies the reasoning objective or operation to be performed;
• Format specification: constrains the response to a predefined structure, ensuring syntactic validity and
facilitating automated evaluation.
The prompt template used in this study is presented below. It shows how the reasoning preamble, task description, and
format specification are combined.
You are given an instance of a combinatorial optimization problem: [full-problem-name].
This is not a coding task. Do not return any code.
Extract the numeric value of the following feature from the instance:
• Feature name: [feature-name]
• Feature description: [feature-description]
• Expected type: [feature-type]
The instance is provided here: “““
[instance]
”””
Instructions:
• Return a JSON object only, with a single field “value”, i.e., ‘{"value": ...}’.
• The "value" field should contain the numeric value of the required feature and should be of the expected
type (i.e., [feature-type]).
• If the value is unknown/undeterminable, return ‘{"value": null}’.
• No explanations, no extra fields.
Answer:
Additionally, the LLM is equipped with a structured decoding layer,7 which guides the generation process and constrains
the output to follow a predefined JSON schema. This mechanism ensures both syntactic validity and type consistency
with the feature being predicted. The template of the schema is reported below, where [feature-type] indicates the
expected data type of the target feature (e.g., int or float).
7https://blog.vllm.ai/2025/01/14/struct-decode-intro.html, accessed 17 Oct 2025.
8


--- Page 9 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
Instance
You ar e gi ven an 
i nst ance of  a 
combi nat or i al  
opt i mi zat i on 
pr obl em. . .
Prompt
Large Language Model
Structured 
decoding
0. 14
Output
Hand-crafted feature
Problem description
Feature information
0.14
Comparison
Extraction of
Ground truth
Figure 2: Overview of the direct querying pipeline.
{
"type": "object",
"additionalProperties": false,
"required": ["value"],
"properties": {
"value": { "anyOf": [ { "type": "[feature-type]" }, { "type": "null" } ] }
}
}
The correctness of model-generated features is evaluated by comparing them against ground-truth values computed
from the corresponding instances, using both exact-match and error-based criteria.
The overall setup of the direct querying experiment is depicted in Figure 2. For each problem, the LLM receives a
prompt as per template above defined, that reports an instance represented in one of three formats together with a textual
instruction specifying the target feature. The output conforms to the structured decoding format previously introduced.
The direct querying procedure is repeated for all features defined for each problem. Predicted feature values are then
compared with ground-truth values obtained by running handcrafted feature extractors on the same instance.
3.3
Probing of Large Language Models
Probing is employed to investigate whether LLMs encode information about the structural and numerical characteristics
of CO instances. In contrast to direct querying (Section 3.2), which analyzes externally observable behavior, probing
examines the latent representations learned by the model, providing a complementary perspective on how instance
properties are internally encoded.
For each combination of problem and instance representation, every instance is processed by the model without
triggering text generation. Hidden-layer activations are extracted from the final layer, and a single vector representation
for each instance is obtained by applying three common pooling strategies: mean, max, and last (see, e.g., Tang
and Yang [55]). The mean strategy computes the average of the last hidden-layer activations across all tokens; the
max strategy takes the maximum activation along each representation dimension across tokens; and the last strategy
uses the activation vector corresponding to the End-Of-Sequence (EOS) token, i.e., the model final prediction signal
indicating the termination of generation. These pooled vectors provide compact, contextualized embeddings of each
instance, which are then used as input to downstream probing models to assess whether the model internally encodes
instance-level structural and numerical characteristics.
Each pooled vector is then paired with the ground-truth feature values corresponding to the same instance, forming a
dataset for supervised probing. For each feature and representation, we train a separate regression model (probe) to
assess whether the feature can be accurately decoded from the hidden representations, thereby quantifying the extent to
which such information is linearly accessible within the model latent space.
To assess both the accessibility and the complexity of the information encoded in the model representations, we employ
three types of probes:
9


--- Page 10 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
• Linear probe: a standard linear regressor used as a baseline. It follows a conventional preprocessing pipeline
including feature standardization and removal of zero-variance components. High performance with this probe
indicates that the feature is linearly decodable from the hidden representation.
• Simple probe: a one-hidden-layer MLP. The input corresponds to the pooled hidden representation (dimension
3072), followed by a hidden layer of 128 neurons with Rectified Linear Unit (ReLU) activation and a single
scalar output predicting the target feature value. This probe can capture mild non-linear relationships and
serves to test whether limited model capacity improves decoding accuracy.
• Complex probe: a LightGBM regressor designed to capture richer, non-linear dependencies. It is used to
assess whether additional model capacity yields substantial accuracy gains, which would suggest that the
information is encoded in a non-linearly separable manner within the representation.
The evaluation protocol mirrors that adopted for direct querying (Section 3.2), i.e., comparing predicted and ground-truth
feature values through exact-match and error-based metrics.
The same probing methodology, considering the adaption to classifiers instead of regressors, is subsequently extended
to evaluate whether algorithmic performance signals, instead of instance features, are implicitly represented within the
hidden states of the model (Section 3.4).
3.4
Algorithm Selection using Large Language Models
We further extend the probing analysis to examine whether the information encoded in the pooled layers of LLMs
can support downstream tasks, specifically per-instance algorithm selection. In this setting, probing is formulated as a
classification problem, where the objective is to identify, for each instance, the algorithm expected to achieve the best
performance based on its latent representation.
The setup for this experiment mirrors that of the feature-probing analysis, employing the same probe types, with
the key distinction that the target variable is now categorical—that is, the label of the algorithm achieving the best
performance—rather than continuous. Each LLM-derived representation is paired with the ground-truth label of the
best-performing algorithm for the corresponding instance, forming a dataset for supervised classification. A classifier
probe is then trained to determine whether the identity of the winning algorithm can be decoded from the model hidden
representations.
Performance is evaluated in terms of set-aware accuracy, which accounts for instances where multiple algorithms
achieve identical best performance (ties). During training, one of the tied algorithms is randomly selected as the target
label, whereas during evaluation, a prediction is considered correct if the classifier selects any of the algorithms attaining
the best performance for that instance. Formally, given n instances and for each instance i a set of acceptable labels Si,
set-aware accuracy is defined as:
Accset = E(xi,Si)

1{ˆy(xi) ∈Si}

(1)
where ˆy(xi) denotes the predicted (single) label for instance i, described by the features xi, and 1{·} is the indicator
function. E is the expected value. A prediction is thus counted as correct if it matches any of the true winning algorithms
for that instance.
To preserve balanced distributions across instances with different sets of winning algorithms, stratified sampling is
performed over the powerset of possible labels (excluding the empty set). Each subset of algorithms defines a stratum,
ensuring similar proportions of label sets in the training and evaluation splits.
To contextualize the results, we include comparisons against three established baselines:
• Most frequent winner: a naive baseline that always predicts the algorithm most frequently achieving the best
performance in the training set.
• ISA-based classifiers: models trained on instance features derived from previous ISA studies. We consider
three variants: a linear model, a simple baseline model, and a more complex one.
• Handcrafted feature classifiers: models trained on the handcrafted features employed in the preceding
experiments. We consider three variants: a linear model, a simple baseline model, and a more complex one.
Classifier performance is evaluated under a stratified k-fold cross-validation protocol. All classifiers are trained and
tested on identical data partitions, ensuring that each fold exposes the models to the same training and test instances.
These partitions are also reused in the subsequent mixed linear analysis to establish paired comparisons of model
effects, enabling a direct assessment of whether LLM-derived representations achieve comparable predictive power to
traditional ISA-based and handcrafted features.
10


--- Page 11 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
Large Language Model
Instance
...
Extraction
Pooling
Probe 
(regression)
Extraction of 
Winner algorithm
0.14
Ground truth
Dataset 
construction
Comparison
predicts
0.14
Simulated 
Annealing
Solved with a 
predicts
Dataset 
construction
Hand-crafted feature
Dataset 
construction
Portfolio of algorithms
Comparison 
Probe 
(classification)
Traditional algorithm 
selection 
(classification)
Simulated 
Annealing
Ground truth
Comparison 
Simulated 
Annealing
predicts
Figure 3: Overview of the probing process.
The overall probing workflow is illustrated in Figure 3. Each instance is processed by the LLM, from which hidden
representations are extracted and pooled to obtain fixed-length embeddings. These embeddings are paired with ground-
truth labels to construct datasets for regression and classification probing. The regression probes test whether instance
features can be decoded from the latent space, while the classification probes assess whether algorithmic performance
patterns are implicitly captured within it.
3.5
Experimental Setup
All experiments are conducted using the Llama-3.2-3B-Instruct model, selected for its extended context window,
which is essential given the long token sequences produced by all instance representations, particularly the natural
language representation. Other available models were not suitable due to their shorter context limitations. Moreover,
commercial LLMs were deliberately avoided to ensure full reproducibility of the research and to contain costs, which
would otherwise preclude large-scale experimentation of the kind reported in this study.
Figure 4 presents the token distributions for the four COPs, highlighting in different colors the three instance representa-
tions. Overall, the context length of the Llama-3.2-3B-Instruct model (131,072 tokens) is not exceeded for any of
the problems considered, allowing the entire input to be processed without truncation. This ensures that all instance
information is preserved and no input modification is required.
The model is accessed via the Hugging Face interface8 and executed using the VLLM library,9 which provides improved
robustness and efficiency over the standard transformers implementation. VLLM enables effective parallelization of
both text generation in direct querying and hidden-state extraction in probing, allowing efficient processing of large
batches and long input contexts. Hidden activations are fully extracted through VLLM and aggregated using the pooling
strategies described in Section 3.3.
Experiments are executed on an NVIDIA DGX Station equipped with four NVIDIA A100 GPUs (80 GB each). The
full experimental set comprises 26,860 instances, each evaluated under three distinct representations. Separate runs are
performed for direct querying, which involves token generation, and probing, which is computationally faster. The total
runtime amounts to approximately two weeks for direct querying and one week for probing. Post-processing tasks,
including regression and classification analyses, are carried out offline on a MacBook Pro (Apple M4, 32 GB RAM,
4+6 cores). Random seeds, problem sets, and replicate blocks are fixed across all runs to ensure consistent experimental
8https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct, accessed 30 Oct 2025.
9https://docs.vllm.ai/en/latest/, accessed 30 Oct 2025.
11


--- Page 12 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
0
1.0K
2.0K
3.0K
4.0K
5.0K
6.0K
7.0K
8.0K
0
2.0K
4.0K
Frequency [#]
BPP
0
10.0K
20.0K
30.0K
40.0K
50.0K
60.0K
0
1.0K
GCP
0
10.0K
20.0K
30.0K
40.0K
50.0K
60.0K
70.0K
80.0K
Token [#]
0
1.0K
2.0K
Frequency [#]
JSP
4.0K
6.0K
8.0K
10.0K
12.0K
14.0K
16.0K
Token [#]
0
2.0K
4.0K
KP
Standard
Natural language
Code-like
Figure 4: Number of tokens per problem and type of instance representation.
conditions. The complete repository, including configuration files, experimental scripts, and processing pipelines, is
openly released on Zenodo [16].
The probing models are implemented using scikit-learn, with LinearRegression for regression, MostFrequentClas-
sifier and LogisticRegression for classification, and PyTorch with the skorch adapter for the MLP probes applied to
both tasks. Furthermore, LightGBM is used for regression and classification alike. The scikit-learn framework was
selected for its interoperability and consistency, allowing a unified experimental pipeline across all probe types. An ad
hoc adapter class was developed for the set-aware classification task to ensure the correct computation of its evaluation
metric within the same workflow.
Regarding the algorithm portfolios, the results are drawn from the original ISA studies (also referenced in Table 3),
together with the corresponding instances and features labeled as ISA features. The ground-truth features employed
in the experiments presented in Sections 3.2 and 3.3 are computed by dedicated feature-extraction scripts developed
in Python. The handcrafted feature extractors are included in the project code base and integrated within a data
preparation pipeline that processes the benchmark instances and computes the relevant features for the downstream
LLM-related experiments.
The experiments are organized as follows. In the direct querying setting, the prompt template is instantiated with each
problem instance in its given representation (for all three representations), together with the target feature, its description,
and the structured decoding schema guiding the LLM generation. Results are collected for every problem–feature
pair, yielding a total of 611,037 experiments, each executed independently within the LLM pipeline (Figure 2). In the
probing setting (pipeline reported in Figure 3), only the instance representation is provided to the network, resulting
in 80,580 experiments. The last hidden state is extracted, pooled, and used to train regression models on a 70%/30%
train–validation split, with a dedicated regressor trained for each feature. In the algorithm selection setting, both the
pooled last hidden states—covering all combinations of representation and pooling strategy—and the manually designed
features (handcrafted or ISA-based) are used as classifier inputs. Depending on the number of algorithms considered
(see Table 3, third column), either binary or multi-class models are trained using a stratified k-fold cross-validation
scheme (k = 5) for each problem. Overall, the GPU-intensive tasks amount to 691,617 LLM executions, while the
regression and classification models were trained offline on the laptop computer, comprising 1,833,111 regression and
322,320 classification experiments.
4
Experimental Analysis
This section reports the results of the experimental analysis conducted to evaluate the behavior and representational
properties of LLMs when applied to COPs. The experiments follow the methodology introduced in Section 3,
encompassing three complementary analyses: direct querying, feature probing, and algorithm selection probing. Each
analysis focuses on a distinct aspect of model behavior: respectively, explicit feature identification, implicit feature
encoding, and predictive modeling of algorithmic performance.
4.1
Direct Querying of Problem Features
We begin the experimental analysis by examining the outcomes of the direct querying experiments. Table 4 summarizes
the results across the four COPs, grouped by feature complexity:
for features directly extractable from the instance
12


--- Page 13 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
Table 4: Direct querying metrics by problem and representation.
,
,
indicate features of increasing extraction
complexity: directly extractable, low-effort computation, and high-effort computation, respectively. With —, we
indicate that no features of such type are present. Except for the MAE, in all other case the value reported is a proportion
w.r.t. the total.
MAE
Equals
Within 1%
Within 5%
BPP
code-like
34.82
70.85
133.32
0.96
0.46
0.00
0.96
0.51
0.05
0.96
0.54
0.22
natural language
2.98
27.33
158.88
1.00
0.90
0.00
1.00
0.90
0.02
1.00
0.91
0.08
standard
471.22
197.49
102.77
0.44
0.23
0.00
0.44
0.25
0.05
0.44
0.28
0.23
GCP
code-like
12.56
—
81.31
1.00
—
0.01
1.00
—
0.04
1.00
—
0.09
natural language
0.00
—
179.94
1.00
—
0.02
1.00
—
0.08
1.00
—
0.13
standard
910.47
—
22.89
0.54
—
0.01
0.54
—
0.02
0.55
—
0.05
JSP
code-like
0.00
446.61
9.59
1.00
0.44
0.12
1.00
0.45
0.18
1.00
0.49
0.34
natural language
0.42
541.31
5.97
0.98
0.37
0.14
0.98
0.37
0.24
0.98
0.41
0.47
standard
20.53
643.63
18.08
0.49
0.10
0.01
0.49
0.11
0.05
0.49
0.14
0.21
KP
code-like
9,331.44
583,906.78
381.53
0.97
0.19
0.00
0.97
0.21
0.02
0.97
0.24
0.03
natural language
3,968.89
6,984.98
259.40
0.99
0.38
0.00
0.99
0.46
0.03
0.99
0.50
0.09
standard
261,452.48
24,747.88
341.84
0.06
0.11
0.00
0.06
0.15
0.01
0.06
0.19
0.02
representation,
for those requiring low-effort computations (e.g., counts or extrema), and
for features involving more
complex calculations or aggregations. Performance is assessed using multiple metrics: Mean Absolute Error (MAE),
computed as the average absolute deviation from the ground truth; the proportion of exact matches (EQUALS); and the
proportions of predictions within 1% (WITHIN 1%) and 5% (WITHIN 5%) deviation from the ground truth values.
These latter metrics provide a more sensitive view of near-accurate predictions, complementing the MAE analysis.
Overall, the direct querying experiments show that LLMs can reliably infer simple instance features directly from
textual or code-like representations, while struggling with features requiring even minimal computation. Performance is
highest for
features across all problems, with accuracy approaching perfect exact matches, but declines notably for
and
features, indicating limited numerical and procedural reasoning capabilities as expected [41].
Among representations, natural language consistently yields the best results, followed by code-like, whereas standard
inputs perform markedly worse. This suggests that LLMs rely primarily on semantic and contextual cues rather than
structural or symbolic encodings. Generation stability is high, with non-null responses produced in over 99.8% of all
queries, indicating consistent model behavior across problems and representations.
To investigate whether these limitations stem from the models reasoning behavior or from the nature of their internal
representations, the next analysis turns to feature probing. This approach aims to determine whether the information
underlying more complex features is nevertheless implicitly encoded within the latent space, even if it cannot be
explicitly retrieved through direct querying.
4.2
Feature Probing
We now turn to probing to examine whether LLMs implicitly encode structural and numerical information about
problem instances within their internal representations. In contrast to direct querying, which evaluates explicit reasoning
ability, feature probing tests whether such information can be linearly or non-linearly decoded from hidden-layer
activations, offering a deeper insight into the model representational structure.
We compute MAE values for each problem, representation, and feature complexity level, comparing the performance
of direct querying and probing. These results are reported in A and summarized in Figure 5, which illustrates the
difference in MAE between the two approaches across all configurations. In these plots, negative values indicate that
probing yields lower error than direct querying, whereas positive values denote configurations in which direct querying
performs better.
The comparative analysis of MAE values across problems and feature complexity levels reveals a complementary
relationship between direct querying and probing. For high-computation features, probing exhibits a clear advantage,
reflecting a greater capacity to capture and exploit the complex relational information embedded within the problem
representations. Conversely, direct querying performs better for extraction or low-computation features, particularly
with the natural language representation. This finding confirms and extends the observations from the direct querying
13


--- Page 14 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
-400
-200
0
200
400
mean
LinearRegression, max
last
mean
MLPRegressor, max
last
mean
LightGBM, max
last
mean
LinearRegression, max
last
mean
MLPRegressor, max
last
mean
LightGBM, max
last
mean
LinearRegression, max
last
mean
MLPRegressor, max
last
mean
LightGBM, max
last
BPP
-800
-600
-400
-200
0
mean
LinearRegression, max
last
mean
MLPRegressor, max
last
mean
LightGBM, max
last
mean
LinearRegression, max
last
mean
MLPRegressor, max
last
mean
LightGBM, max
last
mean
LinearRegression, max
last
mean
MLPRegressor, max
last
mean
LightGBM, max
last
GCP
-600
-500
-400
-300
-200
-100
0
MAE difference
mean
LinearRegression, max
last
mean
MLPRegressor, max
last
mean
LightGBM, max
last
mean
LinearRegression, max
last
mean
MLPRegressor, max
last
mean
LightGBM, max
last
mean
LinearRegression, max
last
mean
MLPRegressor, max
last
mean
LightGBM, max
last
JSP
-600K -500K -400K -300K -200K -100K
0
100K 200K
MAE difference
mean
LinearRegression, max
last
mean
MLPRegressor, max
last
mean
LightGBM, max
last
mean
LinearRegression, max
last
mean
MLPRegressor, max
last
mean
LightGBM, max
last
mean
LinearRegression, max
last
mean
MLPRegressor, max
last
mean
LightGBM, max
last
KP
code-like
natural language
standard
code-like
natural language
standard
code-like
natural language
standard
code-like
natural language
standard
Feature Types
Figure 5: Difference in MAE of Different Prompting with respect to the Direct Querying.
14


--- Page 15 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
0.0
0.5
1.0
Proportion
standard
code-like
BPP
natural language
0.0
0.5
1.0
Proportion
GCP
0.0
0.5
1.0
Proportion
JSP
0.0
0.5
1.0
Proportion
KP
LightGBM (last)
LightGBM (max)
LightGBM (mean)
LinearRegression (last)
LinearRegression (max)
LinearRegression (mean)
MLPRegressor (last)
MLPRegressor (max)
MLPRegressor (mean)
Direct Querying
Figure 6: Probing and Direct Querying results within 1% to the Ground Truth. We distinguish by feature complexity
(grouped bars), problem (rows), and problem representations (columns).
analysis, showing that the method remains most effective when retrieving explicit, easily accessible descriptors that
require minimal or no transformation.
Beyond the MAE, the approximation capability of each approach was further assessed by measuring the proportion
of predictions falling within 1% of the ground truth. The corresponding results, shown in Figure 6, compare the
performance of the different probing configurations. For completeness, the figure also includes results for direct
querying, with the analysis distinguishing among feature complexity, problem type, and representation.
The findings confirm and extend the trends observed in the MAE analysis. Direct querying remains more effective for
the natural language representation and simple extraction tasks, achieving agreement rates (within 1%) above 0.9
across all problems. In contrast, no single probing configuration emerges as a universal best performer, although the
MLPRegressor generally underperforms relative to the other probes.
Taken together, these results indicate that direct querying and probing capture distinct yet complementary aspects of
the feature space. The former excels with explicit extractions and structurally simple descriptors, whereas the latter
15


--- Page 16 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
leverages higher-level, computed features. Nevertheless, neither approach alone yields satisfactory performance across
all settings.
4.3
Algorithm Selection Probing
While the previous analyses show that certain features cannot be explicitly extracted through prompting, it remains
an open question whether the information required for such features is nevertheless encoded within the model hidden
representations. To explore this, we assess whether these latent embeddings can match the effectiveness of handcrafted
features in the key downstream task of algorithm selection. A complete summary of the statistical analyses performed
below is reported in B.
4.3.1
Comparative Statistical Analysis of Classifier Performance
This subsection presents a comparative statistical analysis of the classifiers employed in the algorithm selection task,
aiming to determine whether their predictive performances differ significantly. The analysis serves as a preliminary
validation step to ensure that the choice of classifier does not confound the interpretation of subsequent results. To this
end, multiple classifiers are evaluated under identical data partitions.
To assess the influence of classifier type on prediction accuracy,10 we fitted a linear mixed-effects model with the block
(problem, replicate) as a random intercept to account for repeated measurements across problem instances. The model
included the classifier as a fixed factor and was estimated via maximum likelihood. This formulation enables testing for
systematic performance differences among classifiers while controlling for variability attributable to specific problems.
The overall effect of the classifier was highly significant (likelihood-ratio test, p < 0.001), indicating that classifier
choice substantially affects predictive accuracy. The estimated fixed effects, expressed relative to the reference
classifier (MostFrequentClassifier), show that both LogisticRegression and LightGBM achieved significantly higher
accuracies than the baseline (+0.192 and +0.149 points, respectively; p < 0.001 for both), whereas the MLPClassifier
exhibited a small but statistically significant decrease in accuracy (−0.011 points; p = 0.044). The random-effect
variance associated with the problem grouping (σ2 = 0.012) indicates moderate between-problem variability, suggesting
that classifier effects are largely consistent across different problem types.
Figure 7 shows the distribution of classifier accuracies for each problem type. Overall, the results confirm the trends
identified by the mixed-effects model: the LightGBM and LogisticRegression classifiers consistently outperform the
MostFrequentClassifier baseline across all problems, whereas the MLPClassifier yields comparable or slightly lower
performance.
Performance differences are also problem-dependent. For example, all classifiers perform relatively well on BPP, while
the spread of accuracies is larger for JSP and KP, indicating greater variability in problem difficulty and model stability.
This pattern aligns with the non-zero between-problem variance estimated by the mixed-effects model.
Given these results, the two underperforming classifiers, i.e., MostFrequentClassifier and MLPClassifier, are excluded
from the subsequent analyses to focus on models that demonstrate stable and competitive predictive behavior. This
ensures that the remaining comparisons more accurately reflect the representational capacity of the features rather than
limitations of the classification models themselves.
4.3.2
Comparative Analysis of ISA-Based and Handcrafted Feature Sets
We now compare the predictive behavior of the ISA-based and handcrafted feature sets to assess whether they
yield systematically different outcomes in the algorithm selection task. While ISA-based features are designed as
comprehensive descriptors and may include or extend the handcrafted ones, it is nevertheless important to evaluate
whether this broader coverage translates into superior predictive power. Establishing this comparison also provides a
consistent baseline for the subsequent analysis, where the predictive power and robustness of manually engineered
features are contrasted with those derived from LLM hidden representations.
To examine whether the predictive behavior of the two baseline feature sets differs systematically, we fit a linear
mixed-effects model with classifier as a fixed factor and block, defined as the (problem, replicate) pair, as a random
intercept. The analysis identified a significant main effect of the specific baseline employed (LRT χ2(1) = 40.0,
p < 0.001). The handcrafted feature baseline achieved a mean accuracy of 0.786 (95% CI [0.737, 0.836]), while the
ISA-based representation yielded an increase of 0.031 points (95% CI [0.023, 0.039]). The model revealed a significant
main effect of classifier, with LightGBM yielding an increase of accuracy of 0.091 points (95% CI [0.083, 0.099]) with
10For these analyses the term accuracy has to be intended in the set-aware sense presented in Section 3.4
16


--- Page 17 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
LogisticRegression
MLP
MostFrequent
LightGBM
0.800
0.825
0.850
0.875
0.900
0.925
0.950
Accuracy
Problem: BPP
LogisticRegression
MLP
MostFrequent
LightGBM
0.75
0.80
0.85
0.90
0.95
1.00
Problem: GCP
LogisticRegression
MLP
MostFrequent
LightGBM
Classifier
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Accuracy
Problem: JSP
LogisticRegression
MLP
MostFrequent
LightGBM
Classifier
0.2
0.3
0.4
0.5
0.6
0.7
Problem: KP
Figure 7: Classification accuracy by problem and classifier. Each panel shows the mean and variability of the accuracy
achieved by the evaluated classifiers across the benchmark problems in the different probing settings.
respect to Logistic Regression. The estimated between-problem variance (σ2 = 0.013) suggests, also in this case,
moderate heterogeneity but consistent improvement across problems.
Complementary tests provide a nuanced view of this effect. A pairwise Tukey comparison on unadjusted data did not
detect a statistically significant difference, reflecting the variability introduced by individual problems. Nonetheless, a
non-parametric Wilcoxon-based Two One-Sided Tests (TOST) procedure with an equivalence margin of 0.01 failed
to demonstrate equivalence (p ≈1 for the upper bound), indicating that the two feature sets cannot be considered
practically interchangeable. Overall, the ISA-based features offer a modest yet consistent advantage over the handcrafted
baseline, supporting their use as the primary reference in subsequent analyses.
4.3.3
Analysis of Representation and Pooling Effects on Probing Performance
To investigate how different combinations of instance representation and pooling strategy influence probing performance,
a full-factorial analysis is conducted to evaluate main and interaction effects between the three representations (standard,
code-like, and natural language) and the three pooling strategies (mean, max, and last) on classifier accuracy. The
analysis controls for problem-specific variability to isolate systematic effects attributable to representation and pooling
choices, providing insight into how these factors shape the extractable information from LLM hidden activations.
We fitted a linear mixed-effects model including classifier as a fixed factor and block (i.e., the pair (problem, replicate))
as a random intercept to assess how different combinations of representation and pooling activations influence classifier
performance in the algorithm selection task. The model evaluated the main and interaction effects of the two factors,
while accounting for systematic differences between classifiers. Likelihood ratio tests indicated significant main
effects of both representation (χ2(6) = 21.32, p = 0.0016) and pooling (χ2(6) = 26.61, p < 0.001), whereas the
representation × pooling interaction was not significant (χ2(4) = 6.77, p = 0.15). A main effect of classifier was also
17


--- Page 18 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
standard
code-like
natural language
0.945
0.950
0.955
Accuracy
Problem: BPP
standard
code-like
natural language
0.980
0.982
0.984
0.986
0.988
0.990
Problem: GCP
standard
code-like
natural language
Representation
0.860
0.880
0.900
Accuracy
Problem: JSP
standard
code-like
natural language
Representation
0.640
0.660
0.680
0.700
Problem: KP
Pooling
mean
max
last
Figure 8: Effects of representation and pooling in the full factorial experiment. Each panel shows the mean accuracy
(95% CI) for combinations of representation and pooling, averaged across problems and classifiers.
observed, with LightGBM outperforming Logistic Regression by approximately 0.03 accuracy points (p < 0.001).
Overall, these results suggest that classifier choice and the two representation-related factors contribute independently
to performance, without evidence of systematic interactions.
The estimated coefficients indicate that the natural language representation performs on par with the standard
representation (difference < 0.01, p = 0.81), confirming that representation has no practical influence overall. Although
the global likelihood-ratio test for the representation factor was significant (χ2(6) = 21.32, p = 0.0016), pairwise
contrasts revealed no systematic advantage for any specific representation. The between-problem variance (σ2 = 0.014)
still indicates moderate heterogeneity, suggesting that these patterns are largely consistent across problems.
On the other hand, among pooling strategies, only max pooling showed a statistically significant advantage over the
mean baseline (+0.011, p = 0.005), while the effect of last pooling was not significant (−0.007, p = 0.064). Although
the magnitude of the improvement is modest, this suggests a slight benefit from aggregating activations via the maximum
rather than the mean.
Figure 8 illustrates the mean accuracies (with 95% confidence intervals) obtained for each problem under different
combinations of representation and pooling strategy. Across all problems, max pooling consistently provides a slight
advantage over the other two pooling methods, regardless of the underlying representation. The effect of representation,
however, is less consistent and appears to depend on the specific problem. This variability suggests that the influence of
representation may interact with problem characteristics, warranting further investigation on a broader set of COPs, a
direction that lies beyond the scope of the present study.
Overall, the analysis confirms that while pooling strategies exert statistically detectable effects, their practical impact on
accuracy is minor. Classifier choice remains the dominant determinant of performance, underscoring the robustness
of LLM-derived embeddings. Nevertheless, for the final analysis we adopt a single configuration consisting of max
pooling and the standard representation to provide a consistent basis for comparison with the baseline models.
4.3.4
Comparative Analysis of ISA and LLM-Derived Representations for Algorithm Selection
In this final analysis, we compare the algorithm selection performance obtained using traditional ISA-based features with
that achieved from LLM-derived hidden-layer activations. The aim is to determine whether information encoded within
LLMs can match or exceed the predictive power of handcrafted instance descriptors in identifying the per-instance best
algorithm. The comparison is conducted between the best-performing configurations identified in the previous analyses:
18


--- Page 19 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
0.7
0.8
0.9
1.0
Accuracy (LLM activations)
0.70
0.75
0.80
0.85
0.90
0.95
1.00
Accuracy (ISA features)
BPP
GCP
JSP
KP
Figure 9: Comparison of Accuracies in Algorithm Selection between ISA features equipped LightGBM and LLM
activation-based features under standard representation.
the ISA-based feature set and the LLM-based setup using max pooling with the standard representation. In both cases,
the LightGBM classifier is employed, as it consistently demonstrated superior predictive performance across earlier
experiments.
Although the fixed effect of representation (ISA vs. LLM) is statistically significant and positive (+0.008, p <
0.001), the estimated between-problem variance (σ2 = 0.012) exceeds the fixed-effect magnitude, indicating that the
improvement provided by ISA is smaller than the variability observed across problems. A non-parametric Wilcoxon-
based TOST, applied to the ranked paired differences, confirmed equivalence at ε = 0.01. The Hodges–Lehmann
95% confidence interval for the median paired difference was [−0.0039, 0.0244], with a median difference of 0.0054,
entirely within the ±0.01 equivalence region. Overall, these results suggest that although the tiny effect of ISA is
statistically detectable, it is practically negligible, and that ISA and LLM can be considered indistinguishable in
predictive performance within a few hundredths of accuracy.
We conclude this analysis by comparing the average accuracies achieved by ISA and LLM in the algorithm selection
task across the different problems. Figure 9 illustrates this comparison, showing that the most challenging problem for
LLM is KP, which likely explains the lack of statistical equivalence observed in the previous tests. Unfortunately, we
are not able to perform a meaningful within-problem analysis, since the number of replicates per problem does not
allow for reliable conclusions. However, an informal inspection suggests that for KP the equivalence margin is around
±0.015, indicating that future studies should consider including problems with a similar structure. Nevertheless, the
observed differences remain within a few hundredths, suggesting that overall performance remains comparable.
5
Conclusions
We investigated how Large Language Models (LLMs) represent Combinatorial Optimization Problems (COPs) and
whether these internal representations can support downstream decision-making tasks. Through a combination of
direct querying and probing analyses, we examined both the explicit reasoning capabilities and the implicit knowledge
encoded within the models across four benchmark problems and three types of instance representations. Two tasks
were considered: feature extraction, assessing whether models can recover meaningful structural and numerical
characteristics from problem instances, and per-instance algorithm selection, testing whether internal representations
can predict the best solver for each instance.
Our results revealed that:
19


--- Page 20 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
• The direct querying experiments (Section 4.1) show that LLMs can infer simple, explicitly stated instance
features from natural language and code-like representations. For low-complexity features, the Mean
Absolute Error (MAE) is moderate and predictions fall within 1% of the ground truth value in most cases,
indicating a moderate surface-level comprehension. However, performance deteriorates markedly for features
requiring even limited computation or aggregation, confirming that current LLMs have difficulty performing
quantitative reasoning beyond explicit extraction.
• The probing analyses (Section 4.2) reveal that LLMs partially encode structural and numerical information
about Combinatorial Optimization (CO) instances in their internal representations. Moreover, such information
can be partially recovered through supervised decoding. In general, probing achieves lower errors with respect
to direct querying on complex features.
• Through an extensive experimental campaign (Section 4.3), we find that LLM-extracted latent representations
effectively serve as instance features surrogates for per-instance algorithm selection. Across all problems, max
pooling offers a consistent but modest advantage, while the effect of instance representation depends on the
problem type. When compared with traditional Instance Space Analysis (ISA)-based features, LLM-derived
embeddings achieve statistically comparable predictive performance, demonstrating that latent representations
can match the effectiveness of human-defined descriptors. Notably, the classifier choice remains critical: the
best-performing model, LightGBM, successfully captures non-linear relationships among both LLM-derived
and human-defined features, as shown by its better predictive performance.
The latter findings reveal an important trade-off. While human-defined features demand substantial domain expertise
and manual engineering, they are computationally inexpensive and inherently more interpretable. In contrast, LLMs
achieve comparable predictive power with minimal feature design effort but at a significantly higher computational cost
for their extraction and with a considerable loss of interpretability. This contrast highlights a broader question for future
research: how to balance interpretability, reproducibility, human effort, and energy efficiency when integrating LLMs
into data-driven optimization pipelines.
Our work is not without limitations. The experimental analysis is restricted to a single LLM family and four benchmark
COPs, which, although representative, do not capture the full diversity of optimization settings. Extending this
investigation to other model architectures, larger instance scales, and real-world optimization problems represents
an important direction for future research. Furthermore, recent progress in structured reasoning and tool-augmented
generation with LLMs offers promising opportunities to strengthen direct querying methods. Another promising avenue
lies in integrating LLM-derived activations into broader ISA pipelines, using them not only for algorithm selection but
also for tasks such as instance clustering, performance landscape modeling, and exploratory analysis of problem spaces.
Acknowledgments
The computational experiments were conducted using the LABIC research infrastructure, supported by funding from the
Regione Autonoma Friuli Venezia Giulia, the Italian Ministry of Foreign Affairs, and the Italian Ministry of University
and Research.
Declaration on the Use of Generative Artificial Intelligence
The authors acknowledge the use of ChatGPT to support language editing, including grammar correction and rephrasing.
Following its use, all text was critically reviewed, verified, and revised by the authors, who accept full responsibility for
the integrity and accuracy of the manuscript.
References
[1] AhmadiTeshnizi, A., Gao, W., Udell, M.: OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and
Large Language Models. In: Forty-first International Conference on Machine Learning, ICML 2024, Vienna,
Austria, July 21-27, 2024, pp. 1–20, OpenReview.net (2024), URL https://openreview.net/forum?id=
YT1dtdLvSN
[2] Ahmeti, A., Musliu, N.:
Hybridizing constraint programming and meta-heuristics for multi-mode
resource-constrained multiple projects scheduling problem. J. Heuristics 31(1), 1–37 (2025), doi:10.1007/
S10732-024-09540-3
[3] Alissa, M., Sim, K., Hart, E.: Automated Algorithm Selection: from Feature-Based to Feature-Free Approaches. J.
Heuristics 29(1), 1–38 (2023), doi:10.1007/S10732-022-09505-4
20


--- Page 21 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
[4] Alleman, M., Mamou, J., Rio, M.A.D., Tang, H., Kim, Y., Chung, S.: Syntactic perturbations reveal represen-
tational correlates of hierarchical phrase structure in pretrained language models. In: Rogers, A., Calixto, I.,
Vulic, I., Saphra, N., Kassner, N., Camburu, O., Bansal, T., Shwartz, V. (eds.) Proceedings of the 6th Workshop
on Representation Learning for NLP, RepL4NLP@ACL-IJCNLP 2021, Online, August 6, 2021, pp. 263–276,
Association for Computational Linguistics (2021), doi:10.18653/V1/2021.REPL4NLP-1.27
[5] Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning to align and translate. In:
Bengio, Y., LeCun, Y. (eds.) 3rd International Conference on Learning Representations, ICLR 2015, San Diego,
CA, USA, May 7-9, 2015, Conference Track Proceedings (2015), URL http://arxiv.org/abs/1409.0473
[6] Belinkov, Y.: Probing classifiers: Promises, shortcomings, and advances. Comput. Linguistics 48(1), 207–219
(2022), doi:10.1162/coli_a_00422
[7] Beloucif, M., Biemann, C.: Probing pre-trained language models for semantic attributes and their values. In:
Moens, M., Huang, X., Specia, L., Yih, S.W. (eds.) Findings of the Association for Computational Linguistics:
EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pp. 2554–2559,
Association for Computational Linguistics (2021), doi:10.18653/V1/2021.FINDINGS-EMNLP.218
[8] Cenikj, G., Petelin, G., Eftimov, T.: TransOptAS: Transformer-Based Algorithm Selection for Single-Objective
Optimization. In: Li, X., Handl, J. (eds.) Proceedings of the Genetic and Evolutionary Computation Conference
Companion, GECCO 2024, Melbourne, VIC, Australia, July 14-18, 2024, pp. 403–406, ACM (2024), doi:
10.1145/3638530.3654191
[9] Ceschia, S., Gansterer, M., Mancini, S., Meneghetti, A.: Solving the Online On-Demand Warehousing Problem.
Comput. Oper. Res. 170, 106760 (2024), doi:10.1016/J.COR.2024.106760
[10] Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y., Ye, W., Zhang,
Y., Chang, Y., Yu, P.S., Yang, Q., Xie, X.: A Survey on Evaluation of Large Language Models. ACM Trans. Intell.
Syst. Technol. 15(3), 39:1–39:45 (2024), doi:10.1145/3641289
[11] Chanin, D., Hunter, A., Camburu, O.: Identifying linear relational concepts in large language models. In: Duh, K.,
Gómez-Adorno, H., Bethard, S. (eds.) Proceedings of the 2024 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL
2024, Mexico City, Mexico, June 16-21, 2024, pp. 1524–1535, Association for Computational Linguistics (2024),
doi:10.18653/V1/2024.NAACL-LONG.85
[12] Civelli, S., Bernardelle, P., Demartini, G.: The impact of persona-based political perspectives on hateful content
detection. In: Long, G., Blumestein, M., Chang, Y., Lewin-Eytan, L., Huang, Z.H., Yom-Tov, E. (eds.) Companion
Proceedings of the ACM on Web Conference 2025, WWW 2025, Sydney, NSW, Australia, 28 April 2025 - 2 May
2025, pp. 1963–1968, ACM (2025), doi:10.1145/3701716.3718383
[13] Coster, A.D., Musliu, N., Schaerf, A., Schoisswohl, J., Smith-Miles, K.:
Algorithm selection and in-
stance space analysis for curriculum-based course timetabling. J. Sched. 25(1), 35–58 (2022), doi:10.1007/
S10951-021-00701-X
[14] Da Ros, F., Di Gaspero, L., Roitero, K.: Probing LLMs on optimization problems: Can they recall and interpret
problem features? In: García-Sánchez, P., Hart, E., Thomson, S.L. (eds.) Applications of Evolutionary Compu-
tation - 28th European Conference, EvoApplications 2025, Held as Part of EvoStar 2025, Trieste, Italy, April
23-25, 2025, Proceedings, Part II, Lecture Notes in Computer Science, vol. 15613, pp. 353–371, Springer (2025),
doi:10.1007/978-3-031-90065-5\_22
[15] Da Ros, F., Soprano, M., Di Gaspero, L., Roitero, K.: Large Language Models for Combinatorial Optimization: A
Systematic Review. arXiv (2025), URL https://arxiv.org/abs/2507.03637
[16] Di Gaspero, L., Da Ros, F.: Data and checkpoints for: Behavior and representation in llms for co (Dec 2025),
doi:10.5281/zenodo.17913884, URL https://doi.org/10.5281/zenodo.17913884
[17] DIMACS: The second dimacs international algorithm implementation challenge: General information (1992),
URL http://archive.dimacs.rutgers.edu/pub/challenge/gen_info.tex, accessed on 26 Oct 2025.
[18] Fan, Z., Ghaddar, B., Wang, X., Xing, L., Zhang, Y., Zhou, Z.: Artificial Intelligence for Operations Research:
Revolutionizing the Operations Research Process. arXiv (2024), URL https://arxiv.org/abs/2401.03244
[19] Huben, R., Cunningham, H., Smith, L.R., Ewart, A., Sharkey, L.: Sparse autoencoders find highly interpretable
features in language models. In: The Twelfth International Conference on Learning Representations, ICLR
2024, Vienna, Austria, May 7-11, 2024, OpenReview.net (2024), URL https://openreview.net/forum?id=
F76bwRSLeK
21


--- Page 22 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
[20] Ju, T., Sun, W., Du, W., Yuan, X., Ren, Z., Liu, G.: How large language models encode context knowledge? A
layer-wise probing study. In: Calzolari, N., Kan, M., Hoste, V., Lenci, A., Sakti, S., Xue, N. (eds.) Proceedings
of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation,
LREC/COLING 2024, 20-25 May, 2024, Torino, Italy, pp. 8235–8246, ELRA and ICCL (2024), URL https:
//aclanthology.org/2024.lrec-main.722
[21] Kim, N., Patel, R., Poliak, A., Wang, A., Xia, P., McCoy, R.T., Tenney, I., Ross, A., Linzen, T., Durme, B.V.,
Bowman, S.R., Pavlick, E.: Probing what different NLP tasks teach machines about function word comprehension.
arXiv (2019), URL http://arxiv.org/abs/1904.11544
[22] Kletzander, L., Musliu, N.: Hyper-heuristics for personnel scheduling domains. Artif. Intell. 334, 104172 (2024),
doi:10.1016/J.ARTINT.2024.104172
[23] Koto, F., Lau, J.H., Baldwin, T.: Discourse probing of pretrained language models. In: Toutanova, K., Rumshisky,
A., Zettlemoyer, L., Hakkani-Tür, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., Zhou, Y. (eds.)
Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 3849–3864,
Association for Computational Linguistics (2021), doi:10.18653/V1/2021.NAACL-MAIN.301
[24] Kumar, P.: Large language models (LLMs): survey, technical frameworks, and future challenges. Artif. Intell. Rev.
57(9), 260 (2024), doi:10.1007/S10462-024-10888-Y
[25] Laban, P., Fabbri, A.R., Xiong, C., Wu, C.S.: Summary of a haystack: A challenge to long-context LLMs and
RAG systems. arXiv (2024), doi:10.48550/arXiv.2407.01370
[26] Li, D., Zhao, H., Zeng, Q., Du, M.: Exploring multilingual probing in large language models: A cross-language
analysis. arXiv (2025), URL https://arxiv.org/abs/2409.14459
[27] Liu, C., Smith-Miles, K., Wauters, T., Costa, A.M.: Instance space analysis for 2d bin packing mathematical
models. Eur. J. Oper. Res. 315(2), 484–498 (2024), doi:10.1016/J.EJOR.2023.12.008
[28] Liu, F., Yao, Y., Guo, P., Yang, Z., Zhao, Z., Lin, X., Tong, X., Yuan, M., Lu, Z., Wang, Z., Zhang, Q.: A
Systematic Survey on Large Language Models for Algorithm Design. arXiv (2024), URL https://doi.org/
10.48550/arXiv.2410.14716
[29] Liu, K.: Using Instance Space Analysis to Study the Bin Packing Problem. Master Thesis (2020), URL https:
//matilda.unimelb.edu.au/matilda/matildadata/bin_packing/bin_packing.pdf
[30] Marks, S., Tegmark, M.: The geometry of truth: Emergent linear structure in large language model representations
of true/false datasets. arXiv (2024), URL https://arxiv.org/abs/2310.06824
[31] Matarazzo, A., Torlone, R.: A survey on large language models with some insights on their capabilities and
limitations. arXiv (2025), URL https://doi.org/10.48550/arXiv.2501.04040
[32] Meguellati, E., Pratama, N.A., Sadiq, S., Demartini, G.: Are large language models good data preprocessors? In:
Long, G., Blumestein, M., Chang, Y., Lewin-Eytan, L., Huang, Z.H., Yom-Tov, E. (eds.) Companion Proceedings
of the ACM on Web Conference 2025, WWW 2025, Sydney, NSW, Australia, 28 April 2025 - 2 May 2025, pp.
2129–2132, ACM (2025), doi:10.1145/3701716.3717568
[33] Michailidis, K., Tsouros, D.C., Guns, T.: CP-Bench: Evaluating Large Language Models for Constraint Modelling.
arXiv (2025), URL https://doi.org/10.48550/arXiv.2506.06052
[34] Minizinc:
The minizinc examples archive - v1.0.0 (2017), URL https://github.com/MiniZinc/
minizinc-examples/tree/master, accessed on 26 Oct 2025.
[35] do Nascimento, O.X., de Queiroz, T.A., Junqueira, L.: Practical constraints in the container loading problem:
Comprehensive formulations and exact algorithm. Comput. Oper. Res. 128, 105186 (2021), doi:10.1016/J.COR.
2020.105186
[36] Pan, X., Fang, J., Wu, F., Zhang, S., Hu, Y., Li, S., Li, X.: Guiding Large Language Models in Modeling
Optimization Problems via Question Partitioning. In: Proceedings of the Thirty-Fourth International Joint
Conference on Artificial Intelligence, IJCAI 2025, Montreal, Canada, August 16-22, 2025, pp. 2657–2665,
ijcai.org (2025), doi:10.24963/IJCAI.2025/296
[37] Pellegrino, A., Akgün, Ö., Dang, N., Kiziltan, Z., Miguel, I.: Transformer-Based Feature Learning for Algorithm
Selection in Combinatorial Optimisation. In: de la Banda, M.G. (ed.) 31st International Conference on Principles
and Practice of Constraint Programming, CP 2025, August 10-15, 2025, Glasgow, Scotland, LIPIcs, vol. 340, pp.
31:1–31:22, Schloss Dagstuhl - Leibniz-Zentrum für Informatik (2025), doi:10.4230/LIPICS.CP.2025.31
22


--- Page 23 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
[38] Ramamonjison, R., Yu, T.T.L., Li, R., Li, H., Carenini, G., Ghaddar, B., He, S., Mostajabdaveh, M., Banitalebi-
Dehkordi, A., Zhou, Z., Zhang, Y.: NL4Opt Competition: Formulating Optimization Problems Based on Their
Natural Language Descriptions. In: Ciccone, M., Stolovitzky, G., Albrecht, J. (eds.) NeurIPS 2022 Competition
Track, November 28 - December 9, 2022, Online, Proceedings of Machine Learning Research, vol. 220, pp.
189–203, PMLR (2021), URL https://proceedings.mlr.press/v220/ramamonjison22a.html
[39] Rice, J.R.: The algorithm selection problem. Adv. Comput. 15, 65–118 (1976), doi:10.1016/S0065-2458(08)
60520-3
[40] Romera-Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar, M.P., Dupont, E., Ruiz, F.J.R., Ellenberg,
J.S., Wang, P., Fawzi, O., Kohli, P., Fawzi, A.: Mathematical discoveries from program search with large language
models. Nat. 625(7995), 468–475 (2024), doi:10.1038/S41586-023-06924-6
[41] Roy, T.S., Baral, A., Jhaveri, A.R., Baig, Y.: Can LLMs understand math? - exploring the pitfalls in mathematical
reasoning. arXiv (2025), URL https://doi.org/10.48550/arXiv.2505.15623
[42] Sartori, C.C., Blum, C.: Combinatorial optimization for all: Using LLMs to aid non-experts in improving
optimization algorithms. arXiv (2025), URL https://doi.org/10.48550/arXiv.2503.10968
[43] Sartori, C.C., Blum, C., Ochoa, G.: STNWeb: A new visualization tool for analyzing optimization algorithms.
Softw. Impacts 17, 100558 (2023), doi:10.1016/J.SIMPA.2023.100558
[44] Sartori, C.C., Blum, C., Ochoa, G.: Large Language Models for the Automated Analysis of Optimization
Algorithms. In: Li, X., Handl, J. (eds.) Proceedings of the Genetic and Evolutionary Computation Conference,
GECCO 2024, Melbourne, VIC, Australia, July 14-18, 2024, pp. 160–168, ACM (2024), doi:10.1145/3638529.
3654086
[45] Shavit, H., Hoos, H.H.: Revisiting SATZilla Features in 2024. In: Chakraborty, S., Jiang, J.R. (eds.) 27th
International Conference on Theory and Applications of Satisfiability Testing, SAT 2024, August 21-24, 2024,
Pune, India, LIPIcs, vol. 305, pp. 27:1–27:26, Schloss Dagstuhl - Leibniz-Zentrum für Informatik (2024),
doi:10.4230/LIPICS.SAT.2024.27
[46] Sim, K., Renau, Q., Hart, E.: Beyond the hype: Benchmarking LLM-evolved heuristics for bin packing. In: García-
Sánchez, P., Hart, E., Thomson, S.L. (eds.) Applications of Evolutionary Computation - 28th European Conference,
EvoApplications 2025, Held as Part of EvoStar 2025, Trieste, Italy, April 23-25, 2025, Proceedings, Part II, Lecture
Notes in Computer Science, vol. 15613, pp. 386–402, Springer (2025), doi:10.1007/978-3-031-90065-5\_24
[47] Smith-Miles, K., Bowly, S.: Generating new test instances by evolving in instance space. Comput. Oper. Res. 63,
102–113 (2015), doi:10.1016/J.COR.2015.04.022
[48] Smith-Miles, K., Christiansen, J., Muñoz, M.A.: Revisiting where are the hard knapsack problems? via Instance
Space Analysis. Comput. Oper. Res. 128, 105184 (2021), doi:10.1016/J.COR.2020.105184
[49] Smith-Miles, K., Muñoz, M.A.: Instance Space Analysis for Algorithm Testing: Methodology and Software Tools.
ACM Comput. Surv. 55(12), 255:1–255:31 (2023), doi:10.1145/3572895
[50] van Stein, N., Bäck, T.: LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically Generating
Metaheuristics. IEEE Trans. Evol. Comput. 29(2), 331–345 (2025), doi:10.1109/TEVC.2024.3497793
[51] van Stein, N., Kononova, A.V., Kotthoff, L., Bäck, T.: Code Evolution Graphs: Understanding Large Language
Model Driven Design of Algorithms. In: Filipic, B. (ed.) Proceedings of the Genetic and Evolutionary Computation
Conference, GECCO 2025, NH Malaga Hotel, Malaga, Spain, July 14-18, 2025, pp. 943–951, ACM (2025),
doi:10.1145/3712256.3726328
[52] van Stein, N., Yin, H., Kononova, A.V., Bäck, T., Ochoa, G.: Behaviour space analysis of LLM-driven meta-
heuristic discovery. arXiv (2025), URL https://doi.org/10.48550/arXiv.2507.03605
[53] Strassl, S., Musliu, N.: Instance space analysis and algorithm selection for the job shop scheduling problem.
Comput. Oper. Res. 141, 105661 (2022), doi:10.1016/J.COR.2021.105661
[54] Talbi, E.: Machine learning into metaheuristics: A survey and taxonomy. ACM Comput. Surv. 54(6), 129:1–129:32
(2022), doi:10.1145/3459664
[55] Tang, Y., Yang, Y.: Pooling and attention: What are effective designs for LLM-based embedding models? (2024),
URL https://arxiv.org/abs/2409.02727
[56] Tsouros, D.C., Verhaeghe, H., Kadioglu, S., Guns, T.: Holy Grail 2.0: From Natural Language to Constraint
Models. arXiv (2023), URL https://doi.org/10.48550/arXiv.2308.01589
[57] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is
All you Need. In: Guyon, I., von Luxburg, U., Bengio, S., Wallach, H.M., Fergus, R., Vishwanathan, S.V.N., Gar-
nett, R. (eds.) Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information
23


--- Page 24 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998–6008 (2017), URL https://
proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html
[58] Vulic, I., Ponti, E.M., Litschko, R., Glavas, G., Korhonen, A.: Probing pretrained language models for lexical
semantics. In: Webber, B., Cohn, T., He, Y., Liu, Y. (eds.) Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 7222–7240,
Association for Computational Linguistics (2020), doi:10.18653/V1/2020.EMNLP-MAIN.586
[59] Wallace, E., Wang, Y., Li, S., Singh, S., Gardner, M.: Do NLP models know numbers? probing numeracy in
embeddings. In: Inui, K., Jiang, J., Ng, V., Wan, X. (eds.) Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language
Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pp. 5306–5314, Association for
Computational Linguistics (2019), doi:10.18653/V1/D19-1534
[60] Wang, Y., Li, K.: Large Language Models and Operations Research: A Structured Survey. arXiv (2025), URL
https://arxiv.org/abs/2509.18180
[61] Wolpert, D.H., Macready, W.G.: No free lunch theorems for optimization. IEEE Trans. Evol. Comput. 1(1), 67–82
(1997), doi:10.1109/4235.585893
[62] Wu, W., Ito, M., Hu, Y., Goko, H., Sasaki, M., Yagiura, M.: Heuristic algorithms based on column generation for
an online product shipping problem. Comput. Oper. Res. 161, 106403 (2024), doi:10.1016/J.COR.2023.106403
[63] Xu, L., Hutter, F., Hoos, H.H., Leyton-Brown, K.: SATzilla: Portfolio-based Algorithm Selection for SAT. J. Artif.
Intell. Res. 32, 565–606 (2008), doi:10.1613/JAIR.2490
[64] Yu, X., Shen, S., Badri-Koohi, B., Seada, H.: Time window optimization for attended home service delivery under
multiple sources of uncertainties. Comput. Oper. Res. 150, 106045 (2023), doi:10.1016/J.COR.2022.106045
24


--- Page 25 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
A
Feature Probing Results
Tables 5–8 report the MAE values obtained for each problem, representation, and feature complexity level, comparing
the performance of direct querying and probing.
25


--- Page 26 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
Table 5: Direct Querying and Probing MAE per Representation. Data represent the mean MAE over different pooling
strategies and feature complexities for probing methods for BPP.
Complexity
Representation
Regressor
Pooling
standard
LinearRegression
mean
116.40
62.44
61.56
max
424.40
232.91
232.53
last
512.41
271.10
272.45
MLP
mean
39.67
11.08
17.00
max
83.04
38.24
25.01
last
84.06
34.21
35.20
LightGBM
mean
140.37
75.37
73.52
max
131.02
74.24
71.97
last
183.57
87.63
85.08
Direct Querying
471.22
197.49
102.77
code-like
LinearRegression
mean
118.11
60.22
59.64
max
476.37
260.48
260.41
last
305.86
172.94
172.64
MLP
mean
80.45
22.55
21.12
max
407.87
45.10
31.61
last
189.94
42.54
33.15
LightGBM
mean
154.69
70.27
70.91
max
108.89
63.43
60.90
last
109.45
71.51
68.41
Direct Querying
34.82
70.85
133.32
natural language
LinearRegression
mean
127.82
70.72
70.13
max
507.07
286.23
287.06
last
493.88
270.37
270.17
MLP
mean
123.56
29.23
15.45
max
457.50
146.76
86.51
last
113.29
37.11
27.05
LightGBM
mean
146.52
75.71
73.81
max
66.67
47.76
46.52
last
165.65
96.26
93.12
Direct Querying
2.98
27.33
158.88
26


--- Page 27 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
Table 6: Direct Querying and Probing MAE per Representation. Data represent the mean MAE over different pooling
strategies and feature complexities for probing methods for GCP.
Complexity
Representation
Regressor
Pooling
standard
LinearRegression
mean
7.89
–
0.80
max
20.17
–
1.34
last
20.55
–
1.41
MLP
mean
13.65
–
1.03
max
21.05
–
1.54
last
18.53
–
1.18
LightGBM
mean
13.30
–
0.69
max
17.00
–
0.73
last
36.44
–
1.39
Direct Querying
910.47
–
22.89
code-like
LinearRegression
mean
6.97
–
0.76
max
24.17
–
1.30
last
10.39
–
1.20
MLP
mean
12.77
–
0.86
max
25.51
–
1.57
last
12.95
–
1.03
LightGBM
mean
11.52
–
0.61
max
18.30
–
0.77
last
16.85
–
1.08
Direct Querying
12.56
–
81.31
natural language
LinearRegression
mean
5.42
–
0.73
max
21.70
–
1.21
last
12.64
–
1.30
MLP
mean
13.19
–
0.93
max
27.63
–
1.55
last
14.82
–
1.10
LightGBM
mean
12.11
–
0.66
max
17.12
–
0.75
last
20.44
–
1.07
Direct Querying
0.00
–
179.94
27


--- Page 28 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
Table 7: Direct Querying and Probing MAE per Representation. Data represent the mean MAE over different pooling
strategies and feature complexities for probing methods for JSP.
Complexity
Representation
Regressor
Pooling
standard
LinearRegression
mean
2.20
68.99
1.41
max
4.64
235.21
13.17
last
3.96
126.26
5.39
MLP
mean
1.32
27.92
1.16
max
1.81
41.59
2.22
last
1.42
31.93
4.53
LightGBM
mean
2.13
37.43
1.31
max
1.04
40.09
1.96
last
3.14
66.71
4.10
Direct Querying
20.53
643.63
18.08
code-like
LinearRegression
mean
1.37
35.68
4.25
max
4.49
162.00
21.00
last
3.41
22.19
4.97
MLP
mean
1.13
19.12
2.22
max
1.87
58.12
4.11
last
2.21
17.64
3.33
LightGBM
mean
1.70
26.34
1.42
max
0.56
21.07
2.33
last
5.40
22.08
7.64
Direct Querying
0.00
446.61
9.59
natural language
LinearRegression
mean
1.07
36.24
3.12
max
3.54
136.68
18.35
last
2.12
50.16
5.28
MLP
mean
0.72
24.39
1.82
max
1.23
35.93
4.42
last
0.89
22.28
2.80
LightGBM
mean
1.44
27.42
1.68
max
0.36
24.08
2.30
last
2.18
36.20
5.52
Direct Querying
0.42
541.31
5.97
28


--- Page 29 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
Table 8: Direct Querying and Probing MAE per Representation. Data represent the mean MAE over different pooling
strategies and feature complexities for probing methods for KP.
Complexity
Representation
Regressor
Pooling
standard
LinearRegression
mean
131,555.59
86.97
6.55
max
201,199.19
159.87
24.13
last
372,968.87
234.50
34.14
MLP
mean
142,425.52
51.74
7.44
max
148,486.27
69.17
14.43
last
141,177.30
79.54
16.26
LightGBM
mean
89,572.79
56.54
8.98
max
74,578.54
62.44
11.82
last
122,960.37
76.68
15.49
Direct Querying
261,452.48
24,747.88
341.84
code-like
LinearRegression
mean
158,325.85
108.73
10.47
max
150,884.66
171.30
33.16
last
144,934.26
125.84
17.98
MLP
mean
143,555.46
66.49
10.16
max
152,150.76
74.78
15.80
last
147,047.79
72.36
13.65
LightGBM
mean
101,037.26
62.95
9.67
max
48,281.40
61.16
13.57
last
116,287.68
75.78
15.56
Direct Querying
9,331.44
583,906.78
381.53
natural language
LinearRegression
mean
101,070.45
74.95
6.25
max
170,965.33
171.01
35.50
last
123,168.13
145.55
25.38
MLP
mean
148,950.19
69.63
9.25
max
153,472.86
81.83
20.94
last
140,138.80
76.12
17.53
LightGBM
mean
90,946.86
64.42
9.79
max
62,731.61
65.14
14.98
last
81,936.02
79.98
20.10
Direct Querying
3,968.89
6,984.98
259.40
29


--- Page 30 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
B
Complete Set of Statistical Analysis Tables
This appendix reports the full results of the mixed linear models used to assess the effects of classifiers, feature
representations, and pooling strategies on accuracy. All models include random intercepts for group-level variability,
with fixed-effect estimates reported alongside standard errors, z-statistics, p-values, and 95% confidence intervals.
• Table 9 summarizes classifier effects.
• Table 10 compares ISA-based and handcrafted features.
• Table 11 presents the full factorial model including representation and pooling factors.
• Table 12 reports the comparison between ISA and LLM-based standard representations using the LightGBM
classifier.
30


--- Page 31 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
Table 9: Mixed linear model for classifier effects (accuracy ∼C(classifier, Treatment(reference=’MostFrequentClassifier’))). The analysis is
based on n = 880 observations and g = 20 groups.
Parameter
Coef.
Std.Err.
z
p-value
[0.025
0.975]
Intercept (MostFrequentClassifier)
0.699
0.025
28.033
< 0.001
0.650
0.748
Classifier: LightGBM (vs MostFrequentClassifier)
0.192
0.006
34.402
< 0.001
0.181
0.203
Classifier: LogisticRegression (vs MostFrequentClassifier)
0.149
0.006
26.705
< 0.001
0.138
0.160
Classifier: MLPClassifier (vs MostFrequentClassifier)
−0.011
0.006
−2.011
0.044
−0.022
−0.000
Group variance
0.012
(0.067)
Table 10: Results of the mixed linear model and post-hoc statistical tests comparing ISA-based and handcrafted features, specified as accuracy ∼C(classifier,
ref=’LogisticRegression’) + C(representation, ref=’hc’). Reported effects include the conditional effects of representation and pooling, and their
interaction. The analysis is based on n = 80 observations and g = 20 groups.
Parameter
Coef.
Std.Err.
z
p-value
[0.025
0.975]
Intercept (LogisticRegression, Handcrafted)
0.786
0.025
30.921
< 0.001
0.737
0.836
Classifier: LightGBM (vs LogisticRegression)
0.091
0.004
22.221
< 0.001
0.083
0.099
Representation: ISA (vs Handcrafted)
0.031
0.004
7.543
< 0.001
0.023
0.039
Group variance
0.013
(0.255)
Comparison
Test
Statistic (df)
p-value / CI
Interpretation
ISA vs Handcrafted
LRT
χ2(1) = 40.017
p = 2.52 × 10−10
Significant
ISA vs Handcrafted
Tukey HSD
Mdiff = 0.0309
padj = 0.2704, CI95% = [−0.0245, 0.0863]
Not significant
ISA vs Handcrafted (ε = 0.01)
Wilcoxon TOST
Mdiff = 0.0295
plow
=
1.82 × 10−12, phigh
≈
1, CI95%
=
[−0.0031, 0.0662]
Not equivalent
31


--- Page 32 ---
DA ROS ET AL.
BEHAVIOR AND REPRESENTATION IN LLMS
Table 11: Results of the full factorial mixed linear model and corresponding likelihood–ratio (LRT) comparisons, specified as accuracy ∼C(classifier,
ref=’LogisticRegression’) + C(representation, ref=’standard’) * C(pooling, ref=’mean’). Reported effects include the conditional effects
of representation and pooling, and their interaction. The analysis is based on n = 360 observations and g = 20 groups.
Parameter
Coef.
Std.Err.
z
p-value
[0.025
0.975]
Intercept (LogisticRegression, standard, Mean)
0.858
0.027
32.114
< 0.001
0.806
0.911
Classifier: LightGBM (vs LogisticRegression)
0.032
0.002
17.763
< 0.001
0.029
0.036
Representation: code-like (vs standard)
-0.006
0.004
-1.532
0.126
-0.013
0.002
Representation: natural language (vs standard)
0.001
0.004
0.242
0.808
-0.007
0.008
Pooling: Last (vs Mean)
-0.007
0.004
-1.854
0.064
-0.015
0.000
Pooling: Max (vs Mean)
0.011
0.004
2.795
0.005
0.003
0.018
Representation × Pooling: code-like × Last
0.005
0.005
1.002
0.316
-0.005
0.016
Representation × Pooling: natural language × Last
0.008
0.005
1.433
0.152
-0.003
0.018
Representation × Pooling: code-like × Max
-0.007
0.005
-1.198
0.231
-0.017
0.004
Representation × Pooling: natural language × Max
-0.004
0.005
-0.815
0.415
-0.015
0.006
Group variance
0.014
(0.268)
Effect / Comparison
Test
Statistic (df)
p-value
Significance
Representation (conditional)
LRT
χ2(6) = 21.315
0.00161
Significant
Pooling (conditional)
LRT
χ2(6) = 26.609
0.000171
Significant
Representation × Pooling
LRT
χ2(4) = 6.772
0.1485
Not significant
Table 12: Results of the mixed linear model for the comparison between ISA and LLM-standard using LightGBM, specified as accuracy ∼C(representation,
ref=’standard’). The analysis is based on n = 80 observations and g = 20 groups.
Parameter
Coef.
Std.Err.
z
p-value
[0.025
0.975]
Intercept (LLM)
0.891
0.054
16.440
< 0.001
0.785
0.997
Representation: ISA (vs LLM)
0.008
0.002
3.313
0.001
0.003
0.013
Group variance
0.012
(0.907)
Comparison
Test
Statistic
p-value / CI
Interpretation
ISA vs LLM (ε = 0.01)
Wilcoxon TOST
Mdiff = 0.0054
plow = 9.54 × 10−17, phigh = 0.021, CI95% =
[−0.0039, 0.0244]
Equivalent
32
