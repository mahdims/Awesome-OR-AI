--- Page 1 ---
CoEdge-RAG: Optimizing Hierarchical Scheduling
for Retrieval-Augmented LLMs in Collaborative
Edge Computing
Guihang Hong∗, Tao Ouyang∗, Kongyange Zhao, Zhi Zhou, Xu Chen†
School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China
{honggh3, zhaokyg}@mail2.sysu.edu.cn, {ouyt33, zhouzhi9, chenxu35}@mail.sysu.edu.cn
Abstract—Motivated by the imperative for real-time respon-
siveness and data privacy preservation, large language models
(LLMs) are increasingly deployed on resource-constrained edge
devices to enable localized inference. To improve output quality,
retrieval-augmented generation (RAG) is an efficient technique
that seamlessly integrates local data into LLMs. However, ex-
isting edge computing paradigms primarily focus on single-
node optimization, neglecting opportunities to holistically exploit
distributed data and heterogeneous resources through cross-node
collaboration. To bridge this gap, we propose CoEdge-RAG, a hi-
erarchical scheduling framework for retrieval-augmented LLMs
in collaborative edge computing. In general, privacy constraints
preclude accurate a priori acquisition of heterogeneous data dis-
tributions across edge nodes, directly impeding RAG performance
optimization. Thus, we first design an online query identification
mechanism using proximal policy optimization (PPO), which
autonomously infers query semantics and establishes cross-
domain knowledge associations in an online manner. Second, we
devise a dynamic inter-node scheduling strategy that balances
workloads across heterogeneous edge nodes by synergizing his-
torical performance analytics with real-time resource thresholds.
Third, we develop an intra-node scheduler based on online convex
optimization, adaptively allocating query processing ratios and
memory resources to optimize the latency-quality trade-off under
fluctuating assigned loads. Comprehensive evaluations across
diverse QA benchmarks demonstrate that our proposed method
significantly boosts the performance of collaborative retrieval-
augmented LLMs, achieving performance gains of 4.23% to
91.39% over baseline methods across all tasks.
Index Terms—LLM, RAG, Edge Computing.
I. INTRODUCTION
Large Language Models (LLMs) such as LLaMA and
DeepSeek [1]–[4], driven by breakthroughs in hardware capa-
bilities and algorithmic innovations, have achieved unprece-
dented performance in artificial general intelligence. Predom-
inantly deployed on cloud platforms due to their substantial
parameter sizes, LLMs necessitate substantial computational
resources for inference. However, as LLM applications ex-
pand, the increasing demands for real-time responsiveness and
strict data privacy requirements have spurred efforts to migrate
LLMs from centralized cloud infrastructures to distributed
edge environments [5], [6].
* These authors contributed equally.
† Corresponding author.
While cost-optimized edge LLMs with reduced model sizes
improve efficiency, they inherently lack contextual awareness
and specialized knowledge for domain-specific applications
[7]–[9]. To alleviate these issues, Retrieval-Augmented Gener-
ation (RAG) has emerged [10]–[12], which dynamically incor-
porates external, domain-specific knowledge during inference
without modifying model parameters. Specifically, RAG first
retrieves the top-k relevant documents from a local vector
database and integrates them into the model input, significantly
improving contextual understanding and output relevance for
subsequent inference. In contrast to computationally intensive
fine-tuning approaches, RAG offers a lightweight, resource-
efficient paradigm, which efficiently leverages edge data while
preserving its privacy through local processing.
Current research on edge-based RAG systems primarily
focuses on single-node optimization, such as on-device infer-
ence [13], [14] or cloud-assisted frameworks [15]–[17], which
assume centralized data storage. However, real-world edge
environments are inherently distributed and heterogeneous,
rendering single-node RAG systems inadequate due to limited
data coverage. As user expectations gradually shift toward
real-time, personalized AI interactions, the transition from
single-node to multi-node architectures becomes imperative.
For instance, in smart healthcare [18], [19], multiple hospital
nodes deploy specialized LLMs trained on localized private
medical knowledge (e.g., for internal medicine or pediatrics)
to support AI-assisted diagnostics. This distributed deployment
enables dynamic query allocation to the most suitable node
based on domain expertise. Furthermore, overlapping domains
(e.g., cold symptom diagnosis) can facilitate adaptive workload
distribution: during peak demand, such as flu season, queries
can be intelligently routed to sub-optimal but capable nodes,
alleviating bottlenecks while maintaining load balancing and
system responsiveness. Thus, efficient query scheduling is
essential, necessitating multidimensional awareness, including
domain specialization, resource availability, and current load,
to holistically optimize both response quality and latency.
The multi-node RAG paradigm, while promising, intro-
duces key challenges in practical deployment: First, due to
privacy concerns, edge nodes typically handle multi-domain
data with unknown distributions, posing significant challenges
in discerning query intents and dynamically routing requests
arXiv:2511.05915v1  [cs.DC]  8 Nov 2025


--- Page 2 ---
to domain-specific nodes without prior knowledge of data
patterns. Second, query arrivals exhibit spatiotemporal het-
erogeneity in volume and domain distribution, while edge
nodes vary in resource capacities and model capabilities. Static
heuristics for query allocation risk node overload, degrading
real-time performance. Consequently, balancing load distri-
bution across nodes while preserving LLM output quality
remains an open challenge. Third, larger LLMs improve
generation quality but increase inference latency. The diversity
of model types across resource-constrained nodes creates a
combinatorial optimization problem for batch query schedul-
ing, requiring careful trade-offs between quality and latency.
In this paper, we present CoEdge-RAG, a novel hierarchical
scheduling framework designed for multi-node collaboration
that jointly optimizes query scheduling, model deployment,
and resource allocation to maximize generation quality under
strict latency constraints. To enable efficient query-to-corpus
matching without prior knowledge of data distributions, we
propose an online intent-aware query identification mecha-
nism, which leverages proximal policy optimization (PPO),
an advanced deep reinforcement learning technique, to infer
query-domain attributions and learn latent matching relation-
ships between queries and edge nodes’ domain knowledge.
Based on the derived probability distributions, we develop an
adaptive inter-node scheduling policy that dynamically routes
queries to heterogeneous edge nodes. This allocation adapts
to real-time demands by incorporating the estimated node
capacity function, ensuring balanced workload distribution
and generation quality across the distributed edge network.
To further optimize edge node efficiency, we transform the
combination optimization of model deployment and resource
allocation into an online convex optimization. We then develop
an intra-node scheduling strategy to dynamically configure
LLM model sizes and memory resources in real-time, comple-
menting our inter-node allocation approach. This fine-grained
optimization enables adaptive trade-offs between inference
latency and generation quality while preserving computational
efficiency. In summary, our hierarchical framework integrates
these components to facilitate efficient, data-aware query pro-
cessing in decentralized edge environments, simultaneously
optimizing both resource utilization and RAG performance.
The key contributions of this work include:
• We focus on the emerging collaborative edge computing
paradigm for retrieval-augmented LLMs, boosting LLM
deployment towards ubiquitous edge intelligence.
• We propose a lightweight online query identification
mechanism that establishes latent correlation match-
ing between user queries and local corpora, enhancing
retrieval-augmented LLMs at the edge to produce more
reliable responses.
• Considering heterogeneous edge capacity, we propose a
hierarchical scheduling framework to improve response
quality within latency limitation. Specifically, centralized
inter-node scheduling balances potential generation qual-
ity and allocated workload, while distributed intra-node
scheduling leverages diverse LLMs to exploit limited
edge resources for more efficient LLM serving.
• Through a comprehensive evaluation on QA benchmarks
with diverse domains, our system demonstrated high
effectiveness and efficiency, achieving 4.23% to 91.39%
performance improvements over baseline methods.
II. MOTIVATION
We conduct several case studies to identify key challenges
in developing RAG-enhanced LLMs for collaborative edge
computing. Our experimental testbed comprises three edge
nodes, each hosting a LLaMA-3B model [4] with a local
corpus consisting of 60% domain-specific data (sports, law,
or finance) and 40% distributed across the other two domains
(20% each). This configuration emulates real-world cross-
domain knowledge distribution while maintaining corpus di-
versity across nodes. Besides, we evaluate output quality using
two common metrics: (i) Rouge-L [20] for lexical overlap
between generated and reference texts, and (ii) BERTScore
[21] for semantic similarity using contextual embeddings.
A. Mismatch between Query Allocation and Edge Corpus
RAG enhances LLM output quality when edge nodes main-
tain corpora highly matched to user queries. However, the
improvement could diminish significantly under ambiguous
corpus distributions across nodes. To verify the correlation
between node-query alignment and system efficacy, we simu-
late 1,500 concurrent queries under three allocation strategies:
Random (queries routed without domain awareness), Domain
(queries assigned to nodes based on their primary domain
focus, e.g., financial queries to the financial node), and Oracle
(queries optimally routed to ideal nodes).
As depicted in Fig. 1, random allocation under opaque
corpus conditions degrades performance markedly, exhibit-
ing a 31.9% reduction in Rouge-L and 15.38% decline in
BERTScore compared to oracle allocation. This is because
query patterns fail to match static corpus-node, rendering
retrieved documents frequently irrelevant. Domain allocation
also exhibits suboptimal performance as it fails to leverage la-
tent cross-domain knowledge from non-primary nodes, thereby
constraining the utilization of distributed corpus expertise.
Rouge-L
BertScore
0.0
0.2
0.4
0.6
0.8
Score
Random
Domain
Oracle
Fig.
1:
Generation
quality
comparison.
Balanced
Mod. Skewed High Skewed
0
10
20
30
40
Latency(ms)
Random
Domain
Oracle
Fig. 2: Latency comparison
with different skewnesses.
B. Temporal Query Skewness and Latency Degradation
In practice, temporal query skewness — where user re-
quests concentrate on specific domains during peak pe-
riods — induces resource contention and increases time-
out risks, particularly under stringent latency constraints.


--- Page 3 ---
40
50
60
70
Latency Constraint(s)
0.35
0.40
0.45
0.50
0.55
Rouge-L
Llama 1B
Hybrid
Llama 3B
(a) Generation quality with dif-
ferent model deployments under
varying latency constraints.
10
20
30
40
50
60
70
80
90
Query Allocation Ratio(%)
30
40
50
60
70
80
90
Latency(ms)
83%
80%
70%
60%
50%
45%
(b) Latency with different re-
source utilization under varying
query ratios.
Fig. 3: Generation quality and latency performances.
We measure query latency as end-to-end execution time
(from request submission to result delivery). To evaluate
the impact, we simulate three skewness patterns across
sports/law/finance domains: Balanced (500/500/500 queries),
Moderately Skewed (750/375/375, sports-biased), and Highly
Skewed (1000/250/250, sports-dominated). As depicted in
Fig. 2, domain-specific allocation suffers severe latency degra-
dation under skewness: moderate skewness and high skewness
increase latency by 47.21% and 93.68%, respectively, versus
balanced scenarios. This stems from fixed routing strategies
overloading domain-designated nodes while leaving idle nodes
underutilized during peak demand. In contrast, oracle allo-
cation dynamically redistributes queries based on cross-node
data relevance, reducing latency by 25.33%–33.55% compared
to domain allocation. Our findings reveal the necessity of
dynamic load-balancing mechanisms that leverage latent inter-
domain correlations across nodes, rather than static domain-
specific routing, to mitigate timeout risks and optimize re-
source utilization.
C. Quality-Latency Tradeoffs in Model Deployment and Re-
source Allocation
Resource-constrained edge nodes can strategically deploy
multiple LLM variants to balance computational efficiency
and output quality. To systematically evaluate this tradeoff, we
conducted a comparative analysis of three configurations: (i)
exclusive deployment of LLaMA-1B, (ii) hybrid deployment
where LLaMA-1B and LLaMA-3B each processed 50% of
queries, and (iii) exclusive deployment of LLaMA-3B. Fig. 3a
reveals constraint-driven performance patterns for 1000 re-
quests: 1) Under strict time constraints (< 50 s): LLaMA-
1B achieved a 0.506 Rouge-L score with zero timeout, while
hybrid deployment and 3B-exclusive may suffer up to 17.9%
and 45.7% quality degradation respectively due to timeout-
induced interruptions. 2) Under relatively relaxed time
budget (> 50 s): Rouge-L score of hybrid deployment jumps
to 0.547 (8% improvement over 1B-exclusive), while LLaMA-
3B requires more than 70 s to fully unleash its potential (0.584
Rouge-L). Thus, a dynamic model deployment is needed based
on varying loads and time constraints, rather than rigidly using
fixed model deployment, to optimize local service quality.
To demonstrate the necessity of dynamic resource man-
agement, we evaluate end-to-end latency under a fixed GPU
memory budget while varying resource allocations and query
distributions between LLaMA-1B and LLaMA-3B. Fig. 3b
illustrates the fraction of GPU memory allocated to LLaMA-
3B (45%–83%) and the corresponding query allocation ratio
(percentage of queries directed to LLaMA-3B). Through ad-
justing query and resource allocations, we identify two critical
scenarios: 1) Resource Starvation in Larger Models (45%–
50% GPU memory for LLaMA-3B): When LLaMA-3B
operates with constrained memory, it becomes a performance
bottleneck under increasing query load. Allocating 90% of
queries to LLaMA-3B — compared to 80% — intensifies
resource contention, increasing latency by up to 34.1%. This
degradation compromises system responsiveness and risks
violating real-time service constraints. 2) Underutilization
of Fast-Response Models (80%–83% GPU memory for
LLaMA-3B): Conversely, excessive memory allocation to
LLaMA-3B starves LLaMA-1B, even in latency-sensitive
workloads where the smaller model should dominate query
processing. With insufficient memory, LLaMA-1B suffers high
tail latency due to contention, paradoxically increasing system
latency by 28.1%–62.5% when more queries are routed to
it. These results demonstrate a key trade-off: static resource
allocation fails to accommodate dynamic query routing pat-
terns, whereas efficient edge inference requires the co-design
of query scheduling and memory allocation with runtime
rebalancing to satisfy real-time service requirements.
III. DESIGN OF COLLABORATIVE LLMS SERVING
CoEdge-RAG aims at maximizing generation quality while
maintaining satisfactory latency requirements. Key insights are
as follows: (i) in terms of generation quality, queries associated
with specific attributes (e.g., subject) are more likely to match
local corpora data at specific edge nodes, thus enhancing the
RAG performance of the edge-deployed LLMs; (ii) in terms of
response latency, efficient workload balancing across multiple
edge nodes mitigates straggler effects, where a single node
is overwhelmed by excessive queries, thus reducing overall
serving time.
The CoEdge-RAG workflow, illustrated in Fig. 4, operates
as follows: A global coordinator
1
first tokenizes and en-
codes incoming queries into embedding vectors, and computes
corresponding probability distributions via an online query
identifier. Such probability vectors indicate matching degrees
between queries and edge nodes, assisting the coordinator to
route queries to designated nodes. Upon receipt, nodes
2
retrieve the top-k semantically relevant local documents via
vector similarity search, augmenting queries with retrieved
contexts to form enriched prompts for subsequent LLM pro-
cessing. Based on query volumes and latency requirements,
each node
3
dynamically selects optimal LLMs from its
model pool for per-GPU deployment and allocates suitable
resource and query proportions to each model. LLM outputs
are assessed via human or automated evaluators, generating
quality metrics (e.g., Rouge, BertScore). These metrics
4
are used to update the online query identifier mechanism,
establishing an iterative optimization loop that strengthens
data-characteristic alignment between queries and edge nodes.


--- Page 4 ---
Diverse LLM serving
Latency
Online update
25%
50%
Resource allocation
Dynamic 
queries
Accuracy
Edge servers
RAG
Query 
identification
75%
50%
Model
selection
Model deployment
Local data
1
2
3
4
Fig. 4: An illustration of CoEdge-RAG.
Note that our work focuses on single-document queries,
wherein the required information can be derived from a single
source document. It aligns with standard RAG baselines [15],
[22], enabling a clear and focused evaluation of the perfor-
mance gains directly attributable to our novel hierarchical
scheduling framework. To address more complex queries, our
system can be easily extended with a preprocessing module
that decomposes them into sub-queries [23]. The scheduler
would then allocate these sub-queries across the diverse edge
nodes, thereby preserving its core optimization benefits while
broadening applicability.
A. Query Identification and Scheduling Optimization
Query Identification: To efficiently characterize dynamic
edge environments, a slot structure is adopted to describe the
long-term CoEdge-RAG procedure. At each time slot t ∈T ,
a bunch of queries with numbers Bt must be served by a
collection of edge nodes N. To leverage the characteristics
of each query, an online identifier is adopted to establish
initial latent correlation matching between user queries and
each node corpus. Specifically, each query i is first processed
by the carefully designed encoder to a semantically rich text
embedding. Then the online query identifier will take it as
input, and generate a probability vector st
i = (st
i1, st
i2, ..., st
iN)
to quantify the matching degree between query i and each
edge node, where entries in the vector hold
P
n∈N
st
in = 1.
Naturally, the set of all probability vectors for queries arriving
at slot t is represented by St = {st
1, st
2, . . . , st
Bt}. These
probability vectors facilitate informed decision optimization
in the following allocation scheduling process and determine
the final allocation node for each query.
Scheduling Optimization: Subsequently, each edge node
can utilize its constrained resources by deploying suitable
LLMs to handle these queries. As outlined in Section II,
edge nodes typically host diverse model series (e.g., LLaMA
and Qwen), each maintaining a dedicated model pool Mn
containing parameter-scaled variants for GPU deployment.
Even though an edge node is equipped with multiple GPUs,
to maximize LLM efficiency, we implement a single-GPU
allocation strategy guided by two principles: (i) aligning with
edge resource limitation, we prioritize smaller-scale LLMs
(e.g., 1B–8B parameters), which can be efficiently supported
at a single GPU. (ii) Deploying an LLM across multiple GPUs
incurs inter-GPU communication and memory synchronization
costs, increasing system complexity and degrading inference
latency compared to single-GPU execution. In this way, for
model m ∈Mn on GPU k of node n, inference latency
is governed by the query load pt
mnk (proportion of allocated
queries) and resource allocation Rt
mnk (proportion of GPU
resources), subject to the constraint P
n
P
k
P
m
pt
mnk = 1. These
scheduling decisions are dynamically adjusted in real-time to
system dynamics, enabling adaptive optimization of model
placements and resource partitioning across the edge hierarchy.
B. Performance Metrics
Latency: In CoEdge-RAG, the time of the RAG inference
service mainly includes the vector search time, the model
processing time, and the time overhead induced by GPU
resource reallocation. The vector search time of retrieving doc-
uments, denoted as TSt
n, can be measured in real-time before
performing LLM inference at each node n. The processing
latency is determined by the query proportion pt
mnk and al-
located resources Rt
mnk, expressed as Lt
mnk(pt
mnkBt, Rt
mnk).
Regarding GPU resource reallocation, time overhead may arise
due to dynamic adjustments in model deployment strategies
under varying query loads. Specifically, adapting to workload
fluctuations may involve (i) loading previously undeployed
models, (ii) unloading existing models, or (iii) adjusting re-
source allocations for currently deployed models.
Based on empirical measurements, the unloading operation
typically incurs negligible overhead (a few hundred millisec-
onds) compared to model loading or resource reallocation.
To formally characterize these behaviors, we first define the
deployment status of model m in the model pool Mn on GPU
k at time slot t as dt
mnk ∈{0, 1}, where dt
mnk = 1 indicates
active deployment and dt
mnk = 0 otherwise. The unloading
state of model m is then derived as:
ULDt
mnk = (1 −dt
mnk) · dt−1
mnk,
(1)
where ULDt
mnk = 1 signifies an ongoing unloading operation
at time slot t, while ULDt
mnk = 0 indicates no unloading
occurs. This formulation explicitly isolates unloading events,
which is critical because although unloading induces changes
in resource allocation(Rt
mnk), its time overhead is nearly
negligible. Thus, the primary sources of reconfiguration la-
tency are: (i) loading previously undeployed models (dt
mnk =
1 ∧dt−1
mnk = 0), and (ii) adjusting resources for persistent
models (Rt
mnk ̸= Rt−1
mnk ∧ULDt
mnk = 0).
Moreover, to prevent resource contention during model
initialization, we enforce serialized loading operations on each
GPU. Consequently, the total loading time for GPU k during
time slot t becomes the sum of individual loading times for
all models requiring deployment. Let lm represent the loading
time of the model m. The aggregate loading time on GPU k
at time slot t is then given by:
TLt
k =
X
m
1Rt
mnk̸=Rt−1
mnk · (1 −ULDt
mnk) · lm,
(2)


--- Page 5 ---
where the indicator function 1Rt
mnk̸=Rt−1
mnk evaluates to 1 when
the memory allocation for model m has changed between time
slots, and 0 otherwise.
Generation Quality: Let Qt
mnk(pt
mnkBt, Rt
mnk, St) rep-
resent the average generation quality of model m in node
n within time slot t, whose value depends primarily on the
number of queries pt
mnkBt it receives, resource proportion
Rt
mnk and query identification vectors St. Additionally, if
model m exceeds its relevant latency requirements while
processing pt
mnkBt queries, the portion of queries that exceed
the requirements will be considered invalid.
C. Problem Formulation
We formulate the CoEdge-RAG optimization problem as
maximizing overall generation quality under strict latency
constraints across resource-constrained edge nodes:
max
X
t
X
m,n,k
pt
mnkQt
mnk(pt
mnkBt, Rt
mnk, St)
(3)
s.t. max
m,n,k

Lt
mnk(pt
mnkBt, Rt
mnk) + TLt
k

≤Lt −TSt
n, ∀t,
(4)
X
m
Rt
mnk ≤Rk, ∀n, k, t,
(5)
Rt
mnk ≥dt
mnkrm, ∀m, n, k, t,
(6)
Rt
mnk ≤dt
mnk, ∀m, n, k, t,
(7)
X
n
X
k
X
m
pt
mnk = 1, ∀t,
(8)
where Lt represents the latency requirement specified in the
service-level objective (SLO), which may vary temporally due
to dynamic factors such as fluctuating user demands, variable
network conditions, or shifts in service priorities (e.g., stricter
latency during peak hours). TSt
n (at time slot t) denotes the
vector search time for queries, Rk denotes the GPU constraints
of GPU k in node n and rm denotes minimum model-
startup memory utilization of model m. Eq. (4) ensures that
every model allocated queries and resources meets the latency
requirement Lt. Eq. (5) enforces GPU memory constraints in
each card. Eq. (6) ensures the minimum memory is allocated
to each deployed model. Eq. (7) forces resources to zero
for undeployed models. Eq. (8) ensures that queries can be
dispatched to models for processing. Unfortunately, solving
the above problem is difficult due to the following challenges:
(C1) Accurate estimation of probability vectors St is hindered
by the heterogeneity of edge corpora, unknown data distribu-
tions, and the high dimensionality of query features; (C2) The
generation quality Qt
mnk(·) and processing latency Lt
mnk(·)
lack closed-form expressions, complicating the optimization
of scheduling policies; (C3) CoEdge-RAG must dynamically
adjust sequential scheduling decisions to accommodate evolv-
ing user queries and heterogeneous edge environments without
prior knowledge of system dynamics.
IV. HIERARCHICAL SCHEDULING OPTIMIZATION
In this section, we propose a hierarchical scheduling frame-
work, CoEdge-RAG, explicitly designed to address the scal-
ability and heterogeneity challenges inherent to collaborative
edge systems.
A. Data-aware Online Query Identification
High-quality retrieval data is crucial for enhancing the
effectiveness of RAG-based LLMs, particularly for domain-
specific user queries. However, privacy-preserving issues typi-
cally obscure the detailed metadata about corpus composition
and distribution across edge nodes. To address challenge C1,
we develop an online query identification mechanism capable
of inferring corpus relevance across nodes without exposing
sensitive distribution metadata.
To model the latent correlation between queries and edge
nodes, each query i is first encoded into a semantically
rich embedding vector et
i using a pre-trained text encoder1.
Such an embedding well captures domain-specific attributes
and informs a routing policy network to generate probability
distribution st
i (defined in Section III-A), representing node-
specific relevance scores. Since response quality implicitly
reflects query-document alignment, we implement a feedback-
driven mechanism that optimizes the policy network through
iterative refinement. For each query i, the edge-generated
response GENi is evaluated against a powerful cloud-LLM
(e.g., DeepSeek-V3) reference REFi, which serves as a qual-
ity benchmark for contextually aligned outputs [14]. Response
quality is assessed using two complementary metrics: Lexical
Alignment: ROUGE-L [20] computes the normalized longest
common subsequence (LCS) between REFi and GENi:
f t
i,R =
LCS(REFi, GENi)
max(len(REFi), len(GENi)).
Semantic Consistency: BERTScore [21] measures the har-
monic mean of the maximum cosine similarities in both
directions (i.e., from generated to reference tokens and vice
versa). The precision and recall are defined as follows:
Prec(·) =
1
|GENi|
|GENi|
X
k=1
max
j
sim(E(GENi)k, E(REFi)j),
Rec(·) =
1
|REFi|
|REFi|
X
j=1
max
k
sim(E(REFi)j, E(GENi)k),
where E(REFi) and E(GENi) represent the token-level
embeddings of the reference and generated responses, respec-
tively. The final BERTScore is then computed as:
f t
i,B = 2 · Prec(GENi, REFi) · Rec(REFi, GENi)
Prec(GENi, REFi) + Rec(REFi, GENi) .
These complementary metrics (i.e., f t
i,R for lexical align-
ment and f t
i,B for semantic fidelity) are combined into a
composite quality score:
f t
i = α1 · f t
i,R + α2 · f t
i,B,
(9)
where α1 and α2 are their weight factors. The rationale
for using f t
i as a proxy for node relevance is grounded in
1https://huggingface.co/BAAI/bge-base-en-v1.5


--- Page 6 ---
the RAG paradigm’s fundamental principle: higher response
quality indicates stronger alignment between the query and
the retrieved corpus. Based on the definition, f t
i quantifies
output performance on the assigned node, reflecting its latent
relevance. This feedback implicitly estimates query-corpus
alignment while evaluating the efficacy of edge-deployed
models, thereby guiding subsequent policy network updates.
For policy network optimization, we employ Proximal
Policy Optimization (PPO) [24], an advanced reinforcement
learning algorithm, to enhance our online query identification
framework. In order to accommodate stochastic and non-
interfering query streams with high-dimensional semantic fea-
tures, we devise a streamlined architecture that eliminates
the value function network while maintaining a policy-only
structure. This design choice yields dual benefits: (i) substan-
tial reduction in computational complexity, and (ii) improved
processing efficiency. Empirical evaluations demonstrate an
average inference latency of 0.02 ms per query, satisfying
stringent real-time system constraints.
In the absence of a critic network, we formulate a standard-
ized feedback metric as the reward signal within each batch:
¯f t
i = f t
i −µ
σ + c ,
(10)
where µ and σ denote the batch-wise mean and standard devi-
ation of feedback values, respectively, and c = 10−8 is a small
constant to avoid division by zero. This operation ensures
stable gradient magnitudes across varying query workloads. In
this way, the policy parameters θ are then optimized through
a modified PPO objective:
Lf = Et

min(ρt
i ¯f t
i , clip(ρt
i, 1 −ϵ, 1 + ϵ) ¯f t
i )

+ βH(πθ),
(11)
where πθ and πθold denote the updated policy network and
the fixed old policy network, respectively. The importance
sampling ratio is defined as ρt
i = πθ(et
i)/πθold(et
i). Using
this ratio and the CLIP mechanism, PPO maintains training
stability by preventing drastic policy deviations. This approach
also facilitates efficient batch reuse for iterative policy refine-
ment, mitigating performance instability from abrupt updates.
These properties align with edge computing requirements
for sample efficiency, low computational overhead, and fast
adaptation to dynamic data, ensuring robust performance in
resource-constrained settings. Additionally, the entropy term
βH(πθ) encourages exploration, preventing premature con-
vergence. In summary, the modified objective Lf achieves an
efficient trade-off between two critical objectives: (i) feedback-
guided refinement through intermediate node LLM outputs
to establish query-document associations, and (ii) stochastic
exploration via policy entropy to ensure diverse node selection.
To minimize training frequency while preserving system sta-
bility, we employ a memory buffer that accumulates historical
feedback and triggers batched policy updates only when the
accumulated queries exceed a predetermined threshold. This
threshold is statically configured based on the average query
load observed over an extended time horizon, ensuring updates
are neither too frequent to incur unnecessary computational
overhead nor too sparse to compromise adaptability. By de-
coupling updates from transient query fluctuations and instead
aligning them with statistically representative workloads, this
approach effectively achieves robust training stability while
reducing the number of network updates. The update process is
designed for efficiency, typically completing within 30 ms per
1000 queries, thereby achieving a favorable balance between
computational resource conservation and responsiveness in
dynamic edge environments.
B. Loading-balancing Inter-node Scheduling
While online query identification establishes initial query-
node mappings, it neglects computational heterogeneity and
inherent node capacity constraints. Consequently, it may in-
troduce critical bottlenecks when bunches of query workloads
are concentrated on a specific domain, inducing huge pro-
cessing delays and service degradation on an overwhelmed
edge node. Prior to addressing C2 and C3, we propose an
inter-node scheduling mechanism that dynamically optimizes
the query allocation proportion pt
n for each node n (where
pt
n := P
k
P
m
pt
mnk), thus balancing the trade-off between gen-
eration quality and workload distribution via two objectives:
(i) prioritizing allocation to nodes with probability scores St
and (ii) balancing allocation based on nodes’ capacity. Once
the query proportion pt
n of each node is determined, the joint
scheduling problem in collaborative edge computing decom-
poses into independent intra-scheduling subproblems (Section
IV-C), thereby largely reducing computational complexity.
The core principle of inter-node scheduling is to determine
the maximum query capacity of each node while adher-
ing to predefined latency constraints. This ensures efficient
query distribution, preventing node overload. Additionally, it
promotes balanced data exploration across nodes, enhancing
performance under high-load scenarios.
During the initialization phase, the system autonomously
profiles each node’s processing capacity through controlled
query bursts across progressively relaxed latency requirements
L. The latency parameter L is systematically varied from 5 s
to 60 s in 5 s increments, revealing the node-specific sensitivity
to time constraints. Beginning with L = 5 s, the system
progressively increases the query load on the target node until
the query drop rate surpasses a predefined threshold (e.g., 1%).
This establishes the maximum sustainable throughput En,5 for
node n under the 5 s latency constraint. For subsequent latency
levels, the system initializes the query distribution volume to
node n as (L/5) · En,5. If the drop rate remains below the
threshold, the load is incrementally increased by En,5 until the
threshold is exceeded, at which point the current throughput
is recorded as En,L. Building upon this empirical character-
ization, the system applies linear regression for simplicity to
derive a node-specific capacity function Cn(Lt), which models
the maximum sustainable query throughput for node n while
satisfying an arbitrary latency requirement Lt:
Cn(Lt) = knLt + bn,
(12)


--- Page 7 ---
Algorithm 1 Inter-Node Scheduling Algorithm
1: Input: Total number of queries Bt, probability vectors
St = {st
1, st
2,..., st
Bt}, maximum number of queries each
node can process Cn.
2: Output: Allocation node at
i for each query i and query
proportion pt
j for each node j
3: Initialize at
i = −1 for each query i
4: Initialize qj = 0 for each node j
5: if Bt > P
n
Cn then
6:
¯C ←P
n
Cn
7:
Cn ←Cn + Cn
¯
C (Bt −¯C)
8: end if
9: for each query i do
10:
at
i ←RANDOMSAMPLE(st
i)
11:
if qat
i ≥Cat
i then
▷Check if node exceeds capacity
12:
Ai ←{j | qj < Cj, ∀j ∈N}
▷Available nodes
with residual capacity
13:
˜st
i ←Normalize(st
i[j] for j ∈Ai)
14:
at
i ←RANDOMSAMPLE(˜st
i)
▷Reassign with
normalized probabilities
15:
end if
16:
qat
i ←qat
i + 1
▷Update node’s query count
17: end for
18: pt
j = qj
Bt for each node j
where kn and bn are node-specific coefficients obtained
through the offline fitting process. This dynamic capacity
estimation guides runtime-adaptive query allocation.
Upon completing initialization, the system will proceed
to the subsequent runtime execution phase. As shown in
Algorithm 1, the inter-node scheduling algorithm first checks
whether the total query volume exceeds the cumulative capac-
ity of all nodes. If so, it proportionally scales up each node’s
capacity limit temporarily. The algorithm then proceeds by
initializing qn(the number of queries) assigned to each node
to zero, and the allocated node index at
i of each query i to -1
(indicating unassigned status). Each query i is subsequently
assigned to a node at
i sampled from the probability vector
st
i. Following this, the system performs a capacity-aware
validation: if the node’s current load qat
i > Cat
i(Lt), the
inter-node scheduling algorithm would reselect an alternative
node from the subset Ai of nodes with residual capacity using
the renormalized probabilities ˜st
i. This design ensures each
node processes a proper number of queries while maintaining
probability-driven load balancing, with the capacity adjustment
mechanism providing elasticity to handle instantaneous query
surges. Finally, the actual allocation proportion pt
j =
qj
Bt
is calculated for node j, which effectively decomposes the
global query scheduling and processing problem into a set of
distributed local subproblems at individual nodes.
C. Adaptive Intra-node Scheduling
To achieve fine-grained RAG performance optimization, we
refine intra-node scheduling policies, focusing on two key
aspects: dynamic model resource allocation and intra-node
query allocation. The resource allocation mechanism criti-
cally balances generation quality and operational efficiency,
as larger models improve quality but increase computational
latency. Under strict SLO latency constraints, dynamically
optimizing pt
mnk and Rt
mnk is essential for efficient LLM
deployment. To address challenges C2 and C3, the core idea is
to approximate generation quality and latency using convex
functions, then applying online convex optimization tech-
niques [25] to derive efficient scheduling decisions without
future system information.
As outlined in Section III-B, we employ a latency prediction
function Lt
mnk for each model m, which estimates processing
time given query load and resource allocation. However, as
noted in C2, the absence of closed-form expressions for Lt
mnk
precludes direct convex optimization. To bridge this gap, we
empirically evaluate four candidate convex functions (linear,
quadratic, exponential, cubic) by measuring latency across
varying resource allocations and query loads. Using rigorous
curve-fitting, we derive latency prediction models and evaluate
their accuracy via Root Mean Square Error (RMSE).
As demonstrated in Table I for the LLaMA model series
(DomainQA dataset; see Section 5.1), the quadratic function
achieves superior empirical fidelity, with normalized RMSE
(NRMSE) values of 2.58% (LLaMA-1B), 6% (LLaMA-3B),
and 1.87% (LLaMA-8B) — striking an optimal balance be-
tween computational tractability and prediction error. Conse-
quently, we adopt the quadratic approximation for real-time
latency prediction:
eLt
mnk = (apt
mnkBt −bRt
mnk)
2+cpt
mnkBt+dRt
mnk+e+∆T,
(13)
where a, b, c, d, e are fitted parameters derived from empirical
data, and ∆T represents a systematic latency offset incor-
porated to enhance robustness against unmodeled real-world
perturbations, such as network delays. Particularly, we adopt
fixed-length document chunks and a fixed number of retrieved
documents, which significantly simplifies the modeling pro-
cess. Since user queries are generally short, the input length
is dominated by the number of retrieved documents. This
allows inference latency to scale approximately linearly with
the retrieval count, enabling the latency predictor to generalize
across different retrieval quantities via simple proportional
scaling, eliminating the need for repeated modeling [13], [14].
TABLE I: Comparison of RMSE across different functions
and models.
Model
Linear
Quadratic
Exponential
Cubic
LLaMA-1B
1.449
1.141
1.130
1.118
LLaMA-3B
1.183
0.674
0.839
0.936
LLaMA-8B
2.289
1.033
2.136
2.402
Likewise, the quality of model outputs intrinsically depends
on the relevance of retrieved contexts, making an accurate a
priori assessment particularly challenging. Transient, request-
level quality estimations are not only computationally costly


--- Page 8 ---
but also prone to misrepresenting a model’s true capabilities.
This distortion arises because poor retrieval — an upstream
failure (e.g., query identification) — artificially suppresses
output quality, causing even highly capable models to appear
deficient. To isolate effective generative performance from
retrieval noise, we employ an offline evaluation framework.
Specifically, each node n conducts batch testing using lo-
cally deployed models m and node-specific data. Crucially,
to control for retrieval variability, models are provided with
queries paired with ground-truth context documents. This
setup functions as a controlled “open-book” examination,
ensuring performance differences reflect only the model’s
reasoning ability under ideal context conditions rather than
stochastic retrieval outcomes. From this evaluation, we derive
a static quality score Qmn, representing the average intrinsic
performance of model m on the data distribution of node
n under optimal context availability. This score inherently
captures both the model’s generative capability and node-
specific data characteristics. For scheduling purposes, Qmn
serves as a robust heuristic: in the absence of certainty about
retrieval success for an incoming query, the node assigns it
to the model most likely to utilize relevant context effec-
tively. Thus, we reduce the complex, dynamic quality function
Qt
mnk(pt
mnkBt, Rt
mnk, St) to a constant approximation Qmn.
Additionally, empirical results (Table III and Fig. 6) confirm
the efficacy and robustness of this approach, showing that Qmn
provides a reliable basis for model selection and scheduling.
Meanwhile, the indicator function in Eq. (2) is also non-
solvable in the convex optimization problem. Therefore, we
introduce a binary variable RCt
mnk ∈{0, 1} to track resource
change in time slot t. RCt
mnk satisfies the following constraint:
Rt−1
mnk −Rt
mnk ≤ε1 + M · RCt
mnk,
(14)
Rt
mnk −Rt−1
mnk ≤ε1 + M · RCt
mnk,
(15)
Rt−1
mnk −Rt
mnk ≥ε1 −M · (1 −RCt
mnk),
(16)
Rt
mnk −Rt−1
mnk ≥ε1 −M · (1 −RCt
mnk),
(17)
where ε1 is a small threshold ensuring only significant re-
source changes are considered, and M is a sufficiently large
constant. These constraints ensure that the binary tracking
variable activates only when the resource change between
consecutive time slots exceeds a small predefined threshold.
After this transformation, TLt
k can be approximated as:
TL
t
k =
X
m
RCt
mnk · (1 −ULDt
mnk) · lm.
(18)
However, in Eq. (18), variable multiplication remains im-
permissible in convex optimization. Therefore, we further sim-
plify the expression. As established in Section III-B, there exist
three distinct resource adjustment strategies, each resulting
in RCt
mnk = 1. Crucially, whenever RCt
mnk = 1 and no
unloading operation occurs (ULDt
mnk = 0), the model must
be reloaded to adapt its resource allocation. This condition
applies to the following three scenarios:
• Initial deployment: Rt−1
mnk = 0 to Rt
mnk > 0;
• Resource increase: Rt−1
mnk > 0 to Rt
mnk > Rt−1
mnk;
• Resource decrease: Rt−1
mnk > 0 to 0 < Rt
mnk < Rt−1
mnk.
In all such cases, the loading time overhead must be explic-
itly accounted for. To formally characterize this behavior, we
introduce two binary state variables: LDt
mnk and RLDt
mnk.
The loading state variable LDt
mnk indicates whether the model
was previously undeployed and is now being deployed, which
can be defined as:
LDt
mnk = dt
mnk · (1 −dt−1
mnk).
(19)
The reloading state variable RLDt
mnk indicates whether the
model remains deployed but undergoes a resource change,
subject to the following constraints:
RLDt
mnk ≤RCt
mnk,
(20)
RLDt
mnk ≤1 −LDt
mnk,
(21)
RLDt
mnk ≤1 −ULDt
mnk,
(22)
RLDt
mnk ≥RCt
mnk + LDt
mnk −ULDt
mnk.
(23)
The constraints ensure that the reloading state is only activated
when a deployed model undergoes resource changes without
being unloaded or newly deployed. Therefore, TL
t
k can be
approximated as:
f
TL
t
k =
X
m
(LDt
mnk + RLDt
mnk) · lm.
(24)
Finally, building on the derived convex latency function
eLt
mnk, static model generation quality Qmn, model loaing time
f
TL
t
k of GPU k, we reduce the original problem into the intra-
node scheduling optimization at each edge node n:
max
X
m,k
pt
mnkQmn
(25)
s.t. max
m,k
eLt
mnk(pt
mnkBt, Rt
mnk) + f
TL
t
k

≤Lt −TSt
n, ∀t,
(26)
X
m
Rt
mnk ≤Rk, ∀n, k, t,
(27)
Rt
mnk ≥dt
mnkrm, ∀m, n, k, t,
(28)
Rt
mnk ≤dt
mnk, ∀m, n, k, t.
(29)
Therefore, the optimization problem under consideration is a
standard convex problem with linear constraints. In this way,
it can be efficiently solved using existing convex optimization
solvers, such as Gurobi or Mosek, within each edge node.
V. EVALUATION
A. Evaluation Setups
Implementation Settings: Our implementation employs
VLLM 0.6.2 [26], a state-of-the-art LLM serving system op-
timized for high-throughput concurrent request processing, as
the inference backend. For the policy network, the architecture
features four fully-connected layers (256-128-64-action dim)
with batch normalization and residual connections. For policy
optimization, we configure PPO with a learning rate of 3e−4
and a clipping threshold (ϵ) of 0.02 to balance exploration and


--- Page 9 ---
stability. For the feedback signal, the weight factor is set to
be 1 and 0.5, respectively, determined through manual tuning
to optimize overall system effectiveness. Retrieval operations
leverage a Faiss-based vector database with a flat index for
exact similarity search, retrieving the top-5 documents per
query. To model heterogeneous edge environments, we deploy
four nodes: two equipped with a single NVIDIA RTX 4090
GPU and two with dual NVIDIA RTX 4090 GPUs, simulating
computational asymmetry. Note that our experiment targets
micro-edge computing clusters, which offer more compute
and memory than IoT devices yet remain resource-constrained
relative to cloud infrastructure2. Thus, RTX 4090, a cost-
effective consumer-grade GPU, is a well-suited choice for
real-world edge deployment [27]. Dynamic query patterns
are emulated using real-world request traces from the ECW-
New-App dataset [28], while domain-specific query bias is
synthetically generated via Dirichlet sampling to simulate
skewed per-slot query distributions typical of edge scenarios.
Datasets and Query Synthesis: To evaluate RAG perfor-
mance across diverse domains in collaborative edge comput-
ing, we employ two datasets, each spanning six domains:
1) DomainQA: Six domains (biomedicine, finance, law,
sports, technology, and travel) are selected from the BAAI
industry corpus series3. Domain-specific corpora at the edge
are constructed by sampling representative data from each
domain. Using DeepSeek-V3 API, we automate the generation
of 3,000 question-answer pairs per domain, ensuring alignment
with semantic and contextual information of the extracted
corpora. 2) Personalized-Proactive-Conversations (PPC)4:
User conversations with a cloud-based LLM assistant are
filtered to retain six distinct identity profiles (student, teacher,
parent, engineer, chef, and writer), reflecting diverse practical
scenarios. Through in-depth dialogue analysis, user inquiries
are transformed into a series of independent query statements,
preserving the authenticity of needs for reusing historical
conversations in real-world applications.
Edge-data Partition: To efficiently characterize the het-
erogeneous data distribution at the edge, similar to [29], the
edge-data partition follows a dual-distribution paradigm: s%
of the data samples from each client are distributed as i.i.d.
document chunks spanning all categories, while the remaining
(100 - s)% are allocated in a non-i.i.d. fashion where each node
is assigned data from three predefined domains. To explicitly
model cross-node knowledge-sharing scenarios, we introduce
an overlapping factor that scales both i.i.d. and non-i.i.d.
data portions, thereby facilitating knowledge-sharing potential
through controlled dataset intersections between nodes.
2Our architecture ensures hardware-agnostic operation: The hierarchical
scheduler only requires model latency prediction and GPU memory con-
straints, decoupled from specific hardware. Additionally, the node profiler
auto-adapts to devices ranging from high-performance GPUs to embedded
systems (e.g., Jetson Orin), while remaining applicable to less powerful
hardware through hardware-agnostic scheduling.
3https://huggingface.co/BAAI
4https://huggingface.co/datasets/erbacher/personalized-proactive-
conversations/viewer
Edge LLMs: We construct a heterogeneous model pool
comprising multiple parameter-efficient variants of widely
adopted architectures. Specifically, we curate three prominent
open-source model series optimized for edge deployment:
LLaMA [30], Qwen [31], and Falcon [32], spanning parameter
sizes of 1B/1.5B, 3B, and 7/8B. This multi-scale parameteri-
zation across architectures underscores the diverse deployment
requirements inherent to edge computing systems.
Evaluation Metrics: To evaluate generation quality, we em-
ploy common lexical and semantic metrics, including Rouge
[20], Bleu [33], Meteor [34], BERTScore [21] to quantify the
similarity between LLM responses and the references, where
higher scores indicate higher quality. In terms of the query
completion performance, we measure the DropRate, i.e., the
fraction of queries violating latency constraints per time slot.
B. Evaluation Results and Analysis
Query-edge Matching Impacts Generation Quality. To
verify the effectiveness of our online query identification
method, we benchmark against three baselines: 1) Random
Allocation randomly routes queries to edge nodes without
semantic awareness. 2) MAB-based Allocation adopts the
LinUCB algorithm [35] to allocate queries based on historical
performance and uncertainty estimates, rather than the feature
extraction of a neural network. 3) Oracle Allocation assigns
user queries to the optimal edge node with perfect knowledge
of corpus distributions. Table II compares the utility of our
method against baselines on the DomainQA and PPC datasets.
The results demonstrate that our approach significantly out-
performs both Random and MAB-based allocation strate-
gies across all metrics (around 4.23%–59.84% improvements
on DomainQA dataset and 3.93%–91.39% improvements on
PPC dataset). Notably, it achieves performance comparable to
the Oracle baseline, highlighting its robustness in practical,
privacy-sensitive edge environments. This superiority stems
from our method’s capacity to model high-dimensional query
semantics and dynamically align them with distributed corpus
data via iterative updates in dynamic edge environments.
In contrast, random allocation — agnostic to data seman-
tics — induces query-node mismatches, precluding LLMs
from exploiting high-quality relevant retrieval data. Besides,
MAB-based allocation fails to model high-dimensional query
features and struggles to adapt to environmental dynamics,
resulting in suboptimal performance.
Workload Balancing Impacts on LLM Performance.
Under controlled experimental conditions with fixed query
loads and stringent latency constraints (DomainQA: 2000
queries, 15 s latency; PPC: 1500 queries, 15 s latency), we
systematically evaluated performance by designating one do-
main as primary and progressively increasing its representation
within query batches to simulate workload skew.
For the DomainQA dataset, inter-node method consistently
outperformed w/o inter-node approaches, demonstrating mean
advantages of 12.65% in Rouge-L and 7.71% in BertScore
across varying domain concentrations. When the primary do-
main proportion increased from 0.5 to 0.9, the Rouge-L score


--- Page 10 ---
TABLE II: Performance comparison of different query allocation method.
ROUGE-1
ROUGE-2
ROUGE-L
BLEU-4
METEOR
BERTScore
DomainQA
Random
0.477
0.322
0.438
0.249
0.460
0.698
MAB
0.569
0.421
0.531
0.337
0.560
0.756
PPO
0.626
0.486
0.589
0.398
0.621
0.788
Oracle
0.648
0.506
0.609
0.413
0.650
0.806
PPC
Random
0.403
0.211
0.373
0.151
0.355
0.682
MAB
0.499
0.316
0.471
0.238
0.456
0.738
PPO
0.555
0.377
0.528
0.289
0.516
0.767
Oracle
0.569
0.388
0.541
0.296
0.526
0.777
0.5
0.6
0.7
0.8
0.9
Proportion of primary domain
0.40
0.45
0.50
0.55
Rouge-L
w/ inter-node
w/o inter-node
(a) Rouge-L performance on Do-
mainQA dataset.
0.5
0.6
0.7
0.8
0.9
Proportion of primary domain
0.60
0.65
0.70
0.75
0.80
BERTScore
w/ inter-node
w/o inter-node
(b) BertScore performance on Do-
mainQA dataset.
0.5
0.6
0.7
0.8
0.9
Proportion of primary domain
0.40
0.45
0.50
Rouge-L
w/ inter-node
w/o inter-node
(c) Rouge-L performance on PPC
dataset.
0.5
0.6
0.7
0.8
0.9
Proportion of primary domain
0.60
0.65
0.70
0.75
0.80
BERTScore
w/ inter-node
w/o inter-node
(d)
BertScore
performance
on
PPC dataset.
Fig. 5: Generation quality of different scheduling strategies.
decreased from 0.527 to 0.485 with inter-node scheduling,
compared to a decrease from 0.474 to 0.416 without inter-node
coordination. Similarly, the BertScore decreased from 0.754 to
0.718 with inter-node scheduling versus a decrease from 0.704
to 0.661 without. Similar trends emerged in PPC evaluations,
where inter-node scheduling maintained 8.21% and 7.13% av-
erage leads in Rouge-L and BertScore, respectively. With inter-
node scheduling, the Rouge-L score decreased from 0.446
to 0.425, while without inter-node coordination it decreased
from 0.422 to 0.383. The BertScore showed greater stability,
decreasing from 0.725 to 0.714 with inter-node scheduling
compared to a decrease from 0.686 to 0.659 without.
The findings highlight two key observations: (i) Perfor-
mance degrades with increasing domain skew across all con-
figurations, underscoring the fundamental difficulties of biased
query distributions. (ii) The dynamic exploration in inter-
node scheduling alleviates this degradation through cross-node
optimization. The architecture demonstrates robust real-world
applicability by maintaining sub-threshold latency while ensur-
ing stable performance and efficient data utilization — partic-
ularly valuable in environments with natural query skewness.
This resilience stems from the system’s probabilistic balancing
mechanism between immediate scheduling requirements and
exploratory node sampling, which prevents local optima that
worsen performance under skewed workloads.
Robustness in Different Latency SLOs. We introduced
baselines with different model combinations to serve user
queries: 1) Small-Param: each node deploys only small-
parameter models (1B/1.5B); 2) Mid-Param: each node
deploys only medium-parameter models (3B); 3) Mixed-
Param.1: each GPU deploys both small and medium-
parameter models with fixed query allocation proportion and
resource propotion; 4) Mixed-Param.2: single-GPU nodes
deploy small and medium-parameter models, while dual-GPU
nodes allocate one GPU to small/medium models and the other
to large-parameter models (7B/8B). Among these, queries are
evenly distributed among deployed models.
Experiments were conducted on the DomainQA (500
queries) and PPC (400 queries) datasets under latency con-
straints of 5 s, 10 s, and 15 s, translating to per-query pro-
cessing budgets of 10–30 ms and 12.5–37.5 ms, respectively.
This setup allowed us to assess RAG system performance
under progressively relaxed latency requirements. As shown
in Table III, intra-node scheduling demonstrates exceptional
adaptability across all latency conditions.
Under strict latency constraints (L = 5 s), our method
maintains a drop rate below 3%, with all metrics ranking
among the top two across both datasets. The Small-Param
configuration, exclusively equipped with small models, per-
forms particularly well, as its design inherently suits latency-
sensitive tasks. In contrast, Mid-Param and Mixed-Param.2
exhibit catastrophic drop rates (44.34%–66.71%) due to their
inability to dynamically allocate queries to smaller models or
reallocate resources. This results in severe degradation in LLM
generation quality. Meanwhile, Mixed-Param.1, although in-
corporating smaller models to share the processing burden, still
exhibits unacceptably poor performance (23.28%–34.44%).
Under moderate latency constraints (L = 10 s), our
method achieves superior performance across all metrics with
a near-zero drop rate. The relaxed latency constraints en-
able the intra-node scheduling mechanism to strategically
allocate more queries to larger models, enhancing overall
system performance. Meanwhile, the Mixed-Param.2 configu-
ration achieves notable improvements by co-deploying small-
, medium-, and large-parameter models, which enables an
effective balance between latency and generation quality. By


--- Page 11 ---
TABLE III: Performance (R-1: ROUGE-1, R-2: ROUGE-2, R-L: ROUGE-L) comparison of different allocation methods within
node over varying L (s) values.
indicate the best performance, and
indicate the second-best performance for each metric.
R-1
R-2
R-L
BLEU-4
METEOR
BERTScore
DropRate(%)
DomainQA
L=5
Small-Param
0.552
0.408
0.520
0.323
0.552
0.743
3.57
Mid-Param
0.204
0.159
0.191
0.132
0.201
0.459
66.71
Mixed-Param.1
0.394
0.299
0.371
0.242
0.392
0.611
34.44
Mixed-Param.2
0.214
0.168
0.203
0.138
0.214
0.468
65.72
Intra-node
0.555
0.410
0.523
0.324
0.556
0.746
2.81
L=10
Small-Param
0.585
0.435
0.552
0.345
0.588
0.768
0.02
Mid-Param
0.588
0.439
0.547
0.355
0.584
0.769
0.01
Mixed-Param.1
0.605
0.459
0.569
0.370
0.606
0.779
0.02
Mixed-Param.2
0.611
0.466
0.574
0.377
0.612
0.782
0.04
Intra-node
0.626
0.484
0.587
0.397
0.624
0.789
0.14
L=15
Small-Param
0.585
0.435
0.552
0.345
0.588
0.768
0.00
Mid-Param
0.626
0.483
0.586
0.395
0.623
0.790
0.00
Mixed-Param.1
0.606
0.459
0.569
0.370
0.606
0.779
0.00
Mixed-Param.2
0.611
0.466
0.574
0.377
0.612
0.782
0.00
Intra-node
0.637
0.499
0.598
0.412
0.635
0.796
0.00
PPC
L=5
Small-Param
0.478
0.296
0.453
0.217
0.442
0.722
1.84
Mid-Param
0.310
0.215
0.295
0.163
0.278
0.557
44.34
Mixed-Param.1
0.396
0.259
0.376
0.192
0.359
0.640
23.28
Mixed-Param.2
0.214
0.145
0.204
0.109
0.197
0.476
60.78
Intra-node
0.478
0.297
0.453
0.216
0.444
0.722
1.96
L=10
Small-Param
0.507
0.324
0.481
0.238
0.473
0.741
0.01
Mid-Param
0.517
0.339
0.490
0.254
0.464
0.747
0.02
Mixed-Param.1
0.529
0.352
0.504
0.264
0.489
0.752
0.71
Mixed-Param.2
0.536
0.359
0.510
0.272
0.500
0.756
0.63
Intra-node
0.550
0.374
0.524
0.287
0.513
0.764
0.46
L=15
Small-Param
0.509
0.325
0.484
0.239
0.474
0.742
0.00
Mid-Param
0.554
0.380
0.527
0.290
0.506
0.767
0.00
Mixed-Param.1
0.533
0.355
0.507
0.266
0.492
0.755
0.00
Mixed-Param.2
0.540
0.361
0.514
0.273
0.503
0.759
0.00
Intra-node
0.564
0.388
0.537
0.301
0.529
0.773
0.00
5
10
15
L
0
1
Proportions
1
0.08
0.07
0.72
0.24
0.2
0.69
Small
Mid
Large
(a) Proportion of queries on Do-
mainQA dataset.
5
10
15
L
0
1
Proportions
1
0.23
0.04
0.47
0.14
0.3
0.82
Small
Mid
Large
(b) Proportion of resources on Do-
mainQA dataset.
5
10
15
L
0
1
Proportions
1
0.18
0.46
0.35
0.36
0.65
Small
Mid
Large
(c) Proportion of queries on PPC
dataset.
5
10
15
L
0
1
Proportions
1
0.13
0.37
0.19
0.5
0.81
Small
Mid
Large
(d) Proportion of resources on
PPC dataset.
Fig. 6: Proportion with different model sizes.
assigning a portion of queries to smaller models to avoid
severe timeouts, and reserving larger models for acceptable
LLM responses, the system ensures both timely responses
and satisfactory generation quality. In contrast, Mixed-Param.1
only deploys small and medium-parameter models, resulting
in relatively lower generation quality. Notably, Small-Param’s
performance plateaus, as its exclusive reliance on small models
prevents it from capitalizing on the relaxed latency constraints.
This limitation highlights the critical role of model diversity
in adaptive scheduling frameworks. For Mid-Param, while the
inter-node scheduling’s capacity exploration mechanism effec-
tively mitigates query overload, the exclusive use of medium-
parameter models eliminates small-model fallback capabilities,
preventing optimal node assignment and consequently com-
promising overall system performance. The empirical evalu-
ation reveals a fundamental trade-off: while an over-reliance
on smaller models ensures higher reliability under stringent
latency constraints, it inherently restricts the achievable quality
improvements when requirements are loosened. These findings
provide compelling evidence for adopting dynamic resource
allocation mechanisms, which enables adaptive performance
optimization across varying operational conditions, thereby
balancing efficiency and effectiveness in real-time systems.
Under relaxed latency constraints (L = 15 s), our method
continues to demonstrate consistent superiority, maintaining
exceptional service quality while simultaneously maximiz-
ing generation fidelity through intelligent adaptive model
deployment. Notably, Mid-Param benefits from the further


--- Page 12 ---
relaxed latency constraints, eliminating the need for inter-
node scheduling with sub-optimal results, which leads to a
significant improvement in generation quality. In contrast,
other strategies show minimal to no performance gains.
The comprehensive experimental evaluation conclusively
demonstrates that our intra-node scheduling paradigm consis-
tently outperforms conventional approaches across the entire
latency constraint spectrum. Unlike these baselines, which
either degrade significantly under stringent temporal require-
ments or fail to capitalize on relaxed constraints to en-
hance output quality, our solution maintains exceptional sta-
bility while simultaneously preserving high generation fidelity.
These results not only validate the proposed architecture’s
robustness but also establish a new benchmark for adaptive
resource management in distributed inference systems.
Dynamic Adjustability Across Different Latency SLOs.
To analyze the adaptive behavior of our intra-node scheduling
strategy across varying latency conditions, we systematically
tracked both query and resource allocation patterns across
different model sizes (Fig. 6). The results reveal three distinct
operational regimes: Under strict latency constraints, the
scheduler demonstrates intelligent prioritization by directing
all of queries to small-parameter models, thereby guarantee-
ing reliable adherence to stringent Service Level Objectives
(SLOs). Under moderate latency constraints, the system au-
tomatically rebalances toward quality optimization: As shown
in Fig. 6a and 6c, medium-parameter models handle 72% and
46% of queries on DomainQA and PPC, respectively, consum-
ing a proportional share of resources (47%, 37%). Concur-
rently, large-parameter models process 28% (30% resources)
and 54% (50% resources) of the workloads. This graded real-
location underscores its adaptive nature, leveraging increased
latency headroom for higher-quality outputs. Notably, resource
allocation patterns exhibit non-linear scaling–large-parameter
models receive disproportionately higher resources per query,
reflecting their greater computational demands. Under relaxed
latency constraints, the scheduling strategy fully exploits the
available latency budget, directing the majority of queries
(65% and 69%) to large-parameter models to maximize re-
sponse quality. The observed dynamics demonstrate our sched-
uler’s capability to make principled latency-quality trade-offs.
Particularly noteworthy is the consistent correlation between
query distribution and resource allocation across both datasets,
validating the robustness of our adaptive mechanisms.
VI. RELATED WORK
A. RAG-based LLM
Significant efforts have been made to optimize RAG sys-
tems from various perspectives. For instance, self-RAG [15]
introduces a reflection token and self-reflection mechanism,
enabling LLMs to dynamically retrieve, generate, and evalu-
ate content during the generation process. BlendedRAG [23]
enhances system accuracy by integrating dense vector indexing
from semantic search with sparse encoder indexing, utilizing
a hybrid query strategy. Beyond improving retrieval quality,
another major thrust of research focuses on enhancing the ex-
ecution efficiency of RAG systems. EdgeRAG [22] addresses
performance issues on edge devices by pruning embedding
vectors and employing caching strategies to reduce retrieval
latency and optimize memory usage. PipeRAG [36] improves
generation efficiency through co-designing algorithms and
systems, enabling pipeline parallelism and dynamic retrieval
adjustments. RAGCache [37] significantly reduces latency
and boosts throughput by efficiently caching and reusing
intermediate states of retrieved documents through a multi-
level dynamic caching mechanism. However, these systems
primarily target single-node optimization. In contrast, our
CoEdge-RAG focuses on multi-node architectures, enhanc-
ing the coordination efficiency of distributed RAG systems
through hierarchical scheduling.
B. Collaborative Edge LLM Serving
PICE [6] introduces a semantic-driven progressive infer-
ence system for edge-cloud environments, employing dynamic
scheduling strategies to achieve high throughput and low-
latency parallel edge inference. Ding et al. [38] enhance
on-device LLM inference by constructing a local external
datastore from historical cloud-based LLM interactions, lever-
aging k-nearest neighbor language modeling method. Helix
[39] optimizes LLM serving in heterogeneous GPU clusters
through layer placement and request scheduling formulated
as a Max-Flow problem, ensuring high throughput and low
latency. Edgeshard [40] partitions LLMs into shards and
utilizes dynamic programming to optimize edge deployment
efficiency. These efforts primarily focus on computational or
architectural optimizations. Crucially, the rich, distributed data
at the edge — a pivotal resource for refining LLM capabilities
— remains underutilized in existing methodologies.
VII. CONCLUSION
This paper presents CoEdge-RAG, a hierarchical framework
for optimizing retrieval-augmented LLMs in collaborative
edge environments. By integrating an online query identifi-
cation mechanism, dynamic inter-node workload balancing,
and intra-node resource allocation, CoEdge-RAG efficiently
unleashes the potential of distributed data and heterogeneous
computational resources to enhance edge-based LLM infer-
ence. Rigorous evaluations demonstrate the efficacy and ro-
bustness of CoEdge-RAG in handling dynamic and diverse
queries, paving the way for its widespread adoption in future
ubiquitous edge intelligence applications.
REFERENCES
[1] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang,
J. Zhang, Z. Dong et al., “A survey of large language models,” arXiv
preprint arXiv:2303.18223, 2023.
[2] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng,
C. Zhang, C. Ruan et al., “Deepseek-v3 technical report,” arXiv preprint
arXiv:2412.19437, 2024.
[3] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman,
D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4
technical report,” arXiv preprint arXiv:2303.08774, 2023.


--- Page 13 ---
[4] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,
A. Mathur, A. Schelten, A. Yang, A. Fan et al., “The llama 3 herd of
models,” arXiv preprint arXiv:2407.21783, 2024.
[5] Z. Lu, X. Li, D. Cai, R. Yi, F. Liu, X. Zhang, N. D. Lane, and M. Xu,
“Small language models: Survey, measurements, and insights,” CoRR,
vol. abs/2409.15790, 2024.
[6] H. Zhan, X. Zhang, H. Tan, H. Tian, D. Yong, J. Zhang, and X. Li,
“PICE: A semantic-driven progressive inference system for LLM serving
in cloud-edge networks,” CoRR, vol. abs/2501.09367, 2025.
[7] G. Xiong, Q. Jin, Z. Lu, and A. Zhang, “Benchmarking retrieval-
augmented generation for medicine,” in Proc. of ACL, 2024, pp. 6233–
6251.
[8] T. Zhang, S. G. Patil, N. Jain, S. Shen, M. Zaharia, I. Stoica, and J. E.
Gonzalez, “RAFT: adapting language model to domain specific RAG,”
CoRR, vol. abs/2403.10131, 2024.
[9] M. A. de Luis Balaguer, V. Benara, R. L. de Freitas Cunha, R. de M. Es-
tev˜ao Filho, T. Hendry, D. Holstein, J. Marsman, N. Mecklenburg,
S. Malvar, L. O. Nunes, R. Padilha, M. Sharp, B. Silva, S. Sharma,
V. Aski, and R. Chandra, “RAG vs fine-tuning: Pipelines, tradeoffs, and
a case study on agriculture,” CoRR, vol. abs/2401.08406, 2024.
[10] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,
H. K¨uttler, M. Lewis, W.-t. Yih, T. Rockt¨aschel et al., “Retrieval-
augmented generation for knowledge-intensive nlp tasks,” Proc. of
NeurIPS, vol. 33, pp. 9459–9474, 2020.
[11] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and
H. Wang, “Retrieval-augmented generation for large language models:
A survey,” arXiv preprint arXiv:2312.10997, 2023.
[12] K. Shuster, S. Poff, M. Chen, D. Kiela, and J. Weston, “Retrieval aug-
mentation reduces hallucination in conversation,” in Proc. of EMNLP,
2021, pp. 3784–3803.
[13] C. Jin, Z. Zhang, X. Jiang, F. Liu, X. Liu, X. Liu, and X. Jin, “Ragcache:
Efficient knowledge caching for retrieval-augmented generation,” arXiv
preprint arXiv:2404.12457, 2024.
[14] T. Ouyang, G. Hong, K. Zhao, Z. Zhou, W. Wu, Z. Lv, and X. Chen,
“Adarag: Adaptive optimization for retrieval augmented generation with
multilevel retrievers at the edge,” in Proc. of INFOCOM, 2025, pp. 1–10.
[15] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, “Self-rag: Learning
to retrieve, generate, and critique through self-reflection,” in Proc. of
ICLR, 2024.
[16] D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt,
and J. Larson, “From local to global: A graph RAG approach to query-
focused summarization,” CoRR, vol. abs/2404.16130, 2024.
[17] H. Wang, W. Huang, Y. Deng, R. Wang, Z. Wang, Y. Wang, F. Mi,
J. Z. Pan, and K. Wong, “Unims-rag: A unified multi-source retrieval-
augmented generation for personalized dialogue systems,” CoRR, vol.
abs/2401.13256, 2024.
[18] R. Yang, T. F. Tan, W. Lu, A. J. Thirunavukarasu, D. S. W. Ting,
and N. Liu, “Large language models in health care: Development,
applications, and challenges,” Health Care Science, vol. 2, no. 4, pp.
255–263, 2023.
[19] M. S. Ali, M. M. Ahsan, L. Tasnim, S. Afrin, K. Biswas, M. M. Hossain,
M. M. Ahmed, R. Hashan, M. K. Islam, and S. Raman, “Federated
learning in healthcare: Model misconducts, security, challenges, applica-
tions, and future research directions–a systematic review,” arXiv preprint
arXiv:2405.13832, 2024.
[20] C.-Y. Lin, “ROUGE: A package for automatic evaluation of summaries,”
in Text Summarization Branches Out, 2004, pp. 74–81.
[21] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, “Bertscore:
Evaluating text generation with BERT,” in Proc. of ILCR, 2020.
[22] K. Seemakhupt, S. Liu, and S. M. Khan, “Edgerag: Online-indexed RAG
for edge devices,” CoRR, vol. abs/2412.21023, 2024.
[23] K. Sawarkar, A. Mangal, and S. R. Solanki, “Blended RAG: improving
RAG (retriever-augmented generation) accuracy with semantic search
and hybrid query-based retrievers,” in Proc. of MIPR.
IEEE, 2024, pp.
155–161.
[24] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
“Proximal policy optimization algorithms,” CoRR, vol. abs/1707.06347,
2017.
[25] H. Yu, M. J. Neely, and X. Wei, “Online convex optimization with
stochastic constraints,” in Proc. of NeurIPS, 2017, pp. 1428–1438.
[26] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez,
H. Zhang, and I. Stoica, “Efficient memory management for large
language model serving with pagedattention,” in Proc. of SOSP, 2023,
pp. 611–626.
[27] J. Park, S. Cho, and D. Han, “Specedge: Scalable edge-assisted serving
framework for interactive llms,” CoRR, vol. abs/2505.17052, 2025.
[28] S. Huang, Z. Wang, H. Zhang, X. Wang, C. Zhang, and W. Wang, “One
for all: Unified workload prediction for dynamic multi-tenant edge cloud
platforms,” in Proc. of ACM SIGKDD, 2023, pp. 788–797.
[29] S. P. Karimireddy, S. Kale, M. Mohri, S. J. Reddi, S. U. Stich, and
A. T. Suresh, “SCAFFOLD: stochastic controlled averaging for federated
learning,” in Proc. of ICML, vol. 119, 2020, pp. 5132–5143.
[30] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle,
A. Letman, A. Mathur, A. Schelten, A. Vaughan et al., “The llama 3
herd of models,” arXiv preprint arXiv:2407.21783, 2024.
[31] Qwen, :, A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li,
D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang,
J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li,
M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Tang, T. Xia, X. Ren,
X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and
Z. Qiu, “Qwen2.5 technical report,” arXiv preprint arXiv:2412.15115,
2025.
[32] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojo-
caru, M. Debbah, ´E. Goffinet, D. Hesslow, J. Launay, Q. Malartic
et al., “The falcon series of open language models,” arXiv preprint
arXiv:2311.16867, 2023.
[33] K. Papineni, S. Roukos, T. Ward, and W. Zhu, “Bleu: a method for
automatic evaluation of machine translation,” in Proc. of ACL, 2002,
pp. 311–318.
[34] S. Banerjee and A. Lavie, “Meteor: An automatic metric for mt
evaluation with improved correlation with human judgments,” in Proc.
of ACL workshop, 2005, pp. 65–72.
[35] L. Li, W. Chu, J. Langford, and R. E. Schapire, “A contextual-bandit
approach to personalized news article recommendation,” in Proc. of
WWW, 2010, pp. 661–670.
[36] W. Jiang, S. Zhang, B. Han, J. Wang, B. Wang, and T. Kraska, “Piperag:
Fast retrieval-augmented generation via adaptive pipeline parallelism,”
in Proc. of ACM SIGKDD, 2025, pp. 589–600.
[37] C. Jin, Z. Zhang, X. Jiang, F. Liu, X. Liu, X. Liu, and X. Jin, “Ragcache:
Efficient knowledge caching for retrieval-augmented generation,” CoRR,
vol. abs/2404.12457, 2024.
[38] Y. Ding, C. Niu, F. Wu, S. Tang, C. Lyu, and G. Chen, “Enhancing
on-device llm inference with historical cloud-based llm interactions,” in
Proc. of ACM SIGKDD, 2024, pp. 597–608.
[39] Y. Mei, Y. Zhuang, X. Miao, J. Yang, Z. Jia, and R. Vinayak, “Helix:
Distributed serving of large language models via max-flow on hetero-
geneous gpus,” arXiv preprint arXiv:2406.01566, 2024.
[40] M. Zhang, X. Shen, J. Cao, Z. Cui, and S. Jiang, “Edgeshard: Efficient
llm inference via collaborative edge computing,” IEEE Internet of Things
Journal, vol. 12, no. 10, pp. 13 119–13 131, 2025.
